file_path,api_count,code
NN.py,20,"b'""""""\nImplementation of a neural network class from scratch.\n\nThe network is built on sigmoid neurons, using SGD for optimization and mean square error function for cost calculation.\n""""""\nimport numpy as np\nimport random\nimport itertools\n\n\ndef sigmoid(z):\n\treturn 1.0/(1.0+np.exp(-1.0*z))\n\ndef derivative_sigmoid(z):\n\ttemp=sigmoid(z)\n\treturn np.multiply(temp,1-temp)\n\ndef one_hot_vector(size,pos):\n\ttemp=np.zeros(size)\n\ttemp[pos]=1\n\treturn np.transpose([temp])\n\nclass NN:\n\tdef __init__(self, n_neurons):\n\t\tself.n_layers=len(n_neurons)\n\t\tself.n_neurons=n_neurons\n\t\tself.b=[np.random.normal(loc=0.0,scale=0.1,size=(size,1)) for size in n_neurons[1:]]\n\t\tself.w=[np.random.normal(loc=0.0,scale=0.1,size=(size_next,size_curr)) for size_curr,size_next in zip(n_neurons[:-1],n_neurons[1:])]\n\n\tdef calc_output(self,input_values):\n\t\tif len(input_values)==self.n_neurons[0]:\n\t\t\tz=input_values\n\t\t\tfor i in range(self.n_layers-1):\n\t\t\t\tz=sigmoid(np.dot(self.w[i],z)+self.b[i])\n\t\t\treturn z\n\n\tdef test(self,test_data):\n\t\t"""""" Function to calculate the accuracy of the NN over the provided test_data """"""\n\t\tif test_data:\n\t\t\tcorrect_answers=0.0\n\t\t\tfor x,y in test_data:\n\t\t\t\toutput=self.calc_output(x)\n\t\t\t\tif np.argmax(output)==y:\n\t\t\t\t\tcorrect_answers+=1\n\t\t\treturn correct_answers/len(test_data)\n\n\n\tdef train(self,training_data,learning_rate=1.0,mini_batch_size=1,n_epochs=10,validation_data=None):\n\t\t""""""\n\t\tFunction which trains the neural network with the provided training data by updating the weights and biases using stochastic gradient descent in order to minimize the mean squared error.\n\n\t\tIf mini-batch size isn\'t provided, this function trains the parameters by the on-line/iterative algorithm.\n\t\t""""""\n\n\t\tif training_data is not None:\n\t\t\tfor epoch_no in range(n_epochs):\n\t\t\t\t"""""" Partition training data into mini-batches (each training data-point is an [input,true_output] pair) """"""\n\t\t\t\trandom.shuffle(training_data)\n\n\t\t\t\tfor batch_no in range(0, len(training_data), mini_batch_size):\n\t\t\t\t\tprint(""\\33[2KEpoch : {0}, Mini-batch : {1}\\r"".format(epoch_no+1,int(batch_no/mini_batch_size+1)),end=\'\')\n\t\t\t\t\t"""""" Initialize matrices to accumulate the changes in weights and biases """"""\n\t\t\t\t\tweight_errors=[np.zeros(np.shape(x)) for x in self.w]\n\t\t\t\t\tbias_errors=[np.zeros(np.shape(x)) for x in self.b]\n\n\t\t\t\t\t"""""" Iterate over all training points in the mini-batch """"""\n\t\t\t\t\tfor i in range(mini_batch_size):\n\t\t\t\t\t\tj=batch_no+i\n\t\t\t\t\t\tif j>=len(training_data):\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\tx=training_data[j][0]\n\t\t\t\t\t\ty=training_data[j][1]\n\t\t\t\t\t\t#print(x,y)\n\t\t\t\t\t\tactivations=[x]\n\t\t\t\t\t\tweighted_inputs=[]\n\t\t\t\t\t\tdelta=[]\n\t\t\t\t\t\tfor layer_index in range(self.n_layers-1):\n\t\t\t\t\t\t\tweighted_inputs.append(np.dot(self.w[layer_index],x)+self.b[layer_index])\n\t\t\t\t\t\t\tx=sigmoid(weighted_inputs[-1])\n\t\t\t\t\t\t\tactivations.append(x)\n\n\t\t\t\t\t\t"""""" Backpropagation algorithm """"""\n\t\t\t\t\t\t"""""" Initialize deltas for last layer """"""\n\t\t\t\t\t\tdelta_L=np.multiply(np.subtract(activations[-1],one_hot_vector(10,y)),derivative_sigmoid(weighted_inputs[-1]))\n\t\t\t\t\t\tdelta.append(delta_L)\n\t\t\t\t\t\t#print(np.shape(delta_L))\n\n\t\t\t\t\t\t"""""" Find deltas for other layers by propagating backwards """"""\n\t\t\t\t\t\tfor l in range(self.n_layers-2,0,-1):\n\t\t\t\t\t\t\tdelta_l=np.multiply(np.dot(np.transpose(self.w[l]),delta[0]),derivative_sigmoid(weighted_inputs[l-1]))\n\t\t\t\t\t\t\tdelta.insert(0,delta_l)\n\t\t\t\t\t\t\t#print(np.shape(delta_l))\n\n\t\t\t\t\t\tfor nw in range(len(weight_errors)):\n\t\t\t\t\t\t\tr,c=np.shape(weight_errors[nw])\n\t\t\t\t\t\t\tweight_errors[nw]=np.add(weight_errors[nw], np.dot(delta[nw][:r],np.transpose(activations[nw][:c])) )\n\t\t\t\t\t\t\tbias_errors[nw]=np.add(bias_errors[nw],delta[nw])\n\n\t\t\t\t\t"""""" Update the weights and biases """"""\n\t\t\t\t\tself.w=np.add(self.w,-learning_rate/mini_batch_size*np.array(weight_errors))\n\t\t\t\t\tself.b=np.add(self.b,-learning_rate/mini_batch_size*np.array(bias_errors))\n\n\t\t\t\t"""""" Testing the current parameters on the training/validation data provided """"""\n\t\t\t\tif not validation_data:\n\t\t\t\t\tprint(""\\33[2KEpoch {0} complete.\\nTraining data accuracy = {1}"".format(epoch_no+1,self.test(training_data)))\n\t\t\t\telse:\n\t\t\t\t\tprint(""\\33[2KEpoch {0} complete.\\nTraining data accuracy = {1}\\tValidation data accuracy = {2}"".format(epoch_no+1,self.test(training_data),self.test(validation_data)))\n'"
handwritten_digit_classifier.py,2,"b'if __name__ == \'__main__\':\n    import NN\n    import mnist_loader as ml\n\n    training_data,validation_data,test_data=ml.load_data_wrapper();\n    """"""\n    Initializing a deep neural network with 30 neurons in hidden layer.\n    Custom NN architecture is also welcome.\n    """"""\n    net=NN.NN([784,30,10]);\n\n    """"""\n    Training goes on for a default of 10 epochs. The accuracy over both the training and the validation datasets is printed after each epoch.\n    """"""\n    net.train(training_data,learning_rate=3.0,mini_batch_size=50,validation_data=validation_data)\n\n    print(""Accuracy over test data = {0}"".format(net.test(test_data)))\n\n    """"""\n    # Code snippet to store the trained weights and biases\n\n    np.save(""digit_classifier_weights"",net.w)\n    np.save(""digit_classifier_biases"",net.b)\n    """"""\n'"
mnist_loader.py,8,"b""import _pickle as cPickle\nimport numpy as np\nimport gzip\n\ndef load_data():\n    f = gzip.open('data/MNIST/mnist.pkl.gz', 'rb')\n    training_data, validation_data, test_data = cPickle.load(f,encoding='latin-1')\n    f.close()\n    return (training_data, validation_data, test_data)\n\ndef load_data_wrapper():\n    tr_d, va_d, te_d = load_data()\n    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n    training_results = tr_d[1]\n    training_data = zip(training_inputs, training_results)\n    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n    validation_data = zip(validation_inputs, va_d[1])\n    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n    test_data = zip(test_inputs, te_d[1])\n    return (list(training_data), list(validation_data), list(test_data))\n\n# Specific functions for tf_DNN.py\ndef one_hot_vector(size,pos):\n    ans=np.zeros([1,size])\n    ans[0,pos]=1\n    return ans\n\ndef get_matrices():\n    tr_d, va_d, te_d = load_data()\n\n    tr_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n    tr_results = [one_hot_vector(10,i) for i in tr_d[1]]\n\n    va_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n    va_results = [one_hot_vector(10,i) for i in va_d[1]]\n\n    te_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n    te_results = [one_hot_vector(10,i) for i in te_d[1]]\n    return np.transpose(np.concatenate(tr_inputs,axis=1)),np.concatenate(tr_results,axis=0),np.transpose(np.concatenate(va_inputs,axis=1)),np.concatenate(va_results,axis=0),np.transpose(np.concatenate(te_inputs,axis=1)),np.concatenate(te_results,axis=0)\n"""
tf_DNN.py,6,"b'""""""\nImplementation of a deep neural network of specified structure using TensorFlow.\nCurrent setup : Xavier Glorot initialization, ReLU activation, \'cross entropy\' cost function (with softmax for last layer) and Gradient Descent optimizer\nAccuracy : ~ 96 %\n\nIt uses matrix-based batch training, with the following possible variations :\n - Gradient Descent / Adam optimizer\n - Mean square / cross-entropy cost functions\n - Sigmoid (which is similar to tanh) / ReLU / softmax activation functions\n\nAfter training is complete, these variations are compared using the corresponding learning curves.\n(the MNIST database shall be used for comparision)\n""""""\n\nimport tensorflow as tf\nimport numpy as np\nimport random\nimport mnist_loader as ml\n\ndef one_hot_vector(size,pos):\n    ans=np.zeros([1,size])\n    ans[0,pos]=1\n    return ans\n\ndef partition_dataset(training_data,size):\n    """""" Function to return groups of size `size` along with their correct outputs """"""\n    random.shuffle(training_data)\n    inp=[]; out=[]; cntr=1;\n    inp.append(training_data[0][0])\n    out.append(one_hot_vector(10,training_data[0][1]))\n    for i in range(1,len(training_data)):\n        if cntr>=size:\n            cntr=0\n            inp.append(training_data[i][0])\n            out.append(one_hot_vector(10,training_data[i][1]))\n        else:\n            inp[-1]=np.concatenate([inp[-1],training_data[i][0]],axis=1)\n            out[-1]=np.concatenate([out[-1],one_hot_vector(10,training_data[i][1])],axis=0)\n            cntr+=1\n\n    return list(zip([np.transpose(x) for x in inp],out))\n\n\nclass DNN:\n    def __init__(self,layer_sizes):\n        self.n_layers=len(layer_sizes)\n\n        # Define variables\n        self.b=[tf.Variable(tf.truncated_normal([1,x],stddev=0.1)) for x in layer_sizes[1:]]\n        self.w=[tf.Variable(tf.truncated_normal([j,k],stddev=0.1)) for j,k in zip(layer_sizes[:-1],layer_sizes[1:])]\n\n        # Define placeholders\n        self.inp=tf.placeholder(tf.float32,[None,layer_sizes[0]])\n        self.correct_output=tf.placeholder(tf.float32,[None,layer_sizes[-1]])\n\n        # Define the computational graph\n        self.result=tf.sigmoid(tf.add(tf.matmul(self.inp,self.w[0]),self.b[0]))\n        logits=None\n        for i in range(1,self.n_layers-1):\n            # remove this `if` statement and keep only the `else` part when using mean square\n            if i==self.n_layers-2:\n                # store only the weighted input for last layer\n                logits=tf.add(tf.matmul(self.result,self.w[i]),self.b[i])\n                """""" Sigmoid activation function """"""\n                # self.result=tf.nn.softmax(logits)\n                """""" ReLU activation function """"""\n                self.result=tf.nn.relu(logits)\n            else:\n                """""" Sigmoid activation function """"""\n                # self.result=tf.sigmoid(tf.add(tf.matmul(self.result,self.w[i]),self.b[i]))\n                """""" ReLU activation function """"""\n                self.result=tf.nn.relu(tf.add(tf.matmul(self.result,self.w[i]),self.b[i]))\n\n        # Compute loss/error\n        """""" Cross entropy loss function """"""\n        self._cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=self.correct_output))\n        """""" Mean square loss function """"""\n        # self._cost=tf.reduce_mean(tf.square(self.correct_output-self.result))\n\n        # Define the training action\n        self.train_step=None\n\n        print(tf.trainable_variables())\n        self._init=tf.global_variables_initializer()\n\n    def train(self, training_data, mini_batch_size=50, n_epochs=50, learning_rate=3.0):\n        """""" Gradient Descent optimizer """"""\n        self.train_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(self._cost)\n        """""" Adam optimizer """"""\n        # self.train_step=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self._cost)\n        accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(self.result,axis=1),tf.argmax(self.correct_output,axis=1)),tf.float32))\n\n        # Get training, validation and test data matrices\n        i_tr,o_tr,i_va,o_va,i_te,o_te=ml.get_matrices()\n\n        sess=tf.Session()\n        sess.run(tf.initialize_all_variables())\n\n        # initialize writer for using TensorBoard\n        tf.summary.scalar(""Training Accuracy"", accuracy)\n        tf.summary.scalar(""Cost"", self._cost)\n        summary_op = tf.summary.merge_all()\n        writer = tf.summary.FileWriter(""./logs"", graph=sess.graph)\n\n        for epoch_no in range(n_epochs):\n            cntr=1\n            print(""Epoch number {0}"".format(epoch_no+1))\n            for batch,batch_output in partition_dataset(training_data,mini_batch_size):\n                _,summary=sess.run([self.train_step,summary_op],{self.inp:batch, self.correct_output:batch_output})\n                writer.add_summary(summary,epoch_no)\n                print(""\\33[2K Mini-batch = {0}. Accuracy = {1}\\r"".format(cntr,sess.run(accuracy,{self.inp:batch, self.correct_output:batch_output})),end=\'\',flush=True)\n                cntr+=1\n\n            # Display accuracies over training and validation data\n            print("""")\n            print(""Training data accuracy = {0}"".format(sess.run(accuracy,{self.inp:i_tr, self.correct_output:o_tr})))\n            print(""Validation data accuracy = {0}"".format(sess.run(accuracy,{self.inp:i_va, self.correct_output:o_va})))\n\n        print(""\\nTest data accuracy = {0}"".format(sess.run(accuracy,{self.inp:i_te, self.correct_output:o_te})))\n        sess.close()\n'"
tf_handwritten_digit_classifier.py,0,"b'import tf_DNN\nimport mnist_loader as ml\n\ntr_d, va_d, te_d = ml.load_data_wrapper()\n# Custom structures are possible - just change the layer sizes/number\nnet=tf_DNN.DNN([784,50,10])\n# Number of epochs, mini-batch size and learning rate can also be initialized here\nnet.train(tr_d)\n'"
Convolutional_Neural_Network/mnist_classifier_cnn.py,0,"b'from tf_CNN import tf_CNN as CNN\nfrom tf_CNN_layers import ConvPoolLayer, ConnectedLayer, SoftmaxOutputLayer\nimport mnist_loader as ml\n\ntr_d, va_d, te_d = ml.load_data_wrapper()\n\ncnet = CNN(\n            [\n                ConvPoolLayer(\n                                (50,28,28,1),\n                                (5,5,20),\n                                1,\n                                (2,2),\n                            ),\n                ConvPoolLayer(\n                                (50,12,12,20),\n                                (3,3,16),\n                                1,\n                                (2,2),\n                                pool_stride=2,\n                                linear_output=True,\n                            ),\n                ConnectedLayer(\n                                n_in=5*5*16,\n                                n_out=1000,\n                                mini_batch_size=50,\n                            ),\n                SoftmaxOutputLayer(\n                                n_in=1000,\n                                n_out=10,\n                                mini_batch_size=50,\n                            )\n            ]\n            )\ncnet.train(tr_d,learning_rate=0.5,test_data=te_d,validation_data=va_d)\n'"
Convolutional_Neural_Network/mnist_loader.py,3,"b""import _pickle as cPickle\nimport numpy as np\nimport gzip\n\ndef load_data():\n    f = gzip.open('../data/MNIST/mnist.pkl.gz', 'rb')\n    training_data, validation_data, test_data = cPickle.load(f,encoding='latin-1')\n    f.close()\n    return (training_data, validation_data, test_data)\n\ndef load_data_wrapper():\n    tr_d, va_d, te_d = load_data()\n    training_inputs = [np.reshape(x, (28, 28)) for x in tr_d[0]]\n    training_results = tr_d[1]\n    training_data = [training_inputs, training_results]\n    validation_inputs = [np.reshape(x, (28, 28)) for x in va_d[0]]\n    validation_data = [validation_inputs, va_d[1]]\n    test_inputs = [np.reshape(x, (28, 28)) for x in te_d[0]]\n    test_data = [test_inputs, te_d[1]]\n    return (training_data, validation_data, test_data)\n"""
Convolutional_Neural_Network/tf_CNN.py,7,"b'""""""\nImplementation of a Convolutional Neural Network in TensorFlow.\n\nThis class provides an interface for linking the different layers which are defined in `tf_CNN_layers.py`.\nThus, CNNs of varying architecture can be created.\n\nFor details regarding these layers, have a look at the `tf_CNN_layers.py` file.\n""""""\n\nimport tensorflow as tf\nimport numpy as np\nimport random\n\ndef one_hot(size,pos):\n    ans=np.zeros(size)\n    ans[pos]=1\n    return ans\n\nclass tf_CNN:\n    def __init__(self, layers):\n        self.layers=layers\n\n        # Check for consistency\n        for l1,l2 in zip(self.layers[:-1],self.layers[1:]):\n            # print(l1.get_output_shape(),l2.get_input_shape())\n            if not l1.get_output_shape()==l2.get_input_shape():\n                print(""Input/output dimensions are not consistent."")\n                exit()\n\n        # Define placeholders\n        self.input=tf.placeholder(tf.float32,self.layers[0].get_input_shape())\n        self.correct_output=tf.placeholder(tf.float32,self.layers[-1].get_output_shape())\n\n        # Get mini-batch size\n        mb_size = self.layers[0].get_input_shape()[0]\n\n        # Define computational graph\n        _intermediate_output = self.layers[0].calc_output(self.input)\n        for i in range(1,len(self.layers)):\n            _intermediate_output = self.layers[i].calc_output(_intermediate_output)\n\n        # Find loss\n        # self._loss = tf.losses.log_loss(labels=self.correct_output,predictions=_intermediate_output)\n        self._loss = -tf.reduce_mean(tf.multiply(self.correct_output,tf.log(_intermediate_output+1e-9)))\n\n        # Find accuracy\n        self._accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(_intermediate_output,axis=1),tf.argmax(self.correct_output,axis=1)),tf.float32))\n\n    def train(self,training_data,learning_rate=1.0, mini_batch_size=50, n_epochs=30,test_data=None,validation_data=None):\n        with tf.Session() as sess:\n            train_step = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(self._loss)\n            sess.run(tf.global_variables_initializer())\n\n            # Add summary writer for graph details\n            writer = tf.summary.FileWriter(""./logs"", graph=sess.graph)\n\n            for epoch_no in range(n_epochs):\n                """"""\n                I - array of image matrices\n                O - array of correct outputs (converted to vector form if required)\n                """"""\n                I,O=training_data\n                n_batches = len(I)//mini_batch_size\n                total_loss=0.0; total_accuracy=0.0\n                for batch_no in range(n_batches):\n                    x = np.stack(I[batch_no*mini_batch_size:(batch_no+1)*mini_batch_size],axis=0)\n                    # Add the `#_feature_maps` dimension\n                    x = np.expand_dims(x,axis=-1)\n                    y = np.stack([one_hot(10,z) for z in O[batch_no*mini_batch_size:(batch_no+1)*mini_batch_size]])\n                    _,curr_loss,accuracy = sess.run([train_step,self._loss,self._accuracy],feed_dict={self.input:x,self.correct_output:y})\n                    print(""\\33[2K Epoch {0} mini-batch {1} : Loss = {2:.5}, Accuracy = {3:.5}\\r"".format(epoch_no+1,batch_no+1,curr_loss,accuracy),end=\'\')\n                    total_loss+=curr_loss\n                    total_accuracy+=accuracy\n                print(""\\33[2K Epoch {0} : Loss = {1:.5}, Accuracy = {2:.5}\\r"".format(epoch_no+1,total_loss/n_batches,total_accuracy/n_batches))\n\n                # Check accuracy and loss on validation data\n                if validation_data:\n                    va_l,va_a=self.predict(sess,validation_data,mini_batch_size)\n                    print(""Validation loss = {0:.5}, validation accuracy = {1:.5}"".format(va_l,va_a))\n\n            # Check accuracy and loss on test data\n            if test_data:\n                te_l,te_a=self.predict(sess,test_data,mini_batch_size)\n                print(""Test data loss = {0:.5}, test data accuracy = {1:.5}"".format(te_l,te_a))\n\n    def predict(self,sess,input_data,mini_batch_size):\n        """"""\n        Function to predict the output for a given set of data.\n        It can be used to evaluate validation and test accuracies and losses.\n        """"""\n        I,O=input_data\n        n_batches = len(I)//mini_batch_size\n        total_loss=0.0; total_accuracy=0.0\n        for batch_no in range(n_batches):\n            x = np.stack(I[batch_no*mini_batch_size:(batch_no+1)*mini_batch_size],axis=0)\n            # Add the `#_feature_maps` dimension\n            x = np.expand_dims(x,axis=-1)\n            y = np.stack([one_hot(10,z) for z in O[batch_no*mini_batch_size:(batch_no+1)*mini_batch_size]])\n            curr_loss,accuracy = sess.run([self._loss,self._accuracy],feed_dict={self.input:x,self.correct_output:y})\n            total_loss+=curr_loss\n            total_accuracy+=accuracy\n        return total_loss/n_batches,total_accuracy/n_batches\n'"
Convolutional_Neural_Network/tf_CNN_layers.py,11,"b'import tensorflow as tf\nimport numpy as np\n\nclass ConvPoolLayer:\n    """""" Definition of a combined convolutional and max-pooling layer """"""\n    def __init__(self,input_shape,filter_shape,filter_stride,pool_shape,pool_stride=None,linear_output=False):\n        """"""\n        Parameters:\n        input_shape - (mini_batch_size, rows, cols, #_feature_maps)\n        filter_shape - (rows, cols, #_resulting_feature_maps)\n        pool_shape - (rows, cols)\n        """"""\n        # Store arguments\n        self.input_shape=input_shape\n        self.filter_shape=filter_shape\n        self.filter_stride=filter_stride\n        self.pool_shape=pool_shape\n\n        if pool_stride is None:\n            self.pool_stride=min(self.pool_shape[0],self.pool_shape[1])\n        else:\n            self.pool_stride = pool_stride\n\n        self.linear_output = linear_output\n\n        # Define variables/parameters for the feature map\n        r,c,k2=self.filter_shape\n        mb,ri,ci,k1=self.input_shape\n        self.shared_weights=tf.Variable(np.random.standard_normal((r,c,k1,k2)).astype(np.float32))\n        # self.shared_biases = tf.Variable(np.random.standard_normal((mb,ri-r+1,ci-c+1,k2),dtype=np.float32))\n        self.shared_biases = tf.constant(np.random.standard_normal((mb,ri-r+1,ci-c+1,k2)).astype(np.float32))\n\n    def get_input_shape(self):\n        """""" Function to return the input Tensor dimensions """"""\n        return self.input_shape\n\n    def get_output_shape(self):\n        """""" Function to calculate and return the output Tensor dimensions """"""\n        # Calculation borrowed from TensorFlow\n        mb,r,c,k1=self.input_shape\n        rf,cf,k2=self.filter_shape\n        h1=int(np.ceil((r-rf+1)/self.filter_stride))\n        w1=int(np.ceil((c-cf+1)/self.filter_stride))\n        rp,cp=self.pool_shape\n        h2=int(np.ceil((h1-rp+1)/self.pool_stride))\n        w2=int(np.ceil((w1-cp+1)/self.pool_stride))\n        if not self.linear_output:\n            return (mb,h2,w2,k2)\n        else:\n            return (mb,h2*w2*k2)\n\n    def calc_output(self,inpt):\n        """"""\n        Input is sent one mini-batch at a time.\n        Dimensions of input are (mini_batch_size, rows, cols, #_feature_maps).\n        An output Tensor is returned, with dimensions as specified by `self.get_output_shape()`\n        """"""\n        # Perform convolution\n        _conv_output = tf.nn.conv2d(inpt,self.shared_weights,[1,self.filter_stride,self.filter_stride,1],padding=\'VALID\')\n        # Add biases\n        _biased_conv_output = tf.add(_conv_output,self.shared_biases)\n        # Apply activation\n        _a = tf.nn.sigmoid(_biased_conv_output)\n        # Perform pooling (max-pooling is default)\n        _pool_output = tf.nn.max_pool(_a,[1,self.pool_shape[0],self.pool_shape[1],1],[1,self.pool_stride,self.pool_stride,1],padding=\'VALID\')\n        # Return output of correct shape\n        if self.linear_output:\n            return tf.reshape(_pool_output,self.get_output_shape())\n        else:\n            return _pool_output\n\nclass ConnectedLayer:\n    """""" Definition of a fully connected layer """"""\n    def __init__(self,n_in,n_out,mini_batch_size,activation=tf.nn.sigmoid):\n        # Store arguments\n        self.n_in = n_in\n        self.n_out = n_out\n        self.mini_batch_size = mini_batch_size\n        self.activation = activation\n\n        # Create variables\n        self.weights = tf.Variable(np.random.standard_normal([self.n_in,self.n_out]).astype(np.float32))\n        self.biases = tf.Variable(np.random.standard_normal([1,self.n_out]).astype(np.float32))\n\n    def get_input_shape(self):\n        """""" Function to return the input Tensor dimensions """"""\n        return (self.mini_batch_size,self.n_in)\n\n    def get_output_shape(self):\n        """""" Function to return the output Tensor dimensions """"""\n        return (self.mini_batch_size,self.n_out)\n\n    def calc_output(self,inpt):\n        """"""\n        Input is sent one mini-batch at a time.\n        Dimensions of input are (mini_batch_size, #_input_neurons).\n        An output Tensor is returned, with dimensions as (mini_batch_size, #_output_neurons)\n        """"""\n        # Compute weighted input and then output\n        _z = tf.matmul(inpt,self.weights) + self.biases\n        _output = self.activation(_z)\n        return _output\n\nclass SoftmaxOutputLayer:\n    """""" Definition of a fully connected output layer with softmax """"""\n    def __init__(self,n_in,n_out,mini_batch_size):\n        # Store arguments\n        self.n_in = n_in\n        self.n_out = n_out\n        self.mini_batch_size = mini_batch_size\n\n        # Create variables\n        self.weights = tf.Variable(np.random.standard_normal([self.n_in,self.n_out]).astype(np.float32))\n        self.biases = tf.Variable(np.random.standard_normal([1,self.n_out]).astype(np.float32))\n\n    def get_input_shape(self):\n        """""" Function to return the input Tensor dimensions """"""\n        return (self.mini_batch_size,self.n_in)\n\n    def get_output_shape(self):\n        """""" Function to return the output Tensor dimensions """"""\n        return (self.mini_batch_size,self.n_out)\n\n    def calc_output(self,inpt):\n        """"""\n        Input is sent one mini-batch at a time.\n        Dimensions of input are (mini_batch_size, #_input_neurons).\n        An output Tensor is returned, with dimensions as (mini_batch_size, #_output_neurons)\n        """"""\n        # Compute weighted input and then output\n        _z = tf.matmul(inpt,self.weights) + self.biases\n        _output = tf.nn.softmax(_z)\n        return _output\n'"
Recurrent_Neural_Networks/RNN.py,40,"b'""""""\nImplementation of a vanilla RNN.\nUses tanh activation function, SGD for optimization, cross-entropy cost function\nwith softmax output layer and custom dimensions for hidden state and input.\n\nRelations between parameters :\nh[t] = tanh( Ux[t] + Wh[t-1] + B1 )\no[t] = softmax( Vh[t] + B2 )\n\nHere, the biases B1 and B2 are optional, and are generally ignored.\n""""""\n\nimport numpy as np\n\ndef tanh(z):\n    # Clipping values before using\n    # (numerical values are system dependent)\n    new_z=np.clip(z,-700,700)\n    f1=np.exp(new_z); f2=np.exp(-1*new_z)\n    return np.divide(f1-f2,f1+f2)\n\n# def tanh_prime(z):\n#     temp = tanh(z)\n#     return 1.0-temp**2\n\ndef softmax(z):\n    # Clipping values before using\n    # (numerical values are system dependent)\n    new_z=np.clip(z,-700,700)\n    return np.exp(new_z)/np.sum(np.exp(new_z))\n\nclass RNN:\n    def __init__(self,state_size,input_size,ignore_bias=True):\n        # List to store hidden state, initialized with a zero matrix\n        self.h=[np.zeros([state_size,1])]\n        # List to store output at each time step\n        self.o=[]\n        # List to store input at each time step\n        self.x=[]\n        # Parameters to be trained; standard initialization\n        self.U=np.random.uniform(-1.0/np.sqrt(input_size),1.0/np.sqrt(input_size),[state_size,input_size])\n        self.V=np.random.uniform(-1.0/np.sqrt(state_size),1.0/np.sqrt(state_size),[input_size,state_size])\n        self.W=np.random.uniform(-1.0/np.sqrt(state_size),1.0/np.sqrt(state_size),[state_size,state_size])\n        # Optional biases\n        self.ignore_bias=ignore_bias\n        if not ignore_bias:\n            self.B1=np.zeros([state_size,1])\n            self.B2=np.zeros([input_size,1])\n\n        # Loss\n        self._loss=0\n\n    def _feed(self,X,Y):\n        """"""\n        Function to evaluate results for each time step.\n        X - input sequence\n        Y - correct output sequence\n        """"""\n        for x,y in zip(X,Y):\n            self.x.append(x)\n            if self.ignore_bias:\n                self.h.append(tanh(np.dot(self.U,x)+np.dot(self.W,self.h[-1])))\n                self.o.append(softmax(np.dot(self.V,self.h[-1])))\n                # Clipping to avoid overflow\n                self._loss+=-np.sum(y*np.log(np.clip(self.o[-1],10**-10,10**10)))\n            else:\n                self.h.append(tanh(np.dot(self.U,x)+np.dot(self.W,self.h[-1])+self.B1))\n                self.o.append(softmax(np.dot(self.V,self.h[-1])+self.B2))\n                # Clipping to avoid overflow\n                self._loss+=-np.sum(y*np.log(np.clip(self.o[-1],10**-10,10**10)))\n\n    def _reset(self):\n        """"""\n        Function that clears the input and output queues and reinitializes\n        the hidden state keeping the other parameters unchanged\n        """"""\n        self.x=[]\n        self.o=[]\n        self._loss=0\n        # self.h=[self.h[-1]]\n        self.h=[self.h[0]]\n\n    def _bptt(self,Y,step=-1):\n        """"""\n        Function which applies truncated backpropagation through time using the\n        existing lists of input and output data, and hidden state\n        Y - corresponding correct output\n        """"""\n        # Total length of sequence / total time\n        T=len(self.x)\n        dEdU=np.zeros(np.shape(self.U))\n        dEdV=np.zeros(np.shape(self.V))\n        dEdW=np.zeros(np.shape(self.W))\n        if self.ignore_bias:\n            for t in range(T-1,-1,-1):\n                delta_o = np.subtract(self.o[t],Y[t])\n                dEdV += np.dot(delta_o,np.transpose(self.h[t]))\n                delta_t = np.multiply(np.dot(np.transpose(self.V),delta_o),1.0-self.h[t]**2)\n                if step==-1:\n                    for bps in range(t-1,0,-1):\n                        dEdW += np.outer(delta_t,self.h[bps-1])\n                        dEdU += np.outer(delta_t,self.x[bps-1])\n                        delta_t=np.dot(np.transpose(self.W),delta_t)*(1.0-self.h[bps-1]**2)\n                else:\n                    for bps in range(t-1,max(0,t-step),-1):\n                        dEdW += np.outer(delta_t,self.h[bps-1])\n                        dEdU += np.outer(delta_t,self.x[bps-1])\n                        delta_t=np.dot(np.transpose(self.W),delta_t)*(1.0-self.h[bps-1]**2)\n\n            return [dEdU, dEdV, dEdW]\n        else:\n            # Initialized but used only if required\n            dEdB1=np.zeros(np.shape(self.B1))\n            dEdB2=np.zeros(np.shape(self.B2))\n            for t in range(T-1,-1,-1):\n                delta_o = np.subtract(self.o[t],Y[t])\n                dEdV += np.dot(delta_o,np.transpose(self.h[t]))\n                delta_t = np.multiply(np.dot(np.transpose(self.V),delta_o),1.0-self.h[t]**2)\n                if step==-1:\n                    for bps in range(t-1,-1,-1):\n                        dEdW += np.outer(delta_t,self.h[bps-1])\n                        dEdU += np.outer(delta_t,self.x[bps-1])\n                        delta_t=np.dot(np.transpose(self.W),delta_t)*(1.0-self.h[bps-1]**2)\n                else:\n                    for bps in range(t-1,max(-1,t-step-1),-1):\n                        dEdW += np.outer(delta_t,self.h[bps-1])\n                        dEdU += np.outer(delta_t,self.x[bps-1])\n                        delta_t=np.dot(np.transpose(self.W),delta_t)*(1.0-self.h[bps-1]**2)\n\n            return [dEdU, dEdV, dEdW, dEdB1, dEdB2]\n\n    def train(self,training_data,learning_rate=0.5,n_epochs=50,bptt_step=-1,transform=lambda x: x):\n        """"""\n        Function to train the RNN. All hyper-parameters are trivial, except perhaps `transform`.\n        It is a container for any function to be applied to `training_data` before it is used.\n        """"""\n        for epoch_no in range(n_epochs):\n            cntr=0\n            total_loss=0.0\n            # Here X and Y are sequences of words\n            for org_X,org_Y in training_data:\n                X=transform(org_X); Y=transform(org_Y)\n                # print(X,Y)\n                cntr+=1\n                self._feed(X,Y)\n                print(""Loss in epoch {0} : batch {1} = {2}"".format(epoch_no+1,cntr,self._loss))\n                total_loss+=self._loss\n                dEdU, dEdV, dEdW = self._bptt(Y,bptt_step)\n                self.W+=-learning_rate*dEdW\n                self.U+=-learning_rate*dEdU\n                self.V+=-learning_rate*dEdV\n                self._reset()\n            print(""Average loss in epoch {0} = {1}"".format(epoch_no+1,total_loss/cntr))\n'"
Recurrent_Neural_Networks/basic_language_modeling.py,0,"b'import ptb_loader as pl\nimport numpy as np\nfrom RNN import RNN\n\ndef run():\n    l,V=pl.load_words()\n    # `l` is list of sentences split into words\n    # `V` is a dict() mapping word with index in vocabulary\n\n    # Convert words to respective indices\n    for i in range(len(l)):\n        for j in range(len(l[i])):\n            l[i][j]=V[l[i][j]]\n\n    # Generate training data\n    training_data=[]\n    for sent in l:\n        training_data.append( (sent[:-1],sent[1:]) )\n\n    """""" Initializing RNN with hidden state of dimension 20x1 """"""\n    rnet=RNN(20,len(V))\n    rnet.train(\n                   training_data[:25],\n                   learning_rate=3.0,\n                   bptt_step=10,\n                   transform=lambda sent: [pl.one_hot(len(V),x) for x in sent]\n               )\n\nif __name__ == \'__main__\':\n    run()\n'"
Recurrent_Neural_Networks/ptb_loader.py,2,"b'""""""\nHelper program to load training, test and validation data from PTB corpus.\nCharacter and word variants available.\n\nEntire data is available in the `data/` directory.\n""""""\nimport numpy as np\n\ndef load_words():\n    path=\'../data/PTB/ptb.train.txt\'\n    words=[]\n    with open(path,\'r\') as f:\n        words.extend(f.read().replace(\'\\n\',\'<eos>\').split())\n\n    cntr=0; V={}\n    for w in words:\n        if V.get(w):\n            pass\n        else:\n            V[w]=cntr\n            cntr+=1\n\n    return (words,V)\n\ndef load_words_raw():\n    l,V = load_words()\n    indices = [V[item] for item in l]\n    index_to_word = {i:w for w,i in V.items()}\n    return indices,index_to_word\n\ndef get_data_and_dict(data_size,batch_size,bptt_steps):\n    l,V = load_words_raw()\n    l=l[:data_size]\n    width = batch_size\n    length = len(l[:-1]) // width\n    length = (length // bptt_steps)*bptt_steps\n    I = np.transpose(np.reshape(l[:length*width],[width,length]))\n    O = np.transpose(np.reshape(l[1:length*width+1],[width,length]))\n    return I,O,V\n\ndef load_chars():\n    path=\'data/PTB/ptb.char.train.txt\'\n    chars=[]\n    with open(path,\'r\') as f:\n        for line in f:\n            chars.append(line.split())\n\n    cntr=0; V={}\n    for grp in chars:\n        for c in grp:\n            if V.get(c):\n                pass\n            else:\n                V[c]=cntr\n                cntr+=1\n\n    return (chars,V)\n'"
Recurrent_Neural_Networks/tf_LSTM.py,12,"b'""""""\nImplementation of Long Short-Term Memory network in TensorFlow.\n\nThis implementation uses one-hot representation for each word in the vocabulary\nfor language modeling purpose.\n""""""\nimport tensorflow as tf\nimport numpy as np\n\ndef one_hot(size,pos):\n    ans=np.zeros([size,1])\n    ans[pos]=1\n    return ans\n\ndef softmax(z):\n    # Naive implementation - doesn\'t handle overflows\n    return np.exp(z)/np.sum(np.exp(z))\n\ndef decay(min_learning_rate,max_learning_rate,frac):\n    return max_learning_rate - (max_learning_rate-min_learning_rate)*frac\n\nclass tf_LSTM:\n    def __init__(\n                    self,\n                    input_size,\n                    batch_size,\n                    bptt_steps,\n                ):\n        # Store arguments\n        self._batch_size=int(batch_size)\n        self._input_size=int(input_size)\n        self._bptt_steps=int(bptt_steps)\n\n        # Define variables\n\n        # Forget Gate\n        self.Wf=tf.Variable(tf.random_uniform([input_size,input_size],-1.0/np.sqrt(input_size),1.0/np.sqrt(input_size)))\n        self.Rf=tf.Variable(tf.random_uniform([input_size,input_size],-1.0/np.sqrt(input_size),1.0/np.sqrt(input_size)))\n        self.Bf=tf.Variable(tf.zeros([input_size,1]))\n        # Input Gate\n        self.Wi=tf.Variable(tf.random_uniform([input_size,input_size],-1.0/np.sqrt(input_size),1.0/np.sqrt(input_size)))\n        self.Ri=tf.Variable(tf.random_uniform([input_size,input_size],-1.0/np.sqrt(input_size),1.0/np.sqrt(input_size)))\n        self.Bi=tf.Variable(tf.zeros([input_size,1]))\n        # Output Gate\n        self.Wo=tf.Variable(tf.random_uniform([input_size,input_size],-1.0/np.sqrt(input_size),1.0/np.sqrt(input_size)))\n        self.Ro=tf.Variable(tf.random_uniform([input_size,input_size],-1.0/np.sqrt(input_size),1.0/np.sqrt(input_size)))\n        self.Bo=tf.Variable(tf.zeros([input_size,1]))\n        # State change\n        self.W=tf.Variable(tf.random_uniform([input_size,input_size],-1.0/np.sqrt(input_size),1.0/np.sqrt(input_size)))\n        self.R=tf.Variable(tf.random_uniform([input_size,input_size],-1.0/np.sqrt(input_size),1.0/np.sqrt(input_size)))\n        self.B=tf.Variable(tf.zeros([input_size,1]))\n\n        # Placeholders\n        self.input=tf.placeholder(tf.int32,[self._bptt_steps,batch_size],name=""Input"")\n        self.correct_output=tf.placeholder(tf.int32,[self._bptt_steps,batch_size],name=""Output"")\n        self._init_cell_state=tf.placeholder(tf.float32,[input_size,batch_size],name=""Initial_Cell_State"")\n        self._init_hidden_state=tf.placeholder(tf.float32,[input_size,batch_size],name=""Initial_Hidden_State"")\n\n        # Computations\n        inp = tf.transpose(tf.one_hot(self.input[0],depth=self._input_size))\n        f = tf.nn.sigmoid(tf.matmul(self.Wf,inp)+tf.matmul(self.Rf,self._init_hidden_state)+self.Bf)\n        i = tf.nn.sigmoid(tf.matmul(self.Wi,inp)+tf.matmul(self.Ri,self._init_hidden_state)+self.Bi)\n        o = tf.nn.sigmoid(tf.matmul(self.Wo,inp)+tf.matmul(self.Ro,self._init_hidden_state)+self.Bo)\n        state_change = tf.nn.tanh(tf.matmul(self.W,inp)+tf.matmul(self.R,self._init_hidden_state)+self.B)\n        self._cell_state = f*self._init_cell_state+i*state_change\n        self._hidden_state = tf.nn.tanh(self._cell_state)*o\n        self._loss = -tf.transpose(tf.one_hot(self.correct_output[0],depth=self._input_size))*tf.log(0.5*self._hidden_state+0.5)\n\n        for cntr in range(1,self._bptt_steps):\n            inp = tf.transpose(tf.one_hot(self.input[cntr],depth=self._input_size))\n            f = tf.nn.sigmoid(tf.matmul(self.Wf,inp)+tf.matmul(self.Rf,self._hidden_state)+self.Bf)\n            i = tf.nn.sigmoid(tf.matmul(self.Wi,inp)+tf.matmul(self.Ri,self._hidden_state)+self.Bi)\n            o = tf.nn.sigmoid(tf.matmul(self.Wo,inp)+tf.matmul(self.Ro,self._hidden_state)+self.Bo)\n            state_change = tf.nn.tanh(tf.matmul(self.W,inp)+tf.matmul(self.R,self._hidden_state)+self.B)\n            self._cell_state = f*self._cell_state+i*state_change\n            self._hidden_state = tf.nn.tanh(self._cell_state)*o\n            self._loss += -tf.transpose(tf.one_hot(self.correct_output[cntr],depth=self._input_size))*tf.log(0.5*self._hidden_state+0.5)\n\n        self._loss=tf.reduce_mean(self._loss)\n        self._init=tf.global_variables_initializer()\n\n    def train(self,input_data,output_data,learning_rate=1.0,n_epochs=30,factor=10):\n        """"""\n        Training data is contained in `input_data`, `output_data`.\n        Both of these arrays have `batch_size` number of columns and arbitrary number of rows.\n        For language modeling :\n        Each element of these arrays is the index of a particular word from the vocabulary.\n\n        `bptt_steps` is the number of steps upto which truncated BPTT will be applied.\n        """"""\n        I=input_data; O=output_data\n        with tf.Session() as sess:\n            sess.run(self._init)\n\n            cell_state = np.zeros([self._input_size,self._batch_size])\n            hidden_state = np.zeros([self._input_size,self._batch_size])\n\n            for epoch_no in range(n_epochs):\n                total_loss = 0.0\n                cur_learning_rate = decay(learning_rate/factor,learning_rate,epoch_no/n_epochs)\n                train = tf.train.GradientDescentOptimizer(learning_rate=cur_learning_rate).minimize(self._loss)\n                print(""Current learning rate = {0}"".format(cur_learning_rate))\n                for cntr in range(len(I)//self._bptt_steps):\n                    _, cell_state, hidden_state, curr_loss = sess.run([train,self._cell_state,self._hidden_state,self._loss],\n                            feed_dict={\n                                        self.input:I[cntr*self._bptt_steps:min(len(I),(cntr+1)*self._bptt_steps),:],\n                                        self.correct_output:O[cntr*self._bptt_steps:min(len(I),(cntr+1)*self._bptt_steps),:],\n                                        self._init_cell_state:cell_state,\n                                        self._init_hidden_state:hidden_state\n                                    })\n                    total_loss += curr_loss\n                    print(""Loss after epoch {0}, batch {1} = {2}"".format(epoch_no+1,(cntr+1)*self._bptt_steps,curr_loss/self._bptt_steps))\n                print(""Average loss in epoch {0} = {1}"".format(epoch_no+1,total_loss/len(I)))\n'"
Recurrent_Neural_Networks/tf_RNN.py,13,"b'""""""\nImplementation of a vanilla Recurrent Neural Network using TensorFlow.\n""""""\n\nimport tensorflow as tf\nimport numpy as np\n\ndef one_hot(size,pos):\n    ans=np.zeros([size,1])\n    ans[pos]=1\n    return ans\n\ndef softmax(z):\n    # Naive implementation - doesn\'t handle overflows\n    return np.exp(z)/np.sum(np.exp(z))\n\ndef decay(min_learning_rate,max_learning_rate,frac):\n    return max_learning_rate - (max_learning_rate-min_learning_rate)*frac\n\nclass tf_RNN:\n    def __init__(\n                    self,\n                    input_size,\n                    batch_size,\n                    state_size,\n                    bptt_steps,\n                    activation=tf.tanh\n                ):\n        # Store arguments\n        self._activation=activation\n        self._batch_size=batch_size\n        self._input_size=input_size\n        self._bptt_steps=bptt_steps\n        self._state_size=state_size\n\n        # Construct the computational graph\n        self.U=tf.Variable(tf.random_uniform([state_size,input_size],-1.0/np.sqrt(input_size),1.0/np.sqrt(input_size)))\n        self.V=tf.Variable(tf.random_uniform([input_size,state_size],-1.0/np.sqrt(input_size),1.0/np.sqrt(input_size)))\n        self.W=tf.Variable(tf.random_uniform([state_size,state_size],-1.0/np.sqrt(input_size),1.0/np.sqrt(input_size)))\n\n        # Biases\n        self.B1=tf.Variable(tf.zeros([state_size,1]))\n        self.B2=tf.Variable(tf.zeros([input_size,1]))\n\n        # Placeholders\n        self.input=tf.placeholder(tf.int32,[self._bptt_steps,batch_size],name=""Input"")\n        self.correct_output=tf.placeholder(tf.int32,[self._bptt_steps,batch_size],name=""Output"")\n        self._initial_state=tf.placeholder(tf.float32,[state_size,batch_size],name=""Initial_State"")\n\n        # Computations\n        inp = tf.transpose(tf.one_hot(self.input[0],depth=self._input_size))\n        self._state=activation(tf.matmul(self.U,inp)+tf.matmul(self.W,self._initial_state)+self.B1)\n        self._output=tf.matmul(self.V,self._state)+self.B2\n        self._loss=tf.nn.softmax_cross_entropy_with_logits(logits=self._output,labels=tf.transpose(tf.one_hot(self.correct_output[0],depth=self._input_size)),dim=0)\n        # self._output = tf.nn.softmax(tf.matmul(self.V,self._state)+self.B2)\n        # self._loss = tf.losses.log_loss(labels=tf.transpose(tf.one_hot(self.correct_output[0],depth=self._input_size)),predictions=self._output)\n\n        for i in range(1,self._bptt_steps):\n            inp = tf.transpose(tf.one_hot(self.input[i],depth=self._input_size))\n            self._state=activation(tf.matmul(self.U,inp)+tf.matmul(self.W,self._state)+self.B1)\n            self._output=tf.matmul(self.V,self._state)+self.B2\n            self._loss+=tf.nn.softmax_cross_entropy_with_logits(logits=self._output,labels=tf.transpose(tf.one_hot(self.correct_output[i],depth=self._input_size)),dim=0)\n            # self._output = tf.nn.softmax(tf.matmul(self.V,self._state)+self.B2)\n            # self._loss += tf.losses.log_loss(labels=tf.transpose(tf.one_hot(self.correct_output[i],depth=self._input_size)),predictions=self._output)\n\n        self._loss=tf.reduce_mean(self._loss)\n        self._init=tf.global_variables_initializer()\n\n    def train(self,input_data,output_data,learning_rate=1.0,n_epochs=30,factor=10):\n        """"""\n        Training data is contained in `input_data`, `output_data`.\n        Both of these arrays have `batch_size` number of columns and arbitrary number of rows.\n        For language modeling :\n        Each element of these arrays is the index of a particular word from the vocabulary.\n\n        `bptt_steps` is the number of steps upto which truncated BPTT will be applied.\n        """"""\n        I=input_data; O=output_data\n        with tf.Session() as sess:\n            sess.run(self._init)\n\n            state = np.zeros([self._state_size,self._batch_size])\n            for epoch_no in range(n_epochs):\n                total_loss = 0.0\n                cur_learning_rate = decay(learning_rate/factor,learning_rate,epoch_no/n_epochs)\n                train = tf.train.GradientDescentOptimizer(learning_rate=cur_learning_rate).minimize(self._loss)\n                print(""Current learning rate = {0}"".format(cur_learning_rate))\n                for cntr in range(len(I)//self._bptt_steps):\n                    _,state,curr_loss = sess.run([train,self._state,self._loss],feed_dict={self.input:I[cntr*self._bptt_steps:min(len(I),(cntr+1)*self._bptt_steps),:],self.correct_output:O[cntr*self._bptt_steps:min(len(I),(cntr+1)*self._bptt_steps),:],self._initial_state:state})\n                    total_loss += curr_loss\n                    print(""Loss after epoch {0}, batch {1} = {2}"".format(epoch_no+1,(cntr+1)*self._bptt_steps,curr_loss/self._bptt_steps))\n                print(""Average loss in epoch {0} = {1}"".format(epoch_no+1,total_loss/len(I)))\n\n    # def calc_output(self,inp,state):\n    #     """"""\n    #     Predicts the output for a given value of state and input.\n    #     Returns a tuple (output, new_state)\n    #     """"""\n    #     assert(len(inp)==np.shape(self.U)[1])\n    #     new_state = self._activation(np.dot(self.U,inp)+np.dot(self.W,state)+self.B1)\n    #     return softmax(np.dot(self.V,new_state)+self.B2),new_state\n    #\n    # def predict(self,index_to_word):\n    #     """""" Function which returns a random string generated by the RNN for a randomly chosen initial word. """"""\n    #     size_V = len(index_to_word)\n    #     init_word_index = np.random.randint(size_V)\n    #\n    #     while index_to_word[init_word_index] == \'<eos>\':\n    #         init_word_index = np.random.randint(size_V)\n    #\n    #     res=[]; init_state = np.zeros([self._state_size,1])\n    #     next_word_index = init_word_index\n    #     while index_to_word[next_word_index] is not \'<eos>\':\n    #         res.append(next_word_index)\n    #         output, new_state = self.calc_output(one_hot(size_V,next_word_index),init_state)\n    #         next_word_index = np.argmax(output)\n    #         init_state = new_state\n    #\n    #     # Convert `res` array to string\n    #     res = [index_to_word[i] for i in res]\n    #     return "" "".join(res)\n'"
Recurrent_Neural_Networks/tf_basic_language_modeling.py,0,"b'import sys\nimport tf_RNN\nimport tf_LSTM\nimport ptb_loader as pl\nimport tensorflow as tf\n\nprint(""Procuring training data ..."")\nI,O,index_to_word = pl.get_data_and_dict(data_size=-1,batch_size=50,bptt_steps=10)\nprint(""Data obtained."")\n\n# print(sys.argv[1])\nif sys.argv[1]==\'1\':\n    print(""Constructing RNN ..."")\n    rnet=tf_RNN.tf_RNN(len(index_to_word),50,50,10,activation=tf.nn.sigmoid)\n    # Train the RNN\n    print(""Training RNN ..."")\n    rnet.train(I,O,learning_rate=1.0,n_epochs=30)\n    # Predict sentences\n    # print(""Predicting sentence ..."")\n    # rnet.predict(index_to_word)\nelif sys.argv[1]==\'2\':\n    print(""Constructing LSTM ..."")\n    lstm=tf_LSTM.tf_LSTM(len(index_to_word),50,10)\n    print(""Training LSTM ..."")\n    lstm.train(I,O,learning_rate=1.0,n_epochs=30)\n'"
