file_path,api_count,code
setup.py,0,"b""from setuptools import setup, find_packages\nfrom codecs import open\nfrom os import path\n\n__version__ = '0.1.0'\n\nhere = path.abspath(path.dirname(__file__))\n\n# get the dependencies and installs\nwith open(path.join(here, 'requirements.txt'), encoding='utf-8') as f:\n    all_reqs = f.read().split('\\n')\n\ninstall_requires = [x.strip() for x in all_reqs if 'git+' not in x]\ndependency_links = [x.strip().replace('git+', '') for x in all_reqs if x.startswith('git+')]\n\nsetup(\n    name='np_ml',\n    version=__version__,\n    description='A tool library of classical machine learning algorithms with only numpy.',\n    url='https://github.com/zhuzilin/NP_ML',\n    download_url='https://github.com/zhuzilin/NP_ML',\n    license='MIT',\n    packages=find_packages(),\n    include_package_data=True,\n    author='zhuzilin',\n    install_requires=install_requires,\n    dependency_links=dependency_links,\n    author_email='zhuzilinallen@gmail.com'\n)"""
examples/affinity_propagation.py,0,"b'from np_ml import AffinityPropagation\nfrom sklearn.datasets.samples_generator import make_blobs\nimport matplotlib.pyplot as plt\n\nif __name__ == \'__main__\':\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    x, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5,\n                                random_state=0)\n\n    ap = AffinityPropagation()\n    label = ap.fit(x, preference=-50)\n    print(""cluster centers: "")\n    print(ap.centers)\n    plt.scatter(x[:, 0], x[:, 1], c=label)\n    plt.show()'"
examples/decision_tree.py,4,"b'import numpy as np\nimport csv\n\nfrom np_ml import DecisionTree, utils\n\nif __name__ == \'__main__\':\n    with open(r\'..\\data\\tic_tac_toe.csv\', newline=\'\') as csvfile:\n        data = np.array(list(csv.reader(csvfile)))\n    np.random.shuffle(data)\n    train = data[:int(data.shape[0]*0.8), :]\n    test = data[int(data.shape[0]*0.8):, :]\n    train_x = train[:, :-1]\n    train_y = train[:, -1]\n    train_y = (train_y == ""positive"")\n    test_x = test[:, :-1]\n    test_y = test[:, -1]\n    test_y = (test_y == ""positive"")\n                  \n    print(""use ID3: "")\n    dt = DecisionTree(max_depth=1)\n    dt.fit(train_x, train_y, type=""ID3"", detailed=False)\n    y_pred = dt.predict(test_x)\n    accuracy = np.sum(y_pred == test_y) / len(test_y)\n    print(""tree struct: "")\n    print(dt.root)\n    print(""accuracy: ""+str(accuracy))\n    \n    print(""use CART: "")\n    dt = DecisionTree(max_depth=1)\n    dt.fit(train_x, train_y, type=""CART"", detailed=False)\n    y_pred = dt.predict(test_x)\n    accuracy = np.sum(y_pred == test_y) / len(test_y)\n    print(""tree struct: "")\n    print(dt.root)\n    print(""accuracy: ""+str(accuracy))'"
examples/kmeans.py,0,"b'from sklearn.datasets import make_blobs\nfrom np_ml import KMeans\nfrom np_ml.utils import plot\n\nif __name__ == \'__main__\':\n    n_samples = 1500\n    random_state = 170\n    x, y = make_blobs(n_samples=n_samples, random_state=random_state)\n    print(type(x))\n    print(x.shape)\n    # Incorrect number of clusters\n    y_pred = KMeans(k=3).fit(x, detailed=True)\n    plot(x, y_pred, title=""KMeans"")'"
examples/knn.py,4,"b'import csv\nimport numpy as np\nfrom np_ml import KNN, utils\n\nif __name__ == \'__main__\':\n    with open(r\'..\\data\\iris.csv\', newline=\'\') as csvfile:\n        data = np.array(list(csv.reader(csvfile)))\n\n    # use binary classifier\n    np.random.shuffle(data)\n    x = np.array(data[:, :-1], dtype=np.float32)\n    x = utils.transform(x, 2)\n    y = data[:, -1]\n    y[y == \'Iris-versicolor\'] = 0\n    y[y == \'Iris-virginica\'] =  1\n    y[y == \'Iris-setosa\'] =  2\n    y = np.array(y, dtype=np.int32)\n    knn = KNN()\n    knn.fit(x, y)\n    utils.plot_boundary(knn, x, y, title=""K Nearest Neighbor"")'"
examples/lda.py,0,"b'from np_ml import LDA, Documents\n\nif __name__ == \'__main__\':\n    print(""data1"")\n    data = [[""apple"", ""orange"", ""banana""], \n            [""apple"", ""orange""],\n            [""orange"", ""banana""],\n            [""cat"", ""dog""], \n            [""dog"", ""tiger""], \n            [""tiger"", ""cat""]]\n    \n    \n    for i, document in enumerate(data):\n        print(""Document {}: {}"".format(i, document))\n    docs = Documents(data=data)\n    lda = LDA()\n    lda.fit(docs)\n    print(docs.reverse_dict)\n    print(""theta (the probability of each topic for every document):"")\n    print(lda.theta)\n    print(""phi (the probability of each word for every topic):"")\n    print(lda.phi)\n    print("""")\n\n    print(""data2"")\n    data = [[1, 2, 3, 1, 2], \n            [1, 4, 5, 4, 4],\n            [1, 4, 2, 5, 5, 4],\n            [1, 3, 3, 2, 3],\n            [1, 1, 3, 2, 2]]\n    \n    for i, document in enumerate(data):\n        print(""Document {}: {}"".format(i, document))\n    docs = Documents(data=data)\n    lda.fit(docs)\n    print(docs.reverse_dict)\n    print(""theta (the probability of each topic for every document):"")\n    print(lda.theta)\n    print(""phi (the probability of each word for every topic):"")\n    print(lda.phi)'"
examples/lle.py,0,"b""import matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom np_ml import LLE\n\nif __name__ == '__main__':\n    lle = LLE()\n    n_points = 1000\n    X, color = datasets.samples_generator.make_s_curve(n_points, random_state=0)\n    \n    y = lle.fit(X)\n    plt.scatter(y[:, 0], y[:, 1], c=color)\n    plt.show()"""
examples/naive_bayes.py,1,"b'""""""\nUse Naive Bayes to discriminate spam e-mail.\n\nSince we need to preprocess text, we import ntlk here.\n""""""\nimport csv\nimport random\nimport numpy as np\nimport string\nfrom tqdm import tqdm\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom np_ml import NaiveBayes\n\nif __name__ == \'__main__\':\n    with open(r\'..\\data\\spam.csv\', encoding=\'latin-1\') as csvfile:\n        data = list(csv.reader(csvfile))\n    print(""preprocessing data..."")\n    data = data[1:]\n    random.shuffle(data)\n    y = [email[0] for email in data]\n    x_pre = [email[1] for email in data]\n    x = []\n    \n    translator = str.maketrans(\'\', \'\', string.punctuation)\n    # translator = str.maketrans(dict.fromkeys(string.punctuation))\n    for text in tqdm(x_pre, ascii=True):\n        text = text.translate(translator)\n        text = text.lower()\n        tokens = nltk.word_tokenize(text)\n        # tokens = [word for word in tokens if word not in stopwords.words(\'english\')]\n        x.append(tokens)\n    \n    train_x = x[:int(len(y)*0.8)]\n    train_y = y[:int(len(y)*0.8)]\n    \n    test_x = x[int(len(y)*0.8):]\n    test_y = y[int(len(y)*0.8):]\n    print(""finish preprocessing data."")\n    print("""")\n    \n    nb = NaiveBayes()\n    nb.fit(train_x, train_y)\n    accuracy = np.sum(np.array(nb.predict(test_x, ys=[\'ham\', \'spam\'])) == np.array(test_y)) / len(test_y)\n    print(""accuracy: "", accuracy)\n    \n    print(""two example:"")\n    example_ham= \'Po de :-):):-):-):-). No need job aha.\'\n    print(""example ham: "")\n    print(example_ham)\n    example_ham = example_ham.lower()\n    example_ham = nltk.word_tokenize(example_ham.translate(translator))\n    print(""predict result: "")\n    print(nb.predict(example_ham, ys=[\'ham\', \'spam\']))\n    print("""")\n    \n    example_spam= \'u r a winner U ave been specially selected 2 receive \xe6\xbe\xb91000 cash or a 4* holiday (flights inc) speak to a live operator 2 claim 0871277810710p/min (18 )\'\n    print(""example spam: "")\n    print(example_spam)\n    example_spam = example_spam.lower()\n    example_spam = nltk.word_tokenize(example_spam.translate(translator))\n    print(""predict result: "")\n    print(nb.predict(example_spam, ys=[\'ham\', \'spam\']))'"
examples/pca.py,0,"b""import matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom np_ml import PCA\n\nif __name__ == '__main__':\n    pca = PCA()\n    n_points = 1000\n    X, color = datasets.samples_generator.make_s_curve(n_points, random_state=0)\n    \n    y = pca.fit(X)\n    plt.scatter(y[:, 0], y[:, 1], c=color)\n    plt.show()\n"""
examples/perceptron_primary.py,5,"b'import numpy as np\nimport csv\n\nfrom np_ml import Perceptron, utils\n\nif __name__ == \'__main__\':\n    with open(r\'..\\data\\iris.csv\', newline=\'\') as csvfile:\n        data = np.array(list(csv.reader(csvfile)))\n    # use binary classifier\n    data = data[data[:, 4] != \'Iris-setosa\']\n    np.random.shuffle(data)\n    x = np.array(data[:, :-1], dtype=np.float32)\n    x = utils.transform(x, 2)\n    y = data[:, -1]\n    y[y == \'Iris-versicolor\'] = 1\n    y[y == \'Iris-virginica\'] = -1\n    y = np.array(y, dtype=np.int32)\n    \n    p = Perceptron(dim=x.shape[-1], eta=0.01, max_epoch=5000)\n    p.fit(x, y, detailed=False)\n    y_pred = p.predict(x)\n    accuracy = np.sum(y_pred == y) / len(y)\n    utils.plot_boundary(p, x, y, title=""Perceptron"", accuracy=accuracy)'"
np_ml/__init__.py,0,b'from .adaboost import *\nfrom .ar import *\nfrom .decision_tree import *\nfrom .hmm import *\nfrom .knn import *\nfrom .lda import *\nfrom .naive_bayes import *\nfrom .perceptron import *\nfrom .random_forest import *\nfrom .svm import *\nfrom .kmeans import *\nfrom .lle import *\nfrom .pca import *\nfrom .affinity_propagation import *\n'
examples/StatisticalLearningMethod/adaboost.py,2,"b'import numpy as np\nfrom np_ml import AdaBoost, TrivialClassification\n\nif __name__ == \'__main__\':\n    print(""--------------------------------------------------------"")\n    print(""AdaBoost simple example!"")\n    print(""example in Statistical Learning Method(\xe3\x80\x8a\xe7\xbb\x9f\xe8\xae\xa1\xe5\xad\xa6\xe4\xb9\xa0\xe6\x96\xb9\xe6\xb3\x95\xe3\x80\x8b)"")\n    print(""--------------------------------------------------------"")\n    x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n    y = np.array([1, 1, 1, -1, -1, -1, 1, 1, 1, -1])\n    print(""x: {}"".format(x))\n    print(""y: {}"".format(y))\n    print("""")\n    adb = AdaBoost(TrivialClassification)\n    adb.fit(x, y, detailed=True)\n    print(""y_pred: {}"".format(adb.predict(x)))\n    '"
examples/StatisticalLearningMethod/decision_tree.py,2,"b'import numpy as np\nfrom np_ml import DecisionTree\n\nif __name__ == ""__main__"":\n    print(""--------------------------------------------------------"")\n    print(""DecisionTree simple example!"")\n    print(""example in Statistical Learning Method(\xe3\x80\x8a\xe7\xbb\x9f\xe8\xae\xa1\xe5\xad\xa6\xe4\xb9\xa0\xe6\x96\xb9\xe6\xb3\x95\xe3\x80\x8b)"")\n    print(""--------------------------------------------------------"")\n    x = np.array([[""young"",    False, False, ""ordinary""],\n                  [""young"",    False, False, ""good""],\n                  [""young"",    True,  False, ""good""],\n                  [""young"",    True,  True,  ""ordinary""],\n                  [""young"",    False, False, ""ordinary""],\n                  [""mid-life"", False, False, ""ordinary""],\n                  [""mid-life"", False, False, ""good""],\n                  [""mid-life"", True,  True,  ""good""],\n                  [""mid-life"", False, True,  ""very good""],\n                  [""mid-life"", False, True,  ""very good""],\n                  [""old"",      False, True,  ""very good""],\n                  [""old"",      False, True,  ""good""],\n                  [""old"",      True,  False, ""good""],\n                  [""old"",      True,  False, ""very good""],\n                  [""old"",      False, False, ""ordinary""]])\n\n    y = np.array([False,\n                  False,\n                  True,\n                  True,\n                  False,\n                  False,\n                  False,\n                  True,\n                  True,\n                  True,\n                  True,\n                  True,\n                  True,\n                  True,\n                  False])\n    print(""x: "")\n    print(x)\n    print(""y: "")\n    print(list(y))\n    print("""")\n    dt = DecisionTree(max_depth=-1)\n    dt.fit(x, y, type=""C4.5"", detailed=True)  # here we can change type to ""ID3"", ""C4.5"" or ""CART""\n    print(""The result tree:"")\n    print(dt.root)\n    print(""y_pred: "")\n    print(dt.predict(x))'"
examples/StatisticalLearningMethod/hmm.py,4,"b'import numpy as np\nfrom np_ml import HMM\n\nif __name__ == \'__main__\':\n    print(""--------------------------------------------------------"")\n    print(""Hidden Markov Model simple example!"")\n    print(""example in Statistical Learning Method(\xe3\x80\x8a\xe7\xbb\x9f\xe8\xae\xa1\xe5\xad\xa6\xe4\xb9\xa0\xe6\x96\xb9\xe6\xb3\x95\xe3\x80\x8b)"")\n    print(""--------------------------------------------------------"")\n    # calculate probability\n    A = np.array([[0.5, 0.2, 0.3],\n                  [0.3, 0.5, 0.2],\n                  [0.2, 0.3, 0.5]])\n    B = np.array([[0.5, 0.5],\n                  [0.4, 0.6],\n                  [0.7, 0.3]])\n    pi = np.array([0.2, 0.4, 0.4])\n    \n    hmm = HMM(A, B, pi)\n\n    O = np.array([0, 1, 0])\n    print(""A: "")\n    print(A)\n    print(""B: "")\n    print(B)\n    print(""pi: "")\n    print(pi)\n    print(""O: "")\n    print(O)\n    print("""")\n    \n    print(""Forward: "")\n    print(hmm.forward(O))\n    print(""Backward: "")\n    print(hmm.backward(O))\n\n    # learning\n    # We will not implement the supervised version, since it need huge amount of data\n    # Baum-Welch algorithm, which is an application of EM\n\n    # predict\n    # Viterbi\n    print(""Viterbi: "")\n    max_prob, path = hmm.Viterbi(O)\n    print(""Maximum probability is:"", max_prob, "" and path is"", path)'"
examples/StatisticalLearningMethod/naive_bayes.py,0,"b'import numpy as np\nfrom np_ml import NaiveBayes\n\nif __name__ == \'__main__\':\n    print(""--------------------------------------------------------"")\n    print(""Naive Bayes simple example!"")\n    print(""example in Statistical Learning Method(\xe3\x80\x8a\xe7\xbb\x9f\xe8\xae\xa1\xe5\xad\xa6\xe4\xb9\xa0\xe6\x96\xb9\xe6\xb3\x95\xe3\x80\x8b)"")\n    print(""--------------------------------------------------------"")\n    # To make it easier, set S as 1, M as 2, L as 3\n    x = [[1, 0],\n                  [1, 1],\n                  [1, 1],\n                  [1, 0],\n                  [1, 0],\n                  [2, 0],\n                  [2, 1],\n                  [2, 1],\n                  [2, 2],\n                  [2, 2],\n                  [3, 2],\n                  [3, 1],\n                  [3, 1],\n                  [3, 2],\n                  [3, 2]]\n    y = [-1, -1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1]\n    print(""x: "")\n    print(x)\n    print(""y: "")\n    print(y)\n    print("""")\n    nb = NaiveBayes()\n    nb.fit(x, y)\n    \n    print(""x_pred: {}"".format([2, 0]))\n    print(""y_pred: {}"".format(nb.predict([2, 0], ys=[1, -1], detailed=True)))'"
examples/StatisticalLearningMethod/perceptron_dual.py,3,"b'import numpy as np\nfrom np_ml import PerceptronDual\n\nif __name__ == \'__main__\':\n    print(""--------------------------------------------------------"")\n    print(""Perceptron dual method simple example!"")\n    print(""example in Statistical Learning Method(\xe3\x80\x8a\xe7\xbb\x9f\xe8\xae\xa1\xe5\xad\xa6\xe4\xb9\xa0\xe6\x96\xb9\xe6\xb3\x95\xe3\x80\x8b)"")\n    print(""--------------------------------------------------------"")\n    p = PerceptronDual()\n    x = np.array([[3, 3], \n                 [4, 3],\n                 [1, 1]])\n    y = np.array([1, 1, -1])\n    print(""x: "")\n    print(x)\n    print(""y: "")\n    print(y)\n    print("""")\n    p.fit(x, y, detailed=True)\n    print(""y_pred: "")\n    print(p.predict(np.array([[3, 3], \n                              [4, 3],\n                              [1, 1]])))'"
examples/StatisticalLearningMethod/perceptron_primary.py,3,"b'import numpy as np\nfrom np_ml import Perceptron\n\nif __name__ == \'__main__\':\n    print(""--------------------------------------------------------"")\n    print(""Perceptron simple example!"")\n    print(""example in Statistical Learning Method(\xe3\x80\x8a\xe7\xbb\x9f\xe8\xae\xa1\xe5\xad\xa6\xe4\xb9\xa0\xe6\x96\xb9\xe6\xb3\x95\xe3\x80\x8b)"")\n    print(""--------------------------------------------------------"")\n    p = Perceptron()\n    x = np.array([[3, 3], \n                 [4, 3],\n                 [1, 1]])\n    y = np.array([1, 1, -1])\n    print(""x: "")\n    print(x)\n    print(""y: "")\n    print(y)\n    print("""")\n    p.fit(x, y, detailed=True)\n    print(""y_pred: "")\n    print(p.predict(np.array([[3, 3], \n                              [4, 3],\n                              [1, 1]])))'"
np_ml/adaboost/__init__.py,0,b'from .adaboost import *'
np_ml/adaboost/adaboost.py,15,"b'import numpy as np\n# x > v or x < v\n# y = 1 or -1\n\nclass TrivialClassification:\n    def __init__(self):\n        self.sign = None\n        self.thres = 0\n    \n    def __str__(self):\n        return self.sign + "" than "" + str(self.thres)\n    \n    def fit(self, x, y, w=None):\n        if w is None:\n            w = np.ones(len(y)) / len(y)\n        data = zip(x, y, w)\n        data = sorted(data, key=lambda s: s[0])\n        [x, y, w] = zip(*data)\n        y = np.array(y)\n        w = np.array(w)\n        correct = np.zeros((2, len(y))) # 0 row for x < v, 1 row for x >= v\n        for i in range(len(y)):\n            w_front = w[:i]\n            w_back  = w[i:]\n            correct[0, i] += np.sum(w_front[y[:i] == 1]) + np.sum(w_back[y[i:] == -1])\n            correct[1, i] += np.sum(w_front[y[:i] == -1]) + np.sum(w_back[y[i:] == 1])\n        idx = np.argmax(correct, axis=1)\n        if correct[0, int(idx[0])] > correct[1, int(idx[1])]:\n            self.sign = ""smaller""\n            self.thres = x[idx[0]]\n        else:\n            self.sign = ""equal to or bigger""\n            self.thres = x[idx[1]]\n            \n    def predict(self, x):\n        if self.sign == ""smaller"":\n            return (x < self.thres)*2-1\n        else:\n            return (x >= self.thres)*2-1\n    \n    def score(self, x, y, w=None): # the wrong percent\n        if w is None:\n            w = np.ones(len(y)) / len(y)\n        return 1 - np.sum(w[self.predict(x) == y])\n\nclass AdaBoost:\n    def __init__(self, weak_learner, epsilon=0.01):\n        self.weak_learner_class = weak_learner\n        self.weak_learners = []\n        self.alphas = []\n        self.epsilon = 0.01\n        \n    @staticmethod\n    def calcAlpha(e):\n        return 0.5*np.log((1-e)/e)\n        \n    def fit(self, x, y, detailed=False):\n        """"""if use detailed, need weak learner has __str__ property""""""\n        w = np.ones(len(y)) / len(y)\n        score = 1\n        epoch = 0\n        while score > self.epsilon:\n            epoch += 1\n            wl = self.weak_learner_class()\n            wl.fit(x, y, w)\n            alpha = AdaBoost.calcAlpha(wl.score(x, y, w))\n            self.alphas.append(alpha)\n            self.weak_learners.append(wl)\n            w = w*np.exp(-alpha*y*self.predict(x))\n            w = w/np.sum(w)\n            score = self.score(x, y)\n            if detailed:\n                print(""Epoch: {}"".format(epoch))\n                print(""Weak learner: {}"".format(wl))\n                print(""alpha: {}"".format(alpha))\n                print(""accuracy: {}"".format(1-score))\n                print()\n                \n    def predict(self, x):\n        ans = np.zeros(x.shape[0])\n        for i in range(len(self.alphas)):\n            ans += self.weak_learners[i].predict(x)*self.alphas[i]\n        return (ans > 0)*2-1\n        \n    def score(self, x, y):\n        return 1 - np.sum(self.predict(x) == y)/len(y)'"
np_ml/affinity_propagation/__init__.py,0,b'from .affinity_propagation import *'
np_ml/affinity_propagation/affinity_propagation.py,15,"b'""""""\n    The implementation is a simpler version of sklearn implementation\n    which is different from the one in wikipedia\n""""""\nimport numpy as np\nimport scipy\n\n\nclass AffinityPropagation:\n    def __init__(self):\n        self.centers = None\n\n    def fit(self, x, damp=0.5, max_epoch=200, convergence_iter=15, preference=None):\n        n_sample = x.shape[0]\n        assert n_sample > 1, ""too little data""\n        centers = np.zeros(x.shape[0])\n        s = -scipy.spatial.distance.cdist(x, x)**2  # caution! the cdist is ||x1 - x_2||\n        if preference is None:\n            preference = np.median(s)\n        s.flat[::(x.shape[0] + 1)] = preference\n        r = np.zeros(s.shape)\n        a = np.zeros(s.shape)\n        epoch = 0\n        same_iter = 0\n        ind = np.arange(n_sample)\n        while epoch < max_epoch and same_iter < convergence_iter:\n            tmp = s + a\n            maxa_ind = np.argmax(tmp, axis=1)\n            maxa = tmp[ind, maxa_ind]\n            tmp[ind, maxa_ind] = -np.inf\n            maxa_2 = np.max(tmp, axis=1)\n\n            r_new = s - maxa[:, None]\n            r_new[ind, maxa_ind] = s[ind, maxa_ind] - maxa_2\n            r = damp * r + (1. - damp) * r_new\n\n            a_new = np.maximum(r, 0)\n            a_new.flat[::n_sample+1] = r.flat[::n_sample+1]\n\n            a_new -= np.sum(a_new, axis=0)\n            da = np.diag(a_new).copy()\n            a_new = a_new.clip(0, np.inf)\n            a_new.flat[::n_sample+1] = da\n            a = damp*a - (1.-damp)*a_new\n\n            centers_new = (np.diag(r)+np.diag(a)) > 0\n            if 0 < np.sum(centers_new) < n_sample and (centers == centers_new).all():\n                same_iter += 1\n            else:\n                centers = centers_new\n                same_iter = 0\n            epoch += 1\n        self.centers = x[centers, :]\n        # after find centers, use knn\n        s.flat[::n_sample+1] = 0.  # change the s from\n        label = s[:, centers].argmax(axis=1)\n        return label\n\n    def predict(self):\n        dist_matrix = scipy.spatial.distance.cdist(x, self.centers)\n        label = np.argmin(dist_matrix, axis=1)\n        return label\n'"
np_ml/ar/AR.py,17,"b'import numpy as np\nimport numpy.linalg as LA\n\n# calculate ACF from l=0~l\ndef ACF(x, l):\n    acf = np.zeros(l+1)\n    for i in range(l+1):\n        cov_matrix = np.cov(x[:len(x)-i], x[i:])\n        acf[i] = cov_matrix[0, 1] / np.sqrt(cov_matrix[0, 0]*cov_matrix[1, 1])\n    return acf\n\n# the sample PACF is computed via the \n# Durbin-Levinson recursive algorithm\ndef PACF(x, l, acf=None):\n    if acf == None:\n        acf = ACF(x, l)\n    pacf = np.zeros([l+1, l+1])\n    for k in range(1,l+1):\n        pacf[k, k] = (acf[k] - np.dot(pacf[k-1, 1:k], acf[k-1:0:-1])) / \\\n                     (1 - np.dot(pacf[k-1, 1:k], acf[1:k]))\n        for j in range(1,k):\n            pacf[k, j] = pacf[k-1, j] - pacf[k, k]*pacf[k-1, k-j]\n            \n    return pacf\n\nclass AR:\n    def __init__(self, p=0):\n        self.p = p\n        self.sigma_a = 0\n        self.phi = None\n        self.res = None\n        \n    def identifyP(self, x, method=""PACF""):\n        if method == ""PACF"":\n            self.identifyP_PACF(x, epsilon=0.05)\n    \n    def identifyP_PACF(self, x, l=20, epsilon=0.05):\n        pacf = PACF(x, l)\n        for i in range(1, l+1):\n            if np.abs(pacf[i, i]) < epsilon:\n                self.p = i-1\n                break\n    \n    def fit(self, x):\n        """"""\n        Just a linear regression using least square.\n        """"""\n        if self.p == 0: # if did not explicitly assign p\n            self.identifyP(x)\n        X = np.zeros([len(x)-self.p, self.p+1])\n        Y = np.zeros([len(x)-self.p, 1])\n        for i in range(len(x)-self.p):\n            Y[i, 0] = x[i+self.p]\n            X[i, 0] = 1\n            X[i, 1:] = x[i:i+self.p] # Caution! Here the order of the input is not the order in the model\n        self.phi = np.matmul(np.matmul(LA.inv(np.matmul(X.T, X)), X.T), Y)\n        self.res = Y - np.matmul(X, self.phi)\n        self.sigma_a = np.sqrt(np.sum(self.res*self.res)/(len(x)-2*self.p-1))\n    \n    def predict(self, x, step=1):\n        X = np.zeros([1, self.p+1])\n        X[0, 0] = 1\n        if step == 1:\n            return np.matmul(X, self.phi)\n        else:\n            pred = []\n            for _ in range(step):\n                pred.append(float(np.matmul(X, self.phi)))\n                X[0, 1:-1] = X[0, 2:]\n                X[0, -1] = pred[-1]\n            return np.array(pred)\n\nif __name__ == \'__main__\':\n    data = np.genfromtxt(r""..\\..\\data\\dgnp82.dat"")\n\n    ar3 = AR(3)\n    ar3.fit(data)\n    print(ar3.phi)\n    print(ar3.sigma_a*ar3.sigma_a)\n    print(ar3.predict(data, step=8))'"
np_ml/ar/__init__.py,0,b''
np_ml/decision_tree/__init__.py,0,b'from .decision_tree import *'
np_ml/decision_tree/decision_tree.py,19,"b'import numpy as np\n\ndef entropy(col):\n    _, cnts = np.unique(col, return_counts=True)\n    cnts = np.array(cnts)/len(col)\n    return -np.sum(cnts*np.log2(cnts))\n\n# For ID3\ndef calcInforGain(col_x, col_y):\n    HD = entropy(col_y)\n    HDA = 0\n    unique, cnts = np.unique(col_x, return_counts=True)\n    cnts = np.array(cnts)/len(col_x)\n    cnts = dict(zip(unique, cnts))\n    for key, val in cnts.items():\n        HDA += val*entropy(col_y[col_x == key])\n    return HD - HDA, unique\n\n# For C4.5\ndef calcInforGainRatio(col_x, col_y):\n    HD = entropy(col_y)\n    HDA = 0\n    unique, cnts = np.unique(col_x, return_counts=True)\n    cnts = np.array(cnts)/len(col_x)\n    cnts = dict(zip(unique, cnts))\n    for key, val in cnts.items():\n        HDA += val*entropy(col_y[col_x == key])\n    return (HD - HDA)/entropy(col_x), unique\n    \n# For CART\ndef Gini(col):\n    unique, cnts = np.unique(col, return_counts=True)\n    cnts = np.array(cnts)/len(col)\n    return 1 - np.sum(cnts ** 2)\n    \ndef findMinGini(col_x, col_y):\n    unique, cnts = np.unique(col_x, return_counts=True)\n    cnts = dict(zip(unique, cnts))\n    min_gini = 1\n    min_key = None\n    for key, cnt in cnts.items():\n        gini = cnt/len(col_y)*Gini(col_y[col_x == key]) + (1-cnt/len(col_y))*Gini(col_y[col_x != key])\n        if gini < min_gini:\n            min_gini = gini\n            min_key = key\n    return min_gini, min_key\n    \nclass Node:\n    def __init__(self, key, val, depth):\n        self.key = key\n        self.val = val\n        self.depth = depth\n        self.children = []\n        \n    def __str__(self, indent=0):\n        ans = """"\n        if not self.children:\n            ans = str(self.key) + "": "" + str(self.val) + """"\n        else:\n            ans += str(self.key) + "": "" + str(self.val) + ""(""\n            for child in self.children:\n                ans += str(child) + "", ""\n            ans += "")""\n        return ans\n    \n    def addChild(self, key, val, depth=0):\n        self.children.append(Node(key, val, depth))\n        return self.children[-1]\n    \nclass DecisionTree:\n    def __init__(self, epsilon=0, max_depth=-1): # here depth=-1 means no constrain\n        self.root = Node(""root"", 0, max_depth)\n        self.epsilon = epsilon\n        self.type = None\n        \n    def fit(self, x, y, type=""CART"", detailed=False):\n        self.type = type\n        if type == ""CART"":\n            self.CARTgenerate(x, y, self.root, detailed)\n        else:\n            self.generate(x, y, self.root, type, detailed)\n    \n    def generate(self, x, y, root, type, detailed):\n        # if empty\n        if x.size == 0:\n            return\n        # if all left are the same kind\n        if np.all(y == True) or np.all(y == False):\n            root.addChild(""leaf"", y[0])\n            return\n        # if all the feature are the same, use the popular one\n        if np.all(x == x[0,:]) or root.depth == 0:\n            unique, cnts = np.unique(y, return_counts=True)\n            cnts = dict(zip(unique, cnts))\n            root.addChild(""leaf"", cnts[True] > cnts[False])\n            return \n            \n        max_gain = 0\n        max_feature = -1\n        max_feature_vals = None\n        \n        for i in range(x.shape[-1]):\n            if type==""ID3"":\n                gain, feature_vals = calcInforGain(x[:, i], y)\n            elif type==""C4.5"":\n                gain, feature_vals = calcInforGainRatio(x[:, i], y)\n            if gain > max_gain:\n                max_gain = gain\n                max_feature = i\n                max_feature_vals = feature_vals\n        if max_gain < self.epsilon:\n            return\n        else:\n            for val in max_feature_vals:\n                child = root.addChild(max_feature, val, root.depth-1)\n                self.generate(np.delete(x[x[:, max_feature]==val], max_feature, axis=-1), y[x[:, max_feature]==val], child, type, detailed)\n    \n    def CARTgenerate(self, x, y, root, detailed, min_gini_old=1):\n        # if empty\n        if x.size == 0:\n            return\n        # if all left are the same kind\n        if np.all(y == True) or np.all(y == False):\n            root.addChild(""leaf"", y[0])\n            return\n        # if all the feature are the same, use the popular one\n        if np.all(x == x[0,:]) or root.depth == 0:\n            unique, cnts = np.unique(y, return_counts=True)\n            cnts = dict(zip(unique, cnts))\n            root.addChild(""leaf"", cnts[True] > cnts[False])\n            return \n            \n        min_gini = 1\n        min_feature = None\n        min_feature_val = None\n        for i in range(x.shape[-1]):\n            gini, feature_val = findMinGini(x[:, i], y)\n            if detailed:\n                print(gini, feature_val, i)\n            if gini < min_gini:\n                min_gini = gini\n                min_feature = i\n                min_feature_val = feature_val\n        if abs(min_gini - min_gini_old) < 1e-6: # all feature are random\n            unique, cnts = np.unique(y, return_counts=True)\n            cnts = dict(zip(unique, cnts))\n            root.addChild(""leaf"", cnts[True] > cnts[False])\n            return\n            \n        child_true = root.addChild((min_feature, min_feature_val,), True, root.depth-1)\n        self.CARTgenerate(x[x[:, min_feature]==min_feature_val], y[x[:, min_feature]==min_feature_val], child_true, detailed, min_gini)\n        child_false = root.addChild((min_feature, min_feature_val,), False, root.depth-1)\n        self.CARTgenerate(x[x[:, min_feature]!=min_feature_val], y[x[:, min_feature]!=min_feature_val], child_false, detailed, min_gini)\n    \n    # TODO: find nice regularization function\n    def pruning(self, root):\n        pass\n    \n    def predict(self, x):\n        assert(len(self.root.children) > 0)\n        if len(x.shape) == 1:\n            tmp = self.root\n            if self.type == \'CART\':\n                while len(tmp.children) > 1:\n                    feature = tmp.children[0].key[0]\n                    if x[feature] == tmp.children[0].key[1]:\n                        tmp = tmp.children[0]\n                    else:\n                        tmp = tmp.children[1]\n                if len(tmp.children) == 1 and tmp.children[0].key == \'leaf\':\n                    return tmp.children[0].val\n            else:\n                while len(tmp.children) > 1:\n                    feature = tmp.children[0].key\n                    if x[feature] == tmp.children[0].val:\n                        tmp = tmp.children[0]\n                    else:\n                        tmp = tmp.children[1]\n                if len(tmp.children) == 1 and tmp.children[0].key == \'leaf\':\n                    return tmp.children[0].val\n        else:\n            assert(len(x.shape) == 2)\n            ans = []\n            for test in x:\n                ans.append(self.predict(test))\n            return ans'"
np_ml/hmm/__init__.py,0,b'from .hmm import *'
np_ml/hmm/hmm.py,10,"b'import numpy as np\n\nclass HMM:\n    def __init__(self, A=None, B=None, pi=None):\n        self.A = A\n        self.B = B\n        self.pi = pi\n    def forward(self, O, detailed=False):\n        alpha = self.pi*self.B[:, O[0]]\n        if detailed:\n            print(""alpha: {}"".format(alpha))\n        for t in range(1, len(O)):\n            alpha = np.squeeze(np.matmul(self.A.T, alpha[..., None]))*self.B[:, O[t]]\n        return np.sum(alpha)\n        \n    def backward(self, O):\n        beta = np.ones(self.pi.shape)\n        for t in range(len(O)-1, 0, -1):\n            beta = np.squeeze(np.matmul(self.A, (self.B[:, O[t]]*beta)[..., None]))\n        return np.sum(self.pi*self.B[:, O[0]]*beta)\n        \n    def Viterbi(self, O):\n        # initialization\n        delta = self.pi*self.B[:, O[0]]\n        psi = np.zeros((len(O), len(delta))) # t*i\n        for t in range(1, len(O)):\n            delta = np.max(delta[..., None]*self.A, axis=-1)*self.B[:, O[t]]\n            psi[t, :] = np.argmax(delta[..., None]*self.A, axis=-1)\n        I = []\n        i = np.argmax(delta)\n        I.append(i)\n        for t in range(len(O)-1, 0, -1):\n            i = psi[t, int(i)]\n            I.append(i)\n        I = [int(i) for i in I]\n        return np.max(delta), I[::-1]'"
np_ml/kmeans/__init__.py,0,b'from .kmeans import *'
np_ml/kmeans/kmeans.py,6,"b'import numpy as np\nimport scipy\n\nclass KMeans:\n    def __init__(self, k=2, eps=1e-5):\n        self.k = k\n        self.eps = eps\n        self.centers = None\n\n    def fit(self, x, detailed=False):\n        if not isinstance(x, np.ndarray):\n            x = np.array(x)\n        self.centers = x[np.random.randint(x.shape[0], size=self.k)]\n        flag = True\n        label = None\n        while flag:\n            flag = False\n            dist_matrix = scipy.spatial.distance.cdist(x, self.centers)\n            label = np.argmin(dist_matrix, axis=1)\n            for i in range(self.k):\n                new_center = x[label == i].mean()\n                if np.linalg.norm(new_center-self.centers[i]) > self.eps:\n                    flag = True\n                self.centers[i] = new_center\n            if detailed:\n                for i in range(self.k):\n                    print(""centers:"")\n                    print(self.centers[i])\n                    print("""")\n        return label\n\n    def predict(self, x):\n        dist_matrix = scipy.spatial.distance.cdist(x, self.centers)\n        label = np.argmin(dist_matrix, axis=1)\n        return label'"
np_ml/knn/__init__.py,0,b'from .knn import *'
np_ml/knn/knn.py,1,"b'# As far as my research, all kinds of approximate way would degrade to linear scan.\n# Therefore, I only implement the linear scan.\n# For more advanced data structures and algorithms, \n# see here: https://stackoverflow.com/questions/5751114/nearest-neighbors-in-high-dimensional-data\nimport numpy as np\nimport numpy.linalg as LA\nfrom heapq import heappush, heappop\n\nclass KNN:\n    def __init__(self):\n        pass\n    \n    def fit(self, x, y):\n        self.train_x = x\n        self.train_y = y\n        \n    def predict(self, x, k=5, similarity=""euclidean"", detailed=False):\n        heap = []\n        if x.ndim == 1:\n            if similarity == ""euclidean"":\n                for i in range(len(self.train_y)):\n                    heappush(heap, (-LA.norm(self.train_x[i, :]-x), list(self.train_x[i, :]), self.train_y[i]))\n                    if len(heap) > k:\n                        heappop(heap)\n            if detailed:\n                print(""For {}, the {} nearest neighbor are:"".format(x, k))\n                for n in heap[::-1]:\n                    print(""x: {}, y: {}"".format(n[1], n[2]))\n            [_, _, votes] = zip(*heap)\n            return max(set(votes), key=votes.count)\n        else:\n            return np.array([self.predict(x[i, :], detailed=detailed) for i in range(x.shape[0])])'"
np_ml/lda/__init__.py,0,b'from .lda import *'
np_ml/lda/lda.py,11,"b'# LDA model with Gibbs Sampling\n# the implementation is based on \n# Darling W M. A theoretical and practical implementation tutorial on topic modeling and gibbs sampling, 2011.\n# Gregor Heinrich, Parameter estimation for text analysis, 2004\n# Many variables may have sparse property which may help optimize \n# computation.\nimport numpy as np\nimport random\nfrom collections import OrderedDict\nimport os\n\nclass Documents:\n    def __init__(self, data=None, dir=None):\n        if data is not None:\n            self.documents = []\n            self.ndoc = len(data)\n            self.dict = {}\n            self.reverse_dict = {}\n            self.nword = 0\n            for document in data:\n                self.documents.append(np.zeros(len(document)))\n                for i in range(len(document)):\n                    word = document[i]\n                    if not word in self.dict:\n                        self.dict[word] = self.nword\n                        self.reverse_dict[self.nword] = word\n                        self.nword += 1\n                    self.documents[-1][i] = self.dict[word]\n        else: # input a directory\n            pass\n                \nclass LDA:\n    def __init__(self, K=2, alpha=0.1, beta=0.1):\n        self.K = K\n        self.alpha=0.1\n        self.beta=0.1\n        # the following variable is correspond to the link provided\n        self.n_d_k = None\n        self.n_k_word = None\n        self.n_k = None\n        self.phi = None\n        self.theta = None\n        \n    def fit(self, data, iter_times=100, detailed=False):\n        # initial variables\n        self.n_d_k = np.zeros((data.ndoc, self.K))\n        self.n_k_word = np.zeros((self.K, data.nword))\n        self.n_k = np.zeros(self.K)\n        self.n_d = np.zeros(data.ndoc)\n        self.p = np.zeros(self.K) # is not normalized\n        z = [np.zeros(len(document)) for document in data.documents] # Here we will only use the shape\n        for d in range(data.ndoc):\n            document = data.documents[d]\n            for w in range(len(document)):\n                word = document[w]\n                k = np.random.randint(self.K)\n                z[d][w] = k\n                self.n_d_k[d, k] += 1\n                self.n_k_word[k, int(word)] += 1\n                self.n_k[k] += 1\n                self.n_d[d] += 1\n        # Gibbs Sampling\n        for epoch in range(iter_times):\n            if detailed:\n                print(""Epoch:"", epoch)\n            for d in range(data.ndoc):\n                document = data.documents[d]\n                for w in range(len(document)):\n                    word = document[w]\n                    k = int(z[d][w])\n                    self.n_d_k[d, k] -= 1\n                    self.n_k_word[k, int(word)] -= 1\n                    self.n_k[k] -= 1\n                    self.n_d[d] -= 1\n                    self.p = (self.n_d_k[d, :]+self.alpha)*(self.n_k_word[:, int(word)]+self.beta)/(self.n_k + self.beta*data.nword)\n                    p = np.random.uniform(0, np.sum(self.p))\n                    # print(""p:"", p)\n                    # print(""self.p:"", self.p)\n                    for k in range(self.K):\n                        if p <= self.p[k]:\n                            z[d][w] = k\n                            self.n_d_k[d, int(k)] += 1\n                            self.n_k_word[k, int(word)] += 1\n                            self.n_k[k] += 1\n                            self.n_d[d] += 1\n                            break\n                        else:\n                            p -= self.p[k]\n        self.phi = np.zeros((self.K, data.nword))\n        for k in range(self.K):\n            self.phi[k, :] = (self.n_k_word[k, :] + self.beta)/(self.n_k[k] + self.beta)\n        self.theta = np.zeros((data.ndoc, self.K))\n        for d in range(data.ndoc):\n            self.theta[d, :] = (self.n_d_k[d, :] + self.alpha)/(self.n_d[d] + self.alpha)\n        '"
np_ml/lle/__init__.py,0,b'from .lle import *'
np_ml/lle/lle.py,8,"b'""""""LLE Algorithm\n\n    for detail please see: https://cs.nyu.edu/~roweis/lle/algorithm.html\n""""""\nimport numpy as np\nimport numpy.linalg as LA\nimport scipy\n\nclass LLE:\n    def __init__(self):\n        pass\n\n    def fit(self, x, k=10, dim=2):\n        x = np.array(x)\n        assert x.shape[0] > k, ""too much neighbor selected!""\n        dist = scipy.spatial.distance.cdist(x, x)\n        nn = np.argsort(dist, axis=1)[:, 1:k+1]\n        W = np.zeros(dist.shape)\n        for i in range(x.shape[0]):\n            Z = x[nn[i]]  # knn for x[i]\n            Z -= x[i]\n            C = np.matmul(Z, Z.T)\n            if k > x.shape[1]:\n                C += 1e-3 * np.trace(C)*np.identity(k)\n            w = np.matmul(LA.inv(C), np.ones((k, 1)))\n            W[i, nn[i]] = w.flatten() / np.sum(w)\n        M = np.matmul((np.identity(x.shape[0])-W).T, (np.identity(x.shape[0])-W))\n        e_val, e_vec = LA.eig(M)\n        idx = e_val.argsort()\n        e_vec = e_vec[:, idx][:, 1:dim+1]\n        \n        return e_vec'"
np_ml/naive_bayes/__init__.py,0,b'from .naive_bayes import *'
np_ml/naive_bayes/naive_bayes.py,0,"b'from tqdm import tqdm\nimport random\n\nclass NaiveBayes:\n    def __init__(self, laplace=1):\n        self.laplace = 1\n        self.data = None\n        self.cnts = None\n        self.total = None\n        self.priors = None\n        \n    def fit(self, x, y, detailed=False):\n        self.x = x\n        self.y = y\n        \n    def predict(self, x, ys, priors=None, detailed=False):\n        """"""ys is the type of output, e.x. [1, -1] or [\'spam\', \'ham\']""""""\n        if self.priors is None:  # if no prior distribution is given, get it from data\n            if priors is not None:\n                self.priors = priors\n            else :\n                self.priors = {}\n                for y in ys:\n                    self.priors[y] = 0\n                for y in self.y:\n                    self.priors[y] += 1\n        # initialize the counting dictionary\n        # here we assume the order in x in insignificance.\n        if self.cnts is None:\n            self.cnts = {}\n            self.total = {}\n            for y in ys:\n                self.cnts[y] = {}\n                self.total[y] = 0\n            for i in range(len(self.y)):\n                assert self.y[i] in ys, ""ys is wrong!""\n                tmp_set = set()\n                for element in self.x[i]:\n                    if element not in tmp_set:\n                        tmp_set.add(element)\n                        if element not in self.cnts[self.y[i]]:\n                            self.cnts[self.y[i]][element] = 1\n                        else:\n                            self.cnts[self.y[i]][element] += 1\n                        self.total[y] += 1\n        if not x or type(x[0]) != list:\n            max_posterior = 0\n            max_y = None\n            for y in ys:\n                posterior = 1\n                for element in x:\n                    if element in self.cnts[y]:\n                        posterior *= (self.cnts[y][element]+self.laplace) / (self.priors[y]+len(self.cnts[y])*self.laplace)\n                    else:\n                        posterior *= self.laplace / (self.priors[y]+len(self.cnts[y])*self.laplace)\n                posterior *= (self.priors[y]+self.laplace) / (len(self.y)+len(self.priors)*self.laplace)\n                if detailed == True:\n                    print(""x:"", x, ""y:"", y, ""posterior:"", posterior)\n                if posterior > max_posterior:\n                    max_posterior = posterior\n                    max_y = y\n            return max_y\n        else:\n            ans = []\n            for i in tqdm(range(len(x)), ascii=True):\n                ans.append(self.predict(x[i], ys, detailed=detailed))\n            return ans\n    def score(self, x, y):\n        pass\n        '"
np_ml/pca/__init__.py,0,b'from .pca import *'
np_ml/pca/pca.py,1,"b'import numpy as np\nimport numpy.linalg as LA\n\nclass PCA:\n    def __init__(self):\n        pass\n\n    def fit(self, x, dim=2):\n        e_val, e_vec = LA.eig(np.cov(x, rowvar=False))\n        idx = e_val.argsort()[::-1]\n        e_vec = e_vec[:, idx][:, :dim]\n        return x.dot(e_vec)'"
np_ml/perceptron/__init__.py,0,b'from .perceptron_primary import *\nfrom .perceptron_dual import *'
np_ml/perceptron/perceptron_dual.py,9,"b'# Simple implementation for 1 layer perceptron\n# dual learning algorithm\nimport numpy as np\nimport numpy.linalg as LA\n\ndef gram(x):\n    return np.matmul(x, x.T)\n\nclass PerceptronDual:\n    def __init__(self, dim=2, eta=1):\n        self.dim = dim\n        self.eta = eta\n        self.alpha = None  # np.random.randn(dim)\n        self.W = np.zeros(dim)  # np.random.randn(dim)\n        self.b = np.zeros(1)  # np.random.randn()\n        self.Gram = None\n        \n    def fit(self, x, y, detailed=False):\n        self.alpha = np.zeros(y.shape)\n        self.Gram = gram(x)\n        i = 0\n        cnt = 0\n        epoch = 0\n        finished = True\n        \n        while cnt != x.shape[0]:\n            cnt += 1\n            if y[i]*(np.dot(self.alpha*y, self.Gram[i, :])+self.b) <= 0:\n                self.alpha[i] += self.eta\n                self.b += self.eta*y[i]\n                cnt = 0\n                epoch += 1\n                if detailed == True:\n                    print(""Epoch:"", epoch, "": alpha"", self.alpha, ""b:"", self.b)\n                    \n            i = (i+1)%x.shape[0]\n        # Add a new axis to make the multiply column-wise\n        self.W = (self.alpha*y)[..., None]*x\n    \n    # the predict and score part are the same as the primary one\n    def predict(self, x):\n        return np.sign((np.sum(self.W*x, axis=-1)+self.b))\n        \n    def score(self, x, y):\n        if x.shape[-1] != self.dim:\n            print(""The input shape is incorrect!"")\n            return 0\n        dis = np.abs(np.sum(self.W*x, axis=-1)+self.b)*y\n        return -np.sum(dis*(dis<0))*1/LA.norm(self.W)\n        '"
np_ml/perceptron/perceptron_primary.py,6,"b'# Simple implementation for 1 layer perceptron\n# primary learning algorithm\nimport numpy as np\nimport numpy.linalg as LA\n\nclass Perceptron:\n    def __init__(self, dim=2, eta=1, max_epoch=None):\n        self.dim = dim\n        self.eta = eta\n        self.W = np.zeros(dim)  # np.random.randn(dim)\n        self.b = np.zeros(1)  # np.random.randn()\n        self.max_epoch = 1000\n        \n    def fit(self, x, y, detailed=False):\n        i = 0\n        cnt = 0\n        epoch = 0\n        finished = True\n        \n        while cnt != x.shape[0] and (self.max_epoch is None or epoch < self.max_epoch):\n            cnt += 1\n            if y[i]*(np.sum(self.W*x[i, :], axis=-1)+self.b) <= 0:\n                self.W += self.eta*y[i]*x[i, :]\n                self.b += self.eta*y[i]\n                cnt = 0\n                epoch += 1\n                if detailed == True:\n                    print(""Epoch:"", epoch, "": W"", self.W, ""b:"", self.b)\n                    \n            i = (i+1)%x.shape[0]\n    \n    def predict(self, x):\n        return np.sign((np.sum(self.W*x, axis=-1)+self.b))\n        \n    def score(self, x, y):\n        if x.shape[-1] != self.dim:\n            print(""The input shape is incorrect!"")\n            return 0\n        dis = np.abs(np.sum(self.W*x, axis=-1)+self.b)*y\n        return -np.sum(dis*(dis<0))*1/LA.norm(self.W)'"
np_ml/random_forest/__init__.py,0,b''
np_ml/random_forest/decision_tree.py,18,"b'import numpy as np\n\ndef entropy(col):\n    _, cnts = np.unique(col, return_counts=True)\n    cnts = np.array(cnts)/len(col)\n    cnts[cnts!=0] = cnts[cnts!=0]*np.log2(cnts[cnts!=0])\n    return -np.sum(cnts)\n\n# For ID3\ndef calcInforGain(col_x, col_y):\n    HD = entropy(col_y)\n    HDA = 0\n    unique = np.unique(col_x)\n    for key in unique:\n        HDA += entropy(col_y[col_x == key])\n    return HD - HDA, unique\n\n# For C4.5\ndef calcInforGainRatio(col_x, col_y):\n    HD = entropy(col_y)\n    HDA = 0\n    unique = np.unique(col_x)\n    for key in unique:\n        HDA += entropy(col_y[col_x == key])\n    return (HD - HDA)/entropy(col_x), unique\n    \n# For CART\ndef Gini(col):\n    unique, cnts = np.unique(col, return_counts=True)\n    cnts = np.array(cnts)/len(col)\n    return 1 - np.sum(cnts ** 2)\n    \ndef findMinGini(col_x, col_y):\n    unique, cnts = np.unique(col_x, return_counts=True)\n    cnts = dict(zip(unique, cnts))\n    min_gini = 1\n    min_key = None\n    for key, cnt in cnts.items():\n        gini = cnt/len(col_y)*Gini(col_y[col_x == key]) + (1-cnt/len(col_y))*Gini(col_y[col_x != key])\n        if gini < min_gini:\n            min_gini = gini\n            min_key = key\n    return min_gini, min_key\n    \nclass Node:\n    def __init__(self, key, val):\n        self.key = key\n        self.val = val\n        self.children = []\n        \n    def __str__(self, indent=0):\n        ans = """"\n        if not self.children:\n            ans = str(self.key) + "": "" + str(self.val) + """"\n        else:\n            ans += str(self.key) + "": "" + str(self.val) + ""(""\n            for child in self.children:\n                ans += str(child) + "", ""\n            ans += "")""\n        return ans\n    \n    def addChild(self, key, val):\n        self.children.append(Node(key, val))\n        return self.children[-1]\n    \nclass DecisionTree:\n    def __init__(self, epsilon=0):\n        self.root = Node(""root"", 0)\n        self.epsilon = epsilon\n        self.type = None\n        \n    def fit(self, x, y, type=""CART"", detailed=False):\n        self.type = type\n        if type == ""CART"":\n            self.CARTgenerate(x, y, self.root, detailed)\n        else:\n            self.generate(x, y, self.root, type, detailed)\n    \n    def generate(self, x, y, root, detailed):\n        # if empty\n        if x.size == 0:\n            return\n        # if all left are the same kind\n        if np.all(y == True) or np.all(y == False):\n            root.addChild(""leaf"", y[0])\n            return\n        # if all the feature are the same, use the popular one\n        if np.all(x == x[0,:]):\n            unique, cnts = np.unique(y, return_counts=True)\n            cnts = dict(zip(unique, cnts))\n            root.addChild(""leaf"", cnts[True] > cnts[False])\n            return \n        \n        max_gain = 0\n        max_feature = -1\n        max_feature_vals = None\n        \n        for i in range(x.shape[-1]):\n            if type==""ID3"":\n                gain, feature_vals = calcInforGain(x[:, i], y)\n            elif type==""C4.5"":\n                gain, feature_vals = calcInforGainRatio(x[:, i], y)\n            if gain > max_gain:\n                max_gain = gain\n                max_feature = i\n                max_feature_vals = feature_vals\n        if max_gain < self.epsilon:\n            return\n        else:\n            for val in max_feature_vals:\n                child = root.addChild(max_feature, val)\n                self.generate(np.delete(x[x[:, max_feature]==val], max_feature, axis=-1), y[x[:, max_feature]==val], child, type, detailed)\n    \n    def CARTgenerate(self, x, y, root, detailed, min_gini_old=1):\n        # if empty\n        if x.size == 0:\n            return\n        # if all left are the same kind\n        if np.all(y == True) or np.all(y == False):\n            root.addChild(""leaf"", y[0])\n            return\n        # if all the feature are the same, use the popular one\n        if np.all(x == x[0,:]):\n            unique, cnts = np.unique(y, return_counts=True)\n            cnts = dict(zip(unique, cnts))\n            root.addChild(""leaf"", cnts[True] > cnts[False])\n            return \n        \n        min_gini = 1\n        min_feature = None\n        min_feature_val = None\n        for i in range(x.shape[-1]):\n            gini, feature_val = findMinGini(x[:, i], y)\n            if detailed:\n                print(gini, feature_val, i)\n            if gini < min_gini:\n                min_gini = gini\n                min_feature = i\n                min_feature_val = feature_val\n        if abs(min_gini - min_gini_old) < 1e-6: # all feature are random\n            unique, cnts = np.unique(y, return_counts=True)\n            cnts = dict(zip(unique, cnts))\n            root.addChild(""leaf"", cnts[True] > cnts[False])\n            return\n            \n        child_true = root.addChild((min_feature, min_feature_val,), True)\n        self.CARTgenerate(x[x[:, min_feature]==min_feature_val], y[x[:, min_feature]==min_feature_val], child_true, detailed, min_gini)\n        child_false = root.addChild((min_feature, min_feature_val,), False)\n        self.CARTgenerate(x[x[:, min_feature]!=min_feature_val], y[x[:, min_feature]!=min_feature_val], child_false, detailed, min_gini)\n    \n    # TODO: find nice regularization function\n    def pruning(self, root):\n        pass\n    \n    def predict(self, x):\n        assert(len(self.root.children) > 0)\n        if len(x.shape) == 1:\n            tmp = self.root\n            if self.type == \'CART\':\n                while len(tmp.children) > 1:\n                    feature = tmp.children[0].key[0]\n                    if x[feature] == tmp.children[0].key[1]:\n                        tmp = tmp.children[0]\n                    else:\n                        tmp = tmp.children[1]\n                if len(tmp.children) == 1 and tmp.children[0].key == \'leaf\':\n                    return tmp.children[0].val\n            else:\n                while len(tmp.children) > 1:\n                    feature = tmp.children[0].key\n                    if x[feature] == tmp.children[0].val:\n                        tmp = tmp.children[0]\n                    else:\n                        tmp = tmp.children[1]\n                if len(tmp.children) == 1 and tmp.children[0].key == \'leaf\':\n                    return tmp.children[0].val\n        else:\n            assert(len(x.shape) == 2)\n            ans = []\n            for test in x:\n                ans.append(self.predict(test))\n            return ans'"
np_ml/random_forest/random_forest.py,5,"b'import numpy as np\nimport csv\nfrom decision_tree import DecisionTree\nfrom collections import Counter\n\nclass RandomForest:\n    def __init__(self, num):\n        self.num = num\n        self.dts = []\n        for _ in range(num):\n            self.dts.append(DecisionTree())\n    def fit(self, x, y, detailed=False):\n        num_attribute = x.shape[1]\n        gap = num_attribute // self.num\n        for i in range(self.num-1):\n            self.dts[i].fit(x[:, i*gap:(i+1)*gap], y, detailed=detailed)\n        self.dts[-1].fit(x[:, (self.num-1)*gap:], y, detailed=detailed)\n    \n    def predict(self, x):\n        votes = []\n        num_attribute = x.shape[1]\n        gap = num_attribute // self.num\n        for i in range(self.num-1):\n            votes.append(self.dts[i].predict(x[:, i*gap:(i+1)*gap]))\n        votes.append(self.dts[-1].predict(x[:, (self.num-1)*gap:]))\n        # print(votes)\n        return np.sum(np.array(votes), axis=0) > len(votes) / 2\n        \n    def evaluate(self, x, y):\n        y_pred = self.predict(x)\n        return np.sum(np.array(y) == np.array(y_pred)) / len(y)\n\nif __name__ == \'__main__\':\n    with open(r\'..\\..\\data\\tic_tac_toe.csv\', newline=\'\') as csvfile:\n        data = np.array(list(csv.reader(csvfile)))\n    # data = np.genfromtxt(\'tic_tac_toe.csv\', delimiter=\',\', dtype=None)\n    np.random.shuffle(data)\n    train = data[:int(data.shape[0]*0.8), :]\n    test = data[int(data.shape[0]*0.8):, :]\n    train_x = train[:, :-1]\n    train_y = train[:, -1]\n    train_y = (train_y == ""positive"")\n    test_x = test[:, :-1]\n    test_y = test[:, -1]\n    test_y = (test_y == ""positive"")\n                  \n    rf = RandomForest(2)\n    rf.fit(train_x, train_y, detailed=False)\n    print(rf.evaluate(test_x, test_y))'"
np_ml/svm/__init__.py,0,b'from .svm import *'
np_ml/svm/svm.py,12,"b'# SVM using SMO algorithm\nimport numpy as np\nimport numpy.linalg as LA\n\nnp.set_printoptions(precision=2)\n\nclass SVM:\n    def __init__(self):\n        self.alpha = None\n        self.b = 0\n        self.kernel = None\n        self.x = None\n        self.y = None\n        \n    def g(self, x0): # TODO: find a vectorized way \n            ans = 0\n            for i in range(len(self.y)):\n                ans += self.alpha[i]*self.y[i]*self.kernel(self.x[i, :], x0)\n            return ans + self.b\n        \n    def fit(self, x, y, kernel=None, C=10., epsilon=0.1, detailed=False):\n        # initialization\n        self.x = x\n        self.y = y\n        if kernel is None: # default using linear kernel\n            self.kernel = lambda x1, x2: np.matmul(x1, x2.T)\n        else:\n            self.kernel = kernel\n        self.alpha = np.zeros(y.shape)\n        E = -y\n        epoch = 0\n        while True: #cnt<20:\n            if detailed:\n                epoch += 1\n                print(""Epoch:"", epoch)\n                if kernel is None:\n                    print(""    W      :"", np.sum((self.alpha*y)[..., None]*x, axis=0))\n                else:\n                    print(""    alpha  :"", self.alpha)\n                print(""    b      :"", self.b)\n                print(""    E      :"", E)\n            # check if all conditions are met and if not find alpha1\n            i1 = -1 # the index for alpha1\n            worst = epsilon\n            for i in range(len(y)):\n                if self.alpha[i] == 0:\n                    if y[i]*self.g(x[i, :]) < 1-worst:\n                        worst = 1 - y[i]*self.g(x[i, :])\n                        i1 = i\n                elif self.alpha[i] == C:\n                    if y[i]*self.g(x[i, :]) > 1+epsilon:\n                        worst = y[i]*self.g(x[i, :]) - 1\n                        i1 = i\n                else:\n                    if abs(y[i]*self.g(x[i, :])-1) > epsilon:\n                        worst = abs(y[i]*self.g(x[i, :])-1)\n                        i1 = i\n            if i1 == -1:\n                break\n            # find alpha2\n            # Here to make it easy, I do not save the E value, but calculate it each time\n            i2 = -1\n            max_abs = -1\n            for i in range(len(y)):\n                if i == i1:\n                    continue\n                if abs(E[i] - E[i1]) > max_abs:\n                    i2 = i\n                    max_abs = abs(E[i] - E[i1])\n            # update\n            if y[i1] != y[i2]:\n                L = max(0., self.alpha[i2]-self.alpha[i1])\n                H = min(C, C+self.alpha[i2]-self.alpha[i1])\n            else:\n                L = max(0., self.alpha[i2]+self.alpha[i1]-C)\n                H = min(C, self.alpha[i2]+self.alpha[i1])\n            # update alpha1, alpha2, b\n            eta = self.kernel(x[i1, :], x[i1, :]) + self.kernel(x[i2, :], x[i2, :]) - 2*self.kernel(x[i1, :], x[i2, :])\n            alpha2_new = min(H, max(L, self.alpha[i2] + y[i2]*(E[i1] - E[i2]) / eta))\n            alpha1_new = self.alpha[i1] + y[i1]*y[i2]*(self.alpha[i2]-alpha2_new)\n            b1_new = -E[i1] + y[i1]*self.kernel(x[i1, :], x[i1, :])*(self.alpha[i1] - alpha1_new) + \\\n                              y[i2]*self.kernel(x[i2, :], x[i1, :])*(self.alpha[i2] - alpha2_new) + self.b\n            b2_new = -E[i2] + y[i1]*self.kernel(x[i1, :], x[i2, :])*(self.alpha[i1] - alpha1_new) + \\\n                              y[i2]*self.kernel(x[i2, :], x[i2, :])*(self.alpha[i2] - alpha2_new) + self.b\n            b_new = (b1_new + b2_new)/2\n            self.alpha[i1] = alpha1_new\n            self.alpha[i2] = alpha2_new\n            self.b = b_new\n            # update E\n            for i in range(len(y)):\n                E[i] = self.g(x[i, :]) - y[i]\n            \n    def predict(self, x):\n        if x.ndim == 1:\n                return np.sign(self.g(x))\n        else:\n            ans = []\n            for i in range(x.shape[0]):\n                ans.append(self.predict(x[i, :]))\n            return ans\n        \nif __name__ == \'__main__\':\n    # solution should be 0.5x + 0.5y - 2 = 0\n    x = np.array([[1., 1.], \n                  [4., 3.],\n                  [3., 3.]], np.float32)\n                  \n    y = np.array([-1., 1., 1.], np.float32)\n\n    svm = SVM()\n    svm.fit(x, y, detailed=True)\n    print(svm.predict(x))\n\n    # more complex kernel\n    x = np.array([[0, 0], \n                  [0, 1],\n                  [1, 1],\n                  [1, 0],\n                  [0, 0.5],\n                  [0.5, 0],\n                  [0.5, 1],\n                  [1, 0.5]], np.float32)\n    y = np.array([-1, -1, -1, -1, 1, 1, 1, 1], np.float32)\n    gaussian = lambda x1, x2: np.exp(-LA.norm(x1-x2)/2)\n    svm = SVM()\n    svm.fit(x, y, kernel=gaussian, detailed=True)\n    print(svm.predict(x))'"
np_ml/utils/__init__.py,0,b'from .plot import *\nfrom .transform import *'
np_ml/utils/plot.py,3,"b'import matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport numpy as np\n\ndef plot(x, y, **kwargs):\n    # can only do 2D plot right now\n    assert(x.shape[-1] == 2)\n    color = (y+2) / 5  # TODO: not hard code color\n    if \'accuracy\' in kwargs:\n        accuracy = kwargs[\'accuracy\']\n    plt.figure()\n    plt.scatter(x[:, 0], x[:, 1], c=color)\n    if \'title\' in kwargs:\n        plt.suptitle(kwargs[\'title\'])\n    if \'accuracy\' in kwargs:\n        plt.title(""Accuracy: %.1f%%"" % (kwargs[\'accuracy\']*100), fontsize=10)\n    plt.show()\n    \ndef plot_boundary(model, x, y, **kwargs):\n    assert(x.shape[-1] == 2)\n    cmap_light = ListedColormap([\'#FFAAAA\', \'#AAFFAA\', \'#AAAAFF\'])\n    cmap_bold = ListedColormap([\'#FF0000\', \'#00FF00\', \'#0000FF\'])\n    if \'h\' in kwargs:\n        h = kwargs[\'h\']\n    else:\n        h = 0.1\n    x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n    y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n    x_grid, y_grid = np.meshgrid(np.arange(x_min, x_max, h),\n                                 np.arange(y_min, y_max, h))\n    Z = model.predict(np.c_[x_grid.ravel(), y_grid.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(x_grid.shape)\n    plt.figure()\n    plt.pcolormesh(x_grid, y_grid, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    plt.scatter(x[:, 0], x[:, 1], c=y, cmap=cmap_bold,\n                edgecolor=\'k\', s=20)\n    plt.xlim(x_grid.min(), x_grid.max())\n    plt.ylim(y_grid.min(), y_grid.max())\n    \n    if \'title\' in kwargs:\n        plt.suptitle(kwargs[\'title\'])\n    if \'accuracy\' in kwargs:\n        plt.title(""Accuracy: %.1f%%"" % (kwargs[\'accuracy\']*100), fontsize=10)\n    plt.show()'"
np_ml/utils/transform.py,1,"b'import numpy as np\n\n# PCA\ndef transform(x, dim):\n    e_val, e_vec = np.linalg.eig(np.cov(x, rowvar=False))\n    idx = e_val.argsort()[::-1]\n    e_vec = e_vec[:, idx][:, :dim]\n    return x.dot(e_vec)'"
