file_path,api_count,code
utils.py,0,"b'import matplotlib.gridspec as gridspec\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\n\n\ndef plot(samples, X_dim, channel):\n\tfig = plt.figure(figsize=(4, 4))\n\tgs = gridspec.GridSpec(4, 4)\n\tgs.update(wspace=0.05, hspace=0.05)\n\n\tdim1 = int(np.sqrt(X_dim))\n\n\tsamples = (samples + 1) / 2\n\n\tfor i, sample in enumerate(samples):\n\t\tax = plt.subplot(gs[i])\n\t\tplt.axis(\'off\')\n\t\tax.set_xticklabels([])\n\t\tax.set_yticklabels([])\n\t\tax.set_aspect(\'equal\')\n\n\t\tif channel == 1:\n\t\t\tplt.imshow(sample.reshape([dim1, dim1]), cmap=plt.get_cmap(\'gray\'))\n\t\telse:\n\t\t\tplt.imshow(sample.reshape([dim1, dim1, channel]))\n\n\treturn fig\n\n\n\ndef pixel_range(img):\n\tvmin, vmax = np.min(img), np.max(img)\n\n\tif vmin * vmax >= 0:\n\t\treturn (vmin, vmax)\n\telse:\n\n\t\tif -vmin > vmax:\n\t\t\tvmax = -vmin\n\t\telse:\n\t\t\tvmin = -vmax\n\n\t\treturn (vmin, vmax)\n\n\ndef translate(img, x, y):\n\t""""""\n\tTranslates the given image.\n\n\t:param x: distance to translate in the positive x-direction\n\t:param y: distance to translate in the positive y-direction\n\t:returns: the translated image as an numpy array\n\t""""""\n\tM = np.float32([[1, 0, x], [0, 1, y]])\n\treturn cv2.warpAffine(img.reshape(28,28), M, (28, 28)).reshape(1,784)\n\n\ndef find_roi(img, ksize, coords):\n\t""""""\n\tFinds the feature with the largest relevance scores.\n\n\t:param img: the image to find the feature with the largest relevance score\n\t:param ksize: the size of the sliding window\n\t:param coords: the coordinates to ignore\n\t:returns: the coordinate of the feature with the largest relevance score. If the window size is larger than 1X1, function returns the position of the leftmost pixel.\n\t""""""\n\tsize = np.shape(img)\n\ttemp = np.copy(img)\n\tfor coord in coords:\n\t\ttemp[coord[0]:coord[0]+ksize[0], coord[1]:coord[1]+ksize[1]] = -np.infty\n\n\tr = size[0] - ksize[0] + 1\n\tc = size[1] - ksize[1] + 1\n\tpool = [np.sum(temp[i:i+ksize[0], j:j+ksize[1]]) for i in range(r) for j in range(c)]\n\n\treturn (np.argmax(pool) // c, np.argmax(pool) % c)\n'"
models/grad.py,5,"b'import tensorflow as tf\nimport numpy as np\n\n\nclass Grad:\n    \n    def __init__(self, sess, nodes, img_size, channel):\n        \'\'\'\n        Initialize the Grad class.\n        \n        :param sess: session on which the computation graph is built\n        :param nodes: list containing the name, tensor, or operation for image input and softmax/logit output [input, output]\n        Assumes usage of tf.placeholder_with_default() for other training hyperparameters\n        :param img_size: list indicating the size of image [width, height]\n        :param channel: number of channels of the image. 1 for grayscale and 3 for rgb\n        \'\'\'\n        \n        self.sess = sess\n        \n        self.nodes = []\n        for node in nodes:\n            \n            if type(node) is str:\n                \n                if node[-1] != 0:    \n                    self.nodes.append(self.sess.graph.get_tensor_by_name(node + \':0\'))\n                else:\n                    self.nodes.append(self.sess.graph.get_tensor_by_name(node))\n            \n            elif type(node) is tf.Tensor:\n                self.nodes.append(node)\n            elif type(node) is tf.Operation:\n                self.nodes.append(self.sess.graph.get_tensor_by_name(node.name + \':0\'))\n            else:\n                raise Exception(\'Node must either be type of {}, {}, or {}.\'.format(str, tf.Tensor, tf.Operation))\n        \n        self.img_size = img_size\n        self.channel = channel\n                \n    def reshape(self, imgs):\n        \'\'\'\n        Reshape the given image to fit the shape required by the input tensor.\n        \n        :param imgs: images to reshape\n        :returns: reshaped images\n        \'\'\'\n        \n        shape = self.nodes[0].get_shape().as_list()\n        shape[0] = -1\n        return np.reshape(imgs, shape)\n    \n    def resize(self, imgs):\n        \'\'\'\n        Resizes the given images to take the shape of [num_images, width, height, channel].\n        \n        :param imgs: images to resize\n        :returns: resized images\n        \'\'\'\n        return np.reshape(imgs, [-1, self.img_size[0], self.img_size[1], self.channel])\n    \n    def inference(self, imgs, argmax=True):\n        \'\'\'\n        Performs inference on the given images.\n        \n        :param imgs: images to perform inference on\n        :param argmax: if True, returns the indices of max logits; else, returns the logits themselves\n        :returns: result of inference\n        \'\'\'\n        \n        imgs = self.reshape(imgs)\n        res = self.sess.run(self.nodes[1], feed_dict={self.nodes[0]: imgs})\n        \n        if argmax:\n            return np.argmax(res, axis=1)\n        else:\n            return res\n    \n    def gradient(self, imgs, inds=None):\n        \'\'\'\n        Calculates the gradient of given images with respect to logits.\n        \n        :param imgs: images to calculate gradient of\n        :param inds: expects an array/list of indices of logits to calculate gradients for.\n        If None is given, use max logits from inference\n        :returns: gradients\n        \'\'\'\n        \n        imgs = self.reshape(imgs)\n        \n        if not inds:\n            inds = self.inference(imgs)\n        \n        logs = [[i, inds[i]] for i in range(len(imgs))]\n        res = self.sess.run(tf.gradients(tf.gather_nd(self.nodes[1], logs), self.nodes[0])[0], feed_dict={self.nodes[0]: imgs})\n        \n        return self.resize(res)\n        \n    def smooth_grad(self, imgs, inds=None, noise_level=None, sample_size=None, interval=None):\n        \'\'\'\n        Calculates SmoothGrad of given images.\n        \n        :param imgs: images to calculate SmoothGrad of\n        :param inds: logit indices to calculate SmoothGrad with respect to\n        :param noise_level: standard deviation of random normal noise. Calculated by (max_val - min_val) * noise_level\n        :param sample_size: number of samples to calculate average gradient\n        :param interval: interval to select smooth grad images\n        :returns: SmoothGrad results\n        \'\'\'\n        res = []\n        \n        imgs = self.resize(imgs)\n        \n        if not inds:\n            inds = self.inference(imgs)\n        if not noise_level:\n            noise_level = 0.1\n        if not sample_size:\n            sample_size = 50\n        if not interval:\n            interval = 1\n        \n        for i in range(len(imgs)):\n            img = imgs[None,i]\n            sigma = (np.max(img) - np.min(img)) * noise_level\n            noise_imgs = [img + np.random.normal(scale=sigma, size=np.shape(img)) for i in range(sample_size)]\n            grads = self.gradient(noise_imgs, [inds[i]] * sample_size)\n            \n            temp = [grads[0]]\n            \n            for j in range(sample_size - 1):\n                temp.append(temp[-1] + grads[j + 1])\n            \n            temp = temp[::interval] / np.reshape(np.arange(0,sample_size,interval) + 1, [-1] + [1] * 3)\n            res.append(temp)\n\n        return np.array(res)\n    \n    def integrated_grad(self, imgs, inds=None, steps=None, use_smooth=None, noise_level=None, sample_size=None):\n        \'\'\'\n        Calculates Integrated Gradients of given images.\n        \n        :param imgs: images to calculated Integrated Gradients of\n        :param inds: indices to calculate Integrated Gradients with respect to\n        :param steps: number of steps to perform Riemann Sum\n        :param use_smooth: indicate whether to use SmoothGrad along with Integrated Gradients\n        :param noise_level: standard deviation of random normal noise when using SmoothGrad.\n        Calculated by (max_val - min_val) * noise_level\n        :param sample_size: number of samples to calculate average gradient when using SmoothGrad\n        :returns: Integrated Gradient results\n        \'\'\'\n        res = []\n        \n        imgs = self.resize(imgs)\n        \n        if not inds:\n            inds = self.inference(imgs)\n        if not use_smooth:\n            use_smooth=False\n        if not steps:\n            steps = 50\n        \n        for i in range(len(imgs)):\n            img = imgs[None,i]\n            scaled_imgs = [(float(i) / steps) * img for i in range(1, steps + 1)]\n            \n            if use_smooth:\n                grads = self.smooth_grad(scaled_imgs, [inds[i]] * steps, noise_level, sample_size)[:, -1]\n            else:\n                grads = self.gradient(scaled_imgs, [inds[i]] * steps)\n            \n            res.append(img * np.average(grads, axis=0))\n        \n        return self.resize(res)\n    \n    def truncate(self, attrs, ptile):\n        \'\'\'\n        Truncate given attribute map to indicated percentile.\n        \n        :param attrs: attribute maps\n        :param ptile: percentile\n        :returns: truncated attribute maps\n        \'\'\'\n        res = []\n        \n        attrs = abs(attrs)\n        \n        for i in range(len(attrs)):\n            temp = np.clip(attrs[i] / np.percentile(attrs[i], ptile), 0, 1)      \n            res.append(temp)\n            \n        return np.array(res)\n    \n    def grayscale(self, attrs):\n        \'\'\'\n        Converts RGB attribute maps to grayscale. If channel is 1, nothing happens.\n        \n        :param attrs: attribute maps\n        :returns: grayscale attribute maps\n        \'\'\'\n        \n        if self.channel == 1:\n            return attrs\n        \n        res = []\n        \n        for i in range(len(attrs)):\n            temp = np.average(attrs[i], axis=2)\n            temp = np.transpose([temp] * 3, axes=[1, 2, 0])\n            res.append(temp)\n        \n        return np.array(res)\n    \n    def visualize_attrs(self, imgs, gradient_type=\'gradient\', inds=None, noise_level=None, sample_size=None, interval=None, use_smooth=None, steps=None, ptile=None):\n        \'\'\'\n        Calculates imgs * attribute maps.\n        \n        :param imgs: images\n        :param gradient_type: type of gradient to use.\n        \'gradient\' for vanilla gradient, \'smooth\' for SmoothGrad, and \'integrated\' for Integrated Gradients.\n        :returns: imgs * attribute maps\n        \'\'\'\n        \n        res = []\n        \n        imgs = self.resize(imgs)\n        \n        if not ptile:\n            ptile = 99\n        \n        if gradient_type == \'gradient\':\n            attrs = self.gradient(imgs, inds)\n        elif gradient_type == \'smooth\':\n            attrs = self.smooth_grad(imgs, inds, noise_level, sample_size, interval)\n        elif gradient_type == \'integrated\':\n            attrs = self.integrated_grad(imgs, inds, steps, use_smooth, noise_level, sample_size)\n        else:\n            raise Exception(\'Unknown type of gradient technique. Must either be type of ""gradient"", ""smooth"", or ""integrated"".\')\n        \n        if self.channel == 1:\n            \n            if gradient_type == \'smooth\':\n                orig_shape = np.shape(attrs)\n                attrs = self.resize(attrs)\n                attrs = self.truncate(attrs)\n                attrs = np.reshape(attrs, orig_shape)\n                return np.expand_dims(imgs, axis=1) * self.truncate(attrs, ptile)\n            else:\n                return imgs * self.truncate(attrs, ptile)\n        \n        else:\n            \n            if gradient_type == \'smooth\':\n                orig_shape = np.shape(attrs)\n                attrs = self.resize(attrs)\n                attrs = self.grayscale(attrs)\n                attrs = self.truncate(attrs)\n                attrs = self.reshape(attrs, orig_shape)\n                return np.expand_dims(imgs, axis=1) * self.truncate(attrs, ptile)\n            else:\n                return imgs * self.truncate(attrs, ptile)'"
models/models_1_1.py,5,"b'import tensorflow as tf\n\n\nclass MNIST_DNN:\n\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, X, reuse=False):\n\n        with tf.variable_scope(self.name) as scope:\n\n            if reuse:\n                scope.reuse_variables()\n\n            dense1 = tf.layers.dense(inputs=X, units=512, activation=tf.nn.relu)\n            dense2 = tf.layers.dense(inputs=dense1, units=100, activation=tf.nn.relu)\n            logits = tf.layers.dense(inputs=dense2, units=10)\n\n        return logits\n\n    @property\n    def vars(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n'"
models/models_1_3.py,22,"b""import tensorflow.contrib.layers as tcl\nimport tensorflow as tf\n\n\nclass MNIST_DNN:\n\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, X, reuse=False):\n\n        with tf.variable_scope(self.name) as scope:\n\n            if reuse:\n                scope.reuse_variables()\n\n            dense1 = tf.layers.dense(inputs=X, units=512, activation=tf.nn.relu)\n            dense2 = tf.layers.dense(inputs=dense1, units=100, activation=tf.nn.relu)\n            logits = tf.layers.dense(inputs=dense2, units=10)\n\n        return dense2, logits\n\n    @property\n    def vars(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n\n\nclass MNIST_G(object):\n\n    def __init__(self, z_dim, name):\n        self.z_dim = z_dim\n        self.name = name\n\n    def __call__(self, z, reuse=False):\n\n        with tf.variable_scope(self.name) as scope:\n\n            if reuse:\n                scope.reuse_variables()\n\n            G_W1 = tf.get_variable('G_W1', [self.z_dim, 128], initializer=tcl.xavier_initializer())\n            G_b1 = tf.get_variable('G_b1', [128], initializer=tf.constant_initializer())\n            G_W2 = tf.get_variable('G_W2', [128, 784], initializer=tcl.xavier_initializer())\n            G_b2 = tf.get_variable('G_b2', [784], initializer=tf.constant_initializer())\n\n            layer1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n            layer2 = tf.nn.sigmoid(tf.matmul(layer1, G_W2) + G_b2)\n\n        return layer2\n\n    @property\n    def vars(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n\n\nclass MNIST_D(object):\n\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, X, reuse=False):\n\n        with tf.variable_scope(self.name) as scope:\n\n            if reuse:\n                scope.reuse_variables()\n\n            D_W1 = tf.get_variable('D_W1', [784, 128], initializer=tcl.xavier_initializer())\n            D_b1 = tf.get_variable('D_b1', [128], initializer=tf.constant_initializer())\n            D_W2 = tf.get_variable('D_W2', [128, 1], initializer=tcl.xavier_initializer())\n            D_b2 = tf.get_variable('D_b2', [1], initializer=tf.constant_initializer())\n\n            layer1 = tf.nn.relu(tf.matmul(X, D_W1) + D_b1)\n            layer2 = tf.matmul(layer1, D_W2) + D_b2\n            prediction = tf.nn.sigmoid(layer2)\n\n        return prediction\n\n    @property\n    def vars(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n"""
models/models_2_1.py,37,"b'import tensorflow as tf\n\n\nclass MNIST_DNN:\n\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, X, training, reuse=False):\n\n        with tf.variable_scope(self.name) as scope:\n\n            if reuse:\n                scope.reuse_variables()\n\n            with tf.variable_scope(\'layer1\'):\n                dense1 = tf.layers.dense(inputs=X, units=512, activation=tf.nn.relu)\n                dropout1 = tf.layers.dropout(inputs=dense1, rate=0.7, training=training)\n\n            with tf.variable_scope(\'layer2\'):\n                dense2 = tf.layers.dense(inputs=dropout1, units=512, activation=tf.nn.relu)\n                dropout2 = tf.layers.dropout(inputs=dense2, rate=0.7, training=training)\n\n            with tf.variable_scope(\'layer3\'):\n                dense3 = tf.layers.dense(inputs=dropout2, units=512, activation=tf.nn.relu)\n                dropout3 = tf.layers.dropout(inputs=dense3, rate=0.7, training=training)\n\n            with tf.variable_scope(\'layer4\'):\n                dense4 = tf.layers.dense(inputs=dropout3, units=512, activation=tf.nn.relu)\n                dropout4 = tf.layers.dropout(inputs=dense4, rate=0.7, training=training)\n\n            with tf.variable_scope(\'layer5\'):\n                logits = tf.layers.dense(inputs=dropout4, units=10)\n\n        return logits\n\n    @property\n    def vars(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n\n\nclass MNIST_CNN:\n\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, X, training, reuse=False):\n\n        with tf.variable_scope(self.name) as scope:\n\n            if reuse:\n                scope.reuse_variables()\n\n            X_img = tf.reshape(X, [-1, 28, 28, 1])\n\n            # Convolutional Layer #1 and Pooling Layer #1\n            with tf.variable_scope(\'layer1\'):\n                conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu)\n                pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], padding=""SAME"", strides=2)\n                dropout1 = tf.layers.dropout(inputs=pool1, rate=0.7, training=training)\n\n            # Convolutional Layer #2 and Pooling Layer #2\n            with tf.variable_scope(\'layer2\'):\n                conv2 = tf.layers.conv2d(inputs=dropout1, filters=64, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu)\n                pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], padding=""SAME"", strides=2)\n                dropout2 = tf.layers.dropout(inputs=pool2, rate=0.7, training=training)\n\n            # Convolutional Layer #3 and Pooling Layer #3\n            with tf.variable_scope(\'layer3\'):\n                conv3 = tf.layers.conv2d(inputs=dropout2, filters=128, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu)\n                pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], padding=""SAME"", strides=2)\n                dropout3 = tf.layers.dropout(inputs=pool3, rate=0.7, training=training)\n\n            # Dense Layer with Relu\n            with tf.variable_scope(\'layer4\'):\n                flat = tf.reshape(dropout3, [-1, 128 * 4 * 4])\n                dense4 = tf.layers.dense(inputs=flat, units=625, activation=tf.nn.relu)\n                dropout4 = tf.layers.dropout(inputs=dense4, rate=0.5, training=training)\n\n            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n            with tf.variable_scope(\'layer5\'):\n                logits = tf.layers.dense(inputs=dropout4, units=10)\n\n        return logits\n\n    @property\n    def vars(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n'"
models/models_2_2.py,7,"b'import tensorflow as tf\n\n\nclass MNIST_DNN:\n\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, X, reuse=False):\n\n        with tf.variable_scope(self.name) as scope:\n\n            if reuse:\n                scope.reuse_variables()\n\n            dense1 = tf.layers.dense(inputs=X, units=512, activation=tf.nn.relu, use_bias=False)\n            dense2 = tf.layers.dense(inputs=dense1, units=512, activation=tf.nn.relu, use_bias=False)\n            dense3 = tf.layers.dense(inputs=dense2, units=512, activation=tf.nn.relu, use_bias=False)\n            dense4 = tf.layers.dense(inputs=dense3, units=512, activation=tf.nn.relu, use_bias=False)\n            logits = tf.layers.dense(inputs=dense4, units=10, use_bias=False)\n\n        return logits\n\n    @property\n    def vars(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n'"
models/models_2_3.py,31,"b""from tensorflow.python.ops import nn_ops, gen_nn_ops\nimport tensorflow as tf\n\n\nclass MNIST_NN:\n\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, X, reuse=False):\n\n        with tf.variable_scope(self.name) as scope:\n\n            if reuse:\n                scope.reuse_variables()\n\n            dense1 = tf.layers.dense(inputs=X, units=512, activation=tf.nn.relu, use_bias=True, name='layer1')\n            dense2 = tf.layers.dense(inputs=dense1, units=128, activation=tf.nn.relu, use_bias=True, name='layer2')\n            logits = tf.layers.dense(inputs=dense2, units=10, activation=None, use_bias=True, name='layer3')\n            prediction = tf.nn.softmax(logits)\n\n\n        return [dense1, dense2, prediction], logits\n\n    @property\n    def vars(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n\n\nclass MNIST_DNN:\n\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, X, reuse=False):\n\n        with tf.variable_scope(self.name) as scope:\n\n            if reuse:\n                scope.reuse_variables()\n\n            dense1 = tf.layers.dense(inputs=X, units=512, activation=tf.nn.relu, use_bias=True)\n            dense2 = tf.layers.dense(inputs=dense1, units=512, activation=tf.nn.relu, use_bias=True)\n            dense3 = tf.layers.dense(inputs=dense2, units=512, activation=tf.nn.relu, use_bias=True)\n            dense4 = tf.layers.dense(inputs=dense3, units=512, activation=tf.nn.relu, use_bias=True)\n            logits = tf.layers.dense(inputs=dense4, units=10, activation=None, use_bias=True)\n            prediction = tf.nn.softmax(logits)\n\n        return [dense1, dense2, dense3, dense4, prediction], logits\n\n    @property\n    def vars(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n\n\nclass LRP:\n\n    def __init__(self, alpha, activations, weights, biases, conv_ksize, pool_ksize, conv_strides, pool_strides, name):\n        self.alpha = alpha\n        self.activations = activations\n        self.weights = weights\n        self.biases = biases\n        self.conv_ksize = conv_ksize\n        self.pool_ksize = pool_ksize\n        self.conv_strides = conv_strides\n        self.pool_strides = pool_strides\n        self.name = name\n\n    def __call__(self, logit):\n\n        with tf.name_scope(self.name):\n            Rs = []\n            j = 0\n\n            for i in range(len(self.activations) - 1):\n\n                if i is 0:\n                    Rs.append(self.activations[i][:,logit,None])\n                    Rs.append(self.backprop_dense(self.activations[i + 1], self.weights[j][:,logit,None], self.biases[j][logit,None], Rs[-1]))\n                    j += 1\n\n                    continue\n\n                elif 'dense' in self.activations[i].name.lower():\n                    Rs.append(self.backprop_dense(self.activations[i + 1], self.weights[j], self.biases[j], Rs[-1]))\n                    j += 1\n                elif 'reshape' in self.activations[i].name.lower():\n                    shape = self.activations[i + 1].get_shape().as_list()\n                    shape[0] = -1\n                    Rs.append(tf.reshape(Rs[-1], shape))\n                elif 'conv' in self.activations[i].name.lower():\n                    Rs.append(self.backprop_conv(self.activations[i + 1], self.weights[j], self.biases[j], Rs[-1], self.conv_strides))\n                    j += 1\n                elif 'pooling' in self.activations[i].name.lower():\n                    if 'max' in self.activations[i].name.lower():\n                        pooling_type = 'max'\n                    else:\n                        pooling_type = 'avg'\n                    Rs.append(self.backprop_pool(self.activations[i + 1], Rs[-1], self.pool_ksize, self.pool_strides, pooling_type))\n                else:\n                    raise Error('Unknown operation.')\n\n            return Rs[-1]\n\n    def backprop_conv(self, activation, kernel, bias, relevance, strides, padding='SAME'):\n        W_p = tf.maximum(0., kernel)\n        b_p = tf.maximum(0., bias)\n        z_p = nn_ops.conv2d(activation, W_p, strides, padding) + b_p\n        s_p = relevance / z_p\n        c_p = nn_ops.conv2d_backprop_input(tf.shape(activation), W_p, s_p, strides, padding)\n\n        W_n = tf.minimum(0., kernel)\n        b_n = tf.minimum(0., bias)\n        z_n = nn_ops.conv2d(activation, W_n, strides, padding) + b_n\n        s_n = relevance / z_n\n        c_n = nn_ops.conv2d_backprop_input(tf.shape(activation), W_n, s_n, strides, padding)\n\n        return activation * (self.alpha * c_p + (1 - self.alpha) * c_n)\n\n    def backprop_pool(self, activation, relevance, ksize, strides, pooling_type, padding='SAME'):\n\n        if pooling_type.lower() is 'avg':\n            z = nn_ops.avg_pool(activation, ksize, strides, padding) + 1e-10\n            s = relevance / z\n            c = gen_nn_ops._avg_pool_grad(tf.shape(activation), s, ksize, strides, padding)\n            return activation * c\n        else:\n            z = nn_ops.max_pool(activation, ksize, strides, padding) + 1e-10\n            s = relevance / z\n            c = gen_nn_ops._max_pool_grad(activation, z, s, ksize, strides, padding)\n            return activation * c\n\n    def backprop_dense(self, activation, kernel, bias, relevance):\n        W_p = tf.maximum(0., kernel)\n        b_p = tf.maximum(0., bias)\n        z_p = tf.matmul(activation, W_p) + b_p\n        s_p = relevance / z_p\n        c_p = tf.matmul(s_p, tf.transpose(W_p))\n\n        W_n = tf.minimum(0., kernel)\n        b_n = tf.minimum(0., bias)\n        z_n = tf.matmul(activation, W_n) + b_n\n        s_n = relevance / z_n\n        c_n = tf.matmul(s_n, tf.transpose(W_n))\n\n        return activation * (self.alpha * c_p + (1 - self.alpha) * c_n)\n"""
models/models_2_4.py,44,"b'from tensorflow.python.ops import nn_ops, gen_nn_ops\nimport tensorflow as tf\n\n\nclass MNIST_CNN:\n\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, X, reuse=False):\n\n        with tf.variable_scope(self.name) as scope:\n\n            if reuse:\n                scope.reuse_variables()\n\n            with tf.variable_scope(\'layer0\'):\n                X_img = tf.reshape(X, [-1, 28, 28, 1])\n\n            # Convolutional Layer #1 and Pooling Layer #1\n            with tf.variable_scope(\'layer1\'):\n                conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=False)\n                pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], padding=""SAME"", strides=2)\n\n            # Convolutional Layer #2 and Pooling Layer #2\n            with tf.variable_scope(\'layer2\'):\n                conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=False)\n                pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], padding=""SAME"", strides=2)\n\n            # Convolutional Layer #3 and Pooling Layer #3\n            with tf.variable_scope(\'layer3\'):\n                conv3 = tf.layers.conv2d(inputs=pool2, filters=128, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=False)\n                pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], padding=""SAME"", strides=2)\n\n            # Dense Layer with Relu\n            with tf.variable_scope(\'layer4\'):\n                flat = tf.reshape(pool3, [-1, 128 * 4 * 4])\n                dense4 = tf.layers.dense(inputs=flat, units=625, activation=tf.nn.relu, use_bias=False)\n\n            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n            with tf.variable_scope(\'layer5\'):\n                logits = tf.layers.dense(inputs=dense4, units=10, use_bias=False)\n                prediction = tf.nn.softmax(logits)\n\n        return [X_img, conv1, pool1, conv2, pool2, conv3, pool3, flat, dense4, prediction], logits\n\n    @property\n    def vars(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n\n\nclass Taylor:\n\n    def __init__(self, activations, weights, conv_ksize, pool_ksize, conv_strides, pool_strides, name):\n\n        self.last_ind = len(activations)\n        for op in activations[::-1]:\n            self.last_ind -= 1\n            if any([word in op.name for word in [\'conv\', \'pooling\', \'dense\']]):\n                break\n\n        self.activations = activations\n        self.weights = weights\n        self.conv_ksize = conv_ksize\n        self.pool_ksize = pool_ksize\n        self.conv_strides = conv_strides\n        self.pool_strides = pool_strides\n        self.name = name\n\n    def __call__(self, logit):\n\n        with tf.name_scope(self.name):\n            Rs = []\n            j = 0\n\n            for i in range(len(self.activations) - 1):\n\n                if i is self.last_ind:\n\n                    if \'conv\' in self.activations[i].name.lower():\n                        Rs.append(self.backprop_conv_input(self.activations[i + 1], self.weights[j], Rs[-1], self.conv_strides))\n                    else:\n                        Rs.append(self.backprop_dense_input(self.activations[i + 1], self.weights[j], Rs[-1]))\n\n                    continue\n\n                if i is 0:\n                    Rs.append(self.activations[i][:,logit,None])\n                    Rs.append(self.backprop_dense(self.activations[i + 1], self.weights[j][:,logit,None], Rs[-1]))\n                    j += 1\n\n                    continue\n\n                elif \'dense\' in self.activations[i].name.lower():\n                    Rs.append(self.backprop_dense(self.activations[i + 1], self.weights[j], Rs[-1]))\n                    j += 1\n                elif \'reshape\' in self.activations[i].name.lower():\n                    shape = self.activations[i + 1].get_shape().as_list()\n                    shape[0] = -1\n                    Rs.append(tf.reshape(Rs[-1], shape))\n                elif \'conv\' in self.activations[i].name.lower():\n                    Rs.append(self.backprop_conv(self.activations[i + 1], self.weights[j], Rs[-1], self.conv_strides))\n                    j += 1\n                elif \'pooling\' in self.activations[i].name.lower():\n\n                    # Apply average pooling backprop regardless of type of pooling layer used, following recommendations by Montavon et al.\n                    # Uncomment code below if you want to apply the winner-take-all redistribution policy suggested by Bach et al.\n                    #\n                    # if \'max\' in self.activations[i].name.lower():\n                    #     pooling_type = \'max\'\n                    # else:\n                    #     pooling_type = \'avg\'\n                    # Rs.append(self.backprop_pool(self.activations[i + 1], Rs[-1], self.pool_ksize, self.pool_strides, pooling_type))\n\n                    Rs.append(self.backprop_pool(self.activations[i + 1], Rs[-1], self.pool_ksize, self.pool_strides, \'avg\'))\n                else:\n                    raise Error(\'Unknown operation.\')\n\n            return Rs[-1]\n\n    def backprop_conv(self, activation, kernel, relevance, strides, padding=\'SAME\'):\n        W_p = tf.maximum(0., kernel)\n        z = nn_ops.conv2d(activation, W_p, strides, padding) + 1e-10\n        s = relevance / z\n        c = nn_ops.conv2d_backprop_input(tf.shape(activation), W_p, s, strides, padding)\n        return activation * c\n\n    def backprop_pool(self, activation, relevance, ksize, strides, pooling_type, padding=\'SAME\'):\n\n        if pooling_type.lower() in \'avg\':\n            z = nn_ops.avg_pool(activation, ksize, strides, padding) + 1e-10\n            s = relevance / z\n            c = gen_nn_ops._avg_pool_grad(tf.shape(activation), s, ksize, strides, padding)\n            return activation * c\n        else:\n            z = nn_ops.max_pool(activation, ksize, strides, padding) + 1e-10\n            s = relevance / z\n            c = gen_nn_ops._max_pool_grad(activation, z, s, ksize, strides, padding)\n            return activation * c\n\n    def backprop_dense(self, activation, kernel, relevance):\n        W_p = tf.maximum(0., kernel)\n        z = tf.matmul(activation, W_p) + 1e-10\n        s = relevance / z\n        c = tf.matmul(s, tf.transpose(W_p))\n        return activation * c\n\n    def backprop_conv_input(self, X, kernel, relevance, strides, padding=\'SAME\', lowest=0., highest=1.):\n        W_p = tf.maximum(0., kernel)\n        W_n = tf.minimum(0., kernel)\n\n        L = tf.ones_like(X, tf.float32) * lowest\n        H = tf.ones_like(X, tf.float32) * highest\n\n        z_o = nn_ops.conv2d(X, kernel, strides, padding)\n        z_p = nn_ops.conv2d(L, W_p, strides, padding)\n        z_n = nn_ops.conv2d(H, W_n, strides, padding)\n\n        z = z_o - z_p - z_n + 1e-10\n        s = relevance / z\n\n        c_o = nn_ops.conv2d_backprop_input(tf.shape(X), kernel, s, strides, padding)\n        c_p = nn_ops.conv2d_backprop_input(tf.shape(X), W_p, s, strides, padding)\n        c_n = nn_ops.conv2d_backprop_input(tf.shape(X), W_n, s, strides, padding)\n\n        return X * c_o - L * c_p - H * c_n\n\n    def backprop_dense_input(self, X, kernel, relevance, lowest=0., highest=1.):\n        W_p = tf.maximum(0., kernel)\n        W_n = tf.minimum(0., kernel)\n\n        L = tf.ones_like(X, tf.float32) * lowest\n        H = tf.ones_like(X, tf.float32) * highest\n\n        z_o = tf.matmul(X, kernel)\n        z_p = tf.matmul(L, W_p)\n        z_n = tf.matmul(H, W_n)\n\n        z = z_o - z_p - z_n + 1e-10\n        s = relevance / z\n\n        c_o = tf.matmul(s, tf.transpose(kernel))\n        c_p = tf.matmul(s, tf.transpose(W_p))\n        c_n = tf.matmul(s, tf.transpose(W_n))\n\n        return X * c_o - L * c_p - H * c_n\n'"
models/models_2_5.py,18,"b'from tensorflow.python.ops import nn_ops, gen_nn_ops\nimport tensorflow as tf\n\nclass MNIST_CNN:\n\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, X, training, reuse=False):\n\n        with tf.variable_scope(self.name) as scope:\n\n            if reuse:\n                scope.reuse_variables()\n\n            with tf.variable_scope(\'layer0\'):\n                X_img = tf.reshape(X, [-1, 28, 28, 1])\n\n            # Convolutional Layer #1 and Pooling Layer #1\n            with tf.variable_scope(\'layer1\'):\n                conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[4, 4], strides=[2, 2], padding=""SAME"", activation=None, use_bias=True)\n                relu1 = tf.nn.relu(conv1)\n\n            # Convolutional Layer #2 and Pooling Layer #2\n            with tf.variable_scope(\'layer2\'):\n                conv2 = tf.layers.conv2d(inputs=relu1, filters=64, kernel_size=[4, 4], strides=[2, 2], padding=""SAME"", activation=None, use_bias=True)\n                relu2 = tf.nn.relu(conv2)\n                drop2 = tf.layers.dropout(inputs=relu2, rate=0.25, training=training)\n                flat = tf.reshape(drop2, [-1, 7 * 7 * 64])\n\n            # Convolutional Layer #3 and Pooling Layer #3\n            with tf.variable_scope(\'layer3\'):\n                dense3 = tf.layers.dense(inputs=flat, units=128, activation=None, use_bias=True)\n                relu3 = tf.nn.relu(dense3)\n                drop3 = tf.layers.dropout(inputs=relu3, rate=0.5, training=training)\n\n            # Dense Layer with Relu\n            with tf.variable_scope(\'layer4\'):\n                logits = tf.layers.dense(inputs=drop3, units=10, activation=None, use_bias=True)\n\n        return [X_img, conv1, relu1, conv2, relu2, dense3, relu3, logits]\n\n    @property\n    def vars(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n'"
models/models_3_1.py,19,"b'import tensorflow as tf\n\n\nclass MNIST_CNN:\n\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, X, reuse=False):\n\n        with tf.variable_scope(self.name) as scope:\n\n            if reuse:\n                scope.reuse_variables()\n\n            with tf.variable_scope(\'layer0\'):\n                X_img = tf.reshape(X, [-1, 28, 28, 1])\n\n            # Convolutional Layer #1 and Pooling Layer #1\n            with tf.variable_scope(\'layer1\'):\n                conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=False)\n                pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], padding=""SAME"", strides=2)\n\n            # Convolutional Layer #2 and Pooling Layer #2\n            with tf.variable_scope(\'layer2\'):\n                conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=False)\n                pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], padding=""SAME"", strides=2)\n\n            # Convolutional Layer #3 and Pooling Layer #3\n            with tf.variable_scope(\'layer3\'):\n                conv3 = tf.layers.conv2d(inputs=pool2, filters=128, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=False)\n                pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], padding=""SAME"", strides=2)\n\n            # Dense Layer with Relu\n            with tf.variable_scope(\'layer4\'):\n                flat = tf.reshape(pool3, [-1, 128 * 4 * 4])\n                dense4 = tf.layers.dense(inputs=flat, units=625, activation=tf.nn.relu, use_bias=False)\n\n            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n            with tf.variable_scope(\'layer5\'):\n                logits = tf.layers.dense(inputs=dense4, units=10, use_bias=False)\n                prediction = tf.nn.softmax(logits)\n\n        return [X_img, conv1, pool1, conv2, pool2, conv3, pool3, flat, dense4, prediction], logits\n\n    @property\n    def vars(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n'"
models/models_3_2.py,19,"b'import tensorflow as tf\n\n\nclass MNIST_CNN:\n\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, X, reuse=False):\n\n        with tf.variable_scope(self.name) as scope:\n\n            if reuse:\n                scope.reuse_variables()\n\n            with tf.variable_scope(\'layer0\'):\n                X_img = tf.reshape(X, [-1, 28, 28, 1])\n\n            # Convolutional Layer #1 and Pooling Layer #1\n            with tf.variable_scope(\'layer1\'):\n                conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=True)\n                pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], padding=""SAME"", strides=2)\n\n            # Convolutional Layer #2 and Pooling Layer #2\n            with tf.variable_scope(\'layer2\'):\n                conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=True)\n                pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], padding=""SAME"", strides=2)\n\n            # Convolutional Layer #3 and Pooling Layer #3\n            with tf.variable_scope(\'layer3\'):\n                conv3 = tf.layers.conv2d(inputs=pool2, filters=128, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=True)\n                pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], padding=""SAME"", strides=2)\n\n            # Dense Layer with Relu\n            with tf.variable_scope(\'layer4\'):\n                flat = tf.reshape(pool3, [-1, 128 * 4 * 4])\n                dense4 = tf.layers.dense(inputs=flat, units=625, activation=tf.nn.relu, use_bias=True)\n\n            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n            with tf.variable_scope(\'layer5\'):\n                logits = tf.layers.dense(inputs=dense4, units=10, use_bias=True)\n                prediction = tf.nn.softmax(logits)\n\n        return [X_img, conv1, pool1, conv2, pool2, conv3, pool3, flat, dense4, prediction], logits\n\n    @property\n    def vars(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n'"
models/models_3_3.py,18,"b'import tensorflow as tf\n\n\nclass MNIST_CNN:\n\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, X, reuse=False):\n\n        with tf.variable_scope(self.name) as scope:\n\n            if reuse:\n                scope.reuse_variables()\n\n            with tf.variable_scope(\'layer0\'):\n                X_img = tf.reshape(X, [-1, 28, 28, 1])\n\n            # Convolutional Layer #1 and Pooling Layer #1\n            with tf.variable_scope(\'layer1\'):\n                conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3], strides=1, padding=""SAME"", activation=tf.nn.relu, use_bias=True)\n                pool1 = tf.layers.conv2d(inputs=conv1, filters=32, kernel_size=[2, 2], strides=2, padding=""SAME"", activation=tf.nn.relu, use_bias=True)\n\n            # Convolutional Layer #2 and Pooling Layer #2\n            with tf.variable_scope(\'layer2\'):\n                conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=[3, 3], strides=1, padding=""SAME"", activation=tf.nn.relu, use_bias=True)\n                pool2 = tf.layers.conv2d(inputs=conv2, filters=64, kernel_size=[2, 2], strides=2, padding=""SAME"", activation=tf.nn.relu, use_bias=True)\n\n            # Convolutional Layer #3 and Pooling Layer #3\n            with tf.variable_scope(\'layer3\'):\n                conv3 = tf.layers.conv2d(inputs=pool2, filters=128, kernel_size=[3, 3], strides=1, padding=""SAME"", activation=tf.nn.relu, use_bias=True)\n\n            # Logits (no activation) Layer\n            with tf.variable_scope(\'layer4\'):\n                dense = tf.layers.conv2d(inputs=conv3, filters=128, kernel_size=[1, 1], strides=1, padding=""SAME"", activation=tf.nn.relu, use_bias=True)\n                pool4 = tf.layers.conv2d(inputs=dense, filters=10, kernel_size=[1, 1], strides=1, padding=""SAME"", activation=tf.nn.relu, use_bias=True)\n                global_avg = tf.layers.average_pooling2d(inputs=pool4, pool_size=[7, 7], strides=1, padding=""VALID"")\n                logits = tf.reshape(global_avg, [-1, 10])\n                prediction = tf.nn.softmax(logits)\n\n        return [X_img, conv1, pool1, conv2, pool2, conv3, dense, pool4, prediction], logits\n\n    @property\n    def vars(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n'"
models/models_3_4.py,19,"b'import tensorflow as tf\n\n\nclass MNIST_CNN:\n\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, X, reuse=False):\n\n        with tf.variable_scope(self.name) as scope:\n\n            if reuse:\n                scope.reuse_variables()\n\n            with tf.variable_scope(\'layer0\'):\n                X_img = tf.reshape(X, [-1, 28, 28, 1])\n\n            # Convolutional Layer #1 and Pooling Layer #1\n            with tf.variable_scope(\'layer1\'):\n                conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=True)\n                pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], padding=""SAME"", strides=2)\n\n            # Convolutional Layer #2 and Pooling Layer #2\n            with tf.variable_scope(\'layer2\'):\n                conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=True)\n                pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], padding=""SAME"", strides=2)\n\n            # Convolutional Layer #3 and Pooling Layer #3\n            with tf.variable_scope(\'layer3\'):\n                conv3 = tf.layers.conv2d(inputs=pool2, filters=128, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=True)\n                pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], padding=""SAME"", strides=2)\n\n            # Dense Layer with Relu\n            with tf.variable_scope(\'layer4\'):\n                flat = tf.reshape(pool3, [-1, 128 * 4 * 4])\n                dense4 = tf.layers.dense(inputs=flat, units=625, activation=tf.nn.relu, use_bias=True)\n\n            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n            with tf.variable_scope(\'layer5\'):\n                logits = tf.layers.dense(inputs=dense4, units=10, use_bias=True)\n                prediction = tf.nn.softmax(logits)\n\n        return [X_img, conv1, pool1, conv2, pool2, conv3, pool3, flat, dense4, prediction], logits\n\n    @property\n    def vars(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n'"
models/models_3_5.py,18,"b'from tensorflow.python.ops import nn_ops, gen_nn_ops\nimport tensorflow as tf\n\nclass MNIST_CNN:\n\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, X, training, reuse=False):\n\n        with tf.variable_scope(self.name) as scope:\n\n            if reuse:\n                scope.reuse_variables()\n\n            with tf.variable_scope(\'layer0\'):\n                X_img = tf.reshape(X, [-1, 28, 28, 1])\n\n            # Convolutional Layer #1 and Pooling Layer #1\n            with tf.variable_scope(\'layer1\'):\n                conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[4, 4], strides=[2, 2], padding=""SAME"", activation=None, use_bias=True)\n                relu1 = tf.nn.relu(conv1)\n\n            # Convolutional Layer #2 and Pooling Layer #2\n            with tf.variable_scope(\'layer2\'):\n                conv2 = tf.layers.conv2d(inputs=relu1, filters=64, kernel_size=[4, 4], strides=[2, 2], padding=""SAME"", activation=None, use_bias=True)\n                relu2 = tf.nn.relu(conv2)\n                drop2 = tf.layers.dropout(inputs=relu2, rate=0.25, training=training)\n                flat = tf.reshape(drop2, [-1, 7 * 7 * 64])\n\n            # Convolutional Layer #3 and Pooling Layer #3\n            with tf.variable_scope(\'layer3\'):\n                dense3 = tf.layers.dense(inputs=flat, units=128, activation=None, use_bias=True)\n                relu3 = tf.nn.relu(dense3)\n                drop3 = tf.layers.dropout(inputs=relu3, rate=0.5, training=training)\n\n            # Dense Layer with Relu\n            with tf.variable_scope(\'layer4\'):\n                logits = tf.layers.dense(inputs=drop3, units=10, activation=None, use_bias=True)\n\n        return [X_img, conv1, relu1, conv2, relu2, dense3, relu3, logits]\n\n    @property\n    def vars(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n'"
models/models_4_1.py,17,"b'import tensorflow as tf\n\n\nclass MNIST_CNN:\n\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, X, reuse=False):\n\n        with tf.variable_scope(self.name) as scope:\n\n            if reuse:\n                scope.reuse_variables()\n\n            with tf.variable_scope(\'layer0\'):\n                X_img = tf.reshape(X, [-1, 40, 40, 1])\n\n            # Convolutional Layer #1 and Pooling Layer #1\n            with tf.variable_scope(\'layer1\'):\n                conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=True)\n                pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], padding=""SAME"", strides=2)\n\n            # Convolutional Layer #2 and Pooling Layer #2\n            with tf.variable_scope(\'layer2\'):\n                conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=True)\n                pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], padding=""SAME"", strides=2)\n\n            # Convolutional Layer #3 and Pooling Layer #3\n            with tf.variable_scope(\'layer3\'):\n                conv3 = tf.layers.conv2d(inputs=pool2, filters=128, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=True)\n                pool3 = tf.layers.average_pooling2d(inputs=conv3, pool_size=[10, 10], strides=1)\n                flat = tf.reshape(pool3, [-1, 128])\n\n            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n            with tf.variable_scope(\'layer4\'):\n                logits = tf.layers.dense(inputs=flat, units=10, use_bias=False)\n                prediction = tf.nn.softmax(logits)\n\n        return [X_img, conv1, pool1, conv2, pool2, conv3, pool3, flat, prediction], logits\n\n    @property\n    def vars(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n'"
models/models_4_2.py,18,"b'from tensorflow.python.ops import nn_ops, gen_nn_ops\nimport tensorflow as tf\n\n\nclass MNIST_CNN:\n\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, X, reuse=False):\n\n        with tf.variable_scope(self.name) as scope:\n\n            if reuse:\n                scope.reuse_variables()\n\n            with tf.variable_scope(\'layer0\'):\n                X_img = tf.reshape(X, [-1, 40, 40, 1])\n\n            # Convolutional Layer #1 and Pooling Layer #1\n            with tf.variable_scope(\'layer1\'):\n                conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=False)\n                pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], padding=""SAME"", strides=2)\n\n            # Convolutional Layer #2 and Pooling Layer #2\n            with tf.variable_scope(\'layer2\'):\n                conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=False)\n                pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], padding=""SAME"", strides=2)\n\n            # Convolutional Layer #3 and Pooling Layer #3\n            with tf.variable_scope(\'layer3\'):\n                conv3 = tf.layers.conv2d(inputs=pool2, filters=128, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=False)\n\n            # Dense Layer with Relu\n            with tf.variable_scope(\'layer4\'):\n                flat = tf.reshape(conv3, [-1, 128 * 10 * 10])\n                dense4 = tf.layers.dense(inputs=flat, units=625, activation=tf.nn.relu, use_bias=False)\n\n            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n            with tf.variable_scope(\'layer5\'):\n                logits = tf.layers.dense(inputs=dense4, units=10, activation=None, use_bias=False)\n                prediction = tf.nn.relu(logits)\n\n        return [X_img, conv1, pool1, conv2, pool2, conv3, flat, dense4, prediction], logits\n\n    @property\n    def vars(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n'"
models/models_4_3.py,18,"b'from tensorflow.python.ops import nn_ops, gen_nn_ops\nimport tensorflow as tf\n\n\nclass MNIST_CNN:\n\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, X, reuse=False):\n\n        with tf.variable_scope(self.name) as scope:\n\n            if reuse:\n                scope.reuse_variables()\n\n            with tf.variable_scope(\'layer0\'):\n                X_img = tf.reshape(X, [-1, 40, 40, 1])\n\n            # Convolutional Layer #1 and Pooling Layer #1\n            with tf.variable_scope(\'layer1\'):\n                conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=False)\n                pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], padding=""SAME"", strides=2)\n\n            # Convolutional Layer #2 and Pooling Layer #2\n            with tf.variable_scope(\'layer2\'):\n                conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=False)\n                pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], padding=""SAME"", strides=2)\n\n            # Convolutional Layer #3 and Pooling Layer #3\n            with tf.variable_scope(\'layer3\'):\n                conv3 = tf.layers.conv2d(inputs=pool2, filters=128, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=False)\n\n            # Dense Layer with Relu\n            with tf.variable_scope(\'layer4\'):\n                flat = tf.reshape(conv3, [-1, 128 * 10 * 10])\n                dense4 = tf.layers.dense(inputs=flat, units=625, activation=tf.nn.relu, use_bias=False)\n\n            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n            with tf.variable_scope(\'layer5\'):\n                logits = tf.layers.dense(inputs=dense4, units=10, activation=None, use_bias=False)\n                prediction = tf.nn.relu(logits)\n\n        return [X_img, conv1, pool1, conv2, pool2, conv3, flat, dense4, prediction], logits\n\n    @property\n    def vars(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n'"
models/models_5_1.py,52,"b'from tensorflow.python.ops import nn_ops, gen_nn_ops\nimport tensorflow as tf\n\n\nclass MNIST_DNN:\n\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, X, reuse=False):\n\n        with tf.variable_scope(self.name) as scope:\n\n            if reuse:\n                scope.reuse_variables()\n\n            dense1 = tf.layers.dense(inputs=X, units=512, activation=tf.nn.relu, use_bias=False)\n            dense2 = tf.layers.dense(inputs=dense1, units=512, activation=tf.nn.relu, use_bias=False)\n            dense3 = tf.layers.dense(inputs=dense2, units=512, activation=tf.nn.relu, use_bias=False)\n            dense4 = tf.layers.dense(inputs=dense3, units=512, activation=tf.nn.relu, use_bias=False)\n            logits = tf.layers.dense(inputs=dense4, units=10, activation=None, use_bias=False)\n            prediction = tf.nn.softmax(logits)\n\n        return [dense1, dense2, dense3, dense4, prediction], logits\n\n    @property\n    def vars(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n\n\nclass MNIST_CNN:\n\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, X, reuse=False):\n\n        with tf.variable_scope(self.name) as scope:\n\n            if reuse:\n                scope.reuse_variables()\n\n            with tf.variable_scope(\'layer0\'):\n                X_img = tf.reshape(X, [-1, 28, 28, 1])\n\n            # Convolutional Layer #1 and Pooling Layer #1\n            with tf.variable_scope(\'layer1\'):\n                conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=False)\n                pool1 = tf.layers.average_pooling2d(inputs=conv1, pool_size=[2, 2], padding=""SAME"", strides=2)\n\n            # Convolutional Layer #2 and Pooling Layer #2\n            with tf.variable_scope(\'layer2\'):\n                conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=False)\n                pool2 = tf.layers.average_pooling2d(inputs=conv2, pool_size=[2, 2], padding=""SAME"", strides=2)\n\n            # Convolutional Layer #3 and Pooling Layer #3\n            with tf.variable_scope(\'layer3\'):\n                conv3 = tf.layers.conv2d(inputs=pool2, filters=128, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=False)\n                pool3 = tf.layers.average_pooling2d(inputs=conv3, pool_size=[2, 2], padding=""SAME"", strides=2)\n\n            # Dense Layer with Relu\n            with tf.variable_scope(\'layer4\'):\n                flat = tf.reshape(pool3, [-1, 128 * 4 * 4])\n                dense4 = tf.layers.dense(inputs=flat, units=625, activation=tf.nn.relu, use_bias=False)\n\n            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n            with tf.variable_scope(\'layer5\'):\n                logits = tf.layers.dense(inputs=dense4, units=10, use_bias=False)\n                prediction = tf.nn.relu(logits)\n\n        return [X_img, conv1, pool1, conv2, pool2, conv3, pool3, flat, dense4, prediction], logits\n\n    @property\n    def vars(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n\n\nclass Taylor:\n\n    def __init__(self, activations, weights, conv_ksize, pool_ksize, conv_strides, pool_strides, name):\n\n        self.last_ind = len(activations)\n        for op in activations:\n            self.last_ind -= 1\n            if any([word in op.name for word in [\'conv\', \'pooling\', \'dense\']]):\n                break\n\n        self.activations = activations\n        self.weights = weights\n        self.conv_ksize = conv_ksize\n        self.pool_ksize = pool_ksize\n        self.conv_strides = conv_strides\n        self.pool_strides = pool_strides\n        self.name = name\n\n    def __call__(self, logit):\n\n        with tf.name_scope(self.name):\n            Rs = []\n            j = 0\n\n            for i in range(len(self.activations) - 1):\n\n                if i is self.last_ind:\n\n                    if \'conv\' in self.activations[i].name.lower():\n                        Rs.append(self.backprop_conv_input(self.activations[i + 1], self.weights[j], Rs[-1], self.conv_strides))\n                    else:\n                        Rs.append(self.backprop_dense_input(self.activations[i + 1], self.weights[j], Rs[-1]))\n\n                    continue\n\n                if i is 0:\n                    Rs.append(self.activations[i][:,logit,None])\n                    Rs.append(self.backprop_dense(self.activations[i + 1], self.weights[j][:,logit,None], Rs[-1]))\n                    j += 1\n\n                    continue\n\n                elif \'dense\' in self.activations[i].name.lower():\n                    Rs.append(self.backprop_dense(self.activations[i + 1], self.weights[j], Rs[-1]))\n                    j += 1\n                elif \'reshape\' in self.activations[i].name.lower():\n                    shape = self.activations[i + 1].get_shape().as_list()\n                    shape[0] = -1\n                    Rs.append(tf.reshape(Rs[-1], shape))\n                elif \'conv\' in self.activations[i].name.lower():\n                    Rs.append(self.backprop_conv(self.activations[i + 1], self.weights[j], Rs[-1], self.conv_strides))\n                    j += 1\n                elif \'pooling\' in self.activations[i].name.lower():\n\n                    # Apply average pooling backprop regardless of type of pooling layer used, following recommendations by Montavon et al.\n                    # Uncomment code below if you want to apply the winner-take-all redistribution policy suggested by Bach et al.\n                    #\n                    # if \'max\' in self.activations[i].name.lower():\n                    #     pooling_type = \'max\'\n                    # else:\n                    #     pooling_type = \'avg\'\n                    # Rs.append(self.backprop_pool(self.activations[i + 1], Rs[-1], self.pool_ksize, self.pool_strides, pooling_type))\n\n                    Rs.append(self.backprop_pool(self.activations[i + 1], Rs[-1], self.pool_ksize, self.pool_strides, \'avg\'))\n                else:\n                    raise Error(\'Unknown operation.\')\n\n            return Rs[-1]\n\n    def backprop_conv(self, activation, kernel, relevance, strides, padding=\'SAME\'):\n        W_p = tf.maximum(0., kernel)\n        z = nn_ops.conv2d(activation, W_p, strides, padding) + 1e-10\n        s = relevance / z\n        c = nn_ops.conv2d_backprop_input(tf.shape(activation), W_p, s, strides, padding)\n        return activation * c\n\n    def backprop_pool(self, activation, relevance, ksize, strides, pooling_type, padding=\'SAME\'):\n\n        if pooling_type.lower() in \'avg\':\n            z = nn_ops.avg_pool(activation, ksize, strides, padding) + 1e-10\n            s = relevance / z\n            c = gen_nn_ops._avg_pool_grad(tf.shape(activation), s, ksize, strides, padding)\n            return activation * c\n        else:\n            z = nn_ops.max_pool(activation, ksize, strides, padding) + 1e-10\n            s = relevance / z\n            c = gen_nn_ops._max_pool_grad(activation, z, s, ksize, strides, padding)\n            return activation * c\n\n    def backprop_dense(self, activation, kernel, relevance):\n        W_p = tf.maximum(0., kernel)\n        z = tf.matmul(activation, W_p) + 1e-10\n        s = relevance / z\n        c = tf.matmul(s, tf.transpose(W_p))\n        return activation * c\n\n    def backprop_conv_input(self, X, kernel, relevance, strides, padding=\'SAME\', lowest=0., highest=1.):\n        W_p = tf.maximum(0., kernel)\n        W_n = tf.minimum(0., kernel)\n\n        L = tf.ones_like(X, tf.float32) * lowest\n        H = tf.ones_like(X, tf.float32) * highest\n\n        z_o = nn_ops.conv2d(X, kernel, strides, padding)\n        z_p = nn_ops.conv2d(L, W_p, strides, padding)\n        z_n = nn_ops.conv2d(H, W_n, strides, padding)\n\n        z = z_o - z_p - z_n + 1e-10\n        s = relevance / z\n\n        c_o = nn_ops.conv2d_backprop_input(tf.shape(X), kernel, s, strides, padding)\n        c_p = nn_ops.conv2d_backprop_input(tf.shape(X), W_p, s, strides, padding)\n        c_n = nn_ops.conv2d_backprop_input(tf.shape(X), W_n, s, strides, padding)\n\n        return X * c_o - L * c_p - H * c_n\n\n    def backprop_dense_input(self, X, kernel, relevance, lowest=0., highest=1.):\n        W_p = tf.maximum(0., kernel)\n        W_n = tf.minimum(0., kernel)\n\n        L = tf.ones_like(X, tf.float32) * lowest\n        H = tf.ones_like(X, tf.float32) * highest\n\n        z_o = tf.matmul(X, kernel)\n        z_p = tf.matmul(L, W_p)\n        z_n = tf.matmul(H, W_n)\n\n        z = z_o - z_p - z_n + 1e-10\n        s = relevance / z\n\n        c_o = tf.matmul(s, tf.transpose(kernel))\n        c_p = tf.matmul(s, tf.transpose(W_p))\n        c_n = tf.matmul(s, tf.transpose(W_n))\n\n        return X * c_o - L * c_p - H * c_n\n'"
models/models_5_2.py,52,"b'from tensorflow.python.ops import nn_ops, gen_nn_ops\nimport tensorflow as tf\n\n\nclass MNIST_DNN:\n\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, X, reuse=False):\n\n        with tf.variable_scope(self.name) as scope:\n\n            if reuse:\n                scope.reuse_variables()\n\n            dense1 = tf.layers.dense(inputs=X, units=512, activation=tf.nn.relu, use_bias=False)\n            dense2 = tf.layers.dense(inputs=dense1, units=512, activation=tf.nn.relu, use_bias=False)\n            dense3 = tf.layers.dense(inputs=dense2, units=512, activation=tf.nn.relu, use_bias=False)\n            dense4 = tf.layers.dense(inputs=dense3, units=512, activation=tf.nn.relu, use_bias=False)\n            logits = tf.layers.dense(inputs=dense4, units=10, activation=None, use_bias=False)\n            prediction = tf.nn.softmax(logits)\n\n        return [dense1, dense2, dense3, dense4, prediction], logits\n\n    @property\n    def vars(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n\n\nclass MNIST_CNN:\n\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, X, reuse=False):\n\n        with tf.variable_scope(self.name) as scope:\n\n            if reuse:\n                scope.reuse_variables()\n\n            with tf.variable_scope(\'layer0\'):\n                X_img = tf.reshape(X, [-1, 28, 28, 1])\n\n            # Convolutional Layer #1 and Pooling Layer #1\n            with tf.variable_scope(\'layer1\'):\n                conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=False)\n                pool1 = tf.layers.average_pooling2d(inputs=conv1, pool_size=[2, 2], padding=""SAME"", strides=2)\n\n            # Convolutional Layer #2 and Pooling Layer #2\n            with tf.variable_scope(\'layer2\'):\n                conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=False)\n                pool2 = tf.layers.average_pooling2d(inputs=conv2, pool_size=[2, 2], padding=""SAME"", strides=2)\n\n            # Convolutional Layer #3 and Pooling Layer #3\n            with tf.variable_scope(\'layer3\'):\n                conv3 = tf.layers.conv2d(inputs=pool2, filters=128, kernel_size=[3, 3], padding=""SAME"", activation=tf.nn.relu, use_bias=False)\n                pool3 = tf.layers.average_pooling2d(inputs=conv3, pool_size=[2, 2], padding=""SAME"", strides=2)\n\n            # Dense Layer with Relu\n            with tf.variable_scope(\'layer4\'):\n                flat = tf.reshape(pool3, [-1, 128 * 4 * 4])\n                dense4 = tf.layers.dense(inputs=flat, units=625, activation=tf.nn.relu, use_bias=False)\n\n            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n            with tf.variable_scope(\'layer5\'):\n                logits = tf.layers.dense(inputs=dense4, units=10, activation=None, use_bias=False)\n                prediction = tf.nn.relu(logits)\n\n        return [X_img, conv1, pool1, conv2, pool2, conv3, pool3, flat, dense4, prediction], logits\n\n    @property\n    def vars(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n\n\nclass Taylor:\n\n    def __init__(self, activations, weights, conv_ksize, pool_ksize, conv_strides, pool_strides, name):\n\n        self.last_ind = len(activations)\n        for op in activations:\n            self.last_ind -= 1\n            if any([word in op.name for word in [\'conv\', \'pooling\', \'dense\']]):\n                break\n\n        self.activations = activations\n        self.weights = weights\n        self.conv_ksize = conv_ksize\n        self.pool_ksize = pool_ksize\n        self.conv_strides = conv_strides\n        self.pool_strides = pool_strides\n        self.name = name\n\n    def __call__(self, logit):\n\n        with tf.name_scope(self.name):\n            Rs = []\n            j = 0\n\n            for i in range(len(self.activations) - 1):\n\n                if i is self.last_ind:\n\n                    if \'conv\' in self.activations[i].name.lower():\n                        Rs.append(self.backprop_conv_input(self.activations[i + 1], self.weights[j], Rs[-1], self.conv_strides))\n                    else:\n                        Rs.append(self.backprop_dense_input(self.activations[i + 1], self.weights[j], Rs[-1]))\n\n                    continue\n\n                if i is 0:\n                    Rs.append(self.activations[i][:,logit,None])\n                    Rs.append(self.backprop_dense(self.activations[i + 1], self.weights[j][:,logit,None], Rs[-1]))\n                    j += 1\n\n                    continue\n\n                if \'dense\' in self.activations[i].name.lower():\n                    Rs.append(self.backprop_dense(self.activations[i + 1], self.weights[j], Rs[-1]))\n                    j += 1\n                elif \'reshape\' in self.activations[i].name.lower():\n                    shape = self.activations[i + 1].get_shape().as_list()\n                    shape[0] = -1\n                    Rs.append(tf.reshape(Rs[-1], shape))\n                elif \'conv\' in self.activations[i].name.lower():\n                    Rs.append(self.backprop_conv(self.activations[i + 1], self.weights[j], Rs[-1], self.conv_strides))\n                    j += 1\n                elif \'pooling\' in self.activations[i].name.lower():\n\n                    # Apply average pooling backprop regardless of type of pooling layer used, following recommendations by Montavon et al.\n                    # Uncomment code below if you want to apply the winner-take-all redistribution policy suggested by Bach et al.\n                    #\n                    # if \'max\' in self.activations[i].name.lower():\n                    #     pooling_type = \'max\'\n                    # else:\n                    #     pooling_type = \'avg\'\n                    # Rs.append(self.backprop_pool(self.activations[i + 1], Rs[-1], self.pool_ksize, self.pool_strides, pooling_type))\n\n                    Rs.append(self.backprop_pool(self.activations[i + 1], Rs[-1], self.pool_ksize, self.pool_strides, \'avg\'))\n                else:\n                    raise Exception(\'Unknown operation.\')\n\n            return Rs[-1]\n\n    def backprop_conv(self, activation, kernel, relevance, strides, padding=\'SAME\'):\n        W_p = tf.maximum(0., kernel)\n        z = nn_ops.conv2d(activation, W_p, strides, padding) + 1e-10\n        s = relevance / z\n        c = nn_ops.conv2d_backprop_input(tf.shape(activation), W_p, s, strides, padding)\n        return activation * c\n\n    def backprop_pool(self, activation, relevance, ksize, strides, pooling_type, padding=\'SAME\'):\n\n        if pooling_type.lower() in \'avg\':\n            z = nn_ops.avg_pool(activation, ksize, strides, padding) + 1e-10\n            s = relevance / z\n            c = gen_nn_ops._avg_pool_grad(tf.shape(activation), s, ksize, strides, padding)\n            return activation * c\n        else:\n            z = nn_ops.max_pool(activation, ksize, strides, padding) + 1e-10\n            s = relevance / z\n            c = gen_nn_ops._max_pool_grad(activation, z, s, ksize, strides, padding)\n            return activation * c\n\n    def backprop_dense(self, activation, kernel, relevance):\n        W_p = tf.maximum(0., kernel)\n        z = tf.matmul(activation, W_p) + 1e-10\n        s = relevance / z\n        c = tf.matmul(s, tf.transpose(W_p))\n        return activation * c\n\n    def backprop_conv_input(self, X, kernel, relevance, strides, padding=\'SAME\', lowest=0., highest=1.):\n        W_p = tf.maximum(0., kernel)\n        W_n = tf.minimum(0., kernel)\n\n        L = tf.ones_like(X, tf.float32) * lowest\n        H = tf.ones_like(X, tf.float32) * highest\n\n        z_o = nn_ops.conv2d(X, kernel, strides, padding)\n        z_p = nn_ops.conv2d(L, W_p, strides, padding)\n        z_n = nn_ops.conv2d(H, W_n, strides, padding)\n\n        z = z_o - z_p - z_n + 1e-10\n        s = relevance / z\n\n        c_o = nn_ops.conv2d_backprop_input(tf.shape(X), kernel, s, strides, padding)\n        c_p = nn_ops.conv2d_backprop_input(tf.shape(X), W_p, s, strides, padding)\n        c_n = nn_ops.conv2d_backprop_input(tf.shape(X), W_n, s, strides, padding)\n\n        return X * c_o - L * c_p - H * c_n\n\n    def backprop_dense_input(self, X, kernel, relevance, lowest=0., highest=1.):\n        W_p = tf.maximum(0., kernel)\n        W_n = tf.minimum(0., kernel)\n\n        L = tf.ones_like(X, tf.float32) * lowest\n        H = tf.ones_like(X, tf.float32) * highest\n\n        z_o = tf.matmul(X, kernel)\n        z_p = tf.matmul(L, W_p)\n        z_n = tf.matmul(H, W_n)\n\n        z = z_o - z_p - z_n + 1e-10\n        s = relevance / z\n\n        c_o = tf.matmul(s, tf.transpose(kernel))\n        c_p = tf.matmul(s, tf.transpose(W_p))\n        c_n = tf.matmul(s, tf.transpose(W_n))\n\n        return X * c_o - L * c_p - H * c_n\n'"
