file_path,api_count,code
be_happy.py,0,"b'\nimport cv2\nimport time\nimport imageio\nimport numpy as np\n\n\nfrom lib.core.api.facer import FaceAna\n\nfrom lib.core.headpose.pose import get_head_pose, line_pairs\nfacer = FaceAna()\n\n\n\ndef video(video_path_or_cam):\n    start_holder = False\n    vide_capture=cv2.VideoCapture(video_path_or_cam)\n    buff=[]\n    counter=1\n    while 1:\n        counter+=1\n\n\n        ret, image = vide_capture.read()\n\n        image=cv2.resize(image, None, fx=2., fy=2.)\n        pattern = np.zeros_like(image)\n\n        img_show = image.copy()\n        img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        star=time.time()\n        boxes, landmarks, states = facer.run(img)\n\n        duration=time.time()-star\n        print(\'one iamge cost %f s\'%(duration))\n\n\n        for face_index in range(landmarks.shape[0]):\n\n            #######head pose\n            #reprojectdst, euler_angle=get_head_pose(landmarks[face_index],img_show)\n\n            if args.mask:\n                face_bbox_keypoints = np.concatenate(\n                    (landmarks[face_index][:17, :], np.flip(landmarks[face_index][17:27, :], axis=0)), axis=0)\n\n                pattern = cv2.fillPoly(pattern, [face_bbox_keypoints.astype(np.int)], (1., 1., 1.))\n            # for start, end in line_pairs:\n            #     cv2.line(img_show, reprojectdst[start], reprojectdst[end], (0, 0, 255),2)\n            #\n            # cv2.putText(img_show, ""X: "" + ""{:7.2f}"".format(euler_angle[0, 0]), (20, 20), cv2.FONT_HERSHEY_SIMPLEX,\n            #             0.75, (0, 0, 0), thickness=2)\n            # cv2.putText(img_show, ""Y: "" + ""{:7.2f}"".format(euler_angle[1, 0]), (20, 50), cv2.FONT_HERSHEY_SIMPLEX,\n            #             0.75, (0, 0, 0), thickness=2)\n            # cv2.putText(img_show, ""Z: "" + ""{:7.2f}"".format(euler_angle[2, 0]), (20, 80), cv2.FONT_HERSHEY_SIMPLEX,\n            #             0.75, (0, 0, 0), thickness=2)\n\n            for landmarks_index in range(landmarks[face_index].shape[0]):\n\n                x_y = landmarks[face_index][landmarks_index]\n                cv2.circle(img_show, (int(x_y[0]), int(x_y[1])), 3,\n                           (222, 222, 222), -1)\n\n\n\n\n\n\n\n\n        img_show = cv2.cvtColor(img_show, cv2.COLOR_BGR2RGB)\n        img_show=cv2.resize(img_show,None,fx=0.4,fy=0.4)\n\n\n        if start_holder:\n            buff.append(img_show)\n        if args.mask:\n            cv2.namedWindow(""masked"", 0)\n            cv2.imshow(""masked"", img * pattern)\n\n        cv2.namedWindow(""capture"", 0)\n        cv2.imshow(""capture"", img_show)\n        key=cv2.waitKey(1)\n\n        if key == ord(\'s\'):\n            start_holder=True\n        if key==ord(\'q\'):\n            buff=[x for i,x in  enumerate(buff) if i%2==0 ]\n            gif = imageio.mimsave(\'sample.gif\', buff, \'GIF\', duration=0.08)\n            return\n\nif __name__==\'__main__\':\n    import argparse\n\n    parser = argparse.ArgumentParser(description=\'Start train.\')\n    parser.add_argument(\'--video\', dest=\'video\', type=str, default=None, \\\n                        help=\'the camera id (default: 0)\')\n    parser.add_argument(\'--cam_id\', dest=\'cam_id\', type=int, default=0, \\\n                        help=\'the camera to use\')\n    parser.add_argument(\'--mask\', dest=\'mask\', type=bool, default=False, \\\n                        help=\'mask the face or not\')\n    args = parser.parse_args()\n\n\n    if args.video is not None:\n        video(args.video)\n    else:\n        video(args.cam_id)\n\n'"
config.py,0,"b'\n\nimport os\nimport numpy as np\nfrom easydict import EasyDict as edict\n\nconfig = edict()\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""           ### if to use cuda,\n\nconfig.DETECT = edict()\nconfig.DETECT.model_path=\'./model/detector\'         ### saved_model or tflite\nconfig.DETECT.topk=10                               ### max boxes\nconfig.DETECT.min_face=1600                         ### max boxes\nconfig.DETECT.thres=0.5                             ### thres for nms\nconfig.DETECT.iou_thres=0.3                         ### iou thres for nms\nconfig.DETECT.input_shape=(256,320,3)               ### input shape for detector\n\n\nconfig.KEYPOINTS = edict()\nconfig.KEYPOINTS.model_path=\'./model/keypoints\'     ### saved_model or tflite\nconfig.KEYPOINTS.dense_dim=136+3+4                  #### output dimension\nconfig.KEYPOINTS.p_num=68                           #### 68 points\nconfig.KEYPOINTS.base_extend_range=[0.2,0.3]        ####\nconfig.KEYPOINTS.input_shape = (160,160,3)          # input size during training , 160\n\nconfig.TRACE= edict()\nconfig.TRACE.ema_or_one_euro=\'euro\'                 ### post process\nconfig.TRACE.pixel_thres=1\nconfig.TRACE.smooth_box=0.3                         ## if use euro, this will be disable\nconfig.TRACE.smooth_landmark=0.95                   ## if use euro, this will be disable\nconfig.TRACE.iou_thres=0.5\n\nconfig.DATA = edict()\nconfig.DATA.pixel_means = np.array([127., 127., 127.]) # RGB\n\n\n\n\n\n\nconfig.http_server = edict()\nconfig.http_server.ip=""0.0.0.0""\nconfig.http_server.port=5000\n\n\n'"
demo.py,0,"b'\nimport cv2\nimport time\nimport numpy as np\nimport os\nimport argparse\n\n\nfrom lib.core.api.facer import FaceAna\nfrom lib.core.headpose.pose import get_head_pose, line_pairs\nfrom lib.web.http import app\n\nfrom config import config as cfg\n\n\n\ndef video(video_path_or_cam):\n    facer = FaceAna()\n    vide_capture=cv2.VideoCapture(video_path_or_cam)\n\n    while 1:\n\n        ret, image = vide_capture.read()\n\n        pattern = np.zeros_like(image)\n\n        img_show = image.copy()\n\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        star=time.time()\n        boxes, landmarks, states = facer.run(image)\n\n        duration=time.time()-star\n        print(\'one iamge cost %f s\'%(duration))\n\n        for face_index in range(landmarks.shape[0]):\n\n            #######head pose need develop\n            #reprojectdst, euler_angle=get_head_pose(landmarks[face_index],img_show)\n\n            if args.mask:\n                face_bbox_keypoints = np.concatenate(\n                    (landmarks[face_index][:17, :], np.flip(landmarks[face_index][17:27, :], axis=0)), axis=0)\n\n                pattern = cv2.fillPoly(pattern, [face_bbox_keypoints.astype(np.int)], (1., 1., 1.))\n\n\n            # for start, end in line_pairs:\n            #     cv2.line(img_show, reprojectdst[start], reprojectdst[end], (0, 0, 255),2)\n            #\n            # cv2.putText(img_show, ""X: "" + ""{:7.2f}"".format(euler_angle[0, 0]), (20, 20), cv2.FONT_HERSHEY_SIMPLEX,\n            #             0.75, (0, 0, 0), thickness=2)\n            # cv2.putText(img_show, ""Y: "" + ""{:7.2f}"".format(euler_angle[1, 0]), (20, 50), cv2.FONT_HERSHEY_SIMPLEX,\n            #             0.75, (0, 0, 0), thickness=2)\n            # cv2.putText(img_show, ""Z: "" + ""{:7.2f}"".format(euler_angle[2, 0]), (20, 80), cv2.FONT_HERSHEY_SIMPLEX,\n            #             0.75, (0, 0, 0), thickness=2)\n\n            for landmarks_index in range(landmarks[face_index].shape[0]):\n\n                x_y = landmarks[face_index][landmarks_index]\n                cv2.circle(img_show, (int(x_y[0]), int(x_y[1])), 3,\n                           (222, 222, 222), -1)\n\n\n        cv2.namedWindow(""capture"", 0)\n        cv2.imshow(""capture"", img_show)\n\n        if args.mask:\n            cv2.namedWindow(""masked"", 0)\n            cv2.imshow(""masked"", image*pattern)\n\n        key=cv2.waitKey(1)\n        if key==ord(\'q\'):\n            return\n\n\ndef images(image_dir):\n    facer = FaceAna()\n\n    image_list=os.listdir(image_dir)\n    image_list=[x for x in image_list if \'jpg\' in x or \'png\' in x]\n    image_list.sort()\n\n\n    for image_name in image_list:\n\n        image=cv2.imread(os.path.join(image_dir,image_name))\n\n        pattern = np.zeros_like(image)\n\n        img_show = image.copy()\n\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        star=time.time()\n        boxes, landmarks, states = facer.run(image)\n\n        ###no track\n        facer.reset()\n\n        duration=time.time()-star\n        print(\'one iamge cost %f s\'%(duration))\n\n        for face_index in range(landmarks.shape[0]):\n\n            #######head pose\n            #reprojectdst, euler_angle=get_head_pose(landmarks[face_index],img_show)\n\n            if args.mask:\n                face_bbox_keypoints = np.concatenate(\n                    (landmarks[face_index][:17, :], np.flip(landmarks[face_index][17:27, :], axis=0)), axis=0)\n\n                pattern = cv2.fillPoly(pattern, [face_bbox_keypoints.astype(np.int)], (1., 1., 1.))\n\n\n            # for start, end in line_pairs:\n            #     cv2.line(img_show, reprojectdst[start], reprojectdst[end], (0, 0, 255),2)\n            #\n            # cv2.putText(img_show, ""X: "" + ""{:7.2f}"".format(euler_angle[0, 0]), (20, 20), cv2.FONT_HERSHEY_SIMPLEX,\n            #             0.75, (0, 0, 0), thickness=2)\n            # cv2.putText(img_show, ""Y: "" + ""{:7.2f}"".format(euler_angle[1, 0]), (20, 50), cv2.FONT_HERSHEY_SIMPLEX,\n            #             0.75, (0, 0, 0), thickness=2)\n            # cv2.putText(img_show, ""Z: "" + ""{:7.2f}"".format(euler_angle[2, 0]), (20, 80), cv2.FONT_HERSHEY_SIMPLEX,\n            #             0.75, (0, 0, 0), thickness=2)\n\n            for landmarks_index in range(landmarks[face_index].shape[0]):\n\n                x_y = landmarks[face_index][landmarks_index]\n                cv2.circle(img_show, (int(x_y[0]), int(x_y[1])), 3,\n                           (222, 222, 222), -1)\n\n\n        cv2.namedWindow(""capture"", 0)\n        cv2.imshow(""capture"", img_show)\n\n        if args.mask:\n            cv2.namedWindow(""masked"", 0)\n            cv2.imshow(""masked"", image*pattern)\n\n        key=cv2.waitKey(0)\n        if key==ord(\'q\'):\n            return\n\n\n\ndef web_demo():\n    app.run(host=cfg.http_server.ip,\n            port=cfg.http_server.port,\n            debug=True)\ndef build_argparse():\n    parser = argparse.ArgumentParser(description=\'Start train.\')\n    parser.add_argument(\'--video\', dest=\'video\', type=str, default=None, \\\n                        help=\'the camera id (default: 0)\')\n    parser.add_argument(\'--cam_id\', dest=\'cam_id\', type=int, default=0, \\\n                        help=\'the camera to use\')\n    parser.add_argument(\'--img_dir\', dest=\'img_dir\', type=str, default=None, \\\n                        help=\'the images dir to use\')\n\n    parser.add_argument(\'--mask\', dest=\'mask\', type=bool, default=False, \\\n                        help=\'mask the face or not\')\n\n    parser.add_argument(\'--web\', dest=\'web\', type=bool, default=False, \\\n                        help=\'mask the face or not\')\n\n    args = parser.parse_args()\n    return  args\n\nif __name__==\'__main__\':\n\n\n\n\n    args=build_argparse()\n\n\n    if args.web:\n        web_demo()\n\n\n    elif args.img_dir is not None:\n        images(args.img_dir)\n\n    elif args.video is not None:\n        video(args.video)\n    else:\n        video(args.cam_id)\n\n'"
web_demo_test.py,0,"b'import requests\nimport cv2\n\nfrom lib.web.http import app\n\n\nfrom config import config as cfg\n\n\n\nurl=\'http://192.168.42.117:5000/peppa_pig_face_engine/excuter\'\n\n\npic_path=""./figure/test1.jpg""\n\nres = {""image"":open(pic_path,\'rb\')}\n\nimg = cv2.imread(pic_path)\n# encode image as jpeg\n_, img_encoded = cv2.imencode(\'.jpg\', img)\n\n\n\nres = requests.post(url,data=img_encoded.tostring())\n\nprint(res.content)'"
lib/__init__.py,0,b''
lib/core/__init__.py,0,b''
lib/logger/__init__.py,0,b''
lib/logger/logger.py,0,"b""#-*-coding:utf-8-*-\n\n\n\n\n#-*-coding:utf-8-*-\n\nimport logging\n\n\ndef get_logger(LEVEL,log_file=None):\n    head = '[%(asctime)-15s] [%(levelname)s] %(message)s '\n    if LEVEL=='info':\n        logging.basicConfig(level=logging.INFO, format=head)\n    elif LEVEL=='debug':\n        logging.basicConfig(level=logging.DEBUG, format=head)\n    logger = logging.getLogger()\n\n    if log_file !=None:\n\n        fh = logging.FileHandler(log_file)\n        logger.addHandler(fh)\n    return logger\n\nlogger=get_logger('debug')\n"""
lib/web/__init__.py,0,b''
lib/web/http.py,0,"b'from flask import Flask,request,Response\nimport json\n\nfrom lib.core.api.facer import FaceAna\nfrom lib.core.headpose.pose import get_head_pose, line_pairs\n\nfrom lib.web.utils import parse_img,parse_as_dict\n\n\n##init a model there as global model\nfacer = FaceAna()\n\napp = Flask(""peppa_pig_face_engine"")\n\n\n@app.route(\'/peppa_pig_face_engine/healthcheck\')\ndef healthcheck():\n    print(\'peppa_pig_face_engine/healthcheck excuted \')\n    return \'Hello, i am peppa_pig_face\'\n\n\n\n@app.route(\'/peppa_pig_face_engine/excuter\',methods=[\'GET\',\'POST\'])\ndef excuter():\n\n    image=parse_img(request.data)\n\n    boxes, landmarks, states = facer.run(image)\n\n    ### no track\n    facer.reset()\n\n    res=parse_as_dict(landmarks)\n    print(\'peppa_pig_face_engine/excuter,    one image processed \')\n    print(res)\n    response = Response(\n        response=json.dumps(res),\n        status=200,\n        mimetype=\'application/json\'\n    )\n    return response\n\n\n\nif __name__ ==\'__main__\':\n\n    app.run(host=\'0.0.0.0\',port=5000,  debug=True)\n\n'"
lib/web/utils.py,0,"b'import cv2\nimport numpy as np\nimport json\n\n\ndef parse_img(img_str):\n\n    image_data = np.fromstring(img_str, np.uint8)\n    image_data = cv2.imdecode(image_data, cv2.IMREAD_COLOR)\n\n\n    # cv2.imshow(\'parse\',image_data)\n    # cv2.waitKey(0)\n    return image_data\n\n\n\n\n\ndef parse_as_dict(landmarks):\n\n\n\n    res=[]\n    for face_index in range(landmarks.shape[0]):\n\n        cur_face={}\n\n        bbox = [int(np.min(landmarks[face_index][:, 0])),\n                int(np.min(landmarks[face_index][:, 1])),\n                int(np.max(landmarks[face_index][:, 0])),\n                int(np.max(landmarks[face_index][:, 1]))]\n        cur_face[""bbox""]=bbox\n\n\n        landmarks_list=[]\n\n        for landmarks_index in range(landmarks[face_index].shape[0]):\n            x_y = landmarks[face_index][landmarks_index]\n            x_y_float=[int(x_y[0]),int(x_y[1])]\n            landmarks_list.append(x_y_float)\n\n        cur_face[""landmark""] = landmarks_list\n\n\n\n        res.append(cur_face)\n\n\n    return res'"
lib/core/LK/__init__.py,0,b'#-*-coding:utf-8-*- '
lib/core/LK/lk.py,0,"b'#-*-coding:utf-8-*-\nfrom config import config as cfg\nimport numpy as np\nimport math\n\nclass GroupTrack():\n    def __init__(self):\n        self.old_frame = None\n        self.previous_landmarks_set = None\n\n        self.with_landmark = True\n        self.thres=cfg.TRACE.pixel_thres\n        self.alpha=cfg.TRACE.smooth_landmark\n        self.iou_thres=cfg.TRACE.iou_thres\n\n        if \'ema\' in cfg.TRACE.ema_or_one_euro:\n            self.filter=EmaFilter(self.alpha)\n        else:\n            self.filter=OneEuroFilter()\n\n\n    def calculate(self, img, now_landmarks_set):\n\n        if self.previous_landmarks_set is None or self.previous_landmarks_set.shape[0]==0:\n            self.previous_landmarks_set=now_landmarks_set\n            result = now_landmarks_set\n\n        else:\n            if self.previous_landmarks_set.shape[0]==0:\n                return now_landmarks_set\n            else:\n                result=[]\n\n                for i in range(now_landmarks_set.shape[0]):\n                    not_in_flag = True\n                    for j in range(self.previous_landmarks_set.shape[0]):\n                        if self.iou(now_landmarks_set[i],self.previous_landmarks_set[j])>self.iou_thres:\n\n                            result.append(self.smooth(now_landmarks_set[i],self.previous_landmarks_set[j]))\n                            not_in_flag=False\n                            break\n                    if not_in_flag:\n                        result.append(now_landmarks_set[i])\n\n\n\n\n        result=np.array(result)\n        self.previous_landmarks_set=result\n\n        return result\n\n    def iou(self,p_set0,p_set1):\n        rec1=[np.min(p_set0[:,0]),np.min(p_set0[:,1]),np.max(p_set0[:,0]),np.max(p_set0[:,1])]\n        rec2 = [np.min(p_set1[:, 0]), np.min(p_set1[:, 1]), np.max(p_set1[:, 0]), np.max(p_set1[:, 1])]\n\n        # computing area of each rectangles\n        S_rec1 = (rec1[2] - rec1[0]) * (rec1[3] - rec1[1])\n        S_rec2 = (rec2[2] - rec2[0]) * (rec2[3] - rec2[1])\n\n        # computing the sum_area\n        sum_area = S_rec1 + S_rec2\n\n        # find the each edge of intersect rectangle\n        x1 = max(rec1[0], rec2[0])\n        y1 = max(rec1[1], rec2[1])\n        x2 = min(rec1[2], rec2[2])\n        y2 = min(rec1[3], rec2[3])\n\n        # judge if there is an intersect\n        intersect = max(0, x2 - x1) * max(0, y2 - y1)\n\n        return intersect / (sum_area - intersect)\n\n\n    def smooth(self,now_landmarks,previous_landmarks):\n\n        result=[]\n        for i in range(now_landmarks.shape[0]):\n\n            dis = np.sqrt(np.square(now_landmarks[i][0] - previous_landmarks[i][0]) + np.square(now_landmarks[i][1] - previous_landmarks[i][1]))\n\n            if dis < self.thres:\n                result.append(previous_landmarks[i])\n            else:\n                result.append(self.filter(now_landmarks[i], previous_landmarks[i]))\n\n        return np.array(result)\n\n\n    def do_moving_average(self, p_now, p_previous):\n        p = self.alpha * p_now + (1 - self.alpha) * p_previous\n        return p\n\n\n\n\ndef smoothing_factor(t_e, cutoff):\n    r = 2 * math.pi * cutoff * t_e\n    return r / (r + 1)\n\n\ndef exponential_smoothing(a, x, x_prev):\n    return a * x + (1 - a) * x_prev\n\n\nclass OneEuroFilter:\n    def __init__(self, dx0=0.0, min_cutoff=1.0, beta=0.0,\n                 d_cutoff=1.0):\n        """"""Initialize the one euro filter.""""""\n        # The parameters.\n        self.min_cutoff = float(min_cutoff)\n        self.beta = float(beta)\n        self.d_cutoff = float(d_cutoff)\n        # Previous values.\n\n        self.dx_prev = float(dx0)\n        #self.t_prev = float(t0)\n\n    def __call__(self, x,x_prev):\n\n        if x_prev is None:\n\n            return x\n\n\n\n\n        """"""Compute the filtered signal.""""""\n        t_e = 1\n\n        # The filtered derivative of the signal.\n        a_d = smoothing_factor(t_e, self.d_cutoff)\n        dx = (x - x_prev) / t_e\n        dx_hat = exponential_smoothing(a_d, dx, self.dx_prev)\n\n        # The filtered signal.\n        cutoff = self.min_cutoff + self.beta * abs(dx_hat)\n        a = smoothing_factor(t_e, cutoff)\n        x_hat = exponential_smoothing(a, x, x_prev)\n\n        # Memorize the previous values.\n\n        self.dx_prev = dx_hat\n\n\n        return x_hat\n\n\n\n\n\nclass EmaFilter():\n    def __init__(self,alpha):\n        self.alpha=alpha\n\n    def __call__(self,p_now,p_previous ):\n        p=exponential_smoothing(self.alpha,p_now,p_previous)\n\n        return p\n\n'"
lib/core/api/__init__.py,0,b''
lib/core/api/face_detector.py,2,"b'import numpy as np\nimport cv2\nimport tensorflow as tf\nimport math\nimport time\n\nfrom config import config as cfg\n\nclass FaceDetector:\n    def __init__(self):\n        """"""\n        the model was constructed by the params in config.py\n        """"""\n\n        self.model_path=cfg.DETECT.model_path\n        self.thres=cfg.DETECT.thres\n        self.input_shape=cfg.DETECT.input_shape\n\n\n\n        if \'lite\' in self.model_path:\n            self.model = tf.lite.Interpreter(model_path=self.model_path)\n            self.model.allocate_tensors()\n            self.input_details = self.model.get_input_details()\n            self.output_details = self.model.get_output_details()\n            self.tflite=True\n        else:\n            self.model = tf.saved_model.load(self.model_path)\n\n            self.tflite = False\n\n\n    def __call__(self, image,\n                 score_threshold=cfg.DETECT.thres,\n                 iou_threshold=cfg.DETECT.iou_thres,\n                 input_shape=(240,320)):\n        """"""Detect faces.\n\n        Arguments:\n            image: a numpy uint8 array with shape [height, width, 3],\n                that represents a RGB image.\n            input_shape: (h,w)\n            score_threshold: a float number.\n            iou_thres: a float number.\n        Returns:\n            boxes: a float numpy array of shape [num_faces, 5].\n\n        """"""\n        if not self.tflite:\n            if input_shape is None:\n                h,w,c=image.shape\n                input_shape = (math.ceil(h / 64 ) * 64,\n                               math.ceil(w / 64 ) * 64)\n            else:\n                h, w = input_shape\n                input_shape = (math.ceil(h / 64 ) * 64,\n                               math.ceil(w / 64 ) * 64)\n\n            image_fornet, scale_x, scale_y,dx,dy = self.preprocess(image,\n                                                                   target_height=input_shape[0],\n                                                                   target_width =input_shape[1])\n\n            if cfg.DETECT.input_shape[2] == 1:\n                ##gray scale\n                image_fornet = cv2.cvtColor(image_fornet, cv2.COLOR_RGB2GRAY)\n                image_fornet = np.expand_dims(image_fornet, axis=-1)\n\n\n            image_fornet = np.expand_dims(image_fornet, 0)\n\n            start = time.time()\n            bboxes = self.model.inference(image_fornet)\n            print(\'xx\', time.time() - start)\n        else:\n            input_shape = (320, 320)\n            image_fornet, scale_x, scale_y, dx, dy = self.preprocess(image,\n                                                                     target_height=input_shape[0],\n                                                                     target_width=input_shape[1])\n            if cfg.DETECT.input_shape[2] == 1:\n                ##gray scale\n                image_fornet = cv2.cvtColor(image_fornet, cv2.COLOR_RGB2GRAY)\n                image_fornet = np.expand_dims(image_fornet, axis=-1)\n\n\n            image_fornet = np.expand_dims(image_fornet, 0).astype(np.float32)\n\n            start = time.time()\n            self.model.set_tensor(self.input_details[0][\'index\'], image_fornet)\n\n            self.model.invoke()\n\n            bboxes = self.model.get_tensor(self.output_details[0][\'index\'])\n\n            print(\'xx\', time.time() - start)\n\n        bboxes=self.py_nms(np.array(bboxes[0]),iou_thres=iou_threshold,score_thres=score_threshold)\n\n        ###recorver to raw image\n        boxes_scaler = np.array([  (input_shape[1]) / scale_x,\n                                   (input_shape[0]) / scale_y,\n                                   (input_shape[1]) / scale_x,\n                                   (input_shape[0]) / scale_y,\n                                    1.], dtype=\'float32\')\n\n        boxes_bias=np.array( [ dx / scale_x,\n                               dy / scale_y,\n                               dx / scale_x,\n                               dy / scale_y,\n                               0.], dtype=\'float32\')\n        bboxes = bboxes * boxes_scaler-boxes_bias\n\n        return bboxes\n\n\n    def preprocess(self, image, target_height, target_width):\n\n        h, w, c = image.shape\n\n        bimage = np.zeros(shape=[target_height, target_width, c], dtype=image.dtype) + np.array(cfg.DATA.pixel_means,\n                                                                                                dtype=image.dtype)\n        scale_y = target_height / h\n        scale_x = target_width / w\n\n        scale=min(scale_x,scale_y)\n\n        image = cv2.resize(image, None, fx=scale, fy=scale)\n\n        h_, w_, _ = image.shape\n\n        dx=(target_width-w_)//2\n        dy=(target_height-h_)//2\n        bimage[dy:h_+dy, dx:w_+dx, :] = image\n\n        return bimage, scale, scale, dx, dy\n\n    def py_nms(self,bboxes, iou_thres, score_thres):\n\n        upper_thres = np.where(bboxes[:, 4] > score_thres)[0]\n\n        bboxes = bboxes[upper_thres]\n\n        x1 = bboxes[:, 0]\n        y1 = bboxes[:, 1]\n        x2 = bboxes[:, 2]\n        y2 = bboxes[:, 3]\n\n        order = np.argsort(bboxes[:, 4])[::-1]\n\n        keep = []\n\n        while order.shape[0] > 0:\n            cur = order[0]\n\n            keep.append(cur)\n\n            area = (bboxes[cur, 2] - bboxes[cur, 0]) * (bboxes[cur, 3] - bboxes[cur, 1])\n\n            x1_reain = x1[order[1:]]\n            y1_reain = y1[order[1:]]\n            x2_reain = x2[order[1:]]\n            y2_reain = y2[order[1:]]\n\n            xx1 = np.maximum(bboxes[cur, 0], x1_reain)\n            yy1 = np.maximum(bboxes[cur, 1], y1_reain)\n            xx2 = np.minimum(bboxes[cur, 2], x2_reain)\n            yy2 = np.minimum(bboxes[cur, 3], y2_reain)\n\n            intersection = np.maximum(0, yy2 - yy1) * np.maximum(0, xx2 - xx1)\n\n            iou = intersection / (area + (y2_reain - y1_reain) * (x2_reain - x1_reain) - intersection)\n\n            ##keep the low iou\n            low_iou_position = np.where(iou < iou_thres)[0]\n\n            order = order[low_iou_position + 1]\n\n        return bboxes[keep]\n\n\n\n\n'"
lib/core/api/face_landmark.py,2,"b'# -*-coding:utf-8-*-\nimport tensorflow as tf\nimport cv2\nimport numpy as np\n\nfrom config import config as cfg\nfrom lib.logger.logger import logger\nimport time\n\n\nclass FaceLandmark:\n    """"""\n           the model was constructed by the params in config.py\n    """"""\n\n    def __init__(self):\n        self.model_path = cfg.KEYPOINTS.model_path\n        self.min_face = 20\n        self.keypoints_num = cfg.KEYPOINTS.p_num\n\n\n        if \'lite\' in self.model_path:\n            self.model = tf.lite.Interpreter(model_path=self.model_path)\n            self.model.allocate_tensors()\n            self.input_details = self.model.get_input_details()\n            self.output_details = self.model.get_output_details()\n            self.tflite=True\n        else:\n            self.model = tf.saved_model.load(self.model_path)\n\n            self.tflite = False\n\n\n    ##below are the method  run for one by one, will be deprecated in the future\n    def __call__(self, img, bboxes):\n        \'\'\'\n        should be batched process\n        but process one by one, more simple\n        :param img:\n        :param bboxes:\n        :return: landmark and some cls results\n        \'\'\'\n\n        landmark_result = []\n        states_result = []\n\n        for i, bbox in enumerate(bboxes):\n\n            image_croped, detail = self.preprocess(img, bbox, i)\n\n\n\n            if cfg.KEYPOINTS.input_shape[2]==1:\n\n                ##gray scale\n                image_croped=cv2.cvtColor(image_croped,cv2.COLOR_RGB2GRAY)\n                image_croped=np.expand_dims(image_croped,axis=-1)\n            ##inference\n            image_croped = np.expand_dims(image_croped, axis=0)\n            print(image_croped.shape)\n\n\n            if not self.tflite:\n                res = self.model.inference(image_croped)\n\n                landmark = res[\'landmark\'].numpy().reshape(\n                    (-1, self.keypoints_num, 2))\n                states = res[\'cls\'].numpy()\n            else:\n\n\n\n                image_croped = image_croped.astype(np.float32)\n                self.model.set_tensor(\n                    self.input_details[0][\'index\'], image_croped)\n                self.model.invoke()\n\n                landmark = self.model.get_tensor(\n                    self.output_details[2][\'index\']).reshape((-1, self.keypoints_num, 2))\n                states = self.model.get_tensor(self.output_details[0][\'index\'])\n\n            landmark = self.postprocess(landmark, detail)\n\n            if landmark is not None:\n                landmark_result.append(landmark)\n                states_result.append(states)\n\n        return np.array(landmark_result), np.array(states_result)\n\n    def preprocess(self, image, bbox, i):\n        """"""\n        :param image: raw image\n        :param bbox: the bbox for the face\n        :param i: index of face\n        :return:\n        """"""\n        ##preprocess\n        bbox_width = bbox[2] - bbox[0]\n        bbox_height = bbox[3] - bbox[1]\n        if bbox_width <= self.min_face or bbox_height <= self.min_face:\n            return None, None\n        add = int(max(bbox_width, bbox_height))\n        bimg = cv2.copyMakeBorder(image, add, add, add, add,\n                                  borderType=cv2.BORDER_CONSTANT,\n                                  value=cfg.DATA.pixel_means)\n        bbox += add\n\n        face_width = (1 + 2 * cfg.KEYPOINTS.base_extend_range[0]) * bbox_width\n        center = [(bbox[0] + bbox[2]) // 2, (bbox[1] + bbox[3]) // 2]\n\n        ### make the box as square\n        bbox[0] = center[0] - face_width // 2\n        bbox[1] = center[1] - face_width // 2\n        bbox[2] = center[0] + face_width // 2\n        bbox[3] = center[1] + face_width // 2\n\n        # crop\n        bbox = bbox.astype(np.int)\n        crop_image = bimg[bbox[1]:bbox[3], bbox[0]:bbox[2], :]\n\n        h, w, _ = crop_image.shape\n        crop_image = cv2.resize(crop_image, (cfg.KEYPOINTS.input_shape[1],\n                                             cfg.KEYPOINTS.input_shape[0]))\n\n\n\n        cv2.imshow(\'i am watching u * * %d\' % i, crop_image)\n\n        return crop_image, [h, w, bbox[1], bbox[0], add]\n\n    def postprocess(self, landmark, detail):\n\n        ##recorver, and grouped as [68,2]\n        landmark = landmark[0]\n        # landmark[:, 0] = landmark[:, 0] * w + bbox[0] -add\n        # landmark[:, 1] = landmark[:, 1] * h + bbox[1] -add\n        landmark[:, 0] = landmark[:, 0] * detail[1] + detail[3] - detail[4]\n        landmark[:, 1] = landmark[:, 1] * detail[0] + detail[2] - detail[4]\n\n        return landmark\n\n\n\n\n\n\n\n\n\n    ##below are the method run for batch\n    def batch_call(self, image, bboxes):\n\n        if len(bboxes) == 0:\n            return np.array([]), np.array([])\n\n        images_batched, details_batched = self.batch_preprocess(image, bboxes)\n        if not self.tflite:\n            res = self.model.inference(images_batched)\n\n            ##reshape it as [n,keypoint_num,2]\n            landmark = res[\'landmark\'].numpy().reshape((-1, self.keypoints_num, 2))\n            states = res[\'cls\'].numpy()\n        else:\n\n            images_batched=images_batched.astype(np.float32)\n            self.model.set_tensor(self.input_details[0][\'index\'], images_batched)\n            self.model.invoke()\n\n            landmark = self.model.get_tensor(self.output_details[2][\'index\']).reshape((-1, self.keypoints_num, 2))\n            states = self.model.get_tensor(self.output_details[0][\'index\'])\n\n\n        landmark = self.batch_postprocess(landmark, details_batched)\n\n        return landmark, states\n\n    def batch_preprocess(self, image, bboxes):\n        """"""\n        :param image: raw image\n        :param bbox: the bbox for the face\n        :return:\n        """"""\n\n        images_batched = []\n        details = []  ### details about the extra params that needed in postprocess\n\n        for i, bbox in enumerate(bboxes):\n            ##preprocess\n            bbox_width = bbox[2] - bbox[0]\n            bbox_height = bbox[3] - bbox[1]\n            if bbox_width <= self.min_face or bbox_height <= self.min_face:\n                return None, None\n            add = int(max(bbox_width, bbox_height))\n            bimg = cv2.copyMakeBorder(image, add, add, add, add,\n                                      borderType=cv2.BORDER_CONSTANT,\n                                      value=cfg.DATA.pixel_means)\n            bbox += add\n\n            face_width = (1 + 2 * cfg.KEYPOINTS.base_extend_range[0]) * bbox_width\n            center = [(bbox[0] + bbox[2]) // 2, (bbox[1] + bbox[3]) // 2]\n\n            ### make the box as square\n            bbox[0] = center[0] - face_width // 2\n            bbox[1] = center[1] - face_width // 2\n            bbox[2] = center[0] + face_width // 2\n            bbox[3] = center[1] + face_width // 2\n\n            # crop\n            bbox = bbox.astype(np.int)\n            crop_image = bimg[bbox[1]:bbox[3], bbox[0]:bbox[2], :]\n\n            h, w, _ = crop_image.shape\n            crop_image = cv2.resize(crop_image, (cfg.KEYPOINTS.input_shape[1],\n                                                 cfg.KEYPOINTS.input_shape[0]))\n\n            cv2.imshow(\'i am watching u * * %d\' % i, crop_image)\n\n\n            if cfg.KEYPOINTS.input_shape[2]==1:\n\n                ##gray scale\n                crop_image=cv2.cvtColor(crop_image,cv2.COLOR_RGB2GRAY)\n                crop_image=np.expand_dims(crop_image,axis=-1)\n\n            images_batched.append(crop_image)\n\n            details.append([h, w, bbox[1], bbox[0], add])\n\n        return np.array(images_batched), np.array(details)\n\n    def batch_postprocess(self, landmark, details):\n\n        assert landmark.shape[0] == details.shape[0]\n\n        # landmark[:, :, 0] = landmark[:, :, 0] * w + bbox[0] - add\n        # landmark[:, :, 1] = landmark[:, :, 1] * h + bbox[1] - add\n\n        landmark[:, :, 0] = landmark[:, :, 0] * details[:, 1:2] + details[:, 3:4] - details[:, 4:]\n        landmark[:, :, 1] = landmark[:, :, 1] * details[:, 0:1] + details[:, 2:3] - details[:, 4:]\n\n        return landmark\n'"
lib/core/api/facer.py,0,"b""import cv2\nimport numpy as np\nimport time\n\nfrom lib.core.api.face_landmark import FaceLandmark\nfrom lib.core.api.face_detector import FaceDetector\nfrom lib.core.LK.lk import GroupTrack,OneEuroFilter,EmaFilter\n\nfrom config import config as cfg\n\nclass FaceAna():\n\n    '''\n    by default the top3 facea sorted by area will be calculated for time reason\n    '''\n    def __init__(self):\n\n        \n        self.face_detector = FaceDetector()\n        self.face_landmark = FaceLandmark()\n        self.trace = GroupTrack()\n\n\n\n        ###another thread should run detector in a slow way and update the track_box\n        self.track_box=None\n        self.previous_image=None\n        self.previous_box=None\n\n        self.diff_thres=5\n        self.top_k = cfg.DETECT.topk\n        self.min_face=cfg.DETECT.min_face\n        self.iou_thres=cfg.TRACE.iou_thres\n        self.alpha=cfg.TRACE.smooth_box\n\n        if 'ema' in cfg.TRACE.ema_or_one_euro:\n            self.filter = EmaFilter(self.alpha)\n        else:\n            self.filter = OneEuroFilter()\n\n    def run(self,image):\n\n        ###### run detector\n        if self.diff_frames(self.previous_image,image):\n            boxes = self.face_detector(image)\n            self.previous_image=image\n            boxes = self.judge_boxs(self.track_box, boxes)\n\n        else:\n            boxes=self.track_box\n            self.previous_image = image\n\n\n\n        boxes=self.sort_and_filter(boxes)\n\n        boxes_return = np.array(boxes)\n\n\n        #### batch predict for face landmark\n        landmarks,states=self.face_landmark.batch_call(image,boxes)\n\n\n        ### refine the landmark\n        landmarks = self.trace.calculate(image, landmarks)\n\n\n        #### refine the bboxes\n        track=[]\n        for i in range(landmarks.shape[0]):\n            track.append([np.min(landmarks[i][:,0]),np.min(landmarks[i][:,1]),np.max(landmarks[i][:,0]),np.max(landmarks[i][:,1])])\n        tmp_box=np.array(track)\n\n        self.track_box = self.judge_boxs(boxes_return, tmp_box)\n\n\n        return self.track_box,landmarks,states\n\n    def diff_frames(self,previous_frame,image):\n        '''\n        diff value for two value,\n        determin if to excute the detection\n\n        :param previous_frame:  RGB  array\n        :param image:           RGB  array\n        :return:                True or False\n        '''\n        if previous_frame is None:\n            return True\n        else:\n\n            _diff = cv2.absdiff(previous_frame, image)\n\n            diff=np.sum(_diff)/previous_frame.shape[0]/previous_frame.shape[1]/3.\n\n            if diff>self.diff_thres:\n                return True\n            else:\n                return False\n\n    def sort_and_filter(self,bboxes):\n        '''\n        find the top_k max bboxes, and filter the small face\n\n        :param bboxes:\n        :return:\n        '''\n\n        if len(bboxes)<1:\n            return []\n\n\n        area=(bboxes[:,2]-bboxes[:,0])*(bboxes[:,3]-bboxes[:,1])\n        select_index=area>self.min_face\n\n        area=area[select_index]\n        bboxes=bboxes[select_index,:]\n        if bboxes.shape[0]>self.top_k:\n            picked=area.argsort()[-self.top_k:][::-1]\n            sorted_bboxes=[bboxes[x] for x in picked]\n        else:\n            sorted_bboxes=bboxes\n        return np.array(sorted_bboxes)\n\n    def judge_boxs(self,previuous_bboxs,now_bboxs):\n        '''\n        function used to calculate the tracking bboxes\n\n        :param previuous_bboxs:[[x1,y1,x2,y2],... ]\n        :param now_bboxs: [[x1,y1,x2,y2],... ]\n        :return:\n        '''\n        def iou(rec1, rec2):\n\n\n            # computing area of each rectangles\n            S_rec1 = (rec1[2] - rec1[0]) * (rec1[3] - rec1[1])\n            S_rec2 = (rec2[2] - rec2[0]) * (rec2[3] - rec2[1])\n\n            # computing the sum_area\n            sum_area = S_rec1 + S_rec2\n\n            # find the each edge of intersect rectangle\n            x1 = max(rec1[0], rec2[0])\n            y1 = max(rec1[1], rec2[1])\n            x2 = min(rec1[2], rec2[2])\n            y2 = min(rec1[3], rec2[3])\n\n            # judge if there is an intersect\n            intersect =max(0,x2-x1) * max(0,y2-y1)\n\n            return intersect / (sum_area - intersect)\n\n        if previuous_bboxs is None:\n            return now_bboxs\n\n        result=[]\n\n        for i in range(now_bboxs.shape[0]):\n            contain = False\n            for j in range(previuous_bboxs.shape[0]):\n                if iou(now_bboxs[i], previuous_bboxs[j]) > self.iou_thres:\n                    result.append(self.smooth(now_bboxs[i],previuous_bboxs[j]))\n                    contain=True\n                    break\n            if not contain:\n                result.append(now_bboxs[i][0:4])\n\n\n        return np.array(result)\n\n    def smooth(self,now_box,previous_box):\n\n        return self.filter(now_box[:4], previous_box[:4])\n\n\n\n\n\n\n    def reset(self):\n        '''\n        reset the previous info used foe tracking,\n\n        :return:\n        '''\n        self.track_box = None\n        self.previous_image = None\n        self.previous_box = None\n\n\n"""
lib/core/beauty/__init__.py,0,b''
lib/core/beauty/thin.py,0,"b""import numpy as np\nimport cv2\nimport math\n\n\n\n'''\nThe algorithm is quiet time consume now ,need optimise\n'''\n\n\nclass FaceThin():\n    def __init__(self):\n        pass\n\n    def localTranslationWarp(self,srcImg, startX, startY, endX, endY, radius):\n\n        ddradius = float(radius * radius)\n        copyImg = srcImg.copy()\n\n\n        ddmc = (endX - startX) * (endX - startX) + (endY - startY) * (endY - startY)\n        H, W, C = srcImg.shape\n\n        for i in range(W):\n            for j in range(H):\n                # \xe8\xae\xa1\xe7\xae\x97\xe8\xaf\xa5\xe7\x82\xb9\xe6\x98\xaf\xe5\x90\xa6\xe5\x9c\xa8\xe5\xbd\xa2\xe5\x8f\x98\xe5\x9c\x86\xe7\x9a\x84\xe8\x8c\x83\xe5\x9b\xb4\xe4\xb9\x8b\xe5\x86\x85\n                # \xe4\xbc\x98\xe5\x8c\x96\xef\xbc\x8c\xe7\xac\xac\xe4\xb8\x80\xe6\xad\xa5\xef\xbc\x8c\xe7\x9b\xb4\xe6\x8e\xa5\xe5\x88\xa4\xe6\x96\xad\xe6\x98\xaf\xe4\xbc\x9a\xe5\x9c\xa8\xef\xbc\x88startX,startY)\xe7\x9a\x84\xe7\x9f\xa9\xe9\x98\xb5\xe6\xa1\x86\xe4\xb8\xad\n                if math.fabs(i - startX) > radius and math.fabs(j - startY) > radius:\n                    continue\n\n                distance = (i - startX) * (i - startX) + (j - startY) * (j - startY)\n\n                if (distance < ddradius):\n                    # \xe8\xae\xa1\xe7\xae\x97\xe5\x87\xba\xef\xbc\x88i,j\xef\xbc\x89\xe5\x9d\x90\xe6\xa0\x87\xe7\x9a\x84\xe5\x8e\x9f\xe5\x9d\x90\xe6\xa0\x87\n                    # \xe8\xae\xa1\xe7\xae\x97\xe5\x85\xac\xe5\xbc\x8f\xe4\xb8\xad\xe5\x8f\xb3\xe8\xbe\xb9\xe5\xb9\xb3\xe6\x96\xb9\xe5\x8f\xb7\xe9\x87\x8c\xe7\x9a\x84\xe9\x83\xa8\xe5\x88\x86\n                    ratio = (ddradius - distance) / (ddradius - distance + ddmc)\n                    ratio = ratio * ratio\n\n                    # \xe6\x98\xa0\xe5\xb0\x84\xe5\x8e\x9f\xe4\xbd\x8d\xe7\xbd\xae\n                    UX = i - ratio * (endX - startX)\n                    UY = j - ratio * (endY - startY)\n\n                    # \xe6\xa0\xb9\xe6\x8d\xae\xe5\x8f\x8c\xe7\xba\xbf\xe6\x80\xa7\xe6\x8f\x92\xe5\x80\xbc\xe6\xb3\x95\xe5\xbe\x97\xe5\x88\xb0UX\xef\xbc\x8cUY\xe7\x9a\x84\xe5\x80\xbc\n                    value = self.BilinearInsert(srcImg, UX, UY)\n                    # \xe6\x94\xb9\xe5\x8f\x98\xe5\xbd\x93\xe5\x89\x8d i \xef\xbc\x8cj\xe7\x9a\x84\xe5\x80\xbc\n                    copyImg[j, i] = value\n\n        return copyImg\n\n\n\n    def BilinearInsert(self,src, ux, uy):\n        w, h, c = src.shape\n        if c == 3:\n            x1 = int(ux)\n            x2 = x1 + 1\n            y1 = int(uy)\n            y2 = y1 + 1\n\n            part1 = src[y1, x1].astype(np.float) * (float(x2) - ux) * (float(y2) - uy)\n            part2 = src[y1, x2].astype(np.float) * (ux - float(x1)) * (float(y2) - uy)\n            part3 = src[y2, x1].astype(np.float) * (float(x2) - ux) * (uy - float(y1))\n            part4 = src[y2, x2].astype(np.float) * (ux - float(x1)) * (uy - float(y1))\n\n            insertValue = part1 + part2 + part3 + part4\n\n            return insertValue.astype(np.int8)\n\n\n\n    def __call__(self, image,landmarks):\n\n        if len(landmarks) == 0:\n            return\n\n        for landmarks_node in landmarks:\n            left_landmark_top = landmarks_node[3]\n            left_landmark_down = landmarks_node[5]\n\n            right_landmark_top = landmarks_node[13]\n            right_landmark_down = landmarks_node[15]\n\n            endPt = landmarks_node[30]\n\n            # 4-6\n            r_left = math.sqrt(\n                np.square(left_landmark_top[ 0] - left_landmark_down[0]) +\n                np.square(left_landmark_top[ 1] - left_landmark_down[1]) )\n\n            # 14-16\n            r_right = math.sqrt(\n                np.square(right_landmark_top[0] - right_landmark_down[ 0])  +\n                np.square(right_landmark_top[1] - right_landmark_down[ 1] ))\n\n            # left\n            thin_image = self.localTranslationWarp(image,\n                                                   left_landmark_top[0],\n                                                   left_landmark_top[1],\n                                                   endPt[0],\n                                                   endPt[1],\n                                                   r_left)\n            # right\n            thin_image = self.localTranslationWarp(thin_image,\n                                                   right_landmark_top[0],\n                                                   right_landmark_top[1],\n                                                   endPt[0],\n                                                   endPt[1],\n                                                   r_right)\n\n\n        cv2.imshow('thin', thin_image)\n\n"""
lib/core/headpose/__init__.py,0,b'#-*-coding:utf-8-*- '
lib/core/headpose/pose.py,0,"b'#-*-coding:utf-8-*-\n\n\nimport cv2\nimport numpy as np\n\n\n\n\n# object_pts = np.float32([[6.825897, 6.760612, 4.402142],\n#                          [1.330353, 7.122144, 6.903745],\n#                          [-1.330353, 7.122144, 6.903745],\n#                          [-6.825897, 6.760612, 4.402142],\n#                          [5.311432, 5.485328, 3.987654],\n#                          [1.789930, 5.393625, 4.413414],\n#                          [-1.789930, 5.393625, 4.413414],\n#                          [-5.311432, 5.485328, 3.987654],\n#                          [2.005628, 1.409845, 6.165652],\n#                          [-2.005628, 1.409845, 6.165652],\n#                          [2.774015, -2.080775, 5.048531],\n#                          [-2.774015, -2.080775, 5.048531],\n#                          [0.000000, -3.116408, 6.097667],\n#                          [0.000000, -7.415691, 4.070434]])\nobject_pts = np.float32([[6.825897, 6.760612, 4.402142],\n                         [1.330353, 7.122144, 6.903745],\n                         [-1.330353, 7.122144, 6.903745],\n                         [-6.825897, 6.760612, 4.402142],\n                         [5.311432, 5.485328, 3.987654],\n                         [1.789930, 5.393625, 4.413414],\n                         [-1.789930, 5.393625, 4.413414],\n                         [-5.311432, 5.485328, 3.987654],\n                         [2.005628, 1.409845, 6.165652],\n                         [-2.005628, 1.409845, 6.165652]])\nreprojectsrc = np.float32([[10.0, 10.0, 10.0],\n                           [10.0, 10.0, -10.0],\n                           [10.0, -10.0, -10.0],\n                           [10.0, -10.0, 10.0],\n                           [-10.0, 10.0, 10.0],\n                           [-10.0, 10.0, -10.0],\n                           [-10.0, -10.0, -10.0],\n                           [-10.0, -10.0, 10.0]])\n\nline_pairs = [[0, 1], [1, 2], [2, 3], [3, 0],\n              [4, 5], [5, 6], [6, 7], [7, 4],\n              [0, 4], [1, 5], [2, 6], [3, 7]]\n\n\ndef get_head_pose(shape,img):\n    h,w,_=img.shape\n    K = [w, 0.0, w//2,\n         0.0, w, h//2,\n         0.0, 0.0, 1.0]\n    # Assuming no lens distortion\n    D = [0, 0, 0.0, 0.0, 0]\n\n    cam_matrix = np.array(K).reshape(3, 3).astype(np.float32)\n    dist_coeffs = np.array(D).reshape(5, 1).astype(np.float32)\n\n\n\n    # image_pts = np.float32([shape[17], shape[21], shape[22], shape[26], shape[36],\n    #                         shape[39], shape[42], shape[45], shape[31], shape[35],\n    #                         shape[48], shape[54], shape[57], shape[8]])\n    image_pts = np.float32([shape[17], shape[21], shape[22], shape[26], shape[36],\n                            shape[39], shape[42], shape[45], shape[31], shape[35]])\n    _, rotation_vec, translation_vec = cv2.solvePnP(object_pts, image_pts, cam_matrix, dist_coeffs)\n\n    reprojectdst, _ = cv2.projectPoints(reprojectsrc, rotation_vec, translation_vec, cam_matrix,\n                                        dist_coeffs)\n\n    reprojectdst = tuple(map(tuple, reprojectdst.reshape(8, 2)))\n\n    # calc euler angle\n    rotation_mat, _ = cv2.Rodrigues(rotation_vec)\n    pose_mat = cv2.hconcat((rotation_mat, translation_vec))\n    _, _, _, _, _, _, euler_angle = cv2.decomposeProjectionMatrix(pose_mat)\n\n    return reprojectdst, euler_angle'"
