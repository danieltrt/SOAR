file_path,api_count,code
python/common_flags.py,0,"b'# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Define flags are common for both train.py and eval.py scripts.""""""\nimport sys\n\nfrom tensorflow.python.platform import flags\nimport logging\n\nimport datasets\nimport model\n\nFLAGS = flags.FLAGS\n\nlogging.basicConfig(\n    level=logging.DEBUG,\n    stream=sys.stderr,\n    format=\'%(levelname)s \'\n    \'%(asctime)s.%(msecs)06d: \'\n    \'%(filename)s: \'\n    \'%(lineno)d \'\n    \'%(message)s\',\n    datefmt=\'%Y-%m-%d %H:%M:%S\')\n\n\ndef define():\n  """"""Define common flags.""""""\n  # yapf: disable\n  flags.DEFINE_integer(\'batch_size\', 32,\n                       \'Batch size.\')\n\n  flags.DEFINE_integer(\'crop_width\', None,\n                       \'Width of the central crop for images.\')\n\n  flags.DEFINE_integer(\'crop_height\', None,\n                       \'Height of the central crop for images.\')\n\n  flags.DEFINE_string(\'train_log_dir\', \'/home/ucmed/opt/python/models-master/research/attention_ocr/python/logs\',\n                      \'Directory where to write event logs.\')\n\n  flags.DEFINE_string(\'dataset_name\', \'fsns\',\n                      \'Name of the dataset. Supported: fsns\')\n\n  flags.DEFINE_string(\'split_name\', \'train\',\n                      \'Dataset split name to run evaluation for: test,train.\')\n\n  flags.DEFINE_string(\'dataset_dir\', None,\n                      \'Dataset root folder.\')\n\n  flags.DEFINE_string(\'checkpoint\', \'\',\n                      \'Path for checkpoint to restore weights from.\')\n\n  flags.DEFINE_string(\'master\',\n                      \'\',\n                      \'BNS name of the TensorFlow master to use.\')\n\n  # Model hyper parameters\n  flags.DEFINE_float(\'learning_rate\', 0.004,\n                     \'learning rate\')\n\n  flags.DEFINE_string(\'optimizer\', \'momentum\',\n                      \'the optimizer to use\')\n\n  flags.DEFINE_string(\'momentum\', 0.9,\n                      \'momentum value for the momentum optimizer if used\')\n\n  flags.DEFINE_bool(\'use_augment_input\', True,\n                    \'If True will use image augmentation\')\n\n  # Method hyper parameters\n  # conv_tower_fn\n  flags.DEFINE_string(\'final_endpoint\', \'Mixed_5d\',\n                      \'Endpoint to cut inception tower\')\n\n  # sequence_logit_fn\n  flags.DEFINE_bool(\'use_attention\', True,\n                    \'If True will use the attention mechanism\')\n\n  flags.DEFINE_bool(\'use_autoregression\', True,\n                    \'If True will use autoregression (a feedback link)\')\n\n  flags.DEFINE_integer(\'num_lstm_units\', 256,\n                       \'number of LSTM units for sequence LSTM\')\n\n  flags.DEFINE_float(\'weight_decay\', 0.00004,\n                     \'weight decay for char prediction FC layers\')\n\n  flags.DEFINE_float(\'lstm_state_clip_value\', 10.0,\n                     \'cell state is clipped by this value prior to the cell\'\n                     \' output activation\')\n\n  # \'sequence_loss_fn\'\n  flags.DEFINE_float(\'label_smoothing\', 0.1,\n                     \'weight for label smoothing\')\n\n  flags.DEFINE_bool(\'ignore_nulls\', True,\n                    \'ignore null characters for computing the loss\')\n\n  flags.DEFINE_bool(\'average_across_timesteps\', False,\n                    \'divide the returned cost by the total label weight\')\n  # yapf: enable\n\n\ndef get_crop_size():\n  if FLAGS.crop_width and FLAGS.crop_height:\n    return (FLAGS.crop_width, FLAGS.crop_height)\n  else:\n    return None\n\n\ndef create_dataset(split_name):\n  ds_module = getattr(datasets, FLAGS.dataset_name)\n  return ds_module.get_split(split_name, dataset_dir=FLAGS.dataset_dir)\n\n\ndef create_mparams():\n  return {\n      \'conv_tower_fn\':\n      model.ConvTowerParams(final_endpoint=FLAGS.final_endpoint),\n      \'sequence_logit_fn\':\n      model.SequenceLogitsParams(\n          use_attention=FLAGS.use_attention,\n          use_autoregression=FLAGS.use_autoregression,\n          num_lstm_units=FLAGS.num_lstm_units,\n          weight_decay=FLAGS.weight_decay,\n          lstm_state_clip_value=FLAGS.lstm_state_clip_value),\n      \'sequence_loss_fn\':\n      model.SequenceLossParams(\n          label_smoothing=FLAGS.label_smoothing,\n          ignore_nulls=FLAGS.ignore_nulls,\n          average_across_timesteps=FLAGS.average_across_timesteps)\n  }\n\n\ndef create_model(*args, **kwargs):\n  ocr_model = model.Model(mparams=create_mparams(), *args, **kwargs)\n  return ocr_model\n'"
python/data_provider.py,22,"b'# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Functions to read, decode and pre-process input data for the Model.\n""""""\nimport collections\nimport functools\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport inception_preprocessing\n\n# Tuple to store input data endpoints for the Model.\n# It has following fields (tensors):\n#    images: input images,\n#      shape [batch_size x H x W x 3];\n#    labels: ground truth label ids,\n#      shape=[batch_size x seq_length];\n#    labels_one_hot: labels in one-hot encoding,\n#      shape [batch_size x seq_length x num_char_classes];\nInputEndpoints = collections.namedtuple(\n    \'InputEndpoints\', [\'images\', \'images_orig\', \'labels\', \'labels_one_hot\'])\n\n# A namedtuple to define a configuration for shuffled batch fetching.\n#   num_batching_threads: A number of parallel threads to fetch data.\n#   queue_capacity: a max number of elements in the batch shuffling queue.\n#   min_after_dequeue: a min number elements in the queue after a dequeue, used\n#     to ensure a level of mixing of elements.\nShuffleBatchConfig = collections.namedtuple(\'ShuffleBatchConfig\', [\n    \'num_batching_threads\', \'queue_capacity\', \'min_after_dequeue\'\n])\n\nDEFAULT_SHUFFLE_CONFIG = ShuffleBatchConfig(\n    num_batching_threads=8, queue_capacity=3000, min_after_dequeue=1000)\n\n\ndef augment_image(image):\n  """"""Augmentation the image with a random modification.\n\n  Args:\n    image: input Tensor image of rank 3, with the last dimension\n           of size 3.\n\n  Returns:\n    Distorted Tensor image of the same shape.\n  """"""\n  with tf.variable_scope(\'AugmentImage\'):\n    height = image.get_shape().dims[0].value\n    width = image.get_shape().dims[1].value\n\n    # Random crop cut from the street sign image, resized to the same size.\n    # Assures that the crop is covers at least 0.8 area of the input image.\n    bbox_begin, bbox_size, _ = tf.image.sample_distorted_bounding_box(\n        tf.shape(image),\n        bounding_boxes=tf.zeros([0, 0, 4]),\n        min_object_covered=0.8,\n        aspect_ratio_range=[0.8, 1.2],\n        area_range=[0.8, 1.0],\n        use_image_if_no_bounding_boxes=True)\n    distorted_image = tf.slice(image, bbox_begin, bbox_size)\n\n    # Randomly chooses one of the 4 interpolation methods\n    distorted_image = inception_preprocessing.apply_with_random_selector(\n        distorted_image,\n        lambda x, method: tf.image.resize_images(x, [height, width], method),\n        num_cases=4)\n    distorted_image.set_shape([height, width, 3])\n\n    # Color distortion\n    distorted_image = inception_preprocessing.apply_with_random_selector(\n        distorted_image,\n        functools.partial(\n            inception_preprocessing.distort_color, fast_mode=False),\n        num_cases=4)\n    distorted_image = tf.clip_by_value(distorted_image, -1.5, 1.5)\n\n  return distorted_image\n\n\ndef central_crop(image, crop_size):\n  """"""Returns a central crop for the specified size of an image.\n\n  Args:\n    image: A tensor with shape [height, width, channels]\n    crop_size: A tuple (crop_width, crop_height)\n\n  Returns:\n    A tensor of shape [crop_height, crop_width, channels].\n  """"""\n  with tf.variable_scope(\'CentralCrop\'):\n    target_width, target_height = crop_size\n    image_height, image_width = tf.shape(image)[0], tf.shape(image)[1]\n    assert_op1 = tf.Assert(\n        tf.greater_equal(image_height, target_height),\n        [\'image_height < target_height\', image_height, target_height])\n    assert_op2 = tf.Assert(\n        tf.greater_equal(image_width, target_width),\n        [\'image_width < target_width\', image_width, target_width])\n    with tf.control_dependencies([assert_op1, assert_op2]):\n      offset_width = (image_width - target_width) / 2\n      offset_height = (image_height - target_height) / 2\n      return tf.image.crop_to_bounding_box(image, offset_height, offset_width,\n                                           target_height, target_width)\n\n\ndef preprocess_image(image, augment=False, central_crop_size=None,\n                     num_towers=4):\n  """"""Normalizes image to have values in a narrow range around zero.\n\n  Args:\n    image: a [H x W x 3] uint8 tensor.\n    augment: optional, if True do random image distortion.\n    central_crop_size: A tuple (crop_width, crop_height).\n    num_towers: optional, number of shots of the same image in the input image.\n\n  Returns:\n    A float32 tensor of shape [H x W x 3] with RGB values in the required\n    range.\n  """"""\n  with tf.variable_scope(\'PreprocessImage\'):\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    if augment or central_crop_size:\n      if num_towers == 1:\n        images = [image]\n      else:\n        images = tf.split(value=image, num_or_size_splits=num_towers, axis=1)\n      if central_crop_size:\n        view_crop_size = (central_crop_size[0] / num_towers,\n                          central_crop_size[1])\n        images = [central_crop(img, view_crop_size) for img in images]\n      if augment:\n        images = [augment_image(img) for img in images]\n      image = tf.concat(images, 1)\n\n    image = tf.subtract(image, 0.5)\n    image = tf.multiply(image, 2.5)\n\n  return image\n\n\ndef get_data(dataset,\n             batch_size,\n             augment=False,\n             central_crop_size=None,\n             shuffle_config=None,\n             shuffle=True):\n  """"""Wraps calls to DatasetDataProviders and shuffle_batch.\n\n  For more details about supported Dataset objects refer to datasets/fsns.py.\n\n  Args:\n    dataset: a slim.data.dataset.Dataset object.\n    batch_size: number of samples per batch.\n    augment: optional, if True does random image distortion.\n    central_crop_size: A CharLogittuple (crop_width, crop_height).\n    shuffle_config: A namedtuple ShuffleBatchConfig.\n    shuffle: if True use data shuffling.\n\n  Returns:\n\n  """"""\n  if not shuffle_config:\n    shuffle_config = DEFAULT_SHUFFLE_CONFIG\n\n  provider = slim.dataset_data_provider.DatasetDataProvider(\n      dataset,\n      shuffle=shuffle,\n      common_queue_capacity=2 * batch_size,\n      common_queue_min=batch_size)\n  image_orig, label = provider.get([\'image\', \'label\'])\n  \n  image = preprocess_image(\n      image_orig, augment, central_crop_size, num_towers=dataset.num_of_views)\n  label_one_hot = slim.one_hot_encoding(label, dataset.num_char_classes)\n\n  images, images_orig, labels, labels_one_hot = (tf.train.shuffle_batch(\n      [image, image_orig, label, label_one_hot],\n      batch_size=batch_size,\n      num_threads=shuffle_config.num_batching_threads,\n      capacity=shuffle_config.queue_capacity,\n      min_after_dequeue=shuffle_config.min_after_dequeue))\n\n  return InputEndpoints(\n      images=images,\n      images_orig=images_orig,\n      labels=labels,\n      labels_one_hot=labels_one_hot)\n'"
python/data_provider_test.py,3,"b'# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for data_provider.""""""\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.slim import queues\n\nimport datasets\nimport data_provider\n\n\nclass DataProviderTest(tf.test.TestCase):\n  def setUp(self):\n    tf.test.TestCase.setUp(self)\n\n  def test_preprocessed_image_values_are_in_range(self):\n    image_shape = (5, 4, 3)\n    fake_image = np.random.randint(low=0, high=255, size=image_shape)\n    image_tf = data_provider.preprocess_image(fake_image)\n\n    with self.test_session() as sess:\n      image_np = sess.run(image_tf)\n\n    self.assertEqual(image_np.shape, image_shape)\n    min_value, max_value = np.min(image_np), np.max(image_np)\n    self.assertTrue((-1.28 < min_value) and (min_value < 1.27))\n    self.assertTrue((-1.28 < max_value) and (max_value < 1.27))\n\n  def test_provided_data_has_correct_shape(self):\n    batch_size = 4\n    data = data_provider.get_data(\n        dataset=datasets.fsns_test.get_test_split(),\n        batch_size=batch_size,\n        augment=True,\n        central_crop_size=None)\n\n    with self.test_session() as sess, queues.QueueRunners(sess):\n      images_np, labels_np = sess.run([data.images, data.labels_one_hot])\n\n    self.assertEqual(images_np.shape, (batch_size, 150, 600, 3))\n    self.assertEqual(labels_np.shape, (batch_size, 37, 134))\n\n  def test_optionally_applies_central_crop(self):\n    batch_size = 4\n    data = data_provider.get_data(\n        dataset=datasets.fsns_test.get_test_split(),\n        batch_size=batch_size,\n        augment=True,\n        central_crop_size=(500, 100))\n\n    with self.test_session() as sess, queues.QueueRunners(sess):\n      images_np = sess.run(data.images)\n\n    self.assertEqual(images_np.shape, (batch_size, 100, 500, 3))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
python/demo_inference.py,5,"b'# -*- coding: utf-8 -*\n\n""""""A script to run inference on a set of image files.\n\nNOTE #1: The Attention OCR model was trained only using FSNS train dataset and\nit will work only for images which look more or less similar to french street\nnames. In order to apply it to images from a different distribution you need\nto retrain (or at least fine-tune) it using images from that distribution.\n\nNOTE #2: This script exists for demo purposes only. It is highly recommended\nto use tools and mechanisms provided by the TensorFlow Serving system to run\ninference on TensorFlow models in production:\nhttps://www.tensorflow.org/serving/serving_basic\n\nUsage:\npython demo_inference.py --batch_size=32 \\\n  --checkpoint=model.ckpt-399731\\\n  --image_path_pattern=./datasets/data/fsns/temp/fsns_train_%02d.png\n""""""\nimport numpy as np\nimport PIL.Image\n\nimport tensorflow as tf\nfrom tensorflow.python.platform import flags\nfrom tensorflow.python.training import monitored_session\n\nimport common_flags\nimport datasets\nimport data_provider\n\nFLAGS = flags.FLAGS\ncommon_flags.define()\n\n# e.g. ./datasets/data/fsns/temp/fsns_train_%02d.png\nflags.DEFINE_string(\'image_path_pattern\', \'\',\n                    \'A file pattern with a placeholder for the image index.\')\n\n\ndef get_dataset_image_size(dataset_name):\n  # Ideally this info should be exposed through the dataset interface itself.\n  # But currently it is not available by other means.\n  ds_module = getattr(datasets, dataset_name)\n  height, width, _ = ds_module.DEFAULT_CONFIG[\'image_shape\']\n  return width, height\n\n\ndef load_images(file_pattern, batch_size, dataset_name):\n  width, height = get_dataset_image_size(dataset_name)\n  images_actual_data = np.ndarray(shape=(batch_size, height, width, 3),\n                                  dtype=\'uint8\')\n  for i in range(batch_size):\n    path = file_pattern % i\n    print(""Reading %s"" % path)\n    pil_image = PIL.Image.open(tf.gfile.GFile(path,\'rb\'))\n    pil_image=pil_image.resize((600,150),PIL.Image.ANTIALIAS)\n    images_actual_data[i, ...] = np.asarray(pil_image)\n  return images_actual_data\n\n\ndef create_model(batch_size, dataset_name):\n  width, height = get_dataset_image_size(dataset_name)\n  dataset = common_flags.create_dataset(split_name=FLAGS.split_name)\n  model = common_flags.create_model(\n    num_char_classes=dataset.num_char_classes,\n    seq_length=dataset.max_sequence_length,\n    num_views=dataset.num_of_views,\n    null_code=dataset.null_code,\n    charset=dataset.charset)\n  raw_images = tf.placeholder(tf.uint8, shape=[batch_size, height, width, 3])\n  images = tf.map_fn(data_provider.preprocess_image, raw_images,\n                     dtype=tf.float32)\n  endpoints = model.create_base(images, labels_one_hot=None)\n  return raw_images, endpoints\n\n\ndef run(checkpoint, batch_size, dataset_name, image_path_pattern):\n  images_placeholder, endpoints = create_model(batch_size,\n                                               dataset_name)\n  images_data = load_images(image_path_pattern, batch_size,\n                            dataset_name)\n  session_creator = monitored_session.ChiefSessionCreator(\n    checkpoint_filename_with_path=checkpoint)\n  with monitored_session.MonitoredSession(\n      session_creator=session_creator) as sess:\n    predictions = sess.run(endpoints.predicted_text,\n                           feed_dict={images_placeholder: images_data})\n  return predictions.tolist()\n\n\ndef main(_):\n  print(""Predicted strings:"")\n  predictions = run(FLAGS.checkpoint, FLAGS.batch_size, FLAGS.dataset_name,\n                  FLAGS.image_path_pattern)\n  for line in predictions:\n    print(line)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
python/demo_inference_test.py,5,"b""#!/usr/bin/python\n# -*- coding: UTF-8 -*-\nimport demo_inference\nimport tensorflow as tf\nfrom tensorflow.python.training import monitored_session\n\n_CHECKPOINT = 'model.ckpt-399731'\n_CHECKPOINT_URL = 'http://download.tensorflow.org/models/attention_ocr_2017_08_09.tar.gz'\n\n\nclass DemoInferenceTest(tf.test.TestCase):\n  def setUp(self):\n    super(DemoInferenceTest, self).setUp()\n    for suffix in ['.meta', '.index', '.data-00000-of-00001']:\n      filename = _CHECKPOINT + suffix\n      self.assertTrue(tf.gfile.Exists(filename),\n                      msg='Missing checkpoint file %s. '\n                          'Please download and extract it from %s' %\n                          (filename, _CHECKPOINT_URL))\n    self._batch_size = 32\n\n  def test_moving_variables_properly_loaded_from_a_checkpoint(self):\n    batch_size = 32\n    dataset_name = 'fsns'\n    images_placeholder, endpoints = demo_inference.create_model(batch_size,\n                                                                dataset_name)\n    image_path_pattern = 'testdata/fsns_train_%02d.png'\n    images_data = demo_inference.load_images(image_path_pattern, batch_size,\n                                             dataset_name)\n    tensor_name = 'AttentionOcr_v1/conv_tower_fn/INCE/InceptionV3/Conv2d_2a_3x3/BatchNorm/moving_mean'\n    moving_mean_tf = tf.get_default_graph().get_tensor_by_name(\n      tensor_name + ':0')\n    reader = tf.train.NewCheckpointReader(_CHECKPOINT)\n    moving_mean_expected = reader.get_tensor(tensor_name)\n\n    session_creator = monitored_session.ChiefSessionCreator(\n      checkpoint_filename_with_path=_CHECKPOINT)\n    with monitored_session.MonitoredSession(\n        session_creator=session_creator) as sess:\n      moving_mean_np = sess.run(moving_mean_tf,\n                                feed_dict={images_placeholder: images_data})\n\n    self.assertAllEqual(moving_mean_expected, moving_mean_np)\n\n  def test_correct_results_on_test_data(self):\n    image_path_pattern = 'testdata/fsns_train_%02d.png'\n    predictions = demo_inference.run(_CHECKPOINT, self._batch_size,\n                                     'fsns',\n                                     image_path_pattern)\n    self.assertEqual([\n      'Boulevard de Lunel\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Rue de Provence\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Rue de Port Maria\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Avenue Charles Gounod\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Rue de l\xe2\x80\x98Aurore\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Rue de Beuzeville\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Rue d\xe2\x80\x98Orbey\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Rue Victor Schoulcher\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Rue de la Gare\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Rue des Tulipes\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Rue Andr\xc3\xa9 Maginot\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Route de Pringy\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Rue des Landelles\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Rue des Ilettes\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Avenue de Maurin\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Rue Th\xc3\xa9resa\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',  # GT='Rue Th\xc3\xa9r\xc3\xa9sa'\n      'Route de la Balme\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Rue H\xc3\xa9l\xc3\xa8ne Roederer\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Rue Emile Bernard\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Place de la Mairie\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Rue des Perrots\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Rue de la Lib\xc3\xa9ration\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Impasse du Capcir\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Avenue de la Grand Mare\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Rue Pierre Brossolette\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Rue de Provence\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Rue du Docteur Mourre\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Rue d\xe2\x80\x98Ortheuil\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Rue des Sarments\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Rue du Centre\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Impasse Pierre Mourgues\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91',\n      'Rue Marcel Dassault\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91\xe2\x96\x91'\n    ], predictions)\n\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
python/eval.py,3,"b'# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Script to evaluate a trained Attention OCR model.\n\nA simple usage example:\npython eval.py\n""""""\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nfrom tensorflow import app\nfrom tensorflow.python.platform import flags\n\nimport data_provider\nimport common_flags\n\nFLAGS = flags.FLAGS\ncommon_flags.define()\n\n# yapf: disable\nflags.DEFINE_integer(\'num_batches\', 16,\n                     \'Number of batches to run eval for.\')\n\nflags.DEFINE_string(\'eval_log_dir\', \'/tmp/attention_ocr/eval\',\n                    \'Directory where the evaluation results are saved to.\')\n\nflags.DEFINE_integer(\'eval_interval_secs\', 60,\n                     \'Frequency in seconds to run evaluations.\')\n\nflags.DEFINE_integer(\'number_of_steps\', None,\n                     \'Number of times to run evaluation.\')\n# yapf: enable\n\n\ndef main(_):\n  if not tf.gfile.Exists(FLAGS.eval_log_dir):\n    tf.gfile.MakeDirs(FLAGS.eval_log_dir)\n\n  dataset = common_flags.create_dataset(split_name=FLAGS.split_name)\n  model = common_flags.create_model(dataset.num_char_classes,\n                                    dataset.max_sequence_length,\n                                    dataset.num_of_views, dataset.null_code)\n  data = data_provider.get_data(\n      dataset,\n      FLAGS.batch_size,\n      augment=False,\n      central_crop_size=common_flags.get_crop_size())\n  endpoints = model.create_base(data.images, labels_one_hot=None)\n  model.create_loss(data, endpoints)\n  eval_ops = model.create_summaries(\n      data, endpoints, dataset.charset, is_training=False)\n\n  slim.get_or_create_global_step()\n  session_config = tf.ConfigProto(device_count={""GPU"": 0})\n  slim.evaluation.evaluation_loop(\n      master=FLAGS.master,\n      checkpoint_dir=FLAGS.train_log_dir,\n      logdir=FLAGS.eval_log_dir,\n      eval_op=eval_ops,\n      num_evals=FLAGS.num_batches,\n      eval_interval_secs=FLAGS.eval_interval_secs,\n      max_number_of_evaluations=FLAGS.number_of_steps,\n      session_config=session_config)\n\n\nif __name__ == \'__main__\':\n  app.run()\n'"
python/inception_preprocessing.py,63,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images for the Inception networks.""""""\n\n# TODO(gorban): add as a dependency, when slim or tensorflow/models are pipfied\n# Source:\n# https://raw.githubusercontent.com/tensorflow/models/a9d0e6e8923a4/slim/preprocessing/inception_preprocessing.py\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\n\n\ndef apply_with_random_selector(x, func, num_cases):\n  """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n  Args:\n    x: input Tensor.\n    func: Python function to apply.\n    num_cases: Python int32, number of cases to sample sel from.\n\n  Returns:\n    The result of func(x, sel), where func receives the value of the\n    selector as a python integer, but sel is sampled dynamically.\n  """"""\n  sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n  # Pass the real x only to one of the func calls.\n  return control_flow_ops.merge([\n      func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n      for case in range(num_cases)\n  ])[0]\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n  """"""Distort the color of a Tensor image.\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    color_ordering: Python int, a type of distortion (valid values: 0-3).\n    fast_mode: Avoids slower ops (random_hue and random_contrast)\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  Raises:\n    ValueError: if color_ordering not in [0, 3]\n  """"""\n  with tf.name_scope(scope, \'distort_color\', [image]):\n    if fast_mode:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      else:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n    else:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      elif color_ordering == 1:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n      elif color_ordering == 2:\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      elif color_ordering == 3:\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      else:\n        raise ValueError(\'color_ordering must be in [0, 3]\')\n\n    # The random_* ops do not necessarily clamp.\n    return tf.clip_by_value(image, 0.0, 1.0)\n\n\ndef distorted_bounding_box_crop(image,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n  """"""Generates cropped_image using a one of the bboxes randomly distorted.\n\n  See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n  Args:\n    image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the\n      whole image.\n    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n      area of the image must contain at least this fraction of any bounding box\n      supplied.\n    aspect_ratio_range: An optional list of `floats`. The cropped area of the\n      image must have an aspect ratio = width / height within this range.\n    area_range: An optional list of `floats`. The cropped area of the image\n      must contain a fraction of the supplied image within in this range.\n    max_attempts: An optional `int`. Number of attempts at generating a cropped\n      region of the image of the specified constraints. After `max_attempts`\n      failures, return the entire image.\n    scope: Optional scope for name_scope.\n  Returns:\n    A tuple, a 3-D Tensor cropped_image and the distorted bbox\n  """"""\n  with tf.name_scope(scope, \'distorted_bounding_box_crop\', [image, bbox]):\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n    # A large fraction of image datasets contain a human-annotated bounding\n    # box delineating the region of the image containing the object of interest.\n    # We choose to create a new bounding box for the object which is a randomly\n    # distorted version of the human-annotated bounding box that obeys an\n    # allowed range of aspect ratios, sizes and overlap with the human-annotated\n    # bounding box. If no box is supplied, then we assume the bounding box is\n    # the entire image.\n    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n        tf.shape(image),\n        bounding_boxes=bbox,\n        min_object_covered=min_object_covered,\n        aspect_ratio_range=aspect_ratio_range,\n        area_range=area_range,\n        max_attempts=max_attempts,\n        use_image_if_no_bounding_boxes=True)\n    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n\n    # Crop the image to the specified bounding box.\n    cropped_image = tf.slice(image, bbox_begin, bbox_size)\n    return cropped_image, distort_bbox\n\n\ndef preprocess_for_train(image,\n                         height,\n                         width,\n                         bbox,\n                         fast_mode=True,\n                         scope=None):\n  """"""Distort one image for training a network.\n\n  Distorting images provides a useful technique for augmenting the data\n  set during training in order to make the network invariant to aspects\n  of the image that do not effect the label.\n\n  Additionally it would create image_summaries to display the different\n  transformations applied to the image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n    height: integer\n    width: integer\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations (i.e.\n      bi-cubic resizing, random_hue or random_contrast).\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of distorted image used for training with range [-1, 1].\n  """"""\n  with tf.name_scope(scope, \'distort_image\', [image, height, width, bbox]):\n    if bbox is None:\n      bbox = tf.constant(\n          [0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n    image_with_box = tf.image.draw_bounding_boxes(\n        tf.expand_dims(image, 0), bbox)\n    tf.summary.image(\'image_with_bounding_boxes\', image_with_box)\n\n    distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)\n    # Restore the shape since the dynamic slice based upon the bbox_size loses\n    # the third dimension.\n    distorted_image.set_shape([None, None, 3])\n    image_with_distorted_box = tf.image.draw_bounding_boxes(\n        tf.expand_dims(image, 0), distorted_bbox)\n    tf.summary.image(\'images_with_distorted_bounding_box\',\n                     image_with_distorted_box)\n\n    # This resizing operation may distort the images because the aspect\n    # ratio is not respected. We select a resize method in a round robin\n    # fashion based on the thread number.\n    # Note that ResizeMethod contains 4 enumerated resizing methods.\n\n    # We select only 1 case for fast_mode bilinear.\n    num_resize_cases = 1 if fast_mode else 4\n    distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, method: tf.image.resize_images(x, [height, width], method=method),\n        num_cases=num_resize_cases)\n\n    tf.summary.image(\'cropped_resized_image\',\n                     tf.expand_dims(distorted_image, 0))\n\n    # Randomly flip the image horizontally.\n    distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n    # Randomly distort the colors. There are 4 ways to do it.\n    distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, ordering: distort_color(x, ordering, fast_mode),\n        num_cases=4)\n\n    tf.summary.image(\'final_distorted_image\',\n                     tf.expand_dims(distorted_image, 0))\n    distorted_image = tf.subtract(distorted_image, 0.5)\n    distorted_image = tf.multiply(distorted_image, 2.0)\n    return distorted_image\n\n\ndef preprocess_for_eval(image,\n                        height,\n                        width,\n                        central_fraction=0.875,\n                        scope=None):\n  """"""Prepare one image for evaluation.\n\n  If height and width are specified it would output an image with that size by\n  applying resize_bilinear.\n\n  If central_fraction is specified it would cropt the central fraction of the\n  input image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details)\n    height: integer\n    width: integer\n    central_fraction: Optional Float, fraction of the image to crop.\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of prepared image.\n  """"""\n  with tf.name_scope(scope, \'eval_image\', [image, height, width]):\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Crop the central region of the image with an area containing 87.5% of\n    # the original image.\n    if central_fraction:\n      image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n    if height and width:\n      # Resize the image to the specified height and width.\n      image = tf.expand_dims(image, 0)\n      image = tf.image.resize_bilinear(\n          image, [height, width], align_corners=False)\n      image = tf.squeeze(image, [0])\n    image = tf.subtract(image, 0.5)\n    image = tf.multiply(image, 2.0)\n    return image\n\n\ndef preprocess_image(image,\n                     height,\n                     width,\n                     is_training=False,\n                     bbox=None,\n                     fast_mode=True):\n  """"""Pre-process one image for training or evaluation.\n\n  Args:\n    image: 3-D Tensor [height, width, channels] with the image.\n    height: integer, image expected height.\n    width: integer, image expected width.\n    is_training: Boolean. If true it would transform an image for train,\n      otherwise it would transform it for evaluation.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations.\n\n  Returns:\n    3-D float Tensor containing an appropriately scaled image\n\n  Raises:\n    ValueError: if user does not provide bounding box\n  """"""\n  if is_training:\n    return preprocess_for_train(image, height, width, bbox, fast_mode)\n  else:\n    return preprocess_for_eval(image, height, width)\n'"
python/metrics.py,26,"b'# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Quality metrics for the model.""""""\n\nimport tensorflow as tf\n\n\ndef char_accuracy(predictions, targets, rej_char, streaming=False):\n  """"""Computes character level accuracy.\n\n  Both predictions and targets should have the same shape\n  [batch_size x seq_length].\n\n  Args:\n    predictions: predicted characters ids.\n    targets: ground truth character ids.\n    rej_char: the character id used to mark an empty element (end of sequence).\n    streaming: if True, uses the streaming mean from the slim.metric module.\n\n  Returns:\n    a update_ops for execution and value tensor whose value on evaluation\n    returns the total character accuracy.\n  """"""\n  with tf.variable_scope(\'CharAccuracy\'):\n    predictions.get_shape().assert_is_compatible_with(targets.get_shape())\n\n    targets = tf.to_int32(targets)\n    const_rej_char = tf.constant(rej_char, shape=targets.get_shape())\n    weights = tf.to_float(tf.not_equal(targets, const_rej_char))\n    correct_chars = tf.to_float(tf.equal(predictions, targets))\n    accuracy_per_example = tf.div(\n        tf.reduce_sum(tf.multiply(correct_chars, weights), 1),\n        tf.reduce_sum(weights, 1))\n    if streaming:\n      return tf.contrib.metrics.streaming_mean(accuracy_per_example)\n    else:\n      return tf.reduce_mean(accuracy_per_example)\n\n\ndef sequence_accuracy(predictions, targets, rej_char, streaming=False):\n  """"""Computes sequence level accuracy.\n\n  Both input tensors should have the same shape: [batch_size x seq_length].\n\n  Args:\n    predictions: predicted character classes.\n    targets: ground truth character classes.\n    rej_char: the character id used to mark empty element (end of sequence).\n    streaming: if True, uses the streaming mean from the slim.metric module.\n\n  Returns:\n    a update_ops for execution and value tensor whose value on evaluation\n    returns the total sequence accuracy.\n  """"""\n\n  with tf.variable_scope(\'SequenceAccuracy\'):\n    predictions.get_shape().assert_is_compatible_with(targets.get_shape())\n\n    targets = tf.to_int32(targets)\n    const_rej_char = tf.constant(\n        rej_char, shape=targets.get_shape(), dtype=tf.int32)\n    include_mask = tf.not_equal(targets, const_rej_char)\n    include_predictions = tf.to_int32(\n        tf.where(include_mask, predictions,\n                 tf.zeros_like(predictions) + rej_char))\n    correct_chars = tf.to_float(tf.equal(include_predictions, targets))\n    correct_chars_counts = tf.cast(\n        tf.reduce_sum(correct_chars, reduction_indices=[1]), dtype=tf.int32)\n    target_length = targets.get_shape().dims[1].value\n    target_chars_counts = tf.constant(\n        target_length, shape=correct_chars_counts.get_shape())\n    accuracy_per_example = tf.to_float(\n        tf.equal(correct_chars_counts, target_chars_counts))\n    if streaming:\n      return tf.contrib.metrics.streaming_mean(accuracy_per_example)\n    else:\n      return tf.reduce_mean(accuracy_per_example)\n'"
python/metrics_test.py,10,"b'# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for the metrics module.""""""\nimport contextlib\nimport numpy as np\nimport tensorflow as tf\n\nimport metrics\n\n\nclass AccuracyTest(tf.test.TestCase):\n  def setUp(self):\n    tf.test.TestCase.setUp(self)\n    self.rng = np.random.RandomState([11, 23, 50])\n    self.num_char_classes = 3\n    self.batch_size = 4\n    self.seq_length = 5\n    self.rej_char = 42\n\n  @contextlib.contextmanager\n  def initialized_session(self):\n    """"""Wrapper for test session context manager with required initialization.\n\n    Yields:\n      A session object that should be used as a context manager.\n    """"""\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      sess.run(tf.local_variables_initializer())\n      yield sess\n\n  def _fake_labels(self):\n    return self.rng.randint(\n        low=0,\n        high=self.num_char_classes,\n        size=(self.batch_size, self.seq_length),\n        dtype=\'int32\')\n\n  def _incorrect_copy(self, values, bad_indexes):\n    incorrect = np.copy(values)\n    incorrect[bad_indexes] = values[bad_indexes] + 1\n    return incorrect\n\n  def test_sequence_accuracy_identical_samples(self):\n    labels_tf = tf.convert_to_tensor(self._fake_labels())\n\n    accuracy_tf = metrics.sequence_accuracy(labels_tf, labels_tf,\n                                            self.rej_char)\n    with self.initialized_session() as sess:\n      accuracy_np = sess.run(accuracy_tf)\n\n    self.assertAlmostEqual(accuracy_np, 1.0)\n\n  def test_sequence_accuracy_one_char_difference(self):\n    ground_truth_np = self._fake_labels()\n    ground_truth_tf = tf.convert_to_tensor(ground_truth_np)\n    prediction_tf = tf.convert_to_tensor(\n        self._incorrect_copy(ground_truth_np, bad_indexes=((0, 0))))\n\n    accuracy_tf = metrics.sequence_accuracy(prediction_tf, ground_truth_tf,\n                                            self.rej_char)\n    with self.initialized_session() as sess:\n      accuracy_np = sess.run(accuracy_tf)\n\n    # 1 of 4 sequences is incorrect.\n    self.assertAlmostEqual(accuracy_np, 1.0 - 1.0 / self.batch_size)\n\n  def test_char_accuracy_one_char_difference_with_padding(self):\n    ground_truth_np = self._fake_labels()\n    ground_truth_tf = tf.convert_to_tensor(ground_truth_np)\n    prediction_tf = tf.convert_to_tensor(\n        self._incorrect_copy(ground_truth_np, bad_indexes=((0, 0))))\n\n    accuracy_tf = metrics.char_accuracy(prediction_tf, ground_truth_tf,\n                                        self.rej_char)\n    with self.initialized_session() as sess:\n      accuracy_np = sess.run(accuracy_tf)\n\n    chars_count = self.seq_length * self.batch_size\n    self.assertAlmostEqual(accuracy_np, 1.0 - 1.0 / chars_count)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
python/model.py,58,"b'# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Functions to build the Attention OCR model.\n\nUsage example:\n  ocr_model = model.Model(num_char_classes, seq_length, num_of_views)\n\n  data = ... # create namedtuple InputEndpoints\n  endpoints = model.create_base(data.images, data.labels_one_hot)\n  # endpoints.predicted_chars is a tensor with predicted character codes.\n  total_loss = model.create_loss(data, endpoints)\n""""""\nimport sys\nimport collections\nimport logging\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nfrom tensorflow.contrib.slim.nets import inception\n\nimport metrics\nimport sequence_layers\nimport utils\n\nOutputEndpoints = collections.namedtuple(\'OutputEndpoints\', [\n  \'chars_logit\', \'chars_log_prob\', \'predicted_chars\', \'predicted_scores\',\n  \'predicted_text\'\n])\n\n# TODO(gorban): replace with tf.HParams when it is released.\nModelParams = collections.namedtuple(\'ModelParams\', [\n  \'num_char_classes\', \'seq_length\', \'num_views\', \'null_code\'\n])\n\nConvTowerParams = collections.namedtuple(\'ConvTowerParams\', [\'final_endpoint\'])\n\nSequenceLogitsParams = collections.namedtuple(\'SequenceLogitsParams\', [\n  \'use_attention\', \'use_autoregression\', \'num_lstm_units\', \'weight_decay\',\n  \'lstm_state_clip_value\'\n])\n\nSequenceLossParams = collections.namedtuple(\'SequenceLossParams\', [\n  \'label_smoothing\', \'ignore_nulls\', \'average_across_timesteps\'\n])\n\nEncodeCoordinatesParams = collections.namedtuple(\'EncodeCoordinatesParams\', [\n  \'enabled\'\n])\n\n\ndef _dict_to_array(id_to_char, default_character):\n  num_char_classes = max(id_to_char.keys()) + 1\n  array = [default_character] * num_char_classes\n  for k, v in id_to_char.items():\n    array[k] = v\n  return array\n\n\nclass CharsetMapper(object):\n  """"""A simple class to map tensor ids into strings.\n\n    It works only when the character set is 1:1 mapping between individual\n    characters and individual ids.\n\n    Make sure you call tf.tables_initializer().run() as part of the init op.\n    """"""\n\n  def __init__(self, charset, default_character=\'?\'):\n    """"""Creates a lookup table.\n\n    Args:\n      charset: a dictionary with id-to-character mapping.\n    """"""\n    mapping_strings = tf.constant(_dict_to_array(charset, default_character))\n    self.table = tf.contrib.lookup.index_to_string_table_from_tensor(\n      mapping=mapping_strings, default_value=default_character)\n\n  def get_text(self, ids):\n    """"""Returns a string corresponding to a sequence of character ids.\n\n        Args:\n          ids: a tensor with shape [batch_size, max_sequence_length]\n        """"""\n    return tf.reduce_join(\n      self.table.lookup(tf.to_int64(ids)), reduction_indices=1)\n\n\ndef get_softmax_loss_fn(label_smoothing):\n  """"""Returns sparse or dense loss function depending on the label_smoothing.\n\n    Args:\n      label_smoothing: weight for label smoothing\n\n    Returns:\n      a function which takes labels and predictions as arguments and returns\n      a softmax loss for the selected type of labels (sparse or dense).\n    """"""\n  if label_smoothing > 0:\n\n    def loss_fn(labels, logits):\n      return (tf.nn.softmax_cross_entropy_with_logits(\n        logits=logits, labels=labels))\n  else:\n\n    def loss_fn(labels, logits):\n      return tf.nn.sparse_softmax_cross_entropy_with_logits(\n        logits=logits, labels=labels)\n\n  return loss_fn\n\n\nclass Model(object):\n  """"""Class to create the Attention OCR Model.""""""\n\n  def __init__(self,\n               num_char_classes,\n               seq_length,\n               num_views,\n               null_code,\n               mparams=None,\n               charset=None):\n    """"""Initialized model parameters.\n\n    Args:\n      num_char_classes: size of character set.\n      seq_length: number of characters in a sequence.\n      num_views: Number of views (conv towers) to use.\n      null_code: A character code corresponding to a character which\n        indicates end of a sequence.\n      mparams: a dictionary with hyper parameters for methods,  keys -\n        function names, values - corresponding namedtuples.\n      charset: an optional dictionary with a mapping between character ids and\n        utf8 strings. If specified the OutputEndpoints.predicted_text will\n        utf8 encoded strings corresponding to the character ids returned by\n        OutputEndpoints.predicted_chars (by default the predicted_text contains\n        an empty vector). \n        NOTE: Make sure you call tf.tables_initializer().run() if the charset\n        specified.\n    """"""\n    super(Model, self).__init__()\n    self._params = ModelParams(\n      num_char_classes=num_char_classes,\n      seq_length=seq_length,\n      num_views=num_views,\n      null_code=null_code)\n    self._mparams = self.default_mparams()\n    if mparams:\n      self._mparams.update(mparams)\n    self._charset = charset\n\n  def default_mparams(self):\n    return {\n      \'conv_tower_fn\':\n        ConvTowerParams(final_endpoint=\'Mixed_5d\'),\n      \'sequence_logit_fn\':\n        SequenceLogitsParams(\n          use_attention=True,\n          use_autoregression=True,\n          num_lstm_units=256,\n          weight_decay=0.00004,\n          lstm_state_clip_value=10.0),\n      \'sequence_loss_fn\':\n        SequenceLossParams(\n          label_smoothing=0.1,\n          ignore_nulls=True,\n          average_across_timesteps=False),\n      \'encode_coordinates_fn\': EncodeCoordinatesParams(enabled=False)\n    }\n\n  def set_mparam(self, function, **kwargs):\n    self._mparams[function] = self._mparams[function]._replace(**kwargs)\n\n  def conv_tower_fn(self, images, is_training=True, reuse=None):\n    """"""Computes convolutional features using the InceptionV3 model.\n\n    Args:\n      images: A tensor of shape [batch_size, height, width, channels].\n      is_training: whether is training or not.\n      reuse: whether or not the network and its variables should be reused. To\n        be able to reuse \'scope\' must be given.\n\n    Returns:\n      A tensor of shape [batch_size, OH, OW, N], where OWxOH is resolution of\n      output feature map and N is number of output features (depends on the\n      network architecture).\n    """"""\n    mparams = self._mparams[\'conv_tower_fn\']\n    logging.debug(\'Using final_endpoint=%s\', mparams.final_endpoint)\n    with tf.variable_scope(\'conv_tower_fn/INCE\'):\n      if reuse:\n        tf.get_variable_scope().reuse_variables()\n      with slim.arg_scope(inception.inception_v3_arg_scope()):\n        with slim.arg_scope([slim.batch_norm, slim.dropout],\n                            is_training=is_training):\n          net, _ = inception.inception_v3_base(\n            images, final_endpoint=mparams.final_endpoint)\n      return net\n\n  def _create_lstm_inputs(self, net):\n    """"""Splits an input tensor into a list of tensors (features).\n\n    Args:\n      net: A feature map of shape [batch_size, num_features, feature_size].\n\n    Raises:\n      AssertionError: if num_features is less than seq_length.\n\n    Returns:\n      A list with seq_length tensors of shape [batch_size, feature_size]\n    """"""\n    num_features = net.get_shape().dims[1].value\n    if num_features < self._params.seq_length:\n      raise AssertionError(\'Incorrect dimension #1 of input tensor\'\n                           \' %d should be bigger than %d (shape=%s)\' %\n                           (num_features, self._params.seq_length,\n                            net.get_shape()))\n    elif num_features > self._params.seq_length:\n      logging.warning(\'Ignoring some features: use %d of %d (shape=%s)\',\n                      self._params.seq_length, num_features, net.get_shape())\n      net = tf.slice(net, [0, 0, 0], [-1, self._params.seq_length, -1])\n\n    return tf.unstack(net, axis=1)\n\n  def sequence_logit_fn(self, net, labels_one_hot):\n    mparams = self._mparams[\'sequence_logit_fn\']\n    # TODO(gorban): remove /alias suffixes from the scopes.\n    with tf.variable_scope(\'sequence_logit_fn/SQLR\'):\n      layer_class = sequence_layers.get_layer_class(mparams.use_attention,\n                                                    mparams.use_autoregression)\n      layer = layer_class(net, labels_one_hot, self._params, mparams)\n      return layer.create_logits()\n\n  def max_pool_views(self, nets_list):\n    """"""Max pool across all nets in spatial dimensions.\n\n    Args:\n      nets_list: A list of 4D tensors with identical size.\n\n    Returns:\n      A tensor with the same size as any input tensors.\n    """"""\n    batch_size, height, width, num_features = [\n      d.value for d in nets_list[0].get_shape().dims\n    ]\n    xy_flat_shape = (batch_size, 1, height * width, num_features)\n    nets_for_merge = []\n    with tf.variable_scope(\'max_pool_views\', values=nets_list):\n      for net in nets_list:\n        nets_for_merge.append(tf.reshape(net, xy_flat_shape))\n      merged_net = tf.concat(nets_for_merge, 1)\n      net = slim.max_pool2d(\n        merged_net, kernel_size=[len(nets_list), 1], stride=1)\n      net = tf.reshape(net, (batch_size, height, width, num_features))\n    return net\n\n  def pool_views_fn(self, nets):\n    """"""Combines output of multiple convolutional towers into a single tensor.\n\n    It stacks towers one on top another (in height dim) in a 4x1 grid.\n    The order is arbitrary design choice and shouldn\'t matter much.\n\n    Args:\n      nets: list of tensors of shape=[batch_size, height, width, num_features].\n\n    Returns:\n      A tensor of shape [batch_size, seq_length, features_size].\n    """"""\n    with tf.variable_scope(\'pool_views_fn/STCK\'):\n      net = tf.concat(nets, 1)\n      batch_size = net.get_shape().dims[0].value\n      feature_size = net.get_shape().dims[3].value\n      return tf.reshape(net, [batch_size, -1, feature_size])\n\n  def char_predictions(self, chars_logit):\n    """"""Returns confidence scores (softmax values) for predicted characters.\n\n    Args:\n      chars_logit: chars logits, a tensor with shape\n        [batch_size x seq_length x num_char_classes]\n\n    Returns:\n      A tuple (ids, log_prob, scores), where:\n        ids - predicted characters, a int32 tensor with shape\n          [batch_size x seq_length];\n        log_prob - a log probability of all characters, a float tensor with\n          shape [batch_size, seq_length, num_char_classes];\n        scores - corresponding confidence scores for characters, a float\n        tensor\n          with shape [batch_size x seq_length].\n    """"""\n    log_prob = utils.logits_to_log_prob(chars_logit)\n    ids = tf.to_int32(tf.argmax(log_prob, axis=2), name=\'predicted_chars\')\n    mask = tf.cast(\n      slim.one_hot_encoding(ids, self._params.num_char_classes), tf.bool)\n    all_scores = tf.nn.softmax(chars_logit)\n    selected_scores = tf.boolean_mask(all_scores, mask, name=\'char_scores\')\n    scores = tf.reshape(selected_scores, shape=(-1, self._params.seq_length))\n    return ids, log_prob, scores\n\n  def encode_coordinates_fn(self, net):\n    """"""Adds one-hot encoding of coordinates to different views in the networks.\n\n    For each ""pixel"" of a feature map it adds a onehot encoded x and y\n    coordinates.\n\n    Args:\n      net: a tensor of shape=[batch_size, height, width, num_features]\n\n    Returns:\n      a tensor with the same height and width, but altered feature_size.\n    """"""\n    mparams = self._mparams[\'encode_coordinates_fn\']\n    if mparams.enabled:\n      batch_size, h, w, _ = net.shape.as_list()\n      x, y = tf.meshgrid(tf.range(w), tf.range(h))\n      w_loc = slim.one_hot_encoding(x, num_classes=w)\n      h_loc = slim.one_hot_encoding(y, num_classes=h)\n      loc = tf.concat([h_loc, w_loc], 2)\n      loc = tf.tile(tf.expand_dims(loc, 0), [batch_size, 1, 1, 1])\n      return tf.concat([net, loc], 3)\n    else:\n      return net\n\n  def create_base(self,\n                  images,\n                  labels_one_hot,\n                  scope=\'AttentionOcr_v1\',\n                  reuse=None):\n    """"""Creates a base part of the Model (no gradients, losses or summaries).\n\n    Args:\n      images: A tensor of shape [batch_size, height, width, channels].\n      labels_one_hot: Optional (can be None) one-hot encoding for ground truth\n        labels. If provided the function will create a model for training.\n      scope: Optional variable_scope.\n      reuse: whether or not the network and its variables should be reused. To\n        be able to reuse \'scope\' must be given.\n\n    Returns:\n      A named tuple OutputEndpoints.\n    """"""\n    logging.debug(\'images: %s\', images)\n    is_training = labels_one_hot is not None\n    \n    with tf.variable_scope(scope, reuse=reuse):\n      views = tf.split(\n        value=images, num_or_size_splits=self._params.num_views, axis=2)\n      logging.debug(\'Views=%d single view: %s\', len(views), views[0])\n\n      nets = [\n        self.conv_tower_fn(v, is_training, reuse=(i != 0))\n        for i, v in enumerate(views)\n      ]\n      logging.debug(\'Conv tower: %s\', nets[0])\n\n      nets = [self.encode_coordinates_fn(net) for net in nets]\n      logging.debug(\'Conv tower w/ encoded coordinates: %s\', nets[0])\n\n      net = self.pool_views_fn(nets)\n      logging.debug(\'Pooled views: %s\', net)\n\n      chars_logit = self.sequence_logit_fn(net, labels_one_hot)\n      logging.debug(\'chars_logit: %s\', chars_logit)\n\n      predicted_chars, chars_log_prob, predicted_scores = (\n        self.char_predictions(chars_logit))\n      if self._charset:\n        character_mapper = CharsetMapper(self._charset)\n        predicted_text = character_mapper.get_text(predicted_chars)\n      else:\n        predicted_text = tf.constant([])\n    return OutputEndpoints(\n      chars_logit=chars_logit,\n      chars_log_prob=chars_log_prob,\n      predicted_chars=predicted_chars,\n      predicted_scores=predicted_scores,\n      predicted_text=predicted_text)\n\n  def create_loss(self, data, endpoints):\n    """"""Creates all losses required to train the model.\n\n    Args:\n      data: InputEndpoints namedtuple.\n      endpoints: Model namedtuple.\n\n    Returns:\n      Total loss.\n    """"""\n    # NOTE: the return value of ModelLoss is not used directly for the\n    # gradient computation because under the hood it calls slim.losses.AddLoss,\n    # which registers the loss in an internal collection and later returns it\n    # as part of GetTotalLoss. We need to use total loss because model may have\n    # multiple losses including regularization losses.\n    self.sequence_loss_fn(endpoints.chars_logit, data.labels)\n    total_loss = slim.losses.get_total_loss()\n    tf.summary.scalar(\'TotalLoss\', total_loss)\n    return total_loss\n\n  def label_smoothing_regularization(self, chars_labels, weight=0.1):\n    """"""Applies a label smoothing regularization.\n\n    Uses the same method as in https://arxiv.org/abs/1512.00567.\n\n    Args:\n      chars_labels: ground truth ids of charactes,\n        shape=[batch_size, seq_length];\n      weight: label-smoothing regularization weight.\n\n    Returns:\n      A sensor with the same shape as the input.\n    """"""\n    one_hot_labels = tf.one_hot(\n      chars_labels, depth=self._params.num_char_classes, axis=-1)\n    pos_weight = 1.0 - weight\n    neg_weight = weight / self._params.num_char_classes\n    return one_hot_labels * pos_weight + neg_weight\n\n  def sequence_loss_fn(self, chars_logits, chars_labels):\n    """"""Loss function for char sequence.\n\n    Depending on values of hyper parameters it applies label smoothing and can\n    also ignore all null chars after the first one.\n\n    Args:\n      chars_logits: logits for predicted characters,\n        shape=[batch_size, seq_length, num_char_classes];\n      chars_labels: ground truth ids of characters,\n        shape=[batch_size, seq_length];\n      mparams: method hyper parameters.\n\n    Returns:\n      A Tensor with shape [batch_size] - the log-perplexity for each sequence.\n    """"""\n    mparams = self._mparams[\'sequence_loss_fn\']\n    with tf.variable_scope(\'sequence_loss_fn/SLF\'):\n      if mparams.label_smoothing > 0:\n        smoothed_one_hot_labels = self.label_smoothing_regularization(\n          chars_labels, mparams.label_smoothing)\n        labels_list = tf.unstack(smoothed_one_hot_labels, axis=1)\n      else:\n        # NOTE: in case of sparse softmax we are not using one-hot\n        # encoding.\n        labels_list = tf.unstack(chars_labels, axis=1)\n\n      batch_size, seq_length, _ = chars_logits.shape.as_list()\n      if mparams.ignore_nulls:\n        weights = tf.ones((batch_size, seq_length), dtype=tf.float32)\n      else:\n        # Suppose that reject character is the last in the charset.\n        reject_char = tf.constant(\n          self._params.num_char_classes - 1,\n          shape=(batch_size, seq_length),\n          dtype=tf.int64)\n        known_char = tf.not_equal(chars_labels, reject_char)\n        weights = tf.to_float(known_char)\n\n      logits_list = tf.unstack(chars_logits, axis=1)\n      weights_list = tf.unstack(weights, axis=1)\n      loss = tf.contrib.legacy_seq2seq.sequence_loss(\n        logits_list,\n        labels_list,\n        weights_list,\n        softmax_loss_function=get_softmax_loss_fn(mparams.label_smoothing),\n        average_across_timesteps=mparams.average_across_timesteps)\n      tf.losses.add_loss(loss)\n      return loss\n\n  def create_summaries(self, data, endpoints, charset, is_training):\n    """"""Creates all summaries for the model.\n\n    Args:\n      data: InputEndpoints namedtuple.\n      endpoints: OutputEndpoints namedtuple.\n      charset: A dictionary with mapping between character codes and\n        unicode characters. Use the one provided by a dataset.charset.\n      is_training: If True will create summary prefixes for training job,\n        otherwise - for evaluation.\n\n    Returns:\n      A list of evaluation ops\n    """"""\n\n    def sname(label):\n      prefix = \'train\' if is_training else \'eval\'\n      return \'%s/%s\' % (prefix, label)\n\n    max_outputs = 4\n    # TODO(gorban): uncomment, when tf.summary.text released.\n    charset_mapper = CharsetMapper(charset)\n    pr_text = charset_mapper.get_text(\n    endpoints.predicted_chars[:max_outputs,:])\n    tf.summary.text(sname(\'text/pr\'), pr_text)\n    gt_text = charset_mapper.get_text(data.labels[:max_outputs,:])\n    tf.summary.text(sname(\'text/gt\'), gt_text)\n    tf.summary.image(sname(\'image\'), data.images, max_outputs=max_outputs)\n\n    if is_training:\n      tf.summary.image(\n        sname(\'image/orig\'), data.images_orig, max_outputs=max_outputs)\n      for var in tf.trainable_variables():\n        tf.summary.histogram(var.op.name, var)\n      return None\n\n    else:\n      names_to_values = {}\n      names_to_updates = {}\n\n      def use_metric(name, value_update_tuple):\n        names_to_values[name] = value_update_tuple[0]\n        names_to_updates[name] = value_update_tuple[1]\n\n      use_metric(\'CharacterAccuracy\',\n                 metrics.char_accuracy(\n                   endpoints.predicted_chars,\n                   data.labels,\n                   streaming=True,\n                   rej_char=self._params.null_code))\n      # Sequence accuracy computed by cutting sequence at the first null char\n      use_metric(\'SequenceAccuracy\',\n                 metrics.sequence_accuracy(\n                   endpoints.predicted_chars,\n                   data.labels,\n                   streaming=True,\n                   rej_char=self._params.null_code))\n\n      for name, value in names_to_values.items():\n        summary_name = \'eval/\' + name\n        tf.summary.scalar(summary_name, tf.Print(value, [value], summary_name))\n      return list(names_to_updates.values())\n\n  def create_init_fn_to_restore(self, master_checkpoint,\n                                inception_checkpoint=None):\n    """"""Creates an init operations to restore weights from various checkpoints.\n\n    Args:\n      master_checkpoint: path to a checkpoint which contains all weights for\n        the whole model.\n      inception_checkpoint: path to a checkpoint which contains weights for the\n        inception part only.\n\n    Returns:\n      a function to run initialization ops.\n    """"""\n    all_assign_ops = []\n    all_feed_dict = {}\n\n    def assign_from_checkpoint(variables, checkpoint):\n      logging.info(\'Request to re-store %d weights from %s\',\n                   len(variables), checkpoint)\n      if not variables:\n        logging.error(\'Can\\\'t find any variables to restore.\')\n        sys.exit(1)\n      assign_op, feed_dict = slim.assign_from_checkpoint(checkpoint, variables)\n      all_assign_ops.append(assign_op)\n      all_feed_dict.update(feed_dict)\n\n    logging.info(\'variables_to_restore:\\n%s\' % utils.variables_to_restore().keys())\n    logging.info(\'moving_average_variables:\\n%s\' % [v.op.name for v in tf.moving_average_variables()])\n    logging.info(\'trainable_variables:\\n%s\' % [v.op.name for v in tf.trainable_variables()])\n    if master_checkpoint:\n      assign_from_checkpoint(utils.variables_to_restore(), master_checkpoint)\n\n    if inception_checkpoint:\n      variables = utils.variables_to_restore(\n        \'AttentionOcr_v1/conv_tower_fn/INCE\', strip_scope=True)\n      assign_from_checkpoint(variables, inception_checkpoint)\n\n    def init_assign_fn(sess):\n      logging.info(\'Restoring checkpoint(s)\')\n      sess.run(all_assign_ops, all_feed_dict)\n\n    return init_assign_fn\n'"
python/model_test.py,32,"b'# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for the model.""""""\n\nimport numpy as np\nfrom six.moves import xrange\nimport string\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport model\nimport data_provider\n\n\ndef create_fake_charset(num_char_classes):\n  charset = {}\n  for i in xrange(num_char_classes):\n    charset[i] = string.printable[i % len(string.printable)]\n  return charset\n\n\nclass ModelTest(tf.test.TestCase):\n  def setUp(self):\n    tf.test.TestCase.setUp(self)\n\n    self.rng = np.random.RandomState([11, 23, 50])\n\n    self.batch_size = 4\n    self.image_width = 600\n    self.image_height = 30\n    self.seq_length = 40\n    self.num_char_classes = 72\n    self.null_code = 62\n    self.num_views = 4\n\n    feature_size = 288\n    self.conv_tower_shape = (self.batch_size, 1, 72, feature_size)\n    self.features_shape = (self.batch_size, self.seq_length, feature_size)\n    self.chars_logit_shape = (self.batch_size, self.seq_length,\n                              self.num_char_classes)\n    self.length_logit_shape = (self.batch_size, self.seq_length + 1)\n\n    self.initialize_fakes()\n\n  def initialize_fakes(self):\n    self.images_shape = (self.batch_size, self.image_height, self.image_width,\n                         3)\n    self.fake_images = tf.constant(\n        self.rng.randint(low=0, high=255,\n                         size=self.images_shape).astype(\'float32\'),\n        name=\'input_node\')\n    self.fake_conv_tower_np = self.rng.randn(\n        *self.conv_tower_shape).astype(\'float32\')\n    self.fake_conv_tower = tf.constant(self.fake_conv_tower_np)\n    self.fake_logits = tf.constant(\n        self.rng.randn(*self.chars_logit_shape).astype(\'float32\'))\n    self.fake_labels = tf.constant(\n        self.rng.randint(\n            low=0,\n            high=self.num_char_classes,\n            size=(self.batch_size, self.seq_length)).astype(\'int64\'))\n\n  def create_model(self, charset=None):\n    return model.Model(\n        self.num_char_classes, self.seq_length, num_views=4, null_code=62,\n        charset=charset)\n\n  def test_char_related_shapes(self):\n    ocr_model = self.create_model()\n    with self.test_session() as sess:\n      endpoints_tf = ocr_model.create_base(\n          images=self.fake_images, labels_one_hot=None)\n\n      sess.run(tf.global_variables_initializer())\n      endpoints = sess.run(endpoints_tf)\n\n      self.assertEqual((self.batch_size, self.seq_length,\n                        self.num_char_classes), endpoints.chars_logit.shape)\n      self.assertEqual((self.batch_size, self.seq_length,\n                        self.num_char_classes), endpoints.chars_log_prob.shape)\n      self.assertEqual((self.batch_size, self.seq_length),\n                       endpoints.predicted_chars.shape)\n      self.assertEqual((self.batch_size, self.seq_length),\n                       endpoints.predicted_scores.shape)\n\n  def test_predicted_scores_are_within_range(self):\n    ocr_model = self.create_model()\n\n    _, _, scores = ocr_model.char_predictions(self.fake_logits)\n    with self.test_session() as sess:\n      scores_np = sess.run(scores)\n\n    values_in_range = (scores_np >= 0.0) & (scores_np <= 1.0)\n    self.assertTrue(\n        np.all(values_in_range),\n        msg=(\'Scores contains out of the range values %s\' %\n             scores_np[np.logical_not(values_in_range)]))\n\n  def test_conv_tower_shape(self):\n    with self.test_session() as sess:\n      ocr_model = self.create_model()\n      conv_tower = ocr_model.conv_tower_fn(self.fake_images)\n\n      sess.run(tf.global_variables_initializer())\n      conv_tower_np = sess.run(conv_tower)\n\n      self.assertEqual(self.conv_tower_shape, conv_tower_np.shape)\n\n  def test_model_size_less_then1_gb(self):\n    # NOTE: Actual amount of memory occupied my TF during training will be at\n    # least 4X times bigger because of space need to store original weights,\n    # updates, gradients and variances. It also depends on the type of used\n    # optimizer.\n    ocr_model = self.create_model()\n    ocr_model.create_base(images=self.fake_images, labels_one_hot=None)\n    with self.test_session() as sess:\n      tfprof_root = tf.profiler.profile(\n          sess.graph,\n          options=tf.profiler.ProfileOptionBuilder.trainable_variables_parameter())\n\n      model_size_bytes = 4 * tfprof_root.total_parameters\n      self.assertLess(model_size_bytes, 1 * 2**30)\n\n  def test_create_summaries_is_runnable(self):\n    ocr_model = self.create_model()\n    data = data_provider.InputEndpoints(\n        images=self.fake_images,\n        images_orig=self.fake_images,\n        labels=self.fake_labels,\n        labels_one_hot=slim.one_hot_encoding(self.fake_labels,\n                                             self.num_char_classes))\n    endpoints = ocr_model.create_base(\n        images=self.fake_images, labels_one_hot=None)\n    charset = create_fake_charset(self.num_char_classes)\n    summaries = ocr_model.create_summaries(\n        data, endpoints, charset, is_training=False)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      sess.run(tf.local_variables_initializer())\n      tf.tables_initializer().run()\n      sess.run(summaries)  # just check it is runnable\n\n  def test_sequence_loss_function_without_label_smoothing(self):\n    model = self.create_model()\n    model.set_mparam(\'sequence_loss_fn\', label_smoothing=0)\n\n    loss = model.sequence_loss_fn(self.fake_logits, self.fake_labels)\n    with self.test_session() as sess:\n      loss_np = sess.run(loss)\n\n    # This test checks that the loss function is \'runnable\'.\n    self.assertEqual(loss_np.shape, tuple())\n\n  def encode_coordinates_alt(self, net):\n    """"""An alternative implemenation for the encoding coordinates.\n\n    Args:\n      net: a tensor of shape=[batch_size, height, width, num_features]\n\n    Returns:\n      a list of tensors with encoded image coordinates in them.\n    """"""\n    batch_size, h, w, _ = net.shape.as_list()\n    h_loc = [\n      tf.tile(\n          tf.reshape(\n              tf.contrib.layers.one_hot_encoding(\n                  tf.constant([i]), num_classes=h), [h, 1]), [1, w])\n      for i in xrange(h)\n    ]\n    h_loc = tf.concat([tf.expand_dims(t, 2) for t in h_loc], 2)\n    w_loc = [\n      tf.tile(\n          tf.contrib.layers.one_hot_encoding(tf.constant([i]), num_classes=w),\n          [h, 1]) for i in xrange(w)\n    ]\n    w_loc = tf.concat([tf.expand_dims(t, 2) for t in w_loc], 2)\n    loc = tf.concat([h_loc, w_loc], 2)\n    loc = tf.tile(tf.expand_dims(loc, 0), [batch_size, 1, 1, 1])\n    return tf.concat([net, loc], 3)\n\n  def test_encoded_coordinates_have_correct_shape(self):\n    model = self.create_model()\n    model.set_mparam(\'encode_coordinates_fn\', enabled=True)\n    conv_w_coords_tf = model.encode_coordinates_fn(self.fake_conv_tower)\n\n    with self.test_session() as sess:\n      conv_w_coords = sess.run(conv_w_coords_tf)\n\n    batch_size, height, width, feature_size = self.conv_tower_shape\n    self.assertEqual(conv_w_coords.shape, (batch_size, height, width,\n                                           feature_size + height + width))\n\n  def test_disabled_coordinate_encoding_returns_features_unchanged(self):\n    model = self.create_model()\n    model.set_mparam(\'encode_coordinates_fn\', enabled=False)\n    conv_w_coords_tf = model.encode_coordinates_fn(self.fake_conv_tower)\n\n    with self.test_session() as sess:\n      conv_w_coords = sess.run(conv_w_coords_tf)\n\n    self.assertAllEqual(conv_w_coords, self.fake_conv_tower_np)\n\n  def test_coordinate_encoding_is_correct_for_simple_example(self):\n    shape = (1, 2, 3, 4)  # batch_size, height, width, feature_size\n    fake_conv_tower = tf.constant(2 * np.ones(shape), dtype=tf.float32)\n    model = self.create_model()\n    model.set_mparam(\'encode_coordinates_fn\', enabled=True)\n    conv_w_coords_tf = model.encode_coordinates_fn(fake_conv_tower)\n\n    with self.test_session() as sess:\n      conv_w_coords = sess.run(conv_w_coords_tf)\n\n    # Original features\n    self.assertAllEqual(conv_w_coords[0, :, :, :4],\n                        [[[2, 2, 2, 2], [2, 2, 2, 2], [2, 2, 2, 2]],\n                         [[2, 2, 2, 2], [2, 2, 2, 2], [2, 2, 2, 2]]])\n    # Encoded coordinates\n    self.assertAllEqual(conv_w_coords[0, :, :, 4:],\n                        [[[1, 0, 1, 0, 0], [1, 0, 0, 1, 0], [1, 0, 0, 0, 1]],\n                         [[0, 1, 1, 0, 0], [0, 1, 0, 1, 0], [0, 1, 0, 0, 1]]])\n\n  def test_alt_implementation_of_coordinate_encoding_returns_same_values(self):\n    model = self.create_model()\n    model.set_mparam(\'encode_coordinates_fn\', enabled=True)\n    conv_w_coords_tf = model.encode_coordinates_fn(self.fake_conv_tower)\n    conv_w_coords_alt_tf = self.encode_coordinates_alt(self.fake_conv_tower)\n\n    with self.test_session() as sess:\n      conv_w_coords_tf, conv_w_coords_alt_tf = sess.run(\n          [conv_w_coords_tf, conv_w_coords_alt_tf])\n\n    self.assertAllEqual(conv_w_coords_tf, conv_w_coords_alt_tf)\n\n  def test_predicted_text_has_correct_shape_w_charset(self):\n    charset = create_fake_charset(self.num_char_classes)\n    ocr_model = self.create_model(charset=charset)\n\n    with self.test_session() as sess:\n      endpoints_tf = ocr_model.create_base(\n          images=self.fake_images, labels_one_hot=None)\n\n      sess.run(tf.global_variables_initializer())\n      tf.tables_initializer().run()\n      endpoints = sess.run(endpoints_tf)\n\n      self.assertEqual(endpoints.predicted_text.shape, (self.batch_size,))\n      self.assertEqual(len(endpoints.predicted_text[0]), self.seq_length)\n\n\nclass CharsetMapperTest(tf.test.TestCase):\n  def test_text_corresponds_to_ids(self):\n    charset = create_fake_charset(36)\n    ids = tf.constant(\n        [[17, 14, 21, 21, 24], [32, 24, 27, 21, 13]], dtype=tf.int64)\n    charset_mapper = model.CharsetMapper(charset)\n\n    with self.test_session() as sess:\n      tf.tables_initializer().run()\n      text = sess.run(charset_mapper.get_text(ids))\n\n    self.assertAllEqual(text, [\'hello\', \'world\'])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
python/sequence_layers.py,19,"b'# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Various implementations of sequence layers for character prediction.\n\nA \'sequence layer\' is a part of a computation graph which is responsible of\nproducing a sequence of characters using extracted image features. There are\nmany reasonable ways to implement such layers. All of them are using RNNs.\nThis module provides implementations which uses \'attention\' mechanism to\nspatially \'pool\' image features and also can use a previously predicted\ncharacter to predict the next (aka auto regression).\n\nUsage:\n  Select one of available classes, e.g. Attention or use a wrapper function to\n  pick one based on your requirements:\n  layer_class = sequence_layers.get_layer_class(use_attention=True,\n                                                use_autoregression=True)\n  layer = layer_class(net, labels_one_hot, model_params, method_params)\n  char_logits = layer.create_logits()\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport abc\nimport logging\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom tensorflow.contrib import slim\n\n\ndef orthogonal_initializer(shape, dtype=tf.float32, *args, **kwargs):\n  """"""Generates orthonormal matrices with random values.\n\n  Orthonormal initialization is important for RNNs:\n    http://arxiv.org/abs/1312.6120\n    http://smerity.com/articles/2016/orthogonal_init.html\n\n  For non-square shapes the returned matrix will be semi-orthonormal: if the\n  number of columns exceeds the number of rows, then the rows are orthonormal\n  vectors; but if the number of rows exceeds the number of columns, then the\n  columns are orthonormal vectors.\n\n  We use SVD decomposition to generate an orthonormal matrix with random\n  values. The same way as it is done in the Lasagne library for Theano. Note\n  that both u and v returned by the svd are orthogonal and random. We just need\n  to pick one with the right shape.\n\n  Args:\n    shape: a shape of the tensor matrix to initialize.\n    dtype: a dtype of the initialized tensor.\n    *args: not used.\n    **kwargs: not used.\n\n  Returns:\n    An initialized tensor.\n  """"""\n  del args\n  del kwargs\n  flat_shape = (shape[0], np.prod(shape[1:]))\n  w = np.random.randn(*flat_shape)\n  u, _, v = np.linalg.svd(w, full_matrices=False)\n  w = u if u.shape == flat_shape else v\n  return tf.constant(w.reshape(shape), dtype=dtype)\n\n\nSequenceLayerParams = collections.namedtuple(\'SequenceLogitsParams\', [\n    \'num_lstm_units\', \'weight_decay\', \'lstm_state_clip_value\'\n])\n\n\nclass SequenceLayerBase(object):\n  """"""A base abstruct class for all sequence layers.\n\n  A child class has to define following methods:\n    get_train_input\n    get_eval_input\n    unroll_cell\n  """"""\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self, net, labels_one_hot, model_params, method_params):\n    """"""Stores argument in member variable for further use.\n\n    Args:\n      net: A tensor with shape [batch_size, num_features, feature_size] which\n        contains some extracted image features.\n      labels_one_hot: An optional (can be None) ground truth labels for the\n        input features. Is a tensor with shape\n        [batch_size, seq_length, num_char_classes]\n      model_params: A namedtuple with model parameters (model.ModelParams).\n      method_params: A SequenceLayerParams instance.\n    """"""\n    self._params = model_params\n    self._mparams = method_params\n    self._net = net\n    self._labels_one_hot = labels_one_hot\n    self._batch_size = net.get_shape().dims[0].value\n\n    # Initialize parameters for char logits which will be computed on the fly\n    # inside an LSTM decoder.\n    self._char_logits = {}\n    regularizer = slim.l2_regularizer(self._mparams.weight_decay)\n    self._softmax_w = slim.model_variable(\n        \'softmax_w\',\n        [self._mparams.num_lstm_units, self._params.num_char_classes],\n        initializer=orthogonal_initializer,\n        regularizer=regularizer)\n    self._softmax_b = slim.model_variable(\n        \'softmax_b\', [self._params.num_char_classes],\n        initializer=tf.zeros_initializer(),\n        regularizer=regularizer)\n\n  @abc.abstractmethod\n  def get_train_input(self, prev, i):\n    """"""Returns a sample to be used to predict a character during training.\n\n    This function is used as a loop_function for an RNN decoder.\n\n    Args:\n      prev: output tensor from previous step of the RNN. A tensor with shape:\n        [batch_size, num_char_classes].\n      i: index of a character in the output sequence.\n\n    Returns:\n      A tensor with shape [batch_size, ?] - depth depends on implementation\n      details.\n    """"""\n    pass\n\n  @abc.abstractmethod\n  def get_eval_input(self, prev, i):\n    """"""Returns a sample to be used to predict a character during inference.\n\n    This function is used as a loop_function for an RNN decoder.\n\n    Args:\n      prev: output tensor from previous step of the RNN. A tensor with shape:\n        [batch_size, num_char_classes].\n      i: index of a character in the output sequence.\n\n    Returns:\n      A tensor with shape [batch_size, ?] - depth depends on implementation\n      details.\n    """"""\n    raise AssertionError(\'Not implemented\')\n\n  @abc.abstractmethod\n  def unroll_cell(self, decoder_inputs, initial_state, loop_function, cell):\n    """"""Unrolls an RNN cell for all inputs.\n\n    This is a placeholder to call some RNN decoder. It has a similar to\n    tf.seq2seq.rnn_decode interface.\n\n    Args:\n      decoder_inputs: A list of 2D Tensors* [batch_size x input_size]. In fact,\n        most of existing decoders in presence of a loop_function use only the\n        first element to determine batch_size and length of the list to\n        determine number of steps.\n      initial_state: 2D Tensor with shape [batch_size x cell.state_size].\n      loop_function: function will be applied to the i-th output in order to\n        generate the i+1-st input (see self.get_input).\n      cell: rnn_cell.RNNCell defining the cell function and size.\n\n    Returns:\n      A tuple of the form (outputs, state), where:\n        outputs: A list of character logits of the same length as\n        decoder_inputs of 2D Tensors with shape [batch_size x num_characters].\n        state: The state of each cell at the final time-step.\n          It is a 2D Tensor of shape [batch_size x cell.state_size].\n    """"""\n    pass\n\n  def is_training(self):\n    """"""Returns True if the layer is created for training stage.""""""\n    return self._labels_one_hot is not None\n\n  def char_logit(self, inputs, char_index):\n    """"""Creates logits for a character if required.\n\n    Args:\n      inputs: A tensor with shape [batch_size, ?] (depth is implementation\n        dependent).\n      char_index: A integer index of a character in the output sequence.\n\n    Returns:\n      A tensor with shape [batch_size, num_char_classes]\n    """"""\n    if char_index not in self._char_logits:\n      self._char_logits[char_index] = tf.nn.xw_plus_b(inputs, self._softmax_w,\n                                                      self._softmax_b)\n    return self._char_logits[char_index]\n\n  def char_one_hot(self, logit):\n    """"""Creates one hot encoding for a logit of a character.\n\n    Args:\n      logit: A tensor with shape [batch_size, num_char_classes].\n\n    Returns:\n      A tensor with shape [batch_size, num_char_classes]\n    """"""\n    prediction = tf.argmax(logit, axis=1)\n    return slim.one_hot_encoding(prediction, self._params.num_char_classes)\n\n  def get_input(self, prev, i):\n    """"""A wrapper for get_train_input and get_eval_input.\n\n    Args:\n      prev: output tensor from previous step of the RNN. A tensor with shape:\n        [batch_size, num_char_classes].\n      i: index of a character in the output sequence.\n\n    Returns:\n      A tensor with shape [batch_size, ?] - depth depends on implementation\n      details.\n    """"""\n    if self.is_training():\n      return self.get_train_input(prev, i)\n    else:\n      return self.get_eval_input(prev, i)\n\n  def create_logits(self):\n    """"""Creates character sequence logits for a net specified in the constructor.\n\n    A ""main"" method for the sequence layer which glues together all pieces.\n\n    Returns:\n      A tensor with shape [batch_size, seq_length, num_char_classes].\n    """"""\n    with tf.variable_scope(\'LSTM\'):\n      first_label = self.get_input(prev=None, i=0)\n      decoder_inputs = [first_label] + [None] * (self._params.seq_length - 1)\n      lstm_cell = tf.contrib.rnn.LSTMCell(\n          self._mparams.num_lstm_units,\n          use_peepholes=False,\n          cell_clip=self._mparams.lstm_state_clip_value,\n          state_is_tuple=True,\n          initializer=orthogonal_initializer)\n      lstm_outputs, _ = self.unroll_cell(\n          decoder_inputs=decoder_inputs,\n          initial_state=lstm_cell.zero_state(self._batch_size, tf.float32),\n          loop_function=self.get_input,\n          cell=lstm_cell)\n\n    with tf.variable_scope(\'logits\'):\n      logits_list = [\n          tf.expand_dims(self.char_logit(logit, i), dim=1)\n          for i, logit in enumerate(lstm_outputs)\n      ]\n\n    return tf.concat(logits_list, 1)\n\n\nclass NetSlice(SequenceLayerBase):\n  """"""A layer which uses a subset of image features to predict each character.\n  """"""\n\n  def __init__(self, *args, **kwargs):\n    super(NetSlice, self).__init__(*args, **kwargs)\n    self._zero_label = tf.zeros(\n        [self._batch_size, self._params.num_char_classes])\n\n  def get_image_feature(self, char_index):\n    """"""Returns a subset of image features for a character.\n\n    Args:\n      char_index: an index of a character.\n\n    Returns:\n      A tensor with shape [batch_size, ?]. The output depth depends on the\n      depth of input net.\n    """"""\n    batch_size, features_num, _ = [d.value for d in self._net.get_shape()]\n    slice_len = int(features_num / self._params.seq_length)\n    # In case when features_num != seq_length, we just pick a subset of image\n    # features, this choice is arbitrary and there is no intuitive geometrical\n    # interpretation. If features_num is not dividable by seq_length there will\n    # be unused image features.\n    net_slice = self._net[:, char_index:char_index + slice_len, :]\n    feature = tf.reshape(net_slice, [batch_size, -1])\n    logging.debug(\'Image feature: %s\', feature)\n    return feature\n\n  def get_eval_input(self, prev, i):\n    """"""See SequenceLayerBase.get_eval_input for details.""""""\n    del prev\n    return self.get_image_feature(i)\n\n  def get_train_input(self, prev, i):\n    """"""See SequenceLayerBase.get_train_input for details.""""""\n    return self.get_eval_input(prev, i)\n\n  def unroll_cell(self, decoder_inputs, initial_state, loop_function, cell):\n    """"""See SequenceLayerBase.unroll_cell for details.""""""\n    return tf.contrib.legacy_seq2seq.rnn_decoder(\n        decoder_inputs=decoder_inputs,\n        initial_state=initial_state,\n        cell=cell,\n        loop_function=self.get_input)\n\n\nclass NetSliceWithAutoregression(NetSlice):\n  """"""A layer similar to NetSlice, but it also uses auto regression.\n\n  The ""auto regression"" means that we use network output for previous character\n  as a part of input for the current character.\n  """"""\n\n  def __init__(self, *args, **kwargs):\n    super(NetSliceWithAutoregression, self).__init__(*args, **kwargs)\n\n  def get_eval_input(self, prev, i):\n    """"""See SequenceLayerBase.get_eval_input for details.""""""\n    if i == 0:\n      prev = self._zero_label\n    else:\n      logit = self.char_logit(prev, char_index=i - 1)\n      prev = self.char_one_hot(logit)\n    image_feature = self.get_image_feature(char_index=i)\n    return tf.concat([image_feature, prev], 1)\n\n  def get_train_input(self, prev, i):\n    """"""See SequenceLayerBase.get_train_input for details.""""""\n    if i == 0:\n      prev = self._zero_label\n    else:\n      prev = self._labels_one_hot[:, i - 1, :]\n    image_feature = self.get_image_feature(i)\n    return tf.concat([image_feature, prev], 1)\n\n\nclass Attention(SequenceLayerBase):\n  """"""A layer which uses attention mechanism to select image features.""""""\n\n  def __init__(self, *args, **kwargs):\n    super(Attention, self).__init__(*args, **kwargs)\n    self._zero_label = tf.zeros(\n        [self._batch_size, self._params.num_char_classes])\n\n  def get_eval_input(self, prev, i):\n    """"""See SequenceLayerBase.get_eval_input for details.""""""\n    del prev, i\n    # The attention_decoder will fetch image features from the net, no need for\n    # extra inputs.\n    return self._zero_label\n\n  def get_train_input(self, prev, i):\n    """"""See SequenceLayerBase.get_train_input for details.""""""\n    return self.get_eval_input(prev, i)\n\n  def unroll_cell(self, decoder_inputs, initial_state, loop_function, cell):\n    return tf.contrib.legacy_seq2seq.attention_decoder(\n        decoder_inputs=decoder_inputs,\n        initial_state=initial_state,\n        attention_states=self._net,\n        cell=cell,\n        loop_function=self.get_input)\n\n\nclass AttentionWithAutoregression(Attention):\n  """"""A layer which uses both attention and auto regression.""""""\n\n  def __init__(self, *args, **kwargs):\n    super(AttentionWithAutoregression, self).__init__(*args, **kwargs)\n\n  def get_train_input(self, prev, i):\n    """"""See SequenceLayerBase.get_train_input for details.""""""\n    if i == 0:\n      return self._zero_label\n    else:\n      # TODO(gorban): update to gradually introduce gt labels.\n      return self._labels_one_hot[:, i - 1, :]\n\n  def get_eval_input(self, prev, i):\n    """"""See SequenceLayerBase.get_eval_input for details.""""""\n    if i == 0:\n      return self._zero_label\n    else:\n      logit = self.char_logit(prev, char_index=i - 1)\n      return self.char_one_hot(logit)\n\n\ndef get_layer_class(use_attention, use_autoregression):\n  """"""A convenience function to get a layer class based on requirements.\n\n  Args:\n    use_attention: if True a returned class will use attention.\n    use_autoregression: if True a returned class will use auto regression.\n\n  Returns:\n    One of available sequence layers (child classes for SequenceLayerBase).\n  """"""\n  if use_attention and use_autoregression:\n    layer_class = AttentionWithAutoregression\n  elif use_attention and not use_autoregression:\n    layer_class = Attention\n  elif not use_attention and not use_autoregression:\n    layer_class = NetSlice\n  elif not use_attention and use_autoregression:\n    layer_class = NetSliceWithAutoregression\n  else:\n    raise AssertionError(\'Unsupported sequence layer class\')\n\n  logging.debug(\'Use %s as a layer class\', layer_class.__name__)\n  return layer_class\n'"
python/sequence_layers_test.py,9,"b'# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for sequence_layers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport model\nimport sequence_layers\n\n\ndef fake_net(batch_size, num_features, feature_size):\n  return tf.convert_to_tensor(\n      np.random.uniform(size=(batch_size, num_features, feature_size)),\n      dtype=tf.float32)\n\n\ndef fake_labels(batch_size, seq_length, num_char_classes):\n  labels_np = tf.convert_to_tensor(\n      np.random.randint(\n          low=0, high=num_char_classes, size=(batch_size, seq_length)))\n  return slim.one_hot_encoding(labels_np, num_classes=num_char_classes)\n\n\ndef create_layer(layer_class, batch_size, seq_length, num_char_classes):\n  model_params = model.ModelParams(\n      num_char_classes=num_char_classes,\n      seq_length=seq_length,\n      num_views=1,\n      null_code=num_char_classes)\n  net = fake_net(\n      batch_size=batch_size, num_features=seq_length * 5, feature_size=6)\n  labels_one_hot = fake_labels(batch_size, seq_length, num_char_classes)\n  layer_params = sequence_layers.SequenceLayerParams(\n      num_lstm_units=10, weight_decay=0.00004, lstm_state_clip_value=10.0)\n  return layer_class(net, labels_one_hot, model_params, layer_params)\n\n\nclass SequenceLayersTest(tf.test.TestCase):\n  def test_net_slice_char_logits_with_correct_shape(self):\n    batch_size = 2\n    seq_length = 4\n    num_char_classes = 3\n\n    layer = create_layer(sequence_layers.NetSlice, batch_size, seq_length,\n                         num_char_classes)\n    char_logits = layer.create_logits()\n\n    self.assertEqual(\n        tf.TensorShape([batch_size, seq_length, num_char_classes]),\n        char_logits.get_shape())\n\n  def test_net_slice_with_autoregression_char_logits_with_correct_shape(self):\n    batch_size = 2\n    seq_length = 4\n    num_char_classes = 3\n\n    layer = create_layer(sequence_layers.NetSliceWithAutoregression,\n                         batch_size, seq_length, num_char_classes)\n    char_logits = layer.create_logits()\n\n    self.assertEqual(\n        tf.TensorShape([batch_size, seq_length, num_char_classes]),\n        char_logits.get_shape())\n\n  def test_attention_char_logits_with_correct_shape(self):\n    batch_size = 2\n    seq_length = 4\n    num_char_classes = 3\n\n    layer = create_layer(sequence_layers.Attention, batch_size, seq_length,\n                         num_char_classes)\n    char_logits = layer.create_logits()\n\n    self.assertEqual(\n        tf.TensorShape([batch_size, seq_length, num_char_classes]),\n        char_logits.get_shape())\n\n  def test_attention_with_autoregression_char_logits_with_correct_shape(self):\n    batch_size = 2\n    seq_length = 4\n    num_char_classes = 3\n\n    layer = create_layer(sequence_layers.AttentionWithAutoregression,\n                         batch_size, seq_length, num_char_classes)\n    char_logits = layer.create_logits()\n\n    self.assertEqual(\n        tf.TensorShape([batch_size, seq_length, num_char_classes]),\n        char_logits.get_shape())\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
python/train.py,14,"b'# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Script to train the Attention OCR model.\n\nA simple usage example:\npython train.py\n""""""\nimport os\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = \'\'\nimport collections\nimport logging\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nfrom tensorflow import app\nfrom tensorflow.python.platform import flags\nfrom tensorflow.contrib.tfprof import model_analyzer\n\nimport data_provider\nimport common_flags\n\nFLAGS = flags.FLAGS\ncommon_flags.define()\n\n# yapf: disable\nflags.DEFINE_integer(\'task\', 0,\n                     \'The Task ID. This value is used when training with \'\n                     \'multiple workers to identify each worker.\')\n\nflags.DEFINE_integer(\'ps_tasks\', 0,\n                     \'The number of parameter servers. If the value is 0, then\'\n                     \' the parameters are handled locally by the worker.\')\n\nflags.DEFINE_integer(\'save_summaries_secs\', 60,\n                     \'The frequency with which summaries are saved, in \'\n                     \'seconds.\')\n\nflags.DEFINE_integer(\'save_interval_secs\', 600,\n                     \'Frequency in seconds of saving the model.\')\n\nflags.DEFINE_integer(\'max_number_of_steps\', int(1e10),\n                     \'The maximum number of gradient steps.\')\n\nflags.DEFINE_string(\'checkpoint_inception\', \'\',\n                    \'Checkpoint to recover inception weights from.\')\n\nflags.DEFINE_float(\'clip_gradient_norm\', 2.0,\n                   \'If greater than 0 then the gradients would be clipped by \'\n                   \'it.\')\n\nflags.DEFINE_bool(\'sync_replicas\', False,\n                  \'If True will synchronize replicas during training.\')\n\nflags.DEFINE_integer(\'replicas_to_aggregate\', 1,\n                     \'The number of gradients updates before updating params.\')\n\nflags.DEFINE_integer(\'total_num_replicas\', 1,\n                     \'Total number of worker replicas.\')\n\nflags.DEFINE_integer(\'startup_delay_steps\', 15,\n                     \'Number of training steps between replicas startup.\')\n\nflags.DEFINE_boolean(\'reset_train_dir\', False,\n                     \'If true will delete all files in the train_log_dir\')\n\nflags.DEFINE_boolean(\'show_graph_stats\', False,\n                     \'Output model size stats to stderr.\')\n# yapf: enable\n\nTrainingHParams = collections.namedtuple(\'TrainingHParams\', [\n    \'learning_rate\',\n    \'optimizer\',\n    \'momentum\',\n    \'use_augment_input\',\n])\n\n\ndef get_training_hparams():\n  return TrainingHParams(\n      learning_rate=FLAGS.learning_rate,\n      optimizer=FLAGS.optimizer,\n      momentum=FLAGS.momentum,\n      use_augment_input=FLAGS.use_augment_input)\n\n\ndef create_optimizer(hparams):\n  """"""Creates optimized based on the specified flags.""""""\n  if hparams.optimizer == \'momentum\':\n    optimizer = tf.train.MomentumOptimizer(\n        hparams.learning_rate, momentum=hparams.momentum)\n  elif hparams.optimizer == \'adam\':\n    optimizer = tf.train.AdamOptimizer(hparams.learning_rate)\n  elif hparams.optimizer == \'adadelta\':\n    optimizer = tf.train.AdadeltaOptimizer(hparams.learning_rate)\n  elif hparams.optimizer == \'adagrad\':\n    optimizer = tf.train.AdagradOptimizer(hparams.learning_rate)\n  elif hparams.optimizer == \'rmsprop\':\n    optimizer = tf.train.RMSPropOptimizer(\n        hparams.learning_rate, momentum=hparams.momentum)\n  return optimizer\n\n\ndef train(loss, init_fn, hparams):\n  """"""Wraps slim.learning.train to run a training loop.\n\n  Args:\n    loss: a loss tensor\n    init_fn: A callable to be executed after all other initialization is done.\n    hparams: a model hyper parameters\n  """"""\n  optimizer = create_optimizer(hparams)\n\n  if FLAGS.sync_replicas:\n    replica_id = tf.constant(FLAGS.task, tf.int32, shape=())\n    optimizer = tf.LegacySyncReplicasOptimizer(\n        opt=optimizer,\n        replicas_to_aggregate=FLAGS.replicas_to_aggregate,\n        replica_id=replica_id,\n        total_num_replicas=FLAGS.total_num_replicas)\n    sync_optimizer = optimizer\n    startup_delay_steps = 0\n  else:\n    startup_delay_steps = 0\n    sync_optimizer = None\n\n  train_op = slim.learning.create_train_op(\n      loss,\n      optimizer,\n      summarize_gradients=True,\n      clip_gradient_norm=FLAGS.clip_gradient_norm)\n\n  slim.learning.train(\n      train_op=train_op,\n      logdir=FLAGS.train_log_dir,\n      graph=loss.graph,\n      master=FLAGS.master,\n      is_chief=(FLAGS.task == 0),\n      number_of_steps=FLAGS.max_number_of_steps,\n      save_summaries_secs=FLAGS.save_summaries_secs,\n      save_interval_secs=FLAGS.save_interval_secs,\n      startup_delay_steps=startup_delay_steps,\n      sync_optimizer=sync_optimizer,\n      init_fn=init_fn)\n\n\ndef prepare_training_dir():\n  if not tf.gfile.Exists(FLAGS.train_log_dir):\n    logging.info(\'Create a new training directory %s\', FLAGS.train_log_dir)\n    tf.gfile.MakeDirs(FLAGS.train_log_dir)\n  else:\n    if FLAGS.reset_train_dir:\n      logging.info(\'Reset the training directory %s\', FLAGS.train_log_dir)\n      tf.gfile.DeleteRecursively(FLAGS.train_log_dir)\n      tf.gfile.MakeDirs(FLAGS.train_log_dir)\n    else:\n      logging.info(\'Use already existing training directory %s\',\n                   FLAGS.train_log_dir)\n\n\ndef calculate_graph_metrics():\n  param_stats = model_analyzer.print_model_analysis(\n      tf.get_default_graph(),\n      tfprof_options=model_analyzer.TRAINABLE_VARS_PARAMS_STAT_OPTIONS)\n  return param_stats.total_parameters\n\n\ndef main(_):\n  prepare_training_dir()\n\n  dataset = common_flags.create_dataset(split_name=FLAGS.split_name)\n  model = common_flags.create_model(dataset.num_char_classes,\n                                    dataset.max_sequence_length,\n                                    dataset.num_of_views, dataset.null_code)\n  hparams = get_training_hparams()\n\n  # If ps_tasks is zero, the local device is used. When using multiple\n  # (non-local) replicas, the ReplicaDeviceSetter distributes the variables\n  # across the different devices.\n  device_setter = tf.train.replica_device_setter(\n      FLAGS.ps_tasks, merge_devices=True)\n  with tf.device(device_setter):\n    data = data_provider.get_data(\n        dataset,\n        FLAGS.batch_size,\n        augment=hparams.use_augment_input,\n        central_crop_size=common_flags.get_crop_size())\n    endpoints = model.create_base(data.images, data.labels_one_hot)\n    total_loss = model.create_loss(data, endpoints)\n    model.create_summaries(data, endpoints, dataset.charset, is_training=True)\n    init_fn = model.create_init_fn_to_restore(FLAGS.checkpoint,\n                                              FLAGS.checkpoint_inception)\n    if FLAGS.show_graph_stats:\n      logging.info(\'Total number of weights in the graph: %s\',\n                   calculate_graph_metrics())\n    train(total_loss, init_fn, hparams)\n\n\nif __name__ == \'__main__\':\n  app.run()\n'"
python/utils.py,6,"b'# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Functions to support building models for StreetView text transcription.""""""\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\n\ndef logits_to_log_prob(logits):\n  """"""Computes log probabilities using numerically stable trick.\n\n  This uses two numerical stability tricks:\n  1) softmax(x) = softmax(x - c) where c is a constant applied to all\n  arguments. If we set c = max(x) then the softmax is more numerically\n  stable.\n  2) log softmax(x) is not numerically stable, but we can stabilize it\n  by using the identity log softmax(x) = x - log sum exp(x)\n\n  Args:\n    logits: Tensor of arbitrary shape whose last dimension contains logits.\n\n  Returns:\n    A tensor of the same shape as the input, but with corresponding log\n    probabilities.\n  """"""\n\n  with tf.variable_scope(\'log_probabilities\'):\n    reduction_indices = len(logits.shape.as_list()) - 1\n    max_logits = tf.reduce_max(\n        logits, reduction_indices=reduction_indices, keep_dims=True)\n    safe_logits = tf.subtract(logits, max_logits)\n    sum_exp = tf.reduce_sum(\n        tf.exp(safe_logits),\n        reduction_indices=reduction_indices,\n        keep_dims=True)\n    log_probs = tf.subtract(safe_logits, tf.log(sum_exp))\n  return log_probs\n\n\ndef variables_to_restore(scope=None, strip_scope=False):\n  """"""Returns a list of variables to restore for the specified list of methods.\n\n  It is supposed that variable name starts with the method\'s scope (a prefix\n  returned by _method_scope function).\n\n  Args:\n    methods_names: a list of names of configurable methods.\n    strip_scope: if True will return variable names without method\'s scope.\n      If methods_names is None will return names unchanged.\n    model_scope: a scope for a whole model.\n\n  Returns:\n    a dictionary mapping variable names to variables for restore.\n  """"""\n  if scope:\n    variable_map = {}\n    method_variables = slim.get_variables_to_restore(include=[scope])\n    for var in method_variables:\n      if strip_scope:\n        var_name = var.op.name[len(scope) + 1:]\n      else:\n        var_name = var.op.name\n      variable_map[var_name] = var\n\n    return variable_map\n  else:\n    return {v.op.name: v for v in slim.get_variables_to_restore()}\n'"
python/datasets/__init__.py,0,"b'# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n# import sys\n# sys.path.insert(0, ""./"")\nimport datasets.fsns\nimport datasets.fsns_test\nimport datasets.newtextdataset\n\n__all__ = [fsns, fsns_test]\n'"
python/datasets/fsns.py,11,"b'# -*- coding: utf-8 -*-\n# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Configuration to read FSNS dataset https://goo.gl/3Ldm8v.""""""\n\nimport os\nimport re\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nimport logging\n\nDEFAULT_DATASET_DIR = os.path.join(os.path.dirname(__file__), \'data/fsns\')\n\n# The dataset configuration, should be used only as a default value.\nDEFAULT_CONFIG = {\n    \'name\': \'FSNS\',\n    \'splits\': {\n        \'train\': {\n            \'size\': 1044868,\n            \'pattern\': \'train/train*\'\n        },\n        \'test\': {\n            \'size\': 20404,\n            \'pattern\': \'test/test*\'\n        },\n        \'validation\': {\n            \'size\': 16150,\n            \'pattern\': \'validation/validation*\'\n        }\n    },\n    \'charset_filename\': \'dic.txt\',\n    \'image_shape\': (150,600,3),\n    \'num_of_views\': 1,\n    \'max_sequence_length\': 37,\n    \'null_code\': 5462,\n    \'items_to_descriptions\': {\n        \'image\': \'A [150 x 600 x 3] color image.\',\n        \'label\': \'Characters codes.\',\n        \'text\': \'A unicode string.\',\n        \'length\': \'A length of the encoded text.\',\n        \'num_of_views\': \'A number of different views stored within the image.\'\n    }\n}\n\n\ndef read_charset(filename, null_character=u\'\\u2591\'):\n\n  """"""Reads a charset definition from a tab separated text file.\n\n  charset file has to have format compatible with the FSNS dataset.\n\n  Args:\n    filename: a path to the charset file.\n    null_character: a unicode character used to replace \'<null>\' character. the\n      default value is a light shade block \'\xe2\x96\x91\'.\n\n  Returns:\n    a dictionary with keys equal to character codes and values - unicode\n    characters.\n  """"""\n  pattern = re.compile(r\'(\\d+)\\t(.+)\')\n  charset = {}\n  with tf.gfile.GFile(filename) as f:\n    for i, line in enumerate(f):\n      m = pattern.match(line)\n      if m is None:\n        logging.warning(\'incorrect charset file. line #%d: %s\', i, line)\n        continue\n      code = int(m.group(1))\n      #char = m.group(2).decode(\'utf-8\')\n      char = m.group(2)\n      #print(char)\n      if char == \'<nul>\':\n        char = null_character\n      charset[code] = char\n  return charset\n\n\nclass _NumOfViewsHandler(slim.tfexample_decoder.ItemHandler):\n  """"""Convenience handler to determine number of views stored in an image.""""""\n\n  def __init__(self, width_key, original_width_key, num_of_views):\n    super(_NumOfViewsHandler, self).__init__([width_key, original_width_key])\n    self._width_key = width_key\n    self._original_width_key = original_width_key\n    self._num_of_views = num_of_views\n\n  def tensors_to_item(self, keys_to_tensors):\n    return tf.to_int64(\n        self._num_of_views * keys_to_tensors[self._original_width_key] /\n        keys_to_tensors[self._width_key])\n\n\ndef get_split(split_name, dataset_dir=None, config=None):\n  """"""Returns a dataset tuple for FSNS dataset.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources, by default it uses\n      a predefined CNS path (see DEFAULT_DATASET_DIR).\n    config: A dictionary with dataset configuration. If None - will use the\n      DEFAULT_CONFIG.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  """"""\n  if not dataset_dir:\n    dataset_dir = DEFAULT_DATASET_DIR\n\n  if not config:\n    config = DEFAULT_CONFIG\n\n  if split_name not in config[\'splits\']:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  logging.info(\'Using %s dataset split_name=%s dataset_dir=%s\', config[\'name\'],\n               split_name, dataset_dir)\n\n  # Ignores the \'image/height\' feature.\n  zero = tf.zeros([1], dtype=tf.int64)\n  keys_to_features = {\n      \'image/encoded\':\n      tf.FixedLenFeature((), tf.string, default_value=\'\'),\n      \'image/format\':\n      tf.FixedLenFeature((), tf.string, default_value=\'png\'),\n      \'image/width\':\n      tf.FixedLenFeature([1], tf.int64, default_value=zero),\n      \'image/orig_width\':\n      tf.FixedLenFeature([1], tf.int64, default_value=zero),\n      \'image/class\':\n      tf.FixedLenFeature([config[\'max_sequence_length\']], tf.int64),\n      \'image/unpadded_class\':\n      tf.VarLenFeature(tf.int64),\n      \'image/text\':\n      tf.FixedLenFeature([1], tf.string, default_value=\'\'),\n  }\n  items_to_handlers = {\n      \'image\':\n      slim.tfexample_decoder.Image(\n          shape=config[\'image_shape\'],\n          image_key=\'image/encoded\',\n          format_key=\'image/format\'),\n      \'label\':\n      slim.tfexample_decoder.Tensor(tensor_key=\'image/class\'),\n      \'text\':\n      slim.tfexample_decoder.Tensor(tensor_key=\'image/text\'),\n      \'num_of_views\':\n      _NumOfViewsHandler(\n          width_key=\'image/width\',\n          original_width_key=\'image/orig_width\',\n          num_of_views=config[\'num_of_views\'])\n  }\n  decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features,\n                                                    items_to_handlers)\n  charset_file = os.path.join(dataset_dir, config[\'charset_filename\'])\n  charset = read_charset(charset_file)\n\n  print(charset)\n  file_pattern = os.path.join(dataset_dir,\n                              config[\'splits\'][split_name][\'pattern\'])\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=tf.TFRecordReader,\n      decoder=decoder,\n      num_samples=config[\'splits\'][split_name][\'size\'],\n      items_to_descriptions=config[\'items_to_descriptions\'],\n      #  additional parameters for convenience.\n      charset=charset,\n      num_char_classes=len(charset),\n      num_of_views=config[\'num_of_views\'],\n      max_sequence_length=config[\'max_sequence_length\'],\n      null_code=config[\'null_code\'])\n'"
python/datasets/fsns_test.py,5,"b'# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for FSNS datasets module.""""""\n\nimport collections\nimport os\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport datasets.fsns\nimport datasets.unittest_utils\n\nFLAGS = tf.flags.FLAGS\n\n\ndef get_test_split():\n  config = fsns.DEFAULT_CONFIG.copy()\n  config[\'splits\'] = {\'test\': {\'size\': 50, \'pattern\': \'fsns-00000-of-00001\'}}\n  return fsns.get_split(\'test\', dataset_dir(), config)\n\n\ndef dataset_dir():\n  return os.path.join(os.path.dirname(__file__), \'testdata/fsns\')\n\n\nclass FsnsTest(tf.test.TestCase):\n  def test_decodes_example_proto(self):\n    expected_label = range(37)\n    expected_image, encoded = unittest_utils.create_random_image(\n        \'PNG\', shape=(150, 600, 3))\n    serialized = unittest_utils.create_serialized_example({\n        \'image/encoded\': [encoded],\n        \'image/format\': [\'PNG\'],\n        \'image/class\':\n        expected_label,\n        \'image/unpadded_class\':\n        range(10),\n        \'image/text\': [\'Raw text\'],\n        \'image/orig_width\': [150],\n        \'image/width\': [600]\n    })\n\n    decoder = fsns.get_split(\'train\', dataset_dir()).decoder\n    with self.test_session() as sess:\n      data_tuple = collections.namedtuple(\'DecodedData\', decoder.list_items())\n      data = sess.run(data_tuple(*decoder.decode(serialized)))\n\n    self.assertAllEqual(expected_image, data.image)\n    self.assertAllEqual(expected_label, data.label)\n    self.assertEqual([\'Raw text\'], data.text)\n    self.assertEqual([1], data.num_of_views)\n\n  def test_label_has_shape_defined(self):\n    serialized = \'fake\'\n    decoder = fsns.get_split(\'train\', dataset_dir()).decoder\n\n    [label_tf] = decoder.decode(serialized, [\'label\'])\n\n    self.assertEqual(label_tf.get_shape().dims[0], 37)\n\n  def test_dataset_tuple_has_all_extra_attributes(self):\n    dataset = fsns.get_split(\'train\', dataset_dir())\n\n    self.assertTrue(dataset.charset)\n    self.assertTrue(dataset.num_char_classes)\n    self.assertTrue(dataset.num_of_views)\n    self.assertTrue(dataset.max_sequence_length)\n    self.assertTrue(dataset.null_code)\n\n  def test_can_use_the_test_data(self):\n    batch_size = 1\n    dataset = get_test_split()\n    provider = slim.dataset_data_provider.DatasetDataProvider(\n        dataset,\n        shuffle=True,\n        common_queue_capacity=2 * batch_size,\n        common_queue_min=batch_size)\n    image_tf, label_tf = provider.get([\'image\', \'label\'])\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      with slim.queues.QueueRunners(sess):\n        image_np, label_np = sess.run([image_tf, label_tf])\n\n    self.assertEqual((150, 600, 3), image_np.shape)\n    self.assertEqual((37, ), label_np.shape)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
python/datasets/newtextdataset.py,0,"b""import datasets.fsns as fsns\n\nDEFAULT_DATASET_DIR = 'datasets/data/fsns/train'\n\nDEFAULT_CONFIG = {\n    'name':\n        'MYDATASET',\n    'splits': {\n        'train': {\n            'size': 1800000,\n            'pattern': 'tfexample_train*'\n        },\n        'test': {\n            'size': 0,\n            'pattern': 'tfexample_test*'\n        }\n    },\n    'charset_filename':\n        'dic.txt',\n    'image_shape': (150, 600, 3),\n    'num_of_views':\n        1,\n    'max_sequence_length':\n        37,\n    'null_code':\n        5462,\n    'items_to_descriptions': {\n        'image':\n            'A [150 x 600 x 3] color image.',\n        'label':\n            'Characters codes.',\n        'text':\n            'A unicode string.',\n        'length':\n            'A length of the encoded text.',\n        'num_of_views':\n            'A number of different views stored within the image.'\n    }\n}\n\n\ndef get_split(split_name, dataset_dir=None, config=None):\n  if not dataset_dir:\n    dataset_dir = DEFAULT_DATASET_DIR\n  if not config:\n    config = DEFAULT_CONFIG\n\n  return fsns.get_split(split_name, dataset_dir, config)"""
python/datasets/unittest_utils.py,3,"b'# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Functions to make unit testing easier.""""""\n\nfrom io import StringIO\nimport numpy as np\nfrom PIL import Image as PILImage\nimport tensorflow as tf\n\n\ndef create_random_image(image_format, shape):\n  """"""Creates an image with random values.\n\n  Args:\n    image_format: An image format (PNG or JPEG).\n    shape: A tuple with image shape (including channels).\n\n  Returns:\n    A tuple (<numpy ndarray>, <a string with encoded image>)\n  """"""\n  image = np.random.randint(low=0, high=255, size=shape, dtype=\'uint8\')\n  io = StringIO.StringIO()\n  image_pil = PILImage.fromarray(image)\n  image_pil.save(io, image_format, subsampling=0, quality=100)\n  return image, io.getvalue()\n\n\ndef create_serialized_example(name_to_values):\n  """"""Creates a tf.Example proto using a dictionary.\n\n  It automatically detects type of values and define a corresponding feature.\n\n  Args:\n    name_to_values: A dictionary.\n\n  Returns:\n    tf.Example proto.\n  """"""\n  example = tf.train.Example()\n  for name, values in name_to_values.items():\n    feature = example.features.feature[name]\n    if isinstance(values[0], str):\n      add = feature.bytes_list.value.extend\n    elif isinstance(values[0], float):\n      add = feature.float32_list.value.extend\n    elif isinstance(values[0], int):\n      add = feature.int64_list.value.extend\n    else:\n      raise AssertionError(\'Unsupported type: %s\' % type(values[0]))\n    add(values)\n  return example.SerializeToString()\n'"
python/datasets/unittest_utils_test.py,3,"b'# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for unittest_utils.""""""\nimport StringIO\n\nimport numpy as np\nfrom PIL import Image as PILImage\nimport tensorflow as tf\n\nimport unittest_utils\n\n\nclass UnittestUtilsTest(tf.test.TestCase):\n  def test_creates_an_image_of_specified_shape(self):\n    image, _ = unittest_utils.create_random_image(\'PNG\', (10, 20, 3))\n    self.assertEqual(image.shape, (10, 20, 3))\n\n  def test_encoded_image_corresponds_to_numpy_array(self):\n    image, encoded = unittest_utils.create_random_image(\'PNG\', (20, 10, 3))\n    pil_image = PILImage.open(StringIO.StringIO(encoded))\n    self.assertAllEqual(image, np.array(pil_image))\n\n  def test_created_example_has_correct_values(self):\n    example_serialized = unittest_utils.create_serialized_example({\n        \'labels\': [1, 2, 3],\n        \'data\': [\'FAKE\']\n    })\n    example = tf.train.Example()\n    example.ParseFromString(example_serialized)\n    self.assertProtoEquals(""""""\n      features {\n        feature {\n          key: ""labels""\n           value { int64_list {\n             value: 1\n             value: 2\n             value: 3\n           }}\n         }\n         feature {\n           key: ""data""\n           value { bytes_list {\n             value: ""FAKE""\n           }}\n         }\n      }\n    """""", example)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
