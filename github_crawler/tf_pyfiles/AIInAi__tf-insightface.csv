file_path,api_count,code
apps/__init__.py,0,b''
apps/demo.py,0,"b""import cv2\nimport numpy as np\nfrom models import face_track_server, face_describer_server, face_db, camera_server\nfrom configs import configs\n\n'''\nThe demo app utilize all servers in model folder with simple business scenario/logics:\nI have a camera product and I need to use it to find all visitors in my store who came here before.\n\nMain logics is in the process function, where you can further customize.\n'''\n\n\nclass Demo(camera_server.CameraServer):\n\n    def __init__(self, *args, **kwargs):\n        super(Demo, self).__init__(*args, **kwargs)\n        self.face_tracker = face_track_server.FaceTrackServer()\n        self.face_describer = face_describer_server.FDServer(\n            model_fp=configs.face_describer_model_fp,\n            input_tensor_names=configs.face_describer_input_tensor_names,\n            output_tensor_names=configs.face_describer_output_tensor_names,\n            device=configs.face_describer_device)\n        self.face_db = face_db.Model()\n\n    def processs(self, frame):\n        # Step1. Find and track face (frame ---> [Face_Tracker] ---> Faces Loactions)\n        self.face_tracker.process(frame)\n        _faces = self.face_tracker.get_faces()\n\n        # Uncomment below to visualize face\n        # _faces_loc = self.face_tracker.get_faces_loc()\n        # self._viz_faces(_faces_loc, frame)\n\n        # Step2. For each face, get the cropped face area, feeding it to face describer (insightface) to get 512-D Feature Embedding\n        _face_descriptions = []\n        _num_faces = len(_faces)\n        if _num_faces == 0:\n            return\n        for _face in _faces:\n            _face_resize = cv2.resize(_face, configs.face_describer_tensor_shape)\n            _data_feed = [np.expand_dims(_face_resize, axis=0), configs.face_describer_drop_out_rate]\n            _face_description = self.face_describer.inference(_data_feed)[0][0]\n            _face_descriptions.append(_face_description)\n\n            # Step3. For each face, check whether there are similar faces and if not save it to db.\n            # Below naive and verbose implementation is to tutor you how this work\n            _similar_faces = self.face_db.get_similar_faces(_face_description)\n            if len(_similar_faces) == 0 or len(self.face_db.faces) == 0:\n                self.face_db.add_face(face_img=_face, face_description=_face_description)\n        print('[Demo] -----------------------------------------------------------')\n\n    def _viz_faces(self, faces_loc, frame):\n        for _face_loc in faces_loc:\n            x1 = int(_face_loc[0] * self.face_tracker.cam_w)\n            y1 = int(_face_loc[1] * self.face_tracker.cam_h)\n            x2 = int(_face_loc[2] * self.face_tracker.cam_w)\n            y2 = int(_face_loc[3] * self.face_tracker.cam_h)\n            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n        cv2.imshow('faces', frame)\n        cv2.waitKey(1)\n\n\nif __name__ == '__main__':\n    demo = Demo(camera_address=0)\n    demo.run()\n\n"""
apps/example.py,0,"b""import cv2\nimport numpy as np\nfrom models import base_server\nfrom configs import configs\n\n# Read example image\ntest_img = cv2.imread(configs.test_img_fp)\ntest_img = cv2.resize(test_img, configs.face_describer_tensor_shape)\n\n# Define input tensors feed to session graph\ndropout_rate = 0.5\ninput_data = [np.expand_dims(test_img, axis=0), dropout_rate]\n\n# Define a Base Server\nsrv = base_server.BaseServer(model_fp=configs.face_describer_model_fp,\n                             input_tensor_names=configs.face_describer_input_tensor_names,\n                             output_tensor_names=configs.face_describer_output_tensor_names,\n                             device=configs.face_describer_device)\n# Run prediction\nprediction = srv.inference(data=input_data)\n\n# Print results\nprint('512-D Features are \\n{}'.format(prediction))\n"""
configs/__init__.py,0,b''
configs/configs.py,0,"b""import os\n\nBASE_PATH = '/'.join(os.getcwd().split('/')[:-1]) # Using ubuntu machine may require removing this -1\nface_describer_input_tensor_names = ['img_inputs:0', 'dropout_rate:0']\nface_describer_output_tensor_names = ['resnet_v1_50/E_BN2/Identity:0']\nface_describer_device = '/cpu:0'\nface_describer_model_fp = '{}/pretrained/insightface.pb'.format(BASE_PATH)\nface_describer_tensor_shape = (112, 112)\nface_describer_drop_out_rate = 0.1\ntest_img_fp = '{}/tests/test.jpg'.format(BASE_PATH)\n\nface_similarity_threshold = 800"""
deployments/__init__.py,0,b''
models/__init__.py,0,b''
models/base_server.py,9,"b""import tensorflow as tf\n\n\nclass BaseServer(object):\n    in_progress = False\n    prediction = None\n    session = None\n    graph = None\n    frozen = False\n    feed_dict = {}\n    output_ops = []\n    input_ops = []\n\n    def __init__(self, model_fp, input_tensor_names, output_tensor_names, device, frozen=True):\n        self.model_fp = model_fp\n        self.input_tensor_names = input_tensor_names\n        self.output_tensor_names = output_tensor_names\n        self.frozen = frozen\n\n        with tf.device(device):\n            self._load_graph()\n            self._init_predictor()\n\n    def _load_graph(self):\n        if self.frozen:\n            self._load_frozen_graph()\n        else:\n            self._restore_from_ckpt()\n\n    def _restore_from_ckpt(self):\n        self.saver = tf.train.Saver()\n        self.saver.restore(self.session, self.model_fp)\n\n    def _load_frozen_graph(self):\n        self.graph = tf.Graph()\n        with self.graph.as_default():\n            od_graph_def = tf.GraphDef()\n            with tf.gfile.GFile(self.model_fp, 'rb') as fid:\n                serialized_graph = fid.read()\n                od_graph_def.ParseFromString(serialized_graph)\n                tf.import_graph_def(od_graph_def, name='')\n        tf.get_default_graph().finalize()\n\n    def _init_predictor(self):\n        tf_config = tf.ConfigProto()\n        tf_config.gpu_options.allow_growth = True\n        with self.graph.as_default():\n            self.session = tf.Session(config=tf_config, graph=self.graph)\n            self._fetch_tensors()\n\n    def _fetch_tensors(self):\n        assert len(self.input_tensor_names) > 0\n        assert len(self.output_tensor_names) > 0\n        for _tensor_name in self.input_tensor_names:\n            _op = self.graph.get_tensor_by_name(_tensor_name)\n            self.input_ops.append(_op)\n            self.feed_dict[_op] = None\n        for _tensor_name in self.output_tensor_names:\n            _op = self.graph.get_tensor_by_name(_tensor_name)\n            self.output_ops.append(_op)\n\n    def _set_feed_dict(self, data):\n        assert len(data) == len(self.input_ops)\n        with self.graph.as_default():\n            for ind, op in enumerate(self.input_ops):\n                self.feed_dict[op] = data[ind]\n\n    def inference(self, data):\n        self.in_progress = True\n\n        with self.graph.as_default():\n            self._set_feed_dict(data=data)\n            print('[Base Server] Running inference...')\n            self.prediction = self.session.run(self.output_ops, feed_dict=self.feed_dict)\n        self.in_progress = False\n\n        return self.prediction\n\n    def get_status(self):\n        return self.in_progress\n\n    def kill_predictor(self):\n        # In old version tensorflow\n        # session sometimes will not be closed automatically\n        self.session.close()\n        self.session = None\n"""
models/camera_server.py,0,"b""import cv2\n\n'''\nThis server:\n    Input: Camera address\n    Process: Start a camera thread\n    Output: Run an overridden process function for each frame\n'''\n\n\nclass CameraServer(object):\n\n    camera_address = None\n    cam = None\n    in_progress = False\n\n    def __init__(self, camera_address):\n        self.camera_address = camera_address\n\n    def get_status(self):\n        return self.in_progress\n\n    # Must be overridden\n    def processs(self, frame):\n        raise NotImplementedError\n\n    def run(self):\n        print('[Camera Server] Camera is initializing ...')\n        if self.camera_address is not None:\n            self.cam = cv2.VideoCapture(self.camera_address)\n        else:\n            print('[Camera Server] Camera is not available!')\n            return\n\n        while True:\n            self.in_progress = True\n\n            # Grab a single frame of video\n            ret, frame = self.cam.read()\n            self.processs(frame)\n        self.in_progress = False\n"""
models/face_db.py,0,"b""'''\nA dummy db storaing faces in memory\nFeel free to make it fancier like hooking with postgres or whatever\nThis model here is just for simple demo app under apps\nDon't use it for production dude.\n'''\nfrom services import face_services\nimport numpy as np\n\n\nclass Model(object):\n\n    faces = []\n    faces_discriptions = []\n\n    def add_face(self, face_img, face_description):\n        self.faces.append(face_img)\n        self.faces_discriptions.append(face_description)\n\n    def drop_all(self):\n        self.faces = []\n        self.faces_discriptions = []\n\n    def get_all(self):\n        return self.faces, self.faces_discriptions\n\n    def get_similar_faces(self, face_description):\n        print('[Face DB] Looking for similar faces in a DataBase of {} faces...'.format(len(self.faces)))\n        if len(self.faces) == 0:\n            return []\n        # Use items in Python 3*, below is by default for Python 2*\n        similar_face_idx = face_services.compare_faces(self.faces_discriptions, face_description)\n        similar_faces = np.array(self.faces)[similar_face_idx]\n        num_similar_faces = len(similar_faces)\n        print('[Face DB] Found {} similar faces in a DataBase of {} faces...'.format(num_similar_faces, len(self.faces)))\n        return similar_faces\n"""
models/face_describer_server.py,0,"b'import base_server\n\n\nclass FDServer(base_server.BaseServer):\n\n    def __init__(self, *args, **kwargs):\n        super(FDServer, self).__init__(*args, **kwargs)\n'"
models/face_track_server.py,0,"b""import cv2\nimport face_recognition\n\n'''\nThis server:\n    Input: Camera frame\n    Output: Relative locations for each face, with [(tr_x, tr_y, bl_x, bl_y)]\n\nx1,y1 ------\n|          |\n|          |\n|          |\n--------x2,y2\n'''\n\n\nclass FaceTrackServer(object):\n\n    faces = []\n    face_locations = []\n    face_relative_locations = []\n    cam_h = None\n    cam_w = None\n    camera_address = None\n\n    def __init__(self, down_scale_factor=0.25):\n        assert 0 <= down_scale_factor <= 1\n        self.down_scale_factor = down_scale_factor\n\n    def get_cam_info(self):\n        return {'camera': {'width': self.cam_w, 'height': self.cam_h, 'address': self.camera_address}}\n\n    def reset(self):\n        self.face_relative_locations = []\n        self.face_locations = []\n        self.faces = []\n\n    def process(self, frame):\n        self.reset()\n        self.cam_h, self.cam_w, _ = frame.shape\n        # Resize frame of video to 1/4 size for faster face recognition processing\n        small_frame = cv2.resize(frame, (0, 0), fx=self.down_scale_factor, fy=self.down_scale_factor)\n\n        # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n        rgb_small_frame = small_frame[:, :, ::-1]\n        self.face_locations = face_recognition.face_locations(rgb_small_frame)\n        # Display the results\n        for y1_sm, x2_sm, y2_sm, x1_sm in self.face_locations:\n            # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n            x1 = int(x1_sm / self.down_scale_factor)\n            x2 = int(x2_sm / self.down_scale_factor)\n            y1 = int(y1_sm / self.down_scale_factor)\n            y2 = int(y2_sm / self.down_scale_factor)\n\n            x1_rltv = x1 / self.cam_w\n            x2_rltv = x2 / self.cam_w\n            y1_rltv = y1 / self.cam_h\n            y2_rltv = y2 / self.cam_h\n\n            _face_area = frame[x1:x2, y1:y2, :]\n            if _face_area.size == 0:\n                continue\n            self.faces.append(_face_area)\n            self.face_relative_locations.append([x1_rltv, y1_rltv, x2_rltv, y2_rltv])\n            # cv2.imshow('faces', frame[y1:y2, x1:x2, :])\n            # cv2.waitKey(0)\n        print('[FaceTracker Server] Found {} faces!'.format(len(self.faces)))\n        return self.faces\n\n    def get_faces_loc(self, relative=True):\n        if relative:\n            return self.face_relative_locations\n        else:\n            return self.face_locations\n\n    def get_faces(self):\n        return self.faces"""
services/__init__.py,0,b''
services/face_services.py,0,"b""'''\nSome of below services are referring:\nhttp://face-recognition.readthedocs.io/en/latest/_modules/face_recognition/api.html\n\n'''\n\nimport numpy as np\nfrom configs import configs\n\n\ndef face_distance(face_encodings, face_to_compare):\n    if len(face_encodings) == 0:\n        return np.empty((0))\n    face_dist_value = np.linalg.norm(face_encodings - face_to_compare, axis=1)\n    print('[Face Services | face_distance] Distance between two faces is {}'.format(face_dist_value))\n    return face_dist_value\n\n\ndef compare_faces(known_face_encodings, face_encoding_to_check, tolerance=configs.face_similarity_threshold):\n    true_list = list(face_distance(known_face_encodings, face_encoding_to_check) <= tolerance)\n    similar_indx = list(np.where(true_list)[0])\n    return similar_indx\n"""
tests/__init__.py,0,b''
tools/__init__.py,0,b''
tools/graph_freezer.py,5,"b'import os, argparse\nimport tensorflow as tf\nfrom tensorflow.python.framework import graph_util\n\ndir = os.path.dirname(os.path.realpath(__file__))\n\n\ndef freeze_graph(model_folder, net_name):\n    # We retrieve our checkpoint fullpath\n    checkpoint = tf.train.get_checkpoint_state(model_folder)\n    print(\'[checkpoint]: {}\'.format(checkpoint))\n    input_checkpoint = checkpoint.model_checkpoint_path\n\n    # We precise the file fullname of our freezed graph\n    absolute_model_folder = ""/"".join(input_checkpoint.split(\'/\')[:-1])\n    output_graph = absolute_model_folder + ""/%s.pb"" % net_name\n    # Before exporting our graph, we need to precise what is our output node\n    # This is how TF decides what part of the Graph he has to keep and what part it can dump\n    # NOTE: this variable is plural, because you can have multiple output nodes\n    output_node_names = ""img_inputs,dropout_rate,resnet_v1_50/E_BN2/Identity""\n\n    # We clear devices to allow TensorFlow to control on which device it will load operations\n    clear_devices = True\n\n    # We import the meta graph and retrieve a Saver\n    saver = tf.train.import_meta_graph(input_checkpoint + \'.meta\', clear_devices=clear_devices)\n\n    # We retrieve the protobuf graph definition\n    graph = tf.get_default_graph()\n    input_graph_def = graph.as_graph_def()\n\n    # We start a session and restore the graph weights\n    with tf.Session() as sess:\n        saver.restore(sess, input_checkpoint)\n        graph_def = sess.graph.as_graph_def()\n        node_f = open(\'nodes.txt\', \'w\')\n        for node in graph_def.node:\n            node_f.writelines(node.name + \'\\n\')\n            print node.name\n        node_f.close()\n        # We use a built-in TF helper to export variables to constants\n        output_graph_def = graph_util.convert_variables_to_constants(\n            sess,  # The session is used to retrieve the weights\n            input_graph_def,  # The graph_def is used to retrieve the nodes\n            output_node_names.split("","")  # The output node names are used to select the usefull nodes\n        )\n\n        # Finally we serialize and dump the output graph to the filesystem\n        with tf.gfile.GFile(output_graph, ""wb"") as f:\n            f.write(output_graph_def.SerializeToString())\n        print(""%d ops in the final graph."" % len(output_graph_def.node))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--model_folder"", type=str, help=""Model folder to export"")\n    parser.add_argument(""--net_name"", type=str, help=""Node name to export"")\n    args = parser.parse_args()\n\n    freeze_graph(args.model_folder, args.net_name)'"
utils/__init__.py,0,b''
