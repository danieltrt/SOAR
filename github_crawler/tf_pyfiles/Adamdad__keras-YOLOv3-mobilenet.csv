file_path,api_count,code
coco_annotation.py,0,"b'import json\nfrom collections import defaultdict\n\nname_box_id = defaultdict(list)\nid_name = dict()\nf = open(\n    ""mscoco2017/annotations/instances_train2017.json"",\n    encoding=\'utf-8\')\ndata = json.load(f)\n\nannotations = data[\'annotations\']\nfor ant in annotations:\n    id = ant[\'image_id\']\n    name = \'mscoco2017/train2017/%012d.jpg\' % id\n    cat = ant[\'category_id\']\n\n    if cat >= 1 and cat <= 11:\n        cat = cat - 1\n    elif cat >= 13 and cat <= 25:\n        cat = cat - 2\n    elif cat >= 27 and cat <= 28:\n        cat = cat - 3\n    elif cat >= 31 and cat <= 44:\n        cat = cat - 5\n    elif cat >= 46 and cat <= 65:\n        cat = cat - 6\n    elif cat == 67:\n        cat = cat - 7\n    elif cat == 70:\n        cat = cat - 9\n    elif cat >= 72 and cat <= 82:\n        cat = cat - 10\n    elif cat >= 84 and cat <= 90:\n        cat = cat - 11\n\n    name_box_id[name].append([ant[\'bbox\'], cat])\n\nf = open(\'train.txt\', \'w\')\nfor key in name_box_id.keys():\n    f.write(key)\n    box_infos = name_box_id[key]\n    for info in box_infos:\n        x_min = int(info[0][0])\n        y_min = int(info[0][1])\n        x_max = x_min + int(info[0][2])\n        y_max = y_min + int(info[0][3])\n\n        box_info = "" %d,%d,%d,%d,%d"" % (\n            x_min, y_min, x_max, y_max, int(info[1]))\n        f.write(box_info)\n    f.write(\'\\n\')\nf.close()\n'"
convert.py,0,"b'#! /usr/bin/env python\n""""""\nReads Darknet config and weights and creates Keras model with TF backend.\n\n""""""\n\nimport argparse\nimport configparser\nimport io\nimport os\nfrom collections import defaultdict\n\nimport numpy as np\nfrom keras import backend as K\nfrom keras.layers import (Conv2D, Input, ZeroPadding2D, Add,\n                          UpSampling2D, MaxPooling2D, Concatenate)\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Model\nfrom keras.regularizers import l2\nfrom keras.utils.vis_utils import plot_model as plot\n\n\nparser = argparse.ArgumentParser(description=\'Darknet To Keras Converter.\')\nparser.add_argument(\'config_path\', help=\'Path to Darknet cfg file.\')\nparser.add_argument(\'weights_path\', help=\'Path to Darknet weights file.\')\nparser.add_argument(\'output_path\', help=\'Path to output Keras model file.\')\nparser.add_argument(\n    \'-p\',\n    \'--plot_model\',\n    help=\'Plot generated Keras model and save as image.\',\n    action=\'store_true\')\nparser.add_argument(\n    \'-w\',\n    \'--weights_only\',\n    help=\'Save as Keras weights file instead of model file.\',\n    action=\'store_true\')\n\ndef unique_config_sections(config_file):\n    """"""Convert all config sections to have unique names.\n\n    Adds unique suffixes to config sections for compability with configparser.\n    """"""\n    section_counters = defaultdict(int)\n    output_stream = io.StringIO()\n    with open(config_file) as fin:\n        for line in fin:\n            if line.startswith(\'[\'):\n                section = line.strip().strip(\'[]\')\n                _section = section + \'_\' + str(section_counters[section])\n                section_counters[section] += 1\n                line = line.replace(section, _section)\n            output_stream.write(line)\n    output_stream.seek(0)\n    return output_stream\n\n# %%\ndef _main(args):\n    config_path = os.path.expanduser(args.config_path)\n    weights_path = os.path.expanduser(args.weights_path)\n    assert config_path.endswith(\'.cfg\'), \'{} is not a .cfg file\'.format(\n        config_path)\n    assert weights_path.endswith(\n        \'.weights\'), \'{} is not a .weights file\'.format(weights_path)\n\n    output_path = os.path.expanduser(args.output_path)\n    assert output_path.endswith(\n        \'.h5\'), \'output path {} is not a .h5 file\'.format(output_path)\n    output_root = os.path.splitext(output_path)[0]\n\n    # Load weights and config.\n    print(\'Loading weights.\')\n    weights_file = open(weights_path, \'rb\')\n    major, minor, revision = np.ndarray(\n        shape=(3, ), dtype=\'int32\', buffer=weights_file.read(12))\n    if (major*10+minor)>=2 and major<1000 and minor<1000:\n        seen = np.ndarray(shape=(1,), dtype=\'int64\', buffer=weights_file.read(8))\n    else:\n        seen = np.ndarray(shape=(1,), dtype=\'int32\', buffer=weights_file.read(4))\n    print(\'Weights Header: \', major, minor, revision, seen)\n\n    print(\'Parsing Darknet config.\')\n    unique_config_file = unique_config_sections(config_path)\n    cfg_parser = configparser.ConfigParser()\n    cfg_parser.read_file(unique_config_file)\n\n    print(\'Creating Keras model.\')\n    input_layer = Input(shape=(None, None, 3))\n    prev_layer = input_layer\n    all_layers = []\n\n    weight_decay = float(cfg_parser[\'net_0\'][\'decay\']\n                         ) if \'net_0\' in cfg_parser.sections() else 5e-4\n    count = 0\n    out_index = []\n    for section in cfg_parser.sections():\n        print(\'Parsing section {}\'.format(section))\n        if section.startswith(\'convolutional\'):\n            filters = int(cfg_parser[section][\'filters\'])\n            size = int(cfg_parser[section][\'size\'])\n            stride = int(cfg_parser[section][\'stride\'])\n            pad = int(cfg_parser[section][\'pad\'])\n            activation = cfg_parser[section][\'activation\']\n            batch_normalize = \'batch_normalize\' in cfg_parser[section]\n\n            padding = \'same\' if pad == 1 and stride == 1 else \'valid\'\n\n            # Setting weights.\n            # Darknet serializes convolutional weights as:\n            # [bias/beta, [gamma, mean, variance], conv_weights]\n            prev_layer_shape = K.int_shape(prev_layer)\n\n            weights_shape = (size, size, prev_layer_shape[-1], filters)\n            darknet_w_shape = (filters, weights_shape[2], size, size)\n            weights_size = np.product(weights_shape)\n\n            print(\'conv2d\', \'bn\'\n                  if batch_normalize else \'  \', activation, weights_shape)\n\n            conv_bias = np.ndarray(\n                shape=(filters, ),\n                dtype=\'float32\',\n                buffer=weights_file.read(filters * 4))\n            count += filters\n\n            if batch_normalize:\n                bn_weights = np.ndarray(\n                    shape=(3, filters),\n                    dtype=\'float32\',\n                    buffer=weights_file.read(filters * 12))\n                count += 3 * filters\n\n                bn_weight_list = [\n                    bn_weights[0],  # scale gamma\n                    conv_bias,  # shift beta\n                    bn_weights[1],  # running mean\n                    bn_weights[2]  # running var\n                ]\n\n            conv_weights = np.ndarray(\n                shape=darknet_w_shape,\n                dtype=\'float32\',\n                buffer=weights_file.read(weights_size * 4))\n            count += weights_size\n\n            # DarkNet conv_weights are serialized Caffe-style:\n            # (out_dim, in_dim, height, width)\n            # We would like to set these to Tensorflow order:\n            # (height, width, in_dim, out_dim)\n            conv_weights = np.transpose(conv_weights, [2, 3, 1, 0])\n            conv_weights = [conv_weights] if batch_normalize else [\n                conv_weights, conv_bias\n            ]\n\n            # Handle activation.\n            act_fn = None\n            if activation == \'leaky\':\n                pass  # Add advanced activation later.\n            elif activation != \'linear\':\n                raise ValueError(\n                    \'Unknown activation function `{}` in section {}\'.format(\n                        activation, section))\n\n            # Create Conv2D layer\n            if stride>1:\n                # Darknet uses left and top padding instead of \'same\' mode\n                prev_layer = ZeroPadding2D(((1,0),(1,0)))(prev_layer)\n            conv_layer = (Conv2D(\n                filters, (size, size),\n                strides=(stride, stride),\n                kernel_regularizer=l2(weight_decay),\n                use_bias=not batch_normalize,\n                weights=conv_weights,\n                activation=act_fn,\n                padding=padding))(prev_layer)\n\n            if batch_normalize:\n                conv_layer = (BatchNormalization(\n                    weights=bn_weight_list))(conv_layer)\n            prev_layer = conv_layer\n\n            if activation == \'linear\':\n                all_layers.append(prev_layer)\n            elif activation == \'leaky\':\n                act_layer = LeakyReLU(alpha=0.1)(prev_layer)\n                prev_layer = act_layer\n                all_layers.append(act_layer)\n\n        elif section.startswith(\'route\'):\n            ids = [int(i) for i in cfg_parser[section][\'layers\'].split(\',\')]\n            layers = [all_layers[i] for i in ids]\n            if len(layers) > 1:\n                print(\'Concatenating route layers:\', layers)\n                concatenate_layer = Concatenate()(layers)\n                all_layers.append(concatenate_layer)\n                prev_layer = concatenate_layer\n            else:\n                skip_layer = layers[0]  # only one layer to route\n                all_layers.append(skip_layer)\n                prev_layer = skip_layer\n\n        elif section.startswith(\'maxpool\'):\n            size = int(cfg_parser[section][\'size\'])\n            stride = int(cfg_parser[section][\'stride\'])\n            all_layers.append(\n                MaxPooling2D(\n                    pool_size=(size, size),\n                    strides=(stride, stride),\n                    padding=\'same\')(prev_layer))\n            prev_layer = all_layers[-1]\n\n        elif section.startswith(\'shortcut\'):\n            index = int(cfg_parser[section][\'from\'])\n            activation = cfg_parser[section][\'activation\']\n            assert activation == \'linear\', \'Only linear activation supported.\'\n            all_layers.append(Add()([all_layers[index], prev_layer]))\n            prev_layer = all_layers[-1]\n\n        elif section.startswith(\'upsample\'):\n            stride = int(cfg_parser[section][\'stride\'])\n            assert stride == 2, \'Only stride=2 supported.\'\n            all_layers.append(UpSampling2D(stride)(prev_layer))\n            prev_layer = all_layers[-1]\n\n        elif section.startswith(\'yolo\'):\n            out_index.append(len(all_layers)-1)\n            all_layers.append(None)\n            prev_layer = all_layers[-1]\n\n        elif section.startswith(\'net\'):\n            pass\n\n        else:\n            raise ValueError(\n                \'Unsupported section header type: {}\'.format(section))\n\n    # Create and save model.\n    if len(out_index)==0: out_index.append(len(all_layers)-1)\n    model = Model(inputs=input_layer, outputs=[all_layers[i] for i in out_index])\n    print(model.summary())\n    if args.weights_only:\n        model.save_weights(\'{}\'.format(output_path))\n        print(\'Saved Keras weights to {}\'.format(output_path))\n    else:\n        model.save(\'{}\'.format(output_path))\n        print(\'Saved Keras model to {}\'.format(output_path))\n\n    # Check to see if all weights have been read.\n    remaining_weights = len(weights_file.read()) / 4\n    weights_file.close()\n    print(\'Read {} of {} from Darknet weights.\'.format(count, count +\n                                                       remaining_weights))\n    if remaining_weights > 0:\n        print(\'Warning: {} unused weights\'.format(remaining_weights))\n\n    if args.plot_model:\n        plot(model, to_file=\'{}.png\'.format(output_root), show_shapes=True)\n        print(\'Saved model plot to {}.png\'.format(output_root))\n\n\nif __name__ == \'__main__\':\n    _main(parser.parse_args())\n'"
kmeans.py,0,"b'import numpy as np\n\n\nclass YOLO_Kmeans:\n\n    def __init__(self, cluster_number, filename):\n        self.cluster_number = cluster_number\n        self.filename = ""2012_train.txt""\n\n    def iou(self, boxes, clusters):  # 1 box -> k clusters\n        n = boxes.shape[0]\n        k = self.cluster_number\n\n        box_area = boxes[:, 0] * boxes[:, 1]\n        box_area = box_area.repeat(k)\n        box_area = np.reshape(box_area, (n, k))\n\n        cluster_area = clusters[:, 0] * clusters[:, 1]\n        cluster_area = np.tile(cluster_area, [1, n])\n        cluster_area = np.reshape(cluster_area, (n, k))\n\n        box_w_matrix = np.reshape(boxes[:, 0].repeat(k), (n, k))\n        cluster_w_matrix = np.reshape(np.tile(clusters[:, 0], (1, n)), (n, k))\n        min_w_matrix = np.minimum(cluster_w_matrix, box_w_matrix)\n\n        box_h_matrix = np.reshape(boxes[:, 1].repeat(k), (n, k))\n        cluster_h_matrix = np.reshape(np.tile(clusters[:, 1], (1, n)), (n, k))\n        min_h_matrix = np.minimum(cluster_h_matrix, box_h_matrix)\n        inter_area = np.multiply(min_w_matrix, min_h_matrix)\n\n        result = inter_area / (box_area + cluster_area - inter_area)\n        return result\n\n    def avg_iou(self, boxes, clusters):\n        accuracy = np.mean([np.max(self.iou(boxes, clusters), axis=1)])\n        return accuracy\n\n    def kmeans(self, boxes, k, dist=np.median):\n        box_number = boxes.shape[0]\n        distances = np.empty((box_number, k))\n        last_nearest = np.zeros((box_number,))\n        np.random.seed()\n        clusters = boxes[np.random.choice(\n            box_number, k, replace=False)]  # init k clusters\n        while True:\n\n            distances = 1 - self.iou(boxes, clusters)\n\n            current_nearest = np.argmin(distances, axis=1)\n            if (last_nearest == current_nearest).all():\n                break  # clusters won\'t change\n            for cluster in range(k):\n                clusters[cluster] = dist(  # update clusters\n                    boxes[current_nearest == cluster], axis=0)\n\n            last_nearest = current_nearest\n\n        return clusters\n\n    def result2txt(self, data):\n        f = open(""yolo_anchors.txt"", \'w\')\n        row = np.shape(data)[0]\n        for i in range(row):\n            if i == 0:\n                x_y = ""%d,%d"" % (data[i][0], data[i][1])\n            else:\n                x_y = "", %d,%d"" % (data[i][0], data[i][1])\n            f.write(x_y)\n        f.close()\n\n    def txt2boxes(self):\n        f = open(self.filename, \'r\')\n        dataSet = []\n        for line in f:\n            infos = line.split("" "")\n            length = len(infos)\n            for i in range(1, length):\n                width = int(infos[i].split("","")[2]) - \\\n                    int(infos[i].split("","")[0])\n                height = int(infos[i].split("","")[3]) - \\\n                    int(infos[i].split("","")[1])\n                dataSet.append([width, height])\n        result = np.array(dataSet)\n        f.close()\n        return result\n\n    def txt2clusters(self):\n        all_boxes = self.txt2boxes()\n        result = self.kmeans(all_boxes, k=self.cluster_number)\n        result = result[np.lexsort(result.T[0, None])]\n        self.result2txt(result)\n        print(""K anchors:\\n {}"".format(result))\n        print(""Accuracy: {:.2f}%"".format(\n            self.avg_iou(all_boxes, result) * 100))\n\n\nif __name__ == ""__main__"":\n    cluster_number = 9\n    filename = ""2012_train.txt""\n    kmeans = YOLO_Kmeans(cluster_number, filename)\n    kmeans.txt2clusters()\n'"
train.py,0,"b'""""""\nRetrain the YOLO model for your own dataset.\n""""""\n\nimport numpy as np\nimport keras.backend as K\nfrom keras.layers import Input, Lambda\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n\nfrom yolo3.model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss\nfrom yolo3.utils import get_random_data\n\n\ndef _main():\n    annotation_path = \'train.txt\'\n    log_dir = \'logs/000/\'\n    classes_path = \'model_data/voc_classes.txt\'\n    anchors_path = \'model_data/yolo_anchors.txt\'\n    class_names = get_classes(classes_path)\n    num_classes = len(class_names)\n    anchors = get_anchors(anchors_path)\n\n    input_shape = (416,416) # multiple of 32, hw\n\n    is_tiny_version = len(anchors)==6 # default setting\n    if is_tiny_version:\n        model = create_tiny_model(input_shape, anchors, num_classes,\n            freeze_body=2, weights_path=\'model_data/tiny_yolo_weights.h5\')\n    else:\n        model = create_model(input_shape, anchors, num_classes,\n            freeze_body=2, weights_path=\'model_data/yolo_weights.h5\') # make sure you know what you freeze\n\n    logging = TensorBoard(log_dir=log_dir)\n    checkpoint = ModelCheckpoint(log_dir + \'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5\',\n        monitor=\'val_loss\', save_weights_only=True, save_best_only=True, period=3)\n    reduce_lr = ReduceLROnPlateau(monitor=\'val_loss\', factor=0.1, patience=3, verbose=1)\n    early_stopping = EarlyStopping(monitor=\'val_loss\', min_delta=0, patience=10, verbose=1)\n\n    val_split = 0.1\n    with open(annotation_path) as f:\n        lines = f.readlines()\n    np.random.seed(10101)\n    np.random.shuffle(lines)\n    np.random.seed(None)\n    num_val = int(len(lines)*val_split)\n    num_train = len(lines) - num_val\n\n    # Train with frozen layers first, to get a stable loss.\n    # Adjust num epochs to your dataset. This step is enough to obtain a not bad model.\n    if True:\n        model.compile(optimizer=Adam(lr=1e-3), loss={\n            # use custom yolo_loss Lambda layer.\n            \'yolo_loss\': lambda y_true, y_pred: y_pred})\n\n        batch_size = 32\n        print(\'Train on {} samples, val on {} samples, with batch size {}.\'.format(num_train, num_val, batch_size))\n        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n                steps_per_epoch=max(1, num_train//batch_size),\n                validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n                validation_steps=max(1, num_val//batch_size),\n                epochs=50,\n                initial_epoch=0,\n                callbacks=[logging, checkpoint])\n        model.save_weights(log_dir + \'trained_weights_stage_1.h5\')\n\n    # Unfreeze and continue training, to fine-tune.\n    # Train longer if the result is not good.\n    if True:\n        for i in range(len(model.layers)):\n            model.layers[i].trainable = True\n        model.compile(optimizer=Adam(lr=1e-4), loss={\'yolo_loss\': lambda y_true, y_pred: y_pred}) # recompile to apply the change\n        print(\'Unfreeze all of the layers.\')\n\n        batch_size = 32 # note that more GPU memory is required after unfreezing the body\n        print(\'Train on {} samples, val on {} samples, with batch size {}.\'.format(num_train, num_val, batch_size))\n        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n            steps_per_epoch=max(1, num_train//batch_size),\n            validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n            validation_steps=max(1, num_val//batch_size),\n            epochs=100,\n            initial_epoch=50,\n            callbacks=[logging, checkpoint, reduce_lr, early_stopping])\n        model.save_weights(log_dir + \'trained_weights_final.h5\')\n\n    # Further training if needed.\n\n\ndef get_classes(classes_path):\n    \'\'\'loads the classes\'\'\'\n    with open(classes_path) as f:\n        class_names = f.readlines()\n    class_names = [c.strip() for c in class_names]\n    return class_names\n\ndef get_anchors(anchors_path):\n    \'\'\'loads the anchors from a file\'\'\'\n    with open(anchors_path) as f:\n        anchors = f.readline()\n    anchors = [float(x) for x in anchors.split(\',\')]\n    return np.array(anchors).reshape(-1, 2)\n\n\ndef create_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n            weights_path=\'model_data/yolo_weights.h5\'):\n    \'\'\'create the training model\'\'\'\n    K.clear_session() # get a new session\n    image_input = Input(shape=(None, None, 3))\n    h, w = input_shape\n    num_anchors = len(anchors)\n\n    y_true = [Input(shape=(h//{0:32, 1:16, 2:8}[l], w//{0:32, 1:16, 2:8}[l], \\\n        num_anchors//3, num_classes+5)) for l in range(3)]\n\n    model_body = yolo_body(image_input, num_anchors//3, num_classes)\n    print(\'Create YOLOv3 model with {} anchors and {} classes.\'.format(num_anchors, num_classes))\n\n    if load_pretrained:\n        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n        print(\'Load weights {}.\'.format(weights_path))\n        if freeze_body in [1, 2]:\n            # Freeze darknet53 body or freeze all but 3 output layers.\n            num = (185, len(model_body.layers)-3)[freeze_body-1]\n            for i in range(num): model_body.layers[i].trainable = False\n            print(\'Freeze the first {} layers of total {} layers.\'.format(num, len(model_body.layers)))\n\n    model_loss = Lambda(yolo_loss, output_shape=(1,), name=\'yolo_loss\',\n        arguments={\'anchors\': anchors, \'num_classes\': num_classes, \'ignore_thresh\': 0.5})(\n        [*model_body.output, *y_true])\n    model = Model([model_body.input, *y_true], model_loss)\n\n    return model\n\ndef create_tiny_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n            weights_path=\'model_data/tiny_yolo_weights.h5\'):\n    \'\'\'create the training model, for Tiny YOLOv3\'\'\'\n    K.clear_session() # get a new session\n    image_input = Input(shape=(None, None, 3))\n    h, w = input_shape\n    num_anchors = len(anchors)\n\n    y_true = [Input(shape=(h//{0:32, 1:16}[l], w//{0:32, 1:16}[l], \\\n        num_anchors//2, num_classes+5)) for l in range(2)]\n\n    model_body = tiny_yolo_body(image_input, num_anchors//2, num_classes)\n    print(\'Create Tiny YOLOv3 model with {} anchors and {} classes.\'.format(num_anchors, num_classes))\n\n    if load_pretrained:\n        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n        print(\'Load weights {}.\'.format(weights_path))\n        if freeze_body in [1, 2]:\n            # Freeze the darknet body or freeze all but 2 output layers.\n            num = (20, len(model_body.layers)-2)[freeze_body-1]\n            for i in range(num): model_body.layers[i].trainable = False\n            print(\'Freeze the first {} layers of total {} layers.\'.format(num, len(model_body.layers)))\n\n    model_loss = Lambda(yolo_loss, output_shape=(1,), name=\'yolo_loss\',\n        arguments={\'anchors\': anchors, \'num_classes\': num_classes, \'ignore_thresh\': 0.7})(\n        [*model_body.output, *y_true])\n    model = Model([model_body.input, *y_true], model_loss)\n\n    return model\n\ndef data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes):\n    \'\'\'data generator for fit_generator\'\'\'\n    n = len(annotation_lines)\n    i = 0\n    while True:\n        image_data = []\n        box_data = []\n        for b in range(batch_size):\n            if i==0:\n                np.random.shuffle(annotation_lines)\n            image, box = get_random_data(annotation_lines[i], input_shape, random=True)\n            image_data.append(image)\n            box_data.append(box)\n            i = (i+1) % n\n        image_data = np.array(image_data)\n        box_data = np.array(box_data)\n        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)\n        yield [image_data, *y_true], np.zeros(batch_size)\n\ndef data_generator_wrapper(annotation_lines, batch_size, input_shape, anchors, num_classes):\n    n = len(annotation_lines)\n    if n==0 or batch_size<=0: return None\n    return data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes)\n\nif __name__ == \'__main__\':\n    _main()\n'"
train_Mobilenet.py,0,"b'""""""\r\nRetrain the YOLO model for your own dataset.\r\n""""""\r\n\r\nimport numpy as np\r\nimport keras.backend as K\r\nfrom keras.layers import Input, Lambda\r\nfrom keras.models import Model\r\nfrom keras.optimizers import Adam\r\nfrom keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\r\nimport os\r\nfrom yolo3.model_Mobilenet import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss\r\nfrom yolo3.utils import get_random_data\r\n\r\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\r\ndef _main():\r\n    train_path = \'car_train.txt\'\r\n    val_path = \'car_val.txt\'\r\n    log_dir = \'logs/carMobilenet/001_Mobilenet_finetune/\'\r\n    classes_path = \'model_data/car_classes.txt\'\r\n    anchors_path = \'model_data/yolo_anchors.txt\'\r\n    class_names = get_classes(classes_path)\r\n    num_classes = len(class_names)\r\n    anchors = get_anchors(anchors_path)\r\n\r\n    input_shape = (320,320) # multiple of 32, hw\r\n\r\n    is_tiny_version = len(anchors)==6 # default setting\r\n    if is_tiny_version:\r\n        model = create_tiny_model(input_shape, anchors, num_classes,\r\n            freeze_body=2)\r\n    else:\r\n        model = create_model(input_shape, anchors, num_classes,load_pretrained=False,\r\n                             weights_path=\'logs/carMobilenet/000_Mobilenet_finetune/trained_weights_final.h5\',\r\n            freeze_body=2) # make sure you know what you freeze\r\n\r\n    logging = TensorBoard(log_dir=log_dir)\r\n    # checkpoint = ModelCheckpoint(log_dir + \'car_mobilenet_yolov3.ckpt\',\r\n    #    monitor=\'val_loss\', save_weights_only=False, period=1)\r\n    checkpoint = ModelCheckpoint(log_dir + \'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5\',\r\n        monitor=\'val_loss\', save_weights_only=True, save_best_only=True, period=3)\r\n    reduce_lr = ReduceLROnPlateau(monitor=\'val_loss\', factor=0.1, patience=3, verbose=1)\r\n    early_stopping = EarlyStopping(monitor=\'val_loss\', min_delta=0, patience=10, verbose=1)\r\n\r\n    with open(train_path) as t_f:\r\n        t_lines = t_f.readlines()\r\n    np.random.seed(10101)\r\n    np.random.shuffle(t_lines)\r\n    np.random.seed(None)\r\n    v_lines = t_lines[8000:]\r\n    t_lines = t_lines[:8000]\r\n    num_train = len(t_lines)\r\n    # with open(val_path) as v_f:\r\n    #     v_lines = v_f.readlines()\r\n    # np.random.seed(10010)\r\n    # np.random.shuffle(v_lines)\r\n    # np.random.seed(None)\r\n    num_val = len(v_lines)\r\n\r\n    # Train with frozen layers first, to get a stable loss.\r\n    # Adjust num epochs to your dataset. This step is enough to obtain a not bad model.\r\n    if True:\r\n        model.compile(optimizer=Adam(lr=1e-3), loss={\r\n            # use custom yolo_loss Lambda layer.\r\n            \'yolo_loss\': lambda y_true, y_pred: y_pred})\r\n\r\n        batch_size = 16\r\n        print(\'Train on {} samples, val on {} samples, with batch size {}.\'.format(num_train, num_val, batch_size))\r\n        model.fit_generator(data_generator_wrapper(t_lines, batch_size, input_shape, anchors, num_classes),\r\n                steps_per_epoch=max(1, num_train//batch_size),\r\n                validation_data=data_generator_wrapper(v_lines, batch_size, input_shape, anchors, num_classes),\r\n                validation_steps=max(1, num_val//batch_size),\r\n                epochs=30,\r\n                initial_epoch=0,\r\n                callbacks=[logging, checkpoint])\r\n        model.save_weights(log_dir + \'trained_weights_stage_1.h5\')\r\n\r\n    # Unfreeze and continue training, to fine-tune.\r\n    # Train longer if the result is not good.\r\n    if True:\r\n        print(""Unfreeze and continue training, to fine-tune."")\r\n        for i in range(len(model.layers)):\r\n            model.layers[i].trainable= True\r\n        model.compile(optimizer=Adam(lr=1e-4), loss={\'yolo_loss\': lambda y_true, y_pred: y_pred}) # recompile to apply the change\r\n        batch_size = 16 # note that more GPU memory is required after unfreezing the body\r\n        print(\'Train on {} samples, val on {} samples, with batch size {}.\'.format(num_train, num_val, batch_size))\r\n        model.fit_generator(data_generator_wrapper(t_lines, batch_size, input_shape, anchors, num_classes),\r\n            steps_per_epoch=max(1, num_train//batch_size),\r\n            validation_data=data_generator_wrapper(v_lines, batch_size, input_shape, anchors, num_classes),\r\n            validation_steps=max(1, num_val//batch_size),\r\n            epochs=20,\r\n            initial_epoch=0,\r\n            callbacks=[logging, checkpoint, reduce_lr, early_stopping])\r\n        model.save_weights(log_dir + \'trained_weights_final.h5\')\r\n\r\n    # Further training if needed.\r\n\r\n\r\ndef get_classes(classes_path):\r\n    \'\'\'loads the classes\'\'\'\r\n    with open(classes_path) as f:\r\n        class_names = f.readlines()\r\n    class_names = [c.strip() for c in class_names]\r\n    return class_names\r\n\r\ndef get_anchors(anchors_path):\r\n    \'\'\'loads the anchors from a file\'\'\'\r\n    with open(anchors_path) as f:\r\n        anchors = f.readline()\r\n    anchors = [float(x) for x in anchors.split(\',\')]\r\n    return np.array(anchors).reshape(-1, 2)\r\n\r\n\r\ndef create_model(input_shape, anchors, num_classes, load_pretrained=False, freeze_body=2,\r\n            weights_path=\'model_data/yolo_weights.h5\'):\r\n    \'\'\'create the training model\'\'\'\r\n    K.clear_session() # get a new session\r\n    image_input = Input(shape=(None, None, 3))\r\n    h, w = input_shape\r\n    num_anchors = len(anchors)\r\n\r\n    y_true = [Input(shape=(h//{0:32, 1:16, 2:8}[l], w//{0:32, 1:16, 2:8}[l], \\\r\n        num_anchors//3, num_classes+5)) for l in range(3)]\r\n\r\n    model_body = yolo_body(image_input, num_anchors//3, num_classes)\r\n    print(\'Create YOLOv3 model with {} anchors and {} classes.\'.format(num_anchors, num_classes))\r\n\r\n    if load_pretrained:\r\n        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\r\n        print(\'Load weights {}.\'.format(weights_path))\r\n        if freeze_body in [1, 2]:\r\n            # Freeze darknet53 body or freeze all but 3 output layers.\r\n            num = (185, len(model_body.layers)-3)[freeze_body-1]\r\n            for i in range(num): model_body.layers[i].trainable = False\r\n            print(\'Freeze the first {} layers of total {} layers.\'.format(num, len(model_body.layers)))\r\n\r\n    print(model_body.output)\r\n    model_loss = Lambda(yolo_loss, output_shape=(1,), name=\'yolo_loss\',\r\n        arguments={\'anchors\': anchors, \'num_classes\': num_classes, \'ignore_thresh\': 0.5})(\r\n        [*model_body.output, *y_true])\r\n    model = Model([model_body.input, *y_true], model_loss)\r\n\r\n    return model\r\n\r\ndef create_tiny_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\r\n            weights_path=\'model_data/tiny_yolo_weights.h5\'):\r\n    \'\'\'create the training model, for Tiny YOLOv3\'\'\'\r\n    K.clear_session() # get a new session\r\n    image_input = Input(shape=(None, None, 3))\r\n    h, w = input_shape\r\n    num_anchors = len(anchors)\r\n\r\n    y_true = [Input(shape=(h//{0:32, 1:16}[l], w//{0:32, 1:16}[l], \\\r\n        num_anchors//2, num_classes+5)) for l in range(2)]\r\n\r\n    model_body = tiny_yolo_body(image_input, num_anchors//2, num_classes)\r\n    print(\'Create Tiny YOLOv3 model with {} anchors and {} classes.\'.format(num_anchors, num_classes))\r\n\r\n    if load_pretrained:\r\n        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\r\n        print(\'Load weights {}.\'.format(weights_path))\r\n        if freeze_body in [1, 2]:\r\n            # Freeze the darknet body or freeze all but 2 output layers.\r\n            num = (20, len(model_body.layers)-2)[freeze_body-1]\r\n            for i in range(num): model_body.layers[i].trainable = False\r\n            print(\'Freeze the first {} layers of total {} layers.\'.format(num, len(model_body.layers)))\r\n\r\n    model_loss = Lambda(yolo_loss, output_shape=(1,), name=\'yolo_loss\',\r\n        arguments={\'anchors\': anchors, \'num_classes\': num_classes, \'ignore_thresh\': 0.7})(\r\n        [*model_body.output, *y_true])\r\n    model = Model([model_body.input, *y_true], model_loss)\r\n\r\n    return model\r\n\r\ndef data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes):\r\n    \'\'\'data generator for fit_generator\'\'\'\r\n    n = len(annotation_lines)\r\n    i = 0\r\n    while True:\r\n        image_data = []\r\n        box_data = []\r\n        for b in range(batch_size):\r\n            if i==0:\r\n                np.random.shuffle(annotation_lines)\r\n            image, box = get_random_data(annotation_lines[i], input_shape, random=True)\r\n            image_data.append(image)\r\n            box_data.append(box)\r\n            i = (i+1) % n\r\n        image_data = np.array(image_data)\r\n        box_data = np.array(box_data)\r\n        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)\r\n        yield [image_data, *y_true], np.zeros(batch_size)\r\n\r\ndef data_generator_wrapper(annotation_lines, batch_size, input_shape, anchors, num_classes):\r\n    n = len(annotation_lines)\r\n    if n==0 or batch_size<=0: return None\r\n    return data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes)\r\n\r\nif __name__ == \'__main__\':\r\n    _main()\r\n\r\n\r\n'"
train_bottleneck.py,0,"b'""""""\nRetrain the YOLO model for your own dataset.\n""""""\nimport os\nimport numpy as np\nimport keras.backend as K\nfrom keras.layers import Input, Lambda\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n\nfrom yolo3.model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss\nfrom yolo3.utils import get_random_data\n\n\ndef _main():\n    annotation_path = \'train.txt\'\n    log_dir = \'logs/000/\'\n    classes_path = \'model_data/coco_classes.txt\'\n    anchors_path = \'model_data/yolo_anchors.txt\'\n    class_names = get_classes(classes_path)\n    num_classes = len(class_names)\n    anchors = get_anchors(anchors_path)\n\n    input_shape = (416,416) # multiple of 32, hw\n\n    model, bottleneck_model, last_layer_model = create_model(input_shape, anchors, num_classes,\n            freeze_body=2, weights_path=\'model_data/yolo_weights.h5\') # make sure you know what you freeze\n\n    logging = TensorBoard(log_dir=log_dir)\n    checkpoint = ModelCheckpoint(log_dir + \'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5\',\n        monitor=\'val_loss\', save_weights_only=True, save_best_only=True, period=3)\n    reduce_lr = ReduceLROnPlateau(monitor=\'val_loss\', factor=0.1, patience=3, verbose=1)\n    early_stopping = EarlyStopping(monitor=\'val_loss\', min_delta=0, patience=10, verbose=1)\n\n    val_split = 0.1\n    with open(annotation_path) as f:\n        lines = f.readlines()\n    np.random.seed(10101)\n    np.random.shuffle(lines)\n    np.random.seed(None)\n    num_val = int(len(lines)*val_split)\n    num_train = len(lines) - num_val\n\n    # Train with frozen layers first, to get a stable loss.\n    # Adjust num epochs to your dataset. This step is enough to obtain a not bad model.\n    if True:\n        # perform bottleneck training\n        if not os.path.isfile(""bottlenecks.npz""):\n            print(""calculating bottlenecks"")\n            batch_size=8\n            bottlenecks=bottleneck_model.predict_generator(data_generator_wrapper(lines, batch_size, input_shape, anchors, num_classes, random=False, verbose=True),\n             steps=(len(lines)//batch_size)+1, max_queue_size=1)\n            np.savez(""bottlenecks.npz"", bot0=bottlenecks[0], bot1=bottlenecks[1], bot2=bottlenecks[2])\n    \n        # load bottleneck features from file\n        dict_bot=np.load(""bottlenecks.npz"")\n        bottlenecks_train=[dict_bot[""bot0""][:num_train], dict_bot[""bot1""][:num_train], dict_bot[""bot2""][:num_train]]\n        bottlenecks_val=[dict_bot[""bot0""][num_train:], dict_bot[""bot1""][num_train:], dict_bot[""bot2""][num_train:]]\n\n        # train last layers with fixed bottleneck features\n        batch_size=8\n        print(""Training last layers with bottleneck features"")\n        print(\'with {} samples, val on {} samples and batch size {}.\'.format(num_train, num_val, batch_size))\n        last_layer_model.compile(optimizer=\'adam\', loss={\'yolo_loss\': lambda y_true, y_pred: y_pred})\n        last_layer_model.fit_generator(bottleneck_generator(lines[:num_train], batch_size, input_shape, anchors, num_classes, bottlenecks_train),\n                steps_per_epoch=max(1, num_train//batch_size),\n                validation_data=bottleneck_generator(lines[num_train:], batch_size, input_shape, anchors, num_classes, bottlenecks_val),\n                validation_steps=max(1, num_val//batch_size),\n                epochs=30,\n                initial_epoch=0, max_queue_size=1)\n        model.save_weights(log_dir + \'trained_weights_stage_0.h5\')\n        \n        # train last layers with random augmented data\n        model.compile(optimizer=Adam(lr=1e-3), loss={\n            # use custom yolo_loss Lambda layer.\n            \'yolo_loss\': lambda y_true, y_pred: y_pred})\n        batch_size = 16\n        print(\'Train on {} samples, val on {} samples, with batch size {}.\'.format(num_train, num_val, batch_size))\n        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n                steps_per_epoch=max(1, num_train//batch_size),\n                validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n                validation_steps=max(1, num_val//batch_size),\n                epochs=50,\n                initial_epoch=0,\n                callbacks=[logging, checkpoint])\n        model.save_weights(log_dir + \'trained_weights_stage_1.h5\')\n\n    # Unfreeze and continue training, to fine-tune.\n    # Train longer if the result is not good.\n    if True:\n        for i in range(len(model.layers)):\n            model.layers[i].trainable = True\n        model.compile(optimizer=Adam(lr=1e-4), loss={\'yolo_loss\': lambda y_true, y_pred: y_pred}) # recompile to apply the change\n        print(\'Unfreeze all of the layers.\')\n\n        batch_size = 4 # note that more GPU memory is required after unfreezing the body\n        print(\'Train on {} samples, val on {} samples, with batch size {}.\'.format(num_train, num_val, batch_size))\n        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n            steps_per_epoch=max(1, num_train//batch_size),\n            validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n            validation_steps=max(1, num_val//batch_size),\n            epochs=100,\n            initial_epoch=50,\n            callbacks=[logging, checkpoint, reduce_lr, early_stopping])\n        model.save_weights(log_dir + \'trained_weights_final.h5\')\n\n    # Further training if needed.\n\n\ndef get_classes(classes_path):\n    \'\'\'loads the classes\'\'\'\n    with open(classes_path) as f:\n        class_names = f.readlines()\n    class_names = [c.strip() for c in class_names]\n    return class_names\n\ndef get_anchors(anchors_path):\n    \'\'\'loads the anchors from a file\'\'\'\n    with open(anchors_path) as f:\n        anchors = f.readline()\n    anchors = [float(x) for x in anchors.split(\',\')]\n    return np.array(anchors).reshape(-1, 2)\n\n\ndef create_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n            weights_path=\'model_data/yolo_weights.h5\'):\n    \'\'\'create the training model\'\'\'\n    K.clear_session() # get a new session\n    image_input = Input(shape=(None, None, 3))\n    h, w = input_shape\n    num_anchors = len(anchors)\n\n    y_true = [Input(shape=(h//{0:32, 1:16, 2:8}[l], w//{0:32, 1:16, 2:8}[l], \\\n        num_anchors//3, num_classes+5)) for l in range(3)]\n\n    model_body = yolo_body(image_input, num_anchors//3, num_classes)\n    print(\'Create YOLOv3 model with {} anchors and {} classes.\'.format(num_anchors, num_classes))\n\n    if load_pretrained:\n        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n        print(\'Load weights {}.\'.format(weights_path))\n        if freeze_body in [1, 2]:\n            # Freeze darknet53 body or freeze all but 3 output layers.\n            num = (185, len(model_body.layers)-3)[freeze_body-1]\n            for i in range(num): model_body.layers[i].trainable = False\n            print(\'Freeze the first {} layers of total {} layers.\'.format(num, len(model_body.layers)))\n\n    # get output of second last layers and create bottleneck model of it\n    out1=model_body.layers[246].output\n    out2=model_body.layers[247].output\n    out3=model_body.layers[248].output\n    bottleneck_model = Model([model_body.input, *y_true], [out1, out2, out3])\n\n    # create last layer model of last layers from yolo model\n    in0 = Input(shape=bottleneck_model.output[0].shape[1:].as_list()) \n    in1 = Input(shape=bottleneck_model.output[1].shape[1:].as_list())\n    in2 = Input(shape=bottleneck_model.output[2].shape[1:].as_list())\n    last_out0=model_body.layers[249](in0)\n    last_out1=model_body.layers[250](in1)\n    last_out2=model_body.layers[251](in2)\n    model_last=Model(inputs=[in0, in1, in2], outputs=[last_out0, last_out1, last_out2])\n    model_loss_last =Lambda(yolo_loss, output_shape=(1,), name=\'yolo_loss\',\n        arguments={\'anchors\': anchors, \'num_classes\': num_classes, \'ignore_thresh\': 0.5})(\n        [*model_last.output, *y_true])\n    last_layer_model = Model([in0,in1,in2, *y_true], model_loss_last)\n\n    \n    model_loss = Lambda(yolo_loss, output_shape=(1,), name=\'yolo_loss\',\n        arguments={\'anchors\': anchors, \'num_classes\': num_classes, \'ignore_thresh\': 0.5})(\n        [*model_body.output, *y_true])\n    model = Model([model_body.input, *y_true], model_loss)\n\n    return model, bottleneck_model, last_layer_model\n\ndef data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes, random=True, verbose=False):\n    \'\'\'data generator for fit_generator\'\'\'\n    n = len(annotation_lines)\n    i = 0\n    while True:\n        image_data = []\n        box_data = []\n        for b in range(batch_size):\n            if i==0 and random:\n                np.random.shuffle(annotation_lines)\n            image, box = get_random_data(annotation_lines[i], input_shape, random=random)\n            image_data.append(image)\n            box_data.append(box)\n            i = (i+1) % n\n        image_data = np.array(image_data)\n        if verbose:\n            print(""Progress: "",i,""/"",n)\n        box_data = np.array(box_data)\n        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)\n        yield [image_data, *y_true], np.zeros(batch_size)\n\ndef data_generator_wrapper(annotation_lines, batch_size, input_shape, anchors, num_classes, random=True, verbose=False):\n    n = len(annotation_lines)\n    if n==0 or batch_size<=0: return None\n    return data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes, random, verbose)\n\ndef bottleneck_generator(annotation_lines, batch_size, input_shape, anchors, num_classes, bottlenecks):\n    n = len(annotation_lines)\n    i = 0\n    while True:\n        box_data = []\n        b0=np.zeros((batch_size,bottlenecks[0].shape[1],bottlenecks[0].shape[2],bottlenecks[0].shape[3]))\n        b1=np.zeros((batch_size,bottlenecks[1].shape[1],bottlenecks[1].shape[2],bottlenecks[1].shape[3]))\n        b2=np.zeros((batch_size,bottlenecks[2].shape[1],bottlenecks[2].shape[2],bottlenecks[2].shape[3]))\n        for b in range(batch_size):\n            _, box = get_random_data(annotation_lines[i], input_shape, random=False, proc_img=False)\n            box_data.append(box)\n            b0[b]=bottlenecks[0][i]\n            b1[b]=bottlenecks[1][i]\n            b2[b]=bottlenecks[2][i]\n            i = (i+1) % n\n        box_data = np.array(box_data)\n        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)\n        yield [b0, b1, b2, *y_true], np.zeros(batch_size)\n\nif __name__ == \'__main__\':\n    _main()\n'"
voc_annotation.py,0,"b'import xml.etree.ElementTree as ET\nfrom os import getcwd\n\nsets=[(\'2007\', \'train\'), (\'2007\', \'val\'), (\'2007\', \'test\')]\n\nclasses = [""aeroplane"", ""bicycle"", ""bird"", ""boat"", ""bottle"", ""bus"", ""car"", ""cat"", ""chair"", ""cow"", ""diningtable"", ""dog"", ""horse"", ""motorbike"", ""person"", ""pottedplant"", ""sheep"", ""sofa"", ""train"", ""tvmonitor""]\n\n\ndef convert_annotation(year, image_id, list_file):\n    in_file = open(\'VOCdevkit/VOC%s/Annotations/%s.xml\'%(year, image_id))\n    tree=ET.parse(in_file)\n    root = tree.getroot()\n\n    for obj in root.iter(\'object\'):\n        difficult = obj.find(\'difficult\').text\n        cls = obj.find(\'name\').text\n        if cls not in classes or int(difficult)==1:\n            continue\n        cls_id = classes.index(cls)\n        xmlbox = obj.find(\'bndbox\')\n        b = (int(xmlbox.find(\'xmin\').text), int(xmlbox.find(\'ymin\').text), int(xmlbox.find(\'xmax\').text), int(xmlbox.find(\'ymax\').text))\n        list_file.write("" "" + "","".join([str(a) for a in b]) + \',\' + str(cls_id))\n\nwd = getcwd()\n\nfor year, image_set in sets:\n    image_ids = open(\'VOCdevkit/VOC%s/ImageSets/Main/%s.txt\'%(year, image_set)).read().strip().split()\n    list_file = open(\'%s_%s.txt\'%(year, image_set), \'w\')\n    for image_id in image_ids:\n        list_file.write(\'%s/VOCdevkit/VOC%s/JPEGImages/%s.jpg\'%(wd, year, image_id))\n        convert_annotation(year, image_id, list_file)\n        list_file.write(\'\\n\')\n    list_file.close()\n\n'"
yolo.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nClass definition of YOLO_v3 style detection model on image and video\n""""""\n\nimport colorsys\nimport os\nfrom timeit import default_timer as timer\n\nimport numpy as np\nfrom keras import backend as K\nfrom keras.models import load_model\nfrom keras.layers import Input\nfrom PIL import Image, ImageFont, ImageDraw\n\nfrom yolo3.model import yolo_eval, yolo_body, tiny_yolo_body\nfrom yolo3.utils import letterbox_image\nimport os\nfrom keras.utils import multi_gpu_model\n\nclass YOLO(object):\n    _defaults = {\n        ""model_path"": \'model_data/yolo.h5\',\n        ""anchors_path"": \'model_data/yolo_anchors.txt\',\n        ""classes_path"": \'model_data/coco_classes.txt\',\n        ""score"" : 0.3,\n        ""iou"" : 0.45,\n        ""model_image_size"" : (416, 416),\n        ""gpu_num"" : 1,\n    }\n\n    @classmethod\n    def get_defaults(cls, n):\n        if n in cls._defaults:\n            return cls._defaults[n]\n        else:\n            return ""Unrecognized attribute name \'"" + n + ""\'""\n\n    def __init__(self, **kwargs):\n        self.__dict__.update(self._defaults) # set up default values\n        self.__dict__.update(kwargs) # and update with user overrides\n        self.class_names = self._get_class()\n        self.anchors = self._get_anchors()\n        self.sess = K.get_session()\n        self.boxes, self.scores, self.classes = self.generate()\n\n    def _get_class(self):\n        classes_path = os.path.expanduser(self.classes_path)\n        with open(classes_path) as f:\n            class_names = f.readlines()\n        class_names = [c.strip() for c in class_names]\n        return class_names\n\n    def _get_anchors(self):\n        anchors_path = os.path.expanduser(self.anchors_path)\n        with open(anchors_path) as f:\n            anchors = f.readline()\n        anchors = [float(x) for x in anchors.split(\',\')]\n        return np.array(anchors).reshape(-1, 2)\n\n    def generate(self):\n        model_path = os.path.expanduser(self.model_path)\n        assert model_path.endswith(\'.h5\'), \'Keras model or weights must be a .h5 file.\'\n\n        # Load model, or construct model and load weights.\n        num_anchors = len(self.anchors)\n        num_classes = len(self.class_names)\n        is_tiny_version = num_anchors==6 # default setting\n        try:\n            self.yolo_model = load_model(model_path, compile=False)\n        except:\n            self.yolo_model = tiny_yolo_body(Input(shape=(None,None,3)), num_anchors//2, num_classes) \\\n                if is_tiny_version else yolo_body(Input(shape=(None,None,3)), num_anchors//3, num_classes)\n            self.yolo_model.load_weights(self.model_path) # make sure model, anchors and classes match\n        else:\n            assert self.yolo_model.layers[-1].output_shape[-1] == \\\n                num_anchors/len(self.yolo_model.output) * (num_classes + 5), \\\n                \'Mismatch between model and given anchor and class sizes\'\n\n        print(\'{} model, anchors, and classes loaded.\'.format(model_path))\n\n        # Generate colors for drawing bounding boxes.\n        hsv_tuples = [(x / len(self.class_names), 1., 1.)\n                      for x in range(len(self.class_names))]\n        self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n        self.colors = list(\n            map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)),\n                self.colors))\n        np.random.seed(10101)  # Fixed seed for consistent colors across runs.\n        np.random.shuffle(self.colors)  # Shuffle colors to decorrelate adjacent classes.\n        np.random.seed(None)  # Reset seed to default.\n\n        # Generate output tensor targets for filtered bounding boxes.\n        self.input_image_shape = K.placeholder(shape=(2, ))\n        if self.gpu_num>=2:\n            self.yolo_model = multi_gpu_model(self.yolo_model, gpus=self.gpu_num)\n        boxes, scores, classes = yolo_eval(self.yolo_model.output, self.anchors,\n                len(self.class_names), self.input_image_shape,\n                score_threshold=self.score, iou_threshold=self.iou)\n        return boxes, scores, classes\n\n    def detect_image(self, image):\n        start = timer()\n\n        if self.model_image_size != (None, None):\n            assert self.model_image_size[0]%32 == 0, \'Multiples of 32 required\'\n            assert self.model_image_size[1]%32 == 0, \'Multiples of 32 required\'\n            boxed_image = letterbox_image(image, tuple(reversed(self.model_image_size)))\n        else:\n            new_image_size = (image.width - (image.width % 32),\n                              image.height - (image.height % 32))\n            boxed_image = letterbox_image(image, new_image_size)\n        image_data = np.array(boxed_image, dtype=\'float32\')\n\n        print(image_data.shape)\n        image_data /= 255.\n        image_data = np.expand_dims(image_data, 0)  # Add batch dimension.\n\n        out_boxes, out_scores, out_classes = self.sess.run(\n            [self.boxes, self.scores, self.classes],\n            feed_dict={\n                self.yolo_model.input: image_data,\n                self.input_image_shape: [image.size[1], image.size[0]],\n                K.learning_phase(): 0\n            })\n\n        print(\'Found {} boxes for {}\'.format(len(out_boxes), \'img\'))\n\n        font = ImageFont.truetype(font=\'font/FiraMono-Medium.otf\',\n                    size=np.floor(3e-2 * image.size[1] + 0.5).astype(\'int32\'))\n        thickness = (image.size[0] + image.size[1]) // 300\n\n        for i, c in reversed(list(enumerate(out_classes))):\n            predicted_class = self.class_names[c]\n            box = out_boxes[i]\n            score = out_scores[i]\n\n            label = \'{} {:.2f}\'.format(predicted_class, score)\n            draw = ImageDraw.Draw(image)\n            label_size = draw.textsize(label, font)\n\n            top, left, bottom, right = box\n            top = max(0, np.floor(top + 0.5).astype(\'int32\'))\n            left = max(0, np.floor(left + 0.5).astype(\'int32\'))\n            bottom = min(image.size[1], np.floor(bottom + 0.5).astype(\'int32\'))\n            right = min(image.size[0], np.floor(right + 0.5).astype(\'int32\'))\n            print(label, (left, top), (right, bottom))\n\n            if top - label_size[1] >= 0:\n                text_origin = np.array([left, top - label_size[1]])\n            else:\n                text_origin = np.array([left, top + 1])\n\n            # My kingdom for a good redistributable image drawing library.\n            for i in range(thickness):\n                draw.rectangle(\n                    [left + i, top + i, right - i, bottom - i],\n                    outline=self.colors[c])\n            draw.rectangle(\n                [tuple(text_origin), tuple(text_origin + label_size)],\n                fill=self.colors[c])\n            draw.text(text_origin, label, fill=(0, 0, 0), font=font)\n            del draw\n\n        end = timer()\n        print(end - start)\n        return image\n\n    def close_session(self):\n        self.sess.close()\n\ndef detect_video(yolo, video_path, output_path=""""):\n    import cv2\n    vid = cv2.VideoCapture(video_path)\n    if not vid.isOpened():\n        raise IOError(""Couldn\'t open webcam or video"")\n    video_FourCC    = int(vid.get(cv2.CAP_PROP_FOURCC))\n    video_fps       = vid.get(cv2.CAP_PROP_FPS)\n    video_size      = (int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)),\n                        int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n    isOutput = True if output_path != """" else False\n    if isOutput:\n        print(""!!! TYPE:"", type(output_path), type(video_FourCC), type(video_fps), type(video_size))\n        out = cv2.VideoWriter(output_path, video_FourCC, video_fps, video_size)\n    accum_time = 0\n    curr_fps = 0\n    fps = ""FPS: ??""\n    prev_time = timer()\n    while True:\n        return_value, frame = vid.read()\n        image = Image.fromarray(frame)\n        image = yolo.detect_image(image)\n        result = np.asarray(image)\n        curr_time = timer()\n        exec_time = curr_time - prev_time\n        prev_time = curr_time\n        accum_time = accum_time + exec_time\n        curr_fps = curr_fps + 1\n        if accum_time > 1:\n            accum_time = accum_time - 1\n            fps = ""FPS: "" + str(curr_fps)\n            curr_fps = 0\n        cv2.putText(result, text=fps, org=(3, 15), fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n                    fontScale=0.50, color=(255, 0, 0), thickness=2)\n        cv2.namedWindow(""result"", cv2.WINDOW_NORMAL)\n        cv2.imshow(""result"", result)\n        if isOutput:\n            out.write(result)\n        if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n            break\n    yolo.close_session()\n\n'"
yolo_Mobilenet.py,1,"b'#! /usr/bin/env python\r\n# -*- coding: utf-8 -*-\r\n""""""\r\nRun a YOLO_v3 style detection model on test images.\r\n""""""\r\n\r\nimport colorsys\r\nimport os\r\nfrom timeit import default_timer as timer\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom keras import backend as K\r\nfrom keras.models import load_model\r\nfrom keras.layers import Input\r\nfrom PIL import Image, ImageFont, ImageDraw\r\n\r\nfrom yolo3.model_Mobilenet import yolo_eval, yolo_body, tiny_yolo_body\r\nfrom yolo3.utils import letterbox_image\r\nimport os\r\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = \'0\'\r\nfrom keras.utils import multi_gpu_model\r\ngpu_num=1\r\n\r\nclass YOLO(object):\r\n    def __init__(self):\r\n        self.model_path = \'logs/carMobilenet/001_Mobilenet_finetune/trained_weights_final.h5\' # model path or trained weights path\r\n        self.anchors_path = \'model_data/yolo_anchors.txt\'\r\n        self.classes_path = \'model_data/car_classes.txt\'\r\n        self.score = 0.3\r\n        self.iou = 0.45\r\n        self.class_names = self._get_class()\r\n        self.anchors = self._get_anchors()\r\n        self.sess = K.get_session()\r\n        self.model_image_size = (320, 320) # fixed size or (None, None), hw\r\n        self.boxes, self.scores, self.classes = self.generate()\r\n\r\n    def _get_class(self):\r\n        classes_path = os.path.expanduser(self.classes_path)\r\n        with open(classes_path) as f:\r\n            class_names = f.readlines()\r\n        class_names = [c.strip() for c in class_names]\r\n        return class_names\r\n\r\n    def _get_anchors(self):\r\n        anchors_path = os.path.expanduser(self.anchors_path)\r\n        with open(anchors_path) as f:\r\n            anchors = f.readline()\r\n        anchors = [float(x) for x in anchors.split(\',\')]\r\n        return np.array(anchors).reshape(-1, 2)\r\n\r\n    def generate(self):\r\n        \'\'\'to generate the bounding boxes\'\'\'\r\n        model_path = os.path.expanduser(self.model_path)\r\n        assert model_path.endswith(\'.h5\'), \'Keras model or weights must be a .h5 file.\'\r\n\r\n        # Load model, or construct model and load weights.\r\n        num_anchors = len(self.anchors)\r\n        num_classes = len(self.class_names)\r\n        is_tiny_version = num_anchors==6 # default setting\r\n        try:\r\n            self.yolo_model = load_model(model_path, compile=False)\r\n        except:\r\n            self.yolo_model = tiny_yolo_body(Input(shape=(None,None,3)), num_anchors//2, num_classes) \\\r\n                if is_tiny_version else yolo_body(Input(shape=(None,None,3)), num_anchors//3, num_classes)\r\n            self.yolo_model.load_weights(self.model_path) # make sure model, anchors and classes match\r\n        else:\r\n            assert self.yolo_model.layers[-1].output_shape[-1] == \\\r\n                num_anchors/len(self.yolo_model.output) * (num_classes + 5), \\\r\n                \'Mismatch between model and given anchor and class sizes\'\r\n\r\n        print(\'{} model, anchors, and classes loaded.\'.format(model_path))\r\n\r\n        # Generate colors for drawing bounding boxes.\r\n        # hsv_tuples = [(x / len(self.class_names), 1., 1.)\r\n        #               for x in range(len(self.class_names))]\r\n        # self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\r\n        # self.colors = list(\r\n        #     map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)),\r\n        #         self.colors))\r\n        # np.random.seed(10101)  # Fixed seed for consistent colors across runs.\r\n        # np.random.shuffle(self.colors)  # Shuffle colors to decorrelate adjacent classes.\r\n        # np.random.seed(None)  # Reset seed to default.\r\n\r\n        # Generate output tensor targets for filtered bounding boxes.\r\n        self.input_image_shape = K.placeholder(shape=(2, ))\r\n        if gpu_num>=2:\r\n            self.yolo_model = multi_gpu_model(self.yolo_model, gpus=gpu_num)\r\n        boxes, scores, classes = yolo_eval(self.yolo_model.output, self.anchors,\r\n                len(self.class_names), self.input_image_shape,\r\n                score_threshold=self.score, iou_threshold=self.iou)\r\n        # default arg\r\n        # self.yolo_model->\'model_data/yolo.h5\'\r\n        # self.anchors->\'model_data/yolo_anchors.txt\'-> 9 scales for anchors\r\n        return boxes, scores, classes\r\n\r\n    def detect_image(self, image):\r\n        # start = timer()\r\n        rects = []\r\n        if self.model_image_size != (None, None):\r\n            assert self.model_image_size[0]%32 == 0, \'Multiples of 32 required\'\r\n            assert self.model_image_size[1]%32 == 0, \'Multiples of 32 required\'\r\n            boxed_image = letterbox_image(image, tuple(reversed(self.model_image_size)))\r\n        else:\r\n            new_image_size = (image.width - (image.width % 32),\r\n                              image.height - (image.height % 32))\r\n            boxed_image = letterbox_image(image, new_image_size)\r\n        image_data = np.array(boxed_image, dtype=\'float32\')\r\n\r\n        # print(image_data.shape)\r\n        image_data /= 255.\r\n        image_data = np.expand_dims(image_data, 0)  # Add batch dimension.\r\n\r\n        # tf.Session.run(fetches, feed_dict=None)\r\n        # Runs the operations and evaluates the tensors in fetches.\r\n        #\r\n        # Args:\r\n        # fetches: A single graph element, or a list of graph elements(described above).\r\n        #\r\n        # feed_dict: A dictionary that maps graph elements to values(described above).\r\n        #\r\n        # Returns:Either a single value if fetches is a single graph element, or a\r\n        # list of values if fetches is a list(described above).\r\n        out_boxes, out_scores, out_classes = self.sess.run(\r\n            [self.boxes, self.scores, self.classes],\r\n            feed_dict={\r\n                self.yolo_model.input: image_data,\r\n                self.input_image_shape: [image.size[1], image.size[0]],\r\n                K.learning_phase(): 0\r\n            })\r\n\r\n        # print(\'Found {} boxes for {}\'.format(len(out_boxes), \'img\'))\r\n\r\n        # font = ImageFont.truetype(font=\'font/FiraMono-Medium.otf\',\r\n        #             size=np.floor(3e-2 * image.size[1] + 0.5).astype(\'int32\'))\r\n        # thickness = (image.size[0] + image.size[1]) // 500\r\n\r\n        for i, c in reversed(list(enumerate(out_classes))):\r\n            predicted_class = self.class_names[c]\r\n            box = out_boxes[i]\r\n            score = out_scores[i]\r\n\r\n            # label = \'{} {:.2f}\'.format(predicted_class, score)\r\n            # draw = ImageDraw.Draw(image)\r\n            # label_size = draw.textsize(label, font)\r\n\r\n            y1, x1, y2, x2 = box\r\n            y1 = max(0, np.floor(y1 + 0.5).astype(\'float32\'))\r\n            x1 = max(0, np.floor(x1 + 0.5).astype(\'float32\'))\r\n            y2 = min(image.size[1], np.floor(y2 + 0.5).astype(\'float32\'))\r\n            x2 = min(image.size[0], np.floor(x2 + 0.5).astype(\'float32\'))\r\n            # print(label, (x1, y1), (x2, y2))\r\n            bbox = dict([(""score"",str(score)),(""x1"",str(x1)),(""y1"", str(y1)),(""x2"", str(x2)),(""y2"", str(y2))])\r\n            rects.append(bbox)\r\n\r\n        #     if y1 - label_size[1] >= 0:\r\n        #         text_origin = np.array([x1, y1 - label_size[1]])\r\n        #     else:\r\n        #         text_origin = np.array([x1, y1 + 1])\r\n        #\r\n        #     # My kingdom for a good redistributable image drawing library.\r\n        #     for i in range(thickness):\r\n        #         draw.rectangle(\r\n        #             [x1 + i, y1 + i, x2 - i, y2 - i],\r\n        #             outline=self.colors[c])\r\n        #     draw.rectangle(\r\n        #         [tuple(text_origin), tuple(text_origin + label_size)],\r\n        #         fill=self.colors[c])\r\n        #     draw.text(text_origin, label, fill=(0, 0, 0), font=font)\r\n        #     del draw\r\n        #\r\n        # end = timer()\r\n        # print(str(end - start))\r\n        return rects\r\n\r\n    def close_session(self):\r\n        self.sess.close()\r\n\r\n\r\ndef detect_video(yolo, video_path, output_path=""""):\r\n    import cv2\r\n    vid = cv2.VideoCapture(video_path)\r\n    if not vid.isOpened():\r\n        raise IOError(""Couldn\'t open webcam or video"")\r\n    video_FourCC    = int(vid.get(cv2.CAP_PROP_FOURCC))\r\n    video_fps       = vid.get(cv2.CAP_PROP_FPS)\r\n    video_size      = (int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)),\r\n                        int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT)))\r\n    isOutput = True if output_path != """" else False\r\n    if isOutput:\r\n        print(""!!! TYPE:"", type(output_path), type(video_FourCC), type(video_fps), type(video_size))\r\n        out = cv2.VideoWriter(output_path, video_FourCC, video_fps, video_size)\r\n    accum_time = 0\r\n    curr_fps = 0\r\n    fps = ""FPS: ??""\r\n    prev_time = timer()\r\n    while True:\r\n        return_value, frame = vid.read()\r\n        image = Image.fromarray(frame)\r\n        image = yolo.detect_image(image)\r\n        result = np.asarray(image)\r\n        curr_time = timer()\r\n        exec_time = curr_time - prev_time\r\n        prev_time = curr_time\r\n        accum_time = accum_time + exec_time\r\n        curr_fps = curr_fps + 1\r\n        if accum_time > 1:\r\n            accum_time = accum_time - 1\r\n            fps = ""FPS: "" + str(curr_fps)\r\n            curr_fps = 0\r\n        cv2.putText(result, text=fps, org=(3, 15), fontFace=cv2.FONT_HERSHEY_SIMPLEX,\r\n                    fontScale=0.50, color=(255, 0, 0), thickness=2)\r\n        cv2.namedWindow(""result"", cv2.WINDOW_NORMAL)\r\n        cv2.imshow(""result"", result)\r\n        if isOutput:\r\n            out.write(result)\r\n        if cv2.waitKey(1) & 0xFF == ord(\'q\'):\r\n            break\r\n    yolo.close_session()\r\n\r\n\r\ndef detect_img(yolo):\r\n    while True:\r\n        img = input(\'Input image filename:\')\r\n        try:\r\n            image = Image.open(img)\r\n        except:\r\n            print(\'Open Error! Try again!\')\r\n            continue\r\n        else:\r\n            r_image = yolo.detect_image(image)\r\n            r_image.show()\r\n    yolo.close_session()\r\n\r\ndef detect_test_draw(yolo,json_name,test_pic):\r\n    import cv2\r\n    import json\r\n\r\n    data_dst = \'dataset/brainwash/\'\r\n    with open(json_name) as load_f:\r\n        load_dict = json.load(load_f)\r\n        for pic in load_dict:\r\n            picname = pic[\'image_path\']\r\n            root,name = os.path.split(picname)\r\n            print(name)\r\n            image = Image.open(data_dst + picname)\r\n            rects = yolo.detect_image(image)\r\n            frame = cv2.imread(data_dst+picname)\r\n            for rect in rects:\r\n                score, x1, y1, x2, y2 = float(rect[\'score\']),int(float(rect[\'x1\'])),int(float(rect[\'y1\'])),int(float(rect[\'x2\'])),int(float(rect[\'y2\']))\r\n                cv2.rectangle(frame,(x1,y1),(x2,y2),(255,255,255),1)\r\n            cv2.imwrite(test_pic+name,frame)\r\n    yolo.close_session()\r\n\r\ndef detect_test(yolo,json_name,test_out_json = \'caltech_new_result_0.001.json\',data_dst = \'../caltech_ped/caltech-pedestrian-dataset-converter/\'):\r\n    import json\r\n    import time\r\n\r\n    #\r\n    with open(json_name) as load_f:\r\n        load_dict = json.load(load_f)\r\n    count = 0\r\n    json_images=[]\r\n    with open(test_out_json,\'w\') as outfile:\r\n        time_start = time.time()\r\n\r\n        for pic in load_dict:\r\n            # root, filename = os.path.split(pic[\'image_path\'])\r\n            # name = filename.split(\'.\')[0]\r\n            # set_id, v_id, frame_id = name.split(\'_\')\r\n            # frame_id = int(frame_id)\r\n            #\r\n            # if frame_id % 30 == 0 and frame_id != 0:\r\n                picname = pic[\'image_path\'][2:]\r\n                count +=1\r\n                print(picname)\r\n                image = Image.open(data_dst + picname)\r\n                rects = yolo.detect_image(image)\r\n                json_image = dict([(""image_path"", picname), (""rects"", rects)])\r\n                json_images.append(json_image)\r\n\r\n        time_end = time.time()\r\n        duration = time_end - time_start\r\n        print(\'totally cost\', duration)\r\n        print(\'{} pictures , average time {}\'.format(count,duration/count))\r\n        str = json.dumps(json_images,indent=4)\r\n        outfile.write(str)\r\n        outfile.close()\r\n    yolo.close_session()\r\n\r\ndef car_detect(yolo,mainFolder = \'/home/wenwen/Viewnyx/FrameImages/\'):\r\n    import json\r\n    fold_list = range(1, 15)\r\n\r\n    for i in fold_list:\r\n        foldname = mainFolder+\'video\'+str(i)\r\n        list = os.listdir(foldname)  # \xe5\x88\x97\xe5\x87\xba\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xe4\xb8\x8b\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe7\x9b\xae\xe5\xbd\x95\xe4\xb8\x8e\xe6\x96\x87\xe4\xbb\xb6\r\n        json_all = {}\r\n        json_f = open(\'car/\'+\'annotation_{}_YOLOv3.json\'.format(\'video\'+str(i)),\'w\')\r\n        for i in range(0, len(list)):\r\n            name,ext = os.path.splitext(list[i])\r\n            if ext==\'.jpg\':\r\n                print(list[i])\r\n                json_pic = {}\r\n                annotation = []\r\n                image = Image.open(foldname+\'/\'+list[i])\r\n                rects = yolo.detect_image(image)\r\n                for rect in rects:\r\n                    score, x1, y1, x2, y2 = float(rect[\'score\']), int(float(rect[\'x1\'])), int(float(rect[\'y1\'])), int(\r\n                        float(rect[\'x2\'])), int(float(rect[\'y2\']))\r\n                    bbox = {""category"": ""sideways"",\r\n                            ""id"": 0,\r\n                            ""shape"": [""Box"",1],\r\n                            ""label"": ""car"",\r\n                            ""x"":x1,\r\n                            ""y"":y1,\r\n                            ""width"":x2-x1,\r\n                            ""height"":y2-y1,\r\n                            ""score"":score}\r\n                    annotation.append(bbox)\r\n                json_pic[""annotations""]=annotation\r\n                json_pic[""height""] = 480\r\n                json_pic[""name""] =  list[i]\r\n                json_pic[""width""] =  640\r\n                json_all[list[i]] = json_pic\r\n        json_f.write(json.dumps(json_all,indent=4))\r\n        json_f.close()\r\n    yolo.close_session()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    car_detect(YOLO())\r\n    #detect_test(YOLO(), json_name=\'../mrsub/mrsub_test.json\',test_out_json=\'mobilenet_train_bw_test_mrsub.json\', data_dst=\'../mrsub/\')\r\n    #detect_test_draw(YOLO(), json_name=\'dataset/brainwash/test_boxes.json\',test_pic=\'./mobilenet_test/\')\r\n'"
yolo_video.py,0,"b'import sys\nimport argparse\nfrom yolo import YOLO, detect_video\nfrom PIL import Image\n\ndef detect_img(yolo):\n    while True:\n        img = input(\'Input image filename:\')\n        try:\n            image = Image.open(img)\n        except:\n            print(\'Open Error! Try again!\')\n            continue\n        else:\n            r_image = yolo.detect_image(image)\n            r_image.show()\n    yolo.close_session()\n\nFLAGS = None\n\nif __name__ == \'__main__\':\n    # class YOLO defines the default value, so suppress any default here\n    parser = argparse.ArgumentParser(argument_default=argparse.SUPPRESS)\n    \'\'\'\n    Command line options\n    \'\'\'\n    parser.add_argument(\n        \'--model\', type=str,\n        help=\'path to model weight file, default \' + YOLO.get_defaults(""model_path"")\n    )\n\n    parser.add_argument(\n        \'--anchors\', type=str,\n        help=\'path to anchor definitions, default \' + YOLO.get_defaults(""anchors_path"")\n    )\n\n    parser.add_argument(\n        \'--classes\', type=str,\n        help=\'path to class definitions, default \' + YOLO.get_defaults(""classes_path"")\n    )\n\n    parser.add_argument(\n        \'--gpu_num\', type=int,\n        help=\'Number of GPU to use, default \' + str(YOLO.get_defaults(""gpu_num""))\n    )\n\n    parser.add_argument(\n        \'--image\', default=False, action=""store_true"",\n        help=\'Image detection mode, will ignore all positional arguments\'\n    )\n    \'\'\'\n    Command line positional arguments -- for video detection mode\n    \'\'\'\n    parser.add_argument(\n        ""--input"", nargs=\'?\', type=str,required=False,default=\'./path2your_video\',\n        help = ""Video input path""\n    )\n\n    parser.add_argument(\n        ""--output"", nargs=\'?\', type=str, default="""",\n        help = ""[Optional] Video output path""\n    )\n\n    FLAGS = parser.parse_args()\n\n    if FLAGS.image:\n        """"""\n        Image detection mode, disregard any remaining command line arguments\n        """"""\n        print(""Image detection mode"")\n        if ""input"" in FLAGS:\n            print("" Ignoring remaining command line arguments: "" + FLAGS.input + "","" + FLAGS.output)\n        detect_img(YOLO(**vars(FLAGS)))\n    elif ""input"" in FLAGS:\n        detect_video(YOLO(**vars(FLAGS)), FLAGS.input, FLAGS.output)\n    else:\n        print(""Must specify at least video_input_path.  See usage with --help."")\n'"
yolo3/__init__.py,0,b''
yolo3/model.py,7,"b'""""""YOLO_v3 Model Defined in Keras.""""""\n\nfrom functools import wraps\n\nimport numpy as np\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras.layers import Conv2D, Add, ZeroPadding2D, UpSampling2D, Concatenate, MaxPooling2D\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Model\nfrom keras.regularizers import l2\n\nfrom yolo3.utils import compose\n\n\n@wraps(Conv2D)\ndef DarknetConv2D(*args, **kwargs):\n    """"""Wrapper to set Darknet parameters for Convolution2D.""""""\n    darknet_conv_kwargs = {\'kernel_regularizer\': l2(5e-4)}\n    darknet_conv_kwargs[\'padding\'] = \'valid\' if kwargs.get(\'strides\')==(2,2) else \'same\'\n    darknet_conv_kwargs.update(kwargs)\n    return Conv2D(*args, **darknet_conv_kwargs)\n\ndef DarknetConv2D_BN_Leaky(*args, **kwargs):\n    """"""Darknet Convolution2D followed by BatchNormalization and LeakyReLU.""""""\n    no_bias_kwargs = {\'use_bias\': False}\n    no_bias_kwargs.update(kwargs)\n    return compose(\n        DarknetConv2D(*args, **no_bias_kwargs),\n        BatchNormalization(),\n        LeakyReLU(alpha=0.1))\n\ndef resblock_body(x, num_filters, num_blocks):\n    \'\'\'A series of resblocks starting with a downsampling Convolution2D\'\'\'\n    # Darknet uses left and top padding instead of \'same\' mode\n    x = ZeroPadding2D(((1,0),(1,0)))(x)\n    x = DarknetConv2D_BN_Leaky(num_filters, (3,3), strides=(2,2))(x)\n    for i in range(num_blocks):\n        y = compose(\n                DarknetConv2D_BN_Leaky(num_filters//2, (1,1)),\n                DarknetConv2D_BN_Leaky(num_filters, (3,3)))(x)\n        x = Add()([x,y])\n    return x\n\ndef darknet_body(x):\n    \'\'\'Darknent body having 52 Convolution2D layers\'\'\'\n    x = DarknetConv2D_BN_Leaky(32, (3,3))(x)\n    x = resblock_body(x, 64, 1)\n    x = resblock_body(x, 128, 2)\n    x = resblock_body(x, 256, 8)\n    x = resblock_body(x, 512, 8)\n    x = resblock_body(x, 1024, 4)\n    return x\n\ndef make_last_layers(x, num_filters, out_filters):\n    \'\'\'6 Conv2D_BN_Leaky layers followed by a Conv2D_linear layer\'\'\'\n    x = compose(\n            DarknetConv2D_BN_Leaky(num_filters, (1,1)),\n            DarknetConv2D_BN_Leaky(num_filters*2, (3,3)),\n            DarknetConv2D_BN_Leaky(num_filters, (1,1)),\n            DarknetConv2D_BN_Leaky(num_filters*2, (3,3)),\n            DarknetConv2D_BN_Leaky(num_filters, (1,1)))(x)\n    y = compose(\n            DarknetConv2D_BN_Leaky(num_filters*2, (3,3)),\n            DarknetConv2D(out_filters, (1,1)))(x)\n    return x, y\n\n\ndef yolo_body(inputs, num_anchors, num_classes):\n    """"""Create YOLO_V3 model CNN body in Keras.""""""\n    darknet = Model(inputs, darknet_body(inputs))\n    x, y1 = make_last_layers(darknet.output, 512, num_anchors*(num_classes+5))\n\n    x = compose(\n            DarknetConv2D_BN_Leaky(256, (1,1)),\n            UpSampling2D(2))(x)\n    x = Concatenate()([x,darknet.layers[152].output])\n    x, y2 = make_last_layers(x, 256, num_anchors*(num_classes+5))\n\n    x = compose(\n            DarknetConv2D_BN_Leaky(128, (1,1)),\n            UpSampling2D(2))(x)\n    x = Concatenate()([x,darknet.layers[92].output])\n    x, y3 = make_last_layers(x, 128, num_anchors*(num_classes+5))\n\n    return Model(inputs, [y1,y2,y3])\n\ndef tiny_yolo_body(inputs, num_anchors, num_classes):\n    \'\'\'Create Tiny YOLO_v3 model CNN body in keras.\'\'\'\n    x1 = compose(\n            DarknetConv2D_BN_Leaky(16, (3,3)),\n            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\'same\'),\n            DarknetConv2D_BN_Leaky(32, (3,3)),\n            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\'same\'),\n            DarknetConv2D_BN_Leaky(64, (3,3)),\n            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\'same\'),\n            DarknetConv2D_BN_Leaky(128, (3,3)),\n            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\'same\'),\n            DarknetConv2D_BN_Leaky(256, (3,3)))(inputs)\n    x2 = compose(\n            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\'same\'),\n            DarknetConv2D_BN_Leaky(512, (3,3)),\n            MaxPooling2D(pool_size=(2,2), strides=(1,1), padding=\'same\'),\n            DarknetConv2D_BN_Leaky(1024, (3,3)),\n            DarknetConv2D_BN_Leaky(256, (1,1)))(x1)\n    y1 = compose(\n            DarknetConv2D_BN_Leaky(512, (3,3)),\n            DarknetConv2D(num_anchors*(num_classes+5), (1,1)))(x2)\n\n    x2 = compose(\n            DarknetConv2D_BN_Leaky(128, (1,1)),\n            UpSampling2D(2))(x2)\n    y2 = compose(\n            Concatenate(),\n            DarknetConv2D_BN_Leaky(256, (3,3)),\n            DarknetConv2D(num_anchors*(num_classes+5), (1,1)))([x2,x1])\n\n    return Model(inputs, [y1,y2])\n\n\ndef yolo_head(feats, anchors, num_classes, input_shape, calc_loss=False):\n    """"""Convert final layer features to bounding box parameters.""""""\n    num_anchors = len(anchors)\n    # Reshape to batch, height, width, num_anchors, box_params.\n    anchors_tensor = K.reshape(K.constant(anchors), [1, 1, 1, num_anchors, 2])\n\n    grid_shape = K.shape(feats)[1:3] # height, width\n    grid_y = K.tile(K.reshape(K.arange(0, stop=grid_shape[0]), [-1, 1, 1, 1]),\n        [1, grid_shape[1], 1, 1])\n    grid_x = K.tile(K.reshape(K.arange(0, stop=grid_shape[1]), [1, -1, 1, 1]),\n        [grid_shape[0], 1, 1, 1])\n    grid = K.concatenate([grid_x, grid_y])\n    grid = K.cast(grid, K.dtype(feats))\n\n    feats = K.reshape(\n        feats, [-1, grid_shape[0], grid_shape[1], num_anchors, num_classes + 5])\n\n    # Adjust preditions to each spatial grid point and anchor size.\n    box_xy = (K.sigmoid(feats[..., :2]) + grid) / K.cast(grid_shape[::-1], K.dtype(feats))\n    box_wh = K.exp(feats[..., 2:4]) * anchors_tensor / K.cast(input_shape[::-1], K.dtype(feats))\n    box_confidence = K.sigmoid(feats[..., 4:5])\n    box_class_probs = K.sigmoid(feats[..., 5:])\n\n    if calc_loss == True:\n        return grid, feats, box_xy, box_wh\n    return box_xy, box_wh, box_confidence, box_class_probs\n\n\ndef yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape):\n    \'\'\'Get corrected boxes\'\'\'\n    box_yx = box_xy[..., ::-1]\n    box_hw = box_wh[..., ::-1]\n    input_shape = K.cast(input_shape, K.dtype(box_yx))\n    image_shape = K.cast(image_shape, K.dtype(box_yx))\n    new_shape = K.round(image_shape * K.min(input_shape/image_shape))\n    offset = (input_shape-new_shape)/2./input_shape\n    scale = input_shape/new_shape\n    box_yx = (box_yx - offset) * scale\n    box_hw *= scale\n\n    box_mins = box_yx - (box_hw / 2.)\n    box_maxes = box_yx + (box_hw / 2.)\n    boxes =  K.concatenate([\n        box_mins[..., 0:1],  # y_min\n        box_mins[..., 1:2],  # x_min\n        box_maxes[..., 0:1],  # y_max\n        box_maxes[..., 1:2]  # x_max\n    ])\n\n    # Scale boxes back to original image shape.\n    boxes *= K.concatenate([image_shape, image_shape])\n    return boxes\n\n\ndef yolo_boxes_and_scores(feats, anchors, num_classes, input_shape, image_shape):\n    \'\'\'Process Conv layer output\'\'\'\n    box_xy, box_wh, box_confidence, box_class_probs = yolo_head(feats,\n        anchors, num_classes, input_shape)\n    boxes = yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape)\n    boxes = K.reshape(boxes, [-1, 4])\n    box_scores = box_confidence * box_class_probs\n    box_scores = K.reshape(box_scores, [-1, num_classes])\n    return boxes, box_scores\n\n\ndef yolo_eval(yolo_outputs,\n              anchors,\n              num_classes,\n              image_shape,\n              max_boxes=20,\n              score_threshold=.6,\n              iou_threshold=.5):\n    """"""Evaluate YOLO model on given input and return filtered boxes.""""""\n    num_layers = len(yolo_outputs)\n    anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]] # default setting\n    input_shape = K.shape(yolo_outputs[0])[1:3] * 32\n    boxes = []\n    box_scores = []\n    for l in range(num_layers):\n        _boxes, _box_scores = yolo_boxes_and_scores(yolo_outputs[l],\n            anchors[anchor_mask[l]], num_classes, input_shape, image_shape)\n        boxes.append(_boxes)\n        box_scores.append(_box_scores)\n    boxes = K.concatenate(boxes, axis=0)\n    box_scores = K.concatenate(box_scores, axis=0)\n\n    mask = box_scores >= score_threshold\n    max_boxes_tensor = K.constant(max_boxes, dtype=\'int32\')\n    boxes_ = []\n    scores_ = []\n    classes_ = []\n    for c in range(num_classes):\n        # TODO: use keras backend instead of tf.\n        class_boxes = tf.boolean_mask(boxes, mask[:, c])\n        class_box_scores = tf.boolean_mask(box_scores[:, c], mask[:, c])\n        nms_index = tf.image.non_max_suppression(\n            class_boxes, class_box_scores, max_boxes_tensor, iou_threshold=iou_threshold)\n        class_boxes = K.gather(class_boxes, nms_index)\n        class_box_scores = K.gather(class_box_scores, nms_index)\n        classes = K.ones_like(class_box_scores, \'int32\') * c\n        boxes_.append(class_boxes)\n        scores_.append(class_box_scores)\n        classes_.append(classes)\n    boxes_ = K.concatenate(boxes_, axis=0)\n    scores_ = K.concatenate(scores_, axis=0)\n    classes_ = K.concatenate(classes_, axis=0)\n\n    return boxes_, scores_, classes_\n\n\ndef preprocess_true_boxes(true_boxes, input_shape, anchors, num_classes):\n    \'\'\'Preprocess true boxes to training input format\n\n    Parameters\n    ----------\n    true_boxes: array, shape=(m, T, 5)\n        Absolute x_min, y_min, x_max, y_max, class_id relative to input_shape.\n    input_shape: array-like, hw, multiples of 32\n    anchors: array, shape=(N, 2), wh\n    num_classes: integer\n\n    Returns\n    -------\n    y_true: list of array, shape like yolo_outputs, xywh are reletive value\n\n    \'\'\'\n    assert (true_boxes[..., 4]<num_classes).all(), \'class id must be less than num_classes\'\n    num_layers = len(anchors)//3 # default setting\n    anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]]\n\n    true_boxes = np.array(true_boxes, dtype=\'float32\')\n    input_shape = np.array(input_shape, dtype=\'int32\')\n    boxes_xy = (true_boxes[..., 0:2] + true_boxes[..., 2:4]) // 2\n    boxes_wh = true_boxes[..., 2:4] - true_boxes[..., 0:2]\n    true_boxes[..., 0:2] = boxes_xy/input_shape[::-1]\n    true_boxes[..., 2:4] = boxes_wh/input_shape[::-1]\n\n    m = true_boxes.shape[0]\n    grid_shapes = [input_shape//{0:32, 1:16, 2:8}[l] for l in range(num_layers)]\n    y_true = [np.zeros((m,grid_shapes[l][0],grid_shapes[l][1],len(anchor_mask[l]),5+num_classes),\n        dtype=\'float32\') for l in range(num_layers)]\n\n    # Expand dim to apply broadcasting.\n    anchors = np.expand_dims(anchors, 0)\n    anchor_maxes = anchors / 2.\n    anchor_mins = -anchor_maxes\n    valid_mask = boxes_wh[..., 0]>0\n\n    for b in range(m):\n        # Discard zero rows.\n        wh = boxes_wh[b, valid_mask[b]]\n        if len(wh)==0: continue\n        # Expand dim to apply broadcasting.\n        wh = np.expand_dims(wh, -2)\n        box_maxes = wh / 2.\n        box_mins = -box_maxes\n\n        intersect_mins = np.maximum(box_mins, anchor_mins)\n        intersect_maxes = np.minimum(box_maxes, anchor_maxes)\n        intersect_wh = np.maximum(intersect_maxes - intersect_mins, 0.)\n        intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]\n        box_area = wh[..., 0] * wh[..., 1]\n        anchor_area = anchors[..., 0] * anchors[..., 1]\n        iou = intersect_area / (box_area + anchor_area - intersect_area)\n\n        # Find best anchor for each true box\n        best_anchor = np.argmax(iou, axis=-1)\n\n        for t, n in enumerate(best_anchor):\n            for l in range(num_layers):\n                if n in anchor_mask[l]:\n                    i = np.floor(true_boxes[b,t,0]*grid_shapes[l][1]).astype(\'int32\')\n                    j = np.floor(true_boxes[b,t,1]*grid_shapes[l][0]).astype(\'int32\')\n                    k = anchor_mask[l].index(n)\n                    c = true_boxes[b,t, 4].astype(\'int32\')\n                    y_true[l][b, j, i, k, 0:4] = true_boxes[b,t, 0:4]\n                    y_true[l][b, j, i, k, 4] = 1\n                    y_true[l][b, j, i, k, 5+c] = 1\n\n    return y_true\n\n\ndef box_iou(b1, b2):\n    \'\'\'Return iou tensor\n\n    Parameters\n    ----------\n    b1: tensor, shape=(i1,...,iN, 4), xywh\n    b2: tensor, shape=(j, 4), xywh\n\n    Returns\n    -------\n    iou: tensor, shape=(i1,...,iN, j)\n\n    \'\'\'\n\n    # Expand dim to apply broadcasting.\n    b1 = K.expand_dims(b1, -2)\n    b1_xy = b1[..., :2]\n    b1_wh = b1[..., 2:4]\n    b1_wh_half = b1_wh/2.\n    b1_mins = b1_xy - b1_wh_half\n    b1_maxes = b1_xy + b1_wh_half\n\n    # Expand dim to apply broadcasting.\n    b2 = K.expand_dims(b2, 0)\n    b2_xy = b2[..., :2]\n    b2_wh = b2[..., 2:4]\n    b2_wh_half = b2_wh/2.\n    b2_mins = b2_xy - b2_wh_half\n    b2_maxes = b2_xy + b2_wh_half\n\n    intersect_mins = K.maximum(b1_mins, b2_mins)\n    intersect_maxes = K.minimum(b1_maxes, b2_maxes)\n    intersect_wh = K.maximum(intersect_maxes - intersect_mins, 0.)\n    intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]\n    b1_area = b1_wh[..., 0] * b1_wh[..., 1]\n    b2_area = b2_wh[..., 0] * b2_wh[..., 1]\n    iou = intersect_area / (b1_area + b2_area - intersect_area)\n\n    return iou\n\n\ndef yolo_loss(args, anchors, num_classes, ignore_thresh=.5, print_loss=False):\n    \'\'\'Return yolo_loss tensor\n\n    Parameters\n    ----------\n    yolo_outputs: list of tensor, the output of yolo_body or tiny_yolo_body\n    y_true: list of array, the output of preprocess_true_boxes\n    anchors: array, shape=(N, 2), wh\n    num_classes: integer\n    ignore_thresh: float, the iou threshold whether to ignore object confidence loss\n\n    Returns\n    -------\n    loss: tensor, shape=(1,)\n\n    \'\'\'\n    num_layers = len(anchors)//3 # default setting\n    yolo_outputs = args[:num_layers]\n    y_true = args[num_layers:]\n    anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]]\n    input_shape = K.cast(K.shape(yolo_outputs[0])[1:3] * 32, K.dtype(y_true[0]))\n    grid_shapes = [K.cast(K.shape(yolo_outputs[l])[1:3], K.dtype(y_true[0])) for l in range(num_layers)]\n    loss = 0\n    m = K.shape(yolo_outputs[0])[0] # batch size, tensor\n    mf = K.cast(m, K.dtype(yolo_outputs[0]))\n\n    for l in range(num_layers):\n        object_mask = y_true[l][..., 4:5]\n        true_class_probs = y_true[l][..., 5:]\n\n        grid, raw_pred, pred_xy, pred_wh = yolo_head(yolo_outputs[l],\n             anchors[anchor_mask[l]], num_classes, input_shape, calc_loss=True)\n        pred_box = K.concatenate([pred_xy, pred_wh])\n\n        # Darknet raw box to calculate loss.\n        raw_true_xy = y_true[l][..., :2]*grid_shapes[l][::-1] - grid\n        raw_true_wh = K.log(y_true[l][..., 2:4] / anchors[anchor_mask[l]] * input_shape[::-1])\n        raw_true_wh = K.switch(object_mask, raw_true_wh, K.zeros_like(raw_true_wh)) # avoid log(0)=-inf\n        box_loss_scale = 2 - y_true[l][...,2:3]*y_true[l][...,3:4]\n\n        # Find ignore mask, iterate over each of batch.\n        ignore_mask = tf.TensorArray(K.dtype(y_true[0]), size=1, dynamic_size=True)\n        object_mask_bool = K.cast(object_mask, \'bool\')\n        def loop_body(b, ignore_mask):\n            true_box = tf.boolean_mask(y_true[l][b,...,0:4], object_mask_bool[b,...,0])\n            iou = box_iou(pred_box[b], true_box)\n            best_iou = K.max(iou, axis=-1)\n            ignore_mask = ignore_mask.write(b, K.cast(best_iou<ignore_thresh, K.dtype(true_box)))\n            return b+1, ignore_mask\n        _, ignore_mask = K.control_flow_ops.while_loop(lambda b,*args: b<m, loop_body, [0, ignore_mask])\n        ignore_mask = ignore_mask.stack()\n        ignore_mask = K.expand_dims(ignore_mask, -1)\n\n        # K.binary_crossentropy is helpful to avoid exp overflow.\n        xy_loss = object_mask * box_loss_scale * K.binary_crossentropy(raw_true_xy, raw_pred[...,0:2], from_logits=True)\n        wh_loss = object_mask * box_loss_scale * 0.5 * K.square(raw_true_wh-raw_pred[...,2:4])\n        confidence_loss = object_mask * K.binary_crossentropy(object_mask, raw_pred[...,4:5], from_logits=True)+ \\\n            (1-object_mask) * K.binary_crossentropy(object_mask, raw_pred[...,4:5], from_logits=True) * ignore_mask\n        class_loss = object_mask * K.binary_crossentropy(true_class_probs, raw_pred[...,5:], from_logits=True)\n\n        xy_loss = K.sum(xy_loss) / mf\n        wh_loss = K.sum(wh_loss) / mf\n        confidence_loss = K.sum(confidence_loss) / mf\n        class_loss = K.sum(class_loss) / mf\n        loss += xy_loss + wh_loss + confidence_loss + class_loss\n        if print_loss:\n            loss = tf.Print(loss, [loss, xy_loss, wh_loss, confidence_loss, class_loss, K.sum(ignore_mask)], message=\'loss: \')\n    return loss\n'"
yolo3/model_Mobilenet.py,9,"b'""""""YOLO_v3 Model Defined in Keras.""""""\r\n\r\nfrom functools import wraps\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom keras import backend as K\r\nfrom keras.layers import merge,Conv2D, Add, ZeroPadding2D, UpSampling2D, Concatenate, MaxPooling2D\r\nfrom keras.layers.advanced_activations import LeakyReLU\r\nfrom keras.layers.normalization import BatchNormalization\r\nfrom keras.models import Model\r\nfrom keras.applications.mobilenet import MobileNet\r\nfrom keras.regularizers import l2\r\n\r\nfrom yolo3.utils import compose\r\n\r\n\r\n@wraps(Conv2D)\r\ndef DarknetConv2D(*args, **kwargs):\r\n    """"""Wrapper to set Darknet parameters for Convolution2D.""""""\r\n    darknet_conv_kwargs = {\'kernel_regularizer\': l2(5e-4)}\r\n    darknet_conv_kwargs[\'padding\'] = \'valid\' if kwargs.get(\'strides\')==(2,2) else \'same\'\r\n    darknet_conv_kwargs.update(kwargs)\r\n    return Conv2D(*args, **darknet_conv_kwargs)\r\n\r\n\r\ndef leakyRelu(x, leak=0.2, name=""LeakyRelu""):\r\n    with tf.variable_scope(name):\r\n        f1 = 0.5 * (1 + leak)\r\n        f2 = 0.5 * (1 - leak)\r\n        return f1 * x + f2 * tf.abs(x)\r\n\r\ndef DarknetConv2D_BN_Leaky(*args, **kwargs):\r\n    """"""Darknet Convolution2D followed by BatchNormalization and LeakyReLU.""""""\r\n    no_bias_kwargs = {\'use_bias\': False}\r\n    no_bias_kwargs.update(kwargs)\r\n    return compose(\r\n        DarknetConv2D(*args, **no_bias_kwargs),\r\n        BatchNormalization(),\r\n        LeakyReLU(alpha=0.1))\r\n\r\ndef resblock_body(x, num_filters, num_blocks):\r\n    \'\'\'A series of resblocks starting with a downsampling Convolution2D\'\'\'\r\n    # Darknet uses left and top padding instead of \'same\' mode\r\n    x = ZeroPadding2D(((1,0),(1,0)))(x)\r\n    x = DarknetConv2D_BN_Leaky(num_filters, (3,3), strides=(2,2))(x)\r\n    for i in range(num_blocks):\r\n        y = compose(\r\n                DarknetConv2D_BN_Leaky(num_filters//2, (1,1)),\r\n                DarknetConv2D_BN_Leaky(num_filters, (3,3)))(x)\r\n        x = Add()([x,y])\r\n    return x\r\n\r\ndef darknet_body(x):\r\n    \'\'\'Darknent body having 52 Convolution2D layers\'\'\'\r\n    # 416 x 416 x 3\r\n    x = DarknetConv2D_BN_Leaky(32, (3,3))(x)\r\n\r\n    # 208 x 208 x 32\r\n    x = resblock_body(x, 64, 1)\r\n\r\n    # 208 x 208 x 64\r\n    x = resblock_body(x, 128, 2)\r\n\r\n    # 104 x 104 x 128\r\n    x = resblock_body(x, 256, 8)\r\n\r\n    # 52 x 52 x 256\r\n    x = resblock_body(x, 512, 8)\r\n\r\n    # 26 x 26 x 512\r\n    x = resblock_body(x, 1024, 4)\r\n\r\n    # 13 x 13 x 1024\r\n    return x\r\n\r\ndef make_last_layers(x, num_filters, out_filters):\r\n    \'\'\'6 Conv2D_BN_Leaky layers followed by a Conv2D_linear layer\'\'\'\r\n    x = compose(\r\n            DarknetConv2D_BN_Leaky(num_filters, (1,1)),\r\n            DarknetConv2D_BN_Leaky(num_filters*2, (3,3)),\r\n            DarknetConv2D_BN_Leaky(num_filters, (1,1)),\r\n            DarknetConv2D_BN_Leaky(num_filters*2, (3,3)),\r\n            DarknetConv2D_BN_Leaky(num_filters, (1,1)))(x)\r\n    y = compose(\r\n            DarknetConv2D_BN_Leaky(num_filters*2, (3,3)),\r\n            DarknetConv2D(out_filters, (1,1)))(x)\r\n    return x, y\r\n\r\n\r\ndef yolo_body(inputs, num_anchors, num_classes):\r\n    """"""Create YOLO_V3 model CNN body in Keras.""""""\r\n    \'\'\'Layer Nanem: input_1 Output: Tensor(""input_1:0"", shape=(?, 416, 416, 3), dtype=float32)\r\n    Layer Nanem: conv1_pad Output: Tensor(""conv1_pad/Pad:0"", shape=(?, 418, 418, 3), dtype=float32)\r\n    Layer Nanem: conv1 Output: Tensor(""conv1/convolution:0"", shape=(?, 208, 208, 32), dtype=float32)\r\n    Layer Nanem: conv1_bn Output: Tensor(""conv1_bn/cond/Merge:0"", shape=(?, 208, 208, 32), dtype=float32)\r\n    Layer Nanem: conv1_relu Output: Tensor(""conv1_relu/Minimum:0"", shape=(?, 208, 208, 32), dtype=float32)\r\n    Layer Nanem: conv_pad_1 Output: Tensor(""conv_pad_1/Pad:0"", shape=(?, 210, 210, 32), dtype=float32)\r\n    Layer Nanem: conv_dw_1 Output: Tensor(""conv_dw_1/depthwise:0"", shape=(?, 208, 208, 32), dtype=float32)\r\n    Layer Nanem: conv_dw_1_bn Output: Tensor(""conv_dw_1_bn/cond/Merge:0"", shape=(?, 208, 208, 32), dtype=float32)\r\n    Layer Nanem: conv_dw_1_relu Output: Tensor(""conv_dw_1_relu/Minimum:0"", shape=(?, 208, 208, 32), dtype=float32)\r\n    Layer Nanem: conv_pw_1 Output: Tensor(""conv_pw_1/convolution:0"", shape=(?, 208, 208, 64), dtype=float32)\r\n    Layer Nanem: conv_pw_1_bn Output: Tensor(""conv_pw_1_bn/cond/Merge:0"", shape=(?, 208, 208, 64), dtype=float32)\r\n    Layer Nanem: conv_pw_1_relu Output: Tensor(""conv_pw_1_relu/Minimum:0"", shape=(?, 208, 208, 64), dtype=float32)\r\n    Layer Nanem: conv_pad_2 Output: Tensor(""conv_pad_2/Pad:0"", shape=(?, 210, 210, 64), dtype=float32)\r\n    Layer Nanem: conv_dw_2 Output: Tensor(""conv_dw_2/depthwise:0"", shape=(?, 104, 104, 64), dtype=float32)\r\n    Layer Nanem: conv_dw_2_bn Output: Tensor(""conv_dw_2_bn/cond/Merge:0"", shape=(?, 104, 104, 64), dtype=float32)\r\n    Layer Nanem: conv_dw_2_relu Output: Tensor(""conv_dw_2_relu/Minimum:0"", shape=(?, 104, 104, 64), dtype=float32)\r\n    Layer Nanem: conv_pw_2 Output: Tensor(""conv_pw_2/convolution:0"", shape=(?, 104, 104, 128), dtype=float32)\r\n    Layer Nanem: conv_pw_2_bn Output: Tensor(""conv_pw_2_bn/cond/Merge:0"", shape=(?, 104, 104, 128), dtype=float32)\r\n    Layer Nanem: conv_pw_2_relu Output: Tensor(""conv_pw_2_relu/Minimum:0"", shape=(?, 104, 104, 128), dtype=float32)\r\n    Layer Nanem: conv_pad_3 Output: Tensor(""conv_pad_3/Pad:0"", shape=(?, 106, 106, 128), dtype=float32)\r\n    Layer Nanem: conv_dw_3 Output: Tensor(""conv_dw_3/depthwise:0"", shape=(?, 104, 104, 128), dtype=float32)\r\n    Layer Nanem: conv_dw_3_bn Output: Tensor(""conv_dw_3_bn/cond/Merge:0"", shape=(?, 104, 104, 128), dtype=float32)\r\n    Layer Nanem: conv_dw_3_relu Output: Tensor(""conv_dw_3_relu/Minimum:0"", shape=(?, 104, 104, 128), dtype=float32)\r\n    Layer Nanem: conv_pw_3 Output: Tensor(""conv_pw_3/convolution:0"", shape=(?, 104, 104, 128), dtype=float32)\r\n    Layer Nanem: conv_pw_3_bn Output: Tensor(""conv_pw_3_bn/cond/Merge:0"", shape=(?, 104, 104, 128), dtype=float32)\r\n    Layer Nanem: conv_pw_3_relu Output: Tensor(""conv_pw_3_relu/Minimum:0"", shape=(?, 104, 104, 128), dtype=float32)\r\n    Layer Nanem: conv_pad_4 Output: Tensor(""conv_pad_4/Pad:0"", shape=(?, 106, 106, 128), dtype=float32)\r\n    Layer Nanem: conv_dw_4 Output: Tensor(""conv_dw_4/depthwise:0"", shape=(?, 52, 52, 128), dtype=float32)\r\n    Layer Nanem: conv_dw_4_bn Output: Tensor(""conv_dw_4_bn/cond/Merge:0"", shape=(?, 52, 52, 128), dtype=float32)\r\n    Layer Nanem: conv_dw_4_relu Output: Tensor(""conv_dw_4_relu/Minimum:0"", shape=(?, 52, 52, 128), dtype=float32)\r\n    Layer Nanem: conv_pw_4 Output: Tensor(""conv_pw_4/convolution:0"", shape=(?, 52, 52, 256), dtype=float32)\r\n    Layer Nanem: conv_pw_4_bn Output: Tensor(""conv_pw_4_bn/cond/Merge:0"", shape=(?, 52, 52, 256), dtype=float32)\r\n    Layer Nanem: conv_pw_4_relu Output: Tensor(""conv_pw_4_relu/Minimum:0"", shape=(?, 52, 52, 256), dtype=float32)\r\n    Layer Nanem: conv_pad_5 Output: Tensor(""conv_pad_5/Pad:0"", shape=(?, 54, 54, 256), dtype=float32)\r\n    Layer Nanem: conv_dw_5 Output: Tensor(""conv_dw_5/depthwise:0"", shape=(?, 52, 52, 256), dtype=float32)\r\n    Layer Nanem: conv_dw_5_bn Output: Tensor(""conv_dw_5_bn/cond/Merge:0"", shape=(?, 52, 52, 256), dtype=float32)\r\n    Layer Nanem: conv_dw_5_relu Output: Tensor(""conv_dw_5_relu/Minimum:0"", shape=(?, 52, 52, 256), dtype=float32)\r\n    Layer Nanem: conv_pw_5 Output: Tensor(""conv_pw_5/convolution:0"", shape=(?, 52, 52, 256), dtype=float32)\r\n    Layer Nanem: conv_pw_5_bn Output: Tensor(""conv_pw_5_bn/cond/Merge:0"", shape=(?, 52, 52, 256), dtype=float32)\r\n    Layer Nanem: conv_pw_5_relu Output: Tensor(""conv_pw_5_relu/Minimum:0"", shape=(?, 52, 52, 256), dtype=float32)\r\n    Layer Nanem: conv_pad_6 Output: Tensor(""conv_pad_6/Pad:0"", shape=(?, 54, 54, 256), dtype=float32)\r\n    Layer Nanem: conv_dw_6 Output: Tensor(""conv_dw_6/depthwise:0"", shape=(?, 26, 26, 256), dtype=float32)\r\n    Layer Nanem: conv_dw_6_bn Output: Tensor(""conv_dw_6_bn/cond/Merge:0"", shape=(?, 26, 26, 256), dtype=float32)\r\n    Layer Nanem: conv_dw_6_relu Output: Tensor(""conv_dw_6_relu/Minimum:0"", shape=(?, 26, 26, 256), dtype=float32)\r\n    Layer Nanem: conv_pw_6 Output: Tensor(""conv_pw_6/convolution:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_pw_6_bn Output: Tensor(""conv_pw_6_bn/cond/Merge:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_pw_6_relu Output: Tensor(""conv_pw_6_relu/Minimum:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_pad_7 Output: Tensor(""conv_pad_7/Pad:0"", shape=(?, 28, 28, 512), dtype=float32)\r\n    Layer Nanem: conv_dw_7 Output: Tensor(""conv_dw_7/depthwise:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_dw_7_bn Output: Tensor(""conv_dw_7_bn/cond/Merge:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_dw_7_relu Output: Tensor(""conv_dw_7_relu/Minimum:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_pw_7 Output: Tensor(""conv_pw_7/convolution:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_pw_7_bn Output: Tensor(""conv_pw_7_bn/cond/Merge:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_pw_7_relu Output: Tensor(""conv_pw_7_relu/Minimum:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_pad_8 Output: Tensor(""conv_pad_8/Pad:0"", shape=(?, 28, 28, 512), dtype=float32)\r\n    Layer Nanem: conv_dw_8 Output: Tensor(""conv_dw_8/depthwise:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_dw_8_bn Output: Tensor(""conv_dw_8_bn/cond/Merge:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_dw_8_relu Output: Tensor(""conv_dw_8_relu/Minimum:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_pw_8 Output: Tensor(""conv_pw_8/convolution:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_pw_8_bn Output: Tensor(""conv_pw_8_bn/cond/Merge:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_pw_8_relu Output: Tensor(""conv_pw_8_relu/Minimum:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_pad_9 Output: Tensor(""conv_pad_9/Pad:0"", shape=(?, 28, 28, 512), dtype=float32)\r\n    Layer Nanem: conv_dw_9 Output: Tensor(""conv_dw_9/depthwise:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_dw_9_bn Output: Tensor(""conv_dw_9_bn/cond/Merge:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_dw_9_relu Output: Tensor(""conv_dw_9_relu/Minimum:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_pw_9 Output: Tensor(""conv_pw_9/convolution:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_pw_9_bn Output: Tensor(""conv_pw_9_bn/cond/Merge:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_pw_9_relu Output: Tensor(""conv_pw_9_relu/Minimum:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_pad_10 Output: Tensor(""conv_pad_10/Pad:0"", shape=(?, 28, 28, 512), dtype=float32)\r\n    Layer Nanem: conv_dw_10 Output: Tensor(""conv_dw_10/depthwise:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_dw_10_bn Output: Tensor(""conv_dw_10_bn/cond/Merge:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_dw_10_relu Output: Tensor(""conv_dw_10_relu/Minimum:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_pw_10 Output: Tensor(""conv_pw_10/convolution:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_pw_10_bn Output: Tensor(""conv_pw_10_bn/cond/Merge:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_pw_10_relu Output: Tensor(""conv_pw_10_relu/Minimum:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_pad_11 Output: Tensor(""conv_pad_11/Pad:0"", shape=(?, 28, 28, 512), dtype=float32)\r\n    Layer Nanem: conv_dw_11 Output: Tensor(""conv_dw_11/depthwise:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_dw_11_bn Output: Tensor(""conv_dw_11_bn/cond/Merge:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_dw_11_relu Output: Tensor(""conv_dw_11_relu/Minimum:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_pw_11 Output: Tensor(""conv_pw_11/convolution:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_pw_11_bn Output: Tensor(""conv_pw_11_bn/cond/Merge:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_pw_11_relu Output: Tensor(""conv_pw_11_relu/Minimum:0"", shape=(?, 26, 26, 512), dtype=float32)\r\n    Layer Nanem: conv_pad_12 Output: Tensor(""conv_pad_12/Pad:0"", shape=(?, 28, 28, 512), dtype=float32)\r\n    Layer Nanem: conv_dw_12 Output: Tensor(""conv_dw_12/depthwise:0"", shape=(?, 13, 13, 512), dtype=float32)\r\n    Layer Nanem: conv_dw_12_bn Output: Tensor(""conv_dw_12_bn/cond/Merge:0"", shape=(?, 13, 13, 512), dtype=float32)\r\n    Layer Nanem: conv_dw_12_relu Output: Tensor(""conv_dw_12_relu/Minimum:0"", shape=(?, 13, 13, 512), dtype=float32)\r\n    Layer Nanem: conv_pw_12 Output: Tensor(""conv_pw_12/convolution:0"", shape=(?, 13, 13, 1024), dtype=float32)\r\n    Layer Nanem: conv_pw_12_bn Output: Tensor(""conv_pw_12_bn/cond/Merge:0"", shape=(?, 13, 13, 1024), dtype=float32)\r\n    Layer Nanem: conv_pw_12_relu Output: Tensor(""conv_pw_12_relu/Minimum:0"", shape=(?, 13, 13, 1024), dtype=float32)\r\n    Layer Nanem: conv_pad_13 Output: Tensor(""conv_pad_13/Pad:0"", shape=(?, 15, 15, 1024), dtype=float32)\r\n    Layer Nanem: conv_dw_13 Output: Tensor(""conv_dw_13/depthwise:0"", shape=(?, 13, 13, 1024), dtype=float32)\r\n    Layer Nanem: conv_dw_13_bn Output: Tensor(""conv_dw_13_bn/cond/Merge:0"", shape=(?, 13, 13, 1024), dtype=float32)\r\n    Layer Nanem: conv_dw_13_relu Output: Tensor(""conv_dw_13_relu/Minimum:0"", shape=(?, 13, 13, 1024), dtype=float32)\r\n    Layer Nanem: conv_pw_13 Output: Tensor(""conv_pw_13/convolution:0"", shape=(?, 13, 13, 1024), dtype=float32)\r\n    Layer Nanem: conv_pw_13_bn Output: Tensor(""conv_pw_13_bn/cond/Merge:0"", shape=(?, 13, 13, 1024), dtype=float32)\r\n    Layer Nanem: conv_pw_13_relu Output: Tensor(""conv_pw_13_relu/Minimum:0"", shape=(?, 13, 13, 1024), dtype=float32)\r\n    Layer Nanem: global_average_pooling2d_1 Output: Tensor(""global_average_pooling2d_1/Mean:0"", shape=(?, 1024), dtype=float32)\r\n    Layer Nanem: reshape_1 Output: Tensor(""reshape_1/Reshape:0"", shape=(?, 1, 1, 1024), dtype=float32)\r\n    Layer Nanem: dropout Output: Tensor(""dropout/cond/Merge:0"", shape=(?, 1, 1, 1024), dtype=float32)\r\n    Layer Nanem: conv_preds Output: Tensor(""conv_preds/BiasAdd:0"", shape=(?, 1, 1, 1000), dtype=float32)\r\n    Layer Nanem: act_softmax Output: Tensor(""act_softmax/truediv:0"", shape=(?, 1, 1, 1000), dtype=float32)\r\n    Layer Nanem: reshape_2 Output: Tensor(""reshape_2/Reshape:0"", shape=(?, 1000), dtype=float32)\r\n    \'\'\'\r\n\r\n    #net, endpoint = inception_v2.inception_v2(inputs)\r\n    mobilenet = MobileNet(input_tensor=inputs,weights=\'imagenet\')\r\n\r\n    # input: 416 x 416 x 3\r\n    # conv_pw_13_relu :13 x 13 x 1024\r\n    # conv_pw_11_relu :26 x 26 x 512\r\n    # conv_pw_5_relu : 52 x 52 x 256\r\n\r\n    f1 = mobilenet.get_layer(\'conv_pw_13_relu\').output\r\n    # f1 :13 x 13 x 1024\r\n    x, y1 = make_last_layers(f1, 512, num_anchors * (num_classes + 5))\r\n\r\n    x = compose(\r\n            DarknetConv2D_BN_Leaky(256, (1,1)),\r\n            UpSampling2D(2))(x)\r\n\r\n    f2 = mobilenet.get_layer(\'conv_pw_11_relu\').output\r\n    # f2: 26 x 26 x 512\r\n    x = Concatenate()([x,f2])\r\n\r\n    x, y2 = make_last_layers(x, 256, num_anchors*(num_classes+5))\r\n\r\n    x = compose(\r\n            DarknetConv2D_BN_Leaky(128, (1,1)),\r\n            UpSampling2D(2))(x)\r\n\r\n    f3 = mobilenet.get_layer(\'conv_pw_5_relu\').output\r\n    # f3 : 52 x 52 x 256\r\n    x = Concatenate()([x, f3])\r\n    x, y3 = make_last_layers(x, 128, num_anchors*(num_classes+5))\r\n\r\n    return Model(inputs = inputs, outputs=[y1,y2,y3])\r\n\r\ndef tiny_yolo_body(inputs, num_anchors, num_classes):\r\n    \'\'\'Create Tiny YOLO_v3 model CNN body in keras.\'\'\'\r\n    x1 = compose(\r\n            DarknetConv2D_BN_Leaky(16, (3,3)),\r\n            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\'same\'),\r\n            DarknetConv2D_BN_Leaky(32, (3,3)),\r\n            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\'same\'),\r\n            DarknetConv2D_BN_Leaky(64, (3,3)),\r\n            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\'same\'),\r\n            DarknetConv2D_BN_Leaky(128, (3,3)),\r\n            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\'same\'),\r\n            DarknetConv2D_BN_Leaky(256, (3,3)))(inputs)\r\n    x2 = compose(\r\n            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\'same\'),\r\n            DarknetConv2D_BN_Leaky(512, (3,3)),\r\n            MaxPooling2D(pool_size=(2,2), strides=(1,1), padding=\'same\'),\r\n            DarknetConv2D_BN_Leaky(1024, (3,3)),\r\n            DarknetConv2D_BN_Leaky(256, (1,1)))(x1)\r\n    y1 = compose(\r\n            DarknetConv2D_BN_Leaky(512, (3,3)),\r\n            DarknetConv2D(num_anchors*(num_classes+5), (1,1)))(x2)\r\n\r\n    x2 = compose(\r\n            DarknetConv2D_BN_Leaky(128, (1,1)),\r\n            UpSampling2D(2))(x2)\r\n    y2 = compose(\r\n            Concatenate(),\r\n            DarknetConv2D_BN_Leaky(256, (3,3)),\r\n            DarknetConv2D(num_anchors*(num_classes+5), (1,1)))([x2,x1])\r\n\r\n    return Model(inputs, [y1,y2])\r\n\r\n\r\ndef yolo_head(feats, anchors, num_classes, input_shape, calc_loss=False):\r\n    """"""Convert final layer features to bounding box parameters.""""""\r\n    num_anchors = len(anchors)\r\n    # Reshape to batch, height, width, num_anchors, box_params.\r\n    anchors_tensor = K.reshape(K.constant(anchors), [1, 1, 1, num_anchors, 2])\r\n\r\n    grid_shape = K.shape(feats)[1:3] # height, width\r\n    grid_y = K.tile(K.reshape(K.arange(0, stop=grid_shape[0]), [-1, 1, 1, 1]),\r\n        [1, grid_shape[1], 1, 1])\r\n    grid_x = K.tile(K.reshape(K.arange(0, stop=grid_shape[1]), [1, -1, 1, 1]),\r\n        [grid_shape[0], 1, 1, 1])\r\n    grid = K.concatenate([grid_x, grid_y])\r\n    grid = K.cast(grid, K.dtype(feats))\r\n\r\n    feats = K.reshape(\r\n        feats, [-1, grid_shape[0], grid_shape[1], num_anchors, num_classes + 5])\r\n\r\n    # Adjust preditions to each spatial grid point and anchor size.\r\n    box_xy = (K.sigmoid(feats[..., :2]) + grid) / K.cast(grid_shape[::-1], K.dtype(feats))\r\n    box_wh = K.exp(feats[..., 2:4]) * anchors_tensor / K.cast(input_shape[::-1], K.dtype(feats))\r\n    box_confidence = K.sigmoid(feats[..., 4:5])\r\n    box_class_probs = K.sigmoid(feats[..., 5:])\r\n\r\n    if calc_loss == True:\r\n        return grid, feats, box_xy, box_wh\r\n    return box_xy, box_wh, box_confidence, box_class_probs\r\n\r\n\r\ndef yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape):\r\n    \'\'\'Get corrected boxes\'\'\'\r\n    box_yx = box_xy[..., ::-1]\r\n    box_hw = box_wh[..., ::-1]\r\n    input_shape = K.cast(input_shape, K.dtype(box_yx))\r\n    image_shape = K.cast(image_shape, K.dtype(box_yx))\r\n    new_shape = K.round(image_shape * K.min(input_shape/image_shape))\r\n    offset = (input_shape-new_shape)/2./input_shape\r\n    scale = input_shape/new_shape\r\n    box_yx = (box_yx - offset) * scale\r\n    box_hw *= scale\r\n\r\n    box_mins = box_yx - (box_hw / 2.)\r\n    box_maxes = box_yx + (box_hw / 2.)\r\n    boxes =  K.concatenate([\r\n        box_mins[..., 0:1],  # y_min\r\n        box_mins[..., 1:2],  # x_min\r\n        box_maxes[..., 0:1],  # y_max\r\n        box_maxes[..., 1:2]  # x_max\r\n    ])\r\n\r\n    # Scale boxes back to original image shape.\r\n    boxes *= K.concatenate([image_shape, image_shape])\r\n    return boxes\r\n\r\n\r\ndef yolo_boxes_and_scores(feats, anchors, num_classes, input_shape, image_shape):\r\n    \'\'\'Process Conv layer output\'\'\'\r\n    box_xy, box_wh, box_confidence, box_class_probs = yolo_head(feats,\r\n        anchors, num_classes, input_shape)\r\n    boxes = yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape)\r\n    boxes = K.reshape(boxes, [-1, 4])\r\n    box_scores = box_confidence * box_class_probs\r\n    box_scores = K.reshape(box_scores, [-1, num_classes])\r\n    return boxes, box_scores\r\n\r\n\r\ndef yolo_eval(yolo_outputs,\r\n              anchors,\r\n              num_classes,\r\n              image_shape,\r\n              max_boxes=20,\r\n              score_threshold=.6,\r\n              iou_threshold=.5):\r\n    """"""Evaluate YOLO model on given input and return filtered boxes.""""""\r\n\r\n    num_layers = len(yolo_outputs)\r\n\r\n    anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]] # default setting\r\n    input_shape = K.shape(yolo_outputs[0])[1:3] * 32\r\n\r\n    # print(""yolo_outputs"",yolo_outputs)\r\n    boxes = []\r\n    box_scores = []\r\n    for l in range(num_layers):\r\n        _boxes, _box_scores = yolo_boxes_and_scores(yolo_outputs[l],\r\n            anchors[anchor_mask[l]], num_classes, input_shape, image_shape)\r\n        boxes.append(_boxes)\r\n        box_scores.append(_box_scores)\r\n    boxes = K.concatenate(boxes, axis=0)\r\n    box_scores = K.concatenate(box_scores, axis=0)\r\n\r\n    mask = box_scores >= score_threshold\r\n    max_boxes_tensor = K.constant(max_boxes, dtype=\'int32\')\r\n    boxes_ = []\r\n    scores_ = []\r\n    classes_ = []\r\n    for c in range(num_classes):\r\n        # TODO: use keras backend instead of tf.\r\n        class_boxes = tf.boolean_mask(boxes, mask[:, c])\r\n        class_box_scores = tf.boolean_mask(box_scores[:, c], mask[:, c])\r\n        nms_index = tf.image.non_max_suppression(\r\n            class_boxes, class_box_scores, max_boxes_tensor, iou_threshold=iou_threshold)\r\n        class_boxes = K.gather(class_boxes, nms_index)\r\n        class_box_scores = K.gather(class_box_scores, nms_index)\r\n        classes = K.ones_like(class_box_scores, \'int32\') * c\r\n        boxes_.append(class_boxes)\r\n        scores_.append(class_box_scores)\r\n        classes_.append(classes)\r\n    boxes_ = K.concatenate(boxes_, axis=0)\r\n    scores_ = K.concatenate(scores_, axis=0)\r\n    classes_ = K.concatenate(classes_, axis=0)\r\n\r\n    return boxes_, scores_, classes_\r\n\r\n\r\ndef preprocess_true_boxes(true_boxes, input_shape, anchors, num_classes):\r\n    \'\'\'Preprocess true boxes to training input format\r\n\r\n    Parameters\r\n    ----------\r\n    true_boxes: array, shape=(m, T, 5)\r\n        Absolute x_min, y_min, x_max, y_max, class_id relative to input_shape.\r\n    input_shape: array-like, hw, multiples of 32\r\n    anchors: array, shape=(N, 2), wh\r\n    num_classes: integer\r\n\r\n    Returns\r\n    -------\r\n    y_true: list of array, shape like yolo_outputs, xywh are reletive value\r\n\r\n    \'\'\'\r\n    assert (true_boxes[..., 4]<num_classes).all(), \'class id must be less than num_classes\'\r\n    num_layers = len(anchors)//3 # default setting\r\n    anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]]\r\n\r\n    true_boxes = np.array(true_boxes, dtype=\'float32\')\r\n    input_shape = np.array(input_shape, dtype=\'int32\')\r\n    boxes_xy = (true_boxes[..., 0:2] + true_boxes[..., 2:4]) // 2\r\n    boxes_wh = true_boxes[..., 2:4] - true_boxes[..., 0:2]\r\n    true_boxes[..., 0:2] = boxes_xy/input_shape[::-1]\r\n    true_boxes[..., 2:4] = boxes_wh/input_shape[::-1]\r\n\r\n    m = true_boxes.shape[0]\r\n    grid_shapes = [input_shape//{0:32, 1:16, 2:8}[l] for l in range(num_layers)]\r\n    y_true = [np.zeros((m,grid_shapes[l][0],grid_shapes[l][1],len(anchor_mask[l]),5+num_classes),\r\n        dtype=\'float32\') for l in range(num_layers)]\r\n\r\n    # Expand dim to apply broadcasting.\r\n    anchors = np.expand_dims(anchors, 0)\r\n    anchor_maxes = anchors / 2.\r\n    anchor_mins = -anchor_maxes\r\n    valid_mask = boxes_wh[..., 0]>0\r\n\r\n    for b in range(m):\r\n        # Discard zero rows.\r\n        wh = boxes_wh[b, valid_mask[b]]\r\n        if len(wh)==0: continue\r\n        # Expand dim to apply broadcasting.\r\n        wh = np.expand_dims(wh, -2)\r\n        box_maxes = wh / 2.\r\n        box_mins = -box_maxes\r\n\r\n        intersect_mins = np.maximum(box_mins, anchor_mins)\r\n        intersect_maxes = np.minimum(box_maxes, anchor_maxes)\r\n        intersect_wh = np.maximum(intersect_maxes - intersect_mins, 0.)\r\n        intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]\r\n        box_area = wh[..., 0] * wh[..., 1]\r\n        anchor_area = anchors[..., 0] * anchors[..., 1]\r\n        iou = intersect_area / (box_area + anchor_area - intersect_area)\r\n\r\n        # Find best anchor for each true box\r\n        best_anchor = np.argmax(iou, axis=-1)\r\n\r\n        for t, n in enumerate(best_anchor):\r\n            for l in range(num_layers):\r\n                if n in anchor_mask[l]:\r\n                    i = np.floor(true_boxes[b,t,0]*grid_shapes[l][1]).astype(\'int32\')\r\n                    j = np.floor(true_boxes[b,t,1]*grid_shapes[l][0]).astype(\'int32\')\r\n                    k = anchor_mask[l].index(n)\r\n                    c = true_boxes[b,t, 4].astype(\'int32\')\r\n                    y_true[l][b, j, i, k, 0:4] = true_boxes[b,t, 0:4]\r\n                    y_true[l][b, j, i, k, 4] = 1\r\n                    y_true[l][b, j, i, k, 5+c] = 1\r\n\r\n    return y_true\r\n\r\n\r\ndef box_iou(b1, b2):\r\n    \'\'\'Return iou tensor\r\n\r\n    Parameters\r\n    ----------\r\n    b1: tensor, shape=(i1,...,iN, 4), xywh\r\n    b2: tensor, shape=(j, 4), xywh\r\n\r\n    Returns\r\n    -------\r\n    iou: tensor, shape=(i1,...,iN, j)\r\n\r\n    \'\'\'\r\n\r\n    # Expand dim to apply broadcasting.\r\n    b1 = K.expand_dims(b1, -2)\r\n    b1_xy = b1[..., :2]\r\n    b1_wh = b1[..., 2:4]\r\n    b1_wh_half = b1_wh/2.\r\n    b1_mins = b1_xy - b1_wh_half\r\n    b1_maxes = b1_xy + b1_wh_half\r\n\r\n    # Expand dim to apply broadcasting.\r\n    b2 = K.expand_dims(b2, 0)\r\n    b2_xy = b2[..., :2]\r\n    b2_wh = b2[..., 2:4]\r\n    b2_wh_half = b2_wh/2.\r\n    b2_mins = b2_xy - b2_wh_half\r\n    b2_maxes = b2_xy + b2_wh_half\r\n\r\n    intersect_mins = K.maximum(b1_mins, b2_mins)\r\n    intersect_maxes = K.minimum(b1_maxes, b2_maxes)\r\n    intersect_wh = K.maximum(intersect_maxes - intersect_mins, 0.)\r\n    intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]\r\n    b1_area = b1_wh[..., 0] * b1_wh[..., 1]\r\n    b2_area = b2_wh[..., 0] * b2_wh[..., 1]\r\n    iou = intersect_area / (b1_area + b2_area - intersect_area)\r\n\r\n    return iou\r\n\r\n\r\ndef yolo_loss(args, anchors, num_classes, ignore_thresh=.5, print_loss=False):\r\n    \'\'\'Return yolo_loss tensor\r\n\r\n    Parameters\r\n    ----------\r\n    yolo_outputs: list of tensor, the output of yolo_body or tiny_yolo_body\r\n    y_true: list of array, the output of preprocess_true_boxes\r\n    anchors: array, shape=(N, 2), wh\r\n    num_classes: integer\r\n    ignore_thresh: float, the iou threshold whether to ignore object confidence loss\r\n\r\n    Returns\r\n    -------\r\n    loss: tensor, shape=(1,)\r\n\r\n    \'\'\'\r\n    num_layers = len(anchors)//3 # default setting\r\n    yolo_outputs = args[:num_layers]\r\n    y_true = args[num_layers:]\r\n    anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]]\r\n    input_shape = K.cast(K.shape(yolo_outputs[0])[1:3] * 32, K.dtype(y_true[0]))\r\n    grid_shapes = [K.cast(K.shape(yolo_outputs[l])[1:3], K.dtype(y_true[0])) for l in range(num_layers)]\r\n    loss = 0\r\n    m = K.shape(yolo_outputs[0])[0] # batch size, tensor\r\n    mf = K.cast(m, K.dtype(yolo_outputs[0]))\r\n\r\n    for l in range(num_layers):\r\n        object_mask = y_true[l][..., 4:5]\r\n        true_class_probs = y_true[l][..., 5:]\r\n\r\n        grid, raw_pred, pred_xy, pred_wh = yolo_head(yolo_outputs[l],\r\n             anchors[anchor_mask[l]], num_classes, input_shape, calc_loss=True)\r\n        pred_box = K.concatenate([pred_xy, pred_wh])\r\n\r\n        # Darknet raw box to calculate loss.\r\n        raw_true_xy = y_true[l][..., :2]*grid_shapes[l][::-1] - grid\r\n        raw_true_wh = K.log(y_true[l][..., 2:4] / anchors[anchor_mask[l]] * input_shape[::-1])\r\n        raw_true_wh = K.switch(object_mask, raw_true_wh, K.zeros_like(raw_true_wh)) # avoid log(0)=-inf\r\n        box_loss_scale = 2 - y_true[l][...,2:3]*y_true[l][...,3:4]\r\n\r\n        # Find ignore mask, iterate over each of batch.\r\n        ignore_mask = tf.TensorArray(K.dtype(y_true[0]), size=1, dynamic_size=True)\r\n        object_mask_bool = K.cast(object_mask, \'bool\')\r\n        def loop_body(b, ignore_mask):\r\n            true_box = tf.boolean_mask(y_true[l][b,...,0:4], object_mask_bool[b,...,0])\r\n            iou = box_iou(pred_box[b], true_box)\r\n            best_iou = K.max(iou, axis=-1)\r\n            ignore_mask = ignore_mask.write(b, K.cast(best_iou<ignore_thresh, K.dtype(true_box)))\r\n            return b+1, ignore_mask\r\n        _, ignore_mask = K.control_flow_ops.while_loop(lambda b,*args: b<m, loop_body, [0, ignore_mask])\r\n        ignore_mask = ignore_mask.stack()\r\n        ignore_mask = K.expand_dims(ignore_mask, -1)\r\n\r\n        # K.binary_crossentropy is helpful to avoid exp overflow.\r\n        xy_loss = object_mask * box_loss_scale * K.binary_crossentropy(raw_true_xy, raw_pred[...,0:2], from_logits=True)\r\n        wh_loss = object_mask * box_loss_scale * 0.5 * K.square(raw_true_wh-raw_pred[...,2:4])\r\n        confidence_loss = object_mask * K.binary_crossentropy(object_mask, raw_pred[...,4:5], from_logits=True)+ \\\r\n            (1-object_mask) * K.binary_crossentropy(object_mask, raw_pred[...,4:5], from_logits=True) * ignore_mask\r\n        class_loss = object_mask * K.binary_crossentropy(true_class_probs, raw_pred[...,5:], from_logits=True)\r\n\r\n        xy_loss = K.sum(xy_loss) / mf\r\n        wh_loss = K.sum(wh_loss) / mf\r\n        confidence_loss = K.sum(confidence_loss) / mf\r\n        class_loss = K.sum(class_loss) / mf\r\n        loss += xy_loss + wh_loss + confidence_loss + class_loss\r\n        if print_loss:\r\n            loss = tf.Print(loss, [loss, xy_loss, wh_loss, confidence_loss, class_loss, K.sum(ignore_mask)], message=\'loss: \')\r\n    return loss\r\n'"
yolo3/model_vgg16.py,9,"b'""""""YOLO_v3 Model Defined in Keras.""""""\n\nfrom functools import wraps\n\nimport numpy as np\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras.layers import merge,Conv2D, Add, ZeroPadding2D, UpSampling2D, Concatenate, MaxPooling2D\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Model\nfrom keras.applications.vgg16 import VGG16\nfrom keras.regularizers import l2\n\nfrom yolo3.utils import compose\n\n\n@wraps(Conv2D)\ndef DarknetConv2D(*args, **kwargs):\n    """"""Wrapper to set Darknet parameters for Convolution2D.""""""\n    darknet_conv_kwargs = {\'kernel_regularizer\': l2(5e-4)}\n    darknet_conv_kwargs[\'padding\'] = \'valid\' if kwargs.get(\'strides\')==(2,2) else \'same\'\n    darknet_conv_kwargs.update(kwargs)\n    return Conv2D(*args, **darknet_conv_kwargs)\n\n\ndef leakyRelu(x, leak=0.2, name=""LeakyRelu""):\n    with tf.variable_scope(name):\n        f1 = 0.5 * (1 + leak)\n        f2 = 0.5 * (1 - leak)\n        return f1 * x + f2 * tf.abs(x)\n\ndef DarknetConv2D_BN_Leaky(*args, **kwargs):\n    """"""Darknet Convolution2D followed by BatchNormalization and LeakyReLU.""""""\n    no_bias_kwargs = {\'use_bias\': False}\n    no_bias_kwargs.update(kwargs)\n    return compose(\n        DarknetConv2D(*args, **no_bias_kwargs),\n        BatchNormalization(),\n        LeakyReLU(alpha=0.1))\n\ndef resblock_body(x, num_filters, num_blocks):\n    \'\'\'A series of resblocks starting with a downsampling Convolution2D\'\'\'\n    # Darknet uses left and top padding instead of \'same\' mode\n    x = ZeroPadding2D(((1,0),(1,0)))(x)\n    x = DarknetConv2D_BN_Leaky(num_filters, (3,3), strides=(2,2))(x)\n    for i in range(num_blocks):\n        y = compose(\n                DarknetConv2D_BN_Leaky(num_filters//2, (1,1)),\n                DarknetConv2D_BN_Leaky(num_filters, (3,3)))(x)\n        x = Add()([x,y])\n    return x\n\ndef darknet_body(x):\n    \'\'\'Darknent body having 52 Convolution2D layers\'\'\'\n    # 416 x 416 x 3\n    x = DarknetConv2D_BN_Leaky(32, (3,3))(x)\n\n    # 208 x 208 x 32\n    x = resblock_body(x, 64, 1)\n\n    # 208 x 208 x 64\n    x = resblock_body(x, 128, 2)\n\n    # 104 x 104 x 128\n    x = resblock_body(x, 256, 8)\n\n    # 52 x 52 x 256\n    x = resblock_body(x, 512, 8)\n\n    # 26 x 26 x 512\n    x = resblock_body(x, 1024, 4)\n\n    # 13 x 13 x 1024\n    return x\n\ndef make_last_layers(x, num_filters, out_filters):\n    \'\'\'6 Conv2D_BN_Leaky layers followed by a Conv2D_linear layer\'\'\'\n    x = compose(\n            DarknetConv2D_BN_Leaky(num_filters, (1,1)),\n            DarknetConv2D_BN_Leaky(num_filters*2, (3,3)),\n            DarknetConv2D_BN_Leaky(num_filters, (1,1)),\n            DarknetConv2D_BN_Leaky(num_filters*2, (3,3)),\n            DarknetConv2D_BN_Leaky(num_filters, (1,1)))(x)\n    y = compose(\n            DarknetConv2D_BN_Leaky(num_filters*2, (3,3)),\n            DarknetConv2D(out_filters, (1,1)))(x)\n    return x, y\n\n\ndef yolo_body(inputs, num_anchors, num_classes):\n    """"""Create YOLO_V3 model CNN body in Keras.""""""\n    \'\'\'\n    Layer Name input_1 Output: Tensor(""input_1:0"", shape=(?, 416, 416, 3), dtype=float32)\n    Layer Name block1_conv1 Output: Tensor(""block1_conv1/Relu:0"", shape=(?, 416, 416, 64), dtype=float32)\n    Layer Name block1_conv2 Output: Tensor(""block1_conv2/Relu:0"", shape=(?, 416, 416, 64), dtype=float32)\n    Layer Name block1_pool Output: Tensor(""block1_pool/MaxPool:0"", shape=(?, 208, 208, 64), dtype=float32)\n    Layer Name block2_conv1 Output: Tensor(""block2_conv1/Relu:0"", shape=(?, 208, 208, 128), dtype=float32)\n    Layer Name block2_conv2 Output: Tensor(""block2_conv2/Relu:0"", shape=(?, 208, 208, 128), dtype=float32)\n    Layer Name block2_pool Output: Tensor(""block2_pool/MaxPool:0"", shape=(?, 104, 104, 128), dtype=float32)\n    Layer Name block3_conv1 Output: Tensor(""block3_conv1/Relu:0"", shape=(?, 104, 104, 256), dtype=float32)\n    Layer Name block3_conv2 Output: Tensor(""block3_conv2/Relu:0"", shape=(?, 104, 104, 256), dtype=float32)\n    Layer Name block3_conv3 Output: Tensor(""block3_conv3/Relu:0"", shape=(?, 104, 104, 256), dtype=float32)\n    Layer Name block3_pool Output: Tensor(""block3_pool/MaxPool:0"", shape=(?, 52, 52, 256), dtype=float32)\n    Layer Name block4_conv1 Output: Tensor(""block4_conv1/Relu:0"", shape=(?, 52, 52, 512), dtype=float32)\n    Layer Name block4_conv2 Output: Tensor(""block4_conv2/Relu:0"", shape=(?, 52, 52, 512), dtype=float32)\n    Layer Name block4_conv3 Output: Tensor(""block4_conv3/Relu:0"", shape=(?, 52, 52, 512), dtype=float32)\n    Layer Name block4_pool Output: Tensor(""block4_pool/MaxPool:0"", shape=(?, 26, 26, 512), dtype=float32)\n    Layer Name block5_conv1 Output: Tensor(""block5_conv1/Relu:0"", shape=(?, 26, 26, 512), dtype=float32)\n    Layer Name block5_conv2 Output: Tensor(""block5_conv2/Relu:0"", shape=(?, 26, 26, 512), dtype=float32)\n    Layer Name block5_conv3 Output: Tensor(""block5_conv3/Relu:0"", shape=(?, 26, 26, 512), dtype=float32)\n    Layer Name block5_pool Output: Tensor(""block5_pool/MaxPool:0"", shape=(?, 13, 13, 512), dtype=float32)\n    \'\'\'\n\n    #net, endpoint = inception_v2.inception_v2(inputs)\n    vgg16 = VGG16(input_tensor=inputs,weights=\'imagenet\',include_top=False)\n    x = vgg16.get_layer(\'block5_pool\').output\n    x = Conv2D(512, (3, 3), activation=\'relu\', padding=\'same\', name=\'block6_conv1\')(x)\n    x = Conv2D(512, (3, 3), activation=\'relu\', padding=\'same\', name=\'block6_conv2\')(x)\n    x = Conv2D(512, (3, 3), activation=\'relu\', padding=\'same\', name=\'block6_conv3\')(x)\n    x = Conv2D(512, (3, 3), activation=\'relu\', padding=\'same\', name=\'block6_conv4\')(x)\n\n    # input: 416 x 416 x 3\n    # block6_conv3 :13 x 13 x 512\n    # block5_conv3 :26 x 26 x 512\n    # block4_conv3 : 52 x 52 x 512\n\n\n    # f1 :13 x 13 x 1024 13 x 13 x 512\n    x, y1 = make_last_layers(x, 512, num_anchors * (num_classes + 5))\n\n    x = compose(\n            DarknetConv2D_BN_Leaky(256, (1,1)),\n            UpSampling2D(2))(x)\n\n    f2 = vgg16.get_layer(\'block5_conv3\').output\n    # f2: 26 x 26 x 512\n    x = Concatenate()([x,f2])\n\n    x, y2 = make_last_layers(x, 256, num_anchors*(num_classes+5))\n\n    x = compose(\n            DarknetConv2D_BN_Leaky(128, (1,1)),\n            UpSampling2D(2))(x)\n\n    f3 = vgg16.get_layer(\'block4_conv3\').output\n    # f3 : 52 x 52 x 256\n    x = Concatenate()([x, f3])\n    x, y3 = make_last_layers(x, 128, num_anchors*(num_classes+5))\n\n    return Model(inputs = inputs, outputs=[y1,y2,y3])\n\ndef tiny_yolo_body(inputs, num_anchors, num_classes):\n    \'\'\'Create Tiny YOLO_v3 model CNN body in keras.\'\'\'\n    x1 = compose(\n            DarknetConv2D_BN_Leaky(16, (3,3)),\n            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\'same\'),\n            DarknetConv2D_BN_Leaky(32, (3,3)),\n            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\'same\'),\n            DarknetConv2D_BN_Leaky(64, (3,3)),\n            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\'same\'),\n            DarknetConv2D_BN_Leaky(128, (3,3)),\n            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\'same\'),\n            DarknetConv2D_BN_Leaky(256, (3,3)))(inputs)\n    x2 = compose(\n            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\'same\'),\n            DarknetConv2D_BN_Leaky(512, (3,3)),\n            MaxPooling2D(pool_size=(2,2), strides=(1,1), padding=\'same\'),\n            DarknetConv2D_BN_Leaky(1024, (3,3)),\n            DarknetConv2D_BN_Leaky(256, (1,1)))(x1)\n    y1 = compose(\n            DarknetConv2D_BN_Leaky(512, (3,3)),\n            DarknetConv2D(num_anchors*(num_classes+5), (1,1)))(x2)\n\n    x2 = compose(\n            DarknetConv2D_BN_Leaky(128, (1,1)),\n            UpSampling2D(2))(x2)\n    y2 = compose(\n            Concatenate(),\n            DarknetConv2D_BN_Leaky(256, (3,3)),\n            DarknetConv2D(num_anchors*(num_classes+5), (1,1)))([x2,x1])\n\n    return Model(inputs, [y1,y2])\n\n\ndef yolo_head(feats, anchors, num_classes, input_shape, calc_loss=False):\n    """"""Convert final layer features to bounding box parameters.""""""\n    num_anchors = len(anchors)\n    # Reshape to batch, height, width, num_anchors, box_params.\n    anchors_tensor = K.reshape(K.constant(anchors), [1, 1, 1, num_anchors, 2])\n\n    grid_shape = K.shape(feats)[1:3] # height, width\n    grid_y = K.tile(K.reshape(K.arange(0, stop=grid_shape[0]), [-1, 1, 1, 1]),\n        [1, grid_shape[1], 1, 1])\n    grid_x = K.tile(K.reshape(K.arange(0, stop=grid_shape[1]), [1, -1, 1, 1]),\n        [grid_shape[0], 1, 1, 1])\n    grid = K.concatenate([grid_x, grid_y])\n    grid = K.cast(grid, K.dtype(feats))\n\n    feats = K.reshape(\n        feats, [-1, grid_shape[0], grid_shape[1], num_anchors, num_classes + 5])\n\n    # Adjust preditions to each spatial grid point and anchor size.\n    box_xy = (K.sigmoid(feats[..., :2]) + grid) / K.cast(grid_shape[::-1], K.dtype(feats))\n    box_wh = K.exp(feats[..., 2:4]) * anchors_tensor / K.cast(input_shape[::-1], K.dtype(feats))\n    box_confidence = K.sigmoid(feats[..., 4:5])\n    box_class_probs = K.sigmoid(feats[..., 5:])\n\n    if calc_loss == True:\n        return grid, feats, box_xy, box_wh\n    return box_xy, box_wh, box_confidence, box_class_probs\n\n\ndef yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape):\n    \'\'\'Get corrected boxes\'\'\'\n    box_yx = box_xy[..., ::-1]\n    box_hw = box_wh[..., ::-1]\n    input_shape = K.cast(input_shape, K.dtype(box_yx))\n    image_shape = K.cast(image_shape, K.dtype(box_yx))\n    new_shape = K.round(image_shape * K.min(input_shape/image_shape))\n    offset = (input_shape-new_shape)/2./input_shape\n    scale = input_shape/new_shape\n    box_yx = (box_yx - offset) * scale\n    box_hw *= scale\n\n    box_mins = box_yx - (box_hw / 2.)\n    box_maxes = box_yx + (box_hw / 2.)\n    boxes =  K.concatenate([\n        box_mins[..., 0:1],  # y_min\n        box_mins[..., 1:2],  # x_min\n        box_maxes[..., 0:1],  # y_max\n        box_maxes[..., 1:2]  # x_max\n    ])\n\n    # Scale boxes back to original image shape.\n    boxes *= K.concatenate([image_shape, image_shape])\n    return boxes\n\n\ndef yolo_boxes_and_scores(feats, anchors, num_classes, input_shape, image_shape):\n    \'\'\'Process Conv layer output\'\'\'\n    box_xy, box_wh, box_confidence, box_class_probs = yolo_head(feats,\n        anchors, num_classes, input_shape)\n    boxes = yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape)\n    boxes = K.reshape(boxes, [-1, 4])\n    box_scores = box_confidence * box_class_probs\n    box_scores = K.reshape(box_scores, [-1, num_classes])\n    return boxes, box_scores\n\n\ndef yolo_eval(yolo_outputs,\n              anchors,\n              num_classes,\n              image_shape,\n              max_boxes=20,\n              score_threshold=.6,\n              iou_threshold=.5):\n    """"""Evaluate YOLO model on given input and return filtered boxes.""""""\n\n    num_layers = len(yolo_outputs)\n\n    anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]] # default setting\n    input_shape = K.shape(yolo_outputs[0])[1:3] * 32\n\n    # print(""yolo_outputs"",yolo_outputs)\n    boxes = []\n    box_scores = []\n    for l in range(num_layers):\n        _boxes, _box_scores = yolo_boxes_and_scores(yolo_outputs[l],\n            anchors[anchor_mask[l]], num_classes, input_shape, image_shape)\n        boxes.append(_boxes)\n        box_scores.append(_box_scores)\n    boxes = K.concatenate(boxes, axis=0)\n    box_scores = K.concatenate(box_scores, axis=0)\n\n    mask = box_scores >= score_threshold\n    max_boxes_tensor = K.constant(max_boxes, dtype=\'int32\')\n    boxes_ = []\n    scores_ = []\n    classes_ = []\n    for c in range(num_classes):\n        # TODO: use keras backend instead of tf.\n        class_boxes = tf.boolean_mask(boxes, mask[:, c])\n        class_box_scores = tf.boolean_mask(box_scores[:, c], mask[:, c])\n        nms_index = tf.image.non_max_suppression(\n            class_boxes, class_box_scores, max_boxes_tensor, iou_threshold=iou_threshold)\n        class_boxes = K.gather(class_boxes, nms_index)\n        class_box_scores = K.gather(class_box_scores, nms_index)\n        classes = K.ones_like(class_box_scores, \'int32\') * c\n        boxes_.append(class_boxes)\n        scores_.append(class_box_scores)\n        classes_.append(classes)\n    boxes_ = K.concatenate(boxes_, axis=0)\n    scores_ = K.concatenate(scores_, axis=0)\n    classes_ = K.concatenate(classes_, axis=0)\n\n    return boxes_, scores_, classes_\n\n\ndef preprocess_true_boxes(true_boxes, input_shape, anchors, num_classes):\n    \'\'\'Preprocess true boxes to training input format\n\n    Parameters\n    ----------\n    true_boxes: array, shape=(m, T, 5)\n        Absolute x_min, y_min, x_max, y_max, class_id relative to input_shape.\n    input_shape: array-like, hw, multiples of 32\n    anchors: array, shape=(N, 2), wh\n    num_classes: integer\n\n    Returns\n    -------\n    y_true: list of array, shape like yolo_outputs, xywh are reletive value\n\n    \'\'\'\n    assert (true_boxes[..., 4]<num_classes).all(), \'class id must be less than num_classes\'\n    num_layers = len(anchors)//3 # default setting\n    anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]]\n\n    true_boxes = np.array(true_boxes, dtype=\'float32\')\n    input_shape = np.array(input_shape, dtype=\'int32\')\n    boxes_xy = (true_boxes[..., 0:2] + true_boxes[..., 2:4]) // 2\n    boxes_wh = true_boxes[..., 2:4] - true_boxes[..., 0:2]\n    true_boxes[..., 0:2] = boxes_xy/input_shape[::-1]\n    true_boxes[..., 2:4] = boxes_wh/input_shape[::-1]\n\n    m = true_boxes.shape[0]\n    grid_shapes = [input_shape//{0:32, 1:16, 2:8}[l] for l in range(num_layers)]\n    y_true = [np.zeros((m,grid_shapes[l][0],grid_shapes[l][1],len(anchor_mask[l]),5+num_classes),\n        dtype=\'float32\') for l in range(num_layers)]\n\n    # Expand dim to apply broadcasting.\n    anchors = np.expand_dims(anchors, 0)\n    anchor_maxes = anchors / 2.\n    anchor_mins = -anchor_maxes\n    valid_mask = boxes_wh[..., 0]>0\n\n    for b in range(m):\n        # Discard zero rows.\n        wh = boxes_wh[b, valid_mask[b]]\n        if len(wh)==0: continue\n        # Expand dim to apply broadcasting.\n        wh = np.expand_dims(wh, -2)\n        box_maxes = wh / 2.\n        box_mins = -box_maxes\n\n        intersect_mins = np.maximum(box_mins, anchor_mins)\n        intersect_maxes = np.minimum(box_maxes, anchor_maxes)\n        intersect_wh = np.maximum(intersect_maxes - intersect_mins, 0.)\n        intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]\n        box_area = wh[..., 0] * wh[..., 1]\n        anchor_area = anchors[..., 0] * anchors[..., 1]\n        iou = intersect_area / (box_area + anchor_area - intersect_area)\n\n        # Find best anchor for each true box\n        best_anchor = np.argmax(iou, axis=-1)\n\n        for t, n in enumerate(best_anchor):\n            for l in range(num_layers):\n                if n in anchor_mask[l]:\n                    i = np.floor(true_boxes[b,t,0]*grid_shapes[l][1]).astype(\'int32\')\n                    j = np.floor(true_boxes[b,t,1]*grid_shapes[l][0]).astype(\'int32\')\n                    k = anchor_mask[l].index(n)\n                    c = true_boxes[b,t, 4].astype(\'int32\')\n                    y_true[l][b, j, i, k, 0:4] = true_boxes[b,t, 0:4]\n                    y_true[l][b, j, i, k, 4] = 1\n                    y_true[l][b, j, i, k, 5+c] = 1\n\n    return y_true\n\n\ndef box_iou(b1, b2):\n    \'\'\'Return iou tensor\n\n    Parameters\n    ----------\n    b1: tensor, shape=(i1,...,iN, 4), xywh\n    b2: tensor, shape=(j, 4), xywh\n\n    Returns\n    -------\n    iou: tensor, shape=(i1,...,iN, j)\n\n    \'\'\'\n\n    # Expand dim to apply broadcasting.\n    b1 = K.expand_dims(b1, -2)\n    b1_xy = b1[..., :2]\n    b1_wh = b1[..., 2:4]\n    b1_wh_half = b1_wh/2.\n    b1_mins = b1_xy - b1_wh_half\n    b1_maxes = b1_xy + b1_wh_half\n\n    # Expand dim to apply broadcasting.\n    b2 = K.expand_dims(b2, 0)\n    b2_xy = b2[..., :2]\n    b2_wh = b2[..., 2:4]\n    b2_wh_half = b2_wh/2.\n    b2_mins = b2_xy - b2_wh_half\n    b2_maxes = b2_xy + b2_wh_half\n\n    intersect_mins = K.maximum(b1_mins, b2_mins)\n    intersect_maxes = K.minimum(b1_maxes, b2_maxes)\n    intersect_wh = K.maximum(intersect_maxes - intersect_mins, 0.)\n    intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]\n    b1_area = b1_wh[..., 0] * b1_wh[..., 1]\n    b2_area = b2_wh[..., 0] * b2_wh[..., 1]\n    iou = intersect_area / (b1_area + b2_area - intersect_area)\n\n    return iou\n\n\ndef yolo_loss(args, anchors, num_classes, ignore_thresh=.5, print_loss=False):\n    \'\'\'Return yolo_loss tensor\n\n    Parameters\n    ----------\n    yolo_outputs: list of tensor, the output of yolo_body or tiny_yolo_body\n    y_true: list of array, the output of preprocess_true_boxes\n    anchors: array, shape=(N, 2), wh\n    num_classes: integer\n    ignore_thresh: float, the iou threshold whether to ignore object confidence loss\n\n    Returns\n    -------\n    loss: tensor, shape=(1,)\n\n    \'\'\'\n    num_layers = len(anchors)//3 # default setting\n    yolo_outputs = args[:num_layers]\n    y_true = args[num_layers:]\n    anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]]\n    input_shape = K.cast(K.shape(yolo_outputs[0])[1:3] * 32, K.dtype(y_true[0]))\n    grid_shapes = [K.cast(K.shape(yolo_outputs[l])[1:3], K.dtype(y_true[0])) for l in range(num_layers)]\n    loss = 0\n    m = K.shape(yolo_outputs[0])[0] # batch size, tensor\n    mf = K.cast(m, K.dtype(yolo_outputs[0]))\n\n    for l in range(num_layers):\n        object_mask = y_true[l][..., 4:5]\n        true_class_probs = y_true[l][..., 5:]\n\n        grid, raw_pred, pred_xy, pred_wh = yolo_head(yolo_outputs[l],\n             anchors[anchor_mask[l]], num_classes, input_shape, calc_loss=True)\n        pred_box = K.concatenate([pred_xy, pred_wh])\n\n        # Darknet raw box to calculate loss.\n        raw_true_xy = y_true[l][..., :2]*grid_shapes[l][::-1] - grid\n        raw_true_wh = K.log(y_true[l][..., 2:4] / anchors[anchor_mask[l]] * input_shape[::-1])\n        raw_true_wh = K.switch(object_mask, raw_true_wh, K.zeros_like(raw_true_wh)) # avoid log(0)=-inf\n        box_loss_scale = 2 - y_true[l][...,2:3]*y_true[l][...,3:4]\n\n        # Find ignore mask, iterate over each of batch.\n        ignore_mask = tf.TensorArray(K.dtype(y_true[0]), size=1, dynamic_size=True)\n        object_mask_bool = K.cast(object_mask, \'bool\')\n        def loop_body(b, ignore_mask):\n            true_box = tf.boolean_mask(y_true[l][b,...,0:4], object_mask_bool[b,...,0])\n            iou = box_iou(pred_box[b], true_box)\n            best_iou = K.max(iou, axis=-1)\n            ignore_mask = ignore_mask.write(b, K.cast(best_iou<ignore_thresh, K.dtype(true_box)))\n            return b+1, ignore_mask\n        _, ignore_mask = K.control_flow_ops.while_loop(lambda b,*args: b<m, loop_body, [0, ignore_mask])\n        ignore_mask = ignore_mask.stack()\n        ignore_mask = K.expand_dims(ignore_mask, -1)\n\n        # K.binary_crossentropy is helpful to avoid exp overflow.\n        xy_loss = object_mask * box_loss_scale * K.binary_crossentropy(raw_true_xy, raw_pred[...,0:2], from_logits=True)\n        wh_loss = object_mask * box_loss_scale * 0.5 * K.square(raw_true_wh-raw_pred[...,2:4])\n        confidence_loss = object_mask * K.binary_crossentropy(object_mask, raw_pred[...,4:5], from_logits=True)+ \\\n            (1-object_mask) * K.binary_crossentropy(object_mask, raw_pred[...,4:5], from_logits=True) * ignore_mask\n        class_loss = object_mask * K.binary_crossentropy(true_class_probs, raw_pred[...,5:], from_logits=True)\n\n        xy_loss = K.sum(xy_loss) / mf\n        wh_loss = K.sum(wh_loss) / mf\n        confidence_loss = K.sum(confidence_loss) / mf\n        class_loss = K.sum(class_loss) / mf\n        loss += xy_loss + wh_loss + confidence_loss + class_loss\n        if print_loss:\n            loss = tf.Print(loss, [loss, xy_loss, wh_loss, confidence_loss, class_loss, K.sum(ignore_mask)], message=\'loss: \')\n    return loss\n'"
yolo3/utils.py,0,"b'""""""Miscellaneous utility functions.""""""\n\nfrom functools import reduce\n\nfrom PIL import Image\nimport numpy as np\nfrom matplotlib.colors import rgb_to_hsv, hsv_to_rgb\n\ndef compose(*funcs):\n    """"""Compose arbitrarily many functions, evaluated left to right.\n\n    Reference: https://mathieularose.com/function-composition-in-python/\n    """"""\n    # return lambda x: reduce(lambda v, f: f(v), funcs, x)\n    if funcs:\n        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)\n    else:\n        raise ValueError(\'Composition of empty sequence not supported.\')\n\ndef letterbox_image(image, size):\n    \'\'\'resize image with unchanged aspect ratio using padding\'\'\'\n    iw, ih = image.size\n    w, h = size\n    scale = min(w/iw, h/ih)\n    nw = int(iw*scale)\n    nh = int(ih*scale)\n\n    image = image.resize((nw,nh), Image.BICUBIC)\n    new_image = Image.new(\'RGB\', size, (128,128,128))\n    new_image.paste(image, ((w-nw)//2, (h-nh)//2))\n    return new_image\n\ndef rand(a=0, b=1):\n    return np.random.rand()*(b-a) + a\n\ndef get_random_data(annotation_line, input_shape, random=True, max_boxes=20, jitter=.3, hue=.1, sat=1.5, val=1.5, proc_img=True):\n    \'\'\'random preprocessing for real-time data augmentation\'\'\'\n    line = annotation_line.split()\n    image = Image.open(line[0])\n    iw, ih = image.size\n    h, w = input_shape\n    box = np.array([np.array(list(map(int,box.split(\',\')))) for box in line[1:]])\n\n    if not random:\n        # resize image\n        scale = min(w/iw, h/ih)\n        nw = int(iw*scale)\n        nh = int(ih*scale)\n        dx = (w-nw)//2\n        dy = (h-nh)//2\n        image_data=0\n        if proc_img:\n            image = image.resize((nw,nh), Image.BICUBIC)\n            new_image = Image.new(\'RGB\', (w,h), (128,128,128))\n            new_image.paste(image, (dx, dy))\n            image_data = np.array(new_image)/255.\n\n        # correct boxes\n        box_data = np.zeros((max_boxes,5))\n        if len(box)>0:\n            np.random.shuffle(box)\n            if len(box)>max_boxes: box = box[:max_boxes]\n            box[:, [0,2]] = box[:, [0,2]]*scale + dx\n            box[:, [1,3]] = box[:, [1,3]]*scale + dy\n            box_data[:len(box)] = box\n\n        return image_data, box_data\n\n    # resize image\n    new_ar = w/h * rand(1-jitter,1+jitter)/rand(1-jitter,1+jitter)\n    scale = rand(.25, 2)\n    if new_ar < 1:\n        nh = int(scale*h)\n        nw = int(nh*new_ar)\n    else:\n        nw = int(scale*w)\n        nh = int(nw/new_ar)\n    image = image.resize((nw,nh), Image.BICUBIC)\n\n    # place image\n    dx = int(rand(0, w-nw))\n    dy = int(rand(0, h-nh))\n    new_image = Image.new(\'RGB\', (w,h), (128,128,128))\n    new_image.paste(image, (dx, dy))\n    image = new_image\n\n    # flip image or not\n    flip = rand()<.5\n    if flip: image = image.transpose(Image.FLIP_LEFT_RIGHT)\n\n    # distort image\n    hue = rand(-hue, hue)\n    sat = rand(1, sat) if rand()<.5 else 1/rand(1, sat)\n    val = rand(1, val) if rand()<.5 else 1/rand(1, val)\n    x = rgb_to_hsv(np.array(image)/255.)\n    x[..., 0] += hue\n    x[..., 0][x[..., 0]>1] -= 1\n    x[..., 0][x[..., 0]<0] += 1\n    x[..., 1] *= sat\n    x[..., 2] *= val\n    x[x>1] = 1\n    x[x<0] = 0\n    image_data = hsv_to_rgb(x) # numpy array, 0 to 1\n\n    # correct boxes\n    box_data = np.zeros((max_boxes,5))\n    if len(box)>0:\n        np.random.shuffle(box)\n        box[:, [0,2]] = box[:, [0,2]]*nw/iw + dx\n        box[:, [1,3]] = box[:, [1,3]]*nh/ih + dy\n        if flip: box[:, [0,2]] = w - box[:, [2,0]]\n        box[:, 0:2][box[:, 0:2]<0] = 0\n        box[:, 2][box[:, 2]>w] = w\n        box[:, 3][box[:, 3]>h] = h\n        box_w = box[:, 2] - box[:, 0]\n        box_h = box[:, 3] - box[:, 1]\n        box = box[np.logical_and(box_w>1, box_h>1)] # discard invalid box\n        if len(box)>max_boxes: box = box[:max_boxes]\n        box_data[:len(box)] = box\n\n    return image_data, box_data\n'"
