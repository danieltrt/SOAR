file_path,api_count,code
actor_learner.py,17,"b'import numpy as np\nfrom multiprocessing import Process\nimport tensorflow as tf\nimport logging\nfrom logger_utils import variable_summaries\nimport os\n\nCHECKPOINT_INTERVAL = 1000000\n \n\nclass ActorLearner(Process):\n    \n    def __init__(self, network_creator, environment_creator, args):\n        \n        super(ActorLearner, self).__init__()\n\n        self.global_step = 0\n\n        self.max_local_steps = args.max_local_steps\n        self.num_actions = args.num_actions\n        self.initial_lr = args.initial_lr\n        self.lr_annealing_steps = args.lr_annealing_steps\n        self.emulator_counts = args.emulator_counts\n        self.device = args.device\n        self.debugging_folder = args.debugging_folder\n        self.network_checkpoint_folder = os.path.join(self.debugging_folder, \'checkpoints/\')\n        self.optimizer_checkpoint_folder = os.path.join(self.debugging_folder, \'optimizer_checkpoints/\')\n        self.last_saving_step = 0\n        self.summary_writer = tf.summary.FileWriter(os.path.join(self.debugging_folder, \'tf\'))\n\n        self.learning_rate = tf.placeholder(tf.float32, shape=[])\n        optimizer_variable_names = \'OptimizerVariables\'\n        self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate, decay=args.alpha, epsilon=args.e,\n                                                   name=optimizer_variable_names)\n\n        self.emulators = np.asarray([environment_creator.create_environment(i)\n                                     for i in range(self.emulator_counts)])\n        self.max_global_steps = args.max_global_steps\n        self.gamma = args.gamma\n        self.game = args.game\n        self.network = network_creator()\n\n        # Optimizer\n        grads_and_vars = self.optimizer.compute_gradients(self.network.loss)\n\n        self.flat_raw_gradients = tf.concat([tf.reshape(g, [-1]) for g, v in grads_and_vars], axis=0)\n\n        # This is not really an operation, but a list of gradient Tensors.\n        # When calling run() on it, the value of those Tensors\n        # (i.e., of the gradients) will be calculated\n        if args.clip_norm_type == \'ignore\':\n            # Unclipped gradients\n            global_norm = tf.global_norm([g for g, v in grads_and_vars], name=\'global_norm\')\n        elif args.clip_norm_type == \'global\':\n            # Clip network grads by network norm\n            gradients_n_norm = tf.clip_by_global_norm(\n                [g for g, v in grads_and_vars], args.clip_norm)\n            global_norm = tf.identity(gradients_n_norm[1], name=\'global_norm\')\n            grads_and_vars = list(zip(gradients_n_norm[0], [v for g, v in grads_and_vars]))\n        elif args.clip_norm_type == \'local\':\n            # Clip layer grads by layer norm\n            gradients = [tf.clip_by_norm(\n                g, args.clip_norm) for g in grads_and_vars]\n            grads_and_vars = list(zip(gradients, [v for g, v in grads_and_vars]))\n            global_norm = tf.global_norm([g for g, v in grads_and_vars], name=\'global_norm\')\n        else:\n            raise Exception(\'Norm type not recognized\')\n        self.flat_clipped_gradients = tf.concat([tf.reshape(g, [-1]) for g, v in grads_and_vars], axis=0)\n\n        self.train_step = self.optimizer.apply_gradients(grads_and_vars)\n\n        config = tf.ConfigProto()\n        if \'gpu\' in self.device:\n            logging.debug(\'Dynamic gpu mem allocation\')\n            config.gpu_options.allow_growth = True\n\n        self.session = tf.Session(config=config)\n\n        self.network_saver = tf.train.Saver()\n\n        self.optimizer_variables = [var for var in tf.global_variables() if optimizer_variable_names in var.name]\n        self.optimizer_saver = tf.train.Saver(self.optimizer_variables, max_to_keep=1, name=\'OptimizerSaver\')\n\n        # Summaries\n        variable_summaries(self.flat_raw_gradients, \'raw_gradients\')\n        variable_summaries(self.flat_clipped_gradients, \'clipped_gradients\')\n        tf.summary.scalar(\'global_norm\', global_norm)\n\n    def save_vars(self, force=False):\n        if force or self.global_step - self.last_saving_step >= CHECKPOINT_INTERVAL:\n            self.last_saving_step = self.global_step\n            self.network_saver.save(self.session, self.network_checkpoint_folder, global_step=self.last_saving_step)\n            self.optimizer_saver.save(self.session, self.optimizer_checkpoint_folder, global_step=self.last_saving_step)\n\n    def rescale_reward(self, reward):\n        """""" Clip immediate reward """"""\n        if reward > 1.0:\n            reward = 1.0\n        elif reward < -1.0:\n            reward = -1.0\n        return reward\n\n    def init_network(self):\n        import os\n        if not os.path.exists(self.network_checkpoint_folder):\n            os.makedirs(self.network_checkpoint_folder)\n        if not os.path.exists(self.optimizer_checkpoint_folder):\n            os.makedirs(self.optimizer_checkpoint_folder)\n\n        last_saving_step = self.network.init(self.network_checkpoint_folder, self.network_saver, self.session)\n\n        path = tf.train.latest_checkpoint(self.optimizer_checkpoint_folder)\n        if path is not None:\n            logging.info(\'Restoring optimizer variables from previous run\')\n            self.optimizer_saver.restore(self.session, path)\n\n        return last_saving_step\n\n    def get_lr(self):\n        if self.global_step <= self.lr_annealing_steps:\n            return self.initial_lr - (self.global_step * self.initial_lr / self.lr_annealing_steps)\n        else:\n            return 0.0\n\n    def cleanup(self):\n        self.save_vars(True)\n        self.session.close()\n\n'"
atari_emulator.py,0,"b'import numpy as np\nfrom ale_python_interface import ALEInterface\nfrom scipy.misc import imresize\nimport random\nfrom environment import BaseEnvironment, FramePool,ObservationPool\n\nIMG_SIZE_X = 84\nIMG_SIZE_Y = 84\nNR_IMAGES = 4\nACTION_REPEAT = 4\nMAX_START_WAIT = 30\nFRAMES_IN_POOL = 2\n\n\nclass AtariEmulator(BaseEnvironment):\n    def __init__(self, actor_id, args):\n        self.ale = ALEInterface()\n        self.ale.setInt(b""random_seed"", args.random_seed * (actor_id +1))\n        # For fuller control on explicit action repeat (>= ALE 0.5.0)\n        self.ale.setFloat(b""repeat_action_probability"", 0.0)\n        # Disable frame_skip and color_averaging\n        # See: http://is.gd/tYzVpj\n        self.ale.setInt(b""frame_skip"", 1)\n        self.ale.setBool(b""color_averaging"", False)\n        full_rom_path = args.rom_path + ""/"" + args.game + "".bin""\n        self.ale.loadROM(str.encode(full_rom_path))\n        self.legal_actions = self.ale.getMinimalActionSet()\n        self.screen_width, self.screen_height = self.ale.getScreenDims()\n        self.lives = self.ale.lives()\n\n        self.random_start = args.random_start\n        self.single_life_episodes = args.single_life_episodes\n        self.call_on_new_frame = args.visualize\n\n        # Processed historcal frames that will be fed in to the network \n        # (i.e., four 84x84 images)\n        self.observation_pool = ObservationPool(np.zeros((IMG_SIZE_X, IMG_SIZE_Y, NR_IMAGES), dtype=np.uint8))\n        self.rgb_screen = np.zeros((self.screen_height, self.screen_width, 3), dtype=np.uint8)\n        self.gray_screen = np.zeros((self.screen_height, self.screen_width,1), dtype=np.uint8)\n        self.frame_pool = FramePool(np.empty((2, self.screen_height,self.screen_width), dtype=np.uint8),\n                                    self.__process_frame_pool)\n\n    def get_legal_actions(self):\n        return self.legal_actions\n\n    def __get_screen_image(self):\n        """"""\n        Get the current frame luminance\n        :return: the current frame\n        """"""\n        self.ale.getScreenGrayscale(self.gray_screen)\n        if self.call_on_new_frame:\n            self.ale.getScreenRGB(self.rgb_screen)\n            self.on_new_frame(self.rgb_screen)\n        return np.squeeze(self.gray_screen)\n\n    def on_new_frame(self, frame):\n        pass\n\n    def __new_game(self):\n        """""" Restart game """"""\n        self.ale.reset_game()\n        self.lives = self.ale.lives()\n        if self.random_start:\n            wait = random.randint(0, MAX_START_WAIT)\n            for _ in range(wait):\n                self.ale.act(self.legal_actions[0])\n\n    def __process_frame_pool(self, frame_pool):\n        """""" Preprocess frame pool """"""\n        \n        img = np.amax(frame_pool, axis=0)\n        img = imresize(img, (84, 84), interp=\'nearest\')\n        img = img.astype(np.uint8)\n        return img\n\n    def __action_repeat(self, a, times=ACTION_REPEAT):\n        """""" Repeat action and grab screen into frame pool """"""\n        reward = 0\n        for i in range(times - FRAMES_IN_POOL):\n            reward += self.ale.act(self.legal_actions[a])\n        # Only need to add the last FRAMES_IN_POOL frames to the frame pool\n        for i in range(FRAMES_IN_POOL):\n            reward += self.ale.act(self.legal_actions[a])\n            self.frame_pool.new_frame(self.__get_screen_image())\n        return reward\n\n    def get_initial_state(self):\n        """""" Get the initial state """"""\n        self.__new_game()\n        for step in range(NR_IMAGES):\n            _ = self.__action_repeat(0)\n            self.observation_pool.new_observation(self.frame_pool.get_processed_frame())\n        if self.__is_terminal():\n            raise Exception(\'This should never happen.\')\n        return self.observation_pool.get_pooled_observations()\n\n    def next(self, action):\n        """""" Get the next state, reward, and game over signal """"""\n\n        reward = self.__action_repeat(np.argmax(action))\n        self.observation_pool.new_observation(self.frame_pool.get_processed_frame())\n        terminal = self.__is_terminal()\n        self.lives = self.ale.lives()\n        observation = self.observation_pool.get_pooled_observations()\n        return observation, reward, terminal\n            \n    def __is_terminal(self):\n        if self.single_life_episodes:\n            return self.__is_over() or (self.lives > self.ale.lives())\n        else:\n            return self.__is_over()\n\n    def __is_over(self):\n        return self.ale.game_over()\n\n    def get_noop(self):\n        return [1.0, 0.0]\n'"
emulator_runner.py,0,"b'from multiprocessing import Process\n\n\nclass EmulatorRunner(Process):\n\n    def __init__(self, id, emulators, variables, queue, barrier):\n        super(EmulatorRunner, self).__init__()\n        self.id = id\n        self.emulators = emulators\n        self.variables = variables\n        self.queue = queue\n        self.barrier = barrier\n\n    def run(self):\n        super(EmulatorRunner, self).run()\n        self._run()\n\n    def _run(self):\n        count = 0\n        while True:\n            instruction = self.queue.get()\n            if instruction is None:\n                break\n            for i, (emulator, action) in enumerate(zip(self.emulators, self.variables[-1])):\n                new_s, reward, episode_over = emulator.next(action)\n                if episode_over:\n                    self.variables[0][i] = emulator.get_initial_state()\n                else:\n                    self.variables[0][i] = new_s\n                self.variables[1][i] = reward\n                self.variables[2][i] = episode_over\n            count += 1\n            self.barrier.put(True)\n\n\n\n'"
environment.py,0,"b'import numpy as np\n\n\nclass BaseEnvironment(object):\n    def get_initial_state(self):\n        """"""\n        Sets the environment to its initial state.\n        :return: the initial state\n        """"""\n        raise NotImplementedError()\n\n    def next(self, action):\n        """"""\n        Appies the current action to the environment.\n        :param action: one hot vector.\n        :return: (observation, reward, is_terminal) tuple\n        """"""\n        raise NotImplementedError()\n\n    def get_legal_actions(self):\n        """"""\n        Get the set of indices of legal actions\n        :return: a numpy array of the indices of legal actions\n        """"""\n        raise NotImplementedError()\n\n    def get_noop(self):\n        """"""\n        Gets the no-op action, to be used with self.next\n        :return: the action\n        """"""\n        raise NotImplementedError()\n\n    def on_new_frame(self, frame):\n        """"""\n        Called whenever a new frame is available.\n        :param frame: raw frame\n        """"""\n        pass\n\n\nclass FramePool(object):\n\n    def __init__(self, frame_pool, operation):\n        self.frame_pool = frame_pool\n        self.frame_pool_index = 0\n        self.frames_in_pool = frame_pool.shape[0]\n        self.operation = operation\n\n    def new_frame(self, frame):\n        self.frame_pool[self.frame_pool_index] = frame\n        self.frame_pool_index = (self.frame_pool_index + 1) % self.frames_in_pool\n\n    def get_processed_frame(self):\n        return self.operation(self.frame_pool)\n\n\nclass ObservationPool(object):\n\n    def __init__(self, observation_pool):\n        self.observation_pool = observation_pool\n        self.pool_size = observation_pool.shape[-1]\n        self.permutation = [self.__shift(list(range(self.pool_size)), i) for i in range(self.pool_size)]\n        self.current_observation_index = 0\n\n    def new_observation(self, observation):\n        self.observation_pool[:, :, self.current_observation_index] = observation\n        self.current_observation_index = (self.current_observation_index + 1) % self.pool_size\n\n    def get_pooled_observations(self):\n        return np.copy(self.observation_pool[:, :, self.permutation[self.current_observation_index]])\n\n    def __shift(self, seq, n):\n        n = n % len(seq)\n        return seq[n:]+seq[:n]\n\n'"
environment_creator.py,0,"b'class EnvironmentCreator(object):\n\n    def __init__(self, args):\n        """"""\n        Creates an object from which new environments can be created\n        :param args:\n        """"""\n\n        from atari_emulator import AtariEmulator\n        from ale_python_interface import ALEInterface\n        filename = args.rom_path + ""/"" + args.game + "".bin""\n        ale_int = ALEInterface()\n        ale_int.loadROM(str.encode(filename))\n        self.num_actions = len(ale_int.getMinimalActionSet())\n        self.create_environment = lambda i: AtariEmulator(i, args)\n\n\n\n'"
logger_utils.py,9,"b'import os\nimport numpy as np\nimport time\nimport json\nimport tensorflow as tf\n\n\ndef load_args(path):\n    if path is None:\n        return {}\n    with open(path, \'r\') as f:\n        return json.load(f)\n\n\ndef save_args(args, folder, file_name=\'args.json\'):\n    args = vars(args)\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n    with open(os.path.join(folder, file_name), \'w\') as f:\n        return json.dump(args, f)\n\n\ndef variable_summaries(var, name):\n    """"""Attach a lot of summaries to a Tensor (for TensorBoard visualization).""""""\n    with tf.name_scope(\'summaries\'):\n        with tf.name_scope(name):\n            mean = tf.reduce_mean(var)\n            tf.summary.scalar(\'mean\', mean)\n            with tf.name_scope(\'stddev\'):\n                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n            tf.summary.scalar(\'stddev\', stddev)\n            tf.summary.scalar(\'max\', tf.reduce_max(var))\n            tf.summary.scalar(\'min\', tf.reduce_min(var))\n'"
networks.py,29,"b'import tensorflow as tf\nimport logging\nimport numpy as np\n\n\ndef flatten(_input):\n    shape = _input.get_shape().as_list()\n    dim = shape[1]*shape[2]*shape[3]\n    return tf.reshape(_input, [-1,dim], name=\'_flattened\')\n\n\ndef conv2d(name, _input, filters, size, channels, stride, padding = \'VALID\', init = ""torch""):\n    w = conv_weight_variable([size,size,channels,filters],\n                             name + \'_weights\', init = init)\n    b = conv_bias_variable([filters], size, size, channels,\n                           name + \'_biases\', init = init)\n    conv = tf.nn.conv2d(_input, w, strides=[1, stride, stride, 1],\n                        padding=padding, name=name + \'_convs\')\n    out = tf.nn.relu(tf.add(conv, b),\n                     name=\'\' + name + \'_activations\')\n    return w, b, out\n\n\ndef conv_weight_variable(shape, name, init = ""torch""):\n    if init == ""glorot_uniform"":\n        receptive_field_size = np.prod(shape[:2])\n        fan_in = shape[-2] * receptive_field_size\n        fan_out = shape[-1] * receptive_field_size\n        d = np.sqrt(6. / (fan_in + fan_out))\n    else:\n        w = shape[0]\n        h = shape[1]\n        input_channels = shape[2]\n        d = 1.0 / np.sqrt(input_channels * w * h)\n\n    initial = tf.random_uniform(shape, minval=-d, maxval=d)\n    return tf.Variable(initial, name=name, dtype=\'float32\')\n\n\ndef conv_bias_variable(shape, w, h, input_channels, name, init= ""torch""):\n    if init == ""glorot_uniform"":\n        initial = tf.zeros(shape)\n    else:\n        d = 1.0 / np.sqrt(input_channels * w * h)\n        initial = tf.random_uniform(shape, minval=-d, maxval=d)\n    return tf.Variable(initial, name=name, dtype=\'float32\')\n\n\ndef fc(name, _input, output_dim, activation = ""relu"", init = ""torch""):\n    input_dim = _input.get_shape().as_list()[1]\n    w = fc_weight_variable([input_dim, output_dim],\n                           name + \'_weights\', init = init)\n    b = fc_bias_variable([output_dim], input_dim,\n                         \'\' + name + \'_biases\', init = init)\n    out = tf.add(tf.matmul(_input, w), b, name= name + \'_out\')\n\n    if activation == ""relu"":\n        out = tf.nn.relu(out, name=\'\' + name + \'_relu\')\n\n    return w, b, out\n\n\ndef fc_weight_variable(shape, name, init=""torch""):\n    if init == ""glorot_uniform"":\n        fan_in = shape[0]\n        fan_out = shape[1]\n        d = np.sqrt(6. / (fan_in + fan_out))\n    else:\n        input_channels = shape[0]\n        d = 1.0 / np.sqrt(input_channels)\n    initial = tf.random_uniform(shape, minval=-d, maxval=d)\n    return tf.Variable(initial, name=name, dtype=\'float32\')\n\n\ndef fc_bias_variable(shape, input_channels, name, init= ""torch""):\n    if init==""glorot_uniform"":\n        initial = tf.zeros(shape, dtype=\'float32\')\n    else:\n        d = 1.0 / np.sqrt(input_channels)\n        initial = tf.random_uniform(shape, minval=-d, maxval=d)\n    return tf.Variable(initial, name=name, dtype=\'float32\')\n\n\ndef softmax(name, _input, output_dim):\n    input_dim = _input.get_shape().as_list()[1]\n    w = fc_weight_variable([input_dim, output_dim], name + \'_weights\')\n    b = fc_bias_variable([output_dim], input_dim, name + \'_biases\')\n    out = tf.nn.softmax(tf.add(tf.matmul(_input, w), b), name= name + \'_policy\')\n    return w, b, out\n\n\ndef log_softmax( name, _input, output_dim):\n    input_dim = _input.get_shape().as_list()[1]\n    w = fc_weight_variable([input_dim, output_dim], name + \'_weights\')\n    b = fc_bias_variable([output_dim], input_dim, name + \'_biases\')\n    out = tf.nn.log_softmax(tf.add(tf.matmul(_input, w), b), name= name + \'_policy\')\n    return w, b, out\n\n\nclass Network(object):\n\n    def __init__(self, conf):\n\n        self.name = conf[\'name\']\n        self.num_actions = conf[\'num_actions\']\n        self.clip_norm = conf[\'clip_norm\']\n        self.clip_norm_type = conf[\'clip_norm_type\']\n        self.device = conf[\'device\']\n\n        with tf.device(self.device):\n            with tf.name_scope(self.name):\n                self.loss_scaling = 5.0\n                self.input_ph = tf.placeholder(tf.uint8, [None, 84, 84, 4], name=\'input\')\n                self.selected_action_ph = tf.placeholder(""float32"", [None, self.num_actions], name=""selected_action"")\n                self.input = tf.scalar_mul(1.0/255.0, tf.cast(self.input_ph, tf.float32))\n\n                # This class should never be used, must be subclassed\n\n                # The output layer\n                self.output = None\n\n    def init(self, checkpoint_folder, saver, session):\n        last_saving_step = 0\n\n        with tf.device(\'/cpu:0\'):\n            # Initialize network parameters\n            path = tf.train.latest_checkpoint(checkpoint_folder)\n            if path is None:\n                logging.info(\'Initializing all variables\')\n                session.run(tf.global_variables_initializer())\n            else:\n                logging.info(\'Restoring network variables from previous run\')\n                saver.restore(session, path)\n                last_saving_step = int(path[path.rindex(\'-\')+1:])\n        return last_saving_step\n\n\nclass NIPSNetwork(Network):\n\n    def __init__(self, conf):\n        super(NIPSNetwork, self).__init__(conf)\n\n        with tf.device(self.device):\n            with tf.name_scope(self.name):\n                _, _, conv1 = conv2d(\'conv1\', self.input, 16, 8, 4, 4)\n\n                _, _, conv2 = conv2d(\'conv2\', conv1, 32, 4, 16, 2)\n\n                _, _, fc3 = fc(\'fc3\', flatten(conv2), 256, activation=""relu"")\n\n                self.output = fc3\n\n\nclass NatureNetwork(Network):\n\n    def __init__(self, conf):\n        super(NatureNetwork, self).__init__(conf)\n\n        with tf.device(self.device):\n            with tf.name_scope(self.name):\n                _, _, conv1 = conv2d(\'conv1\', self.input, 32, 8, 4, 4)\n\n                _, _, conv2 = conv2d(\'conv2\', conv1, 64, 4, 32, 2)\n\n                _, _, conv3 = conv2d(\'conv3\', conv2, 64, 3, 64, 1)\n\n                _, _, fc4 = fc(\'fc4\', flatten(conv3), 512, activation=""relu"")\n\n                self.output = fc4\n'"
paac.py,4,"b'import time\nfrom multiprocessing import Queue\nfrom multiprocessing.sharedctypes import RawArray\nfrom ctypes import c_uint, c_float\nfrom actor_learner import *\nimport logging\n\nfrom emulator_runner import EmulatorRunner\nfrom runners import Runners\nimport numpy as np\n\n\nclass PAACLearner(ActorLearner):\n    def __init__(self, network_creator, environment_creator, args):\n        super(PAACLearner, self).__init__(network_creator, environment_creator, args)\n        self.workers = args.emulator_workers\n\n    @staticmethod\n    def choose_next_actions(network, num_actions, states, session):\n        network_output_v, network_output_pi = session.run(\n            [network.output_layer_v,\n             network.output_layer_pi],\n            feed_dict={network.input_ph: states})\n\n        action_indices = PAACLearner.__sample_policy_action(network_output_pi)\n\n        new_actions = np.eye(num_actions)[action_indices]\n\n        return new_actions, network_output_v, network_output_pi\n\n    def __choose_next_actions(self, states):\n        return PAACLearner.choose_next_actions(self.network, self.num_actions, states, self.session)\n\n    @staticmethod\n    def __sample_policy_action(probs):\n        """"""\n        Sample an action from an action probability distribution output by\n        the policy network.\n        """"""\n        # Subtract a tiny value from probabilities in order to avoid\n        # ""ValueError: sum(pvals[:-1]) > 1.0"" in numpy.multinomial\n        probs = probs - np.finfo(np.float32).epsneg\n\n        action_indexes = [int(np.nonzero(np.random.multinomial(1, p))[0]) for p in probs]\n        return action_indexes\n\n    def _get_shared(self, array, dtype=c_float):\n        """"""\n        Returns a RawArray backed numpy array that can be shared between processes.\n        :param array: the array to be shared\n        :param dtype: the RawArray dtype to use\n        :return: the RawArray backed numpy array\n        """"""\n\n        shape = array.shape\n        shared = RawArray(dtype, array.reshape(-1))\n        return np.frombuffer(shared, dtype).reshape(shape)\n\n    def train(self):\n        """"""\n        Main actor learner loop for parallel advantage actor critic learning.\n        """"""\n\n        self.global_step = self.init_network()\n\n        logging.debug(""Starting training at Step {}"".format(self.global_step))\n        counter = 0\n\n        global_step_start = self.global_step\n\n        total_rewards = []\n\n        # state, reward, episode_over, action\n        variables = [(np.asarray([emulator.get_initial_state() for emulator in self.emulators], dtype=np.uint8)),\n                     (np.zeros(self.emulator_counts, dtype=np.float32)),\n                     (np.asarray([False] * self.emulator_counts, dtype=np.float32)),\n                     (np.zeros((self.emulator_counts, self.num_actions), dtype=np.float32))]\n\n        self.runners = Runners(EmulatorRunner, self.emulators, self.workers, variables)\n        self.runners.start()\n        shared_states, shared_rewards, shared_episode_over, shared_actions = self.runners.get_shared_variables()\n\n        summaries_op = tf.summary.merge_all()\n\n        emulator_steps = [0] * self.emulator_counts\n        total_episode_rewards = self.emulator_counts * [0]\n\n        actions_sum = np.zeros((self.emulator_counts, self.num_actions))\n        y_batch = np.zeros((self.max_local_steps, self.emulator_counts))\n        adv_batch = np.zeros((self.max_local_steps, self.emulator_counts))\n        rewards = np.zeros((self.max_local_steps, self.emulator_counts))\n        states = np.zeros([self.max_local_steps] + list(shared_states.shape), dtype=np.uint8)\n        actions = np.zeros((self.max_local_steps, self.emulator_counts, self.num_actions))\n        values = np.zeros((self.max_local_steps, self.emulator_counts))\n        episodes_over_masks = np.zeros((self.max_local_steps, self.emulator_counts))\n\n        start_time = time.time()\n\n        while self.global_step < self.max_global_steps:\n\n            loop_start_time = time.time()\n\n            max_local_steps = self.max_local_steps\n            for t in range(max_local_steps):\n                next_actions, readouts_v_t, readouts_pi_t = self.__choose_next_actions(shared_states)\n                actions_sum += next_actions\n                for z in range(next_actions.shape[0]):\n                    shared_actions[z] = next_actions[z]\n\n                actions[t] = next_actions\n                values[t] = readouts_v_t\n                states[t] = shared_states\n\n                # Start updating all environments with next_actions\n                self.runners.update_environments()\n                self.runners.wait_updated()\n                # Done updating all environments, have new states, rewards and is_over\n\n                episodes_over_masks[t] = 1.0 - shared_episode_over.astype(np.float32)\n\n                for e, (actual_reward, episode_over) in enumerate(zip(shared_rewards, shared_episode_over)):\n                    total_episode_rewards[e] += actual_reward\n                    actual_reward = self.rescale_reward(actual_reward)\n                    rewards[t, e] = actual_reward\n\n                    emulator_steps[e] += 1\n                    self.global_step += 1\n                    if episode_over:\n                        total_rewards.append(total_episode_rewards[e])\n                        episode_summary = tf.Summary(value=[\n                            tf.Summary.Value(tag=\'rl/reward\', simple_value=total_episode_rewards[e]),\n                            tf.Summary.Value(tag=\'rl/episode_length\', simple_value=emulator_steps[e]),\n                        ])\n                        self.summary_writer.add_summary(episode_summary, self.global_step)\n                        self.summary_writer.flush()\n                        total_episode_rewards[e] = 0\n                        emulator_steps[e] = 0\n                        actions_sum[e] = np.zeros(self.num_actions)\n\n            nest_state_value = self.session.run(\n                self.network.output_layer_v,\n                feed_dict={self.network.input_ph: shared_states})\n\n            estimated_return = np.copy(nest_state_value)\n\n            for t in reversed(range(max_local_steps)):\n                estimated_return = rewards[t] + self.gamma * estimated_return * episodes_over_masks[t]\n                y_batch[t] = np.copy(estimated_return)\n                adv_batch[t] = estimated_return - values[t]\n\n            flat_states = states.reshape([self.max_local_steps * self.emulator_counts] + list(shared_states.shape)[1:])\n            flat_y_batch = y_batch.reshape(-1)\n            flat_adv_batch = adv_batch.reshape(-1)\n            flat_actions = actions.reshape(max_local_steps * self.emulator_counts, self.num_actions)\n\n            lr = self.get_lr()\n            feed_dict = {self.network.input_ph: flat_states,\n                         self.network.critic_target_ph: flat_y_batch,\n                         self.network.selected_action_ph: flat_actions,\n                         self.network.adv_actor_ph: flat_adv_batch,\n                         self.learning_rate: lr}\n\n            _, summaries = self.session.run(\n                [self.train_step, summaries_op],\n                feed_dict=feed_dict)\n\n            self.summary_writer.add_summary(summaries, self.global_step)\n            self.summary_writer.flush()\n\n            counter += 1\n\n            if counter % (2048 / self.emulator_counts) == 0:\n                curr_time = time.time()\n                global_steps = self.global_step\n                last_ten = 0.0 if len(total_rewards) < 1 else np.mean(total_rewards[-10:])\n                logging.info(""Ran {} steps, at {} steps/s ({} steps/s avg), last 10 rewards avg {}""\n                             .format(global_steps,\n                                     self.max_local_steps * self.emulator_counts / (curr_time - loop_start_time),\n                                     (global_steps - global_step_start) / (curr_time - start_time),\n                                     last_ten))\n            self.save_vars()\n\n        self.cleanup()\n\n    def cleanup(self):\n        super(PAACLearner, self).cleanup()\n        self.runners.stop()\n'"
policy_v_network.py,18,"b'from networks import *\n\n\nclass PolicyVNetwork(Network):\n\n    def __init__(self, conf):\n        """""" Set up remaining layers, objective and loss functions, gradient\n        compute and apply ops, network parameter synchronization ops, and\n        summary ops. """"""\n\n        super(PolicyVNetwork, self).__init__(conf)\n\n        self.entropy_regularisation_strength = conf[\'entropy_regularisation_strength\']\n\n        with tf.device(conf[\'device\']):\n            with tf.name_scope(self.name):\n\n                self.critic_target_ph = tf.placeholder(\n                    ""float32"", [None], name=\'target\')\n                self.adv_actor_ph = tf.placeholder(""float"", [None], name=\'advantage\')\n\n                # Final actor layer\n                layer_name = \'actor_output\'\n                _, _, self.output_layer_pi = softmax(layer_name, self.output, self.num_actions)\n                # Final critic layer\n                _, _, self.output_layer_v = fc(\'critic_output\', self.output, 1, activation=""linear"")\n\n                # Avoiding log(0) by adding a very small quantity (1e-30) to output.\n                self.log_output_layer_pi = tf.log(tf.add(self.output_layer_pi, tf.constant(1e-30)),\n                                                  name=layer_name + \'_log_policy\')\n\n                # Entropy: sum_a (-p_a ln p_a)\n                self.output_layer_entropy = tf.reduce_sum(tf.multiply(\n                    tf.constant(-1.0),\n                    tf.multiply(self.output_layer_pi, self.log_output_layer_pi)), reduction_indices=1)\n\n                self.output_layer_v = tf.reshape(self.output_layer_v, [-1])\n\n                # Advantage critic\n                self.critic_loss = tf.subtract(self.critic_target_ph, self.output_layer_v)\n\n                log_output_selected_action = tf.reduce_sum(\n                    tf.multiply(self.log_output_layer_pi, self.selected_action_ph),\n                    reduction_indices=1)\n\n                self.actor_objective_advantage_term = tf.multiply(log_output_selected_action, self.adv_actor_ph)\n                self.actor_objective_entropy_term = tf.multiply(self.entropy_regularisation_strength, self.output_layer_entropy)\n\n                self.actor_objective_mean = tf.reduce_mean(tf.multiply(tf.constant(-1.0),\n                                                                       tf.add(self.actor_objective_advantage_term, self.actor_objective_entropy_term)),\n                                                           name=\'mean_actor_objective\')\n\n                self.critic_loss_mean = tf.reduce_mean(tf.scalar_mul(0.25, tf.pow(self.critic_loss, 2)), name=\'mean_critic_loss\')\n\n                # Loss scaling is used because the learning rate was initially runed tuned to be used with\n                # max_local_steps = 5 and summing over timesteps, which is now replaced with the mean.\n                self.loss = tf.scalar_mul(self.loss_scaling, self.actor_objective_mean + self.critic_loss_mean)\n\nclass NIPSPolicyVNetwork(PolicyVNetwork, NIPSNetwork):\n    pass\n\n\nclass NaturePolicyVNetwork(PolicyVNetwork, NatureNetwork):\n    pass\n'"
runners.py,0,"b'import numpy as np\nfrom multiprocessing import Queue\nfrom multiprocessing.sharedctypes import RawArray\nfrom ctypes import c_uint, c_float, c_double\n\n\nclass Runners(object):\n\n    NUMPY_TO_C_DTYPE = {np.float32: c_float, np.float64: c_double, np.uint8: c_uint}\n\n    def __init__(self, EmulatorRunner, emulators, workers, variables):\n        self.variables = [self._get_shared(var) for var in variables]\n        self.workers = workers\n        self.queues = [Queue() for _ in range(workers)]\n        self.barrier = Queue()\n\n        self.runners = [EmulatorRunner(i, emulators, vars, self.queues[i], self.barrier) for i, (emulators, vars) in\n                        enumerate(zip(np.split(emulators, workers), zip(*[np.split(var, workers) for var in self.variables])))]\n\n    def _get_shared(self, array):\n        """"""\n        Returns a RawArray backed numpy array that can be shared between processes.\n        :param array: the array to be shared\n        :return: the RawArray backed numpy array\n        """"""\n\n        dtype = self.NUMPY_TO_C_DTYPE[array.dtype.type]\n\n        shape = array.shape\n        shared = RawArray(dtype, array.reshape(-1))\n        return np.frombuffer(shared, dtype).reshape(shape)\n\n    def start(self):\n        for r in self.runners:\n            r.start()\n\n    def stop(self):\n        for queue in self.queues:\n            queue.put(None)\n\n    def get_shared_variables(self):\n        return self.variables\n\n    def update_environments(self):\n        for queue in self.queues:\n            queue.put(True)\n\n    def wait_updated(self):\n        for wd in range(self.workers):\n            self.barrier.get()\n'"
test.py,3,"b'import os\nfrom train import get_network_and_environment_creator, bool_arg\nimport logger_utils\nimport argparse\nimport numpy as np\nimport time\nimport tensorflow as tf\nimport random\nfrom paac import PAACLearner\n\n\ndef get_save_frame(name):\n    import imageio\n\n    writer = imageio.get_writer(name + \'.gif\', fps=30)\n\n    def get_frame(frame):\n        writer.append_data(frame)\n\n    return get_frame\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-f\', \'--folder\', type=str, help=""Folder where to save the debugging information."", dest=""folder"", required=True)\n    parser.add_argument(\'-tc\', \'--test_count\', default=\'1\', type=int, help=""The amount of tests to run on the given network"", dest=""test_count"")\n    parser.add_argument(\'-np\', \'--noops\', default=30, type=int, help=""Maximum amount of no-ops to use"", dest=""noops"")\n    parser.add_argument(\'-gn\', \'--gif_name\', default=None, type=str, help=""If provided, a gif will be produced and stored with this name"", dest=""gif_name"")\n    parser.add_argument(\'-gf\', \'--gif_folder\', default=\'\', type=str, help=""The folder where to save gifs."", dest=""gif_folder"")\n    parser.add_argument(\'-d\', \'--device\', default=\'/gpu:0\', type=str, help=""Device to be used (\'/cpu:0\', \'/gpu:0\', \'/gpu:1\',...)"", dest=""device"")\n\n    args = parser.parse_args()\n    arg_file = os.path.join(args.folder, \'args.json\')\n    device = args.device\n    for k, v in logger_utils.load_args(arg_file).items():\n        setattr(args, k, v)\n    args.max_global_steps = 0\n    df = args.folder\n    args.debugging_folder = \'/tmp/logs\'\n    args.device = device\n\n    args.random_start = False\n    args.single_life_episodes = False\n    if args.gif_name:\n        args.visualize = 1\n\n    args.actor_id = 0\n    rng = np.random.RandomState(int(time.time()))\n    args.random_seed = rng.randint(1000)\n\n    network_creator, env_creator = get_network_and_environment_creator(args)\n    network = network_creator()\n    saver = tf.train.Saver()\n\n    rewards = []\n    environments = [env_creator.create_environment(i) for i in range(args.test_count)]\n    if args.gif_name:\n        for i, environment in enumerate(environments):\n            environment.on_new_frame = get_save_frame(os.path.join(args.gif_folder, args.gif_name + str(i)))\n\n    config = tf.ConfigProto()\n    if \'gpu\' in args.device:\n        config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        checkpoints_ = os.path.join(df, \'checkpoints\')\n        network.init(checkpoints_, saver, sess)\n        states = np.asarray([environment.get_initial_state() for environment in environments])\n        if args.noops != 0:\n            for i, environment in enumerate(environments):\n                for _ in range(random.randint(0, args.noops)):\n                    state, _, _ = environment.next(environment.get_noop())\n                    states[i] = state\n\n        episodes_over = np.zeros(args.test_count, dtype=np.bool)\n        rewards = np.zeros(args.test_count, dtype=np.float32)\n        while not all(episodes_over):\n            actions, _, _ = PAACLearner.choose_next_actions(network, env_creator.num_actions, states, sess)\n            for j, environment in enumerate(environments):\n                state, r, episode_over = environment.next(actions[j])\n                states[j] = state\n                rewards[j] += r\n                episodes_over[j] = episode_over\n\n        print(\'Performed {} tests for {}.\'.format(args.test_count, args.game))\n        print(\'Mean: {0:.2f}\'.format(np.mean(rewards)))\n        print(\'Min: {0:.2f}\'.format(np.min(rewards)))\n        print(\'Max: {0:.2f}\'.format(np.max(rewards)))\n        print(\'Std: {0:.2f}\'.format(np.std(rewards)))\n\n\n'"
train.py,0,"b'import argparse\nimport logging\nimport sys\nimport signal\nimport os\nimport copy\n\nimport environment_creator\nfrom paac import PAACLearner\nfrom policy_v_network import NaturePolicyVNetwork, NIPSPolicyVNetwork\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n\n\ndef bool_arg(string):\n    value = string.lower()\n    if value == \'true\':\n        return True\n    elif value == \'false\':\n        return False\n    else:\n        raise argparse.ArgumentTypeError(""Expected True or False, but got {}"".format(string))\n\n\ndef main(args):\n    logging.debug(\'Configuration: {}\'.format(args))\n\n    network_creator, env_creator = get_network_and_environment_creator(args)\n\n    learner = PAACLearner(network_creator, env_creator, args)\n\n    setup_kill_signal_handler(learner)\n\n    logging.info(\'Starting training\')\n    learner.train()\n    logging.info(\'Finished training\')\n\n\ndef setup_kill_signal_handler(learner):\n    main_process_pid = os.getpid()\n\n    def signal_handler(signal, frame):\n        if os.getpid() == main_process_pid:\n            logging.info(\'Signal \' + str(signal) + \' detected, cleaning up.\')\n            learner.cleanup()\n            logging.info(\'Cleanup completed, shutting down...\')\n            sys.exit(0)\n\n    signal.signal(signal.SIGTERM, signal_handler)\n    signal.signal(signal.SIGINT, signal_handler)\n\n\ndef get_network_and_environment_creator(args, random_seed=3):\n    env_creator = environment_creator.EnvironmentCreator(args)\n    num_actions = env_creator.num_actions\n    args.num_actions = num_actions\n    args.random_seed = random_seed\n\n    network_conf = {\'num_actions\': num_actions,\n                    \'entropy_regularisation_strength\': args.entropy_regularisation_strength,\n                    \'device\': args.device,\n                    \'clip_norm\': args.clip_norm,\n                    \'clip_norm_type\': args.clip_norm_type}\n    if args.arch == \'NIPS\':\n        network = NIPSPolicyVNetwork\n    else:\n        network = NaturePolicyVNetwork\n\n    def network_creator(name=\'local_learning\'):\n        nonlocal network_conf\n        copied_network_conf = copy.copy(network_conf)\n        copied_network_conf[\'name\'] = name\n        return network(copied_network_conf)\n\n    return network_creator, env_creator\n\n\ndef get_arg_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-g\', default=\'pong\', help=\'Name of game\', dest=\'game\')\n    parser.add_argument(\'-d\', \'--device\', default=\'/gpu:0\', type=str, help=""Device to be used (\'/cpu:0\', \'/gpu:0\', \'/gpu:1\',...)"", dest=""device"")\n    parser.add_argument(\'--rom_path\', default=\'./atari_roms\', help=\'Directory where the game roms are located (needed for ALE environment)\', dest=""rom_path"")\n    parser.add_argument(\'-v\', \'--visualize\', default=False, type=bool_arg, help=""0: no visualization of emulator; 1: all emulators, for all actors, are visualized; 2: only 1 emulator (for one of the actors) is visualized"", dest=""visualize"")\n    parser.add_argument(\'--e\', default=0.1, type=float, help=""Epsilon for the Rmsprop and Adam optimizers"", dest=""e"")\n    parser.add_argument(\'--alpha\', default=0.99, type=float, help=""Discount factor for the history/coming gradient, for the Rmsprop optimizer"", dest=""alpha"")\n    parser.add_argument(\'-lr\', \'--initial_lr\', default=0.0224, type=float, help=""Initial value for the learning rate. Default = 0.0224"", dest=""initial_lr"")\n    parser.add_argument(\'-lra\', \'--lr_annealing_steps\', default=80000000, type=int, help=""Nr. of global steps during which the learning rate will be linearly annealed towards zero"", dest=""lr_annealing_steps"")\n    parser.add_argument(\'--entropy\', default=0.02, type=float, help=""Strength of the entropy regularization term (needed for actor-critic)"", dest=""entropy_regularisation_strength"")\n    parser.add_argument(\'--clip_norm\', default=3.0, type=float, help=""If clip_norm_type is local/global, grads will be clipped at the specified maximum (avaerage) L2-norm"", dest=""clip_norm"")\n    parser.add_argument(\'--clip_norm_type\', default=""global"", help=""Whether to clip grads by their norm or not. Values: ignore (no clipping), local (layer-wise norm), global (global norm)"", dest=""clip_norm_type"")\n    parser.add_argument(\'--gamma\', default=0.99, type=float, help=""Discount factor"", dest=""gamma"")\n    parser.add_argument(\'--max_global_steps\', default=80000000, type=int, help=""Max. number of training steps"", dest=""max_global_steps"")\n    parser.add_argument(\'--max_local_steps\', default=5, type=int, help=""Number of steps to gain experience from before every update."", dest=""max_local_steps"")\n    parser.add_argument(\'--arch\', default=\'NIPS\', help=""Which network architecture to use: from the NIPS or NATURE paper"", dest=""arch"")\n    parser.add_argument(\'--single_life_episodes\', default=False, type=bool_arg, help=""If True, training episodes will be terminated when a life is lost (for games)"", dest=""single_life_episodes"")\n    parser.add_argument(\'-ec\', \'--emulator_counts\', default=32, type=int, help=""The amount of emulators per agent. Default is 32."", dest=""emulator_counts"")\n    parser.add_argument(\'-ew\', \'--emulator_workers\', default=8, type=int, help=""The amount of emulator workers per agent. Default is 8."", dest=""emulator_workers"")\n    parser.add_argument(\'-df\', \'--debugging_folder\', default=\'logs/\', type=str, help=""Folder where to save the debugging information."", dest=""debugging_folder"")\n    parser.add_argument(\'-rs\', \'--random_start\', default=True, type=bool_arg, help=""Whether or not to start with 30 noops for each env. Default True"", dest=""random_start"")\n    return parser\n\n\nif __name__ == \'__main__\':\n    args = get_arg_parser().parse_args()\n\n    import logger_utils\n    logger_utils.save_args(args, args.debugging_folder)\n    logging.debug(args)\n\n    main(args)\n'"
