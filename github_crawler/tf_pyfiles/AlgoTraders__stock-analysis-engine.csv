file_path,api_count,code
setup.py,0,"b'import os\nimport sys\nimport warnings\nimport unittest\n\ntry:\n    from setuptools import setup\nexcept ImportError:\n    from distutils.core import setup\n\ntry:\n    from distutils.command.build_py import build_py_2to3 as build_py\nexcept ImportError:\n    from distutils.command.build_py import build_py\n\n""""""\nhttps://packaging.python.org/guides/making-a-pypi-friendly-readme/\ncheck the README.rst works on pypi as the\nlong_description with:\ntwine check dist/*\n""""""\nlong_description = open(\'README.rst\').read()\n\ncur_path, cur_script = os.path.split(sys.argv[0])\nos.chdir(os.path.abspath(cur_path))\n\nrequires_that_fail_on_rtd = [\n    \'awscli\',\n    \'h5py\',\n    \'keras\',\n    \'scikit-learn\',\n    \'tables\',\n    \'ta-lib\',\n    \'tensorflow\'\n]\n\ninstall_requires = []\n\ncur_dir = os.path.abspath(os.path.dirname(__file__))\nwith open(os.path.join(\n        cur_dir, \'requirements.txt\'), encoding=\'utf-8\') as f:\n    install_requires = f.read().split()\n\n# if not on readthedocs.io or travis ci get all the pips:\nif (os.getenv(\'READTHEDOCS\', \'\') == \'\'\n        and os.getenv(\'TRAVIS\', \'\') == \'\'):\n    install_requires = install_requires + requires_that_fail_on_rtd\n\nif sys.version_info < (3, 6):\n    warnings.warn(\n        \'Less than Python 3.6 is not supported.\',\n        DeprecationWarning)\n\n\ndef analysis_engine_test_suite():\n    test_loader = unittest.TestLoader()\n    test_suite = test_loader.discover(\'tests\', pattern=\'test_*.py\')\n    return test_suite\n\n\n# Don\'t import analysis_engine module here, since deps may not be installed\nsys.path.insert(\n    0,\n    os.path.join(\n        os.path.dirname(__file__),\n        \'analysis_engine\'))\n\nsetup(\n    name=\'stock-analysis-engine\',\n    cmdclass={\'build_py\': build_py},\n    version=\'1.9.14\',\n    description=(\n        \'Backtest 1000s of minute-by-minute \'\n        \'trading algorithms. Automated \'\n        \'pricing data ingestion from: \'\n        \'IEX Cloud (https://iexcloud.io), \'\n        \'Tradier (https://tradier.com) and \'\n        \'FinViz. Datasets and trading \'\n        \'performance automatically compressed and published \'\n        \'to S3 for building AI training datasets \'\n        \'for teaching DNNs how to trade. \'\n        \'Runs on Kubernetes with Helm and docker-compose. \'\n        \'>150 million trading history rows generated from +5000 algorithms\'\n        \'\'),\n    long_description=long_description,\n    long_description_content_type=\'text/x-rst\',\n    author=\'Jay Johnson\',\n    author_email=\'jay.p.h.johnson@gmail.com\',\n    url=\'https://github.com/AlgoTraders/stock-analysis-engine\',\n    packages=[\n        \'analysis_engine\',\n        \'analysis_engine.ai\',\n        \'analysis_engine.iex\',\n        \'analysis_engine.finviz\',\n        \'analysis_engine.mocks\',\n        \'analysis_engine.indicators\',\n        \'analysis_engine.log\',\n        \'analysis_engine.perf\',\n        \'analysis_engine.scripts\',\n        \'analysis_engine.td\',\n        \'analysis_engine.work_tasks\',\n        \'analysis_engine.yahoo\'\n    ],\n    package_data={},\n    install_requires=install_requires,\n    test_suite=\'setup.analysis_engine_test_suite\',\n    tests_require=[\n    ],\n    scripts=[\n        \'analysis_engine/scripts/aws_backup.py\',\n        \'analysis_engine/scripts/backtest_with_runner.py\',\n        \'analysis_engine/scripts/fetch_new_stock_datasets.py\',\n        \'analysis_engine/scripts/inspect_datasets.py\',\n        \'analysis_engine/scripts/plot_history_from_local_file.py\',\n        \'analysis_engine/scripts/publish_from_s3_to_redis.py\',\n        \'analysis_engine/scripts/publish_ticker_aggregate_from_s3.py\',\n        \'analysis_engine/scripts/run_backtest_and_plot_history.py\',\n        \'analysis_engine/scripts/sa.py\',\n        \'analysis_engine/scripts/start_algo.py\',\n        \'analysis_engine/scripts/train_dnn_from_history.py\',\n        \'tools/backfill-minute-data.sh\',\n        \'tools/logs-dataset-collection.sh\',\n        \'tools/logs-jupyter.sh\',\n        \'tools/logs-workers.sh\',\n        \'tools/run-algo-history-to-file.sh\',\n        \'tools/run-algo-history-to-s3.sh\',\n        \'tools/run-algo-report-to-file.sh\',\n        \'tools/run-algo-report-to-s3.sh\',\n        \'tools/ssh-jupyter.sh\',\n        \'tools/ssh-workers.sh\',\n        \'tools/update-stack.sh\'\n    ],\n    entry_points={\n        \'console_scripts\': [\n            \'sa = sa:run_sa_tool\',\n            (\n                \'fetch = fetch_new_stock_datasets\'\n                \':fetch_new_stock_datasets\'),\n            (\n                \'plot-history = \'\n                \'plot_history_from_local_file\'\n                \':plot_local_history_file\'),\n            (\n                \'bt = run_backtest_and_plot_history\'\n                \':start_backtest_with_plot_history\'),\n            (\n                \'ae = start_algo\'\n                \':start_algo\'),\n        ],\n    },\n    classifiers=[\n        \'Development Status :: 5 - Production/Stable\',\n        \'Intended Audience :: Developers\',\n        \'License :: OSI Approved :: Apache Software License\',\n        \'Operating System :: OS Independent\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: Implementation :: PyPy\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\',\n    ])\n'"
analysis_engine/__init__.py,0,b''
analysis_engine/ae_talib.py,0,"b'""""""\nTA-Lib wrappers\n""""""\n\n# for unittests, allow passing the mocks into the runtime if not found\ntry:\n    import talib as ta\nexcept Exception:\n    import analysis_engine.mocks.mock_talib as ta\n# end of loading talib or mocks\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\n""""""\nOverlap\n\nhttps://mrjbq7.github.io/ta-lib/func_groups/overlap_studies.html\n""""""\n\n\ndef BBANDS(\n        close,\n        timeperiod=5,\n        nbdevup=2,\n        nbdevdn=2,\n        matype=0,\n        verbose=False):\n    """"""BBANDS\n\n    Wrapper for ta.BBANDS for running unittests\n    on ci/cd tools that do not provide talib\n\n    .. code-block:: python\n\n        (upperband,\n         middleband,\n         lowerband) = BBANDS(\n            close,\n            timeperiod=5,\n            nbdevup=2,\n            nbdevdn=2,\n            matype=0)\n\n    :return: upperband, middleband, lowerband\n    :param close: close prices\n    :param timeperiod: number of values\n        (default is ``5``)\n    :param nbdevup: float - standard deviation\n        to set the upper band\n        (default is ``2``)\n    :param nbdevdn: float - standard deviation\n        to set the lower band\n        (default is ``2``)\n    :param matype: moving average type\n        (default is ``0`` simple moving average)\n    :param verbose: show logs\n    """"""\n    if verbose:\n        log.info(\n            \'bbands - start\')\n    return ta.BBANDS(\n        close,\n        timeperiod=timeperiod,\n        nbdevup=nbdevup,\n        nbdevdn=nbdevdn,\n        matype=matype)\n# end of BBANDS\n\n\ndef EMA(\n        close,\n        timeperiod=30,\n        verbose=False):\n    """"""EMA\n\n    Wrapper for ta.EMA for running unittests\n    on ci/cd tools that do not provide talib\n\n    .. code-block:: python\n\n        real = EMA(\n            close,\n            timeperiod=30)\n\n    :return: float\n    :param close: close prices\n    :param timeperiod: number of values\n        (default is ``5``)\n    :param verbose: show logs\n    """"""\n    if verbose:\n        log.info(\n            \'ema - start\')\n    return ta.EMA(\n        close,\n        timeperiod=timeperiod)\n# end of EMA\n\n\ndef WMA(\n        close,\n        timeperiod=30,\n        verbose=False):\n    """"""WMA\n\n    Wrapper for ta.WMA for running unittests\n    on ci/cd tools that do not provide talib\n\n    .. code-block:: python\n\n        real = WMA(\n            close,\n            timeperiod=30)\n\n    :return: float\n    :param close: close prices\n    :param timeperiod: number of values\n        (default is ``5``)\n    :param verbose: show logs\n    """"""\n    if verbose:\n        log.info(\n            \'ema - start\')\n    return ta.WMA(\n        close,\n        timeperiod=timeperiod)\n# end of WMA\n\n\n""""""\nMomentum\n\nhttps://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html\n""""""\n\n\ndef ADX(\n        high=None,\n        low=None,\n        close=None,\n        timeperiod=14,\n        verbose=False):\n    """"""ADX\n\n    Wrapper for ta.ADX for running unittests\n    on ci/cd tools that do not provide talib\n\n    .. code-block:: python\n\n        real = ADX(\n            high,\n            low,\n            close,\n            timeperiod=14)\n\n    :param high: high list\n    :param low: low list\n    :param close: close list\n    :param timeperiod: number of values\n        in ``high``, ``low`` and ``close``\n    :param verbose: show logs\n    """"""\n    if verbose:\n        log.info(\n            \'adx - start\')\n    return ta.ADX(\n        high,\n        low,\n        close,\n        timeperiod)\n# end of ADX\n\n\ndef MACD(\n        close=None,\n        fast_period=12,\n        slow_period=26,\n        signal_period=9,\n        verbose=False):\n    """"""MACD\n\n    Wrapper for ta.MACD for running unittests\n    on ci/cd tools that do not provide talib\n\n    .. code-block:: python\n\n        (macd,\n         macdsignal,\n         macdhist) = MACD(\n            close,\n            fastperiod=12,\n            slowperiod=26,\n            signalperiod=9)\n\n    :param value: list of values\n        (default ``closes``)\n    :param fast_period: integer fast\n        line\n    :param slow_period: integer slow\n        line\n    :param signal_period: integer signal\n        line\n    :param verbose: show logs\n    """"""\n    if verbose:\n        log.info(\n            \'macd - start\')\n    return ta.MACD(\n        close,\n        fastperiod=fast_period,\n        slowperiod=slow_period,\n        signalperiod=signal_period)\n# end of MACD\n\n\ndef MFI(\n        high=None,\n        low=None,\n        close=None,\n        volume=None,\n        timeperiod=None,\n        verbose=False):\n    """"""MFI\n\n    Wrapper for ta.MFI for running unittests\n    on ci/cd tools that do not provide talib\n\n    .. code-block:: python\n\n        real = MFI(\n            high,\n            low,\n            close,\n            volume,\n            timeperiod=14)\n\n    :param high: high list\n    :param low: low list\n    :param close: close list\n    :param timeperiod: number of values\n        in ``high``, ``low`` and ``close``\n    :param verbose: show logs\n    """"""\n    if verbose:\n        log.info(\n            \'mfi - start\')\n    return ta.MFI(\n        high,\n        low,\n        close,\n        volume,\n        timeperiod)\n# end of MFI\n\n\ndef MOM(\n        close=None,\n        timeperiod=None,\n        verbose=False):\n    """"""MOM\n\n    Wrapper for ta.MOM for running unittests\n    on ci/cd tools that do not provide talib\n\n    .. code-block:: python\n\n        real = MOM(\n            close,\n            timeperiod=10)\n\n    :param high: high list\n    :param low: low list\n    :param close: close list\n    :param timeperiod: number of values\n        in ``high``, ``low`` and ``close``\n    :param verbose: show logs\n    """"""\n    if verbose:\n        log.info(\n            \'mom - start\')\n    return ta.MOM(\n        close,\n        timeperiod)\n# end of MOM\n\n\ndef ROC(\n        close=None,\n        timeperiod=None,\n        verbose=False):\n    """"""ROC\n\n    Wrapper for ta.ROC for running unittests\n    on ci/cd tools that do not provide talib\n\n    .. code-block:: python\n\n        real = ROC(\n            close,\n            timeperiod=10)\n\n    :param close: close list\n    :param timeperiod: number of values\n        in ``high``, ``low`` and ``close``\n    :param verbose: show logs\n    """"""\n    if verbose:\n        log.info(\n            \'roc - start\')\n    return ta.ROC(\n        close,\n        timeperiod)\n# end of ROC\n\n\ndef RSI(\n        close=None,\n        timeperiod=None,\n        verbose=False):\n    """"""RSI\n\n    Wrapper for ta.RSI for running unittests\n    on ci/cd tools that do not provide talib\n\n    .. code-block:: python\n\n        real = RSI(\n            close,\n            timeperiod=14)\n\n    :param close: close list\n    :param timeperiod: number of values\n        in ``high``, ``low`` and ``close``\n    :param verbose: show logs\n    """"""\n    if verbose:\n        log.info(\n            \'rsi - start\')\n    return ta.RSI(\n        close,\n        timeperiod)\n# end of RSI\n\n\ndef STOCH(\n        high=None,\n        low=None,\n        close=None,\n        fastk_period=None,\n        slowk_period=None,\n        slowk_matype=None,\n        slowd_period=None,\n        slowd_matype=0,\n        verbose=False):\n    """"""STOCH\n\n    Wrapper for ta.STOCH for running unittests\n    on ci/cd tools that do not provide talib\n\n    .. code-block:: python\n\n        slowk, slowd = STOCH(\n            high,\n            low,\n            close,\n            fastk_period=5,\n            slowk_period=3,\n            slowk_matype=0,\n            slowd_period=3,\n            slowd_matype=0)\n\n    :param high: list of high values\n    :param low: list of low values\n    :param close: list of close values\n    :param fastk_period: integer num\n        of fast k sticks\n    :param slowk_period: integer num\n        of slow k sticks\n    :param slowk_matype: integer moving\n        average\n        (default is ``0``)\n    :param slowd_period: integer num\n        of slow d sticks\n    :param slowd_matype: integer moving\n        average\n        (default is ``0``)\n    :param timeperiod: number of values\n        in ``high``, ``low`` and ``close``\n    :param verbose: show logs\n    """"""\n    if verbose:\n        log.info(\n            \'stoch - start\')\n    return ta.STOCH(\n        high=high,\n        low=low,\n        close=close,\n        fastk_period=fastk_period,\n        slowk_period=slowk_period,\n        slowk_matype=slowk_matype,\n        slowd_period=slowd_period,\n        slowd_matype=slowd_matype)\n# end of STOCH\n\n\ndef STOCHF(\n        high=None,\n        low=None,\n        close=None,\n        fastk_period=None,\n        fastd_period=None,\n        fastd_matype=0,\n        verbose=False):\n    """"""STOCHF\n\n    Wrapper for ta.STOCHF for running unittests\n    on ci/cd tools that do not provide talib\n\n    .. code-block:: python\n\n        fastk, fastd = STOCHF(\n            high,\n            low,\n            close,\n            fastk_period=5,\n            fastd_period=3,\n            fastd_matype=0)\n\n    :param high: list of high values\n    :param low: list of low values\n    :param close: list of close values\n    :param fastk_period: integer num\n        of fast k sticks\n    :param fastd_period: integer num\n        of fast d sticks\n    :param fastd_matype: integer moving\n        average\n        (default is ``0``)\n    :param timeperiod: number of values\n        in ``high``, ``low`` and ``close``\n    :param verbose: show logs\n    """"""\n    if verbose:\n        log.info(\n            \'stoch - start\')\n    return ta.STOCHF(\n        high=high,\n        low=low,\n        close=close,\n        fastk_period=fastk_period,\n        fastd_period=fastd_period,\n        fastd_matype=fastd_matype)\n# end of STOCHF\n\n\ndef WILLR(\n        high=None,\n        low=None,\n        close=None,\n        timeperiod=None,\n        verbose=False):\n    """"""WILLR\n\n    Wrapper for ta.WILLR for running unittests\n    on ci/cd tools that do not provide talib\n\n    .. code-block:: python\n\n        real = WILLR(\n            high,\n            low,\n            close,\n            timeperiod=14)\n\n    :param high: high list\n    :param low: low list\n    :param close: close list\n    :param timeperiod: number of values\n        in ``high``, ``low`` and ``close``\n    :param verbose: show logs\n    """"""\n    if verbose:\n        log.info(\n            \'willr - start\')\n    return ta.WILLR(\n        high,\n        low,\n        close,\n        timeperiod)\n# end of WILLR\n\n\n""""""\nVolume\n\nhttps://mrjbq7.github.io/ta-lib/func_groups/volume_indicators.html\n""""""\n\n\ndef Chaikin(\n        high=None,\n        low=None,\n        close=None,\n        volume=None,\n        verbose=False):\n    """"""Chaikin\n\n    Wrapper for ta.AD for running unittests\n    on ci/cd tools that do not provide talib\n\n    .. code-block:: python\n\n        real = AD(\n            high,\n            low,\n            close,\n            volume)\n\n    :param value: list of values\n        (default should be ``close``)\n    :param volume: list of volume values\n    :param verbose: show logs\n    """"""\n    if verbose:\n        log.info(\n            \'chaikin - start\')\n    return ta.AD(\n        high,\n        low,\n        close,\n        volume)\n# end of Chaikin\n\n\ndef ChaikinADOSC(\n        high=None,\n        low=None,\n        close=None,\n        volume=None,\n        fast_period=3,\n        slow_period=10,\n        verbose=False):\n    """"""ChaikinADOSC\n\n    Wrapper for ta.ADOSC for running unittests\n    on ci/cd tools that do not provide talib\n\n    .. code-block:: python\n\n        real = ADOSC(\n            high,\n            low,\n            close,\n            volume,\n            fastperiod=3,\n            slowperiod=10)\n\n    :param value: list of values\n        (default should be ``close``)\n    :param volume: list of volume values\n    :param verbose: show logs\n    """"""\n    if verbose:\n        log.info(\n            \'chaikinadosc - start\')\n    return ta.ADOSC(\n        high,\n        low,\n        close,\n        volume,\n        fast_period,\n        slow_period)\n# end of ChaikinADOSC\n\n\ndef OBV(\n        value=None,\n        volume=None,\n        verbose=False):\n    """"""OBV\n\n    Wrapper for ta.OBV for running unittests\n    on ci/cd tools that do not provide talib\n\n    .. code-block:: python\n\n        real = OBV(\n            close,\n            volume)\n\n    :param value: list of values\n        (default should be ``close``)\n    :param volume: list of volume values\n    :param verbose: show logs\n    """"""\n    if verbose:\n        log.info(\n            \'obv - start\')\n    return ta.OBV(\n        value,\n        volume)\n# end of OBV\n\n\n""""""\nVolume\n\nhttps://mrjbq7.github.io/ta-lib/func_groups/volatility_indicators.html\n""""""\n\n\ndef ATR(\n        high=None,\n        low=None,\n        close=None,\n        timeperiod=None,\n        verbose=False):\n    """"""ATR\n\n    Wrapper for ta.ATR for running unittests\n    on ci/cd tools that do not provide talib\n\n    .. code-block:: python\n\n        real = ATR(\n            high,\n            low,\n            close,\n            timeperiod=14)\n\n    :param value: list of values\n        (default should be ``close``)\n    :param volume: list of volume values\n    :param verbose: show logs\n    """"""\n    if verbose:\n        log.info(\n            \'atr - start\')\n    return ta.ATR(\n        high,\n        low,\n        close,\n        timeperiod=timeperiod)\n# end of ATR\n\n\ndef NATR(\n        high=None,\n        low=None,\n        close=None,\n        timeperiod=None,\n        verbose=False):\n    """"""NATR\n\n    Wrapper for ta.NATR for running unittests\n    on ci/cd tools that do not provide talib\n\n    .. code-block:: python\n\n        real = NATR(\n            high,\n            low,\n            close,\n            timeperiod=14)\n\n    :param value: list of values\n        (default should be ``close``)\n    :param volume: list of volume values\n    :param verbose: show logs\n    """"""\n    if verbose:\n        log.info(\n            \'natr - start\')\n    return ta.NATR(\n        high,\n        low,\n        close,\n        timeperiod=timeperiod)\n# end of NATR\n\n\ndef TRANGE(\n        high=None,\n        low=None,\n        close=None,\n        verbose=False):\n    """"""TRANGE\n\n    Wrapper for ta.TRANGE for running unittests\n    on ci/cd tools that do not provide talib\n\n    .. code-block:: python\n\n        real = TRANGE(\n            high,\n            low,\n            close)\n\n    :param value: list of values\n        (default should be ``close``)\n    :param volume: list of volume values\n    :param verbose: show logs\n    """"""\n    if verbose:\n        log.info(\n            \'trange - start\')\n    return ta.TRANGE(\n        high,\n        low,\n        close)\n# end of TRANGE\n'"
analysis_engine/algo.py,0,"b'""""""\nAlgorithms automatically provide the following\nmember variables to any custom algorithm that derives\nthe ``analysis_engine.algo.BaseAlgo.process`` method.\n\nBy deriving the ``process()`` member method using an inherited\nclass, you can quickly build algorithms that\ndetermine **buy** and **sell** conditions from\nany of the automatically extracted\ndatasets from the redis pipeline:\n\n- ``self.df_daily``\n- ``self.df_minute``\n- ``self.df_calls``\n- ``self.df_puts``\n- ``self.df_quote``\n- ``self.df_pricing``\n- ``self.df_stats``\n- ``self.df_peers``\n- ``self.df_iex_news``\n- ``self.df_financials``\n- ``self.df_earnings``\n- ``self.df_dividends``\n- ``self.df_company``\n- ``self.df_yahoo_news``\n- ``self.df_tdcalls``\n- ``self.df_tdputs``\n\n**Recent Pricing Information**\n\n- ``self.latest_close``\n- ``self.latest_high``\n- ``self.latest_open``\n- ``self.latest_low``\n- ``self.latest_volume``\n- ``self.today_close``\n- ``self.today_high``\n- ``self.today_open``\n- ``self.today_low``\n- ``self.today_volume``\n- ``self.ask``\n- ``self.bid``\n\n**Latest Backtest Date and Intraday Minute**\n\n- ``self.latest_min``\n- ``self.backtest_date``\n\n.. note:: **self.latest_min** - Latest minute row in ``self.df_minute``\n\n.. note:: **self.backtest_date** - Latest dataset date which is considered the\n    backtest date for historical testing with the data pipeline\n    structure (it\'s the ``date`` key in the dataset node root level)\n\n**Trading Strategy**\n\n- ``self.trade_strategy = \'count\'`` - if the number of indicators\n    saying buy or sell exceeds the buy/sell rules ``min_indicators``\n    the algorithm will trigger a buy or sell\n- ``self.buy_reason`` - derived algorithms can attach custom\n    buy reasons as a string to each trade order\n- ``self.sell_reason`` - derived algorithms can attach custom\n    sell reasons as a string to each trade order\n\n**Timeseries**\n\n- ``self.timeseries``- use an algorithm config to set\n    ``day`` or ``minute`` to process daily or intraday\n    minute by minute datasets. Indicators will still\n    have access to all datasets, this just makes it\n    easier to utilize the helper within an indicator\n    to quickly get the correct dataset:\n\n    .. code-block:: python\n\n        df_status, use_df = self.get_subscribed_dataset(\n            dataset=dataset)\n\n**Balance Information**\n\n- ``self.balance`` - current algorithm account balance\n- ``self.prev_bal`` - previous balance\n- ``self.net_value`` - total value the algorithm has\n    left remaining since starting trading. this includes\n    the number of ``self.num_owned`` shares with the\n    ``self.latest_close`` price included\n- ``self.net_gain`` - amount the algorithm has\n    made since starting including owned shares\n    with the ``self.latest_close`` price included\n\n.. note:: If a key is not in the dataset, the\n    algorithms\'s member variable will be an empty\n    pandas DataFrame created with: ``pandas.DataFrame([])``\n    except ``self.pricing`` which is just a dictionary.\n    Please ensure the engine successfully fetched\n    and cached the dataset in redis using a tool like\n    ``redis-cli`` and a query of ``keys *`` or\n    ``keys <TICKER>_*`` on large deployments.\n\n**Indicator Information**\n\n- ``self.buy_rules`` - optional - custom dictionary for passing\n    buy-side business rules to a custom algorithm\n- ``self.sell_rules`` - optional - custom dictionary for passing\n    sale-side business rules to a custom algorithm\n- ``self.min_buy_indicators`` - if ``self.buy_rules`` has\n    a value for buying if a ``minimum`` number of indicators\n    detect a value that is within a buy condition\n- ``self.min_sell_indicators`` - if ``self.sell_rules`` has\n    a value for selling if a ``minimum`` number of indicators\n    detect a value that is within a sell condition\n- ``self.latest_ind_report`` - latest dictionary of values\n    from the ``IndicatorProcessor.process()``\n- ``self.latest_buys`` - latest indicators saying buy\n- ``self.latest_sells`` - latest indicators saying sell\n- ``self.num_latest_buys`` - latest number of indicators saying buy\n- ``self.num_latest_sells`` - latest number of indicators saying sell\n- ``self.iproc`` - member variables for the ``IndicatorProcessor``\n    that holds all of the custom algorithm indicators\n\nIndicator buy and sell records in ``self.latest_buys`` and\n``self.latest_sells`` have a dictionary structure:\n\n.. code-block:: python\n\n    {\n        \'name\': indicator_name,\n        \'id\': indicator_id,\n        \'report\': indicator_report_dict,\n        \'cell\': indicator cell number\n    }\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport os\nimport json\nimport datetime\nimport pandas as pd\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.utils as ae_utils\nimport analysis_engine.load_history_dataset as load_history_utils\nimport analysis_engine.get_data_from_redis_key as redis_get\nimport analysis_engine.indicators.indicator_processor as ind_processor\nimport analysis_engine.build_trade_history_entry as history_utils\nimport analysis_engine.plot_trading_history as plot_trading_history\nimport analysis_engine.build_buy_order as buy_utils\nimport analysis_engine.build_sell_order as sell_utils\nimport analysis_engine.publish as publish\nimport analysis_engine.build_publish_request as build_publish_request\nimport analysis_engine.load_dataset as load_dataset\nimport analysis_engine.prepare_history_dataset as prepare_history\nimport analysis_engine.prepare_report_dataset as prepare_report\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\nclass BaseAlgo:\n    """"""BaseAlgo\n\n    Run an algorithm against multiple tickers at once through the\n    redis dataframe pipeline provided by\n    `analysis_engine.extract.extract\n    <https://github.com/AlgoTraders/stock-analysis-engine/bl\n    ob/master/analysis_engine/extract.py>`__.\n\n    **Data Pipeline Structure**\n\n    This algorithm can handle an extracted dictionary with structure:\n\n    .. code-block:: python\n\n        import pandas as pd\n        from analysis_engine.algo import BaseAlgo\n        ticker = \'SPY\'\n        demo_algo = BaseAlgo(\n            ticker=ticker,\n            balance=1000.00,\n            commission=6.00,\n            name=f\'test-{ticker}\')\n        date = \'2018-11-05\'\n        dataset_id = f\'{ticker}_{date}\'\n        # mock the data pipeline in redis:\n        data = {\n            ticker: [\n                {\n                    \'id\': dataset_id,\n                    \'date\': date,\n                    \'data\': {\n                        \'daily\': pd.DataFrame([\n                            {\n                                \'high\': 280.01,\n                                \'low\': 270.01,\n                                \'open\': 275.01,\n                                \'close\': 272.02,\n                                \'volume\': 123,\n                                \'date\': \'2018-11-01 15:59:59\'\n                            },\n                            {\n                                \'high\': 281.01,\n                                \'low\': 271.01,\n                                \'open\': 276.01,\n                                \'close\': 273.02,\n                                \'volume\': 124,\n                                \'date\': \'2018-11-02 15:59:59\'\n                            },\n                            {\n                                \'high\': 282.01,\n                                \'low\': 272.01,\n                                \'open\': 277.01,\n                                \'close\': 274.02,\n                                \'volume\': 121,\n                                \'date\': \'2018-11-05 15:59:59\'\n                            }\n                        ]),\n                        \'calls\': pd.DataFrame([]),\n                        \'puts\': pd.DataFrame([]),\n                        \'minute\': pd.DataFrame([]),\n                        \'pricing\': pd.DataFrame([]),\n                        \'quote\': pd.DataFrame([]),\n                        \'news\': pd.DataFrame([]),\n                        \'news1\': pd.DataFrame([]),\n                        \'dividends\': pd.DataFrame([]),\n                        \'earnings\': pd.DataFrame([]),\n                        \'financials\': pd.DataFrame([]),\n                        \'stats\': pd.DataFrame([]),\n                        \'peers\': pd.DataFrame([]),\n                        \'company\': pd.DataFrame([])\n                    }\n                }\n            ]\n        }\n\n        # run the algorithm\n        demo_algo.handle_data(data=data)\n\n        # get the algorithm results\n        results = demo_algo.get_result()\n\n        print(results)\n    """"""\n\n    def __init__(\n            self,\n            ticker=None,\n            balance=5000.0,\n            commission=6.0,\n            tickers=None,\n            name=None,\n            use_key=None,\n            auto_fill=True,\n            version=1,\n            config_file=None,\n            config_dict=None,\n            output_dir=None,\n            publish_to_slack=False,\n            publish_to_s3=False,\n            publish_to_redis=False,\n            publish_input=True,\n            publish_history=True,\n            publish_report=True,\n            load_from_s3_bucket=None,\n            load_from_s3_key=None,\n            load_from_redis_key=None,\n            load_from_file=None,\n            load_compress=False,\n            load_publish=True,\n            load_config=None,\n            report_redis_key=None,\n            report_s3_bucket=None,\n            report_s3_key=None,\n            report_file=None,\n            report_compress=False,\n            report_publish=True,\n            report_config=None,\n            history_redis_key=None,\n            history_s3_bucket=None,\n            history_s3_key=None,\n            history_file=None,\n            history_compress=False,\n            history_publish=True,\n            history_config=None,\n            extract_redis_key=None,\n            extract_s3_bucket=None,\n            extract_s3_key=None,\n            extract_file=None,\n            extract_save_dir=None,\n            extract_compress=False,\n            extract_publish=True,\n            extract_config=None,\n            dataset_type=ae_consts.SA_DATASET_TYPE_ALGO_READY,\n            serialize_datasets=ae_consts.DEFAULT_SERIALIZED_DATASETS,\n            timeseries=None,\n            trade_strategy=None,\n            verbose=False,\n            verbose_processor=False,\n            verbose_indicators=False,\n            verbose_trading=False,\n            verbose_load=False,\n            verbose_extract=False,\n            verbose_history=False,\n            verbose_report=False,\n            inspect_datasets=False,\n            raise_on_err=True,\n            **kwargs):\n        """"""__init__\n\n        Build an analysis algorithm\n\n        Use an algorithm object to:\n\n        1) `Generate algorithm-ready datasets <https://gith\n        ub.com/AlgoTraders/stock-analysis-engine#extra\n        ct-algorithm-ready-datasets>`__\n        2) Backtest trading theories with offline\n        3) Issue trading alerts from the latest fetched datasets\n\n        **(Optional) Trading Parameters**\n\n        :param ticker: single ticker string\n        :param balance: starting capital balance\n            (default is ``5000.00``)\n        :param commission: cost for commission\n            for a single buy or sell trade\n        :param tickers: optional - list of ticker strings\n        :param name: optional - log tracking name\n            or algo name\n        :param use_key: optional - key for saving output\n            in s3, redis, file\n        :param auto_fill: optional - boolean for auto filling\n            buy and sell orders for backtesting (default is\n            ``True``)\n        :param version: optional - version tracking\n            value (default is ``1``)\n\n        **Derived Config Loading for Indicators and Custom Backtest Values**\n\n        :param config_file: path to a json file\n            containing custom algorithm object\n            member values (like indicator configuration and\n            predict future date units ahead for a backtest)\n        :param config_dict: optional - dictionary that\n            can be passed to derived class implementations\n            of: ``def load_from_config(config_dict=config_dict)``\n\n        **Run a Backtest with an Algorithm-Ready Dataset in S3,\n        Redis or a File**\n\n        Use these arguments to load algorithm-ready datasets\n        from supported sources (s3, redis or a file)\n\n        :param load_from_s3_bucket: optional - string load the algo from an\n            a previously-created s3 bucket holding an s3 key with an\n            algorithm-ready dataset for use with:\n            ``handle_data``\n        :param load_from_s3_key: optional - string load the algo from an\n            a previously-created s3 key holding an\n            algorithm-ready dataset for use with:\n            ``handle_data``\n        :param load_from_redis_key: optional - string load the algo from a\n            a previously-created redis key holding an\n            algorithm-ready dataset for use with:\n            ``handle_data``\n        :param load_from_file: optional - string path to\n            a previously-created local file holding an\n            algorithm-ready dataset for use with:\n            ``handle_data``\n        :param load_compress: optional - boolean\n            flag for toggling to decompress\n            or not when loading an algorithm-ready\n            dataset (``True`` means the dataset\n            must be decompressed to load correctly inside\n            an algorithm to run a backtest)\n        :param load_publish: boolean - toggle publishing\n            the load progress to slack, s3, redis or a file\n            (default is ``True``)\n        :param load_config: optional - dictionary\n            for setting member variables to load\n            an algorithm-ready dataset for backtesting\n            (used by ``run_custom_algo``)\n\n        **Algorithm Trade History Arguments**\n\n        :param history_redis_key: optional - string\n            where the algorithm trading history will be stored in\n            an redis key\n        :param history_s3_bucket: optional - string\n            where the algorithm trading history will be stored in\n            an s3 bucket\n        :param history_s3_key: optional - string\n            where the algorithm trading history will be stored in\n            an s3 key\n        :param history_file: optional - string file path for saving\n            the ``trading history``\n        :param history_compress: optional - boolean\n            flag for toggling to decompress\n            or not when loading an algorithm-ready\n            dataset (``True`` means the dataset\n            will be compressed on publish)\n        :param history_publish: boolean - toggle publishing\n            the history to s3, redis or a file\n            (default is ``True``)\n        :param history_config: optional - dictionary\n            for setting member variables to publish\n            an algo ``trade history``\n            to s3, redis, a file or slack\n            (used by ``run_custom_algo``)\n\n        **Algorithm Trade Performance Report Arguments (Output Dataset)**\n\n        :param report_redis_key: optional - string\n            where the algorithm ``trading performance report`` (report)\n            will be stored in an redis key\n        :param report_s3_bucket: optional - string\n            where the algorithm report will be stored in\n            an s3 bucket\n        :param report_s3_key: optional - string\n            where the algorithm report will be stored in\n            an s3 key\n        :param report_file: optional - string file path for saving\n            the ``trading performance report``\n        :param report_compress: optional - boolean\n            flag for toggling to decompress\n            or not when loading an algorithm-ready\n            dataset (``True`` means the dataset\n            will be compressed on publish)\n        :param report_publish: boolean - toggle publishing\n            the ``trading performance report`` s3, redis or a file\n            (default is ``True``)\n        :param report_config: optional - dictionary\n            for setting member variables to publish\n            an algo ``result`` or\n            ``trading performance report``\n            to s3, redis, a file or slack\n            (used by ``run_custom_algo``)\n\n        **Extract an Algorithm-Ready Dataset Arguments**\n\n        :param extract_redis_key: optional - string\n            where the algorithm report will be stored in\n            an redis key\n        :param extract_s3_bucket: optional - string\n            where the algorithm report will be stored in\n            an s3 bucket\n        :param extract_s3_key: optional - string\n            where the algorithm report will be stored in\n            an s3 key\n        :param extract_file: optional - string file path for saving\n            the processed datasets as an``algorithm-ready`` dataset\n        :param extract_save_dir: optional - string path to\n            auto-generated files from the algo\n        :param extract_compress: optional - boolean\n            flag for toggling to decompress\n            or not when loading an algorithm-ready\n            dataset (``True`` means the dataset\n            will be compressed on publish)\n        :param extract_publish: boolean - toggle publishing\n            the used ``algorithm-ready dataset`` to s3, redis or a file\n            (default is ``True``)\n        :param extract_config: optional - dictionary\n            for setting member variables to publish\n            an algo ``input`` dataset (the contents of ``data``\n            from ``self.handle_data(data=data)``\n            (used by ``run_custom_algo``)\n\n        **Dataset Arguments**\n\n        :param dataset_type: optional - dataset type\n            (default is ``ae_consts.SA_DATASET_TYPE_ALGO_READY``)\n        :param serialize_datasets: optional - list of dataset names to\n            deserialize in the dataset\n            (default is ``ae_consts.DEFAULT_SERIALIZED_DATASETS``)\n        :param encoding: optional - string for data encoding\n\n        **(Optional) Publishing arguments**\n\n        :param publish_to_slack: optional - boolean for\n            publishing to slack (default is ``False``)\n        :param publish_to_s3: optional - boolean for\n            publishing to s3 (default is ``False``)\n        :param publish_to_redis: optional - boolean for\n            publishing to redis (default is ``False``)\n        :param publish_input: boolean - toggle publishing\n            all input datasets to s3 and redis\n            (default ``True``)\n        :param publish_history: boolean - toggle publishing\n            the history to s3 and redis\n            (default ``True``)\n        :param publish_report: boolean - toggle publishing\n            any generated datasets to s3 and redis\n            (default ``True``)\n\n        **Timeseries**\n\n        :param timeseries: optional - string to\n            set ``day`` or ``minute`` backtesting\n            or live trading\n            (default is ``minute``)\n\n        **Trading Strategy**\n\n        :param trade_strategy: optional - string to\n            set the type of ``Trading Strategy``\n            for backtesting or live trading\n            (default is ``count``)\n\n        **Debugging arguments**\n\n        :param verbose: optional - boolean for\n            showing verbose algorithm logs\n            (default is ``False``)\n        :param verbose_processor: optional - boolean for\n            showing verbose ``IndicatorProcessor`` logs\n            (default is ``False``)\n        :param verbose_indicators: optional - boolean for\n            showing verbose ``Indicator`` logs\n            (default is ``False`` which means an ``Indicator``\n            can set ``\'verbose\': True`` to enable\n            logging per individal ``Indicator``)\n        :param verbose_trading: optional - boolean for logging\n            in the trading functions including the reasons\n            why a buy or sell was opened\n        :param verbose_load: optional - boolean for debugging\n            algorithm ready dataset loading\n        :param verbose_extract: optional - boolean for debugging\n            algorithm ready dataset extraction\n        :param verbose_history: optional - boolean for debugging\n            trading history dataset\n        :param verbose_report: optional - boolean for debugging\n            algorithm report\n        :param inspect_datasets: optional - boolean for logging\n            what is sent to the algorithm\'s ``process()`` function\n            (default is ``False`` as this will slow processing down)\n        :param raise_on_err: optional - boolean for\n            unittests and developing algorithms with the\n            ``analysis_engine.run_algo.run_algo`` helper.\n            .. note:: When set to ``True`` exceptions will\n                are raised to the calling functions\n        :param output_dir: optional - string path to\n            auto-generated files from the algo\n\n        **Future Argument Placeholder**\n\n        :param kwargs: optional - dictionary of keyword\n            arguments\n        """"""\n        self.buys = []\n        self.sells = []\n        self.num_shares = 0\n        self.tickers = tickers\n        if not self.tickers:\n            if ticker:\n                self.tickers = [\n                    ticker.upper()\n                ]\n            else:\n                raise Exception(\'BaseAlgo - please set a ticker to use\')\n        self.balance = balance\n        self.starting_balance = balance\n        self.starting_close = 0.0\n        self.timeseries = timeseries\n        if not self.timeseries:\n            self.timeseries = \'minute\'\n        self.trade_strategy = trade_strategy\n        if not self.trade_strategy:\n            self.trade_strategy = \'count\'\n        self.timeseries_value = ae_consts.ALGO_TIMESERIES_MINUTE\n        self.trade_horizon = 5\n        self.commission = commission\n        self.result = None\n        self.name = name\n        self.num_owned = None\n        self.num_buys = 0\n        self.num_sells = 0\n        self.ticker_buys = []\n        self.ticker_sell = []\n        self.trade_price = 0.0\n        self.today_high = 0.0\n        self.today_low = 0.0\n        self.today_open = 0.0\n        self.today_close = 0.0\n        self.today_volume = 0\n        self.latest_close = 0.0\n        self.latest_high = 0.0\n        self.latest_open = 0.0\n        self.latest_low = 0.0\n        self.latest_volume = 0\n        self.latest_min = None\n        self.last_minute = None\n        self.backtest_date = None\n        self.ask = 0.0\n        self.bid = 0.0\n        self.prev_bal = None\n        self.prev_num_owned = None\n        self.ds_id = None\n        self.trade_date = None\n        self.trade_type = ae_consts.TRADE_SHARES\n        self.buy_hold_units = 20\n        self.sell_hold_units = 20\n        self.spread_exp_date = None\n        self.last_close = None\n        self.order_history = []\n        self.config_file = config_file\n        self.config_dict = config_dict\n        self.positions = {}\n        self.created_on_date = datetime.datetime.utcnow()\n        self.created_date = self.created_on_date.strftime(\n            ae_consts.COMMON_TICK_DATE_FORMAT)\n        self.created_buy = False\n        self.should_buy = False\n        self.buy_strength = None\n        self.buy_risk = None\n        self.created_sell = False\n        self.should_sell = False\n        self.sell_strength = None\n        self.sell_risk = None\n        self.stop_loss = None\n        self.trailing_stop_loss = None\n\n        self.last_handle_data = None\n        self.last_ds_id = None\n        self.last_ds_date = None\n        self.last_ds_data = None\n\n        self.ds_date = None\n        self.ds_data = None\n        self.df_daily = pd.DataFrame([{}])\n        self.df_minute = pd.DataFrame([{}])\n        self.df_stats = pd.DataFrame([{}])\n        self.df_peers = pd.DataFrame([{}])\n        self.df_financials = pd.DataFrame([])\n        self.df_earnings = pd.DataFrame([{}])\n        self.df_dividends = pd.DataFrame([{}])\n        self.df_quote = pd.DataFrame([{}])\n        self.df_company = pd.DataFrame([{}])\n        self.df_iex_news = pd.DataFrame([{}])\n        self.df_yahoo_news = pd.DataFrame([{}])\n        self.df_calls = pd.DataFrame([{}])\n        self.df_puts = pd.DataFrame([{}])\n        self.df_pricing = pd.DataFrame([{}])\n        self.df_tdcalls = pd.DataFrame([{}])\n        self.df_tdputs = pd.DataFrame([{}])\n        self.empty_pd = pd.DataFrame([{}])\n        self.empty_pd_str = ae_consts.EMPTY_DF_STR\n\n        self.note = None\n        self.debug_msg = \'\'\n        self.version = version\n        self.verbose = verbose\n        self.verbose_processor = verbose_processor\n        self.verbose_indicators = verbose_indicators\n        self.verbose_trading = verbose_trading\n        self.verbose_load = verbose_load\n        self.verbose_extract = verbose_extract\n        self.verbose_history = verbose_history\n        self.verbose_report = verbose_report\n\n        self.inspect_datasets = inspect_datasets\n        self.run_this_date = None\n\n        self.verbose = ae_consts.ev(\n            \'AE_DEBUG\',\n            \'0\') == \'1\'\n\n        self.publish_to_slack = publish_to_slack\n        self.publish_to_s3 = publish_to_s3\n        self.publish_to_redis = publish_to_redis\n        self.publish_history = publish_history\n        self.publish_report = publish_report\n        self.publish_input = publish_input\n        self.raise_on_err = raise_on_err\n\n        if not self.publish_to_s3:\n            self.publish_to_s3 = ae_consts.ENABLED_S3_UPLOAD\n        if not self.publish_to_redis:\n            self.publish_to_redis = ae_consts.ENABLED_REDIS_PUBLISH\n\n        self.output_file_dir = None\n        self.output_file_prefix = None\n\n        if self.raise_on_err:\n            if self.tickers and len(self.tickers):\n                self.output_file_prefix = str(\n                    self.tickers[0]).upper()\n            self.output_file_dir = \'/opt/sa/tests/datasets/algo\'\n\n        if not self.name:\n            self.name = \'myalgo\'\n\n        """"""\n        Load tracking connectivity for recording\n        - input\n        - trade history\n        - algorithm-generated datasets\n        """"""\n\n        # parse optional input args\n        self.save_as_key = use_key\n        if not self.save_as_key:\n            self.save_as_key = (\n                f\'{self.name.replace("" "", """")}-\'\n                f\'{ae_utils.utc_now_str(fmt=""%Y-%m-%d-%H-%M-%S.%f"")}\')\n        self.output_file_dir = \'/opt/sa/tests/datasets/algo\'\n        if not output_dir:\n            self.output_file_dir = output_dir\n\n        # set up default keys\n        self.default_output_file = (\n            f\'{self.output_file_dir}/{self.save_as_key}.json\')\n        self.default_s3_key = f\'{self.save_as_key}.json\'\n        self.default_redis_key = f\'{self.save_as_key}\'\n\n        self.default_load_output_file = (\n            f\'{self.output_file_dir}/ready-{self.save_as_key}.json\')\n        self.default_history_output_file = (\n            f\'{self.output_file_dir}/history-{self.save_as_key}.json\')\n        self.default_report_output_file = (\n            f\'{self.output_file_dir}/report-{self.save_as_key}.json\')\n        self.default_extract_output_file = (\n            f\'{self.output_file_dir}/extract-{self.save_as_key}.json\')\n\n        self.default_load_redis_key = (\n            f\'algo:ready:{self.default_redis_key}\')\n        self.default_history_redis_key = (\n            f\'algo:history:{self.default_redis_key}\')\n        self.default_report_redis_key = (\n            f\'algo:output:{self.default_redis_key}\')\n        self.default_extract_redis_key = (\n            f\'algo:extract:{self.default_redis_key}\')\n\n        if not load_config:\n            load_config = build_publish_request.build_publish_request()\n        if not extract_config:\n            extract_config = build_publish_request.build_publish_request()\n        if not history_config:\n            history_config = build_publish_request.build_publish_request()\n        if not report_config:\n            report_config = build_publish_request.build_publish_request()\n\n        if not load_from_s3_bucket:\n            load_from_s3_bucket = ae_consts.ALGO_READY_DATASET_S3_BUCKET_NAME\n        if not extract_s3_bucket:\n            extract_s3_bucket = ae_consts.ALGO_EXTRACT_DATASET_S3_BUCKET_NAME\n        if not history_s3_bucket:\n            history_s3_bucket = ae_consts.ALGO_HISTORY_DATASET_S3_BUCKET_NAME\n        if not report_s3_bucket:\n            report_s3_bucket = ae_consts.ALGO_REPORT_DATASET_S3_BUCKET_NAME\n\n        # Load the input dataset publishing member variables\n        self.extract_output_dir = extract_config.get(\n            \'output_dir\', None)\n        self.extract_output_file = extract_config.get(\n            \'output_file\', None)\n        self.extract_label = extract_config.get(\n            \'label\', self.name)\n        self.extract_convert_to_json = extract_config.get(\n            \'convert_to_json\', True)\n        self.extract_compress = extract_config.get(\n            \'compress\', ae_consts.ALGO_INPUT_COMPRESS)\n        self.extract_redis_enabled = extract_config.get(\n            \'redis_enabled\', False)\n        self.extract_redis_address = extract_config.get(\n            \'redis_address\', ae_consts.ENABLED_S3_UPLOAD)\n        self.extract_redis_db = extract_config.get(\n            \'redis_db\', ae_consts.REDIS_DB)\n        self.extract_redis_password = extract_config.get(\n            \'redis_password\', ae_consts.REDIS_PASSWORD)\n        self.extract_redis_expire = extract_config.get(\n            \'redis_expire\', ae_consts.REDIS_EXPIRE)\n        self.extract_redis_serializer = extract_config.get(\n            \'redis_serializer\', \'json\')\n        self.extract_redis_encoding = extract_config.get(\n            \'redis_encoding\', \'utf-8\')\n        self.extract_s3_enabled = extract_config.get(\n            \'s3_enabled\', False)\n        self.extract_s3_address = extract_config.get(\n            \'s3_address\', ae_consts.S3_ADDRESS)\n        self.extract_s3_bucket = extract_config.get(\n            \'s3_bucket\', extract_s3_bucket)\n        self.extract_s3_access_key = extract_config.get(\n            \'s3_access_key\', ae_consts.S3_ACCESS_KEY)\n        self.extract_s3_secret_key = extract_config.get(\n            \'s3_secret_key\', ae_consts.S3_SECRET_KEY)\n        self.extract_s3_region_name = extract_config.get(\n            \'s3_region_name\', ae_consts.S3_REGION_NAME)\n        self.extract_s3_secure = extract_config.get(\n            \'s3_secure\', ae_consts.S3_SECURE)\n        self.extract_slack_enabled = extract_config.get(\n            \'slack_enabled\', False)\n        self.extract_slack_code_block = extract_config.get(\n            \'slack_code_block\', False)\n        self.extract_slack_full_width = extract_config.get(\n            \'slack_full_width\', False)\n        self.extract_redis_key = extract_config.get(\n            \'redis_key\', extract_redis_key)\n        self.extract_s3_key = extract_config.get(\n            \'s3_key\', extract_s3_key)\n        self.extract_verbose = extract_config.get(\n            \'verbose\', False)\n\n        # load the trade history publishing member variables\n        self.history_output_dir = history_config.get(\n            \'output_dir\', None)\n        self.history_output_file = history_config.get(\n            \'output_file\', None)\n        self.history_label = history_config.get(\n            \'label\', self.name)\n        self.history_convert_to_json = history_config.get(\n            \'convert_to_json\', True)\n        self.history_compress = history_config.get(\n            \'compress\', ae_consts.ALGO_HISTORY_COMPRESS)\n        self.history_redis_enabled = history_config.get(\n            \'redis_enabled\', False)\n        self.history_redis_address = history_config.get(\n            \'redis_address\', ae_consts.ENABLED_S3_UPLOAD)\n        self.history_redis_db = history_config.get(\n            \'redis_db\', ae_consts.REDIS_DB)\n        self.history_redis_password = history_config.get(\n            \'redis_password\', ae_consts.REDIS_PASSWORD)\n        self.history_redis_expire = history_config.get(\n            \'redis_expire\', ae_consts.REDIS_EXPIRE)\n        self.history_redis_serializer = history_config.get(\n            \'redis_serializer\', \'json\')\n        self.history_redis_encoding = history_config.get(\n            \'redis_encoding\', \'utf-8\')\n        self.history_s3_enabled = history_config.get(\n            \'s3_enabled\', False)\n        self.history_s3_address = history_config.get(\n            \'s3_address\', ae_consts.S3_ADDRESS)\n        self.history_s3_bucket = history_config.get(\n            \'s3_bucket\', history_s3_bucket)\n        self.history_s3_access_key = history_config.get(\n            \'s3_access_key\', ae_consts.S3_ACCESS_KEY)\n        self.history_s3_secret_key = history_config.get(\n            \'s3_secret_key\', ae_consts.S3_SECRET_KEY)\n        self.history_s3_region_name = history_config.get(\n            \'s3_region_name\', ae_consts.S3_REGION_NAME)\n        self.history_s3_secure = history_config.get(\n            \'s3_secure\', ae_consts.S3_SECURE)\n        self.history_slack_enabled = history_config.get(\n            \'slack_enabled\', False)\n        self.history_slack_code_block = history_config.get(\n            \'slack_code_block\', False)\n        self.history_slack_full_width = history_config.get(\n            \'slack_full_width\', False)\n        self.history_redis_key = history_config.get(\n            \'redis_key\', history_redis_key)\n        self.history_s3_key = history_config.get(\n            \'s3_key\', history_s3_key)\n        self.history_verbose = history_config.get(\n            \'verbose\', False)\n\n        # Load publishing for algorithm-generated report member variables\n        self.report_output_dir = report_config.get(\n            \'output_dir\', None)\n        self.report_output_file = report_config.get(\n            \'output_file\', None)\n        self.report_label = report_config.get(\n            \'label\', self.name)\n        self.report_convert_to_json = report_config.get(\n            \'convert_to_json\', True)\n        self.report_compress = report_config.get(\n            \'compress\', ae_consts.ALGO_REPORT_COMPRESS)\n        self.report_redis_enabled = report_config.get(\n            \'redis_enabled\', False)\n        self.report_redis_address = report_config.get(\n            \'redis_address\', ae_consts.ENABLED_S3_UPLOAD)\n        self.report_redis_db = report_config.get(\n            \'redis_db\', ae_consts.REDIS_DB)\n        self.report_redis_password = report_config.get(\n            \'redis_password\', ae_consts.REDIS_PASSWORD)\n        self.report_redis_expire = report_config.get(\n            \'redis_expire\', ae_consts.REDIS_EXPIRE)\n        self.report_redis_serializer = report_config.get(\n            \'redis_serializer\', \'json\')\n        self.report_redis_encoding = report_config.get(\n            \'redis_encoding\', \'utf-8\')\n        self.report_s3_enabled = report_config.get(\n            \'s3_enabled\', False)\n        self.report_s3_address = report_config.get(\n            \'s3_address\', ae_consts.S3_ADDRESS)\n        self.report_s3_bucket = report_config.get(\n            \'s3_bucket\', report_s3_bucket)\n        self.report_s3_access_key = report_config.get(\n            \'s3_access_key\', ae_consts.S3_ACCESS_KEY)\n        self.report_s3_secret_key = report_config.get(\n            \'s3_secret_key\', ae_consts.S3_SECRET_KEY)\n        self.report_s3_region_name = report_config.get(\n            \'s3_region_name\', ae_consts.S3_REGION_NAME)\n        self.report_s3_secure = report_config.get(\n            \'s3_secure\', ae_consts.S3_SECURE)\n        self.report_slack_enabled = report_config.get(\n            \'slack_enabled\', False)\n        self.report_slack_code_block = report_config.get(\n            \'slack_code_block\', False)\n        self.report_slack_full_width = report_config.get(\n            \'slack_full_width\', False)\n        self.report_redis_key = report_config.get(\n            \'redis_key\', report_redis_key)\n        self.report_s3_key = report_config.get(\n            \'s3_key\', report_s3_key)\n        self.report_verbose = report_config.get(\n            \'verbose\', False)\n\n        self.loaded_dataset = None\n\n        # load the algorithm-ready dataset input member variables\n        self.dsload_output_dir = load_config.get(\n            \'output_dir\', None)\n        self.dsload_output_file = load_config.get(\n            \'output_file\', None)\n        self.dsload_label = load_config.get(\n            \'label\', self.name)\n        self.dsload_convert_to_json = load_config.get(\n            \'convert_to_json\', True)\n        self.dsload_compress = load_config.get(\n            \'compress\', load_compress)\n        self.dsload_redis_enabled = load_config.get(\n            \'redis_enabled\', False)\n        self.dsload_redis_address = load_config.get(\n            \'redis_address\', ae_consts.ENABLED_S3_UPLOAD)\n        self.dsload_redis_db = load_config.get(\n            \'redis_db\', ae_consts.REDIS_DB)\n        self.dsload_redis_password = load_config.get(\n            \'redis_password\', ae_consts.REDIS_PASSWORD)\n        self.dsload_redis_expire = load_config.get(\n            \'redis_expire\', ae_consts.REDIS_EXPIRE)\n        self.dsload_redis_serializer = load_config.get(\n            \'redis_serializer\', \'json\')\n        self.dsload_redis_encoding = load_config.get(\n            \'redis_encoding\', \'utf-8\')\n        self.dsload_s3_enabled = load_config.get(\n            \'s3_enabled\', False)\n        self.dsload_s3_address = load_config.get(\n            \'s3_address\', ae_consts.S3_ADDRESS)\n        self.dsload_s3_bucket = load_config.get(\n            \'s3_bucket\', load_from_s3_bucket)\n        self.dsload_s3_access_key = load_config.get(\n            \'s3_access_key\', ae_consts.S3_ACCESS_KEY)\n        self.dsload_s3_secret_key = load_config.get(\n            \'s3_secret_key\', ae_consts.S3_SECRET_KEY)\n        self.dsload_s3_region_name = load_config.get(\n            \'s3_region_name\', ae_consts.S3_REGION_NAME)\n        self.dsload_s3_secure = load_config.get(\n            \'s3_secure\', ae_consts.S3_SECURE)\n        self.dsload_slack_enabled = load_config.get(\n            \'slack_enabled\', False)\n        self.dsload_slack_code_block = load_config.get(\n            \'slack_code_block\', False)\n        self.dsload_slack_full_width = load_config.get(\n            \'slack_full_width\', False)\n        self.dsload_redis_key = load_config.get(\n            \'redis_key\', load_from_redis_key)\n        self.dsload_s3_key = load_config.get(\n            \'s3_key\', load_from_s3_key)\n        self.dsload_verbose = load_config.get(\n            \'verbose\', False)\n\n        self.include_custom = {}\n\n        self.load_from_external_source()\n\n        if self.config_dict:\n            self.timeseries = self.config_dict.get(\n                \'timeseries\',\n                \'day\').lower()\n            self.trade_horizon = int(self.config_dict.get(\n                \'trade_horizon\',\n                \'5\'))\n            self.load_custom_datasets()\n        # end of loading initial values from a config_dict before derived\n\n        self.iproc = None\n        self.iproc_label = \'no-iproc-label\'\n        self.num_indicators = 0\n        self.latest_ind_report = None\n        self.latest_buys = []  # latest indicators saying buy\n        self.latest_sells = []  # latest indicators saying sell\n        self.num_latest_buys = 0  # latest number of indicators saying buy\n        self.num_latest_sells = 0  # latest number of indicators saying sell\n        self.min_buy_indicators = 0\n        self.min_sell_indicators = 0\n        self.buy_rules = {}\n        self.sell_rules = {}\n        self.buy_shares = None\n        self.is_live_trading = False\n        self.found_minute_data = False\n        self.use_minute = None\n        self.intraday_start_min = None\n        self.intraday_end_min = None\n        self.intraday_events = {}\n\n        self.ignore_history_keys = [\n        ]\n        self.ind_conf_ignore_keys = [\n            \'buys\',\n            \'date\',\n            \'id\',\n            \'sells\',\n            \'ticker\'\n        ]\n        self.buy_reason = None\n        self.sell_reason = None\n\n        """"""\n        if this is in a juptyer notebook\n        this will show the plots at the end of\n        each day... please avoid with\n        the command line as the plot\'s window\n        will block the algorithm until the window\n        is closed\n        """"""\n        self.show_balance = ae_consts.ev(\n            \'SHOW_ALGO_BALANCE\',\n            \'0\') == \'1\'\n        self.show_log = False\n        self.red_column = \'close\'\n        self.blue_column = \'balance\'\n        self.green_column = None\n        self.orange_column = None\n        self.net_value = self.starting_balance\n        self.net_gain = self.net_value\n\n        self.load_from_config(\n            config_dict=self.config_dict)\n\n        self.starting_balance = self.balance\n        self.net_value = self.starting_balance\n        self.net_gain = self.net_value\n\n        self.timeseries = str(self.timeseries).lower()\n        if self.timeseries == \'day\':\n            self.timeseries_value = ae_consts.ALGO_TIMESERIES_DAY\n        elif self.timeseries == \'daily\':\n            self.timeseries_value = ae_consts.ALGO_TIMESERIES_DAY\n        elif self.timeseries == \'minute\':\n            self.timeseries_value = ae_consts.ALGO_TIMESERIES_MINUTE\n        elif self.timeseries == \'intraday\':\n            self.timeseries_value = ae_consts.ALGO_TIMESERIES_MINUTE\n        else:\n            self.timeseries_value = ae_consts.ALGO_TIMESERIES_MINUTE\n\n        self.trade_strategy = str(self.trade_strategy).lower()\n        self.trade_off_num_indicators = False\n        if self.trade_strategy == \'count\':\n            self.trade_off_num_indicators = True\n        else:\n            self.trade_off_num_indicators = True\n\n        self.indicator_datasets = []\n        self.determine_indicator_datasets()\n\n        # build the IndicatorProcessor after loading\n        # values from an optional config_dict\n        self.iproc = self.get_indicator_processor()\n        if self.iproc:\n            if not hasattr(self.iproc, \'process\'):\n                err = (\n                    f\'{self.name} - Please implement a process() method \'\n                    \'in the IndicatorProcessor - the current object=\'\n                    f\'{self.iproc} is missing one.\')\n                log.critical(err)\n                raise Exception(err)\n            self.iproc_label = self.iproc.get_label()\n            self.num_indicators = self.iproc.get_num_indicators()\n            self.min_buy_indicators = self.buy_rules.get(\n                \'min_indicators\',\n                self.num_indicators)\n            self.min_sell_indicators = self.sell_rules.get(\n                \'min_indicators\',\n                self.num_indicators)\n        # if indicator_processor exists\n\n    # end of __init__\n\n    def determine_indicator_datasets(\n            self):\n        """"""determine_indicator_datasets\n\n        Indicators are coupled to a dataset in the algorithm\n        config file. This allows for identifying the exact\n        datasets to pull from Redis to speed up backtesting.\n        """"""\n        if self.config_dict:\n            for ind_node in self.config_dict.get(\'indicators\', []):\n                uses_dataset = ind_node.get(\'uses_data\', \'minute\')\n                if uses_dataset not in self.indicator_datasets:\n                    self.indicator_datasets.append(uses_dataset)\n    # end of determine_indicator_datasets\n\n    def get_indicator_datasets(\n            self):\n        """"""get_indicator_datasets""""""\n        return self.indicator_datasets\n    # end of get_indicator_datasets\n\n    def view_date_dataset_records(\n            self,\n            algo_id,\n            ticker,\n            node):\n        """"""view_date_dataset_records\n\n        View the dataset contents for a single node - use it with\n        the algo config_dict by setting:\n\n        ::\n\n            ""run_this_date"": <string date YYYY-MM-DD>\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param node: dataset to process\n        """"""\n        # this will happen twice\n\n        self.load_from_dataset(\n            ds_data=node)\n        self.inspect_dataset(\n            algo_id=algo_id,\n            ticker=ticker,\n            dataset=node)\n        if self.timeseries == \'minute\':\n            if len(self.df_minute.index) <= 1:\n                log.error(\'EMPTY minute dataset\')\n                if self.raise_on_err:\n                    raise Exception(\'EMPTY minute dataset\')\n                return\n            for i, row in self.df_minute.iterrows():\n                log.info(f\'minute={i} date={row[""date""]} close={row[""close""]}\')\n            log.info(f\'minute df len={len(self.df_minute.index)}\')\n        elif self.timeseries == \'day\':\n            if len(self.df_daily.index) == 0:\n                log.error(\'EMPTY daily dataset\')\n                if self.raise_on_err:\n                    raise Exception(\'EMPTY daily dataset\')\n                return\n            if hasattr(self.daily, \'to_json\'):\n                for i, row in self.df_daily.iterrows():\n                    log.info(\n                        f\'day={i} date={row[""date""]} close={row[""close""]}\')\n                log.info(f\'day df len={len(self.daily.index)}\')\n    # end of view_date_dataset_records\n\n    def get_indicator_processor(\n            self,\n            existing_processor=None):\n        """"""get_indicator_processor\n\n        singleton for getting the indicator processor\n\n        :param existing_processor: allow derived algos\n            to build their own indicator\n            processor and pass it to the base\n        """"""\n        if existing_processor:\n            if self.verbose:\n                log.info(\n                    f\'{self.name} - loading existing \'\n                    f\'processor={existing_processor.get_name()}\')\n            self.iproc = existing_processor\n        else:\n            if self.iproc:\n                return self.iproc\n\n            if not self.config_dict:\n                if self.verbose:\n                    log.info(\n                        f\'{self.name} - is missing an algorithm config_dict \'\n                        \'please add one to run indicators\')\n            else:\n                self.iproc = ind_processor.IndicatorProcessor(\n                    config_dict=self.config_dict,\n                    label=f\'{self.name}-prc\',\n                    verbose=self.verbose_processor)\n        # if use new or existing\n\n        return self.iproc\n    # end of get_indicator_processor\n\n    def get_indicator_process_last_indicator(\n            self):\n        """"""get_indicator_process_last_indicator\n\n        Used to pull the indicator object back up\n        to any created ``analysis_engine.algo.BaseAlgo`` objects\n\n        .. tip:: this is for debugging data and code issues inside an\n            indicator\n        """"""\n        return self.get_indicator_processor().get_last_ind_obj()\n    # end of get_indicator_process_last_indicator\n\n    def inspect_dataset(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""inspect_dataset\n\n        Use this method inside of an algorithm\'s ``process()`` method\n        to view the available datasets in the redis cache\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: a dictionary of identifiers (for debugging) and\n        """"""\n        log.info(\'--------------\')\n        log.info(\n            f\'process(algo_id={algo_id}, ticker={ticker}, data:\')\n        for k in dataset:\n            log.info(f\'main keys={k}\')\n        for k in dataset[\'data\']:\n            if hasattr(dataset[\'data\'][k], \'to_json\'):\n                log.info(\n                    f\'data key={k} contains a pandas.DataFrame with \'\n                    f\'rows={len(dataset[""data""][k].index)}\')\n            else:\n                log.info(\n                    f\'data key={k} contains a pandas.DataFrame with \'\n                    \'rows=0\')\n    # end of inspect_dataset\n\n    def process(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""process\n\n        Derive custom algorithm buy and sell conditions\n        before placing orders. Just implement your own\n        ``process`` method.\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: a dictionary of identifiers (for debugging) and\n            multiple pandas ``pandas.DataFrame`` objects. Dictionary where keys\n            represent a label from one of the data sources (``IEX Cloud`` or\n            ``Tradier``). Here is the supported\n            dataset structure for the process method:\n\n            .. note:: There are no required keys for ``data``, the list\n                below is not hard-enforced by default. This is just\n                a reference for what is available with the v1 engine.\n\n            ::\n\n                dataset = {\n                    \'id\': <string TICKER_DATE - redis cache key>,\n                    \'date\': <string DATE>,\n                    \'data\': {\n                        \'daily\': pd.DataFrame([]),\n                        \'minute\': pd.DataFrame([]),\n                        \'quote\': pd.DataFrame([]),\n                        \'stats\': pd.DataFrame([]),\n                        \'peers\': pd.DataFrame([]),\n                        \'news1\': pd.DataFrame([]),\n                        \'financials\': pd.DataFrame([]),\n                        \'earnings\': pd.DataFrame([]),\n                        \'dividends\': pd.DataFrame([]),\n                        \'calls\': pd.DataFrame([]),\n                        \'puts\': pd.DataFrame([]),\n                        \'pricing\': pd.DataFrame([]),\n                        \'news\': pd.DataFrame([])\n                    }\n                }\n\n            example:\n\n            ::\n\n                dataset = {\n                    \'id\': \'SPY_2018-11-02\n                    \'date\': \'2018-11-02\',\n                    \'data\': {\n                        \'daily\': pd.DataFrame,\n                        \'minute\': pd.DataFrame,\n                        \'calls\': pd.DataFrame,\n                        \'puts\': pd.DataFrame,\n                        \'news\': pd.DataFrame\n                    }\n                }\n        """"""\n\n        use_date = self.trade_date\n        num_rows = len(self.df_daily.index)\n        if self.latest_min:\n            use_date = self.latest_min\n\n        if self.verbose or self.show_log:\n            log.info(\n                f\'process {algo_id} - {use_date} - \'\n                f\'{self.name} \'\n                f\'bal={self.balance} \'\n                f\'net={self.net_gain} \'\n                f\'owned={self.num_owned} \'\n                f\'high={self.latest_high} \'\n                f\'low={self.latest_low} \'\n                f\'open={self.latest_open} \'\n                f\'close={self.latest_close} \'\n                f\'vol={self.latest_volume} \'\n                f\'cur_buy={self.num_latest_buys} \'\n                f\'min_buy={self.min_buy_indicators} \'\n                f\'num_buys={self.num_buys} \'\n                f\'cur_sell={self.num_latest_sells} \'\n                f\'min_sell={self.min_sell_indicators} \'\n                f\'num_sells={self.num_sells} \'\n                f\'rows={num_rows}\')\n\n        # flip these on to sell/buy\n        # buys will not FILL if there\'s not enough funds to buy\n        # sells will not FILL if there\'s nothing already owned\n        self.should_sell = False\n        self.should_buy = False\n\n        """"""\n        Want to iterate over daily pricing data\n        to determine buys or sells from the:\n        self.df_daily dataset fetched from IEX?\n\n        # loop over the rows in the daily dataset:\n        for idx, row in self.df_daily.iterrows():\n            print(row)\n        """"""\n    # end of process\n\n    def trade_off_indicator_buy_and_sell_signals(\n            self,\n            ticker,\n            algo_id,\n            reason_for_buy=None,\n            reason_for_sell=None):\n        """"""trade_off_indicator_buy_and_sell_signals\n\n        Check if the minimum number of indicators\n        for a buy or a sell were found. If there\n        were, then commit the trade.\n\n        .. code-block:: python\n\n            if self.trade_off_num_indicators:\n                if self.num_latest_buys >= self.min_buy_indicators:\n                    self.should_buy = True\n                elif self.num_latest_sells >= self.min_sell_indicators:\n                    self.should_sell = True\n\n        :param ticker: ticker symbol\n        :param algo_id: string algo for tracking\n            internal progress for debugging\n        :param reason_for_buy: optional - string\n            for tracking why the algo bought\n        :param reason_for_sell: optional - string\n            for tracking why the algo sold\n        """"""\n\n        if self.trade_off_num_indicators:\n            if self.num_latest_buys >= self.min_buy_indicators:\n                self.should_buy = True\n            elif self.num_latest_sells >= self.min_sell_indicators:\n                self.should_sell = True\n\n        if self.num_owned and self.should_sell:\n            if self.verbose_trading or self.verbose:\n                log.critical(\n                    \'TRADE - SELLDECISION - \'\n                    f\'{algo_id} \'\n                    f\'trade_off_num={self.trade_off_num_indicators} \'\n                    f\'num_sells={self.num_latest_sells} > \'\n                    f\'min_sells={self.min_sell_indicators} \'\n                    f\'should_sell={self.should_sell}\')\n\n            self.create_sell_order(\n                ticker=ticker,\n                shares=self.num_owned,\n                minute=self.use_minute,\n                row={\n                    \'name\': algo_id,\n                    \'close\': self.latest_close,\n                    \'date\': self.trade_date\n                },\n                is_live_trading=self.is_live_trading,\n                reason=reason_for_buy)\n        # if own shares and should sell\n        # else if should buy:\n        elif self.should_buy:\n            if self.verbose_trading or self.verbose:\n                log.critical(\n                    \'TRADE - BUYDECISION - \'\n                    f\'{algo_id} \'\n                    f\'trade_off_num={self.trade_off_num_indicators} \'\n                    f\'num_buys={self.num_latest_buys} > \'\n                    f\'min_buys={self.min_buy_indicators} \'\n                    f\'should_buy={self.should_buy}\')\n            self.create_buy_order(\n                ticker=ticker,\n                shares=self.buy_shares,\n                minute=self.use_minute,\n                row={\n                    \'name\': algo_id,\n                    \'close\': self.latest_close,\n                    \'date\': self.trade_date\n                },\n                is_live_trading=self.is_live_trading,\n                reason=reason_for_sell)\n        # end of should_buy\n\n    # end of trade_off_indicator_buy_and_sell_signals\n\n    def load_from_external_source(\n            self,\n            path_to_file=None,\n            s3_bucket=None,\n            s3_key=None,\n            redis_key=None):\n        """"""load_from_external_source\n\n        Load an algorithm-ready dataset for ``handle_data`` backtesting\n        and trade performance analysis from:\n\n        - Local file\n        - S3\n        - Redis\n\n        :param path_to_file: optional - path to local file\n        :param s3_bucket: optional - s3 s3_bucket\n        :param s3_key: optional - s3 key\n        :param redis_key: optional - redis key\n        """"""\n\n        if path_to_file:\n            self.dsload_output_file = path_to_file\n        if s3_key:\n            self.dsload_s3_key = s3_key\n        if s3_bucket:\n            self.dsload_s3_bucket = s3_bucket\n            self.dsload_s3_enabled = True\n        if redis_key:\n            self.dsload_redis_key = redis_key\n            self.dsload_redis_enabled = True\n\n        if (self.dsload_s3_key and\n                self.dsload_s3_bucket and\n                self.dsload_s3_enabled and\n                not self.loaded_dataset):\n            self.debug_msg = (\n                f\'external load START - s3={self.dsload_s3_address}:\'\n                f\'{self.dsload_s3_bucket}/{self.dsload_s3_key}\')\n            if self.verbose:\n                log.info(self.debug_msg)\n            self.loaded_dataset = load_dataset.load_dataset(\n                s3_enabled=self.dsload_s3_enabled,\n                s3_address=self.dsload_s3_address,\n                s3_key=self.dsload_s3_key,\n                s3_bucket=self.dsload_s3_bucket,\n                s3_access_key=self.dsload_s3_access_key,\n                s3_secret_key=self.dsload_s3_secret_key,\n                s3_region_name=self.dsload_s3_region_name,\n                s3_secure=self.dsload_s3_secure,\n                compress=True,\n                encoding=self.dsload_redis_encoding)\n            if self.loaded_dataset:\n                self.debug_msg = (\n                    f\'external load SUCCESS - s3={self.dsload_s3_address}:\'\n                    f\'{self.dsload_s3_bucket}/{self.dsload_s3_key}\')\n            else:\n                self.debug_msg = (\n                    f\'external load FAILED - s3={self.dsload_s3_address}:\'\n                    f\'{self.dsload_s3_bucket}/{self.dsload_s3_key}\')\n                log.error(self.debug_msg)\n                raise Exception(self.debug_msg)\n        elif (self.dsload_redis_key and\n                self.dsload_redis_enabled and\n                not self.loaded_dataset):\n            self.debug_msg = (\n                f\'external load START - redis={self.dsload_redis_address}:\'\n                f\'{self.dsload_redis_db}/{self.dsload_redis_key}\')\n            log.debug(self.debug_msg)\n            self.loaded_dataset = load_dataset.load_dataset(\n                redis_enabled=self.dsload_redis_enabled,\n                redis_address=self.dsload_redis_address,\n                redis_key=self.dsload_redis_key,\n                redis_db=self.dsload_redis_db,\n                redis_password=self.dsload_redis_password,\n                redis_expire=self.dsload_redis_expire,\n                redis_serializer=self.dsload_redis_serializer,\n                redis_encoding=self.dsload_redis_encoding,\n                compress=self.dsload_compress,\n                encoding=self.dsload_redis_encoding)\n            if self.loaded_dataset:\n                self.debug_msg = (\n                    \'external load SUCCESS - \'\n                    f\'redis={self.dsload_redis_address}:\'\n                    f\'{self.dsload_redis_db}/{self.dsload_redis_key}\')\n            else:\n                self.debug_msg = (\n                    \'external load FAILED - \'\n                    f\'redis={self.dsload_redis_address}:\'\n                    f\'{self.dsload_redis_db}/{self.dsload_redis_key}\')\n                log.error(self.debug_msg)\n                raise Exception(self.debug_msg)\n        elif (self.dsload_output_file and\n                not self.loaded_dataset):\n            if os.path.exists(self.dsload_output_file):\n                self.debug_msg = (\n                    f\'external load START - file={self.dsload_output_file}\')\n                log.debug(self.debug_msg)\n                self.loaded_dataset = load_dataset.load_dataset(\n                    path_to_file=self.dsload_output_file,\n                    compress=self.dsload_compress,\n                    encoding=self.extract_redis_encoding)\n                if self.loaded_dataset:\n                    self.debug_msg = (\n                        \'external load SUCCESS - \'\n                        f\'file={self.dsload_output_file}\')\n                else:\n                    self.debug_msg = (\n                        \'external load FAILED - \'\n                        f\'file={self.dsload_output_file}\')\n                    log.error(self.debug_msg)\n                    raise Exception(self.debug_msg)\n            else:\n                self.debug_msg = (\n                    \'external load - did not find \'\n                    f\'file={self.dsload_output_file}\')\n                log.error(self.debug_msg)\n                raise Exception(self.debug_msg)\n        # end of if supported external loader\n        log.debug(\n            \'external load END\')\n    # end of load_from_external_source\n\n    def publish_report_dataset(\n            self,\n            **kwargs):\n        """"""publish_report_dataset\n\n        publish trade history datasets to caches (redis), archives\n        (minio s3), a local file (``output_file``) and slack\n\n        :param kwargs: keyword argument dictionary\n        :return: tuple: ``status``, ``output_file``\n        """"""\n\n        # parse optional input args\n        output_file = kwargs.get(\n            \'output_file\', None)\n        label = kwargs.get(\n            \'label\', self.name)\n        redis_enabled = kwargs.get(\n            \'redis_enabled\', False)\n        redis_address = kwargs.get(\n            \'redis_address\', self.report_redis_address)\n        redis_db = kwargs.get(\n            \'redis_db\', self.report_redis_db)\n        redis_password = kwargs.get(\n            \'redis_password\', self.report_redis_password)\n        redis_expire = kwargs.get(\n            \'redis_expire\', self.report_redis_expire)\n        redis_serializer = kwargs.get(\n            \'redis_serializer\', self.report_redis_serializer)\n        redis_encoding = kwargs.get(\n            \'redis_encoding\', self.report_redis_encoding)\n        s3_enabled = kwargs.get(\n            \'s3_enabled\', False)\n        s3_address = kwargs.get(\n            \'s3_address\', self.report_s3_address)\n        s3_bucket = kwargs.get(\n            \'s3_bucket\', self.report_s3_bucket)\n        s3_access_key = kwargs.get(\n            \'s3_access_key\', self.report_s3_access_key)\n        s3_secret_key = kwargs.get(\n            \'s3_secret_key\', self.report_s3_secret_key)\n        s3_region_name = kwargs.get(\n            \'s3_region_name\', self.report_s3_region_name)\n        s3_secure = kwargs.get(\n            \'s3_secure\', self.report_s3_secure)\n        slack_enabled = kwargs.get(\n            \'slack_enabled\', self.report_slack_enabled)\n        slack_code_block = kwargs.get(\n            \'slack_code_block\', self.report_slack_code_block)\n        slack_full_width = kwargs.get(\n            \'slack_full_width\', self.report_slack_full_width)\n        redis_key = kwargs.get(\n            \'redis_key\', self.report_redis_key)\n        s3_key = kwargs.get(\n            \'s3_key\', self.report_s3_key)\n        verbose = kwargs.get(\n            \'verbose\', self.report_verbose)\n\n        status = ae_consts.NOT_RUN\n\n        if not self.publish_report:\n            if self.verbose:\n                log.info(\n                    \'report publish - disabled - \'\n                    f\'{self.name} - tickers={self.tickers}\')\n            return status\n\n        output_record = self.create_report_dataset()\n\n        if output_file or s3_enabled or redis_enabled or slack_enabled:\n            if self.verbose:\n                log.info(\n                    f\'report build json - {self.name} - \'\n                    f\'tickers={self.tickers}\')\n\n            use_data = output_record\n            num_bytes = len(str(use_data))\n            num_mb = ae_consts.get_mb(num_bytes)\n            log.info(\n                \'report publish - START - \'\n                f\'{self.name} - tickers={self.tickers} \'\n                f\'file={output_file} size={num_mb}MB \'\n                f\'s3={s3_enabled} s3_key={s3_key} \'\n                f\'redis={redis_enabled} redis_key={redis_key} \'\n                f\'slack={slack_enabled}\')\n            publish_status = publish.publish(\n                data=use_data,\n                label=label,\n                df_compress=True,\n                compress=False,\n                convert_to_dict=False,\n                output_file=output_file,\n                redis_enabled=redis_enabled,\n                redis_key=redis_key,\n                redis_address=redis_address,\n                redis_db=redis_db,\n                redis_password=redis_password,\n                redis_expire=redis_expire,\n                redis_serializer=redis_serializer,\n                redis_encoding=redis_encoding,\n                s3_enabled=s3_enabled,\n                s3_key=s3_key,\n                s3_address=s3_address,\n                s3_bucket=s3_bucket,\n                s3_access_key=s3_access_key,\n                s3_secret_key=s3_secret_key,\n                s3_region_name=s3_region_name,\n                s3_secure=s3_secure,\n                slack_enabled=slack_enabled,\n                slack_code_block=slack_code_block,\n                slack_full_width=slack_full_width,\n                verbose=verbose)\n\n            status = publish_status\n\n            log.info(\n                \'report publish - END - \'\n                f\'{ae_consts.get_status(status=status)} \'\n                f\'{self.name} - tickers={self.tickers} \'\n                f\'file={output_file} s3={s3_key} \'\n                f\'redis={redis_key} size={num_mb}MB\')\n        else:\n            status = ae_consts.SUCCESS\n            if self.verbose:\n                log.info(\n                    f\'{self.name} - report not publishing for \'\n                    f\'output_file={output_file} s3_enabled={s3_enabled} \'\n                    f\'redis_enabled={redis_enabled} \'\n                    f\'slack_enabled={slack_enabled}\')\n        # end of handling for publish\n\n        return status\n    # end of publish_report_dataset\n\n    def create_report_dataset(\n            self):\n        """"""create_report_dataset\n\n        Create the ``Trading Performance Report`` dataset\n        during the ``self.publish_input_dataset()`` member method.\n        Inherited Algorithm classes can derive how they build a\n        custom ``Trading Performance Report`` dataset before publishing\n        by implementing this method in the derived class.\n        """"""\n\n        if self.verbose:\n            log.info(\'report - create start\')\n\n        if self.last_handle_data:\n            data_for_tickers = self.get_supported_tickers_in_data(\n                data=self.last_handle_data)\n        else:\n            data_for_tickers = self.tickers\n\n        num_tickers = len(data_for_tickers)\n        if num_tickers > 0:\n            self.debug_msg = (\n                f\'{self.name} handle - tickers={json.dumps(data_for_tickers)}\')\n\n        output_record = {}\n        for ticker in data_for_tickers:\n            if ticker not in output_record:\n                output_record[ticker] = []\n            num_ticker_datasets = len(self.last_handle_data[ticker])\n            cur_idx = 1\n            for idx, node in enumerate(self.last_handle_data[ticker]):\n                track_label = self.build_progress_label(\n                    progress=cur_idx,\n                    total=num_ticker_datasets)\n                algo_id = f\'{ticker} {track_label}\'\n                if self.verbose:\n                    log.info(\n                        f\'{self.name} report - {algo_id} - ds={node[""date""]}\')\n\n                new_node = {\n                    \'id\': node[\'id\'],\n                    \'date\': node[\'date\'],\n                    \'data\': {}\n                }\n\n                output_record[ticker].append(new_node)\n                cur_idx += 1\n            # end for all self.last_handle_data[ticker]\n        # end of converting dataset\n\n        return output_record\n    # end of create_report_dataset\n\n    def publish_trade_history_dataset(\n            self,\n            **kwargs):\n        """"""publish_trade_history_dataset\n\n        publish trade history datasets to caches (redis), archives\n        (minio s3), a local file (``output_file``) and slack\n\n        :param kwargs: keyword argument dictionary\n        :return: tuple: ``status``, ``output_file``\n        """"""\n\n        # parse optional input args\n        output_file = kwargs.get(\n            \'output_file\', None)\n        label = kwargs.get(\n            \'label\', self.name)\n        redis_enabled = kwargs.get(\n            \'redis_enabled\', False)\n        redis_address = kwargs.get(\n            \'redis_address\', self.history_redis_address)\n        redis_db = kwargs.get(\n            \'redis_db\', self.history_redis_db)\n        redis_password = kwargs.get(\n            \'redis_password\', self.history_redis_password)\n        redis_expire = kwargs.get(\n            \'redis_expire\', self.history_redis_expire)\n        redis_serializer = kwargs.get(\n            \'redis_serializer\', self.history_redis_serializer)\n        redis_encoding = kwargs.get(\n            \'redis_encoding\', self.history_redis_encoding)\n        s3_enabled = kwargs.get(\n            \'s3_enabled\', False)\n        s3_address = kwargs.get(\n            \'s3_address\', self.history_s3_address)\n        s3_bucket = kwargs.get(\n            \'s3_bucket\', self.history_s3_bucket)\n        s3_access_key = kwargs.get(\n            \'s3_access_key\', self.history_s3_access_key)\n        s3_secret_key = kwargs.get(\n            \'s3_secret_key\', self.history_s3_secret_key)\n        s3_region_name = kwargs.get(\n            \'s3_region_name\', self.history_s3_region_name)\n        s3_secure = kwargs.get(\n            \'s3_secure\', self.history_s3_secure)\n        slack_enabled = kwargs.get(\n            \'slack_enabled\', self.history_slack_enabled)\n        slack_code_block = kwargs.get(\n            \'slack_code_block\', self.history_slack_code_block)\n        slack_full_width = kwargs.get(\n            \'slack_full_width\', self.history_slack_full_width)\n        redis_key = kwargs.get(\n            \'redis_key\', self.history_redis_key)\n        s3_key = kwargs.get(\n            \'s3_key\', self.history_s3_key)\n        verbose = kwargs.get(\n            \'verbose\', self.history_verbose)\n        add_metrics_to_key = kwargs.get(\n            \'add_metrics_to_key\', False)\n\n        status = ae_consts.NOT_RUN\n\n        if not self.publish_history:\n            log.info(\n                \'history publish - disabled - \'\n                f\'{self.name} - tickers={self.tickers}\')\n            return status\n        # end of screening for returning early\n\n        output_record = self.create_history_dataset()\n\n        if output_file or s3_enabled or redis_enabled or slack_enabled:\n            if self.verbose:\n                log.info(\n                    f\'history build json - {self.name} - \'\n                    f\'tickers={self.tickers}\')\n\n            # for mass trade history publishing, make it\n            # easy to find the best-of runs\n            if add_metrics_to_key:\n                (self.num_owned,\n                 self.ticker_buys,\n                 self.ticker_sells) = self.get_ticker_positions(\n                    ticker=self.tickers[0])\n\n                status_str = \'NEGATIVE\'\n                if self.net_gain > 0:\n                    status_str = \'POSITIVE\'\n\n                now = datetime.datetime.utcnow()\n                seconds = ae_consts.to_f((\n                    now - self.created_on_date).total_seconds())\n\n                # https://stackoverflow.com/questions/6870824/\n                # what-is-the-maximum-length-of-a-filename-in-s3\n                # 1024 characters\n                s3_key = (\n                    f\'{ae_consts.to_f(self.net_gain)}_netgain_\'\n                    f\'{ae_consts.to_f(self.net_value)}_netvalue_\'\n                    f\'{status_str}_\'\n                    f\'{ae_consts.to_f(self.starting_balance)}_startbalance_\'\n                    f\'{ae_consts.to_f(self.balance)}_endbalance_\'\n                    f\'{self.num_owned}_shares_\'\n                    f\'{ae_consts.to_f(self.latest_close)}_close_\'\n                    f\'{self.num_buys}_buys_{self.num_sells}_sells_\'\n                    f\'{self.min_buy_indicators}_minbuyinds_\'\n                    f\'{self.min_sell_indicators}_minsellinds_\'\n                    f\'{seconds}_seconds_{s3_key}\')[0:1023]\n            # end of if add metrics to key\n\n            use_data = output_record\n            num_bytes = len(str(use_data))\n            num_mb = ae_consts.get_mb(num_bytes)\n            log.info(\n                \'history publish - START - \'\n                f\'{self.name} - ticker={self.tickers[0]} \'\n                f\'file={output_file} size={num_mb}MB \'\n                f\'s3={s3_address}/{s3_bucket} s3_key={s3_key} \'\n                f\'redis={redis_enabled} redis_key={redis_key} \'\n                f\'slack={slack_enabled}\')\n            publish_status = publish.publish(\n                data=use_data,\n                label=label,\n                df_compress=True,\n                compress=False,\n                convert_to_dict=False,\n                output_file=output_file,\n                redis_enabled=redis_enabled,\n                redis_key=redis_key,\n                redis_address=redis_address,\n                redis_db=redis_db,\n                redis_password=redis_password,\n                redis_expire=redis_expire,\n                redis_serializer=redis_serializer,\n                redis_encoding=redis_encoding,\n                s3_enabled=s3_enabled,\n                s3_key=s3_key,\n                s3_address=s3_address,\n                s3_bucket=s3_bucket,\n                s3_access_key=s3_access_key,\n                s3_secret_key=s3_secret_key,\n                s3_region_name=s3_region_name,\n                s3_secure=s3_secure,\n                slack_enabled=slack_enabled,\n                slack_code_block=slack_code_block,\n                slack_full_width=slack_full_width,\n                verbose=verbose)\n\n            status = publish_status\n\n            log.info(\n                \'history publish - END - \'\n                f\'{ae_consts.get_status(status=status)} \'\n                f\'{self.name} - tickers={self.tickers} \'\n                f\'file={output_file} s3={s3_key} redis={redis_key} \'\n                f\'size={num_mb}MB\')\n        else:\n            status = ae_consts.SUCCESS\n            if self.verbose:\n                log.info(\n                    f\'{self.name} - history not publishing for \'\n                    f\'output_file={output_file} s3_enabled={s3_enabled} \'\n                    f\'redis_enabled={redis_enabled} \'\n                    f\'slack_enabled={slack_enabled}\')\n        # end of handling for publish\n\n        return status\n    # end of publish_trade_history_dataset\n\n    def create_history_dataset(\n            self):\n        """"""create_history_dataset\n\n        Create the ``Trading History`` dataset\n        during the ``self.publish_trade_history_dataset()`` member method.\n        Inherited Algorithm classes can derive how they build a\n        custom ``Trading History`` dataset before publishing\n        by implementing this method in the derived class.\n        """"""\n\n        if self.verbose:\n            log.info(\'history - create start\')\n\n        if self.last_handle_data:\n            data_for_tickers = self.get_supported_tickers_in_data(\n                data=self.last_handle_data)\n        else:\n            data_for_tickers = self.tickers\n\n        num_tickers = len(data_for_tickers)\n        if num_tickers > 0:\n            self.debug_msg = (\n                f\'{self.name} handle - tickers={json.dumps(data_for_tickers)}\')\n\n        history_by_ticker = {}\n        for ticker in data_for_tickers:\n            ticker_history_rec_list = self.build_ticker_history(\n                ticker=ticker,\n                ignore_keys=self.ignore_history_keys)\n            history_by_ticker[ticker] = ticker_history_rec_list\n        # end for all tickers to filter\n\n        output_record = {\n            \'tickers\': data_for_tickers,\n            \'version\': int(ae_consts.ALGO_HISTORY_VERSION),\n            \'last_trade_date\': ae_utils.get_last_close_str(),\n            \'algo_config_dict\': self.config_dict,\n            \'algo_name\':  self.name,\n            \'created\': ae_utils.utc_now_str()\n        }\n        for ticker in data_for_tickers:\n            if ticker not in output_record:\n                output_record[ticker] = []\n            num_ticker_datasets = len(history_by_ticker[ticker])\n            cur_idx = 1\n            for idx, node in enumerate(history_by_ticker[ticker]):\n                track_label = self.build_progress_label(\n                    progress=cur_idx,\n                    total=num_ticker_datasets)\n                algo_id = f\'{ticker} {track_label}\'\n                if self.verbose:\n                    log.info(\n                        f\'\'\'{self.name} history - {algo_id} - ds={node.get(\n                            \'minute\',\n                            node.get(\n                                \'date\',\n                                \'no-date-set\'))}\'\'\')\n\n                output_record[ticker].append(node)\n                cur_idx += 1\n            # end for all self.last_handle_data[ticker]\n        # end of converting dataset\n\n        return output_record\n    # end of create_history_dataset\n\n    def build_ticker_history(\n            self,\n            ticker,\n            ignore_keys):\n        """"""build_ticker_history\n\n        For all records in ``self.order_history`` compile\n        a filter list of history records per ``ticker`` while\n        pruning any keys that are in the list of ``ignore_keys``\n\n        :param ticker: string ticker symbol\n        :param ignore_history_keys: list of\n            keys to not include in the\n            history report\n        """"""\n        history_for_ticker = []\n\n        for org_node in self.order_history:\n            status = org_node.get(\'status\', ae_consts.INVALID)\n            if status != ae_consts.INVALID:\n                # trade history dictionaries\n                # will be permanently changed\n                # unless an expensive deep copy is\n                # used by a derived class with:\n                # node = copy.deepcopy(org_node)\n                node = org_node\n                is_valid = True\n                for i in ignore_keys:\n                    node.pop(i, None)\n                if is_valid:\n                    history_for_ticker.append(node)\n        # end of all order history records\n\n        return history_for_ticker\n    # end of build_ticker_history\n\n    def publish_input_dataset(\n            self,\n            **kwargs):\n        """"""publish_input_dataset\n\n        publish input datasets to caches (redis), archives\n        (minio s3), a local file (``output_file``) and slack\n\n        :param kwargs: keyword argument dictionary\n        :return: tuple: ``status``, ``output_file``\n        """"""\n\n        # parse optional input args\n        output_file = kwargs.get(\n            \'output_file\', None)\n        label = kwargs.get(\n            \'label\', self.name)\n        redis_enabled = kwargs.get(\n            \'redis_enabled\', False)\n        redis_address = kwargs.get(\n            \'redis_address\', self.extract_redis_address)\n        redis_db = kwargs.get(\n            \'redis_db\', self.extract_redis_db)\n        redis_password = kwargs.get(\n            \'redis_password\', self.extract_redis_password)\n        redis_expire = kwargs.get(\n            \'redis_expire\', self.extract_redis_expire)\n        redis_serializer = kwargs.get(\n            \'redis_serializer\', self.extract_redis_serializer)\n        redis_encoding = kwargs.get(\n            \'redis_encoding\', self.extract_redis_encoding)\n        s3_enabled = kwargs.get(\n            \'s3_enabled\', False)\n        s3_address = kwargs.get(\n            \'s3_address\', self.extract_s3_address)\n        s3_bucket = kwargs.get(\n            \'s3_bucket\', self.extract_s3_bucket)\n        s3_access_key = kwargs.get(\n            \'s3_access_key\', self.extract_s3_access_key)\n        s3_secret_key = kwargs.get(\n            \'s3_secret_key\', self.extract_s3_secret_key)\n        s3_region_name = kwargs.get(\n            \'s3_region_name\', self.extract_s3_region_name)\n        s3_secure = kwargs.get(\n            \'s3_secure\', self.extract_s3_secure)\n        slack_enabled = kwargs.get(\n            \'slack_enabled\', self.extract_slack_enabled)\n        slack_code_block = kwargs.get(\n            \'slack_code_block\', self.extract_slack_code_block)\n        slack_full_width = kwargs.get(\n            \'slack_full_width\', self.extract_slack_full_width)\n        redis_key = kwargs.get(\n            \'redis_key\', self.extract_redis_key)\n        s3_key = kwargs.get(\n            \'s3_key\', self.extract_s3_key)\n        verbose = kwargs.get(\n            \'verbose\', self.extract_verbose)\n\n        status = ae_consts.NOT_RUN\n\n        if not self.publish_input:\n            log.info(\n                \'input publish - disabled - \'\n                f\'{self.name} - tickers={self.tickers}\')\n            return status\n\n        output_record = self.create_algorithm_ready_dataset()\n\n        if output_file or s3_enabled or redis_enabled or slack_enabled:\n            if self.verbose:\n                log.info(\n                    f\'input build json - {self.name} - tickers={self.tickers}\')\n\n            use_data = output_record\n            num_bytes = len(str(use_data))\n            num_mb = ae_consts.get_mb(num_bytes)\n            log.info(\n                f\'input publish - START - {self.name} - \'\n                f\'tickers={self.tickers} file={output_file} size={num_mb}MB \'\n                f\'s3={s3_enabled} s3_key={s3_key} redis={redis_enabled} \'\n                f\'redis_key={redis_key} slack={slack_enabled}\')\n            publish_status = publish.publish(\n                data=use_data,\n                label=label,\n                df_compress=True,\n                compress=False,\n                convert_to_dict=False,\n                output_file=output_file,\n                redis_enabled=redis_enabled,\n                redis_key=redis_key,\n                redis_address=redis_address,\n                redis_db=redis_db,\n                redis_password=redis_password,\n                redis_expire=redis_expire,\n                redis_serializer=redis_serializer,\n                redis_encoding=redis_encoding,\n                s3_enabled=s3_enabled,\n                s3_key=s3_key,\n                s3_address=s3_address,\n                s3_bucket=s3_bucket,\n                s3_access_key=s3_access_key,\n                s3_secret_key=s3_secret_key,\n                s3_region_name=s3_region_name,\n                s3_secure=s3_secure,\n                slack_enabled=slack_enabled,\n                slack_code_block=slack_code_block,\n                slack_full_width=slack_full_width,\n                verbose=verbose)\n\n            status = publish_status\n\n            log.info(\n                f\'input publish - END - {ae_consts.get_status(status=status)} \'\n                f\'{self.name} - tickers={self.tickers} file={output_file} \'\n                f\'s3={s3_key} redis={redis_key} size={num_mb}MB\')\n        else:\n            status = ae_consts.SUCCESS\n            if self.verbose:\n                log.info(\n                    f\'{self.name} - input not publishing for \'\n                    f\'output_file={output_file} s3_enabled={s3_enabled} \'\n                    f\'redis_enabled={redis_enabled} \'\n                    f\'slack_enabled={slack_enabled}\')\n        # end of handling for publish\n\n        return status\n    # end of publish_input_dataset\n\n    def create_algorithm_ready_dataset(\n            self):\n        """"""create_algorithm_ready_dataset\n\n        Create the ``Algorithm-Ready`` dataset\n        during the ``self.publish_input_dataset()`` member method.\n        Inherited Algorithm classes can derive how they build a\n        custom ``Algorithm-Ready`` dataset before publishing\n        by implementing this method in the derived class.\n        """"""\n\n        if self.verbose:\n            log.info(\'algo-ready - create start\')\n\n        data_for_tickers = self.get_supported_tickers_in_data(\n            data=self.last_handle_data)\n\n        num_tickers = len(data_for_tickers)\n        if num_tickers > 0:\n            self.debug_msg = (\n                f\'{self.name} handle - tickers={json.dumps(data_for_tickers)}\')\n\n        output_record = {}\n        for ticker in data_for_tickers:\n            if ticker not in output_record:\n                output_record[ticker] = []\n            num_ticker_datasets = len(self.last_handle_data[ticker])\n            cur_idx = 1\n            for idx, node in enumerate(self.last_handle_data[ticker]):\n                track_label = self.build_progress_label(\n                    progress=cur_idx,\n                    total=num_ticker_datasets)\n                algo_id = f\'{ticker} {track_label}\'\n                if self.verbose:\n                    log.info(\n                        f\'{self.name} convert - {algo_id} - ds={node[""date""]}\')\n\n                new_node = {\n                    \'id\': node[\'id\'],\n                    \'date\': node[\'date\'],\n                    \'data\': {}\n                }\n\n                # parse the dataset node and set member variables\n                self.debug_msg = (\n                    f\'{ticker} START - convert load dataset \'\n                    f\'id={node.get(""id"", ""missing-id"")}\')\n                self.load_from_dataset(\n                    ds_data=node)\n                for ds_key in node[\'data\']:\n                    empty_ds = self.empty_pd_str\n                    data_val = node[\'data\'][ds_key]\n                    if ds_key not in new_node[\'data\']:\n                        new_node[\'data\'][ds_key] = empty_ds\n                    self.debug_msg = (\n                        f\'convert node={node} ds_key={ds_key}\')\n                    if hasattr(data_val, \'to_json\'):\n                        new_node[\'data\'][ds_key] = data_val.to_json(\n                            orient=\'records\',\n                            date_format=\'iso\')\n                    else:\n                        if not data_val:\n                            new_node[\'data\'][ds_key] = empty_ds\n                        else:\n                            new_node[\'data\'][ds_key] = json.dumps(\n                                data_val)\n                    # if/else\n                # for all dataset values in data\n                self.debug_msg = (\n                    f\'{ticker} END - convert load dataset \'\n                    f\'id={node.get(""id"", ""missing-id"")}\')\n\n                output_record[ticker].append(new_node)\n                cur_idx += 1\n            # end for all self.last_handle_data[ticker]\n        # end of converting dataset\n\n        return output_record\n    # end of create_algorithm_ready_dataset\n\n    def get_ticker_positions(\n            self,\n            ticker):\n        """"""get_ticker_positions\n\n        get the current positions for a ticker and\n        returns a tuple:\n        ``num_owned (integer), buys (list), sells (list)```\n\n        .. code-block:: python\n\n            num_owned, buys, sells = self.get_ticker_positions(\n                ticker=ticker)\n\n        :param ticker: ticker to lookup\n        """"""\n        buys = None\n        sells = None\n        num_owned = None\n        self.num_buys = 0\n        self.num_sells = 0\n        if ticker in self.positions:\n            num_owned = self.positions[ticker].get(\n                \'shares\',\n                None)\n            buys = self.positions[ticker].get(\n                \'buys\',\n                [])\n            sells = self.positions[ticker].get(\n                \'sells\',\n                [])\n            self.num_buys = len(buys)\n            self.num_sells = len(sells)\n        # if own the ticker\n\n        self.net_value = ae_consts.to_f(self.balance)\n        if self.latest_close and num_owned:\n            self.net_value = ae_consts.to_f(\n                self.balance + (\n                    num_owned * self.latest_close))\n\n        self.net_gain = ae_consts.to_f(\n            self.net_value - self.starting_balance)\n\n        return num_owned, buys, sells\n    # end of get_ticker_positions\n\n    def get_trade_history_node(\n                self):\n        """"""get_trade_history_node\n\n            Helper for quickly building a history node\n            on a derived algorithm. Whatever member variables\n            are in the base class ``analysis_engine.algo.BaseAlgo``\n            will be added automatically into the returned:\n            ``historical transaction dictionary``\n\n            .. tip:: if you get a ``None`` back it means there\n                could be a bug in how you are using the member\n                variables (likely created an invalid math\n                calculation) or could be a bug in the helper:\n                `build_trade_history_entry <https://\n                github.com/AlgoTraders/stock-analysis-engine/blob/ma\n                ster/analysis_engine/build_trade_history_entry.py>`__\n        """"""\n        history_dict = history_utils.build_trade_history_entry(\n            ticker=self.ticker,\n            algo_start_price=self.starting_close,\n            original_balance=self.starting_balance,\n            num_owned=self.num_owned,\n            close=self.trade_price,\n            balance=self.balance,\n            commission=self.commission,\n            date=self.trade_date,\n            minute=self.use_minute,\n            trade_type=self.trade_type,\n            high=self.latest_high,\n            low=self.latest_low,\n            open_val=self.latest_open,\n            volume=self.latest_volume,\n            today_high=self.today_high,\n            today_low=self.today_low,\n            today_open_val=self.today_open,\n            today_close=self.today_close,\n            today_volume=self.today_volume,\n            ask=self.ask,\n            bid=self.bid,\n            stop_loss=self.stop_loss,\n            trailing_stop_loss=self.trailing_stop_loss,\n            buy_hold_units=self.buy_hold_units,\n            sell_hold_units=self.sell_hold_units,\n            spread_exp_date=self.spread_exp_date,\n            prev_balance=self.prev_bal,\n            prev_num_owned=self.prev_num_owned,\n            total_buys=self.num_buys,\n            total_sells=self.num_sells,\n            buy_triggered=self.should_buy,\n            buy_strength=self.buy_strength,\n            buy_risk=self.buy_risk,\n            sell_triggered=self.should_sell,\n            sell_strength=self.sell_strength,\n            sell_risk=self.sell_risk,\n            num_indicators_buy=self.num_latest_buys,\n            num_indicators_sell=self.num_latest_sells,\n            min_buy_indicators=self.min_buy_indicators,\n            min_sell_indicators=self.min_sell_indicators,\n            net_gain=self.net_gain,\n            net_value=self.net_value,\n            note=self.note,\n            ds_id=self.ds_id,\n            version=self.version)\n        return history_dict\n    # end of get_trade_history_node\n\n    def load_from_config(\n            self,\n            config_dict):\n        """"""load_from_config\n\n        support for replaying algorithms from a trading history\n\n        :param config_dict: algorithm configuration values\n            usually from a previous trading history or for\n            quickly testing dataset theories in a development\n            environment\n        """"""\n        if config_dict:\n            if not self.verbose:\n                self.verbose = config_dict.get(\'verbose\', False)\n            if self.verbose:\n                for k in config_dict:\n                    log.debug(\n                        f\'setting algo member={k} to \'\n                        f\'config value={config_dict[k]}\')\n            # end of logging all config keys\n            for k in config_dict:\n                self.__dict__[k] = config_dict[k]\n            # end of assigning config keys to member variables\n        # end if config dict set\n    # end of load_from_config\n\n    def get_name(self):\n        """"""get_name""""""\n        return self.name\n    # end of get_name\n\n    def get_result(self):\n        """"""get_result""""""\n\n        self.debug_msg = (\n            \'building results\')\n        finished_date = ae_utils.utc_now_str()\n        self.result = {\n            \'name\': self.name,\n            \'created\': self.created_date,\n            \'updated\': finished_date,\n            \'open_positions\': self.positions,\n            \'buys\': self.get_buys(),\n            \'sells\': self.get_sells(),\n            \'num_processed\': len(self.order_history),\n            \'history\': self.order_history,\n            \'balance\': self.balance,\n            \'commission\': self.commission\n        }\n\n        return self.result\n    # end of get_result\n\n    def get_debug_msg(\n            self):\n        """"""get_debug_msg\n\n        debug algorithms that failed\n        by viewing the last ``self.debug_msg`` they\n        set\n        """"""\n        return self.debug_msg\n    # end of get_debug_msg\n\n    def get_tickers(\n            self):\n        """"""get_tickers""""""\n        return self.tickers\n    # end of get_tickers\n\n    def get_balance(\n            self):\n        """"""get_balance""""""\n        return self.balance\n    # end of get_balance\n\n    def get_commission(\n            self):\n        """"""get_commission""""""\n        return self.commission\n    # end of get_commission\n\n    def get_buys(\n            self):\n        """"""get_buys""""""\n        return self.buys\n    # end of get_buys\n\n    def get_sells(\n            self):\n        """"""get_sells""""""\n        return self.sells\n    # end of get_sells\n\n    def get_history_dataset(\n            self):\n        """"""get_history_dataset""""""\n        return prepare_history.prepare_history_dataset(\n            data=self.create_history_dataset(),\n            convert_to_dict=False)\n    # end of get_history_dataset\n\n    def get_report_dataset(\n            self):\n        """"""get_report_dataset""""""\n        return prepare_report.prepare_report_dataset(\n            data=self.create_report_dataset(),\n            convert_to_dict=False)\n    # end of get_report_dataset\n\n    def get_owned_shares(\n            self,\n            ticker):\n        """"""get_owned_shares\n\n        :param ticker: ticker to lookup\n        """"""\n        num_owned = 0\n        if ticker in self.positions:\n            num_owned = self.positions[ticker].get(\n                \'shares\',\n                None)\n        return num_owned\n    # end of get_owned_shares\n\n    def create_buy_order(\n            self,\n            ticker,\n            row,\n            minute=None,\n            shares=None,\n            reason=None,\n            orient=\'records\',\n            date_format=\'iso\',\n            is_live_trading=False):\n        """"""create_buy_order\n\n        create a buy order at the close or ask price\n\n        .. note:: setting the ``minute`` is required to build\n            a minute-by-minute ``Trading History``\n\n        :param ticker: string ticker\n        :param shares: optional - integer number of shares to buy\n            if None buy max number of shares at the ``close`` with the\n            available ``balance`` amount.\n        :param row: ``dictionary`` or ``pandas.DataFrame``\n            row record that will be converted to a\n            json-serialized string\n        :param minute: optional - string datetime when the order\n            minute the order was placed. For ``day`` timeseries\n            this is the close of trading (16:00:00 for the day)\n            and for ``minute`` timeseries the value will be the\n            latest minute from the ``self.df_minute``\n            ``pandas.DataFrame``. Normally this value should be\n            set to the ``self.use_minute``, and the format is\n            ``ae_consts.COMMON_TICK_DATE_FORMAT``\n        :param reason: optional - reason for creating the order\n            which is useful for troubleshooting order histories\n        :param orient: optional - pandas orient for ``row.to_json()``\n        :param date_format: optional - pandas date_format\n            parameter for ``row.to_json()``\n        :param is_live_trading: optional - bool for filling trades\n            for live trading or for backtest tuning filled\n            (default ``False`` which is backtest mode)\n        """"""\n        close = row[\'close\']\n        required_amount_for_a_buy = close + self.commission\n        if required_amount_for_a_buy > self.balance:\n            if self.verbose_trading:\n                log.info(\n                    f\'{self.name} - buy - not enough funds={self.balance} < \'\n                    f\'required={required_amount_for_a_buy} with \'\n                    f\'shares={self.num_owned}\')\n            return\n\n        dataset_date = row[\'date\']\n        use_date = dataset_date\n        if minute:\n            use_date = minute\n\n        if self.verbose_trading:\n            log.info(\n                f\'{self.name} - buy start {use_date} {ticker}@{close} - \'\n                f\'shares={shares}\')\n\n        new_buy = None\n\n        order_details = row\n        if hasattr(row, \'to_json\'):\n            order_details = row.to_json(\n                orient=orient,\n                date_format=date_format),\n\n        try:\n            num_owned = self.get_owned_shares(\n                ticker=ticker)\n            new_buy = buy_utils.build_buy_order(\n                ticker=ticker,\n                close=close,\n                num_owned=num_owned,\n                shares=shares,\n                balance=self.balance,\n                commission=self.commission,\n                date=dataset_date,\n                minute=minute,\n                use_key=f\'{ticker}_{dataset_date}\',\n                details=order_details,\n                is_live_trading=is_live_trading,\n                reason=reason)\n\n            prev_shares = num_owned\n            if not prev_shares:\n                prev_shares = 0\n            prev_bal = ae_consts.to_f(self.balance)\n            if new_buy[\'status\'] == ae_consts.TRADE_FILLED:\n                if ticker in self.positions:\n                    self.positions[ticker][\'shares\'] = int(\n                        new_buy[\'shares\'])\n                    self.positions[ticker][\'buys\'].append(\n                        new_buy)\n                    (self.num_owned,\n                     self.ticker_buys,\n                     self.ticker_sells) = self.get_ticker_positions(\n                        ticker=ticker)\n                    self.created_buy = True\n                else:\n                    self.positions[ticker] = {\n                        \'shares\': new_buy[\'shares\'],\n                        \'buys\': [\n                            new_buy\n                        ],\n                        \'sells\': []\n                    }\n                self.balance = new_buy[\'balance\']\n                if self.verbose_trading:\n                    log.info(\n                        f\'{self.name} - buy end {use_date} {ticker}@{close} \'\n                        f\'{ae_consts.get_status(status=new_buy[""status""])} \'\n                        f\'shares={new_buy[""shares""]} \'\n                        f\'cost={new_buy[""buy_price""]} bal={self.balance} \'\n                        f\'prev_shares={prev_shares} prev_bal={prev_bal}\')\n            else:\n                if self.verbose_trading:\n                    log.info(\n                        f\'{self.name} - buy fail {use_date} {ticker}@{close} \'\n                        f\'{ae_consts.get_status(status=new_buy[""status""])} \'\n                        f\'shares={num_owned} cost={new_buy[""buy_price""]} \'\n                        f\'bal={self.balance} \')\n            # end of if trade worked or not\n\n            # update the buys\n            self.buys.append(new_buy)\n\n            (self.num_owned,\n             self.ticker_buys,\n             self.ticker_sells) = self.get_ticker_positions(\n                ticker=ticker)\n\n            # record the ticker\'s event if it\'s a minute timeseries\n            if minute:\n                self.last_history_dict = self.get_trade_history_node()\n                if self.latest_ind_report:\n                    for k in self.latest_ind_report:\n                        if k not in self.ind_conf_ignore_keys:\n                            self.last_history_dict[k] = (\n                                self.latest_ind_report[k])\n        except Exception as e:\n            self.debug_msg = (\n                f\'{self.name} - buy {ticker}@{close} - FAILED with ex={e}\')\n            log.error(self.debug_msg)\n            if self.raise_on_err:\n                raise e\n        # end of try/ex\n\n    # end of create_buy_order\n\n    def create_sell_order(\n            self,\n            ticker,\n            row,\n            minute=None,\n            shares=None,\n            reason=None,\n            orient=\'records\',\n            date_format=\'iso\',\n            is_live_trading=False):\n        """"""create_sell_order\n\n        create a sell order at the close or ask price\n\n        .. note:: setting the ``minute`` is required to build\n            a minute-by-minute ``Trading History``\n\n        :param ticker: string ticker\n        :param shares: optional - integer number of shares to sell\n            if None sell all owned shares at the ``close``\n        :param row: ``pandas.DataFrame`` row record that will\n            be converted to a json-serialized string\n        :param minute: optional - string datetime when the order\n            minute the order was placed. For ``day`` timeseries\n            this is the close of trading (16:00:00 for the day)\n            and for ``minute`` timeseries the value will be the\n            latest minute from the ``self.df_minute``\n            ``pandas.DataFrame``. Normally this value should be\n            set to the ``self.use_minute``, and the format is\n            ``ae_consts.COMMON_TICK_DATE_FORMAT``\n        :param reason: optional - reason for creating the order\n            which is useful for troubleshooting order histories\n        :param orient: optional - pandas orient for ``row.to_json()``\n        :param date_format: optional - pandas date_format\n            parameter for ``row.to_json()``\n        :param is_live_trading: optional - bool for filling trades\n            for live trading or for backtest tuning filled\n            (default ``False`` which is backtest mode)\n        """"""\n        close = row[\'close\']\n        required_amount_for_a_sell = self.commission\n        if required_amount_for_a_sell > self.balance:\n            if self.verbose_trading:\n                log.info(\n                    f\'{self.name} - sell - not enough funds={self.balance} < \'\n                    f\'required={required_amount_for_a_sell} with \'\n                    f\'shareds={self.num_owned}\')\n            return\n\n        dataset_date = row[\'date\']\n        use_date = dataset_date\n        if minute:\n            use_date = minute\n\n        if self.verbose_trading:\n            log.info(\n                f\'{self.name} - sell start {use_date} {ticker}@{close}\')\n\n        new_sell = None\n        order_details = row\n        if hasattr(row, \'to_json\'):\n            order_details = row.to_json(\n                orient=orient,\n                date_format=date_format),\n\n        try:\n            num_owned = self.get_owned_shares(\n                ticker=ticker)\n            new_sell = sell_utils.build_sell_order(\n                ticker=ticker,\n                close=close,\n                num_owned=num_owned,\n                shares=shares,\n                balance=self.balance,\n                commission=self.commission,\n                date=dataset_date,\n                minute=minute,\n                use_key=f\'{ticker}_{dataset_date}\',\n                details=order_details,\n                is_live_trading=is_live_trading,\n                reason=reason)\n\n            prev_shares = num_owned\n            if not prev_shares:\n                prev_shares = 0\n            prev_bal = ae_consts.to_f(self.balance)\n            if new_sell[\'status\'] == ae_consts.TRADE_FILLED:\n                if ticker in self.positions:\n                    self.positions[ticker][\'shares\'] = int(\n                        new_sell[\'shares\'])\n                    self.positions[ticker][\'sells\'].append(\n                        new_sell)\n                    (self.num_owned,\n                     self.ticker_buys,\n                     self.ticker_sells) = self.get_ticker_positions(\n                        ticker=ticker)\n                    self.created_sell = True\n                else:\n                    self.positions[ticker] = {\n                        \'shares\': new_sell[\'shares\'],\n                        \'buys\': [],\n                        \'sells\': [\n                            new_sell\n                        ]\n                    }\n                self.balance = new_sell[\'balance\']\n                if self.verbose_trading:\n                    log.info(\n                        f\'{self.name} - sell end {use_date} {ticker}@{close} \'\n                        f\'{ae_consts.get_status(status=new_sell[""status""])} \'\n                        f\'shares={num_owned} cost={new_sell[""sell_price""]} \'\n                        f\'bal={self.balance} prev_shares={prev_shares} \'\n                        f\'prev_bal={prev_bal}\')\n            else:\n                if self.verbose_trading:\n                    log.info(\n                        f\'{self.name} - sell fail {use_date} {ticker}@{close} \'\n                        f\'{ae_consts.get_status(status=new_sell[""status""])} \'\n                        f\'shares={num_owned} cost={new_sell[""sell_price""]} \'\n                        f\'bal={self.balance}\')\n            # end of if trade worked or not\n\n            # update the sells\n            self.sells.append(new_sell)\n\n            (self.num_owned,\n             self.ticker_buys,\n             self.ticker_sells) = self.get_ticker_positions(\n                ticker=ticker)\n\n            # record the ticker\'s event if it\'s a minute timeseries\n            if minute:\n                self.last_history_dict = self.get_trade_history_node()\n                if self.latest_ind_report:\n                    for k in self.latest_ind_report:\n                        if k not in self.ind_conf_ignore_keys:\n                            self.last_history_dict[k] = (\n                                self.latest_ind_report[k])\n        except Exception as e:\n            self.debug_msg = (\n                f\'{self.name} - sell {ticker}@{close} - FAILED with ex={e}\')\n            log.error(self.debug_msg)\n            if self.raise_on_err:\n                raise e\n        # end of try/ex\n\n    # end of create_sell_order\n\n    def build_progress_label(\n            self,\n            progress,\n            total):\n        """"""build_progress_label\n\n        create a progress label string for the logs\n\n        :param progress: progress counter\n        :param total: total number of counts\n        """"""\n        percent_done = ae_consts.get_percent_done(\n            progress=progress,\n            total=total)\n        progress_label = f\'{percent_done} {progress}/{total}\'\n        return progress_label\n    # end of build_progress_label\n\n    def get_supported_tickers_in_data(\n            self,\n            data):\n        """"""get_supported_tickers_in_data\n\n        For all updates found in ``data`` compare to the\n        supported list of ``self.tickers`` to make sure\n        the updates are relevant for this algorithm.\n\n        :param data: new data stream to process in this\n            algo\n        """"""\n        data_for_tickers = []\n        for ticker in self.tickers:\n            if ticker in data:\n                data_for_tickers.append(\n                    ticker)\n        # end of finding tickers for this algo\n\n        return data_for_tickers\n    # end of get_supported_tickers_in_data\n\n    def load_from_dataset(\n            self,\n            ds_data):\n        """"""load_from_dataset\n\n        Load the member variables from the extracted\n        ``ds_data`` dataset.\n\n        algorithms automatically provide the following\n        member variables to  ``myalgo.process()`` for\n        quickly building algorithms:\n\n        - ``self.df_daily``\n        - ``self.df_minute``\n        - ``self.df_calls``\n        - ``self.df_puts``\n        - ``self.df_quote``\n        - ``self.df_pricing``\n        - ``self.df_stats``\n        - ``self.df_peers``\n        - ``self.df_iex_news``\n        - ``self.df_financials``\n        - ``self.df_earnings``\n        - ``self.df_dividends``\n        - ``self.df_company``\n        - ``self.df_yahoo_news``\n        - ``self.df_tdcalls``\n        - ``self.df_tdputs``\n\n        .. note:: If a key is not in the dataset, the\n            algorithms\'s member variable will be an empty\n            ``pandas.DataFrame([])``. Please ensure the engine\n            cached the dataset in redis using a tool like\n            ``redis-cli`` to verify the values are in\n            memory.\n\n        :param ds_data: extracted, structured\n            dataset from redis\n        """"""\n\n        # back up for debugging/tracking/comparing\n        self.last_ds_id = self.ds_id\n        self.last_ds_date = self.ds_date\n        self.last_ds_data = self.ds_data\n\n        # load the new one\n        self.ds_data = ds_data\n\n        self.ds_id = self.ds_data.get(\n            \'id\',\n            \'missing-ID\')\n        self.ds_date = self.ds_data.get(\n            \'date\',\n            \'missing-DATE\')\n        self.ds_data = self.ds_data.get(\n            \'data\',\n            \'missing-DATA\')\n        self.df_daily = self.ds_data.get(\n            \'daily\',\n            self.empty_pd)\n        self.df_minute = self.ds_data.get(\n            \'minute\',\n            self.empty_pd)\n        self.df_stats = self.ds_data.get(\n            \'stats\',\n            self.empty_pd)\n        self.df_peers = self.ds_data.get(\n            \'peers\',\n            self.empty_pd)\n        self.df_financials = self.ds_data.get(\n            \'financials\',\n            self.empty_pd)\n        self.df_earnings = self.ds_data.get(\n            \'earnings\',\n            self.empty_pd)\n        self.df_dividends = self.ds_data.get(\n            \'dividends\',\n            self.empty_pd)\n        self.df_quote = self.ds_data.get(\n            \'quote\',\n            self.empty_pd)\n        self.df_company = self.ds_data.get(\n            \'company\',\n            self.empty_pd)\n        self.df_iex_news = self.ds_data.get(\n            \'news1\',\n            self.empty_pd)\n        self.df_yahoo_news = self.ds_data.get(\n            \'news\',\n            self.empty_pd)\n        self.df_calls = self.ds_data.get(\n            \'calls\',\n            self.empty_pd)\n        self.df_puts = self.ds_data.get(\n            \'puts\',\n            self.empty_pd)\n        self.df_pricing = self.ds_data.get(\n            \'pricing\',\n            {})\n        self.df_tdcalls = self.ds_data.get(\n            \'tdcalls\',\n            self.empty_pd)\n        self.df_tdputs = self.ds_data.get(\n            \'tdputs\',\n            self.empty_pd)\n\n        self.latest_min = None\n        self.backtest_date = self.ds_date\n        self.found_minute_data = False\n\n        if not hasattr(self.df_daily, \'index\'):\n            self.df_daily = self.empty_pd\n        if not hasattr(self.df_minute, \'index\'):\n            self.df_minute = self.empty_pd\n        else:\n            if \'date\' in self.df_minute:\n                self.latest_min = self.df_minute[\'date\'].iloc[-1]\n                self.found_minute_data = True\n        if not self.found_minute_data:\n            if ae_consts.is_df(self.df_tdcalls):\n                if \'date\' in self.df_tdcalls:\n                    self.latest_min = self.df_tdcalls[\'date\'].iloc[-1]\n                    self.found_minute_data = True\n            if ae_consts.is_df(self.df_tdputs):\n                if \'date\' in self.df_tdputs:\n                    self.latest_min = self.df_tdputs[\'date\'].iloc[-1]\n                    self.found_minute_data = True\n        if not hasattr(self.df_stats, \'index\'):\n            self.df_stats = self.empty_pd\n        if not hasattr(self.df_peers, \'index\'):\n            self.df_peers = self.empty_pd\n        if not hasattr(self.df_financials, \'index\'):\n            self.df_financials = self.empty_pd\n        if not hasattr(self.df_earnings, \'index\'):\n            self.df_earnings = self.empty_pd\n        if not hasattr(self.df_dividends, \'index\'):\n            self.df_dividends = self.empty_pd\n        if not hasattr(self.df_quote, \'index\'):\n            self.df_quote = self.empty_pd\n        if not hasattr(self.df_company, \'index\'):\n            self.df_company = self.empty_pd\n        if not hasattr(self.df_iex_news, \'index\'):\n            self.df_iex_news = self.empty_pd\n        if not hasattr(self.df_yahoo_news, \'index\'):\n            self.df_yahoo_news = self.empty_pd\n        if not hasattr(self.df_calls, \'index\'):\n            self.df_calls = self.empty_pd\n        if not hasattr(self.df_puts, \'index\'):\n            self.df_puts = self.empty_pd\n        if not hasattr(self.df_pricing, \'index\'):\n            self.df_pricing = self.empty_pd\n        if not hasattr(self.df_tdcalls, \'index\'):\n            self.df_tdcalls = self.empty_pd\n        if not hasattr(self.df_tdputs, \'index\'):\n            self.df_tdputs = self.empty_pd\n\n        # set internal values:\n        self.trade_date = self.ds_date\n        self.created_buy = False\n        self.created_sell = False\n        self.should_buy = False\n        self.should_sell = False\n\n        # by default assume close of trading for the day\n        self.use_minute = f\'{self.trade_date} 16:00:00\'\n\n        try:\n            if hasattr(self.df_daily, \'index\'):\n                columns = list(self.df_daily.columns.values)\n                if \'high\' in columns:\n                    self.today_high = float(\n                        self.df_daily.iloc[-1][\'high\'])\n                    self.latest_high = self.today_high\n                if \'low\' in columns:\n                    self.today_low = float(\n                        self.df_daily.iloc[-1][\'low\'])\n                    self.latest_low = self.today_low\n                if \'open\' in columns:\n                    self.today_open = float(\n                        self.df_daily.iloc[-1][\'open\'])\n                    self.latest_open = self.today_open\n                if \'close\' in columns:\n                    self.today_close = float(\n                        self.df_daily.iloc[-1][\'close\'])\n                    self.trade_price = self.today_close\n                    self.latest_close = self.trade_price\n                    if not self.starting_close:\n                        self.starting_close = self.today_close\n                if \'volume\' in columns:\n                    self.today_volume = int(\n                        self.df_daily.iloc[-1][\'volume\'])\n                    self.latest_volume = self.today_volume\n            if hasattr(self.df_minute, \'index\'):\n                columns = list(self.df_minute.columns.values)\n                if \'high\' in columns:\n                    self.latest_high = float(\n                        self.df_minute.iloc[-1][\'high\'])\n                if \'low\' in columns:\n                    self.latest_low = float(\n                        self.df_minute.iloc[-1][\'low\'])\n                if \'open\' in columns:\n                    self.latest_open = float(\n                        self.df_minute.iloc[-1][\'open\'])\n                if \'close\' in columns:\n                    self.latest_close = float(\n                        self.df_minute.iloc[-1][\'close\'])\n                    self.trade_price = self.latest_close\n                    if not self.starting_close:\n                        self.starting_close = self.latest_close\n                if \'volume\' in columns:\n                    self.latest_volume = int(\n                        self.df_minute.iloc[-1][\'volume\'])\n        except Exception as e:\n            self.debug_msg = (\n                f\'{self.name} handle - FAILED getting latest prices \'\n                f\'for algo={self.ds_id} - ds={self.ds_date} ex={e}\')\n            log.error(self.debug_msg)\n            if self.raise_on_err:\n                raise e\n        # end of trying to get the latest prices out of the\n        # datasets\n    # end of load_from_dataset\n\n    def reset_for_next_run(\n            self):\n        """"""reset_for_next_run\n\n        work in progress - clean up all internal member variables\n        for another run\n\n        .. note:: random or probablistic predictions may not\n            create the same trading history_output_file\n        """"""\n        self.debug_msg = \'\'\n        self.loaded_dataset = None\n        self.last_history_dict = None\n        self.last_handle_data = None\n        self.order_history = []\n        self.use_minute = None\n        self.intraday_start_min = None\n        self.intraday_end_min = None\n        self.intraday_events = {}\n    # end of reset_for_next_run\n\n    def populate_intraday_events_dict(\n            self,\n            start_min,\n            end_min):\n        """"""populate_intraday_events_dict\n\n        For tracking intraday buy/sell/news events with indicators\n        use this method to build a dictionary where keys\n        are the minutes between ``start_date`` and ``end_date``.\n        If both are ``None`` then the ``self.df_minute``\n\n        :param start_min: start datetime for building the\n            ``self.intraday_events`` dictionary keys\n        :param end_min: end datetime for building the\n            ``self.intraday_events`` dictionary keys\n        """"""\n        self.intraday_events = {}\n        if not self.found_minute_data:\n            return\n\n        if end_min < start_min:\n            raise Exception(\n                \'Invalid end_min must be greater than start_min - \'\n                \'self.populate_intraday_events_dict(\'\n                f\'start_min={start_min}, end_min={end_min}) \'\n                f\'algo={self.name}\')\n\n        num_minutes = ((end_min - start_min).total_seconds() / 60.0) + 1\n\n        if num_minutes > 1440:\n            raise Exception(\n                f\'Invalid number of minutes={num_minutes} between \'\n                f\'start_min={start_min} and end_min={end_min} is more than \'\n                \'the number of minutes in a single day: 1440 \'\n                f\'algo={self.name}\')\n\n        log.info(f\'num_minutes={num_minutes} between: {start_min} - {end_min}\')\n\n        self.intraday_start_min = start_min\n        self.intraday_end_min = end_min\n\n        cur_min = start_min\n        while cur_min <= end_min:\n            min_str = cur_min.strftime(ae_consts.COMMON_TICK_DATE_FORMAT)\n            self.intraday_events[min_str] = {}\n            for t in self.tickers:\n                self.intraday_events[min_str][t] = []\n            cur_min += datetime.timedelta(minutes=1)\n        # end of while minutes to add to the self.intraday_events dict\n    # end of populate_intraday_events_dict\n\n    def record_trade_history_for_dataset(\n            self,\n            node):\n        """"""record_trade_history_for_dataset\n\n        Build a daily or minute-by-minute trading\n        history\n\n        To run an algorithm minute-by-minute set the\n        configuration to use:\n\n        .. code-block:: python\n\n            \'timeseries\': \'minute\'\n\n        :param node: cached dataset dictionary node\n        """"""\n        # if set to minutes, but this dataset is missing minute-data\n        # then record as if it was a daily\n        use_day_timeseries = (\n            self.timeseries_value == ae_consts.ALGO_TIMESERIES_DAY)\n        use_minute_timeseries = (\n            self.timeseries_value == ae_consts.ALGO_TIMESERIES_MINUTE)\n        if use_day_timeseries or (\n                not self.found_minute_data and\n                use_minute_timeseries):\n            self.use_minute = f\'{self.trade_date} 16:00:00\'\n            self.last_history_dict = self.get_trade_history_node()\n            if self.last_history_dict:\n                if self.latest_ind_report:\n                    for k in self.latest_ind_report:\n                        if k not in self.ind_conf_ignore_keys:\n                            self.last_history_dict[k] = (\n                                self.latest_ind_report[k])\n                self.order_history.append(self.last_history_dict)\n        # end of if day timeseries\n        elif (use_minute_timeseries and self.found_minute_data):\n            # add the end of day point to the history\n            self.last_history_dict = self.get_trade_history_node()\n            if self.last_history_dict:\n                if self.latest_ind_report:\n                    for k in self.latest_ind_report:\n                        if k not in self.ind_conf_ignore_keys:\n                            self.last_history_dict[k] = (\n                                self.latest_ind_report[k])\n                self.order_history.append(self.last_history_dict)\n        else:\n            raise Exception(\n                f\'Unsupported self.timeseries={self.timeseries} and \'\n                f\'self.found_minute_data={self.found_minute_data} - \'\n                \'please use timeseries=day or timeseries=minute or \'\n                \'timeseries=intraday and ensure the \'\n                \'datasets have \\\'minute\\\' data\')\n        # end of processing trading history for this dataset\n    # end of record_trade_history_for_dataset\n\n    def handle_data(\n            self,\n            data):\n        """"""handle_data\n\n        process new data for the algorithm using a multi-ticker\n        mapping structure\n\n        :param data: dictionary of extracted data from\n            the redis pipeline with a structure:\n            ::\n\n                ticker = \'SPY\'\n                # string usually: YYYY-MM-DD\n                date = \'2018-11-05\'\n                # redis cache key for the dataset format: <ticker>_<date>\n                dataset_id = f\'{ticker}_{date}\'\n                dataset = {\n                    ticker: [\n                        {\n                            \'id\': dataset_id,\n                            \'date\': date,\n                            \'data\': {\n                                \'daily\': pd.DataFrame([]),\n                                \'minute\': pd.DataFrame([]),\n                                \'quote\': pd.DataFrame([]),\n                                \'stats\': pd.DataFrame([]),\n                                \'peers\': pd.DataFrame([]),\n                                \'news1\': pd.DataFrame([]),\n                                \'financials\': pd.DataFrame([]),\n                                \'earnings\': pd.DataFrame([]),\n                                \'dividends\': pd.DataFrame([]),\n                                \'calls\': pd.DataFrame([]),\n                                \'puts\': pd.DataFrame([]),\n                                \'pricing\': pd.DataFrame([]),\n                                \'news\': pd.DataFrame([])\n                            }\n                        }\n                    ]\n                }\n\n        """"""\n\n        self.debug_msg = (\n            f\'{self.name} handle - start\')\n\n        if self.loaded_dataset:\n            if self.verbose:\n                log.info(\n                    f\'{self.name} handle - using existing dataset \'\n                    f\'file={self.dsload_output_file} \'\n                    f\'s3={self.dsload_s3_key} \'\n                    f\'redis={self.dsload_redis_key}\')\n            data = self.loaded_dataset\n\n        data_for_tickers = self.get_supported_tickers_in_data(\n            data=data)\n\n        num_tickers = len(data_for_tickers)\n        if num_tickers > 0:\n            self.debug_msg = (\n                f\'{self.name} handle - \'\n                f\'tickers={json.dumps(data_for_tickers)}\')\n\n        for ticker in data_for_tickers:\n            num_ticker_datasets = len(data[ticker])\n            cur_idx = 1\n            for idx, node in enumerate(data[ticker]):\n                node_date = node.get(\'date\', \'missing-date\')\n                track_label = self.build_progress_label(\n                    progress=cur_idx,\n                    total=num_ticker_datasets)\n                algo_id = (\n                    f\'{ticker} {track_label}\')\n                self.debug_msg = (\n                    f\'{self.name} handle - {algo_id} - \'\n                    f\'id={node[""id""]} ds={node_date}\')\n\n                valid_run = False\n                if self.run_this_date:\n                    if node_date == self.run_this_date:\n                        log.critical(\n                            f\'{self.name} handle - starting at \'\n                            f\'date={node_date} with just this dataset: \')\n                        log.info(\n                            f\'{node[""data""]}\')\n                        valid_run = True\n                        self.verbose = True\n                        self.verbose_trading = True\n\n                        if self.inspect_dataset:\n                            self.view_date_dataset_records(\n                                algo_id=algo_id,\n                                ticker=ticker,\n                                node=node)\n                else:\n                    valid_run = True\n\n                if valid_run:\n                    self.ticker = ticker\n                    self.prev_bal = self.balance\n                    self.prev_num_owned = self.num_owned\n\n                    (self.num_owned,\n                     self.ticker_buys,\n                     self.ticker_sells) = self.get_ticker_positions(\n                        ticker=ticker)\n\n                    use_daily_timeseries = (\n                        self.timeseries_value == ae_consts.ALGO_TIMESERIES_DAY)\n\n                    node[\'data\'][\'custom\'] = self.include_custom\n\n                    if use_daily_timeseries:\n                        self.handle_daily_dataset(\n                            algo_id=algo_id,\n                            ticker=ticker,\n                            node=node)\n                    else:\n                        self.handle_minute_dataset(\n                            algo_id=algo_id,\n                            ticker=ticker,\n                            node=node,\n                            start_row=node.get(\'start_row\', 0))\n                    # end of processing datasets for day vs minute\n                # if not debugging a specific dataset in the cache\n\n                if (self.show_balance and\n                        (self.num_buys > 0 or self.num_sells > 0)):\n                    self.debug_msg = (\n                        f\'{self.name} handle - plot start balance\')\n                    self.plot_trading_history_with_balance(\n                        algo_id=algo_id,\n                        ticker=ticker,\n                        node=node)\n                    self.debug_msg = (\n                        f\'{self.name} handle - plot done balance\')\n                # if showing plots while the algo runs\n\n                if self.verbose:\n                    log.info(\n                        f\'{self.name} done {node_date}\')\n\n                cur_idx += 1\n        # for all supported tickers\n\n        # store the last handle dataset\n        self.last_handle_data = data\n\n        self.debug_msg = (\n            f\'{self.name} handle - end tickers={num_tickers}\')\n\n    # end of handle_data\n\n    def handle_daily_dataset(\n            self,\n            algo_id,\n            ticker,\n            node):\n        """"""handle_daily_dataset\n\n        handle running the algorithm with daily values\n\n        This method will call ``BaseAlgo.process()`` once per day\n        which is also utilizing the daily caching strategy\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param node: dataset to process\n        """"""\n\n        # parse the dataset node and set member variables\n        self.debug_msg = (\n            f\'{ticker} START - load dataset id={node.get(""id"", ""missing-id"")}\')\n        self.load_from_dataset(\n            ds_data=node)\n        self.debug_msg = (\n            f\'{ticker} END - load dataset id={node.get(""id"", ""missing-id"")}\')\n\n        """"""\n        Indicator Processor\n\n        processes the dataset: df_daily\n        """"""\n        self.latest_buys = []\n        self.latest_sells = []\n        if self.iproc:\n            self.debug_msg = f\'{ticker} BASEALGO-START - indicator processing\'\n            self.latest_ind_report = self.iproc.process(\n                algo_id=algo_id,\n                ticker=self.ticker,\n                dataset=node)\n            self.latest_buys = self.latest_ind_report.get(\n                \'buys\',\n                [])\n            self.latest_sells = self.latest_ind_report.get(\n                \'sells\',\n                [])\n            self.debug_msg = f\'{ticker} BASEALGO-END - indicator processing\'\n        # end of indicator processing\n\n        self.num_latest_buys = len(self.latest_buys)\n        self.num_latest_sells = len(self.latest_sells)\n\n        """"""\n        Call the Algorithm\'s process() method\n        """"""\n        self.debug_msg = (\n            f\'{ticker} START - process id={node.get(""id"", ""missing-id"")}\')\n        self.process(\n            algo_id=algo_id,\n            ticker=self.ticker,\n            dataset=node)\n        self.debug_msg = (\n            f\'{ticker} END - process id={node.get(""id"", ""missing-id"")}\')\n\n        """"""\n        Execute trades based off self.trade_strategy\n        """"""\n        self.debug_msg = (\n            f\'{ticker} START - trade id={node.get(""id"", ""missing-id"")}\')\n        self.trade_off_indicator_buy_and_sell_signals(\n            ticker=ticker,\n            algo_id=algo_id,\n            reason_for_buy=self.buy_reason,\n            reason_for_sell=self.sell_reason)\n        self.debug_msg = (\n            f\'{ticker} END - trade id={node.get(""id"", ""missing-id"")}\')\n\n        """"""\n        Record the Trading History record\n\n        analysis/review using: myalgo.get_result()\n        """"""\n        self.debug_msg = (\n            f\'{ticker} START - history id={node.get(""id"", ""missing-id"")}\')\n        self.record_trade_history_for_dataset(\n            node=node)\n        self.debug_msg = (\n            f\'{ticker} END - history id={node.get(""id"", ""missing-id"")}\')\n    # end of handle_daily_dataset\n\n    def prepare_for_new_indicator_run(\n            self):\n        """"""prepare_for_new_indicator_run\n\n        Call this for non-daily datasets specifically if the\n        algorithm is using ``minute`` timeseries\n        """"""\n        self.prev_bal = self.balance\n        self.prev_num_owned = self.num_owned\n        self.should_buy = False\n        self.should_sell = False\n        self.num_latest_buys = 0\n        self.num_latest_sells = 0\n    # end of prepare_for_new_indicator_run\n\n    def handle_minute_dataset(\n            self,\n            algo_id,\n            ticker,\n            node,\n            start_row=0):\n        """"""handle_minute_dataset\n\n        handle running the algorithm with daily values\n\n        This method will call ``BaseAlgo.process()`` once per day\n        which is also utilizing the daily caching strategy\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param node: dataset to process\n        :param start_row: start row default is ``0``\n        """"""\n        # parse the dataset node and set member variables\n        node_id = node.get(\'id\', \'missing-id\')\n        node_date = node.get(\'date\', \'missing-date\')\n        self.debug_msg = (\n            f\'{ticker} START - load dataset id={node_id}\')\n        self.load_from_dataset(\n            ds_data=node)\n        self.debug_msg = (\n            f\'{ticker} END - load dataset id={node_id}\')\n\n        if not self.found_minute_data:\n            if self.verbose:\n                log.error(\n                    f\'algo={self.name} is missing minute data for \'\n                    f\'day={node_date}\')\n            """"""\n            Record the Trading History record\n\n            analysis/review using: myalgo.get_result()\n            """"""\n            self.use_minute = (\n                f\'{self.trade_date} 16:00:00\')\n            self.debug_msg = (\n                f\'{ticker} START - saving for missing minute history \'\n                f\'id={node_id}\')\n            self.record_trade_history_for_dataset(\n                node=node)\n            self.debug_msg = (\n                f\'{ticker} END - for missing history id={node_id}\')\n            return\n\n        num_rows = len(self.df_minute.index)\n        if num_rows == 0:\n            log.warn(\n                f\'no minute data for {ticker} on: {node_id} \'\n                f\'rows={num_rows}\')\n            return\n        # if no minute data found\n\n        for minute_idx, row in self.df_minute[start_row:].iterrows():\n\n            # map the latest values for the algo to use\n            # as if the minute was the latest trading time\n            # as it iterates minute-by-minute\n            self.latest_min = row.get(\'date\', None)\n            if not self.latest_min:\n                log.warn(\n                    f\'no cached minute data found in cache for {ticker} \'\n                    f\'on: {node_id} rows={num_rows}\')\n                return\n            self.latest_high = row.get(\'high\', None)\n            self.latest_low = row.get(\'low\', None)\n            self.latest_open = row.get(\'open\', None)\n            self.latest_close = row.get(\'close\', None)\n            self.latest_volume = row.get(\'volume\', None)\n            self.trade_price = self.latest_close\n            self.use_minute = self.latest_min.strftime(\n                ae_consts.COMMON_TICK_DATE_FORMAT)\n\n            self.show_log = False\n            # log every 5 days just to see progress\n            if self.last_minute:\n                num_day_since_last_log = (\n                    self.latest_min - self.last_minute).days\n                if num_day_since_last_log > 5:\n                    self.show_log = True\n                    self.last_minute = self.latest_min\n            else:\n                # start on monday\n                if self.latest_min.weekday() == 0:\n                    self.last_minute = self.latest_min\n\n            if not self.starting_close:\n                self.starting_close = self.latest_close\n\n            # allow algos to set these custom strings\n            # for tracking why a buy and sell happened\n            self.buy_reason = None\n            self.sell_reason = None\n\n            self.prepare_for_new_indicator_run()\n\n            track_label = self.build_progress_label(\n                progress=(minute_idx + 1),\n                total=num_rows)\n            minute_algo_id = (\n                f\'{algo_id} at minute \'\n                f\'{self.latest_min} - {track_label}\')\n\n            (self.num_owned,\n             self.ticker_buys,\n             self.ticker_sells) = self.get_ticker_positions(\n                ticker=ticker)\n\n            """"""\n            Indicator Processor\n\n            processes the dataset: minute df\n            """"""\n            self.latest_buys = []\n            self.latest_sells = []\n            if self.iproc:\n                self.debug_msg = (\n                    f\'{ticker} START - indicator processing \'\n                    f\'daily [0-{minute_idx + 1}]\')\n\n                # prune off the minutes that are not the latest\n                node[\'data\'][\'minute\'] = self.df_minute.iloc[0:(minute_idx+1)]\n                self.latest_ind_report = self.iproc.process(\n                    algo_id=minute_algo_id,\n                    ticker=self.ticker,\n                    dataset=node)\n                self.latest_buys = self.latest_ind_report.get(\n                    \'buys\',\n                    [])\n                self.latest_sells = self.latest_ind_report.get(\n                    \'sells\',\n                    [])\n                self.debug_msg = (\n                    f\'{ticker} END - indicator processing\')\n            # end of indicator processing\n\n            self.num_latest_buys = len(self.latest_buys)\n            self.num_latest_sells = len(self.latest_sells)\n\n            if self.inspect_datasets:\n                self.inspect_dataset(\n                    algo_id=algo_id,\n                    ticker=ticker,\n                    dataset=node)\n\n            """"""\n            Call the Algorithm\'s process() method\n            """"""\n            self.debug_msg = (\n                f\'{ticker} START - process id={node_id}\')\n            self.process(\n                algo_id=algo_id,\n                ticker=self.ticker,\n                dataset=node)\n            self.debug_msg = (\n                f\'{ticker} END - process id={node_id}\')\n\n            """"""\n            Execute trades based off self.trade_strategy\n            """"""\n            self.debug_msg = (\n                f\'{ticker} START - trade id={node_id}\')\n            self.trade_off_indicator_buy_and_sell_signals(\n                ticker=ticker,\n                algo_id=algo_id,\n                reason_for_buy=self.buy_reason,\n                reason_for_sell=self.sell_reason)\n            self.debug_msg = (\n                f\'{ticker} END - trade id={node_id}\')\n\n            """"""\n            Record the Trading History record\n\n            analysis/review using: myalgo.get_result()\n            """"""\n            self.debug_msg = (\n                f\'{ticker} START - history id={node_id}\')\n            self.record_trade_history_for_dataset(\n                node=node)\n            self.debug_msg = (\n                f\'{ticker} END - history id={node_id}\')\n        # end for all rows in the minute dataset\n    # end of handle_minute_dataset\n\n    def plot_trading_history_with_balance(\n            self,\n            algo_id,\n            ticker,\n            node):\n        """"""\n\n        This will live plot the trading history after each\n        day is done\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param node: dataset to process\n        """"""\n        trading_history_dict = self.get_history_dataset()\n        history_df = trading_history_dict[ticker]\n        if not hasattr(history_df, \'to_json\'):\n            return\n\n        first_date = history_df[\'date\'].iloc[0]\n        end_date = history_df[\'date\'].iloc[-1]\n        title = (\n            f\'Trading History {ticker} for Algo \'\n            f\'{trading_history_dict[""algo_name""]}\\n\'\n            f\'Backtest dates from {first_date} to {end_date}\')\n        use_xcol = \'date\'\n        use_as_date_format = \'%d\\n%b\'\n        if self.config_dict[\'timeseries\'] == \'minute\':\n            use_xcol = \'minute\'\n            use_as_date_format = \'%d %H:%M:%S\\n%b\'\n        xlabel = f\'Dates vs {trading_history_dict[""algo_name""]} values\'\n        ylabel = f\'Algo {trading_history_dict[""algo_name""]}\\nvalues\'\n        df_filter = (history_df[\'close\'] > 0.01)\n\n        # set default columns:\n        red = self.red_column\n        blue = self.blue_column\n        green = self.green_column\n        orange = self.orange_column\n\n        plot_trading_history.plot_trading_history(\n            title=title,\n            df=history_df,\n            red=red,\n            blue=blue,\n            green=green,\n            orange=orange,\n            date_col=use_xcol,\n            date_format=use_as_date_format,\n            xlabel=xlabel,\n            ylabel=ylabel,\n            df_filter=df_filter,\n            show_plot=True,\n            dropna_for_all=True)\n\n    # end of plot_trading_history_with_balance\n\n    def load_custom_datasets(\n            self):\n        """"""load_custom_datasets\n\n        Handler for loading custom datasets for indicators\n\n        .. tip:: Custom datasets allow indicators to analyze\n            more than the default pricing data provided by\n            ``IEX Cloud`` and ``Tradier``. This is helpful for\n            building indicators to analyze and train AI from\n            a previous algorithm ``Trading History``.\n        """"""\n        label = f\'load_custom_ds\'\n        ticker = self.tickers[0]\n        ds_list = self.config_dict.get(\n            \'custom_datasets\',\n            [])\n        redis_address = ae_consts.REDIS_ADDRESS\n        redis_db = ae_consts.REDIS_DB\n        redis_password = ae_consts.REDIS_PASSWORD\n        for node in ds_list:\n            ds_key = node[\'ds_key\']\n            redis_loc = node.get(\'redis_loc\', None)\n            s3_loc = node.get(\'s3_loc\', None)\n            ds_type = node.get(\'type\', \'trade_history\').lower()\n            label = f\'load_custom_ds_{ds_key}\',\n            log.info(\n                f\'loading {ds_key} {ds_type} from \'\n                f\'redis={redis_loc} \'\n                f\'s3={s3_loc}\')\n\n            custom_ds = None\n            publish_to_redis = False\n\n            if redis_loc:\n                if ds_type == \'trade_history\':\n                    get_result = redis_get.get_data_from_redis_key(\n                        label=label,\n                        host=redis_address.split(\':\')[0],\n                        port=redis_address.split(\':\')[-1],\n                        password=redis_password,\n                        db=redis_db,\n                        key=redis_loc,\n                        decompress_df=True)\n\n                    if get_result[\'status\'] == ae_consts.SUCCESS:\n                        publish_to_redis = False\n                        custom_ds = get_result[\'rec\'][\'data\']\n                    else:\n                        log.info(\n                            f\'did not find {ds_key} {ds_type} in redis=\'\n                            f\'{redis_loc}\')\n                        publish_to_redis = True\n            # if need to load from redis\n\n            if (\n                    s3_loc and\n                    publish_to_redis):\n                s3_bucket = s3_loc.split(\'/\')[2]\n                s3_key = s3_loc.split(\'/\')[-1]\n                if ds_type == \'trade_history\':\n                    load_res = load_history_utils.load_history_dataset(\n                        s3_bucket=s3_bucket,\n                        s3_key=s3_key)\n                    custom_ds = load_res[ticker]\n                    publish_to_redis = True\n            # if need to download from s3\n\n            if ds_type == \'trade_history\':\n                if ticker in custom_ds:\n                    custom_ticker_df = pd.DataFrame(\n                        custom_ds[ticker])\n                    if ae_consts.is_df(df=custom_ticker_df):\n                        date_cols = [\n                            \'date\',\n                            \'minute\'\n                        ]\n                        for d in date_cols:\n                            if d in custom_ticker_df:\n                                custom_ticker_df[d] = pd.to_datetime(\n                                    custom_ticker_df[d])\n                        if \'minute\' in custom_ticker_df:\n                            custom_ticker_df.sort_values(\n                                by=[\n                                    \'minute\'\n                                ],\n                                ascending=True)\n                        elif \'date\' in custom_ticker_df:\n                            custom_ticker_df.sort_values(\n                                by=[\n                                    \'date\'\n                                ],\n                                ascending=True)\n                    custom_ds[ticker] = custom_ticker_df\n                    self.include_custom[ds_key] = custom_ds\n\n            if publish_to_redis:\n                log.info(\n                    f\'publishing {ds_key} to redis={redis_loc}\')\n                publish.publish(\n                    data=custom_ds,\n                    df_compress=True,\n                    convert_to_json=False,\n                    compress=False,\n                    label=f\'load_custom_ds_{ds_key}\',\n                    redis_enabled=True,\n                    redis_key=redis_loc,\n                    redis_address=redis_address,\n                    redis_db=redis_db,\n                    redis_password=redis_password,\n                    s3_enabled=False,\n                    slack_enabled=False,\n                    verbose=False)\n            # end of publishing to redis for speeding up next run\n        # end of for all custom datasets to load\n    # end of load_custom_datasets\n\n# end of BaseAlgo\n'"
analysis_engine/algo_runner.py,0,"b'""""""\nA class for running backtests and the latest pricing data\nwith an automated publishing of the ``Trading History`` to S3\n""""""\n\nimport os\nimport datetime\nimport json\nimport pandas as pd\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.utils as ae_utils\nimport analysis_engine.build_dataset_node as build_dataset_node\nimport analysis_engine.load_history_dataset as load_history_utils\nimport analysis_engine.run_custom_algo as run_custom_algo\nimport analysis_engine.publish as publish\nimport analysis_engine.algo as base_algo\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\nclass AlgoRunner:\n    """"""AlgoRunner\n\n    Run an algorithm backtest or with the latest\n    pricing data and publish the compressed\n    trading history to s3 which can be used\n    to train AI\n\n    **Full Backtest**\n\n    .. code-block:: python\n\n        import analysis_engine.algo_runner as algo_runner\n        runner = algo_runner.AlgoRunner(\'SPY\')\n        runner.start()\n\n    **Run Algorithm with Latest Pricing Data**\n\n    .. code-block:: python\n\n        import analysis_engine.algo_runner as algo_runner\n        import analysis_engine.plot_trading_history as plot\n        ticker = \'SPY\'\n        runner = algo_runner.AlgoRunner(ticker)\n        # run the algorithm with the latest 200 minutes:\n        df = runner.latest()\n        print(df[[\'minute\', \'close\']].tail(5))\n        plot.plot_trading_history(\n            title=(\n                f\'{ticker} - ${df[""close""].iloc[-1]} \'\n                f\'at: {df[""minute""].iloc[-1]}\'),\n            df=df)\n\n    """"""\n\n    def __init__(\n            self,\n            ticker,\n            algo_config=None,\n            start_date=None,\n            end_date=None,\n            history_loc=None,\n            predictions_loc=None,\n            run_on_engine=False,\n            verbose_algo=False,\n            verbose_processor=False,\n            verbose_indicators=False,\n            **kwargs):\n        """"""__init__\n\n        constructor\n\n        :param ticker: string ticker\n        :param algo_config: optional - string path to file\n            (default is ``./cfg/default_algo.json``)\n        :param history_loc: optional - string trading history location\n            (default is ``s3://algohistory/trade_history_{ticker}``)\n        :param predictions_loc: optional - string predictions location\n        :param start_date: optional - string start date\n        :param end_date: optional - string end date\n        :param run_on_engine: optional - bool flag for running local\n            or distributed on an engine\n            (default is ``False`` - local)\n        :param verbose_algo: optional - bool flag for\n            debugging the algo\n            (default is ``False``)\n        :param verbose_processor: optional - bool flag for\n            debugging the algo\'s indicator processor\n            (default is ``False``)\n        :param verbose_indicators: optional - bool flag for\n            debugging the algo\'s indicators\n            (default is ``False``)\n        :param kwargs: keyword args dictionary\n        """"""\n        self.ticker = None\n        if ticker:\n            self.ticker = str(ticker).upper()\n        self.start_date = start_date\n        self.end_date = end_date\n        self.start_day = None\n        self.end_day = None\n        self.run_on_engine = run_on_engine\n        self.algo_history_loc = (\n            f\'s3://algohistory/trade_history_{self.ticker}\')\n        self.algo_predictions_loc = (\n            f\'s3://predictions/{self.ticker}\')\n        if history_loc:\n            self.algo_history_loc = history_loc\n        if predictions_loc:\n            self.algo_predictions_loc = predictions_loc\n\n        self.pt_s3_access_key = ae_consts.ev(\n            \'PREDICTIONS_S3_ACCESS_KEY\',\n            ae_consts.S3_ACCESS_KEY)\n        self.pt_s3_secret_key = ae_consts.ev(\n            \'PREDICTIONS_S3_SECRET_KEY\',\n            ae_consts.S3_SECRET_KEY)\n        self.pt_s3_address = ae_consts.ev(\n            \'PREDICTIONS_S3_ADDRESS\',\n            ae_consts.S3_ADDRESS)\n        self.pt_s3_secure = str(ae_consts.ev(\n            \'PREDICTIONS_S3_SECURE\',\n            ae_consts.S3_SECURE)) == \'1\'\n        self.pt_s3_region = ae_consts.ev(\n            \'PREDICTIONS_S3_REGION_NAME\',\n            ae_consts.S3_REGION_NAME)\n        self.pt_s3_bucket = self.algo_predictions_loc.split(\'/\')[-2]\n        self.pt_s3_key = self.algo_predictions_loc.split(\'/\')[-1]\n\n        self.config_dict = None\n        self.use_config_file = algo_config\n        if not self.use_config_file:\n            if os.path.exists(\'./cfg/default_algo.json\'):\n                self.use_config_file = (\n                    \'./cfg/default_algo.json\')\n            elif os.path.exists(\'/opt/sa/cfg/default_algo.json\'):\n                self.use_config_file = (\n                    \'/opt/sa/cfg/default_algo.json\')\n            else:\n                log.critical(\n                    f\'Failed: missing algo_config argument pointing to a \'\n                    f\'config file\')\n                return\n        self.config_dict = json.loads(open(self.use_config_file, \'r\').read())\n        self.algo_mod_path = self.config_dict.get(\n            \'algo_path\',\n            ae_consts.ALGO_MODULE_PATH)\n        if not os.path.exists(self.algo_mod_path):\n            log.critical(\n                f\'missing algorithm module file from config: \'\n                f\'{self.algo_mod_path}\')\n            return\n\n        self.balance = 10000.0\n        self.commission = 6.0\n        self.use_start_date = None\n        self.use_end_date = None\n\n        self.inspect_datasets = False\n        self.history_json_file = None\n        self.run_this_date = None\n        self.algo_obj = None\n        self.algo_report_loc = None\n        self.algo_extract_loc = None\n        self.backtest_loc = None\n        self.raise_on_err = True\n\n        self.ssl_options = ae_consts.SSL_OPTIONS\n        self.transport_options = ae_consts.TRANSPORT_OPTIONS\n        self.broker_url = ae_consts.WORKER_BROKER_URL\n        self.backend_url = ae_consts.WORKER_BACKEND_URL\n        self.path_to_config_module = ae_consts.WORKER_CELERY_CONFIG_MODULE\n        self.include_tasks = ae_consts.INCLUDE_TASKS\n        self.load_from_s3_bucket = None\n        self.load_from_s3_key = None\n        self.load_from_redis_key = None\n        self.load_from_file = None\n        self.load_compress = True\n        self.load_publish = True\n        self.load_config = None\n        self.report_redis_key = None\n        self.report_s3_bucket = None\n        self.report_s3_key = None\n        self.report_file = None\n        self.report_compress = True\n        self.report_publish = False\n        self.report_config = None\n        self.history_redis_key = None\n        self.history_s3_bucket = None\n        self.history_s3_key = None\n        self.history_file = None\n        self.history_compress = True\n        self.history_publish = True\n        self.history_config = None\n        self.extract_redis_key = None\n        self.extract_s3_bucket = None\n        self.extract_s3_key = None\n        self.extract_file = None\n        self.extract_save_dir = None\n        self.extract_compress = False\n        self.extract_publish = False\n        self.extract_config = None\n        self.s3_enabled = True\n        self.s3_access_key = ae_consts.S3_ACCESS_KEY\n        self.s3_secret_key = ae_consts.S3_SECRET_KEY\n        self.s3_region_name = ae_consts.S3_REGION_NAME\n        self.s3_address = ae_consts.S3_ADDRESS\n        self.s3_bucket = ae_consts.S3_BUCKET\n        self.s3_key = None\n        self.s3_secure = ae_consts.S3_SECURE\n        self.redis_enabled = True\n        self.redis_address = ae_consts.REDIS_ADDRESS\n        self.redis_key = None\n        self.redis_password = ae_consts.REDIS_PASSWORD\n        self.redis_db = ae_consts.REDIS_DB\n        self.redis_expire = ae_consts.REDIS_EXPIRE\n        self.redis_serializer = \'json\'\n        self.redis_encoding = \'utf-8\'\n        self.publish_to_s3 = True\n        self.publish_to_redis = True\n        self.publish_to_slack = False\n        self.slack_enabled = False\n        self.slack_code_block = False\n        self.slack_full_width = False\n\n        self.dataset_type = ae_consts.SA_DATASET_TYPE_ALGO_READY\n        self.serialize_datasets = ae_consts.DEFAULT_SERIALIZED_DATASETS\n        self.compress = False\n        self.encoding = \'utf-8\'\n        self.debug = False\n        self.num_rows = 0\n\n        self.auto_fill = True\n        self.timeseries = \'minute\'\n        self.trade_strategy = \'count\'\n        self.algo_history_s3_bucket = None\n        self.algo_history_s3_key = None\n\n        if self.start_date:\n            self.use_start_date = f\'{str(self.start_date)} 00:00:00\'\n            datetime.datetime.strptime(\n                self.start_date,\n                ae_consts.COMMON_DATE_FORMAT)\n        if self.end_date:\n            self.use_end_date = f\'{str(self.end_date)} 00:00:00\'\n            datetime.datetime.strptime(\n                self.end_date,\n                ae_consts.COMMON_DATE_FORMAT)\n\n        """"""\n        Finalize the algo config\n        """"""\n        self.debug = False\n        self.verbose_algo = verbose_algo\n        self.verbose_processor = verbose_processor\n        self.verbose_indicators = verbose_indicators\n\n        if self.config_dict:\n            if self.ticker:\n                self.config_dict[\'ticker\'] = self.ticker\n            else:\n                self.ticker = (\n                    str(self.config_dict[\'ticker\']).upper())\n            self.config_dict[\'balance\'] = self.balance\n            self.config_dict[\'commission\'] = self.commission\n\n        if self.verbose_algo:\n            self.config_dict[\'verbose\'] = self.verbose_algo\n        if self.verbose_processor:\n            self.config_dict[\'verbose_processor\'] = self.verbose_processor\n        if self.verbose_indicators:\n            self.config_dict[\'verbose_indicators\'] = self.verbose_indicators\n        if self.inspect_datasets:\n            self.config_dict[\'inspect_datasets\'] = self.inspect_datasets\n        if self.run_this_date:\n            self.config_dict[\'run_this_date\'] = self.run_this_date\n\n        if self.backtest_loc:\n            if \'s3://\' in self.backtest_loc:\n                self.load_from_s3_bucket = self.backtest_loc.split(\'/\')[-2]\n                self.load_from_s3_key = self.backtest_loc.split(\'/\')[-1]\n            elif \'redis://\' in self.backtest_loc:\n                self.load_from_redis_key = self.backtest_loc.split(\'/\')[-1]\n            elif \'file:/\' in self.backtest_loc:\n                self.load_from_file = self.backtest_loc.split(\':\')[-1]\n            else:\n                log.error(\n                    \'invalid -b <backtest dataset file> specified. \'\n                    f\'{self.backtest_loc} \'\n                    \'please use either: \'\n                    \'-b file:/opt/sa/tests/datasets/algo/SPY-latest.json or \'\n                    \'-b s3://algoready/SPY-latest.json or \'\n                    \'-b redis://SPY-latest\')\n            self.load_publish = True\n\n        if self.algo_history_loc:\n            if \'s3://\' in self.algo_history_loc:\n                self.history_s3_bucket = self.algo_history_loc.split(\'/\')[-2]\n                self.history_s3_key = self.algo_history_loc.split(\'/\')[-1]\n            elif \'redis://\' in self.algo_history_loc:\n                self.history_redis_key = self.algo_history_loc.split(\'/\')[-1]\n            elif \'file:/\' in self.algo_history_loc:\n                self.history_file = self.algo_history_loc.split(\':\')[-1]\n            else:\n                log.error(\n                    \'invalid -p <backtest dataset file> specified. \'\n                    f\'{self.algo_history_loc} \'\n                    \'please use either: \'\n                    \'-p file:/opt/sa/tests/datasets/algo/SPY-latest.json or \'\n                    \'-p s3://algoready/SPY-latest.json or \'\n                    \'-p redis://SPY-latest\')\n            self.history_publish = True\n\n        if self.algo_report_loc:\n            if \'s3://\' in self.algo_report_loc:\n                self.report_s3_bucket = self.algo_report_loc.split(\'/\')[-2]\n                self.report_s3_key = self.algo_report_loc.split(\'/\')[-1]\n            elif \'redis://\' in self.algo_report_loc:\n                self.report_redis_key = self.algo_report_loc.split(\'/\')[-1]\n            elif \'file:/\' in self.algo_report_loc:\n                self.report_file = self.algo_report_loc.split(\':\')[-1]\n            else:\n                log.error(\n                    \'invalid -o <backtest dataset file> specified. \'\n                    f\'{self.algo_report_loc} \'\n                    \'please use either: \'\n                    \'-o file:/opt/sa/tests/datasets/algo/SPY-latest.json or \'\n                    \'-o s3://algoready/SPY-latest.json or \'\n                    \'-o redis://SPY-latest\')\n            self.report_publish = True\n\n        if self.algo_extract_loc:\n            if \'s3://\' in self.algo_extract_loc:\n                self.extract_s3_bucket = self.algo_extract_loc.split(\'/\')[-2]\n                self.extract_s3_key = self.algo_extract_loc.split(\'/\')[-1]\n            elif \'redis://\' in self.algo_extract_loc:\n                self.extract_redis_key = self.algo_extract_loc.split(\'/\')[-1]\n            elif \'file:/\' in self.algo_extract_loc:\n                self.extract_file = self.algo_extract_loc.split(\':\')[-1]\n            else:\n                log.error(\n                    \'invalid -e <backtest dataset file> specified. \'\n                    f\'{self.algo_extract_loc} \'\n                    \'please use either: \'\n                    \'-e file:/opt/sa/tests/datasets/algo/SPY-latest.json or \'\n                    \'-e s3://algoready/SPY-latest.json or \'\n                    \'-e redis://SPY-latest\')\n            self.extract_publish = True\n\n        self.use_name = self.config_dict.get(\n            \'name\',\n            \'missing-algo-name\')\n        self.auto_fill = self.config_dict.get(\n            \'auto_fill\',\n            self.auto_fill)\n        self.timeseries = self.config_dict.get(\n            \'timeseries\',\n            self.timeseries)\n        self.trade_strategy = self.config_dict.get(\n            \'trade_strategy\',\n            self.trade_strategy)\n\n        self.iex_datasets = ae_consts.IEX_DATASETS_DEFAULT\n        self.common_fetch_vals = {}\n        self.common_fetch_vals[\'ticker\'] = self.ticker\n        self.common_fetch_vals[\'celery_disabled\'] = True\n        self.common_fetch_vals[\'label\'] = \'lp\'\n        self.common_fetch_vals[\'iex_datasets\'] = self.iex_datasets\n        self.common_fetch_vals[\'s3_enabled\'] = False\n        self.common_fetch_vals[\'s3_address\'] = None\n        self.common_fetch_vals[\'s3_bucket\'] = None\n        self.common_fetch_vals[\'s3_key\'] = None\n        self.common_fetch_vals[\'s3_access_key\'] = None\n        self.common_fetch_vals[\'s3_secret_key\'] = None\n        self.common_fetch_vals[\'s3_region_name\'] = None\n        self.common_fetch_vals[\'s3_secure\'] = None\n        self.common_fetch_vals[\'redis_enabled\'] = True\n        self.common_fetch_vals[\'redis_address\'] = self.redis_address\n        self.common_fetch_vals[\'redis_password\'] = self.redis_password\n        self.common_fetch_vals[\'redis_expire\'] = None\n        self.common_fetch_vals[\'redis_db\'] = self.redis_db\n\n        self.history_df = None\n        self.slack_enabled = False\n    # end of __init__\n\n    def start(\n            self):\n        """"""start\n\n        Start the algorithm backtest\n        """"""\n\n        log.info(\n            \'starting algo \'\n            f\'s3://{self.history_s3_bucket}/{self.history_s3_key}\')\n\n        self.algo_res = run_custom_algo.run_custom_algo(\n            mod_path=self.algo_mod_path,\n            ticker=self.config_dict[\'ticker\'],\n            balance=self.config_dict[\'balance\'],\n            commission=self.config_dict[\'commission\'],\n            name=self.use_name,\n            start_date=self.use_start_date,\n            end_date=self.use_end_date,\n            auto_fill=self.auto_fill,\n            config_dict=self.config_dict,\n            load_from_s3_bucket=self.load_from_s3_bucket,\n            load_from_s3_key=self.load_from_s3_key,\n            load_from_redis_key=self.load_from_redis_key,\n            load_from_file=self.load_from_file,\n            load_compress=self.load_compress,\n            load_publish=self.load_publish,\n            load_config=self.load_config,\n            report_redis_key=self.report_redis_key,\n            report_s3_bucket=self.report_s3_bucket,\n            report_s3_key=self.report_s3_key,\n            report_file=self.report_file,\n            report_compress=self.report_compress,\n            report_publish=self.report_publish,\n            report_config=self.report_config,\n            history_redis_key=self.history_redis_key,\n            history_s3_bucket=self.history_s3_bucket,\n            history_s3_key=self.history_s3_key,\n            history_file=self.history_file,\n            history_compress=self.history_compress,\n            history_publish=self.history_publish,\n            history_config=self.history_config,\n            extract_redis_key=self.extract_redis_key,\n            extract_s3_bucket=self.extract_s3_bucket,\n            extract_s3_key=self.extract_s3_key,\n            extract_file=self.extract_file,\n            extract_save_dir=self.extract_save_dir,\n            extract_compress=self.extract_compress,\n            extract_publish=self.extract_publish,\n            extract_config=self.extract_config,\n            publish_to_slack=self.publish_to_slack,\n            publish_to_s3=self.publish_to_s3,\n            publish_to_redis=self.publish_to_redis,\n            dataset_type=self.dataset_type,\n            serialize_datasets=self.serialize_datasets,\n            compress=self.compress,\n            encoding=self.encoding,\n            redis_enabled=self.redis_enabled,\n            redis_key=self.redis_key,\n            redis_address=self.redis_address,\n            redis_db=self.redis_db,\n            redis_password=self.redis_password,\n            redis_expire=self.redis_expire,\n            redis_serializer=self.redis_serializer,\n            redis_encoding=self.redis_encoding,\n            s3_enabled=self.s3_enabled,\n            s3_key=self.s3_key,\n            s3_address=self.s3_address,\n            s3_bucket=self.s3_bucket,\n            s3_access_key=self.s3_access_key,\n            s3_secret_key=self.s3_secret_key,\n            s3_region_name=self.s3_region_name,\n            s3_secure=self.s3_secure,\n            slack_enabled=self.slack_enabled,\n            slack_code_block=self.slack_code_block,\n            slack_full_width=self.slack_full_width,\n            dataset_publish_extract=self.extract_publish,\n            dataset_publish_history=self.history_publish,\n            dataset_publish_report=self.report_publish,\n            run_on_engine=self.run_on_engine,\n            auth_url=self.broker_url,\n            backend_url=self.backend_url,\n            include_tasks=self.include_tasks,\n            ssl_options=self.ssl_options,\n            transport_options=self.transport_options,\n            path_to_config_module=self.path_to_config_module,\n            timeseries=self.timeseries,\n            trade_strategy=self.trade_strategy,\n            verbose=self.verbose_algo,\n            raise_on_err=self.raise_on_err)\n\n        self.wait_for_algo_to_finish()\n    # end of start\n\n    def wait_for_algo_to_finish(\n            self):\n        """"""wait_for_algo_to_finish\n\n        wait until the algorithm finishes\n        """"""\n\n        self.algo_history_s3_bucket = None\n        self.algo_history_s3_key = None\n        self.task_id = None\n        if self.run_on_engine:\n            self.task_id = self.algo_res.get(\n                \'rec\', {}).get(\'task_id\', None)\n        if self.task_id:\n            log.info(\n                f\'waiting - algo task_id={self.task_id} to finish\')\n            res = self.task_id.get()\n            history_config = res.get(\n                \'algo_req\', {}).get(\n                    \'history_config\', None)\n            self.algo_history_s3_bucket = history_config.get(\n                \'s3_bucket\', None)\n            self.algo_history_s3_key = history_config.get(\n                \'s3_key\', None)\n            self.load_trading_history()\n        else:\n            history_records = self.algo_res.get(\n                \'rec\', {}).get(\n                    \'history\', None)\n            self.algo_history_s3_bucket = self.history_s3_bucket\n            self.algo_history_s3_key = self.history_s3_key\n            self.history_df = pd.DataFrame(history_records)\n            self.determine_latest_times_in_history()\n\n        self.num_rows = len(self.history_df.index)\n\n        log.info(\n            f\'loaded history rows={self.num_rows} \'\n            f\'dates={self.start_date} to {self.end_date}\')\n    # end of wait_for_algo_to_finish\n\n    def determine_latest_times_in_history(\n            self):\n        """"""determine_latest_times_in_history\n\n        determine the latest minute or day in the pricing dataset\n        and convert ``date`` and ``minute`` columns to ``datetime``\n        objects\n        """"""\n        self.history_df[\'date\'] = pd.to_datetime(\n            self.history_df[\'date\'])\n        self.start_day = self.history_df[\'date\'].iloc[0]\n        self.end_day = self.history_df[\'date\'].iloc[-1]\n        self.start_date = self.history_df[\'date\'].iloc[0]\n        self.end_date = self.history_df[\'date\'].iloc[-1]\n        if \'minute\' in self.history_df:\n            self.history_df[\'minute\'] = pd.to_datetime(\n                self.history_df[\'minute\'])\n            self.start_date = self.history_df[\'minute\'].iloc[0]\n            self.end_date = self.history_df[\'minute\'].iloc[-1]\n    # end of determine_latest_times_in_history\n\n    def load_trading_history(\n            self,\n            s3_access_key=None,\n            s3_secret_key=None,\n            s3_address=None,\n            s3_region=None,\n            s3_bucket=None,\n            s3_key=None,\n            s3_secure=ae_consts.NOT_SET,\n            **kwargs):\n        """"""load_trading_history\n\n        Helper for loading an algorithm ``Trading History`` from\n        S3\n\n        :param s3_access_key: access key\n        :param s3_secret_key: secret\n        :param s3_address: address\n        :param s3_region: region\n        :param s3_bucket: bucket\n        :param s3_key: key\n        :param s3_secure: secure flag\n        :param kwargs: support for keyword arg dict\n        """"""\n        use_s3_access_key = self.s3_access_key\n        use_s3_secret_key = self.s3_secret_key\n        use_s3_address = self.s3_address\n        use_s3_region = self.s3_region_name\n        use_s3_bucket = self.s3_bucket\n        use_s3_key = self.s3_key\n        use_s3_secure = self.s3_secure\n\n        if s3_access_key:\n            use_s3_access_key = s3_access_key\n        if s3_secret_key:\n            use_s3_secret_key = s3_secret_key\n        if s3_address:\n            use_s3_address = s3_address\n        if s3_region:\n            use_s3_region = s3_region\n        if s3_bucket:\n            use_s3_bucket = s3_bucket\n        if s3_key:\n            use_s3_key = s3_key\n        if s3_secure != ae_consts.NOT_SET:\n            use_s3_secure = s3_secure\n\n        if s3_key and s3_bucket:\n            log.info(\n                f\'using - td s3://{use_s3_address}/\'\n                f\'{use_s3_bucket}/{use_s3_key}\')\n        else:\n            log.info(\n                f\'load - td s3://{use_s3_address}/\'\n                f\'{use_s3_bucket}/{use_s3_key}\')\n\n        load_res = load_history_utils.load_history_dataset(\n            s3_access_key=use_s3_access_key,\n            s3_secret_key=use_s3_secret_key,\n            s3_address=use_s3_address,\n            s3_region_name=use_s3_region,\n            s3_bucket=use_s3_bucket,\n            s3_key=use_s3_key,\n            s3_secure=use_s3_secure)\n\n        if self.ticker not in load_res:\n            log.critical(\n                \'failed to load history: \'\n                f\'s3://{self.s3_bucket}/{self.s3_key}\')\n            self.history_df = None\n            return\n\n        self.history_df = load_res[self.ticker]\n        self.determine_latest_times_in_history()\n\n        log.info(\n            f\'found - td s3://{use_s3_address}/\'\n            f\'{use_s3_bucket}/{use_s3_key} \'\n            f\'rows={len(self.history_df.index)}\')\n    # end of load_trading_history\n\n    def get_history(\n            self):\n        """"""get_history""""""\n        return self.history_df\n    # end of get_history\n\n    def get_latest_day(\n            self):\n        """"""get_latest_day""""""\n        if \'date\' in self.history_df:\n            return self.end_day\n        else:\n            return None\n    # end of get_latest_day\n\n    def get_latest_minute(\n            self):\n        """"""get_latest_minute""""""\n        if \'minute\' in self.history_df:\n            return self.end_date\n        else:\n            return None\n    # end of get_latest_minute\n\n    def publish_trading_history(\n            self,\n            records_for_history,\n            pt_s3_access_key=None,\n            pt_s3_secret_key=None,\n            pt_s3_address=None,\n            pt_s3_region=None,\n            pt_s3_bucket=None,\n            pt_s3_key=None,\n            pt_s3_secure=ae_consts.NOT_SET,\n            **kwargs):\n        """"""publish_trading_history\n\n        Helper for publishing a trading history\n        to another S3 service like AWS\n\n        :param records_for_history: list of dictionaries\n            for the history file\n        :param pt_s3_access_key: access key\n        :param pt_s3_secret_key: secret\n        :param pt_s3_address: address\n        :param pt_s3_region: region\n        :param pt_s3_bucket: bucket\n        :param pt_s3_key: key\n        :param pt_s3_secure: secure flag\n        :param kwargs: support for keyword arg dict\n        """"""\n        use_s3_access_key = self.pt_s3_access_key\n        use_s3_secret_key = self.pt_s3_secret_key\n        use_s3_address = self.pt_s3_address\n        use_s3_region = self.pt_s3_region\n        use_s3_bucket = self.pt_s3_bucket\n        use_s3_key = self.pt_s3_key\n        use_s3_secure = self.pt_s3_secure\n\n        use_s3_enabled = kwargs.get(\n            \'s3_enabled\', True)\n        use_redis_enabled = kwargs.get(\n            \'redis_enabled\', False)\n        use_redis_address = kwargs.get(\n            \'redis_address\', None)\n        use_redis_db = kwargs.get(\n            \'redis_db\', None)\n        use_redis_key = kwargs.get(\n            \'redis_key\', None)\n        use_redis_password = kwargs.get(\n            \'redis_password\', None)\n        use_redis_expire = kwargs.get(\n            \'redis_expire\', None)\n        use_redis_serializer = kwargs.get(\n            \'redis_serializer\', \'json\')\n        use_redis_encoding = kwargs.get(\n            \'redis_encoding\', \'utf-8\')\n        verbose = kwargs.get(\n            \'verbose\', False)\n\n        if pt_s3_access_key:\n            use_s3_access_key = pt_s3_access_key\n        if pt_s3_secret_key:\n            use_s3_secret_key = pt_s3_secret_key\n        if pt_s3_address:\n            use_s3_address = pt_s3_address\n        if pt_s3_region:\n            use_s3_region = pt_s3_region\n        if pt_s3_bucket:\n            use_s3_bucket = pt_s3_bucket\n        if pt_s3_key:\n            use_s3_key = pt_s3_key\n        if pt_s3_secure != ae_consts.NOT_SET:\n            use_s3_secure = pt_s3_secure\n\n        rec = {\n            \'tickers\': self.ticker,\n            \'version\': int(ae_consts.ALGO_HISTORY_VERSION),\n            \'last_trade_date\': ae_utils.get_last_close_str(),\n            \'algo_config_dict\': self.config_dict,\n            \'algo_name\':  self.use_name,\n            \'created\': ae_utils.utc_now_str(),\n            self.ticker: records_for_history\n        }\n\n        num_bytes = len(str(rec))\n        num_mb = ae_consts.get_mb(num_bytes)\n\n        msg = (\n            f\'publish - {self.ticker} - {rec[""last_trade_date""]} \'\n            # f\'{use_s3_access_key} with: {use_s3_secret_key} \'\n            f\'s3_loc={use_s3_address}/{use_s3_bucket}/{use_s3_key} \'\n            f\'mb={num_mb}MB\')\n        log.info(msg)\n\n        publish.publish(\n            data=rec,\n            label=\'pub\',\n            df_compress=True,\n            compress=False,\n            convert_to_dict=False,\n            output_file=None,\n            redis_enabled=use_redis_enabled,\n            redis_key=use_redis_key,\n            redis_address=use_redis_address,\n            redis_db=use_redis_db,\n            redis_password=use_redis_password,\n            redis_expire=use_redis_expire,\n            redis_serializer=use_redis_serializer,\n            redis_encoding=use_redis_encoding,\n            s3_enabled=use_s3_enabled,\n            s3_key=use_s3_key,\n            s3_address=use_s3_address,\n            s3_bucket=use_s3_bucket,\n            s3_access_key=use_s3_access_key,\n            s3_secret_key=use_s3_secret_key,\n            s3_region_name=use_s3_region,\n            s3_secure=use_s3_secure,\n            slack_enabled=False,\n            verbose=verbose)\n    # end of publish_trading_history\n\n    def latest(\n            self,\n            date_str=None,\n            start_row=-200,\n            extract_iex=True,\n            extract_yahoo=False,\n            extract_td=True,\n            verbose=False,\n            **kwargs):\n        """"""latest\n\n        Run the algorithm with the latest pricing data. Also\n        supports running a backtest for a historical date in\n        the pricing history (format ``YYYY-MM-DD``)\n\n        :param date_str: optional - string start date ``YYYY-MM-DD``\n            default is the latest close date\n        :param start_row: negative number of rows back\n            from the end of the list in the data\n            default is ``-200`` where this means the algorithm\n            will process the latest 200 rows in the minute\n            dataset\n        :param extract_iex: bool flag for extracting from ``IEX``\n        :param extract_yahoo: bool flag for extracting from ``Yahoo``\n            which is disabled as of 1/2019\n        :param extract_td: bool flag for extracting from ``Tradier``\n        :param verbose: bool flag for logs\n        :param kwargs: keyword arg dict\n        """"""\n        use_date_str = date_str\n        if not use_date_str:\n            use_date_str = ae_utils.get_last_close_str()\n\n        log.info(\n            f\'creating algo\')\n        self.algo_obj = base_algo.BaseAlgo(\n            ticker=self.config_dict[\'ticker\'],\n            balance=self.config_dict[\'balance\'],\n            commission=self.config_dict[\'commission\'],\n            name=self.use_name,\n            start_date=self.use_start_date,\n            end_date=self.use_end_date,\n            auto_fill=self.auto_fill,\n            config_dict=self.config_dict,\n            load_from_s3_bucket=self.load_from_s3_bucket,\n            load_from_s3_key=self.load_from_s3_key,\n            load_from_redis_key=self.load_from_redis_key,\n            load_from_file=self.load_from_file,\n            load_compress=self.load_compress,\n            load_publish=self.load_publish,\n            load_config=self.load_config,\n            report_redis_key=self.report_redis_key,\n            report_s3_bucket=self.report_s3_bucket,\n            report_s3_key=self.report_s3_key,\n            report_file=self.report_file,\n            report_compress=self.report_compress,\n            report_publish=self.report_publish,\n            report_config=self.report_config,\n            history_redis_key=self.history_redis_key,\n            history_s3_bucket=self.history_s3_bucket,\n            history_s3_key=self.history_s3_key,\n            history_file=self.history_file,\n            history_compress=self.history_compress,\n            history_publish=self.history_publish,\n            history_config=self.history_config,\n            extract_redis_key=self.extract_redis_key,\n            extract_s3_bucket=self.extract_s3_bucket,\n            extract_s3_key=self.extract_s3_key,\n            extract_file=self.extract_file,\n            extract_save_dir=self.extract_save_dir,\n            extract_compress=self.extract_compress,\n            extract_publish=self.extract_publish,\n            extract_config=self.extract_config,\n            publish_to_slack=self.publish_to_slack,\n            publish_to_s3=self.publish_to_s3,\n            publish_to_redis=self.publish_to_redis,\n            dataset_type=self.dataset_type,\n            serialize_datasets=self.serialize_datasets,\n            compress=self.compress,\n            encoding=self.encoding,\n            redis_enabled=self.redis_enabled,\n            redis_key=self.redis_key,\n            redis_address=self.redis_address,\n            redis_db=self.redis_db,\n            redis_password=self.redis_password,\n            redis_expire=self.redis_expire,\n            redis_serializer=self.redis_serializer,\n            redis_encoding=self.redis_encoding,\n            s3_enabled=self.s3_enabled,\n            s3_key=self.s3_key,\n            s3_address=self.s3_address,\n            s3_bucket=self.s3_bucket,\n            s3_access_key=self.s3_access_key,\n            s3_secret_key=self.s3_secret_key,\n            s3_region_name=self.s3_region_name,\n            s3_secure=self.s3_secure,\n            slack_enabled=self.slack_enabled,\n            slack_code_block=self.slack_code_block,\n            slack_full_width=self.slack_full_width,\n            dataset_publish_extract=self.extract_publish,\n            dataset_publish_history=self.history_publish,\n            dataset_publish_report=self.report_publish,\n            run_on_engine=self.run_on_engine,\n            auth_url=self.broker_url,\n            backend_url=self.backend_url,\n            include_tasks=self.include_tasks,\n            ssl_options=self.ssl_options,\n            transport_options=self.transport_options,\n            path_to_config_module=self.path_to_config_module,\n            timeseries=self.timeseries,\n            trade_strategy=self.trade_strategy,\n            verbose=False,\n            raise_on_err=self.raise_on_err)\n\n        log.info(\n            f\'run latest - start\')\n\n        ticker = self.config_dict[\'ticker\']\n        dataset_id = f\'{ticker}_{use_date_str}\'\n        self.common_fetch_vals[\'base_key\'] = dataset_id\n        verbose_extract = self.config_dict.get(\'verbose_extract\', False)\n        indicator_datasets = self.algo_obj.get_indicator_datasets()\n        ticker_data = build_dataset_node.build_dataset_node(\n            ticker=ticker,\n            date=use_date_str,\n            datasets=indicator_datasets,\n            service_dict=self.common_fetch_vals,\n            verbose=verbose_extract)\n\n        algo_data_req = {\n            ticker: [\n                {\n                    \'id\': dataset_id,  # id is currently the cache key in redis\n                    \'date\': use_date_str,  # used to confirm dates in asc order\n                    \'data\': ticker_data,\n                    \'start_row\': start_row\n                }\n            ]\n        }\n\n        if verbose:\n            log.info(\n                f\'extract - {dataset_id} \'\n                f\'dataset={len(algo_data_req[ticker])}\')\n\n        # this could be a separate celery task\n        try:\n            if verbose:\n                log.info(\n                    f\'handle_data START - {dataset_id}\')\n            self.algo_obj.handle_data(\n                data=algo_data_req)\n            if verbose:\n                log.info(\n                    f\'handle_data END - {dataset_id}\')\n        except Exception as e:\n            a_name = self.algo_obj.get_name()\n            a_debug_msg = self.algo_obj.get_debug_msg()\n            if not a_debug_msg:\n                a_debug_msg = \'debug message not set\'\n            # a_config_dict = ae_consts.ppj(self.algo_obj.config_dict)\n            msg = (\n                f\'{dataset_id} - algo={a_name} \'\n                f\'encountered exception in handle_data tickers={ticker} \'\n                f\'from ex={e} \'\n                f\'and failed during operation: {a_debug_msg}\')\n            log.critical(f\'{msg}\')\n        # end try/ex\n\n        log.info(\'run latest - create history\')\n\n        history_ds = self.algo_obj.create_history_dataset()\n        self.history_df = pd.DataFrame(history_ds[ticker])\n        self.determine_latest_times_in_history()\n\n        self.num_rows = len(self.history_df.index)\n\n        if verbose:\n            log.info(self.history_df[[\'minute\', \'close\']].tail(5))\n\n        log.info(\n            f\'run latest minute={self.end_date} - \'\n            f\'rows={self.num_rows} - done\')\n\n        return self.get_history()\n    # end of latest\n\n# end of AlgoRunner\n'"
analysis_engine/api_requests.py,0,"b'""""""\nHelpers and examples for supported API Requests that each Celery\nTask supports:\n\n- analysis_engine.work_tasks.get_new_pricing_data\n- analysis_engine.work_tasks.handle_pricing_update_task\n- analysis_engine.work_tasks.publish_pricing_update\n\n""""""\n\nimport datetime\nimport pandas as pd\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.utils as ae_utils\nimport analysis_engine.iex.consts as iex_consts\nimport analysis_engine.td.consts as td_consts\nimport analysis_engine.options_dates as opt_dates\n\n\ndef get_ds_dict(\n        ticker,\n        base_key=None,\n        ds_id=None,\n        label=None,\n        service_dict=None):\n    """"""get_ds_dict\n\n    Get a dictionary with all cache keys for a ticker and return\n    the dictionary. Use this method to decouple your apps\n    from the underlying cache key implementations (if you\n    do not need them).\n\n    :param ticker: ticker\n    :param base_key: optional - base key that is prepended\n                     in all cache keys\n    :param ds_id: optional - dataset id (useful for\n                  external database id)\n    :param label: optional - tracking label in the logs\n    :param service_dict: optional - parent call functions and Celery\n                         tasks can use this dictionary to seed the\n                         common service routes and endpoints. Refer\n                         to ``analysis_engine.consts.SERVICE_VALS``\n                         for automatically-copied over keys by this\n                         helper.\n    """"""\n\n    if not ticker:\n        raise Exception(\'please pass in a ticker\')\n\n    use_base_key = base_key\n    if not use_base_key:\n        last_str = ae_utils.get_last_close_str(\n            fmt=ae_consts.COMMON_DATE_FORMAT)\n        use_base_key = f\'{ticker}_{last_str}\'\n\n    date_str = ae_utils.utc_date_str(\n        fmt=ae_consts.COMMON_DATE_FORMAT)\n    now_str = ae_utils.utc_now_str(\n        fmt=ae_consts.COMMON_TICK_DATE_FORMAT)\n\n    daily_redis_key = (\n        f\'{use_base_key}_{ae_consts.DAILY_S3_BUCKET_NAME}\')\n    minute_redis_key = (\n        f\'{use_base_key}_{ae_consts.MINUTE_S3_BUCKET_NAME}\')\n    quote_redis_key = (\n        f\'{use_base_key}_{ae_consts.QUOTE_S3_BUCKET_NAME}\')\n    stats_redis_key = (\n        f\'{use_base_key}_{ae_consts.STATS_S3_BUCKET_NAME}\')\n    peers_redis_key = (\n        f\'{use_base_key}_{ae_consts.PEERS_S3_BUCKET_NAME}\')\n    news_iex_redis_key = (\n        f\'{use_base_key}_{ae_consts.NEWS_S3_BUCKET_NAME}1\')\n    financials_redis_key = (\n        f\'{use_base_key}_{ae_consts.FINANCIALS_S3_BUCKET_NAME}\')\n    earnings_redis_key = (\n        f\'{use_base_key}_{ae_consts.EARNINGS_S3_BUCKET_NAME}\')\n    dividends_redis_key = (\n        f\'{use_base_key}_{ae_consts.DIVIDENDS_S3_BUCKET_NAME}\')\n    company_redis_key = (\n        f\'{use_base_key}_{ae_consts.COMPANY_S3_BUCKET_NAME}\')\n    options_yahoo_redis_key = (\n        f\'{use_base_key}_{ae_consts.OPTIONS_S3_BUCKET_NAME}\')\n    call_options_yahoo_redis_key = (\n        f\'{use_base_key}_calls\')\n    put_options_yahoo_redis_key = (\n        f\'{use_base_key}_puts\')\n    pricing_yahoo_redis_key = (\n        f\'{use_base_key}_{ae_consts.PRICING_S3_BUCKET_NAME}\')\n    news_yahoo_redis_key = (\n        f\'{use_base_key}_{ae_consts.NEWS_S3_BUCKET_NAME}\')\n    call_options_td_redis_key = (\n        f\'{use_base_key}_tdcalls\')\n    put_options_td_redis_key = (\n        f\'{use_base_key}_tdputs\')\n\n    ds_cache_dict = {\n        \'daily\': daily_redis_key,\n        \'minute\': minute_redis_key,\n        \'quote\': quote_redis_key,\n        \'stats\': stats_redis_key,\n        \'peers\': peers_redis_key,\n        \'news1\': news_iex_redis_key,\n        \'financials\': financials_redis_key,\n        \'earnings\': earnings_redis_key,\n        \'dividends\': dividends_redis_key,\n        \'company\': company_redis_key,\n        \'options\': options_yahoo_redis_key,\n        \'calls\': call_options_yahoo_redis_key,\n        \'puts\': put_options_yahoo_redis_key,\n        \'pricing\': pricing_yahoo_redis_key,\n        \'news\': news_yahoo_redis_key,\n        \'tdcalls\': call_options_td_redis_key,\n        \'tdputs\': put_options_td_redis_key,\n        \'ticker\': ticker,\n        \'ds_id\': ds_id,\n        \'label\': label,\n        \'created\': now_str,\n        \'date\': date_str,\n        \'manifest_key\': use_base_key,\n        \'version\': ae_consts.CACHE_DICT_VERSION\n    }\n\n    # set keys/values for redis/minio from the\n    # service_dict - helper method for\n    # launching job chains\n    if service_dict:\n        for k in ae_consts.SERVICE_VALS:\n            if k in service_dict:\n                ds_cache_dict[k] = service_dict[k]\n\n    return ds_cache_dict\n# end of get_ds_dict\n\n\ndef build_get_new_pricing_request(\n        label=None):\n    """"""build_get_new_pricing_request\n\n    Build a sample Celery task API request:\n    analysis_engine.work_tasks.get_new_pricing_data\n\n    Used for testing: run_get_new_pricing_data(work)\n\n    :param label: log label to use\n    """"""\n    ticker = ae_consts.TICKER\n    ticker_id = ae_consts.TICKER_ID\n    base_key = f\'\'\'{ticker}_{datetime.datetime.utcnow().strftime(\n        \'%Y_%m_%d_%H_%M_%S\')}\'\'\'\n    s3_bucket_name = ae_consts.S3_BUCKET\n    s3_key = base_key\n    redis_key = base_key\n    use_strike = None\n    contract_type = \'C\'\n    get_pricing = True\n    get_news = True\n    get_options = True\n\n    work = {\n        \'ticker\': ticker,\n        \'ticker_id\': ticker_id,\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'strike\': use_strike,\n        \'contract\': contract_type,\n        \'get_pricing\': get_pricing,\n        \'get_news\': get_news,\n        \'get_options\': get_options\n    }\n\n    if label:\n        work[\'label\'] = label\n\n    return work\n# end of build_get_new_pricing_request\n\n\ndef build_cache_ready_pricing_dataset(\n        label=None):\n    """"""build_cache_ready_pricing_dataset\n\n    Build a cache-ready pricing dataset to replicate\n    the ``get_new_pricing_data`` task\n\n    :param label: log label to use\n    """"""\n\n    pricing_dict = {\n        \'ask\': 0.0,\n        \'askSize\': 8,\n        \'averageDailyVolume10Day\': 67116380,\n        \'averageDailyVolume3Month\': 64572187,\n        \'bid\': 0.0,\n        \'bidSize\': 10,\n        \'close\': 287.6,\n        \'currency\': \'USD\',\n        \'esgPopulated\': False,\n        \'exchange\': \'PCX\',\n        \'exchangeDataDelayedBy\': 0,\n        \'exchangeTimezoneName\': \'America/New_York\',\n        \'exchangeTimezoneShortName\': \'EDT\',\n        \'fiftyDayAverage\': 285.21735,\n        \'fiftyDayAverageChange\': 2.8726501,\n        \'fiftyDayAverageChangePercent\': 0.010071794,\n        \'fiftyTwoWeekHigh\': 291.74,\n        \'fiftyTwoWeekHighChange\': -3.649994,\n        \'fiftyTwoWeekHighChangePercent\': -0.012511119,\n        \'fiftyTwoWeekLow\': 248.02,\n        \'fiftyTwoWeekLowChange\': 40.069992,\n        \'fiftyTwoWeekLowChangePercent\': 0.16155952,\n        \'fiftyTwoWeekRange\': \'248.02 - 291.74\',\n        \'financialCurrency\': \'USD\',\n        \'fullExchangeName\': \'NYSEArca\',\n        \'gmtOffSetMilliseconds\': -14400000,\n        \'high\': 289.03,\n        \'language\': \'en-US\',\n        \'longName\': \'SPDR S&amp;P 500 ETF\',\n        \'low\': 287.88,\n        \'market\': \'us_market\',\n        \'marketCap\': 272023797760,\n        \'marketState\': \'POSTPOST\',\n        \'messageBoardId\': \'finmb_6160262\',\n        \'open\': 288.74,\n        \'postMarketChange\': 0.19998169,\n        \'postMarketChangePercent\': 0.06941398,\n        \'postMarketPrice\': 288.3,\n        \'postMarketTime\': 1536623987,\n        \'priceHint\': 2,\n        \'quoteSourceName\': \'Delayed Quote\',\n        \'quoteType\': \'ETF\',\n        \'region\': \'US\',\n        \'regularMarketChange\': 0.48999023,\n        \'regularMarketChangePercent\': 0.17037213,\n        \'regularMarketDayHigh\': 289.03,\n        \'regularMarketDayLow\': 287.88,\n        \'regularMarketDayRange\': \'287.88 - 289.03\',\n        \'regularMarketOpen\': 288.74,\n        \'regularMarketPreviousClose\': 287.6,\n        \'regularMarketPrice\': 288.09,\n        \'regularMarketTime\': 1536609602,\n        \'regularMarketVolume\': 50210903,\n        \'sharesOutstanding\': 944232000,\n        \'shortName\': \'SPDR S&P 500\',\n        \'sourceInterval\': 15,\n        \'symbol\': \'SPY\',\n        \'tradeable\': True,\n        \'trailingThreeMonthNavReturns\': 7.71,\n        \'trailingThreeMonthReturns\': 7.63,\n        \'twoHundredDayAverage\': 274.66153,\n        \'twoHundredDayAverageChange\': 13.428467,\n        \'twoHundredDayAverageChangePercent\': 0.048890963,\n        \'volume\': 50210903,\n        \'ytdReturn\': 9.84\n    }\n    calls_df_as_json = pd.DataFrame([{\n        \'ask\': 106,\n        \'bid\': 105.36,\n        \'change\': 4.0899963,\n        \'contractSize\': \'REGULAR\',\n        \'contractSymbol\': \'SPY181019P00380000\',\n        \'currency\': \'USD\',\n        \'expiration\': 1539907200,\n        \'impliedVolatility\': 1.5991230981,\n        \'inTheMoney\': True,\n        \'lastPrice\': 91.82,\n        \'lastTradeDate\': 1539027901,\n        \'openInterest\': 0,\n        \'percentChange\': 4.4543633,\n        \'strike\': 380,\n        \'volume\': 37\n    }]).to_json(\n        orient=\'records\')\n    puts_df_as_json = pd.DataFrame([{\n        \'ask\': 106,\n        \'bid\': 105.36,\n        \'change\': 4.0899963,\n        \'contractSize\': \'REGULAR\',\n        \'contractSymbol\': \'SPY181019P00380000\',\n        \'currency\': \'USD\',\n        \'expiration\': 1539907200,\n        \'impliedVolatility\': 1.5991230981,\n        \'inTheMoney\': True,\n        \'lastPrice\': 91.82,\n        \'lastTradeDate\': 1539027901,\n        \'openInterest\': 0,\n        \'percentChange\': 4.4543633,\n        \'strike\': 380,\n        \'volume\': 37\n    }]).to_json(\n        orient=\'records\')\n    news_list = [\n        {\n            \'d\': \'16 hours ago\',\n            \'s\': \'Yahoo Finance\',\n            \'sp\': \'Some Title 1\',\n            \'sru\': \'http://news.google.com/news/url?values\',\n            \'t\': \'Some Title 1\',\n            \'tt\': \'1493311950\',\n            \'u\': \'http://finance.yahoo.com/news/url1\',\n            \'usg\': \'ke1\'\n        },\n        {\n            \'d\': \'18 hours ago\',\n            \'s\': \'Yahoo Finance\',\n            \'sp\': \'Some Title 2\',\n            \'sru\': \'http://news.google.com/news/url?values\',\n            \'t\': \'Some Title 2\',\n            \'tt\': \'1493311950\',\n            \'u\': \'http://finance.yahoo.com/news/url2\',\n            \'usg\': \'key2\'\n        }\n    ]\n\n    options_dict = {\n        \'exp_date\': \'2018-10-19\',\n        \'calls\': calls_df_as_json,\n        \'puts\': puts_df_as_json,\n        \'num_calls\': 1,\n        \'num_puts\': 1\n    }\n\n    cache_data = {\n        \'news\': news_list,\n        \'options\': options_dict,\n        \'pricing\': pricing_dict\n    }\n    return cache_data\n# end of build_cache_ready_pricing_dataset\n\n\ndef build_publish_pricing_request(\n        label=None):\n    """"""build_publish_pricing_request\n\n    Build a sample Celery task API request:\n    analysis_engine.work_tasks.publisher_pricing_update\n\n    Used for testing: run_publish_pricing_update(work)\n\n    :param label: log label to use\n    """"""\n    ticker = ae_consts.TICKER\n    ticker_id = ae_consts.TICKER_ID\n    base_key = f\'\'\'{ticker}_{datetime.datetime.utcnow().strftime(\n        \'%Y_%m_%d_%H_%M_%S\')}\'\'\'\n    s3_bucket_name = ae_consts.S3_BUCKET\n    s3_key = base_key\n    redis_key = base_key\n    use_strike = None\n    contract_type = \'C\'\n    use_data = build_cache_ready_pricing_dataset()\n\n    work = {\n        \'ticker\': ticker,\n        \'ticker_id\': ticker_id,\n        \'strike\': use_strike,\n        \'contract\': contract_type,\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'df_compress\': True,\n        \'data\': use_data\n    }\n\n    if label:\n        work[\'label\'] = label\n\n    return work\n# end of build_publish_pricing_request\n\n\ndef build_publish_from_s3_to_redis_request(\n        label=None):\n    """"""build_publish_from_s3_to_redis_request\n\n    Build a sample Celery task API request:\n    analysis_engine.work_tasks.publish_from_s3_to_redis\n\n    Used for testing: run_publish_from_s3_to_redis(work)\n\n    :param label: log label to use\n    """"""\n    ticker = ae_consts.TICKER\n    ticker_id = ae_consts.TICKER_ID\n    base_key = f\'\'\'{ticker}_{datetime.datetime.utcnow().strftime(\n        \'%Y_%m_%d_%H_%M_%S\')}\'\'\'\n    s3_bucket_name = ae_consts.S3_BUCKET\n    s3_key = base_key\n    redis_key = base_key\n    s3_enabled = True\n    redis_enabled = True\n\n    work = {\n        \'ticker\': ticker,\n        \'ticker_id\': ticker_id,\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'s3_enabled\': s3_enabled,\n        \'redis_enabled\': redis_enabled\n    }\n\n    if label:\n        work[\'label\'] = label\n\n    return work\n# end of build_publish_from_s3_to_redis_request\n\n\ndef build_publish_ticker_aggregate_from_s3_request(\n        label=None):\n    """"""build_publish_ticker_aggregate_from_s3_request\n\n    Build a sample Celery task API request:\n    analysis_engine.work_tasks.publish_ticker_aggregate_from_s3\n\n    Used for testing: run_publish_ticker_aggregate_from_s3(work)\n\n    :param label: log label to use\n    """"""\n    ticker = ae_consts.TICKER\n    ticker_id = ae_consts.TICKER_ID\n    s3_bucket_name = ae_consts.S3_BUCKET\n    s3_compiled_bucket_name = ae_consts.S3_COMPILED_BUCKET\n    s3_key = f\'{ticker}_latest\'\n    redis_key = f\'{ticker}_latest\'\n    s3_enabled = True\n    redis_enabled = True\n\n    work = {\n        \'ticker\': ticker,\n        \'ticker_id\': ticker_id,\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_compiled_bucket\': s3_compiled_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'s3_enabled\': s3_enabled,\n        \'redis_enabled\': redis_enabled\n    }\n\n    if label:\n        work[\'label\'] = label\n\n    return work\n# end of build_publish_ticker_aggregate_from_s3_request\n\n\ndef build_prepare_dataset_request(\n        label=None):\n    """"""build_prepare_dataset_request\n\n    Build a sample Celery task API request:\n    analysis_engine.work_tasks.prepare_pricing_dataset\n\n    Used for testing: run_prepare_pricing_dataset(work)\n\n    :param label: log label to use\n    """"""\n    ticker = ae_consts.TICKER\n    ticker_id = ae_consts.TICKER_ID\n    base_key = f\'\'\'{ticker}_{datetime.datetime.utcnow().strftime(\n        \'%Y_%m_%d_%H_%M_%S\')}\'\'\'\n    s3_bucket_name = ae_consts.S3_BUCKET\n    s3_key = base_key\n    redis_key = base_key\n    s3_prepared_bucket_name = ae_consts.PREPARE_S3_BUCKET_NAME\n    s3_prepared_key = f\'{base_key}.csv\'\n    redis_prepared_key = f\'{base_key}\'\n    ignore_columns = None\n    s3_enabled = True\n    redis_enabled = True\n\n    work = {\n        \'ticker\': ticker,\n        \'ticker_id\': ticker_id,\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'prepared_s3_key\': s3_prepared_key,\n        \'prepared_s3_bucket\': s3_prepared_bucket_name,\n        \'prepared_redis_key\': redis_prepared_key,\n        \'ignore_columns\': ignore_columns,\n        \'s3_enabled\': s3_enabled,\n        \'redis_enabled\': redis_enabled\n    }\n\n    if label:\n        work[\'label\'] = label\n\n    return work\n# end of build_prepare_dataset_request\n\n\ndef build_analyze_dataset_request(\n        label=None):\n    """"""build_analyze_dataset_request\n\n    Build a sample Celery task API request:\n    analysis_engine.work_tasks.analyze_pricing_dataset\n\n    Used for testing: run_analyze_pricing_dataset(work)\n\n    :param label: log label to use\n    """"""\n    ticker = ae_consts.TICKER\n    ticker_id = ae_consts.TICKER_ID\n    base_key = f\'\'\'{ticker}_{datetime.datetime.utcnow().strftime(\n        \'%Y_%m_%d_%H_%M_%S\')}\'\'\'\n    s3_bucket_name = ae_consts.PREPARE_S3_BUCKET_NAME\n    s3_key = base_key\n    redis_key = base_key\n    s3_analyzed_bucket_name = ae_consts.ANALYZE_S3_BUCKET_NAME\n    s3_analyzed_key = f\'{base_key}.csv\'\n    redis_analyzed_key = f\'{base_key}\'\n    ignore_columns = None\n    s3_enabled = True\n    redis_enabled = True\n\n    work = {\n        \'ticker\': ticker,\n        \'ticker_id\': ticker_id,\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'analyzed_s3_key\': s3_analyzed_key,\n        \'analyzed_s3_bucket\': s3_analyzed_bucket_name,\n        \'analyzed_redis_key\': redis_analyzed_key,\n        \'ignore_columns\': ignore_columns,\n        \'s3_enabled\': s3_enabled,\n        \'redis_enabled\': redis_enabled\n    }\n\n    if label:\n        work[\'label\'] = label\n\n    return work\n# end of build_analyze_dataset_request\n\n\n""""""\nIEX API Requests\n""""""\n\n\ndef build_iex_fetch_daily_request(\n        label=None):\n    """"""build_iex_fetch_daily_request\n\n    Fetch `daily\n    data <https://iexcloud.io/docs/api/#historical-prices>`__\n    from IEX\n\n    :param label: log label to use\n    """"""\n    ticker = ae_consts.TICKER\n    base_key = f\'\'\'{ticker}_daily_{datetime.datetime.utcnow().strftime(\n        \'%Y_%m_%d_%H_%M_%S\')}\'\'\'\n    s3_bucket_name = ae_consts.DAILY_S3_BUCKET_NAME\n    s3_key = base_key\n    redis_key = base_key\n    s3_enabled = True\n    redis_enabled = True\n\n    work = {\n        \'ft_type\': iex_consts.FETCH_DAILY,\n        \'fd_type\': iex_consts.DATAFEED_DAILY,\n        \'ticker\': ticker,\n        \'timeframe\': \'5y\',\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'s3_enabled\': s3_enabled,\n        \'redis_enabled\': redis_enabled\n    }\n\n    if label:\n        work[\'label\'] = label\n\n    return work\n# end of build_iex_fetch_daily_request\n\n\ndef build_iex_fetch_minute_request(\n        label=None):\n    """"""build_iex_fetch_minute_request\n\n    Fetch `minute\n    data <https://iexcloud.io/docs/api/#historical-prices>`__\n    from IEX\n\n    :param label: log label to use\n    """"""\n    ticker = ae_consts.TICKER\n    base_key = f\'\'\'{ticker}_minute_{datetime.datetime.utcnow().strftime(\n        \'%Y_%m_%d_%H_%M_%S\')}\'\'\'\n    s3_bucket_name = ae_consts.MINUTE_S3_BUCKET_NAME\n    s3_key = base_key\n    redis_key = base_key\n    s3_enabled = True\n    redis_enabled = True\n\n    work = {\n        \'ft_type\': iex_consts.FETCH_MINUTE,\n        \'fd_type\': iex_consts.DATAFEED_MINUTE,\n        \'ticker\': ticker,\n        \'timeframe\': \'1d\',\n        \'last_close\': None,\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'s3_enabled\': s3_enabled,\n        \'redis_enabled\': redis_enabled\n    }\n\n    if label:\n        work[\'label\'] = label\n\n    return work\n# end of build_iex_fetch_minute_request\n\n\ndef build_iex_fetch_quote_request(\n        label=None):\n    """"""build_iex_fetch_quote_request\n\n    Fetch `quote\n    data <https://iexcloud.io/docs/api/#quote>`__\n    from IEX\n\n    :param label: log label to use\n    """"""\n    ticker = ae_consts.TICKER\n    base_key = f\'\'\'{ticker}_quote_{datetime.datetime.utcnow().strftime(\n        \'%Y_%m_%d_%H_%M_%S\')}\'\'\'\n    s3_bucket_name = ae_consts.QUOTE_S3_BUCKET_NAME\n    s3_key = base_key\n    redis_key = base_key\n    s3_enabled = True\n    redis_enabled = True\n\n    work = {\n        \'ft_type\': iex_consts.FETCH_QUOTE,\n        \'fd_type\': iex_consts.DATAFEED_QUOTE,\n        \'ticker\': ticker,\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'s3_enabled\': s3_enabled,\n        \'redis_enabled\': redis_enabled\n    }\n\n    if label:\n        work[\'label\'] = label\n\n    return work\n# end of build_iex_fetch_quote_request\n\n\ndef build_iex_fetch_stats_request(\n        label=None):\n    """"""build_iex_fetch_stats_request\n\n    Fetch `stats\n    data <https://iexcloud.io/docs/api/#key-stats>`__\n    from IEX\n\n    :param label: log label to use\n    """"""\n    ticker = ae_consts.TICKER\n    base_key = f\'\'\'{ticker}_stat_{datetime.datetime.utcnow().strftime(\n        \'%Y_%m_%d_%H_%M_%S\')}\'\'\'\n    s3_bucket_name = ae_consts.STATS_S3_BUCKET_NAME\n    s3_key = base_key\n    redis_key = base_key\n    s3_enabled = True\n    redis_enabled = True\n\n    work = {\n        \'ft_type\': iex_consts.FETCH_STATS,\n        \'fd_type\': iex_consts.DATAFEED_STATS,\n        \'ticker\': ticker,\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'s3_enabled\': s3_enabled,\n        \'redis_enabled\': redis_enabled\n    }\n\n    if label:\n        work[\'label\'] = label\n\n    return work\n# end of build_iex_fetch_stats_request\n\n\ndef build_iex_fetch_peers_request(\n        label=None):\n    """"""build_iex_fetch_peers_request\n\n    Fetch `peers\n    data <https://iexcloud.io/docs/api/#peers>`__\n    from IEX\n\n    :param label: log label to use\n    """"""\n    ticker = ae_consts.TICKER\n    base_key = f\'\'\'{ticker}_peer_{datetime.datetime.utcnow().strftime(\n        \'%Y_%m_%d_%H_%M_%S\')}\'\'\'\n    s3_bucket_name = ae_consts.PEERS_S3_BUCKET_NAME\n    s3_key = base_key\n    redis_key = base_key\n    s3_enabled = True\n    redis_enabled = True\n\n    work = {\n        \'ft_type\': iex_consts.FETCH_PEERS,\n        \'fd_type\': iex_consts.DATAFEED_PEERS,\n        \'ticker\': ticker,\n        \'timeframe\': \'1d\',\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'s3_enabled\': s3_enabled,\n        \'redis_enabled\': redis_enabled\n    }\n\n    if label:\n        work[\'label\'] = label\n\n    return work\n# end of build_iex_fetch_peers_request\n\n\ndef build_iex_fetch_news_request(\n        label=None):\n    """"""build_iex_fetch_news_request\n\n    Fetch `news\n    data <https://iexcloud.io/docs/api/#news>`__\n    from IEX\n\n    :param label: log label to use\n    """"""\n    ticker = ae_consts.TICKER\n    base_key = f\'\'\'{ticker}_news_{datetime.datetime.utcnow().strftime(\n        \'%Y_%m_%d_%H_%M_%S\')}\'\'\'\n    s3_bucket_name = ae_consts.NEWS_S3_BUCKET_NAME\n    s3_key = base_key\n    redis_key = base_key\n    s3_enabled = True\n    redis_enabled = True\n\n    work = {\n        \'ft_type\': iex_consts.FETCH_NEWS,\n        \'fd_type\': iex_consts.DATAFEED_NEWS,\n        \'ticker\': ticker,\n        \'timeframe\': \'1d\',\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'s3_enabled\': s3_enabled,\n        \'redis_enabled\': redis_enabled\n    }\n\n    if label:\n        work[\'label\'] = label\n\n    return work\n# end of build_iex_fetch_news_request\n\n\ndef build_iex_fetch_financials_request(\n        label=None):\n    """"""build_iex_fetch_financials_request\n\n    Fetch `financials\n    data <https://iexcloud.io/docs/api/#financials>`__\n    from IEX\n\n    :param label: log label to use\n    """"""\n    ticker = ae_consts.TICKER\n    base_key = f\'\'\'{ticker}_financial_{datetime.datetime.utcnow().strftime(\n        \'%Y_%m_%d_%H_%M_%S\')}\'\'\'\n    s3_bucket_name = ae_consts.FINANCIALS_S3_BUCKET_NAME\n    s3_key = base_key\n    redis_key = base_key\n    s3_enabled = True\n    redis_enabled = True\n\n    work = {\n        \'ft_type\': iex_consts.FETCH_FINANCIALS,\n        \'fd_type\': iex_consts.DATAFEED_FINANCIALS,\n        \'ticker\': ticker,\n        \'timeframe\': \'1d\',\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'s3_enabled\': s3_enabled,\n        \'redis_enabled\': redis_enabled\n    }\n\n    if label:\n        work[\'label\'] = label\n\n    return work\n# end of build_iex_fetch_financials_request\n\n\ndef build_iex_fetch_earnings_request(\n        label=None):\n    """"""build_iex_fetch_earnings_request\n\n    Fetch `earnings\n    data <https://iexcloud.io/docs/api/#earnings>`__\n    from IEX\n\n    :param label: log label to use\n    """"""\n    ticker = ae_consts.TICKER\n    base_key = f\'\'\'{ticker}_earning_{datetime.datetime.utcnow().strftime(\n        \'%Y_%m_%d_%H_%M_%S\')}\'\'\'\n    s3_bucket_name = ae_consts.EARNINGS_S3_BUCKET_NAME\n    s3_key = base_key\n    redis_key = base_key\n    s3_enabled = True\n    redis_enabled = True\n\n    work = {\n        \'ft_type\': iex_consts.FETCH_EARNINGS,\n        \'fd_type\': iex_consts.DATAFEED_EARNINGS,\n        \'ticker\': ticker,\n        \'timeframe\': \'1d\',\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'s3_enabled\': s3_enabled,\n        \'redis_enabled\': redis_enabled\n    }\n\n    if label:\n        work[\'label\'] = label\n\n    return work\n# end of build_iex_fetch_earnings_request\n\n\ndef build_iex_fetch_dividends_request(\n        label=None):\n    """"""build_iex_fetch_dividends_request\n\n    Fetch `dividends\n    data <https://iexcloud.io/docs/api/#dividends>`__\n    from IEX\n\n    :param label: log label to use\n    """"""\n    ticker = ae_consts.TICKER\n    base_key = f\'\'\'{ticker}_dividend_{datetime.datetime.utcnow().strftime(\n        \'%Y_%m_%d_%H_%M_%S\')}\'\'\'\n    s3_bucket_name = ae_consts.DIVIDENDS_S3_BUCKET_NAME\n    s3_key = base_key\n    redis_key = base_key\n    s3_enabled = True\n    redis_enabled = True\n\n    work = {\n        \'ft_type\': iex_consts.FETCH_DIVIDENDS,\n        \'fd_type\': iex_consts.DATAFEED_DIVIDENDS,\n        \'ticker\': ticker,\n        \'timeframe\': \'2y\',\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'s3_enabled\': s3_enabled,\n        \'redis_enabled\': redis_enabled\n    }\n\n    if label:\n        work[\'label\'] = label\n\n    return work\n# end of build_iex_fetch_dividends_request\n\n\ndef build_iex_fetch_company_request(\n        label=None):\n    """"""build_iex_fetch_company_request\n\n    Fetch `company\n    data <https://iexcloud.io/docs/api/#company>`__\n    from IEX\n\n    :param label: log label to use\n    """"""\n    ticker = ae_consts.TICKER\n    base_key = f\'\'\'{ticker}_company_{datetime.datetime.utcnow().strftime(\n        \'%Y_%m_%d_%H_%M_%S\')}\'\'\'\n    s3_bucket_name = ae_consts.COMPANY_S3_BUCKET_NAME\n    s3_key = base_key\n    redis_key = base_key\n    s3_enabled = True\n    redis_enabled = True\n\n    work = {\n        \'ft_type\': iex_consts.FETCH_COMPANY,\n        \'fd_type\': iex_consts.DATAFEED_COMPANY,\n        \'ticker\': ticker,\n        \'timeframe\': \'1d\',\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'s3_enabled\': s3_enabled,\n        \'redis_enabled\': redis_enabled\n    }\n\n    if label:\n        work[\'label\'] = label\n\n    return work\n# end of build_iex_fetch_company_request\n\n\ndef build_screener_analysis_request(\n        ticker=None,\n        tickers=None,\n        fv_urls=None,\n        fetch_mode=\'iex\',\n        iex_datasets=ae_consts.IEX_DATASETS_DEFAULT,\n        determine_sells=None,\n        determine_buys=None,\n        label=\'screener\'):\n    """"""build_screener_analysis_request\n\n    Build a dictionary request for the task:\n    ``analysis_engine.work_tasks.run_screener_analysis``\n\n    :param ticker: ticker to add to the analysis\n    :param tickers: tickers to add to the analysis\n    :param fv_urls: finviz urls\n    :param fetch_mode: supports pulling from ``iex``,\n        ``yahoo``, ``all`` (defaults to ``iex``)\n    :param iex_datasets: datasets to fetch from\n        ``iex`` (defaults to ``analysis_engine.con\n        sts.IEX_DATASETS_DEFAULT``)\n    :param determine_sells: string custom Celery task\n        name for handling sell-side processing\n    :param determine_buys: string custom Celery task\n        name for handling buy-side processing\n    :param label: log tracking label\n    :return: initial request dictionary:\n        ::\n\n            req = {\n                \'tickers\': use_tickers,\n                \'fv_urls\': use_urls,\n                \'fetch_mode\': fetch_mode,\n                \'iex_datasets\': iex_datasets,\n                \'s3_bucket\': s3_bucket_name,\n                \'s3_enabled\': s3_enabled,\n                \'redis_enabled\': redis_enabled,\n                \'determine_sells\': determine_sells,\n                \'determine_buys\': determine_buys,\n                \'label\': label\n            }\n    """"""\n    use_urls = []\n    if fv_urls:\n        for f in fv_urls:\n            if f not in use_urls:\n                use_urls.append(f)\n\n    use_tickers = tickers\n    if ticker:\n        if not tickers:\n            use_tickers = [\n                ticker\n            ]\n        if ticker.upper() not in use_tickers:\n            use_tickers.append(ticker.upper())\n\n    s3_bucket_name = ae_consts.SCREENER_S3_BUCKET_NAME\n    s3_enabled = True\n    redis_enabled = True\n\n    req = {\n        \'tickers\': use_tickers,\n        \'urls\': use_urls,\n        \'fetch_mode\': fetch_mode,\n        \'iex_datasets\': iex_datasets,\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_enabled\': s3_enabled,\n        \'redis_enabled\': redis_enabled,\n        \'determine_sells\': determine_sells,\n        \'determine_buys\': determine_buys,\n        \'label\': label\n    }\n    return req\n# end build_screener_analysis_request\n\n\n""""""\nTradier API Requests\n""""""\n\n\ndef build_td_fetch_calls_request(\n        label=None):\n    """"""build_td_fetch_calls_request\n\n    Fetch tradier calls\n\n    :param label: log label to use\n    """"""\n    ticker = ae_consts.TICKER\n    base_key = f\'\'\'{ticker}_tdcalls_{datetime.datetime.utcnow().strftime(\n        \'%Y_%m_%d_%H_%M_%S\')}\'\'\'\n    s3_bucket_name = \'tdcalls\'\n    s3_key = base_key\n    redis_key = base_key\n    s3_enabled = True\n    redis_enabled = True\n\n    exp_date = opt_dates.option_expiration().strftime(\n        ae_consts.COMMON_DATE_FORMAT)\n\n    work = {\n        \'ft_type\': td_consts.FETCH_TD_CALLS,\n        \'fd_type\': td_consts.DATAFEED_TD_CALLS,\n        \'ticker\': ticker,\n        \'exp_date\': exp_date,\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'s3_enabled\': s3_enabled,\n        \'redis_enabled\': redis_enabled\n    }\n\n    if label:\n        work[\'label\'] = label\n\n    return work\n# end of build_td_fetch_calls_request\n\n\ndef build_td_fetch_puts_request(\n        label=None):\n    """"""build_td_fetch_puts_request\n\n    Fetch tradier puts\n\n    :param label: log label to use\n    """"""\n    ticker = ae_consts.TICKER\n    base_key = f\'\'\'{ticker}_tdputs_{datetime.datetime.utcnow().strftime(\n        \'%Y_%m_%d_%H_%M_%S\')}\'\'\'\n    s3_bucket_name = \'tdputs\'\n    s3_key = base_key\n    redis_key = base_key\n    s3_enabled = True\n    redis_enabled = True\n\n    exp_date = opt_dates.option_expiration().strftime(\n        ae_consts.COMMON_DATE_FORMAT)\n\n    work = {\n        \'ft_type\': td_consts.FETCH_TD_PUTS,\n        \'fd_type\': td_consts.DATAFEED_TD_PUTS,\n        \'ticker\': ticker,\n        \'exp_date\': exp_date,\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'s3_enabled\': s3_enabled,\n        \'redis_enabled\': redis_enabled\n    }\n\n    if label:\n        work[\'label\'] = label\n\n    return work\n# end of build_td_fetch_puts_request\n'"
analysis_engine/build_algo_request.py,0,"b'""""""\nBuild a dictionary for running an algorithm\n""""""\n\nimport datetime\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.utils as ae_utils\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef build_algo_request(\n        ticker=None,\n        tickers=None,\n        use_key=None,\n        start_date=None,\n        end_date=None,\n        datasets=None,\n        balance=None,\n        commission=None,\n        num_shares=None,\n        config_file=None,\n        config_dict=None,\n        load_config=None,\n        history_config=None,\n        report_config=None,\n        extract_config=None,\n        timeseries=None,\n        trade_strategy=None,\n        cache_freq=\'daily\',\n        label=\'algo\'):\n    """"""build_algo_request\n\n    Create a dictionary for building an algorithm. This is\n    opinionated to how the underlying date-based caching\n    strategy is running per day. Each business day becomes\n    a possible dataset to process with an algorithm.\n\n    :param ticker: ticker\n    :param tickers: optional - list of tickers\n    :param use_key: redis and s3 to store the algo result\n    :param start_date: string date format ``YYYY-MM-DD HH:MM:SS``\n    :param end_date: string date format ``YYYY-MM-DD HH:MM:SS``\n    :param datasets: list of string dataset types\n    :param balance: starting capital balance\n    :param commission: commission for buy or sell\n    :param num_shares: optional - integer number of starting shares\n    :param cache_freq: optional - cache frequency\n        (``daily`` is default)\n    :param label: optional - algo log tracking name\n    :param config_file: path to a json file\n        containing custom algorithm object\n        member values (like indicator configuration and\n        predict future date units ahead for a backtest)\n    :param config_dict: optional - dictionary that\n        can be passed to derived class implementations\n        of: ``def load_from_config(config_dict=config_dict)``\n\n    **Timeseries**\n\n    :param timeseries: optional - string to\n        set ``day`` or ``minute`` backtesting\n        or live trading\n        (default is ``minute``)\n\n    **Trading Strategy**\n\n    :param trade_strategy: optional - string to\n        set the type of ``Trading Strategy``\n        for backtesting or live trading\n        (default is ``count``)\n\n    **Algorithm Dataset Extraction, Loading and Publishing arguments**\n\n    :param load_config: optional - dictionary\n        for setting member variables to load an\n        agorithm-ready dataset from\n        a file, s3 or redis\n    :param history_config: optional - dictionary\n        for setting member variables to publish\n        an algo ``trade history`` to s3, redis, a file\n        or slack\n    :param report_config: optional - dictionary\n        for setting member variables to publish\n        an algo ``trading performance report`` to s3,\n        redis, a file or slack\n    :param extract_config: optional - dictionary\n        for setting member variables to publish\n        an algo ``trading performance report`` to s3,\n        redis, a file or slack\n\n    """"""\n    use_tickers = []\n    if ticker:\n        use_tickers = [\n            ticker.upper()\n        ]\n    if tickers:\n        for t in tickers:\n            if t not in use_tickers:\n                use_tickers.append(t.upper())\n\n    s3_bucket_name = ae_consts.ALGO_RESULT_S3_BUCKET_NAME\n    s3_key = use_key\n    redis_key = use_key\n    s3_enabled = True\n    redis_enabled = True\n\n    work = {\n        \'tickers\': use_tickers,\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'s3_enabled\': s3_enabled,\n        \'redis_enabled\': redis_enabled,\n        \'extract_datasets\': [],\n        \'cache_freq\': cache_freq,\n        \'config_file\': config_file,\n        \'config_dict\': config_dict,\n        \'balance\': balance,\n        \'commission\': commission,\n        \'load_config\': load_config,\n        \'history_config\': history_config,\n        \'report_config\': report_config,\n        \'extract_config\': extract_config,\n        \'start_date\': None,\n        \'end_date\': None,\n        \'timeseries\': timeseries,\n        \'trade_strategy\': trade_strategy,\n        \'version\': 1,\n        \'label\': label\n    }\n\n    start_date_val = ae_utils.get_date_from_str(start_date)\n    end_date_val = ae_utils.get_date_from_str(end_date)\n    if start_date_val > end_date_val:\n        raise Exception(\n            f\'Invalid start_date={start_date} \'\n            f\'must be less than end_date={end_date}\')\n\n    use_dates = []\n    new_dataset = None\n    cur_date = start_date_val\n    if not work[\'start_date\']:\n        work[\'start_date\'] = start_date_val.strftime(\n            ae_consts.COMMON_TICK_DATE_FORMAT)\n    if not work[\'end_date\']:\n        work[\'end_date\'] = end_date_val.strftime(\n            ae_consts.COMMON_TICK_DATE_FORMAT)\n    while cur_date <= end_date_val:\n        if cur_date.weekday() < 5:\n            for t in use_tickers:\n                if cache_freq == \'daily\':\n                    new_dataset = f\'\'\'{t}_{cur_date.strftime(\n                        ae_consts.COMMON_DATE_FORMAT)}\'\'\'\n                else:\n                    new_dataset = f\'\'\'{t}_{cur_date.strftime(\n                        ae_consts.COMMON_TICK_DATE_FORMAT)}\'\'\'\n                if new_dataset:\n                    use_dates.append(new_dataset)\n                new_dataset = None\n            # end for all tickers\n        # end of valid days M-F\n        if cache_freq == \'daily\':\n            cur_date += datetime.timedelta(days=1)\n        else:\n            cur_date += datetime.timedelta(minute=1)\n    # end of walking all dates to add\n\n    if len(use_dates) > 0:\n        work[\'extract_datasets\'] = use_dates\n\n        log.debug(\n            f\'tickers={work[""tickers""]} balance={work[""balance""]} \'\n            f\'start={work[""extract_datasets""][0]} \'\n            f\'end={work[""extract_datasets""][-1]} \'\n            f\'cache_freq={cache_freq} request={ae_consts.ppj(work)}\')\n    else:\n        log.error(\n            f\'there are not enough dates to test between \'\n            f\'start={start_date_val} end={end_date_val} \'\n            f\'tickers={work[""tickers""]} cache_freq={cache_freq} \'\n            f\'request={ae_consts.ppj(work)}\')\n\n    return work\n# end of build_algo_request\n'"
analysis_engine/build_buy_order.py,0,"b'""""""\nHelper for creating a buy order\n""""""\n\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.utils as ae_utils\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef build_buy_order(\n        ticker,\n        num_owned,\n        close,\n        balance,\n        commission,\n        date,\n        details,\n        use_key,\n        minute=None,\n        shares=None,\n        version=1,\n        auto_fill=True,\n        is_live_trading=False,\n        backtest_shares_default=10,\n        reason=None):\n    """"""build_buy_order\n\n    Create an algorithm buy order as a dictionary\n\n    .. note:: setting the ``minute`` is required to build\n        a minute-by-minute ``Trading History``\n\n    :param ticker: ticker\n    :param num_owned: integer current owned\n        number of shares for this asset\n    :param close: float closing price of the asset\n    :param balance: float amount of available capital\n    :param commission: float for commission costs\n    :param date: string trade date for that row usually\n        ``COMMON_DATE_FORMAT`` (``YYYY-MM-DD``)\n    :param minute: optional - string with the minute that the\n        order was placed. format is\n        ``COMMON_TICK_DATE_FORMAT`` (``YYYY-MM-DD HH:MM:SS``)\n    :param details: dictionary for full row of values to review\n        all buys after the algorithm finishes.\n        (usually ``row.to_json()``)\n    :param use_key: string for redis and s3 publishing of the algorithm\n        result dictionary as a json-serialized dictionary\n    :param shares: optional - integer number of shares to buy\n        if None buy max number of shares at the ``close`` with the\n        available ``balance`` amount.\n    :param version: optional - version tracking integer\n    :param auto_fill: optional - bool for not assuming the trade\n        filled (default ``True``)\n    :param is_live_trading: optional - bool for filling trades\n        for live trading or for backtest tuning filled\n        (default ``False`` which is backtest mode)\n    :param backtest_shares_default: optional - integer for\n        simulating shares during a backtest even if there\n        are not enough funds\n        (default ``10``)\n    :param reason: optional - string for recording why the algo\n        decided to buy for review after the algorithm finishes\n    """"""\n    status = ae_consts.TRADE_OPEN\n    s3_bucket_name = ae_consts.ALGO_BUYS_S3_BUCKET_NAME\n    s3_key = use_key\n    redis_key = use_key\n    s3_enabled = True\n    redis_enabled = True\n\n    cost_of_trade = None\n    new_shares = num_owned\n    new_balance = balance\n    created_date = None\n\n    tradable_funds = balance - (2.0 * commission)\n\n    if not is_live_trading:\n        if not shares:\n            shares = backtest_shares_default\n        tradable_funds = (\n            (shares * close) + (2.0 * commission))\n\n    if close > 0.1 and tradable_funds > 10.0:\n        can_buy_num_shares = shares\n        if not can_buy_num_shares:\n            can_buy_num_shares = int(tradable_funds / close)\n        cost_of_trade = ae_consts.to_f(\n            val=(can_buy_num_shares * close) + commission)\n        if can_buy_num_shares > 0:\n            if cost_of_trade > balance:\n                status = ae_consts.TRADE_NOT_ENOUGH_FUNDS\n            else:\n                created_date = ae_utils.utc_now_str()\n                if auto_fill:\n                    new_shares = int(num_owned + can_buy_num_shares)\n                    new_balance = ae_consts.to_f(balance - cost_of_trade)\n                    status = ae_consts.TRADE_FILLED\n                else:\n                    new_shares = shares\n                    new_balance = balance\n        else:\n            status = ae_consts.TRADE_NOT_ENOUGH_FUNDS\n    else:\n        status = ae_consts.TRADE_NOT_ENOUGH_FUNDS\n\n    order_dict = {\n        \'ticker\': ticker,\n        \'status\': status,\n        \'balance\': new_balance,\n        \'shares\': new_shares,\n        \'buy_price\': cost_of_trade,\n        \'prev_balance\': balance,\n        \'prev_shares\': num_owned,\n        \'close\': close,\n        \'details\': details,\n        \'reason\': reason,\n        \'date\': date,\n        \'minute\': minute,\n        \'created\': created_date,\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'s3_enabled\': s3_enabled,\n        \'redis_enabled\': redis_enabled,\n        \'version\': version\n    }\n\n    use_date = minute\n    if not use_date:\n        use_date = date\n\n    log.debug(\n        f\'{ticker} {use_date} buy \'\n        f\'{ae_consts.get_status(status=order_dict[""status""])} \'\n        f\'order={ae_consts.ppj(order_dict)}\')\n\n    return order_dict\n# end of build_buy_order\n'"
analysis_engine/build_dataset_node.py,0,"b'""""""\nBuild a dictionary by extracting all required pricing datasets\nfor the algorithm\'s indicators out of Redis\n\nThis dictionary should be passed to an algorithm\'s ``handle_data``\nmethod like:\n\n.. code-block:: python\n\n    algo.handle_data(build_dataset_node())\n""""""\n\nimport pandas as pd\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.utils as ae_utils\nimport analysis_engine.api_requests as api_requests\nimport analysis_engine.iex.extract_df_from_redis as iex_extract_utils\nimport analysis_engine.td.extract_df_from_redis as td_extract_utils\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef build_dataset_node(\n        ticker,\n        datasets,\n        date=None,\n        service_dict=None,\n        log_label=None,\n        redis_enabled=True,\n        redis_address=None,\n        redis_db=None,\n        redis_password=None,\n        redis_expire=None,\n        redis_key=None,\n        s3_enabled=True,\n        s3_address=None,\n        s3_bucket=None,\n        s3_access_key=None,\n        s3_secret_key=None,\n        s3_region_name=None,\n        s3_secure=False,\n        s3_key=None,\n        verbose=False):\n    """"""build_dataset_node\n\n    Helper for building a dictionary that of\n    cached datasets from redis.\n\n    The datasets should be built from\n    off the algorithm\'s config indicators\n    ``uses_data`` fields which if not\n    set will default to ``minute`` data\n\n    :param ticker: string ticker\n    :param datasets: list of string dataset names\n        to extract from redis\n    :param date: optional - string datetime formatted\n        ``YYYY-MM-DD``\n        (default is last trading close date)\n    :param service_dict: optional - dictionary for all\n        service connectivity to Redis and Minio if not\n        set the arguments for all ``s3_*`` and ``redis_*``\n        will be used to lookup data in Redis and Minio\n\n    **(Optional) Redis connectivity arguments**\n\n    :param redis_enabled: bool - toggle for auto-caching all\n        datasets in Redis\n        (default is ``True``)\n    :param redis_address: Redis connection string\n        format is ``host:port``\n        (default is ``localhost:6379``)\n    :param redis_db: Redis db to use\n        (default is ``0``)\n    :param redis_password: optional - Redis password\n        (default is ``None``)\n    :param redis_expire: optional - Redis expire value\n        (default is ``None``)\n    :param redis_key: optional - redis key not used\n        (default is ``None``)\n\n    :param s3_enabled: bool - toggle for turning on/off\n        Minio or AWS S3\n        (default is ``True``)\n    :param s3_address: Minio S3 connection string address\n        format is ``host:port``\n        (default is ``localhost:9000``)\n    :param s3_bucket: S3 Bucket for storing the artifacts\n        (default is ``dev``) which should be viewable on a browser:\n        http://localhost:9000/minio/dev/\n    :param s3_access_key: S3 Access key\n        (default is ``trexaccesskey``)\n    :param s3_secret_key: S3 Secret key\n        (default is ``trex123321``)\n    :param s3_region_name: S3 region name\n        (default is ``us-east-1``)\n    :param s3_secure: Transmit using tls encryption\n        (default is ``False``)\n    :param s3_key: optional s3 key not used\n        (default is ``None``)\n\n    **Debugging**\n\n    :param log_label: optional - log label string\n    :param verbose: optional - flag for debugging\n        (default to ``False``)\n    """"""\n\n    label = log_label\n    if not label:\n        label = \'build_bt\'\n\n    if not date:\n        date = ae_utils.get_last_close_str()\n\n    td_convert_to_datetime = (\n        ae_consts.TRADIER_CONVERT_TO_DATETIME)\n\n    date_key = f\'{ticker}_{date}\'\n\n    base_req = api_requests.get_ds_dict(\n        ticker=ticker,\n        base_key=date_key,\n        ds_id=label,\n        service_dict=service_dict)\n\n    if not service_dict:\n        base_req[\'redis_enabled\'] = redis_enabled\n        base_req[\'redis_address\'] = (\n            redis_address if redis_address else ae_consts.REDIS_ADDRESS)\n        base_req[\'redis_password\'] = (\n            redis_password if redis_password else ae_consts.REDIS_PASSWORD)\n        base_req[\'redis_db\'] = (\n            redis_db if redis_db else ae_consts.REDIS_DB)\n        base_req[\'redis_expire\'] = (\n            redis_expire if redis_expire else ae_consts.REDIS_EXPIRE)\n        base_req[\'s3_enabled\'] = s3_enabled\n        base_req[\'s3_bucket\'] = (\n            s3_bucket if s3_bucket else ae_consts.S3_BUCKET)\n        base_req[\'s3_address\'] = (\n            s3_address if s3_address else ae_consts.S3_ADDRESS)\n        base_req[\'s3_secure\'] = (\n            s3_secure if s3_secure else ae_consts.S3_SECURE)\n        base_req[\'s3_region_name\'] = (\n            s3_region_name if s3_region_name else ae_consts.S3_REGION_NAME)\n        base_req[\'s3_access_key\'] = (\n            s3_access_key if s3_access_key else ae_consts.S3_ACCESS_KEY)\n        base_req[\'s3_secret_key\'] = (\n            s3_secret_key if s3_secret_key else ae_consts.S3_SECRET_KEY)\n        base_req[\'redis_key\'] = date_key\n        base_req[\'s3_key\'] = date_key\n\n    if verbose:\n        log.info(\n            f\'extracting {date_key} req: {base_req}\')\n        """"""\n        for showing connectivity args in the logs\n        log.debug(\n            f\'bt {date_key} {ae_consts.ppj(base_req)}\')\n        """"""\n\n    iex_daily_status = ae_consts.FAILED\n    iex_minute_status = ae_consts.FAILED\n    iex_quote_status = ae_consts.FAILED\n    iex_stats_status = ae_consts.FAILED\n    iex_peers_status = ae_consts.FAILED\n    iex_news_status = ae_consts.FAILED\n    iex_financials_status = ae_consts.FAILED\n    iex_earnings_status = ae_consts.FAILED\n    iex_dividends_status = ae_consts.FAILED\n    iex_company_status = ae_consts.FAILED\n    td_calls_status = ae_consts.FAILED\n    td_puts_status = ae_consts.FAILED\n\n    iex_daily_df = None\n    iex_minute_df = None\n    iex_quote_df = None\n    iex_stats_df = None\n    iex_peers_df = None\n    iex_news_df = None\n    iex_financials_df = None\n    iex_earnings_df = None\n    iex_dividends_df = None\n    iex_company_df = None\n    td_calls_df = None\n    td_puts_df = None\n\n    if \'daily\' in datasets:\n        iex_daily_status, iex_daily_df = \\\n            iex_extract_utils.extract_daily_dataset(\n                ticker=ticker,\n                date=date,\n                work_dict=base_req,\n                verbose=verbose)\n        if iex_daily_status != ae_consts.SUCCESS:\n            if verbose:\n                log.warn(f\'unable to extract iex_daily={ticker}\')\n    if \'minute\' in datasets:\n        iex_minute_status, iex_minute_df = \\\n            iex_extract_utils.extract_minute_dataset(\n                ticker=ticker,\n                date=date,\n                work_dict=base_req,\n                verbose=verbose)\n        if iex_minute_status != ae_consts.SUCCESS:\n            if verbose:\n                log.warn(f\'unable to extract iex_minute={ticker}\')\n    if \'quote\' in datasets:\n        iex_quote_status, iex_quote_df = \\\n            iex_extract_utils.extract_quote_dataset(\n                ticker=ticker,\n                date=date,\n                work_dict=base_req,\n                verbose=verbose)\n        if iex_quote_status != ae_consts.SUCCESS:\n            if verbose:\n                log.warn(f\'unable to extract iex_quote={ticker}\')\n    if \'stats\' in datasets:\n        iex_stats_df, iex_stats_df = \\\n            iex_extract_utils.extract_stats_dataset(\n                ticker=ticker,\n                date=date,\n                work_dict=base_req,\n                verbose=verbose)\n        if iex_stats_status != ae_consts.SUCCESS:\n            if verbose:\n                log.warn(f\'unable to extract iex_stats={ticker}\')\n    if \'peers\' in datasets:\n        iex_peers_df, iex_peers_df = \\\n            iex_extract_utils.extract_peers_dataset(\n                ticker=ticker,\n                date=date,\n                work_dict=base_req,\n                verbose=verbose)\n        if iex_peers_status != ae_consts.SUCCESS:\n            if verbose:\n                log.warn(f\'unable to extract iex_peers={ticker}\')\n    if \'news\' in datasets:\n        iex_news_status, iex_news_df = \\\n            iex_extract_utils.extract_news_dataset(\n                ticker=ticker,\n                date=date,\n                work_dict=base_req,\n                verbose=verbose)\n        if iex_news_status != ae_consts.SUCCESS:\n            if verbose:\n                log.warn(f\'unable to extract iex_news={ticker}\')\n    if \'financials\' in datasets:\n        iex_financials_status, iex_financials_df = \\\n            iex_extract_utils.extract_financials_dataset(\n                ticker=ticker,\n                date=date,\n                work_dict=base_req,\n                verbose=verbose)\n        if iex_financials_status != ae_consts.SUCCESS:\n            if verbose:\n                log.warn(f\'unable to extract iex_financials={ticker}\')\n    if \'earnings\' in datasets:\n        iex_earnings_status, iex_earnings_df = \\\n            iex_extract_utils.extract_earnings_dataset(\n                ticker=ticker,\n                date=date,\n                work_dict=base_req,\n                verbose=verbose)\n        if iex_earnings_status != ae_consts.SUCCESS:\n            if verbose:\n                log.warn(f\'unable to extract iex_earnings={ticker}\')\n    if \'dividends\' in datasets:\n        iex_dividends_status, iex_dividends_df = \\\n            iex_extract_utils.extract_dividends_dataset(\n                ticker=ticker,\n                date=date,\n                work_dict=base_req,\n                verbose=verbose)\n        if iex_dividends_status != ae_consts.SUCCESS:\n            if verbose:\n                log.warn(f\'unable to extract iex_dividends={ticker}\')\n    if \'company\' in datasets:\n        iex_company_status, iex_company_df = \\\n            iex_extract_utils.extract_company_dataset(\n                ticker=ticker,\n                date=date,\n                work_dict=base_req,\n                verbose=verbose)\n        if iex_company_status != ae_consts.SUCCESS:\n            if verbose:\n                log.warn(f\'unable to extract iex_company={ticker}\')\n    # end of iex extracts\n\n    """"""\n    Yahoo disabled on Jan 2019\n\n    yahoo_news_status = ae_consts.FAILED\n    yahoo_options_status = ae_consts.FAILED\n    yahoo_pricing_status = ae_consts.FAILED\n    yahoo_option_calls_df = None\n    yahoo_option_puts_df = None\n    yahoo_pricing_df = None\n    yahoo_news_df = None\n\n    if \'options\' in datasets:\n        yahoo_options_status, yahoo_option_calls_df = \\\n            yahoo_extract_utils.extract_option_calls_dataset(\n                base_req)\n        yahoo_options_status, yahoo_option_puts_df = \\\n            yahoo_extract_utils.extract_option_puts_dataset(\n                base_req)\n        if yahoo_options_status != ae_consts.SUCCESS:\n            if verbose:\n                log.warn(f\'unable to extract yahoo_options={ticker}\')\n    if \'pricing\' in datasets:\n        yahoo_pricing_status, yahoo_pricing_df = \\\n            yahoo_extract_utils.extract_pricing_dataset(\n                base_req)\n        if yahoo_pricing_status != ae_consts.SUCCESS:\n            if verbose:\n                log.warn(f\'unable to extract yahoo_pricing={ticker}\')\n    if \'news\' in datasets:\n        yahoo_news_status, yahoo_news_df = \\\n            yahoo_extract_utils.extract_yahoo_news_dataset(\n                base_req)\n        if yahoo_news_status != ae_consts.SUCCESS:\n            if verbose:\n                log.warn(f\'unable to extract yahoo_news={ticker}\')\n    # end of yahoo extracts\n    """"""\n\n    """"""\n    Tradier Extraction\n    Debug by setting:\n\n    base_req[\'verbose_td\'] = True\n    """"""\n    if (\n            \'calls\' in datasets or\n            \'tdcalls\' in datasets):\n        td_calls_status, td_calls_df = \\\n            td_extract_utils.extract_option_calls_dataset(\n                ticker=ticker,\n                date=date,\n                work_dict=base_req,\n                verbose=verbose)\n        if td_calls_status != ae_consts.SUCCESS:\n            if verbose:\n                log.warn(f\'unable to extract tdcalls={ticker}\')\n        else:\n            if ae_consts.is_df(\n                    df=td_calls_df):\n                for c in td_convert_to_datetime:\n                    if c in td_calls_df:\n                        td_calls_df[c] = pd.to_datetime(\n                            td_calls_df[c],\n                            format=ae_consts.COMMON_TICK_DATE_FORMAT)\n                if \'date\' in td_calls_df:\n                    td_calls_df.sort_values(\n                        \'date\',\n                        ascending=True)\n        # end of converting dates\n    # end of Tradier calls extraction\n\n    if (\n            \'puts\' in datasets or\n            \'tdputs\' in datasets):\n        td_puts_status, td_puts_df = \\\n            td_extract_utils.extract_option_puts_dataset(\n                ticker=ticker,\n                date=date,\n                work_dict=base_req,\n                verbose=verbose)\n        if td_puts_status != ae_consts.SUCCESS:\n            if verbose:\n                log.warn(f\'unable to extract tdputs={ticker}\')\n        else:\n            if ae_consts.is_df(\n                    df=td_puts_df):\n                for c in td_convert_to_datetime:\n                    if c in td_puts_df:\n                        td_puts_df[c] = pd.to_datetime(\n                            td_puts_df[c],\n                            format=ae_consts.COMMON_TICK_DATE_FORMAT)\n                if \'date\' in td_puts_df:\n                    td_puts_df.sort_values(\n                        \'date\',\n                        ascending=True)\n        # end of converting dates\n    # end of Tradier puts extraction\n\n    ticker_data = {\n        \'daily\': iex_daily_df,\n        \'minute\': iex_minute_df,\n        \'quote\': iex_quote_df,\n        \'stats\': iex_stats_df,\n        \'peers\': iex_peers_df,\n        \'news1\': iex_news_df,\n        \'financials\': iex_financials_df,\n        \'earnings\': iex_earnings_df,\n        \'dividends\': iex_dividends_df,\n        \'company\': iex_company_df,\n        \'tdcalls\': td_calls_df,\n        \'tdputs\': td_puts_df,\n        \'calls\': None,  # yahoo - here for legacy\n        \'news\': None,  # yahoo - here for legacy\n        \'pricing\': None,  # yahoo - here for legacy\n        \'puts\': None  # yahoo - here for legacy\n    }\n\n    return ticker_data\n# end of build_dataset_node\n'"
analysis_engine/build_df_from_redis.py,0,"b'""""""\nHelper for getting json-serialized pandas DataFrames\nfrom redis\n\nDebug redis calls with:\n\n::\n\n    export DEBUG_REDIS=1\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n\n""""""\n\nimport redis\nimport pandas as pd\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.build_result as build_result\nimport analysis_engine.get_data_from_redis_key as redis_get\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef build_df_from_redis(\n        label=None,\n        client=None,\n        address=None,\n        host=None,\n        port=None,\n        password=None,\n        db=None,\n        key=None,\n        expire=None,\n        is_compressed=True,\n        serializer=\'json\',\n        encoding=\'utf-8\',\n        orient=\'records\',\n        verbose=False):\n    """"""build_df_from_redis\n\n    :param label: log tracking label\n    :param client: initialized redis client\n    :param address: redis address: <host:port>\n    :param host: redis host\n    :param port: redis port\n    :param password: redis password\n    :param db: redis db\n    :param key: redis key\n    :param expire: not used yet - redis expire\n    :param is_compressed: optional boolean - the\n        object is a compressed string and the\n        default is ``True``\n    :param serializer: support for future\n        pickle objects in redis\n    :param encoding: format of the encoded key in redis\n    :param orient: use the same orient value as\n        the ``to_json(orient=\'records\')`` used\n        to deserialize the DataFrame correctly.\n    :param verbose: optional - boolean for turning on logging\n    """"""\n\n    data = None\n    valid_df = False\n    df = None\n\n    rec = {\n        \'valid_df\': valid_df,\n        \'data\': data\n    }\n    res = build_result.build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec=rec)\n\n    log_id = label if label else \'build-df\'\n\n    try:\n        if verbose:\n            log.info(f\'{log_id} calling get redis key={key}\')\n\n        use_host = host\n        use_port = port\n        if not use_host and not use_port:\n            if address:\n                use_host = address.split(\':\')[0]\n                use_port = int(address.split(\':\')[1])\n\n        use_client = client\n        if not use_client:\n            if verbose:\n                log.debug(\n                    f\'{log_id} connecting to redis={use_host}:{use_port}@{db}\')\n            use_client = redis.Redis(\n                host=use_host,\n                port=use_port,\n                password=password,\n                db=db)\n\n        redis_res = redis_get.get_data_from_redis_key(\n            label=log_id,\n            client=use_client,\n            host=use_host,\n            port=use_port,\n            password=password,\n            db=db,\n            key=key,\n            expire=expire,\n            decompress_df=is_compressed,\n            serializer=\'json\',\n            encoding=encoding)\n\n        valid_df = False\n        if redis_res[\'status\'] == ae_consts.SUCCESS:\n            data = redis_res[\'rec\'].get(\n                \'data\',\n                None)\n            if data:\n                if ae_consts.ev(\'DEBUG_REDIS\', \'0\') == \'1\':\n                    log.debug(\n                        f\'{log_id} - found key={key} \'\n                        f\'data={ae_consts.ppj(data)}\')\n                else:\n                    if verbose:\n                        log.info(\n                            f\'{log_id} - loading df from key={key}\')\n                    df = pd.read_json(\n                        data,\n                        orient=\'records\')\n                    valid_df = True\n            else:\n                if verbose:\n                    log.info(f\'{log_id} key={key} no data\')\n            # if data\n\n            rec[\'data\'] = df\n            rec[\'valid_df\'] = valid_df\n\n            res = build_result.build_result(\n                status=ae_consts.SUCCESS,\n                err=None,\n                rec=rec)\n            return res\n        else:\n            if verbose:\n                log.info(f\'{log_id} no data key={key}\')\n            res = build_result.build_result(\n                status=ae_consts.SUCCESS,\n                err=None,\n                rec=rec)\n            return res\n    except Exception as e:\n        err = (\n            f\'{log_id} failed - build_df_from_redis data={data == ""0""} \'\n            f\'key={key} ex={e}\')\n        log.error(err)\n        res = build_result.build_result(\n            status=ae_consts.ERR,\n            err=err,\n            rec=rec)\n    # end of try/ex for getting redis data\n\n    return res\n# end of build_df_from_redis\n'"
analysis_engine/build_entry_call_spread_details.py,0,"b'""""""\nHelper for determining the pricing for\nan entry position on a bull call spread\n""""""\n\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.build_option_spread_details as spread_utils\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef build_entry_call_spread_details(\n        ticker,\n        close,\n        num_contracts,\n        low_strike,\n        low_ask,\n        low_bid,\n        high_strike,\n        high_ask,\n        high_bid):\n    """"""build_entry_call_spread_details\n\n    Calculate pricing information for\n    buying into ``Vertical Bull Call Option Spread`` contracts\n\n    :param ticker: string ticker symbol\n    :param num_contracts: integer number of contracts\n    :param low_strike: float - strike for\n        the low leg of the spread\n    :param low_ask: float - ask price for\n        the low leg of the spread\n    :param low_bid: float - bid price for\n        the low leg of the spread\n    :param high_strike: float - strike  for\n        the high leg of the spread\n    :param high_ask: float - ask price for\n        the high leg of the spread\n    :param high_bid: float - bid price for\n        the high leg of the spread\n    """"""\n\n    spread_details = spread_utils.build_option_spread_details(\n        trade_type=ae_consts.TRADE_ENTRY,\n        spread_type=ae_consts.SPREAD_VERTICAL_BULL,\n        option_type=ae_consts.OPTION_CALL,\n        close=close,\n        num_contracts=num_contracts,\n        low_strike=low_strike,\n        low_ask=low_ask,\n        low_bid=low_bid,\n        high_strike=high_strike,\n        high_ask=high_ask,\n        high_bid=high_bid)\n\n    log.debug(\n        f\'{ticker} \'\n        f\'type={ae_consts.get_status(status=spread_details[""trade_type""])} \'\n        f\'spread={ae_consts.get_status(status=spread_details[""spread_type""])} \'\n        f\'option={ae_consts.get_status(status=spread_details[""option_type""])} \'\n        f\'close={close} spread={ae_consts.ppj(spread_details)}\')\n\n    return spread_details\n# end of build_entry_call_spread_details\n'"
analysis_engine/build_entry_put_spread_details.py,0,"b'""""""\nHelper for determining the pricing for\nan entry position on a bear put spread\n""""""\n\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.build_option_spread_details as spread_utils\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef build_entry_put_spread_details(\n        ticker,\n        close,\n        num_contracts,\n        low_strike,\n        low_ask,\n        low_bid,\n        high_strike,\n        high_ask,\n        high_bid):\n    """"""build_entry_put_spread_details\n\n    Calculate pricing information for\n    buying into ``Vertical Bear Put Option Spread`` contracts\n\n    :param ticker: string ticker symbol\n    :param num_contracts: integer number of contracts\n    :param low_strike: float - strike for\n        the low leg of the spread\n    :param low_ask: float - ask price for\n        the low leg of the spread\n    :param low_bid: float - bid price for\n        the low leg of the spread\n    :param high_strike: float - strike  for\n        the high leg of the spread\n    :param high_ask: float - ask price for\n        the high leg of the spread\n    :param high_bid: float - bid price for\n        the high leg of the spread\n    """"""\n\n    spread_details = spread_utils.build_option_spread_details(\n        trade_type=ae_consts.TRADE_ENTRY,\n        spread_type=ae_consts.SPREAD_VERTICAL_BEAR,\n        option_type=ae_consts.OPTION_PUT,\n        close=close,\n        num_contracts=num_contracts,\n        low_strike=low_strike,\n        low_ask=low_ask,\n        low_bid=low_bid,\n        high_strike=high_strike,\n        high_ask=high_ask,\n        high_bid=high_bid)\n\n    log.debug(\n        f\'{ticker} \'\n        f\'type={ae_consts.get_status(status=spread_details[""trade_type""])} \'\n        f\'spread={ae_consts.get_status(status=spread_details[""spread_type""])} \'\n        f\'option={ae_consts.get_status(status=spread_details[""option_type""])} \'\n        f\'close={close} spread={ae_consts.ppj(spread_details)}\')\n\n    return spread_details\n# end of build_entry_put_spread_details\n'"
analysis_engine/build_exit_call_spread_details.py,0,"b'""""""\nHelper for determining the pricing for\nan exit position on a bull call spread\n""""""\n\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.build_option_spread_details as spread_utils\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef build_exit_call_spread_details(\n        ticker,\n        close,\n        num_contracts,\n        low_strike,\n        low_ask,\n        low_bid,\n        high_strike,\n        high_ask,\n        high_bid):\n    """"""build_exit_call_spread_details\n\n    Calculate pricing information for\n    selling (closing-out) ``Vertical Bull Call Option Spread`` contracts\n\n    :param ticker: string ticker symbol\n    :param num_contracts: integer number of contracts\n    :param low_strike: float - strike for\n        the low leg of the spread\n    :param low_ask: float - ask price for\n        the low leg of the spread\n    :param low_bid: float - bid price for\n        the low leg of the spread\n    :param high_strike: float - strike  for\n        the high leg of the spread\n    :param high_ask: float - ask price for\n        the high leg of the spread\n    :param high_bid: float - bid price for\n        the high leg of the spread\n    """"""\n\n    spread_details = spread_utils.build_option_spread_details(\n        trade_type=ae_consts.TRADE_EXIT,\n        spread_type=ae_consts.SPREAD_VERTICAL_BULL,\n        option_type=ae_consts.OPTION_CALL,\n        close=close,\n        num_contracts=num_contracts,\n        low_strike=low_strike,\n        low_ask=low_ask,\n        low_bid=low_bid,\n        high_strike=high_strike,\n        high_ask=high_ask,\n        high_bid=high_bid)\n\n    log.debug(\n        f\'{ticker} \'\n        f\'type={ae_consts.get_status(status=spread_details[""trade_type""])} \'\n        f\'spread={ae_consts.get_status(status=spread_details[""spread_type""])} \'\n        f\'option={ae_consts.get_status(status=spread_details[""option_type""])} \'\n        f\'close={close} spread={ae_consts.ppj(spread_details)}\')\n\n    return spread_details\n# end of build_exit_call_spread_details\n'"
analysis_engine/build_exit_put_spread_details.py,0,"b'""""""\nHelper for determining the pricing for\nan exit position on a bear put spread\n""""""\n\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.build_option_spread_details as spread_utils\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef build_exit_put_spread_details(\n        ticker,\n        close,\n        num_contracts,\n        low_strike,\n        low_ask,\n        low_bid,\n        high_strike,\n        high_ask,\n        high_bid):\n    """"""build_exit_put_spread_details\n\n    Calculate pricing information for\n    selling (closing-out) ``Vertical Bear Put Option Spread`` contracts\n\n    :param ticker: string ticker name\n    :param num_contracts: integer number of contracts\n    :param low_strike: float - strike for\n        the low leg of the spread\n    :param low_ask: float - ask price for\n        the low leg of the spread\n    :param low_bid: float - bid price for\n        the low leg of the spread\n    :param high_strike: float - strike  for\n        the high leg of the spread\n    :param high_ask: float - ask price for\n        the high leg of the spread\n    :param high_bid: float - bid price for\n        the high leg of the spread\n    """"""\n\n    spread_details = spread_utils.build_option_spread_details(\n        trade_type=ae_consts.TRADE_EXIT,\n        spread_type=ae_consts.SPREAD_VERTICAL_BEAR,\n        option_type=ae_consts.OPTION_PUT,\n        close=close,\n        num_contracts=num_contracts,\n        low_strike=low_strike,\n        low_ask=low_ask,\n        low_bid=low_bid,\n        high_strike=high_strike,\n        high_ask=high_ask,\n        high_bid=high_bid)\n\n    log.debug(\n        f\'{ticker} \'\n        f\'type={ae_consts.get_status(status=spread_details[""trade_type""])} \'\n        f\'spread={ae_consts.get_status(status=spread_details[""spread_type""])} \'\n        f\'option={ae_consts.get_status(status=spread_details[""option_type""])} \'\n        f\'close={close} spread={ae_consts.ppj(spread_details)}\')\n\n    return spread_details\n# end of build_exit_put_spread_details\n'"
analysis_engine/build_option_spread_details.py,0,"b'""""""\nBuild option spread pricing details\n""""""\n\nimport analysis_engine.consts as ae_consts\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef build_option_spread_details(\n        trade_type,\n        spread_type,\n        option_type,\n        close,\n        num_contracts,\n        low_strike,\n        low_ask,\n        low_bid,\n        high_strike,\n        high_ask,\n        high_bid):\n    """"""build_option_spread_details\n\n    Calculate pricing information for supported spreads\n    including ``max loss``, ``max profit``, and ``mid price`` (break\n    even coming soon)\n\n    :param trade_type: entry (``TRADE_ENTRY``) or\n        exit (``TRADE_EXIT``) of a spread position\n    :param spread_type: vertical bull (``SPREAD_VERTICAL_BULL``)\n        and vertical bear (``SPREAD_VERTICAL_BEAR``)\n        are the only supported calculations for now\n    :param option_type: call (``OPTION_CALL``) or put\n        (``OPTION_PUT``)\n    :param close: closing price of the underlying\n        asset\n    :param num_contracts: integer number of contracts\n    :param low_strike: float - strike for\n        the low leg of the spread\n    :param low_ask: float - ask price for\n        the low leg of the spread\n    :param low_bid: float - bid price for\n        the low leg of the spread\n    :param high_strike: float - strike  for\n        the high leg of the spread\n    :param high_ask: float - ask price for\n        the high leg of the spread\n    :param high_bid: float - bid price for\n        the high leg of the spread\n    """"""\n\n    details = {\n        \'status\': ae_consts.NOT_RUN,\n        \'trade_type\': trade_type,\n        \'spread_type\': spread_type,\n        \'option_type\': option_type,\n        \'num_contracts\': num_contracts,\n        \'low_strike\': low_strike,\n        \'low_bid\': low_bid,\n        \'low_ask\': low_ask,\n        \'high_strike\': high_strike,\n        \'high_bid\': high_bid,\n        \'high_ask\': high_ask,\n        \'cost\': None,\n        \'revenue\': None,\n        \'low_bidask_mid\': None,\n        \'high_bidask_mid\': None,\n        \'mid_price\': None,\n        \'nat_price\': None,\n        \'strike_width\': None,\n        \'break_even\': None,\n        \'max_loss\': None,\n        \'max_profit\': None,\n        \'spread_id\': None\n    }\n\n    low_distance = int(close) - low_strike\n    high_distance = high_strike - int(close)\n    details[\'strike_width\'] = ae_consts.to_f(\n        high_strike - low_strike)\n    details[\'spread_id\'] = (\n        f\'T_{trade_type}_S_{spread_type}_O_{option_type}_low_{low_distance}\'\n        f\'_high_{high_distance}_w_{details[""strike_width""]}\')\n    details[\'low_bidask_mid\'] = ae_consts.to_f(low_bid + low_ask / 2.0)\n    details[\'high_bidask_mid\'] = ae_consts.to_f(high_bid + high_ask / 2.0)\n    details[\'mid_price\'] = ae_consts.to_f(abs(\n        details[\'low_bidask_mid\'] - details[\'high_bidask_mid\']))\n    details[\'nat_price\'] = ae_consts.to_f(abs(\n        details[\'low_bidask_mid\'] - details[\'high_bidask_mid\']))\n\n    cost_of_contracts_at_mid_price = None\n    revenue_of_contracts_at_mid_price = None\n\n    if trade_type == ae_consts.TRADE_ENTRY:\n        cost_of_contracts_at_mid_price = ae_consts.to_f(\n            100.0 * num_contracts * details[\'mid_price\'])\n        revenue_of_contracts_at_mid_price = ae_consts.to_f(\n            100.0 * num_contracts * (\n                details[\'strike_width\'] - details[\'mid_price\']))\n        if spread_type == ae_consts.SPREAD_VERTICAL_BULL:\n            if option_type == ae_consts.OPTION_CALL:  # debit spread\n                details[\'max_loss\'] = cost_of_contracts_at_mid_price\n                details[\'max_profit\'] = revenue_of_contracts_at_mid_price\n            else:\n                details[\'max_loss\'] = cost_of_contracts_at_mid_price\n                details[\'max_profit\'] = revenue_of_contracts_at_mid_price\n        else:  # bear\n            if option_type == ae_consts.OPTION_CALL:  # debit spread\n                details[\'max_loss\'] = cost_of_contracts_at_mid_price\n                details[\'max_profit\'] = revenue_of_contracts_at_mid_price\n            else:\n                details[\'max_loss\'] = cost_of_contracts_at_mid_price\n                details[\'max_profit\'] = revenue_of_contracts_at_mid_price\n\n    else:  # trade exit calculations:\n        revenue_of_contracts_at_mid_price = ae_consts.to_f(\n            100.0 * num_contracts * details[\'mid_price\'])\n        cost_of_contracts_at_mid_price = ae_consts.to_f(\n            100.0 * num_contracts * (\n                details[\'strike_width\'] - details[\'mid_price\']))\n        if spread_type == ae_consts.SPREAD_VERTICAL_BULL:\n            if option_type == ae_consts.OPTION_CALL:  # credit spread\n                details[\'max_profit\'] = revenue_of_contracts_at_mid_price\n                details[\'max_loss\'] = cost_of_contracts_at_mid_price\n            else:\n                details[\'max_profit\'] = revenue_of_contracts_at_mid_price\n                details[\'max_loss\'] = cost_of_contracts_at_mid_price\n        else:  # bear\n            if option_type == ae_consts.OPTION_CALL:  # credit spread\n                details[\'max_profit\'] = revenue_of_contracts_at_mid_price\n                details[\'max_loss\'] = cost_of_contracts_at_mid_price\n            else:\n                details[\'max_profit\'] = revenue_of_contracts_at_mid_price\n                details[\'max_loss\'] = cost_of_contracts_at_mid_price\n    # end of supported types of spreads\n\n    details[\'cost\'] = cost_of_contracts_at_mid_price\n    details[\'revenue\'] = revenue_of_contracts_at_mid_price\n\n    log.debug(\n        f\'type={ae_consts.get_status(status=trade_type)} \'\n        f\'spread={ae_consts.get_status(status=spread_type)} \'\n        f\'option={ae_consts.get_status(status=option_type)} \'\n        f\'close={close} spread_id={details[""spread_id""]} \'\n        f\'revenue={revenue_of_contracts_at_mid_price} \'\n        f\'cost={cost_of_contracts_at_mid_price} \'\n        f\'mid={details[""mid_price""]} width={details[""strike_width""]} \'\n        f\'max_profit={details[""max_profit""]} max_loss={details[""max_loss""]}\')\n\n    return details\n# end of build_option_spread_details\n'"
analysis_engine/build_publish_request.py,0,"b'""""""\nHelper for building a dictionary for the:\n``analysis_engine.publish.publish`` function\n""""""\n\nimport analysis_engine.consts as ae_consts\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef build_publish_request(\n        ticker=None,\n        tickers=None,\n        convert_to_json=False,\n        output_file=None,\n        compress=False,\n        redis_enabled=ae_consts.ENABLED_REDIS_PUBLISH,\n        redis_key=None,\n        redis_address=ae_consts.REDIS_ADDRESS,\n        redis_db=ae_consts.REDIS_DB,\n        redis_password=ae_consts.REDIS_PASSWORD,\n        redis_expire=ae_consts.REDIS_EXPIRE,\n        redis_serializer=\'json\',\n        redis_encoding=\'utf-8\',\n        s3_enabled=ae_consts.ENABLED_S3_UPLOAD,\n        s3_key=None,\n        s3_address=ae_consts.S3_ADDRESS,\n        s3_bucket=ae_consts.S3_BUCKET,\n        s3_access_key=ae_consts.S3_ACCESS_KEY,\n        s3_secret_key=ae_consts.S3_SECRET_KEY,\n        s3_region_name=ae_consts.S3_REGION_NAME,\n        s3_secure=ae_consts.S3_SECURE,\n        slack_enabled=False,\n        slack_code_block=False,\n        slack_full_width=False,\n        verbose=False,\n        label=\'publisher\'):\n    """"""build_publish_request\n\n    Build a dictionary for helping to quickly publish\n    to multiple optional endpoints:\n    - a local file path (``output_file``)\n    - minio (``s3_bucket`` and ``s3_key``)\n    - redis (``redis_key``)\n    - slack\n\n    :param ticker: ticker\n    :param tickers: optional - list of tickers\n    :param label: optional - algo log tracking name\n    :param output_file: path to save the data\n        to a file\n    :param compress: optional - compress before publishing\n    :param verbose: optional - boolean to log output\n    :param kwargs: optional - future argument support\n\n    **(Optional) Redis connectivity arguments**\n\n    :param redis_enabled: bool - toggle for auto-caching all\n        datasets in Redis\n        (default is ``ENABLED_REDIS_PUBLISH``)\n    :param redis_key: string - key to save the data in redis\n        (default is ``None``)\n    :param redis_address: Redis connection string format: ``host:port``\n        (default is ``REDIS_ADDRESS``)\n    :param redis_db: Redis db to use\n        (default is ``REDIS_DB``)\n    :param redis_password: optional - Redis password\n        (default is ``REDIS_PASSWORD``)\n    :param redis_expire: optional - Redis expire value\n        (default is ``REDIS_EXPIRE``)\n    :param redis_serializer: not used yet - support for future\n        pickle objects in redis (default is ``json``)\n    :param redis_encoding: format of the encoded key in redis\n        (default is ``utf-8``)\n\n    **(Optional) Minio (S3) connectivity arguments**\n\n    :param s3_enabled: bool - toggle for auto-archiving on Minio (S3)\n        (default is ``ENABLED_S3_UPLOAD``)\n    :param s3_key: string - key to save the data in redis\n        (default is ``None``)\n    :param s3_address: Minio S3 connection string format: ``host:port``\n        (default is ``S3_ADDRESS``)\n    :param s3_bucket: S3 Bucket for storing the artifacts\n        (default is ``S3_BUCKET``) which should be viewable on a browser:\n        http://localhost:9000/minio/dev/\n    :param s3_access_key: S3 Access key\n        (default is ``S3_ACCESS_KEY``)\n    :param s3_secret_key: S3 Secret key\n        (default is ``S3_SECRET_KEY``)\n    :param s3_region_name: S3 region name\n        (default is ``S3_REGION_NAME``)\n    :param s3_secure: Transmit using tls encryption\n        (default is ``S3_SECURE``)\n\n    **(Optional) Slack arguments**\n\n    :param slack_enabled: optional - boolean for\n        publishing to slack\n    :param slack_code_block: optional - boolean for\n        publishing as a code black in slack\n    :param slack_full_width: optional - boolean for\n        publishing as a to slack using the full\n        width allowed\n    """"""\n    use_tickers = []\n    if ticker:\n        use_tickers = [\n            ticker.upper()\n        ]\n    if tickers:\n        for t in tickers:\n            if t not in use_tickers:\n                use_tickers.append(t.upper())\n\n    work = {\n        \'tickers\': use_tickers,\n        \'label\': label,\n        \'convert_to_json\': convert_to_json,\n        \'output_file\': output_file,\n        \'compress\': compress,\n        \'redis_enabled\': redis_enabled,\n        \'redis_key\': redis_key,\n        \'redis_address\': redis_address,\n        \'redis_db\': redis_db,\n        \'redis_password\': redis_password,\n        \'redis_expire\': redis_expire,\n        \'redis_serializer\': redis_serializer,\n        \'redis_encoding\': redis_encoding,\n        \'s3_enabled\': s3_enabled,\n        \'s3_key\': s3_key,\n        \'s3_address\': s3_address,\n        \'s3_bucket\': s3_bucket,\n        \'s3_access_key\': s3_access_key,\n        \'s3_secret_key\': s3_secret_key,\n        \'s3_region_name\': s3_region_name,\n        \'s3_secure\': s3_secure,\n        \'slack_enabled\': slack_enabled,\n        \'slack_code_block\': slack_code_block,\n        \'slack_full_width\': slack_full_width,\n        \'verbose\': verbose,\n        \'version\': 1,\n        \'label\': label\n    }\n\n    log.debug(f\'created publish_request={ae_consts.ppj(work)}\')\n\n    return work\n# end of build_publish_request\n'"
analysis_engine/build_result.py,0,"b'""""""\nBuild a result dictionary\n""""""\n\nimport analysis_engine.consts as ae_consts\n\n\ndef build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec=None):\n    """"""build_result\n\n    Common result builder for helping standardize\n    results per function and unittesting\n\n    :param status: starting status value from ``consts.py``\n    :param err: optional error message\n    :param rec: dictionary for the result\n    """"""\n\n    res = {\n        \'status\': status,\n        \'err\': err,\n        \'rec\': rec\n    }\n    if not res:\n        res[\'rec\'] = {}\n\n    return res\n# end of build_result\n'"
analysis_engine/build_sell_order.py,0,"b'""""""\nHelper for creating a sell order\n""""""\n\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.utils as ae_utils\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef build_sell_order(\n        ticker,\n        num_owned,\n        close,\n        balance,\n        commission,\n        date,\n        details,\n        use_key,\n        minute=None,\n        shares=None,\n        version=1,\n        auto_fill=True,\n        is_live_trading=False,\n        backtest_shares_default=10,\n        reason=None):\n    """"""build_sell_order\n\n    Create an algorithm sell order as a dictionary\n\n    :param ticker: ticker\n    :param num_owned: integer current owned\n        number of shares for this asset\n    :param close: float closing price of the asset\n    :param balance: float amount of available capital\n    :param commission: float for commission costs\n    :param date: string trade date for that row usually\n        ``COMMON_DATE_FORMAT`` (``YYYY-MM-DD``)\n    :param minute: optional - string with the minute that the\n        order was placed. format is\n        ``COMMON_TICK_DATE_FORMAT`` (``YYYY-MM-DD HH:MM:SS``)\n    :param details: dictionary for full row of values to review\n        all sells after the algorithm finishes.\n        (usually ``row.to_json()``)\n    :param use_key: string for redis and s3 publishing of the algorithm\n        result dictionary as a json-serialized dictionary\n    :param shares: optional - integer number of shares to sell\n        if None sell all ``num_owned`` shares at the ``close``.\n    :param version: optional - version tracking integer\n    :param auto_fill: optional - bool for not assuming the trade\n        filled (default ``True``)\n    :param is_live_trading: optional - bool for filling trades\n        for live trading or for backtest tuning filled\n        (default ``False`` which is backtest mode)\n    :param backtest_shares_default: optional - integer for\n        simulating shares during a backtest even if there\n        are not enough funds\n        (default ``10``)\n    :param reason: optional - string for recording why the algo\n        decided to sell for review after the algorithm finishes\n    """"""\n    status = ae_consts.TRADE_OPEN\n    s3_bucket_name = ae_consts.ALGO_SELLS_S3_BUCKET_NAME\n    s3_key = use_key\n    redis_key = use_key\n    s3_enabled = True\n    redis_enabled = True\n\n    cost_of_trade = None\n    sell_price = 0.0\n    new_shares = num_owned\n    new_balance = balance\n    created_date = None\n\n    tradable_funds = balance - commission\n\n    if num_owned == 0:\n        status = ae_consts.TRADE_NO_SHARES_TO_SELL\n    elif close > 0.1 and tradable_funds > 10.0:\n        cost_of_trade = commission\n        if shares:\n            if shares > num_owned:\n                shares = num_owned\n        else:\n            shares = num_owned\n        sell_price = ae_consts.to_f(\n            val=(shares * close) + commission)\n        if cost_of_trade > balance:\n            status = ae_consts.TRADE_NOT_ENOUGH_FUNDS\n        else:\n            created_date = ae_utils.utc_now_str()\n            if auto_fill:\n                new_shares = num_owned - shares\n                new_balance = ae_consts.to_f(balance + sell_price)\n                status = ae_consts.TRADE_FILLED\n            else:\n                new_shares = shares\n                new_balance = balance\n    else:\n        status = ae_consts.TRADE_NOT_ENOUGH_FUNDS\n\n    order_dict = {\n        \'ticker\': ticker,\n        \'status\': status,\n        \'balance\': new_balance,\n        \'shares\': new_shares,\n        \'sell_price\': sell_price,\n        \'prev_balance\': balance,\n        \'prev_shares\': num_owned,\n        \'close\': close,\n        \'details\': details,\n        \'reason\': reason,\n        \'date\': date,\n        \'minute\': minute,\n        \'created\': created_date,\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'s3_enabled\': s3_enabled,\n        \'redis_enabled\': redis_enabled,\n        \'version\': version\n    }\n\n    use_date = minute\n    if not use_date:\n        use_date = date\n\n    log.debug(\n        f\'{ticker} {use_date} sell \'\n        f\'{ae_consts.get_status(status=order_dict[""status""])} \'\n        f\'order={ae_consts.ppj(order_dict)}\')\n\n    return order_dict\n# end of build_sell_order\n'"
analysis_engine/build_trade_history_entry.py,0,"b'""""""\nHelper for building an algorithm trading and performance history\nas a dictionary that can be reviewed during or after an\nalgorithm finishes running\n""""""\n\nimport analysis_engine.consts as ae_consts\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef build_trade_history_entry(\n        ticker,\n        num_owned,\n        close,\n        balance,\n        commission,\n        date,\n        trade_type,\n        algo_start_price,\n        original_balance,\n        minute=None,\n        high=None,\n        low=None,\n        open_val=None,\n        volume=None,\n        ask=None,\n        bid=None,\n        today_high=None,\n        today_low=None,\n        today_open_val=None,\n        today_close=None,\n        today_volume=None,\n        stop_loss=None,\n        trailing_stop_loss=None,\n        buy_hold_units=None,\n        sell_hold_units=None,\n        spread_exp_date=None,\n        spread_id=None,\n        low_strike=None,\n        low_bid=None,\n        low_ask=None,\n        low_volume=None,\n        low_open_int=None,\n        low_delta=None,\n        low_gamma=None,\n        low_theta=None,\n        low_vega=None,\n        low_rho=None,\n        low_impl_vol=None,\n        low_intrinsic=None,\n        low_extrinsic=None,\n        low_theo_price=None,\n        low_theo_volatility=None,\n        low_max_covered=None,\n        low_exp_date=None,\n        high_strike=None,\n        high_bid=None,\n        high_ask=None,\n        high_volume=None,\n        high_open_int=None,\n        high_delta=None,\n        high_gamma=None,\n        high_theta=None,\n        high_vega=None,\n        high_rho=None,\n        high_impl_vol=None,\n        high_intrinsic=None,\n        high_extrinsic=None,\n        high_theo_price=None,\n        high_theo_volatility=None,\n        high_max_covered=None,\n        high_exp_date=None,\n        prev_balance=None,\n        prev_num_owned=None,\n        total_buys=None,\n        total_sells=None,\n        buy_triggered=None,\n        buy_strength=None,\n        buy_risk=None,\n        sell_triggered=None,\n        sell_strength=None,\n        sell_risk=None,\n        num_indicators_buy=None,\n        num_indicators_sell=None,\n        min_buy_indicators=None,\n        min_sell_indicators=None,\n        net_gain=None,\n        net_value=None,\n        ds_id=None,\n        note=None,\n        err=None,\n        entry_spread_dict=None,\n        version=1,\n        verbose=False):\n    """"""build_trade_history_entry\n\n    Build a dictionary for tracking an algorithm profitability per ticker\n    and for ``TRADE_SHARES``, ``TRADE_VERTICAL_BULL_SPREAD``, or\n    ``TRADE_VERTICAL_BEAR_SPREAD`` trading types.\n\n    .. note:: setting the ``minute`` is required to build\n        a minute-by-minute ``Trading History``\n\n    :param ticker: string ticker or symbol\n    :param num_owned: integer current owned\n        number of ``shares`` for this asset or number of\n        currently owned ``contracts`` for an options\n        spread.\n    :param close: float ``close`` price of the\n        underlying asset\n    :param balance: float amount of available capital\n    :param commission: float for commission costs\n    :param date: string trade date for that row usually\n        ``COMMON_DATE_FORMAT`` (``YYYY-MM-DD``)\n    :param minute: optional - string for recording the minute\n        the trade was place, and the format is\n        ``COMMON_TICK_DATE_FORMAT`` (``YYYY-MM-DD HH:MM:SS``)\n        this is optional if the algorithm is set up to\n        trade using a ``day`` value for timeseries.\n    :param trade_type: type of the trade - supported values:\n            ``TRADE_SHARES``,\n            ``TRADE_VERTICAL_BULL_SPREAD``,\n            ``TRADE_VERTICAL_BEAR_SPREAD``\n    :param algo_start_price: float starting close/contract price\n        for this algo\n    :param original_balance: float starting original account\n        balance for this algo\n    :param high: optional - float underlying stock asset ``high`` price\n    :param low: optional - float underlying stock asset ``low`` price\n    :param open_val: optional - float underlying stock asset ``open`` price\n    :param volume: optional - integer underlying stock asset ``volume``\n    :param ask: optional - float ``ask`` price of the\n        stock (for buying ``shares``)\n    :param bid: optional - float ``bid`` price of the\n        stock (for selling ``shares``)\n    :param today_high: optional - float ``high`` from\n        the daily dataset (if available)\n    :param today_low: optional - float ``low`` from\n        the daily dataset (if available)\n    :param today_open_val: optional - float ``open`` from\n        the daily dataset (if available)\n    :param today_close: optional - float ``close`` from\n        the daily dataset (if available)\n    :param today_volume: optional - float ``volume`` from\n        the daily dataset (if available)\n    :param stop_loss: optional - float ``stop_loss`` price of the\n        stock/spread (for selling ``shares`` vs ``contracts``)\n    :param trailing_stop_loss: optional - float ``trailing_stop_loss``\n        price of the stock/spread (for selling ``shares`` vs ``contracts``)\n    :param buy_hold_units: optional - number of units\n        to hold buys - helps with algorithm tuning\n    :param sell_hold_units: optional - number of units\n        to hold sells - helps with algorithm tuning\n    :param spread_exp_date: optional - string spread contract\n        expiration date (``COMMON_DATE_FORMAT`` (``YYYY-MM-DD``)\n    :param spread_id: optional - spread identifier for reviewing\n        spread performances\n    :param low_strike: optional\n        - only for vertical bull/bear trade types\n        ``low leg strike price`` of the spread\n    :param low_bid: optional\n        - only for vertical bull/bear trade types\n        ``low leg bid`` of the spread\n    :param low_ask: optional\n        - only for vertical bull/bear trade types\n        ``low leg ask`` of the spread\n    :param low_volume: optional\n        - only for vertical bull/bear trade types\n        ``low leg volume`` of the spread\n    :param low_open_int: optional\n        - only for vertical bull/bear trade types\n        ``low leg open interest`` of the spread\n    :param low_delta: optional\n        - only for vertical bull/bear trade types\n        ``low leg delta`` of the spread\n    :param low_gamma: optional\n        - only for vertical bull/bear trade types\n        ``low leg gamma`` of the spread\n    :param low_theta: optional\n        - only for vertical bull/bear trade types\n        ``low leg theta`` of the spread\n    :param low_vega: optional\n        - only for vertical bull/bear trade types\n        ``low leg vega`` of the spread\n    :param low_rho: optional\n        - only for vertical bull/bear trade types\n        ``low leg rho`` of the spread\n    :param low_impl_vol: optional\n        - only for vertical bull/bear trade types\n        ``low leg implied volatility`` of the spread\n    :param low_intrinsic: optional\n        - only for vertical bull/bear trade types\n        ``low leg intrinsic`` of the spread\n    :param low_extrinsic: optional\n        - only for vertical bull/bear trade types\n        ``low leg extrinsic`` of the spread\n    :param low_theo_price: optional\n        - only for vertical bull/bear trade types\n        ``low leg theoretical price`` of the spread\n    :param low_theo_volatility: optional\n        - only for vertical bull/bear trade types\n        ``low leg theoretical volatility`` of the spread\n    :param low_max_covered: optional\n        - only for vertical bull/bear trade types\n        ``low leg max covered returns`` of the spread\n    :param low_exp_date: optional\n        - only for vertical bull/bear trade types\n        ``low leg expiration date`` of the spread\n    :param high_strike: optional\n        - only for vertical bull/bear trade types\n        ``high leg strike price`` of the spread\n    :param high_bid: optional\n        - only for vertical bull/bear trade types\n        ``high leg bid`` of the spread\n    :param high_ask: optional\n        - only for vertical bull/bear trade types\n        ``high leg ask`` of the spread\n    :param high_volume: optional\n        - only for vertical bull/bear trade types\n        ``high leg volume`` of the spread\n    :param high_open_int: optional\n        - only for vertical bull/bear trade types\n        ``high leg open interest`` of the spread\n    :param high_delta: optional\n        - only for vertical bull/bear trade types\n        ``high leg delta`` of the spread\n    :param high_gamma: optional\n        - only for vertical bull/bear trade types\n        ``high leg gamma`` of the spread\n    :param high_theta: optional\n        - only for vertical bull/bear trade types\n        ``high leg theta`` of the spread\n    :param high_vega: optional\n        - only for vertical bull/bear trade types\n        ``high leg vega`` of the spread\n    :param high_rho: optional\n        - only for vertical bull/bear trade types\n        ``high leg rho`` of the spread\n    :param high_impl_vol: optional\n        - only for vertical bull/bear trade types\n        ``high leg implied volatility`` of the spread\n    :param high_intrinsic: optional\n        - only for vertical bull/bear trade types\n        ``high leg intrinsic`` of the spread\n    :param high_extrinsic: optional\n        - only for vertical bull/bear trade types\n        ``high leg extrinsic`` of the spread\n    :param high_theo_price: optional\n        - only for vertical bull/bear trade types\n        ``high leg theoretical price`` of the spread\n    :param high_theo_volatility: optional\n        - only for vertical bull/bear trade types\n        ``high leg theoretical volatility`` of the spread\n    :param high_max_covered: optional\n        - only for vertical bull/bear trade types\n        ``high leg max covered returns`` of the spread\n    :param high_exp_date: optional\n        - only for vertical bull/bear trade types\n        ``high leg expiration date`` of the spread\n    :param prev_balance: optional - previous balance\n        for this algo\n    :param prev_num_owned: optional - previous num of\n        ``shares`` or ``contracts``\n    :param total_buys: optional - total buy orders\n        for this algo\n    :param total_sells: optional - total sell orders\n        for this algo\n    :param buy_triggered: optional - bool\n        ``buy`` conditions in the algorithm triggered\n    :param buy_strength: optional - float\n        custom strength/confidence rating for tuning\n        algorithm performance for desirable\n        sensitivity and specificity\n    :param buy_risk: optional - float\n        custom risk rating for tuning algorithm\n        peformance for avoiding custom risk for buy\n        conditions\n    :param sell_triggered: optional - bool\n        ``sell`` conditions in the algorithm triggered\n    :param sell_strength: optional - float\n        custom strength/confidence rating for tuning\n        algorithm performance for desirable\n        sensitivity and specificity\n    :param sell_risk: optional - float\n        custom risk rating for tuning algorithm\n        peformance for avoiding custom risk for buy\n        conditions\n    :param num_indicators_buy: optional - integer\n        number of indicators the ``IndicatorProcessor``\n        processed and said to ``buy`` an asset\n    :param num_indicators_sell: optional - integer\n        number of indicators the ``IndicatorProcessor``\n        processed and said to ``sell`` an asset\n    :param min_buy_indicators: optional - integer\n        minimum number of indicators required to trigger\n        a ``buy`` order\n    :param min_sell_indicators: optional - integer\n        minimum number of indicators required to trigger\n        a ``sell`` order\n        net_gain=None,\n        net_value=None,\n    :param net_value: optional - float total value the algorithm\n        has left remaining since starting trading. this includes\n        the number of ``self.num_owned`` shares with the\n        ``self.latest_close`` price included\n    :param net_gain: optional - float amount the algorithm has\n        made since starting including owned shares\n        with the ``self.latest_close`` price included\n\n    :param ds_id: optional - datset id for debugging\n    :param note: optional - string for tracking high level\n        testing notes on algorithm indicator ratings and\n        internal message passing during an algorithms\'s\n        ``self.process`` method\n    :param err: optional - string for tracking errors\n    :param entry_spread_dict: optional - on exit spreads\n        the calculation of net gain can use the entry\n        spread to determine specific performance metrics\n        (work in progress)\n    :param version: optional - version tracking order history\n    :param verbose: optional - bool log each history node\n        (default is ``False``)\n    """"""\n    status = ae_consts.NOT_RUN\n    algo_status = ae_consts.NOT_RUN\n    err = None\n    balance_net_gain = 0.0\n    breakeven_price = None\n    max_profit = None  # only for option spreads\n    max_loss = None  # only for option spreads\n    exp_date = None  # only for option spreads\n\n    # latest price - start price of the algo\n    price_change_since_start = close - algo_start_price\n\n    if close:\n        if close < 0.01:\n            status = ae_consts.INVALID\n\n    history_dict = {\n        \'ticker\': ticker,\n        \'algo_start_price\': ae_consts.to_f(algo_start_price),\n        \'algo_price_change\': ae_consts.to_f(price_change_since_start),\n        \'original_balance\': ae_consts.to_f(original_balance),\n        \'status\': status,\n        \'algo_status\': algo_status,\n        \'buy_now\': buy_triggered,\n        \'buy_strength\': buy_strength,\n        \'buy_risk\': buy_risk,\n        \'sell_now\': sell_triggered,\n        \'sell_strength\': sell_strength,\n        \'sell_risk\': sell_risk,\n        \'num_indicators_buy\': num_indicators_buy,\n        \'num_indicators_sell\': num_indicators_sell,\n        \'min_buy_indicators\': min_buy_indicators,\n        \'min_sell_indicators\': min_sell_indicators,\n        \'ds_id\': ds_id,\n        \'num_owned\': num_owned,\n        \'close\': ae_consts.to_f(close),\n        \'balance\': ae_consts.to_f(balance),\n        \'commission\': ae_consts.to_f(commission),\n        \'date\': date,\n        \'minute\': minute,\n        \'trade_type\': trade_type,\n        \'high\': ae_consts.to_f(high),\n        \'low\': ae_consts.to_f(low),\n        \'open\': ae_consts.to_f(open_val),\n        \'volume\': volume,\n        \'ask\': ae_consts.to_f(ask),\n        \'bid\': ae_consts.to_f(bid),\n        \'today_high\': ae_consts.to_f(today_high),\n        \'today_low\': ae_consts.to_f(today_low),\n        \'today_open_val\': ae_consts.to_f(today_open_val),\n        \'today_close\': ae_consts.to_f(today_close),\n        \'today_volume\': ae_consts.to_f(today_volume),\n        \'stop_loss\': ae_consts.to_f(stop_loss),\n        \'trailing_stop_loss\': ae_consts.to_f(trailing_stop_loss),\n        \'buy_hold_units\': buy_hold_units,\n        \'sell_hold_units\': sell_hold_units,\n        \'low_strike\': low_strike,\n        \'low_bid\': ae_consts.to_f(low_bid),\n        \'low_ask\': ae_consts.to_f(low_ask),\n        \'low_volume\': low_volume,\n        \'low_open_int\': low_open_int,\n        \'low_delta\': low_delta,\n        \'low_gamma\': low_gamma,\n        \'low_theta\': low_theta,\n        \'low_vega\': low_vega,\n        \'low_rho\': low_rho,\n        \'low_impl_vol\': low_impl_vol,\n        \'low_intrinsic\': low_intrinsic,\n        \'low_extrinsic\': low_extrinsic,\n        \'low_theo_price\': low_theo_price,\n        \'low_theo_volatility\': low_theo_volatility,\n        \'low_max_covered\': low_max_covered,\n        \'low_exp_date\': low_exp_date,\n        \'high_strike\': high_strike,\n        \'high_bid\': ae_consts.to_f(high_bid),\n        \'high_ask\': ae_consts.to_f(high_ask),\n        \'high_volume\': high_volume,\n        \'high_open_int\': high_open_int,\n        \'high_delta\': high_delta,\n        \'high_gamma\': high_gamma,\n        \'high_theta\': high_theta,\n        \'high_vega\': high_vega,\n        \'high_rho\': high_rho,\n        \'high_impl_vol\': high_impl_vol,\n        \'high_intrinsic\': high_intrinsic,\n        \'high_extrinsic\': high_extrinsic,\n        \'high_theo_price\': high_theo_price,\n        \'high_theo_volatility\': high_theo_volatility,\n        \'high_max_covered\': high_max_covered,\n        \'high_exp_date\': high_exp_date,\n        \'spread_id\': spread_id,\n        \'net_gain\': net_gain,\n        \'net_value\': net_value,\n        \'breakeven_price\': breakeven_price,\n        \'max_profit\': ae_consts.to_f(max_profit),\n        \'max_loss\': ae_consts.to_f(max_loss),\n        \'exp_date\': exp_date,\n        \'prev_balance\': ae_consts.to_f(prev_balance),\n        \'prev_num_owned\': ae_consts.to_f(prev_num_owned),\n        \'total_buys\': total_buys,\n        \'total_sells\': total_sells,\n        \'note\': note,\n        \'err\': err,\n        \'version\': version\n    }\n\n    # evaluate if the algorithm is gaining\n    # cash over the test\n    if balance and original_balance:\n        # net change on the balance\n        # note this needs to be upgraded to\n        # support orders per ticker\n        # single tickers will work for v1\n        balance_net_gain = balance - original_balance\n        if balance_net_gain > 0.0:\n            algo_status = ae_consts.ALGO_PROFITABLE\n        else:\n            algo_status = ae_consts.ALGO_NOT_PROFITABLE\n    else:\n        history_dict[\'err\'] = (\n            f\'{ticker} ds_id={ds_id} missing balance={balance} and \'\n            f\'original_balance={original_balance}\')\n        algo_status = ae_consts.ALGO_ERROR\n    # if starting balance and original_balance exist\n    # to determine algorithm trade profitability\n\n    # if there are no shares to sell then\n    # there\'s no current trade open\n    if num_owned and num_owned < 1:\n        status = ae_consts.TRADE_NO_SHARES_TO_SELL\n    else:\n        if close < 0.01:\n            history_dict[\'err\'] = (\n                f\'{ticker} ds_id={ds_id} close={close} must be greater \'\n                f\'than 0.01\')\n            status = ae_consts.TRADE_ERROR\n        elif algo_start_price < 0.01:\n            history_dict[\'err\'] = (\n                f\'{ticker} ds_id={ds_id} \'\n                f\'algo_start_price={algo_start_price} must be greater \'\n                f\'than 0.01\')\n            status = ae_consts.TRADE_ERROR\n        else:\n            price_net_gain = close - algo_start_price\n            if price_net_gain > 0.0:\n                status = ae_consts.TRADE_PROFITABLE\n            else:\n                status = ae_consts.TRADE_NOT_PROFITABLE\n    # if starting price when algo started and close exist\n    # determine if this trade profitability\n\n    # Assign calculated values:\n    history_dict[\'net_gain\'] = net_gain\n    history_dict[\'balance_net_gain\'] = balance_net_gain\n    history_dict[\'breakeven_price\'] = breakeven_price\n    history_dict[\'max_profit\'] = max_profit\n    history_dict[\'max_loss\'] = max_loss\n    history_dict[\'exp_date\'] = exp_date\n\n    # assign statuses\n    history_dict[\'status\'] = status\n    history_dict[\'algo_status\'] = algo_status\n\n    use_date = minute\n    if not use_date:\n        use_date = date\n\n    if verbose:\n        log.debug(\n            f\'{ticker} ds_id={ds_id} {use_date} \'\n            f\'algo={ae_consts.get_status(status=history_dict[""algo_status""])} \'\n            f\'trade={ae_consts.get_status(status=history_dict[""status""])} \'\n            f\'history={ae_consts.ppj(history_dict)}\')\n\n    return history_dict\n# end of build_trade_history_entry\n'"
analysis_engine/charts.py,0,"b'""""""\nCharting functions with matplotlib, numpy, pandas, and seaborn\n\nChange the footnote with:\n\n::\n\n    export PLOT_FOOTNOTE=""custom footnote on images""\n\n.. note: most of these functions were ported from\n         the repo: https://github.com/jay-johnson/scipype\n\n""""""\n\nimport datetime\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.build_result as build_result\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef add_footnote(\n        fig=None,\n        xpos=0.90,\n        ypos=0.01,\n        text=None,\n        color=\'#888888\',\n        fontsize=8):\n    """"""add_footnote\n\n    Add a footnote based off the environment key:\n    ``PLOT_FOOTNOTE``\n\n    :param fig: add the footnote to this figure object\n    :param xpos: x-axes position\n    :param ypos: y-axis position\n    :param text: text in the footnote\n    :param color: font color\n    :param fontsize: text size for the footnote text\n    """"""\n    if not fig:\n        return\n\n    use_footnote = text\n    if not use_footnote:\n        use_footnote = ae_consts.ev(\n            \'PLOT_FOOTNOTE\',\n            \'algotraders\')\n\n    fig.text(\n        xpos,\n        ypos,\n        use_footnote,\n        va=\'bottom\',\n        fontsize=fontsize,\n        color=color)\n# end of add_footnote\n\n\ndef set_common_seaborn_fonts():\n    """"""set_common_seaborn_fonts\n\n    Set the font and text style\n    """"""\n    sns.set(font=\'serif\')\n    sns.set_context(\n        \'paper\',\n        rc={\n            \'font.size\': 12,\n            \'axes.titlesize\': 12,\n            \'axes.labelsize\': 10\n        }\n    )\n# end of set_common_seaborn_fonts\n\n\ndef send_final_log(\n        log_label,\n        fn_name,\n        result):\n    """"""send_final_log\n\n    :param log_label: log identifier\n    :param fn_name: function name\n    :param result: dictionary result\n    """"""\n\n    if not fn_name:\n        log.error(\n            f\'{log_label} missing fn_name parameter for send_final_log\')\n        return\n\n    if not result:\n        log.error(\n            f\'{log_label} missing result parameter for send_final_log\')\n        return\n\n    str_result = (\n        f\'{log_label} - {fn_name} - done \'\n        f\'status={ae_consts.get_status(result[""status""])} \'\n        f\'err={result[""err""]}\')\n\n    if result[\'status\'] == ae_consts.ERR:\n        log.error(str_result)\n    else:\n        log.info(str_result)\n    # handle log based off status\n\n# end of send_final_log\n\n\ndef show_with_entities(\n        log_label,\n        xlabel,\n        ylabel,\n        title,\n        ax,\n        fig,\n        legend_list=None,\n        show_plot=True):\n    """"""show_with_entities\n\n    Helper for showing a plot with a legend and a\n    footnoe\n\n    :param log_label: log identifier\n    :param xlabel: x-axis label\n    :param ylabel: y-axis label\n    :param title: title of the plot\n    :param ax: axes\n    :param fig: figure\n    :param legend_list: list of legend items to show\n    :param show_plot: bool to show the plot\n    """"""\n\n    log.debug(\n        f\'{log_label} - \'\n        \'show_with_entities\'\n        \' - start\')\n\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n\n    ax.set_title(title)\n\n    if not legend_list:\n        ax.legend(\n            loc=""best"",\n            prop={\n                ""size"": ""medium""\n            }\n        )\n    else:\n        ax.legend(\n            legend_list,\n            loc=""best"",\n            prop={\n                ""size"": ""medium""\n            }\n        )\n    # end of if/else legend entries\n\n    add_footnote(fig)\n    plt.tight_layout()\n\n    if show_plot:\n        plt.show()\n    else:\n        plt.plot()\n# end of show_with_entities\n\n\ndef plot_overlay_pricing_and_volume(\n        log_label,\n        ticker,\n        df,\n        xlabel=None,\n        ylabel=None,\n        high_color=ae_consts.PLOT_COLORS[\'high\'],\n        close_color=ae_consts.PLOT_COLORS[\'blue\'],\n        volume_color=ae_consts.PLOT_COLORS[\'green\'],\n        date_format=ae_consts.IEX_MINUTE_DATE_FORMAT,\n        show_plot=True,\n        dropna_for_all=True):\n    """"""plot_overlay_pricing_and_volume\n\n    Plot pricing (high, low, open, close) and volume as\n    an overlay off the x-axis\n\n    Here is a sample chart from the\n    `Stock Analysis Jupyter Intro Notebook <https://github.com/Al\n    goTraders/stock-analysis-engine/blob/master/co\n    mpose/docker/notebooks/Stock-Analysis-Intro.ipynb>`__\n\n    .. image:: https://i.imgur.com/pH368gy.png\n\n    :param log_label: log identifier\n    :param ticker: ticker name\n    :param df: timeseries ``pandas.DateFrame``\n    :param xlabel: x-axis label\n    :param ylabel: y-axis label\n    :param high_color: optional - high plot color\n    :param close_color: optional - close plot color\n    :param volume_color: optional - volume color\n    :param data_format: optional - date format string this must\n        be a valid value for the ``df[\'date\']`` column\n        that would work with:\n        ``datetime.datetime.stftime(date_format)``\n    :param show_plot: optional - bool to show the plot\n    :param dropna_for_all: optional - bool to toggle keep None\'s in\n        the plot ``df`` (default is drop them\n        for display purposes)\n    """"""\n\n    rec = {\n        \'fig\': None,\n        \'ax\': None,\n        \'ax2\': None\n    }\n    result = build_result.build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec=rec)\n\n    try:\n\n        log.info(\n            f\'{log_label} - \'\n            \'plot_overlay_pricing_and_volume\'\n            \' - start\')\n\n        set_common_seaborn_fonts()\n\n        use_df = df\n        if dropna_for_all:\n            log.info(\n                f\'{log_label} - \'\n                \'plot_overlay_pricing_and_volume\'\n                \' - dropna_for_all\')\n            use_df = df.dropna(axis=0, how=\'any\')\n        # end of pre-plot dataframe scrubbing\n\n        fig, ax = plt.subplots(\n            sharex=True,\n            sharey=True,\n            figsize=(15.0, 10.0))\n        ax.plot(\n            use_df[\'date\'],\n            use_df[\'close\'],\n            color=close_color)\n        ax.plot(\n            use_df[\'date\'],\n            use_df[\'high\'],\n            color=high_color)\n\n        # use a second axis to display the\n        # volume since it is in a different range\n        # this will fill under the\n        # volume\'s y values as well\n        ax2 = ax.twinx()\n        ax2.plot(\n            use_df[\'date\'],\n            use_df[\'volume\'],\n            linestyle=\'-\',\n            color=volume_color,\n            alpha=0.6)\n        ax2.fill_between(\n            use_df[\'date\'].values,\n            0,\n            use_df[\'volume\'].values,\n            color=volume_color,\n            alpha=0.5)\n        # setup the second plot for volume\n        ax2.set_ylim([0, ax2.get_ylim()[1] * 3])\n\n        plt.grid(True)\n        use_xlabel = xlabel\n        use_ylabel = ylabel\n        if not use_xlabel:\n            xlabel = \'Minute Dates\'\n        if not use_ylabel:\n            ylabel = f\'{ticker} High and Close Prices\'\n        plt.xlabel(use_xlabel)\n        plt.ylabel(use_ylabel)\n\n        # Build a date vs Close DataFrame\n        start_date = \'\'\n        end_date = \'\'\n        try:\n            start_date = str(use_df.iloc[0][\'date\'].strftime(date_format))\n            end_date = str(use_df.iloc[-1][\'date\'].strftime(date_format))\n        except Exception:\n            date_format = \'%Y-%m-%d\'\n            start_date = str(use_df.iloc[0][\'date\'].strftime(date_format))\n            end_date = str(use_df.iloc[-1][\'date\'].strftime(date_format))\n\n        use_title = (\n            f\'{ticker} Pricing from: {start_date} to {end_date}\')\n        ax.set_title(use_title)\n\n        # Merge in the second axis (volume) Legend\n        handles, labels = plt.gca().get_legend_handles_labels()\n        newLabels, newHandles = [], []\n        for handle, label in zip(handles, labels):\n            if label not in newLabels:\n                newLabels.append(label)\n                newHandles.append(handle)\n        lines = ax.get_lines() + ax2.get_lines() + newHandles\n        ax.legend(\n            lines,\n            [l.get_label() for l in lines],\n            loc=\'best\',\n            shadow=True)\n\n        # Build out the xtick chart by the dates\n        ax.xaxis.grid(True, which=\'minor\')\n        ax.fmt_xdata = mdates.DateFormatter(date_format)\n        ax.xaxis.set_major_formatter(ax.fmt_xdata)\n        ax.xaxis.set_minor_formatter(ax.fmt_xdata)\n\n        # turn off the grids on volume\n        ax2.fmt_xdata = mdates.DateFormatter(date_format)\n        ax2.xaxis.grid(False)\n        ax2.yaxis.grid(False)\n        ax2.yaxis.set_ticklabels([])\n\n        fig.autofmt_xdate()\n\n        show_with_entities(\n            log_label=log_label,\n            xlabel=xlabel,\n            ylabel=ylabel,\n            title=use_title,\n            ax=ax,\n            fig=fig,\n            show_plot=show_plot)\n\n        rec[\'fig\'] = fig\n        rec[\'ax\'] = ax\n        rec[\'ax2\'] = ax2\n\n        result = build_result.build_result(\n            status=ae_consts.SUCCESS,\n            err=None,\n            rec=rec)\n\n    except Exception as e:\n        err = (\n            \'failed plot_overlay_pricing_and_volume \'\n            f\'and volume with ex={e}\')\n        result = build_result.build_result(\n            status=ae_consts.ERR,\n            err=err,\n            rec=rec)\n    # end of try/ex\n\n    send_final_log(\n        log_label=log_label,\n        fn_name=\'plot_overlay_pricing_and_volume\',\n        result=result)\n\n    return result\n# end of plot_overlay_pricing_and_volume\n\n\ndef plot_hloc_pricing(\n        log_label,\n        ticker,\n        df,\n        title,\n        show_plot=True,\n        dropna_for_all=True):\n    """"""plot_hloc_pricing\n\n    Plot the high, low, open and close columns together on a chart\n\n    :param log_label: log identifier\n    :param ticker: ticker\n    :param df: initialized ``pandas.DataFrame``\n    :param title: title for the chart\n    :param show_plot: bool to show the plot\n    :param dropna_for_all: optional - bool to toggle keep None\'s in\n                           the plot ``df`` (default is drop them\n                           for display purposes)\n    """"""\n\n    rec = {\n        \'ax\': None,\n        \'fig\': None\n    }\n    result = build_result.build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec=rec)\n\n    try:\n\n        log.info(\n            f\'{log_label} - \'\n            \'plot_hloc_pricing\'\n            \' - start\')\n\n        set_common_seaborn_fonts()\n\n        fig, ax = plt.subplots(\n            figsize=(15.0, 10.0))\n\n        use_df = df\n        if dropna_for_all:\n            log.info(\n                f\'{log_label} - \'\n                \'plot_hloc_pricing\'\n                \' - dropna_for_all\')\n            use_df = df.dropna(axis=0, how=\'any\')\n        # end of pre-plot dataframe scrubbing\n\n        plt.plot(\n            use_df[\'date\'],\n            use_df[\'high\'],\n            label=\'High\',\n            color=ae_consts.PLOT_COLORS[\'high\'],\n            alpha=0.4)\n        plt.plot(\n            use_df[\'date\'],\n            use_df[\'low\'],\n            label=\'Low\',\n            color=ae_consts.PLOT_COLORS[\'low\'],\n            alpha=0.4)\n        plt.plot(\n            use_df[\'date\'],\n            use_df[\'close\'],\n            label=\'Close\',\n            color=ae_consts.PLOT_COLORS[\'close\'],\n            alpha=0.4)\n        plt.plot(\n            use_df[\'date\'],\n            use_df[\'open\'],\n            label=\'Open\',\n            color=ae_consts.PLOT_COLORS[\'open\'],\n            alpha=0.4)\n\n        xlabel = \'Dates\'\n        ylabel = \'Prices\'\n\n        plt.grid(True)\n        plt.xlabel(xlabel)\n        plt.ylabel(ylabel)\n\n        # Build a date vs Close DataFrame\n        start_date = str(use_df.iloc[0][\'date\'].strftime(\'%Y-%m-%d\'))\n        end_date = str(use_df.iloc[-1][\'date\'].strftime(\'%Y-%m-%d\'))\n        if not title:\n            title = (\n                f\'{ticker} Pricing from: {start_date} to {end_date}\')\n        ax.set_title(title)\n\n        # Build out the xtick chart by the dates\n        ax.xaxis.grid(True, which=\'minor\')\n\n        legend_list = [\n            \'high\',\n            \'low\',\n            \'close\',\n            \'open\'\n        ]\n\n        fig.autofmt_xdate()\n        show_with_entities(\n            log_label=log_label,\n            xlabel=xlabel,\n            ylabel=ylabel,\n            title=title,\n            ax=ax,\n            fig=fig,\n            legend_list=legend_list,\n            show_plot=show_plot)\n\n        rec[\'ax\'] = ax\n        rec[\'fig\'] = fig\n\n        result = build_result.build_result(\n            status=ae_consts.SUCCESS,\n            err=None,\n            rec=rec)\n\n    except Exception as e:\n        err = (\n            f\'failed plot_hloc_pricing with ex={e}\')\n        log.error(err)\n        result = build_result.build_result(\n            status=ae_consts.ERR,\n            err=err,\n            rec=rec)\n    # end of try/ex\n\n    send_final_log(\n        log_label=log_label,\n        fn_name=\'plot_hloc_pricing\',\n        result=result)\n\n    return result\n# end of plot_hloc_pricing\n\n\ndef plot_df(\n        log_label,\n        title,\n        column_list,\n        df,\n        xcol=\'date\',\n        xlabel=\'Date\',\n        ylabel=\'Pricing\',\n        linestyle=\'-\',\n        color=\'blue\',\n        show_plot=True,\n        dropna_for_all=True):\n    """"""plot_df\n    :param log_label: log identifier\n    :param title: title of the plot\n    :param column_list: list of columns in the df to show\n    :param df: initialized ``pandas.DataFrame``\n    :param xcol: x-axis column in the initialized ``pandas.DataFrame``\n    :param xlabel: x-axis label\n    :param ylabel: y-axis label\n    :param linestyle: style of the plot line\n    :param color: color to use\n    :param show_plot: bool to show the plot\n    :param dropna_for_all: optional - bool to toggle keep None\'s in\n                           the plot ``df`` (default is drop them\n                           for display purposes)\n    """"""\n\n    rec = {\n        \'ax\': None,\n        \'fig\': None\n    }\n    result = build_result.build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec=rec)\n\n    try:\n\n        log.info(\n            f\'{log_label} - \'\n            \'plot_df\'\n            \' - start\')\n\n        use_df = df\n        if dropna_for_all:\n            log.info(\n                f\'{log_label} - \'\n                \'plot_df\'\n                \' - dropna_for_all\')\n            use_df = df.dropna(axis=0, how=\'any\')\n        # end of pre-plot dataframe scrubbing\n\n        set_common_seaborn_fonts()\n\n        hex_color = ae_consts.PLOT_COLORS[\'blue\']\n        fig, ax = plt.subplots(figsize=(15.0, 10.0))\n\n        if linestyle == \'-\':\n            use_df[column_list].plot(\n                x=xcol,\n                linestyle=linestyle,\n                ax=ax,\n                color=hex_color,\n                rot=0)\n        else:\n            use_df[column_list].plot(\n                kind=\'bar\',\n                x=xcol,\n                ax=ax,\n                color=hex_color,\n                rot=0)\n\n        if \'date\' in str(xcol).lower():\n            # plot the column\n            min_date = use_df.iloc[0][xcol]\n            max_date = use_df.iloc[-1][xcol]\n\n            # give a bit of space at each end of the plot - aesthetics\n            span = max_date - min_date\n            extra = int(span.days * 0.03) * datetime.timedelta(days=1)\n            ax.set_xlim([min_date - extra, max_date + extra])\n\n            # format the x tick marks\n            ax.xaxis.set_minor_formatter(mdates.DateFormatter(\'%b\\n%Y\'))\n            ax.xaxis.set_major_locator(plt.NullLocator())\n            ax.xaxis.set_minor_locator(\n                mdates.MonthLocator(\n                    bymonthday=1,\n                    interval=1))\n\n            # grid, legend and yLabel\n            ax.xaxis.grid(True, which=\'minor\')\n\n        # end of date formatting\n\n        show_with_entities(\n            log_label=log_label,\n            xlabel=xlabel,\n            ylabel=ylabel,\n            title=title,\n            ax=ax,\n            fig=fig,\n            show_plot=show_plot)\n\n        rec[\'ax\'] = ax\n        rec[\'fig\'] = fig\n\n        result = build_result.build_result(\n            status=ae_consts.SUCCESS,\n            err=None,\n            rec=rec)\n\n    except Exception as e:\n        err = (\n            f\'failed plot_df title={title} with ex={e}\')\n        result = build_result.build_result(\n            status=ae_consts.ERR,\n            err=err,\n            rec=rec)\n    # end of try/ex\n\n    send_final_log(\n        log_label=log_label,\n        fn_name=\'plot_df\',\n        result=result)\n\n    return result\n# end of plot_df\n\n\ndef dist_plot(\n        log_label,\n        df,\n        width=10.0,\n        height=10.0,\n        title=\'Distribution Plot\',\n        style=\'default\',\n        xlabel=\'\',\n        ylabel=\'\',\n        show_plot=True,\n        dropna_for_all=True):\n    """"""dist_plot\n\n    Show a distribution plot for the passed in dataframe: ``df``\n\n    :param log_label: log identifier\n    :param df: initialized ``pandas.DataFrame``\n    :param width: width of figure\n    :param height: height of figure\n    :param style: style to use\n    :param xlabel: x-axis label\n    :param ylabel: y-axis label\n    :param show_plot: bool to show plot or not\n    :param dropna_for_all: optional - bool to toggle keep None\'s in\n                           the plot ``df`` (default is drop them\n                           for display purposes)\n    """"""\n\n    rec = {}\n    result = build_result.build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec=rec)\n\n    try:\n\n        log.info(\n            f\'{log_label} - \'\n            \'dist_plot\'\n            \' - start\')\n\n        set_common_seaborn_fonts()\n\n        fig, ax = plt.subplots(\n            figsize=(width, height))\n\n        if style == \'default\':\n            sns.set_context(\'poster\')\n            sns.axes_style(\'darkgrid\')\n\n        use_df = df\n        if dropna_for_all:\n            log.info(\n                f\'{log_label} - \'\n                \'dist_plot\'\n                \' - dropna_for_all\')\n            use_df = df.dropna(axis=0, how=\'any\')\n        # end of pre-plot dataframe scrubbing\n\n        sns.distplot(\n            use_df,\n            color=ae_consts.PLOT_COLORS[\'blue\'],\n            ax=ax)\n\n        if xlabel != \'\':\n            ax.set_xlabel(xlabel)\n        if ylabel != \'\':\n            ax.set_ylabel(ylabel)\n\n        plt.tight_layout()\n        plt.subplots_adjust(top=0.9)\n        fig.suptitle(title)\n\n        show_with_entities(\n            log_label=log_label,\n            xlabel=xlabel,\n            ylabel=ylabel,\n            title=title,\n            ax=ax,\n            fig=fig,\n            show_plot=show_plot)\n\n        rec[\'ax\'] = ax\n        rec[\'fig\'] = fig\n\n        result = build_result.build_result(\n            status=ae_consts.SUCCESS,\n            err=None,\n            rec=rec)\n\n    except Exception as e:\n        err = (\n            f\'failed dist_plot title={title} with ex={e}\')\n        log.error(err)\n        result = build_result.build_result(\n            status=ae_consts.ERR,\n            err=err,\n            rec=rec)\n    # end of try/ex\n\n    send_final_log(\n        log_label=log_label,\n        fn_name=\'dist_plot\',\n        result=result)\n\n    return result\n# end of dist_plot\n'"
analysis_engine/compress_data.py,0,"b'""""""\nHelper for compressing a ``dict`` or ``pandas.DataFrame``\n""""""\n\nimport json\nimport zlib\nimport analysis_engine.consts as ae_consts\n\n\ndef compress_data(\n        data,\n        encoding=\'utf-8\',\n        date_format=None):\n    """"""compress_data\n\n    Helper for compressing ``data`` which can be\n    either a ``dict`` or a ``pandas.DataFrame``\n    objects with zlib.\n\n    :param data: ``dict`` or ``pandas.DataFrame`` object\n        to compress\n    :param encoding: optional encoding - default is ``utf-8``\n    :param date_format: optional date format - default is ``None``\n    """"""\n\n    converted_json = None\n    if ae_consts.is_df(df=data):\n        if date_format:\n            converted_json = data.to_json(\n                orient=\'records\',\n                date_format=date_format)\n        else:\n            converted_json = data.to_json(\n                orient=\'records\')\n    else:\n        converted_json = data\n\n    converted_str = json.dumps(\n        converted_json).encode(\n            encoding)\n    compressed_str = zlib.compress(converted_str, 9)\n\n    return compressed_str\n# end of compress_data\n'"
analysis_engine/consts.py,0,"b'""""""\nConsts and helper functions\n\n**Algorithm Environment Variables**\n\n.. code-block:: python\n\n    ALGO_MODULE_PATH = ev(\n        \'ALGO_MODULE_PATH\',\n        \'/opt/sa/analysis_engine/mocks/example_algo_minute.py\')\n    ALGO_BASE_MODULE_PATH = ev(\n        \'ALGO_BASE_MODULE_PATH\',\n        \'/opt/sa/analysis_engine/algo.py\')\n    ALGO_MODULE_NAME = ev(\n        \'ALGO_MODULE_NAME\',\n        \'example_algo_minute\')\n    ALGO_VERSION = ev(\n        \'ALGO_VERSION\',\n        \'1\')\n    ALGO_BUYS_S3_BUCKET_NAME = ev(\n        \'ALGO_BUYS_S3_BUCKET_NAME\',\n        \'algobuys\')\n    ALGO_SELLS_S3_BUCKET_NAME = ev(\n        \'ALGO_SELLS_S3_BUCKET_NAME\',\n        \'algosells\')\n    ALGO_RESULT_S3_BUCKET_NAME = ev(\n        \'ALGO_RESULT_S3_BUCKET_NAME\',\n        \'algoresults\')\n    ALGO_READY_DATASET_S3_BUCKET_NAME = ev(\n        \'ALGO_READY_DATASET_S3_BUCKET_NAME\',\n        \'algoready\')\n    ALGO_EXTRACT_DATASET_S3_BUCKET_NAME = ev(\n        \'ALGO_EXTRACT_DATASET_S3_BUCKET_NAME\',\n        \'algoready\')\n    ALGO_HISTORY_DATASET_S3_BUCKET_NAME = ev(\n        \'ALGO_HISTORY_DATASET_S3_BUCKET_NAME\',\n        \'algohistory\')\n    ALGO_REPORT_DATASET_S3_BUCKET_NAME = ev(\n        \'ALGO_REPORT_DATASET_S3_BUCKET_NAME\',\n        \'algoreport\')\n    ALGO_BACKUP_DATASET_S3_BUCKET_NAME = ev(\n        \'ALGO_BACKUP_DATASET_S3_BUCKET_NAME\',\n        \'algobackup\')\n    ALGO_READY_DIR = ev(\n        \'ALGO_READY_DIR\',\n        \'/tmp\')\n    ALGO_EXTRACT_DIR = ev(\n        \'ALGO_EXTRACT_DIR\',\n        \'/tmp\')\n    ALGO_HISTORY_DIR = ev(\n        \'ALGO_HISTORY_HISTORY_DIR\',\n        \'/tmp\')\n    ALGO_REPORT_DIR = ev(\n        \'ALGO_REPORT_DIR\',\n        \'/tmp\')\n    ALGO_LOAD_DIR = ev(\n        \'ALGO_LOAD_DIR\',\n        \'/tmp\')\n    ALGO_BACKUP_DIR = ev(\n        \'ALGO_BACKUP_DIR\',\n        \'/tmp\')\n    ALGO_READY_REDIS_ADDRESS = ev(\n        \'ALGO_READY_REDIS_ADDRESS\',\n        \'localhost:6379\')\n    ALGO_EXTRACT_REDIS_ADDRESS = ev(\n        \'ALGO_EXTRACT_REDIS_ADDRESS\',\n        \'localhost:6379\')\n    ALGO_HISTORY_REDIS_ADDRESS = ev(\n        \'ALGO_HISTORY_REDIS_ADDRESS\',\n        \'localhost:6379\')\n    ALGO_REPORT_REDIS_ADDRESS = ev(\n        \'ALGO_REPORT_REDIS_ADDRESS\',\n        \'localhost:6379\')\n    ALGO_BACKUP_REDIS_ADDRESS = ev(\n        \'ALGO_BACKUP_REDIS_ADDRESS\',\n        \'localhost:6379\')\n    ALGO_HISTORY_VERSION = ev(\n        \'ALGO_HISTORY_VERSION\',\n        \'1\')\n    ALGO_REPORT_VERSION = ev(\n        \'ALGO_REPORT_VERSION\',\n        \'1\')\n\n**Stock and Analysis Environment Variables**\n\n::\n\n    TICKER = ev(\n        \'TICKER\',\n        \'SPY\')\n    TICKER_ID = int(ev(\n        \'TICKER_ID\',\n        \'1\'))\n    DEFAULT_TICKERS = ev(\n        \'DEFAULT_TICKERS\',\n        \'SPY,AMZN,TSLA,NFLX\').split(\',\')\n    NEXT_EXP = opt_dates.option_expiration()\n    NEXT_EXP_STR = NEXT_EXP.strftime(\'%Y-%m-%d\')\n\n**Logging Environment Variables**\n\n.. code-block:: python\n\n    LOG_CONFIG_PATH = ev(\n        \'LOG_CONFIG_PATH\',\n        \'./analysis_engine/log/logging.json\')\n\n**Slack Environment Variables**\n\n.. code-block:: python\n\n    SLACK_WEBHOOK = ev(\n        \'SLACK_WEBHOOK\',\n        None)\n    SLACK_ACCESS_TOKEN = ev(\n        \'SLACK_ACCESS_TOKEN\',\n        None\n    )\n    SLACK_PUBLISH_PLOT_CHANNELS = ev(\n        \'SLACK_PUBLISH_PLOT_CHANNELS\',\n        None\n    )\n    PROD_SLACK_ALERTS = ev(\n        \'PROD_SLACK_ALERTS\',\n        \'0\')\n\n**Celery Environment Variables**\n\n.. code-block:: python\n\n    SSL_OPTIONS = {}\n    TRANSPORT_OPTIONS = {}\n    WORKER_BROKER_URL = ev(\n        \'WORKER_BROKER_URL\',\n        \'redis://localhost:6379/11\')\n    WORKER_BACKEND_URL = ev(\n        \'WORKER_BACKEND_URL\',\n        \'redis://localhost:6379/12\')\n    WORKER_CELERY_CONFIG_MODULE = ev(\n        \'WORKER_CELERY_CONFIG_MODULE\',\n        \'analysis_engine.work_tasks.celery_config\')\n    WORKER_TASKS = ev(\n        \'WORKER_TASKS\',\n        (\'analysis_engine.work_tasks.task_run_algo\'))\n    INCLUDE_TASKS = WORKER_TASKS.split(\',\')\n\n**Supported S3 Environment Variables**\n\n.. code-block:: python\n\n    ENABLED_S3_UPLOAD = ev(\n        \'ENABLED_S3_UPLOAD\',\n        \'0\') == \'1\'\n    S3_ACCESS_KEY = ev(\n        \'AWS_ACCESS_KEY_ID\',\n        \'trexaccesskey\')\n    S3_SECRET_KEY = ev(\n        \'AWS_SECRET_ACCESS_KEY\',\n        \'trex123321\')\n    S3_REGION_NAME = ev(\n        \'AWS_DEFAULT_REGION\',\n        \'us-east-1\')\n    S3_ADDRESS = ev(\n        \'S3_ADDRESS\',\n        \'0.0.0.0:9000\')\n    S3_SECURE = ev(\n        \'S3_SECURE\',\n        \'0\') == \'1\'\n    S3_BUCKET = ev(\n        \'S3_BUCKET\',\n        \'pricing\')\n    S3_COMPILED_BUCKET = ev(\n        \'S3_COMPILED_BUCKET\',\n        \'compileddatasets\')\n    S3_KEY = ev(\n        \'S3_KEY\',\n        \'test_key\')\n    DAILY_S3_BUCKET_NAME = ev(\n        \'DAILY_S3_BUCKET_NAME\',\n        \'daily\')\n    MINUTE_S3_BUCKET_NAME = ev(\n        \'MINUTE_S3_BUCKET_NAME\',\n        \'minute\')\n    QUOTE_S3_BUCKET_NAME = ev(\n        \'QUOTE_S3_BUCKET_NAME\',\n        \'quote\')\n    STATS_S3_BUCKET_NAME = ev(\n        \'STATS_S3_BUCKET_NAME\',\n        \'stats\')\n    PEERS_S3_BUCKET_NAME = ev(\n        \'PEERS_S3_BUCKET_NAME\',\n        \'peers\')\n    NEWS_S3_BUCKET_NAME = ev(\n        \'NEWS_S3_BUCKET_NAME\',\n        \'news\')\n    FINANCIALS_S3_BUCKET_NAME = ev(\n        \'FINANCIALS_S3_BUCKET_NAME\',\n        \'financials\')\n    EARNINGS_S3_BUCKET_NAME = ev(\n        \'EARNINGS_S3_BUCKET_NAME\',\n        \'earnings\')\n    DIVIDENDS_S3_BUCKET_NAME = ev(\n        \'DIVIDENDS_S3_BUCKET_NAME\',\n        \'dividends\')\n    COMPANY_S3_BUCKET_NAME = ev(\n        \'COMPANY_S3_BUCKET_NAME\',\n        \'company\')\n    PREPARE_S3_BUCKET_NAME = ev(\n        \'PREPARE_S3_BUCKET_NAME\',\n        \'prepared\')\n    ANALYZE_S3_BUCKET_NAME = ev(\n        \'ANALYZE_S3_BUCKET_NAME\',\n        \'analyzed\')\n    SCREENER_S3_BUCKET_NAME = ev(\n        \'SCREENER_S3_BUCKET_NAME\',\n        \'screener-data\')\n    PRICING_S3_BUCKET_NAME = ev(\n        \'PRICING_S3_BUCKET_NAME\',\n        \'pricing\')\n    OPTIONS_S3_BUCKET_NAME = ev(\n        \'OPTIONS_S3_BUCKET_NAME\',\n        \'options\')\n\n**Supported Redis Environment Variables**\n\n.. code-block:: python\n\n    ENABLED_REDIS_PUBLISH = ev(\n        \'ENABLED_REDIS_PUBLISH\',\n        \'0\') == \'1\'\n    REDIS_ADDRESS = ev(\n        \'REDIS_ADDRESS\',\n        \'localhost:6379\')\n    REDIS_KEY = ev(\n        \'REDIS_KEY\',\n        \'test_redis_key\')\n    REDIS_PASSWORD = ev(\n        \'REDIS_PASSWORD\',\n        None)\n    REDIS_DB = int(ev(\n        \'REDIS_DB\',\n        \'0\'))\n    REDIS_EXPIRE = ev(\n        \'REDIS_EXPIRE\',\n        None)\n\n""""""\n\nimport os\nimport sys\nimport json\nimport analysis_engine.options_dates as opt_dates\n\n\ndef ev(\n        k,\n        v):\n    \'\'\'ev\n\n    :param k: environment variable key\n    :param v: environment variable value\n    \'\'\'\n    val = os.getenv(k, v)\n    if val:\n        return val.strip()\n    return val\n# end of ev\n\n\nSUCCESS = 0\nFAILED = 1\nERR = 2\nEX = 3\nNOT_RUN = 4\nINVALID = 5\nNOT_DONE = 6\nNOT_SET = 7\nEMPTY = 8\nTRADE_OPEN = 9\nTRADE_NOT_ENOUGH_FUNDS = 10\nTRADE_FILLED = 11\nTRADE_NO_SHARES_TO_SELL = 12\nTRADE_EXPIRED = 13\nTRADE_SHARES = 14\nTRADE_VERTICAL_BULL_SPREAD = 15\nTRADE_VERTICAL_BEAR_SPREAD = 16\nTRADE_PROFITABLE = 17\nTRADE_NOT_PROFITABLE = 18\nTRADE_HIT_STOP_LOSS = 19\nTRADE_HIT_STOP_LOSS_PERCENT = 20\nTRADE_HIT_TAILING_STOP_LOSS = 21\nTRADE_HIT_TAILING_STOP_LOSS_PERCENT = 22\nTRADE_INVALID = 23\nTRADE_ERROR = 24\nTRADE_ENTRY = 25\nTRADE_EXIT = 26\nBACKTEST_FOUND_TRADE_PROFITABLE = 27\nBACKTEST_FOUND_TRADE_NOT_PROFITABLE = 28\nBACKTEST_FOUND_TRADE_NEVER_FILLED = 29  # limit order price never hit\nBACKTEST_FOUND_TRADE_EXPIRED = 30  # trades assumed are expired after a day\nSPREAD_VERTICAL_BULL = 31\nSPREAD_VERTICAL_BEAR = 32\nOPTION_CALL = 33\nOPTION_PUT = 34\nALGO_PROFITABLE = 35\nALGO_NOT_PROFITABLE = 36\nALGO_ERROR = 37\nALGO_NOT_ACTIVE = 38\nS3_FAILED = 39\nREDIS_FAILED = 40\nFILE_FAILED = 41\nSLACK_FAILED = 42\nALGO_TIMESERIES_DAY = 43  # evaluate trade performance on daily-units\nALGO_TIMESERIES_MINUTE = 44  # evaluate trade performance on minute-units\nALGO_TRADE_INDICATOR_COUNTS = 45  # trade off num indicators said buy/sell\nMISSING_TOKEN = 46\n\n# Assuming the engine is running in UTC timezones\nEST_OFFSET_HOURS = int(\n    os.getenv(\'EST_OFFSET_HOURS\', \'5\'))\n\nINDICATOR_CATEGORY_MOMENTUM = 60\nINDICATOR_CATEGORY_OVERLAP = 61\nINDICATOR_CATEGORY_PRICE = 62\nINDICATOR_CATEGORY_VOLUME = 63\nINDICATOR_CATEGORY_VOLATILITY = 64\nINDICATOR_CATEGORY_SINGLE_CALL = 65\nINDICATOR_CATEGORY_SINGLE_PUT = 66\nINDICATOR_CATEGORY_BULL_CALL = 67\nINDICATOR_CATEGORY_BEAR_PUT = 68\nINDICATOR_CATEGORY_QUARTERLY = 69\nINDICATOR_CATEGORY_YEARLY = 70\nINDICATOR_CATEGORY_INCOME_STMT = 71\nINDICATOR_CATEGORY_CASH_FLOW = 72\nINDICATOR_CATEGORY_BALANCE_SHEET = 73\nINDICATOR_CATEGORY_PRESS_RELEASE = 74\nINDICATOR_CATEGORY_CUSTOM = 75\nINDICATOR_CATEGORY_NEWS = 76\nINDICATOR_CATEGORY_EARNINGS = 77\nINDICATOR_CATEGORY_CSUITE = 78\nINDICATOR_CATEGORY_SPLITS = 79\nINDICATOR_CATEGORY_REVERSE_SPLITS = 80\nINDICATOR_CATEGORY_DISTRIBUTIONS = 81\nINDICATOR_CATEGORY_SPINOFFS = 82\nINDICATOR_CATEGORY_MERGER_ACQ = 83\nINDICATOR_CATEGORY_EXCHANGE_INCLUSION = 84\nINDICATOR_CATEGORY_EXCHANGE_EXCLUSION = 85\nINDICATOR_CATEGORY_TRIAL_POSITIVE = 86\nINDICATOR_CATEGORY_TRIAL_NEGATIVE = 87\nINDICATOR_CATEGORY_SHORT_SELLERS = 88\nINDICATOR_CATEGORY_REAL_ESTATE = 89\nINDICATOR_CATEGORY_HOUSING = 90\nINDICATOR_CATEGORY_PIPELINE = 91\nINDICATOR_CATEGORY_CONSTRUCTION = 94\nINDICATOR_CATEGORY_FED = 93\nINDICATOR_CATEGORY_UNKNOWN = 94\n\nINDICATOR_TYPE_TECHNICAL = 200\nINDICATOR_TYPE_FUNDAMENTAL = 201\nINDICATOR_TYPE_NEWS = 202\nINDICATOR_TYPE_SECTOR = 203\nINDICATOR_TYPE_MARKET = 204\nINDICATOR_TYPE_DIVIDEND = 205\nINDICATOR_TYPE_CUSTOM = 206\nINDICATOR_TYPE_UNKNOWN = 207\n\nINDICATOR_USES_DAILY_DATA = 500\nINDICATOR_USES_MINUTE_DATA = 501\nINDICATOR_USES_QUOTE_DATA = 502\nINDICATOR_USES_STATS_DATA = 503\nINDICATOR_USES_PEERS_DATA = 504\nINDICATOR_USES_NEWS_DATA = 505\nINDICATOR_USES_FINANCIAL_DATA = 506\nINDICATOR_USES_EARNINGS_DATA = 507\nINDICATOR_USES_DIVIDENDS_DATA = 508\nINDICATOR_USES_COMPANY_DATA = 509\nINDICATOR_USES_PRICING_DATA = 510\nINDICATOR_USES_OPTIONS_DATA = 511\nINDICATOR_USES_CALLS_DATA = 512\nINDICATOR_USES_PUTS_DATA = 513\nINDICATOR_USES_DATA_UNSUPPORTED = 514\nINDICATOR_USES_DATA_ANY = 515\nINDICATOR_USES_TDCALLS_DATA = 516\nINDICATOR_USES_TDPUTS_DATA = 517\n\nINT_INDICATOR_NOT_PROCESSED = 600\nINT_INDICATOR_IGNORE_ACTION = 601\nINT_INDICATOR_BUY_ACTION = 602\nINT_INDICATOR_SELL_ACTION = 603\n\nINDICATOR_BUY = \'buy\'\nINDICATOR_SELL = \'sell\'\nINDICATOR_IGNORE = \'ignore\'\nINDICATOR_RESET = \'reset\'\n\nSA_MODE_PREPARE = 10000\nSA_MODE_ANALYZE = 10001\nSA_MODE_PREDICT = 10002\nSA_MODE_EXTRACT = 10003\nSA_MODE_SHOW_DATASET = 10004\nSA_MODE_RESTORE_REDIS_DATASET = 10005\nSA_MODE_RUN_ALGO = 10006\nSA_MODE_SHOW_HISTORY_DATASET = 10007\nSA_MODE_SHOW_REPORT_DATASET = 10008\n\nSA_DATASET_TYPE_ALGO_READY = 20000\nSA_DATASET_TYPE_TRADING_HISTORY = 20001\nSA_DATASET_TYPE_TRADING_REPORT = 20002\n\nPLOT_ACTION_SHOW = 21000\nPLOT_ACTION_SAVE_TO_S3 = 21001\nPLOT_ACTION_SAVE_AS_FILE = 21002\n\nFETCH_MODE_ALL = 30000\nFETCH_MODE_YHO = 30001\nFETCH_MODE_IEX = 30002\nFETCH_MODE_TD = 30003\nFETCH_MODE_INTRADAY = 30004\nFETCH_MODE_DAILY = 30005\nFETCH_MODE_WEEKLY = 30006\nFETCH_MODE_INITIAL = 30007\n\n# GMT: Monday, January 19, 1970 12:26:40 PM\nEPOCH_MINIMUM_DATE = 1600000\n\nOPTIONS_UPPER_STRIKE = float(os.getenv(\n    \'OPTIONS_UPPER_STRIKE\',\n    \'10.0\'))\nOPTIONS_LOWER_STRIKE = float(os.getenv(\n    \'OPTIONS_UPPER_STRIKE\',\n    \'10.0\'))\nMAX_OPTIONS_UPPER_STRIKE = float(os.getenv(\n    \'MAX_OPTIONS_UPPER_STRIKE\',\n    \'200\'))\nMAX_OPTIONS_LOWER_STRIKE = float(os.getenv(\n    \'MAX_OPTIONS_LOWER_STRIKE\',\n    \'200\'))\nTRADIER_CONVERT_TO_DATETIME = [\n    \'date\',\n    \'created\',\n    \'ask_date\',\n    \'bid_date\',\n    \'trade_date\'\n]\n\n# version of python\nIS_PY2 = sys.version[0] == \'2\'\nNUM_BYTES_IN_AN_MB = 1048576\n\nAPP_NAME = ev(\n    \'APP_NAME\',\n    \'ae\')\nLOG_CONFIG_PATH = ev(\n    \'LOG_CONFIG_PATH\',\n    \'./analysis_engine/log/logging.json\')\nSSL_OPTIONS = {}\nTRANSPORT_OPTIONS = {}\nWORKER_BROKER_URL = ev(\n    \'WORKER_BROKER_URL\',\n    \'redis://localhost:6379/13\')\nWORKER_BACKEND_URL = ev(\n    \'WORKER_BACKEND_URL\',\n    \'redis://localhost:6379/14\')\nWORKER_CELERY_CONFIG_MODULE = ev(\n    \'WORKER_CELERY_CONFIG_MODULE\',\n    \'analysis_engine.work_tasks.celery_config\')\nWORKER_TASKS = ev(\n    \'WORKER_TASKS\',\n    (\'analysis_engine.work_tasks.task_run_algo,\'\n     \'analysis_engine.work_tasks.get_new_pricing_data,\'\n     \'analysis_engine.work_tasks.handle_pricing_update_task,\'\n     \'analysis_engine.work_tasks.prepare_pricing_dataset,\'\n     \'analysis_engine.work_tasks.publish_from_s3_to_redis,\'\n     \'analysis_engine.work_tasks.publish_pricing_update,\'\n     \'analysis_engine.work_tasks.task_screener_analysis,\'\n     \'analysis_engine.work_tasks.publish_ticker_aggregate_from_s3\'))\nINCLUDE_TASKS = WORKER_TASKS.split(\',\')\nCELERY_DISABLED = ev(\'CELERY_DISABLED\', \'0\') == \'1\'\n\n########################################\n#\n# Custom Variables\n#\n########################################\nTICKER = ev(\n    \'TICKER\',\n    \'SPY\')\nTICKER_ID = int(ev(\n    \'TICKER_ID\',\n    \'1\'))\nDEFAULT_TICKERS = ev(\n    \'DEFAULT_TICKERS\',\n    \'SPY,AMZN,TSLA,NFLX\').split(\',\')\nNEXT_EXP = opt_dates.option_expiration()\nNEXT_EXP_STR = NEXT_EXP.strftime(\'%Y-%m-%d\')\nDAILY_S3_BUCKET_NAME = ev(\n    \'DAILY_S3_BUCKET_NAME\',\n    \'daily\')\nMINUTE_S3_BUCKET_NAME = ev(\n    \'MINUTE_S3_BUCKET_NAME\',\n    \'minute\')\nQUOTE_S3_BUCKET_NAME = ev(\n    \'QUOTE_S3_BUCKET_NAME\',\n    \'quote\')\nSTATS_S3_BUCKET_NAME = ev(\n    \'STATS_S3_BUCKET_NAME\',\n    \'stats\')\nPEERS_S3_BUCKET_NAME = ev(\n    \'PEERS_S3_BUCKET_NAME\',\n    \'peers\')\nNEWS_S3_BUCKET_NAME = ev(\n    \'NEWS_S3_BUCKET_NAME\',\n    \'news\')\nFINANCIALS_S3_BUCKET_NAME = ev(\n    \'FINANCIALS_S3_BUCKET_NAME\',\n    \'financials\')\nEARNINGS_S3_BUCKET_NAME = ev(\n    \'EARNINGS_S3_BUCKET_NAME\',\n    \'earnings\')\nDIVIDENDS_S3_BUCKET_NAME = ev(\n    \'DIVIDENDS_S3_BUCKET_NAME\',\n    \'dividends\')\nCOMPANY_S3_BUCKET_NAME = ev(\n    \'COMPANY_S3_BUCKET_NAME\',\n    \'company\')\nFETCH_MODE = ev(\n    \'FETCH_MODE\',\n    \'all\')\nPREPARE_S3_BUCKET_NAME = ev(\n    \'PREPARE_S3_BUCKET_NAME\',\n    \'prepared\')\nANALYZE_S3_BUCKET_NAME = ev(\n    \'ANALYZE_S3_BUCKET_NAME\',\n    \'analyzed\')\nSCREENER_S3_BUCKET_NAME = ev(\n    \'SCREENER_S3_BUCKET_NAME\',\n    \'screener-data\')\nPRICING_S3_BUCKET_NAME = ev(\n    \'PRICING_S3_BUCKET_NAME\',\n    \'pricing\')\nOPTIONS_S3_BUCKET_NAME = ev(\n    \'OPTIONS_S3_BUCKET_NAME\',\n    \'options\')\nPREPARE_DATA_MIN_SIZE = 11\nPLOT_COLORS = {\n    \'red\': \'#E74C3C\',\n    \'feldspar\': \'#D19275\',\n    \'copper\': \'#EDC393\',\n    \'brown\': \'#6B4226\',\n    \'orange\': \'#FF7D40\',\n    \'maroon\': \'#800000\',\n    \'gray\': \'#8B8989\',\n    \'black\': \'#111111\',\n    \'pink\': \'#FFCCCC\',\n    \'green\': \'#2ECC71\',\n    \'blue\': \'#3498db\',\n    \'darkblue\': \'#000080\',\n    \'lightgreen\': \'#C0FF3E\',\n    \'darkgreen\': \'#385E0F\',\n    \'gold\': \'#FFCC11\',\n    \'yellow\': \'#FFE600\',\n    \'volumetop\': \'#385E0F\',\n    \'volume\': \'#ADFF2F\',\n    \'high\': \'#CC1100\',\n    \'low\': \'#164E71\',\n    \'open\': \'#608DC0\',\n    \'close\': \'#99CC32\',\n    \'white\': \'#FFFFFF\'\n}\n\nIEX_DAILY_DATE_FORMAT = \'%Y-%b-%d\'\nIEX_MINUTE_DATE_FORMAT = \'%Y-%m-%d %I:%M:%S %p\'\nIEX_TICK_DATE_FORMAT = \'%Y-%m-%d %I:%M:%S %p\'\nIEX_QUOTE_DATE_FORMAT = \'%B %d, %Y\'\nIEX_DATASETS_DEFAULT = [\n    \'daily\',\n    \'minute\',\n    \'quote\',\n    \'stats\',\n    \'peers\',\n    \'news\',\n    \'financials\',\n    \'earnings\',\n    \'dividends\',\n    \'company\'\n]\nIEX_INTRADAY_DATASETS = [\n    \'minute\',\n    \'news\'\n]\nIEX_DAILY_DATASETS = [\n    \'minute\',\n    \'daily\',\n    \'news\'\n]\nIEX_WEEKLY_DATASETS = [\n    \'minute\',\n    \'financials\',\n    \'earnings\',\n    \'dividends\',\n    \'peers\',\n    \'news\',\n    \'company\'\n]\n# Financial + Earnings are expensive\n# so disabled for new users just\n# getting started\nIEX_INITIAL_DATASETS = [\n    \'daily\',\n    \'minute\',\n    \'stats\',\n    \'news\',\n    \'company\'\n]\n\nBACKUP_DATASETS = [\n    \'tdcalls\',\n    \'tdputs\',\n    \'pricing\',\n    \'options\',\n    \'calls\',\n    \'puts\',\n    \'news1\'\n] + IEX_DATASETS_DEFAULT\nif os.getenv(\'BACKUP_DATASETS\', False):\n    BACKUP_DATASETS = os.getenv(\'BACKUP_DATASETS\', \'\').split(\',\')\n\nCOMMON_DATE_FORMAT = \'%Y-%m-%d\'\nCOMMON_TICK_DATE_FORMAT = \'%Y-%m-%d %H:%M:%S\'\nCACHE_DICT_VERSION = 1\n\nSLACK_WEBHOOK = ev(\n    \'SLACK_WEBHOOK\',\n    None)\nSLACK_ACCESS_TOKEN = ev(\n    \'SLACK_ACCESS_TOKEN\',\n    None\n)\nSLACK_PUBLISH_PLOT_CHANNELS = ev(\n    \'SLACK_PUBLISH_PLOT_CHANNELS\',\n    None\n)\nPROD_SLACK_ALERTS = ev(\n    \'PROD_SLACK_ALERTS\',\n    \'0\')\nDATASET_COLLECTION_VERSION = 1\nDATASET_COLLECTION_SLACK_ALERTS = ev(\n    \'DATASET_COLLECTION_SLACK_ALERTS\',\n    \'0\')\nSLACK_FINVIZ_ALL_COLUMNS = [\n    \'ticker\',\n    \'price\',\n    \'volume\',\n    \'change\',\n    \'pe\',\n    \'market_cap\',\n    \'company\',\n    \'industry\',\n    \'sector\',\n    \'country\'\n]\nSLACK_FINVIZ_COLUMNS = [\n    \'ticker\',\n    \'price\',\n    \'volume\',\n    \'change\',\n    \'pe\',\n    \'market_cap\'\n]\nALGO_INPUT_COMPRESS = (ev(\n    \'ALGO_INPUT_COMPRESS\',\n    \'0\') == \'1\')\nALGO_LOAD_COMPRESS = (ev(\n    \'ALGO_LOAD_COMPRESS\',\n    \'0\') == \'1\')\nALGO_HISTORY_COMPRESS = (ev(\n    \'ALGO_HISTORY_COMPRESS\',\n    \'1\') == \'1\')\nALGO_HISTORY_VERSION = ev(\n    \'ALGO_HISTORY_VERSION\',\n    \'1\')\nALGO_REPORT_COMPRESS = (ev(\n    \'ALGO_REPORT_COMPRESS\',\n    \'0\') == \'1\')\nALGO_REPORT_VERSION = ev(\n    \'ALGO_REPORT_VERSION\',\n    \'1\')\nDEFAULT_SERIALIZED_DATASETS = [\n    \'daily\',\n    \'minute\',\n    \'quote\',\n    \'stats\',\n    \'peers\',\n    \'news1\',\n    \'financials\',\n    \'earnings\',\n    \'dividends\',\n    \'company\',\n    \'news\',\n    \'calls\',\n    \'puts\',\n    \'pricing\',\n    \'tdcalls\',\n    \'tdputs\'\n]\nEMPTY_DF_STR = \'[{}]\'\nEMPTY_DF_LIST = [{}]\n\n########################################\n#\n# Algorithm Variables\n#\n########################################\nALGO_MODULE_PATH = ev(\n    \'ALGO_MODULE_PATH\',\n    \'/opt/sa/analysis_engine/mocks/example_algo_minute.py\')\nALGO_BASE_MODULE_PATH = ev(\n    \'ALGO_BASE_MODULE_PATH\',\n    \'/opt/sa/analysis_engine/algo.py\')\nALGO_MODULE_NAME = ev(\n    \'ALGO_MODULE_NAME\',\n    \'example_algo_minute\')\nALGO_VERSION = ev(\n    \'ALGO_VERSION\',\n    \'1\')\nALGO_BUYS_S3_BUCKET_NAME = ev(\n    \'ALGO_BUYS_S3_BUCKET_NAME\',\n    \'algobuys\')\nALGO_SELLS_S3_BUCKET_NAME = ev(\n    \'ALGO_SELLS_S3_BUCKET_NAME\',\n    \'algosells\')\nALGO_RESULT_S3_BUCKET_NAME = ev(\n    \'ALGO_RESULT_S3_BUCKET_NAME\',\n    \'algoresult\')\nALGO_EXTRACT_DATASET_S3_BUCKET_NAME = ev(\n    \'ALGO_EXTRACT_DATASET_S3_BUCKET_NAME\',\n    \'algoready\')\nALGO_READY_DATASET_S3_BUCKET_NAME = ev(\n    \'ALGO_READY_DATASET_S3_BUCKET_NAME\',\n    \'algoready\')\nALGO_HISTORY_DATASET_S3_BUCKET_NAME = ev(\n    \'ALGO_HISTORY_DATASET_S3_BUCKET_NAME\',\n    \'algohistory\')\nALGO_REPORT_DATASET_S3_BUCKET_NAME = ev(\n    \'ALGO_REPORT_DATASET_S3_BUCKET_NAME\',\n    \'algoreport\')\nALGO_BACKUP_DATASET_S3_BUCKET_NAME = ev(\n    \'ALGO_BACKUP_DATASET_S3_BUCKET_NAME\',\n    \'algobackup\')\nALGO_READY_DIR = ev(\n    \'ALGO_READY_DIR\',\n    \'/tmp\')\nALGO_EXTRACT_DIR = ev(\n    \'ALGO_EXTRACT_DIR\',\n    \'/tmp\')\nALGO_HISTORY_DIR = ev(\n    \'ALGO_HISTORY_HISTORY_DIR\',\n    \'/tmp\')\nALGO_REPORT_DIR = ev(\n    \'ALGO_REPORT_DIR\',\n    \'/tmp\')\nALGO_LOAD_DIR = ev(\n    \'ALGO_LOAD_DIR\',\n    \'/tmp\')\nALGO_BACKUP_DIR = ev(\n    \'ALGO_BACKUP_DIR\',\n    \'/tmp\')\nALGO_READY_REDIS_ADDRESS = ev(\n    \'ALGO_READY_REDIS_ADDRESS\',\n    \'localhost:6379\')\nALGO_EXTRACT_REDIS_ADDRESS = ev(\n    \'ALGO_EXTRACT_REDIS_ADDRESS\',\n    \'localhost:6379\')\nALGO_HISTORY_REDIS_ADDRESS = ev(\n    \'ALGO_HISTORY_REDIS_ADDRESS\',\n    \'localhost:6379\')\nALGO_REPORT_REDIS_ADDRESS = ev(\n    \'ALGO_REPORT_REDIS_ADDRESS\',\n    \'localhost:6379\')\nALGO_BACKUP_REDIS_ADDRESS = ev(\n    \'ALGO_BACKUP_REDIS_ADDRESS\',\n    \'localhost:6379\')\n\n########################################\n#\n# Indicator Variables\n#\n########################################\nINDICATOR_BASE_MODULE = ev(\n    \'INDICATOR_BASE_MODULE\',\n    \'analysis_engine.indicators.base_indicator.BaseIndicator\')\nINDICATOR_BASE_MODULE_PATH = ev(\n    \'INDICATOR_BASE_MODULE_PATH\',\n    \'analysis_engine/indicators/base_indicator.py\')\nINDICATOR_IGNORED_CONIGURABLE_KEYS = [\n    \'name\',\n    \'module_path\',\n    \'category\',\n    \'type\',\n    \'uses_data\',\n    \'obj\',\n    \'report\'\n]\n\n########################################\n#\n# S3 Variables\n#\n########################################\nENABLED_S3_UPLOAD = ev(\n    \'ENABLED_S3_UPLOAD\',\n    \'0\') == \'1\'\nS3_ACCESS_KEY = ev(\n    \'AWS_ACCESS_KEY_ID\',\n    \'trexaccesskey\')\nS3_SECRET_KEY = ev(\n    \'AWS_SECRET_ACCESS_KEY\',\n    \'trex123321\')\nS3_REGION_NAME = ev(\n    \'AWS_DEFAULT_REGION\',\n    \'us-east-1\')\nS3_ADDRESS = ev(\n    \'S3_ADDRESS\',\n    \'0.0.0.0:9000\')\nS3_SECURE = ev(\n    \'S3_SECURE\',\n    \'0\') == \'1\'\nS3_BUCKET = ev(\n    \'S3_BUCKET\',\n    \'pricing\')\nS3_COMPILED_BUCKET = ev(\n    \'S3_COMPILED_BUCKET\',\n    \'compileddatasets\')\nS3_KEY = ev(\n    \'S3_KEY\',\n    \'test_key\')\n\n########################################\n#\n# Redis Variables\n#\n########################################\nENABLED_REDIS_PUBLISH = ev(\n    \'ENABLED_REDIS_PUBLISH\',\n    \'0\') == \'1\'\nREDIS_ADDRESS = ev(\n    \'REDIS_ADDRESS\',\n    \'localhost:6379\')\nREDIS_KEY = ev(\n    \'REDIS_KEY\',\n    \'test_redis_key\')\nREDIS_PASSWORD = ev(\n    \'REDIS_PASSWORD\',\n    None)\nREDIS_DB = int(ev(\n    \'REDIS_DB\',\n    \'0\'))\nREDIS_EXPIRE = ev(\n    \'REDIS_EXPIRE\',\n    None)\n\n# copy these values over\n# when calling child tasks from a\n# parent where the engine is\n# running inside a fully-dockerized\n# environment like kubernetes\n# or docker-compose\nSERVICE_VALS = [\n    \'ticker\',\n    \'s3_address\',\n    \'s3_access_key\',\n    \'s3_secret_key\',\n    \'s3_bucket\',\n    \'s3_secure\',\n    \'s3_region_name\',\n    \'redis_address\',\n    \'redis_db\',\n    \'redis_password\',\n    \'redis_expire\'\n]\n\n\ndef get_status(\n        status):\n    """"""get_status\n\n    Return the string label for an integer status code\n    which should be one of the ones above.\n\n    :param status: integer status code\n    """"""\n    if status == SUCCESS:\n        return \'SUCCESS\'\n    elif status == FAILED:\n        return \'FAILED\'\n    elif status == ERR:\n        return \'ERR\'\n    elif status == EX:\n        return \'EX\'\n    elif status == NOT_RUN:\n        return \'NOT_RUN\'\n    elif status == INVALID:\n        return \'INVALID\'\n    elif status == NOT_DONE:\n        return \'NOT_DONE\'\n    elif status == NOT_SET:\n        return \'NOT_SET\'\n    elif status == EMPTY:\n        return \'EMPTY\'\n    elif status == SA_MODE_PREPARE:\n        return \'SA_MODE_PREPARE\'\n    elif status == SA_MODE_ANALYZE:\n        return \'SA_MODE_ANALYZE\'\n    elif status == SA_MODE_PREDICT:\n        return \'SA_MODE_PREDICT\'\n    elif status == SA_MODE_EXTRACT:\n        return \'SA_MODE_EXTRACT\'\n    elif status == SA_MODE_SHOW_DATASET:\n        return \'SA_MODE_SHOW_DATASET\'\n    elif status == SA_MODE_RESTORE_REDIS_DATASET:\n        return \'SA_MODE_RESTORE_REDIS_DATASET\'\n    elif status == SA_MODE_RUN_ALGO:\n        return \'SA_MODE_RUN_ALGO\'\n    elif status == SA_MODE_SHOW_HISTORY_DATASET:\n        return \'SA_MODE_SHOW_HISTORY_DATASET\'\n    elif status == SA_MODE_SHOW_REPORT_DATASET:\n        return \'SA_MODE_SHOW_REPORT_DATASET\'\n    elif status == PLOT_ACTION_SHOW:\n        return \'PLOT_ACTION_SHOW\'\n    elif status == PLOT_ACTION_SAVE_TO_S3:\n        return \'PLOT_ACTION_SAVE_TO_S3\'\n    elif status == PLOT_ACTION_SAVE_AS_FILE:\n        return \'PLOT_ACTION_SAVE_AS_FILE\'\n    elif status == TRADE_OPEN:\n        return \'TRADE_OPEN\'\n    elif status == TRADE_NOT_ENOUGH_FUNDS:\n        return \'TRADE_NOT_ENOUGH_FUNDS\'\n    elif status == TRADE_FILLED:\n        return \'TRADE_FILLED\'\n    elif status == TRADE_NO_SHARES_TO_SELL:\n        return \'TRADE_NO_SHARES_TO_SELL\'\n    elif status == TRADE_EXPIRED:\n        return \'TRADE_EXPIRED\'\n    elif status == TRADE_SHARES:\n        return \'TRADE_SHARES\'\n    elif status == TRADE_VERTICAL_BULL_SPREAD:\n        return \'TRADE_VERTICAL_BULL_SPREAD\'\n    elif status == TRADE_VERTICAL_BEAR_SPREAD:\n        return \'TRADE_VERTICAL_BEAR_SPREAD\'\n    elif status == TRADE_PROFITABLE:\n        return \'TRADE_PROFITABLE\'\n    elif status == TRADE_NOT_PROFITABLE:\n        return \'TRADE_NOT_PROFITABLE\'\n    elif status == TRADE_HIT_STOP_LOSS:\n        return \'TRADE_HIT_STOP_LOSS\'\n    elif status == TRADE_HIT_STOP_LOSS_PERCENT:\n        return \'TRADE_HIT_STOP_LOSS_PERCENT\'\n    elif status == TRADE_HIT_TAILING_STOP_LOSS:\n        return \'TRADE_HIT_TAILING_STOP_LOSS\'\n    elif status == TRADE_HIT_TAILING_STOP_LOSS_PERCENT:\n        return \'TRADE_HIT_TAILING_STOP_LOSS_PERCENT\'\n    elif status == TRADE_INVALID:\n        return \'TRADE_INVALID\'\n    elif status == TRADE_ERROR:\n        return \'TRADE_ERROR\'\n    elif status == TRADE_ENTRY:\n        return \'TRADE_ENTRY\'\n    elif status == TRADE_EXIT:\n        return \'TRADE_EXIT\'\n    elif status == BACKTEST_FOUND_TRADE_PROFITABLE:\n        return \'BACKTEST_FOUND_TRADE_PROFITABLE\'\n    elif status == BACKTEST_FOUND_TRADE_NOT_PROFITABLE:\n        return \'BACKTEST_FOUND_TRADE_NOT_PROFITABLE\'\n    elif status == BACKTEST_FOUND_TRADE_NEVER_FILLED:\n        return \'BACKTEST_FOUND_TRADE_NEVER_FILLED\'\n    elif status == BACKTEST_FOUND_TRADE_EXPIRED:\n        return \'BACKTEST_FOUND_TRADE_EXPIRED\'\n    elif status == SPREAD_VERTICAL_BULL:\n        return \'SPREAD_VERTICAL_BULL\'\n    elif status == SPREAD_VERTICAL_BEAR:\n        return \'SPREAD_VERTICAL_BEAR\'\n    elif status == OPTION_CALL:\n        return \'OPTION_CALL\'\n    elif status == OPTION_PUT:\n        return \'OPTION_PUT\'\n    elif status == ALGO_PROFITABLE:\n        return \'ALGO_PROFITABLE\'\n    elif status == ALGO_NOT_PROFITABLE:\n        return \'ALGO_NOT_PROFITABLE\'\n    elif status == ALGO_ERROR:\n        return \'ALGO_ERROR\'\n    elif status == ALGO_NOT_ACTIVE:\n        return \'ALGO_NOT_ACTIVE\'\n    elif status == S3_FAILED:\n        return \'S3_FAILED\'\n    elif status == REDIS_FAILED:\n        return \'REDIS_FAILED\'\n    elif status == FILE_FAILED:\n        return \'FILE_FAILED\'\n    elif status == SLACK_FAILED:\n        return \'SLACK_FAILED\'\n    elif status == ALGO_TIMESERIES_DAY:\n        return \'ALGO_TIMESERIES_DAY\'\n    elif status == ALGO_TIMESERIES_MINUTE:\n        return \'ALGO_TIMESERIES_MINUTE\'\n    elif status == ALGO_TRADE_INDICATOR_COUNTS:\n        return \'ALGO_TRADE_INDICATOR_COUNTS\'\n    elif status == SA_DATASET_TYPE_ALGO_READY:\n        return \'ALGO_READY\'\n    elif status == SA_DATASET_TYPE_TRADING_HISTORY:\n        return \'TRADING_HISTORY\'\n    elif status == SA_DATASET_TYPE_TRADING_REPORT:\n        return \'TRADING_REPORT\'\n    elif status == MISSING_TOKEN:\n        return \'MISSING_TOKEN\'\n    else:\n        return f\'unsupported status={status}\'\n# end of get_status\n\n\ndef ppj(\n        json_data):\n    """"""ppj\n\n    :param json_data: dictionary to convert to\n                      a pretty-printed, multi-line string\n    """"""\n    return str(\n        json.dumps(\n            json_data,\n            sort_keys=True,\n            indent=4,\n            separators=(\',\', \': \')))\n# end of ppj\n\n\ndef to_float_str(\n        val):\n    """"""to_float_str\n\n    convert the float to a string with 2 decimal points of\n    precision\n\n    :param val: float to change to a 2-decimal string\n    """"""\n    return str(""%0.2f"" % float(val))\n# end of to_float_str\n\n\ndef to_f(\n        val):\n    """"""to_f\n\n    truncate the float to 2 decimal points of\n    precision\n\n    :param val: float to change\n    """"""\n    if val is None:\n        return None\n\n    return float(to_float_str(val))\n# end of to_f\n\n\ndef get_mb(\n        num):\n    """"""get_mb\n\n    convert a the number of bytes (as an ``integer``)\n    to megabytes with 2 decimal points of precision\n\n    :param num: integer - number of bytes\n    """"""\n    return to_f(num / NUM_BYTES_IN_AN_MB)\n# end get_mb\n\n\ndef get_percent_done(\n        progress,\n        total):\n    """"""get_percent_done\n\n    calculate percentage done to 2 decimal points of\n    precision\n\n    :param progress: progress counter\n    :param total: total number of counts\n    """"""\n    return to_f(float(float(progress)/float(total)*100.00))\n# end of get_percent_done\n\n\ndef is_celery_disabled(\n        work_dict=None):\n    """"""is_celery_disabled\n\n    :param work_dict: request to check\n    """"""\n    env_disabled = ev(\'CELERY_DISABLED\', \'0\') == \'1\'\n    request_disabled = False\n    if work_dict:\n        request_disabled = work_dict.get(\'celery_disabled\', False)\n    return (env_disabled or request_disabled)\n# end of is_celery_disabled\n\n\nINDICATOR_TYPE_MAPPING = {\n    \'technical\': INDICATOR_TYPE_TECHNICAL,\n    \'fundamental\': INDICATOR_TYPE_FUNDAMENTAL,\n    \'news\': INDICATOR_TYPE_NEWS,\n    \'sector\': INDICATOR_TYPE_SECTOR,\n    \'market\': INDICATOR_TYPE_MARKET,\n    \'dividend\': INDICATOR_TYPE_DIVIDEND,\n    \'custom\': INDICATOR_TYPE_CUSTOM,\n    \'unknown\': INDICATOR_TYPE_UNKNOWN\n}\n\n\nINDICATOR_CATEGORY_MAPPING = {\n    \'momentum\': INDICATOR_CATEGORY_MOMENTUM,\n    \'overlap\': INDICATOR_CATEGORY_OVERLAP,\n    \'price\': INDICATOR_CATEGORY_PRICE,\n    \'volume\': INDICATOR_CATEGORY_VOLUME,\n    \'volatility\': INDICATOR_CATEGORY_VOLATILITY,\n    \'single_call\': INDICATOR_CATEGORY_SINGLE_CALL,\n    \'single_put\': INDICATOR_CATEGORY_SINGLE_PUT,\n    \'bull_call\': INDICATOR_CATEGORY_BULL_CALL,\n    \'bear_put\': INDICATOR_CATEGORY_BEAR_PUT,\n    \'quarterly\': INDICATOR_CATEGORY_QUARTERLY,\n    \'yearly\': INDICATOR_CATEGORY_YEARLY,\n    \'income_statement\': INDICATOR_CATEGORY_INCOME_STMT,\n    \'cash_flow\': INDICATOR_CATEGORY_CASH_FLOW,\n    \'balance_sheet\': INDICATOR_CATEGORY_BALANCE_SHEET,\n    \'press_release\': INDICATOR_CATEGORY_PRESS_RELEASE,\n    \'custom\': INDICATOR_CATEGORY_CUSTOM,\n    \'news\': INDICATOR_CATEGORY_NEWS,\n    \'earnings\': INDICATOR_CATEGORY_EARNINGS,\n    \'csuite\': INDICATOR_CATEGORY_CSUITE,\n    \'splits\': INDICATOR_CATEGORY_SPLITS,\n    \'rev_splits\': INDICATOR_CATEGORY_REVERSE_SPLITS,\n    \'distributions\': INDICATOR_CATEGORY_DISTRIBUTIONS,\n    \'spinoffs\': INDICATOR_CATEGORY_SPINOFFS,\n    \'merger_acq\': INDICATOR_CATEGORY_MERGER_ACQ,\n    \'exchange_inclusion\': INDICATOR_CATEGORY_EXCHANGE_INCLUSION,\n    \'exchange_exclusion\': INDICATOR_CATEGORY_EXCHANGE_EXCLUSION,\n    \'trial_positive\': INDICATOR_CATEGORY_TRIAL_POSITIVE,\n    \'trial_negative\': INDICATOR_CATEGORY_TRIAL_NEGATIVE,\n    \'short_sellers\': INDICATOR_CATEGORY_SHORT_SELLERS,\n    \'real_estate\': INDICATOR_CATEGORY_REAL_ESTATE,\n    \'housing\': INDICATOR_CATEGORY_HOUSING,\n    \'pipeline\': INDICATOR_CATEGORY_PIPELINE,\n    \'construction\': INDICATOR_CATEGORY_CONSTRUCTION,\n    \'fed\': INDICATOR_CATEGORY_FED,\n    \'unknown\': INDICATOR_CATEGORY_UNKNOWN,\n}\n\nINDICATOR_USES_DATA_MAPPING = {\n    \'daily\': INDICATOR_USES_DAILY_DATA,\n    \'minute\': INDICATOR_USES_MINUTE_DATA,\n    \'quote\': INDICATOR_USES_QUOTE_DATA,\n    \'stats\': INDICATOR_USES_STATS_DATA,\n    \'peers\': INDICATOR_USES_PEERS_DATA,\n    \'news\': INDICATOR_USES_NEWS_DATA,\n    \'financials\': INDICATOR_USES_FINANCIAL_DATA,\n    \'earnings\': INDICATOR_USES_EARNINGS_DATA,\n    \'dividends\': INDICATOR_USES_DIVIDENDS_DATA,\n    \'company\': INDICATOR_USES_COMPANY_DATA,\n    \'pricing\': INDICATOR_USES_PRICING_DATA,\n    \'options\': INDICATOR_USES_OPTIONS_DATA,\n    \'calls\': INDICATOR_USES_CALLS_DATA,\n    \'puts\': INDICATOR_USES_PUTS_DATA,\n    \'unsupported\': INDICATOR_USES_DATA_UNSUPPORTED,\n    \'tdcalls\': INDICATOR_USES_TDCALLS_DATA,\n    \'tdputs\': INDICATOR_USES_TDPUTS_DATA,\n    \'any\': INDICATOR_USES_DATA_ANY\n}\n\n\nINDICATOR_ACTIONS = {\n    INDICATOR_RESET: INT_INDICATOR_NOT_PROCESSED,\n    INDICATOR_IGNORE: INT_INDICATOR_IGNORE_ACTION,\n    INDICATOR_BUY: INT_INDICATOR_BUY_ACTION,\n    INDICATOR_SELL: INT_INDICATOR_SELL_ACTION\n}\n\n\nALGO_TIMESERIES = {\n    \'daily\': ALGO_TIMESERIES_DAY,\n    \'minute\': ALGO_TIMESERIES_MINUTE,\n    \'intraday\': ALGO_TIMESERIES_MINUTE\n}\n\n\ndef get_indicator_type_as_int(\n        val=None):\n    """"""get_indicator_type_as_int\n\n    convert the string to the ``INDICATOR_TYPE_MAPPING``\n    integer value\n\n    :param val: integer to lookup in the ``INDICATOR_TYPE_MAPPING``\n        dictionary\n    """"""\n    if not val:\n        return INDICATOR_TYPE_UNKNOWN\n    else:\n        if val in INDICATOR_TYPE_MAPPING:\n            return INDICATOR_TYPE_MAPPING[val]\n        else:\n            return INDICATOR_TYPE_UNKNOWN\n    # if supported or not\n# end of get_indicator_type_as_int\n\n\ndef get_indicator_category_as_int(\n        val=None):\n    """"""get_indicator_category_as_int\n\n    convert the string to the ``INDICATOR_CATEGORY_MAPPING``\n    integer value\n\n    :param val: integer to lookup in the ``INDICATOR_CATEGORY_MAPPING``\n        dictionary\n    """"""\n    if not val:\n        return INDICATOR_CATEGORY_UNKNOWN\n    else:\n        if val in INDICATOR_CATEGORY_MAPPING:\n            return INDICATOR_CATEGORY_MAPPING[val]\n        else:\n            return INDICATOR_CATEGORY_UNKNOWN\n    # if supported or not\n# end of get_indicator_category_as_int\n\n\ndef get_indicator_uses_data_as_int(\n        val=None):\n    """"""get_indicator_uses_data_as_int\n\n    convert the string to the ``INDICATOR_USES_DATA_MAPPING``\n    integer value\n\n    :param val: integer to lookup in the ``INDICATOR_USES_DATA_MAPPING``\n        dictionary\n    """"""\n    if not val:\n        return INDICATOR_USES_DATA_ANY\n    else:\n        if val in INDICATOR_USES_DATA_MAPPING:\n            return INDICATOR_USES_DATA_MAPPING[val]\n        else:\n            return INDICATOR_USES_DATA_UNSUPPORTED\n    # if supported or not\n# end of get_indicator_uses_data_as_int\n\n\ndef get_algo_timeseries_from_int(\n        val):\n    """"""get_algo_timeseries_from_int\n\n    convert the integer value to the timeseries string\n    found in the ``analysis_engine.consts.ALGO_TIMESERIES``\n    dictionary\n\n    :param val: integer value for finding the\n        string timeseries label\n    """"""\n    for a in ALGO_TIMESERIES:\n        if ALGO_TIMESERIES[a] == val:\n            return a\n    return f\'unsupported algorithm timeseries value={val}\'\n# end of get_algo_timeseries_from_int\n\n\ndef is_df(\n        df):\n    """"""is_df\n\n    Test if ``df`` is a valid ``pandas.DataFrame``\n\n    :param df: ``pandas.DataFrame``\n    """"""\n    return (\n        hasattr(df, \'to_json\'))\n# end of is_df\n\n\ndef get_redis_host_and_port(\n        addr=None,\n        req=None):\n    """"""get_redis_host_and_port\n\n    parse the env ``REDIS_ADDRESS`` or ``addr`` string\n    or a dictionary ``req`` and\n    return a tuple for (host (str), port (int))\n\n    :param addr: optional - string redis address to parse\n        format is ``host:port``\n    :param req: optional - dictionary where the host and port\n        are under the keys ``redis_host`` and ``redis_port``\n    """"""\n    use_addr = REDIS_ADDRESS\n    redis_host = None\n    redis_port = None\n    if addr:\n        use_addr = addr\n    split_arr = use_addr.split(\':\')\n    redis_host = split_arr[0]\n    redis_port = int(split_arr[1])\n    if req:\n        redis_host = req.get(\n            \'redis_host\',\n            redis_host)\n        redis_port = int(req.get(\n            \'redis_port\',\n            redis_port))\n    return redis_host, redis_port\n# end of get_redis_host_and_port\n'"
analysis_engine/convert_df_to_json.py,0,"b'""""""\nConvert json to a pandas dataframe\n""""""\n\nimport pandas as pd\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef convert_json_to_df(\n        data_json,\n        sorted_by_key=""date"",\n        in_ascending=True):\n    """"""convert_json_to_df\n\n    Convert json to a pandas dataframe\n\n    :param data_json: json to convert\n    :param sorted_by_key: key to sort on\n    :param in_ascending: ascending order (True by default)\n    """"""\n    log.info(f\'start sort={sorted_by_key} asc={in_ascending}\')\n    new_df = pd.read_json(\n        data_json).sort_values(\n            by=sorted_by_key,\n            ascending=in_ascending)\n    log.info(\n        \'done\')\n    return new_df\n# end of convert_json_to_df\n'"
analysis_engine/dataset_scrub_utils.py,0,"b'""""""\nPerform dataset scrubbing actions\nand return the scrubbed dataset as a ready-to-go\ndata feed. This is an approach for normalizing\nan internal data feed.\n\nSupported environment variables:\n\n::\n\n    # verbose logging in this module\n    # note this can take longer to transform\n    # DataFrames and is not recommended for\n    # production:\n    export DEBUG_FETCH=1\n\nIngress Scrubbing supports converting an incoming\ndataset (from IEX) and converts it to one\nof the following data feed and returned as a\n``pandas DataFrame``:\n\n.. code-block:: python\n\n    DATAFEED_DAILY = 900\n    DATAFEED_MINUTE = 901\n    DATAFEED_QUOTE = 902\n    DATAFEED_STATS = 903\n    DATAFEED_PEERS = 904\n    DATAFEED_NEWS = 905\n    DATAFEED_FINANCIALS = 906\n    DATAFEED_EARNINGS = 907\n    DATAFEED_DIVIDENDS = 908\n    DATAFEED_COMPANY = 909\n    DATAFEED_PRICING_YAHOO = 1100\n    DATAFEED_OPTIONS_YAHOO = 1101\n    DATAFEED_NEWS_YAHOO = 1102\n\n""""""\n\nimport datetime\nimport pandas as pd\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.utils as ae_utils\nimport analysis_engine.iex.consts as iex_consts\nimport analysis_engine.yahoo.consts as yahoo_consts\nimport analysis_engine.td.consts as td_consts\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef debug_msg(\n        label,\n        datafeed_type,\n        msg_format,\n        date_str,\n        df):\n    """"""debug_msg\n\n    Debug helper for debugging scrubbing handlers\n\n    :param label: log label\n    :param datafeed_type: fetch type\n    :param msg_format: message to include\n    :param date_str: date string\n    :param df: ``pandas DataFrame`` or ``None``\n    """"""\n\n    msg = msg_format.format(\'_\', date_str)\n\n    dft_msg = \'\'\n    if (\n            datafeed_type == yahoo_consts.DATAFEED_PRICING_YAHOO or\n            datafeed_type == yahoo_consts.DATAFEED_OPTIONS_YAHOO or\n            datafeed_type == yahoo_consts.DATAFEED_NEWS_YAHOO):\n        dft_msg = yahoo_consts.get_datafeed_str_yahoo(\n            df_type=datafeed_type)\n    elif (\n            datafeed_type == td_consts.DATAFEED_TD_CALLS or\n            datafeed_type == td_consts.DATAFEED_TD_PUTS):\n        dft_msg = td_consts.get_datafeed_str_td(\n            df_type=datafeed_type)\n    else:\n        dft_msg = iex_consts.get_datafeed_str(\n            df_type=datafeed_type)\n\n    if ae_consts.ev(\'DEBUG_FETCH\', \'0\') == \'1\':\n        if \'START\' in msg:\n            log.info(\n                f\'{label} - {dft_msg} \'\n                f\'-------------------------\'\n                f\'------------------------------------\')\n        msg = msg_format.format(\n            df,\n            date_str),\n        if hasattr(df, \'empty\'):\n            log.info(\n                f\'{label} - {dft_msg} - {msg} found df={df} \'\n                f\'columns={df.columns.values}\')\n        else:\n            log.info(\n                f\'{label} - {dft_msg} - {msg} found df={df}\')\n\n        if \'END\' in msg:\n            log.info(\n                f\'{label} - {dft_msg} \'\n                f\'-------------------------\'\n                f\'------------------------------------\')\n    else:\n        log.info(\n            f\'{label} - {dft_msg} - {msg}\')\n    # end of debug pre-scrub logging\n\n# end of debug_msg\n\n\ndef build_dates_from_df_col(\n        df,\n        use_date_str,\n        src_col=\'minute\',\n        src_date_format=ae_consts.COMMON_TICK_DATE_FORMAT,\n        output_date_format=ae_consts.COMMON_TICK_DATE_FORMAT):\n    """"""build_dates_from_df_col\n\n    Converts a string date column series in a ``pandas.DataFrame``\n    to a well-formed date string list.\n\n    :param src_col: source column name\n    :param use_date_str: date string for today\n    :param src_date_format: format of the string in the\n                            ```df[src_col]`` columne\n    :param output_date_format: write the new date strings\n                               in this format.\n    :param df: source ``pandas.DataFrame``\n    """"""\n    new_dates = []\n    for idx, i in enumerate(df[src_col]):\n        org_new_str = \'\'\n        if \':\' not in i:\n            split_arr = i.split(\' \')\n            org_new_str = (\n                f\'{use_date_str} \'\n                f\'{split_arr[0]}\'\n                f\':00:00 \'\n                f\'{split_arr[1]}\')\n        else:\n            org_new_str = (\n                f\'{use_date_str} \'\n                f\'{i}:00\')\n        new_date_val = datetime.datetime.strptime(\n            org_new_str,\n            src_date_format)\n        new_str = new_date_val.strftime(\n            output_date_format)\n        new_dates.append(new_str)\n    # for all rows\n\n    return new_dates\n# end of build_dates_from_df_col\n\n\ndef ingress_scrub_dataset(\n        label,\n        datafeed_type,\n        df,\n        date_str=None,\n        msg_format=None,\n        scrub_mode=\'sort-by-date\',\n        ds_id=\'no-id\'):\n    """"""ingress_scrub_dataset\n\n    Scrub a ``pandas.DataFrame`` from an Ingress pricing service\n    and return the resulting ``pandas.DataFrame``\n\n    :param label: log label\n    :param datafeed_type: ``analysis_engine.iex.consts.DATAFEED_*`` type\n        or ``analysis_engine.yahoo.consts.DATAFEED_*```\n        type\n        .. code-block:: python\n\n            DATAFEED_DAILY = 900\n            DATAFEED_MINUTE = 901\n            DATAFEED_QUOTE = 902\n            DATAFEED_STATS = 903\n            DATAFEED_PEERS = 904\n            DATAFEED_NEWS = 905\n            DATAFEED_FINANCIALS = 906\n            DATAFEED_EARNINGS = 907\n            DATAFEED_DIVIDENDS = 908\n            DATAFEED_COMPANY = 909\n            DATAFEED_PRICING_YAHOO = 1100\n            DATAFEED_OPTIONS_YAHOO = 1101\n            DATAFEED_NEWS_YAHOO = 1102\n\n    :param df: ``pandas DataFrame``\n    :param date_str: date string for simulating historical dates\n                     or ``datetime.datetime.now()`` if not\n                     set\n    :param msg_format: msg format for a ``string.format()``\n    :param scrub_mode: mode to scrub this dataset\n    :param ds_id: dataset identifier\n    """"""\n\n    if not hasattr(df, \'empty\'):\n        log.debug(\n            f\'{label} - {datafeed_type} no dataset_id={ds_id}\')\n        return None\n\n    out_df = df\n\n    daily_date_format = \'%I:%M %p\'\n    minute_date_format = \'%I:%M %p\'\n\n    use_date_str = date_str\n    last_close_date = ae_utils.last_close()\n    today_str = last_close_date.strftime(\'%Y-%m-%d\')\n\n    year_str = today_str.split(\'-\')[0]\n    if not use_date_str:\n        use_date_str = today_str\n\n    daily_date_format = ae_consts.IEX_DAILY_DATE_FORMAT\n    minute_date_format = ae_consts.IEX_MINUTE_DATE_FORMAT\n\n    """"""\n    use_msg_format = msg_format\n    if not msg_format:\n        use_msg_format = \'df={} date_str={}\'\n\n    debug_msg(\n        label=label,\n        datafeed_type=datafeed_type,\n        msg_format=f\'START - {use_msg_format}\',\n        date_str=use_date_str,\n        df=df)\n    """"""\n\n    try:\n        if scrub_mode == \'sort-by-date\':\n            if datafeed_type == iex_consts.DATAFEED_DAILY:\n                new_dates = []\n                if \'label\' in df:\n                    for idx, i in enumerate(out_df[\'label\']):\n                        split_arr = i.split(\' \')\n                        new_str = \'\'\n                        if \',\' not in i:\n                            # Oct 3\n                            new_str = (\n                                f\'{year_str}-{split_arr[0]}-{split_arr[1]}\')\n                        else:\n                            # Aug 29, 18\n                            new_str = (\n                                f\'20{split_arr[2]}-{split_arr[0]}-\'\n                                f\'{split_arr[1].replace("","", """")}\')\n                        new_dates.append(new_str)\n                    # end for all rows\n                    out_df[\'date\'] = pd.to_datetime(\n                        new_dates,\n                        format=daily_date_format)\n                # end if label is in df\n            elif datafeed_type == iex_consts.DATAFEED_MINUTE:\n                new_dates = []\n                if \'label\' in df:\n                    new_dates = build_dates_from_df_col(\n                        src_col=\'label\',\n                        src_date_format=minute_date_format,\n                        use_date_str=use_date_str,\n                        df=out_df)\n                    out_df[\'date\'] = pd.to_datetime(\n                        new_dates,\n                        format=\'%Y-%m-%d %H:%M:%S\')\n                # end if label is in df\n            elif datafeed_type == iex_consts.DATAFEED_QUOTE:\n                columns_list = out_df.columns.values\n                if \'latestTime\' in columns_list:\n                    out_df[\'date\'] = pd.to_datetime(\n                        out_df[\'latestTime\'],\n                        format=ae_consts.IEX_QUOTE_DATE_FORMAT)\n                if \'latestUpdate\' in columns_list:\n                    out_df[\'latest_update\'] = pd.to_datetime(\n                        out_df[\'latestUpdate\'],\n                        unit=\'ns\')\n                if \'extendedPriceTime\' in columns_list:\n                    out_df[\'extended_price_time\'] = pd.to_datetime(\n                        out_df[\'extendedPriceTime\'],\n                        unit=\'ns\')\n                if \'iexLastUpdated\' in columns_list:\n                    out_df[\'iex_last_update\'] = pd.to_datetime(\n                        out_df[\'iexLastUpdated\'],\n                        unit=\'ns\')\n                if \'openTime\' in columns_list:\n                    out_df[\'open_time\'] = pd.to_datetime(\n                        out_df[\'openTime\'],\n                        unit=\'ns\')\n                if \'closeTime\' in columns_list:\n                    out_df[\'close_time\'] = pd.to_datetime(\n                        out_df[\'closeTime\'],\n                        unit=\'ns\')\n                # end if label is in df\n            elif datafeed_type == iex_consts.DATAFEED_STATS:\n                log.debug(\'{label} - {datafeed_type} - no scrub_mode\')\n            elif datafeed_type == iex_consts.DATAFEED_PEERS:\n                log.debug(\'{label} - {datafeed_type} - no scrub_mode\')\n            elif datafeed_type == iex_consts.DATAFEED_NEWS:\n                log.debug(\'{label} - {datafeed_type} - no scrub_mode\')\n            elif datafeed_type == iex_consts.DATAFEED_FINANCIALS:\n                log.debug(\'{label} - {datafeed_type} - no scrub_mode\')\n            elif datafeed_type == iex_consts.DATAFEED_EARNINGS:\n                log.debug(\'{label} - {datafeed_type} - no scrub_mode\')\n            elif datafeed_type == iex_consts.DATAFEED_DIVIDENDS:\n                log.debug(\'{label} - {datafeed_type} - no scrub_mode\')\n            elif datafeed_type == iex_consts.DATAFEED_COMPANY:\n                log.debug(\'{label} - {datafeed_type} - no scrub_mode\')\n            elif datafeed_type == yahoo_consts.DATAFEED_PRICING_YAHOO:\n                if \'date\' in df:\n                    out_df[\'date\'] = pd.to_datetime(\n                        df[\'date\'],\n                        format=daily_date_format)\n            elif datafeed_type == yahoo_consts.DATAFEED_OPTIONS_YAHOO:\n                if \'date\' in df:\n                    out_df[\'date\'] = pd.to_datetime(\n                        df[\'date\'],\n                        format=daily_date_format)\n            elif datafeed_type == yahoo_consts.DATAFEED_NEWS_YAHOO:\n                if \'date\' in df:\n                    out_df[\'date\'] = pd.to_datetime(\n                        df[\'date\'],\n                        format=daily_date_format)\n            elif datafeed_type == td_consts.DATAFEED_TD_CALLS:\n                log.debug(\'{label} - {datafeed_type} - no scrub_mode\')\n            elif datafeed_type == td_consts.DATAFEED_TD_PUTS:\n                log.debug(\'{label} - {datafeed_type} - no scrub_mode\')\n            else:\n                log.debug(\'{label} - {datafeed_type} - no scrub_mode\')\n            # if/else\n        else:\n            log.debug(\n                f\'{label} - {datafeed_type} - \'\n                f\'missing support in ingress_scrub_dataset\')\n    except Exception as e:\n        log.critical(\n            f\'{label} - {datafeed_type} sort={scrub_mode} - \'\n            f\'failed with ex={e} data={df}\')\n        out_df = None\n    # end of try/ex\n\n    """"""\n    debug_msg(\n        label=label,\n        datafeed_type=datafeed_type,\n        msg_format=\'END - df={} date_str={}\',\n        date_str=use_date_str,\n        df=out_df)\n    """"""\n\n    return out_df\n# end of ingress_scrub_dataset\n\n\ndef extract_scrub_dataset(\n        label,\n        datafeed_type,\n        df,\n        date_str=None,\n        msg_format=None,\n        scrub_mode=\'sort-by-date\',\n        ds_id=\'no-id\'):\n    """"""extract_scrub_dataset\n\n    Scrub a cached ``pandas.DataFrame`` that was stored\n    in Redis and return the resulting ``pandas.DataFrame``\n\n    :param label: log label\n    :param datafeed_type: ``analysis_engine.iex.consts.DATAFEED_*`` type\n        or ``analysis_engine.yahoo.consts.DATAFEED_*```\n        type\n        .. code-block:: python\n\n            DATAFEED_DAILY = 900\n            DATAFEED_MINUTE = 901\n            DATAFEED_QUOTE = 902\n            DATAFEED_STATS = 903\n            DATAFEED_PEERS = 904\n            DATAFEED_NEWS = 905\n            DATAFEED_FINANCIALS = 906\n            DATAFEED_EARNINGS = 907\n            DATAFEED_DIVIDENDS = 908\n            DATAFEED_COMPANY = 909\n            DATAFEED_PRICING_YAHOO = 1100\n            DATAFEED_OPTIONS_YAHOO = 1101\n            DATAFEED_NEWS_YAHOO = 1102\n    :param df: ``pandas DataFrame``\n    :param date_str: date string for simulating historical dates\n                     or ``datetime.datetime.now()`` if not\n                     set\n    :param msg_format: msg format for a ``string.format()``\n    :param scrub_mode: mode to scrub this dataset\n    :param ds_id: dataset identifier\n    """"""\n\n    if not hasattr(df, \'empty\'):\n        log.info(\n            f\'{label} - {datafeed_type} no dataset_id={ds_id}\')\n        return None\n\n    return df\n# end of extract_scrub_dataset\n'"
analysis_engine/dict_to_csv.py,0,"b'""""""\nUtility to output an n-level nested dictionary as a CSV\n""""""\n\nimport csv\nimport os\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef flatten_dict(\n        data,\n        parent_key=\'\',\n        sep=\'_\'):\n    """"""flatten_dict\n\n    Flatten an n-level nested dictionary for csv output\n\n    :param data: Dictionary to be parsed\n    :param parent_key: The nested parent key\n    :param sep: The separator to use between keys\n    """"""\n    items = []\n    for key, value in data.items():\n        new_key = parent_key + sep + key if parent_key else key\n        if isinstance(value, dict):\n            items.extend(flatten_dict(value, new_key, sep=sep).items())\n        elif isinstance(value, list):\n            for idx, val in enumerate(value):\n                temp_key = f\'{new_key}_{idx}\'\n                items.extend(flatten_dict(\n                    val,\n                    temp_key,\n                    sep=sep).items())\n        else:\n            items.append((new_key, value))\n    return dict(items)\n# end of flatten_dict\n\n\ndef dict_to_csv(\n        data,\n        filename=\'test\'):\n    """"""dict_to_csv\n\n    Convert a dictionary to an output CSV\n\n    :param data: Dictionary to be converted\n    :param filename: The name of the CSV to produce\n    """"""\n    noext_filename = os.path.splitext(filename)[0]\n    log.info((f\'START dict={data} conversion to csv={noext_filename}.csv\'))\n    flattened_data = flatten_dict(data)\n    with open(f\'{noext_filename}.csv\', \'w\') as f:\n        w = csv.DictWriter(f, flattened_data.keys())\n        w.writeheader()\n        w.writerow(flattened_data)\n# end of dict_to_csv\n'"
analysis_engine/extract.py,0,"b'""""""\n**Extraction API Examples**\n\n**Extract All Data for a Ticker**\n\n.. code-block:: python\n\n    import analysis_engine.extract as ae_extract\n    print(ae_extract.extract(\'SPY\'))\n\n**Extract Latest Minute Pricing for Stocks and Options**\n\n.. code-block:: python\n\n    import analysis_engine.extract as ae_extract\n    print(ae_extract.extract(\n        \'SPY\',\n        datasets=[\'minute\', \'tdcalls\', \'tdputs\']))\n\n**Extract Historical Data**\n\nExtract historical data with the ``date`` argument formatted ``YYYY-MM-DD``:\n\n.. code-block:: python\n\n    import analysis_engine.extract as ae_extract\n    print(ae_extract.extract(\n        \'AAPL\',\n        datasets=[\'minute\', \'daily\', \'financials\', \'earnings\', \'dividends\'],\n        date=\'2019-02-15\'))\n\n**Additional Extraction APIs**\n\n`IEX Cloud Extraction API Reference <https://stock-analysis-engine.\nreadthedocs.io/en/latest/iex_api.html#iex-extraction-api-reference>`__\n\n`Tradier Extraction API Reference <https://stock-analysis-engine.\nreadthedocs.io/en/latest/tradier.html#tradier-extraction-api-reference>`__\n\n""""""\n\nimport os\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.utils as ae_utils\nimport analysis_engine.build_dataset_node as build_dataset_node\nimport analysis_engine.api_requests as api_requests\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef extract(\n        ticker=None,\n        datasets=None,\n        tickers=None,\n        use_key=None,\n        extract_mode=\'all\',\n        iex_datasets=None,\n        date=None,\n        redis_enabled=True,\n        redis_address=None,\n        redis_db=None,\n        redis_password=None,\n        redis_expire=None,\n        s3_enabled=True,\n        s3_address=None,\n        s3_bucket=None,\n        s3_access_key=None,\n        s3_secret_key=None,\n        s3_region_name=None,\n        s3_secure=False,\n        celery_disabled=True,\n        broker_url=None,\n        result_backend=None,\n        label=None,\n        verbose=False):\n    """"""extract\n\n    Extract all cached datasets for a stock ``ticker`` or\n    a list of ``tickers`` and returns a dictionary. Please\n    make sure the datasets are already cached in Redis\n    before running this method. If not please refer to\n    the ``analysis_engine.fetch.fetch`` function\n    to prepare the datasets on your environment.\n\n    Python example:\n\n    .. code-block:: python\n\n        from analysis_engine.extract import extract\n        d = extract(ticker=\'NFLX\')\n        print(d)\n        for k in d[\'NFLX\']:\n            print(f\'dataset key: {k}\')\n\n    **Extract Intraday Stock and Options Minute Pricing Data**\n\n    This works by using the ``date`` and ``datasets`` arguments\n    as filters:\n\n    .. code-block:: python\n\n        import analysis_engine.extract as ae_extract\n        print(ae_extract.extract(\n            ticker=\'SPY\',\n            datasets=[\'minute\', \'tdcalls\', \'tdputs\'])\n\n    This was created for reducing the amount of typying in\n    Jupyter notebooks. It can be set up for use with a\n    distributed engine as well with the optional arguments\n    depending on your connectitivty requirements.\n\n    .. note:: Please ensure Redis and Minio are running\n              before trying to extract tickers\n\n    **Stock tickers to extract**\n\n    :param ticker: single stock ticker/symbol/ETF to extract\n    :param tickers: optional - list of tickers to extract\n    :param use_key: optional - extract historical key from Redis\n        usually formatted ``<TICKER>_<date formatted YYYY-MM-DD>``\n\n    **(Optional) Data sources, datafeeds and datasets to gather**\n\n    :param iex_datasets: list of strings for gathering specific `IEX\n        datasets <https://iexcloud.io/>`__\n        which are set as consts: ``analysis_engine.iex.consts.FETCH_*``.\n    :param date: optional - string date formatted\n        ``YYYY-MM-DD`` - if not set use last close date\n    :param datasets: list of strings for indicator\n        dataset extraction - preferred method\n        (defaults to ``BACKUP_DATASETS``)\n\n    **(Optional) Redis connectivity arguments**\n\n    :param redis_enabled: bool - toggle for auto-caching all\n        datasets in Redis\n        (default is ``True``)\n    :param redis_address: Redis connection string format: ``host:port``\n        (default is ``localhost:6379``)\n    :param redis_db: Redis db to use\n        (default is ``0``)\n    :param redis_password: optional - Redis password\n        (default is ``None``)\n    :param redis_expire: optional - Redis expire value\n        (default is ``None``)\n\n    **(Optional) Minio (S3) connectivity arguments**\n\n    :param s3_enabled: bool - toggle for auto-archiving on Minio (S3)\n        (default is ``True``)\n    :param s3_address: Minio S3 connection string format: ``host:port``\n        (default is ``localhost:9000``)\n    :param s3_bucket: S3 Bucket for storing the artifacts\n        (default is ``dev``) which should be viewable on a browser:\n        http://localhost:9000/minio/dev/\n    :param s3_access_key: S3 Access key\n        (default is ``trexaccesskey``)\n    :param s3_secret_key: S3 Secret key\n        (default is ``trex123321``)\n    :param s3_region_name: S3 region name\n        (default is ``us-east-1``)\n    :param s3_secure: Transmit using tls encryption\n        (default is ``False``)\n\n    **(Optional) Celery worker broker connectivity arguments**\n\n    :param celery_disabled: bool - toggle synchronous mode or publish\n        to an engine connected to the `Celery broker and backend\n        <https://github.com/celery/celery#transports-and-backends>`__\n        (default is ``True`` - synchronous mode without an engine\n        or need for a broker or backend for Celery)\n    :param broker_url: Celery broker url\n        (default is ``redis://0.0.0.0:6379/13``)\n    :param result_backend: Celery backend url\n        (default is ``redis://0.0.0.0:6379/14``)\n    :param label: tracking log label\n\n    **(Optional) Debugging**\n\n    :param verbose: bool - show extract warnings\n        and other debug logging (default is False)\n\n    **Supported environment variables**\n\n    ::\n\n        export REDIS_ADDRESS=""localhost:6379""\n        export REDIS_DB=""0""\n        export S3_ADDRESS=""localhost:9000""\n        export S3_BUCKET=""dev""\n        export AWS_ACCESS_KEY_ID=""trexaccesskey""\n        export AWS_SECRET_ACCESS_KEY=""trex123321""\n        export AWS_DEFAULT_REGION=""us-east-1""\n        export S3_SECURE=""0""\n        export WORKER_BROKER_URL=""redis://0.0.0.0:6379/13""\n        export WORKER_BACKEND_URL=""redis://0.0.0.0:6379/14""\n    """"""\n\n    rec = {}\n    extract_requests = []\n\n    use_tickers = tickers\n    if ticker:\n        use_tickers = [ticker]\n    else:\n        if not use_tickers:\n            use_tickers = []\n\n    default_iex_datasets = [\n        \'daily\',\n        \'minute\',\n        \'quote\',\n        \'stats\',\n        \'peers\',\n        \'news\',\n        \'financials\',\n        \'earnings\',\n        \'dividends\',\n        \'company\'\n    ]\n\n    if not iex_datasets:\n        iex_datasets = default_iex_datasets\n\n    use_indicator_datasets = datasets\n    if not use_indicator_datasets:\n        use_indicator_datasets = ae_consts.BACKUP_DATASETS\n\n    if redis_enabled:\n        if not redis_address:\n            redis_address = os.getenv(\n                \'REDIS_ADDRESS\',\n                \'localhost:6379\')\n        if not redis_password:\n            redis_password = os.getenv(\n                \'REDIS_PASSWORD\',\n                None)\n        if not redis_db:\n            redis_db = int(os.getenv(\n                \'REDIS_DB\',\n                \'0\'))\n        if not redis_expire:\n            redis_expire = os.getenv(\n                \'REDIS_EXPIRE\',\n                None)\n    if s3_enabled:\n        if not s3_address:\n            s3_address = os.getenv(\n                \'S3_ADDRESS\',\n                \'localhost:9000\')\n        if not s3_access_key:\n            s3_access_key = os.getenv(\n                \'AWS_ACCESS_KEY_ID\',\n                \'trexaccesskey\')\n        if not s3_secret_key:\n            s3_secret_key = os.getenv(\n                \'AWS_SECRET_ACCESS_KEY\',\n                \'trex123321\')\n        if not s3_region_name:\n            s3_region_name = os.getenv(\n                \'AWS_DEFAULT_REGION\',\n                \'us-east-1\')\n        if not s3_secure:\n            s3_secure = os.getenv(\n                \'S3_SECURE\',\n                \'0\') == \'1\'\n        if not s3_bucket:\n            s3_bucket = os.getenv(\n                \'S3_BUCKET\',\n                \'dev\')\n    if not broker_url:\n        broker_url = os.getenv(\n            \'WORKER_BROKER_URL\',\n            \'redis://0.0.0.0:6379/13\')\n    if not result_backend:\n        result_backend = os.getenv(\n            \'WORKER_BACKEND_URL\',\n            \'redis://0.0.0.0:6379/14\')\n\n    if not label:\n        label = \'get-latest\'\n\n    last_close_str = ae_utils.get_last_close_str()\n    use_date_str = last_close_str\n    if date:\n        use_date_str = date\n\n    ticker_key = use_key\n    if use_key:\n        ticker_key = f\'{ticker}_{last_close_str}\'\n    else:\n        ticker_key = f\'{ticker}_{use_date_str}\'\n\n    common_vals = {}\n    common_vals[\'base_key\'] = ticker_key\n    common_vals[\'celery_disabled\'] = celery_disabled\n    common_vals[\'ticker\'] = ticker\n    common_vals[\'label\'] = label\n    common_vals[\'iex_datasets\'] = iex_datasets\n    common_vals[\'s3_enabled\'] = s3_enabled\n    common_vals[\'s3_bucket\'] = s3_bucket\n    common_vals[\'s3_address\'] = s3_address\n    common_vals[\'s3_secure\'] = s3_secure\n    common_vals[\'s3_region_name\'] = s3_region_name\n    common_vals[\'s3_access_key\'] = s3_access_key\n    common_vals[\'s3_secret_key\'] = s3_secret_key\n    common_vals[\'s3_key\'] = ticker_key\n    common_vals[\'redis_enabled\'] = redis_enabled\n    common_vals[\'redis_address\'] = redis_address\n    common_vals[\'redis_password\'] = redis_password\n    common_vals[\'redis_db\'] = redis_db\n    common_vals[\'redis_key\'] = ticker_key\n    common_vals[\'redis_expire\'] = redis_expire\n\n    common_vals[\'redis_address\'] = redis_address\n    common_vals[\'s3_address\'] = s3_address\n\n    if verbose:\n        log.info(\n            f\'{label} - extract ticker={ticker} last_close={last_close_str} \'\n            f\'base_key={common_vals[""base_key""]} \'\n            f\'redis_address={common_vals[""redis_address""]} \'\n            f\'s3_address={common_vals[""s3_address""]}\')\n\n    """"""\n    Extract Supported Datasets\n    """"""\n\n    for ticker in use_tickers:\n        req = api_requests.get_ds_dict(\n            ticker=ticker,\n            base_key=common_vals[\'base_key\'],\n            ds_id=label)\n        extract_requests.append(req)\n    # end of for all ticker in use_tickers\n\n    for extract_req in extract_requests:\n        ticker_data = build_dataset_node.build_dataset_node(\n            ticker=ticker,\n            date=use_date_str,\n            datasets=use_indicator_datasets,\n            verbose=verbose)\n\n        rec[ticker] = ticker_data\n    # end of for service_dict in extract_requests\n\n    return rec\n# end of extract\n'"
analysis_engine/extract_utils.py,0,"b'""""""\n\n**Dataset Extraction Utilities**\n\nHelper for extracting a dataset from Redis or S3 and\nload it into a ``pandas.DataFrame``. This was designed\nto ignore the source of the dataset (IEX vs Yahoo) and\nperform the extract and load operations without\nknowledge of the underlying dataset.\n\nSupported environment variables:\n\n::\n\n    # verbose logging in this module\n    export DEBUG_EXTRACT=1\n\n    # verbose logging for just Redis operations in this module\n    export DEBUG_REDIS_EXTRACT=1\n\n    # verbose logging for just S3 operations in this module\n    export DEBUG_S3_EXTRACT=1\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n\n""""""\n\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.build_df_from_redis as build_df\nimport analysis_engine.dataset_scrub_utils as scrub_utils\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef perform_extract(\n        df_type,\n        df_str,\n        work_dict,\n        dataset_id_key=\'ticker\',\n        scrub_mode=\'sort-by-date\',\n        verbose=False):\n    """"""perform_extract\n\n    Helper for extracting from Redis or S3\n\n    :param df_type: datafeed type enum\n    :param ds_str: dataset string name\n    :param work_dict: incoming work request dictionary\n    :param dataset_id_key: configurable dataset identifier\n                           key for tracking scrubbing and\n                           debugging errors\n    :param scrub_mode: scrubbing mode on extraction for\n                       one-off cleanup before analysis\n    :param verbose: optional - boolean for turning on logging\n    """"""\n    status = ae_consts.FAILED\n    ds_id = work_dict.get(\n        dataset_id_key,\n        None)\n    label = work_dict.get(\n        \'label\',\n        \'extract\')\n    s3_bucket = work_dict.get(\n        \'s3_bucket\',\n        ae_consts.S3_BUCKET)\n    s3_key = work_dict.get(\n        \'s3_key\',\n        ae_consts.S3_KEY)\n    redis_key = work_dict.get(\n        \'redis_key\',\n        ae_consts.REDIS_KEY)\n    s3_enabled = work_dict.get(\n        \'s3_enabled\',\n        ae_consts.ENABLED_S3_UPLOAD)\n    s3_access_key = work_dict.get(\n        \'s3_access_key\',\n        ae_consts.S3_ACCESS_KEY)\n    s3_secret_key = work_dict.get(\n        \'s3_secret_key\',\n        ae_consts.S3_SECRET_KEY)\n    s3_region_name = work_dict.get(\n        \'s3_region_name\',\n        ae_consts.S3_REGION_NAME)\n    s3_address = work_dict.get(\n        \'s3_address\',\n        ae_consts.S3_ADDRESS)\n    s3_secure = work_dict.get(\n        \'s3_secure\',\n        ae_consts.S3_SECURE)\n    redis_address = work_dict.get(\n        \'redis_address\',\n        ae_consts.REDIS_ADDRESS)\n    redis_password = work_dict.get(\n        \'redis_password\',\n        ae_consts.REDIS_PASSWORD)\n    redis_db = work_dict.get(\n        \'redis_db\',\n        ae_consts.REDIS_DB)\n    redis_expire = work_dict.get(\n        \'redis_expire\',\n        ae_consts.REDIS_EXPIRE)\n\n    if verbose:\n        log.info(\n            f\'{label} - {df_str} - START - \'\n            f\'ds_id={ds_id} scrub_mode={scrub_mode} \'\n            f\'redis_address={redis_address}@{redis_db} redis_key={redis_key} \'\n            f\'s3={s3_enabled} s3_address={s3_address} s3_bucket={s3_bucket} \'\n            f\'s3_key={s3_key}\')\n\n    if verbose or ae_consts.ev(\'DEBUG_REDIS_EXTRACT\', \'0\') == \'1\':\n        log.info(\n            f\'{label} - {df_str} - ds_id={ds_id} redis \'\n            f\'pw={redis_password} expire={redis_expire}\')\n\n    if verbose or ae_consts.ev(\'DEBUG_S3_EXTRACT\', \'0\') == \'1\':\n        log.info(\n            f\'{label} - {df_str} - ds_id={ds_id} s3 \'\n            f\'ak={s3_access_key} sk={s3_secret_key} \'\n            f\'region={s3_region_name} secure={s3_secure}\')\n\n    extract_res = None\n    try:\n        extract_res = build_df.build_df_from_redis(\n            label=label,\n            address=redis_address,\n            db=redis_db,\n            key=redis_key,\n            verbose=verbose)\n    except Exception as e:\n        extract_res = None\n        log.error(\n            f\'{label} - {df_str} - ds_id={ds_id} failed extract from \'\n            f\'redis={redis_address}@{redis_db} key={redis_key} ex={e}\')\n    # end of try/ex extract from redis\n\n    if not extract_res:\n        return status, None\n\n    valid_df = (\n        extract_res[\'status\'] == ae_consts.SUCCESS\n        and extract_res[\'rec\'][\'valid_df\'])\n\n    if not valid_df:\n        if verbose or ae_consts.ev(\'DEBUG_S3_EXTRACT\', \'0\') == \'1\':\n            log.error(\n                f\'{label} - {df_str} ds_id={ds_id} invalid df \'\n                f\'status={ae_consts.get_status(status=extract_res[""status""])} \'\n                f\'extract_res={extract_res}\')\n        return status, None\n\n    extract_df = extract_res[\'rec\'][\'data\']\n\n    if verbose:\n        log.info(\n            f\'{label} - {df_str} ds_id={ds_id} extract scrub={scrub_mode}\')\n\n    scrubbed_df = scrub_utils.extract_scrub_dataset(\n        label=label,\n        scrub_mode=scrub_mode,\n        datafeed_type=df_type,\n        msg_format=\'df={} date_str={}\',\n        ds_id=ds_id,\n        df=extract_df)\n\n    status = ae_consts.SUCCESS\n\n    return status, scrubbed_df\n# end of perform_extract\n'"
analysis_engine/fetch.py,0,"b'""""""\nDataset Fetch API\n""""""\n\nimport os\nimport json\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.utils as ae_utils\nimport analysis_engine.iex.consts as iex_consts\nimport analysis_engine.work_tasks.get_new_pricing_data as price_utils\nimport analysis_engine.iex.extract_df_from_redis as iex_extract_utils\nimport analysis_engine.yahoo.extract_df_from_redis as yahoo_extract_utils\nimport analysis_engine.td.extract_df_from_redis as td_extract_utils\nimport analysis_engine.api_requests as api_requests\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef fetch(\n        ticker=None,\n        tickers=None,\n        fetch_mode=None,\n        iex_datasets=None,\n        redis_enabled=True,\n        redis_address=None,\n        redis_db=None,\n        redis_password=None,\n        redis_expire=None,\n        s3_enabled=True,\n        s3_address=None,\n        s3_bucket=None,\n        s3_access_key=None,\n        s3_secret_key=None,\n        s3_region_name=None,\n        s3_secure=False,\n        celery_disabled=True,\n        broker_url=None,\n        result_backend=None,\n        label=None,\n        verbose=False):\n    """"""fetch\n\n    Fetch all supported datasets for a stock ``ticker`` or\n    a list of ``tickers`` and returns a dictionary. Once\n    run, the datasets will all be cached in Redis and archived\n    in Minio (S3) by default.\n\n    Python example:\n\n    .. code-block:: python\n\n        from analysis_engine.fetch import fetch\n        d = fetch(ticker=\'NFLX\')\n        print(d)\n        for k in d[\'NFLX\']:\n            print(f\'dataset key: {k}\')\n\n    By default, it synchronously automates:\n\n        - fetching all datasets\n        - caching all datasets in Redis\n        - archiving all datasets in Minio (S3)\n        - returns all datasets in a single dictionary\n\n    This was created for reducing the amount of typying in\n    Jupyter notebooks. It can be set up for use with a\n    distributed engine as well with the optional arguments\n    depending on your connectitivty requirements.\n\n    .. note:: Please ensure Redis and Minio are running\n              before trying to extract tickers\n\n    **Stock tickers to fetch**\n\n    :param ticker: single stock ticker/symbol/ETF to fetch\n    :param tickers: optional - list of tickers to fetch\n\n    **(Optional) Data sources, datafeeds and datasets to gather**\n\n    :param fetch_mode: data sources - default is ``all`` (both IEX\n        and Yahoo), ``iex`` for only IEX, ``yahoo`` for only Yahoo.\n    :param iex_datasets: list of strings for gathering specific `IEX\n        datasets <https://iexcloud.io/>`__\n        which are set as consts: ``analysis_engine.iex.consts.FETCH_*``.\n\n    **(Optional) Redis connectivity arguments**\n\n    :param redis_enabled: bool - toggle for auto-caching all\n        datasets in Redis\n        (default is ``True``)\n    :param redis_address: Redis connection string format: ``host:port``\n        (default is ``localhost:6379``)\n    :param redis_db: Redis db to use\n        (default is ``0``)\n    :param redis_password: optional - Redis password\n        (default is ``None``)\n    :param redis_expire: optional - Redis expire value\n        (default is ``None``)\n\n    **(Optional) Minio (S3) connectivity arguments**\n\n    :param s3_enabled: bool - toggle for auto-archiving on Minio (S3)\n        (default is ``True``)\n    :param s3_address: Minio S3 connection string format: ``host:port``\n        (default is ``localhost:9000``)\n    :param s3_bucket: S3 Bucket for storing the artifacts\n        (default is ``dev``) which should be viewable on a browser:\n        http://localhost:9000/minio/dev/\n    :param s3_access_key: S3 Access key\n        (default is ``trexaccesskey``)\n    :param s3_secret_key: S3 Secret key\n        (default is ``trex123321``)\n    :param s3_region_name: S3 region name\n        (default is ``us-east-1``)\n    :param s3_secure: Transmit using tls encryption\n        (default is ``False``)\n\n    **(Optional) Celery worker broker connectivity arguments**\n\n    :param celery_disabled: bool - toggle synchronous mode or publish\n        to an engine connected to the `Celery broker and backend\n        <https://github.com/celery/celery#transports-and-backends>`__\n        (default is ``True`` - synchronous mode without an engine\n        or need for a broker or backend for Celery)\n    :param broker_url: Celery broker url\n        (default is ``redis://0.0.0.0:6379/13``)\n    :param result_backend: Celery backend url\n        (default is ``redis://0.0.0.0:6379/14``)\n    :param label: tracking log label\n\n    **(Optional) Debugging**\n\n    :param verbose: bool - show fetch warnings\n        and other debug logging (default is False)\n\n    **Supported environment variables**\n\n    ::\n\n        export REDIS_ADDRESS=""localhost:6379""\n        export REDIS_DB=""0""\n        export S3_ADDRESS=""localhost:9000""\n        export S3_BUCKET=""dev""\n        export AWS_ACCESS_KEY_ID=""trexaccesskey""\n        export AWS_SECRET_ACCESS_KEY=""trex123321""\n        export AWS_DEFAULT_REGION=""us-east-1""\n        export S3_SECURE=""0""\n        export WORKER_BROKER_URL=""redis://0.0.0.0:6379/13""\n        export WORKER_BACKEND_URL=""redis://0.0.0.0:6379/14""\n    """"""\n\n    rec = {}\n\n    extract_records = []\n\n    use_tickers = tickers\n    if ticker:\n        use_tickers = [ticker]\n    else:\n        if not use_tickers:\n            use_tickers = []\n\n    default_iex_datasets = [\n        \'daily\',\n        \'minute\',\n        \'quote\',\n        \'stats\',\n        \'peers\',\n        \'news\',\n        \'financials\',\n        \'earnings\',\n        \'dividends\',\n        \'company\'\n    ]\n\n    use_iex_datasets = iex_consts.FETCH_DATASETS\n    if len(use_iex_datasets) == 0:\n        use_iex_datasets = default_iex_datasets\n    if not iex_datasets:\n        iex_datasets = use_iex_datasets\n    if not fetch_mode:\n        fetch_mode = \'all\'\n\n    if redis_enabled:\n        if not redis_address:\n            redis_address = os.getenv(\n                \'REDIS_ADDRESS\',\n                \'localhost:6379\')\n        if not redis_password:\n            redis_password = os.getenv(\n                \'REDIS_PASSWORD\',\n                None)\n        if not redis_db:\n            redis_db = int(os.getenv(\n                \'REDIS_DB\',\n                \'0\'))\n        if not redis_expire:\n            redis_expire = os.getenv(\n                \'REDIS_EXPIRE\',\n                None)\n    if s3_enabled:\n        if not s3_address:\n            s3_address = os.getenv(\n                \'S3_ADDRESS\',\n                \'localhost:9000\')\n        if not s3_access_key:\n            s3_access_key = os.getenv(\n                \'AWS_ACCESS_KEY_ID\',\n                \'trexaccesskey\')\n        if not s3_secret_key:\n            s3_secret_key = os.getenv(\n                \'AWS_SECRET_ACCESS_KEY\',\n                \'trex123321\')\n        if not s3_region_name:\n            s3_region_name = os.getenv(\n                \'AWS_DEFAULT_REGION\',\n                \'us-east-1\')\n        if not s3_secure:\n            s3_secure = os.getenv(\n                \'S3_SECURE\',\n                \'0\') == \'1\'\n        if not s3_bucket:\n            s3_bucket = os.getenv(\n                \'S3_BUCKET\',\n                \'dev\')\n    if not broker_url:\n        broker_url = os.getenv(\n            \'WORKER_BROKER_URL\',\n            \'redis://0.0.0.0:6379/13\')\n    if not result_backend:\n        result_backend = os.getenv(\n            \'WORKER_BACKEND_URL\',\n            \'redis://0.0.0.0:6379/14\')\n\n    if not label:\n        label = \'get-latest\'\n\n    num_tickers = len(use_tickers)\n    last_close_str = ae_utils.get_last_close_str()\n\n    if iex_datasets:\n        log.info(\n            f\'{label} - getting latest for tickers={num_tickers} \'\n            f\'iex={json.dumps(iex_datasets)}\')\n    else:\n        log.info(f\'{label} - getting latest for tickers={num_tickers}\')\n\n    for ticker in use_tickers:\n\n        ticker_key = f\'{ticker}_{last_close_str}\'\n\n        fetch_req = api_requests.build_get_new_pricing_request()\n        fetch_req[\'base_key\'] = ticker_key\n        fetch_req[\'celery_disabled\'] = celery_disabled\n        fetch_req[\'ticker\'] = ticker\n        fetch_req[\'label\'] = label\n        fetch_req[\'fetch_mode\'] = fetch_mode\n        fetch_req[\'iex_datasets\'] = iex_datasets\n        fetch_req[\'s3_enabled\'] = s3_enabled\n        fetch_req[\'s3_bucket\'] = s3_bucket\n        fetch_req[\'s3_address\'] = s3_address\n        fetch_req[\'s3_secure\'] = s3_secure\n        fetch_req[\'s3_region_name\'] = s3_region_name\n        fetch_req[\'s3_access_key\'] = s3_access_key\n        fetch_req[\'s3_secret_key\'] = s3_secret_key\n        fetch_req[\'s3_key\'] = ticker_key\n        fetch_req[\'redis_enabled\'] = redis_enabled\n        fetch_req[\'redis_address\'] = redis_address\n        fetch_req[\'redis_password\'] = redis_password\n        fetch_req[\'redis_db\'] = redis_db\n        fetch_req[\'redis_key\'] = ticker_key\n        fetch_req[\'redis_expire\'] = redis_expire\n\n        fetch_req[\'redis_address\'] = redis_address\n        fetch_req[\'s3_address\'] = s3_address\n\n        log.info(\n            f\'{label} - fetching ticker={ticker} last_close={last_close_str} \'\n            f\'redis_address={fetch_req[""redis_address""]} \'\n            f\'s3_address={fetch_req[""s3_address""]}\')\n\n        fetch_res = price_utils.run_get_new_pricing_data(\n            work_dict=fetch_req)\n        if fetch_res[\'status\'] == ae_consts.SUCCESS:\n            log.info(\n                f\'{label} - fetched ticker={ticker} \'\n                \'preparing for extraction\')\n            extract_req = fetch_req\n            extract_records.append(extract_req)\n        else:\n            log.warning(\n                f\'{label} - failed getting ticker={ticker} data \'\n                f\'status={ae_consts.get_status(status=fetch_res[""status""])} \'\n                f\'err={fetch_res[""err""]}\')\n        # end of if worked or not\n    # end for all tickers to fetch\n\n    """"""\n    Extract Datasets\n    """"""\n\n    iex_daily_status = ae_consts.FAILED\n    iex_minute_status = ae_consts.FAILED\n    iex_quote_status = ae_consts.FAILED\n    iex_stats_status = ae_consts.FAILED\n    iex_peers_status = ae_consts.FAILED\n    iex_news_status = ae_consts.FAILED\n    iex_financials_status = ae_consts.FAILED\n    iex_earnings_status = ae_consts.FAILED\n    iex_dividends_status = ae_consts.FAILED\n    iex_company_status = ae_consts.FAILED\n    yahoo_news_status = ae_consts.FAILED\n    yahoo_options_status = ae_consts.FAILED\n    yahoo_pricing_status = ae_consts.FAILED\n    td_calls_status = ae_consts.FAILED\n    td_puts_status = ae_consts.FAILED\n\n    iex_daily_df = None\n    iex_minute_df = None\n    iex_quote_df = None\n    iex_stats_df = None\n    iex_peers_df = None\n    iex_news_df = None\n    iex_financials_df = None\n    iex_earnings_df = None\n    iex_dividends_df = None\n    iex_company_df = None\n    yahoo_option_calls_df = None\n    yahoo_option_puts_df = None\n    yahoo_pricing_df = None\n    yahoo_news_df = None\n    td_calls_df = None\n    td_puts_df = None\n\n    extract_iex = True\n    if fetch_mode not in [\'all\', \'iex\']:\n        extract_iex = False\n\n    extract_yahoo = True\n    if fetch_mode not in [\'all\', \'yahoo\']:\n        extract_yahoo = False\n\n    extract_td = True\n    if fetch_mode not in [\'all\', \'td\']:\n        extract_td = False\n\n    for service_dict in extract_records:\n        ticker_data = {}\n        ticker = service_dict[\'ticker\']\n\n        extract_req = api_requests.get_ds_dict(\n            ticker=ticker,\n            base_key=service_dict.get(\'base_key\', None),\n            ds_id=label,\n            service_dict=service_dict)\n\n        if \'daily\' in iex_datasets or extract_iex:\n            iex_daily_status, iex_daily_df = \\\n                iex_extract_utils.extract_daily_dataset(\n                    extract_req)\n            if iex_daily_status != ae_consts.SUCCESS:\n                if verbose:\n                    log.warning(f\'unable to fetch iex_daily={ticker}\')\n        if \'minute\' in iex_datasets or extract_iex:\n            iex_minute_status, iex_minute_df = \\\n                iex_extract_utils.extract_minute_dataset(\n                    extract_req)\n            if iex_minute_status != ae_consts.SUCCESS:\n                if verbose:\n                    log.warning(f\'unable to fetch iex_minute={ticker}\')\n        if \'quote\' in iex_datasets or extract_iex:\n            iex_quote_status, iex_quote_df = \\\n                iex_extract_utils.extract_quote_dataset(\n                    extract_req)\n            if iex_quote_status != ae_consts.SUCCESS:\n                if verbose:\n                    log.warning(f\'unable to fetch iex_quote={ticker}\')\n        if \'stats\' in iex_datasets or extract_iex:\n            iex_stats_df, iex_stats_df = \\\n                iex_extract_utils.extract_stats_dataset(\n                    extract_req)\n            if iex_stats_status != ae_consts.SUCCESS:\n                if verbose:\n                    log.warning(f\'unable to fetch iex_stats={ticker}\')\n        if \'peers\' in iex_datasets or extract_iex:\n            iex_peers_df, iex_peers_df = \\\n                iex_extract_utils.extract_peers_dataset(\n                    extract_req)\n            if iex_peers_status != ae_consts.SUCCESS:\n                if verbose:\n                    log.warning(f\'unable to fetch iex_peers={ticker}\')\n        if \'news\' in iex_datasets or extract_iex:\n            iex_news_status, iex_news_df = \\\n                iex_extract_utils.extract_news_dataset(\n                    extract_req)\n            if iex_news_status != ae_consts.SUCCESS:\n                if verbose:\n                    log.warning(f\'unable to fetch iex_news={ticker}\')\n        if \'financials\' in iex_datasets or extract_iex:\n            iex_financials_status, iex_financials_df = \\\n                iex_extract_utils.extract_financials_dataset(\n                    extract_req)\n            if iex_financials_status != ae_consts.SUCCESS:\n                if verbose:\n                    log.warning(f\'unable to fetch iex_financials={ticker}\')\n        if \'earnings\' in iex_datasets or extract_iex:\n            iex_earnings_status, iex_earnings_df = \\\n                iex_extract_utils.extract_dividends_dataset(\n                    extract_req)\n            if iex_earnings_status != ae_consts.SUCCESS:\n                if verbose:\n                    log.warning(f\'unable to fetch iex_earnings={ticker}\')\n        if \'dividends\' in iex_datasets or extract_iex:\n            iex_dividends_status, iex_dividends_df = \\\n                iex_extract_utils.extract_dividends_dataset(\n                    extract_req)\n            if iex_dividends_status != ae_consts.SUCCESS:\n                if verbose:\n                    log.warning(f\'unable to fetch iex_dividends={ticker}\')\n        if \'company\' in iex_datasets or extract_iex:\n            iex_company_status, iex_company_df = \\\n                iex_extract_utils.extract_dividends_dataset(\n                    extract_req)\n            if iex_company_status != ae_consts.SUCCESS:\n                if verbose:\n                    log.warning(f\'unable to fetch iex_company={ticker}\')\n        # end of iex extracts\n\n        if extract_yahoo:\n            yahoo_options_status, yahoo_option_calls_df = \\\n                yahoo_extract_utils.extract_option_calls_dataset(\n                    extract_req)\n            yahoo_options_status, yahoo_option_puts_df = \\\n                yahoo_extract_utils.extract_option_puts_dataset(\n                    extract_req)\n            if yahoo_options_status != ae_consts.SUCCESS:\n                if verbose:\n                    log.warning(f\'unable to fetch yahoo_options={ticker}\')\n            yahoo_pricing_status, yahoo_pricing_df = \\\n                yahoo_extract_utils.extract_pricing_dataset(\n                    extract_req)\n            if yahoo_pricing_status != ae_consts.SUCCESS:\n                if verbose:\n                    log.warning(f\'unable to fetch yahoo_pricing={ticker}\')\n            yahoo_news_status, yahoo_news_df = \\\n                yahoo_extract_utils.extract_yahoo_news_dataset(\n                    extract_req)\n            if yahoo_news_status != ae_consts.SUCCESS:\n                if verbose:\n                    log.warning(f\'unable to fetch yahoo_news={ticker}\')\n        # end of yahoo extracts\n\n        if extract_td:\n            td_calls_status, td_calls_df = \\\n                td_extract_utils.extract_option_calls_dataset(\n                    extract_req)\n            if td_calls_status != ae_consts.SUCCESS:\n                if verbose:\n                    log.warning(f\'unable to fetch tdcalls={ticker}\')\n            td_puts_status, td_puts_df = \\\n                td_extract_utils.extract_option_puts_dataset(\n                    extract_req)\n            if td_puts_status != ae_consts.SUCCESS:\n                if verbose:\n                    log.warning(f\'unable to fetch tdputs={ticker}\')\n        # td extracts\n\n        ticker_data[\'daily\'] = iex_daily_df\n        ticker_data[\'minute\'] = iex_minute_df\n        ticker_data[\'quote\'] = iex_quote_df\n        ticker_data[\'stats\'] = iex_stats_df\n        ticker_data[\'peers\'] = iex_peers_df\n        ticker_data[\'news1\'] = iex_news_df\n        ticker_data[\'financials\'] = iex_financials_df\n        ticker_data[\'earnings\'] = iex_earnings_df\n        ticker_data[\'dividends\'] = iex_dividends_df\n        ticker_data[\'company\'] = iex_company_df\n        ticker_data[\'calls\'] = yahoo_option_calls_df\n        ticker_data[\'puts\'] = yahoo_option_puts_df\n        ticker_data[\'pricing\'] = yahoo_pricing_df\n        ticker_data[\'news\'] = yahoo_news_df\n        ticker_data[\'tdcalls\'] = td_calls_df\n        ticker_data[\'tdputs\'] = td_puts_df\n\n        rec[ticker] = ticker_data\n    # end of for service_dict in extract_records\n\n    return rec\n# end of fetch\n'"
analysis_engine/get_data_from_redis_key.py,0,"b'""""""\nHelper for getting data from redis\n\nDebug redis calls with:\n\n::\n\n    export DEBUG_REDIS=1\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n\n""""""\n\nimport json\nimport zlib\nimport redis\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.build_result as build_result\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef get_data_from_redis_key(\n        label=None,\n        client=None,\n        host=None,\n        port=None,\n        password=None,\n        db=None,\n        key=None,\n        expire=None,\n        decompress_df=False,\n        serializer=\'json\',\n        encoding=\'utf-8\'):\n    """"""get_data_from_redis_key\n\n    :param label: log tracking label\n    :param client: initialized redis client\n    :param host: not used yet - redis host\n    :param port: not used yet - redis port\n    :param password: not used yet - redis password\n    :param db: not used yet - redis db\n    :param key: not used yet - redis key\n    :param expire: not used yet - redis expire\n    :param decompress_df: used for decompressing\n        ``pandas.DataFrame`` automatically\n    :param serializer: not used yet - support for future\n                       pickle objects in redis\n    :param encoding: format of the encoded key in redis\n    """"""\n\n    decoded_data = None\n    data = None\n\n    rec = {\n        \'data\': data\n    }\n    res = build_result.build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec=rec)\n\n    log_id = label if label else \'get-data\'\n\n    try:\n\n        use_client = client\n        if not use_client:\n            log.debug(\n                f\'{log_id} - get key={key} new \'\n                f\'client={host}:{port}@{db}\')\n            use_client = redis.Redis(\n                host=host,\n                port=port,\n                password=password,\n                db=db)\n        else:\n            log.debug(f\'{log_id} - get key={key} client\')\n        # create Redis client if not set\n\n        # https://redis-py.readthedocs.io/en/latest/index.html#redis.StrictRedis.get  # noqa\n        raw_data = use_client.get(\n            name=key)\n\n        if raw_data:\n\n            if decompress_df:\n                try:\n                    data = zlib.decompress(\n                        raw_data).decode(\n                            encoding)\n                    rec[\'data\'] = json.loads(data)\n\n                    return build_result.build_result(\n                        status=ae_consts.SUCCESS,\n                        err=None,\n                        rec=rec)\n                except Exception as f:\n                    if (\n                            \'while decompressing data: \'\n                            \'incorrect header check\') in str(f):\n                        data = None\n                        log.critical(\n                            f\'unable to decompress_df in redis_key={key} \'\n                            f\'ex={f}\')\n                    else:\n                        log.error(\n                            f\'failed decompress_df in redis_key={key} \'\n                            f\'ex={f}\')\n                        raise f\n            # allow decompression failure to fallback to previous method\n\n            if not data:\n                log.debug(f\'{log_id} - decoding key={key} encoding={encoding}\')\n                decoded_data = raw_data.decode(encoding)\n\n                log.debug(\n                    f\'{log_id} - deserial key={key} serializer={serializer}\')\n\n                if serializer == \'json\':\n                    data = json.loads(decoded_data)\n                elif serializer == \'df\':\n                    data = decoded_data\n                else:\n                    data = decoded_data\n\n                if data:\n                    if ae_consts.ev(\'DEBUG_REDIS\', \'0\') == \'1\':\n                        log.info(\n                            f\'{log_id} - found key={key} \'\n                            f\'data={ae_consts.ppj(data)}\')\n                    else:\n                        log.debug(f\'{log_id} - found key={key}\')\n            # log snippet - if data\n\n            rec[\'data\'] = data\n\n            return build_result.build_result(\n                status=ae_consts.SUCCESS,\n                err=None,\n                rec=rec)\n        else:\n            log.debug(f\'{log_id} - no data key={key}\')\n            return build_result.build_result(\n                status=ae_consts.SUCCESS,\n                err=None,\n                rec=rec)\n    except Exception as e:\n        err = (\n            f\'{log_id} failed - redis get from decoded={decoded_data} \'\n            f\'data={data} key={key} ex={e}\')\n        log.error(err)\n        res = build_result.build_result(\n            status=ae_consts.ERR,\n            err=err,\n            rec=rec)\n    # end of try/ex for getting redis data\n\n    return res\n# end of get_data_from_redis_key\n'"
analysis_engine/get_pricing.py,0,"b'""""""\n\nUtilities for getting pricing data from yahoo finance for:\n\n- pricing\n- news\n- options\n\nInternal version of:\nhttps://github.com/neberej/pinance/master/pinance/engine/yfinance2.py\n""""""\n\n\nimport datetime\nimport json\nimport random\nimport urllib.request\nimport urllib.error\nimport urllib.parse\nimport pandas as pd\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.utils as ae_utils\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef create_url(\n        ticker,\n        exp_date_str):\n    """"""create_url\n\n    :param ticker: ticker to look up\n    :param exp_date_str: expiration\n    """"""\n    srv = random.randrange(1, 3, 1)\n    if exp_date_str:\n        return (\n            f\'https://query{srv}.finance.yahoo.com/v7/\'\n            f\'finance/options/{ticker}?&date={exp_date_str}\')\n    else:\n        return (\n            f\'https://query{srv}.finance.yahoo.com/v7/\'\n            f\'finance/options/{ticker}\')\n# end of create_url\n\n\n# Convert date/time to unix time for options\ndef totimestamp(\n        inputdate,\n        epoch=datetime.datetime(1970, 1, 1)):\n    """"""totimestamp\n\n    :param 1970:\n    :param 1:\n    :param 1:\n    """"""\n    dt = datetime.datetime.strptime(inputdate, \'%Y-%m-%d\')\n    td = dt - epoch\n    timestamp = (\n        td.microseconds + (\n            td.seconds + td.days * 24 * 3600\n        ) * 10**6) / 1e6  # td.total_seconds()\n    return int(timestamp)\n# end of totimestamp\n\n\n# Make request to yahoo finance\ndef make_request(\n        ticker,\n        exp_date_str=None):\n    """"""make_request\n\n    :param ticker: ticker to use\n    :param exp_date_str: contract expiration date format ``YYYY-MM-DD``\n    """"""\n    if exp_date_str:\n        use_exp_date = totimestamp(exp_date_str)\n        url = create_url(ticker, use_exp_date)\n    else:\n        url = create_url(ticker, None)\n\n    try:\n        response = json.loads(\n            urllib.request.urlopen(url).read().decode(\'utf-8\'))\n    except urllib.error.URLError as e:\n        if hasattr(e, \'reason\'):\n            return []\n        elif hasattr(e, \'code\'):\n            return []\n    return response\n# end of make_request\n\n\ndef extract_options_data(\n        response,\n        contract_type,\n        strike=None):\n    """"""extract_options_data\n\n    :param response: previous response data\n    :param contract_type: ``C`` for calls or ``P`` for puts\n    :param strike: strike price\n    """"""\n    if strike:\n        log.debug(f\'getting contract={contract_type} strike={strike}\')\n        if contract_type == \'C\':\n            calls = response[\'optionChain\'][\'result\'][0][\'options\'][0][\'calls\']\n            for call in calls:\n                if call[\'strike\'] == round(strike, 1):\n                    return [\n                        call\n                    ]\n\n        elif contract_type == \'P\':\n            puts = response[\'optionChain\'][\'result\'][0][\'options\'][0][\'puts\']\n            for put in puts:\n                if put[\'strike\'] == round(strike, 1):\n                    return [\n                        put\n                    ]\n    else:\n        log.debug(\n            \'getting all chains\')\n    # end of if strike\n\n    return response[\'optionChain\'][\'result\'][0][\'options\']\n# end of extract_options_data\n\n\ndef get_quotes(\n        ticker):\n    """"""get_quotes\n\n    :param ticker: ticker to get pricing data\n    """"""\n    response = make_request(\n        ticker=ticker,\n        exp_date_str=None)\n    try:\n        quotes_data = response[\'optionChain\'][\'result\'][0][\'quote\']\n        return quotes_data\n    except Exception as e:\n        log.error(f\'failed get_quotes(ticker={ticker}) with ex={e}\')\n        return []\n# end of get_quotes\n\n\ndef get_options(\n        ticker,\n        contract_type,\n        exp_date_str,\n        strike=None):\n    """"""get_options\n\n    :param ticker: ticker to lookup\n    :param exp_date_str: ``YYYY-MM-DD`` expiration date format\n    :param strike: optional strike price, ``None`` returns\n                   all option chains\n    :param contract_type: ``C`` calls or ``P`` for puts, if\n                          ``strike=None`` then the ``contract_type``\n                          is ignored\n    """"""\n    log.info(\n        f\'get_options ticker={ticker} contract={contract_type} \'\n        f\'exp_date={exp_date_str} strike={strike}\')\n\n    response = make_request(\n        ticker=ticker,\n        exp_date_str=exp_date_str)\n    try:\n        options_data = extract_options_data(\n            response=response,\n            contract_type=contract_type,\n            strike=strike)\n        options_dict = {\n            \'date\': ae_utils.get_last_close_str(),\n            \'exp_date\': None,\n            \'num_calls\': None,\n            \'num_puts\': None,\n            \'calls\': None,\n            \'puts\': None\n        }\n        if \'expirationDate\' in options_data[0]:\n            epoch_exp = options_data[0][\'expirationDate\']\n            options_dict[\'exp_date\'] = \\\n                datetime.datetime.fromtimestamp(\n                    epoch_exp).strftime(\n                        ae_consts.COMMON_TICK_DATE_FORMAT)\n        calls_df = pd.DataFrame(\n            options_data[0][\'calls\'])\n        options_dict[\'num_calls\'] = len(\n            options_data[0][\'calls\'])\n        options_dict[\'calls\'] = calls_df.to_json(\n            orient=\'records\')\n        puts_df = pd.DataFrame(\n            options_data[0][\'puts\'])\n        options_dict[\'num_puts\'] = len(\n            options_data[0][\'puts\'])\n        options_dict[\'puts\'] = puts_df.to_json(\n            orient=\'records\')\n\n        return options_dict\n    except Exception as e:\n        log.error(\n            \'failed get_options(\'\n            f\'ticker={ticker}, \'\n            f\'contract_type={contract_type}, \'\n            f\'exp_date_str={exp_date_str}, \'\n            f\'strike={strike}) with ex={e}\')\n        return []\n# end of get_options\n'"
analysis_engine/get_task_results.py,0,"b'""""""\nGet Task Results\n\nDebug by setting the environment variable:\n\n::\n\n        export DEBUG_TASK=1\n\n""""""\n\nimport analysis_engine.consts as ae_consts\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef get_task_results(\n        work_dict=None,\n        result=None,\n        **kwargs):\n    """"""get_task_results\n\n    If celery is disabled by the\n    environment key ``export CELERY_DISABLED=1``\n    or requested in the ``work_dict[\'celery_disabled\'] = True`` then\n    return the task result dictionary, otherwise\n    return ``None``.\n\n    This method is useful for allowing tests\n    to override the returned payloads during task chaining\n    using ``@mock.patch``.\n\n    :param work_dict: task work dictionary\n    :param result: task result dictionary\n    :param kwargs: keyword arguments\n    """"""\n\n    send_results_back = None\n    cel_disabled = False\n    if work_dict:\n        if ae_consts.is_celery_disabled(\n                work_dict=work_dict):\n            send_results_back = result\n            cel_disabled = True\n    # end of sending back results if told to do so\n\n    if ae_consts.ev(\'DEBUG_TASK\', \'0\') == \'1\':\n        status = ae_consts.NOT_SET\n        err = None\n        record = None\n        label = None\n        if result:\n            status = result.get(\n                \'status\',\n                ae_consts.NOT_SET)\n            err = result.get(\n                \'err\',\n                None)\n            record = result.get(\n                \'rec\',\n                None)\n        if work_dict:\n            label = work_dict.get(\n                \'label\',\n                None)\n        log_id = \'get_task_results\'\n        if label:\n            log_id = f\'{label} - get_task_results\'\n\n        result_details = record\n        if record:\n            result_details = ae_consts.ppj(record)\n\n        status_details = status\n        if status:\n            status_details = ae_consts.get_status(status=status)\n\n        work_details = work_dict\n        if work_dict:\n            work_details = ae_consts.ppj(work_dict)\n\n        if status == ae_consts.SUCCESS or not cel_disabled:\n            log.info(\n                f\'{log_id} - celery_disabled={cel_disabled} \'\n                f\'status={status_details} err={err} work_dict={work_details} \'\n                f\'result={result_details}\')\n        else:\n            log.error(\n                f\'{log_id} - celery_disabled={cel_disabled} \'\n                f\'status={status_details} err={err} work_dict={work_details} \'\n                f\'result={result_details}\')\n    # end of if debugging the task results\n\n    return send_results_back\n# end of get_task_results\n'"
analysis_engine/holidays.py,0,"b'""""""\nHoliday detection for US Markets\n\n`Stack Overflow for this module <https://stackoverflow.com/questions/33094297/\ncreate-trading-holiday-calendar-with-pandas>`__\n""""""\n\nimport datetime as dt\nimport pandas.tseries.holiday as pd_holiday\n\n\nclass USTradingCalendar(\n        pd_holiday.AbstractHolidayCalendar):\n    """"""USTradingCalendar""""""\n    rules = [\n        pd_holiday.Holiday(\n            \'NewYearsDay\',\n            month=1,\n            day=1,\n            observance=pd_holiday.nearest_workday),\n        pd_holiday.USMartinLutherKingJr,\n        pd_holiday.USPresidentsDay,\n        pd_holiday.GoodFriday,\n        pd_holiday.USMemorialDay,\n        pd_holiday.Holiday(\n            \'USIndependenceDay\',\n            month=7,\n            day=4,\n            observance=pd_holiday.nearest_workday),\n        pd_holiday.USLaborDay,\n        pd_holiday.USThanksgivingDay,\n        pd_holiday.Holiday(\n            \'Christmas\',\n            month=12,\n            day=25,\n            observance=pd_holiday.nearest_workday)\n    ]\n# end of USTradingCalendar\n\n\ndef get_trading_close_holidays(\n        year=None):\n    """"""get_trading_close_holidays\n\n    Get Trading Holidays for the year\n\n    :param year: optional - year integer\n    """"""\n    use_year = year\n    if not use_year:\n        use_year = int(dt.datetime.utcnow().year)\n    inst = USTradingCalendar()\n    return inst.holidays(\n        dt.datetime(use_year-1, 12, 31),\n        dt.datetime(use_year, 12, 31))\n# end of get_trading_close_holidays\n\n\ndef is_holiday(\n        date=None,\n        date_str=None,\n        fmt=\'%Y-%m-%d\'):\n    """"""is_holiday\n\n    Determine if the ``date`` is a holiday, if not then determine\n    if today is a holiday. Returns ``True`` if it is a holiday and\n    ``False`` if it is not a holiday in the US Markets.\n\n    :param date: optional - datetime object object\n        for calling ``get_trading_close_holidays(year=date.year)``\n    :param date_str: optional - date string formatted with ``fmt``\n    :param fmt: optional - datetime.strftime formatter\n    """"""\n    cal_df = None\n    use_date = dt.datetime.utcnow()\n    if date:\n        use_date = date\n    else:\n        if date_str:\n            use_date = dt.datetime.strptime(\n                date_str,\n                fmt)\n    cal_df = get_trading_close_holidays(\n        year=use_date.year)\n    use_date_str = use_date.strftime(fmt)\n    for d in cal_df.to_list():\n        if d.strftime(fmt) == use_date_str:\n            return True\n    return False\n# end of is_holiday\n'"
analysis_engine/load_algo_dataset_from_file.py,0,"b'""""""\nHelper for loading datasets from a file\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.prepare_dict_for_algo as prepare_utils\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef load_algo_dataset_from_file(\n        path_to_file,\n        serialize_datasets=ae_consts.DEFAULT_SERIALIZED_DATASETS,\n        compress=True,\n        encoding=\'utf-8\'):\n    """"""load_algo_dataset_from_file\n\n    Load an algorithm-ready dataset for algorithm backtesting\n    from a local file\n\n    :param path_to_file: string - path to file holding an\n        algorithm-ready dataset\n    :param serialize_datasets: optional - list of dataset names to\n        deserialize in the dataset\n    :param compress: optional - boolean flag for decompressing\n        the contents of the ``path_to_file`` if necessary\n        (default is ``True`` and algorithms\n        use ``zlib`` for compression)\n    :param encoding: optional - string for data encoding\n    """"""\n    log.info(\n        f\'start: {path_to_file}\')\n    data_from_file = None\n    file_args = \'rb\'\n    if not compress:\n        file_args = \'r\'\n    with open(path_to_file, file_args) as cur_file:\n        data_from_file = cur_file.read()\n\n    if not data_from_file:\n        log.error(f\'missing data from file={path_to_file}\')\n        return None\n\n    return prepare_utils.prepare_dict_for_algo(\n        data=data_from_file,\n        compress=compress,\n        convert_to_dict=True,\n        encoding=encoding)\n# end of load_algo_dataset_from_file\n'"
analysis_engine/load_algo_dataset_from_redis.py,0,"b'""""""\nHelper for loading datasets from redis\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.get_data_from_redis_key as redis_utils\nimport analysis_engine.prepare_dict_for_algo as prepare_utils\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef load_algo_dataset_from_redis(\n        redis_key,\n        redis_address,\n        redis_db,\n        redis_password,\n        redis_expire=None,\n        redis_serializer=\'json\',\n        serialize_datasets=ae_consts.DEFAULT_SERIALIZED_DATASETS,\n        compress=False,\n        encoding=\'utf-8\'):\n    """"""load_algo_dataset_from_redis\n\n    Load an algorithm-ready dataset for algorithm backtesting\n    from a redis key\n\n    :param serialize_datasets: optional - list of dataset names to\n        deserialize in the dataset\n    :param compress: optional - boolean flag for decompressing\n        the contents of the ``path_to_file`` if necessary\n        (default is ``False`` and algorithms\n        use ``zlib`` for compression)\n    :param encoding: optional - string for data encoding\n    """"""\n    log.debug(\'start\')\n    data_from_file = None\n\n    redis_host = redis_address.split(\':\')[0]\n    redis_port = int(redis_address.split(\':\')[0])\n\n    redis_res = redis_utils.get_data_from_redis_key(\n        key=redis_key,\n        host=redis_host,\n        port=redis_port,\n        db=redis_db,\n        password=redis_password,\n        expire=redis_expire,\n        serializer=redis_serializer,\n        encoding=encoding)\n\n    if redis_res[\'status\'] != ae_consts.SUCCESS:\n        log.error(\n            \'failed getting data from \'\n            f\'redis={redis_address}:{redis_db}/{redis_key}\')\n        return None\n\n    data_from_file = redis_res[\'rec\'][\'data\']\n    if not data_from_file:\n        log.error(\n            f\'missing data from redis={redis_address}:{redis_db}/{redis_key}\')\n        return None\n\n    return prepare_utils.prepare_dict_for_algo(\n        data=data_from_file,\n        compress=compress,\n        convert_to_dict=True,\n        encoding=encoding)\n# end of load_algo_dataset_from_redis\n'"
analysis_engine/load_algo_dataset_from_s3.py,0,"b'""""""\nHelper for loading datasets from s3\n""""""\n\nimport boto3\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.prepare_dict_for_algo as prepare_utils\nimport analysis_engine.s3_read_contents_from_key as s3_utils\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef load_algo_dataset_from_s3(\n        s3_key,\n        s3_address,\n        s3_bucket,\n        s3_access_key,\n        s3_secret_key,\n        s3_region_name,\n        s3_secure,\n        serialize_datasets=ae_consts.DEFAULT_SERIALIZED_DATASETS,\n        compress=False,\n        encoding=\'utf-8\'):\n    """"""load_algo_dataset_from_s3\n\n    Load an algorithm-ready dataset for algorithm backtesting\n    from a local file\n\n    :param serialize_datasets: optional - list of dataset names to\n        deserialize in the dataset\n    :param compress: optional - boolean flag for decompressing\n        the contents of the ``path_to_file`` if necessary\n        (default is ``False`` and algorithms\n        use ``zlib`` for compression)\n    :param encoding: optional - string for data encoding\n\n    **Minio (S3) connectivity arguments**\n\n    :param s3_enabled: bool - toggle for auto-archiving on Minio (S3)\n        (default is ``True``)\n    :param s3_key: string - key to save the data in redis\n        (default is ``None``)\n    :param s3_address: Minio S3 connection string format: ``host:port``\n        (default is ``localhost:9000``)\n    :param s3_bucket: S3 Bucket for storing the artifacts\n        (default is ``dev``) which should be viewable on a browser:\n        http://localhost:9000/minio/dev/\n    :param s3_access_key: S3 Access key\n        (default is ``trexaccesskey``)\n    :param s3_secret_key: S3 Secret key\n        (default is ``trex123321``)\n    :param s3_region_name: S3 region name\n        (default is ``us-east-1``)\n    :param s3_secure: Transmit using tls encryption\n        (default is ``False``)\n    """"""\n    log.info(\n        f\'start s3={s3_address}:{s3_bucket}/{s3_key}\')\n\n    data_from_file = None\n\n    endpoint_url = f\'http://{s3_address}\'\n    if s3_secure:\n        endpoint_url = f\'https://{s3_address}\'\n\n    s3 = boto3.resource(\n        \'s3\',\n        endpoint_url=endpoint_url,\n        aws_access_key_id=s3_access_key,\n        aws_secret_access_key=s3_secret_key,\n        region_name=s3_region_name,\n        config=boto3.session.Config(signature_version=\'s3v4\'))\n\n    # compressed files will not work with json.dumps\n    try:\n        data_from_file = s3_utils.s3_read_contents_from_key(\n            s3=s3,\n            s3_bucket_name=s3_bucket,\n            s3_key=s3_key,\n            encoding=encoding,\n            convert_as_json=not compress,\n            compress=compress)\n    except Exception as e:\n        if (\n                \'An error occurred (NoSuchBucket) \'\n                \'when calling the GetObject operation\') in str(e):\n            msg = (\n                f\'missing s3_bucket={s3_address} in s3_address={s3_bucket}\')\n            log.error(msg)\n            raise Exception(msg)\n        else:\n            raise Exception(e)\n\n    if not data_from_file:\n        log.error(\n            \'missing data from s3={s3_address}:{s3_bucket}/{s3_key}\')\n        return None\n\n    return prepare_utils.prepare_dict_for_algo(\n        data=data_from_file,\n        compress=False,\n        convert_to_dict=True,\n        encoding=encoding)\n# end of load_algo_dataset_from_s3\n'"
analysis_engine/load_dataset.py,0,"b'""""""\nLoad an algorithm dataset from file, s3 or redis\n\nSupported Datasets:\n\n- ``SA_DATASET_TYPE_ALGO_READY`` - Algorithm-ready datasets\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport os\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.load_algo_dataset_from_file as file_utils\nimport analysis_engine.load_algo_dataset_from_s3 as s3_utils\nimport analysis_engine.load_algo_dataset_from_redis as redis_utils\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef load_dataset(\n        algo_dataset=None,\n        dataset_type=ae_consts.SA_DATASET_TYPE_ALGO_READY,\n        serialize_datasets=ae_consts.DEFAULT_SERIALIZED_DATASETS,\n        path_to_file=None,\n        compress=False,\n        encoding=\'utf-8\',\n        redis_enabled=True,\n        redis_key=None,\n        redis_address=None,\n        redis_db=None,\n        redis_password=None,\n        redis_expire=None,\n        redis_serializer=\'json\',\n        redis_encoding=\'utf-8\',\n        s3_enabled=True,\n        s3_key=None,\n        s3_address=None,\n        s3_bucket=None,\n        s3_access_key=None,\n        s3_secret_key=None,\n        s3_region_name=None,\n        s3_secure=False,\n        slack_enabled=False,\n        slack_code_block=False,\n        slack_full_width=False,\n        verbose=False):\n    """"""load_dataset\n\n    Load an algorithm dataset from file, s3 or redis\n\n    :param algo_dataset: optional - already loaded algorithm-ready dataset\n    :param dataset_type: optional - dataset type\n        (default is ``SA_DATASET_TYPE_ALGO_READY``)\n    :param path_to_file: optional - path to an algorithm-ready dataset\n        in a file\n    :param serialize_datasets: optional - list of dataset names to\n        deserialize in the dataset\n    :param compress: optional - boolean flag for decompressing\n        the contents of the ``path_to_file`` if necessary\n        (default is ``False`` and algorithms\n        use ``zlib`` for compression)\n    :param encoding: optional - string for data encoding\n\n    **(Optional) Redis connectivity arguments**\n\n    :param redis_enabled: bool - toggle for auto-caching all\n        datasets in Redis\n        (default is ``True``)\n    :param redis_key: string - key to save the data in redis\n        (default is ``None``)\n    :param redis_address: Redis connection string format: ``host:port``\n        (default is ``localhost:6379``)\n    :param redis_db: Redis db to use\n        (default is ``0``)\n    :param redis_password: optional - Redis password\n        (default is ``None``)\n    :param redis_expire: optional - Redis expire value\n        (default is ``None``)\n    :param redis_serializer: not used yet - support for future\n        pickle objects in redis\n    :param redis_encoding: format of the encoded key in redis\n\n    **(Optional) Minio (S3) connectivity arguments**\n\n    :param s3_enabled: bool - toggle for auto-archiving on Minio (S3)\n        (default is ``True``)\n    :param s3_key: string - key to save the data in redis\n        (default is ``None``)\n    :param s3_address: Minio S3 connection string format: ``host:port``\n        (default is ``localhost:9000``)\n    :param s3_bucket: S3 Bucket for storing the artifacts\n        (default is ``dev``) which should be viewable on a browser:\n        http://localhost:9000/minio/dev/\n    :param s3_access_key: S3 Access key\n        (default is ``trexaccesskey``)\n    :param s3_secret_key: S3 Secret key\n        (default is ``trex123321``)\n    :param s3_region_name: S3 region name\n        (default is ``us-east-1``)\n    :param s3_secure: Transmit using tls encryption\n        (default is ``False``)\n\n    **(Optional) Slack arguments**\n\n    :param slack_enabled: optional - boolean for\n        publishing to slack\n    :param slack_code_block: optional - boolean for\n        publishing as a code black in slack\n    :param slack_full_width: optional - boolean for\n        publishing as a to slack using the full\n        width allowed\n\n    Additonal arguments\n\n    :param verbose: optional - bool for increasing\n        logging\n    """"""\n\n    use_ds = algo_dataset\n    if not use_ds:\n        log.info(\n            f\'loading {ae_consts.get_status(status=dataset_type)} from \'\n            f\'file={path_to_file} s3={s3_key} redis={redis_key}\')\n    # load if not created\n\n    supported_type = False\n    if dataset_type == ae_consts.SA_DATASET_TYPE_ALGO_READY:\n        supported_type = True\n        if (path_to_file and\n                not use_ds):\n            if not os.path.exists(path_to_file):\n                log.error(f\'missing file: {path_to_file}\')\n            use_ds = file_utils.load_algo_dataset_from_file(\n                path_to_file=path_to_file,\n                compress=compress,\n                encoding=redis_encoding,\n                serialize_datasets=serialize_datasets)\n        elif (s3_key and\n                not use_ds):\n            use_ds = s3_utils.load_algo_dataset_from_s3(\n                s3_key=s3_key,\n                s3_address=s3_address,\n                s3_bucket=s3_bucket,\n                s3_access_key=s3_access_key,\n                s3_secret_key=s3_secret_key,\n                s3_region_name=s3_region_name,\n                s3_secure=s3_secure,\n                compress=compress,\n                encoding=redis_encoding,\n                serialize_datasets=serialize_datasets)\n        elif (redis_key and\n                not use_ds):\n            use_ds = redis_utils.load_algo_dataset_from_redis(\n                redis_key=redis_key,\n                redis_address=redis_address,\n                redis_db=redis_db,\n                redis_password=redis_password,\n                redis_expire=redis_expire,\n                redis_serializer=redis_serializer,\n                compress=compress,\n                encoding=redis_encoding,\n                serialize_datasets=serialize_datasets)\n    else:\n        supported_type = False\n        use_ds = None\n        log.error(\n            f\'loading {dataset_type} from file={path_to_file} \'\n            f\'s3={s3_key} redis={redis_key}\')\n    # load if not created\n\n    if not use_ds and supported_type:\n        log.error(\n            f\'unable to load a dataset from file={path_to_file} \'\n            f\'s3={s3_key} redis={redis_key}\')\n\n    return use_ds\n# end of load_dataset\n'"
analysis_engine/load_history_dataset.py,0,"b'""""""\nLoad an ``Trading History`` dataset from file, s3 -\nredis coming soon\n\nSupported Datasets:\n\n- ``SA_DATASET_TYPE_TRADING_HISTORY`` - trading history datasets\n\n""""""\n\nimport os\nimport analysis_engine.consts as consts\nimport analysis_engine.load_history_dataset_from_file as file_utils\nimport analysis_engine.load_history_dataset_from_s3 as s3_utils\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef load_history_dataset(\n        history_dataset=None,\n        dataset_type=None,\n        serialize_datasets=None,\n        path_to_file=None,\n        compress=None,\n        encoding=\'utf-8\',\n        convert_to_dict=False,\n        redis_enabled=None,\n        redis_key=None,\n        redis_address=None,\n        redis_db=None,\n        redis_password=None,\n        redis_expire=None,\n        redis_serializer=\'json\',\n        redis_encoding=\'utf-8\',\n        s3_enabled=None,\n        s3_key=None,\n        s3_address=None,\n        s3_bucket=None,\n        s3_access_key=None,\n        s3_secret_key=None,\n        s3_region_name=None,\n        s3_secure=None,\n        slack_enabled=False,\n        slack_code_block=False,\n        slack_full_width=False,\n        verbose=False):\n    """"""load_history_dataset\n\n    Load a ``Trading History`` Dataset from file, s3 - note\n    redis is not supported yet\n\n    :param history_dataset: optional - already loaded history dataset\n    :param dataset_type: optional - dataset type\n        (default is ``analysis_engine.consts.SA_DATASET_TYPE_TRADING_HISTORY``)\n    :param path_to_file: optional - path to a trading history dataset\n        in a file\n    :param serialize_datasets: optional - list of dataset names to\n        deserialize in the dataset\n    :param compress: optional - boolean flag for decompressing\n        the contents of the ``path_to_file`` if necessary\n        (default is ``True`` and uses ``zlib`` for compression)\n    :param encoding: optional - string for data encoding\n    :param convert_to_dict: optional - boolean flag for decoding\n        as a dictionary during prepare\n\n    **(Optional) Redis connectivity arguments**\n\n    :param redis_enabled: bool - toggle for auto-caching all\n        datasets in Redis\n        (default is ``analysis_engine.consts.ENABLED_REDIS_PUBLISH``)\n    :param redis_key: string - key to save the data in redis\n        (default is ``None``)\n    :param redis_address: Redis connection string format: ``host:port``\n        (default is ``analysis_engine.consts.REDIS_ADDRESS``)\n    :param redis_db: Redis db to use\n        (default is ``analysis_engine.consts.REDIS_DB``)\n    :param redis_password: optional - Redis password\n        (default is ``analysis_engine.consts.REDIS_PASSWORD``)\n    :param redis_expire: optional - Redis expire value\n        (default is ``None``)\n    :param redis_serializer: not used yet - support for future\n        pickle objects in redis\n        (default is ``json``)\n    :param redis_encoding: format of the encoded key in redis\n        (default is ``utf-8``)\n\n    **(Optional) Minio (S3) connectivity arguments**\n\n    :param s3_enabled: bool - toggle for auto-archiving on Minio (S3)\n        (default is ``analysis_engine.consts.ENABLED_S3_UPLOAD``)\n    :param s3_key: string - key to save the data in redis\n        (default is ``None``)\n    :param s3_address: Minio S3 connection string format: ``host:port``\n        (default is ``analysis_engine.consts.S3_ADDRESS``)\n    :param s3_bucket: S3 Bucket for storing the artifacts\n        (default is ``analysis_engine.consts.S3_BUCKET``) which should be\n        viewable on a browser:\n        http://localhost:9000/minio/\n    :param s3_access_key: S3 Access key\n        (default is ``analysis_engine.consts.S3_ACCESS_KEY``)\n    :param s3_secret_key: S3 Secret key\n        (default is ``analysis_engine.consts.S3_SECRET_KEY``)\n    :param s3_region_name: S3 region name\n        (default is ``analysis_engine.consts.S3_REGION_NAME``)\n    :param s3_secure: Transmit using tls encryption\n        (default is ``analysis_engine.consts.S3_SECURE``)\n\n    **(Optional) Slack arguments**\n\n    :param slack_enabled: optional - boolean for\n        publishing to slack\n    :param slack_code_block: optional - boolean for\n        publishing as a code black in slack\n    :param slack_full_width: optional - boolean for\n        publishing as a to slack using the full\n        width allowed\n\n    Additonal arguments\n\n    :param verbose: optional - bool for increasing\n        logging\n    """"""\n\n    if not dataset_type:\n        dataset_type = consts.SA_DATASET_TYPE_TRADING_HISTORY\n    if not serialize_datasets:\n        serialize_datasets = consts.DEFAULT_SERIALIZED_DATASETS\n    if not redis_enabled:\n        redis_enabled = consts.ENABLED_REDIS_PUBLISH\n    if not redis_address:\n        redis_address = consts.REDIS_ADDRESS\n    if not redis_db:\n        redis_db = consts.REDIS_DB\n    if not redis_password:\n        redis_password = consts.REDIS_PASSWORD\n    if not s3_enabled:\n        s3_enabled = consts.ENABLED_S3_UPLOAD\n    if not s3_address:\n        s3_address = consts.S3_ADDRESS\n    if not s3_bucket:\n        s3_bucket = consts.S3_BUCKET\n    if not s3_access_key:\n        s3_access_key = consts.S3_ACCESS_KEY\n    if not s3_secret_key:\n        s3_secret_key = consts.S3_SECRET_KEY\n    if not s3_region_name:\n        s3_region_name = consts.S3_REGION_NAME\n    if not s3_secure:\n        s3_secure = consts.S3_SECURE\n    if compress is None:\n        compress = consts.ALGO_HISTORY_COMPRESS\n\n    use_ds = history_dataset\n    if not use_ds:\n        log.info(\n            f\'loading {consts.get_status(status=dataset_type)} from \'\n            f\'file={path_to_file} s3={s3_key} redis={redis_key}\')\n    # load if not created\n\n    supported_type = False\n    if dataset_type == consts.SA_DATASET_TYPE_TRADING_HISTORY:\n        supported_type = True\n        if (path_to_file and\n                not use_ds):\n            if not os.path.exists(path_to_file):\n                log.error(f\'missing file: {path_to_file}\')\n            use_ds = file_utils.load_history_dataset_from_file(\n                path_to_file=path_to_file,\n                compress=compress,\n                encoding=redis_encoding)\n        elif (s3_key and\n                not use_ds):\n            use_ds = s3_utils.load_history_dataset_from_s3(\n                s3_key=s3_key,\n                s3_address=s3_address,\n                s3_bucket=s3_bucket,\n                s3_access_key=s3_access_key,\n                s3_secret_key=s3_secret_key,\n                s3_region_name=s3_region_name,\n                s3_secure=s3_secure,\n                compress=compress,\n                encoding=redis_encoding,\n                convert_to_dict=convert_to_dict,\n                serialize_datasets=serialize_datasets)\n    else:\n        supported_type = False\n        use_ds = None\n        log.error(\n            f\'loading {dataset_type} from file={path_to_file} \'\n            f\'s3={s3_key} redis={redis_key}\')\n    # load if not created\n\n    if not use_ds and supported_type:\n        log.error(\n            f\'unable to load a dataset from file={path_to_file} \'\n            f\'s3={s3_key} redis={redis_key}\')\n\n    return use_ds\n# end of load_history_dataset\n'"
analysis_engine/load_history_dataset_from_file.py,0,"b'""""""\nHelper for loading ``Trading History`` dataset from a file\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport analysis_engine.prepare_history_dataset as prepare_history\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef load_history_dataset_from_file(\n        path_to_file,\n        compress=False,\n        encoding=\'utf-8\'):\n    """"""load_history_dataset_from_file\n\n    Load a ``Trading History`` dataset from a local file\n\n    :param path_to_file: string - path to file holding an\n        ``Trading History`` dataset\n    :param compress: optional - boolean flag for decompressing\n        the contents of the ``path_to_file`` if necessary\n        (default is ``False`` and algorithms\n        use ``zlib`` for compression)\n    :param encoding: optional - string for data encoding\n    """"""\n    log.debug(\'start\')\n    data_from_file = None\n    with open(path_to_file, \'r\') as cur_file:\n        data_from_file = cur_file.read()\n\n    if not data_from_file:\n        log.error(f\'missing data from file={path_to_file}\')\n        return None\n\n    return prepare_history.prepare_history_dataset(\n        data=data_from_file,\n        compress=compress,\n        convert_to_dict=True,\n        encoding=encoding)\n# end of load_history_dataset_from_file\n'"
analysis_engine/load_history_dataset_from_s3.py,0,"b'""""""\nHelper for loading ``Trading History`` datasets from s3\n""""""\n\nimport boto3\nimport analysis_engine.consts as consts\nimport analysis_engine.prepare_history_dataset as prepare_utils\nimport analysis_engine.s3_read_contents_from_key as s3_utils\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef load_history_dataset_from_s3(\n        s3_key,\n        s3_address,\n        s3_bucket,\n        s3_access_key,\n        s3_secret_key,\n        s3_region_name,\n        s3_secure,\n        serialize_datasets=consts.DEFAULT_SERIALIZED_DATASETS,\n        convert_as_json=True,\n        convert_to_dict=False,\n        compress=False,\n        encoding=\'utf-8\'):\n    """"""load_history_dataset_from_s3\n\n    Load an algorithm-ready dataset for algorithm backtesting\n    from a local file\n\n    :param serialize_datasets: optional - list of dataset names to\n        deserialize in the dataset\n    :param convert_as_json: optional - boolean flag for decoding\n        as a dictionary\n    :param convert_to_dict: optional - boolean flag for decoding\n        as a dictionary during prepare\n    :param compress: optional - boolean flag for decompressing\n        the contents of the ``path_to_file`` if necessary\n        (default is ``False`` and algorithms\n        use ``zlib`` for compression)\n    :param encoding: optional - string for data encoding\n\n    **Minio (S3) connectivity arguments**\n\n    :param s3_enabled: bool - toggle for auto-archiving on Minio (S3)\n        (default is ``True``)\n    :param s3_key: string - key to save the data in redis\n        (default is ``None``)\n    :param s3_address: Minio S3 connection string format: ``host:port``\n        (default is ``localhost:9000``)\n    :param s3_bucket: S3 Bucket for storing the artifacts\n        (default is ``dev``) which should be viewable on a browser:\n        http://localhost:9000/minio/dev/\n    :param s3_access_key: S3 Access key\n        (default is ``trexaccesskey``)\n    :param s3_secret_key: S3 Secret key\n        (default is ``trex123321``)\n    :param s3_region_name: S3 region name\n        (default is ``us-east-1``)\n    :param s3_secure: Transmit using tls encryption\n        (default is ``False``)\n    """"""\n    log.info(f\'start s3={s3_address}:{s3_bucket}/{s3_key}\')\n\n    data_from_file = None\n\n    endpoint_url = f\'http{""s"" if s3_secure else """"}://{s3_address}\'\n\n    s3 = boto3.resource(\n        \'s3\',\n        endpoint_url=endpoint_url,\n        aws_access_key_id=s3_access_key,\n        aws_secret_access_key=s3_secret_key,\n        region_name=s3_region_name,\n        config=boto3.session.Config(signature_version=\'s3v4\'))\n\n    try:\n        data_from_file = s3_utils.s3_read_contents_from_key(\n            s3=s3,\n            s3_bucket_name=s3_bucket,\n            s3_key=s3_key,\n            encoding=encoding,\n            convert_as_json=convert_as_json,\n            compress=compress)\n    except Exception as e:\n        if (\n                \'An error occurred (NoSuchBucket) \'\n                \'when calling the GetObject operation\') in str(e):\n            msg = (\n                f\'missing s3_bucket={s3_bucket} in s3_address={s3_address}\')\n            log.error(msg)\n            raise Exception(msg)\n        else:\n            raise Exception(e)\n\n    if not data_from_file:\n        log.error(f\'missing data from s3={s3_address}:{s3_bucket}/{s3_key}\')\n        return None\n\n    return prepare_utils.prepare_history_dataset(\n        data=data_from_file,\n        compress=False,\n        convert_to_dict=convert_to_dict,\n        encoding=encoding)\n# end of load_history_dataset_from_s3\n'"
analysis_engine/load_report_dataset_from_file.py,0,"b'""""""\nHelper for loading ``Trading Performance Report`` dataset from a file\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport analysis_engine.prepare_report_dataset as prepare_report\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef load_report_dataset_from_file(\n        path_to_file,\n        compress=False,\n        encoding=\'utf-8\'):\n    """"""load_report_dataset_from_file\n\n    Load an ``Trading Performance Report`` dataset\n    from a local file\n\n    :param path_to_file: string - path to file holding an\n        ``Trading Performance Report`` dataset\n    :param compress: optional - boolean flag for decompressing\n        the contents of the ``path_to_file`` if necessary\n        (default is ``False`` and algorithms\n        use ``zlib`` for compression)\n    :param encoding: optional - string for data encoding\n    """"""\n    log.debug(\'start\')\n    data_from_file = None\n    with open(path_to_file, \'r\') as cur_file:\n        data_from_file = cur_file.read()\n\n    if not data_from_file:\n        log.error(f\'missing data from file={path_to_file}\')\n        return None\n\n    return prepare_report.prepare_report_dataset(\n        data=data_from_file,\n        compress=compress,\n        convert_to_dict=True,\n        encoding=encoding)\n# end of load_report_dataset_from_file\n'"
analysis_engine/options_dates.py,0,"b'""""""\nOption Date Utilities\n=====================\n\nThese are a collection of functions for determining\nwhen the current options cycle expires (3rd Friday of most months)\nand for calculating historical option expiration dates.\n\n.. tip:: If you need to automate looking up the current option\n    cycle expiration, then please checkout using the script:\n\n    ::\n\n        /opt/sa/analysis_engine/scripts/print_next_expiration_date.py\n        2018-10-19\n\n""""""\n\nimport datetime\nimport pandas.tseries.offsets as pd_bday\nimport analysis_engine.holidays as hdays\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef get_options_for_years(\n        years=[\n            \'2014\',\n            \'2015\',\n            \'2016\',\n            \'2016\',\n            \'2017\',\n            \'2018\',\n            \'2019\',\n            \'2020\',\n            \'2021\',\n            \'2022\',\n        ]):\n    """"""get_options_for_years\n\n    :param years: number of years back\n    :param months: number of months to build year\n    """"""\n\n    entities = []\n    months = [\n        \'01\',\n        \'02\',\n        \'03\',\n        \'04\',\n        \'05\',\n        \'06\',\n        \'07\',\n        \'08\',\n        \'09\',\n        \'10\',\n        \'11\',\n        \'12\'\n    ]\n    option_expirations = {\n    }\n    opts = []\n\n    for year in years:\n        for month in months:\n            target_date_str = str(month + \'-01-\' + year)\n            target_date = datetime.datetime.strptime(\n                target_date_str,\n                \'%m-%d-%Y\')\n            option_expiration_date = option_expiration(\n                target_date)\n            option_expiration_str = option_expiration_date.strftime(\n                \'%m-%d-%Y\')\n            option_expirations[option_expiration_str] = option_expiration_date\n            opts.append(option_expiration_date.strftime(\n                \'%m-%d-%Y\'))\n        # end of for all months\n    # end of building option expiration dates\n\n    now = datetime.datetime.now() + datetime.timedelta(days=0)\n    num_legs = 20\n    num_done = 1\n    last_exp_date = 0\n    date_format = \'%m-%d-%Y\'\n    str_output = now.strftime(date_format)\n    log.info(f\'current date={str_output}\')\n\n    entities.append(str_output)\n\n    for option_exp in opts:\n        option_exp_date = datetime.datetime.strptime(\n            option_exp,\n            date_format)\n        if option_exp_date >= now:\n            if num_legs > 0:\n                if (last_exp_date == 0):\n                    delta = (option_exp_date - now).days\n                else:\n                    delta = (option_exp_date - last_exp_date).days\n\n                entities.append(str(delta))\n                entities.append(option_exp_date.strftime(\'%m-%d-%Y\'))\n                entities.append(\'Leg \' + str(num_done))\n\n            else:\n                break\n            num_legs -= 1\n            num_done += 1\n            last_exp_date = option_exp_date\n    # end of processing\n\n    return entities\n# end of get_options_for_years\n\n\ndef historical_options(\n        years=[\n            \'2014\',\n            \'2015\',\n            \'2016\',\n            \'2017\',\n            \'2018\',\n            \'2019\',\n            \'2020\',\n            \'2021\',\n            \'2022\',\n            \'2023\',\n            \'2024\',\n            \'2025\',\n            \'2026\',\n            \'2027\',\n            \'2028\'\n        ]):\n    """"""historical_options\n\n    :param years: years to build\n    """"""\n\n    entities = []\n    months = [\n        \'01\',\n        \'02\',\n        \'03\',\n        \'04\',\n        \'05\',\n        \'06\',\n        \'07\',\n        \'08\',\n        \'09\',\n        \'10\',\n        \'11\',\n        \'12\'\n    ]\n    option_expirations = {\n    }\n    opts = []\n\n    for year in years:\n        for month in months:\n            target_date_str = str(month + \'-01-\' + year)\n            target_date = datetime.datetime.strptime(\n                target_date_str,\n                \'%m-%d-%Y\')\n            option_expiration_date = option_expiration(\n                target_date)\n            option_expiration_str = option_expiration_date.strftime(\n                \'%m-%d-%Y\')\n            option_expirations[option_expiration_str] = option_expiration_date\n            opts.append(option_expiration_date.strftime(\n                \'%m-%d-%Y\'))\n        # end of for all months\n\n    # end of building option expiration dates\n\n    now = datetime.datetime.strptime(\'01-01-2009\', \'%m-%d-%Y\')\n    num_legs = 400\n    num_done = 1\n    date_format = \'%m-%d-%Y\'\n\n    for option_exp in opts:\n        option_exp_date = datetime.datetime.strptime(option_exp, date_format)\n        if option_exp_date >= now:\n            if num_legs > 0:\n                entities.append(option_exp_date.strftime(\'%m-%d-%Y\'))\n            else:\n                break\n            num_legs -= 1\n            num_done += 1\n    # end of processing\n\n    return entities\n# end of historical_options\n\n\ndef get_options_between_dates(\n        start_date,\n        end_date):\n    """"""get_options_between_dates\n\n    :param start_date: start date\n    :param end_date: end date\n    """"""\n    valid_options = []\n\n    for rec in historical_options():\n        opt_date = datetime.datetime.strptime(\n            str(rec),\n            \'%m-%d-%Y\').date()\n        if start_date <= opt_date <= end_date:\n            valid_options.append(opt_date.strftime(\'%Y-%m-%d\'))\n\n    return valid_options\n# end of get_options_between_dates\n\n\ndef option_expiration(\n        date=None):\n    """"""option_expiration\n\n    :param date: date to find the current expiration\n    """"""\n    cur_date = date\n    if not cur_date:\n        cur_date = datetime.datetime.now()\n    while (not (cur_date.weekday() == 4 and 14 < cur_date.day < 22)):\n        cur_date = cur_date + datetime.timedelta(days=1)\n\n    if hdays.is_holiday(\n            date=cur_date):\n        test_date = cur_date - datetime.timedelta(days=1)\n        if cur_date.weekday() == 0:\n            test_date = cur_date - datetime.timedelta(days=3)\n        if hdays.is_holiday(\n                date=test_date):\n            test_date = cur_date - datetime.timedelta(days=4)\n            if hdays.is_holiday(\n                    date=test_date):\n                test_date = cur_date - datetime.timedelta(days=5)\n        cur_date = test_date\n    # end of if this date is a holiday and go back\n\n    return cur_date\n# end of option_expiration\n\n\ndef get_options_for_today():\n    """"""get_options_for_today\n\n    Get a list of option expiration nodes where the last cell\n    has the current option cycle\'s expiration date.\n\n    """"""\n    cur_date = datetime.datetime.now()\n    cycle_exps = historical_options()\n    previous_exp = None\n    valid_option_exps = []\n    for idx, org_exp_date_str in enumerate(cycle_exps):\n        log.debug(f\'cycle={idx} expiration={org_exp_date_str}\')\n        exp_date = datetime.datetime.strptime(\n            org_exp_date_str,\n            \'%m-%d-%Y\')\n        exp_date_str = exp_date.strftime(\n            \'%Y-%m-%d\')\n\n        cycle_start_date = exp_date - pd_bday.BDay(19)\n        if previous_exp:\n            cycle_start_date = previous_exp + pd_bday.BDay(1)\n        cycle_start_date_str = cycle_start_date.strftime(\n            \'%m-%d-%Y\')\n        valid_option_exps.append({\n            \'exp_date\': exp_date,\n            \'exp_date_str\': exp_date_str,\n            \'cycle_start\': cycle_start_date,\n            \'cycle_start_str\': cycle_start_date_str\n        })\n        if cur_date < exp_date:\n            break\n        previous_exp = exp_date\n    # end of for all historical options\n\n    return valid_option_exps\n# end of get_options_for_today\n'"
analysis_engine/plot_trading_history.py,0,"b'""""""\nPlot a ``Trading History`` dataset using seaborn and matplotlib\n""""""\n\nimport datetime\nimport matplotlib.pyplot as plt\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.utils as ae_utils\nimport analysis_engine.charts as ae_charts\nimport analysis_engine.build_result as build_result\nimport analysis_engine.send_to_slack as slack_utils\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef plot_trading_history(\n        title,\n        df,\n        red=None,\n        red_color=None,\n        red_label=None,\n        blue=None,\n        blue_color=None,\n        blue_label=None,\n        green=None,\n        green_color=None,\n        green_label=None,\n        orange=None,\n        orange_color=None,\n        orange_label=None,\n        date_col=\'minute\',\n        xlabel=\'Minutes\',\n        ylabel=\'Algo Trading History Values\',\n        linestyle=\'-\',\n        width=9.0,\n        height=9.0,\n        date_format=\'%d\\n%b\',\n        df_filter=None,\n        start_date=None,\n        footnote_text=None,\n        footnote_xpos=0.70,\n        footnote_ypos=0.01,\n        footnote_color=\'#888888\',\n        footnote_fontsize=8,\n        scale_y=False,\n        show_plot=True,\n        dropna_for_all=False,\n        verbose=False,\n        send_plots_to_slack=False):\n    """"""plot_trading_history\n\n    Plot columns up to 4 lines from the ``Trading History`` dataset\n\n    :param title: title of the plot\n    :param df: dataset which is ``pandas.DataFrame``\n    :param red: string - column name to plot in\n        ``red_color`` (or default ``ae_consts.PLOT_COLORS[red]``)\n        where the column is in the ``df`` and\n        accessible with:``df[red]``\n        (default is ``high``)\n    :param red_color: hex color code to plot the data in the\n        ``df[red]``  (default is ``ae_consts.PLOT_COLORS[\'red\']``)\n    :param red_label: optional - string for the label used\n        to identify the ``red`` line in the legend\n    :param blue: string - column name to plot in\n        ``blue_color`` (or default ``ae_consts.PLOT_COLORS[\'blue\']``)\n        where the column is in the ``df`` and\n        accessible with:``df[blue]``\n        (default is ``close``)\n    :param blue_color: hex color code to plot the data in the\n        ``df[blue]``  (default is ``ae_consts.PLOT_COLORS[\'blue\']``)\n    :param blue_label: optional - string for the label used\n        to identify the ``blue`` line in the legend\n    :param green: string - column name to plot in\n        ``green_color`` (or default ``ae_consts.PLOT_COLORS[\'darkgreen\']``)\n        where the column is in the ``df`` and\n        accessible with:``df[green]``\n    :param green_color: hex color code to plot the data in the\n        ``df[green]``  (default is ``ae_consts.PLOT_COLORS[\'darkgreen\']``)\n    :param green_label: optional - string for the label used\n        to identify the ``green`` line in the legend\n    :param orange: string - column name to plot in\n        ``orange_color`` (or default ``ae_consts.PLOT_COLORS[\'orange\']``)\n        where the column is in the ``df`` and\n        accessible with:``df[orange]``\n    :param orange_color: hex color code to plot the data in the\n        ``df[orange]``  (default is ``ae_consts.PLOT_COLORS[\'orange\']``)\n    :param orange_label: optional - string for the label used\n        to identify the ``orange`` line in the legend\n    :param date_col: string - date column name\n        (default is ``minute``)\n    :param xlabel: x-axis label\n    :param ylabel: y-axis label\n    :param linestyle: style of the plot line\n    :param width: float - width of the image\n    :param height: float - height of the image\n    :param date_format: string - format for dates\n    :param df_filter: optional - initialized ``pandas.DataFrame`` query\n        for reducing the ``df`` records before plotting. As an eaxmple\n        ``df_filter=(df[\'close\'] > 0.01)`` would find only records in\n        the ``df`` with a ``close`` value greater than ``0.01``\n    :param start_date: optional - string ``datetime``\n        for plotting only from a date formatted as\n        ``YYYY-MM-DD HH\\\\:MM\\\\:SS``\n    :param footnote_text: optional - string footnote text\n        (default is ``algotraders <DATE>``)\n    :param footnote_xpos: optional - float for footnote position\n        on the x-axies\n        (default is ``0.75``)\n    :param footnote_ypos: optional - float for footnote position\n        on the y-axies\n        (default is ``0.01``)\n    :param footnote_color: optional - string hex color code for\n        the footnote text\n        (default is ``#888888``)\n    :param footnote_fontsize: optional - float footnote font size\n        (default is ``8``)\n    :param scale_y: optional - bool to scale the y-axis with\n        .. code-block:: python\n\n            use_ax.set_ylim(\n                [0, use_ax.get_ylim()[1] * 3])\n    :param show_plot: bool to show the plot\n    :param dropna_for_all: optional - bool to toggle keep None\'s in\n        the plot ``df`` (default is drop them for display purposes)\n    :param verbose: optional - bool to show logs for debugging\n        a dataset\n    :param send_plots_to_slack: optional - bool to send the dnn plot to slack\n    """"""\n\n    rec = {\n        \'ax1\': None,\n        \'ax2\': None,\n        \'ax3\': None,\n        \'ax4\': None,\n        \'fig\': None\n    }\n    result = build_result.build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec=rec)\n\n    if verbose:\n        log.info(\'plot_trading_history - start\')\n\n    use_red = red_color\n    use_blue = blue_color\n    use_green = green_color\n    use_orange = orange_color\n\n    if not use_red:\n        use_red = ae_consts.PLOT_COLORS[\'red\']\n    if not use_blue:\n        use_blue = ae_consts.PLOT_COLORS[\'blue\']\n    if not use_green:\n        use_green = ae_consts.PLOT_COLORS[\'darkgreen\']\n    if not use_orange:\n        use_orange = ae_consts.PLOT_COLORS[\'orange\']\n\n    use_footnote = footnote_text\n    if not use_footnote:\n        ft_date = (\n            datetime.datetime.now().strftime(\n                ae_consts.COMMON_TICK_DATE_FORMAT))\n        use_footnote = (\n            f\'algotraders - {ft_date}\')\n\n    if date_col not in df:\n        log.error(\n            f\'failed to find date_col={date_col} \'\n            f\'in df={df.columns.values}\')\n        result = build_result.build_result(\n            status=ae_consts.ERR,\n            err=None,\n            rec=rec)\n\n    column_list = [\n        date_col\n    ]\n\n    all_plots = []\n    if red:\n        column_list.append(red)\n        all_plots.append({\n            \'column\': red,\n            \'color\': use_red})\n    else:\n        if \'high\' in df:\n            red = \'high\'\n            column_list.append(red)\n            all_plots.append({\n                \'column\': \'high\',\n                \'color\': use_red})\n    if blue:\n        column_list.append(blue)\n        all_plots.append({\n            \'column\': blue,\n            \'color\': use_blue})\n    else:\n        if \'close\' in df:\n            blue = \'close\'\n            column_list.append(blue)\n            all_plots.append({\n                \'column\': \'close\',\n                \'color\': use_blue})\n    if green:\n        column_list.append(green)\n        all_plots.append({\n            \'column\': green,\n            \'color\': use_green})\n    if orange:\n        column_list.append(orange)\n        all_plots.append({\n            \'column\': orange,\n            \'color\': use_orange})\n\n    use_df = df\n    if start_date:\n        start_date_value = datetime.datetime.strptime(\n            start_date,\n            ae_consts.COMMON_TICK_DATE_FORMAT)\n        use_df = df[(df[date_col] >= start_date_value)][column_list]\n    # end of filtering by start date\n\n    if verbose:\n        log.info(\n            f\'plot_history_df start_date={start_date} \'\n            f\'df.index={len(use_df.index)} column_list={column_list}\')\n\n    if hasattr(df_filter, \'to_json\'):\n        # Was seeing this warning below in Jupyter:\n        # UserWarning: Boolean Series key\n        # will be reindexed to match DataFrame index\n        # use_df = use_df[df_filter][column_list]\n        # now using:\n        use_df = use_df.loc[df_filter, column_list]\n\n    log.info(\n            f\'plot_history_df start_date={start_date} \'\n            f\'df.index={len(use_df.index)} column_list={column_list}\')\n\n    if dropna_for_all:\n        use_df = use_df.dropna(axis=0, how=\'any\')\n        if verbose:\n            log.info(\'plot_history_df dropna_for_all\')\n    # end of pre-plot dataframe scrubbing\n\n    ae_charts.set_common_seaborn_fonts()\n\n    hex_color = ae_consts.PLOT_COLORS[\'blue\']\n    fig, ax = plt.subplots(\n        sharex=True,\n        sharey=True,\n        figsize=(\n            width,\n            height))\n\n    # Convert matplotlib date numbers to strings for dates to\n    # avoid dealing with weekend date gaps in plots\n    date_strings, date_labels = \\\n        ae_utils.get_trade_open_xticks_from_date_col(\n            use_df[date_col])\n\n    """"""\n    hit the slice warning with this approach before\n    and one trying df[date_col] = df[date_col].dt.strftime\n\n    SettingWithCopyWarning\n    Try using .loc[row_indexer,col_indexer] = value instead\n\n    use_df[date_col].replace(\n        use_df[date_col].dt.strftime(\n            ae_consts.COMMON_TICK_DATE_FORMAT),\n        inplace=True)\n    trying this:\n    https://stackoverflow.com/questions/19738169/\n    convert-column-of-date-objects-in-pandas-dataframe-to-strings\n    """"""\n    use_df[date_col] = use_df[date_col].apply(lambda x: x.strftime(\n        ae_consts.COMMON_TICK_DATE_FORMAT))\n\n    all_axes = []\n    num_plots = len(all_plots)\n    first_ax = None\n    for idx, node in enumerate(all_plots):\n        column_name = node[\'column\']\n        hex_color = node[\'color\']\n\n        use_ax = ax\n        if idx > 0:\n            use_ax = ax.twinx()\n        else:\n            first_ax = ax\n\n        if verbose:\n            log.info(\n                f\'plot_history_df - \'\n                f\'{idx + 1}/{num_plots} - \'\n                f\'{column_name} in {hex_color} - \'\n                f\'ax={use_ax}\')\n\n        all_axes.append(use_ax)\n        use_ax.plot(\n            use_df[date_col],\n            use_df[column_name],\n            linestyle=linestyle,\n            color=hex_color)\n        if idx > 0:\n            if scale_y:\n                use_ax.set_ylim(\n                    [0, use_ax.get_ylim()[1] * 3])\n            use_ax.yaxis.set_ticklabels([])\n            use_ax.yaxis.set_ticks([])\n            use_ax.xaxis.grid(False)\n            use_ax.yaxis.grid(False)\n        # end if this is not the fist axis\n    # end of for all plots\n\n    first_ax.set_xticks(date_strings)\n    first_ax.set_xticklabels(date_labels, rotation=45, ha=\'right\')\n\n    lines = []\n    for idx, cur_ax in enumerate(all_axes):\n        ax_lines = cur_ax.get_lines()\n        for line in ax_lines:\n            label_name = str(line.get_label())\n            use_label = label_name\n            if idx == 0:\n                if red_label:\n                    use_label = red_label\n            elif idx == 1:\n                if blue_label:\n                    use_label = blue_label\n            elif idx == 2:\n                use_label = label_name[-20:]\n                if green_label:\n                    use_label = green_label\n            elif idx == 3:\n                use_label = label_name[-20:]\n                if orange_label:\n                    use_label = orange_label\n            else:\n                if len(label_name) > 10:\n                    use_label = label_name[-20:]\n            # end of fixing the labels in the legend\n            line.set_label(use_label)\n            if line.get_label() not in lines:\n                lines.append(line)\n        rec[f\'ax{idx + 1}\'] = cur_ax\n    # end of compiling a new-shortened legend while removing dupes\n\n    for idx, cur_ax in enumerate(all_axes):\n        if cur_ax:\n            if cur_ax.get_legend():\n                cur_ax.get_legend().remove()\n    # end of removing all previous legends\n\n    if verbose:\n        log.info(\n            f\'legend lines={[l.get_label() for l in lines]}\')\n    # log what\'s going to be in the legend\n\n    ax.legend(\n        lines,\n        [l.get_label() for l in lines],\n        loc=\'best\',\n        shadow=True)\n\n    fig.autofmt_xdate()\n\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n\n    ax.set_title(title)\n    ae_charts.add_footnote(\n        fig=fig,\n        xpos=footnote_xpos,\n        ypos=footnote_ypos,\n        text=use_footnote,\n        color=footnote_color,\n        fontsize=footnote_fontsize)\n    plt.tight_layout()\n\n    if send_plots_to_slack:\n        slack_utils.post_plot(plt, title=title)\n\n    if show_plot:\n        plt.show()\n    else:\n        plt.plot()\n\n    rec[\'fig\'] = fig\n\n    result = build_result.build_result(\n        status=ae_consts.SUCCESS,\n        err=None,\n        rec=rec)\n\n    return result\n# end of plot_history_df\n'"
analysis_engine/prepare_dict_for_algo.py,0,"b'""""""\nHelper for converting a dictionary to an algorithm-ready\ndataset\n""""""\n\nimport json\nimport zlib\nimport pandas as pd\nimport analysis_engine.consts as ae_consts\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef prepare_dict_for_algo(\n        data,\n        compress=False,\n        encoding=\'utf-8\',\n        convert_to_dict=False,\n        dataset_names=None):\n    """"""prepare_dict_for_algo\n\n    :param data: string holding contents of an algorithm-ready\n        file, s3 key or redis-key\n    :param compress: optional - boolean flag for decompressing\n        the contents of the ``data`` if necessary\n        (default is ``False`` and algorithms\n        use ``zlib`` for compression)\n    :param convert_to_dict: optional - bool for s3 use ``False``\n        and for files use ``True``\n    :param encoding: optional - string for data encoding\n    :param dataset_names: optional - list of string keys\n        for each dataset node in:\n        ``dataset[ticker][0][\'data\'][dataset_names[0]]``\n    """"""\n    log.debug(\'start\')\n    use_data = None\n    parsed_data = None\n    data_as_dict = None\n\n    if compress:\n        log.debug(\'decompressing\')\n        parsed_data = zlib.decompress(\n            data).decode(\n                encoding)\n    else:\n        parsed_data = data\n\n    if not parsed_data:\n        log.error(\'failed parsing\')\n        return None\n\n    log.debug(\'loading as dict\')\n    use_data = {}\n    if convert_to_dict:\n        data_as_dict = json.loads(parsed_data)\n    else:\n        data_as_dict = parsed_data\n    if len(data_as_dict) == 0:\n        log.error(\n            \'empty algorithm-ready dictionary\')\n        return use_data\n\n    empty_pd = pd.DataFrame([{}])\n\n    use_serialized_datasets = dataset_names\n    if not use_serialized_datasets:\n        use_serialized_datasets = ae_consts.DEFAULT_SERIALIZED_DATASETS\n    log.info(f\'converting serialized_datasets={use_serialized_datasets}\')\n    num_datasets = 0\n    for ticker in data_as_dict:\n        if ticker not in use_data:\n            use_data[ticker] = []\n        for node in data_as_dict[ticker]:\n            new_node = {\n                \'id\': node[\'id\'],\n                \'date\': node[\'date\'],\n                \'data\': {}\n            }\n            for ds_key in node[\'data\']:\n                if ds_key in use_serialized_datasets:\n                    new_node[\'data\'][ds_key] = empty_pd\n                    if node[\'data\'][ds_key]:\n                        new_node[\'data\'][ds_key] = pd.read_json(\n                            node[\'data\'][ds_key],\n                            orient=\'records\')\n                        num_datasets += 1\n                # if supported dataset key\n            # end for all datasets in this node\n            use_data[ticker].append(new_node)\n        # end for all datasets on this date to load\n    # end for all tickers in the dataset\n\n    if num_datasets:\n        log.info(f\'found datasets={num_datasets}\')\n    else:\n        log.error(f\'did not find any datasets={num_datasets}\')\n\n    return use_data\n# end of prepare_dict_for_algo\n'"
analysis_engine/prepare_history_dataset.py,0,"b'""""""\nHelper for loading a ``Trading History`` dataset\n""""""\n\nimport json\nimport zlib\nimport pandas as pd\nimport analysis_engine.consts as ae_consts\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef prepare_history_dataset(\n        data,\n        compress=False,\n        encoding=\'utf-8\',\n        convert_to_dict=False,\n        include_keys=None,\n        ignore_keys=None,\n        convert_to_dates=None,\n        verbose=False):\n    """"""prepare_history_dataset\n\n    Load a ``Trading History`` dataset into a dictionary\n    with a ``pd.DataFrame`` for the trading history record\n    list\n\n    :param data: string holding contents of a ``Trading History``\n        from a file, s3 key or redis-key\n    :param compress: optional - boolean flag for decompressing\n        the contents of the ``data`` if necessary\n        (default is ``False`` and algorithms\n        use ``zlib`` for compression)\n    :param convert_to_dict: optional - bool for s3 use ``False``\n        and for files use ``True``\n    :param encoding: optional - string for data encoding\n    :param include_keys: optional - list of string keys\n        to include before from the dataset\n        .. note:: tickers are automatically included in the ``pd.DataFrame``\n    :param ignore_keys: optional - list of string keys\n        to remove before building the ``pd.DataFrame``\n    :param convert_to_dates: optional - list of string keys\n        to convert to datetime before building the ``pd.DataFrame``\n    :param verbose: optional - bool show the logs\n        (default is ``False``)\n    """"""\n    if verbose:\n        log.debug(\'start\')\n    use_data = None\n    parsed_data = None\n    data_as_dict = None\n\n    if compress:\n        if verbose:\n            log.debug(\'decompressing\')\n        parsed_data = zlib.decompress(\n            data).decode(\n                encoding)\n    else:\n        parsed_data = data\n\n    if not parsed_data:\n        log.error(\'failed parsing\')\n        return None\n\n    if verbose:\n        log.debug(\'loading as dict\')\n    use_data = {}\n    if convert_to_dict:\n        try:\n            data_as_dict = json.loads(parsed_data)\n        except Exception as e:\n            if (\n                    \'the JSON object must be str, bytes or \'\n                    \'bytearray, not\') in str(e):\n                log.critical(\n                    f\'failed decoding json for string - double \'\n                    f\'compression for history dataset found ex={e}\')\n            data_as_dict = parsed_data\n    else:\n        data_as_dict = parsed_data\n    if len(data_as_dict) == 0:\n        log.error(\n            \'empty trading history dictionary\')\n        return use_data\n\n    convert_these_date_keys = [\n        \'date\',\n        \'minute\',\n        \'exp_date\'\n    ]\n\n    use_include_keys = [\n        \'tickers\',\n        \'version\',\n        \'last_trade_data\',\n        \'algo_config_dict\',\n        \'algo_name\',\n        \'created\'\n    ]\n    if include_keys:\n        use_include_keys = include_keys\n\n    use_ignore_keys = []\n    if ignore_keys:\n        use_ignore_keys = ignore_keys\n\n    for k in data_as_dict:\n        if k in use_include_keys:\n            use_data[k] = data_as_dict[k]\n\n    all_records = []\n    num_records = 0\n    for ticker in data_as_dict[\'tickers\']:\n        if ticker not in use_data:\n            use_data[ticker] = []\n        for node in data_as_dict[ticker]:\n\n            for ignore in use_ignore_keys:\n                node.pop(ignore, None)\n\n            all_records.append(node)\n        # end for all datasets on this date to load\n\n        num_records = len(all_records)\n\n        if num_records:\n            if verbose:\n                log.info(f\'found records={num_records}\')\n            history_df = pd.DataFrame(all_records)\n            for dc in convert_these_date_keys:\n                if dc in history_df:\n                    history_df[dc] = pd.to_datetime(\n                        history_df[dc],\n                        format=ae_consts.COMMON_TICK_DATE_FORMAT)\n            # end of converting all date columns\n            use_data[ticker] = history_df\n        else:\n            log.error(\n                f\'did not find any records={num_records} in history dataset\')\n    # end for all tickers in the dataset\n\n    return use_data\n# end of prepare_history_dataset\n'"
analysis_engine/prepare_report_dataset.py,0,"b'""""""\nHelper for loading a ``Trading Performance Report`` dataset\n""""""\n\nimport json\nimport zlib\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef prepare_report_dataset(\n        data,\n        compress=False,\n        encoding=\'utf-8\',\n        convert_to_dict=False,\n        dataset_names=None,\n        verbose=False):\n    """"""prepare_report_dataset\n\n    Load a ``Trading Performance Report`` dataset into a dictionary\n    with a ``pd.DataFrame`` for the trading report record\n    list\n\n    :param data: string holding contents of an ``Trading\n        Performance Report`` from a file, s3 key or redis-key\n    :param compress: optional - boolean flag for decompressing\n        the contents of the ``data`` if necessary\n        (default is ``False`` and algorithms\n        use ``zlib`` for compression)\n    :param convert_to_dict: optional - bool for s3 use ``False``\n        and for files use ``True``\n    :param encoding: optional - string for data encoding\n    :param dataset_names: optional - list of string keys\n        for each dataset node in:\n        ``dataset[ticker][0][\'data\'][dataset_names[0]]``\n    :param verbose: optional - bool show the logs\n        (default is ``False``)\n    """"""\n    if verbose:\n        log.debug(\'start\')\n    use_data = None\n    parsed_data = None\n    data_as_dict = None\n\n    if compress:\n        if verbose:\n            log.debug(\'decompressing\')\n        parsed_data = zlib.decompress(\n            data).decode(\n                encoding)\n    else:\n        parsed_data = data\n\n    if not parsed_data:\n        log.error(\'failed parsing\')\n        return None\n\n    if verbose:\n        log.debug(\'loading as dict\')\n    use_data = {}\n    if convert_to_dict:\n        data_as_dict = json.loads(parsed_data)\n    else:\n        data_as_dict = parsed_data\n    if len(data_as_dict) == 0:\n        log.error(\n            \'empty trading performance report dictionary\')\n        return use_data\n\n    for ticker in data_as_dict[\'tickers\']:\n        if ticker not in use_data:\n            use_data[ticker] = []\n        for node in data_as_dict[ticker]:\n            new_node = {\n                \'id\': node[\'id\'],\n                \'date\': node[\'date\'],\n                \'data\': {}\n            }\n            """"""\n            for ds_key in node[\'data\']:\n                if ds_key in use_serialized_datasets:\n                    new_node[\'data\'][ds_key] = empty_pd\n                    if node[\'data\'][ds_key]:\n                        new_node[\'data\'][ds_key] = pd.read_json(\n                            node[\'data\'][ds_key],\n                            orient=\'records\')\n                        num_datasets += 1\n                # if supported dataset key\n            # end for all datasets in this node\n            """"""\n            use_data[ticker].append(new_node)\n        # end for all datasets on this date to load\n    # end for all tickers in the dataset\n\n    """"""\n    if num_datasets:\n        if verbose:\n            log.info(f\'found datasets={num_datasets}\')\n    else:\n        log.error(f\'did not find any datasets={num_datasets}\')\n    """"""\n\n    return use_data\n# end of prepare_report_dataset\n'"
analysis_engine/publish.py,0,"b'""""""\nDataset Publishing API\n""""""\n\nimport json\nimport boto3\nimport redis\nimport zlib\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.compress_data as compress_data\nimport analysis_engine.set_data_in_redis_key as redis_utils\nimport analysis_engine.send_to_slack as slack_utils\nimport analysis_engine.write_to_file as file_utils\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef publish(\n        data,\n        label=None,\n        convert_to_json=False,\n        is_df=False,\n        output_file=None,\n        df_compress=False,\n        compress=False,\n        redis_enabled=True,\n        redis_key=None,\n        redis_address=None,\n        redis_db=None,\n        redis_password=None,\n        redis_expire=None,\n        redis_serializer=\'json\',\n        redis_encoding=\'utf-8\',\n        s3_enabled=True,\n        s3_key=None,\n        s3_address=None,\n        s3_bucket=None,\n        s3_access_key=None,\n        s3_secret_key=None,\n        s3_region_name=None,\n        s3_secure=False,\n        slack_enabled=False,\n        slack_code_block=False,\n        slack_full_width=False,\n        verbose=False,\n        silent=False,\n        **kwargs):\n    """"""publish\n\n    Publish ``data`` to multiple optional endpoints:\n    - a local file path (``output_file``)\n    - minio (``s3_bucket`` and ``s3_key``)\n    - redis (``redis_key``)\n    - slack\n\n    :return: status value\n    :param data: data to publish\n    :param convert_to_json: convert ``data`` to a\n        json-serialized string. this function will throw if\n        ``json.dumps(data)`` fails\n    :param is_df: convert ``pd.DataFrame`` using\n        ``pd.DataFrame.to_json()`` to a\n        json-serialized string. this function will throw if\n        ``to_json()`` fails\n    :param label: log tracking label\n    :param output_file: path to save the data\n        to a file\n    :param df_compress: optional - compress data that is a\n        ``pandas.DataFrame`` before publishing\n    :param compress: optional - compress before publishing\n        (default is ``False``)\n    :param verbose: optional - boolean to log output\n        (default is ``False``)\n    :param silent: optional - boolean no log output\n        (default is ``False``)\n    :param kwargs: optional - future argument support\n\n    **(Optional) Redis connectivity arguments**\n\n    :param redis_enabled: bool - toggle for auto-caching all\n        datasets in Redis\n        (default is ``True``)\n    :param redis_key: string - key to save the data in redis\n        (default is ``None``)\n    :param redis_address: Redis connection string format: ``host:port``\n        (default is ``localhost:6379``)\n    :param redis_db: Redis db to use\n        (default is ``0``)\n    :param redis_password: optional - Redis password\n        (default is ``None``)\n    :param redis_expire: optional - Redis expire value\n        (default is ``None``)\n    :param redis_serializer: not used yet - support for future\n        pickle objects in redis\n    :param redis_encoding: format of the encoded key in redis\n\n    **(Optional) Minio (S3) connectivity arguments**\n\n    :param s3_enabled: bool - toggle for auto-archiving on Minio (S3)\n        (default is ``True``)\n    :param s3_key: string - key to save the data in redis\n        (default is ``None``)\n    :param s3_address: Minio S3 connection string format: ``host:port``\n        (default is ``localhost:9000``)\n    :param s3_bucket: S3 Bucket for storing the artifacts\n        (default is ``dev``) which should be viewable on a browser:\n        http://localhost:9000/minio/dev/\n    :param s3_access_key: S3 Access key\n        (default is ``trexaccesskey``)\n    :param s3_secret_key: S3 Secret key\n        (default is ``trex123321``)\n    :param s3_region_name: S3 region name\n        (default is ``us-east-1``)\n    :param s3_secure: Transmit using tls encryption\n        (default is ``False``)\n\n    **(Optional) Slack arguments**\n\n    :param slack_enabled: optional - boolean for\n        publishing to slack\n    :param slack_code_block: optional - boolean for\n        publishing as a code black in slack\n    :param slack_full_width: optional - boolean for\n        publishing as a to slack using the full\n        width allowed\n    """"""\n\n    status = ae_consts.NOT_RUN\n    use_data = data\n    if (\n            not df_compress and\n            not is_df and\n            not use_data):\n        log.info(\'missing data\')\n        return ae_consts.INVALID\n\n    if convert_to_json and not is_df:\n        if verbose:\n            log.debug(\'start convert to json\')\n        use_data = json.dumps(data)\n        if verbose:\n            log.debug(\'done convert to json\')\n    if is_df:\n        if verbose:\n            log.debug(\'start df to_json\')\n        use_data = data.to_json(\n            orient=\'records\',\n            date_format=\'iso\')\n        if verbose:\n            log.debug(\'done df to_json\')\n\n    already_compressed = False\n    if df_compress:\n        use_data = compress_data.compress_data(\n            data=data)\n        already_compressed = True\n    elif compress and not df_compress:\n        if verbose:\n            log.debug(\'compress start\')\n        use_data = zlib.compress(\n            use_data.encode(\n                redis_encoding), 9)\n        already_compressed = True\n        if verbose:\n            log.debug(\'compress end\')\n\n    num_bytes = len(use_data)\n    num_mb = ae_consts.get_mb(num_bytes)\n\n    if verbose:\n        log.debug(\n            f\'start - file={output_file} s3_key={s3_key} \'\n            f\'redis_key={redis_key} slack={slack_enabled} \'\n            f\'compress={compress} size={num_mb}MB\')\n\n    if s3_enabled and s3_address and s3_bucket and s3_key:\n        endpoint_url = f\'http{""s"" if s3_secure else """"}://{s3_address}\'\n\n        if verbose:\n            log.debug(\n                f\'s3 start - {label} endpoint_url={endpoint_url} \'\n                f\'region={s3_region_name}\')\n\n        s3 = boto3.resource(\n            \'s3\',\n            endpoint_url=endpoint_url,\n            aws_access_key_id=s3_access_key,\n            aws_secret_access_key=s3_secret_key,\n            region_name=s3_region_name,\n            config=boto3.session.Config(\n                signature_version=\'s3v4\')\n        )\n\n        if s3.Bucket(s3_bucket) not in s3.buckets.all():\n            if verbose:\n                log.debug(f\'s3 creating bucket={s3_bucket} {label}\')\n            s3.create_bucket(\n                Bucket=s3_bucket)\n\n        if verbose:\n            log.debug(\n                f\'s3 upload start - bytes={num_mb} to \'\n                f\'{s3_bucket}:{s3_key} {label}\')\n\n        s3.Bucket(\n            s3_bucket).put_object(\n                Key=s3_key,\n                Body=use_data)\n\n        if verbose:\n            log.debug(\n                f\'s3 upload done - bytes={num_mb} to \'\n                f\'{s3_bucket}:{s3_key} {label}\')\n\n    # end of s3_enabled\n\n    if redis_enabled and redis_address and redis_key:\n        redis_split = redis_address.split(\':\')\n        redis_host = redis_split[0]\n        redis_port = int(redis_split[1])\n        log.debug(\n            f\'{label if label else """"} \'\n            f\'redis={redis_host}:{redis_port}@{redis_db} connect \'\n            f\'key={redis_key} expire={redis_expire}\')\n\n        rc = redis.Redis(\n            host=redis_host,\n            port=redis_port,\n            password=redis_password,\n            db=redis_db)\n\n        redis_res = redis_utils.set_data_in_redis_key(\n            label=label,\n            client=rc,\n            key=redis_key,\n            data=use_data,\n            already_compressed=already_compressed,\n            serializer=redis_serializer,\n            encoding=redis_encoding,\n            expire=redis_expire,\n            px=None,\n            nx=False,\n            xx=False)\n\n        if redis_res[\'status\'] != ae_consts.SUCCESS:\n            log.error(\n                f\'redis failed - \'\n                f\'{ae_consts.get_status(status=redis_res[""status""])} \'\n                f\'{redis_res[""err""]}\')\n            return ae_consts.REDIS_FAILED\n    # end of redis_enabled\n\n    if output_file:\n        if verbose:\n            log.debug(f\'file start - output_file={output_file}\')\n        file_exists = file_utils.write_to_file(\n            output_file=output_file,\n            data=data)\n        if not file_exists:\n            log.error(\n                f\'file failed - did not find \'\n                f\'output_file={output_file}\')\n            return ae_consts.FILE_FAILED\n        if verbose:\n            log.debug(f\'file done - output_file={output_file}\')\n    # end of writing to file\n\n    if slack_enabled:\n        if verbose:\n            log.debug(\'slack start\')\n        slack_utils.post_success(\n            msg=use_data,\n            block=slack_code_block,\n            full_width=slack_full_width)\n        if verbose:\n            log.debug(\'slack end\')\n    # end of sending to slack\n\n    status = ae_consts.SUCCESS\n\n    if verbose:\n        log.debug(\n            f\'end - {ae_consts.get_status(status=status)} file={output_file} \'\n            f\'s3_key={s3_key} redis_key={redis_key} slack={slack_enabled} \'\n            f\'compress={compress} size={num_mb}MB\')\n\n    return status\n# end of publish\n'"
analysis_engine/restore_dataset.py,0,"b'""""""\nRestore an algorithm dataset from file, s3 or redis to redis\nfor ensuring all datasets are ready for Algorithmic backtesting\n\nSupported Datasets:\n\n- ``SA_DATASET_TYPE_ALGO_READY`` - Algorithm-ready datasets\n""""""\n\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.load_dataset as load_dataset\nimport analysis_engine.show_dataset as show_dataset\nimport analysis_engine.get_data_from_redis_key as redis_utils\nimport analysis_engine.publish as publish\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef restore_dataset(\n        show_summary=True,\n        force_restore=False,\n        algo_dataset=None,\n        dataset_type=ae_consts.SA_DATASET_TYPE_ALGO_READY,\n        serialize_datasets=ae_consts.DEFAULT_SERIALIZED_DATASETS,\n        path_to_file=None,\n        compress=False,\n        encoding=\'utf-8\',\n        redis_enabled=True,\n        redis_key=None,\n        redis_address=None,\n        redis_db=None,\n        redis_password=None,\n        redis_expire=None,\n        redis_serializer=\'json\',\n        redis_encoding=\'utf-8\',\n        redis_output_db=None,\n        s3_enabled=True,\n        s3_key=None,\n        s3_address=None,\n        s3_bucket=None,\n        s3_access_key=None,\n        s3_secret_key=None,\n        s3_region_name=None,\n        s3_secure=False,\n        slack_enabled=False,\n        slack_code_block=False,\n        slack_full_width=False,\n        datasets_compressed=True,\n        verbose=False):\n    """"""restore_dataset\n\n    Restore missing dataset nodes in redis from an algorithm-ready\n    dataset file on disk. Use this to restore redis from scratch.\n\n    :param show_summary: optional - show a summary of the algorithm-ready\n        dataset using ``analysis_engine.show_dataset.show_dataset``\n        (default is ``True``)\n    :param force_restore: optional - boolean - publish whatever is in\n        the algorithm-ready dataset into redis. If ``False`` this will\n        ensure that datasets are only set in redis if they are not already\n        set\n    :param algo_dataset: optional - already loaded algorithm-ready dataset\n    :param dataset_type: optional - dataset type\n        (default is ``SA_DATASET_TYPE_ALGO_READY``)\n    :param serialize_datasets: optional - list of dataset names to\n        deserialize in the dataset\n    :param path_to_file: optional - path to an algorithm-ready dataset\n        in a file\n    :param compress: optional - boolean flag for decompressing\n        the contents of the ``path_to_file`` if necessary\n        (default is ``False`` and algorithms\n        use ``zlib`` for compression)\n    :param encoding: optional - string for data encoding\n\n    **(Optional) Redis connectivity arguments**\n\n    :param redis_enabled: bool - toggle for auto-caching all\n        datasets in Redis\n        (default is ``True``)\n    :param redis_key: string - key to save the data in redis\n        (default is ``None``)\n    :param redis_address: Redis connection string format: ``host:port``\n        (default is ``localhost:6379``)\n    :param redis_db: Redis db to use\n        (default is ``0``)\n    :param redis_password: optional - Redis password\n        (default is ``None``)\n    :param redis_expire: optional - Redis expire value\n        (default is ``None``)\n    :param redis_serializer: not used yet - support for future\n        pickle objects in redis\n    :param redis_encoding: format of the encoded key in redis\n    :param redis_output_db: optional - integer publish to a separate\n        redis database\n\n    **(Optional) Minio (S3) connectivity arguments**\n\n    :param s3_enabled: bool - toggle for auto-archiving on Minio (S3)\n        (default is ``True``)\n    :param s3_key: string - key to save the data in redis\n        (default is ``None``)\n    :param s3_address: Minio S3 connection string format: ``host:port``\n        (default is ``localhost:9000``)\n    :param s3_bucket: S3 Bucket for storing the artifacts\n        (default is ``dev``) which should be viewable on a browser:\n        http://localhost:9000/minio/dev/\n    :param s3_access_key: S3 Access key\n        (default is ``trexaccesskey``)\n    :param s3_secret_key: S3 Secret key\n        (default is ``trex123321``)\n    :param s3_region_name: S3 region name\n        (default is ``us-east-1``)\n    :param s3_secure: Transmit using tls encryption\n        (default is ``False``)\n\n    **(Optional) Slack arguments**\n\n    :param slack_enabled: optional - boolean for\n        publishing to slack\n    :param slack_code_block: optional - boolean for\n        publishing as a code black in slack\n    :param slack_full_width: optional - boolean for\n        publishing as a to slack using the full\n        width allowed\n\n    Additonal arguments\n\n    :param datasets_compressed: optional - boolean for\n        publishing as compressed strings\n        default is ``True``\n\n    :param verbose: optional - bool for increasing\n        logging\n    """"""\n\n    use_ds = algo_dataset\n    redis_host = ae_consts.REDIS_ADDRESS.split(\':\')[0]\n    redis_port = int(ae_consts.REDIS_ADDRESS.split(\':\')[1])\n    if redis_address:\n        redis_host = redis_address.split(\':\')[0]\n        redis_port = int(redis_address.split(\':\')[1])\n\n    if show_summary:\n        use_ds = show_dataset.show_dataset(\n            dataset_type=dataset_type,\n            compress=compress,\n            encoding=redis_encoding,\n            path_to_file=path_to_file,\n            s3_key=s3_key,\n            s3_address=s3_address,\n            s3_bucket=s3_bucket,\n            s3_access_key=s3_access_key,\n            s3_secret_key=s3_secret_key,\n            s3_region_name=s3_region_name,\n            s3_secure=s3_secure,\n            redis_key=redis_key,\n            redis_address=redis_address,\n            redis_db=redis_db,\n            redis_password=redis_password,\n            redis_expire=redis_expire,\n            redis_serializer=redis_serializer,\n            serialize_datasets=serialize_datasets)\n\n    # end of if show_summary\n\n    if not use_ds:\n        log.info(\n            f\'loading from file={path_to_file} s3={s3_key} redis={redis_key}\')\n        use_ds = load_dataset.load_dataset(\n            dataset_type=dataset_type,\n            compress=compress,\n            encoding=redis_encoding,\n            path_to_file=path_to_file,\n            s3_key=s3_key,\n            s3_address=s3_address,\n            s3_bucket=s3_bucket,\n            s3_access_key=s3_access_key,\n            s3_secret_key=s3_secret_key,\n            s3_region_name=s3_region_name,\n            s3_secure=s3_secure,\n            redis_key=redis_key,\n            redis_address=redis_address,\n            redis_db=redis_db,\n            redis_password=redis_password,\n            redis_expire=redis_expire,\n            redis_serializer=redis_serializer,\n            serialize_datasets=serialize_datasets)\n    # load if not loaded\n\n    if not use_ds:\n        log.error(\n            f\'unable to load a dataset from file={path_to_file} \'\n            f\'s3={s3_key} redis={redis_key}\')\n        return None\n\n    log.info(\'restore - start\')\n    total_to_restore = 0\n    for ticker in use_ds:\n        for ds_node in use_ds[ticker]:\n            for ds_key in ds_node[\'data\']:\n                if ds_key in serialize_datasets:\n                    total_to_restore += 1\n    # end of counting total_to_restore\n\n    log.info(f\'restore - records={total_to_restore}\')\n    num_done = 0\n    for ticker in use_ds:\n        for ds_node in use_ds[ticker]:\n            ds_parent_key = ds_node[\'id\']\n            log.info(\n                f\'restore - parent_key={ds_parent_key} - \'\n                f\'\'\'{ae_consts.get_percent_done(\n                    progress=num_done,\n                    total=total_to_restore)} {num_done}/{total_to_restore}\'\'\')\n            if verbose:\n                print(ds_parent_key)\n\n            cache_res = redis_utils.get_data_from_redis_key(\n                host=redis_host,\n                port=redis_port,\n                password=redis_password,\n                db=redis_db,\n                key=ds_parent_key,\n                decompress_df=datasets_compressed,\n                serializer=redis_serializer,\n                encoding=redis_encoding,\n                expire=redis_expire,\n                label=f\'restore-{ds_parent_key}\')\n\n            should_restore = False\n            if (not force_restore and\n                    cache_res[\'status\'] == ae_consts.SUCCESS and\n                    \'data\' in cache_res[\'rec\'] and\n                    cache_res[\'rec\'][\'data\'] and\n                    len(cache_res[\'rec\'][\'data\']) > 10):\n                should_restore = False\n            else:\n                should_restore = True\n            if should_restore:\n                log.info(\n                    f\' - parent {ds_parent_key} restore\')\n                new_parent_rec = {\n                    \'exp_date\': None,\n                    \'publish_pricing_update\': None,\n                    \'date\': ds_node[\'date\'],\n                    \'updated\': None,\n                    \'version\': ae_consts.DATASET_COLLECTION_VERSION\n                }\n                for sname in serialize_datasets:\n                    if sname in ds_node[\'data\']:\n                        if hasattr(\n                                ds_node[\'data\'][sname],\n                                \'index\'):\n                            new_parent_rec[sname] = \\\n                                ds_node[\'data\'][sname].to_json(\n                                    orient=\'records\',\n                                    date_format=\'iso\')\n                        else:\n                            new_parent_rec[sname] = \\\n                                ds_node[\'data\'][sname]\n\n                publish.publish(\n                    data=new_parent_rec,\n                    redis_enabled=True,\n                    redis_key=ds_parent_key,\n                    redis_db=redis_output_db,\n                    redis_address=redis_address,\n                    redis_password=redis_password,\n                    redis_expire=redis_expire,\n                    redis_serializer=redis_serializer,\n                    redis_encoding=redis_encoding,\n                    s3_enabled=False,\n                    output_file=None,\n                    df_compress=datasets_compressed,\n                    verbose=verbose)\n\n            for ds_key in ds_node[\'data\']:\n                if ds_key in serialize_datasets:\n                    new_key = f\'{ds_parent_key}_{ds_key}\'\n                    if hasattr(\n                            ds_node[\'data\'][ds_key],\n                            \'index\'):\n                        loaded_df = ds_node[\'data\'][ds_key]\n                        if len(loaded_df.index) > 0:\n                            if verbose:\n                                print(f\' - checking: {new_key}\')\n\n                            cache_res = redis_utils.get_data_from_redis_key(\n                                host=redis_host,\n                                port=redis_port,\n                                password=redis_password,\n                                db=redis_db,\n                                key=new_key,\n                                decompress_df=datasets_compressed,\n                                serializer=redis_serializer,\n                                encoding=redis_encoding,\n                                expire=redis_expire,\n                                label=f\'restore-{new_key}\')\n\n                            should_restore = False\n                            success_status = (\n                                cache_res[\'status\'] == ae_consts.SUCCESS)\n                            if (not force_restore and\n                                    success_status and\n                                    \'data\' in cache_res[\'rec\'] and\n                                    cache_res[\'rec\'][\'data\'] and\n                                    len(cache_res[\'rec\'][\'data\']) > 10):\n                                should_restore = False\n                            else:\n                                if (str(cache_res[\'rec\'][\'data\']) !=\n                                        ae_consts.EMPTY_DF_STR):\n                                    should_restore = True\n                            if should_restore:\n                                log.info(\n                                    \'restore nested dataset: \'\n                                    f\'{ds_parent_key} to: {new_key}\')\n                                publish.publish(\n                                    data=loaded_df,\n                                    redis_enabled=True,\n                                    redis_key=new_key,\n                                    redis_db=redis_output_db,\n                                    redis_address=redis_address,\n                                    redis_password=redis_password,\n                                    redis_expire=redis_expire,\n                                    redis_serializer=redis_serializer,\n                                    redis_encoding=redis_encoding,\n                                    s3_enabled=False,\n                                    output_file=None,\n                                    df_compress=datasets_compressed,\n                                    verbose=verbose)\n                            else:\n                                if verbose:\n                                    print(f\' - checking: {new_key} - SKIP\')\n\n                        if verbose:\n                            print(f\' - {new_key} - no data to sync\')\n                    # end of is a dataframe\n                    # else:\n                    # end of handling dataframe vs dictionary\n\n                    num_done += 1\n        # end of for all datasets\n        print(\'-----------------------------------\')\n    # end for all dataset to restore\n\n    log.info(f\'restore - done - num_done={num_done} total={total_to_restore}\')\n\n    return use_ds\n# end of restore_dataset\n'"
analysis_engine/run_algo.py,0,"b'""""""\nRun an Algo\n\n**Supported environment variables**\n\n::\n\n    export REDIS_ADDRESS=""localhost:6379""\n    export REDIS_DB=""0""\n    export S3_ADDRESS=""localhost:9000""\n    export S3_BUCKET=""dev""\n    export AWS_ACCESS_KEY_ID=""trexaccesskey""\n    export AWS_SECRET_ACCESS_KEY=""trex123321""\n    export AWS_DEFAULT_REGION=""us-east-1""\n    export S3_SECURE=""0""\n    export WORKER_BROKER_URL=""redis://0.0.0.0:6379/13""\n    export WORKER_BACKEND_URL=""redis://0.0.0.0:6379/14""\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport os\nimport datetime\nimport json\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.algo as base_algo\nimport analysis_engine.utils as ae_utils\nimport analysis_engine.build_algo_request as algo_utils\nimport analysis_engine.build_dataset_node as build_ds_node\nimport analysis_engine.build_result as build_result\nimport analysis_engine.api_requests as api_requests\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef run_algo(\n        ticker=None,\n        tickers=None,\n        algo=None,  # optional derived ``analysis_engine.algo.Algo`` instance\n        balance=None,     # float starting base capital\n        commission=None,  # float for single trade commission for buy or sell\n        start_date=None,  # string YYYY-MM-DD HH:MM:SS\n        end_date=None,    # string YYYY-MM-DD HH:MM:SS\n        datasets=None,    # string list of identifiers\n        num_owned_dict=None,  # not supported\n        cache_freq=\'daily\',   # \'minute\' not supported\n        auto_fill=True,\n        load_config=None,\n        report_config=None,\n        history_config=None,\n        extract_config=None,\n        use_key=None,\n        extract_mode=\'all\',\n        iex_datasets=None,\n        redis_enabled=True,\n        redis_address=None,\n        redis_db=None,\n        redis_password=None,\n        redis_expire=None,\n        redis_key=None,\n        s3_enabled=True,\n        s3_address=None,\n        s3_bucket=None,\n        s3_access_key=None,\n        s3_secret_key=None,\n        s3_region_name=None,\n        s3_secure=False,\n        s3_key=None,\n        celery_disabled=True,\n        broker_url=None,\n        result_backend=None,\n        label=None,\n        name=None,\n        timeseries=None,\n        trade_strategy=None,\n        verbose=False,\n        publish_to_slack=True,\n        publish_to_s3=True,\n        publish_to_redis=True,\n        extract_datasets=None,\n        config_file=None,\n        config_dict=None,\n        version=1,\n        raise_on_err=True,\n        **kwargs):\n    """"""run_algo\n\n    Run an algorithm with steps:\n\n        1) Extract redis keys between dates\n        2) Compile a data pipeline dictionary (call it ``data``)\n        3) Call algorithm\'s ``myalgo.handle_data(data=data)``\n\n    .. note:: If no ``algo`` is set, the\n        ``analysis_engine.algo.BaseAlgo`` algorithm\n        is used.\n\n    .. note:: Please ensure Redis and Minio are running\n        before trying to extract tickers\n\n    **Stock tickers to extract**\n\n    :param ticker: single stock ticker/symbol/ETF to extract\n    :param tickers: optional - list of tickers to extract\n    :param use_key: optional - extract historical key from Redis\n\n    **Algo Configuration**\n\n    :param algo: derived instance of ``analysis_engine.algo.Algo`` object\n    :param balance: optional - float balance parameter\n        can also be set on the ``algo`` object if not\n        set on the args\n    :param commission: float for single trade commission for\n        buy or sell. can also be set on the ``algo`` objet\n    :param start_date: string ``YYYY-MM-DD_HH:MM:SS`` cache value\n    :param end_date: string ``YYYY-MM-DD_HH:MM:SS`` cache value\n    :param dataset_types: list of strings that are ``iex`` or ``yahoo``\n        datasets that are cached.\n    :param cache_freq: optional - depending on if you are running data feeds\n        on a ``daily`` cron (default) vs every ``minute`` (or faster)\n    :param num_owned_dict: not supported yet\n    :param auto_fill: optional - boolean for auto filling\n        buy/sell orders for backtesting (default is\n        ``True``)\n    :param trading_calendar: ``trading_calendar.TradingCalendar``\n        object, by default ``analysis_engine.calendars.\n        always_open.AlwaysOpen`` trading calendar\n        # TradingCalendar by ``TFSExchangeCalendar``\n    :param config_file: path to a json file\n        containing custom algorithm object\n        member values (like indicator configuration and\n        predict future date units ahead for a backtest)\n    :param config_dict: optional - dictionary that\n        can be passed to derived class implementations\n        of: ``def load_from_config(config_dict=config_dict)``\n\n    **Timeseries**\n\n    :param timeseries: optional - string to\n        set ``day`` or ``minute`` backtesting\n        or live trading\n        (default is ``minute``)\n\n    **Trading Strategy**\n\n    :param trade_strategy: optional - string to\n        set the type of ``Trading Strategy``\n        for backtesting or live trading\n        (default is ``count``)\n\n    **Algorithm Dataset Loading, Extracting, Reporting\n    and Trading History arguments**\n\n    :param load_config: optional - dictionary\n        for setting member variables to load an\n        agorithm-ready dataset from\n        a file, s3 or redis\n    :param report_config: optional - dictionary\n        for setting member variables to publish\n        an algo ``trading performance report`` to s3,\n        redis, a file or slack\n    :param history_config: optional - dictionary\n        for setting member variables to publish\n        an algo ``trade history`` to s3, redis, a file\n        or slack\n    :param extract_config: optional - dictionary\n        for setting member variables to publish\n        an algo ``trading performance report`` to s3,\n        redis, a file or slack\n\n    **(Optional) Data sources, datafeeds and datasets to gather**\n\n    :param iex_datasets: list of strings for gathering specific `IEX\n        datasets <https://iexcloud.io/>`__\n        which are set as consts: ``analysis_engine.iex.consts.FETCH_*``.\n\n    **(Optional) Redis connectivity arguments**\n\n    :param redis_enabled: bool - toggle for auto-caching all\n        datasets in Redis\n        (default is ``True``)\n    :param redis_address: Redis connection string\n        format is ``host:port``\n        (default is ``localhost:6379``)\n    :param redis_db: Redis db to use\n        (default is ``0``)\n    :param redis_password: optional - Redis password\n        (default is ``None``)\n    :param redis_expire: optional - Redis expire value\n        (default is ``None``)\n    :param redis_key: optional - redis key not used\n        (default is ``None``)\n\n    **(Optional) Minio (S3) connectivity arguments**\n\n    :param s3_enabled: bool - toggle for auto-archiving on Minio (S3)\n        (default is ``True``)\n    :param s3_address: Minio S3 connection string\n        format ``host:port``\n        (default is ``localhost:9000``)\n    :param s3_bucket: S3 Bucket for storing the artifacts\n        (default is ``dev``) which should be viewable on a browser:\n        http://localhost:9000/minio/dev/\n    :param s3_access_key: S3 Access key\n        (default is ``trexaccesskey``)\n    :param s3_secret_key: S3 Secret key\n        (default is ``trex123321``)\n    :param s3_region_name: S3 region name\n        (default is ``us-east-1``)\n    :param s3_secure: Transmit using tls encryption\n        (default is ``False``)\n    :param s3_key: optional s3 key not used\n        (default is ``None``)\n\n    **(Optional) Celery worker broker connectivity arguments**\n\n    :param celery_disabled: bool - toggle synchronous mode or publish\n        to an engine connected to the `Celery broker and backend\n        <https://github.com/celery/celery#transports-and-backends>`__\n        (default is ``True`` - synchronous mode without an engine\n        or need for a broker or backend for Celery)\n    :param broker_url: Celery broker url\n        (default is ``redis://0.0.0.0:6379/13``)\n    :param result_backend: Celery backend url\n        (default is ``redis://0.0.0.0:6379/14``)\n    :param label: tracking log label\n    :param publish_to_slack: optional - boolean for\n        publishing to slack (coming soon)\n    :param publish_to_s3: optional - boolean for\n        publishing to s3 (coming soon)\n    :param publish_to_redis: optional - boolean for\n        publishing to redis (coming soon)\n\n    **(Optional) Debugging**\n\n    :param verbose: bool - show extract warnings\n        and other debug logging (default is False)\n    :param raise_on_err: optional - boolean for\n        unittests and developing algorithms with the\n        ``analysis_engine.run_algo.run_algo`` helper.\n        When set to ``True`` exceptions will\n        are raised to the calling functions\n\n    :param kwargs: keyword arguments dictionary\n    """"""\n\n    # dictionary structure with a list sorted on: ascending dates\n    # algo_data_req[ticker][list][dataset] = pd.DataFrame\n    algo_data_req = {}\n    extract_requests = []\n    return_algo = False  # return created algo objects for use by caller\n    rec = {}\n    msg = None\n\n    use_tickers = tickers\n    use_balance = balance\n    use_commission = commission\n\n    if ticker:\n        use_tickers = [ticker]\n    else:\n        if not use_tickers:\n            use_tickers = []\n\n    # if these are not set as args, but the algo object\n    # has them, use them instead:\n    if algo:\n        if len(use_tickers) == 0:\n            use_tickers = algo.get_tickers()\n        if not use_balance:\n            use_balance = algo.get_balance()\n        if not use_commission:\n            use_commission = algo.get_commission()\n\n    default_iex_datasets = [\n        \'daily\',\n        \'minute\',\n        \'quote\',\n        \'stats\',\n        \'peers\',\n        \'news\',\n        \'financials\',\n        \'earnings\',\n        \'dividends\',\n        \'company\'\n    ]\n\n    if not iex_datasets:\n        iex_datasets = default_iex_datasets\n\n    if redis_enabled:\n        if not redis_address:\n            redis_address = os.getenv(\n                \'REDIS_ADDRESS\',\n                \'localhost:6379\')\n        if not redis_password:\n            redis_password = os.getenv(\n                \'REDIS_PASSWORD\',\n                None)\n        if not redis_db:\n            redis_db = int(os.getenv(\n                \'REDIS_DB\',\n                \'0\'))\n        if not redis_expire:\n            redis_expire = os.getenv(\n                \'REDIS_EXPIRE\',\n                None)\n    if s3_enabled:\n        if not s3_address:\n            s3_address = os.getenv(\n                \'S3_ADDRESS\',\n                \'localhost:9000\')\n        if not s3_access_key:\n            s3_access_key = os.getenv(\n                \'AWS_ACCESS_KEY_ID\',\n                \'trexaccesskey\')\n        if not s3_secret_key:\n            s3_secret_key = os.getenv(\n                \'AWS_SECRET_ACCESS_KEY\',\n                \'trex123321\')\n        if not s3_region_name:\n            s3_region_name = os.getenv(\n                \'AWS_DEFAULT_REGION\',\n                \'us-east-1\')\n        if not s3_secure:\n            s3_secure = os.getenv(\n                \'S3_SECURE\',\n                \'0\') == \'1\'\n        if not s3_bucket:\n            s3_bucket = os.getenv(\n                \'S3_BUCKET\',\n                \'dev\')\n    if not broker_url:\n        broker_url = os.getenv(\n            \'WORKER_BROKER_URL\',\n            \'redis://0.0.0.0:6379/11\')\n    if not result_backend:\n        result_backend = os.getenv(\n            \'WORKER_BACKEND_URL\',\n            \'redis://0.0.0.0:6379/12\')\n\n    if not label:\n        label = \'run-algo\'\n\n    num_tickers = len(use_tickers)\n    last_close_str = ae_utils.get_last_close_str()\n\n    if iex_datasets:\n        if verbose:\n            log.info(\n                f\'{label} - tickers={num_tickers} \'\n                f\'iex={json.dumps(iex_datasets)}\')\n    else:\n        if verbose:\n            log.info(f\'{label} - tickers={num_tickers}\')\n\n    ticker_key = use_key\n    if not ticker_key:\n        ticker_key = f\'{ticker}_{last_close_str}\'\n\n    if not algo:\n        algo = base_algo.BaseAlgo(\n            ticker=None,\n            tickers=use_tickers,\n            balance=use_balance,\n            commission=use_commission,\n            config_dict=config_dict,\n            name=label,\n            auto_fill=auto_fill,\n            timeseries=timeseries,\n            trade_strategy=trade_strategy,\n            publish_to_slack=publish_to_slack,\n            publish_to_s3=publish_to_s3,\n            publish_to_redis=publish_to_redis,\n            raise_on_err=raise_on_err)\n        return_algo = True\n        # the algo object is stored\n        # in the result at: res[\'rec\'][\'algo\']\n\n    if not algo:\n        msg = f\'{label} - missing algo object\'\n        log.error(msg)\n        return build_result.build_result(\n                status=ae_consts.EMPTY,\n                err=msg,\n                rec=rec)\n\n    if raise_on_err:\n        log.debug(f\'{label} - enabling algo exception raises\')\n        algo.raise_on_err = True\n\n    indicator_datasets = algo.get_indicator_datasets()\n    if len(indicator_datasets) == 0:\n        indicator_datasets = ae_consts.BACKUP_DATASETS\n        log.info(\n            f\'using all datasets={indicator_datasets}\')\n\n    verbose_extract = False\n    if config_dict:\n        verbose_extract = config_dict.get(\'verbose_extract\', False)\n\n    common_vals = {}\n    common_vals[\'base_key\'] = ticker_key\n    common_vals[\'celery_disabled\'] = celery_disabled\n    common_vals[\'ticker\'] = ticker\n    common_vals[\'label\'] = label\n    common_vals[\'iex_datasets\'] = iex_datasets\n    common_vals[\'s3_enabled\'] = s3_enabled\n    common_vals[\'s3_bucket\'] = s3_bucket\n    common_vals[\'s3_address\'] = s3_address\n    common_vals[\'s3_secure\'] = s3_secure\n    common_vals[\'s3_region_name\'] = s3_region_name\n    common_vals[\'s3_access_key\'] = s3_access_key\n    common_vals[\'s3_secret_key\'] = s3_secret_key\n    common_vals[\'s3_key\'] = ticker_key\n    common_vals[\'redis_enabled\'] = redis_enabled\n    common_vals[\'redis_address\'] = redis_address\n    common_vals[\'redis_password\'] = redis_password\n    common_vals[\'redis_db\'] = redis_db\n    common_vals[\'redis_key\'] = ticker_key\n    common_vals[\'redis_expire\'] = redis_expire\n\n    use_start_date_str = start_date\n    use_end_date_str = end_date\n    last_close_date = ae_utils.last_close()\n    end_date_val = None\n\n    cache_freq_fmt = ae_consts.COMMON_TICK_DATE_FORMAT\n\n    if not use_end_date_str:\n        use_end_date_str = last_close_date.strftime(\n            cache_freq_fmt)\n\n    end_date_val = ae_utils.get_date_from_str(\n        date_str=use_end_date_str,\n        fmt=cache_freq_fmt)\n    start_date_val = None\n\n    if not use_start_date_str:\n        start_date_val = end_date_val - datetime.timedelta(\n            days=60)\n        use_start_date_str = start_date_val.strftime(\n            cache_freq_fmt)\n    else:\n        start_date_val = datetime.datetime.strptime(\n            use_start_date_str,\n            ae_consts.COMMON_TICK_DATE_FORMAT)\n\n    total_dates = (end_date_val - start_date_val).days\n\n    if end_date_val < start_date_val:\n        msg = (\n            f\'{label} - invalid dates - start_date={start_date_val} is after \'\n            f\'end_date={end_date_val}\')\n        raise Exception(msg)\n\n    if verbose:\n        log.info(\n            f\'{label} - days={total_dates} \'\n            f\'start={use_start_date_str} \'\n            f\'end={use_end_date_str} \'\n            f\'datasets={indicator_datasets}\')\n\n    for ticker in use_tickers:\n        req = algo_utils.build_algo_request(\n            ticker=ticker,\n            use_key=use_key,\n            start_date=use_start_date_str,\n            end_date=use_end_date_str,\n            datasets=datasets,\n            balance=use_balance,\n            cache_freq=cache_freq,\n            timeseries=timeseries,\n            trade_strategy=trade_strategy,\n            label=label)\n        ticker_key = f\'{ticker}_{last_close_str}\'\n        common_vals[\'ticker\'] = ticker\n        common_vals[\'base_key\'] = ticker_key\n        common_vals[\'redis_key\'] = ticker_key\n        common_vals[\'s3_key\'] = ticker_key\n\n        for date_key in req[\'extract_datasets\']:\n            date_req = api_requests.get_ds_dict(\n                ticker=ticker,\n                base_key=date_key,\n                ds_id=label,\n                service_dict=common_vals)\n            node_date_key = date_key.replace(\n                f\'{ticker}_\',\n                \'\')\n            extract_requests.append({\n                \'id\': date_key,\n                \'ticker\': ticker,\n                \'date_key\': date_key,\n                \'date\': node_date_key,\n                \'req\': date_req})\n    # end of for all ticker in use_tickers\n\n    first_extract_date = None\n    last_extract_date = None\n    total_extract_requests = len(extract_requests)\n    cur_idx = 1\n    for idx, extract_node in enumerate(extract_requests):\n\n        extract_ticker = extract_node[\'ticker\']\n        extract_date = extract_node[\'date\']\n        ds_node_id = extract_node[\'id\']\n\n        if not first_extract_date:\n            first_extract_date = extract_date\n        last_extract_date = extract_date\n        perc_progress = ae_consts.get_percent_done(\n            progress=cur_idx,\n            total=total_extract_requests)\n        percent_label = (\n            f\'{label} \'\n            f\'ticker={extract_ticker} \'\n            f\'date={extract_date} \'\n            f\'{perc_progress} \'\n            f\'{idx}/{total_extract_requests} \'\n            f\'{indicator_datasets}\')\n        if verbose:\n            log.info(\n                f\'extracting - {percent_label}\')\n\n        ticker_bt_data = build_ds_node.build_dataset_node(\n            ticker=extract_ticker,\n            date=extract_date,\n            service_dict=common_vals,\n            datasets=indicator_datasets,\n            log_label=label,\n            verbose=verbose_extract)\n\n        if ticker not in algo_data_req:\n            algo_data_req[ticker] = []\n\n        algo_data_req[ticker].append({\n            \'id\': ds_node_id,  # id is currently the cache key in redis\n            \'date\': extract_date,  # used to confirm dates in asc order\n            \'data\': ticker_bt_data\n        })\n\n        if verbose:\n            log.info(\n                f\'extract - {percent_label} \'\n                f\'dataset={len(algo_data_req[ticker])}\')\n        cur_idx += 1\n    # end of for service_dict in extract_requests\n\n    # this could be a separate celery task\n    status = ae_consts.NOT_RUN\n    if len(algo_data_req) == 0:\n        msg = (\n            f\'{label} - nothing to test - no data found for \'\n            f\'tickers={use_tickers} \'\n            f\'between {first_extract_date} and {last_extract_date}\')\n        log.info(msg)\n        return build_result.build_result(\n            status=ae_consts.EMPTY,\n            err=msg,\n            rec=rec)\n\n    # this could be a separate celery task\n    try:\n        if verbose:\n            log.info(\n                f\'handle_data START - {percent_label} from \'\n                f\'{first_extract_date} to {last_extract_date}\')\n        algo.handle_data(\n            data=algo_data_req)\n        if verbose:\n            log.info(\n                f\'handle_data END - {percent_label} from \'\n                f\'{first_extract_date} to {last_extract_date}\')\n    except Exception as e:\n        a_name = algo.get_name()\n        a_debug_msg = algo.get_debug_msg()\n        if not a_debug_msg:\n            a_debug_msg = \'debug message not set\'\n        a_config_dict = ae_consts.ppj(algo.config_dict)\n        msg = (\n            f\'{percent_label} - algo={a_name} \'\n            f\'encountered exception in handle_data tickers={use_tickers} \'\n            f\'from {first_extract_date} to {last_extract_date} ex={e} \'\n            f\'and failed during operation: {a_debug_msg}\')\n        if raise_on_err:\n            if algo:\n                try:\n                    ind_obj = \\\n                        algo.get_indicator_process_last_indicator()\n                    if ind_obj:\n                        ind_obj_path = ind_obj.get_path_to_module()\n                        ind_obj_config = ae_consts.ppj(\n                            ind_obj.get_config())\n                        found_error_hint = False\n                        if hasattr(ind_obj.use_df, \'to_json\'):\n                            if len(ind_obj.use_df.index) == 0:\n                                log.critical(\n                                    f\'indicator failure report for \'\n                                    f\'last module: \'\n                                    f\'{ind_obj_path} \'\n                                    f\'indicator={ind_obj.get_name()} \'\n                                    f\'config={ind_obj_config} \'\n                                    f\'dataset={ind_obj.use_df.head(5)} \'\n                                    f\'name_of_dataset={ind_obj.uses_data}\')\n                                log.critical(\n                                    f\'--------------------------------------\'\n                                    f\'--------------------------------------\')\n                                log.critical(\n                                    f\'Please check if this indicator: \'\n                                    f\'{ind_obj_path} \'\n                                    f\'supports Empty Dataframes\')\n                                log.critical(\n                                    f\'--------------------------------------\'\n                                    f\'--------------------------------------\')\n                                found_error_hint = True\n                        # indicator error hints\n\n                        if not found_error_hint:\n                            log.critical(\n                                f\'indicator failure report for last module: \'\n                                f\'{ind_obj_path} \'\n                                f\'indicator={ind_obj.get_name()} \'\n                                f\'config={ind_obj_config} \'\n                                f\'dataset={ind_obj.use_df.head(5)} \'\n                                f\'name_of_dataset={ind_obj.uses_data}\')\n                except Exception as f:\n                    log.critical(\n                        f\'failed to pull indicator processor \'\n                        f\'last indicator for debugging \'\n                        f\'from ex={e} with parsing ex={f}\')\n                # end of ignoring non-supported ways of creating\n                # indicator processors\n            log.error(msg)\n            log.error(\n                f\'algo failure report: \'\n                f\'algo={a_name} handle_data() \'\n                f\'config={a_config_dict} \')\n            log.critical(\n                f\'algo failed during operation: {a_debug_msg}\')\n            raise e\n        else:\n            log.error(msg)\n            return build_result.build_result(\n                status=ae_consts.ERR,\n                err=msg,\n                rec=rec)\n    # end of try/ex\n\n    # this could be a separate celery task\n    try:\n        if verbose:\n            log.info(\n                f\'get_result START - {percent_label} from \'\n                f\'{first_extract_date} to {last_extract_date}\')\n        rec = algo.get_result()\n        status = ae_consts.SUCCESS\n        if verbose:\n            log.info(\n                f\'get_result END - {percent_label} from \'\n                f\'{first_extract_date} to {last_extract_date}\')\n    except Exception as e:\n        msg = (\n            f\'{percent_label} - algo={algo.get_name()} encountered exception \'\n            f\'in get_result tickers={use_tickers} from \'\n            f\'{first_extract_date} to {last_extract_date} ex={e}\')\n        if raise_on_err:\n            if algo:\n                log.error(\n                    f\'algo={algo.get_name()} failed in get_result with \'\n                    f\'debug_msg={algo.get_debug_msg()}\')\n            log.error(msg)\n            raise e\n        else:\n            log.error(msg)\n            return build_result.build_result(\n                status=ae_consts.ERR,\n                err=msg,\n                rec=rec)\n    # end of try/ex\n\n    if return_algo:\n        rec[\'algo\'] = algo\n\n    return build_result.build_result(\n        status=status,\n        err=msg,\n        rec=rec)\n# end of run_algo\n'"
analysis_engine/run_custom_algo.py,0,"b'""""""\nThis is a wrapper for running your own custom algorithms\n\n.. note:: Please refer to the `sa.py <https://\n    github.com/AlgoTraders/stock-analysis-engine/blob/master/\n    analysis_engine/scripts/sa.py>`__\n    for the lastest usage examples.\n\nExample with the command line tool:\n\n::\n\n    bt -t SPY -g /opt/sa/analysis_engine/mocks/example_algo_minute.py\n\n""""""\n\nimport os\nimport inspect\nimport types\nimport importlib.machinery\nimport datetime\nimport json\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.build_algo_request as build_algo_request\nimport analysis_engine.build_publish_request as build_publish_request\nimport analysis_engine.build_result as build_result\nimport analysis_engine.run_algo as run_algo\nimport analysis_engine.work_tasks.get_celery_app as get_celery_app\nimport analysis_engine.algo as ae_algo\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef run_custom_algo(\n        mod_path,\n        ticker=\'SPY\',\n        balance=50000,\n        commission=6.0,\n        start_date=None,\n        end_date=None,\n        name=\'myalgo\',\n        auto_fill=True,\n        config_file=None,\n        config_dict=None,\n        load_from_s3_bucket=None,\n        load_from_s3_key=None,\n        load_from_redis_key=None,\n        load_from_file=None,\n        load_compress=False,\n        load_publish=True,\n        load_config=None,\n        report_redis_key=None,\n        report_s3_bucket=None,\n        report_s3_key=None,\n        report_file=None,\n        report_compress=False,\n        report_publish=True,\n        report_config=None,\n        history_redis_key=None,\n        history_s3_bucket=None,\n        history_s3_key=None,\n        history_file=None,\n        history_compress=False,\n        history_publish=True,\n        history_config=None,\n        extract_redis_key=None,\n        extract_s3_bucket=None,\n        extract_s3_key=None,\n        extract_file=None,\n        extract_save_dir=None,\n        extract_compress=False,\n        extract_publish=True,\n        extract_config=None,\n        publish_to_s3=True,\n        publish_to_redis=True,\n        publish_to_slack=True,\n        dataset_type=ae_consts.SA_DATASET_TYPE_ALGO_READY,\n        serialize_datasets=ae_consts.DEFAULT_SERIALIZED_DATASETS,\n        compress=False,\n        encoding=\'utf-8\',\n        redis_enabled=True,\n        redis_key=None,\n        redis_address=None,\n        redis_db=None,\n        redis_password=None,\n        redis_expire=None,\n        redis_serializer=\'json\',\n        redis_encoding=\'utf-8\',\n        s3_enabled=True,\n        s3_key=None,\n        s3_address=None,\n        s3_bucket=None,\n        s3_access_key=None,\n        s3_secret_key=None,\n        s3_region_name=None,\n        s3_secure=False,\n        slack_enabled=False,\n        slack_code_block=False,\n        slack_full_width=False,\n        timeseries=None,\n        trade_strategy=None,\n        verbose=False,\n        debug=False,\n        dataset_publish_extract=False,\n        dataset_publish_history=False,\n        dataset_publish_report=False,\n        run_on_engine=False,\n        auth_url=ae_consts.WORKER_BROKER_URL,\n        backend_url=ae_consts.WORKER_BACKEND_URL,\n        include_tasks=ae_consts.INCLUDE_TASKS,\n        ssl_options=ae_consts.SSL_OPTIONS,\n        transport_options=ae_consts.TRANSPORT_OPTIONS,\n        path_to_config_module=ae_consts.WORKER_CELERY_CONFIG_MODULE,\n        raise_on_err=True):\n    """"""run_custom_algo\n\n    Run a custom algorithm that derives the\n    ``analysis_engine.algo.BaseAlgo`` class\n\n    .. note:: Make sure to only have **1**\n        class defined in an algo module. Imports from\n        other modules should work just fine.\n\n    **Algorithm arguments**\n\n    :param mod_path: file path to custom\n        algorithm class module\n    :param ticker: ticker symbol\n    :param balance: float - starting balance capital\n        for creating buys and sells\n    :param commission: float - cost pet buy or sell\n    :param name: string - name for tracking algorithm\n        in the logs\n    :param start_date: string - start date for backtest with\n        format ``YYYY-MM-DD HH:MM:SS``\n    :param end_date: end date for backtest with\n        format ``YYYY-MM-DD HH:MM:SS``\n    :param auto_fill: optional - boolean for auto filling\n        buy and sell orders for backtesting\n        (default is ``True``)\n    :param config_file: path to a json file\n        containing custom algorithm object\n        member values (like indicator configuration and\n        predict future date units ahead for a backtest)\n    :param config_dict: optional - dictionary that\n        can be passed to derived class implementations\n        of: ``def load_from_config(config_dict=config_dict)``\n\n    **Timeseries**\n\n    :param timeseries: optional - string to\n        set ``day`` or ``minute`` backtesting\n        or live trading\n        (default is ``minute``)\n\n    **Trading Strategy**\n\n    :param trade_strategy: optional - string to\n        set the type of ``Trading Strategy``\n        for backtesting or live trading\n        (default is ``count``)\n\n    **Running Distributed Algorithms on the Engine Workers**\n\n    :param run_on_engine: optional - boolean\n        flag for publishing custom algorithms\n        to Celery ae workers for distributing\n        algorithm workloads\n        (default is ``False`` which will run algos locally)\n        this is required for distributing algorithms\n    :param auth_url: Celery broker address\n        (default is ``redis://localhost:6379/11``\n        or ``analysis_engine.consts.WORKER_BROKER_URL``\n        environment variable)\n        this is required for distributing algorithms\n    :param backend_url: Celery backend address\n        (default is ``redis://localhost:6379/12``\n        or ``analysis_engine.consts.WORKER_BACKEND_URL``\n        environment variable)\n        this is required for distributing algorithms\n    :param include_tasks: list of modules containing tasks to add\n        (default is ``analysis_engine.consts.INCLUDE_TASKS``)\n    :param ssl_options: security options dictionary\n        (default is ``analysis_engine.consts.SSL_OPTIONS``)\n    :param trasport_options: transport options dictionary\n        (default is ``analysis_engine.consts.TRANSPORT_OPTIONS``)\n    :param path_to_config_module: config module for advanced\n        Celery worker connectivity requirements\n        (default is ``analysis_engine.work_tasks.celery_config``\n        or ``analysis_engine.consts.WORKER_CELERY_CONFIG_MODULE``)\n\n    **Load Algorithm-Ready Dataset From Source**\n\n    Use these arguments to load algorithm-ready datasets\n    from supported sources (file, s3 or redis)\n\n    :param load_from_s3_bucket: optional - string load the algo from an\n        a previously-created s3 bucket holding an s3 key with an\n        algorithm-ready dataset for use with:\n        ``handle_data``\n    :param load_from_s3_key: optional - string load the algo from an\n        a previously-created s3 key holding an\n        algorithm-ready dataset for use with:\n        ``handle_data``\n    :param load_from_redis_key: optional - string load the algo from a\n        a previously-created redis key holding an\n        algorithm-ready dataset for use with:\n        ``handle_data``\n    :param load_from_file: optional - string path to\n        a previously-created local file holding an\n        algorithm-ready dataset for use with:\n        ``handle_data``\n    :param load_compress: optional - boolean\n        flag for toggling to decompress\n        or not when loading an algorithm-ready\n        dataset (``True`` means the dataset\n        must be decompressed to load correctly inside\n        an algorithm to run a backtest)\n    :param load_publish: boolean - toggle publishing\n        the load progress to slack, s3, redis or a file\n        (default is ``True``)\n    :param load_config: optional - dictionary\n        for setting member variables to load an\n        agorithm-ready dataset from\n        a file, s3 or redis\n\n    **Publishing Control Bool Flags**\n\n    :param publish_to_s3: optional - boolean for\n        toggling publishing to s3 on/off\n        (default is ``True``)\n    :param publish_to_redis: optional - boolean for\n        publishing to redis on/off\n        (default is ``True``)\n    :param publish_to_slack: optional - boolean for\n        publishing to slack\n        (default is ``True``)\n\n    **Algorithm Trade History Arguments**\n\n    :param history_redis_key: optional - string\n        where the algorithm trading history will be stored in\n        an redis key\n    :param history_s3_bucket: optional - string\n        where the algorithm trading history will be stored in\n        an s3 bucket\n    :param history_s3_key: optional - string\n        where the algorithm trading history will be stored in\n        an s3 key\n    :param history_file: optional - string key\n        where the algorithm trading history will be stored in\n        a file serialized as a json-string\n    :param history_compress: optional - boolean\n        flag for toggling to decompress\n        or not when loading an algorithm-ready\n        dataset (``True`` means the dataset\n        will be compressed on publish)\n    :param history_publish: boolean - toggle publishing\n        the history to s3, redis or a file\n        (default is ``True``)\n    :param history_config: optional - dictionary\n        for setting member variables to publish\n        an algo ``trade history`` to s3, redis, a file\n        or slack\n\n    **Algorithm Trade Performance Report Arguments (Output Dataset)**\n\n    :param report_redis_key: optional - string\n        where the algorithm ``trading performance report`` (report)\n        will be stored in an redis key\n    :param report_s3_bucket: optional - string\n        where the algorithm report will be stored in\n        an s3 bucket\n    :param report_s3_key: optional - string\n        where the algorithm report will be stored in\n        an s3 key\n    :param report_file: optional - string key\n        where the algorithm report will be stored in\n        a file serialized as a json-string\n    :param report_compress: optional - boolean\n        flag for toggling to decompress\n        or not when loading an algorithm-ready\n        dataset (``True`` means the dataset\n        will be compressed on publish)\n    :param report_publish: boolean - toggle publishing\n        the ``trading performance report`` s3, redis or a file\n        (default is ``True``)\n    :param report_config: optional - dictionary\n        for setting member variables to publish\n        an algo ``trading performance report`` to s3,\n        redis, a file or slack\n\n    **Extract an Algorithm-Ready Dataset Arguments**\n\n    :param extract_redis_key: optional - string\n        where the algorithm report will be stored in\n        an redis key\n    :param extract_s3_bucket: optional - string\n        where the algorithm report will be stored in\n        an s3 bucket\n    :param extract_s3_key: optional - string\n        where the algorithm report will be stored in\n        an s3 key\n    :param extract_file: optional - string key\n        where the algorithm report will be stored in\n        a file serialized as a json-string\n    :param extract_save_dir: optional - string path to\n        auto-generated files from the algo\n    :param extract_compress: optional - boolean\n        flag for toggling to decompress\n        or not when loading an algorithm-ready\n        dataset (``True`` means the dataset\n        will be compressed on publish)\n    :param extract_publish: boolean - toggle publishing\n        the used ``algorithm-ready dataset`` to s3, redis or a file\n        (default is ``True``)\n    :param extract_config: optional - dictionary\n        for setting member variables to publish\n        an algo ``trading performance report`` to s3,\n        redis, a file or slack\n\n    **Dataset Arguments**\n\n    :param dataset_type: optional - dataset type\n        (default is ``SA_DATASET_TYPE_ALGO_READY``)\n    :param serialize_datasets: optional - list of dataset names to\n        deserialize in the dataset\n        (default is ``DEFAULT_SERIALIZED_DATASETS``)\n    :param encoding: optional - string for data encoding\n\n    **Publish Algorithm Datasets to S3, Redis or a File**\n\n    :param dataset_publish_extract: optional - bool\n        for publishing the algorithm\'s\n        ``algorithm-ready``\n        dataset to: s3, redis or file\n    :param dataset_publish_history: optional - bool\n        for publishing the algorithm\'s\n        ``trading history``\n        dataset to: s3, redis or file\n    :param dataset_publish_report: optional - bool\n        for publishing the algorithm\'s\n        ``trading performance report``\n        dataset to: s3, redis or file\n\n    **Redis connectivity arguments**\n\n    :param redis_enabled: bool - toggle for auto-caching all\n        datasets in Redis\n        (default is ``True``)\n    :param redis_key: string - key to save the data in redis\n        (default is ``None``)\n    :param redis_address: Redis connection string format: ``host:port``\n        (default is ``localhost:6379``)\n    :param redis_db: Redis db to use\n        (default is ``0``)\n    :param redis_password: optional - Redis password\n        (default is ``None``)\n    :param redis_expire: optional - Redis expire value\n        (default is ``None``)\n    :param redis_serializer: not used yet - support for future\n        pickle objects in redis\n    :param redis_encoding: format of the encoded key in redis\n\n    **Minio (S3) connectivity arguments**\n\n    :param s3_enabled: bool - toggle for auto-archiving on Minio (S3)\n        (default is ``True``)\n    :param s3_key: string - key to save the data in redis\n        (default is ``None``)\n    :param s3_address: Minio S3 connection string format: ``host:port``\n        (default is ``localhost:9000``)\n    :param s3_bucket: S3 Bucket for storing the artifacts\n        (default is ``dev``) which should be viewable on a browser:\n        http://localhost:9000/minio/dev/\n    :param s3_access_key: S3 Access key\n        (default is ``trexaccesskey``)\n    :param s3_secret_key: S3 Secret key\n        (default is ``trex123321``)\n    :param s3_region_name: S3 region name\n        (default is ``us-east-1``)\n    :param s3_secure: Transmit using tls encryption\n        (default is ``False``)\n\n    **Slack arguments**\n\n    :param slack_enabled: optional - boolean for\n        publishing to slack\n    :param slack_code_block: optional - boolean for\n        publishing as a code black in slack\n    :param slack_full_width: optional - boolean for\n        publishing as a to slack using the full\n        width allowed\n\n    **Debugging arguments**\n\n    :param debug: optional - bool for debug tracking\n    :param verbose: optional - bool for increasing\n        logging\n    :param raise_on_err: boolean - set this to ``False`` on prod\n        to ensure exceptions do not interrupt services.\n        With the default (``True``) any exceptions from the library\n        and your own algorithm are sent back out immediately exiting\n        the backtest.\n    """"""\n\n    module_name = \'BaseAlgo\'\n    custom_algo_module = None\n    new_algo_object = None\n    use_custom_algo = False\n    found_algo_module = True\n    should_publish_extract_dataset = False\n    should_publish_history_dataset = False\n    should_publish_report_dataset = False\n    use_config_file = None\n    use_config_dict = config_dict\n    if config_file:\n        if os.path.exists(config_file):\n            use_config_file = config_file\n            if not config_dict:\n                try:\n                    use_config_dict = json.loads(open(\n                        config_file, \'r\').read())\n                except Exception as e:\n                    msg = (\n                        f\'failed parsing json config_file={config_file} \'\n                        f\'with ex={e}\')\n                    log.error(msg)\n                    raise Exception(msg)\n    # end of loading the config_file\n\n    err = None\n    if mod_path:\n        module_name = mod_path.split(\'/\')[-1]\n        loader = importlib.machinery.SourceFileLoader(\n            module_name,\n            mod_path)\n        custom_algo_module = types.ModuleType(\n            loader.name)\n        loader.exec_module(\n            custom_algo_module)\n        use_custom_algo = True\n\n        for member in inspect.getmembers(custom_algo_module):\n            if module_name in str(member):\n                found_algo_module = True\n                break\n        # for all members in this custom module file\n    # if loading a custom algorithm module from a file on disk\n\n    if not found_algo_module:\n        err = (\n            f\'unable to find custom algorithm module={custom_algo_module}\')\n        if mod_path:\n            err = (\n                \'analysis_engine.run_custom_algo.run_custom_algo was unable \'\n                f\'to find custom algorithm module={custom_algo_module} with \'\n                f\'provided path to \\n file: {mod_path} \\n\'\n                \'\\n\'\n                \'Please confirm \'\n                \'that the class inherits from the BaseAlgo class like:\\n\'\n                \'\\n\'\n                \'import analysis_engine.algo\\n\'\n                \'class MyAlgo(analysis_engine.algo.BaseAlgo):\\n \'\n                \'\\n\'\n                \'If it is then please file an issue on github:\\n \'\n                \'https://github.com/AlgoTraders/stock-analysis-engine/\'\n                \'issues/new \\n\\nFor now this error results in a shutdown\'\n                \'\\n\')\n        # if mod_path set\n\n        if verbose or debug:\n            log.error(err)\n        return build_result.build_result(\n            status=ae_consts.ERR,\n            err=err,\n            rec=None)\n    # if not found_algo_module\n\n    use_start_date = start_date\n    use_end_date = end_date\n    if not use_end_date:\n        end_date = datetime.datetime.utcnow()\n        use_end_date = end_date.strftime(\n            ae_consts.COMMON_TICK_DATE_FORMAT)\n    if not use_start_date:\n        start_date = end_date - datetime.timedelta(days=75)\n        use_start_date = start_date.strftime(\n            ae_consts.COMMON_TICK_DATE_FORMAT)\n        if verbose:\n            log.info(\n                f\'{name} {ticker} setting default start_date={use_start_date}\')\n\n    # Load an algorithm-ready dataset from:\n    # file, s3, or redis\n    if not load_config:\n        load_config = build_publish_request.build_publish_request(\n            ticker=ticker,\n            output_file=None,\n            s3_bucket=None,\n            s3_key=None,\n            redis_key=None,\n            compress=load_compress,\n            redis_enabled=publish_to_redis,\n            redis_address=redis_address,\n            redis_db=redis_db,\n            redis_password=redis_password,\n            redis_expire=redis_expire,\n            redis_serializer=redis_serializer,\n            redis_encoding=redis_encoding,\n            s3_enabled=publish_to_s3,\n            s3_address=s3_address,\n            s3_access_key=s3_access_key,\n            s3_secret_key=s3_secret_key,\n            s3_region_name=s3_region_name,\n            s3_secure=s3_secure,\n            slack_enabled=publish_to_slack,\n            slack_code_block=slack_code_block,\n            slack_full_width=slack_full_width,\n            verbose=verbose,\n            label=f\'load-{name}\')\n        if load_from_file:\n            load_config[\'output_file\'] = load_from_file\n        if load_from_redis_key:\n            load_config[\'redis_key\'] = load_from_redis_key\n            load_config[\'redis_enabled\'] = True\n        if load_from_s3_bucket and load_from_s3_key:\n            load_config[\'s3_bucket\'] = load_from_s3_bucket\n            load_config[\'s3_key\'] = load_from_s3_key\n            load_config[\'s3_enabled\'] = True\n    # end of building load_config dictionary if not already set\n\n    # Automatically save all datasets to an algorithm-ready:\n    # file, s3, or redis\n    if not extract_config:\n        extract_config = build_publish_request.build_publish_request(\n            ticker=ticker,\n            output_file=None,\n            s3_bucket=None,\n            s3_key=None,\n            redis_key=None,\n            compress=extract_compress,\n            redis_enabled=publish_to_redis,\n            redis_address=redis_address,\n            redis_db=redis_db,\n            redis_password=redis_password,\n            redis_expire=redis_expire,\n            redis_serializer=redis_serializer,\n            redis_encoding=redis_encoding,\n            s3_enabled=publish_to_s3,\n            s3_address=s3_address,\n            s3_access_key=s3_access_key,\n            s3_secret_key=s3_secret_key,\n            s3_region_name=s3_region_name,\n            s3_secure=s3_secure,\n            slack_enabled=publish_to_slack,\n            slack_code_block=slack_code_block,\n            slack_full_width=slack_full_width,\n            verbose=verbose,\n            label=f\'extract-{name}\')\n        should_publish_extract_dataset = False\n        if extract_file:\n            extract_config[\'output_file\'] = extract_file\n            should_publish_extract_dataset = True\n        if extract_redis_key and publish_to_redis:\n            extract_config[\'redis_key\'] = extract_redis_key\n            extract_config[\'redis_enabled\'] = True\n            should_publish_extract_dataset = True\n        if extract_s3_bucket and extract_s3_key and publish_to_s3:\n            extract_config[\'s3_bucket\'] = extract_s3_bucket\n            extract_config[\'s3_key\'] = extract_s3_key\n            extract_config[\'s3_enabled\'] = True\n            should_publish_extract_dataset = True\n        else:\n            extract_config[\'s3_enabled\'] = False\n    # end of building extract_config dictionary if not already set\n\n    # Automatically save the trading performance report:\n    # file, s3, or redis\n    if not report_config:\n        report_config = build_publish_request.build_publish_request(\n            ticker=ticker,\n            output_file=None,\n            s3_bucket=None,\n            s3_key=None,\n            redis_key=None,\n            compress=report_compress,\n            redis_enabled=publish_to_redis,\n            redis_address=redis_address,\n            redis_db=redis_db,\n            redis_password=redis_password,\n            redis_expire=redis_expire,\n            redis_serializer=redis_serializer,\n            redis_encoding=redis_encoding,\n            s3_enabled=publish_to_s3,\n            s3_address=s3_address,\n            s3_access_key=s3_access_key,\n            s3_secret_key=s3_secret_key,\n            s3_region_name=s3_region_name,\n            s3_secure=s3_secure,\n            slack_enabled=publish_to_slack,\n            slack_code_block=slack_code_block,\n            slack_full_width=slack_full_width,\n            verbose=verbose,\n            label=f\'report-{name}\')\n        should_publish_report_dataset = False\n        if report_file:\n            report_config[\'output_file\'] = report_file\n            should_publish_report_dataset = True\n        if report_redis_key and publish_to_redis:\n            report_config[\'redis_key\'] = report_redis_key\n            report_config[\'redis_enabled\'] = True\n            should_publish_report_dataset = True\n        if report_s3_bucket and report_s3_key and publish_to_s3:\n            report_config[\'s3_bucket\'] = report_s3_bucket\n            report_config[\'s3_key\'] = report_s3_key\n            report_config[\'s3_enabled\'] = True\n            should_publish_report_dataset = True\n    # end of building report_config dictionary if not already set\n\n    # Automatically save the trade history:\n    # file, s3, or redis\n    if not history_config:\n        history_config = build_publish_request.build_publish_request(\n            ticker=ticker,\n            output_file=None,\n            s3_bucket=None,\n            s3_key=None,\n            redis_key=None,\n            compress=report_compress,\n            redis_enabled=publish_to_redis,\n            redis_address=redis_address,\n            redis_db=redis_db,\n            redis_password=redis_password,\n            redis_expire=redis_expire,\n            redis_serializer=redis_serializer,\n            redis_encoding=redis_encoding,\n            s3_enabled=publish_to_s3,\n            s3_address=s3_address,\n            s3_access_key=s3_access_key,\n            s3_secret_key=s3_secret_key,\n            s3_region_name=s3_region_name,\n            s3_secure=s3_secure,\n            slack_enabled=publish_to_slack,\n            slack_code_block=slack_code_block,\n            slack_full_width=slack_full_width,\n            verbose=verbose,\n            label=f\'history-{name}\')\n        should_publish_history_dataset = False\n        if history_file:\n            history_config[\'output_file\'] = history_file\n            should_publish_history_dataset = True\n        if history_redis_key and publish_to_redis:\n            history_config[\'redis_key\'] = history_redis_key\n            history_config[\'redis_enabled\'] = True\n            should_publish_history_dataset = True\n        if history_s3_bucket and history_s3_key and publish_to_s3:\n            history_config[\'s3_bucket\'] = history_s3_bucket\n            history_config[\'s3_key\'] = history_s3_key\n            history_config[\'s3_enabled\'] = True\n            should_publish_history_dataset = True\n    # end of building history_config dictionary if not already set\n\n    if verbose:\n        remove_vals = [\n            \'s3_access_key\',\n            \'s3_secret_key\',\n            \'redis_password\'\n        ]\n        debug_extract_config = {}\n        for k in extract_config:\n            if k not in remove_vals:\n                debug_extract_config[k] = extract_config[k]\n        debug_report_config = {}\n        for k in report_config:\n            if k not in remove_vals:\n                debug_report_config[k] = report_config[k]\n        debug_history_config = {}\n        for k in history_config:\n            if k not in remove_vals:\n                debug_history_config[k] = history_config[k]\n        debug_load_config = {}\n        for k in load_config:\n            if k not in remove_vals:\n                debug_load_config[k] = load_config[k]\n        log.info(\n            f\'{name} {ticker} using extract config \'\n            f\'{ae_consts.ppj(debug_extract_config)}\')\n        log.info(\n            f\'{name} {ticker} using report config \'\n            f\'{ae_consts.ppj(debug_report_config)}\')\n        log.info(\n            f\'{name} {ticker} using trade history config \'\n            f\'{ae_consts.ppj(debug_history_config)}\')\n        log.info(\n            f\'{name} {ticker} using load config \'\n            f\'{ae_consts.ppj(debug_load_config)}\')\n        log.info(\n            f\'{name} {ticker} - building algo request\')\n    # end of verbose\n\n    algo_req = build_algo_request.build_algo_request(\n        ticker=ticker,\n        balance=balance,\n        commission=commission,\n        start_date=use_start_date,\n        end_date=use_end_date,\n        timeseries=timeseries,\n        trade_strategy=trade_strategy,\n        config_file=use_config_file,\n        config_dict=use_config_dict,\n        load_config=load_config,\n        history_config=history_config,\n        report_config=report_config,\n        extract_config=extract_config,\n        label=name)\n\n    algo_req[\'name\'] = name\n    algo_req[\'should_publish_extract_dataset\'] = should_publish_extract_dataset\n    algo_req[\'should_publish_history_dataset\'] = should_publish_history_dataset\n    algo_req[\'should_publish_report_dataset\'] = should_publish_report_dataset\n\n    algo_res = build_result.build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec=None)\n\n    if run_on_engine:\n        rec = {\n            \'algo_req\': algo_req,\n            \'task_id\': None\n        }\n        task_name = (\n            \'analysis_engine.work_tasks.\'\n            \'task_run_algo.task_run_algo\')\n        if verbose:\n            log.info(f\'starting distributed algo task={task_name}\')\n        elif debug:\n            log.info(\n                \'starting distributed algo by publishing to \'\n                f\'task={task_name} broker={auth_url} backend={backend_url}\')\n\n        # Get the Celery app\n        app = get_celery_app.get_celery_app(\n            name=__name__,\n            auth_url=auth_url,\n            backend_url=backend_url,\n            path_to_config_module=path_to_config_module,\n            ssl_options=ssl_options,\n            transport_options=transport_options,\n            include_tasks=include_tasks)\n\n        if debug:\n            log.info(\n                f\'calling distributed algo task={task_name} \'\n                f\'request={ae_consts.ppj(algo_req)}\')\n        elif verbose:\n            log.info(f\'calling distributed algo task={task_name}\')\n\n        job_id = app.send_task(\n            task_name,\n            (algo_req,))\n        if verbose:\n            log.info(f\'calling task={task_name} - success job_id={job_id}\')\n        rec[\'task_id\'] = job_id\n        algo_res = build_result.build_result(\n            status=ae_consts.SUCCESS,\n            err=None,\n            rec=rec)\n        return algo_res\n    # end of run_on_engine\n\n    if use_custom_algo:\n        if verbose:\n            log.info(\n                f\'inspecting {custom_algo_module} for class {module_name}\')\n        use_class_member_object = None\n        for member in inspect.getmembers(custom_algo_module):\n            if module_name in str(member):\n                if verbose:\n                    log.info(f\'start {name} with {member[1]}\')\n                use_class_member_object = member\n                break\n        # end of looking over the class definition but did not find it\n\n        if use_class_member_object:\n            new_algo_object = member[1](\n                **algo_req)\n        else:\n            err = (\n                \'did not find a derived analysis_engine.algo.BaseAlgo \'\n                f\'class in the module file={mod_path} \'\n                f\'for ticker={ticker} algo_name={name}\')\n\n            if verbose or debug:\n                log.error(err)\n\n            return build_result.build_result(\n                status=ae_consts.ERR,\n                err=err,\n                rec=None)\n        # end of finding a valid algorithm object\n    else:\n        new_algo_object = ae_algo.BaseAlgo(\n            **algo_req)\n    # if using a custom module path or the BaseAlgo\n\n    if new_algo_object:\n        # heads up - logging this might have passwords in the algo_req\n        # log.debug(\n        #     f\'{name} algorithm request: {algo_req}\')\n        if verbose:\n            log.info(\n                f\'{name} - run ticker={ticker} from {use_start_date} \'\n                f\'to {use_end_date}\')\n        algo_res = run_algo.run_algo(\n            algo=new_algo_object,\n            raise_on_err=raise_on_err,\n            **algo_req)\n        algo_res[\'algo\'] = new_algo_object\n        if verbose:\n            log.info(\n                f\'{name} - run ticker={ticker} from {use_start_date} \'\n                f\'to {use_end_date}\')\n        if custom_algo_module:\n            if verbose:\n                log.info(\n                    f\'{name} - done run_algo \'\n                    f\'custom_algo_module={custom_algo_module} \'\n                    f\'module_name={module_name} ticker={ticker} \'\n                    f\'from {use_start_date} to {use_end_date}\')\n        else:\n            if verbose:\n                log.info(\n                    f\'{name} - done run_algo BaseAlgo ticker={ticker} \'\n                    f\'from {use_start_date} to {use_end_date}\')\n    else:\n        err = (\n            \'missing a derived analysis_engine.algo.BaseAlgo \'\n            f\'class in the module file={mod_path} for ticker={ticker} \'\n            f\'algo_name={name}\')\n        return build_result.build_result(\n            status=ae_consts.ERR,\n            err=err,\n            rec=None)\n    # end of finding a valid algorithm object\n\n    algo = algo_res.get(\n        \'algo\',\n        None)\n\n    if not algo:\n        err = (\n            f\'failed creating algorithm object - ticker={ticker} \'\n            f\'status={ae_consts.get_status(status=algo_res[""status""])} \'\n            f\'error={algo_res[""err""]} algo name={name} \'\n            f\'custom_algo_module={custom_algo_module} \'\n            f\'module_name={module_name} \'\n            f\'from {use_start_date} to {use_end_date}\')\n        return build_result.build_result(\n            status=ae_consts.ERR,\n            err=err,\n            rec=None)\n\n    if should_publish_extract_dataset or dataset_publish_extract:\n        s3_log = \'\'\n        redis_log = \'\'\n        file_log = \'\'\n        use_log = \'publish\'\n\n        if (extract_config[\'redis_address\'] and\n                extract_config[\'redis_db\'] >= 0 and\n                extract_config[\'redis_key\']):\n            redis_log = (\n                f\'redis://{extract_config[""redis_address""]}\'\n                f\'@{extract_config[""redis_db""]}/{extract_config[""redis_key""]}\')\n            use_log += f\' {redis_log}\'\n        else:\n            extract_config[\'redis_enabled\'] = False\n        if (extract_config[\'s3_address\'] and\n                extract_config[\'s3_bucket\'] and\n                extract_config[\'s3_key\']):\n            s3_log = (\n                f\'s3://{extract_config[""s3_address""]}\'\n                f\'/{extract_config[""s3_bucket""]}/{extract_config[""s3_key""]}\')\n            use_log += f\' {s3_log}\'\n        else:\n            extract_config[\'s3_enabled\'] = False\n        if extract_config[\'output_file\']:\n            file_log = f\'file:{extract_config[""output_file""]}\'\n            use_log += f\' {file_log}\'\n\n        if verbose:\n            log.info(\n                f\'{name} - publish - start ticker={ticker} \'\n                f\'algorithm-ready {use_log}\')\n\n        publish_status = algo.publish_input_dataset(\n            **extract_config)\n        if publish_status != ae_consts.SUCCESS:\n            msg = (\n                \'failed to publish algorithm-ready datasets \'\n                f\'with status {ae_consts.get_status(status=publish_status)} \'\n                f\'attempted to {use_log}\')\n            log.error(msg)\n            return build_result.build_result(\n                status=ae_consts.ERR,\n                err=err,\n                rec=None)\n\n        if verbose:\n            log.info(\n                f\'{name} - publish - done ticker={ticker} \'\n                f\'algorithm-ready {use_log}\')\n    # if publish the algorithm-ready dataset\n\n    if should_publish_history_dataset or dataset_publish_history:\n        s3_log = \'\'\n        redis_log = \'\'\n        file_log = \'\'\n        use_log = \'publish\'\n\n        if (history_config[\'redis_address\'] and\n                history_config[\'redis_db\'] >= 0 and\n                history_config[\'redis_key\']):\n            redis_log = (\n                f\'redis://{history_config[""redis_address""]}\'\n                f\'@{history_config[""redis_db""]}/{history_config[""redis_key""]}\')\n            use_log += f\' {redis_log}\'\n        else:\n            history_config[\'redis_enabled\'] = False\n        if (history_config[\'s3_address\'] and\n                history_config[\'s3_bucket\'] and\n                history_config[\'s3_key\']):\n            s3_log = (\n                f\'s3://{history_config[""s3_address""]}\'\n                f\'/{history_config[""s3_bucket""]}/{history_config[""s3_key""]}\')\n            use_log += f\' {s3_log}\'\n        else:\n            history_config[\'s3_enabled\'] = False\n\n        if history_config[\'output_file\']:\n            file_log = f\'file:{history_config[""output_file""]}\'\n            use_log += f\' {file_log}\'\n\n        if verbose:\n            log.info(\n                f\'{name} - publish - start ticker={ticker} trading \'\n                f\'history {use_log}\')\n\n        publish_status = algo.publish_trade_history_dataset(\n            **history_config)\n        if publish_status != ae_consts.SUCCESS:\n            msg = (\n                \'failed to publish trading history datasets \'\n                f\'with status {ae_consts.get_status(status=publish_status)} \'\n                f\'attempted to {use_log}\')\n            log.error(msg)\n            return build_result.build_result(\n                status=ae_consts.ERR,\n                err=err,\n                rec=None)\n\n        if verbose:\n            log.info(\n                f\'{name} - publish - done ticker={ticker} trading \'\n                f\'history {use_log}\')\n    # if publish an trading history dataset\n\n    if should_publish_report_dataset or dataset_publish_report:\n        s3_log = \'\'\n        redis_log = \'\'\n        file_log = \'\'\n        use_log = \'publish\'\n\n        if (report_config[\'redis_address\'] and\n                report_config[\'redis_db\'] >= 0 and\n                report_config[\'redis_key\']):\n            redis_log = (\n                f\'redis://{report_config[""redis_address""]}\'\n                f\'@{report_config[""redis_db""]}/{report_config[""redis_key""]}\')\n            use_log += f\' {redis_log}\'\n        else:\n            report_config[\'redis_enabled\'] = False\n        if (report_config[\'s3_address\'] and\n                report_config[\'s3_bucket\'] and\n                report_config[\'s3_key\']):\n            s3_log = (\n                f\'s3://{report_config[""s3_address""]}\'\n                f\'/{report_config[""s3_bucket""]}/{report_config[""s3_key""]}\')\n            use_log += f\' {s3_log}\'\n        else:\n            report_config[\'s3_enabled\'] = False\n        if report_config[\'output_file\']:\n            file_log = f\'file:{report_config[""output_file""]}\'\n            use_log += f\' {file_log}\'\n\n        if verbose:\n            log.info(\n                f\'{name} - publishing ticker={ticker} trading performance \'\n                f\'report {use_log}\')\n\n        publish_status = algo.publish_report_dataset(\n            **report_config)\n        if publish_status != ae_consts.SUCCESS:\n            msg = (\n                \'failed to publish trading performance report datasets \'\n                f\'with status {ae_consts.get_status(status=publish_status)} \'\n                f\'attempted to {use_log}\')\n            log.error(msg)\n            return build_result.build_result(\n                status=ae_consts.ERR,\n                err=err,\n                rec=None)\n\n        if verbose:\n            log.info(\n                f\'{name} - publish - done ticker={ticker} trading performance \'\n                f\'report {use_log}\')\n    # if publish an trading performance report dataset\n\n    if verbose:\n        log.info(\n            f\'{name} - done publishing datasets for ticker={ticker} \'\n            f\'from {use_start_date} to {use_end_date}\')\n\n    return algo_res\n# end of run_custom_algo\n'"
analysis_engine/s3_read_contents_from_key.py,0,"b'""""""\nWrapper for downloading an S3 key as a string\n""""""\n\nimport json\nimport zlib\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef s3_read_contents_from_key(\n        s3,\n        s3_bucket_name,\n        s3_key,\n        encoding=\'utf-8\',\n        convert_as_json=True,\n        compress=False):\n    """"""s3_read_contents_from_key\n\n    Download the S3 key contents as a string. This\n    will raise exceptions.\n\n    :param s3: existing S3 object\n    :param s3_bucket_name: bucket name\n    :param s3_key: S3 key\n    :param encoding: utf-8 by default\n    :param convert_to_json: auto-convert to a dict\n    :param compress: decompress using ``zlib``\n    """"""\n\n    log.debug(\n        f\'getting s3.Object({s3_bucket_name}, {s3_key})\')\n    s3_obj = s3.Object(s3_bucket_name, s3_key)\n\n    raw_contents = None\n    if compress:\n        log.debug(\n            f\'zlib.decompress(\'\n            f\'s3_obj.get()[""Body""].read()\'\n            f\'.decode({encoding})\')\n        raw_contents = zlib.decompress(\n            s3_obj.get()[\'Body\'].read()).decode(\n                encoding)\n    else:\n        log.debug(\n            f\'s3_obj.get()[""Body""].read().decode({encoding})\')\n        s3_contents = s3_obj.get()[\'Body\'].read()\n        raw_contents = s3_contents.decode(encoding)\n    # if compressed or not\n\n    data = None\n    if convert_as_json:\n        data = json.loads(raw_contents)\n    else:\n        data = raw_contents\n    # if convert to json or not\n    return data\n# end of s3_read_contents_from_key\n'"
analysis_engine/send_to_slack.py,0,"b'""""""\nHelper for extracting details from Celery task and\nsending it to a slack webhook.\n\nSupported environment variables:\n\n::\n\n    # slack webhook\n    export SLACK_WEBHOOK=https://hooks.slack.com/services/\n\n""""""\n\nimport os\nimport json\nimport requests\nimport tabulate as tb\nimport analysis_engine.consts as ae_consts\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\npublish_msg_error = (\n    \'please add a SLACK_WEBHOOK environment variable to publish messages\')\nmessage_types = {\n    \'success\': [{""color"": ""good"", ""title"": ""SUCCESS""}],\n    \'failure\': [{""color"": ""danger"", ""title"": ""FAILED""}],\n    \'message\': [{""title"": ""MESSAGE""}]\n}\n\n\ndef post_success(msg,\n                 jupyter=False,\n                 block=False,\n                 full_width=False):\n    """"""Post a SUCCESS message to slack\n\n    :param msg: A string, list, or dict to send to slack\n    """"""\n    return get_response(\'success\',\n                        msg,\n                        jupyter=jupyter,\n                        block=block,\n                        full_width=full_width)\n\n\ndef post_failure(msg,\n                 jupyter=False,\n                 block=False,\n                 full_width=False):\n    """"""Post a FAILURE message to slack\n\n    :param msg: A string, list, or dict to send to slack\n    """"""\n    return get_response(\'failure\',\n                        msg,\n                        jupyter=jupyter,\n                        block=block,\n                        full_width=full_width)\n\n\ndef post_message(msg,\n                 jupyter=False,\n                 block=False,\n                 full_width=False):\n    """"""Post any message to slack\n\n    :param msg: A string, list, or dict to send to slack\n    """"""\n    return get_response(\'message\',\n                        msg,\n                        jupyter=jupyter,\n                        block=block,\n                        full_width=full_width)\n\n\ndef get_response(msg_type,\n                 msg,\n                 jupyter=False,\n                 block=False,\n                 full_width=False):\n    """"""Attempt to create and send any message type to slack\n\n    :param msg_type: A string of either \'success\', \'failure\', or \'message\'\n    :param msg: A string, list, or dict to send to slack\n    """"""\n    response = {\'status\': ae_consts.FAILED}\n    if msg_type in message_types:\n        if not os.getenv(\'SLACK_WEBHOOK\', False):\n            log.info(f\'post_{msg_type} - {publish_msg_error}\')\n            return response\n        if msg:\n            attachments = [{""attachments"": message_types[msg_type]}]\n            fields = parse_msg(msg, block=block)\n            if fields:\n                if full_width:\n                    attachments.append({""text"": fields[0].pop(""value"")})\n                else:\n                    attachments[0][""attachments""][0][""fields""] = fields\n                response = post(attachments, jupyter=jupyter)\n    return response\n\n\ndef parse_msg(msg, block=False):\n    """"""Create an array of fields for slack from the msg type\n\n    :param msg: A string, list, or dict to massage for sending to slack\n    """"""\n    if type(msg) is str:\n        if block:\n            return [{""value"": f""```{msg}```""}]\n        return [{""value"": msg}]\n    elif type(msg) is list:\n        if block:\n            string_list = \'\\n\'.join(f""{str(x)}"" for x in msg)\n            return [{""value"": f""```{string_list}```""}]\n        return [{""value"": str(x)} for x in msg]\n    elif type(msg) is dict:\n        if block:\n            string_dict = \'\\n\'.join(\n                f""{str(k)}: {str(v)}"" for k, v in msg.items())\n            return [{""value"": f""```{string_dict}```""}]\n        return [{""value"": f""{str(k)}: {str(v)}""} for k, v in msg.items()]\n    return None\n\n\ndef post(attachments, jupyter=False):\n    """"""Send created attachments to slack\n\n    :param attachments: Values to post to slack\n    """"""\n    SLACK_WEBHOOK = ae_consts.ev(\'SLACK_WEBHOOK\', None)\n    result = {\'status\': ae_consts.FAILED}\n    if not os.getenv(\'SLACK_WEBHOOK\', False):\n        log.info(f\'post - {publish_msg_error}\')\n        return result\n    if attachments and SLACK_WEBHOOK:\n        try:\n            # if not jupyter:\n            #     log.debug(f\'Attempting to post attachments={attachments} \'\n            #               \'to slack_webhook exists\')\n            for attachment in attachments:\n                r = requests.post(SLACK_WEBHOOK, data=json.dumps(attachment))\n                if str(r.status_code) == ""200"":\n                    # log.info((\n                    #   f\'\'\'Successful post of attachment={\n                    #       attachment if not jupyter else\n                    #       True if attachment else False} \'\'\'\n                    #   \'to slack_webhook\'))\n                    result[\'status\'] = ae_consts.SUCCESS\n                else:\n                    log.error(\n                        f\'\'\'Failed to post attachment={\n                            attachment if not jupyter else\n                            True if attachment else False} \'\'\'\n                        f\'with status_code={r.status_code}\')\n                    result[\'status\'] = ae_consts.FAILED\n                    break\n        except Exception as e:\n            log.error(\n                f\'\'\'Failed to post attachments={\n                    attachments if not jupyter else\n                    True if attachments else False} \'\'\'\n                f\'with ex={e}\')\n            result[\'status\'] = ae_consts.ERR\n            result[\'err\'] = e\n    else:\n        log.info(\n            \'Skipping post to slack due to missing \'\n            f\'\'\'attachments={\n                attachments if not jupyter else\n                True if attachments else False} or SLACK_WEBHOOK \'\'\'\n            f\'missing={False if SLACK_WEBHOOK else True}\')\n    return result\n\n\ndef post_df(\n        df,\n        columns=None,\n        block=True,\n        jupyter=True,\n        full_width=True,\n        tablefmt=\'github\'):\n    """"""post_df\n\n    Post a ``pandas.DataFrame`` to Slack\n\n    :param df: ``pandas.DataFrame`` object\n    :param columns: ordered list of columns to for the table\n                    header row\n                    (``None`` by default)\n    :param block: bool for\n                  post as a Slack-formatted block ```like this```\n                  (``True`` by default)\n    :param jupyter: bool for\n                    jupyter attachment handling\n                    (``True`` by default)\n    :param full_width: bool to ensure the width is preserved\n                       the Slack message  (``True`` by default)\n    :param tablefmt: string for table format (``github`` by default).\n                     Additional format values can be found on:\n                     https://bitbucket.org/astanin/python-tabulate\n    """"""\n\n    if not os.getenv(\'SLACK_WEBHOOK\', False):\n        log.info(f\'post_df - {publish_msg_error}\')\n        return\n    if not hasattr(df, \'index\'):\n        log.debug(\'post_df - no df \')\n        return\n\n    log.debug(\n        f\'post_df - df.index={len(df.index)} \'\n        f\'columns={columns} fmt={tablefmt}\')\n\n    msg = None\n    if columns:\n        msg = tb.tabulate(\n            df[columns],\n            headers=columns,\n            tablefmt=tablefmt)\n    else:\n        msg = tb.tabulate(\n            df,\n            tablefmt=tablefmt)\n    # end of if/else\n\n    post_success(\n        msg=msg,\n        block=block,\n        jupyter=jupyter,\n        full_width=full_width)\n# end of post_df\n\n\ndef post_plot(plot,\n              filename=\'tmp\',\n              title=None):\n    """"""post_plot\n\n    Post a matlibplot plot to Slack\n\n    :param plot: matlibplot pyplot figure\n    :param filename: filename of plot\n    :param title: Title for slack plot postiing\n    """"""\n\n    result = {\'status\': ae_consts.FAILED}\n    if not os.getenv(\'SLACK_ACCESS_TOKEN\', False):\n        log.info(\n            \'post_plot - please add a SLACK_ACCESS_TOKEN environment \'\n            \'variable to publish plots\')\n        return result\n    if not filename:\n        log.info(\n            \'post_plot - no filename provided\'\n        )\n        return result\n    url = \'https://slack.com/api/files.upload\'\n    filename = f\'{filename.split(""."")[0]}.png\'\n    channels = [f\'#{channel}\' for channel in os.getenv(\n        \'SLACK_PUBLISH_PLOT_CHANNELS\',\n        \'general\').split(\',\')]\n    try:\n        log.info(f\'post_plot - temporarily saving plot: {filename}\')\n        plot.savefig(filename)\n        tmp_file = {\n            \'file\': (filename, open(filename, \'rb\'), \'png\')\n        }\n        payload = {\n            \'channels\':        channels,\n            \'filename\':        filename,\n            \'title\':           title if title else filename,\n            \'token\':           os.getenv(\'SLACK_ACCESS_TOKEN\'),\n        }\n        log.info(f\'post_plot - sending payload: {payload} to url: {url}\')\n        r = requests.post(url,\n                          params=payload,\n                          files=tmp_file)\n        log.info(f\'post_plot - removing temporarily saved plot: {filename}\')\n        os.remove(filename)\n        if str(r.status_code) == ""200"":\n            log.info(f\'post_plot - posted plot to slack channels: {channels}\')\n            result[\'status\'] = ae_consts.SUCCESS\n    except Exception as e:\n        log.info(f\'post_plot - failed with Exception: {e}\')\n        result[\'status\'] = ae_consts.ERR\n        result[\'err\'] = e\n    return result\n# end of post_plot\n'"
analysis_engine/set_data_in_redis_key.py,0,"b'""""""\nHelper for setting data in redis\n\nDebug redis calls with:\n\n::\n\n    export DEBUG_REDIS=1\n""""""\n\nimport json\nimport redis\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.build_result as build_result\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef set_data_in_redis_key(\n        label=None,\n        data=None,\n        client=None,\n        host=None,\n        port=None,\n        password=None,\n        db=None,\n        key=None,\n        expire=None,\n        px=None,\n        nx=False,\n        xx=False,\n        already_compressed=False,\n        serializer=\'json\',\n        encoding=\'utf-8\'):\n    """"""set_data_in_redis_key\n\n    :param label: log tracking label\n    :param data: data to set in redis\n    :param client: initialized redis client\n    :param host: not used yet - redis host\n    :param port: not used yet - redis port\n    :param password: not used yet - redis password\n    :param db: not used yet - redis db\n    :param key: not used yet - redis key\n    :param expire: redis expire\n    :param px: redis px\n    :param nx: redis nx\n    :param xx: redis xx\n    :param serializer: not used yet - support for future\n        pickle objects in redis\n    :param already_compressed: bool for handling\n        compression to string already has happend\n    :param encoding: format of the encoded key in redis\n    """"""\n\n    data_str = None\n    encoded_data = None\n\n    rec = {}\n    res = build_result.build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec=rec)\n\n    log_id = label if label else \'set-redis\'\n\n    try:\n        log.debug(\n            f\'{log_id} serializer={serializer} encoding={encoding} \'\n            f\'for key={key}\')\n        if already_compressed:\n            encoded_data = data\n        else:\n            if serializer == \'json\':\n                data_str = json.dumps(data)\n                encoded_data = data_str.encode(encoding)\n            else:\n                encoded_data = None\n                err = (\n                    f\'{log_id} unsupported serializer={serializer} \'\n                    f\'encoding={encoding} key={key}\')\n                log.error(err)\n                res = build_result.build_result(\n                    status=ae_consts.ERR,\n                    err=err,\n                    rec=rec)\n                return res\n        # if supported serializer\n\n        if encoded_data:\n            if ae_consts.ev(\'DEBUG_REDIS\', \'0\') == \'1\':\n                log.debug(f\'{log_id} set - key={key} data={encoded_data}\')\n\n            use_client = client\n            if not use_client:\n                log.debug(\n                    f\'{log_id} set key={key} new client={host}:{port}@{db}\')\n                use_client = redis.Redis(\n                    host=host,\n                    port=port,\n                    password=password,\n                    db=db)\n            else:\n                log.debug(f\'{log_id} set key={key} client\')\n            # create Redis client if not set\n\n            use_client.set(\n                name=key,\n                value=encoded_data,\n                ex=expire,\n                px=px,\n                nx=nx,\n                xx=xx)\n            res = build_result.build_result(\n                status=ae_consts.SUCCESS,\n                err=None,\n                rec=rec)\n            return res\n        else:\n            err = f\'{log_id} no data for key={key}\'\n            log.error(err)\n            res = build_result.build_result(\n                status=ae_consts.ERR,\n                err=err,\n                rec=rec)\n            return res\n        # end of if have data to set\n    except Exception as e:\n        err = (\n            f\'{log_id} failed - redis set from data={str(data)[0:200]} \'\n            f\'encoded_data={str(encoded_data)[0:200]} \'\n            f\'key={key} ex={e}\')\n        log.error(err)\n        res = build_result.build_result(\n            status=ae_consts.ERR,\n            err=err,\n            rec=rec)\n    # end of try/ex for setting redis data\n\n    return res\n# end of set_data_in_redis_key\n'"
analysis_engine/show_dataset.py,0,"b'""""""\nShow an algorithm dataset from file, s3 or redis\n\nSupported Datasets:\n\n- ``SA_DATASET_TYPE_ALGO_READY`` - Algorithm-ready datasets\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.load_dataset as load_dataset\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef show_dataset(\n        algo_dataset=None,\n        dataset_type=ae_consts.SA_DATASET_TYPE_ALGO_READY,\n        serialize_datasets=ae_consts.DEFAULT_SERIALIZED_DATASETS,\n        path_to_file=None,\n        compress=False,\n        encoding=\'utf-8\',\n        redis_enabled=True,\n        redis_key=None,\n        redis_address=None,\n        redis_db=None,\n        redis_password=None,\n        redis_expire=None,\n        redis_serializer=\'json\',\n        redis_encoding=\'utf-8\',\n        s3_enabled=True,\n        s3_key=None,\n        s3_address=None,\n        s3_bucket=None,\n        s3_access_key=None,\n        s3_secret_key=None,\n        s3_region_name=None,\n        s3_secure=False,\n        slack_enabled=False,\n        slack_code_block=False,\n        slack_full_width=False,\n        verbose=False):\n    """"""show_dataset\n\n    Show a supported dataset\'s internal structure and preview some\n    of the values to debug mapping, serialization issues\n\n    :param algo_dataset: optional - already loaded algorithm-ready dataset\n    :param dataset_type: optional - dataset type\n        (default is ``SA_DATASET_TYPE_ALGO_READY``)\n    :param serialize_datasets: optional - list of dataset names to\n        deserialize in the dataset\n    :param path_to_file: optional - path to an algorithm-ready dataset\n        in a file\n    :param compress: optional - boolean flag for decompressing\n        the contents of the ``path_to_file`` if necessary\n        (default is ``False`` and algorithms\n        use ``zlib`` for compression)\n    :param encoding: optional - string for data encoding\n\n    **(Optional) Redis connectivity arguments**\n\n    :param redis_enabled: bool - toggle for auto-caching all\n        datasets in Redis\n        (default is ``True``)\n    :param redis_key: string - key to save the data in redis\n        (default is ``None``)\n    :param redis_address: Redis connection string format: ``host:port``\n        (default is ``localhost:6379``)\n    :param redis_db: Redis db to use\n        (default is ``0``)\n    :param redis_password: optional - Redis password\n        (default is ``None``)\n    :param redis_expire: optional - Redis expire value\n        (default is ``None``)\n    :param redis_serializer: not used yet - support for future\n        pickle objects in redis\n    :param redis_encoding: format of the encoded key in redis\n\n    **(Optional) Minio (S3) connectivity arguments**\n\n    :param s3_enabled: bool - toggle for auto-archiving on Minio (S3)\n        (default is ``True``)\n    :param s3_key: string - key to save the data in redis\n        (default is ``None``)\n    :param s3_address: Minio S3 connection string format: ``host:port``\n        (default is ``localhost:9000``)\n    :param s3_bucket: S3 Bucket for storing the artifacts\n        (default is ``dev``) which should be viewable on a browser:\n        http://localhost:9000/minio/dev/\n    :param s3_access_key: S3 Access key\n        (default is ``trexaccesskey``)\n    :param s3_secret_key: S3 Secret key\n        (default is ``trex123321``)\n    :param s3_region_name: S3 region name\n        (default is ``us-east-1``)\n    :param s3_secure: Transmit using tls encryption\n        (default is ``False``)\n\n    **(Optional) Slack arguments**\n\n    :param slack_enabled: optional - boolean for\n        publishing to slack\n    :param slack_code_block: optional - boolean for\n        publishing as a code black in slack\n    :param slack_full_width: optional - boolean for\n        publishing as a to slack using the full\n        width allowed\n\n    Additonal arguments\n\n    :param verbose: optional - bool for increasing\n        logging\n    """"""\n\n    use_ds = algo_dataset\n    if not use_ds:\n        log.info(\n            f\'loading from file={path_to_file} s3={s3_key} redis={redis_key}\')\n        use_ds = load_dataset.load_dataset(\n            dataset_type=dataset_type,\n            compress=compress,\n            encoding=redis_encoding,\n            path_to_file=path_to_file,\n            s3_key=s3_key,\n            s3_address=s3_address,\n            s3_bucket=s3_bucket,\n            s3_access_key=s3_access_key,\n            s3_secret_key=s3_secret_key,\n            s3_region_name=s3_region_name,\n            s3_secure=s3_secure,\n            redis_key=redis_key,\n            redis_address=redis_address,\n            redis_db=redis_db,\n            redis_password=redis_password,\n            redis_expire=redis_expire,\n            redis_serializer=redis_serializer,\n            serialize_datasets=serialize_datasets)\n\n        if not use_ds:\n            log.error(\n                f\'unable to load a dataset from file={path_to_file} \'\n                f\'s3={s3_key} redis={redis_key}\')\n            return None\n    # load if not created\n\n    if dataset_type == ae_consts.SA_DATASET_TYPE_ALGO_READY:\n        print(\'-----------------------------------\')\n        for root_key in use_ds:\n            print(root_key)\n        all_dates = []\n        all_ids = []\n        first_node = None\n        last_node = None\n        end_nodes = []\n        for root_key in use_ds:\n            second_layer = use_ds[root_key]\n            for ds in second_layer:\n                if \'date\' in ds:\n                    if len(all_dates) == 0:\n                        print(\'\\ndates found in dataset\')\n                    cur_date = ds.get(\n                        \'date\',\n                        None)\n                    if cur_date:\n                        print(cur_date)\n                        all_dates.append(cur_date)\n                if not first_node:\n                    first_node = ds\n                end_nodes.append(ds)\n                last_node = ds\n                if \'id\' in ds:\n                    if len(all_ids) == 0:\n                        print(\'\\nids in the file\')\n                    cur_id = ds.get(\n                        \'id\',\n                        None)\n                    if cur_id:\n                        print(cur_id)\n                        all_ids.append(cur_id)\n        if first_node and last_node:\n            show_first = {}\n            for ds_key in first_node:\n                if ds_key == \'data\':\n                    show_first[ds_key] = {}\n                    for ds_name in first_node[ds_key]:\n                        print(f\'first_node has dataset with name: {ds_name}\')\n                        show_first[ds_key][ds_name] = \'EMPTY_DF\'\n                        if hasattr(\n                                first_node[ds_key][ds_name],\n                                \'index\'):\n                            show_first[ds_key][ds_name] = (\n                                \'pd.DataFrame() rows=\'\n                                f\'{len(first_node[ds_key][ds_name].index)}\')\n                else:\n                    show_first[ds_key] = first_node[ds_key]\n            print(f\'\\nfirst node:\\n{ae_consts.ppj(show_first)}\\n\')\n\n            num_records = len(all_ids)\n            cur_cell = num_records - 4\n            for cur_node in end_nodes[-5:]:\n                show_node = {}\n                for ds_key in cur_node:\n                    if ds_key == \'data\':\n                        show_node[ds_key] = {}\n                        for ds_name in cur_node[ds_key]:\n                            show_node[ds_key][ds_name] = \'EMPTY_DF\'\n                            if hasattr(\n                                    cur_node[ds_key][ds_name],\n                                    \'index\'):\n                                show_node[ds_key][ds_name] = (\n                                    \'pd.DataFrame() rows=\'\n                                    f\'{len(cur_node[ds_key][ds_name].index)}\')\n                    else:\n                        show_node[ds_key] = cur_node[ds_key]\n                # end of show cur_node\n                print(\n                    f\'node={cur_cell}/{num_records} values:\'\n                    f\'\\n{ae_consts.ppj(show_node)}\\n\')\n                cur_cell += 1\n            # end of end_nodes\n        else:\n            if not first_node:\n                print(\'missing first node in dataset\')\n            if not last_node:\n                print(\'missing last node in dataset\')\n        if len(all_dates) > 0:\n            print(\n                f\'root_keys={use_ds} from {all_dates[0]} \'\n                f\'to {all_dates[-1]}\')\n        else:\n            print(f\'root_keys={use_ds} missing dates\')\n\n        print(\'-----------------------------------\')\n\n    return use_ds\n# end of show_dataset\n'"
analysis_engine/start_worker.py,0,"b""#!/usr/bin/env python\n\nimport celery\nimport analysis_engine.consts as consts\nimport analysis_engine.work_tasks.get_celery_app as get_celery_app\nimport spylunking.log.setup_logging as log_utils\n\n\n# Disable celery log hijacking\n# https://github.com/celery/celery/issues/2509\n@celery.signals.setup_logging.connect\ndef setup_celery_logging(**kwargs):\n    pass\n\n\nlog = log_utils.build_colorized_logger(\n    name=consts.APP_NAME,\n    log_config_path=consts.LOG_CONFIG_PATH)\n\nlog.info(f'start - {consts.APP_NAME}')\n\nlog.info(\n    f'broker={consts.WORKER_BROKER_URL} backend={consts.WORKER_BACKEND_URL} '\n    f'config={consts.WORKER_CELERY_CONFIG_MODULE} '\n    f'include_tasks={consts.WORKER_TASKS}')\n\n# Get the Celery app from the project's get_celery_app module\napp = get_celery_app.get_celery_app(\n    name=consts.APP_NAME,\n    path_to_config_module=consts.WORKER_CELERY_CONFIG_MODULE,\n    auth_url=consts.WORKER_BROKER_URL,\n    backend_url=consts.WORKER_BACKEND_URL,\n    include_tasks=consts.INCLUDE_TASKS)\n\nlog.info('starting celery')\napp.start()\n\nlog.info(f'end - {consts.APP_NAME}')\n"""
analysis_engine/url_helper.py,0,"b'""""""\nHelpful wrapper taken from:\nhttps://www.peterbe.com/plog/best-practice-with-retries-with-requests\n""""""\n\nimport requests\nimport requests.adapters as adapters\nimport requests.packages.urllib3.util.retry as requests_retry\n\n\ndef url_helper(\n        sess=None,\n        retries=10,\n        backoff_factor=0.3,\n        status_forcelist=(500, 502, 504)):\n    """"""url_helper\n\n    :param sess: ``requests.Session``\n        object like\n\n        .. code-block:: python\n\n            s = requests.Session()\n            s.auth = (\'user\', \'pass\')\n            s.headers.update({\'x-test\': \'true\'})\n\n            response = url_helper(sesssion=s).get(\n                \'https://www.peterbe.com\'\n            )\n\n    :param retries: number of retries\n        default is ``3``\n    :param backoff_factor: seconds per attempt\n        default is ``0.3``\n    :param status_forcelist: optional tuple list\n        of retry error HTTP status codes\n        default is ``500, 502, 504``\n    """"""\n    session = sess or requests.Session()\n    retry = requests_retry.Retry(\n        total=retries,\n        read=retries,\n        connect=retries,\n        backoff_factor=backoff_factor,\n        status_forcelist=status_forcelist,\n    )\n    adapter = adapters.HTTPAdapter(max_retries=retry)\n    session.mount(\'http://\', adapter)\n    session.mount(\'https://\', adapter)\n    return session\n# end of url_helper\n'"
analysis_engine/utils.py,0,"b'""""""\nDate utils\n""""""\n\nimport datetime\nimport analysis_engine.consts as ae_consts\n\n\ndef last_close():\n    """"""last_close\n\n    Get last trading close time as a python ``datetime``\n\n    How it works:\n\n    - During market hours the returned ``datetime`` will be\n        ``datetime.datetime.utcnow() - datetime.timedelta(hours=5)``\n    - Before or after market hours, the returned ``datetime``\n        will be 4:00 PM EST on the previous trading day which\n        could be a Friday if this is called on a Saturday or Sunday.\n\n    .. note:: does not detect holidays and non-trading\n        days yet and assumes the system time is\n        set to EST or UTC\n    """"""\n    now = (\n        datetime.datetime.utcnow() - datetime.timedelta(hours=5))\n    today = now.date()\n    close = datetime.datetime(\n        year=today.year,\n        month=today.month,\n        day=today.day,\n        hour=16)\n    market_start_time = datetime.datetime(\n        year=today.year,\n        month=today.month,\n        day=today.day,\n        hour=9,\n        minute=30,\n        second=0)\n    market_end_time = datetime.datetime(\n        year=today.year,\n        month=today.month,\n        day=today.day,\n        hour=16,\n        minute=0,\n        second=0)\n\n    if today.weekday() == 5:\n        return close - datetime.timedelta(days=1)\n    elif today.weekday() == 6:\n        return close - datetime.timedelta(days=2)\n    elif market_start_time <= now <= market_end_time:\n        return now\n    else:\n        if now.hour < 16:\n            close -= datetime.timedelta(days=1)\n            if close.weekday() == 5:  # saturday\n                return close - datetime.timedelta(days=1)\n            elif close.weekday() == 6:  # sunday\n                return close - datetime.timedelta(days=2)\n            return close\n        return close\n    # if/else\n# end of last_close\n\n\ndef get_last_close_str(\n        fmt=ae_consts.COMMON_DATE_FORMAT):\n    """"""get_last_close_str\n\n    Get the Last Trading Close Date as a string\n    with default formatting ae_consts.COMMON_DATE_FORMAT\n    (YYYY-MM-DD)\n\n    :param fmt: optional output format (default\n        ae_consts.COMMON_DATE_FORMAT)\n    """"""\n    return last_close().strftime(fmt)\n# end of get_last_close_str\n\n\ndef utc_now_str(\n        fmt=ae_consts.COMMON_TICK_DATE_FORMAT):\n    """"""utc_now_str\n\n    Get the UTC now as a string\n    with default formatting ae_consts.COMMON_TICK_DATE_FORMAT\n    (YYYY-MM-DD HH:MM:SS)\n\n    :param fmt: optional output format (default\n        ae_consts.COMMON_TICK_DATE_FORMAT)\n    """"""\n    return datetime.datetime.utcnow().strftime(\n        fmt)\n# end of utc_now_str\n\n\ndef utc_date_str(\n        fmt=ae_consts.COMMON_DATE_FORMAT):\n    """"""utc_date_str\n\n    Get the UTC date as a string\n    with default formatting ``COMMON_DATE_FORMAT``\n\n    :param fmt: optional output format (default\n        COMMON_DATE_FORMAT ``YYYY-MM-DD``)\n    """"""\n    return datetime.datetime.utcnow().strftime(\n        fmt)\n# end of utc_date_str\n\n\ndef get_date_from_str(\n        date_str,\n        fmt=ae_consts.COMMON_TICK_DATE_FORMAT):\n    """"""get_date_from_str\n\n    Convert a date to a string where the\n    default date formatting is ``ae_consts.COMMON_TICK_DATE_FORMAT``\n\n    :param date_str: string date value with a format of ``fmt``\n    :param fmt: date format ``YYYY-MM-DD HH:MM:SS`` by default\n    """"""\n\n    return datetime.datetime.strptime(\n        date_str,\n        fmt)\n# end of get_date_from_str\n\n\ndef get_trade_open_xticks_from_date_col(\n        date_list):\n    """"""get_trade_open_xticks_from_date_col\n\n    Call this to plot date strings in order\n    with just the trading open as the xticks\n\n    :param date_list: column from the ``pandas.DataFrame``\n        like ``date_list=df[\'minute\']``\n    """"""\n\n    open_of_trading_fmt = \'%Y-%m-%d 09:30:00\'\n\n    date_strings = []\n    date_labels = []\n\n    last_date = None\n    final_date = None\n    for idx, date in enumerate(date_list):\n        new_day = False\n        final_date = date\n        if not last_date:\n            last_date = date\n            next_open_of_trading = datetime.datetime.strptime(\n                (date + datetime.timedelta(days=1)).strftime(\n                    open_of_trading_fmt),\n                ae_consts.COMMON_TICK_DATE_FORMAT)\n            new_day = True\n        else:\n            if date > last_date and date > next_open_of_trading:\n                new_day = True\n                last_date = date\n                next_open_of_trading = datetime.datetime.strptime(\n                    (date + datetime.timedelta(days=1)).strftime(\n                        open_of_trading_fmt),\n                    ae_consts.COMMON_TICK_DATE_FORMAT)\n        if new_day:\n            date_strings.append(\n                date.strftime(ae_consts.COMMON_TICK_DATE_FORMAT))\n            date_labels.append(\n                date.strftime(\'%m-%d %H:30\'))\n        # end of adding new day\n    # end of for all dates to find the opens\n\n    if final_date:\n        should_add_last_point = False\n        if final_date > last_date:\n            should_add_last_point = True\n\n        if should_add_last_point:\n            date_strings.append(\n                final_date.strftime(ae_consts.COMMON_TICK_DATE_FORMAT))\n            date_labels.append(\n                final_date.strftime(\'%m-%d %H:%M\'))\n        # end of adding final point\n    # end of adding final point\n\n    return date_strings, date_labels\n# end of get_trade_open_xticks_from_date_col\n\n\ndef convert_epoch_to_datetime_string(\n        epoch,\n        fmt=ae_consts.COMMON_TICK_DATE_FORMAT,\n        use_utc=True):\n    """"""convert_epoch_to_datetime_string\n\n    :param epoch: integer epoch time value\n    :param fmt: optional string date format\n    :param use_utc: if utc or local time - default is ``True``\n    """"""\n\n    if use_utc:\n        return datetime.datetime.utcfromtimestamp(epoch).strftime(fmt)\n    else:\n        return datetime.datetime.fromtimestamp(epoch).strftime(fmt)\n# end of convert_epoch_to_datetime_string\n\n\ndef epoch_to_dt(\n        epoch,\n        use_utc=False,\n        convert_to_est=True):\n    """"""epoch_to_dt\n\n    Convert epoch milliseconds to datetime with configurable\n    offset hour calculations using the environment variable:\n    ``export EST_OFFSET_HOURS=<NUMBER_OF_HOURS>`` with\n    default ``5`` assuming running in UTC\n\n    :param epoch: integer milliseconds\n    :param use_utc: boolean to convert from ``UTC``\n        default is ``False``\n    :param convert_to_est: boolean to convert from\n        ``UTC`` to ``EST``\n    """"""\n\n    converted_time = None\n    if use_utc:\n        converted_time = datetime.datetime.utcfromtimestamp(\n            epoch)\n    else:\n        converted_time = datetime.datetime.fromtimestamp(\n            epoch)\n    # if/else\n\n    if convert_to_est:\n        converted_time = (\n            converted_time - datetime.timedelta(\n                hours=ae_consts.EST_OFFSET_HOURS))\n\n    return converted_time\n# end of epoch_to_dt\n\n\ndef get_days_between_dates(\n        from_historical_date,\n        last_close_to_use=None):\n    """"""get_days_between_dates\n\n    :param from_historical_date: historical date in time to start walking\n                                 forward until the last close datetime\n    :param last_close_to_use: starting date in time (left leg of window)\n    """"""\n    use_last_close = last_close_to_use\n    if not use_last_close:\n        use_last_close = last_close()\n\n    dates = []\n    while from_historical_date < last_close_to_use:\n        dates.append(from_historical_date)\n        from_historical_date += datetime.timedelta(\n            days=1)\n    return dates\n# end of get_days_between_dates\n'"
analysis_engine/write_to_file.py,0,"b'""""""\nHelper for writing to a file - used by unittests\n""""""\n\nimport os\nimport json\n\n\ndef write_to_file(\n        output_file,\n        data):\n    """"""write_to_file\n\n    write ``data`` to the ``output_file`` file\n\n    :param output_file: path to file\n    :param data: string contents for ``output_file``\n    """"""\n    use_str = data\n    try:\n        use_str = json.dumps(data)\n    except Exception:\n        use_str = data\n    with open(output_file, \'w\') as cur_file:\n        cur_file.write(use_str)\n\n    return os.path.exists(output_file)\n# end of write_to_file\n'"
tests/__init__.py,0,b''
tests/test_algo_with_indicators.py,0,"b'""""""\nTest file for classes and functions:\n\n- analysis_engine.algo.BaseAlgo\n- analysis_engine.indicators.indicator_processor\n- analysis_engine.indicators.build_indicator_node\n- analysis_engine.indicators.base_indicator\n- analysis_engine.indicators.load_indicator_from_module\n- analysis_engine.build_algo_request\n- analysis_engine.build_buy_order\n- analysis_engine.build_sell_order\n- analysis_engine.build_trade_history_entry\n\n""""""\n\nimport pandas as pd\nimport json\nimport mock\nimport analysis_engine.utils as ae_utils\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.algo as base_algo\nimport analysis_engine.mocks.mock_talib as mock_talib\nimport analysis_engine.mocks.base_test as base_test\n\n\ndef mock_write_to_file(\n        output_file,\n        data):\n    print(\n        \'mock - mock_write_to_file(\'\n        f\'output_file={output_file}, data={len(data)})\')\n    return True\n# end of mock_write_to_file\n\n\ndef mock_write_to_file_failed(\n        output_file,\n        data):\n    print(\n        \'mock - fail - mock_write_to_file(\'\n        f\'output_file={output_file}, data={len(data)})\')\n    return False\n# end of mock_write_to_file_failed\n\n\nclass TestAlgoWithIndicators(base_test.BaseTestCase):\n    """"""TestAlgoWithIndicators""""""\n\n    ticker = None\n    last_close_str = None\n\n    def setUp(\n            self):\n        """"""setUp""""""\n        self.ticker = \'SPY\'\n        self.timeseries = \'day\'\n        self.trade_strategy = \'count\'\n        self.daily_dataset = json.loads(\n            open(\'tests/datasets/spy-daily.json\', \'r\').read())\n        self.daily_df = pd.DataFrame(self.daily_dataset)\n        self.daily_df[\'date\'] = pd.to_datetime(\n            self.daily_df[\'date\'])\n        self.start_date_str = self.daily_df[\'date\'].iloc[0].strftime(\n            ae_consts.COMMON_TICK_DATE_FORMAT)\n        self.end_date_str = self.daily_df[\'date\'].iloc[-1].strftime(\n            ae_consts.COMMON_TICK_DATE_FORMAT)\n        self.minute_df = pd.DataFrame([])\n        self.options_df = pd.DataFrame([])\n        self.use_date = \'2018-11-05\'\n        self.dataset_id = f\'{self.ticker}_{self.use_date}\'\n        self.datasets = [\n            \'daily\'\n        ]\n        self.data = {}\n        self.data[self.ticker] = [\n            {\n                \'id\': self.dataset_id,\n                \'date\': self.use_date,\n                \'data\': {\n                    \'daily\': self.daily_df,\n                    \'minute\': self.minute_df,\n                    \'options\': self.options_df\n                }\n            }\n        ]\n        self.balance = 10000.00\n        self.last_close_str = ae_utils.get_last_close_str(\n            fmt=ae_consts.COMMON_DATE_FORMAT)\n        self.output_dir = (\n            \'/opt/sa/tests/datasets/algo\')\n\n        self.willr_close_path = (\n            \'analysis_engine/mocks/example_indicator_williamsr.py\')\n        self.willr_open_path = (\n            \'analysis_engine/mocks/example_indicator_williamsr_open.py\')\n        self.algo_config_dict = {\n            \'name\': \'test_5_days_ahead\',\n            \'algo_module_path\': None,\n            \'algo_version\': 1,\n            \'timeseries\': self.timeseries,\n            \'trade_strategy\': self.trade_strategy,\n            \'trade_horizon\': 5,\n            \'num_owned\': 10,\n            \'buy_shares\': 10,\n            \'balance\': 100000,\n            \'ticker\': \'SPY\',\n            \'verbose\': True,\n            \'verbose_processor\': True,\n            \'positions\': {\n                \'SPY\': {\n                    \'shares\': 10,\n                    \'buys\': [],\n                    \'sells\': []\n                }\n            },\n            \'buy_rules\': {\n                \'confidence\': 75,\n                \'min_indicators\': 3\n            },\n            \'sell_rules\': {\n                \'confidence\': 75,\n                \'min_indicators\': 3\n            },\n            \'indicators\': [\n                {\n                    \'name\': \'willr_-70_-30\',\n                    \'module_path\': self.willr_close_path,\n                    \'category\': \'technical\',\n                    \'type\': \'momentum\',\n                    \'uses_data\': \'daily\',\n                    \'high\': 0,\n                    \'low\': 0,\n                    \'close\': 0,\n                    \'open\': 0,\n                    \'willr_value\': 0,\n                    \'num_points\': 10,\n                    \'buy_below\': -70,\n                    \'sell_above\': -30,\n                    \'is_buy\': False,\n                    \'is_sell\': False,\n                    \'verbose\': True\n                },\n                {\n                    \'name\': \'willr_-80_-20\',\n                    \'module_path\': self.willr_close_path,\n                    \'category\': \'technical\',\n                    \'type\': \'momentum\',\n                    \'uses_data\': \'daily\',\n                    \'high\': 0,\n                    \'low\': 0,\n                    \'close\': 0,\n                    \'open\': 0,\n                    \'willr_value\': 0,\n                    \'num_points\': 10,\n                    \'buy_below\': -80,\n                    \'sell_above\': -20,\n                    \'is_buy\': False,\n                    \'is_sell\': False\n                },\n                {\n                    \'name\': \'willr_-90_-10\',\n                    \'module_path\': self.willr_close_path,\n                    \'category\': \'technical\',\n                    \'type\': \'momentum\',\n                    \'uses_data\': \'daily\',\n                    \'high\': 0,\n                    \'low\': 0,\n                    \'close\': 0,\n                    \'open\': 0,\n                    \'willr_value\': 0,\n                    \'num_points\': 10,\n                    \'buy_below\': -90,\n                    \'sell_above\': -10,\n                    \'is_buy\': False,\n                    \'is_sell\': False\n                },\n                {\n                    \'name\': \'willr_open_-80_-20\',\n                    \'module_path\': self.willr_open_path,\n                    \'category\': \'technical\',\n                    \'type\': \'momentum\',\n                    \'uses_data\': \'daily\',\n                    \'high\': 0,\n                    \'low\': 0,\n                    \'close\': 0,\n                    \'open\': 0,\n                    \'willr_open_value\': 0,\n                    \'num_points\': 15,\n                    \'buy_below\': -80,\n                    \'sell_above\': -20,\n                    \'is_buy\': False,\n                    \'is_sell\': False\n                }\n            ],\n            \'slack\': {\n                \'webhook\': None\n            }\n        }\n\n    # end of setUp\n\n    @mock.patch(\n        (\'analysis_engine.ae_talib.WILLR\'),\n        new=mock_talib.MockWILLRBuy)\n    @mock.patch(\n        (\'analysis_engine.write_to_file.write_to_file\'),\n        new=mock_write_to_file)\n    def test_run_daily_indicator_with_algo_config_buy(self):\n        """"""test_run_daily_indicator_with_algo_config_buy""""""\n        algo = base_algo.BaseAlgo(\n            ticker=self.ticker,\n            balance=self.balance,\n            start_date_str=self.start_date_str,\n            end_date_str=self.end_date_str,\n            config_dict=self.algo_config_dict)\n        self.assertEqual(\n            algo.name,\n            self.algo_config_dict[\'name\'])\n        self.assertEqual(\n            algo.tickers,\n            [self.ticker])\n        algo.handle_data(\n            data=self.data)\n\n        res = algo.get_result()\n        print(ae_consts.ppj(res))\n        self.assertEqual(\n            res[\'history\'][0][\'total_sells\'],\n            0)\n        self.assertEqual(\n            res[\'history\'][0][\'total_buys\'],\n            1)\n    # end of test_run_daily_indicator_with_algo_config_buy\n\n    @mock.patch(\n        (\'analysis_engine.ae_talib.WILLR\'),\n        new=mock_talib.MockWILLRSell)\n    @mock.patch(\n        (\'analysis_engine.write_to_file.write_to_file\'),\n        new=mock_write_to_file)\n    def test_run_daily_indicator_with_algo_config_sell(self):\n        """"""test_run_daily_indicator_with_algo_config_sell""""""\n        algo = base_algo.BaseAlgo(\n            ticker=self.ticker,\n            balance=self.balance,\n            start_date_str=self.start_date_str,\n            end_date_str=self.end_date_str,\n            config_dict=self.algo_config_dict)\n        self.assertEqual(\n            algo.name,\n            self.algo_config_dict[\'name\'])\n        self.assertEqual(\n            algo.tickers,\n            [self.ticker])\n        algo.handle_data(\n            data=self.data)\n\n        res = algo.get_result()\n        print(ae_consts.ppj(res))\n        self.assertEqual(\n            res[\'history\'][0][\'total_sells\'],\n            1)\n        self.assertEqual(\n            res[\'history\'][0][\'total_buys\'],\n            0)\n    # end of test_run_daily_indicator_with_algo_config_sell\n\n    @mock.patch(\n        (\'analysis_engine.ae_talib.WILLR\'),\n        new=mock_talib.MockWILLRIgnore)\n    @mock.patch(\n        (\'analysis_engine.write_to_file.write_to_file\'),\n        new=mock_write_to_file)\n    def test_run_daily_indicator_with_algo_config_ignore(self):\n        """"""test_run_daily_indicator_with_algo_config_ignore""""""\n        algo = base_algo.BaseAlgo(\n            ticker=self.ticker,\n            balance=self.balance,\n            start_date_str=self.start_date_str,\n            end_date_str=self.end_date_str,\n            config_dict=self.algo_config_dict)\n        self.assertEqual(\n            algo.name,\n            self.algo_config_dict[\'name\'])\n        self.assertEqual(\n            algo.tickers,\n            [self.ticker])\n        algo.handle_data(\n            data=self.data)\n\n        res = algo.get_result()\n        print(ae_consts.ppj(res))\n        self.assertTrue(\n            res[\'history\'][0][\'total_sells\'] == 0)\n        self.assertTrue(\n            res[\'history\'][0][\'total_buys\'] == 0)\n    # end of test_run_daily_indicator_with_algo_config_ignore\n\n    """"""\n    Integration Tests\n\n    Please ensure redis and minio are running and run this:\n\n    ::\n\n        export INT_TESTS=1\n\n    """"""\n\n    @mock.patch(\n        (\'analysis_engine.write_to_file.write_to_file\'),\n        new=mock_write_to_file)\n    def test_integration_daily_indicator_with_algo_config(self):\n        """"""test_integration_daily_indicator_with_algo_config""""""\n        if ae_consts.ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n\n        algo = base_algo.BaseAlgo(\n            ticker=self.ticker,\n            balance=self.balance,\n            start_date_str=self.start_date_str,\n            end_date_str=self.end_date_str,\n            config_dict=self.algo_config_dict)\n        self.assertEqual(\n            algo.name,\n            self.algo_config_dict[\'name\'])\n        self.assertEqual(\n            algo.tickers,\n            [self.ticker])\n        algo.handle_data(\n            data=self.data)\n\n        res = algo.get_result()\n        print(ae_consts.ppj(res))\n        self.assertTrue(\n            res[\'history\'][0][\'total_sells\'] >= 1)\n        self.assertTrue(\n            res[\'history\'][0][\'total_buys\'] == 0)\n    # end of test_integration_daily_indicator_with_algo_config\n\n# end of TestAlgoWithIndicators\n'"
tests/test_base_algo.py,0,"b'""""""\nTest file for classes and functions:\n\n- analysis_engine.algo.base_algo.BaseAlgo\n- analysis_engine.run_algo.run_algo\n- analysis_engine.build_algo_request\n- analysis_engine.build_buy_order\n- analysis_engine.build_sell_order\n- analysis_engine.build_trade_history_entry\n\n""""""\n\nimport os\nimport uuid\nimport glob\nimport datetime\nimport pandas as pd\nimport mock\nimport types as use_types\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.utils as ae_utils\nimport analysis_engine.show_dataset as show_dataset\nimport analysis_engine.mocks.mock_talib as mock_talib\nimport analysis_engine.algo as base_algo\nimport analysis_engine.run_algo as run_algo\nimport analysis_engine.mocks.mock_boto3_s3 as mock_s3\nimport analysis_engine.mocks.mock_redis as mock_redis\nimport analysis_engine.mocks.base_test as base_test\nimport analysis_engine.build_algo_request as build_algo_request\nimport analysis_engine.build_buy_order as build_buy_order\nimport analysis_engine.build_sell_order as build_sell_order\nimport analysis_engine.build_trade_history_entry as build_trade_history_entry\nimport analysis_engine.build_publish_request as build_publish_request\nimport analysis_engine.get_data_from_redis_key as redis_get\n\n\ndef mock_write_to_file(\n        output_file,\n        data):\n    print(\n        \'mock - mock_write_to_file(\'\n        f\'output_file={output_file}, data={len(data)})\')\n    return True\n# end of mock_write_to_file\n\n\ndef mock_write_to_file_failed(\n        output_file,\n        data):\n    print(\n        \'mock - fail - mock_write_to_file(\'\n        f\'output_file={output_file}, data={len(data)})\')\n    return False\n# end of mock_write_to_file_failed\n\n\ndef mock_request_success_result(\n        url,\n        data):\n    """"""mock_request_success_result\n\n    Mock slack post_success\n\n    :param kwargs: keyword args dict\n    """"""\n    res = {\'status_code\': 200}\n    return use_types.SimpleNamespace(**res)\n# end of mock_request_success_result\n\n\nclass TestBaseAlgo(base_test.BaseTestCase):\n    """"""TestBaseAlgo""""""\n\n    ticker = None\n    last_close_str = None\n\n    def setUp(\n            self):\n        """"""setUp""""""\n        self.ticker = \'SPY\'\n        self.timeseries = \'day\'\n        self.trade_strategy = \'count\'\n        self.start_date_str = (\n            \'2018-11-01 15:59:59\'  # Thursday\n        )\n        self.end_date_str = (\n            \'2018-11-05 15:59:59\'  # Monday\n        )\n        self.use_end_date = datetime.datetime.now()\n        self.use_end_date_str = self.use_end_date.strftime(\n            ae_consts.COMMON_TICK_DATE_FORMAT)\n        self.use_start_date = (\n            datetime.datetime.now() - datetime.timedelta(days=3))\n        self.use_start_date_str = self.use_start_date.strftime(\n            ae_consts.COMMON_TICK_DATE_FORMAT)\n        self.daily_df = pd.DataFrame([\n            {\n                \'high\': 280.01,\n                \'low\': 270.01,\n                \'open\': 275.01,\n                \'close\': 272.02,\n                \'volume\': 123,\n                \'date\': self.start_date_str  # Thursday\n            },\n            {\n                \'high\': 281.01,\n                \'low\': 271.01,\n                \'open\': 276.01,\n                \'close\': 273.02,\n                \'volume\': 125,\n                \'date\': \'2018-11-02 15:59:59\'  # Friday\n            },\n            {\n                \'high\': 282.01,\n                \'low\': 272.01,\n                \'open\': 277.01,\n                \'close\': 274.02,\n                \'volume\': 121,\n                \'date\': self.end_date_str  # Monday\n            }\n        ])\n        self.minute_df = pd.DataFrame([])\n        self.options_df = pd.DataFrame([])\n        self.use_date = \'2018-11-05\'\n        self.dataset_id = f\'{self.ticker}_{self.use_date}\'\n        self.datasets = [\n            \'daily\'\n        ]\n        self.data = {}\n        self.data[self.ticker] = [\n            {\n                \'id\': self.dataset_id,\n                \'date\': self.use_date,\n                \'data\': {\n                    \'daily\': self.daily_df,\n                    \'minute\': self.minute_df,\n                    \'options\': self.options_df\n                }\n            }\n        ]\n        self.balance = 10000.00\n        self.last_close_str = ae_utils.get_last_close_str(\n            fmt=ae_consts.COMMON_DATE_FORMAT)\n        self.output_dir = (\n            \'/opt/sa/tests/datasets/algo\')\n\n        self.example_indicator_path = (\n            \'analysis_engine/mocks/example_indicator_williamsr.py\')\n        self.algo_config_dict = {\n            \'name\': \'test_5_days_ahead\',\n            \'verbose\': True,\n            \'algo_module_path\': None,\n            \'algo_version\': 1,\n            \'trade_horizon_units\': \'day\',\n            \'trade_horizon\': 5,\n            \'buy_rules\': {\n                \'confidence\': 75,\n                \'min_indicators\': 1\n            },\n            \'sell_rules\': {\n                \'confidence\': 75,\n                \'min_indicators\': 1\n            },\n            \'indicators\': [\n                {\n                    \'name\': \'willr\',\n                    \'module_path\': self.example_indicator_path,\n                    \'category\': \'technical\',\n                    \'type\': \'momentum\',\n                    \'uses_data\': \'daily\',\n                    \'num_points\': 12,\n                    \'buy_above\': 60,\n                    \'sell_below\': 20,\n                    \'verbose\': True\n                }\n            ],\n            \'slack\': {\n                \'webhook\': None\n            }\n        }\n\n    # end of setUp\n\n    def validate_dataset_structure(\n            self,\n            cur_algo):\n        """"""validate_dataset_structure\n\n        validate an algo\'s ``self.loaded_dataset`` has a valid structure\n\n        :param cur_algo: ``analysis_engine.algo.BaseAlgo`` object\n        """"""\n        self.assertTrue(\n            self.ticker in cur_algo.loaded_dataset)\n        self.assertTrue(\n            \'id\' in cur_algo.loaded_dataset[self.ticker][0])\n        self.assertTrue(\n            \'date\' in cur_algo.loaded_dataset[self.ticker][0])\n        self.assertTrue(\n            \'data\' in cur_algo.loaded_dataset[self.ticker][0])\n\n        expected_datasets = ae_consts.DEFAULT_SERIALIZED_DATASETS\n        loaded_ds = cur_algo.loaded_dataset[self.ticker]\n        show_dataset.show_dataset(\n            algo_dataset=cur_algo.loaded_dataset)\n        """"""\n        print(f\'testing: {self.ticker} in node={loaded_ds}\')\n        """"""\n        idx = 0\n        for ds_node in loaded_ds:\n            # print(str(ds_node)[0:40])\n            self.assertTrue(\n                \'id\' in ds_node)\n            self.assertTrue(\n                \'date\' in ds_node)\n            self.assertTrue(\n                \'data\' in ds_node)\n            ds_data = ds_node[\'data\']\n            for ds_key in expected_datasets:\n                """"""\n                print(\n                    f\'checking {ds_key} is in cur_algo.load\'\n                    f\'ed_dataset[{self.ticker}][{idx}][\\\'data\\\']\')\n                """"""\n                if ds_key not in [\n                            \'tdcalls\',\n                            \'tdputs\'\n                        ]:\n                    self.assertTrue(\n                        ds_key in ds_data)\n                    self.assertTrue(\n                        hasattr(\n                            ds_data[ds_key],\n                            \'empty\'))\n                    self.assertTrue(\n                        hasattr(\n                            ds_data[ds_key],\n                            \'to_json\'))\n                    self.assertTrue(\n                        hasattr(\n                            ds_data[ds_key],\n                            \'index\'))\n            # for all keys make sure the required fields exist\n            idx += 1\n        # for all ordered dataset nodes\n    # end of validate_dataset_structure\n\n    def test_build_algo_request_daily(self):\n        """"""test_build_algo_request_daily""""""\n        use_key = \'test_build_algo_request_daily\'\n        req = build_algo_request.build_algo_request(\n            ticker=self.ticker,\n            use_key=use_key,\n            start_date=self.start_date_str,\n            end_date=self.end_date_str,\n            datasets=self.datasets,\n            balance=self.balance,\n            label=use_key)\n        self.assertEqual(\n            req[\'tickers\'],\n            [self.ticker])\n        self.assertEqual(\n            req[\'extract_datasets\'],\n            [\n                \'SPY_2018-11-01\',\n                \'SPY_2018-11-02\',\n                \'SPY_2018-11-05\'\n            ])\n    # end of test_build_algo_request_daily\n\n    def test_build_algo_request_daily_with_config(self):\n        """"""test_build_algo_request_daily_with_config""""""\n        use_key = \'test_build_algo_request_daily_with_config\'\n        req = build_algo_request.build_algo_request(\n            ticker=self.ticker,\n            use_key=use_key,\n            start_date=self.start_date_str,\n            end_date=self.end_date_str,\n            datasets=self.datasets,\n            balance=self.balance,\n            config_file=\'not-real\',\n            config_dict=self.algo_config_dict,\n            label=use_key)\n        self.assertEqual(\n            req[\'tickers\'],\n            [self.ticker])\n        self.assertEqual(\n            req[\'config_file\'],\n            \'not-real\')\n        self.assertEqual(\n            req[\'extract_datasets\'],\n            [\n                \'SPY_2018-11-01\',\n                \'SPY_2018-11-02\',\n                \'SPY_2018-11-05\'\n            ])\n    # end of test_build_algo_request_daily_with_config\n\n    def test_build_buy_order(self):\n        """"""test_build_buy_order""""""\n        use_key = \'test_build_buy_order\'\n        date = \'2018-11-02\'\n        close = 280.00\n        buy_num = 5\n        use_balance = self.balance\n        expected_buy_price = 1412.00\n        expected_balance = use_balance - expected_buy_price\n        expected_prev_shares = 10\n        expected_num_shares = expected_prev_shares + buy_num\n        commission = 12.0\n        details = {\n            \'test\': use_key\n        }\n        req = build_buy_order.build_buy_order(\n            ticker=self.ticker,\n            close=close,            # 280.00\n            balance=use_balance,    # 10000.00\n            commission=commission,  # 12 to buy and 12 to sell\n            details=details,\n            date=date,\n            num_owned=expected_prev_shares,  # currently owned shares\n            shares=buy_num,  # buy 5 shares for (5 * 280) + 12 = 1412.00\n            use_key=use_key,\n            reason=f\'testing {use_key}\')\n        print(ae_consts.ppj(req))\n        self.assertEqual(\n            ae_consts.get_status(status=req[\'status\']),\n            \'TRADE_FILLED\')\n        self.assertEqual(\n            req[\'ticker\'],\n            self.ticker)\n        self.assertEqual(\n            req[\'close\'],\n            close)\n        self.assertEqual(\n            req[\'buy_price\'],\n            expected_buy_price)\n        self.assertEqual(\n            req[\'prev_shares\'],\n            expected_prev_shares)\n        self.assertEqual(\n            req[\'prev_balance\'],\n            self.balance)\n        self.assertEqual(\n            req[\'shares\'],\n            expected_num_shares)\n        self.assertEqual(\n            req[\'balance\'],\n            expected_balance)\n        self.assertEqual(\n            req[\'details\'],\n            details)\n    # end of test_build_buy_order\n\n    def test_build_buy_order_not_enough_funds(self):\n        """"""test_build_buy_order_not_enough_funds""""""\n        use_key = \'test_build_buy_order_not_enough_funds\'\n        date = \'2018-11-02\'\n        close = 280.00\n        buy_num = 5\n        use_balance = 1411.00\n        expected_buy_price = 1412.00\n        expected_balance = use_balance - 0\n        expected_prev_shares = 10\n        expected_num_shares = expected_prev_shares\n        commission = 12.0\n        details = {\n            \'test\': use_key\n        }\n        req = build_buy_order.build_buy_order(\n            ticker=self.ticker,\n            close=close,            # 280.00\n            balance=use_balance,    # 1411.00\n            commission=commission,  # 12 to buy and 12 to sell\n            details=details,\n            date=date,\n            num_owned=expected_prev_shares,  # currently owned shares\n            shares=buy_num,  # buy 5 shares for (5 * 280) + 12 = 1412.00\n            use_key=use_key,\n            reason=f\'testing {use_key}\')\n        print(ae_consts.ppj(req))\n        self.assertEqual(\n            ae_consts.get_status(status=req[\'status\']),\n            \'TRADE_NOT_ENOUGH_FUNDS\')\n        self.assertEqual(\n            req[\'ticker\'],\n            self.ticker)\n        self.assertEqual(\n            req[\'close\'],\n            close)\n        self.assertEqual(\n            req[\'buy_price\'],\n            expected_buy_price)\n        self.assertEqual(\n            req[\'prev_shares\'],\n            expected_prev_shares)\n        self.assertEqual(\n            req[\'prev_balance\'],\n            use_balance)\n        self.assertEqual(\n            req[\'shares\'],\n            expected_num_shares)\n        self.assertEqual(\n            req[\'balance\'],\n            expected_balance)\n        self.assertEqual(\n            req[\'details\'],\n            details)\n    # end of test_build_buy_order_not_enough_funds\n\n    def test_build_sell_order(self):\n        """"""test_build_sell_order""""""\n        use_key = \'test_build_sell_order\'\n        details = {\n            \'test\': use_key\n        }\n        date = \'2018-11-02\'\n        close = 280.00\n        use_balance = self.balance\n        commission = 11.5\n        sell_num = 7  # (7 * 280) + 11.5 = 1971.5\n        expected_sell_price = 1971.5\n        expected_balance = use_balance + expected_sell_price\n        expected_prev_shares = 13\n        expected_num_shares = expected_prev_shares - sell_num\n        req = build_sell_order.build_sell_order(\n            ticker=self.ticker,\n            close=close,\n            balance=use_balance,\n            commission=commission,\n            details=details,\n            date=date,\n            num_owned=expected_prev_shares,\n            shares=sell_num,\n            use_key=use_key,\n            reason=f\'testing {use_key}\')\n        print(ae_consts.ppj(req))\n        self.assertEqual(\n            ae_consts.get_status(status=req[\'status\']),\n            \'TRADE_FILLED\')\n        self.assertEqual(\n            req[\'ticker\'],\n            self.ticker)\n        self.assertEqual(\n            req[\'close\'],\n            close)\n        self.assertEqual(\n            req[\'sell_price\'],\n            expected_sell_price)\n        self.assertEqual(\n            req[\'prev_shares\'],\n            expected_prev_shares)\n        self.assertEqual(\n            req[\'prev_balance\'],\n            use_balance)\n        self.assertEqual(\n            req[\'shares\'],\n            expected_num_shares)\n        self.assertEqual(\n            req[\'balance\'],\n            expected_balance)\n        self.assertEqual(\n            req[\'details\'],\n            details)\n    # end of test_build_sell_order\n\n    def test_build_sell_order_not_enough_funds(self):\n        """"""test_build_sell_order_not_enough_funds""""""\n        use_key = \'test_build_sell_order_not_enough_funds\'\n        details = {\n            \'test\': use_key\n        }\n        date = \'2018-11-02\'\n        close = 280.00\n        use_balance = 9.0  # if there\'s not enough to cover commissions\n        commission = 11.5\n        sell_num = 7  # (7 * 280) + 11.5 = 1971.5\n        expected_sell_price = 0.0\n        expected_balance = use_balance + expected_sell_price\n        expected_prev_shares = 13\n        expected_num_shares = expected_prev_shares - 0\n        req = build_sell_order.build_sell_order(\n            ticker=self.ticker,\n            close=close,\n            balance=use_balance,\n            commission=commission,\n            details=details,\n            date=date,\n            num_owned=expected_prev_shares,\n            shares=sell_num,\n            use_key=use_key,\n            reason=f\'testing {use_key}\')\n        print(ae_consts.ppj(req))\n        self.assertEqual(\n            ae_consts.get_status(status=req[\'status\']),\n            \'TRADE_NOT_ENOUGH_FUNDS\')\n        self.assertEqual(\n            req[\'ticker\'],\n            self.ticker)\n        self.assertEqual(\n            req[\'close\'],\n            close)\n        self.assertEqual(\n            req[\'sell_price\'],\n            expected_sell_price)\n        self.assertEqual(\n            req[\'prev_shares\'],\n            expected_prev_shares)\n        self.assertEqual(\n            req[\'prev_balance\'],\n            use_balance)\n        self.assertEqual(\n            req[\'shares\'],\n            expected_num_shares)\n        self.assertEqual(\n            req[\'balance\'],\n            expected_balance)\n        self.assertEqual(\n            req[\'details\'],\n            details)\n    # end of test_build_sell_order_not_enough_funds\n\n    def test_build_sell_order_not_owned_asset(self):\n        """"""test_build_sell_order_not_owned_asset""""""\n        use_key = \'test_build_sell_order_not_owned_asset\'\n        details = {\n            \'test\': use_key\n        }\n        date = \'2018-11-02\'\n        close = 280.00\n        use_balance = 42.0\n        commission = 11.5\n        sell_num = 7  # (7 * 280) + 11.5 = 1971.5\n        expected_sell_price = 0.0\n        expected_balance = use_balance + expected_sell_price\n        expected_prev_shares = 0\n        expected_num_shares = expected_prev_shares - 0\n        req = build_sell_order.build_sell_order(\n            ticker=self.ticker,\n            close=close,\n            balance=use_balance,\n            commission=commission,\n            details=details,\n            date=date,\n            num_owned=expected_prev_shares,\n            shares=sell_num,\n            use_key=use_key,\n            reason=f\'testing {use_key}\')\n        print(ae_consts.ppj(req))\n        self.assertEqual(\n            ae_consts.get_status(status=req[\'status\']),\n            \'TRADE_NO_SHARES_TO_SELL\')\n        self.assertEqual(\n            req[\'ticker\'],\n            self.ticker)\n        self.assertEqual(\n            req[\'close\'],\n            close)\n        self.assertEqual(\n            req[\'sell_price\'],\n            expected_sell_price)\n        self.assertEqual(\n            req[\'prev_shares\'],\n            expected_prev_shares)\n        self.assertEqual(\n            req[\'prev_balance\'],\n            use_balance)\n        self.assertEqual(\n            req[\'shares\'],\n            expected_num_shares)\n        self.assertEqual(\n            req[\'balance\'],\n            expected_balance)\n        self.assertEqual(\n            req[\'details\'],\n            details)\n    # end of test_build_sell_order_not_owned_asset\n\n    @mock.patch(\n        (\'redis.Redis\'),\n        new=mock_redis.MockRedis)\n    @mock.patch(\n        (\'analysis_engine.write_to_file.write_to_file\'),\n        new=mock_write_to_file)\n    def test_run_daily(self):\n        """"""test_run_daily""""""\n        test_name = \'test_run_daily\'\n        algo = base_algo.BaseAlgo(\n            ticker=self.ticker,\n            balance=self.balance,\n            timeseries=self.timeseries,\n            trade_strategy=self.trade_strategy,\n            name=test_name)\n        self.assertEqual(\n            algo.name,\n            test_name)\n        self.assertEqual(\n            algo.tickers,\n            [self.ticker])\n        print(self.data)\n        algo.handle_data(\n            data=self.data)\n    # end of test_run_daily\n\n    @mock.patch(\n        (\'analysis_engine.ae_talib.WILLR\'),\n        new=mock_talib.MockWILLRBuy)\n    @mock.patch(\n        (\'analysis_engine.write_to_file.write_to_file\'),\n        new=mock_write_to_file)\n    def test_run_daily_with_config(self):\n        """"""test_run_daily_with_config""""""\n        algo = base_algo.BaseAlgo(\n            ticker=self.ticker,\n            balance=self.balance,\n            timeseries=self.timeseries,\n            trade_strategy=self.trade_strategy,\n            config_dict=self.algo_config_dict)\n        self.assertEqual(\n            algo.name,\n            self.algo_config_dict[\'name\'])\n        self.assertEqual(\n            algo.tickers,\n            [self.ticker])\n        print(self.data)\n        algo.handle_data(\n            data=self.data)\n    # end of test_run_daily_with_config\n\n    @mock.patch(\n        (\'redis.Redis\'),\n        new=mock_redis.MockRedis)\n    @mock.patch(\n        (\'analysis_engine.write_to_file.write_to_file\'),\n        new=mock_write_to_file)\n    def test_run_algo_daily(self):\n        """"""test_run_algo_daily""""""\n        test_name = \'test_run_algo_daily\'\n        balance = self.balance\n        commission = 13.5\n        algo = base_algo.BaseAlgo(\n            ticker=self.ticker,\n            balance=balance,\n            commission=commission,\n            timeseries=self.timeseries,\n            trade_strategy=self.trade_strategy,\n            name=test_name)\n        rec = run_algo.run_algo(\n            ticker=self.ticker,\n            algo=algo,\n            start_date=self.use_start_date_str,\n            end_date=self.use_end_date_str,\n            label=test_name,\n            raise_on_err=True)\n        self.assertEqual(\n            algo.name,\n            test_name)\n        self.assertEqual(\n            algo.tickers,\n            [self.ticker])\n        print(rec)\n    # end of test_run_algo_daily\n\n    @mock.patch(\n        (\'analysis_engine.write_to_file.write_to_file\'),\n        new=mock_write_to_file)\n    def test_algo_config_dict_assignments(self):\n        """"""test_algo_config_dict_assignments""""""\n        ticker = \'SPY\'\n        config_dict = {\n            \'latest_high\': 800.0,\n            \'latest_low\': 100.0,\n            \'latest_open\': 300.0,\n            \'latest_close\': 400.0,\n            \'latest_volume\': 500,\n            \'num_owned\': 7,\n            \'balance\': 2000.0\n        }\n        demo_algo = base_algo.BaseAlgo(\n            ticker=ticker,\n            balance=1000.00,\n            commission=6.00,\n            config_dict=config_dict,\n            timeseries=self.timeseries,\n            trade_strategy=self.trade_strategy,\n            name=f\'test-{ticker}\')\n        self.assertEqual(\n            demo_algo.latest_high,\n            config_dict[\'latest_high\'])\n        self.assertEqual(\n            demo_algo.latest_low,\n            config_dict[\'latest_low\'])\n        self.assertEqual(\n            demo_algo.latest_open,\n            config_dict[\'latest_open\'])\n        self.assertEqual(\n            demo_algo.latest_close,\n            config_dict[\'latest_close\'])\n        self.assertEqual(\n            demo_algo.latest_volume,\n            config_dict[\'latest_volume\'])\n        self.assertEqual(\n            demo_algo.num_owned,\n            config_dict[\'num_owned\'])\n        self.assertEqual(\n            demo_algo.balance,\n            config_dict[\'balance\'])\n    # end of test_algo_config_dict_assignments\n\n    @mock.patch(\n        (\'redis.Redis\'),\n        new=mock_redis.MockRedis)\n    @mock.patch(\n        (\'analysis_engine.write_to_file.write_to_file\'),\n        new=mock_write_to_file)\n    def test_sample_algo_code_in_docstring(self):\n        """"""test_sample_algo_code_in_docstring""""""\n        ticker = \'SPY\'\n        demo_algo = base_algo.BaseAlgo(\n            ticker=ticker,\n            balance=1000.00,\n            commission=6.00,\n            timeseries=self.timeseries,\n            trade_strategy=self.trade_strategy,\n            name=f\'test-{ticker}\')\n        date = \'2018-11-05\'\n        dataset_id = f\'{ticker}_{date}\'\n\n        # mock the data pipeline in redis:\n        data = {\n            ticker: [\n                {\n                    \'id\': dataset_id,\n                    \'date\': date,\n                    \'data\': {\n                        \'daily\': pd.DataFrame([\n                            {\n                                \'high\': 280.01,\n                                \'low\': 270.01,\n                                \'open\': 275.01,\n                                \'close\': 272.02,\n                                \'volume\': 123,\n                                \'date\': \'2018-11-01 15:59:59\'\n                            },\n                            {\n                                \'high\': 281.01,\n                                \'low\': 271.01,\n                                \'open\': 276.01,\n                                \'close\': 273.02,\n                                \'volume\': 124,\n                                \'date\': \'2018-11-02 15:59:59\'\n                            },\n                            {\n                                \'high\': 282.01,\n                                \'low\': 272.01,\n                                \'open\': 277.01,\n                                \'close\': 274.02,\n                                \'volume\': 121,\n                                \'date\': \'2018-11-05 15:59:59\'\n                            }\n                        ]),\n                        \'calls\': pd.DataFrame([]),\n                        \'puts\': pd.DataFrame([]),\n                        \'minute\': pd.DataFrame([]),\n                        \'pricing\': pd.DataFrame([]),\n                        \'quote\': pd.DataFrame([]),\n                        \'news\': pd.DataFrame([]),\n                        \'news1\': pd.DataFrame([]),\n                        \'dividends\': pd.DataFrame([]),\n                        \'earnings\': pd.DataFrame([]),\n                        \'financials\': pd.DataFrame([]),\n                        \'stats\': pd.DataFrame([]),\n                        \'peers\': pd.DataFrame([]),\n                        \'tdcalls\': pd.DataFrame([]),\n                        \'tdputs\': pd.DataFrame([]),\n                        \'company\': pd.DataFrame([])\n                        # DEFAULT_SERIALIZED_DATASETS\n                    }\n                }\n            ]\n        }\n\n        # run the algorithm\n        demo_algo.handle_data(data=data)\n\n        # get the algorithm results\n        results = demo_algo.get_result()\n\n        print(ae_consts.ppj(results))\n        print(results[\'history\'][0].get(\'err\', \'no error\'))\n        self.assertEqual(\n            results[\'balance\'],\n            1000.0)\n        self.assertEqual(\n            len(results[\'history\']),\n            1)\n        self.assertEqual(\n            ae_consts.get_status(results[\'history\'][0][\'status\']),\n            \'TRADE_NOT_PROFITABLE\')\n        self.assertEqual(\n            ae_consts.get_status(results[\'history\'][0][\'algo_status\']),\n            \'ALGO_NOT_PROFITABLE\')\n        demo_algo.loaded_dataset = demo_algo.last_handle_data\n        self.validate_dataset_structure(cur_algo=demo_algo)\n    # end of test_sample_algo_code_in_docstring\n\n    def test_trade_history_algo_not_trade_profitable(self):\n        history = build_trade_history_entry.build_trade_history_entry(\n            ticker=\'notreal\',\n            original_balance=1000.00,\n            num_owned=20,\n            algo_start_price=270.01,\n            close=280.41,\n            balance=123,\n            commission=6,\n            ds_id=\'SPY_2018-11-02\',\n            date=\'today\',\n            trade_type=ae_consts.TRADE_SHARES)\n        print(history.get(\'err\', \'no error\'))\n        self.assertEqual(\n            ae_consts.get_status(status=history[\'status\']),\n            \'TRADE_PROFITABLE\')\n        self.assertEqual(\n            ae_consts.get_status(status=history[\'algo_status\']),\n            \'ALGO_NOT_PROFITABLE\')\n        self.assertEqual(\n            history[\'balance\'],\n            123)\n        self.assertEqual(\n            history[\'commission\'],\n            6)\n        self.assertEqual(\n            history[\'close\'],\n            280.41)\n        self.assertEqual(\n            history[\'net_gain\'],\n            None)\n    # end of test_trade_history_algo_not_trade_profitable\n\n    @mock.patch(\n        (\'redis.Redis\'),\n        new=mock_redis.MockRedis)\n    @mock.patch(\n        (\'analysis_engine.write_to_file.write_to_file\'),\n        new=mock_write_to_file)\n    def test_run_derived_algo_daily(self):\n        """"""test_run_derived_algo_daily""""""\n        test_name = \'test_run_derived_algo_daily\'\n        balance = self.balance\n        commission = 13.5\n\n        class DerivedAlgoTest(base_algo.BaseAlgo):\n            """"""\n            # alternative inheritance - python 2\n            def __init__(\n                    self,\n                    ticker,\n                    balance,\n                    commission=6.0,\n                    tickers=None,\n                    name=None,\n                    auto_fill=True,\n                    timeseries=None,\n                    trade_strategy=None,\n                    config_dict=None):\n                BaseAlgo.__init__(\n                    self,\n                    ticker=ticker,\n                    balance=balance,\n                    commission=commission,\n                    tickers=tickers,\n                    name=name,\n                    auto_fill=auto_fill,\n                    timeseries=None,\n                    trade_strategy=None,\n                    config_dict=config_dict)\n            """"""\n\n            def __init__(\n                    self,\n                    ticker,\n                    balance,\n                    commission=6.0,\n                    tickers=None,\n                    name=None,\n                    auto_fill=True,\n                    timeseries=None,\n                    trade_strategy=None,\n                    config_dict=None):\n                """"""__init__\n\n                :param ticker: test ticker\n                :param balance: test balance\n                :param commission: test commission\n                :param tickers: test tickers\n                :param name: name of algo\n                :param auto_fill: auto fill trade for backtesting\n                :param config_dict: config_dict\n                """"""\n                if config_dict:\n                    config_dict[\'verbose\'] = True\n                super(DerivedAlgoTest, self).__init__(\n                    ticker=ticker,\n                    balance=balance,\n                    commission=commission,\n                    tickers=tickers,\n                    name=name,\n                    auto_fill=auto_fill,\n                    timeseries=timeseries,\n                    trade_strategy=trade_strategy,\n                    config_dict=config_dict,\n                    verbose=True)\n\n                self.daily_results = []\n                self.num_daily_found = 0\n            # end of __init__\n\n            def process(\n                    self,\n                    algo_id,\n                    ticker,\n                    dataset):\n                """"""process\n\n                derived process method\n\n                :param algo_id: algorithm id\n                :param ticker: ticker\n                :param dataset: datasets for this date\n                """"""\n                self.daily_results.append(dataset)\n\n                assert(hasattr(self.df_daily, \'index\'))\n                assert(hasattr(self.df_minute, \'index\'))\n                assert(hasattr(self.df_puts, \'index\'))\n                assert(hasattr(self.df_calls, \'index\'))\n                assert(hasattr(self.df_pricing, \'index\'))\n                assert(hasattr(self.df_quote, \'index\'))\n                assert(hasattr(self.df_stats, \'index\'))\n                assert(hasattr(self.df_peers, \'index\'))\n                assert(hasattr(self.df_iex_news, \'index\'))\n                assert(hasattr(self.df_financials, \'index\'))\n                assert(hasattr(self.df_earnings, \'index\'))\n                assert(hasattr(self.df_dividends, \'index\'))\n                assert(hasattr(self.df_company, \'index\'))\n                assert(hasattr(self.df_yahoo_news, \'index\'))\n\n                self.num_daily_found += len(self.df_daily.index)\n            # end of process\n\n            def get_test_values(\n                    self):\n                """"""get_test_values""""""\n                return self.daily_results\n            # end of get_test_values\n\n            def get_num_daily_found(\n                    self):\n                """"""get_test_values""""""\n                return self.num_daily_found\n            # end of get_num_daily_found\n\n        # end of DerivedAlgoTest\n\n        algo = DerivedAlgoTest(\n            ticker=self.ticker,\n            balance=balance,\n            commission=commission,\n            timeseries=self.timeseries,\n            trade_strategy=self.trade_strategy,\n            name=test_name)\n        print(f\'starting algo {datetime.datetime.now()}\')\n        algo_res = run_algo.run_algo(\n            ticker=self.ticker,\n            algo=algo,\n            start_date=self.use_start_date_str,\n            end_date=self.use_end_date_str,\n            label=test_name,\n            raise_on_err=True,\n            verbose=True)\n        print(ae_consts.ppj(algo_res))\n        print(f\'done algo {datetime.datetime.now()}\')\n        self.assertEqual(\n            algo.name,\n            test_name)\n        self.assertEqual(\n            algo.tickers,\n            [self.ticker])\n        self.assertTrue(\n            len(algo.get_test_values()) >= 1)\n        self.assertTrue(\n            len(algo_res[\'rec\'][\'history\']) >= 1)\n        self.assertEqual(\n            ae_consts.get_status(status=algo_res[\'status\']),\n            \'SUCCESS\')\n        self.assertEqual(\n            len(algo_res[\'rec\'][\'history\']),\n            len(algo.get_test_values()))\n        print(\n            f\'dates: {self.use_start_date_str} to {self.use_end_date_str}\')\n    # end of test_run_derived_algo_daily\n\n    @mock.patch(\n        (\'redis.Redis\'),\n        new=mock_redis.MockRedis)\n    @mock.patch(\n        (\'boto3.resource\'),\n        new=mock_s3.build_boto3_resource)\n    @mock.patch(\n        (\'analysis_engine.write_to_file.write_to_file\'),\n        new=mock_write_to_file)\n    def test_algo_can_save_all_input_datasets_publish_disabled(self):\n        """"""test_algo_can_save_all_input_datasets_publish_disabled""""""\n        test_name = \'test_run_algo_daily\'\n        balance = self.balance\n        commission = 13.5\n        algo = base_algo.BaseAlgo(\n            ticker=self.ticker,\n            balance=balance,\n            commission=commission,\n            timeseries=self.timeseries,\n            trade_strategy=self.trade_strategy,\n            publish_history=False,\n            publish_report=False,\n            publish_input=False,\n            name=test_name)\n        algo_res = run_algo.run_algo(\n            ticker=self.ticker,\n            algo=algo,\n            start_date=self.use_start_date_str,\n            end_date=self.use_end_date_str,\n            label=test_name,\n            raise_on_err=True)\n        self.assertTrue(\n            len(algo_res[\'rec\'][\'history\']) >= 1)\n        self.assertEqual(\n            algo.name,\n            test_name)\n        self.assertEqual(\n            algo.tickers,\n            [self.ticker])\n        output_file = f\'/opt/sa/tests/datasets/algo/{test_name}.json\'\n        redis_enabled = False\n        redis_key = f\'{test_name}\'\n        s3_enabled = False\n        s3_key = f\'{test_name}.json\'\n        compress = True\n        slack_enabled = False\n        slack_code_block = True\n        slack_full_width = False\n        verbose = True\n        publish_input_req = build_publish_request.build_publish_request(\n            label=test_name,\n            convert_to_json=True,\n            output_file=output_file,\n            compress=compress,\n            redis_enabled=redis_enabled,\n            redis_key=redis_key,\n            s3_enabled=s3_enabled,\n            s3_key=s3_key,\n            slack_enabled=slack_enabled,\n            slack_code_block=slack_code_block,\n            slack_full_width=slack_full_width,\n            verbose=verbose)\n        publish_input_status = algo.publish_input_dataset(\n            **publish_input_req)\n        self.assertEqual(\n            ae_consts.get_status(status=publish_input_status),\n            \'NOT_RUN\')\n    # end of test_algo_can_save_all_input_datasets_publish_disabled\n\n    @mock.patch(\n        (\'redis.Redis\'),\n        new=mock_redis.MockRedis)\n    @mock.patch(\n        (\'boto3.resource\'),\n        new=mock_s3.build_boto3_resource)\n    @mock.patch(\n        (\'requests.post\'),\n        new=mock_request_success_result)\n    @mock.patch(\n        (\'analysis_engine.write_to_file.write_to_file\'),\n        new=mock_write_to_file)\n    def test_algo_can_save_all_input_datasets_to_file(self):\n        """"""test_algo_can_save_all_input_datasets_to_file""""""\n        test_name = \'test_run_algo_daily\'\n        balance = self.balance\n        commission = 13.5\n        algo = base_algo.BaseAlgo(\n            ticker=self.ticker,\n            balance=balance,\n            commission=commission,\n            timeseries=self.timeseries,\n            trade_strategy=self.trade_strategy,\n            name=test_name)\n        algo_res = run_algo.run_algo(\n            ticker=self.ticker,\n            algo=algo,\n            start_date=self.use_start_date_str,\n            end_date=self.use_end_date_str,\n            label=test_name,\n            raise_on_err=True)\n        self.assertTrue(\n            len(algo_res[\'rec\'][\'history\']) >= 1)\n        self.assertEqual(\n            algo.name,\n            test_name)\n        self.assertEqual(\n            algo.tickers,\n            [self.ticker])\n        output_file = f\'./tests/datasets/algo/{test_name}.json\'\n        redis_enabled = True\n        redis_key = f\'{test_name}\'\n        s3_enabled = True\n        s3_key = f\'{test_name}.json\'\n        compress = True\n        slack_enabled = True\n        slack_code_block = True\n        slack_full_width = False\n        verbose = True\n        publish_input_req = build_publish_request.build_publish_request(\n            label=test_name,\n            convert_to_json=True,\n            output_file=output_file,\n            compress=compress,\n            redis_enabled=redis_enabled,\n            redis_key=redis_key,\n            s3_enabled=s3_enabled,\n            s3_key=s3_key,\n            slack_enabled=slack_enabled,\n            slack_code_block=slack_code_block,\n            slack_full_width=slack_full_width,\n            verbose=verbose)\n        publish_input_status = algo.publish_input_dataset(\n            **publish_input_req)\n        self.assertEqual(\n            ae_consts.get_status(status=publish_input_status),\n            \'SUCCESS\')\n    # end of test_algo_can_save_all_input_datasets_to_file\n\n    @mock.patch(\n        (\'redis.Redis\'),\n        new=mock_redis.MockRedis)\n    @mock.patch(\n        (\'boto3.resource\'),\n        new=mock_s3.build_boto3_resource)\n    @mock.patch(\n        (\'requests.post\'),\n        new=mock_request_success_result)\n    @mock.patch(\n        (\'analysis_engine.write_to_file.write_to_file\'),\n        new=mock_write_to_file_failed)\n    def test_algo_can_save_all_input_datasets_to_file_failed(self):\n        """"""test_algo_can_save_all_input_datasets_to_file_failed""""""\n        test_name = \'test_run_algo_daily\'\n        balance = self.balance\n        commission = 13.5\n        algo = base_algo.BaseAlgo(\n            ticker=self.ticker,\n            balance=balance,\n            commission=commission,\n            timeseries=self.timeseries,\n            trade_strategy=self.trade_strategy,\n            name=test_name)\n        algo_res = run_algo.run_algo(\n            ticker=self.ticker,\n            algo=algo,\n            start_date=self.use_start_date_str,\n            end_date=self.use_end_date_str,\n            label=test_name,\n            raise_on_err=True)\n        self.assertTrue(\n            len(algo_res[\'rec\'][\'history\']) >= 1)\n        self.assertEqual(\n            algo.name,\n            test_name)\n        self.assertEqual(\n            algo.tickers,\n            [self.ticker])\n        output_file = f\'./tests/datasets/algo/{test_name}.json\'\n        redis_enabled = True\n        redis_key = f\'{test_name}\'\n        s3_enabled = True\n        s3_key = f\'{test_name}.json\'\n        compress = True\n        slack_enabled = True\n        slack_code_block = True\n        slack_full_width = False\n        verbose = True\n        publish_input_req = build_publish_request.build_publish_request(\n            label=test_name,\n            convert_to_json=True,\n            output_file=output_file,\n            compress=compress,\n            redis_enabled=redis_enabled,\n            redis_key=redis_key,\n            s3_enabled=s3_enabled,\n            s3_key=s3_key,\n            slack_enabled=slack_enabled,\n            slack_code_block=slack_code_block,\n            slack_full_width=slack_full_width,\n            verbose=verbose)\n        publish_input_status = algo.publish_input_dataset(\n            **publish_input_req)\n        self.assertEqual(\n            ae_consts.get_status(status=publish_input_status),\n            \'FILE_FAILED\')\n    # end of test_algo_can_save_all_input_datasets_to_file_failed\n\n    """"""\n    Integration Tests\n\n    Please ensure redis and minio are running and run this:\n\n    ::\n\n        export INT_TESTS=1\n\n    """"""\n\n    def test_integration_algo_publish_input_dataset_to_redis(self):\n        """"""test_integration_algo_publish_input_dataset_to_redis""""""\n        if ae_consts.ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n\n        test_name = (\n            \'test_integration_algo_publish_input_dataset_to_redis\')\n        balance = self.balance\n        commission = 13.5\n        algo = base_algo.BaseAlgo(\n            ticker=self.ticker,\n            balance=balance,\n            commission=commission,\n            timeseries=self.timeseries,\n            trade_strategy=self.trade_strategy,\n            name=test_name)\n        algo_res = run_algo.run_algo(\n            ticker=self.ticker,\n            algo=algo,\n            start_date=self.use_start_date_str,\n            end_date=self.use_end_date_str,\n            label=test_name,\n            raise_on_err=True)\n        self.assertTrue(\n            len(algo_res[\'rec\'][\'history\']) >= 1)\n        self.assertEqual(\n            algo.name,\n            test_name)\n        self.assertEqual(\n            algo.tickers,\n            [self.ticker])\n        output_file = f\'./tests/datasets/algo/{test_name}.json\'\n        redis_enabled = True\n        redis_key = f\'{test_name}\'\n        s3_enabled = True\n        s3_key = f\'{test_name}.json\'\n        compress = True\n        slack_enabled = True\n        slack_code_block = True\n        slack_full_width = False\n        verbose = True\n        publish_input_req = build_publish_request.build_publish_request(\n            label=test_name,\n            convert_to_json=True,\n            output_file=output_file,\n            compress=compress,\n            redis_enabled=redis_enabled,\n            redis_key=redis_key,\n            s3_enabled=s3_enabled,\n            s3_key=s3_key,\n            slack_enabled=slack_enabled,\n            slack_code_block=slack_code_block,\n            slack_full_width=slack_full_width,\n            verbose=verbose)\n        publish_input_status = algo.publish_input_dataset(\n            **publish_input_req)\n        self.assertEqual(\n            ae_consts.get_status(status=publish_input_status),\n            \'SUCCESS\')\n        redis_res = redis_get.get_data_from_redis_key(\n            host=publish_input_req[\'redis_address\'].split(\':\')[0],\n            port=publish_input_req[\'redis_address\'].split(\':\')[1],\n            password=publish_input_req[\'redis_password\'],\n            db=publish_input_req[\'redis_db\'],\n            key=publish_input_req[\'redis_key\'],\n            serializer=publish_input_req[\'redis_serializer\'],\n            encoding=publish_input_req[\'redis_encoding\'],\n            decompress_df=True)\n        self.assertEqual(\n            ae_consts.get_status(status=redis_res[\'status\']),\n            \'SUCCESS\')\n        print(\n            f\'found data size={len(redis_res[""rec""][""data""])} in \'\n            f\'redis_key={publish_input_req[""redis_key""]}\')\n        self.assertTrue(\n            len(redis_res[\'rec\'][\'data\']) >= 1)\n    # end of test_integration_algo_publish_input_dataset_to_redis\n\n    def test_integration_algo_publish_input_dataset_to_file(self):\n        """"""test_integration_algo_publish_input_dataset_to_file""""""\n        if ae_consts.ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n\n        test_name = (\n            \'test_integration_algo_publish_input_dataset_to_file\')\n        balance = self.balance\n        commission = 13.5\n        algo = base_algo.BaseAlgo(\n            ticker=self.ticker,\n            balance=balance,\n            commission=commission,\n            timeseries=self.timeseries,\n            trade_strategy=self.trade_strategy,\n            name=test_name)\n        algo_res = run_algo.run_algo(\n            ticker=self.ticker,\n            algo=algo,\n            start_date=self.use_start_date_str,\n            end_date=self.use_end_date_str,\n            label=test_name,\n            raise_on_err=True)\n        self.assertTrue(\n            len(algo_res[\'rec\'][\'history\']) >= 1)\n        self.assertEqual(\n            algo.name,\n            test_name)\n        self.assertEqual(\n            algo.tickers,\n            [self.ticker])\n        test_should_create_this_file = (\n            f\'./tests/datasets/algo/{test_name}-{str(uuid.uuid4())}.json\')\n        redis_enabled = True\n        redis_key = f\'{test_name}\'\n        s3_enabled = True\n        s3_key = f\'{test_name}.json\'\n        compress = True\n        slack_enabled = True\n        slack_code_block = True\n        slack_full_width = False\n        verbose = True\n        publish_input_req = build_publish_request.build_publish_request(\n            label=test_name,\n            convert_to_json=True,\n            output_file=test_should_create_this_file,\n            compress=compress,\n            redis_enabled=redis_enabled,\n            redis_key=redis_key,\n            s3_enabled=s3_enabled,\n            s3_key=s3_key,\n            slack_enabled=slack_enabled,\n            slack_code_block=slack_code_block,\n            slack_full_width=slack_full_width,\n            verbose=verbose)\n        publish_input_status = algo.publish_input_dataset(\n            **publish_input_req)\n        self.assertEqual(\n            ae_consts.get_status(status=publish_input_status),\n            \'SUCCESS\')\n        self.assertTrue(os.path.exists(test_should_create_this_file))\n        # now load it into an algo\n    # end of test_integration_algo_publish_input_dataset_to_file\n\n    def test_integration_algo_load_from_file(self):\n        """"""test_integration_algo_load_from_file""""""\n        if ae_consts.ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n\n        test_name = (\n            \'test_integration_algo_load_from_file\')\n        test_file_regex = (\n            f\'{self.output_dir}/test_integration_algo_load_from_file*.json\')\n        files = sorted(\n            glob.iglob(test_file_regex),\n            key=os.path.getctime,\n            reverse=True)\n\n        load_config_req = None\n        latest_file = (\n            f\'./tests/datasets/algo/{test_name}-{str(uuid.uuid4())}.json\')\n        if len(files) == 0:\n            algo = base_algo.BaseAlgo(\n                ticker=self.ticker,\n                balance=self.balance,\n                commission=6.0,\n                timeseries=self.timeseries,\n                trade_strategy=self.trade_strategy,\n                name=test_name)\n            run_algo.run_algo(\n                ticker=self.ticker,\n                algo=algo,\n                start_date=self.use_start_date_str,\n                end_date=self.use_end_date_str,\n                label=test_name,\n                raise_on_err=True)\n            publish_input_req = build_publish_request.build_publish_request(\n                label=test_name,\n                output_file=latest_file,\n                convert_to_json=True,\n                compress=True,\n                redis_enabled=False,\n                s3_enabled=False,\n                slack_enabled=False,\n                verbose=True)\n            publish_input_status = \\\n                algo.publish_input_dataset(\n                    **publish_input_req)\n            self.assertEqual(\n                ae_consts.get_status(status=publish_input_status),\n                \'SUCCESS\')\n        else:\n            latest_file = files[0]\n        # end of create a file\n\n        self.assertTrue(os.path.exists(latest_file))\n\n        load_config_req = build_publish_request.build_publish_request(\n            label=test_name,\n            convert_to_json=True,\n            output_file=latest_file,\n            compress=False,\n            redis_enabled=False,\n            s3_enabled=False)\n        file_algo = base_algo.BaseAlgo(\n            ticker=self.ticker,\n            balance=self.balance,\n            commission=6.0,\n            timeseries=self.timeseries,\n            trade_strategy=self.trade_strategy,\n            name=f\'load-from-file_{test_name}\',\n            load_config=load_config_req)\n\n        print(file_algo.loaded_dataset)\n        self.validate_dataset_structure(cur_algo=file_algo)\n    # end of test_integration_algo_load_from_file\n\n    def test_integration_algo_publish_input_s3_and_load(self):\n        """"""test_integration_algo_publish_input_s3_and_load""""""\n        if ae_consts.ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n\n        test_name = (\n            \'test_integration_algo_publish_input_s3_and_load\')\n        balance = self.balance\n        commission = 13.5\n        algo = base_algo.BaseAlgo(\n            ticker=self.ticker,\n            balance=balance,\n            commission=commission,\n            timeseries=self.timeseries,\n            trade_strategy=self.trade_strategy,\n            name=test_name)\n        algo_res = run_algo.run_algo(\n            ticker=self.ticker,\n            algo=algo,\n            start_date=self.use_start_date_str,\n            end_date=self.use_end_date_str,\n            label=test_name,\n            raise_on_err=True)\n        self.assertTrue(\n            len(algo_res[\'rec\'][\'history\']) >= 1)\n        self.assertEqual(\n            algo.name,\n            test_name)\n        self.assertEqual(\n            algo.tickers,\n            [self.ticker])\n        unique_id = str(uuid.uuid4())\n        test_should_create_this_file = (\n            f\'./tests/datasets/algo/{test_name}-{unique_id}.json\')\n        redis_enabled = True\n        redis_key = f\'{test_name}:{unique_id}\'\n        s3_enabled = True\n        s3_key = f\'{test_name}-{unique_id}.json\'\n        compress = True\n        slack_enabled = True\n        slack_code_block = True\n        slack_full_width = False\n        verbose = True\n        redis_db = 1\n        unittest_bucket = \'unittest-algo\'\n        publish_input_req = build_publish_request.build_publish_request(\n            label=test_name,\n            convert_to_json=True,\n            output_file=test_should_create_this_file,\n            compress=compress,\n            redis_enabled=redis_enabled,\n            redis_key=redis_key,\n            redis_db=redis_db,\n            s3_enabled=s3_enabled,\n            s3_key=s3_key,\n            s3_bucket=unittest_bucket,\n            slack_enabled=slack_enabled,\n            slack_code_block=slack_code_block,\n            slack_full_width=slack_full_width,\n            verbose=verbose)\n        load_config_req = build_publish_request.build_publish_request(\n            label=test_name,\n            convert_to_json=True,\n            output_file=test_should_create_this_file,\n            compress=compress,\n            redis_enabled=redis_enabled,\n            redis_key=redis_key,\n            redis_db=redis_db,\n            s3_enabled=s3_enabled,\n            s3_key=s3_key,\n            s3_bucket=unittest_bucket,\n            slack_enabled=slack_enabled,\n            slack_code_block=slack_code_block,\n            slack_full_width=slack_full_width,\n            verbose=verbose)\n        publish_input_status = algo.publish_input_dataset(\n            **publish_input_req)\n        self.assertEqual(\n            ae_consts.get_status(status=publish_input_status),\n            \'SUCCESS\')\n        self.assertTrue(os.path.exists(test_should_create_this_file))\n        # now load it into an algo\n\n        print(\'\')\n        print(\'---------------\')\n        print(\'starting s3 load integration test\')\n\n        load_config_req[\'s3_key\'] = s3_key\n        s3_algo = base_algo.BaseAlgo(\n            ticker=self.ticker,\n            balance=balance,\n            commission=commission,\n            name=test_name,\n            start_date=self.use_start_date_str,\n            end_date=self.use_end_date_str,\n            timeseries=self.timeseries,\n            trade_strategy=self.trade_strategy,\n            load_config=load_config_req)\n\n        self.validate_dataset_structure(cur_algo=s3_algo)\n    # end of test_integration_algo_publish_input_s3_and_load\n\n    def test_integration_algo_publish_input_redis_and_load(self):\n        """"""test_integration_algo_publish_input_redis_and_load""""""\n        if ae_consts.ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n\n        test_name = (\n            \'test_integration_algo_publish_input_redis_and_load\')\n        balance = self.balance\n        commission = 13.5\n        algo = base_algo.BaseAlgo(\n            ticker=self.ticker,\n            balance=balance,\n            commission=commission,\n            timeseries=self.timeseries,\n            trade_strategy=self.trade_strategy,\n            name=test_name)\n        algo_res = run_algo.run_algo(\n            ticker=self.ticker,\n            algo=algo,\n            start_date=self.use_start_date_str,\n            end_date=self.use_end_date_str,\n            label=test_name,\n            raise_on_err=True)\n        self.assertTrue(\n            len(algo_res[\'rec\'][\'history\']) >= 1)\n        self.assertEqual(\n            algo.name,\n            test_name)\n        self.assertEqual(\n            algo.tickers,\n            [self.ticker])\n        unique_id = str(uuid.uuid4())\n        test_should_create_this_file = (\n            f\'./tests/datasets/algo/{test_name}-{unique_id}.json\')\n        redis_enabled = True\n        redis_key = f\'{test_name}:{unique_id}\'\n        s3_enabled = True\n        s3_key = f\'{test_name}-{unique_id}.json\'\n        compress = True\n        slack_enabled = True\n        slack_code_block = True\n        slack_full_width = False\n        verbose = True\n        redis_db = 1\n        unittest_bucket = \'unittest-algo\'\n        publish_input_req = build_publish_request.build_publish_request(\n            label=test_name,\n            output_file=test_should_create_this_file,\n            compress=compress,\n            redis_enabled=redis_enabled,\n            redis_key=redis_key,\n            redis_db=redis_db,\n            s3_enabled=s3_enabled,\n            s3_key=s3_key,\n            s3_bucket=unittest_bucket,\n            slack_enabled=slack_enabled,\n            slack_code_block=slack_code_block,\n            slack_full_width=slack_full_width,\n            verbose=verbose)\n        load_config_req = build_publish_request.build_publish_request(\n            label=test_name,\n            output_file=test_should_create_this_file,\n            compress=compress,\n            redis_enabled=redis_enabled,\n            redis_key=redis_key,\n            redis_db=redis_db,\n            s3_enabled=s3_enabled,\n            s3_key=s3_key,\n            s3_bucket=unittest_bucket,\n            slack_enabled=slack_enabled,\n            slack_code_block=slack_code_block,\n            slack_full_width=slack_full_width,\n            verbose=verbose)\n        publish_input_status = algo.publish_input_dataset(\n            **publish_input_req)\n        self.assertEqual(\n            ae_consts.get_status(status=publish_input_status),\n            \'SUCCESS\')\n        self.assertTrue(os.path.exists(test_should_create_this_file))\n        # now load it into an algo\n\n        print(\'\')\n        print(\'---------------\')\n        print(\'starting redis publish integration test\')\n\n        load_config_req[\'redis_key\'] = s3_key\n        redis_algo = base_algo.BaseAlgo(\n            ticker=self.ticker,\n            balance=balance,\n            commission=commission,\n            name=test_name,\n            timeseries=self.timeseries,\n            trade_strategy=self.trade_strategy,\n            load_config=load_config_req)\n\n        self.validate_dataset_structure(cur_algo=redis_algo)\n    # end of test_integration_algo_publish_input_redis_and_load\n\n    def test_integration_algo_restore_ready_back_to_redis(self):\n        """"""test_integration_algo_restore_ready_back_to_redis""""""\n        if ae_consts.ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n\n        test_name = (\n            \'test_integration_algo_restore_ready_back_to_redis\')\n        balance = self.balance\n        commission = 13.5\n        algo = base_algo.BaseAlgo(\n            ticker=self.ticker,\n            balance=balance,\n            commission=commission,\n            timeseries=self.timeseries,\n            trade_strategy=self.trade_strategy,\n            name=test_name)\n        algo_res = run_algo.run_algo(\n            ticker=self.ticker,\n            algo=algo,\n            start_date=self.use_start_date_str,\n            end_date=self.use_end_date_str,\n            label=test_name,\n            raise_on_err=True)\n        self.assertTrue(\n            len(algo_res[\'rec\'][\'history\']) >= 1)\n        self.assertEqual(\n            algo.name,\n            test_name)\n        self.assertEqual(\n            algo.tickers,\n            [self.ticker])\n        unique_id = str(uuid.uuid4())\n        test_should_create_this_file = (\n            f\'./tests/datasets/algo/{test_name}-{unique_id}.json\')\n        redis_enabled = True\n        redis_key = f\'{test_name}:{unique_id}\'\n        s3_enabled = True\n        s3_key = f\'{test_name}-{unique_id}.json\'\n        compress = True\n        slack_enabled = True\n        slack_code_block = True\n        slack_full_width = False\n        verbose = True\n        unittest_bucket = \'unittest-algo\'\n        publish_input_req = build_publish_request.build_publish_request(\n            label=test_name,\n            convert_to_json=True,\n            output_file=test_should_create_this_file,\n            compress=compress,\n            redis_enabled=redis_enabled,\n            redis_key=redis_key,\n            redis_db=1,\n            s3_enabled=s3_enabled,\n            s3_key=s3_key,\n            s3_bucket=unittest_bucket,\n            slack_enabled=slack_enabled,\n            slack_code_block=slack_code_block,\n            slack_full_width=slack_full_width,\n            verbose=verbose)\n        load_config_req = build_publish_request.build_publish_request(\n            label=test_name,\n            convert_to_json=True,\n            output_file=test_should_create_this_file,\n            compress=compress,\n            redis_enabled=redis_enabled,\n            redis_key=redis_key,\n            redis_db=1,  # publish to the redis database 1\n            s3_enabled=s3_enabled,\n            s3_key=s3_key,\n            s3_bucket=unittest_bucket,\n            slack_enabled=slack_enabled,\n            slack_code_block=slack_code_block,\n            slack_full_width=slack_full_width,\n            verbose=verbose)\n        publish_input_status = algo.publish_input_dataset(\n            **publish_input_req)\n        self.assertEqual(\n            ae_consts.get_status(status=publish_input_status),\n            \'SUCCESS\')\n        self.assertTrue(os.path.exists(test_should_create_this_file))\n        # now load it into an algo\n\n        print(\'\')\n        print(\'---------------\')\n        print(\'starting redis publish integration test\')\n\n        load_config_req[\'redis_key\'] = s3_key\n        redis_algo = base_algo.BaseAlgo(\n            ticker=self.ticker,\n            balance=balance,\n            commission=commission,\n            name=test_name,\n            timeseries=self.timeseries,\n            trade_strategy=self.trade_strategy,\n            load_config=load_config_req)\n\n        self.validate_dataset_structure(cur_algo=redis_algo)\n    # end of test_integration_algo_restore_ready_back_to_redis\n\n# end of TestBaseAlgo\n'"
tests/test_finviz_fetch_api.py,0,"b'""""""\nTest file for:\nFinViz Fetch API\n""""""\n\nimport mock\nfrom analysis_engine.mocks.base_test import BaseTestCase\nfrom analysis_engine.consts import ev\nfrom analysis_engine.consts import get_status\nfrom analysis_engine.finviz.fetch_api \\\n    import fetch_tickers_from_screener\n\n\nclass MockResponse:\n    """"""MockResponse""""""\n\n    def __init__(\n            self):\n        self.text = None\n        self.status_code = 200\n    # end of __init__\n\n# end of MockResponse\n\n\ndef mock_request_get(\n        url):\n    """"""mock_request_get_success\n\n    :param url: url to test\n    """"""\n    res = MockResponse()\n\n    if \'success\' in url:\n        res.status_code = 200\n        res.text = (\n            \'<html><body>\'\n            \'<table>\'\n            \'<tbody>\'\n            \'<tr>\'\n            \'<td class=""screener-body-table-nw"">1</td>\'\n            \'<td class=""screener-body-table-nw"">QS</td>\'\n            \'<td class=""screener-body-table-nw"">Quant Solutions</td>\'\n            \'<td class=""screener-body-table-nw"">Financial</td>\'\n            \'<td class=""screener-body-table-nw"">Quant Engines</td>\'\n            \'<td class=""screener-body-table-nw"">USA</td>\'\n            \'<td class=""screener-body-table-nw"">1.1B</td>\'     # mcap\n            \'<td class=""screener-body-table-nw"">30.01</td>\'    # p/e\n            \'<td class=""screener-body-table-nw"">45.01</td>\'    # prc\n            \'<td class=""screener-body-table-nw"">3.05%</td>\'    # chg\n            \'<td class=""screener-body-table-nw"">177,777</td>\'  # vol\n            \'</tr>\'\n            \'<tr>\'\n            \'<td class=""screener-body-table-nw"">2</td>\'\n            \'<td class=""screener-body-table-nw"">SPY</td>\'\n            \'<td class=""screener-body-table-nw"">Spyder</td>\'\n            \'<td class=""screener-body-table-nw"">Financial</td>\'\n            \'<td class=""screener-body-table-nw"">ETF</td>\'\n            \'<td class=""screener-body-table-nw"">USA</td>\'\n            \'<td class=""screener-body-table-nw"">-</td>\'\n            \'<td class=""screener-body-table-nw"">-</td>\'\n            \'<td class=""screener-body-table-nw"">280.40</td>\'\n            \'<td class=""screener-body-table-nw"">2.05%</td>\'\n            \'<td class=""screener-body-table-nw"">117,872,097</td>\'\n            \'</tr>\'\n            \'<tr>\'\n            \'<td class=""screener-body-table-nw"">3</td>\'\n            \'<td class=""screener-body-table-nw"">VXX</td>\'\n            \'<td class=""screener-body-table-nw"">iPath S&P 500 VIX</td>\'\n            \'<td class=""screener-body-table-nw"">Financial</td>\'\n            \'<td class=""screener-body-table-nw"">ETF</td>\'\n            \'<td class=""screener-body-table-nw"">USA</td>\'\n            \'<td class=""screener-body-table-nw"">-</td>\'\n            \'<td class=""screener-body-table-nw"">-</td>\'\n            \'<td class=""screener-body-table-nw"">31.98</td>\'\n            \'<td class=""screener-body-table-nw"">-7.33%</td>\'\n            \'<td class=""screener-body-table-nw"">53,700,383</td>\'\n            \'</tr>\'\n            \'</tbody>\'\n            \'</table>\'\n            \'</body>\'\n            \'</html>\')\n    elif \'empty\' in url:\n        res.status_code = 200\n        res.text = (\n            \'<html><body>\'\n            \'<table>\'\n            \'<tr>\'\n            \'</tr>\'\n            \'</table>\'\n            \'</body>\'\n            \'</html>\')\n    elif \'failure\' in url:\n        res.status_code = 500\n        res.text = (\n            \'<html><body>\'\n            \'error\'\n            \'</body>\'\n            \'</html>\')\n    elif \'exception\' in url:\n        raise Exception(f\'mock_request_get - threw for url={url}\')\n    return res\n# end of mock_request_get\n\n\nclass TestFinVizFetchAPI(BaseTestCase):\n    """"""TestFinVizFetchAPI""""""\n\n    @mock.patch(\n        (\'requests.get\'),\n        new=mock_request_get)\n    def test_fetch_tickers_from_screener_success(self):\n        """"""test_fetch_tickers_from_screener_success""""""\n        url = (\n            \'success-\'\n            \'https://finviz.com/screener.ashx?\'\n            \'v=111&\'\n            \'f=an_recom_strongbuy,\'\n            \'exch_nyse,fa_ltdebteq_low,fa_sales5years_o10&\'\n            \'ft=4\')\n        res = fetch_tickers_from_screener(\n            url=url)\n        self.assertIsNotNone(\n            res)\n        self.assertTrue(\n            len(res[\'rec\'][\'data\']) > 0)\n        self.assertEqual(\n            get_status(status=res[\'status\']),\n            \'SUCCESS\')\n        self.assertEqual(\n            res[\'rec\'][\'data\'][\'ticker\'][0],\n            \'QS\')\n        self.assertEqual(\n            res[\'rec\'][\'data\'][\'ticker\'][1],\n            \'SPY\')\n        self.assertEqual(\n            res[\'rec\'][\'data\'][\'ticker\'][2],\n            \'VXX\')\n        self.assertEqual(\n            res[\'rec\'][\'tickers\'][0],\n            \'QS\')\n        self.assertEqual(\n            res[\'rec\'][\'tickers\'][1],\n            \'SPY\')\n        self.assertEqual(\n            res[\'rec\'][\'tickers\'][2],\n            \'VXX\')\n    # end of test_fetch_tickers_from_screener_success\n\n    @mock.patch(\n        (\'requests.get\'),\n        new=mock_request_get)\n    def test_fetch_tickers_from_screener_empty_data(self):\n        """"""test_fetch_tickers_from_screener_empty_data""""""\n        url = (\n            \'empty-\'\n            \'https://finviz.com/screener.ashx?\'\n            \'v=111&\'\n            \'f=an_recom_strongbuy,\'\n            \'exch_nyse,fa_ltdebteq_low,fa_sales5years_o10&\'\n            \'ft=4\')\n        res = fetch_tickers_from_screener(\n            url=url)\n        self.assertIsNotNone(\n            res)\n        self.assertTrue(\n            len(res[\'rec\'][\'data\']) == 0)\n        self.assertEqual(\n            get_status(status=res[\'status\']),\n            \'SUCCESS\')\n    # end of test_fetch_tickers_from_screener_empty_data\n\n    @mock.patch(\n        (\'requests.get\'),\n        new=mock_request_get)\n    def test_fetch_tickers_from_screener_failure_data(self):\n        """"""test_fetch_tickers_from_screener_failure_data""""""\n        url = (\n            \'failure-\'\n            \'https://finviz.com/screener.ashx?\'\n            \'v=111&\'\n            \'f=an_recom_strongbuy,\'\n            \'exch_nyse,fa_ltdebteq_low,fa_sales5years_o10&\'\n            \'ft=4\')\n        res = fetch_tickers_from_screener(\n            url=url)\n        self.assertIsNotNone(\n            res)\n        self.assertEqual(\n            get_status(status=res[\'status\']),\n            \'ERR\')\n        self.assertTrue(\n            \'finviz returned non-ok HTTP\' in res[\'err\'])\n        self.assertIsNone(\n            res[\'rec\'][\'data\'])\n    # end of test_fetch_tickers_from_screener_failure_data\n\n    @mock.patch(\n        (\'requests.get\'),\n        new=mock_request_get)\n    def test_fetch_tickers_from_screener_exception(self):\n        """"""test_fetch_tickers_from_screener_exception""""""\n        url = (\n            \'exception-\'\n            \'https://finviz.com/screener.ashx?\'\n            \'v=111&\'\n            \'f=an_recom_strongbuy,\'\n            \'exch_nyse,fa_ltdebteq_low,fa_sales5years_o10&\'\n            \'ft=4\')\n        res = fetch_tickers_from_screener(\n            url=url)\n        self.assertIsNotNone(\n            res)\n        self.assertEqual(\n            get_status(status=res[\'status\']),\n            \'EX\')\n        self.assertIsNone(\n            res[\'rec\'][\'data\'])\n    # end of test_fetch_tickers_from_screener_exception\n\n    """"""\n    Integration Tests\n\n    Please ensure redis and minio are running and run this:\n\n    ::\n\n        export INT_TESTS=1\n\n    """"""\n\n    def debug_df(\n            self,\n            df):\n        """"""debug_df\n\n        :param df: ``pandas.DataFrame`` from a fetch\n        """"""\n        print(\'-----------------------------------\')\n        print(f\'dataframe: {df}\')\n        print(\'\')\n        print(f\'dataframe columns:\\n{df.columns.values}\')\n        print(\'-----------------------------------\')\n    # end of debug_df\n\n    def test_integration_test_fetch_tickers_from_screener(self):\n        """"""test_integration_test_fetch_tickers_from_screener""""""\n        if ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n\n        default_url = (\n            \'https://finviz.com/screener.ashx?\'\n            \'v=111&\'\n            \'f=an_recom_strongbuy,\'\n            \'exch_nyse,fa_ltdebteq_low,fa_sales5years_o10&\'\n            \'ft=4\')\n        url = ev(\'INT_TEST_FINVIZ_SCREEN_URL\', default_url)\n        res = fetch_tickers_from_screener(\n            url=url)\n        self.assertIsNotNone(\n            res)\n        self.assertTrue(\n            len(res[\'rec\'][\'data\']) > 0)\n        self.assertEqual(\n            get_status(status=res[\'status\']),\n            \'SUCCESS\')\n        self.debug_df(df=res[\'rec\'][\'data\'])\n    # end of test_integration_test_fetch_tickers_from_screener\n\n# end of TestFinVizFetchAPI\n'"
tests/test_get_ds_dict.py,0,"b'""""""\nTest file for:\nBuild Dataset Cache Dict\n""""""\n\nfrom analysis_engine.mocks.base_test import BaseTestCase\nfrom analysis_engine.consts import CACHE_DICT_VERSION\nfrom analysis_engine.consts import COMMON_DATE_FORMAT\nfrom analysis_engine.utils import get_last_close_str\nfrom analysis_engine.api_requests import get_ds_dict\n\n\nclass TestBuildDatasetCacheDict(BaseTestCase):\n    """"""TestBuildDatasetCacheDict""""""\n\n    ticker = None\n    last_close_str = None\n\n    def setUp(\n            self):\n        """"""setUp""""""\n        self.ticker = \'AAPL\'\n        self.last_close_str = get_last_close_str(fmt=COMMON_DATE_FORMAT)\n    # end of setUp\n\n    def test_get_ds_dict(self):\n        """"""test_get_ds_dict""""""\n        test_name = \'test_build_dataset_cache_dict\'\n        base_key = f\'{self.ticker}_{self.last_close_str}\'\n        cache_dict = get_ds_dict(\n            ticker=self.ticker,\n            label=test_name)\n\n        self.assertIsNotNone(\n            cache_dict)\n        self.assertEqual(\n            cache_dict[\'ticker\'],\n            self.ticker)\n        self.assertEqual(\n            cache_dict[\'daily\'],\n            f\'{base_key}_daily\')\n        self.assertEqual(\n            cache_dict[\'minute\'],\n            f\'{base_key}_minute\')\n        self.assertEqual(\n            cache_dict[\'quote\'],\n            f\'{base_key}_quote\')\n        self.assertEqual(\n            cache_dict[\'stats\'],\n            f\'{base_key}_stats\')\n        self.assertEqual(\n            cache_dict[\'peers\'],\n            f\'{base_key}_peers\')\n        self.assertEqual(\n            cache_dict[\'news1\'],\n            f\'{base_key}_news1\')\n        self.assertEqual(\n            cache_dict[\'financials\'],\n            f\'{base_key}_financials\')\n        self.assertEqual(\n            cache_dict[\'earnings\'],\n            f\'{base_key}_earnings\')\n        self.assertEqual(\n            cache_dict[\'dividends\'],\n            f\'{base_key}_dividends\')\n        self.assertEqual(\n            cache_dict[\'company\'],\n            f\'{base_key}_company\')\n        self.assertEqual(\n            cache_dict[\'options\'],\n            f\'{base_key}_options\')\n        self.assertEqual(\n            cache_dict[\'pricing\'],\n            f\'{base_key}_pricing\')\n        self.assertEqual(\n            cache_dict[\'news\'],\n            f\'{base_key}_news\')\n        self.assertEqual(\n            cache_dict[\'version\'],\n            CACHE_DICT_VERSION)\n    # end of test_get_ds_dict\n\n# end of TestBuildDatasetCacheDict\n'"
tests/test_get_new_pricing.py,0,"b'""""""\nTest file for:\nupdate prices\n""""""\n\nimport mock\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.mocks.mock_pinance\nimport analysis_engine.mocks.mock_iex\nimport analysis_engine.mocks.base_test as base_test\nimport analysis_engine.work_tasks.get_new_pricing_data as run_get\nimport analysis_engine.api_requests as api_requests\n\n\ndef mock_success_task_result(\n        **kwargs):\n    """"""mock_success_task_result\n\n    :param kwargs: keyword args dict\n    """"""\n    res = kwargs\n    res[\'result\'][\'status\'] = ae_consts.SUCCESS\n    res[\'result\'][\'err\'] = None\n    return res\n# end of mock_success_task_result\n\n\ndef mock_success_iex_fetch(\n        **kwargs):\n    """"""mock_success_iex_fetch\n\n    :param kwargs: keyword args dict\n    """"""\n    res = {\n        \'status\': ae_consts.SUCCESS,\n        \'err\': None,\n        \'rec\': {\n            \'data\': kwargs\n        }\n    }\n    return res\n# end of mock_success_iex_fetch\n\n\ndef mock_success_td_fetch(\n        **kwargs):\n    """"""mock_success_td_fetch\n\n    :param kwargs: keyword args dict\n    """"""\n    res = {\n        \'status\': ae_consts.SUCCESS,\n        \'err\': None,\n        \'rec\': {\n            \'data\': kwargs\n        }\n    }\n    return res\n# end of mock_success_td_fetch\n\n\ndef mock_err_task_result(\n        **kwargs):\n    """"""mock_err_task_result\n\n    :param kwargs: keyword args dict\n    """"""\n    res = kwargs\n    res[\'result\'][\'status\'] = ae_consts.ERR\n    res[\'result\'][\'err\'] = \'test exception\'\n    return res\n# end of mock_err_task_result\n\n\ndef mock_exception_run_publish_pricing_update(\n        **kwargs):\n    """"""mock_exception_run_publish_pricing_update\n\n    :param kwargs: keyword args dict\n    """"""\n    raise Exception(\n        \'test throwing mock_exception_run_publish_pricing_update\')\n# end of mock_exception_run_publish_pricing_update\n\n\ndef mock_error_iex_fetch(\n        **kwargs):\n    """"""mock_error_iex_fetch\n\n    :param kwargs: keyword args dict\n    """"""\n    raise Exception(\n        \'test throwing mock_error_iex_fetch\')\n# end of mock_error_iex_fetch\n\n\nclass TestGetNewPricing(base_test.BaseTestCase):\n    """"""TestGetNewPricing""""""\n\n    @mock.patch(\n        \'pinance.Pinance\',\n        new=analysis_engine.mocks.mock_pinance.MockPinance)\n    @mock.patch(\n        (\'analysis_engine.iex.get_data.\'\n         \'get_data_from_iex\'),\n        new=mock_success_iex_fetch)\n    @mock.patch(\n        (\'analysis_engine.td.get_data.\'\n         \'get_data_from_td\'),\n        new=mock_success_td_fetch)\n    @mock.patch(\n        (\'analysis_engine.get_pricing.\'\n         \'get_options\'),\n        new=analysis_engine.mocks.mock_pinance.mock_get_options)\n    @mock.patch(\n        (\'analysis_engine.get_task_results.\'\n         \'get_task_results\'),\n        new=mock_success_task_result)\n    def test_success_get_new_pricing(self):\n        """"""test_success_get_new_pricing""""""\n        # yahoo is disabled\n        return 0\n        work = api_requests.build_get_new_pricing_request()\n        work[\'label\'] = \'test_success_get_new_pricing\'\n        res = run_get.run_get_new_pricing_data(\n            work)\n        self.assertTrue(\n            res[\'status\'] == ae_consts.SUCCESS)\n        self.assertTrue(\n            res[\'err\'] is None)\n        self.assertIsNotNone(\n            res[\'rec\'][\'news\'])\n        self.assertTrue(\n            len(res[\'rec\'][\'news\']) >= 1)\n        self.assertTrue(\n            len(res[\'rec\'][\'pricing\']) >= 1)\n        self.assertTrue(\n            len(res[\'rec\'][\'options\']) >= 1)\n    # end of test_success_get_new_pricing\n\n    @mock.patch(\n        \'pinance.Pinance\',\n        new=analysis_engine.mocks.mock_pinance.MockPinance)\n    @mock.patch(\n        (\'analysis_engine.iex.get_data.\'\n         \'get_data_from_iex\'),\n        new=mock_success_iex_fetch)\n    @mock.patch(\n        (\'analysis_engine.td.get_data.\'\n         \'get_data_from_td\'),\n        new=mock_success_td_fetch)\n    @mock.patch(\n        (\'analysis_engine.work_tasks.publish_pricing_update.\'\n         \'run_publish_pricing_update\'),\n        new=mock_exception_run_publish_pricing_update)\n    def test_err_get_new_pricing(self):\n        """"""test_err_get_new_pricing""""""\n        work = api_requests.build_get_new_pricing_request()\n        work[\'label\'] = \'test_err_get_new_pricing\'\n        res = run_get.run_get_new_pricing_data(\n            work)\n        self.assertTrue(\n            res[\'status\'] == ae_consts.MISSING_TOKEN or\n            res[\'status\'] == ae_consts.ERR)\n    # end of test_err_get_new_pricing\n\n    @mock.patch(\n        (\'analysis_engine.iex.get_data.\'\n         \'get_data_from_iex\'),\n        new=mock_error_iex_fetch)\n    @mock.patch(\n        (\'analysis_engine.td.get_data.\'\n         \'get_data_from_td\'),\n        new=mock_success_td_fetch)\n    @mock.patch(\n        \'pinance.Pinance\',\n        new=analysis_engine.mocks.mock_pinance.MockPinance)\n    @mock.patch(\n        (\'analysis_engine.get_pricing.\'\n         \'get_options\'),\n        new=analysis_engine.mocks.mock_pinance.mock_get_options)\n    @mock.patch(\n        (\'analysis_engine.get_task_results.\'\n         \'get_task_results\'),\n        new=mock_success_task_result)\n    def test_success_if_iex_errors(self):\n        """"""test_success_if_iex_errors""""""\n        work = api_requests.build_get_new_pricing_request()\n        work[\'label\'] = \'test_success_if_iex_errors\'\n        res = run_get.run_get_new_pricing_data(\n            work)\n        self.assertTrue(\n            res[\'status\'] == ae_consts.SUCCESS)\n    # end of test_success_if_iex_errors\n\n# end of TestGetNewPricing\n'"
tests/test_iex_dataset_extraction.py,0,"b'""""""\nTest file for:\nIEX Extract Data\n""""""\n\nfrom analysis_engine.mocks.base_test import BaseTestCase\nfrom analysis_engine.consts import TICKER\nfrom analysis_engine.consts import SUCCESS\nfrom analysis_engine.consts import ev\nfrom analysis_engine.consts import get_status\nfrom analysis_engine.api_requests \\\n    import get_ds_dict\nfrom analysis_engine.iex.extract_df_from_redis \\\n    import extract_daily_dataset\nfrom analysis_engine.iex.extract_df_from_redis \\\n    import extract_minute_dataset\nfrom analysis_engine.iex.extract_df_from_redis \\\n    import extract_quote_dataset\nfrom analysis_engine.iex.extract_df_from_redis \\\n    import extract_stats_dataset\nfrom analysis_engine.iex.extract_df_from_redis \\\n    import extract_peers_dataset\nfrom analysis_engine.iex.extract_df_from_redis \\\n    import extract_news_dataset\nfrom analysis_engine.iex.extract_df_from_redis \\\n    import extract_financials_dataset\nfrom analysis_engine.iex.extract_df_from_redis \\\n    import extract_earnings_dataset\nfrom analysis_engine.iex.extract_df_from_redis \\\n    import extract_dividends_dataset\nfrom analysis_engine.iex.extract_df_from_redis \\\n    import extract_company_dataset\nfrom spylunking.log.setup_logging import build_colorized_logger\n\nlog = build_colorized_logger(\n    name=__name__)\n\n\nclass TestIEXDatasetExtraction(BaseTestCase):\n    """"""TestIEXDatasetExtraction""""""\n\n    def setUp(self):\n        """"""setUp""""""\n        self.ticker = TICKER\n    # end of setUp\n\n    """"""\n    Integration Tests\n\n    Please ensure redis and minio are running and run this:\n\n    ::\n\n        export INT_TESTS=1\n\n    """"""\n\n    def debug_df(\n            self,\n            df):\n        """"""debug_df\n\n        :param df: ``pandas.DataFrame`` from a fetch\n        """"""\n        print(\'-----------------------------------\')\n        print(f\'dataframe: {df}\')\n        print(\'\')\n        print(f\'dataframe columns:\\n{df.columns.values}\')\n        print(\'-----------------------------------\')\n    # end of debug_df\n\n    def _check(self, df, status, label, work):\n        if status == SUCCESS:\n            self.assertIsNotNone(\n                df)\n            self.debug_df(df=df)\n        else:\n            log.critical(\n                f\'{label} is missing in redis for ticker={work[""ticker""]} \'\n                f\'status={get_status(status=status)}\')\n\n    def test_integration_extract_daily_dataset(self):\n        """"""test_integration_extract_daily_dataset""""""\n        if ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n        ticker = \'NFLX\'\n        label = \'IEX daily dataset\'\n        # build dataset cache dictionary\n        work = get_ds_dict(\n            ticker=ticker,\n            label=label)\n\n        status, df = extract_daily_dataset(\n            work_dict=work)\n        self._check(df=df, status=status, label=label, work=work)\n    # end of test_integration_extract_daily_dataset\n\n    def test_integration_extract_minute_dataset(self):\n        """"""test_integration_extract_minute_dataset""""""\n        if ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n        ticker = \'NFLX\'\n        label = \'IEX minute dataset\'\n        # build dataset cache dictionary\n        work = get_ds_dict(\n            ticker=ticker,\n            label=label)\n\n        status, df = extract_minute_dataset(\n            work_dict=work)\n        self._check(df=df, status=status, label=label, work=work)\n    # end of test_integration_extract_minute_dataset\n\n    def test_integration_extract_quote_dataset(self):\n        """"""test_integration_extract_quote_dataset""""""\n        if ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n        ticker = \'NFLX\'\n        label = \'IEX quote dataset\'\n        # build dataset cache dictionary\n        work = get_ds_dict(\n            ticker=ticker,\n            label=label)\n\n        status, df = extract_quote_dataset(\n            work_dict=work)\n        self._check(df=df, status=status, label=label, work=work)\n    # end of test_integration_extract_quote_dataset\n\n    def test_integration_extract_stats_dataset(self):\n        """"""test_integration_extract_stats_dataset""""""\n        if ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n        ticker = \'NFLX\'\n        label = \'IEX stats dataset\'\n        # build dataset cache dictionary\n        work = get_ds_dict(\n            ticker=ticker,\n            label=label)\n\n        status, df = extract_stats_dataset(\n            work_dict=work)\n        self._check(df=df, status=status, label=label, work=work)\n    # end of test_integration_extract_stats_dataset\n\n    def test_integration_extract_peers_dataset(self):\n        """"""test_integration_extract_peers_dataset""""""\n        if ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n        ticker = \'NFLX\'\n        label = \'IEX peers dataset\'\n        # build dataset cache dictionary\n        work = get_ds_dict(\n            ticker=ticker,\n            label=label)\n\n        status, df = extract_peers_dataset(\n            work_dict=work)\n        self._check(df=df, status=status, label=label, work=work)\n    # end of test_integration_extract_peers_dataset\n\n    def test_integration_extract_news_dataset(self):\n        """"""test_integration_extract_news_dataset""""""\n        if ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n        ticker = \'NFLX\'\n        label = \'IEX news dataset\'\n        # build dataset cache dictionary\n        work = get_ds_dict(\n            ticker=ticker,\n            label=label)\n\n        status, df = extract_news_dataset(\n            work_dict=work)\n        self._check(df=df, status=status, label=label, work=work)\n    # end of test_integration_extract_news_dataset\n\n    def test_integration_extract_financials_dataset(self):\n        """"""test_integration_extract_financials_dataset""""""\n        if ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n        ticker = \'NFLX\'\n        label = \'IEX financials dataset\'\n        # build dataset cache dictionary\n        work = get_ds_dict(\n            ticker=ticker,\n            label=label)\n\n        status, df = extract_financials_dataset(\n            work_dict=work)\n        self._check(df=df, status=status, label=label, work=work)\n    # end of test_integration_extract_financials_dataset\n\n    def test_integration_extract_earnings_dataset(self):\n        """"""test_integration_extract_earnings_dataset""""""\n        if ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n        ticker = \'NFLX\'\n        label = \'IEX earnings dataset\'\n        # build dataset cache dictionary\n        work = get_ds_dict(\n            ticker=ticker,\n            label=label)\n\n        status, df = extract_earnings_dataset(\n            work_dict=work)\n        self._check(df=df, status=status, label=label, work=work)\n    # end of test_integration_extract_earnings_dataset\n\n    def test_integration_extract_dividends_dataset(self):\n        """"""test_integration_extract_dividends_dataset""""""\n        if ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n        ticker = \'NFLX\'\n        label = \'IEX dividends dataset\'\n        # build dataset cache dictionary\n        work = get_ds_dict(\n            ticker=ticker,\n            label=label)\n\n        status, df = extract_dividends_dataset(\n            work_dict=work)\n        self._check(df=df, status=status, label=label, work=work)\n    # end of test_integration_extract_dividends_dataset\n\n    def test_integration_extract_company_dataset(self):\n        """"""test_integration_extract_company_dataset""""""\n        if ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n        ticker = \'NFLX\'\n        label = \'IEX company dataset\'\n        # build dataset cache dictionary\n        work = get_ds_dict(\n            ticker=ticker,\n            label=label)\n\n        status, df = extract_company_dataset(\n            work_dict=work)\n        self._check(df=df, status=status, label=label, work=work)\n    # end of test_integration_extract_company_dataset\n\n# end of TestIEXDatasetExtraction\n'"
tests/test_iex_fetch_data.py,0,"b'""""""\nTest file for:\nIEX Fetch Data\n""""""\n\nimport mock\nimport analysis_engine.mocks.mock_iex as mock_iex\nfrom analysis_engine.mocks.base_test import BaseTestCase\nfrom analysis_engine.consts import ev\nfrom analysis_engine.iex.consts import FETCH_FINANCIALS\nfrom analysis_engine.consts import FETCH_MODE_IEX\nfrom analysis_engine.iex.fetch_data \\\n    import fetch_data\nfrom analysis_engine.api_requests \\\n    import build_iex_fetch_minute_request\nfrom analysis_engine.api_requests \\\n    import build_iex_fetch_daily_request\nfrom analysis_engine.api_requests \\\n    import build_iex_fetch_quote_request\nfrom analysis_engine.api_requests \\\n    import build_iex_fetch_stats_request\nfrom analysis_engine.api_requests \\\n    import build_iex_fetch_peers_request\nfrom analysis_engine.api_requests \\\n    import build_iex_fetch_news_request\nfrom analysis_engine.api_requests \\\n    import build_iex_fetch_financials_request\nfrom analysis_engine.api_requests \\\n    import build_iex_fetch_earnings_request\nfrom analysis_engine.api_requests \\\n    import build_iex_fetch_dividends_request\nfrom analysis_engine.api_requests \\\n    import build_iex_fetch_company_request\nfrom analysis_engine.api_requests \\\n    import build_get_new_pricing_request\nfrom analysis_engine.work_tasks.get_new_pricing_data \\\n    import get_new_pricing_data\n\n\nclass TestIEXFetchData(BaseTestCase):\n    """"""TestIEXFetchData""""""\n\n    @mock.patch(\n        (\'analysis_engine.iex.helpers_for_iex_api.get_from_iex\'),\n        new=mock_iex.mock_daily)\n    def test_fetch_daily(self):\n        """"""test_fetch_daily""""""\n        test_name = \'test_fetch_daily\'\n        work = build_iex_fetch_daily_request(\n            label=test_name)\n\n        work[\'ticker\'] = test_name\n\n        res = fetch_data(\n            work_dict=work)\n        self.assertIsNotNone(\n            res)\n        self.assertEqual(\n            res[\'symbol\'][0],\n            work[\'ticker\'])\n    # end of test_fetch_daily\n\n    @mock.patch(\n        (\'analysis_engine.iex.helpers_for_iex_api.get_from_iex\'),\n        new=mock_iex.mock_minute)\n    def test_fetch_minute(self):\n        """"""test_fetch_minute""""""\n        test_name = \'test_fetch_minute\'\n        work = build_iex_fetch_minute_request(\n            label=test_name)\n\n        work[\'ticker\'] = test_name\n\n        res = fetch_data(\n            work_dict=work)\n        self.assertIsNotNone(\n            res)\n        self.assertEqual(\n            res[\'symbol\'][0],\n            work[\'ticker\'])\n    # end of test_fetch_minute\n\n    @mock.patch(\n        (\'analysis_engine.iex.helpers_for_iex_api.get_from_iex\'),\n        new=mock_iex.mock_quote)\n    def test_fetch_quote(self):\n        """"""test_fetch_quote""""""\n        work = build_iex_fetch_quote_request(\n            label=\'test_fetch_quote\')\n\n        work[\'ticker\'] = \'test_fetch_quote\'\n\n        res = fetch_data(\n            work_dict=work)\n        self.assertIsNotNone(\n            res)\n        self.assertEqual(\n            res[\'symbol\'][0],\n            work[\'ticker\'])\n        self.assertEqual(\n            res[\'testcase\'][0],\n            \'mock-quote\')\n    # end of test_fetch_quote\n\n    @mock.patch(\n        (\'analysis_engine.iex.helpers_for_iex_api.get_from_iex\'),\n        new=mock_iex.mock_stats)\n    def test_fetch_stats(self):\n        """"""test_fetch_stats""""""\n        test_name = \'test_fetch_stats\'\n        work = build_iex_fetch_stats_request(\n            label=test_name)\n\n        work[\'ticker\'] = test_name\n\n        res = fetch_data(\n            work_dict=work)\n        self.assertIsNotNone(\n            res)\n        self.assertEqual(\n            res[\'symbol\'][0],\n            work[\'ticker\'])\n    # end of test_fetch_stats\n\n    @mock.patch(\n        (\'analysis_engine.iex.helpers_for_iex_api.get_from_iex\'),\n        new=mock_iex.mock_peers)\n    def test_fetch_peers(self):\n        """"""test_fetch_peers""""""\n        test_name = \'test_fetch_peers\'\n        work = build_iex_fetch_peers_request(\n            label=test_name)\n\n        work[\'ticker\'] = test_name\n\n        res = fetch_data(\n            work_dict=work)\n        self.assertIsNotNone(\n            res)\n        self.assertEqual(\n            res[\'symbol\'][0],\n            work[\'ticker\'])\n    # end of test_fetch_peers\n\n    @mock.patch(\n        (\'analysis_engine.iex.helpers_for_iex_api.get_from_iex\'),\n        new=mock_iex.mock_news)\n    def test_fetch_news(self):\n        """"""test_fetch_news""""""\n        test_name = \'test_fetch_news\'\n        work = build_iex_fetch_news_request(\n            label=test_name)\n\n        work[\'ticker\'] = test_name\n\n        res = fetch_data(\n            work_dict=work)\n        self.assertIsNotNone(\n            res)\n        self.assertEqual(\n            res[\'symbol\'][0],\n            work[\'ticker\'])\n    # end of test_fetch_news\n\n    @mock.patch(\n        (\'analysis_engine.iex.helpers_for_iex_api.get_from_iex\'),\n        new=mock_iex.mock_financials)\n    def test_fetch_financials(self):\n        """"""test_fetch_financials""""""\n        test_name = \'test_fetch_financials\'\n        work = build_iex_fetch_financials_request(\n            label=test_name)\n\n        work[\'ticker\'] = test_name\n\n        res = fetch_data(\n            work_dict=work)\n        self.assertIsNotNone(\n            res)\n        self.assertEqual(\n            res[\'symbol\'][0],\n            work[\'ticker\'])\n    # end of test_fetch_financials\n\n    @mock.patch(\n        (\'analysis_engine.iex.helpers_for_iex_api.get_from_iex\'),\n        new=mock_iex.mock_earnings)\n    def test_fetch_earnings(self):\n        """"""test_fetch_earnings""""""\n        test_name = \'test_fetch_earnings\'\n        work = build_iex_fetch_earnings_request(\n            label=test_name)\n\n        work[\'ticker\'] = test_name\n\n        res = fetch_data(\n            work_dict=work)\n        self.assertIsNotNone(\n            res)\n        self.assertEqual(\n            res[\'symbol\'][0],\n            work[\'ticker\'])\n    # end of test_fetch_earnings\n\n    @mock.patch(\n        (\'analysis_engine.iex.helpers_for_iex_api.get_from_iex\'),\n        new=mock_iex.mock_dividends)\n    def test_fetch_dividends(self):\n        """"""test_fetch_dividends""""""\n        test_name = \'test_fetch_dividends\'\n        work = build_iex_fetch_dividends_request(\n            label=test_name)\n\n        work[\'ticker\'] = test_name\n\n        res = fetch_data(\n            work_dict=work)\n        self.assertIsNotNone(\n            res)\n        self.assertEqual(\n            res[\'symbol\'][0],\n            work[\'ticker\'])\n    # end of test_fetch_dividends\n\n    @mock.patch(\n        (\'analysis_engine.iex.helpers_for_iex_api.get_from_iex\'),\n        new=mock_iex.mock_company)\n    def test_fetch_company(self):\n        """"""test_fetch_company""""""\n        test_name = \'test_fetch_company\'\n        work = build_iex_fetch_company_request(\n            label=test_name)\n\n        work[\'ticker\'] = test_name\n\n        res = fetch_data(\n            work_dict=work)\n        self.assertIsNotNone(\n            res)\n        self.assertEqual(\n            res[\'symbol\'][0],\n            work[\'ticker\'])\n    # end of test_fetch_company\n\n    """"""\n    Integration Tests\n\n    Please ensure redis and minio are running and run this:\n\n    ::\n\n        export INT_TESTS=1\n\n    """"""\n\n    def debug_df(\n            self,\n            df):\n        """"""debug_df\n\n        :param df: ``pandas.DataFrame`` from a fetch\n        """"""\n        print(\'-----------------------------------\')\n        print(f\'dataframe: {df}\')\n        print(\'\')\n        print(f\'dataframe columns:\\n{df.columns.values}\')\n        print(\'-----------------------------------\')\n    # end of debug_df\n\n    def test_integration_fetch_daily(self):\n        """"""test_integration_fetch_daily""""""\n        if ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n\n        # store data\n        work = build_iex_fetch_daily_request(\n            label=\'test_integration_fetch_daily\')\n\n        res = fetch_data(\n            work_dict=work)\n        self.assertIsNotNone(\n            res)\n        self.debug_df(df=res)\n    # end of test_integration_fetch_daily\n\n    def test_integration_fetch_minute(self):\n        """"""test_integration_fetch_minute""""""\n        if ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n\n        # store data\n        work = build_iex_fetch_minute_request(\n            label=\'test_integration_fetch_minute\')\n\n        res = fetch_data(\n            work_dict=work)\n        self.assertIsNotNone(\n            res)\n        self.debug_df(df=res)\n    # end of test_integration_fetch_minute\n\n    def test_integration_fetch_quote(self):\n        """"""test_integration_fetch_quote""""""\n        if ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n\n        # store data\n        work = build_iex_fetch_quote_request(\n            label=\'test_integration_fetch_quote\')\n\n        res = fetch_data(\n            work_dict=work)\n        self.assertIsNotNone(\n            res)\n        self.debug_df(df=res)\n    # end of test_integration_fetch_quote\n\n    def test_integration_fetch_stats(self):\n        """"""test_integration_fetch_stats""""""\n        if ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n\n        # store data\n        work = build_iex_fetch_stats_request(\n            label=\'test_integration_fetch_stats\')\n\n        res = fetch_data(\n            work_dict=work)\n        self.assertIsNotNone(\n            res)\n        self.debug_df(df=res)\n    # end of test_integration_fetch_stats\n\n    def test_integration_fetch_peers(self):\n        """"""test_integration_fetch_peers""""""\n        if ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n\n        # store data\n        work = build_iex_fetch_peers_request(\n            label=\'test_integration_fetch_peers\')\n\n        res = fetch_data(\n            work_dict=work)\n        self.assertIsNotNone(\n            res)\n        self.debug_df(df=res)\n    # end of test_integration_fetch_peers\n\n    def test_integration_fetch_news(self):\n        """"""test_integration_fetch_news""""""\n        if ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n\n        # store data\n        work = build_iex_fetch_news_request(\n            label=\'test_integration_fetch_news\')\n\n        res = fetch_data(\n            work_dict=work)\n        self.assertIsNotNone(\n            res)\n        self.debug_df(df=res)\n    # end of test_integration_fetch_news\n\n    def test_integration_fetch_financials(self):\n        """"""test_integration_fetch_financials""""""\n        if ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n\n        label = \'test_integration_fetch_financials\'\n\n        # store data\n        work = build_iex_fetch_financials_request(\n            label=label)\n        work[\'ticker\'] = \'TSLA\'\n\n        res = fetch_data(\n            work_dict=work)\n        self.assertIsNotNone(\n            res)\n        self.debug_df(df=res)\n    # end of test_integration_fetch_financials\n\n    def test_integration_fetch_earnings(self):\n        """"""test_integration_fetch_earnings""""""\n        if ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n\n        # store data\n        work = build_iex_fetch_earnings_request(\n            label=\'test_integration_fetch_earnings\')\n        work[\'ticker\'] = \'AAPL\'\n\n        res = fetch_data(\n            work_dict=work)\n        self.assertIsNotNone(\n            res)\n        self.debug_df(df=res)\n    # end of test_integration_fetch_earnings\n\n    def test_integration_fetch_dividends(self):\n        """"""test_integration_fetch_dividends""""""\n        if ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n\n        # store data\n        work = build_iex_fetch_dividends_request(\n            label=\'test_integration_fetch_dividends\')\n        work[\'ticker\'] = \'AAPL\'\n\n        res = fetch_data(\n            work_dict=work)\n        self.assertIsNotNone(\n            res)\n        self.debug_df(df=res)\n    # end of test_integration_fetch_dividends\n\n    def test_integration_fetch_company(self):\n        """"""test_integration_fetch_company""""""\n        if ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n\n        # store data\n        work = build_iex_fetch_company_request(\n            label=\'test_integration_fetch_company\')\n\n        res = fetch_data(\n            work_dict=work)\n        self.assertIsNotNone(\n            res)\n        self.debug_df(df=res)\n    # end of test_integration_fetch_company\n\n    def test_integration_get_financials_helper(self):\n        """"""test_integration_get_financials_helper\n\n        After running, there should be an updated timestamp on\n        the s3 key:\n\n        ::\n\n            testing_<TICKER>_financials\n\n        View the financials bucket:\n\n        ::\n\n            aws --endpoint-url http://localhost:9000 s3 ls s3://financials\n\n        View the redis cache using the redis-cli:\n\n        ::\n\n            ./tools/redis-cli.sh\n            127.0.0.1:6379> keys testing_TSLA_financials\n            1) ""testing_TSLA_financials""\n\n        """"""\n        if ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n\n        label = \'test_integration_get_financials_helper\'\n\n        # store data\n        work = build_get_new_pricing_request(\n            label=label)\n\n        work[\'fetch_mode\'] = FETCH_MODE_IEX\n        work[\'iex_datasets\'] = [\n            FETCH_FINANCIALS\n        ]\n        work[\'ticker\'] = \'AAPL\'\n        work[\'s3_bucket\'] = \'testing\'\n        work[\'s3_key\'] = f\'testing_{work[""ticker""]}\'\n        work[\'redis_key\'] = f\'testing_{work[""ticker""]}\'\n        work[\'celery_disabled\'] = True\n        dataset_results = get_new_pricing_data(\n            work)\n\n        self.assertIsNotNone(\n            dataset_results)\n        self.assertIsNotNone(\n            len(dataset_results[\'rec\'][\'financials\']) >= 5)\n    # end of test_integration_get_financials_helper\n\n# end of TestIEXFetchData\n'"
tests/test_indicator_processor.py,0,"b'""""""\nTest file for classes and functions:\n\n- analysis_engine.indicators.base_indicator\n- analysis_engine.indicators.indicator_processor\n\n""""""\n\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.mocks.base_test as base_test\nimport analysis_engine.indicators.indicator_processor as ind_proc\n\n\nclass TestIndicatorProcessor(base_test.BaseTestCase):\n    """"""TestIndicatorProcessor""""""\n\n    ticker = None\n    last_close_str = None\n\n    def setUp(\n            self):\n        """"""setUp""""""\n        self.ticker = \'SPY\'\n        self.example_module_path = (\n            \'analysis_engine/mocks/example_indicator_williamsr.py\')\n        self.test_data = {\n            ""name"": ""test_5_days_ahead"",\n            ""algo_module_path"": None,\n            ""algo_version"": 1,\n            ""trade_horizon_units"": ""day"",\n            ""trade_horizon"": 5,\n            ""buy_rules"": {\n                ""confidence"": 75,\n                ""min_indicators"": 1\n            },\n            ""sell_rules"": {\n                ""confidence"": 75,\n                ""min_indicators"": 1\n            },\n            ""indicators"": [\n                {\n                    ""name"": ""willr_1"",\n                    ""module_path"": self.example_module_path,\n                    ""category"": ""technical"",\n                    ""type"": ""momentum"",\n                    ""uses_data"": ""daily"",\n                    ""num_points"": 19,\n                    ""buy_above"": 80,\n                    ""sell_below"": 10\n                },\n                {\n                    ""name"": ""willr_2"",\n                    ""module_path"": self.example_module_path,\n                    ""category"": ""technical"",\n                    ""type"": ""momentum"",\n                    ""uses_data"": ""daily"",\n                    ""num_points"": 15,\n                    ""buy_above"": 60,\n                    ""sell_below"": 20\n                },\n                {\n                    ""name"": ""baseindicator"",\n                    ""category"": ""fundamental"",\n                    ""type"": ""balance_sheet"",\n                    ""uses_data"": ""daily""\n                }\n            ],\n            ""slack"": {\n                ""webhook"": None\n            }\n        }\n    # end of setUp\n\n    def test_build_indicator_processor(self):\n        """"""test_build_algo_request_daily""""""\n        print(self.test_data)\n        proc = ind_proc.IndicatorProcessor(\n            config_dict=self.test_data)\n        self.assertTrue(\n            len(proc.get_indicators()) == 3)\n        indicators = proc.get_indicators()\n        for idx, ind_id in enumerate(indicators):\n            ind_node = indicators[ind_id]\n            print(ind_node)\n            self.assertTrue(\n                ind_node[\'obj\'] is not None)\n            if idx == 2:\n                self.assertEqual(\n                    ind_node[\'report\'][\'path_to_module\'],\n                    ae_consts.INDICATOR_BASE_MODULE_PATH)\n            else:\n                self.assertEqual(\n                    ind_node[\'report\'][\'path_to_module\'],\n                    self.example_module_path)\n    # end of test_build_indicator_processor\n\n# end of TestIndicatorProcessor\n'"
tests/test_load_indicator_from_file.py,0,"b'""""""\nTest file for classes and functions:\n\n- analysis_engine.indicators.load_indicator_from_module\n- analysis_engine.indicators.indicator_processor\n\n""""""\n\nimport mock\nimport analysis_engine.mocks.mock_talib as mock_talib\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.indicators.load_indicator_from_module as load_ind\nimport analysis_engine.mocks.base_test as base_test\n\n\nclass TestLoadIndicatorFromFile(base_test.BaseTestCase):\n    """"""TestLoadIndicatorFromFile""""""\n\n    ticker = None\n    last_close_str = None\n\n    def setUp(\n            self):\n        """"""setUp""""""\n        self.ticker = \'SPY\'\n        self.example_module_path = (\n            \'analysis_engine/mocks/example_indicator_williamsr.py\')\n        self.test_data = {\n            ""name"": ""test_5_days_ahead"",\n            ""algo_module_path"": None,\n            ""algo_version"": 1,\n            ""trade_horizon_units"": ""day"",\n            ""trade_horizon"": 5,\n            ""buy_rules"": {\n                ""confidence"": 75,\n                ""min_indicators"": 1\n            },\n            ""sell_rules"": {\n                ""confidence"": 75,\n                ""min_indicators"": 1\n            },\n            ""indicators"": [\n                {\n                    ""name"": ""willr"",\n                    ""module_path"": self.example_module_path,\n                    ""category"": ""technical"",\n                    ""type"": ""momentum"",\n                    ""dataset_df"": ""daily"",\n                    \'high\': 0,\n                    \'low\': 0,\n                    \'open\': 0,\n                    \'willr_value\': 0,\n                    ""num_points"": 19,\n                    ""buy_above"": 80,\n                    ""sell_below"": 10\n                },\n                {\n                    ""name"": ""willr"",\n                    ""module_path"": self.example_module_path,\n                    ""category"": ""technical"",\n                    ""type"": ""momentum"",\n                    ""dataset_df"": ""daily"",\n                    \'high\': 0,\n                    \'low\': 0,\n                    \'open\': 0,\n                    \'willr_value\': 0,\n                    ""num_points"": 15,\n                    ""buy_above"": 60,\n                    ""sell_below"": 20\n                },\n                {\n                    ""name"": ""baseindicator"",\n                    ""category"": ""fundamental"",\n                    ""type"": ""balance_sheet"",\n                    ""dataset_df"": ""daily""\n                }\n            ],\n            ""slack"": {\n                ""webhook"": None\n            }\n        }\n    # end of setUp\n\n    @mock.patch(\n        (\'analysis_engine.ae_talib.WILLR\'),\n        new=mock_talib.MockWILLRBuy)\n    def test_load_indicator_from_example_indicator_file(self):\n        """"""test_load_indicator_from_example_indicator_file""""""\n        log_label = \'my_ind_1\'\n        ind_1 = load_ind.load_indicator_from_module(\n            module_name=\'ExampleIndicatorWilliamsR\',\n            log_label=log_label,\n            path_to_module=self.example_module_path,\n            ind_dict=self.test_data[\'indicators\'][0])\n        self.assertTrue(\n            ind_1 is not None)\n        self.assertEqual(\n            ind_1.get_name(),\n            log_label)\n    # end of test_load_indicator_from_example_indicator_file\n\n    @mock.patch(\n        (\'analysis_engine.ae_talib.WILLR\'),\n        new=mock_talib.MockWILLRBuy)\n    def test_load_multiple_indicator_from_same_example_indicator_file(self):\n        """"""test_load_multiple_indicator_from_same_example_indicator_file""""""\n        log_label_1 = \'my_ind_1\'\n        log_label_2 = \'my_ind_2\'\n        log_label_3 = \'base_ind_1\'\n        ind_1 = load_ind.load_indicator_from_module(\n            module_name=\'ExampleIndicatorWilliamsR\',\n            log_label=log_label_1,\n            path_to_module=self.example_module_path,\n            ind_dict=self.test_data[\'indicators\'][0])\n        self.assertTrue(\n            ind_1 is not None)\n        self.assertEqual(\n            ind_1.get_name(),\n            log_label_1)\n        ind_2 = load_ind.load_indicator_from_module(\n            module_name=\'ExampleIndicatorWilliamsR\',\n            log_label=log_label_2,\n            path_to_module=self.example_module_path,\n            ind_dict=self.test_data[\'indicators\'][1])\n        self.assertTrue(\n            ind_2 is not None)\n        self.assertEqual(\n            ind_2.get_name(),\n            log_label_2)\n        self.assertEqual(\n            ind_1.get_path_to_module(),\n            ind_2.get_path_to_module())\n        ind_3 = load_ind.load_indicator_from_module(\n            module_name=\'BaseIndicator\',\n            log_label=log_label_3,\n            ind_dict=self.test_data[\'indicators\'][2])\n        self.assertEqual(\n            ind_3.get_path_to_module(),\n            ae_consts.INDICATOR_BASE_MODULE_PATH)\n        self.assertEqual(\n            ind_3.get_name(),\n            log_label_3)\n    # end of test_load_indicator_from_example_indicator_file\n\n# end of TestLoadIndicatorFromFile\n'"
tests/test_prepare_pricing_dataset.py,0,"b'""""""\n\nTest file for - publish from s3 to redis\n========================================\n\nIntegration Tests\n-----------------\n\nPlease ensure ``redis`` and ``minio`` are running and export this:\n\n::\n\n    export INT_TESTS=1\n\n""""""\n\nimport os\nimport uuid\nimport json\nimport mock\nfrom analysis_engine.mocks.mock_boto3_s3 import \\\n    build_boto3_resource\nfrom analysis_engine.mocks.mock_boto3_s3 import \\\n    mock_publish_from_s3_to_redis\nfrom analysis_engine.mocks.mock_boto3_s3 import \\\n    mock_publish_from_s3_to_redis_err\nfrom analysis_engine.mocks.mock_boto3_s3 import \\\n    mock_publish_from_s3_exception\nfrom analysis_engine.mocks.mock_redis import MockRedis\nfrom analysis_engine.mocks.mock_redis import MockRedisFailToConnect\nfrom analysis_engine.mocks.base_test import BaseTestCase\nfrom analysis_engine.consts import SUCCESS\nfrom analysis_engine.consts import ERR\nfrom analysis_engine.consts import PREPARE_DATA_MIN_SIZE\nfrom analysis_engine.api_requests \\\n    import build_cache_ready_pricing_dataset\nfrom analysis_engine.work_tasks.prepare_pricing_dataset \\\n    import run_prepare_pricing_dataset\nfrom analysis_engine.api_requests \\\n    import build_prepare_dataset_request\nfrom spylunking.log.setup_logging import build_colorized_logger\n\n\nlog = build_colorized_logger(\n    name=__name__)\n\n\ndef mock_success_task_result(\n        **kwargs):\n    """"""mock_success_task_result\n\n    :param kwargs: keyword args dict\n    """"""\n    log.info(\'MOCK - mock_success_task_result\')\n    res = kwargs\n    res[\'result\'][\'status\'] = SUCCESS\n    res[\'result\'][\'err\'] = None\n    return res\n# end of mock_success_task_result\n\n\ndef mock_err_task_result(\n        **kwargs):\n    """"""mock_err_task_result\n\n    :param kwargs: keyword args dict\n    """"""\n    log.info(\'MOCK - mock_err_task_result\')\n    res = kwargs\n    res[\'result\'][\'status\'] = ERR\n    res[\'result\'][\'err\'] = \'test exception\'\n    return res\n# end of mock_err_task_result\n\n\ndef mock_s3_read_contents_from_key(\n        s3,\n        s3_bucket_name,\n        s3_key,\n        encoding=\'utf-8\',\n        convert_as_json=True):\n    """"""mock_s3_read_contents_from_key\n\n    Download the S3 key contents as a string. This\n    will raise exceptions.\n\n    :param s3_obj: existing S3 object\n    :param s3_bucket_name: bucket name\n    :param s3_key: S3 key\n    :param encoding: utf-8 by default\n    :param convert_to_json: auto-convert to a dict\n    """"""\n    log.info(\'MOCK - mock_s3_read_contents_from_key\')\n    data = build_cache_ready_pricing_dataset()\n    if not convert_as_json:\n        data = json.dumps(data)\n    return data\n# end of mock_s3_read_contents_from_key\n\n\ndef mock_exception_run_publish_pricing_update(\n        **kwargs):\n    """"""mock_exception_run_publish_pricing_update\n\n    :param kwargs: keyword args dict\n    """"""\n    raise Exception(\n        \'test throwing mock_exception_run_publish_pricing_update\')\n# end of mock_exception_run_publish_pricing_update\n\n\ndef mock_redis_get_exception(\n        **kwargs):\n    """"""mock_redis_get_exception\n\n    :param kwargs: keyword args dict\n    """"""\n    raise Exception(\n        \'test throwing mock_redis_get_exception\')\n# end of mock_redis_get_exception\n\n\ndef mock_flatten_dict_err(\n        **kwargs):\n    """"""mock_flatten_dict_err\n\n    :param kwargs: keyword args dict\n    """"""\n    raise Exception(\n        \'test throwing mock_flatten_dict_err\')\n# end of mock_flatten_dict_err\n\n\ndef mock_flatten_dict_empty(\n        **kwags):\n    """"""mock_flatten_dict_empty\n\n    :param kwargs: keyword args dict\n    """"""\n    return None\n# end of mock_flatten_dict_empty\n\n\ndef mock_redis_get_data_none(\n        **kwargs):\n    return {\n        \'status\': SUCCESS,\n        \'err\': None,\n        \'rec\': {\n            \'data\': None\n        }\n    }\n# end of mock_redis_get_data_none\n\n\ndef mock_flatten_dict_too_small(\n        **kwargs):\n    """"""mock_flatten_dict_too_small\n\n    :param kwargs: keyword args dict\n    """"""\n    return \'012345678901234567890\'[0:PREPARE_DATA_MIN_SIZE-1]\n# end of mock_flatten_dict_too_small\n\n\nclass TestPreparePricingDataset(BaseTestCase):\n    """"""TestPreparePricingDataset""""""\n\n    def build_test_key(\n            self,\n            test_name=None):\n        """"""build_test_key\n\n        :param test_name: use this test label name\n        """"""\n        use_test_name = test_name\n        if not use_test_name:\n            use_test_name = str(uuid.uuid4())\n        test_key = f\'{__name__}_{use_test_name}\'\n        return test_key\n    # end of build_test_key\n\n    # throws on redis client creation\n    @mock.patch(\n        (\'redis.Redis\'),\n        new=MockRedisFailToConnect)\n    def test_redis_connection_exception_prepare_pricing(self):\n        """"""test_redis_connection_exception_prepare_pricing""""""\n        expected_err = \'test MockRedisFailToConnect\'\n        redis_key = self.build_test_key(\n            test_name=\'test_redis_connection_exception_prepare_pricing\')\n        s3_key = redis_key\n        work = build_prepare_dataset_request()\n        work[\'s3_key\'] = s3_key\n        work[\'redis_key\'] = redis_key\n        res = run_prepare_pricing_dataset(\n            work)\n        self.assertEqual(\n            res[\'status\'],\n            ERR)\n        self.assertTrue(\n            expected_err in res[\'err\'])\n        self.assertTrue(\n            res[\'rec\'] is not None)\n    # end of test_redis_connection_exception_prepare_pricing\n\n    # this will mock the redis set and get\n    @mock.patch(\n        (\'redis.Redis\'),\n        new=MockRedis)\n    @mock.patch(\n        (\'analysis_engine.get_data_from_redis_key.\'\n         \'get_data_from_redis_key\'),\n        new=mock_redis_get_exception)\n    def test_redis_get_exception_prepare_pricing(self):\n        """"""test_redis_get_exception_prepare_pricing""""""\n        expected_err = \'test throwing mock_redis_get_exception\'\n        redis_key = self.build_test_key(\n            test_name=\'test_redis_get_exception_prepare_pricing\')\n        s3_key = redis_key\n        work = build_prepare_dataset_request()\n        work[\'s3_key\'] = s3_key\n        work[\'redis_key\'] = redis_key\n        res = run_prepare_pricing_dataset(\n            work)\n        self.assertEqual(\n            res[\'status\'],\n            ERR)\n        self.assertTrue(\n            expected_err in res[\'err\'])\n        self.assertTrue(\n            res[\'rec\'] is not None)\n    # end of test_redis_get_exception_prepare_pricing\n\n    # this will mock the redis set and get\n    @mock.patch(\n        (\'redis.Redis\'),\n        new=MockRedis)\n    @mock.patch(\n        (\'boto3.resource\'),\n        new=build_boto3_resource)\n    @mock.patch(\n        (\'analysis_engine.s3_read_contents_from_key.\'\n         \'s3_read_contents_from_key\'),\n        new=mock_s3_read_contents_from_key)\n    @mock.patch(\n        (\'analysis_engine.work_tasks.publish_from_s3_to_redis.\'\n         \'publish_from_s3_to_redis\'),\n        new=mock_publish_from_s3_to_redis)\n    def test_redis_get_no_data_found_for_prepare_pricing(self):\n        """"""test_redis_get_no_data_found_for_prepare_pricing""""""\n        expected_err = (\n            \'did not find any data to prepare in redis_key=\')\n        test_name = \'test_redis_get_no_data_found_for_prepare_pricing\'\n        redis_key = self.build_test_key(\n            test_name=test_name)\n        s3_key = redis_key\n        os.environ.pop(\'TEST_S3_CONTENTS\', None)\n        work = build_prepare_dataset_request()\n        work[\'s3_key\'] = s3_key\n        work[\'redis_key\'] = redis_key\n        res = run_prepare_pricing_dataset(\n            work)\n        self.assertEqual(\n            res[\'status\'],\n            ERR)\n        self.assertTrue(\n            expected_err in res[\'err\'])\n        self.assertTrue(\n            res[\'rec\'] is not None)\n    # end of test_redis_get_no_data_found_for_prepare_pricing\n\n    # this will mock the redis set and get\n    @mock.patch(\n        (\'redis.Redis\'),\n        new=MockRedis)\n    @mock.patch(\n        (\'boto3.resource\'),\n        new=build_boto3_resource)\n    @mock.patch(\n        (\'analysis_engine.s3_read_contents_from_key.\'\n         \'s3_read_contents_from_key\'),\n        new=mock_s3_read_contents_from_key)\n    @mock.patch(\n        (\'analysis_engine.work_tasks.publish_from_s3_to_redis.\'\n         \'publish_from_s3_to_redis\'),\n        new=mock_publish_from_s3_to_redis_err)\n    def test_redis_get_no_data_found_for_prepare_pricing_err(self):\n        """"""test_redis_get_no_data_found_for_prepare_pricing_err""""""\n        expected_err = (\n            \'prepare ERR failed loading from bucket\')\n        test_name = \'test_redis_get_no_data_found_for_prepare_pricing_err\'\n        redis_key = self.build_test_key(\n            test_name=test_name)\n        s3_key = redis_key\n        value = {\n            \'test_name\': test_name\n        }\n        os.environ[\'TEST_S3_CONTENTS\'] = json.dumps(value)\n        work = build_prepare_dataset_request()\n        work[\'s3_key\'] = s3_key\n        work[\'redis_key\'] = redis_key\n        res = run_prepare_pricing_dataset(\n            work)\n        self.assertEqual(\n            res[\'status\'],\n            ERR)\n        self.assertTrue(\n            expected_err in res[\'err\'])\n        self.assertTrue(\n            res[\'rec\'] is not None)\n    # end of test_redis_get_no_data_found_for_prepare_pricing_err\n\n    # this will mock the redis set and get\n    @mock.patch(\n        (\'redis.Redis\'),\n        new=MockRedis)\n    @mock.patch(\n        (\'boto3.resource\'),\n        new=build_boto3_resource)\n    @mock.patch(\n        (\'analysis_engine.s3_read_contents_from_key.\'\n         \'s3_read_contents_from_key\'),\n        new=mock_s3_read_contents_from_key)\n    @mock.patch(\n        (\'analysis_engine.work_tasks.publish_from_s3_to_redis.\'\n         \'publish_from_s3_to_redis\'),\n        new=mock_publish_from_s3_to_redis)\n    def test_data_invalid_json_to_prepare(self):\n        """"""test_data_invalid_json_to_prepare""""""\n        test_name = \'test_data_invalid_json_to_prepare\'\n        redis_key = self.build_test_key(\n            test_name=test_name)\n        s3_key = redis_key\n        expected_err = (\n            \'prepare did not find any data to prepare in \'\n            f\'redis_key={redis_key}\')\n        value = {\n            \'BAD_JSON\': test_name\n        }\n        value_str = json.dumps(value)[0:PREPARE_DATA_MIN_SIZE-1]\n        os.environ[\'TEST_S3_CONTENTS\'] = value_str\n        work = build_prepare_dataset_request()\n        work[\'s3_key\'] = s3_key\n        work[\'redis_key\'] = redis_key\n        res = run_prepare_pricing_dataset(\n            work)\n        self.assertEqual(\n            res[\'status\'],\n            ERR)\n        self.assertTrue(\n            expected_err in res[\'err\'])\n        self.assertTrue(\n            res[\'rec\'] is not None)\n    # end of test_data_invalid_json_to_prepare\n\n    # this will mock the redis set and get\n    @mock.patch(\n        (\'redis.Redis\'),\n        new=MockRedis)\n    @mock.patch(\n        (\'boto3.resource\'),\n        new=build_boto3_resource)\n    @mock.patch(\n        (\'analysis_engine.s3_read_contents_from_key.\'\n         \'s3_read_contents_from_key\'),\n        new=mock_s3_read_contents_from_key)\n    @mock.patch(\n        (\'analysis_engine.work_tasks.publish_from_s3_to_redis.\'\n         \'publish_from_s3_to_redis\'),\n        new=mock_publish_from_s3_to_redis)\n    def test_data_too_small_to_prepare(self):\n        """"""test_data_too_small_to_prepare""""""\n        expected_err = (\n            \'not enough data=\')\n        test_name = \'test_data_too_small_to_prepare\'\n        redis_key = self.build_test_key(\n            test_name=test_name)\n        s3_key = redis_key\n        value = {\n            \'0\': \'1\'\n        }\n        value_str = json.dumps(value)[0:PREPARE_DATA_MIN_SIZE-1]\n        os.environ[\'TEST_S3_CONTENTS\'] = value_str\n        work = build_prepare_dataset_request()\n        work[\'s3_key\'] = s3_key\n        work[\'redis_key\'] = redis_key\n        res = run_prepare_pricing_dataset(\n            work)\n        self.assertEqual(\n            res[\'status\'],\n            ERR)\n        self.assertTrue(\n            expected_err in res[\'err\'])\n        self.assertTrue(\n            res[\'rec\'] is not None)\n    # end of test_data_too_small_to_prepare\n\n    # this will mock the redis set and get\n    @mock.patch(\n        (\'redis.Redis\'),\n        new=MockRedis)\n    @mock.patch(\n        (\'analysis_engine.work_tasks.publish_from_s3_to_redis.\'\n         \'publish_from_s3_to_redis\'),\n        new=mock_publish_from_s3_exception)\n    def test_s3_publish_exception(self):\n        """"""test_s3_publish_exception""""""\n        expected_err = \'test mock_publish_from_s3_exception\'\n        redis_key = self.build_test_key(\n            test_name=\'test_s3_publish_exception\')\n        s3_key = redis_key\n        work = build_prepare_dataset_request()\n        work[\'s3_key\'] = s3_key\n        work[\'redis_key\'] = redis_key\n        res = run_prepare_pricing_dataset(\n            work)\n        self.assertEqual(\n            res[\'status\'],\n            ERR)\n        self.assertTrue(\n            expected_err in res[\'err\'])\n        self.assertTrue(\n            res[\'rec\'] is not None)\n    # end of test_s3_publish_exception\n\n    # this will mock the redis set and get\n    @mock.patch(\n        (\'redis.Redis\'),\n        new=MockRedis)\n    @mock.patch(\n        (\'boto3.resource\'),\n        new=build_boto3_resource)\n    @mock.patch(\n        (\'analysis_engine.s3_read_contents_from_key.\'\n         \'s3_read_contents_from_key\'),\n        new=mock_s3_read_contents_from_key)\n    @mock.patch(\n        (\'analysis_engine.work_tasks.publish_from_s3_to_redis.\'\n         \'publish_from_s3_to_redis\'),\n        new=mock_publish_from_s3_to_redis)\n    @mock.patch(\n        (\'analysis_engine.dict_to_csv.flatten_dict\'),\n        new=mock_flatten_dict_err)\n    def test_failed_flatten_dict(self):\n        """"""test_failed_flatten_dict""""""\n        expected_err = (\n            \'flatten - convert to csv failed with ex=\')\n        test_name = \'test_failed_flatten_dict\'\n        redis_key = self.build_test_key(\n            test_name=test_name)\n        s3_key = redis_key\n        value = self.get_pricing_test_data()\n        value_str = json.dumps(value)\n        os.environ[\'TEST_S3_CONTENTS\'] = value_str\n        work = build_prepare_dataset_request()\n        work[\'s3_key\'] = s3_key\n        work[\'redis_key\'] = redis_key\n        res = run_prepare_pricing_dataset(\n            work)\n        self.assertEqual(\n            res[\'status\'],\n            ERR)\n        self.assertTrue(\n            expected_err in res[\'err\'])\n        self.assertTrue(\n            res[\'rec\'] is not None)\n    # end of test_failed_flatten_dict\n\n    # this will mock the redis set and get\n    @mock.patch(\n        (\'redis.Redis\'),\n        new=MockRedis)\n    @mock.patch(\n        (\'boto3.resource\'),\n        new=build_boto3_resource)\n    @mock.patch(\n        (\'analysis_engine.s3_read_contents_from_key.\'\n         \'s3_read_contents_from_key\'),\n        new=mock_s3_read_contents_from_key)\n    @mock.patch(\n        (\'analysis_engine.work_tasks.publish_from_s3_to_redis.\'\n         \'publish_from_s3_to_redis\'),\n        new=mock_publish_from_s3_to_redis)\n    @mock.patch(\n        (\'analysis_engine.dict_to_csv.flatten_dict\'),\n        new=mock_flatten_dict_empty)\n    def test_empty_flatten_dict(self):\n        """"""test_empty_flatten_dict""""""\n        expected_err = (\n            \'flatten - did not return any data from redis_key=\')\n        test_name = \'test_empty_flatten_dict\'\n        redis_key = self.build_test_key(\n            test_name=test_name)\n        s3_key = redis_key\n        value = self.get_pricing_test_data()\n        value_str = json.dumps(value)\n        os.environ[\'TEST_S3_CONTENTS\'] = value_str\n        work = build_prepare_dataset_request()\n        work[\'s3_key\'] = s3_key\n        work[\'redis_key\'] = redis_key\n        res = run_prepare_pricing_dataset(\n            work)\n        self.assertEqual(\n            res[\'status\'],\n            ERR)\n        self.assertTrue(\n            expected_err in res[\'err\'])\n        self.assertTrue(\n            res[\'rec\'] is not None)\n    # end of test_empty_flatten_dict\n\n    # this will mock the redis set and get\n    @mock.patch(\n        (\'redis.Redis\'),\n        new=MockRedis)\n    @mock.patch(\n        (\'boto3.resource\'),\n        new=build_boto3_resource)\n    @mock.patch(\n        (\'analysis_engine.s3_read_contents_from_key.\'\n         \'s3_read_contents_from_key\'),\n        new=mock_s3_read_contents_from_key)\n    @mock.patch(\n        (\'analysis_engine.work_tasks.publish_from_s3_to_redis.\'\n         \'publish_from_s3_to_redis\'),\n        new=mock_publish_from_s3_to_redis)\n    @mock.patch(\n        (\'analysis_engine.dict_to_csv.flatten_dict\'),\n        new=mock_flatten_dict_too_small)\n    def test_too_small_flatten_dict(self):\n        """"""test_too_small_flatten_dict""""""\n        expected_err = (\n            \'prepare - there is not enough data=\')\n        test_name = \'test_too_small_flatten_dict\'\n        redis_key = self.build_test_key(\n            test_name=test_name)\n        s3_key = redis_key\n        value = self.get_pricing_test_data()\n        value_str = json.dumps(value)\n        os.environ[\'TEST_S3_CONTENTS\'] = value_str\n        work = build_prepare_dataset_request()\n        work[\'s3_key\'] = s3_key\n        work[\'redis_key\'] = redis_key\n        res = run_prepare_pricing_dataset(\n            work)\n        self.assertEqual(\n            res[\'status\'],\n            ERR)\n        self.assertTrue(\n            expected_err in res[\'err\'])\n        self.assertTrue(\n            res[\'rec\'] is not None)\n    # end of test_too_small_flatten_dict\n\n    # this will mock the redis set and get\n    @mock.patch(\n        (\'redis.Redis\'),\n        new=MockRedis)\n    @mock.patch(\n        (\'boto3.resource\'),\n        new=build_boto3_resource)\n    @mock.patch(\n        (\'analysis_engine.s3_read_contents_from_key.\'\n         \'s3_read_contents_from_key\'),\n        new=mock_s3_read_contents_from_key)\n    @mock.patch(\n        (\'analysis_engine.work_tasks.publish_from_s3_to_redis.\'\n         \'publish_from_s3_to_redis\'),\n        new=mock_publish_from_s3_to_redis)\n    def test_prepare_pricing_data_success(self):\n        """"""test_prepare_pricing_data_success""""""\n        test_name = \'test_prepare_pricing_data_success\'\n        redis_key = self.build_test_key(\n            test_name=test_name)\n        s3_key = redis_key\n        value = self.get_pricing_test_data()\n        value_str = json.dumps(value)\n        os.environ[\'TEST_S3_CONTENTS\'] = value_str\n        work = build_prepare_dataset_request()\n        work[\'s3_key\'] = s3_key\n        work[\'redis_key\'] = redis_key\n        res = run_prepare_pricing_dataset(\n            work)\n        self.assertEqual(\n            res[\'status\'],\n            SUCCESS)\n        self.assertEqual(\n            res[\'err\'],\n            None)\n        self.assertTrue(\n            res[\'rec\'] is not None)\n        self.assertEqual(\n            res[\'rec\'][\'initial_size\'],\n            3111)\n        self.assertTrue(\n            res[\'rec\'][\'initial_data\'] is not None)\n        self.assertEqual(\n            res[\'rec\'][\'prepared_size\'],\n            3743)\n        self.assertTrue(\n            res[\'rec\'][\'prepared_data\'] is not None)\n    # end of test_prepare_pricing_data_success\n\n# end of TestPreparePricingDataset\n'"
tests/test_publish_from_s3_to_redis.py,0,"b'""""""\n\nTest file for - publish from s3 to redis\n========================================\n\nIntegration Tests\n-----------------\n\nPlease ensure ``redis`` and ``minio`` are running and export this:\n\n::\n\n    export INT_TESTS=1\n\n""""""\n\nimport json\nimport mock\nimport analysis_engine.mocks.mock_boto3_s3\nimport analysis_engine.mocks.mock_redis\nfrom analysis_engine.mocks.base_test import BaseTestCase\nfrom analysis_engine.consts import S3_ACCESS_KEY\nfrom analysis_engine.consts import S3_SECRET_KEY\nfrom analysis_engine.consts import S3_REGION_NAME\nfrom analysis_engine.consts import S3_ADDRESS\nfrom analysis_engine.consts import S3_SECURE\nfrom analysis_engine.consts import REDIS_ADDRESS\nfrom analysis_engine.consts import REDIS_KEY\nfrom analysis_engine.consts import REDIS_PASSWORD\nfrom analysis_engine.consts import REDIS_DB\nfrom analysis_engine.consts import REDIS_EXPIRE\nfrom analysis_engine.consts import TICKER\nfrom analysis_engine.consts import SUCCESS\nfrom analysis_engine.consts import ERR\nfrom analysis_engine.consts import ev\nfrom analysis_engine.api_requests \\\n    import build_cache_ready_pricing_dataset\nfrom analysis_engine.work_tasks.publish_from_s3_to_redis \\\n    import run_publish_from_s3_to_redis\nfrom analysis_engine.api_requests \\\n    import build_publish_from_s3_to_redis_request\nfrom spylunking.log.setup_logging import build_colorized_logger\n\n\nlog = build_colorized_logger(\n    name=__name__)\n\n\ndef mock_success_task_result(\n        **kwargs):\n    """"""mock_success_task_result\n\n    :param kwargs: keyword args dict\n    """"""\n    log.info(\'MOCK - mock_success_task_result\')\n    res = kwargs\n    res[\'result\'][\'status\'] = SUCCESS\n    res[\'result\'][\'err\'] = None\n    return res\n# end of mock_success_task_result\n\n\ndef mock_err_task_result(\n        **kwargs):\n    """"""mock_err_task_result\n\n    :param kwargs: keyword args dict\n    """"""\n    log.info(\'MOCK - mock_err_task_result\')\n    res = kwargs\n    res[\'result\'][\'status\'] = ERR\n    res[\'result\'][\'err\'] = \'test exception\'\n    return res\n# end of mock_err_task_result\n\n\ndef mock_s3_read_contents_from_key(\n        s3,\n        s3_bucket_name,\n        s3_key,\n        encoding=\'utf-8\',\n        convert_as_json=True):\n    """"""mock_s3_read_contents_from_key\n\n    Download the S3 key contents as a string. This\n    will raise exceptions.\n\n    :param s3_obj: existing S3 object\n    :param s3_bucket_name: bucket name\n    :param s3_key: S3 key\n    :param encoding: utf-8 by default\n    :param convert_to_json: auto-convert to a dict\n    """"""\n    log.info(\'MOCK - mock_s3_read_contents_from_key\')\n    data = build_cache_ready_pricing_dataset()\n    if not convert_as_json:\n        data = json.dumps(data)\n    return data\n# end of mock_s3_read_contents_from_key\n\n\nclass TestPublishFromS3ToRedis(BaseTestCase):\n    """"""TestPublishFromS3ToRedis""""""\n\n    @mock.patch(\n        (\'boto3.resource\'),\n        new=analysis_engine.mocks.mock_boto3_s3.build_boto3_resource)\n    @mock.patch(\n        (\'redis.Redis\'),\n        new=analysis_engine.mocks.mock_redis.MockRedis)\n    @mock.patch(\n        (\'analysis_engine.get_task_results.\'\n         \'get_task_results\'),\n        new=mock_success_task_result)\n    @mock.patch(\n        (\'analysis_engine.s3_read_contents_from_key.\'\n         \'s3_read_contents_from_key\'),\n        new=mock_s3_read_contents_from_key)\n    def test_success_publish_from_s3_to_redis(self):\n        """"""test_success_publish_from_s3_to_redis""""""\n        work = build_publish_from_s3_to_redis_request()\n        work[\'s3_enabled\'] = 1\n        work[\'redis_enabled\'] = 1\n        work[\'s3_access_key\'] = S3_ACCESS_KEY\n        work[\'s3_secret_key\'] = S3_SECRET_KEY\n        work[\'s3_region_name\'] = S3_REGION_NAME\n        work[\'s3_address\'] = S3_ADDRESS\n        work[\'s3_secure\'] = S3_SECURE\n        work[\'redis_address\'] = REDIS_ADDRESS\n        work[\'redis_db\'] = REDIS_DB\n        work[\'redis_key\'] = REDIS_KEY\n        work[\'redis_password\'] = REDIS_PASSWORD\n        work[\'redis_expire\'] = REDIS_EXPIRE\n        work[\'s3_bucket\'] = \'integration-tests\'\n        work[\'s3_key\'] = \'integration-test-v1\'\n        work[\'redis_key\'] = \'integration-test-v1\'\n\n        res = run_publish_from_s3_to_redis(\n            work)\n        self.assertTrue(\n            res[\'status\'] == SUCCESS)\n        self.assertTrue(\n            res[\'err\'] is None)\n        self.assertTrue(\n            res[\'rec\'] is not None)\n        record = res[\'rec\']\n        self.assertEqual(\n            record[\'ticker\'],\n            TICKER)\n        self.assertEqual(\n            record[\'s3_enabled\'],\n            True)\n        self.assertEqual(\n            record[\'redis_enabled\'],\n            True)\n        self.assertEqual(\n            record[\'s3_bucket\'],\n            work[\'s3_bucket\'])\n        self.assertEqual(\n            record[\'s3_key\'],\n            work[\'s3_key\'])\n        self.assertEqual(\n            record[\'redis_key\'],\n            work[\'redis_key\'])\n    # end of test_success_publish_from_s3_to_redis\n\n    def test_err_publish_from_s3_to_redis(self):\n        """"""test_err_publish_from_s3_to_redis""""""\n        work = build_publish_from_s3_to_redis_request()\n        work[\'ticker\'] = None\n        res = run_publish_from_s3_to_redis(\n            work)\n        self.assertTrue(\n            res[\'status\'] == ERR)\n        self.assertTrue(\n            res[\'err\'] == \'missing ticker\')\n    # end of test_err_publish_from_s3_to_redis\n\n    """"""\n    Integration Tests\n\n    Please ensure redis and minio are running and run this:\n\n    ::\n\n        export INT_TESTS=1\n\n    """"""\n\n    @mock.patch(\n        (\'analysis_engine.get_task_results.\'\n         \'get_task_results\'),\n        new=mock_success_task_result)\n    def test_integration_publish_from_s3_to_redis(self):\n        """"""test_integration_publish_from_s3_to_redis""""""\n        if ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n\n        work = build_publish_from_s3_to_redis_request()\n        work[\'s3_enabled\'] = 1\n        work[\'redis_enabled\'] = 1\n        work[\'s3_access_key\'] = S3_ACCESS_KEY\n        work[\'s3_secret_key\'] = S3_SECRET_KEY\n        work[\'s3_region_name\'] = S3_REGION_NAME\n        work[\'s3_address\'] = S3_ADDRESS\n        work[\'s3_secure\'] = S3_SECURE\n        work[\'redis_address\'] = REDIS_ADDRESS\n        work[\'redis_db\'] = REDIS_DB\n        work[\'redis_key\'] = REDIS_KEY\n        work[\'redis_password\'] = REDIS_PASSWORD\n        work[\'redis_expire\'] = REDIS_EXPIRE\n        work[\'s3_bucket\'] = \'integration-tests\'\n        work[\'s3_key\'] = \'integration-test-v1\'\n        work[\'redis_key\'] = \'integration-test-v1\'\n\n        res = run_publish_from_s3_to_redis(\n            work)\n        self.assertTrue(\n            res[\'status\'] == SUCCESS)\n        self.assertTrue(\n            res[\'err\'] is None)\n        self.assertTrue(\n            res[\'rec\'] is not None)\n        record = res[\'rec\']\n        self.assertEqual(\n            record[\'ticker\'],\n            TICKER)\n        self.assertEqual(\n            record[\'s3_enabled\'],\n            True)\n        self.assertEqual(\n            record[\'redis_enabled\'],\n            True)\n        self.assertEqual(\n            record[\'s3_bucket\'],\n            work[\'s3_bucket\'])\n        self.assertEqual(\n            record[\'s3_key\'],\n            work[\'s3_key\'])\n        self.assertEqual(\n            record[\'redis_key\'],\n            work[\'redis_key\'])\n    # end of test_integration_publish_from_s3_to_redis\n\n# end of TestPublishFromS3ToRedis\n'"
tests/test_publish_pricing_update.py,0,"b'""""""\n\nTest file for - update prices\n=============================\n\nIntegration Tests\n-----------------\n\nPlease ensure ``redis`` and ``minio`` are running and export this:\n\n::\n\n    export INT_TESTS=1\n\n""""""\n\nimport os\nimport mock\nimport analysis_engine.mocks.mock_pinance\nimport analysis_engine.mocks.mock_boto3_s3\nimport analysis_engine.mocks.mock_redis\nfrom analysis_engine.mocks.base_test import BaseTestCase\nfrom analysis_engine.consts import TICKER\nfrom analysis_engine.consts import SUCCESS\nfrom analysis_engine.consts import ERR\nfrom analysis_engine.consts import S3_ACCESS_KEY\nfrom analysis_engine.consts import S3_SECRET_KEY\nfrom analysis_engine.consts import S3_REGION_NAME\nfrom analysis_engine.consts import S3_ADDRESS\nfrom analysis_engine.consts import S3_SECURE\nfrom analysis_engine.consts import REDIS_ADDRESS\nfrom analysis_engine.consts import REDIS_KEY\nfrom analysis_engine.consts import REDIS_PASSWORD\nfrom analysis_engine.consts import REDIS_DB\nfrom analysis_engine.consts import REDIS_EXPIRE\nfrom analysis_engine.consts import ev\nfrom analysis_engine.work_tasks.publish_pricing_update \\\n    import run_publish_pricing_update\nfrom analysis_engine.api_requests \\\n    import build_publish_pricing_request\n\n\ndef mock_success_task_result(\n        **kwargs):\n    """"""mock_success_task_result\n\n    :param kwargs: keyword args dict\n    """"""\n    res = kwargs\n    res[\'result\'][\'status\'] = SUCCESS\n    res[\'result\'][\'err\'] = None\n    return res\n# end of mock_success_task_result\n\n\ndef mock_err_task_result(\n        **kwargs):\n    """"""mock_err_task_result\n\n    :param kwargs: keyword args dict\n    """"""\n    res = kwargs\n    res[\'result\'][\'status\'] = ERR\n    res[\'result\'][\'err\'] = \'test exception\'\n    return res\n# end of mock_err_task_result\n\n\nclass TestPublishPricingData(BaseTestCase):\n    """"""TestPublishPricingData""""""\n\n    @mock.patch(\n        \'pinance.Pinance\',\n        new=analysis_engine.mocks.mock_pinance.MockPinance)\n    @mock.patch(\n        (\'analysis_engine.get_pricing.\'\n         \'get_options\'),\n        new=analysis_engine.mocks.mock_pinance.mock_get_options)\n    @mock.patch(\n        (\'analysis_engine.get_task_results.\'\n         \'get_task_results\'),\n        new=mock_success_task_result)\n    def test_success_publish_pricing_data(self):\n        """"""test_success_publish_pricing_data""""""\n        work = build_publish_pricing_request()\n        res = run_publish_pricing_update(\n            work)\n        self.assertTrue(\n            res[\'status\'] == SUCCESS)\n        self.assertTrue(\n            res[\'err\'] is None)\n        self.assertTrue(\n            res[\'rec\'] is not None)\n        record = res[\'rec\']\n        self.assertEqual(\n            record[\'ticker\'],\n            TICKER)\n        self.assertEqual(\n            record[\'s3_enabled\'],\n            False)\n        self.assertEqual(\n            record[\'redis_enabled\'],\n            False)\n        self.assertEqual(\n            record[\'s3_bucket\'],\n            work[\'s3_bucket\'])\n        self.assertEqual(\n            record[\'s3_key\'],\n            work[\'s3_key\'])\n        self.assertEqual(\n            record[\'redis_key\'],\n            work[\'redis_key\'])\n    # end of test_success_publish_pricing_data\n\n    def test_err_publish_pricing_data(self):\n        """"""test_err_publish_pricing_data""""""\n        work = build_publish_pricing_request()\n        work[\'ticker\'] = None\n        res = run_publish_pricing_update(\n            work)\n        self.assertTrue(\n            res[\'status\'] == ERR)\n        self.assertTrue(\n            res[\'err\'] == \'missing ticker\')\n    # end of test_err_publish_pricing_data\n\n    @mock.patch(\n        \'pinance.Pinance\',\n        new=analysis_engine.mocks.mock_pinance.MockPinance)\n    @mock.patch(\n        (\'analysis_engine.get_pricing.\'\n         \'get_options\'),\n        new=analysis_engine.mocks.mock_pinance.mock_get_options)\n    @mock.patch(\n        (\'analysis_engine.get_task_results.\'\n         \'get_task_results\'),\n        new=mock_success_task_result)\n    @mock.patch(\n        (\'boto3.resource\'),\n        new=analysis_engine.mocks.mock_boto3_s3.build_boto3_resource)\n    def test_success_s3_upload(self):\n        """"""test_success_s3_upload""""""\n        work = build_publish_pricing_request()\n        work[\'s3_enabled\'] = 1\n        work[\'redis_enabled\'] = 0\n        work[\'s3_access_key\'] = S3_ACCESS_KEY\n        work[\'s3_secret_key\'] = S3_SECRET_KEY\n        work[\'s3_region_name\'] = S3_REGION_NAME\n        work[\'s3_address\'] = S3_ADDRESS\n        work[\'s3_secure\'] = S3_SECURE\n        res = run_publish_pricing_update(\n            work)\n        self.assertTrue(\n            res[\'status\'] == SUCCESS)\n    # end of test_success_s3_upload\n\n    @mock.patch(\n        \'pinance.Pinance\',\n        new=analysis_engine.mocks.mock_pinance.MockPinance)\n    @mock.patch(\n        (\'analysis_engine.get_pricing.\'\n         \'get_options\'),\n        new=analysis_engine.mocks.mock_pinance.mock_get_options)\n    @mock.patch(\n        (\'analysis_engine.get_task_results.\'\n         \'get_task_results\'),\n        new=mock_success_task_result)\n    @mock.patch(\n        (\'redis.Redis\'),\n        new=analysis_engine.mocks.mock_redis.MockRedis)\n    def test_success_redis_set(self):\n        """"""test_success_redis_set""""""\n        work = build_publish_pricing_request()\n        work[\'s3_enabled\'] = 0\n        work[\'redis_enabled\'] = 1\n        work[\'redis_address\'] = REDIS_ADDRESS\n        work[\'redis_db\'] = REDIS_DB\n        work[\'redis_key\'] = REDIS_KEY\n        work[\'redis_password\'] = REDIS_PASSWORD\n        work[\'redis_expire\'] = REDIS_EXPIRE\n        res = run_publish_pricing_update(\n            work)\n        self.assertTrue(\n            res[\'status\'] == SUCCESS)\n    # end of test_success_redis_set\n\n    """"""\n    Integration Tests\n\n    Please ensure redis and minio are running and run this:\n\n    ::\n\n        export INT_TESTS=1\n\n    """"""\n\n    @mock.patch(\n        \'pinance.Pinance\',\n        new=analysis_engine.mocks.mock_pinance.MockPinance)\n    @mock.patch(\n        (\'analysis_engine.get_pricing.\'\n         \'get_options\'),\n        new=analysis_engine.mocks.mock_pinance.mock_get_options)\n    @mock.patch(\n        (\'analysis_engine.get_task_results.\'\n         \'get_task_results\'),\n        new=mock_success_task_result)\n    def test_integration_s3_upload(self):\n        """"""test_integration_s3_upload""""""\n        if ev(\'INT_TESTS\', \'0\') == \'1\':\n            work = build_publish_pricing_request()\n            work[\'s3_enabled\'] = 1\n            work[\'redis_enabled\'] = 0\n            work[\'s3_access_key\'] = S3_ACCESS_KEY\n            work[\'s3_secret_key\'] = S3_SECRET_KEY\n            work[\'s3_region_name\'] = S3_REGION_NAME\n            work[\'s3_address\'] = S3_ADDRESS\n            work[\'s3_secure\'] = S3_SECURE\n            work[\'s3_bucket\'] = \'integration-tests\'\n            work[\'s3_key\'] = \'integration-test-v1\'\n            work[\'redis_key\'] = \'integration-test-v1\'\n            os.environ.pop(\'AWS_DEFAULT_PROFILE\', None)\n            res = run_publish_pricing_update(\n                work)\n            self.assertTrue(\n                res[\'status\'] == SUCCESS)\n    # end of test_integration_s3_upload\n\n    @mock.patch(\n        \'pinance.Pinance\',\n        new=analysis_engine.mocks.mock_pinance.MockPinance)\n    @mock.patch(\n        (\'analysis_engine.get_pricing.\'\n         \'get_options\'),\n        new=analysis_engine.mocks.mock_pinance.mock_get_options)\n    @mock.patch(\n        (\'analysis_engine.get_task_results.\'\n         \'get_task_results\'),\n        new=mock_success_task_result)\n    def test_integration_redis_set(self):\n        """"""test_integration_redis_set""""""\n        if ev(\'INT_TESTS\', \'0\') == \'1\':\n            work = build_publish_pricing_request()\n            work[\'s3_enabled\'] = 0\n            work[\'redis_enabled\'] = 1\n            work[\'redis_address\'] = REDIS_ADDRESS\n            work[\'redis_db\'] = REDIS_DB\n            work[\'redis_key\'] = REDIS_KEY\n            work[\'redis_password\'] = REDIS_PASSWORD\n            work[\'redis_expire\'] = REDIS_EXPIRE\n            work[\'redis_key\'] = \'integration-test-v1\'\n            work[\'s3_key\'] = \'integration-test-v1\'\n            res = run_publish_pricing_update(\n                work)\n            self.assertTrue(\n                res[\'status\'] == SUCCESS)\n    # end of test_integration_redis_set\n\n# end of TestPublishPricingData\n'"
tests/test_runner_for_algos.py,0,"b'""""""\nTest file for:\nalgo_runner.py\n""""""\n\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.utils as ae_utils\nimport analysis_engine.algo_runner as algo_runner\nimport analysis_engine.mocks.base_test as base_test\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\nclass TestRunnerForAlgos(base_test.BaseTestCase):\n    """"""TestRunnerForAlgos""""""\n\n    def setUp(self):\n        """"""setUp""""""\n        self.ticker = ae_consts.TICKER\n        self.algo_config = (\n            f\'./cfg/default_algo.json\')\n        self.algo_history_loc = (\n            f\'s3://ztestalgos/test_history_{self.ticker}\')\n    # end of setUp\n\n    """"""\n    Integration Tests\n\n    Please ensure redis and minio are running and run this:\n\n    ::\n\n        export INT_TESTS=1\n\n    """"""\n\n    def debug_df(\n            self,\n            df):\n        """"""debug_df\n\n        :param df: ``pandas.DataFrame`` from a fetch\n        """"""\n        print(\'-----------------------------------\')\n        print(f\'dataframe: {df}\')\n        print(\'\')\n        print(f\'dataframe columns:\\n{df.columns.values}\')\n        print(\'-----------------------------------\')\n    # end of debug_df\n\n    def test_latest(self):\n        """"""test_latest""""""\n        if ae_consts.ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n        ticker = \'SPY\'\n        start_date = ae_utils.get_last_close_str()\n        # build dataset cache dictionary\n        runner = algo_runner.AlgoRunner(\n            ticker=ticker,\n            start_date=start_date,\n            end_date=None,\n            history_loc=self.algo_history_loc,\n            algo_config=self.algo_config,\n            verbose_algo=True,\n            verbose_processor=False,\n            verbose_indicators=False)\n\n        req = {\n            \'ticker\': ticker,\n            \'date_str\': start_date,\n            \'start_row\': -200\n        }\n        df = runner.latest(\n            **req)\n        self.assertEqual(\n            len(df.index),\n            len(runner.get_history().index))\n    # end of test_latest\n\n# end of TestRunnerForAlgos\n'"
tests/test_send_to_slack.py,0,"b'""""""\nTest file for:\nupdate prices\n""""""\n\nimport mock\nimport os\nimport matplotlib.pyplot as plt\nfrom types import SimpleNamespace\nfrom analysis_engine.mocks.base_test import BaseTestCase\nfrom analysis_engine.consts import SUCCESS\nfrom analysis_engine.consts import FAILED\nfrom analysis_engine.send_to_slack import post_failure\nfrom analysis_engine.send_to_slack import post_message\nfrom analysis_engine.send_to_slack import post_success\nfrom analysis_engine.send_to_slack import post_plot\n\n\ndef mock_request_success_result(url, data=None, params=None, files=None):\n    """"""mock_request_success_result\n\n    :param kwargs: keyword args dict\n    """"""\n    res = {\'status_code\': 200}\n    return SimpleNamespace(**res)\n# end of mock_request_success_result\n\n\ndef mock_request_failure_result(url, data=None, params=None, files=None):\n    """"""mock_request_failure_result\n\n    :param kwargs: keyword args dict\n    """"""\n    res = {\'status_code\': 400}\n    return SimpleNamespace(**res)\n# end of mock_request_failure_result\n\n\nclass TestSendToSlack(BaseTestCase):\n    """"""TestSendToSlack""""""\n\n    backupWebhook = None\n    backupAccessToken = None\n    backupChannels = None\n\n    def setUp(self):\n        """"""setUp""""""\n        if os.getenv(\'SLACK_WEBHOOK\'):\n            self.backupWebhook = os.getenv(\'SLACK_WEBHOOK\')\n        os.environ[\'SLACK_WEBHOOK\'] = \'https://test.com\'\n        if os.getenv(\'SLACK_ACCESS_TOKEN\'):\n            self.backupWebhook = os.getenv(\'SLACK_ACCESS_TOKEN\')\n        os.environ[\'SLACK_ACCESS_TOKEN\'] = \'test_access_token\'\n        if os.getenv(\'SLACK_PUBLISH_PLOT_CHANNELS\'):\n            self.backupWebhook = os.getenv(\'SLACK_PUBLISH_PLOT_CHANNELS\')\n        os.environ[\'SLACK_PUBLISH_PLOT_CHANNELS\'] = \'general\'\n    # end of setUp\n\n    def tearDown(self):\n        """"""tearDown""""""\n        if self.backupWebhook:\n            os.environ[\'SLACK_WEBHOOK\'] = self.backupWebhook\n            self.backupWebhook = None\n        else:\n            os.environ.pop(\'SLACK_WEBHOOK\', None)\n\n        if self.backupAccessToken:\n            os.environ[\'SLACK_ACCESS_TOKEN\'] = self.backupAccessToken\n            self.backupAccessToken = None\n        else:\n            os.environ.pop(\'SLACK_ACCESS_TOKEN\', None)\n\n        if self.backupChannels:\n            os.environ[\'SLACK_PUBLISH_PLOT_CHANNELS\'] = self.backupChannels\n            self.backupChannels = None\n        else:\n            os.environ.pop(\'SLACK_PUBLISH_PLOT_CHANNELS\', None)\n    # end of tearDown\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_success_send_to_slack_string_success(self):\n        """"""test_post_success_send_to_slack_string_success""""""\n        res = post_success(\'test\')\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_success_send_to_slack_string_success\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_success_send_to_slack_string_failure(self):\n        """"""test_post_success_send_to_slack_string_failure""""""\n        res = post_success(\'test\')\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_success_send_to_slack_string_failure\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_failure_send_to_slack_string_success(self):\n        """"""test_post_failure_send_to_slack_string_success""""""\n        res = post_failure(\'test\')\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_failure_send_to_slack_string_success\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_failure_send_to_slack_string_failure(self):\n        """"""test_post_failure_send_to_slack_string_failure""""""\n        res = post_failure(\'test\')\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_failure_send_to_slack_string_failure\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_message_send_to_slack_string_success(self):\n        """"""test_post_message_send_to_slack_string_success""""""\n        res = post_message(\'test\')\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_message_send_to_slack_string_success\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_message_send_to_slack_string_failure(self):\n        """"""test_post_message_send_to_slack_string_failure""""""\n        res = post_message(\'test\')\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_message_send_to_slack_string_failure\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_success_send_to_slack_dict_success(self):\n        """"""test_post_success_send_to_slack_dict_success""""""\n        res = post_success({\'test\': \'value\'})\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_success_send_to_slack_dict_success\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_success_send_to_slack_dict_failure(self):\n        """"""test_post_success_send_to_slack_dict_failure""""""\n        res = post_success({\'test\': \'value\'})\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_success_send_to_slack_dict_failure\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_failure_send_to_slack_dict_success(self):\n        """"""test_post_failure_send_to_slack_dict_success""""""\n        res = post_failure({\'test\': \'value\'})\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_failure_send_to_slack_dict_success\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_failure_send_to_slack_dict_failure(self):\n        """"""test_post_failure_send_to_slack_dict_failure""""""\n        res = post_failure({\'test\': \'value\'})\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_failure_send_to_slack_dict_failure\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_message_send_to_slack_dict_success(self):\n        """"""test_post_message_send_to_slack_dict_success""""""\n        res = post_message({\'test\': \'value\'})\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_message_send_to_slack_dict_success\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_message_send_to_slack_dict_failure(self):\n        """"""test_post_message_send_to_slack_dict_failure""""""\n        res = post_message({\'test\': \'value\'})\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_message_send_to_slack_dict_failure\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_success_send_to_slack_list_success(self):\n        """"""test_post_success_send_to_slack_list_success""""""\n        res = post_success([\'test\', \'test 2\'])\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_success_send_to_slack_list_success\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_success_send_to_slack_list_failure(self):\n        """"""test_post_success_send_to_slack_list_failure""""""\n        res = post_success([\'test\', \'test 2\'])\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_success_send_to_slack_list_failure\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_failure_send_to_slack_list_success(self):\n        """"""test_post_failure_send_to_slack_list_success""""""\n        res = post_failure([\'test\', \'test 2\'])\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_failure_send_to_slack_list_success\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_failure_send_to_slack_list_failure(self):\n        """"""test_post_failure_send_to_slack_list_failure""""""\n        res = post_failure([\'test\', \'test 2\'])\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_failure_send_to_slack_list_failure\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_message_send_to_slack_list_success(self):\n        """"""test_post_message_send_to_slack_list_success""""""\n        res = post_message([\'test\', \'test 2\'])\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_message_send_to_slack_list_success\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_message_send_to_slack_list_failure(self):\n        """"""test_post_message_send_to_slack_list_failure""""""\n        res = post_message([\'test\', \'test 2\'])\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_message_send_to_slack_list_failure\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_success_send_to_slack_string_success_jupyter(self):\n        """"""test_post_success_send_to_slack_string_success_jupyter""""""\n        res = post_success(\'test\', jupyter=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_success_send_to_slack_string_success_jupyter\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_success_send_to_slack_string_failure_jupyter(self):\n        """"""test_post_success_send_to_slack_string_failure_jupyter""""""\n        res = post_success(\'test\', jupyter=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_success_send_to_slack_string_failure_jupyter\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_failure_send_to_slack_string_success_jupyter(self):\n        """"""test_post_failure_send_to_slack_string_success_jupyter""""""\n        res = post_failure(\'test\', jupyter=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_failure_send_to_slack_string_success_jupyter\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_failure_send_to_slack_string_failure_jupyter(self):\n        """"""test_post_failure_send_to_slack_string_failure_jupyter""""""\n        res = post_failure(\'test\', jupyter=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_failure_send_to_slack_string_failure_jupyter\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_message_send_to_slack_string_success_jupyter(self):\n        """"""test_post_message_send_to_slack_string_success_jupyter""""""\n        res = post_message(\'test\', jupyter=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_message_send_to_slack_string_success_jupyter\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_message_send_to_slack_string_failure_jupyter(self):\n        """"""test_post_message_send_to_slack_string_failure_jupyter""""""\n        res = post_message(\'test\', jupyter=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_message_send_to_slack_string_failure_jupyter\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_success_send_to_slack_dict_success_jupyter(self):\n        """"""test_post_success_send_to_slack_dict_success_jupyter""""""\n        res = post_success({\'test\': \'value\'}, jupyter=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_success_send_to_slack_dict_success_jupyter\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_success_send_to_slack_dict_failure_jupyter(self):\n        """"""test_post_success_send_to_slack_dict_failure_jupyter""""""\n        res = post_success({\'test\': \'value\'}, jupyter=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_success_send_to_slack_dict_failure_jupyter\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_failure_send_to_slack_dict_success_jupyter(self):\n        """"""test_post_failure_send_to_slack_dict_success_jupyter""""""\n        res = post_failure({\'test\': \'value\'}, jupyter=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_failure_send_to_slack_dict_success_jupyter\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_failure_send_to_slack_dict_failure_jupyter(self):\n        """"""test_post_failure_send_to_slack_dict_failure_jupyter""""""\n        res = post_failure({\'test\': \'value\'}, jupyter=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_failure_send_to_slack_dict_failure_jupyter\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_message_send_to_slack_dict_success_jupyter(self):\n        """"""test_post_message_send_to_slack_dict_success_jupyter""""""\n        res = post_message({\'test\': \'value\'}, jupyter=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_message_send_to_slack_dict_success_jupyter\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_message_send_to_slack_dict_failure_jupyter(self):\n        """"""test_post_message_send_to_slack_dict_failure_jupyter""""""\n        res = post_message({\'test\': \'value\'}, jupyter=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_message_send_to_slack_dict_failure_jupyter\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_success_send_to_slack_list_success_jupyter(self):\n        """"""test_post_success_send_to_slack_list_success_jupyter""""""\n        res = post_success([\'test\', \'test 2\'], jupyter=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_success_send_to_slack_list_success_jupyter\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_success_send_to_slack_list_failure_jupyter(self):\n        """"""test_post_success_send_to_slack_list_failure_jupyter""""""\n        res = post_success([\'test\', \'test 2\'], jupyter=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_success_send_to_slack_list_failure_jupyter\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_failure_send_to_slack_list_success_jupyter(self):\n        """"""test_post_failure_send_to_slack_list_success_jupyter""""""\n        res = post_failure([\'test\', \'test 2\'], jupyter=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_failure_send_to_slack_list_success_jupyter\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_failure_send_to_slack_list_failure_jupyter(self):\n        """"""test_post_failure_send_to_slack_list_failure_jupyter""""""\n        res = post_failure([\'test\', \'test 2\'], jupyter=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_failure_send_to_slack_list_failure_jupyter\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_message_send_to_slack_list_success_jupyter(self):\n        """"""test_post_message_send_to_slack_list_success_jupyter""""""\n        res = post_message([\'test\', \'test 2\'], jupyter=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_message_send_to_slack_list_success_jupyter\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_message_send_to_slack_list_failure_jupyter(self):\n        """"""test_post_message_send_to_slack_list_failure_jupyter""""""\n        res = post_message([\'test\', \'test 2\'], jupyter=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_message_send_to_slack_list_failure_jupyter\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_success_send_to_slack_string_success_block(self):\n        """"""test_post_success_send_to_slack_string_success_block""""""\n        res = post_success(\'test\', block=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_success_send_to_slack_string_success_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_success_send_to_slack_string_failure_block(self):\n        """"""test_post_success_send_to_slack_string_failure_block""""""\n        res = post_success(\'test\', block=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_success_send_to_slack_string_failure_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_failure_send_to_slack_string_success_block(self):\n        """"""test_post_failure_send_to_slack_string_success_block""""""\n        res = post_failure(\'test\', block=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_failure_send_to_slack_string_success_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_failure_send_to_slack_string_failure_block(self):\n        """"""test_post_failure_send_to_slack_string_failure_block""""""\n        res = post_failure(\'test\', block=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_failure_send_to_slack_string_failure_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_message_send_to_slack_string_success_block(self):\n        """"""test_post_message_send_to_slack_string_success_block""""""\n        res = post_message(\'test\', block=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_message_send_to_slack_string_success_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_message_send_to_slack_string_failure_block(self):\n        """"""test_post_message_send_to_slack_string_failure_block""""""\n        res = post_message(\'test\', block=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_message_send_to_slack_string_failure_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_success_send_to_slack_dict_success_block(self):\n        """"""test_post_success_send_to_slack_dict_success_block""""""\n        res = post_success({\'test\': \'value\'}, block=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_success_send_to_slack_dict_success_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_success_send_to_slack_dict_failure_block(self):\n        """"""test_post_success_send_to_slack_dict_failure_block""""""\n        res = post_success({\'test\': \'value\'}, block=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_success_send_to_slack_dict_failure_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_failure_send_to_slack_dict_success_block(self):\n        """"""test_post_failure_send_to_slack_dict_success_block""""""\n        res = post_failure({\'test\': \'value\'}, block=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_failure_send_to_slack_dict_success_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_failure_send_to_slack_dict_failure_block(self):\n        """"""test_post_failure_send_to_slack_dict_failure_block""""""\n        res = post_failure({\'test\': \'value\'}, block=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_failure_send_to_slack_dict_failure_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_message_send_to_slack_dict_success_block(self):\n        """"""test_post_message_send_to_slack_dict_success_block""""""\n        res = post_message({\'test\': \'value\'}, block=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_message_send_to_slack_dict_success_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_message_send_to_slack_dict_failure_block(self):\n        """"""test_post_message_send_to_slack_dict_failure_block""""""\n        res = post_message({\'test\': \'value\'}, block=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_message_send_to_slack_dict_failure_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_success_send_to_slack_list_success_block(self):\n        """"""test_post_success_send_to_slack_list_success_block""""""\n        res = post_success([\'test\', \'test 2\'], block=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_success_send_to_slack_list_success_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_success_send_to_slack_list_failure_block(self):\n        """"""test_post_success_send_to_slack_list_failure_block""""""\n        res = post_success([\'test\', \'test 2\'], block=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_success_send_to_slack_list_failure_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_failure_send_to_slack_list_success_block(self):\n        """"""test_post_failure_send_to_slack_list_success_block""""""\n        res = post_failure([\'test\', \'test 2\'], block=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_failure_send_to_slack_list_success_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_failure_send_to_slack_list_failure_block(self):\n        """"""test_post_failure_send_to_slack_list_failure_block""""""\n        res = post_failure([\'test\', \'test 2\'], block=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_failure_send_to_slack_list_failure_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_message_send_to_slack_list_success_block(self):\n        """"""test_post_message_send_to_slack_list_success_block""""""\n        res = post_message([\'test\', \'test 2\'], block=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_message_send_to_slack_list_success_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_message_send_to_slack_list_failure_block(self):\n        """"""test_post_message_send_to_slack_list_failure_block""""""\n        res = post_message([\'test\', \'test 2\'], block=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_message_send_to_slack_list_failure_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_success_send_to_slack_string_success_width(self):\n        """"""test_post_success_send_to_slack_string_success_width""""""\n        res = post_success(\'test\', full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_success_send_to_slack_string_success_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_success_send_to_slack_string_failure_width(self):\n        """"""test_post_success_send_to_slack_string_failure_width""""""\n        res = post_success(\'test\', full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_success_send_to_slack_string_failure_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_failure_send_to_slack_string_success_width(self):\n        """"""test_post_failure_send_to_slack_string_success_width""""""\n        res = post_failure(\'test\', full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_failure_send_to_slack_string_success_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_failure_send_to_slack_string_failure_width(self):\n        """"""test_post_failure_send_to_slack_string_failure_width""""""\n        res = post_failure(\'test\', full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_failure_send_to_slack_string_failure_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_message_send_to_slack_string_success_width(self):\n        """"""test_post_message_send_to_slack_string_success_width""""""\n        res = post_message(\'test\', full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_message_send_to_slack_string_success_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_message_send_to_slack_string_failure_width(self):\n        """"""test_post_message_send_to_slack_string_failure_width""""""\n        res = post_message(\'test\', full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_message_send_to_slack_string_failure_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_success_send_to_slack_dict_success_width(self):\n        """"""test_post_success_send_to_slack_dict_success_width""""""\n        res = post_success({\'test\': \'value\'}, full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_success_send_to_slack_dict_success_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_success_send_to_slack_dict_failure_width(self):\n        """"""test_post_success_send_to_slack_dict_failure_width""""""\n        res = post_success({\'test\': \'value\'}, full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_success_send_to_slack_dict_failure_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_failure_send_to_slack_dict_success_width(self):\n        """"""test_post_failure_send_to_slack_dict_success_width""""""\n        res = post_failure({\'test\': \'value\'}, full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_failure_send_to_slack_dict_success_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_failure_send_to_slack_dict_failure_width(self):\n        """"""test_post_failure_send_to_slack_dict_failure_width""""""\n        res = post_failure({\'test\': \'value\'}, full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_failure_send_to_slack_dict_failure_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_message_send_to_slack_dict_success_width(self):\n        """"""test_post_message_send_to_slack_dict_success_width""""""\n        res = post_message({\'test\': \'value\'}, full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_message_send_to_slack_dict_success_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_message_send_to_slack_dict_failure_width(self):\n        """"""test_post_message_send_to_slack_dict_failure_width""""""\n        res = post_message({\'test\': \'value\'}, full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_message_send_to_slack_dict_failure_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_success_send_to_slack_list_success_width(self):\n        """"""test_post_success_send_to_slack_list_success_width""""""\n        res = post_success([\'test\', \'test 2\'], full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_success_send_to_slack_list_success_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_success_send_to_slack_list_failure_width(self):\n        """"""test_post_success_send_to_slack_list_failure_width""""""\n        res = post_success([\'test\', \'test 2\'], full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_success_send_to_slack_list_failure_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_failure_send_to_slack_list_success_width(self):\n        """"""test_post_failure_send_to_slack_list_success_width""""""\n        res = post_failure([\'test\', \'test 2\'], full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_failure_send_to_slack_list_success_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_failure_send_to_slack_list_failure_width(self):\n        """"""test_post_failure_send_to_slack_list_failure_width""""""\n        res = post_failure([\'test\', \'test 2\'], full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_failure_send_to_slack_list_failure_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_message_send_to_slack_list_success_width(self):\n        """"""test_post_message_send_to_slack_list_success_width""""""\n        res = post_message([\'test\', \'test 2\'], full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_message_send_to_slack_list_success_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_message_send_to_slack_list_failure_width(self):\n        """"""test_post_message_send_to_slack_list_failure_width""""""\n        res = post_message([\'test\', \'test 2\'], full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_message_send_to_slack_list_failure_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_success_send_to_slack_string_success_jupyter_block(self):\n        """"""test_post_success_send_to_slack_string_success_jupyter_block""""""\n        res = post_success(\'test\', jupyter=True, block=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_success_send_to_slack_string_success_jupyter_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_success_send_to_slack_string_failure_jupyter_block(self):\n        """"""test_post_success_send_to_slack_string_failure_jupyter_block""""""\n        res = post_success(\'test\', jupyter=True, block=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_success_send_to_slack_string_failure_jupyter_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_failure_send_to_slack_string_success_jupyter_block(self):\n        """"""test_post_failure_send_to_slack_string_success_jupyter_block""""""\n        res = post_failure(\'test\', jupyter=True, block=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_failure_send_to_slack_string_success_jupyter_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_failure_send_to_slack_string_failure_jupyter_block(self):\n        """"""test_post_failure_send_to_slack_string_failure_jupyter_block""""""\n        res = post_failure(\'test\', jupyter=True, block=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_failure_send_to_slack_string_failure_jupyter_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_message_send_to_slack_string_success_jupyter_block(self):\n        """"""test_post_message_send_to_slack_string_success_jupyter_block""""""\n        res = post_message(\'test\', jupyter=True, block=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_message_send_to_slack_string_success_jupyter_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_message_send_to_slack_string_failure_jupyter_block(self):\n        """"""test_post_message_send_to_slack_string_failure_jupyter_block""""""\n        res = post_message(\'test\', jupyter=True, block=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_message_send_to_slack_string_failure_jupyter_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_success_send_to_slack_dict_success_jupyter_block(self):\n        """"""test_post_success_send_to_slack_dict_success_jupyter_block""""""\n        res = post_success({\'test\': \'value\'}, jupyter=True, block=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_success_send_to_slack_dict_success_jupyter_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_success_send_to_slack_dict_failure_jupyter_block(self):\n        """"""test_post_success_send_to_slack_dict_failure_jupyter_block""""""\n        res = post_success({\'test\': \'value\'}, jupyter=True, block=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_success_send_to_slack_dict_failure_jupyter_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_failure_send_to_slack_dict_success_jupyter_block(self):\n        """"""test_post_failure_send_to_slack_dict_success_jupyter_block""""""\n        res = post_failure({\'test\': \'value\'}, jupyter=True, block=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_failure_send_to_slack_dict_success_jupyter_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_failure_send_to_slack_dict_failure_jupyter_block(self):\n        """"""test_post_failure_send_to_slack_dict_failure_jupyter_block""""""\n        res = post_failure({\'test\': \'value\'}, jupyter=True, block=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_failure_send_to_slack_dict_failure_jupyter_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_message_send_to_slack_dict_success_jupyter_block(self):\n        """"""test_post_message_send_to_slack_dict_success_jupyter_block""""""\n        res = post_message({\'test\': \'value\'}, jupyter=True, block=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_message_send_to_slack_dict_success_jupyter_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_message_send_to_slack_dict_failure_jupyter_block(self):\n        """"""test_post_message_send_to_slack_dict_failure_jupyter_block""""""\n        res = post_message({\'test\': \'value\'}, jupyter=True, block=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_message_send_to_slack_dict_failure_jupyter_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_success_send_to_slack_list_success_jupyter_block(self):\n        """"""test_post_success_send_to_slack_list_success_jupyter_block""""""\n        res = post_success([\'test\', \'test 2\'], jupyter=True, block=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_success_send_to_slack_list_success_jupyter_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_success_send_to_slack_list_failure_jupyter_block(self):\n        """"""test_post_success_send_to_slack_list_failure_jupyter_block""""""\n        res = post_success([\'test\', \'test 2\'], jupyter=True, block=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_success_send_to_slack_list_failure_jupyter_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_failure_send_to_slack_list_success_jupyter_block(self):\n        """"""test_post_failure_send_to_slack_list_success_jupyter_block""""""\n        res = post_failure([\'test\', \'test 2\'], jupyter=True, block=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_failure_send_to_slack_list_success_jupyter_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_failure_send_to_slack_list_failure_jupyter_block(self):\n        """"""test_post_failure_send_to_slack_list_failure_jupyter_block""""""\n        res = post_failure([\'test\', \'test 2\'], jupyter=True, block=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_failure_send_to_slack_list_failure_jupyter_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_message_send_to_slack_list_success_jupyter_block(self):\n        """"""test_post_message_send_to_slack_list_success_jupyter_block""""""\n        res = post_message([\'test\', \'test 2\'], jupyter=True, block=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_message_send_to_slack_list_success_jupyter_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_message_send_to_slack_list_failure_jupyter_block(self):\n        """"""test_post_message_send_to_slack_list_failure_jupyter_block""""""\n        res = post_message([\'test\', \'test 2\'], jupyter=True, block=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_message_send_to_slack_list_failure_jupyter_block\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_success_send_to_slack_string_success_jupyter_width(self):\n        """"""test_post_success_send_to_slack_string_success_jupyter_width""""""\n        res = post_success(\'test\', jupyter=True, full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_success_send_to_slack_string_success_jupyter_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_success_send_to_slack_string_failure_jupyter_width(self):\n        """"""test_post_success_send_to_slack_string_failure_jupyter_width""""""\n        res = post_success(\'test\', jupyter=True, full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_success_send_to_slack_string_failure_jupyter_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_failure_send_to_slack_string_success_jupyter_width(self):\n        """"""test_post_failure_send_to_slack_string_success_jupyter_width""""""\n        res = post_failure(\'test\', jupyter=True, full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_failure_send_to_slack_string_success_jupyter_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_failure_send_to_slack_string_failure_jupyter_width(self):\n        """"""test_post_failure_send_to_slack_string_failure_jupyter_width""""""\n        res = post_failure(\'test\', jupyter=True, full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_failure_send_to_slack_string_failure_jupyter_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_message_send_to_slack_string_success_jupyter_width(self):\n        """"""test_post_message_send_to_slack_string_success_jupyter_width""""""\n        res = post_message(\'test\', jupyter=True, full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_message_send_to_slack_string_success_jupyter_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_message_send_to_slack_string_failure_jupyter_width(self):\n        """"""test_post_message_send_to_slack_string_failure_jupyter_width""""""\n        res = post_message(\'test\', jupyter=True, full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_message_send_to_slack_string_failure_jupyter_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_success_send_to_slack_dict_success_jupyter_width(self):\n        """"""test_post_success_send_to_slack_dict_success_jupyter_width""""""\n        res = post_success({\'test\': \'value\'}, jupyter=True, full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_success_send_to_slack_dict_success_jupyter_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_success_send_to_slack_dict_failure_jupyter_width(self):\n        """"""test_post_success_send_to_slack_dict_failure_jupyter_width""""""\n        res = post_success({\'test\': \'value\'}, jupyter=True, full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_success_send_to_slack_dict_failure_jupyter_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_failure_send_to_slack_dict_success_jupyter_width(self):\n        """"""test_post_failure_send_to_slack_dict_success_jupyter_width""""""\n        res = post_failure({\'test\': \'value\'}, jupyter=True, full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_failure_send_to_slack_dict_success_jupyter_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_failure_send_to_slack_dict_failure_jupyter_width(self):\n        """"""test_post_failure_send_to_slack_dict_failure_jupyter_width""""""\n        res = post_failure({\'test\': \'value\'}, jupyter=True, full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_failure_send_to_slack_dict_failure_jupyter_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_message_send_to_slack_dict_success_jupyter_width(self):\n        """"""test_post_message_send_to_slack_dict_success_jupyter_width""""""\n        res = post_message({\'test\': \'value\'}, jupyter=True, full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_message_send_to_slack_dict_success_jupyter_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_message_send_to_slack_dict_failure_jupyter_width(self):\n        """"""test_post_message_send_to_slack_dict_failure_jupyter_width""""""\n        res = post_message({\'test\': \'value\'}, jupyter=True, full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_message_send_to_slack_dict_failure_jupyter_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_success_send_to_slack_list_success_jupyter_width(self):\n        """"""test_post_success_send_to_slack_list_success_jupyter_width""""""\n        res = post_success([\'test\', \'test 2\'], jupyter=True, full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_success_send_to_slack_list_success_jupyter_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_success_send_to_slack_list_failure_jupyter_width(self):\n        """"""test_post_success_send_to_slack_list_failure_jupyter_width""""""\n        res = post_success([\'test\', \'test 2\'], jupyter=True, full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_success_send_to_slack_list_failure_jupyter_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_failure_send_to_slack_list_success_jupyter_width(self):\n        """"""test_post_failure_send_to_slack_list_success_jupyter_width""""""\n        res = post_failure([\'test\', \'test 2\'], jupyter=True, full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_failure_send_to_slack_list_success_jupyter_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_failure_send_to_slack_list_failure_jupyter_width(self):\n        """"""test_post_failure_send_to_slack_list_failure_jupyter_width""""""\n        res = post_failure([\'test\', \'test 2\'], jupyter=True, full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_failure_send_to_slack_list_failure_jupyter_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_message_send_to_slack_list_success_jupyter_width(self):\n        """"""test_post_message_send_to_slack_list_success_jupyter_width""""""\n        res = post_message([\'test\', \'test 2\'], jupyter=True, full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_message_send_to_slack_list_success_jupyter_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_message_send_to_slack_list_failure_jupyter_width(self):\n        """"""test_post_message_send_to_slack_list_failure_jupyter_width""""""\n        res = post_message([\'test\', \'test 2\'], jupyter=True, full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_message_send_to_slack_list_failure_jupyter_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_success_send_to_slack_string_success_block_width(self):\n        """"""test_post_success_send_to_slack_string_success_block_width""""""\n        res = post_success(\'test\', block=True, full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_success_send_to_slack_string_success_block_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_success_send_to_slack_string_failure_block_width(self):\n        """"""test_post_success_send_to_slack_string_failure_block_width""""""\n        res = post_success(\'test\', block=True, full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_success_send_to_slack_string_failure_block_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_failure_send_to_slack_string_success_block_width(self):\n        """"""test_post_failure_send_to_slack_string_success_block_width""""""\n        res = post_failure(\'test\', block=True, full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_failure_send_to_slack_string_success_block_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_failure_send_to_slack_string_failure_block_width(self):\n        """"""test_post_failure_send_to_slack_string_failure_block_width""""""\n        res = post_failure(\'test\', block=True, full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_failure_send_to_slack_string_failure_block_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_message_send_to_slack_string_success_block_width(self):\n        """"""test_post_message_send_to_slack_string_success_block_width""""""\n        res = post_message(\'test\', block=True, full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_message_send_to_slack_string_success_block_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_message_send_to_slack_string_failure_block_width(self):\n        """"""test_post_message_send_to_slack_string_failure_block_width""""""\n        res = post_message(\'test\', block=True, full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_message_send_to_slack_string_failure_block_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_success_send_to_slack_dict_success_block_width(self):\n        """"""test_post_success_send_to_slack_dict_success_block_width""""""\n        res = post_success({\'test\': \'value\'}, block=True, full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_success_send_to_slack_dict_success_block_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_success_send_to_slack_dict_failure_block_width(self):\n        """"""test_post_success_send_to_slack_dict_failure_block_width""""""\n        res = post_success({\'test\': \'value\'}, block=True, full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_success_send_to_slack_dict_failure_block_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_failure_send_to_slack_dict_success_block_width(self):\n        """"""test_post_failure_send_to_slack_dict_success_block_width""""""\n        res = post_failure({\'test\': \'value\'}, block=True, full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_failure_send_to_slack_dict_success_block_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_failure_send_to_slack_dict_failure_block_width(self):\n        """"""test_post_failure_send_to_slack_dict_failure_block_width""""""\n        res = post_failure({\'test\': \'value\'}, block=True, full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_failure_send_to_slack_dict_failure_block_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_message_send_to_slack_dict_success_block_width(self):\n        """"""test_post_message_send_to_slack_dict_success_block_width""""""\n        res = post_message({\'test\': \'value\'}, block=True, full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_message_send_to_slack_dict_success_block_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_message_send_to_slack_dict_failure_block_width(self):\n        """"""test_post_message_send_to_slack_dict_failure_block_width""""""\n        res = post_message({\'test\': \'value\'}, block=True, full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_message_send_to_slack_dict_failure_block_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_success_send_to_slack_list_success_block_width(self):\n        """"""test_post_success_send_to_slack_list_success_block_width""""""\n        res = post_success([\'test\', \'test 2\'], block=True, full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_success_send_to_slack_list_success_block_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_success_send_to_slack_list_failure_block_width(self):\n        """"""test_post_success_send_to_slack_list_failure_block_width""""""\n        res = post_success([\'test\', \'test 2\'], block=True, full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_success_send_to_slack_list_failure_block_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_failure_send_to_slack_list_success_block_width(self):\n        """"""test_post_failure_send_to_slack_list_success_block_width""""""\n        res = post_failure([\'test\', \'test 2\'], block=True, full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_failure_send_to_slack_list_success_block_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_failure_send_to_slack_list_failure_block_width(self):\n        """"""test_post_failure_send_to_slack_list_failure_block_width""""""\n        res = post_failure([\'test\', \'test 2\'], block=True, full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_failure_send_to_slack_list_failure_block_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_message_send_to_slack_list_success_block_width(self):\n        """"""test_post_message_send_to_slack_list_success_block_width""""""\n        res = post_message([\'test\', \'test 2\'], block=True, full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_message_send_to_slack_list_success_block_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_message_send_to_slack_list_failure_block_width(self):\n        """"""test_post_message_send_to_slack_list_failure_block_width""""""\n        res = post_message([\'test\', \'test 2\'], block=True, full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_message_send_to_slack_list_failure_block_width\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_success_send_to_slack_string_success_all(self):\n        """"""test_post_success_send_to_slack_string_success_all""""""\n        res = post_success(\'test\',\n                           jupyter=True,\n                           block=True,\n                           full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_success_send_to_slack_string_success_all\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_success_send_to_slack_string_failure_all(self):\n        """"""test_post_success_send_to_slack_string_failure_all""""""\n        res = post_success(\'test\',\n                           jupyter=True,\n                           block=True,\n                           full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_success_send_to_slack_string_failure_all\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_failure_send_to_slack_string_success_all(self):\n        """"""test_post_failure_send_to_slack_string_success_all""""""\n        res = post_failure(\'test\',\n                           jupyter=True,\n                           block=True,\n                           full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_failure_send_to_slack_string_success_all\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_failure_send_to_slack_string_failure_all(self):\n        """"""test_post_failure_send_to_slack_string_failure_all""""""\n        res = post_failure(\'test\',\n                           jupyter=True,\n                           block=True,\n                           full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_failure_send_to_slack_string_failure_all\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_message_send_to_slack_string_success_all(self):\n        """"""test_post_message_send_to_slack_string_success_all""""""\n        res = post_message(\'test\',\n                           jupyter=True,\n                           block=True,\n                           full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_message_send_to_slack_string_success_all\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_message_send_to_slack_string_failure_all(self):\n        """"""test_post_message_send_to_slack_string_failure_all""""""\n        res = post_message(\'test\',\n                           jupyter=True,\n                           block=True,\n                           full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_message_send_to_slack_string_failure_all\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_success_send_to_slack_dict_success_all(self):\n        """"""test_post_success_send_to_slack_dict_success_all""""""\n        res = post_success({\'test\': \'value\'},\n                           jupyter=True,\n                           block=True,\n                           full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_success_send_to_slack_dict_success_all\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_success_send_to_slack_dict_failure_all(self):\n        """"""test_post_success_send_to_slack_dict_failure_all""""""\n        res = post_success({\'test\': \'value\'},\n                           jupyter=True,\n                           block=True,\n                           full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_success_send_to_slack_dict_failure_all\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_failure_send_to_slack_dict_success_all(self):\n        """"""test_post_failure_send_to_slack_dict_success_all""""""\n        res = post_failure({\'test\': \'value\'},\n                           jupyter=True,\n                           block=True,\n                           full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_failure_send_to_slack_dict_success_all\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_failure_send_to_slack_dict_failure_all(self):\n        """"""test_post_failure_send_to_slack_dict_failure_all""""""\n        res = post_failure({\'test\': \'value\'},\n                           jupyter=True,\n                           block=True,\n                           full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_failure_send_to_slack_dict_failure_all\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_message_send_to_slack_dict_success_all(self):\n        """"""test_post_message_send_to_slack_dict_success_all""""""\n        res = post_message({\'test\': \'value\'},\n                           jupyter=True,\n                           block=True,\n                           full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_message_send_to_slack_dict_success_all\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_message_send_to_slack_dict_failure_all(self):\n        """"""test_post_message_send_to_slack_dict_failure_all""""""\n        res = post_message({\'test\': \'value\'},\n                           jupyter=True,\n                           block=True,\n                           full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_message_send_to_slack_dict_failure_all\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_success_send_to_slack_list_success_all(self):\n        """"""test_post_success_send_to_slack_list_success_all""""""\n        res = post_success([\'test\', \'test 2\'],\n                           jupyter=True,\n                           block=True,\n                           full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_success_send_to_slack_list_success_all\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_success_send_to_slack_list_failure_all(self):\n        """"""test_post_success_send_to_slack_list_failure_all""""""\n        res = post_success([\'test\', \'test 2\'],\n                           jupyter=True,\n                           block=True,\n                           full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_success_send_to_slack_list_failure_all\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_failure_send_to_slack_list_success_all(self):\n        """"""test_post_failure_send_to_slack_list_success_all""""""\n        res = post_failure([\'test\', \'test 2\'],\n                           jupyter=True,\n                           block=True,\n                           full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_failure_send_to_slack_list_success_all\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_failure_send_to_slack_list_failure_all(self):\n        """"""test_post_failure_send_to_slack_list_failure_all""""""\n        res = post_failure([\'test\', \'test 2\'],\n                           jupyter=True,\n                           block=True,\n                           full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_failure_send_to_slack_list_failure_all\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_message_send_to_slack_list_success_all(self):\n        """"""test_post_message_send_to_slack_list_success_all""""""\n        res = post_message([\'test\', \'test 2\'],\n                           jupyter=True,\n                           block=True,\n                           full_width=True)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_message_send_to_slack_list_success_all\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_message_send_to_slack_list_failure_all(self):\n        """"""test_post_message_send_to_slack_list_failure_all""""""\n        res = post_message([\'test\', \'test 2\'],\n                           jupyter=True,\n                           block=True,\n                           full_width=True)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_message_send_to_slack_list_failure_all\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_success_result)\n    def test_post_plot_send_to_slack_success(self):\n        """"""test_post_plot_send_to_slack_success""""""\n        res = post_plot(plt)\n        self.assertTrue(res[\'status\'] == SUCCESS)\n    # end of test_post_plot_send_to_slack_success\n\n    @mock.patch(\n        \'requests.post\',\n        new=mock_request_failure_result)\n    def test_post_plot_send_to_slack_failure(self):\n        """"""test_post_plot_send_to_slack_failure""""""\n        res = post_plot(plt)\n        self.assertTrue(res[\'status\'] == FAILED)\n    # end of test_post_plot_send_to_slack_failure\n\n# end of TestSendToSlack\n'"
tests/test_td_api.py,0,"b'""""""\nTest file for:\nTradier Extract Data\n""""""\n\nimport json\nimport requests\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.options_dates as opt_dates\nimport analysis_engine.url_helper as url_helper\nimport analysis_engine.td.consts as td_consts\nimport analysis_engine.api_requests as api_requests\nimport analysis_engine.mocks.base_test as base_test\nimport analysis_engine.td.fetch_data as td_fetch\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\nclass TestTDAPI(base_test.BaseTestCase):\n    """"""TestTDAPI""""""\n\n    def setUp(self):\n        """"""setUp""""""\n        self.ticker = ae_consts.TICKER\n    # end of setUp\n\n    """"""\n    Integration Tests\n\n    Please ensure redis and minio are running and run this:\n\n    ::\n\n        export INT_TESTS=1\n\n    """"""\n\n    def debug_df(\n            self,\n            df):\n        """"""debug_df\n\n        :param df: ``pandas.DataFrame`` from a fetch\n        """"""\n        print(\'-----------------------------------\')\n        print(f\'dataframe: {df}\')\n        print(\'\')\n        print(f\'dataframe columns:\\n{df.columns.values}\')\n        print(\'-----------------------------------\')\n    # end of debug_df\n\n    def test_integration_account_credentials(self):\n        """"""test_integration_account_credentials""""""\n        if ae_consts.ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n        headers = td_consts.get_auth_headers()\n        session = requests.Session()\n        session.headers = headers\n        self.exp_date = opt_dates.option_expiration().strftime(\n            ae_consts.COMMON_DATE_FORMAT)\n        use_url = td_consts.TD_URLS[\'options\'].format(\n            self.ticker,\n            self.exp_date)\n        response = url_helper.url_helper(sess=session).get(\n                use_url\n            )\n        self.assertEqual(\n            response.status_code,\n            200)\n        self.assertTrue(\n            len(json.loads(response.text)) > 0)\n    # end of test_integration_account_credentials\n\n    def test_integration_fetch_calls_dataset(self):\n        """"""test_integration_fetch_calls_dataset""""""\n        if ae_consts.ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n        ticker = \'SPY\'\n        label = \'TD calls dataset\'\n        # build dataset cache dictionary\n        work = api_requests.get_ds_dict(\n            ticker=ticker,\n            label=label)\n\n        status, df = td_fetch.fetch_data(\n            work_dict=work,\n            fetch_type=\'tdcalls\')\n        if status == ae_consts.SUCCESS:\n            self.assertIsNotNone(\n                df)\n            self.debug_df(df=df)\n        else:\n            log.critical(\n                f\'{label} is missing in redis \'\n                f\'for ticker={work[""ticker""]} \'\n                f\'status={ae_consts.get_status(status=status)}\')\n    # end of test_integration_fetch_calls_dataset\n\n    def test_integration_fetch_puts_dataset(self):\n        """"""test_integration_fetch_puts_dataset""""""\n        if ae_consts.ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n        ticker = \'SPY\'\n        label = \'TD puts dataset\'\n        # build dataset cache dictionary\n        work = api_requests.get_ds_dict(\n            ticker=ticker,\n            label=label)\n\n        status, df = td_fetch.fetch_data(\n            work_dict=work,\n            fetch_type=\'tdputs\')\n        if status == ae_consts.SUCCESS:\n            self.assertIsNotNone(\n                df)\n            self.debug_df(df=df)\n        else:\n            log.critical(\n                f\'{label} is missing in redis \'\n                f\'for ticker={work[""ticker""]} \'\n                f\'status={ae_consts.get_status(status=status)}\')\n    # end of test_integration_fetch_puts_dataset\n\n# end of TestTDAPI\n'"
tests/test_yahoo_dataset_extraction.py,0,"b'""""""\nTest file for:\nYahoo Extract Data\n""""""\n\nimport mock\nimport json\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.mocks.base_test as base_test\nimport analysis_engine.build_result as build_result\nimport analysis_engine.api_requests as api_requests\nimport analysis_engine.yahoo.extract_df_from_redis as yahoo_extract\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef mock_extract_pricing_from_redis_success(\n        label,\n        host,\n        port,\n        db,\n        password,\n        key,\n        **kwargs):\n    """"""mock_extract_pricing_from_redis_success\n\n    :param label: test label\n    :param address: test address\n    :param db: test db\n    :param key: test key\n    :param kwargs: additional keyword args as a dictionary\n    """"""\n    sample_record = api_requests.build_cache_ready_pricing_dataset(\n        label=(\n            f\'{label}.{host}.{port}.{db}.{key}.\'\n            \'mock_extract_pricing_from_redis_success\'))\n    rec = {\n        \'data\': sample_record[\'pricing\']\n    }\n    res = build_result.build_result(\n        status=ae_consts.SUCCESS,\n        err=None,\n        rec=rec)\n    return res\n# end of mock_extract_pricing_from_redis_success\n\n\ndef mock_extract_news_from_redis_success(\n        label,\n        host,\n        port,\n        db,\n        password,\n        key,\n        **kwargs):\n    """"""mock_extract_news_from_redis_success\n\n    :param label: test label\n    :param address: test address\n    :param db: test db\n    :param key: test key\n    :param kwargs: additional keyword args as a dictionary\n    """"""\n    sample_record = api_requests.build_cache_ready_pricing_dataset(\n        label=(\n            f\'{label}.{host}.{port}.{db}.{key}.\'\n            \'mock_extract_news_from_redis_success\'))\n    rec = {\n        \'data\': sample_record[\'news\']\n    }\n    res = build_result.build_result(\n        status=ae_consts.SUCCESS,\n        err=None,\n        rec=rec)\n    return res\n# end of mock_extract_news_from_redis_success\n\n\ndef mock_extract_options_from_redis_success(\n        label,\n        host,\n        port,\n        db,\n        password,\n        key,\n        **kwargs):\n    """"""mock_extract_options_from_redis_success\n\n    :param label: test label\n    :param address: test address\n    :param db: test db\n    :param key: test key\n    :param kwargs: additional keyword args as a dictionary\n    """"""\n    sample_record = api_requests.build_cache_ready_pricing_dataset(\n        label=(\n            f\'{label}.{host}.{port}.{db}.{key}.\'\n            \'mock_extract_options_from_redis_success\'))\n    options_dict = sample_record[\'options\']\n    rec = {\n        \'data\': options_dict\n    }\n    res = build_result.build_result(\n        status=ae_consts.SUCCESS,\n        err=None,\n        rec=rec)\n    return res\n# end of mock_extract_options_from_redis_success\n\n\nclass TestYahooDatasetExtraction(base_test.BaseTestCase):\n    """"""TestYahooDatasetExtraction""""""\n\n    def setUp(self):\n        """"""setUp""""""\n        self.ticker = ae_consts.TICKER\n    # end of setUp\n\n    @mock.patch(\n        (\n            \'analysis_engine.get_data_from_redis_key.\'\n            \'get_data_from_redis_key\'),\n        new=mock_extract_pricing_from_redis_success)\n    def test_extract_pricing_success(self):\n        """"""test_extract_pricing_success""""""\n        test_name = \'test_extract_pricing_dataset_success\'\n        work = api_requests.get_ds_dict(\n            ticker=self.ticker,\n            label=test_name)\n\n        status, df = yahoo_extract.extract_pricing_dataset(\n            work_dict=work)\n        self.assertIsNotNone(\n            df)\n        self.assertEqual(\n            ae_consts.get_status(status=status),\n            \'SUCCESS\')\n        self.assertTrue(\n            len(df.index) == 1)\n        self.assertEqual(\n            df[\'regularMarketPrice\'][0],\n            288.09)\n    # end of test_extract_pricing_success\n\n    @mock.patch(\n        (\n            \'analysis_engine.get_data_from_redis_key.\'\n            \'get_data_from_redis_key\'),\n        new=mock_extract_news_from_redis_success)\n    def test_extract_news_success(self):\n        """"""test_extract_news_success""""""\n        test_name = \'test_extract_news_success\'\n        work = api_requests.get_ds_dict(\n            ticker=self.ticker,\n            label=test_name)\n\n        status, df = yahoo_extract.extract_yahoo_news_dataset(\n            work_dict=work)\n        self.assertIsNotNone(\n            df)\n        self.assertEqual(\n            ae_consts.get_status(status=status),\n            \'SUCCESS\')\n        self.assertTrue(\n            len(df.index) == 2)\n        self.assertEqual(\n            df[\'u\'][1],\n            \'http://finance.yahoo.com/news/url2\')\n        self.assertEqual(\n            df[\'tt\'][1],\n            \'1493311950\')\n    # end of test_extract_news_success\n\n    @mock.patch(\n        (\n            \'analysis_engine.get_data_from_redis_key.\'\n            \'get_data_from_redis_key\'),\n        new=mock_extract_options_from_redis_success)\n    def test_extract_option_calls_success(self):\n        """"""test_extract_option_calls_success""""""\n        test_name = \'test_extract_option_calls_success\'\n        work = api_requests.get_ds_dict(\n            ticker=self.ticker,\n            label=test_name)\n\n        status, df = yahoo_extract.extract_option_calls_dataset(\n            work_dict=work)\n        self.assertIsNotNone(\n            df)\n        self.assertEqual(\n            ae_consts.get_status(status=status),\n            \'SUCCESS\')\n        self.assertTrue(\n            len(df.index) == 1)\n        self.assertEqual(\n            df[\'strike\'][0],\n            380)\n    # end of test_extract_option_calls_success\n\n    @mock.patch(\n        (\n            \'analysis_engine.get_data_from_redis_key.\'\n            \'get_data_from_redis_key\'),\n        new=mock_extract_options_from_redis_success)\n    def test_extract_option_puts_success(self):\n        """"""test_extract_option_puts_success""""""\n        test_name = \'test_extract_option_puts_success\'\n        work = api_requests.get_ds_dict(\n            ticker=self.ticker,\n            label=test_name)\n\n        status, df = yahoo_extract.extract_option_puts_dataset(\n            work_dict=work)\n        self.assertIsNotNone(\n            df)\n        self.assertEqual(\n            ae_consts.get_status(status=status),\n            \'SUCCESS\')\n        self.assertTrue(\n            len(df.index) == 1)\n        self.assertEqual(\n            df[\'strike\'][0],\n            380)\n    # end of test_extract_option_puts_success\n\n    """"""\n    Integration Tests\n\n    Please ensure redis and minio are running and run this:\n\n    ::\n\n        export INT_TESTS=1\n\n    """"""\n\n    def debug_df(\n            self,\n            df):\n        """"""debug_df\n\n        :param df: ``pandas.DataFrame`` from a fetch\n        """"""\n        print(\'-----------------------------------\')\n        print(f\'dataframe: {df}\')\n        print(\'\')\n        print(f\'dataframe columns:\\n{df.columns.values}\')\n        print(\'-----------------------------------\')\n    # end of debug_df\n\n    def test_integration_extract_pricing(self):\n        """"""test_integration_extract_pricing""""""\n        if ae_consts.ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n\n        # build dataset cache dictionary\n        work = api_requests.get_ds_dict(\n            ticker=\'SPY\',\n            label=\'test_integration_extract_pricing\')\n\n        status, df = yahoo_extract.extract_pricing_dataset(\n            work_dict=work)\n        if status == ae_consts.SUCCESS:\n            self.assertIsNotNone(\n                df)\n            self.debug_df(df=df)\n        else:\n            log.critical(\n                \'Yahoo Pricing are missing in redis \'\n                f\'for ticker={work[""ticker""]} \'\n                f\'status={ae_consts.get_status(status=status)}\')\n    # end of test_integration_extract_pricing\n\n    def test_integration_extract_yahoo_news(self):\n        """"""test_integration_extract_yahoo_news""""""\n        if ae_consts.ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n\n        # build dataset cache dictionary\n        work = api_requests.get_ds_dict(\n            ticker=\'SPY\',\n            label=\'test_integration_extract_news\')\n\n        status, df = yahoo_extract.extract_yahoo_news_dataset(\n            work_dict=work)\n        if status == ae_consts.SUCCESS:\n            self.assertIsNotNone(\n                df)\n            self.debug_df(df=df)\n        else:\n            log.critical(\n                \'Yahoo News is missing in redis \'\n                f\'for ticker={work[""ticker""]} \'\n                f\'status={ae_consts.get_status(status=status)}\')\n    # end of test_integration_extract_yahoo_news\n\n    def test_integration_extract_option_calls(self):\n        """"""test_integration_extract_option_calls""""""\n        if ae_consts.ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n\n        # build dataset cache dictionary\n        work = api_requests.get_ds_dict(\n            ticker=\'SPY\',\n            base_key=\'SPY_2018-12-31\',\n            label=\'test_integration_extract_option_calls\')\n\n        status, df = yahoo_extract.extract_option_calls_dataset(\n            work_dict=work)\n        if status == ae_consts.SUCCESS:\n            self.assertIsNotNone(\n                df)\n            self.debug_df(df=df)\n            self.assertTrue(ae_consts.is_df(df=df))\n            for i, r in df.iterrows():\n                print(ae_consts.ppj(json.loads(r.to_json())))\n            log.info(\n                \'done printing option call data\')\n        else:\n            log.critical(\n                \'Yahoo Option Calls are missing in redis \'\n                f\'for ticker={work[""ticker""]} \'\n                f\'status={ae_consts.get_status(status=status)}\')\n    # end of test_integration_extract_option_calls\n\n    def test_integration_extract_option_puts(self):\n        """"""test_integration_extract_option_puts""""""\n        if ae_consts.ev(\'INT_TESTS\', \'0\') == \'0\':\n            return\n\n        # build dataset cache dictionary\n        work = api_requests.get_ds_dict(\n            ticker=\'SPY\',\n            label=\'test_integration_extract_option_puts\')\n\n        status, df = yahoo_extract.extract_option_puts_dataset(\n            work_dict=work)\n        if status == ae_consts.SUCCESS:\n            self.assertIsNotNone(\n                df)\n            self.debug_df(df=df)\n        else:\n            log.critical(\n                \'Yahoo Option Puts are missing in redis \'\n                f\'for ticker={work[""ticker""]} \'\n                f\'status={ae_consts.get_status(status=status)}\')\n    # end of test_integration_extract_option_puts\n\n# end of TestYahooDatasetExtraction\n'"
tools/extract_to_file.py,0,"b'#!/usr/bin/env python\n\n""""""\nTool for extracting datasets from\nredis for developing and tuning algorithms offline\n\n.. note:: This tool requires redis to be running with\n    fetched datasets already stored in supported\n    keys\n""""""\n\nimport os\nimport sys\nimport json\nfrom analysis_engine.consts import ppj\nfrom analysis_engine.extract import extract\n\nticker = \'SPY\'\nres = extract(\n    ticker=ticker)\n\ndaily_df = res[\'SPY\'][\'daily\']\nminute_df = res[\'SPY\'][\'minute\']\n\nout_dir = \'/opt/sa/tests/datasets\'\nif not os.path.exists(out_dir):\n    print(f\'missing output dir: {out_dir}\')\n    sys.exit(1)\n\ndaily_file = f\'{out_dir}/{ticker.lower()}-daily.json\'\nminute_file = f\'{out_dir}/{ticker.lower()}-minute.json\'\nprint(\'converting dates\')\n\nprint(f\'converting to pretty printed json file={daily_file}\')\ndaily_out_json = ppj(json.loads(daily_df.iloc[-100:-1].to_json(\n    orient=\'records\',\n    date_format=\'iso\')))\nprint(f\'writing to file daily_file={daily_file}\')\nwith open(daily_file, \'w\') as f:\n    f.write(daily_out_json)\n\nif not os.path.exists(daily_file):\n    print(f\'failed creating daily ticker={ticker} daily_file={daily_file}\')\n\nprint(f\'converting to pretty printed json file={minute_file}\')\nminute_out_json = ppj(json.loads(minute_df.iloc[-100:-1].to_json(\n    orient=\'records\',\n    date_format=\'iso\')))\nprint(f\'writing to file minute_file={minute_file}\')\nwith open(minute_file, \'w\') as f:\n    f.write(minute_out_json)\n\nif not os.path.exists(minute_file):\n    print(f\'failed creating minute ticker={ticker} minute_file={minute_file}\')\n'"
tools/fix_iex_missing_daily.py,0,"b'#!/usr/bin/env python\n\n""""""\nTool for fixing option dates that have \'date\' == 0\n\n.. note:: This tool requires redis to be running with\n    fetched datasets already stored in supported\n    keys\n""""""\n\nimport pandas as pd\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.utils as ae_utils\nimport analysis_engine.extract as ae_extract\nimport analysis_engine.publish as ae_publish\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(\n    name=\'fixer\')\n\n\ndef fix_df(\n        df):\n    """"""fix_df\n\n    :param df: ``pandas.DataFrame`` df\n    """"""\n\n    use_df = df\n\n    new_recs = []\n\n    cols = list(use_df.columns.values)\n    num_to_copy = len(use_df.index) - 2\n    for idx, row in use_df.iterrows():\n        new_row = {}\n        for c in cols:\n            new_row[c] = row[c]\n            if c == \'date\':\n                log.info(f\' - including: {new_row[c]}\')\n        new_recs.append(new_row)\n        if num_to_copy > 0:\n            num_to_copy -= 1\n        else:\n            break\n\n    fixed_df = pd.DataFrame(new_recs)\n\n    if \'date\' in fixed_df:\n        fixed_df.sort_values(\n                by=[\n                    \'date\'\n                ],\n                ascending=True).reset_index()\n\n    print(fixed_df)\n    return fixed_df\n# end of fix_df\n\n\nuse_redis_address = ae_consts.REDIS_ADDRESS\nlast_close_str = ae_utils.get_last_close_str(\n    ae_consts.COMMON_DATE_FORMAT)\nuse_date_str = last_close_str\n\nsrc_date = \'2019-02-15\'\ndst_date = src_date\ndst_date = \'2019-02-14\'\ntickers = [\'SPY\']\nfor ticker in tickers:\n\n    log.info(\n        f\'extracting src df for ticker: {ticker}\')\n\n    res = None\n\n    # get from a date or the latest if not set\n    if src_date:\n        use_key = f\'{ticker}_{src_date}\'\n        res = ae_extract.extract(\n            ticker=ticker,\n            date=src_date)\n    else:\n        res = ae_extract.extract(\n            ticker=ticker)\n\n    src_df = res[ticker][\'daily\']\n    fix_key = (\n        f\'{ticker}_{dst_date}_daily\')\n\n    dst_df = fix_df(\n        df=src_df)\n    log.info(\n        f\'src df for {ticker} on {src_date}\')\n    print(src_df)\n    log.info(len(src_df.index))\n    log.info(\'-------------\')\n    log.info(\n        f\'fixed df for {ticker} on {dst_date}\')\n    print(dst_df)\n    log.info(len(dst_df.index))\n    log.info(\n        f\'publishing fix to redis: {fix_key}\')\n    pub_res = ae_publish.publish(\n        data=dst_df,\n        redis_key=fix_key,\n        redis_address=use_redis_address,\n        df_compress=True,\n        verbose=True)\n\n    dst_key = f\'{ticker}_{dst_date}\'\n    ext_res = ae_extract.extract(\n        ticker=ticker,\n        date=dst_date,\n        verbose=False)\n\n    extracted_fix_df = ext_res[ticker][\'daily\']\n\n    if len(extracted_fix_df.index) == len(dst_df.index):\n        log.info(\n            \'fixed df was published and extracted with the same \'\n            \'row counts: \'\n            f\'len(extracted_fix_df.index) \'\n            \'== \'\n            f\'{len(dst_df.index)}\')\n    else:\n        log.critical(\n            \'FAILED - fixed df was published and extracted with \'\n            \'different row counts: \'\n            f\'{len(extracted_fix_df.index)} \'\n            \'== \'\n            f\'{len(dst_df.index)}\')\n\n        log.error(\'extracted df\')\n        print(extracted_fix_df)\n        log.error(\'should be the dst_df\')\n        print(dst_df)\n        log.critical(\n            f\'tried extracting from key: {use_key}\')\n        break\n# end of for all tickers\n'"
analysis_engine/ai/__init__.py,0,b''
analysis_engine/ai/build_datasets_using_scalers.py,0,"b'""""""\nBuild scaler normalized train and test datasets\nfrom a ``pandas.DataFrame`` (like a ``Trading History`` stored in s3)\n\n.. note:: This function will create multiple copies of the data so\n    this is a memory intensive call which may overflow the\n    available memory on a machine if there are many rows\n""""""\n\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.ai.build_scaler_dataset_from_df as scaler_utils\nimport spylunking.log.setup_logging as log_utils\nimport sklearn.model_selection as tt_split\n\nlog = log_utils.build_colorized_logger(\n    name=__name__)\n\n\ndef build_datasets_using_scalers(\n        train_features,\n        test_feature,\n        df,\n        test_size,\n        seed,\n        min_feature=-1,\n        max_feature=1):\n    """"""build_datasets_using_scalers\n\n    Build train and test datasets using a\n    `MinMaxScaler <https://scikit-learn.org/stable/\n    modules/generated/\n    sklearn.preprocessing.MinMaxScaler.html>`__ for normalizing a dataset\n    before training a deep neural network.\n\n    Here\'s the returned dictionary:\n\n    .. code-block:: python\n\n        res = {\n            \'status\': status,\n            \'scaled_train_df\': scaled_train_df,\n            \'scaled_test_df\': scaled_test_df,\n            \'scaler_train\': scaler_train,\n            \'scaler_test\': scaler_test,\n            \'x_train\': x_train,\n            \'y_train\': y_train,\n            \'x_test\': x_test,\n            \'y_test\': y_test,\n        }\n\n    :param train_features: list of strings with all columns (features)\n        to train\n    :param test_feature: string name of the column to predict.\n        This is a single column name in the``df``\n        (which is a ``pandas.DataFrame``).\n    :param df: dataframe to build scaler test and train datasets\n    :param test_size: percent of test to train rows\n    :param min_feature: min scaler range\n        with default ``-1``\n    :param max_feature: max scaler range\n        with default ``1``\n    """"""\n\n    status = ae_consts.NOT_RUN\n    scaled_train_df = None\n    scaled_test_df = None\n    scaler_train = None\n    scaler_test = None\n    x_train = None\n    y_train = None\n    x_test = None\n    y_test = None\n\n    res = {\n        \'status\': status,\n        \'scaled_train_df\': scaled_train_df,\n        \'scaled_test_df\': scaled_test_df,\n        \'scaler_train\': scaler_train,\n        \'scaler_test\': scaler_test,\n        \'x_train\': x_train,\n        \'y_train\': y_train,\n        \'x_test\': x_test,\n        \'y_test\': y_test,\n    }\n\n    try:\n        log.info(\n            f\'building scalers \'\n            f\'df.rows={len(df.index)} \'\n            f\'columns={len(list(df.columns.values))} \'\n            f\'train_features={len(train_features)} \'\n            f\'test_feature={test_feature}\')\n\n        if test_feature not in df:\n            log.error(\n                f\'did not find test_feature={test_feature} in \'\n                f\'df columns={df.columns.values}\')\n            status = ae_consts.FAILED\n            res[\'status\'] = status\n            return res\n        for single_train_feature in train_features:\n            if single_train_feature not in df:\n                log.error(\n                    f\'did not find \'\n                    f\'train_feature={single_train_feature} in \'\n                    f\'df columns={df.columns.values}\')\n                status = ae_consts.FAILED\n                res[\'status\'] = status\n                return res\n\n        train_df = df[train_features]\n        test_df = df[[test_feature]]\n\n        log.info(\n            f\'building scaled train df\')\n        scaled_train_res = scaler_utils.build_scaler_dataset_from_df(\n            df=train_df,\n            min_feature=min_feature,\n            max_feature=max_feature)\n\n        log.info(\n            f\'building scaled test df\')\n        scaled_test_res = scaler_utils.build_scaler_dataset_from_df(\n            df=test_df,\n            min_feature=min_feature,\n            max_feature=max_feature)\n\n        log.info(\n            f\'scaled df transform \'\n            f\'train_status={scaled_train_res[""status""] == ae_consts.SUCCESS} \'\n            f\'test_status={scaled_test_res[""status""] == ae_consts.SUCCESS}\')\n\n        if scaled_train_res[\'status\'] == ae_consts.SUCCESS \\\n           and scaled_test_res[\'status\'] == ae_consts.SUCCESS:\n            log.info(\n                f\'scaled train_rows={len(scaled_train_res[""df""])} \'\n                f\'test_rows={len(scaled_test_res[""df""])}\')\n\n            scaler_train = scaled_train_res[\'scaler\']\n            scaler_test = scaled_test_res[\'scaler\']\n            scaled_train_df = scaled_train_res[\'df\']\n            scaled_test_df = scaled_test_res[\'df\']\n            (x_train,\n             x_test,\n             y_train,\n             y_test) = tt_split.train_test_split(\n                scaled_train_df,\n                scaled_test_df,\n                test_size=test_size,\n                random_state=seed)\n        else:\n            log.error(\n                f\'failed df transform \'\n                f\'train_status={scaled_train_res[""status""]} \'\n                f\'test_status={scaled_test_res[""status""]}\')\n            status = ae_consts.FAILED\n            res[\'status\'] = status\n            return res\n        # if built both train and test successfully\n\n        log.info(\n            f\'train_rows={len(train_df.index)} \'\n            f\'test_rows={len(test_df.index)} \'\n            f\'x_train={len(x_train)} \'\n            f\'x_test={len(x_test)} \'\n            f\'y_train={len(y_train)} \'\n            f\'y_test={len(y_test)}\')\n\n        res[\'scaled_train_df\'] = scaled_train_df\n        res[\'scaled_test_df\'] = scaled_test_df\n        res[\'scaler_train\'] = scaler_train\n        res[\'scaler_test\'] = scaler_test\n        res[\'x_train\'] = x_train\n        res[\'y_train\'] = y_train\n        res[\'x_test\'] = x_test\n        res[\'y_test\'] = y_test\n\n        status = ae_consts.SUCCESS\n\n    except Exception as e:\n        log.error(\n            f\'failed with ex={e} \'\n            f\'building scalers \'\n            f\'df.rows={len(df.index)} \'\n            f\'columns={list(df.columns.values)} \'\n            f\'train_features={train_features} \'\n            f\'test_feature={test_feature}\')\n        status = ae_consts.ERR\n    # try/ex\n\n    res[\'status\'] = status\n    return res\n# end of build_datasets_using_scalers\n'"
analysis_engine/ai/build_regression_dnn.py,0,"b'""""""\nBuild a deep neural network for regression predictions\n""""""\n\nimport json\nimport spylunking.log.setup_logging as log_utils\nimport keras.models as keras_models\nimport keras.layers as keras_layers\n\nlog = log_utils.build_colorized_logger(\n    name=__name__)\n\n\ndef build_regression_dnn(\n        num_features,\n        compile_config,\n        model_json=None,\n        model_config=None):\n    """"""build_regression_dnn\n\n    :param num_features: input_dim for the number of\n                         features in the data\n    :param compile_config: dictionary of compile options\n    :param model_json: keras model json to build the model\n    :param model_config: optional dictionary for model\n    """"""\n\n    model = None\n    num_layers = 0\n\n    if model_json:\n        log.info(\n            f\'loading from model_json={model_json}\')\n        model = keras_models.model_from_json(\n            json.dumps(model_json))\n    elif model_config:\n        model = keras_models.Sequential()\n        log.info(\n            f\'building \'\n            f\'dnn num_features={num_features} \'\n            f\'model_config={model_config}\')\n        for idx, node in enumerate(model_config[\'layers\']):\n            layer_type = node.get(\n                \'layer_type\',\n                \'dense\').lower()\n            if layer_type == \'dense\':\n                if num_layers == 0:\n                    model.add(\n                        keras_layers.Dense(\n                            int(node[\'num_neurons\']),\n                            input_dim=num_features,\n                            kernel_initializer=node[\'init\'],\n                            activation=node[\'activation\']))\n                else:\n                    model.add(\n                        keras_layers.Dense(\n                            int(node[\'num_neurons\']),\n                            kernel_initializer=node[\'init\'],\n                            activation=node[\'activation\']))\n            else:\n                if layer_type == \'dropout\':\n                    model.add(\n                        keras_layers.Dropout(\n                            float(node[\'rate\'])))\n            # end of supported model types\n            num_layers += 1\n        # end of all layers\n    else:\n        # https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/  # noqa\n        log.info(\n            f\'default dnn num_features={num_features}\')\n        model.add(\n            keras_layers.Dense(\n                8,\n                input_dim=num_features,\n                kernel_initializer=\'normal\',\n                activation=\'relu\'))\n        model.add(\n            keras_layers.Dense(\n                6,\n                kernel_initializer=\'normal\',\n                activation=\'relu\'))\n        model.add(\n            keras_layers.Dense(\n                1,\n                kernel_initializer=\'normal\'))\n    # end of building a regression dnn\n\n    # if model was defined\n    if model:\n        log.info(\n            f\'compiling={compile_config}\')\n        # compile the model\n        loss = compile_config.get(\n            \'loss\',\n            \'mse\')\n        optimizer = compile_config.get(\n            \'optimizer\',\n            \'adam\')\n        metrics = compile_config.get(\n            \'metrics\',\n            [\n                \'mse\',\n                \'mae\',\n                \'mape\',\n                \'cosine\'\n            ])\n        model.compile(\n            loss=loss,\n            optimizer=optimizer,\n            metrics=metrics)\n    else:\n        log.error(\n            f\'failed building regression model={model}\')\n    # if could compile model\n\n    return model\n# end of build_regression_dnn\n'"
analysis_engine/ai/build_scaler_dataset_from_df.py,0,"b'""""""\nBuild a scaler normalized\n``pandas.DataFrame`` from an existing ``pandas.DataFrame``\n""""""\n\nimport sklearn.preprocessing as preproc\nimport analysis_engine.consts as ae_consts\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(\n    name=__name__)\n\n\ndef build_scaler_dataset_from_df(\n        df,\n        min_feature=-1,\n        max_feature=1):\n    """"""build_scaler_dataset_from_df\n\n    Helper for building scaler datasets from an existing ``pandas.DataFrame``\n\n    returns a dictionary:\n\n    .. code-block:: python\n\n        return {\n            \'status\': status,   # NOT_RUN | SUCCESS | ERR\n            \'scaler\': scaler,   # MinMaxScaler\n            \'df\': df  # scaled df from df arg\n        }\n\n    :param df: ``pandas.DataFrame`` to convert to scalers\n    :param min_feature: min feature range for scaler normalization\n        with default ``-1``\n    :param max_feature: max feature range for scaler normalization\n        with default ``1``\n    """"""\n\n    status = ae_consts.NOT_RUN\n    scaler = None\n    output_df = None\n\n    res = {\n        \'status\': status,\n        \'scaler\': scaler,\n        \'df\': output_df\n    }\n\n    try:\n        log.debug(\n            f\'building scaler range=[{min_feature}, {max_feature}]\')\n        scaler = preproc.MinMaxScaler(\n            feature_range=(\n                min_feature,\n                max_feature))\n\n        log.info(\n            f\'scaler.fit_transform(df) rows={len(df.index)}\')\n        output_df = scaler.fit_transform(\n            df.values)\n        status = ae_consts.SUCCESS\n    except Exception as e:\n        log.error(\n            f\'failed build_scaler_dataset_from_df \'\n            f\'with ex={e} \'\n            f\'range=[{min_feature}, {max_feature}]\')\n        status = ae_consts.ERR\n    # end of try/ex\n\n    res = {\n        \'status\': status,\n        \'scaler\': scaler,\n        \'df\': output_df\n    }\n    return res\n# end of build_scaler_dataset_from_df\n'"
analysis_engine/ai/plot_dnn_fit_history.py,0,"b'""""""\nPlot a deep neural network\'s history output after training\n\nPlease check out this `blog post for more information on\nhow this works <https://\nmachinelearningmastery.com/custom-metrics-deep-learning-keras-python/>`__\n""""""\n\nimport datetime\nimport matplotlib.pyplot as plt\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.charts as ae_charts\nimport analysis_engine.build_result as build_result\nimport spylunking.log.setup_logging as log_utils\nfrom analysis_engine.send_to_slack import post_plot\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef plot_dnn_fit_history(\n        title,\n        df,\n        red,\n        red_color=None,\n        red_label=None,\n        blue=None,\n        blue_color=None,\n        blue_label=None,\n        green=None,\n        green_color=None,\n        green_label=None,\n        orange=None,\n        orange_color=None,\n        orange_label=None,\n        xlabel=\'Training Epochs\',\n        ylabel=\'Error Values\',\n        linestyle=\'-\',\n        width=9.0,\n        height=9.0,\n        date_format=\'%d\\n%b\',\n        df_filter=None,\n        start_date=None,\n        footnote_text=None,\n        footnote_xpos=0.70,\n        footnote_ypos=0.01,\n        footnote_color=\'#888888\',\n        footnote_fontsize=8,\n        scale_y=False,\n        show_plot=True,\n        dropna_for_all=False,\n        verbose=False,\n        send_plots_to_slack=False):\n    """"""plot_dnn_fit_history\n\n    Plot a DNN\'s fit history using `Keras fit history object <https://ker\n    as.io/visualization/#training-history-visualization>`__\n\n    :param title: title of the plot\n    :param df: dataset which is ``pandas.DataFrame``\n    :param red: string - column name to plot in\n        ``red_color`` (or default ``ae_consts.PLOT_COLORS[red]``)\n        where the column is in the ``df`` and\n        accessible with:``df[red]``\n    :param red_color: hex color code to plot the data in the\n        ``df[red]``  (default is ``ae_consts.PLOT_COLORS[\'red\']``)\n    :param red_label: optional - string for the label used\n        to identify the ``red`` line in the legend\n    :param blue: string - column name to plot in\n        ``blue_color`` (or default ``ae_consts.PLOT_COLORS[\'blue\']``)\n        where the column is in the ``df`` and\n        accessible with:``df[blue]``\n    :param blue_color: hex color code to plot the data in the\n        ``df[blue]``  (default is ``ae_consts.PLOT_COLORS[\'blue\']``)\n    :param blue_label: optional - string for the label used\n        to identify the ``blue`` line in the legend\n    :param green: string - column name to plot in\n        ``green_color`` (or default ``ae_consts.PLOT_COLORS[\'darkgreen\']``)\n        where the column is in the ``df`` and\n        accessible with:``df[green]``\n    :param green_color: hex color code to plot the data in the\n        ``df[green]``  (default is ``ae_consts.PLOT_COLORS[\'darkgreen\']``)\n    :param green_label: optional - string for the label used\n        to identify the ``green`` line in the legend\n    :param orange: string - column name to plot in\n        ``orange_color`` (or default ``ae_consts.PLOT_COLORS[\'orange\']``)\n        where the column is in the ``df`` and\n        accessible with:``df[orange]``\n    :param orange_color: hex color code to plot the data in the\n        ``df[orange]``  (default is ``ae_consts.PLOT_COLORS[\'orange\']``)\n    :param orange_label: optional - string for the label used\n        to identify the ``orange`` line in the legend\n    :param xlabel: x-axis label\n    :param ylabel: y-axis label\n    :param linestyle: style of the plot line\n    :param width: float - width of the image\n    :param height: float - height of the image\n    :param date_format: string - format for dates\n    :param df_filter: optional - initialized ``pandas.DataFrame`` query\n        for reducing the ``df`` records before plotting. As an eaxmple\n        ``df_filter=(df[\'close\'] > 0.01)`` would find only records in\n        the ``df`` with a ``close`` value greater than ``0.01``\n    :param start_date: optional - string ``datetime``\n        for plotting only from a date formatted as\n        ``YYYY-MM-DD HH\\\\:MM\\\\:SS``\n    :param footnote_text: optional - string footnote text\n        (default is ``algotraders <DATE>``)\n    :param footnote_xpos: optional - float for footnote position\n        on the x-axies\n        (default is ``0.75``)\n    :param footnote_ypos: optional - float for footnote position\n        on the y-axies\n        (default is ``0.01``)\n    :param footnote_color: optional - string hex color code for\n        the footnote text\n        (default is ``#888888``)\n    :param footnote_fontsize: optional - float footnote font size\n        (default is ``8``)\n    :param scale_y: optional - bool to scale the y-axis with\n        .. code-block:: python\n\n            use_ax.set_ylim(\n                [0, use_ax.get_ylim()[1] * 3])\n    :param show_plot: bool to show the plot\n    :param dropna_for_all: optional - bool to toggle keep None\'s in\n        the plot ``df`` (default is drop them for display purposes)\n    :param verbose: optional - bool to show logs for debugging\n        a dataset\n    :param send_plots_to_slack: optional - bool to send the dnn plot to slack\n    """"""\n\n    rec = {\n        \'ax1\': None,\n        \'fig\': None\n    }\n    result = build_result.build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec=rec)\n\n    if verbose:\n        log.info(\n            f\'plot_dnn_fit_history - start\')\n\n    use_red = red_color\n    use_blue = blue_color\n    use_green = green_color\n    use_orange = orange_color\n\n    if not use_red:\n        use_red = ae_consts.PLOT_COLORS[\'red\']\n    if not use_blue:\n        use_blue = ae_consts.PLOT_COLORS[\'blue\']\n    if not use_green:\n        use_green = ae_consts.PLOT_COLORS[\'darkgreen\']\n    if not use_orange:\n        use_orange = ae_consts.PLOT_COLORS[\'orange\']\n\n    use_footnote = footnote_text\n    if not use_footnote:\n        use_footnote = f\'\'\'algotraders - {datetime.datetime.now().strftime(\n            ae_consts.COMMON_TICK_DATE_FORMAT)}\'\'\'\n\n    column_list = []\n    all_plots = []\n    if red:\n        column_list.append(red)\n        all_plots.append({\n            \'column\': red,\n            \'color\': use_red})\n    if blue:\n        column_list.append(blue)\n        all_plots.append({\n            \'column\': blue,\n            \'color\': use_blue})\n    if green:\n        column_list.append(green)\n        all_plots.append({\n            \'column\': green,\n            \'color\': use_green})\n    if orange:\n        column_list.append(orange)\n        all_plots.append({\n            \'column\': orange,\n            \'color\': use_orange})\n\n    use_df = df\n\n    if hasattr(df_filter, \'to_json\'):\n        # Was seeing this warning below in Jupyter:\n        # UserWarning: Boolean Series key\n        # will be reindexed to match DataFrame index\n        # use_df = use_df[df_filter][column_list]\n        # now using:\n        use_df = use_df.loc[df_filter, column_list]\n\n    if verbose:\n        log.info(\n            f\'plot_dnn_fit_history \'\n            f\'filter df.index={len(use_df.index)} \'\n            f\'column_list={column_list}\')\n\n    ae_charts.set_common_seaborn_fonts()\n\n    hex_color = ae_consts.PLOT_COLORS[\'blue\']\n    fig, ax = plt.subplots(\n        sharex=True,\n        sharey=True,\n        figsize=(\n            width,\n            height))\n\n    all_axes = [\n        ax\n    ]\n    num_plots = len(all_plots)\n    for idx, node in enumerate(all_plots):\n        column_name = node[\'column\']\n        hex_color = node[\'color\']\n\n        use_ax = ax\n\n        if verbose:\n            log.info(\n                f\'plot_dnn_fit_history - \'\n                f\'{idx + 1}/{num_plots} - \'\n                f\'{column_name} \'\n                f\'in \'\n                f\'{hex_color} - \'\n                f\'ax={use_ax}\')\n\n        use_ax.plot(\n            use_df[column_name],\n            label=column_name,\n            linestyle=linestyle,\n            color=hex_color)\n        # end if this is not the fist axis\n\n    # end of for all plots\n\n    lines = []\n    for idx, cur_ax in enumerate(all_axes):\n        ax_lines = cur_ax.get_lines()\n        for line in ax_lines:\n            label_name = str(line.get_label())\n            use_label = label_name\n            if idx == 0:\n                if red_label:\n                    use_label = red_label\n            elif idx == 1:\n                if blue_label:\n                    use_label = blue_label\n            elif idx == 2:\n                use_label = label_name[-20:]\n                if green_label:\n                    use_label = green_label\n            elif idx == 3:\n                use_label = label_name[-20:]\n                if orange_label:\n                    use_label = orange_label\n            else:\n                if len(label_name) > 10:\n                    use_label = label_name[-20:]\n            # end of fixing the labels in the legend\n            line.set_label(use_label)\n            if line.get_label() not in lines:\n                lines.append(line)\n        rec[f\'ax{idx + 1}\'] = cur_ax\n    # end of compiling a new-shortened legend while removing dupes\n\n    for idx, cur_ax in enumerate(all_axes):\n        if cur_ax:\n            if cur_ax.get_legend():\n                cur_ax.get_legend().remove()\n    # end of removing all previous legends\n\n    if verbose:\n        log.info(\n            f\'legend lines={[l.get_label() for l in lines]}\')\n    # log what\'s going to be in the legend\n\n    ax.legend(\n        lines,\n        [l.get_label() for l in lines],\n        loc=\'best\',\n        shadow=True)\n\n    fig.autofmt_xdate()\n\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n\n    ax.set_title(title)\n    ae_charts.add_footnote(\n        fig=fig,\n        xpos=footnote_xpos,\n        ypos=footnote_ypos,\n        text=use_footnote,\n        color=footnote_color,\n        fontsize=footnote_fontsize)\n    plt.tight_layout()\n\n    if send_plots_to_slack:\n        post_plot(plt, title=title)\n\n    if show_plot:\n        plt.show()\n    else:\n        plt.plot()\n\n    rec[\'fig\'] = fig\n\n    result = build_result.build_result(\n        status=ae_consts.SUCCESS,\n        err=None,\n        rec=rec)\n\n    return result\n# end of plot_dnn_fit_history\n'"
analysis_engine/finviz/__init__.py,0,b''
analysis_engine/finviz/consts.py,0,"b'""""""\nFinViz constants and static values\n""""""\n\nFETCH_SCREENER_TICKERS = 1200\n\nDATAFEED_SCREENER_TICKERS = 1300\n\nDEFAULT_FINVIZ_COLUMNS = [\n    \'ticker_id\',\n    \'ticker\',\n    \'company\',\n    \'sector\',\n    \'industry\',\n    \'country\',\n    \'market_cap\',\n    \'pe\',\n    \'price\',\n    \'change\',\n    \'volume\'\n]\n\n\ndef get_ft_str_finviz(\n        ft_type):\n    """"""get_ft_str_finviz\n\n    :param ft_type: enum fetch type value to return\n                    as a string\n    """"""\n    if ft_type == FETCH_SCREENER_TICKERS:\n        return \'fv_screener\'\n    else:\n        return f\'unsupported ft_type={ft_type}\'\n    # end of if/else\n# end of get_ft_str_finviz\n\n\ndef get_datafeed_str_finviz(\n        df_type):\n    """"""get_ft_str_finviz\n\n    :param df_type: enum fetch type value to return\n                    as a string\n    """"""\n    if df_type == DATAFEED_SCREENER_TICKERS:\n        return \'fv_screener\'\n    else:\n        return f\'unsupported df_type={df_type}\'\n# end of get_datafeed_str_finviz\n'"
analysis_engine/finviz/fetch_api.py,0,"b'""""""\nSupported Fetch calls\n\n- Convert a FinViz Screener URL to a list of\n  tickers.\n\n""""""\n\nimport requests\nimport bs4\nimport pandas as pd\nimport analysis_engine.build_result as req_utils\nfrom analysis_engine.utils import get_last_close_str\nfrom analysis_engine.consts import NOT_RUN\nfrom analysis_engine.consts import SUCCESS\nfrom analysis_engine.consts import ERR\nfrom analysis_engine.consts import EX\nfrom analysis_engine.finviz.consts import DEFAULT_FINVIZ_COLUMNS\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef fetch_tickers_from_screener(\n        url,\n        columns=DEFAULT_FINVIZ_COLUMNS,\n        as_json=False,\n        soup_selector=\'td.screener-body-table-nw\',\n        label=\'fz-screen-converter\'):\n    """"""fetch_tickers_from_screener\n\n    Convert all the tickers on a FinViz screener\n    url to a ``pandas.DataFrame``. Returns a dictionary\n    with a ticker list and DataFrame or a json-serialized\n    DataFrame in a string (by default ``as_json=False`` will\n    return a ``pandas.DataFrame`` if the\n    ``returned-dictionary[\'status\'] == SUCCESS``\n\n    Works with urls created on:\n\n    https://finviz.com/screener.ashx\n\n    .. code-block:: python\n\n        import analysis_engine.finviz.fetch_api as fv\n\n        url = (\n            \'https://finviz.com/screener.ashx?\'\n            \'v=111&\'\n            \'f=cap_midunder,exch_nyse,fa_div_o5,idx_sp500\'\n            \'&ft=4\')\n        res = fv.fetch_tickers_from_screener(url=url)\n        print(res)\n\n    :param url: FinViz screener url\n    :param columns: ordered header column as a list of strings\n                    and corresponds to the header row from the\n                    FinViz screener table\n    :param soup_selector: ``bs4.BeautifulSoup.selector`` string\n                          for pulling selected html data\n                          (by default ``td.screener-body-table-nw``)\n    :param as_json: FinViz screener url\n    :param label: log tracking label string\n    """"""\n\n    rec = {\n        \'data\': None,\n        \'created\': get_last_close_str(),\n        \'tickers\': []\n    }\n    res = req_utils.build_result(\n        status=NOT_RUN,\n        err=None,\n        rec=rec)\n\n    try:\n\n        log.info(f\'{label} fetching url={url}\')\n\n        response = requests.get(url)\n\n        if response.status_code != requests.codes.ok:\n            err = (\n                f\'{label} finviz returned non-ok HTTP (200) \'\n                f\'status_code={response.status_code} with \'\n                f\'text={response.text} for url={url}\')\n            log.error(err)\n            return req_utils.build_result(\n                status=ERR,\n                err=err,\n                rec=rec)\n        # end of checking for a good HTTP response status code\n\n        soup = bs4.BeautifulSoup(\n            response.text,\n            features=\'html.parser\')\n        selected = soup.select(soup_selector)\n\n        log.debug(f\'{label} found={len(selected)} url={url}\')\n\n        ticker_list = []\n        rows = []\n        use_columns = columns\n        num_columns = len(use_columns)\n        new_row = {}\n        col_idx = 0\n\n        for idx, node in enumerate(selected):\n\n            if col_idx >= num_columns:\n                col_idx = 0\n            column_name = use_columns[col_idx]\n            test_text = str(node.text).lower().strip()\n            col_idx += 1\n\n            if column_name != \'ignore\' and (\n                    test_text != \'save as portfolio\'\n                    and test_text != \'export\'):\n\n                cur_text = str(node.text).strip()\n\n                if column_name == \'ticker\':\n                    ticker_list.append(cur_text)\n                    new_row[column_name] = cur_text.upper()\n                else:\n                    new_row[column_name] = cur_text\n                # end of filtering bad sections around table\n\n                if len(new_row) >= num_columns:\n                    log.debug(f\'{label} adding ticker={new_row[""ticker""]}\')\n                    rows.append(new_row)\n                    new_row = {}\n                    col_idx = 0\n                # end of if valid row\n            # end if column is valid\n        # end of walking through all matched html data on the screener\n\n        log.debug(\n            f\'{label} done convert url={url} to tickers={ticker_list} \'\n            f\'rows={len(rows)}\')\n\n        df = pd.DataFrame(\n            rows)\n\n        log.info(\n            f\'{label} fetch done - df={len(df.index)} from url={url} \'\n            f\'with tickers={ticker_list} rows={len(rows)}\')\n\n        rec[\'tickers\'] = ticker_list\n        rec[\'data\'] = df\n\n        res = req_utils.build_result(\n            status=SUCCESS,\n            err=None,\n            rec=rec)\n\n    except Exception as e:\n        rec[\'tickers\'] = []\n        rec[\'data\'] = None\n        err = (\n            f\'{label} failed converting screen url={url} to list with ex={e}\')\n        log.error(err)\n        res = req_utils.build_result(\n            status=EX,\n            err=err,\n            rec=rec)\n    # end of try/ex\n\n    return res\n# end of fetch_tickers_from_screener\n'"
analysis_engine/iex/__init__.py,0,b''
analysis_engine/iex/build_auth_url.py,0,"b'""""""\nBuild an authenticated url for IEX Cloud\n""""""\n\n\ndef build_auth_url(\n        url,\n        token=None):\n    """"""build_auth_url\n\n    Helper for constructing authenticated IEX urls\n    using an ``IEX Publishable Token`` with a valid\n    `IEX Cloud Beta Account <https://\n    iexcloud.io/cloud-login#/register/>`__\n\n    This will return a string with the token as a query\n    parameter on the HTTP url\n\n    :param url: initial url to make authenticated\n    :param token: optional - string ``IEX Publishable Token``\n        (defaults to ``IEX_TOKEN`` environment variable or\n        ``None``)\n    """"""\n    if token:\n        return (\n            f\'{url}?token={token}\')\n    else:\n        return url\n# end of build_auth_url\n'"
analysis_engine/iex/collect_and_merge_data.py,0,"b'""""""\nGet Pricing using most of the pieces of example from:\nhttps://github.com/timkpaine/hedgedata/blob/master/scripts/fetch_data.py\n\nThis will fetch various pricing, financials, earnings,\ndividends, news and other data from\n`IEX <https://iexcloud.io/>`__.\n\nData Attribution\n================\n\nIEX Real-Time Price\n===================\n\n- Cite IEX using the following text and link:\n  ""Data provided by [IEX](https://iextrading.com/developer/docs/).""\n- Provide a link to\n  https://iextrading.com/api-exhibit-a in your terms of service.\n- Additionally, if you display our TOPS price data,\n  cite ""IEX Real-Time Price"" near the price.\n\n""""""\n\nimport os\nimport os.path\nimport pandas as pd\nimport analysis_engine.iex.consts\nfrom hedgedata.backfill import whichBackfill\nfrom hedgedata.data import FIELDS\nfrom hedgedata.distributor import Distributer\nfrom spylunking.log.setup_logging import build_colorized_logger\n# from hedgedata import sp500_constituents\n\n\nlog = build_colorized_logger(\n    name=__name__)\n\n\n_DISTRIBUTOR = Distributer.default()\n\n\ndef backfillData(\n        distributor,\n        symbols,\n        fields,\n        output=\'cache\'):\n    """"""backfillData\n\n    :param distributor: distributor object from:\n                        https://github.com/timkpaine/hedgedata\n    :param symbols: list of symbols to iterate\n    :param fields: path to files\n    :param output: output dir\n    """"""\n    if not os.path.exists(output):\n        os.makedirs(output)\n\n    for field in fields:\n        if os.path.exists(os.path.join(output, field) + \'.csv\'):\n            data_orig = pd.read_csv(os.path.join(output, field) + \'.csv\')\n            for k in (\n                    \'date\',\n                    \'datetime\',\n                    \'reportDate\',\n                    \'EPSReportDate\',\n                    \'exDate\'):\n                if k in data_orig.columns:\n                    data_orig[k] = pd.to_datetime(data_orig[k])\n\n        else:\n            data_orig = pd.DataFrame()\n\n        for symbol, data in whichBackfill(field)(_DISTRIBUTOR, symbols):\n            if data.empty:\n                log.critical(\'Skipping %s for %s\' % (symbol, field))\n                continue\n\n            log.critical(\'Filling %s for %s\' % (symbol, field))\n            data.reset_index(inplace=True)\n\n            if field == \'PEERS\':\n                data = data[[\'peer\']]\n\n            data[\'KEY\'] = symbol\n            data_orig = pd.concat(\n                [data_orig, data]) if not data_orig.empty else data\n\n        if not data_orig.empty:\n            data_orig.set_index(\n                analysis_engine.iex.consts.get_default_fields(\n                    field),\n                inplace=True)\n            data_orig[\n                ~data_orig.index.duplicated(\n                    keep=\'first\')].to_csv(\n                        os.path.join(output, field) + \'.csv\')\n\n\nif __name__ == \'__main__\':\n    syms = [\n        \'SPY\',\n        \'AMZN\',\n        \'TLSA\',\n        \'NFLX\'\n    ]\n    backfillData(\n        symbols=syms,\n        fields=FIELDS,\n        output=\'./cache\')\n'"
analysis_engine/iex/consts.py,0,"b'""""""\n**IEX Cloud Environment Variables**\n\n.. code-block:: python\n\n    IEX_API_VERSION = os.getenv(\n        \'IEX_API_VERSION\',\n        \'beta\')\n    IEX_URL_BASE = os.getenv(\n        \'IEX_URL\',\n        f\'https://cloud.iexapis.com/{IEX_API_VERSION}\')\n    IEX_URL_BASE_V1 = os.getenv(\n        \'IEX_URL_V1\',\n        \'https://api.iextrading.com/1.0/\')\n    IEX_TOKEN = os.getenv(\n        \'IEX_TOKEN\',\n        None)\n    IEX_PROXIES = os.getenv(\n        \'IEX_PROXIES\',\n        None)\n    DEFAULT_FETCH_DATASETS=""daily,minute,quote,stats,\n    peers,news,financials,earnings,dividends,company""\n\n""""""\n\nimport os\n\n\nFETCH_DAILY = 800\nFETCH_MINUTE = 801\nFETCH_QUOTE = 802\nFETCH_STATS = 803\nFETCH_PEERS = 804\nFETCH_NEWS = 805\nFETCH_FINANCIALS = 806\nFETCH_EARNINGS = 807\nFETCH_DIVIDENDS = 808\nFETCH_COMPANY = 809\n\nDATAFEED_DAILY = 900\nDATAFEED_MINUTE = 901\nDATAFEED_QUOTE = 902\nDATAFEED_STATS = 903\nDATAFEED_PEERS = 904\nDATAFEED_NEWS = 905\nDATAFEED_FINANCIALS = 906\nDATAFEED_EARNINGS = 907\nDATAFEED_DIVIDENDS = 908\nDATAFEED_COMPANY = 909\n\nDEFAULT_FETCH_DATASETS = [\n    FETCH_DAILY,\n    FETCH_MINUTE,\n    FETCH_QUOTE,\n    FETCH_STATS,\n    FETCH_PEERS,\n    FETCH_NEWS,\n    FETCH_FINANCIALS,\n    FETCH_EARNINGS,\n    FETCH_DIVIDENDS,\n    FETCH_COMPANY\n]\nTIMESENSITIVE_DATASETS = [\n    FETCH_MINUTE,\n    FETCH_QUOTE,\n    FETCH_NEWS\n]\nFUNDAMENTAL_DATASETS = [\n    FETCH_QUOTE,\n    FETCH_FINANCIALS,\n    FETCH_EARNINGS,\n    FETCH_DIVIDENDS,\n    FETCH_STATS\n]\n\nIEX_DATE_FORMAT = \'%Y-%m-%d\'\nIEX_TICK_FORMAT = \'%Y-%m-%d %H:%M:%S\'\nIEX_FETCH_MINUTE_FORMAT = \'%H:%M\'\n\n# IEX Cloud Environment Variables\nIEX_API_VERSION = os.getenv(\n    \'IEX_API_VERSION\',\n    \'beta\')\nIEX_URL_BASE = os.getenv(\n    \'IEX_URL\',\n    f\'https://cloud.iexapis.com/{IEX_API_VERSION}\')\nIEX_URL_BASE_V1 = os.getenv(\n    \'IEX_URL_V1\',\n    \'https://api.iextrading.com/1.0/\')\nIEX_TOKEN = os.getenv(\n    \'IEX_TOKEN\',\n    None)\nIEX_PROXIES = os.getenv(\n    \'IEX_PROXIES\',\n    None)\nIEX_DATE_FIELDS = [\n    \'date\',\n    \'EPSReportDate\',\n    \'fiscalEndDate\',\n    \'exDate\',\n    \'declaredDate\',\n    \'paymentDate\',\n    \'recordDate\',\n    \'reportDate\',\n    \'datetime\',\n    \'expectedDate\',\n    \'latestTime\',\n    \'DailyListTimestamp\',\n    \'RecordUpdateTime\']\nIEX_TIME_FIELDS = [\n    \'closeTime\',\n    \'close.time\',\n    \'delayedPriceTime\',\n    \'extendedPriceTime\',\n    \'iexLastUpdated\',\n    \'latestTime\',\n    \'openTime\',\n    \'open.time\'\n    \'processedTime\',\n    \'time\',\n    \'timestamp\',\n    \'lastUpdated\']\nIEX_EPOCH_FIELDS = [\n    \'datetime\'\n]\nIEX_SECOND_FIELDS = []\n\nENV_FETCH_DATASETS = os.getenv(\n    \'DEFAULT_FETCH_DATASETS_IEX\',\n    None)\nif ENV_FETCH_DATASETS:\n    SPLIT_FETCH_DATASETS_IEX = ENV_FETCH_DATASETS.split(\',\')\n    DEFAULT_FETCH_DATASETS = []\n    for d in SPLIT_FETCH_DATASETS_IEX:\n        if d == \'minute\':\n            DEFAULT_FETCH_DATASETS.append(\n                FETCH_MINUTE)\n        elif d == \'daily\':\n            DEFAULT_FETCH_DATASETS.append(\n                FETCH_DAILY)\n        elif d == \'quote\':\n            DEFAULT_FETCH_DATASETS.append(\n                FETCH_QUOTE)\n        elif d == \'stats\':\n            DEFAULT_FETCH_DATASETS.append(\n                FETCH_STATS)\n        elif d == \'peers\':\n            DEFAULT_FETCH_DATASETS.append(\n                FETCH_PEERS)\n        elif d == \'news\':\n            DEFAULT_FETCH_DATASETS.append(\n                FETCH_NEWS)\n        elif d == \'financials\':\n            DEFAULT_FETCH_DATASETS.append(\n                FETCH_FINANCIALS)\n        elif d == \'earnings\':\n            DEFAULT_FETCH_DATASETS.append(\n                FETCH_EARNINGS)\n        elif d == \'dividends\':\n            DEFAULT_FETCH_DATASETS.append(\n                FETCH_DIVIDENDS)\n        elif d == \'company\':\n            DEFAULT_FETCH_DATASETS.append(\n                FETCH_COMPANY)\n# end of building env-datasets to get\n\nFETCH_DATASETS = DEFAULT_FETCH_DATASETS\n\n\ndef get_ft_str(\n        ft_type):\n    """"""get_ft_str\n\n    :param ft_type: enum fetch type value to return\n                    as a string\n    """"""\n    ft_str = str(ft_type).lower()\n    if ft_type == FETCH_DAILY or ft_str == \'daily\':\n        return \'daily\'\n    elif ft_type == FETCH_MINUTE or ft_str == \'minute\':\n        return \'minute\'\n    elif ft_type == FETCH_QUOTE or ft_str == \'quote\':\n        return \'quote\'\n    elif ft_type == FETCH_STATS or ft_str == \'stats\':\n        return \'stats\'\n    elif ft_type == FETCH_PEERS or ft_str == \'peers\':\n        return \'peers\'\n    elif ft_type == FETCH_NEWS or ft_str == \'news\':\n        return \'news\'\n    elif ft_type == FETCH_FINANCIALS or ft_str == \'financials\':\n        return \'financials\'\n    elif ft_type == FETCH_EARNINGS or ft_str == \'earnings\':\n        return \'earnings\'\n    elif ft_type == FETCH_DIVIDENDS or ft_str == \'dividends\':\n        return \'dividends\'\n    elif ft_type == FETCH_COMPANY or ft_str == \'company\':\n        return \'company\'\n    else:\n        return f\'unsupported ft_type={ft_type}\'\n# end of get_ft_str\n\n\ndef get_datafeed_str(\n        df_type):\n    """"""get_datafeed_str\n\n    :param df_type: enum fetch type value to return\n                    as a string\n    """"""\n    if df_type == DATAFEED_DAILY:\n        return \'daily\'\n    elif df_type == DATAFEED_MINUTE:\n        return \'minute\'\n    elif df_type == DATAFEED_QUOTE:\n        return \'quote\'\n    elif df_type == DATAFEED_STATS:\n        return \'stats\'\n    elif df_type == DATAFEED_PEERS:\n        return \'peers\'\n    elif df_type == DATAFEED_NEWS:\n        return \'news\'\n    elif df_type == DATAFEED_FINANCIALS:\n        return \'financials\'\n    elif df_type == DATAFEED_EARNINGS:\n        return \'earnings\'\n    elif df_type == DATAFEED_DIVIDENDS:\n        return \'dividends\'\n    elif df_type == DATAFEED_COMPANY:\n        return \'company\'\n    else:\n        return f\'unsupported df_type={df_type}\'\n# end of get_datafeed_str\n'"
analysis_engine/iex/extract_df_from_redis.py,0,"b'""""""\nExtract an IEX dataset from Redis and\nreturn it as a ``pandas.DataFrame`` or None\n\nPlease refer to the `Extraction API reference\nfor additional support <https://stock-analysis-engine.readthedocs.io/\nen/latest/extract.html>`__\n\n""""""\n\nimport copy\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.utils as ae_utils\nimport analysis_engine.api_requests as api_requests\nimport analysis_engine.iex.consts as iex_consts\nimport analysis_engine.extract_utils as extract_utils\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\nkeys = {\n    \'company\': iex_consts.DATAFEED_COMPANY,\n    \'daily\': iex_consts.DATAFEED_DAILY,\n    \'dividends\': iex_consts.DATAFEED_DIVIDENDS,\n    \'earnings\': iex_consts.DATAFEED_EARNINGS,\n    \'financials\': iex_consts.DATAFEED_FINANCIALS,\n    \'minute\': iex_consts.DATAFEED_MINUTE,\n    \'news1\': iex_consts.DATAFEED_NEWS,\n    \'peers\': iex_consts.DATAFEED_PEERS,\n    \'quote\': iex_consts.DATAFEED_QUOTE,\n    \'stats\': iex_consts.DATAFEED_STATS\n}\n\n\ndef extract_daily_dataset(\n        ticker=None,\n        date=None,\n        work_dict=None,\n        scrub_mode=\'sort-by-date\',\n        verbose=False):\n    """"""extract_daily_dataset\n\n    Extract the IEX daily data for a ticker from Redis and\n    return it as a tuple (status, ``pandas.Dataframe``)\n\n    .. code-block:: python\n\n        import analysis_engine.iex.extract_df_from_redis as iex_extract\n\n        # extract by historical date is also supported as an arg\n        # date=\'2019-02-15\'\n        daily_status, daily_df = iex_extract.extract_daily_dataset(\n            ticker=\'SPY\')\n        print(daily_df)\n\n    :param ticker: string ticker to extract\n    :param date: optional - string date to extract\n        formatted ``YYYY-MM-DD``\n    :param work_dict: dictionary of args\n    :param scrub_mode: type of scrubbing handler to run\n    :param verbose: optional - boolean for turning on logging\n    """"""\n    return extract_dataset(\n        key=\'daily\',\n        work_dict=work_dict,\n        ticker=ticker,\n        date=date,\n        scrub_mode=scrub_mode,\n        verbose=verbose)\n# end of extract_daily_dataset\n\n\ndef extract_minute_dataset(\n        ticker=None,\n        date=None,\n        work_dict=None,\n        scrub_mode=\'sort-by-date\',\n        verbose=False):\n    """"""extract_minute_dataset\n\n    Extract the IEX minute intraday data for a ticker from Redis and\n    return it as a tuple (status, ``pandas.Dataframe``)\n\n    .. code-block:: python\n\n        import analysis_engine.iex.extract_df_from_redis as iex_extract\n\n        # extract by historical date is also supported as an arg\n        # date=\'2019-02-15\'\n        minute_status, minute_df = iex_extract.extract_minute_dataset(\n            ticker=\'SPY\')\n        print(minute_df)\n\n    :param ticker: string ticker to extract\n    :param date: optional - string date to extract\n        formatted ``YYYY-MM-DD``\n    :param scrub_mode: type of scrubbing handler to run\n    :param work_dict: dictionary of args\n    :param verbose: optional - boolean for turning on logging\n    """"""\n    return extract_dataset(\n        key=\'minute\',\n        work_dict=work_dict,\n        ticker=ticker,\n        date=date,\n        scrub_mode=scrub_mode,\n        verbose=verbose)\n# end of extract_minute_dataset\n\n\ndef extract_quote_dataset(\n        ticker=None,\n        date=None,\n        work_dict=None,\n        scrub_mode=\'sort-by-date\',\n        verbose=False):\n    """"""extract_quote_dataset\n\n    Extract the IEX quote data for a ticker from Redis and\n    return it as a tuple (status, ``pandas.Dataframe``)\n\n    .. code-block:: python\n\n        import analysis_engine.iex.extract_df_from_redis as iex_extract\n\n        # extract by historical date is also supported as an arg\n        # date=\'2019-02-15\'\n        quote_status, quote_df = iex_extract.extract_quote_dataset(\n            ticker=\'SPY\')\n        print(quote_df)\n\n    :param ticker: string ticker to extract\n    :param date: optional - string date to extract\n        formatted ``YYYY-MM-DD``\n    :param work_dict: dictionary of args\n    :param scrub_mode: type of scrubbing handler to run\n    :param verbose: optional - boolean for turning on logging\n    """"""\n    return extract_dataset(\n        key=\'quote\',\n        work_dict=work_dict,\n        ticker=ticker,\n        date=date,\n        scrub_mode=scrub_mode,\n        verbose=verbose)\n# end of extract_quote_dataset\n\n\ndef extract_stats_dataset(\n        ticker=None,\n        date=None,\n        work_dict=None,\n        scrub_mode=\'sort-by-date\',\n        verbose=False):\n    """"""extract_stats_dataset\n\n    Extract the IEX statistics data for a ticker from Redis and\n    return it as a tuple (status, ``pandas.Dataframe``)\n\n    .. code-block:: python\n\n        import analysis_engine.iex.extract_df_from_redis as iex_extract\n\n        # extract by historical date is also supported as an arg\n        # date=\'2019-02-15\'\n        stats_status, stats_df = iex_extract.extract_stats_dataset(\n            ticker=\'SPY\')\n        print(stats_df)\n\n    :param ticker: string ticker to extract\n    :param date: optional - string date to extract\n        formatted ``YYYY-MM-DD``\n    :param work_dict: dictionary of args\n    :param scrub_mode: type of scrubbing handler to run\n    :param verbose: optional - boolean for turning on logging\n    """"""\n    return extract_dataset(\n        key=\'stats\',\n        work_dict=work_dict,\n        ticker=ticker,\n        date=date,\n        scrub_mode=scrub_mode,\n        verbose=verbose)\n# end of extract_stats_dataset\n\n\ndef extract_peers_dataset(\n        ticker=None,\n        date=None,\n        work_dict=None,\n        scrub_mode=\'sort-by-date\',\n        verbose=False):\n    """"""extract_peers_dataset\n\n    Extract the IEX peers data for a ticker from Redis and\n    return it as a tuple (status, ``pandas.Dataframe``)\n\n    .. code-block:: python\n\n        import analysis_engine.iex.extract_df_from_redis as iex_extract\n\n        # extract by historical date is also supported as an arg\n        # date=\'2019-02-15\'\n        peers_status, peers_df = iex_extract.extract_peers_dataset(\n            ticker=\'SPY\')\n        print(peers_df)\n\n    :param ticker: string ticker to extract\n    :param date: optional - string date to extract\n        formatted ``YYYY-MM-DD``\n    :param work_dict: dictionary of args\n    :param scrub_mode: type of scrubbing handler to run\n    :param verbose: optional - boolean for turning on logging\n    """"""\n    return extract_dataset(\n        key=\'peers\',\n        work_dict=work_dict,\n        ticker=ticker,\n        date=date,\n        scrub_mode=scrub_mode,\n        verbose=verbose)\n# end of extract_peers_dataset\n\n\ndef extract_news_dataset(\n        ticker=None,\n        date=None,\n        work_dict=None,\n        scrub_mode=\'sort-by-date\',\n        verbose=False):\n    """"""extract_news_dataset\n\n    Extract the IEX news data for a ticker from Redis and\n    return it as a tuple (status, ``pandas.Dataframe``)\n\n    .. code-block:: python\n\n        import analysis_engine.iex.extract_df_from_redis as iex_extract\n\n        # extract by historical date is also supported as an arg\n        # date=\'2019-02-15\'\n        news_status, news_df = iex_extract.extract_news_dataset(\n            ticker=\'SPY\')\n        print(news_df)\n\n    :param ticker: string ticker to extract\n    :param date: optional - string date to extract\n        formatted ``YYYY-MM-DD``\n    :param work_dict: dictionary of args\n    :param scrub_mode: type of scrubbing handler to run\n    :param verbose: optional - boolean for turning on logging\n    """"""\n    return extract_dataset(\n        key=\'news1\',\n        work_dict=work_dict,\n        ticker=ticker,\n        date=date,\n        scrub_mode=scrub_mode,\n        verbose=verbose)\n# end of extract_news_dataset\n\n\ndef extract_financials_dataset(\n        ticker=None,\n        date=None,\n        work_dict=None,\n        scrub_mode=\'sort-by-date\',\n        verbose=False):\n    """"""extract_financials_dataset\n\n    Extract the IEX financial data for a ticker from Redis and\n    return it as a tuple (status, ``pandas.Dataframe``)\n\n    .. code-block:: python\n\n        import analysis_engine.iex.extract_df_from_redis as iex_extract\n\n        # extract by historical date is also supported as an arg\n        # date=\'2019-02-15\'\n        fin_status, fin_df = iex_extract.extract_financials_dataset(\n            ticker=\'SPY\')\n        print(fin_df)\n\n    :param ticker: string ticker to extract\n    :param date: optional - string date to extract\n        formatted ``YYYY-MM-DD``\n    :param work_dict: dictionary of args\n    :param scrub_mode: type of scrubbing handler to run\n    :param verbose: optional - boolean for turning on logging\n    """"""\n    return extract_dataset(\n        key=\'financials\',\n        work_dict=work_dict,\n        ticker=ticker,\n        date=date,\n        scrub_mode=scrub_mode,\n        verbose=verbose)\n# end of extract_financials_dataset\n\n\ndef extract_earnings_dataset(\n        ticker=None,\n        date=None,\n        work_dict=None,\n        scrub_mode=\'sort-by-date\',\n        verbose=False):\n    """"""extract_earnings_dataset\n\n    Extract the IEX earnings data for a ticker from Redis and\n    return it as a tuple (status, ``pandas.Dataframe``)\n\n    .. code-block:: python\n\n        import analysis_engine.iex.extract_df_from_redis as iex_extract\n\n        # extract by historical date is also supported as an arg\n        # date=\'2019-02-15\'\n        earn_status, earn_df = iex_extract.extract_earnings_dataset(\n            ticker=\'SPY\')\n        print(earn_df)\n\n    :param ticker: string ticker to extract\n    :param date: optional - string date to extract\n        formatted ``YYYY-MM-DD``\n    :param work_dict: dictionary of args\n    :param scrub_mode: type of scrubbing handler to run\n    :param verbose: optional - boolean for turning on logging\n    """"""\n    return extract_dataset(\n        key=\'earnings\',\n        work_dict=work_dict,\n        ticker=ticker,\n        date=date,\n        scrub_mode=scrub_mode,\n        verbose=verbose)\n# end of extract_earnings_dataset\n\n\ndef extract_dividends_dataset(\n        ticker=None,\n        date=None,\n        work_dict=None,\n        scrub_mode=\'sort-by-date\',\n        verbose=False):\n    """"""extract_dividends_dataset\n\n    Extract the IEX dividends data for a ticker from Redis and\n    return it as a tuple (status, ``pandas.Dataframe``)\n\n    .. code-block:: python\n\n        import analysis_engine.iex.extract_df_from_redis as iex_extract\n\n        # extract by historical date is also supported as an arg\n        # date=\'2019-02-15\'\n        div_status, div_df = iex_extract.extract_dividends_dataset(\n            ticker=\'SPY\')\n        print(div_df)\n\n    :param ticker: string ticker to extract\n    :param date: optional - string date to extract\n        formatted ``YYYY-MM-DD``\n    :param work_dict: dictionary of args\n    :param scrub_mode: type of scrubbing handler to run\n    :param verbose: optional - boolean for turning on logging\n    """"""\n    return extract_dataset(\n        key=\'dividends\',\n        work_dict=work_dict,\n        ticker=ticker,\n        date=date,\n        scrub_mode=scrub_mode,\n        verbose=verbose)\n# end of extract_dividends_dataset\n\n\ndef extract_company_dataset(\n        ticker=None,\n        date=None,\n        work_dict=None,\n        scrub_mode=\'NO_SORT\',\n        verbose=False):\n    """"""extract_company_dataset\n\n    Extract the IEX company data for a ticker from Redis and\n    return it as a tuple (status, ``pandas.Dataframe``)\n\n    .. code-block:: python\n\n        import analysis_engine.iex.extract_df_from_redis as iex_extract\n\n        # extract by historical date is also supported as an arg\n        # date=\'2019-02-15\'\n        comp_status, comp_df = iex_extract.extract_company_dataset(\n            ticker=\'SPY\')\n        print(comp_df)\n\n    :param ticker: string ticker to extract\n    :param date: optional - string date to extract\n        formatted ``YYYY-MM-DD``\n    :param work_dict: dictionary of args\n    :param scrub_mode: type of scrubbing handler to run\n    :param verbose: optional - boolean for turning on logging\n    """"""\n    return extract_dataset(\n        key=\'company\',\n        work_dict=work_dict,\n        ticker=ticker,\n        date=date,\n        scrub_mode=scrub_mode,\n        verbose=verbose)\n# end of extract_company_dataset\n\n\ndef extract_dataset(\n        key,\n        ticker=None,\n        date=None,\n        work_dict=None,\n        scrub_mode=\'NO_SORT\',\n        verbose=False):\n    """"""extract_dataset\n\n    Extract the IEX key data for a ticker from Redis and\n    return it as a tuple (status, ``pandas.Dataframe``)\n\n    :param key: IEX dataset key\n    :param ticker: string ticker to extract\n    :param date: optional - string date to extract\n        formatted ``YYYY-MM-DD``\n    :param work_dict: dictionary of args\n    :param scrub_mode: type of scrubbing handler to run\n    :param verbose: optional - boolean for turning on logging\n    """"""\n    if not key or key not in keys:\n        log.error(\n            f\'unsupported extract key={key} in keys={keys}\')\n        return None\n    label = key\n    df_type = keys[key]\n    df_str = iex_consts.get_datafeed_str(df_type=df_type)\n    latest_close_date = ae_utils.get_last_close_str()\n\n    use_date = date\n    if work_dict:\n        if not ticker:\n            ticker = work_dict.get(\'ticker\', None)\n    if not work_dict:\n        work_dict = api_requests.get_ds_dict(\n            ticker=ticker)\n\n    req = copy.deepcopy(work_dict)\n\n    if not use_date:\n        use_date = latest_close_date\n\n    redis_key = f\'{ticker}_{use_date}_{key}\'\n    req[\'redis_key\'] = redis_key\n    req[\'s3_key\'] = redis_key\n\n    if verbose:\n        log.info(\n            f\'{label} - {df_str} - \'\n            f\'date={date} \'\n            f\'redis_key={req[""redis_key""]} \'\n            f\'s3_key={req[""s3_key""]} \'\n            f\'{ae_consts.ppj(req)}\')\n\n    return extract_utils.perform_extract(\n        df_type=df_type,\n        df_str=df_str,\n        work_dict=req,\n        scrub_mode=scrub_mode,\n        verbose=verbose)\n# end of extract_dataset\n'"
analysis_engine/iex/fetch_api.py,0,"b'""""""\nFetch API calls for pulling IEX Cloud Data from\na valid IEX account\n\n.. warning:: Running these API calls will impact\n    your account\'s monthly quota. Please be\n    aware of your usage when calling these.\n\nPlease set the environment variable ``IEX_TOKEN`` to\nyour account token before running these calls.\n\nMore steps can be found on the docs in the\n`IEX API <https://stock-analysis-engine.readth\nedocs.io/en/latest/iex_api.html#iex-api>`__\n\n**Command Line Tool Fetching Examples**\n\nWith the Analysis Engine stack running you can use\nthe pip\'s included ``fetch`` command line tool with the\nfollowing arguments to pull data (and automate it).\n\n**Fetch Minute Data**\n\n::\n\n    fetch -t AAPL -g min\n\n**Fetch Daily Data**\n\n::\n\n    fetch -t AAPL -g day\n\n**Fetch Quote Data**\n\n::\n\n    fetch -t AAPL -g quote\n\n**Fetch Stats Data**\n\n::\n\n    fetch -t AAPL -g stats\n\n**Fetch Peers Data**\n\n::\n\n    fetch -t AAPL -g peers\n\n**Fetch News Data**\n\n::\n\n    fetch -t AAPL -g news\n\n**Fetch Financials Data**\n\n::\n\n    fetch -t AAPL -g fin\n\n**Fetch Earnings Data**\n\n::\n\n    fetch -t AAPL -g earn\n\n**Fetch Dividends Data**\n\n::\n\n    fetch -t AAPL -g div\n\n**Fetch Company Data**\n\n::\n\n    fetch -t AAPL -g comp\n\n**Command Line Fetch Debugging**\n\nAdd the ``-d`` flag to the ``fetch`` command to enable\nverbose logging. Here is an example:\n\n::\n\n    fetch -t AAPL -g news -d\n\n""""""\n\nimport pandas as pd\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.utils as ae_utils\nimport analysis_engine.dataset_scrub_utils as dataset_utils\nimport analysis_engine.iex.consts as iex_consts\nimport analysis_engine.iex.helpers_for_iex_api as iex_helpers\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef fetch_daily(\n        ticker=None,\n        work_dict=None,\n        scrub_mode=\'sort-by-date\',\n        verbose=False):\n    """"""fetch_daily\n\n    Fetch the IEX daily data for a ticker and\n    return it as a ``pandas.DataFrame``.\n\n    https://iexcloud.io/docs/api/#historical-prices\n\n    .. code-block:: python\n\n        import analysis_engine.iex.fetch_api as iex_fetch\n\n        daily_df = iex_fetch.fetch_daily(ticker=\'SPY\')\n        print(daily_df)\n\n    :param ticker: string ticker to fetch\n    :param work_dict: dictionary of args\n        used by the automation\n    :param scrub_mode: optional - string\n        type of scrubbing handler to run\n    :param verbose: optional - bool to log for debugging\n    """"""\n    label = None\n    if work_dict:\n        if not ticker:\n            ticker = work_dict.get(\'ticker\', None)\n        label = work_dict.get(\'label\', None)\n\n    use_url = (\n        f\'/stock/{ticker}/chart/1m\')\n\n    if verbose:\n        log.info(\n            f\'{label} - daily - url={use_url} \'\n            f\'req={work_dict} \'\n            f\'ticker={ticker}\')\n\n    resp_json = iex_helpers.get_from_iex(\n        url=use_url,\n        token=iex_consts.IEX_TOKEN,\n        verbose=verbose)\n\n    df = pd.DataFrame(resp_json)\n\n    if verbose:\n        log.info(\n            f\'{label} - daily - url={use_url} \'\n            f\'ticker={ticker} response \'\n            f\'df={df.tail(5)}\')\n\n    if len(df.index) == 0:\n        return df\n\n    iex_helpers.convert_datetime_columns(\n        df=df)\n\n    cols_to_drop = [\n    ]\n\n    remove_these = None\n    for c in df:\n        if c in cols_to_drop:\n            if not remove_these:\n                remove_these = []\n            remove_these.append(c)\n\n    if remove_these:\n        df = df.drop(columns=remove_these)\n\n    df.set_index(\n        [\n            \'date\'\n        ]).sort_values(by=[\'date\'], ascending=True)\n\n    return df\n# end of fetch_daily\n\n\ndef fetch_minute(\n        ticker=None,\n        backfill_date=None,\n        work_dict=None,\n        scrub_mode=\'sort-by-date\',\n        verbose=False):\n    """"""fetch_minute\n\n    Fetch the IEX minute intraday data for a ticker and\n    return it as a ``pandas.DataFrame``.\n\n    https://iexcloud.io/docs/api/#historical-prices\n\n    .. code-block:: python\n\n        import analysis_engine.iex.fetch_api as iex_fetch\n\n        minute_df = iex_fetch.fetch_minute(ticker=\'SPY\')\n        print(minute_df)\n\n    :param ticker: string ticker to fetch\n    :param backfill_date: optional - date string formatted\n        ``YYYY-MM-DD`` for filling in missing minute data\n    :param work_dict: dictionary of args\n        used by the automation\n    :param scrub_mode: optional - string\n        type of scrubbing handler to run\n    :param verbose: optional - bool to log for debugging\n    """"""\n    label = None\n    use_date = backfill_date\n    from_historical_date = None\n    last_close_to_use = None\n    dates = []\n\n    if work_dict:\n        label = work_dict.get(\'label\', None)\n        use_date = work_dict.get(\'use_date\', None)\n        if not ticker:\n            ticker = work_dict.get(\'ticker\', None)\n        if not backfill_date:\n            use_date = work_dict.get(\'backfill_date\', None)\n        if \'from_historical_date\' in work_dict:\n            from_historical_date = work_dict[\'from_historical_date\']\n        if \'last_close_to_use\' in work_dict:\n            last_close_to_use = work_dict[\'last_close_to_use\']\n        if from_historical_date and last_close_to_use:\n            dates = ae_utils.get_days_between_dates(\n                from_historical_date=work_dict[\'from_historical_date\'],\n                last_close_to_use=last_close_to_use)\n\n    use_url = (\n        f\'/stock/{ticker}/chart/1d\')\n\n    if use_date:\n        # no - chars in the date\n        use_url = (\n            f\'/stock/{ticker}/chart/date/{use_date.replace(""-"", """")}\')\n\n    if verbose:\n        log.info(\n            f\'{label} - minute - url={use_url} \'\n            f\'req={work_dict} ticker={ticker} \'\n            f\'fhdate={from_historical_date} \'\n            f\'last_close={last_close_to_use} \'\n            f\'dates={dates}\')\n\n    resp_json = iex_helpers.get_from_iex(\n        url=use_url,\n        token=iex_consts.IEX_TOKEN,\n        verbose=verbose)\n\n    df = pd.DataFrame(resp_json)\n\n    if verbose:\n        log.info(\n            f\'{label} - minute - url={use_url} \'\n            f\'ticker={ticker} response \'\n            f\'df={df.tail(5)}\')\n\n    if \'date\' not in df:\n        log.error(\n            f\'unable to download IEX Cloud minute \'\n            f\'data for {ticker} on backfill_date={use_date} \'\n            f\'df: {df} from url: {use_url} with response: {resp_json}\')\n        return df\n\n    if len(df.index) == 0:\n        return df\n\n    iex_helpers.convert_datetime_columns(\n        df=df)\n\n    if not use_date:\n        use_date = df[\'date\'].iloc[-1].strftime(\'%Y-%m-%d\')\n\n    new_minutes = dataset_utils.build_dates_from_df_col(\n        src_col=\'minute\',\n        use_date_str=use_date,\n        df=df)\n    df[\'date\'] = pd.to_datetime(\n        new_minutes,\n        format=ae_consts.COMMON_TICK_DATE_FORMAT)\n    # make sure dates are set as strings in the cache\n    df[\'date\'] = df[\'date\'].dt.strftime(\n        ae_consts.COMMON_TICK_DATE_FORMAT)\n    df.set_index(\n        [\n            \'date\'\n        ])\n    return df\n# end of fetch_minute\n\n\ndef fetch_quote(\n        ticker=None,\n        work_dict=None,\n        scrub_mode=\'sort-by-date\',\n        verbose=False):\n    """"""fetch_quote\n\n    Fetch the IEX quote data for a ticker and\n    return as a ``pandas.DataFrame``.\n\n    https://iexcloud.io/docs/api/#quote\n\n    .. code-block:: python\n\n        import analysis_engine.iex.fetch_api as iex_fetch\n\n        quote_df = iex_fetch.fetch_quote(ticker=\'SPY\')\n        print(quote_df)\n\n    :param ticker: string ticker to fetch\n    :param work_dict: dictionary of args\n        used by the automation\n    :param scrub_mode: optional - string\n        type of scrubbing handler to run\n    :param verbose: optional - bool to log for debugging\n    """"""\n    label = None\n    if work_dict:\n        if not ticker:\n            ticker = work_dict.get(\'ticker\', None)\n        label = work_dict.get(\'label\', None)\n\n    use_url = (\n        f\'/stock/{ticker}/quote\')\n\n    if verbose:\n        log.info(\n            f\'{label} - quote - url={use_url} \'\n            f\'req={work_dict} ticker={ticker}\')\n\n    resp_json = iex_helpers.get_from_iex(\n        url=use_url,\n        token=iex_consts.IEX_TOKEN,\n        verbose=verbose)\n\n    df = pd.DataFrame([resp_json])\n\n    if verbose:\n        log.info(\n            f\'{label} - quote - url={use_url} \'\n            f\'ticker={ticker} response \'\n            f\'df={df.tail(5)}\')\n\n    if len(df.index) == 0:\n        return df\n\n    iex_helpers.convert_datetime_columns(\n        df=df)\n\n    cols_to_drop = []\n    remove_these = None\n    if len(cols_to_drop) > 0:\n        for c in df:\n            if c in cols_to_drop:\n                if not remove_these:\n                    remove_these = []\n                remove_these.append(c)\n\n    if remove_these:\n        df = df.drop(columns=remove_these)\n\n    return df\n# end of fetch_quote\n\n\ndef fetch_stats(\n        ticker=None,\n        work_dict=None,\n        scrub_mode=\'sort-by-date\',\n        verbose=False):\n    """"""fetch_stats\n\n    Fetch the IEX statistics data for a ticker and\n    return it as a ``pandas.DataFrame``.\n\n    https://iexcloud.io/docs/api/#key-stats\n\n    .. code-block:: python\n\n        import analysis_engine.iex.fetch_api as iex_fetch\n\n        stats_df = iex_fetch.fetch_stats(ticker=\'SPY\')\n        print(stats_df)\n\n    :param ticker: string ticker to fetch\n    :param work_dict: dictionary of args\n        used by the automation\n    :param scrub_mode: optional - string\n        type of scrubbing handler to run\n    :param verbose: optional - bool to log for debugging\n    """"""\n    label = None\n    if work_dict:\n        if not ticker:\n            ticker = work_dict.get(\'ticker\', None)\n        label = work_dict.get(\'label\', None)\n\n    use_url = (\n        f\'/stock/{ticker}/stats\')\n\n    if verbose:\n        log.info(\n            f\'{label} - stats - url={use_url} \'\n            f\'req={work_dict} ticker={ticker}\')\n\n    resp_json = iex_helpers.get_from_iex(\n        url=use_url,\n        token=iex_consts.IEX_TOKEN,\n        verbose=verbose)\n\n    df = pd.DataFrame([resp_json])\n\n    if verbose:\n        log.info(\n            f\'{label} - stats - url={use_url} \'\n            f\'ticker={ticker} response \'\n            f\'df={df.tail(5)}\')\n\n    if len(df.index) == 0:\n        return df\n\n    iex_helpers.convert_datetime_columns(\n        df=df)\n\n    cols_to_drop = []\n    remove_these = None\n    if len(cols_to_drop) > 0:\n        for c in df:\n            if c in cols_to_drop:\n                if not remove_these:\n                    remove_these = []\n                remove_these.append(c)\n\n    if remove_these:\n        df = df.drop(columns=remove_these)\n\n    return df\n# end of fetch_stats\n\n\ndef fetch_peers(\n        ticker=None,\n        work_dict=None,\n        scrub_mode=\'sort-by-date\',\n        verbose=False):\n    """"""fetch_peers\n\n    Fetch the IEX peers data for a ticker and\n    return it as a ``pandas.DataFrame``.\n\n    https://iexcloud.io/docs/api/#peers\n\n    .. code-block:: python\n\n        import analysis_engine.iex.fetch_api as iex_fetch\n\n        peers_df = iex_fetch.fetch_peers(ticker=\'SPY\')\n        print(peers_df)\n\n    :param ticker: string ticker to fetch\n    :param work_dict: dictionary of args\n        used by the automation\n    :param scrub_mode: optional - string\n        type of scrubbing handler to run\n    :param verbose: optional - bool to log for debugging\n    """"""\n    label = None\n    if work_dict:\n        if not ticker:\n            ticker = work_dict.get(\'ticker\', None)\n        label = work_dict.get(\'label\', None)\n\n    use_url = (\n        f\'/stock/{ticker}/relevant\')\n\n    if verbose:\n        log.info(\n            f\'{label} - peers - url={use_url} \'\n            f\'req={work_dict} ticker={ticker}\')\n\n    resp_json = iex_helpers.get_from_iex(\n        url=use_url,\n        token=iex_consts.IEX_TOKEN,\n        verbose=verbose)\n\n    df = pd.DataFrame(resp_json)\n\n    if verbose:\n        log.info(\n            f\'{label} - peers - url={use_url} \'\n            f\'ticker={ticker} response \'\n            f\'df={df.tail(5)}\')\n\n    if len(df.index) == 0:\n        return df\n\n    iex_helpers.convert_datetime_columns(\n        df=df)\n\n    cols_to_drop = []\n    remove_these = None\n    if len(cols_to_drop) > 0:\n        for c in df:\n            if c in cols_to_drop:\n                if not remove_these:\n                    remove_these = []\n                remove_these.append(c)\n\n    if remove_these:\n        df = df.drop(columns=remove_these)\n\n    return df\n# end of fetch_peers\n\n\ndef fetch_news(\n        ticker=None,\n        num_news=5,\n        work_dict=None,\n        scrub_mode=\'sort-by-date\',\n        verbose=False):\n    """"""fetch_news\n\n    Fetch the IEX news data for a ticker and\n    return it as a ``pandas.DataFrame``.\n\n    https://iexcloud.io/docs/api/#news\n\n    .. code-block:: python\n\n        import analysis_engine.iex.fetch_api as iex_fetch\n\n        news_df = iex_fetch.fetch_news(ticker=\'SPY\')\n        print(news_df)\n\n    :param ticker: string ticker to fetch\n    :param num_news: optional - int number of news\n        articles to fetch\n        (default is ``5`` articles)\n    :param work_dict: dictionary of args\n        used by the automation\n    :param scrub_mode: optional - string\n        type of scrubbing handler to run\n    :param verbose: optional - bool to log for debugging\n    """"""\n    label = None\n    if work_dict:\n        if not ticker:\n            ticker = work_dict.get(\'ticker\', None)\n        label = work_dict.get(\'label\', None)\n        if not num_news:\n            num_news = int(work_dict.get(\'num_news\', 5))\n\n    use_url = (\n        f\'/stock/{ticker}/news/last/{num_news}\')\n\n    if verbose:\n        log.info(\n            f\'{label} - news - url={use_url} \'\n            f\'req={work_dict} ticker={ticker}\')\n\n    resp_json = iex_helpers.get_from_iex(\n        url=use_url,\n        token=iex_consts.IEX_TOKEN,\n        verbose=verbose)\n\n    df = pd.DataFrame(resp_json)\n\n    if verbose:\n        log.info(\n            f\'{label} - news - url={use_url} \'\n            f\'ticker={ticker} response \'\n            f\'df={df.tail(5)}\')\n\n    if len(df.index) == 0:\n        return df\n\n    if \'datetime\' not in df:\n        log.error(\n            f\'unable to download IEX Cloud news \'\n            f\'data for {ticker} \'\n            f\'df: {df} from url: {use_url} with response: {resp_json}\')\n        return df\n\n    df[\'datetime\'] = pd.to_datetime(\n        df[\'datetime\'],\n        unit=\'ms\',\n        errors=\'coerce\')\n\n    cols_to_drop = []\n    remove_these = None\n    if len(cols_to_drop) > 0:\n        for c in df:\n            if c in cols_to_drop:\n                if not remove_these:\n                    remove_these = []\n                remove_these.append(c)\n\n    if remove_these:\n        df = df.drop(columns=remove_these)\n\n    return df\n# end of fetch_news\n\n\ndef fetch_financials(\n        ticker=None,\n        work_dict=None,\n        scrub_mode=\'sort-by-date\',\n        verbose=False):\n    """"""fetch_financials\n\n    Fetch the IEX financial data for a ticker and\n    return it as a ``pandas.DataFrame``.\n\n    https://iexcloud.io/docs/api/#financials\n\n    .. code-block:: python\n\n        import analysis_engine.iex.fetch_api as iex_fetch\n\n        fin_df = iex_fetch.fetch_financials(ticker=\'SPY\')\n        print(fin_df)\n\n    :param ticker: string ticker to fetch\n    :param work_dict: dictionary of args\n        used by the automation\n    :param scrub_mode: optional - string\n        type of scrubbing handler to run\n    :param verbose: optional - bool to log for debugging\n    """"""\n    label = None\n    if work_dict:\n        if not ticker:\n            ticker = work_dict.get(\'ticker\', None)\n        label = work_dict.get(\'label\', None)\n\n    use_url = (\n        f\'/stock/{ticker}/financials\')\n\n    if verbose:\n        log.info(\n            f\'{label} - fins - url={use_url} \'\n            f\'req={work_dict} ticker={ticker}\')\n\n    resp_json = iex_helpers.get_from_iex(\n        url=use_url,\n        token=iex_consts.IEX_TOKEN,\n        verbose=verbose)\n\n    df = pd.DataFrame(resp_json.get(\'financials\', []))\n\n    if verbose:\n        log.info(\n            f\'{label} - fins - url={use_url} \'\n            f\'ticker={ticker} response \'\n            f\'df={df.tail(5)}\')\n\n    if len(df.index) == 0:\n        return df\n\n    iex_helpers.convert_datetime_columns(\n        df=df)\n\n    cols_to_drop = []\n    remove_these = None\n    if len(cols_to_drop) > 0:\n        for c in df:\n            if c in cols_to_drop:\n                if not remove_these:\n                    remove_these = []\n                remove_these.append(c)\n\n    if remove_these:\n        df = df.drop(columns=remove_these)\n\n    return df\n# end of fetch_financials\n\n\ndef fetch_earnings(\n        ticker=None,\n        work_dict=None,\n        scrub_mode=\'sort-by-date\',\n        verbose=False):\n    """"""fetch_earnings\n\n    Fetch the IEX earnings data for a ticker and\n    return it as a ``pandas.DataFrame``.\n\n    https://iexcloud.io/docs/api/#earnings\n\n    .. code-block:: python\n\n        import analysis_engine.iex.fetch_api as iex_fetch\n\n        earn_df = iex_fetch.fetch_earnings(ticker=\'SPY\')\n        print(earn_df)\n\n    :param ticker: string ticker to fetch\n    :param work_dict: dictionary of args\n        used by the automation\n    :param scrub_mode: optional - string\n        type of scrubbing handler to run\n    :param verbose: optional - bool to log for debugging\n    """"""\n    label = None\n    if work_dict:\n        if not ticker:\n            ticker = work_dict.get(\'ticker\', None)\n        label = work_dict.get(\'label\', None)\n\n    use_url = (\n        f\'/stock/{ticker}/earnings\')\n\n    if verbose:\n        log.info(\n            f\'{label} - earns - url={use_url} \'\n            f\'req={work_dict} ticker={ticker}\')\n\n    resp_json = iex_helpers.get_from_iex(\n        url=use_url,\n        token=iex_consts.IEX_TOKEN,\n        verbose=verbose)\n\n    df = pd.DataFrame(resp_json.get(\'earnings\', []))\n\n    if verbose:\n        log.info(\n            f\'{label} - earns - url={use_url} \'\n            f\'ticker={ticker} response \'\n            f\'df={df.tail(5)}\')\n\n    if len(df.index) == 0:\n        return df\n\n    iex_helpers.convert_datetime_columns(\n        df=df)\n\n    cols_to_drop = []\n    remove_these = None\n    if len(cols_to_drop) > 0:\n        for c in df:\n            if c in cols_to_drop:\n                if not remove_these:\n                    remove_these = []\n                remove_these.append(c)\n\n    if remove_these:\n        df = df.drop(columns=remove_these)\n\n    return df\n# end of fetch_earnings\n\n\ndef fetch_dividends(\n        ticker=None,\n        timeframe=\'3m\',\n        work_dict=None,\n        scrub_mode=\'sort-by-date\',\n        verbose=False):\n    """"""fetch_dividends\n\n    Fetch the IEX dividends data for a ticker and\n    return it as a ``pandas.DataFrame``.\n\n    https://iexcloud.io/docs/api/#dividends\n\n    .. code-block:: python\n\n        import analysis_engine.iex.fetch_api as iex_fetch\n\n        div_df = iex_fetch.fetch_dividends(ticker=\'SPY\')\n        print(div_df)\n\n    :param ticker: string ticker to fetch\n    :param timeframe: optional - string for setting\n        dividend lookback period used for\n        (default is ``3m`` for three months)\n    :param work_dict: dictionary of args\n        used by the automation\n    :param scrub_mode: optional - string\n        type of scrubbing handler to run\n    :param verbose: optional - bool to log for debugging\n    """"""\n    label = None\n    if work_dict:\n        if not ticker:\n            ticker = work_dict.get(\'ticker\', None)\n        label = work_dict.get(\'label\', None)\n        if not timeframe:\n            timeframe = work_dict.get(\'timeframe\', \'3m\')\n\n    use_url = (\n        f\'/stock/{ticker}/dividends/{timeframe}\')\n\n    if verbose:\n        log.info(\n            f\'{label} - divs - url={use_url} \'\n            f\'req={work_dict} ticker={ticker}\')\n\n    resp_json = iex_helpers.get_from_iex(\n        url=use_url,\n        token=iex_consts.IEX_TOKEN,\n        verbose=verbose)\n\n    df = pd.DataFrame(resp_json)\n\n    if verbose:\n        log.info(\n            f\'{label} - divs - url={use_url} \'\n            f\'ticker={ticker} response \'\n            f\'df={df.tail(5)}\')\n\n    if len(df.index) == 0:\n        return df\n\n    iex_helpers.convert_datetime_columns(\n        df=df)\n\n    cols_to_drop = []\n    remove_these = None\n    if len(cols_to_drop) > 0:\n        for c in df:\n            if c in cols_to_drop:\n                if not remove_these:\n                    remove_these = []\n                remove_these.append(c)\n\n    if remove_these:\n        df = df.drop(columns=remove_these)\n\n    return df\n# end of fetch_dividends\n\n\ndef fetch_company(\n        ticker=None,\n        work_dict=None,\n        scrub_mode=\'NO_SORT\',\n        verbose=False):\n    """"""fetch_company\n\n    Fetch the IEX company data for a ticker and\n    return it as a ``pandas.DataFrame``.\n\n    https://iexcloud.io/docs/api/#company\n\n    .. code-block:: python\n\n        import analysis_engine.iex.fetch_api as iex_fetch\n\n        comp_df = iex_fetch.fetch_company(ticker=\'SPY\')\n        print(comp_df)\n\n    :param ticker: string ticker to fetch\n    :param work_dict: dictionary of args\n        used by the automation\n    :param scrub_mode: optional - string\n        type of scrubbing handler to run\n    :param verbose: optional - bool to log for debugging\n    """"""\n    label = None\n    if work_dict:\n        if not ticker:\n            ticker = work_dict.get(\'ticker\', None)\n        label = work_dict.get(\'label\', None)\n\n    use_url = (\n        f\'/stock/{ticker}/company\')\n\n    if verbose:\n        log.info(\n            f\'{label} - comp - url={use_url} \'\n            f\'req={work_dict} ticker={ticker}\')\n\n    resp_json = iex_helpers.get_from_iex(\n        url=use_url,\n        token=iex_consts.IEX_TOKEN,\n        verbose=verbose)\n\n    df = pd.DataFrame([resp_json])\n\n    if verbose:\n        log.info(\n            f\'{label} - comp - url={use_url} \'\n            f\'ticker={ticker} response \'\n            f\'df={df}\')\n\n    if len(df.index) == 0:\n        return df\n\n    iex_helpers.convert_datetime_columns(\n        df=df)\n\n    cols_to_drop = []\n    remove_these = None\n    if len(cols_to_drop) > 0:\n        for c in df:\n            if c in cols_to_drop:\n                if not remove_these:\n                    remove_these = []\n                remove_these.append(c)\n\n    if remove_these:\n        df = df.drop(columns=remove_these)\n\n    return df\n# end of fetch_company\n'"
analysis_engine/iex/fetch_data.py,0,"b'""""""\nFetch data from\n`IEX <https://iexcloud.io/>`__\nwith the factory method ``fetch_data``\n""""""\n\nimport analysis_engine.iex.consts as iex_consts\nimport analysis_engine.iex.fetch_api as fetch_api\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef fetch_data(\n        work_dict,\n        fetch_type=None,\n        verbose=False):\n    """"""fetch_data\n\n    Factory method for fetching data from\n    IEX using an enum or string alias. Returns\n    a pandas ``DataFrame`` and only supports\n    one ticker at a time.\n\n    Supported enums from: ``analysis_engine.iex.consts``\n\n    ::\n\n        fetch_type = iex_consts.FETCH_DAILY\n        fetch_type = iex_consts.FETCH_MINUTE\n        fetch_type = iex_consts.FETCH_QUOTE\n        fetch_type = iex_consts.FETCH_STATS\n        fetch_type = iex_consts.FETCH_PEERS\n        fetch_type = iex_consts.FETCH_NEWS\n        fetch_type = iex_consts.FETCH_FINANCIALS\n        fetch_type = iex_consts.FETCH_EARNINGS\n        fetch_type = iex_consts.FETCH_DIVIDENDS\n        fetch_type = iex_consts.FETCH_COMPANY\n\n    Supported ``work_dict[\'ft_type\']`` string values:\n\n    ::\n\n        work_dict[\'ft_type\'] = \'daily\'\n        work_dict[\'ft_type\'] = \'minute\'\n        work_dict[\'ft_type\'] = \'quote\'\n        work_dict[\'ft_type\'] = \'stats\'\n        work_dict[\'ft_type\'] = \'peers\'\n        work_dict[\'ft_type\'] = \'news\'\n        work_dict[\'ft_type\'] = \'financials\'\n        work_dict[\'ft_type\'] = \'earnings\'\n        work_dict[\'ft_type\'] = \'dividends\'\n        work_dict[\'ft_type\'] = \'company\'\n\n    :param work_dict: dictionary of args for the pEX call\n    :param fetch_type: optional - name or enum of the fetcher to create\n                       can also be a lower case string\n                       in work_dict[\'ft_type\']\n    :param verbose: optional - boolean enable debug logging\n    """"""\n    use_fetch_name = None\n    if not fetch_type:\n        fetch_type = work_dict.get(\n            \'ft_type\',\n            None)\n    if fetch_type:\n        use_fetch_name = str(fetch_type).lower()\n\n    log.debug(\n        f\'name={use_fetch_name} \'\n        f\'type={fetch_type} args={work_dict}\')\n\n    if (\n            use_fetch_name == \'daily\' or\n            fetch_type == iex_consts.FETCH_DAILY):\n        return fetch_api.fetch_daily(\n            work_dict=work_dict,\n            verbose=verbose)\n    elif (\n            use_fetch_name == \'minute\' or\n            fetch_type == iex_consts.FETCH_MINUTE):\n        return fetch_api.fetch_minute(\n            work_dict=work_dict,\n            verbose=verbose)\n    elif (\n            use_fetch_name == \'quote\' or\n            fetch_type == iex_consts.FETCH_QUOTE):\n        return fetch_api.fetch_quote(\n            work_dict=work_dict,\n            verbose=verbose)\n    elif (\n            use_fetch_name == \'stats\' or\n            fetch_type == iex_consts.FETCH_STATS):\n        return fetch_api.fetch_stats(\n            work_dict=work_dict,\n            verbose=verbose)\n    elif (\n            use_fetch_name == \'peers\' or\n            fetch_type == iex_consts.FETCH_PEERS):\n        return fetch_api.fetch_peers(\n            work_dict=work_dict,\n            verbose=verbose)\n    elif (\n            use_fetch_name == \'news\' or\n            fetch_type == iex_consts.FETCH_NEWS):\n        return fetch_api.fetch_news(\n            work_dict=work_dict,\n            verbose=verbose)\n    elif (\n            use_fetch_name == \'financials\' or\n            fetch_type == iex_consts.FETCH_FINANCIALS):\n        return fetch_api.fetch_financials(\n            work_dict=work_dict,\n            verbose=verbose)\n    elif (\n            use_fetch_name == \'earnings\' or\n            fetch_type == iex_consts.FETCH_EARNINGS):\n        return fetch_api.fetch_earnings(\n            work_dict=work_dict,\n            verbose=verbose)\n    elif (\n            use_fetch_name == \'dividends\' or\n            fetch_type == iex_consts.FETCH_DIVIDENDS):\n        return fetch_api.fetch_dividends(\n            work_dict=work_dict,\n            verbose=verbose)\n    elif (\n            use_fetch_name == \'company\' or\n            fetch_type == iex_consts.FETCH_COMPANY):\n        return fetch_api.fetch_company(\n            work_dict=work_dict,\n            verbose=verbose)\n    else:\n        log.error(\n            f\'label={work_dict.get(""label"", None)} - \'\n            f\'unsupported fetch_data(\'\n            f\'work_dict={work_dict}, \'\n            f\'fetch_type={fetch_type}\'\n            f\')\')\n        raise NotImplementedError\n    # end of supported fetchers\n# end of fetch_data\n'"
analysis_engine/iex/get_data.py,0,"b'""""""\nCommon Fetch for any supported Get from IEX using HTTP\n\nSupported environment variables:\n\n::\n\n    # debug the fetch routines with:\n    export DEBUG_IEX_DATA=1\n\n""""""\n\nimport os\nimport datetime\nimport copy\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.iex.consts as iex_consts\nimport analysis_engine.build_result as build_result\nimport analysis_engine.api_requests as api_requests\nimport analysis_engine.iex.fetch_data as iex_fetch_data\nimport analysis_engine.work_tasks.publish_pricing_update as publisher\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef get_data_from_iex(\n        work_dict):\n    """"""get_data_from_iex\n\n    Get data from IEX - this requires an account\n\n    :param work_dict: request dictionary\n    """"""\n    label = \'get_data_from_iex\'\n\n    log.debug(\n        f\'task - {label} - start \'\n        f\'work_dict={work_dict}\')\n\n    rec = {\n        \'data\': None,\n        \'updated\': None\n    }\n    res = {\n        \'status\': ae_consts.NOT_RUN,\n        \'err\': None,\n        \'rec\': rec\n    }\n\n    ticker = None\n    field = None\n    ft_type = None\n\n    try:\n\n        ticker = work_dict.get(\'ticker\', ae_consts.TICKER)\n        field = work_dict.get(\'field\', \'daily\')\n        ft_type = work_dict.get(\'ft_type\', None)\n        ft_str = str(ft_type).lower()\n        label = work_dict.get(\'label\', label)\n        orient = work_dict.get(\'orient\', \'records\')\n        backfill_date = work_dict.get(\'backfill_date\', None)\n        verbose = work_dict.get(\'verbose\', False)\n\n        iex_req = None\n        if ft_type == iex_consts.FETCH_DAILY or ft_str == \'daily\':\n            ft_type == iex_consts.FETCH_DAILY\n            iex_req = api_requests.build_iex_fetch_daily_request(\n                label=label)\n        elif ft_type == iex_consts.FETCH_MINUTE or ft_str == \'minute\':\n            ft_type == iex_consts.FETCH_MINUTE\n            iex_req = api_requests.build_iex_fetch_minute_request(\n                label=label)\n        elif ft_type == iex_consts.FETCH_QUOTE or ft_str == \'quote\':\n            ft_type == iex_consts.FETCH_QUOTE\n            iex_req = api_requests.build_iex_fetch_quote_request(\n                label=label)\n        elif ft_type == iex_consts.FETCH_STATS or ft_str == \'stats\':\n            ft_type == iex_consts.FETCH_STATS\n            iex_req = api_requests.build_iex_fetch_stats_request(\n                label=label)\n        elif ft_type == iex_consts.FETCH_PEERS or ft_str == \'peers\':\n            ft_type == iex_consts.FETCH_PEERS\n            iex_req = api_requests.build_iex_fetch_peers_request(\n                label=label)\n        elif ft_type == iex_consts.FETCH_NEWS or ft_str == \'news\':\n            ft_type == iex_consts.FETCH_NEWS\n            iex_req = api_requests.build_iex_fetch_news_request(\n                label=label)\n        elif ft_type == iex_consts.FETCH_FINANCIALS or ft_str == \'financials\':\n            ft_type == iex_consts.FETCH_FINANCIALS\n            iex_req = api_requests.build_iex_fetch_financials_request(\n                label=label)\n        elif ft_type == iex_consts.FETCH_EARNINGS or ft_str == \'earnings\':\n            ft_type == iex_consts.FETCH_EARNINGS\n            iex_req = api_requests.build_iex_fetch_earnings_request(\n                label=label)\n        elif ft_type == iex_consts.FETCH_DIVIDENDS or ft_str == \'dividends\':\n            ft_type == iex_consts.FETCH_DIVIDENDS\n            iex_req = api_requests.build_iex_fetch_dividends_request(\n                label=label)\n        elif ft_type == iex_consts.FETCH_COMPANY or ft_str == \'company\':\n            ft_type == iex_consts.FETCH_COMPANY\n            iex_req = api_requests.build_iex_fetch_company_request(\n                label=label)\n        else:\n            log.error(\n                f\'{label} - unsupported ft_type={ft_type} \'\n                f\'ft_str={ft_str} ticker={ticker}\')\n            raise NotImplementedError\n        # if supported fetch request type\n\n        iex_req[\'ticker\'] = ticker\n        clone_keys = [\n            \'ticker\',\n            \'s3_address\',\n            \'s3_bucket\',\n            \'s3_key\',\n            \'redis_address\',\n            \'redis_db\',\n            \'redis_password\',\n            \'redis_key\'\n        ]\n\n        for k in clone_keys:\n            if k in iex_req:\n                iex_req[k] = work_dict.get(k, f\'{k}-missing-in-{label}\')\n        # end of cloning keys\n\n        if not iex_req:\n            err = (\n                f\'{label} - ticker={ticker} \'\n                f\'did not build an IEX request \'\n                f\'for work={work_dict}\')\n            log.error(err)\n            res = build_result.build_result(\n                status=ae_consts.ERR,\n                err=err,\n                rec=rec)\n            return res\n        else:\n            log.debug(\n                f\'{label} - ticker={ticker} \'\n                f\'field={field} \'\n                f\'orient={orient} fetch\')\n        # if invalid iex request\n\n        df = None\n        try:\n            if \'from\' in work_dict:\n                iex_req[\'from\'] = datetime.datetime.strptime(\n                    \'%Y-%m-%d %H:%M:%S\',\n                    work_dict[\'from\'])\n            if backfill_date:\n                iex_req[\'backfill_date\'] = backfill_date\n                iex_req[\'redis_key\'] = (\n                    f\'{ticker}_{backfill_date}_{field}\')\n                iex_req[\'s3_key\'] = (\n                    f\'{ticker}_{backfill_date}_{field}\')\n\n            if os.getenv(\'SHOW_SUCCESS\', \'0\') == \'1\':\n                log.info(\n                    f\'fetching IEX {field} req={iex_req}\')\n            else:\n                log.debug(\n                    f\'fetching IEX {field} req={iex_req}\')\n\n            df = iex_fetch_data.fetch_data(\n                work_dict=iex_req,\n                fetch_type=ft_type,\n                verbose=verbose)\n            rec[\'data\'] = df.to_json(\n                orient=orient,\n                date_format=\'iso\')\n            rec[\'updated\'] = datetime.datetime.utcnow().strftime(\n                \'%Y-%m-%d %H:%M:%S\')\n        except Exception as f:\n            log.error(\n                f\'{label} - ticker={ticker} field={ft_type} \'\n                f\'failed fetch_data \'\n                f\'with ex={f}\')\n        # end of try/ex\n\n        if ae_consts.ev(\'DEBUG_IEX_DATA\', \'0\') == \'1\':\n            log.debug(\n                f\'{label} ticker={ticker} \'\n                f\'field={field} data={rec[""data""]} to_json\')\n        else:\n            log.debug(\n                f\'{label} ticker={ticker} field={field} to_json\')\n        # end of if/else found data\n\n        upload_and_cache_req = copy.deepcopy(iex_req)\n        upload_and_cache_req[\'celery_disabled\'] = True\n        upload_and_cache_req[\'data\'] = rec[\'data\']\n        if not upload_and_cache_req[\'data\']:\n            upload_and_cache_req[\'data\'] = \'{}\'\n        use_field = field\n        if use_field == \'news\':\n            use_field = \'news1\'\n        if \'redis_key\' in work_dict:\n            rk = work_dict.get(\'redis_key\', iex_req[\'redis_key\'])\n            if backfill_date:\n                rk = f\'{ticker}_{backfill_date}\'\n            upload_and_cache_req[\'redis_key\'] = (\n                f\'{rk}_{use_field}\')\n        if \'s3_key\' in work_dict:\n            sk = work_dict.get(\'s3_key\', iex_req[\'s3_key\'])\n            if backfill_date:\n                sk = f\'{ticker}_{backfill_date}\'\n            upload_and_cache_req[\'s3_key\'] = (\n                f\'{sk}_{use_field}\')\n\n        try:\n            update_res = publisher.run_publish_pricing_update(\n                work_dict=upload_and_cache_req)\n            update_status = update_res.get(\n                \'status\',\n                ae_consts.NOT_SET)\n            log.debug(\n                f\'{label} publish update \'\n                f\'status={ae_consts.get_status(status=update_status)} \'\n                f\'data={update_res}\')\n        except Exception:\n            err = (\n                f\'{label} - failed to upload iex \'\n                f\'data={upload_and_cache_req} to \'\n                f\'to s3_key={upload_and_cache_req[""s3_key""]} \'\n                f\'and redis_key={upload_and_cache_req[""redis_key""]}\')\n            log.error(err)\n        # end of try/ex to upload and cache\n\n        if not rec[\'data\']:\n            log.debug(\n                f\'{label} - ticker={ticker} no IEX data \'\n                f\'field={field} to publish\')\n        # end of if/else\n\n        res = build_result.build_result(\n            status=ae_consts.SUCCESS,\n            err=None,\n            rec=rec)\n\n    except Exception as e:\n        res = build_result.build_result(\n            status=ae_consts.ERR,\n            err=(\n                f\'failed - get_data_from_iex \'\n                f\'dict={work_dict} with ex={e}\'),\n            rec=rec)\n    # end of try/ex\n\n    log.debug(\n        f\'task - get_data_from_iex done - \'\n        f\'{label} - \'\n        f\'status={ae_consts.get_status(res[""status""])} \'\n        f\'err={res[""err""]}\')\n\n    return res\n# end of get_data_from_iex\n'"
analysis_engine/iex/get_default_fields.py,0,"b'""""""\nGet defaults fields for supported teyps of data\n""""""\n\nimport analysis_engine.iex.consts as iex_consts\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef get_default_fields(\n            field):\n    """"""get_default_fields\n\n    :param field: types of data to get\n    """"""\n    use_field = str(field).lower()\n    if use_field == \'daily\' or field == iex_consts.DATAFEED_DAILY:\n        return [\'KEY\', \'date\']\n    elif use_field == \'quote\' or field == iex_consts.DATAFEED_QUOTE:\n        return [\'KEY\', \'date\', \'minute\']\n    elif use_field == \'stats\' or field == iex_consts.DATAFEED_STATS:\n        return [\'KEY\']\n    elif use_field == \'peers\' or field == iex_consts.DATAFEED_PEERS:\n        return [\'KEY\', \'peer\']\n    elif use_field == \'news\' or field == iex_consts.DATAFEED_NEWS:\n        return [\'KEY\', \'datetime\']\n    elif use_field == \'financials\' or field == iex_consts.DATAFEED_FINANCIALS:\n        return [\'KEY\', \'reportDate\']\n    elif use_field == \'earnings\' or field == iex_consts.DATAFEED_EARNINGS:\n        return [\'KEY\', \'EPSReportDate\']\n    elif use_field == \'dividends\' or field == iex_consts.DATAFEED_DIVIDENDS:\n        return [\'KEY\', \'exDate\']\n    elif use_field == \'company\' or field == iex_consts.DATAFEED_COMPANY:\n        return [\'KEY\']\n    else:\n        log.error(\n            f\'get_default_fields({field}) is not a supported \'\n            \'field\')\n        raise NotImplementedError\n# end of get_default_fields\n'"
analysis_engine/iex/get_pricing_on_date.py,0,"b'""""""\nGet latest pricing from cached IEX pricing data\n""""""\n\nimport copy\nimport json\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.utils as ae_utils\nimport analysis_engine.api_requests as api_requests\nimport analysis_engine.iex.consts as iex_consts\nimport analysis_engine.iex.extract_df_from_redis as iex_extract_utils\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef get_pricing_on_date(\n        ticker,\n        date_str=None,\n        label=None):\n    """"""get_pricing_on_date\n\n    Get the latest pricing data from the\n    cached IEX data in redis. Use this to\n    keep costs down!\n\n    .. code-block:: python\n\n        import analysis_engine.iex.get_pricing_on_date as iex_cache\n        print(iex_cache.get_pricing_on_date(\'SPY\'))\n        print(iex_cache.get_pricing_on_date(\n            ticker=\'SPY\',\n            date_str=\'2019-02-07\'))\n\n    :param ticker: ticker string\n    :param date_str: optional - string date\n        to pull data from redis. if ``None`` use\n        today\'s date. format is\n        ``ae_consts.COMMON_TICK_DATE_FORMAT``\n    :param label: log label from tracking\n    """"""\n\n    ret_dict = {\n        \'status\': ae_consts.NOT_SET,\n        \'pricing_type\': None,\n        \'high\': None,\n        \'low\': None,\n        \'open\': None,\n        \'close\': None,\n        \'volume\': None,\n        \'date\': None,\n        \'minute\': None,\n        \'average\': None,\n        \'changeOverTime\': None,\n        \'label\': None,\n        \'marketAverage\': None,\n        \'marketChangeOverTime\': None,\n        \'marketClose\': None,\n        \'marketHigh\': None,\n        \'marketLow\': None,\n        \'marketNotional\': None,\n        \'marketNumberOfTrades\': None,\n        \'marketOpen\': None,\n        \'marketVolume\': None,\n        \'notional\': None,\n        \'numberOfTrades\': None\n    }\n\n    use_date_str = None\n    if date_str:\n        use_date_str = (\n            f\'{ticker}_{date_str}\')\n\n    all_extract_reqs = api_requests.get_ds_dict(\n        ticker=ticker,\n        base_key=use_date_str,\n        label=label)\n\n    minute_key = all_extract_reqs[\'minute\']\n    daily_key = all_extract_reqs[\'daily\']\n    base_ex_req = {\n        \'ticker\': ticker,\n        \'s3_bucket\': \'pricing\',\n        \'s3_key\': minute_key,\n        \'redis_key\': minute_key,\n        \'s3_enabled\': True,\n        \'s3_access_key\': ae_consts.S3_ACCESS_KEY,\n        \'s3_secret_key\': ae_consts.S3_SECRET_KEY,\n        \'s3_region_name\': ae_consts.S3_REGION_NAME,\n        \'s3_address\': ae_consts.S3_ADDRESS,\n        \'s3_secure\': ae_consts.S3_SECURE,\n        \'redis_address\': ae_consts.REDIS_ADDRESS,\n        \'redis_password\': ae_consts.REDIS_PASSWORD,\n        \'redis_db\': ae_consts.REDIS_DB,\n        \'redis_expire\': ae_consts.REDIS_EXPIRE,\n        \'redis_enabled\': True,\n        \'fetch_mode\': \'td\',\n        \'analysis_type\': None,\n        \'iex_datasets\': [],\n        \'debug\': False,\n        \'label\': label,\n        \'celery_disabled\': True\n    }\n    log.debug(\n        f\'{ticker} - minute={minute_key} daily={daily_key}\')\n    reqs = []\n    minute_ex_req = copy.deepcopy(base_ex_req)\n    minute_ex_req[\'ex_type\'] = iex_consts.FETCH_MINUTE\n    minute_ex_req[\'iex_datasets\'] = [\n        iex_consts.FETCH_MINUTE\n    ]\n    reqs.append(minute_ex_req)\n    daily_ex_req = copy.deepcopy(base_ex_req)\n    daily_ex_req[\'ex_type\'] = iex_consts.FETCH_DAILY\n    daily_ex_req[\'s3_key\'] = daily_key\n    daily_ex_req[\'redis_key\'] = daily_key\n    daily_ex_req[\'iex_datasets\'] = [\n        iex_consts.FETCH_DAILY\n    ]\n    reqs.append(daily_ex_req)\n    try:\n        for ex_req in reqs:\n            iex_status = ae_consts.FAILED\n            iex_df = None\n            if ex_req[\'ex_type\'] == iex_consts.FETCH_MINUTE:\n                iex_status, iex_df = \\\n                    iex_extract_utils.extract_minute_dataset(\n                        work_dict=ex_req)\n            else:\n                iex_status, iex_df = \\\n                    iex_extract_utils.extract_daily_dataset(\n                        work_dict=ex_req)\n            # end of extracting\n\n            if ae_consts.is_df(df=iex_df):\n                if \'date\' in iex_df:\n                    iex_df.sort_values(\n                        by=[\n                            \'date\'\n                        ],\n                        ascending=True)\n                    ret_dict = json.loads(iex_df.iloc[-1].to_json())\n                    if \'date\' in ret_dict:\n                        try:\n                            ret_dict[\'date\'] = ae_utils.epoch_to_dt(\n                                epoch=int(ret_dict[\'date\']/1000),\n                                use_utc=False,\n                                convert_to_est=False).strftime(\n                                    ae_consts.COMMON_TICK_DATE_FORMAT)\n\n                        except Exception as f:\n                            log.critical(\n                                f\'failed converting {ret_dict} date to str \'\n                                f\'with ex={f}\')\n                    if ex_req[\'ex_type\'] == iex_consts.FETCH_MINUTE:\n                        ret_dict[\'pricing_type\'] = \'minute\'\n                        ret_dict[\'minute\'] = ret_dict.get(\n                            \'date\',\n                            None)\n                    else:\n                        ret_dict[\'pricing_type\'] = \'daily\'\n                    ret_dict[\'status\'] = iex_status\n                    return ret_dict\n            # if a valid df then return it\n    except Exception as e:\n        log.critical(\n            f\'failed to get {ticker} iex minute data with ex={e}\')\n        ret_dict[\'status\'] = ae_consts.ERR\n    # end of try/ex to get latest pricing\n\n    return ret_dict\n# end of get_pricing_on_date\n'"
analysis_engine/iex/helpers_for_iex_api.py,0,"b'""""""\nFunctions for getting data from IEX using HTTP\n\n**Debugging**\n\nPlease set the ``verbose`` argument to ``True``\nto enable debug logging with these calls\n""""""\n\ntry:\n    import urllib.parse as urlparse\nexcept ImportError:\n    import urlparse as urlparse\nimport requests\nimport pandas as pd\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.iex.consts as iex_consts\nimport analysis_engine.iex.build_auth_url as iex_auth\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef convert_datetime_columns(\n        df,\n        date_cols=None,\n        second_cols=None,\n        tcols=None,\n        ecols=None):\n    """"""convert_datetime_columns\n\n    Convert the IEX date columns in the ``df`` to ``datetime`` objects\n\n    :param df: ``pandas.DataFrame`` to set columns to\n        datetime objects\n    :param date_cols: list of columns to convert with a date string format\n        formatted: ``YYYY-MM-DD``\n    :param second_cols: list of columns to convert with a date string format\n        formatted: ``YYYY-MM-DD HH:MM:SS``\n    :param tcols: list of columns to convert with a time format\n        (this is for millisecond epoch integers)\n    :param ecols: list of columns to convert with a time format\n        (this is for nanosecond epoch integers)\n    """"""\n    date_cols = date_cols or iex_consts.IEX_DATE_FIELDS\n    second_cols = second_cols or iex_consts.IEX_SECOND_FIELDS\n    tcols = tcols or iex_consts.IEX_TIME_FIELDS\n    ecols = ecols or iex_consts.IEX_EPOCH_FIELDS\n\n    for col in date_cols:\n        if col in df:\n            df[col] = pd.to_datetime(\n                df[col],\n                format=iex_consts.IEX_DATE_FORMAT,\n                errors=\'coerce\')\n\n    for col in second_cols:\n        if col in df:\n            df[col] = pd.to_datetime(\n                df[col],\n                format=iex_consts.IEX_TICK_FORMAT,\n                errors=\'coerce\')\n\n    for tcol in tcols:\n        if tcol in df:\n            df[tcol] = pd.to_datetime(\n                df[tcol],\n                unit=\'ms\',\n                errors=\'coerce\')\n\n    for ecol in ecols:\n        if ecol in df:\n            df[ecol] = pd.to_datetime(\n                df[ecol],\n                unit=\'ns\',\n                errors=\'coerce\')\n# end of convert_datetime_columns\n\n\ndef get_from_iex_v1(\n        url,\n        verbose=False):\n    """"""get_from_iex_v1\n\n    Get data from the IEX Trading API (v1)\n    https//api.iextrading.com/1.0/\n\n    :param url: IEX V1 Resource URL\n    :param verbose: optional - bool turn on logging\n    """"""\n    url = (\n        f\'{iex_consts.IEX_URL_BASE_V1}{url}\')\n    resp = requests.get(\n        urlparse(url).geturl(),\n        proxies=iex_consts.IEX_PROXIES)\n    if resp.status_code == 200:\n        res_data = resp.json()\n        if verbose:\n            proxy_str = \'\'\n            if iex_consts.IEX_PROXIES:\n                proxy_str = (\n                    f\'proxies={iex_consts.IEX_PROXIES} \')\n            log.info(\n                f\'IEXAPI_V1 - url={url} \'\n                f\'{proxy_str}\'\n                f\'status_code={resp.status_code} \'\n                f\'data={ae_consts.ppj(res_data)}\')\n        return res_data\n    raise Exception(\n        f\'Failed to get data from IEX V1 API with \'\n        f\'function=get_from_iex_v1 \'\n        f\'url={url} which sent \'\n        f\'response {resp.status_code} - {resp.text}\')\n# end of get_from_iex_v1\n\n\ndef get_from_iex_cloud(\n        url,\n        token=None,\n        verbose=False):\n    """"""get_from_iex_cloud\n\n    Get data from IEX Cloud API (v2)\n    https://iexcloud.io\n\n    :param url: IEX resource url\n    :param token: optional - string token for your user\'s\n        account\n    :param verbose: optional - bool turn on logging\n    """"""\n    url = (\n        f\'{iex_consts.IEX_URL_BASE}{url}\')\n    resp = requests.get(\n        url,\n        proxies=iex_consts.IEX_PROXIES)\n    if resp.status_code == requests.codes.OK:\n        res_data = resp.json()\n        if verbose:\n            proxy_str = \'\'\n            if iex_consts.IEX_PROXIES:\n                proxy_str = (\n                    f\'proxies={iex_consts.IEX_PROXIES} \')\n            log.info(\n                f\'IEXAPI - response data for \'\n                f\'url={url.replace(token, ""REDACTED"")} \'\n                f\'{proxy_str}\'\n                f\'status_code={resp.status_code} \'\n                f\'data={ae_consts.ppj(res_data)}\')\n        return res_data\n    raise Exception(\n        f\'Failed to get data from IEX Cloud with \'\n        f\'function=get_from_iex_cloud \'\n        f\'url={url} which sent \'\n        f\'response {resp.status_code} - {resp.text}\')\n# end of get_from_iex_cloud\n\n\ndef handle_get_from_iex(\n        url,\n        token=None,\n        version=None,\n        verbose=False):\n    """"""handle_get_from_iex\n\n    Implementation for getting data from the IEX\n    v2 or v1 api depending on if the ``token``\n    argument is set:\n\n    - `IEX Cloud (v2) <https://iexcloud.io>`__\n    - `IEX Trading API (v1) <https://iextrading.com/developer/docs/>`__\n\n    :param url: IEX resource url\n    :param token: optional - string token for your user\'s\n        account\n    :version: optional - string version for the IEX Cloud\n        (default is ``beta``)\n    :param verbose: optional - bool turn on logging\n    """"""\n    if token:\n        return get_from_iex_cloud(\n            url=url,\n            token=token,\n            verbose=verbose)\n    return get_from_iex_v1(\n        url=url,\n        verbose=verbose)\n# handle_get_from_iex\n\n\ndef get_from_iex(\n        url,\n        token=None,\n        version=None,\n        verbose=False):\n    """"""get_from_iex\n\n    Helper for getting data from an IEX\n    publishable API endpoint using a token\n    as a query param on the http url.\n\n    :param url: IEX resource url\n    :param token: optional - string token for your user\'s\n        account\n    :param version: optional - version string\n    :param verbose: optional - bool turn on logging\n    """"""\n\n    full_url = iex_auth.build_auth_url(\n        url=url,\n        token=token)\n    return handle_get_from_iex(\n        url=full_url,\n        token=token,\n        version=version,\n        verbose=verbose)\n# end of get_from_iex\n'"
analysis_engine/indicators/__init__.py,0,b''
analysis_engine/indicators/adx.py,0,"b'"""""" Custom Average Directional Index - ADX\nhttps://www.investopedia.com/terms/a/adx.asp\n\nMomentum\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport analysis_engine.ae_talib as ae_talib\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.indicators.base_indicator as base_indicator\n\n\nclass IndicatorADX(base_indicator.BaseIndicator):\n    """"""IndicatorADX""""""\n\n    def __init__(\n            self,\n            **kwargs):\n        """"""__init__\n\n        Custom indicator example for showing a\n        ``ADX``\n        within an algo for analyzing intraday minute datasets\n\n        Please refer to the `analysis_engine.indicators.base_indicator.Ba\n        seIndicator source code for the latest supported parameters <ht\n        tps://github.com/AlgoTraders/stock-analysis-engine\n        /blob/master/\n        analysis_engine/indicators/base_indicator.py>`__\n\n        :param kwargs: keyword arguments\n        """"""\n        super().__init__(**kwargs)\n    # end of __init__\n\n    def get_configurables(\n            self,\n            **kwargs):\n        """"""get_configurables\n\n        helper for setting up algorithm configs for this indicator\n        and programmatically set the values based off the domain\n        rules\n\n        .. code-block:: python\n\n            from analysis_engine.indicators.adx \\\n                import IndicatorADX\n            ind = IndicatorADX(config_dict={\n                    \'verbose\': True\n                }).get_configurables()\n\n        :param kwargs: keyword args dictionary\n        """"""\n        self.ind_confs = []\n\n        # common:\n        self.build_base_configurables(\n            ind_type=\'momentum\',\n            category=\'technical\',\n            uses_data=self.config.get(\n                \'uses_data\',\n                \'minute\'),\n            version=1)\n\n        # custom:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'num_points\',\n            conf_type=\'int\',\n            max_value=200,\n            min_value=2,\n            default_value=100,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            default_value=5.0,\n            inc_interval=1))\n\n        # output / reporting\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'adx_value\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'amount_to_close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'percent_value\',\n            conf_type=\'percent\',\n            is_output_only=True))\n\n        default_values_dict = {}\n        for node in self.ind_confs:\n            name = node[\'name\']\n            default_value = node.get(\n                \'default\',\n                None)\n            default_values_dict[name] = default_value\n\n        use_file = None\n        try:\n            if __file__:\n                use_file = __file__\n        except Exception:\n            use_file = None\n\n        self.starter_dict = {\n            \'name\': self.__class__.__name__.lower().replace(\n                \'indicator\',\n                \'\'),\n            \'module_path\': use_file,\n            \'category\': default_values_dict.get(\n                \'category\',\n                \'momentum\'),\n            \'type\': default_values_dict.get(\n                \'type\',\n                \'technical\'),\n            \'uses_data\': default_values_dict.get(\n                \'uses_data\',\n                \'minute\'),\n            \'verbose\': default_values_dict.get(\n                \'verbose\',\n                False)\n        }\n        self.starter_dict.update(default_values_dict)\n\n        self.lg(\n            f\'configurables={ae_consts.ppj(self.ind_confs)} for \'\n            f\'class={self.__class__.__name__} in file={use_file} \'\n            f\'starter:\\n {ae_consts.ppj(self.starter_dict)}\')\n\n        return self.ind_confs\n    # end of get_configurables\n\n    def get_starter_dict(\n            self):\n        if not self.starter_dict:\n            self.get_configurables()\n        return self.starter_dict\n    # end of get_starter_dict\n\n    def process(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""process\n\n        Derive custom indicator processing to determine buy and sell\n        conditions before placing orders. Just implement your own\n        ``process`` method.\n\n        Please refer to the TA Lib guides for details on building indicators:\n\n        - Overlap Studies\n          https://mrjbq7.github.io/ta-lib/func_groups/overlap_studies.html\n        - Momentum Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html\n        - Volume Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volume_indicators.html\n        - Volatility Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volatility_indicators.html\n        - Price Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/price_transform.html\n        - Cycle Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/cycle_indicators.html\n        - Pattern Recognition\n          https://mrjbq7.github.io/ta-lib/func_groups/pattern_recognition.html\n        - Statistic Functions\n          https://mrjbq7.github.io/ta-lib/func_groups/statistic_functions.html\n        - Math Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/math_transform.html\n        - Math Operators\n          https://mrjbq7.github.io/ta-lib/func_groups/math_operators.html\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: dictionary of ``pandas.DataFrame(s)`` to process\n        """"""\n\n        # set the algo config indicator \'uses_data\' to \'day\' or \'minute\'\n        df_status, self.use_df = self.get_subscribed_dataset(\n            dataset=dataset)\n\n        if df_status == ae_consts.EMPTY:\n            self.lg(\'process end - no data found\')\n            return\n\n        # notice the self.num_points is now a member variable\n        # because the BaseIndicator class\'s __init__\n        # converts any self.config keys into useable\n        # member variables automatically in your derived class\n        self.lg(\n            f\'process - num_points={self.num_points} \'\n            f\'df={len(self.use_df.index)}\')\n        """"""\n        real = ADX(high, low, close, timeperiod=14)\n        """"""\n        num_records = len(self.use_df.index)\n        if num_records > self.num_points:\n            cur_value = self.use_df[\'close\'].iloc[-1]\n            first_date = self.use_df[\'date\'].iloc[0]\n            end_date = self.use_df[\'date\'].iloc[-1]\n            start_row = num_records - self.num_points\n            self.use_df = self.use_df[start_row:num_records]\n            """"""\n            for idx, row in self.use_df[start_row:-1].iterrows():\n                high = row[\'high\']\n                low = row[\'low\']\n                open_val = row[\'open\']\n                close = row[\'close\']\n                row_date = row[\'date\']\n                self.lg(\n                    f\'{row_date} - high={high}, low={low}, \'\n                    f\'close={close}, period={self.num_points}\')\n            """"""\n            highs = self.use_df[\'high\'].values\n            lows = self.use_df[\'low\'].values\n            closes = self.use_df[\'close\'].values\n\n            self.adx_value = ae_consts.to_f(ae_talib.ADX(\n                high=highs,\n                low=lows,\n                close=closes,\n                timeperiod=self.num_points)[-1])\n\n            """"""\n            Determine a buy or a sell as a label\n            """"""\n\n            if cur_value <= 0:\n                self.lg(f\'invalid current_value={cur_value}\')\n                return\n\n            self.close = cur_value\n            self.amount_to_close = ae_consts.to_f(\n                cur_value - self.adx_value)\n            self.percent_value = ae_consts.to_f(\n                self.amount_to_close / cur_value * 100.0)\n\n            self.is_buy = ae_consts.INDICATOR_IGNORE\n            self.is_sell = ae_consts.INDICATOR_IGNORE\n\n            if (self.buy_above_percent != -1 and\n                    self.percent_value > self.buy_above_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n            elif (self.buy_below_percent != -1 and\n                    self.percent_value > self.buy_below_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n\n            if (self.sell_above_percent != -1 and\n                    self.percent_value > self.sell_above_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n            elif (self.sell_below_percent != -1 and\n                    self.percent_value > self.sell_below_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n\n            self.lg(\n                f\'process end - {first_date} to {end_date} \'\n                f\'buy_below={self.buy_below_percent} \'\n                f\'buy_above={self.buy_above_percent} is_buy={self.is_buy} \'\n                f\'sell_below={self.sell_below_percent} \'\n                f\'sell_above={self.sell_above_percent} is_sell={self.is_sell}\')\n        else:\n            self.lg(\'process end\')\n    # end of process\n\n    def reset_internals(\n            self):\n        """"""reset_internals""""""\n        self.is_buy = ae_consts.INDICATOR_RESET\n        self.is_sell = ae_consts.INDICATOR_RESET\n    # end of reset_internals\n\n# end of IndicatorADX\n\n\ndef get_indicator(\n        **kwargs):\n    """"""get_indicator\n\n    Make sure to define the ``get_indicator`` for your custom\n    algorithms to work as a backup with the ``sa.py`` tool...\n    Not anticipating issues, but if we do with importlib\n    this is the backup plan.\n\n    Please file an issue if you see something weird and would like\n    some help:\n    https://github.com/AlgoTraders/stock-analysis-engine/issues\n\n    :param kwargs: dictionary of keyword arguments\n    """"""\n    print(\'getting indicator\')\n    return IndicatorADX(**kwargs)\n# end of get_indicator\n'"
analysis_engine/indicators/atr.py,0,"b'""""""\nCustom Average True Range - ATR\n\nhttps://www.investopedia.com/terms/a/atr.asp\n\nVolatility\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport analysis_engine.ae_talib as ae_talib\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.indicators.base_indicator as base_indicator\n\n\nclass IndicatorATR(base_indicator.BaseIndicator):\n    """"""IndicatorATR""""""\n\n    def __init__(\n            self,\n            **kwargs):\n        """"""__init__\n\n        Custom indicator example for showing a\n        ``ATR``\n        within an algo for analyzing intraday minute datasets\n\n        Please refer to the `analysis_engine.indicators.base_indicator.Ba\n        seIndicator source code for the latest supported parameters <ht\n        tps://github.com/AlgoTraders/stock-analysis-engine\n        /blob/master/\n        analysis_engine/indicators/base_indicator.py>`__\n\n        :param kwargs: keyword arguments\n        """"""\n        super().__init__(**kwargs)\n    # end of __init__\n\n    def get_configurables(\n            self,\n            **kwargs):\n        """"""get_configurables\n\n        helper for setting up algorithm configs for this indicator\n        and programmatically set the values based off the domain\n        rules\n\n        .. code-block:: python\n\n            from analysis_engine.indicators.atr \\\n                import IndicatorATR\n            ind = IndicatorATR(config_dict={\n                    \'verbose\': True\n                }).get_configurables()\n\n        :param kwargs: keyword args dictionary\n        """"""\n        self.ind_confs = []\n\n        # common:\n        self.build_base_configurables(\n            ind_type=\'volatility\',\n            category=\'technical\',\n            uses_data=self.config.get(\n                \'uses_data\',\n                \'minute\'),\n            version=1)\n\n        # custom:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'num_points\',\n            conf_type=\'int\',\n            max_value=200,\n            min_value=2,\n            default_value=100,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n\n        # output / reporting\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'atr_value\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'amount_to_close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'percent_value\',\n            conf_type=\'percent\',\n            is_output_only=True))\n\n        default_values_dict = {}\n        for node in self.ind_confs:\n            name = node[\'name\']\n            default_value = node.get(\n                \'default\',\n                None)\n            default_values_dict[name] = default_value\n\n        use_file = None\n        try:\n            if __file__:\n                use_file = __file__\n        except Exception:\n            use_file = None\n\n        self.starter_dict = {\n            \'name\': self.__class__.__name__.lower().replace(\n                \'indicator\',\n                \'\'),\n            \'module_path\': use_file,\n            \'category\': default_values_dict.get(\n                \'category\',\n                \'momentum\'),\n            \'type\': default_values_dict.get(\n                \'type\',\n                \'technical\'),\n            \'uses_data\': default_values_dict.get(\n                \'uses_data\',\n                \'minute\'),\n            \'verbose\': default_values_dict.get(\n                \'verbose\',\n                False)\n        }\n        self.starter_dict.update(default_values_dict)\n\n        self.lg(\n            f\'configurables={ae_consts.ppj(self.ind_confs)} for \'\n            f\'class={self.__class__.__name__} in file={use_file} \'\n            f\'starter:\\n {ae_consts.ppj(self.starter_dict)}\')\n\n        return self.ind_confs\n    # end of get_configurables\n\n    def get_starter_dict(\n            self):\n        if not self.starter_dict:\n            self.get_configurables()\n        return self.starter_dict\n    # end of get_starter_dict\n\n    def process(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""process\n\n        Derive custom indicator processing to determine buy and sell\n        conditions before placing orders. Just implement your own\n        ``process`` method.\n\n        Please refer to the TA Lib guides for details on building indicators:\n\n        - Overlap Studies\n          https://mrjbq7.github.io/ta-lib/func_groups/overlap_studies.html\n        - Momentum Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html\n        - Volume Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volume_indicators.html\n        - Volatility Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volatility_indicators.html\n        - Price Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/price_transform.html\n        - Cycle Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/cycle_indicators.html\n        - Pattern Recognition\n          https://mrjbq7.github.io/ta-lib/func_groups/pattern_recognition.html\n        - Statistic Functions\n          https://mrjbq7.github.io/ta-lib/func_groups/statistic_functions.html\n        - Math Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/math_transform.html\n        - Math Operators\n          https://mrjbq7.github.io/ta-lib/func_groups/math_operators.html\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: dictionary of ``pandas.DataFrame(s)`` to process\n        """"""\n\n        # set the algo config indicator \'uses_data\' to \'day\' or \'minute\'\n        df_status, self.use_df = self.get_subscribed_dataset(\n            dataset=dataset)\n\n        if df_status == ae_consts.EMPTY:\n            self.lg(\'process end - no data found\')\n            return\n\n        # notice the self.num_points is now a member variable\n        # because the BaseIndicator class\'s __init__\n        # converts any self.config keys into useable\n        # member variables automatically in your derived class\n        self.lg(\n            f\'process - num_points={self.num_points} \'\n            f\'df={len(self.use_df.index)}\')\n        """"""\n        real = ATR(high, low, close, timeperiod=14)\n        """"""\n        num_records = len(self.use_df.index)\n        if num_records > self.num_points:\n            cur_value = self.use_df[\'close\'].iloc[-1]\n            first_date = self.use_df[\'date\'].iloc[0]\n            end_date = self.use_df[\'date\'].iloc[-1]\n            start_row = num_records - self.num_points\n            self.use_df = self.use_df[start_row:num_records]\n            """"""\n            for idx, row in self.use_df[start_row:-1].iterrows():\n                high = row[\'high\']\n                low = row[\'low\']\n                open_val = row[\'open\']\n                close = row[\'close\']\n                row_date = row[\'date\']\n                self.lg(\n                    f\'{row_date} - high={high}, low={low}, \'\n                    f\'close={close}, period={self.num_points}\')\n            """"""\n            highs = self.use_df[\'high\'].values\n            lows = self.use_df[\'low\'].values\n            closes = self.use_df[\'close\'].values\n\n            self.atr_value = ae_consts.to_f(ae_talib.ATR(\n                high=highs,\n                low=lows,\n                close=closes,\n                timeperiod=self.num_points)[-1])\n\n            """"""\n            Determine a buy or a sell as a label\n            """"""\n\n            if cur_value <= 0:\n                self.lg(f\'invalid current_value={cur_value}\')\n                return\n\n            self.close = cur_value\n            self.amount_to_close = ae_consts.to_f(\n                cur_value - self.atr_value)\n            self.percent_value = ae_consts.to_f(\n                self.amount_to_close / cur_value * 100.0)\n\n            self.is_buy = ae_consts.INDICATOR_IGNORE\n            self.is_sell = ae_consts.INDICATOR_IGNORE\n\n            if (self.buy_above_percent != -1 and\n                    self.percent_value > self.buy_above_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n            elif (self.buy_below_percent != -1 and\n                    self.percent_value > self.buy_below_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n\n            if (self.sell_above_percent != -1 and\n                    self.percent_value > self.sell_above_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n            elif (self.sell_below_percent != -1 and\n                    self.percent_value > self.sell_below_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n\n            self.lg(\n                f\'process end - {first_date} to {end_date} \'\n                f\'buy_below={self.buy_below_percent} \'\n                f\'buy_above={self.buy_above_percent} is_buy={self.is_buy} \'\n                f\'sell_below={self.sell_below_percent} \'\n                f\'sell_above={self.sell_above_percent} is_sell={self.is_sell}\')\n        else:\n            self.lg(\'process end\')\n    # end of process\n\n    def reset_internals(\n            self):\n        """"""reset_internals""""""\n        self.is_buy = ae_consts.INDICATOR_RESET\n        self.is_sell = ae_consts.INDICATOR_RESET\n    # end of reset_internals\n\n# end of IndicatorATR\n\n\ndef get_indicator(\n        **kwargs):\n    """"""get_indicator\n\n    Make sure to define the ``get_indicator`` for your custom\n    algorithms to work as a backup with the ``sa.py`` tool...\n    Not anticipating issues, but if we do with importlib\n    this is the backup plan.\n\n    Please file an issue if you see something weird and would like\n    some help:\n    https://github.com/AlgoTraders/stock-analysis-engine/issues\n\n    :param kwargs: dictionary of keyword arguments\n    """"""\n    print(\'getting indicator\')\n    return IndicatorATR(**kwargs)\n# end of get_indicator\n'"
analysis_engine/indicators/base_indicator.py,0,"b'""""""\nBase Indicator Class for deriving your own indicators\nto use within an ``analysis_engine.indicators.in\ndicator_processor.IndicatorProcessor``\n""""""\n\nimport uuid\nimport pandas as pd\nimport logging\nimport analysis_engine.consts as ae_consts\nimport spylunking.log.setup_logging as log_utils\n\n\nclass BaseIndicator:\n    """"""BaseIndicator""""""\n\n    def __init__(\n            self,\n            config_dict,\n            path_to_module=None,\n            name=None,\n            verbose=False):\n        """"""__init__\n\n        Base class for building your own indicators to work\n        within an ``IndicatorProcessor``.\n\n        Please derive the ``self.process()`` method as needed\n\n        .. tip:: any keys passed in with ``config_dict`` will\n            become class member variables that can be accessed\n            and used as normal member variables within the\n            derived Indicator class\n\n        :param config_dict: dictionary for this indicator\n        :param name: name of the indicator\n        :param path_to_module: work in progress -\n            this will allow loading indicators from\n            outside the repo like the derived algorithm\n            classes\n        :param verbose: optional - bool for toggling more logs\n        """"""\n        self.name = name\n        self.log = log_utils.build_colorized_logger(\n            name=name)\n\n        self.config = config_dict\n        self.path_to_module = path_to_module\n        self.verbose = verbose\n\n        if not self.config:\n            raise Exception(\n                \'please provide a config_dict for loading \'\n                \'the buy and sell rules for this indicator\')\n\n        if not self.verbose:\n            self.verbose = self.config.get(\n                \'verbose\',\n                False)\n\n        if not self.name:\n            self.name = f\'ind_{str(uuid.uuid4()).replace(""-"", """")}\'\n\n        self.starter_dict = None\n        self.previous_df = self.config.get(\n            \'previous_df\',\n            None)\n        self.name_of_df = self.config.get(\n            \'uses_data\',\n            None)\n        self.uses_data = self.name_of_df\n        self.report = self.config.get(\n            \'report\',\n            {})\n        self.ind_id = self.report.get(\n            \'id\',\n            self.name)\n        self.metrics = self.report.get(\n            \'metrics\',\n            {})\n        self.ind_type = self.metrics.get(\n            \'type\',\n            ae_consts.INDICATOR_TYPE_UNKNOWN)\n        self.ind_category = self.metrics.get(\n            \'category\',\n            ae_consts.INDICATOR_CATEGORY_UNKNOWN)\n        self.ind_uses_data = self.metrics.get(\n            \'ind_uses_data\',\n            ae_consts.INDICATOR_USES_DATA_ANY)\n        self.dataset_df_str = self.config.get(\n            \'dataset_df\',\n            None)\n\n        self.report_key_prefix = self.report.get(\n            \'report_key_prefix\',\n            self.name)\n\n        # this should be mostly numeric values\n        # to allow converting to an AI-ready dataset\n        # once the algorithm finishes\n        self.report_dict = {\n            \'type\': self.ind_type,\n            \'category\': self.ind_category,\n            \'uses_data\': self.ind_uses_data\n        }\n\n        self.report_ignore_keys = self.config.get(\n            \'report_ignore_keys\',\n            ae_consts.INDICATOR_IGNORED_CONIGURABLE_KEYS)\n        self.use_df = pd.DataFrame(\n            ae_consts.EMPTY_DF_LIST)\n        self.configurables = self.config\n        self.ind_confs = []\n        self.convert_config_keys_to_members()\n    # end of __init__\n\n    def get_config(\n            self):\n        """"""get_config""""""\n        pruned_config = {}\n\n        # remove the obj\n        remove_keys = [\n            \'obj\'\n        ]\n        for k in self.config:\n            if k not in remove_keys:\n                pruned_config[k] = self.config[k]\n        return pruned_config\n    # end of get_config\n\n    def convert_config_keys_to_members(\n            self):\n        """"""convert_config_keys_to_members\n\n        This converts any key in the config to\n        a member variable that can be used with the\n        your derived indicators like: ``self.<KEY_IN_CONFIG>``\n        """"""\n        for k in self.config:\n            if k not in self.report_ignore_keys:\n                self.__dict__[k] = self.config[k]\n    # end of convert_config_keys_to_members\n\n    def build_configurable_node(\n            self,\n            name,\n            conf_type,\n            current_value=None,\n            default_value=None,\n            max_value=None,\n            min_value=None,\n            is_output_only=False,\n            inc_interval=None,\n            notes=None,\n            **kwargs):\n        """"""build_configurable_node\n\n        Helper for building a single configurable type\n        node for programmatically creating algo configs\n\n        :param name: name of the member configurable\n        :param conf_type: string - configurable type\n        :param current_value: optional - current value\n        :param default_value: optional - default value\n        :param max_value: optional - maximum value\n        :param min_value: optional - minimum value\n        :param is_output_only: optional - bool for setting\n            the input parameter as an output-only value\n            (default is ``False``)\n        :param inc_interval: optional - float value\n            for controlling how the tests should increment\n            while walking between the ``min_value`` and the\n            ``max_value``\n        :param notes: optional - string notes\n        :param kwargs: optional - derived keyword args dictionary\n        """"""\n        node = {\n            \'name\': name,\n            \'type\': conf_type,\n            \'value\': current_value,\n            \'default\': default_value,\n            \'max\': max_value,\n            \'min\': min_value,\n            \'is_output_only\': is_output_only,\n            \'inc_interval\': inc_interval,\n            \'notes\': notes\n        }\n        for k in kwargs:\n            node[k] = kwargs[k]\n        return node\n    # end of build_configurable_node\n\n    def build_base_configurables(\n            self,\n            ind_type=\'momentum\',\n            category=\'technical\',\n            uses_data=\'minute\',\n            version=1):\n        """"""build_base_configurables\n\n        :param ind_type: string indicator type\n        :param category: string indicator category\n        :param uses_data: string indicator\n            usess this type of data\n        :param version: integer for building\n            configurables for the testing\n            generation version\n        """"""\n        self.ind_confs = []\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'category\',\n            conf_type=\'str\',\n            default_value=category,\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'type\',\n            conf_type=\'str\',\n            default_value=ind_type,\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'uses_data\',\n            conf_type=\'str\',\n            default_value=self.config.get(\n                \'uses_data\',\n                uses_data),\n            is_output_only=True))\n\n        if version == 1:\n            self.ind_confs.append(self.build_configurable_node(\n                name=\'is_buy\',\n                conf_type=\'int\',\n                is_output_only=True))\n            self.ind_confs.append(self.build_configurable_node(\n                name=\'is_sell\',\n                conf_type=\'int\',\n                is_output_only=True))\n    # end of build_base_configurables\n\n    def get_configurables(\n            self,\n            **kwargs):\n        """"""get_configurables\n\n        **Derive this in your indicators**\n\n        This is used as a helper for setting up algorithm\n        configs for this indicator and to programmatically set\n        the values based off the domain rules\n\n        :param kwargs: optional keyword args\n        """"""\n        self.ind_confs = []\n\n        self.lg(\n            f\'configurables={ae_consts.ppj(self.ind_confs)} for \'\n            f\'class={self.__class__.__name__}\')\n\n        return self.ind_confs\n    # end of get_configurables\n\n    def set_configurables(\n            self,\n            config_dict):\n        """"""set_configurables\n\n        :param config_dict: indicator config dictionary\n        """"""\n        self.configurables = config_dict\n        return self.configurables\n    # end of set_configurables\n\n    def lg(\n            self,\n            msg,\n            level=logging.INFO):\n        """"""lg\n\n        Log only if the indicator has ``self.verbose``\n        set to ``True`` or if the ``level == logging.CRITICAL`` or\n        ``level = logging.ERROR`` otherwise no logs\n\n        :param msg: string message to log\n        :param level: set the logging level\n            (default is ``logging.INFO``)\n        """"""\n        if self.verbose:\n            if level == logging.INFO:\n                self.log.info(msg)\n                return\n            elif level == logging.ERROR:\n                self.log.error(msg)\n                return\n            elif level == logging.DEBUG:\n                self.log.debug(msg)\n                return\n            elif level == logging.WARN:\n                self.log.warn(msg)\n                return\n            elif level == logging.CRITICAL:\n                self.log.critical(msg)\n                return\n        else:\n            if level == logging.ERROR:\n                self.log.error(msg)\n                return\n            elif level == logging.CRITICAL:\n                self.log.critical(msg)\n                return\n    # end lg\n\n    def get_report_prefix(\n            self):\n        """"""get_report_prefix""""""\n        return self.report_key_prefix\n    # end of get_report_prefix\n\n    def build_report_key(\n            self,\n            key,\n            prefix_key,\n            key_type,\n            cur_report_dict):\n        """"""build_report_key\n\n        :param prefix_key:\n        """"""\n        report_key = (\n            f\'{prefix_key}_{key}\')\n        if report_key in cur_report_dict:\n            report_key = (\n                f\'{report_key}_\'\n                f\'{str(uuid.uuid4()).replace(""-"", """")}\')\n        # end of building a key to prevent stomping data\n\n        return report_key\n    # end of build_report_key\n\n    def get_report(\n            self,\n            verbose=False):\n        """"""get_report\n\n        Get the indicator\'s current output node\n        that is used for the trading performance report\n        generated at the end of the algorithm\n\n        .. note:: the report dict should mostly be numeric\n            types to enable AI predictions after removing\n            non-numeric columns\n\n        :param verbose: optional - boolean for toggling\n            to show the report\n        """"""\n        cur_report_dict = {}\n\n        # allow derived indicators to build their own report prefix\n        report_prefix_key_name = self.get_report_prefix()\n\n        for key in self.report_dict:\n\n            is_valid = True\n            if key in self.report_ignore_keys:\n                is_valid = False\n\n            if is_valid:\n                report_key = self.build_report_key(\n                    key,\n                    prefix_key=report_prefix_key_name,\n                    key_type=\'report\',\n                    cur_report_dict=cur_report_dict)\n                cur_report_dict[report_key] = self.report_dict[key]\n        # for all keys to output into the report\n\n        buy_value = None\n        sell_value = None\n\n        for key in self.configurables:\n\n            is_valid = True\n            if key in self.report_ignore_keys:\n                is_valid = False\n            elif key not in self.__dict__:\n                is_valid = False\n\n            if is_valid:\n                report_key = self.build_report_key(\n                    key,\n                    prefix_key=report_prefix_key_name,\n                    key_type=\'conf\',\n                    cur_report_dict=cur_report_dict)\n\n                use_value = None\n                if key == \'is_buy\':\n                    buy_value = self.__dict__[key]\n                    if buy_value:\n                        use_value = \\\n                            ae_consts.INDICATOR_ACTIONS[buy_value]\n                    else:\n                        use_value = \\\n                            ae_consts.INT_INDICATOR_NOT_PROCESSED\n                elif key == \'is_sell\':\n                    sell_value = self.__dict__[key]\n                    if sell_value:\n                        use_value = \\\n                            ae_consts.INDICATOR_ACTIONS[sell_value]\n                    else:\n                        use_value = \\\n                            ae_consts.INT_INDICATOR_NOT_PROCESSED\n                else:\n                    use_value = self.__dict__[key]\n                # end of deciding value\n\n                cur_report_dict[report_key] = use_value\n            # if valid\n\n        # end of all configurables for this indicator\n\n        if verbose or self.verbose:\n            self.lg(\n                f\'indicator={self.name} \'\n                f\'report={ae_consts.ppj(cur_report_dict)} \'\n                f\'buy={buy_value} sell={sell_value}\')\n\n        return cur_report_dict\n    # end of get_report\n\n    def get_path_to_module(\n            self):\n        """"""get_path_to_module""""""\n        return self.path_to_module\n    # end of get_path_to_module\n\n    def get_name(\n            self):\n        """"""get_name""""""\n        return self.name\n    # end of get_name\n\n    def get_dataset_by_name(\n            self,\n            dataset,\n            dataset_name):\n        """"""get_dataset_by_name\n\n        Method for getting just a dataset\n        by the dataset_name`` inside the cached\n        ``dataset[\'data\']`` dictionary of ``pd.Dataframe(s)``\n\n        :param dataset: cached dataset value\n            that holds the dictionaries: ``dataset[\'data\']``\n        :param dataset_name: optional - name of the\n            supported ``pd.DataFrame`` that is in the\n            cached ``dataset[\'data\']`` dictionary\n            of dataframes\n        """"""\n        return dataset[\'data\'].get(\n            dataset_name,\n            pd.DataFrame(ae_consts.EMPTY_DF_LIST))\n    # end of get_dataset_by_name\n\n    def get_subscribed_dataset(\n            self,\n            dataset,\n            dataset_name=None):\n        """"""get_subscribed_dataset\n\n        Method for getting just the subscribed dataset\n        else use the ``dataset_name`` argument dataset\n\n        :param dataset: cached dataset value\n            that holds the dictionaries: ``dataset[\'data\']``\n        :param dataset_name: optional - name of the\n            supported ``pd.DataFrame`` that is in the\n            cached ``dataset[\'data\']`` dictionary\n            of dataframes\n        """"""\n        ret_df = None\n        if dataset_name:\n            ret_df = dataset[\'data\'].get(\n                dataset_name,\n                pd.DataFrame(ae_consts.EMPTY_DF_LIST))\n        else:\n            ret_df = dataset[\'data\'].get(\n                self.name_of_df,\n                pd.DataFrame(ae_consts.EMPTY_DF_LIST))\n\n        if hasattr(ret_df, \'index\'):\n            return ae_consts.SUCCESS, ret_df\n        else:\n            return ae_consts.EMPTY, ret_df\n    # end of get_subscribed_dataset\n\n    def reset_internals(\n            self,\n            **kwargs):\n        """"""reset_internals\n\n        Support a cleanup action before indicators\n        run between datasets. Derived classes can\n        implement custom cleanup actions that need\n        to run before each ``IndicatorProcessor.process()``\n        call is run on the next cached dataset\n\n        :param kwargs: keyword args dictionary\n        """"""\n        return ae_consts.SUCCESS\n    # end of reset_internals\n\n    def handle_subscribed_dataset(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""handle_subscribed_dataset\n\n        Filter the algorithm\'s ``dataset`` to just the\n        dataset the indicator is set up to use as defined by\n        the member variable:\n\n        - ``self.name_of_df`` - string value like ``daily``, ``minute``\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: dictionary of ``pd.DataFrame(s)`` to process\n        """"""\n\n        # certain datasets like minutes or options may\n        # want to refer to the previous dataset\n        self.previous_df = dataset\n\n        # call derived class\'s process()\n        self.process(\n            algo_id=algo_id,\n            ticker=ticker,\n            dataset=dataset)\n    # end of handle_subscribed_dataset\n\n    def process(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""process\n\n        Derive custom indicator processing to determine buy and sell\n        conditions before placing orders. Just implement your own\n        ``process`` method.\n\n        Please refer to the TA Lib guides for details on building indicators:\n\n        - Overlap Studies\n          https://mrjbq7.github.io/ta-lib/func_groups/overlap_studies.html\n        - Momentum Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html\n        - Volume Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volume_indicators.html\n        - Volatility Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volatility_indicators.html\n        - Price Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/price_transform.html\n        - Cycle Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/cycle_indicators.html\n        - Pattern Recognition\n          https://mrjbq7.github.io/ta-lib/func_groups/pattern_recognition.html\n        - Statistic Functions\n          https://mrjbq7.github.io/ta-lib/func_groups/statistic_functions.html\n        - Math Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/math_transform.html\n        - Math Operators\n          https://mrjbq7.github.io/ta-lib/func_groups/math_operators.html\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: dictionary of ``pd.DataFrame(s)`` to process\n        """"""\n        self.lg(f\'{self.name} BASE_IND process - start\')\n        self.lg(f\'{self.name} BASE_IND process - end\')\n    # end of process\n\n# end of BaseIndicator\n'"
analysis_engine/indicators/bollinger_bands.py,0,"b'""""""\nCustom BollingerBands\n\nhttps://www.investopedia.com/terms/b/bollingerbands.asp\n\nOverlap\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport analysis_engine.ae_talib as ae_talib\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.indicators.base_indicator as base_indicator\n\n\nclass IndicatorBollingerBands(base_indicator.BaseIndicator):\n    """"""IndicatorBollingerBands""""""\n\n    def __init__(\n            self,\n            **kwargs):\n        """"""__init__\n\n        Custom indicator example for showing a\n        ``Bollinger Band``\n        within an algo for analyzing intraday minute datasets\n\n        Please refer to the `analysis_engine.indicators.base_indicator.Ba\n        seIndicator source code for the latest supported parameters <ht\n        tps://github.com/AlgoTraders/stock-analysis-engine\n        /blob/master/\n        analysis_engine/indicators/base_indicator.py>`__\n\n        :param kwargs: keyword arguments\n        """"""\n        super().__init__(**kwargs)\n    # end of __init__\n\n    def get_configurables(\n            self,\n            **kwargs):\n        """"""get_configurables\n\n        helper for setting up algorithm configs for this indicator\n        and programmatically set the values based off the domain\n        rules\n\n        .. code-block:: python\n\n            from analysis_engine.indicators.bollinger_bands \\\n                import IndicatorBollingerBands\n            ind = IndicatorBollingerBands(config_dict={\n                    \'verbose\': True\n                }).get_configurables()\n\n        :param kwargs: keyword args dictionary\n        """"""\n        self.ind_confs = []\n\n        # common:\n        self.build_base_configurables(\n            ind_type=\'overlap\',\n            category=\'technical\',\n            uses_data=self.config.get(\n                \'uses_data\',\n                \'minute\'),\n            version=1)\n\n        # custom:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'num_points\',\n            conf_type=\'int\',\n            max_value=200,\n            min_value=2,\n            default_value=20,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_below_percent\',\n            conf_type=\'percent\',\n            min_value=1.0,\n            max_value=100.0,\n            default_value=1.0,\n            inc_interval=3.0))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_above_percent\',\n            conf_type=\'percent\',\n            min_value=1.0,\n            max_value=100.0,\n            default_value=1.0,\n            inc_interval=3.0))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'upper_stdev\',\n            conf_type=\'float\',\n            min_value=0.5,\n            max_value=10.0,\n            default_value=0.5,\n            inc_interval=1.0))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'lower_stdev\',\n            conf_type=\'float\',\n            min_value=0.5,\n            max_value=10.0,\n            default_value=0.5,\n            inc_interval=1.0))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'matype\',\n            conf_type=\'int\',\n            min_value=0,\n            max_value=0,\n            default_value=0,\n            inc_interval=1))\n\n        # output / reporting:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'upperband\',\n            conf_type=\'int\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'middleband\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'lowerband\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'upper_lower_width\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'amount_to_low\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'amount_to_high\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'percent_to_low\',\n            conf_type=\'percent\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'percent_to_high\',\n            conf_type=\'percent\',\n            is_output_only=True))\n\n        default_values_dict = {}\n        for node in self.ind_confs:\n            name = node[\'name\']\n            default_value = node.get(\n                \'default\',\n                None)\n            default_values_dict[name] = default_value\n\n        use_file = None\n        try:\n            if __file__:\n                use_file = __file__\n        except Exception:\n            use_file = None\n\n        self.starter_dict = {\n            \'name\': self.__class__.__name__.lower().replace(\n                \'indicator\',\n                \'\'),\n            \'module_path\': use_file,\n            \'category\': default_values_dict.get(\n                \'category\',\n                \'momentum\'),\n            \'type\': default_values_dict.get(\n                \'type\',\n                \'technical\'),\n            \'uses_data\': default_values_dict.get(\n                \'uses_data\',\n                \'minute\'),\n            \'verbose\': default_values_dict.get(\n                \'verbose\',\n                False)\n        }\n        self.starter_dict.update(default_values_dict)\n\n        self.lg(\n            f\'configurables={ae_consts.ppj(self.ind_confs)} for \'\n            f\'class={self.__class__.__name__} in file={use_file} \'\n            f\'starter:\\n {ae_consts.ppj(self.starter_dict)}\')\n\n        return self.ind_confs\n    # end of get_configurables\n\n    def get_starter_dict(\n            self):\n        if not self.starter_dict:\n            self.get_configurables()\n        return self.starter_dict\n    # end of get_starter_dict\n\n    def process(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""process\n\n        Derive custom indicator processing to determine buy and sell\n        conditions before placing orders. Just implement your own\n        ``process`` method.\n\n        Please refer to the TA Lib guides for details on building indicators:\n\n        - Overlap Studies\n          https://mrjbq7.github.io/ta-lib/func_groups/overlap_studies.html\n        - Momentum Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html\n        - Volume Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volume_indicators.html\n        - Volatility Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volatility_indicators.html\n        - Price Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/price_transform.html\n        - Cycle Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/cycle_indicators.html\n        - Pattern Recognition\n          https://mrjbq7.github.io/ta-lib/func_groups/pattern_recognition.html\n        - Statistic Functions\n          https://mrjbq7.github.io/ta-lib/func_groups/statistic_functions.html\n        - Math Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/math_transform.html\n        - Math Operators\n          https://mrjbq7.github.io/ta-lib/func_groups/math_operators.html\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: dictionary of ``pandas.DataFrame(s)`` to process\n        """"""\n\n        # set the algo config indicator \'uses_data\' to \'day\' or \'minute\'\n        df_status, self.use_df = self.get_subscribed_dataset(\n            dataset=dataset)\n\n        if df_status == ae_consts.EMPTY:\n            self.lg(\'process end - no data found\')\n            return\n\n        # notice the self.num_points is now a member variable\n        # because the BaseIndicator class\'s __init__\n        # converts any self.config keys into useable\n        # member variables automatically in your derived class\n        self.lg(\n            f\'process - num_points={self.num_points} \'\n            f\'df={len(self.use_df.index)}\')\n        """"""\n        upperband, middleband, lowerband = BBANDS(\n            close,\n            timeperiod=5,\n            nbdevup=2,\n            nbdevdn=2,\n            matype=0)\n        """"""\n        num_records = len(self.use_df.index)\n        if num_records > self.num_points:\n            cur_value = self.use_df[\'close\'].iloc[-1]\n            first_date = self.use_df[\'date\'].iloc[0]\n            end_date = self.use_df[\'date\'].iloc[-1]\n            start_row = num_records - self.num_points\n            self.use_df = self.use_df[start_row:num_records]\n            """"""\n            for idx, row in self.use_df[start_row:-1].iterrows():\n                high = row[\'high\']\n                low = row[\'low\']\n                open_val = row[\'open\']\n                close = row[\'close\']\n                row_date = row[\'date\']\n                self.lg(\n                    f\'{row_date} - high={high}, low={low}, \'\n                    f\'close={close}, period={self.num_points}\')\n            """"""\n            closes = self.use_df[\'close\'].values\n\n            (upperbands,\n             middlebands,\n             lowerbands) = ae_talib.BBANDS(\n                 close=closes,\n                 timeperiod=self.num_points,\n                 nbdevup=self.upper_stdev,\n                 nbdevdn=self.lower_stdev,\n                 matype=self.matype)\n\n            """"""\n            Determine a buy or a sell as a label\n            """"""\n\n            self.upperband = ae_consts.to_f(upperbands[-1])\n            self.middleband = ae_consts.to_f(middlebands[-1])\n            self.lowerband = ae_consts.to_f(lowerbands[-1])\n\n            if cur_value <= 0:\n                self.lg(f\'invalid current_value={cur_value}\')\n                return\n\n            self.amount_to_low = ae_consts.to_f(cur_value - self.lowerband)\n            self.amount_to_high = ae_consts.to_f(self.upperband - cur_value)\n\n            if self.amount_to_low < 0:\n                self.percent_to_low = -1 * ae_consts.to_f(\n                    self.amount_to_low / cur_value * 100.0)\n            else:\n                self.percent_to_low = ae_consts.to_f(\n                    self.amount_to_low / cur_value * 100.0)\n\n            if self.amount_to_high < 0:\n                self.percent_to_high = -1 * ae_consts.to_f(\n                    self.amount_to_high / cur_value * 100.0)\n            else:\n                self.percent_to_high = ae_consts.to_f(\n                    self.amount_to_high / cur_value * 100.0)\n\n            self.is_buy = ae_consts.INDICATOR_IGNORE\n            self.is_sell = ae_consts.INDICATOR_IGNORE\n\n            if self.percent_to_low > self.buy_below_percent:\n                self.is_buy = ae_consts.INDICATOR_BUY\n            elif self.percent_to_high > self.sell_above_percent:\n                self.is_sell = ae_consts.INDICATOR_SELL\n\n            self.lg(\n                f\'process end - {first_date} to {end_date} \'\n                f\'buy_below={self.buy_below_percent} is_buy={self.is_buy} \'\n                f\'sell_above={self.sell_above_percent} is_sell={self.is_sell}\')\n        else:\n            self.lg(\'process end\')\n    # end of process\n\n    def reset_internals(\n            self):\n        """"""reset_internals""""""\n        self.is_buy = ae_consts.INDICATOR_RESET\n        self.is_sell = ae_consts.INDICATOR_RESET\n    # end of reset_internals\n\n# end of IndicatorBollingerBands\n\n\ndef get_indicator(\n        **kwargs):\n    """"""get_indicator\n\n    Make sure to define the ``get_indicator`` for your custom\n    algorithms to work as a backup with the ``sa.py`` tool...\n    Not anticipating issues, but if we do with importlib\n    this is the backup plan.\n\n    Please file an issue if you see something weird and would like\n    some help:\n    https://github.com/AlgoTraders/stock-analysis-engine/issues\n\n    :param kwargs: dictionary of keyword arguments\n    """"""\n    print(\'getting indicator\')\n    return IndicatorBollingerBands(**kwargs)\n# end of get_indicator\n'"
analysis_engine/indicators/build_indicator_node.py,0,"b'""""""\nBuild a single indicator for an algorithm\n""""""\n\nimport uuid\nimport copy\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.utils as ae_utils\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef build_indicator_node(\n        node,\n        label=None):\n    """"""build_indicator_node\n\n    Parse a dictionary in the algorithm config ``indicators`` list\n    and return a dictionary\n\n    Supported values found in:\n    `analysis_engine/consts.py <https://\n    github.com/AlgoTraders/stock-analysis-engine/\n    blob/master/analysis_engine/consts.py>`__\n\n    :param node: single dictionary from the config\'s ``indicators`` list\n    :param label: optional - string log tracking\n        this class in the logs (usually just the algo\n        name is good enough to help debug issues\n        when running distributed)\n    :return: dictionary\n    """"""\n    if not label:\n        label = \'build_indicator_node\'\n\n    name = node.get(\n        \'name\',\n        None)\n    if not name:\n        raise Exception(\n            f\'{label} missing ""name"" in \'\n            f\'indicator dictionary={node}\')\n    # end of name check\n\n    ind_id = str(uuid.uuid4()).replace(\'-\', \'\')\n    uses_dataset_str = node.get(\n        \'uses_data\',\n        \'daily\')\n    uses_dataset = ae_consts.get_indicator_uses_data_as_int(\n        val=uses_dataset_str)\n    if uses_dataset == ae_consts.INDICATOR_USES_DATA_UNSUPPORTED:\n        uses_dataset = ae_consts.INDICATOR_USES_DATA_ANY\n        log.debug(\n            f\'{label} - unsupported indicator \'\n            f\'uses_dataset={uses_dataset_str} defaulting \'\n            f\'to ""daily""\')\n    # end of supported indicator dataset types\n\n    ind_category_str = node.get(\n        \'category\',\n        \'momentum\')\n    ind_category = ae_consts.get_indicator_category_as_int(\n        val=ind_category_str)\n    if ind_category == ae_consts.INDICATOR_CATEGORY_UNKNOWN:\n        ind_category = ae_consts.INDICATOR_CATEGORY_MOMENTUM\n        log.debug(\n            f\'{label} - unsupported indicator \'\n            f\'category={ind_category} defaulting \'\n            f\'to ""momentum""\')\n    # end of supported indicator category\n\n    ind_type_str = node.get(\n        \'type\',\n        \'technical\')\n    ind_type = ae_consts.get_indicator_type_as_int(\n        val=ind_type_str)\n    if ind_type == ae_consts.INDICATOR_TYPE_UNKNOWN:\n        ind_type = ae_consts.INDICATOR_TYPE_TECHNICAL\n        log.debug(\n            f\'{label} - unsupported indicator \'\n            f\'type={ind_type} defaulting to ""technical""\')\n    # end of supported indicator type\n\n    # allow easier key discovery\n    use_unique_id = node.get(\n        \'unique_id\',\n        False)\n    ind_name = (\n        f\'{name}\')\n    if use_unique_id:\n        ind_name = (\n            f\'{name}_\'\n            f\'{uses_dataset}_\'\n            f\'{ind_id}\')\n\n    use_module_name = None\n    use_path_to_module = None\n\n    # none will use the BaseIndicator which does nothing\n    use_path_to_module = node.get(\n        \'module_path\',\n        ae_consts.INDICATOR_BASE_MODULE_PATH)\n    if not use_path_to_module:\n        raise Exception(\n            f\'Failed building Indicator node with missing \'\n            f\'module_path node={node}\')\n    use_module_name = node.get(\n        \'module_name\',\n        node.get(\n            \'name\',\n            ind_id))\n\n    default_report_ignore_keys = \\\n        ae_consts.INDICATOR_IGNORED_CONIGURABLE_KEYS\n\n    report_dict = {\n        \'id\': ind_id,\n        \'name\': ind_name,\n        \'created\': ae_utils.utc_now_str(),\n        \'version\': 1,\n        \'module_name\': use_module_name,\n        \'path_to_module\': use_path_to_module,\n        \'report_ignore_keys\': default_report_ignore_keys,\n        \'metrics\': {\n            \'type\': ind_type,\n            \'category\': ind_category,\n            \'uses_data\': uses_dataset\n        }\n    }\n\n    labeled_node = copy.deepcopy(node)\n\n    # allow a node\'s sub report dir to be patched with this\n    # tracking + reporting data\n    # the algorithms will flatten:\n    #    indicators[ind_name][\'report\'][\'metrics\']\n    # for trading performance report generation\n    if \'report\' in labeled_node:\n        labeled_node[\'report\'].update(report_dict)\n    else:\n        labeled_node[\'report\'] = report_dict\n\n    return labeled_node\n# end of build_indicator_node\n'"
analysis_engine/indicators/chaikin.py,0,"b'""""""\nCustom Chaikin\n\nhttps://www.investopedia.com/terms/c/chaikinoscillator.asp\nhttps://www.investopedia.com/articles/active-trading/\n031914/understanding-chaikin-oscillator.asp\n\nVolume\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport pandas as pd\nimport analysis_engine.ae_talib as ae_talib\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.indicators.base_indicator as base_indicator\n\n\nclass IndicatorChaikin(base_indicator.BaseIndicator):\n    """"""IndicatorChaikin""""""\n\n    def __init__(\n            self,\n            **kwargs):\n        """"""__init__\n\n        Custom indicator example for showing a\n        ``Chaikin``\n        within an algo for analyzing intraday minute datasets\n\n        Please refer to the `analysis_engine.indicators.base_indicator.Ba\n        seIndicator source code for the latest supported parameters <ht\n        tps://github.com/AlgoTraders/stock-analysis-engine\n        /blob/master/\n        analysis_engine/indicators/base_indicator.py>`__\n\n        :param kwargs: keyword arguments\n        """"""\n        super().__init__(**kwargs)\n    # end of __init__\n\n    def get_configurables(\n            self,\n            **kwargs):\n        """"""get_configurables\n\n        helper for setting up algorithm configs for this indicator\n        and programmatically set the values based off the domain\n        rules\n\n        .. code-block:: python\n\n            from analysis_engine.indicators.chaikin \\\n                import IndicatorChaikin\n            ind = IndicatorChaikin(config_dict={\n                    \'verbose\': True\n                }).get_configurables()\n\n        :param kwargs: keyword args dictionary\n        """"""\n        self.ind_confs = []\n\n        # common:\n        self.build_base_configurables(\n            ind_type=\'volume\',\n            category=\'technical\',\n            uses_data=self.config.get(\n                \'uses_data\',\n                \'minute\'),\n            version=1)\n\n        # custom:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'num_points\',\n            conf_type=\'int\',\n            max_value=200,\n            min_value=2,\n            default_value=100,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'highwater_volume\',\n            conf_type=\'int\',\n            default=None))\n\n        # output / reporting:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'chaikin_value\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'amount_to_close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'percent_value\',\n            conf_type=\'percent\',\n            is_output_only=True))\n\n        default_values_dict = {}\n        for node in self.ind_confs:\n            name = node[\'name\']\n            default_value = node.get(\n                \'default\',\n                None)\n            default_values_dict[name] = default_value\n\n        use_file = None\n        try:\n            if __file__:\n                use_file = __file__\n        except Exception:\n            use_file = None\n\n        self.starter_dict = {\n            \'name\': self.__class__.__name__.lower().replace(\n                \'indicator\',\n                \'\'),\n            \'module_path\': use_file,\n            \'category\': default_values_dict.get(\n                \'category\',\n                \'momentum\'),\n            \'type\': default_values_dict.get(\n                \'type\',\n                \'technical\'),\n            \'uses_data\': default_values_dict.get(\n                \'uses_data\',\n                \'minute\'),\n            \'verbose\': default_values_dict.get(\n                \'verbose\',\n                False)\n        }\n        self.starter_dict.update(default_values_dict)\n\n        self.lg(\n            f\'configurables={ae_consts.ppj(self.ind_confs)} for \'\n            f\'class={self.__class__.__name__} in file={use_file} \'\n            f\'starter:\\n {ae_consts.ppj(self.starter_dict)}\')\n\n        return self.ind_confs\n    # end of get_configurables\n\n    def get_starter_dict(\n            self):\n        if not self.starter_dict:\n            self.get_configurables()\n        return self.starter_dict\n    # end of get_starter_dict\n\n    def process(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""process\n\n        Derive custom indicator processing to determine buy and sell\n        conditions before placing orders. Just implement your own\n        ``process`` method.\n\n        Please refer to the TA Lib guides for details on building indicators:\n\n        - Overlap Studies\n          https://mrjbq7.github.io/ta-lib/func_groups/overlap_studies.html\n        - Momentum Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html\n        - Volume Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volume_indicators.html\n        - Volatility Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volatility_indicators.html\n        - Price Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/price_transform.html\n        - Cycle Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/cycle_indicators.html\n        - Pattern Recognition\n          https://mrjbq7.github.io/ta-lib/func_groups/pattern_recognition.html\n        - Statistic Functions\n          https://mrjbq7.github.io/ta-lib/func_groups/statistic_functions.html\n        - Math Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/math_transform.html\n        - Math Operators\n          https://mrjbq7.github.io/ta-lib/func_groups/math_operators.html\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: dictionary of ``pandas.DataFrame(s)`` to process\n        """"""\n\n        # set the algo config indicator \'uses_data\' to \'day\' or \'minute\'\n        df_status, self.use_df = self.get_subscribed_dataset(\n            dataset=dataset)\n\n        if df_status == ae_consts.EMPTY:\n            self.lg(\'process end - no data found\')\n            return\n\n        # notice the self.num_points is now a member variable\n        # because the BaseIndicator class\'s __init__\n        # converts any self.config keys into useable\n        # member variables automatically in your derived class\n        self.lg(\n            f\'process - num_points={self.num_points} \'\n            f\'df={len(self.use_df.index)}\')\n        """"""\n        real = AD(high, low, close, volume)\n        """"""\n        num_records = len(self.use_df.index)\n        if num_records > self.num_points:\n            cur_value = self.use_df[\'close\'].iloc[-1]\n            first_date = self.use_df[\'date\'].iloc[0]\n            end_date = self.use_df[\'date\'].iloc[-1]\n            start_row = num_records - self.num_points\n            self.use_df = self.use_df[start_row:num_records]\n            """"""\n            for idx, row in self.use_df[start_row:-1].iterrows():\n                high = row[\'high\']\n                low = row[\'low\']\n                open_val = row[\'open\']\n                close = row[\'close\']\n                row_date = row[\'date\']\n                self.lg(\n                    f\'{row_date} - high={high}, low={low}, \'\n                    f\'close={close}, period={self.num_points}\')\n            """"""\n            highs = self.use_df[\'high\'].values\n            lows = self.use_df[\'low\'].values\n            closes = self.use_df[\'close\'].values\n            volumes = pd.to_numeric(self.use_df[\'volume\'])\n\n            self.chaikin_value = ae_consts.to_f(ae_talib.Chaikin(\n                high=highs,\n                low=lows,\n                close=closes,\n                volume=volumes).iloc[-1])\n\n            """"""\n            Determine a buy or a sell as a label\n            """"""\n\n            if cur_value <= 0:\n                self.lg(f\'invalid current_value={cur_value}\')\n                return\n\n            self.close = cur_value\n            self.amount_to_close = ae_consts.to_f(\n                cur_value - self.chaikin_value)\n            self.percent_value = ae_consts.to_f(\n                self.amount_to_close / cur_value * 100.0)\n\n            self.is_buy = ae_consts.INDICATOR_IGNORE\n            self.is_sell = ae_consts.INDICATOR_IGNORE\n\n            if (self.buy_above_percent != -1 and\n                    self.percent_value > self.buy_above_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n            elif (self.buy_below_percent != -1 and\n                    self.percent_value > self.buy_below_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n\n            if (self.sell_above_percent != -1 and\n                    self.percent_value > self.sell_above_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n            elif (self.sell_below_percent != -1 and\n                    self.percent_value > self.sell_below_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n\n            self.lg(\n                f\'process end - {first_date} to {end_date} \'\n                f\'buy_below={self.buy_below_percent} \'\n                f\'buy_above={self.buy_above_percent} is_buy={self.is_buy} \'\n                f\'sell_below={self.sell_below_percent} \'\n                f\'sell_above={self.sell_above_percent} is_sell={self.is_sell}\')\n        else:\n            self.lg(\'process end\')\n    # end of process\n\n    def reset_internals(\n            self):\n        """"""reset_internals""""""\n        self.is_buy = ae_consts.INDICATOR_RESET\n        self.is_sell = ae_consts.INDICATOR_RESET\n    # end of reset_internals\n\n# end of IndicatorChaikin\n\n\ndef get_indicator(\n        **kwargs):\n    """"""get_indicator\n\n    Make sure to define the ``get_indicator`` for your custom\n    algorithms to work as a backup with the ``sa.py`` tool...\n    Not anticipating issues, but if we do with importlib\n    this is the backup plan.\n\n    Please file an issue if you see something weird and would like\n    some help:\n    https://github.com/AlgoTraders/stock-analysis-engine/issues\n\n    :param kwargs: dictionary of keyword arguments\n    """"""\n    print(\'getting indicator\')\n    return IndicatorChaikin(**kwargs)\n# end of get_indicator\n'"
analysis_engine/indicators/chaikin_osc.py,0,"b'""""""\nCustom Chaikin Oscillator\n\nhttps://www.investopedia.com/terms/c/chaikinoscillator.asp\nhttps://www.investopedia.com/articles/active-trading/\n031914/understanding-chaikin-oscillator.asp\n\nVolume\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport pandas as pd\nimport analysis_engine.ae_talib as ae_talib\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.indicators.base_indicator as base_indicator\n\n\nclass IndicatorChaikinOSC(base_indicator.BaseIndicator):\n    """"""IndicatorChaikinOSC""""""\n\n    def __init__(\n            self,\n            **kwargs):\n        """"""__init__\n\n        Custom indicator example for showing a\n        ``ChaikinOSC``\n        within an algo for analyzing intraday minute datasets\n\n        Please refer to the `analysis_engine.indicators.base_indicator.Ba\n        seIndicator source code for the latest supported parameters <ht\n        tps://github.com/AlgoTraders/stock-analysis-engine\n        /blob/master/\n        analysis_engine/indicators/base_indicator.py>`__\n\n        :param kwargs: keyword arguments\n        """"""\n        super().__init__(**kwargs)\n    # end of __init__\n\n    def get_configurables(\n            self,\n            **kwargs):\n        """"""get_configurables\n\n        helper for setting up algorithm configs for this indicator\n        and programmatically set the values based off the domain\n        rules\n\n        .. code-block:: python\n\n            from analysis_engine.indicators.chaikin_osc \\\n                import IndicatorChaikinOSC\n            ind = IndicatorChaikinOSC(config_dict={\n                    \'verbose\': True\n                }).get_configurables()\n\n        :param kwargs: keyword args dictionary\n        """"""\n        self.ind_confs = []\n\n        # common:\n        self.build_base_configurables(\n            ind_type=\'volume\',\n            category=\'technical\',\n            uses_data=self.config.get(\n                \'uses_data\',\n                \'minute\'),\n            version=1)\n\n        # custom:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'num_points\',\n            conf_type=\'int\',\n            max_value=200,\n            min_value=2,\n            default_value=100,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'fast_period\',\n            conf_type=\'int\',\n            max_value=3,\n            default=3,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'slow_period\',\n            conf_type=\'int\',\n            max_value=10,\n            default=10,\n            inc_interval=1))\n\n        # output / reporting:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'chaikinosc_value\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'amount_to_close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'percent_value\',\n            conf_type=\'percent\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'highwater_volume\',\n            conf_type=\'int\',\n            is_output_only=True))\n\n        default_values_dict = {}\n        for node in self.ind_confs:\n            name = node[\'name\']\n            default_value = node.get(\n                \'default\',\n                None)\n            default_values_dict[name] = default_value\n\n        use_file = None\n        try:\n            if __file__:\n                use_file = __file__\n        except Exception:\n            use_file = None\n\n        self.starter_dict = {\n            \'name\': self.__class__.__name__.lower().replace(\n                \'indicator\',\n                \'\'),\n            \'module_path\': use_file,\n            \'category\': default_values_dict.get(\n                \'category\',\n                \'momentum\'),\n            \'type\': default_values_dict.get(\n                \'type\',\n                \'technical\'),\n            \'uses_data\': default_values_dict.get(\n                \'uses_data\',\n                \'minute\'),\n            \'verbose\': default_values_dict.get(\n                \'verbose\',\n                False)\n        }\n        self.starter_dict.update(default_values_dict)\n\n        self.lg(\n            f\'configurables={ae_consts.ppj(self.ind_confs)} for \'\n            f\'class={self.__class__.__name__} in file={use_file} \'\n            f\'starter:\\n {ae_consts.ppj(self.starter_dict)}\')\n\n        return self.ind_confs\n    # end of get_configurables\n\n    def get_starter_dict(\n            self):\n        if not self.starter_dict:\n            self.get_configurables()\n        return self.starter_dict\n    # end of get_starter_dict\n\n    def process(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""process\n\n        Derive custom indicator processing to determine buy and sell\n        conditions before placing orders. Just implement your own\n        ``process`` method.\n\n        Please refer to the TA Lib guides for details on building indicators:\n\n        - Overlap Studies\n          https://mrjbq7.github.io/ta-lib/func_groups/overlap_studies.html\n        - Momentum Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html\n        - Volume Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volume_indicators.html\n        - Volatility Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volatility_indicators.html\n        - Price Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/price_transform.html\n        - Cycle Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/cycle_indicators.html\n        - Pattern Recognition\n          https://mrjbq7.github.io/ta-lib/func_groups/pattern_recognition.html\n        - Statistic Functions\n          https://mrjbq7.github.io/ta-lib/func_groups/statistic_functions.html\n        - Math Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/math_transform.html\n        - Math Operators\n          https://mrjbq7.github.io/ta-lib/func_groups/math_operators.html\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: dictionary of ``pandas.DataFrame(s)`` to process\n        """"""\n\n        # set the algo config indicator \'uses_data\' to \'day\' or \'minute\'\n        df_status, self.use_df = self.get_subscribed_dataset(\n            dataset=dataset)\n\n        if df_status == ae_consts.EMPTY:\n            self.lg(\'process end - no data found\')\n            return\n\n        # notice the self.num_points is now a member variable\n        # because the BaseIndicator class\'s __init__\n        # converts any self.config keys into useable\n        # member variables automatically in your derived class\n        self.lg(\n            f\'process - num_points={self.num_points} \'\n            f\'df={len(self.use_df.index)}\')\n        """"""\n        real = ADOSC(\n            high,\n            low,\n            close,\n            volume,\n            fastperiod=3,\n            slowperiod=10)\n        """"""\n        num_records = len(self.use_df.index)\n        if num_records > self.num_points:\n            cur_value = self.use_df[\'close\'].iloc[-1]\n            first_date = self.use_df[\'date\'].iloc[0]\n            end_date = self.use_df[\'date\'].iloc[-1]\n            start_row = num_records - self.num_points\n            self.use_df = self.use_df[start_row:num_records]\n            """"""\n            for idx, row in self.use_df[start_row:-1].iterrows():\n                high = row[\'high\']\n                low = row[\'low\']\n                open_val = row[\'open\']\n                close = row[\'close\']\n                row_date = row[\'date\']\n                self.lg(\n                    f\'{row_date} - high={high}, low={low}, \'\n                    f\'close={close}, period={self.num_points}\')\n            """"""\n            highs = self.use_df[\'high\'].values\n            lows = self.use_df[\'low\'].values\n            closes = self.use_df[\'close\'].values\n            volumes = pd.to_numeric(self.use_df[\'volume\'])\n\n            self.chaikinosc_value = ae_consts.to_f(ae_talib.ChaikinADOSC(\n                high=highs,\n                low=lows,\n                close=closes,\n                volume=volumes,\n                fast_period=self.fast_period,\n                slow_period=self.slow_period).iloc[-1])\n\n            """"""\n            Determine a buy or a sell as a label\n            """"""\n\n            if cur_value <= 0:\n                self.lg(f\'invalid current_value={cur_value}\')\n                return\n\n            self.close = cur_value\n            self.amount_to_close = ae_consts.to_f(\n                cur_value - self.chaikinosc_value)\n            self.percent_value = ae_consts.to_f(\n                self.amount_to_close / cur_value * 100.0)\n\n            self.is_buy = ae_consts.INDICATOR_IGNORE\n            self.is_sell = ae_consts.INDICATOR_IGNORE\n\n            if (self.buy_above_percent != -1 and\n                    self.percent_value > self.buy_above_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n            elif (self.buy_below_percent != -1 and\n                    self.percent_value > self.buy_below_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n\n            if (self.sell_above_percent != -1 and\n                    self.percent_value > self.sell_above_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n            elif (self.sell_below_percent != -1 and\n                    self.percent_value > self.sell_below_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n\n            self.lg(\n                f\'process end - {first_date} to {end_date} \'\n                f\'buy_below={self.buy_below_percent} \'\n                f\'buy_above={self.buy_above_percent} is_buy={self.is_buy} \'\n                f\'sell_below={self.sell_below_percent} \'\n                f\'sell_above={self.sell_above_percent} is_sell={self.is_sell}\')\n        else:\n            self.lg(\'process end\')\n    # end of process\n\n    def reset_internals(\n            self):\n        """"""reset_internals""""""\n        self.is_buy = ae_consts.INDICATOR_RESET\n        self.is_sell = ae_consts.INDICATOR_RESET\n    # end of reset_internals\n\n# end of IndicatorChaikinOSC\n\n\ndef get_indicator(\n        **kwargs):\n    """"""get_indicator\n\n    Make sure to define the ``get_indicator`` for your custom\n    algorithms to work as a backup with the ``sa.py`` tool...\n    Not anticipating issues, but if we do with importlib\n    this is the backup plan.\n\n    Please file an issue if you see something weird and would like\n    some help:\n    https://github.com/AlgoTraders/stock-analysis-engine/issues\n\n    :param kwargs: dictionary of keyword arguments\n    """"""\n    print(\'getting indicator\')\n    return IndicatorChaikinOSC(**kwargs)\n# end of get_indicator\n'"
analysis_engine/indicators/ema.py,0,"b'""""""\nCustom Exponential Moving Average\n\nhttps://www.investopedia.com/terms/e/ema.asp\n\nOverlap\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport analysis_engine.ae_talib as ae_talib\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.indicators.base_indicator as base_indicator\n\n\nclass IndicatorEMA(base_indicator.BaseIndicator):\n    """"""IndicatorEMA""""""\n\n    def __init__(\n            self,\n            **kwargs):\n        """"""__init__\n\n        Custom indicator example for showing a\n        ``EMA``\n        within an algo for analyzing intraday minute datasets\n\n        Please refer to the `analysis_engine.indicators.base_indicator.Ba\n        seIndicator source code for the latest supported parameters <ht\n        tps://github.com/AlgoTraders/stock-analysis-engine\n        /blob/master/\n        analysis_engine/indicators/base_indicator.py>`__\n\n        :param kwargs: keyword arguments\n        """"""\n        super().__init__(**kwargs)\n    # end of __init__\n\n    def get_configurables(\n            self,\n            **kwargs):\n        """"""get_configurables\n\n        helper for setting up algorithm configs for this indicator\n        and programmatically set the values based off the domain\n        rules\n\n        .. code-block:: python\n\n            from analysis_engine.indicators.ema \\\n                import IndicatorEMA\n            ind = IndicatorEMA(config_dict={\n                    \'verbose\': True\n                }).get_configurables()\n\n        :param kwargs: keyword args dictionary\n        """"""\n        self.ind_confs = []\n\n        # common:\n        self.build_base_configurables(\n            ind_type=\'overlap\',\n            category=\'technical\',\n            uses_data=self.config.get(\n                \'uses_data\',\n                \'minute\'),\n            version=1)\n\n        # custom:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'num_points\',\n            conf_type=\'int\',\n            max_value=200,\n            min_value=2,\n            default_value=100,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n\n        # output / reporting:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'ema_value\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'amount_to_close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'percent_value\',\n            conf_type=\'percent\',\n            is_output_only=True))\n\n        default_values_dict = {}\n        for node in self.ind_confs:\n            name = node[\'name\']\n            default_value = node.get(\n                \'default\',\n                None)\n            default_values_dict[name] = default_value\n\n        use_file = None\n        try:\n            if __file__:\n                use_file = __file__\n        except Exception:\n            use_file = None\n\n        self.starter_dict = {\n            \'name\': self.__class__.__name__.lower().replace(\n                \'indicator\',\n                \'\'),\n            \'module_path\': use_file,\n            \'category\': default_values_dict.get(\n                \'category\',\n                \'momentum\'),\n            \'type\': default_values_dict.get(\n                \'type\',\n                \'technical\'),\n            \'uses_data\': default_values_dict.get(\n                \'uses_data\',\n                \'minute\'),\n            \'verbose\': default_values_dict.get(\n                \'verbose\',\n                False)\n        }\n        self.starter_dict.update(default_values_dict)\n\n        self.lg(\n            f\'configurables={ae_consts.ppj(self.ind_confs)} for \'\n            f\'class={self.__class__.__name__} in file={use_file} \'\n            f\'starter:\\n {ae_consts.ppj(self.starter_dict)}\')\n\n        return self.ind_confs\n    # end of get_configurables\n\n    def get_starter_dict(\n            self):\n        if not self.starter_dict:\n            self.get_configurables()\n        return self.starter_dict\n    # end of get_starter_dict\n\n    def process(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""process\n\n        Derive custom indicator processing to determine buy and sell\n        conditions before placing orders. Just implement your own\n        ``process`` method.\n\n        Please refer to the TA Lib guides for details on building indicators:\n\n        - Overlap Studies\n          https://mrjbq7.github.io/ta-lib/func_groups/overlap_studies.html\n        - Momentum Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html\n        - Volume Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volume_indicators.html\n        - Volatility Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volatility_indicators.html\n        - Price Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/price_transform.html\n        - Cycle Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/cycle_indicators.html\n        - Pattern Recognition\n          https://mrjbq7.github.io/ta-lib/func_groups/pattern_recognition.html\n        - Statistic Functions\n          https://mrjbq7.github.io/ta-lib/func_groups/statistic_functions.html\n        - Math Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/math_transform.html\n        - Math Operators\n          https://mrjbq7.github.io/ta-lib/func_groups/math_operators.html\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: dictionary of ``pandas.DataFrame(s)`` to process\n        """"""\n\n        # set the algo config indicator \'uses_data\' to \'day\' or \'minute\'\n        df_status, self.use_df = self.get_subscribed_dataset(\n            dataset=dataset)\n\n        if df_status == ae_consts.EMPTY:\n            self.lg(\'process end - no data found\')\n            return\n\n        # notice the self.num_points is now a member variable\n        # because the BaseIndicator class\'s __init__\n        # converts any self.config keys into useable\n        # member variables automatically in your derived class\n        self.lg(\n            f\'process - num_points={self.num_points} \'\n            f\'df={len(self.use_df.index)}\')\n        """"""\n        real = EMA(close, timeperiod=30)\n        """"""\n        num_records = len(self.use_df.index)\n        if num_records > self.num_points:\n            cur_value = self.use_df[\'close\'].iloc[-1]\n            first_date = self.use_df[\'date\'].iloc[0]\n            end_date = self.use_df[\'date\'].iloc[-1]\n            start_row = num_records - self.num_points\n            self.use_df = self.use_df[start_row:num_records]\n            """"""\n            for idx, row in self.use_df[start_row:-1].iterrows():\n                high = row[\'high\']\n                low = row[\'low\']\n                open_val = row[\'open\']\n                close = row[\'close\']\n                row_date = row[\'date\']\n                self.lg(\n                    f\'{row_date} - high={high}, low={low}, \'\n                    f\'close={close}, period={self.num_points}\')\n            """"""\n            closes = self.use_df[\'close\'].values\n\n            self.ema_value = ae_consts.to_f(ae_talib.EMA(\n                 close=closes,\n                 timeperiod=self.num_points)[-1])\n\n            """"""\n            Determine a buy or a sell as a label\n            """"""\n\n            if cur_value <= 0:\n                self.lg(f\'invalid current_value={cur_value}\')\n                return\n\n            self.close = cur_value\n            self.amount_to_close = ae_consts.to_f(cur_value - self.ema_value)\n            self.percent_value = ae_consts.to_f(\n                self.amount_to_close / cur_value * 100.0)\n\n            self.is_buy = ae_consts.INDICATOR_IGNORE\n            self.is_sell = ae_consts.INDICATOR_IGNORE\n\n            if (self.buy_above_percent != -1 and\n                    self.percent_value > self.buy_above_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n            elif (self.buy_below_percent != -1 and\n                    self.percent_value > self.buy_below_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n\n            if (self.sell_above_percent != -1 and\n                    self.percent_value > self.sell_above_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n            elif (self.sell_below_percent != -1 and\n                    self.percent_value > self.sell_below_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n\n            self.lg(\n                f\'process end - {first_date} to {end_date} \'\n                f\'buy_below={self.buy_below_percent} \'\n                f\'buy_above={self.buy_above_percent} is_buy={self.is_buy} \'\n                f\'sell_below={self.sell_below_percent} \'\n                f\'sell_above={self.sell_above_percent} is_sell={self.is_sell}\')\n        else:\n            self.lg(\'process end\')\n    # end of process\n\n    def reset_internals(\n            self):\n        """"""reset_internals""""""\n        self.is_buy = ae_consts.INDICATOR_RESET\n        self.is_sell = ae_consts.INDICATOR_RESET\n    # end of reset_internals\n\n# end of IndicatorEMA\n\n\ndef get_indicator(\n        **kwargs):\n    """"""get_indicator\n\n    Make sure to define the ``get_indicator`` for your custom\n    algorithms to work as a backup with the ``sa.py`` tool...\n    Not anticipating issues, but if we do with importlib\n    this is the backup plan.\n\n    Please file an issue if you see something weird and would like\n    some help:\n    https://github.com/AlgoTraders/stock-analysis-engine/issues\n\n    :param kwargs: dictionary of keyword arguments\n    """"""\n    print(\'getting indicator\')\n    return IndicatorEMA(**kwargs)\n# end of get_indicator\n'"
analysis_engine/indicators/get_category_as_int.py,0,"b'""""""\nAlgo data helper for mapping indicator category\nto an integer label value for downstream dataset predictions\n""""""\n\nimport analysis_engine.consts as ae_consts\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef get_category_as_int(\n        node,\n        label=None):\n    """"""get_category_as_int\n\n    Helper for converting feature labels to numeric values\n\n    :param node: convert the dictionary\'s ``category``\n        string to the integer mapped value\n    """"""\n    if not label:\n        label = \'get_category_as_int\'\n    ind_category_str = node.get(\n        \'category\',\n        \'momentum\')\n    if not ind_category_str:\n        return ae_consts.INDICATOR_CATEGORY_UNKNOWN\n    elif ind_category_str == \'momentum\':\n        return ae_consts.INDICATOR_CATEGORY_MOMENTUM\n    elif ind_category_str == \'overlap\':\n        return ae_consts.INDICATOR_CATEGORY_OVERLAP\n    elif ind_category_str == \'price\':\n        return ae_consts.INDICATOR_CATEGORY_PRICE\n    elif ind_category_str == \'volume\':\n        return ae_consts.INDICATOR_CATEGORY_VOLUME\n    elif ind_category_str == \'volatility\':\n        return ae_consts.INDICATOR_CATEGORY_VOLATILITY\n    elif ind_category_str == \'single_call\':\n        return ae_consts.INDICATOR_CATEGORY_SINGLE_CALL\n    elif ind_category_str == \'single_put\':\n        return ae_consts.INDICATOR_CATEGORY_SINGLE_PUT\n    elif ind_category_str == \'bull_call\':\n        return ae_consts.INDICATOR_CATEGORY_BULL_CALL\n    elif ind_category_str == \'bear_put\':\n        return ae_consts.INDICATOR_CATEGORY_BEAR_PUT\n    elif ind_category_str == \'quarterly\':\n        return ae_consts.INDICATOR_CATEGORY_QUARTERLY\n    elif ind_category_str == \'yearly\':\n        return ae_consts.INDICATOR_CATEGORY_YEARLY\n    elif ind_category_str == \'income_statement\':\n        return ae_consts.INDICATOR_CATEGORY_INCOME_STMT\n    elif ind_category_str == \'cash_flow\':\n        return ae_consts.INDICATOR_CATEGORY_CASH_FLOW\n    elif ind_category_str == \'balance_sheet\':\n        return ae_consts.INDICATOR_CATEGORY_BALANCE_SHEET\n    elif ind_category_str == \'press_release\':\n        return ae_consts.INDICATOR_CATEGORY_PRESS_RELEASE\n    elif ind_category_str == \'news\':\n        return ae_consts.INDICATOR_CATEGORY_NEWS\n    elif ind_category_str == \'earnings\':\n        return ae_consts.INDICATOR_CATEGORY_EARNINGS\n    elif ind_category_str == \'splits\':\n        return ae_consts.INDICATOR_CATEGORY_EARNINGS\n    elif ind_category_str == \'reverse_splits\':\n        return ae_consts.INDICATOR_CATEGORY_EARNINGS\n    elif ind_category_str == \'distributions\':\n        return ae_consts.INDICATOR_CATEGORY_EARNINGS\n    elif ind_category_str == \'spinoffs\':\n        return ae_consts.INDICATOR_CATEGORY_EARNINGS\n    elif ind_category_str == \'merger_acq\':\n        return ae_consts.INDICATOR_CATEGORY_EARNINGS\n    elif ind_category_str == \'exchange_inclusion\':\n        return ae_consts.INDICATOR_CATEGORY_EARNINGS\n    elif ind_category_str == \'exchange_exclusion\':\n        return ae_consts.INDICATOR_CATEGORY_EARNINGS\n    elif ind_category_str == \'clinical_trial_positive\':\n        return ae_consts.INDICATOR_CATEGORY_EARNINGS\n    elif ind_category_str == \'clinical_trial_negative\':\n        return ae_consts.INDICATOR_CATEGORY_EARNINGS\n    elif ind_category_str == \'short_sellers\':\n        return ae_consts.INDICATOR_CATEGORY_EARNINGS\n    elif ind_category_str == \'custom\':\n        return ae_consts.INDICATOR_CATEGORY_CUSTOM\n    elif ind_category_str == \'news\':\n        return ae_consts.INDICATOR_CATEGORY_NEWS\n    elif ind_category_str == \'earnings\':\n        return ae_consts.INDICATOR_CATEGORY_EARNINGS\n    elif ind_category_str == \'csuite\':\n        return ae_consts.INDICATOR_CATEGORY_CSUITE\n    elif ind_category_str == \'splits\':\n        return ae_consts.INDICATOR_CATEGORY_SPLITS\n    elif ind_category_str == \'reverse_splits\':\n        return ae_consts.INDICATOR_CATEGORY_REVERSE_SPLITS\n    elif ind_category_str == \'distributions\':\n        return ae_consts.INDICATOR_CATEGORY_DISTRIBUTIONS\n    elif ind_category_str == \'spinoffs\':\n        return ae_consts.INDICATOR_CATEGORY_SPINOFFS\n    elif ind_category_str == \'merger_acq\':\n        return ae_consts.INDICATOR_CATEGORY_MERGER_ACQ\n    elif ind_category_str == \'exchange_inclusion\':\n        return ae_consts.INDICATOR_CATEGORY_EXCHANGE_INCLUSION\n    elif ind_category_str == \'exchange_exclusion\':\n        return ae_consts.INDICATOR_CATEGORY_EXCHANGE_EXCLUSION\n    elif ind_category_str == \'trial_positive\':\n        return ae_consts.INDICATOR_CATEGORY_TRIAL_POSITIVE\n    elif ind_category_str == \'trial_negative\':\n        return ae_consts.INDICATOR_CATEGORY_TRIAL_NEGATIVE\n    elif ind_category_str == \'short_sellers\':\n        return ae_consts.INDICATOR_CATEGORY_SHORT_SELLERS\n    elif ind_category_str == \'real_estate\':\n        return ae_consts.INDICATOR_CATEGORY_REAL_ESTATE\n    elif ind_category_str == \'housing\':\n        return ae_consts.INDICATOR_CATEGORY_HOUSING\n    elif ind_category_str == \'pipeline\':\n        return ae_consts.INDICATOR_CATEGORY_PIPELINE\n    elif ind_category_str == \'construction\':\n        return ae_consts.INDICATOR_CATEGORY_CONSTRUCTION\n    elif ind_category_str == \'fed\':\n        return ae_consts.INDICATOR_CATEGORY_FED\n    else:\n        if label:\n            log.error(\n                f\'{label} - unsupported indicator \'\n                f\'uses_dataset={ind_category_str} defaulting to ""unknown""\')\n        else:\n            log.error(\n                \'unsupported indicator \'\n                f\'uses_dataset={ind_category_str} defaulting to ""unknown""\')\n        return ae_consts.INDICATOR_CATEGORY_UNKNOWN\n    # end of supported indicator dataset types\n# end of get_category_as_int\n'"
analysis_engine/indicators/indicator_processor.py,0,"b'""""""\nIndicator Processor\n\n- v1 Indicator type: ``supported``\n    Binary decision support on buys and sells\n    This is like an alert threshold that is ``on`` or ``off``\n\n- v2 Indicator type: ``not supported``\n    Support for buy or sell value range\n    This is like an alert threshold between a ``lower``\n    and ``upper`` bound\n""""""\n\nimport os\nimport json\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.indicators.build_indicator_node as build_indicator\nimport analysis_engine.indicators.load_indicator_from_module as load_indicator\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\nclass IndicatorProcessor:\n    """"""IndicatorProcessor""""""\n\n    def __init__(\n            self,\n            config_dict,\n            config_file=None,\n            ticker=None,\n            label=None,\n            verbose=False,\n            verbose_indicators=False):\n        """"""__init__\n\n        Algorithm\'s use the ``IndicatorProcessor`` to drive\n        how the underlying indicators are created and configured\n        to determine buy and sell conditions. Create an\n        IndicatorProcessor by passing in a valid:\n\n        ``config_dict`` or a path to a local `config_file``\n\n        Please refer to the `included algorithm config file <http\n        s://github.com/AlgoTraders/stock-analysis-engine/blob/mas\n        ter/tests/algo_configs/test_5_days_ahead.json>`__ for\n        more details on how to create your own.\n\n        :param config_dict: - dictionary for creating\n            indicators and rules for buy/sell conditions\n            and parameters for each indicator\n        :param config_file: path to a json file\n            containing custom algorithm object\n            member values (like indicator configuration and\n            predict future date units ahead for a backtest)\n        :param ticker: optional - single ticker string\n            indicators should focus on math, fundamentals,\n            sentiment and other data, but the context about\n            which ticker this is for should hopefully be\n            abstracted from how an indicator predicts\n            buy and sell conditions\n        :param label: optional - string log tracking\n            this class in the logs (usually just the algo\n            name is good enough to help debug issues\n            when running distributed)\n        :param verbose: optional - bool for more logging\n            (default is ``False``)\n        :param verbose_indicators: optional - bool for more logging\n            for all indicators managed by this ``IndicatorProcessor``\n            (default is ``False``)\n        """"""\n\n        self.config_dict = config_dict\n        if not self.config_dict:\n            if config_file:\n                if not os.path.exists(config_file):\n                    raise Exception(\n                        f\'Unable to find config_file: {config_file}\')\n                # end of if file does not exist on the disk\n                self.config_dict = json.loads(\n                    open(config_file, \'r\').read())\n        # end of trying to ensure the config_dict is ready\n\n        if not self.config_dict:\n            raise Exception(\n                \'Missing either a config_dict or a config_file to \'\n                \'create the IndicatorProcessor\')\n\n        self.last_ind_obj = None\n        self.ticker = ticker\n        self.ind_dict = {}\n        self.num_indicators = len(self.config_dict.get(\n            \'indicators\',\n            []))\n        self.label = label\n        if not self.label:\n            self.label = \'idprc\'\n\n        self.latest_report = {}\n        self.reports = []\n\n        self.verbose = verbose\n        self.verbose_indicators = verbose_indicators\n\n        if not self.verbose_indicators:\n            self.verbose_indicators = self.config_dict.get(\n                \'verbose_indicators\',\n                False)\n\n        self.build_indicators_for_config(\n            config_dict=self.config_dict)\n    # end of __init__\n\n    def get_last_ind_obj(\n            self):\n        """"""get_last_ind_obj""""""\n        return self.last_ind_obj\n    # end of get_last_ind_obj\n\n    def get_latest_report(\n            self,\n            algo_id=None,\n            ticker=None,\n            dataset=None):\n        """"""get_latest_report\n\n        Return the latest report as a method that can be\n        customized by a derived class from the\n        ``IndicatorProcessor``\n\n        :param algo_id: optional - string -\n            algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: optional - string - ticker\n        :param dataset: optional - a dictionary of\n            identifiers (for debugging) and\n            multiple pandas ``pd.DataFrame`` objects. Dictionary where keys\n            represent a label from one of the data sources (``IEX``,\n            ``Yahoo``, ``FinViz`` or other). Here is the supported\n            dataset structure for the process method:\n        """"""\n\n        return self.latest_report\n    # end of get_latest_report\n\n    def get_num_indicators(\n            self):\n        """"""get_num_indicators""""""\n        return self.num_indicators\n    # end of get_num_indicators\n\n    def get_label(\n            self):\n        """"""get_label""""""\n        return self.label\n    # end of get_label\n\n    def get_indicators(\n            self):\n        """"""get_indicators""""""\n        return self.ind_dict\n    # end of get_indicators\n\n    def build_indicators_for_config(\n            self,\n            config_dict):\n        """"""build_indicators_for_config\n\n        Convert the dictionary into an internal dictionary\n        for quickly processing results\n\n        :param config_dict: initailized algorithm config\n            dictionary\n        """"""\n\n        if \'indicators\' not in config_dict:\n            log.error(\'missing ""indicators"" list in the config_dict\')\n            return\n\n        if self.verbose:\n            log.info(\n                f\'{self.label} start - \'\n                f\'building indicators={self.num_indicators}\')\n\n        for idx, node in enumerate(config_dict[\'indicators\']):\n            percent_done = ae_consts.get_percent_done(\n                progress=(idx + 1),\n                total=self.num_indicators)\n            percent_label = (\n                f\'ticker={self.ticker} {percent_done} \'\n                f\'{idx+1}/{self.num_indicators}\')\n            # this will throw on errors parsing to make\n            # it easeir to debug\n            # before starting the algo and waiting for an error\n            # in the middle of a backtest\n            new_node = build_indicator.build_indicator_node(\n                node=node)\n            if new_node:\n                indicator_key_name = new_node[\'report\'][\'name\']\n                if self.verbose:\n                    log.info(\n                        f\'{self.label} - \'\n                        f\'preparing indicator={indicator_key_name} \'\n                        f\'node={new_node} {percent_label}\')\n                else:\n                    log.debug(\n                        f\'{self.label} - \'\n                        f\'preparing indicator={indicator_key_name} \'\n                        f\'{percent_label}\')\n                self.ind_dict[indicator_key_name] = new_node\n                self.ind_dict[indicator_key_name][\'obj\'] = None\n\n                base_class_indicator = node.get(\n                    \'base_class\',\n                    \'BaseIndicator\')\n\n                self.ind_dict[indicator_key_name][\'obj\'] = \\\n                    load_indicator.load_indicator_from_module(\n                        module_name=new_node[\'report\'][\'module_name\'],\n                        path_to_module=new_node[\'report\'][\'path_to_module\'],\n                        ind_dict=new_node,\n                        log_label=indicator_key_name,\n                        base_class_module_name=base_class_indicator,\n                        verbose=self.verbose_indicators)\n\n                log.debug(\n                    f\'{self.label} - \'\n                    f\'created indicator={indicator_key_name} \'\n                    f\'{percent_label}\')\n            else:\n                raise Exception(\n                    f\'{self.label} - \'\n                    f\'failed creating indicator {idx} node={node}\')\n        # end for all indicators in the config\n\n        if self.verbose:\n            log.info(\n                f\'{self.label} done - \'\n                f\'built={len(self.ind_dict)} \'\n                f\'from indicators={self.num_indicators}\')\n    # end of build_indicators_for_config\n\n    def process(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""process\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: a dictionary of identifiers (for debugging) and\n            multiple pandas ``pd.DataFrame`` objects. Dictionary where keys\n            represent a label from one of the data sources (``IEX``,\n            ``Yahoo``, ``FinViz`` or other). Here is the supported\n            dataset structure for the process method:\n        """"""\n        self.latest_report = {\n            \'id\': algo_id,\n            \'ticker\': ticker,\n            \'buys\': [],\n            \'sells\': [],\n            \'num_indicators\': self.num_indicators,\n            \'date\': dataset.get(\'date\', None)\n        }\n        for idx, ind_id in enumerate(self.ind_dict):\n            ind_node = self.ind_dict[ind_id]\n            ind_obj = ind_node[\'obj\']\n            percent_done = ae_consts.get_percent_done(\n                progress=(idx + 1),\n                total=self.num_indicators)\n            percent_label = (\n                f\'ticker={self.ticker} {percent_done} \'\n                f\'{idx+1}/{self.num_indicators}\')\n            ind_obj.reset_internals()\n            if self.verbose:\n                log.info(\n                    f\'{self.label} - {ind_obj.get_name()} \'\n                    f\'start {percent_label}\')\n            # this will throw on errors to help with debugging\n            self.last_ind_obj = ind_obj\n            ind_obj.handle_subscribed_dataset(\n                algo_id=algo_id,\n                ticker=ticker,\n                dataset=dataset)\n            new_report = ind_obj.get_report()\n            if self.verbose:\n                log.info(\n                    f\'{self.label} - {ind_obj.get_name()} \'\n                    f\'end {percent_label} \'\n                    f\'report: {ae_consts.ppj(new_report)}\')\n            self.latest_report.update(new_report)\n\n            is_buy_value = ind_obj.is_buy\n            is_sell_value = ind_obj.is_sell\n\n            """"""""\n            v1 indicator type: supported\n            binary decision support on buys and sells\n            (like an alert threshold that is on or off)\n\n            v2 indicator type: not supported\n            support for buy/sell value range\n            (like an alert threshold between a lower and upper bound)\n            """"""\n            if (hasattr(ind_obj, \'is_buy\') and\n                    hasattr(ind_obj, \'is_sell\')):\n                is_buy_value = ind_obj.is_buy\n                is_sell_value = ind_obj.is_sell\n\n            if is_buy_value == ae_consts.INDICATOR_BUY:\n                self.latest_report[\'buys\'].append({\n                    \'cell\': idx,\n                    \'name\': ind_obj.get_name(),\n                    \'id\': ind_id,\n                    \'report\': new_report})\n            elif is_sell_value == ae_consts.INDICATOR_SELL:\n                self.latest_report[\'sells\'].append({\n                    \'cell\': idx,\n                    \'name\': ind_obj.get_name(),\n                    \'id\': ind_id,\n                    \'report\': new_report})\n        # end of for all indicators\n\n        self.reports.append(self.latest_report)\n\n        # allow derived indicator processors to build custom reports\n        return self.get_latest_report(\n            algo_id=algo_id,\n            ticker=ticker,\n            dataset=dataset)\n    # end of process\n\n# end of IndicatorProcessor\n'"
analysis_engine/indicators/load_indicator_from_module.py,0,"b'""""""\nHelper for loading derived Indicators from a local module file\n""""""\n\nimport os\nimport inspect\nimport types\nimport importlib.machinery\nimport uuid\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.indicators.base_indicator as base_indicator\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef load_indicator_from_module(\n        module_name,\n        ind_dict,\n        path_to_module=None,\n        log_label=None,\n        base_class_module_name=\'BaseIndicator\',\n        verbose=False):\n    """"""load_indicator_from_module\n\n    Load a custom indicator from a file\n\n    :param module_name: string name of the indicator module\n        use in to load the module\n    :param path_to_module: optional - path to custom indicator file\n        (default is to use the\n        ``analysis_engine.indicators.base_indicator.BaseIndicator`` or\n        if set the ``ind_dict[\'module_path\']`` value)\n    :param ind_dict: dictionary of keyword arguments\n        to pass to the newly created derived Indicator\'s\n        constructor\n    :param log_label: optional - log tracking label\n        for helping to find this indicator\'s logs\n        (if not set the default name is the\n        ``module_name`` string value)\n    :param base_class_module_name: optional - string name\n        for using a non-standard indicator base class\n    :param verbose: optional - bool for more logging\n        (default is ``False``)\n    """"""\n\n    default_base_module_path = ae_consts.INDICATOR_BASE_MODULE_PATH\n\n    # modules need custom names to prevent\n    # runtime collisions/module stomping\n    # this allows building multiple indicator objects\n    # that use the same filename but in different\n    # locations on disk\n    use_module_name = (\n        f\'{module_name}_{str(uuid.uuid4())[0:8].replace(""-"", """")}\')\n\n    use_log_label = log_label\n    if not use_log_label:\n        use_log_label = use_module_name\n\n    if not path_to_module:\n        path_to_module = ind_dict.get(\n            \'module_path\',\n            None)\n\n    if \'verbose\' not in ind_dict:\n        ind_dict[\'verbose\'] = verbose\n\n    if not path_to_module:\n        return base_indicator.BaseIndicator(\n            config_dict=ind_dict,\n            name=use_log_label,\n            path_to_module=default_base_module_path)\n\n    if not os.path.exists(path_to_module):\n        raise Exception(\n            f\'{use_module_name} - did not find Indicator module at \'\n            f\'path={path_to_module} please confirm the file exists on disk \'\n            \'and if you are using a container, confirm it is \'\n            \'accessible within the container\')\n\n    loader = importlib.machinery.SourceFileLoader(\n        use_module_name,\n        path_to_module)\n    custom_indicator_module = types.ModuleType(\n        loader.name)\n    loader.exec_module(\n        custom_indicator_module)\n\n    found_base_object = False\n    class_member_in_module = None\n    for member in inspect.getmembers(custom_indicator_module):\n        if module_name in str(member):\n            found_base_object = True\n            class_member_in_module = member\n            break\n    # for all members in this custom module file\n\n    if not found_base_object:\n        raise Exception(\n            f\'{use_module_name} load_indicator_from_module error - \'\n            f\'did not find Indicator with base class={base_class_module_name} \'\n            f\'from module at path={path_to_module} - please confirm \'\n            \'the file has just one class that \'\n            \'inherits from the base Indicator class: \'\n            \'analysis_engine.indicators.base_indicator.BaseIndicator \'\n            \'and try again\')\n\n        err = (\n            f\'{use_module_name} load_indicator_from_module error - \'\n            \'unable to find custom indicator derived from \'\n            f\'module={base_class_module_name} at file path={path_to_module}\')\n        if path_to_module:\n            err = (\n                f\'{use_module_name} load_indicator_from_module error - \'\n                \'analysis_engine.indicators.base_indicator.BaseIndicator \'\n                \'was unable to find custom Indicator \'\n                f\'module={custom_indicator_module} with provided path to \\n \'\n                f\'file: {path_to_module} \\n\'\n                \'\\n\'\n                \'Please confirm \'\n                \'that the class inherits from the BaseIndicator \'\n                \'class like:\\n\'\n                \'\\n\'\n                \'import analysis_engine.indicators.base_indicator \'\n                \'as base_ind\\n\'\n                \'class MyIndicator(base_ind.BaseIndicator):\\n\'\n                \'\\n\'\n                \'If it is then please file an issue on github:\\n \'\n                \'https://github.com/AlgoTraders/stock-analysis-engine/\'\n                \'issues/new \\n\\nFor now this error results in a shutdown\'\n                \'\\n\')\n        log.error(err)\n        raise Exception(err)\n    # end of if did not find the module with the correct Base Class\n\n    if ind_dict.get(\'verbose\', False):\n        log.info(\n            f\'load - custom indicator module={use_module_name} \'\n            f\'from file={path_to_module} member={class_member_in_module}\')\n    ind = class_member_in_module[1](\n        config_dict=ind_dict,\n        name=use_log_label,\n        path_to_module=path_to_module)\n    if ind_dict.get(\'verbose\', False):\n        log.info(\n            f\'ready - custom indicator={ind.__class__.__name__} from \'\n            f\'module={use_module_name} from file={path_to_module} \'\n            f\'member={class_member_in_module}\')\n\n    return ind\n# end of load_indicator_from_module\n'"
analysis_engine/indicators/macd.py,0,"b'""""""\nCustom Moving Average Convergence Divergence - MACD\n\nhttps://www.investopedia.com/terms/a/adx.asp\n\nMomentum\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport analysis_engine.ae_talib as ae_talib\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.indicators.base_indicator as base_indicator\n\n\nclass IndicatorMACD(base_indicator.BaseIndicator):\n    """"""IndicatorMACD""""""\n\n    def __init__(\n            self,\n            **kwargs):\n        """"""__init__\n\n        Custom indicator example for showing a\n        ``MACD``\n        within an algo for analyzing intraday minute datasets\n\n        Please refer to the `analysis_engine.indicators.base_indicator.Ba\n        seIndicator source code for the latest supported parameters <ht\n        tps://github.com/AlgoTraders/stock-analysis-engine\n        /blob/master/\n        analysis_engine/indicators/base_indicator.py>`__\n\n        :param kwargs: keyword arguments\n        """"""\n        super().__init__(**kwargs)\n    # end of __init__\n\n    def get_configurables(\n            self,\n            **kwargs):\n        """"""get_configurables\n\n        helper for setting up algorithm configs for this indicator\n        and programmatically set the values based off the domain\n        rules\n\n        .. code-block:: python\n\n            from analysis_engine.indicators.macd \\\n                import IndicatorMACD\n            ind = IndicatorMACD(config_dict={\n                    \'verbose\': True\n                }).get_configurables()\n\n        :param kwargs: keyword args dictionary\n        """"""\n        self.ind_confs = []\n\n        # common:\n        self.build_base_configurables(\n            ind_type=\'momentum\',\n            category=\'technical\',\n            uses_data=self.config.get(\n                \'uses_data\',\n                \'minute\'),\n            version=1)\n\n        # custom:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'num_points\',\n            conf_type=\'int\',\n            max_value=200,\n            min_value=2,\n            default_value=100,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'fast_period\',\n            conf_type=\'float\',\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'slow_period\',\n            conf_type=\'float\',\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'signal_period\',\n            conf_type=\'float\',\n            inc_interval=1))\n\n        # output / reporting:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'macd_value\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'macd_signal\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'macd_hist\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'percent_value\',\n            conf_type=\'percent\',\n            is_output_only=True))\n\n        default_values_dict = {}\n        for node in self.ind_confs:\n            name = node[\'name\']\n            default_value = node.get(\n                \'default\',\n                None)\n            default_values_dict[name] = default_value\n\n        use_file = None\n        try:\n            if __file__:\n                use_file = __file__\n        except Exception:\n            use_file = None\n\n        self.starter_dict = {\n            \'name\': self.__class__.__name__.lower().replace(\n                \'indicator\',\n                \'\'),\n            \'module_path\': use_file,\n            \'category\': default_values_dict.get(\n                \'category\',\n                \'momentum\'),\n            \'type\': default_values_dict.get(\n                \'type\',\n                \'technical\'),\n            \'uses_data\': default_values_dict.get(\n                \'uses_data\',\n                \'minute\'),\n            \'verbose\': default_values_dict.get(\n                \'verbose\',\n                False)\n        }\n        self.starter_dict.update(default_values_dict)\n\n        self.lg(\n            f\'configurables={ae_consts.ppj(self.ind_confs)} for \'\n            f\'class={self.__class__.__name__} in file={use_file} \'\n            f\'starter:\\n {ae_consts.ppj(self.starter_dict)}\')\n\n        return self.ind_confs\n    # end of get_configurables\n\n    def get_starter_dict(\n            self):\n        if not self.starter_dict:\n            self.get_configurables()\n        return self.starter_dict\n    # end of get_starter_dict\n\n    def process(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""process\n\n        Derive custom indicator processing to determine buy and sell\n        conditions before placing orders. Just implement your own\n        ``process`` method.\n\n        Please refer to the TA Lib guides for details on building indicators:\n\n        - Overlap Studies\n          https://mrjbq7.github.io/ta-lib/func_groups/overlap_studies.html\n        - Momentum Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html\n        - Volume Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volume_indicators.html\n        - Volatility Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volatility_indicators.html\n        - Price Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/price_transform.html\n        - Cycle Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/cycle_indicators.html\n        - Pattern Recognition\n          https://mrjbq7.github.io/ta-lib/func_groups/pattern_recognition.html\n        - Statistic Functions\n          https://mrjbq7.github.io/ta-lib/func_groups/statistic_functions.html\n        - Math Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/math_transform.html\n        - Math Operators\n          https://mrjbq7.github.io/ta-lib/func_groups/math_operators.html\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: dictionary of ``pandas.DataFrame(s)`` to process\n        """"""\n\n        # set the algo config indicator \'uses_data\' to \'day\' or \'minute\'\n        df_status, self.use_df = self.get_subscribed_dataset(\n            dataset=dataset)\n\n        if df_status == ae_consts.EMPTY:\n            self.lg(\'process end - no data found\')\n            return\n\n        # notice the self.num_points is now a member variable\n        # because the BaseIndicator class\'s __init__\n        # converts any self.config keys into useable\n        # member variables automatically in your derived class\n        self.lg(\n            f\'process - num_points={self.num_points} \'\n            f\'df={len(self.use_df.index)}\')\n        """"""\n        (macd,\n         macdsignal,\n         macdhist) = MACD(\n            close,\n            fastperiod=12,\n            slowperiod=26,\n            signalperiod=9)\n        """"""\n        num_records = len(self.use_df.index)\n        if num_records > self.num_points:\n            cur_value = self.use_df[\'close\'].iloc[-1]\n            first_date = self.use_df[\'date\'].iloc[0]\n            end_date = self.use_df[\'date\'].iloc[-1]\n            start_row = num_records - self.num_points\n            self.use_df = self.use_df[start_row:num_records]\n            """"""\n            for idx, row in self.use_df[start_row:-1].iterrows():\n                high = row[\'high\']\n                low = row[\'low\']\n                open_val = row[\'open\']\n                close = row[\'close\']\n                row_date = row[\'date\']\n                self.lg(\n                    f\'{row_date} - high={high}, low={low}, \'\n                    f\'close={close}, period={self.num_points}\')\n            """"""\n            closes = self.use_df[\'close\'].values\n\n            (self.macd_value,\n             self.macd_signal,\n             self.macd_hist) = ae_consts.to_f(ae_talib.MACD(\n                close=closes,\n                fast_period=self.fast_period,\n                slow_period=self.slow_period,\n                signal_period=self.signal_period).iloc[-1])\n\n            """"""\n            Determine a buy or a sell as a label\n            """"""\n\n            if cur_value <= 0:\n                self.lg(f\'invalid current_value={cur_value}\')\n                return\n\n            self.close = cur_value\n            self.amount_to_close = ae_consts.to_f(\n                cur_value - self.macd_value)\n            self.percent_value = ae_consts.to_f(\n                self.amount_to_close / cur_value * 100.0)\n\n            self.is_buy = ae_consts.INDICATOR_IGNORE\n            self.is_sell = ae_consts.INDICATOR_IGNORE\n\n            if (self.buy_above_percent != -1 and\n                    self.percent_value > self.buy_above_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n            elif (self.buy_below_percent != -1 and\n                    self.percent_value > self.buy_below_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n\n            if (self.sell_above_percent != -1 and\n                    self.percent_value > self.sell_above_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n            elif (self.sell_below_percent != -1 and\n                    self.percent_value > self.sell_below_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n\n            self.lg(\n                f\'process end - {first_date} to {end_date} \'\n                f\'buy_below={self.buy_below_percent} \'\n                f\'buy_above={self.buy_above_percent} is_buy={self.is_buy} \'\n                f\'sell_below={self.sell_below_percent} \'\n                f\'sell_above={self.sell_above_percent} is_sell={self.is_sell}\')\n        else:\n            self.lg(\'process end\')\n    # end of process\n\n    def reset_internals(\n            self):\n        """"""reset_internals""""""\n        self.is_buy = ae_consts.INDICATOR_RESET\n        self.is_sell = ae_consts.INDICATOR_RESET\n    # end of reset_internals\n\n# end of IndicatorMACD\n\n\ndef get_indicator(\n        **kwargs):\n    """"""get_indicator\n\n    Make sure to define the ``get_indicator`` for your custom\n    algorithms to work as a backup with the ``sa.py`` tool...\n    Not anticipating issues, but if we do with importlib\n    this is the backup plan.\n\n    Please file an issue if you see something weird and would like\n    some help:\n    https://github.com/AlgoTraders/stock-analysis-engine/issues\n\n    :param kwargs: dictionary of keyword arguments\n    """"""\n    print(\'getting indicator\')\n    return IndicatorMACD(**kwargs)\n# end of get_indicator\n'"
analysis_engine/indicators/mfi.py,0,"b'""""""\nCustom Money Flow Index - MFI\n\nhttps://www.investopedia.com/terms/m/mfi.asp\n\nMomentum\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport pandas as pd\nimport analysis_engine.ae_talib as ae_talib\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.indicators.base_indicator as base_indicator\n\n\nclass IndicatorMFI(base_indicator.BaseIndicator):\n    """"""IndicatorMFI""""""\n\n    def __init__(\n            self,\n            **kwargs):\n        """"""__init__\n\n        Custom indicator example for showing a\n        ``MFI``\n        within an algo for analyzing intraday minute datasets\n\n        Please refer to the `analysis_engine.indicators.base_indicator.Ba\n        seIndicator source code for the latest supported parameters <ht\n        tps://github.com/AlgoTraders/stock-analysis-engine\n        /blob/master/\n        analysis_engine/indicators/base_indicator.py>`__\n\n        :param kwargs: keyword arguments\n        """"""\n        super().__init__(**kwargs)\n    # end of __init__\n\n    def get_configurables(\n            self,\n            **kwargs):\n        """"""get_configurables\n\n        helper for setting up algorithm configs for this indicator\n        and programmatically set the values based off the domain\n        rules\n\n        .. code-block:: python\n\n            from analysis_engine.indicators.mfi \\\n                import IndicatorMFI\n            ind = IndicatorMFI(config_dict={\n                    \'verbose\': True\n                }).get_configurables()\n\n        :param kwargs: keyword args dictionary\n        """"""\n        self.ind_confs = []\n\n        # common:\n        self.build_base_configurables(\n            ind_type=\'momentum\',\n            category=\'technical\',\n            uses_data=self.config.get(\n                \'uses_data\',\n                \'minute\'),\n            version=1)\n\n        # custom:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'num_points\',\n            conf_type=\'int\',\n            max_value=200,\n            min_value=2,\n            default_value=100,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n\n        # output / reporting:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'mfi_value\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'amount_to_close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'percent_value\',\n            conf_type=\'percent\',\n            is_output_only=True))\n\n        default_values_dict = {}\n        for node in self.ind_confs:\n            name = node[\'name\']\n            default_value = node.get(\n                \'default\',\n                None)\n            default_values_dict[name] = default_value\n\n        use_file = None\n        try:\n            if __file__:\n                use_file = __file__\n        except Exception:\n            use_file = None\n\n        self.starter_dict = {\n            \'name\': self.__class__.__name__.lower().replace(\n                \'indicator\',\n                \'\'),\n            \'module_path\': use_file,\n            \'category\': default_values_dict.get(\n                \'category\',\n                \'momentum\'),\n            \'type\': default_values_dict.get(\n                \'type\',\n                \'technical\'),\n            \'uses_data\': default_values_dict.get(\n                \'uses_data\',\n                \'minute\'),\n            \'verbose\': default_values_dict.get(\n                \'verbose\',\n                False)\n        }\n        self.starter_dict.update(default_values_dict)\n\n        self.lg(\n            f\'configurables={ae_consts.ppj(self.ind_confs)} for \'\n            f\'class={self.__class__.__name__} in file={use_file} \'\n            f\'starter:\\n {ae_consts.ppj(self.starter_dict)}\')\n\n        return self.ind_confs\n    # end of get_configurables\n\n    def get_starter_dict(\n            self):\n        if not self.starter_dict:\n            self.get_configurables()\n        return self.starter_dict\n    # end of get_starter_dict\n\n    def process(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""process\n\n        Derive custom indicator processing to determine buy and sell\n        conditions before placing orders. Just implement your own\n        ``process`` method.\n\n        Please refer to the TA Lib guides for details on building indicators:\n\n        - Overlap Studies\n          https://mrjbq7.github.io/ta-lib/func_groups/overlap_studies.html\n        - Momentum Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html\n        - Volume Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volume_indicators.html\n        - Volatility Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volatility_indicators.html\n        - Price Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/price_transform.html\n        - Cycle Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/cycle_indicators.html\n        - Pattern Recognition\n          https://mrjbq7.github.io/ta-lib/func_groups/pattern_recognition.html\n        - Statistic Functions\n          https://mrjbq7.github.io/ta-lib/func_groups/statistic_functions.html\n        - Math Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/math_transform.html\n        - Math Operators\n          https://mrjbq7.github.io/ta-lib/func_groups/math_operators.html\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: dictionary of ``pandas.DataFrame(s)`` to process\n        """"""\n\n        # set the algo config indicator \'uses_data\' to \'day\' or \'minute\'\n        df_status, self.use_df = self.get_subscribed_dataset(\n            dataset=dataset)\n\n        if df_status == ae_consts.EMPTY:\n            self.lg(\'process end - no data found\')\n            return\n\n        # notice the self.num_points is now a member variable\n        # because the BaseIndicator class\'s __init__\n        # converts any self.config keys into useable\n        # member variables automatically in your derived class\n        self.lg(\n            f\'process - num_points={self.num_points} \'\n            f\'df={len(self.use_df.index)}\')\n        """"""\n        real = MFI(high, low, close, timeperiod=14)\n        """"""\n        num_records = len(self.use_df.index)\n        if num_records > self.num_points:\n            cur_value = self.use_df[\'close\'].iloc[-1]\n            first_date = self.use_df[\'date\'].iloc[0]\n            end_date = self.use_df[\'date\'].iloc[-1]\n            start_row = num_records - self.num_points\n            self.use_df = self.use_df[start_row:num_records]\n            """"""\n            for idx, row in self.use_df[start_row:-1].iterrows():\n                high = row[\'high\']\n                low = row[\'low\']\n                open_val = row[\'open\']\n                close = row[\'close\']\n                row_date = row[\'date\']\n                self.lg(\n                    f\'{row_date} - high={high}, low={low}, \'\n                    f\'close={close}, period={self.num_points}\')\n            """"""\n            highs = self.use_df[\'high\'].values\n            lows = self.use_df[\'low\'].values\n            closes = self.use_df[\'close\'].values\n            volumes = pd.to_numeric(self.use_df[\'volume\'])\n\n            self.mfi_value = ae_consts.to_f(ae_talib.MFI(\n                high=highs,\n                low=lows,\n                close=closes,\n                volume=volumes,\n                timeperiod=self.num_points).iloc[-1])\n\n            """"""\n            Determine a buy or a sell as a label\n            """"""\n\n            if cur_value <= 0:\n                self.lg(f\'invalid current_value={cur_value}\')\n                return\n\n            self.close = cur_value\n            self.amount_to_close = ae_consts.to_f(\n                cur_value - self.mfi_value)\n            self.percent_value = ae_consts.to_f(\n                self.amount_to_close / cur_value * 100.0)\n\n            self.is_buy = ae_consts.INDICATOR_IGNORE\n            self.is_sell = ae_consts.INDICATOR_IGNORE\n\n            if (self.buy_above_percent != -1 and\n                    self.percent_value > self.buy_above_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n            elif (self.buy_below_percent != -1 and\n                    self.percent_value > self.buy_below_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n\n            if (self.sell_above_percent != -1 and\n                    self.percent_value > self.sell_above_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n            elif (self.sell_below_percent != -1 and\n                    self.percent_value > self.sell_below_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n\n            self.lg(\n                f\'process end - {first_date} to {end_date} \'\n                f\'buy_below={self.buy_below_percent} \'\n                f\'buy_above={self.buy_above_percent} is_buy={self.is_buy} \'\n                f\'sell_below={self.sell_below_percent} \'\n                f\'sell_above={self.sell_above_percent} is_sell={self.is_sell}\')\n        else:\n            self.lg(\'process end\')\n    # end of process\n\n    def reset_internals(\n            self):\n        """"""reset_internals""""""\n        self.is_buy = ae_consts.INDICATOR_RESET\n        self.is_sell = ae_consts.INDICATOR_RESET\n    # end of reset_internals\n\n# end of IndicatorMFI\n\n\ndef get_indicator(\n        **kwargs):\n    """"""get_indicator\n\n    Make sure to define the ``get_indicator`` for your custom\n    algorithms to work as a backup with the ``sa.py`` tool...\n    Not anticipating issues, but if we do with importlib\n    this is the backup plan.\n\n    Please file an issue if you see something weird and would like\n    some help:\n    https://github.com/AlgoTraders/stock-analysis-engine/issues\n\n    :param kwargs: dictionary of keyword arguments\n    """"""\n    print(\'getting indicator\')\n    return IndicatorMFI(**kwargs)\n# end of get_indicator\n'"
analysis_engine/indicators/mom.py,0,"b'""""""\nCustom Momentum - MOM\n\nMomentum\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport analysis_engine.ae_talib as ae_talib\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.indicators.base_indicator as base_indicator\n\n\nclass IndicatorMOM(base_indicator.BaseIndicator):\n    """"""IndicatorMOM""""""\n\n    def __init__(\n            self,\n            **kwargs):\n        """"""__init__\n\n        Custom indicator example for showing a\n        ``MOM``\n        within an algo for analyzing intraday minute datasets\n\n        Please refer to the `analysis_engine.indicators.base_indicator.Ba\n        seIndicator source code for the latest supported parameters <ht\n        tps://github.com/AlgoTraders/stock-analysis-engine/blob/master/\n        analysis_engine/indicators/base_indicator.py>`__\n\n        :param kwargs: keyword arguments\n        """"""\n        super().__init__(**kwargs)\n    # end of __init__\n\n    def get_configurables(\n            self,\n            **kwargs):\n        """"""get_configurables\n\n        helper for setting up algorithm configs for this indicator\n        and programmatically set the values based off the domain\n        rules\n\n        .. code-block:: python\n\n            from analysis_engine.indicators.mom \\\n                import IndicatorMOM\n            ind = IndicatorMOM(config_dict={\n                    \'verbose\': True\n                }).get_configurables()\n\n        :param kwargs: keyword args dictionary\n        """"""\n        self.ind_confs = []\n\n        # common:\n        self.build_base_configurables(\n            ind_type=\'momentum\',\n            category=\'technical\',\n            uses_data=self.config.get(\n                \'uses_data\',\n                \'minute\'),\n            version=1)\n\n        # custom:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'num_points\',\n            conf_type=\'int\',\n            max_value=200,\n            min_value=2,\n            default_value=100,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n\n        # output / reporting:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'mom_value\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'amount_to_close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'percent_value\',\n            conf_type=\'percent\',\n            is_output_only=True))\n\n        default_values_dict = {}\n        for node in self.ind_confs:\n            name = node[\'name\']\n            default_value = node.get(\n                \'default\',\n                None)\n            default_values_dict[name] = default_value\n\n        use_file = None\n        try:\n            if __file__:\n                use_file = __file__\n        except Exception:\n            use_file = None\n\n        self.starter_dict = {\n            \'name\': self.__class__.__name__.lower().replace(\n                \'indicator\',\n                \'\'),\n            \'module_path\': use_file,\n            \'category\': default_values_dict.get(\n                \'category\',\n                \'momentum\'),\n            \'type\': default_values_dict.get(\n                \'type\',\n                \'technical\'),\n            \'uses_data\': default_values_dict.get(\n                \'uses_data\',\n                \'minute\'),\n            \'verbose\': default_values_dict.get(\n                \'verbose\',\n                False)\n        }\n        self.starter_dict.update(default_values_dict)\n\n        self.lg(\n            f\'configurables={ae_consts.ppj(self.ind_confs)} for \'\n            f\'class={self.__class__.__name__} in file={use_file} \'\n            f\'starter:\\n {ae_consts.ppj(self.starter_dict)}\')\n\n        return self.ind_confs\n    # end of get_configurables\n\n    def get_starter_dict(\n            self):\n        if not self.starter_dict:\n            self.get_configurables()\n        return self.starter_dict\n    # end of get_starter_dict\n\n    def process(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""process\n\n        Derive custom indicator processing to determine buy and sell\n        conditions before placing orders. Just implement your own\n        ``process`` method.\n\n        Please refer to the TA Lib guides for details on building indicators:\n\n        - Overlap Studies\n          https://mrjbq7.github.io/ta-lib/func_groups/overlap_studies.html\n        - Momentum Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html\n        - Volume Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volume_indicators.html\n        - Volatility Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volatility_indicators.html\n        - Price Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/price_transform.html\n        - Cycle Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/cycle_indicators.html\n        - Pattern Recognition\n          https://mrjbq7.github.io/ta-lib/func_groups/pattern_recognition.html\n        - Statistic Functions\n          https://mrjbq7.github.io/ta-lib/func_groups/statistic_functions.html\n        - Math Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/math_transform.html\n        - Math Operators\n          https://mrjbq7.github.io/ta-lib/func_groups/math_operators.html\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: dictionary of ``pandas.DataFrame(s)`` to process\n        """"""\n\n        # set the algo config indicator \'uses_data\' to \'day\' or \'minute\'\n        df_status, self.use_df = self.get_subscribed_dataset(\n            dataset=dataset)\n\n        if df_status == ae_consts.EMPTY:\n            self.lg(\'process end - no data found\')\n            return\n\n        # notice the self.num_points is now a member variable\n        # because the BaseIndicator class\'s __init__\n        # converts any self.config keys into useable\n        # member variables automatically in your derived class\n        self.lg(\n            f\'process - num_points={self.num_points} \'\n            f\'df={len(self.use_df.index)}\')\n        """"""\n        real = MOM(close, timeperiod=10)\n        """"""\n        num_records = len(self.use_df.index)\n        if num_records > self.num_points:\n            cur_value = self.use_df[\'close\'].iloc[-1]\n            first_date = self.use_df[\'date\'].iloc[0]\n            end_date = self.use_df[\'date\'].iloc[-1]\n            start_row = num_records - self.num_points\n            self.use_df = self.use_df[start_row:num_records]\n            """"""\n            for idx, row in self.use_df[start_row:-1].iterrows():\n                high = row[\'high\']\n                low = row[\'low\']\n                open_val = row[\'open\']\n                close = row[\'close\']\n                row_date = row[\'date\']\n                self.lg(\n                    f\'{row_date} - high={high}, low={low}, \'\n                    f\'close={close}, period={self.num_points}\')\n            """"""\n            closes = self.use_df[\'close\'].values\n\n            self.mom_value = ae_consts.to_f(ae_talib.MOM(\n                close=closes,\n                timeperiod=self.num_points)[-1])\n\n            """"""\n            Determine a buy or a sell as a label\n            """"""\n\n            if cur_value <= 0:\n                self.lg(f\'invalid current_value={cur_value}\')\n                return\n\n            self.close = cur_value\n            self.amount_to_close = ae_consts.to_f(\n                cur_value - self.mom_value)\n            self.percent_value = ae_consts.to_f(\n                self.amount_to_close / cur_value * 100.0)\n\n            self.is_buy = ae_consts.INDICATOR_IGNORE\n            self.is_sell = ae_consts.INDICATOR_IGNORE\n\n            if (self.buy_above_percent != -1 and\n                    self.percent_value > self.buy_above_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n            elif (self.buy_below_percent != -1 and\n                    self.percent_value > self.buy_below_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n\n            if (self.sell_above_percent != -1 and\n                    self.percent_value > self.sell_above_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n            elif (self.sell_below_percent != -1 and\n                    self.percent_value > self.sell_below_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n\n            self.lg(\n                f\'process end - {first_date} to {end_date} \'\n                f\'buy_below={self.buy_below_percent} \'\n                f\'buy_above={self.buy_above_percent} is_buy={self.is_buy} \'\n                f\'sell_below={self.sell_below_percent} \'\n                f\'sell_above={self.sell_above_percent} is_sell={self.is_sell}\')\n        else:\n            self.lg(\'process end\')\n    # end of process\n\n    def reset_internals(\n            self):\n        """"""reset_internals""""""\n        self.is_buy = ae_consts.INDICATOR_RESET\n        self.is_sell = ae_consts.INDICATOR_RESET\n    # end of reset_internals\n\n# end of IndicatorMOM\n\n\ndef get_indicator(\n        **kwargs):\n    """"""get_indicator\n\n    Make sure to define the ``get_indicator`` for your custom\n    algorithms to work as a backup with the ``sa.py`` tool...\n    Not anticipating issues, but if we do with importlib\n    this is the backup plan.\n\n    Please file an issue if you see something weird and would like\n    some help:\n    https://github.com/AlgoTraders/stock-analysis-engine/issues\n\n    :param kwargs: dictionary of keyword arguments\n    """"""\n    print(\'getting indicator\')\n    return IndicatorMOM(**kwargs)\n# end of get_indicator\n'"
analysis_engine/indicators/natr.py,0,"b'""""""\nCustom Normalized Average True Range - NATR\n\nhttps://www.investopedia.com/terms/a/atr.asp\n\nVolatility\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport analysis_engine.ae_talib as ae_talib\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.indicators.base_indicator as base_indicator\n\n\nclass IndicatorNATR(base_indicator.BaseIndicator):\n    """"""IndicatorNATR""""""\n\n    def __init__(\n            self,\n            **kwargs):\n        """"""__init__\n\n        Custom indicator example for showing a\n        ``NATR``\n        within an algo for analyzing intraday minute datasets\n\n        Please refer to the `analysis_engine.indicators.base_indicator.Ba\n        seIndicator source code for the latest supported parameters <ht\n        tps://github.com/AlgoTraders/stock-analysis-engine/blob/master/\n        analysis_engine/indicators/base_indicator.py>`__\n\n        :param kwargs: keyword arguments\n        """"""\n        super().__init__(**kwargs)\n    # end of __init__\n\n    def get_configurables(\n            self,\n            **kwargs):\n        """"""get_configurables\n\n        helper for setting up algorithm configs for this indicator\n        and programmatically set the values based off the domain\n        rules\n\n        .. code-block:: python\n\n            from analysis_engine.indicators.natr \\\n                import IndicatorNATR\n            ind = IndicatorNATR(config_dict={\n                    \'verbose\': True\n                }).get_configurables()\n\n        :param kwargs: keyword args dictionary\n        """"""\n        self.ind_confs = []\n\n        # common:\n        self.build_base_configurables(\n            ind_type=\'volatility\',\n            category=\'technical\',\n            uses_data=self.config.get(\n                \'uses_data\',\n                \'minute\'),\n            version=1)\n\n        # custom:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'num_points\',\n            conf_type=\'int\',\n            max_value=200,\n            min_value=2,\n            default_value=100,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n\n        # output / reporting:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'natr_value\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'amount_to_close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'percent_value\',\n            conf_type=\'percent\',\n            is_output_only=True))\n\n        default_values_dict = {}\n        for node in self.ind_confs:\n            name = node[\'name\']\n            default_value = node.get(\n                \'default\',\n                None)\n            default_values_dict[name] = default_value\n\n        use_file = None\n        try:\n            if __file__:\n                use_file = __file__\n        except Exception:\n            use_file = None\n\n        self.starter_dict = {\n            \'name\': self.__class__.__name__.lower().replace(\n                \'indicator\',\n                \'\'),\n            \'module_path\': use_file,\n            \'category\': default_values_dict.get(\n                \'category\',\n                \'momentum\'),\n            \'type\': default_values_dict.get(\n                \'type\',\n                \'technical\'),\n            \'uses_data\': default_values_dict.get(\n                \'uses_data\',\n                \'minute\'),\n            \'verbose\': default_values_dict.get(\n                \'verbose\',\n                False)\n        }\n        self.starter_dict.update(default_values_dict)\n\n        self.lg(\n            f\'configurables={ae_consts.ppj(self.ind_confs)} for \'\n            f\'class={self.__class__.__name__} in file={use_file} \'\n            f\'starter:\\n {ae_consts.ppj(self.starter_dict)}\')\n\n        return self.ind_confs\n    # end of get_configurables\n\n    def get_starter_dict(\n            self):\n        if not self.starter_dict:\n            self.get_configurables()\n        return self.starter_dict\n    # end of get_starter_dict\n\n    def process(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""process\n\n        Derive custom indicator processing to determine buy and sell\n        conditions before placing orders. Just implement your own\n        ``process`` method.\n\n        Please refer to the TA Lib guides for details on building indicators:\n\n        - Overlap Studies\n          https://mrjbq7.github.io/ta-lib/func_groups/overlap_studies.html\n        - Momentum Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html\n        - Volume Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volume_indicators.html\n        - Volatility Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volatility_indicators.html\n        - Price Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/price_transform.html\n        - Cycle Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/cycle_indicators.html\n        - Pattern Recognition\n          https://mrjbq7.github.io/ta-lib/func_groups/pattern_recognition.html\n        - Statistic Functions\n          https://mrjbq7.github.io/ta-lib/func_groups/statistic_functions.html\n        - Math Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/math_transform.html\n        - Math Operators\n          https://mrjbq7.github.io/ta-lib/func_groups/math_operators.html\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: dictionary of ``pandas.DataFrame(s)`` to process\n        """"""\n\n        # set the algo config indicator \'uses_data\' to \'day\' or \'minute\'\n        df_status, self.use_df = self.get_subscribed_dataset(\n            dataset=dataset)\n\n        if df_status == ae_consts.EMPTY:\n            self.lg(\'process end - no data found\')\n            return\n\n        # notice the self.num_points is now a member variable\n        # because the BaseIndicator class\'s __init__\n        # converts any self.config keys into useable\n        # member variables automatically in your derived class\n        self.lg(\n            f\'process - num_points={self.num_points} \'\n            f\'df={len(self.use_df.index)}\')\n        """"""\n        real = NATR(high, low, close, timeperiod=14)\n        """"""\n        num_records = len(self.use_df.index)\n        if num_records > self.num_points:\n            cur_value = self.use_df[\'close\'].iloc[-1]\n            first_date = self.use_df[\'date\'].iloc[0]\n            end_date = self.use_df[\'date\'].iloc[-1]\n            start_row = num_records - self.num_points\n            self.use_df = self.use_df[start_row:num_records]\n            """"""\n            for idx, row in self.use_df[start_row:-1].iterrows():\n                high = row[\'high\']\n                low = row[\'low\']\n                open_val = row[\'open\']\n                close = row[\'close\']\n                row_date = row[\'date\']\n                self.lg(\n                    f\'{row_date} - high={high}, low={low}, \'\n                    f\'close={close}, period={self.num_points}\')\n            """"""\n            highs = self.use_df[\'high\'].values\n            lows = self.use_df[\'low\'].values\n            closes = self.use_df[\'close\'].values\n\n            self.natr_value = ae_consts.to_f(ae_talib.NATR(\n                high=highs,\n                low=lows,\n                close=closes,\n                timeperiod=self.num_points)[-1])\n\n            """"""\n            Determine a buy or a sell as a label\n            """"""\n\n            if cur_value <= 0:\n                self.lg(f\'invalid current_value={cur_value}\')\n                return\n\n            self.close = cur_value\n            self.amount_to_close = ae_consts.to_f(\n                cur_value - self.natr_value)\n            self.percent_value = ae_consts.to_f(\n                self.amount_to_close / cur_value * 100.0)\n\n            self.is_buy = ae_consts.INDICATOR_IGNORE\n            self.is_sell = ae_consts.INDICATOR_IGNORE\n\n            if (self.buy_above_percent != -1 and\n                    self.percent_value > self.buy_above_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n            elif (self.buy_below_percent != -1 and\n                    self.percent_value > self.buy_below_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n\n            if (self.sell_above_percent != -1 and\n                    self.percent_value > self.sell_above_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n            elif (self.sell_below_percent != -1 and\n                    self.percent_value > self.sell_below_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n\n            self.lg(\n                f\'process end - {first_date} to {end_date} \'\n                f\'buy_below={self.buy_below_percent} \'\n                f\'buy_above={self.buy_above_percent} is_buy={self.is_buy} \'\n                f\'sell_below={self.sell_below_percent} \'\n                f\'sell_above={self.sell_above_percent} is_sell={self.is_sell}\')\n        else:\n            self.lg(\'process end\')\n    # end of process\n\n    def reset_internals(\n            self):\n        """"""reset_internals""""""\n        self.is_buy = ae_consts.INDICATOR_RESET\n        self.is_sell = ae_consts.INDICATOR_RESET\n    # end of reset_internals\n\n# end of IndicatorNATR\n\n\ndef get_indicator(\n        **kwargs):\n    """"""get_indicator\n\n    Make sure to define the ``get_indicator`` for your custom\n    algorithms to work as a backup with the ``sa.py`` tool...\n    Not anticipating issues, but if we do with importlib\n    this is the backup plan.\n\n    Please file an issue if you see something weird and would like\n    some help:\n    https://github.com/AlgoTraders/stock-analysis-engine/issues\n\n    :param kwargs: dictionary of keyword arguments\n    """"""\n    print(\'getting indicator\')\n    return IndicatorNATR(**kwargs)\n# end of get_indicator\n'"
analysis_engine/indicators/obv.py,0,"b'""""""\nCustom On Balance Volume\n\nhttps://www.investopedia.com/terms/o/onbalancevolume.asp\n\nVolume\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport pandas as pd\nimport analysis_engine.ae_talib as ae_talib\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.indicators.base_indicator as base_indicator\n\n\nclass IndicatorOnBalanceVolume(base_indicator.BaseIndicator):\n    """"""IndicatorOnBalanceVolume""""""\n\n    def __init__(\n            self,\n            **kwargs):\n        """"""__init__\n\n        Custom indicator example for showing a\n        ``OnBalanceVolume``\n        within an algo for analyzing intraday minute datasets\n\n        Please refer to the `analysis_engine.indicators.base_indicator.Ba\n        seIndicator source code for the latest supported parameters <ht\n        tps://github.com/AlgoTraders/stock-analysis-engine/blob/master/\n        analysis_engine/indicators/base_indicator.py>`__\n\n        :param kwargs: keyword arguments\n        """"""\n        super().__init__(**kwargs)\n    # end of __init__\n\n    def get_configurables(\n            self,\n            **kwargs):\n        """"""get_configurables\n\n        helper for setting up algorithm configs for this indicator\n        and programmatically set the values based off the domain\n        rules\n\n        .. code-block:: python\n\n            from analysis_engine.indicators.obv \\\n                import IndicatorOnBalanceVolume\n            ind = IndicatorOnBalanceVolume(config_dict={\n                    \'verbose\': True\n                }).get_configurables()\n\n        :param kwargs: keyword args dictionary\n        """"""\n        self.ind_confs = []\n\n        # common:\n        self.build_base_configurables(\n            ind_type=\'volume\',\n            category=\'technical\',\n            uses_data=self.config.get(\n                \'uses_data\',\n                \'minute\'),\n            version=1)\n\n        # custom:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'num_points\',\n            conf_type=\'int\',\n            max_value=200,\n            min_value=2,\n            default_value=100,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n\n        # output / reporting:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'obv_value\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'amount_to_close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'percent_value\',\n            conf_type=\'percent\',\n            is_output_only=True))\n\n        default_values_dict = {}\n        for node in self.ind_confs:\n            name = node[\'name\']\n            default_value = node.get(\n                \'default\',\n                None)\n            default_values_dict[name] = default_value\n\n        use_file = None\n        try:\n            if __file__:\n                use_file = __file__\n        except Exception:\n            use_file = None\n\n        self.starter_dict = {\n            \'name\': self.__class__.__name__.lower().replace(\n                \'indicator\',\n                \'\'),\n            \'module_path\': use_file,\n            \'category\': default_values_dict.get(\n                \'category\',\n                \'momentum\'),\n            \'type\': default_values_dict.get(\n                \'type\',\n                \'technical\'),\n            \'uses_data\': default_values_dict.get(\n                \'uses_data\',\n                \'minute\'),\n            \'verbose\': default_values_dict.get(\n                \'verbose\',\n                False)\n        }\n        self.starter_dict.update(default_values_dict)\n\n        self.lg(\n            f\'configurables={ae_consts.ppj(self.ind_confs)} for \'\n            f\'class={self.__class__.__name__} in file={use_file} \'\n            f\'starter:\\n {ae_consts.ppj(self.starter_dict)}\')\n\n        return self.ind_confs\n    # end of get_configurables\n\n    def get_starter_dict(\n            self):\n        if not self.starter_dict:\n            self.get_configurables()\n        return self.starter_dict\n    # end of get_starter_dict\n\n    def process(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""process\n\n        Derive custom indicator processing to determine buy and sell\n        conditions before placing orders. Just implement your own\n        ``process`` method.\n\n        Please refer to the TA Lib guides for details on building indicators:\n\n        - Overlap Studies\n          https://mrjbq7.github.io/ta-lib/func_groups/overlap_studies.html\n        - Momentum Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html\n        - Volume Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volume_indicators.html\n        - Volatility Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volatility_indicators.html\n        - Price Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/price_transform.html\n        - Cycle Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/cycle_indicators.html\n        - Pattern Recognition\n          https://mrjbq7.github.io/ta-lib/func_groups/pattern_recognition.html\n        - Statistic Functions\n          https://mrjbq7.github.io/ta-lib/func_groups/statistic_functions.html\n        - Math Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/math_transform.html\n        - Math Operators\n          https://mrjbq7.github.io/ta-lib/func_groups/math_operators.html\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: dictionary of ``pandas.DataFrame(s)`` to process\n        """"""\n\n        # set the algo config indicator \'uses_data\' to \'day\' or \'minute\'\n        df_status, self.use_df = self.get_subscribed_dataset(\n            dataset=dataset)\n\n        if df_status == ae_consts.EMPTY:\n            self.lg(\'process end - no data found\')\n            return\n\n        # notice the self.num_points is now a member variable\n        # because the BaseIndicator class\'s __init__\n        # converts any self.config keys into useable\n        # member variables automatically in your derived class\n        self.lg(\n            f\'process - num_points={self.num_points} \'\n            f\'df={len(self.use_df.index)}\')\n        """"""\n        real = OBV(close, volume)\n        """"""\n        num_records = len(self.use_df.index)\n        if num_records > self.num_points:\n            cur_value = self.use_df[\'close\'].iloc[-1]\n            first_date = self.use_df[\'date\'].iloc[0]\n            end_date = self.use_df[\'date\'].iloc[-1]\n            start_row = num_records - self.num_points\n            self.use_df = self.use_df[start_row:num_records]\n            """"""\n            for idx, row in self.use_df[start_row:-1].iterrows():\n                high = row[\'high\']\n                low = row[\'low\']\n                open_val = row[\'open\']\n                close = row[\'close\']\n                row_date = row[\'date\']\n                self.lg(\n                    f\'{row_date} - high={high}, low={low}, \'\n                    f\'close={close}, period={self.num_points}\')\n            """"""\n            closes = self.use_df[\'close\'].values\n            volumes = pd.to_numeric(self.use_df[\'volume\'])\n\n            self.obv_value = ae_consts.to_f(ae_talib.OBV(\n                 value=closes,\n                 volume=volumes).iloc[-1])\n\n            """"""\n            Determine a buy or a sell as a label\n            """"""\n\n            if cur_value <= 0:\n                self.lg(f\'invalid current_value={cur_value}\')\n                return\n\n            self.close = cur_value\n            self.amount_to_close = ae_consts.to_f(cur_value - self.obv_value)\n            self.percent_value = ae_consts.to_f(\n                self.amount_to_close / cur_value * 100.0)\n\n            self.is_buy = ae_consts.INDICATOR_IGNORE\n            self.is_sell = ae_consts.INDICATOR_IGNORE\n\n            if (self.buy_above_percent != -1 and\n                    self.percent_value > self.buy_above_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n            elif (self.buy_below_percent != -1 and\n                    self.percent_value > self.buy_below_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n\n            if (self.sell_above_percent != -1 and\n                    self.percent_value > self.sell_above_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n            elif (self.sell_below_percent != -1 and\n                    self.percent_value > self.sell_below_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n\n            self.lg(\n                f\'process end - {first_date} to {end_date} \'\n                f\'buy_below={self.buy_below_percent} \'\n                f\'buy_above={self.buy_above_percent} is_buy={self.is_buy} \'\n                f\'sell_below={self.sell_below_percent} \'\n                f\'sell_above={self.sell_above_percent} is_sell={self.is_sell}\')\n        else:\n            self.lg(\'process end\')\n    # end of process\n\n    def reset_internals(\n            self):\n        """"""reset_internals""""""\n        self.is_buy = ae_consts.INDICATOR_RESET\n        self.is_sell = ae_consts.INDICATOR_RESET\n    # end of reset_internals\n\n# end of IndicatorOnBalanceVolume\n\n\ndef get_indicator(\n        **kwargs):\n    """"""get_indicator\n\n    Make sure to define the ``get_indicator`` for your custom\n    algorithms to work as a backup with the ``sa.py`` tool...\n    Not anticipating issues, but if we do with importlib\n    this is the backup plan.\n\n    Please file an issue if you see something weird and would like\n    some help:\n    https://github.com/AlgoTraders/stock-analysis-engine/issues\n\n    :param kwargs: dictionary of keyword arguments\n    """"""\n    print(\'getting indicator\')\n    return IndicatorOnBalanceVolume(**kwargs)\n# end of get_indicator\n'"
analysis_engine/indicators/roc.py,0,"b'""""""\nCustom Price of Rate of Change - ROC\n\nhttps://www.investopedia.com/terms/p/pricerateofchange.asp\n\nMomentum\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport analysis_engine.ae_talib as ae_talib\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.indicators.base_indicator as base_indicator\n\n\nclass IndicatorROC(base_indicator.BaseIndicator):\n    """"""IndicatorROC""""""\n\n    def __init__(\n            self,\n            **kwargs):\n        """"""__init__\n\n        Custom indicator example for showing a\n        ``ROC``\n        within an algo for analyzing intraday minute datasets\n\n        Please refer to the `analysis_engine.indicators.base_indicator.Ba\n        seIndicator source code for the latest supported parameters <ht\n        tps://github.com/AlgoTraders/stock-analysis-engine/blob/master/\n        analysis_engine/indicators/base_indicator.py>`__\n\n        :param kwargs: keyword arguments\n        """"""\n        super().__init__(**kwargs)\n    # end of __init__\n\n    def get_configurables(\n            self,\n            **kwargs):\n        """"""get_configurables\n\n        helper for setting up algorithm configs for this indicator\n        and programmatically set the values based off the domain\n        rules\n\n        .. code-block:: python\n\n            from analysis_engine.indicators.roc \\\n                import IndicatorROC\n            ind = IndicatorROC(config_dict={\n                    \'verbose\': True\n                }).get_configurables()\n\n        :param kwargs: keyword args dictionary\n        """"""\n        self.ind_confs = []\n\n        # common:\n        self.build_base_configurables(\n            ind_type=\'momentum\',\n            category=\'technical\',\n            uses_data=self.config.get(\n                \'uses_data\',\n                \'minute\'),\n            version=1)\n\n        # custom:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'num_points\',\n            conf_type=\'int\',\n            max_value=200,\n            min_value=2,\n            default_value=100,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n\n        # output / reporting:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'roc_value\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'amount_to_close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'percent_value\',\n            conf_type=\'percent\',\n            is_output_only=True))\n\n        default_values_dict = {}\n        for node in self.ind_confs:\n            name = node[\'name\']\n            default_value = node.get(\n                \'default\',\n                None)\n            default_values_dict[name] = default_value\n\n        use_file = None\n        try:\n            if __file__:\n                use_file = __file__\n        except Exception:\n            use_file = None\n\n        self.starter_dict = {\n            \'name\': self.__class__.__name__.lower().replace(\n                \'indicator\',\n                \'\'),\n            \'module_path\': use_file,\n            \'category\': default_values_dict.get(\n                \'category\',\n                \'momentum\'),\n            \'type\': default_values_dict.get(\n                \'type\',\n                \'technical\'),\n            \'uses_data\': default_values_dict.get(\n                \'uses_data\',\n                \'minute\'),\n            \'verbose\': default_values_dict.get(\n                \'verbose\',\n                False)\n        }\n        self.starter_dict.update(default_values_dict)\n\n        self.lg(\n            f\'configurables={ae_consts.ppj(self.ind_confs)} for \'\n            f\'class={self.__class__.__name__} in file={use_file} \'\n            f\'starter:\\n {ae_consts.ppj(self.starter_dict)}\')\n\n        return self.ind_confs\n    # end of get_configurables\n\n    def get_starter_dict(\n            self):\n        if not self.starter_dict:\n            self.get_configurables()\n        return self.starter_dict\n    # end of get_starter_dict\n\n    def process(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""process\n\n        Derive custom indicator processing to determine buy and sell\n        conditions before placing orders. Just implement your own\n        ``process`` method.\n\n        Please refer to the TA Lib guides for details on building indicators:\n\n        - Overlap Studies\n          https://mrjbq7.github.io/ta-lib/func_groups/overlap_studies.html\n        - Momentum Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html\n        - Volume Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volume_indicators.html\n        - Volatility Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volatility_indicators.html\n        - Price Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/price_transform.html\n        - Cycle Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/cycle_indicators.html\n        - Pattern Recognition\n          https://mrjbq7.github.io/ta-lib/func_groups/pattern_recognition.html\n        - Statistic Functions\n          https://mrjbq7.github.io/ta-lib/func_groups/statistic_functions.html\n        - Math Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/math_transform.html\n        - Math Operators\n          https://mrjbq7.github.io/ta-lib/func_groups/math_operators.html\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: dictionary of ``pandas.DataFrame(s)`` to process\n        """"""\n\n        # set the algo config indicator \'uses_data\' to \'day\' or \'minute\'\n        df_status, self.use_df = self.get_subscribed_dataset(\n            dataset=dataset)\n\n        if df_status == ae_consts.EMPTY:\n            self.lg(\'process end - no data found\')\n            return\n\n        # notice the self.num_points is now a member variable\n        # because the BaseIndicator class\'s __init__\n        # converts any self.config keys into useable\n        # member variables automatically in your derived class\n        self.lg(\n            f\'process - num_points={self.num_points} \'\n            f\'df={len(self.use_df.index)}\')\n        """"""\n        real = ROC(close, timeperiod=10)\n        """"""\n        num_records = len(self.use_df.index)\n        if num_records > self.num_points:\n            cur_value = self.use_df[\'close\'].iloc[-1]\n            first_date = self.use_df[\'date\'].iloc[0]\n            end_date = self.use_df[\'date\'].iloc[-1]\n            start_row = num_records - self.num_points\n            self.use_df = self.use_df[start_row:num_records]\n            """"""\n            for idx, row in self.use_df[start_row:-1].iterrows():\n                high = row[\'high\']\n                low = row[\'low\']\n                open_val = row[\'open\']\n                close = row[\'close\']\n                row_date = row[\'date\']\n                self.lg(\n                    f\'{row_date} - high={high}, low={low}, \'\n                    f\'close={close}, period={self.num_points}\')\n            """"""\n            closes = self.use_df[\'close\'].values\n\n            self.roc_value = ae_consts.to_f(ae_talib.ROC(\n                close=closes,\n                timeperiod=self.num_points)[-1])\n\n            """"""\n            Determine a buy or a sell as a label\n            """"""\n\n            if cur_value <= 0:\n                self.lg(f\'invalid current_value={cur_value}\')\n                return\n\n            self.close = cur_value\n            self.amount_to_close = ae_consts.to_f(\n                cur_value - self.roc_value)\n            self.percent_value = ae_consts.to_f(\n                self.amount_to_close / cur_value * 100.0)\n\n            self.is_buy = ae_consts.INDICATOR_IGNORE\n            self.is_sell = ae_consts.INDICATOR_IGNORE\n\n            if (self.buy_above_percent != -1 and\n                    self.percent_value > self.buy_above_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n            elif (self.buy_below_percent != -1 and\n                    self.percent_value > self.buy_below_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n\n            if (self.sell_above_percent != -1 and\n                    self.percent_value > self.sell_above_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n            elif (self.sell_below_percent != -1 and\n                    self.percent_value > self.sell_below_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n\n            self.lg(\n                f\'process end - {first_date} to {end_date} \'\n                f\'buy_below={self.buy_below_percent} \'\n                f\'buy_above={self.buy_above_percent} is_buy={self.is_buy} \'\n                f\'sell_below={self.sell_below_percent} \'\n                f\'sell_above={self.sell_above_percent} is_sell={self.is_sell}\')\n        else:\n            self.lg(\'process end\')\n    # end of process\n\n    def reset_internals(\n            self):\n        """"""reset_internals""""""\n        self.is_buy = ae_consts.INDICATOR_RESET\n        self.is_sell = ae_consts.INDICATOR_RESET\n    # end of reset_internals\n\n# end of IndicatorROC\n\n\ndef get_indicator(\n        **kwargs):\n    """"""get_indicator\n\n    Make sure to define the ``get_indicator`` for your custom\n    algorithms to work as a backup with the ``sa.py`` tool...\n    Not anticipating issues, but if we do with importlib\n    this is the backup plan.\n\n    Please file an issue if you see something weird and would like\n    some help:\n    https://github.com/AlgoTraders/stock-analysis-engine/issues\n\n    :param kwargs: dictionary of keyword arguments\n    """"""\n    print(\'getting indicator\')\n    return IndicatorROC(**kwargs)\n# end of get_indicator\n'"
analysis_engine/indicators/rsi.py,0,"b'""""""\nCustom Relative Strength Index - RSI\n\nhttps://www.investopedia.com/terms/r/rsi.asp\n\nMomentum\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport analysis_engine.ae_talib as ae_talib\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.indicators.base_indicator as base_indicator\n\n\nclass IndicatorRSI(base_indicator.BaseIndicator):\n    """"""IndicatorRSI""""""\n\n    def __init__(\n            self,\n            **kwargs):\n        """"""__init__\n\n        Custom indicator example for showing a\n        ``RSI``\n        within an algo for analyzing intraday minute datasets\n\n        Please refer to the `analysis_engine.indicators.base_indicator.Ba\n        seIndicator source code for the latest supported parameters <ht\n        tps://github.com/AlgoTraders/stock-analysis-engine/blob/master/\n        analysis_engine/indicators/base_indicator.py>`__\n\n        :param kwargs: keyword arguments\n        """"""\n        super().__init__(**kwargs)\n    # end of __init__\n\n    def get_configurables(\n            self,\n            **kwargs):\n        """"""get_configurables\n\n        helper for setting up algorithm configs for this indicator\n        and programmatically set the values based off the domain\n        rules\n\n        .. code-block:: python\n\n            from analysis_engine.indicators.rsi \\\n                import IndicatorRSI\n            ind = IndicatorRSI(config_dict={\n                    \'verbose\': True\n                }).get_configurables()\n\n        :param kwargs: keyword args dictionary\n        """"""\n        self.ind_confs = []\n\n        # common:\n        self.build_base_configurables(\n            ind_type=\'momentum\',\n            category=\'technical\',\n            uses_data=self.config.get(\n                \'uses_data\',\n                \'minute\'),\n            version=1)\n\n        # custom:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'num_points\',\n            conf_type=\'int\',\n            max_value=200,\n            min_value=2,\n            default_value=100,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n\n        # output / reporting:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'rsi_value\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'amount_to_close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'percent_value\',\n            conf_type=\'percent\',\n            is_output_only=True))\n\n        default_values_dict = {}\n        for node in self.ind_confs:\n            name = node[\'name\']\n            default_value = node.get(\n                \'default\',\n                None)\n            default_values_dict[name] = default_value\n\n        use_file = None\n        try:\n            if __file__:\n                use_file = __file__\n        except Exception:\n            use_file = None\n\n        self.starter_dict = {\n            \'name\': self.__class__.__name__.lower().replace(\n                \'indicator\',\n                \'\'),\n            \'module_path\': use_file,\n            \'category\': default_values_dict.get(\n                \'category\',\n                \'momentum\'),\n            \'type\': default_values_dict.get(\n                \'type\',\n                \'technical\'),\n            \'uses_data\': default_values_dict.get(\n                \'uses_data\',\n                \'minute\'),\n            \'verbose\': default_values_dict.get(\n                \'verbose\',\n                False)\n        }\n        self.starter_dict.update(default_values_dict)\n\n        self.lg(\n            f\'configurables={ae_consts.ppj(self.ind_confs)} for \'\n            f\'class={self.__class__.__name__} in file={use_file} \'\n            f\'starter:\\n {ae_consts.ppj(self.starter_dict)}\')\n\n        return self.ind_confs\n    # end of get_configurables\n\n    def get_starter_dict(\n            self):\n        if not self.starter_dict:\n            self.get_configurables()\n        return self.starter_dict\n    # end of get_starter_dict\n\n    def process(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""process\n\n        Derive custom indicator processing to determine buy and sell\n        conditions before placing orders. Just implement your own\n        ``process`` method.\n\n        Please refer to the TA Lib guides for details on building indicators:\n\n        - Overlap Studies\n          https://mrjbq7.github.io/ta-lib/func_groups/overlap_studies.html\n        - Momentum Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html\n        - Volume Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volume_indicators.html\n        - Volatility Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volatility_indicators.html\n        - Price Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/price_transform.html\n        - Cycle Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/cycle_indicators.html\n        - Pattern Recognition\n          https://mrjbq7.github.io/ta-lib/func_groups/pattern_recognition.html\n        - Statistic Functions\n          https://mrjbq7.github.io/ta-lib/func_groups/statistic_functions.html\n        - Math Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/math_transform.html\n        - Math Operators\n          https://mrjbq7.github.io/ta-lib/func_groups/math_operators.html\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: dictionary of ``pandas.DataFrame(s)`` to process\n        """"""\n\n        # set the algo config indicator \'uses_data\' to \'day\' or \'minute\'\n        df_status, self.use_df = self.get_subscribed_dataset(\n            dataset=dataset)\n\n        if df_status == ae_consts.EMPTY:\n            self.lg(\'process end - no data found\')\n            return\n\n        # notice the self.num_points is now a member variable\n        # because the BaseIndicator class\'s __init__\n        # converts any self.config keys into useable\n        # member variables automatically in your derived class\n        self.lg(\n            f\'process - num_points={self.num_points} \'\n            f\'df={len(self.use_df.index)}\')\n        """"""\n        real = RSI(close, timeperiod=14)\n        """"""\n        num_records = len(self.use_df.index)\n        if num_records > self.num_points:\n            cur_value = self.use_df[\'close\'].iloc[-1]\n            first_date = self.use_df[\'date\'].iloc[0]\n            end_date = self.use_df[\'date\'].iloc[-1]\n            start_row = num_records - self.num_points\n            self.use_df = self.use_df[start_row:num_records]\n            """"""\n            for idx, row in self.use_df[start_row:-1].iterrows():\n                high = row[\'high\']\n                low = row[\'low\']\n                open_val = row[\'open\']\n                close = row[\'close\']\n                row_date = row[\'date\']\n                self.lg(\n                    f\'{row_date} - high={high}, low={low}, \'\n                    f\'close={close}, period={self.num_points}\')\n            """"""\n            closes = self.use_df[\'close\'].values\n\n            self.rsi_value = ae_consts.to_f(ae_talib.RSI(\n                close=closes,\n                timeperiod=self.num_points)[-1])\n\n            """"""\n            Determine a buy or a sell as a label\n            """"""\n\n            if cur_value <= 0:\n                self.lg(f\'invalid current_value={cur_value}\')\n                return\n\n            self.close = cur_value\n            self.amount_to_close = ae_consts.to_f(\n                cur_value - self.rsi_value)\n            self.percent_value = ae_consts.to_f(\n                self.amount_to_close / cur_value * 100.0)\n\n            self.is_buy = ae_consts.INDICATOR_IGNORE\n            self.is_sell = ae_consts.INDICATOR_IGNORE\n\n            if (self.buy_above_percent != -1 and\n                    self.percent_value > self.buy_above_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n            elif (self.buy_below_percent != -1 and\n                    self.percent_value > self.buy_below_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n\n            if (self.sell_above_percent != -1 and\n                    self.percent_value > self.sell_above_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n            elif (self.sell_below_percent != -1 and\n                    self.percent_value > self.sell_below_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n\n            self.lg(\n                f\'process end - {first_date} to {end_date} \'\n                f\'buy_below={self.buy_below_percent} \'\n                f\'buy_above={self.buy_above_percent} is_buy={self.is_buy} \'\n                f\'sell_below={self.sell_below_percent} \'\n                f\'sell_above={self.sell_above_percent} is_sell={self.is_sell}\')\n        else:\n            self.lg(\'process end\')\n    # end of process\n\n    def reset_internals(\n            self):\n        """"""reset_internals""""""\n        self.is_buy = ae_consts.INDICATOR_RESET\n        self.is_sell = ae_consts.INDICATOR_RESET\n    # end of reset_internals\n\n# end of IndicatorRSI\n\n\ndef get_indicator(\n        **kwargs):\n    """"""get_indicator\n\n    Make sure to define the ``get_indicator`` for your custom\n    algorithms to work as a backup with the ``sa.py`` tool...\n    Not anticipating issues, but if we do with importlib\n    this is the backup plan.\n\n    Please file an issue if you see something weird and would like\n    some help:\n    https://github.com/AlgoTraders/stock-analysis-engine/issues\n\n    :param kwargs: dictionary of keyword arguments\n    """"""\n    print(\'getting indicator\')\n    return IndicatorRSI(**kwargs)\n# end of get_indicator\n'"
analysis_engine/indicators/stoch.py,0,"b'""""""\nCustom Stochastics - STOCH\n\nhttps://www.investopedia.com/terms/a/adx.asp\n\nMomentum\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport analysis_engine.ae_talib as ae_talib\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.indicators.base_indicator as base_indicator\n\n\nclass IndicatorSTOCH(base_indicator.BaseIndicator):\n    """"""IndicatorSTOCH""""""\n\n    def __init__(\n            self,\n            **kwargs):\n        """"""__init__\n\n        Custom indicator example for showing a\n        ``STOCH``\n        within an algo for analyzing intraday minute datasets\n\n        Please refer to the `analysis_engine.indicators.base_indicator.Ba\n        seIndicator source code for the latest supported parameters <ht\n        tps://github.com/AlgoTraders/stock-analysis-engine/blob/master/\n        analysis_engine/indicators/base_indicator.py>`__\n\n        :param kwargs: keyword arguments\n        """"""\n        super().__init__(**kwargs)\n    # end of __init__\n\n    def get_configurables(\n            self,\n            **kwargs):\n        """"""get_configurables\n\n        helper for setting up algorithm configs for this indicator\n        and programmatically set the values based off the domain\n        rules\n\n        .. code-block:: python\n\n            from analysis_engine.indicators.stoch \\\n                import IndicatorSTOCH\n            ind = IndicatorSTOCH(config_dict={\n                    \'verbose\': True\n                }).get_configurables()\n\n        :param kwargs: keyword args dictionary\n        """"""\n        self.ind_confs = []\n\n        # common:\n        self.build_base_configurables(\n            ind_type=\'momentum\',\n            category=\'technical\',\n            uses_data=self.config.get(\n                \'uses_data\',\n                \'minute\'),\n            version=1)\n\n        # custom:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'num_points\',\n            conf_type=\'int\',\n            max_value=200,\n            min_value=2,\n            default_value=100,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n\n        # output / reporting:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'slowk_value\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'slowd_value\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'amount_to_close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'percent_value\',\n            conf_type=\'percent\',\n            is_output_only=True))\n\n        default_values_dict = {}\n        for node in self.ind_confs:\n            name = node[\'name\']\n            default_value = node.get(\n                \'default\',\n                None)\n            default_values_dict[name] = default_value\n\n        use_file = None\n        try:\n            if __file__:\n                use_file = __file__\n        except Exception:\n            use_file = None\n\n        self.starter_dict = {\n            \'name\': self.__class__.__name__.lower().replace(\n                \'indicator\',\n                \'\'),\n            \'module_path\': use_file,\n            \'category\': default_values_dict.get(\n                \'category\',\n                \'momentum\'),\n            \'type\': default_values_dict.get(\n                \'type\',\n                \'technical\'),\n            \'uses_data\': default_values_dict.get(\n                \'uses_data\',\n                \'minute\'),\n            \'verbose\': default_values_dict.get(\n                \'verbose\',\n                False)\n        }\n        self.starter_dict.update(default_values_dict)\n\n        self.lg(\n            f\'configurables={ae_consts.ppj(self.ind_confs)} for \'\n            f\'class={self.__class__.__name__} in file={use_file} \'\n            f\'starter:\\n {ae_consts.ppj(self.starter_dict)}\')\n\n        return self.ind_confs\n    # end of get_configurables\n\n    def get_starter_dict(\n            self):\n        if not self.starter_dict:\n            self.get_configurables()\n        return self.starter_dict\n    # end of get_starter_dict\n\n    def process(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""process\n\n        Derive custom indicator processing to determine buy and sell\n        conditions before placing orders. Just implement your own\n        ``process`` method.\n\n        Please refer to the TA Lib guides for details on building indicators:\n\n        - Overlap Studies\n          https://mrjbq7.github.io/ta-lib/func_groups/overlap_studies.html\n        - Momentum Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html\n        - Volume Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volume_indicators.html\n        - Volatility Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volatility_indicators.html\n        - Price Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/price_transform.html\n        - Cycle Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/cycle_indicators.html\n        - Pattern Recognition\n          https://mrjbq7.github.io/ta-lib/func_groups/pattern_recognition.html\n        - Statistic Functions\n          https://mrjbq7.github.io/ta-lib/func_groups/statistic_functions.html\n        - Math Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/math_transform.html\n        - Math Operators\n          https://mrjbq7.github.io/ta-lib/func_groups/math_operators.html\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: dictionary of ``pandas.DataFrame(s)`` to process\n        """"""\n\n        # set the algo config indicator \'uses_data\' to \'day\' or \'minute\'\n        df_status, self.use_df = self.get_subscribed_dataset(\n            dataset=dataset)\n\n        if df_status == ae_consts.EMPTY:\n            self.lg(\'process end - no data found\')\n            return\n\n        # notice the self.num_points is now a member variable\n        # because the BaseIndicator class\'s __init__\n        # converts any self.config keys into useable\n        # member variables automatically in your derived class\n        self.lg(\n            f\'process - num_points={self.num_points} \'\n            f\'df={len(self.use_df.index)}\')\n        """"""\n        slowk, slowd = STOCH(\n            high,\n            low,\n            close,\n            fastk_period=5,\n            slowk_period=3,\n            slowk_matype=0,\n            slowd_period=3,\n            slowd_matype=0)\n        """"""\n        num_records = len(self.use_df.index)\n        if num_records > self.num_points:\n            cur_value = self.use_df[\'close\'].iloc[-1]\n            first_date = self.use_df[\'date\'].iloc[0]\n            end_date = self.use_df[\'date\'].iloc[-1]\n            start_row = num_records - self.num_points\n            self.use_df = self.use_df[start_row:num_records]\n            """"""\n            for idx, row in self.use_df[start_row:-1].iterrows():\n                high = row[\'high\']\n                low = row[\'low\']\n                open_val = row[\'open\']\n                close = row[\'close\']\n                row_date = row[\'date\']\n                self.lg(\n                    f\'{row_date} - high={high}, low={low}, \'\n                    f\'close={close}, period={self.num_points}\')\n            """"""\n            highs = self.use_df[\'high\'].values\n            lows = self.use_df[\'low\'].values\n            closes = self.use_df[\'close\'].values\n\n            (self.slowk_value,\n             self.slowd_value) = ae_consts.to_f(ae_talib.STOCH(\n                high=highs,\n                low=lows,\n                close=closes,\n                fastk_period=self.fastk_period,\n                slowk_period=self.slowk_period,\n                slowk_matype=self.slowk_matype,\n                slowd_period=self.slowd_period,\n                slowd_matype=self.slowd_matype)[-1])\n\n            """"""\n            Determine a buy or a sell as a label\n            """"""\n\n            if cur_value <= 0:\n                self.lg(f\'invalid current_value={cur_value}\')\n                return\n\n            self.close = cur_value\n            self.amount_to_close = ae_consts.to_f(\n                cur_value - self.adx_value)\n            self.percent_value = ae_consts.to_f(\n                self.amount_to_close / cur_value * 100.0)\n\n            self.is_buy = ae_consts.INDICATOR_IGNORE\n            self.is_sell = ae_consts.INDICATOR_IGNORE\n\n            if (self.buy_above_percent != -1 and\n                    self.percent_value > self.buy_above_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n            elif (self.buy_below_percent != -1 and\n                    self.percent_value > self.buy_below_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n\n            if (self.sell_above_percent != -1 and\n                    self.percent_value > self.sell_above_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n            elif (self.sell_below_percent != -1 and\n                    self.percent_value > self.sell_below_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n\n            self.lg(\n                f\'process end - {first_date} to {end_date} \'\n                f\'buy_below={self.buy_below_percent} \'\n                f\'buy_above={self.buy_above_percent} is_buy={self.is_buy} \'\n                f\'sell_below={self.sell_below_percent} \'\n                f\'sell_above={self.sell_above_percent} is_sell={self.is_sell}\')\n        else:\n            self.lg(\'process end\')\n    # end of process\n\n    def reset_internals(\n            self):\n        """"""reset_internals""""""\n        self.is_buy = ae_consts.INDICATOR_RESET\n        self.is_sell = ae_consts.INDICATOR_RESET\n    # end of reset_internals\n\n# end of IndicatorSTOCH\n\n\ndef get_indicator(\n        **kwargs):\n    """"""get_indicator\n\n    Make sure to define the ``get_indicator`` for your custom\n    algorithms to work as a backup with the ``sa.py`` tool...\n    Not anticipating issues, but if we do with importlib\n    this is the backup plan.\n\n    Please file an issue if you see something weird and would like\n    some help:\n    https://github.com/AlgoTraders/stock-analysis-engine/issues\n\n    :param kwargs: dictionary of keyword arguments\n    """"""\n    print(\'getting indicator\')\n    return IndicatorSTOCH(**kwargs)\n# end of get_indicator\n'"
analysis_engine/indicators/stochf.py,0,"b'""""""\nCustom Stochastics - STOCHF\n\nhttps://www.investopedia.com/terms/s/stochasticoscillator.asp\n\nMomentum\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport analysis_engine.ae_talib as ae_talib\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.indicators.base_indicator as base_indicator\n\n\nclass IndicatorSTOCHF(base_indicator.BaseIndicator):\n    """"""IndicatorSTOCHF""""""\n\n    def __init__(\n            self,\n            **kwargs):\n        """"""__init__\n\n        Custom indicator example for showing a\n        ``STOCHF``\n        within an algo for analyzing intraday minute datasets\n\n        Please refer to the `analysis_engine.indicators.base_indicator.Ba\n        seIndicator source code for the latest supported parameters <ht\n        tps://github.com/AlgoTraders/stock-analysis-engine/blob/master/\n        analysis_engine/indicators/base_indicator.py>`__\n\n        :param kwargs: keyword arguments\n        """"""\n        super().__init__(**kwargs)\n    # end of __init__\n\n    def get_configurables(\n            self,\n            **kwargs):\n        """"""get_configurables\n\n        helper for setting up algorithm configs for this indicator\n        and programmatically set the values based off the domain\n        rules\n\n        .. code-block:: python\n\n            from analysis_engine.indicators.stochf \\\n                import IndicatorSTOCHF\n            ind = IndicatorSTOCHF(config_dict={\n                    \'verbose\': True\n                }).get_configurables()\n\n        :param kwargs: keyword args dictionary\n        """"""\n        self.ind_confs = []\n\n        # common:\n        self.build_base_configurables(\n            ind_type=\'momentum\',\n            category=\'technical\',\n            uses_data=self.config.get(\n                \'uses_data\',\n                \'minute\'),\n            version=1)\n\n        # custom:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'num_points\',\n            conf_type=\'int\',\n            max_value=200,\n            min_value=2,\n            default_value=100,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'fastk_period\',\n            conf_type=\'int\',\n            min_value=1,\n            max_value=40,\n            default_value=1,\n            inc_interval=2))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'fastd_period\',\n            conf_type=\'int\',\n            min_value=1,\n            max_value=40,\n            default_value=1,\n            inc_interval=2))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'fastd_matype\',\n            conf_type=\'int\',\n            min_value=0,\n            max_value=0,\n            default_value=0,\n            inc_interval=0))\n\n        # output / reporting:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'fastk_value\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'fastd_value\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'amount_to_close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'percent_value\',\n            conf_type=\'percent\',\n            is_output_only=True))\n\n        default_values_dict = {}\n        for node in self.ind_confs:\n            name = node[\'name\']\n            default_value = node.get(\n                \'default\',\n                None)\n            default_values_dict[name] = default_value\n\n        use_file = None\n        try:\n            if __file__:\n                use_file = __file__\n        except Exception:\n            use_file = None\n\n        self.starter_dict = {\n            \'name\': self.__class__.__name__.lower().replace(\n                \'indicator\',\n                \'\'),\n            \'module_path\': use_file,\n            \'category\': default_values_dict.get(\n                \'category\',\n                \'momentum\'),\n            \'type\': default_values_dict.get(\n                \'type\',\n                \'technical\'),\n            \'uses_data\': default_values_dict.get(\n                \'uses_data\',\n                \'minute\'),\n            \'verbose\': default_values_dict.get(\n                \'verbose\',\n                False)\n        }\n        self.starter_dict.update(default_values_dict)\n\n        self.lg(\n            f\'configurables={ae_consts.ppj(self.ind_confs)} for \'\n            f\'class={self.__class__.__name__} in file={use_file} \'\n            f\'starter:\\n {ae_consts.ppj(self.starter_dict)}\')\n\n        return self.ind_confs\n    # end of get_configurables\n\n    def get_starter_dict(\n            self):\n        if not self.starter_dict:\n            self.get_configurables()\n        return self.starter_dict\n    # end of get_starter_dict\n\n    def process(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""process\n\n        Derive custom indicator processing to determine buy and sell\n        conditions before placing orders. Just implement your own\n        ``process`` method.\n\n        Please refer to the TA Lib guides for details on building indicators:\n\n        - Overlap Studies\n          https://mrjbq7.github.io/ta-lib/func_groups/overlap_studies.html\n        - Momentum Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html\n        - Volume Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volume_indicators.html\n        - Volatility Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volatility_indicators.html\n        - Price Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/price_transform.html\n        - Cycle Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/cycle_indicators.html\n        - Pattern Recognition\n          https://mrjbq7.github.io/ta-lib/func_groups/pattern_recognition.html\n        - Statistic Functions\n          https://mrjbq7.github.io/ta-lib/func_groups/statistic_functions.html\n        - Math Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/math_transform.html\n        - Math Operators\n          https://mrjbq7.github.io/ta-lib/func_groups/math_operators.html\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: dictionary of ``pandas.DataFrame(s)`` to process\n        """"""\n\n        # set the algo config indicator \'uses_data\' to \'day\' or \'minute\'\n        df_status, self.use_df = self.get_subscribed_dataset(\n            dataset=dataset)\n\n        if df_status == ae_consts.EMPTY:\n            self.lg(\'process end - no data found\')\n            return\n\n        # notice the self.num_points is now a member variable\n        # because the BaseIndicator class\'s __init__\n        # converts any self.config keys into useable\n        # member variables automatically in your derived class\n        self.lg(\n            f\'process - num_points={self.num_points} \'\n            f\'df={len(self.use_df.index)}\')\n        """"""\n        fastk, fastd = STOCHF(\n            high,\n            low,\n            close,\n            fastk_period=5,\n            fastd_period=3,\n            fastd_matype=0)\n        """"""\n        num_records = len(self.use_df.index)\n        if num_records > self.num_points:\n            cur_value = self.use_df[\'close\'].iloc[-1]\n            first_date = self.use_df[\'date\'].iloc[0]\n            end_date = self.use_df[\'date\'].iloc[-1]\n            start_row = num_records - self.num_points\n            self.use_df = self.use_df[start_row:num_records]\n            """"""\n            for idx, row in self.use_df[start_row:-1].iterrows():\n                high = row[\'high\']\n                low = row[\'low\']\n                open_val = row[\'open\']\n                close = row[\'close\']\n                row_date = row[\'date\']\n                self.lg(\n                    f\'{row_date} - high={high}, low={low}, \'\n                    f\'close={close}, period={self.num_points}\')\n            """"""\n            highs = self.use_df[\'high\'].values\n            lows = self.use_df[\'low\'].values\n            closes = self.use_df[\'close\'].values\n\n            (self.fastk_value,\n             self.fastd_value) = ae_consts.to_f(ae_talib.STOCHF(\n                high=highs,\n                low=lows,\n                close=closes,\n                fastk_period=self.fastk_period,\n                fastd_period=self.fastd_period,\n                fastd_matype=self.fastd_matype)[-1])\n\n            """"""\n            Determine a buy or a sell as a label\n            """"""\n\n            if cur_value <= 0:\n                self.lg(f\'invalid current_value={cur_value}\')\n                return\n\n            self.close = cur_value\n            self.amount_to_close = ae_consts.to_f(\n                cur_value - self.adx_value)\n            self.percent_value = ae_consts.to_f(\n                self.amount_to_close / cur_value * 100.0)\n\n            self.is_buy = ae_consts.INDICATOR_IGNORE\n            self.is_sell = ae_consts.INDICATOR_IGNORE\n\n            if (self.buy_above_percent != -1 and\n                    self.percent_value > self.buy_above_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n            elif (self.buy_below_percent != -1 and\n                    self.percent_value > self.buy_below_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n\n            if (self.sell_above_percent != -1 and\n                    self.percent_value > self.sell_above_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n            elif (self.sell_below_percent != -1 and\n                    self.percent_value > self.sell_below_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n\n            self.lg(\n                f\'process end - {first_date} to {end_date} \'\n                f\'buy_below={self.buy_below_percent} \'\n                f\'buy_above={self.buy_above_percent} is_buy={self.is_buy} \'\n                f\'sell_below={self.sell_below_percent} \'\n                f\'sell_above={self.sell_above_percent} is_sell={self.is_sell}\')\n        else:\n            self.lg(\'process end\')\n    # end of process\n\n    def reset_internals(\n            self):\n        """"""reset_internals""""""\n        self.is_buy = ae_consts.INDICATOR_RESET\n        self.is_sell = ae_consts.INDICATOR_RESET\n    # end of reset_internals\n\n# end of IndicatorSTOCHF\n\n\ndef get_indicator(\n        **kwargs):\n    """"""get_indicator\n\n    Make sure to define the ``get_indicator`` for your custom\n    algorithms to work as a backup with the ``sa.py`` tool...\n    Not anticipating issues, but if we do with importlib\n    this is the backup plan.\n\n    Please file an issue if you see something weird and would like\n    some help:\n    https://github.com/AlgoTraders/stock-analysis-engine/issues\n\n    :param kwargs: dictionary of keyword arguments\n    """"""\n    print(\'getting indicator\')\n    return IndicatorSTOCHF(**kwargs)\n# end of get_indicator\n'"
analysis_engine/indicators/trange.py,0,"b'""""""\nCustom True Range - TRANGE\n\nhttps://www.investopedia.com/terms/a/atr.asp\n\nVolatility\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport analysis_engine.ae_talib as ae_talib\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.indicators.base_indicator as base_indicator\n\n\nclass IndicatorTRANGE(base_indicator.BaseIndicator):\n    """"""IndicatorTRANGE""""""\n\n    def __init__(\n            self,\n            **kwargs):\n        """"""__init__\n\n        Custom indicator example for showing a\n        ``TRANGE``\n        within an algo for analyzing intraday minute datasets\n\n        Please refer to the `analysis_engine.indicators.base_indicator.Ba\n        seIndicator source code for the latest supported parameters <ht\n        tps://github.com/AlgoTraders/stock-analysis-engine/blob/master/\n        analysis_engine/indicators/base_indicator.py>`__\n\n        :param kwargs: keyword arguments\n        """"""\n        super().__init__(**kwargs)\n    # end of __init__\n\n    def get_configurables(\n            self,\n            **kwargs):\n        """"""get_configurables\n\n        helper for setting up algorithm configs for this indicator\n        and programmatically set the values based off the domain\n        rules\n\n        .. code-block:: python\n\n            from analysis_engine.indicators.trange \\\n                import IndicatorTRANGE\n            ind = IndicatorTRANGE(config_dict={\n                    \'verbose\': True\n                }).get_configurables()\n\n        :param kwargs: keyword args dictionary\n        """"""\n        self.ind_confs = []\n\n        # common:\n        self.build_base_configurables(\n            ind_type=\'volatility\',\n            category=\'technical\',\n            uses_data=self.config.get(\n                \'uses_data\',\n                \'minute\'),\n            version=1)\n\n        # custom:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'num_points\',\n            conf_type=\'int\',\n            max_value=200,\n            min_value=2,\n            default_value=100,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n\n        # output / reporting:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'trange_value\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'amount_to_close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'percent_value\',\n            conf_type=\'percent\',\n            is_output_only=True))\n\n        default_values_dict = {}\n        for node in self.ind_confs:\n            name = node[\'name\']\n            default_value = node.get(\n                \'default\',\n                None)\n            default_values_dict[name] = default_value\n\n        use_file = None\n        try:\n            if __file__:\n                use_file = __file__\n        except Exception:\n            use_file = None\n\n        self.starter_dict = {\n            \'name\': self.__class__.__name__.lower().replace(\n                \'indicator\',\n                \'\'),\n            \'module_path\': use_file,\n            \'category\': default_values_dict.get(\n                \'category\',\n                \'momentum\'),\n            \'type\': default_values_dict.get(\n                \'type\',\n                \'technical\'),\n            \'uses_data\': default_values_dict.get(\n                \'uses_data\',\n                \'minute\'),\n            \'verbose\': default_values_dict.get(\n                \'verbose\',\n                False)\n        }\n        self.starter_dict.update(default_values_dict)\n\n        self.lg(\n            f\'configurables={ae_consts.ppj(self.ind_confs)} for \'\n            f\'class={self.__class__.__name__} in file={use_file} \'\n            f\'starter:\\n {ae_consts.ppj(self.starter_dict)}\')\n\n        return self.ind_confs\n    # end of get_configurables\n\n    def get_starter_dict(\n            self):\n        if not self.starter_dict:\n            self.get_configurables()\n        return self.starter_dict\n    # end of get_starter_dict\n\n    def process(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""process\n\n        Derive custom indicator processing to determine buy and sell\n        conditions before placing orders. Just implement your own\n        ``process`` method.\n\n        Please refer to the TA Lib guides for details on building indicators:\n\n        - Overlap Studies\n          https://mrjbq7.github.io/ta-lib/func_groups/overlap_studies.html\n        - Momentum Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html\n        - Volume Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volume_indicators.html\n        - Volatility Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volatility_indicators.html\n        - Price Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/price_transform.html\n        - Cycle Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/cycle_indicators.html\n        - Pattern Recognition\n          https://mrjbq7.github.io/ta-lib/func_groups/pattern_recognition.html\n        - Statistic Functions\n          https://mrjbq7.github.io/ta-lib/func_groups/statistic_functions.html\n        - Math Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/math_transform.html\n        - Math Operators\n          https://mrjbq7.github.io/ta-lib/func_groups/math_operators.html\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: dictionary of ``pandas.DataFrame(s)`` to process\n        """"""\n\n        # set the algo config indicator \'uses_data\' to \'day\' or \'minute\'\n        df_status, self.use_df = self.get_subscribed_dataset(\n            dataset=dataset)\n\n        if df_status == ae_consts.EMPTY:\n            self.lg(\'process end - no data found\')\n            return\n\n        # notice the self.num_points is now a member variable\n        # because the BaseIndicator class\'s __init__\n        # converts any self.config keys into useable\n        # member variables automatically in your derived class\n        self.lg(\n            f\'process - num_points={self.num_points} \'\n            f\'df={len(self.use_df.index)}\')\n        """"""\n        real = TRANGE(high, low, close)\n        """"""\n        num_records = len(self.use_df.index)\n        if num_records > self.num_points:\n            cur_value = self.use_df[\'close\'].iloc[-1]\n            first_date = self.use_df[\'date\'].iloc[0]\n            end_date = self.use_df[\'date\'].iloc[-1]\n            start_row = num_records - self.num_points\n            self.use_df = self.use_df[start_row:num_records]\n            """"""\n            for idx, row in self.use_df[start_row:-1].iterrows():\n                high = row[\'high\']\n                low = row[\'low\']\n                open_val = row[\'open\']\n                close = row[\'close\']\n                row_date = row[\'date\']\n                self.lg(\n                    f\'{row_date} - high={high}, low={low}, \'\n                    f\'close={close}, period={self.num_points}\')\n            """"""\n            highs = self.use_df[\'high\'].values\n            lows = self.use_df[\'low\'].values\n            closes = self.use_df[\'close\'].values\n\n            self.trange_value = ae_consts.to_f(ae_talib.TRANGE(\n                high=highs,\n                low=lows,\n                close=closes)[-1])\n\n            """"""\n            Determine a buy or a sell as a label\n            """"""\n\n            if cur_value <= 0:\n                self.lg(f\'invalid current_value={cur_value}\')\n                return\n\n            self.close = cur_value\n            self.amount_to_close = ae_consts.to_f(\n                cur_value - self.trange_value)\n            self.percent_value = ae_consts.to_f(\n                self.amount_to_close / cur_value * 100.0)\n\n            self.is_buy = ae_consts.INDICATOR_IGNORE\n            self.is_sell = ae_consts.INDICATOR_IGNORE\n\n            if (self.buy_above_percent != -1 and\n                    self.percent_value > self.buy_above_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n            elif (self.buy_below_percent != -1 and\n                    self.percent_value > self.buy_below_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n\n            if (self.sell_above_percent != -1 and\n                    self.percent_value > self.sell_above_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n            elif (self.sell_below_percent != -1 and\n                    self.percent_value > self.sell_below_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n\n            self.lg(\n                f\'process end - {first_date} to {end_date} \'\n                f\'buy_below={self.buy_below_percent} \'\n                f\'buy_above={self.buy_above_percent} is_buy={self.is_buy} \'\n                f\'sell_below={self.sell_below_percent} \'\n                f\'sell_above={self.sell_above_percent} is_sell={self.is_sell}\')\n        else:\n            self.lg(\'process end\')\n    # end of process\n\n    def reset_internals(\n            self):\n        """"""reset_internals""""""\n        self.is_buy = ae_consts.INDICATOR_RESET\n        self.is_sell = ae_consts.INDICATOR_RESET\n    # end of reset_internals\n\n# end of IndicatorTRANGE\n\n\ndef get_indicator(\n        **kwargs):\n    """"""get_indicator\n\n    Make sure to define the ``get_indicator`` for your custom\n    algorithms to work as a backup with the ``sa.py`` tool...\n    Not anticipating issues, but if we do with importlib\n    this is the backup plan.\n\n    Please file an issue if you see something weird and would like\n    some help:\n    https://github.com/AlgoTraders/stock-analysis-engine/issues\n\n    :param kwargs: dictionary of keyword arguments\n    """"""\n    print(\'getting indicator\')\n    return IndicatorTRANGE(**kwargs)\n# end of get_indicator\n'"
analysis_engine/indicators/williamsr.py,0,"b'""""""\nCustom Williams Percent R Indicator\n\nhttps://www.investopedia.com/terms/w/williamsr.asp\n\nMomentum\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport analysis_engine.ae_talib as ae_talib\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.indicators.base_indicator as base_indicator\n\n\nclass IndicatorWilliamsR(base_indicator.BaseIndicator):\n    """"""IndicatorWilliamsR""""""\n\n    def __init__(\n            self,\n            **kwargs):\n        """"""__init__\n\n        Custom indicator example for showing a Williams Percent R\n        within an algo for analyzing intraday minute datasets\n\n        Please refer to the `analysis_engine.indicators.base_indicator.Ba\n        seIndicator source code for the latest supported parameters <ht\n        tps://github.com/AlgoTraders/stock-analysis-engine/blob/master/\n        analysis_engine/indicators/base_indicator.py>`__\n\n        :param kwargs: keyword arguments\n        """"""\n        super().__init__(**kwargs)\n    # end of __init__\n\n    def get_configurables(\n            self,\n            **kwargs):\n        """"""get_configurables\n\n        helper for setting up algorithm configs for this indicator\n        and programmatically set the values based off the domain\n        rules\n\n        .. code-block:: python\n\n            from analysis_engine.indicators.williamsr import IndicatorWilliamsR\n            ind = IndicatorWilliamsR(config_dict={\n                    \'verbose\': True\n                }).get_configurables()\n\n        :param kwargs: keyword args dictionary\n        """"""\n        self.ind_confs = []\n\n        # common:\n        self.build_base_configurables(\n            ind_type=\'momentum\',\n            category=\'technical\',\n            uses_data=self.config.get(\n                \'uses_data\',\n                \'minute\'),\n            version=1)\n\n        # custom:\n        """"""\n        24,000 tests with this config:\n        """"""\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'num_points\',\n            conf_type=\'int\',\n            max_value=200,\n            min_value=3,\n            default_value=20,\n            inc_interval=10))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_below\',\n            conf_type=\'float\',\n            max_value=-70.0,\n            min_value=-90.0,\n            default_value=-80.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_above\',\n            conf_type=\'float\',\n            max_value=-1.0,\n            min_value=-29.0,\n            default_value=-20.0,\n            inc_interval=1))\n\n        # output / reporting:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'willr_value\',\n            conf_type=\'float\',\n            is_output_only=True))\n\n        default_values_dict = {}\n        for node in self.ind_confs:\n            name = node[\'name\']\n            default_value = node.get(\n                \'default\',\n                None)\n            default_values_dict[name] = default_value\n\n        use_file = None\n        try:\n            if __file__:\n                use_file = __file__\n        except Exception:\n            use_file = None\n\n        self.starter_dict = {\n            \'name\': self.__class__.__name__.lower().replace(\n                \'indicator\',\n                \'\'),\n            \'module_path\': use_file,\n            \'category\': default_values_dict.get(\n                \'category\',\n                \'momentum\'),\n            \'type\': default_values_dict.get(\n                \'type\',\n                \'technical\'),\n            \'uses_data\': default_values_dict.get(\n                \'uses_data\',\n                \'minute\'),\n            \'verbose\': default_values_dict.get(\n                \'verbose\',\n                False)\n        }\n        self.starter_dict.update(default_values_dict)\n\n        self.lg(\n            f\'configurables={ae_consts.ppj(self.ind_confs)} for \'\n            f\'class={self.__class__.__name__} in file={use_file} \'\n            f\'starter:\\n {ae_consts.ppj(self.starter_dict)}\')\n\n        return self.ind_confs\n    # end of get_configurables\n\n    def get_starter_dict(\n            self):\n        if not self.starter_dict:\n            self.get_configurables()\n        return self.starter_dict\n    # end of get_starter_dict\n\n    def process(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""process\n\n        Derive custom indicator processing to determine buy and sell\n        conditions before placing orders. Just implement your own\n        ``process`` method.\n\n        Please refer to the TA Lib guides for details on building indicators:\n\n        - Overlap Studies\n          https://mrjbq7.github.io/ta-lib/func_groups/overlap_studies.html\n        - Momentum Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html\n        - Volume Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volume_indicators.html\n        - Volatility Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volatility_indicators.html\n        - Price Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/price_transform.html\n        - Cycle Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/cycle_indicators.html\n        - Pattern Recognition\n          https://mrjbq7.github.io/ta-lib/func_groups/pattern_recognition.html\n        - Statistic Functions\n          https://mrjbq7.github.io/ta-lib/func_groups/statistic_functions.html\n        - Math Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/math_transform.html\n        - Math Operators\n          https://mrjbq7.github.io/ta-lib/func_groups/math_operators.html\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: dictionary of ``pandas.DataFrame(s)`` to process\n        """"""\n\n        # set the algo config indicator \'uses_data\' to \'day\' or \'minute\'\n        df_status, self.use_df = self.get_subscribed_dataset(\n            dataset=dataset)\n\n        self.willr_value = None\n        if df_status == ae_consts.EMPTY:\n            self.lg(\'process end - no data found\')\n            return\n\n        # notice the self.num_points is now a member variable\n        # because the BaseIndicator class\'s __init__\n        # converts any self.config keys into useable\n        # member variables automatically in your derived class\n        self.lg(\n            f\'process - num_points={self.num_points} \'\n            f\'df={len(self.use_df.index)}\')\n        """"""\n        real = WILLR(high, low, close, timeperiod=14)\n        """"""\n        num_records = len(self.use_df.index)\n        if num_records > self.num_points:\n            first_date = self.use_df[\'date\'].iloc[0]\n            end_date = self.use_df[\'date\'].iloc[-1]\n            start_row = num_records - self.num_points\n            self.use_df = self.use_df[start_row:num_records].dropna(\n                axis=0,\n                how=\'any\')\n\n            if len(self.use_df.index) == 0:\n                self.lg(f\'empty dataframe={self.uses_data} on date={end_date}\')\n                return\n\n            """"""\n            for idx, row in self.use_df[start_row:-1].iterrows():\n                high = row[\'high\']\n                low = row[\'low\']\n                open_val = row[\'open\']\n                close = row[\'close\']\n                row_date = row[\'date\']\n                self.lg(\n                    f\'{row_date} - high={high}, low={low}, \'\n                    f\'close={close}, period={self.num_points}\')\n            """"""\n            highs = self.use_df[\'high\'].values\n            lows = self.use_df[\'low\'].values\n            closes = self.use_df[\'close\'].values\n            willr_values = ae_talib.WILLR(\n                highs,\n                lows,\n                closes,\n                self.num_points)\n            self.willr_value = ae_consts.to_f(\n                willr_values[-1])\n\n            """"""\n            Determine a buy or a sell as a label\n            """"""\n\n            self.is_buy = ae_consts.INDICATOR_IGNORE\n            self.is_sell = ae_consts.INDICATOR_IGNORE\n\n            if self.willr_value < self.buy_below:\n                self.is_buy = ae_consts.INDICATOR_BUY\n\n            if self.willr_value > self.sell_above:\n                self.is_sell = ae_consts.INDICATOR_SELL\n\n            self.lg(\n                f\'process end - {first_date} to {end_date} \'\n                f\'willr_value={self.willr_value} \'\n                f\'buy_below={self.buy_below} is_buy={self.is_buy} \'\n                f\'sell_above={self.sell_above} is_sell={self.is_sell}\')\n        else:\n            self.lg(f\'process end - willr={self.willr_value}\')\n    # end of process\n\n    def reset_internals(\n            self):\n        """"""reset_internals""""""\n        self.is_buy = ae_consts.INDICATOR_RESET\n        self.is_sell = ae_consts.INDICATOR_RESET\n    # end of reset_internals\n\n# end of IndicatorWilliamsR\n\n\ndef get_indicator(\n        **kwargs):\n    """"""get_indicator\n\n    Make sure to define the ``get_indicator`` for your custom\n    algorithms to work as a backup with the ``sa.py`` tool...\n    Not anticipating issues, but if we do with importlib\n    this is the backup plan.\n\n    Please file an issue if you see something weird and would like\n    some help:\n    https://github.com/AlgoTraders/stock-analysis-engine/issues\n\n    :param kwargs: dictionary of keyword arguments\n    """"""\n    print(\'getting indicator\')\n    return IndicatorWilliamsR(**kwargs)\n# end of get_indicator\n'"
analysis_engine/indicators/williamsr_open.py,0,"b'""""""\nCustom Williams Percent R Indicator that\nuses Open instead of Close\n\nhttps://www.investopedia.com/terms/w/williamsr.asp\n\nMomentum\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport analysis_engine.ae_talib as ae_talib\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.indicators.base_indicator as base_indicator\n\n\nclass IndicatorWilliamsROpen(base_indicator.BaseIndicator):\n    """"""IndicatorWilliamsROpen""""""\n\n    def __init__(\n            self,\n            **kwargs):\n        """"""__init__\n\n        Custom indicator example for showing a Williams Percent R\n        that uses ``open`` instead of ``close``\n        within an algo for analyzing intraday minute datasets\n\n        Please refer to the `analysis_engine.indicators.base_indicator.Ba\n        seIndicator source code for the latest supported parameters <ht\n        tps://github.com/AlgoTraders/stock-analysis-engine/blob/master/\n        analysis_engine/indicators/base_indicator.py>`__\n\n        :param kwargs: keyword arguments\n        """"""\n        super().__init__(**kwargs)\n    # end of __init__\n\n    def get_configurables(\n            self,\n            **kwargs):\n        """"""get_configurables\n\n        helper for setting up algorithm configs for this indicator\n        and programmatically set the values based off the domain\n        rules\n\n        .. code-block:: python\n\n            from analysis_engine.indicators.williamsr_open\n                import IndicatorWilliamsROpen\n            ind = IndicatorWilliamsROpen(config_dict={\n                    \'verbose\': True\n                }).get_configurables()\n\n        :param kwargs: keyword args dictionary\n        """"""\n        self.ind_confs = []\n\n        # common:\n        self.build_base_configurables(\n            ind_type=\'momentum\',\n            category=\'technical\',\n            uses_data=self.config.get(\n                \'uses_data\',\n                \'minute\'),\n            version=1)\n\n        # custom:\n        """"""\n        24,000 tests with this config:\n        """"""\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'num_points\',\n            conf_type=\'int\',\n            max_value=200,\n            min_value=3,\n            default_value=20,\n            inc_interval=10))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_below\',\n            conf_type=\'float\',\n            max_value=-70.0,\n            min_value=-90.0,\n            default_value=-80.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_above\',\n            conf_type=\'float\',\n            max_value=-1.0,\n            min_value=-29.0,\n            default_value=-20.0,\n            inc_interval=1))\n\n        # output / reporting:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'willr_open_value\',\n            conf_type=\'float\',\n            is_output_only=True))\n\n        default_values_dict = {}\n        for node in self.ind_confs:\n            name = node[\'name\']\n            default_value = node.get(\n                \'default\',\n                None)\n            default_values_dict[name] = default_value\n\n        use_file = None\n        try:\n            if __file__:\n                use_file = __file__\n        except Exception:\n            use_file = None\n\n        self.starter_dict = {\n            \'name\': self.__class__.__name__.lower().replace(\n                \'indicator\',\n                \'\'),\n            \'module_path\': use_file,\n            \'category\': default_values_dict.get(\n                \'category\',\n                \'momentum\'),\n            \'type\': default_values_dict.get(\n                \'type\',\n                \'technical\'),\n            \'uses_data\': default_values_dict.get(\n                \'uses_data\',\n                \'minute\'),\n            \'verbose\': default_values_dict.get(\n                \'verbose\',\n                False)\n        }\n        self.starter_dict.update(default_values_dict)\n\n        self.lg(\n            f\'configurables={ae_consts.ppj(self.ind_confs)} for \'\n            f\'class={self.__class__.__name__} in file={use_file} \'\n            f\'starter:\\n {ae_consts.ppj(self.starter_dict)}\')\n\n        return self.ind_confs\n    # end of get_configurables\n\n    def get_starter_dict(\n            self):\n        if not self.starter_dict:\n            self.get_configurables()\n        return self.starter_dict\n    # end of get_starter_dict\n\n    def process(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""process\n\n        Derive custom indicator processing to determine buy and sell\n        conditions before placing orders. Just implement your own\n        ``process`` method.\n\n        Please refer to the TA Lib guides for details on building indicators:\n\n        - Overlap Studies\n          https://mrjbq7.github.io/ta-lib/func_groups/overlap_studies.html\n        - Momentum Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html\n        - Volume Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volume_indicators.html\n        - Volatility Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volatility_indicators.html\n        - Price Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/price_transform.html\n        - Cycle Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/cycle_indicators.html\n        - Pattern Recognition\n          https://mrjbq7.github.io/ta-lib/func_groups/pattern_recognition.html\n        - Statistic Functions\n          https://mrjbq7.github.io/ta-lib/func_groups/statistic_functions.html\n        - Math Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/math_transform.html\n        - Math Operators\n          https://mrjbq7.github.io/ta-lib/func_groups/math_operators.html\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: dictionary of ``pandas.DataFrame(s)`` to process\n        """"""\n\n        # set the algo config indicator \'uses_data\' to \'day\' or \'minute\'\n        df_status, self.use_df = self.get_subscribed_dataset(\n            dataset=dataset)\n\n        self.willr_open_value = None\n        if df_status == ae_consts.EMPTY:\n            self.lg(\'process end - no data found\')\n            return\n\n        # notice the self.num_points is now a member variable\n        # because the BaseIndicator class\'s __init__\n        # converts any self.config keys into useable\n        # member variables automatically in your derived class\n        self.lg(\n            f\'process start - num_points={self.num_points} \'\n            f\'df={len(self.use_df.index)}\')\n\n        """"""\n        real = WILLR(high, low, open, timeperiod=14)\n        """"""\n        num_records = len(self.use_df.index)\n        if num_records > self.num_points:\n            first_date = self.use_df[\'date\'].iloc[0]\n            end_date = self.use_df[\'date\'].iloc[-1]\n            start_row = num_records - self.num_points\n            self.use_df = self.use_df[start_row:num_records]\n            """"""\n            for idx, row in self.use_df[start_row:-1].iterrows():\n                high = row[\'high\']\n                low = row[\'low\']\n                open_val = row[\'open\']\n                close = row[\'close\']\n                row_date = row[\'date\']\n                self.lg(\n                    f\'{row_date} - WILLR(high={high}, low={low}, \'\n                    f\'open={open_val}, period={self.num_points})\')\n            """"""\n            highs = self.use_df[\'high\'].values\n            lows = self.use_df[\'low\'].values\n            opens = self.use_df[\'open\'].values\n            willr_values = ae_talib.WILLR(\n                highs,\n                lows,\n                opens,\n                self.num_points)\n            self.willr_open_value = ae_consts.to_f(\n                willr_values[-1])\n\n            """"""\n            Determine a buy or a sell as a label\n            """"""\n\n            self.is_buy = ae_consts.INDICATOR_IGNORE\n            self.is_sell = ae_consts.INDICATOR_IGNORE\n\n            if self.willr_open_value < self.buy_below:\n                self.is_buy = ae_consts.INDICATOR_BUY\n\n            if self.willr_open_value > self.sell_above:\n                self.is_sell = ae_consts.INDICATOR_SELL\n\n            self.lg(\n                f\'process end - {first_date} to {end_date} \'\n                f\'willr_open_value={self.willr_open_value} \'\n                f\'buy_below={self.buy_below} is_buy={self.is_buy} \'\n                f\'sell_above={self.sell_above} is_sell={self.is_sell}\')\n        else:\n            self.lg(f\'process end - willr={self.willr_open_value}\')\n    # end of process\n\n    def reset_internals(\n            self):\n        """"""reset_internals""""""\n        self.is_buy = ae_consts.INDICATOR_RESET\n        self.is_sell = ae_consts.INDICATOR_RESET\n    # end of reset_internals\n\n# end of IndicatorWilliamsROpen\n\n\ndef get_indicator(\n        **kwargs):\n    """"""get_indicator\n\n    Make sure to define the ``get_indicator`` for your custom\n    algorithms to work as a backup with the ``sa.py`` tool...\n    Not anticipating issues, but if we do with importlib\n    this is the backup plan.\n\n    Please file an issue if you see something weird and would like\n    some help:\n    https://github.com/AlgoTraders/stock-analysis-engine/issues\n\n    :param kwargs: dictionary of keyword arguments\n    """"""\n    print(\'getting indicator\')\n    return IndicatorWilliamsROpen(**kwargs)\n# end of get_indicator\n'"
analysis_engine/indicators/wma.py,0,"b'""""""\nCustom Weighted Moving Average\n\nhttps://www.investopedia.com/articles/technical/060401.asp\n\nOverlap\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport analysis_engine.ae_talib as ae_talib\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.indicators.base_indicator as base_indicator\n\n\nclass IndicatorWMA(base_indicator.BaseIndicator):\n    """"""IndicatorWMA""""""\n\n    def __init__(\n            self,\n            **kwargs):\n        """"""__init__\n\n        Custom indicator example for showing a\n        ``WMA``\n        within an algo for analyzing intraday minute datasets\n\n        Please refer to the `analysis_engine.indicators.base_indicator.Ba\n        seIndicator source code for the latest supported parameters <ht\n        tps://github.com/AlgoTraders/stock-analysis-engine/blob/master/\n        analysis_engine/indicators/base_indicator.py>`__\n\n        :param kwargs: keyword arguments\n        """"""\n        super().__init__(**kwargs)\n    # end of __init__\n\n    def get_configurables(\n            self,\n            **kwargs):\n        """"""get_configurables\n\n        helper for setting up algorithm configs for this indicator\n        and programmatically set the values based off the domain\n        rules\n\n        .. code-block:: python\n\n            from analysis_engine.indicators.wma \\\n                import IndicatorWMA\n            ind = IndicatorWMA(config_dict={\n                    \'verbose\': True\n                }).get_configurables()\n\n        :param kwargs: keyword args dictionary\n        """"""\n        self.ind_confs = []\n\n        # common:\n        self.build_base_configurables(\n            ind_type=\'overlap\',\n            category=\'technical\',\n            uses_data=self.config.get(\n                \'uses_data\',\n                \'minute\'),\n            version=1)\n\n        # custom:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'num_points\',\n            conf_type=\'int\',\n            max_value=200,\n            min_value=2,\n            default_value=100,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_below_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_above_percent\',\n            conf_type=\'percent\',\n            min_value=3.0,\n            max_value=100.0,\n            default_value=5.0,\n            inc_interval=1))\n\n        # output / reporting:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'wma_value\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'amount_to_close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'close\',\n            conf_type=\'float\',\n            is_output_only=True))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'percent_value\',\n            conf_type=\'percent\',\n            is_output_only=True))\n\n        default_values_dict = {}\n        for node in self.ind_confs:\n            name = node[\'name\']\n            default_value = node.get(\n                \'default\',\n                None)\n            default_values_dict[name] = default_value\n\n        use_file = None\n        try:\n            if __file__:\n                use_file = __file__\n        except Exception:\n            use_file = None\n\n        self.starter_dict = {\n            \'name\': self.__class__.__name__.lower().replace(\n                \'indicator\',\n                \'\'),\n            \'module_path\': use_file,\n            \'category\': default_values_dict.get(\n                \'category\',\n                \'momentum\'),\n            \'type\': default_values_dict.get(\n                \'type\',\n                \'technical\'),\n            \'uses_data\': default_values_dict.get(\n                \'uses_data\',\n                \'minute\'),\n            \'verbose\': default_values_dict.get(\n                \'verbose\',\n                False)\n        }\n        self.starter_dict.update(default_values_dict)\n\n        self.lg(\n            f\'configurables={ae_consts.ppj(self.ind_confs)} for \'\n            f\'class={self.__class__.__name__} in file={use_file} \'\n            f\'starter:\\n {ae_consts.ppj(self.starter_dict)}\')\n\n        return self.ind_confs\n    # end of get_configurables\n\n    def get_starter_dict(\n            self):\n        if not self.starter_dict:\n            self.get_configurables()\n        return self.starter_dict\n    # end of get_starter_dict\n\n    def process(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""process\n\n        Derive custom indicator processing to determine buy and sell\n        conditions before placing orders. Just implement your own\n        ``process`` method.\n\n        Please refer to the TA Lib guides for details on building indicators:\n\n        - Overlap Studies\n          https://mrjbq7.github.io/ta-lib/func_groups/overlap_studies.html\n        - Momentum Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html\n        - Volume Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volume_indicators.html\n        - Volatility Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volatility_indicators.html\n        - Price Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/price_transform.html\n        - Cycle Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/cycle_indicators.html\n        - Pattern Recognition\n          https://mrjbq7.github.io/ta-lib/func_groups/pattern_recognition.html\n        - Statistic Functions\n          https://mrjbq7.github.io/ta-lib/func_groups/statistic_functions.html\n        - Math Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/math_transform.html\n        - Math Operators\n          https://mrjbq7.github.io/ta-lib/func_groups/math_operators.html\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: dictionary of ``pandas.DataFrame(s)`` to process\n        """"""\n\n        # set the algo config indicator \'uses_data\' to \'day\' or \'minute\'\n        df_status, self.use_df = self.get_subscribed_dataset(\n            dataset=dataset)\n\n        if df_status == ae_consts.EMPTY:\n            self.lg(\'process end - no data found\')\n            return\n\n        # notice the self.num_points is now a member variable\n        # because the BaseIndicator class\'s __init__\n        # converts any self.config keys into useable\n        # member variables automatically in your derived class\n        self.lg(\n            f\'process - num_points={self.num_points} \'\n            f\'df={len(self.use_df.index)}\')\n        """"""\n        real = WMA(close, timeperiod=30)\n        """"""\n        num_records = len(self.use_df.index)\n        if num_records > self.num_points:\n            cur_value = self.use_df[\'close\'].iloc[-1]\n            first_date = self.use_df[\'date\'].iloc[0]\n            end_date = self.use_df[\'date\'].iloc[-1]\n            start_row = num_records - self.num_points\n            self.use_df = self.use_df[start_row:num_records]\n            """"""\n            for idx, row in self.use_df[start_row:-1].iterrows():\n                high = row[\'high\']\n                low = row[\'low\']\n                open_val = row[\'open\']\n                close = row[\'close\']\n                row_date = row[\'date\']\n                self.lg(\n                    f\'{row_date} - high={high}, low={low}, \'\n                    f\'close={close}, period={self.num_points}\')\n            """"""\n            closes = self.use_df[\'close\'].values\n\n            self.wma_value = ae_consts.to_f(ae_talib.WMA(\n                 close=closes,\n                 timeperiod=self.num_points)[-1])\n\n            """"""\n            Determine a buy or a sell as a label\n            """"""\n\n            if cur_value <= 0:\n                self.lg(f\'invalid current_value={cur_value}\')\n                return\n\n            self.close = cur_value\n            self.amount_to_close = ae_consts.to_f(cur_value - self.wma_value)\n            self.percent_value = ae_consts.to_f(\n                self.amount_to_close / cur_value * 100.0)\n\n            self.is_buy = ae_consts.INDICATOR_IGNORE\n            self.is_sell = ae_consts.INDICATOR_IGNORE\n\n            if (self.buy_above_percent != -1 and\n                    self.percent_value > self.buy_above_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n            elif (self.buy_below_percent != -1 and\n                    self.percent_value > self.buy_below_percent):\n                self.is_buy = ae_consts.INDICATOR_BUY\n\n            if (self.sell_above_percent != -1 and\n                    self.percent_value > self.sell_above_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n            elif (self.sell_below_percent != -1 and\n                    self.percent_value > self.sell_below_percent):\n                self.is_sell = ae_consts.INDICATOR_SELL\n\n            self.lg(\n                f\'process end - {first_date} to {end_date} \'\n                f\'buy_below={self.buy_below_percent} \'\n                f\'buy_above={self.buy_above_percent} is_buy={self.is_buy} \'\n                f\'sell_below={self.sell_below_percent} \'\n                f\'sell_above={self.sell_above_percent} is_sell={self.is_sell}\')\n        else:\n            self.lg(\'process end\')\n    # end of process\n\n    def reset_internals(\n            self):\n        """"""reset_internals""""""\n        self.is_buy = ae_consts.INDICATOR_RESET\n        self.is_sell = ae_consts.INDICATOR_RESET\n    # end of reset_internals\n\n# end of IndicatorWMA\n\n\ndef get_indicator(\n        **kwargs):\n    """"""get_indicator\n\n    Make sure to define the ``get_indicator`` for your custom\n    algorithms to work as a backup with the ``sa.py`` tool...\n    Not anticipating issues, but if we do with importlib\n    this is the backup plan.\n\n    Please file an issue if you see something weird and would like\n    some help:\n    https://github.com/AlgoTraders/stock-analysis-engine/issues\n\n    :param kwargs: dictionary of keyword arguments\n    """"""\n    print(\'getting indicator\')\n    return IndicatorWMA(**kwargs)\n# end of get_indicator\n'"
analysis_engine/log/__init__.py,0,b''
analysis_engine/mocks/__init__.py,0,b''
analysis_engine/mocks/base_test.py,0,"b'""""""\nBase Test Case class\n\nThis class provides common functionality for most\nunittests:\n\n- turns on debugging values\n- adds and removes test environment variables\n- interface for building test data\n""""""\n\nimport os\nimport unittest\nimport analysis_engine.api_requests as api_requests\n\n\nclass BaseTestCase(unittest.TestCase):\n    """"""BaseTestCase""""""\n\n    debug = False\n    celery_disabled_value = None\n    debug_get_pricing = None\n    debug_pub_pricing = None\n    backup_s3_contents = None\n\n    def setUp(\n            self):\n        """"""setUp""""""\n        self.backup_s3_contents = None\n        self.celery_disabled_value = None\n        self.debug_get_pricing = None\n        self.has_ta_lib = False\n        if (os.getenv(""READTHEDOCS"", """") == """"\n                and os.getenv(""TRAVIS"", """") == """"):\n            self.has_ta_lib = True\n\n        if os.getenv(\n                \'CELERY_DISABLED\',\n                \'not-set\') != \'not-set\':\n            self.celery_disabled_value = os.getenv(\n                \'CELERY_DISABLED\',\n                None)\n        os.environ[\'CELERY_DISABLED\'] = \'1\'\n        if os.getenv(\n                \'DEBUG_GET_PRICING\',\n                \'not-set\') != \'not-set\':\n            self.debug_get_pricing = os.getenv(\n                \'DEBUG_GET_PRICING\',\n                None)\n        os.environ[\'DEBUG_GET_PRICING\'] = \'1\'\n        if os.getenv(\n                \'DEBUG_PUB_PRICING\',\n                \'not-set\') != \'not-set\':\n            self.debug_pub_pricing = os.getenv(\n                \'DEBUG_PUB_PRICING\',\n                None)\n        os.environ[\'DEBUG_PUB_PRICING\'] = \'1\'\n        if os.getenv(\n                \'TEST_S3_CONTENTS\',\n                \'not-set\') != \'not-set\':\n            self.backup_s3_contents = os.getenv(\n                \'TEST_S3_CONTENTS\',\n                None)\n        os.environ.pop(\'TEST_S3_CONTENTS\', None)\n    # end of setUp\n\n    def tearDown(\n            self):\n        """"""tearDown""""""\n        if self.celery_disabled_value:\n            os.environ[\'CELERY_DISABLED\'] = self.celery_disabled_value\n        else:\n            os.environ.pop(\'CELERY_DISABLED\', None)\n        if self.debug_get_pricing:\n            os.environ[\'DEBUG_GET_PRICING\'] = self.debug_get_pricing\n        else:\n            os.environ.pop(\'DEBUG_GET_PRICING\', None)\n        if self.debug_pub_pricing:\n            os.environ[\'DEBUG_PUB_PRICING\'] = self.debug_pub_pricing\n        else:\n            os.environ.pop(\'DEBUG_PUB_PRICING\', None)\n        if self.backup_s3_contents:\n            os.environ[\'TEST_S3_CONTENTS\'] = self.backup_s3_contents\n        else:\n            os.environ.pop(\'TEST_S3_CONTENTS\', None)\n    # end of tearDown\n\n    def get_pricing_test_data(\n            self,\n            test_name=None):\n        """"""get_pricing_test_data""""""\n        test_data = api_requests.build_cache_ready_pricing_dataset()\n        if test_name:\n            test_data[\'_TEST_NAME\'] = test_name\n        else:\n            test_data[\'_TEST_NAME\'] = \'not-set\'\n        return test_data\n    # end of get_pricing_test_data\n\n# end of BaseTestCase\n'"
analysis_engine/mocks/example_algo_minute.py,0,"b'""""""\nExample Minute Algorithm for showing how\nto run an algorithm on intraday minute timeseries datasets\n\n**What does the base class provide?**\n\nAlgorithms automatically provide the following\nmember variables to any custom algorithm that derives\nthe ``analysis_engine.algo.BaseAlgo.process`` method.\n\nBy deriving the ``process()`` member method using an inherited\nclass, you can quickly build algorithms that\ndetermine **buy** and **sell** conditions from\nany of the automatically extracted\ndatasets from the redis pipeline:\n\n- ``self.df_daily``\n- ``self.df_minute``\n- ``self.df_calls``\n- ``self.df_puts``\n- ``self.df_quote``\n- ``self.df_pricing``\n- ``self.df_stats``\n- ``self.df_peers``\n- ``self.df_iex_news``\n- ``self.df_financials``\n- ``self.df_earnings``\n- ``self.df_dividends``\n- ``self.df_company``\n- ``self.df_yahoo_news``\n\n**Recent Pricing Information**\n\n- ``self.latest_close``\n- ``self.latest_high``\n- ``self.latest_open``\n- ``self.latest_low``\n- ``self.latest_volume``\n- ``self.ask``\n- ``self.bid``\n\n**Latest Backtest Date and Intraday Minute**\n\n- ``self.latest_min``\n- ``self.backtest_date``\n\n.. note:: **self.latest_min** - Latest minute row in ``self.df_minute``\n\n.. note:: **self.backtest_date** - Latest dataset date which is considered the\n    backtest date for historical testing with the data pipeline\n    structure (it\'s the ``date`` key in the dataset node root level)\n\n**Balance Information**\n\n- ``self.balance``\n- ``self.prev_bal``\n\n.. note:: If a key is not in the dataset, the\n    algorithms\'s member variable will be an empty\n    pandas DataFrame created with: ``pd.DataFrame([])``\n    except ``self.pricing`` which is just a dictionary.\n    Please ensure the engine successfully fetched\n    and cached the dataset in redis using a tool like\n    ``redis-cli`` and a query of ``keys *`` or\n    ``keys <TICKER>_*`` on large deployments.\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport analysis_engine.algo as base_algo\nimport analysis_engine.utils as ae_utils\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\nclass ExampleMinuteAlgo(base_algo.BaseAlgo):\n    """"""ExampleMinuteAlgo""""""\n\n    def __init__(\n            self,\n            **kwargs):\n        """"""__init__\n\n        Custom ticker algo for analyzing intraday\n        minute datasets\n\n        Please refer to the `analysis_engine.algo.Ba\n        seAlgo source code for the latest supported parameters <ht\n        tps://github.com/AlgoTraders/stock-analysis-engine\n        /blob/master/\n        analysis_engine/algo.py>`__\n\n        :param kwargs: keyword arguments\n        """"""\n        log.info(f\'base - {kwargs.get(""name"", ""no-name-set"")}\')\n        super().__init__(**kwargs)\n        log.info(f\'ready - {self.name}\')\n    # end of __init__\n\n    def process(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""process\n\n        Derive custom algorithm buy and sell conditions\n        before placing orders. Just implement your own\n        ``process`` method.\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: a dictionary of identifiers (for debugging) and\n            multiple pandas ``pd.DataFrame`` objects. Dictionary where keys\n            represent a label from one of the data sources (``IEX``,\n            ``Yahoo``, ``FinViz`` or other). Here is the supported\n            dataset structure for the process method:\n\n            .. note:: There are no required keys for ``data``, the list\n                below is not hard-enforced by default. This is just\n                a reference for what is available with the v1 engine.\n\n            ::\n\n                dataset = {\n                    \'id\': <string TICKER_DATE - redis cache key>,\n                    \'date\': <string DATE>,\n                    \'data\': {\n                        \'daily\': pd.DataFrame([]),\n                        \'minute\': pd.DataFrame([]),\n                        \'quote\': pd.DataFrame([]),\n                        \'stats\': pd.DataFrame([]),\n                        \'peers\': pd.DataFrame([]),\n                        \'news1\': pd.DataFrame([]),\n                        \'financials\': pd.DataFrame([]),\n                        \'earnings\': pd.DataFrame([]),\n                        \'dividends\': pd.DataFrame([]),\n                        \'calls\': pd.DataFrame([]),\n                        \'puts\': pd.DataFrame([]),\n                        \'pricing\': dictionary,\n                        \'news\': pd.DataFrame([])\n                    }\n                }\n\n            example:\n\n            ::\n\n                dataset = {\n                    \'id\': \'SPY_2018-11-02\n                    \'date\': \'2018-11-02\',\n                    \'data\': {\n                        \'daily\': pd.DataFrame,\n                        \'minute\': pd.DataFrame,\n                        \'calls\': pd.DataFrame,\n                        \'puts\': pd.DataFrame,\n                        \'news\': pd.DataFrame\n                    }\n                }\n        """"""\n        num_minute_rows = len(self.df_minute.index)\n        label = (\n            f\'process - {self.name} - ticker={ticker}\')\n        log.info(\n            f\'{label} - start - \'\n            f\'date={self.backtest_date} minute={self.latest_min} \'\n            f\'close={self.latest_close} high={self.latest_high} \'\n            f\'low={self.latest_low} open={self.latest_open} \'\n            f\'volume={self.latest_volume} intraday rows={num_minute_rows}\')\n\n        # walk through today\'s intraday records:\n        # Provided by IEX\n        num_done = 1\n        total_records = num_minute_rows\n        for idx, row in self.df_minute.iterrows():\n            percent_label = self.build_progress_label(\n                progress=num_done,\n                total=total_records)\n            if \'date\' in row:\n                # want to debug rows on stdout?\n                # print(row)\n                log.info(\n                    f\'{label} - ANALYZE - \'\n                    f\'date={self.backtest_date} at \'\n                    f\'{row[""date""].strftime(""%H:%M:%S"")} close={row[""close""]} \'\n                    f\'high={row[""high""]} low={row[""low""]} open={row[""open""]} \'\n                    f\'volume={row[""volume""]} - {percent_label}\')\n                num_done += 1\n        # end for all rows in the dataset\n\n        log.info(\n            f\'{label} - done - \'\n            f\'date={self.backtest_date} minute={self.latest_min} \'\n            f\'balance={self.balance} previous_balance={self.prev_bal} \'\n            f\'shares={self.num_owned} buys={self.num_buys} \'\n            f\'sells={self.num_sells} close={self.latest_close} \'\n            f\'high={self.latest_high} low={self.latest_low} \'\n            f\'open={self.latest_open} volume={self.latest_volume} \'\n            f\'intraday rows={num_minute_rows}\')\n\n    # end of process\n\n    def get_result(\n            self):\n        """"""get_result""""""\n\n        log.info(\'building results\')\n        finished_date = ae_utils.utc_now_str()\n        self.result = {\n            \'name\': self.name,\n            \'created\': self.created_date,\n            \'updated\': finished_date,\n            \'open_positions\': self.positions,\n            \'buys\': self.get_buys(),\n            \'sells\': self.get_sells(),\n            \'num_processed\': len(self.order_history),\n            \'history\': self.order_history,\n            \'balance\': self.balance,\n            \'commission\': self.commission\n        }\n\n        return self.result\n    # end of get_result\n\n# end of ExampleMinuteAlgo\n\n\ndef get_algo(\n        **kwargs):\n    """"""get_algo\n\n    Make sure to define the ``get_algo`` for your custom\n    algorithms to work as a backup with the ``sa.py`` tool...\n    Not anticipating issues, but if we do with importlib\n    this is the backup plan.\n\n    Please file an issue if you see something weird and would like\n    some help:\n    https://github.com/AlgoTraders/stock-analysis-engine/issues\n\n    :param kwargs: dictionary of keyword arguments\n    """"""\n    log.info(\'getting algo\')\n    return ExampleMinuteAlgo(**kwargs)\n# end of get_algo\n'"
analysis_engine/mocks/example_indicator_williamsr.py,0,"b'""""""\nCustom Williams Percent R Indicator\n\nhttps://www.investopedia.com/terms/w/williamsr.asp\n\nMomentum\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport analysis_engine.ae_talib as ae_talib\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.indicators.base_indicator as base_indicator\n\n\nclass ExampleIndicatorWilliamsR(base_indicator.BaseIndicator):\n    """"""ExampleIndicatorWilliamsR""""""\n\n    def __init__(\n            self,\n            **kwargs):\n        """"""__init__\n\n        Custom indicator example for showing a Williams Percent R\n        within an algo for analyzing intraday minute datasets\n\n        Please refer to the `analysis_engine.indicators.base_indicator.Ba\n        seIndicator source code for the latest supported parameters <ht\n        tps://github.com/AlgoTraders/stock\n        -analysis-engine/blob/master/\n        analysis_engine/indicators/base_indicator.py>`__\n\n        :param kwargs: keyword arguments\n        """"""\n        super().__init__(**kwargs)\n    # end of __init__\n\n    def get_configurables(\n            self,\n            **kwargs):\n        """"""get_configurables\n\n        helper for setting up algorithm configs for this indicator\n        and programmatically set the values based off the domain\n        rules\n\n        .. code-block:: python\n\n            from analysis_engine.mocks.example_indicator_williamsr \\\n                import ExampleIndicatorWilliamsR\n            ind = ExampleIndicatorWilliamsR(config_dict={\n                    \'verbose\': True\n                }).get_configurables()\n\n        :param kwargs: keyword args dictionary\n        """"""\n        self.ind_confs = []\n\n        # common:\n        self.build_base_configurables(\n            ind_type=\'momentum\',\n            category=\'technical\',\n            uses_data=self.config.get(\n                \'uses_data\',\n                \'minute\'),\n            version=1)\n\n        # custom:\n        """"""\n        24,000 tests with this config:\n        """"""\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'num_points\',\n            conf_type=\'int\',\n            max_value=200,\n            min_value=3,\n            default_value=20,\n            inc_interval=10))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_below\',\n            conf_type=\'float\',\n            max_value=-70.0,\n            min_value=-90.0,\n            default_value=-80.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_above\',\n            conf_type=\'float\',\n            max_value=-1.0,\n            min_value=-29.0,\n            default_value=-20.0,\n            inc_interval=1))\n\n        # output / reporting:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'willr_value\',\n            conf_type=\'float\',\n            is_output_only=True))\n\n        default_values_dict = {}\n        for node in self.ind_confs:\n            name = node[\'name\']\n            default_value = node.get(\n                \'default\',\n                None)\n            default_values_dict[name] = default_value\n\n        use_file = None\n        try:\n            if __file__:\n                use_file = __file__\n        except Exception:\n            use_file = None\n\n        self.starter_dict = {\n            \'name\': self.__class__.__name__.lower().replace(\n                \'indicator\',\n                \'\'),\n            \'module_path\': use_file,\n            \'category\': default_values_dict.get(\n                \'category\',\n                \'momentum\'),\n            \'type\': default_values_dict.get(\n                \'type\',\n                \'technical\'),\n            \'uses_data\': default_values_dict.get(\n                \'uses_data\',\n                \'minute\'),\n            \'verbose\': default_values_dict.get(\n                \'verbose\',\n                False)\n        }\n        self.starter_dict.update(default_values_dict)\n\n        self.lg(\n            f\'configurables={ae_consts.ppj(self.ind_confs)} for \'\n            f\'class={self.__class__.__name__} in file={use_file} \'\n            f\'starter:\\n {ae_consts.ppj(self.starter_dict)}\')\n\n        return self.ind_confs\n    # end of get_configurables\n\n    def get_starter_dict(\n            self):\n        if not self.starter_dict:\n            self.get_configurables()\n        return self.starter_dict\n    # end of get_starter_dict\n\n    def process(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""process\n\n        Derive custom indicator processing to determine buy and sell\n        conditions before placing orders. Just implement your own\n        ``process`` method.\n\n        Please refer to the TA Lib guides for details on building indicators:\n\n        - Overlap Studies\n          https://mrjbq7.github.io/ta-lib/func_groups/overlap_studies.html\n        - Momentum Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html\n        - Volume Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volume_indicators.html\n        - Volatility Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volatility_indicators.html\n        - Price Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/price_transform.html\n        - Cycle Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/cycle_indicators.html\n        - Pattern Recognition\n          https://mrjbq7.github.io/ta-lib/func_groups/pattern_recognition.html\n        - Statistic Functions\n          https://mrjbq7.github.io/ta-lib/func_groups/statistic_functions.html\n        - Math Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/math_transform.html\n        - Math Operators\n          https://mrjbq7.github.io/ta-lib/func_groups/math_operators.html\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: dictionary of ``pandas.DataFrame(s)`` to process\n        """"""\n\n        # set the algo config indicator \'uses_data\' to \'day\' or \'minute\'\n        df_status, self.use_df = self.get_subscribed_dataset(\n            dataset=dataset)\n\n        self.willr_value = None\n        if df_status == ae_consts.EMPTY:\n            self.lg(\'process end - no data found\')\n            return\n\n        # notice the self.num_points is now a member variable\n        # because the BaseIndicator class\'s __init__\n        # converts any self.config keys into useable\n        # member variables automatically in your derived class\n        self.lg(\n            f\'process - num_points={self.num_points} \'\n            f\'df={len(self.use_df.index)}\')\n        """"""\n        real = WILLR(high, low, close, timeperiod=14)\n        """"""\n        num_records = len(self.use_df.index)\n        if num_records > self.num_points:\n            first_date = self.use_df[\'date\'].iloc[0]\n            end_date = self.use_df[\'date\'].iloc[-1]\n            start_row = num_records - self.num_points\n            self.use_df = self.use_df[start_row:num_records].dropna(\n                axis=0,\n                how=\'any\')\n\n            if len(self.use_df.index) == 0:\n                self.lg(f\'empty dataframe={self.uses_data} on date={end_date}\')\n                return\n\n            """"""\n            for idx, row in self.use_df[start_row:-1].iterrows():\n                high = row[\'high\']\n                low = row[\'low\']\n                open_val = row[\'open\']\n                close = row[\'close\']\n                row_date = row[\'date\']\n                self.lg(\n                    f\'{row_date} - WILLR(high={high}, low={low}, \'\n                    f\'close={close}, period={self.num_points})\')\n            """"""\n            highs = self.use_df[\'high\'].values\n            lows = self.use_df[\'low\'].values\n            closes = self.use_df[\'close\'].values\n            willr_values = ae_talib.WILLR(\n                highs,\n                lows,\n                closes,\n                self.num_points)\n            self.willr_value = ae_consts.to_f(\n                willr_values[-1])\n\n            """"""\n            Determine a buy or a sell as a label\n            """"""\n\n            self.is_buy = ae_consts.INDICATOR_IGNORE\n            self.is_sell = ae_consts.INDICATOR_IGNORE\n\n            if self.willr_value < self.buy_below:\n                self.is_buy = ae_consts.INDICATOR_BUY\n\n            if self.willr_value > self.sell_above:\n                self.is_sell = ae_consts.INDICATOR_SELL\n\n            self.lg(\n                f\'process end - {first_date} to {end_date} \'\n                f\'willr_value={self.willr_value} buy_below={self.buy_below} \'\n                f\'is_buy={self.is_buy} sell_above={self.sell_above} \'\n                f\'is_sell={self.is_sell}\')\n        else:\n            self.lg(f\'process end - willr={self.willr_value}\')\n    # end of process\n\n    def reset_internals(\n            self):\n        """"""reset_internals""""""\n        self.is_buy = ae_consts.INDICATOR_RESET\n        self.is_sell = ae_consts.INDICATOR_RESET\n    # end of reset_internals\n\n# end of ExampleIndicatorWilliamsR\n\n\ndef get_indicator(\n        **kwargs):\n    """"""get_indicator\n\n    Make sure to define the ``get_indicator`` for your custom\n    algorithms to work as a backup with the ``sa.py`` tool...\n    Not anticipating issues, but if we do with importlib\n    this is the backup plan.\n\n    Please file an issue if you see something weird and would like\n    some help:\n    https://github.com/AlgoTraders/stock-analysis-engine/issues\n\n    :param kwargs: dictionary of keyword arguments\n    """"""\n    print(\'getting indicator\')\n    return ExampleIndicatorWilliamsR(**kwargs)\n# end of get_indicator\n'"
analysis_engine/mocks/example_indicator_williamsr_open.py,0,"b'""""""\nCustom Williams Percent R Indicator that\nuses Open instead of Close\n\nhttps://www.investopedia.com/terms/w/williamsr.asp\n\nMomentum\n\n**Supported environment variables**\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n""""""\n\nimport analysis_engine.ae_talib as ae_talib\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.indicators.base_indicator as base_indicator\n\n\nclass ExampleIndicatorWilliamsROpen(base_indicator.BaseIndicator):\n    """"""ExampleIndicatorWilliamsROpen""""""\n\n    def __init__(\n            self,\n            **kwargs):\n        """"""__init__\n\n        Custom indicator example for showing a Williams Percent R\n        that uses ``open`` instead of ``close``\n        within an algo for analyzing intraday minute datasets\n\n        Please refer to the `analysis_engine.indicators.base_indicator.Ba\n        seIndicator source code for the latest supported parameters <ht\n        tps://github.com/AlgoTraders/stock\n        -analysis-engine/blob/master/\n        analysis_engine/indicators/base_indicator.py>`__\n\n        :param kwargs: keyword arguments\n        """"""\n        super().__init__(**kwargs)\n    # end of __init__\n\n    def get_configurables(\n            self,\n            **kwargs):\n        """"""get_configurables\n\n        helper for setting up algorithm configs for this indicator\n        and programmatically set the values based off the domain\n        rules\n\n        .. code-block:: python\n\n            from analysis_engine.mocks.example_indicator_williamsr_open \\\n                import ExampleIndicatorWilliamsROpen\n            ind = ExampleIndicatorWilliamsROpen(config_dict={\n                    \'verbose\': True\n                }).get_configurables()\n\n        :param kwargs: keyword args dictionary\n        """"""\n        self.ind_confs = []\n\n        # common:\n        self.build_base_configurables(\n            ind_type=\'momentum\',\n            category=\'technical\',\n            uses_data=self.config.get(\n                \'uses_data\',\n                \'minute\'),\n            version=1)\n\n        # custom:\n        """"""\n        24,000 tests with this config:\n        """"""\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'num_points\',\n            conf_type=\'int\',\n            max_value=200,\n            min_value=3,\n            default_value=20,\n            inc_interval=10))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'buy_below\',\n            conf_type=\'float\',\n            max_value=-70.0,\n            min_value=-90.0,\n            default_value=-80.0,\n            inc_interval=1))\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'sell_above\',\n            conf_type=\'float\',\n            max_value=-1.0,\n            min_value=-29.0,\n            default_value=-20.0,\n            inc_interval=1))\n\n        # output / reporting:\n        self.ind_confs.append(self.build_configurable_node(\n            name=\'willr_open_value\',\n            conf_type=\'float\',\n            is_output_only=True))\n\n        default_values_dict = {}\n        for node in self.ind_confs:\n            name = node[\'name\']\n            default_value = node.get(\n                \'default\',\n                None)\n            default_values_dict[name] = default_value\n\n        use_file = None\n        try:\n            if __file__:\n                use_file = __file__\n        except Exception:\n            use_file = None\n\n        self.starter_dict = {\n            \'name\': self.__class__.__name__.lower().replace(\n                \'indicator\',\n                \'\'),\n            \'module_path\': use_file,\n            \'category\': default_values_dict.get(\n                \'category\',\n                \'momentum\'),\n            \'type\': default_values_dict.get(\n                \'type\',\n                \'technical\'),\n            \'uses_data\': default_values_dict.get(\n                \'uses_data\',\n                \'minute\'),\n            \'verbose\': default_values_dict.get(\n                \'verbose\',\n                False)\n        }\n        self.starter_dict.update(default_values_dict)\n\n        self.lg(\n            f\'configurables={ae_consts.ppj(self.ind_confs)} for \'\n            f\'class={self.__class__.__name__} in file={use_file} \'\n            f\'starter:\\n {ae_consts.ppj(self.starter_dict)}\')\n\n        return self.ind_confs\n    # end of get_configurables\n\n    def get_starter_dict(\n            self):\n        if not self.starter_dict:\n            self.get_configurables()\n        return self.starter_dict\n    # end of get_starter_dict\n\n    def process(\n            self,\n            algo_id,\n            ticker,\n            dataset):\n        """"""process\n\n        Derive custom indicator processing to determine buy and sell\n        conditions before placing orders. Just implement your own\n        ``process`` method.\n\n        Please refer to the TA Lib guides for details on building indicators:\n\n        - Overlap Studies\n          https://mrjbq7.github.io/ta-lib/func_groups/overlap_studies.html\n        - Momentum Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html\n        - Volume Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volume_indicators.html\n        - Volatility Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/volatility_indicators.html\n        - Price Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/price_transform.html\n        - Cycle Indicators\n          https://mrjbq7.github.io/ta-lib/func_groups/cycle_indicators.html\n        - Pattern Recognition\n          https://mrjbq7.github.io/ta-lib/func_groups/pattern_recognition.html\n        - Statistic Functions\n          https://mrjbq7.github.io/ta-lib/func_groups/statistic_functions.html\n        - Math Transform\n          https://mrjbq7.github.io/ta-lib/func_groups/math_transform.html\n        - Math Operators\n          https://mrjbq7.github.io/ta-lib/func_groups/math_operators.html\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: dictionary of ``pandas.DataFrame(s)`` to process\n        """"""\n\n        # set the algo config indicator \'uses_data\' to \'day\' or \'minute\'\n        df_status, self.use_df = self.get_subscribed_dataset(\n            dataset=dataset)\n\n        self.willr_open_value = None\n        if df_status == ae_consts.EMPTY:\n            self.lg(\'process end - no data found\')\n            return\n\n        # notice the self.num_points is now a member variable\n        # because the BaseIndicator class\'s __init__\n        # converts any self.config keys into useable\n        # member variables automatically in your derived class\n        self.lg(\n            f\'process - num_points={self.num_points} \'\n            f\'df={len(self.use_df.index)}\')\n        """"""\n        real = WILLR(high, low, open, timeperiod=14)\n        """"""\n        num_records = len(self.use_df.index)\n        if num_records > self.num_points:\n            first_date = self.use_df[\'date\'].iloc[0]\n            end_date = self.use_df[\'date\'].iloc[-1]\n            start_row = num_records - self.num_points\n            self.use_df = self.use_df[start_row:num_records].dropna(\n                axis=0,\n                how=\'any\')\n\n            if len(self.use_df.index) == 0:\n                self.lg(f\'empty dataframe={self.uses_data} on date={end_date}\')\n                return\n\n            """"""\n            for idx, row in self.use_df[start_row:-1].iterrows():\n                high = row[\'high\']\n                low = row[\'low\']\n                open_val = row[\'open\']\n                close = row[\'close\']\n                row_date = row[\'date\']\n                self.lg(\n                    f\'{row_date} - WILLROPEN(high={high}, low={low}, \'\n                    f\'open={open_val}, period={self.num_points})\')\n            """"""\n            highs = self.use_df[\'high\'].values\n            lows = self.use_df[\'low\'].values\n            opens = self.use_df[\'open\'].values\n            willr_open_values = ae_talib.WILLR(\n                highs,\n                lows,\n                opens,\n                self.num_points)\n            self.willr_open_value = ae_consts.to_f(\n                willr_open_values[-1])\n\n            """"""\n            Determine a buy or a sell as a label\n            """"""\n\n            self.is_buy = ae_consts.INDICATOR_IGNORE\n            self.is_sell = ae_consts.INDICATOR_IGNORE\n\n            if self.willr_open_value < self.buy_below:\n                self.is_buy = ae_consts.INDICATOR_BUY\n\n            if self.willr_open_value > self.sell_above:\n                self.is_sell = ae_consts.INDICATOR_SELL\n\n            self.lg(\n                f\'process end - {first_date} to {end_date} \'\n                f\'willr_open_value={self.willr_open_value} \'\n                f\'buy_below={self.buy_below} is_buy={self.is_buy} \'\n                f\'sell_above={self.sell_above} is_sell={self.is_sell}\')\n        else:\n            self.lg(f\'process end - willr={self.willr_open_value}\')\n    # end of process\n\n    def reset_internals(\n            self):\n        """"""reset_internals""""""\n        self.is_buy = ae_consts.INDICATOR_RESET\n        self.is_sell = ae_consts.INDICATOR_RESET\n    # end of reset_internals\n\n# end of ExampleIndicatorWilliamsROpen\n\n\ndef get_indicator(\n        **kwargs):\n    """"""get_indicator\n\n    Make sure to define the ``get_indicator`` for your custom\n    algorithms to work as a backup with the ``sa.py`` tool...\n    Not anticipating issues, but if we do with importlib\n    this is the backup plan.\n\n    Please file an issue if you see something weird and would like\n    some help:\n    https://github.com/AlgoTraders/stock-analysis-engine/issues\n\n    :param kwargs: dictionary of keyword arguments\n    """"""\n    print(\'getting indicator\')\n    return ExampleIndicatorWilliamsROpen(**kwargs)\n# end of get_indicator\n'"
analysis_engine/mocks/mock_algo_trading.py,0,"b'""""""\nMock Algorithm Methods for unittesting things\nlike previously-owned shares or sell-side indicators\nwithout owning shares\n""""""\n\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef mock_algo_owns_shares_in_ticker_before_starting(\n        obj,\n        ticker):\n    """"""mock_algo_owns_shares_in_ticker_before_starting\n\n    Support mocking owned shares to test indicator selling\n\n    If you can modify your algorithm ``config_dict`` you can\n    also set a ``positions`` dictionary like:\n\n    .. code-block:: python\n\n        algo_config_dict = {\n            # other values omitted for docs\n            \'positions\': {\n                \'SPY\': {\n                    \'shares\': 10000,\n                    \'buys\': [],\n                    \'sells\': []\n                }\n            }\n        }\n\n\n    Use with your custom algorithm unittests:\n\n    .. code-block:: python\n\n        import mock\n        import analysis_engine.mocks.mock_algo_trading as mock_trading\n\n        @mock.patch(\n            (\'analysis_engine.algo.BaseAlgo.get_ticker_positions\'),\n            new=mock_trading.mock_algo_owns_shares_in_ticker_before_starting)\n\n\n    :param obj: algorithm object\n    :param ticker: ticker symbol\n    """"""\n    num_owned = 10000\n    buys = []\n    sells = []\n    return num_owned, buys, sells\n# end of mock_algo_owns_shares_in_ticker_before_starting\n'"
analysis_engine/mocks/mock_boto3_s3.py,0,"b'""""""\nMock boto3 s3 objects\n""""""\n\nimport os\nimport json\nimport analysis_engine.consts as ae_consts\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef mock_s3_read_contents_from_key_ev(\n        s3,\n        s3_bucket_name,\n        s3_key,\n        encoding,\n        convert_as_json):\n    """"""mock_s3_read_contents_from_key\n\n    :param s3: s3 client\n    :param s3_bucket_name: bucket name\n    :param s3_key: key\n    :param encoding: utf-8\n    :param convert_as_json: convert to json\n    """"""\n\n    env_key = \'TEST_S3_CONTENTS\'\n    str_contents = ae_consts.ev(\n        env_key,\n        None)\n\n    log.info(\n        f\'returning mock s3={s3_bucket_name}:{s3_key} contents={str_contents} \'\n        f\'encoding={encoding} json={convert_as_json} env_key={env_key}\')\n\n    if not str_contents:\n        return str_contents\n\n    if convert_as_json:\n        return json.loads(str_contents)\n# end of mock_s3_read_contents_from_key_ev\n\n\ndef mock_publish_from_s3_to_redis(\n        work_dict):\n    """"""mock_publish_from_s3_to_redis\n\n    :param work_dict: dictionary for driving the task\n    """"""\n\n    env_key = \'TEST_S3_CONTENTS\'\n    redis_key = work_dict.get(\n        \'redis_key\',\n        env_key)\n    str_dict = ae_consts.ev(\n        env_key,\n        None)\n    log.info(\n        \'mock_publish_from_s3_to_redis - \'\n        f\'setting key={redis_key} value={str_dict}\')\n    data = None\n    if str_dict:\n        os.environ[redis_key] = str_dict\n        data = str_dict.encode(\'utf-8\')\n    else:\n        os.environ[redis_key] = \'\'\n        data = None\n\n    status = ae_consts.SUCCESS\n    err = None\n    return {\n        \'status\': status,\n        \'err\': err,\n        \'rec\': {\n            \'data\': data\n        }\n    }\n# end of mock_publish_from_s3_to_redis\n\n\ndef mock_publish_from_s3_to_redis_err(\n        work_dict):\n    """"""mock_publish_from_s3_to_redis_err\n\n    :param work_dict: dictionary for driving the task\n    """"""\n\n    env_key = \'TEST_S3_CONTENTS\'\n    redis_key = work_dict.get(\n        \'redis_key\',\n        env_key)\n    str_dict = ae_consts.ev(\n        env_key,\n        None)\n    log.info(\n        \'mock_publish_from_s3_to_redis_err - \'\n        f\'setting key={redis_key} value={str_dict}\')\n    data = None\n    if str_dict:\n        os.environ[redis_key] = str_dict\n        data = str_dict.encode(\'utf-8\')\n    else:\n        os.environ[redis_key] = \'\'\n        data = None\n\n    status = ae_consts.ERR\n    err = None\n    return {\n        \'status\': status,\n        \'err\': err,\n        \'rec\': {\n            \'data\': data\n        }\n    }\n# end of mock_publish_from_s3_to_redis_err\n\n\ndef mock_publish_from_s3_exception(\n        work_dict):\n    """"""mock_publish_from_s3_exception\n\n    :param work_dict: dictionary for driving the task\n    """"""\n    raise Exception(\n        \'test mock_publish_from_s3_exception\')\n# end of mock_publish_from_s3_exception\n\n\nclass MockBotoS3Bucket:\n    """"""MockBotoS3Bucket""""""\n\n    def __init__(\n            self,\n            name):\n        """"""__init__\n\n        build mock bucket\n\n        :param name: name of the bucket\n        """"""\n        self.name = name\n        self.datas = []  # payloads uploaded to s3\n        self.keys = []   # keys uploaded to s3\n    # end of __init__\n\n    def put_object(\n            self,\n            Key=None,\n            Body=None):\n        """"""put_object\n\n        :param Key: new Key name\n        :param Body: new Payload in Key\n        """"""\n\n        log.debug(\n            f\'mock - MockBotoS3Bucket.put_object(Key={Key}, \'\n            f\'Body={Body})\')\n\n        self.keys.append(Key)\n        self.datas.append(Body)\n    # end of put_object\n\n# end of MockBotoS3Bucket\n\n\nclass MockBotoS3AllBuckets:\n    """"""MockBotoS3AllBuckets""""""\n\n    def __init__(\n            self):\n        """"""__init__""""""\n        self.buckets = {}\n    # end of __init__\n\n    def add(\n            self,\n            bucket_name):\n        """"""add\n\n        :param bucket_name: bucket name to add\n        """"""\n        if bucket_name not in self.buckets:\n            log.info(\n                f\'adding bucket={bucket_name} total={len(self.buckets) + 1}\')\n            self.buckets[bucket_name] = MockBotoS3Bucket(\n                name=bucket_name)\n\n        return self.buckets[bucket_name]\n    # end of add\n\n    def all(\n            self):\n        """"""all""""""\n        return self.buckets\n    # end of all\n\n# end of MockBotoS3AllBuckets\n\n\nclass MockBotoS3:\n    """"""MockBotoS3""""""\n\n    def __init__(\n            self,\n            name=\'mock_s3\',\n            endpoint_url=None,\n            aws_access_key_id=None,\n            aws_secret_access_key=None,\n            region_name=None,\n            config=None):\n        """"""__init__\n\n        build mock object\n\n        :param name: name of client\n        :param endpoint_url: endpoint url\n        :param aws_access_key_id: aws access key\n        :param aws_secret_access_key: aws secret key\n        :param region_name: region name\n        :param config: config object\n        """"""\n\n        self.name = name\n        self.endpoint_url = endpoint_url\n        self.aws_access_key_id = aws_access_key_id\n        self.aws_secret_access_key = aws_secret_access_key\n        self.region_name = region_name\n        self.config = config\n        self.buckets = MockBotoS3AllBuckets()\n        self.keys = []\n    # end of __init__\n\n    def Bucket(\n            self,\n            name):\n        """"""Bucket\n\n        :param name: name of new bucket\n        """"""\n        log.info(\n            f\'MockBotoS3.Bucket({name})\')\n        return self.buckets.add(\n            bucket_name=name)\n    # end of Bucket\n\n    def create_bucket(\n            self,\n            Bucket=None):\n        """"""create_bucket\n\n        :param bucket_name: name of the new bucket\n        """"""\n        log.info(\n            f\'mock - MockBotoS3.create_bucket(Bucket={Bucket})\')\n        return self.buckets.add(\n            bucket_name=Bucket)\n    # end of create_bucket\n\n# end of MockBotoS3\n\n\ndef build_boto3_resource(\n        name=\'mock_s3\',\n        endpoint_url=None,\n        aws_access_key_id=None,\n        aws_secret_access_key=None,\n        region_name=None,\n        config=None):\n    """"""build_boto3_resource\n\n    :param name: name of client\n    :param endpoint_url: endpoint url\n    :param aws_access_key_id: aws access key\n    :param aws_secret_access_key: aws secret key\n    :param region_name: region name\n    :param config: config object\n    """"""\n\n    if \'s3\' in name.lower():\n        return MockBotoS3(\n            name=\'mock_s3\',\n            endpoint_url=endpoint_url,\n            aws_access_key_id=aws_access_key_id,\n            aws_secret_access_key=aws_secret_access_key,\n            region_name=region_name,\n            config=config)\n    else:\n        return MockBotoS3(\n            name=\'mock_s3\',\n            endpoint_url=endpoint_url,\n            aws_access_key_id=aws_access_key_id,\n            aws_secret_access_key=aws_secret_access_key,\n            region_name=region_name,\n            config=config)\n# end of build_boto3_resource\n'"
analysis_engine/mocks/mock_iex.py,0,"b'""""""\nMocking data fetch api calls\n""""""\n\nimport datetime\n\n\ndef mock_daily(\n        url,\n        token=None,\n        version=None,\n        verbose=False):\n    """"""mock_daily\n\n    mock minute history for a chart\n\n    :param url: IEX resource url\n    :param token: optional - string token for your user\'s\n        account\n    :param version: optional - version string\n    :param verbose: optional - boolean debug logging\n    """"""\n\n    val = {\n        \'url\': url,\n        \'version\': version,\n        \'symbol\': url.split(\'/\')[2],\n        \'timeframe\': \'3m\',\n        \'date\': datetime.datetime.now().strftime(\'%Y-%m-%d\'),\n        \'testcase\': \'mock-daily\'\n    }\n    return [val]\n# end of mock_daily\n\n\ndef mock_minute(\n        url,\n        token=None,\n        version=None,\n        verbose=False):\n    """"""mock_minute\n\n    mock minute history for a chart\n\n    :param url: IEX resource url\n    :param token: optional - string token for your user\'s\n        account\n    :param version: optional - version string\n    :param verbose: optional - boolean debug logging\n    """"""\n\n    now = datetime.datetime.now()\n    val = {\n        \'url\': url,\n        \'version\': version,\n        \'symbol\': url.split(\'/\')[2],\n        \'timeframe\': \'1d\',\n        \'date\': now.strftime(\'%Y-%m-%d\'),\n        \'minute\': now.strftime(\'%H:%M\'),\n        \'testcase\': \'mock-minute\'\n\n    }\n    return [val]\n# end of mock_minute\n\n\ndef mock_quote(\n        url,\n        token=None,\n        version=None,\n        verbose=False):\n    """"""mock_quote\n\n    mock quote\n\n    :param url: IEX resource url\n    :param token: optional - string token for your user\'s\n        account\n    :param version: optional - version string\n    :param verbose: optional - boolean debug logging\n    """"""\n\n    val = {\n        \'url\': url,\n        \'version\': version,\n        \'symbol\': url.split(\'/\')[2],\n        \'testcase\': \'mock-quote\'\n    }\n    return val\n# end of mock_quote\n\n\ndef mock_stats(\n        url,\n        token=None,\n        version=None,\n        verbose=False):\n    """"""mock_stats\n\n    mock stats\n\n    :param url: IEX resource url\n    :param token: optional - string token for your user\'s\n        account\n    :param version: optional - version string\n    :param verbose: optional - boolean debug logging\n    """"""\n\n    val = {\n        \'url\': url,\n        \'version\': version,\n        \'symbol\': url.split(\'/\')[2],\n        \'testcase\': \'mock-stats\'\n    }\n    return val\n# end of mock_stats\n\n\ndef mock_peers(\n        url,\n        token=None,\n        version=None,\n        verbose=False):\n    """"""mock_peers\n\n    mock peers\n\n    :param url: IEX resource url\n    :param token: optional - string token for your user\'s\n        account\n    :param version: optional - version string\n    :param verbose: optional - boolean debug logging\n    """"""\n\n    val = {\n        \'url\': url,\n        \'version\': version,\n        \'symbol\': url.split(\'/\')[2],\n        \'testcase\': \'mock-peers\'\n    }\n    return [val]\n# end of mock_peers\n\n\ndef mock_news(\n        url,\n        token=None,\n        version=None,\n        verbose=False):\n    """"""mock_news\n\n    mock news\n\n    :param url: IEX resource url\n    :param token: optional - string token for your user\'s\n        account\n    :param version: optional - version string\n    :param verbose: optional - boolean debug logging\n    """"""\n    now = datetime.datetime.now()\n    epoch = datetime.datetime.utcfromtimestamp(0)\n    now_ms = (now - epoch).total_seconds() * 1000.0\n\n    val = {\n        \'url\': url,\n        \'version\': version,\n        \'datetime\': now_ms,\n        \'symbol\': url.split(\'/\')[2],\n        \'count\': 5,\n        \'testcase\': \'mock-news\'\n    }\n    return [val]\n# end of mock_news\n\n\ndef mock_financials(\n        url,\n        token=None,\n        version=None,\n        verbose=False):\n    """"""mock_financials\n\n    mock financials\n\n    :param url: IEX resource url\n    :param token: optional - string token for your user\'s\n        account\n    :param version: optional - version string\n    :param verbose: optional - boolean debug logging\n    """"""\n\n    val = {\n        \'url\': url,\n        \'version\': version,\n        \'symbol\': url.split(\'/\')[2],\n        \'testcase\': \'mock-financials\'\n    }\n    return {\n        \'financials\': [val]\n    }\n# end of mock_financials\n\n\ndef mock_earnings(\n        url,\n        token=None,\n        version=None,\n        verbose=False):\n    """"""mock_earnings\n\n    mock earnings\n\n    :param url: IEX resource url\n    :param token: optional - string token for your user\'s\n        account\n    :param version: optional - version string\n    :param verbose: optional - boolean debug logging\n    """"""\n\n    val = {\n        \'url\': url,\n        \'version\': version,\n        \'symbol\': url.split(\'/\')[2],\n        \'testcase\': \'mock-earnings\'\n    }\n    return {\n        \'earnings\': [val]\n    }\n# end of mock_earnings\n\n\ndef mock_dividends(\n        url,\n        token=None,\n        version=None,\n        verbose=False):\n    """"""mock_dividends\n\n    mock dividends\n\n    :param url: IEX resource url\n    :param token: optional - string token for your user\'s\n        account\n    :param version: optional - version string\n    :param verbose: optional - boolean debug logging\n    """"""\n\n    val = {\n        \'url\': url,\n        \'version\': version,\n        \'symbol\': url.split(\'/\')[2],\n        \'testcase\': \'mock-dividends\'\n    }\n    return [val]\n# end of mock_dividends\n\n\ndef mock_company(\n        url,\n        token=None,\n        version=None,\n        verbose=False):\n    """"""mock_company\n\n    mock company\n\n    :param url: IEX resource url\n    :param token: optional - string token for your user\'s\n        account\n    :param version: optional - version string\n    :param verbose: optional - boolean debug logging\n    """"""\n\n    val = {\n        \'url\': url,\n        \'version\': version,\n        \'symbol\': url.split(\'/\')[2],\n        \'testcase\': \'mock-company\'\n    }\n    return val\n# end of mock_company\n'"
analysis_engine/mocks/mock_pinance.py,0,"b'""""""\nMock Pinance Object for unittests\n""""""\n\nimport os\nimport analysis_engine.api_requests as api_requests\n\n\ndef mock_get_options(\n        ticker=None,\n        contract_type=None,\n        exp_date_str=None,\n        strike=None):\n    """"""mock_get_options\n\n    :param ticker: ticker to lookup\n    :param exp_date_str: ``YYYY-MM-DD`` expiration date format\n    :param strike: optional strike price, ``None`` returns\n                    all option chains\n    :param contract_type: ``C`` calls or ``P`` for puts, if\n                            ``strike=None`` then the ``contract_type``\n                            is ignored\n    """"""\n    mock_cache_data = api_requests.build_cache_ready_pricing_dataset(\n        label=\'ticker\')\n    options_data = os.getenv(\n        \'TEST_OPTIONS\',\n        mock_cache_data[\'options\'])\n    return options_data\n# end of mock_get_options\n\n\nclass MockPinance:\n    """"""MockPinance""""""\n\n    def __init__(\n            self,\n            symbol=\'SPY\'):\n        """"""__init__\n\n        :param symbol: ticker\n        """"""\n\n        mock_cache_data = api_requests.build_cache_ready_pricing_dataset(\n            label=\'ticker\')\n        self.symbol = symbol\n        self.quotes_data = mock_cache_data[\'pricing\']\n        self.news_data = mock_cache_data[\'news\']\n        self.options_data = mock_cache_data[\'options\']\n    # end of __init__\n\n    def get_quotes(\n            self):\n        """"""get_quotes""""""\n        return self.quotes_data\n    # end of get_quotes\n\n    def get_news(\n            self):\n        """"""get_news""""""\n        return self.news_data\n    # end of get_news\n\n    def get_options(\n            self,\n            ticker=None,\n            contract_type=None,\n            exp_date_str=None,\n            strike=None):\n        """"""get_options\n\n        :param ticker: ticker to lookup\n        :param exp_date_str: ``YYYY-MM-DD`` expiration date format\n        :param strike: optional strike price, ``None`` returns\n                       all option chains\n        :param contract_type: ``C`` calls or ``P`` for puts, if\n                              ``strike=None`` then the ``contract_type``\n                              is ignored\n        """"""\n        return [\n            self.options_data\n        ]\n    # end of get_options\n\n# end of MockPinance\n'"
analysis_engine/mocks/mock_redis.py,0,"b'""""""\nMock redis objects\n""""""\n\nimport analysis_engine.consts as ae_consts\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\nclass MockRedisFailToConnect:\n    """"""MockRedisFailToConnect""""""\n\n    def __init__(\n            self,\n            host=None,\n            port=None,\n            password=None,\n            db=None):\n        """"""__init__\n\n        build a mock redis client that will raise an exception\n        to test failures during connection\n\n        :param host: hostname\n        :param port: port\n        :param password: password\n        :param db: database number\n        """"""\n        raise Exception(\n            \'test MockRedisFailToConnect\')\n    # end of __init__\n\n# end of MockRedisFailToConnect\n\n\nclass MockRedis:\n    """"""MockRedis""""""\n\n    def __init__(\n            self,\n            host=None,\n            port=None,\n            password=None,\n            db=None):\n        """"""__init__\n\n        build mock redis client\n\n        :param host: hostname\n        :param port: port\n        :param password: password\n        :param db: database number\n        """"""\n        self.host = host\n        self.port = port\n        self.password = password\n        self.db = db\n        self.cache_dict = {}  # cache dictionary replicating redis\n        self.keys = []        # cache redis keys\n    # end of __init__\n\n    def set(\n            self,\n            name=None,\n            value=None,\n            ex=None,\n            px=None,\n            nx=False,\n            xx=False):\n        """"""set\n\n        mock redis set\n\n        :param name: cache key name\n        :param value: value to cache\n        :param ex: expire time\n        :param px: redis values\n        :param nx: redis values\n        :param xx: redis values\n        """"""\n\n        log.info(\n            \'mock - MockRedis.set(\'\n            f\'name={name}, \'\n            f\'value={value}, \'\n            f\'ex={ex}, \'\n            f\'px={px}, \'\n            f\'nx={nx}, \'\n            f\'xx={xx})\')\n        self.cache_dict[name] = value\n        self.keys.append(name)\n    # end of set\n\n    def get(\n            self,\n            name=None):\n        """"""get\n\n        mock redis get\n\n        :param name: name of the key to check\n        """"""\n        if not name:\n            err = (\n                \'mock - MockRedis.get(\'\n                f\'name={name}\'\n                \') - missing a name\')\n            log.error(err)\n            raise Exception(err)\n        value_in_dict = self.cache_dict.get(\n            name,\n            None)\n        if not value_in_dict:\n            value_in_env = ae_consts.ev(\n                name,\n                None)\n            log.info(\n                \'mock - MockRedis.get(\'\n                f\'name={name}) env={value_in_env}\')\n            if value_in_env:\n                return value_in_env.encode(\'utf-8\')\n            else:\n                return None\n        else:\n            log.info(\n                \'mock - MockRedis.get(\'\n                f\'name={name}) cached_value={value_in_dict}\')\n            return value_in_dict\n        # end of get data from dict vs in the env\n    # end of get\n\n# end of MockRedis\n'"
analysis_engine/mocks/mock_talib.py,0,"b'""""""\nMock TA-Lib objects\n""""""\n\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef MockWILLRBuy(\n        high=None,\n        low=None,\n        close=None,\n        timeperiod=None):\n    """"""MockWILLRBuy\n\n    build a mock wiliams r object that will report\n    an ``buy`` value to test indicators without\n    having talib installed\n\n    :param high: list of highs\n    :param low: list of lows\n    :param close: list of closes\n    :param timeperiod: integer number of values\n        in ``high``, ``low`` and ``close``\n    """"""\n    log.warn(\'mock - MockTALib.WILLR - BUY\')\n    retval = []\n    for h in high:\n        retval.append(None)\n    retval[-1] = -99.9\n    return retval\n# end of MockWILLRBuy\n\n\ndef MockWILLRSell(\n        high=None,\n        low=None,\n        close=None,\n        timeperiod=None):\n    """"""MockWILLRSell\n\n    build a mock wiliams r object that will report\n    an ``sell`` value to test indicators without\n    having talib installed\n\n    :param high: list of highs\n    :param low: list of lows\n    :param close: list of closes\n    :param timeperiod: integer number of values\n        in ``high``, ``low`` and ``close``\n    """"""\n    log.warn(\'mock - MockTALib.WILLR - SELL\')\n    retval = []\n    for h in high:\n        retval.append(None)\n    retval[-1] = -1.0\n    return retval\n# end of MockWILLRSell\n\n\ndef MockWILLRIgnore(\n        high=None,\n        low=None,\n        close=None,\n        timeperiod=None):\n    """"""MockWILLRIgnore\n\n    build a mock wiliams r object that will report\n    an ``ignore`` value to test indicators without\n    having talib installed\n\n    :param high: list of highs\n    :param low: list of lows\n    :param close: list of closes\n    :param timeperiod: integer number of values\n        in ``high``, ``low`` and ``close``\n    """"""\n    log.warn(\'mock - MockTALib.WILLR - IGNORE\')\n    retval = []\n    for h in high:\n        retval.append(None)\n    retval[-1] = -50.0\n    return retval\n# end of MockWILLRIgnore\n'"
analysis_engine/perf/__init__.py,0,b''
analysis_engine/perf/profile_algo_runner.py,0,"b'""""""\nExample tool for to profiling algorithm performance for:\n\n- CPU\n- Memory\n- Profiler\n- Heatmap\n\nThe pip includes `vprof for profiling algorithm code\nperformance <https://github.com/nvdv/vprof>`__\n\n#.  Start vprof in remote mode in a first terminal\n\n    .. note:: This command will start a webapp on port ``3434``\n\n    ::\n\n        vprof -r -p 3434\n\n#.  Start Profiler in a second terminal\n\n    .. note:: This command pushes data to the webapp\n        in the other terminal listening on port ``3434``\n\n    ::\n\n        vprof -c cm ./analysis_engine/perf/profile_algo_runner.py\n""""""\n\nimport datetime\nimport vprof.runner as perf_runner\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.algo_runner as algo_runner\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(\n    name=\'profile-algo\')\n\n\ndef start():\n    """"""start""""""\n\n    back_a_few_days = (\n        datetime.datetime.now() - datetime.timedelta(days=3))\n    start_date = back_a_few_days.strftime(\n        ae_consts.COMMON_DATE_FORMAT)\n\n    ticker = \'SPY\'\n    s3_bucket = \'perftests\'\n    s3_key = (\n        f\'{ticker}_{start_date}\')\n\n    algo_config = (\n        f\'./cfg/default_algo.json\')\n    history_loc = (\n        f\'s3://{s3_bucket}/{s3_key}\')\n\n    log.info(\n        f\'building {ticker} trade history \'\n        f\'start_date={start_date} \'\n        f\'config={algo_config} \'\n        f\'history_loc={history_loc}\')\n\n    runner = algo_runner.AlgoRunner(\n        ticker=ticker,\n        start_date=start_date,\n        history_loc=history_loc,\n        algo_config=algo_config,\n        verbose_algo=False,\n        verbose_processor=False,\n        verbose_indicators=False)\n\n    runner.start()\n# end of start\n\n\nif __name__ == \'__main__\':\n    perf_runner.run(start, \'cm\', args=(), host=\'0.0.0.0\', port=3434)\n'"
analysis_engine/scripts/__init__.py,0,b''
analysis_engine/scripts/aws_backup.py,0,"b'#!/usr/bin/env python\n\nimport os\nimport sys\nimport datetime\nimport argparse\nimport celery\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.run_custom_algo as run_custom_algo\nimport analysis_engine.work_tasks.get_celery_app as get_celery_app\nimport analysis_engine.plot_trading_history as plot_trading_history\nimport analysis_engine.charts as ae_charts\nimport analysis_engine.iex.extract_df_from_redis as extract_utils\nimport analysis_engine.show_dataset as show_dataset\nimport analysis_engine.load_history_dataset_from_file as load_history\nimport analysis_engine.load_report_dataset_from_file as load_report\nimport analysis_engine.restore_dataset as restore_dataset\nimport analysis_engine.work_tasks.prepare_pricing_dataset as prep_dataset\nimport analysis_engine.api_requests as api_requests\nimport spylunking.log.setup_logging as log_utils\n\n\n# Disable celery log hijacking\n# https://github.com/celery/celery/issues/2509\n@celery.signals.setup_logging.connect\ndef setup_celery_logging(**kwargs):\n    pass\n\n\nlog = log_utils.build_colorized_logger(\n    name=\'aws-backup\',\n    log_config_path=ae_consts.LOG_CONFIG_PATH)\n\n\ndef restore_missing_dataset_values_from_algo_ready_file(\n        ticker,\n        path_to_file,\n        redis_address,\n        redis_password,\n        redis_db=ae_consts.REDIS_DB,\n        output_redis_db=None,\n        compress=False,\n        encoding=\'utf-8\',\n        dataset_type=ae_consts.SA_DATASET_TYPE_ALGO_READY,\n        serialize_datasets=ae_consts.DEFAULT_SERIALIZED_DATASETS,\n        show_summary=True):\n    """"""restore_missing_dataset_values_from_algo_ready_file\n\n    restore missing dataset nodes in redis from an algorithm-ready\n    dataset file on disk - use this to restore redis from scratch\n\n    :param ticker: string ticker\n    :param path_to_file: string path to file on disk\n    :param redis_address: redis server endpoint adddress with\n        format ``host:port``\n    :param redis_password: optional - string password for redis\n    :param redis_db: redis db (default is ``REDIS_DB``)\n    :param output_redis_db: optional - integer for different\n        redis database (default is ``None``)\n    :param compress: contents in algorithm-ready file are\n        compressed (default is ``False``)\n    :param encoding: byte encoding of algorithm-ready file\n        (default is ``utf-8``)\n    :param dataset_type: optional - dataset type\n        (default is ``SA_DATASET_TYPE_ALGO_READY``)\n    :param serialize_datasets: optional - list of dataset names to\n        deserialize in the dataset\n    :param show_summary: optional - show a summary of the algorithm-ready\n        dataset using ``analysis_engine.show_dataset.show_dataset``\n        (default is ``True``)\n    """"""\n    if not os.path.exists(path_to_file):\n        log.error(f\'missing file={path_to_file} for restore\')\n        return\n\n    if dataset_type == ae_consts.SA_DATASET_TYPE_ALGO_READY:\n        log.info(f\'restore start - load dataset from file={path_to_file}\')\n    else:\n        log.error(\n            f\'restore dataset unsupported type={dataset_type} for \'\n            f\'file={path_to_file}\')\n        return\n\n    if not output_redis_db:\n        output_redis_db = redis_db\n\n    restore_dataset.restore_dataset(\n        show_summary=show_summary,\n        path_to_file=path_to_file,\n        compress=compress,\n        encoding=encoding,\n        dataset_type=dataset_type,\n        serialize_datasets=serialize_datasets,\n        redis_address=redis_address,\n        redis_password=redis_password,\n        redis_db=redis_db,\n        redis_output_db=output_redis_db,\n        verbose=False)\n    log.info(f\'restore done - dataset in file={path_to_file}\')\n# end of restore_missing_dataset_values_from_algo_ready_file\n\n\ndef examine_dataset_in_file(\n        path_to_file,\n        compress=False,\n        encoding=\'utf-8\',\n        ticker=None,\n        dataset_type=ae_consts.SA_DATASET_TYPE_ALGO_READY,\n        serialize_datasets=ae_consts.DEFAULT_SERIALIZED_DATASETS,):\n    """"""examine_dataset_in_file\n\n    Show the internal dataset dictionary structure in dataset file\n\n    :param path_to_file: path to file\n    :param compress: optional - boolean flag for decompressing\n        the contents of the ``path_to_file`` if necessary\n        (default is ``False`` and algorithms\n        use ``zlib`` for compression)\n    :param encoding: optional - string for data encoding\n    :param ticker: optional - string ticker symbol\n        to verify is in the dataset\n    :param dataset_type: optional - dataset type\n        (default is ``SA_DATASET_TYPE_ALGO_READY``)\n    :param serialize_datasets: optional - list of dataset names to\n        deserialize in the dataset\n    """"""\n    if dataset_type == ae_consts.SA_DATASET_TYPE_ALGO_READY:\n        log.info(f\'show start - load dataset from file={path_to_file}\')\n        show_dataset.show_dataset(\n            path_to_file=path_to_file,\n            compress=compress,\n            encoding=encoding,\n            dataset_type=dataset_type,\n            serialize_datasets=serialize_datasets)\n        log.info(f\'show done - dataset in file={path_to_file}\')\n    elif dataset_type == ae_consts.SA_DATASET_TYPE_TRADING_HISTORY:\n        log.info(\n            \'load trading history dataset \'\n            f\'from file={path_to_file}\')\n        trading_history_dict = load_history.load_history_dataset_from_file(\n            path_to_file=path_to_file,\n            compress=compress,\n            encoding=encoding)\n        history_df = trading_history_dict[ticker]\n\n        first_date = history_df[\'date\'].iloc[0]\n        end_date = history_df[\'date\'].iloc[-1]\n        title = (\n            f\'Trading History {ticker} for Algo \'\n            f\'{trading_history_dict[""algo_name""]}\\n\'\n            f\'Backtest dates from {first_date} to {end_date}\')\n        xcol = \'date\'\n        xlabel = f\'Dates vs {trading_history_dict[""algo_name""]} values\'\n        ylabel = (\n            f\'Algo Values from columns:\\n{list(history_df.columns.values)}\')\n        df_filter = (history_df[\'close\'] > 0.01)\n\n        # set default hloc columns:\n        red = \'close\'\n        blue = \'low\'\n        green = \'high\'\n        orange = \'open\'\n\n        log.info(\n            \'available columns to plot in dataset: \'\n            f\'{ae_consts.ppj(list(history_df.columns.values))}\')\n\n        plot_trading_history.plot_trading_history(\n            title=title,\n            df=history_df,\n            red=red,\n            blue=blue,\n            green=green,\n            orange=orange,\n            date_col=xcol,\n            xlabel=xlabel,\n            ylabel=ylabel,\n            df_filter=df_filter,\n            show_plot=True,\n            dropna_for_all=False)\n    elif dataset_type == ae_consts.SA_DATASET_TYPE_TRADING_REPORT:\n        log.info(\n            \'load trading performance report dataset \'\n            f\'from file={path_to_file}\')\n        trading_report_dict = load_report.load_report_dataset_from_file(\n            path_to_file=path_to_file,\n            compress=compress,\n            encoding=encoding)\n        print(trading_report_dict)\n    else:\n        log.error(\n            f\'show unsupported dataset type={dataset_type} for \'\n            f\'file={path_to_file}\')\n        return\n# end of examine_dataset_in_file\n\n\ndef run_aws_backup():\n    """"""run_aws_backup\n\n    Run buy and sell analysis on a stock to send alerts to subscribed\n    users\n\n    """"""\n\n    log.debug(\'start - aws-backup\')\n\n    parser = argparse.ArgumentParser(\n        description=(\n            \'stock analysis tool\'))\n    parser.add_argument(\n        \'-t\',\n        help=(\n            \'ticker\'),\n        required=True,\n        dest=\'ticker\')\n    parser.add_argument(\n        \'-e\',\n        help=(\n            \'file path to extract an \'\n            \'algorithm-ready datasets from redis\'),\n        required=False,\n        dest=\'algo_extract_loc\')\n    parser.add_argument(\n        \'-l\',\n        help=(\n            \'show dataset in this file\'),\n        required=False,\n        dest=\'show_from_file\')\n    parser.add_argument(\n        \'-H\',\n        help=(\n            \'show trading history dataset in this file\'),\n        required=False,\n        dest=\'show_history_from_file\')\n    parser.add_argument(\n        \'-E\',\n        help=(\n            \'show trading performance report dataset in this file\'),\n        required=False,\n        dest=\'show_report_from_file\')\n    parser.add_argument(\n        \'-L\',\n        help=(\n            \'restore an algorithm-ready dataset file back into redis\'),\n        required=False,\n        dest=\'restore_algo_file\')\n    parser.add_argument(\n        \'-f\',\n        help=(\n            \'run in mode: prepare dataset from \'\n            \'redis key or s3 key\'),\n        required=False,\n        dest=\'prepare_mode\',\n        action=\'store_true\')\n    parser.add_argument(\n        \'-J\',\n        help=(\n            \'plot action - after preparing you can use: \'\n            \'-J show to open the image (good for debugging)\'),\n        required=False,\n        dest=\'plot_action\')\n    parser.add_argument(\n        \'-b\',\n        help=(\n            \'run a backtest using the dataset in \'\n            \'a file path/s3 key/redis key formats: \'\n            \'file:/opt/sa/tests/datasets/algo/SPY-latest.json or \'\n            \'s3://algoready/SPY-latest.json or \'\n            \'redis:SPY-latest\'),\n        required=False,\n        dest=\'backtest_loc\')\n    parser.add_argument(\n        \'-B\',\n        help=(\n            \'optional - broker url for Celery\'),\n        required=False,\n        dest=\'broker_url\')\n    parser.add_argument(\n        \'-C\',\n        help=(\n            \'optional - broker url for Celery\'),\n        required=False,\n        dest=\'backend_url\')\n    parser.add_argument(\n        \'-w\',\n        help=(\n            \'optional - flag for publishing an algorithm job \'\n            \'using Celery to the ae workers\'),\n        required=False,\n        dest=\'run_on_engine\',\n        action=\'store_true\')\n    parser.add_argument(\n        \'-k\',\n        help=(\n            \'optional - s3 access key\'),\n        required=False,\n        dest=\'s3_access_key\')\n    parser.add_argument(\n        \'-K\',\n        help=(\n            \'optional - s3 secret key\'),\n        required=False,\n        dest=\'s3_secret_key\')\n    parser.add_argument(\n        \'-a\',\n        help=(\n            \'optional - s3 address format: <host:port>\'),\n        required=False,\n        dest=\'s3_address\')\n    parser.add_argument(\n        \'-Z\',\n        help=(\n            \'optional - s3 secure: default False\'),\n        required=False,\n        dest=\'s3_secure\')\n    parser.add_argument(\n        \'-s\',\n        help=(\n            \'optional - start date: YYYY-MM-DD\'),\n        required=False,\n        dest=\'start_date\')\n    parser.add_argument(\n        \'-n\',\n        help=(\n            \'optional - end date: YYYY-MM-DD\'),\n        required=False,\n        dest=\'end_date\')\n    parser.add_argument(\n        \'-u\',\n        help=(\n            \'optional - s3 bucket name\'),\n        required=False,\n        dest=\'s3_bucket_name\')\n    parser.add_argument(\n        \'-G\',\n        help=(\n            \'optional - s3 region name\'),\n        required=False,\n        dest=\'s3_region_name\')\n    parser.add_argument(\n        \'-g\',\n        help=(\n            \'Path to a custom algorithm module file \'\n            \'on disk. This module must have a single \'\n            \'class that inherits from: \'\n            \'https://github.com/AlgoTraders/stock-analysis-engine/\'\n            \'blob/master/\'\n            \'analysis_engine/algo.py Additionally you \'\n            \'can find the Example-Minute-Algorithm here: \'\n            \'https://github.com/AlgoTraders/stock-analysis-engine/\'\n            \'blob/master/analysis_engine/mocks/\'\n            \'example_algo_minute.py\'),\n        required=False,\n        dest=\'run_algo_in_file\')\n    parser.add_argument(\n        \'-p\',\n        help=(\n            \'optional - s3 bucket/file for trading history\'),\n        required=False,\n        dest=\'algo_history_loc\')\n    parser.add_argument(\n        \'-o\',\n        help=(\n            \'optional - s3 bucket/file for trading performance report\'),\n        required=False,\n        dest=\'algo_report_loc\')\n    parser.add_argument(\n        \'-r\',\n        help=(\n            \'optional - redis_address format: <host:port>\'),\n        required=False,\n        dest=\'redis_address\')\n    parser.add_argument(\n        \'-R\',\n        help=(\n            \'optional - redis and s3 key name\'),\n        required=False,\n        dest=\'keyname\')\n    parser.add_argument(\n        \'-m\',\n        help=(\n            \'optional - redis database number (0 by default)\'),\n        required=False,\n        dest=\'redis_db\')\n    parser.add_argument(\n        \'-x\',\n        help=(\n            \'optional - redis expiration in seconds\'),\n        required=False,\n        dest=\'redis_expire\')\n    parser.add_argument(\n        \'-z\',\n        help=(\n            \'optional - strike price\'),\n        required=False,\n        dest=\'strike\')\n    parser.add_argument(\n        \'-c\',\n        help=(\n            \'optional - algorithm config_file path for setting \'\n            \'up internal algorithm trading strategies and \'\n            \'indicators\'),\n        required=False,\n        dest=\'config_file\')\n    parser.add_argument(\n        \'-P\',\n        help=(\n            \'optional - get pricing data if ""1"" or ""0"" disabled\'),\n        required=False,\n        dest=\'get_pricing\')\n    parser.add_argument(\n        \'-N\',\n        help=(\n            \'optional - get news data if ""1"" or ""0"" disabled\'),\n        required=False,\n        dest=\'get_news\')\n    parser.add_argument(\n        \'-O\',\n        help=(\n            \'optional - get options data if ""1"" or ""0"" disabled\'),\n        required=False,\n        dest=\'get_options\')\n    parser.add_argument(\n        \'-i\',\n        help=(\n            \'optional - ignore column names (comma separated)\'),\n        required=False,\n        dest=\'ignore_columns\')\n    parser.add_argument(\n        \'-d\',\n        help=(\n            \'debug\'),\n        required=False,\n        dest=\'debug\',\n        action=\'store_true\')\n    args = parser.parse_args()\n\n    mode = \'prepare\'\n    plot_action = ae_consts.PLOT_ACTION_SHOW\n    ticker = ae_consts.TICKER\n    ticker_id = ae_consts.TICKER_ID\n    ssl_options = ae_consts.SSL_OPTIONS\n    transport_options = ae_consts.TRANSPORT_OPTIONS\n    broker_url = ae_consts.WORKER_BROKER_URL\n    backend_url = ae_consts.WORKER_BACKEND_URL\n    path_to_config_module = ae_consts.WORKER_CELERY_CONFIG_MODULE\n    include_tasks = ae_consts.INCLUDE_TASKS\n    s3_access_key = ae_consts.S3_ACCESS_KEY\n    s3_secret_key = ae_consts.S3_SECRET_KEY\n    s3_region_name = ae_consts.S3_REGION_NAME\n    s3_address = ae_consts.S3_ADDRESS\n    s3_secure = ae_consts.S3_SECURE\n    s3_bucket_name = ae_consts.S3_BUCKET\n    s3_key = ae_consts.S3_KEY\n    redis_address = ae_consts.REDIS_ADDRESS\n    redis_key = ae_consts.REDIS_KEY\n    redis_password = ae_consts.REDIS_PASSWORD\n    redis_db = ae_consts.REDIS_DB\n    redis_expire = ae_consts.REDIS_EXPIRE\n    dataset_type = ae_consts.SA_DATASET_TYPE_ALGO_READY\n    serialize_datasets = ae_consts.DEFAULT_SERIALIZED_DATASETS\n    output_redis_key = None\n    output_s3_bucket = None\n    output_s3_key = None\n    ignore_columns = None\n    compress = False\n    encoding = \'utf-8\'\n    slack_enabled = False\n    slack_code_block = False\n    slack_full_width = False\n    verbose = False\n    debug = False\n\n    redis_serializer = \'json\'\n    redis_encoding = \'utf-8\'\n    output_redis_key = None\n    output_s3_bucket = None\n    output_s3_key = None\n    s3_enabled = True\n    redis_enabled = True\n    ignore_columns = None\n    debug = False\n\n    run_on_engine = False\n    show_from_file = None\n    show_history_from_file = None\n    show_report_from_file = None\n    restore_algo_file = None\n    backtest_loc = None\n    use_custom_algo = False\n    algo_history_loc = \'s3://algohistory\'\n    algo_report_loc = \'s3://algoreport\'\n    algo_extract_loc = \'s3://algoready\'\n\n    use_balance = 5000.0\n    use_commission = 6.0\n    auto_fill = True\n    use_start_date = \'2018-11-01 00:00:00\'\n    use_end_date = None\n    use_config_file = None\n    use_name = \'myalgo\'\n\n    if args.ticker:\n        ticker = args.ticker.upper()\n    if args.broker_url:\n        broker_url = args.broker_url\n    if args.backend_url:\n        backend_url = args.backend_url\n    if args.s3_access_key:\n        s3_access_key = args.s3_access_key\n    if args.s3_secret_key:\n        s3_secret_key = args.s3_secret_key\n    if args.s3_region_name:\n        s3_region_name = args.s3_region_name\n    if args.s3_address:\n        s3_address = args.s3_address\n        s3_enabled = True\n    if args.s3_secure:\n        s3_secure = args.s3_secure\n    if args.s3_bucket_name:\n        s3_bucket_name = args.s3_bucket_name\n    if args.keyname:\n        s3_key = args.keyname\n        redis_key = args.keyname\n    if args.redis_address:\n        redis_address = args.redis_address\n    if args.redis_db:\n        redis_db = args.redis_db\n    if args.redis_expire:\n        redis_expire = args.redis_expire\n    if args.prepare_mode:\n        mode = ae_consts.SA_MODE_PREPARE\n    if args.ignore_columns:\n        ignore_columns_org = args.ignore_columns\n        ignore_columns = ignore_columns_org.split("","")\n    if args.plot_action:\n        if str(args.plot_action).lower() == \'show\':\n            plot_action = ae_consts.PLOT_ACTION_SHOW\n        elif str(args.plot_action).lower() == \'s3\':\n            plot_action = ae_consts.PLOT_ACTION_SAVE_TO_S3\n        elif str(args.plot_action).lower() == \'save\':\n            plot_action = ae_consts.PLOT_ACTION_SAVE_AS_FILE\n        else:\n            plot_action = ae_consts.PLOT_ACTION_SHOW\n            log.warning(f\'unsupported plot_action: {args.plot_action}\')\n\n    if args.debug:\n        debug = True\n\n    if args.algo_extract_loc:\n        mode = ae_consts.SA_MODE_EXTRACT\n    if args.show_from_file:\n        show_from_file = args.show_from_file\n        mode = ae_consts.SA_MODE_SHOW_DATASET\n    if args.show_history_from_file:\n        show_history_from_file = args.show_history_from_file\n        mode = ae_consts.SA_MODE_SHOW_HISTORY_DATASET\n    if args.show_report_from_file:\n        show_report_from_file = args.show_report_from_file\n        mode = ae_consts.SA_MODE_SHOW_REPORT_DATASET\n    if args.restore_algo_file:\n        restore_algo_file = args.restore_algo_file\n        mode = ae_consts.SA_MODE_RESTORE_REDIS_DATASET\n    if args.run_algo_in_file:\n        mode = ae_consts.SA_MODE_RUN_ALGO\n    if args.backtest_loc:\n        mode = ae_consts.SA_MODE_RUN_ALGO\n    if args.start_date:\n        try:\n            use_start_date = f\'{str(args.start_date)} 00:00:00\'\n            datetime.datetime.strptime(\n                args.start_date,\n                ae_consts.COMMON_DATE_FORMAT)\n        except Exception as e:\n            msg = (\n                \'please use a start date formatted as: \'\n                f\'{ae_consts.COMMON_DATE_FORMAT}\\n\'\n                f\'error was: {e}\')\n            log.error(msg)\n            sys.exit(1)\n        # end of testing for a valid date\n    # end of args.start_date\n    if args.end_date:\n        try:\n            use_end_date = f\'{str(args.end_date)} 00:00:00\'\n            datetime.datetime.strptime(\n                args.end_date,\n                ae_consts.COMMON_DATE_FORMAT)\n        except Exception as e:\n            msg = (\n                \'please use an end date formatted as: \'\n                f\'{ae_consts.COMMON_DATE_FORMAT}\\n\'\n                f\'error was: {e}\')\n            log.error(msg)\n            sys.exit(1)\n        # end of testing for a valid date\n    # end of args.end_date\n    if args.config_file:\n        use_config_file = args.config_file\n        if not os.path.exists(use_config_file):\n            log.error(\n                f\'Failed: unable to find config file: -c {use_config_file}\')\n            sys.exit(1)\n\n    config_dict = None\n    load_from_s3_bucket = None\n    load_from_s3_key = None\n    load_from_redis_key = None\n    load_from_file = None\n    load_compress = False\n    load_publish = True\n    load_config = None\n    report_redis_key = None\n    report_s3_bucket = None\n    report_s3_key = None\n    report_file = None\n    report_compress = False\n    report_publish = False\n    report_config = None\n    history_redis_key = None\n    history_s3_bucket = None\n    history_s3_key = None\n    history_file = None\n    history_compress = False\n    history_publish = False\n    history_config = None\n    extract_redis_key = None\n    extract_s3_bucket = None\n    extract_s3_key = None\n    extract_file = None\n    extract_save_dir = None\n    extract_compress = False\n    extract_publish = True\n    extract_config = None\n    publish_to_slack = False\n    publish_to_s3 = True\n    publish_to_redis = False\n    use_timeseries = \'day\'\n    use_trade_strategy = \'count\'\n\n    valid = False\n    required_task = False\n    work = None\n    task_name = None\n    work = {}\n    path_to_tasks = \'analysis_engine.work_tasks\'\n    if mode == ae_consts.SA_MODE_PREPARE:\n        task_name = (\n            f\'{path_to_tasks}.\'\n            \'prepare_pricing_dataset.prepare_pricing_dataset\')\n        work = api_requests.build_prepare_dataset_request()\n        if output_s3_key:\n            work[\'prepared_s3_key\'] = output_s3_key\n        if output_s3_bucket:\n            work[\'prepared_s3_bucket\'] = output_s3_bucket\n        if output_redis_key:\n            work[\'prepared_redis_key\'] = output_redis_key\n        work[\'ignore_columns\'] = ignore_columns\n        valid = True\n        required_task = True\n    elif mode == ae_consts.SA_MODE_EXTRACT:\n        if args.algo_extract_loc:\n            algo_extract_loc = args.algo_extract_loc\n            if (\'file:/\' not in algo_extract_loc and\n                    \'s3://\' not in algo_extract_loc and\n                    \'redis://\' not in algo_extract_loc):\n                log.error(\n                    \'invalid -e <extract_to_file_or_s3_key_or_redis_key> \'\n                    \'specified. please use either: \'\n                    \'-e file:/opt/sa/tests/datasets/algo/SPY-latest.json or \'\n                    \'-e s3://algoready/SPY-latest.json or \'\n                    \'-e redis://SPY-latest\')\n                sys.exit(1)\n            if \'s3://\' in algo_extract_loc:\n                extract_s3_bucket = algo_extract_loc.split(\'/\')[-2]\n                extract_s3_key = algo_extract_loc.split(\'/\')[-1]\n            elif \'redis://\' in algo_extract_loc:\n                extract_redis_key = algo_extract_loc.split(\'/\')[-1]\n            elif \'file:/\' in algo_extract_loc:\n                extract_file = algo_extract_loc.split(\':\')[-1]\n        # end of parsing supported transport for loading\n\n        use_custom_algo = True\n    elif mode == ae_consts.SA_MODE_SHOW_DATASET:\n        examine_dataset_in_file(\n            ticker=ticker,\n            path_to_file=show_from_file)\n        log.info(f\'done showing {ticker} dataset from file={show_from_file}\')\n        sys.exit(0)\n    elif mode == ae_consts.SA_MODE_SHOW_HISTORY_DATASET:\n        examine_dataset_in_file(\n            ticker=ticker,\n            dataset_type=ae_consts.SA_DATASET_TYPE_TRADING_HISTORY,\n            path_to_file=show_history_from_file)\n        log.info(\n            f\'done showing trading history {ticker} dataset from \'\n            f\'file={show_from_file}\')\n        sys.exit(0)\n    elif mode == ae_consts.SA_MODE_SHOW_REPORT_DATASET:\n        examine_dataset_in_file(\n            ticker=ticker,\n            dataset_type=ae_consts.SA_DATASET_TYPE_TRADING_REPORT,\n            path_to_file=show_report_from_file)\n        log.info(\n            f\'done showing trading performance report {ticker} dataset from \'\n            f\'file={show_from_file}\')\n        sys.exit(0)\n    elif mode == ae_consts.SA_MODE_RESTORE_REDIS_DATASET:\n        restore_missing_dataset_values_from_algo_ready_file(\n            ticker=ticker,\n            path_to_file=restore_algo_file,\n            redis_address=redis_address,\n            redis_password=redis_password,\n            redis_db=redis_db,\n            output_redis_db=redis_db,\n            dataset_type=ae_consts.SA_DATASET_TYPE_ALGO_READY,\n            serialize_datasets=ae_consts.DEFAULT_SERIALIZED_DATASETS)\n        log.info(\n            f\'done restoring {ticker} dataset from file={restore_algo_file} \'\n            f\'into redis_db={redis_db}\')\n        sys.exit(0)\n    elif mode == ae_consts.SA_MODE_RUN_ALGO:\n        if args.run_algo_in_file:\n            if not os.path.exists(args.run_algo_in_file):\n                log.error(\n                    f\'missing algorithm module file: {args.run_algo_in_file}\')\n                sys.exit(1)\n\n        if args.backtest_loc:\n            backtest_loc = args.backtest_loc\n            if (\'file:/\' not in backtest_loc and\n                    \'s3://\' not in backtest_loc and\n                    \'redis://\' not in backtest_loc):\n                log.error(\n                    \'invalid -b <backtest dataset file> specified. \'\n                    f\'{backtest_loc} \'\n                    \'please use either: \'\n                    \'-b file:/opt/sa/tests/datasets/algo/SPY-latest.json or \'\n                    \'-b s3://algoready/SPY-latest.json or \'\n                    \'-b redis://SPY-latest\')\n                sys.exit(1)\n            if \'s3://\' in backtest_loc:\n                load_from_s3_bucket = backtest_loc.split(\'/\')[-2]\n                load_from_s3_key = backtest_loc.split(\'/\')[-1]\n            elif \'redis://\' in backtest_loc:\n                load_from_redis_key = backtest_loc.split(\'/\')[-1]\n            elif \'file:/\' in backtest_loc:\n                load_from_file = backtest_loc.split(\':\')[-1]\n            load_publish = True\n        # end of parsing supported transport - loading an algo-ready\n\n        if args.algo_history_loc:\n            algo_history_loc = args.algo_history_loc\n            if (\'file:/\' not in algo_history_loc and\n                    \'s3://\' not in algo_history_loc and\n                    \'redis://\' not in algo_history_loc):\n                log.error(\n                    \'invalid -p <backtest dataset file> specified. \'\n                    f\'{algo_history_loc} \'\n                    \'please use either: \'\n                    \'-p file:/opt/sa/tests/datasets/algo/SPY-latest.json or \'\n                    \'-p s3://algoready/SPY-latest.json or \'\n                    \'-p redis://SPY-latest\')\n                sys.exit(1)\n            if \'s3://\' in algo_history_loc:\n                history_s3_bucket = algo_history_loc.split(\'/\')[-2]\n                history_s3_key = algo_history_loc.split(\'/\')[-1]\n            elif \'redis://\' in algo_history_loc:\n                history_redis_key = algo_history_loc.split(\'/\')[-1]\n            elif \'file:/\' in algo_history_loc:\n                history_file = algo_history_loc.split(\':\')[-1]\n            history_publish = True\n        # end of parsing supported transport - trading history\n\n        if args.algo_report_loc:\n            algo_report_loc = args.algo_report_loc\n            if (\'file:/\' not in algo_report_loc and\n                    \'s3://\' not in algo_report_loc and\n                    \'redis://\' not in algo_report_loc):\n                log.error(\n                    \'invalid -o <backtest dataset file> specified. \'\n                    f\'{algo_report_loc} \'\n                    \'please use either: \'\n                    \'-o file:/opt/sa/tests/datasets/algo/SPY-latest.json or \'\n                    \'-o s3://algoready/SPY-latest.json or \'\n                    \'-o redis://SPY-latest\')\n                sys.exit(1)\n            if \'s3://\' in algo_report_loc:\n                report_s3_bucket = algo_report_loc.split(\'/\')[-2]\n                report_s3_key = algo_report_loc.split(\'/\')[-1]\n            elif \'redis://\' in algo_report_loc:\n                report_redis_key = algo_report_loc.split(\'/\')[-1]\n            elif \'file:/\' in algo_report_loc:\n                report_file = algo_report_loc.split(\':\')[-1]\n            report_publish = True\n        # end of parsing supported transport - trading performance report\n\n        if args.algo_extract_loc:\n            algo_extract_loc = args.algo_extract_loc\n            if (\'file:/\' not in algo_extract_loc and\n                    \'s3://\' not in algo_extract_loc and\n                    \'redis://\' not in algo_extract_loc):\n                log.error(\n                    \'invalid -e <backtest dataset file> specified. \'\n                    f\'{algo_extract_loc} \'\n                    \'please use either: \'\n                    \'-e file:/opt/sa/tests/datasets/algo/SPY-latest.json or \'\n                    \'-e s3://algoready/SPY-latest.json or \'\n                    \'-e redis://SPY-latest\')\n                sys.exit(1)\n            if \'s3://\' in algo_extract_loc:\n                extract_s3_bucket = algo_extract_loc.split(\'/\')[-2]\n                extract_s3_key = algo_extract_loc.split(\'/\')[-1]\n            elif \'redis://\' in algo_extract_loc:\n                extract_redis_key = algo_extract_loc.split(\'/\')[-1]\n            elif \'file:/\' in algo_extract_loc:\n                extract_file = algo_extract_loc.split(\':\')[-1]\n            extract_publish = True\n        # end of parsing supported transport - extract algorithm-ready\n\n        use_custom_algo = True\n    # end of set up for backtest\n\n    if use_custom_algo:\n\n        if args.run_on_engine:\n            run_on_engine = True\n            log.info(\'starting algo on the engine\')\n        else:\n            log.info(\'starting algo\')\n\n        algo_res = run_custom_algo.run_custom_algo(\n            mod_path=args.run_algo_in_file,\n            ticker=ticker,\n            balance=use_balance,\n            commission=use_commission,\n            start_date=use_start_date,\n            end_date=use_end_date,\n            config_file=use_config_file,\n            name=use_name,\n            auto_fill=auto_fill,\n            config_dict=config_dict,\n            load_from_s3_bucket=load_from_s3_bucket,\n            load_from_s3_key=load_from_s3_key,\n            load_from_redis_key=load_from_redis_key,\n            load_from_file=load_from_file,\n            load_compress=load_compress,\n            load_publish=load_publish,\n            load_config=load_config,\n            report_redis_key=report_redis_key,\n            report_s3_bucket=report_s3_bucket,\n            report_s3_key=report_s3_key,\n            report_file=report_file,\n            report_compress=report_compress,\n            report_publish=report_publish,\n            report_config=report_config,\n            history_redis_key=history_redis_key,\n            history_s3_bucket=history_s3_bucket,\n            history_s3_key=history_s3_key,\n            history_file=history_file,\n            history_compress=history_compress,\n            history_publish=history_publish,\n            history_config=history_config,\n            extract_redis_key=extract_redis_key,\n            extract_s3_bucket=extract_s3_bucket,\n            extract_s3_key=extract_s3_key,\n            extract_file=extract_file,\n            extract_save_dir=extract_save_dir,\n            extract_compress=extract_compress,\n            extract_publish=extract_publish,\n            extract_config=extract_config,\n            publish_to_slack=publish_to_slack,\n            publish_to_s3=publish_to_s3,\n            publish_to_redis=publish_to_redis,\n            dataset_type=dataset_type,\n            serialize_datasets=serialize_datasets,\n            compress=compress,\n            encoding=encoding,\n            redis_enabled=redis_enabled,\n            redis_key=redis_key,\n            redis_address=redis_address,\n            redis_db=redis_db,\n            redis_password=redis_password,\n            redis_expire=redis_expire,\n            redis_serializer=redis_serializer,\n            redis_encoding=redis_encoding,\n            s3_enabled=s3_enabled,\n            s3_key=s3_key,\n            s3_address=s3_address,\n            s3_bucket=s3_bucket_name,\n            s3_access_key=s3_access_key,\n            s3_secret_key=s3_secret_key,\n            s3_region_name=s3_region_name,\n            s3_secure=s3_secure,\n            slack_enabled=slack_enabled,\n            slack_code_block=slack_code_block,\n            slack_full_width=slack_full_width,\n            dataset_publish_extract=extract_publish,\n            dataset_publish_history=history_publish,\n            dataset_publish_report=report_publish,\n            run_on_engine=run_on_engine,\n            auth_url=broker_url,\n            backend_url=backend_url,\n            include_tasks=include_tasks,\n            ssl_options=ssl_options,\n            transport_options=transport_options,\n            path_to_config_module=path_to_config_module,\n            timeseries=use_timeseries,\n            trade_strategy=use_trade_strategy,\n            verbose=verbose)\n\n        show_label = f\'algo.name={use_name}\'\n        show_extract = f\'{algo_extract_loc}\'\n        show_history = f\'{algo_history_loc}\'\n        show_report = f\'{algo_report_loc}\'\n        base_label = (\n            f\'load={args.run_algo_in_file} extract={show_extract} \'\n            f\'history={show_history} report={show_report}\')\n        show_label = (\n            f\'{ticker} running in engine \'\n            f\'\'\'task_id={algo_res[""rec""].get(\n                ""task_id"",\n                ""missing-task-id"")} {base_label}\'\'\')\n        if not run_on_engine:\n            algo_trade_history_recs = algo_res[\'rec\'].get(\n                \'history\',\n                [])\n            show_label = (\n                f\'{ticker} algo.name={use_name} {base_label} \'\n                f\'trade_history_len={len(algo_trade_history_recs)}\')\n        if args.debug:\n            log.info(f\'algo_res={algo_res}\')\n            if algo_res[\'status\'] == ae_consts.SUCCESS:\n                log.info(\n                    f\'{ae_consts.get_status(status=algo_res[""status""])} - \'\n                    f\'done running {show_label}\')\n            else:\n                log.error(\n                    f\'{ae_consts.get_status(status=algo_res[""status""])} - \'\n                    f\'done running {show_label}\')\n        else:\n            if algo_res[\'status\'] == ae_consts.SUCCESS:\n                log.info(\n                    f\'{ae_consts.get_status(status=algo_res[""status""])} - \'\n                    f\'done running {show_label}\')\n            else:\n                log.error(f\'run_custom_algo returned error: {algo_res[""err""]}\')\n                sys.exit(1)\n        # end of running the custom algo handler\n\n        if mode == ae_consts.SA_MODE_EXTRACT:\n            log.info(f\'done extracting dataset - {ticker}\')\n        elif mode == ae_consts.SA_MODE_RUN_ALGO:\n            log.info(f\'done running algo - {ticker}\')\n\n        sys.exit(0)\n    # end of handling mode-specific arg assignments\n\n    # sanity checking the work and task are valid\n    if not valid:\n        log.error(\n            \'usage error: missing a supported mode: \'\n            \'-f (for prepare a dataset) \')\n        sys.exit(1)\n    if required_task and not task_name:\n        log.error(\n            \'usage error: missing a supported task_name\')\n        sys.exit(1)\n    # end of sanity checks\n\n    work[\'ticker\'] = ticker\n    work[\'ticker_id\'] = ticker_id\n    work[\'s3_bucket\'] = s3_bucket_name\n    work[\'s3_key\'] = s3_key\n    work[\'redis_key\'] = redis_key\n    work[\'s3_access_key\'] = s3_access_key\n    work[\'s3_secret_key\'] = s3_secret_key\n    work[\'s3_region_name\'] = s3_region_name\n    work[\'s3_address\'] = s3_address\n    work[\'s3_secure\'] = s3_secure\n    work[\'redis_address\'] = redis_address\n    work[\'redis_password\'] = redis_password\n    work[\'redis_db\'] = redis_db\n    work[\'redis_expire\'] = redis_expire\n    work[\'s3_enabled\'] = s3_enabled\n    work[\'redis_enabled\'] = redis_enabled\n    work[\'debug\'] = debug\n    work[\'label\'] = f\'ticker={ticker}\'\n\n    task_res = None\n    if ae_consts.is_celery_disabled():\n        work[\'celery_disabled\'] = True\n        log.debug(f\'starting without celery work={ae_consts.ppj(work)}\')\n        if mode == ae_consts.SA_MODE_PREPARE:\n            task_res = prep_dataset.prepare_pricing_dataset(\n                work)\n\n        if debug:\n            log.info(\n                f\'done - result={ae_consts.ppj(task_res)} task={task_name} \'\n                f\'status={ae_consts.get_status(status=task_res[""status""])} \'\n                f\'err={task_res[""err""]} label={work[""label""]}\')\n        else:\n            log.info(\n                f\'done - result task={task_name} \'\n                f\'status={ae_consts.get_status(status=task_res[""status""])} \'\n                f\'err={task_res[""err""]} label={work[""label""]}\')\n\n        if task_res[\'status\'] == ae_consts.SUCCESS:\n            image_res = None\n            label = work[\'label\']\n            ticker = work[\'ticker\']\n            if plot_action == ae_consts.PLOT_ACTION_SHOW:\n                log.info(\n                    \'showing plot\')\n                """"""\n                minute_key = f\'{redis_key}_minute\'\n                minute_df_res = build_df.build_df_from_redis(\n                    label=label\',\n                    address=redis_address,\n                    db=redis_db,\n                    key=minute_key)\n\n                minute_df = None\n                if (\n                        minute_df_res[\'status\'] == SUCCESS\n                        and minute_df_res[\'rec\'][\'valid_df\']):\n                    minute_df = minute_df_res[\'rec\'][\'data\']\n                    print(minute_df.columns.values)\n                    column_list = [\n                        \'close\',\n                        \'date\'\n                    ]\n                """"""\n                today_str = datetime.datetime.now().strftime(\n                    \'%Y-%m-%d\')\n                extract_req = work\n                extract_req[\'redis_key\'] = f\'{work[""redis_key""]}_minute\'\n                extract_status, minute_df = \\\n                    extract_utils.extract_minute_dataset(\n                        work_dict=work)\n                if extract_status == ae_consts.SUCCESS:\n                    log.info(\n                        f\'{label} - ticker={ticker} \'\n                        f\'creating chart date={today_str}\')\n                    """"""\n                    Plot Pricing with the Volume Overlay:\n                    """"""\n                    image_res = ae_charts.plot_overlay_pricing_and_volume(\n                        log_label=label,\n                        ticker=ticker,\n                        date_format=ae_consts.IEX_MINUTE_DATE_FORMAT,\n                        df=minute_df,\n                        show_plot=True)\n\n                    """"""\n                    Plot the High-Low-Open-Close Pricing:\n                    """"""\n                    """"""\n                    image_res = ae_charts.plot_hloc_pricing(\n                        log_label=label,\n                        ticker=ticker,\n                        title=f\'{ticker} - Minute Pricing - {today_str}\',\n                        df=minute_df,\n                        show_plot=True)\n                    """"""\n\n                    """"""\n                    Plot by custom columns in the DataFrame\n                    """"""\n                    """"""\n                    column_list = minute_df.columns.values\n                    column_list = [\n                        \'date\',\n                        \'close\',\n                        \'high\',\n                        \'low\',\n                        \'open\'\n                    ]\n                    image_res = ae_charts.plot_df(\n                        log_label=label,\n                        title=\'Pricing Title\',\n                        column_list=column_list,\n                        df=minute_df,\n                        xcol=\'date\',\n                        xlabel=\'Date\',\n                        ylabel=\'Pricing\',\n                        show_plot=True)\n                    """"""\n            elif plot_action == ae_consts.PLOT_ACTION_SAVE_TO_S3:\n                log.info(\n                    \'coming soon - support to save to s3\')\n            elif plot_action == ae_consts.PLOT_ACTION_SAVE_AS_FILE:\n                log.info(\n                    \'coming soon - support to save as file\')\n            if image_res:\n                log.info(\n                    f\'{label} show plot - \'\n                    f\'status={ae_consts.get_status(image_res[""status""])} \'\n                    f\'err={image_res[""err""]}\')\n    else:\n        log.info(\n            f\'connecting to broker={broker_url} backend={backend_url}\')\n\n        # Get the Celery app\n        app = get_celery_app.get_celery_app(\n            name=__name__,\n            auth_url=broker_url,\n            backend_url=backend_url,\n            path_to_config_module=path_to_config_module,\n            ssl_options=ssl_options,\n            transport_options=transport_options,\n            include_tasks=include_tasks)\n\n        log.info(f\'calling task={task_name} - work={ae_consts.ppj(work)}\')\n        job_id = app.send_task(\n            task_name,\n            (work,))\n        log.info(f\'calling task={task_name} - success job_id={job_id}\')\n    # end of if/else\n\n# end of run_aws_backup\n\n\nif __name__ == \'__main__\':\n    run_aws_backup()\n'"
analysis_engine/scripts/backtest_with_runner.py,0,"b'#!/usr/bin/env python\n\n""""""\nAlgorithm Runner API Example Script\n\n**Run Full Backtest**\n\n::\n\n    backtest_with_runner.py -t TICKER -b S3_BUCKET -k S3_KEY -c ALGO_CONFIG\n\n**Run Algorithm with Latest Pricing Data**\n\n::\n\n    backtest_with_runner.py -l -t TICKER -b S3_BUCKET -k S3_KEY -c ALGO_CONFIG\n\nDebug by adding ``-d`` as an argument\n""""""\n\nimport sys\nimport argparse\nimport analysis_engine.utils as ae_utils\nimport analysis_engine.algo_runner as algo_runner\nimport analysis_engine.plot_trading_history as plot\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(\n    name=\'algo-runner\')\n\n\ndef backtest_with_runner():\n    """"""backtest_with_runner\n\n    build and publish a trading history from an algorithm config.\n\n    ::\n\n        backtest_with_runner.py -t TICKER -c ALGO_CONFIG -s START_DATE\n        -k S3_KEY -b S3_BUCKET -l\n    """"""\n\n    parser = argparse.ArgumentParser(\n        description=(\n            \'backtest an algorithm and publish \'\n            \'the trading history\'))\n    parser.add_argument(\n        \'-t\',\n        help=(\'ticker symbol\'),\n        required=False,\n        dest=\'ticker\')\n    parser.add_argument(\n        \'-k\',\n        help=(\'s3_key\'),\n        required=False,\n        dest=\'s3_key\')\n    parser.add_argument(\n        \'-b\',\n        help=(\'s3_bucket\'),\n        required=False,\n        dest=\'s3_bucket\')\n    parser.add_argument(\n        \'-s\',\n        help=(\'start date format YYYY-MM-DD\'),\n        required=False,\n        dest=\'start_date\')\n    parser.add_argument(\n        \'-c\',\n        help=(\'algo config file\'),\n        required=False,\n        dest=\'algo_config\')\n    parser.add_argument(\n        \'-l\',\n        help=(\n            \'run a backtest with the latest \'\n            \'pricing data\'),\n        required=False,\n        dest=\'latest\',\n        action=\'store_true\')\n    parser.add_argument(\n        \'-d\',\n        help=\'debug\',\n        required=False,\n        dest=\'debug\',\n        action=\'store_true\')\n    args = parser.parse_args()\n\n    ticker = \'SPY\'\n    s3_bucket = (\n        f\'algohistory\')\n    s3_key = (\n        f\'trade_history_{ticker}\')\n    start_date = (\n        f\'2019-01-01\')\n    algo_config = (\n        f\'/opt/sa/cfg/default_algo.json\')\n    latest = False\n    show_plot = True\n    debug = False\n\n    if args.ticker:\n        ticker = args.ticker.upper()\n    if args.s3_key:\n        s3_key = args.s3_key\n    if args.s3_bucket:\n        s3_bucket = args.s3_bucket\n    if args.start_date:\n        start_date = args.start_date\n    if args.algo_config:\n        algo_config = args.algo_config\n    if args.latest:\n        latest = True\n        start_date = ae_utils.get_last_close_str()\n    if args.debug:\n        debug = True\n\n    history_loc = (\n        f\'s3://{s3_bucket}/{s3_key}\')\n\n    log.info(\n        f\'building {ticker} trade history \'\n        f\'start_date={start_date} \'\n        f\'config={algo_config} \'\n        f\'history_loc={history_loc}\')\n\n    runner = algo_runner.AlgoRunner(\n        ticker=ticker,\n        start_date=start_date,\n        history_loc=history_loc,\n        algo_config=algo_config,\n        verbose_algo=debug,\n        verbose_processor=False,\n        verbose_indicators=False)\n\n    trading_history_df = None\n    if latest:\n        trading_history_df = runner.latest()\n        log.info(\n            f\'{ticker} latest:\')\n        print(trading_history_df[[\'minute\', \'close\']].tail(5))\n        log.info(\n            f\'Other available columns to plot:\')\n        print(trading_history_df.columns.values)\n        if show_plot:\n            plot.plot_trading_history(\n                title=(\n                    f\'{ticker} at \'\n                    f\'${trading_history_df[""close""].iloc[-1]} \'\n                    f\'at: \'\n                    f\'{trading_history_df[""minute""].iloc[-1]}\'),\n                df=trading_history_df,\n                red=\'high\',\n                blue=\'close\')\n    else:\n        runner.start()\n\n    sys.exit(0)\n# end of backtest_with_runner\n\n\nif __name__ == \'__main__\':\n    backtest_with_runner()\n'"
analysis_engine/scripts/fetch_new_stock_datasets.py,0,"b'#!/usr/bin/env python\n\n""""""\n\nFetch new pricing datasets for a one or many tickers at once or\npull screeners from IEX Cloud (https://iexcloud.io),\nTradier (https://tradier.com/) and FinViz (https://finviz.com/)\n\n1) Fetch pricing data\n2) Publish pricing data to Redis and Minio\n\n**Examples**\n\n**Fetch Intraday Minute Pricing Data**\n\n::\n\n    fetch -t QQQ -g min\n\n**Fetch Intraday Option Chains for Calls and Puts**\n\n::\n\n    fetch -t QQQ -g td\n\n**Fetch Intraday News, Minute and Options**\n\n::\n\n    fetch -t QQQ -g news,min,td\n\n**Debugging**\n\nTurn on verbose debugging with the ``-d`` argument:\n\n::\n\n    fetch -t QQQ -g min -d\n\n""""""\n\nimport os\nimport argparse\nimport celery\nimport analysis_engine.work_tasks.get_celery_app as get_celery_app\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.iex.consts as iex_consts\nimport analysis_engine.api_requests as api_requests\nimport analysis_engine.work_tasks.get_new_pricing_data as task_pricing\nimport analysis_engine.work_tasks.task_screener_analysis as screener_utils\nimport analysis_engine.utils as ae_utils\nimport spylunking.log.setup_logging as log_utils\n\n\n# Disable celery log hijacking\n# https://github.com/celery/celery/issues/2509\n@celery.signals.setup_logging.connect\ndef setup_celery_logging(**kwargs):\n    pass\n\n\nlog = log_utils.build_colorized_logger(\n    name=\'fetch\',\n    log_config_path=ae_consts.LOG_CONFIG_PATH)\n\n\ndef start_screener_analysis(\n        req):\n    """"""start_screener_analysis\n\n    Start screener-driven analysis with a simple workflow:\n\n    1) Convert FinViz screeners into a list of tickers\n       and a ``pandas.DataFrames`` from each ticker\'s html row\n    2) Build unique list of tickers\n    3) Pull datasets for each ticker\n    4) Run sale-side processing - coming soon\n    5) Run buy-side processing - coming soon\n    6) Issue alerts to slack - coming soon\n\n    :param req: dictionary to start the screener analysis\n    """"""\n    label = req.get(\n        \'label\',\n        \'screener\')\n    log.info(f\'{label} - start screener analysis\')\n    req[\'celery_disabled\'] = True\n    analysis_res = screener_utils.run_screener_analysis(\n        work_dict=req)\n    log.info(f\'{label} - done screener analysis result={analysis_res}\')\n# end of start_screener_analysis\n\n\ndef fetch_new_stock_datasets():\n    """"""fetch_new_stock_datasets\n\n    Collect datasets for a ticker from IEX Cloud or Tradier\n\n    .. warning: IEX Cloud charges per request. Here are example\n        commands to help you monitor your usage while handling\n        first time users and automation (intraday, daily, and weekly\n        options are supported).\n\n    **Setup**\n\n    ::\n\n        export IEX_TOKEN=YOUR_IEX_CLOUD_TOKEN\n        export TD_TOKEN=YOUR_TRADIER_TOKEN\n\n    **Pull Data for a Ticker from IEX and Tradier**\n\n    ::\n\n        fetch -t TICKER\n\n    **Pull from All Supported IEX Feeds**\n\n    ::\n\n        fetch -t TICKER -g iex-all\n\n    **Pull from All Supported Tradier Feeds**\n\n    ::\n\n        fetch -t TICKER -g td\n\n    **Intraday IEX and Tradier Feeds (only minute and news to reduce costs)**\n\n    ::\n\n        fetch -t TICKER -g intra\n        # or manually:\n        # fetch -t TICKER -g td,iex_min,iex_news\n\n    **Daily IEX Feeds (daily and news)**\n\n    ::\n\n        fetch -t TICKER -g daily\n        # or manually:\n        # fetch -t TICKER -g iex_day,iex_news\n\n    **Weekly IEX Feeds (company, financials, earnings, dividends, and peers)**\n\n    ::\n\n        fetch -t TICKER -g weekly\n        # or manually:\n        # fetch -t TICKER -g iex_fin,iex_earn,iex_div,iex_peers,iex_news,\n        # iex_comp\n\n    **IEX Minute**\n\n    ::\n\n        fetch -t TICKER -g iex_min\n\n    **IEX News**\n\n    ::\n\n        fetch -t TICKER -g iex_news\n\n    **IEX Daily**\n\n    ::\n\n        fetch -t TICKER -g iex_day\n\n    **IEX Stats**\n\n    ::\n\n        fetch -t TICKER -g iex_stats\n\n    **IEX Peers**\n\n    ::\n\n        fetch -t TICKER -g iex_peers\n\n    **IEX Financials**\n\n    ::\n\n        fetch -t TICKER -g iex_fin\n\n    **IEX Earnings**\n\n    ::\n\n        fetch -t TICKER -g iex_earn\n\n    **IEX Dividends**\n\n    ::\n\n        fetch -t TICKER -g iex_div\n\n    **IEX Quote**\n\n    ::\n\n        fetch -t TICKER -g iex_quote\n\n    **IEX Company**\n\n    ::\n\n        fetch -t TICKER -g iex_comp\n\n    .. note:: This requires the following services are listening on:\n\n        - redis ``localhost:6379``\n        - minio ``localhost:9000``\n\n    """"""\n    log.info(\n        \'start - fetch_new_stock_datasets\')\n\n    parser = argparse.ArgumentParser(\n        description=(\n            \'Download and store the latest stock pricing, \'\n            \'news, and options chain data \'\n            \'and store it in Minio (S3) and Redis. \'\n            \'Also includes support for getting FinViz \'\n            \'screener tickers\'))\n    parser.add_argument(\n        \'-t\',\n        help=(\n            \'ticker\'),\n        required=False,\n        dest=\'ticker\')\n    parser.add_argument(\n        \'-g\',\n        help=(\n            \'optional - fetch mode: \'\n            \'initial = default fetch from initial data feeds \'\n            \'(IEX and Tradier), \'\n            \'intra = fetch intraday from IEX and Tradier, \'\n            \'daily or day = fetch daily from IEX, \'\n            \'weekly = fetch weekly from IEX, \'\n            \'all = fetch from all data feeds, \'\n            \'td = fetch from Tradier feeds only, \'\n            \'iex = fetch from IEX Cloud feeds only, \'\n            \'min or minute or iex_min = fetch IEX Cloud intraday \'\n            \'per-minute feed \'\n            \'https://iexcloud.io/docs/api/#historical-prices, \'\n            \'day or daily or iex_day = fetch IEX Cloud daily feed \'\n            \'https://iexcloud.io/docs/api/#historical-prices, \'\n            \'quote or iex_quote = fetch IEX Cloud quotes feed \'\n            \'https://iexcloud.io/docs/api/#quote, \'\n            \'stats or iex_stats = fetch IEX Cloud key stats feed \'\n            \'https://iexcloud.io/docs/api/#key-stats, \'\n            \'peers or iex_peers = fetch from just IEX Cloud peers feed \'\n            \'https://iexcloud.io/docs/api/#peers, \'\n            \'news or iex_news = fetch IEX Cloud news feed \'\n            \'https://iexcloud.io/docs/api/#news, \'\n            \'fin or iex_fin = fetch IEX Cloud financials feed\'\n            \'https://iexcloud.io/docs/api/#financials, \'\n            \'earn or iex_earn = fetch from just IEX Cloud earnings feeed \'\n            \'https://iexcloud.io/docs/api/#earnings, \'\n            \'div or iex_div = fetch from just IEX Cloud dividends feed\'\n            \'https://iexcloud.io/docs/api/#dividends, \'\n            \'iex_comp = fetch from just IEX Cloud company feed \'\n            \'https://iexcloud.io/docs/api/#company\'),\n        required=False,\n        dest=\'fetch_mode\')\n    parser.add_argument(\n        \'-i\',\n        help=(\n            \'optional - ticker id \'\n            \'not used without a database\'),\n        required=False,\n        dest=\'ticker_id\')\n    parser.add_argument(\n        \'-e\',\n        help=(\n            \'optional - options expiration date\'),\n        required=False,\n        dest=\'exp_date_str\')\n    parser.add_argument(\n        \'-l\',\n        help=(\n            \'optional - path to the log config file\'),\n        required=False,\n        dest=\'log_config_path\')\n    parser.add_argument(\n        \'-b\',\n        help=(\n            \'optional - broker url for Celery\'),\n        required=False,\n        dest=\'broker_url\')\n    parser.add_argument(\n        \'-B\',\n        help=(\n            \'optional - backend url for Celery\'),\n        required=False,\n        dest=\'backend_url\')\n    parser.add_argument(\n        \'-k\',\n        help=(\n            \'optional - s3 access key\'),\n        required=False,\n        dest=\'s3_access_key\')\n    parser.add_argument(\n        \'-s\',\n        help=(\n            \'optional - s3 secret key\'),\n        required=False,\n        dest=\'s3_secret_key\')\n    parser.add_argument(\n        \'-a\',\n        help=(\n            \'optional - s3 address format: <host:port>\'),\n        required=False,\n        dest=\'s3_address\')\n    parser.add_argument(\n        \'-S\',\n        help=(\n            \'optional - s3 ssl or not\'),\n        required=False,\n        dest=\'s3_secure\')\n    parser.add_argument(\n        \'-u\',\n        help=(\n            \'optional - s3 bucket name\'),\n        required=False,\n        dest=\'s3_bucket_name\')\n    parser.add_argument(\n        \'-G\',\n        help=(\n            \'optional - s3 region name\'),\n        required=False,\n        dest=\'s3_region_name\')\n    parser.add_argument(\n        \'-p\',\n        help=(\n            \'optional - redis_password\'),\n        required=False,\n        dest=\'redis_password\')\n    parser.add_argument(\n        \'-r\',\n        help=(\n            \'optional - redis_address format: <host:port>\'),\n        required=False,\n        dest=\'redis_address\')\n    parser.add_argument(\n        \'-n\',\n        help=(\n            \'optional - redis and s3 key name\'),\n        required=False,\n        dest=\'keyname\')\n    parser.add_argument(\n        \'-m\',\n        help=(\n            \'optional - redis database number (0 by default)\'),\n        required=False,\n        dest=\'redis_db\')\n    parser.add_argument(\n        \'-x\',\n        help=(\n            \'optional - redis expiration in seconds\'),\n        required=False,\n        dest=\'redis_expire\')\n    parser.add_argument(\n        \'-z\',\n        help=(\n            \'optional - strike price\'),\n        required=False,\n        dest=\'strike\')\n    parser.add_argument(\n        \'-c\',\n        help=(\n            \'optional - contract type ""C"" for calls ""P"" for puts\'),\n        required=False,\n        dest=\'contract_type\')\n    parser.add_argument(\n        \'-P\',\n        help=(\n            \'optional - get pricing data if ""1"" or ""0"" disabled\'),\n        required=False,\n        dest=\'get_pricing\')\n    parser.add_argument(\n        \'-N\',\n        help=(\n            \'optional - get news data if ""1"" or ""0"" disabled\'),\n        required=False,\n        dest=\'get_news\')\n    parser.add_argument(\n        \'-O\',\n        help=(\n            \'optional - get options data if ""1"" or ""0"" disabled\'),\n        required=False,\n        dest=\'get_options\')\n    parser.add_argument(\n        \'-U\',\n        help=(\n            \'optional - s3 enabled for publishing if ""1"" or \'\n            \'""0"" is disabled\'),\n        required=False,\n        dest=\'s3_enabled\')\n    parser.add_argument(\n        \'-R\',\n        help=(\n            \'optional - redis enabled for publishing if ""1"" or \'\n            \'""0"" is disabled\'),\n        required=False,\n        dest=\'redis_enabled\')\n    parser.add_argument(\n        \'-A\',\n        help=(\n            \'optional - run an analysis \'\n            \'supported modes: scn\'),\n        required=False,\n        dest=\'analysis_type\')\n    parser.add_argument(\n        \'-L\',\n        help=(\n            \'optional - screener urls to pull \'\n            \'tickers for analysis\'),\n        required=False,\n        dest=\'urls\')\n    parser.add_argument(\n        \'-Z\',\n        help=(\n            \'disable run without an engine for local testing and demos\'),\n        required=False,\n        dest=\'celery_enabled\',\n        action=\'store_true\')\n    parser.add_argument(\n        \'-F\',\n        help=(\n            \'optional - backfill date for filling in \'\n            \'gaps for the IEX Cloud minute dataset \'\n            \'format is YYYY-MM-DD\'),\n        required=False,\n        dest=\'backfill_date\')\n    parser.add_argument(\n        \'-d\',\n        help=(\n            \'debug\'),\n        required=False,\n        dest=\'debug\',\n        action=\'store_true\')\n    args = parser.parse_args()\n\n    run_offline = True\n    ticker = ae_consts.TICKER\n    ticker_id = ae_consts.TICKER_ID\n    fetch_mode = \'initial\'\n    exp_date_str = ae_consts.NEXT_EXP_STR\n    ssl_options = ae_consts.SSL_OPTIONS\n    transport_options = ae_consts.TRANSPORT_OPTIONS\n    broker_url = ae_consts.WORKER_BROKER_URL\n    backend_url = ae_consts.WORKER_BACKEND_URL\n    celery_config_module = ae_consts.WORKER_CELERY_CONFIG_MODULE\n    include_tasks = ae_consts.INCLUDE_TASKS\n    s3_access_key = ae_consts.S3_ACCESS_KEY\n    s3_secret_key = ae_consts.S3_SECRET_KEY\n    s3_region_name = ae_consts.S3_REGION_NAME\n    s3_address = ae_consts.S3_ADDRESS\n    s3_secure = ae_consts.S3_SECURE\n    s3_bucket_name = ae_consts.S3_BUCKET\n    s3_key = ae_consts.S3_KEY\n    redis_address = ae_consts.REDIS_ADDRESS\n    redis_key = ae_consts.REDIS_KEY\n    redis_password = ae_consts.REDIS_PASSWORD\n    redis_db = ae_consts.REDIS_DB\n    redis_expire = ae_consts.REDIS_EXPIRE\n    strike = None\n    contract_type = None\n    get_pricing = True\n    get_news = True\n    get_options = True\n    s3_enabled = True\n    redis_enabled = True\n    analysis_type = None\n    backfill_date = None\n    debug = False\n\n    if args.ticker:\n        ticker = args.ticker.upper()\n    if args.ticker_id:\n        ticker_id = args.ticker_id\n    if args.exp_date_str:\n        exp_date_str = ae_consts.NEXT_EXP_STR\n    if args.broker_url:\n        broker_url = args.broker_url\n    if args.backend_url:\n        backend_url = args.backend_url\n    if args.s3_access_key:\n        s3_access_key = args.s3_access_key\n    if args.s3_secret_key:\n        s3_secret_key = args.s3_secret_key\n    if args.s3_region_name:\n        s3_region_name = args.s3_region_name\n    if args.s3_address:\n        s3_address = args.s3_address\n    if args.s3_secure:\n        s3_secure = args.s3_secure\n    if args.s3_bucket_name:\n        s3_bucket_name = args.s3_bucket_name\n    if args.keyname:\n        s3_key = args.keyname\n        redis_key = args.keyname\n    if args.redis_address:\n        redis_address = args.redis_address\n    if args.redis_password:\n        redis_password = args.redis_password\n    if args.redis_db:\n        redis_db = args.redis_db\n    if args.redis_expire:\n        redis_expire = args.redis_expire\n    if args.strike:\n        strike = args.strike\n    if args.contract_type:\n        contract_type = args.contract_type\n    if args.get_pricing:\n        get_pricing = args.get_pricing == \'1\'\n    if args.get_news:\n        get_news = args.get_news == \'1\'\n    if args.get_options:\n        get_options = args.get_options == \'1\'\n    if args.s3_enabled:\n        s3_enabled = args.s3_enabled == \'1\'\n    if args.redis_enabled:\n        redis_enabled = args.redis_enabled == \'1\'\n    if args.fetch_mode:\n        fetch_mode = str(args.fetch_mode).lower()\n    if args.analysis_type:\n        analysis_type = str(args.analysis_type).lower()\n    if args.celery_enabled:\n        run_offline = False\n    if args.backfill_date:\n        backfill_date = args.backfill_date\n    if args.debug:\n        debug = True\n\n    work = api_requests.build_get_new_pricing_request()\n\n    work[\'ticker\'] = ticker\n    work[\'ticker_id\'] = ticker_id\n    work[\'s3_bucket\'] = s3_bucket_name\n    work[\'s3_key\'] = s3_key\n    work[\'redis_key\'] = redis_key\n    work[\'strike\'] = strike\n    work[\'contract\'] = contract_type\n    work[\'exp_date\'] = exp_date_str\n    work[\'s3_access_key\'] = s3_access_key\n    work[\'s3_secret_key\'] = s3_secret_key\n    work[\'s3_region_name\'] = s3_region_name\n    work[\'s3_address\'] = s3_address\n    work[\'s3_secure\'] = s3_secure\n    work[\'redis_address\'] = redis_address\n    work[\'redis_password\'] = redis_password\n    work[\'redis_db\'] = redis_db\n    work[\'redis_expire\'] = redis_expire\n    work[\'get_pricing\'] = get_pricing\n    work[\'get_news\'] = get_news\n    work[\'get_options\'] = get_options\n    work[\'s3_enabled\'] = s3_enabled\n    work[\'redis_enabled\'] = redis_enabled\n    work[\'fetch_mode\'] = fetch_mode\n    work[\'analysis_type\'] = analysis_type\n    work[\'iex_datasets\'] = iex_consts.DEFAULT_FETCH_DATASETS\n    work[\'backfill_date\'] = backfill_date\n    work[\'debug\'] = debug\n    work[\'label\'] = f\'ticker={ticker}\'\n\n    if analysis_type == \'scn\':\n        label = f\'screener={work[""ticker""]}\'\n        fv_urls = []\n        if args.urls:\n            fv_urls = str(args.urls).split(\'|\')\n        if len(fv_urls) == 0:\n            fv_urls = os.getenv(\'SCREENER_URLS\', []).split(\'|\')\n        screener_req = api_requests.build_screener_analysis_request(\n            ticker=ticker,\n            fv_urls=fv_urls,\n            label=label)\n        work.update(screener_req)\n        start_screener_analysis(\n            req=work)\n    # end of analysis_type\n    else:\n        last_close_date = ae_utils.last_close()\n        last_close_str = last_close_date.strftime(\n            ae_consts.COMMON_DATE_FORMAT)\n        cache_base_key = f\'{ticker}_{last_close_str}\'\n        if not args.keyname:\n            work[\'s3_key\'] = cache_base_key\n            work[\'redis_key\'] = cache_base_key\n\n        path_to_tasks = \'analysis_engine.work_tasks\'\n        task_name = (\n            f\'{path_to_tasks}\'\n            f\'.get_new_pricing_data.get_new_pricing_data\')\n        task_res = None\n        if ae_consts.is_celery_disabled() or run_offline:\n            work[\'celery_disabled\'] = True\n            work[\'verbose\'] = debug\n            log.debug(\n                f\'starting without celery work={ae_consts.ppj(work)} \'\n                f\'offline={run_offline}\')\n            task_res = task_pricing.get_new_pricing_data(\n                work)\n            status_str = ae_consts.get_status(status=task_res[\'status\'])\n\n            cur_date = backfill_date\n            if not backfill_date:\n                cur_date = ae_utils.get_last_close_str()\n            redis_arr = work[""redis_address""].split(\':\')\n            include_results = \'\'\n            if debug:\n                include_results = task_res[\'rec\']\n            if task_res[\'status\'] == ae_consts.SUCCESS:\n                if task_res[\'rec\'][\'num_success\'] == 0:\n                    log.error(\n                        f\'failed fetching ticker={work[""ticker""]} \'\n                        f\'from {fetch_mode} - please check the \'\n                        \'environment variables\')\n                else:\n                    log.info(\n                        f\'done fetching ticker={work[""ticker""]} \'\n                        f\'mode={fetch_mode} \'\n                        f\'status={status_str} \'\n                        f\'err={task_res[""err""]} {include_results}\')\n                    print(\n                        \'View keys in redis with:\\n\'\n                        f\'redis-cli -h {redis_arr[0]} \'\n                        \'keys \'\n                        f\'""{work[""ticker""]}_{cur_date}*""\')\n            elif task_res[\'status\'] == ae_consts.MISSING_TOKEN:\n                print(\n                    \'Set an IEX or Tradier token: \'\n                    \'\\n\'\n                    \'  export IEX_TOKEN=YOUR_IEX_TOKEN\\n\'\n                    \'  export TD_TOKEN=YOUR_TD_TOKEN\\n\')\n            else:\n                log.error(\n                    f\'done fetching ticker={work[""ticker""]} \'\n                    f\'mode={fetch_mode} \'\n                    f\'status={status_str} \'\n                    f\'err={task_res[""err""]}\')\n            # if/else debug\n        else:\n            log.debug(\n                f\'connecting to broker={broker_url} \'\n                f\'backend={backend_url}\')\n\n            # Get the Celery app\n            app = get_celery_app.get_celery_app(\n                name=__name__,\n                auth_url=broker_url,\n                backend_url=backend_url,\n                path_to_config_module=celery_config_module,\n                ssl_options=ssl_options,\n                transport_options=transport_options,\n                include_tasks=include_tasks)\n\n            log.debug(f\'calling task={task_name} - work={ae_consts.ppj(work)}\')\n            job_id = app.send_task(\n                task_name,\n                (work,))\n            log.debug(f\'task={task_name} - job_id={job_id}\')\n        # end of if/else\n    # end of supported modes\n# end of fetch_new_stock_datasets\n\n\nif __name__ == \'__main__\':\n    fetch_new_stock_datasets()\n'"
analysis_engine/scripts/inspect_datasets.py,0,"b'#!/usr/bin/env python\n\n""""""\nTool for inspecting cached pricing data to find common errors.\nThis tool uses the\n`Extraction API <https://stock-analysis-engine.\nreadthedocs.io/en/latest/extract.html>`__ to look for dates\nthat are not in sync with the redis cached date.\n\n.. note:: This tool requires redis to be running with\n    fetched datasets already stored in supported\n    keys\n\n**Examples**\n\n**Inspect Minute Datasets for a Ticker**\n\n::\n\n    inspect_datasets.py -t SPY\n\n**Inspect Daily Datasets for a Ticker**\n\n::\n\n    inspect_datasets.py -t AAPL -g daily\n    # or\n    # inspect_datasets.py -t AAPL -g day\n\n**Usage**\n\n::\n\n    inspect_datasets.py -h\n    usage: inspect_datasets.py [-h] [-t TICKER] [-g DATASETS] [-s START_DATE]\n\n    Inspect datasets looking for dates in redis that look incorrect\n\n    optional arguments:\n    -h, --help     show this help message and exit\n    -t TICKER      ticker\n    -g DATASETS    optional - datasets: minute or min = examine IEX Cloud\n                    intraday minute data, daily or day = examine IEX Cloud\n                    daily\n                    data, quote = examine IEX Cloud quotes data, stats =\n                    examine\n                    IEX Cloud key stats data, peers = examine IEX Cloud\n                    peers\n                    data, news = examine IEX Cloud news data, fin = examine\n                    IEX\n                    Cloud financials data, earn = examine IEX Cloud earnings\n                    data, div = examine IEX Cloud dividendsdata, comp =\n                    examine\n                    IEX Cloud company data, calls = examine Tradier calls\n                    data,\n                    puts = examine Tradier puts data, and comma delimited is\n                    supported as well\n    -s START_DATE  start date format YYYY-MM-DD (default is 2019-01-01)\n""""""\n\nimport datetime\nimport argparse\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.utils as ae_utils\nimport analysis_engine.extract as ae_extract\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(\n    name=\'inspect-redis-data\',\n    handler_name=\'no_date_colors\')\n\n\ndef inspect_datasets(\n        ticker=None,\n        start_date=None,\n        datasets=None):\n    """"""inspect_datasets\n\n    Loop over all cached data in redis by going sequentially per date\n    and examine the latest ``date`` value in the cache to\n    check if it matches the redis key\'s date.\n\n    For IEX Cloud minute data errors, running this function will print out\n    commands to fix any issues (if possible):\n\n    ::\n\n        fetch -t TICKER -g iex_min -F DATE_TO_FIX\n\n    :param ticker: optional - string ticker\n    :param start_date: optional - datetime\n        start date for the loop\n        (default is ``2019-01-01``)\n    :param datasets: optional - list of strings\n        to extract specific, supported datasets\n        (default is ``[\'minute\']``)\n    """"""\n\n    if not start_date:\n        start_date = datetime.datetime(\n            year=2019,\n            month=1,\n            day=1)\n    if not datasets:\n        datasets = [\n            \'minute\'\n        ]\n    if not ticker:\n        ticker = \'SPY\'\n\n    tickers = [\n        ticker\n    ]\n\n    fix_suggestions = []\n    last_close = ae_utils.last_close()\n    for ticker in tickers:\n\n        not_done = True\n        cur_date = start_date\n        while not_done:\n            cur_date_str = cur_date.strftime(ae_consts.COMMON_DATE_FORMAT)\n\n            log.info(\n                f\'extracting {ticker} date={cur_date_str}\')\n\n            res = None\n\n            # get from a date or the latest if not set\n            if cur_date_str:\n                res = ae_extract.extract(\n                    ticker=ticker,\n                    date=cur_date_str,\n                    datasets=datasets)\n            else:\n                res = ae_extract.extract(\n                    ticker=ticker,\n                    datasets=datasets)\n\n            weekday_name = cur_date.strftime(\'%A\')\n\n            for ds_name in datasets:\n                df = res[ticker][ds_name]\n\n                if ae_consts.is_df(df=df):\n                    if \'date\' in df:\n                        latest_date = df[\'date\'].iloc[-1]\n                        latest_date_str = latest_date.strftime(\n                            ae_consts.COMMON_DATE_FORMAT)\n                        if latest_date_str == cur_date_str:\n                            log.info(\n                                f\'valid - {ds_name} latest dates match \'\n                                f\'{weekday_name}: \'\n                                f\'{latest_date_str} == {cur_date_str}\')\n                        else:\n                            if ds_name != \'daily\':\n                                log.critical(\n                                    f\'{ds_name} latest dates does \'\n                                    f\'NOT match on \'\n                                    f\'{weekday_name} {cur_date_str} found: \'\n                                    f\'{latest_date_str}\')\n                            else:\n                                one_day_back = (\n                                    latest_date + datetime.timedelta(days=1))\n                                if weekday_name == \'Monday\':\n                                    one_day_back = (\n                                        latest_date + datetime.timedelta(\n                                            days=3))\n                                latest_date_str = one_day_back.strftime(\n                                    ae_consts.COMMON_DATE_FORMAT)\n                                if latest_date_str == cur_date_str:\n                                    log.info(\n                                        f\'valid - {ds_name} latest dates \'\n                                        f\'match \'\n                                        f\'{weekday_name}: \'\n                                        f\'{latest_date_str} == \'\n                                        f\'{cur_date_str}\')\n                                else:\n                                    log.critical(\n                                        f\'{ds_name} latest dates does \'\n                                        f\'NOT match on \'\n                                        f\'{weekday_name} {cur_date_str} \'\n                                        f\'found: \'\n                                        f\'{latest_date_str}\')\n\n                            if ds_name == \'minute\':\n                                fix_suggestions.append(\n                                    f\'fetch -t {ticker} -g iex_min \'\n                                    f\'-F {cur_date_str}\')\n                    else:\n                        log.error(\n                            f\'{ds_name} df does not have a date column \'\n                            f\'on {cur_date_str}\')\n                else:\n                    log.error(\n                        f\'Missing {ds_name} df on {cur_date_str}\')\n            # end of inspecting datasets\n\n            if cur_date > last_close:\n                not_done = False\n            else:\n                cur_date += datetime.timedelta(days=1)\n                not_a_weekday = True\n                while not_a_weekday:\n                    weekday = cur_date.date().weekday()\n                    if weekday > 4:\n                        log.debug(\n                            \'SKIP weekend day: \'\n                            f\'{cur_date.strftime(""%A on %Y-%m-%d"")}\')\n                        cur_date += datetime.timedelta(days=1)\n                    else:\n                        not_a_weekday = False\n        # end for all dates\n    # end of for all tickers\n\n    if len(fix_suggestions) > 0:\n        print(\'-------------------------------\')\n        print(\n            \'Detected invalid dates - below are the suggested fixes \'\n            \'to run using the fetch command.\')\n        print(\n            \' - Please be aware fetching data may incur usages and \'\n            \'costs on your account\')\n        for s in fix_suggestions:\n            print(s)\n    else:\n        log.info(\n            \'done\')\n# end inspect_datasets\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(\n        description=(\n            \'Inspect datasets looking for dates in redis \'\n            \'that look incorrect\'))\n    parser.add_argument(\n        \'-t\',\n        help=(\n            \'ticker\'),\n        required=False,\n        dest=\'ticker\')\n    parser.add_argument(\n        \'-g\',\n        help=(\n            \'optional - datasets: \'\n            \'minute or min = examine IEX Cloud intraday minute data, \'\n            \'daily or day  = examine IEX Cloud daily data, \'\n            \'quote = examine IEX Cloud quotes data, \'\n            \'stats = examine IEX Cloud key stats data, \'\n            \'peers = examine IEX Cloud peers data, \'\n            \'news = examine IEX Cloud news data, \'\n            \'fin = examine IEX Cloud financials data, \'\n            \'earn = examine IEX Cloud earnings data, \'\n            \'div = examine IEX Cloud dividendsdata, \'\n            \'comp = examine IEX Cloud company data, \'\n            \'calls = examine Tradier calls data, \'\n            \'puts = examine Tradier puts data, \'\n            \'and comma delimited is supported as well\'),\n        required=False,\n        dest=\'datasets\')\n    parser.add_argument(\n        \'-s\',\n        help=(\n            \'start date format YYYY-MM-DD (default is 2019-01-01)\'),\n        required=False,\n        dest=\'start_date\')\n    args = parser.parse_args()\n\n    start_date = datetime.datetime(\n        year=2019,\n        month=1,\n        day=1)\n    datasets = [\n        \'minute\'\n    ]\n    ticker = \'SPY\'\n\n    valid = True\n    if args.ticker:\n        ticker = args.ticker.upper()\n    if args.datasets:\n        datasets = []\n        for key in args.datasets.lower().split(\',\'):\n            if key == \'news\':\n                datasets.append(\'news1\')\n            elif key == \'min\':\n                datasets.append(\'minute\')\n            elif key == \'day\':\n                datasets.append(\'daily\')\n            elif key == \'fin\':\n                datasets.append(\'financials\')\n            elif key == \'earn\':\n                datasets.append(\'earnings\')\n            elif key == \'div\':\n                datasets.append(\'dividends\')\n            elif key == \'comp\':\n                datasets.append(\'company\')\n            elif key == \'calls\':\n                datasets.append(\'tdcalls\')\n            elif key == \'puts\':\n                datasets.append(\'tdputs\')\n            else:\n                if key not in ae_consts.BACKUP_DATASETS:\n                    log.error(\n                        f\'unsupported dataset key: {key} \'\n                        \'please use a supported key: \'\n                        f\'{ae_consts.BACKUP_DATASETS}\')\n                    valid = False\n                else:\n                    datasets.append(key)\n    if args.start_date:\n        start_date = datetime.datetime.strptime(\n            args.start_date,\n            \'%Y-%m-%d\')\n\n    if valid:\n        inspect_datasets(\n            ticker=ticker,\n            start_date=start_date,\n            datasets=datasets)\n'"
analysis_engine/scripts/plot_history_from_local_file.py,0,"b'#!/usr/bin/env python\n\n""""""\nA tool for plotting an algorithm\'s ``Trading History`` from\na locally saved file from running the backtester with\nthe save to file option enabled:\n\n::\n\n    run_backtest_and_plot_history.py -t SPY -f <SAVE_HISTORY_TO_THIS_FILE>\n""""""\n\nimport os\nimport argparse\nimport pandas as pd\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.plot_trading_history as plot_trading_history\nimport spylunking.log.setup_logging as log_utils\n\n\nlog = log_utils.build_colorized_logger(\n    name=\'plot-history\',\n    log_config_path=ae_consts.LOG_CONFIG_PATH)\n\n\ndef plot_local_history_file():\n    """"""plot_local_history_file\n\n    Run a derived algorithm with an algorithm config dictionary\n\n    :param config_dict: algorithm config dictionary\n    """"""\n\n    log.debug(\'start - plot\')\n\n    parser = argparse.ArgumentParser(\n        description=(\n            \'plot a local algorithm trading history file\'))\n    parser.add_argument(\n        \'-f\',\n        help=(\n            \'plot this trading history dataframe \'\n            \'saved in this file\'),\n        required=False,\n        dest=\'history_json_file\')\n    parser.add_argument(\n        \'-d\',\n        help=(\n            \'debug\'),\n        required=False,\n        dest=\'debug\',\n        action=\'store_true\')\n    args = parser.parse_args()\n\n    history_json_file = None\n    debug = False\n\n    if args.history_json_file:\n        history_json_file = args.history_json_file\n    if args.debug:\n        debug = True\n\n    if not history_json_file:\n        log.error(\n            \'usage error - please run with: \'\n            \'-f <path to local trading history file>\')\n        return\n    elif not os.path.exists(history_json_file):\n        log.error(f\'did not find trading history file={history_json_file}\')\n        return\n    # end of checking the file arg is set and exists on disk\n\n    log.info(f\'plotting history to: {history_json_file}\')\n    history_df = pd.read_json(\n        history_json_file,\n        orient=\'records\')\n\n    history_df[\'date\'] = pd.to_datetime(\n        history_df[\'date\'])\n    history_df[\'minute\'] = pd.to_datetime(\n        history_df[\'minute\'])\n    ticker = history_df[\'ticker\'].iloc[0]\n\n    log.info(\'plotting history\')\n\n    first_date = history_df[\'date\'].iloc[0]\n    end_date = history_df[\'date\'].iloc[-1]\n    title = (\n        f\'Trading History {ticker}\\n\'\n        f\'Backtest dates from {first_date} to {end_date}\')\n    use_xcol = \'date\'\n    use_as_date_format = \'%d\\n%b\'\n    use_minute = False\n    if \'minute\' in history_df:\n        found_valid_minute = history_df[\'minute\'].iloc[0]\n        if found_valid_minute:\n            use_minute = True\n\n    if use_minute:\n        use_xcol = \'minute\'\n        use_as_date_format = \'%d %H:%M:%S\\n%b\'\n    xlabel = \'Dates vs Algo values\'\n    ylabel = \'Algo values\'\n    df_filter = (history_df[\'close\'] > 1.00)\n\n    # set default hloc columns:\n    blue = None\n    green = None\n    orange = None\n\n    red = \'close\'\n    blue = \'balance\'\n\n    if debug:\n        for i, r in history_df.iterrows():\n            log.info(f\'{r[""minute""]} - {r[""close""]}\')\n    # end of debug\n\n    show_plot = True\n    if show_plot:\n        plot_trading_history.plot_trading_history(\n            title=title,\n            df=history_df,\n            red=red,\n            blue=blue,\n            green=green,\n            orange=orange,\n            date_col=use_xcol,\n            date_format=use_as_date_format,\n            xlabel=xlabel,\n            ylabel=ylabel,\n            df_filter=df_filter,\n            show_plot=True,\n            dropna_for_all=True)\n\n# end of plot_local_history_file\n\n\nif __name__ == \'__main__\':\n    plot_local_history_file()\n'"
analysis_engine/scripts/plot_history_from_s3.py,0,"b'#!/usr/bin/env python\n\n""""""\nA tool for plotting an algorithm\'s ``Trading History`` from\na file in s3 from running the backtester with\nthe save to file option enabled:\n\n::\n\n    run_backtest_and_plot_history.py -t SPY -f <SAVE_HISTORY_TO_THIS_FILE>\n""""""\n\nimport argparse\nimport pandas as pd\nimport analysis_engine.consts as consts\nimport analysis_engine.plot_trading_history as plot_trading_history\nimport analysis_engine.load_history_dataset as load_history\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=\'view-history-in-s3\')\n\n\ndef plot_history_from_s3():\n    """"""plot_history_from_s3\n\n    Run a derived algorithm with an algorithm config dictionary\n\n    :param config_dict: algorithm config dictionary\n    """"""\n\n    log.debug(\'start - plot\')\n\n    parser = argparse.ArgumentParser(\n        description=(\n            \'plot a local algorithm trading history file\'))\n    parser.add_argument(\n        \'-b\',\n        help=(\n            \'saved in this s3 bucket\'),\n        required=False,\n        dest=\'s3_bucket\')\n    parser.add_argument(\n        \'-k\',\n        help=(\n            \'saved in this s3 key\'),\n        required=False,\n        dest=\'history_json_file\')\n    parser.add_argument(\n        \'-d\',\n        help=(\n            \'debug\'),\n        required=False,\n        dest=\'debug\',\n        action=\'store_true\')\n    args = parser.parse_args()\n\n    s3_access_key = consts.S3_ACCESS_KEY\n    s3_secret_key = consts.S3_SECRET_KEY\n    s3_region_name = consts.S3_REGION_NAME\n    s3_address = consts.S3_ADDRESS\n    s3_secure = consts.S3_SECURE\n    compress = True\n\n    s3_bucket = (\n        \'bt-spy-williamsr-2018-12-05-22-44-50-714400\')\n    s3_key = (\n        \'-181.55_netgain_9818.45_netvalue_NEGATIVE_\'\n        \'10000.0_startbalance_1710.95_endbalance_\'\n        \'30_shares_270.25_close_3_buys_0_sells_\'\n        \'1_minbuyinds_1_minsellinds_\'\n        \'43.52_seconds_\'\n        \'trade_history-SPY_williamsr_test_\'\n        \'0.73_for_176_of_24000.json\')\n\n    debug = False\n\n    if args.debug:\n        debug = True\n\n    load_res = load_history.load_history_dataset(\n        s3_enabled=True,\n        s3_key=s3_key,\n        s3_address=s3_address,\n        s3_bucket=s3_bucket,\n        s3_access_key=s3_access_key,\n        s3_secret_key=s3_secret_key,\n        s3_region_name=s3_region_name,\n        s3_secure=s3_secure,\n        compress=compress)\n\n    algo_config = load_res.get(\n        \'algo_config_dict\',\n        None)\n    algo_name = load_res.get(\n        \'algo_name\',\n        None)\n    tickers = load_res.get(\n        \'tickers\',\n        [\n            \'SPY\',\n        ])\n    ticker = tickers[0]\n\n    log.info(f\'found algo: {algo_name}\')\n    log.info(f\'config: {consts.ppj(algo_config)}\')\n\n    history_df = load_res[ticker]\n    history_df[\'date\'] = pd.to_datetime(\n        history_df[\'date\'])\n    history_df[\'minute\'] = pd.to_datetime(\n        history_df[\'minute\'])\n    ticker = history_df[\'ticker\'].iloc[0]\n\n    log.info(\'plotting history\')\n\n    first_date = history_df[\'date\'].iloc[0]\n    end_date = history_df[\'date\'].iloc[-1]\n    title = (\n        f\'Trading History {ticker}\\n\'\n        f\'Backtest dates from {first_date} to {end_date}\')\n    use_xcol = \'date\'\n    use_as_date_format = \'%d\\n%b\'\n    use_minute = False\n    if \'minute\' in history_df:\n        found_valid_minute = history_df[\'minute\'].iloc[0]\n        if found_valid_minute:\n            use_minute = True\n\n    if use_minute:\n        use_xcol = \'minute\'\n        use_as_date_format = \'%d %H:%M:%S\\n%b\'\n    xlabel = \'Dates vs Algo values\'\n    ylabel = \'Algo values\'\n    df_filter = (history_df[\'close\'] > 1.00)\n\n    # set default hloc columns:\n    blue = None\n    green = None\n    orange = None\n\n    red = \'close\'\n    blue = \'balance\'\n\n    if debug:\n        for i, r in history_df.iterrows():\n            log.info(f\'{r[""minute""]} - {r[""close""]}\')\n    # end of debug\n\n    show_plot = True\n    if show_plot:\n        plot_trading_history.plot_trading_history(\n            title=title,\n            df=history_df,\n            red=red,\n            blue=blue,\n            green=green,\n            orange=orange,\n            date_col=use_xcol,\n            date_format=use_as_date_format,\n            xlabel=xlabel,\n            ylabel=ylabel,\n            df_filter=df_filter,\n            show_plot=True,\n            dropna_for_all=True)\n\n# end of plot_history_from_s3\n\n\nif __name__ == \'__main__\':\n    plot_history_from_s3()\n'"
analysis_engine/scripts/print_last_close_date.py,0,"b""#!/usr/bin/env python\n\nfrom analysis_engine.utils import last_close\nlast_close_str = last_close().strftime('%Y-%m-%d %H:%M:%S')\nprint(last_close_str)\n"""
analysis_engine/scripts/print_next_expiration_date.py,0,"b""#!/usr/bin/env python\n\nfrom analysis_engine.options_dates import get_options_for_today\nexp_dates = get_options_for_today()\nprint(exp_dates[-1]['exp_date_str'])\n"""
analysis_engine/scripts/publish_from_s3_to_redis.py,0,"b'#!/usr/bin/env python\n\n""""""\n\nPublish the contents of an S3 key to a\nRedis key\n\nSteps:\n------\n\n1) Parse arguments\n2) Download pricing data as a Celery task\n3) Publish pricing data as a Celery tasks\n4) Coming Soon - Start buy/sell analysis as Celery task(s)\n\n""""""\n\nimport argparse\nimport analysis_engine.work_tasks.publish_from_s3_to_redis \\\n    as task_publisher\nfrom analysis_engine.work_tasks.get_celery_app import get_celery_app\nfrom celery import signals\nfrom spylunking.log.setup_logging import build_colorized_logger\nfrom analysis_engine.api_requests import build_publish_from_s3_to_redis_request\nfrom analysis_engine.consts import LOG_CONFIG_PATH\nfrom analysis_engine.consts import TICKER\nfrom analysis_engine.consts import TICKER_ID\nfrom analysis_engine.consts import WORKER_BROKER_URL\nfrom analysis_engine.consts import WORKER_BACKEND_URL\nfrom analysis_engine.consts import WORKER_CELERY_CONFIG_MODULE\nfrom analysis_engine.consts import INCLUDE_TASKS\nfrom analysis_engine.consts import SSL_OPTIONS\nfrom analysis_engine.consts import TRANSPORT_OPTIONS\nfrom analysis_engine.consts import S3_ACCESS_KEY\nfrom analysis_engine.consts import S3_SECRET_KEY\nfrom analysis_engine.consts import S3_REGION_NAME\nfrom analysis_engine.consts import S3_ADDRESS\nfrom analysis_engine.consts import S3_SECURE\nfrom analysis_engine.consts import S3_BUCKET\nfrom analysis_engine.consts import S3_KEY\nfrom analysis_engine.consts import REDIS_ADDRESS\nfrom analysis_engine.consts import REDIS_KEY\nfrom analysis_engine.consts import REDIS_PASSWORD\nfrom analysis_engine.consts import REDIS_DB\nfrom analysis_engine.consts import REDIS_EXPIRE\nfrom analysis_engine.consts import get_status\nfrom analysis_engine.consts import ppj\nfrom analysis_engine.consts import is_celery_disabled\n\n\n# Disable celery log hijacking\n# https://github.com/celery/celery/issues/2509\n@signals.setup_logging.connect\ndef setup_celery_logging(**kwargs):\n    pass\n\n\nlog = build_colorized_logger(\n    name=\'pub-from-s3-to-redis\',\n    log_config_path=LOG_CONFIG_PATH)\n\n\ndef publish_from_s3_to_redis():\n    """"""publish_from_s3_to_redis\n\n    Download an S3 key and publish it\'s contents to Redis\n\n    """"""\n\n    log.info(\n        \'start - publish_from_s3_to_redis\')\n\n    parser = argparse.ArgumentParser(\n        description=(\n            \'Download and store the latest stock pricing, \'\n            \'news, and options chain data \'\n            \'and store it in S3 and Redis. \'\n            \'Once stored, this will also \'\n            \'start the buy and sell trading analysis.\'))\n    parser.add_argument(\n        \'-t\',\n        help=(\n            \'ticker\'),\n        required=True,\n        dest=\'ticker\')\n    parser.add_argument(\n        \'-i\',\n        help=(\n            \'optional - ticker id \'\n            \'not used without a database\'),\n        required=False,\n        dest=\'ticker_id\')\n    parser.add_argument(\n        \'-l\',\n        help=(\n            \'optional - path to the log config file\'),\n        required=False,\n        dest=\'log_config_path\')\n    parser.add_argument(\n        \'-b\',\n        help=(\n            \'optional - broker url for Celery\'),\n        required=False,\n        dest=\'broker_url\')\n    parser.add_argument(\n        \'-B\',\n        help=(\n            \'optional - backend url for Celery\'),\n        required=False,\n        dest=\'backend_url\')\n    parser.add_argument(\n        \'-k\',\n        help=(\n            \'optional - s3 access key\'),\n        required=False,\n        dest=\'s3_access_key\')\n    parser.add_argument(\n        \'-s\',\n        help=(\n            \'optional - s3 secret key\'),\n        required=False,\n        dest=\'s3_secret_key\')\n    parser.add_argument(\n        \'-a\',\n        help=(\n            \'optional - s3 address format: <host:port>\'),\n        required=False,\n        dest=\'s3_address\')\n    parser.add_argument(\n        \'-S\',\n        help=(\n            \'optional - s3 ssl or not\'),\n        required=False,\n        dest=\'s3_secure\')\n    parser.add_argument(\n        \'-u\',\n        help=(\n            \'optional - s3 bucket name\'),\n        required=False,\n        dest=\'s3_bucket_name\')\n    parser.add_argument(\n        \'-g\',\n        help=(\n            \'optional - s3 region name\'),\n        required=False,\n        dest=\'s3_region_name\')\n    parser.add_argument(\n        \'-p\',\n        help=(\n            \'optional - redis_password\'),\n        required=False,\n        dest=\'redis_password\')\n    parser.add_argument(\n        \'-r\',\n        help=(\n            \'optional - redis_address format: <host:port>\'),\n        required=False,\n        dest=\'redis_address\')\n    parser.add_argument(\n        \'-n\',\n        help=(\n            \'optional - redis and s3 key name\'),\n        required=False,\n        dest=\'keyname\')\n    parser.add_argument(\n        \'-m\',\n        help=(\n            \'optional - redis database number (0 by default)\'),\n        required=False,\n        dest=\'redis_db\')\n    parser.add_argument(\n        \'-x\',\n        help=(\n            \'optional - redis expiration in seconds\'),\n        required=False,\n        dest=\'redis_expire\')\n    parser.add_argument(\n        \'-d\',\n        help=(\n            \'debug\'),\n        required=False,\n        dest=\'debug\',\n        action=\'store_true\')\n    args = parser.parse_args()\n\n    ticker = TICKER\n    ticker_id = TICKER_ID\n    ssl_options = SSL_OPTIONS\n    transport_options = TRANSPORT_OPTIONS\n    broker_url = WORKER_BROKER_URL\n    backend_url = WORKER_BACKEND_URL\n    celery_config_module = WORKER_CELERY_CONFIG_MODULE\n    include_tasks = INCLUDE_TASKS\n    s3_access_key = S3_ACCESS_KEY\n    s3_secret_key = S3_SECRET_KEY\n    s3_region_name = S3_REGION_NAME\n    s3_address = S3_ADDRESS\n    s3_secure = S3_SECURE\n    s3_bucket_name = S3_BUCKET\n    s3_key = S3_KEY\n    redis_address = REDIS_ADDRESS\n    redis_key = REDIS_KEY\n    redis_password = REDIS_PASSWORD\n    redis_db = REDIS_DB\n    redis_expire = REDIS_EXPIRE\n    debug = False\n\n    if args.ticker:\n        ticker = args.ticker.upper()\n    if args.ticker_id:\n        ticker = args.ticker_id\n    if args.broker_url:\n        broker_url = args.broker_url\n    if args.backend_url:\n        backend_url = args.backend_url\n    if args.s3_access_key:\n        s3_access_key = args.s3_access_key\n    if args.s3_secret_key:\n        s3_secret_key = args.s3_secret_key\n    if args.s3_region_name:\n        s3_region_name = args.s3_region_name\n    if args.s3_address:\n        s3_address = args.s3_address\n    if args.s3_secure:\n        s3_secure = args.s3_secure\n    if args.s3_bucket_name:\n        s3_bucket_name = args.s3_bucket_name\n    if args.keyname:\n        s3_key = args.keyname\n        redis_key = args.keyname\n    if args.redis_address:\n        redis_address = args.redis_address\n    if args.redis_password:\n        redis_password = args.redis_password\n    if args.redis_db:\n        redis_db = args.redis_db\n    if args.redis_expire:\n        redis_expire = args.redis_expire\n    if args.debug:\n        debug = True\n\n    work = build_publish_from_s3_to_redis_request()\n\n    work[\'ticker\'] = ticker\n    work[\'ticker_id\'] = ticker_id\n    work[\'s3_bucket\'] = s3_bucket_name\n    work[\'s3_key\'] = s3_key\n    work[\'redis_key\'] = redis_key\n    work[\'s3_access_key\'] = s3_access_key\n    work[\'s3_secret_key\'] = s3_secret_key\n    work[\'s3_region_name\'] = s3_region_name\n    work[\'s3_address\'] = s3_address\n    work[\'s3_secure\'] = s3_secure\n    work[\'redis_address\'] = redis_address\n    work[\'redis_password\'] = redis_password\n    work[\'redis_db\'] = redis_db\n    work[\'redis_expire\'] = redis_expire\n    work[\'debug\'] = debug\n    work[\'label\'] = f\'ticker={ticker}\'\n\n    path_to_tasks = \'analysis_engine.work_tasks\'\n    task_name = (\n        f\'{path_to_tasks}.publish_from_s3_to_redis.publish_from_s3_to_redis\')\n    task_res = None\n    if is_celery_disabled():\n        work[\'celery_disabled\'] = True\n        log.debug(f\'starting without celery work={ppj(work)}\')\n        task_res = task_publisher.publish_from_s3_to_redis(\n            work_dict=work)\n        if debug:\n            log.info(\n                f\'done - result={ppj(task_res)} task={task_name} \'\n                f\'status={get_status(status=task_res[""status""])} \'\n                f\'err={task_res[""err""]} label={work[""label""]}\')\n        else:\n            log.info(\n                f\'done - result task={task_name} \'\n                f\'status={get_status(status=task_res[""status""])} \'\n                f\'err={task_res[""err""]} label={work[""label""]}\')\n        # if/else debug\n    else:\n        log.info(f\'connecting to broker={broker_url} backend={backend_url}\')\n\n        # Get the Celery app\n        app = get_celery_app(\n            name=__name__,\n            auth_url=broker_url,\n            backend_url=backend_url,\n            path_to_config_module=celery_config_module,\n            ssl_options=ssl_options,\n            transport_options=transport_options,\n            include_tasks=include_tasks)\n\n        log.info(f\'calling task={task_name} - work={ppj(work)}\')\n        job_id = app.send_task(\n            task_name,\n            (work,))\n        log.info(f\'calling task={task_name} - success job_id={job_id}\')\n    # end of if/else\n# end of publish_from_s3_to_redis\n\n\nif __name__ == \'__main__\':\n    publish_from_s3_to_redis()\n'"
analysis_engine/scripts/publish_ticker_aggregate_from_s3.py,0,"b'#!/usr/bin/env python\n\n""""""\n\nPublish the aggregated S3 contents of a ticker to a\nRedis key and back to S3\n\nSteps:\n------\n\n1) Parse arguments\n2) Download and aggregate ticker data from S3 as a Celery task\n3) Publish aggregated data to S3 as a Celery task\n4) Publish aggregated data to Redis as a Celery task\n\n""""""\n\nimport argparse\nimport analysis_engine.work_tasks.publish_ticker_aggregate_from_s3 \\\n    as task_publisher\nfrom celery import signals\nfrom analysis_engine.work_tasks.get_celery_app import get_celery_app\nfrom spylunking.log.setup_logging import build_colorized_logger\nfrom analysis_engine.api_requests import \\\n    build_publish_ticker_aggregate_from_s3_request\nfrom analysis_engine.consts import LOG_CONFIG_PATH\nfrom analysis_engine.consts import TICKER\nfrom analysis_engine.consts import TICKER_ID\nfrom analysis_engine.consts import WORKER_BROKER_URL\nfrom analysis_engine.consts import WORKER_BACKEND_URL\nfrom analysis_engine.consts import WORKER_CELERY_CONFIG_MODULE\nfrom analysis_engine.consts import INCLUDE_TASKS\nfrom analysis_engine.consts import SSL_OPTIONS\nfrom analysis_engine.consts import TRANSPORT_OPTIONS\nfrom analysis_engine.consts import S3_ACCESS_KEY\nfrom analysis_engine.consts import S3_SECRET_KEY\nfrom analysis_engine.consts import S3_REGION_NAME\nfrom analysis_engine.consts import S3_ADDRESS\nfrom analysis_engine.consts import S3_SECURE\nfrom analysis_engine.consts import S3_BUCKET\nfrom analysis_engine.consts import S3_COMPILED_BUCKET\nfrom analysis_engine.consts import S3_KEY\nfrom analysis_engine.consts import REDIS_ADDRESS\nfrom analysis_engine.consts import REDIS_KEY\nfrom analysis_engine.consts import REDIS_PASSWORD\nfrom analysis_engine.consts import REDIS_DB\nfrom analysis_engine.consts import REDIS_EXPIRE\nfrom analysis_engine.consts import get_status\nfrom analysis_engine.consts import ppj\nfrom analysis_engine.consts import is_celery_disabled\n\n\n# Disable celery log hijacking\n# https://github.com/celery/celery/issues/2509\n@signals.setup_logging.connect\ndef setup_celery_logging(**kwargs):\n    pass\n\n\nlog = build_colorized_logger(\n    name=\'pub-tic-agg-s3-to-redis\',\n    log_config_path=LOG_CONFIG_PATH)\n\n\ndef publish_ticker_aggregate_from_s3():\n    """"""publish_ticker_aggregate_from_s3\n\n    Download all ticker data from S3 and publish it\'s contents\n    to Redis and back to S3\n\n    """"""\n\n    log.info(\n        \'start - publish_ticker_aggregate_from_s3\')\n\n    parser = argparse.ArgumentParser(\n        description=(\n            \'Download and aggregated all ticker data, \'\n            \'and store it in S3 and Redis. \'))\n    parser.add_argument(\n        \'-t\',\n        help=(\n            \'ticker\'),\n        required=True,\n        dest=\'ticker\')\n    parser.add_argument(\n        \'-i\',\n        help=(\n            \'optional - ticker id \'\n            \'not used without a database\'),\n        required=False,\n        dest=\'ticker_id\')\n    parser.add_argument(\n        \'-l\',\n        help=(\n            \'optional - path to the log config file\'),\n        required=False,\n        dest=\'log_config_path\')\n    parser.add_argument(\n        \'-b\',\n        help=(\n            \'optional - broker url for Celery\'),\n        required=False,\n        dest=\'broker_url\')\n    parser.add_argument(\n        \'-B\',\n        help=(\n            \'optional - backend url for Celery\'),\n        required=False,\n        dest=\'backend_url\')\n    parser.add_argument(\n        \'-k\',\n        help=(\n            \'optional - s3 access key\'),\n        required=False,\n        dest=\'s3_access_key\')\n    parser.add_argument(\n        \'-s\',\n        help=(\n            \'optional - s3 secret key\'),\n        required=False,\n        dest=\'s3_secret_key\')\n    parser.add_argument(\n        \'-a\',\n        help=(\n            \'optional - s3 address format: <host:port>\'),\n        required=False,\n        dest=\'s3_address\')\n    parser.add_argument(\n        \'-S\',\n        help=(\n            \'optional - s3 ssl or not\'),\n        required=False,\n        dest=\'s3_secure\')\n    parser.add_argument(\n        \'-u\',\n        help=(\n            \'optional - s3 bucket name\'),\n        required=False,\n        dest=\'s3_bucket_name\')\n    parser.add_argument(\n        \'-c\',\n        help=(\n            \'optional - s3 compiled bucket name\'),\n        required=False,\n        dest=\'s3_compiled_bucket_name\')\n    parser.add_argument(\n        \'-g\',\n        help=(\n            \'optional - s3 region name\'),\n        required=False,\n        dest=\'s3_region_name\')\n    parser.add_argument(\n        \'-p\',\n        help=(\n            \'optional - redis_password\'),\n        required=False,\n        dest=\'redis_password\')\n    parser.add_argument(\n        \'-r\',\n        help=(\n            \'optional - redis_address format: <host:port>\'),\n        required=False,\n        dest=\'redis_address\')\n    parser.add_argument(\n        \'-n\',\n        help=(\n            \'optional - redis and s3 key name\'),\n        required=False,\n        dest=\'keyname\')\n    parser.add_argument(\n        \'-m\',\n        help=(\n            \'optional - redis database number (0 by default)\'),\n        required=False,\n        dest=\'redis_db\')\n    parser.add_argument(\n        \'-x\',\n        help=(\n            \'optional - redis expiration in seconds\'),\n        required=False,\n        dest=\'redis_expire\')\n    parser.add_argument(\n        \'-d\',\n        help=(\n            \'debug\'),\n        required=False,\n        dest=\'debug\',\n        action=\'store_true\')\n    args = parser.parse_args()\n\n    ticker = TICKER\n    ticker_id = TICKER_ID\n    ssl_options = SSL_OPTIONS\n    transport_options = TRANSPORT_OPTIONS\n    broker_url = WORKER_BROKER_URL\n    backend_url = WORKER_BACKEND_URL\n    celery_config_module = WORKER_CELERY_CONFIG_MODULE\n    include_tasks = INCLUDE_TASKS\n    s3_access_key = S3_ACCESS_KEY\n    s3_secret_key = S3_SECRET_KEY\n    s3_region_name = S3_REGION_NAME\n    s3_address = S3_ADDRESS\n    s3_secure = S3_SECURE\n    s3_bucket_name = S3_BUCKET\n    s3_compiled_bucket_name = S3_COMPILED_BUCKET\n    s3_key = S3_KEY\n    redis_address = REDIS_ADDRESS\n    redis_key = REDIS_KEY\n    redis_password = REDIS_PASSWORD\n    redis_db = REDIS_DB\n    redis_expire = REDIS_EXPIRE\n    debug = False\n\n    if args.ticker:\n        ticker = args.ticker.upper()\n    if args.ticker_id:\n        ticker = args.ticker_id\n    if args.broker_url:\n        broker_url = args.broker_url\n    if args.backend_url:\n        backend_url = args.backend_url\n    if args.s3_access_key:\n        s3_access_key = args.s3_access_key\n    if args.s3_secret_key:\n        s3_secret_key = args.s3_secret_key\n    if args.s3_region_name:\n        s3_region_name = args.s3_region_name\n    if args.s3_address:\n        s3_address = args.s3_address\n    if args.s3_secure:\n        s3_secure = args.s3_secure\n    if args.s3_bucket_name:\n        s3_bucket_name = args.s3_bucket_name\n    if args.s3_compiled_bucket_name:\n        s3_compiled_bucket_name = args.s3_compiled_bucket_name\n    if args.keyname:\n        s3_key = args.keyname\n        redis_key = args.keyname\n    if args.redis_address:\n        redis_address = args.redis_address\n    if args.redis_password:\n        redis_password = args.redis_password\n    if args.redis_db:\n        redis_db = args.redis_db\n    if args.redis_expire:\n        redis_expire = args.redis_expire\n    if args.debug:\n        debug = True\n\n    work = build_publish_ticker_aggregate_from_s3_request()\n\n    work[\'ticker\'] = ticker\n    work[\'ticker_id\'] = ticker_id\n    work[\'s3_bucket\'] = s3_bucket_name\n    work[\'s3_compiled_bucket\'] = s3_compiled_bucket_name\n    if args.keyname:\n        work[\'s3_key\'] = s3_key\n        work[\'redis_key\'] = redis_key\n    work[\'s3_access_key\'] = s3_access_key\n    work[\'s3_secret_key\'] = s3_secret_key\n    work[\'s3_region_name\'] = s3_region_name\n    work[\'s3_address\'] = s3_address\n    work[\'s3_secure\'] = s3_secure\n    work[\'redis_address\'] = redis_address\n    work[\'redis_password\'] = redis_password\n    work[\'redis_db\'] = redis_db\n    work[\'redis_expire\'] = redis_expire\n    work[\'debug\'] = debug\n    work[\'label\'] = f\'ticker={ticker}\'\n\n    path_to_tasks = \'analysis_engine.work_tasks\'\n    task_name = (\n        f\'{path_to_tasks}.publish_ticker_aggregate_from_s3.\'\n        \'publish_ticker_aggregate_from_s3\')\n    task_res = None\n    if is_celery_disabled():\n        work[\'celery_disabled\'] = True\n        log.debug(\n            f\'starting without celery work={ppj(work)}\')\n        task_res = task_publisher.publish_ticker_aggregate_from_s3(\n            work_dict=work)\n        if debug:\n            log.info(\n                f\'done - result={ppj(task_res)} task={task_name} \'\n                f\'status={get_status(status=task_res[""status""])} \'\n                f\'err={task_res[""err""]} label={work[""label""]}\')\n        else:\n            log.info(\n                f\'done - result task={task_name} \'\n                f\'status={get_status(status=task_res[""status""])} \'\n                f\'err={task_res[""err""]} label={work[""label""]}\')\n        # if/else debug\n    else:\n        log.info(f\'connecting to broker={broker_url} backend={backend_url}\')\n\n        # Get the Celery app\n        app = get_celery_app(\n            name=__name__,\n            auth_url=broker_url,\n            backend_url=backend_url,\n            path_to_config_module=celery_config_module,\n            ssl_options=ssl_options,\n            transport_options=transport_options,\n            include_tasks=include_tasks)\n\n        log.info(f\'calling task={task_name} - work={ppj(work)}\')\n        job_id = app.send_task(\n            task_name,\n            (work,))\n        log.info(f\'calling task={task_name} - success job_id={job_id}\')\n    # end of if/else\n# end of publish_ticker_aggregate_from_s3\n\n\nif __name__ == \'__main__\':\n    publish_ticker_aggregate_from_s3()\n'"
analysis_engine/scripts/run_backtest_and_plot_history.py,0,"b'#!/usr/bin/env python\n\n""""""\nA tool for showing how to build an algorithm and\nrun a backtest with an algorithm config dictionary\n\n.. code-block:: python\n\n    import analysis_engine.consts as ae_consts\n    import analysis_engine.algo as base_algo\n    import analysis_engine.run_algo as run_algo\n\n    ticker = \'SPY\'\n\n    willr_close_path = (\n        \'analysis_engine/mocks/example_indicator_williamsr.py\')\n    willr_open_path = (\n        \'analysis_engine/mocks/example_indicator_williamsr_open.py\')\n    algo_config_dict = {\n        \'name\': \'min-runner\',\n        \'timeseries\': timeseries,\n        \'trade_horizon\': 5,\n        \'num_owned\': 10,\n        \'buy_shares\': 10,\n        \'balance\': 10000.0,\n        \'commission\': 6.0,\n        \'ticker\': ticker,\n        \'algo_module_path\': None,\n        \'algo_version\': 1,\n        \'verbose\': False,               # log in the algorithm\n        \'verbose_processor\': False,     # log in the indicator processor\n        \'verbose_indicators\': False,    # log all indicators\n        \'verbose_trading\': True,        # log in the algo trading methods\n        \'positions\': {\n            ticker: {\n                \'shares\': 10,\n                \'buys\': [],\n                \'sells\': []\n            }\n        },\n        \'buy_rules\': {\n            \'confidence\': 75,\n            \'min_indicators\': 3\n        },\n        \'sell_rules\': {\n            \'confidence\': 75,\n            \'min_indicators\': 3\n        },\n        \'indicators\': [\n            {\n                \'name\': \'willr_-70_-30\',\n                \'module_path\': willr_close_path,\n                \'category\': \'technical\',\n                \'type\': \'momentum\',\n                \'uses_data\': \'minute\',\n                \'high\': 0,\n                \'low\': 0,\n                \'close\': 0,\n                \'open\': 0,\n                \'willr_value\': 0,\n                \'num_points\': 80,\n                \'buy_below\': -70,\n                \'sell_above\': -30,\n                \'is_buy\': False,\n                \'is_sell\': False,\n                \'verbose\': False  # log in just this indicator\n            },\n            {\n                \'name\': \'willr_-80_-20\',\n                \'module_path\': willr_close_path,\n                \'category\': \'technical\',\n                \'type\': \'momentum\',\n                \'uses_data\': \'minute\',\n                \'high\': 0,\n                \'low\': 0,\n                \'close\': 0,\n                \'open\': 0,\n                \'willr_value\': 0,\n                \'num_points\': 30,\n                \'buy_below\': -80,\n                \'sell_above\': -20,\n                \'is_buy\': False,\n                \'is_sell\': False\n            },\n            {\n                \'name\': \'willr_-90_-10\',\n                \'module_path\': willr_close_path,\n                \'category\': \'technical\',\n                \'type\': \'momentum\',\n                \'uses_data\': \'minute\',\n                \'high\': 0,\n                \'low\': 0,\n                \'close\': 0,\n                \'open\': 0,\n                \'willr_value\': 0,\n                \'num_points\': 60,\n                \'buy_below\': -90,\n                \'sell_above\': -10,\n                \'is_buy\': False,\n                \'is_sell\': False\n            },\n            {\n                \'name\': \'willr_open_-80_-20\',\n                \'module_path\': willr_open_path,\n                \'category\': \'technical\',\n                \'type\': \'momentum\',\n                \'uses_data\': \'minute\',\n                \'high\': 0,\n                \'low\': 0,\n                \'close\': 0,\n                \'open\': 0,\n                \'willr_open_value\': 0,\n                \'num_points\': 80,\n                \'buy_below\': -80,\n                \'sell_above\': -20,\n                \'is_buy\': False,\n                \'is_sell\': False\n            }\n        ],\n        \'slack\': {\n            \'webhook\': None\n        }\n    }\n\n    class ExampleCustomAlgo(base_algo.BaseAlgo):\n        def process(self, algo_id, ticker, dataset):\n            if self.verbose:\n                print(\n                    f\'process start - {self.name} \'\n                    f\'date={self.backtest_date} minute={self.latest_min} \'\n                    f\'close={self.latest_close} high={self.latest_high} \'\n                    f\'low={self.latest_low} open={self.latest_open} \'\n                    f\'volume={self.latest_volume}\')\n        # end of process\n    # end of ExampleCustomAlgo\n\n\n    algo_obj = ExampleCustomAlgo(\n        ticker=algo_config_dict[\'ticker\'],\n        config_dict=algo_config_dict)\n\n    algo_res = run_algo.run_algo(\n        ticker=algo_config_dict[\'ticker\'],\n        algo=algo_obj,\n        raise_on_err=True)\n\n    if algo_res[\'status\'] != ae_consts.SUCCESS:\n        print(\n            \'failed running algo backtest \'\n            f\'{algo_obj.get_name()} hit status: \'\n            f\'{ae_consts.get_status(status=algo_res[\'status\'])} \'\n            f\'error: {algo_res[""err""]}\')\n    else:\n        print(\n            f\'backtest: {algo_obj.get_name()} \'\n            f\'{ae_consts.get_status(status=algo_res[""status""])} - \'\n            \'plotting history\')\n    # if not successful\n\n""""""\n\nimport os\nimport sys\nimport datetime\nimport argparse\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.algo as base_algo\nimport analysis_engine.run_algo as run_algo\nimport analysis_engine.plot_trading_history as plot_trading_history\nimport analysis_engine.build_publish_request as build_publish_request\nimport spylunking.log.setup_logging as log_utils\n\n\nlog = log_utils.build_colorized_logger(\n    name=\'bt\',\n    log_config_path=ae_consts.LOG_CONFIG_PATH)\n\n\ndef build_example_algo_config(\n        ticker,\n        timeseries=\'minute\'):\n    """"""build_example_algo_config\n\n    helper for building an algorithm config dictionary\n\n    :returns: algorithm config dictionary\n    """"""\n    willr_close_path = (\n        \'analysis_engine/mocks/example_indicator_williamsr.py\')\n    willr_open_path = (\n        \'analysis_engine/mocks/example_indicator_williamsr_open.py\')\n    algo_config_dict = {\n        \'name\': \'backtest\',\n        \'timeseries\': timeseries,\n        \'trade_horizon\': 5,\n        \'num_owned\': 10,\n        \'buy_shares\': 10,\n        \'balance\': 10000.0,\n        \'commission\': 6.0,\n        \'ticker\': ticker,\n        \'algo_module_path\': None,\n        \'algo_version\': 1,\n        \'verbose\': False,  # log in the algorithm\n        \'verbose_processor\': False,  # log in the indicator processor\n        \'verbose_indicators\': False,  # log all indicators\n        \'verbose_trading\': False,  # log in the algo trading methods\n        \'inspect_datasets\': False,  # log dataset metrics - slow\n        \'positions\': {\n            ticker: {\n                \'shares\': 10,\n                \'buys\': [],\n                \'sells\': []\n            }\n        },\n        \'buy_rules\': {\n            \'confidence\': 75,\n            \'min_indicators\': 3\n        },\n        \'sell_rules\': {\n            \'confidence\': 75,\n            \'min_indicators\': 3\n        },\n        \'indicators\': [\n            {\n                \'name\': \'willr_-70_-30\',\n                \'module_path\': willr_close_path,\n                \'category\': \'technical\',\n                \'type\': \'momentum\',\n                \'uses_data\': \'minute\',\n                \'high\': 0,\n                \'low\': 0,\n                \'close\': 0,\n                \'open\': 0,\n                \'willr_value\': 0,\n                \'num_points\': 80,\n                \'buy_below\': -70,\n                \'sell_above\': -30,\n                \'is_buy\': False,\n                \'is_sell\': False,\n                \'verbose\': False  # log in just this indicator\n            },\n            {\n                \'name\': \'willr_-80_-20\',\n                \'module_path\': willr_close_path,\n                \'category\': \'technical\',\n                \'type\': \'momentum\',\n                \'uses_data\': \'minute\',\n                \'high\': 0,\n                \'low\': 0,\n                \'close\': 0,\n                \'open\': 0,\n                \'willr_value\': 0,\n                \'num_points\': 30,\n                \'buy_below\': -80,\n                \'sell_above\': -20,\n                \'is_buy\': False,\n                \'is_sell\': False\n            },\n            {\n                \'name\': \'willr_-90_-10\',\n                \'module_path\': willr_close_path,\n                \'category\': \'technical\',\n                \'type\': \'momentum\',\n                \'uses_data\': \'minute\',\n                \'high\': 0,\n                \'low\': 0,\n                \'close\': 0,\n                \'open\': 0,\n                \'willr_value\': 0,\n                \'num_points\': 60,\n                \'buy_below\': -90,\n                \'sell_above\': -10,\n                \'is_buy\': False,\n                \'is_sell\': False\n            },\n            {\n                \'name\': \'willr_open_-80_-20\',\n                \'module_path\': willr_open_path,\n                \'category\': \'technical\',\n                \'type\': \'momentum\',\n                \'uses_data\': \'minute\',\n                \'high\': 0,\n                \'low\': 0,\n                \'close\': 0,\n                \'open\': 0,\n                \'willr_open_value\': 0,\n                \'num_points\': 80,\n                \'buy_below\': -80,\n                \'sell_above\': -20,\n                \'is_buy\': False,\n                \'is_sell\': False\n            }\n        ],\n        \'slack\': {\n            \'webhook\': None\n        }\n    }\n\n    return algo_config_dict\n# end of build_example_algo_config\n\n\nclass ExampleCustomAlgo(base_algo.BaseAlgo):\n    """"""ExampleCustomAlgo""""""\n\n    def process(self, algo_id, ticker, dataset):\n        """"""process\n\n        Run a custom algorithm after all the indicators\n        from the ``algo_config_dict`` have been processed and all\n        the number crunching is done. This allows the algorithm\n        class to focus on the high-level trade execution problems\n        like bid-ask spreads and opening the buy/sell trade orders.\n\n        **How does it work?**\n\n        The engine provides a data stream from the latest\n        pricing updates stored in redis. Once new data is\n        stored in redis, algorithms will be able to use\n        each ``dataset`` as a chance to evaluate buy and\n        sell decisions. These are your own custom logic\n        for trading based off what the indicators find\n        and any non-indicator data provided from within\n        the ``dataset`` dictionary.\n\n        **Dataset Dictionary Structure**\n\n        Here is what the ``dataset`` variable\n        looks like when your algorithm\'s ``process``\n        method is called (assuming you have redis running\n        with actual pricing data too):\n\n        .. code-block:: python\n\n            dataset = {\n                \'id\': dataset_id,\n                \'date\': date,\n                \'data\': {\n                    \'daily\': pd.DataFrame([]),\n                    \'minute\': pd.DataFrame([]),\n                    \'quote\': pd.DataFrame([]),\n                    \'stats\': pd.DataFrame([]),\n                    \'peers\': pd.DataFrame([]),\n                    \'news1\': pd.DataFrame([]),\n                    \'financials\': pd.DataFrame([]),\n                    \'earnings\': pd.DataFrame([]),\n                    \'dividends\': pd.DataFrame([]),\n                    \'calls\': pd.DataFrame([]),\n                    \'puts\': pd.DataFrame([]),\n                    \'pricing\': pd.DataFrame([]),\n                    \'news\': pd.DataFrame([])\n                }\n            }\n\n        .. tip:: you can also inspect these datasets by setting\n            the algorithm\'s config dictionary key\n            ``""inspect_datasets"": True``\n\n        :param algo_id: string - algo identifier label for debugging datasets\n            during specific dates\n        :param ticker: string - ticker\n        :param dataset: a dictionary of identifiers (for debugging) and\n            multiple pandas ``DataFrame`` objects.\n        """"""\n        if self.verbose:\n            log.info(\n                f\'process start - {self.name} balance={self.balance} \'\n                f\'date={self.backtest_date} minute={self.latest_min} \'\n                f\'close={self.latest_close} high={self.latest_high} \'\n                f\'low={self.latest_low} open={self.latest_open} \'\n                f\'volume={self.latest_volume}\')\n    # end of process\n\n# end of ExampleCustomAlgo\n\n\ndef run_backtest_and_plot_history(\n        config_dict):\n    """"""run_backtest_and_plot_history\n\n    Run a derived algorithm with an algorithm config dictionary\n\n    :param config_dict: algorithm config dictionary\n    """"""\n\n    log.debug(\'start - sa\')\n\n    parser = argparse.ArgumentParser(\n        description=(\n            \'stock analysis tool\'))\n    parser.add_argument(\n        \'-t\',\n        help=(\n            \'ticker\'),\n        required=True,\n        dest=\'ticker\')\n    parser.add_argument(\n        \'-e\',\n        help=(\n            \'file path to extract an \'\n            \'algorithm-ready datasets from redis\'),\n        required=False,\n        dest=\'algo_extract_loc\')\n    parser.add_argument(\n        \'-l\',\n        help=(\n            \'show dataset in this file\'),\n        required=False,\n        dest=\'show_from_file\')\n    parser.add_argument(\n        \'-H\',\n        help=(\n            \'show trading history dataset in this file\'),\n        required=False,\n        dest=\'show_history_from_file\')\n    parser.add_argument(\n        \'-E\',\n        help=(\n            \'show trading performance report dataset in this file\'),\n        required=False,\n        dest=\'show_report_from_file\')\n    parser.add_argument(\n        \'-L\',\n        help=(\n            \'restore an algorithm-ready dataset file back into redis\'),\n        required=False,\n        dest=\'restore_algo_file\')\n    parser.add_argument(\n        \'-f\',\n        help=(\n            \'save the trading history dataframe \'\n            \'to this file\'),\n        required=False,\n        dest=\'history_json_file\')\n    parser.add_argument(\n        \'-J\',\n        help=(\n            \'plot action - after preparing you can use: \'\n            \'-J show to open the image (good for debugging)\'),\n        required=False,\n        dest=\'plot_action\')\n    parser.add_argument(\n        \'-b\',\n        help=(\n            \'run a backtest using the dataset in \'\n            \'a file path/s3 key/redis key formats: \'\n            \'file:/opt/sa/tests/datasets/algo/SPY-latest.json or \'\n            \'s3://algoready/SPY-latest.json or \'\n            \'redis:SPY-latest\'),\n        required=False,\n        dest=\'backtest_loc\')\n    parser.add_argument(\n        \'-B\',\n        help=(\n            \'optional - broker url for Celery\'),\n        required=False,\n        dest=\'broker_url\')\n    parser.add_argument(\n        \'-C\',\n        help=(\n            \'optional - broker url for Celery\'),\n        required=False,\n        dest=\'backend_url\')\n    parser.add_argument(\n        \'-w\',\n        help=(\n            \'optional - flag for publishing an algorithm job \'\n            \'using Celery to the ae workers\'),\n        required=False,\n        dest=\'run_on_engine\',\n        action=\'store_true\')\n    parser.add_argument(\n        \'-k\',\n        help=(\n            \'optional - s3 access key\'),\n        required=False,\n        dest=\'s3_access_key\')\n    parser.add_argument(\n        \'-K\',\n        help=(\n            \'optional - s3 secret key\'),\n        required=False,\n        dest=\'s3_secret_key\')\n    parser.add_argument(\n        \'-a\',\n        help=(\n            \'optional - s3 address format: <host:port>\'),\n        required=False,\n        dest=\'s3_address\')\n    parser.add_argument(\n        \'-Z\',\n        help=(\n            \'optional - s3 secure: default False\'),\n        required=False,\n        dest=\'s3_secure\')\n    parser.add_argument(\n        \'-s\',\n        help=(\n            \'optional - start date: YYYY-MM-DD\'),\n        required=False,\n        dest=\'start_date\')\n    parser.add_argument(\n        \'-n\',\n        help=(\n            \'optional - end date: YYYY-MM-DD\'),\n        required=False,\n        dest=\'end_date\')\n    parser.add_argument(\n        \'-u\',\n        help=(\n            \'optional - s3 bucket name\'),\n        required=False,\n        dest=\'s3_bucket_name\')\n    parser.add_argument(\n        \'-G\',\n        help=(\n            \'optional - s3 region name\'),\n        required=False,\n        dest=\'s3_region_name\')\n    parser.add_argument(\n        \'-g\',\n        help=(\n            \'Path to a custom algorithm module file \'\n            \'on disk. This module must have a single \'\n            \'class that inherits from: \'\n            \'https://github.com/AlgoTraders/stock-analysis-engine/\'\n            \'blob/master/\'\n            \'analysis_engine/algo.py Additionally you \'\n            \'can find the Example-Minute-Algorithm here: \'\n            \'https://github.com/AlgoTraders/stock-analysis-engine/\'\n            \'blob/master/analysis_engine/mocks/\'\n            \'example_algo_minute.py\'),\n        required=False,\n        dest=\'run_algo_in_file\')\n    parser.add_argument(\n        \'-p\',\n        help=(\n            \'optional - s3 bucket/file for trading history\'),\n        required=False,\n        dest=\'algo_history_loc\')\n    parser.add_argument(\n        \'-o\',\n        help=(\n            \'optional - s3 bucket/file for trading performance report\'),\n        required=False,\n        dest=\'algo_report_loc\')\n    parser.add_argument(\n        \'-r\',\n        help=(\n            \'optional - redis_address format: <host:port>\'),\n        required=False,\n        dest=\'redis_address\')\n    parser.add_argument(\n        \'-R\',\n        help=(\n            \'optional - redis and s3 key name\'),\n        required=False,\n        dest=\'keyname\')\n    parser.add_argument(\n        \'-m\',\n        help=(\n            \'optional - redis database number (0 by default)\'),\n        required=False,\n        dest=\'redis_db\')\n    parser.add_argument(\n        \'-x\',\n        help=(\n            \'optional - redis expiration in seconds\'),\n        required=False,\n        dest=\'redis_expire\')\n    parser.add_argument(\n        \'-c\',\n        help=(\n            \'optional - algorithm config_file path for setting \'\n            \'up internal algorithm trading strategies and \'\n            \'indicators\'),\n        required=False,\n        dest=\'config_file\')\n    parser.add_argument(\n        \'-v\',\n        help=(\n            \'set the Algorithm to verbose logging\'),\n        required=False,\n        dest=\'verbose_algo\',\n        action=\'store_true\')\n    parser.add_argument(\n        \'-P\',\n        help=(\n            \'set the Algorithm\\\'s IndicatorProcessor to verbose logging\'),\n        required=False,\n        dest=\'verbose_processor\',\n        action=\'store_true\')\n    parser.add_argument(\n        \'-I\',\n        help=(\n            \'set all Algorithm\\\'s Indicators to verbose logging \'\n            \'(note indivdual indicators support a \\\'verbose\\\' key \'\n            \'that can be set to True to debug just one \'\n            \'indicator)\'),\n        required=False,\n        dest=\'verbose_indicators\',\n        action=\'store_true\')\n    parser.add_argument(\n        \'-V\',\n        help=(\n            \'inspect the datasets an algorithm is processing - this\'\n            \'will slow down processing to show debugging\'),\n        required=False,\n        dest=\'inspect_datasets\',\n        action=\'store_true\')\n    parser.add_argument(\n        \'-j\',\n        help=(\n            \'run the algorithm on just this specific date in the datasets \'\n            \'- specify the date in a format: YYYY-MM-DD like: 2018-11-29\'),\n        required=False,\n        dest=\'run_this_date\')\n    parser.add_argument(\n        \'-d\',\n        help=(\n            \'debug\'),\n        required=False,\n        dest=\'debug\',\n        action=\'store_true\')\n    args = parser.parse_args()\n\n    ticker = ae_consts.TICKER\n    use_balance = 10000.0\n    use_commission = 6.0\n    use_start_date = None\n    use_end_date = None\n    use_config_file = None\n    debug = False\n    verbose_algo = None\n    verbose_processor = None\n    verbose_indicators = None\n    inspect_datasets = None\n    history_json_file = None\n    run_this_date = None\n\n    s3_access_key = ae_consts.S3_ACCESS_KEY\n    s3_secret_key = ae_consts.S3_SECRET_KEY\n    s3_region_name = ae_consts.S3_REGION_NAME\n    s3_address = ae_consts.S3_ADDRESS\n    s3_secure = ae_consts.S3_SECURE\n    redis_address = ae_consts.REDIS_ADDRESS\n    redis_password = ae_consts.REDIS_PASSWORD\n    redis_db = ae_consts.REDIS_DB\n    redis_expire = ae_consts.REDIS_EXPIRE\n\n    if args.s3_access_key:\n        s3_access_key = args.s3_access_key\n    if args.s3_secret_key:\n        s3_secret_key = args.s3_secret_key\n    if args.s3_region_name:\n        s3_region_name = args.s3_region_name\n    if args.s3_address:\n        s3_address = args.s3_address\n    if args.s3_secure:\n        s3_secure = args.s3_secure\n    if args.redis_address:\n        redis_address = args.redis_address\n    if args.redis_db:\n        redis_db = args.redis_db\n    if args.redis_expire:\n        redis_expire = args.redis_expire\n    if args.history_json_file:\n        history_json_file = args.history_json_file\n    if args.ticker:\n        ticker = args.ticker.upper()\n    if args.debug:\n        debug = True\n    if args.verbose_algo:\n        verbose_algo = True\n    if args.verbose_processor:\n        verbose_processor = True\n    if args.verbose_indicators:\n        verbose_indicators = True\n    if args.inspect_datasets:\n        inspect_datasets = True\n    if args.run_this_date:\n        run_this_date = args.run_this_date\n\n    if args.start_date:\n        try:\n            use_start_date = f\'{str(args.start_date)} 00:00:00\'\n            datetime.datetime.strptime(\n                args.start_date,\n                ae_consts.COMMON_DATE_FORMAT)\n        except Exception as e:\n            msg = (\n                \'please use a start date formatted as: \'\n                f\'{ae_consts.COMMON_DATE_FORMAT}\\nerror was: {e}\')\n            log.error(msg)\n            sys.exit(1)\n        # end of testing for a valid date\n    # end of args.start_date\n    if args.end_date:\n        try:\n            use_end_date = f\'{str(args.end_date)} 00:00:00\'\n            datetime.datetime.strptime(\n                args.end_date,\n                ae_consts.COMMON_DATE_FORMAT)\n        except Exception as e:\n            msg = (\n                \'please use an end date formatted as: \'\n                f\'{ae_consts.COMMON_DATE_FORMAT}\\nerror was: {e}\')\n            log.error(msg)\n            sys.exit(1)\n        # end of testing for a valid date\n    # end of args.end_date\n    if args.config_file:\n        use_config_file = args.config_file\n        if not os.path.exists(use_config_file):\n            log.error(\n                f\'Failed: unable to find config file: -c {use_config_file}\')\n            sys.exit(1)\n\n    if args.backtest_loc:\n        backtest_loc = args.backtest_loc\n        if (\'file:/\' not in backtest_loc and\n                \'s3://\' not in backtest_loc and\n                \'redis://\' not in backtest_loc):\n            log.error(\n                \'invalid -b <backtest dataset file> specified. \'\n                f\'{backtest_loc} \'\n                \'please use either: \'\n                \'-b file:/opt/sa/tests/datasets/algo/SPY-latest.json or \'\n                \'-b s3://algoready/SPY-latest.json or \'\n                \'-b redis://SPY-latest\')\n            sys.exit(1)\n\n        load_from_s3_bucket = None\n        load_from_s3_key = None\n        load_from_redis_key = None\n        load_from_file = None\n\n        if \'s3://\' in backtest_loc:\n            load_from_s3_bucket = backtest_loc.split(\'/\')[-2]\n            load_from_s3_key = backtest_loc.split(\'/\')[-1]\n        elif \'redis://\' in backtest_loc:\n            load_from_redis_key = backtest_loc.split(\'/\')[-1]\n        elif \'file:/\' in backtest_loc:\n            load_from_file = backtest_loc.split(\':\')[-1]\n        # end of parsing supported transport - loading an algo-ready\n\n        load_config = build_publish_request.build_publish_request(\n            ticker=ticker,\n            output_file=load_from_file,\n            s3_bucket=load_from_s3_bucket,\n            s3_key=load_from_s3_key,\n            redis_key=load_from_redis_key,\n            redis_address=redis_address,\n            redis_db=redis_db,\n            redis_password=redis_password,\n            redis_expire=redis_expire,\n            s3_address=s3_address,\n            s3_access_key=s3_access_key,\n            s3_secret_key=s3_secret_key,\n            s3_region_name=s3_region_name,\n            s3_secure=s3_secure,\n            verbose=debug,\n            label=f\'load-{backtest_loc}\')\n        if load_from_file:\n            load_config[\'output_file\'] = load_from_file\n        if load_from_redis_key:\n            load_config[\'redis_key\'] = load_from_redis_key\n            load_config[\'redis_enabled\'] = True\n        if load_from_s3_bucket and load_from_s3_key:\n            load_config[\'s3_bucket\'] = load_from_s3_bucket\n            load_config[\'s3_key\'] = load_from_s3_key\n            load_config[\'s3_enabled\'] = True\n\n    if debug:\n        log.info(\'starting algo\')\n\n    config_dict[\'ticker\'] = ticker\n    config_dict[\'balance\'] = use_balance\n    config_dict[\'commission\'] = use_commission\n\n    if verbose_algo:\n        config_dict[\'verbose\'] = verbose_algo\n    if verbose_processor:\n        config_dict[\'verbose_processor\'] = verbose_processor\n    if verbose_indicators:\n        config_dict[\'verbose_indicators\'] = verbose_indicators\n    if inspect_datasets:\n        config_dict[\'inspect_datasets\'] = inspect_datasets\n    if run_this_date:\n        config_dict[\'run_this_date\'] = run_this_date\n\n    algo_obj = ExampleCustomAlgo(\n        ticker=config_dict[\'ticker\'],\n        config_dict=config_dict)\n\n    algo_res = run_algo.run_algo(\n        ticker=ticker,\n        algo=algo_obj,\n        start_date=use_start_date,\n        end_date=use_end_date,\n        raise_on_err=True)\n\n    if algo_res[\'status\'] != ae_consts.SUCCESS:\n        log.error(\n            \'failed running algo backtest \'\n            f\'{algo_obj.get_name()} hit status: \'\n            f\'{ae_consts.get_status(status=algo_res[""status""])} \'\n            f\'error: {algo_res[""err""]}\')\n        return\n    # if not successful\n\n    log.info(\n        f\'backtest: {algo_obj.get_name()} \'\n        f\'{ae_consts.get_status(status=algo_res[""status""])}\')\n\n    trading_history_dict = algo_obj.get_history_dataset()\n    history_df = trading_history_dict[ticker]\n    if not hasattr(history_df, \'to_json\'):\n        return\n\n    if history_json_file:\n        log.info(f\'saving history to: {history_json_file}\')\n        history_df.to_json(\n            history_json_file,\n            orient=\'records\',\n            date_format=\'iso\')\n\n    log.info(\'plotting history\')\n\n    use_xcol = \'date\'\n    use_as_date_format = \'%d\\n%b\'\n    xlabel = f\'Dates vs {trading_history_dict[""algo_name""]} values\'\n    ylabel = f\'Algo {trading_history_dict[""algo_name""]}\\nvalues\'\n    df_filter = (history_df[\'close\'] > 0.01)\n    first_date = history_df[df_filter][\'date\'].iloc[0]\n    end_date = history_df[df_filter][\'date\'].iloc[-1]\n    if config_dict[\'timeseries\'] == \'minute\':\n        use_xcol = \'minute\'\n        use_as_date_format = \'%d %H:%M:%S\\n%b\'\n        first_date = history_df[df_filter][\'minute\'].iloc[0]\n        end_date = history_df[df_filter][\'minute\'].iloc[-1]\n    title = (\n        f\'Trading History {ticker} for Algo \'\n        f\'{trading_history_dict[""algo_name""]}\\n\'\n        f\'Backtest dates from {first_date} to {end_date}\')\n\n    # set default hloc columns:\n    blue = None\n    green = None\n    orange = None\n\n    red = \'close\'\n    blue = \'balance\'\n\n    if debug:\n        for i, r in history_df.iterrows():\n            log.debug(f\'{r[""minute""]} - {r[""close""]}\')\n\n    plot_trading_history.plot_trading_history(\n        title=title,\n        df=history_df,\n        red=red,\n        blue=blue,\n        green=green,\n        orange=orange,\n        date_col=use_xcol,\n        date_format=use_as_date_format,\n        xlabel=xlabel,\n        ylabel=ylabel,\n        df_filter=df_filter,\n        show_plot=True,\n        dropna_for_all=True)\n\n# end of run_backtest_and_plot_history\n\n\ndef start_backtest_with_plot_history():\n    """"""start_backtest_with_plot_history\n\n    setup.py helper for kicking off a backtest\n    that will plot the trading history using\n    seaborn and matplotlib showing\n    the algorithm\'s balance vs the closing price\n    of the asset\n    """"""\n    run_backtest_and_plot_history(\n        config_dict=build_example_algo_config(\n            ticker=\'SPY\',\n            timeseries=\'minute\'))\n# end of start_backtest_with_plot_history\n\n\nif __name__ == \'__main__\':\n    start_backtest_with_plot_history()\n'"
analysis_engine/scripts/sa.py,0,"b'#!/usr/bin/env python\n\n""""""\n\n**Stock Analysis Command Line Tool**\n\n1) Get an algorithm-ready dataset\n\n- Fetch and extract algorithm-ready datasets\n- Optional - Preparing a dataset from s3 or redis. A prepared\n  dataset can be used for analysis.\n\n2) Run an algorithm using the cached datasets\n\n- Coming Soon - Analyze datasets and store output (generated csvs)\n  in s3 and redis.\n- Coming Soon - Make predictions using an analyzed dataset\n\n**Supported Actions**\n\n#.  **Algorithm-Ready** Datasets\n\n    Algo-ready datasets were created by the Algorithm Extraction API.\n\n    You can tune algorithm performance by deriving your own algorithm\n    from the `analysis_engine.algo.BaseAlgo <ht\n    tps://github.com/AlgoTraders/stock-analysis-engine/blob/master/\n    analysis_engine/algo.py>`__ and then loading the dataset from\n    s3, redis or a file by passing the correct arguments.\n\n    Command line actions:\n\n    - **Extract** algorithm-ready datasets out of redis to a file\n\n        ::\n\n            sa -t SPY -e ~/SPY-$(date +""%Y-%m-%d"").json\n\n    - **View** algorithm-ready datasets in a file\n\n        ::\n\n            sa -t SPY -l ~/SPY-$(date +""%Y-%m-%d"").json\n\n    - **Restore** algorithm-ready datasets from a file to redis\n\n        This also works as a backup tool for archiving an entire\n        single ticker dataset from redis to a single file. (zlib compression\n        is code-complete but has not been debugged end-to-end)\n\n        ::\n\n            sa -t SPY -L ~/SPY-$(date +""%Y-%m-%d"").json\n\n        .. warning:: if the output redis key or s3 key already exists, this\n            process will overwrite the previously stored values\n\n#.  **Run an Algorithm**\n\n    Please refer to the `included Minute Algorithm <https://github.com/Algo\n    Traders/stock-analysis-engine/blob/master/analysis_engine/mocks/e\n    xample_algo_minute.py>`__ for an up to date reference.\n\n    ::\n\n        sa -t SPY -g /opt/sa/analysis_engine/mocks/example_algo_minute.py\n\n""""""\n\nimport os\nimport sys\nimport datetime\nimport argparse\nimport celery\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.run_custom_algo as run_custom_algo\nimport analysis_engine.work_tasks.get_celery_app as get_celery_app\nimport analysis_engine.plot_trading_history as plot_trading_history\nimport analysis_engine.charts as ae_charts\nimport analysis_engine.iex.extract_df_from_redis as extract_utils\nimport analysis_engine.show_dataset as show_dataset\nimport analysis_engine.load_history_dataset_from_file as load_history\nimport analysis_engine.load_report_dataset_from_file as load_report\nimport analysis_engine.restore_dataset as restore_dataset\nimport analysis_engine.work_tasks.prepare_pricing_dataset as prep_dataset\nimport analysis_engine.api_requests as api_requests\nimport spylunking.log.setup_logging as log_utils\n\n\n# Disable celery log hijacking\n# https://github.com/celery/celery/issues/2509\n@celery.signals.setup_logging.connect\ndef setup_celery_logging(**kwargs):\n    pass\n\n\nlog = log_utils.build_colorized_logger(\n    name=\'sa\',\n    log_config_path=ae_consts.LOG_CONFIG_PATH)\n\n\ndef restore_missing_dataset_values_from_algo_ready_file(\n        ticker,\n        path_to_file,\n        redis_address,\n        redis_password,\n        redis_db=ae_consts.REDIS_DB,\n        output_redis_db=None,\n        compress=True,\n        encoding=\'utf-8\',\n        dataset_type=ae_consts.SA_DATASET_TYPE_ALGO_READY,\n        serialize_datasets=ae_consts.DEFAULT_SERIALIZED_DATASETS,\n        show_summary=True):\n    """"""restore_missing_dataset_values_from_algo_ready_file\n\n    restore missing dataset nodes in redis from an algorithm-ready\n    dataset file on disk - use this to restore redis from scratch\n\n    :param ticker: string ticker\n    :param path_to_file: string path to file on disk\n    :param redis_address: redis server endpoint adddress with\n        format ``host:port``\n    :param redis_password: optional - string password for redis\n    :param redis_db: redis db (default is ``REDIS_DB``)\n    :param output_redis_db: optional - integer for different\n        redis database (default is ``None``)\n    :param compress: contents in algorithm-ready file are\n        compressed (default is ``True``)\n    :param encoding: byte encoding of algorithm-ready file\n        (default is ``utf-8``)\n    :param dataset_type: optional - dataset type\n        (default is ``SA_DATASET_TYPE_ALGO_READY``)\n    :param serialize_datasets: optional - list of dataset names to\n        deserialize in the dataset\n    :param show_summary: optional - show a summary of the algorithm-ready\n        dataset using ``analysis_engine.show_dataset.show_dataset``\n        (default is ``True``)\n    """"""\n    if not os.path.exists(path_to_file):\n        log.error(f\'missing file={path_to_file} for restore\')\n        return\n\n    if dataset_type == ae_consts.SA_DATASET_TYPE_ALGO_READY:\n        log.info(f\'restore start - load dataset from file={path_to_file}\')\n    else:\n        log.error(\n            \'restore dataset unsupported \'\n            f\'type={dataset_type} for file={path_to_file}\')\n        return\n\n    if not output_redis_db:\n        output_redis_db = redis_db\n\n    restore_dataset.restore_dataset(\n        show_summary=show_summary,\n        path_to_file=path_to_file,\n        compress=compress,\n        encoding=encoding,\n        dataset_type=dataset_type,\n        serialize_datasets=serialize_datasets,\n        redis_address=redis_address,\n        redis_password=redis_password,\n        redis_db=redis_db,\n        redis_output_db=output_redis_db,\n        verbose=False)\n    log.info(f\'restore done - dataset in file={path_to_file}\')\n# end of restore_missing_dataset_values_from_algo_ready_file\n\n\ndef examine_dataset_in_file(\n        path_to_file,\n        compress=False,\n        encoding=\'utf-8\',\n        ticker=None,\n        dataset_type=ae_consts.SA_DATASET_TYPE_ALGO_READY,\n        serialize_datasets=ae_consts.DEFAULT_SERIALIZED_DATASETS,):\n    """"""examine_dataset_in_file\n\n    Show the internal dataset dictionary structure in dataset file\n\n    :param path_to_file: path to file\n    :param compress: optional - boolean flag for decompressing\n        the contents of the ``path_to_file`` if necessary\n        (default is ``False`` and algorithms\n        use ``zlib`` for compression)\n    :param encoding: optional - string for data encoding\n    :param ticker: optional - string ticker symbol\n        to verify is in the dataset\n    :param dataset_type: optional - dataset type\n        (default is ``SA_DATASET_TYPE_ALGO_READY``)\n    :param serialize_datasets: optional - list of dataset names to\n        deserialize in the dataset\n    """"""\n    if dataset_type == ae_consts.SA_DATASET_TYPE_ALGO_READY:\n        log.info(f\'show start - load dataset from file={path_to_file}\')\n        show_dataset.show_dataset(\n            path_to_file=path_to_file,\n            compress=compress,\n            encoding=encoding,\n            dataset_type=dataset_type,\n            serialize_datasets=serialize_datasets)\n        log.info(f\'show done - dataset in file={path_to_file}\')\n    elif dataset_type == ae_consts.SA_DATASET_TYPE_TRADING_HISTORY:\n        log.info(\n            \'load trading history dataset \'\n            f\'from file={path_to_file}\')\n        trading_history_dict = load_history.load_history_dataset_from_file(\n            path_to_file=path_to_file,\n            compress=compress,\n            encoding=encoding)\n        history_df = trading_history_dict[ticker]\n\n        first_date = history_df[\'date\'].iloc[0]\n        end_date = history_df[\'date\'].iloc[-1]\n        title = (\n            f\'Trading History {ticker} for Algo \'\n            f\'{trading_history_dict[""algo_name""]}\\n\'\n            f\'Backtest dates from {first_date} to {end_date}\')\n        xcol = \'date\'\n        xlabel = f\'Dates vs {trading_history_dict[""algo_name""]} values\'\n        ylabel = (\n            \'Algo Values from columns:\\n\'\n            f\'{list(history_df.columns.values)}\')\n        df_filter = (history_df[\'close\'] > 0.01)\n\n        # set default hloc columns:\n        red = \'close\'\n        blue = \'low\'\n        green = \'high\'\n        orange = \'open\'\n\n        log.info(\n            \'available columns to plot in dataset: \'\n            f\'{ae_consts.ppj(list(history_df.columns.values))}\')\n\n        plot_trading_history.plot_trading_history(\n            title=title,\n            df=history_df,\n            red=red,\n            blue=blue,\n            green=green,\n            orange=orange,\n            date_col=xcol,\n            xlabel=xlabel,\n            ylabel=ylabel,\n            df_filter=df_filter,\n            show_plot=True,\n            dropna_for_all=False)\n    elif dataset_type == ae_consts.SA_DATASET_TYPE_TRADING_REPORT:\n        log.info(\n            \'load trading performance report dataset \'\n            f\'from file={path_to_file}\')\n        trading_report_dict = load_report.load_report_dataset_from_file(\n            path_to_file=path_to_file,\n            compress=compress,\n            encoding=encoding)\n        print(trading_report_dict)\n    else:\n        log.error(\n            f\'show unsupported dataset type={dataset_type} \'\n            f\'for file={path_to_file}\')\n        return\n# end of examine_dataset_in_file\n\n\ndef run_sa_tool():\n    """"""run_sa_tool\n\n    Run buy and sell analysis on a stock to send alerts to subscribed\n    users\n\n    """"""\n\n    log.debug(\'start - sa\')\n\n    parser = argparse.ArgumentParser(\n        description=(\n            \'stock analysis tool\'))\n    parser.add_argument(\n        \'-t\',\n        help=(\n            \'ticker\'),\n        required=True,\n        dest=\'ticker\')\n    parser.add_argument(\n        \'-e\',\n        help=(\n            \'file path to extract an \'\n            \'algorithm-ready datasets from redis\'),\n        required=False,\n        dest=\'algo_extract_loc\')\n    parser.add_argument(\n        \'-l\',\n        help=(\n            \'show dataset in this file\'),\n        required=False,\n        dest=\'show_from_file\')\n    parser.add_argument(\n        \'-H\',\n        help=(\n            \'show trading history dataset in this file\'),\n        required=False,\n        dest=\'show_history_from_file\')\n    parser.add_argument(\n        \'-E\',\n        help=(\n            \'show trading performance report dataset in this file\'),\n        required=False,\n        dest=\'show_report_from_file\')\n    parser.add_argument(\n        \'-L\',\n        help=(\n            \'restore an algorithm-ready dataset file back into redis\'),\n        required=False,\n        dest=\'restore_algo_file\')\n    parser.add_argument(\n        \'-f\',\n        help=(\n            \'run in mode: prepare dataset from \'\n            \'redis key or s3 key\'),\n        required=False,\n        dest=\'prepare_mode\',\n        action=\'store_true\')\n    parser.add_argument(\n        \'-J\',\n        help=(\n            \'plot action - after preparing you can use: \'\n            \'-J show to open the image (good for debugging)\'),\n        required=False,\n        dest=\'plot_action\')\n    parser.add_argument(\n        \'-b\',\n        help=(\n            \'run a backtest using the dataset in \'\n            \'a file path/s3 key/redis key formats: \'\n            \'file:/opt/sa/tests/datasets/algo/SPY-latest.json or \'\n            \'s3://algoready/SPY-latest.json or \'\n            \'redis:SPY-latest\'),\n        required=False,\n        dest=\'backtest_loc\')\n    parser.add_argument(\n        \'-B\',\n        help=(\n            \'optional - broker url for Celery\'),\n        required=False,\n        dest=\'broker_url\')\n    parser.add_argument(\n        \'-C\',\n        help=(\n            \'optional - broker url for Celery\'),\n        required=False,\n        dest=\'backend_url\')\n    parser.add_argument(\n        \'-w\',\n        help=(\n            \'optional - flag for publishing an algorithm job \'\n            \'using Celery to the ae workers\'),\n        required=False,\n        dest=\'run_on_engine\',\n        action=\'store_true\')\n    parser.add_argument(\n        \'-k\',\n        help=(\n            \'optional - s3 access key\'),\n        required=False,\n        dest=\'s3_access_key\')\n    parser.add_argument(\n        \'-K\',\n        help=(\n            \'optional - s3 secret key\'),\n        required=False,\n        dest=\'s3_secret_key\')\n    parser.add_argument(\n        \'-a\',\n        help=(\n            \'optional - s3 address format: <host:port>\'),\n        required=False,\n        dest=\'s3_address\')\n    parser.add_argument(\n        \'-Z\',\n        help=(\n            \'optional - s3 secure: default False\'),\n        required=False,\n        dest=\'s3_secure\')\n    parser.add_argument(\n        \'-s\',\n        help=(\n            \'optional - start date: YYYY-MM-DD\'),\n        required=False,\n        dest=\'start_date\')\n    parser.add_argument(\n        \'-n\',\n        help=(\n            \'optional - end date: YYYY-MM-DD\'),\n        required=False,\n        dest=\'end_date\')\n    parser.add_argument(\n        \'-u\',\n        help=(\n            \'optional - s3 bucket name\'),\n        required=False,\n        dest=\'s3_bucket_name\')\n    parser.add_argument(\n        \'-G\',\n        help=(\n            \'optional - s3 region name\'),\n        required=False,\n        dest=\'s3_region_name\')\n    parser.add_argument(\n        \'-g\',\n        help=(\n            \'Path to a custom algorithm module file \'\n            \'on disk. This module must have a single \'\n            \'class that inherits from: \'\n            \'https://github.com/AlgoTraders/stock-analysis-engine/\'\n            \'blob/master/\'\n            \'analysis_engine/algo.py Additionally you \'\n            \'can find the Example-Minute-Algorithm here: \'\n            \'https://github.com/AlgoTraders/stock-analysis-engine/\'\n            \'blob/master/analysis_engine/mocks/\'\n            \'example_algo_minute.py\'),\n        required=False,\n        dest=\'run_algo_in_file\')\n    parser.add_argument(\n        \'-p\',\n        help=(\n            \'optional - s3 bucket/file for trading history\'),\n        required=False,\n        dest=\'algo_history_loc\')\n    parser.add_argument(\n        \'-o\',\n        help=(\n            \'optional - s3 bucket/file for trading performance report\'),\n        required=False,\n        dest=\'algo_report_loc\')\n    parser.add_argument(\n        \'-r\',\n        help=(\n            \'optional - redis_address format: <host:port>\'),\n        required=False,\n        dest=\'redis_address\')\n    parser.add_argument(\n        \'-R\',\n        help=(\n            \'optional - redis and s3 key name\'),\n        required=False,\n        dest=\'keyname\')\n    parser.add_argument(\n        \'-m\',\n        help=(\n            \'optional - redis database number (0 by default)\'),\n        required=False,\n        dest=\'redis_db\')\n    parser.add_argument(\n        \'-x\',\n        help=(\n            \'optional - redis expiration in seconds\'),\n        required=False,\n        dest=\'redis_expire\')\n    parser.add_argument(\n        \'-z\',\n        help=(\n            \'optional - strike price\'),\n        required=False,\n        dest=\'strike\')\n    parser.add_argument(\n        \'-c\',\n        help=(\n            \'optional - algorithm config_file path for setting \'\n            \'up internal algorithm trading strategies and \'\n            \'indicators\'),\n        required=False,\n        dest=\'config_file\')\n    parser.add_argument(\n        \'-P\',\n        help=(\n            \'optional - get pricing data if ""1"" or ""0"" disabled\'),\n        required=False,\n        dest=\'get_pricing\')\n    parser.add_argument(\n        \'-N\',\n        help=(\n            \'optional - get news data if ""1"" or ""0"" disabled\'),\n        required=False,\n        dest=\'get_news\')\n    parser.add_argument(\n        \'-O\',\n        help=(\n            \'optional - get options data if ""1"" or ""0"" disabled\'),\n        required=False,\n        dest=\'get_options\')\n    parser.add_argument(\n        \'-i\',\n        help=(\n            \'optional - ignore column names (comma separated)\'),\n        required=False,\n        dest=\'ignore_columns\')\n    parser.add_argument(\n        \'-d\',\n        help=(\n            \'debug\'),\n        required=False,\n        dest=\'debug\',\n        action=\'store_true\')\n    args = parser.parse_args()\n\n    mode = \'prepare\'\n    plot_action = ae_consts.PLOT_ACTION_SHOW\n    ticker = ae_consts.TICKER\n    ticker_id = ae_consts.TICKER_ID\n    ssl_options = ae_consts.SSL_OPTIONS\n    transport_options = ae_consts.TRANSPORT_OPTIONS\n    broker_url = ae_consts.WORKER_BROKER_URL\n    backend_url = ae_consts.WORKER_BACKEND_URL\n    path_to_config_module = ae_consts.WORKER_CELERY_CONFIG_MODULE\n    include_tasks = ae_consts.INCLUDE_TASKS\n    s3_access_key = ae_consts.S3_ACCESS_KEY\n    s3_secret_key = ae_consts.S3_SECRET_KEY\n    s3_region_name = ae_consts.S3_REGION_NAME\n    s3_address = ae_consts.S3_ADDRESS\n    s3_secure = ae_consts.S3_SECURE\n    s3_bucket_name = ae_consts.S3_BUCKET\n    s3_key = ae_consts.S3_KEY\n    redis_address = ae_consts.REDIS_ADDRESS\n    redis_key = ae_consts.REDIS_KEY\n    redis_password = ae_consts.REDIS_PASSWORD\n    redis_db = ae_consts.REDIS_DB\n    redis_expire = ae_consts.REDIS_EXPIRE\n    dataset_type = ae_consts.SA_DATASET_TYPE_ALGO_READY\n    serialize_datasets = ae_consts.DEFAULT_SERIALIZED_DATASETS\n    output_redis_key = None\n    output_s3_bucket = None\n    output_s3_key = None\n    ignore_columns = None\n    compress = False\n    encoding = \'utf-8\'\n    slack_enabled = False\n    slack_code_block = False\n    slack_full_width = False\n    verbose = False\n    debug = False\n\n    redis_serializer = \'json\'\n    redis_encoding = \'utf-8\'\n    output_redis_key = None\n    output_s3_bucket = None\n    output_s3_key = None\n    s3_enabled = True\n    redis_enabled = True\n    ignore_columns = None\n    debug = False\n\n    run_on_engine = False\n    show_from_file = None\n    show_history_from_file = None\n    show_report_from_file = None\n    restore_algo_file = None\n    backtest_loc = None\n    use_custom_algo = False\n    algo_history_loc = \'s3://algohistory\'\n    algo_report_loc = \'s3://algoreport\'\n    algo_extract_loc = \'s3://algoready\'\n\n    use_balance = 5000.0\n    use_commission = 6.0\n    auto_fill = True\n    use_start_date = None\n    use_end_date = None\n    use_config_file = None\n    use_name = \'myalgo\'\n\n    if args.ticker:\n        ticker = args.ticker.upper()\n    if args.broker_url:\n        broker_url = args.broker_url\n    if args.backend_url:\n        backend_url = args.backend_url\n    if args.s3_access_key:\n        s3_access_key = args.s3_access_key\n    if args.s3_secret_key:\n        s3_secret_key = args.s3_secret_key\n    if args.s3_region_name:\n        s3_region_name = args.s3_region_name\n    if args.s3_address:\n        s3_address = args.s3_address\n        s3_enabled = True\n    if args.s3_secure:\n        s3_secure = args.s3_secure\n    if args.s3_bucket_name:\n        s3_bucket_name = args.s3_bucket_name\n    if args.keyname:\n        s3_key = args.keyname\n        redis_key = args.keyname\n    if args.redis_address:\n        redis_address = args.redis_address\n    if args.redis_db:\n        redis_db = args.redis_db\n    if args.redis_expire:\n        redis_expire = args.redis_expire\n    if args.prepare_mode:\n        mode = ae_consts.SA_MODE_PREPARE\n    if args.ignore_columns:\n        ignore_columns_org = args.ignore_columns\n        ignore_columns = ignore_columns_org.split("","")\n    if args.plot_action:\n        if str(args.plot_action).lower() == \'show\':\n            plot_action = ae_consts.PLOT_ACTION_SHOW\n        elif str(args.plot_action).lower() == \'s3\':\n            plot_action = ae_consts.PLOT_ACTION_SAVE_TO_S3\n        elif str(args.plot_action).lower() == \'save\':\n            plot_action = ae_consts.PLOT_ACTION_SAVE_AS_FILE\n        else:\n            plot_action = ae_consts.PLOT_ACTION_SHOW\n            log.warning(f\'unsupported plot_action: {args.plot_action}\')\n\n    if args.debug:\n        debug = True\n\n    if args.algo_extract_loc:\n        mode = ae_consts.SA_MODE_EXTRACT\n    if args.show_from_file:\n        show_from_file = args.show_from_file\n        mode = ae_consts.SA_MODE_SHOW_DATASET\n    if args.show_history_from_file:\n        show_history_from_file = args.show_history_from_file\n        mode = ae_consts.SA_MODE_SHOW_HISTORY_DATASET\n    if args.show_report_from_file:\n        show_report_from_file = args.show_report_from_file\n        mode = ae_consts.SA_MODE_SHOW_REPORT_DATASET\n    if args.restore_algo_file:\n        restore_algo_file = args.restore_algo_file\n        mode = ae_consts.SA_MODE_RESTORE_REDIS_DATASET\n    if args.run_algo_in_file:\n        mode = ae_consts.SA_MODE_RUN_ALGO\n    if args.backtest_loc:\n        mode = ae_consts.SA_MODE_RUN_ALGO\n    if args.start_date:\n        try:\n            use_start_date = f\'{str(args.start_date)} 00:00:00\'\n            datetime.datetime.strptime(\n                args.start_date,\n                ae_consts.COMMON_DATE_FORMAT)\n        except Exception as e:\n            msg = (\n                \'please use a start date formatted as: \'\n                f\'{ae_consts.COMMON_DATE_FORMAT}\\n\'\n                f\'error was: {e}\')\n            log.error(msg)\n            sys.exit(1)\n        # end of testing for a valid date\n    # end of args.start_date\n    if args.end_date:\n        try:\n            use_end_date = f\'{str(args.end_date)} 00:00:00\'\n            datetime.datetime.strptime(\n                args.end_date,\n                ae_consts.COMMON_DATE_FORMAT)\n        except Exception as e:\n            msg = (\n                \'please use an end date formatted as: \'\n                f\'{ae_consts.COMMON_DATE_FORMAT}\\n\'\n                f\'error was: {e}\')\n            log.error(msg)\n            sys.exit(1)\n        # end of testing for a valid date\n    # end of args.end_date\n    if args.config_file:\n        use_config_file = args.config_file\n        if not os.path.exists(use_config_file):\n            log.error(\n                f\'Failed: unable to find config file: -c {use_config_file}\')\n            sys.exit(1)\n\n    config_dict = None\n    load_from_s3_bucket = None\n    load_from_s3_key = None\n    load_from_redis_key = None\n    load_from_file = None\n    load_compress = False\n    load_publish = True\n    load_config = None\n    report_redis_key = None\n    report_s3_bucket = None\n    report_s3_key = None\n    report_file = None\n    report_compress = False\n    report_publish = True\n    report_config = None\n    history_redis_key = None\n    history_s3_bucket = None\n    history_s3_key = None\n    history_file = None\n    history_compress = False\n    history_publish = True\n    history_config = None\n    extract_redis_key = None\n    extract_s3_bucket = None\n    extract_s3_key = None\n    extract_file = None\n    extract_save_dir = None\n    extract_compress = False\n    extract_publish = True\n    extract_config = None\n    publish_to_slack = False\n    publish_to_s3 = True\n    publish_to_redis = True\n    use_timeseries = \'day\'\n    use_trade_strategy = \'count\'\n\n    valid = False\n    required_task = False\n    work = None\n    task_name = None\n    work = {}\n    path_to_tasks = \'analysis_engine.work_tasks\'\n    if mode == ae_consts.SA_MODE_PREPARE:\n        task_name = (\n            f\'{path_to_tasks}.\'\n            \'prepare_pricing_dataset.prepare_pricing_dataset\')\n        work = api_requests.build_prepare_dataset_request()\n        if output_s3_key:\n            work[\'prepared_s3_key\'] = output_s3_key\n        if output_s3_bucket:\n            work[\'prepared_s3_bucket\'] = output_s3_bucket\n        if output_redis_key:\n            work[\'prepared_redis_key\'] = output_redis_key\n        work[\'ignore_columns\'] = ignore_columns\n        valid = True\n        required_task = True\n    elif mode == ae_consts.SA_MODE_EXTRACT:\n        if args.algo_extract_loc:\n            algo_extract_loc = args.algo_extract_loc\n            if (\'file:/\' not in algo_extract_loc and\n                    \'s3://\' not in algo_extract_loc and\n                    \'redis://\' not in algo_extract_loc):\n                log.error(\n                    \'invalid -e <extract_to_file_or_s3_key_or_redis_key> \'\n                    \'specified. please use either: \'\n                    \'-e file:/opt/sa/tests/datasets/algo/SPY-latest.json or \'\n                    \'-e s3://algoready/SPY-latest.json or \'\n                    \'-e redis://SPY-latest\')\n                sys.exit(1)\n            if \'s3://\' in algo_extract_loc:\n                extract_s3_bucket = algo_extract_loc.split(\'/\')[-2]\n                extract_s3_key = algo_extract_loc.split(\'/\')[-1]\n            elif \'redis://\' in algo_extract_loc:\n                extract_redis_key = algo_extract_loc.split(\'/\')[-1]\n            elif \'file:/\' in algo_extract_loc:\n                extract_file = algo_extract_loc.split(\':\')[-1]\n        # end of parsing supported transport for loading\n\n        use_custom_algo = True\n    elif mode == ae_consts.SA_MODE_SHOW_DATASET:\n        examine_dataset_in_file(\n            ticker=ticker,\n            path_to_file=show_from_file)\n        log.info(\n            f\'done showing {ticker} dataset from file={show_from_file}\')\n        sys.exit(0)\n    elif mode == ae_consts.SA_MODE_SHOW_HISTORY_DATASET:\n        examine_dataset_in_file(\n            ticker=ticker,\n            dataset_type=ae_consts.SA_DATASET_TYPE_TRADING_HISTORY,\n            path_to_file=show_history_from_file)\n        log.info(\n            f\'done showing trading history {ticker} dataset from \'\n            f\'file={show_from_file}\')\n        sys.exit(0)\n    elif mode == ae_consts.SA_MODE_SHOW_REPORT_DATASET:\n        examine_dataset_in_file(\n            ticker=ticker,\n            dataset_type=ae_consts.SA_DATASET_TYPE_TRADING_REPORT,\n            path_to_file=show_report_from_file)\n        log.info(\n            f\'done showing trading performance report {ticker} dataset from \'\n            f\'file={show_from_file}\')\n        sys.exit(0)\n    elif mode == ae_consts.SA_MODE_RESTORE_REDIS_DATASET:\n        restore_missing_dataset_values_from_algo_ready_file(\n            ticker=ticker,\n            path_to_file=restore_algo_file,\n            redis_address=redis_address,\n            redis_password=redis_password,\n            redis_db=redis_db,\n            output_redis_db=redis_db,\n            dataset_type=ae_consts.SA_DATASET_TYPE_ALGO_READY,\n            serialize_datasets=ae_consts.DEFAULT_SERIALIZED_DATASETS)\n        log.info(\n            f\'done restoring {ticker} dataset from file={restore_algo_file} \'\n            f\'into redis_db={redis_db}\')\n        sys.exit(0)\n    elif mode == ae_consts.SA_MODE_RUN_ALGO:\n        if args.run_algo_in_file:\n            if not os.path.exists(args.run_algo_in_file):\n                log.error(\n                    f\'missing algorithm module file: {args.run_algo_in_file}\')\n                sys.exit(1)\n\n        if args.backtest_loc:\n            backtest_loc = args.backtest_loc\n            if (\'file:/\' not in backtest_loc and\n                    \'s3://\' not in backtest_loc and\n                    \'redis://\' not in backtest_loc):\n                log.error(\n                    \'invalid -b <backtest dataset file> specified. \'\n                    f\'{backtest_loc} \'\n                    \'please use either: \'\n                    \'-b file:/opt/sa/tests/datasets/algo/SPY-latest.json or \'\n                    \'-b s3://algoready/SPY-latest.json or \'\n                    \'-b redis://SPY-latest\')\n                sys.exit(1)\n            if \'s3://\' in backtest_loc:\n                load_from_s3_bucket = backtest_loc.split(\'/\')[-2]\n                load_from_s3_key = backtest_loc.split(\'/\')[-1]\n            elif \'redis://\' in backtest_loc:\n                load_from_redis_key = backtest_loc.split(\'/\')[-1]\n            elif \'file:/\' in backtest_loc:\n                load_from_file = backtest_loc.split(\':\')[-1]\n            load_publish = True\n        # end of parsing supported transport - loading an algo-ready\n\n        if args.algo_history_loc:\n            algo_history_loc = args.algo_history_loc\n            if (\'file:/\' not in algo_history_loc and\n                    \'s3://\' not in algo_history_loc and\n                    \'redis://\' not in algo_history_loc):\n                log.error(\n                    \'invalid -p <backtest dataset file> specified. \'\n                    f\'{algo_history_loc} \'\n                    \'please use either: \'\n                    \'-p file:/opt/sa/tests/datasets/algo/SPY-latest.json or \'\n                    \'-p s3://algoready/SPY-latest.json or \'\n                    \'-p redis://SPY-latest\')\n                sys.exit(1)\n            if \'s3://\' in algo_history_loc:\n                history_s3_bucket = algo_history_loc.split(\'/\')[-2]\n                history_s3_key = algo_history_loc.split(\'/\')[-1]\n            elif \'redis://\' in algo_history_loc:\n                history_redis_key = algo_history_loc.split(\'/\')[-1]\n            elif \'file:/\' in algo_history_loc:\n                history_file = algo_history_loc.split(\':\')[-1]\n            history_publish = True\n        # end of parsing supported transport - trading history\n\n        if args.algo_report_loc:\n            algo_report_loc = args.algo_report_loc\n            if (\'file:/\' not in algo_report_loc and\n                    \'s3://\' not in algo_report_loc and\n                    \'redis://\' not in algo_report_loc):\n                log.error(\n                    \'invalid -o <backtest dataset file> specified. \'\n                    f\'{algo_report_loc} \'\n                    \'please use either: \'\n                    \'-o file:/opt/sa/tests/datasets/algo/SPY-latest.json or \'\n                    \'-o s3://algoready/SPY-latest.json or \'\n                    \'-o redis://SPY-latest\')\n                sys.exit(1)\n            if \'s3://\' in algo_report_loc:\n                report_s3_bucket = algo_report_loc.split(\'/\')[-2]\n                report_s3_key = algo_report_loc.split(\'/\')[-1]\n            elif \'redis://\' in algo_report_loc:\n                report_redis_key = algo_report_loc.split(\'/\')[-1]\n            elif \'file:/\' in algo_report_loc:\n                report_file = algo_report_loc.split(\':\')[-1]\n            report_publish = True\n        # end of parsing supported transport - trading performance report\n\n        if args.algo_extract_loc:\n            algo_extract_loc = args.algo_extract_loc\n            if (\'file:/\' not in algo_extract_loc and\n                    \'s3://\' not in algo_extract_loc and\n                    \'redis://\' not in algo_extract_loc):\n                log.error(\n                    \'invalid -e <backtest dataset file> specified. \'\n                    f\'{algo_extract_loc} \'\n                    \'please use either: \'\n                    \'-e file:/opt/sa/tests/datasets/algo/SPY-latest.json or \'\n                    \'-e s3://algoready/SPY-latest.json or \'\n                    \'-e redis://SPY-latest\')\n                sys.exit(1)\n            if \'s3://\' in algo_extract_loc:\n                extract_s3_bucket = algo_extract_loc.split(\'/\')[-2]\n                extract_s3_key = algo_extract_loc.split(\'/\')[-1]\n            elif \'redis://\' in algo_extract_loc:\n                extract_redis_key = algo_extract_loc.split(\'/\')[-1]\n            elif \'file:/\' in algo_extract_loc:\n                extract_file = algo_extract_loc.split(\':\')[-1]\n            extract_publish = True\n        # end of parsing supported transport - extract algorithm-ready\n\n        use_custom_algo = True\n    # end of set up for backtest\n\n    if use_custom_algo:\n\n        if args.run_on_engine:\n            run_on_engine = True\n            log.info(\'starting algo on the engine\')\n        else:\n            log.info(\'starting algo\')\n\n        algo_res = run_custom_algo.run_custom_algo(\n            mod_path=args.run_algo_in_file,\n            ticker=ticker,\n            balance=use_balance,\n            commission=use_commission,\n            start_date=use_start_date,\n            end_date=use_end_date,\n            config_file=use_config_file,\n            name=use_name,\n            auto_fill=auto_fill,\n            config_dict=config_dict,\n            load_from_s3_bucket=load_from_s3_bucket,\n            load_from_s3_key=load_from_s3_key,\n            load_from_redis_key=load_from_redis_key,\n            load_from_file=load_from_file,\n            load_compress=load_compress,\n            load_publish=load_publish,\n            load_config=load_config,\n            report_redis_key=report_redis_key,\n            report_s3_bucket=report_s3_bucket,\n            report_s3_key=report_s3_key,\n            report_file=report_file,\n            report_compress=report_compress,\n            report_publish=report_publish,\n            report_config=report_config,\n            history_redis_key=history_redis_key,\n            history_s3_bucket=history_s3_bucket,\n            history_s3_key=history_s3_key,\n            history_file=history_file,\n            history_compress=history_compress,\n            history_publish=history_publish,\n            history_config=history_config,\n            extract_redis_key=extract_redis_key,\n            extract_s3_bucket=extract_s3_bucket,\n            extract_s3_key=extract_s3_key,\n            extract_file=extract_file,\n            extract_save_dir=extract_save_dir,\n            extract_compress=extract_compress,\n            extract_publish=extract_publish,\n            extract_config=extract_config,\n            publish_to_slack=publish_to_slack,\n            publish_to_s3=publish_to_s3,\n            publish_to_redis=publish_to_redis,\n            dataset_type=dataset_type,\n            serialize_datasets=serialize_datasets,\n            compress=compress,\n            encoding=encoding,\n            redis_enabled=redis_enabled,\n            redis_key=redis_key,\n            redis_address=redis_address,\n            redis_db=redis_db,\n            redis_password=redis_password,\n            redis_expire=redis_expire,\n            redis_serializer=redis_serializer,\n            redis_encoding=redis_encoding,\n            s3_enabled=s3_enabled,\n            s3_key=s3_key,\n            s3_address=s3_address,\n            s3_bucket=s3_bucket_name,\n            s3_access_key=s3_access_key,\n            s3_secret_key=s3_secret_key,\n            s3_region_name=s3_region_name,\n            s3_secure=s3_secure,\n            slack_enabled=slack_enabled,\n            slack_code_block=slack_code_block,\n            slack_full_width=slack_full_width,\n            dataset_publish_extract=extract_publish,\n            dataset_publish_history=history_publish,\n            dataset_publish_report=report_publish,\n            run_on_engine=run_on_engine,\n            auth_url=broker_url,\n            backend_url=backend_url,\n            include_tasks=include_tasks,\n            ssl_options=ssl_options,\n            transport_options=transport_options,\n            path_to_config_module=path_to_config_module,\n            timeseries=use_timeseries,\n            trade_strategy=use_trade_strategy,\n            verbose=verbose)\n\n        show_label = f\'algo.name={use_name}\'\n        show_extract = f\'{algo_extract_loc}\'\n        show_history = f\'{algo_history_loc}\'\n        show_report = f\'{algo_report_loc}\'\n        base_label = (\n            f\'load={args.run_algo_in_file} extract={show_extract} \'\n            f\'history={show_history} report={show_report}\')\n        show_label = (\n            f\'{ticker} running in engine \'\n            f\'\'\'task_id={algo_res[\'rec\'].get(\n                \'task_id\',\n                \'missing-task-id\')} {base_label}\'\'\')\n        if not run_on_engine:\n            algo_trade_history_recs = algo_res[\'rec\'].get(\n                \'history\',\n                [])\n            show_label = (\n                f\'{ticker} algo.name={use_name} {base_label} \'\n                f\'trade_history_len={len(algo_trade_history_recs)}\')\n        if args.debug:\n            log.info(f\'algo_res={algo_res}\')\n            if algo_res[\'status\'] == ae_consts.SUCCESS:\n                log.info(\n                    f\'{ae_consts.get_status(status=algo_res[""status""])} - \'\n                    f\'done running {show_label}\')\n            else:\n                log.error(\n                    f\'{ae_consts.get_status(status=algo_res[""status""])} - \'\n                    f\'done running {show_label}\')\n        else:\n            if algo_res[\'status\'] == ae_consts.SUCCESS:\n                log.info(\n                    f\'{ae_consts.get_status(status=algo_res[""status""])} - \'\n                    f\'done running {show_label}\')\n            else:\n                log.error(\n                    f\'run_custom_algo returned error: {algo_res[""err""]}\')\n                sys.exit(1)\n        # end of running the custom algo handler\n\n        if mode == ae_consts.SA_MODE_EXTRACT:\n            log.info(f\'done extracting dataset - {ticker}\')\n        elif mode == ae_consts.SA_MODE_RUN_ALGO:\n            log.info(f\'done running algo - {ticker}\')\n\n        sys.exit(0)\n    # end of handling mode-specific arg assignments\n\n    # sanity checking the work and task are valid\n    if not valid:\n        log.error(\n            \'usage error: missing a supported mode: \'\n            \'-f (for prepare a dataset) \')\n        sys.exit(1)\n    if required_task and not task_name:\n        log.error(\n            \'usage error: missing a supported task_name\')\n        sys.exit(1)\n    # end of sanity checks\n\n    work[\'ticker\'] = ticker\n    work[\'ticker_id\'] = ticker_id\n    work[\'s3_bucket\'] = s3_bucket_name\n    work[\'s3_key\'] = s3_key\n    work[\'redis_key\'] = redis_key\n    work[\'s3_access_key\'] = s3_access_key\n    work[\'s3_secret_key\'] = s3_secret_key\n    work[\'s3_region_name\'] = s3_region_name\n    work[\'s3_address\'] = s3_address\n    work[\'s3_secure\'] = s3_secure\n    work[\'redis_address\'] = redis_address\n    work[\'redis_password\'] = redis_password\n    work[\'redis_db\'] = redis_db\n    work[\'redis_expire\'] = redis_expire\n    work[\'s3_enabled\'] = s3_enabled\n    work[\'redis_enabled\'] = redis_enabled\n    work[\'debug\'] = debug\n    work[\'label\'] = f\'ticker={ticker}\'\n\n    task_res = None\n    if ae_consts.is_celery_disabled():\n        work[\'celery_disabled\'] = True\n        log.debug(f\'starting without celery work={ae_consts.ppj(work)}\')\n        if mode == ae_consts.SA_MODE_PREPARE:\n            task_res = prep_dataset.prepare_pricing_dataset(\n                work)\n\n        if debug:\n            log.info(\n                f\'done - result={ae_consts.ppj(task_res)} task={task_name} \'\n                f\'status={ae_consts.get_status(status=task_res[""status""])} \'\n                f\'err={task_res[""err""]} label={work[""label""]}\')\n        else:\n            log.info(\n                f\'done - result task={task_name} \'\n                f\'status={ae_consts.get_status(status=task_res[""status""])} \'\n                f\'err={task_res[""err""]} label={work[""label""]}\')\n\n        if task_res[\'status\'] == ae_consts.SUCCESS:\n            image_res = None\n            label = work[\'label\']\n            ticker = work[\'ticker\']\n            if plot_action == ae_consts.PLOT_ACTION_SHOW:\n                log.info(\n                    \'showing plot\')\n                """"""\n                minute_key = f\'{redis_key}_minute\'\n                minute_df_res = build_df.build_df_from_redis(\n                    label=label\',\n                    address=redis_address,\n                    db=redis_db,\n                    key=minute_key)\n\n                minute_df = None\n                if (\n                        minute_df_res[\'status\'] == SUCCESS\n                        and minute_df_res[\'rec\'][\'valid_df\']):\n                    minute_df = minute_df_res[\'rec\'][\'data\']\n                    print(minute_df.columns.values)\n                    column_list = [\n                        \'close\',\n                        \'date\'\n                    ]\n                """"""\n                today_str = datetime.datetime.now().strftime(\n                    \'%Y-%m-%d\')\n                extract_req = work\n                extract_req[\'redis_key\'] = f\'{work[""redis_key""]}_minute\'\n                extract_status, minute_df = \\\n                    extract_utils.extract_minute_dataset(\n                        work_dict=work)\n                if extract_status == ae_consts.SUCCESS:\n                    log.info(\n                        f\'{label} - ticker={ticker} creating chart \'\n                        f\'date={today_str}\')\n                    """"""\n                    Plot Pricing with the Volume Overlay:\n                    """"""\n                    image_res = ae_charts.plot_overlay_pricing_and_volume(\n                        log_label=label,\n                        ticker=ticker,\n                        date_format=ae_consts.IEX_MINUTE_DATE_FORMAT,\n                        df=minute_df,\n                        show_plot=True)\n\n                    """"""\n                    Plot the High-Low-Open-Close Pricing:\n                    """"""\n                    """"""\n                    image_res = ae_charts.plot_hloc_pricing(\n                        log_label=label,\n                        ticker=ticker,\n                        title=f\'{ticker} - Minute Pricing - {today_str}\',\n                        df=minute_df,\n                        show_plot=True)\n                    """"""\n\n                    """"""\n                    Plot by custom columns in the DataFrame\n                    """"""\n                    """"""\n                    column_list = minute_df.columns.values\n                    column_list = [\n                        \'date\',\n                        \'close\',\n                        \'high\',\n                        \'low\',\n                        \'open\'\n                    ]\n                    image_res = ae_charts.plot_df(\n                        log_label=label,\n                        title=\'Pricing Title\',\n                        column_list=column_list,\n                        df=minute_df,\n                        xcol=\'date\',\n                        xlabel=\'Date\',\n                        ylabel=\'Pricing\',\n                        show_plot=True)\n                    """"""\n            elif plot_action == ae_consts.PLOT_ACTION_SAVE_TO_S3:\n                log.info(\n                    \'coming soon - support to save to s3\')\n            elif plot_action == ae_consts.PLOT_ACTION_SAVE_AS_FILE:\n                log.info(\n                    \'coming soon - support to save as file\')\n            if image_res:\n                log.info(\n                    f\'{label} show plot - \'\n                    f\'status={ae_consts.get_status(image_res[""status""])} \'\n                    f\'err={image_res[""err""]}\')\n    else:\n        log.info(f\'connecting to broker={broker_url} backend={backend_url}\')\n\n        # Get the Celery app\n        app = get_celery_app.get_celery_app(\n            name=__name__,\n            auth_url=broker_url,\n            backend_url=backend_url,\n            path_to_config_module=path_to_config_module,\n            ssl_options=ssl_options,\n            transport_options=transport_options,\n            include_tasks=include_tasks)\n\n        log.info(f\'calling task={task_name} - work={ae_consts.ppj(work)}\')\n        job_id = app.send_task(\n            task_name,\n            (work,))\n        log.info(f\'calling task={task_name} - success job_id={job_id}\')\n    # end of if/else\n\n# end of run_sa_tool\n\n\nif __name__ == \'__main__\':\n    run_sa_tool()\n'"
analysis_engine/scripts/start_algo.py,0,"b'#!/usr/bin/env python\n\nimport os\nimport sys\nimport datetime\nimport json\nimport argparse\nimport pandas as pd\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.plot_trading_history as plot_trading_history\nimport analysis_engine.load_history_dataset as load_history_utils\nimport analysis_engine.run_custom_algo as run_custom_algo\nimport spylunking.log.setup_logging as log_utils\n\n\nlog = log_utils.build_colorized_logger(\n    name=\'ae\',\n    log_config_path=ae_consts.LOG_CONFIG_PATH)\n\n\ndef start_algo():\n    """"""start_algo\n\n    Run a derived algorithm with an algorithm config dictionary\n\n    :param config_dict: algorithm config dictionary\n    """"""\n\n    log.debug(\'start - ae\')\n\n    parser = argparse.ArgumentParser(\n        description=(\n            \'stock analysis tool\'))\n    parser.add_argument(\n        \'-t\',\n        help=(\n            \'ticker\'),\n        required=False,\n        dest=\'ticker\')\n    parser.add_argument(\n        \'-e\',\n        help=(\n            \'file path to extract an \'\n            \'algorithm-ready datasets from redis\'),\n        required=False,\n        dest=\'algo_extract_loc\')\n    parser.add_argument(\n        \'-l\',\n        help=(\n            \'show dataset in this file\'),\n        required=False,\n        dest=\'show_from_file\')\n    parser.add_argument(\n        \'-H\',\n        help=(\n            \'show trading history dataset in this file\'),\n        required=False,\n        dest=\'show_history_from_file\')\n    parser.add_argument(\n        \'-E\',\n        help=(\n            \'show trading performance report dataset in this file\'),\n        required=False,\n        dest=\'show_report_from_file\')\n    parser.add_argument(\n        \'-L\',\n        help=(\n            \'restore an algorithm-ready dataset file back into redis\'),\n        required=False,\n        dest=\'restore_algo_file\')\n    parser.add_argument(\n        \'-f\',\n        help=(\n            \'save the trading history dataframe \'\n            \'to this file\'),\n        required=False,\n        dest=\'history_json_file\')\n    parser.add_argument(\n        \'-J\',\n        help=(\n            \'plot action - after preparing you can use: \'\n            \'-J show to open the image (good for debugging)\'),\n        required=False,\n        dest=\'plot_action\')\n    parser.add_argument(\n        \'-b\',\n        help=(\n            \'run a backtest using the dataset in \'\n            \'a file path/s3 key/redis key formats: \'\n            \'file:/opt/sa/tests/datasets/algo/SPY-latest.json or \'\n            \'s3://algoready/SPY-latest.json or \'\n            \'redis:SPY-latest\'),\n        required=False,\n        dest=\'backtest_loc\')\n    parser.add_argument(\n        \'-B\',\n        help=(\n            \'optional - broker url for Celery\'),\n        required=False,\n        dest=\'broker_url\')\n    parser.add_argument(\n        \'-C\',\n        help=(\n            \'optional - broker url for Celery\'),\n        required=False,\n        dest=\'backend_url\')\n    parser.add_argument(\n        \'-w\',\n        help=(\n            \'optional - flag for publishing an algorithm job \'\n            \'using Celery to the ae workers\'),\n        required=False,\n        dest=\'run_on_engine\',\n        action=\'store_true\')\n    parser.add_argument(\n        \'-k\',\n        help=(\n            \'optional - s3 access key\'),\n        required=False,\n        dest=\'s3_access_key\')\n    parser.add_argument(\n        \'-K\',\n        help=(\n            \'optional - s3 secret key\'),\n        required=False,\n        dest=\'s3_secret_key\')\n    parser.add_argument(\n        \'-a\',\n        help=(\n            \'optional - s3 address format: <host:port>\'),\n        required=False,\n        dest=\'s3_address\')\n    parser.add_argument(\n        \'-Z\',\n        help=(\n            \'optional - s3 secure: default False\'),\n        required=False,\n        dest=\'s3_secure\')\n    parser.add_argument(\n        \'-s\',\n        help=(\n            \'optional - start date: YYYY-MM-DD\'),\n        required=False,\n        dest=\'start_date\')\n    parser.add_argument(\n        \'-n\',\n        help=(\n            \'optional - end date: YYYY-MM-DD\'),\n        required=False,\n        dest=\'end_date\')\n    parser.add_argument(\n        \'-u\',\n        help=(\n            \'optional - s3 bucket name\'),\n        required=False,\n        dest=\'s3_bucket_name\')\n    parser.add_argument(\n        \'-G\',\n        help=(\n            \'optional - s3 region name\'),\n        required=False,\n        dest=\'s3_region_name\')\n    parser.add_argument(\n        \'-g\',\n        help=(\n            \'Path to a custom algorithm module file \'\n            \'on disk. This module must have a single \'\n            \'class that inherits from: \'\n            \'https://github.com/AlgoTraders/stock-analysis-engine/\'\n            \'blob/master/\'\n            \'analysis_engine/algo.py Additionally you \'\n            \'can find the Example-Minute-Algorithm here: \'\n            \'https://github.com/AlgoTraders/stock-analysis-engine/\'\n            \'blob/master/analysis_engine/mocks/\'\n            \'example_algo_minute.py\'),\n        required=False,\n        dest=\'run_algo_in_file\')\n    parser.add_argument(\n        \'-p\',\n        help=(\n            \'optional - s3 bucket/file for trading history\'),\n        required=False,\n        dest=\'algo_history_loc\')\n    parser.add_argument(\n        \'-o\',\n        help=(\n            \'optional - s3 bucket/file for trading performance report\'),\n        required=False,\n        dest=\'algo_report_loc\')\n    parser.add_argument(\n        \'-r\',\n        help=(\n            \'optional - redis_address format: <host:port>\'),\n        required=False,\n        dest=\'redis_address\')\n    parser.add_argument(\n        \'-R\',\n        help=(\n            \'optional - redis and s3 key name\'),\n        required=False,\n        dest=\'keyname\')\n    parser.add_argument(\n        \'-m\',\n        help=(\n            \'optional - redis database number (0 by default)\'),\n        required=False,\n        dest=\'redis_db\')\n    parser.add_argument(\n        \'-x\',\n        help=(\n            \'optional - redis expiration in seconds\'),\n        required=False,\n        dest=\'redis_expire\')\n    parser.add_argument(\n        \'-c\',\n        help=(\n            \'optional - algorithm config_file path for setting \'\n            \'up internal algorithm trading strategies and \'\n            \'indicators\'),\n        required=False,\n        dest=\'config_file\')\n    parser.add_argument(\n        \'-v\',\n        help=(\n            \'set the Algorithm to verbose logging\'),\n        required=False,\n        dest=\'verbose_algo\',\n        action=\'store_true\')\n    parser.add_argument(\n        \'-P\',\n        help=(\n            \'set the Algorithm\\\'s IndicatorProcessor to verbose logging\'),\n        required=False,\n        dest=\'verbose_processor\',\n        action=\'store_true\')\n    parser.add_argument(\n        \'-I\',\n        help=(\n            \'set all Algorithm\\\'s Indicators to verbose logging \'\n            \'(note indivdual indicators support a \\\'verbose\\\' key \'\n            \'that can be set to True to debug just one \'\n            \'indicator)\'),\n        required=False,\n        dest=\'verbose_indicators\',\n        action=\'store_true\')\n    parser.add_argument(\n        \'-V\',\n        help=(\n            \'inspect the datasets an algorithm is processing - this\'\n            \'will slow down processing to show debugging\'),\n        required=False,\n        dest=\'inspect_datasets\',\n        action=\'store_true\')\n    parser.add_argument(\n        \'-j\',\n        help=(\n            \'run the algorithm on just this specific date in the datasets \'\n            \'- specify the date in a format: YYYY-MM-DD like: 2018-11-29\'),\n        required=False,\n        dest=\'run_this_date\')\n    parser.add_argument(\n        \'-d\',\n        help=(\n            \'debug\'),\n        required=False,\n        dest=\'debug\',\n        action=\'store_true\')\n    args = parser.parse_args()\n\n    ticker = None\n    use_config_file = \'/opt/sa/cfg/default_algo.json\'\n    algo_mod_path = \'/opt/sa/analysis_engine/algo.py\'\n    use_balance = 10000.0\n    use_commission = 6.0\n    use_start_date = None\n    use_end_date = None\n    debug = False\n    verbose_algo = None\n    verbose_processor = None\n    verbose_indicators = None\n    inspect_datasets = None\n    history_json_file = None\n    run_this_date = None\n    algo_obj = None\n    algo_history_loc = None\n    algo_report_loc = None\n    algo_extract_loc = None\n    backtest_loc = None\n\n    ssl_options = ae_consts.SSL_OPTIONS\n    transport_options = ae_consts.TRANSPORT_OPTIONS\n    broker_url = ae_consts.WORKER_BROKER_URL\n    backend_url = ae_consts.WORKER_BACKEND_URL\n    path_to_config_module = ae_consts.WORKER_CELERY_CONFIG_MODULE\n    include_tasks = ae_consts.INCLUDE_TASKS\n    load_from_s3_bucket = None\n    load_from_s3_key = None\n    load_from_redis_key = None\n    load_from_file = None\n    load_compress = True\n    load_publish = True\n    load_config = None\n    report_redis_key = None\n    report_s3_bucket = None\n    report_s3_key = None\n    report_file = None\n    report_compress = True\n    report_publish = False\n    report_config = None\n    history_redis_key = None\n    history_s3_bucket = None\n    history_s3_key = None\n    history_file = None\n    history_compress = True\n    history_publish = True\n    history_config = None\n    extract_redis_key = None\n    extract_s3_bucket = None\n    extract_s3_key = None\n    extract_file = None\n    extract_save_dir = None\n    extract_compress = False\n    extract_publish = False\n    extract_config = None\n    s3_enabled = True\n    s3_access_key = ae_consts.S3_ACCESS_KEY\n    s3_secret_key = ae_consts.S3_SECRET_KEY\n    s3_region_name = ae_consts.S3_REGION_NAME\n    s3_address = ae_consts.S3_ADDRESS\n    s3_bucket_name = ae_consts.S3_BUCKET\n    s3_key = None\n    s3_secure = ae_consts.S3_SECURE\n    redis_enabled = True\n    redis_address = ae_consts.REDIS_ADDRESS\n    redis_key = None\n    redis_password = ae_consts.REDIS_PASSWORD\n    redis_db = ae_consts.REDIS_DB\n    redis_expire = ae_consts.REDIS_EXPIRE\n    redis_serializer = \'json\'\n    redis_encoding = \'utf-8\'\n    publish_to_s3 = True\n    publish_to_redis = False\n    publish_to_slack = False\n    slack_enabled = False\n    slack_code_block = False\n    slack_full_width = False\n\n    dataset_type = ae_consts.SA_DATASET_TYPE_ALGO_READY\n    serialize_datasets = ae_consts.DEFAULT_SERIALIZED_DATASETS\n    compress = False\n    encoding = \'utf-8\'\n    debug = False\n    run_on_engine = False\n\n    auto_fill = True\n    timeseries = \'minute\'\n    trade_strategy = \'count\'\n\n    if args.s3_access_key:\n        s3_access_key = args.s3_access_key\n    if args.s3_secret_key:\n        s3_secret_key = args.s3_secret_key\n    if args.s3_region_name:\n        s3_region_name = args.s3_region_name\n    if args.s3_address:\n        s3_address = args.s3_address\n    if args.s3_secure:\n        s3_secure = args.s3_secure\n    if args.redis_address:\n        redis_address = args.redis_address\n    if args.redis_db:\n        redis_db = args.redis_db\n    if args.redis_expire:\n        redis_expire = args.redis_expire\n    if args.history_json_file:\n        history_json_file = args.history_json_file\n    if args.ticker:\n        ticker = args.ticker.upper()\n    if args.debug:\n        debug = True\n    if args.verbose_algo:\n        verbose_algo = True\n    if args.verbose_processor:\n        verbose_processor = True\n    if args.verbose_indicators:\n        verbose_indicators = True\n    if args.inspect_datasets:\n        inspect_datasets = True\n    if args.run_this_date:\n        run_this_date = args.run_this_date\n    if args.start_date:\n        try:\n            use_start_date = f\'{str(args.start_date)} 00:00:00\'\n            datetime.datetime.strptime(\n                args.start_date,\n                ae_consts.COMMON_DATE_FORMAT)\n        except Exception as e:\n            msg = (\n                \'please use a start date formatted as: \'\n                f\'{ae_consts.COMMON_DATE_FORMAT}\\n\'\n                f\'error was: {e}\')\n            log.error(msg)\n            sys.exit(1)\n        # end of testing for a valid date\n    # end of args.start_date\n    if args.end_date:\n        try:\n            use_end_date = f\'{str(args.end_date)} 00:00:00\'\n            datetime.datetime.strptime(\n                args.end_date,\n                ae_consts.COMMON_DATE_FORMAT)\n        except Exception as e:\n            msg = (\n                \'please use an end date formatted as: \'\n                f\'{ae_consts.COMMON_DATE_FORMAT}\\n\'\n                f\'error was: {e}\')\n            log.error(msg)\n            sys.exit(1)\n        # end of testing for a valid date\n    # end of args.end_date\n    config_dict = None\n    if args.run_algo_in_file:\n        if not os.path.exists(args.run_algo_in_file):\n            log.error(\n                f\'missing algorithm module file: {args.run_algo_in_file}\')\n            sys.exit(1)\n        algo_mod_path = args.run_algo_in_file\n    if args.config_file:\n        use_config_file = args.config_file\n\n    if not os.path.exists(use_config_file):\n        log.error(f\'Failed: unable to find config file: -c {use_config_file}\')\n        sys.exit(1)\n    config_dict = json.loads(open(use_config_file).read())\n    algo_mod_path = config_dict.get(\n        \'algo_path\',\n        algo_mod_path)\n    if not os.path.exists(algo_mod_path):\n        log.error(\n            f\'missing algorithm module file from config: {algo_mod_path}\')\n        sys.exit(1)\n\n    """"""\n    Finalize the algo config\n    """"""\n    if config_dict:\n        use_balance = float(config_dict.get(\n            \'balance\',\n            use_balance))\n        use_commission = float(config_dict.get(\n            \'commission\',\n            use_commission))\n        ticker = str(config_dict.get(\n            \'ticker\',\n            ticker)).upper()\n\n        config_dict[\'ticker\'] = ticker\n        config_dict[\'balance\'] = use_balance\n        config_dict[\'commission\'] = use_commission\n    else:\n        if not ticker:\n            ticker = str(config_dict.get(\n                \'ticker\',\n                ae_consts.TICKER)).upper()\n    if not ticker:\n        log.error(\n            \'usage error: please set a ticker with -t <TICKER>\')\n        sys.exit(1)\n\n    if verbose_algo:\n        config_dict[\'verbose\'] = verbose_algo\n    if verbose_processor:\n        config_dict[\'verbose_processor\'] = verbose_processor\n    if verbose_indicators:\n        config_dict[\'verbose_indicators\'] = verbose_indicators\n    if inspect_datasets:\n        config_dict[\'inspect_datasets\'] = inspect_datasets\n    if run_this_date:\n        config_dict[\'run_this_date\'] = run_this_date\n\n    log.info(\n        \'starting\')\n\n    """"""\n    Run a custom algo module from disk\n    """"""\n    if algo_mod_path:\n\n        if args.backtest_loc:\n            backtest_loc = args.backtest_loc\n            if (\'file:/\' not in backtest_loc and\n                    \'s3://\' not in backtest_loc and\n                    \'redis://\' not in backtest_loc):\n                log.error(\n                    \'invalid -b <backtest dataset file> specified. \'\n                    f\'{backtest_loc} \'\n                    \'please use either: \'\n                    \'-b file:/opt/sa/tests/datasets/algo/SPY-latest.json or \'\n                    \'-b s3://algoready/SPY-latest.json or \'\n                    \'-b redis://SPY-latest\')\n                sys.exit(1)\n            if \'s3://\' in backtest_loc:\n                load_from_s3_bucket = backtest_loc.split(\'/\')[-2]\n                load_from_s3_key = backtest_loc.split(\'/\')[-1]\n            elif \'redis://\' in backtest_loc:\n                load_from_redis_key = backtest_loc.split(\'/\')[-1]\n            elif \'file:/\' in backtest_loc:\n                load_from_file = backtest_loc.split(\':\')[-1]\n            load_publish = True\n        # end of parsing supported transport - loading an algo-ready\n\n        if args.algo_history_loc:\n            algo_history_loc = args.algo_history_loc\n            if (\'file:/\' not in algo_history_loc and\n                    \'s3://\' not in algo_history_loc and\n                    \'redis://\' not in algo_history_loc):\n                log.error(\n                    \'invalid -p <backtest dataset file> specified. \'\n                    f\'{algo_history_loc} \'\n                    \'please use either: \'\n                    \'-p file:/opt/sa/tests/datasets/algo/SPY-latest.json or \'\n                    \'-p s3://algoready/SPY-latest.json or \'\n                    \'-p redis://SPY-latest\')\n                sys.exit(1)\n            if \'s3://\' in algo_history_loc:\n                history_s3_bucket = algo_history_loc.split(\'/\')[-2]\n                history_s3_key = algo_history_loc.split(\'/\')[-1]\n            elif \'redis://\' in algo_history_loc:\n                history_redis_key = algo_history_loc.split(\'/\')[-1]\n            elif \'file:/\' in algo_history_loc:\n                history_file = algo_history_loc.split(\':\')[-1]\n            history_publish = True\n        # end of parsing supported transport - trading history\n\n        if args.algo_report_loc:\n            algo_report_loc = args.algo_report_loc\n            if (\'file:/\' not in algo_report_loc and\n                    \'s3://\' not in algo_report_loc and\n                    \'redis://\' not in algo_report_loc):\n                log.error(\n                    \'invalid -o <backtest dataset file> specified. \'\n                    f\'{algo_report_loc} \'\n                    \'please use either: \'\n                    \'-o file:/opt/sa/tests/datasets/algo/SPY-latest.json or \'\n                    \'-o s3://algoready/SPY-latest.json or \'\n                    \'-o redis://SPY-latest\')\n                sys.exit(1)\n            if \'s3://\' in algo_report_loc:\n                report_s3_bucket = algo_report_loc.split(\'/\')[-2]\n                report_s3_key = algo_report_loc.split(\'/\')[-1]\n            elif \'redis://\' in algo_report_loc:\n                report_redis_key = algo_report_loc.split(\'/\')[-1]\n            elif \'file:/\' in algo_report_loc:\n                report_file = algo_report_loc.split(\':\')[-1]\n            report_publish = True\n        # end of parsing supported transport - trading performance report\n\n        if args.algo_extract_loc:\n            algo_extract_loc = args.algo_extract_loc\n            if (\'file:/\' not in algo_extract_loc and\n                    \'s3://\' not in algo_extract_loc and\n                    \'redis://\' not in algo_extract_loc):\n                log.error(\n                    \'invalid -e <backtest dataset file> specified. \'\n                    f\'{algo_extract_loc} \'\n                    \'please use either: \'\n                    \'-e file:/opt/sa/tests/datasets/algo/SPY-latest.json or \'\n                    \'-e s3://algoready/SPY-latest.json or \'\n                    \'-e redis://SPY-latest\')\n                sys.exit(1)\n            if \'s3://\' in algo_extract_loc:\n                extract_s3_bucket = algo_extract_loc.split(\'/\')[-2]\n                extract_s3_key = algo_extract_loc.split(\'/\')[-1]\n            elif \'redis://\' in algo_extract_loc:\n                extract_redis_key = algo_extract_loc.split(\'/\')[-1]\n            elif \'file:/\' in algo_extract_loc:\n                extract_file = algo_extract_loc.split(\':\')[-1]\n            extract_publish = True\n        # end of parsing supported transport - extract algorithm-ready\n\n        if args.run_on_engine:\n            run_on_engine = True\n            if verbose_algo:\n                log.info(\'starting algo on the engine\')\n\n        use_name = config_dict.get(\n            \'name\',\n            \'missing-algo-name\')\n        auto_fill = config_dict.get(\n            \'auto_fill\',\n            auto_fill)\n        timeseries = config_dict.get(\n            \'timeseries\',\n            timeseries)\n        trade_strategy = config_dict.get(\n            \'trade_strategy\',\n            trade_strategy)\n\n        algo_res = run_custom_algo.run_custom_algo(\n            mod_path=algo_mod_path,\n            ticker=config_dict[\'ticker\'],\n            balance=config_dict[\'balance\'],\n            commission=config_dict[\'commission\'],\n            name=use_name,\n            start_date=use_start_date,\n            end_date=use_end_date,\n            auto_fill=auto_fill,\n            config_dict=config_dict,\n            load_from_s3_bucket=load_from_s3_bucket,\n            load_from_s3_key=load_from_s3_key,\n            load_from_redis_key=load_from_redis_key,\n            load_from_file=load_from_file,\n            load_compress=load_compress,\n            load_publish=load_publish,\n            load_config=load_config,\n            report_redis_key=report_redis_key,\n            report_s3_bucket=report_s3_bucket,\n            report_s3_key=report_s3_key,\n            report_file=report_file,\n            report_compress=report_compress,\n            report_publish=report_publish,\n            report_config=report_config,\n            history_redis_key=history_redis_key,\n            history_s3_bucket=history_s3_bucket,\n            history_s3_key=history_s3_key,\n            history_file=history_file,\n            history_compress=history_compress,\n            history_publish=history_publish,\n            history_config=history_config,\n            extract_redis_key=extract_redis_key,\n            extract_s3_bucket=extract_s3_bucket,\n            extract_s3_key=extract_s3_key,\n            extract_file=extract_file,\n            extract_save_dir=extract_save_dir,\n            extract_compress=extract_compress,\n            extract_publish=extract_publish,\n            extract_config=extract_config,\n            publish_to_slack=publish_to_slack,\n            publish_to_s3=publish_to_s3,\n            publish_to_redis=publish_to_redis,\n            dataset_type=dataset_type,\n            serialize_datasets=serialize_datasets,\n            compress=compress,\n            encoding=encoding,\n            redis_enabled=redis_enabled,\n            redis_key=redis_key,\n            redis_address=redis_address,\n            redis_db=redis_db,\n            redis_password=redis_password,\n            redis_expire=redis_expire,\n            redis_serializer=redis_serializer,\n            redis_encoding=redis_encoding,\n            s3_enabled=s3_enabled,\n            s3_key=s3_key,\n            s3_address=s3_address,\n            s3_bucket=s3_bucket_name,\n            s3_access_key=s3_access_key,\n            s3_secret_key=s3_secret_key,\n            s3_region_name=s3_region_name,\n            s3_secure=s3_secure,\n            slack_enabled=slack_enabled,\n            slack_code_block=slack_code_block,\n            slack_full_width=slack_full_width,\n            dataset_publish_extract=extract_publish,\n            dataset_publish_history=history_publish,\n            dataset_publish_report=report_publish,\n            run_on_engine=run_on_engine,\n            auth_url=broker_url,\n            backend_url=backend_url,\n            include_tasks=include_tasks,\n            ssl_options=ssl_options,\n            transport_options=transport_options,\n            path_to_config_module=path_to_config_module,\n            timeseries=timeseries,\n            trade_strategy=trade_strategy,\n            verbose=verbose_algo)\n\n        show_label = f\'algo.name={use_name}\'\n        show_extract = f\'{algo_extract_loc}\'\n        show_history = f\'{algo_history_loc}\'\n        show_report = f\'{algo_report_loc}\'\n        base_label = (\n            f\'load={args.run_algo_in_file} extract={show_extract} \'\n            f\'history={show_history} report={show_report}\')\n\n        task_id = None\n        if run_on_engine:\n            task_id = algo_res.get(\'rec\', {}).get(\'task_id\', None)\n        if task_id:\n            log.info(f\'waiting on task_id={task_id} to finish\')\n            res = task_id.get()\n            history_config = res.get(\n                \'algo_req\', {}).get(\n                    \'history_config\', None)\n            s3_bucket = history_config.get(\'s3_bucket\', None)\n            s3_key = history_config.get(\'s3_key\', None)\n            load_res = load_history_utils.load_history_dataset(\n                s3_bucket=s3_bucket,\n                s3_key=s3_key)\n\n            history_df = load_res[ticker]\n            log.info(\n                \'plotting history df with columns: \'\n                f\'{history_df.columns.values}\')\n\n            history_df[\'date\'] = pd.to_datetime(\n                history_df[\'date\'])\n            history_df[\'minute\'] = pd.to_datetime(\n                history_df[\'minute\'])\n            ticker = history_df[\'ticker\'].iloc[0]\n\n            first_date = history_df[\'date\'].iloc[0]\n            end_date = history_df[\'date\'].iloc[-1]\n            title = (\n                f\'Trading History {ticker}\\n\'\n                f\'Backtest dates from {first_date} to {end_date}\')\n            use_xcol = \'date\'\n            use_as_date_format = \'%d\\n%b\'\n            use_minute = False\n            if \'minute\' in history_df:\n                found_valid_minute = history_df[\'minute\'].iloc[0]\n                if found_valid_minute:\n                    use_minute = True\n\n            if use_minute:\n                use_xcol = \'minute\'\n                use_as_date_format = \'%d %H:%M:%S\\n%b\'\n            xlabel = \'Dates vs Algo values\'\n            ylabel = \'Algo values\'\n            df_filter = (history_df[\'close\'] > 1.00)\n\n            # set default columns:\n            blue = None\n            green = None\n            orange = None\n\n            red = \'balance\'\n            blue = \'close\'\n\n            plot_trading_history.plot_trading_history(\n                title=title,\n                df=history_df,\n                red=red,\n                blue=blue,\n                green=green,\n                orange=orange,\n                date_col=use_xcol,\n                date_format=use_as_date_format,\n                xlabel=xlabel,\n                ylabel=ylabel,\n                df_filter=df_filter,\n                show_plot=True,\n                dropna_for_all=True)\n\n            return\n        else:\n            algo_obj = algo_res.get(\n                \'algo\',\n                None)\n        # end of getting the algo results from s3\n\n        if not algo_obj:\n            log.error(f\'{show_label} - did not create algorithm object\')\n            sys.exit(1)\n\n        if not run_on_engine:\n            algo_trade_history_recs = algo_res[\'rec\'].get(\n                \'history\',\n                [])\n            show_label = (\n                f\'{ticker} algo.name={use_name} {base_label} \'\n                f\'trade_history_len={len(algo_trade_history_recs)}\')\n        if args.debug:\n            log.info(f\'algo_res={algo_res}\')\n            if algo_res[\'status\'] == ae_consts.SUCCESS:\n                log.info(\n                    f\'\'\'{ae_consts.get_status(\n                        status=algo_res[\'status\'])} - \'\'\'\n                    f\'done running {show_label}\')\n            else:\n                log.error(\n                    f\'\'\'{ae_consts.get_status(\n                        status=algo_res[\'status\'])} - \'\'\'\n                    f\'done running {show_label}\')\n        else:\n            if algo_res[\'status\'] == ae_consts.SUCCESS:\n                log.info(\n                    f\'\'\'{ae_consts.get_status(\n                        status=algo_res[\'status\'])} - \'\'\'\n                    f\'done running {show_label}\')\n            else:\n                log.error(\n                    f\'run_custom_algo returned error: {algo_res[""err""]}\')\n                sys.exit(1)\n        # end of running the custom algo handler\n\n    else:\n        log.error(\n            f\'missing an algorithm mod path: {algo_mod_path}\')\n        sys.exit(1)\n    # end if running a custom algorithm module\n\n    if algo_obj:\n\n        trading_history_dict = algo_obj.get_history_dataset()\n        history_df = trading_history_dict[ticker]\n        if not hasattr(history_df, \'to_json\'):\n            return\n\n        if history_json_file:\n            log.info(f\'saving history to: {history_json_file}\')\n            history_df.to_json(\n                history_json_file,\n                orient=\'records\',\n                date_format=\'iso\')\n\n        log.info(\'plotting history\')\n\n        first_date = history_df[\'date\'].iloc[0]\n        end_date = history_df[\'date\'].iloc[-1]\n        title = (\n            f\'Trading History {ticker} for Algo \'\n            f\'{trading_history_dict[""algo_name""]}\\n\'\n            f\'Backtest dates from {first_date} to {end_date}\')\n        use_xcol = \'date\'\n        use_as_date_format = \'%d\\n%b\'\n        if config_dict[\'timeseries\'] == \'minute\':\n            use_xcol = \'minute\'\n            use_as_date_format = \'%d %H:%M:%S\\n%b\'\n        xlabel = f\'Dates vs {trading_history_dict[""algo_name""]} values\'\n        ylabel = f\'Algo {trading_history_dict[""algo_name""]}\\nvalues\'\n        df_filter = (history_df[\'close\'] > 0.01)\n\n        # set default hloc columns:\n        blue = None\n        green = None\n        orange = None\n\n        red = \'close\'\n        blue = \'balance\'\n\n        if debug:\n            for i, r in history_df.iterrows():\n                log.info(f\'{r[""minute""]} - {r[""close""]}\')\n\n        plot_trading_history.plot_trading_history(\n            title=title,\n            df=history_df,\n            red=red,\n            blue=blue,\n            green=green,\n            orange=orange,\n            date_col=use_xcol,\n            date_format=use_as_date_format,\n            xlabel=xlabel,\n            ylabel=ylabel,\n            df_filter=df_filter,\n            show_plot=True,\n            dropna_for_all=True)\n\n# end of start_algo\n\n\nif __name__ == \'__main__\':\n    start_algo()\n'"
analysis_engine/scripts/train_dnn_from_history.py,1,"b'#!/usr/bin/env python\n\n""""""\nTrain a DNN from a trading history\n\n::\n\n    train_from_history.py -b s3_bucket -k s3_key\n""""""\n\nimport argparse\nimport datetime\nimport numpy as np\nimport numpy.random as np_random\nimport pandas as pd\nimport pandas.api.types as pandas_types\nimport sklearn.model_selection as tt_split\nimport keras.wrappers.scikit_learn as keras_scikit\nimport tensorflow as tf\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.load_history_dataset as load_history\nimport analysis_engine.ai.build_regression_dnn as build_dnn\nimport analysis_engine.ai.build_datasets_using_scalers as build_scaler_datasets\nimport analysis_engine.ai.build_scaler_dataset_from_df as build_scaler_df\nimport analysis_engine.ai.plot_dnn_fit_history as plot_fit_history\nimport analysis_engine.plot_trading_history as plot_trading_history\nimport spylunking.log.setup_logging as log_utils\nfrom copy import deepcopy\n\n# ensure reproducible results\n# machinelearningmastery.com/reproducible-results-neural-networks-keras/\nnp_random.seed(1)\n\nlog = log_utils.build_colorized_logger(\n    name=\'train-dnn-from-history\')\n\nchoices = [\'close\', \'high\', \'low\']\n\n\ndef train_and_predict_from_history_in_s3():\n    """"""train_and_predict_from_history_in_s3\n\n    Run a derived algorithm with an algorithm config dictionary\n\n    :param config_dict: algorithm config dictionary\n    """"""\n\n    log.debug(\'start - plot\')\n\n    parser = argparse.ArgumentParser(\n        description=(\n            \'train a dnn to predict a column from a\'\n            \'a trading history file in s3\'))\n    parser.add_argument(\n        \'-b\',\n        help=(\n            \'s3 bucket\'),\n        required=False,\n        dest=\'s3_bucket\')\n    parser.add_argument(\n        \'-k\',\n        help=(\n            \'s3 key\'),\n        required=False,\n        dest=\'s3_key\')\n    parser.add_argument(\n        \'-q\',\n        help=(\n            \'disable scaler normalization and \'\n            \'only use 2 of close, high, or low with open to \'\n            \'predict the remainder of close, high, or low\'),\n        required=False,\n        dest=\'disable_scaler\',\n        action=\'store_true\')\n    parser.add_argument(\n        \'-c\',\n        help=(\n            \'column(s) to predict\'),\n        required=False,\n        dest=\'predict_features\',\n        choices=choices,\n        nargs=\'*\')\n    parser.add_argument(\n        \'-n\',\n        help=(\n            \'number of DNNs to create\'),\n        required=False,\n        dest=\'number_of_dnns\',\n        default=1,\n        type=int)\n    parser.add_argument(\n        \'-s\',\n        help=(\n            \'send plots to slack\'),\n        required=False,\n        dest=\'send_plots_to_slack\',\n        action=\'store_true\')\n    parser.add_argument(\n        \'-d\',\n        help=(\n            \'debug\'),\n        required=False,\n        dest=\'debug\',\n        action=\'store_true\')\n    args = parser.parse_args()\n\n    use_scalers = True\n    s3_access_key = ae_consts.S3_ACCESS_KEY\n    s3_secret_key = ae_consts.S3_SECRET_KEY\n    s3_region_name = ae_consts.S3_REGION_NAME\n    s3_address = ae_consts.S3_ADDRESS\n    s3_secure = ae_consts.S3_SECURE\n    compress = True\n\n    s3_bucket = (\n        f\'algohistory\')\n    s3_key = (\n        f\'algo_training_SPY.json\')\n    predict_features = [\'close\']\n    number_of_dnns = 1\n    send_plots_to_slack = False\n\n    debug = False\n\n    if args.s3_bucket:\n        s3_bucket = args.s3_bucket\n    if args.s3_key:\n        s3_key = args.s3_key\n    if args.disable_scaler:\n        use_scalers = False\n    if args.debug:\n        debug = True\n    if args.predict_features and len(args.predict_features):\n        predict_features = set(args.predict_features)  # remove any duplicates\n    if args.number_of_dnns != number_of_dnns:\n        number_of_dnns = args.number_of_dnns\n    if args.send_plots_to_slack:\n        send_plots_to_slack = args.send_plots_to_slack\n\n    load_res = load_history.load_history_dataset(\n        s3_enabled=True,\n        s3_key=s3_key,\n        s3_address=s3_address,\n        s3_bucket=s3_bucket,\n        s3_access_key=s3_access_key,\n        s3_secret_key=s3_secret_key,\n        s3_region_name=s3_region_name,\n        s3_secure=s3_secure,\n        compress=compress)\n\n    algo_config = load_res.get(\n        \'algo_config_dict\',\n        None)\n    algo_name = load_res.get(\n        \'algo_name\',\n        None)\n    tickers = load_res.get(\n        \'tickers\',\n        [\n            \'SPY\',\n        ])\n    ticker = tickers[0]\n\n    log.info(\n        f\'found algo: {algo_name}\')\n\n    if debug:\n        log.info(\n            f\'config: {ae_consts.ppj(algo_config)}\')\n\n    df = load_res[ticker]\n    df[\'date\'] = pd.to_datetime(\n        df[\'date\'])\n    df[\'minute\'] = pd.to_datetime(\n        df[\'minute\'])\n    ticker = df[\'ticker\'].iloc[0]\n\n    dnn_config = {\n        \'layers\': [\n            {\n                \'num_neurons\': 150,\n                \'init\': \'uniform\',\n                \'activation\': \'relu\'\n            },\n            {\n                \'num_neurons\': 100,\n                \'init\': \'uniform\',\n                \'activation\': \'relu\'\n            },\n            {\n                \'num_neurons\': 50,\n                \'init\': \'uniform\',\n                \'activation\': \'relu\'\n            },\n            {\n                \'num_neurons\': 1,\n                \'init\': \'uniform\',\n                \'activation\': \'relu\'\n            }\n        ]\n    }\n\n    compile_config = {\n        \'loss\': \'mse\',\n        \'optimizer\': \'adam\',\n        \'metrics\': [\n            \'accuracy\',\n            \'mse\',\n            \'mae\',\n            \'mape\',\n            \'cosine\'\n        ]\n    }\n\n    for predict_feature in predict_features:\n        for index in range(number_of_dnns):\n            log.info(f\'Creating DNN-{index+1} for column: {predict_feature}\')\n            create_column_dnn(\n                predict_feature=predict_feature,\n                ticker=ticker,\n                debug=debug,\n                use_scalers=use_scalers,\n                df=deepcopy(df),\n                dnn_config=deepcopy(dnn_config),\n                compile_config=compile_config,\n                s3_bucket=s3_bucket,\n                s3_key=s3_key,\n                send_plots_to_slack=send_plots_to_slack)\n# end of train_and_predict_from_history_in_s3\n\n\ndef create_column_dnn(\n    predict_feature=\'close\',\n    ticker=\'\',\n    debug=False,\n    use_epochs=10,\n    use_batch_size=10,\n    use_test_size=0.1,\n    use_random_state=1,\n    use_seed=7,\n    use_shuffle=False,\n    model_verbose=True,\n    fit_verbose=True,\n    use_scalers=True,\n    df=[],\n    dnn_config={},\n    compile_config={},\n    s3_bucket=\'\',\n    s3_key=\'\',\n        send_plots_to_slack=False):\n    """"""create_column_dnn\n\n    For scaler-normalized datasets this will\n    compile numeric columns and ignore string/non-numeric\n    columns as training and test feature columns\n\n    :param predict_feature: Column to create DNN with\n    :param ticker: Ticker being used\n    :param debug: Debug mode\n    :param use_epochs: Epochs times to use\n    :param use_batch_size: Batch size to use\n    :param use_test_size: Test size to use\n    :param use_random_state: Random state to train with\n    :param use_seed: Seed used to build scalar datasets\n    :param use_shuffle: To shuffle the regression estimator or not\n    :param model_verbose: To use a verbose Keras regression model or not\n    :param fit_verbose: To use a verbose fitting of the regression estimator\n    :param use_scalers: To build using scalars or not\n    :param df: Ticker dataset\n    :param dnn_config: Deep Neural Net keras model json to build the model\n    :param compile_config: Deep Neural Net dictionary of compile options\n    :param s3_bucket: S3 Bucket\n    :param s3_key: S3 Key\n    """"""\n\n    df_filter = (df[f\'{predict_feature}\'] >= 0.1)\n    first_date = df[df_filter][\'date\'].iloc[0]\n    end_date = df[df_filter][\'date\'].iloc[-1]\n\n    if \'minute\' in df:\n        found_valid_minute = df[\'minute\'].iloc[0]\n        if found_valid_minute:\n            first_date = df[df_filter][\'minute\'].iloc[0]\n            end_date = df[df_filter][\'minute\'].iloc[-1]\n\n    num_rows = len(df.index)\n    log.info(\n        f\'prepared training data from \'\n        f\'history {s3_bucket}@{s3_key} \'\n        f\'rows={num_rows} \'\n        f\'dates: {first_date} to {end_date}\')\n\n    if debug:\n        for i, r in df.iterrows():\n            log.info(\n                f\'{r[""minute""]} - {r[""{}"".format(predict_feature)]}\')\n        # end of for loop\n\n        log.info(\n            f\'columns: {df.columns.values}\')\n        log.info(\n            f\'rows: {len(df.index)}\')\n    # end of debug\n\n    use_all_features = use_scalers\n    all_features = []\n    train_features = []\n    if use_all_features:\n        for c in df.columns.values:\n            if (\n                    pandas_types.is_numeric_dtype(df[c]) and\n                    c not in train_features):\n                if c != predict_feature:\n                    train_features.append(c)\n                if c not in all_features:\n                    all_features.append(c)\n\n        dnn_config[\'layers\'][-1][\'activation\'] = (\n            \'sigmoid\')\n    else:\n        temp_choices = choices[:]\n        temp_choices.remove(predict_feature)\n        train_features = [\'open\']\n        train_features.extend(temp_choices)\n        all_features = [\n            f\'{predict_feature}\'\n        ] + train_features\n\n    num_features = len(train_features)\n    features_and_minute = [\n        \'minute\'\n    ] + all_features\n\n    log.info(\n        f\'converting columns to floats\')\n\n    timeseries_df = df[df_filter][features_and_minute].fillna(-10000.0)\n    converted_df = timeseries_df[all_features].astype(\'float32\')\n\n    train_df = None\n    test_df = None\n    scaler_predictions = None\n    if use_all_features:\n        scaler_res = build_scaler_datasets.build_datasets_using_scalers(\n            train_features=train_features,\n            test_feature=predict_feature,\n            df=converted_df,\n            test_size=use_test_size,\n            seed=use_seed)\n        if scaler_res[\'status\'] != ae_consts.SUCCESS:\n            log.error(\n                \'failed to build scaler train and test datasets\')\n            return\n        train_df = scaler_res[\'scaled_train_df\']\n        test_df = scaler_res[\'scaled_test_df\']\n        x_train = scaler_res[\'x_train\']\n        x_test = scaler_res[\'x_test\']\n        y_train = scaler_res[\'y_train\']\n        y_test = scaler_res[\'y_test\']\n        scaler_predictions = scaler_res[\'scaler_test\']\n    else:\n        log.info(\n            f\'building train and test dfs from subset of features\')\n        train_df = converted_df[train_features]\n        test_df = converted_df[[predict_feature]]\n\n        log.info(\n            f\'splitting {num_rows} into test and training \'\n            f\'size={use_test_size}\')\n\n        (x_train,\n         x_test,\n         y_train,\n         y_test) = tt_split.train_test_split(\n            train_df,\n            test_df,\n            test_size=use_test_size,\n            random_state=use_random_state)\n\n    log.info(\n        f\'split breakdown - \'\n        f\'x_train={len(x_train)} \'\n        f\'x_test={len(x_test)} \'\n        f\'y_train={len(y_train)} \'\n        f\'y_test={len(y_test)}\')\n\n    def set_model():\n        return build_dnn.build_regression_dnn(\n            num_features=num_features,\n            compile_config=compile_config,\n            model_config=dnn_config)\n\n    estimator = keras_scikit.KerasRegressor(\n        build_fn=set_model,\n        epochs=use_epochs,\n        batch_size=use_batch_size,\n        verbose=model_verbose)\n\n    log.info(\n        f\'fitting estimator - \'\n        f\'predicting={predict_feature} \'\n        f\'epochs={use_epochs} \'\n        f\'batch={use_batch_size} \'\n        f\'test_size={use_test_size} \'\n        f\'seed={use_seed}\')\n\n    history = estimator.fit(\n        x_train,\n        y_train,\n        validation_data=(\n            x_train,\n            y_train),\n        epochs=use_epochs,\n        batch_size=use_batch_size,\n        shuffle=use_shuffle,\n        verbose=fit_verbose)\n\n    created_on = (\n        datetime.datetime.now().strftime(\n            ae_consts.COMMON_TICK_DATE_FORMAT))\n    plot_fit_history.plot_dnn_fit_history(\n        df=history.history,\n        title=(\n            f\'DNN Errors Over Training Epochs\\n\'\n            f\'Training Data: s3://{s3_bucket}/{s3_key}\\n\'\n            f\'Created: {created_on}\'),\n        red=\'mean_squared_error\',\n        blue=\'mean_absolute_error\',\n        green=\'acc\',\n        orange=\'cosine_proximity\',\n        send_plots_to_slack=send_plots_to_slack)\n\n    # on production use newly fetched pricing data\n    # not the training data\n    predict_records = []\n    if use_all_features:\n        prediction_res = build_scaler_df.build_scaler_dataset_from_df(\n            df=converted_df[train_features])\n        if prediction_res[\'status\'] == ae_consts.SUCCESS:\n            predict_records = prediction_res[\'df\']\n    else:\n        predict_records = converted_df[train_features]\n\n    log.info(\n        f\'making predictions: {len(predict_records)}\')\n\n    predictions = estimator.model.predict(\n        predict_records,\n        verbose=True)\n\n    np.set_printoptions(threshold=np.nan)\n    indexes = tf.argmax(predictions, axis=1)\n    data = {}\n    data[\'indexes\'] = indexes\n    price_predictions = []\n    if use_all_features and scaler_predictions:\n        price_predictions = [\n            ae_consts.to_f(x) for x in\n            scaler_predictions.inverse_transform(\n                predictions.reshape(-1, 1)).reshape(-1)]\n    else:\n        price_predictions = [ae_consts.to_f(x[0]) for x in predictions]\n\n    timeseries_df[f\'predicted_{predict_feature}\'] = price_predictions\n    timeseries_df[\'error\'] = (\n        timeseries_df[f\'{predict_feature}\'] -\n        timeseries_df[f\'predicted_{predict_feature}\'])\n\n    output_features = [\n        \'minute\',\n        f\'{predict_feature}\',\n        f\'predicted_{predict_feature}\',\n        \'error\'\n    ]\n\n    date_str = (\n        f\'Dates: {timeseries_df[""minute""].iloc[0]} \'\n        f\'to \'\n        f\'{timeseries_df[""minute""].iloc[-1]}\')\n\n    log.info(\n        f\'historical {predict_feature} with predicted {predict_feature}: \'\n        f\'{timeseries_df[output_features]}\')\n    log.info(\n        date_str)\n    log.info(\n        f\'Columns: {output_features}\')\n\n    average_error = ae_consts.to_f(\n        timeseries_df[\'error\'].sum() / len(timeseries_df.index))\n\n    log.info(\n        f\'Average historical {predict_feature} \'\n        f\'vs predicted {predict_feature} error: \'\n        f\'{average_error}\')\n\n    log.info(\n        f\'plotting historical {predict_feature} vs predicted {predict_feature}\'\n        f\' from training with columns={num_features}\')\n\n    ts_filter = (timeseries_df[f\'{predict_feature}\'] > 0.1)\n    latest_feature = (\n        timeseries_df[ts_filter][f\'{predict_feature}\'].iloc[-1])\n    latest_predicted_feature = (\n        timeseries_df[ts_filter][f\'predicted_{predict_feature}\'].iloc[-1])\n\n    log.info(\n        f\'{end_date} {predict_feature}={latest_feature} \'\n        f\'with \'\n        f\'predicted_{predict_feature}={latest_predicted_feature}\')\n\n    plot_trading_history.plot_trading_history(\n        title=(\n            f\'{ticker} - Historical {predict_feature.title()} vs \'\n            f\'Predicted {predict_feature.title()}\\n\'\n            f\'Number of Training Features: {num_features}\\n\'\n            f\'{date_str}\'),\n        df=timeseries_df,\n        red=f\'{predict_feature}\',\n        blue=f\'predicted_{predict_feature}\',\n        green=None,\n        orange=None,\n        date_col=\'minute\',\n        date_format=\'%d %H:%M:%S\\n%b\',\n        xlabel=\'minute\',\n        ylabel=(\n            f\'Historical {predict_feature.title()} vs \'\n            f\'Predicted {predict_feature.title()}\'),\n        df_filter=ts_filter,\n        width=8.0,\n        height=8.0,\n        show_plot=True,\n        dropna_for_all=False,\n        send_plots_to_slack=send_plots_to_slack)\n# end of create_column_dnn\n\n\nif __name__ == \'__main__\':\n    train_and_predict_from_history_in_s3()\n'"
analysis_engine/td/__init__.py,0,b''
analysis_engine/td/consts.py,0,"b'""""""\nTradier Consts, Environment Variables and Authentication\nHelper\n""""""\n\nimport os\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\nTD_ENDPOINT_API = os.getenv(\n    \'TD_ENDPOINT_API\',\n    \'api.tradier.com\')\nTD_ENDPOINT_DATA = os.getenv(\n    \'TD_ENDPOINT_DATA\',\n    \'sandbox.tradier.com\')\nTD_ENDPOINT_STREAM = os.getenv(\n    \'TD_ENDPOINT_STREAM\',\n    \'sandbox.tradier.com\')\nTD_TOKEN = os.getenv(\n    \'TD_TOKEN\',\n    \'MISSING_TD_TOKEN\')\nTD_URLS = {\n    \'account\': (\n        f\'https://{TD_ENDPOINT_DATA}\'\n        \'/v1/user/profile\'),\n    \'options\': (\n        f\'https://{TD_ENDPOINT_DATA}\'\n        \'/v1/markets/options/chains\'\n        f\'?symbol={""{}""}&expiration={""{}""}\')\n}\n\nFETCH_TD_CALLS = 10000\nFETCH_TD_PUTS = 10001\n\nDATAFEED_TD_CALLS = 11000\nDATAFEED_TD_PUTS = 11001\n\nDEFAULT_FETCH_DATASETS_TD = [\n    FETCH_TD_CALLS,\n    FETCH_TD_PUTS\n]\nTIMESENSITIVE_DATASETS_TD = [\n    FETCH_TD_CALLS,\n    FETCH_TD_PUTS\n]\n\nENV_FETCH_DATASETS_TD = os.getenv(\n    \'ENV_FETCH_DATASETS_TD\',\n    None)\nif ENV_FETCH_DATASETS_TD:\n    SPLIT_FETCH_DATASETS_TD = \\\n        ENV_FETCH_DATASETS_TD.split(\',\')\n    DEFAULT_FETCH_DATASETS_TD = []\n    for d in SPLIT_FETCH_DATASETS_TD:\n        if d == \'tdcalls\':\n            DEFAULT_FETCH_DATASETS_TD.append(\n                FETCH_TD_CALLS)\n        elif d == \'tdputs\':\n            DEFAULT_FETCH_DATASETS_TD.append(\n                FETCH_TD_PUTS)\n# end of handling custom ENV_FETCH_DATASETS_TD\n\nFETCH_DATASETS_TD = DEFAULT_FETCH_DATASETS_TD\n\nTD_OPTION_COLUMNS = [\n    \'ask\',\n    \'ask_date\',\n    \'asksize\',\n    \'bid\',\n    \'bid_date\',\n    \'bidsize\',\n    \'date\',\n    \'exp_date\',\n    \'last\',\n    \'last_volume\',\n    \'open_interest\',\n    \'opt_type\',\n    \'strike\',\n    \'ticker\',\n    \'trade_date\',\n    \'created\',\n    \'volume\'\n]\n\nTD_EPOCH_COLUMNS = [\n    \'ask_date\',\n    \'bid_date\',\n    \'trade_date\'\n]\n\n\ndef get_ft_str_td(\n        ft_type):\n    """"""get_ft_str_td\n\n    :param ft_type: enum fetch type value to return\n                    as a string\n    """"""\n    if ft_type == FETCH_TD_CALLS:\n        return \'tdcalls\'\n    elif ft_type == FETCH_TD_PUTS:\n        return \'tdputs\'\n    else:\n        return f\'unsupported ft_type={ft_type}\'\n# end of get_ft_str_td\n\n\ndef get_datafeed_str_td(\n        df_type):\n    """"""get_datafeed_str_td\n\n    :param df_type: enum fetch type value to return\n                    as a string\n    """"""\n    if df_type == DATAFEED_TD_CALLS:\n        return \'tdcalls\'\n    elif df_type == DATAFEED_TD_PUTS:\n        return \'tdputs\'\n    else:\n        return f\'unsupported df_type={df_type}\'\n# end of get_datafeed_str_td\n\n\ndef get_auth_headers(\n        use_token=TD_TOKEN,\n        env_token=None):\n    """"""get_auth_headers\n\n    Get connection and auth headers for Tradier account:\n    https://developer.tradier.com/getting_started\n\n    :param use_token: optional - token\n        instead of the default ``TD_TOKEN``\n    :param env_token: optional - env key to use\n        instead of the default ``TD_TOKEN``\n    """"""\n    token = TD_TOKEN\n    if env_token:\n        token = os.getenv(\n            env_token,\n            TD_TOKEN)\n    headers = {\n        \'Accept\': \'application/json\',\n        \'Authorization\': f\'Bearer {token}\'\n    }\n    return headers\n# end of get_auth_headers\n'"
analysis_engine/td/extract_df_from_redis.py,0,"b'""""""\nExtract an TD dataset from Redis (S3 support coming soon) and\nload it into a ``pandas.DataFrame``\n\nSupported environment variables:\n\n::\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n\n""""""\n\nimport pandas as pd\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.utils as ae_utils\nimport analysis_engine.dataset_scrub_utils as scrub_utils\nimport analysis_engine.get_data_from_redis_key as redis_get\nimport analysis_engine.td.consts as td_consts\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef extract_option_calls_dataset(\n        ticker=None,\n        date=None,\n        work_dict=None,\n        scrub_mode=\'sort-by-date\',\n        verbose=False):\n    """"""extract_option_calls_dataset\n\n    Extract the TD options calls for a ticker and\n    return a tuple (status, ``pandas.Dataframe``)\n\n    .. code-block:: python\n\n        import analysis_engine.td.extract_df_from_redis as td_extract\n\n        # extract by historical date is also supported as an arg\n        # date=\'2019-02-15\'\n        calls_status, calls_df = td_extract.extract_option_calls_dataset(\n            ticker=\'SPY\')\n        print(calls_df)\n\n    :param ticker: string ticker to extract\n    :param date: optional - string date to extract\n        formatted ``YYYY-MM-DD``\n    :param work_dict: dictionary of args\n    :param scrub_mode: optional - string type of\n        scrubbing handler to run\n    :param verbose: optional - boolean for turning on logging\n    """"""\n    label = \'extract_td_calls\'\n    latest_close_date = ae_utils.get_last_close_str()\n    use_date = date\n    if work_dict:\n        if not ticker:\n            ticker = work_dict.get(\'ticker\', None)\n        label = f\'{work_dict.get(""label"", label)}\'\n    if not use_date:\n        use_date = latest_close_date\n\n    ds_id = ticker\n    df_type = td_consts.DATAFEED_TD_CALLS\n    df_str = td_consts.get_datafeed_str_td(df_type=df_type)\n    redis_db = ae_consts.REDIS_DB\n    redis_key = f\'{ticker}_{use_date}_tdcalls\'\n    redis_host, redis_port = ae_consts.get_redis_host_and_port(\n        req=work_dict)\n    redis_password = ae_consts.REDIS_PASSWORD\n    s3_key = redis_key\n\n    if work_dict:\n        redis_db = work_dict.get(\n            \'redis_db\',\n            redis_db)\n        redis_password = work_dict.get(\n            \'redis_password\',\n            redis_password)\n        verbose = work_dict.get(\n            \'verbose_td\',\n            verbose)\n\n    if verbose:\n        log.info(\n            f\'{label} - {df_str} - start - redis_key={redis_key} \'\n            f\'s3_key={s3_key}\')\n\n    exp_date_str = None\n    calls_df = None\n    status = ae_consts.NOT_RUN\n    try:\n        redis_rec = redis_get.get_data_from_redis_key(\n            label=label,\n            host=redis_host,\n            port=redis_port,\n            db=redis_db,\n            password=redis_password,\n            key=redis_key,\n            decompress_df=True)\n\n        status = redis_rec[\'status\']\n        if verbose:\n            log.info(\n                f\'{label} - {df_str} redis get data key={redis_key} \'\n                f\'status={ae_consts.get_status(status=status)}\')\n\n        if status == ae_consts.SUCCESS:\n            calls_json = None\n            if \'tdcalls\' in redis_rec[\'rec\'][\'data\']:\n                calls_json = redis_rec[\'rec\'][\'data\'][\'tdcalls\']\n            elif \'calls\' in redis_rec[\'rec\'][\'data\']:\n                calls_json = redis_rec[\'rec\'][\'data\'][\'calls\']\n            else:\n                calls_json = redis_rec[\'rec\'][\'data\']\n            if not calls_json:\n                return ae_consts.SUCCESS, pd.DataFrame([])\n            if verbose:\n                log.info(f\'{label} - {df_str} redis convert calls to df\')\n            exp_date_str = None\n            try:\n                calls_df = pd.read_json(\n                    calls_json,\n                    orient=\'records\')\n                if len(calls_df.index) == 0:\n                    return ae_consts.SUCCESS, pd.DataFrame([])\n                if \'date\' not in calls_df:\n                    if verbose:\n                        log.error(\n                            \'failed to find date column in TD calls \'\n                            f\'df={calls_df} from lens={len(calls_df.index)}\')\n                    return ae_consts.SUCCESS, pd.DataFrame([])\n                calls_df.sort_values(\n                        by=[\n                            \'date\',\n                            \'strike\'\n                        ])\n                """"""\n                for i, r in calls_df.iterrows():\n                    print(r[\'date\'])\n                convert_epochs = [\n                    \'ask_date\',\n                    \'bid_date\',\n                    \'trade_date\'\n                ]\n                for c in convert_epochs:\n                    if c in calls_df:\n                        calls_df[c] = pd.DatetimeIndex(pd.to_datetime(\n                            calls_df[c],\n                            format=ae_consts.COMMON_TICK_DATE_FORMAT\n                        )).tz_localize(\n                            \'UTC\').tz_convert(\n                                \'US/Eastern\')\n                # dates converted\n                """"""\n                exp_date_str = (\n                    calls_df[\'exp_date\'].iloc[-1])\n\n                calls_df[\'date\'] = calls_df[\'date\'].dt.strftime(\n                    ae_consts.COMMON_TICK_DATE_FORMAT)\n\n            except Exception as f:\n                not_fixed = True\n                if (\n                        \'Can only use .dt accessor with \'\n                        \'datetimelike values\') in str(f):\n                    try:\n                        log.critical(\n                            f\'fixing dates in {redis_key}\')\n                        # remove epoch second data and\n                        # use only the millisecond date values\n                        bad_date = ae_consts.EPOCH_MINIMUM_DATE\n                        calls_df[\'date\'][\n                            calls_df[\'date\'] < bad_date] = None\n                        calls_df = calls_df.dropna(axis=0, how=\'any\')\n                        fmt = ae_consts.COMMON_TICK_DATE_FORMAT\n                        calls_df[\'date\'] = pd.to_datetime(\n                            calls_df[\'date\'],\n                            unit=\'ms\').dt.strftime(fmt)\n                        not_fixed = False\n                    except Exception as g:\n                        log.critical(\n                            f\'failed to parse date column {calls_df[""date""]} \'\n                            f\'with dt.strftime ex={f} and EPOCH EX={g}\')\n                        return ae_consts.SUCCESS, pd.DataFrame([])\n                # if able to fix error or not\n\n                if not_fixed:\n                    log.debug(\n                        f\'{label} - {df_str} redis_key={redis_key} \'\n                        f\'no calls df found or ex={f}\')\n                    return ae_consts.SUCCESS, pd.DataFrame([])\n                # if unable to fix - return out\n\n                log.error(\n                    f\'{label} - {df_str} redis_key={redis_key} \'\n                    f\'no calls df found or ex={f}\')\n                return ae_consts.SUCCESS, pd.DataFrame([])\n            # end of try/ex to convert to df\n            if verbose:\n                log.info(\n                    f\'{label} - {df_str} redis_key={redis_key} \'\n                    f\'calls={len(calls_df.index)} exp_date={exp_date_str}\')\n        else:\n            if verbose:\n                log.info(\n                    f\'{label} - {df_str} did not find valid redis \'\n                    f\'option calls in redis_key={redis_key} \'\n                    f\'status={ae_consts.get_status(status=status)}\')\n\n    except Exception as e:\n        if verbose:\n            log.error(\n                f\'{label} - {df_str} - ds_id={ds_id} failed getting option \'\n                f\'calls from redis={redis_host}:{redis_port}@{redis_db} \'\n                f\'key={redis_key} ex={e}\')\n        return ae_consts.ERR, pd.DataFrame([])\n    # end of try/ex extract from redis\n\n    if verbose:\n        log.info(\n            f\'{label} - {df_str} ds_id={ds_id} extract scrub={scrub_mode}\')\n\n    scrubbed_df = scrub_utils.extract_scrub_dataset(\n        label=label,\n        scrub_mode=scrub_mode,\n        datafeed_type=df_type,\n        msg_format=\'df={} date_str={}\',\n        ds_id=ds_id,\n        df=calls_df)\n\n    status = ae_consts.SUCCESS\n\n    return status, scrubbed_df\n# end of extract_option_calls_dataset\n\n\ndef extract_option_puts_dataset(\n        ticker=None,\n        date=None,\n        work_dict=None,\n        scrub_mode=\'sort-by-date\',\n        verbose=False):\n    """"""extract_option_puts_dataset\n\n    Extract the TD options puts for a ticker and\n    return a tuple (status, ``pandas.Dataframe``)\n\n    .. code-block:: python\n\n        import analysis_engine.td.extract_df_from_redis as td_extract\n\n        # extract by historical date is also supported as an arg\n        # date=\'2019-02-15\'\n        puts_status, puts_df = td_extract.extract_option_puts_dataset(\n            ticker=\'SPY\')\n        print(puts_df)\n\n    :param ticker: string ticker to extract\n    :param date: optional - string date to extract\n        formatted ``YYYY-MM-DD``\n    :param work_dict: dictionary of args\n    :param scrub_mode: optional - string type of\n        scrubbing handler to run\n    :param verbose: optional - boolean for turning on logging\n    """"""\n    label = \'extract_td_puts\'\n    latest_close_date = ae_utils.get_last_close_str()\n    use_date = date\n    if work_dict:\n        if not ticker:\n            ticker = work_dict.get(\'ticker\', None)\n        label = f\'{work_dict.get(""label"", label)}\'\n    if not use_date:\n        use_date = latest_close_date\n\n    ds_id = ticker\n    df_type = td_consts.DATAFEED_TD_PUTS\n    df_str = td_consts.get_datafeed_str_td(df_type=df_type)\n    redis_db = ae_consts.REDIS_DB\n    redis_key = f\'{ticker}_{use_date}_tdputs\'\n    redis_host, redis_port = ae_consts.get_redis_host_and_port(\n        req=work_dict)\n    redis_password = ae_consts.REDIS_PASSWORD\n    s3_key = redis_key\n\n    if work_dict:\n        redis_db = work_dict.get(\n            \'redis_db\',\n            redis_db)\n        redis_password = work_dict.get(\n            \'redis_password\',\n            redis_password)\n        verbose = work_dict.get(\n            \'verbose_td\',\n            verbose)\n\n    if verbose:\n        log.info(\n            f\'{label} - {df_str} - start - redis_key={redis_key} \'\n            f\'s3_key={s3_key}\')\n\n    exp_date_str = None\n    puts_df = None\n    status = ae_consts.NOT_RUN\n    try:\n        redis_rec = redis_get.get_data_from_redis_key(\n            label=label,\n            host=redis_host,\n            port=redis_port,\n            db=redis_db,\n            password=redis_password,\n            key=redis_key,\n            decompress_df=True)\n\n        status = redis_rec[\'status\']\n        if verbose:\n            log.info(\n                f\'{label} - {df_str} redis get data key={redis_key} \'\n                f\'status={ae_consts.get_status(status=status)}\')\n\n        if status == ae_consts.SUCCESS:\n            puts_json = None\n            if \'tdputs\' in redis_rec[\'rec\'][\'data\']:\n                puts_json = redis_rec[\'rec\'][\'data\'][\'tdputs\']\n            if \'puts\' in redis_rec[\'rec\'][\'data\']:\n                puts_json = redis_rec[\'rec\'][\'data\'][\'puts\']\n            else:\n                puts_json = redis_rec[\'rec\'][\'data\']\n            if not puts_json:\n                return ae_consts.SUCCESS, pd.DataFrame([])\n            if verbose:\n                log.info(f\'{label} - {df_str} redis convert puts to df\')\n            try:\n                puts_df = pd.read_json(\n                    puts_json,\n                    orient=\'records\')\n                if len(puts_df.index) == 0:\n                    return ae_consts.SUCCESS, pd.DataFrame([])\n                if \'date\' not in puts_df:\n                    log.debug(\n                        \'failed to find date column in TD puts \'\n                        f\'df={puts_df} len={len(puts_df.index)}\')\n                    return ae_consts.SUCCESS, pd.DataFrame([])\n                puts_df.sort_values(\n                        by=[\n                            \'date\',\n                            \'strike\'\n                        ])\n                """"""\n                for i, r in calls_df.iterrows():\n                    print(r[\'date\'])\n                convert_epochs = [\n                    \'ask_date\',\n                    \'bid_date\',\n                    \'trade_date\'\n                ]\n                for c in convert_epochs:\n                    if c in puts_df:\n                        puts_df[c] = pd.DatetimeIndex(pd.to_datetime(\n                            puts_df[c],\n                            format=ae_consts.COMMON_TICK_DATE_FORMAT\n                        )).tz_localize(\n                            \'UTC\').tz_convert(\n                                \'US/Eastern\')\n                # dates converted\n                """"""\n                exp_date_str = (\n                    puts_df[\'exp_date\'].iloc[-1])\n\n                puts_df[\'date\'] = puts_df[\'date\'].dt.strftime(\n                    ae_consts.COMMON_TICK_DATE_FORMAT)\n\n            except Exception:\n                log.debug(\n                    f\'{label} - {df_str} redis_key={redis_key} \'\n                    \'no puts df found\')\n                return ae_consts.SUCCESS, pd.DataFrame([])\n            # end of try/ex to convert to df\n            if verbose:\n                log.info(\n                    f\'{label} - {df_str} redis_key={redis_key} \'\n                    f\'puts={len(puts_df.index)} exp_date={exp_date_str}\')\n        else:\n            if verbose:\n                log.info(\n                    f\'{label} - {df_str} did not find valid redis \'\n                    f\'option puts in redis_key={redis_key} \'\n                    f\'status={ae_consts.get_status(status=status)}\')\n\n    except Exception as e:\n        if verbose:\n            log.error(\n                f\'{label} - {df_str} - ds_id={ds_id} failed getting option \'\n                f\'puts from redis={redis_host}:{redis_port}@{redis_db} \'\n                f\'key={redis_key} ex={e}\')\n        return ae_consts.ERR, pd.DataFrame([])\n    # end of try/ex extract from redis\n\n    if verbose:\n        log.info(\n            f\'{label} - {df_str} ds_id={ds_id} extract scrub={scrub_mode}\')\n\n    scrubbed_df = scrub_utils.extract_scrub_dataset(\n        label=label,\n        scrub_mode=scrub_mode,\n        datafeed_type=df_type,\n        msg_format=\'df={} date_str={}\',\n        ds_id=ds_id,\n        df=puts_df)\n\n    status = ae_consts.SUCCESS\n\n    return status, scrubbed_df\n# end of extract_option_puts_dataset\n'"
analysis_engine/td/fetch_api.py,0,"b'""""""\nFetch API calls wrapping Tradier\n\nSupported environment variables:\n\n::\n\n    # verbose logging in this module\n    export DEBUG_FETCH=1\n\n""""""\n\nimport json\nimport datetime\nimport requests\nimport pandas as pd\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.utils as ae_utils\nimport analysis_engine.options_dates as opt_dates\nimport analysis_engine.url_helper as url_helper\nimport analysis_engine.dataset_scrub_utils as scrub_utils\nimport analysis_engine.td.consts as td_consts\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef fetch_calls(\n        ticker=None,\n        work_dict=None,\n        scrub_mode=\'sort-by-date\',\n        verbose=False):\n    """"""fetch_calls\n\n    Fetch Tradier option calls for a ticker and\n    return a tuple: (status, ``pandas.DataFrame``)\n\n    .. code-block:: python\n\n        import analysis_engine.td.fetch_api as td_fetch\n\n        # Please set the TD_TOKEN environment variable to your token\n        calls_status, calls_df = td_fetch.fetch_calls(\n            ticker=\'SPY\')\n\n        print(f\'Fetched SPY Option Calls from Tradier status={calls_status}:\')\n        print(calls_df)\n\n    :param ticker: string ticker to fetch\n    :param work_dict: dictionary of args\n        used by the automation\n    :param scrub_mode: optional - string type of\n        scrubbing handler to run\n    :param verbose: optional - bool for debugging\n    """"""\n    label = \'fetch_calls\'\n    datafeed_type = td_consts.DATAFEED_TD_CALLS\n    exp_date = None\n    latest_pricing = {}\n    latest_close = None\n\n    if work_dict:\n        ticker = work_dict.get(\n            \'ticker\',\n            ticker)\n        label = work_dict.get(\n            \'label\',\n            label)\n        exp_date = work_dict.get(\n            \'exp_date\',\n            exp_date)\n        latest_pricing = work_dict.get(\n            \'latest_pricing\',\n            latest_pricing)\n        latest_close = latest_pricing.get(\n            \'close\',\n            latest_close)\n\n    log.debug(\n        f\'{label} - calls - close={latest_close} \'\n        f\'ticker={ticker}\')\n\n    exp_date = opt_dates.option_expiration().strftime(\n        ae_consts.COMMON_DATE_FORMAT)\n    use_url = td_consts.TD_URLS[\'options\'].format(\n        ticker,\n        exp_date)\n    headers = td_consts.get_auth_headers()\n    session = requests.Session()\n    session.headers = headers\n    res = url_helper.url_helper(sess=session).get(\n        use_url\n    )\n\n    if res.status_code != requests.codes.OK:\n        if res.status_code in [401, 403]:\n            log.critical(\n                \'Please check the TD_TOKEN is correct \'\n                f\'received {res.status_code} during \'\n                \'fetch for: calls\')\n        else:\n            log.info(\n                f\'failed to get call with response={res} \'\n                f\'code={res.status_code} \'\n                f\'text={res.text}\')\n        return ae_consts.EMPTY, pd.DataFrame([{}])\n    records = json.loads(res.text)\n    org_records = records.get(\n        \'options\', {}).get(\n            \'option\', [])\n\n    if len(org_records) == 0:\n        log.info(\n            \'failed to get call records \'\n            \'text={}\'.format(\n                res.text))\n        return ae_consts.EMPTY, pd.DataFrame([{}])\n\n    options_list = []\n\n    # assumes UTC conversion will work with the system clock\n    created_minute = (\n        datetime.datetime.utcnow() - datetime.timedelta(hours=5)).strftime(\n            \'%Y-%m-%d %H:%M:00\')\n    last_close_date = ae_utils.get_last_close_str(\n        fmt=\'%Y-%m-%d %H:%M:00\')\n\n    # hit bug where dates were None\n    if not last_close_date:\n        last_close_date = created_minute\n\n    for node in org_records:\n        node[\'date\'] = last_close_date\n        node[\'created\'] = created_minute\n        node[\'ticker\'] = ticker\n        if (\n                node[\'option_type\'] == \'call\' and\n                node[\'expiration_type\'] == \'standard\' and\n                float(node[\'bid\']) > 0.01):\n            node[\'opt_type\'] = int(ae_consts.OPTION_CALL)\n            node[\'exp_date\'] = node[\'expiration_date\']\n\n            new_node = {}\n            for col in td_consts.TD_OPTION_COLUMNS:\n                if col in node:\n                    if col in td_consts.TD_EPOCH_COLUMNS:\n                        # trade_date can be None\n                        if node[col] == 0:\n                            new_node[col] = None\n                        else:\n                            new_node[col] = ae_utils.epoch_to_dt(\n                                epoch=node[col]/1000,\n                                use_utc=False,\n                                convert_to_est=True).strftime(\n                                    ae_consts.COMMON_TICK_DATE_FORMAT)\n                            """"""\n                            Debug epoch ms converter:\n                            """"""\n                            """"""\n                            print(\'-----------\')\n                            print(col)\n                            print(node[col])\n                            print(new_node[col])\n                            print(\'===========\')\n                            """"""\n                        # if/else valid date\n                    else:\n                        new_node[col] = node[col]\n                    # if date column to convert\n                # if column is in the row\n            # convert all columns\n\n            options_list.append(new_node)\n    # end of records\n\n    full_df = pd.DataFrame(options_list).sort_values(\n        by=[\n            \'strike\'\n        ],\n        ascending=True)\n\n    num_chains = len(full_df.index)\n\n    df = None\n    if latest_close:\n        df_filter = (\n            (full_df[\'strike\'] >=\n                (latest_close - ae_consts.OPTIONS_LOWER_STRIKE)) &\n            (full_df[\'strike\'] <=\n                (latest_close + ae_consts.OPTIONS_UPPER_STRIKE)))\n        df = full_df[df_filter].copy().sort_values(\n            by=[\n                \'date\',\n                \'strike\'\n            ]).reset_index()\n    else:\n        mid_chain_idx = int(num_chains / 2)\n        low_idx = int(\n            mid_chain_idx - ae_consts.MAX_OPTIONS_LOWER_STRIKE)\n        high_idx = int(\n            mid_chain_idx + ae_consts.MAX_OPTIONS_UPPER_STRIKE)\n        if low_idx < 0:\n            low_idx = 0\n        if high_idx > num_chains:\n            high_idx = num_chains\n        df = full_df[low_idx:high_idx].copy().sort_values(\n            by=[\n                \'date\',\n                \'strike\'\n            ]).reset_index()\n\n    scrubbed_df = scrub_utils.ingress_scrub_dataset(\n        label=label,\n        scrub_mode=scrub_mode,\n        datafeed_type=datafeed_type,\n        msg_format=\'df={} date_str={}\',\n        ds_id=ticker,\n        date_str=exp_date,\n        df=df)\n\n    return ae_consts.SUCCESS, scrubbed_df\n# end of fetch_calls\n\n\ndef fetch_puts(\n        ticker=None,\n        work_dict=None,\n        scrub_mode=\'sort-by-date\',\n        verbose=False):\n    """"""fetch_puts\n\n    Fetch Tradier option puts for a ticker and\n    return a tuple: (status, ``pandas.DataFrame``)\n\n    .. code-block:: python\n\n        import analysis_engine.td.fetch_api as td_fetch\n\n        puts_status, puts_df = td_fetch.fetch_puts(\n            ticker=\'SPY\')\n\n        print(f\'Fetched SPY Option Puts from Tradier status={puts_status}:\')\n        print(puts_df)\n\n    :param ticker: string ticker to fetch\n    :param work_dict: dictionary of args\n        used by the automation\n    :param scrub_mode: optional - string type of\n        scrubbing handler to run\n    :param verbose: optional - bool for debugging\n    """"""\n    label = \'fetch_calls\'\n    datafeed_type = td_consts.DATAFEED_TD_PUTS\n    exp_date = None\n    latest_pricing = {}\n    latest_close = None\n\n    if work_dict:\n        ticker = work_dict.get(\n            \'ticker\',\n            ticker)\n        label = work_dict.get(\n            \'label\',\n            label)\n        exp_date = work_dict.get(\n            \'exp_date\',\n            exp_date)\n        latest_pricing = work_dict.get(\n            \'latest_pricing\',\n            latest_pricing)\n        latest_close = latest_pricing.get(\n            \'close\',\n            latest_close)\n\n    if verbose:\n        log.info(\n            f\'{label} - puts - close={latest_close} \'\n            f\'ticker={ticker}\')\n\n    exp_date = opt_dates.option_expiration().strftime(\n        ae_consts.COMMON_DATE_FORMAT)\n    use_url = td_consts.TD_URLS[\'options\'].format(\n        ticker,\n        exp_date)\n    headers = td_consts.get_auth_headers()\n    session = requests.Session()\n    session.headers = headers\n    res = url_helper.url_helper(sess=session).get(\n        use_url\n    )\n\n    if res.status_code != requests.codes.OK:\n        if res.status_code in [401, 403]:\n            log.critical(\n                \'Please check the TD_TOKEN is correct \'\n                f\'received {res.status_code} during \'\n                \'fetch for: puts\')\n        else:\n            log.info(\n                f\'failed to get put with response={res} \'\n                f\'code={res.status_code} \'\n                f\'text={res.text}\')\n        return ae_consts.EMPTY, pd.DataFrame([{}])\n    records = json.loads(res.text)\n    org_records = records.get(\n        \'options\', {}).get(\n            \'option\', [])\n\n    if len(org_records) == 0:\n        log.info(\n            \'failed to get put records \'\n            \'text={}\'.format(\n                res.text))\n        return ae_consts.EMPTY, pd.DataFrame([{}])\n\n    options_list = []\n\n    # assumes UTC conversion will work with the system clock\n    created_minute = (\n        datetime.datetime.utcnow() - datetime.timedelta(hours=5)).strftime(\n            \'%Y-%m-%d %H:%M:00\')\n    last_close_date = ae_utils.get_last_close_str(\n        fmt=\'%Y-%m-%d %H:%M:00\')\n\n    # hit bug where dates were None\n    if not last_close_date:\n        last_close_date = created_minute\n\n    for node in org_records:\n        node[\'date\'] = last_close_date\n        node[\'created\'] = created_minute\n        node[\'ticker\'] = ticker\n        if (\n                node[\'option_type\'] == \'put\' and\n                node[\'expiration_type\'] == \'standard\' and\n                float(node[\'bid\']) > 0.01):\n            node[\'opt_type\'] = int(ae_consts.OPTION_PUT)\n            node[\'exp_date\'] = node[\'expiration_date\']\n\n            new_node = {}\n            for col in td_consts.TD_OPTION_COLUMNS:\n                if col in node:\n                    if col in td_consts.TD_EPOCH_COLUMNS:\n                        # trade_date can be None\n                        if node[col] == 0:\n                            new_node[col] = None\n                        else:\n                            new_node[col] = ae_utils.epoch_to_dt(\n                                epoch=node[col]/1000,\n                                use_utc=False,\n                                convert_to_est=True).strftime(\n                                    ae_consts.COMMON_TICK_DATE_FORMAT)\n                            """"""\n                            Debug epoch ms converter:\n                            """"""\n                            """"""\n                            print(\'-----------\')\n                            print(col)\n                            print(node[col])\n                            print(new_node[col])\n                            print(\'===========\')\n                            """"""\n                        # if/else valid date\n                    else:\n                        new_node[col] = node[col]\n                    # if date column to convert\n                # if column is in the row\n            # convert all columns\n\n            options_list.append(new_node)\n    # end of records\n\n    full_df = pd.DataFrame(options_list).sort_values(\n        by=[\n            \'strike\'\n        ],\n        ascending=True)\n\n    num_chains = len(full_df.index)\n\n    df = None\n    if latest_close:\n        df_filter = (\n            (full_df[\'strike\'] >=\n                (latest_close - ae_consts.OPTIONS_LOWER_STRIKE)) &\n            (full_df[\'strike\'] <=\n                (latest_close + ae_consts.OPTIONS_UPPER_STRIKE)))\n        df = full_df[df_filter].copy().sort_values(\n            by=[\n                \'date\',\n                \'strike\'\n            ]).reset_index()\n    else:\n        mid_chain_idx = int(num_chains / 2)\n        low_idx = int(\n            mid_chain_idx - ae_consts.MAX_OPTIONS_LOWER_STRIKE)\n        high_idx = int(\n            mid_chain_idx + ae_consts.MAX_OPTIONS_UPPER_STRIKE)\n        if low_idx < 0:\n            low_idx = 0\n        if high_idx > num_chains:\n            high_idx = num_chains\n        df = full_df[low_idx:high_idx].copy().sort_values(\n            by=[\n                \'date\',\n                \'strike\'\n            ]).reset_index()\n\n    scrubbed_df = scrub_utils.ingress_scrub_dataset(\n        label=label,\n        scrub_mode=scrub_mode,\n        datafeed_type=datafeed_type,\n        msg_format=\'df={} date_str={}\',\n        ds_id=ticker,\n        date_str=exp_date,\n        df=df)\n\n    return ae_consts.SUCCESS, scrubbed_df\n# end of fetch_puts\n'"
analysis_engine/td/fetch_data.py,0,"b'""""""\nFetch data from Tradier:\nhttps://developer.tradier.com/getting_started\n""""""\n\nimport json\nimport copy\nimport pandas as pd\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.td.consts as td_consts\nimport analysis_engine.options_dates as opt_dates\nimport analysis_engine.td.fetch_api as td_fetch\nimport analysis_engine.td.extract_df_from_redis as td_extract\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef fetch_data(\n        work_dict,\n        fetch_type=None):\n    """"""fetch_data\n\n    Factory method for fetching data from\n    TD using an enum or string alias. Returns\n    a pandas ``DataFrame`` and only supports\n    one ticker at a time.\n\n    Supported enums from: ``analysis_engine.td.consts``\n\n    ::\n\n        fetch_type = FETCH_TD_CALLS\n        fetch_type = FETCH_TD_PUTS\n\n    Supported ``work_dict[\'ft_type\']`` string values:\n\n    ::\n\n        work_dict[\'ft_type\'] = \'tdcalls\'\n        work_dict[\'ft_type\'] = \'tdputs\'\n\n    :param work_dict: dictionary of args for the Tradier api\n    :param fetch_type: optional - name or enum of the fetcher to create\n                       can also be a lower case string\n                       in work_dict[\'ft_type\']\n    """"""\n    use_fetch_name = None\n    ticker = work_dict.get(\n        \'ticker\',\n        None)\n    if not fetch_type:\n        fetch_type = work_dict.get(\n            \'ft_type\',\n            None)\n    if fetch_type:\n        use_fetch_name = str(fetch_type).lower()\n\n    if \'exp_date\' not in work_dict:\n        work_dict[\'exp_date\'] = opt_dates.option_expiration().strftime(\n            ae_consts.COMMON_DATE_FORMAT)\n\n    log.debug(f\'name={use_fetch_name} type={fetch_type} args={work_dict}\')\n\n    status_df = ae_consts.NOT_SET\n    df = pd.DataFrame([{}])\n\n    if (\n            use_fetch_name == \'tdcalls\' or\n            fetch_type == td_consts.FETCH_TD_CALLS):\n        status_df, fetch_df = td_fetch.fetch_calls(\n            work_dict=work_dict)\n\n        if status_df == ae_consts.SUCCESS:\n            log.debug(\n                \'call - merge df\')\n            work_copy = copy.deepcopy(\n                work_dict)\n            work_copy[\'ft_type\'] = td_consts.FETCH_TD_CALLS\n            work_copy[\'fd_type\'] = \'tdcalls\'\n            if \'tdcalls\' in work_dict:\n                work_copy[\'redis_key\'] = work_dict[\'tdcalls\']\n                work_copy[\'s3_key\'] = f\'{work_dict[""tdcalls""]}.json\'\n            else:\n                work_copy[\'redis_key\'] = f\'{work_dict[""redis_key""]}_tdcalls\'\n                work_copy[\'s3_key\'] = f\'{work_dict[""redis_key""]}_tdcalls\'\n            ext_status, ext_df = \\\n                td_extract.extract_option_calls_dataset(\n                    work_dict=work_copy)\n            if ext_status == ae_consts.SUCCESS and len(ext_df.index) > 0:\n                log.debug(\n                    f\'call - merging fetch={len(fetch_df.index)} \'\n                    f\'with ext={len(ext_df.index)}\')\n                """"""\n                for testing compression:\n                """"""\n                """"""\n                import sys\n                print(ext_df[\'date\'])\n                print(ext_df[\'ask_date\'])\n                print(ext_df[\'bid_date\'])\n                print(ext_df[\'trade_date\'])\n                sys.exit(1)\n                """"""\n                extracted_records = json.loads(ext_df.to_json(\n                    orient=\'records\'))\n                fetched_records = json.loads(fetch_df.to_json(\n                    orient=\'records\'))\n                new_records = []\n                dates_by_strike_dict = {}\n                for ex_row in extracted_records:\n                    date_strike_name = (\n                        f\'{ex_row[""created""]}_{ex_row[""strike""]}\')\n                    if date_strike_name not in dates_by_strike_dict:\n                        new_node = {}\n                        for c in td_consts.TD_OPTION_COLUMNS:\n                            if c in ex_row:\n                                new_node[c] = ex_row[c]\n                        # end of for all columns to copy over\n                        new_node.pop(\'index\', None)\n                        new_node.pop(\'level_0\', None)\n                        new_records.append(new_node)\n                        dates_by_strike_dict[date_strike_name] = True\n                # build extracted records\n\n                for ft_row in fetched_records:\n                    date_strike_name = (\n                        f\'{ft_row[""created""]}_{ft_row[""strike""]}\')\n                    try:\n                        if date_strike_name not in dates_by_strike_dict:\n                            new_node = {}\n                            for c in td_consts.TD_OPTION_COLUMNS:\n                                if c in ft_row:\n                                    new_node[c] = ft_row[c]\n                            # end of for all columns to copy over\n                            new_node.pop(\'index\', None)\n                            new_node.pop(\'level_0\', None)\n                            new_records.append(new_node)\n                            dates_by_strike_dict[date_strike_name] = True\n                        else:\n                            log.error(\n                                f\'already have {ticker} call - \'\n                                f\'date={ft_row[""created""]} \'\n                                f\'strike={ft_row[""strike""]}\')\n                    except Exception as p:\n                        log.critical(f\'failed fetching call with ex={p}\')\n                        return ae_consts.ERR, None\n                    # end of adding fetched records after the extracted\n\n                df = pd.DataFrame(new_records)\n                df.sort_values(\n                    by=[\n                        \'date\',\n                        \'strike\'\n                    ],\n                    ascending=True)\n                log.debug(f\'call - merged={len(df.index)}\')\n            else:\n                df = fetch_df.sort_values(\n                    by=[\n                        \'date\',\n                        \'strike\'\n                    ],\n                    ascending=True)\n        else:\n            log.warn(\n                f\'{ticker} - no data found for calls\')\n        # if able to merge fetch + last for today\n    elif (\n            use_fetch_name == \'tdputs\' or\n            fetch_type == td_consts.FETCH_TD_PUTS):\n        status_df, fetch_df = td_fetch.fetch_puts(\n            work_dict=work_dict)\n        if status_df == ae_consts.SUCCESS:\n            log.debug(\n                \'put - merge df\')\n            work_copy = copy.deepcopy(\n                work_dict)\n            work_copy[\'ft_type\'] = td_consts.FETCH_TD_PUTS\n            work_copy[\'fd_type\'] = \'tdputs\'\n            if \'tdputs\' in work_dict:\n                work_copy[\'redis_key\'] = work_dict[\'tdputs\']\n                work_copy[\'s3_key\'] = f\'{work_dict[""tdputs""]}.json\'\n            else:\n                work_copy[\'redis_key\'] = f\'{work_dict[""redis_key""]}_tdputs\'\n                work_copy[\'s3_key\'] = f\'{work_dict[""s3_key""]}_tdputs\'\n            ext_status, ext_df = \\\n                td_extract.extract_option_puts_dataset(\n                    work_dict=work_copy)\n            if ext_status == ae_consts.SUCCESS and len(ext_df.index) > 0:\n                log.debug(\n                    f\'put - merging fetch={len(fetch_df.index)} with \'\n                    f\'ext={len(ext_df.index)}\')\n                """"""\n                for testing compression:\n                """"""\n                """"""\n                import sys\n                print(ext_df[\'date\'])\n                sys.exit(1)\n                """"""\n                extracted_records = json.loads(ext_df.to_json(\n                    orient=\'records\'))\n                fetched_records = json.loads(fetch_df.to_json(\n                    orient=\'records\'))\n                new_records = []\n                dates_by_strike_dict = {}\n                for ex_row in extracted_records:\n                    date_strike_name = (\n                        f\'{ex_row[""created""]}_{ex_row[""strike""]}\')\n                    if date_strike_name not in dates_by_strike_dict:\n                        new_node = {}\n                        for c in td_consts.TD_OPTION_COLUMNS:\n                            if c in ex_row:\n                                new_node[c] = ex_row[c]\n                        # end of for all columns to copy over\n                        new_node.pop(\'index\', None)\n                        new_node.pop(\'level_0\', None)\n                        new_records.append(new_node)\n                        dates_by_strike_dict[date_strike_name] = True\n                # build extracted records\n\n                for ft_row in fetched_records:\n                    date_strike_name = (\n                        f\'{ft_row[""created""]}_{ft_row[""strike""]}\')\n                    try:\n                        if date_strike_name not in dates_by_strike_dict:\n                            new_node = {}\n                            for c in td_consts.TD_OPTION_COLUMNS:\n                                if c in ft_row:\n                                    new_node[c] = ft_row[c]\n                            # end of for all columns to copy over\n                            new_node.pop(\'index\', None)\n                            new_node.pop(\'level_0\', None)\n                            new_records.append(new_node)\n                            dates_by_strike_dict[date_strike_name] = True\n                        else:\n                            log.error(\n                                f\'already have {ticker} put - \'\n                                f\'date={ft_row[""created""]} \'\n                                f\'strike={ft_row[""strike""]}\')\n                    except Exception as p:\n                        log.critical(f\'failed fetching puts with ex={p}\')\n                        return ae_consts.ERR, None\n                # end of adding fetched records after the extracted\n\n                df = pd.DataFrame(new_records)\n                df.sort_values(\n                    by=[\n                        \'date\',\n                        \'strike\'\n                    ],\n                    ascending=True)\n                log.debug(f\'put - merged={len(df.index)}\')\n            else:\n                df = fetch_df.sort_values(\n                    by=[\n                        \'date\',\n                        \'strike\'\n                    ],\n                    ascending=True)\n        else:\n            log.warn(\n                f\'{ticker} - no data found for puts\')\n        # if able to merge fetch + last for today\n    else:\n        log.error(\n            f\'label={work_dict.get(""label"", None)} - \'\n            f\'unsupported fetch_data(\'\n            f\'work_dict={work_dict}, \'\n            f\'fetch_type={fetch_type}\'\n            f\')\')\n        raise NotImplementedError\n    # end of supported fetchers\n\n    return status_df, df\n# end of fetch_data\n'"
analysis_engine/td/get_data.py,0,"b'""""""\nParse data from TD\n\nSupported environment variables:\n\n::\n\n    export DEBUG_TD_DATA=1\n\n""""""\n\nimport datetime\nimport copy\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.build_result as build_result\nimport analysis_engine.api_requests as api_requests\nimport analysis_engine.td.fetch_data as td_fetch_data\nimport analysis_engine.td.consts as td_consts\nimport analysis_engine.work_tasks.publish_pricing_update as publisher\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef get_data_from_td(\n        work_dict):\n    """"""get_data_from_td\n\n    Get pricing data from Tradier\n\n    :param work_dict: request dictionary\n    """"""\n    label = \'get_data_from_td\'\n\n    log.debug(f\'task - {label} - start work_dict={work_dict}\')\n\n    rec = {\n        \'data\': None,\n        \'updated\': None\n    }\n    res = {\n        \'status\': ae_consts.NOT_RUN,\n        \'err\': None,\n        \'rec\': rec\n    }\n\n    ticker = None\n    field = None\n    ft_type = None\n\n    try:\n\n        ticker = work_dict.get(\n            \'ticker\',\n            ae_consts.TICKER)\n        field = work_dict.get(\n            \'field\',\n            \'daily\')\n        ft_type = work_dict.get(\n            \'ft_type\',\n            None)\n        ft_str = str(ft_type).lower()\n        label = work_dict.get(\n            \'label\',\n            label)\n        orient = work_dict.get(\n            \'orient\',\n            \'records\')\n\n        td_req = None\n        if ft_type == td_consts.FETCH_TD_CALLS or ft_str == \'tdcalls\':\n            ft_type == td_consts.FETCH_TD_CALLS\n            td_req = api_requests.build_td_fetch_calls_request(\n                label=label)\n        elif ft_type == td_consts.FETCH_TD_PUTS or ft_str == \'tdputs\':\n            ft_type == td_consts.FETCH_TD_PUTS\n            td_req = api_requests.build_td_fetch_puts_request(\n                label=label)\n        else:\n            log.error(\n                f\'{label} - unsupported ft_type={ft_type} ft_str={ft_str} \'\n                f\'ticker={ticker}\')\n            raise NotImplementedError\n        # if supported fetch request type\n\n        clone_keys = [\n            \'latest_pricing\',\n            \'ticker\',\n            \'s3_address\',\n            \'s3_bucket\',\n            \'s3_key\',\n            \'redis_address\',\n            \'redis_db\',\n            \'redis_password\',\n            \'redis_key\'\n        ]\n\n        for k in clone_keys:\n            if k in work_dict:\n                td_req[k] = work_dict.get(\n                    k,\n                    f\'{k}-missing-in-{label}\')\n        # end of cloning keys\n\n        if not td_req:\n            err = (\n                f\'{label} - ticker={td_req[""ticker""]} did not build a TD \'\n                f\'request for work={work_dict}\')\n            log.error(err)\n            res = build_result.build_result(\n                status=ae_consts.ERR,\n                err=err,\n                rec=rec)\n            return res\n        else:\n            log.debug(\n                f\'{label} - ticker={td_req[""ticker""]} field={field} \'\n                f\'orient={orient} fetch\')\n        # if invalid td request\n\n        df = None\n        try:\n            if \'from\' in work_dict:\n                td_req[\'from\'] = datetime.datetime.strptime(\n                    \'%Y-%m-%d %H:%M:%S\',\n                    work_dict[\'from\'])\n            status_df, df = td_fetch_data.fetch_data(\n                work_dict=td_req,\n                fetch_type=ft_type)\n\n            if status_df == ae_consts.SUCCESS:\n                rec[\'data\'] = df.to_json(\n                    orient=orient)\n                rec[\'updated\'] = datetime.datetime.utcnow().strftime(\n                    \'%Y-%m-%d %H:%M:%S\')\n            elif status_df == ae_consts.EMPTY:\n                res = build_result.build_result(\n                    status=ae_consts.ERR,\n                    err=(\n                        f\'did not fetch any data\'),\n                    rec=rec)\n                return res\n            else:\n                err = (\n                    f\'{label} - ticker={td_req[""ticker""]} \'\n                    f\'td_fetch_data.fetch_data field={ft_type} \'\n                    \'failed fetch_data\')\n                log.critical(err)\n                res = build_result.build_result(\n                    status=ae_consts.ERR,\n                    err=err,\n                    rec=rec)\n                return res\n        except Exception as f:\n            err = (\n                f\'{label} - ticker={td_req[""ticker""]} field={ft_type} \'\n                f\'failed fetch_data with ex={f}\')\n            log.critical(err)\n            res = build_result.build_result(\n                status=ae_consts.ERR,\n                err=err,\n                rec=rec)\n            return res\n        # end of try/ex\n\n        if ae_consts.ev(\'DEBUG_TD_DATA\', \'0\') == \'1\':\n            log.debug(\n                f\'{label} ticker={td_req[""ticker""]} field={field} \'\n                f\'data={rec[""data""]} to_json\')\n        else:\n            log.debug(\n                f\'{label} ticker={td_req[""ticker""]} field={field} to_json\')\n        # end of if/else found data\n\n        upload_and_cache_req = copy.deepcopy(td_req)\n        upload_and_cache_req[\'celery_disabled\'] = True\n        upload_and_cache_req[\'data\'] = rec[\'data\']\n        if not upload_and_cache_req[\'data\']:\n            upload_and_cache_req[\'data\'] = \'{}\'\n        use_field = field\n        if use_field == \'news\':\n            use_field = \'news1\'\n        if \'redis_key\' in work_dict:\n            upload_and_cache_req[\'redis_key\'] = (\n                f\'\'\'{work_dict.get(\n                    \'redis_key\',\n                    td_req[\'redis_key\'])}_\'\'\'\n                f\'{use_field}\')\n        if \'s3_key\' in work_dict:\n            upload_and_cache_req[\'s3_key\'] = (\n                f\'\'\'{work_dict.get(\n                    \'s3_key\',\n                    td_req[\'s3_key\'])}_\'\'\'\n                f\'{use_field}\')\n\n        try:\n            update_res = publisher.run_publish_pricing_update(\n                work_dict=upload_and_cache_req)\n            update_status = update_res.get(\n                \'status\',\n                ae_consts.NOT_SET)\n            log.debug(\n                f\'{label} publish update \'\n                f\'status={ae_consts.get_status(status=update_status)} \'\n                f\'data={update_res}\')\n        except Exception:\n            err = (\n                f\'{label} - failed to upload td data={upload_and_cache_req} \'\n                f\'to s3_key={upload_and_cache_req[""s3_key""]} and \'\n                f\'redis_key={upload_and_cache_req[""redis_key""]}\')\n            log.error(err)\n        # end of try/ex to upload and cache\n\n        if not rec[\'data\']:\n            log.debug(\n                f\'{label} - ticker={td_req[""ticker""]} no Tradier data \'\n                f\'field={field} to publish\')\n        # end of if/else\n\n        res = build_result.build_result(\n            status=ae_consts.SUCCESS,\n            err=None,\n            rec=rec)\n\n    except Exception as e:\n        res = build_result.build_result(\n            status=ae_consts.ERR,\n            err=(\n                \'failed - get_data_from_td \'\n                f\'dict={work_dict} with ex={e}\'),\n            rec=rec)\n    # end of try/ex\n\n    log.debug(\n        \'task - get_data_from_td done - \'\n        f\'{label} - status={ae_consts.get_status(res[""status""])} \'\n        f\'err={res[""err""]}\')\n\n    return res\n# end of get_data_from_td\n'"
analysis_engine/work_tasks/__init__.py,0,b''
analysis_engine/work_tasks/celery_config.py,0,"b""import os\n\nbroker_url = os.getenv(\n    'WORKER_BROKER_URL',\n    'redis://0.0.0.0:6379/11')\nresult_backend = os.getenv(\n    'WORKER_BACKEND_URL',\n    'redis://0.0.0.0:6379/12')\n\n# http://docs.celeryproject.org/en/latest/userguide/optimizing.html\n\n# these are targeted at optimizing processing\n# on long-running tasks\n# while increasing reliability\n\n# http://docs.celeryproject.org/en/latest/userguide/configuration.html#std:setting-worker_prefetch_multiplier  # noqa\nworker_prefetch_multiplier = 1\n\n# http://docs.celeryproject.org/en/latest/userguide/configuration.html#std:setting-broker_heartbeat  # noqa\nbroker_heartbeat = 240  # seconds\n\n# http://docs.celeryproject.org/en/latest/userguide/configuration.html#std:setting-broker_connection_max_retries  # noqa\nbroker_connection_max_retries = None\n\n# http://docs.celeryproject.org/en/latest/userguide/configuration.html#std:setting-task_acks_late  # noqa\ntask_acks_late = True\n\n# http://docs.celeryproject.org/en/latest/userguide/calling.html#calling-retry\ntask_publish_retry_policy = {\n    'interval_max': 1,\n    'max_retries': 120,     # None = forever\n    'interval_start': 0.1,\n    'interval_step': 0.2}\n\ntask_serializer = 'json'\nresult_serializer = 'json'\naccept_content = ['json']\ntimezone = 'America/Los_Angeles'\n\ntask_routes = {}\n"""
analysis_engine/work_tasks/celery_service_config.py,0,"b""import os\n\nbroker_url = os.getenv(\n    'WORKER_BROKER_URL',\n    'redis://redis:6379/11')\nresult_backend = os.getenv(\n    'WORKER_BACKEND_URL',\n    'redis://redis:6379/12')\n\n# http://docs.celeryproject.org/en/latest/userguide/optimizing.html\n\n# these are targeted at optimizing processing\n# on long-running tasks\n# while increasing reliability\n\n# http://docs.celeryproject.org/en/latest/userguide/configuration.html#std:setting-worker_prefetch_multiplier  # noqa\nworker_prefetch_multiplier = 1\n\n# http://docs.celeryproject.org/en/latest/userguide/configuration.html#std:setting-broker_heartbeat  # noqa\nbroker_heartbeat = 240  # seconds\n\n# http://docs.celeryproject.org/en/latest/userguide/configuration.html#std:setting-broker_connection_max_retries  # noqa\nbroker_connection_max_retries = None\n\n# http://docs.celeryproject.org/en/latest/userguide/configuration.html#std:setting-task_acks_late  # noqa\ntask_acks_late = True\n\n# http://docs.celeryproject.org/en/latest/userguide/calling.html#calling-retry\ntask_publish_retry_policy = {\n    'interval_max': 1,\n    'max_retries': 120,     # None = forever\n    'interval_start': 0.1,\n    'interval_step': 0.2}\n\ntask_serializer = 'json'\nresult_serializer = 'json'\naccept_content = ['json']\ntimezone = 'America/Los_Angeles'\n\ntask_routes = {}\n"""
analysis_engine/work_tasks/custom_task.py,0,"b'""""""\nCustom Celery Task Handling\n===========================\n\nDefine your own ``on_failure`` and ``on_success``\nwith the ``analysis_engine.work_tasks.custom_task.CustomTask`` custom\nclass object.\n\nDebug values with the environment variable:\n\n::\n\n    export DEBUG_TASK=1\n\n""""""\n\nimport celery\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.send_to_slack as slack_utils\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\nclass CustomTask(celery.Task):\n    """"""CustomTask""""""\n\n    log_label = \'custom_task\'\n\n    def build_log_label_from_args(\n            self,\n            args):\n        """"""build_log_label_from_args\n\n        :param args: list of celery args\n        """"""\n        if not args:\n            return\n        if len(args) > 0:\n            self.log_label = f\'\'\'label=[{args[0].get(\n                \'label\',\n                args[0].get(\n                    \'name\',\n                    self.name))}]\'\'\'\n        else:\n            return\n    # end of build_log_label_from_args\n\n    def on_success(\n            self,\n            retval,\n            task_id,\n            args,\n            kwargs):\n        """"""on_success\n\n        Handle custom actions when a task completes\n        successfully.\n\n        http://docs.celeryproject.org/en/latest/reference/celery.app.task.html\n\n        :param retval: return value\n        :param task_id: celery task id\n        :param args: arguments passed into task\n        :param kwargs: keyword arguments passed into task\n        """"""\n\n        self.build_log_label_from_args(\n            args=args)\n\n        if ae_consts.ev(\'DEBUG_TASK\', \'0\') == \'1\':\n            log.info(\n                f\'on_success {self.log_label} - retval={retval} \'\n                f\'task_id={task_id} args={args} kwargs={kwargs}\')\n        else:\n            log.info(f\'on_success {self.log_label} - task_id={task_id}\')\n    # end of on_success\n\n    def on_failure(\n            self,\n            exc,\n            task_id,\n            args,\n            kwargs,\n            einfo):\n        """"""on_failure\n\n        Handle custom actions when a task completes\n        not successfully. As an example, if the task throws an\n        exception, then this ``on_failure`` method can\n        customize how to handle **exceptional** cases.\n\n        http://docs.celeryproject.org/en/latest/userguide/tasks.html#task-inheritance\n\n        :param exc: exception\n        :param task_id: task id\n        :param args: arguments passed into task\n        :param kwargs: keyword arguments passed into task\n        :param einfo: exception info\n        """"""\n\n        self.build_log_label_from_args(\n            args=args)\n\n        use_exc = str(exc)\n        if ae_consts.ev(\'DEBUG_TASK\', \'0\') == \'1\':\n            log.error(\n                f\'on_failure {self.log_label} - exc={use_exc} \'\n                f\'args={args} kwargs={kwargs}\')\n        else:\n            log.error(f\'on_failure {self.log_label} - exc={use_exc}\')\n        if ae_consts.ev(\'PROD_SLACK_ALERTS\', \'0\') == \'1\':\n            slack_utils.post_failure([f\'on_failure {self.log_label}\',\n                                      f\'exc={use_exc}\'])\n    # end of on_failure\n\n# end of CustomTask\n'"
analysis_engine/work_tasks/get_celery_app.py,0,"b'""""""\nGet a Celery Application Helper\n""""""\n\nimport os\nimport celery\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef get_celery_app(\n        name=os.getenv(\n            \'APP_NAME\',\n            \'worker\'),\n        auth_url=os.getenv(\n            \'WORKER_BROKER_URL\',\n            \'redis://localhost:6379/11\'),\n        backend_url=os.getenv(\n            \'WORKER_BACKEND_URL\',\n            \'redis://localhost:6379/12\'),\n        include_tasks=[],\n        ssl_options=None,\n        transport_options=None,\n        path_to_config_module=os.getenv(\n            \'WORKER_CELERY_CONFIG_MODULE\',\n            \'analysis_engine.work_tasks.celery_config\'),\n        worker_log_format=os.getenv(\n            \'WORKER_LOG_FORMAT\',\n            \'%(asctime)s: %(levelname)s %(message)s\'),\n        **kwargs):\n    """"""get_celery_app\n\n    Build a Celery app with support for environment variables\n    to set endpoints locations.\n\n    - export WORKER_BROKER_URL=redis://localhost:6379/11\n    - export WORKER_BACKEND_URL=redis://localhost:6379/12\n    - export WORKER_CELERY_CONFIG_MODULE=analysis_engine.work_tasks.cel\n      ery_config\n\n    .. note:: Jupyter notebooks need to use the\n        ``WORKER_CELERY_CONFIG_MODULE=analysis_engine.work_tasks.celery\n        service_config`` value which uses resolvable hostnames with\n        docker compose:\n\n        - export WORKER_BROKER_URL=redis://redis:6379/11\n        - export WORKER_BACKEND_URL=redis://redis:6379/12\n\n    :param name: name for this app\n    :param auth_url: Celery broker address\n        (default is ``redis://localhost:6379/11``\n        or ``analysis_engine.consts.WORKER_BROKER_URL``\n        environment variable)\n        this is required for distributing algorithms\n    :param backend_url: Celery backend address\n        (default is ``redis://localhost:6379/12``\n        or ``analysis_engine.consts.WORKER_BACKEND_URL``\n        environment variable)\n        this is required for distributing algorithms\n    :param include_tasks: list of modules containing tasks to add\n    :param ssl_options: security options dictionary\n        (default is ``analysis_engine.consts.SSL_OPTIONS``)\n    :param trasport_options: transport options dictionary\n        (default is ``analysis_engine.consts.TRANSPORT_OPTIONS``)\n    :param path_to_config_module: config module for advanced\n        Celery worker connectivity requirements\n        (default is ``analysis_engine.work_tasks.celery_config``\n        or ``analysis_engine.consts.WORKER_CELERY_CONFIG_MODULE``)\n    :param worker_log_format: format for logs\n    """"""\n\n    if len(include_tasks) == 0:\n        log.error(f\'creating celery app={name} MISSING tasks={include_tasks}\')\n    else:\n        log.info(f\'creating celery app={name} tasks={include_tasks}\')\n\n    # get the Celery application\n    app = celery.Celery(\n        name,\n        broker_url=auth_url,\n        result_backend=backend_url,\n        include=include_tasks)\n\n    app.config_from_object(\n        path_to_config_module,\n        namespace=\'CELERY\')\n\n    app.conf.update(kwargs)\n\n    if transport_options:\n        log.info(f\'loading transport_options={transport_options}\')\n        app.conf.update(**transport_options)\n    # custom tranport options\n\n    if ssl_options:\n        log.info(f\'loading ssl_options={ssl_options}\')\n        app.conf.update(**ssl_options)\n    # custom ssl options\n\n    if len(include_tasks) > 0:\n        app.autodiscover_tasks(include_tasks)\n\n    return app\n# end of get_celery_app\n'"
analysis_engine/work_tasks/get_new_pricing_data.py,0,"b'""""""\n**Get New Pricing Data Task**\n\nThis will fetch data (pricing, financials, earnings, dividends, options,\nand more) from these sources:\n\n#.  IEX\n\n#.  Tradier\n\n#.  Yahoo - disabled as of 2019/01/03\n\n**Detailed example for getting new pricing data**\n\n.. code-block:: python\n\n    import datetime\n    import build_get_new_pricing_request\n    from analysis_engine.api_requests\n    from analysis_engine.work_tasks.get_new_pricing_data\n        import get_new_pricing_data\n\n    # store data\n    cur_date = datetime.datetime.now().strftime(\'%Y-%m-%d\')\n    work = build_get_new_pricing_request(\n        label=f\'get-pricing-{cur_date}\')\n    work[\'ticker\'] = \'TSLA\'\n    work[\'s3_bucket\'] = \'pricing\'\n    work[\'s3_key\'] = f\'{work[""ticker""]}-{cur_date}\'\n    work[\'redis_key\'] = f\'{work[""ticker""]}-{cur_date}\'\n    work[\'celery_disabled\'] = True\n    res = get_new_pricing_data(\n        work)\n    print(\'full result dictionary:\')\n    print(res)\n    if res[\'data\']:\n        print(\n            \'named datasets returned as \'\n            \'json-serialized pandas DataFrames:\')\n        for k in res[\'data\']:\n            print(f\' - {k}\')\n\n.. warning:: When fetching pricing data from sources like IEX,\n    Please ensure the returned values are\n    not serialized pandas Dataframes to prevent\n    issues with celery task results. Instead\n    it is preferred to returned a ``df.to_json()``\n    before sending the results into the\n    results backend.\n\n.. tip:: This task uses the `analysis_engine.work_tasks.\n    custom_task.CustomTask class <https://github.com/A\n    lgoTraders/stock-analysis-engine/blob/master/anal\n    ysis_engine/work_tasks/custom_task.py>`__ for\n    task event handling.\n\n**Sample work_dict request for this method**\n\n`analysis_engine.api_requests.build_get_new_pricing_request <https://\ngithub.com/AlgoTraders/stock-analysis-engine/blob/master/\nanalysis_engine/api_requests.py#L49>`__\n\n**Supported Environment Variables**\n\n::\n\n    export DEBUG_RESULTS=1\n\n""""""\n\nimport datetime\nimport copy\nimport celery\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.utils as ae_utils\nimport analysis_engine.iex.consts as iex_consts\nimport analysis_engine.iex.get_pricing_on_date as iex_pricing\nimport analysis_engine.td.consts as td_consts\nimport analysis_engine.build_result as build_result\nimport analysis_engine.get_task_results as get_task_results\nimport analysis_engine.work_tasks.custom_task as custom_task\nimport analysis_engine.options_dates as opt_dates\nimport analysis_engine.work_tasks.publish_pricing_update as publisher\nimport analysis_engine.yahoo.get_data as yahoo_data\nimport analysis_engine.iex.get_data as iex_data\nimport analysis_engine.td.get_data as td_data\nimport analysis_engine.send_to_slack as slack_utils\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\n@celery.task(\n    bind=True,\n    base=custom_task.CustomTask,\n    queue=\'get_new_pricing_data\')\ndef get_new_pricing_data(\n        self,\n        work_dict):\n    """"""get_new_pricing_data\n\n    Get Ticker information on:\n\n    - prices - turn off with ``work_dict.get_pricing = False``\n    - news - turn off with ``work_dict.get_news = False``\n    - options - turn off with ``work_dict.get_options = False``\n\n    :param work_dict: dictionary for key/values\n    """"""\n\n    label = \'get_new_pricing_data\'\n\n    log.debug(\n        f\'task - {label} - start \'\n        f\'work_dict={work_dict}\')\n\n    num_success = 0\n    ticker = ae_consts.TICKER\n    ticker_id = ae_consts.TICKER_ID\n    rec = {\n        \'pricing\': None,\n        \'options\': None,\n        \'calls\': None,\n        \'puts\': None,\n        \'news\': None,\n        \'daily\': None,\n        \'minute\': None,\n        \'quote\': None,\n        \'stats\': None,\n        \'peers\': None,\n        \'iex_news\': None,\n        \'financials\': None,\n        \'earnings\': None,\n        \'dividends\': None,\n        \'company\': None,\n        \'exp_date\': None,\n        \'publish_pricing_update\': None,\n        \'num_success\': num_success,\n        \'date\': ae_utils.utc_now_str(),\n        \'updated\': None,\n        \'version\': ae_consts.DATASET_COLLECTION_VERSION\n    }\n    res = {\n        \'status\': ae_consts.NOT_RUN,\n        \'err\': None,\n        \'rec\': rec\n    }\n\n    try:\n        ticker = work_dict.get(\n            \'ticker\',\n            ticker)\n        ticker_id = work_dict.get(\n            \'ticker_id\',\n            ae_consts.TICKER_ID)\n        s3_bucket = work_dict.get(\n            \'s3_bucket\',\n            ae_consts.S3_BUCKET)\n        s3_key = work_dict.get(\n            \'s3_key\',\n            ae_consts.S3_KEY)\n        redis_key = work_dict.get(\n            \'redis_key\',\n            ae_consts.REDIS_KEY)\n        exp_date = work_dict.get(\n            \'exp_date\',\n            None)\n        cur_date = ae_utils.last_close()\n        cur_strike = work_dict.get(\n            \'strike\',\n            None)\n        contract_type = str(work_dict.get(\n            \'contract\',\n            \'C\')).upper()\n        label = work_dict.get(\n            \'label\',\n            label)\n        iex_datasets = work_dict.get(\n            \'iex_datasets\',\n            iex_consts.DEFAULT_FETCH_DATASETS)\n        td_datasets = work_dict.get(\n            \'td_datasets\',\n            td_consts.DEFAULT_FETCH_DATASETS_TD)\n        fetch_mode = work_dict.get(\n            \'fetch_mode\',\n            ae_consts.FETCH_MODE_ALL)\n        iex_token = work_dict.get(\n            \'iex_token\',\n            iex_consts.IEX_TOKEN)\n        td_token = work_dict.get(\n            \'td_token\',\n            td_consts.TD_TOKEN)\n        str_fetch_mode = str(fetch_mode).lower()\n        backfill_date = work_dict.get(\n            \'backfill_date\',\n            None)\n        verbose = work_dict.get(\n            \'verbose\',\n            False)\n\n        # control flags to deal with feed issues:\n        get_iex_data = True\n        get_td_data = True\n\n        if (\n                fetch_mode == ae_consts.FETCH_MODE_ALL\n                or str_fetch_mode == \'initial\'):\n            get_iex_data = True\n            get_td_data = True\n            iex_datasets = ae_consts.IEX_INITIAL_DATASETS\n        elif (\n                fetch_mode == ae_consts.FETCH_MODE_ALL\n                or str_fetch_mode == \'all\'):\n            get_iex_data = True\n            get_td_data = True\n            iex_datasets = ae_consts.IEX_DATASETS_DEFAULT\n        elif (\n                fetch_mode == ae_consts.FETCH_MODE_YHO\n                or str_fetch_mode == \'yahoo\'):\n            get_iex_data = False\n            get_td_data = False\n        elif (\n                fetch_mode == ae_consts.FETCH_MODE_IEX\n                or str_fetch_mode == \'iex-all\'):\n            get_iex_data = True\n            get_td_data = False\n            iex_datasets = ae_consts.IEX_DATASETS_DEFAULT\n        elif (\n                fetch_mode == ae_consts.FETCH_MODE_IEX\n                or str_fetch_mode == \'iex\'):\n            get_iex_data = True\n            get_td_data = False\n            iex_datasets = ae_consts.IEX_INTRADAY_DATASETS\n        elif (\n                fetch_mode == ae_consts.FETCH_MODE_INTRADAY\n                or str_fetch_mode == \'intra\'):\n            get_iex_data = True\n            get_td_data = True\n            iex_datasets = ae_consts.IEX_INTRADAY_DATASETS\n        elif (\n                fetch_mode == ae_consts.FETCH_MODE_DAILY\n                or str_fetch_mode == \'daily\'):\n            get_iex_data = True\n            get_td_data = False\n            iex_datasets = ae_consts.IEX_DAILY_DATASETS\n        elif (\n                fetch_mode == ae_consts.FETCH_MODE_WEEKLY\n                or str_fetch_mode == \'weekly\'):\n            get_iex_data = True\n            get_td_data = False\n            iex_datasets = ae_consts.IEX_WEEKLY_DATASETS\n        elif (\n                fetch_mode == ae_consts.FETCH_MODE_TD\n                or str_fetch_mode == \'td\'):\n            get_iex_data = False\n            get_td_data = True\n        else:\n            get_iex_data = False\n            get_td_data = False\n\n            fetch_arr = str_fetch_mode.split(\',\')\n            found_fetch = False\n            iex_datasets = []\n            for fetch_name in fetch_arr:\n                if fetch_name not in iex_datasets:\n                    if fetch_name == \'iex_min\':\n                        iex_datasets.append(\'minute\')\n                    elif fetch_name == \'min\':\n                        iex_datasets.append(\'minute\')\n                    elif fetch_name == \'minute\':\n                        iex_datasets.append(\'minute\')\n                    elif fetch_name == \'day\':\n                        iex_datasets.append(\'daily\')\n                    elif fetch_name == \'daily\':\n                        iex_datasets.append(\'daily\')\n                    elif fetch_name == \'iex_day\':\n                        iex_datasets.append(\'daily\')\n                    elif fetch_name == \'quote\':\n                        iex_datasets.append(\'quote\')\n                    elif fetch_name == \'iex_quote\':\n                        iex_datasets.append(\'quote\')\n                    elif fetch_name == \'iex_stats\':\n                        iex_datasets.append(\'stats\')\n                    elif fetch_name == \'stats\':\n                        iex_datasets.append(\'stats\')\n                    elif fetch_name == \'peers\':\n                        iex_datasets.append(\'peers\')\n                    elif fetch_name == \'iex_peers\':\n                        iex_datasets.append(\'peers\')\n                    elif fetch_name == \'news\':\n                        iex_datasets.append(\'news\')\n                    elif fetch_name == \'iex_news\':\n                        iex_datasets.append(\'news\')\n                    elif fetch_name == \'fin\':\n                        iex_datasets.append(\'financials\')\n                    elif fetch_name == \'iex_fin\':\n                        iex_datasets.append(\'financials\')\n                    elif fetch_name == \'earn\':\n                        iex_datasets.append(\'earnings\')\n                    elif fetch_name == \'iex_earn\':\n                        iex_datasets.append(\'earnings\')\n                    elif fetch_name == \'div\':\n                        iex_datasets.append(\'dividends\')\n                    elif fetch_name == \'iex_div\':\n                        iex_datasets.append(\'dividends\')\n                    elif fetch_name == \'comp\':\n                        iex_datasets.append(\'company\')\n                    elif fetch_name == \'iex_comp\':\n                        iex_datasets.append(\'company\')\n                    elif fetch_name == \'td\':\n                        get_td_data = True\n                    else:\n                        log.warn(\n                            \'unsupported IEX dataset \'\n                            f\'{fetch_name}\')\n            found_fetch = (\n                len(iex_datasets) != 0)\n            if not found_fetch:\n                log.error(\n                    f\'{label} - unsupported \'\n                    f\'fetch_mode={fetch_mode} value\')\n            else:\n                get_iex_data = True\n                log.debug(\n                    f\'{label} - \'\n                    f\'fetching={len(iex_datasets)} \'\n                    f\'{iex_datasets} \'\n                    f\'fetch_mode={fetch_mode}\')\n        # end of screening custom fetch_mode settings\n\n        num_tokens = 0\n\n        if get_iex_data:\n            if not iex_token:\n                log.warn(\n                    f\'{label} - \'\n                    \'please set a valid IEX Cloud Account token (\'\n                    \'https://iexcloud.io/cloud-login/#/register\'\n                    \') to fetch data from IEX Cloud. It must be \'\n                    \'set as an environment variable like: \'\n                    \'export IEX_TOKEN=<token>\')\n                get_iex_data = False\n            else:\n                num_tokens += 1\n        # sanity check - disable IEX fetch if the token is not set\n        if get_td_data:\n            missing_td_token = [\n                \'MISSING_TD_TOKEN\',\n                \'SETYOURTDTOKEN\',\n                \'SETYOURTRADIERTOKENHERE\'\n            ]\n            if td_token in missing_td_token:\n                log.warn(\n                    f\'{label} - \'\n                    \'please set a valid Tradier Account token (\'\n                    \'https://developer.tradier.com/user/sign_up\'\n                    \') to fetch pricing data from Tradier. It must be \'\n                    \'set as an environment variable like: \'\n                    \'export TD_TOKEN=<token>\')\n                get_td_data = False\n            else:\n                num_tokens += 1\n        # sanity check - disable Tradier fetch if the token is not set\n\n        """"""\n        as of Thursday, Jan. 3, 2019:\n        https://developer.yahoo.com/yql/\n        Important EOL Notice: As of Thursday, Jan. 3, 2019\n        the YQL service at query.yahooapis.com will be retired\n        """"""\n        get_yahoo_data = False\n\n        if (\n                not get_iex_data and\n                not get_td_data and\n                not get_yahoo_data):\n            err = None\n            if num_tokens == 0:\n                res[\'status\'] = ae_consts.MISSING_TOKEN\n                err = (\n                    f\'Please set a valid IEX_TOKEN or TD_TOKEN \'\n                    f\'environment variable\')\n            else:\n                err = (\n                    f\'Please set at least one supported datafeed from \'\n                    f\'either: \'\n                    f\'IEX Cloud (fetch -t TICKER -g iex) or \'\n                    f\'Tradier (fetch -t TICKER -g td) \'\n                    f\'for \'\n                    f\'ticker={ticker} \'\n                    f\'cur_date={cur_date} \'\n                    f\'IEX enabled={get_iex_data} \'\n                    f\'TD enabled={get_td_data} \'\n                    f\'YHO enabled={get_yahoo_data}\')\n                res[\'status\'] = ae_consts.ERR\n                res[\'err\'] = err\n            return get_task_results.get_task_results(\n                work_dict=work_dict,\n                result=res)\n        # end of checking that there is at least 1 feed on\n\n        if not exp_date:\n            exp_date = opt_dates.option_expiration(\n                date=exp_date)\n        else:\n            exp_date = datetime.datetime.strptime(\n                exp_date,\n                \'%Y-%m-%d\')\n\n        rec[\'updated\'] = cur_date.strftime(\'%Y-%m-%d %H:%M:%S\')\n        log.debug(\n            f\'{label} getting pricing for ticker={ticker} \'\n            f\'cur_date={cur_date} exp_date={exp_date} \'\n            f\'IEX={get_iex_data} \'\n            f\'TD={get_td_data} \'\n            f\'YHO={get_yahoo_data}\')\n\n        yahoo_rec = {\n            \'ticker\': ticker,\n            \'pricing\': None,\n            \'options\': None,\n            \'calls\': None,\n            \'puts\': None,\n            \'news\': None,\n            \'exp_date\': None,\n            \'publish_pricing_update\': None,\n            \'date\': None,\n            \'updated\': None\n        }\n\n        # disabled on 2019-01-03\n        if get_yahoo_data:\n            log.debug(f\'{label} YHO ticker={ticker}\')\n            yahoo_res = yahoo_data.get_data_from_yahoo(\n                work_dict=work_dict)\n            status_str = ae_consts.get_status(status=yahoo_res[\'status\'])\n            if yahoo_res[\'status\'] == ae_consts.SUCCESS:\n                yahoo_rec = yahoo_res[\'rec\']\n                msg = (\n                    f\'{label} YHO ticker={ticker} \'\n                    f\'redis_key={redis_key}_[price,news,options] \'\n                    f\'status={status_str} err={yahoo_res[""err""]}\')\n                if ae_consts.ev(\'SHOW_SUCCESS\', \'0\') == \'1\':\n                    log.info(msg)\n                else:\n                    log.debug(msg)\n                rec[\'pricing\'] = yahoo_rec.get(\'pricing\', \'{}\')\n                rec[\'news\'] = yahoo_rec.get(\'news\', \'{}\')\n                rec[\'options\'] = yahoo_rec.get(\'options\', \'{}\')\n                rec[\'calls\'] = rec[\'options\'].get(\n                    \'calls\', ae_consts.EMPTY_DF_STR)\n                rec[\'puts\'] = rec[\'options\'].get(\n                    \'puts\', ae_consts.EMPTY_DF_STR)\n                num_success += 1\n            else:\n                log.error(\n                    f\'{label} failed YHO ticker={ticker} \'\n                    f\'status={status_str} err={yahoo_res[""err""]}\')\n        # end of get from yahoo\n\n        if get_iex_data:\n            num_iex_ds = len(iex_datasets)\n            log.debug(f\'{label} IEX datasets={num_iex_ds}\')\n            for idx, ft_type in enumerate(iex_datasets):\n                dataset_field = iex_consts.get_ft_str(\n                    ft_type=ft_type)\n\n                log.debug(\n                    f\'{label} IEX={idx}/{num_iex_ds} \'\n                    f\'field={dataset_field} ticker={ticker}\')\n                iex_label = f\'{label}-{dataset_field}\'\n                iex_req = copy.deepcopy(work_dict)\n                iex_req[\'label\'] = iex_label\n                iex_req[\'ft_type\'] = ft_type\n                iex_req[\'field\'] = dataset_field\n                iex_req[\'ticker\'] = ticker\n                iex_req[\'backfill_date\'] = backfill_date\n                iex_req[\'verbose\'] = verbose\n\n                iex_res = iex_data.get_data_from_iex(\n                    work_dict=iex_req)\n\n                status_str = (\n                    ae_consts.get_status(status=iex_res[\'status\']))\n                if iex_res[\'status\'] == ae_consts.SUCCESS:\n                    iex_rec = iex_res[\'rec\']\n                    rk = f\'{redis_key}_{dataset_field}\'\n                    if backfill_date:\n                        rk = (\n                            f\'{ticker}_{backfill_date}_\'\n                            f\'{dataset_field}\')\n                    msg = (\n                        f\'{label} IEX ticker={ticker} \'\n                        f\'redis_key={rk} \'\n                        f\'field={dataset_field} \'\n                        f\'status={status_str} \'\n                        f\'err={iex_res[""err""]}\')\n                    if ae_consts.ev(\'SHOW_SUCCESS\', \'0\') == \'1\':\n                        log.info(msg)\n                    else:\n                        log.debug(msg)\n                    if dataset_field == \'news\':\n                        rec[\'iex_news\'] = iex_rec[\'data\']\n                    else:\n                        rec[dataset_field] = iex_rec[\'data\']\n                    num_success += 1\n                else:\n                    log.debug(\n                        f\'{label} failed IEX ticker={ticker} \'\n                        f\'field={dataset_field} \'\n                        f\'status={status_str} err={iex_res[""err""]}\')\n                # end of if/else succcess\n            # end idx, ft_type in enumerate(iex_datasets):\n        # end of if get_iex_data\n\n        if get_td_data:\n            num_td_ds = len(td_datasets)\n\n            latest_pricing = None\n            try:\n                latest_pricing = iex_pricing.get_pricing_on_date(\n                    ticker=ticker,\n                    date_str=None,\n                    label=label)\n            except Exception as e:\n                log.critical(\n                    f\'failed to get {ticker} iex latest pricing data \'\n                    f\'with ex={e}\')\n            # end of trying to extract the latest pricing data from redis\n\n            log.debug(\n                f\'{label} TD datasets={num_td_ds} \'\n                f\'pricing={latest_pricing}\')\n\n            for idx, ft_type in enumerate(td_datasets):\n                dataset_field = td_consts.get_ft_str_td(\n                    ft_type=ft_type)\n                log.debug(\n                    f\'{label} TD={idx}/{num_td_ds} \'\n                    f\'field={dataset_field} ticker={ticker}\')\n                td_label = (\n                    f\'{label}-{dataset_field}\')\n                td_req = copy.deepcopy(work_dict)\n                td_req[\'label\'] = td_label\n                td_req[\'ft_type\'] = ft_type\n                td_req[\'field\'] = dataset_field\n                td_req[\'ticker\'] = ticker\n                td_req[\'latest_pricing\'] = latest_pricing\n                td_res = td_data.get_data_from_td(\n                    work_dict=td_req)\n\n                status_str = (\n                    ae_consts.get_status(status=td_res[\'status\']))\n                if td_res[\'status\'] == ae_consts.SUCCESS:\n                    td_rec = td_res[\'rec\']\n                    msg = (\n                        f\'{label} TD ticker={ticker} \'\n                        f\'redis_key={redis_key}_{dataset_field} \'\n                        f\'field={dataset_field} \'\n                        f\'status={status_str} \'\n                        f\'err={td_res[""err""]}\')\n                    if ae_consts.ev(\'SHOW_SUCCESS\', \'0\') == \'1\':\n                        log.info(msg)\n                    else:\n                        log.debug(msg)\n                    if dataset_field == \'tdcalls\':\n                        rec[\'tdcalls\'] = td_rec[\'data\']\n                    if dataset_field == \'tdputs\':\n                        rec[\'tdputs\'] = td_rec[\'data\']\n                    else:\n                        rec[dataset_field] = td_rec[\'data\']\n                    num_success += 1\n                else:\n                    log.critical(\n                        f\'{label} failed TD ticker={ticker} \'\n                        f\'field={dataset_field} \'\n                        f\'status={status_str} err={td_res[""err""]}\')\n                # end of if/else succcess\n            # end idx, ft_type in enumerate(td_datasets):\n        # end of if get_td_data\n\n        rec[\'num_success\'] = num_success\n\n        update_req = {\n            \'data\': rec\n        }\n        update_req[\'ticker\'] = ticker\n        update_req[\'ticker_id\'] = ticker_id\n        update_req[\'strike\'] = cur_strike\n        update_req[\'contract\'] = contract_type\n        update_req[\'s3_enabled\'] = work_dict.get(\n            \'s3_enabled\',\n            ae_consts.ENABLED_S3_UPLOAD)\n        update_req[\'redis_enabled\'] = work_dict.get(\n            \'redis_enabled\',\n            ae_consts.ENABLED_REDIS_PUBLISH)\n        update_req[\'s3_bucket\'] = s3_bucket\n        update_req[\'s3_key\'] = s3_key\n        update_req[\'s3_access_key\'] = work_dict.get(\n            \'s3_access_key\',\n            ae_consts.S3_ACCESS_KEY)\n        update_req[\'s3_secret_key\'] = work_dict.get(\n            \'s3_secret_key\',\n            ae_consts.S3_SECRET_KEY)\n        update_req[\'s3_region_name\'] = work_dict.get(\n            \'s3_region_name\',\n            ae_consts.S3_REGION_NAME)\n        update_req[\'s3_address\'] = work_dict.get(\n            \'s3_address\',\n            ae_consts.S3_ADDRESS)\n        update_req[\'s3_secure\'] = work_dict.get(\n            \'s3_secure\',\n            ae_consts.S3_SECURE)\n        update_req[\'redis_key\'] = redis_key\n        update_req[\'redis_address\'] = work_dict.get(\n            \'redis_address\',\n            ae_consts.REDIS_ADDRESS)\n        update_req[\'redis_password\'] = work_dict.get(\n            \'redis_password\',\n            ae_consts.REDIS_PASSWORD)\n        update_req[\'redis_db\'] = int(work_dict.get(\n            \'redis_db\',\n            ae_consts.REDIS_DB))\n        update_req[\'redis_expire\'] = work_dict.get(\n            \'redis_expire\',\n            ae_consts.REDIS_EXPIRE)\n        update_req[\'updated\'] = rec[\'updated\']\n        update_req[\'label\'] = label\n        update_req[\'celery_disabled\'] = True\n        update_status = ae_consts.NOT_SET\n\n        if backfill_date:\n            update_req[\'redis_key\'] = (\n                f\'{ticker}_{backfill_date}\')\n            update_req[\'s3_key\'] = (\n                f\'{ticker}_{backfill_date}\')\n\n        try:\n            update_res = publisher.run_publish_pricing_update(\n                work_dict=update_req)\n            update_status = update_res.get(\n                \'status\',\n                ae_consts.NOT_SET)\n            status_str = ae_consts.get_status(status=update_status)\n            if ae_consts.ev(\'DEBUG_RESULTS\', \'0\') == \'1\':\n                log.debug(\n                    f\'{label} update_res \'\n                    f\'status={status_str} \'\n                    f\'data={ae_consts.ppj(update_res)}\')\n            else:\n                log.debug(\n                    f\'{label} run_publish_pricing_update \'\n                    f\'status={status_str}\')\n            # end of if/else\n\n            rec[\'publish_pricing_update\'] = update_res\n            res = build_result.build_result(\n                status=ae_consts.SUCCESS,\n                err=None,\n                rec=rec)\n        except Exception as f:\n            err = (\n                f\'{label} publisher.run_publish_pricing_update failed \'\n                f\'with ex={f}\')\n            log.error(err)\n            res = build_result.build_result(\n                status=ae_consts.ERR,\n                err=err,\n                rec=rec)\n        # end of trying to publish results to connected services\n\n    except Exception as e:\n        res = build_result.build_result(\n            status=ae_consts.ERR,\n            err=(\n                \'failed - get_new_pricing_data \'\n                f\'dict={work_dict} with ex={e}\'),\n            rec=rec)\n        log.error(\n            f\'{label} - {res[""err""]}\')\n    # end of try/ex\n\n    if ae_consts.ev(\'DATASET_COLLECTION_SLACK_ALERTS\', \'0\') == \'1\':\n        env_name = \'DEV\'\n        if ae_consts.ev(\'PROD_SLACK_ALERTS\', \'1\') == \'1\':\n            env_name = \'PROD\'\n        done_msg = (\n            f\'Dataset collected ticker=*{ticker}* on \'\n            f\'env=*{env_name}* \'\n            f\'redis_key={redis_key} s3_key={s3_key} \'\n            f\'IEX={get_iex_data} \'\n            f\'TD={get_td_data} \'\n            f\'YHO={get_yahoo_data}\')\n        log.debug(f\'{label} sending slack msg={done_msg}\')\n        if res[\'status\'] == ae_consts.SUCCESS:\n            slack_utils.post_success(\n                msg=done_msg,\n                block=False,\n                jupyter=True)\n        else:\n            slack_utils.post_failure(\n                msg=done_msg,\n                block=False,\n                jupyter=True)\n        # end of if/else success\n    # end of publishing to slack\n\n    log.debug(\n        \'task - get_new_pricing_data done - \'\n        f\'{label} - status={ae_consts.get_status(res[""status""])}\')\n\n    return get_task_results.get_task_results(\n        work_dict=work_dict,\n        result=res)\n# end of get_new_pricing_data\n\n\ndef run_get_new_pricing_data(\n        work_dict):\n    """"""run_get_new_pricing_data\n\n    Celery wrapper for running without celery\n\n    :param work_dict: task data\n    """"""\n\n    label = work_dict.get(\n        \'label\',\n        \'\')\n\n    log.debug(f\'run_get_new_pricing_data - {label} - start\')\n\n    response = build_result.build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec={})\n    task_res = {}\n\n    # allow running without celery\n    if ae_consts.is_celery_disabled(\n            work_dict=work_dict):\n        work_dict[\'celery_disabled\'] = True\n        task_res = get_new_pricing_data(\n            work_dict)\n        if task_res:\n            response = task_res.get(\n                \'result\',\n                task_res)\n            if ae_consts.ev(\'DEBUG_RESULTS\', \'0\') == \'1\':\n                response_details = response\n                try:\n                    response_details = ae_consts.ppj(response)\n                except Exception:\n                    response_details = response\n\n                log.debug(\n                    f\'{label} task result={response_details}\')\n        else:\n            log.error(\n                f\'{label} celery was disabled but the task={response} \'\n                \'did not return anything\')\n        # end of if response\n    else:\n        task_res = get_new_pricing_data.delay(\n            work_dict=work_dict)\n        rec = {\n            \'task_id\': task_res\n        }\n        response = build_result.build_result(\n            status=ae_consts.SUCCESS,\n            err=None,\n            rec=rec)\n    # if celery enabled\n\n    if response:\n        status_str = ae_consts.get_status(response[\'status\'])\n        if ae_consts.ev(\'DEBUG_RESULTS\', \'0\') == \'1\':\n            log.debug(\n                f\'run_get_new_pricing_data - {label} - done \'\n                f\'status={status_str} \'\n                f\'err={response[""err""]} \'\n                f\'rec={response[""rec""]}\')\n        else:\n            log.debug(\n                f\'run_get_new_pricing_data - {label} - done \'\n                f\'status={status_str} \'\n                f\'rec={response[""rec""]}\')\n    else:\n        log.debug(\n            f\'run_get_new_pricing_data - {label} - done \'\n            \'no response\')\n    # end of if/else response\n\n    return response\n# end of run_get_new_pricing_data\n'"
analysis_engine/work_tasks/handle_pricing_update_task.py,0,"b'""""""\n**Handle Pricing Update Task**\n\nGet the latest stock news, quotes and options chains for a\nticker and publish the values to redis and S3 for downstream analysis.\n\nWrites pricing updates to S3 and Redis by\nbuilding a list of publishing sub-task:\n\n**Sample work_dict request for this method**\n\n`analysis_engine.api_requests.publish_pricing_update <https://\ngithub.com/AlgoTraders/stock-analysis-engine/blob/master/\nanalysis_engine/api_requests.py#L218>`__\n\n::\n\n    work = {\n        \'ticker\': ticker,\n        \'ticker_id\': ticker_id,\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'prepared_s3_key\': s3_prepared_key,\n        \'prepared_s3_bucket\': s3_prepared_bucket_name,\n        \'prepared_redis_key\': redis_prepared_key,\n        \'ignore_columns\': ignore_columns,\n        \'s3_enabled\': s3_enabled,\n        \'redis_enabled\': redis_enabled\n    }\n\n.. tip:: This task uses the `analysis_engine.work_tasks.\n    custom_task.CustomTask class <https://github.com/A\n    lgoTraders/stock-analysis-engine/blob/master/anal\n    ysis_engine/work_tasks/custom_task.py>`__ for\n    task event handling.\n\n**Supported Environment Variables**\n\n::\n\n    export DEBUG_RESULTS=1\n\n""""""\n\nimport datetime\nimport celery.task as celery_task\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.get_task_results as get_task_results\nimport analysis_engine.work_tasks.custom_task as custom_task\nimport analysis_engine.build_result as build_result\nimport analysis_engine.work_tasks.publish_pricing_update as publisher\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\n@celery_task(\n    bind=True,\n    base=custom_task.CustomTask,\n    queue=\'handle_pricing_update_task\')\ndef handle_pricing_update_task(\n        self,\n        work_dict):\n    """"""handle_pricing_update_task\n\n    Writes pricing updates to S3 and Redis\n\n    :param work_dict: dictionary for key/values\n    """"""\n\n    label = \'update_prices\'\n\n    log.info(f\'task - {label} - start\')\n\n    ticker = ae_consts.TICKER\n    ticker_id = 1\n    rec = {\n        \'ticker\': None,\n        \'ticker_id\': None,\n        \'pricing_s3_bucket\': None,\n        \'pricing_s3_key\': None,\n        \'pricing_size\': None,\n        \'pricing_redis_key\': None,\n        \'news_s3_bucket\': None,\n        \'news_s3_key\': None,\n        \'news_size\': None,\n        \'news_redis_key\': None,\n        \'options_s3_bucket\': None,\n        \'options_s3_key\': None,\n        \'options_size\': None,\n        \'options_redis_key\': None\n    }\n    res = build_result.build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec=rec)\n\n    try:\n        ticker = work_dict.get(\n            \'ticker\',\n            ae_consts.TICKER)\n        ticker_id = int(work_dict.get(\n            \'ticker_id\',\n            1))\n\n        rec[\'ticker\'] = ticker\n        rec[\'ticker_id\'] = ticker_id\n\n        pricing_data = work_dict[\'pricing\']\n        news_data = work_dict[\'news\']\n        options_data = work_dict[\'options\']\n        calls_data = options_data.get(\n            \'calls\',\n            ae_consts.EMPTY_DF_STR)\n        puts_data = options_data.get(\n            \'puts\',\n            ae_consts.EMPTY_DF_STR)\n        updated = work_dict[\'updated\']\n        label = work_dict.get(\n            \'label\',\n            label)\n\n        cur_date = datetime.datetime.utcnow()\n        cur_date_str = cur_date.strftime(\n            \'%Y_%m_%d_%H_%M_%S\')\n\n        pricing_s3_key = work_dict.get(\n            \'pricing_s3_key\',\n            f\'pricing_ticker_{ticker}_id_{ticker_id}_date_{cur_date_str}\')\n        news_s3_key = work_dict.get(\n            \'news_s3_key\',\n            f\'news_ticker_{ticker}_id_{ticker_id}_date_{cur_date_str}\')\n        options_s3_key = work_dict.get(\n            \'options_s3_key\',\n            f\'options_ticker_{ticker}_id_{ticker_id}_date_{cur_date_str}\')\n        calls_s3_key = work_dict.get(\n            \'calls_s3_key\',\n            f\'calls_ticker_{ticker}_id_{ticker_id}_date_{cur_date_str}\')\n        puts_s3_key = work_dict.get(\n            \'puts_s3_key\',\n            f\'puts_ticker_{ticker}_id_{ticker_id}_date_{cur_date_str}\')\n\n        pricing_s3_bucket = work_dict.get(\n            \'pricing_s3_bucket\',\n            \'pricing\')\n        news_s3_bucket = work_dict.get(\n            \'news_s3_bucket\',\n            \'news\')\n        options_s3_bucket = work_dict.get(\n            \'options_s3_bucket\',\n            \'options\')\n\n        pricing_by_ticker_redis_key = work_dict.get(\n            \'pricing_redis_key\',\n            f\'price_{ticker}\')\n        news_by_ticker_redis_key = work_dict.get(\n            \'news_redis_key\',\n            f\'news_{ticker}\')\n        options_by_ticker_redis_key = work_dict.get(\n            \'options_redis_key\',\n            f\'options_{ticker}\')\n        calls_by_ticker_redis_key = work_dict.get(\n            \'calls_redis_key\',\n            f\'calls_{ticker}\')\n        puts_by_ticker_redis_key = work_dict.get(\n            \'puts_redis_key\',\n            f\'puts_{ticker}\')\n\n        pricing_size = len(str(\n            pricing_data))\n        news_size = len(str(\n            news_data))\n        options_size = len(str(\n            options_data))\n        calls_size = len(str(\n            calls_data))\n        puts_size = len(str(\n            puts_data))\n\n        payloads_to_publish = [\n            {\n                \'ticker\': ticker,\n                \'ticker_id\': ticker_id,\n                \'s3_bucket\': pricing_s3_bucket,\n                \'s3_key\': pricing_s3_key,\n                \'data\': pricing_data,\n                \'redis_key\': pricing_by_ticker_redis_key,\n                \'size\': pricing_size,\n                \'updated\': updated,\n                \'label\': label\n            },\n            {\n                \'ticker\': ticker,\n                \'ticker_id\': ticker_id,\n                \'s3_bucket\': options_s3_bucket,\n                \'s3_key\': options_s3_key,\n                \'data\': options_data,\n                \'redis_key\': options_by_ticker_redis_key,\n                \'size\': options_size,\n                \'updated\': updated,\n                \'label\': label\n            },\n            {\n                \'ticker\': ticker,\n                \'ticker_id\': ticker_id,\n                \'s3_bucket\': options_s3_bucket,\n                \'s3_key\': calls_s3_key,\n                \'data\': calls_data,\n                \'redis_key\': calls_by_ticker_redis_key,\n                \'size\': calls_size,\n                \'updated\': updated,\n                \'label\': label\n            },\n            {\n                \'ticker\': ticker,\n                \'ticker_id\': ticker_id,\n                \'s3_bucket\': options_s3_bucket,\n                \'s3_key\': puts_s3_key,\n                \'data\': puts_data,\n                \'redis_key\': puts_by_ticker_redis_key,\n                \'size\': puts_size,\n                \'updated\': updated,\n                \'label\': label\n            },\n            {\n                \'ticker\': ticker,\n                \'ticker_id\': ticker_id,\n                \'s3_bucket\': news_s3_bucket,\n                \'s3_key\': news_s3_key,\n                \'data\': news_data,\n                \'redis_key\': news_by_ticker_redis_key,\n                \'size\': news_size,\n                \'updated\': updated,\n                \'label\': label\n            }\n        ]\n\n        rec[\'pricing_s3_bucket\'] = pricing_s3_bucket\n        rec[\'pricing_s3_key\'] = pricing_s3_key\n        rec[\'pricing_redis_key\'] = pricing_by_ticker_redis_key\n        rec[\'news_s3_bucket\'] = news_s3_bucket\n        rec[\'news_s3_key\'] = news_s3_key\n        rec[\'news_redis_key\'] = news_by_ticker_redis_key\n        rec[\'options_s3_bucket\'] = options_s3_bucket\n        rec[\'options_s3_key\'] = options_s3_bucket\n        rec[\'options_redis_key\'] = options_by_ticker_redis_key\n\n        total_payloads = len(payloads_to_publish)\n\n        log.info(\n            f\'{label} ticker={ticker} processing payloads={total_payloads}\')\n\n        for ridx, r in enumerate(payloads_to_publish):\n            log.info(\n                f\'{label} ticker={ticker} update={ridx}/{total_payloads} \'\n                f\'key={r[""s3_key""]} redis_key={r[""redis_key""]}\')\n            r[\'celery_disabled\'] = False\n            r[\'label\'] = f\'handle_pricing_update_task-{label}\'\n            payload_res = \\\n                publisher.task_publish_pricing_update(\n                    work_dict=r)\n            log.info(\n                f\'{label} ticker={ticker} update={ridx}/{total_payloads} \'\n                f\'status={ae_consts.get_status(status=payload_res[""status""])} \'\n                f\'s3_key={r[""s3_key""]} redis_key={r[""redis_key""]}\')\n        # end of for all payloads to publish\n\n        res = build_result.build_result(\n            status=ae_consts.SUCCESS,\n            err=None,\n            rec=rec)\n\n    except Exception as e:\n        res = build_result.build_result(\n            status=ae_consts.ERR,\n            err=(\n                \'failed - handle_pricing_update_task \'\n                f\'dict={work_dict} with ex={e}\'),\n            rec=rec)\n        log.error(f\'{label} - {res[""err""]}\')\n    # end of try/ex\n\n    log.info(\n        \'task - handle_pricing_update_task done - \'\n        f\'{label} - status={ae_consts.get_status(res[""status""])}\')\n\n    return get_task_results.get_task_results(\n        work_dict=work_dict,\n        result=res)\n# end of handle_pricing_update_task\n\n\ndef run_handle_pricing_update_task(\n        work_dict):\n    """"""run_handle_pricing_update_task\n\n    Celery wrapper for running without celery\n\n    :param work_dict: task data\n    """"""\n\n    label = work_dict.get(\n        \'label\',\n        \'\')\n\n    log.info(f\'run_handle_pricing_update_task - {label} - start\')\n\n    response = build_result.build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec={})\n    task_res = {}\n\n    log.info(\n        f\'run_handle_pricing_update_task - {label} - done \'\n        f\'status={ae_consts.get_status(response[""status""])} \'\n        f\'err={response[""err""]} rec={response[""rec""]}\')\n\n    # allow running without celery\n    if ae_consts.is_celery_disabled(\n            work_dict=work_dict):\n        work_dict[\'celery_disabled\'] = True\n        task_res = handle_pricing_update_task(\n            work_dict)\n        if task_res:\n            response = task_res.get(\n                \'result\',\n                task_res)\n            if ae_consts.ev(\'DEBUG_RESULTS\', \'0\') == \'1\':\n                response_details = response\n                try:\n                    response_details = ae_consts.ppj(response)\n                except Exception:\n                    response_details = response\n                log.info(\n                    f\'{label} handle_pricing_update_task \'\n                    f\'task result={response_details}\')\n        else:\n            log.error(\n                f\'{label} celery was disabled but the task={response} \'\n                \'did not return anything\')\n        # end of if response\n    else:\n        task_res = handle_pricing_update_task.delay(\n            work_dict=work_dict)\n        rec = {\n            \'task_id\': task_res\n        }\n        response = build_result.build_result(\n            status=ae_consts.SUCCESS,\n            err=None,\n            rec=rec)\n    # if celery enabled\n\n    if response:\n        if ae_consts.ev(\'DEBUG_RESULTS\', \'0\') == \'1\':\n            log.info(\n                f\'run_handle_pricing_update_task - {label} - done \'\n                f\'status={ae_consts.get_status(response[""status""])} \'\n                f\'err={response[""err""]} rec={response[""rec""]}\')\n        else:\n            log.info(\n                f\'run_handle_pricing_update_task - {label} - done \'\n                f\'status={ae_consts.get_status(response[""status""])} \'\n                f\'err={response[""err""]}\')\n    else:\n        log.info(\n            f\'run_handle_pricing_update_task - {label} - done \'\n            \'no response\')\n    # end of if/else response\n\n    return response\n# end of run_handle_pricing_update_task\n'"
analysis_engine/work_tasks/prepare_pricing_dataset.py,0,"b'""""""\n**Prepare Pricing Dataset**\n\nPrepare dataset for analysis. This task collapses\nnested json dictionaries into a\ncsv file with a header row and stores the output\nfile in s3 and redis automatically.\n\n- if key not in redis, load the key by the same name from s3\n- prepare dataset from redis key\n- the dataset will be stored as a dictionary with a pandas dataframe\n\n**Sample work_dict request for this method**\n\n`analysis_engine.api_requests.build_prepare_dataset_request <https://\ngithub.com/AlgoTraders/stock-analysis-engine/blob/master/\nanalysis_engine/api_requests.py#L300>`__\n\n::\n\n    work_request = {\n        \'ticker\': ticker,\n        \'ticker_id\': ticker_id,\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'prepared_s3_key\': s3_prepared_key,\n        \'prepared_s3_bucket\': s3_prepared_bucket_name,\n        \'prepared_redis_key\': redis_prepared_key,\n        \'ignore_columns\': ignore_columns,\n        \'s3_enabled\': s3_enabled,\n        \'redis_enabled\': redis_enabled\n    }\n\n.. tip:: This task uses the `analysis_engine.work_tasks.\n    custom_task.CustomTask class <https://github.com/A\n    lgoTraders/stock-analysis-engine/blob/master/anal\n    ysis_engine/work_tasks/custom_task.py>`__ for\n    task event handling.\n\n**Supported Environment Variables**\n\n::\n\n    export DEBUG_PREPARE=1\n    export DEBUG_RESULTS=1\n\n""""""\n\nimport datetime\nimport redis\nimport celery.task as celery_task\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.build_result as build_result\nimport analysis_engine.api_requests as api_requests\nimport analysis_engine.get_data_from_redis_key as redis_get\nimport analysis_engine.get_task_results as get_task_results\nimport analysis_engine.work_tasks.custom_task as custom_task\nimport analysis_engine.work_tasks.publish_from_s3_to_redis as s3_to_redis\nimport analysis_engine.dict_to_csv as dict_to_csv\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\n@celery_task(\n    bind=True,\n    base=custom_task.CustomTask,\n    queue=\'prepare_pricing_dataset\')\ndef prepare_pricing_dataset(\n        self,\n        work_dict):\n    """"""prepare_pricing_dataset\n\n    Prepare dataset for analysis. Supports loading dataset from\n    s3 if not found in redis. Outputs prepared artifact as a csv\n    to s3 and redis.\n\n    :param work_dict: dictionary for key/values\n    """"""\n\n    label = \'prepare\'\n\n    log.info(f\'task - {label} - start work_dict={work_dict}\')\n\n    initial_data = None\n\n    ticker = ae_consts.TICKER\n    ticker_id = ae_consts.TICKER_ID\n    rec = {\n        \'ticker\': None,\n        \'ticker_id\': None,\n        \'s3_enabled\': True,\n        \'redis_enabled\': True,\n        \'s3_bucket\': None,\n        \'s3_key\': None,\n        \'redis_key\': None,\n        \'prepared_s3_key\': None,\n        \'prepared_s3_bucket\': None,\n        \'prepared_redis_key\': None,\n        \'prepared_data\': None,\n        \'prepared_size\': None,\n        \'initial_data\': None,\n        \'initial_size\': None,\n        \'ignore_columns\': None,\n        \'updated\': None\n    }\n    res = build_result.build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec=rec)\n\n    try:\n        ticker = work_dict.get(\n            \'ticker\',\n            ae_consts.TICKER)\n        ticker_id = int(work_dict.get(\n            \'ticker_id\',\n            ae_consts.TICKER_ID))\n\n        if not ticker:\n            res = build_result.build_result(\n                status=ae_consts.ERR,\n                err=\'missing ticker\',\n                rec=rec)\n            return res\n\n        label = work_dict.get(\n            \'label\',\n            label)\n        s3_key = work_dict.get(\n            \'s3_key\',\n            None)\n        s3_bucket_name = work_dict.get(\n            \'s3_bucket\',\n            \'pricing\')\n        s3_access_key = work_dict.get(\n            \'s3_access_key\',\n            ae_consts.S3_ACCESS_KEY)\n        s3_secret_key = work_dict.get(\n            \'s3_secret_key\',\n            ae_consts.S3_SECRET_KEY)\n        s3_region_name = work_dict.get(\n            \'s3_region_name\',\n            ae_consts.S3_REGION_NAME)\n        s3_address = work_dict.get(\n            \'s3_address\',\n            ae_consts.S3_ADDRESS)\n        s3_secure = work_dict.get(\n            \'s3_secure\',\n            ae_consts.S3_SECURE) == \'1\'\n        redis_address = work_dict.get(\n            \'redis_address\',\n            ae_consts.REDIS_ADDRESS)\n        redis_key = work_dict.get(\n            \'redis_key\',\n            ae_consts.REDIS_KEY)\n        redis_password = work_dict.get(\n            \'redis_password\',\n            ae_consts.REDIS_PASSWORD)\n        redis_db = work_dict.get(\n            \'redis_db\',\n            None)\n        if not redis_db:\n            redis_db = ae_consts.REDIS_DB\n        redis_expire = None\n        if \'redis_expire\' in work_dict:\n            redis_expire = work_dict.get(\n                \'redis_expire\',\n                ae_consts.REDIS_EXPIRE)\n        updated = work_dict.get(\n            \'updated\',\n            datetime.datetime.utcnow().strftime(\n                \'%Y_%m_%d_%H_%M_%S\'))\n        prepared_s3_key = work_dict.get(\n            \'prepared_s3_key\',\n            f\'{ticker}_{updated}.csv\')\n        prepared_s3_bucket = work_dict.get(\n            \'prepared_s3_bucket\',\n            \'prepared\')\n        prepared_redis_key = work_dict.get(\n            \'prepared_redis_key\',\n            \'prepared\')\n        ignore_columns = work_dict.get(\n            \'ignore_columns\',\n            None)\n        log.info(\n            f\'{label} redis enabled address={redis_address}@{redis_db} \'\n            f\'key={redis_key} \'\n            f\'prepare_s3={prepared_s3_bucket}:{prepared_s3_key} \'\n            f\'prepare_redis={prepared_redis_key} \'\n            f\'ignore_columns={ignore_columns}\')\n        redis_host = redis_address.split(\':\')[0]\n        redis_port = redis_address.split(\':\')[1]\n\n        enable_s3 = True\n        enable_redis_publish = True\n\n        rec[\'ticker\'] = ticker\n        rec[\'ticker_id\'] = ticker_id\n        rec[\'s3_bucket\'] = s3_bucket_name\n        rec[\'s3_key\'] = s3_key\n        rec[\'redis_key\'] = redis_key\n        rec[\'prepared_s3_key\'] = prepared_s3_key\n        rec[\'prepared_s3_bucket\'] = prepared_s3_bucket\n        rec[\'prepared_redis_key\'] = prepared_redis_key\n        rec[\'updated\'] = updated\n        rec[\'s3_enabled\'] = enable_s3\n        rec[\'redis_enabled\'] = enable_redis_publish\n\n        try:\n            log.info(\n                f\'{label} connecting redis={redis_host}:{redis_port} \'\n                f\'db={redis_db} key={redis_key} \'\n                f\'updated={updated} expire={redis_expire}\')\n            rc = redis.Redis(\n                host=redis_host,\n                port=redis_port,\n                password=redis_password,\n                db=redis_db)\n        except Exception as e:\n            err = (\n                f\'{label} failed - redis connection to \'\n                f\'address={redis_address}@{redis_port} \'\n                f\'db={redis_db} key={redis_key} ex={e}\')\n            res = build_result.build_result(\n                status=ae_consts.ERR,\n                err=err,\n                rec=rec)\n            return res\n        # end of try/ex for connecting to redis\n\n        initial_data_res = redis_get.get_data_from_redis_key(\n            label=label,\n            client=rc,\n            key=redis_key)\n\n        log.info(\n            f\'{label} get redis key={redis_key} \'\n            f\'status={ae_consts.get_status(initial_data_res[""status""])} \'\n            f\'err={initial_data_res[""err""]}\')\n\n        initial_data = initial_data_res[\'rec\'].get(\n            \'data\',\n            None)\n\n        if enable_s3 and not initial_data:\n\n            log.info(\n                f\'{label} failed to find redis_key={redis_key} trying s3 from \'\n                f\'s3_key={s3_key} s3_bucket={s3_bucket_name} \'\n                f\'s3_address={s3_address}\')\n\n            get_from_s3_req = \\\n                api_requests.build_publish_from_s3_to_redis_request()\n\n            get_from_s3_req[\'s3_enabled\'] = enable_s3\n            get_from_s3_req[\'s3_access_key\'] = s3_access_key\n            get_from_s3_req[\'s3_secret_key\'] = s3_secret_key\n            get_from_s3_req[\'s3_region_name\'] = s3_region_name\n            get_from_s3_req[\'s3_address\'] = s3_address\n            get_from_s3_req[\'s3_secure\'] = s3_secure\n            get_from_s3_req[\'s3_key\'] = s3_key\n            get_from_s3_req[\'s3_bucket\'] = s3_bucket_name\n            get_from_s3_req[\'redis_key\'] = redis_key\n            get_from_s3_req[\'label\'] = f\'{label}-run_publish_from_s3_to_redis\'\n\n            log.info(f\'{label} load from s3={s3_key} to redis={redis_key}\')\n\n            try:\n                # run in synchronous mode:\n                get_from_s3_req[\'celery_disabled\'] = True\n                task_res = s3_to_redis.run_publish_from_s3_to_redis(\n                    get_from_s3_req)\n                if task_res.get(\n                        \'status\',\n                        ae_consts.ERR) == ae_consts.SUCCESS:\n                    log.info(\n                        f\'{label} loaded s3={s3_bucket_name}:{s3_key} \'\n                        f\'to redis={redis_key} retrying\')\n                    initial_data_res = redis_get.get_data_from_redis_key(\n                        label=label,\n                        client=rc,\n                        key=redis_key)\n\n                    log.info(\n                        f\'{label} get redis try=2 key={redis_key} status=\'\n                        f\'{ae_consts.get_status(initial_data_res[""status""])} \'\n                        f\'err={initial_data_res[""err""]}\')\n\n                    initial_data = initial_data_res[\'rec\'].get(\n                        \'data\',\n                        None)\n                else:\n                    err = (\n                        f\'{label} ERR failed loading from \'\n                        f\'bucket={s3_bucket_name} s3_key={s3_key} to \'\n                        f\'redis_key={redis_key} with res={task_res}\')\n                    log.error(err)\n                    res = build_result.build_result(\n                        status=ae_consts.ERR,\n                        err=err,\n                        rec=rec)\n                    return res\n            except Exception as e:\n                err = (\n                    f\'{label} extract from s3 and publish to redis failed \'\n                    f\'loading data from bucket={s3_bucket_name} in \'\n                    f\'s3_key={s3_key} with publish to redis_key={redis_key} \'\n                    f\'with ex={e}\')\n                log.error(err)\n                res = build_result.build_result(\n                    status=ae_consts.ERR,\n                    err=err,\n                    rec=rec)\n                return res\n            # end of try/ex for publishing from s3->redis\n        # end of if enable_s3\n\n        if not initial_data:\n            err = (\n                f\'{label} did not find any data to prepare in \'\n                f\'redis_key={redis_key} or s3_key={s3_key} in \'\n                f\'bucket={s3_bucket_name}\')\n            log.error(err)\n            res = build_result.build_result(\n                status=ae_consts.ERR,\n                err=err,\n                rec=rec)\n            return res\n\n        initial_data_num_chars = len(str(initial_data))\n        initial_size_value = None\n        initial_size_str = None\n        if initial_data_num_chars < ae_consts.PREPARE_DATA_MIN_SIZE:\n            err = (\n                f\'{label} not enough data={initial_data_num_chars} in \'\n                f\'redis_key={redis_key} or s3_key={s3_key} in \'\n                f\'bucket={s3_bucket_name}\')\n            log.error(err)\n            res = build_result.build_result(\n                status=ae_consts.ERR,\n                err=err,\n                rec=rec)\n            return res\n        else:\n            initial_size_value = initial_data_num_chars / 1024000\n            initial_size_str = ae_consts.to_f(initial_size_value)\n            if ae_consts.ev(\'DEBUG_PREPARE\', \'0\') == \'1\':\n                log.info(\n                    f\'{label} initial - redis_key={redis_key} \'\n                    f\'data={str(initial_data)}\')\n            else:\n                log.info(\n                    f\'{label} initial - redis_key={redis_key} data \'\n                    f\'size={initial_size_str} MB\')\n        # end of trying to get initial_data\n\n        rec[\'initial_data\'] = initial_data\n        rec[\'initial_size\'] = initial_data_num_chars\n\n        prepare_data = None\n\n        try:\n            if ae_consts.ev(\'DEBUG_PREPARE\', \'0\') == \'1\':\n                log.info(\n                    f\'{label} data={ae_consts.ppj(initial_data)} - flatten - \'\n                    f\'{initial_size_str} MB from redis_key={redis_key}\')\n            else:\n                log.info(\n                    f\'{label} flatten - {initial_size_str} MB from \'\n                    f\'redis_key={redis_key}\')\n            prepare_data = dict_to_csv.flatten_dict(\n                data=initial_data)\n        except Exception as e:\n            prepare_data = None\n            err = (\n                f\'{label} flatten - convert to csv failed with ex={e} \'\n                f\'redis_key={redis_key}\')\n            log.error(err)\n            res = build_result.build_result(\n                status=ae_consts.ERR,\n                err=err,\n                rec=rec)\n            return res\n        # end of try/ex\n\n        if not prepare_data:\n            err = (\n                f\'{label} flatten - did not return any data from \'\n                f\'redis_key={redis_key} or s3_key={s3_key} in \'\n                f\'bucket={s3_bucket_name}\')\n            log.error(err)\n            res = build_result.build_result(\n                status=ae_consts.ERR,\n                err=err,\n                rec=rec)\n            return res\n        # end of prepare_data\n\n        prepare_data_num_chars = len(str(prepare_data))\n        prepare_size_value = None\n\n        if prepare_data_num_chars < ae_consts.PREPARE_DATA_MIN_SIZE:\n            err = (\n                f\'{label} prepare - there is not enough \'\n                f\'data={prepare_data_num_chars} in redis_key={redis_key}\')\n            log.error(err)\n            res = build_result.build_result(\n                status=ae_consts.ERR,\n                err=err,\n                rec=rec)\n            return res\n        else:\n            prepare_size_value = prepare_data_num_chars / 1024000\n            prepare_size_str = ae_consts.to_f(prepare_size_value)\n            if ae_consts.ev(\'DEBUG_PREPARE\', \'0\') == \'1\':\n                log.info(\n                    f\'{label} data={redis_key} - prepare - \'\n                    f\'redis_key={ae_consts.ppj(prepare_data)}\')\n            else:\n                log.info(\n                    f\'{label} prepare - redis_key={redis_key} data \'\n                    f\'size={prepare_size_str} MB\')\n        # end of trying to the size of the prepared data\n\n        rec[\'prepared_data\'] = prepare_data\n        rec[\'prepared_size\'] = prepare_data_num_chars\n\n        res = build_result.build_result(\n            status=ae_consts.SUCCESS,\n            err=None,\n            rec=rec)\n\n        rc = None\n\n    except Exception as e:\n        res = build_result.build_result(\n            status=ae_consts.ERR,\n            err=(\n                \'failed - prepare_pricing_dataset \'\n                f\'dict={work_dict} with ex={e}\'),\n            rec=rec)\n        log.error(f\'{label} - {res[""err""]}\')\n    # end of try/ex\n\n    log.info(\n        \'task - prepare_pricing_dataset done - \'\n        f\'{label} - status={ae_consts.get_status(res[""status""])}\')\n\n    return get_task_results.get_task_results(\n        work_dict=work_dict,\n        result=res)\n# end of prepare_pricing_dataset\n\n\ndef run_prepare_pricing_dataset(\n        work_dict):\n    """"""run_prepare_pricing_dataset\n\n    Celery wrapper for running without celery\n\n    :param work_dict: task data\n    """"""\n\n    label = work_dict.get(\n        \'label\',\n        \'\')\n\n    log.info(f\'run_prepare_pricing_dataset - {label} - start\')\n\n    response = build_result.build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec={})\n    task_res = {}\n\n    # allow running without celery\n    if ae_consts.is_celery_disabled(\n            work_dict=work_dict):\n        work_dict[\'celery_disabled\'] = True\n        task_res = prepare_pricing_dataset(\n            work_dict)\n        if task_res:\n            response = task_res.get(\n                \'result\',\n                task_res)\n            if ae_consts.ev(\'DEBUG_RESULTS\', \'0\') == \'1\':\n                response_details = response\n                try:\n                    response_details = ae_consts.ppj(response)\n                except Exception:\n                    response_details = response\n                log.info(f\'{label} task result={response_details}\')\n        else:\n            log.error(\n                f\'{label} celery was disabled but the task={response} \'\n                \'did not return anything\')\n        # end of if response\n    else:\n        task_res = prepare_pricing_dataset.delay(\n            work_dict=work_dict)\n        rec = {\n            \'task_id\': task_res\n        }\n        response = build_result.build_result(\n            status=ae_consts.SUCCESS,\n            err=None,\n            rec=rec)\n    # if celery enabled\n\n    if response:\n        if ae_consts.ev(\'DEBUG_RESULTS\', \'0\') == \'1\':\n            log.info(\n                f\'run_prepare_pricing_dataset - {label} - done \'\n                f\'status={ae_consts.get_status(response[""status""])} \'\n                f\'err={response[""err""]} rec={response[""rec""]}\')\n        else:\n            log.info(\n                f\'run_prepare_pricing_dataset - {label} - done \'\n                f\'status={ae_consts.get_status(response[""status""])} \'\n                f\'err={response[""err""]}\')\n    else:\n        log.info(\n            f\'run_prepare_pricing_dataset - {label} - done \'\n            \'no response\')\n    # end of if/else response\n\n    return response\n# end of run_prepare_pricing_dataset\n'"
analysis_engine/work_tasks/publish_from_s3_to_redis.py,0,"b'""""""\n**Publish Data from S3 to Redis Task**\n\nPublish S3 key with stock data to redis\nand s3 (if either of them are running and enabled)\n\n- redis - using `redis-py <https://github.com/andymccurdy/redis-py>`__\n- s3 - using boto3\n\n**Sample work_dict request for this method**\n\n`analysis_engine.api_requests.build_publish_from_s3_to_redis_request <https://\ngithub.com/AlgoTraders/stock-analysis-engine/blob/master/\nanalysis_engine/api_requests.py#L386>`__\n\n::\n\n    work_request = {\n        \'ticker\': ticker,\n        \'ticker_id\': ticker_id,\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'s3_enabled\': s3_enabled,\n        \'redis_enabled\': redis_enabled\n    }\n\n.. tip:: This task uses the `analysis_engine.work_tasks.\n    custom_task.CustomTask class <https://github.com/A\n    lgoTraders/stock-analysis-engine/blob/master/anal\n    ysis_engine/work_tasks/custom_task.py>`__ for\n    task event handling.\n\n**Supported Environment Variables**\n\n::\n\n    export DEBUG_RESULTS=1\n\n""""""\n\nimport boto3\nimport redis\nimport celery.task as celery_task\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.build_result as build_result\nimport analysis_engine.get_task_results as get_task_results\nimport analysis_engine.work_tasks.custom_task as custom_task\nimport analysis_engine.set_data_in_redis_key as redis_set\nimport analysis_engine.s3_read_contents_from_key as s3_read_contents_from_key\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\n@celery_task(\n    bind=True,\n    base=custom_task.CustomTask,\n    queue=\'publish_from_s3_to_redis\')\ndef publish_from_s3_to_redis(\n        self,\n        work_dict):\n    """"""publish_from_s3_to_redis\n\n    Publish Ticker Data from S3 to Redis\n\n    :param work_dict: dictionary for key/values\n    """"""\n\n    label = \'pub-s3-to-redis\'\n\n    log.info(f\'task - {label} - start work_dict={work_dict}\')\n\n    ticker = ae_consts.TICKER\n    ticker_id = ae_consts.TICKER_ID\n    rec = {\n        \'ticker\': None,\n        \'ticker_id\': None,\n        \'s3_enabled\': True,\n        \'redis_enabled\': True,\n        \'s3_bucket\': None,\n        \'s3_key\': None,\n        \'redis_key\': None,\n        \'updated\': None\n    }\n    res = build_result.build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec=rec)\n\n    try:\n        ticker = work_dict.get(\n            \'ticker\',\n            ae_consts.TICKER)\n        ticker_id = int(work_dict.get(\n            \'ticker_id\',\n            ae_consts.TICKER_ID))\n\n        if not ticker:\n            res = build_result.build_result(\n                status=ae_consts.ERR,\n                err=\'missing ticker\',\n                rec=rec)\n            return res\n\n        s3_key = work_dict.get(\n            \'s3_key\',\n            None)\n        s3_bucket_name = work_dict.get(\n            \'s3_bucket\',\n            \'pricing\')\n        redis_key = work_dict.get(\n            \'redis_key\',\n            None)\n        updated = work_dict.get(\n            \'updated\',\n            None)\n        serializer = work_dict.get(\n            \'serializer\',\n            \'json\')\n        encoding = work_dict.get(\n            \'encoding\',\n            \'utf-8\')\n        label = work_dict.get(\n            \'label\',\n            label)\n\n        enable_s3_read = True\n        enable_redis_publish = True\n\n        rec[\'ticker\'] = ticker\n        rec[\'ticker_id\'] = ticker_id\n        rec[\'s3_bucket\'] = s3_bucket_name\n        rec[\'s3_key\'] = s3_key\n        rec[\'redis_key\'] = redis_key\n        rec[\'updated\'] = updated\n        rec[\'s3_enabled\'] = enable_s3_read\n        rec[\'redis_enabled\'] = enable_redis_publish\n\n        data = None\n\n        if enable_s3_read:\n\n            log.info(f\'{label} parsing s3 values\')\n            access_key = work_dict.get(\n                \'s3_access_key\',\n                ae_consts.S3_ACCESS_KEY)\n            secret_key = work_dict.get(\n                \'s3_secret_key\',\n                ae_consts.S3_SECRET_KEY)\n            region_name = work_dict.get(\n                \'s3_region_name\',\n                ae_consts.S3_REGION_NAME)\n            service_address = work_dict.get(\n                \'s3_address\',\n                ae_consts.S3_ADDRESS)\n            secure = work_dict.get(\n                \'s3_secure\',\n                ae_consts.S3_SECURE) == \'1\'\n\n            endpoint_url = f\'http{""s"" if secure else """"}://{service_address}\'\n\n            log.info(\n                f\'{label} building s3 endpoint_url={endpoint_url} \'\n                f\'region={region_name}\')\n\n            s3 = boto3.resource(\n                \'s3\',\n                endpoint_url=endpoint_url,\n                aws_access_key_id=access_key,\n                aws_secret_access_key=secret_key,\n                region_name=region_name,\n                config=boto3.session.Config(\n                    signature_version=\'s3v4\')\n            )\n\n            try:\n                log.info(\n                    f\'{label} checking bucket={s3_bucket_name} exists\')\n                if s3.Bucket(s3_bucket_name) not in s3.buckets.all():\n                    log.info(\n                        f\'{label} creating bucket={s3_bucket_name}\')\n                    s3.create_bucket(\n                        Bucket=s3_bucket_name)\n            except Exception as e:\n                log.info(\n                    f\'{label} failed creating bucket={s3_bucket_name} \'\n                    f\'with ex={e}\')\n            # end of try/ex for creating bucket\n\n            try:\n                log.info(\n                    f\'{label} reading to s3={s3_bucket_name}/{s3_key} \'\n                    f\'updated={updated}\')\n                data = s3_read_contents_from_key.s3_read_contents_from_key(\n                    s3=s3,\n                    s3_bucket_name=s3_bucket_name,\n                    s3_key=s3_key,\n                    encoding=encoding,\n                    convert_as_json=True)\n\n                initial_size_value = \\\n                    len(str(data)) / 1024000\n                initial_size_str = ae_consts.to_f(initial_size_value)\n                if ae_consts.ev(\'DEBUG_S3\', \'0\') == \'1\':\n                    log.info(\n                        f\'{label} read s3={s3_bucket_name}/{s3_key} \'\n                        f\'data={ae_consts.ppj(data)}\')\n                else:\n                    log.info(\n                        f\'{label} read s3={s3_bucket_name}/{s3_key} \'\n                        f\'data size={initial_size_str} MB\')\n            except Exception as e:\n                err = (\n                    f\'{label} failed reading bucket={s3_bucket_name} \'\n                    f\'key={s3_key} ex={e}\')\n                log.error(\n                    err)\n                res = build_result.build_result(\n                    status=ae_consts.NOT_RUN,\n                    err=err,\n                    rec=rec)\n            # end of try/ex for creating bucket\n        else:\n            log.info(\n                f\'{label} SKIP S3 read bucket={s3_bucket_name} \'\n                f\'key={s3_key}\')\n        # end of if enable_s3_read\n\n        if enable_redis_publish:\n            redis_address = work_dict.get(\n                \'redis_address\',\n                ae_consts.REDIS_ADDRESS)\n            redis_key = work_dict.get(\n                \'redis_key\',\n                ae_consts.REDIS_KEY)\n            redis_password = work_dict.get(\n                \'redis_password\',\n                ae_consts.REDIS_PASSWORD)\n            redis_db = work_dict.get(\n                \'redis_db\',\n                None)\n            if not redis_db:\n                redis_db = ae_consts.REDIS_DB\n            redis_expire = None\n            if \'redis_expire\' in work_dict:\n                redis_expire = work_dict.get(\n                    \'redis_expire\',\n                    ae_consts.REDIS_EXPIRE)\n            log.info(\n                f\'redis enabled address={redis_address}@{redis_db} \'\n                f\'key={redis_key}\')\n            redis_host = redis_address.split(\':\')[0]\n            redis_port = redis_address.split(\':\')[1]\n            try:\n                if ae_consts.ev(\'DEBUG_REDIS\', \'0\') == \'1\':\n                    log.info(\n                        f\'{label} publishing redis={redis_host}:{redis_port} \'\n                        f\'db={redis_db} key={redis_key} updated={updated} \'\n                        f\'expire={redis_expire} data={ae_consts.ppj(data)}\')\n                else:\n                    log.info(\n                        f\'{label} publishing redis={redis_host}:{redis_port} \'\n                        f\'db={redis_db} key={redis_key} updated={updated} \'\n                        f\'expire={redis_expire}\')\n                # end of if/else\n\n                rc = redis.Redis(\n                    host=redis_host,\n                    port=redis_port,\n                    password=redis_password,\n                    db=redis_db)\n\n                redis_set_res = redis_set.set_data_in_redis_key(\n                    label=label,\n                    client=rc,\n                    key=redis_key,\n                    data=data,\n                    serializer=serializer,\n                    encoding=encoding,\n                    expire=redis_expire,\n                    px=None,\n                    nx=False,\n                    xx=False)\n\n                log.info(\n                    f\'{label} redis_set \'\n                    f\'status={ae_consts.get_status(redis_set_res[""status""])} \'\n                    f\'err={redis_set_res[""err""]}\')\n\n            except Exception as e:\n                log.error(\n                    f\'{label} failed - redis publish to \'\n                    f\'key={redis_key} ex={e}\')\n            # end of try/ex for creating bucket\n        else:\n            log.info(f\'{label} SKIP REDIS publish key={redis_key}\')\n        # end of if enable_redis_publish\n\n        res = build_result.build_result(\n            status=ae_consts.SUCCESS,\n            err=None,\n            rec=rec)\n\n    except Exception as e:\n        res = build_result.build_result(\n            status=ae_consts.ERR,\n            err=(\n                \'failed - publish_from_s3_to_redis \'\n                f\'dict={work_dict} with ex={e}\'),\n            rec=rec)\n        log.error(f\'{label} - {res[""err""]}\')\n    # end of try/ex\n\n    log.info(\n        \'task - publish_from_s3_to_redis done - \'\n        f\'{label} - status={ae_consts.get_status(res[""status""])}\')\n\n    return get_task_results.get_task_results(\n        work_dict=work_dict,\n        result=res)\n# end of publish_from_s3_to_redis\n\n\ndef run_publish_from_s3_to_redis(\n        work_dict):\n    """"""run_publish_from_s3_to_redis\n\n    Celery wrapper for running without celery\n\n    :param work_dict: task data\n    """"""\n\n    label = work_dict.get(\n        \'label\',\n        \'\')\n\n    log.info(f\'run_publish_from_s3_to_redis - {label} - start\')\n\n    response = build_result.build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec={})\n    task_res = {}\n\n    # allow running without celery\n    if ae_consts.is_celery_disabled(\n            work_dict=work_dict):\n        work_dict[\'celery_disabled\'] = True\n        task_res = publish_from_s3_to_redis(\n            work_dict=work_dict)\n        if task_res:\n            response = task_res.get(\n                \'result\',\n                task_res)\n            if ae_consts.ev(\'DEBUG_RESULTS\', \'0\') == \'1\':\n                response_details = response\n                try:\n                    response_details = ae_consts.ppj(response)\n                except Exception:\n                    response_details = response\n                log.info(f\'{label} task result={response_details}\')\n        else:\n            log.error(\n                f\'{label} celery was disabled but the task={response} \'\n                \'did not return anything\')\n        # end of if response\n    else:\n        task_res = publish_from_s3_to_redis.delay(\n            work_dict=work_dict)\n        rec = {\n            \'task_id\': task_res\n        }\n        response = build_result.build_result(\n            status=ae_consts.SUCCESS,\n            err=None,\n            rec=rec)\n    # if celery enabled\n\n    if response:\n        if ae_consts.ev(\'DEBUG_RESULTS\', \'0\') == \'1\':\n            log.info(\n                f\'run_publish_from_s3_to_redis - {label} - done \'\n                f\'status={ae_consts.get_status(response[""status""])} \'\n                f\'err={response[""err""]} rec={response[""rec""]}\')\n        else:\n            log.info(\n                f\'run_publish_from_s3_to_redis - {label} - done \'\n                f\'status={ae_consts.get_status(response[""status""])} \'\n                f\'err={response[""err""]}\')\n    else:\n        log.info(f\'run_publish_from_s3_to_redis - {label} - done no response\')\n    # end of if/else response\n\n    return response\n# end of run_publish_from_s3_to_redis\n'"
analysis_engine/work_tasks/publish_pricing_update.py,0,"b'""""""\n**Publish Pricing Data Task**\n\nPublish new stock data to external services and systems\n(redis and s3) provided the system(s) are running and enabled.\n\n- redis - using `redis-py <https://github.com/andymccurdy/redis-py>`__\n- s3 - using boto3\n\n**Sample work_dict request for this method**\n\n`analysis_engine.api_requests.publish_pricing_update <https://\ngithub.com/AlgoTraders/stock-analysis-engine/blob/master/\nanalysis_engine/api_requests.py#L344>`__\n\n::\n\n    work_request = {\n        \'ticker\': ticker,\n        \'ticker_id\': ticker_id,\n        \'strike\': use_strike,\n        \'contract\': contract_type,\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'data\': use_data\n    }\n\n.. tip:: This task uses the `analysis_engine.work_tasks.\n    custom_task.CustomTask class <https://github.com/A\n    lgoTraders/stock-analysis-engine/blob/master/anal\n    ysis_engine/work_tasks/custom_task.py>`__ for\n    task event handling.\n\n**Supported Environment Variables**\n\n::\n\n    export DEBUG_RESULTS=1\n\n""""""\n\nimport boto3\nimport redis\nimport json\nimport zlib\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.build_result as build_result\nimport analysis_engine.get_task_results as get_task_results\nimport analysis_engine.work_tasks.custom_task as custom_task\nimport analysis_engine.set_data_in_redis_key as redis_set\nimport celery.task as celery_task\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\n@celery_task(\n    bind=True,\n    base=custom_task.CustomTask,\n    queue=\'publish_pricing_update\')\ndef publish_pricing_update(\n        self,\n        work_dict):\n    """"""publish_pricing_update\n\n    Publish Ticker Data to S3 and Redis\n\n    - prices - turn off with ``work_dict.get_pricing = False``\n    - news - turn off with ``work_dict.get_news = False``\n    - options - turn off with ``work_dict.get_options = False``\n\n    :param work_dict: dictionary for key/values\n    """"""\n\n    label = \'publish_pricing\'\n\n    log.debug(f\'task - {label} - start\')\n\n    ticker = ae_consts.TICKER\n    ticker_id = ae_consts.TICKER_ID\n    rec = {\n        \'ticker\': None,\n        \'ticker_id\': None,\n        \'s3_enabled\': False,\n        \'redis_enabled\': False,\n        \'s3_bucket\': None,\n        \'s3_key\': None,\n        \'redis_key\': None,\n        \'updated\': None\n    }\n    res = build_result.build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec=rec)\n\n    try:\n        ticker = work_dict.get(\n            \'ticker\',\n            ae_consts.TICKER)\n        ticker_id = int(work_dict.get(\n            \'ticker_id\',\n            ae_consts.TICKER_ID))\n\n        if not ticker:\n            res = build_result.build_result(\n                status=ae_consts.ERR,\n                err=\'missing ticker\',\n                rec=rec)\n            return res\n\n        label = work_dict.get(\n            \'label\',\n            label)\n        s3_key = work_dict.get(\n            \'s3_key\',\n            None)\n        s3_bucket_name = work_dict.get(\n            \'s3_bucket\',\n            \'pricing\')\n        redis_key = work_dict.get(\n            \'redis_key\',\n            None)\n        data = work_dict.get(\n            \'data\',\n            None)\n        updated = work_dict.get(\n            \'updated\',\n            None)\n        enable_s3_upload = work_dict.get(\n            \'s3_enabled\',\n            ae_consts.ENABLED_S3_UPLOAD)\n        enable_redis_publish = work_dict.get(\n            \'redis_enabled\',\n            ae_consts.ENABLED_REDIS_PUBLISH)\n        serializer = work_dict.get(\n            \'serializer\',\n            \'json\')\n        encoding = work_dict.get(\n            \'encoding\',\n            \'utf-8\')\n\n        rec[\'ticker\'] = ticker\n        rec[\'ticker_id\'] = ticker_id\n        rec[\'s3_bucket\'] = s3_bucket_name\n        rec[\'s3_key\'] = s3_key\n        rec[\'redis_key\'] = redis_key\n        rec[\'updated\'] = updated\n        rec[\'s3_enabled\'] = enable_s3_upload\n        rec[\'redis_enabled\'] = enable_redis_publish\n\n        if (\n                enable_s3_upload and\n                s3_bucket_name != \'MISSING_AN_S3_BUCKET\'):\n            access_key = work_dict.get(\n                \'s3_access_key\',\n                ae_consts.S3_ACCESS_KEY)\n            secret_key = work_dict.get(\n                \'s3_secret_key\',\n                ae_consts.S3_SECRET_KEY)\n            region_name = work_dict.get(\n                \'s3_region_name\',\n                ae_consts.S3_REGION_NAME)\n            service_address = work_dict.get(\n                \'s3_address\',\n                ae_consts.S3_ADDRESS)\n            secure = work_dict.get(\n                \'s3_secure\',\n                ae_consts.S3_SECURE) == \'1\'\n\n            endpoint_url = f\'http{""s"" if secure else """"}://{service_address}\'\n\n            log.debug(\n                f\'{label} building s3 endpoint_url={endpoint_url} \'\n                f\'region={region_name}\')\n\n            s3 = boto3.resource(\n                \'s3\',\n                endpoint_url=endpoint_url,\n                aws_access_key_id=access_key,\n                aws_secret_access_key=secret_key,\n                region_name=region_name,\n                config=boto3.session.Config(\n                    signature_version=\'s3v4\')\n            )\n\n            try:\n                log.debug(f\'{label} checking bucket={s3_bucket_name} exists\')\n                if s3.Bucket(s3_bucket_name) not in s3.buckets.all():\n                    log.debug(f\'{label} creating bucket={s3_bucket_name}\')\n                    s3.create_bucket(\n                        Bucket=s3_bucket_name)\n            except Exception as e:\n                log.debug(\n                    f\'{label} failed creating bucket={s3_bucket_name} \'\n                    f\'with ex={e}\')\n            # end of try/ex for creating bucket\n\n            try:\n                log.debug(\n                    f\'{label} uploading to s3={s3_bucket_name}/{s3_key} \'\n                    f\'updated={updated}\')\n                s3.Bucket(s3_bucket_name).put_object(\n                    Key=s3_key,\n                    Body=json.dumps(data).encode(encoding))\n            except Exception as e:\n                log.error(\n                    f\'{label} failed uploading bucket={s3_bucket_name} \'\n                    f\'key={s3_key} ex={e}\')\n            # end of try/ex for creating bucket\n        else:\n            log.debug(\n                f\'{label} SKIP S3 upload bucket={s3_bucket_name} \'\n                f\'key={s3_key}\')\n        # end of if enable_s3_upload\n\n        if enable_redis_publish:\n            redis_address = work_dict.get(\n                \'redis_address\',\n                ae_consts.REDIS_ADDRESS)\n            redis_key = work_dict.get(\n                \'redis_key\',\n                ae_consts.REDIS_KEY)\n            redis_password = work_dict.get(\n                \'redis_password\',\n                ae_consts.REDIS_PASSWORD)\n            redis_db = work_dict.get(\n                \'redis_db\',\n                ae_consts.REDIS_DB)\n            redis_expire = None\n            if \'redis_expire\' in work_dict:\n                redis_expire = work_dict.get(\n                    \'redis_expire\',\n                    ae_consts.REDIS_EXPIRE)\n            log.debug(\n                f\'redis enabled address={redis_address}@{redis_db} \'\n                f\'key={redis_key}\')\n            redis_host = None\n            redis_port = None\n            try:\n                redis_host = redis_address.split(\':\')[0]\n                redis_port = redis_address.split(\':\')[1]\n            except Exception as c:\n                err = (\n                    f\'{label} failed parsing redis_address={redis_address} \'\n                    f\'with ex={c} \'\n                    \'please set one with the format: \'\n                    \'<hostname>:<port>\')\n                log.critical(err)\n                res = build_result.build_result(\n                    status=ae_consts.ERR,\n                    err=err,\n                    rec=rec)\n                return res\n            # end of checking that redis_address is valid\n\n            try:\n                log.debug(\n                    f\'{label} publishing redis={redis_host}:{redis_port} \'\n                    f\'db={redis_db} key={redis_key} \'\n                    f\'updated={updated} expire={redis_expire}\')\n\n                rc = redis.Redis(\n                    host=redis_host,\n                    port=redis_port,\n                    password=redis_password,\n                    db=redis_db)\n\n                already_compressed = False\n                uses_data = data\n                try:\n                    uses_data = zlib.compress(json.dumps(data).encode(\n                        encoding),\n                        9)\n                    already_compressed = True\n                except Exception as p:\n                    log.critical(\n                        \'failed to compress dataset for \'\n                        f\'redis_key={redis_key} with ex={p}\')\n\n                redis_set_res = redis_set.set_data_in_redis_key(\n                    label=label,\n                    client=rc,\n                    key=redis_key,\n                    data=uses_data,\n                    already_compressed=already_compressed,\n                    serializer=serializer,\n                    encoding=encoding,\n                    expire=redis_expire,\n                    px=None,\n                    nx=False,\n                    xx=False)\n\n                log.debug(\n                    f\'{label} redis_set \'\n                    f\'status={ae_consts.get_status(redis_set_res[""status""])} \'\n                    f\'err={redis_set_res[""err""]}\')\n\n            except Exception as e:\n                log.error(\n                    f\'{label} failed - redis publish to \'\n                    f\'key={redis_key} ex={e}\')\n            # end of try/ex for creating bucket\n        else:\n            log.debug(f\'{label} SKIP REDIS publish key={redis_key}\')\n        # end of if enable_redis_publish\n\n        res = build_result.build_result(\n            status=ae_consts.SUCCESS,\n            err=None,\n            rec=rec)\n\n    except Exception as e:\n        res = build_result.build_result(\n            status=ae_consts.ERR,\n            err=(\n                \'failed - publish_pricing_update \'\n                f\'dict={work_dict} with ex={e}\'),\n            rec=rec)\n        log.error(\n            f\'{label} - {res[""err""]}\')\n    # end of try/ex\n\n    log.debug(\n        f\'task - publish_pricing_update done - {label} - \'\n        f\'status={ae_consts.get_status(res[""status""])}\')\n\n    return get_task_results.get_task_results(\n        work_dict=work_dict,\n        result=res)\n# end of publish_pricing_update\n\n\ndef run_publish_pricing_update(\n        work_dict):\n    """"""run_publish_pricing_update\n\n    Celery wrapper for running without celery\n\n    :param work_dict: task data\n    """"""\n\n    label = work_dict.get(\n        \'label\',\n        \'\')\n\n    log.debug(f\'run_publish_pricing_update - {label} - start\')\n\n    response = build_result.build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec={})\n    task_res = {}\n\n    # allow running without celery\n    if ae_consts.is_celery_disabled(\n            work_dict=work_dict):\n        work_dict[\'celery_disabled\'] = True\n        task_res = publish_pricing_update(\n            work_dict=work_dict)\n        if task_res:\n            response = task_res.get(\n                \'result\',\n                task_res)\n            if ae_consts.ev(\'DEBUG_RESULTS\', \'0\') == \'1\':\n                response_details = response\n                try:\n                    response_details = ae_consts.ppj(response)\n                except Exception:\n                    response_details = response\n                log.debug(f\'{label} task result={response_details}\')\n        else:\n            log.error(\n                f\'{label} celery was disabled but the task={response} \'\n                \'did not return anything\')\n        # end of if response\n    else:\n        task_res = publish_pricing_update.delay(\n            work_dict=work_dict)\n        rec = {\n            \'task_id\': task_res\n        }\n        response = build_result.build_result(\n            status=ae_consts.SUCCESS,\n            err=None,\n            rec=rec)\n    # if celery enabled\n\n    if response:\n        if ae_consts.ev(\'DEBUG_RESULTS\', \'0\') == \'1\':\n            log.debug(\n                f\'run_publish_pricing_update - {label} - done \'\n                f\'status={ae_consts.get_status(response[""status""])} \'\n                f\'err={response[""err""]} rec={response[""rec""]}\')\n        else:\n            log.debug(\n                f\'run_publish_pricing_update - {label} - done \'\n                f\'status={ae_consts.get_status(response[""status""])} \'\n                f\'err={response[""err""]}\')\n    else:\n        log.debug(f\'run_publish_pricing_update - {label} - done no response\')\n    # end of if/else response\n\n    return response\n# end of run_publish_pricing_update\n'"
analysis_engine/work_tasks/publish_ticker_aggregate_from_s3.py,0,"b'""""""\n**Publish Aggregate Ticker Data from S3 Task**\n\nPublish S3 key with aggregated stock data to redis\nand s3 (if either of them are running and enabled)\n\n- redis - using `redis-py <https://github.com/andymccurdy/redis-py>`__\n- s3 - using boto3\n\n**Sample work_dict request for this method**\n\n`analysis_engine.api_requests.build_publish_ticker_aggregate_from_s3\n_request <https://\ngithub.com/AlgoTraders/stock-analysis-engine/blob/master/\nanalysis_engine/api_requests.py#L426>`__\n\n::\n\n    work_request = {\n        \'ticker\': ticker,\n        \'ticker_id\': ticker_id,\n        \'s3_bucket\': s3_bucket_name,\n        \'s3_compiled_bucket\': s3_compiled_bucket_name,\n        \'s3_key\': s3_key,\n        \'redis_key\': redis_key,\n        \'s3_enabled\': s3_enabled,\n        \'redis_enabled\': redis_enabled\n    }\n\n.. tip:: This task uses the `analysis_engine.work_tasks.\n    custom_task.CustomTask class <https://github.com/A\n    lgoTraders/stock-analysis-engine/blob/master/anal\n    ysis_engine/work_tasks/custom_task.py>`__ for\n    task event handling.\n\n**Supported Environment Variables**\n\n::\n\n    export DEBUG_RESULTS=1\n\n""""""\n\nimport boto3\nimport json\nimport re\nimport redis\nimport zlib\nimport celery.task as celery_task\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.build_result as build_result\nimport analysis_engine.get_task_results as get_task_results\nimport analysis_engine.work_tasks.custom_task as custom_task\nimport analysis_engine.set_data_in_redis_key as redis_set\nimport spylunking.log.setup_logging as log_utils\nimport analysis_engine.s3_read_contents_from_key as s3_read_contents_from_key\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\n@celery_task(\n    bind=True,\n    base=custom_task.CustomTask,\n    queue=\'publish_ticker_aggregate_from_s3\')\ndef publish_ticker_aggregate_from_s3(\n        self,\n        work_dict):\n    """"""publish_ticker_aggregate_from_s3\n\n    Publish Aggregated Ticker Data from S3 to Redis\n\n    :param work_dict: dictionary for key/values\n    """"""\n\n    label = \'pub-tic-agg-s3-to-redis\'\n\n    log.info(f\'task - {label} - start work_dict={work_dict}\')\n\n    ticker = ae_consts.TICKER\n    ticker_id = ae_consts.TICKER_ID\n    rec = {\n        \'ticker\': None,\n        \'ticker_id\': None,\n        \'s3_read_enabled\': True,\n        \'s3_upload_enabled\': True,\n        \'redis_enabled\': True,\n        \'s3_bucket\': None,\n        \'s3_compiled_bucket\': None,\n        \'s3_key\': None,\n        \'redis_key\': None,\n        \'updated\': None\n    }\n    res = build_result.build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec=rec)\n\n    try:\n        ticker = work_dict.get(\n            \'ticker\',\n            ae_consts.TICKER)\n        ticker_id = int(work_dict.get(\n            \'ticker_id\',\n            ae_consts.TICKER_ID))\n\n        if not ticker:\n            res = build_result.build_result(\n                status=ae_consts.ERR,\n                err=\'missing ticker\',\n                rec=rec)\n            return res\n\n        label = work_dict.get(\n            \'label\',\n            label)\n        s3_key = work_dict.get(\n            \'s3_key\',\n            None)\n        s3_bucket_name = work_dict.get(\n            \'s3_bucket\',\n            \'pricing\')\n        s3_compiled_bucket_name = work_dict.get(\n            \'s3_compiled_bucket\',\n            \'compileddatasets\')\n        redis_key = work_dict.get(\n            \'redis_key\',\n            None)\n        updated = work_dict.get(\n            \'updated\',\n            None)\n        enable_s3_upload = work_dict.get(\n            \'s3_upload_enabled\',\n            ae_consts.ENABLED_S3_UPLOAD)\n        enable_redis_publish = work_dict.get(\n            \'redis_enabled\',\n            ae_consts.ENABLED_REDIS_PUBLISH)\n        serializer = work_dict.get(\n            \'serializer\',\n            \'json\')\n        encoding = work_dict.get(\n            \'encoding\',\n            \'utf-8\')\n\n        enable_s3_read = True\n\n        rec[\'ticker\'] = ticker\n        rec[\'ticker_id\'] = ticker_id\n        rec[\'s3_bucket\'] = s3_bucket_name\n        rec[\'s3_compiled_bucket\'] = s3_compiled_bucket_name\n        rec[\'s3_key\'] = s3_key\n        rec[\'redis_key\'] = redis_key\n        rec[\'updated\'] = updated\n        rec[\'s3_read_enabled\'] = enable_s3_read\n        rec[\'s3_upload_enabled\'] = enable_s3_upload\n        rec[\'redis_enabled\'] = enable_redis_publish\n\n        if enable_s3_read:\n            log.info(f\'{label} parsing s3 values\')\n            access_key = work_dict.get(\n                \'s3_access_key\',\n                ae_consts.S3_ACCESS_KEY)\n            secret_key = work_dict.get(\n                \'s3_secret_key\',\n                ae_consts.S3_SECRET_KEY)\n            region_name = work_dict.get(\n                \'s3_region_name\',\n                ae_consts.S3_REGION_NAME)\n            service_address = work_dict.get(\n                \'s3_address\',\n                ae_consts.S3_ADDRESS)\n            secure = work_dict.get(\n                \'s3_secure\',\n                ae_consts.S3_SECURE) == \'1\'\n\n            endpoint_url = f\'http{""s"" if secure else """"}://{service_address}\'\n\n            log.info(\n                f\'{label} building s3 endpoint_url={endpoint_url} \'\n                f\'region={region_name}\')\n\n            s3 = boto3.resource(\n                \'s3\',\n                endpoint_url=endpoint_url,\n                aws_access_key_id=access_key,\n                aws_secret_access_key=secret_key,\n                region_name=region_name,\n                config=boto3.session.Config(\n                    signature_version=\'s3v4\')\n            )\n\n            try:\n                log.info(f\'{label} checking bucket={s3_bucket_name} exists\')\n                if s3.Bucket(s3_bucket_name) not in s3.buckets.all():\n                    log.info(f\'{label} creating bucket={s3_bucket_name}\')\n                    s3.create_bucket(\n                        Bucket=s3_bucket_name)\n            except Exception as e:\n                log.info(\n                    f\'{label} failed creating bucket={s3_bucket_name} \'\n                    f\'with ex={e}\')\n            # end of try/ex for creating bucket\n\n            try:\n                log.info(f\'{label} checking bucket={s3_bucket_name} keys\')\n                date_keys = []\n                keys = []\n                # {TICKER}_YYYY-DD-MM regex\n                reg = r\'^.*_\\d{4}-(0?[1-9]|1[012])-(0?[1-9]|[12][0-9]|3[01])$\'\n                for bucket in s3.buckets.all():\n                    for key in bucket.objects.all():\n                        if (ticker.lower() in key.key.lower() and\n                                bool(re.compile(reg).search(key.key))):\n                            keys.append(key.key)\n                            date_keys.append(\n                                key.key.split(f\'{ticker}_\')[1])\n            except Exception as e:\n                log.info(\n                    f\'{label} failed to get bucket={s3_bucket_name} \'\n                    f\'keys with ex={e}\')\n            # end of try/ex for getting bucket keys\n\n            if keys:\n                data = []\n                for idx, key in enumerate(keys):\n                    try:\n                        log.info(\n                            f\'{label} reading to s3={s3_bucket_name}/{key} \'\n                            f\'updated={updated}\')\n                        loop_data = s3_read_contents_from_key.\\\n                            s3_read_contents_from_key(\n                                s3=s3,\n                                s3_bucket_name=s3_bucket_name,\n                                s3_key=key,\n                                encoding=encoding,\n                                convert_as_json=True)\n\n                        initial_size_value = \\\n                            len(str(loop_data)) / 1024000\n                        initial_size_str = ae_consts.to_f(initial_size_value)\n                        if ae_consts.ev(\'DEBUG_S3\', \'0\') == \'1\':\n                            log.info(\n                                f\'{label} read s3={s3_bucket_name}/{key} \'\n                                f\'data={ae_consts.ppj(loop_data)}\')\n                        else:\n                            log.info(\n                                f\'{label} read s3={s3_bucket_name}/{key} data \'\n                                f\'size={initial_size_str} MB\')\n                        data.append({f\'{date_keys[idx]}\': loop_data})\n                    except Exception as e:\n                        err = (\n                            f\'{label} failed reading bucket={s3_bucket_name} \'\n                            f\'key={key} ex={e}\')\n                        log.error(\n                            err)\n                        res = build_result.build_result(\n                            status=ae_consts.NOT_RUN,\n                            err=err,\n                            rec=rec)\n                    # end of try/ex for creating bucket\n            else:\n                log.info(\n                    f\'{label} No keys found in S3 \'\n                    f\'bucket={s3_bucket_name} for ticker={ticker}\')\n        else:\n            log.info(\n                f\'{label} SKIP S3 read bucket={s3_bucket_name} \'\n                f\'ticker={ticker}\')\n        # end of if enable_s3_read\n\n        if data and enable_s3_upload:\n            try:\n                log.info(\n                    f\'{label} checking bucket={s3_compiled_bucket_name} \'\n                    \'exists\')\n                if s3.Bucket(s3_compiled_bucket_name) not in s3.buckets.all():\n                    log.info(\n                        f\'{label} creating bucket={s3_compiled_bucket_name}\')\n                    s3.create_bucket(\n                        Bucket=s3_compiled_bucket_name)\n            except Exception as e:\n                log.info(\n                    f\'{label} failed creating \'\n                    f\'bucket={s3_compiled_bucket_name} with ex={e}\')\n            # end of try/ex for creating bucket\n\n            try:\n                cmpr_data = zlib.compress(json.dumps(data).encode(encoding), 9)\n\n                if ae_consts.ev(\'DEBUG_S3\', \'0\') == \'1\':\n                    log.info(\n                        f\'{label} uploading to \'\n                        f\'s3={s3_compiled_bucket_name}/{s3_key} \'\n                        f\'data={ae_consts.ppj(loop_data)} updated={updated}\')\n                else:\n                    sizes = {\'MB\': 1024000,\n                             \'GB\': 1024000000,\n                             \'TB\': 1024000000000,\n                             \'PB\': 1024000000000000}\n                    initial_size_value = len(str(data))\n                    org_data_size = \'MB\'\n                    for key in sizes.keys():\n                        size = float(initial_size_value) / float(sizes[key])\n                        if size > 1024:\n                            continue\n                        org_data_size = key\n                        initial_size_value = size\n                        break\n                    initial_size_str = ae_consts.to_f(initial_size_value)\n\n                    cmpr_data_size_value = len(cmpr_data)\n                    cmpr_data_size = \'MB\'\n                    for key in sizes.keys():\n                        size = float(cmpr_data_size_value) / float(sizes[key])\n                        if size > 1024:\n                            continue\n                        cmpr_data_size = key\n                        cmpr_data_size_value = size\n                        break\n                    cmpr_size_str = ae_consts.to_f(cmpr_data_size_value)\n                    log.info(\n                        f\'{label} uploading to \'\n                        f\'s3={s3_compiled_bucket_name}/{s3_key} data \'\n                        f\'original_size={initial_size_str} {org_data_size} \'\n                        f\'compressed_size={cmpr_size_str} {cmpr_data_size} \'\n                        f\'updated={updated}\')\n                s3.Bucket(s3_compiled_bucket_name).put_object(\n                    Key=s3_key,\n                    Body=cmpr_data)\n            except Exception as e:\n                log.error(\n                    f\'{label} failed \'\n                    f\'uploading bucket={s3_compiled_bucket_name} \'\n                    f\'key={s3_key} ex={e}\')\n            # end of try/ex for creating bucket\n        else:\n            log.info(\n                f\'{label} SKIP S3 upload bucket={s3_bucket_name} key={s3_key}\')\n        # end of if enable_s3_upload\n\n        if data and enable_redis_publish:\n            redis_address = work_dict.get(\n                \'redis_address\',\n                ae_consts.REDIS_ADDRESS)\n            redis_key = work_dict.get(\n                \'redis_key\',\n                ae_consts.REDIS_KEY)\n            redis_password = work_dict.get(\n                \'redis_password\',\n                ae_consts.REDIS_PASSWORD)\n            redis_db = work_dict.get(\n                \'redis_db\',\n                None)\n            if not redis_db:\n                redis_db = ae_consts.REDIS_DB\n            redis_expire = None\n            if \'redis_expire\' in work_dict:\n                redis_expire = work_dict.get(\n                    \'redis_expire\',\n                    ae_consts.REDIS_EXPIRE)\n            log.info(\n                f\'redis enabled address={redis_address}@{redis_db} \'\n                f\'key={redis_key}\')\n            redis_host = redis_address.split(\':\')[0]\n            redis_port = redis_address.split(\':\')[1]\n            try:\n                if ae_consts.ev(\'DEBUG_REDIS\', \'0\') == \'1\':\n                    log.info(\n                        f\'{label} publishing redis={redis_host}:{redis_port} \'\n                        f\'db={redis_db} key={redis_key} updated={updated} \'\n                        f\'expire={redis_expire} data={ae_consts.ppj(data)}\')\n                else:\n                    log.info(\n                        f\'{label} publishing redis={redis_host}:{redis_port} \'\n                        f\'db={redis_db} key={redis_key} \'\n                        f\'updated={updated} expire={redis_expire}\')\n                # end of if/else\n\n                rc = redis.Redis(\n                    host=redis_host,\n                    port=redis_port,\n                    password=redis_password,\n                    db=redis_db)\n\n                redis_set_res = redis_set.set_data_in_redis_key(\n                    label=label,\n                    client=rc,\n                    key=redis_key,\n                    data=data,\n                    serializer=serializer,\n                    encoding=encoding,\n                    expire=redis_expire,\n                    px=None,\n                    nx=False,\n                    xx=False)\n\n                log.info(\n                    f\'{label} redis_set \'\n                    f\'status={ae_consts.get_status(redis_set_res[""status""])} \'\n                    f\'err={redis_set_res[""err""]}\')\n\n            except Exception as e:\n                log.error(\n                    f\'{label} failed - redis publish to \'\n                    f\'key={redis_key} ex={e}\')\n            # end of try/ex for creating bucket\n        else:\n            log.info(f\'{label} SKIP REDIS publish key={redis_key}\')\n        # end of if enable_redis_publish\n\n        res = build_result.build_result(\n            status=ae_consts.SUCCESS,\n            err=None,\n            rec=rec)\n\n    except Exception as e:\n        res = build_result.build_result(\n            status=ae_consts.ERR,\n            err=(f\'failed - publish_from_s3 dict={work_dict} with ex={e}\'),\n            rec=rec)\n        log.error(f\'{label} - {res[""err""]}\')\n    # end of try/ex\n\n    log.info(\n        \'task - publish_from_s3 done - \'\n        f\'{label} - status={ae_consts.get_status(res[""status""])}\')\n\n    return get_task_results.get_task_results(\n        work_dict=work_dict,\n        result=res)\n# end of publish_ticker_aggregate_from_s3\n\n\ndef run_publish_ticker_aggregate_from_s3(\n        work_dict):\n    """"""run_publish_ticker_aggregate_from_s3\n\n    Celery wrapper for running without celery\n\n    :param work_dict: task data\n    """"""\n\n    label = work_dict.get(\n        \'label\',\n        \'\')\n\n    log.info(f\'run_publish_ticker_aggregate_from_s3 - {label} - start\')\n\n    response = build_result.build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec={})\n    task_res = {}\n\n    # allow running without celery\n    if ae_consts.is_celery_disabled(\n            work_dict=work_dict):\n        work_dict[\'celery_disabled\'] = True\n        task_res = publish_ticker_aggregate_from_s3(\n            work_dict=work_dict)\n        if task_res:\n            response = task_res.get(\n                \'result\',\n                task_res)\n            if ae_consts.ev(\'DEBUG_RESULTS\', \'0\') == \'1\':\n                response_details = response\n                try:\n                    response_details = ae_consts.ppj(response)\n                except Exception:\n                    response_details = response\n                log.info(f\'{label} task result={response_details}\')\n        else:\n            log.error(\n                f\'{label} celery was disabled but the task={response} \'\n                \'did not return anything\')\n        # end of if response\n    else:\n        task_res = publish_ticker_aggregate_from_s3.delay(\n            work_dict=work_dict)\n        rec = {\n            \'task_id\': task_res\n        }\n        response = build_result.build_result(\n            status=ae_consts.SUCCESS,\n            err=None,\n            rec=rec)\n    # if celery enabled\n\n    if response:\n        if ae_consts.ev(\'DEBUG_RESULTS\', \'0\') == \'1\':\n            log.info(\n                f\'run_publish_ticker_aggregate_from_s3 - {label} - done \'\n                f\'status={ae_consts.get_status(response[""status""])} \'\n                f\'err={response[""err""]} rec={response[""rec""]}\')\n        else:\n            log.info(\n                f\'run_publish_ticker_aggregate_from_s3 - {label} - done \'\n                f\'status={ae_consts.get_status(response[""status""])} \'\n                f\'err={response[""err""]}\')\n    else:\n        log.info(\n            f\'run_publish_ticker_aggregate_from_s3 - {label} - done \'\n            \'no response\')\n    # end of if/else response\n\n    return response\n# end of run_publish_ticker_aggregate_from_s3\n'"
analysis_engine/work_tasks/run_distributed_algorithm.py,0,"b'""""""\nRunning Distributed Algorithms Across Many Celery Workers\n\nUse this module to handle algorithm backtesting (tuning)\nor for live trading.\n\nUnder the hood, this is a Celery task handler\nthat processes jobs from a broker\'s\nmessaging queue. This allows the Analysis Engine to process\nmany algorithmic workloads concurrently using Celery\'s horizontally-\nscalable worker pool architecture.\n\n**Publish a SPY 60-day Backtest to the Distributed Analysis Engine**\n\n::\n\n    ./tools/run-algo-with-publishing.sh SPY 60 -w\n\nor manually with:\n\n::\n\n    use_date=$(date +""%Y-%m-%d"")\n    ticker_dataset=""${ticker}-${use_date}.json""\n    history_loc=""s3://algohistory/${ticker_dataset}""\n    report_loc=""s3://algoreport/${ticker_dataset}""\n    extract_loc=""s3://algoready/${ticker_dataset}""\n    backtest_loc=""file:/tmp/${ticker_dataset}""\n\n    sa -t ${ticker} \\\n        -p ${history_loc} \\\n        -o ${report_loc} \\\n        -e ${extract_loc} \\\n        -b ${backtest_loc} \\\n        -s ${backtest_start_date} \\\n        -n ${use_date} \\\n        -w\n\n""""""\n\nimport inspect\nimport types\nimport importlib.machinery\nimport datetime\nimport celery.task as celery_task\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.build_result as build_result\nimport analysis_engine.work_tasks.custom_task as custom_task\nimport analysis_engine.run_algo as run_algo\nimport analysis_engine.algo as ae_algo  # base algo\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\n@celery_task.task(\n    bind=True,\n    base=custom_task.CustomTask,\n    queue=\'run_distributed_algorithm\')\ndef run_distributed_algorithm(\n        self,\n        algo_req):\n    """"""run_distributed_algorithm\n\n    Process an Algorithm using a Celery task that is\n    processed by a Celery worker\n\n    :param algo_req: dictionary for key/values for\n        running an algorithm using Celery workers\n    """"""\n\n    label = algo_req.get(\n        \'name\',\n        \'ae-algo\')\n    verbose = algo_req.get(\n        \'verbose_task\',\n        False)\n    debug = algo_req.get(\n        \'debug\',\n        False)\n\n    # please be careful logging prod passwords:\n    if debug:\n        log.info(f\'task - {label} - start algo_req={algo_req}\')\n    elif verbose:\n        log.info(f\'task - {label} - start \')\n    # end of start log\n\n    rec = {}\n    res = build_result.build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec=rec)\n\n    created_algo_object = None\n    custom_algo_module = None\n    new_algo_object = None\n    use_custom_algo = False\n    found_algo_module = True  # assume the BaseAlgo\n    should_publish_extract_dataset = False\n    should_publish_history_dataset = False\n    should_publish_report_dataset = False\n\n    ticker = algo_req.get(\n        \'ticker\',\n        \'SPY\')\n    num_days_back = algo_req.get(\n        \'num_days_back\',\n        75)\n    name = algo_req.get(\n        \'name\',\n        \'ae-algo\')\n    algo_module_path = algo_req.get(\n        \'mod_path\',\n        None)\n    module_name = algo_req.get(\n        \'module_name\',\n        \'BaseAlgo\')\n    custom_algo_module = algo_req.get(\n        \'custom_algo_module\',\n        None)\n    new_algo_object = algo_req.get(\n        \'new_algo_object\',\n        None)\n    use_custom_algo = algo_req.get(\n        \'use_custom_algo\',\n        False)\n    should_publish_extract_dataset = algo_req.get(\n        \'should_publish_extract_dataset\',\n        False)\n    should_publish_history_dataset = algo_req.get(\n        \'should_publish_history_dataset\',\n        False)\n    should_publish_report_dataset = algo_req.get(\n        \'should_publish_report_dataset\',\n        False)\n    start_date = algo_req.get(\n        \'start_date\',\n        None)\n    end_date = algo_req.get(\n        \'end_date\',\n        None)\n    raise_on_err = algo_req.get(\n        \'raise_on_err\',\n        True)\n    report_config = algo_req.get(\n        \'report_config\',\n        None)\n    history_config = algo_req.get(\n        \'history_config\',\n        None)\n    extract_config = algo_req.get(\n        \'extract_config\',\n        None)\n\n    err = None\n    if algo_module_path:\n        found_algo_module = False\n        module_name = algo_module_path.split(\'/\')[-1]\n        loader = importlib.machinery.SourceFileLoader(\n            module_name,\n            algo_module_path)\n        custom_algo_module = types.ModuleType(\n            loader.name)\n        loader.exec_module(\n            custom_algo_module)\n        use_custom_algo = True\n\n        for member in inspect.getmembers(custom_algo_module):\n            if module_name in str(member):\n                found_algo_module = True\n                break\n        # for all members in this custom module file\n    # if loading a custom algorithm module from a file on disk\n\n    if not found_algo_module:\n        err = (\n            f\'{label} - unable to find custom algorithm \'\n            f\'module={custom_algo_module} module_path={algo_module_path}\')\n        if algo_module_path:\n            err = (\n                f\'{label} - analysis_engine.\'\n                \'work_tasks.run_distributed_algorithm was unable \'\n                f\'to find custom algorithm module={custom_algo_module} with \'\n                f\'provided path to \\n file: {algo_module_path} \\n\'\n                \'\\n\'\n                \'Please confirm \'\n                \'that the class inherits from the BaseAlgo class like:\\n\'\n                \'\\n\'\n                \'import analysis_engine.algo\\n\'\n                \'class MyAlgo(analysis_engine.algo.BaseAlgo):\\n \'\n                \'\\n\'\n                \'If it is then please file an issue on github:\\n \'\n                \'https://github.com/AlgoTraders/stock-analysis-engine/\'\n                \'issues/new \\n\\nFor now this error results in a shutdown\'\n                \'\\n\')\n        # if algo_module_path set\n\n        log.error(err)\n        res = build_result.build_result(\n            status=ae_consts.ERR,\n            err=err,\n            rec=None)\n        task_result = {\n            \'status\': res[\'status\'],\n            \'err\': res[\'err\'],\n            \'algo_req\': algo_req,\n            \'rec\': rec\n        }\n        return task_result\n    # if not found_algo_module\n\n    use_start_date = start_date\n    use_end_date = end_date\n    if not use_end_date:\n        end_date = datetime.datetime.utcnow()\n        use_end_date = end_date.strftime(\n            ae_consts.COMMON_TICK_DATE_FORMAT)\n    if not use_start_date:\n        start_date = end_date - datetime.timedelta(days=num_days_back)\n        use_start_date = start_date.strftime(\n            ae_consts.COMMON_TICK_DATE_FORMAT)\n    dataset_publish_extract = algo_req.get(\n        \'dataset_publish_extract\',\n        False)\n    dataset_publish_history = algo_req.get(\n        \'dataset_publish_history\',\n        False)\n    dataset_publish_report = algo_req.get(\n        \'dataset_publish_report\',\n        False)\n    try:\n        if use_custom_algo:\n            if verbose:\n                log.info(\n                    f\'inspecting {custom_algo_module} for class {module_name}\')\n            use_class_member_object = None\n            for member in inspect.getmembers(custom_algo_module):\n                if module_name in str(member):\n                    if verbose:\n                        log.info(f\'start {name} with {member[1]}\')\n                    use_class_member_object = member\n                    break\n            # end of looking over the class definition but did not find it\n\n            if use_class_member_object:\n                if algo_req.get(\'backtest\', False):\n                    new_algo_object = member[1](\n                        ticker=algo_req[\'ticker\'],\n                        config_dict=algo_req)\n                else:\n                    new_algo_object = member[1](\n                        **algo_req)\n            else:\n                err = (\n                    f\'{label} - did not find a derived \'\n                    \'analysis_engine.algo.BaseAlgo \'\n                    f\'class in the module file={algo_module_path} \'\n                    f\'for ticker={ticker} algo_name={name}\')\n                log.error(err)\n                res = build_result.build_result(\n                    status=ae_consts.ERR,\n                    err=err,\n                    rec=None)\n                task_result = {\n                    \'status\': res[\'status\'],\n                    \'err\': res[\'err\'],\n                    \'algo_req\': algo_req,\n                    \'rec\': rec\n                }\n                return task_result\n            # end of finding a valid algorithm object\n        else:\n            new_algo_object = ae_algo.BaseAlgo(\n                **algo_req)\n        # if using a custom module path or the BaseAlgo\n\n        if new_algo_object:\n            # heads up - logging this might have passwords in the algo_req\n            # log.debug(f\'{name} algorithm request: {algo_req}\')\n            if verbose:\n                log.info(\n                    f\'{name} - run START ticker={ticker} \'\n                    f\'from {use_start_date} to {use_end_date}\')\n            if algo_req.get(\'backtest\', False):\n                algo_res = run_algo.run_algo(\n                    algo=new_algo_object,\n                    config_dict=algo_req)\n                created_algo_object = new_algo_object\n            else:\n                algo_res = run_algo.run_algo(\n                    algo=new_algo_object,\n                    **algo_req)\n                created_algo_object = new_algo_object\n\n            if verbose:\n                log.info(\n                    f\'{name} - run DONE ticker={ticker} \'\n                    f\'from {use_start_date} to {use_end_date}\')\n            if debug:\n                if custom_algo_module:\n                    log.info(\n                        f\'{name} - done run_algo \'\n                        f\'custom_algo_module={custom_algo_module} \'\n                        f\'module_name={module_name} ticker={ticker} \'\n                        f\'from {use_start_date} to {use_end_date}\')\n                else:\n                    log.info(\n                        f\'{name} - done run_algo BaseAlgo ticker={ticker} \'\n                        f\'from {use_start_date} to {use_end_date}\')\n        else:\n            err = (\n                f\'{label} - missing a derived analysis_engine.algo.BaseAlgo \'\n                f\'class in the module file={algo_module_path} for \'\n                f\'ticker={ticker} algo_name={name}\')\n            log.error(err)\n            res = build_result.build_result(\n                status=ae_consts.ERR,\n                err=err,\n                rec=None)\n            task_result = {\n                \'status\': res[\'status\'],\n                \'err\': res[\'err\'],\n                \'algo_req\': algo_req,\n                \'rec\': rec\n            }\n            return task_result\n        # end of finding a valid algorithm object\n\n        if not created_algo_object:\n            err = (\n                f\'{label} - failed creating algorithm object - \'\n                f\'ticker={ticker} \'\n                f\'status={ae_consts.get_status(status=algo_res[""status""])} \'\n                f\'error={algo_res[""err""]} algo name={name} \'\n                f\'custom_algo_module={custom_algo_module} \'\n                f\'module_name={module_name} \'\n                f\'from {use_start_date} to {use_end_date}\')\n            res = build_result.build_result(\n                status=ae_consts.ERR,\n                err=err,\n                rec=None)\n            task_result = {\n                \'status\': res[\'status\'],\n                \'err\': res[\'err\'],\n                \'algo_req\': algo_req,\n                \'rec\': rec\n            }\n            return task_result\n        # end of stop early\n\n        if should_publish_extract_dataset or dataset_publish_extract:\n            s3_log = \'\'\n            redis_log = \'\'\n            file_log = \'\'\n            use_log = \'publish\'\n\n            if (extract_config[\'redis_address\'] and\n                    extract_config[\'redis_db\'] and\n                    extract_config[\'redis_key\']):\n                redis_log = (\n                    f\'redis://{extract_config[""redis_address""]}\'\n                    f\'@{extract_config[""redis_db""]}\'\n                    f\'/{extract_config[""redis_key""]}\')\n                use_log += f\' {redis_log}\'\n            else:\n                extract_config[\'redis_enabled\'] = False\n            if (extract_config[\'s3_address\'] and\n                    extract_config[\'s3_bucket\'] and\n                    extract_config[\'s3_key\']):\n                s3_log = (\n                    f\'s3://{extract_config[""s3_address""]}\'\n                    f\'/{extract_config[""s3_bucket""]}\'\n                    f\'/{extract_config[""s3_key""]}\')\n                use_log += f\' {s3_log}\'\n            else:\n                extract_config[\'s3_enabled\'] = False\n            if extract_config[\'output_file\']:\n                file_log = f\'file:{extract_config[""output_file""]}\'\n                use_log += f\' {file_log}\'\n\n            if verbose:\n                log.info(\n                    f\'{name} - publish - start ticker={ticker} \'\n                    f\'algorithm-ready {use_log}\')\n\n            publish_status = created_algo_object.publish_input_dataset(\n                **extract_config)\n            if publish_status != ae_consts.SUCCESS:\n                msg = (\n                    \'failed to publish algorithm-ready datasets with \'\n                    f\'status {ae_consts.get_status(status=publish_status)} \'\n                    f\'attempted to {use_log}\')\n                log.error(msg)\n                res = build_result.build_result(\n                    status=ae_consts.ERR,\n                    err=err,\n                    rec=None)\n                task_result = {\n                    \'status\': res[\'status\'],\n                    \'err\': res[\'err\'],\n                    \'algo_req\': algo_req,\n                    \'rec\': rec\n                }\n                return task_result\n            # end of stop early\n\n            if verbose:\n                log.info(\n                    f\'{name} - publish - done ticker={ticker} \'\n                    f\'algorithm-ready {use_log}\')\n        # if publish the algorithm-ready dataset\n\n        if should_publish_history_dataset or dataset_publish_history:\n            s3_log = \'\'\n            redis_log = \'\'\n            file_log = \'\'\n            use_log = \'publish\'\n\n            if (history_config[\'redis_address\'] and\n                    history_config[\'redis_db\'] and\n                    history_config[\'redis_key\']):\n                redis_log = (\n                    f\'redis://{history_config[""redis_address""]}\'\n                    f\'@{history_config[""redis_db""]}\'\n                    f\'/{history_config[""redis_key""]}\')\n                use_log += f\' {redis_log}\'\n            if (history_config[\'s3_address\'] and\n                    history_config[\'s3_bucket\'] and\n                    history_config[\'s3_key\']):\n                s3_log = (\n                    f\'s3://{history_config[""s3_address""]}\'\n                    f\'/{history_config[""s3_bucket""]}\'\n                    f\'/{history_config[""s3_key""]}\')\n                use_log += f\' {s3_log}\'\n            if history_config[\'output_file\']:\n                file_log = f\'file:{history_config[""output_file""]}\'\n                use_log += f\' {file_log}\'\n\n            if verbose:\n                log.info(\n                    f\'{name} - publish - start ticker={ticker} trading \'\n                    f\'history {use_log}\')\n\n            publish_status = \\\n                created_algo_object.publish_trade_history_dataset(\n                    **history_config)\n            if publish_status != ae_consts.SUCCESS:\n                msg = (\n                    \'failed to publish trading history datasets with \'\n                    f\'status {ae_consts.get_status(status=publish_status)} \'\n                    f\'attempted to {use_log}\')\n                log.error(msg)\n                res = build_result.build_result(\n                    status=ae_consts.ERR,\n                    err=err,\n                    rec=None)\n                task_result = {\n                    \'status\': res[\'status\'],\n                    \'err\': res[\'err\'],\n                    \'algo_req\': algo_req,\n                    \'rec\': rec\n                }\n                return task_result\n            # end of stop early\n\n            if verbose:\n                log.info(\n                    f\'{name} - publish - done ticker={ticker} trading \'\n                    f\'history {use_log}\')\n        # if publish an trading history dataset\n\n        if should_publish_report_dataset or dataset_publish_report:\n            s3_log = \'\'\n            redis_log = \'\'\n            file_log = \'\'\n            use_log = \'publish\'\n\n            if (report_config[\'redis_address\'] and\n                    report_config[\'redis_db\'] and\n                    report_config[\'redis_key\']):\n                redis_log = (\n                    f\'redis://{report_config[""redis_address""]}\'\n                    f\'@{report_config[""redis_db""]}\'\n                    f\'/{report_config[""redis_key""]}\')\n                use_log += f\' {redis_log}\'\n            if (report_config[\'s3_address\'] and\n                    report_config[\'s3_bucket\'] and\n                    report_config[\'s3_key\']):\n                s3_log = (\n                    f\'s3://{report_config[""s3_address""]}\'\n                    f\'/{report_config[""s3_bucket""]}\'\n                    f\'/{report_config[""s3_key""]}\')\n                use_log += f\' {s3_log}\'\n            if report_config[\'output_file\']:\n                file_log = f\' file:{report_config[""output_file""]}\'\n                use_log += f\' {file_log}\'\n\n            if verbose:\n                log.info(\n                    f\'{name} - publishing ticker={ticker} trading performance \'\n                    f\'report {use_log}\')\n\n            publish_status = created_algo_object.publish_report_dataset(\n                **report_config)\n            if publish_status != ae_consts.SUCCESS:\n                msg = (\n                    \'failed to publish trading performance \'\n                    \'report datasets with \'\n                    f\'status {ae_consts.get_status(status=publish_status)} \'\n                    f\'attempted to {use_log}\')\n                log.error(msg)\n                res = build_result.build_result(\n                    status=ae_consts.ERR,\n                    err=err,\n                    rec=None)\n                task_result = {\n                    \'status\': res[\'status\'],\n                    \'err\': res[\'err\'],\n                    \'algo_req\': algo_req,\n                    \'rec\': rec\n                }\n                return task_result\n            # end of stop early\n\n            if verbose:\n                log.info(\n                    f\'{name} - publish - done ticker={ticker} trading \'\n                    f\'performance report {use_log}\')\n        # if publish an trading performance report dataset\n\n        if verbose:\n            log.info(\n                f\'{name} - done publishing datasets for ticker={ticker} \'\n                f\'from {use_start_date} to {use_end_date}\')\n\n        rec[\'history_config\'] = history_config\n        rec[\'report_config\'] = report_config\n\n        res = build_result.build_result(\n            status=ae_consts.SUCCESS,\n            err=None,\n            rec=rec)\n\n    except Exception as e:\n        res = build_result.build_result(\n            status=ae_consts.ERR,\n            err=(\n                \'failed - run_distributed_algorithm \'\n                f\'dict={algo_req} with ex={e}\'),\n            rec=rec)\n        if raise_on_err:\n            raise e\n        else:\n            log.error(f\'{label} - {res[""err""]}\')\n    # end of try/ex\n\n    if verbose:\n        log.info(\n            \'task - run_distributed_algorithm done - \'\n            f\'{label} - status={ae_consts.get_status(res[""status""])}\')\n\n    task_result = {\n        \'status\': res[\'status\'],\n        \'err\': res[\'err\'],\n        \'algo_req\': algo_req,\n        \'rec\': rec\n    }\n    return task_result\n# end of run_distributed_algorithm\n'"
analysis_engine/work_tasks/task_run_algo.py,0,"b'""""""\nRunning Distributed Algorithms Across Many Celery Workers\n\nUse this module to handle algorithm backtesting (tuning)\nor for live trading.\n\nUnder the hood, this is a Celery task handler\nthat processes jobs from a broker\'s\nmessaging queue. This allows the Analysis Engine to process\nmany algorithmic workloads concurrently using Celery\'s horizontally-\nscalable worker pool architecture.\n\n""""""\n\nimport inspect\nimport types\nimport importlib.machinery\nimport datetime\nimport celery.task as celery_task\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.build_result as build_result\nimport analysis_engine.work_tasks.custom_task as custom_task\nimport analysis_engine.run_algo as run_algo\nimport analysis_engine.algo as ae_algo  # base algo\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\n@celery_task.task(\n    bind=True,\n    base=custom_task.CustomTask,\n    queue=\'task_run_algo\')\ndef task_run_algo(\n        self,\n        algo_req):\n    """"""task_run_algo\n\n    Process an Algorithm\n\n    :param algo_req: dictionary for key/values for\n        running an algorithm using Celery workers\n    """"""\n\n    label = algo_req.get(\n        \'name\',\n        \'ae-algo\')\n    verbose = algo_req.get(\n        \'verbose_task\',\n        False)\n    debug = algo_req.get(\n        \'debug\',\n        False)\n\n    # please be careful logging prod passwords:\n    if debug:\n        log.info(f\'task - {label} - start algo_req={algo_req}\')\n    elif verbose:\n        log.info(f\'task - {label} - start \')\n    # end of start log\n\n    rec = {}\n    res = build_result.build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec=rec)\n\n    created_algo_object = None\n    custom_algo_module = None\n    new_algo_object = None\n    use_custom_algo = False\n    found_algo_module = True  # assume the BaseAlgo\n    should_publish_extract_dataset = False\n    should_publish_history_dataset = False\n    should_publish_report_dataset = False\n\n    ticker = algo_req.get(\n        \'ticker\',\n        \'SPY\')\n    num_days_back = algo_req.get(\n        \'num_days_back\',\n        75)\n    name = algo_req.get(\n        \'name\',\n        \'ae-algo\')\n    algo_module_path = algo_req.get(\n        \'mod_path\',\n        None)\n    module_name = algo_req.get(\n        \'module_name\',\n        \'BaseAlgo\')\n    custom_algo_module = algo_req.get(\n        \'custom_algo_module\',\n        None)\n    new_algo_object = algo_req.get(\n        \'new_algo_object\',\n        None)\n    use_custom_algo = algo_req.get(\n        \'use_custom_algo\',\n        False)\n    should_publish_extract_dataset = algo_req.get(\n        \'should_publish_extract_dataset\',\n        False)\n    should_publish_history_dataset = algo_req.get(\n        \'should_publish_history_dataset\',\n        False)\n    should_publish_report_dataset = algo_req.get(\n        \'should_publish_report_dataset\',\n        False)\n    start_date = algo_req.get(\n        \'start_date\',\n        None)\n    end_date = algo_req.get(\n        \'end_date\',\n        None)\n    raise_on_err = algo_req.get(\n        \'raise_on_err\',\n        True)\n    report_config = algo_req.get(\n        \'report_config\',\n        None)\n    history_config = algo_req.get(\n        \'history_config\',\n        None)\n    extract_config = algo_req.get(\n        \'extract_config\',\n        None)\n\n    err = None\n    if algo_module_path:\n        found_algo_module = False\n        module_name = algo_module_path.split(\'/\')[-1]\n        loader = importlib.machinery.SourceFileLoader(\n            module_name,\n            algo_module_path)\n        custom_algo_module = types.ModuleType(\n            loader.name)\n        loader.exec_module(\n            custom_algo_module)\n        use_custom_algo = True\n\n        for member in inspect.getmembers(custom_algo_module):\n            if module_name in str(member):\n                found_algo_module = True\n                break\n        # for all members in this custom module file\n    # if loading a custom algorithm module from a file on disk\n\n    if not found_algo_module:\n        err = (\n            f\'{label} - unable to find custom algorithm \'\n            f\'module={custom_algo_module} module_path={algo_module_path}\')\n        if algo_module_path:\n            err = (\n                f\'{label} - analysis_engine.work_tasks.task_run_algo \'\n                \'was unable to find custom algorithm \'\n                f\'module={custom_algo_module} with provided path to \\n \'\n                f\'file: {algo_module_path} \\n\'\n                \'\\n\'\n                \'Please confirm \'\n                \'that the class inherits from the BaseAlgo class like:\\n\'\n                \'\\n\'\n                \'import analysis_engine.algo\\n\'\n                \'class MyAlgo(analysis_engine.algo.BaseAlgo):\\n \'\n                \'\\n\'\n                \'If it is then please file an issue on github:\\n \'\n                \'https://github.com/AlgoTraders/stock-analysis-engine/\'\n                \'issues/new \\n\\nFor now this error results in a shutdown\'\n                \'\\n\')\n        # if algo_module_path set\n\n        log.error(err)\n        res = build_result.build_result(\n            status=ae_consts.ERR,\n            err=err,\n            rec=None)\n        task_result = {\n            \'status\': res[\'status\'],\n            \'err\': res[\'err\'],\n            \'algo_req\': algo_req,\n            \'rec\': rec\n        }\n        return task_result\n    # if not found_algo_module\n\n    use_start_date = start_date\n    use_end_date = end_date\n    if not use_end_date:\n        end_date = datetime.datetime.utcnow()\n        use_end_date = end_date.strftime(\n            ae_consts.COMMON_TICK_DATE_FORMAT)\n    if not use_start_date:\n        start_date = end_date - datetime.timedelta(days=num_days_back)\n        use_start_date = start_date.strftime(\n            ae_consts.COMMON_TICK_DATE_FORMAT)\n    dataset_publish_extract = algo_req.get(\n        \'dataset_publish_extract\',\n        False)\n    dataset_publish_history = algo_req.get(\n        \'dataset_publish_history\',\n        False)\n    dataset_publish_report = algo_req.get(\n        \'dataset_publish_report\',\n        False)\n    try:\n        if use_custom_algo:\n            if verbose:\n                log.info(\n                    f\'inspecting {custom_algo_module} for class {module_name}\')\n            use_class_member_object = None\n            for member in inspect.getmembers(custom_algo_module):\n                if module_name in str(member):\n                    if verbose:\n                        log.info(f\'start {name} with {member[1]}\')\n                    use_class_member_object = member\n                    break\n            # end of looking over the class definition but did not find it\n\n            if use_class_member_object:\n                if algo_req.get(\'backtest\', False):\n                    new_algo_object = member[1](\n                        ticker=algo_req[\'ticker\'],\n                        config_dict=algo_req)\n                else:\n                    new_algo_object = member[1](\n                        **algo_req)\n            else:\n                err = (\n                    f\'{label} - did not find a derived \'\n                    \'analysis_engine.algo.BaseAlgo \'\n                    f\'class in the module file={algo_module_path} \'\n                    f\'for ticker={ticker} algo_name={name}\')\n                log.error(err)\n                res = build_result.build_result(\n                    status=ae_consts.ERR,\n                    err=err,\n                    rec=None)\n                task_result = {\n                    \'status\': res[\'status\'],\n                    \'err\': res[\'err\'],\n                    \'algo_req\': algo_req,\n                    \'rec\': rec\n                }\n                return task_result\n            # end of finding a valid algorithm object\n        else:\n            new_algo_object = ae_algo.BaseAlgo(\n                **algo_req)\n        # if using a custom module path or the BaseAlgo\n\n        if new_algo_object:\n            # heads up - logging this might have passwords in the algo_req\n            # log.debug(f\'{name} algorithm request: {algo_req}\')\n            if verbose:\n                log.info(\n                    f\'{name} - run START ticker={ticker} \'\n                    f\'from {use_start_date} to {use_end_date}\')\n            if algo_req.get(\'backtest\', False):\n                algo_res = run_algo.run_algo(\n                    algo=new_algo_object,\n                    config_dict=algo_req)\n                created_algo_object = new_algo_object\n            else:\n                algo_res = run_algo.run_algo(\n                    algo=new_algo_object,\n                    **algo_req)\n                created_algo_object = new_algo_object\n\n            if verbose:\n                log.info(\n                    f\'{name} - run DONE ticker={ticker} \'\n                    f\'from {use_start_date} to {use_end_date}\')\n            if debug:\n                if custom_algo_module:\n                    log.info(\n                        f\'{name} - done run_algo \'\n                        f\'custom_algo_module={custom_algo_module} \'\n                        f\'module_name={module_name} ticker={ticker} \'\n                        f\'from {use_start_date} to {use_end_date}\')\n                else:\n                    log.info(\n                        f\'{name} - done run_algo BaseAlgo ticker={ticker} \'\n                        f\'from {use_start_date} to {use_end_date}\')\n        else:\n            err = (\n                f\'{label} - missing a derived analysis_engine.algo.BaseAlgo \'\n                f\'class in the module file={algo_module_path} for \'\n                f\'ticker={ticker} algo_name={name}\')\n            log.error(err)\n            res = build_result.build_result(\n                status=ae_consts.ERR,\n                err=err,\n                rec=None)\n            task_result = {\n                \'status\': res[\'status\'],\n                \'err\': res[\'err\'],\n                \'algo_req\': algo_req,\n                \'rec\': rec\n            }\n            return task_result\n        # end of finding a valid algorithm object\n\n        if not created_algo_object:\n            err = (\n                f\'{label} - failed creating algorithm object - ticker={ticker}\'\n                f\' status={ae_consts.get_status(status=algo_res[""status""])} \'\n                f\'error={algo_res[""err""]} algo name={name} \'\n                f\'custom_algo_module={custom_algo_module} \'\n                f\'module_name={module_name} \'\n                f\'from {use_start_date} to {use_end_date}\')\n            res = build_result.build_result(\n                status=ae_consts.ERR,\n                err=err,\n                rec=None)\n            task_result = {\n                \'status\': res[\'status\'],\n                \'err\': res[\'err\'],\n                \'algo_req\': algo_req,\n                \'rec\': rec\n            }\n            return task_result\n        # end of stop early\n\n        if should_publish_extract_dataset or dataset_publish_extract:\n            s3_log = \'\'\n            redis_log = \'\'\n            file_log = \'\'\n            use_log = \'publish\'\n\n            if (extract_config[\'redis_address\'] and\n                    extract_config[\'redis_db\'] and\n                    extract_config[\'redis_key\']):\n                redis_log = (\n                    f\'redis://{extract_config[""redis_address""]}\'\n                    f\'@{extract_config[""redis_db""]}\'\n                    f\'/{extract_config[""redis_key""]}\')\n                use_log += f\' {redis_log}\'\n            else:\n                extract_config[\'redis_enabled\'] = False\n            if (extract_config[\'s3_address\'] and\n                    extract_config[\'s3_bucket\'] and\n                    extract_config[\'s3_key\']):\n                s3_log = (\n                    f\'s3://{extract_config[""s3_address""]}\'\n                    f\'/{extract_config[""s3_bucket""]}\'\n                    f\'/{extract_config[""s3_key""]}\')\n                use_log += f\' {s3_log}\'\n            else:\n                extract_config[\'s3_enabled\'] = False\n            if extract_config[\'output_file\']:\n                file_log = f\'file:{extract_config[""output_file""]}\'\n                use_log += f\' {file_log}\'\n\n            if verbose:\n                log.info(\n                    f\'{name} - publish - start ticker={ticker} \'\n                    f\'algorithm-ready {use_log}\')\n\n            publish_status = created_algo_object.publish_input_dataset(\n                **extract_config)\n            if publish_status != ae_consts.SUCCESS:\n                msg = (\n                    \'failed to publish algorithm-ready datasets with \'\n                    f\'status {ae_consts.get_status(status=publish_status)} \'\n                    f\'attempted to {use_log}\')\n                log.error(msg)\n                res = build_result.build_result(\n                    status=ae_consts.ERR,\n                    err=err,\n                    rec=None)\n                task_result = {\n                    \'status\': res[\'status\'],\n                    \'err\': res[\'err\'],\n                    \'algo_req\': algo_req,\n                    \'rec\': rec\n                }\n                return task_result\n            # end of stop early\n\n            if verbose:\n                log.info(\n                    f\'{name} - publish - done ticker={ticker} \'\n                    f\'algorithm-ready {use_log}\')\n        # if publish the algorithm-ready dataset\n\n        if should_publish_history_dataset or dataset_publish_history:\n            s3_log = \'\'\n            redis_log = \'\'\n            file_log = \'\'\n            use_log = \'publish\'\n\n            if (history_config[\'redis_address\'] and\n                    history_config[\'redis_db\'] and\n                    history_config[\'redis_key\']):\n                redis_log = (\n                    f\'redis://{history_config[""redis_address""]}\'\n                    f\'@{history_config[""redis_db""]}\'\n                    f\'/{history_config[""redis_key""]}\')\n                use_log += f\' {redis_log}\'\n            if (history_config[\'s3_address\'] and\n                    history_config[\'s3_bucket\'] and\n                    history_config[\'s3_key\']):\n                s3_log = (\n                    f\'s3://{history_config[""s3_address""]}\'\n                    f\'/{history_config[""s3_bucket""]}\'\n                    f\'/{history_config[""s3_key""]}\')\n                use_log += f\' {s3_log}\'\n            if history_config[\'output_file\']:\n                file_log = f\'file:{history_config[""output_file""]}\'\n                use_log += f\' {file_log}\'\n\n            if verbose:\n                log.info(\n                    f\'{name} - publish - start ticker={ticker} trading \'\n                    f\'history {use_log}\')\n\n            publish_status = \\\n                created_algo_object.publish_trade_history_dataset(\n                    **history_config)\n            if publish_status != ae_consts.SUCCESS:\n                msg = (\n                    \'failed to publish trading history datasets with \'\n                    f\'status {ae_consts.get_status(status=publish_status)} \'\n                    f\'attempted to {use_log}\')\n                log.error(msg)\n                res = build_result.build_result(\n                    status=ae_consts.ERR,\n                    err=err,\n                    rec=None)\n                task_result = {\n                    \'status\': res[\'status\'],\n                    \'err\': res[\'err\'],\n                    \'algo_req\': algo_req,\n                    \'rec\': rec\n                }\n                return task_result\n            # end of stop early\n\n            if verbose:\n                log.info(\n                    f\'{name} - publish - done ticker={ticker} trading \'\n                    f\'history {use_log}\')\n        # if publish an trading history dataset\n\n        if should_publish_report_dataset or dataset_publish_report:\n            s3_log = \'\'\n            redis_log = \'\'\n            file_log = \'\'\n            use_log = \'publish\'\n\n            if (report_config[\'redis_address\'] and\n                    report_config[\'redis_db\'] and\n                    report_config[\'redis_key\']):\n                redis_log = (\n                    f\'redis://{report_config[""redis_address""]}\'\n                    f\'@{report_config[""redis_db""]}\'\n                    f\'/{report_config[""redis_key""]}\')\n                use_log += f\' {redis_log}\'\n            if (report_config[\'s3_address\'] and\n                    report_config[\'s3_bucket\'] and\n                    report_config[\'s3_key\']):\n                s3_log = (\n                    f\'s3://{report_config[""s3_address""]}\'\n                    f\'/{report_config[""s3_bucket""]}\'\n                    f\'/{report_config[""s3_key""]}\')\n                use_log += f\' {s3_log}\'\n            if report_config[\'output_file\']:\n                file_log = f\' file:{report_config[""output_file""]}\'\n                use_log += f\' {file_log}\'\n\n            if verbose:\n                log.info(\n                    f\'{name} - publishing ticker={ticker} trading \'\n                    f\'performance report {use_log}\')\n\n            publish_status = created_algo_object.publish_report_dataset(\n                **report_config)\n            if publish_status != ae_consts.SUCCESS:\n                msg = (\n                    \'failed to publish trading performance \'\n                    \'report datasets with \'\n                    f\'status {ae_consts.get_status(status=publish_status)} \'\n                    f\'attempted to {use_log}\')\n                log.error(msg)\n                res = build_result.build_result(\n                    status=ae_consts.ERR,\n                    err=err,\n                    rec=None)\n                task_result = {\n                    \'status\': res[\'status\'],\n                    \'err\': res[\'err\'],\n                    \'algo_req\': algo_req,\n                    \'rec\': rec\n                }\n                return task_result\n            # end of stop early\n\n            if verbose:\n                log.info(\n                    f\'{name} - publish - done ticker={ticker} trading \'\n                    f\'performance report {use_log}\')\n        # if publish an trading performance report dataset\n\n        if verbose:\n            log.info(\n                f\'{name} - done publishing datasets for ticker={ticker} \'\n                f\'from {use_start_date} to {use_end_date}\')\n\n        rec[\'history_config\'] = history_config\n        rec[\'report_config\'] = report_config\n\n        res = build_result.build_result(\n            status=ae_consts.SUCCESS,\n            err=None,\n            rec=rec)\n\n    except Exception as e:\n        res = build_result.build_result(\n            status=ae_consts.ERR,\n            err=(\n                \'failed - task_run_algo \'\n                f\'dict={algo_req} with ex={e}\'),\n            rec=rec)\n        if raise_on_err:\n            raise e\n        else:\n            log.error(f\'{label} - {res[""err""]}\')\n    # end of try/ex\n\n    if verbose:\n        log.info(\n            \'task - task_run_algo done - \'\n            f\'{label} - status={ae_consts.get_status(res[""status""])}\')\n\n    task_result = {\n        \'status\': res[\'status\'],\n        \'err\': res[\'err\'],\n        \'algo_req\': algo_req,\n        \'rec\': rec\n    }\n    return task_result\n# end of task_run_algo\n'"
analysis_engine/work_tasks/task_screener_analysis.py,0,"b'""""""\nWork in progress - screener-driven analysis task\n\n**Supported environment variables**\n\n::\n\n    export DEBUG_RESULTS=1\n\n""""""\n\nimport os\nimport celery.task as celery_task\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.get_task_results as get_task_results\nimport analysis_engine.fetch as fetch_utils\nimport analysis_engine.build_result as build_result\nimport analysis_engine.finviz.fetch_api as finviz_utils\nimport analysis_engine.work_tasks.custom_task as custom_task\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\n@celery_task(\n    bind=True,\n    base=custom_task.CustomTask,\n    queue=\'task_screener_analysis\')\ndef task_screener_analysis(\n        self,\n        work_dict):\n    """"""task_screener_analysis\n\n    :param work_dict: task dictionary\n    """"""\n\n    label = work_dict.get(\n        \'label\',\n        \'screener\')\n\n    log.info(f\'{label} - start\')\n\n    rec = {}\n    res = build_result.build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec=rec)\n\n    """"""\n    Input - Set up dataset sources to collect\n    """"""\n\n    ticker = work_dict.get(\n        \'ticker\',\n        None)\n    org_tickers = work_dict.get(\n        \'tickers\',\n        None)\n\n    if not ticker and not org_tickers:\n        res = build_result.build_result(\n            status=ae_consts.ERR,\n            err=\'missing ticker or tickers\',\n            rec=rec)\n\n    tickers = []\n    if not org_tickers:\n        if ticker:\n            tickers = [\n                ticker\n            ]\n    else:\n        for t in org_tickers:\n            upper_cased_ticker = str(t).upper()\n            if upper_cased_ticker not in tickers:\n                tickers.append(upper_cased_ticker)\n        # build a unique ticker list\n    # end of ensuring tickers is a unique list of\n    # upper-cased ticker symbol strings\n\n    # fetch from: \'all\', \'iex\' or \'yahoo\'\n    fetch_mode = work_dict.get(\n        \'fetch_mode\',\n        os.getenv(\n            \'FETCH_MODE\',\n            \'iex\'))\n    iex_datasets = work_dict.get(\n        \'iex_datasets\',\n        os.getenv(\n            \'IEX_DATASETS_DEFAULT\',\n            ae_consts.IEX_DATASETS_DEFAULT))\n\n    # if defined, these are task functions for\n    # calling customiized determine Celery tasks\n    determine_sells_callback = work_dict.get(\n        \'determine_sells\',\n        None)\n    determine_buys_callback = work_dict.get(\n        \'determine_buys\',\n        None)\n\n    try:\n\n        log.info(\n            f\'{label} fetch={fetch_mode} tickers={tickers} \'\n            f\'iex_datasets={iex_datasets} \'\n            f\'sell_task={determine_sells_callback} \'\n            f\'buy_task={determine_buys_callback}\')\n\n        """"""\n        Input - Set up required urls for building buckets\n        """"""\n        fv_urls = work_dict.get(\n            \'urls\',\n            )\n\n        if not fv_urls:\n            res = build_result.build_result(\n                status=ae_consts.ERR,\n                err=\'missing required urls list of screeners\',\n                rec=rec)\n\n        # stop if something errored out with the\n        # celery helper for turning off celery to debug\n        # without an engine running\n        if res[\'err\']:\n            log.error(\n                f\'{label} - tickers={tickers} fetch={fetch_mode} \'\n                f\'iex_datasets={iex_datasets} hit validation err={res[""err""]}\')\n\n            return get_task_results.get_task_results(\n                work_dict=work_dict,\n                result=res)\n        # end of input validation checks\n\n        num_urls = len(fv_urls)\n        log.info(f\'{label} - running urls={fv_urls}\')\n\n        fv_dfs = []\n        for uidx, url in enumerate(fv_urls):\n            log.info(\n                f\'{label} - url={uidx}/{num_urls} url={url}\')\n            fv_res = finviz_utils.fetch_tickers_from_screener(\n                url=url)\n            if fv_res[\'status\'] == ae_consts.SUCCESS:\n                fv_dfs.append(fv_res[\'rec\'][\'data\'])\n                for ft_tick in fv_res[\'rec\'][\'tickers\']:\n                    upper_ft_ticker = ft_tick.upper()\n                    if upper_ft_ticker not in tickers:\n                        tickers.append(upper_ft_ticker)\n                # end of for all found tickers\n            else:\n                log.error(f\'{label} - failed url={uidx}/{num_urls} url={url}\')\n            # if success vs log the error\n        # end of urls to get pandas.DataFrame and unique tickers\n\n        """"""\n        Find tickers in screens\n        """"""\n\n        num_tickers = len(tickers)\n\n        log.info(\n            f\'{label} - fetching tickers={num_tickers} from urls={num_urls}\')\n\n        """"""\n        pull ticker data\n        """"""\n\n        fetch_recs = fetch_utils.fetch(\n            tickers=tickers,\n            fetch_mode=fetch_mode,\n            iex_datasets=iex_datasets)\n\n        if fetch_recs:\n            rec = fetch_recs\n\n            """"""\n            Output - Where is data getting cached and archived?\n            (this helps to retroactively evaluate trading performance)\n            """"""\n\n            res = build_result.build_result(\n                status=ae_consts.SUCCESS,\n                err=None,\n                rec=rec)\n        else:\n            err = (\n                f\'{label} - tickers={ticker} failed fetch={fetch_mode} \'\n                f\'iex_datasets={iex_datasets}\')\n            res = build_result.build_result(\n                status=ae_consts.ERR,\n                err=err,\n                rec=rec)\n\n        log.info(f\'{label} - done\')\n    except Exception as e:\n        err = (\n            f\'{label} - tickers={tickers} fetch={fetch_mode} hit ex={e} \')\n        log.error(err)\n        res = build_result.build_result(\n            status=ae_consts.ERR,\n            err=err,\n            rec=rec)\n    # end of try/ex\n\n    return get_task_results.get_task_results(\n        work_dict=work_dict,\n        result=res)\n# end of task_screener_analysis\n\n\ndef run_screener_analysis(\n        work_dict):\n    """"""run_screener_analysis\n\n    Celery wrapper for running without celery\n\n    :param work_dict: task data\n    """"""\n\n    fn_name = \'run_screener_analysis\'\n    label = f\'\'\'{fn_name} - {work_dict.get(\n        \'label\',\n        \'\')}\'\'\'\n\n    log.info(f\'{label} - start\')\n\n    response = build_result.build_result(\n        status=ae_consts.NOT_RUN,\n        err=None,\n        rec={})\n    task_res = {}\n\n    # allow running without celery\n    if ae_consts.is_celery_disabled(\n            work_dict=work_dict):\n        work_dict[\'celery_disabled\'] = True\n        task_res = task_screener_analysis(\n            work_dict)\n        if task_res:\n            response = task_res.get(\n                \'result\',\n                task_res)\n            if ae_consts.ev(\'DEBUG_RESULTS\', \'0\') == \'1\':\n                response_details = response\n                try:\n                    response_details = ae_consts.ppj(response)\n                except Exception:\n                    response_details = response\n\n                log.info(\n                    f\'{label} task result={response_details}\')\n        else:\n            log.error(\n                f\'{label} celery was disabled but the task={response} \'\n                \'did not return anything\')\n        # end of if response\n    else:\n        task_res = task_screener_analysis.delay(\n            work_dict=work_dict)\n        rec = {\n            \'task_id\': task_res\n        }\n        response = build_result.build_result(\n            status=ae_consts.SUCCESS,\n            err=None,\n            rec=rec)\n    # if celery enabled\n\n    if response:\n        if ae_consts.ev(\'DEBUG_RESULTS\', \'0\') == \'1\':\n            log.info(\n                f\'{label} - done \'\n                f\'status={ae_consts.get_status(response[""status""])} \'\n                f\'err={response[""err""]} rec={response[""rec""]}\')\n        else:\n            log.info(\n                f\'{label} - done \'\n                f\'status={ae_consts.get_status(response[""status""])} \'\n                f\'err={response[""err""]}\')\n    else:\n        log.info(f\'{label} - done no response\')\n    # end of if/else response\n\n    return response\n# end of run_screener_analysis\n'"
analysis_engine/yahoo/__init__.py,0,b''
analysis_engine/yahoo/consts.py,0,"b'""""""\nCommon and constants\n\nSupported environment variables:\n\n::\n\n    export DEFAULT_FETCH_DATASETS_YAHOO=""pricing_yahoo,options_yahoo,\n    news_yahoo""\n\n""""""\n\nimport os\nfrom spylunking.log.setup_logging import build_colorized_logger\n\nlog = build_colorized_logger(\n    name=__name__)\n\n\nFETCH_PRICING_YAHOO = 1000\nFETCH_OPTIONS_YAHOO = 1001\nFETCH_NEWS_YAHOO = 1002\n\nDATAFEED_PRICING_YAHOO = 1100\nDATAFEED_OPTIONS_YAHOO = 1101\nDATAFEED_NEWS_YAHOO = 1102\n\nDEFAULT_FETCH_DATASETS_YAHOO = [\n    FETCH_PRICING_YAHOO,\n    FETCH_OPTIONS_YAHOO,\n    FETCH_NEWS_YAHOO\n]\nTIMESENSITIVE_DATASETS_YAHOO = [\n    FETCH_PRICING_YAHOO,\n    FETCH_OPTIONS_YAHOO,\n    FETCH_NEWS_YAHOO\n]\n\nENV_FETCH_DATASETS_YAHOO = os.getenv(\n    \'DEFAULT_FETCH_DATASETS_YAHOO\',\n    None)\nFETCH_DATASETS_YAHOO = DEFAULT_FETCH_DATASETS_YAHOO\nif ENV_FETCH_DATASETS_YAHOO:\n    FETCH_DATASETS_YAHOO = \\\n        ENV_FETCH_DATASETS_YAHOO.split(\',\')\n\n\ndef get_ft_str_yahoo(\n        ft_type):\n    """"""get_ft_str_yahoo\n\n    :param ft_type: enum fetch type value to return\n                    as a string\n    """"""\n    if ft_type == FETCH_PRICING_YAHOO:\n        return \'pricing_yahoo\'\n    elif ft_type == FETCH_OPTIONS_YAHOO:\n        return \'options_yahoo\'\n    elif ft_type == FETCH_NEWS_YAHOO:\n        return \'news_yahoo\'\n    else:\n        return f\'unsupported ft_type={ft_type}\'\n# end of get_ft_str_yahoo\n\n\ndef get_datafeed_str_yahoo(\n        df_type):\n    """"""get_datafeed_str_yahoo\n\n    :param df_type: enum fetch type value to return\n                    as a string\n    """"""\n    if df_type == DATAFEED_PRICING_YAHOO:\n        return \'pricing_yahoo\'\n    elif df_type == DATAFEED_OPTIONS_YAHOO:\n        return \'options_yahoo\'\n    elif df_type == DATAFEED_NEWS_YAHOO:\n        return \'news_yahoo\'\n    else:\n        return f\'unsupported df_type={df_type}\'\n# end of get_datafeed_str_yahoo\n'"
analysis_engine/yahoo/extract_df_from_redis.py,0,"b'""""""\nExtract an Yahoo dataset from Redis (S3 support coming soon) and\nload it into a ``pandas.DataFrame``\n\nSupported environment variables:\n\n::\n\n    # verbose logging in this module\n    export DEBUG_EXTRACT=1\n\n    # verbose logging for just Redis operations in this module\n    export DEBUG_REDIS_EXTRACT=1\n\n    # verbose logging for just S3 operations in this module\n    export DEBUG_S3_EXTRACT=1\n\n    # to show debug, trace logging please export ``SHARED_LOG_CFG``\n    # to a debug logger json file. To turn on debugging for this\n    # library, you can export this variable to the repo\'s\n    # included file with the command:\n    export SHARED_LOG_CFG=/opt/sa/analysis_engine/log/debug-logging.json\n\n""""""\n\nimport pandas as pd\nimport analysis_engine.consts as ae_consts\nimport analysis_engine.utils as ae_utils\nimport analysis_engine.dataset_scrub_utils as scrub_utils\nimport analysis_engine.get_data_from_redis_key as redis_get\nimport analysis_engine.yahoo.consts as yahoo_consts\nimport spylunking.log.setup_logging as log_utils\n\nlog = log_utils.build_colorized_logger(name=__name__)\n\n\ndef extract_pricing_dataset(\n        work_dict,\n        scrub_mode=\'sort-by-date\'):\n    """"""extract_pricing_dataset\n\n    Extract the Yahoo pricing data for a ticker and\n    return it as a pandas Dataframe\n\n    :param work_dict: dictionary of args\n    :param scrub_mode: type of scrubbing handler to run\n    """"""\n    label = work_dict.get(\'label\', \'extract\')\n    ds_id = work_dict.get(\'ticker\')\n    df_type = yahoo_consts.DATAFEED_PRICING_YAHOO\n    df_str = yahoo_consts.get_datafeed_str_yahoo(df_type=df_type)\n    redis_key = work_dict.get(\n        \'redis_key\',\n        work_dict.get(\'pricing\', \'missing-redis-key\'))\n    s3_key = work_dict.get(\n        \'s3_key\',\n        work_dict.get(\'pricing\', \'missing-s3-key\'))\n    redis_host = work_dict.get(\n        \'redis_host\',\n        None)\n    redis_port = work_dict.get(\n        \'redis_port\',\n        None)\n    redis_db = work_dict.get(\n        \'redis_db\',\n        ae_consts.REDIS_DB)\n\n    log.debug(\n        f\'{label} - {df_str} - start - redis_key={redis_key} s3_key={s3_key}\')\n\n    if not redis_host and not redis_port:\n        redis_host = ae_consts.REDIS_ADDRESS.split(\':\')[0]\n        redis_port = ae_consts.REDIS_ADDRESS.split(\':\')[1]\n\n    df = None\n    status = ae_consts.NOT_RUN\n    try:\n        redis_rec = redis_get.get_data_from_redis_key(\n            label=label,\n            host=redis_host,\n            port=redis_port,\n            db=redis_db,\n            password=work_dict.get(\'password\', None),\n            key=redis_key,\n            decompress_df=True)\n\n        status = redis_rec[\'status\']\n        log.debug(\n            f\'{label} - {df_str} redis get data key={redis_key} \'\n            f\'status={ae_consts.get_status(status=status)}\')\n\n        if status == ae_consts.SUCCESS:\n            log.debug(f\'{label} - {df_str} redis convert pricing to json\')\n            cached_dict = redis_rec[\'rec\'][\'data\']\n            log.debug(f\'{label} - {df_str} redis convert pricing to df\')\n            try:\n                df = pd.DataFrame(\n                    cached_dict,\n                    index=[0])\n            except Exception:\n                log.debug(\n                    f\'{label} - {df_str} redis_key={redis_key} \'\n                    \'no pricing df found\')\n                return ae_consts.EMPTY, None\n            # end of try/ex to convert to df\n            log.debug(\n                f\'{label} - {df_str} redis_key={redis_key} done \'\n                \'convert pricing to df\')\n        else:\n            log.debug(\n                f\'{label} - {df_str} did not find valid redis pricing \'\n                f\'in redis_key={redis_key} \'\n                f\'status={ae_consts.get_status(status=status)}\')\n\n    except Exception as e:\n        log.debug(\n            f\'{label} - {df_str} - ds_id={ds_id} failed getting pricing from \'\n            f\'redis={redis_host}:{redis_port}@{redis_db} \'\n            f\'key={redis_key} ex={e}\')\n        return ae_consts.ERR, None\n    # end of try/ex extract from redis\n\n    log.debug(\n        f\'{label} - {df_str} ds_id={ds_id} extract scrub={scrub_mode}\')\n\n    scrubbed_df = scrub_utils.extract_scrub_dataset(\n        label=label,\n        scrub_mode=scrub_mode,\n        datafeed_type=df_type,\n        msg_format=\'df={} date_str={}\',\n        ds_id=ds_id,\n        df=df)\n\n    status = ae_consts.SUCCESS\n\n    return status, scrubbed_df\n# end of extract_pricing_dataset\n\n\ndef extract_yahoo_news_dataset(\n        work_dict,\n        scrub_mode=\'sort-by-date\'):\n    """"""extract_yahoo_news_dataset\n\n    Extract the Yahoo news data for a ticker and\n    return it as a pandas Dataframe\n\n    :param work_dict: dictionary of args\n    :param scrub_mode: type of scrubbing handler to run\n    """"""\n    label = work_dict.get(\'label\', \'extract\')\n    ds_id = work_dict.get(\'ticker\')\n    df_type = yahoo_consts.DATAFEED_NEWS_YAHOO\n    df_str = yahoo_consts.get_datafeed_str_yahoo(df_type=df_type)\n    redis_key = work_dict.get(\n        \'redis_key\',\n        work_dict.get(\'news\', \'missing-redis-key\'))\n    s3_key = work_dict.get(\n        \'s3_key\',\n        work_dict.get(\'news\', \'missing-s3-key\'))\n    redis_host = work_dict.get(\n        \'redis_host\',\n        None)\n    redis_port = work_dict.get(\n        \'redis_port\',\n        None)\n    redis_db = work_dict.get(\n        \'redis_db\',\n        ae_consts.REDIS_DB)\n\n    log.debug(\n        f\'{label} - {df_str} - start - redis_key={redis_key} s3_key={s3_key}\')\n\n    if not redis_host and not redis_port:\n        redis_host = ae_consts.REDIS_ADDRESS.split(\':\')[0]\n        redis_port = ae_consts.REDIS_ADDRESS.split(\':\')[1]\n\n    df = None\n    status = ae_consts.NOT_RUN\n    try:\n        redis_rec = redis_get.get_data_from_redis_key(\n            label=label,\n            host=redis_host,\n            port=redis_port,\n            db=redis_db,\n            password=work_dict.get(\'password\', None),\n            key=redis_key,\n            decompress_df=True)\n\n        status = redis_rec[\'status\']\n        log.debug(\n            f\'{label} - {df_str} redis get data key={redis_key} \'\n            f\'status={ae_consts.get_status(status=status)}\')\n\n        if status == ae_consts.SUCCESS:\n            cached_dict = redis_rec[\'rec\'][\'data\']\n            log.debug(f\'{label} - {df_str} redis convert news to df\')\n            try:\n                df = pd.DataFrame(\n                    cached_dict)\n            except Exception:\n                log.debug(\n                    f\'{label} - {df_str} redis_key={redis_key} \'\n                    \'no news df found\')\n                return ae_consts.EMPTY, None\n            # end of try/ex to convert to df\n            log.debug(\n                f\'{label} - {df_str} redis_key={redis_key} done \'\n                f\'convert news to df\')\n        else:\n            log.debug(\n                f\'{label} - {df_str} did not find valid redis news \'\n                f\'in redis_key={redis_key} \'\n                f\'status={ae_consts.get_status(status=status)}\')\n\n    except Exception as e:\n        log.debug(\n            f\'{label} - {df_str} - ds_id={ds_id} failed getting news from \'\n            f\'redis={redis_host}:{redis_port}@{redis_db} key={redis_key} \'\n            f\'ex={e}\')\n        return ae_consts.ERR, None\n    # end of try/ex extract from redis\n\n    log.debug(f\'{label} - {df_str} ds_id={ds_id} extract scrub={scrub_mode}\')\n\n    scrubbed_df = scrub_utils.extract_scrub_dataset(\n        label=label,\n        scrub_mode=scrub_mode,\n        datafeed_type=df_type,\n        msg_format=\'df={} date_str={}\',\n        ds_id=ds_id,\n        df=df)\n\n    status = ae_consts.SUCCESS\n\n    return status, scrubbed_df\n# end of extract_yahoo_news_dataset\n\n\ndef extract_option_calls_dataset(\n        work_dict,\n        scrub_mode=\'sort-by-date\'):\n    """"""extract_option_calls_dataset\n\n    Extract the Yahoo options calls for a ticker and\n    return it as a ``pandas.Dataframe``\n\n    :param work_dict: dictionary of args\n    :param scrub_mode: type of scrubbing handler to run\n    """"""\n    label = f\'{work_dict.get(""label"", ""extract"")}-calls\'\n    ds_id = work_dict.get(\'ticker\')\n    df_type = yahoo_consts.DATAFEED_OPTIONS_YAHOO\n    df_str = yahoo_consts.get_datafeed_str_yahoo(df_type=df_type)\n    redis_key = work_dict.get(\n        \'redis_key\',\n        work_dict.get(\'calls\', \'missing-redis-key\'))\n    s3_key = work_dict.get(\n        \'s3_key\',\n        work_dict.get(\'calls\', \'missing-s3-key\'))\n    redis_host = work_dict.get(\n        \'redis_host\',\n        None)\n    redis_port = work_dict.get(\n        \'redis_port\',\n        None)\n    redis_db = work_dict.get(\n        \'redis_db\',\n        ae_consts.REDIS_DB)\n\n    log.debug(\n        f\'{label} - {df_str} - start - redis_key={redis_key} s3_key={s3_key}\')\n\n    if not redis_host and not redis_port:\n        redis_host = ae_consts.REDIS_ADDRESS.split(\':\')[0]\n        redis_port = ae_consts.REDIS_ADDRESS.split(\':\')[1]\n\n    exp_date_str = None\n    calls_df = None\n    status = ae_consts.NOT_RUN\n    try:\n        redis_rec = redis_get.get_data_from_redis_key(\n            label=label,\n            host=redis_host,\n            port=redis_port,\n            db=redis_db,\n            password=work_dict.get(\'password\', None),\n            key=redis_key,\n            decompress_df=True)\n\n        status = redis_rec[\'status\']\n        log.debug(\n            f\'{label} - {df_str} redis get data key={redis_key} \'\n            f\'status={ae_consts.get_status(status=status)}\')\n\n        if status == ae_consts.SUCCESS:\n            calls_json = None\n            if \'calls\' in redis_rec[\'rec\'][\'data\']:\n                calls_json = redis_rec[\'rec\'][\'data\'][\'calls\']\n            else:\n                calls_json = redis_rec[\'rec\'][\'data\']\n            log.debug(f\'{label} - {df_str} redis convert calls to df\')\n            exp_date_str = None\n            try:\n                calls_df = pd.read_json(\n                    calls_json,\n                    orient=\'records\')\n                exp_epoch_value = calls_df[\'expiration\'].iloc[-1]\n                exp_date_str = ae_utils.convert_epoch_to_datetime_string(\n                    epoch=exp_epoch_value,\n                    fmt=ae_consts.COMMON_DATE_FORMAT,\n                    use_utc=True)\n            except Exception:\n                log.debug(\n                    f\'{label} - {df_str} redis_key={redis_key} \'\n                    \'no calls df found\')\n                return ae_consts.EMPTY, None\n            # end of try/ex to convert to df\n            log.debug(\n                f\'{label} - {df_str} redis_key={redis_key} \'\n                f\'calls={len(calls_df.index)} exp_date={exp_date_str}\')\n        else:\n            log.debug(\n                f\'{label} - {df_str} did not find valid redis option calls \'\n                f\'in redis_key={redis_key} \'\n                f\'status={ae_consts.get_status(status=status)}\')\n\n    except Exception as e:\n        log.debug(\n            f\'{label} - {df_str} - ds_id={ds_id} failed getting option calls \'\n            f\'from redis={redis_host}:{redis_port}@{redis_db} \'\n            f\'key={redis_key} ex={e}\')\n        return ae_consts.ERR, None\n    # end of try/ex extract from redis\n\n    log.debug(f\'{label} - {df_str} ds_id={ds_id} extract scrub={scrub_mode}\')\n\n    scrubbed_df = scrub_utils.extract_scrub_dataset(\n        label=label,\n        scrub_mode=scrub_mode,\n        datafeed_type=df_type,\n        msg_format=\'df={} date_str={}\',\n        ds_id=ds_id,\n        df=calls_df)\n\n    status = ae_consts.SUCCESS\n\n    return status, scrubbed_df\n# end of extract_option_calls_dataset\n\n\ndef extract_option_puts_dataset(\n        work_dict,\n        scrub_mode=\'sort-by-date\'):\n    """"""extract_option_puts_dataset\n\n    Extract the Yahoo options puts for a ticker and\n    return it as a ``pandas.Dataframe``\n\n    :param work_dict: dictionary of args\n    :param scrub_mode: type of scrubbing handler to run\n    """"""\n    label = f\'{work_dict.get(""label"", ""extract"")}-puts\'\n    ds_id = work_dict.get(\'ticker\')\n    df_type = yahoo_consts.DATAFEED_OPTIONS_YAHOO\n    df_str = yahoo_consts.get_datafeed_str_yahoo(df_type=df_type)\n    redis_key = work_dict.get(\n        \'redis_key\',\n        work_dict.get(\'puts\', \'missing-redis-key\'))\n    s3_key = work_dict.get(\n        \'s3_key\',\n        work_dict.get(\'puts\', \'missing-s3-key\'))\n    redis_host = work_dict.get(\n        \'redis_host\',\n        None)\n    redis_port = work_dict.get(\n        \'redis_port\',\n        None)\n    redis_db = work_dict.get(\n        \'redis_db\',\n        ae_consts.REDIS_DB)\n\n    log.debug(\n        f\'{label} - {df_str} - start - redis_key={redis_key} s3_key={s3_key}\')\n\n    if not redis_host and not redis_port:\n        redis_host = ae_consts.REDIS_ADDRESS.split(\':\')[0]\n        redis_port = ae_consts.REDIS_ADDRESS.split(\':\')[1]\n\n    exp_date_str = None\n    puts_df = None\n    status = ae_consts.NOT_RUN\n    try:\n        redis_rec = redis_get.get_data_from_redis_key(\n            label=label,\n            host=redis_host,\n            port=redis_port,\n            db=redis_db,\n            password=work_dict.get(\'password\', None),\n            key=redis_key,\n            decompress_df=True)\n\n        status = redis_rec[\'status\']\n        log.debug(\n            f\'{label} - {df_str} redis get data key={redis_key} \'\n            f\'status={ae_consts.get_status(status=status)}\')\n\n        if status == ae_consts.SUCCESS:\n            puts_json = None\n            if \'puts\' in redis_rec[\'rec\'][\'data\']:\n                puts_json = redis_rec[\'rec\'][\'data\'][\'puts\']\n            else:\n                puts_json = redis_rec[\'rec\'][\'data\']\n            log.debug(f\'{label} - {df_str} redis convert puts to df\')\n            try:\n                puts_df = pd.read_json(\n                    puts_json,\n                    orient=\'records\')\n                exp_epoch_value = puts_df[\'expiration\'].iloc[-1]\n                exp_date_str = ae_utils.convert_epoch_to_datetime_string(\n                    epoch=exp_epoch_value,\n                    fmt=ae_consts.COMMON_DATE_FORMAT,\n                    use_utc=True)\n            except Exception:\n                log.debug(\n                    f\'{label} - {df_str} redis_key={redis_key} \'\n                    \'no puts df found\')\n                return ae_consts.EMPTY, None\n            # end of try/ex to convert to df\n            log.debug(\n                f\'{label} - {df_str} redis_key={redis_key} \'\n                f\'puts={len(puts_df.index)} exp_date={exp_date_str}\')\n        else:\n            log.debug(\n                f\'{label} - {df_str} did not find valid redis option puts \'\n                f\'in redis_key={redis_key} \'\n                f\'status={ae_consts.get_status(status=status)}\')\n\n    except Exception as e:\n        log.debug(\n            f\'{label} - {df_str} - ds_id={ds_id} failed getting option puts \'\n            f\'from redis={redis_host}:{redis_port}@{redis_db} \'\n            f\'key={redis_key} ex={e}\')\n        return ae_consts.ERR, None\n    # end of try/ex extract from redis\n\n    log.debug(f\'{label} - {df_str} ds_id={ds_id} extract scrub={scrub_mode}\')\n\n    scrubbed_df = scrub_utils.extract_scrub_dataset(\n        label=label,\n        scrub_mode=scrub_mode,\n        datafeed_type=df_type,\n        msg_format=\'df={} date_str={}\',\n        ds_id=ds_id,\n        df=puts_df)\n\n    status = ae_consts.SUCCESS\n\n    return status, scrubbed_df\n# end of extract_option_puts_dataset\n'"
analysis_engine/yahoo/get_data.py,0,"b'""""""\nParse data from yahoo\n""""""\n\nimport copy\nimport datetime\nimport pinance\nimport analysis_engine.options_dates as opt_dates\nimport analysis_engine.get_pricing as yahoo_get_pricing\nimport analysis_engine.build_result as build_result\nimport analysis_engine.work_tasks.publish_pricing_update as \\\n    publisher\nimport spylunking.log.setup_logging as log_utils\nfrom analysis_engine.consts import TICKER\nfrom analysis_engine.consts import SUCCESS\nfrom analysis_engine.consts import NOT_RUN\nfrom analysis_engine.consts import ERR\nfrom analysis_engine.consts import NOT_SET\nfrom analysis_engine.consts import COMMON_TICK_DATE_FORMAT\nfrom analysis_engine.consts import EMPTY_DF_STR\nfrom analysis_engine.consts import get_status\nfrom analysis_engine.consts import ppj\nfrom analysis_engine.utils import get_last_close_str\n\nlog = log_utils.build_colorized_logger(\n    name=__name__)\n\n\ndef get_data_from_yahoo(\n        work_dict):\n    """"""get_data_from_yahoo\n\n    Get data from yahoo\n\n    :param work_dict: request dictionary\n    """"""\n    label = \'get_data_from_yahoo\'\n\n    log.info(f\'task - {label} - start work_dict={work_dict}\')\n\n    num_news_rec = 0\n    num_option_calls = 0\n    num_option_puts = 0\n    cur_high = -1\n    cur_low = -1\n    cur_open = -1\n    cur_close = -1\n    cur_volume = -1\n\n    rec = {\n        \'pricing\': None,\n        \'options\': None,\n        \'calls\': None,\n        \'puts\': None,\n        \'news\': None,\n        \'exp_date\': None,\n        \'publish_pricing_update\': None,\n        \'date\': None,\n        \'updated\': None\n    }\n    res = {\n        \'status\': NOT_RUN,\n        \'err\': None,\n        \'rec\': rec\n    }\n\n    try:\n\n        ticker = work_dict.get(\n            \'ticker\',\n            TICKER)\n        exp_date = work_dict.get(\n            \'exp_date\',\n            None)\n        cur_strike = work_dict.get(\n            \'strike\',\n            None)\n        contract_type = str(work_dict.get(\n            \'contract\',\n            \'C\')).upper()\n        get_pricing = work_dict.get(\n            \'get_pricing\',\n            True)\n        get_news = work_dict.get(\n            \'get_news\',\n            True)\n        get_options = work_dict.get(\n            \'get_options\',\n            True)\n        orient = work_dict.get(\n            \'orient\',\n            \'records\')\n        label = work_dict.get(\n            \'label\',\n            label)\n\n        ticker_results = pinance.Pinance(ticker)\n        num_news_rec = 0\n\n        use_date = exp_date\n        if not exp_date:\n            exp_date = opt_dates.option_expiration(\n                 date=exp_date)\n            use_date = exp_date.strftime(\'%Y-%m-%d\')\n\n        """"""\n        Debug control flags\n\n        Quickly turn specific fetches off:\n\n        get_news = False\n        get_pricing = False\n        get_options = False\n\n        """"""\n        if get_pricing:\n            log.info(f\'{label} getting ticker={ticker} pricing\')\n            ticker_results.get_quotes()\n            if ticker_results.quotes_data:\n                pricing_dict = ticker_results.quotes_data\n\n                cur_high = pricing_dict.get(\n                    \'regularMarketDayHigh\',\n                    None)\n                cur_low = pricing_dict.get(\n                    \'regularMarketDayLow\',\n                    None)\n                cur_open = pricing_dict.get(\n                    \'regularMarketOpen\',\n                    None)\n                cur_close = pricing_dict.get(\n                    \'regularMarketPreviousClose\',\n                    None)\n                cur_volume = pricing_dict.get(\n                    \'regularMarketVolume\',\n                    None)\n                pricing_dict[\'high\'] = cur_high\n                pricing_dict[\'low\'] = cur_low\n                pricing_dict[\'open\'] = cur_open\n                pricing_dict[\'close\'] = cur_close\n                pricing_dict[\'volume\'] = cur_volume\n                pricing_dict[\'date\'] = get_last_close_str()\n                if \'regularMarketTime\' in pricing_dict:\n                    pricing_dict[\'market_time\'] = \\\n                        datetime.datetime.fromtimestamp(\n                            pricing_dict[\'regularMarketTime\']).strftime(\n                                COMMON_TICK_DATE_FORMAT)\n                if \'postMarketTime\' in pricing_dict:\n                    pricing_dict[\'post_market_time\'] = \\\n                        datetime.datetime.fromtimestamp(\n                            pricing_dict[\'postMarketTime\']).strftime(\n                                COMMON_TICK_DATE_FORMAT)\n\n                log.info(\n                    f\'{label} ticker={ticker} converting pricing to \'\n                    f\'df orient={orient}\')\n\n                try:\n                    rec[\'pricing\'] = pricing_dict\n                except Exception as f:\n                    rec[\'pricing\'] = \'{}\'\n                    log.info(\n                        f\'{label} ticker={ticker} failed converting pricing \'\n                        f\'data={ppj(pricing_dict)} to df ex={f}\')\n                # try/ex\n\n                log.info(\n                    f\'{label} ticker={ticker} done converting pricing to \'\n                    f\'df orient={orient}\')\n\n            else:\n                log.error(\n                    f\'{label} ticker={ticker} \'\n                    f\'missing quotes_data={ticker_results.quotes_data}\')\n            # end of if ticker_results.quotes_data\n\n            log.info(\n                f\'{label} ticker={ticker} close={cur_close} vol={cur_volume}\')\n        else:\n            log.info(f\'{label} skip - getting ticker={ticker} pricing\')\n        # if get_pricing\n\n        if get_news:\n            log.info(\n                f\'{label} getting ticker={ticker} news\')\n            ticker_results.get_news()\n            if ticker_results.news_data:\n                news_list = None\n                try:\n                    news_list = ticker_results.news_data\n                    log.info(\n                        f\'{label} ticker={ticker} converting news to \'\n                        f\'df orient={orient}\')\n\n                    num_news_rec = len(news_list)\n\n                    rec[\'news\'] = news_list\n                except Exception as f:\n                    rec[\'news\'] = \'{}\'\n                    log.info(\n                        f\'{label} ticker={ticker} failed converting news \'\n                        f\'data={news_list} to df ex={f}\')\n                # try/ex\n\n                log.info(\n                    f\'{label} ticker={ticker} done converting news to \'\n                    f\'df orient={orient}\')\n            else:\n                log.info(\n                    f\'{label} ticker={ticker} Yahoo NO \'\n                    f\'news={ticker_results.news_data}\')\n            # end of if ticker_results.news_data\n        else:\n            log.info(\n                f\'{label} skip - getting ticker={ticker} news\')\n        # end if get_news\n\n        if get_options:\n\n            get_all_strikes = True\n            if get_all_strikes:\n                cur_strike = None\n            else:\n                if cur_close:\n                    cur_strike = int(cur_close)\n                if not cur_strike:\n                    cur_strike = 287\n\n            log.info(\n                f\'{label} ticker={ticker} num_news={num_news_rec} get options \'\n                f\'close={cur_close} exp_date={use_date} \'\n                f\'contract={contract_type} strike={cur_strike}\')\n\n            options_dict = \\\n                yahoo_get_pricing.get_options(\n                    ticker=ticker,\n                    exp_date_str=use_date,\n                    contract_type=contract_type,\n                    strike=cur_strike)\n\n            rec[\'options\'] = \'{}\'\n\n            try:\n                log.info(\n                    f\'{label} ticker={ticker} converting options to \'\n                    f\'df orient={orient}\')\n\n                num_option_calls = options_dict.get(\n                    \'num_calls\',\n                    None)\n                num_option_puts = options_dict.get(\n                    \'num_puts\',\n                    None)\n                rec[\'options\'] = {\n                    \'exp_date\': options_dict.get(\n                        \'exp_date\',\n                        None),\n                    \'calls\': options_dict.get(\n                        \'calls\',\n                        None),\n                    \'puts\': options_dict.get(\n                        \'puts\',\n                        None),\n                    \'num_calls\': num_option_calls,\n                    \'num_puts\': num_option_puts\n                }\n                rec[\'calls\'] = rec[\'options\'].get(\n                    \'calls\',\n                    EMPTY_DF_STR)\n                rec[\'puts\'] = rec[\'options\'].get(\n                    \'puts\',\n                    EMPTY_DF_STR)\n            except Exception as f:\n                rec[\'options\'] = \'{}\'\n                log.info(\n                    f\'{label} ticker={ticker} failed converting options \'\n                    f\'data={options_dict} to df ex={f}\')\n            # try/ex\n\n            log.info(\n                f\'{label} ticker={ticker} done converting options to \'\n                f\'df orient={orient} num_calls={num_option_calls} \'\n                f\'num_puts={num_option_puts}\')\n\n        else:\n            log.info(\n                f\'{label} skip - getting ticker={ticker} options\')\n        # end of if get_options\n\n        log.info(\n            f\'{label} yahoo pricing for ticker={ticker} close={cur_close} \'\n            f\'num_calls={num_option_calls} num_puts={num_option_puts} \'\n            f\'news={num_news_rec}\')\n\n        fields_to_upload = [\n            \'pricing\',\n            \'options\',\n            \'calls\',\n            \'puts\',\n            \'news\'\n        ]\n\n        for field_name in fields_to_upload:\n            upload_and_cache_req = copy.deepcopy(work_dict)\n            upload_and_cache_req[\'celery_disabled\'] = True\n            upload_and_cache_req[\'data\'] = rec[field_name]\n            if not upload_and_cache_req[\'data\']:\n                upload_and_cache_req[\'data\'] = \'{}\'\n\n            if \'redis_key\' in work_dict:\n                upload_and_cache_req[\'redis_key\'] = f\'\'\'{work_dict.get(\n                        \'redis_key\',\n                        f\'{ticker}_{field_name}\')}_{field_name}\'\'\'\n            if \'s3_key\' in work_dict:\n                upload_and_cache_req[\'s3_key\'] = f\'\'\'{work_dict.get(\n                        \'s3_key\',\n                        f\'{ticker}_{field_name}\')}_{field_name}\'\'\'\n            try:\n                update_res = publisher.run_publish_pricing_update(\n                    work_dict=upload_and_cache_req)\n                update_status = update_res.get(\n                    \'status\',\n                    NOT_SET)\n                log.info(\n                    f\'{label} publish update \'\n                    f\'status={get_status(status=update_status)} \'\n                    f\'data={update_res}\')\n            except Exception:\n                err = (\n                    f\'{label} - failed to upload YAHOO \'\n                    f\'data={upload_and_cache_req} to \'\n                    f\'s3_key={upload_and_cache_req[""s3_key""]} and \'\n                    f\'redis_key={upload_and_cache_req[""redis_key""]}\')\n                log.error(err)\n            # end of try/ex to upload and cache\n            if not rec[field_name]:\n                log.debug(\n                    f\'{label} - ticker={ticker} no data from YAHOO for \'\n                    f\'field_name={field_name}\')\n        # end of for all fields\n\n        res = build_result.build_result(\n            status=SUCCESS,\n            err=None,\n            rec=rec)\n    except Exception as e:\n        res = build_result.build_result(\n            status=ERR,\n            err=(\n                \'failed - get_data_from_yahoo \'\n                f\'dict={work_dict} with ex={e}\'),\n            rec=rec)\n        log.error(f\'{label} - {res[""err""]}\')\n    # end of try/ex\n\n    log.info(\n        \'task - get_data_from_yahoo done - \'\n        f\'{label} - status={get_status(res[""status""])}\')\n\n    return res\n# end of get_data_from_yahoo\n'"
docker/jupyter/jupyter_notebook_config.py,0,"b'import os\nfrom notebook.auth import passwd\n\nc = get_config()  # noqa\n\nc.NotebookApp.allow_origin = \'*\'\nc.NotebookApp.trust_xheaders = True\nc.NotebookApp.port = int(os.getenv(\n    ""JUPYTER_PORT"", ""8888""))\nc.NotebookApp.password = passwd(os.getenv(\n    ""JUPYTER_PASS"", ""admin""))\n'"
docs/source/conf.py,0,"b""# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath('.'))\n\n\n# -- Project information -----------------------------------------------------\n\nimport os\nimport sys\nfrom unittest.mock import MagicMock\nfrom recommonmark.parser import CommonMarkParser\n\non_rtd = os.getenv('READTHEDOCS', '') != ''\n\n# Using working source code sphinx conf.py on read the docs:\n# https://github.com/mahmoud/boltons/blob/master/docs/conf.py#L20\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nCUR_PATH = os.path.dirname(os.path.abspath(__file__))\nPROJECT_PATH = os.path.abspath(CUR_PATH + '/../')\nPACKAGE_SOURCE_PATH_FROM_DOCS = os.path.abspath('../../')\nsys.path.insert(0, PROJECT_PATH)\nsys.path.insert(0, PACKAGE_SOURCE_PATH_FROM_DOCS)\n\nsource_code_dirs = [\n    'analysis_engine/',\n    'analysis_engine/ai/',\n    'analysis_engine/iex/',\n    'analysis_engine/indicators/',\n    'analysis_engine/finviz/',\n    'analysis_engine/mocks/',\n    'analysis_engine/perf/',\n    'analysis_engine/scripts/',\n    'analysis_engine/td/',\n    'analysis_engine/work_tasks/',\n    'analysis_engine/yahoo/'\n]\n\nfor source_code_dir_name in source_code_dirs:\n    use_dir = f'{PACKAGE_SOURCE_PATH_FROM_DOCS}/{source_code_dir_name}'\n    if os.path.exists(use_dir):\n        sys.path.insert(0, use_dir)\n    else:\n        if on_rtd:\n            print(f'did not find from docs path dir: {use_dir}')\n# end for all source dirs\n\nproject = 'Stock Analysis Engine'\ncopyright = '2019, Jay Johnson'\nauthor = 'Jay Johnson'\n\nhtml_theme_options = {}\nif on_rtd:\n\n    class Mock(MagicMock):\n        @classmethod\n        def __getattr__(cls, name):\n            return MagicMock()\n\n    MOCK_MODULES = [\n        'h5py',\n        'pycurl',\n        'sklearn',\n        'sklearn.base',\n        'sklearn.cross_validation',\n        'sklearn.model_selection',\n        'sklearn.pipeline',\n        'sklearn.preprocessing',\n        'keras',\n        'keras.models',\n        'keras.layers',\n        'keras.wrappers.scikit_learn',\n        'tensorflow',\n        'tensorflow.aux-bin',\n        'tensorflow.contrib',\n        'tensorflow.core',\n        'tensorflow.examples',\n        'tensorflow.include',\n        'tensorflow.python',\n        'tensorflow.python.training',\n        'tensorflow.tools',\n        'vprof',\n        'vprof.runner'\n    ]\n    sys.modules.update((mod_name, Mock()) for mod_name in MOCK_MODULES)\n\n    html_theme_options = {\n        'canonical_url': '',\n        'logo_only': False,\n        'display_version': True,\n        'prev_next_buttons_location': 'bottom',\n        # Toc options\n        'collapse_navigation': False,\n        'sticky_navigation': True,\n        'navigation_depth': 4\n    }\n# if on readthedocs\n\n# -- Project information -----------------------------------------------------\n\nproject = 'Stock Analysis Engine'\ncopyright = '2019, Jay Johnson'\nauthor = 'Jay Johnson'\n\n\n# The short X.Y version\nversion = ''\n# The full version, including alpha/beta/rc tags\nrelease = '1.0.0'\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = '1.0'\n\nautosummary_generate = True\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.autosummary',\n    'sphinx.ext.doctest',\n    'sphinx.ext.intersphinx',\n    'sphinx.ext.coverage',\n    'sphinx.ext.viewcode',\n    'celery.contrib.sphinx',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\nsource_suffix = ['.rst', '.md']\n\nsource_parsers = {\n    '.md': CommonMarkParser,\n}\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set 'language' from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path .\nexclude_patterns = ['_build']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'sphinx'\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {'python': ('https://docs.python.org/2.7', None)}\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = os.getenv(\n    'DOC_THEME',\n    'sphinx_rtd_theme')\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named 'default.css' will overwrite the builtin 'default.css'.\nhtml_static_path = ['_static']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don't match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``['localtoc.html', 'relations.html', 'sourcelink.html',\n# 'searchbox.html']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'StockAnalysisEnginedoc'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size ('letterpaper' or 'a4paper').\n    #\n    # 'papersize': 'letterpaper',\n\n    # The font size ('10pt', '11pt' or '12pt').\n    #\n    # 'pointsize': '10pt',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # 'preamble': '',\n\n    # Latex figure (float) alignment\n    #\n    # 'figure_align': 'htbp',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc,\n     'StockAnalysisEngine.tex',\n     'Stock Analysis Engine Documentation',\n     'Jay Johnson',\n     'manual'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, 'stockanalysisengine', 'Stock Analysis Engine Documentation',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc,\n     'StockAnalysisEngine',\n     'Stock Analysis Engine Documentation',\n     author,\n     'StockAnalysisEngine',\n     'One line description of project.',\n     'Miscellaneous'),\n]\n\n\n# -- Extension configuration -------------------------------------------------\n\n# -- Options for intersphinx extension ---------------------------------------\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {'https://docs.python.org/': None}\n\n# -- Options for todo extension ----------------------------------------------\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n"""
