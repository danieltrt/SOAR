file_path,api_count,code
cocob.py,4,"b'# Copyright 2017 Francesco Orabona. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\n""""""\nCOntinuos COin Betting (COCOB) optimizer\nSee \'Training Deep Networks without Learning Rates Through Coin Betting\'\nhttps://arxiv.org/abs/1705.07795 \n""""""\n\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.training.optimizer import Optimizer\nimport tensorflow as tf\n\n\n\nclass COCOB(Optimizer):\n    def __init__(self, alpha=100, use_locking=False, name=\'COCOB\'):\n        \'\'\'\n        constructs a new COCOB optimizer\n        \'\'\'\n        super(COCOB, self).__init__(use_locking, name)\n        self._alpha = alpha\n\n    def _create_slots(self, var_list):\n        for v in var_list:\n            with ops.colocate_with(v):\n                gradients_sum = constant_op.constant(0,\n                                                     shape=v.get_shape(),\n                                                     dtype=v.dtype.base_dtype)\n                grad_norm_sum = constant_op.constant(0,\n                                                     shape=v.get_shape(),\n                                                     dtype=v.dtype.base_dtype)\n                L = constant_op.constant(1e-8, shape=v.get_shape(), dtype=v.dtype.base_dtype)\n                tilde_w = constant_op.constant(0.0, shape=v.get_shape(), dtype=v.dtype.base_dtype)\n                reward = constant_op.constant(0.0, shape=v.get_shape(), dtype=v.dtype.base_dtype)\n\n            self._get_or_make_slot(v, L, ""L"", self._name)\n            self._get_or_make_slot(v, grad_norm_sum, ""grad_norm_sum"", self._name)\n            self._get_or_make_slot(v, gradients_sum, ""gradients_sum"", self._name)\n            self._get_or_make_slot(v, tilde_w, ""tilde_w"", self._name)\n            self._get_or_make_slot(v, reward, ""reward"", self._name)\n\n    def _apply_dense(self, grad, var):\n        gradients_sum = self.get_slot(var, ""gradients_sum"")\n        grad_norm_sum = self.get_slot(var, ""grad_norm_sum"")\n        tilde_w = self.get_slot(var, ""tilde_w"")\n        L = self.get_slot(var, ""L"")\n        reward = self.get_slot(var, ""reward"")\n\n        L_update = tf.maximum(L, tf.abs(grad))\n        gradients_sum_update = gradients_sum + grad\n        grad_norm_sum_update = grad_norm_sum + tf.abs(grad)\n        reward_update = tf.maximum(reward - grad * tilde_w, 0)\n        new_w = -gradients_sum_update / (\n        L_update * (tf.maximum(grad_norm_sum_update + L_update, self._alpha * L_update))) * (reward_update + L_update)\n        var_update = var - tilde_w + new_w\n        tilde_w_update = new_w\n\n        gradients_sum_update_op = state_ops.assign(gradients_sum, gradients_sum_update)\n        grad_norm_sum_update_op = state_ops.assign(grad_norm_sum, grad_norm_sum_update)\n        var_update_op = state_ops.assign(var, var_update)\n        tilde_w_update_op = state_ops.assign(tilde_w, tilde_w_update)\n        L_update_op = state_ops.assign(L, L_update)\n        reward_update_op = state_ops.assign(reward, reward_update)\n\n        return control_flow_ops.group(*[gradients_sum_update_op,\n                                        var_update_op,\n                                        grad_norm_sum_update_op,\n                                        tilde_w_update_op,\n                                        reward_update_op,\n                                        L_update_op])\n\n    def _apply_sparse(self, grad, var):\n        return self._apply_dense(grad, var)\n\n    def _resource_apply_dense(self, grad, handle):\n        return self._apply_dense(grad, handle)\n'"
extractor.py,0,"b'import re\nimport pandas as pd\nimport numpy as np\n\nterm_pat = re.compile(\'(.+?):(.+)\')\npat = re.compile(\n    \'(.+)_([a-z][a-z]\\.)?((?:wikipedia\\.org)|(?:commons\\.wikimedia\\.org)|(?:www\\.mediawiki\\.org))_([a-z_-]+?)$\')\n\n# Debug output to ensure pattern still works\n# print(pat.fullmatch(\'BLEACH_zh.wikipedia.org_all-accessspider\').groups())\n# print(pat.fullmatch(\'Accueil_commons.wikimedia.org_all-access_spider\').groups())\n\n\ndef extract(source) -> pd.DataFrame:\n    """"""\n    Extracts features from url. Features: agent, site, country, term, marker\n    :param source: urls\n    :return: DataFrame, one column per feature\n    """"""\n    if isinstance(source, pd.Series):\n        source = source.values\n    agents = np.full_like(source, np.NaN)\n    sites = np.full_like(source, np.NaN)\n    countries = np.full_like(source, np.NaN)\n    terms = np.full_like(source, np.NaN)\n    markers = np.full_like(source, np.NaN)\n\n    for i in range(len(source)):\n        l = source[i]\n        match = pat.fullmatch(l)\n        assert match, ""Non-matched string %s"" % l\n        term = match.group(1)\n        country = match.group(2)\n        if country:\n            countries[i] = country[:-1]\n        site = match.group(3)\n        sites[i] = site\n        agents[i] = match.group(4)\n        if site != \'wikipedia.org\':\n            term_match = term_pat.match(term)\n            if term_match:\n                markers[i] = term_match.group(1)\n                term = term_match.group(2)\n        terms[i] = term\n\n    return pd.DataFrame({\n        \'agent\': agents,\n        \'site\': sites,\n        \'country\': countries,\n        \'term\': terms,\n        \'marker\': markers,\n        \'page\': source\n    })\n'"
feeder.py,13,"b'from collections import UserList, UserDict\nfrom typing import Union, Iterable, Tuple, Dict, Any\n\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport os.path\n\n\ndef _meta_file(path):\n    return os.path.join(path, \'feeder_meta.pkl\')\n\n\nclass VarFeeder:\n    """"""\n    Helper to avoid feed_dict and manual batching. Maybe I had to use TFRecords instead.\n    Builds temporary TF graph, injects variables into, and saves variables to TF checkpoint.\n    In a train time, variables can be built by build_vars() and content restored by FeederVars.restore()\n    """"""\n    def __init__(self, path: str,\n                 tensor_vars: Dict[str, Union[pd.DataFrame, pd.Series, np.ndarray]] = None,\n                 plain_vars: Dict[str, Any] = None):\n        """"""\n        :param path: dir to store data\n        :param tensor_vars: Variables to save as Tensors (pandas DataFrames/Series or numpy arrays)\n        :param plain_vars: Variables to save as Python objects\n        """"""\n        tensor_vars = tensor_vars or dict()\n\n        def get_values(v):\n            v = v.values if hasattr(v, \'values\') else v\n            if not isinstance(v, np.ndarray):\n                v = np.array(v)\n            if v.dtype == np.float64:\n                v = v.astype(np.float32)\n            return v\n\n        values = [get_values(var) for var in tensor_vars.values()]\n\n        self.shapes = [var.shape for var in values]\n        self.dtypes = [v.dtype for v in values]\n        self.names = list(tensor_vars.keys())\n        self.path = path\n        self.plain_vars = plain_vars\n\n        if not os.path.exists(path):\n            os.mkdir(path)\n\n        with open(_meta_file(path), mode=\'wb\') as file:\n            pickle.dump(self, file)\n\n        with tf.Graph().as_default():\n            tensor_vars = self._build_vars()\n            placeholders = [tf.placeholder(tf.as_dtype(dtype), shape=shape) for dtype, shape in\n                            zip(self.dtypes, self.shapes)]\n            assigners = [tensor_var.assign(placeholder) for tensor_var, placeholder in\n                         zip(tensor_vars, placeholders)]\n            feed = {ph: v for ph, v in zip(placeholders, values)}\n            saver = tf.train.Saver(self._var_dict(tensor_vars), max_to_keep=1)\n            init = tf.global_variables_initializer()\n\n            with tf.Session(config=tf.ConfigProto(device_count={\'GPU\': 0})) as sess:\n                sess.run(init)\n                sess.run(assigners, feed_dict=feed)\n                save_path = os.path.join(path, \'feeder.cpt\')\n                saver.save(sess, save_path, write_meta_graph=False, write_state=False)\n\n    def _var_dict(self, variables):\n        return {name: var for name, var in zip(self.names, variables)}\n\n    def _build_vars(self):\n        def make_tensor(shape, dtype, name):\n            tf_type = tf.as_dtype(dtype)\n            if tf_type == tf.string:\n                empty = \'\'\n            elif tf_type == tf.bool:\n                empty = False\n            else:\n                empty = 0\n            init = tf.constant(empty, shape=shape, dtype=tf_type)\n            return tf.get_local_variable(name=name, initializer=init, dtype=tf_type)\n\n        with tf.device(""/cpu:0""):\n            with tf.name_scope(\'feeder_vars\'):\n                return [make_tensor(shape, dtype, name) for shape, dtype, name in\n                        zip(self.shapes, self.dtypes, self.names)]\n\n    def create_vars(self):\n        """"""\n        Builds variable list to use in current graph. Should be called during graph building stage\n        :return: variable list with additional restore and create_saver methods\n        """"""\n        return FeederVars(self._var_dict(self._build_vars()), self.plain_vars, self.path)\n\n    @staticmethod\n    def read_vars(path):\n        with open(_meta_file(path), mode=\'rb\') as file:\n            feeder = pickle.load(file)\n        assert feeder.path == path\n        return feeder.create_vars()\n\n\nclass FeederVars(UserDict):\n    def __init__(self, tensors: dict, plain_vars: dict, path):\n        variables = dict(tensors)\n        if plain_vars:\n            variables.update(plain_vars)\n        super().__init__(variables)\n        self.path = path\n        self.saver = tf.train.Saver(tensors, name=\'varfeeder_saver\')\n        for var in variables:\n            if var not in self.__dict__:\n                self.__dict__[var] = variables[var]\n\n    def restore(self, session):\n        """"""\n        Restores variable content\n        :param session: current session\n        :return: variable list\n        """"""\n        self.saver.restore(session, os.path.join(self.path, \'feeder.cpt\'))\n        return self\n'"
hparams.py,0,"b'import tensorflow.contrib.training as training\nimport re\n\n# Manually selected params\nparams_s32 = dict(\n    batch_size=256,\n    #train_window=380,\n    train_window=283,\n    train_skip_first=0,\n    rnn_depth=267,\n    use_attn=False,\n    attention_depth=64,\n    attention_heads=1,\n    encoder_readout_dropout=0.4768781146510798,\n\n    encoder_rnn_layers=1,\n    decoder_rnn_layers=1,\n\n    # decoder_state_dropout_type=[\'outside\',\'outside\'],\n    decoder_input_dropout=[1.0, 1.0, 1.0],\n    decoder_output_dropout=[0.975, 1.0, 1.0],  # min 0.95\n    decoder_state_dropout=[0.99, 0.995, 0.995],  # min 0.95\n    decoder_variational_dropout=[False, False, False],\n    # decoder_candidate_l2=[0.0, 0.0],\n    # decoder_gates_l2=[0.0, 0.0],\n    #decoder_state_dropout_type=\'outside\',\n    #decoder_input_dropout=1.0,\n    #decoder_output_dropout=1.0,\n    #decoder_state_dropout=0.995, #0.98,  # min 0.95\n    # decoder_variational_dropout=False,\n    decoder_candidate_l2=0.0,\n    decoder_gates_l2=0.0,\n\n    fingerprint_fc_dropout=0.8232342370695286,\n    gate_dropout=0.9967589439360334,#0.9786,\n    gate_activation=\'none\',\n    encoder_dropout=0.030490422531402273,\n    encoder_stability_loss=0.0,  # max 100\n    encoder_activation_loss=1e-06, # max 0.001\n    decoder_stability_loss=0.0, # max 100\n    decoder_activation_loss=5e-06,  # max 0.001\n)\n\n# Default incumbent on last smac3 search\nparams_definc = dict(\n    batch_size=256,\n    train_window=100,\n    train_skip_first=0,\n    rnn_depth=128,\n    use_attn=True,\n    attention_depth=64,\n    attention_heads=1,\n    encoder_readout_dropout=0.4768781146510798,\n\n    encoder_rnn_layers=1,\n    decoder_rnn_layers=1,\n\n    decoder_input_dropout=[1.0, 1.0, 1.0],\n    decoder_output_dropout=[1.0, 1.0, 1.0],\n    decoder_state_dropout=[0.995, 0.995, 0.995],\n    decoder_variational_dropout=[False, False, False],\n    decoder_candidate_l2=0.0,\n    decoder_gates_l2=0.0,\n    fingerprint_fc_dropout=0.8232342370695286,\n    gate_dropout=0.8961710392091516,\n    gate_activation=\'none\',\n    encoder_dropout=0.030490422531402273,\n    encoder_stability_loss=0.0,\n    encoder_activation_loss=1e-05,\n    decoder_stability_loss=0.0,\n    decoder_activation_loss=5e-05,\n)\n\n# Found incumbent 0.35503610596060753\n#""decoder_activation_loss=\'1e-05\'"", ""decoder_output_dropout:0=\'1.0\'"", ""decoder_rnn_layers=\'1\'"", ""decoder_state_dropout:0=\'0.995\'"", ""encoder_activation_loss=\'1e-05\'"", ""encoder_rnn_layers=\'1\'"", ""gate_dropout=\'0.7934826952854418\'"", ""rnn_depth=\'243\'"", ""train_window=\'135\'"", ""use_attn=\'1\'"", ""attention_depth=\'17\'"", ""attention_heads=\'2\'"", ""encoder_readout_dropout=\'0.7711751356092252\'"", ""fingerprint_fc_dropout=\'0.9693950737901414\'""\nparams_foundinc = dict(\n    batch_size=256,\n    train_window=135,\n    train_skip_first=0,\n    rnn_depth=243,\n    use_attn=True,\n    attention_depth=17,\n    attention_heads=2,\n    encoder_readout_dropout=0.7711751356092252,\n\n    encoder_rnn_layers=1,\n    decoder_rnn_layers=1,\n\n    decoder_input_dropout=[1.0, 1.0, 1.0],\n    decoder_output_dropout=[1.0, 1.0, 1.0],\n    decoder_state_dropout=[0.995, 0.995, 0.995],\n    decoder_variational_dropout=[False, False, False],\n    decoder_candidate_l2=0.0,\n    decoder_gates_l2=0.0,\n    fingerprint_fc_dropout=0.9693950737901414,\n    gate_dropout=0.7934826952854418,\n    gate_activation=\'none\',\n    encoder_dropout=0.0,\n    encoder_stability_loss=0.0,\n    encoder_activation_loss=1e-05,\n    decoder_stability_loss=0.0,\n    decoder_activation_loss=1e-05,\n)\n\n# 81 on smac_run0 (0.3552077534247418 x 7)\n#{\'decoder_activation_loss\': 0.0, \'decoder_output_dropout:0\': 0.85, \'decoder_rnn_layers\': 2, \'decoder_state_dropout:0\': 0.995,\n# \'encoder_activation_loss\': 0.0, \'encoder_rnn_layers\': 2, \'gate_dropout\': 0.7665920904244501, \'rnn_depth\': 201,\n#  \'train_window\': 143, \'use_attn\': 1, \'attention_depth\': 17, \'attention_heads\': 2, \'decoder_output_dropout:1\': 0.975,\n# \'decoder_state_dropout:1\': 0.99, \'encoder_dropout\': 0.0304904225, \'encoder_readout_dropout\': 0.4444295965935664, \'fingerprint_fc_dropout\': 0.26412480387331017}\nparams_inst81 = dict(\n    batch_size=256,\n    train_window=143,\n    train_skip_first=0,\n    rnn_depth=201,\n    use_attn=True,\n    attention_depth=17,\n    attention_heads=2,\n    encoder_readout_dropout=0.4444295965935664,\n\n    encoder_rnn_layers=2,\n    decoder_rnn_layers=2,\n\n    decoder_input_dropout=[1.0, 1.0, 1.0],\n    decoder_output_dropout=[0.85, 0.975, 1.0],\n    decoder_state_dropout=[0.995, 0.99, 0.995],\n    decoder_variational_dropout=[False, False, False],\n    decoder_candidate_l2=0.0,\n    decoder_gates_l2=0.0,\n    fingerprint_fc_dropout=0.26412480387331017,\n    gate_dropout=0.7665920904244501,\n    gate_activation=\'none\',\n    encoder_dropout=0.0304904225,\n    encoder_stability_loss=0.0,\n    encoder_activation_loss=0.0,\n    decoder_stability_loss=0.0,\n    decoder_activation_loss=0.0,\n)\n# 121 on smac_run0 (0.3548671560628074 x 3)\n# {\'decoder_activation_loss\': 1e-05, \'decoder_output_dropout:0\': 0.975, \'decoder_rnn_layers\': 2, \'decoder_state_dropout:0\': 1.0,\n# \'encoder_activation_loss\': 1e-05, \'encoder_rnn_layers\': 1, \'gate_dropout\': 0.8631496699358483, \'rnn_depth\': 122,\n#  \'train_window\': 269, \'use_attn\': 1, \'attention_depth\': 29, \'attention_heads\': 4, \'decoder_output_dropout:1\': 0.975,\n# \'decoder_state_dropout:1\': 0.975, \'encoder_readout_dropout\': 0.9835390239895767, \'fingerprint_fc_dropout\': 0.7452161827064421}\n\n# 83 on smac_run1 (0.355050330259362 x 7)\n# {\'decoder_activation_loss\': 1e-06, \'decoder_output_dropout:0\': 0.925, \'decoder_rnn_layers\': 2, \'decoder_state_dropout:0\': 0.98,\n#  \'encoder_activation_loss\': 1e-06, \'encoder_rnn_layers\': 1, \'gate_dropout\': 0.9275441207192259, \'rnn_depth\': 138,\n# \'train_window\': 84, \'use_attn\': 1, \'attention_depth\': 52, \'attention_heads\': 2, \'decoder_output_dropout:1\': 0.925,\n# \'decoder_state_dropout:1\': 0.98, \'encoder_readout_dropout\': 0.6415488109353416, \'fingerprint_fc_dropout\': 0.2581296623398802}\n\n\nparams_inst83 = dict(\n    batch_size=256,\n    train_window=84,\n    train_skip_first=0,\n    rnn_depth=138,\n    use_attn=True,\n    attention_depth=52,\n    attention_heads=2,\n    encoder_readout_dropout=0.6415488109353416,\n\n    encoder_rnn_layers=1,\n    decoder_rnn_layers=2,\n\n    decoder_input_dropout=[1.0, 1.0, 1.0],\n    decoder_output_dropout=[0.925, 0.925, 1.0],\n    decoder_state_dropout=[0.98, 0.98, 0.995],\n    decoder_variational_dropout=[False, False, False],\n    decoder_candidate_l2=0.0,\n    decoder_gates_l2=0.0,\n    fingerprint_fc_dropout=0.2581296623398802,\n    gate_dropout=0.9275441207192259,\n    gate_activation=\'none\',\n    encoder_dropout=0.0,\n    encoder_stability_loss=0.0,\n    encoder_activation_loss=1e-06,\n    decoder_stability_loss=0.0,\n    decoder_activation_loss=1e-06,\n)\n\ndef_params = params_s32\n\nsets = {\n    \'s32\':params_s32,\n    \'definc\':params_definc,\n    \'foundinc\':params_foundinc,\n    \'inst81\':params_inst81,\n    \'inst83\':params_inst83,\n}\n\n\ndef build_hparams(params=def_params):\n    return training.HParams(**params)\n\n\ndef build_from_set(set_name):\n    return build_hparams(sets[set_name])\n\n\n'"
input_pipe.py,42,"b'import tensorflow as tf\n\nfrom feeder import VarFeeder\nfrom enum import Enum\nfrom typing import List, Iterable\nimport numpy as np\nimport pandas as pd\n\n\nclass ModelMode(Enum):\n    TRAIN = 0\n    EVAL = 1,\n    PREDICT = 2\n\n\nclass Split:\n    def __init__(self, test_set: List[tf.Tensor], train_set: List[tf.Tensor], test_size: int, train_size: int):\n        self.test_set = test_set\n        self.train_set = train_set\n        self.test_size = test_size\n        self.train_size = train_size\n\n\nclass Splitter:\n    def cluster_pages(self, cluster_idx: tf.Tensor):\n        """"""\n        Shuffles pages so all user_agents of each unique pages stays together in a shuffled list\n        :param cluster_idx: Tensor[uniq_pages, n_agents], each value is index of pair (uniq_page, agent) in other page tensors\n        :return: list of page indexes for use in a global page tensors\n        """"""\n        size = cluster_idx.shape[0].value\n        random_idx = tf.random_shuffle(tf.range(0, size, dtype=tf.int32), self.seed)\n        shuffled_pages = tf.gather(cluster_idx, random_idx)\n        # Drop non-existent (uniq_page, agent) pairs. Non-existent pair has index value = -1\n        mask = shuffled_pages >= 0\n        page_idx = tf.boolean_mask(shuffled_pages, mask)\n        return page_idx\n\n    def __init__(self, tensors: List[tf.Tensor], cluster_indexes: tf.Tensor, n_splits, seed, train_sampling=1.0,\n                 test_sampling=1.0):\n        size = tensors[0].shape[0].value\n        self.seed = seed\n        clustered_index = self.cluster_pages(cluster_indexes)\n        index_len = tf.shape(clustered_index)[0]\n        assert_op = tf.assert_equal(index_len, size, message=\'n_pages is not equals to size of clustered index\')\n        with tf.control_dependencies([assert_op]):\n            split_nitems = int(round(size / n_splits))\n            split_size = [split_nitems] * n_splits\n            split_size[-1] = size - (n_splits - 1) * split_nitems\n            splits = tf.split(clustered_index, split_size)\n            complements = [tf.random_shuffle(tf.concat(splits[:i] + splits[i + 1:], axis=0), seed) for i in\n                           range(n_splits)]\n            splits = [tf.random_shuffle(split, seed) for split in splits]\n\n        def mk_name(prefix, tensor):\n            return prefix + \'_\' + tensor.name[:-2]\n\n        def prepare_split(i):\n            test_size = split_size[i]\n            train_size = size - test_size\n            test_sampled_size = int(round(test_size * test_sampling))\n            train_sampled_size = int(round(train_size * train_sampling))\n            test_idx = splits[i][:test_sampled_size]\n            train_idx = complements[i][:train_sampled_size]\n            test_set = [tf.gather(tensor, test_idx, name=mk_name(\'test\', tensor)) for tensor in tensors]\n            tran_set = [tf.gather(tensor, train_idx, name=mk_name(\'train\', tensor)) for tensor in tensors]\n            return Split(test_set, tran_set, test_sampled_size, train_sampled_size)\n\n        self.splits = [prepare_split(i) for i in range(n_splits)]\n\n\nclass FakeSplitter:\n    def __init__(self, tensors: List[tf.Tensor], n_splits, seed, test_sampling=1.0):\n        total_pages = tensors[0].shape[0].value\n        n_pages = int(round(total_pages * test_sampling))\n\n        def mk_name(prefix, tensor):\n            return prefix + \'_\' + tensor.name[:-2]\n\n        def prepare_split(i):\n            idx = tf.random_shuffle(tf.range(0, n_pages, dtype=tf.int32), seed + i)\n            train_tensors = [tf.gather(tensor, idx, name=mk_name(\'shfl\', tensor)) for tensor in tensors]\n            if test_sampling < 1.0:\n                sampled_idx = idx[:n_pages]\n                test_tensors = [tf.gather(tensor, sampled_idx, name=mk_name(\'shfl_test\', tensor)) for tensor in tensors]\n            else:\n                test_tensors = train_tensors\n            return Split(test_tensors, train_tensors, n_pages, total_pages)\n\n        self.splits = [prepare_split(i) for i in range(n_splits)]\n\n\nclass InputPipe:\n    def cut(self, hits, start, end):\n        """"""\n        Cuts [start:end] diapason from input data\n        :param hits: hits timeseries\n        :param start: start index\n        :param end: end index\n        :return: tuple (train_hits, test_hits, dow, lagged_hits)\n        """"""\n        # Pad hits to ensure we have enough array length for prediction\n        hits = tf.concat([hits, tf.fill([self.predict_window], np.NaN)], axis=0)\n        cropped_hit = hits[start:end]\n\n        # cut day of week\n        cropped_dow = self.inp.dow[start:end]\n\n        # Cut lagged hits\n        # gather() accepts only int32 indexes\n        cropped_lags = tf.cast(self.inp.lagged_ix[start:end], tf.int32)\n        # Mask for -1 (no data) lag indexes\n        lag_mask = cropped_lags < 0\n        # Convert -1 to 0 for gather(), it don\'t accept anything exotic\n        cropped_lags = tf.maximum(cropped_lags, 0)\n        # Translate lag indexes to hit values\n        lagged_hit = tf.gather(hits, cropped_lags)\n        # Convert masked (see above) or NaN lagged hits to zeros\n        lag_zeros = tf.zeros_like(lagged_hit)\n        lagged_hit = tf.where(lag_mask | tf.is_nan(lagged_hit), lag_zeros, lagged_hit)\n\n        # Split for train and test\n        x_hits, y_hits = tf.split(cropped_hit, [self.train_window, self.predict_window], axis=0)\n\n        # Convert NaN to zero in for train data\n        x_hits = tf.where(tf.is_nan(x_hits), tf.zeros_like(x_hits), x_hits)\n        return x_hits, y_hits, cropped_dow, lagged_hit\n\n    def cut_train(self, hits, *args):\n        """"""\n        Cuts a segment of time series for training. Randomly chooses starting point.\n        :param hits: hits timeseries\n        :param args: pass-through data, will be appended to result\n        :return: result of cut() + args\n        """"""\n        n_days = self.predict_window + self.train_window\n        # How much free space we have to choose starting day\n        free_space = self.inp.data_days - n_days - self.back_offset - self.start_offset\n        if self.verbose:\n            lower_train_start = self.inp.data_start + pd.Timedelta(self.start_offset, \'D\')\n            lower_test_end = lower_train_start + pd.Timedelta(n_days, \'D\')\n            lower_test_start = lower_test_end - pd.Timedelta(self.predict_window, \'D\')\n            upper_train_start = self.inp.data_start + pd.Timedelta(free_space - 1, \'D\')\n            upper_test_end = upper_train_start + pd.Timedelta(n_days, \'D\')\n            upper_test_start = upper_test_end - pd.Timedelta(self.predict_window, \'D\')\n            print(f""Free space for training: {free_space} days."")\n            print(f"" Lower train {lower_train_start}, prediction {lower_test_start}..{lower_test_end}"")\n            print(f"" Upper train {upper_train_start}, prediction {upper_test_start}..{upper_test_end}"")\n        # Random starting point\n        offset = tf.random_uniform((), self.start_offset, free_space, dtype=tf.int32, seed=self.rand_seed)\n        end = offset + n_days\n        # Cut all the things\n        return self.cut(hits, offset, end) + args\n\n    def cut_eval(self, hits, *args):\n        """"""\n        Cuts segment of time series for evaluation.\n        Always cuts train_window + predict_window length segment beginning at start_offset point\n        :param hits: hits timeseries\n        :param args: pass-through data, will be appended to result\n        :return: result of cut() + args\n        """"""\n        end = self.start_offset + self.train_window + self.predict_window\n        return self.cut(hits, self.start_offset, end) + args\n\n    def reject_filter(self, x_hits, y_hits, *args):\n        """"""\n        Rejects timeseries having too many zero datapoints (more than self.max_train_empty)\n        """"""\n        if self.verbose:\n            print(""max empty %d train %d predict"" % (self.max_train_empty, self.max_predict_empty))\n        zeros_x = tf.reduce_sum(tf.to_int32(tf.equal(x_hits, 0.0)))\n        keep = zeros_x <= self.max_train_empty\n        return keep\n\n    def make_features(self, x_hits, y_hits, dow, lagged_hits, pf_agent, pf_country, pf_site, page_ix,\n                      page_popularity, year_autocorr, quarter_autocorr):\n        """"""\n        Main method. Assembles input data into final tensors\n        """"""\n        # Split day of week to train and test\n        x_dow, y_dow = tf.split(dow, [self.train_window, self.predict_window], axis=0)\n\n        # Normalize hits\n        mean = tf.reduce_mean(x_hits)\n        std = tf.sqrt(tf.reduce_mean(tf.squared_difference(x_hits, mean)))\n        norm_x_hits = (x_hits - mean) / std\n        norm_y_hits = (y_hits - mean) / std\n        norm_lagged_hits = (lagged_hits - mean) / std\n\n        # Split lagged hits to train and test\n        x_lagged, y_lagged = tf.split(norm_lagged_hits, [self.train_window, self.predict_window], axis=0)\n\n        # Combine all page features into single tensor\n        stacked_features = tf.stack([page_popularity, quarter_autocorr, year_autocorr])\n        flat_page_features = tf.concat([pf_agent, pf_country, pf_site, stacked_features], axis=0)\n        page_features = tf.expand_dims(flat_page_features, 0)\n\n        # Train features\n        x_features = tf.concat([\n            # [n_days] -> [n_days, 1]\n            tf.expand_dims(norm_x_hits, -1),\n            x_dow,\n            x_lagged,\n            # Stretch page_features to all training days\n            # [1, features] -> [n_days, features]\n            tf.tile(page_features, [self.train_window, 1])\n        ], axis=1)\n\n        # Test features\n        y_features = tf.concat([\n            # [n_days] -> [n_days, 1]\n            y_dow,\n            y_lagged,\n            # Stretch page_features to all testing days\n            # [1, features] -> [n_days, features]\n            tf.tile(page_features, [self.predict_window, 1])\n        ], axis=1)\n\n        return x_hits, x_features, norm_x_hits, x_lagged, y_hits, y_features, norm_y_hits, mean, std, flat_page_features, page_ix\n\n    def __init__(self, inp: VarFeeder, features: Iterable[tf.Tensor], n_pages: int, mode: ModelMode, n_epoch=None,\n                 batch_size=127, runs_in_burst=1, verbose=True, predict_window=60, train_window=500,\n                 train_completeness_threshold=1, predict_completeness_threshold=1, back_offset=0,\n                 train_skip_first=0, rand_seed=None):\n        """"""\n        Create data preprocessing pipeline\n        :param inp: Raw input data\n        :param features: Features tensors (subset of data in inp)\n        :param n_pages: Total number of pages\n        :param mode: Train/Predict/Eval mode selector\n        :param n_epoch: Number of epochs. Generates endless data stream if None\n        :param batch_size:\n        :param runs_in_burst: How many batches can be consumed at short time interval (burst). Multiplicator for prefetch()\n        :param verbose: Print additional information during graph construction\n        :param predict_window: Number of days to predict\n        :param train_window: Use train_window days for traning\n        :param train_completeness_threshold: Percent of zero datapoints allowed in train timeseries.\n        :param predict_completeness_threshold: Percent of zero datapoints allowed in test/predict timeseries.\n        :param back_offset: Don\'t use back_offset days at the end of timeseries\n        :param train_skip_first: Don\'t use train_skip_first days at the beginning of timeseries\n        :param rand_seed:\n\n        """"""\n        self.n_pages = n_pages\n        self.inp = inp\n        self.batch_size = batch_size\n        self.rand_seed = rand_seed\n        self.back_offset = back_offset\n        if verbose:\n            print(""Mode:%s, data days:%d, Data start:%s, data end:%s, features end:%s "" % (\n            mode, inp.data_days, inp.data_start, inp.data_end, inp.features_end))\n\n        if mode == ModelMode.TRAIN:\n            # reserve predict_window at the end for validation\n            assert inp.data_days - predict_window > predict_window + train_window, \\\n                ""Predict+train window length (+predict window for validation) is larger than total number of days in dataset""\n            self.start_offset = train_skip_first\n        elif mode == ModelMode.EVAL or mode == ModelMode.PREDICT:\n            self.start_offset = inp.data_days - train_window - back_offset\n            if verbose:\n                train_start = inp.data_start + pd.Timedelta(self.start_offset, \'D\')\n                eval_start = train_start + pd.Timedelta(train_window, \'D\')\n                end = eval_start + pd.Timedelta(predict_window - 1, \'D\')\n                print(""Train start %s, predict start %s, end %s"" % (train_start, eval_start, end))\n            assert self.start_offset >= 0\n\n        self.train_window = train_window\n        self.predict_window = predict_window\n        self.attn_window = train_window - predict_window + 1\n        self.max_train_empty = int(round(train_window * (1 - train_completeness_threshold)))\n        self.max_predict_empty = int(round(predict_window * (1 - predict_completeness_threshold)))\n        self.mode = mode\n        self.verbose = verbose\n\n        # Reserve more processing threads for eval/predict because of larger batches\n        num_threads = 3 if mode == ModelMode.TRAIN else 6\n\n        # Choose right cutter function for current ModelMode\n        cutter = {ModelMode.TRAIN: self.cut_train, ModelMode.EVAL: self.cut_eval, ModelMode.PREDICT: self.cut_eval}\n        # Create dataset, transform features and assemble batches\n        root_ds = tf.data.Dataset.from_tensor_slices(tuple(features)).repeat(n_epoch)\n        batch = (root_ds\n                 .map(cutter[mode])\n                 .filter(self.reject_filter)\n                 .map(self.make_features, num_parallel_calls=num_threads)\n                 .batch(batch_size)\n                 .prefetch(runs_in_burst * 2)\n                 )\n\n        self.iterator = batch.make_initializable_iterator()\n        it_tensors = self.iterator.get_next()\n\n        # Assign all tensors to class variables\n        self.true_x, self.time_x, self.norm_x, self.lagged_x, self.true_y, self.time_y, self.norm_y, self.norm_mean, \\\n        self.norm_std, self.page_features, self.page_ix = it_tensors\n\n        self.encoder_features_depth = self.time_x.shape[2].value\n\n    def load_vars(self, session):\n        self.inp.restore(session)\n\n    def init_iterator(self, session):\n        session.run(self.iterator.initializer)\n\n\ndef page_features(inp: VarFeeder):\n    return (inp.hits, inp.pf_agent, inp.pf_country, inp.pf_site,\n            inp.page_ix, inp.page_popularity, inp.year_autocorr, inp.quarter_autocorr)\n'"
make_features.py,0,"b'import pandas as pd\nimport numpy as np\nimport os.path\nimport os\nimport argparse\n\nimport extractor\nfrom feeder import VarFeeder\nimport numba\nfrom typing import Tuple, Dict, Collection, List\n\n\ndef read_cached(name) -> pd.DataFrame:\n    """"""\n    Reads csv file (maybe zipped) from data directory and caches it\'s content as a pickled DataFrame\n    :param name: file name without extension\n    :return: file content\n    """"""\n    cached = \'data/%s.pkl\' % name\n    sources = [\'data/%s.csv\' % name, \'data/%s.csv.zip\' % name]\n    if os.path.exists(cached):\n        return pd.read_pickle(cached)\n    else:\n        for src in sources:\n            if os.path.exists(src):\n                df = pd.read_csv(src)\n                df.to_pickle(cached)\n                return df\n\n\ndef read_all() -> pd.DataFrame:\n    """"""\n    Reads source data for training/prediction\n    """"""\n    def read_file(file):\n        df = read_cached(file).set_index(\'Page\')\n        df.columns = df.columns.astype(\'M8[D]\')\n        return df\n\n    # Path to cached data\n    path = os.path.join(\'data\', \'all.pkl\')\n    if os.path.exists(path):\n        df = pd.read_pickle(path)\n    else:\n        # Official data\n        df = read_file(\'train_2\')\n        # Scraped data\n        scraped = read_file(\'2017-08-15_2017-09-11\')\n        # Update last two days by scraped data\n        df[pd.Timestamp(\'2017-09-10\')] = scraped[\'2017-09-10\']\n        df[pd.Timestamp(\'2017-09-11\')] = scraped[\'2017-09-11\']\n\n        df = df.sort_index()\n        # Cache result\n        df.to_pickle(path)\n    return df\n\n# todo:remove\ndef make_holidays(tagged, start, end) -> pd.DataFrame:\n    def read_df(lang):\n        result = pd.read_pickle(\'data/holidays/%s.pkl\' % lang)\n        return result[~result.dw].resample(\'D\').size().rename(lang)\n\n    holidays = pd.DataFrame([read_df(lang) for lang in [\'de\', \'en\', \'es\', \'fr\', \'ja\', \'ru\', \'zh\']])\n    holidays = holidays.loc[:, start:end].fillna(0)\n    result =tagged[[\'country\']].join(holidays, on=\'country\').drop(\'country\', axis=1).fillna(0).astype(np.int8)\n    result.columns = pd.DatetimeIndex(result.columns.values)\n    return result\n\n\ndef read_x(start, end) -> pd.DataFrame:\n    """"""\n    Gets source data from start to end date. Any date can be None\n    """"""\n    df = read_all()\n    # User GoogleAnalitycsRoman has really bad data with huge traffic spikes in all incarnations.\n    # Wikipedia banned him, we\'ll ban it too\n    bad_roman = df.index.str.startswith(""User:GoogleAnalitycsRoman"")\n    df = df[~bad_roman]\n    if start and end:\n        return df.loc[:, start:end]\n    elif end:\n        return df.loc[:, :end]\n    else:\n        return df\n\n\n@numba.jit(nopython=True)\ndef single_autocorr(series, lag):\n    """"""\n    Autocorrelation for single data series\n    :param series: traffic series\n    :param lag: lag, days\n    :return:\n    """"""\n    s1 = series[lag:]\n    s2 = series[:-lag]\n    ms1 = np.mean(s1)\n    ms2 = np.mean(s2)\n    ds1 = s1 - ms1\n    ds2 = s2 - ms2\n    divider = np.sqrt(np.sum(ds1 * ds1)) * np.sqrt(np.sum(ds2 * ds2))\n    return np.sum(ds1 * ds2) / divider if divider != 0 else 0\n\n\n@numba.jit(nopython=True)\ndef batch_autocorr(data, lag, starts, ends, threshold, backoffset=0):\n    """"""\n    Calculate autocorrelation for batch (many time series at once)\n    :param data: Time series, shape [n_pages, n_days]\n    :param lag: Autocorrelation lag\n    :param starts: Start index for each series\n    :param ends: End index for each series\n    :param threshold: Minimum support (ratio of time series length to lag) to calculate meaningful autocorrelation.\n    :param backoffset: Offset from the series end, days.\n    :return: autocorrelation, shape [n_series]. If series is too short (support less than threshold),\n    autocorrelation value is NaN\n    """"""\n    n_series = data.shape[0]\n    n_days = data.shape[1]\n    max_end = n_days - backoffset\n    corr = np.empty(n_series, dtype=np.float64)\n    support = np.empty(n_series, dtype=np.float64)\n    for i in range(n_series):\n        series = data[i]\n        end = min(ends[i], max_end)\n        real_len = end - starts[i]\n        support[i] = real_len/lag\n        if support[i] > threshold:\n            series = series[starts[i]:end]\n            c_365 = single_autocorr(series, lag)\n            c_364 = single_autocorr(series, lag-1)\n            c_366 = single_autocorr(series, lag+1)\n            # Average value between exact lag and two nearest neighborhs for smoothness\n            corr[i] = 0.5 * c_365 + 0.25 * c_364 + 0.25 * c_366\n        else:\n            corr[i] = np.NaN\n    return corr #, support\n\n\n@numba.jit(nopython=True)\ndef find_start_end(data: np.ndarray):\n    """"""\n    Calculates start and end of real traffic data. Start is an index of first non-zero, non-NaN value,\n     end is index of last non-zero, non-NaN value\n    :param data: Time series, shape [n_pages, n_days]\n    :return:\n    """"""\n    n_pages = data.shape[0]\n    n_days = data.shape[1]\n    start_idx = np.full(n_pages, -1, dtype=np.int32)\n    end_idx = np.full(n_pages, -1, dtype=np.int32)\n    for page in range(n_pages):\n        # scan from start to the end\n        for day in range(n_days):\n            if not np.isnan(data[page, day]) and data[page, day] > 0:\n                start_idx[page] = day\n                break\n        # reverse scan, from end to start\n        for day in range(n_days - 1, -1, -1):\n            if not np.isnan(data[page, day]) and data[page, day] > 0:\n                end_idx[page] = day\n                break\n    return start_idx, end_idx\n\n\ndef prepare_data(start, end, valid_threshold) -> Tuple[pd.DataFrame, pd.DataFrame, np.ndarray, np.ndarray]:\n    """"""\n    Reads source data, calculates start and end of each series, drops bad series, calculates log1p(series)\n    :param start: start date of effective time interval, can be None to start from beginning\n    :param end: end date of effective time interval, can be None to return all data\n    :param valid_threshold: minimal ratio of series real length to entire (end-start) interval. Series dropped if\n    ratio is less than threshold\n    :return: tuple(log1p(series), nans, series start, series end)\n    """"""\n    df = read_x(start, end)\n    starts, ends = find_start_end(df.values)\n    # boolean mask for bad (too short) series\n    page_mask = (ends - starts) / df.shape[1] < valid_threshold\n    print(""Masked %d pages from %d"" % (page_mask.sum(), len(df)))\n    inv_mask = ~page_mask\n    df = df[inv_mask]\n    nans = pd.isnull(df)\n    return np.log1p(df.fillna(0)), nans, starts[inv_mask], ends[inv_mask]\n\n\ndef lag_indexes(begin, end) -> List[pd.Series]:\n    """"""\n    Calculates indexes for 3, 6, 9, 12 months backward lag for the given date range\n    :param begin: start of date range\n    :param end: end of date range\n    :return: List of 4 Series, one for each lag. For each Series, index is date in range(begin, end), value is an index\n     of target (lagged) date in a same Series. If target date is out of (begin,end) range, index is -1\n    """"""\n    dr = pd.date_range(begin, end)\n    # key is date, value is day index\n    base_index = pd.Series(np.arange(0, len(dr)), index=dr)\n\n    def lag(offset):\n        dates = dr - offset\n        return pd.Series(data=base_index.loc[dates].fillna(-1).astype(np.int16).values, index=dr)\n\n    return [lag(pd.DateOffset(months=m)) for m in (3, 6, 9, 12)]\n\n\ndef make_page_features(pages: np.ndarray) -> pd.DataFrame:\n    """"""\n    Calculates page features (site, country, agent, etc) from urls\n    :param pages: Source urls\n    :return: DataFrame with features as columns and urls as index\n    """"""\n    tagged = extractor.extract(pages).set_index(\'page\')\n    # Drop useless features\n    features: pd.DataFrame = tagged.drop([\'term\', \'marker\'], axis=1)\n    return features\n\n\ndef uniq_page_map(pages:Collection):\n    """"""\n    Finds agent types (spider, desktop, mobile, all) for each unique url, i.e. groups pages by agents\n    :param pages: all urls (must be presorted)\n    :return: array[num_unique_urls, 4], where each column corresponds to agent type and each row corresponds to unique url.\n     Value is an index of page in source pages array. If agent is missing, value is -1\n    """"""\n    import re\n    result = np.full([len(pages), 4], -1, dtype=np.int32)\n    pat = re.compile(\n        \'(.+(?:(?:wikipedia\\.org)|(?:commons\\.wikimedia\\.org)|(?:www\\.mediawiki\\.org)))_([a-z_-]+?)\')\n    prev_page = None\n    num_page = -1\n    agents = {\'all-access_spider\': 0, \'desktop_all-agents\': 1, \'mobile-web_all-agents\': 2, \'all-access_all-agents\': 3}\n    for i, entity in enumerate(pages):\n        match = pat.fullmatch(entity)\n        assert match\n        page = match.group(1)\n        agent = match.group(2)\n        if page != prev_page:\n            prev_page = page\n            num_page += 1\n        result[num_page, agents[agent]] = i\n    return result[:num_page+1]\n\n\ndef encode_page_features(df) -> Dict[str, pd.DataFrame]:\n    """"""\n    Applies one-hot encoding to page features and normalises result\n    :param df: page features DataFrame (one column per feature)\n    :return: dictionary feature_name:encoded_values. Encoded values is [n_pages,n_values] array\n    """"""\n    def encode(column) -> pd.DataFrame:\n        one_hot = pd.get_dummies(df[column], drop_first=False)\n        # noinspection PyUnresolvedReferences\n        return (one_hot - one_hot.mean()) / one_hot.std()\n\n    return {str(column): encode(column) for column in df}\n\n\ndef normalize(values: np.ndarray):\n    return (values - values.mean()) / np.std(values)\n\n\ndef run():\n    parser = argparse.ArgumentParser(description=\'Prepare data\')\n    parser.add_argument(\'data_dir\')\n    parser.add_argument(\'--valid_threshold\', default=0.0, type=float, help=""Series minimal length threshold (pct of data length)"")\n    parser.add_argument(\'--add_days\', default=64, type=int, help=""Add N days in a future for prediction"")\n    parser.add_argument(\'--start\', help=""Effective start date. Data before the start is dropped"")\n    parser.add_argument(\'--end\', help=""Effective end date. Data past the end is dropped"")\n    parser.add_argument(\'--corr_backoffset\', default=0, type=int, help=\'Offset for correlation calculation\')\n    args = parser.parse_args()\n\n    # Get the data\n    df, nans, starts, ends = prepare_data(args.start, args.end, args.valid_threshold)\n\n    # Our working date range\n    data_start, data_end = df.columns[0], df.columns[-1]\n\n    # We have to project some date-dependent features (day of week, etc) to the future dates for prediction\n    features_end = data_end + pd.Timedelta(args.add_days, unit=\'D\')\n    print(f""start: {data_start}, end:{data_end}, features_end:{features_end}"")\n\n    # Group unique pages by agents\n    assert df.index.is_monotonic_increasing\n    page_map = uniq_page_map(df.index.values)\n\n    # Yearly(annual) autocorrelation\n    raw_year_autocorr = batch_autocorr(df.values, 365, starts, ends, 1.5, args.corr_backoffset)\n    year_unknown_pct = np.sum(np.isnan(raw_year_autocorr))/len(raw_year_autocorr)  # type: float\n\n    # Quarterly autocorrelation\n    raw_quarter_autocorr = batch_autocorr(df.values, int(round(365.25/4)), starts, ends, 2, args.corr_backoffset)\n    quarter_unknown_pct = np.sum(np.isnan(raw_quarter_autocorr)) / len(raw_quarter_autocorr)  # type: float\n\n    print(""Percent of undefined autocorr = yearly:%.3f, quarterly:%.3f"" % (year_unknown_pct, quarter_unknown_pct))\n\n    # Normalise all the things\n    year_autocorr = normalize(np.nan_to_num(raw_year_autocorr))\n    quarter_autocorr = normalize(np.nan_to_num(raw_quarter_autocorr))\n\n    # Calculate and encode page features\n    page_features = make_page_features(df.index.values)\n    encoded_page_features = encode_page_features(page_features)\n\n    # Make time-dependent features\n    features_days = pd.date_range(data_start, features_end)\n    #dow = normalize(features_days.dayofweek.values)\n    week_period = 7 / (2 * np.pi)\n    dow_norm = features_days.dayofweek.values / week_period\n    dow = np.stack([np.cos(dow_norm), np.sin(dow_norm)], axis=-1)\n\n    # Assemble indices for quarterly lagged data\n    lagged_ix = np.stack(lag_indexes(data_start, features_end), axis=-1)\n\n    page_popularity = df.median(axis=1)\n    page_popularity = (page_popularity - page_popularity.mean()) / page_popularity.std()\n\n    # Put NaNs back\n    df[nans] = np.NaN\n\n    # Assemble final output\n    tensors = dict(\n        hits=df,\n        lagged_ix=lagged_ix,\n        page_map=page_map,\n        page_ix=df.index.values,\n        pf_agent=encoded_page_features[\'agent\'],\n        pf_country=encoded_page_features[\'country\'],\n        pf_site=encoded_page_features[\'site\'],\n        page_popularity=page_popularity,\n        year_autocorr=year_autocorr,\n        quarter_autocorr=quarter_autocorr,\n        dow=dow,\n    )\n    plain = dict(\n        features_days=len(features_days),\n        data_days=len(df.columns),\n        n_pages=len(df),\n        data_start=data_start,\n        data_end=data_end,\n        features_end=features_end\n\n    )\n\n    # Store data to the disk\n    VarFeeder(args.data_dir, tensors, plain)\n\n\nif __name__ == \'__main__\':\n    run()\n'"
model.py,92,"b'import tensorflow as tf\n\nimport tensorflow.contrib.cudnn_rnn as cudnn_rnn\nimport tensorflow.contrib.rnn as rnn\nimport tensorflow.contrib.layers as layers\nfrom tensorflow.python.util import nest\n\nfrom input_pipe import InputPipe, ModelMode\n\nGRAD_CLIP_THRESHOLD = 10\nRNN = cudnn_rnn.CudnnGRU\n# RNN = tf.contrib.cudnn_rnn.CudnnLSTM\n# RNN = tf.contrib.cudnn_rnn.CudnnRNNRelu\n\n\ndef default_init(seed):\n    # replica of tf.glorot_uniform_initializer(seed=seed)\n    return layers.variance_scaling_initializer(factor=1.0,\n                                               mode=""FAN_AVG"",\n                                               uniform=True,\n                                               seed=seed)\n\n\ndef selu(x):\n    """"""\n    SELU activation\n    https://arxiv.org/abs/1706.02515\n    :param x:\n    :return:\n    """"""\n    with tf.name_scope(\'elu\') as scope:\n        alpha = 1.6732632423543772848170429916717\n        scale = 1.0507009873554804934193349852946\n        return scale * tf.where(x >= 0.0, x, alpha * tf.nn.elu(x))\n\n\ndef make_encoder(time_inputs, encoder_features_depth, is_train, hparams, seed, transpose_output=True):\n    """"""\n    Builds encoder, using CUDA RNN\n    :param time_inputs: Input tensor, shape [batch, time, features]\n    :param encoder_features_depth: Static size for features dimension\n    :param is_train:\n    :param hparams:\n    :param seed:\n    :param transpose_output: Transform RNN output to batch-first shape\n    :return:\n    """"""\n\n    def build_rnn():\n        return RNN(num_layers=hparams.encoder_rnn_layers, num_units=hparams.rnn_depth,\n                   #input_size=encoder_features_depth,\n                   kernel_initializer=tf.initializers.random_uniform(minval=-0.05, maxval=0.05,\n                                                                      seed=seed + 1 if seed else None),\n                   direction=\'unidirectional\',\n                   dropout=hparams.encoder_dropout if is_train else 0, seed=seed)\n\n    cuda_model = build_rnn()\n\n    # [batch, time, features] -> [time, batch, features]\n    time_first = tf.transpose(time_inputs, [1, 0, 2])\n    rnn_time_input = time_first\n    if RNN == tf.contrib.cudnn_rnn.CudnnLSTM:\n        rnn_out, (rnn_state, c_state) = cuda_model(inputs=rnn_time_input)\n    else:\n        rnn_out, (rnn_state,) = cuda_model(inputs=rnn_time_input)\n        c_state = None\n    if transpose_output:\n        rnn_out = tf.transpose(rnn_out, [1, 0, 2])\n    return rnn_out, rnn_state, c_state\n\n\ndef compressed_readout(rnn_out, hparams, dropout, seed):\n    """"""\n    FC compression layer, reduces RNN output depth to hparams.attention_depth\n    :param rnn_out:\n    :param hparams:\n    :param dropout:\n    :param seed:\n    :return:\n    """"""\n    if dropout < 1.0:\n        rnn_out = tf.nn.dropout(rnn_out, dropout, seed=seed)\n    return tf.layers.dense(rnn_out, hparams.attention_depth,\n                           use_bias=True,\n                           activation=selu,\n                           kernel_initializer=layers.variance_scaling_initializer(factor=1.0, seed=seed),\n                           name=\'compress_readout\'\n                           )\n\n\ndef make_fingerprint(x, is_train, fc_dropout, seed):\n    """"""\n    Calculates \'fingerprint\' of timeseries, to feed into attention layer\n    :param x:\n    :param is_train:\n    :param fc_dropout:\n    :param seed:\n    :return:\n    """"""\n    with tf.variable_scope(""fingerpint""):\n        # x = tf.expand_dims(x, -1)\n        with tf.variable_scope(\'convnet\', initializer=layers.variance_scaling_initializer(seed=seed)):\n            c11 = tf.layers.conv1d(x, filters=16, kernel_size=7, activation=tf.nn.relu, padding=\'same\')\n            c12 = tf.layers.conv1d(c11, filters=16, kernel_size=3, activation=tf.nn.relu, padding=\'same\')\n            pool1 = tf.layers.max_pooling1d(c12, 2, 2, padding=\'same\')\n            c21 = tf.layers.conv1d(pool1, filters=32, kernel_size=3, activation=tf.nn.relu, padding=\'same\')\n            c22 = tf.layers.conv1d(c21, filters=32, kernel_size=3, activation=tf.nn.relu, padding=\'same\')\n            pool2 = tf.layers.max_pooling1d(c22, 2, 2, padding=\'same\')\n            c31 = tf.layers.conv1d(pool2, filters=64, kernel_size=3, activation=tf.nn.relu, padding=\'same\')\n            c32 = tf.layers.conv1d(c31, filters=64, kernel_size=3, activation=tf.nn.relu, padding=\'same\')\n            pool3 = tf.layers.max_pooling1d(c32, 2, 2, padding=\'same\')\n            dims = pool3.shape.dims\n            pool3 = tf.reshape(pool3, [-1, dims[1].value * dims[2].value])\n            if is_train and fc_dropout < 1.0:\n                cnn_out = tf.nn.dropout(pool3, fc_dropout, seed=seed)\n            else:\n                cnn_out = pool3\n        with tf.variable_scope(\'fc_convnet\',\n                               initializer=layers.variance_scaling_initializer(factor=1.0, mode=\'FAN_IN\', seed=seed)):\n            fc_encoder = tf.layers.dense(cnn_out, 512, activation=selu, name=\'fc_encoder\')\n            out_encoder = tf.layers.dense(fc_encoder, 16, activation=selu, name=\'out_encoder\')\n    return out_encoder\n\n\ndef attn_readout_v3(readout, attn_window, attn_heads, page_features, seed):\n    # input: [n_days, batch, readout_depth]\n    # [n_days, batch, readout_depth] -> [batch(readout_depth), width=n_days, channels=batch]\n    readout = tf.transpose(readout, [2, 0, 1])\n    # [batch(readout_depth), width, channels] -> [batch, height=1, width, channels]\n    inp = readout[:, tf.newaxis, :, :]\n\n    # attn_window = train_window - predict_window + 1\n    # [batch, attn_window * n_heads]\n    filter_logits = tf.layers.dense(page_features, attn_window * attn_heads, name=""attn_focus"",\n                                    kernel_initializer=default_init(seed)\n                                    # kernel_initializer=layers.variance_scaling_initializer(uniform=True)\n                                    # activation=selu,\n                                    # kernel_initializer=layers.variance_scaling_initializer(factor=1.0, mode=\'FAN_IN\')\n                                    )\n    # [batch, attn_window * n_heads] -> [batch, attn_window, n_heads]\n    filter_logits = tf.reshape(filter_logits, [-1, attn_window, attn_heads])\n\n    # attns_max = tf.nn.softmax(filter_logits, dim=1)\n    attns_max = filter_logits / tf.reduce_sum(filter_logits, axis=1, keep_dims=True)\n    # [batch, attn_window, n_heads] -> [width(attn_window), channels(batch), n_heads]\n    attns_max = tf.transpose(attns_max, [1, 0, 2])\n\n    # [width(attn_window), channels(batch), n_heads] -> [height(1), width(attn_window), channels(batch), multiplier(n_heads)]\n    attn_filter = attns_max[tf.newaxis, :, :, :]\n    # [batch(readout_depth), height=1, width=n_days, channels=batch] -> [batch(readout_depth), height=1, width=predict_window, channels=batch*n_heads]\n    averaged = tf.nn.depthwise_conv2d_native(inp, attn_filter, [1, 1, 1, 1], \'VALID\')\n    # [batch, height=1, width=predict_window, channels=readout_depth*n_neads] -> [batch(depth), predict_window, batch*n_heads]\n    attn_features = tf.squeeze(averaged, 1)\n    # [batch(depth), predict_window, batch*n_heads] -> [batch*n_heads, predict_window, depth]\n    attn_features = tf.transpose(attn_features, [2, 1, 0])\n    # [batch * n_heads, predict_window, depth] -> n_heads * [batch, predict_window, depth]\n    heads = [attn_features[head_no::attn_heads] for head_no in range(attn_heads)]\n    # n_heads * [batch, predict_window, depth] -> [batch, predict_window, depth*n_heads]\n    result = tf.concat(heads, axis=-1)\n    # attn_diag = tf.unstack(attns_max, axis=-1)\n    return result, None\n\n\ndef calc_smape_rounded(true, predicted, weights):\n    """"""\n    Calculates SMAPE on rounded submission values. Should be close to official SMAPE in competition\n    :param true:\n    :param predicted:\n    :param weights: Weights mask to exclude some values\n    :return:\n    """"""\n    n_valid = tf.reduce_sum(weights)\n    true_o = tf.round(tf.expm1(true))\n    pred_o = tf.maximum(tf.round(tf.expm1(predicted)), 0.0)\n    summ = tf.abs(true_o) + tf.abs(pred_o)\n    zeros = summ < 0.01\n    raw_smape = tf.abs(pred_o - true_o) / summ * 2.0\n    smape = tf.where(zeros, tf.zeros_like(summ, dtype=tf.float32), raw_smape)\n    return tf.reduce_sum(smape * weights) / n_valid\n\n\ndef smape_loss(true, predicted, weights):\n    """"""\n    Differentiable SMAPE loss\n    :param true: Truth values\n    :param predicted: Predicted values\n    :param weights: Weights mask to exclude some values\n    :return:\n    """"""\n    epsilon = 0.1  # Smoothing factor, helps SMAPE to be well-behaved near zero\n    true_o = tf.expm1(true)\n    pred_o = tf.expm1(predicted)\n    summ = tf.maximum(tf.abs(true_o) + tf.abs(pred_o) + epsilon, 0.5 + epsilon)\n    smape = tf.abs(pred_o - true_o) / summ * 2.0\n    return tf.losses.compute_weighted_loss(smape, weights, loss_collection=None)\n\n\ndef decode_predictions(decoder_readout, inp: InputPipe):\n    """"""\n    Converts normalized prediction values to log1p(pageviews), e.g. reverts normalization\n    :param decoder_readout: Decoder output, shape [n_days, batch]\n    :param inp: Input tensors\n    :return:\n    """"""\n    # [n_days, batch] -> [batch, n_days]\n    batch_readout = tf.transpose(decoder_readout)\n    batch_std = tf.expand_dims(inp.norm_std, -1)\n    batch_mean = tf.expand_dims(inp.norm_mean, -1)\n    return batch_readout * batch_std + batch_mean\n\n\ndef calc_loss(predictions, true_y, additional_mask=None):\n    """"""\n    Calculates losses, ignoring NaN true values (assigning zero loss to them)\n    :param predictions: Predicted values\n    :param true_y: True values\n    :param additional_mask:\n    :return: MAE loss, differentiable SMAPE loss, competition SMAPE loss\n    """"""\n    # Take into account NaN\'s in true values\n    mask = tf.is_finite(true_y)\n    # Fill NaNs by zeros (can use any value)\n    true_y = tf.where(mask, true_y, tf.zeros_like(true_y))\n    # Assign zero weight to NaNs\n    weights = tf.to_float(mask)\n    if additional_mask is not None:\n        weights = weights * tf.expand_dims(additional_mask, axis=0)\n\n    mae_loss = tf.losses.absolute_difference(labels=true_y, predictions=predictions, weights=weights)\n    return mae_loss, smape_loss(true_y, predictions, weights), calc_smape_rounded(true_y, predictions,\n                                                                                  weights), tf.size(true_y)\n\n\ndef make_train_op(loss, ema_decay=None, prefix=None):\n    optimizer = tf.train.AdamOptimizer()\n    glob_step = tf.train.get_global_step()\n\n    # Add regularization losses\n    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n    total_loss = loss + reg_losses if reg_losses else loss\n\n    # Clip gradients\n    grads_and_vars = optimizer.compute_gradients(total_loss)\n    gradients, variables = zip(*grads_and_vars)\n    clipped_gradients, glob_norm = tf.clip_by_global_norm(gradients, GRAD_CLIP_THRESHOLD)\n    sgd_op, glob_norm = optimizer.apply_gradients(zip(clipped_gradients, variables)), glob_norm\n\n    # Apply SGD averaging\n    if ema_decay:\n        ema = tf.train.ExponentialMovingAverage(decay=ema_decay, num_updates=glob_step)\n        if prefix:\n            # Some magic to handle multiple models trained in single graph\n            ema_vars = [var for var in variables if var.name.startswith(prefix)]\n        else:\n            ema_vars = variables\n        update_ema = ema.apply(ema_vars)\n        with tf.control_dependencies([sgd_op]):\n            training_op = tf.group(update_ema)\n    else:\n        training_op = sgd_op\n        ema = None\n    return training_op, glob_norm, ema\n\n\ndef convert_cudnn_state_v2(h_state, hparams, seed, c_state=None, dropout=1.0):\n    """"""\n    Converts RNN state tensor from cuDNN representation to TF RNNCell compatible representation.\n    :param h_state: tensor [num_layers, batch_size, depth]\n    :param c_state: LSTM additional state, should be same shape as h_state\n    :return: TF cell representation matching RNNCell.state_size structure for compatible cell\n    """"""\n\n    def squeeze(seq):\n        return tuple(seq) if len(seq) > 1 else seq[0]\n\n    def wrap_dropout(structure):\n        if dropout < 1.0:\n            return nest.map_structure(lambda x: tf.nn.dropout(x, keep_prob=dropout, seed=seed), structure)\n        else:\n            return structure\n\n    # Cases:\n    # decoder_layer = encoder_layers, straight mapping\n    # encoder_layers > decoder_layers: get outputs of upper encoder layers\n    # encoder_layers < decoder_layers: feed encoder outputs to lower decoder layers, feed zeros to top layers\n    h_layers = tf.unstack(h_state)\n    if hparams.encoder_rnn_layers >= hparams.decoder_rnn_layers:\n        return squeeze(wrap_dropout(h_layers[hparams.encoder_rnn_layers - hparams.decoder_rnn_layers:]))\n    else:\n        lower_inputs = wrap_dropout(h_layers)\n        upper_inputs = [tf.zeros_like(h_layers[0]) for _ in\n                        range(hparams.decoder_rnn_layers - hparams.encoder_rnn_layers)]\n        return squeeze(lower_inputs + upper_inputs)\n\n\ndef rnn_stability_loss(rnn_output, beta):\n    """"""\n    REGULARIZING RNNS BY STABILIZING ACTIVATIONS\n    https://arxiv.org/pdf/1511.08400.pdf\n    :param rnn_output: [time, batch, features]\n    :return: loss value\n    """"""\n    if beta == 0.0:\n        return 0.0\n    # [time, batch, features] -> [time, batch]\n    l2 = tf.sqrt(tf.reduce_sum(tf.square(rnn_output), axis=-1))\n    #  [time, batch] -> []\n    return beta * tf.reduce_mean(tf.square(l2[1:] - l2[:-1]))\n\n\ndef rnn_activation_loss(rnn_output, beta):\n    """"""\n    REGULARIZING RNNS BY STABILIZING ACTIVATIONS\n    https://arxiv.org/pdf/1511.08400.pdf\n    :param rnn_output: [time, batch, features]\n    :return: loss value\n    """"""\n    if beta == 0.0:\n        return 0.0\n    return tf.nn.l2_loss(rnn_output) * beta\n\n\nclass Model:\n    def __init__(self, inp: InputPipe, hparams, is_train, seed, graph_prefix=None, asgd_decay=None, loss_mask=None):\n        """"""\n        Encoder-decoder prediction model\n        :param inp: Input tensors\n        :param hparams:\n        :param is_train:\n        :param seed:\n        :param graph_prefix: Subgraph prefix for multi-model graph\n        :param asgd_decay: Decay for SGD averaging\n        :param loss_mask: Additional mask for losses calculation (one value for each prediction day), shape=[predict_window]\n        """"""\n        self.is_train = is_train\n        self.inp = inp\n        self.hparams = hparams\n        self.seed = seed\n        self.inp = inp\n\n        encoder_output, h_state, c_state = make_encoder(inp.time_x, inp.encoder_features_depth, is_train, hparams, seed,\n                                                        transpose_output=False)\n        # Encoder activation losses\n        enc_stab_loss = rnn_stability_loss(encoder_output, hparams.encoder_stability_loss / inp.train_window)\n        enc_activation_loss = rnn_activation_loss(encoder_output, hparams.encoder_activation_loss / inp.train_window)\n\n        # Convert state from cuDNN representation to TF RNNCell-compatible representation\n        encoder_state = convert_cudnn_state_v2(h_state, hparams, c_state,\n                                               dropout=hparams.gate_dropout if is_train else 1.0)\n\n        # Attention calculations\n        # Compress encoder outputs\n        enc_readout = compressed_readout(encoder_output, hparams,\n                                         dropout=hparams.encoder_readout_dropout if is_train else 1.0, seed=seed)\n        # Calculate fingerprint from input features\n        fingerprint_inp = tf.concat([inp.lagged_x, tf.expand_dims(inp.norm_x, -1)], axis=-1)\n        fingerprint = make_fingerprint(fingerprint_inp, is_train, hparams.fingerprint_fc_dropout, seed)\n        # Calculate attention vector\n        attn_features, attn_weights = attn_readout_v3(enc_readout, inp.attn_window, hparams.attention_heads,\n                                                      fingerprint, seed=seed)\n\n        # Run decoder\n        decoder_targets, decoder_outputs = self.decoder(encoder_state,\n                                                        attn_features if hparams.use_attn else None,\n                                                        inp.time_y, inp.norm_x[:, -1])\n        # Decoder activation losses\n        dec_stab_loss = rnn_stability_loss(decoder_outputs, hparams.decoder_stability_loss / inp.predict_window)\n        dec_activation_loss = rnn_activation_loss(decoder_outputs, hparams.decoder_activation_loss / inp.predict_window)\n\n        # Get final denormalized predictions\n        self.predictions = decode_predictions(decoder_targets, inp)\n\n        # Calculate losses and build training op\n        if inp.mode == ModelMode.PREDICT:\n            # Pseudo-apply ema to get variable names later in ema.variables_to_restore()\n            # This is copypaste from make_train_op()\n            if asgd_decay:\n                self.ema = tf.train.ExponentialMovingAverage(decay=asgd_decay)\n                variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n                if graph_prefix:\n                    ema_vars = [var for var in variables if var.name.startswith(graph_prefix)]\n                else:\n                    ema_vars = variables\n                self.ema.apply(ema_vars)\n        else:\n            self.mae, smape_loss, self.smape, self.loss_item_count = calc_loss(self.predictions, inp.true_y,\n                                                                               additional_mask=loss_mask)\n            if is_train:\n                # Sum all losses\n                total_loss = smape_loss + enc_stab_loss + dec_stab_loss + enc_activation_loss + dec_activation_loss\n                self.train_op, self.glob_norm, self.ema = make_train_op(total_loss, asgd_decay, prefix=graph_prefix)\n\n\n\n    def default_init(self, seed_add=0):\n        return default_init(self.seed + seed_add)\n\n    def decoder(self, encoder_state, attn_features, prediction_inputs, previous_y):\n        """"""\n        :param encoder_state: shape [batch_size, encoder_rnn_depth]\n        :param prediction_inputs: features for prediction days, tensor[batch_size, time, input_depth]\n        :param previous_y: Last day pageviews, shape [batch_size]\n        :param attn_features: Additional features from attention layer, shape [batch, predict_window, readout_depth*n_heads]\n        :return: decoder rnn output\n        """"""\n        hparams = self.hparams\n\n        def build_cell(idx):\n            with tf.variable_scope(\'decoder_cell\', initializer=self.default_init(idx)):\n                cell = rnn.GRUBlockCell(self.hparams.rnn_depth)\n                has_dropout = hparams.decoder_input_dropout[idx] < 1 \\\n                              or hparams.decoder_state_dropout[idx] < 1 or hparams.decoder_output_dropout[idx] < 1\n\n                if self.is_train and has_dropout:\n                    attn_depth = attn_features.shape[-1].value if attn_features is not None else 0\n                    input_size = attn_depth + prediction_inputs.shape[-1].value + 1 if idx == 0 else self.hparams.rnn_depth\n                    cell = rnn.DropoutWrapper(cell, dtype=tf.float32, input_size=input_size,\n                                              variational_recurrent=hparams.decoder_variational_dropout[idx],\n                                              input_keep_prob=hparams.decoder_input_dropout[idx],\n                                              output_keep_prob=hparams.decoder_output_dropout[idx],\n                                              state_keep_prob=hparams.decoder_state_dropout[idx], seed=self.seed + idx)\n                return cell\n\n        if hparams.decoder_rnn_layers > 1:\n            cells = [build_cell(idx) for idx in range(hparams.decoder_rnn_layers)]\n            cell = rnn.MultiRNNCell(cells)\n        else:\n            cell = build_cell(0)\n\n        nest.assert_same_structure(encoder_state, cell.state_size)\n        predict_days = self.inp.predict_window\n        assert prediction_inputs.shape[1] == predict_days\n\n        # [batch_size, time, input_depth] -> [time, batch_size, input_depth]\n        inputs_by_time = tf.transpose(prediction_inputs, [1, 0, 2])\n\n        # Return raw outputs for RNN losses calculation\n        return_raw_outputs = self.hparams.decoder_stability_loss > 0.0 or self.hparams.decoder_activation_loss > 0.0\n\n        # Stop condition for decoding loop\n        def cond_fn(time, prev_output, prev_state, array_targets: tf.TensorArray, array_outputs: tf.TensorArray):\n            return time < predict_days\n\n        # FC projecting layer to get single predicted value from RNN output\n        def project_output(tensor):\n            return tf.layers.dense(tensor, 1, name=\'decoder_output_proj\', kernel_initializer=self.default_init())\n\n        def loop_fn(time, prev_output, prev_state, array_targets: tf.TensorArray, array_outputs: tf.TensorArray):\n            """"""\n            Main decoder loop\n            :param time: Day number\n            :param prev_output: Output(prediction) from previous step\n            :param prev_state: RNN state tensor from previous step\n            :param array_targets: Predictions, each step will append new value to this array\n            :param array_outputs: Raw RNN outputs (for regularization losses)\n            :return:\n            """"""\n            # RNN inputs for current step\n            features = inputs_by_time[time]\n\n            # [batch, predict_window, readout_depth * n_heads] -> [batch, readout_depth * n_heads]\n            if attn_features is not None:\n                #  [batch_size, 1] + [batch_size, input_depth]\n                attn = attn_features[:, time, :]\n                # Append previous predicted value + attention vector to input features\n                next_input = tf.concat([prev_output, features, attn], axis=1)\n            else:\n                # Append previous predicted value to input features\n                next_input = tf.concat([prev_output, features], axis=1)\n\n            # Run RNN cell\n            output, state = cell(next_input, prev_state)\n            # Make prediction from RNN outputs\n            projected_output = project_output(output)\n            # Append step results to the buffer arrays\n            if return_raw_outputs:\n                array_outputs = array_outputs.write(time, output)\n            array_targets = array_targets.write(time, projected_output)\n            # Increment time and return\n            return time + 1, projected_output, state, array_targets, array_outputs\n\n        # Initial values for loop\n        loop_init = [tf.constant(0, dtype=tf.int32),\n                     tf.expand_dims(previous_y, -1),\n                     encoder_state,\n                     tf.TensorArray(dtype=tf.float32, size=predict_days),\n                     tf.TensorArray(dtype=tf.float32, size=predict_days) if return_raw_outputs else tf.constant(0)]\n        # Run the loop\n        _, _, _, targets_ta, outputs_ta = tf.while_loop(cond_fn, loop_fn, loop_init)\n\n        # Get final tensors from buffer arrays\n        targets = targets_ta.stack()\n        # [time, batch_size, 1] -> [time, batch_size]\n        targets = tf.squeeze(targets, axis=-1)\n        raw_outputs = outputs_ta.stack() if return_raw_outputs else None\n        return targets, raw_outputs\n'"
trainer.py,34,"b'import os.path\nimport shutil\nimport sys\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import trange\nfrom typing import List, Tuple\nimport heapq\nimport logging\nimport pandas as pd\nfrom enum import Enum\n\nfrom hparams import build_from_set, build_hparams\nfrom feeder import VarFeeder\nfrom input_pipe import InputPipe, ModelMode, Splitter,FakeSplitter, page_features\nfrom model import Model\nimport argparse\n\n\nlog = logging.getLogger(\'trainer\')\n\nclass Ema:\n    def __init__(self, k=0.99):\n        self.k = k\n        self.state = None\n        self.steps = 0\n\n    def __call__(self, *args, **kwargs):\n        v = args[0]\n        self.steps += 1\n        if self.state is None:\n            self.state = v\n        else:\n            eff_k = min(1 - 1 / self.steps, self.k)\n            self.state = eff_k * self.state + (1 - eff_k) * v\n        return self.state\n\n\nclass Metric:\n    def __init__(self, name: str, op, smoothness: float = None):\n        self.name = name\n        self.op = op\n        self.smoother = Ema(smoothness) if smoothness else None\n        self.epoch_values = []\n        self.best_value = np.Inf\n        self.best_step = 0\n        self.last_epoch = -1\n        self.improved = False\n        self._top = []\n\n    @property\n    def avg_epoch(self):\n        return np.mean(self.epoch_values)\n\n    @property\n    def best_epoch(self):\n        return np.min(self.epoch_values)\n\n    @property\n    def last(self):\n        return self.epoch_values[-1] if self.epoch_values else np.nan\n\n    @property\n    def top(self):\n        return -np.mean(self._top)\n\n\n    def update(self, value, epoch, step):\n        if self.smoother:\n            value = self.smoother(value)\n        if epoch > self.last_epoch:\n            self.epoch_values = []\n            self.last_epoch = epoch\n        self.epoch_values.append(value)\n        if value < self.best_value:\n            self.best_value = value\n            self.best_step = step\n            self.improved = True\n        else:\n            self.improved = False\n        if len(self._top) >= 5:\n            heapq.heappushpop(self._top, -value)\n        else:\n            heapq.heappush(self._top, -value)\n\n\nclass AggMetric:\n    def __init__(self, metrics: List[Metric]):\n        self.metrics = metrics\n\n    def _mean(self, fun) -> float:\n        # noinspection PyTypeChecker\n        return np.mean([fun(metric) for metric in self.metrics])\n\n    @property\n    def avg_epoch(self):\n        return self._mean(lambda m: m.avg_epoch)\n\n    @property\n    def best_epoch(self):\n        return self._mean(lambda m: m.best_epoch)\n\n    @property\n    def last(self):\n        return self._mean(lambda m: m.last)\n\n    @property\n    def top(self):\n        return self._mean(lambda m: m.top)\n\n    @property\n    def improved(self):\n        return np.any([metric.improved for metric in self.metrics])\n\n\nclass DummyMetric:\n    @property\n    def avg_epoch(self):\n        return np.nan\n\n    @property\n    def best_epoch(self):\n        return np.nan\n\n    @property\n    def last(self):\n        return np.nan\n\n    @property\n    def top(self):\n        return np.nan\n\n    @property\n    def improved(self):\n        return False\n\n    @property\n    def metrics(self):\n        return []\n\n\nclass Stage(Enum):\n    TRAIN = 0\n    EVAL_SIDE = 1\n    EVAL_FRWD = 2\n    EVAL_SIDE_EMA = 3\n    EVAL_FRWD_EMA = 4\n\n\nclass ModelTrainerV2:\n    def __init__(self, train_model: Model, eval: List[Tuple[Stage, Model]], model_no=0,\n                 patience=None, stop_metric=None, summary_writer=None):\n        self.train_model = train_model\n        if eval:\n            self.eval_stages, self.eval_models = zip(*eval)\n        else:\n            self.eval_stages, self.eval_models = [], []\n        self.stopped = False\n        self.model_no = model_no\n        self.patience = patience\n        self.best_metric = np.inf\n        self.bad_epochs = 0\n        self.stop_metric = stop_metric\n        self.summary_writer = summary_writer\n\n        def std_metrics(model: Model, smoothness):\n            return [Metric(\'SMAPE\', model.smape, smoothness), Metric(\'MAE\', model.mae, smoothness)]\n\n        self._metrics = {Stage.TRAIN: std_metrics(train_model, 0.9) + [Metric(\'GrNorm\', train_model.glob_norm)]}\n        for stage, model in eval:\n            self._metrics[stage] = std_metrics(model, None)\n        self.dict_metrics = {key: {metric.name: metric for metric in metrics} for key, metrics in self._metrics.items()}\n\n    def init(self, sess):\n        for model in list(self.eval_models) + [self.train_model]:\n            model.inp.init_iterator(sess)\n\n    @property\n    def metrics(self):\n        return self._metrics\n\n    @property\n    def train_ops(self):\n        model = self.train_model\n        return [model.train_op]  # , model.summaries\n\n    def metric_ops(self, key):\n        return [metric.op for metric in self._metrics[key]]\n\n    def process_metrics(self, key, run_results, epoch, step):\n        metrics = self._metrics[key]\n        summaries = []\n        for result, metric in zip(run_results, metrics):\n            metric.update(result, epoch, step)\n            summaries.append(tf.Summary.Value(tag=f""{key.name}/{metric.name}_0"", simple_value=result))\n        return summaries\n\n    def end_epoch(self):\n        if self.stop_metric:\n            best_metric = self.stop_metric(self.dict_metrics)# self.dict_metrics[Stage.EVAL_FRWD][\'SMAPE\'].avg_epoch\n            if self.best_metric > best_metric:\n                self.best_metric = best_metric\n                self.bad_epochs = 0\n            else:\n                self.bad_epochs += 1\n                if self.bad_epochs > self.patience:\n                    self.stopped = True\n\n\nclass MultiModelTrainer:\n    def __init__(self, trainers: List[ModelTrainerV2], inc_step_op,\n                 misc_global_ops=None):\n        self.trainers = trainers\n        self.inc_step = inc_step_op\n        self.global_ops = misc_global_ops or []\n        self.eval_stages = trainers[0].eval_stages\n\n    def active(self):\n        return [trainer for trainer in self.trainers if not trainer.stopped]\n\n    def _metric_step(self, stage, initial_ops, sess: tf.Session, epoch: int, step=None, repeats=1, summary_every=1):\n        ops = initial_ops\n        offsets, lengths = [], []\n        trainers = self.active()\n        for trainer in trainers:\n            offsets.append(len(ops))\n            metric_ops = trainer.metric_ops(stage)\n            lengths.append(len(metric_ops))\n            ops.extend(metric_ops)\n        if repeats > 1:\n            all_results = np.stack([np.array(sess.run(ops)) for _ in range(repeats)])\n            results = np.mean(all_results, axis=0)\n        else:\n            results = sess.run(ops)\n        if step is None:\n            step = results[0]\n\n        for trainer, offset, length in zip(trainers, offsets, lengths):\n            chunk = results[offset: offset + length]\n            summaries = trainer.process_metrics(stage, chunk, epoch, step)\n            if trainer.summary_writer and step > 200 and (step % summary_every == 0):\n                summary = tf.Summary(value=summaries)\n                trainer.summary_writer.add_summary(summary, global_step=step)\n        return results\n\n    def train_step(self, sess: tf.Session, epoch: int):\n        ops = [self.inc_step] + self.global_ops\n        for trainer in self.active():\n            ops.extend(trainer.train_ops)\n        results = self._metric_step(Stage.TRAIN, ops, sess, epoch, summary_every=20)\n        #return results[:len(self.global_ops) + 1] # step, grad_norm\n        return results[0]\n\n    def eval_step(self, sess: tf.Session, epoch: int, step, n_batches, stages:List[Stage]=None):\n        target_stages = stages if stages is not None else self.eval_stages\n        for stage in target_stages:\n            self._metric_step(stage, [], sess, epoch, step, repeats=n_batches)\n\n    def metric(self, stage, name):\n        return AggMetric([trainer.dict_metrics[stage][name] for trainer in self.trainers])\n\n    def end_epoch(self):\n        for trainer in self.active():\n            trainer.end_epoch()\n\n    def has_active(self):\n        return len(self.active())\n\n\nclass ModelTrainer:\n    def __init__(self, train_model, eval_model, model_no=0, summary_writer=None, keep_best=5, patience=None):\n        self.train_model = train_model\n        self.eval_model = eval_model\n        self.stopped = False\n        self.smooth_train_mae = Ema()\n        self.smooth_train_smape = Ema()\n        self.smooth_eval_mae = Ema(0.5)\n        self.smooth_eval_smape = Ema(0.5)\n        self.smooth_grad = Ema(0.9)\n        self.summary_writer = summary_writer\n        self.model_no = model_no\n        self.best_top_n_loss = []\n        self.keep_best = keep_best\n        self.best_step = 0\n        self.patience = patience\n        self.train_pipe = train_model.inp\n        self.eval_pipe = eval_model.inp\n        self.epoch_mae = []\n        self.epoch_smape = []\n        self.last_epoch = -1\n\n    @property\n    def train_ops(self):\n        model = self.train_model\n        return [model.train_op, model.update_ema, model.summaries, model.mae, model.smape, model.glob_norm]\n\n    def process_train_results(self, run_results, offset, global_step, write_summary):\n        offset += 2\n        summaries, mae, smape, glob_norm = run_results[offset:offset + 4]\n        results = self.smooth_train_mae(mae), self.smooth_train_smape(smape), self.smooth_grad(glob_norm)\n        if self.summary_writer and write_summary:\n            self.summary_writer.add_summary(summaries, global_step=global_step)\n        return np.array(results)\n\n    @property\n    def eval_ops(self):\n        model = self.eval_model\n        return [model.mae, model.smape]\n\n    @property\n    def eval_len(self):\n        return len(self.eval_ops)\n\n    @property\n    def train_len(self):\n        return len(self.train_ops)\n\n    @property\n    def best_top_loss(self):\n        return -np.array(self.best_top_n_loss).mean()\n\n    @property\n    def best_epoch_mae(self):\n        return min(self.epoch_mae) if self.epoch_mae else np.NaN\n\n    @property\n    def mean_epoch_mae(self):\n        return np.mean(self.epoch_mae) if self.epoch_mae else np.NaN\n\n    @property\n    def mean_epoch_smape(self):\n        return np.mean(self.epoch_smape) if self.epoch_smape else np.NaN\n\n    @property\n    def best_epoch_smape(self):\n        return min(self.epoch_smape) if self.epoch_smape else np.NaN\n\n    def remember_for_epoch(self, epoch, mae, smape):\n        if epoch > self.last_epoch:\n            self.last_epoch = epoch\n            self.epoch_mae = []\n            self.epoch_smape = []\n        self.epoch_mae.append(mae)\n        self.epoch_smape.append(smape)\n\n    @property\n    def best_epoch_metrics(self):\n        return np.array([self.best_epoch_mae, self.best_epoch_smape])\n\n    @property\n    def mean_epoch_metrics(self):\n        return np.array([self.mean_epoch_mae, self.mean_epoch_smape])\n\n    def process_eval_results(self, run_results, offset, global_step, epoch):\n        totals = np.zeros(self.eval_len, np.float)\n        for result in run_results:\n            items = np.array(result[offset:offset + self.eval_len])\n            totals += items\n        results = totals / len(run_results)\n        mae, smape = results\n        if self.summary_writer and global_step > 200:\n            summary = tf.Summary(value=[\n                tf.Summary.Value(tag=f""test/MAE_{self.model_no}"", simple_value=mae),\n                tf.Summary.Value(tag=f""test/SMAPE_{self.model_no}"", simple_value=smape),\n            ])\n            self.summary_writer.add_summary(summary, global_step=global_step)\n        smooth_mae = self.smooth_eval_mae(mae)\n        smooth_smape = self.smooth_eval_smape(smape)\n        self.remember_for_epoch(epoch, mae, smape)\n\n        current_loss = -smooth_smape\n\n        prev_best_n = np.mean(self.best_top_n_loss) if self.best_top_n_loss else -np.inf\n        if self.best_top_n_loss:\n            log.debug(""Current loss=%.3f, old best=%.3f, wait steps=%d"", -current_loss,\n                      -max(self.best_top_n_loss), global_step - self.best_step)\n\n        if len(self.best_top_n_loss) >= self.keep_best:\n            heapq.heappushpop(self.best_top_n_loss, current_loss)\n        else:\n            heapq.heappush(self.best_top_n_loss, current_loss)\n        log.debug(""Best loss=%.3f, top_5 avg loss=%.3f, top_5=%s"",\n                  -max(self.best_top_n_loss), -np.mean(self.best_top_n_loss),\n                  "","".join([""%.3f"" % -mae for mae in self.best_top_n_loss]))\n        new_best_n = np.mean(self.best_top_n_loss)\n\n        new_best = new_best_n > prev_best_n\n        if new_best:\n            self.best_step = global_step\n            log.debug(""New best step %d, current loss=%.3f"", global_step, -current_loss)\n        else:\n            step_count = global_step - self.best_step\n            if step_count > self.patience:\n                self.stopped = True\n\n        return mae, smape, new_best, smooth_mae, smooth_smape\n\n\ndef train(name, hparams, multi_gpu=False, n_models=1, train_completeness_threshold=0.01,\n          seed=None, logdir=\'data/logs\', max_epoch=100, patience=2, train_sampling=1.0,\n          eval_sampling=1.0, eval_memsize=5, gpu=0, gpu_allow_growth=False, save_best_model=False,\n          forward_split=False, write_summaries=False, verbose=False, asgd_decay=None, tqdm=True,\n          side_split=True, max_steps=None, save_from_step=None, do_eval=True, predict_window=63):\n\n    eval_k = int(round(26214 * eval_memsize / n_models))\n    eval_batch_size = int(\n        eval_k / (hparams.rnn_depth * hparams.encoder_rnn_layers))  # 128 -> 1024, 256->512, 512->256\n    eval_pct = 0.1\n    batch_size = hparams.batch_size\n    train_window = hparams.train_window\n    tf.reset_default_graph()\n    if seed:\n        tf.set_random_seed(seed)\n\n    with tf.device(""/cpu:0""):\n        inp = VarFeeder.read_vars(""data/vars"")\n        if side_split:\n            splitter = Splitter(page_features(inp), inp.page_map, 3, train_sampling=train_sampling,\n                                test_sampling=eval_sampling, seed=seed)\n        else:\n            splitter = FakeSplitter(page_features(inp), 3, seed=seed, test_sampling=eval_sampling)\n\n    real_train_pages = splitter.splits[0].train_size\n    real_eval_pages = splitter.splits[0].test_size\n\n    items_per_eval = real_eval_pages * eval_pct\n    eval_batches = int(np.ceil(items_per_eval / eval_batch_size))\n    steps_per_epoch = real_train_pages // batch_size\n    eval_every_step = int(round(steps_per_epoch * eval_pct))\n    # eval_every_step = int(round(items_per_eval * train_sampling / batch_size))\n\n    global_step = tf.train.get_or_create_global_step()\n    inc_step = tf.assign_add(global_step, 1)\n\n\n    all_models: List[ModelTrainerV2] = []\n\n    def create_model(scope, index, prefix, seed):\n\n        with tf.variable_scope(\'input\') as inp_scope:\n            with tf.device(""/cpu:0""):\n                split = splitter.splits[index]\n                pipe = InputPipe(inp, features=split.train_set, n_pages=split.train_size,\n                                 mode=ModelMode.TRAIN, batch_size=batch_size, n_epoch=None, verbose=verbose,\n                                 train_completeness_threshold=train_completeness_threshold,\n                                 predict_completeness_threshold=train_completeness_threshold, train_window=train_window,\n                                 predict_window=predict_window,\n                                 rand_seed=seed, train_skip_first=hparams.train_skip_first,\n                                 back_offset=predict_window if forward_split else 0)\n                inp_scope.reuse_variables()\n                if side_split:\n                    side_eval_pipe = InputPipe(inp, features=split.test_set, n_pages=split.test_size,\n                                               mode=ModelMode.EVAL, batch_size=eval_batch_size, n_epoch=None,\n                                               verbose=verbose, predict_window=predict_window,\n                                               train_completeness_threshold=0.01, predict_completeness_threshold=0,\n                                               train_window=train_window, rand_seed=seed, runs_in_burst=eval_batches,\n                                               back_offset=predict_window * (2 if forward_split else 1))\n                else:\n                    side_eval_pipe = None\n                if forward_split:\n                    forward_eval_pipe = InputPipe(inp, features=split.test_set, n_pages=split.test_size,\n                                                  mode=ModelMode.EVAL, batch_size=eval_batch_size, n_epoch=None,\n                                                  verbose=verbose, predict_window=predict_window,\n                                                  train_completeness_threshold=0.01, predict_completeness_threshold=0,\n                                                  train_window=train_window, rand_seed=seed, runs_in_burst=eval_batches,\n                                                  back_offset=predict_window)\n                else:\n                    forward_eval_pipe = None\n        avg_sgd = asgd_decay is not None\n        #asgd_decay = 0.99 if avg_sgd else None\n        train_model = Model(pipe, hparams, is_train=True, graph_prefix=prefix, asgd_decay=asgd_decay, seed=seed)\n        scope.reuse_variables()\n\n        eval_stages = []\n        if side_split:\n            side_eval_model = Model(side_eval_pipe, hparams, is_train=False,\n                                    #loss_mask=np.concatenate([np.zeros(50, dtype=np.float32), np.ones(10, dtype=np.float32)]),\n                                    seed=seed)\n            eval_stages.append((Stage.EVAL_SIDE, side_eval_model))\n            if avg_sgd:\n                eval_stages.append((Stage.EVAL_SIDE_EMA, side_eval_model))\n        if forward_split:\n            forward_eval_model = Model(forward_eval_pipe, hparams, is_train=False, seed=seed)\n            eval_stages.append((Stage.EVAL_FRWD, forward_eval_model))\n            if avg_sgd:\n                eval_stages.append((Stage.EVAL_FRWD_EMA, forward_eval_model))\n\n        if write_summaries:\n            summ_path = f""{logdir}/{name}_{index}""\n            if os.path.exists(summ_path):\n                shutil.rmtree(summ_path)\n            summ_writer = tf.summary.FileWriter(summ_path)  # , graph=tf.get_default_graph()\n        else:\n            summ_writer = None\n        if do_eval and forward_split:\n            stop_metric = lambda metrics: metrics[Stage.EVAL_FRWD][\'SMAPE\'].avg_epoch\n        else:\n            stop_metric = None\n        return ModelTrainerV2(train_model, eval_stages, index, patience=patience,\n                              stop_metric=stop_metric,\n                              summary_writer=summ_writer)\n\n\n    if n_models == 1:\n        with tf.device(f""/gpu:{gpu}""):\n            scope = tf.get_variable_scope()\n            all_models = [create_model(scope, 0, None, seed=seed)]\n    else:\n        for i in range(n_models):\n            device = f""/gpu:{i}"" if multi_gpu else f""/gpu:{gpu}""\n            with tf.device(device):\n                prefix = f""m_{i}""\n                with tf.variable_scope(prefix) as scope:\n                    all_models.append(create_model(scope, i, prefix=prefix, seed=seed + i))\n    trainer = MultiModelTrainer(all_models, inc_step)\n    if save_best_model or save_from_step:\n        saver_path = f\'data/cpt/{name}\'\n        if os.path.exists(saver_path):\n            shutil.rmtree(saver_path)\n        os.makedirs(saver_path)\n        saver = tf.train.Saver(max_to_keep=10, name=\'train_saver\')\n    else:\n        saver = None\n    avg_sgd = asgd_decay is not None\n    if avg_sgd:\n        from itertools import chain\n        def ema_vars(model):\n            ema = model.train_model.ema\n            return {ema.average_name(v):v for v in model.train_model.ema._averages}\n\n        ema_names = dict(chain(*[ema_vars(model).items() for model in all_models]))\n        #ema_names = all_models[0].train_model.ema.variables_to_restore()\n        ema_loader = tf.train.Saver(var_list=ema_names,  max_to_keep=1, name=\'ema_loader\')\n        ema_saver = tf.train.Saver(max_to_keep=1, name=\'ema_saver\')\n    else:\n        ema_loader = None\n\n    init = tf.global_variables_initializer()\n\n    if forward_split and do_eval:\n        eval_smape = trainer.metric(Stage.EVAL_FRWD, \'SMAPE\')\n        eval_mae = trainer.metric(Stage.EVAL_FRWD, \'MAE\')\n    else:\n        eval_smape = DummyMetric()\n        eval_mae = DummyMetric()\n\n    if side_split and do_eval:\n        eval_mae_side = trainer.metric(Stage.EVAL_SIDE, \'MAE\')\n        eval_smape_side = trainer.metric(Stage.EVAL_SIDE, \'SMAPE\')\n    else:\n        eval_mae_side = DummyMetric()\n        eval_smape_side = DummyMetric()\n\n    train_smape = trainer.metric(Stage.TRAIN, \'SMAPE\')\n    train_mae = trainer.metric(Stage.TRAIN, \'MAE\')\n    grad_norm = trainer.metric(Stage.TRAIN, \'GrNorm\')\n    eval_stages = []\n    ema_eval_stages = []\n    if forward_split and do_eval:\n        eval_stages.append(Stage.EVAL_FRWD)\n        ema_eval_stages.append(Stage.EVAL_FRWD_EMA)\n    if side_split and do_eval:\n        eval_stages.append(Stage.EVAL_SIDE)\n        ema_eval_stages.append(Stage.EVAL_SIDE_EMA)\n\n    # gpu_options=tf.GPUOptions(allow_growth=False),\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n                                          gpu_options=tf.GPUOptions(allow_growth=gpu_allow_growth))) as sess:\n        sess.run(init)\n        # pipe.load_vars(sess)\n        inp.restore(sess)\n        for model in all_models:\n            model.init(sess)\n        # if beholder:\n        #    visualizer = Beholder(session=sess, logdir=summ_path)\n        step = 0\n        prev_top = np.inf\n        best_smape = np.inf\n        # Contains best value (first item) and subsequent values\n        best_epoch_smape = []\n\n        for epoch in range(max_epoch):\n\n            # n_steps = pusher.n_pages // batch_size\n            if tqdm:\n                tqr = trange(steps_per_epoch, desc=""%2d"" % (epoch + 1), leave=False)\n            else:\n                tqr = range(steps_per_epoch)\n\n            for _ in tqr:\n                try:\n                    step = trainer.train_step(sess, epoch)\n                except tf.errors.OutOfRangeError:\n                    break\n                    # if beholder:\n                    #  if step % 5 == 0:\n                    # noinspection PyUnboundLocalVariable\n                    #  visualizer.update()\n                if step % eval_every_step == 0:\n                    if eval_stages:\n                        trainer.eval_step(sess, epoch, step, eval_batches, stages=eval_stages)\n\n                    if save_best_model and epoch > 0 and eval_smape.last < best_smape:\n                        best_smape = eval_smape.last\n                        saver.save(sess, f\'data/cpt/{name}/cpt\', global_step=step)\n                    if save_from_step and step >= save_from_step:\n                        saver.save(sess, f\'data/cpt/{name}/cpt\', global_step=step)\n\n                    if avg_sgd and ema_eval_stages:\n                        ema_saver.save(sess, \'data/cpt_tmp/ema\',  write_meta_graph=False)\n                        # restore ema-backed vars\n                        ema_loader.restore(sess, \'data/cpt_tmp/ema\')\n\n                        trainer.eval_step(sess, epoch, step, eval_batches, stages=ema_eval_stages)\n                        # restore normal vars\n                        ema_saver.restore(sess, \'data/cpt_tmp/ema\')\n\n                MAE = ""%.3f/%.3f/%.3f"" % (eval_mae.last, eval_mae_side.last, train_mae.last)\n                improvement = \'\xe2\x86\x91\' if eval_smape.improved else \' \'\n                SMAPE = ""%s%.3f/%.3f/%.3f"" % (improvement, eval_smape.last, eval_smape_side.last,  train_smape.last)\n                if tqdm:\n                    tqr.set_postfix(gr=grad_norm.last, MAE=MAE, SMAPE=SMAPE)\n                if not trainer.has_active() or (max_steps and step > max_steps):\n                    break\n\n            if tqdm:\n                tqr.close()\n            trainer.end_epoch()\n            if not best_epoch_smape or eval_smape.avg_epoch < best_epoch_smape[0]:\n                best_epoch_smape = [eval_smape.avg_epoch]\n            else:\n                best_epoch_smape.append(eval_smape.avg_epoch)\n\n            current_top = eval_smape.top\n            if prev_top > current_top:\n                prev_top = current_top\n                has_best_indicator = \'\xe2\x86\x91\'\n            else:\n                has_best_indicator = \' \'\n            status = ""%2d: Best top SMAPE=%.3f%s (%s)"" % (\n                epoch + 1, current_top, has_best_indicator,\n                "","".join([""%.3f"" % m.top for m in eval_smape.metrics]))\n\n            if trainer.has_active():\n                status += "", frwd/side best MAE=%.3f/%.3f, SMAPE=%.3f/%.3f; avg MAE=%.3f/%.3f, SMAPE=%.3f/%.3f, %d am"" % \\\n                          (eval_mae.best_epoch, eval_mae_side.best_epoch, eval_smape.best_epoch, eval_smape_side.best_epoch,\n                           eval_mae.avg_epoch,  eval_mae_side.avg_epoch,  eval_smape.avg_epoch,  eval_smape_side.avg_epoch,\n                           trainer.has_active())\n                print(status, file=sys.stderr)\n            else:\n                print(status, file=sys.stderr)\n                print(""Early stopping!"", file=sys.stderr)\n                break\n            if max_steps and step > max_steps:\n                print(""Max steps calculated"", file=sys.stderr)\n                break\n            sys.stderr.flush()\n\n        # noinspection PyUnboundLocalVariable\n        return np.mean(best_epoch_smape, dtype=np.float64)\n\n\ndef predict(checkpoints, hparams, return_x=False, verbose=False, predict_window=6, back_offset=0, n_models=1,\n            target_model=0, asgd=False, seed=1, batch_size=1024):\n    with tf.variable_scope(\'input\') as inp_scope:\n        with tf.device(""/cpu:0""):\n            inp = VarFeeder.read_vars(""data/vars"")\n            pipe = InputPipe(inp, page_features(inp), inp.n_pages, mode=ModelMode.PREDICT, batch_size=batch_size,\n                             n_epoch=1, verbose=verbose,\n                             train_completeness_threshold=0.01,\n                             predict_window=predict_window,\n                             predict_completeness_threshold=0.0, train_window=hparams.train_window,\n                             back_offset=back_offset)\n    asgd_decay = 0.99 if asgd else None\n    if n_models == 1:\n        model = Model(pipe, hparams, is_train=False, seed=seed, asgd_decay=asgd_decay)\n    else:\n        models = []\n        for i in range(n_models):\n            prefix = f""m_{i}""\n            with tf.variable_scope(prefix) as scope:\n                models.append(Model(pipe, hparams, is_train=False, seed=seed, asgd_decay=asgd_decay, graph_prefix=prefix))\n        model = models[target_model]\n\n    if asgd:\n        var_list = model.ema.variables_to_restore()\n        prefix = f""m_{target_model}""\n        for var in list(var_list.keys()):\n            if var.endswith(\'ExponentialMovingAverage\') and not var.startswith(prefix):\n                del var_list[var]\n    else:\n        var_list = None\n    saver = tf.train.Saver(name=\'eval_saver\', var_list=var_list)\n    x_buffer = []\n    predictions = None\n    with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))) as sess:\n        pipe.load_vars(sess)\n        for checkpoint in checkpoints:\n            pred_buffer = []\n            pipe.init_iterator(sess)\n            saver.restore(sess, checkpoint)\n            cnt = 0\n            while True:\n                try:\n                    if return_x:\n                        pred, x, pname = sess.run([model.predictions, model.inp.true_x, model.inp.page_ix])\n                    else:\n                        pred, pname = sess.run([model.predictions, model.inp.page_ix])\n                    utf_names = [str(name, \'utf-8\') for name in pname]\n                    pred_df = pd.DataFrame(index=utf_names, data=np.expm1(pred))\n                    pred_buffer.append(pred_df)\n                    if return_x:\n                        # noinspection PyUnboundLocalVariable\n                        x_values = pd.DataFrame(index=utf_names, data=np.round(np.expm1(x)).astype(np.int64))\n                        x_buffer.append(x_values)\n                    newline = cnt % 80 == 0\n                    if cnt > 0:\n                        print(\'.\', end=\'\\n\' if newline else \'\', flush=True)\n                    if newline:\n                        print(cnt, end=\'\')\n                    cnt += 1\n                except tf.errors.OutOfRangeError:\n                    print(\'\xf0\x9f\x8e\x89\')\n                    break\n            cp_predictions = pd.concat(pred_buffer)\n            if predictions is None:\n                predictions = cp_predictions\n            else:\n                predictions += cp_predictions\n    predictions /= len(checkpoints)\n    offset = pd.Timedelta(back_offset, \'D\')\n    start_prediction = inp.data_end + pd.Timedelta(\'1D\') - offset\n    end_prediction = start_prediction + pd.Timedelta(predict_window - 1, \'D\')\n    predictions.columns = pd.date_range(start_prediction, end_prediction)\n    if return_x:\n        x = pd.concat(x_buffer)\n        start_data = inp.data_end - pd.Timedelta(hparams.train_window - 1, \'D\') - back_offset\n        end_data = inp.data_end - back_offset\n        x.columns = pd.date_range(start_data, end_data)\n        return predictions, x\n    else:\n        return predictions\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'Train the model\')\n    parser.add_argument(\'--name\', default=\'s32\', help=\'Model name to identify different logs/checkpoints\')\n    parser.add_argument(\'--hparam_set\', default=\'s32\', help=""Hyperparameters set to use (see hparams.py for available sets)"")\n    parser.add_argument(\'--n_models\', default=1, type=int, help=""Jointly train n models with different seeds"")\n    parser.add_argument(\'--multi_gpu\', default=False,  action=\'store_true\', help=""Use multiple GPUs for multi-model training, one GPU per model"")\n    parser.add_argument(\'--seed\', default=5, type=int, help=""Random seed"")\n    parser.add_argument(\'--logdir\', default=\'data/logs\', help=""Directory for summary logs"")\n    parser.add_argument(\'--max_epoch\', type=int, default=100, help=""Max number of epochs"")\n    parser.add_argument(\'--patience\', type=int, default=2, help=""Early stopping: stop after N epochs without improvement. Requires do_eval=True"")\n    parser.add_argument(\'--train_sampling\', type=float, default=1.0, help=""Sample this percent of data for training"")\n    parser.add_argument(\'--eval_sampling\', type=float, default=1.0, help=""Sample this percent of data for evaluation"")\n    parser.add_argument(\'--eval_memsize\', type=int, default=5, help=""Approximate amount of avalable memory on GPU, used for calculation of optimal evaluation batch size"")\n    parser.add_argument(\'--gpu\', default=0, type=int, help=\'GPU instance to use\')\n    parser.add_argument(\'--gpu_allow_growth\', default=False,  action=\'store_true\', help=\'Allow to gradually increase GPU memory usage instead of grabbing all available memory at start\')\n    parser.add_argument(\'--save_best_model\', default=False,  action=\'store_true\', help=\'Save best model during training. Requires do_eval=True\')\n    parser.add_argument(\'--no_forward_split\', default=True, dest=\'forward_split\',  action=\'store_false\', help=\'Use walk-forward split for model evaluation. Requires do_eval=True\')\n    parser.add_argument(\'--side_split\', default=False, action=\'store_true\', help=\'Use side split for model evaluation. Requires do_eval=True\')\n    parser.add_argument(\'--no_eval\', default=True, dest=\'do_eval\', action=\'store_false\', help=""Don\'t evaluate model quality during training"")\n    parser.add_argument(\'--no_summaries\', default=True, dest=\'write_summaries\', action=\'store_false\', help=""Don\'t Write Tensorflow summaries"")\n    parser.add_argument(\'--verbose\', default=False, action=\'store_true\', help=\'Print additional information during graph construction\')\n    parser.add_argument(\'--asgd_decay\', type=float,  help=""EMA decay for averaged SGD. Not use ASGD if not set"")\n    parser.add_argument(\'--no_tqdm\', default=True, dest=\'tqdm\', action=\'store_false\', help=""Don\'t use tqdm for status display during training"")\n    parser.add_argument(\'--max_steps\', type=int, help=""Stop training after max steps"")\n    parser.add_argument(\'--save_from_step\', type=int, help=""Save model on each evaluation (10 evals per epoch), starting from this step"")\n    parser.add_argument(\'--predict_window\', default=63, type=int, help=""Number of days to predict"")\n    args = parser.parse_args()\n\n    param_dict = dict(vars(args))\n    param_dict[\'hparams\'] = build_from_set(args.hparam_set)\n    del param_dict[\'hparam_set\']\n    train(**param_dict)\n\n    # hparams = build_hparams()\n    # result = train(""definc_attn"", hparams, n_models=1, train_sampling=1.0, eval_sampling=1.0, patience=5, multi_gpu=True,\n    #                save_best_model=False, gpu=0, eval_memsize=15, seed=5, verbose=True, forward_split=False,\n    #                write_summaries=True, side_split=True, do_eval=False, predict_window=63, asgd_decay=None, max_steps=11500,\n    #                save_from_step=10500)\n\n    # print(""Training result:"", result)\n    # preds = predict(\'data/cpt/fair_365-15428\', 380, hparams, verbose=True, back_offset=60, n_models=3)\n    # print(preds)\n'"
