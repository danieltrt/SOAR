file_path,api_count,code
9-serving/mnist_client.py,1,"b'#!/usr/bin/env python2.7\n\nimport os\nimport random\nimport numpy\n\nfrom PIL import Image\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nfrom tensorflow_serving.apis import predict_pb2\nfrom tensorflow_serving.apis import prediction_service_pb2\n\nfrom grpc.beta import implementations\n\nfrom mnist import MNIST # pylint: disable=no-name-in-module\n\nTF_MODEL_SERVER_HOST = os.getenv(""TF_MODEL_SERVER_HOST"", ""127.0.0.1"")\nTF_MODEL_SERVER_PORT = int(os.getenv(""TF_MODEL_SERVER_PORT"", 9000))\nTF_DATA_DIR = os.getenv(""TF_DATA_DIR"", ""/tmp/data/"")\nTF_MNIST_IMAGE_PATH = os.getenv(""TF_MNIST_IMAGE_PATH"", None)\nTF_MNIST_TEST_IMAGE_NUMBER = int(os.getenv(""TF_MNIST_TEST_IMAGE_NUMBER"", -1))\n\nif TF_MNIST_IMAGE_PATH != None:\n  raw_image = Image.open(TF_MNIST_IMAGE_PATH)\n  int_image = numpy.array(raw_image)\n  image = numpy.reshape(int_image, 784).astype(numpy.float32)\nelif TF_MNIST_TEST_IMAGE_NUMBER > -1:\n  test_data_set = input_data.read_data_sets(TF_DATA_DIR, one_hot=True).test\n  image = test_data_set.images[TF_MNIST_TEST_IMAGE_NUMBER]\nelse:\n  test_data_set = input_data.read_data_sets(TF_DATA_DIR, one_hot=True).test\n  image = random.choice(test_data_set.images)\n\nchannel = implementations.insecure_channel(\n    TF_MODEL_SERVER_HOST, TF_MODEL_SERVER_PORT)\nstub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n\nrequest = predict_pb2.PredictRequest()\nrequest.model_spec.name = ""mnist""\nrequest.model_spec.signature_name = ""serving_default""\nrequest.inputs[\'x\'].CopyFrom(\n    tf.contrib.util.make_tensor_proto(image, shape=[1, 28, 28]))\n\nresult = stub.Predict(request, 10.0)  # 10 secs timeout\n\nprint(result)\nprint(MNIST.display(image, threshold=0))\nprint(""Your model says the above number is... %d!"" %\n      result.outputs[""classes""].int_val[0])\n'"
1-docker/src/main.py,56,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \'License\');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \'AS IS\' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A simple MNIST classifier which displays summaries in TensorBoard.\n\nThis is an unimpressive MNIST model, but it is a good example of using\ntf.name_scope to make a graph legible in the TensorBoard graph explorer, and of\nnaming summary tags so that they are grouped meaningfully in TensorBoard.\n\nIt demonstrates the functionality of every TensorBoard dashboard.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport sys\n\nimport tensorflow as tf\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nFLAGS = None\n\n\ndef train():\n  # Import data\n  mnist = input_data.read_data_sets(FLAGS.data_dir,\n                                    one_hot=True,\n                                    fake_data=FLAGS.fake_data)\n\n  # Create a multilayer model.\n\n  # Input placeholders\n  with tf.name_scope(\'input\'):\n    x = tf.placeholder(tf.float32, [None, 784], name=\'x-input\')\n    y_ = tf.placeholder(tf.float32, [None, 10], name=\'y-input\')\n\n  with tf.name_scope(\'input_reshape\'):\n    image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])\n    tf.summary.image(\'input\', image_shaped_input, 10)\n\n  # We can\'t initialize these variables to 0 - the network will get stuck.\n  def weight_variable(shape):\n    """"""Create a weight variable with appropriate initialization.""""""\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\n  def bias_variable(shape):\n    """"""Create a bias variable with appropriate initialization.""""""\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\n  def variable_summaries(var):\n    """"""Attach a lot of summaries to a Tensor (for TensorBoard visualization).""""""\n    with tf.name_scope(\'summaries\'):\n      mean = tf.reduce_mean(var)\n      tf.summary.scalar(\'mean\', mean)\n      with tf.name_scope(\'stddev\'):\n        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n      tf.summary.scalar(\'stddev\', stddev)\n      tf.summary.scalar(\'max\', tf.reduce_max(var))\n      tf.summary.scalar(\'min\', tf.reduce_min(var))\n      tf.summary.histogram(\'histogram\', var)\n\n  def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n    """"""Reusable code for making a simple neural net layer.\n\n    It does a matrix multiply, bias add, and then uses ReLU to nonlinearize.\n    It also sets up name scoping so that the resultant graph is easy to read,\n    and adds a number of summary ops.\n    """"""\n    # Adding a name scope ensures logical grouping of the layers in the graph.\n    with tf.name_scope(layer_name):\n      # This Variable will hold the state of the weights for the layer\n      with tf.name_scope(\'weights\'):\n        weights = weight_variable([input_dim, output_dim])\n        variable_summaries(weights)\n      with tf.name_scope(\'biases\'):\n        biases = bias_variable([output_dim])\n        variable_summaries(biases)\n      with tf.name_scope(\'Wx_plus_b\'):\n        preactivate = tf.matmul(input_tensor, weights) + biases\n        tf.summary.histogram(\'pre_activations\', preactivate)\n      activations = act(preactivate, name=\'activation\')\n      tf.summary.histogram(\'activations\', activations)\n      return activations\n\n  hidden1 = nn_layer(x, 784, 500, \'layer1\')\n\n  with tf.name_scope(\'dropout\'):\n    keep_prob = tf.placeholder(tf.float32)\n    tf.summary.scalar(\'dropout_keep_probability\', keep_prob)\n    dropped = tf.nn.dropout(hidden1, keep_prob)\n\n  # Do not apply softmax activation yet, see below.\n  y = nn_layer(dropped, 500, 10, \'layer2\', act=tf.identity)\n\n  with tf.name_scope(\'cross_entropy\'):\n    # The raw formulation of cross-entropy,\n    #\n    # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.softmax(y)),\n    #                               reduction_indices=[1]))\n    #\n    # can be numerically unstable.\n    #\n    # So here we use tf.nn.softmax_cross_entropy_with_logits on the\n    # raw outputs of the nn_layer above, and then average across\n    # the batch.\n    diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n    with tf.name_scope(\'total\'):\n      cross_entropy = tf.reduce_mean(diff)\n  tf.summary.scalar(\'cross_entropy\', cross_entropy)\n\n  with tf.name_scope(\'train\'):\n    train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(\n        cross_entropy)\n\n  with tf.name_scope(\'accuracy\'):\n    with tf.name_scope(\'correct_prediction\'):\n      correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    with tf.name_scope(\'accuracy\'):\n      accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n  tf.summary.scalar(\'accuracy\', accuracy)\n\n  # Merge all the summaries and write them out to\n  # /tmp/tensorflow/mnist/logs/mnist_with_summaries (by default)\n  merged = tf.summary.merge_all()\n\n  def feed_dict(train):\n    """"""Make a TensorFlow feed_dict: maps data onto Tensor placeholders.""""""\n    if train or FLAGS.fake_data:\n      xs, ys = mnist.train.next_batch(100, fake_data=FLAGS.fake_data)\n      k = FLAGS.dropout\n    else:\n      xs, ys = mnist.test.images, mnist.test.labels\n      k = 1.0\n    return {x: xs, y_: ys, keep_prob: k}\n\n  sess = tf.InteractiveSession()\n  train_writer = tf.summary.FileWriter(FLAGS.log_dir + \'/train\', sess.graph)\n  test_writer = tf.summary.FileWriter(FLAGS.log_dir + \'/test\')\n  tf.global_variables_initializer().run()\n  # Train the model, and also write summaries.\n  # Every 10th step, measure test-set accuracy, and write test summaries\n  # All other steps, run train_step on training data, & add training summaries\n\n\n  for i in range(FLAGS.max_steps):\n    if i % 10 == 0:  # Record summaries and test-set accuracy\n      summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n      test_writer.add_summary(summary, i)\n      print(\'Accuracy at step %s: %s\' % (i, acc))\n    else:  # Record train set summaries, and train\n      if i % 100 == 99:  # Record execution stats\n        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n        run_metadata = tf.RunMetadata()\n        summary, _ = sess.run([merged, train_step],\n                              feed_dict=feed_dict(True),\n                              options=run_options,\n                              run_metadata=run_metadata)\n        train_writer.add_run_metadata(run_metadata, \'step%03d\' % i)\n        train_writer.add_summary(summary, i)\n        print(\'Adding run metadata for\', i)\n      else:  # Record a summary\n        summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n        train_writer.add_summary(summary, i)\n  train_writer.close()\n  test_writer.close()\n\n\ndef main(_):\n  train()\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\'--fake_data\', nargs=\'?\', const=True, type=bool,\n                      default=False,\n                      help=\'If true, uses fake data for unit testing.\')\n  parser.add_argument(\'--max_steps\', type=int, default=1000,\n                      help=\'Number of steps to run trainer.\')\n  parser.add_argument(\'--learning_rate\', type=float, default=0.001,\n                      help=\'Initial learning rate\')\n  parser.add_argument(\'--dropout\', type=float, default=0.9,\n                      help=\'Keep probability for training dropout.\')\n  parser.add_argument(\n      \'--data_dir\',\n      type=str,\n      default=os.path.join(os.getenv(\'TEST_TMPDIR\', \'/tmp\'),\n                           \'tensorflow/input_data\'),\n      help=\'Directory for storing input data\')\n  parser.add_argument(\n      \'--log_dir\',\n      type=str,\n      default=os.path.join(os.getenv(\'TEST_TMPDIR\', \'/tmp\'),\n                           \'tensorflow/logs\'),\n      help=\'Summaries log directory\')\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)'"
7-distributed-tensorflow/solution-src/main.py,62,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \'License\');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \'AS IS\' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A simple MNIST classifier which displays summaries in TensorBoard.\n\nThis is an unimpressive MNIST model, but it is a good example of using\ntf.name_scope to make a graph legible in the TensorBoard graph explorer, and of\nnaming summary tags so that they are grouped meaningfully in TensorBoard.\n\nIt demonstrates the functionality of every TensorBoard dashboard.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport sys\nimport ast\nimport json\n\nimport tensorflow as tf\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nFLAGS = None\n\ndef train():\n  tf_config_json = os.environ.get(""TF_CONFIG"", ""{}"")\n  tf_config = json.loads(tf_config_json)\n\n  task = tf_config.get(""task"", {})\n  cluster_spec = tf_config.get(""cluster"", {})\n  cluster_spec_object = tf.train.ClusterSpec(cluster_spec)\n  job_name = task[""type""]\n  task_id = task[""index""]\n  server_def = tf.train.ServerDef(\n      cluster=cluster_spec_object.as_cluster_def(),\n      protocol=""grpc"",\n      job_name=job_name,\n      task_index=task_id)\n  server = tf.train.Server(server_def)\n\n  is_chief = (job_name == \'master\')\n\n  # Import data\n  mnist = input_data.read_data_sets(FLAGS.data_dir,\n                                    one_hot=True,\n                                    fake_data=FLAGS.fake_data)\n\n  \n  # Create a multilayer model.\n\n\n  # Between-graph replication\n  with tf.device(tf.train.replica_device_setter(\n    worker_device=""/job:worker/task:%d"" % task_id,\n    cluster=cluster_spec)):\n\n    # count the number of updates\n    global_step = tf.get_variable(\n      \'global_step\',\n      [],\n      initializer = tf.constant_initializer(0),\n      trainable = False)\n\n    # Input placeholders\n    with tf.name_scope(\'input\'):\n      x = tf.placeholder(tf.float32, [None, 784], name=\'x-input\')\n      y_ = tf.placeholder(tf.float32, [None, 10], name=\'y-input\')\n\n    with tf.name_scope(\'input_reshape\'):\n      image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])\n      tf.summary.image(\'input\', image_shaped_input, 10)\n\n    # We can\'t initialize these variables to 0 - the network will get stuck.\n    def weight_variable(shape):\n      """"""Create a weight variable with appropriate initialization.""""""\n      initial = tf.truncated_normal(shape, stddev=0.1)\n      return tf.Variable(initial)\n\n    def bias_variable(shape):\n      """"""Create a bias variable with appropriate initialization.""""""\n      initial = tf.constant(0.1, shape=shape)\n      return tf.Variable(initial)\n\n    def variable_summaries(var):\n      """"""Attach a lot of summaries to a Tensor (for TensorBoard visualization).""""""\n      with tf.name_scope(\'summaries\'):\n        mean = tf.reduce_mean(var)\n        tf.summary.scalar(\'mean\', mean)\n        with tf.name_scope(\'stddev\'):\n          stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n        tf.summary.scalar(\'stddev\', stddev)\n        tf.summary.scalar(\'max\', tf.reduce_max(var))\n        tf.summary.scalar(\'min\', tf.reduce_min(var))\n        tf.summary.histogram(\'histogram\', var)\n\n    def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n      """"""Reusable code for making a simple neural net layer.\n\n      It does a matrix multiply, bias add, and then uses ReLU to nonlinearize.\n      It also sets up name scoping so that the resultant graph is easy to read,\n      and adds a number of summary ops.\n      """"""\n      # Adding a name scope ensures logical grouping of the layers in the graph.\n      with tf.name_scope(layer_name):\n        # This Variable will hold the state of the weights for the layer\n        with tf.name_scope(\'weights\'):\n          weights = weight_variable([input_dim, output_dim])\n          variable_summaries(weights)\n        with tf.name_scope(\'biases\'):\n          biases = bias_variable([output_dim])\n          variable_summaries(biases)\n        with tf.name_scope(\'Wx_plus_b\'):\n          preactivate = tf.matmul(input_tensor, weights) + biases\n          tf.summary.histogram(\'pre_activations\', preactivate)\n        activations = act(preactivate, name=\'activation\')\n        tf.summary.histogram(\'activations\', activations)\n        return activations\n\n    hidden1 = nn_layer(x, 784, 500, \'layer1\')\n\n    with tf.name_scope(\'dropout\'):\n      keep_prob = tf.placeholder(tf.float32)\n      tf.summary.scalar(\'dropout_keep_probability\', keep_prob)\n      dropped = tf.nn.dropout(hidden1, keep_prob)\n\n    # Do not apply softmax activation yet, see below.\n    y = nn_layer(dropped, 500, 10, \'layer2\', act=tf.identity)\n\n    with tf.name_scope(\'cross_entropy\'):\n      # The raw formulation of cross-entropy,\n      #\n      # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.softmax(y)),\n      #                               reduction_indices=[1]))\n      #\n      # can be numerically unstable.\n      #\n      # So here we use tf.nn.softmax_cross_entropy_with_logits on the\n      # raw outputs of the nn_layer above, and then average across\n      # the batch.\n      diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n      with tf.name_scope(\'total\'):\n        cross_entropy = tf.reduce_mean(diff)\n    tf.summary.scalar(\'cross_entropy\', cross_entropy)\n\n    with tf.name_scope(\'train\'):\n      train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(\n          cross_entropy)\n\n    with tf.name_scope(\'accuracy\'):\n      with tf.name_scope(\'correct_prediction\'):\n        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n      with tf.name_scope(\'accuracy\'):\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    tf.summary.scalar(\'accuracy\', accuracy)\n\n    # Merge all the summaries and write them out to\n    # /tmp/tensorflow/mnist/logs/mnist_with_summaries (by default)\n    merged = tf.summary.merge_all()  \n\n    init_op = tf.global_variables_initializer()\n\n  def feed_dict(train):\n    """"""Make a TensorFlow feed_dict: maps data onto Tensor placeholders.""""""\n    if train or FLAGS.fake_data:\n      xs, ys = mnist.train.next_batch(100, fake_data=FLAGS.fake_data)\n      k = FLAGS.dropout\n    else:\n      xs, ys = mnist.test.images, mnist.test.labels\n      k = 1.0\n    return {x: xs, y_: ys, keep_prob: k}\n\n\n\n  sv = tf.train.Supervisor(is_chief=is_chief,\n\t\t\t\t\t\tglobal_step=global_step,\n\t\t\t\t\t\tinit_op=init_op,\n\t\t\t\t\t\tlogdir=FLAGS.logdir)\n\n  with sv.prepare_or_wait_for_session(server.target) as sess:  \n    train_writer = tf.summary.FileWriter(FLAGS.logdir + \'/train\', sess.graph)\n    test_writer = tf.summary.FileWriter(FLAGS.logdir + \'/test\')\n    # Train the model, and also write summaries.\n    # Every 10th step, measure test-set accuracy, and write test summaries\n    # All other steps, run train_step on training data, & add training summaries\n\n    for i in range(FLAGS.max_steps):\n      if i % 10 == 0:  # Record summaries and test-set accuracy\n        summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n        test_writer.add_summary(summary, i)\n        print(\'Accuracy at step %s: %s\' % (i, acc))\n      else:  # Record train set summaries, and train\n        if i % 100 == 99:  # Record execution stats\n          run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n          run_metadata = tf.RunMetadata()\n          summary, _ = sess.run([merged, train_step],\n                                feed_dict=feed_dict(True),\n                                options=run_options,\n                                run_metadata=run_metadata)\n          train_writer.add_run_metadata(run_metadata, \'step%03d\' % i)\n          train_writer.add_summary(summary, i)\n          print(\'Adding run metadata for\', i)\n        else:  # Record a summary\n          summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n          train_writer.add_summary(summary, i)\n    train_writer.close()\n    test_writer.close()\n\n\ndef main(_):\n  train()\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\'--fake_data\', nargs=\'?\', const=True, type=bool,\n                      default=False,\n                      help=\'If true, uses fake data for unit testing.\')\n  parser.add_argument(\'--max_steps\', type=int, default=1000,\n                      help=\'Number of steps to run trainer.\')\n  parser.add_argument(\'--learning_rate\', type=float, default=0.001,\n                      help=\'Initial learning rate\')\n  parser.add_argument(\'--dropout\', type=float, default=0.9,\n                      help=\'Keep probability for training dropout.\')\n  parser.add_argument(\n      \'--data_dir\',\n      type=str,\n      default=os.path.join(os.getenv(\'TEST_TMPDIR\', \'/tmp\'),\n                           \'tensorflow/input_data\'),\n      help=\'Directory for storing input data\')\n  parser.add_argument(\n      \'--logdir\',\n      type=str,\n      default=os.path.join(os.getenv(\'TEST_TMPDIR\', \'/tmp\'),\n                           \'tensorflow/logs\'),\n      help=\'Summaries log directory\')\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)'"
8-hyperparam-sweep/src/main.py,16,"b'import click\nimport tensorflow as tf\nimport numpy as np\nfrom skimage.data import astronaut\nfrom scipy.misc import imresize, imsave, imread\n\nimg = imread(\'./starry.jpg\')\nimg = imresize(img, (100, 100))\nsave_dir = \'output\'\nepochs = 2000\n\n\ndef linear_layer(X, layer_size, layer_name):\n    with tf.variable_scope(layer_name):\n        W = tf.Variable(tf.random_uniform([X.get_shape().as_list()[1], layer_size], dtype=tf.float32), name=\'W\')\n        b = tf.Variable(tf.zeros([layer_size]), name=\'b\')\n        return tf.nn.relu(tf.matmul(X, W) + b)\n\n@click.command()\n@click.option(""--learning-rate"", default=0.01) \n@click.option(""--hidden-layers"", default=7)\n@click.option(""--logdir"")\ndef main(learning_rate, hidden_layers, logdir=\'./logs/1\'):\n    X = tf.placeholder(dtype=tf.float32, shape=(None, 2), name=\'X\') \n    y = tf.placeholder(dtype=tf.float32, shape=(None, 3), name=\'y\')\n    current_input = X\n    for layer_id in range(hidden_layers):\n        h = linear_layer(current_input, 20, \'layer{}\'.format(layer_id))\n        current_input = h\n\n    y_pred = linear_layer(current_input, 3, \'output\')\n\n    #loss will be distance between predicted and true RGB\n    loss = tf.reduce_mean(tf.reduce_sum(tf.squared_difference(y, y_pred), 1))\n    tf.summary.scalar(\'loss\', loss)\n\n    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n    merged_summary_op = tf.summary.merge_all()  \n\n    res_img = tf.cast(tf.clip_by_value(tf.reshape(y_pred, (1,) + img.shape), 0, 255), tf.uint8)\n    img_summary = tf.summary.image(\'out\', res_img, max_outputs=1)\n    \n    xs, ys = get_data(img)\n\n    with tf.Session() as sess:\n        tf.global_variables_initializer().run()  \n        train_writer = tf.summary.FileWriter(logdir + \'/train\', sess.graph)\n        test_writer = tf.summary.FileWriter(logdir + \'/test\')\n        batch_size = 50\n        for i in range(epochs):\n            # Get a random sampling of the dataset\n            idxs = np.random.permutation(range(len(xs)))\n            # The number of batches we have to iterate over\n            n_batches = len(idxs) // batch_size\n            # Now iterate over our stochastic minibatches:\n            for batch_i in range(n_batches):\n                batch_idxs = idxs[batch_i * batch_size: (batch_i + 1) * batch_size]\n                sess.run([train_op, loss, merged_summary_op], feed_dict={X: xs[batch_idxs], y: ys[batch_idxs]})\n                if batch_i % 100 == 0:\n                    c, summary = sess.run([loss, merged_summary_op], feed_dict={X: xs[batch_idxs], y: ys[batch_idxs]})\n                    train_writer.add_summary(summary, (i * n_batches * batch_size) + batch_i)\n                    print(""epoch {}, (l2) loss {}"".format(i, c))           \n\n            if i % 10 == 0:\n                img_summary_res = sess.run(img_summary, feed_dict={X: xs, y: ys})\n                test_writer.add_summary(img_summary_res, i * n_batches * batch_size)\n\ndef get_data(img):\n    xs = []\n    ys = []\n    for row_i in range(img.shape[0]):\n        for col_i in range(img.shape[1]):\n            xs.append([row_i, col_i])\n            ys.append(img[row_i, col_i])\n\n    xs = (xs - np.mean(xs)) / np.std(xs)\n    return xs, np.array(ys)\n\nif __name__ == ""__main__"":\n    main()'"
