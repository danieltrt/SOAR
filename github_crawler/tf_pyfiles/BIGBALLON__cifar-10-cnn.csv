file_path,api_count,code
1_Lecun_Network/LeNet_dp_da_keras.py,0,"b""import keras\nimport numpy as np\nfrom keras import optimizers\nfrom keras.datasets import cifar10\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\nfrom keras.callbacks import LearningRateScheduler, TensorBoard\nfrom keras.preprocessing.image import ImageDataGenerator\n\nbatch_size    = 128\nepochs        = 200\niterations    = 391\nnum_classes   = 10\nmean          = [125.307, 122.95, 113.865]\nstd           = [62.9932, 62.0887, 66.7048]\n\ndef build_model():\n    model = Sequential()\n    model.add(Conv2D(6, (5, 5), padding='valid', activation = 'relu', kernel_initializer='he_normal', input_shape=(32,32,3)))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n    model.add(Conv2D(16, (5, 5), padding='valid', activation = 'relu', kernel_initializer='he_normal'))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(120, activation = 'relu', kernel_initializer='he_normal'))\n    model.add(Dense(84, activation = 'relu', kernel_initializer='he_normal'))\n    model.add(Dense(10, activation = 'softmax', kernel_initializer='he_normal'))\n    sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n    return model\n\ndef scheduler(epoch):\n    if epoch < 100:\n        return 0.01\n    if epoch < 150:\n        return 0.005\n    return 0.001\n\nif __name__ == '__main__':\n\n    # load data\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n    y_train = keras.utils.to_categorical(y_train, num_classes)\n    y_test = keras.utils.to_categorical(y_test, num_classes)\n    x_train = x_train.astype('float32')\n    x_test = x_test.astype('float32')\n    \n    # data preprocessing  [raw - mean / std]\n    for i in range(3):\n        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n        \n    # build network\n    model = build_model()\n    print(model.summary())\n    # set callback\n    tb_cb = TensorBoard(log_dir='./lenet_dp_da', histogram_freq=0)\n    change_lr = LearningRateScheduler(scheduler)\n    cbks = [change_lr,tb_cb]\n\n    # using real-time data augmentation\n    print('Using real-time data augmentation.')\n    datagen = ImageDataGenerator(horizontal_flip=True,\n            width_shift_range=0.125,height_shift_range=0.125,fill_mode='constant',cval=0.)\n\n    datagen.fit(x_train)\n\n    # start train \n    model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n                        steps_per_epoch=iterations,\n                        epochs=epochs,\n                        callbacks=cbks,\n                        validation_data=(x_test, y_test))\n    # save model\n    model.save('lenet_dp_da.h5')\n\n\n\n"""
1_Lecun_Network/LeNet_dp_da_wd_keras.py,0,"b""import keras\nimport numpy as np\nfrom keras import optimizers\nfrom keras.datasets import cifar10\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\nfrom keras.callbacks import LearningRateScheduler, TensorBoard\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.regularizers import l2\n\nbatch_size    = 128\nepochs        = 200\niterations    = 391\nnum_classes   = 10\nweight_decay  = 0.0001\nmean          = [125.307, 122.95, 113.865]\nstd           = [62.9932, 62.0887, 66.7048]\n\ndef build_model():\n    model = Sequential()\n    model.add(Conv2D(6, (5, 5), padding='valid', activation = 'relu', kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay), input_shape=(32,32,3)))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n    model.add(Conv2D(16, (5, 5), padding='valid', activation = 'relu', kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay)))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(120, activation = 'relu', kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay) ))\n    model.add(Dense(84, activation = 'relu', kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay) ))\n    model.add(Dense(10, activation = 'softmax', kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay) ))\n    sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n    return model\n\ndef scheduler(epoch):\n    if epoch < 100:\n        return 0.01\n    if epoch < 150:\n        return 0.005\n    return 0.001\n\nif __name__ == '__main__':\n\n    # load data\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n    y_train = keras.utils.to_categorical(y_train, num_classes)\n    y_test = keras.utils.to_categorical(y_test, num_classes)\n    x_train = x_train.astype('float32')\n    x_test = x_test.astype('float32')\n    \n    # data preprocessing  [raw - mean / std]\n    for i in range(3):\n        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n\n    # build network\n    model = build_model()\n    print(model.summary())\n\n    # set callback\n    tb_cb = TensorBoard(log_dir='./lenet_dp_da_wd', histogram_freq=0)\n    change_lr = LearningRateScheduler(scheduler)\n    cbks = [change_lr,tb_cb]\n\n    # using real-time data augmentation\n    print('Using real-time data augmentation.')\n    datagen = ImageDataGenerator(horizontal_flip=True,\n            width_shift_range=0.125,height_shift_range=0.125,fill_mode='constant',cval=0.)\n\n    datagen.fit(x_train)\n\n    # start train \n    model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n                        steps_per_epoch=iterations,\n                        epochs=epochs,\n                        callbacks=cbks,\n                        validation_data=(x_test, y_test))\n    # save model\n    model.save('lenet_dp_da_wd.h5')\n"""
1_Lecun_Network/LeNet_dp_keras.py,0,"b""import keras\nimport numpy as np\nfrom keras import optimizers\nfrom keras.datasets import cifar10\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\nfrom keras.callbacks import LearningRateScheduler, TensorBoard\n\nbatch_size    = 128\nepochs        = 200\niterations    = 391\nnum_classes   = 10\n\ndef build_model():\n    model = Sequential()\n    model.add(Conv2D(6, (5, 5), padding='valid', activation = 'relu', kernel_initializer='he_normal', input_shape=(32,32,3)))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n    model.add(Conv2D(16, (5, 5), padding='valid', activation = 'relu', kernel_initializer='he_normal'))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(120, activation = 'relu', kernel_initializer='he_normal'))\n    model.add(Dense(84, activation = 'relu', kernel_initializer='he_normal'))\n    model.add(Dense(10, activation = 'softmax', kernel_initializer='he_normal'))\n    sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n    return model\n\ndef scheduler(epoch):\n    if epoch < 100:\n        return 0.01\n    if epoch < 150:\n        return 0.005\n    return 0.001\n\nif __name__ == '__main__':\n\n    # load data\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n    y_train = keras.utils.to_categorical(y_train, num_classes)\n    y_test = keras.utils.to_categorical(y_test, num_classes)\n    x_train = x_train.astype('float32')\n    x_test = x_test.astype('float32')\n    \n    # x_train /= 255\n    # x_test /= 255\n    mean  = [125.307, 122.95, 113.865]\n    std   = [62.9932, 62.0887, 66.7048]\n    for i in range(3):\n        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n\n    # build network\n    model = build_model()\n    print(model.summary())\n    \n    # set callback\n    tb_cb = TensorBoard(log_dir='./lenet_dp', histogram_freq=0)\n    change_lr = LearningRateScheduler(scheduler)\n    cbks = [change_lr,tb_cb]\n\n    # start train \n    model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,callbacks=cbks,\n                  validation_data=(x_test, y_test), shuffle=True)\n    # save model\n    model.save('lenet_dp.h5')\n\n\n\n"""
1_Lecun_Network/LeNet_keras.py,0,"b""import keras\nfrom keras import optimizers\nfrom keras.datasets import cifar10\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\nfrom keras.callbacks import LearningRateScheduler, TensorBoard\n\ndef build_model():\n    model = Sequential()\n    model.add(Conv2D(6, (5, 5), padding='valid', activation = 'relu', kernel_initializer='he_normal', input_shape=(32,32,3)))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n    model.add(Conv2D(16, (5, 5), padding='valid', activation = 'relu', kernel_initializer='he_normal'))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(120, activation = 'relu', kernel_initializer='he_normal'))\n    model.add(Dense(84, activation = 'relu', kernel_initializer='he_normal'))\n    model.add(Dense(10, activation = 'softmax', kernel_initializer='he_normal'))\n    sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n    return model\n\ndef scheduler(epoch):\n    if epoch < 100:\n        return 0.01\n    if epoch < 150:\n        return 0.005\n    return 0.001\n\nif __name__ == '__main__':\n\n    # load data\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n    y_train = keras.utils.to_categorical(y_train, 10)\n    y_test = keras.utils.to_categorical(y_test, 10)\n    x_train = x_train.astype('float32')\n    x_test = x_test.astype('float32')\n    x_train /= 255.0\n    x_test /= 255.0\n\n    # build network\n    model = build_model()\n    print(model.summary())\n\n    # set callback\n    tb_cb = TensorBoard(log_dir='./lenet', histogram_freq=0)\n    change_lr = LearningRateScheduler(scheduler)\n    cbks = [change_lr,tb_cb]\n\n    # start train\n    model.fit(x_train, y_train,\n              batch_size=128,\n              epochs=200,\n              callbacks=cbks,\n              validation_data=(x_test, y_test),\n              shuffle=True)\n\n    # save model\n    model.save('lenet.h5')"""
2_Network_in_Network/Network_in_Network_bn_keras.py,2,"b""import keras\r\nimport numpy as np\r\nfrom keras.datasets import cifar10\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Activation, Flatten\r\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, AveragePooling2D\r\nfrom keras.initializers import RandomNormal  \r\nfrom keras.layers.normalization import BatchNormalization\r\nfrom keras import optimizers\r\nfrom keras.callbacks import LearningRateScheduler, TensorBoard\r\n\r\nbatch_size    = 128\r\nepochs        = 200\r\niterations    = 391\r\nnum_classes   = 10\r\ndropout       = 0.5\r\nweight_decay  = 0.0001\r\nlog_filepath  = './nin_bn'\r\n\r\nfrom keras import backend as K\r\nif('tensorflow' == K.backend()):\r\n    import tensorflow as tf\r\n    from keras.backend.tensorflow_backend import set_session\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    sess = tf.Session(config=config)\r\n    \r\n\r\ndef color_preprocessing(x_train,x_test):\r\n    x_train = x_train.astype('float32')\r\n    x_test = x_test.astype('float32')\r\n    mean = [125.307, 122.95, 113.865]\r\n    std  = [62.9932, 62.0887, 66.7048]\r\n    for i in range(3):\r\n        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\r\n        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\r\n\r\n    return x_train, x_test\r\n\r\ndef scheduler(epoch):\r\n    if epoch <= 60:\r\n        return 0.05\r\n    if epoch <= 120:\r\n        return 0.01\r\n    if epoch <= 160:    \r\n        return 0.002\r\n    return 0.0004\r\n\r\ndef build_model():\r\n  model = Sequential()\r\n\r\n  model.add(Conv2D(192, (5, 5), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\r\n  model.add(BatchNormalization())\r\n  model.add(Activation('relu'))\r\n  model.add(Conv2D(160, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\r\n  model.add(BatchNormalization())\r\n  model.add(Activation('relu'))\r\n  model.add(Conv2D(96, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\r\n  model.add(BatchNormalization())\r\n  model.add(Activation('relu'))\r\n  model.add(MaxPooling2D(pool_size=(3, 3),strides=(2,2),padding = 'same'))\r\n  \r\n  model.add(Dropout(dropout))\r\n  \r\n  model.add(Conv2D(192, (5, 5), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\r\n  model.add(BatchNormalization())\r\n  model.add(Activation('relu'))\r\n  model.add(Conv2D(192, (1, 1),padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\r\n  model.add(BatchNormalization())\r\n  model.add(Activation('relu'))\r\n  model.add(Conv2D(192, (1, 1),padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\r\n  model.add(BatchNormalization())\r\n  model.add(Activation('relu'))\r\n  model.add(MaxPooling2D(pool_size=(3, 3),strides=(2,2),padding = 'same'))\r\n  \r\n  model.add(Dropout(dropout))\r\n  \r\n  model.add(Conv2D(192, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\r\n  model.add(BatchNormalization())\r\n  model.add(Activation('relu'))\r\n  model.add(Conv2D(192, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\r\n  model.add(BatchNormalization())\r\n  model.add(Activation('relu'))\r\n  model.add(Conv2D(10, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\r\n  model.add(BatchNormalization())\r\n  model.add(Activation('relu'))\r\n  \r\n  model.add(GlobalAveragePooling2D())\r\n  model.add(Activation('softmax'))\r\n  \r\n  sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\r\n  model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\r\n  return model\r\n\r\nif __name__ == '__main__':\r\n\r\n    # load data\r\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n    y_train = keras.utils.to_categorical(y_train, num_classes)\r\n    y_test = keras.utils.to_categorical(y_test, num_classes)\r\n    \r\n    x_train, x_test = color_preprocessing(x_train, x_test)\r\n\r\n    # build network\r\n    model = build_model()\r\n    print(model.summary())\r\n\r\n    # set callback\r\n    tb_cb = TensorBoard(log_dir=log_filepath, histogram_freq=0)\r\n    change_lr = LearningRateScheduler(scheduler)\r\n    cbks = [change_lr,tb_cb]\r\n\r\n    # set data augmentation\r\n    print('Using real-time data augmentation.')\r\n    datagen = ImageDataGenerator(horizontal_flip=True,width_shift_range=0.125,height_shift_range=0.125,fill_mode='constant',cval=0.)\r\n    datagen.fit(x_train)\r\n\r\n    # start training\r\n    model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),steps_per_epoch=iterations,epochs=epochs,callbacks=cbks,validation_data=(x_test, y_test))\r\n    model.save('nin_bn.h5')\r\n"""
2_Network_in_Network/Network_in_Network_keras.py,2,"b""import keras\r\nimport numpy as np\r\nfrom keras.datasets import cifar10\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Activation, Flatten\r\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, AveragePooling2D\r\nfrom keras.initializers import RandomNormal  \r\nfrom keras import optimizers\r\nfrom keras.callbacks import LearningRateScheduler, TensorBoard\r\nfrom keras.layers.normalization import BatchNormalization\r\n\r\nbatch_size    = 128\r\nepochs        = 200\r\niterations    = 391\r\nnum_classes   = 10\r\ndropout       = 0.5\r\nweight_decay  = 0.0001\r\nlog_filepath  = './nin'\r\n\r\nfrom keras import backend as K\r\nif('tensorflow' == K.backend()):\r\n    import tensorflow as tf\r\n    from keras.backend.tensorflow_backend import set_session\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    sess = tf.Session(config=config)\r\n    \r\ndef color_preprocessing(x_train,x_test):\r\n    x_train = x_train.astype('float32')\r\n    x_test = x_test.astype('float32')\r\n    mean = [125.307, 122.95, 113.865]\r\n    std  = [62.9932, 62.0887, 66.7048]\r\n    for i in range(3):\r\n        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\r\n        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\r\n\r\n    return x_train, x_test\r\n\r\ndef scheduler(epoch):\r\n    if epoch <= 80:\r\n        return 0.01\r\n    if epoch <= 140:\r\n        return 0.005\r\n    return 0.001\r\n\r\ndef build_model():\r\n  model = Sequential()\r\n\r\n  model.add(Conv2D(192, (5, 5), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\r\n  model.add(Activation('relu'))\r\n  model.add(Conv2D(160, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\r\n  model.add(Activation('relu'))\r\n  model.add(Conv2D(96, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\r\n  model.add(Activation('relu'))\r\n  model.add(MaxPooling2D(pool_size=(3, 3),strides=(2,2),padding = 'same'))\r\n  \r\n  model.add(Dropout(dropout))\r\n  \r\n  model.add(Conv2D(192, (5, 5), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\r\n  model.add(Activation('relu'))\r\n  model.add(Conv2D(192, (1, 1),padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\r\n  model.add(Activation('relu'))\r\n  model.add(Conv2D(192, (1, 1),padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\r\n  model.add(Activation('relu'))\r\n  model.add(MaxPooling2D(pool_size=(3, 3),strides=(2,2),padding = 'same'))\r\n  \r\n  model.add(Dropout(dropout))\r\n  \r\n  model.add(Conv2D(192, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\r\n  model.add(Activation('relu'))\r\n  model.add(Conv2D(192, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\r\n  model.add(Activation('relu'))\r\n  model.add(Conv2D(10, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\r\n  model.add(Activation('relu'))\r\n  \r\n  model.add(GlobalAveragePooling2D())\r\n  model.add(Activation('softmax'))\r\n  \r\n  sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\r\n  model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\r\n  return model\r\n\r\nif __name__ == '__main__':\r\n\r\n    # load data\r\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n    y_train = keras.utils.to_categorical(y_train, num_classes)\r\n    y_test = keras.utils.to_categorical(y_test, num_classes)\r\n    \r\n    x_train, x_test = color_preprocessing(x_train, x_test)\r\n\r\n    # build network\r\n    model = build_model()\r\n    print(model.summary())\r\n\r\n    # set callback\r\n    tb_cb = TensorBoard(log_dir=log_filepath, histogram_freq=0)\r\n    change_lr = LearningRateScheduler(scheduler)\r\n    cbks = [change_lr,tb_cb]\r\n\r\n    # set data augmentation\r\n    print('Using real-time data augmentation.')\r\n    datagen = ImageDataGenerator(horizontal_flip=True,width_shift_range=0.125,height_shift_range=0.125,fill_mode='constant',cval=0.)\r\n    datagen.fit(x_train)\r\n\r\n    # start training\r\n    model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),steps_per_epoch=iterations,epochs=epochs,callbacks=cbks,validation_data=(x_test, y_test))\r\n    model.save('nin.h5')\r\n"""
3_Vgg19_Network/Vgg19_keras.py,2,"b""# ========================================================== #\r\n# File name: retrain_cifar-10_bn.py\r\n# Author: BIGBALLON\r\n# Date created: 07/27/2017\r\n# Python Version: 3.5.2\r\n# Tensorflow Vetsion: 1.2.1\r\n# Result: test accuracy about 93.50 ~ 93.70%\r\n# ========================================================== #\r\n\r\nimport keras\r\nimport numpy as np\r\nfrom keras.datasets import cifar10\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Activation, Flatten\r\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, AveragePooling2D\r\nfrom keras.initializers import he_normal\r\nfrom keras import optimizers\r\nfrom keras.callbacks import LearningRateScheduler, TensorBoard\r\nfrom keras.layers.normalization import BatchNormalization\r\nfrom keras.utils.data_utils import get_file\r\n\r\nnum_classes  = 10\r\nbatch_size   = 128\r\nepochs       = 200\r\niterations   = 391\r\ndropout      = 0.5\r\nweight_decay = 0.0001\r\nlog_filepath = r'./vgg19_retrain_logs/'\r\n\r\nfrom keras import backend as K\r\nif('tensorflow' == K.backend()):\r\n    import tensorflow as tf\r\n    from keras.backend.tensorflow_backend import set_session\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    sess = tf.Session(config=config)\r\n\r\ndef scheduler(epoch):\r\n    if epoch < 80:\r\n        return 0.1\r\n    if epoch < 160:\r\n        return 0.01\r\n    return 0.001\r\n\r\nWEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels.h5'\r\nfilepath = get_file('vgg19_weights_tf_dim_ordering_tf_kernels.h5', WEIGHTS_PATH, cache_subdir='models')\r\n\r\n# data loading\r\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\ny_train = keras.utils.to_categorical(y_train, num_classes)\r\ny_test = keras.utils.to_categorical(y_test, num_classes)\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\n\r\n# data preprocessing \r\nx_train[:,:,:,0] = (x_train[:,:,:,0]-123.680)\r\nx_train[:,:,:,1] = (x_train[:,:,:,1]-116.779)\r\nx_train[:,:,:,2] = (x_train[:,:,:,2]-103.939)\r\nx_test[:,:,:,0] = (x_test[:,:,:,0]-123.680)\r\nx_test[:,:,:,1] = (x_test[:,:,:,1]-116.779)\r\nx_test[:,:,:,2] = (x_test[:,:,:,2]-103.939)\r\n\r\n# build model\r\nmodel = Sequential()\r\n\r\n# Block 1\r\nmodel.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block1_conv1', input_shape=x_train.shape[1:]))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Activation('relu'))\r\nmodel.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block1_conv2'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool'))\r\n\r\n# Block 2\r\nmodel.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block2_conv1'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Activation('relu'))\r\nmodel.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block2_conv2'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool'))\r\n\r\n# Block 3\r\nmodel.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block3_conv1'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Activation('relu'))\r\nmodel.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block3_conv2'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Activation('relu'))\r\nmodel.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block3_conv3'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Activation('relu'))\r\nmodel.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block3_conv4'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool'))\r\n\r\n# Block 4\r\nmodel.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block4_conv1'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Activation('relu'))\r\nmodel.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block4_conv2'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Activation('relu'))\r\nmodel.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block4_conv3'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Activation('relu'))\r\nmodel.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block4_conv4'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool'))\r\n\r\n# Block 5\r\nmodel.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block5_conv1'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Activation('relu'))\r\nmodel.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block5_conv2'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Activation('relu'))\r\nmodel.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block5_conv3'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Activation('relu'))\r\nmodel.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block5_conv4'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool'))\r\n\r\n# model modification for cifar-10\r\nmodel.add(Flatten(name='flatten'))\r\nmodel.add(Dense(4096, use_bias = True, kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='fc_cifa10'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Activation('relu'))\r\nmodel.add(Dropout(dropout))\r\nmodel.add(Dense(4096, kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='fc2'))  \r\nmodel.add(BatchNormalization())\r\nmodel.add(Activation('relu'))\r\nmodel.add(Dropout(dropout))      \r\nmodel.add(Dense(10, kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='predictions_cifa10'))        \r\nmodel.add(BatchNormalization())\r\nmodel.add(Activation('softmax'))\r\n\r\n# load pretrained weight from VGG19 by name      \r\nmodel.load_weights(filepath, by_name=True)\r\n\r\n# -------- optimizer setting -------- #\r\nsgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\r\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\r\n\r\ntb_cb = TensorBoard(log_dir=log_filepath, histogram_freq=0)\r\nchange_lr = LearningRateScheduler(scheduler)\r\ncbks = [change_lr,tb_cb]\r\n\r\nprint('Using real-time data augmentation.')\r\ndatagen = ImageDataGenerator(horizontal_flip=True,\r\n        width_shift_range=0.125,height_shift_range=0.125,fill_mode='constant',cval=0.)\r\n\r\ndatagen.fit(x_train)\r\n\r\nmodel.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\r\n                    steps_per_epoch=iterations,\r\n                    epochs=epochs,\r\n                    callbacks=cbks,\r\n                    validation_data=(x_test, y_test))\r\nmodel.save('retrain.h5')\r\n"""
3_Vgg19_Network/Vgg_prediction.py,0,"b'import keras\r\nfrom keras.applications.vgg19 import VGG19\r\nfrom keras.applications.vgg19 import preprocess_input, decode_predictions\r\nfrom PIL import Image\r\nimport numpy as np\r\nimport os.path\r\n\r\n# Using ImageNet pre_trained weights to predict image\'s class(1000 class)\r\n# ImageNet -- http://www.image-net.org/\r\n# make sure your package pillow is the latest version \r\n\r\nmodel = VGG19(weights=\'imagenet\') # load keras ImageNet pre_trained model\r\n\r\nwhile True:\r\n\timg_path = input(\'Please input picture file to predict ( input Q to exit ):  \')\r\n\tif img_path == \'Q\':\r\n\t\tbreak\r\n\tif not os.path.exists(img_path):\r\n\t\tprint(""file not exist!"")\r\n\t\tcontinue\r\n\ttry:\r\n\t\timg = Image.open(img_path)\r\n\t\tori_w,ori_h = img.size\r\n\t\tnew_w = 224.0;\r\n\t\tnew_h = 224.0;\r\n\t\tif ori_w > ori_h:\r\n\t\t\tbs = 224.0 / ori_h;\r\n\t\t\tnew_w = ori_w * bs\r\n\t\t\tweight = int(new_w)\r\n\t\t\theight = int(new_h)\r\n\t\t\timg = img.resize( (weight, height), Image.BILINEAR )\r\n\t\t\tregion = ( weight / 2 - 112, 0, weight / 2 + 112, height)\r\n\t\t\timg = img.crop( region )\r\n\t\telse:\r\n\t\t\tbs = 224.0 / ori_w;\r\n\t\t\tnew_h = ori_h * bs\r\n\t\t\tweight = int(new_w)\r\n\t\t\theight = int(new_h)\r\n\t\t\timg = img.resize( (weight, height), Image.BILINEAR )\r\n\t\t\tregion = ( 0, height / 2 - 112 , weight, height / 2 + 112  )\r\n\t\t\timg = img.crop( region )\r\n\t\tx = np.asarray( img, dtype = \'float32\' )\r\n\t\tx[:, :, 0] = x[:, :, 0] - 123.680\r\n\t\tx[:, :, 1] = x[:, :, 1] - 116.779\r\n\t\tx[:, :, 2] = x[:, :, 2] - 103.939\r\n\t\tx = np.expand_dims(x, axis=0)\r\n\t\tresults = model.predict(x)\r\n\t\tprint(\'Predicted:\', decode_predictions(results, top=5)[0])\r\n\texcept Exception as e:\r\n\t\tpass\r\n\r\n\r\n\t'"
4_Residual_Network/ResNet_keras.py,2,"b'import keras\r\nimport argparse\r\nimport numpy as np\r\nfrom keras.datasets import cifar10, cifar100\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.layers.normalization import BatchNormalization\r\nfrom keras.layers import Conv2D, Dense, Input, add, Activation, GlobalAveragePooling2D\r\nfrom keras.callbacks import LearningRateScheduler, TensorBoard, ModelCheckpoint\r\nfrom keras.models import Model\r\nfrom keras import optimizers, regularizers\r\nfrom keras import backend as K\r\n\r\n# set GPU memory \r\nif(\'tensorflow\' == K.backend()):\r\n    import tensorflow as tf\r\n    from keras.backend.tensorflow_backend import set_session\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    sess = tf.Session(config=config)\r\n\r\n# set parameters via parser\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\'-b\',\'--batch_size\', type=int, default=128, metavar=\'NUMBER\',\r\n                help=\'batch size(default: 128)\')\r\nparser.add_argument(\'-e\',\'--epochs\', type=int, default=200, metavar=\'NUMBER\',\r\n                help=\'epochs(default: 200)\')\r\nparser.add_argument(\'-n\',\'--stack_n\', type=int, default=5, metavar=\'NUMBER\',\r\n                help=\'stack number n, total layers = 6 * n + 2 (default: 5)\')\r\nparser.add_argument(\'-d\',\'--dataset\', type=str, default=""cifar10"", metavar=\'STRING\',\r\n                help=\'dataset. (default: cifar10)\')\r\n\r\nargs = parser.parse_args()\r\n\r\nstack_n            = args.stack_n\r\nlayers             = 6 * stack_n + 2\r\nnum_classes        = 10\r\nimg_rows, img_cols = 32, 32\r\nimg_channels       = 3\r\nbatch_size         = args.batch_size\r\nepochs             = args.epochs\r\niterations         = 50000 // batch_size + 1\r\nweight_decay       = 1e-4\r\n\r\ndef color_preprocessing(x_train,x_test):\r\n    x_train = x_train.astype(\'float32\')\r\n    x_test = x_test.astype(\'float32\')\r\n    mean = [125.307, 122.95, 113.865]\r\n    std  = [62.9932, 62.0887, 66.7048]\r\n    for i in range(3):\r\n        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\r\n        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\r\n    return x_train, x_test\r\n\r\n\r\ndef scheduler(epoch):\r\n    if epoch < 81:\r\n        return 0.1\r\n    if epoch < 122:\r\n        return 0.01\r\n    return 0.001\r\n\r\n\r\ndef residual_network(img_input,classes_num=10,stack_n=5):\r\n    \r\n    def residual_block(x,o_filters,increase=False):\r\n        stride = (1,1)\r\n        if increase:\r\n            stride = (2,2)\r\n\r\n        o1 = Activation(\'relu\')(BatchNormalization(momentum=0.9, epsilon=1e-5)(x))\r\n        conv_1 = Conv2D(o_filters,kernel_size=(3,3),strides=stride,padding=\'same\',\r\n                        kernel_initializer=""he_normal"",\r\n                        kernel_regularizer=regularizers.l2(weight_decay))(o1)\r\n        o2  = Activation(\'relu\')(BatchNormalization(momentum=0.9, epsilon=1e-5)(conv_1))\r\n        conv_2 = Conv2D(o_filters,kernel_size=(3,3),strides=(1,1),padding=\'same\',\r\n                        kernel_initializer=""he_normal"",\r\n                        kernel_regularizer=regularizers.l2(weight_decay))(o2)\r\n        if increase:\r\n            projection = Conv2D(o_filters,kernel_size=(1,1),strides=(2,2),padding=\'same\',\r\n                                kernel_initializer=""he_normal"",\r\n                                kernel_regularizer=regularizers.l2(weight_decay))(o1)\r\n            block = add([conv_2, projection])\r\n        else:\r\n            block = add([conv_2, x])\r\n        return block\r\n\r\n    # build model ( total layers = stack_n * 3 * 2 + 2 )\r\n    # stack_n = 5 by default, total layers = 32\r\n    # input: 32x32x3 output: 32x32x16\r\n    x = Conv2D(filters=16,kernel_size=(3,3),strides=(1,1),padding=\'same\',\r\n               kernel_initializer=""he_normal"",\r\n               kernel_regularizer=regularizers.l2(weight_decay))(img_input)\r\n\r\n    # input: 32x32x16 output: 32x32x16\r\n    for _ in range(stack_n):\r\n        x = residual_block(x,16,False)\r\n\r\n    # input: 32x32x16 output: 16x16x32\r\n    x = residual_block(x,32,True)\r\n    for _ in range(1,stack_n):\r\n        x = residual_block(x,32,False)\r\n    \r\n    # input: 16x16x32 output: 8x8x64\r\n    x = residual_block(x,64,True)\r\n    for _ in range(1,stack_n):\r\n        x = residual_block(x,64,False)\r\n\r\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\r\n    x = Activation(\'relu\')(x)\r\n    x = GlobalAveragePooling2D()(x)\r\n\r\n    # input: 64 output: 10\r\n    x = Dense(classes_num,activation=\'softmax\',kernel_initializer=""he_normal"",\r\n              kernel_regularizer=regularizers.l2(weight_decay))(x)\r\n    return x\r\n\r\n\r\nif __name__ == \'__main__\':\r\n\r\n\r\n    print(""========================================"") \r\n    print(""MODEL: Residual Network ({:2d} layers)"".format(6*stack_n+2)) \r\n    print(""BATCH SIZE: {:3d}"".format(batch_size)) \r\n    print(""WEIGHT DECAY: {:.4f}"".format(weight_decay))\r\n    print(""EPOCHS: {:3d}"".format(epochs))\r\n    print(""DATASET: {:}"".format(args.dataset))\r\n\r\n\r\n    print(""== LOADING DATA... =="")\r\n    # load data\r\n    global num_classes\r\n    if args.dataset == ""cifar100"":\r\n        num_classes = 100\r\n        (x_train, y_train), (x_test, y_test) = cifar100.load_data()\r\n    else:\r\n        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n    y_train = keras.utils.to_categorical(y_train, num_classes)\r\n    y_test = keras.utils.to_categorical(y_test, num_classes)\r\n    \r\n\r\n    print(""== DONE! ==\\n== COLOR PREPROCESSING... =="")\r\n    # color preprocessing\r\n    x_train, x_test = color_preprocessing(x_train, x_test)\r\n\r\n\r\n    print(""== DONE! ==\\n== BUILD MODEL... =="")\r\n    # build network\r\n    img_input = Input(shape=(img_rows,img_cols,img_channels))\r\n    output    = residual_network(img_input,num_classes,stack_n)\r\n    resnet    = Model(img_input, output)\r\n    \r\n    # print model architecture if you need.\r\n    # print(resnet.summary())\r\n\r\n\r\n    # set optimizer\r\n    sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\r\n    resnet.compile(loss=\'categorical_crossentropy\', optimizer=sgd, metrics=[\'accuracy\'])\r\n\r\n    # set callback\r\n    cbks = [TensorBoard(log_dir=\'./resnet_{:d}_{}/\'.format(layers,args.dataset), histogram_freq=0),\r\n            LearningRateScheduler(scheduler)]\r\n    \r\n    # dump checkpoint if you need.(add it to cbks)\r\n    # ModelCheckpoint(\'./checkpoint-{epoch}.h5\', save_best_only=False, mode=\'auto\', period=10)\r\n\r\n    # set data augmentation\r\n    print(""== USING REAL-TIME DATA AUGMENTATION, START TRAIN... =="")\r\n    datagen = ImageDataGenerator(horizontal_flip=True,\r\n                                 width_shift_range=0.125,\r\n                                 height_shift_range=0.125,\r\n                                 fill_mode=\'constant\',cval=0.)\r\n\r\n    datagen.fit(x_train)\r\n\r\n    # start training\r\n    resnet.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\r\n                         steps_per_epoch=iterations,\r\n                         epochs=epochs,\r\n                         callbacks=cbks,\r\n                         validation_data=(x_test, y_test))\r\n    resnet.save(\'resnet_{:d}_{}.h5\'.format(layers,args.dataset))\r\n'"
5_Wide_Residual_Network/Wide_ResNet_keras.py,2,"b""import keras\r\nimport numpy as np\r\nfrom keras.datasets import cifar10\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.layers.normalization import BatchNormalization\r\nfrom keras.layers import Conv2D, Dense, Input, add, Activation, Flatten, AveragePooling2D\r\nfrom keras.callbacks import LearningRateScheduler, TensorBoard\r\nfrom keras.regularizers import l2\r\nfrom keras import optimizers\r\nfrom keras.models import Model\r\n\r\nDEPTH              = 28\r\nWIDE               = 10\r\nIN_FILTERS         = 16\r\n\r\nCLASS_NUM          = 10\r\nIMG_ROWS, IMG_COLS = 32, 32\r\nIMG_CHANNELS       = 3\r\n\r\nBATCH_SIZE         = 128\r\nEPOCHS             = 200\r\nITERATIONS         = 50000 // BATCH_SIZE + 1\r\nWEIGHT_DECAY       = 0.0005\r\nLOG_FILE_PATH      = './w_resnet/'\r\n\r\n\r\nfrom keras import backend as K\r\n# set GPU memory \r\nif('tensorflow' == K.backend()):\r\n    import tensorflow as tf\r\n    from keras.backend.tensorflow_backend import set_session\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    sess = tf.Session(config=config)\r\n\r\ndef scheduler(epoch):\r\n    if epoch < 60:\r\n        return 0.1\r\n    if epoch < 120:\r\n        return 0.02\r\n    if epoch < 160:\r\n        return 0.004\r\n    return 0.0008\r\n\r\ndef color_preprocessing(x_train,x_test):\r\n    x_train = x_train.astype('float32')\r\n    x_test = x_test.astype('float32')\r\n    mean = [125.3, 123.0, 113.9]\r\n    std  = [63.0,  62.1,  66.7]\r\n    for i in range(3):\r\n        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\r\n        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\r\n\r\n    return x_train, x_test\r\n\r\ndef wide_residual_network(img_input,classes_num,depth,k):\r\n    print('Wide-Resnet %dx%d' %(depth, k))\r\n    n_filters  = [16, 16*k, 32*k, 64*k]\r\n    n_stack    = (depth - 4) // 6\r\n\r\n    def conv3x3(x,filters):\r\n        return Conv2D(filters=filters, kernel_size=(3,3), strides=(1,1), padding='same',\r\n        kernel_initializer='he_normal',\r\n        kernel_regularizer=l2(WEIGHT_DECAY),\r\n        use_bias=False)(x)\r\n\r\n    def bn_relu(x):\r\n        x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\r\n        x = Activation('relu')(x)\r\n        return x\r\n\r\n    def residual_block(x,out_filters,increase=False):\r\n        global IN_FILTERS\r\n        stride = (1,1)\r\n        if increase:\r\n            stride = (2,2)\r\n            \r\n        o1 = bn_relu(x)\r\n        \r\n        conv_1 = Conv2D(out_filters,\r\n            kernel_size=(3,3),strides=stride,padding='same',\r\n            kernel_initializer='he_normal',\r\n            kernel_regularizer=l2(WEIGHT_DECAY),\r\n            use_bias=False)(o1)\r\n\r\n        o2 = bn_relu(conv_1)\r\n        \r\n        conv_2 = Conv2D(out_filters, \r\n            kernel_size=(3,3), strides=(1,1), padding='same',\r\n            kernel_initializer='he_normal',\r\n            kernel_regularizer=l2(WEIGHT_DECAY),\r\n            use_bias=False)(o2)\r\n        if increase or IN_FILTERS != out_filters:\r\n            proj = Conv2D(out_filters,\r\n                                kernel_size=(1,1),strides=stride,padding='same',\r\n                                kernel_initializer='he_normal',\r\n                                kernel_regularizer=l2(WEIGHT_DECAY),\r\n                                use_bias=False)(o1)\r\n            block = add([conv_2, proj])\r\n        else:\r\n            block = add([conv_2,x])\r\n        return block\r\n\r\n    def wide_residual_layer(x,out_filters,increase=False):\r\n        global IN_FILTERS\r\n        x = residual_block(x,out_filters,increase)\r\n        IN_FILTERS = out_filters\r\n        for _ in range(1,int(n_stack)):\r\n            x = residual_block(x,out_filters)\r\n        return x\r\n\r\n\r\n    x = conv3x3(img_input,n_filters[0])\r\n    x = wide_residual_layer(x,n_filters[1])\r\n    x = wide_residual_layer(x,n_filters[2],increase=True)\r\n    x = wide_residual_layer(x,n_filters[3],increase=True)\r\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\r\n    x = Activation('relu')(x)\r\n    x = AveragePooling2D((8,8))(x)\r\n    x = Flatten()(x)\r\n    x = Dense(classes_num,\r\n        activation='softmax',\r\n        kernel_initializer='he_normal',\r\n        kernel_regularizer=l2(WEIGHT_DECAY),\r\n        use_bias=False)(x)\r\n    return x\r\n\r\nif __name__ == '__main__':\r\n\r\n    # load data\r\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n    y_train = keras.utils.to_categorical(y_train, CLASS_NUM)\r\n    y_test = keras.utils.to_categorical(y_test, CLASS_NUM)\r\n    \r\n    # color preprocessing\r\n    x_train, x_test = color_preprocessing(x_train, x_test)\r\n\r\n    # build network\r\n    img_input = Input(shape=(IMG_ROWS,IMG_COLS,IMG_CHANNELS))\r\n    output = wide_residual_network(img_input,CLASS_NUM,DEPTH,WIDE)\r\n    resnet = Model(img_input, output)\r\n    print(resnet.summary())\r\n    # set optimizer\r\n    sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\r\n    resnet.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\r\n\r\n    # set callback\r\n    tb_cb = TensorBoard(log_dir=LOG_FILE_PATH, histogram_freq=0)\r\n    change_lr = LearningRateScheduler(scheduler)\r\n    cbks = [change_lr,tb_cb]\r\n\r\n    # set data augmentation\r\n    print('Using real-time data augmentation.')\r\n    datagen = ImageDataGenerator(horizontal_flip=True,\r\n            width_shift_range=0.125,height_shift_range=0.125,fill_mode='reflect')\r\n\r\n    datagen.fit(x_train)\r\n\r\n    # start training\r\n    resnet.fit_generator(datagen.flow(x_train, y_train,batch_size=BATCH_SIZE),\r\n                        steps_per_epoch=ITERATIONS,\r\n                        epochs=EPOCHS,\r\n                        callbacks=cbks,\r\n                        validation_data=(x_test, y_test))\r\n    resnet.save('wresnet.h5')\r\n"""
6_ResNeXt/ResNeXt_keras.py,2,"b'import keras\r\nimport math\r\nimport numpy as np\r\nfrom keras import optimizers\r\nfrom keras.datasets import cifar10\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.layers.normalization import BatchNormalization\r\nfrom keras.layers import Conv2D, Dense, Input, add, Activation, GlobalAveragePooling2D\r\nfrom keras.layers import Lambda, concatenate\r\nfrom keras.initializers import he_normal\r\nfrom keras.callbacks import LearningRateScheduler, TensorBoard, ModelCheckpoint\r\nfrom keras.models import Model\r\nfrom keras.regularizers import l2\r\n\r\nCARDINALITY        = 8            # 4 or 8 or 16\r\nBASE_WIDTH         = 64\r\nIN_PLANES          = 64\r\n\r\nIMG_ROWS, IMG_COLS = 32, 32\r\nIMG_CHANNELS       = 3\r\nCLASS_NUM          = 10\r\nBATCH_SIZE         = 64           # 32 or 64 or 128\r\nepochs             = 300\r\nITERATIONS         = 50000 // BATCH_SIZE + 1\r\nWEIGHT_DECAY       = 5e-4\r\n\r\nmean = [125.3, 123.0, 113.9]\r\nstd  = [63.0,  62.1,  66.7]\r\n\r\n\r\nfrom keras import backend as K\r\nif(\'tensorflow\' == K.backend()):\r\n    import tensorflow as tf\r\n    from keras.backend.tensorflow_backend import set_session\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    sess = tf.Session(config=config)\r\n\r\ndef scheduler(epoch):\r\n    if epoch < 150:\r\n        return 0.1\r\n    if epoch < 225:\r\n        return 0.01\r\n    return 0.001\r\n\r\ndef resnext(img_input,classes_num):\r\n    global IN_PLANES\r\n    def bn_relu(x):\r\n        x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\r\n        x = Activation(\'relu\')(x)\r\n        return x\r\n\r\n    def group_conv(x,planes,stride):\r\n        h = planes // CARDINALITY\r\n        groups = []\r\n        for i in range(CARDINALITY):\r\n            group = Lambda(lambda z: z[:,:,:, i * h : i * h + h])(x)\r\n            groups.append(Conv2D(h,kernel_size=(3,3),strides=stride,kernel_initializer=""he_normal"",\r\n                kernel_regularizer=l2(WEIGHT_DECAY),\r\n                padding=\'same\',use_bias=False)(group))\r\n        x = concatenate(groups)\r\n        return x\r\n\r\n    def residual_block(x,planes,stride=(1,1)):\r\n\r\n        D = int(math.floor(planes * (BASE_WIDTH/64.0)))\r\n        C = CARDINALITY\r\n\r\n        shortcut = x\r\n        \r\n        y = Conv2D(D*C,kernel_size=(1,1),strides=(1,1),padding=\'same\',kernel_initializer=""he_normal"",kernel_regularizer=l2(WEIGHT_DECAY),use_bias=False)(shortcut)\r\n        y = bn_relu(y)\r\n\r\n        y = group_conv(y,D*C,stride)\r\n        y = bn_relu(y)\r\n\r\n        y = Conv2D(planes * 4, kernel_size=(1,1), strides=(1,1), padding=\'same\', kernel_initializer=""he_normal"",kernel_regularizer=l2(WEIGHT_DECAY),use_bias=False)(y)\r\n        y = bn_relu(y)\r\n\r\n        if stride != (1,1) or IN_PLANES != planes * 4:\r\n            shortcut = Conv2D(planes * 4, kernel_size=(1,1), strides=stride, padding=\'same\', kernel_initializer=""he_normal"",kernel_regularizer=l2(WEIGHT_DECAY),use_bias=False)(x)\r\n            shortcut = BatchNormalization(momentum=0.9, epsilon=1e-5)(shortcut)\r\n        \r\n        y = add([y,shortcut])\r\n        y = Activation(\'relu\')(y)\r\n        return y\r\n    \r\n    def residual_layer(x, blocks, planes, stride=(1,1)):\r\n        x = residual_block(x, planes, stride)\r\n        IN_PLANES = planes * 4\r\n        for i in range(1,blocks):\r\n            x = residual_block(x,planes)\r\n        return x\r\n    \r\n    def conv3x3(x,filters):\r\n        x = Conv2D(filters=filters, kernel_size=(3,3), strides=(1,1), padding=\'same\',kernel_initializer=""he_normal"",kernel_regularizer=l2(WEIGHT_DECAY),use_bias=False)(x)\r\n        x = bn_relu(x)\r\n        return x\r\n\r\n    def dense_layer(x):\r\n        return Dense(classes_num,activation=\'softmax\',kernel_initializer=""he_normal"",kernel_regularizer=l2(WEIGHT_DECAY),use_bias=False)(x)\r\n\r\n\r\n    # build the resnext model    \r\n    x = conv3x3(img_input,64)\r\n    x = residual_layer(x, 3, 64)\r\n    x = residual_layer(x, 3, 128,stride=(2,2))\r\n    x = residual_layer(x, 3, 256,stride=(2,2))\r\n    x = GlobalAveragePooling2D()(x)\r\n    x = dense_layer(x)\r\n    return x\r\n\r\nif __name__ == \'__main__\':\r\n\r\n    # load data\r\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n    y_train = keras.utils.to_categorical(y_train, CLASS_NUM)\r\n    y_test  = keras.utils.to_categorical(y_test, CLASS_NUM)\r\n    x_train = x_train.astype(\'float32\')\r\n    x_test  = x_test.astype(\'float32\')\r\n    \r\n    # - mean / std\r\n    for i in range(3):\r\n        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\r\n        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\r\n\r\n    # build network\r\n    img_input = Input(shape=(IMG_ROWS,IMG_COLS,IMG_CHANNELS))\r\n    output    = resnext(img_input,CLASS_NUM)\r\n    resnet    = Model(img_input, output)\r\n\r\n    print(resnet.summary())\r\n\r\n    # set optimizer\r\n    sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\r\n    resnet.compile(loss=\'categorical_crossentropy\', optimizer=sgd, metrics=[\'accuracy\'])\r\n\r\n    # set callback\r\n    tb_cb     = TensorBoard(log_dir=\'./resnext/\', histogram_freq=0)\r\n    change_lr = LearningRateScheduler(scheduler)\r\n    ckpt      = ModelCheckpoint(\'./ckpt.h5\', save_best_only=False, mode=\'auto\', period=25)\r\n    cbks      = [change_lr,tb_cb,ckpt]\r\n\r\n    # set data augmentation\r\n    print(\'Using real-time data augmentation.\')\r\n    datagen   = ImageDataGenerator(horizontal_flip=True,width_shift_range=0.125,height_shift_range=0.125,fill_mode=\'constant\',cval=0.)\r\n\r\n    datagen.fit(x_train)\r\n\r\n    # start training\r\n    resnet.fit_generator(datagen.flow(x_train, y_train,batch_size=BATCH_SIZE), steps_per_epoch=ITERATIONS, epochs=epochs, callbacks=cbks,validation_data=(x_test, y_test))\r\n    resnet.save(\'resnext.h5\')\r\n'"
7_DenseNet/DenseNet_keras.py,2,"b""import keras\r\nimport math\r\nimport numpy as np\r\nfrom keras.datasets import cifar10\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.layers.normalization import BatchNormalization\r\nfrom keras.layers import Conv2D, Dense, Input, add, Activation, AveragePooling2D, GlobalAveragePooling2D, Lambda, concatenate\r\nfrom keras.initializers import he_normal\r\nfrom keras.layers.merge import Concatenate\r\nfrom keras.callbacks import LearningRateScheduler, TensorBoard, ModelCheckpoint\r\nfrom keras.models import Model\r\nfrom keras import optimizers, regularizers\r\n\r\ngrowth_rate        = 12 \r\ndepth              = 100\r\ncompression        = 0.5\r\n\r\nimg_rows, img_cols = 32, 32\r\nimg_channels       = 3\r\nnum_classes        = 10\r\nbatch_size         = 64         # 64 or 32 or other\r\nepochs             = 300\r\niterations         = 782       \r\nweight_decay       = 1e-4\r\n\r\nmean = [125.307, 122.95, 113.865]\r\nstd  = [62.9932, 62.0887, 66.7048]\r\n\r\nfrom keras import backend as K\r\nif('tensorflow' == K.backend()):\r\n    import tensorflow as tf\r\n    from keras.backend.tensorflow_backend import set_session\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    sess = tf.Session(config=config)\r\n\r\ndef scheduler(epoch):\r\n    if epoch < 150:\r\n        return 0.1\r\n    if epoch < 225:\r\n        return 0.01\r\n    return 0.001\r\n\r\ndef densenet(img_input,classes_num):\r\n    def conv(x, out_filters, k_size):\r\n        return Conv2D(filters=out_filters,\r\n                      kernel_size=k_size,\r\n                      strides=(1,1),\r\n                      padding='same',\r\n                      kernel_initializer='he_normal',\r\n                      kernel_regularizer=regularizers.l2(weight_decay),\r\n                      use_bias=False)(x)\r\n\r\n    def dense_layer(x):\r\n        return Dense(units=classes_num,\r\n                     activation='softmax',\r\n                     kernel_initializer='he_normal',\r\n                     kernel_regularizer=regularizers.l2(weight_decay))(x)\r\n\r\n    def bn_relu(x):\r\n        x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\r\n        x = Activation('relu')(x)\r\n        return x\r\n\r\n    def bottleneck(x):\r\n        channels = growth_rate * 4\r\n        x = bn_relu(x)\r\n        x = conv(x, channels, (1,1))\r\n        x = bn_relu(x)\r\n        x = conv(x, growth_rate, (3,3))\r\n        return x\r\n\r\n    def single(x):\r\n        x = bn_relu(x)\r\n        x = conv(x, growth_rate, (3,3))\r\n        return x\r\n\r\n    def transition(x, inchannels):\r\n        outchannels = int(inchannels * compression)\r\n        x = bn_relu(x)\r\n        x = conv(x, outchannels, (1,1))\r\n        x = AveragePooling2D((2,2), strides=(2, 2))(x)\r\n        return x, outchannels\r\n\r\n    def dense_block(x,blocks,nchannels):\r\n        concat = x\r\n        for i in range(blocks):\r\n            x = bottleneck(concat)\r\n            concat = concatenate([x,concat], axis=-1)\r\n            nchannels += growth_rate\r\n        return concat, nchannels\r\n\r\n\r\n    nblocks = (depth - 4) // 6 \r\n    nchannels = growth_rate * 2\r\n\r\n\r\n    x = conv(img_input, nchannels, (3,3))\r\n    x, nchannels = dense_block(x,nblocks,nchannels)\r\n    x, nchannels = transition(x,nchannels)\r\n    x, nchannels = dense_block(x,nblocks,nchannels)\r\n    x, nchannels = transition(x,nchannels)\r\n    x, nchannels = dense_block(x,nblocks,nchannels)\r\n    x = bn_relu(x)\r\n    x = GlobalAveragePooling2D()(x)\r\n    x = dense_layer(x)\r\n    return x\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    # load data\r\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n    y_train = keras.utils.to_categorical(y_train, num_classes)\r\n    y_test  = keras.utils.to_categorical(y_test, num_classes)\r\n    x_train = x_train.astype('float32')\r\n    x_test  = x_test.astype('float32')\r\n    \r\n    # - mean / std\r\n    for i in range(3):\r\n        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\r\n        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\r\n\r\n    # build network\r\n    img_input = Input(shape=(img_rows,img_cols,img_channels))\r\n    output    = densenet(img_input,num_classes)\r\n    model     = Model(img_input, output)\r\n    \r\n    # model.load_weights('ckpt.h5')\r\n\r\n    print(model.summary())\r\n\r\n    # from keras.utils import plot_model\r\n    # plot_model(model, show_shapes=True, to_file='model.png')\r\n\r\n    # set optimizer\r\n    sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\r\n    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\r\n\r\n    # set callback\r\n    tb_cb     = TensorBoard(log_dir='./densenet/', histogram_freq=0)\r\n    change_lr = LearningRateScheduler(scheduler)\r\n    ckpt      = ModelCheckpoint('./ckpt.h5', save_best_only=False, mode='auto', period=10)\r\n    cbks      = [change_lr,tb_cb,ckpt]\r\n\r\n    # set data augmentation\r\n    print('Using real-time data augmentation.')\r\n    datagen   = ImageDataGenerator(horizontal_flip=True,width_shift_range=0.125,height_shift_range=0.125,fill_mode='constant',cval=0.)\r\n\r\n    datagen.fit(x_train)\r\n\r\n    # start training\r\n    model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size), steps_per_epoch=iterations, epochs=epochs, callbacks=cbks,validation_data=(x_test, y_test))\r\n    model.save('densenet.h5')\r\n"""
8_SENet/SENet_Keras.py,2,"b'import keras\nimport math\nimport numpy as np\nfrom keras.datasets import cifar10\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers import Conv2D, Dense, Input, add, Activation, GlobalAveragePooling2D, multiply, Reshape\nfrom keras.layers import Lambda, concatenate\nfrom keras.initializers import he_normal\nfrom keras.callbacks import LearningRateScheduler, TensorBoard, ModelCheckpoint\nfrom keras.models import Model\nfrom keras import optimizers\nfrom keras import regularizers\n\ncardinality        = 4          # 4 or 8 or 16 or 32\nbase_width         = 64\ninplanes           = 64\nexpansion          = 4\n\nimg_rows, img_cols = 32, 32     \nimg_channels       = 3\nnum_classes        = 10\nbatch_size         = 64  # 120       \niterations         = 781 # 416       # total data / iterations = batch size\nepochs             = 250\nweight_decay       = 0.0005\n\nmean = [125.307, 122.95, 113.865]\nstd  = [62.9932, 62.0887, 66.7048]\n\nfrom keras import backend as K\nif(\'tensorflow\' == K.backend()):\n    import tensorflow as tf\n    from keras.backend.tensorflow_backend import set_session\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    sess = tf.Session(config=config)\n\n    \ndef scheduler(epoch):\n    if epoch < 150:\n        return 0.1\n    if epoch < 225:\n        return 0.01\n    return 0.001\n\ndef resnext(img_input,classes_num):\n    global inplanes\n    def add_common_layer(x):\n        x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n        x = Activation(\'relu\')(x)\n        return x\n\n    def group_conv(x,planes,stride):\n        h = planes // cardinality\n        groups = []\n        for i in range(cardinality):\n            group = Lambda(lambda z: z[:,:,:, i * h : i * h + h])(x)\n            groups.append(Conv2D(h,kernel_size=(3,3),strides=stride,kernel_initializer=he_normal(),kernel_regularizer=regularizers.l2(weight_decay),padding=\'same\',use_bias=False)(group))\n        x = concatenate(groups)\n        return x\n\n    def residual_block(x,planes,stride=(1,1)):\n\n        D = int(math.floor(planes * (base_width/64.0)))\n        C = cardinality\n\n        shortcut = x\n        \n        y = Conv2D(D*C,kernel_size=(1,1),strides=(1,1),padding=\'same\',kernel_initializer=he_normal(),kernel_regularizer=regularizers.l2(weight_decay),use_bias=False)(shortcut)\n        y = add_common_layer(y)\n\n        y = group_conv(y,D*C,stride)\n        y = add_common_layer(y)\n\n        y = Conv2D(planes*expansion, kernel_size=(1,1), strides=(1,1), padding=\'same\', kernel_initializer=he_normal(),kernel_regularizer=regularizers.l2(weight_decay),use_bias=False)(y)\n        y = add_common_layer(y)\n\n        if stride != (1,1) or inplanes != planes * expansion:\n            shortcut = Conv2D(planes * expansion, kernel_size=(1,1), strides=stride, padding=\'same\', kernel_initializer=he_normal(),kernel_regularizer=regularizers.l2(weight_decay),use_bias=False)(x)\n            shortcut = BatchNormalization(momentum=0.9, epsilon=1e-5)(shortcut)\n\n        y = squeeze_excite_block(y)\n\n        y = add([y,shortcut])\n        y = Activation(\'relu\')(y)\n        return y\n    \n    def residual_layer(x, blocks, planes, stride=(1,1)):\n        x = residual_block(x, planes, stride)\n        inplanes = planes * expansion\n        for i in range(1,blocks):\n            x = residual_block(x,planes)\n        return x\n\n    def squeeze_excite_block(input, ratio=16):\n        init = input\n        channel_axis = 1 if K.image_data_format() == ""channels_first"" else -1  # compute channel axis\n        filters = init._keras_shape[channel_axis]  # infer input number of filters\n        se_shape = (1, 1, filters) if K.image_data_format() == \'channels_last\' else (filters, 1, 1)  # determine Dense matrix shape\n\n        se = GlobalAveragePooling2D()(init)\n        se = Reshape(se_shape)(se)\n        se = Dense(filters // ratio, activation=\'relu\', kernel_initializer=\'he_normal\', kernel_regularizer=regularizers.l2(weight_decay), use_bias=False)(se)\n        se = Dense(filters, activation=\'sigmoid\', kernel_initializer=\'he_normal\', kernel_regularizer=regularizers.l2(weight_decay), use_bias=False)(se)\n        x = multiply([init, se])\n        return x\n\n    def conv3x3(x,filters):\n        x = Conv2D(filters=filters, kernel_size=(3,3), strides=(1,1), padding=\'same\',kernel_initializer=he_normal(),kernel_regularizer=regularizers.l2(weight_decay),use_bias=False)(x)\n        return add_common_layer(x)\n\n    def dense_layer(x):\n        return Dense(classes_num,activation=\'softmax\',kernel_initializer=he_normal(),kernel_regularizer=regularizers.l2(weight_decay))(x)\n\n\n    # build the resnext model    \n    x = conv3x3(img_input,64)\n    x = residual_layer(x, 3, 64)\n    x = residual_layer(x, 3, 128,stride=(2,2))\n    x = residual_layer(x, 3, 256,stride=(2,2))\n    x = GlobalAveragePooling2D()(x)\n    x = dense_layer(x)\n    return x\n\nif __name__ == \'__main__\':\n\n    # load data\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n    y_train = keras.utils.to_categorical(y_train, num_classes)\n    y_test  = keras.utils.to_categorical(y_test, num_classes)\n    x_train = x_train.astype(\'float32\')\n    x_test  = x_test.astype(\'float32\')\n    \n    # - mean / std\n    for i in range(3):\n        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n\n    # build network\n    img_input = Input(shape=(img_rows,img_cols,img_channels))\n    output    = resnext(img_input,num_classes)\n    senet    = Model(img_input, output)\n    print(senet.summary())\n\n    # load weight\n    # senet.load_weights(\'senet.h5\')\n\n    # set optimizer\n    sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n    senet.compile(loss=\'categorical_crossentropy\', optimizer=sgd, metrics=[\'accuracy\'])\n\n    # set callback\n    tb_cb     = TensorBoard(log_dir=\'./senet/\', histogram_freq=0)                                   # tensorboard log\n    change_lr = LearningRateScheduler(scheduler)                                                    # learning rate scheduler\n    ckpt      = ModelCheckpoint(\'./ckpt_senet.h5\', save_best_only=False, mode=\'auto\', period=10)    # checkpoint \n    cbks      = [change_lr,tb_cb,ckpt]                   \n\n    # set data augmentation\n    print(\'Using real-time data augmentation.\')\n    datagen   = ImageDataGenerator(horizontal_flip=True,width_shift_range=0.125,height_shift_range=0.125,fill_mode=\'constant\',cval=0.)\n\n    datagen.fit(x_train)\n\n    # start training\n    senet.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size), steps_per_epoch=iterations, epochs=epochs, callbacks=cbks,validation_data=(x_test, y_test))\n    senet.save(\'senet.h5\')\n'"
9_Multi-GPU/densenet_multi_gpu.py,5,"b""import keras\nimport math\nimport numpy as np\nfrom keras.datasets import cifar10\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers import Conv2D, Dense, Input, add, Activation, AveragePooling2D, GlobalAveragePooling2D\nfrom keras.layers import Lambda, concatenate\nfrom keras.initializers import he_normal\nfrom keras.layers.merge import Concatenate\nfrom keras.callbacks import LearningRateScheduler, TensorBoard, ModelCheckpoint\nfrom keras.models import Model\nfrom keras import optimizers\nfrom keras import regularizers\nfrom keras.utils import plot_model\n\nimport tensorflow as tf\nfrom keras.models import *\nfrom keras.layers import merge, Lambda\nfrom keras import backend as K\n\n\n# change the following para according to your GPUs.\ngpu_number         = 2\nbatch_size         = 64         # 64 or 32 or smaller\nepochs             = 250        \niterations         = 782       \n\ngrowth_rate        = 24 \ndepth              = 164\ncompression        = 0.5\n\nimg_rows, img_cols = 32, 32\nimg_channels       = 3\nnum_classes        = 10\nweight_decay       = 0.0001\n\nmean = [125.307, 122.95, 113.865]\nstd  = [62.9932, 62.0887, 66.7048]\n\n# set GPU memory \nif('tensorflow' == K.backend()):\n    from keras.backend.tensorflow_backend import set_session\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    sess = tf.Session(config=config)\n\ndef slice_batch(x, n_gpus, part):\n    sh = K.shape(x)\n    L =  sh[0] // n_gpus\n    if part == n_gpus - 1:\n        return x[part*L:]\n    return x[part*L:(part+1)*L]\n\n\ndef to_multi_gpu(model, n_gpus=2):\n    if n_gpus ==1:\n        return model\n    \n    with tf.device('/cpu:0'):\n        x = Input(model.input_shape[1:])\n    towers = []\n    for g in range(n_gpus):\n        with tf.device('/gpu:' + str(g)):\n            slice_g = Lambda(slice_batch, lambda shape: shape, arguments={'n_gpus':n_gpus, 'part':g})(x)\n            towers.append(model(slice_g))\n\n    with tf.device('/cpu:0'):\n        merged = Concatenate(axis=0)(towers)\n    return Model(inputs=[x], outputs=merged)\n\ndef scheduler(epoch):\n    if epoch <= 75:\n        return 0.1\n    if epoch <= 150:\n        return 0.01\n    if epoch <= 210:\n        return 0.001\n    return 0.0005\n\ndef densenet(img_input,classes_num):\n\n    def bn_relu(x):\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n        return x\n\n    def bottleneck(x):\n        channels = growth_rate * 4\n        x = bn_relu(x)\n        x = Conv2D(channels,kernel_size=(1,1),strides=(1,1),padding='same',kernel_initializer=he_normal(),kernel_regularizer=regularizers.l2(weight_decay),use_bias=False)(x)\n        x = bn_relu(x)\n        x = Conv2D(growth_rate,kernel_size=(3,3),strides=(1,1),padding='same',kernel_initializer=he_normal(),kernel_regularizer=regularizers.l2(weight_decay),use_bias=False)(x)\n        return x\n\n    def single(x):\n        x = bn_relu(x)\n        x = Conv2D(growth_rate,kernel_size=(3,3),strides=(1,1),padding='same',kernel_initializer=he_normal(),kernel_regularizer=regularizers.l2(weight_decay),use_bias=False)(x)\n        return x\n\n    def transition(x, inchannels):\n        outchannels = int(inchannels * compression)\n        x = bn_relu(x)\n        x = Conv2D(outchannels,kernel_size=(1,1),strides=(1,1),padding='same',kernel_initializer=he_normal(),kernel_regularizer=regularizers.l2(weight_decay),use_bias=False)(x)\n        x = AveragePooling2D((2,2), strides=(2, 2))(x)\n        return x, outchannels\n\n    def dense_block(x,blocks,nchannels):\n        concat = x\n        for i in range(blocks):\n            x = bottleneck(concat)\n            concat = concatenate([x,concat], axis=-1)\n            nchannels += growth_rate\n        return concat, nchannels\n\n    def dense_layer(x):\n        return Dense(classes_num,activation='softmax',kernel_initializer=he_normal(),kernel_regularizer=regularizers.l2(weight_decay))(x)\n\n\n    nblocks = (depth - 4) // 6 \n    nchannels = growth_rate * 2\n\n    x = Conv2D(nchannels,kernel_size=(3,3),strides=(1,1),padding='same',kernel_initializer=he_normal(),kernel_regularizer=regularizers.l2(weight_decay),use_bias=False)(img_input)\n\n    x, nchannels = dense_block(x,nblocks,nchannels)\n    x, nchannels = transition(x,nchannels)\n    x, nchannels = dense_block(x,nblocks,nchannels)\n    x, nchannels = transition(x,nchannels)\n    x, nchannels = dense_block(x,nblocks,nchannels)\n    x, nchannels = transition(x,nchannels)\n    x = bn_relu(x)\n    x = GlobalAveragePooling2D()(x)\n    x = dense_layer(x)\n    return x\n\n\nif __name__ == '__main__':\n\n    # load data\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n    y_train = keras.utils.to_categorical(y_train, num_classes)\n    y_test  = keras.utils.to_categorical(y_test, num_classes)\n    x_train = x_train.astype('float32')\n    x_test  = x_test.astype('float32')\n    \n    # - mean / std\n    for i in range(3):\n        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n\n    # build network\n    img_input = Input(shape=(img_rows,img_cols,img_channels))\n    output    = densenet(img_input,num_classes)\n    model     = Model(img_input, output)\n\n    \n    # -------------- Multi-GPU-------------------#\n    model     = to_multi_gpu(model,n_gpus=gpu_number) \n    # -------------- Multi-GPU-------------------#\n\n    # model.load_weights('ckpt.h5')\n    \n    # https://github.com/fchollet/keras/issues/3210\n    # plot_model(model, show_shapes=True, to_file='model.png')\n    \n    print(model.summary())\n\n    # set optimizer\n    sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\n    # set callback\n    tb_cb     = TensorBoard(log_dir='./densenet/', histogram_freq=0)\n    change_lr = LearningRateScheduler(scheduler)\n    ckpt      = ModelCheckpoint('./ckpt.h5', save_best_only=False, mode='auto', period=10)\n    cbks      = [change_lr,tb_cb,ckpt]\n\n    # set data augmentation\n    print('Using real-time data augmentation.')\n    datagen   = ImageDataGenerator(horizontal_flip=True,width_shift_range=0.125,height_shift_range=0.125,fill_mode='constant',cval=0.)\n\n    datagen.fit(x_train)\n\n    # start training\n    model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size), steps_per_epoch=iterations, epochs=epochs, callbacks=cbks,validation_data=(x_test, y_test))\n    model.save('densenet.h5')"""
Tensorflow_version/Network_in_Network.py,60,"b'# ===================================================================== #\r\n# File name:           Network_in_Network.py\r\n# Author:              BIGBALLON\r\n# Date update:         07/28/2017\r\n# Python Version:      3.5.2\r\n# Tensorflow Version:  1.2.1\r\n# Description: \r\n#     Implement Network in Network(only use tensorflow) \r\n#     Paper Link: (Network In Network) https://arxiv.org/abs/1312.4400\r\n#      Trick Used:\r\n#         Data augmentation parameters\r\n#         Color normalization\r\n#         Weight Decay\r\n#         Weight initialization\r\n#         Use Nesterov momentum\r\n# Dataset:             Cifar-10\r\n# Testing accuracy:    91.18% - 91.25%\r\n# ===================================================================== #\r\n\r\nimport tensorflow as tf\r\nfrom data_utility import *\r\n\r\nFLAGS = tf.app.flags.FLAGS\r\ntf.app.flags.DEFINE_string(\'log_save_path\', \'./nin_logs\', \'Directory where to save tensorboard log\')\r\ntf.app.flags.DEFINE_string(\'model_save_path\', \'./model/\', \'Directory where to save model weights\')\r\ntf.app.flags.DEFINE_integer(\'batch_size\', 128, \'batch size\')\r\ntf.app.flags.DEFINE_integer(\'iteration\', 391, \'iteration\')\r\ntf.app.flags.DEFINE_float(\'weight_decay\', 0.0001, \'weight decay\')\r\ntf.app.flags.DEFINE_float(\'dropout\', 0.5, \'dropout\')\r\ntf.app.flags.DEFINE_float(\'epochs\', 164, \'epochs\')\r\ntf.app.flags.DEFINE_float(\'momentum\', 0.9, \'momentum\')\r\n\r\n# ========================================================== #\r\n# \xe2\x94\x9c\xe2\x94\x80 conv()\r\n# \xe2\x94\x9c\xe2\x94\x80 activation(x)\r\n# \xe2\x94\x9c\xe2\x94\x80 max_pool()\r\n# \xe2\x94\x94\xe2\x94\x80 global_avg_pool()\r\n# ========================================================== #\r\n\r\ndef conv(x, shape, use_bias=True, std=0.05):\r\n    random_initializer = tf.random_normal_initializer(stddev=std)\r\n    W = tf.get_variable(\'weights\', shape=shape, initializer=random_initializer)\r\n    b = tf.get_variable(\'bias\', shape=[shape[3]], initializer=tf.zeros_initializer)\r\n    x = tf.nn.conv2d(x, W, strides=[1,1,1,1], padding=\'SAME\')\r\n    if use_bias:\r\n        x = tf.nn.bias_add(x,b)\r\n    return x\r\n\r\ndef activation(x):\r\n    return tf.nn.relu(x) \r\n\r\ndef max_pool(input, k_size=3, stride=2):\r\n    return tf.nn.max_pool(input, ksize=[1, k_size, k_size, 1], strides=[1, stride, stride, 1], padding=\'SAME\')\r\n\r\ndef global_avg_pool(input, k_size=1, stride=1):\r\n    return tf.nn.avg_pool(input, ksize=[1,k_size,k_size,1], strides=[1,stride,stride,1], padding=\'VALID\')\r\n\r\ndef learning_rate_schedule(epoch_num):\r\n      if epoch_num < 81:\r\n          return 0.05\r\n      elif epoch_num < 121:\r\n          return 0.01\r\n      else:\r\n          return 0.001\r\n\r\ndef main(_):\r\n    train_x, train_y, test_x, test_y = prepare_data()\r\n    train_x, test_x = color_preprocessing(train_x, test_x)\r\n\r\n    # define placeholder x, y_ , keep_prob, learning_rate\r\n    with tf.name_scope(\'input\'):\r\n        x  = tf.placeholder(tf.float32,[None, image_size, image_size, 3], name=\'input_x\')\r\n        y_ = tf.placeholder(tf.float32, [None, class_num], name=\'input_y\')\r\n    with tf.name_scope(\'keep_prob\'):\r\n        keep_prob = tf.placeholder(tf.float32)\r\n    with tf.name_scope(\'learning_rate\'):\r\n        learning_rate = tf.placeholder(tf.float32)\r\n\r\n    # build_network\r\n\r\n    with tf.variable_scope(\'conv1\'):\r\n        output = conv(x,[5, 5, 3, 192],std=0.01)\r\n        output = activation(output)\r\n\r\n    with tf.variable_scope(\'mlp1-1\'):\r\n        output = conv(output,[1, 1, 192, 160])\r\n        output = activation(output)\r\n\r\n    with tf.variable_scope(\'mlp1-2\'):\r\n        output = conv(output,[1, 1, 160, 96])\r\n        output = activation(output)\r\n\r\n    with tf.name_scope(\'max_pool-1\'):\r\n        output  = max_pool(output, 3, 2)\r\n\r\n    with tf.name_scope(\'dropout-1\'):\r\n        output = tf.nn.dropout(output,keep_prob)\r\n\r\n    with tf.variable_scope(\'conv2\'):\r\n        output = conv(output,[5, 5, 96, 192])\r\n        output = activation(output)\r\n\r\n    with tf.variable_scope(\'mlp2-1\'):\r\n        output = conv(output,[1, 1, 192, 192])\r\n        output = activation(output)\r\n\r\n    with tf.variable_scope(\'mlp2-2\'):\r\n        output = conv(output,[1, 1, 192, 192])\r\n        output = activation(output)\r\n\r\n    with tf.name_scope(\'max_pool-2\'):\r\n        output  = max_pool(output, 3, 2)\r\n\r\n    with tf.name_scope(\'dropout-2\'):\r\n        output = tf.nn.dropout(output,keep_prob)\r\n\r\n    with tf.variable_scope(\'conv3\'):\r\n        output = conv(output,[3, 3, 192, 192])\r\n        output = activation(output)\r\n\r\n    with tf.variable_scope(\'mlp3-1\'):\r\n        output = conv(output,[1, 1, 192, 192])\r\n        output = activation(output)\r\n\r\n    with tf.variable_scope(\'mlp3-2\'):\r\n        output = conv(output,[1, 1, 192, 10])\r\n        output = activation(output)\r\n\r\n    with tf.name_scope(\'global_avg_pool\'):\r\n        output  = global_avg_pool(output, 8, 1)\r\n\r\n    with tf.name_scope(\'moftmax\'):\r\n        output  = tf.reshape(output,[-1,10])\r\n\r\n    # loss function: cross_entropy\r\n    # weight decay: l2 * WEIGHT_DECAY\r\n    # train_step: training operation\r\n\r\n    with tf.name_scope(\'cross_entropy\'):\r\n        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=output))\r\n\r\n    with tf.name_scope(\'l2_loss\'):\r\n        l2 = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\r\n\r\n    with tf.name_scope(\'train_step\'):\r\n        train_step = tf.train.MomentumOptimizer(learning_rate, FLAGS.momentum,use_nesterov=True).minimize(cross_entropy + l2 * FLAGS.weight_decay)\r\n\r\n    with tf.name_scope(\'prediction\'):\r\n        correct_prediction = tf.equal(tf.argmax(output,1), tf.argmax(y_,1))\r\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\r\n\r\n    # initial an saver to save model\r\n    saver = tf.train.Saver()\r\n    \r\n    # for testing\r\n    def run_testing(sess):\r\n        acc = 0.0\r\n        loss = 0.0\r\n        pre_index = 0\r\n        add = 1000\r\n        for it in range(10):\r\n            batch_x = test_x[pre_index:pre_index+add]\r\n            batch_y = test_y[pre_index:pre_index+add]\r\n            pre_index = pre_index + add\r\n            loss_, acc_  = sess.run([cross_entropy,accuracy],feed_dict={x:batch_x, y_:batch_y, keep_prob: 1.0})\r\n            loss += loss_ / 10.0\r\n            acc += acc_ / 10.0\r\n        summary = tf.Summary(value=[tf.Summary.Value(tag=""test_loss"", simple_value=loss), \r\n                                tf.Summary.Value(tag=""test_accuracy"", simple_value=acc)])\r\n        return acc, loss, summary\r\n\r\n    with tf.Session() as sess:\r\n        \r\n        sess.run(tf.global_variables_initializer())\r\n        summary_writer = tf.summary.FileWriter(FLAGS.log_save_path,sess.graph)\r\n\r\n        # epoch = 164 \r\n        # batch size = 128\r\n        # iteration = 391\r\n        # we should make sure [bath_size * iteration = data_set_number]\r\n\r\n        for ep in range(1,FLAGS.epochs+1):\r\n            lr = learning_rate_schedule(ep)\r\n            pre_index = 0\r\n            train_acc = 0.0\r\n            train_loss = 0.0\r\n            start_time = time.time()\r\n\r\n            print(""\\nepoch %d/%d:"" %(ep,FLAGS.epochs))\r\n\r\n            for it in range(1,FLAGS.iteration+1):\r\n                if pre_index+FLAGS.batch_size < 50000:\r\n                    batch_x = train_x[pre_index:pre_index+FLAGS.batch_size]\r\n                    batch_y = train_y[pre_index:pre_index+FLAGS.batch_size]\r\n                else:\r\n                    batch_x = train_x[pre_index:]\r\n                    batch_y = train_y[pre_index:]\r\n\r\n\r\n                batch_x = data_augmentation(batch_x)\r\n\r\n                _, batch_loss = sess.run([train_step, cross_entropy],feed_dict={x:batch_x, y_:batch_y, keep_prob: FLAGS.dropout, learning_rate: lr})\r\n                batch_acc = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob: 1.0})\r\n\r\n                train_loss += batch_loss\r\n                train_acc  += batch_acc\r\n                pre_index  += FLAGS.batch_size\r\n\r\n                if it == FLAGS.iteration:\r\n                    train_loss /= FLAGS.iteration\r\n                    train_acc /= FLAGS.iteration\r\n                    \r\n                    train_summary = tf.Summary(value=[tf.Summary.Value(tag=""train_loss"", simple_value=train_loss), \r\n                                          tf.Summary.Value(tag=""train_accuracy"", simple_value=train_acc)])\r\n\r\n                    val_acc, val_loss, test_summary = run_testing(sess)\r\n\r\n                    summary_writer.add_summary(train_summary, ep)\r\n                    summary_writer.add_summary(test_summary, ep)\r\n                    summary_writer.flush()\r\n\r\n                    print(""iteration: %d/%d, cost_time: %ds, train_loss: %.4f, train_acc: %.4f, test_loss: %.4f, test_acc: %.4f"" %(it, FLAGS.iteration, int(time.time()-start_time), train_loss, train_acc, val_loss, val_acc))\r\n                else:\r\n                    print(""iteration: %d/%d, train_loss: %.4f, train_acc: %.4f"" %(it, FLAGS.iteration, train_loss / it, train_acc / it) , end=\'\\r\')\r\n\r\n        save_path = saver.save(sess, FLAGS.model_save_path)\r\n        print(""Model saved in file: %s"" % save_path)        \r\n\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    tf.app.run()\r\n\r\n    \r\n\r\n\r\n\r\n          \r\n'"
Tensorflow_version/Network_in_Network_bn.py,64,"b'import tensorflow as tf\r\nfrom data_utility import *\r\n\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\ntf.app.flags.DEFINE_string(\'log_save_path\', \'./nin_logs\', \'Directory where to save tensorboard log\')\r\ntf.app.flags.DEFINE_string(\'model_save_path\', \'./model/\', \'Directory where to save model weights\')\r\ntf.app.flags.DEFINE_integer(\'batch_size\', 128, \'batch size\')\r\ntf.app.flags.DEFINE_integer(\'iteration\', 391, \'iteration\')\r\ntf.app.flags.DEFINE_float(\'weight_decay\', 0.0001, \'weight decay\')\r\ntf.app.flags.DEFINE_float(\'dropout\', 0.5, \'dropout\')\r\ntf.app.flags.DEFINE_float(\'epochs\', 200, \'epochs\')\r\ntf.app.flags.DEFINE_float(\'momentum\', 0.9, \'momentum\')\r\n\r\ndef conv(x, phase, shape):\r\n    he_initializer = tf.contrib.keras.initializers.he_normal()\r\n    W = tf.get_variable(\'weights\', shape=shape, initializer=he_initializer)\r\n    b = tf.get_variable(\'bias\', shape=[shape[3]], initializer=tf.zeros_initializer)\r\n    x = tf.nn.conv2d(x, W, strides=[1,1,1,1], padding=\'SAME\')\r\n    x = tf.nn.bias_add(x,b)\r\n    # return tf.contrib.layers.batch_norm(x,is_training=phase)    \r\n    return tf.layers.batch_normalization(x,axis=-1,training=phase,name=""bn"")\r\n\r\ndef activation(x):\r\n    return tf.nn.relu(x) \r\n\r\ndef max_pool(input, k_size=3, stride=2):\r\n    return tf.nn.max_pool(input, ksize=[1, k_size, k_size, 1], strides=[1, stride, stride, 1], padding=\'SAME\')\r\n\r\ndef global_avg_pool(input, k_size=1, stride=1):\r\n    return tf.nn.avg_pool(input, ksize=[1,k_size,k_size,1], strides=[1,stride,stride,1], padding=\'VALID\')\r\n\r\ndef inference(x, phase, keep_prob):\r\n    with tf.variable_scope(\'conv1\'):\r\n        x = conv(x, phase, [5, 5, 3, 192])\r\n        x = activation(x)\r\n\r\n    with tf.variable_scope(\'mlp1-1\'):\r\n        x = conv(x, phase, [1, 1, 192, 160])\r\n        x = activation(x)\r\n\r\n    with tf.variable_scope(\'mlp1-2\'):\r\n        x = conv(x, phase, [1, 1, 160, 96])\r\n        x = activation(x)\r\n\r\n    with tf.name_scope(\'max_pool-1\'):\r\n        x  = max_pool(x, 3, 2)\r\n\r\n    with tf.name_scope(\'dropout-1\'):\r\n        x = tf.nn.dropout(x,keep_prob)\r\n\r\n    with tf.variable_scope(\'conv2\'):\r\n        x = conv(x, phase, [5, 5, 96, 192])\r\n        x = activation(x)\r\n\r\n    with tf.variable_scope(\'mlp2-1\'):\r\n        x = conv(x, phase, [1, 1, 192, 192])\r\n        x = activation(x)\r\n\r\n    with tf.variable_scope(\'mlp2-2\'):\r\n        x = conv(x, phase, [1, 1, 192, 192])\r\n        x = activation(x)\r\n\r\n    with tf.name_scope(\'max_pool-2\'):\r\n        x  = max_pool(x, 3, 2)\r\n\r\n    with tf.name_scope(\'dropout-2\'):\r\n        x = tf.nn.dropout(x,keep_prob)\r\n\r\n    with tf.variable_scope(\'conv3\'):\r\n        x = conv(x, phase, [3, 3, 192, 192])\r\n        x = activation(x)\r\n\r\n    with tf.variable_scope(\'mlp3-1\'):\r\n        x = conv(x, phase, [1, 1, 192, 192])\r\n        x = activation(x)\r\n\r\n    with tf.variable_scope(\'mlp3-2\'):\r\n        x = conv(x, phase, [1, 1, 192, 10])\r\n        x = activation(x)\r\n\r\n    with tf.name_scope(\'global_avg_pool\'):\r\n        x  = global_avg_pool(x, 8, 1)\r\n        output  = tf.reshape(x,[-1,10])\r\n\r\n    return output\r\n\r\ndef cal_loss(output, y_):\r\n    with tf.name_scope(\'cross_entropy\'):\r\n        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=output))\r\n\r\n    with tf.name_scope(\'l2_loss\'):\r\n        l2 = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\r\n    tf.summary.scalar(""train_loss"", cross_entropy)\r\n    return cross_entropy, l2\r\n\r\ndef training(cost, l2, lr):\r\n    with tf.name_scope(\'train_op\'):\r\n        optimizer = tf.train.MomentumOptimizer(lr, FLAGS.momentum,use_nesterov=True)\r\n        # extra update ops for batch normalization\r\n        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n        with tf.control_dependencies(extra_update_ops):\r\n            train_op = optimizer.minimize(cost + l2 * FLAGS.weight_decay)\r\n    return train_op\r\n\r\ndef evaluate(output,y_):\r\n    with tf.name_scope(\'prediction\'):\r\n        correct_prediction = tf.equal(tf.argmax(output,1), tf.argmax(y_,1))\r\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\r\n    tf.summary.scalar(""train_accuracy"", accuracy)\r\n    return accuracy\r\n\r\ndef lr_schedule(epoch):\r\n    if epoch <= 60:\r\n        return 0.05\r\n    if epoch <= 120:\r\n        return 0.01\r\n    if epoch <= 160:    \r\n        return 0.002\r\n    return 0.0004\r\n\r\ndef main(_):\r\n    train_x, train_y, test_x, test_y = prepare_data()\r\n    train_x, test_x = color_preprocessing(train_x, test_x)\r\n\r\n    # define placeholder x, y_ , keep_prob and learning_rate\r\n    with tf.name_scope(\'input\'):\r\n        x  = tf.placeholder(tf.float32,[None, image_size, image_size, 3], name=\'input_x\')\r\n        y_ = tf.placeholder(tf.float32, [None, class_num], name=\'input_y\')\r\n        phase = tf.placeholder(tf.bool, name=\'phase\')\r\n\r\n    with tf.name_scope(\'dropout\'):\r\n        keep_prob = tf.placeholder(tf.float32)\r\n    with tf.name_scope(\'learning_rate\'):\r\n        learning_rate = tf.placeholder(tf.float32)\r\n\r\n    # build_network\r\n    output = inference(x,phase,keep_prob)\r\n    loss, l2 = cal_loss(output,y_)\r\n    train_op = training(loss,l2,learning_rate)\r\n    eval_op = evaluate(output,y_)\r\n\r\n    summary_op = tf.summary.merge_all()\r\n\r\n    # initial an saver to save model\r\n    saver = tf.train.Saver()\r\n\r\n    # for testing\r\n    def run_testing(sess, test_x, test_y, loss, eval_op):\r\n        batch_val_loss = []\r\n        batch_val_acc = []\r\n        pre_index = 0\r\n        add = 1000\r\n        for it in range(10):\r\n            test_batch_x = test_x[pre_index:pre_index+add]\r\n            test_batch_y = test_y[pre_index:pre_index+add]\r\n            pre_index = pre_index + add\r\n\r\n            loss_, acc_  = sess.run([loss,eval_op],feed_dict={x:test_batch_x, y_:test_batch_y, phase: False, keep_prob: 1.0})\r\n\r\n            batch_val_loss.append(loss_)\r\n            batch_val_acc.append(acc_)\r\n\r\n        eval_loss, eval_acc = np.mean(batch_val_loss), np.mean(batch_val_acc)\r\n        \r\n        eval_summary = tf.Summary()\r\n        eval_summary.value.add(tag=""test_loss"", simple_value=eval_loss)\r\n        eval_summary.value.add(tag=""test_accuracy"", simple_value=eval_acc)\r\n\r\n        return eval_acc, eval_loss, eval_summary\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        summary_writer = tf.summary.FileWriter(FLAGS.log_save_path,sess.graph)\r\n\r\n        for ep in range(1,FLAGS.epochs+1):\r\n            lr = lr_schedule(ep)\r\n            pre_index = 0\r\n            train_acc = 0.0\r\n            train_loss = 0.0\r\n            start_time = time.time()\r\n\r\n            print(""\\nepoch %d/%d:"" %(ep,FLAGS.epochs))\r\n\r\n            for it in range(1,FLAGS.iteration+1):\r\n                if pre_index+FLAGS.batch_size < 50000:\r\n                    batch_x = train_x[pre_index:pre_index+FLAGS.batch_size]\r\n                    batch_y = train_y[pre_index:pre_index+FLAGS.batch_size]\r\n                else:\r\n                    batch_x = train_x[pre_index:]\r\n                    batch_y = train_y[pre_index:]\r\n\r\n\r\n                batch_x = data_augmentation(batch_x)\r\n\r\n                ts = time.time()\r\n                _, batch_loss, sum_op = sess.run([train_op, loss, summary_op],feed_dict={x:batch_x, y_:batch_y, phase:True, keep_prob: FLAGS.dropout, learning_rate: lr})\r\n                te = time.time() - ts\r\n\r\n                batch_acc = sess.run(eval_op, feed_dict={x:batch_x, y_:batch_y, phase: False, keep_prob: 1.0})\r\n\r\n                pre_index  += FLAGS.batch_size\r\n                train_loss += batch_loss\r\n                train_acc  += batch_acc\r\n\r\n                if it == FLAGS.iteration:\r\n                    train_loss /= FLAGS.iteration\r\n                    train_acc /= FLAGS.iteration\r\n\r\n                    eval_acc, eval_loss, eval_summary = run_testing(sess, test_x, test_y, loss, eval_op)\r\n\r\n                    summary_writer.add_summary(sum_op, ep)\r\n                    summary_writer.add_summary(eval_summary, ep)\r\n                    summary_writer.flush()\r\n\r\n                    print(""iteration: %d/%d, cost_time: %ds, train_loss: %.4f, train_acc: %.4f, test_loss: %.4f, test_acc: %.4f"" %(it, FLAGS.iteration, int(time.time()-start_time), train_loss, train_acc, eval_loss, eval_acc))\r\n                else:\r\n                    print(""iteration: %d/%d, cost_time: %.3fs, train_loss: %.4f, train_acc: %.4f"" %(it, FLAGS.iteration, te, train_loss / it, train_acc / it) , end=\'\\r\')\r\n            if ep % 10 == 0:\r\n                ckpt_path = FLAGS.model_save_path + ""model.ckpt%03d""%ep\r\n                save_path = saver.save(sess, ckpt_path)\r\n                print(""Model saved in file: %s"" % save_path)  \r\n\r\n        ckpt_path = FLAGS.model_save_path + ""nin.ckpt""\r\n        save_path = saver.save(sess, ckpt_path)\r\n        print(""Model saved in file: %s"" % save_path)\r\n    \r\nif __name__ == \'__main__\':\r\n    tf.app.run()\r\n\r\n    \r\n\r\n\r\n\r\n          \r\n\r\n'"
Tensorflow_version/data_utility.py,0,"b'# -*- coding:utf-8 -*-  \r\n\r\nimport os\r\nimport sys\r\nimport time\r\nimport pickle\r\nimport random\r\nimport math\r\nimport numpy as np\r\n\r\nclass_num       = 10\r\nimage_size      = 32\r\nimg_channels    = 3\r\n\r\n\r\n# ========================================================== #\r\n# \xe2\x94\x9c\xe2\x94\x80 prepare_data() \r\n#  \xe2\x94\x9c\xe2\x94\x80 download training data if not exist by download_data()\r\n#  \xe2\x94\x9c\xe2\x94\x80 load data by load_data()\r\n#  \xe2\x94\x94\xe2\x94\x80 shuffe and return data\r\n# ========================================================== #\r\n\r\n\r\n\r\ndef download_data():\r\n    dirname  = \'cifar-10-batches-py\'\r\n    origin   = \'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\'\r\n    fname = \'cifar-10-python.tar.gz\'\r\n    fpath = \'./\' + dirname\r\n\r\n    download = False\r\n    if os.path.exists(fpath) or os.path.isfile(fname):\r\n        download = False\r\n        print(""DataSet aready exist!"")\r\n    else:\r\n        download = True\r\n    if download:\r\n        print(\'Downloading data from\', origin)\r\n        import urllib.request\r\n        import tarfile\r\n\r\n        def reporthook(count, block_size, total_size):\r\n            global start_time\r\n            if count == 0:\r\n                start_time = time.time()\r\n                return\r\n            duration = time.time() - start_time\r\n            progress_size = int(count * block_size)\r\n            speed = int(progress_size / (1024 * duration))\r\n            percent = min(int(count*block_size*100/total_size),100)\r\n            sys.stdout.write(""\\r...%d%%, %d MB, %d KB/s, %d seconds passed"" %\r\n                            (percent, progress_size / (1024 * 1024), speed, duration))\r\n            sys.stdout.flush()\r\n\r\n        urllib.request.urlretrieve(origin, fname, reporthook)\r\n        print(\'Download finished. Start extract!\', origin)\r\n        if (fname.endswith(""tar.gz"")):\r\n            tar = tarfile.open(fname, ""r:gz"")\r\n            tar.extractall()\r\n            tar.close()\r\n        elif (fname.endswith(""tar"")):\r\n            tar = tarfile.open(fname, ""r:"")\r\n            tar.extractall()\r\n            tar.close()\r\n\r\ndef unpickle(file):\r\n    with open(file, \'rb\') as fo:\r\n        dict = pickle.load(fo, encoding=\'bytes\')\r\n    return dict\r\n\r\ndef load_data_one(file):\r\n    batch  = unpickle(file)\r\n    data   = batch[b\'data\']\r\n    labels = batch[b\'labels\']\r\n    print(""Loading %s : %d."" %(file, len(data)))\r\n    return data, labels\r\n\r\ndef load_data(files, data_dir, label_count):\r\n    global image_size, img_channels\r\n    data, labels = load_data_one(data_dir + \'/\' + files[0])\r\n    for f in files[1:]:\r\n        data_n, labels_n = load_data_one(data_dir + \'/\' + f)\r\n        data = np.append(data,data_n,axis=0)\r\n        labels = np.append(labels,labels_n,axis=0)\r\n    labels = np.array( [ [ float( i == label ) for i in range(label_count) ] for label in labels ] )\r\n    data = data.reshape([-1,img_channels, image_size, image_size])\r\n    data = data.transpose([0, 2, 3, 1])\r\n    return data, labels\r\n\r\ndef prepare_data():\r\n    print(""======Loading data======"")\r\n    download_data()\r\n    data_dir = \'./cifar-10-batches-py\'\r\n    image_dim = image_size * image_size * img_channels\r\n    meta = unpickle( data_dir + \'/batches.meta\')\r\n\r\n    label_names = meta[b\'label_names\']\r\n    label_count = len(label_names)\r\n    train_files = [ \'data_batch_%d\' % d for d in range(1,6) ]\r\n    train_data, train_labels = load_data(train_files, data_dir, label_count)\r\n    test_data, test_labels = load_data([ \'test_batch\' ], data_dir, label_count)\r\n\r\n    print(""Train data:"",np.shape(train_data), np.shape(train_labels))\r\n    print(""Test data :"",np.shape(test_data), np.shape(test_labels))\r\n    print(""======Load finished======"")\r\n\r\n    print(""======Shuffling data======"")\r\n    indices = np.random.permutation(len(train_data))\r\n    train_data = train_data[indices]\r\n    train_labels = train_labels[indices]\r\n    print(""======Prepare Finished======"")\r\n\r\n    return train_data, train_labels, test_data, test_labels\r\n\r\n\r\n# ========================================================== #\r\n# \xe2\x94\x9c\xe2\x94\x80 _random_crop() \r\n# \xe2\x94\x9c\xe2\x94\x80 _random_flip_leftright()\r\n# \xe2\x94\x9c\xe2\x94\x80 data_augmentation()\r\n# \xe2\x94\x94\xe2\x94\x80 color_preprocessing()\r\n# ========================================================== #\r\n\r\ndef _random_crop(batch, crop_shape, padding=None):\r\n        oshape = np.shape(batch[0])\r\n        \r\n        if padding:\r\n            oshape = (oshape[0] + 2*padding, oshape[1] + 2*padding)\r\n        new_batch = []\r\n        npad = ((padding, padding), (padding, padding), (0, 0))\r\n        for i in range(len(batch)):\r\n            new_batch.append(batch[i])\r\n            if padding:\r\n                new_batch[i] = np.lib.pad(batch[i], pad_width=npad,\r\n                                          mode=\'constant\', constant_values=0)\r\n            nh = random.randint(0, oshape[0] - crop_shape[0])\r\n            nw = random.randint(0, oshape[1] - crop_shape[1])\r\n            new_batch[i] = new_batch[i][nh:nh + crop_shape[0],\r\n                                        nw:nw + crop_shape[1]]\r\n        return new_batch\r\n\r\ndef _random_flip_leftright(batch):\r\n        for i in range(len(batch)):\r\n            if bool(random.getrandbits(1)):\r\n                batch[i] = np.fliplr(batch[i])\r\n        return batch\r\n\r\ndef color_preprocessing(x_train,x_test):\r\n    x_train = x_train.astype(\'float32\')\r\n    x_test = x_test.astype(\'float32\')\r\n    mean = [125.307, 122.95, 113.865]\r\n    std  = [62.9932, 62.0887, 66.7048]\r\n    for i in range(3):\r\n        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\r\n        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\r\n    return x_train, x_test\r\n\r\ndef data_augmentation(batch):\r\n    batch = _random_flip_leftright(batch)\r\n    batch = _random_crop(batch, [32,32], 4)\r\n    return batch\r\n'"
Tensorflow_version/vgg_19.py,66,"b'# -*- coding:utf-8 -*-  \r\n# ========================================================== #\r\n# File name: vgg_19.py\r\n# Author: BIGBALLON\r\n# Date created: 07/22/2017\r\n# Python Version: 3.5.2\r\n# Description: implement vgg19 network to train cifar10 \r\n# Result: test accuracy about 93.28% - 93.32%\r\n# ========================================================== #\r\n\r\nimport tensorflow as tf\r\nfrom data_utility import *\r\n\r\niterations      = 200\r\nbatch_size      = 250\r\ntotal_epoch     = 164\r\nweight_decay    = 0.0003\r\ndropout_rate    = 0.5\r\nmomentum_rate   = 0.9\r\nlog_save_path   = \'./vgg_logs\'\r\nmodel_save_path = \'./model/\'\r\n\r\n\r\n# ========================================================== #\r\n# \xe2\x94\x9c\xe2\x94\x80 bias_variable()\r\n# \xe2\x94\x9c\xe2\x94\x80 conv2d()           With Batch Normalization\r\n# \xe2\x94\x9c\xe2\x94\x80 max_pool()\r\n# \xe2\x94\x94\xe2\x94\x80 global_avg_pool()\r\n# ========================================================== #\r\n\r\n\r\ndef bias_variable(shape):\r\n    initial = tf.constant(0.1, shape=shape, dtype=tf.float32 )\r\n    return tf.Variable(initial)\r\n\r\ndef conv2d(x, W):\r\n    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding=\'SAME\')\r\n\r\ndef max_pool(input, k_size=1, stride=1, name=None):\r\n    return tf.nn.max_pool(input, ksize=[1, k_size, k_size, 1], strides=[1, stride, stride, 1], padding=\'SAME\',name=name)\r\n\r\ndef batch_norm(input):\r\n    return tf.contrib.layers.batch_norm(input, decay=0.9, center=True, scale=True, epsilon=1e-3, is_training=train_flag, updates_collections=None)\r\n\r\n# ========================================================== #\r\n# \xe2\x94\x9c\xe2\x94\x80 _random_crop() \r\n# \xe2\x94\x9c\xe2\x94\x80 _random_flip_leftright()\r\n# \xe2\x94\x9c\xe2\x94\x80 data_augmentation()\r\n# \xe2\x94\x9c\xe2\x94\x80 data_preprocessing()\r\n# \xe2\x94\x94\xe2\x94\x80 learning_rate_schedule()\r\n# ========================================================== #\r\n\r\ndef _random_crop(batch, crop_shape, padding=None):\r\n        oshape = np.shape(batch[0])\r\n        \r\n        if padding:\r\n            oshape = (oshape[0] + 2*padding, oshape[1] + 2*padding)\r\n        new_batch = []\r\n        npad = ((padding, padding), (padding, padding), (0, 0))\r\n        for i in range(len(batch)):\r\n            new_batch.append(batch[i])\r\n            if padding:\r\n                new_batch[i] = np.lib.pad(batch[i], pad_width=npad,\r\n                                          mode=\'constant\', constant_values=0)\r\n            nh = random.randint(0, oshape[0] - crop_shape[0])\r\n            nw = random.randint(0, oshape[1] - crop_shape[1])\r\n            new_batch[i] = new_batch[i][nh:nh + crop_shape[0],\r\n                                        nw:nw + crop_shape[1]]\r\n        return new_batch\r\n\r\ndef _random_flip_leftright(batch):\r\n        for i in range(len(batch)):\r\n            if bool(random.getrandbits(1)):\r\n                batch[i] = np.fliplr(batch[i])\r\n        return batch\r\n\r\ndef data_preprocessing(x_train,x_test):\r\n\r\n    x_train = x_train.astype(\'float32\')\r\n    x_test = x_test.astype(\'float32\')\r\n\r\n    x_train[:,:,:,0] = (x_train[:,:,:,0] - np.mean(x_train[:,:,:,0])) / np.std(x_train[:,:,:,0])\r\n    x_train[:,:,:,1] = (x_train[:,:,:,1] - np.mean(x_train[:,:,:,1])) / np.std(x_train[:,:,:,1])\r\n    x_train[:,:,:,2] = (x_train[:,:,:,2] - np.mean(x_train[:,:,:,2])) / np.std(x_train[:,:,:,2])\r\n\r\n    x_test[:,:,:,0] = (x_test[:,:,:,0] - np.mean(x_test[:,:,:,0])) / np.std(x_test[:,:,:,0])\r\n    x_test[:,:,:,1] = (x_test[:,:,:,1] - np.mean(x_test[:,:,:,1])) / np.std(x_test[:,:,:,1])\r\n    x_test[:,:,:,2] = (x_test[:,:,:,2] - np.mean(x_test[:,:,:,2])) / np.std(x_test[:,:,:,2])\r\n\r\n    return x_train, x_test\r\n\r\ndef learning_rate_schedule(epoch_num):\r\n      if epoch_num < 81:\r\n          return 0.1\r\n      elif epoch_num < 121:\r\n          return 0.01\r\n      else:\r\n          return 0.001\r\n\r\ndef data_augmentation(batch):\r\n    batch = _random_flip_leftright(batch)\r\n    batch = _random_crop(batch, [32,32], 4)\r\n    return batch\r\n\r\ndef run_testing(sess,ep):\r\n    acc = 0.0\r\n    loss = 0.0\r\n    pre_index = 0\r\n    add = 1000\r\n    for it in range(10):\r\n        batch_x = test_x[pre_index:pre_index+add]\r\n        batch_y = test_y[pre_index:pre_index+add]\r\n        pre_index = pre_index + add\r\n        loss_, acc_  = sess.run([cross_entropy,accuracy],feed_dict={x:batch_x, y_:batch_y, keep_prob: 1.0, train_flag: False})\r\n        loss += loss_ / 10.0\r\n        acc += acc_ / 10.0\r\n    summary = tf.Summary(value=[tf.Summary.Value(tag=""test_loss"", simple_value=loss), \r\n                            tf.Summary.Value(tag=""test_accuracy"", simple_value=acc)])\r\n    return acc, loss, summary\r\n\r\n\r\n# ========================================================== #\r\n# \xe2\x94\x9c\xe2\x94\x80 main()\r\n# Training and Testing \r\n# Save train/teset loss and acc for visualization\r\n# Save Model in ./model\r\n# ========================================================== #\r\n\r\n\r\nif __name__ == \'__main__\':\r\n\r\n    train_x, train_y, test_x, test_y = prepare_data()\r\n    train_x, test_x = data_preprocessing(train_x, test_x)\r\n\r\n    # define placeholder x, y_ , keep_prob, learning_rate\r\n    x  = tf.placeholder(tf.float32,[None, image_size, image_size, 3])\r\n    y_ = tf.placeholder(tf.float32, [None, class_num])\r\n    keep_prob = tf.placeholder(tf.float32)\r\n    learning_rate = tf.placeholder(tf.float32)\r\n    train_flag = tf.placeholder(tf.bool)\r\n\r\n    # build_network\r\n\r\n    W_conv1_1 = tf.get_variable(\'conv1_1\', shape=[3, 3, 3, 64], initializer=tf.contrib.keras.initializers.he_normal())\r\n    b_conv1_1 = bias_variable([64])\r\n    output  = tf.nn.relu( batch_norm(conv2d(x,W_conv1_1) + b_conv1_1))\r\n    \r\n    W_conv1_2 = tf.get_variable(\'conv1_2\', shape=[3, 3, 64, 64], initializer=tf.contrib.keras.initializers.he_normal())\r\n    b_conv1_2 = bias_variable([64])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv1_2) + b_conv1_2))\r\n    output  = max_pool(output, 2, 2, ""pool1"")\r\n\r\n    W_conv2_1 = tf.get_variable(\'conv2_1\', shape=[3, 3, 64, 128], initializer=tf.contrib.keras.initializers.he_normal())\r\n    b_conv2_1 = bias_variable([128])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv2_1) + b_conv2_1))\r\n    \r\n\r\n    W_conv2_2 = tf.get_variable(\'conv2_2\', shape=[3, 3, 128, 128], initializer=tf.contrib.keras.initializers.he_normal())\r\n    b_conv2_2 = bias_variable([128])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv2_2) + b_conv2_2))\r\n    output  = max_pool(output, 2, 2, ""pool2"")\r\n\r\n    W_conv3_1 = tf.get_variable(\'conv3_1\', shape=[3, 3, 128, 256], initializer=tf.contrib.keras.initializers.he_normal())\r\n    b_conv3_1 = bias_variable([256])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_1) + b_conv3_1))\r\n\r\n\r\n    W_conv3_2 = tf.get_variable(\'conv3_2\', shape=[3, 3, 256, 256], initializer=tf.contrib.keras.initializers.he_normal())\r\n    b_conv3_2 = bias_variable([256])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_2) + b_conv3_2))\r\n\r\n    W_conv3_3 = tf.get_variable(\'conv3_3\', shape=[3, 3, 256, 256], initializer=tf.contrib.keras.initializers.he_normal())\r\n    b_conv3_3 = bias_variable([256])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_3) + b_conv3_3))\r\n\r\n    W_conv3_4 = tf.get_variable(\'conv3_4\', shape=[3, 3, 256, 256], initializer=tf.contrib.keras.initializers.he_normal())\r\n    b_conv3_4 = bias_variable([256])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_4) + b_conv3_4))\r\n    output  = max_pool(output, 2, 2, ""pool3"")\r\n\r\n    W_conv4_1 = tf.get_variable(\'conv4_1\', shape=[3, 3, 256, 512], initializer=tf.contrib.keras.initializers.he_normal())\r\n    b_conv4_1 = bias_variable([512])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_1) + b_conv4_1))\r\n\r\n\r\n    W_conv4_2 = tf.get_variable(\'conv4_2\', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\r\n    b_conv4_2 = bias_variable([512])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_2) + b_conv4_2))\r\n\r\n    W_conv4_3 = tf.get_variable(\'conv4_3\', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\r\n    b_conv4_3 = bias_variable([512])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_3) + b_conv4_3))\r\n\r\n    W_conv4_4 = tf.get_variable(\'conv4_4\', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\r\n    b_conv4_4 = bias_variable([512])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_4)) + b_conv4_4)\r\n    output  = max_pool(output, 2, 2)\r\n\r\n    W_conv5_1 = tf.get_variable(\'conv5_1\', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\r\n    b_conv5_1 = bias_variable([512])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_1) + b_conv5_1))\r\n\r\n    W_conv5_2 = tf.get_variable(\'conv5_2\', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\r\n    b_conv5_2 = bias_variable([512])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_2) + b_conv5_2))\r\n\r\n    W_conv5_3 = tf.get_variable(\'conv5_3\', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\r\n    b_conv5_3 = bias_variable([512])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_3) + b_conv5_3))\r\n\r\n    W_conv5_4 = tf.get_variable(\'conv5_4\', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\r\n    b_conv5_4 = bias_variable([512])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_4) + b_conv5_4))\r\n\r\n    # output = tf.contrib.layers.flatten(output)\r\n    output = tf.reshape(output,[-1,2*2*512])\r\n\r\n    W_fc1 = tf.get_variable(\'fc1\', shape=[2048,4096], initializer=tf.contrib.keras.initializers.he_normal())\r\n    b_fc1 = bias_variable([4096])\r\n    output = tf.nn.relu( batch_norm(tf.matmul(output,W_fc1) + b_fc1) )\r\n    output  = tf.nn.dropout(output,keep_prob)\r\n    \r\n    W_fc2 = tf.get_variable(\'fc7\', shape=[4096,4096], initializer=tf.contrib.keras.initializers.he_normal())\r\n    b_fc2 = bias_variable([4096])\r\n    output = tf.nn.relu( batch_norm(tf.matmul(output,W_fc2) + b_fc2) )\r\n    output  = tf.nn.dropout(output,keep_prob)\r\n\r\n\r\n    W_fc3 = tf.get_variable(\'fc3\', shape=[4096,10], initializer=tf.contrib.keras.initializers.he_normal())\r\n    b_fc3 = bias_variable([10])\r\n    output = tf.nn.relu( batch_norm(tf.matmul(output,W_fc3) + b_fc3) )\r\n    # output  = tf.reshape(output,[-1,10])\r\n\r\n\r\n\r\n    # loss function: cross_entropy\r\n    # train_step: training operation\r\n    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=output))\r\n    l2 = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\r\n    train_step = tf.train.MomentumOptimizer(learning_rate, momentum_rate,use_nesterov=True).minimize(cross_entropy + l2 * weight_decay)\r\n    \r\n    correct_prediction = tf.equal(tf.argmax(output,1), tf.argmax(y_,1))\r\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\r\n\r\n    # initial an saver to save model\r\n    saver = tf.train.Saver()\r\n   \r\n    with tf.Session() as sess:\r\n        \r\n        sess.run(tf.global_variables_initializer())\r\n        summary_writer = tf.summary.FileWriter(log_save_path,sess.graph)\r\n\r\n        # epoch = 164 \r\n        # make sure [bath_size * iteration = data_set_number]\r\n\r\n        for ep in range(1,total_epoch+1):\r\n            lr = learning_rate_schedule(ep)\r\n            pre_index = 0\r\n            train_acc = 0.0\r\n            train_loss = 0.0\r\n            start_time = time.time()\r\n\r\n            print(""\\nepoch %d/%d:"" %(ep,total_epoch))\r\n\r\n            for it in range(1,iterations+1):\r\n                batch_x = train_x[pre_index:pre_index+batch_size]\r\n                batch_y = train_y[pre_index:pre_index+batch_size]\r\n\r\n                batch_x = data_augmentation(batch_x)\r\n\r\n                _, batch_loss = sess.run([train_step, cross_entropy],feed_dict={x:batch_x, y_:batch_y, keep_prob: dropout_rate, learning_rate: lr, train_flag: True})\r\n                batch_acc = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob: 1.0, train_flag: True})\r\n\r\n                train_loss += batch_loss\r\n                train_acc  += batch_acc\r\n                pre_index  += batch_size\r\n\r\n                if it == iterations:\r\n                    train_loss /= iterations\r\n                    train_acc /= iterations\r\n\r\n                    loss_, acc_  = sess.run([cross_entropy,accuracy],feed_dict={x:batch_x, y_:batch_y, keep_prob: 1.0, train_flag: True})\r\n                    train_summary = tf.Summary(value=[tf.Summary.Value(tag=""train_loss"", simple_value=train_loss), \r\n                                          tf.Summary.Value(tag=""train_accuracy"", simple_value=train_acc)])\r\n\r\n                    val_acc, val_loss, test_summary = run_testing(sess,ep)\r\n\r\n                    summary_writer.add_summary(train_summary, ep)\r\n                    summary_writer.add_summary(test_summary, ep)\r\n                    summary_writer.flush()\r\n\r\n                    print(""iteration: %d/%d, cost_time: %ds, train_loss: %.4f, train_acc: %.4f, test_loss: %.4f, test_acc: %.4f"" %(it, iterations, int(time.time()-start_time), train_loss, train_acc, val_loss, val_acc))\r\n                else:\r\n                    print(""iteration: %d/%d, train_loss: %.4f, train_acc: %.4f"" %(it, iterations, train_loss / it, train_acc / it) , end=\'\\r\')\r\n\r\n        save_path = saver.save(sess, model_save_path)\r\n        print(""Model saved in file: %s"" % save_path)  \r\n'"
Tensorflow_version/vgg_19_pretrain.py,81,"b'# -*- coding:utf-8 -*-  \r\n# ========================================================== #\r\n# File name: vgg_19.py\r\n# Author: BIGBALLON\r\n# Date created: 07/22/2017\r\n# Python Version: 3.5.2\r\n# Description: implement vgg19 network to train cifar10 \r\n# ========================================================== #\r\n\r\nimport tensorflow as tf\r\nfrom data_utility import *\r\n\r\niterations      = 200\r\nbatch_size      = 250\r\ntotal_epoch     = 164\r\nweight_decay    = 0.0005 # change it for test\r\ndropout_rate    = 0.5\r\nmomentum_rate   = 0.9\r\nlog_save_path   = \'./pretrain_vgg_logs\'\r\nmodel_save_path = \'./model/\'\r\n\r\n\r\n# ========================================================== #\r\n# \xe2\x94\x9c\xe2\x94\x80 bias_variable()\r\n# \xe2\x94\x9c\xe2\x94\x80 conv2d()           With Batch Normalization\r\n# \xe2\x94\x9c\xe2\x94\x80 max_pool()\r\n# \xe2\x94\x94\xe2\x94\x80 global_avg_pool()\r\n# ========================================================== #\r\n\r\n\r\ndef bias_variable(shape):\r\n    initial = tf.constant(0.1, shape=shape, dtype=tf.float32 )\r\n    return tf.Variable(initial)\r\n\r\ndef conv2d(x, W):\r\n    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding=\'SAME\')\r\n\r\ndef max_pool(input, k_size=1, stride=1, name=None):\r\n    return tf.nn.max_pool(input, ksize=[1, k_size, k_size, 1], strides=[1, stride, stride, 1], padding=\'SAME\',name=name)\r\n\r\ndef batch_norm(input):\r\n    return tf.contrib.layers.batch_norm(input, decay=0.9, center=True, scale=True, epsilon=1e-3, is_training=train_flag, updates_collections=None)\r\n\r\n# ========================================================== #\r\n# \xe2\x94\x9c\xe2\x94\x80 _random_crop() \r\n# \xe2\x94\x9c\xe2\x94\x80 _random_flip_leftright()\r\n# \xe2\x94\x9c\xe2\x94\x80 data_augmentation()\r\n# \xe2\x94\x9c\xe2\x94\x80 data_preprocessing()\r\n# \xe2\x94\x94\xe2\x94\x80 learning_rate_schedule()\r\n# ========================================================== #\r\n\r\ndef _random_crop(batch, crop_shape, padding=None):\r\n        oshape = np.shape(batch[0])\r\n        \r\n        if padding:\r\n            oshape = (oshape[0] + 2*padding, oshape[1] + 2*padding)\r\n        new_batch = []\r\n        npad = ((padding, padding), (padding, padding), (0, 0))\r\n        for i in range(len(batch)):\r\n            new_batch.append(batch[i])\r\n            if padding:\r\n                new_batch[i] = np.lib.pad(batch[i], pad_width=npad,\r\n                                          mode=\'constant\', constant_values=0)\r\n            nh = random.randint(0, oshape[0] - crop_shape[0])\r\n            nw = random.randint(0, oshape[1] - crop_shape[1])\r\n            new_batch[i] = new_batch[i][nh:nh + crop_shape[0],\r\n                                        nw:nw + crop_shape[1]]\r\n        return new_batch\r\n\r\ndef _random_flip_leftright(batch):\r\n        for i in range(len(batch)):\r\n            if bool(random.getrandbits(1)):\r\n                batch[i] = np.fliplr(batch[i])\r\n        return batch\r\n\r\ndef data_preprocessing(x_train,x_test):\r\n\r\n    x_train = x_train.astype(\'float32\')\r\n    x_test = x_test.astype(\'float32\')\r\n    x_train[:,:,:,0] = (x_train[:,:,:,0]-123.680)\r\n    x_train[:,:,:,1] = (x_train[:,:,:,1]-116.779)\r\n    x_train[:,:,:,2] = (x_train[:,:,:,2]-103.939)\r\n    x_test[:,:,:,0] = (x_test[:,:,:,0]-123.680)\r\n    x_test[:,:,:,1] = (x_test[:,:,:,1]-116.779)\r\n    x_test[:,:,:,2] = (x_test[:,:,:,2]-103.939)\r\n\r\n    return x_train, x_test\r\n\r\ndef learning_rate_schedule(epoch_num):\r\n      if epoch_num < 81:\r\n          return 0.1\r\n      elif epoch_num < 121:\r\n          return 0.01\r\n      else:\r\n          return 0.001\r\n\r\ndef data_augmentation(batch):\r\n    batch = _random_flip_leftright(batch)\r\n    batch = _random_crop(batch, [32,32], 4)\r\n    return batch\r\n\r\ndef run_testing(sess,ep):\r\n    acc = 0.0\r\n    loss = 0.0\r\n    pre_index = 0\r\n    add = 1000\r\n    for it in range(10):\r\n        batch_x = test_x[pre_index:pre_index+add]\r\n        batch_y = test_y[pre_index:pre_index+add]\r\n        pre_index = pre_index + add\r\n        loss_, acc_  = sess.run([cross_entropy,accuracy],feed_dict={x:batch_x, y_:batch_y, keep_prob: 1.0, train_flag: False})\r\n        loss += loss_ / 10.0\r\n        acc += acc_ / 10.0\r\n    summary = tf.Summary(value=[tf.Summary.Value(tag=""test_loss"", simple_value=loss), \r\n                            tf.Summary.Value(tag=""test_accuracy"", simple_value=acc)])\r\n    return acc, loss, summary\r\n\r\n\r\n# ========================================================== #\r\n# \xe2\x94\x9c\xe2\x94\x80 main()\r\n# Training and Testing \r\n# Save train/teset loss and acc for visualization\r\n# Save Model in ./model\r\n# ========================================================== #\r\n\r\n\r\nif __name__ == \'__main__\':\r\n\r\n    train_x, train_y, test_x, test_y = prepare_data()\r\n    train_x, test_x = data_preprocessing(train_x, test_x)\r\n\r\n    # load pretrained weight from vgg19.npy\r\n    params_dict = np.load(\'vgg19.npy\',encoding=\'latin1\').item()\r\n\r\n    # define placeholder x, y_ , keep_prob, learning_rate\r\n    x  = tf.placeholder(tf.float32,[None, image_size, image_size, 3])\r\n    y_ = tf.placeholder(tf.float32, [None, class_num])\r\n    keep_prob = tf.placeholder(tf.float32)\r\n    learning_rate = tf.placeholder(tf.float32)\r\n    train_flag = tf.placeholder(tf.bool)\r\n\r\n    # build_network\r\n\r\n    W_conv1_1 = tf.Variable(params_dict[\'conv1_1\'][0])\r\n    b_conv1_1 = tf.Variable(params_dict[\'conv1_1\'][1])\r\n    output  = tf.nn.relu( batch_norm(conv2d(x,W_conv1_1) + b_conv1_1))\r\n    \r\n    W_conv1_2 = tf.Variable(params_dict[\'conv1_2\'][0])\r\n    b_conv1_2 = tf.Variable(params_dict[\'conv1_2\'][1])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv1_2) + b_conv1_2))\r\n    output  = max_pool(output, 2, 2, ""pool1"")\r\n\r\n    W_conv2_1 = tf.Variable(params_dict[\'conv2_1\'][0])\r\n    b_conv2_1 = tf.Variable(params_dict[\'conv2_2\'][1])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv2_1) + b_conv2_1))\r\n    \r\n\r\n    W_conv2_2 = tf.Variable(params_dict[\'conv2_2\'][0])\r\n    b_conv2_2 = tf.Variable(params_dict[\'conv2_2\'][1])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv2_2) + b_conv2_2))\r\n    output  = max_pool(output, 2, 2, ""pool2"")\r\n\r\n    W_conv3_1 = tf.Variable(params_dict[\'conv3_1\'][0])\r\n    b_conv3_1 = tf.Variable(params_dict[\'conv3_1\'][1])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_1) + b_conv3_1))\r\n\r\n\r\n    W_conv3_2 = tf.Variable(params_dict[\'conv3_2\'][0])\r\n    b_conv3_2 = tf.Variable(params_dict[\'conv3_2\'][1])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_2) + b_conv3_2))\r\n\r\n    W_conv3_3 = tf.Variable(params_dict[\'conv3_3\'][0])\r\n    b_conv3_3 = tf.Variable(params_dict[\'conv3_3\'][1])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_3) + b_conv3_3))\r\n\r\n    W_conv3_4 = tf.Variable(params_dict[\'conv3_4\'][0])\r\n    b_conv3_4 = tf.Variable(params_dict[\'conv3_4\'][1])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_4) + b_conv3_4))\r\n    output  = max_pool(output, 2, 2, ""pool3"")\r\n\r\n    W_conv4_1 = tf.Variable(params_dict[\'conv4_1\'][0])\r\n    b_conv4_1 = tf.Variable(params_dict[\'conv4_1\'][1])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_1) + b_conv4_1))\r\n\r\n\r\n    W_conv4_2 = tf.Variable(params_dict[\'conv4_2\'][0])\r\n    b_conv4_2 = tf.Variable(params_dict[\'conv4_2\'][1])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_2) + b_conv4_2))\r\n\r\n    W_conv4_3 = tf.Variable(params_dict[\'conv4_3\'][0])\r\n    b_conv4_3 = tf.Variable(params_dict[\'conv4_3\'][1])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_3) + b_conv4_3))\r\n\r\n    W_conv4_4 = tf.Variable(params_dict[\'conv4_4\'][0])\r\n    b_conv4_4 = tf.Variable(params_dict[\'conv4_4\'][1])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_4)) + b_conv4_4)\r\n    output  = max_pool(output, 2, 2)\r\n\r\n    W_conv5_1 = tf.Variable(params_dict[\'conv5_1\'][0])\r\n    b_conv5_1 = tf.Variable(params_dict[\'conv5_1\'][1])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_1) + b_conv5_1))\r\n\r\n    W_conv5_2 = tf.Variable(params_dict[\'conv5_2\'][0])\r\n    b_conv5_2 = tf.Variable(params_dict[\'conv5_2\'][1])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_2) + b_conv5_2))\r\n\r\n    W_conv5_3 = tf.Variable(params_dict[\'conv5_3\'][0])\r\n    b_conv5_3 = tf.Variable(params_dict[\'conv5_3\'][1])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_3) + b_conv5_3))\r\n\r\n    W_conv5_4 = tf.Variable(params_dict[\'conv5_4\'][0])\r\n    b_conv5_4 = tf.Variable(params_dict[\'conv5_4\'][1])\r\n    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_4) + b_conv5_4))\r\n\r\n    output = tf.reshape(output,[-1,2*2*512])\r\n\r\n    W_fc1 = tf.get_variable(\'fc1\', shape=[2048,4096], initializer=tf.contrib.keras.initializers.he_normal())\r\n    b_fc1 = bias_variable([4096])\r\n    output = tf.nn.relu( batch_norm(tf.matmul(output,W_fc1) + b_fc1) )\r\n    output  = tf.nn.dropout(output,keep_prob)\r\n    \r\n    W_fc2 = tf.Variable(params_dict[\'fc7\'][0])\r\n    b_fc2 = tf.Variable(params_dict[\'fc7\'][1])\r\n    output = tf.nn.relu( batch_norm(tf.matmul(output,W_fc2) + b_fc2) )\r\n    output  = tf.nn.dropout(output,keep_prob)\r\n\r\n\r\n    W_fc3 = tf.get_variable(\'fc3\', shape=[4096,10], initializer=tf.contrib.keras.initializers.he_normal())\r\n    b_fc3 = bias_variable([10])\r\n    output = tf.nn.relu( batch_norm(tf.matmul(output,W_fc3) + b_fc3) )\r\n\r\n    # loss function: cross_entropy\r\n    # train_step: training operation\r\n    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=output))\r\n    l2 = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\r\n    train_step = tf.train.MomentumOptimizer(learning_rate, momentum_rate,use_nesterov=True).minimize(cross_entropy + l2 * weight_decay)\r\n    \r\n    correct_prediction = tf.equal(tf.argmax(output,1), tf.argmax(y_,1))\r\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\r\n\r\n    # initial an saver to save model\r\n    saver = tf.train.Saver()\r\n   \r\n    with tf.Session() as sess:\r\n        \r\n        sess.run(tf.global_variables_initializer())\r\n        summary_writer = tf.summary.FileWriter(log_save_path,sess.graph)\r\n\r\n        # epoch = 164 \r\n        # make sure [bath_size * iteration = data_set_number]\r\n\r\n        for ep in range(1,total_epoch+1):\r\n            lr = learning_rate_schedule(ep)\r\n            pre_index = 0\r\n            train_acc = 0.0\r\n            train_loss = 0.0\r\n            start_time = time.time()\r\n\r\n            print(""\\nepoch %d/%d:"" %(ep,total_epoch))\r\n\r\n            for it in range(1,iterations+1):\r\n                batch_x = train_x[pre_index:pre_index+batch_size]\r\n                batch_y = train_y[pre_index:pre_index+batch_size]\r\n\r\n                batch_x = data_augmentation(batch_x)\r\n\r\n                _, batch_loss = sess.run([train_step, cross_entropy],feed_dict={x:batch_x, y_:batch_y, keep_prob: dropout_rate, learning_rate: lr, train_flag: True})\r\n                batch_acc = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob: 1.0, train_flag: True})\r\n\r\n                train_loss += batch_loss\r\n                train_acc  += batch_acc\r\n                pre_index  += batch_size\r\n\r\n                if it == iterations:\r\n                    train_loss /= iterations\r\n                    train_acc /= iterations\r\n\r\n                    loss_, acc_  = sess.run([cross_entropy,accuracy],feed_dict={x:batch_x, y_:batch_y, keep_prob: 1.0, train_flag: True})\r\n                    train_summary = tf.Summary(value=[tf.Summary.Value(tag=""train_loss"", simple_value=train_loss), \r\n                                          tf.Summary.Value(tag=""train_accuracy"", simple_value=train_acc)])\r\n\r\n                    val_acc, val_loss, test_summary = run_testing(sess,ep)\r\n\r\n                    summary_writer.add_summary(train_summary, ep)\r\n                    summary_writer.add_summary(test_summary, ep)\r\n                    summary_writer.flush()\r\n\r\n                    print(""iteration: %d/%d, cost_time: %ds, train_loss: %.4f, train_acc: %.4f, test_loss: %.4f, test_acc: %.4f"" %(it, iterations, int(time.time()-start_time), train_loss, train_acc, val_loss, val_acc))\r\n                else:\r\n                    print(""iteration: %d/%d, train_loss: %.4f, train_acc: %.4f"" %(it, iterations, train_loss / it, train_acc / it) , end=\'\\r\')\r\n\r\n        save_path = saver.save(sess, model_save_path)\r\n        print(""Model saved in file: %s"" % save_path)  \r\n'"
for_girl/00_original.py,0,"b""import keras\nfrom keras.optimizers import SGD\nfrom keras.datasets import cifar10\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\nfrom keras.callbacks import LearningRateScheduler, TensorBoard\n\ndef build_model():\n    model = Sequential()\n    model.add(Conv2D(6, (5, 5), padding='valid', activation = 'relu', kernel_initializer='he_normal', input_shape=(32,32,3)))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n    model.add(Conv2D(16, (5, 5), padding='valid', activation = 'relu', kernel_initializer='he_normal'))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(120, activation = 'relu', kernel_initializer='he_normal'))\n    model.add(Dense(84, activation = 'relu', kernel_initializer='he_normal'))\n    model.add(Dense(10, activation = 'softmax', kernel_initializer='he_normal'))\n    sgd = SGD(lr=.1, momentum=0.9, nesterov=True)\n    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n    return model\n\ndef scheduler(epoch):\n    if epoch < 81:\n        return 0.05\n    if epoch < 122:\n        return 0.005\n    return 0.0005\n\nif __name__ == '__main__':\n\n    # load data\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n    y_train = keras.utils.to_categorical(y_train, 10)\n    y_test = keras.utils.to_categorical(y_test, 10)\n    x_train = x_train.astype('float32') / 255.0\n    x_test = x_test.astype('float32') / 255.0\n\n    # build network\n    model = build_model()\n    \n    # set callback\n    tb_cb = TensorBoard(log_dir='./lenet', histogram_freq=0)\n    change_lr = LearningRateScheduler(scheduler)\n    cbks = [change_lr,tb_cb]\n\n    # start traing \n    model.fit(x_train, y_train,batch_size=128,epochs=200,callbacks=cbks,\n                  validation_data=(x_test, y_test), shuffle=True)\n    # save model\n    model.save('lenet.h5')\n\n\n\n"""
for_girl/01_print_summary.py,2,"b""import keras\nfrom keras.optimizers import SGD\nfrom keras.datasets import cifar10\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\nfrom keras.callbacks import LearningRateScheduler, TensorBoard\n\nfrom keras import backend as K\n# set GPU memory \nif('tensorflow' == K.backend()):\n    import tensorflow as tf\n    from keras.backend.tensorflow_backend import set_session\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    sess = tf.Session(config=config)\n\ndef build_model():\n    model = Sequential()\n    model.add(Conv2D(6, (5, 5), padding='valid', activation = 'relu', kernel_initializer='he_normal', input_shape=(32,32,3)))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n    model.add(Conv2D(16, (5, 5), padding='valid', activation = 'relu', kernel_initializer='he_normal'))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(120, activation = 'relu', kernel_initializer='he_normal'))\n    model.add(Dense(84, activation = 'relu', kernel_initializer='he_normal'))\n    model.add(Dense(10, activation = 'softmax', kernel_initializer='he_normal'))\n    sgd = SGD(lr=.1, momentum=0.9, nesterov=True)\n    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n    return model\n\ndef scheduler(epoch):\n    if epoch < 81:\n        return 0.05\n    if epoch < 122:\n        return 0.005\n    return 0.0005\n\nif __name__ == '__main__':\n\n    # load data\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n    y_train = keras.utils.to_categorical(y_train, 10)\n    y_test = keras.utils.to_categorical(y_test, 10)\n    x_train = x_train.astype('float32') / 255.0\n    x_test = x_test.astype('float32') / 255.0\n\n    # build network\n    model = build_model()\n    \n    # print summary\n    print(model.summary())\n    \n    # set callback\n    tb_cb = TensorBoard(log_dir='./lenet', histogram_freq=0)\n    change_lr = LearningRateScheduler(scheduler)\n    cbks = [change_lr,tb_cb]\n\n    # start traing \n    model.fit(x_train, y_train,batch_size=128,epochs=200,callbacks=cbks,\n                  validation_data=(x_test, y_test), shuffle=True)\n    # save model\n    model.save('lenet.h5')\n\n\n\n"""
for_girl/02_set_momory.py,2,"b""import keras\nfrom keras.optimizers import SGD\nfrom keras.datasets import cifar10\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\nfrom keras.callbacks import LearningRateScheduler, TensorBoard\n\nfrom keras import backend as K\n# set GPU memory \nif('tensorflow' == K.backend()):\n    import tensorflow as tf\n    from keras.backend.tensorflow_backend import set_session\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    sess = tf.Session(config=config)\n\ndef build_model():\n    model = Sequential()\n    model.add(Conv2D(6, (5, 5), padding='valid', activation = 'relu', kernel_initializer='he_normal', input_shape=(32,32,3)))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n    model.add(Conv2D(16, (5, 5), padding='valid', activation = 'relu', kernel_initializer='he_normal'))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(120, activation = 'relu', kernel_initializer='he_normal'))\n    model.add(Dense(84, activation = 'relu', kernel_initializer='he_normal'))\n    model.add(Dense(10, activation = 'softmax', kernel_initializer='he_normal'))\n    sgd = SGD(lr=.1, momentum=0.9, nesterov=True)\n    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n    return model\n\ndef scheduler(epoch):\n    if epoch < 81:\n        return 0.05\n    if epoch < 122:\n        return 0.005\n    return 0.0005\n\nif __name__ == '__main__':\n\n    # load data\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n    y_train = keras.utils.to_categorical(y_train, 10)\n    y_test = keras.utils.to_categorical(y_test, 10)\n    x_train = x_train.astype('float32') / 255.0\n    x_test = x_test.astype('float32') / 255.0\n\n    # build network\n    model = build_model()\n    \n    # set callback\n    tb_cb = TensorBoard(log_dir='./lenet', histogram_freq=0)\n    change_lr = LearningRateScheduler(scheduler)\n    cbks = [change_lr,tb_cb]\n\n    # start traing \n    model.fit(x_train, y_train,batch_size=128,epochs=200,callbacks=cbks,\n                  validation_data=(x_test, y_test), shuffle=True)\n    # save model\n    model.save('lenet.h5')\n\n\n\n"""
for_girl/03_save_pic.py,2,"b""import keras\nfrom keras.optimizers import SGD\nfrom keras.datasets import cifar10\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\nfrom keras.callbacks import LearningRateScheduler, TensorBoard\n\nfrom keras import backend as K\n# set GPU memory \nif('tensorflow' == K.backend()):\n    import tensorflow as tf\n    from keras.backend.tensorflow_backend import set_session\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    sess = tf.Session(config=config)\n\ndef build_model():\n    model = Sequential()\n    model.add(Conv2D(6, (5, 5), padding='valid', activation = 'relu', kernel_initializer='he_normal', input_shape=(32,32,3)))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n    model.add(Conv2D(16, (5, 5), padding='valid', activation = 'relu', kernel_initializer='he_normal'))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(120, activation = 'relu', kernel_initializer='he_normal'))\n    model.add(Dense(84, activation = 'relu', kernel_initializer='he_normal'))\n    model.add(Dense(10, activation = 'softmax', kernel_initializer='he_normal'))\n    sgd = SGD(lr=.1, momentum=0.9, nesterov=True)\n    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n    return model\n\ndef scheduler(epoch):\n    if epoch < 81:\n        return 0.05\n    if epoch < 122:\n        return 0.005\n    return 0.0005\n\nif __name__ == '__main__':\n\n    # load data\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n    y_train = keras.utils.to_categorical(y_train, 10)\n    y_test = keras.utils.to_categorical(y_test, 10)\n    x_train = x_train.astype('float32') / 255.0\n    x_test = x_test.astype('float32') / 255.0\n\n    # build network\n    model = build_model()\n    \n    # print summary\n    print(model.summary())\n    \n    # save model picture\n    from keras.utils import plot_model\n    plot_model(model, to_file='model.png')\n\n\n    # set callback\n    tb_cb = TensorBoard(log_dir='./lenet', histogram_freq=0)\n    change_lr = LearningRateScheduler(scheduler)\n    cbks = [change_lr,tb_cb]\n\n    # start traing \n    model.fit(x_train, y_train,batch_size=128,epochs=200,callbacks=cbks,\n                  validation_data=(x_test, y_test), shuffle=True)\n    # save model\n    model.save('lenet.h5')\n\n\n"""
htd/ResNet.py,2,"b'import keras\r\nimport argparse\r\nimport math\r\nimport numpy as np\r\nfrom keras.datasets import cifar10, cifar100\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.layers.normalization import BatchNormalization\r\nfrom keras.layers import Conv2D, Dense, Input, add, Activation, GlobalAveragePooling2D\r\nfrom keras.callbacks import LearningRateScheduler, TensorBoard, ModelCheckpoint\r\nfrom keras.models import Model\r\nfrom keras import optimizers, regularizers\r\nfrom keras import backend as K\r\n\r\n# set GPU memory \r\nif(\'tensorflow\' == K.backend()):\r\n    import tensorflow as tf\r\n    from keras.backend.tensorflow_backend import set_session\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    sess = tf.Session(config=config)\r\n\r\n# set parameters via parser\r\nparser = argparse.ArgumentParser(description=\'Residual Network Training code(Keras version)\')\r\nparser.add_argument(\'-b\',\'--batch_size\', type=int, default=128, metavar=\'NUMBER\',\r\n                help=\'batch size(default: 128)\')\r\nparser.add_argument(\'-e\',\'--epochs\', type=int, default=200, metavar=\'NUMBER\',\r\n                help=\'epochs(default: 200)\')\r\nparser.add_argument(\'-n\',\'--stack_n\', type=int, default=5, metavar=\'NUMBER\',\r\n                help=\'stack number n, total layers = 6 * n + 2 (default: 5)\')\r\nparser.add_argument(\'-d\',\'--dataset\', type=str, default=""cifar10"", metavar=\'STRING\',\r\n                help=\'dataset. (default: cifar10)\')\r\nparser.add_argument(\'-s\',\'--scheduler\', type=str, default=""step_decay"", metavar=\'STRING\',\r\n                help=\'learning rate scheduler. [step_decay, cos or tanh] (default: step_decay)\')\r\nparser.add_argument(\'-c\',\'--count\', type=int, default=1, metavar=\'NUMBER\',\r\n                help=\'runs number. (default: 1)\')\r\n\r\nargs = parser.parse_args()\r\n\r\nstack_n            = args.stack_n\r\nbatch_size         = args.batch_size\r\nepochs             = args.epochs\r\nscheduler          = args.scheduler\r\nlayers             = 6 * stack_n + 2\r\nimg_rows, img_cols = 32, 32\r\nimg_channels       = 3 \r\niterations         = 50000 // batch_size + 1\r\nweight_decay       = 1e-4\r\n\r\n# param for cos & tanh\r\nstart_lr           = 0.1\r\nend_lr             = 0.0\r\n\r\n# learning rate scheduler\r\n# step_decay , cos and tanh\r\ndef step_decay_scheduler(epoch):\r\n    if epoch < 81:\r\n        return 0.1\r\n    if epoch < 122:\r\n        return 0.01\r\n    return 0.001\r\n\r\ndef cos_scheduler(epoch):\r\n    return (start_lr+end_lr)/2.+(start_lr-end_lr)/2.*math.cos(math.pi/2.0*(epoch/(epochs/2.0)))\r\n\r\ndef tanh_scheduler(epoch):\r\n    start = -6.0\r\n    end = 3.0\r\n    return start_lr / 2.0 * (1- math.tanh( (end-start)*epoch/epochs + start))\r\n\r\nclass AccHistory(keras.callbacks.Callback):\r\n    def on_train_begin(self, logs={}):\r\n        self.val_acc = {\'batch\':[], \'epoch\':[]}\r\n\r\n    def on_batch_end(self, batch, logs={}):\r\n        self.val_acc[\'batch\'].append(logs.get(\'val_acc\'))\r\n\r\n    def on_epoch_end(self, batch, logs={}):\r\n        self.val_acc[\'epoch\'].append(logs.get(\'val_acc\'))\r\n\r\n    def print_best_acc(self):\r\n        print(\'== BEST ACC: {:.4f} ==\'.format(max(self.val_acc[\'epoch\'])))\r\n\r\n# residual networks\r\ndef residual_network(img_input,classes_num=10,stack_n=5):\r\n    \r\n    def bn_relu(x):\r\n    \tx = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\r\n    \treturn Activation(\'relu\')(x)\r\n    def conv(x,out,size,stride):\r\n    \treturn Conv2D(out,kernel_size=size,strides=stride,\r\n    \t\t   padding=\'same\',\r\n    \t\t   kernel_initializer=""he_normal"",\r\n    \t\t   kernel_regularizer=regularizers.l2(weight_decay)\r\n    \t\t   )(x)\r\n\r\n    def residual_block(x,o_filters,increase=False):\r\n        stride = (2,2) if increase else (1,1)\r\n     \r\n        o1     = bn_relu(x)\r\n        conv_1 = conv(o1,o_filters,(3,3),stride) \r\n        o2     = bn_relu(conv_1)\r\n        conv_2 = conv(o2,o_filters,(3,3),(1,1))\r\n     \r\n        if increase:\r\n            projection = conv(o1,o_filters,(1,1),(2,2))\r\n            block = add([conv_2, projection])\r\n        else:\r\n            block = add([conv_2, x])\r\n        return block\r\n\r\n    # build model ( total layers = stack_n * 3 * 2 + 2 )\r\n    # stack_n = 5 by default, total layers = 32\r\n    # input: 32x32x3 output: 32x32x16\r\n\r\n    x = Conv2D(filters=16,kernel_size=(3,3),strides=(1,1),padding=\'same\',\r\n               kernel_initializer=""he_normal"",\r\n               kernel_regularizer=regularizers.l2(weight_decay))(img_input)\r\n\r\n    # input: 32x32x16 output: 32x32x16\r\n    for _ in range(stack_n):\r\n        x = residual_block(x,16,False)\r\n\r\n    # input: 32x32x16 output: 16x16x32\r\n    x = residual_block(x,32,True)\r\n    for _ in range(1,stack_n):\r\n        x = residual_block(x,32,False)\r\n    \r\n    # input: 16x16x32 output: 8x8x64\r\n    x = residual_block(x,64,True)\r\n    for _ in range(1,stack_n):\r\n        x = residual_block(x,64,False)\r\n\r\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\r\n    x = Activation(\'relu\')(x)\r\n    x = GlobalAveragePooling2D()(x)\r\n\r\n    # input: 64 output: 10\r\n    out = Dense(classes_num,activation=\'softmax\',\r\n    \t      kernel_initializer=""he_normal"",\r\n              kernel_regularizer=regularizers.l2(weight_decay))(x)\r\n    return out\r\n\r\nif __name__ == \'__main__\':\r\n\r\n    print(""\\n========================================"") \r\n    print(""MODEL: Residual Network ({:2d} layers)"".format(6*stack_n+2)) \r\n    print(""BATCH SIZE: {:3d}"".format(batch_size)) \r\n    print(""WEIGHT DECAY: {:.4f}"".format(weight_decay))\r\n    print(""EPOCHS: {:3d}"".format(epochs))\r\n    print(""DATASET: {:}"".format(args.dataset))\r\n    print(""LEARNING RATE SCHEDULER: {:}"".format(args.scheduler))\r\n\r\n\r\n    print(""\\n== LOADING DATA... =="")\r\n    \r\n    # load data cifar-10 or cifar-100\r\n    if args.dataset == ""cifar100"":\r\n        num_classes = 100\r\n        mean = [129.3, 124.1, 112.4]\r\n        std  = [68.2, 65.4, 70.4]\r\n        (x_train, y_train), (x_test, y_test) = cifar100.load_data()\r\n    else:\r\n        num_classes = 10\r\n        mean = [125.3, 123.0, 113.9]\r\n        std  = [63.0, 62.1, 66.7]\r\n        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n\r\n    y_train = keras.utils.to_categorical(y_train, num_classes)\r\n    y_test = keras.utils.to_categorical(y_test, num_classes)\r\n\r\n    print(""\\n== DONE! ==\\n\\n== COLOR PREPROCESSING... =="")\r\n   \r\n    # color preprocessing\r\n    x_train = x_train.astype(\'float32\')\r\n    x_test = x_test.astype(\'float32\')\r\n    for i in range(3):\r\n        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\r\n        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\r\n\r\n    print(""\\n== DONE! ==\\n\\n== BUILD MODEL... =="")\r\n   \r\n    # build network\r\n    img_input = Input(shape=(img_rows,img_cols,img_channels))\r\n    output    = residual_network(img_input,num_classes,stack_n)\r\n    resnet    = Model(img_input, output)\r\n    \r\n    # print model architecture if you need.\r\n    # print(resnet.summary())\r\n\r\n    # set optimizer\r\n    sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\r\n    resnet.compile(loss=\'categorical_crossentropy\', optimizer=sgd, metrics=[\'accuracy\'])\r\n\r\n    # set callback\r\n    if scheduler == \'step_decay\':\r\n         lrs = LearningRateScheduler(step_decay_scheduler)\r\n    elif scheduler == \'cos\':\r\n         lrs = LearningRateScheduler(cos_scheduler)\r\n    elif scheduler == \'tanh\':\r\n         lrs = LearningRateScheduler(tanh_scheduler)\r\n\r\n    tb   = TensorBoard(log_dir=\'./resnet_{:d}_{}_{}_{}/\'.format(layers,args.dataset,scheduler,args.count), histogram_freq=0)\r\n    ckpt = ModelCheckpoint(\'resnet_{:d}_{}_{}_{}.h5\'.format(layers,args.dataset,scheduler,args.count), \r\n                            monitor=\'val_acc\', verbose=0, save_best_only=True, mode=\'auto\', period=1)\r\n    his  = AccHistory()\r\n    cbks = [tb,lrs,ckpt,his]\r\n    \r\n    # set data augmentation\r\n    print(""\\n== USING REAL-TIME DATA AUGMENTATION, START TRAIN... =="")\r\n    datagen = ImageDataGenerator(horizontal_flip=True,\r\n                                 width_shift_range=0.125,\r\n                                 height_shift_range=0.125,\r\n                                 fill_mode=\'constant\',cval=0.)\r\n\r\n    datagen.fit(x_train)\r\n\r\n    # start training\r\n    resnet.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\r\n                         steps_per_epoch=iterations,\r\n                         epochs=epochs,\r\n                         callbacks=cbks,\r\n                         validation_data=(x_test, y_test))\r\n    his.print_best_acc()\r\n    print(""\\n== FINISH TRAINING! =="") '"
