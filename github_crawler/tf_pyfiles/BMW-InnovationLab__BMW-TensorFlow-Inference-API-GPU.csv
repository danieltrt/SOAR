file_path,api_count,code
src/main/deep_learning_service.py,0,"b'import os\nimport re\nimport json\nimport uuid\nfrom inference.inference_engines_factory import InferenceEngineFactory\nfrom inference.exceptions import ModelNotFound, InvalidModelConfiguration, ModelNotLoaded, InferenceEngineNotFound, \\\n    InvalidInputData, ApplicationError\n\n\nclass DeepLearningService:\n\n    def __init__(self):\n        """"""\n        Sets the models base directory, and initializes some dictionaries.\n        Saves the loaded model\'s hashes to a json file, so the values are saved even though the API went down.\n        """"""\n        # dictionary to hold the model instances (model_name: string -> model_instance: AbstractInferenceEngine)\n        self.models_dict = {}\n        # read from json file and append to dict\n        file_name = \'model_hash.json\'\n        file_exists = os.path.exists(file_name)\n        if file_exists:\n            try:\n                with open(file_name) as json_file:\n                    self.models_hash_dict = json.load(json_file)\n            except:\n                self.models_hash_dict = {}\n        else:\n            with open(\'model_hash.json\', \'w\'):\n                self.models_hash_dict = {}\n        self.labels_hash_dict = {}\n        self.base_models_dir = \'/models\'\n\n    def load_model(self, model_name, force_reload=False):\n        """"""\n        Loads a model by passing the model path to the factory.\n        The factory will return a loaded model instance that will be stored in a dictionary.\n        :param model_name: Model name\n        :param force_reload: Boolean to specify if we need to reload a model on each call\n        :return: Boolean\n        """"""\n        if not force_reload and self.model_loaded(model_name):\n            return True\n        model_path = os.path.join(self.base_models_dir, model_name)\n        try:\n            self.models_dict[model_name] = InferenceEngineFactory.get_engine(model_path)\n            return True\n        except ApplicationError as e:\n            raise e\n\n    def load_all_models(self):\n        """"""\n        Loads all the available models.\n        :return: Returns a List of all models and their respective hashed values\n        """"""\n        self.load_models(self.list_models())\n        models = self.list_models()\n        for model in models:\n            if model not in self.models_hash_dict:\n                self.models_hash_dict[model] = str(uuid.uuid4())\n        for key in list(self.models_hash_dict):\n            if key not in models:\n                del self.models_hash_dict[key]\n        # append to json file\n        with open(\'model_hash.json\', ""w"") as fp:\n            json.dump(self.models_hash_dict, fp)\n        return self.models_hash_dict\n\n    def load_models(self, model_names):\n        """"""\n        Loads a set of available models.\n        :param model_names: List of available models\n        :return:\n        """"""\n        for model in model_names:\n            self.load_model(model)\n\n    async def run_model(self, model_name, input_data, draw, predict_batch):\n        """"""\n        Loads the model in case it was never loaded and calls the inference engine class to get a prediction.\n        :param model_name: Model name\n        :param input_data: Batch of images or a single image\n        :param draw: Boolean to specify if we need to draw the response on the input image\n        :param predict_batch: Boolean to specify if there is a batch of images in a request or not\n        :return: Model response in case draw was set to False, else an actual image\n        """"""\n        if re.match(r\'[0-9a-fA-F]{8}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{12}\', model_name,\n                    flags=0):\n            for key, value in self.models_hash_dict.items():\n                if value == model_name:\n                    model_name = key\n        if self.model_loaded(model_name):\n            try:\n                if predict_batch:\n                    return await self.models_dict[model_name].run_batch(input_data, draw, predict_batch)\n                else:\n                    if not draw:\n                        return await self.models_dict[model_name].infer(input_data, draw, predict_batch)\n                    else:\n                        await self.models_dict[model_name].infer(input_data, draw, predict_batch)\n            except ApplicationError as e:\n                raise e\n        else:\n            try:\n                self.load_model(model_name)\n                return await self.run_model(model_name, input_data, draw, predict_batch)\n            except ApplicationError as e:\n                raise e\n\n    def list_models(self):\n        """"""\n        Lists all the available models.\n        :return: List of models\n        """"""\n        return [folder for folder in os.listdir(self.base_models_dir) if\n                os.path.isdir(os.path.join(self.base_models_dir, folder))]\n\n    def model_loaded(self, model_name):\n        """"""\n        Returns the model in case it was loaded.\n        :param model_name: Model name\n        :return: Model name\n        """"""\n        return model_name in self.models_dict.keys()\n\n    def get_labels(self, model_name):\n        """"""\n        Loads the model in case it\'s not loaded.\n        Returns the model\'s labels.\n        :param model_name: Model name\n        :return: List of model labels\n        """"""\n        if not self.model_loaded(model_name):\n            self.load_model(model_name)\n        return self.models_dict[model_name].labels\n\n    def get_labels_custom(self, model_name):\n        """"""\n        Hashes every label of a specific model.\n        :param model_name: Model name\n        :return: A list of mode\'s labels with their hashed values\n        """"""\n        if re.match(r\'[0-9a-fA-F]{8}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{12}\', model_name,\n                    flags=0):\n            for key, value in self.models_hash_dict.items():\n                if value == model_name:\n                    model_name = key\n        models = self.list_models()\n        if model_name not in self.labels_hash_dict:\n            model_dict = {}\n            for label in self.models_dict[model_name].labels:\n                model_dict[label] = str(uuid.uuid4())\n            self.labels_hash_dict[model_name] = model_dict\n        for key in list(self.labels_hash_dict):\n            if key not in models:\n                del self.labels_hash_dict[key]\n        return self.labels_hash_dict[model_name]\n\n    def get_config(self, model_name):\n        """"""\n        Returns the model\'s configuration.\n        :param model_name: Model name\n        :return: List of model\'s configuration\n        """"""\n        if not self.model_loaded(model_name):\n            self.load_model(model_name)\n        return self.models_dict[model_name].configuration\n'"
src/main/models.py,0,"b'class ApiResponse:\n\n    def __init__(self, success=True, data=None, error=None):\n        """"""\n        Defines the response shape\n        :param success: A boolean that returns if the request has succeeded or not\n        :param data: The model\'s response\n        :param error: The error in case an exception was raised\n        """"""\n        self.data = data\n        self.error = error.get_message() if error is not None else \'\'\n        self.success = success\n'"
src/main/start.py,0,"b'import sys\nfrom typing import List\nfrom models import ApiResponse\nfrom inference.errors import Error\nfrom starlette.responses import FileResponse\nfrom starlette.staticfiles import StaticFiles\nfrom starlette.middleware.cors import CORSMiddleware\nfrom deep_learning_service import DeepLearningService\nfrom fastapi import FastAPI, Form, File, UploadFile, Header\nfrom inference.exceptions import ModelNotFound, InvalidModelConfiguration, ApplicationError, ModelNotLoaded, \\\n\tInferenceEngineNotFound, InvalidInputData\n\n\nsys.path.append(\'./inference\')\n\ndl_service = DeepLearningService()\nerror_logging = Error()\napp = FastAPI(version=\'1.0\', title=\'BMW InnovationLab tensorflow gpu inference Automation\',\n\t\t\t  description=""<b>API for performing tensorflow gpu inference</b></br></br>""\n\t\t\t\t\t\t  ""<b>Contact the developers:</b></br>""\n\t\t\t\t\t\t  ""<b>Antoine Charbel: <a href=\'mailto:antoine.charbel@inmind.ai\'>antoine.charbel@inmind.ai</a></b></br>""\n\t\t\t\t\t\t  ""<b>BMW Innovation Lab: <a href=\'mailto:innovation-lab@bmw.de\'>innovation-lab@bmw.de</a></b>"")\n\n\n# app.mount(""/public"", StaticFiles(directory=""/main/public""), name=""public"")\n\n# app.add_middleware(\n#     CORSMiddleware,\n#     allow_origins=[""*""],\n#     allow_credentials=True,\n#     allow_methods=[""*""],\n#     allow_headers=[""*""],\n# )\n\n\n@app.get(\'/load\')\ndef load_custom():\n\t""""""\n\tLoads all the available models.\n\t:return: All the available models with their respective hashed values\n\t""""""\n\ttry:\n\t\treturn dl_service.load_all_models()\n\texcept ApplicationError as e:\n\t\treturn ApiResponse(success=False, error=e)\n\texcept Exception:\n\t\treturn ApiResponse(success=False, error=\'unexpected server error\')\n\n\n@app.post(\'/detect\')\nasync def detect_custom(model: str = Form(...), image: UploadFile = File(...)):\n\t""""""\n\tPerforms a prediction for a specified image using one of the available models.\n\t:param model: Model name or model hash\n\t:param image: Image file\n\t:return: Model\'s Bounding boxes\n\t""""""\n\tdraw_boxes = False\n\tpredict_batch = False\n\ttry:\n\t\toutput = await dl_service.run_model(model, image, draw_boxes, predict_batch)\n\t\terror_logging.info(\'request successful;\' + str(output))\n\t\treturn output\n\texcept ApplicationError as e:\n\t\terror_logging.warning(model + \';\' + str(e))\n\t\treturn ApiResponse(success=False, error=e)\n\texcept Exception as e:\n\t\terror_logging.error(model + \' \' + str(e))\n\t\treturn ApiResponse(success=False, error=\'unexpected server error\')\n\n\n@app.post(\'/get_labels\')\ndef get_labels_custom(model: str = Form(...)):\n\t""""""\n\tLists the model\'s labels with their hashed values.\n\t:param model: Model name or model hash\n\t:return: A list of the model\'s labels with their hashed values\n\t""""""\n\treturn dl_service.get_labels_custom(model)\n\n\n@app.get(\'/models/{model_name}/load\')\nasync def load(model_name: str, force: bool = False):\n\t""""""\n\tLoads a model specified as a query parameter.\n\t:param model_name: Model name\n\t:param force: Boolean for model force reload on each call\n\t:return: APIResponse\n\t""""""\n\ttry:\n\t\tdl_service.load_model(model_name, force)\n\t\treturn ApiResponse(success=True)\n\texcept ApplicationError as e:\n\t\treturn ApiResponse(success=False, error=e)\n\n\n@app.get(\'/models\')\nasync def list_models(user_agent: str = Header(None)):\n\t""""""\n\tLists all available models.\n\t:param user_agent:\n\t:return: APIResponse\n\t""""""\n\treturn ApiResponse(data={\'models\': dl_service.list_models()})\n\n\n@app.post(\'/models/{model_name}/predict\')\nasync def run_model(model_name: str, input_data: UploadFile = File(...)):\n\t""""""\n\tPerforms a prediction by giving both model name and image file.\n\t:param model_name: Model name\n\t:param input_data: An image file\n\t:return: APIResponse containing the prediction\'s bounding boxes\n\t""""""\n\ttry:\n\t\toutput = await dl_service.run_model(model_name, input_data, draw=False, predict_batch=False)\n\t\terror_logging.info(\'request successful;\' + str(output))\n\t\treturn ApiResponse(data=output)\n\texcept ApplicationError as e:\n\t\terror_logging.warning(model_name + \';\' + str(e))\n\t\treturn ApiResponse(success=False, error=e)\n\texcept Exception as e:\n\t\terror_logging.error(model_name + \' \' + str(e))\n\t\treturn ApiResponse(success=False, error=\'unexpected server error\')\n\n\n@app.post(\'/models/{model_name}/predict_batch\', include_in_schema=False)\nasync def run_model_batch(model_name: str, input_data: List[UploadFile] = File(...)):\n\t""""""\n\tPerforms a prediction by giving both model name and image file(s).\n\t:param model_name: Model name\n\t:param input_data: A batch of image files or a single image file\n\t:return: APIResponse containing prediction(s) bounding boxes\n\t""""""\n\ttry:\n\t\toutput = await dl_service.run_model(model_name, input_data, draw=False, predict_batch=True)\n\t\terror_logging.info(\'request successful;\' + str(output))\n\t\treturn ApiResponse(data=output)\n\texcept ApplicationError as e:\n\t\terror_logging.warning(model_name + \';\' + str(e))\n\t\treturn ApiResponse(success=False, error=e)\n\texcept Exception as e:\n\t\tprint(e)\n\t\terror_logging.error(model_name + \' \' + str(e))\n\t\treturn ApiResponse(success=False, error=\'unexpected server error\')\n\n\n@app.post(\'/models/{model_name}/predict_image\')\nasync def predict_image(model_name: str, input_data: UploadFile = File(...)):\n\t""""""\n\tDraws bounding box(es) on image and returns it.\n\t:param model_name: Model name\n\t:param input_data: Image file\n\t:return: Image file\n\t""""""\n\ttry:\n\t\toutput = await dl_service.run_model(model_name, input_data, draw=True, predict_batch=False)\n\t\terror_logging.info(\'request successful;\' + str(output))\n\t\treturn FileResponse(""/main/result.jpg"", media_type=""image/jpg"")\n\texcept ApplicationError as e:\n\t\terror_logging.warning(model_name + \';\' + str(e))\n\t\treturn ApiResponse(success=False, error=e)\n\texcept Exception as e:\n\t\terror_logging.error(model_name + \' \' + str(e))\n\t\treturn ApiResponse(success=False, error=\'unexpected server error\')\n\n\n@app.get(\'/models/{model_name}/labels\')\nasync def list_model_labels(model_name: str):\n\t""""""\n\tLists all the model\'s labels.\n\t:param model_name: Model name\n\t:return: List of model\'s labels\n\t""""""\n\tlabels = dl_service.get_labels(model_name)\n\treturn ApiResponse(data=labels)\n\n\n@app.get(\'/models/{model_name}/config\')\nasync def list_model_config(model_name: str):\n\t""""""\n\tLists all the model\'s configuration.\n\t:param model_name: Model name\n\t:return: List of model\'s configuration\n\t""""""\n\tconfig = dl_service.get_config(model_name)\n\treturn ApiResponse(data=config)\n'"
src/main/inference/__init__.py,0,b''
src/main/inference/base_error.py,0,"b'import logging\nfrom datetime import datetime\nfrom abc import ABC, abstractmethod\n\n\nclass AbstractError(ABC):\n\n    def __init__(self):\n        """"""\n        Sets the logger file, level, and format.\n        The logging file will contain the logging level, request date, request status, and model response.\n        """"""\n        self.logger = logging.getLogger(\'logger\')\n        date = datetime.now().strftime(\'%Y-%m-%d\')\n        file_path = \'logs/\' + date + \'.log\'\n        self.handler = logging.FileHandler(file_path)\n        self.handler.setLevel(logging.INFO)\n        self.handler.setFormatter(logging.Formatter(""%(levelname)s;%(asctime)s;%(message)s""))\n        self.logger.addHandler(self.handler)\n\n    @abstractmethod\n    def info(self, message):\n        """"""\n        Logs an info message to the logging file.\n        :param message: Containing the request status and the model response\n        :return:\n        """"""\n        pass\n\n    @abstractmethod\n    def warning(self, message):\n        """"""\n        Logs a warning message to the logging file.\n        :param message: Containing the request status and the model response\n        :return:\n        """"""\n        pass\n\n    @abstractmethod\n    def error(self, message):\n        """"""\n        Logs an Error message to the logging file.\n        :param message: Containing the request status and the model response\n        :return:\n        """"""\n        pass\n'"
src/main/inference/base_inference_engine.py,0,"b'from abc import ABC, abstractmethod\nfrom inference.exceptions import InvalidModelConfiguration, ModelNotLoaded, ApplicationError\n\n\nclass AbstractInferenceEngine(ABC):\n\n\tdef __init__(self, model_path):\n\t\t""""""\n\t\tTakes a model path and calls the load function.\n\t\t:param model_path: The model\'s path\n\t\t:return:\n\t\t""""""\n\t\tself.labels = []\n\t\tself.configuration = {}\n\t\tself.model_path = model_path\n\t\ttry:\n\t\t\tself.validate_configuration()\n\t\texcept ApplicationError as e:\n\t\t\traise e\n\t\ttry:\n\t\t\tself.load()\n\t\texcept ApplicationError as e:\n\t\t\traise e\n\t\texcept Exception as e:\n\t\t\tprint(e)\n\t\t\traise ModelNotLoaded()\n\n\t@abstractmethod\n\tdef load(self):\n\t\t""""""\n\t\tLoads the model based on the underlying implementation.\n\t\t""""""\n\t\tpass\n\n\t@abstractmethod\n\tdef free(self):\n\t\t""""""\n\t\tPerforms any manual memory implementation required to when unloading a model.\n\t\tWill be called when the class\'s destructor is called.\n\t\t""""""\n\t\tpass\n\n\t@abstractmethod\n\tasync def infer(self, input_data, draw, predict_batch):\n\t\t""""""\n\t\tPerforms the required inference based on the underlying implementation of this class.\n\t\tCould be used to return classification predictions, object detection coordinates...\n\t\t:param predict_batch: Boolean\n\t\t:param input_data: A single image\n\t\t:param draw: Used to draw bounding boxes on image instead of returning them\n\t\t:return: A bounding-box\n\t\t""""""\n\t\tpass\n\n\t@abstractmethod\n\tasync def run_batch(self, input_data, draw, predict_batch):\n\t\t""""""\n\t\tIterates over images and returns a prediction for each one.\n\t\t:param predict_batch: Boolean\n\t\t:param input_data: List of images\n\t\t:param draw: Used to draw bounding boxes on image instead of returning them\n\t\t:return: List of bounding-boxes\n\t\t""""""\n\t\tpass\n\n\t@abstractmethod\n\tdef validate_configuration(self):\n\t\t""""""\n\t\tValidates that the model and its files are valid based on the underlying implementation\'s requirements.\n\t\tCan check for configuration values, folder structure...\n\t\t""""""\n\t\tpass\n\n\t@abstractmethod\n\tdef set_model_configuration(self, data):\n\t\t""""""\n\t\tTakes the configuration from the config.json file\n\t\t:param data: Json data\n\t\t:return:\n\t\t""""""\n\t\tpass\n\n\t@abstractmethod\n\tdef validate_json_configuration(self, data):\n\t\t""""""\n\t\tValidates the configuration of the config.json file.\n\t\t:param data: Json data\n\t\t:return:\n\t\t""""""\n\t\tpass\n\n\tdef __del__(self):\n\t\tself.free()\n'"
src/main/inference/errors.py,0,"b'import os\nimport logging\nfrom datetime import datetime\nfrom inference.base_error import AbstractError\n\n\nclass Error(AbstractError):\n\n    def __init__(self):\n        if \'logs\' not in os.listdir():\n            os.mkdir(\'logs\')\n        self.date = None\n        super().__init__()\n\n    def info(self, message):\n        self.check_date()\n        self.logger.info(message)\n\n    def warning(self, message):\n        self.check_date()\n        self.logger.warning(message)\n\n    def error(self, message):\n        self.check_date()\n        self.logger.error(message)\n\n    def check_date(self):\n        """"""\n        Divides logging per day. Each logging file corresponds to a specific day.\n        It also removes all logging files exceeding one year.\n        :return:\n        """"""\n        self.date = datetime.now().strftime(\'%Y-%m-%d\')\n        file_path = self.date + \'.log\'\n        if file_path not in os.listdir(\'logs\'):\n            self.logger.removeHandler(self.handler)\n            self.handler = logging.FileHandler(\'logs/\' + file_path)\n            self.handler.setLevel(logging.INFO)\n            self.handler.setFormatter(logging.Formatter(""%(levelname)s;%(asctime)s;%(message)s""))\n            self.logger.addHandler(self.handler)\n        oldest_log_file = os.listdir(\'logs\')[0]\n        oldest_date = oldest_log_file.split(\'.\')[0]\n        a = datetime.strptime(datetime.now().strftime(\'%Y-%m-%d\'), \'%Y-%m-%d\')\n        b = datetime.strptime(oldest_date, \'%Y-%m-%d\')\n        delta = a - b\n        if delta.days > 365:\n            os.remove(\'logs/\' + oldest_log_file)\n'"
src/main/inference/exceptions.py,0,"b'__metaclass__ = type\n\n\nclass ApplicationError(Exception):\n    """"""Base class for other exceptions""""""\n\n    def __init__(self, default_message, additional_message=\'\'):\n        self.default_message = default_message\n        self.additional_message = additional_message\n\n    def __str__(self):\n        return self.get_message()\n\n    def get_message(self):\n        return self.default_message if self.additional_message == \'\' else ""{}: {}"".format(self.default_message,\n                                                                                          self.additional_message)\n\n\nclass InvalidModelConfiguration(ApplicationError):\n    """"""Raised when the model\'s configuration is corrupted""""""\n\n    def __init__(self, additional_message=\'\'):\n        # super(\'Invalid model configuration\', additional_message)\n        super().__init__(\'Invalid model configuration\', additional_message)\n\n\nclass ModelNotFound(ApplicationError):\n    """"""Raised when the model is not found""""""\n\n    def __init__(self, additional_message=\'\'):\n        # super(\'Model not found\', additional_message)\n        super().__init__(\'Model not found\', additional_message)\n\n\nclass ModelNotLoaded(ApplicationError):\n    """"""Raised when the model is not loaded""""""\n\n    def __init__(self, additional_message=\'\'):\n        # super(\'Error loading model\', additional_message)\n        super().__init__(\'Error loading model\', additional_message)\n\n\nclass InvalidInputData(ApplicationError):\n    """"""Raised when the input data is corrupted""""""\n\n    def __init__(self, additional_message=\'\'):\n        # super(\'Invalid input data\', additional_message)\n        super().__init__(\'Invalid input data\', additional_message)\n\n\nclass InferenceEngineNotFound(ApplicationError):\n    """"""Raised when the Inference Engine is not found""""""\n\n    def __init__(self, additional_message=\'\'):\n        # super(\'Inference engine not found\', additional_message)\n        super().__init__(\'Inference engine not found\', additional_message)\n'"
src/main/inference/inference_engines_factory.py,0,"b'import os\nimport json\nfrom inference.exceptions import ModelNotFound, ApplicationError, InvalidModelConfiguration, InferenceEngineNotFound, ModelNotLoaded\n\n\nclass InferenceEngineFactory:\n\n    @staticmethod\n    def get_engine(path_to_model):\n        """"""\n        Reads the model\'s inference engine from the model\'s configuration and calls the right inference engine class.\n        :param path_to_model: Model\'s path\n        :return: The model\'s instance\n        """"""\n        if not os.path.exists(path_to_model):\n            raise ModelNotFound()\n        try:\n            configuration = json.loads(open(os.path.join(path_to_model, \'config.json\')).read())\n        except Exception:\n            raise InvalidModelConfiguration(\'config.json not found or corrupted\')\n        try:\n            inference_engine_name = configuration[\'inference_engine_name\']\n        except Exception:\n            raise InvalidModelConfiguration(\'missing inference engine name in config.json\')\n        try:\n            # import one of the available inference engine class (in this project there\'s only one), and return a\n            # model instance\n            return getattr(__import__(inference_engine_name), \'InferenceEngine\')(path_to_model)\n        except ApplicationError as e:\n            print(e)\n            raise e\n        except Exception as e:\n            print(e)\n            raise InferenceEngineNotFound(inference_engine_name)\n'"
src/main/inference/tensorflow_detection.py,5,"b'import os\nimport jsonschema\nimport asyncio\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom PIL import Image, ImageDraw, ImageFont\nfrom object_detection.utils import label_map_util\nfrom inference.base_inference_engine import AbstractInferenceEngine\nfrom inference.exceptions import InvalidModelConfiguration, InvalidInputData, ApplicationError\n\n\nclass InferenceEngine(AbstractInferenceEngine):\n\n\tdef __init__(self, model_path):\n\t\tself.label_path = """"\n\t\tself.NUM_CLASSES = None\n\t\tself.sess = None\n\t\tself.label_map = None\n\t\tself.categories = None\n\t\tself.category_index = None\n\t\tself.detection_graph = None\n\t\tself.image_tensor = None\n\t\tself.d_boxes = None\n\t\tself.d_scores = None\n\t\tself.d_classes = None\n\t\tself.num_d = None\n\t\tself.font = ImageFont.truetype(""/main/fonts/DejaVuSans.ttf"", 20)\n\t\tsuper().__init__(model_path)\n\n\tdef load(self):\n\t\twith open(os.path.join(self.model_path, \'config.json\')) as f:\n\t\t\tdata = json.load(f)\n\t\ttry:\n\t\t\tself.validate_json_configuration(data)\n\t\t\tself.set_model_configuration(data)\n\t\texcept ApplicationError as e:\n\t\t\traise e\n\n\t\tself.label_path = os.path.join(self.model_path, \'object-detection.pbtxt\')\n\t\tself.label_map = label_map_util.load_labelmap(self.label_path)\n\t\tself.categories = label_map_util.convert_label_map_to_categories(self.label_map,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t max_num_classes=self.NUM_CLASSES,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t use_display_name=True)\n\t\tfor dict in self.categories:\n\t\t\tself.labels.append(dict[\'name\'])\n\n\t\tself.category_index = label_map_util.create_category_index(self.categories)\n\t\tself.detection_graph = tf.Graph()\n\t\twith self.detection_graph.as_default():\n\t\t\tod_graph_def = tf.GraphDef()\n\t\t\twith tf.gfile.GFile(os.path.join(self.model_path, \'frozen_inference_graph.pb\'), \'rb\') as fid:\n\t\t\t\tserialized_graph = fid.read()\n\t\t\t\tod_graph_def.ParseFromString(serialized_graph)\n\t\t\t\ttf.import_graph_def(od_graph_def, name=\'\')\n\t\t\tself.image_tensor = self.detection_graph.get_tensor_by_name(\'image_tensor:0\')\n\t\t\tself.d_boxes = self.detection_graph.get_tensor_by_name(\'detection_boxes:0\')\n\t\t\tself.d_scores = self.detection_graph.get_tensor_by_name(\'detection_scores:0\')\n\t\t\tself.d_classes = self.detection_graph.get_tensor_by_name(\'detection_classes:0\')\n\t\t\tself.num_d = self.detection_graph.get_tensor_by_name(\'num_detections:0\')\n\t\tself.sess = tf.Session(graph=self.detection_graph)\n\t\timg = Image.open(""object_detection/image1.jpg"")\n\t\timg_expanded = np.expand_dims(img, axis=0)\n\t\tself.sess.run(\n\t\t\t[self.d_boxes, self.d_scores, self.d_classes, self.num_d],\n\t\t\tfeed_dict={self.image_tensor: img_expanded})\n\n\tasync def infer(self, input_data, draw, predict_batch):\n\t\tawait asyncio.sleep(0.00001)\n\t\ttry:\n\t\t\tpillow_image = Image.open(input_data.file).convert(\'RGB\')\n\t\t\tnp_image = np.array(pillow_image)\n\t\texcept Exception as e:\n\t\t\traise InvalidInputData(\'corrupted image\')\n\t\ttry:\n\t\t\twith open(self.model_path + \'/config.json\') as f:\n\t\t\t\tdata = json.load(f)\n\t\texcept Exception as e:\n\t\t\traise InvalidModelConfiguration(\'config.json not found or corrupted\')\n\t\tjson_confidence = data[\'confidence\']\n\t\tjson_predictions = data[\'predictions\']\n\t\twith self.detection_graph.as_default():\n\t\t\t# Expand dimension since the model expects image to have shape [1, None, None, 3].\n\t\t\timg_expanded = np.expand_dims(np_image, axis=0)\n\t\t\t(boxes, scores, classes, num) = self.sess.run(\n\t\t\t\t[self.d_boxes, self.d_scores, self.d_classes, self.num_d],\n\t\t\t\tfeed_dict={self.image_tensor: img_expanded})\n\t\tclasses_names = ([self.category_index.get(i) for i in classes[0]])\n\t\tnames_start = []\n\t\tfor name in classes_names:\n\t\t\tif name is not None:\n\t\t\t\tnames_start.append(name[\'name\'])\n\t\theight, width, depth = np_image.shape\n\t\tnames = []\n\t\tconfidence = []\n\t\tids = []\n\t\tbounding_boxes = []\n\t\tfor i in range(json_predictions):\n\t\t\tif scores[0][i] * 100 >= json_confidence:\n\t\t\t\tymin = int(round(boxes[0][i][0] * height)) if int(round(boxes[0][i][0] * height)) > 0 else 0\n\t\t\t\txmin = int(round(boxes[0][i][1] * width)) if int(round(boxes[0][i][1] * height)) > 0 else 0\n\t\t\t\tymax = int(round(boxes[0][i][2] * height)) if int(round(boxes[0][i][2] * height)) > 0 else 0\n\t\t\t\txmax = int(round(boxes[0][i][3] * width)) if int(round(boxes[0][i][3] * height)) > 0 else 0\n\t\t\t\ttmp = dict([(\'left\', xmin), (\'top\', ymin), (\'right\', xmax), (\'bottom\', ymax)])\n\t\t\t\tbounding_boxes.append(tmp)\n\t\t\t\tconfidence.append(float(scores[0][i] * 100))\n\t\t\t\tids.append(int(classes[0][i]))\n\t\t\t\tnames.append(names_start[i])\n\n\t\tresponses_list = zip(names, confidence, bounding_boxes, ids)\n\n\t\toutput = []\n\t\tfor response in responses_list:\n\t\t\ttmp = dict([(\'ObjectClassName\', response[0]), (\'confidence\', response[1]), (\'coordinates\', response[2]),\n\t\t\t\t\t\t(\'ObjectClassId\', response[3])])\n\t\t\toutput.append(tmp)\n\t\tif predict_batch:\n\t\t\tresponse = dict([(\'bounding-boxes\', output), (\'ImageName\', input_data.filename)])\n\t\telse:\n\t\t\tresponse = dict([(\'bounding-boxes\', output)])\n\n\t\tif not draw:\n\t\t\treturn response\n\t\telse:\n\t\t\ttry:\n\t\t\t\tself.draw_image(pillow_image, response)\n\t\t\texcept ApplicationError as e:\n\t\t\t\traise e\n\t\t\texcept Exception as e:\n\t\t\t\traise e\n\n\tasync def run_batch(self, input_data, draw, predict_batch):\n\t\tresult_list = []\n\t\tfor image in input_data:\n\t\t\tpost_process = await self.infer(image, draw, predict_batch)\n\t\t\tif post_process is not None:\n\t\t\t\tresult_list.append(post_process)\n\t\treturn result_list\n\n\tdef draw_image(self, image, response):\n\t\t""""""\n\t\tDraws on image and saves it.\n\t\t:param image: image of type pillow image\n\t\t:param response: inference response\n\t\t:return:\n\t\t""""""\n\t\tdraw = ImageDraw.Draw(image)\n\t\tfor bbox in response[\'bounding-boxes\']:\n\t\t\tdraw.rectangle([bbox[\'coordinates\'][\'left\'], bbox[\'coordinates\'][\'top\'], bbox[\'coordinates\'][\'right\'],\n\t\t\t\t\t\t\tbbox[\'coordinates\'][\'bottom\']], outline=""red"")\n\t\t\tleft = bbox[\'coordinates\'][\'left\']\n\t\t\ttop = bbox[\'coordinates\'][\'top\']\n\t\t\tconf = ""{0:.2f}"".format(bbox[\'confidence\'])\n\t\t\tdraw.text((int(left), int(top) - 20), str(conf) + ""% "" + str(bbox[\'ObjectClassName\']), \'red\', self.font)\n\t\timage.save(\'/main/result.jpg\', \'PNG\')\n\n\tdef free(self):\n\t\tpass\n\n\tdef validate_configuration(self):\n\t\t# check if weights file exists\n\t\tif not os.path.exists(os.path.join(self.model_path, \'frozen_inference_graph.pb\')):\n\t\t\traise InvalidModelConfiguration(\'frozen_inference_graph.pb not found\')\n\t\t# check if labels file exists\n\t\tif not os.path.exists(os.path.join(self.model_path, \'object-detection.pbtxt\')):\n\t\t\traise InvalidModelConfiguration(\'object-detection.pbtxt not found\')\n\t\treturn True\n\n\tdef set_model_configuration(self, data):\n\t\tself.configuration[\'framework\'] = data[\'framework\']\n\t\tself.configuration[\'type\'] = data[\'type\']\n\t\tself.configuration[\'network\'] = data[\'network\']\n\t\tself.NUM_CLASSES = data[\'number_of_classes\']\n\n\tdef validate_json_configuration(self, data):\n\t\twith open(os.path.join(\'inference\', \'ConfigurationSchema.json\')) as f:\n\t\t\tschema = json.load(f)\n\t\ttry:\n\t\t\tjsonschema.validate(data, schema)\n\t\texcept Exception as e:\n\t\t\traise InvalidModelConfiguration(e)\n'"
src/main/object_detection/core/__init__.py,0,b'\n'
src/main/object_detection/core/box_list.py,8,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Bounding Box List definition.\n\nBoxList represents a list of bounding boxes as tensorflow\ntensors, where each bounding box is represented as a row of 4 numbers,\n[y_min, x_min, y_max, x_max].  It is assumed that all bounding boxes\nwithin a given list correspond to a single image.  See also\nbox_list_ops.py for common box related operations (such as area, iou, etc).\n\nOptionally, users can add additional related fields (such as weights).\nWe assume the following things to be true about fields:\n* they correspond to boxes in the box_list along the 0th dimension\n* they have inferrable rank at graph construction time\n* all dimensions except for possibly the 0th can be inferred\n  (i.e., not None) at graph construction time.\n\nSome other notes:\n  * Following tensorflow conventions, we use height, width ordering,\n  and correspondingly, y,x (or ymin, xmin, ymax, xmax) ordering\n  * Tensors are always provided as (flat) [N, 4] tensors.\n""""""\n\nimport tensorflow as tf\n\n\nclass BoxList(object):\n  """"""Box collection.""""""\n\n  def __init__(self, boxes):\n    """"""Constructs box collection.\n\n    Args:\n      boxes: a tensor of shape [N, 4] representing box corners\n\n    Raises:\n      ValueError: if invalid dimensions for bbox data or if bbox data is not in\n          float32 format.\n    """"""\n    if len(boxes.get_shape()) != 2 or boxes.get_shape()[-1] != 4:\n      raise ValueError(\'Invalid dimensions for box data.\')\n    if boxes.dtype != tf.float32:\n      raise ValueError(\'Invalid tensor type: should be tf.float32\')\n    self.data = {\'boxes\': boxes}\n\n  def num_boxes(self):\n    """"""Returns number of boxes held in collection.\n\n    Returns:\n      a tensor representing the number of boxes held in the collection.\n    """"""\n    return tf.shape(self.data[\'boxes\'])[0]\n\n  def num_boxes_static(self):\n    """"""Returns number of boxes held in collection.\n\n    This number is inferred at graph construction time rather than run-time.\n\n    Returns:\n      Number of boxes held in collection (integer) or None if this is not\n        inferrable at graph construction time.\n    """"""\n    return self.data[\'boxes\'].get_shape()[0].value\n\n  def get_all_fields(self):\n    """"""Returns all fields.""""""\n    return self.data.keys()\n\n  def get_extra_fields(self):\n    """"""Returns all non-box fields (i.e., everything not named \'boxes\').""""""\n    return [k for k in self.data.keys() if k != \'boxes\']\n\n  def add_field(self, field, field_data):\n    """"""Add field to box list.\n\n    This method can be used to add related box data such as\n    weights/labels, etc.\n\n    Args:\n      field: a string key to access the data via `get`\n      field_data: a tensor containing the data to store in the BoxList\n    """"""\n    self.data[field] = field_data\n\n  def has_field(self, field):\n    return field in self.data\n\n  def get(self):\n    """"""Convenience function for accessing box coordinates.\n\n    Returns:\n      a tensor with shape [N, 4] representing box coordinates.\n    """"""\n    return self.get_field(\'boxes\')\n\n  def set(self, boxes):\n    """"""Convenience function for setting box coordinates.\n\n    Args:\n      boxes: a tensor of shape [N, 4] representing box corners\n\n    Raises:\n      ValueError: if invalid dimensions for bbox data\n    """"""\n    if len(boxes.get_shape()) != 2 or boxes.get_shape()[-1] != 4:\n      raise ValueError(\'Invalid dimensions for box data.\')\n    self.data[\'boxes\'] = boxes\n\n  def get_field(self, field):\n    """"""Accesses a box collection and associated fields.\n\n    This function returns specified field with object; if no field is specified,\n    it returns the box coordinates.\n\n    Args:\n      field: this optional string parameter can be used to specify\n        a related field to be accessed.\n\n    Returns:\n      a tensor representing the box collection or an associated field.\n\n    Raises:\n      ValueError: if invalid field\n    """"""\n    if not self.has_field(field):\n      raise ValueError(\'field \' + str(field) + \' does not exist\')\n    return self.data[field]\n\n  def set_field(self, field, value):\n    """"""Sets the value of a field.\n\n    Updates the field of a box_list with a given value.\n\n    Args:\n      field: (string) name of the field to set value.\n      value: the value to assign to the field.\n\n    Raises:\n      ValueError: if the box_list does not have specified field.\n    """"""\n    if not self.has_field(field):\n      raise ValueError(\'field %s does not exist\' % field)\n    self.data[field] = value\n\n  def get_center_coordinates_and_sizes(self, scope=None):\n    """"""Computes the center coordinates, height and width of the boxes.\n\n    Args:\n      scope: name scope of the function.\n\n    Returns:\n      a list of 4 1-D tensors [ycenter, xcenter, height, width].\n    """"""\n    with tf.name_scope(scope, \'get_center_coordinates_and_sizes\'):\n      box_corners = self.get()\n      ymin, xmin, ymax, xmax = tf.unstack(tf.transpose(box_corners))\n      width = xmax - xmin\n      height = ymax - ymin\n      ycenter = ymin + height / 2.\n      xcenter = xmin + width / 2.\n      return [ycenter, xcenter, height, width]\n\n  def transpose_coordinates(self, scope=None):\n    """"""Transpose the coordinate representation in a boxlist.\n\n    Args:\n      scope: name scope of the function.\n    """"""\n    with tf.name_scope(scope, \'transpose_coordinates\'):\n      y_min, x_min, y_max, x_max = tf.split(\n          value=self.get(), num_or_size_splits=4, axis=1)\n      self.set(tf.concat([x_min, y_min, x_max, y_max], 1))\n\n  def as_tensor_dict(self, fields=None):\n    """"""Retrieves specified fields as a dictionary of tensors.\n\n    Args:\n      fields: (optional) list of fields to return in the dictionary.\n        If None (default), all fields are returned.\n\n    Returns:\n      tensor_dict: A dictionary of tensors specified by fields.\n\n    Raises:\n      ValueError: if specified field is not contained in boxlist.\n    """"""\n    tensor_dict = {}\n    if fields is None:\n      fields = self.get_all_fields()\n    for field in fields:\n      if not self.has_field(field):\n        raise ValueError(\'boxlist must contain all specified fields\')\n      tensor_dict[field] = self.get_field(field)\n    return tensor_dict\n'"
src/main/object_detection/core/box_list_ops.py,165,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Bounding Box List operations.\n\nExample box operations that are supported:\n  * areas: compute bounding box areas\n  * iou: pairwise intersection-over-union scores\n  * sq_dist: pairwise distances between bounding boxes\n\nWhenever box_list_ops functions output a BoxList, the fields of the incoming\nBoxList are retained unless documented otherwise.\n""""""\nimport tensorflow as tf\n\nfrom object_detection.core import box_list\nfrom object_detection.utils import shape_utils\n\n\nclass SortOrder(object):\n  """"""Enum class for sort order.\n\n  Attributes:\n    ascend: ascend order.\n    descend: descend order.\n  """"""\n  ascend = 1\n  descend = 2\n\n\ndef area(boxlist, scope=None):\n  """"""Computes area of boxes.\n\n  Args:\n    boxlist: BoxList holding N boxes\n    scope: name scope.\n\n  Returns:\n    a tensor with shape [N] representing box areas.\n  """"""\n  with tf.name_scope(scope, \'Area\'):\n    y_min, x_min, y_max, x_max = tf.split(\n        value=boxlist.get(), num_or_size_splits=4, axis=1)\n    return tf.squeeze((y_max - y_min) * (x_max - x_min), [1])\n\n\ndef height_width(boxlist, scope=None):\n  """"""Computes height and width of boxes in boxlist.\n\n  Args:\n    boxlist: BoxList holding N boxes\n    scope: name scope.\n\n  Returns:\n    Height: A tensor with shape [N] representing box heights.\n    Width: A tensor with shape [N] representing box widths.\n  """"""\n  with tf.name_scope(scope, \'HeightWidth\'):\n    y_min, x_min, y_max, x_max = tf.split(\n        value=boxlist.get(), num_or_size_splits=4, axis=1)\n    return tf.squeeze(y_max - y_min, [1]), tf.squeeze(x_max - x_min, [1])\n\n\ndef scale(boxlist, y_scale, x_scale, scope=None):\n  """"""scale box coordinates in x and y dimensions.\n\n  Args:\n    boxlist: BoxList holding N boxes\n    y_scale: (float) scalar tensor\n    x_scale: (float) scalar tensor\n    scope: name scope.\n\n  Returns:\n    boxlist: BoxList holding N boxes\n  """"""\n  with tf.name_scope(scope, \'Scale\'):\n    y_scale = tf.cast(y_scale, tf.float32)\n    x_scale = tf.cast(x_scale, tf.float32)\n    y_min, x_min, y_max, x_max = tf.split(\n        value=boxlist.get(), num_or_size_splits=4, axis=1)\n    y_min = y_scale * y_min\n    y_max = y_scale * y_max\n    x_min = x_scale * x_min\n    x_max = x_scale * x_max\n    scaled_boxlist = box_list.BoxList(\n        tf.concat([y_min, x_min, y_max, x_max], 1))\n    return _copy_extra_fields(scaled_boxlist, boxlist)\n\n\ndef clip_to_window(boxlist, window, filter_nonoverlapping=True, scope=None):\n  """"""Clip bounding boxes to a window.\n\n  This op clips any input bounding boxes (represented by bounding box\n  corners) to a window, optionally filtering out boxes that do not\n  overlap at all with the window.\n\n  Args:\n    boxlist: BoxList holding M_in boxes\n    window: a tensor of shape [4] representing the [y_min, x_min, y_max, x_max]\n      window to which the op should clip boxes.\n    filter_nonoverlapping: whether to filter out boxes that do not overlap at\n      all with the window.\n    scope: name scope.\n\n  Returns:\n    a BoxList holding M_out boxes where M_out <= M_in\n  """"""\n  with tf.name_scope(scope, \'ClipToWindow\'):\n    y_min, x_min, y_max, x_max = tf.split(\n        value=boxlist.get(), num_or_size_splits=4, axis=1)\n    win_y_min, win_x_min, win_y_max, win_x_max = tf.unstack(window)\n    y_min_clipped = tf.maximum(tf.minimum(y_min, win_y_max), win_y_min)\n    y_max_clipped = tf.maximum(tf.minimum(y_max, win_y_max), win_y_min)\n    x_min_clipped = tf.maximum(tf.minimum(x_min, win_x_max), win_x_min)\n    x_max_clipped = tf.maximum(tf.minimum(x_max, win_x_max), win_x_min)\n    clipped = box_list.BoxList(\n        tf.concat([y_min_clipped, x_min_clipped, y_max_clipped, x_max_clipped],\n                  1))\n    clipped = _copy_extra_fields(clipped, boxlist)\n    if filter_nonoverlapping:\n      areas = area(clipped)\n      nonzero_area_indices = tf.cast(\n          tf.reshape(tf.where(tf.greater(areas, 0.0)), [-1]), tf.int32)\n      clipped = gather(clipped, nonzero_area_indices)\n    return clipped\n\n\ndef prune_outside_window(boxlist, window, scope=None):\n  """"""Prunes bounding boxes that fall outside a given window.\n\n  This function prunes bounding boxes that even partially fall outside the given\n  window. See also clip_to_window which only prunes bounding boxes that fall\n  completely outside the window, and clips any bounding boxes that partially\n  overflow.\n\n  Args:\n    boxlist: a BoxList holding M_in boxes.\n    window: a float tensor of shape [4] representing [ymin, xmin, ymax, xmax]\n      of the window\n    scope: name scope.\n\n  Returns:\n    pruned_corners: a tensor with shape [M_out, 4] where M_out <= M_in\n    valid_indices: a tensor with shape [M_out] indexing the valid bounding boxes\n     in the input tensor.\n  """"""\n  with tf.name_scope(scope, \'PruneOutsideWindow\'):\n    y_min, x_min, y_max, x_max = tf.split(\n        value=boxlist.get(), num_or_size_splits=4, axis=1)\n    win_y_min, win_x_min, win_y_max, win_x_max = tf.unstack(window)\n    coordinate_violations = tf.concat([\n        tf.less(y_min, win_y_min), tf.less(x_min, win_x_min),\n        tf.greater(y_max, win_y_max), tf.greater(x_max, win_x_max)\n    ], 1)\n    valid_indices = tf.reshape(\n        tf.where(tf.logical_not(tf.reduce_any(coordinate_violations, 1))), [-1])\n    return gather(boxlist, valid_indices), valid_indices\n\n\ndef prune_completely_outside_window(boxlist, window, scope=None):\n  """"""Prunes bounding boxes that fall completely outside of the given window.\n\n  The function clip_to_window prunes bounding boxes that fall\n  completely outside the window, but also clips any bounding boxes that\n  partially overflow. This function does not clip partially overflowing boxes.\n\n  Args:\n    boxlist: a BoxList holding M_in boxes.\n    window: a float tensor of shape [4] representing [ymin, xmin, ymax, xmax]\n      of the window\n    scope: name scope.\n\n  Returns:\n    pruned_boxlist: a new BoxList with all bounding boxes partially or fully in\n      the window.\n    valid_indices: a tensor with shape [M_out] indexing the valid bounding boxes\n     in the input tensor.\n  """"""\n  with tf.name_scope(scope, \'PruneCompleteleyOutsideWindow\'):\n    y_min, x_min, y_max, x_max = tf.split(\n        value=boxlist.get(), num_or_size_splits=4, axis=1)\n    win_y_min, win_x_min, win_y_max, win_x_max = tf.unstack(window)\n    coordinate_violations = tf.concat([\n        tf.greater_equal(y_min, win_y_max), tf.greater_equal(x_min, win_x_max),\n        tf.less_equal(y_max, win_y_min), tf.less_equal(x_max, win_x_min)\n    ], 1)\n    valid_indices = tf.reshape(\n        tf.where(tf.logical_not(tf.reduce_any(coordinate_violations, 1))), [-1])\n    return gather(boxlist, valid_indices), valid_indices\n\n\ndef intersection(boxlist1, boxlist2, scope=None):\n  """"""Compute pairwise intersection areas between boxes.\n\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n    scope: name scope.\n\n  Returns:\n    a tensor with shape [N, M] representing pairwise intersections\n  """"""\n  with tf.name_scope(scope, \'Intersection\'):\n    y_min1, x_min1, y_max1, x_max1 = tf.split(\n        value=boxlist1.get(), num_or_size_splits=4, axis=1)\n    y_min2, x_min2, y_max2, x_max2 = tf.split(\n        value=boxlist2.get(), num_or_size_splits=4, axis=1)\n    all_pairs_min_ymax = tf.minimum(y_max1, tf.transpose(y_max2))\n    all_pairs_max_ymin = tf.maximum(y_min1, tf.transpose(y_min2))\n    intersect_heights = tf.maximum(0.0, all_pairs_min_ymax - all_pairs_max_ymin)\n    all_pairs_min_xmax = tf.minimum(x_max1, tf.transpose(x_max2))\n    all_pairs_max_xmin = tf.maximum(x_min1, tf.transpose(x_min2))\n    intersect_widths = tf.maximum(0.0, all_pairs_min_xmax - all_pairs_max_xmin)\n    return intersect_heights * intersect_widths\n\n\ndef matched_intersection(boxlist1, boxlist2, scope=None):\n  """"""Compute intersection areas between corresponding boxes in two boxlists.\n\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding N boxes\n    scope: name scope.\n\n  Returns:\n    a tensor with shape [N] representing pairwise intersections\n  """"""\n  with tf.name_scope(scope, \'MatchedIntersection\'):\n    y_min1, x_min1, y_max1, x_max1 = tf.split(\n        value=boxlist1.get(), num_or_size_splits=4, axis=1)\n    y_min2, x_min2, y_max2, x_max2 = tf.split(\n        value=boxlist2.get(), num_or_size_splits=4, axis=1)\n    min_ymax = tf.minimum(y_max1, y_max2)\n    max_ymin = tf.maximum(y_min1, y_min2)\n    intersect_heights = tf.maximum(0.0, min_ymax - max_ymin)\n    min_xmax = tf.minimum(x_max1, x_max2)\n    max_xmin = tf.maximum(x_min1, x_min2)\n    intersect_widths = tf.maximum(0.0, min_xmax - max_xmin)\n    return tf.reshape(intersect_heights * intersect_widths, [-1])\n\n\ndef iou(boxlist1, boxlist2, scope=None):\n  """"""Computes pairwise intersection-over-union between box collections.\n\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n    scope: name scope.\n\n  Returns:\n    a tensor with shape [N, M] representing pairwise iou scores.\n  """"""\n  with tf.name_scope(scope, \'IOU\'):\n    intersections = intersection(boxlist1, boxlist2)\n    areas1 = area(boxlist1)\n    areas2 = area(boxlist2)\n    unions = (\n        tf.expand_dims(areas1, 1) + tf.expand_dims(areas2, 0) - intersections)\n    return tf.where(\n        tf.equal(intersections, 0.0),\n        tf.zeros_like(intersections), tf.truediv(intersections, unions))\n\n\ndef matched_iou(boxlist1, boxlist2, scope=None):\n  """"""Compute intersection-over-union between corresponding boxes in boxlists.\n\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding N boxes\n    scope: name scope.\n\n  Returns:\n    a tensor with shape [N] representing pairwise iou scores.\n  """"""\n  with tf.name_scope(scope, \'MatchedIOU\'):\n    intersections = matched_intersection(boxlist1, boxlist2)\n    areas1 = area(boxlist1)\n    areas2 = area(boxlist2)\n    unions = areas1 + areas2 - intersections\n    return tf.where(\n        tf.equal(intersections, 0.0),\n        tf.zeros_like(intersections), tf.truediv(intersections, unions))\n\n\ndef ioa(boxlist1, boxlist2, scope=None):\n  """"""Computes pairwise intersection-over-area between box collections.\n\n  intersection-over-area (IOA) between two boxes box1 and box2 is defined as\n  their intersection area over box2\'s area. Note that ioa is not symmetric,\n  that is, ioa(box1, box2) != ioa(box2, box1).\n\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n    scope: name scope.\n\n  Returns:\n    a tensor with shape [N, M] representing pairwise ioa scores.\n  """"""\n  with tf.name_scope(scope, \'IOA\'):\n    intersections = intersection(boxlist1, boxlist2)\n    areas = tf.expand_dims(area(boxlist2), 0)\n    return tf.truediv(intersections, areas)\n\n\ndef prune_non_overlapping_boxes(\n    boxlist1, boxlist2, min_overlap=0.0, scope=None):\n  """"""Prunes the boxes in boxlist1 that overlap less than thresh with boxlist2.\n\n  For each box in boxlist1, we want its IOA to be more than minoverlap with\n  at least one of the boxes in boxlist2. If it does not, we remove it.\n\n  Args:\n    boxlist1: BoxList holding N boxes.\n    boxlist2: BoxList holding M boxes.\n    min_overlap: Minimum required overlap between boxes, to count them as\n                overlapping.\n    scope: name scope.\n\n  Returns:\n    new_boxlist1: A pruned boxlist with size [N\', 4].\n    keep_inds: A tensor with shape [N\'] indexing kept bounding boxes in the\n      first input BoxList `boxlist1`.\n  """"""\n  with tf.name_scope(scope, \'PruneNonOverlappingBoxes\'):\n    ioa_ = ioa(boxlist2, boxlist1)  # [M, N] tensor\n    ioa_ = tf.reduce_max(ioa_, reduction_indices=[0])  # [N] tensor\n    keep_bool = tf.greater_equal(ioa_, tf.constant(min_overlap))\n    keep_inds = tf.squeeze(tf.where(keep_bool), squeeze_dims=[1])\n    new_boxlist1 = gather(boxlist1, keep_inds)\n    return new_boxlist1, keep_inds\n\n\ndef prune_small_boxes(boxlist, min_side, scope=None):\n  """"""Prunes small boxes in the boxlist which have a side smaller than min_side.\n\n  Args:\n    boxlist: BoxList holding N boxes.\n    min_side: Minimum width AND height of box to survive pruning.\n    scope: name scope.\n\n  Returns:\n    A pruned boxlist.\n  """"""\n  with tf.name_scope(scope, \'PruneSmallBoxes\'):\n    height, width = height_width(boxlist)\n    is_valid = tf.logical_and(tf.greater_equal(width, min_side),\n                              tf.greater_equal(height, min_side))\n    return gather(boxlist, tf.reshape(tf.where(is_valid), [-1]))\n\n\ndef change_coordinate_frame(boxlist, window, scope=None):\n  """"""Change coordinate frame of the boxlist to be relative to window\'s frame.\n\n  Given a window of the form [ymin, xmin, ymax, xmax],\n  changes bounding box coordinates from boxlist to be relative to this window\n  (e.g., the min corner maps to (0,0) and the max corner maps to (1,1)).\n\n  An example use case is data augmentation: where we are given groundtruth\n  boxes (boxlist) and would like to randomly crop the image to some\n  window (window). In this case we need to change the coordinate frame of\n  each groundtruth box to be relative to this new window.\n\n  Args:\n    boxlist: A BoxList object holding N boxes.\n    window: A rank 1 tensor [4].\n    scope: name scope.\n\n  Returns:\n    Returns a BoxList object with N boxes.\n  """"""\n  with tf.name_scope(scope, \'ChangeCoordinateFrame\'):\n    win_height = window[2] - window[0]\n    win_width = window[3] - window[1]\n    boxlist_new = scale(box_list.BoxList(\n        boxlist.get() - [window[0], window[1], window[0], window[1]]),\n                        1.0 / win_height, 1.0 / win_width)\n    boxlist_new = _copy_extra_fields(boxlist_new, boxlist)\n    return boxlist_new\n\n\ndef sq_dist(boxlist1, boxlist2, scope=None):\n  """"""Computes the pairwise squared distances between box corners.\n\n  This op treats each box as if it were a point in a 4d Euclidean space and\n  computes pairwise squared distances.\n\n  Mathematically, we are given two matrices of box coordinates X and Y,\n  where X(i,:) is the i\'th row of X, containing the 4 numbers defining the\n  corners of the i\'th box in boxlist1. Similarly Y(j,:) corresponds to\n  boxlist2.  We compute\n  Z(i,j) = ||X(i,:) - Y(j,:)||^2\n         = ||X(i,:)||^2 + ||Y(j,:)||^2 - 2 X(i,:)\' * Y(j,:),\n\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n    scope: name scope.\n\n  Returns:\n    a tensor with shape [N, M] representing pairwise distances\n  """"""\n  with tf.name_scope(scope, \'SqDist\'):\n    sqnorm1 = tf.reduce_sum(tf.square(boxlist1.get()), 1, keep_dims=True)\n    sqnorm2 = tf.reduce_sum(tf.square(boxlist2.get()), 1, keep_dims=True)\n    innerprod = tf.matmul(boxlist1.get(), boxlist2.get(),\n                          transpose_a=False, transpose_b=True)\n    return sqnorm1 + tf.transpose(sqnorm2) - 2.0 * innerprod\n\n\ndef boolean_mask(boxlist, indicator, fields=None, scope=None):\n  """"""Select boxes from BoxList according to indicator and return new BoxList.\n\n  `boolean_mask` returns the subset of boxes that are marked as ""True"" by the\n  indicator tensor. By default, `boolean_mask` returns boxes corresponding to\n  the input index list, as well as all additional fields stored in the boxlist\n  (indexing into the first dimension).  However one can optionally only draw\n  from a subset of fields.\n\n  Args:\n    boxlist: BoxList holding N boxes\n    indicator: a rank-1 boolean tensor\n    fields: (optional) list of fields to also gather from.  If None (default),\n      all fields are gathered from.  Pass an empty fields list to only gather\n      the box coordinates.\n    scope: name scope.\n\n  Returns:\n    subboxlist: a BoxList corresponding to the subset of the input BoxList\n      specified by indicator\n  Raises:\n    ValueError: if `indicator` is not a rank-1 boolean tensor.\n  """"""\n  with tf.name_scope(scope, \'BooleanMask\'):\n    if indicator.shape.ndims != 1:\n      raise ValueError(\'indicator should have rank 1\')\n    if indicator.dtype != tf.bool:\n      raise ValueError(\'indicator should be a boolean tensor\')\n    subboxlist = box_list.BoxList(tf.boolean_mask(boxlist.get(), indicator))\n    if fields is None:\n      fields = boxlist.get_extra_fields()\n    for field in fields:\n      if not boxlist.has_field(field):\n        raise ValueError(\'boxlist must contain all specified fields\')\n      subfieldlist = tf.boolean_mask(boxlist.get_field(field), indicator)\n      subboxlist.add_field(field, subfieldlist)\n    return subboxlist\n\n\ndef gather(boxlist, indices, fields=None, scope=None):\n  """"""Gather boxes from BoxList according to indices and return new BoxList.\n\n  By default, `gather` returns boxes corresponding to the input index list, as\n  well as all additional fields stored in the boxlist (indexing into the\n  first dimension).  However one can optionally only gather from a\n  subset of fields.\n\n  Args:\n    boxlist: BoxList holding N boxes\n    indices: a rank-1 tensor of type int32 / int64\n    fields: (optional) list of fields to also gather from.  If None (default),\n      all fields are gathered from.  Pass an empty fields list to only gather\n      the box coordinates.\n    scope: name scope.\n\n  Returns:\n    subboxlist: a BoxList corresponding to the subset of the input BoxList\n    specified by indices\n  Raises:\n    ValueError: if specified field is not contained in boxlist or if the\n      indices are not of type int32\n  """"""\n  with tf.name_scope(scope, \'Gather\'):\n    if len(indices.shape.as_list()) != 1:\n      raise ValueError(\'indices should have rank 1\')\n    if indices.dtype != tf.int32 and indices.dtype != tf.int64:\n      raise ValueError(\'indices should be an int32 / int64 tensor\')\n    subboxlist = box_list.BoxList(tf.gather(boxlist.get(), indices))\n    if fields is None:\n      fields = boxlist.get_extra_fields()\n    for field in fields:\n      if not boxlist.has_field(field):\n        raise ValueError(\'boxlist must contain all specified fields\')\n      subfieldlist = tf.gather(boxlist.get_field(field), indices)\n      subboxlist.add_field(field, subfieldlist)\n    return subboxlist\n\n\ndef concatenate(boxlists, fields=None, scope=None):\n  """"""Concatenate list of BoxLists.\n\n  This op concatenates a list of input BoxLists into a larger BoxList.  It also\n  handles concatenation of BoxList fields as long as the field tensor shapes\n  are equal except for the first dimension.\n\n  Args:\n    boxlists: list of BoxList objects\n    fields: optional list of fields to also concatenate.  By default, all\n      fields from the first BoxList in the list are included in the\n      concatenation.\n    scope: name scope.\n\n  Returns:\n    a BoxList with number of boxes equal to\n      sum([boxlist.num_boxes() for boxlist in BoxList])\n  Raises:\n    ValueError: if boxlists is invalid (i.e., is not a list, is empty, or\n      contains non BoxList objects), or if requested fields are not contained in\n      all boxlists\n  """"""\n  with tf.name_scope(scope, \'Concatenate\'):\n    if not isinstance(boxlists, list):\n      raise ValueError(\'boxlists should be a list\')\n    if not boxlists:\n      raise ValueError(\'boxlists should have nonzero length\')\n    for boxlist in boxlists:\n      if not isinstance(boxlist, box_list.BoxList):\n        raise ValueError(\'all elements of boxlists should be BoxList objects\')\n    concatenated = box_list.BoxList(\n        tf.concat([boxlist.get() for boxlist in boxlists], 0))\n    if fields is None:\n      fields = boxlists[0].get_extra_fields()\n    for field in fields:\n      first_field_shape = boxlists[0].get_field(field).get_shape().as_list()\n      first_field_shape[0] = -1\n      if None in first_field_shape:\n        raise ValueError(\'field %s must have fully defined shape except for the\'\n                         \' 0th dimension.\' % field)\n      for boxlist in boxlists:\n        if not boxlist.has_field(field):\n          raise ValueError(\'boxlist must contain all requested fields\')\n        field_shape = boxlist.get_field(field).get_shape().as_list()\n        field_shape[0] = -1\n        if field_shape != first_field_shape:\n          raise ValueError(\'field %s must have same shape for all boxlists \'\n                           \'except for the 0th dimension.\' % field)\n      concatenated_field = tf.concat(\n          [boxlist.get_field(field) for boxlist in boxlists], 0)\n      concatenated.add_field(field, concatenated_field)\n    return concatenated\n\n\ndef sort_by_field(boxlist, field, order=SortOrder.descend, scope=None):\n  """"""Sort boxes and associated fields according to a scalar field.\n\n  A common use case is reordering the boxes according to descending scores.\n\n  Args:\n    boxlist: BoxList holding N boxes.\n    field: A BoxList field for sorting and reordering the BoxList.\n    order: (Optional) descend or ascend. Default is descend.\n    scope: name scope.\n\n  Returns:\n    sorted_boxlist: A sorted BoxList with the field in the specified order.\n\n  Raises:\n    ValueError: if specified field does not exist\n    ValueError: if the order is not either descend or ascend\n  """"""\n  with tf.name_scope(scope, \'SortByField\'):\n    if order != SortOrder.descend and order != SortOrder.ascend:\n      raise ValueError(\'Invalid sort order\')\n\n    field_to_sort = boxlist.get_field(field)\n    if len(field_to_sort.shape.as_list()) != 1:\n      raise ValueError(\'Field should have rank 1\')\n\n    num_boxes = boxlist.num_boxes()\n    num_entries = tf.size(field_to_sort)\n    length_assert = tf.Assert(\n        tf.equal(num_boxes, num_entries),\n        [\'Incorrect field size: actual vs expected.\', num_entries, num_boxes])\n\n    with tf.control_dependencies([length_assert]):\n      # TODO(derekjchow): Remove with tf.device when top_k operation runs\n      # correctly on GPU.\n      with tf.device(\'/cpu:0\'):\n        _, sorted_indices = tf.nn.top_k(field_to_sort, num_boxes, sorted=True)\n\n    if order == SortOrder.ascend:\n      sorted_indices = tf.reverse_v2(sorted_indices, [0])\n\n    return gather(boxlist, sorted_indices)\n\n\ndef visualize_boxes_in_image(image, boxlist, normalized=False, scope=None):\n  """"""Overlay bounding box list on image.\n\n  Currently this visualization plots a 1 pixel thick red bounding box on top\n  of the image.  Note that tf.image.draw_bounding_boxes essentially is\n  1 indexed.\n\n  Args:\n    image: an image tensor with shape [height, width, 3]\n    boxlist: a BoxList\n    normalized: (boolean) specify whether corners are to be interpreted\n      as absolute coordinates in image space or normalized with respect to the\n      image size.\n    scope: name scope.\n\n  Returns:\n    image_and_boxes: an image tensor with shape [height, width, 3]\n  """"""\n  with tf.name_scope(scope, \'VisualizeBoxesInImage\'):\n    if not normalized:\n      height, width, _ = tf.unstack(tf.shape(image))\n      boxlist = scale(boxlist,\n                      1.0 / tf.cast(height, tf.float32),\n                      1.0 / tf.cast(width, tf.float32))\n    corners = tf.expand_dims(boxlist.get(), 0)\n    image = tf.expand_dims(image, 0)\n    return tf.squeeze(tf.image.draw_bounding_boxes(image, corners), [0])\n\n\ndef filter_field_value_equals(boxlist, field, value, scope=None):\n  """"""Filter to keep only boxes with field entries equal to the given value.\n\n  Args:\n    boxlist: BoxList holding N boxes.\n    field: field name for filtering.\n    value: scalar value.\n    scope: name scope.\n\n  Returns:\n    a BoxList holding M boxes where M <= N\n\n  Raises:\n    ValueError: if boxlist not a BoxList object or if it does not have\n      the specified field.\n  """"""\n  with tf.name_scope(scope, \'FilterFieldValueEquals\'):\n    if not isinstance(boxlist, box_list.BoxList):\n      raise ValueError(\'boxlist must be a BoxList\')\n    if not boxlist.has_field(field):\n      raise ValueError(\'boxlist must contain the specified field\')\n    filter_field = boxlist.get_field(field)\n    gather_index = tf.reshape(tf.where(tf.equal(filter_field, value)), [-1])\n    return gather(boxlist, gather_index)\n\n\ndef filter_greater_than(boxlist, thresh, scope=None):\n  """"""Filter to keep only boxes with score exceeding a given threshold.\n\n  This op keeps the collection of boxes whose corresponding scores are\n  greater than the input threshold.\n\n  TODO(jonathanhuang): Change function name to filter_scores_greater_than\n\n  Args:\n    boxlist: BoxList holding N boxes.  Must contain a \'scores\' field\n      representing detection scores.\n    thresh: scalar threshold\n    scope: name scope.\n\n  Returns:\n    a BoxList holding M boxes where M <= N\n\n  Raises:\n    ValueError: if boxlist not a BoxList object or if it does not\n      have a scores field\n  """"""\n  with tf.name_scope(scope, \'FilterGreaterThan\'):\n    if not isinstance(boxlist, box_list.BoxList):\n      raise ValueError(\'boxlist must be a BoxList\')\n    if not boxlist.has_field(\'scores\'):\n      raise ValueError(\'input boxlist must have \\\'scores\\\' field\')\n    scores = boxlist.get_field(\'scores\')\n    if len(scores.shape.as_list()) > 2:\n      raise ValueError(\'Scores should have rank 1 or 2\')\n    if len(scores.shape.as_list()) == 2 and scores.shape.as_list()[1] != 1:\n      raise ValueError(\'Scores should have rank 1 or have shape \'\n                       \'consistent with [None, 1]\')\n    high_score_indices = tf.cast(tf.reshape(\n        tf.where(tf.greater(scores, thresh)),\n        [-1]), tf.int32)\n    return gather(boxlist, high_score_indices)\n\n\ndef non_max_suppression(boxlist, thresh, max_output_size, scope=None):\n  """"""Non maximum suppression.\n\n  This op greedily selects a subset of detection bounding boxes, pruning\n  away boxes that have high IOU (intersection over union) overlap (> thresh)\n  with already selected boxes.  Note that this only works for a single class ---\n  to apply NMS to multi-class predictions, use MultiClassNonMaxSuppression.\n\n  Args:\n    boxlist: BoxList holding N boxes.  Must contain a \'scores\' field\n      representing detection scores.\n    thresh: scalar threshold\n    max_output_size: maximum number of retained boxes\n    scope: name scope.\n\n  Returns:\n    a BoxList holding M boxes where M <= max_output_size\n  Raises:\n    ValueError: if thresh is not in [0, 1]\n  """"""\n  with tf.name_scope(scope, \'NonMaxSuppression\'):\n    if not 0 <= thresh <= 1.0:\n      raise ValueError(\'thresh must be between 0 and 1\')\n    if not isinstance(boxlist, box_list.BoxList):\n      raise ValueError(\'boxlist must be a BoxList\')\n    if not boxlist.has_field(\'scores\'):\n      raise ValueError(\'input boxlist must have \\\'scores\\\' field\')\n    selected_indices = tf.image.non_max_suppression(\n        boxlist.get(), boxlist.get_field(\'scores\'),\n        max_output_size, iou_threshold=thresh)\n    return gather(boxlist, selected_indices)\n\n\ndef _copy_extra_fields(boxlist_to_copy_to, boxlist_to_copy_from):\n  """"""Copies the extra fields of boxlist_to_copy_from to boxlist_to_copy_to.\n\n  Args:\n    boxlist_to_copy_to: BoxList to which extra fields are copied.\n    boxlist_to_copy_from: BoxList from which fields are copied.\n\n  Returns:\n    boxlist_to_copy_to with extra fields.\n  """"""\n  for field in boxlist_to_copy_from.get_extra_fields():\n    boxlist_to_copy_to.add_field(field, boxlist_to_copy_from.get_field(field))\n  return boxlist_to_copy_to\n\n\ndef to_normalized_coordinates(boxlist, height, width,\n                              check_range=True, scope=None):\n  """"""Converts absolute box coordinates to normalized coordinates in [0, 1].\n\n  Usually one uses the dynamic shape of the image or conv-layer tensor:\n    boxlist = box_list_ops.to_normalized_coordinates(boxlist,\n                                                     tf.shape(images)[1],\n                                                     tf.shape(images)[2]),\n\n  This function raises an assertion failed error at graph execution time when\n  the maximum coordinate is smaller than 1.01 (which means that coordinates are\n  already normalized). The value 1.01 is to deal with small rounding errors.\n\n  Args:\n    boxlist: BoxList with coordinates in terms of pixel-locations.\n    height: Maximum value for height of absolute box coordinates.\n    width: Maximum value for width of absolute box coordinates.\n    check_range: If True, checks if the coordinates are normalized or not.\n    scope: name scope.\n\n  Returns:\n    boxlist with normalized coordinates in [0, 1].\n  """"""\n  with tf.name_scope(scope, \'ToNormalizedCoordinates\'):\n    height = tf.cast(height, tf.float32)\n    width = tf.cast(width, tf.float32)\n\n    if check_range:\n      max_val = tf.reduce_max(boxlist.get())\n      max_assert = tf.Assert(tf.greater(max_val, 1.01),\n                             [\'max value is lower than 1.01: \', max_val])\n      with tf.control_dependencies([max_assert]):\n        width = tf.identity(width)\n\n    return scale(boxlist, 1 / height, 1 / width)\n\n\ndef to_absolute_coordinates(boxlist,\n                            height,\n                            width,\n                            check_range=True,\n                            maximum_normalized_coordinate=1.1,\n                            scope=None):\n  """"""Converts normalized box coordinates to absolute pixel coordinates.\n\n  This function raises an assertion failed error when the maximum box coordinate\n  value is larger than maximum_normalized_coordinate (in which case coordinates\n  are already absolute).\n\n  Args:\n    boxlist: BoxList with coordinates in range [0, 1].\n    height: Maximum value for height of absolute box coordinates.\n    width: Maximum value for width of absolute box coordinates.\n    check_range: If True, checks if the coordinates are normalized or not.\n    maximum_normalized_coordinate: Maximum coordinate value to be considered\n      as normalized, default to 1.1.\n    scope: name scope.\n\n  Returns:\n    boxlist with absolute coordinates in terms of the image size.\n\n  """"""\n  with tf.name_scope(scope, \'ToAbsoluteCoordinates\'):\n    height = tf.cast(height, tf.float32)\n    width = tf.cast(width, tf.float32)\n\n    # Ensure range of input boxes is correct.\n    if check_range:\n      box_maximum = tf.reduce_max(boxlist.get())\n      max_assert = tf.Assert(\n          tf.greater_equal(maximum_normalized_coordinate, box_maximum),\n          [\'maximum box coordinate value is larger \'\n           \'than %f: \' % maximum_normalized_coordinate, box_maximum])\n      with tf.control_dependencies([max_assert]):\n        width = tf.identity(width)\n\n    return scale(boxlist, height, width)\n\n\ndef refine_boxes_multi_class(pool_boxes,\n                             num_classes,\n                             nms_iou_thresh,\n                             nms_max_detections,\n                             voting_iou_thresh=0.5):\n  """"""Refines a pool of boxes using non max suppression and box voting.\n\n  Box refinement is done independently for each class.\n\n  Args:\n    pool_boxes: (BoxList) A collection of boxes to be refined. pool_boxes must\n      have a rank 1 \'scores\' field and a rank 1 \'classes\' field.\n    num_classes: (int scalar) Number of classes.\n    nms_iou_thresh: (float scalar) iou threshold for non max suppression (NMS).\n    nms_max_detections: (int scalar) maximum output size for NMS.\n    voting_iou_thresh: (float scalar) iou threshold for box voting.\n\n  Returns:\n    BoxList of refined boxes.\n\n  Raises:\n    ValueError: if\n      a) nms_iou_thresh or voting_iou_thresh is not in [0, 1].\n      b) pool_boxes is not a BoxList.\n      c) pool_boxes does not have a scores and classes field.\n  """"""\n  if not 0.0 <= nms_iou_thresh <= 1.0:\n    raise ValueError(\'nms_iou_thresh must be between 0 and 1\')\n  if not 0.0 <= voting_iou_thresh <= 1.0:\n    raise ValueError(\'voting_iou_thresh must be between 0 and 1\')\n  if not isinstance(pool_boxes, box_list.BoxList):\n    raise ValueError(\'pool_boxes must be a BoxList\')\n  if not pool_boxes.has_field(\'scores\'):\n    raise ValueError(\'pool_boxes must have a \\\'scores\\\' field\')\n  if not pool_boxes.has_field(\'classes\'):\n    raise ValueError(\'pool_boxes must have a \\\'classes\\\' field\')\n\n  refined_boxes = []\n  for i in range(num_classes):\n    boxes_class = filter_field_value_equals(pool_boxes, \'classes\', i)\n    refined_boxes_class = refine_boxes(boxes_class, nms_iou_thresh,\n                                       nms_max_detections, voting_iou_thresh)\n    refined_boxes.append(refined_boxes_class)\n  return sort_by_field(concatenate(refined_boxes), \'scores\')\n\n\ndef refine_boxes(pool_boxes,\n                 nms_iou_thresh,\n                 nms_max_detections,\n                 voting_iou_thresh=0.5):\n  """"""Refines a pool of boxes using non max suppression and box voting.\n\n  Args:\n    pool_boxes: (BoxList) A collection of boxes to be refined. pool_boxes must\n      have a rank 1 \'scores\' field.\n    nms_iou_thresh: (float scalar) iou threshold for non max suppression (NMS).\n    nms_max_detections: (int scalar) maximum output size for NMS.\n    voting_iou_thresh: (float scalar) iou threshold for box voting.\n\n  Returns:\n    BoxList of refined boxes.\n\n  Raises:\n    ValueError: if\n      a) nms_iou_thresh or voting_iou_thresh is not in [0, 1].\n      b) pool_boxes is not a BoxList.\n      c) pool_boxes does not have a scores field.\n  """"""\n  if not 0.0 <= nms_iou_thresh <= 1.0:\n    raise ValueError(\'nms_iou_thresh must be between 0 and 1\')\n  if not 0.0 <= voting_iou_thresh <= 1.0:\n    raise ValueError(\'voting_iou_thresh must be between 0 and 1\')\n  if not isinstance(pool_boxes, box_list.BoxList):\n    raise ValueError(\'pool_boxes must be a BoxList\')\n  if not pool_boxes.has_field(\'scores\'):\n    raise ValueError(\'pool_boxes must have a \\\'scores\\\' field\')\n\n  nms_boxes = non_max_suppression(\n      pool_boxes, nms_iou_thresh, nms_max_detections)\n  return box_voting(nms_boxes, pool_boxes, voting_iou_thresh)\n\n\ndef box_voting(selected_boxes, pool_boxes, iou_thresh=0.5):\n  """"""Performs box voting as described in S. Gidaris and N. Komodakis, ICCV 2015.\n\n  Performs box voting as described in \'Object detection via a multi-region &\n  semantic segmentation-aware CNN model\', Gidaris and Komodakis, ICCV 2015. For\n  each box \'B\' in selected_boxes, we find the set \'S\' of boxes in pool_boxes\n  with iou overlap >= iou_thresh. The location of B is set to the weighted\n  average location of boxes in S (scores are used for weighting). And the score\n  of B is set to the average score of boxes in S.\n\n  Args:\n    selected_boxes: BoxList containing a subset of boxes in pool_boxes. These\n      boxes are usually selected from pool_boxes using non max suppression.\n    pool_boxes: BoxList containing a set of (possibly redundant) boxes.\n    iou_thresh: (float scalar) iou threshold for matching boxes in\n      selected_boxes and pool_boxes.\n\n  Returns:\n    BoxList containing averaged locations and scores for each box in\n    selected_boxes.\n\n  Raises:\n    ValueError: if\n      a) selected_boxes or pool_boxes is not a BoxList.\n      b) if iou_thresh is not in [0, 1].\n      c) pool_boxes does not have a scores field.\n  """"""\n  if not 0.0 <= iou_thresh <= 1.0:\n    raise ValueError(\'iou_thresh must be between 0 and 1\')\n  if not isinstance(selected_boxes, box_list.BoxList):\n    raise ValueError(\'selected_boxes must be a BoxList\')\n  if not isinstance(pool_boxes, box_list.BoxList):\n    raise ValueError(\'pool_boxes must be a BoxList\')\n  if not pool_boxes.has_field(\'scores\'):\n    raise ValueError(\'pool_boxes must have a \\\'scores\\\' field\')\n\n  iou_ = iou(selected_boxes, pool_boxes)\n  match_indicator = tf.to_float(tf.greater(iou_, iou_thresh))\n  num_matches = tf.reduce_sum(match_indicator, 1)\n  # TODO(kbanoop): Handle the case where some boxes in selected_boxes do not\n  # match to any boxes in pool_boxes. For such boxes without any matches, we\n  # should return the original boxes without voting.\n  match_assert = tf.Assert(\n      tf.reduce_all(tf.greater(num_matches, 0)),\n      [\'Each box in selected_boxes must match with at least one box \'\n       \'in pool_boxes.\'])\n\n  scores = tf.expand_dims(pool_boxes.get_field(\'scores\'), 1)\n  scores_assert = tf.Assert(\n      tf.reduce_all(tf.greater_equal(scores, 0)),\n      [\'Scores must be non negative.\'])\n\n  with tf.control_dependencies([scores_assert, match_assert]):\n    sum_scores = tf.matmul(match_indicator, scores)\n  averaged_scores = tf.reshape(sum_scores, [-1]) / num_matches\n\n  box_locations = tf.matmul(match_indicator,\n                            pool_boxes.get() * scores) / sum_scores\n  averaged_boxes = box_list.BoxList(box_locations)\n  _copy_extra_fields(averaged_boxes, selected_boxes)\n  averaged_boxes.add_field(\'scores\', averaged_scores)\n  return averaged_boxes\n\n\ndef pad_or_clip_box_list(boxlist, num_boxes, scope=None):\n  """"""Pads or clips all fields of a BoxList.\n\n  Args:\n    boxlist: A BoxList with arbitrary of number of boxes.\n    num_boxes: First num_boxes in boxlist are kept.\n      The fields are zero-padded if num_boxes is bigger than the\n      actual number of boxes.\n    scope: name scope.\n\n  Returns:\n    BoxList with all fields padded or clipped.\n  """"""\n  with tf.name_scope(scope, \'PadOrClipBoxList\'):\n    subboxlist = box_list.BoxList(shape_utils.pad_or_clip_tensor(\n        boxlist.get(), num_boxes))\n    for field in boxlist.get_extra_fields():\n      subfield = shape_utils.pad_or_clip_tensor(\n          boxlist.get_field(field), num_boxes)\n      subboxlist.add_field(field, subfield)\n    return subboxlist\n\n\ndef select_random_box(boxlist,\n                      default_box=None,\n                      seed=None,\n                      scope=None):\n  """"""Selects a random bounding box from a `BoxList`.\n\n  Args:\n    boxlist: A BoxList.\n    default_box: A [1, 4] float32 tensor. If no boxes are present in `boxlist`,\n      this default box will be returned. If None, will use a default box of\n      [[-1., -1., -1., -1.]].\n    seed: Random seed.\n    scope: Name scope.\n\n  Returns:\n    bbox: A [1, 4] tensor with a random bounding box.\n    valid: A bool tensor indicating whether a valid bounding box is returned\n      (True) or whether the default box is returned (False).\n  """"""\n  with tf.name_scope(scope, \'SelectRandomBox\'):\n    bboxes = boxlist.get()\n    combined_shape = shape_utils.combined_static_and_dynamic_shape(bboxes)\n    number_of_boxes = combined_shape[0]\n    default_box = default_box or tf.constant([[-1., -1., -1., -1.]])\n\n    def select_box():\n      random_index = tf.random_uniform([],\n                                       maxval=number_of_boxes,\n                                       dtype=tf.int32,\n                                       seed=seed)\n      return tf.expand_dims(bboxes[random_index], axis=0), tf.constant(True)\n\n  return tf.cond(\n      tf.greater_equal(number_of_boxes, 1),\n      true_fn=select_box,\n      false_fn=lambda: (default_box, tf.constant(False)))\n\n\ndef get_minimal_coverage_box(boxlist,\n                             default_box=None,\n                             scope=None):\n  """"""Creates a single bounding box which covers all boxes in the boxlist.\n\n  Args:\n    boxlist: A Boxlist.\n    default_box: A [1, 4] float32 tensor. If no boxes are present in `boxlist`,\n      this default box will be returned. If None, will use a default box of\n      [[0., 0., 1., 1.]].\n    scope: Name scope.\n\n  Returns:\n    A [1, 4] float32 tensor with a bounding box that tightly covers all the\n    boxes in the box list. If the boxlist does not contain any boxes, the\n    default box is returned.\n  """"""\n  with tf.name_scope(scope, \'CreateCoverageBox\'):\n    num_boxes = boxlist.num_boxes()\n\n    def coverage_box(bboxes):\n      y_min, x_min, y_max, x_max = tf.split(\n          value=bboxes, num_or_size_splits=4, axis=1)\n      y_min_coverage = tf.reduce_min(y_min, axis=0)\n      x_min_coverage = tf.reduce_min(x_min, axis=0)\n      y_max_coverage = tf.reduce_max(y_max, axis=0)\n      x_max_coverage = tf.reduce_max(x_max, axis=0)\n      return tf.stack(\n          [y_min_coverage, x_min_coverage, y_max_coverage, x_max_coverage],\n          axis=1)\n\n    default_box = default_box or tf.constant([[0., 0., 1., 1.]])\n    return tf.cond(\n        tf.greater_equal(num_boxes, 1),\n        true_fn=lambda: coverage_box(boxlist.get()),\n        false_fn=lambda: default_box)\n'"
src/main/object_detection/core/standard_fields.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Contains classes specifying naming conventions used for object detection.\n\n\nSpecifies:\n  InputDataFields: standard fields used by reader/preprocessor/batcher.\n  DetectionResultFields: standard fields returned by object detector.\n  BoxListFields: standard field used by BoxList\n  TfExampleFields: standard fields for tf-example data format (go/tf-example).\n""""""\n\n\nclass InputDataFields(object):\n  """"""Names for the input tensors.\n\n  Holds the standard data field names to use for identifying input tensors. This\n  should be used by the decoder to identify keys for the returned tensor_dict\n  containing input tensors. And it should be used by the model to identify the\n  tensors it needs.\n\n  Attributes:\n    image: image.\n    image_additional_channels: additional channels.\n    original_image: image in the original input size.\n    key: unique key corresponding to image.\n    source_id: source of the original image.\n    filename: original filename of the dataset (without common path).\n    groundtruth_image_classes: image-level class labels.\n    groundtruth_boxes: coordinates of the ground truth boxes in the image.\n    groundtruth_classes: box-level class labels.\n    groundtruth_label_types: box-level label types (e.g. explicit negative).\n    groundtruth_is_crowd: [DEPRECATED, use groundtruth_group_of instead]\n      is the groundtruth a single object or a crowd.\n    groundtruth_area: area of a groundtruth segment.\n    groundtruth_difficult: is a `difficult` object\n    groundtruth_group_of: is a `group_of` objects, e.g. multiple objects of the\n      same class, forming a connected group, where instances are heavily\n      occluding each other.\n    proposal_boxes: coordinates of object proposal boxes.\n    proposal_objectness: objectness score of each proposal.\n    groundtruth_instance_masks: ground truth instance masks.\n    groundtruth_instance_boundaries: ground truth instance boundaries.\n    groundtruth_instance_classes: instance mask-level class labels.\n    groundtruth_keypoints: ground truth keypoints.\n    groundtruth_keypoint_visibilities: ground truth keypoint visibilities.\n    groundtruth_label_scores: groundtruth label scores.\n    groundtruth_weights: groundtruth weight factor for bounding boxes.\n    num_groundtruth_boxes: number of groundtruth boxes.\n    true_image_shapes: true shapes of images in the resized images, as resized\n      images can be padded with zeros.\n    verified_labels: list of human-verified image-level labels (note, that a\n      label can be verified both as positive and negative).\n    multiclass_scores: the label score per class for each box.\n  """"""\n  image = \'image\'\n  image_additional_channels = \'image_additional_channels\'\n  original_image = \'original_image\'\n  key = \'key\'\n  source_id = \'source_id\'\n  filename = \'filename\'\n  groundtruth_image_classes = \'groundtruth_image_classes\'\n  groundtruth_boxes = \'groundtruth_boxes\'\n  groundtruth_classes = \'groundtruth_classes\'\n  groundtruth_label_types = \'groundtruth_label_types\'\n  groundtruth_is_crowd = \'groundtruth_is_crowd\'\n  groundtruth_area = \'groundtruth_area\'\n  groundtruth_difficult = \'groundtruth_difficult\'\n  groundtruth_group_of = \'groundtruth_group_of\'\n  proposal_boxes = \'proposal_boxes\'\n  proposal_objectness = \'proposal_objectness\'\n  groundtruth_instance_masks = \'groundtruth_instance_masks\'\n  groundtruth_instance_boundaries = \'groundtruth_instance_boundaries\'\n  groundtruth_instance_classes = \'groundtruth_instance_classes\'\n  groundtruth_keypoints = \'groundtruth_keypoints\'\n  groundtruth_keypoint_visibilities = \'groundtruth_keypoint_visibilities\'\n  groundtruth_label_scores = \'groundtruth_label_scores\'\n  groundtruth_weights = \'groundtruth_weights\'\n  num_groundtruth_boxes = \'num_groundtruth_boxes\'\n  true_image_shape = \'true_image_shape\'\n  verified_labels = \'verified_labels\'\n  multiclass_scores = \'multiclass_scores\'\n\n\nclass DetectionResultFields(object):\n  """"""Naming conventions for storing the output of the detector.\n\n  Attributes:\n    source_id: source of the original image.\n    key: unique key corresponding to image.\n    detection_boxes: coordinates of the detection boxes in the image.\n    detection_scores: detection scores for the detection boxes in the image.\n    detection_classes: detection-level class labels.\n    detection_masks: contains a segmentation mask for each detection box.\n    detection_boundaries: contains an object boundary for each detection box.\n    detection_keypoints: contains detection keypoints for each detection box.\n    num_detections: number of detections in the batch.\n  """"""\n\n  source_id = \'source_id\'\n  key = \'key\'\n  detection_boxes = \'detection_boxes\'\n  detection_scores = \'detection_scores\'\n  detection_classes = \'detection_classes\'\n  detection_masks = \'detection_masks\'\n  detection_boundaries = \'detection_boundaries\'\n  detection_keypoints = \'detection_keypoints\'\n  num_detections = \'num_detections\'\n\n\nclass BoxListFields(object):\n  """"""Naming conventions for BoxLists.\n\n  Attributes:\n    boxes: bounding box coordinates.\n    classes: classes per bounding box.\n    scores: scores per bounding box.\n    weights: sample weights per bounding box.\n    objectness: objectness score per bounding box.\n    masks: masks per bounding box.\n    boundaries: boundaries per bounding box.\n    keypoints: keypoints per bounding box.\n    keypoint_heatmaps: keypoint heatmaps per bounding box.\n    is_crowd: is_crowd annotation per bounding box.\n  """"""\n  boxes = \'boxes\'\n  classes = \'classes\'\n  scores = \'scores\'\n  weights = \'weights\'\n  objectness = \'objectness\'\n  masks = \'masks\'\n  boundaries = \'boundaries\'\n  keypoints = \'keypoints\'\n  keypoint_heatmaps = \'keypoint_heatmaps\'\n  is_crowd = \'is_crowd\'\n\n\nclass TfExampleFields(object):\n  """"""TF-example proto feature names for object detection.\n\n  Holds the standard feature names to load from an Example proto for object\n  detection.\n\n  Attributes:\n    image_encoded: JPEG encoded string\n    image_format: image format, e.g. ""JPEG""\n    filename: filename\n    channels: number of channels of image\n    colorspace: colorspace, e.g. ""RGB""\n    height: height of image in pixels, e.g. 462\n    width: width of image in pixels, e.g. 581\n    source_id: original source of the image\n    image_class_text: image-level label in text format\n    image_class_label: image-level label in numerical format\n    object_class_text: labels in text format, e.g. [""person"", ""cat""]\n    object_class_label: labels in numbers, e.g. [16, 8]\n    object_bbox_xmin: xmin coordinates of groundtruth box, e.g. 10, 30\n    object_bbox_xmax: xmax coordinates of groundtruth box, e.g. 50, 40\n    object_bbox_ymin: ymin coordinates of groundtruth box, e.g. 40, 50\n    object_bbox_ymax: ymax coordinates of groundtruth box, e.g. 80, 70\n    object_view: viewpoint of object, e.g. [""frontal"", ""left""]\n    object_truncated: is object truncated, e.g. [true, false]\n    object_occluded: is object occluded, e.g. [true, false]\n    object_difficult: is object difficult, e.g. [true, false]\n    object_group_of: is object a single object or a group of objects\n    object_depiction: is object a depiction\n    object_is_crowd: [DEPRECATED, use object_group_of instead]\n      is the object a single object or a crowd\n    object_segment_area: the area of the segment.\n    object_weight: a weight factor for the object\'s bounding box.\n    instance_masks: instance segmentation masks.\n    instance_boundaries: instance boundaries.\n    instance_classes: Classes for each instance segmentation mask.\n    detection_class_label: class label in numbers.\n    detection_bbox_ymin: ymin coordinates of a detection box.\n    detection_bbox_xmin: xmin coordinates of a detection box.\n    detection_bbox_ymax: ymax coordinates of a detection box.\n    detection_bbox_xmax: xmax coordinates of a detection box.\n    detection_score: detection score for the class label and box.\n  """"""\n  image_encoded = \'image/encoded\'\n  image_format = \'image/format\'  # format is reserved keyword\n  filename = \'image/filename\'\n  channels = \'image/channels\'\n  colorspace = \'image/colorspace\'\n  height = \'image/height\'\n  width = \'image/width\'\n  source_id = \'image/source_id\'\n  image_class_text = \'image/class/text\'\n  image_class_label = \'image/class/label\'\n  object_class_text = \'image/object/class/text\'\n  object_class_label = \'image/object/class/label\'\n  object_bbox_ymin = \'image/object/bbox/ymin\'\n  object_bbox_xmin = \'image/object/bbox/xmin\'\n  object_bbox_ymax = \'image/object/bbox/ymax\'\n  object_bbox_xmax = \'image/object/bbox/xmax\'\n  object_view = \'image/object/view\'\n  object_truncated = \'image/object/truncated\'\n  object_occluded = \'image/object/occluded\'\n  object_difficult = \'image/object/difficult\'\n  object_group_of = \'image/object/group_of\'\n  object_depiction = \'image/object/depiction\'\n  object_is_crowd = \'image/object/is_crowd\'\n  object_segment_area = \'image/object/segment/area\'\n  object_weight = \'image/object/weight\'\n  instance_masks = \'image/segmentation/object\'\n  instance_boundaries = \'image/boundaries/object\'\n  instance_classes = \'image/segmentation/object/class\'\n  detection_class_label = \'image/detection/label\'\n  detection_bbox_ymin = \'image/detection/bbox/ymin\'\n  detection_bbox_xmin = \'image/detection/bbox/xmin\'\n  detection_bbox_ymax = \'image/detection/bbox/ymax\'\n  detection_bbox_xmax = \'image/detection/bbox/xmax\'\n  detection_score = \'image/detection/score\'\n'"
src/main/object_detection/protos/string_int_label_map_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/string_int_label_map.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/string_int_label_map.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n2object_detection/protos/string_int_label_map.proto\\x12\\x17object_detection.protos\\""G\\n\\x15StringIntLabelMapItem\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\n\\n\\x02id\\x18\\x02 \\x01(\\x05\\x12\\x14\\n\\x0c\\x64isplay_name\\x18\\x03 \\x01(\\t\\""Q\\n\\x11StringIntLabelMap\\x12<\\n\\x04item\\x18\\x01 \\x03(\\x0b\\x32..object_detection.protos.StringIntLabelMapItem\')\n)\n\n\n\n\n_STRINGINTLABELMAPITEM = _descriptor.Descriptor(\n  name=\'StringIntLabelMapItem\',\n  full_name=\'object_detection.protos.StringIntLabelMapItem\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'object_detection.protos.StringIntLabelMapItem.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'id\', full_name=\'object_detection.protos.StringIntLabelMapItem.id\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'display_name\', full_name=\'object_detection.protos.StringIntLabelMapItem.display_name\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=79,\n  serialized_end=150,\n)\n\n\n_STRINGINTLABELMAP = _descriptor.Descriptor(\n  name=\'StringIntLabelMap\',\n  full_name=\'object_detection.protos.StringIntLabelMap\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'item\', full_name=\'object_detection.protos.StringIntLabelMap.item\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=152,\n  serialized_end=233,\n)\n\n_STRINGINTLABELMAP.fields_by_name[\'item\'].message_type = _STRINGINTLABELMAPITEM\nDESCRIPTOR.message_types_by_name[\'StringIntLabelMapItem\'] = _STRINGINTLABELMAPITEM\nDESCRIPTOR.message_types_by_name[\'StringIntLabelMap\'] = _STRINGINTLABELMAP\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nStringIntLabelMapItem = _reflection.GeneratedProtocolMessageType(\'StringIntLabelMapItem\', (_message.Message,), dict(\n  DESCRIPTOR = _STRINGINTLABELMAPITEM,\n  __module__ = \'object_detection.protos.string_int_label_map_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.StringIntLabelMapItem)\n  ))\n_sym_db.RegisterMessage(StringIntLabelMapItem)\n\nStringIntLabelMap = _reflection.GeneratedProtocolMessageType(\'StringIntLabelMap\', (_message.Message,), dict(\n  DESCRIPTOR = _STRINGINTLABELMAP,\n  __module__ = \'object_detection.protos.string_int_label_map_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.StringIntLabelMap)\n  ))\n_sym_db.RegisterMessage(StringIntLabelMap)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/main/object_detection/utils/label_map_util.py,1,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Label map utility functions.""""""\n\nimport logging\n\nimport tensorflow as tf\nfrom google.protobuf import text_format\nfrom object_detection.protos import string_int_label_map_pb2\n\n\ndef _validate_label_map(label_map):\n  """"""Checks if a label map is valid.\n\n  Args:\n    label_map: StringIntLabelMap to validate.\n\n  Raises:\n    ValueError: if label map is invalid.\n  """"""\n  for item in label_map.item:\n    if item.id < 0:\n      raise ValueError(\'Label map ids should be >= 0.\')\n    if (item.id == 0 and item.name != \'background\' and\n        item.display_name != \'background\'):\n      raise ValueError(\'Label map id 0 is reserved for the background label\')\n\n\ndef create_category_index(categories):\n  """"""Creates dictionary of COCO compatible categories keyed by category id.\n\n  Args:\n    categories: a list of dicts, each of which has the following keys:\n      \'id\': (required) an integer id uniquely identifying this category.\n      \'name\': (required) string representing category name\n        e.g., \'cat\', \'dog\', \'pizza\'.\n\n  Returns:\n    category_index: a dict containing the same entries as categories, but keyed\n      by the \'id\' field of each category.\n  """"""\n  category_index = {}\n  for cat in categories:\n    category_index[cat[\'id\']] = cat\n  return category_index\n\n\ndef get_max_label_map_index(label_map):\n  """"""Get maximum index in label map.\n\n  Args:\n    label_map: a StringIntLabelMapProto\n\n  Returns:\n    an integer\n  """"""\n  return max([item.id for item in label_map.item])\n\n\ndef convert_label_map_to_categories(label_map,\n                                    max_num_classes,\n                                    use_display_name=True):\n  """"""Loads label map proto and returns categories list compatible with eval.\n\n  This function loads a label map and returns a list of dicts, each of which\n  has the following keys:\n    \'id\': (required) an integer id uniquely identifying this category.\n    \'name\': (required) string representing category name\n      e.g., \'cat\', \'dog\', \'pizza\'.\n  We only allow class into the list if its id-label_id_offset is\n  between 0 (inclusive) and max_num_classes (exclusive).\n  If there are several items mapping to the same id in the label map,\n  we will only keep the first one in the categories list.\n\n  Args:\n    label_map: a StringIntLabelMapProto or None.  If None, a default categories\n      list is created with max_num_classes categories.\n    max_num_classes: maximum number of (consecutive) label indices to include.\n    use_display_name: (boolean) choose whether to load \'display_name\' field\n      as category name.  If False or if the display_name field does not exist,\n      uses \'name\' field as category names instead.\n  Returns:\n    categories: a list of dictionaries representing all possible categories.\n  """"""\n  categories = []\n  list_of_ids_already_added = []\n  if not label_map:\n    label_id_offset = 1\n    for class_id in range(max_num_classes):\n      categories.append({\n          \'id\': class_id + label_id_offset,\n          \'name\': \'category_{}\'.format(class_id + label_id_offset)\n      })\n    return categories\n  for item in label_map.item:\n    if not 0 < item.id <= max_num_classes:\n      logging.info(\'Ignore item %d since it falls outside of requested \'\n                   \'label range.\', item.id)\n      continue\n    if use_display_name and item.HasField(\'display_name\'):\n      name = item.display_name\n    else:\n      name = item.name\n    if item.id not in list_of_ids_already_added:\n      list_of_ids_already_added.append(item.id)\n      categories.append({\'id\': item.id, \'name\': name})\n  return categories\n\n\ndef load_labelmap(path):\n  """"""Loads label map proto.\n\n  Args:\n    path: path to StringIntLabelMap proto text file.\n  Returns:\n    a StringIntLabelMapProto\n  """"""\n  with tf.gfile.GFile(path, \'r\') as fid:\n    label_map_string = fid.read()\n    label_map = string_int_label_map_pb2.StringIntLabelMap()\n    try:\n      text_format.Merge(label_map_string, label_map)\n    except text_format.ParseError:\n      label_map.ParseFromString(label_map_string)\n  _validate_label_map(label_map)\n  return label_map\n\n\ndef get_label_map_dict(label_map_path, use_display_name=False):\n  """"""Reads a label map and returns a dictionary of label names to id.\n\n  Args:\n    label_map_path: path to label_map.\n    use_display_name: whether to use the label map items\' display names as keys.\n\n  Returns:\n    A dictionary mapping label names to id.\n  """"""\n  label_map = load_labelmap(label_map_path)\n  label_map_dict = {}\n  for item in label_map.item:\n    if use_display_name:\n      label_map_dict[item.display_name] = item.id\n    else:\n      label_map_dict[item.name] = item.id\n  return label_map_dict\n\n\ndef create_category_index_from_labelmap(label_map_path):\n  """"""Reads a label map and returns a category index.\n\n  Args:\n    label_map_path: Path to `StringIntLabelMap` proto text file.\n\n  Returns:\n    A category index, which is a dictionary that maps integer ids to dicts\n    containing categories, e.g.\n    {1: {\'id\': 1, \'name\': \'dog\'}, 2: {\'id\': 2, \'name\': \'cat\'}, ...}\n  """"""\n  label_map = load_labelmap(label_map_path)\n  max_num_classes = max(item.id for item in label_map.item)\n  categories = convert_label_map_to_categories(label_map, max_num_classes)\n  return create_category_index(categories)\n\n\ndef create_class_agnostic_category_index():\n  """"""Creates a category index with a single `object` class.""""""\n  return {1: {\'id\': 1, \'name\': \'object\'}}\n'"
src/main/object_detection/utils/ops.py,141,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""A module for helper tensorflow ops.""""""\nimport math\nimport numpy as np\nimport six\n\nimport tensorflow as tf\n\nfrom object_detection.core import box_list\nfrom object_detection.core import box_list_ops\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.utils import shape_utils\nfrom object_detection.utils import static_shape\n\n\ndef expanded_shape(orig_shape, start_dim, num_dims):\n  """"""Inserts multiple ones into a shape vector.\n\n  Inserts an all-1 vector of length num_dims at position start_dim into a shape.\n  Can be combined with tf.reshape to generalize tf.expand_dims.\n\n  Args:\n    orig_shape: the shape into which the all-1 vector is added (int32 vector)\n    start_dim: insertion position (int scalar)\n    num_dims: length of the inserted all-1 vector (int scalar)\n  Returns:\n    An int32 vector of length tf.size(orig_shape) + num_dims.\n  """"""\n  with tf.name_scope(\'ExpandedShape\'):\n    start_dim = tf.expand_dims(start_dim, 0)  # scalar to rank-1\n    before = tf.slice(orig_shape, [0], start_dim)\n    add_shape = tf.ones(tf.reshape(num_dims, [1]), dtype=tf.int32)\n    after = tf.slice(orig_shape, start_dim, [-1])\n    new_shape = tf.concat([before, add_shape, after], 0)\n    return new_shape\n\n\ndef normalized_to_image_coordinates(normalized_boxes, image_shape,\n                                    parallel_iterations=32):\n  """"""Converts a batch of boxes from normal to image coordinates.\n\n  Args:\n    normalized_boxes: a float32 tensor of shape [None, num_boxes, 4] in\n      normalized coordinates.\n    image_shape: a float32 tensor of shape [4] containing the image shape.\n    parallel_iterations: parallelism for the map_fn op.\n\n  Returns:\n    absolute_boxes: a float32 tensor of shape [None, num_boxes, 4] containg the\n      boxes in image coordinates.\n  """"""\n  def _to_absolute_coordinates(normalized_boxes):\n    return box_list_ops.to_absolute_coordinates(\n        box_list.BoxList(normalized_boxes),\n        image_shape[1], image_shape[2], check_range=False).get()\n\n  absolute_boxes = shape_utils.static_or_dynamic_map_fn(\n      _to_absolute_coordinates,\n      elems=(normalized_boxes),\n      dtype=tf.float32,\n      parallel_iterations=parallel_iterations,\n      back_prop=True)\n  return absolute_boxes\n\n\ndef meshgrid(x, y):\n  """"""Tiles the contents of x and y into a pair of grids.\n\n  Multidimensional analog of numpy.meshgrid, giving the same behavior if x and y\n  are vectors. Generally, this will give:\n\n  xgrid(i1, ..., i_m, j_1, ..., j_n) = x(j_1, ..., j_n)\n  ygrid(i1, ..., i_m, j_1, ..., j_n) = y(i_1, ..., i_m)\n\n  Keep in mind that the order of the arguments and outputs is reverse relative\n  to the order of the indices they go into, done for compatibility with numpy.\n  The output tensors have the same shapes.  Specifically:\n\n  xgrid.get_shape() = y.get_shape().concatenate(x.get_shape())\n  ygrid.get_shape() = y.get_shape().concatenate(x.get_shape())\n\n  Args:\n    x: A tensor of arbitrary shape and rank. xgrid will contain these values\n       varying in its last dimensions.\n    y: A tensor of arbitrary shape and rank. ygrid will contain these values\n       varying in its first dimensions.\n  Returns:\n    A tuple of tensors (xgrid, ygrid).\n  """"""\n  with tf.name_scope(\'Meshgrid\'):\n    x = tf.convert_to_tensor(x)\n    y = tf.convert_to_tensor(y)\n    x_exp_shape = expanded_shape(tf.shape(x), 0, tf.rank(y))\n    y_exp_shape = expanded_shape(tf.shape(y), tf.rank(y), tf.rank(x))\n\n    xgrid = tf.tile(tf.reshape(x, x_exp_shape), y_exp_shape)\n    ygrid = tf.tile(tf.reshape(y, y_exp_shape), x_exp_shape)\n    new_shape = y.get_shape().concatenate(x.get_shape())\n    xgrid.set_shape(new_shape)\n    ygrid.set_shape(new_shape)\n\n    return xgrid, ygrid\n\n\ndef fixed_padding(inputs, kernel_size, rate=1):\n  """"""Pads the input along the spatial dimensions independently of input size.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n                 Should be a positive integer.\n    rate: An integer, rate for atrous convolution.\n\n  Returns:\n    output: A tensor of size [batch, height_out, width_out, channels] with the\n      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\n  """"""\n  kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n  pad_total = kernel_size_effective - 1\n  pad_beg = pad_total // 2\n  pad_end = pad_total - pad_beg\n  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n                                  [pad_beg, pad_end], [0, 0]])\n  return padded_inputs\n\n\ndef pad_to_multiple(tensor, multiple):\n  """"""Returns the tensor zero padded to the specified multiple.\n\n  Appends 0s to the end of the first and second dimension (height and width) of\n  the tensor until both dimensions are a multiple of the input argument\n  \'multiple\'. E.g. given an input tensor of shape [1, 3, 5, 1] and an input\n  multiple of 4, PadToMultiple will append 0s so that the resulting tensor will\n  be of shape [1, 4, 8, 1].\n\n  Args:\n    tensor: rank 4 float32 tensor, where\n            tensor -> [batch_size, height, width, channels].\n    multiple: the multiple to pad to.\n\n  Returns:\n    padded_tensor: the tensor zero padded to the specified multiple.\n  """"""\n  tensor_shape = tensor.get_shape()\n  batch_size = static_shape.get_batch_size(tensor_shape)\n  tensor_height = static_shape.get_height(tensor_shape)\n  tensor_width = static_shape.get_width(tensor_shape)\n  tensor_depth = static_shape.get_depth(tensor_shape)\n\n  if batch_size is None:\n    batch_size = tf.shape(tensor)[0]\n\n  if tensor_height is None:\n    tensor_height = tf.shape(tensor)[1]\n    padded_tensor_height = tf.to_int32(\n        tf.ceil(tf.to_float(tensor_height) / tf.to_float(multiple))) * multiple\n  else:\n    padded_tensor_height = int(\n        math.ceil(float(tensor_height) / multiple) * multiple)\n\n  if tensor_width is None:\n    tensor_width = tf.shape(tensor)[2]\n    padded_tensor_width = tf.to_int32(\n        tf.ceil(tf.to_float(tensor_width) / tf.to_float(multiple))) * multiple\n  else:\n    padded_tensor_width = int(\n        math.ceil(float(tensor_width) / multiple) * multiple)\n\n  if tensor_depth is None:\n    tensor_depth = tf.shape(tensor)[3]\n\n  # Use tf.concat instead of tf.pad to preserve static shape\n  if padded_tensor_height != tensor_height:\n    height_pad = tf.zeros([\n        batch_size, padded_tensor_height - tensor_height, tensor_width,\n        tensor_depth\n    ])\n    tensor = tf.concat([tensor, height_pad], 1)\n  if padded_tensor_width != tensor_width:\n    width_pad = tf.zeros([\n        batch_size, padded_tensor_height, padded_tensor_width - tensor_width,\n        tensor_depth\n    ])\n    tensor = tf.concat([tensor, width_pad], 2)\n\n  return tensor\n\n\ndef padded_one_hot_encoding(indices, depth, left_pad):\n  """"""Returns a zero padded one-hot tensor.\n\n  This function converts a sparse representation of indices (e.g., [4]) to a\n  zero padded one-hot representation (e.g., [0, 0, 0, 0, 1] with depth = 4 and\n  left_pad = 1). If `indices` is empty, the result will simply be a tensor of\n  shape (0, depth + left_pad). If depth = 0, then this function just returns\n  `None`.\n\n  Args:\n    indices: an integer tensor of shape [num_indices].\n    depth: depth for the one-hot tensor (integer).\n    left_pad: number of zeros to left pad the one-hot tensor with (integer).\n\n  Returns:\n    padded_onehot: a tensor with shape (num_indices, depth + left_pad). Returns\n      `None` if the depth is zero.\n\n  Raises:\n    ValueError: if `indices` does not have rank 1 or if `left_pad` or `depth are\n      either negative or non-integers.\n\n  TODO(rathodv): add runtime checks for depth and indices.\n  """"""\n  if depth < 0 or not isinstance(depth, six.integer_types):\n    raise ValueError(\'`depth` must be a non-negative integer.\')\n  if left_pad < 0 or not isinstance(left_pad, six.integer_types):\n    raise ValueError(\'`left_pad` must be a non-negative integer.\')\n  if depth == 0:\n    return None\n\n  rank = len(indices.get_shape().as_list())\n  if rank != 1:\n    raise ValueError(\'`indices` must have rank 1, but has rank=%s\' % rank)\n\n  def one_hot_and_pad():\n    one_hot = tf.cast(tf.one_hot(tf.cast(indices, tf.int64), depth,\n                                 on_value=1, off_value=0), tf.float32)\n    return tf.pad(one_hot, [[0, 0], [left_pad, 0]], mode=\'CONSTANT\')\n  result = tf.cond(tf.greater(tf.size(indices), 0), one_hot_and_pad,\n                   lambda: tf.zeros((depth + left_pad, 0)))\n  return tf.reshape(result, [-1, depth + left_pad])\n\n\ndef dense_to_sparse_boxes(dense_locations, dense_num_boxes, num_classes):\n  """"""Converts bounding boxes from dense to sparse form.\n\n  Args:\n    dense_locations:  a [max_num_boxes, 4] tensor in which only the first k rows\n      are valid bounding box location coordinates, where k is the sum of\n      elements in dense_num_boxes.\n    dense_num_boxes: a [max_num_classes] tensor indicating the counts of\n       various bounding box classes e.g. [1, 0, 0, 2] means that the first\n       bounding box is of class 0 and the second and third bounding boxes are\n       of class 3. The sum of elements in this tensor is the number of valid\n       bounding boxes.\n    num_classes: number of classes\n\n  Returns:\n    box_locations: a [num_boxes, 4] tensor containing only valid bounding\n       boxes (i.e. the first num_boxes rows of dense_locations)\n    box_classes: a [num_boxes] tensor containing the classes of each bounding\n       box (e.g. dense_num_boxes = [1, 0, 0, 2] => box_classes = [0, 3, 3]\n  """"""\n\n  num_valid_boxes = tf.reduce_sum(dense_num_boxes)\n  box_locations = tf.slice(dense_locations,\n                           tf.constant([0, 0]), tf.stack([num_valid_boxes, 4]))\n  tiled_classes = [tf.tile([i], tf.expand_dims(dense_num_boxes[i], 0))\n                   for i in range(num_classes)]\n  box_classes = tf.concat(tiled_classes, 0)\n  box_locations.set_shape([None, 4])\n  return box_locations, box_classes\n\n\ndef indices_to_dense_vector(indices,\n                            size,\n                            indices_value=1.,\n                            default_value=0,\n                            dtype=tf.float32):\n  """"""Creates dense vector with indices set to specific value and rest to zeros.\n\n  This function exists because it is unclear if it is safe to use\n    tf.sparse_to_dense(indices, [size], 1, validate_indices=False)\n  with indices which are not ordered.\n  This function accepts a dynamic size (e.g. tf.shape(tensor)[0])\n\n  Args:\n    indices: 1d Tensor with integer indices which are to be set to\n        indices_values.\n    size: scalar with size (integer) of output Tensor.\n    indices_value: values of elements specified by indices in the output vector\n    default_value: values of other elements in the output vector.\n    dtype: data type.\n\n  Returns:\n    dense 1D Tensor of shape [size] with indices set to indices_values and the\n        rest set to default_value.\n  """"""\n  size = tf.to_int32(size)\n  zeros = tf.ones([size], dtype=dtype) * default_value\n  values = tf.ones_like(indices, dtype=dtype) * indices_value\n\n  return tf.dynamic_stitch([tf.range(size), tf.to_int32(indices)],\n                           [zeros, values])\n\n\ndef reduce_sum_trailing_dimensions(tensor, ndims):\n  """"""Computes sum across all dimensions following first `ndims` dimensions.""""""\n  return tf.reduce_sum(tensor, axis=tuple(range(ndims, tensor.shape.ndims)))\n\n\ndef retain_groundtruth(tensor_dict, valid_indices):\n  """"""Retains groundtruth by valid indices.\n\n  Args:\n    tensor_dict: a dictionary of following groundtruth tensors -\n      fields.InputDataFields.groundtruth_boxes\n      fields.InputDataFields.groundtruth_classes\n      fields.InputDataFields.groundtruth_keypoints\n      fields.InputDataFields.groundtruth_instance_masks\n      fields.InputDataFields.groundtruth_is_crowd\n      fields.InputDataFields.groundtruth_area\n      fields.InputDataFields.groundtruth_label_types\n      fields.InputDataFields.groundtruth_difficult\n    valid_indices: a tensor with valid indices for the box-level groundtruth.\n\n  Returns:\n    a dictionary of tensors containing only the groundtruth for valid_indices.\n\n  Raises:\n    ValueError: If the shape of valid_indices is invalid.\n    ValueError: field fields.InputDataFields.groundtruth_boxes is\n      not present in tensor_dict.\n  """"""\n  input_shape = valid_indices.get_shape().as_list()\n  if not (len(input_shape) == 1 or\n          (len(input_shape) == 2 and input_shape[1] == 1)):\n    raise ValueError(\'The shape of valid_indices is invalid.\')\n  valid_indices = tf.reshape(valid_indices, [-1])\n  valid_dict = {}\n  if fields.InputDataFields.groundtruth_boxes in tensor_dict:\n    # Prevents reshape failure when num_boxes is 0.\n    num_boxes = tf.maximum(tf.shape(\n        tensor_dict[fields.InputDataFields.groundtruth_boxes])[0], 1)\n    for key in tensor_dict:\n      if key in [fields.InputDataFields.groundtruth_boxes,\n                 fields.InputDataFields.groundtruth_classes,\n                 fields.InputDataFields.groundtruth_keypoints,\n                 fields.InputDataFields.groundtruth_instance_masks]:\n        valid_dict[key] = tf.gather(tensor_dict[key], valid_indices)\n      # Input decoder returns empty tensor when these fields are not provided.\n      # Needs to reshape into [num_boxes, -1] for tf.gather() to work.\n      elif key in [fields.InputDataFields.groundtruth_is_crowd,\n                   fields.InputDataFields.groundtruth_area,\n                   fields.InputDataFields.groundtruth_difficult,\n                   fields.InputDataFields.groundtruth_label_types]:\n        valid_dict[key] = tf.reshape(\n            tf.gather(tf.reshape(tensor_dict[key], [num_boxes, -1]),\n                      valid_indices), [-1])\n      # Fields that are not associated with boxes.\n      else:\n        valid_dict[key] = tensor_dict[key]\n  else:\n    raise ValueError(\'%s not present in input tensor dict.\' % (\n        fields.InputDataFields.groundtruth_boxes))\n  return valid_dict\n\n\ndef retain_groundtruth_with_positive_classes(tensor_dict):\n  """"""Retains only groundtruth with positive class ids.\n\n  Args:\n    tensor_dict: a dictionary of following groundtruth tensors -\n      fields.InputDataFields.groundtruth_boxes\n      fields.InputDataFields.groundtruth_classes\n      fields.InputDataFields.groundtruth_keypoints\n      fields.InputDataFields.groundtruth_instance_masks\n      fields.InputDataFields.groundtruth_is_crowd\n      fields.InputDataFields.groundtruth_area\n      fields.InputDataFields.groundtruth_label_types\n      fields.InputDataFields.groundtruth_difficult\n\n  Returns:\n    a dictionary of tensors containing only the groundtruth with positive\n    classes.\n\n  Raises:\n    ValueError: If groundtruth_classes tensor is not in tensor_dict.\n  """"""\n  if fields.InputDataFields.groundtruth_classes not in tensor_dict:\n    raise ValueError(\'`groundtruth classes` not in tensor_dict.\')\n  keep_indices = tf.where(tf.greater(\n      tensor_dict[fields.InputDataFields.groundtruth_classes], 0))\n  return retain_groundtruth(tensor_dict, keep_indices)\n\n\ndef replace_nan_groundtruth_label_scores_with_ones(label_scores):\n  """"""Replaces nan label scores with 1.0.\n\n  Args:\n    label_scores: a tensor containing object annoation label scores.\n\n  Returns:\n    a tensor where NaN label scores have been replaced by ones.\n  """"""\n  return tf.where(\n      tf.is_nan(label_scores), tf.ones(tf.shape(label_scores)), label_scores)\n\n\ndef filter_groundtruth_with_crowd_boxes(tensor_dict):\n  """"""Filters out groundtruth with boxes corresponding to crowd.\n\n  Args:\n    tensor_dict: a dictionary of following groundtruth tensors -\n      fields.InputDataFields.groundtruth_boxes\n      fields.InputDataFields.groundtruth_classes\n      fields.InputDataFields.groundtruth_keypoints\n      fields.InputDataFields.groundtruth_instance_masks\n      fields.InputDataFields.groundtruth_is_crowd\n      fields.InputDataFields.groundtruth_area\n      fields.InputDataFields.groundtruth_label_types\n\n  Returns:\n    a dictionary of tensors containing only the groundtruth that have bounding\n    boxes.\n  """"""\n  if fields.InputDataFields.groundtruth_is_crowd in tensor_dict:\n    is_crowd = tensor_dict[fields.InputDataFields.groundtruth_is_crowd]\n    is_not_crowd = tf.logical_not(is_crowd)\n    is_not_crowd_indices = tf.where(is_not_crowd)\n    tensor_dict = retain_groundtruth(tensor_dict, is_not_crowd_indices)\n  return tensor_dict\n\n\ndef filter_groundtruth_with_nan_box_coordinates(tensor_dict):\n  """"""Filters out groundtruth with no bounding boxes.\n\n  Args:\n    tensor_dict: a dictionary of following groundtruth tensors -\n      fields.InputDataFields.groundtruth_boxes\n      fields.InputDataFields.groundtruth_classes\n      fields.InputDataFields.groundtruth_keypoints\n      fields.InputDataFields.groundtruth_instance_masks\n      fields.InputDataFields.groundtruth_is_crowd\n      fields.InputDataFields.groundtruth_area\n      fields.InputDataFields.groundtruth_label_types\n\n  Returns:\n    a dictionary of tensors containing only the groundtruth that have bounding\n    boxes.\n  """"""\n  groundtruth_boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]\n  nan_indicator_vector = tf.greater(tf.reduce_sum(tf.to_int32(\n      tf.is_nan(groundtruth_boxes)), reduction_indices=[1]), 0)\n  valid_indicator_vector = tf.logical_not(nan_indicator_vector)\n  valid_indices = tf.where(valid_indicator_vector)\n\n  return retain_groundtruth(tensor_dict, valid_indices)\n\n\ndef normalize_to_target(inputs,\n                        target_norm_value,\n                        dim,\n                        epsilon=1e-7,\n                        trainable=True,\n                        scope=\'NormalizeToTarget\',\n                        summarize=True):\n  """"""L2 normalizes the inputs across the specified dimension to a target norm.\n\n  This op implements the L2 Normalization layer introduced in\n  Liu, Wei, et al. ""SSD: Single Shot MultiBox Detector.""\n  and Liu, Wei, Andrew Rabinovich, and Alexander C. Berg.\n  ""Parsenet: Looking wider to see better."" and is useful for bringing\n  activations from multiple layers in a convnet to a standard scale.\n\n  Note that the rank of `inputs` must be known and the dimension to which\n  normalization is to be applied should be statically defined.\n\n  TODO(jonathanhuang): Add option to scale by L2 norm of the entire input.\n\n  Args:\n    inputs: A `Tensor` of arbitrary size.\n    target_norm_value: A float value that specifies an initial target norm or\n      a list of floats (whose length must be equal to the depth along the\n      dimension to be normalized) specifying a per-dimension multiplier\n      after normalization.\n    dim: The dimension along which the input is normalized.\n    epsilon: A small value to add to the inputs to avoid dividing by zero.\n    trainable: Whether the norm is trainable or not\n    scope: Optional scope for variable_scope.\n    summarize: Whether or not to add a tensorflow summary for the op.\n\n  Returns:\n    The input tensor normalized to the specified target norm.\n\n  Raises:\n    ValueError: If dim is smaller than the number of dimensions in \'inputs\'.\n    ValueError: If target_norm_value is not a float or a list of floats with\n      length equal to the depth along the dimension to be normalized.\n  """"""\n  with tf.variable_scope(scope, \'NormalizeToTarget\', [inputs]):\n    if not inputs.get_shape():\n      raise ValueError(\'The input rank must be known.\')\n    input_shape = inputs.get_shape().as_list()\n    input_rank = len(input_shape)\n    if dim < 0 or dim >= input_rank:\n      raise ValueError(\n          \'dim must be non-negative but smaller than the input rank.\')\n    if not input_shape[dim]:\n      raise ValueError(\'input shape should be statically defined along \'\n                       \'the specified dimension.\')\n    depth = input_shape[dim]\n    if not (isinstance(target_norm_value, float) or\n            (isinstance(target_norm_value, list) and\n             len(target_norm_value) == depth) and\n            all([isinstance(val, float) for val in target_norm_value])):\n      raise ValueError(\'target_norm_value must be a float or a list of floats \'\n                       \'with length equal to the depth along the dimension to \'\n                       \'be normalized.\')\n    if isinstance(target_norm_value, float):\n      initial_norm = depth * [target_norm_value]\n    else:\n      initial_norm = target_norm_value\n    target_norm = tf.contrib.framework.model_variable(\n        name=\'weights\', dtype=tf.float32,\n        initializer=tf.constant(initial_norm, dtype=tf.float32),\n        trainable=trainable)\n    if summarize:\n      mean = tf.reduce_mean(target_norm)\n      mean = tf.Print(mean, [\'NormalizeToTarget:\', mean])\n      tf.summary.scalar(tf.get_variable_scope().name, mean)\n    lengths = epsilon + tf.sqrt(tf.reduce_sum(tf.square(inputs), dim, True))\n    mult_shape = input_rank*[1]\n    mult_shape[dim] = depth\n    return tf.reshape(target_norm, mult_shape) * tf.truediv(inputs, lengths)\n\n\ndef position_sensitive_crop_regions(image,\n                                    boxes,\n                                    box_ind,\n                                    crop_size,\n                                    num_spatial_bins,\n                                    global_pool,\n                                    extrapolation_value=None):\n  """"""Position-sensitive crop and pool rectangular regions from a feature grid.\n\n  The output crops are split into `spatial_bins_y` vertical bins\n  and `spatial_bins_x` horizontal bins. For each intersection of a vertical\n  and a horizontal bin the output values are gathered by performing\n  `tf.image.crop_and_resize` (bilinear resampling) on a a separate subset of\n  channels of the image. This reduces `depth` by a factor of\n  `(spatial_bins_y * spatial_bins_x)`.\n\n  When global_pool is True, this function implements a differentiable version\n  of position-sensitive RoI pooling used in\n  [R-FCN detection system](https://arxiv.org/abs/1605.06409).\n\n  When global_pool is False, this function implements a differentiable version\n  of position-sensitive assembling operation used in\n  [instance FCN](https://arxiv.org/abs/1603.08678).\n\n  Args:\n    image: A `Tensor`. Must be one of the following types: `uint8`, `int8`,\n      `int16`, `int32`, `int64`, `half`, `float32`, `float64`.\n      A 4-D tensor of shape `[batch, image_height, image_width, depth]`.\n      Both `image_height` and `image_width` need to be positive.\n    boxes: A `Tensor` of type `float32`.\n      A 2-D tensor of shape `[num_boxes, 4]`. The `i`-th row of the tensor\n      specifies the coordinates of a box in the `box_ind[i]` image and is\n      specified in normalized coordinates `[y1, x1, y2, x2]`. A normalized\n      coordinate value of `y` is mapped to the image coordinate at\n      `y * (image_height - 1)`, so as the `[0, 1]` interval of normalized image\n      height is mapped to `[0, image_height - 1] in image height coordinates.\n      We do allow y1 > y2, in which case the sampled crop is an up-down flipped\n      version of the original image. The width dimension is treated similarly.\n      Normalized coordinates outside the `[0, 1]` range are allowed, in which\n      case we use `extrapolation_value` to extrapolate the input image values.\n    box_ind:  A `Tensor` of type `int32`.\n      A 1-D tensor of shape `[num_boxes]` with int32 values in `[0, batch)`.\n      The value of `box_ind[i]` specifies the image that the `i`-th box refers\n      to.\n    crop_size: A list of two integers `[crop_height, crop_width]`. All\n      cropped image patches are resized to this size. The aspect ratio of the\n      image content is not preserved. Both `crop_height` and `crop_width` need\n      to be positive.\n    num_spatial_bins: A list of two integers `[spatial_bins_y, spatial_bins_x]`.\n      Represents the number of position-sensitive bins in y and x directions.\n      Both values should be >= 1. `crop_height` should be divisible by\n      `spatial_bins_y`, and similarly for width.\n      The number of image channels should be divisible by\n      (spatial_bins_y * spatial_bins_x).\n      Suggested value from R-FCN paper: [3, 3].\n    global_pool: A boolean variable.\n      If True, we perform average global pooling on the features assembled from\n        the position-sensitive score maps.\n      If False, we keep the position-pooled features without global pooling\n        over the spatial coordinates.\n      Note that using global_pool=True is equivalent to but more efficient than\n        running the function with global_pool=False and then performing global\n        average pooling.\n    extrapolation_value: An optional `float`. Defaults to `0`.\n      Value used for extrapolation, when applicable.\n  Returns:\n    position_sensitive_features: A 4-D tensor of shape\n      `[num_boxes, K, K, crop_channels]`,\n      where `crop_channels = depth / (spatial_bins_y * spatial_bins_x)`,\n      where K = 1 when global_pool is True (Average-pooled cropped regions),\n      and K = crop_size when global_pool is False.\n  Raises:\n    ValueError: Raised in four situations:\n      `num_spatial_bins` is not >= 1;\n      `num_spatial_bins` does not divide `crop_size`;\n      `(spatial_bins_y*spatial_bins_x)` does not divide `depth`;\n      `bin_crop_size` is not square when global_pool=False due to the\n        constraint in function space_to_depth.\n  """"""\n  total_bins = 1\n  bin_crop_size = []\n\n  for (num_bins, crop_dim) in zip(num_spatial_bins, crop_size):\n    if num_bins < 1:\n      raise ValueError(\'num_spatial_bins should be >= 1\')\n\n    if crop_dim % num_bins != 0:\n      raise ValueError(\'crop_size should be divisible by num_spatial_bins\')\n\n    total_bins *= num_bins\n    bin_crop_size.append(crop_dim // num_bins)\n\n  if not global_pool and bin_crop_size[0] != bin_crop_size[1]:\n    raise ValueError(\'Only support square bin crop size for now.\')\n\n  ymin, xmin, ymax, xmax = tf.unstack(boxes, axis=1)\n  spatial_bins_y, spatial_bins_x = num_spatial_bins\n\n  # Split each box into spatial_bins_y * spatial_bins_x bins.\n  position_sensitive_boxes = []\n  for bin_y in range(spatial_bins_y):\n    step_y = (ymax - ymin) / spatial_bins_y\n    for bin_x in range(spatial_bins_x):\n      step_x = (xmax - xmin) / spatial_bins_x\n      box_coordinates = [ymin + bin_y * step_y,\n                         xmin + bin_x * step_x,\n                         ymin + (bin_y + 1) * step_y,\n                         xmin + (bin_x + 1) * step_x,\n                        ]\n      position_sensitive_boxes.append(tf.stack(box_coordinates, axis=1))\n\n  image_splits = tf.split(value=image, num_or_size_splits=total_bins, axis=3)\n\n  image_crops = []\n  for (split, box) in zip(image_splits, position_sensitive_boxes):\n    crop = tf.image.crop_and_resize(split, box, box_ind, bin_crop_size,\n                                    extrapolation_value=extrapolation_value)\n    image_crops.append(crop)\n\n  if global_pool:\n    # Average over all bins.\n    position_sensitive_features = tf.add_n(image_crops) / len(image_crops)\n    # Then average over spatial positions within the bins.\n    position_sensitive_features = tf.reduce_mean(\n        position_sensitive_features, [1, 2], keep_dims=True)\n  else:\n    # Reorder height/width to depth channel.\n    block_size = bin_crop_size[0]\n    if block_size >= 2:\n      image_crops = [tf.space_to_depth(\n          crop, block_size=block_size) for crop in image_crops]\n\n    # Pack image_crops so that first dimension is for position-senstive boxes.\n    position_sensitive_features = tf.stack(image_crops, axis=0)\n\n    # Unroll the position-sensitive boxes to spatial positions.\n    position_sensitive_features = tf.squeeze(\n        tf.batch_to_space_nd(position_sensitive_features,\n                             block_shape=[1] + num_spatial_bins,\n                             crops=tf.zeros((3, 2), dtype=tf.int32)),\n        squeeze_dims=[0])\n\n    # Reorder back the depth channel.\n    if block_size >= 2:\n      position_sensitive_features = tf.depth_to_space(\n          position_sensitive_features, block_size=block_size)\n\n  return position_sensitive_features\n\n\ndef reframe_box_masks_to_image_masks(box_masks, boxes, image_height,\n                                     image_width):\n  """"""Transforms the box masks back to full image masks.\n\n  Embeds masks in bounding boxes of larger masks whose shapes correspond to\n  image shape.\n\n  Args:\n    box_masks: A tf.float32 tensor of size [num_masks, mask_height, mask_width].\n    boxes: A tf.float32 tensor of size [num_masks, 4] containing the box\n           corners. Row i contains [ymin, xmin, ymax, xmax] of the box\n           corresponding to mask i. Note that the box corners are in\n           normalized coordinates.\n    image_height: Image height. The output mask will have the same height as\n                  the image height.\n    image_width: Image width. The output mask will have the same width as the\n                 image width.\n\n  Returns:\n    A tf.float32 tensor of size [num_masks, image_height, image_width].\n  """"""\n  # TODO(rathodv): Make this a public function.\n  def reframe_box_masks_to_image_masks_default():\n    """"""The default function when there are more than 0 box masks.""""""\n    def transform_boxes_relative_to_boxes(boxes, reference_boxes):\n      boxes = tf.reshape(boxes, [-1, 2, 2])\n      min_corner = tf.expand_dims(reference_boxes[:, 0:2], 1)\n      max_corner = tf.expand_dims(reference_boxes[:, 2:4], 1)\n      transformed_boxes = (boxes - min_corner) / (max_corner - min_corner)\n      return tf.reshape(transformed_boxes, [-1, 4])\n\n    box_masks_expanded = tf.expand_dims(box_masks, axis=3)\n    num_boxes = tf.shape(box_masks_expanded)[0]\n    unit_boxes = tf.concat(\n        [tf.zeros([num_boxes, 2]), tf.ones([num_boxes, 2])], axis=1)\n    reverse_boxes = transform_boxes_relative_to_boxes(unit_boxes, boxes)\n    return tf.image.crop_and_resize(\n        image=box_masks_expanded,\n        boxes=reverse_boxes,\n        box_ind=tf.range(num_boxes),\n        crop_size=[image_height, image_width],\n        extrapolation_value=0.0)\n  image_masks = tf.cond(\n      tf.shape(box_masks)[0] > 0,\n      reframe_box_masks_to_image_masks_default,\n      lambda: tf.zeros([0, image_height, image_width, 1], dtype=tf.float32))\n  return tf.squeeze(image_masks, axis=3)\n\n\ndef merge_boxes_with_multiple_labels(boxes, classes, num_classes):\n  """"""Merges boxes with same coordinates and returns K-hot encoded classes.\n\n  Args:\n    boxes: A tf.float32 tensor with shape [N, 4] holding N boxes.\n    classes: A tf.int32 tensor with shape [N] holding class indices.\n      The class index starts at 0.\n    num_classes: total number of classes to use for K-hot encoding.\n\n  Returns:\n    merged_boxes: A tf.float32 tensor with shape [N\', 4] holding boxes,\n      where N\' <= N.\n    class_encodings: A tf.int32 tensor with shape [N\', num_classes] holding\n      k-hot encodings for the merged boxes.\n    merged_box_indices: A tf.int32 tensor with shape [N\'] holding original\n      indices of the boxes.\n  """"""\n  def merge_numpy_boxes(boxes, classes, num_classes):\n    """"""Python function to merge numpy boxes.""""""\n    if boxes.size < 1:\n      return (np.zeros([0, 4], dtype=np.float32),\n              np.zeros([0, num_classes], dtype=np.int32),\n              np.zeros([0], dtype=np.int32))\n    box_to_class_indices = {}\n    for box_index in range(boxes.shape[0]):\n      box = tuple(boxes[box_index, :].tolist())\n      class_index = classes[box_index]\n      if box not in box_to_class_indices:\n        box_to_class_indices[box] = [box_index, np.zeros([num_classes])]\n      box_to_class_indices[box][1][class_index] = 1\n    merged_boxes = np.vstack(box_to_class_indices.keys()).astype(np.float32)\n    class_encodings = [item[1] for item in box_to_class_indices.values()]\n    class_encodings = np.vstack(class_encodings).astype(np.int32)\n    merged_box_indices = [item[0] for item in box_to_class_indices.values()]\n    merged_box_indices = np.array(merged_box_indices).astype(np.int32)\n    return merged_boxes, class_encodings, merged_box_indices\n\n  merged_boxes, class_encodings, merged_box_indices = tf.py_func(\n      merge_numpy_boxes, [boxes, classes, num_classes],\n      [tf.float32, tf.int32, tf.int32])\n  merged_boxes = tf.reshape(merged_boxes, [-1, 4])\n  class_encodings = tf.reshape(class_encodings, [-1, num_classes])\n  merged_box_indices = tf.reshape(merged_box_indices, [-1])\n  return merged_boxes, class_encodings, merged_box_indices\n\n\ndef nearest_neighbor_upsampling(input_tensor, scale):\n  """"""Nearest neighbor upsampling implementation.\n\n  Nearest neighbor upsampling function that maps input tensor with shape\n  [batch_size, height, width, channels] to [batch_size, height * scale\n  , width * scale, channels]. This implementation only uses reshape and\n  broadcasting to make it TPU compatible.\n\n  Args:\n    input_tensor: A float32 tensor of size [batch, height_in, width_in,\n      channels].\n    scale: An integer multiple to scale resolution of input data.\n  Returns:\n    data_up: A float32 tensor of size\n      [batch, height_in*scale, width_in*scale, channels].\n  """"""\n  with tf.name_scope(\'nearest_neighbor_upsampling\'):\n    (batch_size, height, width,\n     channels) = shape_utils.combined_static_and_dynamic_shape(input_tensor)\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, height, 1, width, 1, channels]) * tf.ones(\n            [1, 1, scale, 1, scale, 1], dtype=input_tensor.dtype)\n    return tf.reshape(output_tensor,\n                      [batch_size, height * scale, width * scale, channels])\n\n\ndef matmul_gather_on_zeroth_axis(params, indices, scope=None):\n  """"""Matrix multiplication based implementation of tf.gather on zeroth axis.\n\n  TODO(rathodv, jonathanhuang): enable sparse matmul option.\n\n  Args:\n    params: A float32 Tensor. The tensor from which to gather values.\n      Must be at least rank 1.\n    indices: A Tensor. Must be one of the following types: int32, int64.\n      Must be in range [0, params.shape[0])\n    scope: A name for the operation (optional).\n\n  Returns:\n    A Tensor. Has the same type as params. Values from params gathered\n    from indices given by indices, with shape indices.shape + params.shape[1:].\n  """"""\n  with tf.name_scope(scope, \'MatMulGather\'):\n    params_shape = shape_utils.combined_static_and_dynamic_shape(params)\n    indices_shape = shape_utils.combined_static_and_dynamic_shape(indices)\n    params2d = tf.reshape(params, [params_shape[0], -1])\n    indicator_matrix = tf.one_hot(indices, params_shape[0])\n    gathered_result_flattened = tf.matmul(indicator_matrix, params2d)\n    return tf.reshape(gathered_result_flattened,\n                      tf.stack(indices_shape + params_shape[1:]))\n\n\ndef matmul_crop_and_resize(image, boxes, crop_size, scope=None):\n  """"""Matrix multiplication based implementation of the crop and resize op.\n\n  Extracts crops from the input image tensor and bilinearly resizes them\n  (possibly with aspect ratio change) to a common output size specified by\n  crop_size. This is more general than the crop_to_bounding_box op which\n  extracts a fixed size slice from the input image and does not allow\n  resizing or aspect ratio change.\n\n  Returns a tensor with crops from the input image at positions defined at\n  the bounding box locations in boxes. The cropped boxes are all resized\n  (with bilinear interpolation) to a fixed size = `[crop_height, crop_width]`.\n  The result is a 4-D tensor `[num_boxes, crop_height, crop_width, depth]`.\n\n  Running time complexity:\n    O((# channels) * (# boxes) * (crop_size)^2 * M), where M is the number\n  of pixels of the longer edge of the image.\n\n  Note that this operation is meant to replicate the behavior of the standard\n  tf.image.crop_and_resize operation but there are a few differences.\n  Specifically:\n    1) The extrapolation value (the values that are interpolated from outside\n      the bounds of the image window) is always zero\n    2) Only XLA supported operations are used (e.g., matrix multiplication).\n    3) There is no `box_indices` argument --- to run this op on multiple images,\n      one must currently call this op independently on each image.\n    4) All shapes and the `crop_size` parameter are assumed to be statically\n      defined.  Moreover, the number of boxes must be strictly nonzero.\n\n  Args:\n    image: A `Tensor`. Must be one of the following types: `uint8`, `int8`,\n      `int16`, `int32`, `int64`, `half`, `float32`, `float64`.\n      A 4-D tensor of shape `[batch, image_height, image_width, depth]`.\n      Both `image_height` and `image_width` need to be positive.\n    boxes: A `Tensor` of type `float32`.\n      A 2-D tensor of shape `[num_boxes, 4]`. The `i`-th row of the tensor\n      specifies the coordinates of a box in the `box_ind[i]` image and is\n      specified in normalized coordinates `[y1, x1, y2, x2]`. A normalized\n      coordinate value of `y` is mapped to the image coordinate at\n      `y * (image_height - 1)`, so as the `[0, 1]` interval of normalized image\n      height is mapped to `[0, image_height - 1] in image height coordinates.\n      We do allow y1 > y2, in which case the sampled crop is an up-down flipped\n      version of the original image. The width dimension is treated similarly.\n      Normalized coordinates outside the `[0, 1]` range are allowed, in which\n      case we use `extrapolation_value` to extrapolate the input image values.\n    crop_size: A list of two integers `[crop_height, crop_width]`. All\n      cropped image patches are resized to this size. The aspect ratio of the\n      image content is not preserved. Both `crop_height` and `crop_width` need\n      to be positive.\n    scope: A name for the operation (optional).\n\n  Returns:\n    A 4-D tensor of shape `[num_boxes, crop_height, crop_width, depth]`\n\n  Raises:\n    ValueError: if image tensor does not have shape\n      `[1, image_height, image_width, depth]` and all dimensions statically\n      defined.\n    ValueError: if boxes tensor does not have shape `[num_boxes, 4]` where\n      num_boxes > 0.\n    ValueError: if crop_size is not a list of two positive integers\n  """"""\n  img_shape = image.shape.as_list()\n  boxes_shape = boxes.shape.as_list()\n  _, img_height, img_width, _ = img_shape\n  if not isinstance(crop_size, list) or len(crop_size) != 2:\n    raise ValueError(\'`crop_size` must be a list of length 2\')\n  dimensions = img_shape + crop_size + boxes_shape\n  if not all([isinstance(dim, int) for dim in dimensions]):\n    raise ValueError(\'all input shapes must be statically defined\')\n  if len(crop_size) != 2:\n    raise ValueError(\'`crop_size` must be a list of length 2\')\n  if len(boxes_shape) != 2 or boxes_shape[1] != 4:\n    raise ValueError(\'`boxes` should have shape `[num_boxes, 4]`\')\n  if len(img_shape) != 4 and img_shape[0] != 1:\n    raise ValueError(\'image should have shape \'\n                     \'`[1, image_height, image_width, depth]`\')\n  num_crops = boxes_shape[0]\n  if not num_crops > 0:\n    raise ValueError(\'number of boxes must be > 0\')\n  if not (crop_size[0] > 0 and crop_size[1] > 0):\n    raise ValueError(\'`crop_size` must be a list of two positive integers.\')\n\n  def _lin_space_weights(num, img_size):\n    if num > 1:\n      alpha = (img_size - 1) / float(num - 1)\n      indices = np.reshape(np.arange(num), (1, num))\n      start_weights = alpha * (num - 1 - indices)\n      stop_weights = alpha * indices\n    else:\n      start_weights = num * [.5 * (img_size - 1)]\n      stop_weights = num * [.5 * (img_size - 1)]\n    return (tf.constant(start_weights, dtype=tf.float32),\n            tf.constant(stop_weights, dtype=tf.float32))\n\n  with tf.name_scope(scope, \'MatMulCropAndResize\'):\n    y1_weights, y2_weights = _lin_space_weights(crop_size[0], img_height)\n    x1_weights, x2_weights = _lin_space_weights(crop_size[1], img_width)\n    [y1, x1, y2, x2] = tf.split(value=boxes, num_or_size_splits=4, axis=1)\n\n    # Pixel centers of input image and grid points along height and width\n    image_idx_h = tf.constant(\n        np.reshape(np.arange(img_height), (1, 1, img_height)), dtype=tf.float32)\n    image_idx_w = tf.constant(\n        np.reshape(np.arange(img_width), (1, 1, img_width)), dtype=tf.float32)\n    grid_pos_h = tf.expand_dims(y1 * y1_weights + y2 * y2_weights, 2)\n    grid_pos_w = tf.expand_dims(x1 * x1_weights + x2 * x2_weights, 2)\n\n    # Create kernel matrices of pairwise kernel evaluations between pixel\n    # centers of image and grid points.\n    kernel_h = tf.nn.relu(1 - tf.abs(image_idx_h - grid_pos_h))\n    kernel_w = tf.nn.relu(1 - tf.abs(image_idx_w - grid_pos_w))\n\n    # TODO(jonathanhuang): investigate whether all channels can be processed\n    # without the explicit unstack --- possibly with a permute and map_fn call.\n    result_channels = []\n    for channel in tf.unstack(image, axis=3):\n      result_channels.append(\n          tf.matmul(\n              tf.matmul(kernel_h, tf.tile(channel, [num_crops, 1, 1])),\n              kernel_w, transpose_b=True))\n    return tf.stack(result_channels, axis=3)\n'"
src/main/object_detection/utils/shape_utils.py,39,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utils used to manipulate tensor shapes.""""""\n\nimport tensorflow as tf\n\nfrom object_detection.utils import static_shape\n\n\ndef _is_tensor(t):\n  """"""Returns a boolean indicating whether the input is a tensor.\n\n  Args:\n    t: the input to be tested.\n\n  Returns:\n    a boolean that indicates whether t is a tensor.\n  """"""\n  return isinstance(t, (tf.Tensor, tf.SparseTensor, tf.Variable))\n\n\ndef _set_dim_0(t, d0):\n  """"""Sets the 0-th dimension of the input tensor.\n\n  Args:\n    t: the input tensor, assuming the rank is at least 1.\n    d0: an integer indicating the 0-th dimension of the input tensor.\n\n  Returns:\n    the tensor t with the 0-th dimension set.\n  """"""\n  t_shape = t.get_shape().as_list()\n  t_shape[0] = d0\n  t.set_shape(t_shape)\n  return t\n\n\ndef pad_tensor(t, length):\n  """"""Pads the input tensor with 0s along the first dimension up to the length.\n\n  Args:\n    t: the input tensor, assuming the rank is at least 1.\n    length: a tensor of shape [1]  or an integer, indicating the first dimension\n      of the input tensor t after padding, assuming length <= t.shape[0].\n\n  Returns:\n    padded_t: the padded tensor, whose first dimension is length. If the length\n      is an integer, the first dimension of padded_t is set to length\n      statically.\n  """"""\n  t_rank = tf.rank(t)\n  t_shape = tf.shape(t)\n  t_d0 = t_shape[0]\n  pad_d0 = tf.expand_dims(length - t_d0, 0)\n  pad_shape = tf.cond(\n      tf.greater(t_rank, 1), lambda: tf.concat([pad_d0, t_shape[1:]], 0),\n      lambda: tf.expand_dims(length - t_d0, 0))\n  padded_t = tf.concat([t, tf.zeros(pad_shape, dtype=t.dtype)], 0)\n  if not _is_tensor(length):\n    padded_t = _set_dim_0(padded_t, length)\n  return padded_t\n\n\ndef clip_tensor(t, length):\n  """"""Clips the input tensor along the first dimension up to the length.\n\n  Args:\n    t: the input tensor, assuming the rank is at least 1.\n    length: a tensor of shape [1]  or an integer, indicating the first dimension\n      of the input tensor t after clipping, assuming length <= t.shape[0].\n\n  Returns:\n    clipped_t: the clipped tensor, whose first dimension is length. If the\n      length is an integer, the first dimension of clipped_t is set to length\n      statically.\n  """"""\n  clipped_t = tf.gather(t, tf.range(length))\n  if not _is_tensor(length):\n    clipped_t = _set_dim_0(clipped_t, length)\n  return clipped_t\n\n\ndef pad_or_clip_tensor(t, length):\n  """"""Pad or clip the input tensor along the first dimension.\n\n  Args:\n    t: the input tensor, assuming the rank is at least 1.\n    length: a tensor of shape [1]  or an integer, indicating the first dimension\n      of the input tensor t after processing.\n\n  Returns:\n    processed_t: the processed tensor, whose first dimension is length. If the\n      length is an integer, the first dimension of the processed tensor is set\n      to length statically.\n  """"""\n  processed_t = tf.cond(\n      tf.greater(tf.shape(t)[0], length),\n      lambda: clip_tensor(t, length),\n      lambda: pad_tensor(t, length))\n  if not _is_tensor(length):\n    processed_t = _set_dim_0(processed_t, length)\n  return processed_t\n\n\ndef combined_static_and_dynamic_shape(tensor):\n  """"""Returns a list containing static and dynamic values for the dimensions.\n\n  Returns a list of static and dynamic values for shape dimensions. This is\n  useful to preserve static shapes when available in reshape operation.\n\n  Args:\n    tensor: A tensor of any type.\n\n  Returns:\n    A list of size tensor.shape.ndims containing integers or a scalar tensor.\n  """"""\n  static_tensor_shape = tensor.shape.as_list()\n  dynamic_tensor_shape = tf.shape(tensor)\n  combined_shape = []\n  for index, dim in enumerate(static_tensor_shape):\n    if dim is not None:\n      combined_shape.append(dim)\n    else:\n      combined_shape.append(dynamic_tensor_shape[index])\n  return combined_shape\n\n\ndef static_or_dynamic_map_fn(fn, elems, dtype=None,\n                             parallel_iterations=32, back_prop=True):\n  """"""Runs map_fn as a (static) for loop when possible.\n\n  This function rewrites the map_fn as an explicit unstack input -> for loop\n  over function calls -> stack result combination.  This allows our graphs to\n  be acyclic when the batch size is static.\n  For comparison, see https://www.tensorflow.org/api_docs/python/tf/map_fn.\n\n  Note that `static_or_dynamic_map_fn` currently is not *fully* interchangeable\n  with the default tf.map_fn function as it does not accept nested inputs (only\n  Tensors or lists of Tensors).  Likewise, the output of `fn` can only be a\n  Tensor or list of Tensors.\n\n  TODO(jonathanhuang): make this function fully interchangeable with tf.map_fn.\n\n  Args:\n    fn: The callable to be performed. It accepts one argument, which will have\n      the same structure as elems. Its output must have the\n      same structure as elems.\n    elems: A tensor or list of tensors, each of which will\n      be unpacked along their first dimension. The sequence of the\n      resulting slices will be applied to fn.\n    dtype:  (optional) The output type(s) of fn. If fn returns a structure of\n      Tensors differing from the structure of elems, then dtype is not optional\n      and must have the same structure as the output of fn.\n    parallel_iterations: (optional) number of batch items to process in\n      parallel.  This flag is only used if the native tf.map_fn is used\n      and defaults to 32 instead of 10 (unlike the standard tf.map_fn default).\n    back_prop: (optional) True enables support for back propagation.\n      This flag is only used if the native tf.map_fn is used.\n\n  Returns:\n    A tensor or sequence of tensors. Each tensor packs the\n    results of applying fn to tensors unpacked from elems along the first\n    dimension, from first to last.\n  Raises:\n    ValueError: if `elems` a Tensor or a list of Tensors.\n    ValueError: if `fn` does not return a Tensor or list of Tensors\n  """"""\n  if isinstance(elems, list):\n    for elem in elems:\n      if not isinstance(elem, tf.Tensor):\n        raise ValueError(\'`elems` must be a Tensor or list of Tensors.\')\n\n    elem_shapes = [elem.shape.as_list() for elem in elems]\n    # Fall back on tf.map_fn if shapes of each entry of `elems` are None or fail\n    # to all be the same size along the batch dimension.\n    for elem_shape in elem_shapes:\n      if (not elem_shape or not elem_shape[0]\n          or elem_shape[0] != elem_shapes[0][0]):\n        return tf.map_fn(fn, elems, dtype, parallel_iterations, back_prop)\n    arg_tuples = zip(*[tf.unstack(elem) for elem in elems])\n    outputs = [fn(arg_tuple) for arg_tuple in arg_tuples]\n  else:\n    if not isinstance(elems, tf.Tensor):\n      raise ValueError(\'`elems` must be a Tensor or list of Tensors.\')\n    elems_shape = elems.shape.as_list()\n    if not elems_shape or not elems_shape[0]:\n      return tf.map_fn(fn, elems, dtype, parallel_iterations, back_prop)\n    outputs = [fn(arg) for arg in tf.unstack(elems)]\n  # Stack `outputs`, which is a list of Tensors or list of lists of Tensors\n  if all([isinstance(output, tf.Tensor) for output in outputs]):\n    return tf.stack(outputs)\n  else:\n    if all([isinstance(output, list) for output in outputs]):\n      if all([all(\n          [isinstance(entry, tf.Tensor) for entry in output_list])\n              for output_list in outputs]):\n        return [tf.stack(output_tuple) for output_tuple in zip(*outputs)]\n  raise ValueError(\'`fn` should return a Tensor or a list of Tensors.\')\n\n\ndef check_min_image_dim(min_dim, image_tensor):\n  """"""Checks that the image width/height are greater than some number.\n\n  This function is used to check that the width and height of an image are above\n  a certain value. If the image shape is static, this function will perform the\n  check at graph construction time. Otherwise, if the image shape varies, an\n  Assertion control dependency will be added to the graph.\n\n  Args:\n    min_dim: The minimum number of pixels along the width and height of the\n             image.\n    image_tensor: The image tensor to check size for.\n\n  Returns:\n    If `image_tensor` has dynamic size, return `image_tensor` with a Assert\n    control dependency. Otherwise returns image_tensor.\n\n  Raises:\n    ValueError: if `image_tensor`\'s\' width or height is smaller than `min_dim`.\n  """"""\n  image_shape = image_tensor.get_shape()\n  image_height = static_shape.get_height(image_shape)\n  image_width = static_shape.get_width(image_shape)\n  if image_height is None or image_width is None:\n    shape_assert = tf.Assert(\n        tf.logical_and(tf.greater_equal(tf.shape(image_tensor)[1], min_dim),\n                       tf.greater_equal(tf.shape(image_tensor)[2], min_dim)),\n        [\'image size must be >= {} in both height and width.\'.format(min_dim)])\n    with tf.control_dependencies([shape_assert]):\n      return tf.identity(image_tensor)\n\n  if image_height < min_dim or image_width < min_dim:\n    raise ValueError(\n        \'image size must be >= %d in both height and width; image dim = %d,%d\' %\n        (min_dim, image_height, image_width))\n\n  return image_tensor\n\n\ndef assert_shape_equal(shape_a, shape_b):\n  """"""Asserts that shape_a and shape_b are equal.\n\n  If the shapes are static, raises a ValueError when the shapes\n  mismatch.\n\n  If the shapes are dynamic, raises a tf InvalidArgumentError when the shapes\n  mismatch.\n\n  Args:\n    shape_a: a list containing shape of the first tensor.\n    shape_b: a list containing shape of the second tensor.\n\n  Returns:\n    Either a tf.no_op() when shapes are all static and a tf.assert_equal() op\n    when the shapes are dynamic.\n\n  Raises:\n    ValueError: When shapes are both static and unequal.\n  """"""\n  if (all(isinstance(dim, int) for dim in shape_a) and\n      all(isinstance(dim, int) for dim in shape_b)):\n    if shape_a != shape_b:\n      raise ValueError(\'Unequal shapes {}, {}\'.format(shape_a, shape_b))\n    else: return tf.no_op()\n  else:\n    return tf.assert_equal(shape_a, shape_b)\n\n\ndef assert_shape_equal_along_first_dimension(shape_a, shape_b):\n  """"""Asserts that shape_a and shape_b are the same along the 0th-dimension.\n\n  If the shapes are static, raises a ValueError when the shapes\n  mismatch.\n\n  If the shapes are dynamic, raises a tf InvalidArgumentError when the shapes\n  mismatch.\n\n  Args:\n    shape_a: a list containing shape of the first tensor.\n    shape_b: a list containing shape of the second tensor.\n\n  Returns:\n    Either a tf.no_op() when shapes are all static and a tf.assert_equal() op\n    when the shapes are dynamic.\n\n  Raises:\n    ValueError: When shapes are both static and unequal.\n  """"""\n  if isinstance(shape_a[0], int) and isinstance(shape_b[0], int):\n    if shape_a[0] != shape_b[0]:\n      raise ValueError(\'Unequal first dimension {}, {}\'.format(\n          shape_a[0], shape_b[0]))\n    else: return tf.no_op()\n  else:\n    return tf.assert_equal(shape_a[0], shape_b[0])\n\n'"
src/main/object_detection/utils/static_shape.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Helper functions to access TensorShape values.\n\nThe rank 4 tensor_shape must be of the form [batch_size, height, width, depth].\n""""""\n\n\ndef get_batch_size(tensor_shape):\n  """"""Returns batch size from the tensor shape.\n\n  Args:\n    tensor_shape: A rank 4 TensorShape.\n\n  Returns:\n    An integer representing the batch size of the tensor.\n  """"""\n  tensor_shape.assert_has_rank(rank=4)\n  return tensor_shape[0].value\n\n\ndef get_height(tensor_shape):\n  """"""Returns height from the tensor shape.\n\n  Args:\n    tensor_shape: A rank 4 TensorShape.\n\n  Returns:\n    An integer representing the height of the tensor.\n  """"""\n  tensor_shape.assert_has_rank(rank=4)\n  return tensor_shape[1].value\n\n\ndef get_width(tensor_shape):\n  """"""Returns width from the tensor shape.\n\n  Args:\n    tensor_shape: A rank 4 TensorShape.\n\n  Returns:\n    An integer representing the width of the tensor.\n  """"""\n  tensor_shape.assert_has_rank(rank=4)\n  return tensor_shape[2].value\n\n\ndef get_depth(tensor_shape):\n  """"""Returns depth from the tensor shape.\n\n  Args:\n    tensor_shape: A rank 4 TensorShape.\n\n  Returns:\n    An integer representing the depth of the tensor.\n  """"""\n  tensor_shape.assert_has_rank(rank=4)\n  return tensor_shape[3].value\n'"
src/main/object_detection/utils/visualization_utils.py,26,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""A set of functions that are used for visualization.\n\nThese functions often receive an image, perform some visualization on the image.\nThe functions do not return a value, instead they modify the image itself.\n\n""""""\nimport collections\nimport functools\n# Set headless-friendly backend.\nimport matplotlib; matplotlib.use(\'Agg\')  # pylint: disable=multiple-statements\nimport matplotlib.pyplot as plt  # pylint: disable=g-import-not-at-top\nimport numpy as np\nimport PIL.Image as Image\nimport PIL.ImageColor as ImageColor\nimport PIL.ImageDraw as ImageDraw\nimport PIL.ImageFont as ImageFont\nimport six\nimport tensorflow as tf\n\nfrom object_detection.core import standard_fields as fields\n\n\n_TITLE_LEFT_MARGIN = 10\n_TITLE_TOP_MARGIN = 10\nSTANDARD_COLORS = [\n    \'AliceBlue\', \'Chartreuse\', \'Aqua\', \'Aquamarine\', \'Azure\', \'Beige\', \'Bisque\',\n    \'BlanchedAlmond\', \'BlueViolet\', \'BurlyWood\', \'CadetBlue\', \'AntiqueWhite\',\n    \'Chocolate\', \'Coral\', \'CornflowerBlue\', \'Cornsilk\', \'Crimson\', \'Cyan\',\n    \'DarkCyan\', \'DarkGoldenRod\', \'DarkGrey\', \'DarkKhaki\', \'DarkOrange\',\n    \'DarkOrchid\', \'DarkSalmon\', \'DarkSeaGreen\', \'DarkTurquoise\', \'DarkViolet\',\n    \'DeepPink\', \'DeepSkyBlue\', \'DodgerBlue\', \'FireBrick\', \'FloralWhite\',\n    \'ForestGreen\', \'Fuchsia\', \'Gainsboro\', \'GhostWhite\', \'Gold\', \'GoldenRod\',\n    \'Salmon\', \'Tan\', \'HoneyDew\', \'HotPink\', \'IndianRed\', \'Ivory\', \'Khaki\',\n    \'Lavender\', \'LavenderBlush\', \'LawnGreen\', \'LemonChiffon\', \'LightBlue\',\n    \'LightCoral\', \'LightCyan\', \'LightGoldenRodYellow\', \'LightGray\', \'LightGrey\',\n    \'LightGreen\', \'LightPink\', \'LightSalmon\', \'LightSeaGreen\', \'LightSkyBlue\',\n    \'LightSlateGray\', \'LightSlateGrey\', \'LightSteelBlue\', \'LightYellow\', \'Lime\',\n    \'LimeGreen\', \'Linen\', \'Magenta\', \'MediumAquaMarine\', \'MediumOrchid\',\n    \'MediumPurple\', \'MediumSeaGreen\', \'MediumSlateBlue\', \'MediumSpringGreen\',\n    \'MediumTurquoise\', \'MediumVioletRed\', \'MintCream\', \'MistyRose\', \'Moccasin\',\n    \'NavajoWhite\', \'OldLace\', \'Olive\', \'OliveDrab\', \'Orange\', \'OrangeRed\',\n    \'Orchid\', \'PaleGoldenRod\', \'PaleGreen\', \'PaleTurquoise\', \'PaleVioletRed\',\n    \'PapayaWhip\', \'PeachPuff\', \'Peru\', \'Pink\', \'Plum\', \'PowderBlue\', \'Purple\',\n    \'Red\', \'RosyBrown\', \'RoyalBlue\', \'SaddleBrown\', \'Green\', \'SandyBrown\',\n    \'SeaGreen\', \'SeaShell\', \'Sienna\', \'Silver\', \'SkyBlue\', \'SlateBlue\',\n    \'SlateGray\', \'SlateGrey\', \'Snow\', \'SpringGreen\', \'SteelBlue\', \'GreenYellow\',\n    \'Teal\', \'Thistle\', \'Tomato\', \'Turquoise\', \'Violet\', \'Wheat\', \'White\',\n    \'WhiteSmoke\', \'Yellow\', \'YellowGreen\'\n]\n\n\ndef save_image_array_as_png(image, output_path):\n  """"""Saves an image (represented as a numpy array) to PNG.\n\n  Args:\n    image: a numpy array with shape [height, width, 3].\n    output_path: path to which image should be written.\n  """"""\n  image_pil = Image.fromarray(np.uint8(image)).convert(\'RGB\')\n  with tf.gfile.Open(output_path, \'w\') as fid:\n    image_pil.save(fid, \'PNG\')\n\n\ndef encode_image_array_as_png_str(image):\n  """"""Encodes a numpy array into a PNG string.\n\n  Args:\n    image: a numpy array with shape [height, width, 3].\n\n  Returns:\n    PNG encoded image string.\n  """"""\n  image_pil = Image.fromarray(np.uint8(image))\n  output = six.BytesIO()\n  image_pil.save(output, format=\'PNG\')\n  png_string = output.getvalue()\n  output.close()\n  return png_string\n\n\ndef draw_bounding_box_on_image_array(image,\n                                     ymin,\n                                     xmin,\n                                     ymax,\n                                     xmax,\n                                     color=\'red\',\n                                     thickness=4,\n                                     display_str_list=(),\n                                     use_normalized_coordinates=True):\n  """"""Adds a bounding box to an image (numpy array).\n\n  Bounding box coordinates can be specified in either absolute (pixel) or\n  normalized coordinates by setting the use_normalized_coordinates argument.\n\n  Args:\n    image: a numpy array with shape [height, width, 3].\n    ymin: ymin of bounding box.\n    xmin: xmin of bounding box.\n    ymax: ymax of bounding box.\n    xmax: xmax of bounding box.\n    color: color to draw bounding box. Default is red.\n    thickness: line thickness. Default value is 4.\n    display_str_list: list of strings to display in box\n                      (each to be shown on its own line).\n    use_normalized_coordinates: If True (default), treat coordinates\n      ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n      coordinates as absolute.\n  """"""\n  image_pil = Image.fromarray(np.uint8(image)).convert(\'RGB\')\n  draw_bounding_box_on_image(image_pil, ymin, xmin, ymax, xmax, color,\n                             thickness, display_str_list,\n                             use_normalized_coordinates)\n  np.copyto(image, np.array(image_pil))\n\n\ndef draw_bounding_box_on_image(image,\n                               ymin,\n                               xmin,\n                               ymax,\n                               xmax,\n                               color=\'red\',\n                               thickness=4,\n                               display_str_list=(),\n                               use_normalized_coordinates=True):\n  """"""Adds a bounding box to an image.\n\n  Bounding box coordinates can be specified in either absolute (pixel) or\n  normalized coordinates by setting the use_normalized_coordinates argument.\n\n  Each string in display_str_list is displayed on a separate line above the\n  bounding box in black text on a rectangle filled with the input \'color\'.\n  If the top of the bounding box extends to the edge of the image, the strings\n  are displayed below the bounding box.\n\n  Args:\n    image: a PIL.Image object.\n    ymin: ymin of bounding box.\n    xmin: xmin of bounding box.\n    ymax: ymax of bounding box.\n    xmax: xmax of bounding box.\n    color: color to draw bounding box. Default is red.\n    thickness: line thickness. Default value is 4.\n    display_str_list: list of strings to display in box\n                      (each to be shown on its own line).\n    use_normalized_coordinates: If True (default), treat coordinates\n      ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n      coordinates as absolute.\n  """"""\n  draw = ImageDraw.Draw(image)\n  im_width, im_height = image.size\n  if use_normalized_coordinates:\n    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n                                  ymin * im_height, ymax * im_height)\n  else:\n    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n  draw.line([(left, top), (left, bottom), (right, bottom),\n             (right, top), (left, top)], width=thickness, fill=color)\n  try:\n    font = ImageFont.truetype(\'arial.ttf\', 24)\n  except IOError:\n    font = ImageFont.load_default()\n\n  # If the total height of the display strings added to the top of the bounding\n  # box exceeds the top of the image, stack the strings below the bounding box\n  # instead of above.\n  display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n  # Each display_str has a top and bottom margin of 0.05x.\n  total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n\n  if top > total_display_str_height:\n    text_bottom = top\n  else:\n    text_bottom = bottom + total_display_str_height\n  # Reverse list and print from bottom to top.\n  for display_str in display_str_list[::-1]:\n    text_width, text_height = font.getsize(display_str)\n    margin = np.ceil(0.05 * text_height)\n    draw.rectangle(\n        [(left, text_bottom - text_height - 2 * margin), (left + text_width,\n                                                          text_bottom)],\n        fill=color)\n    draw.text(\n        (left + margin, text_bottom - text_height - margin),\n        display_str,\n        fill=\'black\',\n        font=font)\n    text_bottom -= text_height - 2 * margin\n\n\ndef draw_bounding_boxes_on_image_array(image,\n                                       boxes,\n                                       color=\'red\',\n                                       thickness=4,\n                                       display_str_list_list=()):\n  """"""Draws bounding boxes on image (numpy array).\n\n  Args:\n    image: a numpy array object.\n    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n           The coordinates are in normalized format between [0, 1].\n    color: color to draw bounding box. Default is red.\n    thickness: line thickness. Default value is 4.\n    display_str_list_list: list of list of strings.\n                           a list of strings for each bounding box.\n                           The reason to pass a list of strings for a\n                           bounding box is that it might contain\n                           multiple labels.\n\n  Raises:\n    ValueError: if boxes is not a [N, 4] array\n  """"""\n  image_pil = Image.fromarray(image)\n  draw_bounding_boxes_on_image(image_pil, boxes, color, thickness,\n                               display_str_list_list)\n  np.copyto(image, np.array(image_pil))\n\n\ndef draw_bounding_boxes_on_image(image,\n                                 boxes,\n                                 color=\'red\',\n                                 thickness=4,\n                                 display_str_list_list=()):\n  """"""Draws bounding boxes on image.\n\n  Args:\n    image: a PIL.Image object.\n    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n           The coordinates are in normalized format between [0, 1].\n    color: color to draw bounding box. Default is red.\n    thickness: line thickness. Default value is 4.\n    display_str_list_list: list of list of strings.\n                           a list of strings for each bounding box.\n                           The reason to pass a list of strings for a\n                           bounding box is that it might contain\n                           multiple labels.\n\n  Raises:\n    ValueError: if boxes is not a [N, 4] array\n  """"""\n  boxes_shape = boxes.shape\n  if not boxes_shape:\n    return\n  if len(boxes_shape) != 2 or boxes_shape[1] != 4:\n    raise ValueError(\'Input must be of size [N, 4]\')\n  for i in range(boxes_shape[0]):\n    display_str_list = ()\n    if display_str_list_list:\n      display_str_list = display_str_list_list[i]\n    draw_bounding_box_on_image(image, boxes[i, 0], boxes[i, 1], boxes[i, 2],\n                               boxes[i, 3], color, thickness, display_str_list)\n\n\ndef _visualize_boxes(image, boxes, classes, scores, category_index, **kwargs):\n  return visualize_boxes_and_labels_on_image_array(\n      image, boxes, classes, scores, category_index=category_index, **kwargs)\n\n\ndef _visualize_boxes_and_masks(image, boxes, classes, scores, masks,\n                               category_index, **kwargs):\n  return visualize_boxes_and_labels_on_image_array(\n      image,\n      boxes,\n      classes,\n      scores,\n      category_index=category_index,\n      instance_masks=masks,\n      **kwargs)\n\n\ndef _visualize_boxes_and_keypoints(image, boxes, classes, scores, keypoints,\n                                   category_index, **kwargs):\n  return visualize_boxes_and_labels_on_image_array(\n      image,\n      boxes,\n      classes,\n      scores,\n      category_index=category_index,\n      keypoints=keypoints,\n      **kwargs)\n\n\ndef _visualize_boxes_and_masks_and_keypoints(\n    image, boxes, classes, scores, masks, keypoints, category_index, **kwargs):\n  return visualize_boxes_and_labels_on_image_array(\n      image,\n      boxes,\n      classes,\n      scores,\n      category_index=category_index,\n      instance_masks=masks,\n      keypoints=keypoints,\n      **kwargs)\n\n\ndef draw_bounding_boxes_on_image_tensors(images,\n                                         boxes,\n                                         classes,\n                                         scores,\n                                         category_index,\n                                         instance_masks=None,\n                                         keypoints=None,\n                                         max_boxes_to_draw=20,\n                                         min_score_thresh=0.2,\n                                         use_normalized_coordinates=True):\n  """"""Draws bounding boxes, masks, and keypoints on batch of image tensors.\n\n  Args:\n    images: A 4D uint8 image tensor of shape [N, H, W, C]. If C > 3, additional\n      channels will be ignored.\n    boxes: [N, max_detections, 4] float32 tensor of detection boxes.\n    classes: [N, max_detections] int tensor of detection classes. Note that\n      classes are 1-indexed.\n    scores: [N, max_detections] float32 tensor of detection scores.\n    category_index: a dict that maps integer ids to category dicts. e.g.\n      {1: {1: \'dog\'}, 2: {2: \'cat\'}, ...}\n    instance_masks: A 4D uint8 tensor of shape [N, max_detection, H, W] with\n      instance masks.\n    keypoints: A 4D float32 tensor of shape [N, max_detection, num_keypoints, 2]\n      with keypoints.\n    max_boxes_to_draw: Maximum number of boxes to draw on an image. Default 20.\n    min_score_thresh: Minimum score threshold for visualization. Default 0.2.\n    use_normalized_coordinates: Whether to assume boxes and kepoints are in\n      normalized coordinates (as opposed to absolute coordiantes).\n      Default is True.\n\n  Returns:\n    4D image tensor of type uint8, with boxes drawn on top.\n  """"""\n  # Additional channels are being ignored.\n  images = images[:, :, :, 0:3]\n  visualization_keyword_args = {\n      \'use_normalized_coordinates\': use_normalized_coordinates,\n      \'max_boxes_to_draw\': max_boxes_to_draw,\n      \'min_score_thresh\': min_score_thresh,\n      \'agnostic_mode\': False,\n      \'line_thickness\': 4\n  }\n\n  if instance_masks is not None and keypoints is None:\n    visualize_boxes_fn = functools.partial(\n        _visualize_boxes_and_masks,\n        category_index=category_index,\n        **visualization_keyword_args)\n    elems = [images, boxes, classes, scores, instance_masks]\n  elif instance_masks is None and keypoints is not None:\n    visualize_boxes_fn = functools.partial(\n        _visualize_boxes_and_keypoints,\n        category_index=category_index,\n        **visualization_keyword_args)\n    elems = [images, boxes, classes, scores, keypoints]\n  elif instance_masks is not None and keypoints is not None:\n    visualize_boxes_fn = functools.partial(\n        _visualize_boxes_and_masks_and_keypoints,\n        category_index=category_index,\n        **visualization_keyword_args)\n    elems = [images, boxes, classes, scores, instance_masks, keypoints]\n  else:\n    visualize_boxes_fn = functools.partial(\n        _visualize_boxes,\n        category_index=category_index,\n        **visualization_keyword_args)\n    elems = [images, boxes, classes, scores]\n\n  def draw_boxes(image_and_detections):\n    """"""Draws boxes on image.""""""\n    image_with_boxes = tf.py_func(visualize_boxes_fn, image_and_detections,\n                                  tf.uint8)\n    return image_with_boxes\n\n  images = tf.map_fn(draw_boxes, elems, dtype=tf.uint8, back_prop=False)\n  return images\n\n\ndef draw_side_by_side_evaluation_image(eval_dict,\n                                       category_index,\n                                       max_boxes_to_draw=20,\n                                       min_score_thresh=0.2,\n                                       use_normalized_coordinates=True):\n  """"""Creates a side-by-side image with detections and groundtruth.\n\n  Bounding boxes (and instance masks, if available) are visualized on both\n  subimages.\n\n  Args:\n    eval_dict: The evaluation dictionary returned by\n      eval_util.result_dict_for_single_example().\n    category_index: A category index (dictionary) produced from a labelmap.\n    max_boxes_to_draw: The maximum number of boxes to draw for detections.\n    min_score_thresh: The minimum score threshold for showing detections.\n    use_normalized_coordinates: Whether to assume boxes and kepoints are in\n      normalized coordinates (as opposed to absolute coordiantes).\n      Default is True.\n\n  Returns:\n    A [1, H, 2 * W, C] uint8 tensor. The subimage on the left corresponds to\n      detections, while the subimage on the right corresponds to groundtruth.\n  """"""\n  detection_fields = fields.DetectionResultFields()\n  input_data_fields = fields.InputDataFields()\n  instance_masks = None\n  if detection_fields.detection_masks in eval_dict:\n    instance_masks = tf.cast(\n        tf.expand_dims(eval_dict[detection_fields.detection_masks], axis=0),\n        tf.uint8)\n  keypoints = None\n  if detection_fields.detection_keypoints in eval_dict:\n    keypoints = tf.expand_dims(\n        eval_dict[detection_fields.detection_keypoints], axis=0)\n  groundtruth_instance_masks = None\n  if input_data_fields.groundtruth_instance_masks in eval_dict:\n    groundtruth_instance_masks = tf.cast(\n        tf.expand_dims(\n            eval_dict[input_data_fields.groundtruth_instance_masks], axis=0),\n        tf.uint8)\n  images_with_detections = draw_bounding_boxes_on_image_tensors(\n      eval_dict[input_data_fields.original_image],\n      tf.expand_dims(eval_dict[detection_fields.detection_boxes], axis=0),\n      tf.expand_dims(eval_dict[detection_fields.detection_classes], axis=0),\n      tf.expand_dims(eval_dict[detection_fields.detection_scores], axis=0),\n      category_index,\n      instance_masks=instance_masks,\n      keypoints=keypoints,\n      max_boxes_to_draw=max_boxes_to_draw,\n      min_score_thresh=min_score_thresh,\n      use_normalized_coordinates=use_normalized_coordinates)\n  images_with_groundtruth = draw_bounding_boxes_on_image_tensors(\n      eval_dict[input_data_fields.original_image],\n      tf.expand_dims(eval_dict[input_data_fields.groundtruth_boxes], axis=0),\n      tf.expand_dims(eval_dict[input_data_fields.groundtruth_classes], axis=0),\n      tf.expand_dims(\n          tf.ones_like(\n              eval_dict[input_data_fields.groundtruth_classes],\n              dtype=tf.float32),\n          axis=0),\n      category_index,\n      instance_masks=groundtruth_instance_masks,\n      keypoints=None,\n      max_boxes_to_draw=None,\n      min_score_thresh=0.0,\n      use_normalized_coordinates=use_normalized_coordinates)\n  return tf.concat([images_with_detections, images_with_groundtruth], axis=2)\n\n\ndef draw_keypoints_on_image_array(image,\n                                  keypoints,\n                                  color=\'red\',\n                                  radius=2,\n                                  use_normalized_coordinates=True):\n  """"""Draws keypoints on an image (numpy array).\n\n  Args:\n    image: a numpy array with shape [height, width, 3].\n    keypoints: a numpy array with shape [num_keypoints, 2].\n    color: color to draw the keypoints with. Default is red.\n    radius: keypoint radius. Default value is 2.\n    use_normalized_coordinates: if True (default), treat keypoint values as\n      relative to the image.  Otherwise treat them as absolute.\n  """"""\n  image_pil = Image.fromarray(np.uint8(image)).convert(\'RGB\')\n  draw_keypoints_on_image(image_pil, keypoints, color, radius,\n                          use_normalized_coordinates)\n  np.copyto(image, np.array(image_pil))\n\n\ndef draw_keypoints_on_image(image,\n                            keypoints,\n                            color=\'red\',\n                            radius=2,\n                            use_normalized_coordinates=True):\n  """"""Draws keypoints on an image.\n\n  Args:\n    image: a PIL.Image object.\n    keypoints: a numpy array with shape [num_keypoints, 2].\n    color: color to draw the keypoints with. Default is red.\n    radius: keypoint radius. Default value is 2.\n    use_normalized_coordinates: if True (default), treat keypoint values as\n      relative to the image.  Otherwise treat them as absolute.\n  """"""\n  draw = ImageDraw.Draw(image)\n  im_width, im_height = image.size\n  keypoints_x = [k[1] for k in keypoints]\n  keypoints_y = [k[0] for k in keypoints]\n  if use_normalized_coordinates:\n    keypoints_x = tuple([im_width * x for x in keypoints_x])\n    keypoints_y = tuple([im_height * y for y in keypoints_y])\n  for keypoint_x, keypoint_y in zip(keypoints_x, keypoints_y):\n    draw.ellipse([(keypoint_x - radius, keypoint_y - radius),\n                  (keypoint_x + radius, keypoint_y + radius)],\n                 outline=color, fill=color)\n\n\ndef draw_mask_on_image_array(image, mask, color=\'red\', alpha=0.4):\n  """"""Draws mask on an image.\n\n  Args:\n    image: uint8 numpy array with shape (img_height, img_height, 3)\n    mask: a uint8 numpy array of shape (img_height, img_height) with\n      values between either 0 or 1.\n    color: color to draw the keypoints with. Default is red.\n    alpha: transparency value between 0 and 1. (default: 0.4)\n\n  Raises:\n    ValueError: On incorrect data type for image or masks.\n  """"""\n  if image.dtype != np.uint8:\n    raise ValueError(\'`image` not of type np.uint8\')\n  if mask.dtype != np.uint8:\n    raise ValueError(\'`mask` not of type np.uint8\')\n  if np.any(np.logical_and(mask != 1, mask != 0)):\n    raise ValueError(\'`mask` elements should be in [0, 1]\')\n  if image.shape[:2] != mask.shape:\n    raise ValueError(\'The image has spatial dimensions %s but the mask has \'\n                     \'dimensions %s\' % (image.shape[:2], mask.shape))\n  rgb = ImageColor.getrgb(color)\n  pil_image = Image.fromarray(image)\n\n  solid_color = np.expand_dims(\n      np.ones_like(mask), axis=2) * np.reshape(list(rgb), [1, 1, 3])\n  pil_solid_color = Image.fromarray(np.uint8(solid_color)).convert(\'RGBA\')\n  pil_mask = Image.fromarray(np.uint8(255.0*alpha*mask)).convert(\'L\')\n  pil_image = Image.composite(pil_solid_color, pil_image, pil_mask)\n  np.copyto(image, np.array(pil_image.convert(\'RGB\')))\n\n\ndef visualize_boxes_and_labels_on_image_array(\n    image,\n    boxes,\n    classes,\n    scores,\n    category_index,\n    instance_masks=None,\n    instance_boundaries=None,\n    keypoints=None,\n    use_normalized_coordinates=False,\n    max_boxes_to_draw=20,\n    min_score_thresh=.5,\n    agnostic_mode=False,\n    line_thickness=4,\n    groundtruth_box_visualization_color=\'black\',\n    skip_scores=False,\n    skip_labels=False):\n  """"""Overlay labeled boxes on an image with formatted scores and label names.\n\n  This function groups boxes that correspond to the same location\n  and creates a display string for each detection and overlays these\n  on the image. Note that this function modifies the image in place, and returns\n  that same image.\n\n  Args:\n    image: uint8 numpy array with shape (img_height, img_width, 3)\n    boxes: a numpy array of shape [N, 4]\n    classes: a numpy array of shape [N]. Note that class indices are 1-based,\n      and match the keys in the label map.\n    scores: a numpy array of shape [N] or None.  If scores=None, then\n      this function assumes that the boxes to be plotted are groundtruth\n      boxes and plot all boxes as black with no classes or scores.\n    category_index: a dict containing category dictionaries (each holding\n      category index `id` and category name `name`) keyed by category indices.\n    instance_masks: a numpy array of shape [N, image_height, image_width] with\n      values ranging between 0 and 1, can be None.\n    instance_boundaries: a numpy array of shape [N, image_height, image_width]\n      with values ranging between 0 and 1, can be None.\n    keypoints: a numpy array of shape [N, num_keypoints, 2], can\n      be None\n    use_normalized_coordinates: whether boxes is to be interpreted as\n      normalized coordinates or not.\n    max_boxes_to_draw: maximum number of boxes to visualize.  If None, draw\n      all boxes.\n    min_score_thresh: minimum score threshold for a box to be visualized\n    agnostic_mode: boolean (default: False) controlling whether to evaluate in\n      class-agnostic mode or not.  This mode will display scores but ignore\n      classes.\n    line_thickness: integer (default: 4) controlling line width of the boxes.\n    groundtruth_box_visualization_color: box color for visualizing groundtruth\n      boxes\n    skip_scores: whether to skip score when drawing a single detection\n    skip_labels: whether to skip label when drawing a single detection\n\n  Returns:\n    uint8 numpy array with shape (img_height, img_width, 3) with overlaid boxes.\n  """"""\n  # Create a display string (and color) for every box location, group any boxes\n  # that correspond to the same location.\n  box_to_display_str_map = collections.defaultdict(list)\n  box_to_color_map = collections.defaultdict(str)\n  box_to_instance_masks_map = {}\n  box_to_instance_boundaries_map = {}\n  box_to_keypoints_map = collections.defaultdict(list)\n  if not max_boxes_to_draw:\n    max_boxes_to_draw = boxes.shape[0]\n  for i in range(min(max_boxes_to_draw, boxes.shape[0])):\n    if scores is None or scores[i] > min_score_thresh:\n      box = tuple(boxes[i].tolist())\n      if instance_masks is not None:\n        box_to_instance_masks_map[box] = instance_masks[i]\n      if instance_boundaries is not None:\n        box_to_instance_boundaries_map[box] = instance_boundaries[i]\n      if keypoints is not None:\n        box_to_keypoints_map[box].extend(keypoints[i])\n      if scores is None:\n        box_to_color_map[box] = groundtruth_box_visualization_color\n      else:\n        display_str = \'\'\n        if not skip_labels:\n          if not agnostic_mode:\n            if classes[i] in category_index.keys():\n              class_name = category_index[classes[i]][\'name\']\n            else:\n              class_name = \'N/A\'\n            display_str = str(class_name)\n        if not skip_scores:\n          if not display_str:\n            display_str = \'{}%\'.format(int(100*scores[i]))\n          else:\n            display_str = \'{}: {}%\'.format(display_str, int(100*scores[i]))\n        box_to_display_str_map[box].append(display_str)\n        if agnostic_mode:\n          box_to_color_map[box] = \'DarkOrange\'\n        else:\n          box_to_color_map[box] = STANDARD_COLORS[\n              classes[i] % len(STANDARD_COLORS)]\n\n  # Draw all boxes onto image.\n  for box, color in box_to_color_map.items():\n    ymin, xmin, ymax, xmax = box\n    if instance_masks is not None:\n      draw_mask_on_image_array(\n          image,\n          box_to_instance_masks_map[box],\n          color=color\n      )\n    if instance_boundaries is not None:\n      draw_mask_on_image_array(\n          image,\n          box_to_instance_boundaries_map[box],\n          color=\'red\',\n          alpha=1.0\n      )\n    draw_bounding_box_on_image_array(\n        image,\n        ymin,\n        xmin,\n        ymax,\n        xmax,\n        color=color,\n        thickness=line_thickness,\n        display_str_list=box_to_display_str_map[box],\n        use_normalized_coordinates=use_normalized_coordinates)\n    if keypoints is not None:\n      draw_keypoints_on_image_array(\n          image,\n          box_to_keypoints_map[box],\n          color=color,\n          radius=line_thickness / 2,\n          use_normalized_coordinates=use_normalized_coordinates)\n\n  return image\n\n\ndef add_cdf_image_summary(values, name):\n  """"""Adds a tf.summary.image for a CDF plot of the values.\n\n  Normalizes `values` such that they sum to 1, plots the cumulative distribution\n  function and creates a tf image summary.\n\n  Args:\n    values: a 1-D float32 tensor containing the values.\n    name: name for the image summary.\n  """"""\n  def cdf_plot(values):\n    """"""Numpy function to plot CDF.""""""\n    normalized_values = values / np.sum(values)\n    sorted_values = np.sort(normalized_values)\n    cumulative_values = np.cumsum(sorted_values)\n    fraction_of_examples = (np.arange(cumulative_values.size, dtype=np.float32)\n                            / cumulative_values.size)\n    fig = plt.figure(frameon=False)\n    ax = fig.add_subplot(\'111\')\n    ax.plot(fraction_of_examples, cumulative_values)\n    ax.set_ylabel(\'cumulative normalized values\')\n    ax.set_xlabel(\'fraction of examples\')\n    fig.canvas.draw()\n    width, height = fig.get_size_inches() * fig.get_dpi()\n    image = np.fromstring(fig.canvas.tostring_rgb(), dtype=\'uint8\').reshape(\n        1, int(height), int(width), 3)\n    return image\n  cdf_plot = tf.py_func(cdf_plot, [values], tf.uint8)\n  tf.summary.image(name, cdf_plot)\n\n\ndef add_hist_image_summary(values, bins, name):\n  """"""Adds a tf.summary.image for a histogram plot of the values.\n\n  Plots the histogram of values and creates a tf image summary.\n\n  Args:\n    values: a 1-D float32 tensor containing the values.\n    bins: bin edges which will be directly passed to np.histogram.\n    name: name for the image summary.\n  """"""\n\n  def hist_plot(values, bins):\n    """"""Numpy function to plot hist.""""""\n    fig = plt.figure(frameon=False)\n    ax = fig.add_subplot(\'111\')\n    y, x = np.histogram(values, bins=bins)\n    ax.plot(x[:-1], y)\n    ax.set_ylabel(\'count\')\n    ax.set_xlabel(\'value\')\n    fig.canvas.draw()\n    width, height = fig.get_size_inches() * fig.get_dpi()\n    image = np.fromstring(\n        fig.canvas.tostring_rgb(), dtype=\'uint8\').reshape(\n            1, int(height), int(width), 3)\n    return image\n  hist_plot = tf.py_func(hist_plot, [values, bins], tf.uint8)\n  tf.summary.image(name, hist_plot)\n'"
