file_path,api_count,code
setup.py,1,"b'# Adapted from https://github.com/pybind/cmake_example/blob/master/setup.py\nimport os\nimport re\nimport sys\nimport platform\nimport subprocess\nimport importlib\nfrom sysconfig import get_paths\n\nfrom setuptools import setup, Extension\nfrom setuptools.command.build_ext import build_ext\nfrom setuptools.command.install import install\nfrom distutils.sysconfig import get_config_var\nfrom distutils.version import LooseVersion\n\nclass RemoveOldRednerBeforeInstall(install):\n    def run(self):\n        # Remove old redner packages installed by distutils\n        from distutils import sysconfig as sc\n        site_packages_dir = sc.get_python_lib()\n        import shutil\n        import glob\n        egg_info_path = glob.glob(os.path.join(site_packages_dir, \'redner-0.0.1-*.egg-info\'))\n        for p in egg_info_path:\n            try:\n                os.remove(p)\n            except:\n                print(\'Warning: detect old redner installation file {} and could not remove it. You may want to remove the file manually.\'.format(p))\n\n        install.run(self)\n\nclass CMakeExtension(Extension):\n    def __init__(self, name, sourcedir, build_with_cuda):\n        Extension.__init__(self, name, sources=[])\n        self.sourcedir = os.path.abspath(sourcedir)\n        self.build_with_cuda = build_with_cuda\n\nclass CopyExtension(Extension):\n    def __init__(self, name, filename_list):\n        Extension.__init__(self, name, sources=[])\n        self.filename_list = filename_list\n\nclass Build(build_ext):\n    def run(self):\n        try:\n            out = subprocess.check_output([\'cmake\', \'--version\'])\n        except OSError:\n            raise RuntimeError(""CMake must be installed to build the following extensions: "" +\n                               "", "".join(e.name for e in self.extensions))\n\n        super().run()\n\n    def build_extension(self, ext):\n        if isinstance(ext, CMakeExtension):\n            extdir = os.path.abspath(os.path.dirname(self.get_ext_fullpath(ext.name)))\n            info = get_paths()\n            include_path = info[\'include\']\n            cmake_args = [\'-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=\' + extdir,\n                          \'-DPYTHON_INCLUDE_PATH=\' + include_path]\n\n            cfg = \'Debug\' if self.debug else \'Release\'\n            build_args = [\'--config\', cfg]\n\n            if platform.system() == ""Windows"":\n                cmake_args += [\'-DCMAKE_LIBRARY_OUTPUT_DIRECTORY_{}={}\'.format(cfg.upper(), extdir),\n                               \'-DCMAKE_RUNTIME_OUTPUT_DIRECTORY_{}={}\'.format(cfg.upper(), extdir)]\n                if sys.maxsize > 2**32:\n                    cmake_args += [\'-A\', \'x64\']\n                build_args += [\'--\', \'/m\']\n            else:\n                cmake_args += [\'-DCMAKE_BUILD_TYPE=\' + cfg]\n                build_args += [\'--\', \'-j8\']\n\n            if ext.build_with_cuda:\n                cmake_args += [\'-DREDNER_CUDA=1\']\n\n            env = os.environ.copy()\n            env[\'CXXFLAGS\'] = \'{} -DVERSION_INFO=\\\\""{}\\\\""\'.format(env.get(\'CXXFLAGS\', \'\'),\n                                                                  self.distribution.get_version())\n            if not os.path.exists(self.build_temp):\n                os.makedirs(self.build_temp)\n            subprocess.check_call([\'cmake\', ext.sourcedir] + cmake_args, cwd=self.build_temp, env=env)\n            subprocess.check_call([\'cmake\', \'--build\', \'.\'] + build_args, cwd=self.build_temp)\n        elif isinstance(ext, CopyExtension):\n            extdir = os.path.abspath(os.path.dirname(self.get_ext_fullpath(ext.name)))\n            # Copy the files to extdir\n            from shutil import copy\n            for f in ext.filename_list:\n                print(\'Copying {} to {}\'.format(f, extdir))\n                copy(f, extdir)\n        else:\n            super().build_extension(ext)\n\ntorch_spec = importlib.util.find_spec(""torch"")\ntf_spec = importlib.util.find_spec(""tensorflow"")\npackages = []\nbuild_with_cuda = False\nif torch_spec is not None:\n    packages.append(\'pyredner\')\n    import torch\n    if torch.cuda.is_available():\n        build_with_cuda = True\nif tf_spec is not None and sys.platform != \'win32\':\n    packages.append(\'pyredner_tensorflow\')\n    if not build_with_cuda:\n        import tensorflow as tf\n        if tf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None):\n            build_with_cuda = True\nif len(packages) == 0:\n    print(\'Error: PyTorch or Tensorflow must be installed. For Windows platform only PyTorch is supported.\')\n    exit()\n# Override build_with_cuda with environment variable\nif \'REDNER_CUDA\' in os.environ:\n    build_with_cuda = os.environ[\'REDNER_CUDA\'] == \'1\'\n\ndynamic_libraries = []\n# Make Embree and OptiX part of the package\nif sys.platform == \'darwin\':\n    dynamic_libraries.append(\'redner-dependencies/embree/lib-macos/libembree3.dylib\')\n    dynamic_libraries.append(\'redner-dependencies/embree/lib-macos/libtbb.dylib\')\n    dynamic_libraries.append(\'redner-dependencies/embree/lib-macos/libtbbmalloc.dylib\')\nelif sys.platform == \'linux\':\n    dynamic_libraries.append(\'redner-dependencies/embree/lib-linux/libembree3.so.3\')\n    dynamic_libraries.append(\'redner-dependencies/embree/lib-linux/libtbb.so.2\')\n    dynamic_libraries.append(\'redner-dependencies/embree/lib-linux/libtbbmalloc.so.2\')\n    if build_with_cuda:\n        dynamic_libraries.append(\'redner-dependencies/optix/lib64/liboptix_prime.so.1\')\nelif sys.platform == \'win32\':\n    dynamic_libraries.append(\'redner-dependencies/embree/bin/embree3.dll\')\n    dynamic_libraries.append(\'redner-dependencies/embree/bin/tbb.dll\')\n    dynamic_libraries.append(\'redner-dependencies/embree/bin/tbbmalloc.dll\')\n    if build_with_cuda:\n        dynamic_libraries.append(\'redner-dependencies/optix/bin64/optix_prime.1.dll\')\n\nproject_name = \'redner\'\nif \'PROJECT_NAME\' in os.environ:\n    project_name = os.environ[\'PROJECT_NAME\']\nsetup(name = project_name,\n      version = \'0.4.25\',\n      description = \'Differentiable rendering without approximation.\',\n      long_description = """"""redner is a differentiable renderer that can take the\n                            derivatives of rendering output with respect to arbitrary\n                            scene parameters, that is, you can backpropagate from the\n                            image to your 3D scene. One of the major usages of redner\n                            is inverse rendering (hence the name redner) through gradient\n                            descent. What sets redner apart are: 1) it computes correct\n                            rendering gradients stochastically without any approximation\n                            and 2) it has a physically-based mode -- which means it can\n                            simulate photons and produce realistic lighting phenomena,\n                            such as shadow and global illumination, and it handles the\n                            derivatives of these features correctly. You can also use\n                            redner in a fast deferred rendering mode for local shading:\n                            in this mode it still has correct gradient estimation and\n                            more elaborate material models compared to most differentiable\n                            renderers out there.\n                         """""",\n      url = \'https://github.com/BachiLi/redner\',\n      classifiers = [\n        \'Development Status :: 4 - Beta\',\n        \'License :: OSI Approved :: MIT License\',\n        \'Programming Language :: C++\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Operating System :: MacOS\',\n        \'Operating System :: POSIX :: Linux\',\n        \'Topic :: Multimedia :: Graphics :: 3D Rendering\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'Topic :: Scientific/Engineering :: Image Recognition\'\n      ],\n      author = \'Tzu-Mao Li\',\n      author_email = \'tzumao@mit.edu\',\n      license = \'MIT\',\n      packages = packages,\n      ext_modules = [CMakeExtension(\'redner\', \'\', build_with_cuda),\n                     CopyExtension(\'redner-dependencies\', dynamic_libraries)],\n      cmdclass = dict(build_ext=Build, install=RemoveOldRednerBeforeInstall),\n      install_requires = [\'scikit-image\', \'imageio\'],\n      keywords = [\'rendering\',\n                  \'Monte Carlo ray tracing\',\n                  \'computer vision\',\n                  \'computer graphics\',\n                  \'differentiable rendering\',\n                  \'PyTorch\',\n                  \'TensorFlow\'],\n      zip_safe = False)\n'"
examples/joint_material_envmap_sh.py,0,"b'import torch\nimport math\nimport numpy as np\nimport pyredner\n\n# This is an example for setting up a spherical harmonics parameterized environment map\n# and jointly optimize for the spherical harmonics coefficients and object materials\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\n# Setup camera\ncam = pyredner.Camera(position = torch.tensor([0.0, 0.0, -5.0]),\n                      look_at = torch.tensor([0.0, 0.0, 0.0]),\n                      up = torch.tensor([0.0, 1.0, 0.0]),\n                      fov = torch.tensor([45.0]), # in degree\n                      clip_near = 1e-2, # needs to > 0\n                      resolution = (256, 256),\n                      fisheye = False)\n\n# Setup material\nmat_grey = pyredner.Material(\\\n    diffuse_reflectance = \\\n        torch.tensor([0.4, 0.4, 0.4], device = pyredner.get_device()),\n    specular_reflectance = \\\n        torch.tensor([0.5, 0.5, 0.5], device = pyredner.get_device()),\n    roughness = \\\n        torch.tensor([0.02], device = pyredner.get_device()))\nmaterials = [mat_grey]\n\n# Setup scene geometry: we use the utility function ""generate_sphere"" to generate a sphere\n# triangle mesh\nvertices, indices, uvs, normals = pyredner.generate_sphere(128, 64)\nshape_sphere = pyredner.Shape(\\\n    vertices = vertices,\n    indices = indices,\n    uvs = uvs,\n    normals = normals,\n    material_id = 0)\nshapes = [shape_sphere]\n\n# Setup lighting: the scene is lit by a single environment map, parameterized by 3rd-order\n# spherical harmonics coefficients.\n# First we setup the target coefficients for r, g, b,\n# taken from https://cseweb.ucsd.edu/~ravir/papers/envmap/envmap.pdf\n# Last 7 coefficients are randomly determined\ncoeffs = torch.tensor([[ 0.79,\n                         0.39, -0.35, -0.34,\n                        -0.11, -0.26, -0.16,  0.56,  0.21,\n                         0.10,  0.05, -0.20, -0.03, -0.10, -0.30, -0.01], # coeffs for red\n                       [ 0.44,\n                         0.35, -0.18, -0.06,\n                        -0.05, -0.22, -0.09,  0.21, -0.05,\n                         0.03,  0.07, -0.01, -0.09, -0.06,  0.03, 0.05], # coeffs for green\n                       [ 0.54,\n                         0.60, -0.27,  0.01,\n                        -0.12, -0.47, -0.15,  0.14, -0.30,\n                         0.10,  0.04,  0.08, -0.10, -0.02, -0.07, 0.06]], # coeffs for blue\n                       device = pyredner.get_device())\n# Deringing: directly using the original coefficients creates aliasing due to truncation,\n#            which results in negative values in the environment map when we rasterize the coefficients.\n#            Our solution is to multiply (convolve in sptial domain) the coefficients with a low pass\n#            filter.\n#            Following the recommendation in https://www.ppsloan.org/publications/shdering.pdf\n#            We use sinc^4 filter with a window size of 6\ndef deringing(coeffs, window):\n    deringed_coeffs = torch.zeros_like(coeffs)\n    deringed_coeffs[:, 0] += coeffs[:, 0]\n    deringed_coeffs[:, 1:1 + 3] += \\\n        coeffs[:, 1:1 + 3] * math.pow(math.sin(math.pi * 1.0 / window) / (math.pi * 1.0 / window), 4.0)\n    deringed_coeffs[:, 4:4 + 5] += \\\n        coeffs[:, 4:4 + 5] * math.pow(math.sin(math.pi * 2.0 / window) / (math.pi * 2.0 / window), 4.0)\n    deringed_coeffs[:, 9:9 + 7] += \\\n        coeffs[:, 9:9 + 7] * math.pow(math.sin(math.pi * 3.0 / window) / (math.pi * 3.0 / window), 4.0)\n    return deringed_coeffs\nderinged_coeffs = deringing(coeffs, 6.0)\nres = (256, 128)\n# We call the utility function SH_reconstruct to rasterize the coefficients into an envmap\nenvmap = pyredner.SH_reconstruct(deringed_coeffs, res)\n# Save the target envmap\npyredner.imwrite(envmap.cpu(), \'results/joint_material_envmap_sh/target_envmap.exr\')\n# Convert the PyTorch tensor into pyredner compatible envmap\nenvmap = pyredner.EnvironmentMap(envmap)\n# Setup the scene\nscene = pyredner.Scene(camera = cam,\n                       shapes = shapes,\n                       materials = materials,\n                       envmap = envmap)\n# Serialize the scene\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 512,\n    max_bounces = 1)\n# Render the target\nrender = pyredner.RenderFunction.apply\nimg = render(0, *scene_args)\n# Save the target image\npyredner.imwrite(img.cpu(), \'results/joint_material_envmap_sh/target.exr\')\npyredner.imwrite(img.cpu(), \'results/joint_material_envmap_sh/target.png\')\n# Read the target image back\ntarget = pyredner.imread(\'results/joint_material_envmap_sh/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda()\n\n# Reset the coefficients to some constant color, repeat the same process as in target envmap\ncoeffs = torch.tensor([[ 0.5,\n                         0.0, 0.0, 0.0,\n                         0.0, 0.0, 0.0, 0.0, 0.0,\n                         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], # coeffs for red\n                       [ 0.5,\n                         0.0, 0.0, 0.0,\n                         0.0, 0.0, 0.0, 0.0, 0.0,\n                         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], # coeffs for green\n                       [ 0.5,\n                         0.0, 0.0, 0.0,\n                         0.0, 0.0, 0.0, 0.0, 0.0,\n                         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]], # coeffs for blue\n                       device = pyredner.get_device(),\n                       requires_grad = True)\nderinged_coeffs = deringing(coeffs, 6.0)\nenvmap = pyredner.SH_reconstruct(deringed_coeffs, res)\npyredner.imwrite(envmap.cpu(), \'results/joint_material_envmap_sh/init_envmap.exr\')\nenvmap = pyredner.EnvironmentMap(envmap)\n# Also reset the material since we want to do joint estimation\n# We use an intermediate ""param"" variable and then take absolute value of it to avoid negative values\n# A better way to resolve this is to use projective SGD\ndiffuse_reflectance_param = \\\n    torch.tensor([0.3, 0.3, 0.3], device = pyredner.get_device(), requires_grad = True)\nspecular_reflectance_param = \\\n    torch.tensor([0.3, 0.3, 0.3], device = pyredner.get_device(), requires_grad = True)\nroughness_param = torch.tensor([0.3], device = pyredner.get_device(), requires_grad = True)\ndiffuse_reflectance = diffuse_reflectance_param.abs()\nspecular_reflectance = specular_reflectance_param.abs()\nroughness = roughness_param.abs() \nmat_grey = pyredner.Material(\\\n    diffuse_reflectance = diffuse_reflectance,\n    specular_reflectance = specular_reflectance,\n    roughness = roughness)\nmaterials = [mat_grey]\nscene = pyredner.Scene(camera = cam,\n                       shapes = shapes,\n                       materials = materials,\n                       envmap = envmap)\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 512,\n    max_bounces = 1)\nimg = render(1, *scene_args)\npyredner.imwrite(img.cpu(), \'results/joint_material_envmap_sh/init.png\')\ndiff = torch.abs(target - img)\npyredner.imwrite(diff.cpu(), \'results/joint_material_envmap_sh/init_diff.png\')\n\n# Finally we can start the Adam iteration\noptimizer = torch.optim.Adam(\\\n    [coeffs, diffuse_reflectance_param, specular_reflectance_param, roughness_param], lr=3e-2)\nfor t in range(400):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    # Repeat the envmap generation & material for the gradients\n    deringed_coeffs = deringing(coeffs, 6.0)\n    envmap = pyredner.SH_reconstruct(deringed_coeffs, res)\n    pyredner.imwrite(envmap.cpu(), \'results/joint_material_envmap_sh/envmap_{}.exr\'.format(t))\n    envmap = pyredner.EnvironmentMap(envmap)\n    diffuse_reflectance = diffuse_reflectance_param.abs()\n    specular_reflectance = specular_reflectance_param.abs()\n    roughness = roughness_param.abs() # avoid going below zero\n    materials[0] = pyredner.Material(\\\n        diffuse_reflectance = diffuse_reflectance,\n        specular_reflectance = specular_reflectance,\n        roughness = roughness)\n    scene = pyredner.Scene(camera = cam,\n                           shapes = shapes,\n                           materials = materials,\n                           envmap = envmap)\n    scene_args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4,\n        max_bounces = 1)\n    img = render(t+1, *scene_args)\n    pyredner.imwrite(img.cpu(), \'results/joint_material_envmap_sh/iter_{}.png\'.format(t))\n    loss = torch.pow(img - target, 2).sum()\n    print(\'loss:\', loss.item())\n\n    loss.backward()\n    optimizer.step()\n\n    # Print the gradients of the coefficients, material parameters\n    print(\'coeffs.grad:\', coeffs.grad)\n    print(\'diffuse_reflectance_param.grad:\', diffuse_reflectance_param.grad)\n    print(\'specular_reflectance_param.grad:\', specular_reflectance_param.grad)\n    print(\'roughness_param.grad:\', roughness_param.grad)\n    # Print the current parameters\n    print(\'coeffs:\', coeffs)\n    print(\'diffuse_reflectance:\', diffuse_reflectance)\n    print(\'specular_reflectance:\', specular_reflectance)\n    print(\'roughness:\', roughness)\n\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 512,\n    max_bounces = 1)\nimg = render(202, *scene_args)\npyredner.imwrite(img.cpu(), \'results/joint_material_envmap_sh/final.exr\')\npyredner.imwrite(img.cpu(), \'results/joint_material_envmap_sh/final.png\')\npyredner.imwrite(torch.abs(target - img).cpu(), \'results/joint_material_envmap_sh/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/joint_material_envmap_sh/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/joint_material_envmap_sh/out.mp4""])\n'"
examples/two_d_mesh.py,0,"b'import pyredner\nimport numpy as np\nimport torch\nimport redner\n\n# Optimize vertices of 2D meshes\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\n# Setup camera: We place the camera at (0, 0, -1), with look vector\n#               (0, 0, 1). We also use an orthographic camera just to\n#               make the projection more ""2D"": the depth is only used\n#               for determining the order of the meshes.\ncam = pyredner.Camera(position = torch.tensor([0.0, 0.0, -1.0]),\n                      look_at = torch.tensor([0.0, 0.0, 0.0]),\n                      up = torch.tensor([0.0, 1.0, 0.0]),\n                      fov = torch.tensor([45.0]), # in degree\n                      clip_near = 1e-2, # needs to > 0\n                      resolution = (256, 256),\n                      camera_type = redner.CameraType.orthographic)\n\n# The materials: \nmat_quad = pyredner.Material(\\\n    diffuse_reflectance = torch.tensor([0.75, 0.75, 0.25],\n    device = pyredner.get_device()))\nmat_tri = pyredner.Material(\\\n    diffuse_reflectance = torch.tensor([0.9, 0.35, 0.35],\n    device = pyredner.get_device()))\nmaterials = [mat_quad, mat_tri]\n\n# We\'ll have a quad and a triangle as our meshes.\n# First we define the 2D coordinates. The size of the screen is\n# from -1.0 to 1.0. Y is pointing up.\nquad_vertices_2d = torch.tensor(\\\n    [[-0.3, 0.5], [0.2, 0.6], [-0.5, -0.3], [0.5, -0.4]],\n    device = pyredner.get_device())\ntri_vertices_2d = torch.tensor(\\\n    [[-0.6, 0.3], [0.4, 0.5], [-0.1, -0.2]],\n    device = pyredner.get_device())\n# We need to pad the depth coordinates for these vertices\n# We\'ll assign depth = 1 for the quad, depth = 0 for the triangle,\n# so the triangle will block the quad.\nquad_vertices = torch.cat((quad_vertices_2d,\n    torch.ones(quad_vertices_2d.shape[0], 1, device = pyredner.get_device())), dim=1).contiguous()\ntri_vertices = torch.cat((tri_vertices_2d,\n    torch.zeros(tri_vertices_2d.shape[0], 1, device = pyredner.get_device())), dim=1).contiguous()\nquad_indices = torch.tensor([[0, 1, 2], [1, 2, 3]], dtype = torch.int32, device = pyredner.get_device())\ntri_indices = torch.tensor([[0, 1, 2]], dtype = torch.int32, device = pyredner.get_device())\nshape_quad = pyredner.Shape(\\\n    vertices = quad_vertices,\n    indices = quad_indices,\n    material_id = 0)\nshape_tri = pyredner.Shape(\\\n    vertices = tri_vertices,\n    indices = tri_indices,\n    material_id = 1)\nshapes = [shape_quad, shape_tri]\n\n# Setup the scene. We don\'t need lights.\nscene = pyredner.Scene(camera = cam,\n                       shapes = shapes,\n                       materials = materials)\n# We output the shape id, so that we can shape it later\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    # Set max bounces to 0, we don\'t need lighting.\n    max_bounces = 0,\n    # Use the diffuse color as the output\n    channels = [redner.channels.diffuse_reflectance])\n\n# Render the scene as our target image.\nrender = pyredner.RenderFunction.apply\nimg = render(0, *args)\npyredner.imwrite(img.cpu(), \'results/two_d_mesh/target.exr\')\npyredner.imwrite(img.cpu(), \'results/two_d_mesh/target.png\')\ntarget = pyredner.imread(\'results/two_d_mesh/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda(device = pyredner.get_device())\n\n# Perturb the scene, this is our initial guess\nquad_vertices_2d = torch.tensor(\\\n    [[-0.5, 0.3], [0.3, 0.4], [-0.7, -0.2], [0.4, -0.3]],\n    device = pyredner.get_device(),\n    requires_grad = True)\ntri_vertices_2d = torch.tensor(\\\n    [[-0.5, 0.4], [0.4, 0.6], [-0.0, -0.3]],\n    device = pyredner.get_device(),\n    requires_grad = True)\n# Need to redo the concatenation\nshape_quad.vertices = torch.cat((quad_vertices_2d,\n    torch.ones(quad_vertices_2d.shape[0], 1, device = pyredner.get_device())), dim=1).contiguous()\nshape_tri.vertices = torch.cat((tri_vertices_2d,\n    torch.zeros(tri_vertices_2d.shape[0], 1, device = pyredner.get_device())), dim=1).contiguous()\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    # Set max bounces to 0, we don\'t need lighting.\n    max_bounces = 0,\n    # Use the diffuse color as the output\n    channels = [redner.channels.diffuse_reflectance])\n# Render the initial guess.\nrender = pyredner.RenderFunction.apply\nimg = render(0, *args)\npyredner.imwrite(img.cpu(), \'results/two_d_mesh/init.exr\')\npyredner.imwrite(img.cpu(), \'results/two_d_mesh/init.png\')\n\n# Optimize for mesh vertices\noptimizer = torch.optim.Adam([quad_vertices_2d, tri_vertices_2d], lr=4e-2)\nfor t in range(200):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    # Forward pass: render the image\n    # Need to redo the concatenation\n    shape_quad.vertices = torch.cat((quad_vertices_2d,\n        torch.ones(quad_vertices_2d.shape[0], 1, device = pyredner.get_device())), dim=1).contiguous()\n    shape_tri.vertices = torch.cat((tri_vertices_2d,\n        torch.zeros(tri_vertices_2d.shape[0], 1, device = pyredner.get_device())), dim=1).contiguous()\n    args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 1,\n        max_bounces = 0,\n        channels = [redner.channels.diffuse_reflectance])\n    img = render(t+1, *args)\n    pyredner.imwrite(img.cpu(), \'results/two_d_mesh/iter_{}.png\'.format(t))\n    loss = (img - target).pow(2).sum()\n    print(\'loss:\', loss.item())\n\n    loss.backward()\n    print(\'quad_vertices_2d.grad:\', quad_vertices_2d.grad)\n    print(\'tri_vertices_2d.grad:\', tri_vertices_2d.grad)\n\n    optimizer.step()\n    print(\'quad_vertices_2d:\', quad_vertices_2d)\n    print(\'tri_vertices_2d:\', tri_vertices_2d)\n\nshape_quad.vertices = torch.cat((quad_vertices_2d,\n    torch.ones(quad_vertices_2d.shape[0], 1, device = pyredner.get_device())), dim=1).contiguous()\nshape_tri.vertices = torch.cat((tri_vertices_2d,\n    torch.zeros(tri_vertices_2d.shape[0], 1, device = pyredner.get_device())), dim=1).contiguous()\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 0,\n    channels = [redner.channels.diffuse_reflectance])\nimg = render(t+1, *args)\n\npyredner.imwrite(img.cpu(), \'results/two_d_mesh/final.exr\')\npyredner.imwrite(img.cpu(), \'results/two_d_mesh/final.png\')\npyredner.imwrite(torch.abs(target - img).cpu(), \'results/two_d_mesh/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/two_d_mesh/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/two_d_mesh/out.mp4""])\n'"
pyredner/__init__.py,0,b'from .device import *\nfrom .camera_type import camera_type\nfrom .camera import *\nfrom .shape import *\nfrom .texture import *\nfrom .material import *\nfrom .area_light import *\nfrom .object import *\nfrom .envmap import *\nfrom .scene import *\nfrom .render_pytorch import *\nfrom .image import *\nfrom .load_obj import load_obj\nfrom .save_obj import save_obj\nfrom .utils import *\nfrom .load_mitsuba import load_mitsuba\nfrom .transform import *\nfrom .channels import channels\nfrom .sampler_type import sampler_type\nfrom .render_utils import *\nfrom .geometry_images import *'
pyredner/area_light.py,0,"b'import torch\n\nclass AreaLight:\n    """"""\n        A mesh-based area light that points to a shape and assigns intensity.\n\n        Args\n        ----------\n        shape_id: int\n\n        intensity: torch.Tensor\n            1-d tensor with size 3 and type float32\n        two_sided: bool\n            Is the light emitting light from the two sides of the faces?\n        directly_visible: bool\n            Can the camera sees the light source directly?\n    """"""\n\n    def __init__(self,\n                 shape_id: int,\n                 intensity: torch.Tensor,\n                 two_sided: bool = False,\n                 directly_visible: bool = True):\n        self.shape_id = shape_id\n        self.intensity = intensity\n        self.two_sided = two_sided\n        self.directly_visible = directly_visible\n\n    def state_dict(self):\n        return {\n            \'shape_id\': self.shape_id,\n            \'intensity\': self.intensity,\n            \'two_sided\': self.two_sided,\n            \'directly_visible\': self.directly_visible\n        }\n\n    @classmethod\n    def load_state_dict(cls, state_dict):\n        return cls(\n            state_dict[\'shape_id\'],\n            state_dict[\'intensity\'],\n            state_dict[\'two_sided\'],\n            state_dict[\'directly_visible\'])\n'"
pyredner/camera.py,0,"b'import torch\nimport pyredner.transform as transform\nimport redner\nimport math\nimport pyredner\nfrom typing import Tuple, Optional, List\n\nclass Camera:\n    """"""\n        Redner supports four types of cameras\\: perspective, orthographic, fisheye, and panorama.\n        The camera takes a look at transform or a cam_to_world matrix to\n        transform from camera local space to world space. It also can optionally\n        take an intrinsic matrix that models field of view and camera skew.\n\n        Args\n        ====\n        position: Optional[torch.Tensor]\n            the origin of the camera, 1-d tensor with size 3 and type float32\n        look_at: Optional[torch.Tensor]\n            the point camera is looking at, 1-d tensor with size 3 and type float32\n        up: Optional[torch.Tensor]\n            the up vector of the camera, 1-d tensor with size 3 and type float32\n        fov: Optional[torch.Tensor]\n            the field of view of the camera in angle, \n            no effect if the camera is a fisheye or panorama camera, \n            1-d tensor with size 1 and type float32\n        clip_near: float\n            the near clipping plane of the camera, need to > 0\n        resolution: Tuple[int, int]\n            the size of the output image in (height, width)\n        viewport: Optional[Tuple[int, int, int, int]]\n            optional viewport argument for rendering only a region of an image in\n            (left_top_y, left_top_x, bottom_right_y, bottom_right_x),\n            bottom_right is not inclusive.\n            if set to None the viewport is the whole image (i.e., (0, 0, cam.height, cam.width))\n        cam_to_world: Optional[torch.Tensor]\n            overrides position, look_at, up vectors\n            4x4 matrix, optional\n        intrinsic_mat: Optional[torch.Tensor]\n            a matrix that transforms a point in camera space before the point\n            is projected to 2D screen space\n            used for modelling field of view and camera skewing\n            after the multiplication the point should be in\n            [-1, 1/aspect_ratio] x [1, -1/aspect_ratio] in homogeneous coordinates\n            the projection is then carried by the specific camera types\n            perspective camera normalizes the homogeneous coordinates\n            while orthogonal camera drop the Z coordinate.\n            ignored by fisheye or panorama cameras\n            overrides fov\n            3x3 matrix, optional\n        distortion_params: Optional[torch.Tensor]\n            an array describing the coefficient of a Brown\xe2\x80\x93Conrady lens distortion model.\n            the array is expected to be 1D with size of 8. the first six coefficients describes\n            the parameters of the rational polynomial for radial distortion (k1~k6) and\n            the last two coefficients are for the tangential distortion (p1~p2).\n            see https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html\n            for more details.\n        camera_type: render.camera_type\n            the type of the camera (perspective, orthographic, fisheye, or panorama)\n        fisheye: bool\n            whether the camera is a fisheye camera\n            (legacy parameter just to ensure compatibility).\n    """"""\n    def __init__(self,\n                 position: Optional[torch.Tensor] = None,\n                 look_at: Optional[torch.Tensor] = None,\n                 up: Optional[torch.Tensor] = None,\n                 fov: Optional[torch.Tensor] = None,\n                 clip_near: float = 1e-4,\n                 resolution: Tuple[int, int] = (256, 256),\n                 viewport: Optional[Tuple[int, int, int, int]] = None,\n                 cam_to_world: Optional[torch.Tensor] = None,\n                 intrinsic_mat: Optional[torch.Tensor] = None,\n                 distortion_params: Optional[torch.Tensor] = None,\n                 camera_type = pyredner.camera_type.perspective,\n                 fisheye: bool = False):\n        if position is not None:\n            assert(position.dtype == torch.float32)\n            assert(len(position.shape) == 1 and position.shape[0] == 3)\n        if look_at is not None:\n            assert(look_at.dtype == torch.float32)\n            assert(len(look_at.shape) == 1 and look_at.shape[0] == 3)\n        if up is not None:\n            assert(up.dtype == torch.float32)\n            assert(len(up.shape) == 1 and up.shape[0] == 3)\n        if fov is not None:\n            assert(fov.dtype == torch.float32)\n            assert(len(fov.shape) == 1 and fov.shape[0] == 1)\n        if cam_to_world is not None:\n            assert(cam_to_world.dtype == torch.float32)\n            assert(len(cam_to_world.shape) == 2 and cam_to_world.shape[0] == 4 and cam_to_world.shape[1] == 4)\n        if intrinsic_mat is not None:\n            assert(intrinsic_mat.dtype == torch.float32)\n            assert(len(intrinsic_mat.shape) == 2 and intrinsic_mat.shape[0] == 3 and intrinsic_mat.shape[1] == 3)\n        assert(isinstance(clip_near, float))\n        if position is None and look_at is None and up is None:\n            assert(cam_to_world is not None)\n\n        self.position = position\n        self.look_at = look_at\n        self.up = up\n        self._fov = fov\n        self._cam_to_world = cam_to_world\n        if cam_to_world is not None:\n            self.world_to_cam = torch.inverse(self.cam_to_world).contiguous()\n        else:\n            self.world_to_cam = None\n        if intrinsic_mat is None:\n            if camera_type == redner.CameraType.perspective:\n                fov_factor = 1.0 / torch.tan(transform.radians(0.5 * fov))\n                o = torch.ones([1], dtype=torch.float32)\n                diag = torch.cat([fov_factor, fov_factor, o], 0)\n                self._intrinsic_mat = torch.diag(diag).contiguous()\n            else:\n                self._intrinsic_mat = torch.eye(3, dtype=torch.float32)\n        else:\n            self._intrinsic_mat = intrinsic_mat\n        self.intrinsic_mat_inv = torch.inverse(self.intrinsic_mat).contiguous()\n        self.distortion_params = distortion_params\n        self.clip_near = clip_near\n        self.resolution = resolution\n        self.viewport = viewport\n        self.camera_type = camera_type\n        if fisheye:\n            self.camera_type = pyredner.camera_type.fisheye\n\n    @property\n    def fov(self):\n        return self._fov\n\n    @fov.setter\n    def fov(self, value):\n        self._fov = value\n        fov_factor = 1.0 / torch.tan(transform.radians(0.5 * self._fov))\n        o = torch.ones([1], dtype=torch.float32)\n        diag = torch.cat([fov_factor, fov_factor, o], 0)\n        self._intrinsic_mat = torch.diag(diag).contiguous()\n        self.intrinsic_mat_inv = torch.inverse(self._intrinsic_mat).contiguous()\n\n    @property\n    def intrinsic_mat(self):\n        return self._intrinsic_mat\n\n    @intrinsic_mat.setter\n    def intrinsic_mat(self, value):\n        if value is not None:\n            self._intrinsic_mat = value\n            self.intrinsic_mat_inv = torch.inverse(self._intrinsic_mat).contiguous()\n        else:\n            assert(self.fov is not None)\n            self.fov = self._fov\n\n    @property\n    def cam_to_world(self):\n        return self._cam_to_world\n\n    @cam_to_world.setter\n    def cam_to_world(self, value):\n        if value is not None:\n            self._cam_to_world = value\n            self.world_to_cam = torch.inverse(self.cam_to_world).contiguous()\n        else:\n            self._cam_to_world = None\n            self.world_to_cam = None\n\n    def state_dict(self):\n        return {\n            \'position\': self._position,\n            \'look_at\': self._look_at,\n            \'up\': self._up,\n            \'fov\': self._fov,\n            \'cam_to_world\': self._cam_to_world,\n            \'intrinsic_mat\': self._intrinsic_mat,\n            \'clip_near\': self.clip_near,\n            \'resolution\': self.resolution,\n            \'camera_type\': self.camera_type\n        }\n\n    @classmethod\n    def load_state_dict(cls, state_dict):\n        out = cls.__new__(Camera)\n        out._position = state_dict[\'position\']\n        out._look_at = state_dict[\'look_at\']\n        out._up = state_dict[\'up\']\n        out._fov = state_dict[\'fov\']\n        out.cam_to_world = state_dict[\'cam_to_world\']\n        out.intrinsic_mat = state_dict[\'intrinsic_mat\']\n        out.clip_near = state_dict[\'clip_near\']\n        out.resolution = state_dict[\'resolution\']\n        out.camera_type = state_dict[\'camera_type\']\n        return out\n\ndef automatic_camera_placement(shapes: List,\n                               resolution: Tuple[int, int]):\n    """"""\n        Given a list of objects or shapes, generates camera parameters automatically\n        using the bounding boxes of the shapes. Place the camera at\n        some distances from the shapes, so that it can see all of them.\n        Inspired by https://github.com/mitsuba-renderer/mitsuba/blob/master/src/librender/scene.cpp#L286\n\n        Parameters\n        ==========\n        shapes: List\n            a list of redner Shape or Object\n        resolution: Tuple[int, int]\n            the size of the output image in (height, width)\n\n        Returns\n        =======\n        pyredner.Camera\n            a camera that can see all the objects.\n    """"""\n    aabb_min = torch.tensor((float(\'inf\'), float(\'inf\'), float(\'inf\')))\n    aabb_max = -torch.tensor((float(\'inf\'), float(\'inf\'), float(\'inf\')))\n    for shape in shapes:\n        v = shape.vertices\n        v_min = torch.min(v, 0)[0].cpu()\n        v_max = torch.max(v, 0)[0].cpu()\n        aabb_min = torch.min(aabb_min, v_min)\n        aabb_max = torch.max(aabb_max, v_max)\n    assert(torch.isfinite(aabb_min).all() and torch.isfinite(aabb_max).all())\n    center = (aabb_max + aabb_min) * 0.5\n    extents = aabb_max - aabb_min\n    max_extents_xy = torch.max(extents[0], extents[1])\n    distance = max_extents_xy / (2 * math.tan(45 * 0.5 * math.pi / 180.0))\n    max_extents_xyz = torch.max(extents[2], max_extents_xy)    \n    return Camera(position = torch.tensor((center[0], center[1], aabb_min[2] - distance)),\n                  look_at = center,\n                  up = torch.tensor((0.0, 1.0, 0.0)),\n                  fov = torch.tensor([45.0]),\n                  clip_near = 0.001 * float(distance),\n                  resolution = resolution)\n\ndef generate_intrinsic_mat(fx: torch.Tensor,\n                           fy: torch.Tensor,\n                           skew: torch.Tensor,\n                           x0: torch.Tensor,\n                           y0: torch.Tensor):\n    """"""\n        | Generate the following 3x3 intrinsic matrix given the parameters.\n        | fx, skew, x0\n        |  0,   fy, y0\n        |  0,    0,  1\n\n        Parameters\n        ==========\n        fx: torch.Tensor\n            Focal length at x dimension. 1D tensor with size 1.\n        fy: torch.Tensor\n            Focal length at y dimension. 1D tensor with size 1.\n        skew: torch.Tensor\n            Axis skew parameter describing shearing transform. 1D tensor with size 1.\n        x0: torch.Tensor\n            Principle point offset at x dimension. 1D tensor with size 1.\n        y0: torch.Tensor\n            Principle point offset at y dimension. 1D tensor with size 1.\n\n        Returns\n        =======\n        torch.Tensor\n            3x3 intrinsic matrix\n    """"""\n    z = torch.zeros_like(fx)\n    o = torch.ones_like(fx)\n    row0 = torch.cat([fx, skew, x0])\n    row1 = torch.cat([ z,   fy, y0])\n    row2 = torch.cat([ z,    z,  o])\n    return torch.stack([row0, row1, row2]).contiguous()\n'"
pyredner/camera_type.py,0,b'import redner\n\nclass CameraType:\n    def __init__(self):\n        self.perspective = redner.CameraType.perspective\n        self.orthographic = redner.CameraType.orthographic\n        self.fisheye = redner.CameraType.fisheye\n        self.panorama = redner.CameraType.panorama\n\ncamera_type = CameraType()\n'
pyredner/channels.py,0,b'import redner\n\nclass Channel:\n    def __init__(self):\n        self.radiance = redner.channels.radiance\n        self.alpha = redner.channels.alpha\n        self.depth = redner.channels.depth\n        self.position = redner.channels.position\n        self.geometry_normal = redner.channels.geometry_normal\n        self.shading_normal = redner.channels.shading_normal\n        self.uv = redner.channels.uv\n        self.barycentric_coordinates = redner.channels.barycentric_coordinates\n        self.diffuse_reflectance = redner.channels.diffuse_reflectance\n        self.specular_reflectance = redner.channels.specular_reflectance\n        self.roughness = redner.channels.roughness\n        self.generic_texture = redner.channels.generic_texture\n        self.vertex_color = redner.channels.vertex_color\n        self.shape_id = redner.channels.shape_id\n        self.triangle_id = redner.channels.triangle_id\n        self.material_id = redner.channels.material_id\n\nchannels = Channel()\n'
pyredner/device.py,0,"b'import torch\n\nuse_gpu = torch.cuda.is_available()\ndevice = torch.device(\'cuda\') if use_gpu else torch.device(\'cpu\')\n\ndef set_use_gpu(v: bool):\n    """"""\n        Set whether to use CUDA or not.\n    """"""\n    global use_gpu\n    global device\n    use_gpu = v\n    if not use_gpu:\n        device = torch.device(\'cpu\')\n    else:\n        assert(torch.cuda.is_available())\n        device = torch.device(\'cuda\')\n\ndef get_use_gpu():\n    """"""\n        Get whether we are using CUDA or not.\n    """"""\n    global use_gpu\n    return use_gpu\n\ndef set_device(d: torch.device):\n    """"""\n        Set the torch device we are using.\n    """"""\n    global device\n    global use_gpu\n    device = d\n    use_gpu = device.type == \'cuda\'\n\ndef get_device():\n    """"""\n        Get the torch device we are using.\n    """"""\n    global device\n    return device\n\n'"
pyredner/envmap.py,0,"b'import pyredner\nimport torch\nimport math\nfrom typing import Union\n\nclass EnvironmentMap:\n    """"""\n        A class representing light sources infinitely far away using an image.\n\n        Args\n        ----------\n        values: Union[torch.Tensor, pyredner.Texture]\n            a float32 tensor with size 3 or [height, width, 3] or a Texture\n        env_to_world: torch.Tensor\n            a float32 4x4 matrix that transforms the environment map\n        directly_visible: bool\n            can the camera sees the light source directly?\n    """"""\n\n    def __init__(self,\n                 values: Union[torch.Tensor, pyredner.Texture],\n                 env_to_world: torch.Tensor = torch.eye(4, 4),\n                 directly_visible: bool = True):\n        # Convert to constant texture if necessary\n        if isinstance(values, torch.Tensor):\n            values = pyredner.Texture(values)\n        assert(env_to_world.dtype == torch.float32)\n        assert(env_to_world.is_contiguous())\n\n        self._values = values\n        self._env_to_world = env_to_world\n        self.world_to_env = torch.inverse(env_to_world).contiguous()\n        self.generate_envmap_pdf()\n        self.directly_visible = directly_visible\n\n    def generate_envmap_pdf(self):\n        values = self.values\n        # Build sampling table\n        luminance = 0.212671 * values.texels[:, :, 0] + \\\n                    0.715160 * values.texels[:, :, 1] + \\\n                    0.072169 * values.texels[:, :, 2]\n        # For each y, compute CDF over x\n        sample_cdf_xs_ = torch.cumsum(luminance, dim = 1)\n        y_weight = torch.sin(\\\n            math.pi * (torch.arange(luminance.shape[0],\n                dtype = torch.float32, device = luminance.device) + 0.5) \\\n             / float(luminance.shape[0]))\n        # Compute CDF for x\n        sample_cdf_ys_ = torch.cumsum(sample_cdf_xs_[:, -1] * y_weight, dim = 0)\n        pdf_norm = (luminance.shape[0] * luminance.shape[1]) / \\\n            (sample_cdf_ys_[-1].item() * (2 * math.pi * math.pi))\n        # Normalize to [0, 1)\n        sample_cdf_xs = (sample_cdf_xs_ - sample_cdf_xs_[:, 0:1]) / \\\n            torch.max(sample_cdf_xs_[:, (luminance.shape[1] - 1):luminance.shape[1]],\n                1e-8 * torch.ones(sample_cdf_xs_.shape[0], 1, device = sample_cdf_ys_.device))\n        sample_cdf_ys = (sample_cdf_ys_ - sample_cdf_ys_[0]) / \\\n            torch.max(sample_cdf_ys_[-1], torch.tensor([1e-8], device = sample_cdf_ys_.device))\n        self.sample_cdf_ys = sample_cdf_ys.contiguous()\n        self.sample_cdf_xs = sample_cdf_xs.contiguous()\n        self.pdf_norm = pdf_norm\n\n    @property\n    def values(self):\n        return self._values\n\n    @values.setter\n    def values(self, value):\n        self._values = value\n        self.generate_envmap_pdf()\n\n    @property\n    def env_to_world(self):\n        return self._env_to_world\n\n    @env_to_world.setter\n    def env_to_world(self, value):\n        self._env_to_world = value\n        self.world_to_env = torch.inverse(self._env_to_world).contiguous()\n\n    def state_dict(self):\n        return {\n            \'values\': self.values.state_dict(),\n            \'env_to_world\': self.env_to_world,\n            \'world_to_env\': self.world_to_env,\n            \'sample_cdf_ys\': self.sample_cdf_ys,\n            \'sample_cdf_xs\': self.sample_cdf_xs,\n            \'pdf_norm\': self.pdf_norm,\n            \'directly_visible\': self.directly_visible\n        }\n\n    @classmethod\n    def load_state_dict(cls, state_dict):\n        out = cls.__new__(EnvironmentMap)\n        out.values = pyredner.Texture.load_state_dict(state_dict[\'values\'])\n        out.env_to_world = state_dict[\'env_to_world\']\n        out.world_to_env = state_dict[\'world_to_env\']\n        out.sample_cdf_ys = state_dict[\'sample_cdf_ys\']\n        out.sample_cdf_xs = state_dict[\'sample_cdf_xs\']\n        out.pdf_norm = state_dict[\'pdf_norm\']\n        out.directly_visible = state_dict[\'directly_visible\']\n        return out\n'"
pyredner/geometry_images.py,0,"b'import numpy as np\nimport torch\nimport math\nimport pyredner\nfrom typing import Optional\n\ndef generate_geometry_image(size: int,\n                            device: Optional[torch.device] = None):\n    """"""\n        Generate an spherical geometry image [Gu et al. 2002 and Praun and Hoppe 2003]\n        of size [2 * size + 1, 2 * size + 1]. This can be used for encoding a genus-0\n        surface into a regular image, so that it is more convienent for a CNN to process.\n        The topology is given by a tesselated octahedron. UV is given by the spherical mapping.\n        Duplicated vertex are mapped to the one with smaller index (so some vertices on the\n        geometry image is unused by the indices).\n\n        Args\n        ====\n        size: int\n            Size of the geometry image.\n        device: Optional[torch.device]\n            Which device should we store the data in.\n            If set to None, use the device from pyredner.get_device().\n\n        Returns\n        =======\n        torch.Tensor\n            vertices of size [(2 * size + 1 * 2 * size + 1), 3]\n        torch.Tensor\n            indices of size [2 * (2 * size + 1 * 2 * size + 1), 3]\n        torch.Tensor\n            uvs of size [(2 * size + 1 * 2 * size + 1), 2]\n    """"""\n    if device is None:\n        device = pyredner.get_device()\n\n    size *= 2\n\n    # Generate vertices and uv by going through each vertex.\n    left_top = np.array([0.0, 0.0, 1.0])\n    top = np.array([0.0, 1.0, 0.0])\n    right_top = np.array([0.0, 0.0, 1.0])\n    left = np.array([-1.0, 0.0, 0.0])\n    middle = np.array([0.0, 0.0, -1.0])\n    right = np.array([1.0, 0.0, 0.0])\n    left_bottom = np.array([0.0, 0.0, 1.0])\n    bottom = np.array([0.0, -1.0, 0.0])\n    right_bottom = np.array([0.0, 0.0, 1.0])\n    vertices = np.zeros([(size+1) * (size+1), 3])\n    uvs = np.zeros([(size+1) * (size + 1), 2])\n    vertex_id = 0\n    half_size = size / 2.0\n    for i in range(size+1): # height\n        for j in range(size+1): # width\n            # Left Top\n            if i  + j <= half_size:\n                org = left_top\n                i_axis = left - left_top\n                j_axis = top - left_top\n                i_ = float(i) / half_size\n                j_ = float(j) / half_size\n            elif (i + j >= half_size and i <= half_size and j <= half_size):\n                org = middle\n                i_axis = top - middle\n                j_axis = left - middle\n                i_ = 1.0 - float(i) / half_size\n                j_ = 1.0 - float(j) / half_size\n            # Right Top\n            elif ((half_size - i + j - half_size) <= half_size and i <= half_size and j >= half_size):\n                org = middle\n                i_axis = top - middle\n                j_axis = right - middle\n                i_ = 1.0 - float(i) / half_size\n                j_ = float(j) / half_size - 1.0\n            elif ((i + size - j) <= half_size):\n                org = right_top\n                i_axis = right - right_top\n                j_axis = top - right_top\n                i_ = float(i) / half_size\n                j_ = 2.0 - float(j) / half_size\n            # Left Bottom\n            elif ((i - half_size + half_size - j) <= half_size and i >= half_size and j <= half_size):\n                org = middle\n                i_axis = bottom - middle\n                j_axis = left - middle\n                i_ = float(i) / half_size - 1.0\n                j_ = 1.0 - float(j) / half_size\n            elif ((size - i + j) <= half_size):\n                org = left_bottom\n                i_axis = left - left_bottom\n                j_axis = bottom - left_bottom\n                i_ = 2.0 - float(i) / half_size\n                j_ = float(j) / half_size\n            # Right Bottom\n            elif ((i - half_size + j - half_size) <= half_size and i >= half_size and j >= half_size):\n                org = middle\n                i_axis = bottom - middle\n                j_axis = right - middle\n                i_ = float(i) / half_size - 1.0\n                j_ = float(j) / half_size - 1.0\n            else:\n                org = right_bottom\n                i_axis = right - right_bottom\n                j_axis = bottom - right_bottom\n                i_ = 2.0 - float(i) / half_size\n                j_ = 2.0 - float(j) / half_size\n            p = org + i_ * i_axis + j_ * j_axis\n            vertices[vertex_id, :] = p / np.linalg.norm(p)\n            # Spherical UV mapping\n            u = 0.5 + math.atan2(float(p[2]), float(p[0])) / (2 * math.pi)\n            v = 0.5 - math.asin(float(p[1])) / math.pi\n            uvs[vertex_id, :] = np.array([u, v])\n            vertex_id += 1\n\n    # Generate indices by going through each triangle.\n    # Duplicated vertex are mapped to the one with smaller index.\n    indices = []\n    for i in range(size): # height\n        for j in range(size): # width\n            left_top = i * (size + 1) + j\n            right_top = i * (size + 1) + j + 1\n            left_bottom = (i + 1) * (size + 1) + j\n            right_bottom = (i + 1) * (size + 1) + j + 1\n            # Wrap rule for octahedron topology\n            if i == 0 and j >= half_size:\n                if j > half_size:\n                    left_top = i * (size + 1) + size - j\n                right_top = i * (size + 1) + (size - (j + 1))\n            elif i == size - 1 and j >= half_size:\n                if j > half_size:\n                    left_bottom = (i + 1) * (size + 1) + size - j\n                right_bottom = (i + 1) * (size + 1) + (size - (j + 1))\n                if j == size - 1:\n                    right_bottom = 0\n            elif j == 0 and i >= half_size:\n                if i > half_size:\n                    left_top = (size - i) * (size + 1) + j\n                left_bottom = (size - (i + 1)) * (size + 1) + j\n            elif j == size - 1 and i >= half_size:\n                if i > half_size:\n                    right_top = (size - i) * (size + 1) + j + 1\n                right_bottom = (size - (i + 1)) * (size + 1) + j + 1\n\n            # Left Top\n            if i < half_size and j < half_size:\n                indices.append((left_top, left_bottom, right_top))\n                indices.append((right_top, left_bottom, right_bottom))\n            # Right Top\n            elif i < half_size and j >= half_size:\n                indices.append((left_top, left_bottom, right_bottom))\n                indices.append((left_top, right_bottom, right_top))\n            # Left Bottom\n            elif i >= half_size and j < half_size:\n                indices.append((left_top, right_bottom, right_top))\n                indices.append((left_top, left_bottom, right_bottom))\n            # Right Bottom\n            else:\n                indices.append((left_top, left_bottom, right_top))\n                indices.append((right_top, left_bottom, right_bottom))\n\n    vertices = torch.tensor(vertices, dtype = torch.float32, device = device)\n    uvs = torch.tensor(uvs, dtype = torch.float32, device = device)\n    indices = torch.tensor(indices, dtype = torch.int32, device = device)\n    return vertices, indices, uvs\n'"
pyredner/image.py,0,"b'import numpy as np\nimport skimage\nimport skimage.io\nimport torch\nimport os\nimport imageio\n\ndef imwrite(img: torch.Tensor,\n            filename: str,\n            gamma: float = 2.2,\n            normalize: bool = False):\n    """"""\n        write img to filename\n\n        Args\n        ====\n        img: torch.Tensor\n            with size [height, width, channel]\n        filename: str\n\n        gamma: float\n            if the image is not an OpenEXR file, apply gamma correction\n        normalize:\n            normalize img to the range [0, 1] before writing\n    """"""\n\n    directory = os.path.dirname(filename)\n    if directory != \'\' and not os.path.exists(directory):\n        os.makedirs(directory)\n    \n    img = img.data.numpy()\n    if normalize:\n        img_rng = np.max(img) - np.min(img)\n        if img_rng > 0:\n            img = (img - np.min(img)) / img_rng\n    if filename[-4:] == \'.exr\':\n        imageio.plugins.freeimage.download()\n        imageio.imwrite(filename, img)\n    else:\n        skimage.io.imsave(filename,\n            (np.power(np.clip(img, 0.0, 1.0), 1.0/gamma) * 255).astype(np.uint8))\n\ndef imread(filename: str,\n           gamma: float = 2.2):\n    """"""\n        read img from filename\n\n        Args\n        ====\n        filename: str\n\n        gamma: float\n            if the image is not an OpenEXR file, apply gamma correction\n\n        Returns\n        =======\n        torch.Tensor\n            a float32 tensor with size [height, width, channel]\n    """"""\n\n    if (filename[-4:] == \'.exr\'):\n        imageio.plugins.freeimage.download()\n        return torch.from_numpy(imageio.imread(filename).astype(np.float32))\n    else:\n        im = skimage.io.imread(filename)\n        if im.ndim == 2:\n            im = np.stack([im, im, im], axis=-1)\n        elif im.shape[2] == 4:\n            im = im[:, :, :3]\n        return torch.from_numpy(np.power(\\\n            skimage.img_as_float(im).astype(np.float32), gamma))\n'"
pyredner/load_mitsuba.py,0,"b'import torch\nimport xml.etree.ElementTree as etree\nimport numpy as np\nimport redner\nimport os\nimport pyredner\nimport pyredner.transform as transform\nfrom typing import Optional\nimport math\n\ndef parse_transform(node):\n    ret = torch.eye(4)\n    for child in node:\n        if child.tag == \'matrix\':\n            value = torch.from_numpy(\\\n                np.reshape(\\\n                    # support both \',\' and \' \' seperator\n                    np.fromstring(child.attrib[\'value\'], dtype=np.float32, sep=\',\' if \',\' in child.attrib[\'value\'] else \' \'),\n                    (4, 4)))\n            ret = value @ ret\n        elif child.tag == \'translate\':\n            x = float(child.attrib[\'x\'])\n            y = float(child.attrib[\'y\'])\n            z = float(child.attrib[\'z\'])\n            value = transform.gen_translate_matrix(torch.tensor([x, y, z]))\n            ret = value @ ret\n        elif child.tag == \'scale\':\n            # single scale value\n            if \'value\' in child.attrib:\n                x = y = z = float(child.attrib[\'value\'])\n            else:\n                x = float(child.attrib[\'x\'])\n                y = float(child.attrib[\'y\'])\n                z = float(child.attrib[\'z\'])\n            value = transform.gen_scale_matrix(torch.tensor([x, y, z]))\n            ret = value @ ret\n        elif child.tag == \'rotate\':\n            x = float(child.attrib[\'x\']) if \'x\' in child.attrib else 0.0\n            y = float(child.attrib[\'y\']) if \'y\' in child.attrib else 0.0\n            z = float(child.attrib[\'z\']) if \'z\' in child.attrib else 0.0\n            angle = transform.radians(float(child.attrib[\'angle\']))\n            axis = np.array([x, y, z])\n            axis = axis / np.linalg.norm(axis)\n            cos_theta = math.cos(angle)\n            sin_theta = math.sin(angle)\n            mat = torch.zeros(4, 4)\n            mat[0, 0] = axis[0] * axis[0] + (1.0 - axis[0] * axis[0]) * cos_theta\n            mat[0, 1] = axis[0] * axis[1] * (1.0 - cos_theta) - axis[2] * sin_theta\n            mat[0, 2] = axis[0] * axis[2] * (1.0 - cos_theta) + axis[1] * sin_theta\n\n            mat[1, 0] = axis[0] * axis[1] * (1.0 - cos_theta) + axis[2] * sin_theta\n            mat[1, 1] = axis[1] * axis[1] + (1.0 - axis[1] * axis[1]) * cos_theta\n            mat[1, 2] = axis[1] * axis[2] * (1.0 - cos_theta) - axis[0] * sin_theta\n\n            mat[2, 0] = axis[0] * axis[2] * (1.0 - cos_theta) - axis[1] * sin_theta\n            mat[2, 1] = axis[1] * axis[2] * (1.0 - cos_theta) + axis[0] * sin_theta\n            mat[2, 2] = axis[2] * axis[2] + (1.0 - axis[2] * axis[2]) * cos_theta\n\n            mat[3, 3] = 1.0\n            ret = mat @ ret\n    return ret\n\ndef parse_vector(str):\n    v = np.fromstring(str, dtype=np.float32, sep=\',\')\n    if v.shape[0] != 3:\n        v = np.fromstring(str, dtype=np.float32, sep=\' \')\n    assert(v.ndim == 1)\n    return torch.from_numpy(v)\n\ndef parse_camera(node):\n    fov = torch.tensor([45.0])\n    position = None\n    look_at = None\n    up = None\n    clip_near = 1e-2\n    resolution = [256, 256]\n    for child in node:\n        if \'name\' in child.attrib:\n            if child.attrib[\'name\'] == \'fov\':\n                fov = torch.tensor([float(child.attrib[\'value\'])])\n            elif child.attrib[\'name\'] == \'toWorld\':\n                has_lookat = False\n                for grandchild in child:\n                    if grandchild.tag.lower() == \'lookat\':\n                        has_lookat = True\n                        position = parse_vector(grandchild.attrib[\'origin\'])\n                        look_at = parse_vector(grandchild.attrib[\'target\'])\n                        up = parse_vector(grandchild.attrib[\'up\'])\n                if not has_lookat:\n                    print(\'Unsupported Mitsuba scene format: please use a look at transform\')\n                    assert(False)\n        if child.tag == \'film\':\n            for grandchild in child:\n                if \'name\' in grandchild.attrib:\n                    if grandchild.attrib[\'name\'] == \'width\':\n                        resolution[1] = int(grandchild.attrib[\'value\'])\n                    elif grandchild.attrib[\'name\'] == \'height\':\n                        resolution[0] = int(grandchild.attrib[\'value\'])\n\n    return pyredner.Camera(position     = position,\n                           look_at      = look_at,\n                           up           = up,\n                           fov          = fov,\n                           clip_near    = clip_near,\n                           resolution   = resolution)\n\ndef parse_material(node, device, two_sided = False):\n\n    def parse_material_bitmap(node, scale = None):\n        reflectance_texture = None\n        uv_scale = torch.tensor([1.0, 1.0])\n        for grandchild in node:\n            if grandchild.attrib[\'name\'] == \'filename\':\n                reflectance_texture = pyredner.imread(grandchild.attrib[\'value\'])\n                if scale:\n                    reflectance_texture = reflectance_texture * scale\n            elif grandchild.attrib[\'name\'] == \'uscale\':\n                uv_scale[0] = float(grandchild.attrib[\'value\'])\n            elif grandchild.attrib[\'name\'] == \'vscale\':\n                uv_scale[1] = float(grandchild.attrib[\'value\'])\n        assert reflectance_texture is not None\n        uv_scale = uv_scale.to(device = device)\n        return reflectance_texture, uv_scale\n    \n    # support mitsuba pulgin \'scale\' for texture\n    def parse_texture(node):\n        if node.attrib[\'type\'] == \'scale\':\n            scale_value = None\n            for grandchild in node:\n                if grandchild.attrib[\'name\'] == \'scale\' and grandchild.tag == \'float\':\n                    scale_value = float(grandchild.attrib[\'value\'])\n                elif grandchild.attrib[\'type\'] == \'bitmap\' and grandchild.tag == \'texture\':\n                    assert scale_value is not None # avoid \'scale\' element is declared below the \'bitmap\'\n                    return parse_material_bitmap(grandchild, scale_value)\n                else:\n                    raise NotImplementedError(\'Unsupported scale param type {}\'.format(grandchild.child[\'type\']))\n        elif node.attrib[\'type\'] == \'bitmap\':\n            return parse_material_bitmap(node)\n        else:\n            raise NotImplementedError(\'Unsupported Texture type {}\'.format(node.attrib[\'type\']))\n    \n    node_id = None\n    if \'id\' in node.attrib:\n        node_id = node.attrib[\'id\']\n    if node.attrib[\'type\'] == \'diffuse\':\n        diffuse_reflectance = torch.tensor([0.5, 0.5, 0.5])\n        diffuse_uv_scale = torch.tensor([1.0, 1.0])\n        specular_reflectance = torch.tensor([0.0, 0.0, 0.0])\n        specular_uv_scale = torch.tensor([1.0, 1.0])\n        roughness = torch.tensor([1.0])\n        for child in node:\n            if child.attrib[\'name\'] == \'reflectance\':\n                if child.tag == \'texture\':\n                    diffuse_reflectance, diffuse_uv_scale = parse_texture(child)\n                elif child.tag == \'rgb\' or child.tag == \'spectrum\' or child.tag == \'srgb\':\n                    diffuse_reflectance = parse_vector(child.attrib[\'value\'])\n                    if child.tag == \'srgb\':\n                        diffuse_reflectance = pyredner.srgb_to_linear(diffuse_reflectance)\n            elif child.attrib[\'name\'] == \'specular\':\n                if child.tag == \'texture\':\n                    specular_reflectance, specular_uv_scale = parse_texture(child)\n                elif child.tag == \'rgb\' or child.tag == \'spectrum\' or child.tag == \'srgb\':\n                    specular_reflectance = parse_vector(child.attrib[\'value\'])\n                    if child.tag == \'srgb\':\n                        specular_reflectance = pyredner.srgb_to_linear(specular_reflectance)\n            elif child.attrib[\'name\'] == \'roughness\':\n                roughness = torch.tensor([float(child.attrib[\'value\'])])\n        diffuse_reflectance = diffuse_reflectance.to(device = device)\n        specular_reflectance = specular_reflectance.to(device = device)\n        roughness = roughness.to(device = device)\n        diffuse_uv_scale = diffuse_uv_scale.to(device = device)\n        specular_uv_scale = specular_uv_scale.to(device = device)\n        return (node_id, pyredner.Material(\\\n                diffuse_reflectance = pyredner.Texture(diffuse_reflectance, diffuse_uv_scale),\n                specular_reflectance = pyredner.Texture(specular_reflectance, specular_uv_scale),\n                roughness = pyredner.Texture(roughness),\n                two_sided = two_sided))\n    elif node.attrib[\'type\'] == \'roughplastic\':\n        diffuse_reflectance = torch.tensor([0.5, 0.5, 0.5])\n        diffuse_uv_scale = torch.tensor([1.0, 1.0])\n        specular_reflectance = torch.tensor([1.0, 1.0, 1.0])\n        specular_uv_scale = torch.tensor([1.0, 1.0])\n        roughness = torch.tensor([0.01])\n        roughness_uv_scale = torch.tensor([1.0, 1.0])\n\n        for child in node:\n            if child.attrib[\'name\'] == \'diffuseReflectance\' or child.attrib[\'name\'] == \'diffuse_reflectance\':\n                if child.tag == \'texture\':\n                    diffuse_reflectance, diffuse_uv_scale = parse_texture(child)\n                elif child.tag == \'rgb\' or child.tag == \'spectrum\' or child.tag == \'srgb\':\n                    diffuse_reflectance = parse_vector(child.attrib[\'value\'])\n                    if child.tag == \'srgb\':\n                        diffuse_reflectance = pyredner.srgb_to_linear(diffuse_reflectance)\n            elif child.attrib[\'name\'] == \'specularReflectance\' or child.attrib[\'name\'] == \'specular_reflectance\':\n                if child.tag == \'texture\':\n                    specular_reflectance, specular_uv_scale = parse_texture(child)\n                elif child.tag == \'rgb\' or child.tag == \'spectrum\' or child.tag == \'srgb\':\n                    specular_reflectance = parse_vector(child.attrib[\'value\'])\n                    if child.tag == \'srgb\':\n                        specular_reflectance = pyredner.srgb_to_linear(specular_reflectance)\n            elif child.attrib[\'name\'] == \'alpha\':\n                if child.tag == \'texture\':\n                    roughness, roughness_uv_scale = parse_texture(child)\n                    roughness = roughness * roughness\n                elif child.tag == \'float\':\n                    alpha = float(child.attrib[\'value\'])\n                    roughness = torch.tensor([alpha * alpha])\n        diffuse_reflectance = diffuse_reflectance.to(device = device)\n        specular_reflectance = specular_reflectance.to(device = device)\n        roughness = roughness.to(device = device)\n        diffuse_uv_scale = diffuse_uv_scale.to(device = device)\n        specular_uv_scale = specular_uv_scale.to(device = device)\n        roughness_uv_scale = roughness_uv_scale.to(device = device)\n        return (node_id, pyredner.Material(\\\n                diffuse_reflectance = pyredner.Texture(diffuse_reflectance, diffuse_uv_scale),\n                specular_reflectance = pyredner.Texture(specular_reflectance, specular_uv_scale),\n                roughness = pyredner.Texture(roughness, roughness_uv_scale),\n                two_sided = two_sided))\n    elif node.attrib[\'type\'] == \'twosided\':\n        ret = parse_material(node[0], device, True)\n        return (node_id, ret[1])\n    # Simply bypass mask\'s opacity\n    elif node.attrib[\'type\'] == \'mask\': #TODO add opacity!!!\n        ret = parse_material(node[0], device)\n        return (node_id, ret[1])\n    else:\n        print(\'Unsupported material type:\', node.attrib[\'type\'])\n        assert(False)\n\ndef parse_shape(node, material_dict, shape_id, device, shape_group_dict = None):\n    if node.attrib[\'type\'] == \'obj\' or node.attrib[\'type\'] == \'serialized\':\n        to_world = torch.eye(4)\n        serialized_shape_id = 0\n        mat_id = -1\n        light_intensity = None\n        filename = \'\'\n        max_smooth_angle = -1\n        for child in node:\n            if \'name\' in child.attrib:\n                if child.attrib[\'name\'] == \'filename\':\n                    filename = child.attrib[\'value\']\n                elif child.attrib[\'name\'] == \'toWorld\':\n                    to_world = parse_transform(child)\n                elif child.attrib[\'name\'] == \'shapeIndex\':\n                    serialized_shape_id = int(child.attrib[\'value\'])\n                elif child.attrib[\'name\'] == \'maxSmoothAngle\':\n                    max_smooth_angle = float(child.attrib[\'value\'])\n            if child.tag == \'ref\':\n                mat_id = material_dict[child.attrib[\'id\']]\n            elif child.tag == \'emitter\':\n                for grandchild in child:\n                    if grandchild.attrib[\'name\'] == \'radiance\':\n                        light_intensity = parse_vector(grandchild.attrib[\'value\'])\n                        if light_intensity.shape[0] == 1:\n                            light_intensity = torch.tensor(\\\n                                         [light_intensity[0],\n                                          light_intensity[0],\n                                          light_intensity[0]])\n\n        if node.attrib[\'type\'] == \'obj\':\n            # Load in CPU for rebuild_topology\n            _, mesh_list, _ = pyredner.load_obj(filename, obj_group = False, device = torch.device(\'cpu\'))\n            vertices = mesh_list[0][1].vertices\n            indices = mesh_list[0][1].indices\n            uvs = mesh_list[0][1].uvs\n            normals = mesh_list[0][1].normals\n            uv_indices = mesh_list[0][1].uv_indices\n            normal_indices = mesh_list[0][1].normal_indices\n        else:\n            assert(node.attrib[\'type\'] == \'serialized\')\n            mitsuba_tri_mesh = redner.load_serialized(filename, serialized_shape_id)\n            vertices = torch.from_numpy(mitsuba_tri_mesh.vertices)\n            indices = torch.from_numpy(mitsuba_tri_mesh.indices)\n            uvs = torch.from_numpy(mitsuba_tri_mesh.uvs)\n            normals = torch.from_numpy(mitsuba_tri_mesh.normals)\n            if uvs.shape[0] == 0:\n                uvs = None\n            if normals.shape[0] == 0:\n                normals = None\n            uv_indices = None # Serialized doesn\'t use different indices for UV & normal\n            normal_indices = None\n\n        # Transform the vertices and normals\n        vertices = torch.cat((vertices, torch.ones(vertices.shape[0], 1)), dim = 1)\n        vertices = vertices @ torch.transpose(to_world, 0, 1)\n        vertices = vertices / vertices[:, 3:4]\n        vertices = vertices[:, 0:3].contiguous()\n        if normals is not None:\n            normals = normals @ (torch.inverse(torch.transpose(to_world, 0, 1))[:3, :3])\n            normals = normals.contiguous()\n        assert(vertices is not None)\n        assert(indices is not None)\n        if max_smooth_angle >= 0:\n            if normals is None:\n                normals = torch.zeros_like(vertices)\n            new_num_vertices = redner.rebuild_topology(\\\n                redner.float_ptr(vertices.data_ptr()),\n                redner.int_ptr(indices.data_ptr()),\n                redner.float_ptr(uvs.data_ptr() if uvs is not None else 0),\n                redner.float_ptr(normals.data_ptr() if normals is not None else 0),\n                redner.int_ptr(uv_indices.data_ptr() if uv_indices is not None else 0),\n                int(vertices.shape[0]),\n                int(indices.shape[0]),\n                max_smooth_angle)\n            print(\'Rebuilt topology, original vertices size: {}, new vertices size: {}\'.format(\\\n                int(vertices.shape[0]), new_num_vertices))\n            vertices.resize_(new_num_vertices, 3)\n            if uvs is not None:\n                uvs.resize_(new_num_vertices, 2)\n            if normals is not None:\n                normals.resize_(new_num_vertices, 3)\n\n        lgt = None\n        if light_intensity is not None:\n            lgt = pyredner.AreaLight(shape_id, light_intensity)\n\n        vertices = vertices.to(device)\n        indices = indices.to(device)\n        if uvs is not None:\n            uvs = uvs.to(device)\n        if normals is not None:\n            normals = normals.to(device)\n        if uv_indices is not None:\n            uv_indices = uv_indices.to(device)\n        if normal_indices is not None:\n            normal_indices = normal_indices.to(device)\n        return pyredner.Shape(vertices,\n                              indices,\n                              uvs=uvs,\n                              normals=normals,\n                              uv_indices=uv_indices,\n                              normal_indices=normal_indices,\n                              material_id=mat_id), lgt\n    elif node.attrib[\'type\'] == \'rectangle\':\n        indices = torch.tensor([[0, 2, 1], [1, 2, 3]],\n                               dtype = torch.int32)\n        vertices = torch.tensor([[-1.0, -1.0, 0.0],\n                                 [-1.0,  1.0, 0.0],\n                                 [ 1.0, -1.0, 0.0],\n                                 [ 1.0,  1.0, 0.0]])\n        uvs = None\n        normals = None\n        to_world = torch.eye(4)\n        mat_id = -1\n        light_intensity = None\n        for child in node:\n            if \'name\' in child.attrib:\n                if child.attrib[\'name\'] == \'toWorld\':\n                    to_world = parse_transform(child)\n            if child.tag == \'ref\':\n                mat_id = material_dict[child.attrib[\'id\']]\n            elif child.tag == \'emitter\':\n                for grandchild in child:\n                    if grandchild.attrib[\'name\'] == \'radiance\':\n                        light_intensity = parse_vector(grandchild.attrib[\'value\'])\n                        if light_intensity.shape[0] == 1:\n                            light_intensity = torch.tensor(\\\n                                         [light_intensity[0],\n                                          light_intensity[0],\n                                          light_intensity[0]])\n        # Transform the vertices\n        # Transform the vertices and normals\n        vertices = torch.cat((vertices, torch.ones(vertices.shape[0], 1)), dim = 1)\n        vertices = vertices @ torch.transpose(to_world, 0, 1)\n        vertices = vertices / vertices[:, 3:4]\n        vertices = vertices[:, 0:3].contiguous()\n        if normals is not None:\n            normals = normals @ (torch.inverse(torch.transpose(to_world, 0, 1))[:3, :3])\n            normals = normals.contiguous()\n        assert(vertices is not None)\n        assert(indices is not None)\n        lgt = None\n        if light_intensity is not None:\n            lgt = pyredner.AreaLight(shape_id, light_intensity)\n\n        vertices = vertices.to(device)\n        indices = indices.to(device)\n        if uvs is not None:\n            uvs = uvs.to(device)\n        if normals is not None:\n            normals = normals.to(device)\n        return pyredner.Shape(vertices, indices, uvs=uvs, normals=normals, material_id=mat_id), lgt\n    # Add instance support \n    # TODO (simply transform & create a new shape now)\n    elif node.attrib[\'type\'] == \'instance\':\n        shape = None\n        for child in node:\n            if \'name\' in child.attrib:\n                if child.attrib[\'name\'] == \'toWorld\':\n                    to_world = parse_transform(child)\n            if child.tag == \'ref\':\n                shape = shape_group_dict[child.attrib[\'id\']]\n        # transform instance\n        vertices = shape.vertices\n        normals = shape.normals\n        vector1 = torch.ones(vertices.shape[0], 1, device = vertices.device)\n        to_world = to_world.to(vertices.device)\n        vertices = torch.cat((vertices, vector1), dim = 1)\n        vertices = vertices @ torch.transpose(to_world, 0, 1)\n        vertices = vertices / vertices[:, 3:4]\n        vertices = vertices[:, 0:3].contiguous()\n        if normals is not None:\n            normals = normals @ (torch.inverse(torch.transpose(to_world, 0, 1))[:3, :3])\n            normals = normals.contiguous()\n        # assert(vertices is not None)\n        # assert(indices is not None)\n        # lgt = None\n        # if light_intensity is not None:\n        #     lgt = pyredner.AreaLight(shape_id, light_intensity)\n\n        return pyredner.Shape(vertices, shape.indices, uvs=shape.uvs, normals=normals, material_ids=shape.material_id), None\n    else:\n        print(\'Shape type {} is not supported!\'.format(node.attrib[\'type\']))\n        assert(False)\n\ndef parse_scene(node, device):\n    cam = None\n    resolution = None\n    materials = []\n    material_dict = {}\n    shapes = []\n    lights = []\n    shape_group_dict = {}\n    envmap = None\n\n    for child in node:\n        if child.tag == \'sensor\':\n            cam = parse_camera(child)\n        elif child.tag == \'bsdf\':\n            node_id, material = parse_material(child, device)\n            if node_id is not None:\n                material_dict[node_id] = len(materials)\n                materials.append(material)\n        # shapegroup for instancing\n        elif child.tag == \'shape\' and child.attrib[\'type\'] == \'shapegroup\':\n            for child_s in child:\n                if child_s.tag == \'shape\':\n                    shape_group_dict[child.attrib[\'id\']] = parse_shape(child_s, material_dict, None)[0]\n        elif child.tag == \'shape\':\n            shape, light = parse_shape(child, material_dict, len(shapes), device, shape_group_dict if child.attrib[\'type\'] == \'instance\' else None)\n            shapes.append(shape)\n            if light is not None:\n                lights.append(light)\n        # Add envmap loading support\n        elif child.tag == \'emitter\' and child.attrib[\'type\'] == \'envmap\':\n            # read envmap params from xml\n            scale = 1.0\n            envmap_filename = None\n            to_world = torch.eye(4)\n            for child_s in child:\n                if child_s.attrib[\'name\'] == \'scale\':\n                    assert child_s.tag == \'float\'\n                    scale = float(child_s.attrib[\'value\'])\n                if child_s.attrib[\'name\'] == \'filename\':\n                    assert child_s.tag == \'string\'\n                    envmap_filename = child_s.attrib[\'value\']\n                if child_s.attrib[\'name\'] == \'toWorld\':\n                    to_world = parse_transform(child_s)\n            # load envmap\n            envmap = scale * pyredner.imread(envmap_filename).to(device)\n            envmap = pyredner.EnvironmentMap(envmap, env_to_world=to_world)\n    return pyredner.Scene(cam, shapes, materials, lights, envmap)\n\ndef load_mitsuba(filename: str,\n                 device: Optional[torch.device] = None):\n    """"""\n        Load from a Mitsuba scene file as PyTorch tensors.\n\n        Args\n        ====\n        filename: str\n            Path to the Mitsuba scene file.\n        device: Optional[torch.device]\n            Which device should we store the data in.\n            If set to None, use the device from pyredner.get_device().\n\n        Returns\n        =======\n        pyredner.Scene\n    """"""\n    if device is None:\n        device = pyredner.get_device()\n\n    tree = etree.parse(filename)\n    root = tree.getroot()\n    cwd = os.getcwd()\n    os.chdir(os.path.dirname(filename))\n    ret = parse_scene(root, device)\n    os.chdir(cwd)\n    return ret\n'"
pyredner/load_obj.py,0,"b'import torch\nimport re\nimport pyredner\nimport os\nfrom typing import Optional\n\nclass WavefrontMaterial:\n    def __init__(self):\n        self.name = """"\n        self.Kd = (0.0, 0.0, 0.0)\n        self.Ks = (0.0, 0.0, 0.0)\n        self.Ns = 0.0\n        self.Ke = (0.0, 0.0, 0.0)\n        self.map_Kd = None\n        self.map_Ks = None\n        self.map_Ns = None\n\nclass TriangleMesh:\n    def __init__(self,\n                 indices,\n                 uv_indices,\n                 normal_indices,\n                 vertices,\n                 uvs,\n                 normals):\n        self.vertices = vertices\n        self.indices = indices\n        self.uv_indices = uv_indices\n        self.normal_indices = normal_indices\n        self.uvs = uvs\n        self.normals = normals\n\ndef load_mtl(filename):\n    mtllib = {}\n    current_mtl = WavefrontMaterial()\n    for line in open(filename, \'r\'):\n        line = line.strip()\n        splitted = re.split(\'\\ +\', line)\n        if splitted[0] == \'newmtl\':\n            if current_mtl.name != """":\n                mtllib[current_mtl.name] = current_mtl\n            current_mtl = WavefrontMaterial()\n            current_mtl.name = splitted[1]\n        elif splitted[0] == \'Kd\':\n            current_mtl.Kd = (float(splitted[1]), float(splitted[2]), float(splitted[3]))\n        elif splitted[0] == \'Ks\':\n            current_mtl.Ks = (float(splitted[1]), float(splitted[2]), float(splitted[3]))\n        elif splitted[0] == \'Ns\':\n            current_mtl.Ns = float(splitted[1])\n        elif splitted[0] == \'Ke\':\n            current_mtl.Ke = (float(splitted[1]), float(splitted[2]), float(splitted[3]))\n        elif splitted[0] == \'map_Kd\':\n            current_mtl.map_Kd = splitted[1]\n        elif splitted[0] == \'map_Ks\':\n            current_mtl.map_Ks = splitted[1]\n        elif splitted[0] == \'map_Ns\':\n            current_mtl.map_Ns = splitted[1]\n    if current_mtl.name != """":\n        mtllib[current_mtl.name] = current_mtl\n    return mtllib\n\n# This is slow, maybe move to C++?\ndef load_obj(filename: str,\n             obj_group: bool = True,\n             flip_tex_coords: bool = True,\n             use_common_indices: bool = False,\n             return_objects: bool = False,\n             device: Optional[torch.device] = None):\n    """"""\n        Load from a Wavefront obj file as PyTorch tensors.\n\n        Args\n        ====\n        filename: str\n            Path to the obj file.\n        obj_group: bool\n            Split the meshes based on materials.\n        flip_tex_coords: bool\n            Flip the v coordinate of uv by applying v\' = 1 - v.\n        use_common_indices: bool\n            Use the same indices for position, uvs, normals.\n            Not recommended since texture seams in the objects sharing.\n            The same positions would cause the optimization to ""tear"" the object.\n        return_objects: bool\n            Output list of Object instead.\n            If there is no corresponding material for a shape, assign a grey material.\n        device: Optional[torch.device]\n            Which device should we store the data in.\n            If set to None, use the device from pyredner.get_device().\n\n        Returns\n        =======\n        if return_objects == True, return a list of Object\n        if return_objects == False, return (material_map, mesh_list, light_map),\n        material_map -> Map[mtl_name, WavefrontMaterial]\n        mesh_list -> List[TriangleMesh]\n        light_map -> Map[mtl_name, torch.Tensor]\n    """"""\n    if device is None:\n        device = pyredner.get_device()\n\n    vertices_pool = []\n    uvs_pool = []\n    normals_pool = []\n    indices = []\n    uv_indices = []\n    normal_indices = []\n    vertices = []\n    uvs = []\n    normals = []\n    vertices_map = {}\n    uvs_map = {}\n    normals_map = {}\n    material_map = {}\n    current_mtllib = {}\n    current_material_name = None\n\n    def create_mesh(indices,\n                    uv_indices,\n                    normal_indices,\n                    vertices,\n                    uvs,\n                    normals):\n        indices = torch.tensor(indices, dtype = torch.int32, device = device)\n        if len(uv_indices) == 0:\n            uv_indices = None\n        else:\n            uv_indices = torch.tensor(uv_indices, dtype = torch.int32, device = device)\n        if len(normal_indices) == 0:\n            normal_indices = None\n        else:\n            normal_indices = torch.tensor(normal_indices, dtype = torch.int32, device = device)\n        vertices = torch.tensor(vertices, device = device)\n        if len(uvs) == 0:\n            uvs = None\n        else:\n            uvs = torch.tensor(uvs, device = device)\n        if len(normals) == 0:\n            normals = None\n        else:\n            normals = torch.tensor(normals, device = device)\n        return TriangleMesh(indices,\n                            uv_indices,\n                            normal_indices,\n                            vertices,\n                            uvs,\n                            normals)\n\n    mesh_list = []\n    light_map = {}\n\n    with open(filename, \'r\') as f:\n        d = os.path.dirname(filename)\n        cwd = os.getcwd()\n        if d != \'\':\n            os.chdir(d)\n        for line in f:\n            line = line.strip()\n            splitted = re.split(\'\\ +\', line)\n            if splitted[0] == \'mtllib\':\n                current_mtllib = load_mtl(splitted[1])\n            elif splitted[0] == \'usemtl\':\n                if len(indices) > 0 and obj_group is True:\n                    # Flush\n                    mesh_list.append((current_material_name,\n                        create_mesh(indices, uv_indices, normal_indices,\n                                    vertices, uvs, normals)))\n                    indices = []\n                    uv_indices = []\n                    normal_indices = []\n                    vertices = []\n                    normals = []\n                    uvs = []\n                    vertices_map = {}\n                    uvs_map = {}\n                    normals_map = {}\n\n                mtl_name = splitted[1]\n                current_material_name = mtl_name\n                if mtl_name not in material_map:\n                    m = current_mtllib[mtl_name]\n                    if m.map_Kd is None:\n                        diffuse_reflectance = torch.tensor(m.Kd,\n                            dtype = torch.float32, device = device)\n                    else:\n                        diffuse_reflectance = pyredner.imread(m.map_Kd).to(device)\n                    if m.map_Ks is None:\n                        specular_reflectance = torch.tensor(m.Ks,\n                            dtype = torch.float32, device = device)\n                    else:\n                        specular_reflectance = pyredner.imread(m.map_Ks).to(device)\n                    if m.map_Ns is None:\n                        roughness = torch.tensor([2.0 / (m.Ns + 2.0)],\n                            dtype = torch.float32, device = device)\n                    else:\n                        roughness = 2.0 / (pyredner.imread(m.map_Ks) + 2.0)\n                        roughness = roughness.to(device)\n                    if m.Ke != (0.0, 0.0, 0.0):\n                        light_map[mtl_name] = torch.tensor(m.Ke, dtype = torch.float32)\n                    material_map[mtl_name] = pyredner.Material(\\\n                        diffuse_reflectance, specular_reflectance, roughness)\n            elif splitted[0] == \'v\':\n                vertices_pool.append([float(splitted[1]), float(splitted[2]), float(splitted[3])])\n            elif splitted[0] == \'vt\':\n                u = float(splitted[1])\n                v = float(splitted[2])\n                if flip_tex_coords:\n                    v = 1 - v\n                uvs_pool.append([u, v])\n            elif splitted[0] == \'vn\':\n                normals_pool.append([float(splitted[1]), float(splitted[2]), float(splitted[3])])\n            elif splitted[0] == \'f\':\n                def num_indices(x):\n                    return len(re.split(\'/\', x))\n                def get_index(x, i):\n                    return int(re.split(\'/\', x)[i])\n                def parse_face_index(x, i):\n                    f = get_index(x, i)\n                    if f > 0:\n                        f -= 1\n                    return f\n                assert(len(splitted) <= 5)\n                def get_vertex_id(indices):\n                    pi = parse_face_index(indices, 0)\n                    uvi = None\n                    if (num_indices(indices) > 1 and re.split(\'/\', indices)[1] != \'\'):\n                        uvi = parse_face_index(indices, 1)\n                    ni = None\n                    if (num_indices(indices) > 2 and re.split(\'/\', indices)[2] != \'\'):\n                        ni = parse_face_index(indices, 2)\n                    if use_common_indices:\n                        # vertex, uv, normals share the same indexing\n                        key = (pi, uvi, ni)\n                        if key in vertices_map:\n                            vertex_id = vertices_map[key]\n                            return vertex_id, vertex_id, vertex_id\n\n                        vertex_id = len(vertices)\n                        vertices_map[key] = vertex_id\n                        vertices.append(vertices_pool[pi])\n                        if uvi is not None:\n                            uvs.append(uvs_pool[uvi])\n                        if ni is not None:\n                            normals.append(normals_pool[ni])\n                        return vertex_id, vertex_id, vertex_id\n                    else:\n                        # vertex, uv, normals use separate indexing\n                        vertex_id = None\n                        uv_id = None\n                        normal_id = None\n\n                        if pi in vertices_map:\n                            vertex_id = vertices_map[pi]\n                        else:\n                            vertex_id = len(vertices)\n                            vertices.append(vertices_pool[pi])\n                            vertices_map[pi] = vertex_id\n\n                        if uvi is not None:\n                            if uvi in uvs_map:\n                                uv_id = uvs_map[uvi]\n                            else:\n                                uv_id = len(uvs)\n                                uvs.append(uvs_pool[uvi])\n                                uvs_map[uvi] = uv_id\n\n                        if ni is not None:\n                            if ni in normals_map:\n                                normal_id = normals_map[ni]\n                            else:\n                                normal_id = len(normals)\n                                normals.append(normals_pool[ni])\n                                normals_map[ni] = normal_id\n                        return vertex_id, uv_id, normal_id\n\n                vid0, uv_id0, n_id0 = get_vertex_id(splitted[1])\n                vid1, uv_id1, n_id1 = get_vertex_id(splitted[2])\n                vid2, uv_id2, n_id2 = get_vertex_id(splitted[3])\n\n                indices.append([vid0, vid1, vid2])\n                if uv_id0 is not None:\n                    assert(uv_id1 is not None and uv_id2 is not None)\n                    uv_indices.append([uv_id0, uv_id1, uv_id2])\n                if n_id0 is not None:\n                    assert(n_id1 is not None and n_id2 is not None)\n                    normal_indices.append([n_id0, n_id1, n_id2])\n                if (len(splitted) == 5):\n                    vid3, uv_id3, n_id3 = get_vertex_id(splitted[4])\n                    indices.append([vid0, vid2, vid3])\n                    if uv_id0 is not None:\n                        assert(uv_id3 is not None)\n                        uv_indices.append([uv_id0, uv_id2, uv_id3])\n                    if n_id0 is not None:\n                        assert(n_id3 is not None)\n                        normal_indices.append([n_id0, n_id2, n_id3])\n\n    mesh_list.append((current_material_name,\n        create_mesh(indices, uv_indices, normal_indices, vertices, uvs, normals)))\n    if d != \'\':\n        os.chdir(cwd)\n\n    if return_objects:\n        objects = []\n        for mtl_name, mesh in mesh_list:\n            if mtl_name in material_map:\n                m = material_map[mtl_name]\n            else:\n                m = pyredner.Material(diffuse_reflectance = \\\n                    torch.tensor((0.5, 0.5, 0.5), device = device))\n            if mtl_name in light_map:\n                l = light_map[mtl_name]\n            else:\n                l = None\n            objects.append(pyredner.Object(\\\n                vertices = mesh.vertices,\n                indices = mesh.indices,\n                material = m,\n                light_intensity = l,\n                uvs = mesh.uvs,\n                normals = mesh.normals,\n                uv_indices = mesh.uv_indices,\n                normal_indices = mesh.normal_indices))\n        return objects\n    else:\n        return material_map, mesh_list, light_map\n'"
pyredner/material.py,0,"b'import pyredner\nimport torch\nfrom typing import Union, Optional\n\nclass Material:\n    """"""\n        redner currently employs a two-layer diffuse-specular material model.\n        More specifically, it is a linear blend between a Lambertian model and\n        a microfacet model with Phong distribution, with Schilick\'s Fresnel approximation.\n        It takes either constant color or 2D textures for the reflectances\n        and roughness, and an optional normal map texture.\n        It can also use vertex color stored in the Shape. In this case\n        the model fallback to a diffuse model.\n\n        Args\n        ====\n        diffuse_reflectance: Optional[Union[torch.Tensor, pyredner.Texture]]\n            A float32 tensor with size 3 or [height, width, 3] or a Texture.\n            Optional if use_vertex_color is True.\n        specular_reflectance: Optional[Union[torch.Tensor, pyredner.Texture]]\n            A float32 tensor with size 3 or [height, width, 3] or a Texture.\n        roughness: Optional[Union[torch.Tensor, pyredner.Texture]]\n            A float32 tensor with size 1 or [height, width, 1] or a Texture.\n        generic_texture: Optional[Union[torch.Tensor, pyredner.Texture]]\n            A float32 tensor with dimension 1 or 3, arbitrary number of channels\n            use render_g_buffer to visualize this texture.\n        normal_map: Optional[Union[torch.Tensor, pyredner.Texture]]\n            A float32 tensor with size 3 or [height, width, 3] or a Texture.\n        two_sided: bool\n            By default, the material only reflect lights on the side the\n            normal is pointing to.\n            Set this to True to make the material reflects from both sides.\n        use_vertex_color: bool\n            Ignores the reflectances and use the vertex color as diffuse color\n    """"""\n    def __init__(self,\n                 diffuse_reflectance: Optional[Union[torch.Tensor, pyredner.Texture]] = None,\n                 specular_reflectance: Optional[Union[torch.Tensor, pyredner.Texture]] = None,\n                 roughness: Optional[Union[torch.Tensor, pyredner.Texture]] = None,\n                 generic_texture: Optional[Union[torch.Tensor, pyredner.Texture]] = None,\n                 normal_map: Optional[Union[torch.Tensor, pyredner.Texture]] = None,\n                 two_sided: bool = False,\n                 use_vertex_color: bool = False):\n        # Search for device\n        device = None\n        if diffuse_reflectance is not None:\n            device = diffuse_reflectance.device\n        if device is None and specular_reflectance is not None:\n            device = specular_reflectance.device\n        if device is None and roughness is not None:\n            device = roughness.device\n        if device is None and generic_texture is not None:\n            device = generic_texture.device\n        if device is None and normal_map is not None:\n            device = normal_map.device\n        self.device = device\n\n        if diffuse_reflectance is None:\n            diffuse_reflectance = pyredner.Texture(\\\n                torch.zeros(3, device = device))\n        if specular_reflectance is None:\n            specular_reflectance = pyredner.Texture(\\\n                torch.zeros(3, device = device))\n            compute_specular_lighting = False\n        else:\n            compute_specular_lighting = True\n        if roughness is None:\n            roughness = pyredner.Texture(\\\n                torch.tensor([1.0], device = device))\n\n        # Convert to constant texture if necessary\n        if isinstance(diffuse_reflectance, torch.Tensor):\n            diffuse_reflectance = pyredner.Texture(diffuse_reflectance)\n        if isinstance(specular_reflectance, torch.Tensor):\n            specular_reflectance = pyredner.Texture(specular_reflectance)\n        if isinstance(roughness, torch.Tensor):\n            roughness = pyredner.Texture(roughness)\n        if generic_texture is not None and isinstance(generic_texture, torch.Tensor):\n            generic_texture = pyredner.Texture(generic_texture)\n        if normal_map is not None and isinstance(normal_map, torch.Tensor):\n            normal_map = pyredner.Texture(normal_map)\n\n        assert((len(diffuse_reflectance.texels.shape) == 1 and diffuse_reflectance.texels.shape[0] == 3) or \\\n               (len(diffuse_reflectance.texels.shape) == 3 and diffuse_reflectance.texels.shape[2] == 3))\n        assert((len(specular_reflectance.texels.shape) == 1 and specular_reflectance.texels.shape[0] == 3) or \\\n               (len(specular_reflectance.texels.shape) == 3 and specular_reflectance.texels.shape[2] == 3))\n        assert((len(roughness.texels.shape) == 1 and roughness.texels.shape[0] == 1) or \\\n               (len(roughness.texels.shape) == 3 and roughness.texels.shape[2] == 1))\n        if normal_map is not None:\n            assert((len(normal_map.texels.shape) == 1 and normal_map.texels.shape[0] == 3) or \\\n                   (len(normal_map.texels.shape) == 3 and normal_map.texels.shape[2] == 3))\n\n        self.diffuse_reflectance = diffuse_reflectance\n        self._specular_reflectance = specular_reflectance\n        self.compute_specular_lighting = compute_specular_lighting\n        self.roughness = roughness\n        self.generic_texture = generic_texture\n        self.normal_map = normal_map\n        self.two_sided = two_sided\n        self.use_vertex_color = use_vertex_color\n\n    @property\n    def specular_reflectance(self):\n        return self._specular_reflectance\n\n    @specular_reflectance.setter\n    def specular_reflectance(self, value):\n        self._specular_reflectance = value\n        if value is not None:\n            self.compute_specular_lighting = True\n        else:\n            self._specular_reflectance = pyredner.Texture(\\\n                torch.zeros(3, device = self.device))\n            self.compute_specular_lighting = False\n\n    def state_dict(self):\n        return {\n            \'diffuse_reflectance\': self.diffuse_reflectance.state_dict(),\n            \'specular_reflectance\': self.specular_reflectance.state_dict(),\n            \'roughness\': self.roughness.state_dict(),\n            \'generic_texture\': self.generic_texture.state_dict(),\n            \'normal_map\': self.normal_map.state_dict() if self.normal_map is not None else None,\n            \'two_sided\': self.two_sided,\n            \'use_vertex_color\': self.use_vertex_color\n        }\n\n    @classmethod\n    def load_state_dict(cls, state_dict):\n        normal_map = state_dict[\'normal_map\']\n        out = cls(\n            pyredner.Texture.load_state_dict(state_dict[\'diffuse_reflectance\']),\n            pyredner.Texture.load_state_dict(state_dict[\'specular_reflectance\']),\n            pyredner.Texture.load_state_dict(state_dict[\'roughness\']),\n            pyredner.Texture.load_state_dict(state_dict[\'generic_texture\']),\n            pyredner.Texture.load_state_dict(normal_map) if normal_map is not None else None,\n            state_dict[\'two_sided\'],\n            state_dict[\'use_vertex_color\'])\n        return out\n'"
pyredner/object.py,0,"b'import pyredner\nimport torch\nfrom typing import Optional\n\nclass Object:\n    """"""\n        Object combines geometry, material, and lighting information\n        and aggregate them in a single class. This is a convinent class\n        for constructing redner scenes.\n\n        redner supports only triangle meshes for now. It stores a pool of\n        vertices and access the pool using integer index. Some times the\n        two vertices can have the same 3D position but different texture\n        coordinates, because UV mapping creates seams and need to duplicate\n        vertices. In this can we can use an additional ""uv_indices"" array\n        to access the uv pool.\n\n        Args\n        ====\n        vertices: torch.Tensor\n            3D position of vertices\n            float32 tensor with size num_vertices x 3\n        indices: torch.Tensor\n            vertex indices of triangle faces.\n            int32 tensor with size num_triangles x 3\n        material: pyredner.Material\n\n        light_intensity: Optional[torch.Tensor]\n            make this object an area light\n            float32 tensor with size 3\n        light_two_sided: boolean\n            Does the light emit from two sides of the shape?\n        uvs: Optional[torch.Tensor]:\n            optional texture coordinates.\n            float32 tensor with size num_uvs x 2\n            doesn\'t need to be the same size with vertices if uv_indices is None\n        normals: Optional[torch.Tensor]\n            shading normal\n            float32 tensor with size num_normals x 3\n            doesn\'t need to be the same size with vertices if normal_indices is None\n        uv_indices: Optional[torch.Tensor]\n            overrides indices when accessing uv coordinates\n            int32 tensor with size num_uvs x 2\n        normal_indices: Optional[torch.Tensor]\n            overrides indices when accessing shading normals\n            int32 tensor with size num_normals x 2\n        colors: Optional[torch.Tensor]\n            optional per-vertex color\n            float32 tensor with size num_vertices x 3\n    """"""\n    def __init__(self,\n                 vertices: torch.Tensor,\n                 indices: torch.Tensor,\n                 material: pyredner.Material,\n                 light_intensity: Optional[torch.Tensor] = None,\n                 light_two_sided: bool = False,\n                 uvs: Optional[torch.Tensor] = None,\n                 normals: Optional[torch.Tensor] = None,\n                 uv_indices: Optional[torch.Tensor] = None,\n                 normal_indices: Optional[torch.Tensor] = None,\n                 colors: Optional[torch.Tensor] = None):\n        self.vertices = vertices\n        self.indices = indices\n        self.uvs = uvs\n        self.normals = normals\n        self.uv_indices = uv_indices\n        self.normal_indices = normal_indices\n        self.colors = colors\n        self.material = material\n        self.light_intensity = light_intensity\n        self.light_two_sided = light_two_sided\n'"
pyredner/render_pytorch.py,0,"b'import torch\nimport numpy as np\nimport redner\nimport pyredner\nimport time\nimport skimage.io\nfrom typing import List, Union, Tuple, Optional\nimport warnings\n\nuse_correlated_random_number = False\ndef set_use_correlated_random_number(v: bool):\n    """"""\n        | There is a bias-variance trade off in the backward pass.\n        | If the forward pass and the backward pass are correlated\n        | the gradients are biased for L2 loss.\n        | E[d/dx(f(x) - y)^2] = E[(f(x) - y) d/dx f(x)]\n        |                     = E[f(x) - y] E[d/dx f(x)]\n        | The last equation only holds when f(x) and d/dx f(x) are independent.\n        | It is usually better to use the unbiased one, but we left it as an option here\n    """"""\n    global use_correlated_random_number\n    use_correlated_random_number = v\n\ndef get_use_correlated_random_number():\n    """"""\n        See set_use_correlated_random_number\n    """"""\n    global use_correlated_random_number\n    return use_correlated_random_number\n\nprint_timing = True\ndef set_print_timing(v: bool):\n    """"""\n        Set whether to print time measurements or not.\n    """"""\n    global print_timing\n    print_timing = v\n\ndef get_print_timing():\n    """"""\n        Get whether we print time measurements or not.\n    """"""\n    global print_timing\n    return print_timing\n\ndef serialize_texture(texture, args, device):\n    if texture is None:\n        args.append(0)\n        return\n    args.append(len(texture.mipmap))\n    for mipmap in texture.mipmap:\n        assert(torch.isfinite(mipmap).all())\n        assert(mipmap.is_contiguous())\n        if mipmap.device != device:\n            warnings.warn(\'Converting texture from {} to {}, this can be inefficient.\'.format(mipmap.device, device))\n        args.append(mipmap.to(device))\n    assert(torch.isfinite(texture.uv_scale).all())\n    args.append(texture.uv_scale.to(device))\n\nclass Context: pass\n\nclass RenderFunction(torch.autograd.Function):\n    """"""\n        The PyTorch interface of C++ redner.\n    """"""\n\n    @staticmethod\n    def serialize_scene(scene: pyredner.Scene,\n                        num_samples: Union[int, Tuple[int, int]],\n                        max_bounces: int,\n                        channels: List = [redner.channels.radiance],\n                        sampler_type = redner.SamplerType.independent,\n                        use_primary_edge_sampling: bool = True,\n                        use_secondary_edge_sampling: bool = True,\n                        sample_pixel_center: bool = False,\n                        device: Optional[torch.device] = None):\n        """"""\n            Given a pyredner scene & rendering options, convert them to a linear list of argument,\n            so that we can use it in PyTorch.\n\n            Args\n            ====\n            scene: pyredner.Scene\n            num_samples: int\n                Number of samples per pixel for forward and backward passes.\n                Can be an integer or a tuple of 2 integers.\n                If a single integer is provided, use the same number of samples\n                for both.\n            max_bounces: int\n                Number of bounces for global illumination,\n                1 means direct lighting only.\n            channels: List[redner.channels]\n                | A list of channels that should present in the output image\n                | following channels are supported\\:\n                | redner.channels.radiance,\n                | redner.channels.alpha,\n                | redner.channels.depth,\n                | redner.channels.position,\n                | redner.channels.geometry_normal,\n                | redner.channels.shading_normal,\n                | redner.channels.uv,\n                | redner.channels.barycentric_coordinates,\n                | redner.channels.diffuse_reflectance,\n                | redner.channels.specular_reflectance,\n                | redner.channels.vertex_color,\n                | redner.channels.roughness,\n                | redner.channels.generic_texture,\n                | redner.channels.shape_id,\n                | redner.channels.triangle_id,\n                | redner.channels.material_id\n                | all channels, except for shape id, triangle id, and material id, are differentiable\n            sampler_type: redner.SamplerType\n                | Which sampling pattern to use?\n                | see `Chapter 7 of the PBRT book <http://www.pbr-book.org/3ed-2018/Sampling_and_Reconstruction.html>`\n                  for an explanation of the difference between different samplers.\n                | Following samplers are supported:\n                | redner.SamplerType.independent\n                | redner.SamplerType.sobol\n            use_primary_edge_sampling: bool\n\n            use_secondary_edge_sampling: bool\n\n            sample_pixel_center: bool\n                Always sample at the pixel center when rendering.\n                This trades noise with aliasing.\n                If this option is activated, the rendering becomes non-differentiable\n                (since there is no antialiasing integral),\n                and redner\'s edge sampling becomes an approximation to the gradients of the aliased rendering.\n\n            device: Optional[torch.device]\n                Which device should we store the data in.\n                If set to None, use the device from pyredner.get_device().\n        """"""\n        if device is None:\n            device = pyredner.get_device()\n        if device.index is None and device.type == \'cuda\':\n            # Assign a default index so that we can avoid generating warnings\n            device = torch.device(\'cuda:\' + str(torch.cuda.current_device()))\n\n        # Record if there is any parameter that requires gradient need discontinuity sampling.\n        # For skipping edge sampling when it is not necessary.\n        requires_visibility_grad = False\n        cam = scene.camera\n        num_shapes = len(scene.shapes)\n        num_materials = len(scene.materials)\n        num_lights = len(scene.area_lights)\n        for light_id, light in enumerate(scene.area_lights):\n            scene.shapes[light.shape_id].light_id = light_id\n\n        if max_bounces == 0:\n            use_secondary_edge_sampling = False\n\n        args = []\n        args.append(num_shapes)\n        args.append(num_materials)\n        args.append(num_lights)\n        assert(cam.position is None or torch.isfinite(cam.position).all())\n        assert(cam.look_at is None or torch.isfinite(cam.look_at).all())\n        assert(cam.up is None or torch.isfinite(cam.up).all())\n        assert(torch.isfinite(cam.intrinsic_mat_inv).all())\n        assert(torch.isfinite(cam.intrinsic_mat).all())\n        if cam.position is not None and cam.position.requires_grad:\n            requires_visibility_grad = True\n        if cam.look_at is not None and cam.look_at.requires_grad:\n            requires_visibility_grad = True\n        if cam.up is not None and cam.up.requires_grad:\n            requires_visibility_grad = True\n        args.append(cam.position.cpu() if cam.position is not None else None)\n        args.append(cam.look_at.cpu() if cam.look_at is not None else None)\n        args.append(cam.up.cpu() if cam.up is not None else None)\n        if cam.cam_to_world is not None:\n            if cam.cam_to_world.requires_grad:\n                requires_visibility_grad = True\n            args.append(cam.cam_to_world.cpu().contiguous())\n        else:\n            args.append(None)\n        if cam.world_to_cam is not None:\n            if cam.world_to_cam.requires_grad:\n                requires_visibility_grad = True\n            args.append(cam.world_to_cam.cpu().contiguous())\n        else:\n            args.append(None)\n        if cam.intrinsic_mat.requires_grad or cam.intrinsic_mat_inv.requires_grad:\n            requires_visibility_grad = True\n        args.append(cam.intrinsic_mat_inv.cpu().contiguous())\n        args.append(cam.intrinsic_mat.cpu().contiguous())\n        if cam.distortion_params is not None:\n            if cam.distortion_params.requires_grad:\n                requires_visibility_grad = True\n            args.append(cam.distortion_params.cpu().contiguous())\n        else:\n            args.append(None)\n        args.append(cam.clip_near)\n        args.append(cam.resolution)\n        viewport = cam.viewport\n        if viewport is None:\n            viewport = (0, 0, cam.resolution[0], cam.resolution[1])\n        # Clamp the viewport if necessary\n        viewport = (max(viewport[0], 0),\n                    max(viewport[1], 0),\n                    min(viewport[2], cam.resolution[0]),\n                    min(viewport[3], cam.resolution[1]))\n        args.append(viewport)\n        args.append(cam.camera_type)\n        for shape in scene.shapes:\n            assert(torch.isfinite(shape.vertices).all())\n            if (shape.uvs is not None):\n                assert(torch.isfinite(shape.uvs).all())\n            if (shape.normals is not None):\n                assert(torch.isfinite(shape.normals).all())\n            if (shape.vertices.requires_grad):\n                requires_visibility_grad = True\n            if shape.vertices.device != device:\n                warnings.warn(\'Converting shape vertices from {} to {}, this can be inefficient.\'.format(shape.vertices.device, device))\n            if shape.indices.device != device:\n                warnings.warn(\'Converting shape indices from {} to {}, this can be inefficient.\'.format(shape.indices.device, device))\n            args.append(shape.vertices.to(device))\n            args.append(shape.indices.to(device))\n            args.append(shape.uvs.to(device) if shape.uvs is not None else None)\n            args.append(shape.normals.to(device) if shape.normals is not None else None)\n            args.append(shape.uv_indices.to(device) if shape.uv_indices is not None else None)\n            args.append(shape.normal_indices.to(device) if shape.normal_indices is not None else None)\n            args.append(shape.colors.to(device) if shape.colors is not None else None)\n            args.append(shape.material_id)\n            args.append(shape.light_id)\n        for material in scene.materials:\n            serialize_texture(material.diffuse_reflectance, args, device)\n            serialize_texture(material.specular_reflectance, args, device)\n            serialize_texture(material.roughness, args, device)\n            serialize_texture(material.generic_texture, args, device)\n            serialize_texture(material.normal_map, args, device)\n            args.append(material.compute_specular_lighting)\n            args.append(material.two_sided)\n            args.append(material.use_vertex_color)\n        for light in scene.area_lights:\n            args.append(light.shape_id)\n            args.append(light.intensity.cpu())\n            args.append(light.two_sided)\n            args.append(light.directly_visible)\n        if scene.envmap is not None:\n            assert(torch.isfinite(scene.envmap.env_to_world).all())\n            assert(torch.isfinite(scene.envmap.world_to_env).all())\n            assert(torch.isfinite(scene.envmap.sample_cdf_ys).all())\n            assert(torch.isfinite(scene.envmap.sample_cdf_xs).all())\n            serialize_texture(scene.envmap.values, args, device)\n            args.append(scene.envmap.env_to_world.cpu())\n            args.append(scene.envmap.world_to_env.cpu())\n            args.append(scene.envmap.sample_cdf_ys.to(device))\n            args.append(scene.envmap.sample_cdf_xs.to(device))\n            args.append(scene.envmap.pdf_norm)\n            args.append(scene.envmap.directly_visible)\n        else:\n            args.append(None)\n\n        args.append(num_samples)\n        args.append(max_bounces)\n        args.append(channels)\n        args.append(sampler_type)\n        if requires_visibility_grad:\n            args.append(use_primary_edge_sampling)\n            args.append(use_secondary_edge_sampling)\n        else:\n            # Don\'t need to do edge sampling if we don\'t require spatial derivatives\n            args.append(False)\n            args.append(False)\n        args.append(sample_pixel_center)\n        args.append(device)\n\n        return args\n\n    @staticmethod\n    def unpack_args(seed,\n                    args,\n                    use_primary_edge_sampling = None,\n                    use_secondary_edge_sampling = None):\n        """"""\n            Given a list of serialized scene arguments, unpack\n            all information into a Context.\n        """"""\n\n        current_index = 0\n        num_shapes = args[current_index]\n        current_index += 1\n        num_materials = args[current_index]\n        current_index += 1\n        num_lights = args[current_index]\n        current_index += 1\n\n        cam_position = args[current_index]\n        current_index += 1\n        cam_look_at = args[current_index]\n        current_index += 1\n        cam_up = args[current_index]\n        current_index += 1\n        cam_to_world = args[current_index]\n        current_index += 1\n        world_to_cam = args[current_index]\n        current_index += 1\n        intrinsic_mat_inv = args[current_index]\n        current_index += 1\n        intrinsic_mat = args[current_index]\n        current_index += 1\n        distortion_params = args[current_index]\n        current_index += 1\n        clip_near = args[current_index]\n        current_index += 1\n        resolution = args[current_index]\n        current_index += 1\n        viewport = args[current_index]\n        current_index += 1\n        camera_type = args[current_index]\n        current_index += 1\n        if cam_to_world is None:\n            camera = redner.Camera(resolution[1],\n                                   resolution[0],\n                                   redner.float_ptr(cam_position.data_ptr()),\n                                   redner.float_ptr(cam_look_at.data_ptr()),\n                                   redner.float_ptr(cam_up.data_ptr()),\n                                   redner.float_ptr(0), # cam_to_world\n                                   redner.float_ptr(0), # world_to_cam\n                                   redner.float_ptr(intrinsic_mat_inv.data_ptr()),\n                                   redner.float_ptr(intrinsic_mat.data_ptr()),\n                                   redner.float_ptr(distortion_params.data_ptr() if distortion_params is not None else 0),\n                                   clip_near,\n                                   camera_type,\n                                   redner.Vector2i(viewport[1], viewport[0]),\n                                   redner.Vector2i(viewport[3], viewport[2]))\n        else:\n            camera = redner.Camera(resolution[1],\n                                   resolution[0],\n                                   redner.float_ptr(0), # cam_position\n                                   redner.float_ptr(0), # cam_look_at\n                                   redner.float_ptr(0), # cam_up\n                                   redner.float_ptr(cam_to_world.data_ptr()),\n                                   redner.float_ptr(world_to_cam.data_ptr()),\n                                   redner.float_ptr(intrinsic_mat_inv.data_ptr()),\n                                   redner.float_ptr(intrinsic_mat.data_ptr()),\n                                   redner.float_ptr(distortion_params.data_ptr() if distortion_params is not None else 0),\n                                   clip_near,\n                                   camera_type,\n                                   redner.Vector2i(viewport[1], viewport[0]),\n                                   redner.Vector2i(viewport[3], viewport[2]))\n        shapes = []\n        for i in range(num_shapes):\n            vertices = args[current_index]\n            current_index += 1\n            indices = args[current_index]\n            current_index += 1\n            uvs = args[current_index]\n            current_index += 1\n            normals = args[current_index]\n            current_index += 1\n            uv_indices = args[current_index]\n            current_index += 1\n            normal_indices = args[current_index]\n            current_index += 1\n            colors = args[current_index]\n            current_index += 1\n            material_id = args[current_index]\n            current_index += 1\n            light_id = args[current_index]\n            current_index += 1\n            assert(vertices.is_contiguous())\n            assert(indices.is_contiguous())\n            if uvs is not None:\n                assert(uvs.is_contiguous())\n            if normals is not None:\n                assert(normals.is_contiguous())\n            if uv_indices is not None:\n                assert(uv_indices.is_contiguous())\n            if normal_indices is not None:\n                assert(normal_indices.is_contiguous())\n            shapes.append(redner.Shape(\\\n                redner.float_ptr(vertices.data_ptr()),\n                redner.int_ptr(indices.data_ptr()),\n                redner.float_ptr(uvs.data_ptr() if uvs is not None else 0),\n                redner.float_ptr(normals.data_ptr() if normals is not None else 0),\n                redner.int_ptr(uv_indices.data_ptr() if uv_indices is not None else 0),\n                redner.int_ptr(normal_indices.data_ptr() if normal_indices is not None else 0),\n                redner.float_ptr(colors.data_ptr() if colors is not None else 0),\n                int(vertices.shape[0]),\n                int(uvs.shape[0]) if uvs is not None else 0,\n                int(normals.shape[0]) if normals is not None else 0,\n                int(indices.shape[0]),\n                material_id,\n                light_id))\n\n        materials = []\n        for i in range(num_materials):\n            num_levels = args[current_index]\n            current_index += 1\n            diffuse_reflectance = []\n            for j in range(num_levels):\n                diffuse_reflectance.append(args[current_index])\n                current_index += 1\n            diffuse_uv_scale = args[current_index]\n            current_index += 1\n            \n            num_levels = args[current_index]\n            current_index += 1\n            specular_reflectance = []\n            for j in range(num_levels):\n                specular_reflectance.append(args[current_index])\n                current_index += 1\n            specular_uv_scale = args[current_index]\n            current_index += 1\n            \n            num_levels = args[current_index]\n            current_index += 1\n            roughness = []\n            for j in range(num_levels):\n                roughness.append(args[current_index])\n                current_index += 1\n            roughness_uv_scale = args[current_index]\n            current_index += 1\n\n            num_levels = args[current_index]\n            current_index += 1\n            generic_texture = []\n            if num_levels > 0:\n                for j in range(num_levels):\n                    generic_texture.append(args[current_index])\n                    current_index += 1\n                generic_uv_scale = args[current_index]\n                current_index += 1\n            else:\n                generic_uv_scale = None\n\n            num_levels = args[current_index]\n            current_index += 1\n            normal_map = []\n            if num_levels > 0:\n                for j in range(num_levels):\n                    normal_map.append(args[current_index])\n                    current_index += 1\n                normal_map_uv_scale = args[current_index]\n                current_index += 1\n            else:\n                normal_map_uv_scale = None\n\n            compute_specular_lighting = args[current_index]\n            current_index += 1\n            two_sided = args[current_index]\n            current_index += 1\n            use_vertex_color = args[current_index]\n            current_index += 1\n\n            if diffuse_reflectance[0].dim() == 1:\n                # Constant texture\n                diffuse_reflectance = redner.Texture3(\\\n                    [redner.float_ptr(diffuse_reflectance[0].data_ptr())],\n                    [0], [0], 3,\n                    redner.float_ptr(diffuse_uv_scale.data_ptr()))\n            else:\n                assert(diffuse_reflectance[0].dim() == 3)\n                diffuse_reflectance = redner.Texture3(\\\n                    [redner.float_ptr(x.data_ptr()) for x in diffuse_reflectance],\n                    [x.shape[1] for x in diffuse_reflectance],\n                    [x.shape[0] for x in diffuse_reflectance],\n                    3,\n                    redner.float_ptr(diffuse_uv_scale.data_ptr()))\n\n            if specular_reflectance[0].dim() == 1:\n                # Constant texture\n                specular_reflectance = redner.Texture3(\\\n                    [redner.float_ptr(specular_reflectance[0].data_ptr())],\n                    [0], [0], 3,\n                    redner.float_ptr(specular_uv_scale.data_ptr()))\n            else:\n                assert(specular_reflectance[0].dim() == 3)\n                specular_reflectance = redner.Texture3(\\\n                    [redner.float_ptr(x.data_ptr()) for x in specular_reflectance],\n                    [x.shape[1] for x in specular_reflectance],\n                    [x.shape[0] for x in specular_reflectance],\n                    3,\n                    redner.float_ptr(specular_uv_scale.data_ptr()))\n\n            if roughness[0].dim() == 1:\n                # Constant texture\n                roughness = redner.Texture1(\\\n                    [redner.float_ptr(roughness[0].data_ptr())],\n                    [0], [0], 1,\n                    redner.float_ptr(roughness_uv_scale.data_ptr()))\n            else:\n                assert(roughness[0].dim() == 3)\n                roughness = redner.Texture1(\\\n                    [redner.float_ptr(x.data_ptr()) for x in roughness],\n                    [x.shape[1] for x in roughness],\n                    [x.shape[0] for x in roughness],\n                    1,\n                    redner.float_ptr(roughness_uv_scale.data_ptr()))\n\n            if len(generic_texture) > 0:\n                assert(generic_texture[0].dim() == 3)\n                generic_texture = redner.TextureN(\\\n                    [redner.float_ptr(x.data_ptr()) for x in generic_texture],\n                    [x.shape[1] for x in generic_texture],\n                    [x.shape[0] for x in generic_texture],\n                    generic_texture[0].shape[2],\n                    redner.float_ptr(generic_uv_scale.data_ptr()))\n            else:\n                generic_texture = redner.TextureN(\\\n                    [], [], [], 0, redner.float_ptr(0))\n\n            if len(normal_map) > 0:\n                assert(normal_map[0].dim() == 3)\n                normal_map = redner.Texture3(\\\n                    [redner.float_ptr(x.data_ptr()) for x in normal_map],\n                    [x.shape[1] for x in normal_map],\n                    [x.shape[0] for x in normal_map],\n                    3,\n                    redner.float_ptr(normal_map_uv_scale.data_ptr()))\n            else:\n                normal_map = redner.Texture3(\\\n                    [], [], [], 0, redner.float_ptr(0))\n            materials.append(redner.Material(\\\n                diffuse_reflectance,\n                specular_reflectance,\n                roughness,\n                generic_texture,\n                normal_map,\n                compute_specular_lighting,\n                two_sided,\n                use_vertex_color))\n\n        area_lights = []\n        for i in range(num_lights):\n            shape_id = args[current_index]\n            current_index += 1\n            intensity = args[current_index]\n            current_index += 1\n            two_sided = args[current_index]\n            current_index += 1\n            directly_visible = args[current_index]\n            current_index += 1\n\n            area_lights.append(redner.AreaLight(\\\n                shape_id,\n                redner.float_ptr(intensity.data_ptr()),\n                two_sided,\n                directly_visible))\n\n        envmap = None\n        if args[current_index] is not None:\n            num_levels = args[current_index]\n            current_index += 1\n            values = []\n            for j in range(num_levels):\n                values.append(args[current_index])\n                current_index += 1\n            envmap_uv_scale = args[current_index]\n            current_index += 1\n            env_to_world = args[current_index]\n            current_index += 1\n            world_to_env = args[current_index]\n            current_index += 1\n            sample_cdf_ys = args[current_index]\n            current_index += 1\n            sample_cdf_xs = args[current_index]\n            current_index += 1\n            pdf_norm = args[current_index]\n            current_index += 1\n            directly_visible = args[current_index]\n            current_index += 1\n            values = redner.Texture3(\\\n                [redner.float_ptr(x.data_ptr()) for x in values],\n                [x.shape[1] for x in values], # width\n                [x.shape[0] for x in values], # height\n                3, # channels\n                redner.float_ptr(envmap_uv_scale.data_ptr()))\n            envmap = redner.EnvironmentMap(\\\n                values,\n                redner.float_ptr(env_to_world.data_ptr()),\n                redner.float_ptr(world_to_env.data_ptr()),\n                redner.float_ptr(sample_cdf_ys.data_ptr()),\n                redner.float_ptr(sample_cdf_xs.data_ptr()),\n                pdf_norm,\n                directly_visible)\n        else:\n            current_index += 1\n\n        # Options\n        num_samples = args[current_index]\n        current_index += 1\n        max_bounces = args[current_index]\n        current_index += 1\n        channels = args[current_index]\n        current_index += 1\n        sampler_type = args[current_index]\n        current_index += 1\n        use_primary_edge_sampling_ = args[current_index]\n        current_index += 1\n        use_secondary_edge_sampling_ = args[current_index]\n        current_index += 1\n        sample_pixel_center = args[current_index]\n        current_index += 1\n        device = args[current_index]\n        current_index += 1\n\n        if use_primary_edge_sampling is None:\n            use_primary_edge_sampling = use_primary_edge_sampling_\n        if use_secondary_edge_sampling is None:\n            use_secondary_edge_sampling = use_secondary_edge_sampling_\n\n        device_index = device.index\n        if device.index is None:\n            device_index = torch.cuda.current_device() if torch.cuda.is_available() else 0\n        start = time.time()\n        scene = redner.Scene(camera,\n                             shapes,\n                             materials,\n                             area_lights,\n                             envmap,\n                             device.type == \'cuda\',\n                             device_index,\n                             use_primary_edge_sampling,\n                             use_secondary_edge_sampling)\n        time_elapsed = time.time() - start\n        if get_print_timing():\n            print(\'Scene construction, time: %.5f s\' % time_elapsed)\n\n        # check that num_samples is a tuple\n        if isinstance(num_samples, int):\n            num_samples = (num_samples, num_samples)\n\n        options = redner.RenderOptions(seed,\n                                       num_samples[0],\n                                       max_bounces,\n                                       channels,\n                                       sampler_type,\n                                       sample_pixel_center)\n\n        ctx = Context()\n        ctx.channels = channels\n        ctx.options = options\n        ctx.resolution = resolution\n        ctx.viewport = viewport\n        ctx.scene = scene\n        ctx.camera = camera\n        ctx.shapes = shapes\n        ctx.materials = materials\n        ctx.area_lights = area_lights\n        ctx.envmap = envmap\n        ctx.scene = scene\n        ctx.options = options\n        ctx.num_samples = num_samples\n        ctx.device = device\n\n        return ctx\n\n    @staticmethod\n    def forward(ctx,\n                seed,\n                *args):\n        """"""\n            Forward rendering pass: given a serialized scene and output an image.\n        """"""\n\n        args_ctx = RenderFunction.unpack_args(seed, args)\n        area_lights = args_ctx.area_lights\n        camera = args_ctx.camera\n        channels = args_ctx.channels\n        envmap = args_ctx.envmap\n        materials = args_ctx.materials\n        num_samples = args_ctx.num_samples\n        options = args_ctx.options\n        resolution = args_ctx.resolution\n        viewport = args_ctx.viewport\n        scene = args_ctx.scene\n        shapes = args_ctx.shapes\n        device = args_ctx.device\n\n        num_channels = redner.compute_num_channels(channels,\n                                                   scene.max_generic_texture_dimension)\n        img_height = viewport[2] - viewport[0]\n        img_width = viewport[3] - viewport[1]\n        rendered_image = torch.zeros(img_height, img_width, num_channels, device = device)\n        start = time.time()\n        redner.render(scene,\n                      options,\n                      redner.float_ptr(rendered_image.data_ptr()),\n                      redner.float_ptr(0), # d_rendered_image\n                      None, # d_scene\n                      redner.float_ptr(0), # translational_gradient_image\n                      redner.float_ptr(0)) # debug_image\n        time_elapsed = time.time() - start\n        if get_print_timing():\n            print(\'Forward pass, time: %.5f s\' % time_elapsed)\n\n        ctx.camera = camera\n        ctx.shapes = shapes\n        ctx.materials = materials\n        ctx.area_lights = area_lights\n        ctx.envmap = envmap\n        ctx.scene = scene\n        ctx.options = options\n        ctx.num_samples = num_samples\n        ctx.device = device\n        ctx.args = args # Important to prevent GC from deallocating the tensors\n        return rendered_image\n\n    @staticmethod\n    def create_gradient_buffers(ctx):\n        scene = ctx.scene\n        options = ctx.options\n        camera = ctx.camera\n        device = ctx.device\n\n        buffers = Context()\n\n        if camera.use_look_at:\n            buffers.d_cam_position = torch.zeros(3, device = device)\n            buffers.d_cam_look = torch.zeros(3, device = device)\n            buffers.d_cam_up = torch.zeros(3, device = device)\n            buffers.d_cam_to_world = None\n            buffers.d_world_to_cam = None\n        else:\n            buffers.d_cam_position = None\n            buffers.d_cam_look = None\n            buffers.d_cam_up = None\n            buffers.d_cam_to_world = torch.zeros(4, 4, device = device)\n            buffers.d_world_to_cam = torch.zeros(4, 4, device = device)\n        buffers.d_intrinsic_mat_inv = torch.zeros(3, 3, device = device)\n        buffers.d_intrinsic_mat = torch.zeros(3, 3, device = device)\n        buffers.d_distortion_params = None\n        if camera.has_distortion_params():\n            buffers.d_distortion_params = torch.zeros(8, device = device)\n        if camera.use_look_at:\n            buffers.d_camera = redner.DCamera(\\\n                redner.float_ptr(buffers.d_cam_position.data_ptr()),\n                redner.float_ptr(buffers.d_cam_look.data_ptr()),\n                redner.float_ptr(buffers.d_cam_up.data_ptr()),\n                redner.float_ptr(0), # cam_to_world\n                redner.float_ptr(0), # world_to_cam\n                redner.float_ptr(buffers.d_intrinsic_mat_inv.data_ptr()),\n                redner.float_ptr(buffers.d_intrinsic_mat.data_ptr()),\n                redner.float_ptr(buffers.d_distortion_params.data_ptr() if buffers.d_distortion_params is not None else 0))\n        else:\n            buffers.d_camera = redner.DCamera(\\\n                redner.float_ptr(0), # pos\n                redner.float_ptr(0), # look\n                redner.float_ptr(0), # up\n                redner.float_ptr(buffers.d_cam_to_world.data_ptr()),\n                redner.float_ptr(buffers.d_world_to_cam.data_ptr()),\n                redner.float_ptr(buffers.d_intrinsic_mat_inv.data_ptr()),\n                redner.float_ptr(buffers.d_intrinsic_mat.data_ptr()),\n                redner.float_ptr(buffers.d_distortion_params.data_ptr() if buffers.d_distortion_params is not None else 0))\n        buffers.d_vertices_list = []\n        buffers.d_uvs_list = []\n        buffers.d_normals_list = []\n        buffers.d_colors_list = []\n        buffers.d_shapes = []\n        for shape in ctx.shapes:\n            num_vertices = shape.num_vertices\n            num_uv_vertices = shape.num_uv_vertices\n            num_normal_vertices = shape.num_normal_vertices\n            d_vertices = torch.zeros(num_vertices, 3, device = device)\n            d_uvs = torch.zeros(num_uv_vertices, 2,\n                device = device) if shape.has_uvs() else None\n            d_normals = torch.zeros(num_normal_vertices, 3,\n                device = device) if shape.has_normals() else None\n            d_colors = torch.zeros(num_vertices, 3,\n                device = device) if shape.has_colors() else None\n            buffers.d_vertices_list.append(d_vertices)\n            buffers.d_uvs_list.append(d_uvs)\n            buffers.d_normals_list.append(d_normals)\n            buffers.d_colors_list.append(d_colors)\n            buffers.d_shapes.append(redner.DShape(\\\n                redner.float_ptr(d_vertices.data_ptr()),\n                redner.float_ptr(d_uvs.data_ptr() if d_uvs is not None else 0),\n                redner.float_ptr(d_normals.data_ptr() if d_normals is not None else 0),\n                redner.float_ptr(d_colors.data_ptr() if d_colors is not None else 0)))\n\n        buffers.d_diffuse_list = []\n        buffers.d_diffuse_uv_scale_list = []\n        buffers.d_specular_list = []\n        buffers.d_specular_uv_scale_list = []\n        buffers.d_roughness_list = []\n        buffers.d_roughness_uv_scale_list = []\n        buffers.d_generic_list = []\n        buffers.d_generic_uv_scale_list = []\n        buffers.d_normal_map_list = []\n        buffers.d_normal_map_uv_scale_list = []\n        buffers.d_materials = []\n        for material in ctx.materials:\n            if material.get_diffuse_size(0)[0] == 0:\n                d_diffuse = [torch.zeros(3, device = device)]\n            else:\n                d_diffuse = []\n                for l in range(material.get_diffuse_levels()):\n                    diffuse_size = material.get_diffuse_size(l)\n                    d_diffuse.append(\\\n                        torch.zeros(diffuse_size[1],\n                                    diffuse_size[0],\n                                    3, device = device))\n\n            if material.get_specular_size(0)[0] == 0:\n                d_specular = [torch.zeros(3, device = device)]\n            else:\n                d_specular = []\n                for l in range(material.get_specular_levels()):\n                    specular_size = material.get_specular_size(l)\n                    d_specular.append(\\\n                        torch.zeros(specular_size[1],\n                                    specular_size[0],\n                                    3, device = device))\n\n            if material.get_roughness_size(0)[0] == 0:\n                d_roughness = [torch.zeros(1, device = device)]\n            else:\n                d_roughness = []\n                for l in range(material.get_roughness_levels()):\n                    roughness_size = material.get_roughness_size(l)\n                    d_roughness.append(\\\n                        torch.zeros(roughness_size[1],\n                                    roughness_size[0],\n                                    1, device = device))\n\n            if material.get_generic_levels() == 0:\n                d_generic = None\n            else:\n                d_generic = []\n                for l in range(material.get_generic_levels()):\n                    generic_size = material.get_generic_size(l)\n                    d_generic.append(\\\n                        torch.zeros(generic_size[2],\n                                    generic_size[1],\n                                    generic_size[0], device = device))\n\n            if material.get_normal_map_levels() == 0:\n                d_normal_map = None\n            else:\n                d_normal_map = []\n                for l in range(material.get_normal_map_levels()):\n                    normal_map_size = material.get_normal_map_size(l)\n                    d_normal_map.append(\\\n                        torch.zeros(normal_map_size[1],\n                                    normal_map_size[0],\n                                    3, device = device))\n\n            buffers.d_diffuse_list.append(d_diffuse)\n            buffers.d_specular_list.append(d_specular)\n            buffers.d_roughness_list.append(d_roughness)\n            buffers.d_generic_list.append(d_generic)\n            buffers.d_normal_map_list.append(d_normal_map)\n            d_diffuse_uv_scale = torch.zeros(2, device = device)\n            d_specular_uv_scale = torch.zeros(2, device = device)\n            d_roughness_uv_scale = torch.zeros(2, device = device)\n            buffers.d_diffuse_uv_scale_list.append(d_diffuse_uv_scale)\n            buffers.d_specular_uv_scale_list.append(d_specular_uv_scale)\n            buffers.d_roughness_uv_scale_list.append(d_roughness_uv_scale)\n            if d_generic is None:\n                d_generic_uv_scale = None\n            else:\n                d_generic_uv_scale = torch.zeros(2, device = device)\n            if d_normal_map is None:\n                d_normal_map_uv_scale = None\n            else:\n                d_normal_map_uv_scale = torch.zeros(2, device = device)\n\n            buffers.d_generic_uv_scale_list.append(d_generic_uv_scale)\n            buffers.d_normal_map_uv_scale_list.append(d_normal_map_uv_scale)\n            if d_diffuse[0].dim() == 1:\n                d_diffuse_tex = redner.Texture3(\\\n                    [redner.float_ptr(d_diffuse[0].data_ptr())],\n                    [0],\n                    [0],\n                    3,\n                    redner.float_ptr(d_diffuse_uv_scale.data_ptr()))\n            else:\n                d_diffuse_tex = redner.Texture3(\\\n                    [redner.float_ptr(x.data_ptr()) for x in d_diffuse],\n                    [x.shape[1] for x in d_diffuse],\n                    [x.shape[0] for x in d_diffuse],\n                    3,\n                    redner.float_ptr(d_diffuse_uv_scale.data_ptr()))\n\n            if d_specular[0].dim() == 1:\n                d_specular_tex = redner.Texture3(\\\n                    [redner.float_ptr(d_specular[0].data_ptr())],\n                    [0],\n                    [0],\n                    3,\n                    redner.float_ptr(d_specular_uv_scale.data_ptr()))\n            else:\n                d_specular_tex = redner.Texture3(\\\n                    [redner.float_ptr(x.data_ptr()) for x in d_specular],\n                    [x.shape[1] for x in d_specular],\n                    [x.shape[0] for x in d_specular],\n                    3,\n                    redner.float_ptr(d_specular_uv_scale.data_ptr()))\n\n            if d_roughness[0].dim() == 1:\n                d_roughness_tex = redner.Texture1(\\\n                    [redner.float_ptr(d_roughness[0].data_ptr())],\n                    [0],\n                    [0],\n                    1,\n                    redner.float_ptr(d_roughness_uv_scale.data_ptr()))\n            else:\n                d_roughness_tex = redner.Texture1(\\\n                    [redner.float_ptr(x.data_ptr()) for x in d_roughness],\n                    [x.shape[1] for x in d_roughness],\n                    [x.shape[0] for x in d_roughness],\n                    1,\n                    redner.float_ptr(d_roughness_uv_scale.data_ptr()))\n\n            if d_generic is None:\n                d_generic_tex = redner.TextureN(\\\n                    [], [], [], 0, redner.float_ptr(0))\n            else:\n                d_generic_tex = redner.TextureN(\\\n                    [redner.float_ptr(x.data_ptr()) for x in d_generic],\n                    [x.shape[1] for x in d_generic],\n                    [x.shape[0] for x in d_generic],\n                    d_generic[0].shape[2],\n                    redner.float_ptr(d_generic_uv_scale.data_ptr()))\n\n            if d_normal_map is None:\n                d_normal_map = redner.Texture3(\\\n                    [], [], [], 0, redner.float_ptr(0))\n            else:\n                d_normal_map = redner.Texture3(\\\n                    [redner.float_ptr(x.data_ptr()) for x in d_normal_map],\n                    [x.shape[1] for x in d_normal_map],\n                    [x.shape[0] for x in d_normal_map],\n                    3,\n                    redner.float_ptr(d_normal_map_uv_scale.data_ptr()))\n            buffers.d_materials.append(redner.DMaterial(\\\n                d_diffuse_tex, d_specular_tex, d_roughness_tex,\n                d_generic_tex, d_normal_map))\n\n        buffers.d_intensity_list = []\n        buffers.d_area_lights = []\n        for light in ctx.area_lights:\n            d_intensity = torch.zeros(3, device = device)\n            buffers.d_intensity_list.append(d_intensity)\n            buffers.d_area_lights.append(\\\n                redner.DAreaLight(redner.float_ptr(d_intensity.data_ptr())))\n\n        buffers.d_envmap = None\n        if ctx.envmap is not None:\n            envmap = ctx.envmap\n            buffers.d_envmap_values = []\n            for l in range(envmap.get_levels()):\n                size = envmap.get_size(l)\n                buffers.d_envmap_values.append(\\\n                    torch.zeros(size[1],\n                                size[0],\n                                3, device = device))\n            buffers.d_envmap_uv_scale = torch.zeros(2, device = device)\n            d_envmap_tex = redner.Texture3(\\\n                [redner.float_ptr(x.data_ptr()) for x in buffers.d_envmap_values],\n                [x.shape[1] for x in buffers.d_envmap_values],\n                [x.shape[0] for x in buffers.d_envmap_values],\n                3,\n                redner.float_ptr(buffers.d_envmap_uv_scale.data_ptr()))\n            buffers.d_world_to_env = torch.zeros(4, 4, device = device)\n            buffers.d_envmap = redner.DEnvironmentMap(\\\n                d_envmap_tex,\n                redner.float_ptr(buffers.d_world_to_env.data_ptr()))\n\n        device_index = device.index\n        if device.index is None:\n            device_index = torch.cuda.current_device() if torch.cuda.is_available() else 0\n        buffers.d_scene = redner.DScene(buffers.d_camera,\n                                        buffers.d_shapes,\n                                        buffers.d_materials,\n                                        buffers.d_area_lights,\n                                        buffers.d_envmap,\n                                        device.type == \'cuda\',\n                                        device_index)\n        return buffers\n\n    @staticmethod\n    def visualize_screen_gradient(grad_img: torch.Tensor,\n                                  seed: int,\n                                  scene: pyredner.Scene,\n                                  num_samples: Union[int, Tuple[int, int]],\n                                  max_bounces: int,\n                                  channels: List = [redner.channels.radiance],\n                                  sampler_type = redner.SamplerType.independent,\n                                  use_primary_edge_sampling: bool = True,\n                                  use_secondary_edge_sampling: bool = True,\n                                  sample_pixel_center: bool = False):\n        """"""\n            Given a serialized scene and output an 2-channel image,\n            which visualizes the derivatives of pixel color with respect to \n            the screen space coordinates.\n\n            Args\n            ====\n            grad_img: Optional[torch.Tensor]\n                The ""adjoint"" of the backpropagation gradient. If you don\'t know\n                what this means just give None\n            seed: int\n                seed for the Monte Carlo random samplers\n            See serialize_scene for the explanation of the rest of the arguments.\n        """"""\n\n        args = RenderFunction.serialize_scene(\\\n            scene = scene,\n            num_samples = num_samples,\n            max_bounces = max_bounces,\n            sampler_type = sampler_type,\n            channels = channels,\n            sample_pixel_center = sample_pixel_center)\n        args_ctx = RenderFunction.unpack_args(\\\n            seed, args, use_primary_edge_sampling, use_secondary_edge_sampling)\n        channels = args_ctx.channels\n        options = args_ctx.options\n        resolution = args_ctx.resolution\n        viewport = args_ctx.viewport\n        scene = args_ctx.scene\n\n        buffers = RenderFunction.create_gradient_buffers(args_ctx)\n        num_channels = redner.compute_num_channels(channels,\n                                                   scene.max_generic_texture_dimension)\n        img_height = viewport[2] - viewport[0]\n        img_width = viewport[3] - viewport[1]\n        screen_gradient_image = torch.zeros(\\\n            img_height, img_width, 2, device = device)\n        if grad_img is not None:\n            assert(grad_img.shape[0] == img_height)\n            assert(grad_img.shape[1] == img_width)\n            assert(grad_img.shape[2] == num_channels)\n        else:\n            grad_img = torch.ones(img_height, img_width, num_channels, device = device)\n        start = time.time()\n        redner.render(scene,\n                      options,\n                      redner.float_ptr(0), # rendered_image\n                      redner.float_ptr(grad_img.data_ptr()), # d_rendered_image\n                      buffers.d_scene,\n                      redner.float_ptr(screen_gradient_image.data_ptr()),\n                      redner.float_ptr(0)) # debug_image\n        time_elapsed = time.time() - start\n        if get_print_timing():\n            print(\'Visualize gradient, time: %.5f s\' % time_elapsed)\n\n        return screen_gradient_image\n\n    @staticmethod\n    def backward(ctx,\n                 grad_img):\n        if not grad_img.is_contiguous():\n            grad_img = grad_img.contiguous()\n        assert(torch.isfinite(grad_img).all())\n        scene = ctx.scene\n        options = ctx.options\n        camera = ctx.camera\n\n        buffers = RenderFunction.create_gradient_buffers(ctx)\n\n        if not get_use_correlated_random_number():\n            # Decouple the forward/backward random numbers by adding a big prime number\n            options.seed += 1000003\n\n        options.num_samples = ctx.num_samples[1]\n        start = time.time()\n        redner.render(scene, options,\n                      redner.float_ptr(0), # rendered_image\n                      redner.float_ptr(grad_img.data_ptr()),\n                      buffers.d_scene,\n                      redner.float_ptr(0), # translational_gradient_image\n                      redner.float_ptr(0)) # debug_image\n        time_elapsed = time.time() - start\n        if get_print_timing():\n            print(\'Backward pass, time: %.5f s\' % time_elapsed)\n\n        ret_list = []\n        ret_list.append(None) # seed\n        ret_list.append(None) # num_shapes\n        ret_list.append(None) # num_materials\n        ret_list.append(None) # num_lights\n        if camera.use_look_at:\n            ret_list.append(buffers.d_cam_position.cpu())\n            ret_list.append(buffers.d_cam_look.cpu())\n            ret_list.append(buffers.d_cam_up.cpu())\n            ret_list.append(None) # cam_to_world\n            ret_list.append(None) # world_to_cam\n        else:\n            ret_list.append(None) # pos\n            ret_list.append(None) # look\n            ret_list.append(None) # up\n            ret_list.append(buffers.d_cam_to_world.cpu())\n            ret_list.append(buffers.d_world_to_cam.cpu())\n        ret_list.append(buffers.d_intrinsic_mat_inv.cpu())\n        ret_list.append(buffers.d_intrinsic_mat.cpu())\n        if not camera.has_distortion_params():\n            ret_list.append(None) # distortion_params\n        else:\n            ret_list.append(buffers.d_distortion_params.cpu())\n        ret_list.append(None) # clip near\n        ret_list.append(None) # resolution\n        ret_list.append(None) # viewport\n        ret_list.append(None) # camera_type\n\n        num_shapes = len(ctx.shapes)\n        for i in range(num_shapes):\n            ret_list.append(buffers.d_vertices_list[i])\n            ret_list.append(None) # indices\n            ret_list.append(buffers.d_uvs_list[i])\n            ret_list.append(buffers.d_normals_list[i])\n            ret_list.append(None) # uv_indices\n            ret_list.append(None) # normal_indices\n            ret_list.append(buffers.d_colors_list[i])\n            ret_list.append(None) # material id\n            ret_list.append(None) # light id\n\n        num_materials = len(ctx.materials)\n        for i in range(num_materials):\n            ret_list.append(None) # num_levels\n            for d_diffuse in buffers.d_diffuse_list[i]:\n                ret_list.append(d_diffuse)\n            ret_list.append(buffers.d_diffuse_uv_scale_list[i])\n            ret_list.append(None) # num_levels\n            for d_specular in buffers.d_specular_list[i]:\n                ret_list.append(d_specular)\n            ret_list.append(buffers.d_specular_uv_scale_list[i])\n            ret_list.append(None) # num_levels\n            for d_roughness in buffers.d_roughness_list[i]:\n                ret_list.append(d_roughness)\n            ret_list.append(buffers.d_roughness_uv_scale_list[i])\n            if buffers.d_generic_list[i] is None:\n                ret_list.append(None) # num_levels\n            else:\n                ret_list.append(None) # num_levels\n                for d_generic in buffers.d_generic_list[i]:\n                    ret_list.append(d_generic)\n                ret_list.append(buffers.d_generic_uv_scale_list[i])\n            if buffers.d_normal_map_list[i] is None:\n                ret_list.append(None) # num_levels\n            else:\n                ret_list.append(None) # num_levels\n                for d_normal_map in buffers.d_normal_map_list[i]:\n                    ret_list.append(d_normal_map)\n                ret_list.append(buffers.d_normal_map_uv_scale_list[i])\n            ret_list.append(None) # compute_specular_lighting\n            ret_list.append(None) # two sided\n            ret_list.append(None) # use_vertex_color\n\n        num_area_lights = len(ctx.area_lights)\n        for i in range(num_area_lights):\n            ret_list.append(None) # shape id\n            ret_list.append(buffers.d_intensity_list[i].cpu())\n            ret_list.append(None) # two_sided\n            ret_list.append(None) # directly_visible\n\n        if ctx.envmap is not None:\n            ret_list.append(None) # num_levels\n            for d_values in buffers.d_envmap_values:\n                ret_list.append(d_values)\n            ret_list.append(buffers.d_envmap_uv_scale)\n            ret_list.append(None) # env_to_world\n            ret_list.append(buffers.d_world_to_env.cpu())\n            ret_list.append(None) # sample_cdf_ys\n            ret_list.append(None) # sample_cdf_xs\n            ret_list.append(None) # pdf_norm\n            ret_list.append(None) # directly_visible\n        else:\n            ret_list.append(None)\n\n        ret_list.append(None) # num samples\n        ret_list.append(None) # num bounces\n        ret_list.append(None) # channels\n        ret_list.append(None) # sampler type\n        ret_list.append(None) # use_primary_edge_sampling\n        ret_list.append(None) # use_secondary_edge_sampling\n        ret_list.append(None) # sample_pixel_center\n        ret_list.append(None) # device\n\n        return tuple(ret_list)\n'"
pyredner/render_utils.py,0,"b'import pyredner\nimport random\nimport redner\nimport torch\nimport math\nfrom typing import Union, Tuple, Optional, List\n\nclass DeferredLight:\n    pass\n\nclass AmbientLight(DeferredLight):\n    """"""\n        Ambient light for deferred rendering.\n    """"""\n    def __init__(self,\n                 intensity: torch.Tensor):\n        self.intensity = intensity\n\n    def render(self,\n               position: torch.Tensor,\n               normal: torch.Tensor,\n               albedo: torch.Tensor):\n        return self.intensity.to(albedo.device) * albedo\n\nclass PointLight(DeferredLight):\n    """"""\n        Point light with squared distance falloff for deferred rendering.\n    """"""\n    def __init__(self,\n                 position: torch.Tensor,\n                 intensity: torch.Tensor):\n        self.position = position\n        self.intensity = intensity\n\n    def render(self,\n               position: torch.Tensor,\n               normal: torch.Tensor,\n               albedo: torch.Tensor):\n        light_dir = self.position.to(position.device) - position\n        # the d^2 term:\n        light_dist_sq = torch.sum(light_dir * light_dir, dim = -1, keepdim = True)\n        light_dist = torch.sqrt(light_dist_sq)\n        # Normalize light direction\n        light_dir = light_dir / light_dist\n        dot_l_n = torch.sum(light_dir * normal, dim = -1, keepdim = True)\n        dot_l_n = torch.max(dot_l_n, torch.zeros_like(dot_l_n))\n        return self.intensity.to(dot_l_n.device) * dot_l_n * (albedo / math.pi) / light_dist_sq \n\nclass DirectionalLight(DeferredLight):\n    """"""\n        Directional light for deferred rendering.\n    """"""\n    def __init__(self,\n                 direction: torch.Tensor,\n                 intensity: torch.Tensor):\n        self.direction = direction\n        self.intensity = intensity\n\n    def render(self,\n               position: torch.Tensor,\n               normal: torch.Tensor,\n               albedo: torch.Tensor):\n        # Normalize light direction\n        light_dir = -self.direction / torch.norm(self.direction)\n        light_dir = light_dir.view(1, 1, 3)\n        light_dir = light_dir.to(normal.device)\n        dot_l_n = torch.sum(light_dir * normal, dim = -1, keepdim = True)\n        dot_l_n = torch.max(dot_l_n, torch.zeros_like(dot_l_n))\n        return self.intensity.to(dot_l_n.device) * dot_l_n * (albedo / math.pi)\n\nclass SpotLight(DeferredLight):\n    """"""\n        Spot light with cosine falloff for deferred rendering.\n        Note that we do not provide the cosine cutoff here since it is not\n        differentiable.\n    """"""\n    def __init__(self,\n                 position: torch.Tensor,\n                 spot_direction: torch.Tensor,\n                 spot_exponent: torch.Tensor,\n                 intensity: torch.Tensor):\n        self.position = position\n        self.spot_direction = spot_direction\n        self.spot_exponent = spot_exponent\n        self.intensity = intensity\n\n    def render(self,\n               position: torch.Tensor,\n               normal: torch.Tensor,\n               albedo: torch.Tensor):\n        light_dir = self.position.to(position.device) - position\n        # Normalize light direction\n        light_dir = light_dir / torch.norm(light_dir, dim = -1, keepdim = True)\n        # Normalize spot direction\n        spot_direction = -self.spot_direction / torch.norm(self.spot_direction)\n        spot_direction = spot_direction.to(light_dir.device)\n        spot_cosine = torch.sum(light_dir * spot_direction, dim = -1, keepdim = True)\n        spot_cosine = torch.max(spot_cosine, torch.zeros_like(spot_cosine))\n        spot_factor = torch.pow(spot_cosine, self.spot_exponent.to(spot_cosine.device))\n        dot_l_n = torch.sum(light_dir * normal, dim = -1, keepdim = True)\n        dot_l_n = torch.max(dot_l_n, torch.zeros_like(dot_l_n))\n        return self.intensity.to(spot_factor.device) * spot_factor * dot_l_n * (albedo / math.pi)\n\ndef render_deferred(scene: Union[pyredner.Scene, List[pyredner.Scene]],\n                    lights: Union[List[DeferredLight], List[List[DeferredLight]]],\n                    alpha: bool = False,\n                    aa_samples: int = 2,\n                    seed: Optional[Union[int, List[int]]] = None,\n                    sample_pixel_center: bool = False,\n                    device: Optional[torch.device] = None):\n    """"""\n        Render the scenes using `deferred rendering <https://en.wikipedia.org/wiki/Deferred_shading>`_.\n        We generate G-buffer images containing world-space position,\n        normal, and albedo using redner, then shade the G-buffer\n        using PyTorch code. Assuming Lambertian shading and does not\n        compute shadow.\n\n        Args\n        ====\n        scene: Union[pyredner.Scene, List[pyredner.Scene]]\n            pyredner Scene containing camera, geometry and material.\n            Can be a single scene or a list for batch render.\n            For batch rendering all scenes need to have the same resolution.\n        lights: Union[List[DeferredLight], List[List[DeferredLight]]]\n            Lights for deferred rendering. If the scene is a list, and only\n            a single list of lights is provided, the same lights are applied\n            to all scenes. If a list of lists of lights is provided, each scene\n            is lit by the corresponding lights.\n        alpha: bool\n            If set to False, generates a 3-channel image,\n            otherwise generates a 4-channel image where the\n            fourth channel is alpha.\n        aa_samples: int\n            Number of samples used for anti-aliasing at both x, y dimensions\n            (e.g. if aa_samples=2, 4 samples are used).\n        seed: Optional[Union[int, List[int]]]\n            Random seed used for sampling. Randomly assigned if set to None.\n            For batch render, if seed it not None, need to provide a list\n            of seeds.\n        sample_pixel_center: bool\n            Always sample at the pixel center when rendering.\n            This trades noise with aliasing.\n            If this option is activated, the rendering becomes non-differentiable\n            (since there is no antialiasing integral),\n            and redner\'s edge sampling becomes an approximation to the gradients of the aliased rendering.\n        device: Optional[torch.device]\n            Which device should we store the data in.\n            If set to None, use the device from pyredner.get_device().\n\n        Returns\n        =======\n        torch.Tensor or List[torch.Tensor]\n            | if input scene is a list: a tensor with size [N, H, W, C], N is the list size\n            | else: a tensor with size [H, W, C]\n            | if alpha == True, C = 4.\n            | else, C = 3.\n    """"""\n    if device is None:\n        device = pyredner.get_device()\n\n    channels = [redner.channels.position,\n                redner.channels.shading_normal,\n                redner.channels.diffuse_reflectance]\n    if alpha:\n        channels.append(redner.channels.alpha)\n    if isinstance(scene, pyredner.Scene):\n        if seed == None:\n            seed = random.randint(0, 16777216)\n        # We do full-screen anti-aliasing: increase the rendering resolution\n        # and downsample it after lighting\n        org_res = scene.camera.resolution\n        org_viewport = scene.camera.viewport\n        scene.camera.resolution = (org_res[0] * aa_samples,\n                                   org_res[1] * aa_samples)\n        if org_viewport is not None:\n            scene.camera.viewport = [i * aa_samples for i in org_viewport]\n        scene_args = pyredner.RenderFunction.serialize_scene(\\\n            scene = scene,\n            num_samples = (1, 1),\n            max_bounces = 0,\n            sampler_type = redner.SamplerType.sobol,\n            channels = channels,\n            use_secondary_edge_sampling = False,\n            sample_pixel_center = sample_pixel_center,\n            device = device)\n        # Need to revert the resolution back\n        scene.camera.resolution = org_res\n        scene.camera.viewport = org_viewport\n        g_buffer = pyredner.RenderFunction.apply(seed, *scene_args)\n        pos = g_buffer[:, :, :3]\n        normal = g_buffer[:, :, 3:6]\n        albedo = g_buffer[:, :, 6:9]\n        img = torch.zeros(g_buffer.shape[0], g_buffer.shape[1], 3, device = device)\n        for light in lights:\n            img = img + light.render(pos, normal, albedo)\n        if alpha:\n            # alpha is in the last channel\n            img = torch.cat((img, g_buffer[:, :, 9:10]), dim = -1)\n        if aa_samples > 1:\n            # Downsample\n            img = img.permute(2, 0, 1) # HWC -> CHW\n            img = img.unsqueeze(0) # CHW -> NCHW\n            if org_viewport is not None:\n                org_size = org_viewport[2] - org_viewport[0], org_viewport[3] - org_viewport[1]\n            else:\n                org_size = org_res\n            img = torch.nn.functional.interpolate(img, size = org_size, mode = \'area\')\n            img = img.squeeze(dim = 0) # NCHW -> CHW\n            img = img.permute(1, 2, 0)\n        return img\n    else:\n        assert(isinstance(scene, list))\n        if seed == None:\n            # Randomly generate a list of seed\n            seed = []\n            for i in range(len(scene)):\n                seed.append(random.randint(0, 16777216))\n        assert(len(seed) == len(scene))\n        if len(lights) > 0 and not isinstance(lights[0], list):\n            # Specialize version: stack g buffers and light all images in parallel\n            g_buffers = []\n            # Render each scene in the batch and stack them together\n            for sc, se in zip(scene, seed):\n                # We do full-screen anti-aliasing: increase the rendering resolution\n                # and downsample it after lighting\n                org_res = sc.camera.resolution\n                org_viewport = sc.camera.viewport\n                sc.camera.resolution = (org_res[0] * aa_samples,\n                                        org_res[1] * aa_samples)\n                if org_viewport is not None:\n                    sc.camera.viewport = [i * aa_samples for i in org_viewport]\n                scene_args = pyredner.RenderFunction.serialize_scene(\\\n                    scene = sc,\n                    num_samples = (1, 1),\n                    max_bounces = 0,\n                    sampler_type = redner.SamplerType.sobol,\n                    channels = channels,\n                    use_secondary_edge_sampling = False,\n                    sample_pixel_center = sample_pixel_center,\n                    device = device)\n                # Need to revert the resolution back\n                sc.camera.resolution = org_res\n                sc.camera.viewport = org_viewport\n                g_buffers.append(pyredner.RenderFunction.apply(se, *scene_args))\n            g_buffers = torch.stack(g_buffers)\n            pos = g_buffers[:, :, :, :3]\n            normal = g_buffers[:, :, :, 3:6]\n            albedo = g_buffers[:, :, :, 6:9]\n            imgs = torch.zeros(g_buffers.shape[0],\n                               g_buffers.shape[1],\n                               g_buffers.shape[2],\n                               3,\n                               device = device)\n            for light in lights:\n                imgs = imgs + light.render(pos, normal, albedo)\n            if alpha:\n                imgs = torch.cat((imgs, g_buffers[:, :, :, 9:10]), dim = -1)\n        else:\n            # If each scene has a different lighting: light them in the loop\n            imgs = []\n            # Render each scene in the batch and stack them together\n            for sc, se, lgts in zip(scene, seed, lights):\n                # We do full-screen anti-aliasing: increase the rendering resolution\n                # and downsample it after lighting\n                org_res = sc.camera.resolution\n                org_viewport = sc.camera.viewport\n                sc.camera.resolution = (org_res[0] * aa_samples,\n                                        org_res[1] * aa_samples)\n                if org_viewport is not None:\n                    sc.camera.viewport = [i * aa_samples for i in org_viewport]\n                scene_args = pyredner.RenderFunction.serialize_scene(\\\n                    scene = sc,\n                    num_samples = (1, 1),\n                    max_bounces = 0,\n                    sampler_type = redner.SamplerType.sobol,\n                    channels = channels,\n                    use_secondary_edge_sampling = False,\n                    sample_pixel_center = sample_pixel_center,\n                    device = device)\n                # Need to revert the resolution back\n                sc.camera.resolution = org_res\n                sc.camera.viewport = org_viewport\n                g_buffer = pyredner.RenderFunction.apply(se, *scene_args)\n                pos = g_buffer[:, :, :3]\n                normal = g_buffer[:, :, 3:6]\n                albedo = g_buffer[:, :, 6:9]\n                img = torch.zeros(g_buffer.shape[0],\n                                  g_buffer.shape[1],\n                                  3,\n                                  device = device)\n                for light in lgts:\n                    img = img + light.render(pos, normal, albedo)\n                if alpha:\n                    # alpha is in the last channel\n                    img = torch.cat((img, g_buffer[:, :, 9:10]), dim = -1)\n                imgs.append(img)\n            imgs = torch.stack(imgs)\n        if aa_samples > 1:\n            # Downsample\n            imgs = imgs.permute(0, 3, 1, 2) # NHWC -> NCHW\n            if org_viewport is not None:\n                org_size = org_viewport[2] - org_viewport[0], org_viewport[3] - org_viewport[1]\n            else:\n                org_size = org_res\n            imgs = torch.nn.functional.interpolate(imgs, size = org_size, mode = \'area\')\n            imgs = imgs.permute(0, 2, 3, 1) # NCHW -> NHWC\n        return imgs\n\ndef render_generic(scene: pyredner.Scene,\n                   channels: List,\n                   max_bounces: int = 1,\n                   sampler_type = pyredner.sampler_type.sobol,\n                   num_samples: Union[int, Tuple[int, int]] = (4, 4),\n                   seed: Optional[int] = None,\n                   sample_pixel_center: bool = False,\n                   device: Optional[torch.device] = None):\n    """"""\n        A generic rendering function that can be either pathtracing or\n        g-buffer rendering or both.\n\n        Args\n        ====\n        scene: Union[pyredner.Scene, List[pyredner.Scene]]\n            pyredner Scene containing camera, geometry and material.\n            Can be a single scene or a list for batch render.\n            For batch rendering all scenes need to have the same resolution.\n        channels: List[pyredner.channels]\n            | A list of the following channels\\:\n            | pyredner.channels.alpha\n            | pyredner.channels.depth\n            | pyredner.channels.position\n            | pyredner.channels.geometry_normal\n            | pyredner.channels.shading_normal\n            | pyredner.channels.uv\n            | pyredner.channels.barycentric_coordinates\n            | pyredner.channels.diffuse_reflectance\n            | pyredner.channels.specular_reflectance\n            | pyredner.channels.roughness\n            | pyredner.channels.generic_texture\n            | pyredner.channels.vertex_color\n            | pyredner.channels.shape_id\n            | pyredner.channels.triangle_id\n            | pyredner.channels.material_id\n        max_bounces: int\n            Number of bounces for global illumination, 1 means direct lighting only.\n        sampler_type: pyredner.sampler_type\n            | Which sampling pattern to use? See \n              `Chapter 7 of the PBRT book <http://www.pbr-book.org/3ed-2018/Sampling_and_Reconstruction.html>`_\n              for an explanation of the difference between different samplers.\n            | Following samplers are supported\\:\n            | pyredner.sampler_type.independent\n            | pyredner.sampler_type.sobol\n        num_samples: int\n            Number of samples per pixel for forward and backward passes.\n            Can be an integer or a tuple of 2 integers.\n        seed: Optional[Union[int, List[int]]]\n            Random seed used for sampling. Randomly assigned if set to None.\n            For batch render, if seed it not None, need to provide a list\n            of seeds.\n        sample_pixel_center: bool\n            Always sample at the pixel center when rendering.\n            This trades noise with aliasing.\n            If this option is activated, the rendering becomes non-differentiable\n            (since there is no antialiasing integral),\n            and redner\'s edge sampling becomes an approximation to the gradients of the aliased rendering.\n        device: Optional[torch.device]\n            Which device should we store the data in.\n            If set to None, use the device from pyredner.get_device().\n\n        Returns\n        =======\n        torch.Tensor or List[torch.Tensor]\n            | if input scene is a list: a tensor with size [N, H, W, C], N is the list size\n            | else: a tensor with size [H, W, C]\n    """"""\n    if device is None:\n        device = pyredner.get_device()\n\n    if isinstance(scene, pyredner.Scene):\n        if seed==None:\n            seed = random.randint(0, 16777216)\n        scene_args = pyredner.RenderFunction.serialize_scene(\\\n            scene = scene,\n            num_samples = num_samples,\n            max_bounces = max_bounces,\n            sampler_type = sampler_type,\n            channels = channels,\n            sample_pixel_center = sample_pixel_center,\n            device = device)\n        return pyredner.RenderFunction.apply(seed, *scene_args)\n    else:\n        assert(isinstance(scene, list))\n        if seed == None:\n            # Randomly generate a list of seed\n            seed = []\n            for i in range(len(scene)):\n                seed.append(random.randint(0, 16777216))\n        assert(len(seed) == len(scene))\n        # Render each scene in the batch and stack them together\n        imgs = []\n        for sc, se in zip(scene, seed):\n            scene_args = pyredner.RenderFunction.serialize_scene(\\\n                scene = sc,\n                num_samples = num_samples,\n                max_bounces = max_bounces,\n                sampler_type = sampler_type,\n                channels = channels,\n                sample_pixel_center = sample_pixel_center,\n                device = device)\n            imgs.append(pyredner.RenderFunction.apply(se, *scene_args))\n        imgs = torch.stack(imgs)\n        return imgs\n\ndef render_g_buffer(scene: Union[pyredner.Scene, List[pyredner.Scene]],\n                    channels: List,\n                    num_samples: Union[int, Tuple[int, int]] = (1, 1),\n                    seed: Optional[Union[int, List[int]]] = None,\n                    sample_pixel_center: bool = False,\n                    device: Optional[torch.device] = None):\n    """"""\n        Render G buffers from the scene.\n\n        Args\n        ====\n        scene: Union[pyredner.Scene, List[pyredner.Scene]]\n            pyredner Scene containing camera, geometry and material.\n            Can be a single scene or a list for batch render.\n            For batch rendering all scenes need to have the same resolution.\n        channels: List[pyredner.channels]\n            | A list of the following channels\\:\n            | pyredner.channels.alpha\n            | pyredner.channels.depth\n            | pyredner.channels.position\n            | pyredner.channels.geometry_normal\n            | pyredner.channels.shading_normal\n            | pyredner.channels.uv\n            | pyredner.channels.barycentric_coordinates\n            | pyredner.channels.diffuse_reflectance\n            | pyredner.channels.specular_reflectance\n            | pyredner.channels.roughness\n            | pyredner.channels.generic_texture\n            | pyredner.channels.vertex_color\n            | pyredner.channels.shape_id\n            | pyredner.channels.triangle_id\n            | pyredner.channels.material_id\n        num_samples: Union[int, Tuple[int, int]]\n            Number of samples for forward and backward passes, respectively.\n            If a single integer is provided, use the same number of samples\n            for both.\n        seed: Optional[Union[int, List[int]]]\n            Random seed used for sampling. Randomly assigned if set to None.\n            For batch render, if seed it not None, need to provide a list\n            of seeds.\n        sample_pixel_center: bool\n            Always sample at the pixel center when rendering.\n            This trades noise with aliasing.\n            If this option is activated, the rendering becomes non-differentiable\n            (since there is no antialiasing integral),\n            and redner\'s edge sampling becomes an approximation to the gradients of the aliased rendering.\n        device: Optional[torch.device]\n            Which device should we store the data in.\n            If set to None, use the device from pyredner.get_device().\n\n        Returns\n        =======\n        torch.Tensor or List[torch.Tensor]\n            | if input scene is a list: a tensor with size [N, H, W, C], N is the list size\n            | else: a tensor with size [H, W, C]\n    """"""\n    return render_generic(scene = scene,\n                          channels = channels,\n                          max_bounces = 0,\n                          sampler_type = redner.SamplerType.sobol,\n                          num_samples = num_samples,\n                          seed = seed,\n                          sample_pixel_center = sample_pixel_center,\n                          device = device)\n\ndef render_pathtracing(scene: Union[pyredner.Scene, List[pyredner.Scene]],\n                       alpha: bool = False,\n                       max_bounces: int = 1,\n                       sampler_type = pyredner.sampler_type.sobol,\n                       num_samples: Union[int, Tuple[int, int]] = (4, 4),\n                       seed: Optional[Union[int, List[int]]] = None,\n                       sample_pixel_center: bool = False,\n                       device: Optional[torch.device] = None):\n    """"""\n        Render a pyredner scene using pathtracing.\n\n        Args\n        ====\n        scene: Union[pyredner.Scene, List[pyredner.Scene]]\n            pyredner Scene containing camera, geometry and material.\n            Can be a single scene or a list for batch render.\n            For batch rendering all scenes need to have the same resolution.\n        max_bounces: int\n            Number of bounces for global illumination, 1 means direct lighting only.\n        sampler_type: pyredner.sampler_type\n            | Which sampling pattern to use? See \n              `Chapter 7 of the PBRT book <http://www.pbr-book.org/3ed-2018/Sampling_and_Reconstruction.html>`_\n              for an explanation of the difference between different samplers.\n            | Following samplers are supported\\:\n            | pyredner.sampler_type.independent\n            | pyredner.sampler_type.sobol\n        num_samples: int\n            Number of samples per pixel for forward and backward passes.\n            Can be an integer or a tuple of 2 integers.\n        seed: Optional[Union[int, List[int]]]\n            Random seed used for sampling. Randomly assigned if set to None.\n            For batch render, if seed it not None, need to provide a list\n            of seeds.\n        sample_pixel_center: bool\n            Always sample at the pixel center when rendering.\n            This trades noise with aliasing.\n            If this option is activated, the rendering becomes non-differentiable\n            (since there is no antialiasing integral),\n            and redner\'s edge sampling becomes an approximation to the gradients of the aliased rendering.\n        device: Optional[torch.device]\n            Which device should we store the data in.\n            If set to None, use the device from pyredner.get_device().\n\n        Returns\n        =======\n        torch.Tensor or List[torch.Tensor]\n            | if input scene is a list: a tensor with size [N, H, W, C], N is the list size\n            | else: a tensor with size [H, W, C]\n            | if alpha == True, C = 4.\n            | else, C = 3.\n    """"""\n    channels = [redner.channels.radiance]\n    if alpha:\n        channels.append(redner.channels.alpha)\n    return render_generic(scene = scene,\n                          channels = channels,\n                          max_bounces = max_bounces,\n                          sampler_type = sampler_type,\n                          num_samples = num_samples,\n                          seed = seed,\n                          sample_pixel_center = sample_pixel_center,\n                          device = device)\n\ndef render_albedo(scene: Union[pyredner.Scene, List[pyredner.Scene]],\n                  alpha: bool = False,\n                  num_samples: Union[int, Tuple[int, int]] = (16, 4),\n                  seed: Optional[Union[int, List[int]]] = None,\n                  sample_pixel_center: bool = False,\n                  device: Optional[torch.device] = None):\n    """"""\n        Render the diffuse albedo colors of the scenes.\n\n        Args\n        ====\n        scene: Union[pyredner.Scene, List[pyredner.Scene]]\n            pyredner Scene containing camera, geometry and material.\n            Can be a single scene or a list for batch render.\n            For batch rendering all scenes need to have the same resolution.\n        alpha: bool\n            If set to False, generates a 3-channel image,\n            otherwise generates a 4-channel image where the\n            fourth channel is alpha.\n        num_samples: Union[int, Tuple[int, int]]\n            number of samples for forward and backward passes, respectively\n            if a single integer is provided, use the same number of samples\n            for both\n        seed: Optional[Union[int, List[int]]]\n            Random seed used for sampling. Randomly assigned if set to None.\n            For batch render, if seed it not None, need to provide a list\n            of seeds.\n        sample_pixel_center: bool\n            Always sample at the pixel center when rendering.\n            This trades noise with aliasing.\n            If this option is activated, the rendering becomes non-differentiable\n            (since there is no antialiasing integral),\n            and redner\'s edge sampling becomes an approximation to the gradients of the aliased rendering.\n        device: Optional[torch.device]\n            Which device should we store the data in.\n            If set to None, use the device from pyredner.get_device().\n\n        Returns\n        =======\n        torch.Tensor or List[torch.Tensor]\n            | if input scene is a list: a tensor with size [N, H, W, C], N is the list size\n            | else: a tensor with size [H, W, C]\n            | if alpha == True, C = 4.\n            | else, C = 3.\n    """"""\n    channels = [redner.channels.diffuse_reflectance]\n    if alpha:\n        channels.append(redner.channels.alpha)\n    return render_g_buffer(scene = scene,\n                           channels = channels,\n                           num_samples = num_samples,\n                           seed = seed,\n                           sample_pixel_center = sample_pixel_center,\n                           device = device)\n'"
pyredner/sampler_type.py,0,b'import redner\n\nclass SamplerType:\n    def __init__(self):\n        self.independent = redner.SamplerType.independent\n        self.sobol = redner.SamplerType.sobol\n\nsampler_type = SamplerType()\n'
pyredner/save_obj.py,0,"b'import pyredner\nfrom typing import Union\nimport os\n\ndef save_obj(shape: Union[pyredner.Object, pyredner.Shape],\n             filename: str,\n             flip_tex_coords = True):\n    """"""\n        Save to a Wavefront obj file from an Object or a Shape.\n\n        Args\n        ====\n        shape: Union[pyredner.Object, pyredner.Shape]\n\n        filename: str\n\n        flip_tex_coords: bool\n            flip the v coordinate of uv by applying v\' = 1 - v\n    """"""\n    directory = os.path.dirname(filename)\n    if directory != \'\' and not os.path.exists(directory):\n        os.makedirs(directory)\n        \n    with open(filename, \'w\') as f:\n        vertices = shape.vertices.data.cpu().numpy()\n        uvs = shape.uvs.cpu().numpy() if shape.uvs is not None else None\n        normals = shape.normals.data.cpu().numpy() if shape.normals is not None else None\n        for i in range(vertices.shape[0]):\n            f.write(\'v {} {} {}\\n\'.format(vertices[i, 0], vertices[i, 1], vertices[i, 2]))\n        if uvs is not None:\n            for i in range(uvs.shape[0]):\n                if flip_tex_coords:\n                    f.write(\'vt {} {}\\n\'.format(uvs[i, 0], 1 - uvs[i, 1]))\n                else:\n                    f.write(\'vt {} {}\\n\'.format(uvs[i, 0], uvs[i, 1]))\n        if normals is not None:\n            for i in range(normals.shape[0]):\n                f.write(\'vn {} {} {}\\n\'.format(normals[i, 0], normals[i, 1], normals[i, 2]))\n        indices = shape.indices.data.cpu().numpy() + 1\n        uv_indices = shape.uv_indices.data.cpu().numpy() + 1 if shape.uv_indices is not None else None\n        normal_indices = shape.normal_indices.data.cpu().numpy() + 1 if shape.normal_indices is not None else None\n        for i in range(indices.shape[0]):\n            vi = (indices[i, 0], indices[i, 1], indices[i, 2])\n            if uv_indices is not None:\n                uvi = (uv_indices[i, 0], uv_indices[i, 1], uv_indices[i, 2])\n            else:\n                if uvs is not None:\n                    uvi = vi\n                else:\n                    uvi = (\'\', \'\', \'\')\n            if normal_indices is not None:\n                ni = (normal_indices[i, 0], normal_indices[i, 1], normal_indices[i, 2])\n            else:\n                if normals is not None:\n                    ni = vi\n                else:\n                    ni = (\'\', \'\', \'\')\n            if normals is not None:\n                f.write(\'f {}/{}/{} {}/{}/{} {}/{}/{}\\n\'.format(\\\n                    vi[0], uvi[0], ni[0],\n                    vi[1], uvi[1], ni[1],\n                    vi[2], uvi[2], ni[2]))\n            elif uvs is not None:\n                f.write(\'f {}/{} {}/{} {}/{}\\n\'.format(\\\n                    vi[0], uvi[0],\n                    vi[1], uvi[1],\n                    vi[2], uvi[2]))\n            else:\n                f.write(\'f {} {} {}\\n\'.format(\\\n                    vi[0],\n                    vi[1],\n                    vi[2]))\n'"
pyredner/scene.py,0,"b'import pyredner\nimport torch\nfrom typing import Optional, List\n\nclass Scene:\n    """"""\n        A scene is a collection of camera, geometry, materials, and light.\n        Currently there are two ways to construct a scene: one is through\n        lists of Shape, Material, and AreaLight. The other one is through\n        a list of Object. It is more recommended to use the Object construction.\n        The Shape/Material/AreaLight options are here for legacy issue.\n\n        Args\n        ====\n            shapes: List[pyredner.Shape] = [],\n            materials: List[pyredner.Material] = [],\n            area_lights: List[pyredner.AreaLight] = [],\n            objects: Optional[List[pyredner.Object]] = None,\n            envmap: Optional[pyredner.EnvironmentMap] = None\n    """"""\n    def __init__(self,\n                 camera: pyredner.Camera,\n                 shapes: List[pyredner.Shape] = [],\n                 materials: List[pyredner.Material] = [],\n                 area_lights: List[pyredner.AreaLight] = [],\n                 objects: Optional[List[pyredner.Object]] = None,\n                 envmap: Optional[pyredner.EnvironmentMap] = None):\n        self.camera = camera\n        self.envmap = envmap\n        if objects is None:\n            self.shapes = shapes\n            self.materials = materials\n            self.area_lights = area_lights\n        else:\n            # Convert objects to shapes/materials/lights\n            shapes = []\n            materials = []\n            area_lights = []\n            material_dict = {}\n            current_material_id = 0\n            for obj in objects:\n                mid = -1\n                if obj.material in material_dict:\n                    mid = material_dict[obj.material]\n                else:\n                    mid = current_material_id\n                    material_dict[obj.material] = current_material_id\n                    materials.append(obj.material)\n                    current_material_id += 1\n                if obj.light_intensity is not None:\n                    current_shape_id = len(shapes)\n                    area_light = pyredner.AreaLight(shape_id = current_shape_id,\n                                                    intensity = obj.light_intensity,\n                                                    two_sided = obj.light_two_sided)\n                    area_lights.append(area_light)\n                shape = pyredner.Shape(vertices = obj.vertices,\n                                       indices = obj.indices,\n                                       material_id = mid,\n                                       uvs = obj.uvs,\n                                       normals = obj.normals,\n                                       uv_indices = obj.uv_indices,\n                                       normal_indices = obj.normal_indices,\n                                       colors = obj.colors)\n                shapes.append(shape)\n            self.shapes = shapes\n            self.materials = materials\n            self.area_lights = area_lights\n\n    def state_dict(self):\n        return {\n            \'camera\': self.camera.state_dict(),\n            \'shapes\': [s.state_dict() for s in self.shapes],\n            \'materials\': [m.state_dict() for m in self.materials],\n            \'area_lights\': [l.state_dict() for l in self.area_lights],\n            \'envmap\': self.envmap.state_dict() if self.envmap is not None else None\n        }\n\n    @classmethod\n    def load_state_dict(cls, state_dict):\n        envmap_dict = state_dict[\'envmap\']\n        return cls(\n            pyredner.Camera.load_state_dict(state_dict[\'camera\']),\n            [pyredner.Shape.load_state_dict(s) for s in state_dict[\'shapes\']],\n            [pyredner.Material.load_state_dict(m) for m in state_dict[\'materials\']],\n            [pyredner.AreaLight.load_state_dict(l) for l in state_dict[\'area_lights\']],\n            pyredner.EnvironmentMap.load_state_dict(envmap_dict) if envmap_dict is not None else None)\n'"
pyredner/shape.py,0,"b'import pyredner\nimport torch\nimport math\nimport redner\nfrom typing import Optional\n\ndef compute_vertex_normal(vertices: torch.Tensor,\n                          indices: torch.Tensor,\n                          weighting_scheme: str = \'max\'):\n    """"""\n        Compute vertex normal by weighted average of nearby face normals.\n        Args\n        ====\n        vertices: torch.Tensor\n            3D position of vertices.\n            float32 tensor with size num_vertices x 3\n        indices: torch.Tensor\n            Vertex indices of triangle faces.\n            int32 tensor with size num_triangles x 3\n        weighting_scheme: str\n            How do we compute the weighting. Currently we support two weighting methods:\n            \'max\' and \'cotangent\'.\n            \'max\' corresponds to Nelson Max\'s algorithm that uses the inverse length and sine of the angle as the weight\n            (see `Weights for Computing Vertex Normals from Facet Vectors <https://escholarship.org/content/qt7657d8h3/qt7657d8h3.pdf?t=ptt283>`_),\n            \'cotangent\' corresponds to weights derived through a discretization of the gradient of triangle area\n            (see, e.g., ""Implicit Fairing of Irregular Meshes using Diffusion and Curvature Flow"" from Desbrun et al.)\n\n        Returns\n        =======\n        torch.Tensor\n            float32 Tensor with size num_vertices x 3 representing vertex normal\n    """"""\n\n    def dot(v1, v2):\n        return torch.sum(v1 * v2, dim = 1)\n    def squared_length(v):\n        return torch.sum(v * v, dim = 1)\n    def length(v):\n        return torch.sqrt(squared_length(v))\n    def safe_asin(v):\n        # Hack: asin(1)\' is infinite, so we want to clamp the contribution\n        return torch.asin(v.clamp(0, 1-1e-6))\n\n    # XXX: This whole thing is inefficient but it\'s PyTorch\'s limitation\n\n    normals = torch.zeros(vertices.shape, dtype = torch.float32, device = vertices.device)\n    v = [vertices[indices[:, 0].long(), :],\n         vertices[indices[:, 1].long(), :],\n         vertices[indices[:, 2].long(), :]]\n    if weighting_scheme == \'max\':\n        for i in range(3):\n            v0 = v[i]\n            v1 = v[(i + 1) % 3]\n            v2 = v[(i + 2) % 3]\n            e1 = v1 - v0\n            e2 = v2 - v0\n            e1_len = length(e1)\n            e2_len = length(e2)\n            side_a = e1 / torch.reshape(e1_len, [-1, 1])\n            side_b = e2 / torch.reshape(e2_len, [-1, 1])\n            if i == 0:\n                n = torch.cross(side_a, side_b)\n                n = torch.where(length(n).reshape(-1, 1).expand(-1, 3) > 0,\n                    n / torch.reshape(length(n), [-1, 1]),\n                    torch.zeros(n.shape, dtype=n.dtype, device=n.device))\n            # numerically stable angle between two unit direction vectors\n            # http://www.plunk.org/~hatch/rightway.php\n            angle = torch.where(dot(side_a, side_b) < 0,\n                torch.tensor(math.pi) - 2.0 * safe_asin(0.5 * length(side_a + side_b)),\n                2.0 * safe_asin(0.5 * length(side_b - side_a)))\n            sin_angle = torch.sin(angle)\n            e1e2 = e1_len * e2_len\n            # contrib is 0 when e1e2 is 0\n            contrib = torch.where(e1e2.reshape(-1, 1).expand(-1, 3) > 0,\n                n * (sin_angle / e1e2).reshape(-1, 1).expand(-1, 3),\n                torch.zeros(n.shape, dtype=torch.float32, device=vertices.device))\n            index = indices[:, i].long().reshape(-1, 1).expand(-1, 3)\n            normals.scatter_add_(0, index, contrib)\n        # Assign 0, 0, 1 to degenerate faces\n        degenerate_normals = torch.zeros(normals.shape, dtype = torch.float32, device = vertices.device)\n        degenerate_normals[:, 2] = 1.0\n        normals = torch.where(length(normals).reshape(-1, 1).expand(-1, 3) > 0,\n            normals / torch.reshape(length(normals), [-1, 1]),\n            degenerate_normals)\n    elif weighting_scheme == \'cotangent\':\n        # Cotangent weighting generates 0-length normal when\n        # the local surface is planar. Prepare weighted average normal\n        # computed using Nelson Max\'s algorithm for those cases.\n        max_normal = compute_vertex_normal(vertices, indices, \'max\')\n        for i in range(3):\n            # Loop over each pair of edges sharing the same vertex,\n            # compute the cotangent and contribute to the third edge.\n            v0 = v[i]\n            v1 = v[(i + 1) % 3]\n            v2 = v[(i + 2) % 3]\n            e1 = v1 - v0\n            e2 = v2 - v0\n            e1_len = length(e1)\n            e2_len = length(e2)\n            side_a = e1 / torch.reshape(e1_len, [-1, 1])\n            side_b = e2 / torch.reshape(e2_len, [-1, 1])\n            if i == 0:\n                n = torch.cross(side_a, side_b)\n                n = torch.where(length(n).reshape(-1, 1).expand(-1, 3) > 0,\n                    n / torch.reshape(length(n), [-1, 1]),\n                    torch.zeros(n.shape, dtype=n.dtype, device=n.device))\n            # numerically stable angle between two unit direction vectors\n            # http://www.plunk.org/~hatch/rightway.php\n            angle = torch.where(dot(side_a, side_b) < 0,\n                torch.tensor(math.pi) - 2.0 * safe_asin(0.5 * length(side_a + side_b)),\n                2.0 * safe_asin(0.5 * length(side_b - side_a)))\n            cotangent = torch.tensor(1.0) / torch.tan(angle)\n            v1_index = indices[:, (i + 1) % 3].long().reshape(-1, 1).expand(-1, 3)\n            v2_index = indices[:, (i + 2) % 3].long().reshape(-1, 1).expand(-1, 3)\n            contrib = (v2 - v1) * cotangent.reshape([-1, 1])\n            normals.scatter_add_(0, v1_index, contrib)\n            normals.scatter_add_(0, v2_index, -contrib)\n        # Make sure the normals are pointing at the right direction\n        normals = torch.where(dot(normals, max_normal).reshape(-1, 1).expand(-1, 3) > 0, normals, -normals)\n        normals = torch.where(length(normals).reshape(-1, 1).expand(-1, 3) > 0.05,\n            normals / torch.reshape(length(normals), [-1, 1]),\n            max_normal)\n    else:\n        assert False, \'Unknown weighting scheme: {}\'.format(weighting_scheme)\n\n    assert(torch.isfinite(normals).all())\n    return normals.contiguous()\n\n\ndef bound_vertices(vertices: torch.Tensor, indices: torch.Tensor):\n    """"""\n        Calculate the indices of boundary vertices of a mesh\n        and express it in Tensor form.\n\n        Args\n        ====\n        vertices: torch.Tensor\n            3D position of vertices.\n            float32 tensor with size num_vertices x 3\n        indices: torch.Tensor\n            Vertex indices of triangle faces.\n            int32 tensor with size num_triangles x 3\n\n        Returns\n        =======\n        bound: torch.Tensor\n            float32 Tensor with size num_vertices representing vertex normal\n            bound[i] = 0. if i-th vertices is on boundary of mesh; else 1.\n    """"""\n    neighbor_sum = torch.zeros(vertices.size(0), device=vertices.device)\n    for i in range(3):\n        contrib = indices[:, (i + 2) % 3] - indices[:, (i + 1) % 3]\n        index = indices[:, i].long()\n        neighbor_sum.scatter_add_(0, index, contrib.float())\n        # neighbor_sum[index[i]] += contrib[i]\n    return torch.where(neighbor_sum == 0,\n                       torch.ones(vertices.size(0), device=vertices.device),\n                       torch.zeros(vertices.size(0), device=vertices.device))\n\ndef smooth(vertices: torch.Tensor,\n                     indices: torch.Tensor,\n                     lmd: torch.float32,\n                     weighting_scheme: str = \'reciprocal\',\n                     control: torch.Tensor = None):\n    """"""\n        Update positions of vertices in a mesh. The shift amount of a vertex equals\n        to lmd times weight sum of all edges to neighbors.\n\n        $v_i += lmd * \\frac {\\sum_{j \\in neighbors(i)} w_{ij}(v_j - v_i)} {\\sum_{j \\in neighbors(i)} w_{ij}}$\n\n        Args\n        ====\n        vertices: torch.Tensor\n            3D position of vertices.\n            float32 tensor with size num_vertices x 3\n        indices: torch.Tensor\n            Vertex indices of triangle faces.\n            int32 tensor with size num_triangles x 3\n        lmd: torch.float32\n            step length coefficient\n        weighting_scheme: str = \'reciprocal\'\n            Different weighting schemes:\n                \'reciprocal\': (default)\n                    w[i][j] = 1 / len(v[j] - v[i])\n                \'uniform\':\n                    w[i][j] = 1\n                \'cotangent\':\n                    w[i][j] = cot(angle(i-m-j)) + cot(angle(i-n-j))\n                    m and n are vertices that form triangles with i and j\n        control: torch.Tensor\n            extra coefficient deciding which vertices to be update.\n            In default case, do not update boundary vertices of the mesh\n                control (default) = bound_vertices(vertices, indices)\n            type help(pyredner.bound_vertices)\n    """"""\n    if control is None:\n        control = bound_vertices(vertices, indices)\n    else:\n        assert control.numel() == vertices.size(0), \'Size of control tensor inconsistent with number of vertices\'\n\n    def dot(v1, v2):\n        return torch.sum(v1 * v2, dim=1)\n\n    def squared_length(v):\n        return torch.sum(v * v, dim=1)\n\n    def length(v):\n        return torch.sqrt(squared_length(v))\n\n    def safe_asin(v):\n        # Hack: asin(1)\' is infinite, so we want to clamp the contribution\n        return torch.asin(v.clamp(0, 1 - 1e-6))\n\n\n    total_contrib = torch.zeros(vertices.shape, dtype=torch.float32, device=vertices.device)\n    total_weight_contrib = torch.zeros(vertices.shape, dtype=torch.float32, device=vertices.device)\n\n    v = [vertices[indices[:, 0].long(), :],\n         vertices[indices[:, 1].long(), :],\n         vertices[indices[:, 2].long(), :]]\n    for i in range(3):\n        v0 = v[i]\n        v1 = v[(i + 1) % 3]\n        v2 = v[(i + 2) % 3]\n        e1 = v1 - v0\n        e2 = v2 - v0\n        e1_len = length(e1)\n        e2_len = length(e2)\n\n        # XXX: Inefficient but it\'s PyTorch\'s limitation\n        e1e2 = e1_len * e2_len\n        # contrib is 0 when e1e2 is 0\n\n        if weighting_scheme == \'reciprocal\':\n            contrib = torch.where(e1e2.reshape(-1, 1).expand(-1, 3) > 0,\n                                  e1 / e1_len.reshape(-1, 1).expand(-1, 3) +\n                                  e2 / e2_len.reshape(-1, 1).expand(-1, 3),\n                                  torch.zeros(v0.shape, dtype=torch.float32, device=vertices.device))\n            weight_contrib = torch.where(e1e2.reshape(-1, 1).expand(-1, 3) > 0,\n                                         torch.tensor(1.) / e1_len.reshape(-1, 1).expand(-1, 3) +\n                                         torch.tensor(1.) / e2_len.reshape(-1, 1).expand(-1, 3),\n                                         torch.zeros(v0.shape, dtype=torch.float32, device=vertices.device))\n            index = indices[:, i].long().reshape(-1, 1).expand(-1, 3)\n            total_contrib.scatter_add_(0, index, contrib)\n            total_weight_contrib.scatter_add_(0, index, weight_contrib)\n        elif weighting_scheme == \'uniform\':\n            contrib = torch.where(e1e2.reshape(-1, 1).expand(-1, 3) > 0,\n                                  e1 + e2,\n                                  torch.zeros(v0.shape, dtype=torch.float32, device=vertices.device))\n            weight_contrib = torch.where(e1e2.reshape(-1, 1).expand(-1, 3) > 0,\n                                         2 * torch.ones(v0.shape, dtype=torch.float32, device=vertices.device),\n                                         torch.zeros(v0.shape, dtype=torch.float32, device=vertices.device))\n            index = indices[:, i].long().reshape(-1, 1).expand(-1, 3)\n            total_contrib.scatter_add_(0, index, contrib)\n            total_weight_contrib.scatter_add_(0, index, weight_contrib)\n        elif weighting_scheme == \'cotangent\':\n            pass\n            side_a = e1 / torch.reshape(e1_len, [-1, 1])\n            side_b = e2 / torch.reshape(e2_len, [-1, 1])\n            angle = torch.where(dot(side_a, side_b) < 0,\n                                torch.tensor(math.pi) - 2.0 * safe_asin(0.5 * length(side_a + side_b)),\n                                2.0 * safe_asin(0.5 * length(side_b - side_a)))\n            cotangent = torch.tensor(1.0) / torch.tan(angle)\n            v1_index = indices[:, (i + 1) % 3].long().reshape(-1, 1).expand(-1, 3)\n            v2_index = indices[:, (i + 2) % 3].long().reshape(-1, 1).expand(-1, 3)\n            contrib = (v2 - v1) * cotangent.reshape([-1, 1])\n            weight_contrib = cotangent.reshape([-1, 1]).expand(-1, 3)\n            total_contrib.scatter_add_(0, v1_index, contrib)\n            total_contrib.scatter_add_(0, v2_index, -contrib)\n            total_weight_contrib.scatter_add_(0, v1_index, weight_contrib)\n            total_weight_contrib.scatter_add_(0, v2_index, weight_contrib)\n        else:\n            assert False, \'Unknown weighting_scheme: {}\'.format(weighting_scheme)\n\n    shift = total_contrib / total_weight_contrib * control.reshape(-1, 1)\n    vertices.data += shift * lmd\n    return\n\ndef compute_uvs(vertices, indices, print_progress = True):\n    """"""\n        Compute UV coordinates of a given mesh using a charting algorithm\n        with least square conformal mapping. This calls the `xatlas <https://github.com/jpcy/xatlas>`_ library.\n        Args\n        ====\n        vertices: torch.Tensor\n            3D position of vertices\n            float32 tensor with size num_vertices x 3\n        indices: torch.Tensor\n            vertex indices of triangle faces.\n            int32 tensor with size num_triangles x 3\n        Returns\n        =======\n        torch.Tensor\n            uv vertices pool, float32 Tensor with size num_uv_vertices x 3\n        torch.Tensor\n            uv indices, int32 Tensor with size num_triangles x 3\n    """"""\n    device = vertices.device\n    vertices = vertices.cpu()\n    indices = indices.cpu()\n\n    uv_trimesh = redner.UVTriMesh(redner.float_ptr(vertices.data_ptr()),\n                                  redner.int_ptr(indices.data_ptr()),\n                                  redner.float_ptr(0),\n                                  redner.int_ptr(0),\n                                  int(vertices.shape[0]),\n                                  0,\n                                  int(indices.shape[0]))\n\n    atlas = redner.TextureAtlas()\n    num_uv_vertices = redner.automatic_uv_map([uv_trimesh], atlas, print_progress)[0]\n\n    uvs = torch.zeros(num_uv_vertices, 2, dtype=torch.float32)\n    uv_indices = torch.zeros_like(indices)\n    uv_trimesh.uvs = redner.float_ptr(uvs.data_ptr())\n    uv_trimesh.uv_indices = redner.int_ptr(uv_indices.data_ptr())\n    uv_trimesh.num_uv_vertices = num_uv_vertices\n\n    redner.copy_texture_atlas(atlas, [uv_trimesh])\n\n    vertices = vertices.to(device)\n    indices = indices.to(device)\n    uvs = uvs.to(device)\n    uv_indices = uv_indices.to(device)\n    return uvs, uv_indices\n\nclass Shape:\n    """"""\n        redner supports only triangle meshes for now. It stores a pool of\n        vertices and access the pool using integer index. Some times the\n        two vertices can have the same 3D position but different texture\n        coordinates, because UV mapping creates seams and need to duplicate\n        vertices. In this can we can use an additional ""uv_indices"" array\n        to access the uv pool.\n        Args\n        ====\n        vertices: torch.Tensor\n            3D position of vertices\n            float32 tensor with size num_vertices x 3\n        indices: torch.Tensor\n            vertex indices of triangle faces.\n            int32 tensor with size num_triangles x 3\n        uvs: Optional[torch.Tensor]:\n            optional texture coordinates.\n            float32 tensor with size num_uvs x 2\n            doesn\'t need to be the same size with vertices if uv_indices is not None\n        normals: Optional[torch.Tensor]\n            shading normal\n            float32 tensor with size num_normals x 3\n            doesn\'t need to be the same size with vertices if normal_indices is not None\n        uv_indices: Optional[torch.Tensor]\n            overrides indices when accessing uv coordinates\n            int32 tensor with size num_uvs x 2\n        normal_indices: Optional[torch.Tensor]\n            overrides indices when accessing shading normals\n            int32 tensor with size num_normals x 2\n    """"""\n    def __init__(self,\n                 vertices: torch.Tensor,\n                 indices: torch.Tensor,\n                 material_id: int,\n                 uvs: Optional[torch.Tensor] = None,\n                 normals: Optional[torch.Tensor] = None,\n                 uv_indices: Optional[torch.Tensor] = None,\n                 normal_indices: Optional[torch.Tensor] = None,\n                 colors: Optional[torch.Tensor] = None):\n        assert(vertices.dtype == torch.float32)\n        assert(vertices.is_contiguous())\n        assert(len(vertices.shape) == 2 and vertices.shape[1] == 3)\n        assert(indices.dtype == torch.int32)\n        assert(indices.is_contiguous())\n        assert(len(indices.shape) == 2 and indices.shape[1] == 3)\n        if uvs is not None:\n            assert(uvs.dtype == torch.float32)\n            assert(uvs.is_contiguous())\n            assert(len(uvs.shape) == 2 and uvs.shape[1] == 2)\n        if normals is not None:\n            assert(normals.dtype == torch.float32)\n            assert(normals.is_contiguous())\n            assert(len(normals.shape) == 2 and normals.shape[1] == 3)\n        if uv_indices is not None:\n            assert(uv_indices.dtype == torch.int32)\n            assert(uv_indices.is_contiguous())\n            assert(len(uv_indices.shape) == 2 and uv_indices.shape[1] == 3)\n        if normal_indices is not None:\n            assert(normal_indices.dtype == torch.int32)\n            assert(normal_indices.is_contiguous())\n            assert(len(normal_indices.shape) == 2 and normal_indices.shape[1] == 3)\n        if colors is not None:\n            assert(colors.dtype == torch.float32)\n            assert(colors.is_contiguous())\n            assert(len(colors.shape) == 2 and colors.shape[1] == 3)\n\n        self.vertices = vertices\n        self.indices = indices\n        self.material_id = material_id\n        self.uvs = uvs\n        self.normals = normals\n        self.uv_indices = uv_indices\n        self.normal_indices = normal_indices\n        self.colors = colors\n        self.light_id = -1\n\n    def state_dict(self):\n        return {\n            \'vertices\': self.vertices,\n            \'indices\': self.indices,\n            \'material_id\': self.material_id,\n            \'light_id\': self.light_id,\n            \'uvs\': self.uvs,\n            \'normals\': self.normals,\n            \'uv_indices\': self.uv_indices,\n            \'normal_indices\': self.normal_indices,\n            \'colors\': self.colors\n        }\n\n    @classmethod\n    def load_state_dict(cls, state_dict):\n        out = cls(\n            state_dict[\'vertices\'],\n            state_dict[\'indices\'],\n            state_dict[\'material_id\'],\n            state_dict[\'uvs\'],\n            state_dict[\'normals\'],\n            state_dict[\'uv_indices\'],\n            state_dict[\'normal_indices\'],\n            state_dict[\'colors\'])\n        out.light_id = state_dict[\'light_id\']\n        return out\n'"
pyredner/texture.py,0,"b'import torch\nimport numpy as np\nimport pyredner\nimport torch\nimport enum\nimport math\nfrom typing import Optional\n\nclass Texture:\n    """"""\n        Representing a texture and its mipmap.\n\n        Args\n        ====\n        texels: torch.Tensor\n            a float32 tensor with size C or [height, width, C]\n        uv_scale: Optional[torch.Tensor]\n            scale the uv coordinates when mapping the texture\n            a float32 tensor with size 2\n    """"""\n\n    def __init__(self,\n                 texels: torch.Tensor,\n                 uv_scale: Optional[torch.Tensor] = None):\n        if uv_scale is None:\n            uv_scale = torch.tensor([1.0, 1.0], device = pyredner.get_device())\n        assert(texels.dtype == torch.float32)\n        assert(uv_scale.dtype == torch.float32)\n        assert(uv_scale.is_contiguous())\n        self._texels = texels\n        self.uv_scale = uv_scale\n        self.generate_mipmap()\n\n    def generate_mipmap(self):\n        texels = self._texels\n        if len(texels.shape) >= 2:\n            # Build a mipmap for texels\n            width = max(texels.shape[0], texels.shape[1])\n            num_levels = min(math.ceil(math.log(width, 2) + 1), 8)\n            num_channels = texels.shape[2]\n            box_filter = torch.ones(num_channels, 1, 2, 2,\n                device = texels.device) / 4.0\n\n            # Convert from HWC to NCHW\n            mipmap = [texels.contiguous()]\n            base_level = texels.unsqueeze(0).permute(0, 3, 1, 2)\n            prev_lvl = base_level\n            for l in range(1, num_levels):\n                # Pad for circular boundary condition\n                current_lvl = torch.nn.functional.pad(\\\n                    input = prev_lvl,\n                    pad = (0, 1, 0, 1),\n                    mode = \'circular\')\n                # Convolve with a box filter\n                current_lvl = torch.nn.functional.conv2d(\\\n                    current_lvl, box_filter,\n                    groups = num_channels)\n                # Downsample\n                next_size = (max(current_lvl.shape[2] // 2, 1),\n                             max(current_lvl.shape[3] // 2, 1))\n                current_lvl = torch.nn.functional.interpolate(\\\n                    current_lvl, size = next_size, mode = \'area\')\n                # NCHW -> CHW -> HWC\n                mipmap.append(current_lvl.squeeze(0).permute(1, 2, 0).contiguous())\n                prev_lvl = current_lvl\n        else:\n            mipmap = [texels]\n\n        self.mipmap = mipmap\n\n    @property\n    def texels(self):\n        return self._texels\n\n    @texels.setter\n    def texels(self, value):\n        self._texels = value\n        self.generate_mipmap()\n\n    @property\n    def device(self):\n        return self.texels.device\n\n    def state_dict(self):\n        return {\n            \'texels\': self.texels,\n            \'mipmap\': self.mipmap,\n            \'uv_scale\': self.uv_scale\n        }\n\n    @classmethod\n    def load_state_dict(cls, state_dict):\n        out = cls.__new__(Texture)\n        out.texels = state_dict[\'texels\']\n        out.mipmap = state_dict[\'mipmap\']\n        out.uv_scale = state_dict[\'uv_scale\'].to(torch.device(\'cpu\'))\n        return out\n'"
pyredner/transform.py,0,"b'import math\nimport numpy as np\nimport torch\n\ndef radians(deg):\n    return (math.pi / 180.0) * deg\n\ndef normalize(v):\n    return v / torch.norm(v)\n\ndef gen_look_at_matrix(pos, look, up):\n    d = normalize(look - pos)\n    right = normalize(torch.cross(d, normalize(up)))\n    new_up = normalize(torch.cross(right, d))\n    z = torch.zeros([1], dtype=torch.float32, device = d.device)\n    o = torch.ones([1], dtype=torch.float32, device = d.device)\n    return torch.transpose(torch.stack([torch.cat([right , z], 0),\n                                        torch.cat([new_up, z], 0),\n                                        torch.cat([d     , z], 0),\n                                        torch.cat([pos   , o], 0)]), 0, 1).contiguous()\n\ndef gen_scale_matrix(scale):\n    o = torch.ones([1], dtype=torch.float32, device = scale.device)\n    return torch.diag(torch.cat([scale, o], 0))\n\ndef gen_translate_matrix(translate):\n    z = torch.zeros([1], dtype=torch.float32, device = translate.device)\n    o = torch.ones([1], dtype=torch.float32, device = translate.device)\n    return torch.stack([torch.cat([o, z, z, translate[0:1]], 0),\n                        torch.cat([z, o, z, translate[1:2]], 0),\n                        torch.cat([z, z, o, translate[2:3]], 0),\n                        torch.cat([z, z, z, o], 0)])\n\ndef gen_perspective_matrix(fov, clip_near, clip_far):\n    clip_dist = clip_far - clip_near\n    cot = 1 / torch.tan(radians(fov / 2.0))\n    z = torch.zeros([1], dtype=torch.float32, device = clip_near.device)\n    o = torch.ones([1], dtype=torch.float32, device = clip_near.device)\n    return torch.stack([torch.cat([cot,   z,             z,                       z], 0),\n                        torch.cat([  z, cot,             z,                       z], 0),\n                        torch.cat([  z,   z, 1 / clip_dist, - clip_near / clip_dist], 0),\n                        torch.cat([  z,   z,             o,                       z], 0)])\n\ndef gen_rotate_matrix(angles: torch.Tensor):\n    """"""\n        Given a 3D Euler angle vector, outputs a rotation matrix.\n\n        Args\n        ====\n            angles: torch.Tensor\n                3D Euler angle\n\n        Returns\n        =======\n            torch.Tensor\n                3x3 rotation matrix\n    """"""\n\n    theta = angles[0]\n    phi = angles[1]\n    psi = angles[2]\n    rot_x = torch.zeros((3, 3), device=angles.device, dtype=torch.float32)\n    rot_y = torch.zeros((3, 3), device=angles.device, dtype=torch.float32)\n    rot_z = torch.zeros((3, 3), device=angles.device, dtype=torch.float32)\n    rot_x[0, 0] = 1\n    rot_x[0, 1] = 0\n    rot_x[0, 2] = 0\n    rot_x[1, 0] = 0\n    rot_x[1, 1] = theta.cos()\n    rot_x[1, 2] = theta.sin()\n    rot_x[2, 0] = 0\n    rot_x[2, 1] = -theta.sin()\n    rot_x[2, 2] = theta.cos()\n    \n    rot_y[0, 0] = phi.cos()\n    rot_y[0, 1] = 0\n    rot_y[0, 2] = -phi.sin()\n    rot_y[1, 0] = 0\n    rot_y[1, 1] = 1\n    rot_y[1, 2] = 0\n    rot_y[2, 0] = phi.sin()\n    rot_y[2, 1] = 0\n    rot_y[2, 2] = phi.cos()\n    \n    rot_z[0, 0] = psi.cos()\n    rot_z[0, 1] = -psi.sin()\n    rot_z[0, 2] = 0\n    rot_z[1, 0] = psi.sin()\n    rot_z[1, 1] = psi.cos()\n    rot_z[1, 2] = 0\n    rot_z[2, 0] = 0\n    rot_z[2, 1] = 0\n    rot_z[2, 2] = 1\n    return rot_z @ (rot_y @ rot_x)\n'"
pyredner/utils.py,0,"b'import torch\nimport math\nimport numpy as np\nimport pyredner\nfrom typing import Optional\n\n####################### Spherical Harmonics utilities ########################\n# Code adapted from ""Spherical Harmonic Lighting: The Gritty Details"", Robin Green\n# http://silviojemma.com/public/papers/lighting/spherical-harmonic-lighting.pdf\ndef associated_legendre_polynomial(l, m, x):\n    pmm = torch.ones_like(x)\n    if m > 0:\n        somx2 = torch.sqrt((1 - x) * (1 + x))\n        fact = 1.0\n        for i in range(1, m + 1):\n            pmm = pmm * (-fact) * somx2\n            fact += 2.0\n    if l == m:\n        return pmm\n    pmmp1 = x * (2.0 * m + 1.0) * pmm\n    if l == m + 1:\n        return pmmp1\n    pll = torch.zeros_like(x)\n    for ll in range(m + 2, l + 1):\n        pll = ((2.0 * ll - 1.0) * x * pmmp1 - (ll + m - 1.0) * pmm) / (ll - m)\n        pmm = pmmp1\n        pmmp1 = pll\n    return pll\n\ndef SH_renormalization(l, m):\n    return math.sqrt((2.0 * l + 1.0) * math.factorial(l - m) / \\\n        (4 * math.pi * math.factorial(l + m)))\n\ndef SH(l, m, theta, phi):\n    if m == 0:\n        return SH_renormalization(l, m) * associated_legendre_polynomial(l, m, torch.cos(theta))\n    elif m > 0:\n        return math.sqrt(2.0) * SH_renormalization(l, m) * \\\n            torch.cos(m * phi) * associated_legendre_polynomial(l, m, torch.cos(theta))\n    else:\n        return math.sqrt(2.0) * SH_renormalization(l, -m) * \\\n            torch.sin(-m * phi) * associated_legendre_polynomial(l, -m, torch.cos(theta))\n\ndef SH_reconstruct(coeffs, res):\n    uv = np.mgrid[0:res[1], 0:res[0]].astype(np.float32)\n    theta = torch.from_numpy((math.pi / res[1]) * (uv[1, :, :] + 0.5))\n    phi = torch.from_numpy((2 * math.pi / res[0]) * (uv[0, :, :] + 0.5))\n    theta = theta.to(coeffs.device)\n    phi = phi.to(coeffs.device)\n    result = torch.zeros(res[1], res[0], coeffs.shape[0], device = coeffs.device)\n    num_order = int(math.sqrt(coeffs.shape[1]))\n    i = 0\n    for l in range(num_order):\n        for m in range(-l, l + 1):\n            sh_factor = SH(l, m, theta, phi)\n            result = result + sh_factor.view(sh_factor.shape[0], sh_factor.shape[1], 1) * coeffs[:, i]\n            i += 1\n    result = torch.max(result,\n        torch.zeros(res[1], res[0], coeffs.shape[0], device = coeffs.device))\n    return result\n#######################################################################################\n\ndef generate_sphere(theta_steps: int,\n                    phi_steps: int,\n                    device: Optional[torch.device] = None):\n    """"""\n        Generate a triangle mesh representing a UV sphere,\n        center at (0, 0, 0) with radius 1.\n\n        Args\n        ====\n        theta_steps: int\n            zenith subdivision\n        phi_steps: int\n            azimuth subdivision\n        device: Optional[torch.device]\n            Which device should we store the data in.\n            If set to None, use the device from pyredner.get_device().\n\n        Returns\n        =======\n        torch.Tensor\n            vertices\n        torch.Tensor\n            indices\n        torch.Tensor\n            uvs\n        torch.Tensor\n            normals\n    """"""\n    if device is None:\n        device = pyredner.get_device()\n\n    d_theta = math.pi / (theta_steps - 1)\n    d_phi = (2 * math.pi) / (phi_steps - 1)\n\n    num_vertices = theta_steps * phi_steps - 2 * (phi_steps - 1)\n    vertices = torch.zeros(num_vertices, 3, device = device)\n    uvs = torch.zeros(num_vertices, 2, device = device)\n    vertices_index = 0\n    for theta_index in range(theta_steps):\n        sin_theta = math.sin(theta_index * d_theta)\n        cos_theta = math.cos(theta_index * d_theta)\n        if theta_index == 0:\n            # For the two polars of the sphere, only generate one vertex\n            vertices[vertices_index, :] = \\\n                torch.tensor([0.0, 1.0, 0.0])\n            uvs[vertices_index, 0] = 0.0\n            uvs[vertices_index, 1] = 0.0\n            vertices_index += 1\n        elif theta_index == theta_steps - 1:\n            # For the two polars of the sphere, only generate one vertex\n            vertices[vertices_index, :] = \\\n                torch.tensor([0.0, -1.0, 0.0])\n            uvs[vertices_index, 0] = 0.0\n            uvs[vertices_index, 1] = 1.0\n            vertices_index += 1\n        else:\n            for phi_index in range(phi_steps):\n                sin_phi = math.sin(phi_index * d_phi)\n                cos_phi = math.cos(phi_index * d_phi)\n                vertices[vertices_index, :] = \\\n                    torch.tensor([sin_theta * cos_phi, cos_theta, sin_theta * sin_phi],\n                        device = device)\n                uvs[vertices_index, 0] = phi_index * d_phi / (2 * math.pi)\n                uvs[vertices_index, 1] = theta_index * d_theta / math.pi\n                vertices_index += 1\n\n    indices = []\n    for theta_index in range(1, theta_steps):\n        for phi_index in range(phi_steps - 1):\n            if theta_index < theta_steps - 1:\n                id0 = phi_steps * theta_index + phi_index - (phi_steps - 1)\n                id1 = phi_steps * theta_index + phi_index + 1 - (phi_steps - 1)\n            else:\n                # There is only one vertex at the pole\n                assert(theta_index == theta_steps - 1)\n                id0 = num_vertices - 1\n                id1 = num_vertices - 1\n            if theta_index > 1:\n                id2 = phi_steps * (theta_index - 1) + phi_index - (phi_steps - 1)\n                id3 = phi_steps * (theta_index - 1) + phi_index + 1 - (phi_steps - 1)\n            else:\n                # There is only one vertex at the pole\n                assert(theta_index == 1)\n                id2 = 0\n                id3 = 0\n\n            if (theta_index < theta_steps - 1):\n                indices.append([id0, id2, id1])\n            if (theta_index > 1):\n                indices.append([id1, id2, id3])\n\n    indices = torch.tensor(indices, dtype = torch.int32, device = device)\n\n    normals = vertices.clone()\n    return (vertices, indices, uvs, normals)\n\ndef generate_quad_light(position: torch.Tensor,\n                        look_at: torch.Tensor,\n                        size: torch.Tensor,\n                        intensity: torch.Tensor):\n    """"""\n        Generate a pyredner.Object that is a quad light source.\n\n        Args\n        ====\n        position: torch.Tensor\n            1-d tensor of size 3\n        look_at: torch.Tensor\n            1-d tensor of size 3\n        size: torch.Tensor\n            1-d tensor of size 2\n        intensity: torch.Tensor\n            1-d tensor of size 3\n\n        Returns\n        =======\n        pyredner.Object\n            quad light source\n    """"""\n    d = look_at - position\n    d = d / torch.norm(d)\n    # ONB -- generate two axes that are orthogonal to d\n    a = 1 / (1 + d[2])\n    b = -d[0] * d[1] * a\n    x = torch.where(d[2] < (-1 + 1e-6),\n                    torch.tensor([0.0, -1.0, 0.0], device = d.device),\n                    torch.stack([1 - d[0] * d[0] * a, b, -d[0]]))\n    y = torch.where(d[2] < (-1 + 1e-6),\n                    torch.tensor([-1.0, 0.0, 0.0], device = d.device),\n                    torch.stack([b, 1 - d[1] * d[1] * a, -d[1]]))\n    v0 = position - x * size[0] * 0.5 - y * size[1] * 0.5\n    v1 = position + x * size[0] * 0.5 - y * size[1] * 0.5\n    v2 = position - x * size[0] * 0.5 + y * size[1] * 0.5\n    v3 = position + x * size[0] * 0.5 + y * size[1] * 0.5\n\n    vertices = torch.stack((v0, v1, v2, v3), dim = 0).to(d.device)\n    indices = torch.tensor([[0, 1, 2],[1, 3, 2]],\n        dtype = torch.int32, device = d.device)\n    m = pyredner.Material(diffuse_reflectance = torch.tensor([0.0, 0.0, 0.0], device = d.device))\n    return pyredner.Object(vertices = vertices,\n                           indices = indices,\n                           material = m,\n                           light_intensity = intensity)\n\ndef linear_to_srgb(x):\n    return torch.where(x <= 0.0031308, 12.92 * x, 1.055 * torch.pow(x, 1.0 / 2.4) - 0.055)\n\ndef srgb_to_linear(x):\n    return torch.where(x <= 0.04045, x / 12.92, torch.pow((x + 0.055) / 1.055, 2.4))\n'"
pyredner_tensorflow/__init__.py,4,"b'import tensorflow as tf\ntry:\n  import redner\nexcept ImportError:\n  print(""Warning: redner is not installed when you import pyredner_tensorflow.""\n        "" Please install redner, by following instructions at https://github.com/BachiLi/redner/wiki""\n        "" or by using pip install redner-gpu or pip install redner."")\nfrom .device import *\nfrom .camera_type import *\nfrom .camera import *\nfrom .shape import *\nfrom .texture import *\nfrom .material import *\nfrom .area_light import *\nfrom .object import *\nfrom .envmap import *\nfrom .scene import *\nfrom .image import *\nfrom .load_obj import load_obj\nfrom .save_obj import save_obj\nfrom .utils import *\nfrom .load_mitsuba import load_mitsuba\nfrom .transform import *\nfrom .channels import *\nfrom .sampler_type import *\nfrom .render_utils import *\nfrom .render_tensorflow import *\nfrom .geometry_images import *\nimport os.path\n\nif tf.__cxx11_abi_flag__ == 0:\n    __data_ptr_module = tf.load_op_library(os.path.join(os.path.dirname(redner.__file__), \'libredner_tf_data_ptr_no_cxx11_abi.so\'))\nelse:\n    assert(tf.__cxx11_abi_flag__ == 1)\n    __data_ptr_module = tf.load_op_library(os.path.join(os.path.dirname(redner.__file__), \'libredner_tf_data_ptr_cxx11_abi.so\'))\n\ndef data_ptr(tensor):    \n    addr_as_uint64 = __data_ptr_module.data_ptr(tensor)\n    return int(addr_as_uint64)\n'"
pyredner_tensorflow/area_light.py,2,"b'import pyredner_tensorflow as pyredner\nimport tensorflow as tf\n\nclass AreaLight:\n    """"""\n        A mesh-based area light that points to a shape and assigns intensity.\n\n        Args\n        ----------\n        shape_id: int\n\n        intensity: tf.Tensor\n            1-d tensor with size 3 and type float32\n        two_sided: bool\n            is the light emitting light from the two sides of the faces?\n        directly_visible: bool\n            can the camera sees the light source directly?\n    """"""\n\n    def __init__(self,\n                 shape_id: int,\n                 intensity: tf.Tensor,\n                 two_sided: bool = False,\n                 directly_visible: bool = True):\n        self.shape_id = shape_id\n        self.intensity = intensity\n        self.two_sided = two_sided\n        self.directly_visible = directly_visible\n\n    def state_dict(self):\n        return {\n            \'shape_id\': self.shape_id,\n            \'intensity\': self.intensity,\n            \'two_sided\': self.two_sided,\n            \'directly_visible\': self.directly_visible\n        }\n\n    @classmethod\n    def load_state_dict(cls, state_dict):\n        return cls(\n            state_dict[\'shape_id\'],\n            state_dict[\'intensity\'],\n            state_dict[\'two_sided\'],\n            state_dict[\'directly_visible\'])\n'"
pyredner_tensorflow/camera.py,71,"b'import tensorflow as tf\nimport pyredner_tensorflow.transform as transform\nimport redner\nimport pyredner_tensorflow as pyredner\nimport math\nfrom typing import Tuple, Optional, List\n\nclass Camera:\n    """"""\n        redner supports four types of cameras: perspective, orthographic, fisheye, and panorama.\n        The camera takes a look at transform or a cam_to_world matrix to\n        transform from camera local space to world space. It also can optionally\n        take an intrinsic matrix that models field of view and camera skew.\n\n        Args\n        ----------\n            position: Optional[tf.Tensor]\n                the origin of the camera, 1-d tensor with size 3 and type float32\n            look_at: Optional[tf.Tensor]\n                the point camera is looking at, 1-d tensor with size 3 and type float32\n            up: Optional[tf.tensor]\n                the up vector of the camera, 1-d tensor with size 3 and type float32\n            fov: Optional[tf.Tensor]\n                the field of view of the camera in angle\n                no effect if the camera is a fisheye or panorama camera\n                1-d tensor with size 1 and type float32\n            clip_near: float\n                the near clipping plane of the camera, need to > 0\n            resolution: Tuple[int, int]\n                the size of the output image in (height, width)\n            viewport: Optional[Tuple[int, int, int, int]]\n                optional viewport argument for rendering only a region of an image in\n                (left_top_y, left_top_x, bottom_right_y, bottom_right_x),\n                bottom_right is not inclusive.\n                if set to None the viewport is the whole image (i.e., (0, 0, cam.height, cam.width))\n            cam_to_world: Optional[tf.Tensor]\n                overrides position, look_at, up vectors\n                4x4 matrix, optional\n            intrinsic_mat: Optional[tf.Tensor]\n                a matrix that transforms a point in camera space before the point\n                is projected to 2D screen space\n                used for modelling field of view and camera skewing\n                after the multiplication the point should be in\n                [-1, 1/aspect_ratio] x [1, -1/aspect_ratio] in homogeneous coordinates\n                the projection is then carried by the specific camera types\n                perspective camera normalizes the homogeneous coordinates\n                while orthogonal camera drop the Z coordinate.\n                ignored by fisheye or panorama cameras\n                overrides fov\n                3x3 matrix, optional\n            distortion_params: Optional[tf.Tensor]\n                an array describing the coefficient of a Brown\xe2\x80\x93Conrady lens distortion model.\n                the array is expected to be 1D with size of 8. the first six coefficients describes\n                the parameters of the rational polynomial for radial distortion (k1~k6) and\n                the last two coefficients are for the tangential distortion (p1~p2).\n                see https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html\n                for more details.\n            camera_type: render.camera_type\n                the type of the camera (perspective, orthographic, fisheye, or panorama)\n            fisheye: bool\n                whether the camera is a fisheye camera\n                (legacy parameter just to ensure compatibility).\n\n    """"""\n    def __init__(self,\n                 position: Optional[tf.Tensor] = None,\n                 look_at: Optional[tf.Tensor] = None,\n                 up: Optional[tf.Tensor] = None,\n                 fov: Optional[tf.Tensor] = None,\n                 clip_near: float = 1e-4,\n                 resolution: Tuple[int] = (256, 256),\n                 viewport: Optional[Tuple[int, int, int, int]] = None,\n                 cam_to_world: Optional[tf.Tensor] = None,\n                 intrinsic_mat: Optional[tf.Tensor] = None,\n                 distortion_params: Optional[tf.Tensor] = None,\n                 camera_type = pyredner.camera_type.perspective,\n                 fisheye: bool = False):\n        assert(tf.executing_eagerly())\n        if position is not None:\n            assert(position.dtype == tf.float32)\n            assert(len(position.shape) == 1 and position.shape[0] == 3)\n        if look_at is not None:\n            assert(look_at.dtype == tf.float32)\n            assert(len(look_at.shape) == 1 and look_at.shape[0] == 3)\n        if up is not None:\n            assert(up.dtype == tf.float32)\n            assert(len(up.shape) == 1 and up.shape[0] == 3)\n        if fov is not None:\n            assert(fov.dtype == tf.float32)\n            assert(len(fov.shape) == 1 and fov.shape[0] == 1)\n        if cam_to_world is not None:\n            assert(cam_to_world.dtype == tf.float32)\n            assert(len(cam_to_world.shape) == 2 and cam_to_world.shape[0] == 4 and cam_to_world.shape[1] == 4)\n        if intrinsic_mat is not None:\n            assert(intrinsic_mat.dtype == tf.float32)\n            assert(len(intrinsic_mat.shape) == 2 and intrinsic_mat.shape[0] == 3 and intrinsic_mat.shape[1] == 3)\n        assert(isinstance(clip_near, float))\n        if position is None and look_at is None and up is None:\n            assert(cam_to_world is  not None)\n        \n        self.position = position\n        self.look_at = look_at\n        self.up = up\n        self.fov = fov\n        with tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n            if cam_to_world is not None:\n                self.cam_to_world = cam_to_world\n            else:\n                self.cam_to_world = None\n            if intrinsic_mat is None:\n                if camera_type == redner.CameraType.perspective:\n                    fov_factor = 1.0 / tf.tan(transform.radians(0.5 * fov))\n                    o = tf.ones([1], dtype=tf.float32)\n                    diag = tf.concat([fov_factor, fov_factor, o], 0)\n                    self._intrinsic_mat = tf.linalg.tensor_diag(diag)\n                else:\n                    self._intrinsic_mat = tf.eye(3, dtype=tf.float32)   \n            else:\n                self._intrinsic_mat = intrinsic_mat\n            self.intrinsic_mat_inv = tf.linalg.inv(self._intrinsic_mat)\n        self.distortion_params = distortion_params\n        self.clip_near = clip_near\n        self.resolution = resolution\n        self.viewport = viewport\n        self.camera_type = camera_type\n        if fisheye:\n            self.camera_type = redner.CameraType.fisheye\n\n    @property\n    def fov(self):\n        return self._fov\n\n    @fov.setter\n    def fov(self, value):\n        if value is not None:\n            self._fov = value\n            with tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n                fov_factor = 1.0 / tf.tan(transform.radians(0.5 * self._fov))\n                o = tf.ones([1], dtype=tf.float32)\n                diag = tf.concat([fov_factor, fov_factor, o], 0)\n                self._intrinsic_mat = tf.linalg.tensor_diag(diag)\n                self.intrinsic_mat_inv = tf.linalg.inv(self._intrinsic_mat)\n        else:\n            self._fov = None\n\n    @property\n    def intrinsic_mat(self):\n        return self._intrinsic_mat\n\n    @intrinsic_mat.setter\n    def intrinsic_mat(self, value):\n        if value is not None:\n            self._intrinsic_mat = value\n            with tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n                self.intrinsic_mat_inv = tf.linalg.inv(self._intrinsic_mat)\n        else:\n            assert(self.fov is not None)\n            self.fov = self._fov\n\n    @property\n    def cam_to_world(self):\n        return self._cam_to_world\n\n    @cam_to_world.setter\n    def cam_to_world(self, value):\n        if value is not None:\n            self._cam_to_world = value\n            with tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n                self.world_to_cam = tf.linalg.inv(self.cam_to_world)\n        else:\n            self._cam_to_world = None\n            self.world_to_cam = None\n\n    def state_dict(self):\n        return {\n            \'position\': self.position,\n            \'look_at\': self.look_at,\n            \'up\': self.up,\n            \'fov\': self.fov,\n            \'cam_to_world\': self._cam_to_world,\n            \'world_to_cam\': self.world_to_cam,\n            \'intrinsic_mat\': self._intrinsic_mat,\n            \'clip_near\': self.clip_near,\n            \'resolution\': self.resolution,\n            \'camera_type\': self.camera_type\n        }\n\n    @classmethod\n    def load_state_dict(cls, state_dict):\n        out = cls.__new__(Camera)\n        out.position = state_dict[\'position\']\n        out.look_at = state_dict[\'look_at\']\n        out.up = state_dict[\'up\']\n        out.fov = state_dict[\'fov\']\n        out.cam_to_world = state_dict[\'cam_to_world\']\n        out.intrinsic_mat = state_dict[\'intrinsic_mat\']\n        out.clip_near = state_dict[\'clip_near\']\n        out.resolution = state_dict[\'resolution\']\n        out.camera_type = state_dict[\'camera_type\']\n        return out\n\ndef automatic_camera_placement(shapes: List,\n                               resolution: Tuple[int, int]):\n    """"""\n        Given a list of shapes, generates camera parameters automatically\n        using the bounding boxes of the shapes. Place the camera at\n        some distances from the shapes, so that it can see all of them.\n        Inspired by https://github.com/mitsuba-renderer/mitsuba/blob/master/src/librender/scene.cpp#L286\n    """"""\n    assert(tf.executing_eagerly())\n    aabb_min = tf.constant((float(\'inf\'), float(\'inf\'), float(\'inf\')))\n    aabb_max = -tf.constant((float(\'inf\'), float(\'inf\'), float(\'inf\')))\n    for shape in shapes:\n        v = shape.vertices    \n        v_min = tf.reduce_min(v, 0)\n        v_max = tf.reduce_max(v, 0)\n        with tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n            v_min = tf.identity(v_min)\n            v_max = tf.identity(v_max)\n        aabb_min = tf.minimum(aabb_min, v_min)\n        aabb_max = tf.maximum(aabb_max, v_max)\n    assert(tf.reduce_all(tf.math.is_finite(aabb_min)) and tf.reduce_all(tf.math.is_finite(aabb_max)))\n    center = (aabb_max + aabb_min) * 0.5\n    extents = aabb_max - aabb_min\n    max_extents_xy = tf.maximum(extents[0], extents[1])\n    distance = max_extents_xy / (2 * math.tan(45 * 0.5 * math.pi / 180.0))\n    max_extents_xyz = tf.maximum(extents[2], max_extents_xy)    \n    return Camera(position = tf.stack((center[0], center[1], aabb_min[2] - distance)),\n                  look_at = center,\n                  up = tf.constant((0.0, 1.0, 0.0)),\n                  fov = tf.constant([45.0]),\n                  clip_near = 0.001 * float(distance),\n                  resolution = resolution)\n\ndef generate_intrinsic_mat(fx: tf.Tensor,\n                           fy: tf.Tensor,\n                           skew: tf.Tensor,\n                           x0: tf.Tensor,\n                           y0: tf.Tensor):\n    """"""\n        | Generate the following 3x3 intrinsic matrix given the parameters.\n        | fx, skew, x0\n        |  0,   fy, y0\n        |  0,    0,  1\n\n        Parameters\n        ==========\n        fx: tf.Tensor\n            Focal length at x dimension. 1D tensor with size 1.\n        fy: tf.Tensor\n            Focal length at y dimension. 1D tensor with size 1.\n        skew: tf.Tensor\n            Axis skew parameter describing shearing transform. 1D tensor with size 1.\n        x0: tf.Tensor\n            Principle point offset at x dimension. 1D tensor with size 1.\n        y0: tf.Tensor\n            Principle point offset at y dimension. 1D tensor with size 1.\n\n        Returns\n        =======\n        tf.Tensor\n            3x3 intrinsic matrix\n    """"""\n    z = tf.zeros_like(fx)\n    o = tf.ones_like(fx)\n    row0 = tf.concat([fx, skew, x0], axis=0)\n    row1 = tf.concat([ z,   fy, y0], axis=0)\n    row2 = tf.concat([ z,    z,  o], axis=0)\n    return tf.stack([row0, row1, row2])\n'"
pyredner_tensorflow/camera_type.py,0,b'import redner\n\nclass CameraType:\n    def __init__(self):\n        self.perspective = redner.CameraType.perspective\n        self.orthographic = redner.CameraType.orthographic\n        self.fisheye = redner.CameraType.fisheye\n        self.panorama = redner.CameraType.panorama\n\ncamera_type = CameraType()'
pyredner_tensorflow/channels.py,0,b'import redner\n\nclass Channel:\n    def __init__(self):\n        self.radiance = redner.channels.radiance\n        self.alpha = redner.channels.alpha\n        self.depth = redner.channels.depth\n        self.position = redner.channels.position\n        self.geometry_normal = redner.channels.geometry_normal\n        self.shading_normal = redner.channels.shading_normal\n        self.uv = redner.channels.uv\n        self.barycentric_coordinates = redner.channels.barycentric_coordinates\n        self.diffuse_reflectance = redner.channels.diffuse_reflectance\n        self.specular_reflectance = redner.channels.specular_reflectance\n        self.roughness = redner.channels.roughness\n        self.generic_texture = redner.channels.generic_texture\n        self.vertex_color = redner.channels.vertex_color\n        self.shape_id = redner.channels.shape_id\n        self.triangle_id = redner.channels.triangle_id\n        self.material_id = redner.channels.material_id\n\nchannels = Channel()\n'
pyredner_tensorflow/device.py,1,"b'import tensorflow as tf\n\nuse_gpu = tf.test.is_gpu_available(\n    cuda_only=True,\n    min_cuda_compute_capability=None\n)\ncpu_device_id = 0\ngpu_device_id = 0\n\ndef get_device_name():\n    """"""\n        Get the current tensorflow device name we are using.\n    """"""\n    global use_gpu\n    global cpu_device_id\n    global gpu_device_id\n    return \'/device:gpu:\' + str(gpu_device_id) if use_gpu else \'/device:cpu:\' + str(cpu_device_id)\n\ndef set_use_gpu(v: bool):\n    """"""\n        Set whether to use CUDA or not.\n    """"""\n    global use_gpu\n    use_gpu = v\n\ndef get_use_gpu():\n    """"""\n        Get whether we are using CUDA or not.\n    """"""\n    global use_gpu\n    return use_gpu\n\ndef set_cpu_device_id(did: int):\n    """"""\n        Set the cpu device id we are using.\n    """"""\n    global cpu_device_id\n    cpu_device_id = did\n\ndef get_cpu_device_id():\n    """"""\n        Get the cpu device id we are using.\n    """"""\n    global cpu_device_id\n    return cpu_device_id\n\ndef set_gpu_device_id(did: int):\n    """"""\n        Set the gpu device id we are using.\n    """"""\n    global gpu_device_id\n    gpu_device_id = did\n\ndef get_gpu_device_id():\n    """"""\n        Get the gpu device id we are using.\n    """"""\n    global gpu_device_id\n    return gpu_device_id\n'"
pyredner_tensorflow/envmap.py,19,"b'import pyredner_tensorflow as pyredner\nimport numpy as np\nimport tensorflow as tf\nimport math\nimport pdb\n\nclass EnvironmentMap:\n    """"""\n        A class representing light sources infinitely far away using an image.\n\n        Args\n        ----------\n        values: Union[tf.Tensor, pyredner.Texture]\n            a float32 tensor with size 3 or [height, width, 3] or a Texture\n        env_to_world: tf.Tensor\n            a float32 4x4 matrix that transforms the environment map\n        directly_visible: bool\n            can the camera sees the light source directly?\n    """"""\n\n    def __init__(self,\n                 values: tf.Tensor,\n                 env_to_world: tf.Tensor = tf.eye(4, 4),\n                 directly_visible: bool = True):\n        # Convert to constant texture if necessary\n        if tf.is_tensor(values):\n            values = pyredner.Texture(values)\n\n        assert(values.texels.dtype == tf.float32)\n\n        self.values = values\n        self.env_to_world = env_to_world\n        self.directly_visible = directly_visible\n\n    def generate_envmap_pdf(self):\n        assert(tf.executing_eagerly())\n        values = self.values\n        with tf.device(pyredner.get_device_name()):\n            # Build sampling table\n            luminance = 0.212671 * values.texels[:, :, 0] + \\\n                        0.715160 * values.texels[:, :, 1] + \\\n                        0.072169 * values.texels[:, :, 2]\n            # For each y, compute CDF over x\n            sample_cdf_xs_ = tf.cumsum(luminance, axis=1)\n\n            y_weight = tf.sin(\n                math.pi * (tf.cast(\n                    tf.range(luminance.shape[0]),\n                    tf.float32) + 0.5) / float(luminance.shape[0]))\n\n            # Compute CDF for x\n            sample_cdf_ys_ = tf.cumsum(sample_cdf_xs_[:, -1] * y_weight, axis=0)\n            pdf_norm = (luminance.shape[0] * luminance.shape[1]) / \\\n                    (sample_cdf_ys_[-1] * (2 * math.pi * math.pi))\n            # Normalize to [0, 1)\n            sample_cdf_xs = (sample_cdf_xs_ - sample_cdf_xs_[:, 0:1]) / \\\n                tf.math.maximum(\n                    sample_cdf_xs_[\n                        :, \n                        (luminance.shape[1] - 1):luminance.shape[1]],\n                        1e-8 * tf.convert_to_tensor(np.ones((sample_cdf_xs_.shape[0], 1)), dtype=tf.float32)\n                    )\n            sample_cdf_ys = (sample_cdf_ys_ - sample_cdf_ys_[0]) / \\\n                tf.math.maximum(sample_cdf_ys_[-1], tf.constant([1e-8]))\n\n            self.sample_cdf_ys = sample_cdf_ys\n            self.sample_cdf_xs = sample_cdf_xs\n            self.pdf_norm = pdf_norm\n\n    @property\n    def values(self):\n        return self._values\n\n    @values.setter\n    def values(self, value):\n        self._values = value\n        self.generate_envmap_pdf()\n\n    @property\n    def env_to_world(self):\n        return self._env_to_world\n\n    @env_to_world.setter\n    def env_to_world(self, value):\n        self._env_to_world = value\n        with tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n            self.world_to_env = tf.linalg.inv(self._env_to_world)\n\n    def state_dict(self):\n        return {\n            \'values\': self.values.state_dict(),\n            \'env_to_world\': self.env_to_world,\n            \'world_to_env\': self.world_to_env,\n            \'sample_cdf_ys\': self.sample_cdf_ys,\n            \'sample_cdf_xs\': self.sample_cdf_xs,\n            \'pdf_norm\': self.pdf_norm,\n            \'directly_visible\': self.directly_visible\n        }\n\n    @classmethod\n    def load_state_dict(cls, state_dict):\n        out = cls.__new__(EnvironmentMap)\n        out.values = pyredner.Texture.load_state_dict(state_dict[\'values\'])\n        out.env_to_world = state_dict[\'env_to_world\']\n        out.world_to_env = state_dict[\'world_to_env\']\n        out.sample_cdf_ys = state_dict[\'sample_cdf_ys\']\n        out.sample_cdf_xs = state_dict[\'sample_cdf_xs\']\n        out.pdf_norm = state_dict[\'pdf_norm\']\n        out.directly_visible = state_dict[\'directly_visible\']\n        return out\n'"
pyredner_tensorflow/geometry_images.py,6,"b'import numpy as np\nimport tensorflow as tf\nimport math\nimport pyredner_tensorflow as pyredner\n\ndef generate_geometry_image(size: int):\n    """"""\n        Generate an spherical geometry image [Gu et al. 2002 and Praun and Hoppe 2003]\n        of size [2 * size + 1, 2 * size + 1]. This can be used for encoding a genus-0\n        surface into a regular image, so that it is more convienent for a CNN to process.\n        The topology is given by a tesselated octahedron. UV is given by the spherical mapping.\n        Duplicated vertex are mapped to the one with smaller index (so some vertices on the\n        geometry image is unused by the indices).\n\n        Args\n        ====\n        size: int\n            size of the geometry image\n        device_name: Optional[str]\n            Which device should we store the data in.\n            If set to None, use the device from pyredner.get_device_name().\n\n        Returns\n        =======\n        tf.Tensor\n            vertices of size [(2 * size + 1 * 2 * size + 1), 3]\n        tf.Tensor\n            indices of size [2 * (2 * size + 1 * 2 * size + 1), 3]\n        tf.Tensor\n            uvs of size [(2 * size + 1 * 2 * size + 1), 2]\n    """"""\n    size *= 2\n\n    # Generate vertices and uv by going through each vertex.\n    left_top = np.array([0.0, 0.0, 1.0])\n    top = np.array([0.0, 1.0, 0.0])\n    right_top = np.array([0.0, 0.0, 1.0])\n    left = np.array([-1.0, 0.0, 0.0])\n    middle = np.array([0.0, 0.0, -1.0])\n    right = np.array([1.0, 0.0, 0.0])\n    left_bottom = np.array([0.0, 0.0, 1.0])\n    bottom = np.array([0.0, -1.0, 0.0])\n    right_bottom = np.array([0.0, 0.0, 1.0])\n    vertices = np.zeros([(size+1) * (size+1), 3])\n    uvs = np.zeros([(size+1) * (size + 1), 2])\n    vertex_id = 0\n    half_size = size / 2.0\n    for i in range(size+1): # height\n        for j in range(size+1): # width\n            # Left Top\n            if i  + j <= half_size:\n                org = left_top\n                i_axis = left - left_top\n                j_axis = top - left_top\n                i_ = float(i) / half_size\n                j_ = float(j) / half_size\n            elif (i + j >= half_size and i <= half_size and j <= half_size):\n                org = middle\n                i_axis = top - middle\n                j_axis = left - middle\n                i_ = 1.0 - float(i) / half_size\n                j_ = 1.0 - float(j) / half_size\n            # Right Top\n            elif ((half_size - i + j - half_size) <= half_size and i <= half_size and j >= half_size):\n                org = middle\n                i_axis = top - middle\n                j_axis = right - middle\n                i_ = 1.0 - float(i) / half_size\n                j_ = float(j) / half_size - 1.0\n            elif ((i + size - j) <= half_size):\n                org = right_top\n                i_axis = right - right_top\n                j_axis = top - right_top\n                i_ = float(i) / half_size\n                j_ = 2.0 - float(j) / half_size\n            # Left Bottom\n            elif ((i - half_size + half_size - j) <= half_size and i >= half_size and j <= half_size):\n                org = middle\n                i_axis = bottom - middle\n                j_axis = left - middle\n                i_ = float(i) / half_size - 1.0\n                j_ = 1.0 - float(j) / half_size\n            elif ((size - i + j) <= half_size):\n                org = left_bottom\n                i_axis = left - left_bottom\n                j_axis = bottom - left_bottom\n                i_ = 2.0 - float(i) / half_size\n                j_ = float(j) / half_size\n            # Right Bottom\n            elif ((i - half_size + j - half_size) <= half_size and i >= half_size and j >= half_size):\n                org = middle\n                i_axis = bottom - middle\n                j_axis = right - middle\n                i_ = float(i) / half_size - 1.0\n                j_ = float(j) / half_size - 1.0\n            else:\n                org = right_bottom\n                i_axis = right - right_bottom\n                j_axis = bottom - right_bottom\n                i_ = 2.0 - float(i) / half_size\n                j_ = 2.0 - float(j) / half_size\n            p = org + i_ * i_axis + j_ * j_axis\n            vertices[vertex_id, :] = p / np.linalg.norm(p)\n            # Spherical UV mapping\n            u = 0.5 + math.atan2(float(p[2]), float(p[0])) / (2 * math.pi)\n            v = 0.5 - math.asin(float(p[1])) / math.pi\n            uvs[vertex_id, :] = np.array([u, v])\n            vertex_id += 1\n\n    # Generate indices by going through each triangle.\n    # Duplicated vertex are mapped to the one with smaller index.\n    indices = []\n    for i in range(size): # height\n        for j in range(size): # width\n            left_top = i * (size + 1) + j\n            right_top = i * (size + 1) + j + 1\n            left_bottom = (i + 1) * (size + 1) + j\n            right_bottom = (i + 1) * (size + 1) + j + 1\n            # Wrap rule for octahedron topology\n            if i == 0 and j >= half_size:\n                if j > half_size:\n                    left_top = i * (size + 1) + size - j\n                right_top = i * (size + 1) + (size - (j + 1))\n            elif i == size - 1 and j >= half_size:\n                if j > half_size:\n                    left_bottom = (i + 1) * (size + 1) + size - j\n                right_bottom = (i + 1) * (size + 1) + (size - (j + 1))\n                if j == size - 1:\n                    right_bottom = 0\n            elif j == 0 and i >= half_size:\n                if i > half_size:\n                    left_top = (size - i) * (size + 1) + j\n                left_bottom = (size - (i + 1)) * (size + 1) + j\n            elif j == size - 1 and i >= half_size:\n                if i > half_size:\n                    right_top = (size - i) * (size + 1) + j + 1\n                right_bottom = (size - (i + 1)) * (size + 1) + j + 1\n\n            # Left Top\n            if i < half_size and j < half_size:\n                indices.append((left_top, left_bottom, right_top))\n                indices.append((right_top, left_bottom, right_bottom))\n            # Right Top\n            elif i < half_size and j >= half_size:\n                indices.append((left_top, left_bottom, right_bottom))\n                indices.append((left_top, right_bottom, right_top))\n            # Left Bottom\n            elif i >= half_size and j < half_size:\n                indices.append((left_top, right_bottom, right_top))\n                indices.append((left_top, left_bottom, right_bottom))\n            # Right Bottom\n            else:\n                indices.append((left_top, left_bottom, right_top))\n                indices.append((right_top, left_bottom, right_bottom))\n\n    vertices = tf.constant(vertices, dtype = tf.float32)\n    uvs = tf.constant(uvs, dtype = tf.float32)\n    indices = tf.constant(indices, dtype = tf.int32)\n    return vertices, indices, uvs\n'"
pyredner_tensorflow/image.py,6,"b'import numpy as np\nimport skimage\nimport skimage.io\nimport tensorflow as tf\nimport os\nimport imageio\n\ndef imwrite(img: tf.Tensor,\n            filename: str,\n            gamma: float = 2.2,\n            normalize: bool = False):\n    """"""\n        write img to filename\n\n        Args\n        ====\n        img: tf.Tensor\n            with size [height, width, channel]\n        filename: str\n\n        gamma: float\n            if the image is not an OpenEXR file, apply gamma correction\n        normalize:\n            normalize img to the range [0, 1] before writing\n    """"""\n    directory = os.path.dirname(filename)\n    if directory != \'\' and not os.path.exists(directory):\n        os.makedirs(directory)\n    assert(tf.executing_eagerly())\n    img = img.numpy()\n    if normalize:\n        img_rng = np.max(img) - np.min(img)\n        if img_rng > 0:\n            img = (img - np.min(img)) / img_rng\n    if filename[-4:] == \'.exr\':\n        imageio.plugins.freeimage.download()\n        imageio.imwrite(filename, img)\n    else:\n        skimage.io.imsave(filename, (np.power(np.clip(img, 0.0, 1.0), 1.0/gamma) * 255).astype(np.uint8))\n\ndef imread(filename: str,\n           gamma: float = 2.2):\n    """"""\n        read img from filename\n\n        Args\n        ====\n        filename: str\n\n        gamma: float\n            if the image is not an OpenEXR file, apply gamma correction\n\n        Returns\n        =======\n        A float32 tensor with size [height, width, channel]\n    """"""\n    if (filename[-4:] == \'.exr\'):\n        imageio.plugins.freeimage.download()\n        return tf.convert_to_tensor(imageio.imread(filename).astype(np.float32))\n    else:\n        im = skimage.io.imread(filename)\n        if im.ndim == 2:\n            im = np.stack([im, im, im], axis=-1)\n        elif im.shape[2] == 4:\n            im = im[:, :, :3]\n        return tf.convert_to_tensor(\n                np.power(skimage.img_as_float(im).astype(np.float32), gamma), \n                dtype=tf.float32\n            )\n'"
pyredner_tensorflow/load_mitsuba.py,44,"b'import tensorflow as tf\nimport xml.etree.ElementTree as etree\nimport numpy as np\nimport redner\nimport os\nimport pyredner_tensorflow as pyredner\nimport pyredner_tensorflow.transform as transform\nimport math\n\ndef parse_transform(node):\n    ret = tf.eye(4)\n    for child in node:\n        if child.tag == \'matrix\':\n            value = tf.convert_to_tensor(\n                np.reshape(\n                    np.fromstring(child.attrib[\'value\'], dtype=np.float32, sep=\' \'),\n                    (4, 4)))\n            ret = value @ ret\n        elif child.tag == \'translate\':\n            x = float(child.attrib[\'x\'])\n            y = float(child.attrib[\'y\'])\n            z = float(child.attrib[\'z\'])\n            value = transform.gen_translate_matrix(tf.constant([x, y, z]))\n            ret = value @ ret\n        elif child.tag == \'scale\':\n            x = float(child.attrib[\'x\'])\n            y = float(child.attrib[\'y\'])\n            z = float(child.attrib[\'z\'])\n            value = transform.gen_scale_matrix(tf.constant([x, y, z]))\n            ret = value @ ret\n        elif child.tag == \'rotate\':\n            x = float(child.attrib[\'x\']) if \'x\' in child.attrib else 0.0\n            y = float(child.attrib[\'y\']) if \'y\' in child.attrib else 0.0\n            z = float(child.attrib[\'z\']) if \'z\' in child.attrib else 0.0\n            angle = transform.radians(float(child.attrib[\'angle\']))\n            axis = np.array([x, y, z])\n            axis = axis / np.linalg.norm(axis)\n            cos_theta = math.cos(angle)\n            sin_theta = math.sin(angle)\n            mat = np.zeros([4, 4], dtype = np.float32)\n            mat[0, 0] = axis[0] * axis[0] + (1.0 - axis[0] * axis[0]) * cos_theta\n            mat[0, 1] = axis[0] * axis[1] * (1.0 - cos_theta) - axis[2] * sin_theta\n            mat[0, 2] = axis[0] * axis[2] * (1.0 - cos_theta) + axis[1] * sin_theta\n\n            mat[1, 0] = axis[0] * axis[1] * (1.0 - cos_theta) + axis[2] * sin_theta\n            mat[1, 1] = axis[1] * axis[1] + (1.0 - axis[1] * axis[1]) * cos_theta\n            mat[1, 2] = axis[1] * axis[2] * (1.0 - cos_theta) - axis[0] * sin_theta\n\n            mat[2, 0] = axis[0] * axis[2] * (1.0 - cos_theta) - axis[1] * sin_theta\n            mat[2, 1] = axis[1] * axis[2] * (1.0 - cos_theta) + axis[0] * sin_theta\n            mat[2, 2] = axis[2] * axis[2] + (1.0 - axis[2] * axis[2]) * cos_theta\n\n            mat[3, 3] = 1.0\n\n            ret = tf.convert_to_tensor(mat) @ ret\n    return ret\n\ndef parse_vector(str):\n    v = np.fromstring(str, dtype=np.float32, sep=\',\')\n    if v.shape[0] != 3:\n        v = np.fromstring(str, dtype=np.float32, sep=\' \')\n    assert(v.ndim == 1)\n    return tf.convert_to_tensor(v)\n\ndef parse_camera(node):\n    fov = tf.constant([45.0])\n    position = None\n    look_at = None\n    up = None\n    clip_near = 1e-2\n    resolution = [256, 256]\n    for child in node:\n        if \'name\' in child.attrib:\n            if child.attrib[\'name\'] == \'fov\':\n                fov = tf.constant([float(child.attrib[\'value\'])])\n            elif child.attrib[\'name\'] == \'toWorld\':\n                has_lookat = False\n                for grandchild in child:\n                    if grandchild.tag.lower() == \'lookat\':\n                        has_lookat = True\n                        position = parse_vector(grandchild.attrib[\'origin\'])\n                        look_at = parse_vector(grandchild.attrib[\'target\'])\n                        up = parse_vector(grandchild.attrib[\'up\'])\n                if not has_lookat:\n                    print(\'Unsupported Mitsuba scene format: please use a look at transform\')\n                    assert(False)\n        if child.tag == \'film\':\n            for grandchild in child:\n                if \'name\' in grandchild.attrib:\n                    if grandchild.attrib[\'name\'] == \'width\':\n                        resolution[1] = int(grandchild.attrib[\'value\'])\n                    elif grandchild.attrib[\'name\'] == \'height\':\n                        resolution[0] = int(grandchild.attrib[\'value\'])\n\n    return pyredner.Camera(position     = position,\n                           look_at      = look_at,\n                           up           = up,\n                           fov          = fov,\n                           clip_near    = clip_near,\n                           resolution   = resolution)\n\ndef parse_material(node, two_sided = False):\n    node_id = None\n    if \'id\' in node.attrib:\n        node_id = node.attrib[\'id\']\n    if node.attrib[\'type\'] == \'diffuse\':\n        diffuse_reflectance = tf.constant([0.5, 0.5, 0.5])\n        diffuse_uv_scale = [1.0, 1.0]\n        specular_reflectance = tf.constant([0.0, 0.0, 0.0])\n        specular_uv_scale = [1.0, 1.0]\n        roughness = tf.constant([1.0])\n\n        for child in node:\n            if child.attrib[\'name\'] == \'reflectance\':\n                if child.tag == \'texture\':\n                    for grandchild in child:\n                        if grandchild.attrib[\'name\'] == \'filename\':\n                            diffuse_reflectance = pyredner.imread(grandchild.attrib[\'value\'])\n                        elif grandchild.attrib[\'name\'] == \'uscale\':\n                            diffuse_uv_scale[0] = float(grandchild.attrib[\'value\'])\n                        elif grandchild.attrib[\'name\'] == \'vscale\':\n                            diffuse_uv_scale[1] = float(grandchild.attrib[\'value\'])\n                elif child.tag == \'rgb\' or child.tag == \'spectrum\' or child.tag == \'srgb\':\n                    diffuse_reflectance = parse_vector(child.attrib[\'value\'])\n                    if child.tag == \'srgb\':\n                        diffuse_reflectance = pyredner.srgb_to_linear(diffuse_reflectance)\n            elif child.attrib[\'name\'] == \'specular\':\n                if child.tag == \'texture\':\n                    for grandchild in child:\n                        if grandchild.attrib[\'name\'] == \'filename\':\n                            specular_reflectance = pyredner.imread(grandchild.attrib[\'value\'])\n                        elif grandchild.attrib[\'name\'] == \'uscale\':\n                            specular_uv_scale[0] = float(grandchild.attrib[\'value\'])\n                        elif grandchild.attrib[\'name\'] == \'vscale\':\n                            specular_uv_scale[1] = float(grandchild.attrib[\'value\'])\n                elif child.tag == \'rgb\' or child.tag == \'spectrum\' or child.tag == \'srgb\':\n                    specular_reflectance = parse_vector(child.attrib[\'value\'])\n                    if child.tag == \'srgb\':\n                        specular_reflectance = pyredner.srgb_to_linear(specular_reflectance)\n            elif child.attrib[\'name\'] == \'roughness\':\n                roughness = tf.constant([float(child.attrib[\'value\'])])\n        \n        diffuse_uv_scale = tf.constant(diffuse_uv_scale)\n        specular_uv_scale = tf.constant(specular_uv_scale)\n\n        return (node_id, pyredner.Material(\n                diffuse_reflectance = pyredner.Texture(diffuse_reflectance, diffuse_uv_scale),\n                specular_reflectance = pyredner.Texture(specular_reflectance, specular_uv_scale),\n                roughness = pyredner.Texture(roughness),\n                two_sided = two_sided))\n\n    elif node.attrib[\'type\'] == \'roughplastic\':\n        diffuse_reflectance = tf.constant([0.5, 0.5, 0.5])\n        diffuse_uv_scale = [1.0, 1.0]\n        # Mitsuba defaults specular reflectance to 1.0, but we use Schilick approximation and \n        # use the specular reflectance for representing both index of refraction and color tint\n        # for metal materials simultaneously.\n        # Schilick\'s appsoximation set R0 to ((n1 - n2) / (n1 + n2))^2. Mitsuba defaults\n        # IOR to n1=1 and n2=1.5, so R0 ~= 0.04\n        specular_reflectance = tf.constant([0.04, 0.04, 0.04])\n        specular_uv_scale = [1.0, 1.0]\n        roughness = tf.constant([0.01])\n        for child in node:\n            if child.attrib[\'name\'] == \'diffuseReflectance\' or child.attrib[\'name\'] == \'diffuse_reflectance\':\n                if child.tag == \'texture\':\n                    for grandchild in child:\n                        if grandchild.attrib[\'name\'] == \'filename\':\n                            diffuse_reflectance = pyredner.imread(grandchild.attrib[\'value\'])\n                        elif grandchild.attrib[\'name\'] == \'uscale\':\n                            diffuse_uv_scale[0] = float(grandchild.attrib[\'value\'])\n                        elif grandchild.attrib[\'name\'] == \'vscale\':\n                            diffuse_uv_scale[1] = float(grandchild.attrib[\'value\'])\n                elif child.tag == \'rgb\' or child.tag == \'spectrum\' or child.tag == \'srgb\':\n                    diffuse_reflectance = parse_vector(child.attrib[\'value\'])\n                    if child.tag == \'srgb\':\n                        diffuse_reflectance = pyredner.srgb_to_linear(diffuse_reflectance)\n            elif child.attrib[\'name\'] == \'specularReflectance\' or child.attrib[\'name\'] == \'specular_reflectance\':\n                if child.tag == \'texture\':\n                    for grandchild in child:\n                        if grandchild.attrib[\'name\'] == \'filename\':\n                            specular_reflectance = pyredner.imread(grandchild.attrib[\'value\'])\n                        elif grandchild.attrib[\'name\'] == \'uscale\':\n                            specular_uv_scale[0] = float(grandchild.attrib[\'value\'])\n                        elif grandchild.attrib[\'name\'] == \'vscale\':\n                            specular_uv_scale[1] = float(grandchild.attrib[\'value\'])\n                elif child.tag == \'rgb\' or child.tag == \'spectrum\' or child.tag == \'srgb\':\n                    specular_reflectance = parse_vector(child.attrib[\'value\'])\n                    if child.tag == \'srgb\':\n                        specular_reflectance = pyredner.srgb_to_linear(specular_reflectance)\n            elif child.attrib[\'name\'] == \'alpha\':\n                if child.tag == \'texture\':\n                    roughness, roughness_uv_scale = parse_texture(child)\n                    roughness = roughness * roughness\n                else:\n                    alpha = float(child.attrib[\'value\'])\n                    roughness = tf.constant([alpha * alpha])\n        \n        diffuse_uv_scale = tf.constant(diffuse_uv_scale)\n        specular_uv_scale = tf.constant(specular_uv_scale)\n\n        return (node_id, pyredner.Material(\n                diffuse_reflectance = pyredner.Texture(diffuse_reflectance, diffuse_uv_scale),\n                specular_reflectance = pyredner.Texture(specular_reflectance, specular_uv_scale),\n                roughness = pyredner.Texture(roughness),\n                two_sided = two_sided))\n    elif node.attrib[\'type\'] == \'twosided\':\n        ret = parse_material(node[0], True)\n        return (node_id, ret[1])\n    else:\n        print(\'Unsupported material type:\', node.attrib[\'type\'])\n        assert(False)\n\ndef parse_shape(node, material_dict, shape_id):\n    if node.attrib[\'type\'] == \'obj\' or node.attrib[\'type\'] == \'serialized\':\n        to_world = tf.eye(4)\n        serialized_shape_id = 0\n        mat_id = -1\n        light_intensity = None\n        filename = \'\'\n        max_smooth_angle = -1\n        for child in node:\n            if \'name\' in child.attrib:\n                if child.attrib[\'name\'] == \'filename\':\n                    filename = child.attrib[\'value\']\n                elif child.attrib[\'name\'] == \'toWorld\':\n                    to_world = parse_transform(child)\n                elif child.attrib[\'name\'] == \'shapeIndex\':\n                    serialized_shape_id = int(child.attrib[\'value\'])\n                elif child.attrib[\'name\'] == \'maxSmoothAngle\':\n                    max_smooth_angle = float(child.attrib[\'value\'])\n            if child.tag == \'ref\':\n                mat_id = material_dict[child.attrib[\'id\']]\n            elif child.tag == \'emitter\':\n                for grandchild in child:\n                    if grandchild.attrib[\'name\'] == \'radiance\':\n                        light_intensity = parse_vector(grandchild.attrib[\'value\'])\n                        if light_intensity.shape[0] == 1:\n                            light_intensity = tf.constant(\n                                         [light_intensity[0],\n                                          light_intensity[0],\n                                          light_intensity[0]])\n\n        if node.attrib[\'type\'] == \'obj\':\n            _, mesh_list, _ = pyredner.load_obj(filename, obj_group = False)\n            # Convert to CPU for rebuild_topology\n            with tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n                vertices = tf.identity(mesh_list[0][1].vertices)\n                indices = tf.identity(mesh_list[0][1].indices)\n                uvs = mesh_list[0][1].uvs\n                normals = mesh_list[0][1].normals\n                uv_indices = mesh_list[0][1].uv_indices\n                normal_indices = mesh_list[0][1].normal_indices\n                if uvs is not None:\n                    uvs = tf.identity(uvs)\n                if normals is not None:\n                    normals = tf.identity(normals)\n                if uv_indices is not None:\n                    uv_indices = tf.identity(uv_indices)\n        else:\n            assert(node.attrib[\'type\'] == \'serialized\')\n            mitsuba_tri_mesh = redner.load_serialized(filename, serialized_shape_id)\n            vertices = tf.convert_to_tensor(mitsuba_tri_mesh.vertices)\n            indices = tf.convert_to_tensor(mitsuba_tri_mesh.indices)\n            uvs = tf.convert_to_tensor(mitsuba_tri_mesh.uvs)\n            normals = tf.convert_to_tensor(mitsuba_tri_mesh.normals)\n            if uvs.shape[0] == 0:\n                uvs = None\n            if normals.shape[0] == 0:\n                normals = None\n            uv_indices = None # Serialized doesn\'t use different indices for UV & normal\n            normal_indices = None\n\n        # Transform the vertices and normals\n        vertices = tf.concat((vertices, tf.ones([vertices.shape[0], 1], dtype=tf.float32)), axis = 1)\n        vertices = vertices @ tf.transpose(to_world, [1, 0])\n        vertices = vertices / vertices[:, 3:4]\n        vertices = vertices[:, 0:3]\n\n        if normals is not None:\n            normals = normals @ (tf.linalg.inv(tf.transpose(to_world, [0, 1]))[:3, :3])\n        assert(vertices is not None)\n        assert(indices is not None)\n        if max_smooth_angle >= 0:\n            if normals is None:\n                normals = tf.zeros_like(vertices)\n            new_num_vertices = redner.rebuild_topology(\\\n                redner.float_ptr(pyredner.data_ptr(vertices)),\n                redner.int_ptr(pyredner.data_ptr(indices)),\n                redner.float_ptr(pyredner.data_ptr(uvs) if uvs is not None else 0),\n                redner.float_ptr(pyredner.data_ptr(normals) if normals is not None else 0),\n                redner.int_ptr(pyredner.data_ptr(uv_indices) if uv_indices is not None else 0),\n                int(vertices.shape[0]),\n                int(indices.shape[0]),\n                max_smooth_angle)\n            print(\'Rebuilt topology, original vertices size: {}, new vertices size: {}\'.format(\\\n                int(vertices.shape[0]), new_num_vertices))\n            vertices.resize_(new_num_vertices, 3)\n            if uvs is not None:\n                uvs.resize_(new_num_vertices, 2)\n            if normals is not None:\n                normals.resize_(new_num_vertices, 3)\n\n        lgt = None\n        if light_intensity is not None:\n            lgt = pyredner.AreaLight(shape_id, light_intensity)\n\n        return pyredner.Shape(vertices=vertices,\n                              indices=indices,\n                              uvs=uvs,\n                              normals=normals,\n                              uv_indices=uv_indices,\n                              normal_indices=normal_indices,\n                              material_id=mat_id), lgt\n    elif node.attrib[\'type\'] == \'rectangle\':\n        indices = tf.constant([[0, 2, 1], [1, 2, 3]],\n                               dtype = tf.int32)\n        vertices = tf.constant([[-1.0, -1.0, 0.0],\n                                 [-1.0,  1.0, 0.0],\n                                 [ 1.0, -1.0, 0.0],\n                                 [ 1.0,  1.0, 0.0]])\n        uvs = None\n        normals = None\n        to_world = tf.eye(4)\n        mat_id = -1\n        light_intensity = None\n        for child in node:\n            if \'name\' in child.attrib:\n                if child.attrib[\'name\'] == \'toWorld\':\n                    to_world = parse_transform(child)\n            if child.tag == \'ref\':\n                mat_id = material_dict[child.attrib[\'id\']]\n            elif child.tag == \'emitter\':\n                for grandchild in child:\n                    if grandchild.attrib[\'name\'] == \'radiance\':\n                        light_intensity = parse_vector(grandchild.attrib[\'value\'])\n                        if light_intensity.shape[0] == 1:\n                            light_intensity = tf.constant(\n                                         [light_intensity[0],\n                                          light_intensity[0],\n                                          light_intensity[0]])\n        # Transform the vertices and normals\n        vertices = tf.concat((vertices, tf.convert_to_tensor(np.ones(vertices.shape[0], 1), dtype=tf.float32)), axis = 1)\n        vertices = vertices @ tf.transpose(to_world, [0, 1])\n        vertices = vertices / vertices[:, 3:4]\n        vertices = vertices[:, 0:3]\n        if normals is not None:\n            normals = normals @ (tf.linalg.inv(tf.transpose(to_world, [0, 1]))[:3, :3])\n        assert(vertices is not None)\n        assert(indices is not None)\n        lgt = None\n        if light_intensity is not None:\n            lgt = pyrender.Light(shape_id, light_intensity)\n\n        \n        return pyredner.Shape(vertices=vertices,\n                              indices=indices,\n                              uvs=uvs,\n                              normals=normals,\n                              material_id=mat_id), lgt\n    else:\n        assert(False)\n\ndef parse_scene(node):\n    cam = None\n    resolution = None\n    materials = []\n    material_dict = {}\n    shapes = []\n    lights = []\n    for child in node:\n        if child.tag == \'sensor\':\n            cam = parse_camera(child)\n        elif child.tag == \'bsdf\':\n            node_id, material = parse_material(child)\n            if node_id is not None:\n                material_dict[node_id] = len(materials)\n                materials.append(material)\n        elif child.tag == \'shape\':\n            shape, light = parse_shape(child, material_dict, len(shapes))\n            shapes.append(shape)\n            if light is not None:\n                lights.append(light)\n    return pyredner.Scene(cam, shapes, materials, lights)\n\ndef load_mitsuba(filename):\n    """"""\n        Load from a Mitsuba scene file as PyTorch tensors.\n    """"""\n\n    tree = etree.parse(filename)\n    root = tree.getroot()\n    cwd = os.getcwd()\n    os.chdir(os.path.dirname(filename))\n    ret = parse_scene(root)\n    os.chdir(cwd)\n    return ret\n'"
pyredner_tensorflow/load_obj.py,14,"b'import tensorflow as tf\nimport re\nimport pyredner_tensorflow as pyredner\nimport os\n\nclass WavefrontMaterial:\n    def __init__(self):\n        self.name = """"\n        self.Kd = (0.0, 0.0, 0.0)\n        self.Ks = (0.0, 0.0, 0.0)\n        self.Ns = 0.0\n        self.Ke = (0.0, 0.0, 0.0)\n        self.map_Kd = None\n        self.map_Ks = None\n        self.map_Ns = None\n\nclass TriangleMesh:\n    def __init__(self,\n                 indices,\n                 uv_indices,\n                 normal_indices,\n                 vertices,\n                 uvs,\n                 normals):\n        self.vertices = vertices\n        self.indices = indices\n        self.uv_indices = uv_indices\n        self.normal_indices = normal_indices\n        self.uvs = uvs\n        self.normals = normals\n\ndef load_mtl(filename):\n    mtllib = {}\n    current_mtl = WavefrontMaterial()\n    with open(filename, \'r\') as f:\n        for line in f.readlines():\n            line = line.strip()\n            splitted = re.split(\'\\ +\', line)\n            if splitted[0] == \'newmtl\':\n                if current_mtl.name != """":\n                    mtllib[current_mtl.name] = current_mtl\n                current_mtl = WavefrontMaterial()\n                current_mtl.name = splitted[1]\n            elif splitted[0] == \'Kd\':\n                current_mtl.Kd = (float(splitted[1]), float(splitted[2]), float(splitted[3]))\n            elif splitted[0] == \'Ks\':\n                current_mtl.Ks = (float(splitted[1]), float(splitted[2]), float(splitted[3]))\n            elif splitted[0] == \'Ns\':\n                current_mtl.Ns = float(splitted[1])\n            elif splitted[0] == \'Ke\':\n                current_mtl.Ke = (float(splitted[1]), float(splitted[2]), float(splitted[3]))\n            elif splitted[0] == \'map_Kd\':\n                current_mtl.map_Kd = splitted[1]\n            elif splitted[0] == \'map_Ks\':\n                current_mtl.map_Ks = splitted[1]\n            elif splitted[0] == \'map_Ns\':\n                current_mtl.map_Ns = splitted[1]\n    if current_mtl.name != """":\n        mtllib[current_mtl.name] = current_mtl\n    return mtllib\n\ndef load_obj(filename: str,\n             obj_group: bool = True,\n             flip_tex_coords: bool = True,\n             use_common_indices: bool = False,\n             return_objects: bool = False):\n    """"""\n        Load from a Wavefront obj file as PyTorch tensors.\n\n        Args\n        ====\n        obj_group: bool\n            split the meshes based on materials\n        flip_tex_coords: bool\n            flip the v coordinate of uv by applying v\' = 1 - v\n        use_common_indices: bool\n            Use the same indices for position, uvs, normals.\n            Not recommended since texture seams in the objects sharing\n            the same positions would cause the optimization to ""tear"" the object\n        return_objects: bool\n            Output list of Object instead.\n            If there is no corresponding material for a shape, assign a grey material.\n\n        Returns\n        =======\n        if return_objects == True, return a list of Object\n        if return_objects == False, return (material_map, mesh_list, light_map),\n        material_map -> Map[mtl_name, WavefrontMaterial]\n        mesh_list -> List[TriangleMesh]\n        light_map -> Map[mtl_name, torch.Tensor]\n    """"""\n    vertices_pool = []\n    uvs_pool = []\n    normals_pool = []\n    indices = []\n    uv_indices = []\n    normal_indices = []\n    vertices = []\n    uvs = []\n    normals = []\n    vertices_map = {}\n    uvs_map = {}\n    normals_map = {}\n    material_map = {}\n    current_mtllib = {}\n    current_material_name = None\n\n    def create_mesh(indices,\n                    uv_indices,\n                    normal_indices,\n                    vertices,\n                    uvs,\n                    normals):\n        indices = tf.constant(indices, dtype = tf.int32)\n        if len(uv_indices) == 0:\n            uv_indices = None\n        else:\n            uv_indices = tf.constant(uv_indices, dtype = tf.int32)\n        if len(normal_indices) == 0:\n            normal_indices = None\n        else:\n            normal_indices = tf.constant(normal_indices, dtype = tf.int32)\n        vertices = tf.constant(vertices)\n        if len(uvs) == 0:\n            uvs = None\n        else:\n            uvs = tf.constant(uvs)\n        if len(normals) == 0:\n            normals = None\n        else:\n            normals = tf.constant(normals)\n        return TriangleMesh(indices,\n                            uv_indices,\n                            normal_indices,\n                            vertices,\n                            uvs,\n                            normals)\n\n    mesh_list = []\n    light_map = {}\n\n    with open(filename, \'r\') as f:\n        d = os.path.dirname(filename)\n        cwd = os.getcwd()\n        if d != \'\':\n            os.chdir(d)\n        for line in f:\n            line = line.strip()\n            splitted = re.split(\'\\ +\', line)\n            if splitted[0] == \'mtllib\':\n                current_mtllib = load_mtl(splitted[1])\n            elif splitted[0] == \'usemtl\':\n                if len(indices) > 0 and obj_group is True:\n                    # Flush\n                    mesh_list.append((current_material_name,\n                        create_mesh(indices, uv_indices, normal_indices,\n                                    vertices, uvs, normals)))\n                    indices = []\n                    uv_indices = []\n                    normal_indices = []\n                    vertices = []\n                    normals = []\n                    uvs = []\n                    vertices_map = {}\n                    uvs_map = {}\n                    normals_map = {}\n\n                mtl_name = splitted[1]\n                current_material_name = mtl_name\n                if mtl_name not in material_map:\n                    m = current_mtllib[mtl_name]\n                    if m.map_Kd is None:\n                        diffuse_reflectance = tf.constant(m.Kd,\n                            dtype = tf.float32)\n                    else:\n                        diffuse_reflectance = pyredner.imread(m.map_Kd)\n                    if m.map_Ks is None:\n                        specular_reflectance = tf.constant(m.Ks,\n                            dtype = tf.float32)\n                    else:\n                        specular_reflectance = pyredner.imread(m.map_Ks)\n                    if m.map_Ns is None:\n                        roughness = tf.constant([2.0 / (m.Ns + 2.0)],\n                            dtype = tf.float32)\n                    else:\n                        roughness = 2.0 / (pyredner.imread(m.map_Ks) + 2.0)\n                    if m.Ke != (0.0, 0.0, 0.0):\n                        light_map[mtl_name] = tf.constant(m.Ke, dtype = tf.float32)\n                    material_map[mtl_name] = pyredner.Material(\n                        diffuse_reflectance, specular_reflectance, roughness)\n            elif splitted[0] == \'v\':\n                vertices_pool.append([float(splitted[1]), float(splitted[2]), float(splitted[3])])\n            elif splitted[0] == \'vt\':\n                u = float(splitted[1])\n                v = float(splitted[2])\n                if flip_tex_coords:\n                    v = 1 - v\n                uvs_pool.append([u, v])\n            elif splitted[0] == \'vn\':\n                normals_pool.append([float(splitted[1]), float(splitted[2]), float(splitted[3])])\n            elif splitted[0] == \'f\':\n                def num_indices(x):\n                    return len(re.split(\'/\', x))\n                def get_index(x, i):\n                    return int(re.split(\'/\', x)[i])\n                def parse_face_index(x, i):\n                    f = get_index(x, i)\n                    if f > 0:\n                        f -= 1\n                    return f\n                assert(len(splitted) <= 5)\n                def get_vertex_id(indices):\n                    pi = parse_face_index(indices, 0)\n                    uvi = None\n                    if (num_indices(indices) > 1 and re.split(\'/\', indices)[1] != \'\'):\n                        uvi = parse_face_index(indices, 1)\n                    ni = None\n                    if (num_indices(indices) > 2 and re.split(\'/\', indices)[2] != \'\'):\n                        ni = parse_face_index(indices, 2)\n                    if use_common_indices:\n                        # vertex, uv, normals share the same indexing\n                        key = (pi, uvi, ni)\n                        if key in vertices_map:\n                            vertex_id = vertices_map[key]\n                            return vertex_id, vertex_id, vertex_id\n\n                        vertex_id = len(vertices)\n                        vertices_map[key] = vertex_id\n                        vertices.append(vertices_pool[pi])\n                        if uvi is not None:\n                            uvs.append(uvs_pool[uvi])\n                        if ni is not None:\n                            normals.append(normals_pool[ni])\n                        return vertex_id, vertex_id, vertex_id\n                    else:\n                        # vertex, uv, normals use separate indexing\n                        vertex_id = None\n                        uv_id = None\n                        normal_id = None\n\n                        if pi in vertices_map:\n                            vertex_id = vertices_map[pi]\n                        else:\n                            vertex_id = len(vertices)\n                            vertices.append(vertices_pool[pi])\n                            vertices_map[pi] = vertex_id\n\n                        if uvi is not None:\n                            if uvi in uvs_map:\n                                uv_id = uvs_map[uvi]\n                            else:\n                                uv_id = len(uvs)\n                                uvs.append(uvs_pool[uvi])\n                                uvs_map[uvi] = uv_id\n\n                        if ni is not None:\n                            if ni in normals_map:\n                                normal_id = normals_map[ni]\n                            else:\n                                normal_id = len(normals)\n                                normals.append(normals_pool[ni])\n                                normals_map[ni] = normal_id\n                        return vertex_id, uv_id, normal_id\n\n                vid0, uv_id0, n_id0 = get_vertex_id(splitted[1])\n                vid1, uv_id1, n_id1 = get_vertex_id(splitted[2])\n                vid2, uv_id2, n_id2 = get_vertex_id(splitted[3])\n\n                indices.append([vid0, vid1, vid2])\n                if uv_id0 is not None:\n                    assert(uv_id1 is not None and uv_id2 is not None)\n                    uv_indices.append([uv_id0, uv_id1, uv_id2])\n                if n_id0 is not None:\n                    assert(n_id1 is not None and n_id2 is not None)\n                    normal_indices.append([n_id0, n_id1, n_id2])\n                if (len(splitted) == 5):\n                    vid3, uv_id3, n_id3 = get_vertex_id(splitted[4])\n                    indices.append([vid0, vid2, vid3])\n                    if uv_id0 is not None:\n                        assert(uv_id3 is not None)\n                        uv_indices.append([uv_id0, uv_id2, uv_id3])\n                    if n_id0 is not None:\n                        assert(n_id3 is not None)\n                        normal_indices.append([n_id0, n_id2, n_id3])\n    \n    mesh_list.append((current_material_name,\n        create_mesh(indices, uv_indices, normal_indices, vertices, uvs, normals)))\n    if d != \'\':\n        os.chdir(cwd)\n\n    if return_objects:\n        objects = []\n        for mtl_name, mesh in mesh_list:\n            if mtl_name in material_map:\n                m = material_map[mtl_name]\n            else:\n                m = pyredner.Material(diffuse_reflectance = \\\n                        tf.constant((0.5, 0.5, 0.5)))\n            if mtl_name in light_map:\n                l = light_map[mtl_name]\n            else:\n                l = None\n            objects.append(pyredner.Object(\\\n                vertices = mesh.vertices,\n                indices = mesh.indices,\n                material = m,\n                light_intensity = l,\n                uvs = mesh.uvs,\n                normals = mesh.normals,\n                uv_indices = mesh.uv_indices,\n                normal_indices = mesh.normal_indices))\n        return objects\n    else:\n        return material_map, mesh_list, light_map\n'"
pyredner_tensorflow/material.py,19,"b'import pyredner_tensorflow as pyredner\nimport tensorflow as tf\nfrom typing import Union, Optional\n\nclass Material:\n    """"""\n        redner currently employs a two-layer diffuse-specular material model.\n        More specifically, it is a linear blend between a Lambertian model and\n        a microfacet model with Phong distribution, with Schilick\'s Fresnel approximation.\n        It takes either constant color or 2D textures for the reflectances\n        and roughness, and an optional normal map texture.\n        It can also use vertex color stored in the Shape. In this case\n        the model fallback to a diffuse model.\n\n        Args\n        ====\n        diffuse_reflectance: Optional[Union[tf.Tensor, pyredner.Texture]]\n            a float32 tensor with size 3 or [height, width, 3] or a Texture\n            optional if use_vertex_color is True\n        specular_reflectance: Optional[Union[tf.Tensor, pyredner.Texture]]\n            a float32 tensor with size 3 or [height, width, 3] or a Texture\n        roughness: Optional[Union[tf.Tensor, pyredner.Texture]]\n            a float32 tensor with size 1 or [height, width, 1] or a Texture\n        generic_texture: Optional[Union[tf.Tensor, pyredner.Texture]]\n            a float32 tensor with dimension 1 or 3, arbitrary number of channels\n            use render_g_buffer to visualize this texture\n        normal_map: Optional[Union[tf.Tensor, pyredner.Texture]]\n            a float32 tensor with size 3 or [height, width, 3] or a Texture\n        two_sided: bool\n            By default, the material only reflect lights on the side the\n            normal is pointing to.\n            Set this to True to make the material reflects from both sides.\n        use_vertex_color: bool\n            ignores the reflectances and use the vertex color as diffuse color\n    """"""\n    def __init__(self,\n                 diffuse_reflectance: Optional[Union[tf.Tensor, pyredner.Texture]] = None,\n                 specular_reflectance: Optional[Union[tf.Tensor, pyredner.Texture]] = None,\n                 roughness: Optional[Union[tf.Tensor, pyredner.Texture]] = None,\n                 generic_texture: Optional[Union[tf.Tensor, pyredner.Texture]] = None,\n                 normal_map: Optional[Union[tf.Tensor, pyredner.Texture]] = None,\n                 two_sided: bool = False,\n                 use_vertex_color: bool = False):\n        if diffuse_reflectance is None:\n            diffuse_reflectance = pyredner.Texture(tf.zeros([3], dtype=tf.float32))\n        if specular_reflectance is None:\n            specular_reflectance = pyredner.Texture(tf.zeros([3], dtype=tf.float32))\n            compute_specular_lighting = False\n        else:\n            compute_specular_lighting = True\n        if roughness is None:\n            roughness = pyredner.Texture(tf.ones([1], dtype=tf.float32))\n\n        # Convert to constant texture if necessary\n        if tf.is_tensor(diffuse_reflectance):\n            diffuse_reflectance = pyredner.Texture(diffuse_reflectance)\n        if tf.is_tensor(specular_reflectance):\n            specular_reflectance = pyredner.Texture(specular_reflectance)\n        if tf.is_tensor(roughness):\n            roughness = pyredner.Texture(roughness)\n        if generic_texture is not None and tf.is_tensor(generic_texture):\n            generic_texture = pyredner.Texture(generic_texture)\n        if normal_map is not None and tf.is_tensor(normal_map):\n            normal_map = pyredner.Texture(normal_map)\n\n        assert((len(diffuse_reflectance.texels.shape) == 1 and diffuse_reflectance.texels.shape[0] == 3) or \\\n               (len(diffuse_reflectance.texels.shape) == 3 and diffuse_reflectance.texels.shape[2] == 3))\n        assert((len(specular_reflectance.texels.shape) == 1 and specular_reflectance.texels.shape[0] == 3) or \\\n               (len(specular_reflectance.texels.shape) == 3 and specular_reflectance.texels.shape[2] == 3))\n        assert((len(roughness.texels.shape) == 1 and roughness.texels.shape[0] == 1) or \\\n               (len(roughness.texels.shape) == 3 and roughness.texels.shape[2] == 1))\n        if normal_map is not None:\n            assert((len(normal_map.texels.shape) == 1 and normal_map.texels.shape[0] == 1) or \\\n                   (len(normal_map.texels.shape) == 3 and normal_map.texels.shape[2] == 1))\n\n        self.diffuse_reflectance = diffuse_reflectance\n        self._specular_reflectance = specular_reflectance\n        self.compute_specular_lighting = compute_specular_lighting\n        self.roughness = roughness\n        self.generic_texture = generic_texture\n        self.normal_map = normal_map\n        self.two_sided = two_sided\n        self.use_vertex_color = use_vertex_color\n\n    @property\n    def specular_reflectance(self):\n        return self._specular_reflectance\n\n    @specular_reflectance.setter\n    def specular_reflectance(self, value):\n        self._specular_reflectance = value\n        if value is not None:\n            self.compute_specular_lighting = True\n        else:\n            self._specular_reflectance = pyredner.Texture(\\\n                tf.zeros([3], dtype=tf.float32))\n            self.compute_specular_lighting = False\n\n    def state_dict(self):\n        return {\n            \'diffuse_reflectance\': self.diffuse_reflectance.state_dict(),\n            \'specular_reflectance\': self.specular_reflectance.state_dict(),\n            \'roughness\': self.roughness.state_dict(),\n            \'generic_texture\': self.generic_texture.state_dict(),\n            \'normal_map\': self.normal_map.state_dict(),\n            \'two_sided\': self.two_sided,\n            \'use_vertex_color\': self.use_vertex_color\n        }\n\n    @classmethod\n    def load_state_dict(cls, state_dict):\n        normal_map = state_dict[\'normal_map\']\n        out = cls(\n            pyredner.Texture.load_state_dict(state_dict[\'diffuse_reflectance\']),\n            pyredner.Texture.load_state_dict(state_dict[\'specular_reflectance\']),\n            pyredner.Texture.load_state_dict(state_dict[\'roughness\']),\n            pyredner.Texture.load_state_dict(generic_texture) if generic_texture is not None else None,\n            pyredner.Texture.load_state_dict(normal_map) if normal_map is not None else None,\n            state_dict[\'two_sided\'],\n            state_dict[\'use_vertex_color\'])\n        return out\n'"
pyredner_tensorflow/object.py,23,"b'import pyredner_tensorflow as pyredner\nimport tensorflow as tf\nfrom typing import Optional\n\nclass Object:\n    """"""\n        Object combines geometry, material, and lighting information\n        and aggregate them in a single class. This is a convinent class\n        for constructing redner scenes.\n\n        redner supports only triangle meshes for now. It stores a pool of\n        vertices and access the pool using integer index. Some times the\n        two vertices can have the same 3D position but different texture\n        coordinates, because UV mapping creates seams and need to duplicate\n        vertices. In this can we can use an additional ""uv_indices"" array\n        to access the uv pool.\n\n        Args\n        ====\n        vertices: tf.Tensor\n            3D position of vertices\n            float32 tensor with size num_vertices x 3\n        indices: tf.Tensor\n            vertex indices of triangle faces.\n            int32 tensor with size num_triangles x 3\n        material: pyredner.Material\n\n        light_intensity: Optional[tf.Tensor]\n            make this object an area light\n            float32 tensor with size 3\n        light_two_sided: boolean\n            Does the light emit from two sides of the shape?\n        uvs: Optional[tf.Tensor]:\n            optional texture coordinates.\n            float32 tensor with size num_uvs x 2\n            doesn\'t need to be the same size with vertices if uv_indices is None\n        normals: Optional[tf.Tensor]\n            shading normal\n            float32 tensor with size num_normals x 3\n            doesn\'t need to be the same size with vertices if normal_indices is None\n        uv_indices: Optional[tf.Tensor]\n            overrides indices when accessing uv coordinates\n            int32 tensor with size num_uvs x 2\n        normal_indices: Optional[tf.Tensor]\n            overrides indices when accessing shading normals\n            int32 tensor with size num_normals x 2\n        colors: Optional[tf.Tensor]\n            optional per-vertex color\n            float32 tensor with size num_vertices x 3\n    """"""\n    def __init__(self,\n                 vertices: tf.Tensor,\n                 indices: tf.Tensor,\n                 material: pyredner.Material,\n                 light_intensity: Optional[tf.Tensor] = None,\n                 light_two_sided: bool = False,\n                 uvs: Optional[tf.Tensor] = None,\n                 normals: Optional[tf.Tensor] = None,\n                 uv_indices: Optional[tf.Tensor] = None,\n                 normal_indices: Optional[tf.Tensor] = None,\n                 colors: Optional[tf.Tensor] = None):\n        assert(vertices.dtype == tf.float32)\n        assert(indices.dtype == tf.int32)\n        if uvs is not None:\n            assert(uvs.dtype == tf.float32)\n        if normals is not None:\n            assert(normals.dtype == tf.float32)\n        if uv_indices is not None:\n            assert(uv_indices.dtype == tf.int32)\n        if normal_indices is not None:\n            assert(normal_indices.dtype == tf.int32)\n        if colors is not None:\n            assert(colors.dtype == tf.float32)\n\n        self.vertices = vertices\n        self.indices = indices\n        self.uvs = uvs\n        self.normals = normals\n        self.uv_indices = uv_indices\n        self.normal_indices = normal_indices\n        self.colors = colors\n        self.material = material\n        self.light_intensity = light_intensity\n        self.light_two_sided = light_two_sided\n'"
pyredner_tensorflow/redner_enum_wrapper.py,9,"b""import tensorflow as tf\nimport redner\n\nclass RednerCameraType:\n    __cameratypes = [\n        redner.CameraType.perspective,\n        redner.CameraType.orthographic,\n        redner.CameraType.fisheye,\n        redner.CameraType.panorama,\n    ]\n\n    @staticmethod\n    def asTensor(cameratype: redner.CameraType) -> tf.Tensor:\n        assert isinstance(cameratype, redner.CameraType)\n\n        for i in range(len(RednerCameraType.__cameratypes)):\n            if RednerCameraType.__cameratypes[i] == cameratype:\n                return tf.constant(i)\n\n\n    @staticmethod\n    def asCameraType(index: tf.Tensor) -> redner.CameraType:\n        try:\n            cameratype = RednerCameraType.__cameratypes[index]\n        except IndexError:\n            print(f'{index} is out of range: [0, {len(RednerCameraType.__cameratypes)})')\n            import sys\n            sys.exit()\n        else:\n            return cameratype\n\n\nclass RednerChannels:\n    __channels = [\n        redner.channels.radiance,\n        redner.channels.alpha,\n        redner.channels.depth,\n        redner.channels.position,\n        redner.channels.geometry_normal,\n        redner.channels.shading_normal,\n        redner.channels.uv,\n        redner.channels.barycentric_coordinates,\n        redner.channels.diffuse_reflectance,\n        redner.channels.specular_reflectance,\n        redner.channels.vertex_color,\n        redner.channels.roughness,\n        redner.channels.generic_texture,\n        redner.channels.vertex_color,\n        redner.channels.shape_id,\n        redner.channels.triangle_id,\n        redner.channels.material_id\n    ]\n\n    @staticmethod\n    def asTensor(channel: redner.channels) -> tf.Tensor:\n        assert isinstance(channel, redner.channels)\n\n        for i in range(len(RednerChannels.__channels)):\n            if RednerChannels.__channels[i] == channel:\n                return tf.constant(i)\n\n    @staticmethod\n    def asChannel(index: tf.Tensor) -> redner.channels:\n        try:\n            channel = RednerChannels.__channels[index]\n        except IndexError:\n            print(f'{index} is out of range: [0, {len(RednerChannels.__channels)})')\n            import sys\n            sys.exit()\n        else:\n            return channel\n\n\nclass RednerSamplerType:\n    __samplertypes = [\n        redner.SamplerType.independent,\n        redner.SamplerType.sobol\n    ]\n\n    @staticmethod\n    def asTensor(samplertype: redner.SamplerType) -> tf.Tensor:\n        assert isinstance(samplertype, redner.SamplerType)\n\n        for i in range(len(RednerSamplerType.__samplertypes)):\n            if RednerSamplerType.__samplertypes[i] == samplertype:\n                return tf.constant(i)\n\n\n    @staticmethod\n    def asSamplerType(index: tf.Tensor) -> redner.SamplerType:\n        try:\n            samplertype = RednerSamplerType.__samplertypes[index]\n        except IndexError:\n            print(f'{index} is out of range: [0, {len(RednerSamplerType.__samplertypes)})')\n            import sys\n            sys.exit()\n        else:\n            return samplertype\n"""
pyredner_tensorflow/render_tensorflow.py,122,"b'import tensorflow as tf\nimport numpy as np\nimport redner\nimport pyredner_tensorflow as pyredner\nimport time\nimport weakref\nimport os\nfrom typing import List, Union, Tuple, Optional\nfrom .redner_enum_wrapper import RednerCameraType, RednerSamplerType, RednerChannels\n\n__EMPTY_TENSOR = tf.constant([])\nuse_correlated_random_number = False\ndef set_use_correlated_random_number(v: bool):\n    """"""\n        | There is a bias-variance trade off in the backward pass.\n        | If the forward pass and the backward pass are correlated\n        | the gradients are biased for L2 loss.\n        | (E[d/dx(f(x) - y)^2] = E[(f(x) - y) d/dx f(x)])\n        |                      = E[f(x) - y] E[d/dx f(x)]\n        | The last equation only holds when f(x) and d/dx f(x) are independent.\n        | It is usually better to use the unbiased one, but we left it as an option here\n    """"""\n    global use_correlated_random_number\n    use_correlated_random_number = v\n\ndef get_use_correlated_random_number():\n    """"""\n        See set_use_correlated_random_number\n    """"""\n    global use_correlated_random_number\n    return use_correlated_random_number\n\ndef get_tensor_dimension(t):\n    """"""Return dimension of the TF tensor in Int\n\n    `get_shape()` returns `TensorShape`.\n\n    """"""\n    return len(t.get_shape())\n\ndef is_empty_tensor(tensor):\n    return  tf.equal(tf.size(tensor), 0)\n\n\nclass Context: pass\n\nprint_timing = True\ndef set_print_timing(v: bool):\n    """"""\n        Set whether to print time measurements or not.\n    """"""\n    global print_timing\n    print_timing = v\n\ndef get_print_timing():\n    """"""\n        Get whether we print time measurements or not.\n    """"""\n    global print_timing\n    return print_timing\n\ndef serialize_texture(texture, args, device_name):\n    if texture is None:\n        args.append(tf.constant(0))\n        return\n    args.append(tf.constant(len(texture.mipmap)))\n    with tf.device(device_name):\n        for mipmap in texture.mipmap:\n            args.append(tf.identity(mipmap))\n        args.append(tf.identity(texture.uv_scale))\n\ndef serialize_scene(scene: pyredner.Scene,\n                    num_samples: Union[int, Tuple[int, int]],\n                    max_bounces: int,\n                    channels = [redner.channels.radiance],\n                    sampler_type = redner.SamplerType.independent,\n                    use_primary_edge_sampling = True,\n                    use_secondary_edge_sampling = True,\n                    sample_pixel_center: bool = False,\n                    device_name: Optional[str] = None) -> List:\n    """"""\n        Given a pyredner scene & rendering options, convert them to a linear list of argument,\n        so that we can use it in TensorFlow.\n\n        Args\n        ====\n        scene: pyredner.Scene\n        num_samples: int\n            number of samples per pixel for forward and backward passes\n            can be an integer or a tuple of 2 integers\n            if a single integer is provided, use the same number of samples\n            for both\n        max_bounces: int\n            number of bounces for global illumination\n            1 means direct lighting only\n        channels: List[redner.channels]\n            | A list of channels that should present in the output image\n            | following channels are supported\\:\n            | redner.channels.radiance,\n            | redner.channels.alpha,\n            | redner.channels.depth,\n            | redner.channels.position,\n            | redner.channels.geometry_normal,\n            | redner.channels.shading_normal,\n            | redner.channels.uv,\n            | redner.channels.barycentric_coordinates,\n            | redner.channels.diffuse_reflectance,\n            | redner.channels.specular_reflectance,\n            | redner.channels.vertex_color,\n            | redner.channels.roughness,\n            | redner.channels.generic_texture,\n            | redner.channels.shape_id,\n            | redner.channels.triangle_id,\n            | redner.channels.material_id\n            | all channels, except for shape id, triangle id and material id, are differentiable\n        sampler_type: redner.SamplerType\n            | Which sampling pattern to use?\n            | see `Chapter 7 of the PBRT book <http://www.pbr-book.org/3ed-2018/Sampling_and_Reconstruction.html>`\n              for an explanation of the difference between different samplers.\n            | Following samplers are supported:\n            | redner.SamplerType.independent\n            | redner.SamplerType.sobol\n        use_primary_edge_sampling: bool\n\n        use_secondary_edge_sampling: bool\n\n        sample_pixel_center: bool\n            Always sample at the pixel center when rendering.\n            This trades noise with aliasing.\n            If this option is activated, the rendering becomes non-differentiable\n            (since there is no antialiasing integral),\n            and redner\'s edge sampling becomes an approximation to the gradients of the aliased rendering.\n\n        device_name: Optional[str]\n            Which device should we store the data in.\n            If set to None, use the device from pyredner.get_device_name().\n    """"""\n    if device_name is None:\n        device_name = pyredner.get_device_name()\n\n    # TODO: figure out a way to determine whether a TF tensor requires gradient or not\n    cam = scene.camera\n    num_shapes = len(scene.shapes)\n    num_materials = len(scene.materials)\n    num_lights = len(scene.area_lights)\n    num_channels = len(channels)\n\n    for light_id, light in enumerate(scene.area_lights):\n        scene.shapes[light.shape_id].light_id = light_id\n\n    if max_bounces == 0:\n        use_secondary_edge_sampling = False\n\n    args = []\n    args.append(tf.constant(device_name))\n    args.append(tf.constant(num_shapes))\n    args.append(tf.constant(num_materials))\n    args.append(tf.constant(num_lights))\n    with tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n        if cam.position is None:\n            args.append(__EMPTY_TENSOR)\n            args.append(__EMPTY_TENSOR)\n            args.append(__EMPTY_TENSOR)\n        else:\n            args.append(tf.identity(cam.position))\n            args.append(tf.identity(cam.look_at))\n            args.append(tf.identity(cam.up))\n        if cam.cam_to_world is None:\n            args.append(__EMPTY_TENSOR)\n            args.append(__EMPTY_TENSOR)\n        else:\n            args.append(tf.identity(cam.cam_to_world))\n            args.append(tf.identity(cam.world_to_cam))\n        args.append(tf.identity(cam.intrinsic_mat_inv))\n        args.append(tf.identity(cam.intrinsic_mat))\n        if cam.distortion_params is not None:\n            args.append(tf.identity(cam.distortion_params))\n        else:\n            args.append(__EMPTY_TENSOR)\n    args.append(tf.constant(cam.clip_near))\n    args.append(tf.constant(cam.resolution))\n    viewport = cam.viewport\n    if viewport is None:\n        viewport = (0, 0, cam.resolution[0], cam.resolution[1])\n    # Clamp the viewport if necessary\n    viewport = (max(viewport[0], 0),\n                max(viewport[1], 0),\n                min(viewport[2], cam.resolution[0]),\n                min(viewport[3], cam.resolution[1]))\n    args.append(tf.constant(viewport))\n    args.append(RednerCameraType.asTensor(cam.camera_type))\n    for shape in scene.shapes:\n        with tf.device(device_name):\n            args.append(tf.identity(shape.vertices))\n            # HACK: tf.bitcast forces tensorflow to copy int32 to GPU memory.\n            # tf.identity stopped working since TF 2.1 (if you print the device\n            # it will say it\'s on GPU, but the address returned by data_ptr is wrong).\n            # Hopefully TF people will fix this in the future.\n            args.append(tf.bitcast(shape.indices, type=tf.int32))\n            if shape.uvs is None:\n                args.append(__EMPTY_TENSOR)\n            else:\n                args.append(tf.identity(shape.uvs))\n            if shape.normals is None:\n                args.append(__EMPTY_TENSOR)\n            else:\n                args.append(tf.identity(shape.normals))\n            if shape.uv_indices is None:\n                args.append(__EMPTY_TENSOR)\n            else:\n                args.append(tf.bitcast(shape.uv_indices, type=tf.int32))\n            if shape.normal_indices is None:\n                args.append(__EMPTY_TENSOR)\n            else:\n                args.append(tf.bitcast(shape.normal_indices, type=tf.int32))\n            if shape.colors is None:\n                args.append(__EMPTY_TENSOR)\n            else:\n                args.append(tf.identity(shape.colors))\n        args.append(tf.constant(shape.material_id))\n        args.append(tf.constant(shape.light_id))\n    for material in scene.materials:\n        serialize_texture(material.diffuse_reflectance, args, device_name)\n        serialize_texture(material.specular_reflectance, args, device_name)\n        serialize_texture(material.roughness, args, device_name)\n        serialize_texture(material.generic_texture, args, device_name)\n        serialize_texture(material.normal_map, args, device_name)\n        args.append(tf.constant(material.compute_specular_lighting))\n        args.append(tf.constant(material.two_sided))\n        args.append(tf.constant(material.use_vertex_color))\n    with tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n        for light in scene.area_lights:\n            args.append(tf.constant(light.shape_id))\n            args.append(tf.identity(light.intensity))\n            args.append(tf.constant(light.two_sided))\n            args.append(tf.constant(light.directly_visible))\n    if scene.envmap is not None:\n        serialize_texture(scene.envmap.values, args, device_name)\n        with tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n            args.append(tf.identity(scene.envmap.env_to_world))\n            args.append(tf.identity(scene.envmap.world_to_env))\n        with tf.device(device_name):\n            args.append(tf.identity(scene.envmap.sample_cdf_ys))\n            args.append(tf.identity(scene.envmap.sample_cdf_xs))\n        args.append(scene.envmap.pdf_norm)\n        args.append(scene.envmap.directly_visible)\n    else:\n        args.append(__EMPTY_TENSOR)\n\n    args.append(tf.constant(num_samples))\n    args.append(tf.constant(max_bounces))\n    args.append(tf.constant(num_channels))\n    for ch in channels:\n        args.append(RednerChannels.asTensor(ch))\n\n    args.append(RednerSamplerType.asTensor(sampler_type))\n    args.append(tf.constant(use_primary_edge_sampling))\n    args.append(tf.constant(use_secondary_edge_sampling))\n    args.append(tf.constant(sample_pixel_center))\n    return args\n\ndef unpack_args(seed,\n                args,\n                use_primary_edge_sampling = None,\n                use_secondary_edge_sampling = None):\n    """"""\n        Given a list of serialized scene arguments, unpack\n        all information into a Context.\n    """"""\n    # Unpack arguments\n    current_index = 0\n    device_name = args[current_index].numpy().decode().lower()\n    current_index += 1\n    num_shapes = int(args[current_index])\n    current_index += 1\n    num_materials = int(args[current_index])\n    current_index += 1\n    num_lights = int(args[current_index])\n    current_index += 1\n\n    # Camera arguments\n    cam_position = args[current_index]\n    current_index += 1\n    cam_look_at = args[current_index]\n    current_index += 1\n    cam_up = args[current_index]\n    current_index += 1\n    cam_to_world = args[current_index]\n    current_index += 1\n    world_to_cam = args[current_index]\n    current_index += 1\n    intrinsic_mat_inv = args[current_index]\n    current_index += 1\n    intrinsic_mat = args[current_index]\n    current_index += 1\n    distortion_params = args[current_index]\n    current_index += 1\n    clip_near = float(args[current_index])\n    current_index += 1\n    resolution = args[current_index].numpy() # Tuple[int, int]\n    current_index += 1\n    viewport = args[current_index].numpy() # Tuple[int, int, int, int]\n    current_index += 1\n    camera_type = RednerCameraType.asCameraType(args[current_index]) # FIXME: Map to custom type\n    current_index += 1\n\n    with tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n        if is_empty_tensor(cam_to_world):\n            camera = redner.Camera(resolution[1],\n                                   resolution[0],\n                                   redner.float_ptr(pyredner.data_ptr(cam_position)),\n                                   redner.float_ptr(pyredner.data_ptr(cam_look_at)),\n                                   redner.float_ptr(pyredner.data_ptr(cam_up)),\n                                   redner.float_ptr(0), # cam_to_world\n                                   redner.float_ptr(0), # world_to_cam\n                                   redner.float_ptr(pyredner.data_ptr(intrinsic_mat_inv)),\n                                   redner.float_ptr(pyredner.data_ptr(intrinsic_mat)),\n                                   redner.float_ptr(pyredner.data_ptr(distortion_params) if distortion_params is not None else 0),\n                                   clip_near,\n                                   camera_type,\n                                   redner.Vector2i(viewport[1], viewport[0]),\n                                   redner.Vector2i(viewport[3], viewport[2]))\n        else:\n            camera = redner.Camera(resolution[1],\n                                   resolution[0],\n                                   redner.float_ptr(0),\n                                   redner.float_ptr(0),\n                                   redner.float_ptr(0),\n                                   redner.float_ptr(pyredner.data_ptr(cam_to_world)),\n                                   redner.float_ptr(pyredner.data_ptr(world_to_cam)),\n                                   redner.float_ptr(pyredner.data_ptr(intrinsic_mat_inv)),\n                                   redner.float_ptr(pyredner.data_ptr(intrinsic_mat)),\n                                   redner.float_ptr(pyredner.data_ptr(distortion_params) if distortion_params is not None else 0),\n                                   clip_near,\n                                   camera_type,\n                                   redner.Vector2i(viewport[1], viewport[0]),\n                                   redner.Vector2i(viewport[3], viewport[2]))\n\n    with tf.device(device_name):\n        shapes = []\n        for i in range(num_shapes):\n            vertices = args[current_index]\n            current_index += 1\n            indices = args[current_index]\n            current_index += 1\n            uvs = args[current_index]\n            current_index += 1\n            normals = args[current_index]\n            current_index += 1\n            uv_indices = args[current_index]\n            current_index += 1\n            normal_indices = args[current_index]\n            current_index += 1\n            colors = args[current_index]\n            current_index += 1\n            material_id = int(args[current_index])\n            current_index += 1\n            light_id = int(args[current_index])\n            current_index += 1\n\n            shapes.append(redner.Shape(\\\n                redner.float_ptr(pyredner.data_ptr(vertices)),\n                redner.int_ptr(pyredner.data_ptr(indices)),\n                redner.float_ptr(pyredner.data_ptr(uvs) if not is_empty_tensor(uvs) else 0),\n                redner.float_ptr(pyredner.data_ptr(normals) if not is_empty_tensor(normals) else 0),\n                redner.int_ptr(pyredner.data_ptr(uv_indices) if not is_empty_tensor(uv_indices) else 0),\n                redner.int_ptr(pyredner.data_ptr(normal_indices) if not is_empty_tensor(normal_indices) else 0),\n                redner.float_ptr(pyredner.data_ptr(colors) if not is_empty_tensor(colors) else 0),\n                int(vertices.shape[0]),\n                int(uvs.shape[0]) if not is_empty_tensor(uvs) else 0,\n                int(normals.shape[0]) if not is_empty_tensor(normals) else 0,\n                int(indices.shape[0]),\n                material_id,\n                light_id))\n\n    materials = []\n    with tf.device(device_name):\n        for i in range(num_materials):\n            num_levels = int(args[current_index])\n            current_index += 1\n            diffuse_reflectance = []\n            for j in range(num_levels):\n                diffuse_reflectance.append(args[current_index])\n                current_index += 1\n            diffuse_uv_scale = args[current_index]\n            current_index += 1\n\n            num_levels = int(args[current_index])\n            current_index += 1\n            specular_reflectance = []\n            for j in range(num_levels):\n                specular_reflectance.append(args[current_index])\n                current_index += 1\n            specular_uv_scale = args[current_index]\n            current_index += 1\n\n            num_levels = int(args[current_index])\n            current_index += 1\n            roughness = []\n            for j in range(num_levels):\n                roughness.append(args[current_index])\n                current_index += 1\n            roughness_uv_scale = args[current_index]\n            current_index += 1\n\n            num_levels = int(args[current_index])\n            current_index += 1\n            generic_texture = []\n            if num_levels > 0:\n                for j in range(num_levels):\n                    generic_texture.append(args[current_index])\n                    current_index += 1\n                generic_uv_scale = args[current_index]\n                current_index += 1\n            else:\n                generic_uv_scale = None\n\n            num_levels = int(args[current_index])\n            current_index += 1\n            normal_map = []\n            if num_levels > 0:\n                for j in range(num_levels):\n                    normal_map.append(args[current_index])\n                    current_index += 1\n                normal_map_uv_scale = args[current_index]\n                current_index += 1\n            else:\n                normal_map_uv_scale = None\n\n            compute_specular_lighting = bool(args[current_index])\n            current_index += 1\n            two_sided = bool(args[current_index])\n            current_index += 1\n            use_vertex_color = bool(args[current_index])\n            current_index += 1\n\n            if get_tensor_dimension(diffuse_reflectance[0]) == 1:\n                diffuse_reflectance = redner.Texture3(\\\n                    [redner.float_ptr(pyredner.data_ptr(diffuse_reflectance[0]))],\n                    [0],\n                    [0],\n                    3, redner.float_ptr(pyredner.data_ptr(diffuse_uv_scale)))\n            else:\n                assert(get_tensor_dimension(diffuse_reflectance[0]) == 3)\n                diffuse_reflectance = redner.Texture3(\\\n                    [redner.float_ptr(pyredner.data_ptr(x)) for x in diffuse_reflectance],\n                    [x.shape[1] for x in diffuse_reflectance],\n                    [x.shape[0] for x in diffuse_reflectance],\n                    3,\n                    redner.float_ptr(pyredner.data_ptr(diffuse_uv_scale)))\n\n            if get_tensor_dimension(specular_reflectance[0]) == 1:\n                specular_reflectance = redner.Texture3(\\\n                    [redner.float_ptr(pyredner.data_ptr(specular_reflectance[0]))],\n                    [0],\n                    [0],\n                    3, redner.float_ptr(pyredner.data_ptr(specular_uv_scale)))\n            else:\n                assert(get_tensor_dimension(specular_reflectance[0]) == 3)\n                specular_reflectance = redner.Texture3(\\\n                    [redner.float_ptr(pyredner.data_ptr(x)) for x in specular_reflectance],\n                    [x.shape[1] for x in specular_reflectance],\n                    [x.shape[0] for x in specular_reflectance],\n                    3,\n                    redner.float_ptr(pyredner.data_ptr(specular_uv_scale)))\n\n            if get_tensor_dimension(roughness[0]) == 1:\n                roughness = redner.Texture1(\\\n                    [redner.float_ptr(pyredner.data_ptr(roughness[0]))],\n                    [0],\n                    [0],\n                    1, redner.float_ptr(pyredner.data_ptr(roughness_uv_scale)))\n            else:\n                assert(get_tensor_dimension(roughness[0]) == 3)\n                roughness = redner.Texture1(\\\n                    [redner.float_ptr(pyredner.data_ptr(x)) for x in roughness],\n                    [x.shape[1] for x in roughness],\n                    [x.shape[0] for x in roughness],\n                    3,\n                    redner.float_ptr(pyredner.data_ptr(roughness_uv_scale)))\n\n            if len(generic_texture) > 0:\n                generic_texture = redner.TextureN(\\\n                    [redner.float_ptr(pyredner.data_ptr(x)) for x in generic_texture],\n                    [x.shape[1] for x in generic_texture],\n                    [x.shape[0] for x in generic_texture],\n                    generic_texture[0].shape[2],\n                    redner.float_ptr(pyredner.data_ptr(generic_uv_scale)))\n            else:\n                generic_texture = redner.TextureN(\\\n                    [], [], [], 0, redner.float_ptr(0))\n\n            if len(normal_map) > 0:\n                normal_map = redner.Texture3(\\\n                    [redner.float_ptr(pyredner.data_ptr(x)) for x in normal_map],\n                    [x.shape[1] for x in normal_map],\n                    [x.shape[0] for x in normal_map],\n                    normal_map[0].shape[2],\n                    redner.float_ptr(pyredner.data_ptr(normal_map_uv_scale)))\n            else:\n                normal_map = redner.Texture3(\\\n                    [], [], [], 0, redner.float_ptr(0))\n\n            materials.append(redner.Material(\\\n                diffuse_reflectance,\n                specular_reflectance,\n                roughness,\n                generic_texture,\n                normal_map,\n                compute_specular_lighting,\n                two_sided,\n                use_vertex_color))\n\n    with tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n        area_lights = []\n        for i in range(num_lights):\n            shape_id = int(args[current_index])\n            current_index += 1\n            intensity = args[current_index]\n            current_index += 1\n            two_sided = bool(args[current_index])\n            current_index += 1\n            directly_visible = bool(args[current_index])\n            current_index += 1\n\n            area_lights.append(redner.AreaLight(\n                shape_id,\n                redner.float_ptr(pyredner.data_ptr(intensity)),\n                two_sided,\n                directly_visible))\n\n    envmap = None\n    if not is_empty_tensor(args[current_index]):\n        num_levels = args[current_index]\n        current_index += 1\n        values = []\n        for j in range(num_levels):\n            values.append(args[current_index])\n            current_index += 1\n        envmap_uv_scale = args[current_index]\n        current_index += 1\n        env_to_world = args[current_index]\n        current_index += 1\n        world_to_env = args[current_index]\n        current_index += 1\n        sample_cdf_ys = args[current_index]\n        current_index += 1\n        sample_cdf_xs = args[current_index]\n        current_index += 1\n        pdf_norm = float(args[current_index])\n        current_index += 1\n        directly_visible = bool(args[current_index])\n        current_index += 1\n\n        assert isinstance(pdf_norm, float)\n        with tf.device(device_name):\n            sample_cdf_ys = redner.float_ptr(pyredner.data_ptr(sample_cdf_ys))\n            sample_cdf_xs = redner.float_ptr(pyredner.data_ptr(sample_cdf_xs))\n        with tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n            env_to_world = redner.float_ptr(pyredner.data_ptr(env_to_world))\n            world_to_env = redner.float_ptr(pyredner.data_ptr(world_to_env))\n        with tf.device(device_name):\n            values = redner.Texture3(\\\n                [redner.float_ptr(pyredner.data_ptr(x)) for x in values],\n                [x.shape[1] for x in values], # width\n                [x.shape[0] for x in values], # height\n                3, # channels\n                redner.float_ptr(pyredner.data_ptr(envmap_uv_scale)))\n        envmap = redner.EnvironmentMap(\\\n            values,\n            env_to_world,\n            world_to_env,\n            sample_cdf_ys,\n            sample_cdf_xs,\n            pdf_norm,\n            directly_visible)\n    else:\n        current_index += 1\n\n    # Options\n    num_samples = args[current_index]\n    current_index += 1\n    if len(num_samples.shape) == 0 or num_samples.shape[0] == 1:\n        num_samples = int(num_samples)\n    else:\n        assert(num_samples.shape[0] == 2)\n        num_samples = (int(num_samples[0]), int(num_samples[1]))\n    max_bounces = int(args[current_index])\n    current_index += 1\n\n    num_channel_args = int(args[current_index])\n    current_index += 1\n\n    channels = []\n    for _ in range(num_channel_args):\n        ch = args[current_index]\n        ch = RednerChannels.asChannel(ch)\n        channels.append(ch)\n        current_index += 1\n\n    sampler_type = args[current_index]\n    sampler_type = RednerSamplerType.asSamplerType(sampler_type)\n    current_index += 1\n\n    use_primary_edge_sampling = args[current_index]\n    current_index += 1\n    use_secondary_edge_sampling = args[current_index]\n    current_index += 1\n    sample_pixel_center = args[current_index]\n    current_index += 1\n\n    device_spec = tf.DeviceSpec.from_string(device_name)\n    use_gpu = device_spec.device_type == \'GPU\'\n    gpu_index = device_spec.device_index if device_spec.device_index is not None else 0\n\n    start = time.time()\n    scene = redner.Scene(camera,\n                         shapes,\n                         materials,\n                         area_lights,\n                         envmap,\n                         use_gpu,\n                         gpu_index,\n                         use_primary_edge_sampling,\n                         use_secondary_edge_sampling)\n    time_elapsed = time.time() - start\n    if get_print_timing():\n        print(\'Scene construction, time: %.5f s\' % time_elapsed)\n\n    # check that num_samples is a tuple\n    if isinstance(num_samples, int):\n        num_samples = (num_samples, num_samples)\n\n    options = redner.RenderOptions(seed,\n                                   num_samples[0],\n                                   max_bounces,\n                                   channels,\n                                   sampler_type,\n                                   sample_pixel_center)\n\n    ctx = Context()\n    ctx.channels = channels\n    ctx.options = options\n    ctx.resolution = resolution\n    ctx.viewport = viewport\n    ctx.scene = scene\n    ctx.camera = camera\n    ctx.shapes = shapes\n    ctx.materials = materials\n    ctx.area_lights = area_lights\n    ctx.envmap = envmap\n    ctx.options = options\n    ctx.num_samples = num_samples\n    ctx.num_channel_args = num_channel_args\n    ctx.device_name = device_name\n\n    return ctx\n\ndef forward(seed:int, *args):\n    """"""\n        Forward rendering pass: given a serialized scene and output an image.\n    """"""\n\n    args_ctx = unpack_args(seed, args)\n    area_lights = args_ctx.area_lights\n    camera = args_ctx.camera\n    channels = args_ctx.channels\n    envmap = args_ctx.envmap\n    materials = args_ctx.materials\n    num_samples = args_ctx.num_samples\n    options = args_ctx.options\n    resolution = args_ctx.resolution\n    viewport = args_ctx.viewport\n    scene = args_ctx.scene\n    shapes = args_ctx.shapes\n    num_channel_args = args_ctx.num_channel_args\n    num_channels = redner.compute_num_channels(channels,\n                                               scene.max_generic_texture_dimension)\n    device_name = args_ctx.device_name\n\n    with tf.device(device_name):\n        img_height = viewport[2] - viewport[0]\n        img_width = viewport[3] - viewport[1]\n        rendered_image = tf.zeros(\n            shape = [img_height, img_width, num_channels],\n            dtype = tf.float32)\n\n        start = time.time()\n        redner.render(scene,\n                      options,\n                      redner.float_ptr(pyredner.data_ptr(rendered_image)),\n                      redner.float_ptr(0), # d_rendered_image\n                      None, # d_scene\n                      redner.float_ptr(0), # translational_gradient_image\n                      redner.float_ptr(0)) # debug_image\n        time_elapsed = time.time() - start\n        if get_print_timing():\n            print(\'Forward pass, time: %.5f s\' % time_elapsed)\n\n    ctx = Context()\n    ctx.camera = camera\n    ctx.shapes = shapes\n    ctx.materials = materials\n    ctx.area_lights = area_lights\n    ctx.envmap = envmap\n    ctx.scene = scene\n    ctx.options = options\n    ctx.num_samples = num_samples\n    ctx.num_channel_args = num_channel_args\n    ctx.args = args # important to avoid GC on tf tensors\n    ctx.device_name = device_name\n    return rendered_image, ctx\n\ndef create_gradient_buffers(ctx):\n    scene = ctx.scene\n    options = ctx.options\n    camera = ctx.camera\n    device_name = ctx.device_name\n\n    buffers = Context()\n\n    with tf.device(device_name):\n        if camera.use_look_at:\n            buffers.d_position = tf.zeros(3, dtype=tf.float32)\n            buffers.d_look_at = tf.zeros(3, dtype=tf.float32)\n            buffers.d_up = tf.zeros(3, dtype=tf.float32)\n            buffers.d_cam_to_world = None\n            buffers.d_world_to_cam = None\n        else:\n            buffers.d_position = None\n            buffers.d_look_at = None\n            buffers.d_up = None\n            buffers.d_cam_to_world = tf.zeros([4, 4], dtype=tf.float32)\n            buffers.d_world_to_cam = tf.zeros([4, 4], dtype=tf.float32)\n        buffers.d_intrinsic_mat_inv = tf.zeros([3,3], dtype=tf.float32)\n        buffers.d_intrinsic_mat = tf.zeros([3,3], dtype=tf.float32)\n        buffers.d_distortion_params = None\n        if camera.has_distortion_params():\n            buffers.d_distortion_params = tf.zeros(8, dtype=tf.float32)\n        if camera.use_look_at:\n            buffers.d_camera = redner.DCamera(\\\n                redner.float_ptr(pyredner.data_ptr(buffers.d_position)),\n                redner.float_ptr(pyredner.data_ptr(buffers.d_look_at)),\n                redner.float_ptr(pyredner.data_ptr(buffers.d_up)),\n                redner.float_ptr(0), # cam_to_world\n                redner.float_ptr(0), # world_to_cam\n                redner.float_ptr(pyredner.data_ptr(buffers.d_intrinsic_mat_inv)),\n                redner.float_ptr(pyredner.data_ptr(buffers.d_intrinsic_mat)),\n                redner.float_ptr(pyredner.data_ptr(buffers.d_distortion_params) if buffers.d_distortion_params is not None else 0))\n        else:\n            buffers.d_camera = redner.DCamera(\\\n                redner.float_ptr(0),\n                redner.float_ptr(0),\n                redner.float_ptr(0),\n                redner.float_ptr(pyredner.data_ptr(buffers.d_cam_to_world)),\n                redner.float_ptr(pyredner.data_ptr(buffers.d_world_to_cam)),\n                redner.float_ptr(pyredner.data_ptr(buffers.d_intrinsic_mat_inv)),\n                redner.float_ptr(pyredner.data_ptr(buffers.d_intrinsic_mat)),\n                redner.float_ptr(pyredner.data_ptr(buffers.d_distortion_params) if buffers.d_distortion_params is not None else 0))\n\n    buffers.d_vertices_list = []\n    buffers.d_uvs_list = []\n    buffers.d_normals_list = []\n    buffers.d_colors_list = []\n    buffers.d_shapes = []\n    with tf.device(device_name):\n        for i, shape in enumerate(ctx.shapes):\n            num_vertices = shape.num_vertices\n            d_vertices = tf.zeros([num_vertices, 3], dtype=tf.float32)\n            d_uvs = tf.zeros([num_vertices, 2], dtype=tf.float32) if shape.has_uvs() else None\n            d_normals = tf.zeros([num_vertices, 3], dtype=tf.float32) if shape.has_normals() else None\n            d_colors = tf.zeros([num_vertices, 3], dtype=tf.float32) if shape.has_colors() else None\n            buffers.d_vertices_list.append(d_vertices)\n            buffers.d_uvs_list.append(d_uvs)\n            buffers.d_normals_list.append(d_normals)\n            buffers.d_colors_list.append(d_colors)\n            buffers.d_shapes.append(redner.DShape(\\\n                redner.float_ptr(pyredner.data_ptr(d_vertices)),\n                redner.float_ptr(pyredner.data_ptr(d_uvs) if d_uvs is not None else 0),\n                redner.float_ptr(pyredner.data_ptr(d_normals) if d_normals is not None else 0),\n                redner.float_ptr(pyredner.data_ptr(d_colors) if d_colors is not None else 0)))\n\n    buffers.d_diffuse_list = []\n    buffers.d_specular_list = []\n    buffers.d_roughness_list = []\n    buffers.d_normal_map_list = []\n    buffers.d_diffuse_uv_scale_list = []\n    buffers.d_specular_uv_scale_list = []\n    buffers.d_roughness_uv_scale_list = []\n    buffers.d_generic_list = []\n    buffers.d_generic_uv_scale_list = []\n    buffers.d_normal_map_uv_scale_list = []\n    buffers.d_materials = []\n    with tf.device(device_name):\n        for material in ctx.materials:\n            if material.get_diffuse_size(0)[0] == 0:\n                d_diffuse = [tf.zeros(3, dtype=tf.float32)]\n            else:\n                d_diffuse = []\n                for l in range(material.get_diffuse_levels()):\n                    diffuse_size = material.get_diffuse_size(l)\n                    d_diffuse.append(\\\n                        tf.zeros([diffuse_size[1],\n                                  diffuse_size[0],\n                                  3], dtype=tf.float32))\n\n            if material.get_specular_size(0)[0] == 0:\n                d_specular = [tf.zeros(3, dtype=tf.float32)]\n            else:\n                d_specular = []\n                for l in range(material.get_specular_levels()):\n                    specular_size = material.get_specular_size(l)\n                    d_specular.append(\\\n                        tf.zeros([specular_size[1],\n                                  specular_size[0],\n                                  3], dtype=tf.float32))\n\n            if material.get_roughness_size(0)[0] == 0:\n                d_roughness = [tf.zeros(1, dtype=tf.float32)]\n            else:\n                d_roughness = []\n                for l in range(material.get_roughness_levels()):\n                    roughness_size = material.get_roughness_size(l)\n                    d_roughness.append(\\\n                        tf.zeros([roughness_size[1],\n                                  roughness_size[0],\n                                  1], dtype=tf.float32))\n            # HACK: tensorflow\'s eager mode uses a cache to store scalar\n            #       constants to avoid memory copy. If we pass scalar tensors\n            #       into the C++ code and modify them, we would corrupt the\n            #       cache, causing incorrect result in future scalar constant\n            #       creations. Thus we force tensorflow to copy by plusing a zero.\n            # (also see https://github.com/tensorflow/tensorflow/issues/11186\n            #  for more discussion regarding copying tensors)\n            if d_roughness[0].shape.num_elements() == 1:\n                d_roughness[0] = d_roughness[0] + 0\n\n            if material.get_generic_levels() == 0:\n                d_generic = None\n            else:\n                d_generic = []\n                for l in range(material.get_generic_levels()):\n                    generic_size = material.get_generic_size(l)\n                    d_generic.append(\\\n                        tf.zeros([generic_size[2],\n                                  generic_size[1],\n                                  generic_size[0]], dtype=tf.float32))\n            \n            if material.get_normal_map_levels() == 0:\n                d_normal_map = None\n            else:\n                d_normal_map = []\n                for l in range(material.get_normal_map_levels()):\n                    normal_map_size = material.get_normal_map_size(l)\n                    d_normal_map.append(\\\n                        tf.zeros([normal_map_size[1],\n                                  normal_map_size[0],\n                                  3], dtype=tf.float32))\n\n            buffers.d_diffuse_list.append(d_diffuse)\n            buffers.d_specular_list.append(d_specular)\n            buffers.d_roughness_list.append(d_roughness)\n            buffers.d_generic_list.append(d_generic)\n            buffers.d_normal_map_list.append(d_normal_map)\n\n            d_diffuse_uv_scale = tf.zeros([2], dtype=tf.float32)\n            d_specular_uv_scale = tf.zeros([2], dtype=tf.float32)\n            d_roughness_uv_scale = tf.zeros([2], dtype=tf.float32)\n            if d_generic is None:\n                d_generic_uv_scale = None\n            else:\n                d_generic_uv_scale = tf.zeros([2], dtype=tf.float32)\n            if d_normal_map is None:\n                d_normal_map_uv_scale = None\n            else:\n                d_normal_map_uv_scale = tf.zeros([2], dtype=tf.float32)\n            buffers.d_diffuse_uv_scale_list.append(d_diffuse_uv_scale)\n            buffers.d_specular_uv_scale_list.append(d_specular_uv_scale)\n            buffers.d_roughness_uv_scale_list.append(d_roughness_uv_scale)\n            buffers.d_generic_uv_scale_list.append(d_generic_uv_scale)\n            buffers.d_normal_map_uv_scale_list.append(d_normal_map_uv_scale)\n\n            if len(d_diffuse[0].shape) == 1:\n                d_diffuse_tex = redner.Texture3(\\\n                    [redner.float_ptr(pyredner.data_ptr(d_diffuse[0]))],\n                    [0],\n                    [0],\n                    3,\n                    redner.float_ptr(pyredner.data_ptr(d_diffuse_uv_scale)))\n            else:\n                d_diffuse_tex = redner.Texture3(\\\n                    [redner.float_ptr(pyredner.data_ptr(x)) for x in d_diffuse],\n                    [x.shape[1] for x in d_diffuse],\n                    [x.shape[0] for x in d_diffuse],\n                    3,\n                    redner.float_ptr(pyredner.data_ptr(d_diffuse_uv_scale)))\n\n            if len(d_specular[0].shape) == 1:\n                d_specular_tex = redner.Texture3(\\\n                    [redner.float_ptr(pyredner.data_ptr(d_specular[0]))],\n                    [0],\n                    [0],\n                    3,\n                    redner.float_ptr(pyredner.data_ptr(d_specular_uv_scale)))\n            else:\n                d_specular_tex = redner.Texture3(\\\n                    [redner.float_ptr(pyredner.data_ptr(x)) for x in d_specular],\n                    [x.shape[1] for x in d_specular],\n                    [x.shape[0] for x in d_specular],\n                    3,\n                    redner.float_ptr(pyredner.data_ptr(d_specular_uv_scale)))\n\n            if len(d_roughness[0].shape) == 1:\n                d_roughness_tex = redner.Texture1(\\\n                    [redner.float_ptr(pyredner.data_ptr(d_roughness[0]))],\n                    [0],\n                    [0],\n                    1,\n                    redner.float_ptr(pyredner.data_ptr(d_roughness_uv_scale)))\n            else:\n                d_roughness_tex = redner.Texture1(\\\n                    [redner.float_ptr(pyredner.data_ptr(x)) for x in d_roughness],\n                    [x.shape[1] for x in d_roughness],\n                    [x.shape[0] for x in d_roughness],\n                    1,\n                    redner.float_ptr(pyredner.data_ptr(d_roughness_uv_scale)))\n\n            if d_generic is None:\n                d_generic_tex = redner.TextureN(\\\n                    [], [], [], 0, redner.float_ptr(0))\n            else:\n                d_generic_tex = redner.TextureN(\\\n                    [redner.float_ptr(pyredner.data_ptr(x)) for x in d_generic],\n                    [x.shape[1] for x in d_generic],\n                    [x.shape[0] for x in d_generic],\n                    d_generic[0].shape[2],\n                    redner.float_ptr(pyredner.data_ptr(d_generic_uv_scale)))\n\n            if d_normal_map is None:\n                d_normal_map = redner.Texture3(\\\n                    [], [], [], 0, redner.float_ptr(0))\n            else:\n                d_normal_map = redner.Texture3(\\\n                    [redner.float_ptr(pyredner.data_ptr(x)) for x in d_normal_map],\n                    [x.shape[1] for x in d_normal_map],\n                    [x.shape[0] for x in d_normal_map],\n                    3,\n                    redner.float_ptr(pyredner.data_ptr(d_normal_map_uv_scale)))\n\n            buffers.d_materials.append(redner.DMaterial(\\\n                d_diffuse_tex, d_specular_tex, d_roughness_tex,\n                d_generic_tex, d_normal_map))\n\n    buffers.d_intensity_list = []\n    buffers.d_area_lights = []\n    with tf.device(device_name):\n        for light in ctx.area_lights:\n            d_intensity = tf.zeros(3, dtype=tf.float32)\n            buffers.d_intensity_list.append(d_intensity)\n            buffers.d_area_lights.append(\\\n                redner.DAreaLight(redner.float_ptr(pyredner.data_ptr(d_intensity))))\n\n    buffers.d_envmap = None\n    if ctx.envmap is not None:\n        envmap = ctx.envmap\n        with tf.device(device_name):\n            buffers.d_envmap_values = []\n            for l in range(envmap.get_levels()):\n                size = envmap.get_size(l)\n                buffers.d_envmap_values.append(\\\n                    tf.zeros([size[1],\n                              size[0],\n                              3], dtype=tf.float32))\n            buffers.d_envmap_uv_scale = tf.zeros([2], dtype=tf.float32)\n            buffers.d_world_to_env = tf.zeros([4, 4], dtype=tf.float32)\n            d_envmap_tex = redner.Texture3(\\\n                [redner.float_ptr(pyredner.data_ptr(x)) for x in buffers.d_envmap_values],\n                [x.shape[1] for x in buffers.d_envmap_values],\n                [x.shape[0] for x in buffers.d_envmap_values],\n                3,\n                redner.float_ptr(pyredner.data_ptr(buffers.d_envmap_uv_scale)))\n            buffers.d_envmap = redner.DEnvironmentMap(d_envmap_tex,\n                redner.float_ptr(pyredner.data_ptr(buffers.d_world_to_env)))\n\n    device_spec = tf.DeviceSpec.from_string(device_name)\n    use_gpu = device_spec.device_type == \'GPU\'\n    gpu_index = device_spec.device_index if device_spec.device_index is not None else 0\n\n    buffers.d_scene = redner.DScene(buffers.d_camera,\n                                    buffers.d_shapes,\n                                    buffers.d_materials,\n                                    buffers.d_area_lights,\n                                    buffers.d_envmap,\n                                    use_gpu,\n                                    gpu_index)\n    return buffers\n\n@tf.custom_gradient\ndef render(*x):\n    """"""\n        The main TensorFlow interface of C++ redner.\n    """"""\n    assert(tf.executing_eagerly())\n    if pyredner.get_use_gpu() and os.environ[\'TF_FORCE_GPU_ALLOW_GROWTH\'] != \'true\':\n        print(\'******************** WARNING ********************\')\n        print(\'Tensorflow by default allocates all GPU memory,\')\n        print(\'causing huge amount of page faults when rendering.\')\n        print(\'Please set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true,\')\n        print(\'so that Tensorflow allocates memory on demand.\')\n        print(\'*************************************************\')\n\n    seed, args = int(x[0]), x[1:]\n    img, ctx = forward(seed, *args)\n\n    def backward(grad_img):\n        scene = ctx.scene\n        options = ctx.options\n        camera = ctx.camera\n        device_name = ctx.device_name\n\n        buffers = create_gradient_buffers(ctx)\n\n        if not get_use_correlated_random_number():\n            # Decod_uple the forward/backward random numbers by adding a big prime number\n            options.seed += 1000003\n        start = time.time()\n\n        options.num_samples = ctx.num_samples[1]\n        with tf.device(device_name):\n            grad_img = tf.identity(grad_img)\n            redner.render(scene,\n                          options,\n                          redner.float_ptr(0), # rendered_image\n                          redner.float_ptr(pyredner.data_ptr(grad_img)),\n                          buffers.d_scene,\n                          redner.float_ptr(0), # translational_gradient_image\n                          redner.float_ptr(0)) # debug_image\n        time_elapsed = time.time() - start\n\n        if get_print_timing():\n            print(\'Backward pass, time: %.5f s\' % time_elapsed)\n\n        ret_list = []\n        ret_list.append(None) # seed\n        ret_list.append(None) # device\n        ret_list.append(None) # num_shapes\n        ret_list.append(None) # num_materials\n        ret_list.append(None) # num_lights\n        if ctx.camera.use_look_at:\n            ret_list.append(buffers.d_position)\n            ret_list.append(buffers.d_look_at)\n            ret_list.append(buffers.d_up)\n            ret_list.append(None) # cam_to_world\n            ret_list.append(None) # world_to_cam\n        else:\n            ret_list.append(None) # pos\n            ret_list.append(None) # look\n            ret_list.append(None) # up\n            ret_list.append(buffers.d_cam_to_world)\n            ret_list.append(buffers.d_world_to_cam)\n        ret_list.append(buffers.d_intrinsic_mat_inv)\n        ret_list.append(buffers.d_intrinsic_mat)\n        if not camera.has_distortion_params():\n            ret_list.append(None) # distortion_params\n        else:\n            ret_list.append(buffers.d_distortion_params.cpu())\n        ret_list.append(None) # clip near\n        ret_list.append(None) # resolution\n        ret_list.append(None) # viewport\n        ret_list.append(None) # camera_type\n\n        num_shapes = len(ctx.shapes)\n        for i in range(num_shapes):\n            ret_list.append(buffers.d_vertices_list[i])\n            ret_list.append(None) # indices\n            ret_list.append(buffers.d_uvs_list[i])\n            ret_list.append(buffers.d_normals_list[i])\n            ret_list.append(None) # uv_indices\n            ret_list.append(None) # normal_indices\n            ret_list.append(buffers.d_colors_list[i])\n            ret_list.append(None) # material id\n            ret_list.append(None) # light id\n\n        num_materials = len(ctx.materials)\n        for i in range(num_materials):\n            ret_list.append(None) # num_levels\n            for d_diffuse in buffers.d_diffuse_list[i]:\n                ret_list.append(d_diffuse)\n            ret_list.append(buffers.d_diffuse_uv_scale_list[i])\n            ret_list.append(None) # num_levels\n            for d_specular in buffers.d_specular_list[i]:\n                ret_list.append(d_specular)\n            ret_list.append(buffers.d_specular_uv_scale_list[i])\n            ret_list.append(None) # num_levels\n            for d_roughness in buffers.d_roughness_list[i]:\n                ret_list.append(d_roughness)\n            ret_list.append(buffers.d_roughness_uv_scale_list[i])\n            if buffers.d_generic_list[i] is None:\n                ret_list.append(None) # num_levels\n            else:\n                ret_list.append(None) # num_levels\n                for d_generic in buffers.d_generic_list[i]:\n                    ret_list.append(d_generic)\n                ret_list.append(buffers.d_generic_uv_scale_list[i])\n            if buffers.d_normal_map_list[i] is None:\n                ret_list.append(None) # num_levels\n            else:\n                ret_list.append(None) # num_levels\n                for d_normal_map in buffers.d_normal_map_list[i]:\n                    ret_list.append(d_normal_map)\n                ret_list.append(buffers.d_normal_map_uv_scale_list[i])\n            ret_list.append(None) # compute_specular_lighting\n            ret_list.append(None) # two sided\n            ret_list.append(None) # use_vertex_color\n\n        num_area_lights = len(ctx.area_lights)\n        for i in range(num_area_lights):\n            ret_list.append(None) # shape id\n            with tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n                ret_list.append(tf.identity(buffers.d_intensity_list[i]))\n            ret_list.append(None) # two_sided\n            ret_list.append(None) # directly_visible\n\n        if ctx.envmap is not None:\n            ret_list.append(None) # num_levels\n            for d_values in buffers.d_envmap_values:\n                ret_list.append(d_values)\n            ret_list.append(buffers.d_envmap_uv_scale)\n            ret_list.append(None) # env_to_world\n            with tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n                ret_list.append(tf.identity(buffers.d_world_to_env))\n            ret_list.append(None) # sample_cdf_ys\n            ret_list.append(None) # sample_cdf_xs\n            ret_list.append(None) # pdf_norm\n            ret_list.append(None) # directly_visible\n        else:\n            ret_list.append(None)\n\n        ret_list.append(None) # num samples\n        ret_list.append(None) # num bounces\n        ret_list.append(None) # num channels\n        for _ in range(ctx.num_channel_args):\n            ret_list.append(None) # channel\n\n        ret_list.append(None) # sampler type\n        ret_list.append(None) # use_primary_edge_sampling\n        ret_list.append(None) # use_secondary_edge_sampling\n        ret_list.append(None) # sample_pixel_center\n\n        return ret_list\n\n    return img, backward\n\ndef visualize_screen_gradient(grad_img: tf.Tensor,\n                              seed: int,\n                              scene: pyredner.Scene,\n                              num_samples: Union[int, Tuple[int, int]],\n                              max_bounces: int,\n                              channels: List = [redner.channels.radiance],\n                              sampler_type = redner.SamplerType.independent,\n                              use_primary_edge_sampling: bool = True,\n                              use_secondary_edge_sampling: bool = True,\n                              sample_pixel_center: bool = False):\n    """"""\n        Given a serialized scene and output an 2-channel image,\n        which visualizes the derivatives of pixel color with respect to \n        the screen space coordinates.\n\n        Args\n        ====\n        grad_img: Optional[tf.Tensor]\n            The ""adjoint"" of the backpropagation gradient. If you don\'t know\n            what this means just give None\n        seed: int\n            seed for the Monte Carlo random samplers\n        See serialize_scene for the explanation of the rest of the arguments.\n    """"""\n\n    args = serialize_scene(\\\n        scene = scene,\n        num_samples = num_samples,\n        max_bounces = max_bounces,\n        sampler_type = sampler_type,\n        channels = channels,\n        sample_pixel_center = sample_pixel_center)\n    args_ctx = unpack_args(\\\n        seed, args, use_primary_edge_sampling, use_secondary_edge_sampling)\n    channels = args_ctx.channels\n    options = args_ctx.options\n    resolution = args_ctx.resolution\n    viewport = args_ctx.viewport\n    scene = args_ctx.scene\n    device_name = args_ctx.device_name\n\n    buffers = create_gradient_buffers(args_ctx)\n    num_channels = redner.compute_num_channels(channels,\n                                               scene.max_generic_texture_dimension)\n    with tf.device(device_name):\n        img_height = viewport[2] - viewport[0]\n        img_width = viewport[3] - viewport[1]\n        screen_gradient_image = tf.zeros(\\\n            shape = [img_height, img_width, 2],\n            dtype = tf.float32)\n        if grad_img is not None:\n            assert(grad_img.shape[0] == resolution[0])\n            assert(grad_img.shape[1] == resolution[1])\n            assert(grad_img.shape[2] == num_channels)\n        else:\n            grad_img = tf.ones(\\\n                shape = [img_height, img_width, num_channels],\n                dtype = tf.float32)\n        start = time.time()\n        redner.render(scene,\n                      options,\n                      redner.float_ptr(0), # rendered_image\n                      redner.float_ptr(pyredner.data_ptr(grad_img)), # d_rendered_image\n                      buffers.d_scene,\n                      redner.float_ptr(pyredner.data_ptr(screen_gradient_image)),\n                      redner.float_ptr(0)) # debug_image\n        time_elapsed = time.time() - start\n    if get_print_timing():\n        print(\'Visualize gradient, time: %.5f s\' % time_elapsed)\n\n    return screen_gradient_image\n'"
pyredner_tensorflow/render_utils.py,54,"b'import pyredner_tensorflow as pyredner\nimport random\nimport redner\nimport tensorflow as tf\nimport math\nfrom typing import Union, Tuple, Optional, List\n\nclass DeferredLight:\n    pass\n\nclass AmbientLight(DeferredLight):\n    """"""\n        Ambient light for deferred rendering.\n    """"""\n    def __init__(self,\n                 intensity: tf.Tensor):\n        self.intensity = intensity\n\n    def render(self,\n               position: tf.Tensor,\n               normal: tf.Tensor,\n               albedo: tf.Tensor):\n        return self.intensity * albedo\n\nclass PointLight(DeferredLight):\n    """"""\n        Point light with squared distance falloff for deferred rendering.\n    """"""\n    def __init__(self,\n                 position: tf.Tensor,\n                 intensity: tf.Tensor):\n        self.position = position\n        self.intensity = intensity\n\n    def render(self,\n               position: tf.Tensor,\n               normal: tf.Tensor,\n               albedo: tf.Tensor):\n        light_dir = self.position - position\n        # the d^2 term:\n        light_dist_sq = tf.reduce_sum(light_dir * light_dir, axis = -1, keepdims = True)\n        light_dist = tf.sqrt(light_dist_sq)\n        # Normalize light direction\n        light_dir = light_dir / light_dist\n        dot_l_n = tf.reduce_sum(light_dir * normal, axis = -1, keepdims = True)\n        dot_l_n = tf.maximum(dot_l_n, tf.zeros_like(dot_l_n))\n        return self.intensity * dot_l_n * (albedo / math.pi) / light_dist_sq \n\nclass DirectionalLight(DeferredLight):\n    """"""\n        Directional light for deferred rendering.\n    """"""\n    def __init__(self,\n                 direction: tf.Tensor,\n                 intensity: tf.Tensor):\n        self.direction = direction\n        self.intensity = intensity\n\n    def render(self,\n               position: tf.Tensor,\n               normal: tf.Tensor,\n               albedo: tf.Tensor):\n        # Normalize light direction\n        light_dir = -self.direction / tf.norm(self.direction)\n        light_dir = tf.reshape(light_dir, (1, 1, 3))\n        dot_l_n = tf.reduce_sum(light_dir * normal, axis = -1, keepdims = True)\n        dot_l_n = tf.maximum(dot_l_n, tf.zeros_like(dot_l_n))\n        return self.intensity * dot_l_n * (albedo / math.pi)\n\nclass SpotLight(DeferredLight):\n    """"""\n        Spot light with cosine falloff for deferred rendering.\n        Note that we do not provide the cosine cutoff here since it is not\n        differentiable.\n    """"""\n    def __init__(self,\n                 position: tf.Tensor,\n                 spot_direction: tf.Tensor,\n                 spot_exponent: tf.Tensor,\n                 intensity: tf.Tensor):\n        self.position = position\n        self.spot_direction = spot_direction\n        self.spot_exponent = spot_exponent\n        self.intensity = intensity\n\n    def render(self,\n               position: tf.Tensor,\n               normal: tf.Tensor,\n               albedo: tf.Tensor):\n        light_dir = self.position - position\n        # Normalize light direction\n        light_dir = light_dir / tf.norm(light_dir, axis = -1, keepdims = True)\n        # Normalize spot direction\n        spot_direction = -self.spot_direction / tf.norm(self.spot_direction)\n        spot_cosine = tf.reduce_sum(light_dir * spot_direction, axis = -1, keepdims = True)\n        spot_cosine = tf.maximum(spot_cosine, tf.zeros_like(spot_cosine))\n        spot_factor = tf.pow(spot_cosine, self.spot_exponent)\n        dot_l_n = tf.reduce_sum(light_dir * normal, axis = -1, keepdims = True)\n        dot_l_n = tf.maximum(dot_l_n, tf.zeros_like(dot_l_n))\n        return self.intensity * spot_factor * dot_l_n * (albedo / math.pi)\n\ndef render_deferred(scene: Union[pyredner.Scene, List[pyredner.Scene]],\n                    lights: Union[List[DeferredLight], List[List[DeferredLight]]],\n                    alpha: bool = False,\n                    aa_samples: int = 2,\n                    seed: Optional[Union[int, List[int]]] = None,\n                    sample_pixel_center: bool = False,\n                    device_name: Optional[str] = None):\n    """"""\n        Render the scenes using `deferred rendering <https://en.wikipedia.org/wiki/Deferred_shading>`_.\n        We generate G-buffer images containing world-space position,\n        normal, and albedo using redner, then shade the G-buffer\n        using TensorFlow code. Assuming Lambertian shading and does not\n        compute shadow.\n\n        Args\n        ====\n        scene: Union[pyredner.Scene, List[pyredner.Scene]]\n            pyredner Scene containing camera, geometry and material.\n            Can be a single scene or a list for batch render.\n            For batch rendering all scenes need to have the same resolution.\n        lights: Union[List[DeferredLight], List[List[DeferredLight]]]\n            Lights for deferred rendering. If the scene is a list, and only\n            a single list of lights is provided, the same lights are applied\n            to all scenes. If a list of lists of lights is provided, each scene\n            is lit by the corresponding lights.\n        alpha: bool\n            If set to False, generates a 3-channel image,\n            otherwise generates a 4-channel image where the\n            fourth channel is alpha.\n        aa_samples: int\n            Number of samples used for anti-aliasing at both x, y dimensions\n            (e.g. if aa_samples=2, 4 samples are used).\n        seed: Optional[Union[int, List[int]]]\n            Random seed used for sampling. Randomly assigned if set to None.\n            For batch render, if seed it not None, need to provide a list\n            of seeds.\n        sample_pixel_center: bool\n            Always sample at the pixel center when rendering.\n            This trades noise with aliasing.\n            If this option is activated, the rendering becomes non-differentiable\n            (since there is no antialiasing integral),\n            and redner\'s edge sampling becomes an approximation to the gradients of the aliased rendering.\n        device_name: Optional[str]\n            Which device should we store the data in.\n            If set to None, use the device from pyredner.get_device_name().\n\n        Returns\n        =======\n        tf.Tensor or List[tf.Tensor]\n            | if input scene is a list: a tensor with size [N, H, W, C], N is the list size\n            | else: a tensor with size [H, W, C]\n            | if alpha == True, C = 4.\n            | else, C = 3.\n    """"""\n    channels = [redner.channels.position,\n                redner.channels.shading_normal,\n                redner.channels.diffuse_reflectance]\n    if alpha:\n        channels.append(redner.channels.alpha)\n    if isinstance(scene, pyredner.Scene):\n        if seed == None:\n            seed = random.randint(0, 16777216)\n        # We do full-screen anti-aliasing: increase the rendering resolution\n        # and downsample it after lighting\n        org_res = scene.camera.resolution\n        org_viewport = scene.camera.viewport\n        scene.camera.resolution = (org_res[0] * aa_samples,\n                                   org_res[1] * aa_samples)\n        if org_viewport is not None:\n            scene.camera.viewport = [i * aa_samples for i in org_viewport]\n        scene_args = pyredner.serialize_scene(\\\n            scene = scene,\n            num_samples = (1, 1),\n            max_bounces = 0,\n            sampler_type = redner.SamplerType.sobol,\n            channels = channels,\n            use_secondary_edge_sampling = False,\n            sample_pixel_center = sample_pixel_center,\n            device_name = device_name)\n        # Need to revert the resolution back\n        scene.camera.resolution = org_res\n        scene.camera.viewport = org_viewport\n        g_buffer = pyredner.render(seed, *scene_args)\n        pos = g_buffer[:, :, :3]\n        normal = g_buffer[:, :, 3:6]\n        albedo = g_buffer[:, :, 6:9]\n        img = tf.zeros((g_buffer.shape[0], g_buffer.shape[1], 3))\n        for light in lights:\n            img = img + light.render(pos, normal, albedo)\n        if alpha:\n            # alpha is in the last channel\n            img = tf.concat((img, g_buffer[:, :, 9:10]), axis = 2)\n        if aa_samples > 1:\n            # Downsample\n            img = tf.expand_dims(img, 0) # HWC -> NHWC\n            if org_viewport is not None:\n                org_size = org_viewport[2] - org_viewport[0], org_viewport[3] - org_viewport[1]\n            else:\n                org_size = org_res\n            # TODO: switch to method = \'area\' when tensorflow implements the gradients...\n            img = tf.image.resize(img, size = org_size, method = \'bilinear\', antialias = True)\n            img = tf.squeeze(img, axis = 0) # NHWC -> HWC\n        return img\n    else:\n        assert(isinstance(scene, list))\n        if seed == None:\n            # Randomly generate a list of seed\n            seed = []\n            for i in range(len(scene)):\n                seed.append(random.randint(0, 16777216))\n        assert(len(seed) == len(scene))\n        if len(lights) > 0 and not isinstance(lights[0], list):\n            # Specialize version: stack g buffers and light all images in parallel\n            g_buffers = []\n            # Render each scene in the batch and stack them together\n            for sc, se in zip(scene, seed):\n                # We do full-screen anti-aliasing: increase the rendering resolution\n                # and downsample it after lighting\n                org_res = sc.camera.resolution\n                org_viewport = sc.camera.viewport\n                sc.camera.resolution = (org_res[0] * aa_samples,\n                                        org_res[1] * aa_samples)\n                if org_viewport is not None:\n                    sc.camera.viewport = [i * aa_samples for i in org_viewport]\n                scene_args = pyredner.serialize_scene(\\\n                    scene = sc,\n                    num_samples = (1, 1),\n                    max_bounces = 0,\n                    sampler_type = redner.SamplerType.sobol,\n                    channels = channels,\n                    use_secondary_edge_sampling = False,\n                    sample_pixel_center = sample_pixel_center,\n                    device_name = device_name)\n                # Need to revert the resolution back\n                sc.camera.resolution = org_res\n                sc.camera.viewport = org_viewport\n                g_buffers.append(pyredner.render(se, *scene_args))\n            g_buffers = tf.stack(g_buffers)\n            pos = g_buffers[:, :, :, :3]\n            normal = g_buffers[:, :, :, 3:6]\n            albedo = g_buffers[:, :, :, 6:9]\n            imgs = tf.zeros((g_buffers.shape[0],\n                             g_buffers.shape[1],\n                             g_buffers.shape[2],\n                             3))\n            for light in lights:\n                imgs = imgs + light.render(pos, normal, albedo)\n            if alpha:\n                imgs = tf.concat((imgs, g_buffers[:, :, :, 9:10]), axis = -1)\n        else:\n            # If each scene has a different lighting: light them in the loop\n            imgs = []\n            # Render each scene in the batch and stack them together\n            for sc, se, lgts in zip(scene, seed, lights):\n                # We do full-screen anti-aliasing: increase the rendering resolution\n                # and downsample it after lighting\n                org_res = sc.camera.resolution\n                org_viewport = sc.camera.viewport\n                sc.camera.resolution = (org_res[0] * aa_samples,\n                                        org_res[1] * aa_samples)\n                if org_viewport is not None:\n                    sc.camera.viewport = [i * aa_samples for i in org_viewport]\n                scene_args = pyredner.serialize_scene(\\\n                    scene = sc,\n                    num_samples = (1, 1),\n                    max_bounces = 0,\n                    sampler_type = redner.SamplerType.sobol,\n                    channels = channels,\n                    use_secondary_edge_sampling = False,\n                    sample_pixel_center = sample_pixel_center,\n                    device_name = device_name)\n                # Need to revert the resolution back\n                sc.camera.resolution = org_res\n                sc.camera.viewport = org_viewport\n                g_buffer = pyredner.render(se, *scene_args)\n                pos = g_buffer[:, :, :3]\n                normal = g_buffer[:, :, 3:6]\n                albedo = g_buffer[:, :, 6:9]\n                img = tf.zeros(g_buffer.shape[0],\n                               g_buffer.shape[1],\n                               3)\n                for light in lgts:\n                    img = img + light.render(pos, normal, albedo)\n                if alpha:\n                    # alpha is in the last channel\n                    img = tf.concat((img, g_buffer[:, :, 9:10]), axis = -1)\n                imgs.append(img)\n            imgs = tf.stack(imgs)\n        if aa_samples > 1:\n            if org_viewport is not None:\n                org_size = org_viewport[2] - org_viewport[0], org_viewport[3] - org_viewport[1]\n            else:\n                org_size = org_res\n            # Downsample\n            # TODO: switch to method = \'area\' when tensorflow implements the gradients...\n            imgs = tf.image.resize(imgs, size = org_size, method = \'bilinear\', antialias = True)\n        return imgs\n\ndef render_generic(scene: pyredner.Scene,\n                   channels: List,\n                   max_bounces: int = 1,\n                   sampler_type = pyredner.sampler_type.sobol,\n                   num_samples: Union[int, Tuple[int, int]] = (4, 4),\n                   seed: Optional[int] = None,\n                   sample_pixel_center: bool = False,\n                   device_name: Optional[str] = None):\n    """"""\n        A generic rendering function that can be either pathtracing or\n        g-buffer rendering or both.\n\n        Args\n        ====\n        scene: Union[pyredner.Scene, List[pyredner.Scene]]\n            pyredner Scene containing camera, geometry and material.\n            Can be a single scene or a list for batch render.\n            For batch rendering all scenes need to have the same resolution.\n        channels: List[pyredner.channels]\n            | A list of the following channels\\:\n            | pyredner.channels.alpha\n            | pyredner.channels.depth\n            | pyredner.channels.position\n            | pyredner.channels.geometry_normal\n            | pyredner.channels.shading_normal\n            | pyredner.channels.uv\n            | pyredner.channels.barycentric_coordinates\n            | pyredner.channels.diffuse_reflectance\n            | pyredner.channels.specular_reflectance\n            | pyredner.channels.roughness\n            | pyredner.channels.generic_texture\n            | pyredner.channels.vertex_color\n            | pyredner.channels.shape_id\n            | pyredner.channels.triangle_id\n            | pyredner.channels.material_id\n        max_bounces: int\n            Number of bounces for global illumination, 1 means direct lighting only.\n        sampler_type: pyredner.sampler_type\n            | Which sampling pattern to use? See \n              `Chapter 7 of the PBRT book <http://www.pbr-book.org/3ed-2018/Sampling_and_Reconstruction.html>`_\n              for an explanation of the difference between different samplers.\n            | Following samplers are supported\\:\n            | pyredner.sampler_type.independent\n            | pyredner.sampler_type.sobol\n        num_samples: int\n            Number of samples per pixel for forward and backward passes.\n            Can be an integer or a tuple of 2 integers.\n        seed: Optional[Union[int, List[int]]]\n            Random seed used for sampling. Randomly assigned if set to None.\n            For batch render, if seed it not None, need to provide a list\n            of seeds.\n        sample_pixel_center: bool\n            Always sample at the pixel center when rendering.\n            This trades noise with aliasing.\n            If this option is activated, the rendering becomes non-differentiable\n            (since there is no antialiasing integral),\n            and redner\'s edge sampling becomes an approximation to the gradients of the aliased rendering.\n        device_name: Optional[str]\n            Which device should we store the data in.\n            If set to None, use the device from pyredner.get_device_name().\n\n        Returns\n        =======\n        tf.Tensor or List[tf.Tensor]\n            | if input scene is a list: a tensor with size [N, H, W, C], N is the list size\n            | else: a tensor with size [H, W, C]\n    """"""\n    if isinstance(scene, pyredner.Scene):\n        if seed==None:\n            seed = random.randint(0, 16777216)\n        scene_args = pyredner.serialize_scene(\\\n            scene = scene,\n            num_samples = num_samples,\n            max_bounces = max_bounces,\n            sampler_type = sampler_type,\n            channels = channels,\n            sample_pixel_center = sample_pixel_center,\n            device_name = device_name)\n        return pyredner.render(seed, *scene_args)\n    else:\n        assert(isinstance(scene, list))\n        if seed == None:\n            # Randomly generate a list of seed\n            seed = []\n            for i in range(len(scene)):\n                seed.append(random.randint(0, 16777216))\n        assert(len(seed) == len(scene))\n        # Render each scene in the batch and stack them together\n        imgs = []\n        for sc, se in zip(scene, seed):\n            scene_args = pyredner.serialize_scene(\\\n                scene = sc,\n                num_samples = num_samples,\n                max_bounces = max_bounces,\n                sampler_type = sampler_type,\n                channels = channels,\n                sample_pixel_center = sample_pixel_center,\n                device_name = device_name)\n            imgs.append(pyredner.render(se, *scene_args))\n        imgs = tf.stack(imgs)\n        return imgs\n\ndef render_g_buffer(scene: pyredner.Scene,\n                    channels: List[redner.channels],\n                    num_samples: Union[int, Tuple[int, int]] = (1, 1),\n                    seed: Optional[int] = None,\n                    sample_pixel_center: bool = False,\n                    device_name: Optional[str] = None):\n    """"""\n        Render a G buffer from the scene.\n\n        Args\n        ====\n        scene: pyredner.Scene\n            pyredner Scene containing camera, geometry, material, and lighting\n        channels: List[pyredner.channels]\n            | A list of the following channels\\:\n            | pyredner.channels.alpha\n            | pyredner.channels.depth\n            | pyredner.channels.position\n            | pyredner.channels.geometry_normal\n            | pyredner.channels.shading_normal\n            | pyredner.channels.uv\n            | pyredner.channels.barycentric_coordinates\n            | pyredner.channels.diffuse_reflectance\n            | pyredner.channels.specular_reflectance\n            | pyredner.channels.roughness\n            | pyredner.channels.generic_texture\n            | pyredner.channels.vertex_color\n            | pyredner.channels.shape_id\n            | pyredner.channels.triangle_id\n            | pyredner.channels.material_id\n        num_samples: Union[int, Tuple[int, int]]\n            Number of samples for forward and backward passes, respectively.\n            If a single integer is provided, use the same number of samples\n            for both.\n        seed: Optional[int]\n            Random seed used for sampling. Randomly assigned if set to None.\n        sample_pixel_center: bool\n            Always sample at the pixel center when rendering.\n            This trades noise with aliasing.\n            If this option is activated, the rendering becomes non-differentiable\n            (since there is no antialiasing integral),\n            and redner\'s edge sampling becomes an approximation to the gradients of the aliased rendering.\n        device_name: Optional[str]\n            Which device should we store the data in.\n            If set to None, use the device from pyredner.get_device_name().\n\n        Returns\n        =======\n        tf.Tensor\n            a tensor with size [H, W, C]\n    """"""\n    return render_generic(scene = scene,\n                          channels = channels,\n                          max_bounces = 0,\n                          sampler_type = redner.SamplerType.sobol,\n                          num_samples = num_samples,\n                          seed = seed,\n                          sample_pixel_center = sample_pixel_center,\n                          device_name = device_name)\n\ndef render_pathtracing(scene: Union[pyredner.Scene, List[pyredner.Scene]],\n                       alpha: bool = False,\n                       max_bounces: int = 1,\n                       sampler_type = pyredner.sampler_type.sobol,\n                       num_samples: Union[int, Tuple[int, int]] = (4, 4),\n                       seed: Optional[Union[int, List[int]]] = None,\n                       sample_pixel_center: bool = False,\n                       device_name: Optional[str] = None):\n    """"""\n        Render a pyredner scene using pathtracing.\n\n        Args\n        ====\n        scene: Union[pyredner.Scene, List[pyredner.Scene]]\n            pyredner Scene containing camera, geometry and material.\n            Can be a single scene or a list for batch render.\n            For batch rendering all scenes need to have the same resolution.\n        max_bounces: int\n            Number of bounces for global illumination, 1 means direct lighting only.\n        sampler_type: pyredner.sampler_type\n            | Which sampling pattern to use? See \n              `Chapter 7 of the PBRT book <http://www.pbr-book.org/3ed-2018/Sampling_and_Reconstruction.html>`_\n              for an explanation of the difference between different samplers.\n            | Following samplers are supported\\:\n            | pyredner.sampler_type.independent\n            | pyredner.sampler_type.sobol\n        num_samples: int\n            Number of samples per pixel for forward and backward passes.\n            Can be an integer or a tuple of 2 integers.\n        seed: Optional[Union[int, List[int]]]\n            Random seed used for sampling. Randomly assigned if set to None.\n            For batch render, if seed it not None, need to provide a list\n            of seeds.\n        sample_pixel_center: bool\n            Always sample at the pixel center when rendering.\n            This trades noise with aliasing.\n            If this option is activated, the rendering becomes non-differentiable\n            (since there is no antialiasing integral),\n            and redner\'s edge sampling becomes an approximation to the gradients of the aliased rendering.\n        device_name: Optional[str]\n            Which device should we store the data in.\n            If set to None, use the device from pyredner.get_device_name().\n\n        Returns\n        =======\n        tf.Tensor or List[tf.Tensor]\n            | if input scene is a list: a tensor with size [N, H, W, C], N is the list size\n            | else: a tensor with size [H, W, C]\n            | if alpha == True, C = 4.\n            | else, C = 3.\n    """"""\n    channels = [redner.channels.radiance]\n    if alpha:\n        channels.append(redner.channels.alpha)\n    return render_generic(scene = scene,\n                          channels = channels,\n                          max_bounces = max_bounces,\n                          sampler_type = sampler_type,\n                          num_samples = num_samples,\n                          seed = seed,\n                          sample_pixel_center = sample_pixel_center,\n                          device_name = device_name)\n\ndef render_albedo(scene: Union[pyredner.Scene, List[pyredner.Scene]],\n                  alpha: bool = False,\n                  num_samples: Union[int, Tuple[int, int]] = (16, 4),\n                  seed: Optional[Union[int, List[int]]] = None,\n                  sample_pixel_center: bool = False,\n                  device_name: Optional[str] = None):\n    """"""\n        Render the diffuse albedo colors of the scenes.\n\n        Args\n        ====\n        scene: Union[pyredner.Scene, List[pyredner.Scene]]\n            pyredner Scene containing camera, geometry and material.\n            Can be a single scene or a list for batch render.\n            For batch rendering all scenes need to have the same resolution.\n        alpha: bool\n            If set to False, generates a 3-channel image,\n            otherwise generates a 4-channel image where the\n            fourth channel is alpha.\n        num_samples: Union[int, Tuple[int, int]]\n            number of samples for forward and backward passes, respectively\n            if a single integer is provided, use the same number of samples\n            for both\n        seed: Optional[Union[int, List[int]]]\n            Random seed used for sampling. Randomly assigned if set to None.\n            For batch render, if seed it not None, need to provide a list\n            of seeds.\n        sample_pixel_center: bool\n            Always sample at the pixel center when rendering.\n            This trades noise with aliasing.\n            If this option is activated, the rendering becomes non-differentiable\n            (since there is no antialiasing integral),\n            and redner\'s edge sampling becomes an approximation to the gradients of the aliased rendering.\n        device_name: Optional[str]\n            Which device should we store the data in.\n            If set to None, use the device from pyredner.get_device_name().\n\n        Returns\n        =======\n        tf.Tensor or List[tf.Tensor]\n            | if input scene is a list: a tensor with size [N, H, W, C], N is the list size\n            | else: a tensor with size [H, W, C]\n            | if alpha == True, C = 4.\n            | else, C = 3.\n    """"""\n    channels = [redner.channels.diffuse_reflectance]\n    if alpha:\n        channels.append(redner.channels.alpha)\n    return render_g_buffer(scene = scene,\n                           channels = channels,\n                           num_samples = num_samples,\n                           seed = seed,\n                           sample_pixel_center = sample_pixel_center,\n                           device_name = device_name)\n'"
pyredner_tensorflow/sampler_type.py,0,b'import redner\nimport tensorflow as tf\n\nclass SamplerType:\n    def __init__(self):\n        self.independent = redner.SamplerType.independent\n        self.sobol = redner.SamplerType.sobol\n\nsampler_type = SamplerType()\n'
pyredner_tensorflow/save_obj.py,0,"b'import pyredner_tensorflow as pyredner\nfrom typing import Union\nimport os\n\ndef save_obj(shape: Union[pyredner.Object, pyredner.Shape],\n             filename: str,\n             flip_tex_coords = True):\n    """"""\n        Save to a Wavefront obj file from an Object or a Shape.\n\n        Args\n        ====\n        shape: Union[pyredner.Object, pyredner.Shape]\n\n        filename: str\n\n        flip_tex_coords: bool\n            flip the v coordinate of uv by applying v\' = 1 - v\n    """"""\n    directory = os.path.dirname(filename)\n    if directory != \'\' and not os.path.exists(directory):\n        os.makedirs(directory)\n    \n    with open(filename, \'w\') as f:\n        vertices = shape.vertices.numpy()\n        uvs = shape.uvs.numpy() if shape.uvs is not None else None\n        normals = shape.normals.numpy() if shape.normals is not None else None\n        for i in range(vertices.shape[0]):\n            f.write(\'v {} {} {}\\n\'.format(vertices[i, 0], vertices[i, 1], vertices[i, 2]))\n        if uvs is not None:\n            for i in range(uvs.shape[0]):\n                if flip_tex_coords:\n                    f.write(\'vt {} {}\\n\'.format(uvs[i, 0], 1 - uvs[i, 1]))\n                else:\n                    f.write(\'vt {} {}\\n\'.format(uvs[i, 0], uvs[i, 1]))\n        if normals is not None:\n            for i in range(normals.shape[0]):\n                f.write(\'vn {} {} {}\\n\'.format(normals[i, 0], normals[i, 1], normals[i, 2]))\n        indices = shape.indices.numpy() + 1\n        uv_indices = shape.uv_indices.numpy() + 1 if shape.uv_indices is not None else None\n        normal_indices = shape.normal_indices.numpy() + 1 if shape.normal_indices is not None else None\n        for i in range(indices.shape[0]):\n            vi = (indices[i, 0], indices[i, 1], indices[i, 2])\n            if uv_indices is not None:\n                uvi = (uv_indices[i, 0], uv_indices[i, 1], uv_indices[i, 2])\n            else:\n                if uvs is not None:\n                    uvi = vi\n                else:\n                    uvi = (\'\', \'\', \'\')\n            if normal_indices is not None:\n                ni = (normal_indices[i, 0], normal_indices[i, 1], normal_indices[i, 2])\n            else:\n                if normals is not None:\n                    ni = vi\n                else:\n                    ni = (\'\', \'\', \'\')\n            if normals is not None:\n                f.write(\'f {}/{}/{} {}/{}/{} {}/{}/{}\\n\'.format(\\\n                    vi[0], uvi[0], ni[0],\n                    vi[1], uvi[1], ni[1],\n                    vi[2], uvi[2], ni[2]))\n            elif uvs is not None:\n                f.write(\'f {}/{} {}/{} {}/{}\\n\'.format(\\\n                    vi[0], uvi[0],\n                    vi[1], uvi[1],\n                    vi[2], uvi[2]))\n            else:\n                f.write(\'f {} {} {}\\n\'.format(\\\n                    vi[0],\n                    vi[1],\n                    vi[2]))\n'"
pyredner_tensorflow/scene.py,0,"b'import pyredner_tensorflow as pyredner\n\nclass Scene:\n    """"""\n        A scene is a collection of camera, geometry, materials, and light.\n        Currently there are two ways to construct a scene: one is through\n        lists of Shape, Material, and AreaLight. The other one is through\n        a list of Object. It is more recommended to use the Object construction.\n        The Shape/Material/AreaLight options are here for legacy issue.\n\n        Args\n        ====\n            shapes: List[pyredner.Shape] = [],\n            materials: List[pyredner.Material] = [],\n            area_lights: List[pyredner.AreaLight] = [],\n            objects: Optional[List[pyredner.Object]] = None,\n            envmap: Optional[pyredner.EnvironmentMap] = None\n    """"""\n    def __init__(self,\n                 camera,\n                 shapes = [],\n                 materials = [],\n                 area_lights = [],\n                 objects = None,\n                 envmap = None):\n        self.camera = camera\n        self.envmap = envmap\n        if objects is None:\n            self.shapes = shapes\n            self.materials = materials\n            self.area_lights = area_lights\n        else:\n            # Convert objects to shapes/materials/lights\n            shapes = []\n            materials = []\n            area_lights = []\n            material_dict = {}\n            current_material_id = 0\n            for obj in objects:\n                mid = -1\n                if obj.material in material_dict:\n                    mid = material_dict[obj.material]\n                else:\n                    mid = current_material_id\n                    material_dict[obj.material] = current_material_id\n                    materials.append(obj.material)\n                    current_material_id += 1\n                if obj.light_intensity is not None:\n                    current_shape_id = len(shapes)\n                    area_light = pyredner.AreaLight(shape_id = current_shape_id,\n                                                    intensity = obj.light_intensity,\n                                                    two_sided = obj.light_two_sided)\n                    area_lights.append(area_light)\n                shape = pyredner.Shape(vertices = obj.vertices,\n                                       indices = obj.indices,\n                                       material_id = mid,\n                                       uvs = obj.uvs,\n                                       normals = obj.normals,\n                                       uv_indices = obj.uv_indices,\n                                       normal_indices = obj.normal_indices,\n                                       colors = obj.colors)\n                shapes.append(shape)\n            self.shapes = shapes\n            self.materials = materials\n            self.area_lights = area_lights\n\n    def state_dict(self):\n        return {\n            \'camera\': self.camera.state_dict(),\n            \'shapes\': [s.state_dict() for s in self.shapes],\n            \'materials\': [m.state_dict() for m in self.materials],\n            \'area_lights\': [l.state_dict() for l in self.area_lights],\n            \'envmap\': self.envmap.state_dict() if self.envmap is not None else None\n        }\n\n    @classmethod\n    def load_state_dict(cls, state_dict):\n        envmap_dict = state_dict[\'envmap\']\n        return cls(\n            pyredner.Camera.load_state_dict(state_dict[\'camera\']),\n            [pyredner.Shape.load_state_dict(s) for s in state_dict[\'shapes\']],\n            [pyredner.Material.load_state_dict(m) for m in state_dict[\'materials\']],\n            [pyredner.AreaLight.load_state_dict(l) for l in state_dict[\'area_lights\']],\n            pyredner.EnvironmentMap.load_state_dict(envmap_dict) if envmap_dict is not None else None)\n'"
pyredner_tensorflow/shape.py,80,"b'import pyredner_tensorflow as pyredner\nimport tensorflow as tf\nimport math\nimport numpy as np\nfrom typing import Optional\nimport redner\n\ndef compute_vertex_normal(vertices: tf.Tensor,\n                          indices: tf.Tensor,\n                          weighting_scheme: str = \'max\'):\n    """"""\n        Compute vertex normal by weighted average of nearby face normals using Nelson Max\'s algorithm.\n        See `Weights for Computing Vertex Normals from Facet Vectors <https://escholarship.org/content/qt7657d8h3/qt7657d8h3.pdf?t=ptt283>`_.\n\n        Args\n        ====\n        vertices: tf.Tensor\n            3D position of vertices\n            float32 tensor with size num_vertices x 3\n        indices: tf.Tensor\n            vertex indices of triangle faces.\n            int32 tensor with size num_triangles x 3\n        weighting_scheme: str\n            How do we compute the weighting. Currently we support two weighting methods:\n            \'max\' and \'cotangent\'.\n            \'max\' corresponds to Nelson Max\'s algorithm that uses the inverse length and sine of the angle as the weight\n            (see `Weights for Computing Vertex Normals from Facet Vectors <https://escholarship.org/content/qt7657d8h3/qt7657d8h3.pdf?t=ptt283>`_),\n            \'cotangent\' corresponds to weights derived through a discretization of the gradient of triangle area\n            (see, e.g., ""Implicit Fairing of Irregular Meshes using Diffusion and Curvature Flow"" from Desbrun et al.)\n\n        Returns\n        =======\n        tf.Tensor\n            per-vertex normal, float32 Tensor with size num_vertices x 3\n    """"""\n\n    def dot(v1, v2):\n        return tf.math.reduce_sum(v1 * v2, axis=1)\n    def squared_length(v):\n        return tf.math.reduce_sum(v * v, axis=1)\n    def length(v):\n        return tf.sqrt(squared_length(v))\n    def safe_asin(v):\n        # Hack: asin(1)\' is infinite, so we want to clamp the contribution\n        return tf.asin(tf.clip_by_value(v, 0, 1-1e-6))\n\n    normals = tf.zeros(vertices.shape, dtype = tf.float32)\n\n    # NOTE: Try tf.TensorArray()\n    v = [tf.gather(vertices, indices[:,0]),\n         tf.gather(vertices, indices[:,1]),\n         tf.gather(vertices, indices[:,2])]\n    if weighting_scheme == \'max\':\n        for i in range(3):\n            v0 = v[i]\n            v1 = v[(i + 1) % 3]\n            v2 = v[(i + 2) % 3]\n            e1 = v1 - v0\n            e2 = v2 - v0\n            e1_len = length(e1)\n            e2_len = length(e2)\n            side_a = e1 / tf.reshape(e1_len, [-1, 1])\n            side_b = e2 / tf.reshape(e2_len, [-1, 1])\n            if i == 0:\n                n = tf.linalg.cross(side_a, side_b)\n                n = tf.where(\\\n                    tf.broadcast_to(tf.reshape(length(n) > 0, (-1, 1)), tf.shape(n)),\n                    n / tf.reshape(length(n), (-1, 1)),\n                    tf.zeros(tf.shape(n), dtype = n.dtype))\n\n            # numerically stable angle between two unit direction vectors\n            # http://www.plunk.org/~hatch/rightway.php\n            angle = tf.where(dot(side_a, side_b) < 0,\n                math.pi - 2.0 * safe_asin(0.5 * length(side_a + side_b)),\n                2.0 * safe_asin(0.5 * length(side_b - side_a)))\n            sin_angle = tf.sin(angle)\n\n            e1e2 = e1_len * e2_len\n            # contrib is 0 when e1e2 is 0\n            contrib = tf.reshape(\\\n                tf.where(e1e2 > 0, sin_angle / e1e2, tf.zeros(tf.shape(e1e2), dtype = e1e2.dtype)), (-1, 1))\n            contrib = n * tf.broadcast_to(contrib, [tf.shape(contrib)[0],3]) # In torch, `expand(-1, 3)`\n            normals += tf.scatter_nd(tf.reshape(indices[:, i], [-1, 1]), contrib, shape = tf.shape(normals))\n\n        degenerate_normals = tf.constant((0.0, 0.0, 1.0))\n        degenerate_normals = tf.broadcast_to(tf.reshape(degenerate_normals, (1, 3)), tf.shape(normals))\n        normals = tf.where(tf.broadcast_to(tf.reshape(length(normals) > 0, (-1, 1)), tf.shape(normals)),\n            normals / tf.reshape(length(normals), (-1, 1)),\n            degenerate_normals)\n    elif weighting_scheme == \'cotangent\':\n        # Cotangent weighting generates 0-length normal when\n        # the local surface is planar. Prepare weighted average normal\n        # computed using Nelson Max\'s algorithm for those cases.\n        max_normal = compute_vertex_normal(vertices, indices, \'max\')\n        for i in range(3):\n            v0 = v[i]\n            v1 = v[(i + 1) % 3]\n            v2 = v[(i + 2) % 3]\n            e1 = v1 - v0\n            e2 = v2 - v0\n            e1_len = length(e1)\n            e2_len = length(e2)\n            side_a = e1 / tf.reshape(e1_len, [-1, 1])\n            side_b = e2 / tf.reshape(e2_len, [-1, 1])\n            if i == 0:\n                n = tf.linalg.cross(side_a, side_b)\n                n = tf.where(\\\n                    tf.broadcast_to(tf.reshape(length(n) > 0, (-1, 1)), tf.shape(n)),\n                    n / tf.reshape(length(n), (-1, 1)),\n                    tf.zeros(tf.shape(n), dtype = n.dtype))\n\n            # numerically stable angle between two unit direction vectors\n            # http://www.plunk.org/~hatch/rightway.php\n            angle = tf.where(dot(side_a, side_b) < 0,\n                math.pi - 2.0 * safe_asin(0.5 * length(side_a + side_b)),\n                2.0 * safe_asin(0.5 * length(side_b - side_a)))\n            cotangent = 1.0 / tf.tan(angle)\n            contrib = (v2 - v1) * tf.reshape(cotangent, [-1, 1])\n            normals += tf.scatter_nd(tf.reshape(indices[:, (i + 1) % 3], [-1, 1]), contrib, shape = tf.shape(normals))\n            normals += tf.scatter_nd(tf.reshape(indices[:, (i + 2) % 3], [-1, 1]), -contrib, shape = tf.shape(normals))\n        # Make sure the normals are pointing at the right direction\n        normals = tf.where(tf.broadcast_to(tf.reshape(dot(normals, max_normal), (-1, 1)), (tf.shape(normals))) > 0,\n                           normals, -normals)\n        normals = tf.where(tf.broadcast_to(tf.reshape(length(normals), (-1, 1)), tf.shape(normals)) > 0.05,\n            normals / tf.reshape(length(normals), [-1, 1]),\n            max_normal)\n    else:\n        assert(False, \'Unknown weighting scheme {}\'.format(weighting_scheme))\n\n    return normals\n\ndef compute_uvs(vertices, indices, print_progress = True):\n    """"""\n        Compute UV coordinates of a given mesh using a charting algorithm\n        with least square conformal mapping. This calls the `xatlas <https://github.com/jpcy/xatlas>`_ library.\n\n        Args\n        ====\n        vertices: tf.Tensor\n            3D position of vertices\n            float32 tensor with size num_vertices x 3\n        indices: tf.Tensor\n            vertex indices of triangle faces.\n            int32 tensor with size num_triangles x 3\n\n        Returns\n        =======\n        tf.Tensor\n            uv vertices pool, float32 Tensor with size num_uv_vertices x 3\n        tf.Tensor\n            uv indices, int32 Tensor with size num_triangles x 3\n    """"""\n    with tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n        vertices = tf.identity(vertices)\n        indices = tf.identity(indices)\n        uv_trimesh = redner.UVTriMesh(redner.float_ptr(pyredner.data_ptr(vertices)),\n                                      redner.int_ptr(pyredner.data_ptr(indices)),\n                                      redner.float_ptr(0),\n                                      redner.int_ptr(0),\n                                      int(vertices.shape[0]),\n                                      0,\n                                      int(indices.shape[0]))\n\n        atlas = redner.TextureAtlas()\n        num_uv_vertices = redner.automatic_uv_map([uv_trimesh], atlas, print_progress)[0]\n\n        uvs = tf.zeros([num_uv_vertices, 2], dtype=tf.float32)\n        uv_indices = tf.zeros_like(indices)\n        uv_trimesh.uvs = redner.float_ptr(pyredner.data_ptr(uvs))\n        uv_trimesh.uv_indices = redner.int_ptr(pyredner.data_ptr(uv_indices))\n        uv_trimesh.num_uv_vertices = num_uv_vertices\n\n        redner.copy_texture_atlas(atlas, [uv_trimesh])\n\n    with tf.device(pyredner.get_device_name()):\n        vertices = tf.identity(vertices)\n        indices = tf.identity(indices)\n        uvs = tf.identity(uvs)\n        uv_indices = tf.identity(uv_indices)\n    return uvs, uv_indices\n\nclass Shape:\n    """"""\n        redner supports only triangle meshes for now. It stores a pool of\n        vertices and access the pool using integer index. Some times the\n        two vertices can have the same 3D position but different texture\n        coordinates, because UV mapping creates seams and need to duplicate\n        vertices. In this can we can use an additional ""uv_indices"" array\n        to access the uv pool.\n\n        Args\n        ====\n        vertices: tf.Tensor\n            3D position of vertices\n            float32 tensor with size num_vertices x 3\n        indices: tf.Tensor\n            vertex indices of triangle faces.\n            int32 tensor with size num_triangles x 3\n        uvs: Optional[tf.Tensor]:\n            optional texture coordinates.\n            float32 tensor with size num_uvs x 2\n            doesn\'t need to be the same size with vertices if uv_indices is not None\n        normals: Optional[tf.Tensor]\n            shading normal\n            float32 tensor with size num_normals x 3\n            doesn\'t need to be the same size with vertices if normal_indices is not None\n        uv_indices: Optional[tf.Tensor]\n            overrides indices when accessing uv coordinates\n            int32 tensor with size num_uvs x 2\n        normal_indices: Optional[tf.Tensor]\n            overrides indices when accessing shading normals\n            int32 tensor with size num_normals x 2\n    """"""\n    def __init__(self,\n                 vertices: tf.Tensor,\n                 indices: tf.Tensor,\n                 material_id: int,\n                 uvs: Optional[tf.Tensor] = None,\n                 normals: Optional[tf.Tensor] = None,\n                 uv_indices: Optional[tf.Tensor] = None,\n                 normal_indices: Optional[tf.Tensor] = None,\n                 colors: Optional[tf.Tensor] = None):\n        assert(vertices.dtype == tf.float32)\n        assert(len(vertices.shape) == 2 and vertices.shape[1] == 3)\n        assert(indices.dtype == tf.int32)\n        assert(len(indices.shape) == 2 and indices.shape[1] == 3)\n        if uvs is not None:\n            assert(uvs.dtype == tf.float32)\n            assert(len(uvs.shape) == 2 and uvs.shape[1] == 2)\n        if normals is not None:\n            assert(normals.dtype == tf.float32)\n            assert(len(normals.shape) == 2 and normals.shape[1] == 3)\n        if uv_indices is not None:\n            assert(uv_indices.dtype == tf.int32)\n            assert(len(uv_indices.shape) == 2 and uv_indices.shape[1] == 3)\n        if normal_indices is not None:\n            assert(normal_indices.dtype == tf.int32)\n            assert(len(normal_indices.shape) == 2 and normal_indices.shape[1] == 3)\n        if colors is not None:\n            assert(colors.dtype == tf.float32)\n            assert(len(colors.shape) == 2 and colors.shape[1] == 3)\n\n        self.vertices = vertices\n        self.indices = indices\n        self.uvs = uvs\n        self.normals = normals\n        self.uv_indices = uv_indices\n        self.normal_indices = normal_indices\n        self.colors = colors\n        self.material_id = material_id\n        self.light_id = -1\n\n    def state_dict(self):\n        return {\n            \'vertices\': self.vertices,\n            \'indices\': self.indices,\n            \'material_id\': self.material_id,\n            \'uvs\': self.uvs,\n            \'normals\': self.normals,\n            \'uv_indices\': self.uv_indices,\n            \'normal_indices\': self.normal_indices,\n            \'colors\': self.colors,\n            \'light_id\': self.light_id\n        }\n\n    @classmethod\n    def load_state_dict(cls, state_dict):\n        out = cls(\n            state_dict[\'vertices\'],\n            state_dict[\'indices\'],\n            state_dict[\'material_id\'],\n            state_dict[\'uvs\'],\n            state_dict[\'normals\'],\n            state_dict[\'uv_indices\'],\n            state_dict[\'normal_indices\'],\n            state_dict[\'colors\'])\n        out.light_id = state_dict[\'light_id\']\n        return out\n'"
pyredner_tensorflow/texture.py,12,"b'import tensorflow as tf\nimport pyredner_tensorflow as pyredner\nimport math\n\nclass Texture:\n    """"""\n        Representing a texture and its mipmap.\n\n        Args\n        ====\n        texels: torch.Tensor\n            a float32 tensor with size C or [height, width, C]\n        uv_scale: Optional[torch.Tensor]\n            scale the uv coordinates when mapping the texture\n            a float32 tensor with size 2\n    """"""\n\n    def __init__(self,\n                 texels,\n                 uv_scale = tf.constant([1.0, 1.0])):\n        assert(tf.executing_eagerly())\n        self.texels = texels\n        self.uv_scale = uv_scale\n\n    def generate_mipmap(self):\n        texels = self._texels\n        if len(texels.shape) >= 2:\n            with tf.device(texels.device):\n                # Build a mipmap for texels\n                width = max(texels.shape[0], texels.shape[1])\n                num_levels = min(math.ceil(math.log(width, 2) + 1), 8)\n                num_channels = texels.shape[2]\n                box_filter = tf.ones([2, 2, num_channels, 1],\n                    dtype=tf.float32) / 4.0\n    \n                mipmap = [texels]\n\n                # HWC -> NHWC\n                base_level = tf.expand_dims(texels, axis=0)\n                prev_lvl = base_level\n                for l in range(1, num_levels):\n                    # Pad for circular boundary condition\n                    prev_lvl = tf.concat([prev_lvl, prev_lvl[:,0:1,:,:]], 1)\n                    prev_lvl = tf.concat([prev_lvl, prev_lvl[:,:,0:1,:]], 2)\n                    # Convolve with a box filter\n                    current_lvl = tf.nn.depthwise_conv2d(\n                        prev_lvl,\n                        box_filter,  # [filter_height, filter_width, in_channels, out_channels]\n                        strides = [1,1,1,1],\n                        padding = \'VALID\',   # No padding\n                        data_format = \'NHWC\'\n                    )\n                    # Downsample\n                    # tf.image.resize is too slow, so we use average pooling\n                    current_lvl = tf.nn.avg_pool2d(\n                        current_lvl,\n                        ksize = 2,\n                        strides = 2,\n                        padding = \'SAME\'\n                    )\n                    mipmap.append(tf.squeeze(current_lvl, axis = 0))\n                    prev_lvl = current_lvl\n        else:\n            mipmap = [texels]\n\n        self.mipmap = mipmap\n\n    @property\n    def texels(self):\n        return self._texels\n\n    @texels.setter\n    def texels(self, value):\n        self._texels = value\n        self.generate_mipmap()\n\n    def state_dict(self):\n        return {\n            \'texels\': self.texels,\n            \'mipmap\': self.mipmap,\n            \'uv_scale\': self.uv_scale\n        }\n\n    @classmethod\n    def load_state_dict(cls, state_dict):\n        out = cls.__new__(Texture)\n        out.texels = state_dict[\'texels\']\n        out.mipmap = state_dict[\'mipmap\']\n        out.uv_scale = state_dict[\'uv_scale\'].numpy()\n        return out\n'"
pyredner_tensorflow/transform.py,38,"b'import math\nimport numpy as np\nimport tensorflow as tf\n\ndef radians(deg):\n    return (math.pi / 180.0) * deg\n\ndef normalize(v):\n    """"""\n\n    NOTE: torch.norm() uses Frobineus norm which is Euclidean and L2\n    """"""\n    return v / tf.norm(v)\n\ndef gen_look_at_matrix(pos, look, up):\n    d = normalize(look - pos)\n    right = normalize(tf.linalg.cross(d, normalize(up)))\n    new_up = normalize(tf.linalg.cross(right, d))\n    z = tf.constant(np.zeros([1]), dtype=tf.float32)\n    o = tf.convert_to_tensor(np.ones([1], dtype=np.float32), dtype=tf.float32)\n    return tf.transpose(tf.stack([tf.concat([right , z], 0),\n                                  tf.concat([new_up, z], 0),\n                                  tf.concat([d     , z], 0),\n                                  tf.concat([pos   , o], 0)]))\n\ndef gen_scale_matrix(scale):\n    o = tf.convert_to_tensor(np.ones([1], dtype=np.float32), dtype=tf.float32)\n    return tf.linalg.tensor_diag(tf.concat([scale, o], 0))\n\ndef gen_translate_matrix(translate):\n    z = tf.constant(np.zeros([1]), dtype=tf.float32)\n    o = tf.convert_to_tensor(np.ones([1], dtype=np.float32), dtype=tf.float32)\n    return tf.stack([tf.concat([o, z, z, translate[0:1]], 0),\n                        tf.concat([z, o, z, translate[1:2]], 0),\n                        tf.concat([z, z, o, translate[2:3]], 0),\n                        tf.concat([z, z, z, o], 0)])\n\ndef gen_perspective_matrix(fov, clip_near, clip_far):\n    clip_dist = clip_far - clip_near\n    cot = 1 / tf.tan(radians(fov / 2.0))\n    z = tf.constant(np.zeros([1]), dtype=tf.float32)\n    o = tf.convert_to_tensor(np.ones([1], dtype=np.float32), dtype=tf.float32)\n    return tf.stack([tf.concat([cot,   z,             z,                       z], 0),\n                     tf.concat([  z, cot,             z,                       z], 0),\n                     tf.concat([  z,   z, 1 / clip_dist, - clip_near / clip_dist], 0),\n                     tf.concat([  z,   z,             o,                       z], 0)])\n\ndef gen_rotate_matrix(angles: tf.Tensor) -> tf.Tensor:\n    """"""\n        Given a 3D Euler angle vector, outputs a rotation matrix.\n\n        Args\n        ====\n            angles: torch.Tensor\n                3D Euler angle\n\n        Returns\n        =======\n            tf.Tensor\n                3x3 rotation matrix\n    """"""\n    theta = angles[0]\n    phi = angles[1]\n    psi = angles[2]\n\n    rot_x = tf.stack([\n        tf.constant([1.0,0,0]),\n        tf.constant([0.0, 1.0, 0.0]) * tf.cos(theta) + tf.constant([0.0, 0.0, 1.0]) * tf.sin(theta),\n        tf.constant([0.0, 1.0, 0.0]) * -tf.sin(theta) + tf.constant([0.0, 0.0, 1.0]) * tf.cos(theta),\n        ]\n    )\n\n    rot_y = tf.stack([\n        tf.constant([1.0, 0.0, 0.0]) * tf.cos(phi) + tf.constant([0.0, 0.0, 1.0]) * -tf.sin(phi),\n        tf.constant([0.0, 1.0, 0.0]),\n        tf.constant([1.0, 0.0, 0.0]) * tf.sin(phi) + tf.constant([0.0, 0.0, 1.0]) * tf.cos(phi),\n        ]\n    )\n\n    rot_z = tf.stack([\n        tf.constant([1.0, 0.0, 0.0]) * tf.cos(psi) + tf.constant([0.0, 1.0, 0.0]) * -tf.sin(psi),\n        tf.constant([1.0, 0.0, 0.0]) * tf.sin(psi) + tf.constant([0.0, 1.0, 0.0]) * tf.cos(psi),\n        tf.constant([0.0, 0.0, 1.0]),\n        ]\n    )\n    return rot_z @ (rot_y @ rot_x)\n  \n'"
pyredner_tensorflow/utils.py,39,"b'import tensorflow as tf\nimport math\nimport numpy as np\nimport pyredner_tensorflow as pyredner\n\n####################### Spherical Harmonics utilities ########################\n# Code adapted from ""Spherical Harmonic Lighting: The Gritty Details"", Robin Green\n# http://silviojemma.com/public/papers/lighting/spherical-harmonic-lighting.pdf\ndef associated_legendre_polynomial(l, m, x):\n    pmm = tf.convert_to_tensor(np.ones_like(x), dtype=tf.float32)\n    if m > 0:\n        somx2 = tf.sqrt((1 - x) * (1 + x))\n        fact = 1.0\n        for i in range(1, m + 1):\n            pmm = pmm * (-fact) * somx2\n            fact += 2.0\n    if l == m:\n        return pmm\n    pmmp1 = x * (2.0 * m + 1.0) * pmm\n    if l == m + 1:\n        return pmmp1\n    pll = tf.convert_to_tensor(np.zeros_like(x), dtype=tf.float32)\n    for ll in range(m + 2, l + 1):\n        pll = ((2.0 * ll - 1.0) * x * pmmp1 - (ll + m - 1.0) * pmm) / (ll - m)\n        pmm = pmmp1\n        pmmp1 = pll\n    return pll\n\ndef SH_renormalization(l, m):\n    return math.sqrt((2.0 * l + 1.0) * math.factorial(l - m) / \\\n        (4 * math.pi * math.factorial(l + m)))\n\ndef SH(l, m, theta, phi):\n    if m == 0:\n        return SH_renormalization(l, m) * associated_legendre_polynomial(l, m, tf.cos(theta))\n    elif m > 0:\n        return math.sqrt(2.0) * SH_renormalization(l, m) * \\\n            tf.cos(m * phi) * associated_legendre_polynomial(l, m, tf.cos(theta))\n    else:\n        return math.sqrt(2.0) * SH_renormalization(l, -m) * \\\n            tf.sin(-m * phi) * associated_legendre_polynomial(l, -m, tf.cos(theta))\n\ndef SH_reconstruct(coeffs, res):\n    uv = np.mgrid[0:res[1], 0:res[0]].astype(np.float32)\n    theta = tf.convert_to_tensor((math.pi / res[1]) * (uv[1, :, :] + 0.5))\n    phi = tf.convert_to_tensor((2 * math.pi / res[0]) * (uv[0, :, :] + 0.5))\n    \n    result = tf.constant(np.zeros([res[1], res[0], coeffs.shape[0]]), dtype=tf.float32)\n    num_order = int(math.sqrt(coeffs.shape[1]))\n    i = 0\n    for l in range(num_order):\n        for m in range(-l, l + 1):\n            sh_factor = SH(l, m, theta, phi)\n            result = result + sh_factor.view(sh_factor.shape[0], sh_factor.shape[1], 1) * coeffs[:, i]\n            i += 1\n    result = tf.math.maximum(result,\n        tf.constant(np.zeros([res[1], res[0], coeffs.shape[0]]), dtype=tf.float32)\n        )\n    return result\n#######################################################################################\n\ndef generate_sphere(theta_steps: int,\n                    phi_steps: int):\n    """"""\n        Generate a triangle mesh representing a UV sphere,\n        center at (0, 0, 0) with radius 1.\n\n        Args\n        ====\n        theta_steps: int\n            zenith subdivision\n        phi_steps: int\n            azimuth subdivision\n\n        Returns\n        =======\n        tf.Tensor\n            vertices\n        tf.Tensor\n            indices\n        tf.Tensor\n            uvs\n        tf.Tensor\n            normals\n    """"""\n    d_theta = math.pi / (theta_steps - 1)\n    d_phi = (2 * math.pi) / (phi_steps - 1)\n\n    num_vertices = theta_steps * phi_steps - 2 * (phi_steps - 1)\n    vertices = np.zeros([num_vertices, 3], dtype = np.float32)\n    uvs = np.zeros([num_vertices, 2], dtype = np.float32)\n    vertices_index = 0\n    for theta_index in range(theta_steps):\n        sin_theta = math.sin(theta_index * d_theta)\n        cos_theta = math.cos(theta_index * d_theta)\n        if theta_index == 0:\n            # For the two polars of the sphere, only generate one vertex\n            vertices[vertices_index, :] = \\\n                np.array([0.0, 1.0, 0.0], dtype = np.float32)\n            uvs[vertices_index, 0] = 0.0\n            uvs[vertices_index, 1] = 0.0\n            vertices_index += 1\n        elif theta_index == theta_steps - 1:\n            # For the two polars of the sphere, only generate one vertex\n            vertices[vertices_index, :] = \\\n                np.array([0.0, -1.0, 0.0], dtype = np.float32)\n            uvs[vertices_index, 0] = 0.0\n            uvs[vertices_index, 1] = 1.0\n            vertices_index += 1\n        else:\n            for phi_index in range(phi_steps):\n                sin_phi = math.sin(phi_index * d_phi)\n                cos_phi = math.cos(phi_index * d_phi)\n                vertices[vertices_index, :] = \\\n                    np.array([sin_theta * cos_phi, cos_theta, sin_theta * sin_phi])\n                uvs[vertices_index, 0] = phi_index * d_phi / (2 * math.pi)\n                uvs[vertices_index, 1] = theta_index * d_theta / math.pi\n                vertices_index += 1\n\n    indices = []\n    for theta_index in range(1, theta_steps):\n        for phi_index in range(phi_steps - 1):\n            if theta_index < theta_steps - 1:\n                id0 = phi_steps * theta_index + phi_index - (phi_steps - 1)\n                id1 = phi_steps * theta_index + phi_index + 1 - (phi_steps - 1)\n            else:\n                # There is only one vertex at the pole\n                assert(theta_index == theta_steps - 1)\n                id0 = num_vertices - 1\n                id1 = num_vertices - 1\n            if theta_index > 1:\n                id2 = phi_steps * (theta_index - 1) + phi_index - (phi_steps - 1)\n                id3 = phi_steps * (theta_index - 1) + phi_index + 1 - (phi_steps - 1)\n            else:\n                # There is only one vertex at the pole\n                assert(theta_index == 1)\n                id2 = 0\n                id3 = 0\n\n            if (theta_index < theta_steps - 1):\n                indices.append([id0, id2, id1])\n            if (theta_index > 1):\n                indices.append([id1, id2, id3])\n\n    indices = tf.convert_to_tensor(indices, dtype=tf.int32)\n    vertices = tf.convert_to_tensor(vertices, dtype=tf.float32)\n    uvs = tf.convert_to_tensor(uvs, dtype=tf.float32)\n    normals = tf.identity(vertices)\n    return (vertices, indices, uvs, normals)\n\n\ndef generate_quad_light(position: tf.Tensor,\n                        look_at: tf.Tensor,\n                        size: tf.Tensor,\n                        intensity: tf.Tensor):\n    """"""\n        Generate a pyredner.Object that is a quad light source.\n\n        Args\n        ====\n        position: tf.Tensor\n            1-d tensor of size 3\n        look_at: tf.Tensor\n            1-d tensor of size 3\n        size: tf.Tensor\n            1-d tensor of size 2\n        intensity: tf.Tensor\n            1-d tensor of size 3\n\n        Returns\n        =======\n        pyredner.Object\n            quad light source\n    """"""\n    d = look_at - position\n    d = d / tf.norm(d)\n    # ONB -- generate two axes that are orthogonal to d\n    a = 1 / (1 + d[2])\n    b = -d[0] * d[1] * a\n    x = tf.where(d[2] < (-1 + 1e-6),\n                 tf.constant([0.0, -1.0, 0.0]),\n                 tf.stack([1 - d[0] * d[0] * a, b, -d[0]]))\n    y = tf.where(d[2] < (-1 + 1e-6),\n                 tf.constant([-1.0, 0.0, 0.0]),\n                 tf.stack([b, 1 - d[1] * d[1] * a, -d[1]]))\n    v0 = position - x * size[0] * 0.5 - y * size[1] * 0.5\n    v1 = position + x * size[0] * 0.5 - y * size[1] * 0.5\n    v2 = position - x * size[0] * 0.5 + y * size[1] * 0.5\n    v3 = position + x * size[0] * 0.5 + y * size[1] * 0.5\n\n    vertices = tf.stack((v0, v1, v2, v3), axis = 0)\n    indices = tf.constant([[0, 1, 2],[1, 3, 2]], dtype = tf.int32)\n    m = pyredner.Material(diffuse_reflectance = tf.constant([0.0, 0.0, 0.0]))\n    return pyredner.Object(vertices = vertices,\n                           indices = indices,\n                           material = m,\n                           light_intensity = intensity)\n\n############################################3\ndef read_tensor(filename, shape):\n    """"""\n\n    Args:\n        filename(str)\n        shape(np.array)\n    """"""\n    with open(filename) as f:\n        tensor = np.array(f.readline().split(), dtype=np.float32)\n        tensor = np.reshape(tensor, shape)\n\n    return tensor\n\ndef linear_to_srgb(x):\n    return tf.where(x <= 0.0031308, 12.92 * x, 1.055 * tf.pow(x, 1.0 / 2.4) - 0.055)\n\ndef srgb_to_linear(x):\n    return tf.where(x <= 0.04045, x / 12.92, tf.pow((x + 0.055) / 1.055, 2.4))'"
tests/test_batch.py,0,"b""import torch\nimport math\nimport pyredner\nimport redner\n\npyredner.set_use_gpu(torch.cuda.is_available())\n\n\n\nclass BatchRenderFunction(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, seed, *args):\n        batch_dims = args[0]\n        args_old_format = args[1:]\n        chunk_len = int(len(args_old_format)/batch_dims)\n        h, w = args_old_format[11]\n        result = torch.zeros(\\\n            batch_dims, h, w, 9, device = pyredner.get_device(), requires_grad=True)\n        for k in range(0, batch_dims):\n            sub_args = args_old_format[k*chunk_len:(k+1)*chunk_len]\n            result[k, :, :, :] = pyredner.RenderFunction.forward(ctx, seed, *sub_args)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_img):\n        #None gradient for seed and batch_dims\n        ret_list = (None, None,)\n        batch_dims = grad_img.shape[0]\n        for k in range(0, batch_dims):\n            #[1:] cuz original backward function returns None grad for seed input, but we manage that ourselves\n            ret_list = ret_list + pyredner.RenderFunction.backward(ctx, grad_img[k,:,:,:])[1:]\n        return ret_list\n\n\nbatch_render = BatchRenderFunction.apply\n\n# Load from the teapot Wavefront object file\nmaterial_map, mesh_list, light_map = pyredner.load_obj('../tutorials/teapot.obj')\n# Compute shading normal\nfor _, mesh in mesh_list:\n    mesh.normals = pyredner.compute_vertex_normal(mesh.vertices, mesh.indices)\n\n# Setup camera\ncam = pyredner.Camera(position = torch.tensor([0.0, 30.0, 200.0]),\n                      look_at = torch.tensor([0.0, 30.0, 0.0]),\n                      up = torch.tensor([0.0, 1.0, 0.0]),\n                      fov = torch.tensor([45.0]), # in degree\n                      clip_near = 1e-2, # needs to > 0\n                      resolution = (256, 256),\n                      fisheye = False)\n\n#\nmesh = mesh_list[0][1]\nshapes = [pyredner.Shape(\\\n        vertices = mesh.vertices,\n        indices = mesh.indices,\n        uvs = mesh.uvs,\n        normals = mesh.normals,\n        material_id = 0)]\n\n\ntex_path='../tutorials/teapot.png'\ntex_tensor = pyredner.imread(tex_path)\nif pyredner.get_use_gpu():\n    tex_tensor = tex_tensor.cuda(device = pyredner.get_device())\n\n\ndiffuse_reflectance = tex_tensor\nmaterials = [pyredner.Material(diffuse_reflectance=diffuse_reflectance)]\n\n\n# Construct the scene.\n# Don't setup any light sources, only use primary visibility.\nscene = pyredner.Scene(cam, shapes, materials, area_lights = [], envmap = None)\n\n# TEST1: render (test forward function)\n\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 0,\n    channels = [redner.channels.position,\n                redner.channels.shading_normal,\n                redner.channels.diffuse_reflectance])\nscene_args = [2] + 2*scene_args\ng_buffer = batch_render(0, *scene_args)\n\nimg1 = g_buffer[0,:,:,6:9]\npyredner.imwrite(img1.cpu(), 'results/test_multichannels/test1.png')\nimg2 = g_buffer[1,:,:,6:9]\npyredner.imwrite(img2.cpu(), 'results/test_multichannels/test2.png')\n\n# TEST2: convergence (test backward function)\ntarget = pyredner.imread('results/test_multichannels/test1.png')\nif pyredner.get_use_gpu():\n    target = target.cuda(device = pyredner.get_device())\n\nbatch_dims = 2\ndiffuse_reflectance = torch.zeros(\\\n    batch_dims, 128, 128, 3, device = pyredner.get_device(), requires_grad=True)\n\nscenes = [scene, scene]\n\noptimizer = torch.optim.Adam([diffuse_reflectance], lr=1e-2)\nfor t in range(200):\n    print('iteration:', t)\n    optimizer.zero_grad()\n    scene_args_batch = [batch_dims]\n    for k in range(0, batch_dims):\n        scenes[k].materials[0].diffuse_reflectance = pyredner.Texture(diffuse_reflectance[k,:,:,:])\n        scene_args = pyredner.RenderFunction.serialize_scene(\\\n            scene = scenes[k],\n            num_samples = 16,\n            max_bounces = 0,\n            channels = [redner.channels.position,\n                    redner.channels.shading_normal,\n                    redner.channels.diffuse_reflectance])\n        scene_args_batch = scene_args_batch + scene_args\n    g_buffer = batch_render(t, *scene_args_batch)\n\n    img1 = g_buffer[0,:,:,6:9]\n    img2 = g_buffer[1,:,:,6:9]\n    loss = (img1 - target).pow(2).sum() + (img2 - target).pow(2).sum()\n    print('loss:', loss.item())\n    loss.backward()\n    optimizer.step()\n\npyredner.imwrite(diffuse_reflectance[0, :, :, :].cpu(), 'results/test_multichannels/testtex1.png')\npyredner.imwrite(diffuse_reflectance[1, :, :, :].cpu(), 'results/test_multichannels/testtex2.png')\n"""
tests/test_bunny_box.py,0,"b'import scipy.ndimage.filters\nimport pyredner\nimport numpy as np\nimport torch\n\npyredner.set_use_gpu(torch.cuda.is_available())\n\nscene = pyredner.load_mitsuba(\'scenes/bunny_box.xml\')\n\nscene.shapes[-1].vertices += torch.tensor([0, 0.01, 0], device = pyredner.get_device())\n\nargs=pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 4,\n    max_bounces = 6)\nrender = pyredner.RenderFunction.apply\n# Render our target. The first argument is the seed for RNG in the renderer.\nimg = render(0, *args)\npyredner.imwrite(img.cpu(), \'results/test_bunny_box/target.exr\')\npyredner.imwrite(img.cpu(), \'results/test_bunny_box/target.png\')\ntarget = pyredner.imread(\'results/test_bunny_box/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda(device = pyredner.get_device())\n\nbunny_vertices = scene.shapes[-1].vertices.clone()\nbunny_translation = torch.tensor([0.1,0.4,0.1], device = pyredner.get_device(), requires_grad=True)\nbunny_rotation = torch.tensor([-0.2,0.1,-0.1], device = pyredner.get_device(), requires_grad=True)\nbunny_rotation_matrix = pyredner.gen_rotate_matrix(bunny_rotation)\n\nscene.shapes[-1].vertices = \\\n    (bunny_vertices-torch.mean(bunny_vertices, 0))@torch.t(bunny_rotation_matrix) + \\\n    torch.mean(bunny_vertices, 0) + bunny_translation\nargs=pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4,\n        max_bounces = 6)\nimg = render(1, *args)\npyredner.imwrite(img.cpu(), \'results/test_bunny_box/init.exr\')\npyredner.imwrite(img.cpu(), \'results/test_bunny_box/init.png\')\n\noptimizer = torch.optim.Adam([bunny_translation, bunny_rotation], lr=1e-2)\nfor t in range(200):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    # Forward pass: render the image\n    bunny_rotation_matrix = pyredner.gen_rotate_matrix(bunny_rotation)\n\n    scene.shapes[-1].vertices = \\\n        (bunny_vertices-torch.mean(bunny_vertices, 0))@torch.t(bunny_rotation_matrix) + \\\n        torch.mean(bunny_vertices, 0) + bunny_translation\n    args=pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4,\n        max_bounces = 6)\n    img = render(t+1, *args)\n    pyredner.imwrite(img.cpu(), \'results/test_bunny_box/iter_{}.png\'.format(t))\n\n    dirac = np.zeros([7,7], dtype=np.float32)\n    dirac[3,3] = 1.0\n    dirac = torch.from_numpy(dirac)\n    f = np.zeros([3, 3, 7, 7], dtype=np.float32)\n    gf = scipy.ndimage.filters.gaussian_filter(dirac, 1.0)\n    f[0, 0, :, :] = gf\n    f[1, 1, :, :] = gf\n    f[2, 2, :, :] = gf\n    f = torch.from_numpy(f)\n    if pyredner.get_use_gpu():\n        f = f.cuda(device = pyredner.get_device())\n    m = torch.nn.AvgPool2d(2)\n\n    res = 256\n    diff_0 = (img - target).view(1, res, res, 3).permute(0, 3, 2, 1)\n    diff_1 = m(torch.nn.functional.conv2d(diff_0, f, padding=3)) # 128 x 128\n    diff_2 = m(torch.nn.functional.conv2d(diff_1, f, padding=3)) # 64 x 64\n    diff_3 = m(torch.nn.functional.conv2d(diff_2, f, padding=3)) # 32 x 32\n    diff_4 = m(torch.nn.functional.conv2d(diff_3, f, padding=3)) # 16 x 16\n    loss = diff_0.pow(2).sum() / (res*res) + \\\n           diff_1.pow(2).sum() / ((res/2)*(res/2)) + \\\n           diff_2.pow(2).sum() / ((res/4)*(res/4)) + \\\n           diff_3.pow(2).sum() / ((res/8)*(res/8)) + \\\n           diff_4.pow(2).sum() / ((res/16)*(res/16))\n    print(\'loss:\', loss.item())\n\n    loss.backward()\n    print(\'bunny_translation.grad:\', bunny_translation.grad)\n    print(\'bunny_rotation.grad:\', bunny_rotation.grad)\n\n    optimizer.step()\n    print(\'bunny_translation:\', bunny_translation)\n    print(\'bunny_rotation:\', bunny_rotation)\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/bunny_box/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/bunny_box/out.mp4""])\n'"
tests/test_camera_distortion.py,0,"b'import pyredner\nimport torch\n\npyredner.set_use_gpu(torch.cuda.is_available())\n\nposition = torch.tensor([1.0, 0.0, -3.0])\nlook_at = torch.tensor([1.0, 0.0, 0.0])\nup = torch.tensor([0.0, 1.0, 0.0])\nfov = torch.tensor([45.0])\nclip_near = 1e-2\n\n# randomly generate distortion parameters\ntorch.manual_seed(1234)\ntarget_distort_params = (torch.rand(8) - 0.5) * 0.05\nresolution = (256, 256)\ncam = pyredner.Camera(position = position,\n                      look_at = look_at,\n                      up = up,\n                      fov = fov,\n                      clip_near = clip_near,\n                      resolution = resolution,\n                      distortion_params = target_distort_params)\n\ncheckerboard_texture = pyredner.imread(\'scenes/teapot.png\')\nif pyredner.get_use_gpu():\n    checkerboard_texture = checkerboard_texture.cuda(device = pyredner.get_device())\n\nmat_checkerboard = pyredner.Material(\\\n    diffuse_reflectance = checkerboard_texture)\nmat_black = pyredner.Material(\\\n    diffuse_reflectance = torch.tensor([0.0, 0.0, 0.0], device = pyredner.get_device()))\n\nplane = pyredner.Object(vertices = torch.tensor([[-1.0,-1.0, 0.0],\n                                                 [-1.0, 1.0, 0.0],\n                                                 [ 1.0,-1.0, 0.0],\n                                                 [ 1.0, 1.0, 0.0]],\n                                                 device = pyredner.get_device()),\n                        indices = torch.tensor([[0, 1, 2],\n                                                [1, 3, 2]],\n                                               dtype = torch.int32,\n                                               device = pyredner.get_device()),\n                        uvs = torch.tensor([[0.05, 0.05],\n                                            [0.05, 0.95],\n                                            [0.95, 0.05],\n                                            [0.95, 0.95]], device = pyredner.get_device()),\n                        material = mat_checkerboard)\nscene = pyredner.Scene(camera=cam, objects=[plane])\nimg = pyredner.render_albedo(scene=scene)\npyredner.imwrite(img.cpu(), \'results/test_camera_distortion/target.exr\')\npyredner.imwrite(img.cpu(), \'results/test_camera_distortion/target.png\')\n# Read the target image we just saved.\ntarget = pyredner.imread(\'results/test_camera_distortion/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda(device = pyredner.get_device())\n\ncam.distortion_params = torch.zeros(8, requires_grad = True)\nscene = pyredner.Scene(camera=cam, objects=[plane])\nimg = pyredner.render_albedo(scene=scene)\npyredner.imwrite(img.cpu(), \'results/test_camera_distortion/init.exr\')\npyredner.imwrite(img.cpu(), \'results/test_camera_distortion/init.png\')\n\n# Optimize for triangle vertices.\noptimizer = torch.optim.Adam([cam.distortion_params], lr=1e-3)\nfor t in range(200):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    scene = pyredner.Scene(camera=cam, objects=[plane])\n    img = pyredner.render_albedo(scene=scene)\n    pyredner.imwrite(img.cpu(), \'results/test_camera_distortion/iter_{}.png\'.format(t))\n    loss = (img - target).pow(2).sum()\n    print(\'loss:\', loss.item())\n\n    loss.backward()\n    print(\'grad:\', cam.distortion_params.grad)\n\n    optimizer.step()\n    print(\'distortion_params:\', cam.distortion_params)\n\nimg = pyredner.render_albedo(scene=scene)\npyredner.imwrite(img.cpu(), \'results/test_camera_distortion/final.exr\')\npyredner.imwrite(img.cpu(), \'results/test_camera_distortion/final.png\')\n\n# Convert the intermediate renderings to a video.\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_camera_distortion/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_camera_distortion/out.mp4""])\n'"
tests/test_compute_uvs.py,0,"b""import pyredner\nimport torch\n\n# Automatic UV mapping example adapted from tutorial 2\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\nmaterial_map, mesh_list, light_map = pyredner.load_obj('scenes/teapot.obj')\nfor _, mesh in mesh_list:\n    mesh.normals = pyredner.compute_vertex_normal(mesh.vertices, mesh.indices)\n    mesh.uvs, mesh.uv_indices = pyredner.compute_uvs(mesh.vertices, mesh.indices)\n\ncam = pyredner.Camera(position = torch.tensor([0.0, 30.0, 200.0]),\n                      look_at = torch.tensor([0.0, 30.0, 0.0]),\n                      up = torch.tensor([0.0, 1.0, 0.0]),\n                      fov = torch.tensor([45.0]), # in degree\n                      clip_near = 1e-2, # needs to > 0\n                      resolution = (256, 256),\n                      fisheye = False)\n\nmaterial_id_map = {}\nmaterials = []\ncount = 0\nfor key, value in material_map.items():\n    material_id_map[key] = count\n    count += 1\n    materials.append(value)\n\nshapes = []\nfor mtl_name, mesh in mesh_list:\n    assert(mesh.normal_indices is None)\n    shapes.append(pyredner.Shape(\\\n        vertices = mesh.vertices,\n        indices = mesh.indices,\n        material_id = material_id_map[mtl_name],\n        uvs = mesh.uvs,\n        normals = mesh.normals,\n        uv_indices = mesh.uv_indices))\n\nenvmap = pyredner.imread('sunsky.exr')\nif pyredner.get_use_gpu():\n    envmap = envmap.cuda()\nenvmap = pyredner.EnvironmentMap(envmap)\n\n# Finally we construct our scene using all the variables we setup previously.\nscene = pyredner.Scene(cam, shapes, materials, area_lights = [], envmap = envmap)\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 512,\n    max_bounces = 1)\nrender = pyredner.RenderFunction.apply\nimg = render(0, *scene_args)\npyredner.imwrite(img.cpu(), 'results/test_compute_uvs/img.exr')\npyredner.imwrite(img.cpu(), 'results/test_compute_uvs/img.png')\n"""
tests/test_compute_vertex_normals.py,0,"b""import pyredner\nimport torch\n\nobjects = pyredner.load_obj('scenes/teapot.obj', return_objects=True)\ncamera = pyredner.automatic_camera_placement(objects, resolution=(512, 512))\nscene = pyredner.Scene(camera = camera, objects = objects)\n\nlight = pyredner.PointLight(position = (camera.position + torch.tensor((0.0, 0.0, 100.0))).to(pyredner.get_device()),\n                            intensity = torch.tensor((20000.0, 30000.0, 20000.0), device = pyredner.get_device()))\n\nimg = pyredner.render_deferred(scene = scene, lights = [light])\npyredner.imwrite(img.cpu(), 'results/test_compute_vertex_normals/no_vertex_normal.exr')\n\nfor obj in objects:\n    obj.normals = pyredner.compute_vertex_normal(obj.vertices, obj.indices, 'max')\nscene = pyredner.Scene(camera = camera, objects = objects)\nimg = pyredner.render_deferred(scene = scene, lights = [light])\npyredner.imwrite(img.cpu(), 'results/test_compute_vertex_normals/max_vertex_normal.exr')\n\nfor obj in objects:\n    obj.normals = pyredner.compute_vertex_normal(obj.vertices, obj.indices, 'cotangent')\nscene = pyredner.Scene(camera = camera, objects = objects)\nimg = pyredner.render_deferred(scene = scene, lights = [light])\npyredner.imwrite(img.cpu(), 'results/test_compute_vertex_normals/cotangent_vertex_normal.exr')"""
tests/test_envmap.py,0,"b'import pyredner\nimport numpy as np\nimport torch\nimport math\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\ncam = pyredner.Camera(position = torch.tensor([0.0, 0.0, -5.0]),\n                      look_at = torch.tensor([0.0, 0.0, 0.0]),\n                      up = torch.tensor([0.0, 1.0, 0.0]),\n                      fov = torch.tensor([45.0]), # in degree\n                      clip_near = 1e-2, # needs to > 0\n                      resolution = (256, 256),\n                      fisheye = False)\n\nmat_grey = pyredner.Material(\\\n    diffuse_reflectance = \\\n        torch.tensor([0.4, 0.4, 0.4], device = pyredner.get_device()),\n    specular_reflectance = \\\n        torch.tensor([0.5, 0.5, 0.5], device = pyredner.get_device()),\n    roughness = \\\n        torch.tensor([0.05], device = pyredner.get_device()))\n\nmaterials = [mat_grey]\n\nvertices, indices, uvs, normals = pyredner.generate_sphere(128, 64)\nshape_sphere = pyredner.Shape(\\\n    vertices = vertices,\n    indices = indices,\n    uvs = uvs,\n    normals = normals,\n    material_id = 0)\nshapes = [shape_sphere]\n\nenvmap = pyredner.imread(\'sunsky.exr\')\nif pyredner.get_use_gpu():\n    envmap = envmap.cuda(device = pyredner.get_device())\nenvmap = pyredner.EnvironmentMap(envmap)\nscene = pyredner.Scene(cam, shapes, materials, [], [], envmap)\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\nrender = pyredner.RenderFunction.apply\nimg = render(0, *scene_args)\npyredner.imwrite(img.cpu(), \'results/test_envmap/target.exr\')\npyredner.imwrite(img.cpu(), \'results/test_envmap/target.png\')\ntarget = pyredner.imread(\'results/test_envmap/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda()\n\nenvmap_texels = torch.tensor(0.5 * torch.ones([32, 64, 3],\n    device = pyredner.get_device()),\n    requires_grad = True)\nenvmap = pyredner.EnvironmentMap(torch.abs(envmap_texels))\nscene = pyredner.Scene(cam, shapes, materials, [], [], envmap)\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\nimg = render(1, *scene_args)\npyredner.imwrite(img.cpu(), \'results/test_envmap/init.png\')\ndiff = torch.abs(target - img)\npyredner.imwrite(diff.cpu(), \'results/test_envmap/init_diff.png\')\n\noptimizer = torch.optim.Adam([envmap_texels], lr=1e-2)\nfor t in range(600):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    envmap = pyredner.EnvironmentMap(torch.abs(envmap_texels))\n    scene = pyredner.Scene(cam, shapes, materials, [], [], envmap)\n    scene_args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4,\n        max_bounces = 1)\n    img = render(t+1, *scene_args)\n    pyredner.imwrite(img.cpu(), \'results/test_envmap/iter_{}.png\'.format(t))\n    pyredner.imwrite(torch.abs(envmap_texels).cpu(), \'results/test_envmap/envmap_{}.exr\'.format(t))\n    loss = torch.pow(img - target, 2).sum()\n    print(\'loss:\', loss.item())\n\n    loss.backward()\n    optimizer.step()\n\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\nimg = render(602, *scene_args)\npyredner.imwrite(img.cpu(), \'results/test_envmap/final.exr\')\npyredner.imwrite(img.cpu(), \'results/test_envmap/final.png\')\npyredner.imwrite(torch.abs(target - img).cpu(), \'results/test_envmap/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_envmap/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_envmap/out.mp4""])\n'"
tests/test_g_buffer.py,0,"b'import pyredner\nimport redner\nimport numpy as np\nimport torch\nimport skimage.transform\n\n# Optimize depth and normal of a teapot\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\n# Set up the pyredner scene for rendering:\nmaterial_map, mesh_list, light_map = pyredner.load_obj(\'scenes/teapot.obj\')\nfor _, mesh in mesh_list:\n    mesh.normals = pyredner.compute_vertex_normal(mesh.vertices, mesh.indices)\n\n# Setup camera\ncam = pyredner.Camera(position = torch.tensor([0.0, 30.0, 200.0]),\n                      look_at = torch.tensor([0.0, 30.0, 0.0]),\n                      up = torch.tensor([0.0, 1.0, 0.0]),\n                      fov = torch.tensor([45.0]), # in degree\n                      clip_near = 1e-2, # needs to > 0\n                      resolution = (256, 256),\n                      fisheye = False)\n\n# Setup materials\nmaterial_id_map = {}\nmaterials = []\ncount = 0\nfor key, value in material_map.items():\n    material_id_map[key] = count\n    count += 1\n    materials.append(value)\n\n# Setup geometries\nshapes = []\nfor mtl_name, mesh in mesh_list:\n    shapes.append(pyredner.Shape(\\\n        vertices = mesh.vertices,\n        indices = mesh.indices,\n        uvs = mesh.uvs,\n        normals = mesh.normals,\n        material_id = material_id_map[mtl_name]))\n\n# We don\'t setup any light source here\n\n# Construct the scene\nscene = pyredner.Scene(cam, shapes, materials, area_lights = [], envmap = None)\n# Serialize the scene\n# Here we specify the output channels as ""depth"", ""shading_normal""\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 0,\n    channels = [redner.channels.depth, redner.channels.shading_normal])\n\n# Render the scene as our target image.\nrender = pyredner.RenderFunction.apply\n# Render. The first argument is the seed for RNG in the renderer.\nimg = render(0, *scene_args)\n# Save the images.\ndepth = img[:, :, 0]\nnormal = img[:, :, 1:4]\npyredner.imwrite(depth.cpu(),\n    \'results/test_g_buffer/target_depth.exr\')\npyredner.imwrite(depth.cpu(),\n    \'results/test_g_buffer/target_depth.png\', normalize = True)\npyredner.imwrite(normal.cpu(),\n    \'results/test_g_buffer/target_normal.exr\')\npyredner.imwrite(normal.cpu(),\n    \'results/test_g_buffer/target_normal.png\', normalize = True)\n# Read the target image we just saved.\ntarget_depth = pyredner.imread(\'results/test_g_buffer/target_depth.exr\')\ntarget_normal = pyredner.imread(\'results/test_g_buffer/target_normal.exr\')\nif pyredner.get_use_gpu():\n    target_depth = target_depth.cuda(device = pyredner.get_device())\n    target_normal = target_normal.cuda(device = pyredner.get_device())\n\n# Perturb the teapot by a translation and a rotation to the object\ntranslation_params = torch.tensor([0.1, -0.1, 0.1],\n    device = pyredner.get_device(), requires_grad=True)\ntranslation = translation_params * 100.0\neuler_angles = torch.tensor([0.1, -0.1, 0.1], requires_grad=True)\n# These are the vertices we want to apply the transformation\nshape0_vertices = shapes[0].vertices.clone()\nshape1_vertices = shapes[1].vertices.clone()\n# We can use pyredner.gen_rotate_matrix to generate 3x3 rotation matrices\nrotation_matrix = pyredner.gen_rotate_matrix(euler_angles)\nif pyredner.get_use_gpu():\n    rotation_matrix = rotation_matrix.cuda()\ncenter = torch.mean(torch.cat([shape0_vertices, shape1_vertices]), 0)\n# Shift the vertices to the center, apply rotation matrix,\n# shift back to the original space\nshapes[0].vertices = \\\n    (shape0_vertices - center) @ torch.t(rotation_matrix) + \\\n    center + translation\nshapes[1].vertices = \\\n    (shape1_vertices - center) @ torch.t(rotation_matrix) + \\\n    center + translation\n# Since we changed the vertices, we need to regenerate the shading normals\nshapes[0].normals = pyredner.compute_vertex_normal(shapes[0].vertices, shapes[0].indices)\nshapes[1].normals = pyredner.compute_vertex_normal(shapes[1].vertices, shapes[1].indices)\n# We need to serialize the scene again to get the new arguments.\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 0,\n    channels = [redner.channels.depth, redner.channels.shading_normal])\n# Render the initial guess.\nimg = render(1, *scene_args)\ndepth = img[:, :, 0]\nnormal = img[:, :, 1:4]\n# Save the images.\npyredner.imwrite(depth.cpu(),\n    \'results/test_g_buffer/init_depth.png\',\n    normalize = True)\npyredner.imwrite(depth.cpu(),\n    \'results/test_g_buffer/init_normal.png\',\n    normalize = True)\n# Compute the difference and save the images.\ndiff_depth = torch.abs(target_depth - depth)\ndiff_normal = torch.abs(target_normal - normal)\npyredner.imwrite(diff_depth.cpu(),\n    \'results/test_g_buffer/init_depth_diff.png\')\npyredner.imwrite(diff_normal.cpu(),\n    \'results/test_g_buffer/init_normal_diff.png\')\n\n# Optimize for triangle vertices.\noptimizer = torch.optim.Adam([translation_params, euler_angles], lr=1e-2)\n# Run 200 Adam iterations.\nfor t in range(200):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    # Forward pass: apply the mesh operation and render the image.\n    translation = translation_params * 100.0\n    rotation_matrix = pyredner.gen_rotate_matrix(euler_angles)\n    if pyredner.get_use_gpu():\n        rotation_matrix = rotation_matrix.cuda(device = pyredner.get_device())\n    center = torch.mean(torch.cat([shape0_vertices, shape1_vertices]), 0)\n    shapes[0].vertices = \\\n        (shape0_vertices - center) @ torch.t(rotation_matrix) + \\\n        center + translation\n    shapes[1].vertices = \\\n        (shape1_vertices - center) @ torch.t(rotation_matrix) + \\\n        center + translation\n    shapes[0].normals = pyredner.compute_vertex_normal(shapes[0].vertices, shapes[0].indices)\n    shapes[1].normals = pyredner.compute_vertex_normal(shapes[1].vertices, shapes[1].indices)\n    scene_args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4, # We use less samples in the Adam loop.\n        max_bounces = 0,\n        channels = [redner.channels.depth, redner.channels.shading_normal])\n    # Important to use a different seed every iteration, otherwise the result\n    # would be biased.\n    img = render(t+1, *scene_args)\n    depth = img[:, :, 0]\n    normal = img[:, :, 1:4]\n    # Save the intermediate render.\n    pyredner.imwrite(depth.cpu(),\n        \'results/test_g_buffer/iter_depth_{}.png\'.format(t),\n        normalize = True)\n    pyredner.imwrite(normal.cpu(),\n        \'results/test_g_buffer/iter_normal_{}.png\'.format(t),\n        normalize = True)\n    # Compute the loss function. Here it is L2.\n    loss = (depth - target_depth).pow(2).sum() / 200.0 + (normal - target_normal).pow(2).sum()\n    print(\'loss:\', loss.item())\n\n    # Backpropagate the gradients.\n    loss.backward()\n    # Print the gradients\n    print(\'translation_params.grad:\', translation_params.grad)\n    print(\'euler_angles.grad:\', euler_angles.grad)\n\n    # Take a gradient descent step.\n    optimizer.step()\n    # Print the current pose parameters.\n    print(\'translation:\', translation)\n    print(\'euler_angles:\', euler_angles)\n\n# Render the final result.\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 0,\n    channels = [redner.channels.depth, redner.channels.shading_normal])\nimg = render(202, *scene_args)\ndepth = img[:, :, 0]\nnormal = img[:, :, 1:4]\n# Save the images.\npyredner.imwrite(depth.cpu(),\n    \'results/test_g_buffer/final_depth.exr\')\npyredner.imwrite(depth.cpu(),\n    \'results/test_g_buffer/init_depth.png\',\n    normalize = True)\npyredner.imwrite(normal.cpu(),\n    \'results/test_g_buffer/final_normal.exr\')\npyredner.imwrite(normal.cpu(),\n    \'results/test_g_buffer/final_normal.png\',\n    normalize = True)\ndiff_depth = torch.abs(target_depth - depth)\ndiff_normal = torch.abs(target_normal - normal)\npyredner.imwrite(diff_depth.cpu(),\n    \'results/test_g_buffer/init_depth_diff.png\')\npyredner.imwrite(diff_normal.cpu(),\n    \'results/test_g_buffer/init_normal_diff.png\')\n\n# Convert the intermediate renderings to a video.\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_g_buffer/iter_depth_%d.png"", ""-vb"", ""20M"",\n    ""results/test_g_buffer/out_depth.mp4""])\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_g_buffer/iter_normal_%d.png"", ""-vb"", ""20M"",\n    ""results/test_g_buffer/out_normal.mp4""])\n'"
tests/test_living_room.py,0,"b'import pyredner\nimport numpy as np\nimport torch\nimport scipy.ndimage.filters\nimport urllib.request\nimport os\nimport zipfile\nfrom shutil import copyfile\n\nif not os.path.isdir(\'scenes/living-room-3\'):\n    print(\'Scene file not found, downloading\')\n    filedata = urllib.request.urlretrieve(\'https://benedikt-bitterli.me/resources/mitsuba/living-room-3.zip\', \'living-room-3.zip\')\n    print(\'Unzipping living-room-3.zip\')\n    zip_ref = zipfile.ZipFile(\'living-room-3.zip\', \'r\')\n    zip_ref.extractall(\'scenes/\')\n    print(\'Copying scene file\')\n    copyfile(\'scenes/living-room-3-scene.xml\', \'scenes/living-room-3/scene.xml\')\n    print(\'Removing zip file\')\n    os.remove(\'living-room-3.zip\')\n\n# Optimize for camera pose\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\n# Load the scene from a Mitsuba scene file\nscene = pyredner.load_mitsuba(\'scenes/living-room-3/scene.xml\')\nprint(\'scene loaded\')\n\nmax_bounces = 6\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 512,\n    max_bounces = max_bounces)\n\nrender = pyredner.RenderFunction.apply\n# Render our target. The first argument is the seed for RNG in the renderer.\nimg = render(0, *args)\npyredner.imwrite(img.cpu(), \'results/test_living_room/target.exr\')\npyredner.imwrite(img.cpu(), \'results/test_living_room/target.png\')\ntarget = pyredner.imread(\'results/test_living_room/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda(device = pyredner.get_device())\n\nscene.camera.look_at = torch.tensor([-0.556408, 0.951295, -3.98066], requires_grad=True)\nscene.camera.position = torch.tensor([0.00419251, 0.973707, -4.80844], requires_grad=True)\nscene.camera.up = torch.tensor([-0.00920347, 0.999741, 0.020835], requires_grad=True)\n\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 512,\n    max_bounces = max_bounces)\n\nimg = render(1, *args)\npyredner.imwrite(img.cpu(), \'results/test_living_room/init.exr\')\npyredner.imwrite(img.cpu(), \'results/test_living_room/init.png\')\ndiff = torch.abs(target - img)\npyredner.imwrite(diff.cpu(), \'results/test_living_room/init_diff.png\')\n\noptimizer = torch.optim.Adam([scene.camera.position, scene.camera.look_at, scene.camera.up],\n    lr = 5e-3, betas=(0.5, 0.9))\niter_count = 0\nfor t in range(600):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    scene.camera = pyredner.Camera(position   = scene.camera.position,\n                                   look_at    = scene.camera.look_at,\n                                   up         = scene.camera.up,\n                                   fov        = scene.camera.fov,\n                                   clip_near  = scene.camera.clip_near,\n                                   resolution = scene.camera.resolution)\n    args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4,\n        max_bounces = max_bounces)\n\n    img = render(t + 1, *args)\n    # Save the intermediate render.\n    pyredner.imwrite(img.cpu(), \'results/test_living_room/iter_{}.png\'.format(t))\n\n    diff = img - target\n    dirac = np.zeros([7,7], dtype = np.float32)\n    dirac[3,3] = 1.0\n    dirac = torch.from_numpy(dirac)\n    f = np.zeros([3, 3, 7, 7], dtype = np.float32)\n    gf = scipy.ndimage.filters.gaussian_filter(dirac, 1.0)\n    f[0, 0, :, :] = gf\n    f[1, 1, :, :] = gf\n    f[2, 2, :, :] = gf\n    f = torch.from_numpy(f)\n    if pyredner.get_use_gpu():\n        f = f.cuda(device = pyredner.get_device())\n    m = torch.nn.AvgPool2d(2)\n\n    r = 256\n    diff_0 = (img - target).view(1, r, r, 3).permute(0, 3, 2, 1)\n    diff_1 = m(torch.nn.functional.conv2d(diff_0, f, padding=3))\n    diff_2 = m(torch.nn.functional.conv2d(diff_1, f, padding=3))\n    diff_3 = m(torch.nn.functional.conv2d(diff_2, f, padding=3))\n    diff_4 = m(torch.nn.functional.conv2d(diff_3, f, padding=3))\n    loss = diff_0.pow(2).sum() / (r*r) + \\\n           diff_1.pow(2).sum() / ((r/2)*(r/2)) + \\\n           diff_2.pow(2).sum() / ((r/4)*(r/4)) + \\\n           diff_3.pow(2).sum() / ((r/8)*(r/8)) + \\\n           diff_4.pow(2).sum() / ((r/16)*(r/16))\n    print(\'loss:\', loss.item())\n    print(\'cam.look_at:\', scene.camera.look_at)\n    print(\'cam.position:\', scene.camera.position)\n    print(\'cam.up:\', scene.camera.up)\n\n    loss.backward()\n    optimizer.step()\n\n    print(\'cam.look_at.grad:\', scene.camera.look_at.grad)\n    print(\'cam.position.grad:\', scene.camera.position.grad)\n    print(\'cam.up.grad:\', scene.camera.up.grad)\n\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 4,\n    max_bounces = max_bounces)\nimg = render(602, *args)\npyredner.imwrite(img.cpu(), \'results/test_living_room/final.exr\')\npyredner.imwrite(img.cpu(), \'results/test_living_room/final.png\')\ndiff = torch.abs(target - img)\npyredner.imwrite(diff.cpu(), \'results/test_living_room/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_living_room/iter_%d.png"", ""-vb"", ""20M"", ""results/test_living_room/out.mp4""])\n'"
tests/test_multichannels.py,0,"b""import pyredner\nimport redner\nimport torch\nimport math\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\n# Load from the teapot Wavefront object file\nmaterial_map, mesh_list, light_map = pyredner.load_obj('../tutorials/teapot.obj')\n# Compute shading normal\nfor _, mesh in mesh_list:\n    mesh.normals = pyredner.compute_vertex_normal(mesh.vertices, mesh.indices)\n\n# Setup camera\ncam = pyredner.Camera(position = torch.tensor([0.0, 30.0, 200.0]),\n                      look_at = torch.tensor([0.0, 30.0, 0.0]),\n                      up = torch.tensor([0.0, 1.0, 0.0]),\n                      fov = torch.tensor([45.0]), # in degree\n                      clip_near = 1e-2, # needs to > 0\n                      resolution = (256, 256),\n                      fisheye = False)\n\n# Get a list of shapes\nshapes = []\nfor mtl_name, mesh in mesh_list:\n    shapes.append(pyredner.Shape(\\\n        vertices = mesh.vertices,\n        indices = mesh.indices,\n        uvs = mesh.uvs,\n        normals = mesh.normals,\n        material_id = 0)) # Set all materials to the generic texture\n\nrender = pyredner.RenderFunction.apply\n\ntex_path='../tutorials/teapot.png'\ntex_tensor = pyredner.imread(tex_path)\nif pyredner.get_use_gpu():\n    tex_tensor = tex_tensor.cuda(device = pyredner.get_device())\n\n\n### TEST 1: regular 3-channels texture rasterization\n\ngeneric_texture = tex_tensor\n\nmaterials = [pyredner.Material(generic_texture=generic_texture)]\n\n# Construct the scene.\n# Don't setup any light sources, only use primary visibility.\nscene = pyredner.Scene(cam, shapes, materials, area_lights = [], envmap = None)\n# Serialize the scene.\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 4, # Still need some samples for anti-aliasing\n    max_bounces = 0,\n    channels = [redner.channels.generic_texture])\n\n# g buffer is the 16-channels texture\ng_buffer = render(0, *scene_args)\nprint(g_buffer.shape)\nimg = g_buffer[:, :, 0:3]\nprint(img.shape)\n\n# Save the images\npyredner.imwrite(img.cpu(), 'results/test_multichannels/target_test1.exr')\npyredner.imwrite(img.cpu(), 'results/test_multichannels/target_test1.png')\n\n### TEST 2: 16-channels texture rasterization\n\ngeneric_texture = generic_texture = torch.zeros(\\\n    128, 128, 16, device = pyredner.get_device())\ngeneric_texture[:, :, 9:12] = tex_tensor\n\nmaterials = [pyredner.Material(generic_texture=generic_texture)]\n\n# Construct the scene.\n# Don't setup any light sources, only use primary visibility.\nscene = pyredner.Scene(cam, shapes, materials, area_lights = [], envmap = None)\n# Serialize the scene.\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 4, # Still need some samples for anti-aliasing\n    max_bounces = 0,\n    channels = [redner.channels.generic_texture])\n# g buffer is the 16-channels texture\ng_buffer = render(0, *scene_args)\nprint(g_buffer.shape)\nimg = g_buffer[:, :, 9:12]\nprint(img.shape)\n\n# Save the images\npyredner.imwrite(img.cpu(), 'results/test_multichannels/target_test2.exr')\npyredner.imwrite(img.cpu(), 'results/test_multichannels/target_test2.png')\n\n\n\n\n\n### TEST 3: test generic_texture gradients: we start from black texture, and must\n#obtain at the end a 16-channels texture with the expected channels at the right positions\n\n#we use test1 output as target\ntarget = pyredner.imread('results/test_multichannels/target_test1.png')\nif pyredner.get_use_gpu():\n    target = target.cuda(device = pyredner.get_device())\n\ngeneric_texture = generic_texture = torch.zeros(\\\n    128, 128, 16, device = pyredner.get_device(), requires_grad=True)\n\nmaterials = [pyredner.Material(generic_texture=generic_texture)]\nscene = pyredner.Scene(cam, shapes, materials, area_lights = [], envmap = None)\n\noptimizer = torch.optim.Adam([generic_texture], lr=1e-2)\nfor t in range(200):\n    print('iteration:', t)\n    optimizer.zero_grad()\n\n    scene.materials[0].generic_texture = pyredner.Texture(generic_texture)\n\n    scene_args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4, # Still need some samples for anti-aliasing\n        max_bounces = 0,\n        channels = [redner.channels.generic_texture])\n\n    g_buffer = render(t, *scene_args)\n    img = g_buffer[:, :, 12:15]\n    loss = (img - target).pow(2).sum()\n    print('loss:', loss.item())\n    loss.backward()\n    optimizer.step()\n\n# Save the images\n# To compute the loss we used g_buffer channels 12 to 15 so the optimization process\n# should have caused generic_texture channels 12 to 15 to converge towards tex_tensor (the original teapot texture)\npyredner.imwrite(generic_texture[:, :, 12:15].cpu(), 'results/test_multichannels/target_test3.exr')\npyredner.imwrite(generic_texture[:, :, 12:15].cpu(), 'results/test_multichannels/target_test3.png')\n"""
tests/test_qmc.py,0,"b'import pyredner\nimport redner\nimport torch\nimport scipy\nimport scipy.ndimage\nimport numpy as np\n\n# Test Quasi Monte Carlo rendering.\n# We optimize for the materials of a Cornell box scene\n\nscene = pyredner.load_mitsuba(\'scenes/cbox/cbox.xml\')\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 512,\n    max_bounces = 5, # Set max_bounces = 5 for global illumination\n    sampler_type = redner.SamplerType.sobol) # Use Sobol sampler\nrender = pyredner.RenderFunction.apply\nimg = render(0, *scene_args)\npyredner.imwrite(img.cpu(), \'results/test_qmc/target.exr\')\npyredner.imwrite(img.cpu(), \'results/test_qmc/target.png\')\ntarget = pyredner.imread(\'results/test_qmc/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda()\n\n# Now let\'s generate an initial guess by perturbing the reference.\n# Let\'s set all the diffuse color to gray by manipulating material.diffuse_reflectance.\n# We also store all the material variables to optimize in a list.\nmaterial_vars = []\nfor mi, m in enumerate(scene.materials):\n    var = torch.tensor([0.5, 0.5, 0.5],\n                       device = pyredner.get_device(),\n                       requires_grad = True)\n    material_vars.append(var)\n    m.diffuse_reflectance = pyredner.Texture(var)\n\n# Serialize the scene and render the initial guess\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 512,\n    max_bounces = 5,\n    sampler_type = redner.SamplerType.sobol)\nimg = render(1, *scene_args)\npyredner.imwrite(img.cpu(), \'results/test_qmc/init.png\')\n\n# Optimize for parameters.\noptimizer = torch.optim.Adam(material_vars, lr=1e-2)\nfor t in range(200):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    # Forward pass: serialize the scene and render the image\n    # Need to redefine the camera\n    scene_args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4,\n        max_bounces = 5,\n        sampler_type = redner.SamplerType.sobol)\n    # Important to use a different seed every iteration, otherwise the result\n    # would be biased.\n    img = render(t+1, *scene_args)\n    pyredner.imwrite(img.cpu(), \'results/test_qmc/iter_{}.png\'.format(t))\n\n    # Compute the loss function.\n    # We clamp the difference between -1 and 1 to prevent\n    # light source from being dominating the loss function\n    loss = (img - target).clamp(-1.0, 1.0).pow(2).sum()\n    print(\'loss:\', loss.item())\n\n    # Backpropagate the gradients.\n    loss.backward()\n\n    # Take a gradient descent step.\n    optimizer.step()\n\n    # Important: the material parameters has hard constraints: the\n    # reflectance and roughness cannot be negative. We enforce them here\n    # by projecting the values to the boundaries.\n    for var in material_vars:\n        var.data = var.data.clamp(1e-5, 1.0)\n        print(var)\n\n# Render the final result.\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 512,\n    max_bounces = 5,\n    sampler_type = redner.SamplerType.sobol)\nimg = render(202, *scene_args)\n# Save the images and differences.\npyredner.imwrite(img.cpu(), \'results/test_qmc/final.exr\')\npyredner.imwrite(img.cpu(), \'results/test_qmc/final.png\')\npyredner.imwrite(torch.abs(target - img).cpu(), \'results/test_qmc/final_diff.png\')\n\n# Convert the intermediate renderings to a video.\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_qmc/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_qmc/out.mp4""])\n'"
tests/test_sample_pixel_center.py,0,"b""import pyredner\nimport torch\n\n# Test the sample pixel center flag\n\npyredner.set_use_gpu(torch.cuda.is_available())\nobjects = pyredner.load_obj('scenes/teapot.obj', return_objects=True)\ncamera = pyredner.automatic_camera_placement(objects, resolution=(128, 128))\nscene = pyredner.Scene(camera = camera, objects = objects)\nimg = pyredner.render_albedo(scene, sample_pixel_center = True)\npyredner.imwrite(img.cpu(), 'results/test_sample_pixel_center/img_no_aa.exr')\nimg = pyredner.render_albedo(scene, sample_pixel_center = False)\npyredner.imwrite(img.cpu(), 'results/test_sample_pixel_center/img_with_aa.exr')"""
tests/test_screen_gradient.py,0,"b""import pyredner\nimport torch\nfrom matplotlib import cm\nimport os\nimport numpy as np\nimport skimage\n\ndef normalize(x, min_, max_):\n    range = max(abs(min_), abs(max_))\n    return (x + range) / (2 * range)\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\nobjects = pyredner.load_obj('scenes/teapot.obj', return_objects=True)\ncamera = pyredner.automatic_camera_placement(objects, resolution=(512, 512))\nscene = pyredner.Scene(camera = camera, objects = objects)\nscreen_gradient_img = pyredner.RenderFunction.visualize_screen_gradient(\n\tgrad_img = None,\n    seed = 0,\n    scene = scene,\n    num_samples = 4,\n    max_bounces = 0,\n    channels = [pyredner.channels.diffuse_reflectance])\n\ndirectory = 'results/test_screen_gradient'\nif directory != '' and not os.path.exists(directory):\n    os.makedirs(directory)\n\nclamp_factor = 0.2\nx_diff = screen_gradient_img[:, :, 0]\ndx = cm.viridis(normalize(x_diff, x_diff.min() * clamp_factor, x_diff.max() * clamp_factor).cpu().numpy())\nskimage.io.imsave('results/test_screen_gradient/dx.png', (dx * 255).astype(np.uint8))\ny_diff = screen_gradient_img[:, :, 1]\ndy = cm.viridis(normalize(y_diff, y_diff.min() * clamp_factor, y_diff.max() * clamp_factor).cpu().numpy())\nskimage.io.imsave('results/test_screen_gradient/dy.png', (dy * 255).astype(np.uint8))\n"""
tests/test_serialize.py,0,"b""import pyredner\nimport numpy as np\nimport torch\n\ncam = pyredner.Camera(position = torch.tensor([0.0, 0.0, -5.0]),\n                      look_at = torch.tensor([0.0, 0.0, 0.0]),\n                      up = torch.tensor([0.0, 1.0, 0.0]),\n                      fov = torch.tensor([45.0]), # in degree\n                      clip_near = 1e-2, # needs to > 0\n                      resolution = (256, 256),\n                      fisheye = False)\n\nmat_grey = pyredner.Material(\\\n    diffuse_reflectance = \\\n        torch.tensor([0.5, 0.5, 0.5], device = pyredner.get_device()))\nmaterials = [mat_grey]\n\nshape_triangle = pyredner.Shape(\\\n    vertices = torch.tensor([[-1.7, 1.0, 0.0], [1.0, 1.0, 0.0], [-0.5, -1.0, 0.0]],\n        device = pyredner.get_device()),\n    indices = torch.tensor([[0, 1, 2]], dtype = torch.int32,\n        device = pyredner.get_device()),\n    uvs = None,\n    normals = None,\n    material_id = 0)\n\nshape_light = pyredner.Shape(\\\n    vertices = torch.tensor([[-1.0, -1.0, -7.0],\n                             [ 1.0, -1.0, -7.0],\n                             [-1.0,  1.0, -7.0],\n                             [ 1.0,  1.0, -7.0]], device = pyredner.get_device()),\n    indices = torch.tensor([[0, 1, 2],[1, 3, 2]],\n        dtype = torch.int32, device = pyredner.get_device()),\n    uvs = None,\n    normals = None,\n    material_id = 0)\n\nshapes = [shape_triangle, shape_light]\nlight = pyredner.AreaLight(shape_id = 1, \n                           intensity = torch.tensor([20.0,20.0,20.0]))\narea_lights = [light]\n\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\n\nscene_state_dict = scene.state_dict()\nscene = pyredner.Scene.load_state_dict(scene_state_dict)\n\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\n\nrender = pyredner.RenderFunction.apply\nimg = render(0, *scene_args)\npyredner.imwrite(img.cpu(), 'results/test_serialize/img.exr')\n"""
tests/test_shadow_blocker.py,0,"b'import pyredner\nimport numpy as np\nimport torch\n\n# Optimize four vertices of a shadow blocker\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\n# Set up the scene using Pytorch tensor\nposition = torch.tensor([0.0, 2.0, -5.0])\nlook_at = torch.tensor([0.0, 0.0, 0.0])\nup = torch.tensor([0.0, 1.0, 0.0])\nfov = torch.tensor([45.0])\nclip_near = 1e-2\n\nresolution = (256, 256)\ncam = pyredner.Camera(position = position,\n                      look_at = look_at,\n                      up = up,\n                      fov = fov,\n                      clip_near = clip_near,\n                      resolution = resolution)\n\nmat_grey = pyredner.Material(\\\n    diffuse_reflectance = torch.tensor([0.5, 0.5, 0.5],\n    device = pyredner.get_device()))\nmat_black = pyredner.Material(\\\n    diffuse_reflectance = torch.tensor([0.0, 0.0, 0.0],\n    device = pyredner.get_device()))\nmaterials = [mat_grey, mat_black]\n\nfloor_vertices = torch.tensor([[-2.0,0.0,-2.0],[-2.0,0.0,2.0],[2.0,0.0,-2.0],[2.0,0.0,2.0]],\n    device = pyredner.get_device())\nfloor_indices = torch.tensor([[0,1,2], [1,3,2]],\n    device = pyredner.get_device(), dtype = torch.int32)\nshape_floor = pyredner.Shape(floor_vertices, floor_indices, 0)\nblocker_vertices = torch.tensor(\\\n    [[-0.5,3.0,-0.5],[-0.5,3.0,0.5],[0.5,3.0,-0.5],[0.5,3.0,0.5]],\n    device = pyredner.get_device())\nblocker_indices = torch.tensor([[0,1,2], [1,3,2]],\n    device = pyredner.get_device(), dtype = torch.int32)\nshape_blocker = pyredner.Shape(blocker_vertices, blocker_indices, 0)\nlight_vertices = torch.tensor(\\\n    [[-0.1,5,-0.1],[-0.1,5,0.1],[0.1,5,-0.1],[0.1,5,0.1]],\n    device = pyredner.get_device())\nlight_indices = torch.tensor([[0,2,1], [1,2,3]],\n    device = pyredner.get_device(), dtype = torch.int32)\nshape_light = pyredner.Shape(light_vertices, light_indices, 1)\nshapes = [shape_floor, shape_blocker, shape_light]\nlight_intensity = torch.tensor([1000.0, 1000.0, 1000.0])\n# The first argument is the shape id of the light\nlight = pyredner.AreaLight(2, light_intensity)\narea_lights = [light]\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\n\n# Alias of the render function\nrender = pyredner.RenderFunction.apply\n# Render our target\nimg = render(0, *args)\npyredner.imwrite(img.cpu(), \'results/test_shadow_blocker/target.exr\')\npyredner.imwrite(img.cpu(), \'results/test_shadow_blocker/target.png\')\ntarget = pyredner.imread(\'results/test_shadow_blocker/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda(device = pyredner.get_device())\n\n# Perturb the scene, this is our initial guess\nshape_blocker.vertices = torch.tensor(\\\n\t[[-0.2,3.5,-0.8],[-0.8,3.0,0.3],[0.4,2.8,-0.8],[0.3,3.2,1.0]],\n\tdevice = pyredner.get_device(),\n    requires_grad=True)\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\n# Render the initial guess\nimg = render(1, *args)\npyredner.imwrite(img.cpu(), \'results/test_shadow_blocker/init.png\')\ndiff = torch.abs(target - img)\npyredner.imwrite(diff.cpu(), \'results/test_shadow_blocker/init_diff.png\')\n\n# Optimize for blocker vertices\noptimizer = torch.optim.Adam([shape_blocker.vertices], lr=1e-2)\nfor t in range(200):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    # Forward pass: render the image\n    args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4,\n        max_bounces = 1)\n    img = render(t+1, *args)\n    pyredner.imwrite(img.cpu(), \'results/test_shadow_blocker/iter_{}.png\'.format(t))\n    loss = (img - target).pow(2).sum()\n    print(\'loss:\', loss.item())\n\n    loss.backward()\n    print(\'grad:\', shape_blocker.vertices.grad)\n\n    optimizer.step()\n    print(\'vertices:\', shape_blocker.vertices)\n\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\nimg = render(202, *args)\npyredner.imwrite(img.cpu(), \'results/test_shadow_blocker/final.exr\')\npyredner.imwrite(img.cpu(), \'results/test_shadow_blocker/final.png\')\npyredner.imwrite(torch.abs(target - img).cpu(), \'results/test_shadow_blocker/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_shadow_blocker/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_shadow_blocker/out.mp4""])\n'"
tests/test_shadow_camera.py,0,"b'import pyredner\nimport numpy as np\nimport torch\n\n# Optimize camera pose looking at shadow\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\n# Set up the scene using Pytorch tensor\nposition = torch.tensor([0.0, 9.0, 0.0])\nlook_at = torch.tensor([0.0, 0.0, 0.0])\nup = torch.tensor([0.0, 0.0, 1.0])\nfov = torch.tensor([45.0])\nclip_near = 1e-2\n\nresolution = (256, 256)\ncam = pyredner.Camera(position = position,\n                     look_at = look_at,\n                     up = up,\n                     fov = fov,\n                     clip_near = clip_near,\n                     resolution = resolution)\n\nmat_grey = pyredner.Material(\\\n    diffuse_reflectance = torch.tensor([0.5, 0.5, 0.5],\n    device = pyredner.get_device()))\nmat_black = pyredner.Material(\\\n    diffuse_reflectance = torch.tensor([0.0, 0.0, 0.0],\n    device = pyredner.get_device()))\nmaterials = [mat_grey, mat_black]\n\nfloor_vertices = torch.tensor([[-20.0,0.0,-20.0],[-20.0,0.0,20.0],[20.0,0.0,-20.0],[20.0,0.0,20.0]],\n    device = pyredner.get_device())\nfloor_indices = torch.tensor([[0,1,2], [1,3,2]],\n    device = pyredner.get_device(), dtype = torch.int32)\nshape_floor = pyredner.Shape(floor_vertices, floor_indices, 0)\nblocker_vertices = torch.tensor(\\\n    [[-0.5,10.0,-0.5],[-0.5,10.0,0.5],[0.5,10.0,-0.5],[0.5,10.0,0.5]],\n    device = pyredner.get_device())\nblocker_indices = torch.tensor([[0,1,2], [1,3,2]],\n    device = pyredner.get_device(), dtype = torch.int32)\nshape_blocker = pyredner.Shape(blocker_vertices, blocker_indices, 0)\nlight_vertices = torch.tensor(\\\n    [[-0.1,15,-0.1],[-0.1,15,0.1],[0.1,15,-0.1],[0.1,15,0.1]],\n    device = pyredner.get_device())\nlight_indices = torch.tensor([[0,2,1], [1,2,3]],\n    device = pyredner.get_device(), dtype = torch.int32)\nshape_light = pyredner.Shape(light_vertices, light_indices, 1)\nshapes = [shape_floor, shape_blocker, shape_light]\nlight_intensity = torch.tensor([5000.0, 5000.0, 5000.0])\n# The first argument is the shape id of the light\nlight = pyredner.AreaLight(2, light_intensity)\narea_lights = [light]\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\n\n# Alias of the render function\nrender = pyredner.RenderFunction.apply\n# Render our target\nimg = render(0, *args)\npyredner.imwrite(img.cpu(), \'results/test_shadow_camera/target.exr\')\npyredner.imwrite(img.cpu(), \'results/test_shadow_camera/target.png\')\ntarget = pyredner.imread(\'results/test_shadow_camera/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda(device = pyredner.get_device())\n\n# Perturb the scene, this is our initial guess\nposition = torch.tensor([-2.0, 7.0, 2.0], requires_grad = True)\nscene.camera = pyredner.Camera(position = position,\n                               look_at = look_at,\n                               up = up,\n                               fov = fov,\n                               clip_near = clip_near,\n                               resolution = resolution)\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\n# Render the initial guess\nimg = render(1, *args)\npyredner.imwrite(img.cpu(), \'results/test_shadow_camera/init.png\')\ndiff = torch.abs(target - img)\npyredner.imwrite(diff.cpu(), \'results/test_shadow_camera/init_diff.png\')\n\n# Optimize for blocker vertices\noptimizer = torch.optim.Adam([position], lr=5e-2)\nfor t in range(200):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    # Forward pass: render the image\n    # Need to rerun the Camera constructor for PyTorch autodiff to compute the derivatives\n    scene.camera = pyredner.Camera(position   = position,\n                                   look_at    = look_at,\n                                   up         = up,\n                                   fov        = fov,\n                                   clip_near  = clip_near,\n                                   resolution = resolution)\n    args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4,\n        max_bounces = 1)\n    img = render(t+1, *args)\n    pyredner.imwrite(img.cpu(), \'results/test_shadow_camera/iter_{}.png\'.format(t))\n    loss = (img - target).pow(2).sum()\n    print(\'loss:\', loss.item())\n\n    loss.backward()\n    print(\'grad:\', position.grad)\n\n    optimizer.step()\n    print(\'position:\', position)\n\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\nimg = render(202, *args)\npyredner.imwrite(img.cpu(), \'results/test_shadow_camera/final.exr\')\npyredner.imwrite(img.cpu(), \'results/test_shadow_camera/final.png\')\npyredner.imwrite(torch.abs(target - img).cpu(), \'results/test_shadow_camera/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_shadow_camera/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_shadow_camera/out.mp4""])\n'"
tests/test_shadow_glossy.py,0,"b'import pyredner\nimport numpy as np\nimport torch\n\n# Optimize four vertices of a shadow blocker, where the receiver is highly glossy\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\n# Set up the scene using Pytorch tensor\nposition = torch.tensor([0.0, 2.0, -4.0])\nlook_at = torch.tensor([0.0, -2.0, 0.0])\nup = torch.tensor([0.0, 1.0, 0.0])\nfov = torch.tensor([45.0])\nclip_near = 1e-2\n\nresolution = (256, 256)\ncam = pyredner.Camera(position = position,\n                     look_at = look_at,\n                     up = up,\n                     fov = fov,\n                     clip_near = clip_near,\n                     resolution = resolution)\n\nmat_shiny = pyredner.Material(\\\n    diffuse_reflectance = torch.tensor([0.0, 0.0, 0.0], device = pyredner.get_device()),\n    specular_reflectance = torch.tensor([1.0, 1.0, 1.0], device = pyredner.get_device()),\n    roughness = torch.tensor([0.0005], device = pyredner.get_device()))\nmat_grey = pyredner.Material(\\\n    diffuse_reflectance = torch.tensor([0.5, 0.5, 0.5],\n    device = pyredner.get_device()))\nmat_black = pyredner.Material(\\\n    diffuse_reflectance = torch.tensor([0.0, 0.0, 0.0],\n    device = pyredner.get_device()))\nmaterials = [mat_shiny, mat_grey, mat_black]\n\nfloor_vertices = torch.tensor([[-4.0,0.0,-4.0],[-4.0,0.0,4.0],[4.0,0.0,-4.0],[4.0,0.0,4.0]],\n    device = pyredner.get_device())\nfloor_indices = torch.tensor([[0,1,2], [1,3,2]],\n    device = pyredner.get_device(), dtype = torch.int32)\nshape_floor = pyredner.Shape(floor_vertices, floor_indices, 0)\nblocker_vertices = torch.tensor(\\\n    [[0.0,1.0,0.5],[0.0,3.0,0.5],[0.8,1.0,0.5],[0.8,3.0,0.5]],\n    device = pyredner.get_device())\nblocker_indices = torch.tensor([[0,1,2], [1,3,2]],\n    device = pyredner.get_device(), dtype = torch.int32)\nshape_blocker = pyredner.Shape(blocker_vertices, blocker_indices, 1)\nlight_vertices = torch.tensor(\\\n    [[-2.0,0.0,4.0],[-2.0,12.0,4.0],[2.0,0.0,4.0],[2.0,12.0,4.0]],\n    device = pyredner.get_device())\nlight_indices = torch.tensor([[0,1,2], [1,3,2]],\n    device = pyredner.get_device(), dtype = torch.int32)\nshape_light = pyredner.Shape(light_vertices, light_indices, 2)\nshapes = [shape_floor, shape_blocker, shape_light]\nlight_intensity = torch.tensor([0.5, 0.5, 0.5])\n# The first argument is the shape id of the light\nlight = pyredner.AreaLight(2, light_intensity)\narea_lights = [light]\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 1024,\n    max_bounces = 1)\n\n# Alias of the render function\nrender = pyredner.RenderFunction.apply\n# Render our target\nimg = render(0, *args)\npyredner.imwrite(img.cpu(), \'results/test_shadow_glossy/target.exr\')\npyredner.imwrite(img.cpu(), \'results/test_shadow_glossy/target.png\')\ntarget = pyredner.imread(\'results/test_shadow_glossy/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda(device = pyredner.get_device())\n\n# Perturb the scene, this is our initial guess\nshape_blocker.vertices = torch.tensor(\\\n    [[-0.6,0.9,0.4],[-0.8,3.3,0.7],[0.2,1.1,0.6],[0.3,3.2,0.4]],\n    device = pyredner.get_device(),\n    requires_grad=True)\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 1024,\n    max_bounces = 1)\n# Render the initial guess\nimg = render(1, *args)\npyredner.imwrite(img.cpu(), \'results/test_shadow_glossy/init.png\')\ndiff = torch.abs(target - img)\npyredner.imwrite(diff.cpu(), \'results/test_shadow_glossy/init_diff.png\')\n\n# Optimize for blocker vertices\noptimizer = torch.optim.Adam([shape_blocker.vertices], lr=2e-2)\nfor t in range(200):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    # Forward pass: render the image\n    args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4,\n        max_bounces = 1)\n    img = render(t+1, *args)\n    pyredner.imwrite(img.cpu(), \'results/test_shadow_glossy/iter_{}.png\'.format(t))\n    loss = (img - target).pow(2).sum()\n    print(\'loss:\', loss.item())\n\n    loss.backward()\n    print(\'grad:\', shape_blocker.vertices.grad)\n\n    optimizer.step()\n    print(\'vertices:\', shape_blocker.vertices)\n\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 1024,\n    max_bounces = 1)\nimg = render(202, *args)\npyredner.imwrite(img.cpu(), \'results/test_shadow_glossy/final.exr\')\npyredner.imwrite(img.cpu(), \'results/test_shadow_glossy/final.png\')\npyredner.imwrite(torch.abs(target - img).cpu(), \'results/test_shadow_glossy/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_shadow_glossy/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_shadow_glossy/out.mp4""])\n'"
tests/test_shadow_light.py,0,"b'import pyredner\nimport numpy as np\nimport torch\n\n# Optimize light translation to match shadow\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\n# Set up the scene using Pytorch tensor\nposition = torch.tensor([0.0, 2.0, -5.0])\nlook_at = torch.tensor([0.0, 0.0, 0.0])\nup = torch.tensor([0.0, 1.0, 0.0])\nfov = torch.tensor([45.0])\nclip_near = 1e-2\n\nresolution = (256, 256)\ncam = pyredner.Camera(position = position,\n                     look_at = look_at,\n                     up = up,\n                     fov = fov,\n                     clip_near = clip_near,\n                     resolution = resolution)\n\nmat_grey = pyredner.Material(\\\n    diffuse_reflectance = torch.tensor([0.5, 0.5, 0.5],\n    device = pyredner.get_device()))\nmat_black = pyredner.Material(\\\n    diffuse_reflectance = torch.tensor([0.0, 0.0, 0.0],\n    device = pyredner.get_device()))\nmaterials = [mat_grey, mat_black]\n\nfloor_vertices = torch.tensor([[-2.0,0.0,-2.0],[-2.0,0.0,2.0],[2.0,0.0,-2.0],[2.0,0.0,2.0]],\n\tdevice = pyredner.get_device())\nfloor_indices = torch.tensor([[0,1,2], [1,3,2]],\n    device = pyredner.get_device(), dtype = torch.int32)\nshape_floor = pyredner.Shape(floor_vertices, floor_indices, 0)\nblocker_vertices = torch.tensor(\\\n    [[-0.5,3.0,-0.5],[-0.5,3.0,0.5],[0.5,3.0,-0.5],[0.5,3.0,0.5]],\n    device = pyredner.get_device())\nblocker_indices = torch.tensor([[0,1,2], [1,3,2]],\n    device = pyredner.get_device(), dtype = torch.int32)\nshape_blocker = pyredner.Shape(blocker_vertices, blocker_indices, 0)\nlight_vertices = torch.tensor(\\\n    [[-0.1,5,-0.1],[-0.1,5,0.1],[0.1,5,-0.1],[0.1,5,0.1]],\n    device = pyredner.get_device())\nlight_indices = torch.tensor([[0,2,1], [1,2,3]],\n    device = pyredner.get_device(), dtype = torch.int32)\nshape_light = pyredner.Shape(light_vertices, light_indices, 1)\nshapes = [shape_floor, shape_blocker, shape_light]\nlight_intensity = torch.tensor([1000.0, 1000.0, 1000.0])\n# The first argument is the shape id of the light\nlight = pyredner.AreaLight(2, light_intensity)\narea_lights = [light]\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\n\n# Alias of the render function\nrender = pyredner.RenderFunction.apply\n# Render our target\nimg = render(0, *args)\npyredner.imwrite(img.cpu(), \'results/test_shadow_light/target.exr\')\npyredner.imwrite(img.cpu(), \'results/test_shadow_light/target.png\')\ntarget = pyredner.imread(\'results/test_shadow_light/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda(device = pyredner.get_device())\n\n# Perturb the scene, this is our initial guess\nlight_translation = torch.tensor([-0.4, -0.4, -0.4],\n    device = pyredner.get_device(), requires_grad=True)\nshape_light.vertices = light_vertices + light_translation\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\n# Render the initial guess\nimg = render(1, *args)\npyredner.imwrite(img.cpu(), \'results/test_shadow_light/init.png\')\ndiff = torch.abs(target - img)\npyredner.imwrite(diff.cpu(), \'results/test_shadow_light/init_diff.png\')\n\n# Optimize for blocker vertices\noptimizer = torch.optim.Adam([light_translation], lr=1e-2)\nfor t in range(200):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    # Forward pass: render the image\n    shape_light.vertices = light_vertices + light_translation\n    args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4,\n        max_bounces = 1)\n    img = render(t+1, *args)\n    pyredner.imwrite(img.cpu(), \'results/test_shadow_light/iter_{}.png\'.format(t))\n    loss = (img - target).pow(2).sum()\n    print(\'loss:\', loss.item())\n\n    loss.backward()\n    print(\'grad:\', light_translation.grad)\n\n    optimizer.step()\n    print(\'light_translation:\', light_translation)\n\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\nimg = render(202, *args)\npyredner.imwrite(img.cpu(), \'results/test_shadow_light/final.exr\')\npyredner.imwrite(img.cpu(), \'results/test_shadow_light/final.png\')\npyredner.imwrite(torch.abs(target - img).cpu(), \'results/test_shadow_light/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_shadow_light/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_shadow_light/out.mp4""])\n'"
tests/test_shadow_receiver.py,0,"b'import pyredner\nimport numpy as np\nimport torch\n\n# Optimize four vertices of a shadow receiver\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\n# Set up the scene using Pytorch tensor\nposition = torch.tensor([0.0, 2.0, -5.0])\nlook_at = torch.tensor([0.0, 0.0, 0.0])\nup = torch.tensor([0.0, 1.0, 0.0])\nfov = torch.tensor([45.0])\nclip_near = 1e-2\n\nresolution = (256, 256)\ncam = pyredner.Camera(position = position,\n                     look_at = look_at,\n                     up = up,\n                     fov = fov,\n                     clip_near = clip_near,\n                     resolution = resolution)\n\nmat_grey = pyredner.Material(\\\n    diffuse_reflectance = torch.tensor([0.5, 0.5, 0.5],\n    device = pyredner.get_device()))\nmat_black = pyredner.Material(\\\n    diffuse_reflectance = torch.tensor([0.0, 0.0, 0.0],\n    device = pyredner.get_device()))\nmaterials = [mat_grey, mat_black]\n\nfloor_vertices = torch.tensor([[-2.0,0.0,-2.0],[-2.0,0.0,2.0],[2.0,0.0,-2.0],[2.0,0.0,2.0]],\n\tdevice = pyredner.get_device())\nfloor_indices = torch.tensor([[0,1,2], [1,3,2]],\n    device = pyredner.get_device(), dtype = torch.int32)\nshape_floor = pyredner.Shape(floor_vertices, floor_indices, 0)\nblocker_vertices = torch.tensor(\\\n    [[-0.5,3.0,-0.5],[-0.5,3.0,0.5],[0.5,3.0,-0.5],[0.5,3.0,0.5]],\n    device = pyredner.get_device())\nblocker_indices = torch.tensor([[0,1,2], [1,3,2]],\n    device = pyredner.get_device(), dtype = torch.int32)\nshape_blocker = pyredner.Shape(blocker_vertices, blocker_indices, 0)\nlight_vertices = torch.tensor(\\\n    [[-0.1,5,-0.1],[-0.1,5,0.1],[0.1,5,-0.1],[0.1,5,0.1]],\n    device = pyredner.get_device())\nlight_indices = torch.tensor([[0,2,1], [1,2,3]],\n    device = pyredner.get_device(), dtype = torch.int32)\nshape_light = pyredner.Shape(light_vertices, light_indices, 1)\nshapes = [shape_floor, shape_blocker, shape_light]\nlight_intensity = torch.tensor([1000.0, 1000.0, 1000.0])\n# The first argument is the shape id of the light\nlight = pyredner.AreaLight(2, light_intensity)\narea_lights = [light]\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\n\n# Alias of the render function\nrender = pyredner.RenderFunction.apply\n# Render our target\nimg = render(0, *args)\npyredner.imwrite(img.cpu(), \'results/test_shadow_receiver/target.exr\')\npyredner.imwrite(img.cpu(), \'results/test_shadow_receiver/target.png\')\ntarget = pyredner.imread(\'results/test_shadow_receiver/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda(device = pyredner.get_device())\n\n# Perturb the scene, this is our initial guess\nshape_floor.vertices = torch.tensor(\\\n\t[[-2.0,-0.2,-2.0],[-2.0,-0.2,2.0],[2.0,-0.2,-2.0],[2.0,-0.2,2.0]],\n\tdevice = pyredner.get_device(),\n    requires_grad = True)\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\n# Render the initial guess\nimg = render(1, *args)\npyredner.imwrite(img.cpu(), \'results/test_shadow_receiver/init.png\')\ndiff = torch.abs(target - img)\npyredner.imwrite(diff.cpu(), \'results/test_shadow_receiver/init_diff.png\')\n\n# Optimize for blocker vertices\noptimizer = torch.optim.Adam([shape_floor.vertices], lr=1e-2)\nfor t in range(200):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    # Forward pass: render the image\n    args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4,\n        max_bounces = 1)\n    img = render(t+1, *args)\n    pyredner.imwrite(img.cpu(), \'results/test_shadow_receiver/iter_{}.png\'.format(t))\n    loss = (img - target).pow(2).sum()\n    print(\'loss:\', loss.item())\n\n    loss.backward()\n    print(\'grad:\', shape_floor.vertices.grad)\n\n    optimizer.step()\n    print(\'vertices:\', shape_floor.vertices)\n\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\nimg = render(202, *args)\npyredner.imwrite(img.cpu(), \'results/test_shadow_receiver/final.exr\')\npyredner.imwrite(img.cpu(), \'results/test_shadow_receiver/final.png\')\npyredner.imwrite(torch.abs(target - img).cpu(), \'results/test_shadow_receiver/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_shadow_receiver/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_shadow_receiver/out.mp4""])\n'"
tests/test_single_triangle.py,0,"b'import pyredner\nimport numpy as np\nimport torch\n\n# Optimize three vertices of a single triangle\n# We first render a target image, then perturb the three vertices and optimize\n# to match the target.\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\n# Set up the pyredner scene for rendering:\n\n# First, we set up the camera.\n# redner assumes all the camera variables live in CPU memory,\n# so you should allocate torch tensors in CPU\ncam = pyredner.Camera(position = torch.tensor([0.0, 0.0, -5.0]),\n                      look_at = torch.tensor([0.0, 0.0, 0.0]),\n                      up = torch.tensor([0.0, 1.0, 0.0]),\n                      fov = torch.tensor([45.0]), # in degree\n                      clip_near = 1e-2, # needs to > 0\n                      resolution = (256, 256),\n                      fisheye = False)\n\n# Next, we setup the materials for the scene.\n# All materials in the scene are stored in a Python list,\n# the index of a material in the list is its material id.\n# Our simple scene only has a single grey material with reflectance 0.5.\n# If you are using GPU, make sure to copy the reflectance to GPU memory.\nmat_grey = pyredner.Material(\\\n    diffuse_reflectance = \\\n        torch.tensor([0.5, 0.5, 0.5], device = pyredner.get_device()))\n# The material list of the scene\nmaterials = [mat_grey]\n\n# Next, we setup the geometry for the scene.\n# 3D objects in redner are called ""Shape"".\n# All shapes in the scene are stored in a Python list,\n# the index of a shape in the list is its shape id.\n# Right now, a shape is always a triangle mesh, which has a list of\n# triangle vertices and a list of triangle indices.\n# The vertices are a Nx3 torch float tensor,\n# and the indices are a Mx3 torch integer tensor.\n# Optionally, for each vertex you can specify its UV coordinate for texture mapping,\n# and a normal for Phong interpolation.\n# Each shape also needs to be assigned a material using material id,\n# which is the index of the material in the material array.\n# If you are using GPU, make sure to copy all tensors of the shape to GPU memory.\nshape_triangle = pyredner.Shape(\\\n    vertices = torch.tensor([[-1.7, 1.0, 0.0], [1.0, 1.0, 0.0], [-0.5, -1.0, 0.0]],\n        device = pyredner.get_device()),\n    indices = torch.tensor([[0, 1, 2]], dtype = torch.int32,\n        device = pyredner.get_device()),\n    uvs = None,\n    normals = None,\n    material_id = 0)\n# Merely having a single triangle is not enough for physically-based rendering.\n# We need to have a light source. Here we setup the shape of a quad area light source,\n# similary to the previous triangle.\nshape_light = pyredner.Shape(\\\n    vertices = torch.tensor([[-1.0, -1.0, -7.0],\n                             [ 1.0, -1.0, -7.0],\n                             [-1.0,  1.0, -7.0],\n                             [ 1.0,  1.0, -7.0]], device = pyredner.get_device()),\n    indices = torch.tensor([[0, 1, 2],[1, 3, 2]],\n        dtype = torch.int32, device = pyredner.get_device()),\n    uvs = None,\n    normals = None,\n    material_id = 0)\n# The shape list of the scene\nshapes = [shape_triangle, shape_light]\n\n# Now we assign some of the shapes in the scene as light sources.\n# Again, all the area light sources in the scene are stored in a Python list.\n# Each area light is attached to a shape using shape id, additionally we need to\n# assign the intensity of the light, which is a length 3 float tensor in CPU. \nlight = pyredner.AreaLight(shape_id = 1, \n                           intensity = torch.tensor([20.0,20.0,20.0]))\narea_lights = [light]\n# Finally we construct our scene using all the variables we setup previously.\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\n# All PyTorch functions take a flat array of PyTorch tensors as input,\n# therefore we need to serialize the scene into an array. The following\n# function is doing this. We also specify how many Monte Carlo samples we want to \n# use per pixel and the number of bounces for indirect illumination here\n# (one bounce means only direct illumination).\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\n\n# Render the scene as our target image.\n# To render the scene, we use our custom PyTorch function in pyredner/render_pytorch.py\n# First setup the alias of the render function\nrender = pyredner.RenderFunction.apply\n# Render. The first argument is the seed for RNG in the renderer.\nimg = render(0, *scene_args)\n# Save the images.\n# The output image is in the GPU memory if you are using GPU.\npyredner.imwrite(img.cpu(), \'results/test_single_triangle/target.exr\')\npyredner.imwrite(img.cpu(), \'results/test_single_triangle/target.png\')\n# Read the target image we just saved.\ntarget = pyredner.imread(\'results/test_single_triangle/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda(device = pyredner.get_device())\n\n# Perturb the scene, this is our initial guess.\nshape_triangle.vertices = torch.tensor(\\\n    [[-2.0,1.5,0.3], [0.9,1.2,-0.3], [-0.4,-1.4,0.2]],\n    device = pyredner.get_device(),\n    requires_grad = True) # Set requires_grad to True since we want to optimize this\n# We need to serialize the scene again to get the new arguments.\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\n# Render the initial guess.\nimg = render(1, *scene_args)\n# Save the images.\npyredner.imwrite(img.cpu(), \'results/test_single_triangle/init.png\')\n# Compute the difference and save the images.\ndiff = torch.abs(target - img)\npyredner.imwrite(diff.cpu(), \'results/test_single_triangle/init_diff.png\')\n\n# Optimize for triangle vertices.\noptimizer = torch.optim.Adam([shape_triangle.vertices], lr=5e-2)\n# Run 200 Adam iterations.\nfor t in range(200):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    # Forward pass: render the image.\n    scene_args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4, # We use less samples in the Adam loop.\n        max_bounces = 1)\n    # Important to use a different seed every iteration, otherwise the result\n    # would be biased.\n    img = render(t+1, *scene_args)\n    # Save the intermediate render.\n    pyredner.imwrite(img.cpu(), \'results/test_single_triangle/iter_{}.png\'.format(t))\n    # Compute the loss function. Here it is L2.\n    loss = (img - target).pow(2).sum()\n    print(\'loss:\', loss.item())\n\n    # Backpropagate the gradients.\n    loss.backward()\n    # Print the gradients of the three vertices.\n    print(\'grad:\', shape_triangle.vertices.grad)\n\n    # Take a gradient descent step.\n    optimizer.step()\n    # Print the current three vertices.\n    print(\'vertices:\', shape_triangle.vertices)\n\n# Render the final result.\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\nimg = render(202, *scene_args)\n# Save the images and differences.\npyredner.imwrite(img.cpu(), \'results/test_single_triangle/final.exr\')\npyredner.imwrite(img.cpu(), \'results/test_single_triangle/final.png\')\npyredner.imwrite(torch.abs(target - img).cpu(), \'results/test_single_triangle/final_diff.png\')\n\n# Convert the intermediate renderings to a video.\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_single_triangle/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_single_triangle/out.mp4""])\n'"
tests/test_single_triangle_background.py,0,"b'import pyredner\nimport redner\nimport numpy as np\nimport torch\nimport skimage.transform\n\n# Optimize three vertices of a single triangle, with a SIGGRAPH logo background\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\n# Set up the pyredner scene for rendering:\n\n# Setup camera\ncam = pyredner.Camera(position = torch.tensor([0.0, 0.0, -5.0]),\n                      look_at = torch.tensor([0.0, 0.0, 0.0]),\n                      up = torch.tensor([0.0, 1.0, 0.0]),\n                      fov = torch.tensor([45.0]), # in degree\n                      clip_near = 1e-2, # needs to > 0\n                      resolution = (256, 256),\n                      fisheye = False)\n\n# Setup materials\nmat_grey = pyredner.Material(\\\n    diffuse_reflectance = \\\n        torch.tensor([0.5, 0.5, 0.5], device = pyredner.get_device()))\n# The material list of the scene\nmaterials = [mat_grey]\n\n# Setup geometries\nshape_triangle = pyredner.Shape(\\\n    vertices = torch.tensor([[-1.7, 1.0, 0.0], [1.0, 1.0, 0.0], [-0.5, -1.0, 0.0]],\n        device = pyredner.get_device()),\n    indices = torch.tensor([[0, 1, 2]], dtype = torch.int32,\n        device = pyredner.get_device()),\n    uvs = None,\n    normals = None,\n    material_id = 0)\n# Setup light source shape\nshape_light = pyredner.Shape(\\\n    vertices = torch.tensor([[-1.0, -1.0, -7.0],\n                             [ 1.0, -1.0, -7.0],\n                             [-1.0,  1.0, -7.0],\n                             [ 1.0,  1.0, -7.0]], device = pyredner.get_device()),\n    indices = torch.tensor([[0, 1, 2],[1, 3, 2]],\n        dtype = torch.int32, device = pyredner.get_device()),\n    uvs = None,\n    normals = None,\n    material_id = 0)\n# The shape list of the scene\nshapes = [shape_triangle, shape_light]\n\n# Setup light source\nlight = pyredner.AreaLight(shape_id = 1, \n                           intensity = torch.tensor([20.0,20.0,20.0]))\narea_lights = [light]\n\n# Construct the scene\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\n# Serialize the scene\n# Here we specify the output channels as ""radiance"" and ""alpha""\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1,\n    channels = [redner.channels.radiance, redner.channels.alpha])\n\n# Render the scene as our target image.\nrender = pyredner.RenderFunction.apply\n# Render. The first argument is the seed for RNG in the renderer.\nimg = render(0, *scene_args)\n# Since we specified alpha as output channel, img has 4 channels now\n# We blend the image with a background image\nbackground = pyredner.imread(\'scenes/textures/siggraph.jpg\')\nbackground = torch.from_numpy(skimage.transform.resize(background.numpy(), (256, 256, 3)))\nif pyredner.get_use_gpu():\n    background = background.cuda(device = pyredner.get_device())\nbackground = background.type_as(img)\nimg = img[:, :, :3] * img[:, :, 3:4] + background * (1 - img[:, :, 3:4])\n\n# Save the images.\n# The output image is in the GPU memory if you are using GPU.\npyredner.imwrite(img.cpu(), \'results/test_single_triangle_background/target.exr\')\npyredner.imwrite(img.cpu(), \'results/test_single_triangle_background/target.png\')\n# Read the target image we just saved.\ntarget = pyredner.imread(\'results/test_single_triangle_background/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda(device = pyredner.get_device())\n\n# Perturb the scene, this is our initial guess.\nshape_triangle.vertices = torch.tensor(\\\n    [[-2.0,1.5,0.3], [0.9,1.2,-0.3], [-0.4,-1.4,0.2]],\n    device = pyredner.get_device(),\n    requires_grad = True)\n# We need to serialize the scene again to get the new arguments.\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1,\n    channels = [redner.channels.radiance, redner.channels.alpha])\n# Render the initial guess.\nimg = render(1, *scene_args)\n# Blend the image with a background image\nimg = img[:, :, :3] * img[:, :, 3:4] + background * (1 - img[:, :, 3:4])\n# Save the images.\npyredner.imwrite(img.cpu(), \'results/test_single_triangle_background/init.png\')\n# Compute the difference and save the images.\ndiff = torch.abs(target - img)\npyredner.imwrite(diff.cpu(), \'results/test_single_triangle_background/init_diff.png\')\n\n# Optimize for triangle vertices.\noptimizer = torch.optim.Adam([shape_triangle.vertices], lr=5e-2)\n# Run 300 Adam iterations.\nfor t in range(300):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    # Forward pass: render the image.\n    scene_args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4, # We use less samples in the Adam loop.\n        max_bounces = 1,\n        channels = [redner.channels.radiance, redner.channels.alpha])\n    # Important to use a different seed every iteration, otherwise the result\n    # would be biased.\n    img = render(t+1, *scene_args)\n    # Blend the image with a background image\n    img = img[:, :, :3] * img[:, :, 3:4] + background * (1 - img[:, :, 3:4])\n    # Save the intermediate render.\n    pyredner.imwrite(img.cpu(), \'results/test_single_triangle_background/iter_{}.png\'.format(t))\n    # Compute the loss function. Here it is L2.\n    loss = (img - target).pow(2).sum()\n    print(\'loss:\', loss.item())\n\n    # Backpropagate the gradients.\n    loss.backward()\n    # Print the gradients of the three vertices.\n    print(\'grad:\', shape_triangle.vertices.grad)\n\n    # Take a gradient descent step.\n    optimizer.step()\n    # Print the current three vertices.\n    print(\'vertices:\', shape_triangle.vertices)\n\n# Render the final result.\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1,\n    channels = [redner.channels.radiance, redner.channels.alpha])\nimg = render(302, *scene_args)\n# Blend the image with a background image\nimg = img[:, :, :3] * img[:, :, 3:4] + background * (1 - img[:, :, 3:4])\n# Save the images and differences.\npyredner.imwrite(img.cpu(), \'results/test_single_triangle_background/final.exr\')\npyredner.imwrite(img.cpu(), \'results/test_single_triangle_background/final.png\')\npyredner.imwrite(torch.abs(target - img).cpu(), \'results/test_single_triangle_background/final_diff.png\')\n\n# Convert the intermediate renderings to a video.\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_single_triangle_background/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_single_triangle_background/out.mp4""])\n'"
tests/test_single_triangle_camera.py,0,"b'import pyredner\nimport numpy as np\nimport torch\n\n# Optimize camera parameters of a single triangle rendering\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\n# Set up the scene using Pytorch tensor\nposition = torch.tensor([0.0, 0.0, -5.0])\nlook_at = torch.tensor([0.0, 0.0, 0.0])\nup = torch.tensor([0.0, 1.0, 0.0])\nfov = torch.tensor([45.0])\nclip_near = 1e-2\n\nresolution = (256, 256)\ncam = pyredner.Camera(position = position,\n                      look_at = look_at,\n                      up = up,\n                      fov = fov,\n                      clip_near = clip_near,\n                      resolution = resolution)\n\nmat_grey = pyredner.Material(\\\n    diffuse_reflectance = torch.tensor([0.5, 0.5, 0.5],\n    device = pyredner.get_device()))\nmaterials = [mat_grey]\nvertices = torch.tensor([[-1.7,1.0,0.0], [1.0,1.0,0.0], [-0.5,-1.0,0.0]],\n                        device = pyredner.get_device())\nindices = torch.tensor([[0, 1, 2]], dtype = torch.int32,\n                       device = pyredner.get_device())\nshape_triangle = pyredner.Shape(vertices, indices, 0)\nlight_vertices = torch.tensor([[-1.0,-1.0,-9.0],[1.0,-1.0,-9.0],[-1.0,1.0,-9.0],[1.0,1.0,-9.0]],\n                              device = pyredner.get_device())\nlight_indices = torch.tensor([[0,1,2],[1,3,2]], dtype = torch.int32,\n                             device = pyredner.get_device())\nshape_light = pyredner.Shape(light_vertices, light_indices, 0)\nshapes = [shape_triangle, shape_light]\nlight_intensity = torch.tensor([30.0,30.0,30.0])\nlight = pyredner.AreaLight(1, light_intensity)\narea_lights = [light]\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\n\n# Alias of the render function\nrender = pyredner.RenderFunction.apply\n# Render our target\nimg = render(0, *args)\npyredner.imwrite(img.cpu(), \'results/test_single_triangle_camera/target.exr\')\npyredner.imwrite(img.cpu(), \'results/test_single_triangle_camera/target.png\')\ntarget = pyredner.imread(\'results/test_single_triangle_camera/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda(device = pyredner.get_device())\n\n# Perturb the scene, this is our initial guess\nposition = torch.tensor([0.0,  0.0, -3.0], requires_grad = True)\nlook_at = torch.tensor([-0.5, -0.5,  0.0], requires_grad = True)\nscene.camera = pyredner.Camera(position = position,\n                               look_at = look_at,\n                               up = up,\n                               fov = fov,\n                               clip_near = clip_near,\n                               resolution = resolution)\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\n# Render the initial guess\nimg = render(1, *args)\npyredner.imwrite(img.cpu(), \'results/test_single_triangle_camera/init.png\')\ndiff = torch.abs(target - img)\npyredner.imwrite(diff.cpu(), \'results/test_single_triangle_camera/init_diff.png\')\n\n# Optimize for camera pose\noptimizer = torch.optim.Adam([position, look_at], lr=2e-2)\nfor t in range(200):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    # Need to rerun the Camera constructor for PyTorch autodiff to compute the derivatives\n    scene.camera = pyredner.Camera(position   = position,\n                                   look_at    = look_at,\n                                   up         = up,\n                                   fov        = fov,\n                                   clip_near  = clip_near,\n                                   resolution = resolution)\n    args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4,\n        max_bounces = 1)\n    img = render(t+1, *args)\n    pyredner.imwrite(img.cpu(), \'results/test_single_triangle_camera/iter_{}.png\'.format(t))\n    loss = (img - target).pow(2).sum()\n    print(\'loss:\', loss.item())\n\n    loss.backward()\n    print(\'position.grad:\', position.grad)\n    print(\'look_at.grad:\', look_at.grad)\n\n    optimizer.step()\n    print(\'position:\', position)\n    print(\'look_at:\', look_at)\n\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\nimg = render(202, *args)\npyredner.imwrite(img.cpu(), \'results/test_single_triangle_camera/final.exr\')\npyredner.imwrite(img.cpu(), \'results/test_single_triangle_camera/final.png\')\npyredner.imwrite(torch.abs(target - img).cpu(), \'results/test_single_triangle_camera/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_single_triangle_camera/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_single_triangle_camera/out.mp4""])\n'"
tests/test_single_triangle_camera_fisheye.py,0,"b'import pyredner\nimport numpy as np\nimport torch\n\n# Optimize fisheye camera parameters of a single triangle rendering\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\n# Set up the scene using Pytorch tensor\nposition = torch.tensor([0.0, 0.0, -1.0])\nlook_at = torch.tensor([0.0, 0.0, 0.0])\nup = torch.tensor([0.0, 1.0, 0.0])\nfov = torch.tensor([45.0])\nclip_near = 1e-2\n\nresolution = (256, 256)\ncam = pyredner.Camera(position = position,\n                      look_at = look_at,\n                      up = up,\n                      fov = fov,\n                      clip_near = clip_near,\n                      resolution = resolution,\n                      fisheye = True)\n\nmat_grey = pyredner.Material(\\\n    diffuse_reflectance = torch.tensor([0.5, 0.5, 0.5],\n                                       device = pyredner.get_device()))\nmaterials = [mat_grey]\nvertices = torch.tensor([[-1.7,1.0,0.0], [1.0,1.0,0.0], [-0.5,-1.0,0.0]], device = pyredner.get_device())\nindices = torch.tensor([[0, 1, 2]], dtype = torch.int32, device = pyredner.get_device())\nshape_triangle = pyredner.Shape(vertices, indices, 0)\nlight_vertices = torch.tensor([[-1.0,-1.0,-9.0],[1.0,-1.0,-9.0],[-1.0,1.0,-9.0],[1.0,1.0,-9.0]],\n                              device = pyredner.get_device())\nlight_indices = torch.tensor([[0,1,2],[1,3,2]], dtype = torch.int32, device = pyredner.get_device())\nshape_light = pyredner.Shape(light_vertices, light_indices, 0)\nshapes = [shape_triangle, shape_light]\nlight_intensity = torch.tensor([30.0,30.0,30.0])\nlight = pyredner.AreaLight(1, light_intensity)\narea_lights = [light]\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\n\n# Alias of the render function\nrender = pyredner.RenderFunction.apply\n# Render our target\nimg = render(0, *args)\npyredner.imwrite(img.cpu(), \'results/test_single_triangle_camera_fisheye/target.exr\')\npyredner.imwrite(img.cpu(), \'results/test_single_triangle_camera_fisheye/target.png\')\ntarget = pyredner.imread(\'results/test_single_triangle_camera_fisheye/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda(device = pyredner.get_device())\n\n# Perturb the scene, this is our initial guess\nposition = torch.tensor([0.5, -0.5, -3.0], requires_grad = True)\nscene.camera = pyredner.Camera(position = position,\n                               look_at = look_at,\n                               up = up,\n                               fov = fov,\n                               clip_near = clip_near,\n                               resolution = resolution,\n                               fisheye = True)\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\n# Render the initial guess\nimg = render(1, *args)\npyredner.imwrite(img.cpu(), \'results/test_single_triangle_camera_fisheye/init.png\')\ndiff = torch.abs(target - img)\npyredner.imwrite(diff.cpu(), \'results/test_single_triangle_camera_fisheye/init_diff.png\')\n\n# Optimize for camera pose\noptimizer = torch.optim.Adam([position], lr=2e-2)\nfor t in range(200):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    # Need to rerun the Camera constructor for PyTorch autodiff to compute the derivatives\n    scene.camera = pyredner.Camera(position   = position,\n                                   look_at    = look_at,\n                                   up         = up,\n                                   fov        = fov,\n                                   clip_near  = clip_near,\n                                   resolution = resolution,\n                                   fisheye    = True)\n    args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4,\n        max_bounces = 1)\n    img = render(t+1, *args)\n    pyredner.imwrite(img.cpu(), \'results/test_single_triangle_camera_fisheye/iter_{}.png\'.format(t))\n    loss = (img - target).pow(2).sum()\n    print(\'loss:\', loss.item())\n\n    loss.backward()\n    print(\'position.grad:\', position.grad)\n\n    optimizer.step()\n    print(\'position:\', position)\n\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\nimg = render(202, *args)\npyredner.imwrite(img.cpu(), \'results/test_single_triangle_camera_fisheye/final.exr\')\npyredner.imwrite(img.cpu(), \'results/test_single_triangle_camera_fisheye/final.png\')\npyredner.imwrite(torch.abs(target - img).cpu(), \'results/test_single_triangle_camera_fisheye/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_single_triangle_camera_fisheye/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_single_triangle_camera_fisheye/out.mp4""])\n'"
tests/test_single_triangle_clipped.py,0,"b'import pyredner\nimport numpy as np\nimport torch\n\n# Optimize three vertices of a single triangle.\n# One of the vertices is behind the camera\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\n# Set up the scene using Pytorch tensor\nposition = torch.tensor([0.0, 0.0, -5.0])\nlook_at = torch.tensor([0.0, 0.0, 0.0])\nup = torch.tensor([0.0, 1.0, 0.0])\nfov = torch.tensor([45.0])\nclip_near = 1e-2\n\nresolution = (256, 256)\ncam = pyredner.Camera(position = position,\n                      look_at = look_at,\n                      up = up,\n                      fov = fov,\n                      clip_near = clip_near,\n                      resolution = resolution)\n\nmat_grey = pyredner.Material(\\\n    diffuse_reflectance = torch.tensor([0.5, 0.5, 0.5], device = pyredner.get_device()))\nmaterials = [mat_grey]\nvertices = torch.tensor([[-1.3,1.0,0.0], [1.0,1.0,0.0], [-0.5,-2.0,-7.0]], device = pyredner.get_device())\nindices = torch.tensor([[0, 1, 2]], dtype = torch.int32, device = pyredner.get_device())\nshape_triangle = pyredner.Shape(vertices, indices, 0)\nlight_vertices = torch.tensor([[-1.0,-1.0,-7.0],[1.0,-1.0,-7.0],[-1.0,1.0,-7.0],[1.0,1.0,-7.0]],\n                              device = pyredner.get_device())\nlight_indices = torch.tensor([[0,1,2],[1,3,2]], dtype = torch.int32, device = pyredner.get_device())\nshape_light = pyredner.Shape(light_vertices, light_indices, 0)\nshapes = [shape_triangle, shape_light]\nlight_intensity = torch.tensor([20.0,20.0,20.0])\nlight = pyredner.AreaLight(1, light_intensity)\narea_lights = [light]\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\n\n# Alias of the render function\nrender = pyredner.RenderFunction.apply\n# Render our target\nimg = render(0, *args)\npyredner.imwrite(img.cpu(), \'results/test_single_triangle_clipped/target.exr\')\npyredner.imwrite(img.cpu(), \'results/test_single_triangle_clipped/target.png\')\ntarget = pyredner.imread(\'results/test_single_triangle_clipped/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda(device = pyredner.get_device())\n\n# Perturb the scene, this is our initial guess\nshape_triangle.vertices = torch.tensor(\\\n    [[-1.0,1.5,0.3], [0.9,1.2,-0.3], [0.0,-3.0,-6.5]],\n    device = pyredner.get_device(),\n    requires_grad=True)\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\n# Render the initial guess\nimg = render(1, *args)\npyredner.imwrite(img.cpu(), \'results/test_single_triangle_clipped/init.png\')\ndiff = torch.abs(target - img)\npyredner.imwrite(diff.cpu(), \'results/test_single_triangle_clipped/init_diff.png\')\n\n# Optimize for triangle vertices\noptimizer = torch.optim.Adam([shape_triangle.vertices], lr=2e-2)\nfor t in range(200):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    # Forward pass: render the image\n    args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4,\n        max_bounces = 1)\n    img = render(t+1, *args)\n    pyredner.imwrite(img.cpu(), \'results/test_single_triangle_clipped/iter_{}.png\'.format(t))\n    loss = (img - target).pow(2).sum()\n    print(\'loss:\', loss.item())\n\n    loss.backward()\n    print(\'grad:\', shape_triangle.vertices.grad)\n\n    optimizer.step()\n    print(\'vertices:\', shape_triangle.vertices)\n\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\nimg = render(202, *args)\npyredner.imwrite(img.cpu(), \'results/test_single_triangle_clipped/final.exr\')\npyredner.imwrite(img.cpu(), \'results/test_single_triangle_clipped/final.png\')\npyredner.imwrite(torch.abs(target - img).cpu(), \'results/test_single_triangle_clipped/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_single_triangle_clipped/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_single_triangle_clipped/out.mp4""])\n'"
tests/test_sphere.py,0,"b""import torch\nimport pyredner\n\nvertices, indices, uvs, normals = pyredner.generate_sphere(64, 128)\nm = pyredner.Material(diffuse_reflectance = torch.tensor((0.5, 0.5, 0.5), device = pyredner.get_device()))\nobj = pyredner.Object(vertices = vertices,\n                      indices = indices,\n                      uvs = uvs,\n                      normals = normals,\n                      material = m)\ncam = pyredner.automatic_camera_placement([obj], resolution = (480, 640))\nscene = pyredner.Scene(objects = [obj], camera = cam)\n\nimg = pyredner.render_g_buffer(scene, channels = [pyredner.channels.uv, pyredner.channels.shading_normal])\nuv_img = torch.cat([img[:, :, :2], torch.zeros(480, 640, 1)], dim=2)\nnormal_img = img[:, :, 2:]\npyredner.imwrite(uv_img, 'results/test_sphere/uv.png')\npyredner.imwrite(normal_img, 'results/test_sphere/normal.png')\n"""
tests/test_svbrdf.py,0,"b'from sys import platform as sys_pf\nif sys_pf == \'darwin\':\n    import matplotlib\n    matplotlib.use(""TkAgg"")\nimport matplotlib.pyplot as plt\nimport pyredner\nimport numpy as np\nimport torch\n\n# Optimize texels of a textured patch\n\n# Perlin noise code taken from Stackoverflow\n# https://stackoverflow.com/questions/42147776/producing-2d-perlin-noise-with-numpy\ndef perlin(x, y, seed=0):\n    # permutation table\n    np.random.seed(seed)\n    p = np.arange(256, dtype=np.int32)\n    np.random.shuffle(p)\n    p = np.stack([p,p]).flatten()\n    # coordinates of the top-left\n    xi = x.astype(np.int32)\n    yi = y.astype(np.int32)\n    # internal coordinates\n    xf = (x - xi).astype(np.float32)\n    yf = (y - yi).astype(np.float32)\n    # fade factors\n    u = fade(xf)\n    v = fade(yf)\n    # noise components\n    n00 = gradient(p[p[xi]+yi],xf,yf)\n    n01 = gradient(p[p[xi]+yi+1],xf,yf-1)\n    n11 = gradient(p[p[xi+1]+yi+1],xf-1,yf-1)\n    n10 = gradient(p[p[xi+1]+yi],xf-1,yf)\n    # combine noises\n    x1 = lerp(n00, n10, u)\n    x2 = lerp(n01, n11, u)\n    return lerp(x1, x2, v)\n\ndef lerp(a,b,x):\n    return a + x * (b-a)\n\ndef fade(t):\n    return 6 * t**5 - 15 * t**4 + 10 * t**3\n\ndef gradient(h,x,y):\n    vectors = np.array([[0,1],[0,-1],[1,0],[-1,0]], dtype=np.float32)\n    g = vectors[h%4]\n    return g[:,:,0] * x + g[:,:,1] * y\n\nlin = np.linspace(0, 5, 256, endpoint=False, dtype=np.float32)\nx, y = np.meshgrid(lin, lin)\ndiffuse = perlin(x, y, seed=0)\ndiffuse = (diffuse - np.min(diffuse) + 1e-3) / (np.max(diffuse) - np.min(diffuse))\ndiffuse = torch.from_numpy(np.tile(np.reshape(diffuse, (256, 256, 1)), (1, 1, 3)))\nspecular = perlin(x, y, seed=1)\nspecular = (specular - np.min(specular) + 1e-3) / (np.max(specular) - np.min(specular))\nspecular = torch.from_numpy(np.tile(np.reshape(specular, (256, 256, 1)), (1, 1, 3)))\nroughness = perlin(x, y, seed=2)\nroughness = (roughness - np.min(roughness) + 1e-3) / (np.max(roughness) - np.min(roughness))\nroughness = torch.from_numpy(np.reshape(roughness, (256, 256, 1)))\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\n# Set up the scene using Pytorch tensor\nposition = torch.tensor([0.0, 0.0, -5.0])\nlook_at = torch.tensor([0.0, 0.0, 0.0])\nup = torch.tensor([0.0, 1.0, 0.0])\nfov = torch.tensor([45.0])\nclip_near = 1e-2\n\nresolution = (256, 256)\ncam = pyredner.Camera(position = position,\n                     look_at = look_at,\n                     up = up,\n                     fov = fov,\n                     clip_near = clip_near,\n                     resolution = resolution)\nif pyredner.get_use_gpu():\n    diffuse = diffuse.cuda(device = pyredner.get_device())\n    specular = specular.cuda(device = pyredner.get_device())\n    roughness = roughness.cuda(device = pyredner.get_device())\nmat_perlin = pyredner.Material(\\\n    diffuse_reflectance = diffuse,\n    specular_reflectance = specular,\n    roughness = roughness)\nmat_black = pyredner.Material(\\\n    diffuse_reflectance = torch.tensor([0.0, 0.0, 0.0], device = pyredner.get_device()))\nmaterials = [mat_perlin, mat_black]\nvertices = torch.tensor([[-1.5,-1.5,0.0], [-1.5,1.5,0.0], [1.5,-1.5,0.0], [1.5,1.5,0.0]],\n                        device = pyredner.get_device())\nindices = torch.tensor([[0, 1, 2], [1, 3, 2]], dtype = torch.int32,\n                       device = pyredner.get_device())\nuvs = torch.tensor([[0.05, 0.05], [0.05, 0.95], [0.95, 0.05], [0.95, 0.95]],\n\t\t\t\t   device = pyredner.get_device())\nshape_plane = pyredner.Shape(vertices = vertices,\n                             indices = indices,\n                             uvs = uvs,\n                             material_id = 0)\nlight_vertices = torch.tensor([[-1.0,-1.0,-7.0],[1.0,-1.0,-7.0],[-1.0,1.0,-7.0],[1.0,1.0,-7.0]],\n                              device = pyredner.get_device())\nlight_indices = torch.tensor([[0,1,2],[1,3,2]], dtype = torch.int32, device = pyredner.get_device())\nshape_light = pyredner.Shape(light_vertices, light_indices, 1)\nshapes = [shape_plane, shape_light]\nlight_intensity = torch.tensor([20.0, 20.0, 20.0])\n# The first argument is the shape id of the light\nlight = pyredner.AreaLight(1, light_intensity)\narea_lights = [light]\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\n\n# Alias of the render function\nrender = pyredner.RenderFunction.apply\n# Render our target\nimg = render(0, *args)\npyredner.imwrite(img.cpu(), \'results/test_svbrdf/target.exr\')\npyredner.imwrite(img.cpu(), \'results/test_svbrdf/target.png\')\ntarget = pyredner.imread(\'results/test_svbrdf/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda(device = pyredner.get_device())\n\n# Our initial guess is three gray textures \ndiffuse_tex = torch.tensor(\\\n    np.ones((256, 256, 3), dtype=np.float32) * 0.5,\n    requires_grad = True,\n    device = pyredner.get_device())\nspecular_tex = torch.tensor(\\\n    np.ones((256, 256, 3), dtype=np.float32) * 0.5,\n    requires_grad = True,\n    device = pyredner.get_device())\nroughness_tex = torch.tensor(\\\n    np.ones((256, 256, 1), dtype=np.float32) * 0.5,\n    requires_grad = True,\n    device = pyredner.get_device())\nmat_perlin.diffuse_reflectance = pyredner.Texture(diffuse_tex)\nmat_perlin.specular_reflectance = pyredner.Texture(specular_tex)\nmat_perlin.roughness = pyredner.Texture(roughness_tex)\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\n# Render the initial guess\nimg = render(1, *args)\npyredner.imwrite(img.cpu(), \'results/test_svbrdf/init.png\')\ndiff = torch.abs(target - img)\npyredner.imwrite(diff.cpu(), \'results/test_svbrdf/init_diff.png\')\n\n# Optimize for triangle vertices\noptimizer = torch.optim.Adam([diffuse_tex, specular_tex, roughness_tex], lr=1e-2)\nfor t in range(200):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    # Forward pass: render the image\n    # Need to rerun the mipmap generation for autodiff to flow through\n    mat_perlin.diffuse_reflectance = pyredner.Texture(diffuse_tex)\n    mat_perlin.specular_reflectance = pyredner.Texture(specular_tex)\n    mat_perlin.roughness = pyredner.Texture(roughness_tex)\n    args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4,\n        max_bounces = 1)\n    img = render(t+1, *args)\n    pyredner.imwrite(img.cpu(), \'results/test_svbrdf/iter_{}.png\'.format(t))\n    loss = (img - target).pow(2).sum()\n    print(\'loss:\', loss.item())\n\n    loss.backward()\n    optimizer.step()\n    diffuse_tex.data = diffuse_tex.data.clamp(0, 1)\n    specular_tex.data = specular_tex.data.clamp(0, 1)\n    roughness_tex.data = roughness_tex.data.clamp(1e-5, 1)\n\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\nimg = render(202, *args)\npyredner.imwrite(img.cpu(), \'results/test_svbrdf/final.exr\')\npyredner.imwrite(img.cpu(), \'results/test_svbrdf/final.png\')\npyredner.imwrite(torch.abs(target - img).cpu(), \'results/test_svbrdf/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_svbrdf/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_svbrdf/out.mp4""])\n'"
tests/test_teapot_normal_map.py,0,"b'import pyredner\nimport redner\nimport numpy as np\nimport torch\nimport skimage.transform\n\n# Optimize the normal map of a teapot\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\n# Set up the pyredner scene for rendering:\nmaterial_map, mesh_list, light_map = pyredner.load_obj(\'scenes/teapot.obj\')\nfor _, mesh in mesh_list:\n    mesh.normals = pyredner.compute_vertex_normal(mesh.vertices, mesh.indices)\n\n# Setup camera\ncam = pyredner.Camera(position = torch.tensor([0.0, 30.0, 200.0]),\n                      look_at = torch.tensor([0.0, 30.0, 0.0]),\n                      up = torch.tensor([0.0, 1.0, 0.0]),\n                      fov = torch.tensor([45.0]), # in degree\n                      clip_near = 1e-2, # needs to > 0\n                      resolution = (256, 256),\n                      fisheye = False)\n\n# Load normal map (downloaded from https://worldwidemuseum.wordpress.com/2013/01/13/carriage-plates-step-2/)\nnormal_map = pyredner.imread(\'scenes/brick_normal.jpg\', gamma=1.0)\nif pyredner.get_use_gpu():\n    normal_map = normal_map.cuda(device = pyredner.get_device())\nnormal_map = pyredner.Texture(normal_map,\n    uv_scale = torch.tensor([4.0, 4.0], device = pyredner.get_device()))\n\n# Setup materials\nmaterial_id_map = {}\nmaterials = []\ncount = 0\nfor key, value in material_map.items():\n    material_id_map[key] = count\n    count += 1\n    # assign normal map\n    value.normal_map = normal_map\n    materials.append(value)\n\n# Setup geometries\nshapes = []\nfor mtl_name, mesh in mesh_list:\n    shapes.append(pyredner.Shape(\\\n        vertices = mesh.vertices,\n        indices = mesh.indices,\n        uvs = mesh.uvs,\n        normals = mesh.normals,\n        material_id = material_id_map[mtl_name]))\n\n# Setup environment map\nenvmap = pyredner.imread(\'sunsky.exr\')\nif pyredner.get_use_gpu():\n    envmap = envmap.cuda(device = pyredner.get_device())\nenvmap = pyredner.EnvironmentMap(envmap)\n\n# Finally we construct our scene using all the variables we setup previously.\nscene = pyredner.Scene(cam, shapes, materials, area_lights = [], envmap = envmap)\n# Serialize the scene\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    sampler_type = redner.SamplerType.sobol,\n    max_bounces = 1)\n# Render the scene as our target image.\nrender = pyredner.RenderFunction.apply\n# Render. The first argument is the seed for RNG in the renderer.\nimg = render(0, *scene_args)\npyredner.imwrite(img.cpu(), \'results/test_teapot_normal_map/target.exr\')\npyredner.imwrite(img.cpu(), \'results/test_teapot_normal_map/target.png\')\ntarget = pyredner.imread(\'results/test_teapot_normal_map/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda(device = pyredner.get_device())\n\n# Generate initial guess -- set the normal map to (0.5, 0.5, 1) for all pixels\nnormal_map = torch.zeros(512, 512, 3, device = pyredner.get_device())\nnormal_map[:, :, 0:2] = 0.5\nnormal_map[:, :, 2] = 1\nnormal_map.requires_grad = True\nnormal_map_tex = pyredner.Texture(normal_map)\nfor m in materials:\n    m.normal_map = normal_map_tex\nscene = pyredner.Scene(cam, shapes, materials, area_lights = [], envmap = envmap)\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    sampler_type = redner.SamplerType.sobol,\n    max_bounces = 1)\n# Render the initial guess.\nimg = render(1, *scene_args)\npyredner.imwrite(img.cpu(), \'results/test_teapot_normal_map/init.png\')\n\n# Optimize for normal map.\noptimizer = torch.optim.Adam([normal_map], lr=1e-2)\n# Run 200 Adam iterations.\nfor t in range(200):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n\n    # Reassign the texture for differentiating mipmap construction\n    normal_map_tex = pyredner.Texture(normal_map)\n    for m in materials:\n        m.normal_map = normal_map_tex\n    scene = pyredner.Scene(cam, shapes, materials, area_lights = [], envmap = envmap)\n    scene_args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4,\n        sampler_type = redner.SamplerType.sobol,\n        max_bounces = 1)\n\n    img = render(t+1, *scene_args)\n    # Save the intermediate render.\n    pyredner.imwrite(img.cpu(), \'results/test_teapot_normal_map/iter_{}.png\'.format(t))\n    # Compute the loss function. Here it is L2.\n    loss = (img - target).pow(2).sum()\n    print(\'loss:\', loss.item())\n\n    # Backpropagate the gradients.\n    loss.backward()\n    # Take a gradient descent step.\n    optimizer.step()\n    # Save the normal map\n    pyredner.imwrite(normal_map.cpu(), \'results/test_teapot_normal_map/normal_{}.png\'.format(t), gamma=1.0)\n\n# Render the final result.\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    sampler_type = redner.SamplerType.sobol,\n    max_bounces = 1)\nimg = render(202, *scene_args)\n# Save the images and differences.\npyredner.imwrite(img.cpu(), \'results/test_teapot_normal_map/final.exr\')\npyredner.imwrite(img.cpu(), \'results/test_teapot_normal_map/final.png\')\npyredner.imwrite(torch.abs(target - img).cpu(), \'results/test_teapot_normal_map/final_diff.png\')\n\n# Convert the intermediate renderings to a video.\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_teapot_normal_map/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_teapot_normal_map/out.mp4""])\n'"
tests/test_teapot_reflectance.py,0,"b'import pyredner\nimport numpy as np\nimport torch\nimport scipy\nimport scipy.ndimage\n\n# Optimize for material parameters and camera pose\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\n# Load the scene from a Mitsuba scene file\nscene = pyredner.load_mitsuba(\'scenes/teapot.xml\')\n\n# The last material is the teapot material, set it to the target\nscene.materials[-1].diffuse_reflectance = \\\n    pyredner.Texture(torch.tensor([0.3, 0.2, 0.2], device = pyredner.get_device()))\nscene.materials[-1].specular_reflectance = \\\n    pyredner.Texture(torch.tensor([0.6, 0.6, 0.6], device = pyredner.get_device()))\nscene.materials[-1].roughness = \\\n    pyredner.Texture(torch.tensor([0.05], device = pyredner.get_device()))\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 1024,\n    max_bounces = 2)\n\n# Alias of the render function\nrender = pyredner.RenderFunction.apply\n# Render our target. The first argument is the seed for RNG in the renderer.\nimg = render(0, *args)\npyredner.imwrite(img.cpu(), \'results/test_teapot_reflectance/target.exr\')\npyredner.imwrite(img.cpu(), \'results/test_teapot_reflectance/target.png\')\ntarget = pyredner.imread(\'results/test_teapot_reflectance/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda(device = pyredner.get_device())\n\n# Perturb the scene, this is our initial guess\ncam = scene.camera\ncam_position = cam.position\ncam_translation = torch.tensor([-0.2, 0.2, -0.2], requires_grad = True)\ndiffuse_reflectance = torch.tensor([0.3, 0.3, 0.3],\n    device = pyredner.get_device(), requires_grad = True)\nspecular_reflectance = torch.tensor([0.5, 0.5, 0.5],\n    device = pyredner.get_device(), requires_grad = True)\nroughness = torch.tensor([0.2],\n    device = pyredner.get_device(), requires_grad = True)\nscene.materials[-1].diffuse_reflectance = pyredner.Texture(diffuse_reflectance)\nscene.materials[-1].specular_reflectance = pyredner.Texture(specular_reflectance)\nscene.materials[-1].roughness = pyredner.Texture(roughness)\nscene.camera = pyredner.Camera(position     = cam_position + cam_translation,\n                               look_at      = cam.look_at + cam_translation,\n                               up           = cam.up,\n                               fov          = cam.fov,\n                               clip_near    = cam.clip_near,\n                               resolution   = cam.resolution,\n                               fisheye      = False)\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 1024,\n    max_bounces = 2)\n# Render the initial guess\nimg = render(1, *args)\npyredner.imwrite(img.cpu(), \'results/test_teapot_reflectance/init.png\')\ndiff = torch.abs(target - img)\npyredner.imwrite(diff.cpu(), \'results/test_teapot_reflectance/init_diff.png\')\n\nlr_base = 1e-2\nlr = lr_base\noptimizer = torch.optim.Adam([diffuse_reflectance,\n                              specular_reflectance,\n                              roughness,\n                              cam_translation], lr=lr)\nnum_iteration = 400\nfor t in range(num_iteration):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    # Forward pass: render the image\n    # need to rerun Camera constructor for autodiff \n    scene.camera = pyredner.Camera(position   = cam_position + cam_translation,\n                                   look_at    = cam.look_at + cam_translation,\n                                   up         = cam.up,\n                                   fov        = cam.fov,\n                                   clip_near  = cam.clip_near,\n                                   resolution = cam.resolution,\n                                   fisheye    = False)\n    args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4,\n        max_bounces = 2)\n    img = render(t+1, *args)\n    pyredner.imwrite(img.cpu(), \'results/test_teapot_reflectance/iter_{}.png\'.format(t))\n    #loss = (img - target).pow(2).sum()\n\n    diff = img - target\n    dirac = np.zeros([7,7], dtype = np.float32)\n    dirac[3,3] = 1.0\n    dirac = torch.from_numpy(dirac)\n    f = np.zeros([3, 3, 7, 7], dtype = np.float32)\n    gf = scipy.ndimage.filters.gaussian_filter(dirac, 1.0)\n    f[0, 0, :, :] = gf\n    f[1, 1, :, :] = gf\n    f[2, 2, :, :] = gf\n    f = torch.from_numpy(f)\n    if pyredner.get_use_gpu():\n        f = f.cuda(device = pyredner.get_device())\n    m = torch.nn.AvgPool2d(2)\n    r = 256\n    diff_0 = (img - target).view(1, r, r, 3).permute(0, 3, 2, 1)\n    diff_1 = m(torch.nn.functional.conv2d(diff_0, f, padding=3))\n    diff_2 = m(torch.nn.functional.conv2d(diff_1, f, padding=3))\n    diff_3 = m(torch.nn.functional.conv2d(diff_2, f, padding=3))\n    diff_4 = m(torch.nn.functional.conv2d(diff_3, f, padding=3))\n    diff_5 = m(torch.nn.functional.conv2d(diff_4, f, padding=3))\n    loss = diff_0.pow(2).sum() / (r*r) + \\\n           diff_1.pow(2).sum() / ((r/2)*(r/2)) + \\\n           diff_2.pow(2).sum() / ((r/4)*(r/4)) + \\\n           diff_3.pow(2).sum() / ((r/8)*(r/8)) + \\\n           diff_4.pow(2).sum() / ((r/16)*(r/16)) + \\\n           diff_5.pow(2).sum() / ((r/32)*(r/32))\n\n    print(\'loss:\', loss.item())\n\n    loss.backward()\n    print(\'diffuse_reflectance.grad:\', diffuse_reflectance.grad)\n    print(\'specular_reflectance.grad:\', specular_reflectance.grad)\n    print(\'roughness.grad:\', roughness.grad)\n    print(\'cam_translation.grad:\', cam_translation.grad)\n\n    # HACK: gradient clipping to deal with outlier gradients\n    torch.nn.utils.clip_grad_norm_(roughness, 10)\n    torch.nn.utils.clip_grad_norm_(cam_translation, 10)\n\n    optimizer.step()\n\n    print(\'diffuse_reflectance:\', diffuse_reflectance)\n    print(\'specular_reflectance:\', specular_reflectance)\n    print(\'roughness:\', roughness)\n    print(\'cam_translation:\', cam_translation)\n\n    # Linearly reduce the learning rate\n    #lr = lr_base * float(num_iteration - t) / float(num_iteration)\n    #for param_group in optimizer.param_groups:\n    #    param_group[\'lr\'] = lr\n\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 1024,\n    max_bounces = 2)\nimg = render(num_iteration + 2, *args)\npyredner.imwrite(img.cpu(), \'results/test_teapot_reflectance/final.exr\')\npyredner.imwrite(img.cpu(), \'results/test_teapot_reflectance/final.png\')\npyredner.imwrite(torch.abs(target - img).cpu(), \'results/test_teapot_reflectance/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_teapot_reflectance/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_teapot_reflectance/out.mp4""])\n'"
tests/test_teapot_specular.py,0,"b'import pyredner\nimport numpy as np\nimport torch\n\n# Optimize for a textured plane in a specular reflection\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\n# Load the scene from a Mitsuba scene file\nscene = pyredner.load_mitsuba(\'scenes/teapot_specular.xml\')\n\n# The last material is the teapot material, set it to a specular material\nscene.materials[-1].diffuse_reflectance = \\\n    pyredner.Texture(torch.tensor([0.15, 0.2, 0.15], device = pyredner.get_device()))\nscene.materials[-1].specular_reflectance = \\\n    pyredner.Texture(torch.tensor([0.8, 0.8, 0.8], device = pyredner.get_device()))\nscene.materials[-1].roughness = \\\n    pyredner.Texture(torch.tensor([0.0001], device = pyredner.get_device()))\nargs=pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 512,\n    max_bounces = 2)\n\nrender = pyredner.RenderFunction.apply\n# Render our target. The first argument is the seed for RNG in the renderer.\nimg = render(0, *args)\npyredner.imwrite(img.cpu(), \'results/test_teapot_specular/target.exr\')\npyredner.imwrite(img.cpu(), \'results/test_teapot_specular/target.png\')\ntarget = pyredner.imread(\'results/test_teapot_specular/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda(device = pyredner.get_device())\n\n# Perturb the scene, this is our initial guess\n# We perturb the last shape, which is the SIGGRAPH logo\nref_pos = scene.shapes[-1].vertices\ntranslation = torch.tensor([20.0, 0.0, 2.0], device = pyredner.get_device(), requires_grad=True)\nscene.shapes[-1].vertices = ref_pos + translation\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 512,\n    max_bounces = 2)\n# Render the initial guess\nimg = render(1, *args)\npyredner.imwrite(img.cpu(), \'results/test_teapot_specular/init.png\')\ndiff = torch.abs(target - img)\npyredner.imwrite(diff.cpu(), \'results/test_teapot_specular/init_diff.png\')\n\nlr = 0.5\noptimizer = torch.optim.Adam([translation], lr=lr)\nnum_iteration = 400\nfor t in range(num_iteration):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n\n    scene.shapes[-1].vertices = ref_pos + translation\n    args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4,\n        max_bounces = 2)\n    img = render(t+1, *args)\n    pyredner.imwrite(img.cpu(), \'results/test_teapot_specular/iter_{}.png\'.format(t))\n    loss = (img - target).pow(2).sum()\n    print(\'loss:\', loss.item())\n\n    loss.backward()\n    print(\'translation.grad:\', translation.grad)\n\n    torch.nn.utils.clip_grad_norm_(translation, 10)\n\n    optimizer.step()\n    print(\'translation:\', translation)\n\n    # Linearly reduce the learning rate\n    lr = 0.5 * float(num_iteration - t) / float(num_iteration)\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\nscene.shapes[-1].vertices = ref_pos + translation\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 512,\n    max_bounces = 2)\nimg = render(num_iteration + 2, *args)\npyredner.imwrite(img.cpu(), \'results/test_teapot_specular/final.exr\')\npyredner.imwrite(img.cpu(), \'results/test_teapot_specular/final.png\')\npyredner.imwrite(torch.abs(target - img).cpu(), \'results/test_teapot_specular/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_teapot_specular/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_teapot_specular/out.mp4""])\n'"
tests/test_texture.py,0,"b'import pyredner\nimport torch\n\n# Optimize four vertices of a textured patch\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\n# Set up the scene using Pytorch tensor\nposition = torch.tensor([0.0, 0.0, -5.0])\nlook_at = torch.tensor([0.0, 0.0, 0.0])\nup = torch.tensor([0.0, 1.0, 0.0])\nfov = torch.tensor([45.0])\nclip_near = 1e-2\n\nresolution = (256, 256)\ncam = pyredner.Camera(position = position,\n                     look_at = look_at,\n                     up = up,\n                     fov = fov,\n                     clip_near = clip_near,\n                     resolution = resolution)\n\ncheckerboard_texture = pyredner.imread(\'checkerboard.exr\')\nif pyredner.get_use_gpu():\n    checkerboard_texture = checkerboard_texture.cuda(device = pyredner.get_device())\n\nmat_checkerboard = pyredner.Material(\\\n    diffuse_reflectance = checkerboard_texture)\nmat_black = pyredner.Material(\\\n    diffuse_reflectance = torch.tensor([0.0, 0.0, 0.0],\n    device = pyredner.get_device()))\nmaterials = [mat_checkerboard, mat_black]\nvertices = torch.tensor([[-1.0,-1.0,0.0], [-1.0,1.0,0.0], [1.0,-1.0,0.0], [1.0,1.0,0.0]],\n                        device = pyredner.get_device())\nindices = torch.tensor([[0, 1, 2], [1, 3, 2]], dtype = torch.int32,\n                       device = pyredner.get_device())\nuvs = torch.tensor([[0.05, 0.05], [0.05, 0.95], [0.95, 0.05], [0.95, 0.95]],\n                   device = pyredner.get_device())\nshape_plane = pyredner.Shape(vertices = vertices,\n                             indices = indices,\n                             uvs = uvs,\n                             material_id = 0)\nlight_vertices = torch.tensor([[-1.0,-1.0,-7.0],[1.0,-1.0,-7.0],[-1.0,1.0,-7.0],[1.0,1.0,-7.0]],\n                              device = pyredner.get_device())\nlight_indices = torch.tensor([[0,1,2],[1,3,2]], dtype = torch.int32, device = pyredner.get_device())\nshape_light = pyredner.Shape(light_vertices, light_indices, 1)\nshapes = [shape_plane, shape_light]\nlight_intensity = torch.tensor([20.0, 20.0, 20.0])\n# The first argument is the shape id of the light\nlight = pyredner.AreaLight(1, light_intensity)\narea_lights = [light]\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\n\n# Alias of the render function\nrender = pyredner.RenderFunction.apply\n# Render our target\nimg = render(0, *args)\npyredner.imwrite(img.cpu(), \'results/test_texture/target.exr\')\npyredner.imwrite(img.cpu(), \'results/test_texture/target.png\')\ntarget = pyredner.imread(\'results/test_texture/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda(device = pyredner.get_device())\n\n# Perturb the scene, this is our initial guess\nshape_plane.vertices = torch.tensor(\\\n    [[-1.1,-1.2,0.0], [-1.3,1.1,0.0], [1.1,-1.1,0.0], [0.8,1.2,0.0]],\n    device = pyredner.get_device(),\n    requires_grad=True)\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\n# Render the initial guess\nimg = render(1, *args)\npyredner.imwrite(img.cpu(), \'results/test_texture/init.png\')\ndiff = torch.abs(target - img)\npyredner.imwrite(diff.cpu(), \'results/test_texture/init_diff.png\')\n\n# Optimize for triangle vertices\noptimizer = torch.optim.Adam([shape_plane.vertices], lr=5e-2)\nfor t in range(200):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    # Forward pass: render the image\n    args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4,\n        max_bounces = 1)\n    img = render(t+1, *args)\n    pyredner.imwrite(img.cpu(), \'results/test_texture/iter_{}.png\'.format(t))\n    loss = (img - target).pow(2).sum()\n    print(\'loss:\', loss.item())\n\n    loss.backward()\n    print(\'grad:\', shape_plane.vertices.grad)\n\n    optimizer.step()\n    print(\'vertices:\', shape_plane.vertices)\n\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\nimg = render(202, *args)\npyredner.imwrite(img.cpu(), \'results/test_texture/final.exr\')\npyredner.imwrite(img.cpu(), \'results/test_texture/final.png\')\npyredner.imwrite(torch.abs(target - img).cpu(), \'results/test_texture/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_texture/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_texture/out.mp4""])\n'"
tests/test_two_triangles.py,0,"b'import pyredner\nimport numpy as np\nimport torch\n\n# Optimize six vertices of a two triangles\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\n# Set up the scene using Pytorch tensor\nposition = torch.tensor([0.0, 0.0, -5.0])\nlook_at = torch.tensor([0.0, 0.0, 0.0])\nup = torch.tensor([0.0, 1.0, 0.0])\nfov = torch.tensor([45.0])\nclip_near = 1e-2\n\nresolution = (256, 256)\ncam = pyredner.Camera(position = position,\n                      look_at = look_at,\n                      up = up,\n                      fov = fov,\n                      clip_near = clip_near,\n                      resolution = resolution)\n\nmat_green = pyredner.Material(\\\n    diffuse_reflectance = torch.tensor([0.35, 0.75, 0.35],\n    device = pyredner.get_device()))\nmat_red = pyredner.Material(\\\n    diffuse_reflectance = torch.tensor([0.75, 0.35, 0.35],\n    device = pyredner.get_device()))\nmat_black = pyredner.Material(\\\n    diffuse_reflectance = torch.tensor([0.0, 0.0, 0.0],\n    device = pyredner.get_device()))\nmaterials = [mat_green,mat_red,mat_black]\ntri0_vertices = torch.tensor(\\\n    [[-1.7,1.0,0.0], [1.0,1.0,0.0], [-0.5,-1.0,0.0]],\n    device = pyredner.get_device())\ntri1_vertices = torch.tensor(\\\n    [[-1.0,1.5,1.0], [0.2,1.5,1.0], [0.2,-1.5,1.0]],\n    device = pyredner.get_device())\ntri0_indices = torch.tensor([[0, 1, 2]], dtype = torch.int32, device = pyredner.get_device())\ntri1_indices = torch.tensor([[0, 1, 2]], dtype = torch.int32, device = pyredner.get_device())\nshape_tri0 = pyredner.Shape(tri0_vertices, tri0_indices, 0)\nshape_tri1 = pyredner.Shape(tri1_vertices, tri1_indices, 1)\nlight_vertices = torch.tensor(\\\n    [[-1.0,-1.0,-7.0],[1.0,-1.0,-7.0],[-1.0,1.0,-7.0],[1.0,1.0,-7.0]],\n    device = pyredner.get_device())\nlight_indices = torch.tensor([[0,1,2],[1,3,2]], dtype = torch.int32, device = pyredner.get_device())\nshape_light = pyredner.Shape(light_vertices, light_indices, 2)\nshapes = [shape_tri0, shape_tri1, shape_light]\nlight_intensity = torch.tensor([20.0,20.0,20.0])\n# The first argument is the shape id of the light\nlight = pyredner.AreaLight(2, light_intensity)\narea_lights = [light]\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\n\n# Alias of the render function\nrender = pyredner.RenderFunction.apply\n# Render our target\nimg = render(0, *args)\npyredner.imwrite(img.cpu(), \'results/test_two_triangles/target.exr\')\npyredner.imwrite(img.cpu(), \'results/test_two_triangles/target.png\')\ntarget = pyredner.imread(\'results/test_two_triangles/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda(device = pyredner.get_device())\n\n# Perturb the scene, this is our initial guess\nshape_tri0.vertices = torch.tensor(\\\n    [[-1.3,1.5,0.1], [1.5,0.7,-0.2], [-0.8,-1.1,0.2]],\n    device = pyredner.get_device(),\n    requires_grad=True)\nshape_tri1.vertices = torch.tensor(\\\n    [[-0.5,1.2,1.2], [0.3,1.7,1.0], [0.5,-1.8,1.3]],\n    device = pyredner.get_device(),\n    requires_grad=True)\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\n# Render the initial guess\nimg = render(1, *args)\npyredner.imwrite(img.cpu(), \'results/test_two_triangles/init.png\')\ndiff = torch.abs(target - img)\npyredner.imwrite(diff.cpu(), \'results/test_two_triangles/init_diff.png\')\n\n# Optimize for triangle vertices\noptimizer = torch.optim.Adam([shape_tri0.vertices, shape_tri1.vertices], lr=5e-2)\nfor t in range(200):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    # Forward pass: render the image\n    args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4,\n        max_bounces = 1)\n    img = render(t+1, *args)\n    pyredner.imwrite(img.cpu(), \'results/test_two_triangles/iter_{}.png\'.format(t))\n    loss = (img - target).pow(2).sum()\n    print(\'loss:\', loss.item())\n\n    loss.backward()\n    print(\'tri0.grad:\', shape_tri0.vertices.grad)\n    print(\'tri1.grad:\', shape_tri1.vertices.grad)\n\n    optimizer.step()\n    print(\'tri0:\', shape_tri0.vertices)\n    print(\'tri1:\', shape_tri1.vertices)\n\nargs = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\nimg = render(202, *args)\npyredner.imwrite(img.cpu(), \'results/test_two_triangles/final.exr\')\npyredner.imwrite(img.cpu(), \'results/test_two_triangles/final.png\')\npyredner.imwrite(torch.abs(target - img).cpu(), \'results/test_two_triangles/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_two_triangles/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_two_triangles/out.mp4""])\n'"
tests/test_vertex_color.py,0,"b'import pyredner\nimport redner\nimport numpy as np\nimport torch\nimport math\n\n# Example of optimizing vertex color of a sphere.\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\ncam = pyredner.Camera(position = torch.tensor([0.0, 0.0, -5.0]),\n                      look_at = torch.tensor([0.0, 0.0, 0.0]),\n                      up = torch.tensor([0.0, 1.0, 0.0]),\n                      fov = torch.tensor([45.0]), # in degree\n                      clip_near = 1e-2, # needs to > 0\n                      resolution = (256, 256))\n\n# Set ""use_vertex_color = True"" to use vertex color\nmat_vertex_color = pyredner.Material(use_vertex_color = True)\nmaterials = [mat_vertex_color]\n\nvertices, indices, uvs, normals = pyredner.generate_sphere(128, 64)\n# For the target we randomize the vertex color.\nvertex_color = torch.zeros_like(vertices).uniform_(0.0, 1.0)\nshape_sphere = pyredner.Shape(\\\n    vertices = vertices,\n    indices = indices,\n    uvs = uvs,\n    normals = normals,\n    colors = vertex_color, # use the \'colors\' field in Shape to store the color\n    material_id = 0)\nshapes = [shape_sphere]\n\nenvmap = pyredner.imread(\'sunsky.exr\')\nif pyredner.get_use_gpu():\n    envmap = envmap.cuda(device = pyredner.get_device())\nenvmap = pyredner.EnvironmentMap(envmap)\nscene = pyredner.Scene(cam, shapes, materials, [], envmap)\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1,\n    channels = [redner.channels.radiance, redner.channels.vertex_color])\nrender = pyredner.RenderFunction.apply\nimg = render(0, *scene_args)\nimg_radiance = img[:, :, :3]\nimg_vertex_color = img[:, :, 3:]\npyredner.imwrite(img_radiance.cpu(), \'results/test_vertex_color/target.exr\')\npyredner.imwrite(img_radiance.cpu(), \'results/test_vertex_color/target.png\')\npyredner.imwrite(img_vertex_color.cpu(), \'results/test_vertex_color/target_color.png\')\ntarget_radiance = pyredner.imread(\'results/test_vertex_color/target.exr\')\nif pyredner.get_use_gpu():\n    target_radiance = target_radiance.cuda()\n\n# Initial guess. Set to 0.5 for all vertices.\nshape_sphere.colors = \\\n    torch.zeros_like(vertices, device = pyredner.get_device()) + 0.5\nshape_sphere.colors.requires_grad = True\n# We render both the radiance and the vertex color here.\n# The vertex color is only for visualization.\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1,\n    channels = [redner.channels.radiance, redner.channels.vertex_color])\nimg = render(1, *scene_args)\nimg_radiance = img[:, :, :3]\nimg_vertex_color = img[:, :, 3:]\npyredner.imwrite(img_radiance.cpu(), \'results/test_vertex_color/init.png\')\npyredner.imwrite(img_vertex_color.cpu(), \'results/test_vertex_color/init_color.png\')\ndiff = torch.abs(target_radiance - img_radiance)\npyredner.imwrite(diff.cpu(), \'results/test_vertex_color/init_diff.png\')\n\noptimizer = torch.optim.Adam([shape_sphere.colors], lr=1e-2)\nfor t in range(100):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    scene_args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4,\n        max_bounces = 1,\n        channels = [redner.channels.radiance, redner.channels.vertex_color])\n    img = render(t+1, *scene_args)\n    img_radiance = img[:, :, :3]\n    img_vertex_color = img[:, :, 3:]\n    pyredner.imwrite(img_radiance.cpu(), \'results/test_vertex_color/iter_{}.png\'.format(t))\n    pyredner.imwrite(img_vertex_color.cpu(), \'results/test_vertex_color/iter_color_{}.png\'.format(t))\n\n    loss = torch.pow(img_radiance - target_radiance, 2).sum()\n    print(\'loss:\', loss.item())\n\n    loss.backward()\n    optimizer.step()\n\n    # Clamp the data to valid range.\n    shape_sphere.colors.data.clamp_(0.0, 1.0)\n\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1,\n    channels = [redner.channels.radiance, redner.channels.vertex_color])\nimg = render(102, *scene_args)\nimg_radiance = img[:, :, :3]\nimg_vertex_color = img[:, :, 3:]\npyredner.imwrite(img_radiance.cpu(), \'results/test_vertex_color/final.exr\')\npyredner.imwrite(img_radiance.cpu(), \'results/test_vertex_color/final.png\')\npyredner.imwrite(img_vertex_color.cpu(), \'results/test_vertex_color/final_color.png\')\npyredner.imwrite(torch.abs(target_radiance - img_radiance).cpu(), \'results/test_vertex_color/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_vertex_color/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_vertex_color/out.mp4""])\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_vertex_color/iter_color_%d.png"", ""-vb"", ""20M"",\n    ""results/test_vertex_color/out_color.mp4""])'"
tests/test_viewport.py,0,"b'import pyredner\nimport numpy as np\nimport torch\n\n# From the test_single_triangle.py test case but with viewport\n\npyredner.set_use_gpu(torch.cuda.is_available())\n\ncam = pyredner.Camera(position = torch.tensor([0.0, 0.0, -5.0]),\n                      look_at = torch.tensor([0.0, 0.0, 0.0]),\n                      up = torch.tensor([0.0, 1.0, 0.0]),\n                      fov = torch.tensor([45.0]), # in degree\n                      clip_near = 1e-2, # needs to > 0\n                      resolution = (1024, 1024),\n                      viewport = (200, 300, 700, 800))\n\nmat_grey = pyredner.Material(\\\n    diffuse_reflectance = \\\n        torch.tensor([0.5, 0.5, 0.5], device = pyredner.get_device()))\nmaterials = [mat_grey]\n\nshape_triangle = pyredner.Shape(\\\n    vertices = torch.tensor([[-1.7, 1.0, 0.0], [1.0, 1.0, 0.0], [-0.5, -1.0, 0.0]],\n        device = pyredner.get_device()),\n    indices = torch.tensor([[0, 1, 2]], dtype = torch.int32,\n        device = pyredner.get_device()),\n    uvs = None,\n    normals = None,\n    material_id = 0)\nshape_light = pyredner.Shape(\\\n    vertices = torch.tensor([[-1.0, -1.0, -7.0],\n                             [ 1.0, -1.0, -7.0],\n                             [-1.0,  1.0, -7.0],\n                             [ 1.0,  1.0, -7.0]], device = pyredner.get_device()),\n    indices = torch.tensor([[0, 1, 2],[1, 3, 2]],\n        dtype = torch.int32, device = pyredner.get_device()),\n    uvs = None,\n    normals = None,\n    material_id = 0)\nshapes = [shape_triangle, shape_light]\n\nlight = pyredner.AreaLight(shape_id = 1, \n                           intensity = torch.tensor([20.0,20.0,20.0]))\narea_lights = [light]\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\n\nrender = pyredner.RenderFunction.apply\nimg = render(0, *scene_args)\npyredner.imwrite(img.cpu(), \'results/test_viewport/target.exr\')\npyredner.imwrite(img.cpu(), \'results/test_viewport/target.png\')\ntarget = pyredner.imread(\'results/test_viewport/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda(device = pyredner.get_device())\n\n# Perturb the scene, this is our initial guess.\nshape_triangle.vertices = torch.tensor(\\\n    [[-2.0,1.5,0.3], [0.9,1.2,-0.3], [-0.4,-1.4,0.2]],\n    device = pyredner.get_device(),\n    requires_grad = True) # Set requires_grad to True since we want to optimize this\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\nimg = render(1, *scene_args)\npyredner.imwrite(img.cpu(), \'results/test_single_triangle/init.png\')\ndiff = torch.abs(target - img)\npyredner.imwrite(diff.cpu(), \'results/test_single_triangle/init_diff.png\')\n\noptimizer = torch.optim.Adam([shape_triangle.vertices], lr=5e-2)\nfor t in range(200):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    scene_args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4, # We use less samples in the Adam loop.\n        max_bounces = 1)\n    img = render(t+1, *scene_args)\n    pyredner.imwrite(img.cpu(), \'results/test_viewport/iter_{}.png\'.format(t))\n    loss = (img - target).pow(2).sum()\n    print(\'loss:\', loss.item())\n\n    loss.backward()\n    print(\'grad:\', shape_triangle.vertices.grad)\n\n    optimizer.step()\n    print(\'vertices:\', shape_triangle.vertices)\n\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\nimg = render(202, *scene_args)\npyredner.imwrite(img.cpu(), \'results/test_viewport/final.exr\')\npyredner.imwrite(img.cpu(), \'results/test_viewport/final.png\')\npyredner.imwrite(torch.abs(target - img).cpu(), \'results/test_viewport/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_viewport/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_viewport/out.mp4""])\n'"
tests/unit_tests.py,0,b'import redner\nimport torch\n\ndef unit_tests():\n    redner.test_sample_primary_rays(False)\n    redner.test_scene_intersect(False)\n    redner.test_sample_point_on_light(False)\n    redner.test_active_pixels(False)\n    redner.test_camera_derivatives()\n    redner.test_camera_distortion()\n    redner.test_d_bsdf()\n    redner.test_d_bsdf_sample()\n    redner.test_d_bsdf_pdf()\n    redner.test_d_intersect()\n    redner.test_d_sample_shape()\n    redner.test_atomic()\n\n    if torch.cuda.is_available():\n        redner.test_sample_primary_rays(True)\n        redner.test_scene_intersect(True)\n        redner.test_sample_point_on_light(True)\n        redner.test_active_pixels(True)\n\nunit_tests()\n\n'
tests_tensorflow/__init__.py,0,b''
tests_tensorflow/test_camera_distortion.py,17,"b'# Tensorflow by default allocates all GPU memory, leaving very little for rendering.\n# We set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true to enforce on demand\n# memory allocation to reduce page faults.\nimport os\nos.environ[\'TF_FORCE_GPU_ALLOW_GROWTH\'] = \'true\'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport pyredner_tensorflow as pyredner\n\npyredner.set_use_gpu(tf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None))\n\nposition = tf.Variable([1.0, 0.0, -3.0], dtype=tf.float32)\nlook_at = tf.Variable([1.0, 0.0, 0.0], dtype=tf.float32)\nup = tf.Variable([0.0, 1.0, 0.0], dtype=tf.float32)\nfov = tf.Variable([45.0], dtype=tf.float32)\nclip_near = 1e-2\n\n# randomly generate distortion parameters\ntf.random.set_seed(1234)\ntarget_distort_params = (tf.random.uniform([8]) - 0.5) * 0.05\nresolution = (256, 256)\ncam = pyredner.Camera(position = position,\n                      look_at = look_at,\n                      up = up,\n                      fov = fov,\n                      clip_near = clip_near,\n                      resolution = resolution,\n                      distortion_params = target_distort_params)\n\ncheckerboard_texture = pyredner.imread(\'scenes/teapot.png\')\nmat_checkerboard = pyredner.Material(\\\n    diffuse_reflectance = checkerboard_texture)\nmat_black = pyredner.Material(\\\n    diffuse_reflectance = tf.Variable([0.0, 0.0, 0.0], dtype=tf.float32))\n\nplane = pyredner.Object(vertices = tf.Variable([[-1.0,-1.0, 0.0],\n                                                [-1.0, 1.0, 0.0],\n                                                [ 1.0,-1.0, 0.0],\n                                                [ 1.0, 1.0, 0.0]]),\n                        indices = tf.constant([[0, 1, 2],\n                                               [1, 3, 2]],\n                                              dtype = tf.int32),\n                        uvs = tf.Variable([[0.05, 0.05],\n                                           [0.05, 0.95],\n                                           [0.95, 0.05],\n                                           [0.95, 0.95]]),\n                        material = mat_checkerboard)\nscene = pyredner.Scene(camera=cam, objects=[plane])\nimg = pyredner.render_albedo(scene=scene)\npyredner.imwrite(img, \'results/test_camera_distortion/target.exr\')\npyredner.imwrite(img, \'results/test_camera_distortion/target.png\')\n# Read the target image we just saved.\ntarget = pyredner.imread(\'results/test_camera_distortion/target.exr\')\n\ncam.distortion_params = tf.Variable(tf.zeros(8), trainable=True)\nscene = pyredner.Scene(camera=cam, objects=[plane])\nimg = pyredner.render_albedo(scene=scene)\npyredner.imwrite(img, \'results/test_camera_distortion/init.exr\')\npyredner.imwrite(img, \'results/test_camera_distortion/init.png\')\n\n# Optimize for triangle vertices.\noptimizer = tf.compat.v1.train.AdamOptimizer(1e-3)\nfor t in range(200):\n    print(\'iteration:\', t)\n\n    with tf.GradientTape() as tape:\n        scene = pyredner.Scene(camera=cam, objects=[plane])\n        img = pyredner.render_albedo(scene=scene)\n        pyredner.imwrite(img, \'results/test_camera_distortion/iter_{}.png\'.format(t))\n        loss = tf.reduce_sum(tf.square(img - target))\n\n    print(\'loss:\', loss)\n\n    grads = tape.gradient(loss, [cam.distortion_params])\n    optimizer.apply_gradients(zip(grads, [cam.distortion_params]))\n\n    print(\'grad:\', grads)\n\n    print(\'distortion_params:\', cam.distortion_params)\n\nimg = pyredner.render_albedo(scene=scene)\npyredner.imwrite(img, \'results/test_camera_distortion/final.exr\')\npyredner.imwrite(img, \'results/test_camera_distortion/final.png\')\n\n# Convert the intermediate renderings to a video.\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_camera_distortion/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_camera_distortion/out.mp4""])\n'"
tests_tensorflow/test_compute_uvs.py,10,"b'# Tensorflow by default allocates all GPU memory, leaving very little for rendering.\n# We set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true to enforce on demand\n# memory allocation to reduce page faults.\nimport os\nos.environ[\'TF_FORCE_GPU_ALLOW_GROWTH\'] = \'true\'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport pyredner_tensorflow as pyredner\nimport redner\n\n# Automatic UV mapping example adapted from tutorial 2\n\n# Use GPU if available\npyredner.set_use_gpu(tf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None))\n\n# Set up the pyredner scene for rendering:\nwith tf.device(pyredner.get_device_name()):\n    material_map, mesh_list, light_map = pyredner.load_obj(\'scenes/teapot.obj\')\n    for _, mesh in mesh_list:\n        mesh.normals = pyredner.compute_vertex_normal(mesh.vertices, mesh.indices)\n        mesh.uvs, mesh.uv_indices = pyredner.compute_uvs(mesh.vertices, mesh.indices)\n\n# Setup camera\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    cam = pyredner.Camera(position = tf.Variable([0.0, 30.0, 200.0], dtype=tf.float32),\n                          look_at = tf.Variable([0.0, 30.0, 0.0], dtype=tf.float32),\n                          up = tf.Variable([0.0, 1.0, 0.0], dtype=tf.float32),\n                          fov = tf.Variable([45.0], dtype=tf.float32), # in degree\n                          clip_near = 1e-2, # needs to > 0\n                          resolution = (256, 256),\n                          fisheye = False)\n\n# Setup materials\nmaterial_id_map = {}\nmaterials = []\ncount = 0\nfor key, value in material_map.items():\n    material_id_map[key] = count\n    count += 1\n    materials.append(value)\n\n# Setup geometries\nshapes = []\nwith tf.device(pyredner.get_device_name()):\n    for mtl_name, mesh in mesh_list:\n        shapes.append(pyredner.Shape(\n            vertices = mesh.vertices,\n            indices = mesh.indices,\n            uvs = mesh.uvs,\n            normals = mesh.normals,\n            material_id = material_id_map[mtl_name]))\n\nwith tf.device(pyredner.get_device_name()):\n    envmap = pyredner.imread(\'sunsky.exr\')\n    envmap = pyredner.EnvironmentMap(envmap)\n\n# Construct the scene\nscene = pyredner.Scene(cam, shapes, materials, area_lights = [], envmap = envmap)\n# Serialize the scene\n# Here we specify the output channels as ""depth"", ""shading_normal""\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 512,\n    max_bounces = 1)\nimg = pyredner.render(0, *scene_args)\npyredner.imwrite(img, \'results/test_compute_uvs/target.exr\')\npyredner.imwrite(img, \'results/test_compute_uvs/target.png\')\ntarget = pyredner.imread(\'results/test_compute_uvs/target.exr\')\n'"
tests_tensorflow/test_compute_vertex_normals.py,3,"b""# Tensorflow by default allocates all GPU memory, leaving very little for rendering.\n# We set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true to enforce on demand\n# memory allocation to reduce page faults.\nimport os\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution() # redner only supports eager mode\nimport pyredner_tensorflow as pyredner\n\nobjects = pyredner.load_obj('scenes/teapot.obj', return_objects=True)\ncamera = pyredner.automatic_camera_placement(objects, resolution=(512, 512))\nscene = pyredner.Scene(camera = camera, objects = objects)\n\nlight = pyredner.PointLight(position = (camera.position + tf.constant((0.0, 0.0, 100.0))),\n                            intensity = tf.constant((20000.0, 30000.0, 20000.0)))\n\nimg = pyredner.render_deferred(scene = scene, lights = [light])\npyredner.imwrite(img, 'results/test_compute_vertex_normals/no_vertex_normal.exr')\n\nfor obj in objects:\n    obj.normals = pyredner.compute_vertex_normal(obj.vertices, obj.indices, 'max')\nscene = pyredner.Scene(camera = camera, objects = objects)\nimg = pyredner.render_deferred(scene = scene, lights = [light])\npyredner.imwrite(img, 'results/test_compute_vertex_normals/max_vertex_normal.exr')\n\nfor obj in objects:\n    obj.normals = pyredner.compute_vertex_normal(obj.vertices, obj.indices, 'cotangent')\nscene = pyredner.Scene(camera = camera, objects = objects)\nimg = pyredner.render_deferred(scene = scene, lights = [light])\npyredner.imwrite(img, 'results/test_compute_vertex_normals/cotangent_vertex_normal.exr')"""
tests_tensorflow/test_envmap.py,23,"b'# Tensorflow by default allocates all GPU memory, leaving very little for rendering.\n# We set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true to enforce on demand\n# memory allocation to reduce page faults.\nimport os\nos.environ[\'TF_FORCE_GPU_ALLOW_GROWTH\'] = \'true\'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport pyredner_tensorflow as pyredner\n\n# Use GPU if available\npyredner.set_use_gpu(tf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None))\n\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    cam = pyredner.Camera(position = tf.Variable([0.0, 0.0, -5.0], dtype=tf.float32),\n                          look_at = tf.Variable([0.0, 0.0, 0.0], dtype=tf.float32),\n                          up = tf.Variable([0.0, 1.0, 0.0], dtype=tf.float32),\n                          fov = tf.Variable([45.0], dtype=tf.float32), # in degree\n                          clip_near = 1e-2, # needs to > 0\n                          resolution = (256, 256),\n                          fisheye = False)\n\nwith tf.device(pyredner.get_device_name()):\n    mat_grey = pyredner.Material(\n        diffuse_reflectance = tf.Variable([0.4, 0.4, 0.4], dtype=tf.float32),\n        specular_reflectance = tf.Variable([0.5, 0.5, 0.5], dtype=tf.float32),\n        roughness = tf.Variable([0.05], dtype=tf.float32))\n\nmaterials = [mat_grey]\n\nwith tf.device(pyredner.get_device_name()):\n    vertices, indices, uvs, normals = pyredner.generate_sphere(128, 64)\n    shape_sphere = pyredner.Shape(\n        vertices = vertices,\n        indices = indices,\n        uvs = uvs,\n        normals = normals,\n        material_id = 0)\nshapes = [shape_sphere]\n\nwith tf.device(pyredner.get_device_name()):\n    envmap = pyredner.imread(\'sunsky.exr\')\n    envmap = pyredner.EnvironmentMap(envmap)\nscene = pyredner.Scene(camera=cam,\n                       shapes=shapes,\n                       materials=materials,\n                       area_lights=[],\n                       envmap=envmap)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\n\nimg = pyredner.render(0, *scene_args)\npyredner.imwrite(img, \'results/test_envmap/target.exr\')\npyredner.imwrite(img, \'results/test_envmap/target.png\')\ntarget = pyredner.imread(\'results/test_envmap/target.exr\')\n\nwith tf.device(pyredner.get_device_name()):\n    envmap_texels = tf.Variable(0.5 * tf.ones([32, 64, 3], dtype=tf.float32),\n                                trainable=True)\n    envmap = pyredner.EnvironmentMap(tf.abs(envmap_texels))\nscene = pyredner.Scene(camera=cam,\n                       shapes=shapes,\n                       materials=materials,\n                       area_lights=[],\n                       envmap=envmap)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\nimg = pyredner.render(1, *scene_args)\npyredner.imwrite(img, \'results/test_envmap/init.png\')\ndiff = tf.abs(target - img)\npyredner.imwrite(diff, \'results/test_envmap/init_diff.png\')\n\noptimizer = tf.compat.v1.train.AdamOptimizer(1e-2)\nfor t in range(600):\n    print(\'iteration:\', t)\n    with tf.GradientTape() as tape:\n        envmap = pyredner.EnvironmentMap(tf.abs(envmap_texels))\n        scene = pyredner.Scene(camera=cam,\n                               shapes=shapes,\n                               materials=materials,\n                               area_lights=[],\n                               envmap=envmap)\n        scene_args = pyredner.serialize_scene(\n            scene = scene,\n            num_samples = 4,\n            max_bounces = 1)\n        img = pyredner.render(t+1, *scene_args)\n\n        loss = tf.reduce_sum(tf.square(img - target))\n        print(\'loss:\', loss)\n\n    pyredner.imwrite(img, \'results/test_envmap/iter_{}.png\'.format(t))\n    pyredner.imwrite(tf.abs(envmap_texels), \'results/test_envmap/envmap_{}.exr\'.format(t))\n\n    grads = tape.gradient(loss, [envmap_texels])\n    optimizer.apply_gradients(zip(grads, [envmap_texels]))\n\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\nimg = pyredner.render(602, *scene_args)\npyredner.imwrite(img, \'results/test_envmap/final.exr\')\npyredner.imwrite(img, \'results/test_envmap/final.png\')\npyredner.imwrite(tf.abs(target - img), \'results/test_envmap/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_envmap/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_envmap/out.mp4""])\n'"
tests_tensorflow/test_g_buffer.py,29,"b'# Tensorflow by default allocates all GPU memory, leaving very little for rendering.\n# We set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true to enforce on demand\n# memory allocation to reduce page faults.\nimport os\nos.environ[\'TF_FORCE_GPU_ALLOW_GROWTH\'] = \'true\'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport pyredner_tensorflow as pyredner\nimport redner\n\n# Optimize depth and normal of a teapot\n\n# Use GPU if available\npyredner.set_use_gpu(tf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None))\n\n# Set up the pyredner scene for rendering:\nwith tf.device(pyredner.get_device_name()):\n    material_map, mesh_list, light_map = pyredner.load_obj(\'scenes/teapot.obj\')\n    for _, mesh in mesh_list:\n        mesh.normals = pyredner.compute_vertex_normal(mesh.vertices, mesh.indices)\n\n# Setup camera\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    cam = pyredner.Camera(position = tf.Variable([0.0, 30.0, 200.0], dtype=tf.float32),\n                          look_at = tf.Variable([0.0, 30.0, 0.0], dtype=tf.float32),\n                          up = tf.Variable([0.0, 1.0, 0.0], dtype=tf.float32),\n                          fov = tf.Variable([45.0], dtype=tf.float32), # in degree\n                          clip_near = 1e-2, # needs to > 0\n                          resolution = (256, 256),\n                          fisheye = False)\n\n# Setup materials\nmaterial_id_map = {}\nmaterials = []\ncount = 0\nfor key, value in material_map.items():\n    material_id_map[key] = count\n    count += 1\n    materials.append(value)\n\n# Setup geometries\nshapes = []\nwith tf.device(pyredner.get_device_name()):\n    for mtl_name, mesh in mesh_list:\n        shapes.append(pyredner.Shape(\n            vertices = mesh.vertices,\n            indices = mesh.indices,\n            uvs = mesh.uvs,\n            normals = mesh.normals,\n            material_id = material_id_map[mtl_name]))\n\n# We don\'t setup any light source here\n\n# Construct the scene\nscene = pyredner.Scene(cam, shapes, materials, area_lights = [], envmap = None)\n# Serialize the scene\n# Here we specify the output channels as ""depth"", ""shading_normal""\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 0,\n    channels = [redner.channels.depth, redner.channels.shading_normal])\n\n# Render. The first argument is the seed for RNG in the renderer.\nimg = pyredner.render(0, *scene_args)\n# Save the images.\ndepth = img[:, :, 0]\nnormal = img[:, :, 1:4]\npyredner.imwrite(depth, \'results/test_g_buffer/target_depth.exr\')\npyredner.imwrite(depth, \'results/test_g_buffer/target_depth.png\', normalize = True)\npyredner.imwrite(normal, \'results/test_g_buffer/target_normal.exr\')\npyredner.imwrite(normal, \'results/test_g_buffer/target_normal.png\', normalize = True)\n# Read the target image we just saved.\ntarget_depth = pyredner.imread(\'results/test_g_buffer/target_depth.exr\')\ntarget_depth = target_depth[:, :, 0]\ntarget_normal = pyredner.imread(\'results/test_g_buffer/target_normal.exr\')\n\nwith tf.device(pyredner.get_device_name()):\n    # Perturb the teapot by a translation and a rotation to the object\n    translation_params = tf.Variable([0.1, -0.1, 0.1], trainable=True)\n    translation = translation_params * 100.0\n    euler_angles = tf.Variable([0.1, -0.1, 0.1], trainable=True)\n\n    # These are the vertices we want to apply the transformation\n    shape0_vertices = tf.identity(shapes[0].vertices)\n    shape1_vertices = tf.identity(shapes[1].vertices)\n    # We can use pyredner.gen_rotate_matrix to generate 3x3 rotation matrices\n    rotation_matrix = pyredner.gen_rotate_matrix(euler_angles)\n    center = tf.math.reduce_mean(tf.concat([shape0_vertices, shape1_vertices], axis=0), axis=0)\n    # Shift the vertices to the center, apply rotation matrix,\n    # shift back to the original space\n    shapes[0].vertices = \\\n        (shape0_vertices - center) @ tf.transpose(rotation_matrix) + \\\n        center + translation\n    shapes[1].vertices = \\\n        (shape1_vertices - center) @ tf.transpose(rotation_matrix) + \\\n        center + translation\n    # Since we changed the vertices, we need to regenerate the shading normals\n    shapes[0].normals = pyredner.compute_vertex_normal(shapes[0].vertices, shapes[0].indices)\n    shapes[1].normals = pyredner.compute_vertex_normal(shapes[1].vertices, shapes[1].indices)\n# We need to serialize the scene again to get the new arguments.\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 0,\n    channels = [redner.channels.depth, redner.channels.shading_normal])\n# Render the initial guess.\nimg = pyredner.render(1, *scene_args)\ndepth = img[:, :, 0]\nnormal = img[:, :, 1:4]\n# Save the images.\npyredner.imwrite(depth, \'results/test_g_buffer/init_depth.png\', normalize = True)\npyredner.imwrite(depth, \'results/test_g_buffer/init_normal.png\', normalize = True)\n# Compute the difference and save the images.\ndiff_depth = tf.abs(target_depth - depth)\ndiff_normal = tf.abs(target_normal - normal)\npyredner.imwrite(diff_depth, \'results/test_g_buffer/init_depth_diff.png\')\npyredner.imwrite(diff_normal, \'results/test_g_buffer/init_normal_diff.png\')\n\n# Optimize for triangle vertices.\noptimizer = tf.compat.v1.train.AdamOptimizer(1e-2)\n# Run 200 Adam iterations.\nfor t in range(200):\n    print(\'iteration:\', t)\n    # Forward pass: apply the mesh operation and render the image.\n    with tf.GradientTape() as tape:\n        translation = translation_params * 100.0\n        rotation_matrix = pyredner.gen_rotate_matrix(euler_angles)\n        center = tf.math.reduce_mean(\n            tf.concat([shape0_vertices, shape1_vertices], axis=0), \n            axis=0)\n        shapes[0].vertices = \\\n            (shape0_vertices - center) @ tf.transpose(rotation_matrix) + \\\n            center + translation\n        shapes[1].vertices = \\\n            (shape1_vertices - center) @ tf.transpose(rotation_matrix) + \\\n            center + translation\n\n        shapes[0].normals = pyredner.compute_vertex_normal(shapes[0].vertices, shapes[0].indices)\n        shapes[1].normals = pyredner.compute_vertex_normal(shapes[1].vertices, shapes[1].indices)\n        scene_args = pyredner.serialize_scene(\n            scene = scene,\n            num_samples = 4, # We use less samples in the Adam loop.\n            max_bounces = 0,\n            channels = [redner.channels.depth, redner.channels.shading_normal])\n        # Important to use a different seed every iteration, otherwise the result\n        # would be biased.\n        img = pyredner.render(t+1, *scene_args)\n\n        depth = img[:, :, 0]\n        normal = img[:, :, 1:4]\n\n        # Save the intermediate render.\n        pyredner.imwrite(depth, \'results/test_g_buffer/iter_depth_{}.png\'.format(t), normalize = True)\n        pyredner.imwrite(normal, \'results/test_g_buffer/iter_normal_{}.png\'.format(t), normalize = True)\n        # Compute the loss function. Here it is L2.\n        loss = tf.reduce_sum(tf.square(depth - target_depth)) / 200.0 \\\n             + tf.reduce_sum(tf.square(normal - target_normal))\n        print(\'loss:\', loss)\n\n    grads = tape.gradient(loss, [translation_params, euler_angles])\n\n    optimizer.apply_gradients(zip(grads, [translation_params, euler_angles]))\n\n    # Print the gradients\n    print(\'translation_params.grad:\', grads[0])\n    print(\'euler_angles.grad:\', grads[0])\n\n    # Take a gradient descent step.\n    # Print the current pose parameters.\n    print(\'translation:\', translation)\n    print(\'euler_angles:\', euler_angles)\n\n# Render the final result.\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 0,\n    channels = [redner.channels.depth, redner.channels.shading_normal])\nimg = pyredner.render(202, *scene_args)\ndepth = img[:, :, 0]\nnormal = img[:, :, 1:4]\n# Save the images.\npyredner.imwrite(depth, \'results/test_g_buffer/final_depth.exr\')\npyredner.imwrite(depth, \'results/test_g_buffer/init_depth.png\', normalize = True)\npyredner.imwrite(normal, \'results/test_g_buffer/final_normal.exr\')\npyredner.imwrite(normal, \'results/test_g_buffer/final_normal.png\', normalize = True)\ndiff_depth = tf.abs(target_depth - depth)\ndiff_normal = tf.abs(target_normal - normal)\npyredner.imwrite(diff_depth, \'results/test_g_buffer/init_depth_diff.png\')\npyredner.imwrite(diff_normal, \'results/test_g_buffer/init_normal_diff.png\')\n\n# Convert the intermediate renderings to a video.\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_g_buffer/iter_depth_%d.png"", ""-vb"", ""20M"",\n    ""results/test_g_buffer/out_depth.mp4""])\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_g_buffer/iter_normal_%d.png"", ""-vb"", ""20M"",\n    ""results/test_g_buffer/out_normal.mp4""])\n'"
tests_tensorflow/test_sample_pixel_center.py,1,"b""import os\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution() # redner only supports eager mode\nimport pyredner_tensorflow as pyredner\n\n# Test the sample pixel center flag\n\nobjects = pyredner.load_obj('scenes/teapot.obj', return_objects=True)\ncamera = pyredner.automatic_camera_placement(objects, resolution=(128, 128))\nscene = pyredner.Scene(camera = camera, objects = objects)\nimg = pyredner.render_albedo(scene, sample_pixel_center = True)\npyredner.imwrite(img.cpu(), 'results/test_sample_pixel_center/img_no_aa.exr')\nimg = pyredner.render_albedo(scene, sample_pixel_center = False)\npyredner.imwrite(img.cpu(), 'results/test_sample_pixel_center/img_with_aa.exr')\n"""
tests_tensorflow/test_save_obj.py,2,"b""# Tensorflow by default allocates all GPU memory, leaving very little for rendering.\n# We set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true to enforce on demand\n# memory allocation to reduce page faults.\nimport os\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport pyredner_tensorflow as pyredner\n\nvertices, indices, uvs, normals = pyredner.generate_sphere(16, 32)\nm = pyredner.Material(diffuse_reflectance = tf.constant((0.5, 0.5, 0.5)))\nobj = pyredner.Object(vertices = vertices,\n                      indices = indices,\n                      uvs = uvs,\n                      normals = normals,\n                      material = m)\nfilename = 'results/test_save_obj/sphere.obj'\ndirectory = os.path.dirname(filename)\nif directory != '' and not os.path.exists(directory):\n    os.makedirs(directory)\npyredner.save_obj(obj, 'results/test_save_obj/sphere.obj')\n"""
tests_tensorflow/test_screen_gradient.py,2,"b""# Tensorflow by default allocates all GPU memory, leaving very little for rendering.\n# We set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true to enforce on demand\n# memory allocation to reduce page faults.\nimport os\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport pyredner_tensorflow as pyredner\nfrom matplotlib import cm\nimport os\nimport numpy as np\nimport skimage\n\ndef normalize(x, min_, max_):\n    range = max(abs(min_), abs(max_))\n    return (x + range) / (2 * range)\n\n# Use GPU if available\npyredner.set_use_gpu(tf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None))\n\nobjects = pyredner.load_obj('scenes/teapot.obj', return_objects=True)\ncamera = pyredner.automatic_camera_placement(objects, resolution=(512, 512))\nscene = pyredner.Scene(camera = camera, objects = objects)\nscreen_gradient_img = pyredner.visualize_screen_gradient(\n\tgrad_img = None,\n    seed = 0,\n    scene = scene,\n    num_samples = 4,\n    max_bounces = 0,\n    channels = [pyredner.channels.diffuse_reflectance])\n\ndirectory = 'results/test_screen_gradient'\nif directory != '' and not os.path.exists(directory):\n    os.makedirs(directory)\n\nclamp_factor = 0.2\nx_diff = screen_gradient_img[:, :, 0].numpy()\ndx = cm.viridis(normalize(x_diff, np.min(x_diff) * clamp_factor, np.max(x_diff) * clamp_factor))\nskimage.io.imsave('results/test_screen_gradient/dx.png', (dx * 255).astype(np.uint8))\ny_diff = screen_gradient_img[:, :, 1]\ndy = cm.viridis(normalize(y_diff, np.min(y_diff) * clamp_factor, np.max(y_diff) * clamp_factor))\nskimage.io.imsave('results/test_screen_gradient/dy.png', (dy * 255).astype(np.uint8))\n"""
tests_tensorflow/test_shadow_blocker.py,29,"b'# Tensorflow by default allocates all GPU memory, leaving very little for rendering.\n# We set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true to enforce on demand\n# memory allocation to reduce page faults.\nimport os\nos.environ[\'TF_FORCE_GPU_ALLOW_GROWTH\'] = \'true\'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport pyredner_tensorflow as pyredner\n\n# Optimize four vertices of a shadow blocker\n\n# Use GPU if available\npyredner.set_use_gpu(tf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None))\n\n# Set up the scene\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    position = tf.Variable([0.0, 2.0, -5.0], dtype=tf.float32)\n    look_at = tf.Variable([0.0, 0.0, 0.0], dtype=tf.float32)\n    up = tf.Variable([0.0, 1.0, 0.0], dtype=tf.float32)\n    fov = tf.Variable([45.0], dtype=tf.float32)\n    clip_near = 1e-2\n    resolution = (256, 256)\n    cam = pyredner.Camera(position = position,\n                          look_at = look_at,\n                          up = up,\n                          fov = fov,\n                          clip_near = clip_near,\n                          resolution = resolution)\n\nwith tf.device(pyredner.get_device_name()):\n    mat_grey = pyredner.Material(\n        diffuse_reflectance = tf.Variable([0.5, 0.5, 0.5], dtype=tf.float32))\n    mat_black = pyredner.Material(\n        diffuse_reflectance = tf.Variable([0.0, 0.0, 0.0], dtype=tf.float32))\n    materials = [mat_grey, mat_black]\n\n    # tf.constant allocates arrays on host memory for int32 arrays (some tensorflow internal mess),\n    # but pyredner.Shape constructor automatically converts the memory to device if necessary.\n    floor_vertices = tf.Variable([[-2.0,0.0,-2.0],[-2.0,0.0,2.0],[2.0,0.0,-2.0],[2.0,0.0,2.0]],\n        dtype=tf.float32)\n    floor_indices = tf.constant([[0,1,2], [1,3,2]], dtype=tf.int32)\n    shape_floor = pyredner.Shape(floor_vertices, floor_indices, 0)\n    blocker_vertices = tf.Variable([[-0.5,3.0,-0.5],[-0.5,3.0,0.5],[0.5,3.0,-0.5],[0.5,3.0,0.5]],\n        dtype=tf.float32)\n    blocker_indices = tf.constant([[0,1,2], [1,3,2]], dtype=tf.int32)\n    shape_blocker = pyredner.Shape(blocker_vertices, blocker_indices, 0)\n    light_vertices = tf.Variable([[-0.1,5,-0.1],[-0.1,5,0.1],[0.1,5,-0.1],[0.1,5,0.1]],\n        dtype=tf.float32)\n    light_indices = tf.constant([[0,2,1], [1,2,3]], dtype=tf.int32)\n    shape_light = pyredner.Shape(light_vertices, light_indices, 1)\n    shapes = [shape_floor, shape_blocker, shape_light]\n\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    light_intensity = tf.Variable([1000.0, 1000.0, 1000.0], dtype=tf.float32)\n# The first argument is the shape id of the light\nlight = pyredner.AreaLight(2, light_intensity)\narea_lights = [light]\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\n\n# Render our target\nimg = pyredner.render(0, *scene_args)\npyredner.imwrite(img, \'results/test_shadow_blocker/target.exr\')\npyredner.imwrite(img, \'results/test_shadow_blocker/target.png\')\ntarget = pyredner.imread(\'results/test_shadow_blocker/target.exr\')\n\n# Perturb the scene, this is our initial guess\nwith tf.device(pyredner.get_device_name()):\n    shape_blocker.vertices = tf.Variable(\n\t[[-0.2,3.5,-0.8],[-0.8,3.0,0.3],[0.4,2.8,-0.8],[0.3,3.2,1.0]],\n        trainable=True)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\n# Render the initial guess\nimg = pyredner.render(1, *scene_args)\npyredner.imwrite(img, \'results/test_shadow_blocker/init.png\')\ndiff = tf.abs(target - img)\npyredner.imwrite(diff, \'results/test_shadow_blocker/init_diff.png\')\n\n# Optimize for blocker vertices\noptimizer = tf.compat.v1.train.AdamOptimizer(1e-2)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 4,\n    max_bounces = 1)\nfor t in range(200):\n    print(\'iteration:\', t)\n    \n    with tf.GradientTape() as tape:\n        # Forward pass: render the image\n        img = pyredner.render(t+1, *scene_args)\n        pyredner.imwrite(img, \'results/test_shadow_blocker/iter_{}.png\'.format(t))\n        loss = tf.reduce_sum(tf.square(img - target))\n\n    print(\'loss:\', loss)\n\n    grads = tape.gradient(loss, [shape_blocker.vertices])\n    optimizer.apply_gradients(zip(grads, [shape_blocker.vertices]))\n    print(\'grad:\', grads[0])\n\n    print(\'vertices:\', shape_blocker.vertices)\n\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\nimg = pyredner.render(202, *scene_args)\npyredner.imwrite(img, \'results/test_shadow_blocker/final.exr\')\npyredner.imwrite(img, \'results/test_shadow_blocker/final.png\')\npyredner.imwrite(tf.abs(target - img), \'results/test_shadow_blocker/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_shadow_blocker/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_shadow_blocker/out.mp4""])\n'"
tests_tensorflow/test_shadow_camera.py,29,"b'# Tensorflow by default allocates all GPU memory, leaving very little for rendering.\n# We set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true to enforce on demand\n# memory allocation to reduce page faults.\nimport os\nos.environ[\'TF_FORCE_GPU_ALLOW_GROWTH\'] = \'true\'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport pyredner_tensorflow as pyredner\n\n# Optimize camera pose looking at shadow\n\n# Use GPU if available\npyredner.set_use_gpu(tf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None))\n\n# Set up the scene\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    position = tf.Variable([0.0, 9.0, 0.0], dtype=tf.float32)\n    look_at = tf.Variable([0.0, 0.0, 0.0], dtype=tf.float32)\n    up = tf.Variable([0.0, 0.0, 1.0], dtype=tf.float32)\n    fov = tf.Variable([45.0], dtype=tf.float32)\n    clip_near = 1e-2\n    resolution = (256, 256)\n    cam = pyredner.Camera(position = position,\n                          look_at = look_at,\n                          up = up,\n                          fov = fov,\n                          clip_near = clip_near,\n                          resolution = resolution)\n\nwith tf.device(pyredner.get_device_name()):\n    mat_grey = pyredner.Material(\n        diffuse_reflectance = tf.Variable([0.5, 0.5, 0.5], dtype=tf.float32))\n    mat_black = pyredner.Material(\n        diffuse_reflectance = tf.Variable([0.0, 0.0, 0.0], dtype=tf.float32))\n    materials = [mat_grey, mat_black]\n\n    # tf.constant allocates arrays on host memory for int32 arrays (some tensorflow internal mess),\n    # but pyredner.Shape constructor automatically converts the memory to device if necessary.\n    floor_vertices = tf.Variable([[-20.0,0.0,-20.0],[-20.0,0.0,20.0],[20.0,0.0,-20.0],[20.0,0.0,20.0]],\n        dtype=tf.float32)\n    floor_indices = tf.constant([[0,1,2], [1,3,2]], dtype=tf.int32)\n    shape_floor = pyredner.Shape(floor_vertices, floor_indices, 0)\n    blocker_vertices = tf.Variable([[-0.5,10.0,-0.5],[-0.5,10.0,0.5],[0.5,10.0,-0.5],[0.5,10.0,0.5]],\n        dtype=tf.float32)\n    blocker_indices = tf.constant([[0,1,2], [1,3,2]], dtype=tf.int32)\n    shape_blocker = pyredner.Shape(blocker_vertices, blocker_indices, 0)\n    light_vertices = tf.Variable([[-0.1,15,-0.1],[-0.1,15,0.1],[0.1,15,-0.1],[0.1,15,0.1]],\n        dtype=tf.float32)\n    light_indices = tf.constant([[0,2,1], [1,2,3]], dtype=tf.int32)\n    shape_light = pyredner.Shape(light_vertices, light_indices, 1)\n    shapes = [shape_floor, shape_blocker, shape_light]\n\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    light_intensity = tf.Variable([5000.0, 5000.0, 5000.0], dtype=tf.float32)\n# The first argument is the shape id of the light\nlight = pyredner.AreaLight(2, light_intensity)\narea_lights = [light]\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\n\n# Alias of the render function\n\n# Render our target\nimg = pyredner.render(0, *scene_args)\npyredner.imwrite(img, \'results/test_shadow_camera/target.exr\')\npyredner.imwrite(img, \'results/test_shadow_camera/target.png\')\ntarget = pyredner.imread(\'results/test_shadow_camera/target.exr\')\n\n# Perturb the scene, this is our initial guess\nwith tf.device(pyredner.get_device_name()):\n    position = tf.Variable([-2.0, 7.0, 2.0], trainable=True)\nscene.camera = pyredner.Camera(position = position,\n                               look_at = look_at,\n                               up = up,\n                               fov = fov,\n                               clip_near = clip_near,\n                               resolution = resolution)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\n# Render the initial guess\nimg = pyredner.render(1, *scene_args)\npyredner.imwrite(img, \'results/test_shadow_camera/init.png\')\ndiff = tf.abs(target - img)\npyredner.imwrite(diff, \'results/test_shadow_camera/init_diff.png\')\n\n# Optimize for blocker vertices\noptimizer = tf.compat.v1.train.AdamOptimizer(5e-2)\nfor t in range(200):\n    print(\'iteration:\', t)\n    \n    with tf.GradientTape() as tape:\n        # Forward pass: render the image\n        # Need to rerun the Camera constructor for PyTorch autodiff to compute the derivatives\n        scene.camera = pyredner.Camera(position   = position,\n                                       look_at    = look_at,\n                                       up         = up,\n                                       fov        = fov,\n                                       clip_near  = clip_near,\n                                       resolution = resolution)\n        scene_args = pyredner.serialize_scene(\n            scene = scene,\n            num_samples = 4,\n            max_bounces = 1)\n        img = pyredner.render(t+1, *scene_args)\n        pyredner.imwrite(img, \'results/test_shadow_camera/iter_{}.png\'.format(t))\n        loss = tf.reduce_sum(tf.square(img - target))\n    print(\'loss:\', loss)\n\n    grads = tape.gradient(loss, [position])\n\n    optimizer.apply_gradients(zip(grads, [position]))\n    print(\'position.grad:\', grads[0])\n    print(\'position:\', position)\n\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\nimg = pyredner.render(202, *scene_args)\npyredner.imwrite(img, \'results/test_shadow_camera/final.exr\')\npyredner.imwrite(img, \'results/test_shadow_camera/final.png\')\npyredner.imwrite(tf.abs(target - img), \'results/test_shadow_camera/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_shadow_camera/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_shadow_camera/out.mp4""])\n'"
tests_tensorflow/test_shadow_glossy.py,33,"b'# Tensorflow by default allocates all GPU memory, leaving very little for rendering.\n# We set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true to enforce on demand\n# memory allocation to reduce page faults.\nimport os\nos.environ[\'TF_FORCE_GPU_ALLOW_GROWTH\'] = \'true\'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport pyredner_tensorflow as pyredner\n\n# Optimize four vertices of a shadow blocker, where the receiver is highly glossy\n\n# Use GPU if available\npyredner.set_use_gpu(tf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None))\n\n# Set up the scene\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    position = tf.Variable([0.0, 2.0, -4.0], dtype=tf.float32)\n    look_at = tf.Variable([0.0, -2.0, 0.0], dtype=tf.float32)\n    up = tf.Variable([0.0, 1.0, 0.0], dtype=tf.float32)\n    fov = tf.Variable([45.0], dtype=tf.float32)\n    clip_near = 1e-2\n    resolution = (256, 256)\n    cam = pyredner.Camera(position = position,\n                          look_at = look_at,\n                          up = up,\n                          fov = fov,\n                          clip_near = clip_near,\n                          resolution = resolution)\n\nwith tf.device(pyredner.get_device_name()):\n    mat_shiny = pyredner.Material(\n        diffuse_reflectance = tf.Variable([0.0, 0.0, 0.0], dtype=tf.float32),\n        specular_reflectance = tf.Variable([1.0, 1.0, 1.0], dtype=tf.float32),\n        roughness = tf.Variable([0.0005], dtype=tf.float32))\n    mat_grey = pyredner.Material(\n        diffuse_reflectance = tf.Variable([0.5, 0.5, 0.5], dtype=tf.float32))\n    mat_black = pyredner.Material(\n        diffuse_reflectance = tf.Variable([0.0, 0.0, 0.0], dtype=tf.float32))\n    materials = [mat_shiny, mat_grey, mat_black]\n\n    #############################################\n    # Shapes\n\n    # tf.constant allocates arrays on host memory for int32 arrays (some tensorflow internal mess),\n    # but pyredner.Shape constructor automatically converts the memory to device if necessary.\n\n    # floor\n    floor_vertices = tf.Variable([[-4.0,0.0,-4.0],[-4.0,0.0,4.0],[4.0,0.0,-4.0],[4.0,0.0,4.0]],\n        dtype=tf.float32)\n    floor_indices = tf.constant([[0,1,2], [1,3,2]], dtype=tf.int32)\n    shape_floor = pyredner.Shape(floor_vertices, floor_indices, 0)\n\n    # blocker\n    blocker_vertices = tf.Variable([[0.0,1.0,0.5],[0.0,3.0,0.5],[0.8,1.0,0.5],[0.8,3.0,0.5]],\n        dtype=tf.float32)\n    blocker_indices = tf.constant([[0,1,2], [1,3,2]], dtype=tf.int32)\n    shape_blocker = pyredner.Shape(blocker_vertices, blocker_indices, 1)\n\n    # light\n    light_vertices = tf.Variable([[-2.0,0.0,4.0],[-2.0,12.0,4.0],[2.0,0.0,4.0],[2.0,12.0,4.0]],\n        dtype=tf.float32)\n    light_indices = tf.constant([[0,1,2], [1,3,2]], dtype=tf.int32)\n    shape_light = pyredner.Shape(light_vertices, light_indices, 2)\n\n    shapes = [shape_floor, shape_blocker, shape_light]\n\n###############################################\n# Light\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    light_intensity = tf.Variable([0.5, 0.5, 0.5], dtype=tf.float32)\n# The first argument is the shape id of the light\nlight = pyredner.AreaLight(2, light_intensity)\narea_lights = [light]\n\n###############################################\n# Scene\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\n\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 1024,\n    max_bounces = 1)\n\n# Alias of the render function\n\n# Render our target\nimg = pyredner.render(0, *scene_args)\npyredner.imwrite(img, \'results/test_shadow_glossy/target.exr\')\npyredner.imwrite(img, \'results/test_shadow_glossy/target.png\')\ntarget = pyredner.imread(\'results/test_shadow_glossy/target.exr\')\n\n# Perturb the scene, this is our initial guess\nwith tf.device(pyredner.get_device_name()):\n    shape_blocker.vertices = tf.Variable(\n        [[-0.6,0.9,0.4],[-0.8,3.3,0.7],[0.2,1.1,0.6],[0.3,3.2,0.4]],\n        dtype=tf.float32,\n        trainable=True)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 1024,\n    max_bounces = 1)\n# Render the initial guess\nimg = pyredner.render(1, *scene_args)\npyredner.imwrite(img, \'results/test_shadow_glossy/init.png\')\ndiff = tf.abs(target - img)\npyredner.imwrite(diff, \'results/test_shadow_glossy/init_diff.png\')\n\n# Optimize for blocker vertices\noptimizer = tf.compat.v1.train.AdamOptimizer(2e-2)\nfor t in range(200):\n    print(\'iteration:\', t)\n    scene_args = pyredner.serialize_scene(\n        scene = scene,\n        num_samples = 4,\n        max_bounces = 1)\n    \n    with tf.GradientTape() as tape:\n        # Forward pass: render the image\n        img = pyredner.render(t+1, *scene_args)\n        pyredner.imwrite(img, \'results/test_shadow_glossy/iter_{}.png\'.format(t))\n        loss = tf.reduce_sum(tf.square(img - target))\n    print(\'loss:\', loss)\n\n    grads = tape.gradient(loss, [shape_blocker.vertices])\n    print(\'grad:\', grads)\n\n    optimizer.apply_gradients(zip(grads, [shape_blocker.vertices]))\n    print(\'vertices:\', shape_blocker.vertices)\n\n\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 1024,\n    max_bounces = 1)\nimg = pyredner.render(202, *scene_args)\npyredner.imwrite(img, \'results/test_shadow_glossy/final.exr\')\npyredner.imwrite(img, \'results/test_shadow_glossy/final.png\')\npyredner.imwrite(tf.abs(target - img), \'results/test_shadow_glossy/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_shadow_glossy/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_shadow_glossy/out.mp4""])\n'"
tests_tensorflow/test_shadow_light.py,28,"b'# Tensorflow by default allocates all GPU memory, leaving very little for rendering.\n# We set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true to enforce on demand\n# memory allocation to reduce page faults.\nimport os\nos.environ[\'TF_FORCE_GPU_ALLOW_GROWTH\'] = \'true\'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport pyredner_tensorflow as pyredner\n\n# Optimize light translation to match shadow\n\n# Use GPU if available\npyredner.set_use_gpu(tf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None))\n\n# Set up the scene\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    position = tf.Variable([0.0, 2.0, -5.0], dtype=tf.float32)\n    look_at = tf.Variable([0.0, 0.0, 0.0], dtype=tf.float32)\n    up = tf.Variable([0.0, 1.0, 0.0], dtype=tf.float32)\n    fov = tf.Variable([45.0], dtype=tf.float32)\n    clip_near = 1e-2\n    resolution = (256, 256)\n    cam = pyredner.Camera(position = position,\n                          look_at = look_at,\n                          up = up,\n                          fov = fov,\n                          clip_near = clip_near,\n                          resolution = resolution)\n\nwith tf.device(pyredner.get_device_name()):\n    mat_grey = pyredner.Material(\n        diffuse_reflectance = tf.Variable([0.5, 0.5, 0.5], dtype=tf.float32))\n    mat_black = pyredner.Material(\n        diffuse_reflectance = tf.Variable([0.0, 0.0, 0.0], dtype=tf.float32))\n    materials = [mat_grey, mat_black]\n\n    floor_vertices = tf.Variable([[-2.0,0.0,-2.0],[-2.0,0.0,2.0],[2.0,0.0,-2.0],[2.0,0.0,2.0]],\n\tdtype=tf.float32)\n    floor_indices = tf.constant([[0,1,2], [1,3,2]], dtype=tf.int32)\n    shape_floor = pyredner.Shape(floor_vertices, floor_indices, 0)\n    blocker_vertices = tf.Variable([[-0.5,3.0,-0.5],[-0.5,3.0,0.5],[0.5,3.0,-0.5],[0.5,3.0,0.5]],\n        dtype=tf.float32)\n    blocker_indices = tf.constant([[0,1,2], [1,3,2]], dtype=tf.int32)\n    shape_blocker = pyredner.Shape(blocker_vertices, blocker_indices, 0)\n    light_vertices = tf.Variable([[-0.1,5,-0.1],[-0.1,5,0.1],[0.1,5,-0.1],[0.1,5,0.1]],\n        dtype=tf.float32)\n    light_indices = tf.constant([[0,2,1], [1,2,3]], dtype=tf.int32)\n    shape_light = pyredner.Shape(light_vertices, light_indices, 1)\n    shapes = [shape_floor, shape_blocker, shape_light]\n\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    light_intensity = tf.Variable([1000.0, 1000.0, 1000.0], dtype=tf.float32)\n# The first argument is the shape id of the light\nlight = pyredner.AreaLight(2, light_intensity)\narea_lights = [light]\n\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\n\n# Alias of the render function\n\n# Render our target\nimg = pyredner.render(0, *scene_args)\npyredner.imwrite(img, \'results/test_shadow_light/target.exr\')\npyredner.imwrite(img, \'results/test_shadow_light/target.png\')\ntarget = pyredner.imread(\'results/test_shadow_light/target.exr\')\n\n# Perturb the scene, this is our initial guess\nwith tf.device(pyredner.get_device_name()):\n    light_translation = tf.Variable([-0.4, -0.4, -0.4], dtype=tf.float32, trainable=True)\n    shape_light.vertices = light_vertices + light_translation\n\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\n# Render the initial guess\nimg = pyredner.render(1, *scene_args)\npyredner.imwrite(img, \'results/test_shadow_light/init.png\')\ndiff = tf.abs(target - img)\npyredner.imwrite(diff, \'results/test_shadow_light/init_diff.png\')\n\n# Optimize for blocker vertices\noptimizer = tf.compat.v1.train.AdamOptimizer(1e-2)\nfor t in range(200):\n    print(\'iteration:\', t)\n\n    with tf.GradientTape() as tape:\n        # Forward pass: render the image\n        shape_light.vertices = light_vertices + light_translation\n        scene_args = pyredner.serialize_scene(\n            scene = scene,\n            num_samples = 4,\n            max_bounces = 1)\n        img = pyredner.render(t+1, *scene_args)\n        pyredner.imwrite(img, \'results/test_shadow_light/iter_{}.png\'.format(t))\n        loss = tf.reduce_sum(tf.square(img - target))\n    print(\'loss:\', loss)\n\n    grads = tape.gradient(loss, [light_translation])\n\n    optimizer.apply_gradients(\n        zip(grads, [light_translation])\n        )\n    print(\'light_translation.grad:\', grads[0])\n    print(\'light_translation:\', light_translation)\n\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\nimg = pyredner.render(202, *scene_args)\npyredner.imwrite(img, \'results/test_shadow_light/final.exr\')\npyredner.imwrite(img, \'results/test_shadow_light/final.png\')\npyredner.imwrite(tf.abs(target - img), \'results/test_shadow_light/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_shadow_light/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_shadow_light/out.mp4""])\n'"
tests_tensorflow/test_shadow_receiver.py,28,"b'# Tensorflow by default allocates all GPU memory, leaving very little for rendering.\n# We set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true to enforce on demand\n# memory allocation to reduce page faults.\nimport os\nos.environ[\'TF_FORCE_GPU_ALLOW_GROWTH\'] = \'true\'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport pyredner_tensorflow as pyredner\n\n# Optimize four vertices of a shadow receiver\n\n# Use GPU if available\npyredner.set_use_gpu(tf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None))\n\n# Set up the scene\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    position = tf.Variable([0.0, 2.0, -5.0], dtype=tf.float32)\n    look_at = tf.Variable([0.0, 0.0, 0.0], dtype=tf.float32)\n    up = tf.Variable([0.0, 1.0, 0.0], dtype=tf.float32)\n    fov = tf.Variable([45.0], dtype=tf.float32)\n    clip_near = 1e-2\n    resolution = (256, 256)\n    cam = pyredner.Camera(position = position,\n                          look_at = look_at,\n                          up = up,\n                          fov = fov,\n                          clip_near = clip_near,\n                          resolution = resolution)\n\nwith tf.device(pyredner.get_device_name()):\n    mat_grey = pyredner.Material(\n        diffuse_reflectance = tf.Variable([0.5, 0.5, 0.5], dtype=tf.float32))\n    mat_black = pyredner.Material(\n        diffuse_reflectance = tf.Variable([0.0, 0.0, 0.0], dtype=tf.float32))\n    materials = [mat_grey, mat_black]\n\n    floor_vertices = tf.Variable([[-2.0,0.0,-2.0],[-2.0,0.0,2.0],[2.0,0.0,-2.0],[2.0,0.0,2.0]],\n\tdtype=tf.float32)\n    floor_indices = tf.constant([[0,1,2], [1,3,2]], dtype=tf.int32)\n    shape_floor = pyredner.Shape(floor_vertices, floor_indices, 0)\n    blocker_vertices = tf.Variable([[-0.5,3.0,-0.5],[-0.5,3.0,0.5],[0.5,3.0,-0.5],[0.5,3.0,0.5]],\n        dtype=tf.float32)\n    blocker_indices = tf.constant([[0,1,2], [1,3,2]], dtype=tf.int32)\n    shape_blocker = pyredner.Shape(blocker_vertices, blocker_indices, 0)\n    light_vertices = tf.Variable([[-0.1,5,-0.1],[-0.1,5,0.1],[0.1,5,-0.1],[0.1,5,0.1]],\n        dtype=tf.float32)\n    light_indices = tf.constant([[0,2,1], [1,2,3]], dtype=tf.int32)\n    shape_light = pyredner.Shape(light_vertices, light_indices, 1)\n    shapes = [shape_floor, shape_blocker, shape_light]\n\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    light_intensity = tf.Variable([1000.0, 1000.0, 1000.0], dtype=tf.float32)\n# The first argument is the shape id of the light\nlight = pyredner.AreaLight(2, light_intensity)\narea_lights = [light]\n\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\n\n# Alias of the render function\n\n# Render our target\nimg = pyredner.render(0, *scene_args)\npyredner.imwrite(img, \'results/test_shadow_receiver/target.exr\')\npyredner.imwrite(img, \'results/test_shadow_receiver/target.png\')\ntarget = pyredner.imread(\'results/test_shadow_receiver/target.exr\')\n\n# Perturb the scene, this is our initial guess\nwith tf.device(pyredner.get_device_name()):\n    shape_floor.vertices = tf.Variable(\n    \t[[-2.0,-0.2,-2.0],[-2.0,-0.2,2.0],[2.0,-0.2,-2.0],[2.0,-0.2,2.0]],\n        trainable=True)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\n# Render the initial guess\nimg = pyredner.render(1, *scene_args)\npyredner.imwrite(img, \'results/test_shadow_receiver/init.png\')\ndiff = tf.abs(target - img)\npyredner.imwrite(diff, \'results/test_shadow_receiver/init_diff.png\')\n\n# Optimize for blocker vertices\n# optimizer = torch.optim.Adam([shape_floor.vertices], lr=1e-2)\noptimizer = tf.compat.v1.train.AdamOptimizer(1e-2)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 4,\n    max_bounces = 1)\nfor t in range(200):\n    print(\'iteration:\', t)\n    \n    with tf.GradientTape() as tape:\n    # Forward pass: render the image\n        img = pyredner.render(t+1, *scene_args)\n        pyredner.imwrite(img, \'results/test_shadow_receiver/iter_{}.png\'.format(t))\n        loss = tf.reduce_sum(tf.square(img - target))\n    print(\'loss:\', loss)\n\n    grads = tape.gradient(loss, [shape_floor.vertices])\n\n    optimizer.apply_gradients(\n        zip(grads, [shape_floor.vertices])\n        )\n\n    print(\'grad:\', grads[0])\n    print(\'vertices:\', shape_floor.vertices)\n\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\nimg = pyredner.render(202, *scene_args)\npyredner.imwrite(img, \'results/test_shadow_receiver/final.exr\')\npyredner.imwrite(img, \'results/test_shadow_receiver/final.png\')\npyredner.imwrite(tf.abs(target - img), \'results/test_shadow_receiver/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_shadow_receiver/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_shadow_receiver/out.mp4""])\n'"
tests_tensorflow/test_single_triangle.py,28,"b'# Tensorflow by default allocates all GPU memory, leaving very little for rendering.\n# We set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true to enforce on demand\n# memory allocation to reduce page faults.\nimport os\nos.environ[\'TF_FORCE_GPU_ALLOW_GROWTH\'] = \'true\'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport pyredner_tensorflow as pyredner\n\n# Optimize three vertices of a single triangle\n# We first render a target image, then perturb the three vertices and optimize\n# to match the target.\n\n# Use GPU if available\npyredner.set_use_gpu(tf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None))\n\n# Set up the pyredner scene for rendering:\n\n# First, we set up the camera.\n# redner assumes all the camera variables live in CPU memory.\n# You can allocate the tensors in CPU in the first place, or pyredner automatically converts them\n# in Camera\'s constructor.\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    cam = pyredner.Camera(position = tf.Variable([0.0, 0.0, -5.0], dtype=tf.float32),\n                          look_at = tf.Variable([0.0, 0.0, 0.0], dtype=tf.float32),\n                          up = tf.Variable([0.0, 1.0, 0.0], dtype=tf.float32),\n                          fov = tf.Variable([45.0], dtype=tf.float32), # in degree\n                          clip_near = 1e-2, # needs to > 0\n                          resolution = (256, 256),\n                          fisheye = False)\n\n# Next, we setup the materials for the scene.\n# All materials in the scene are stored in a Python list,\n# the index of a material in the list is its material id.\n# Our simple scene only has a single grey material with reflectance 0.5.\n# You can allocate the reflectance in GPU in the first place, or pyredner automatically converts them\n# in pyredner\'s constructor.\nwith tf.device(pyredner.get_device_name()):\n    mat_grey = pyredner.Material(\n        diffuse_reflectance = tf.Variable([0.5, 0.5, 0.5], dtype=tf.float32))\n# The material list of the scene\nmaterials = [mat_grey]\n\n# Next, we setup the geometry for the scene.\n# 3D objects in redner are called ""Shape"".\n# All shapes in the scene are stored in a Python list,\n# the index of a shape in the list is its shape id.\n# Right now, a shape is always a triangle mesh, which has a list of\n# triangle vertices and a list of triangle indices.\n# The vertices are a Nx3 torch float tensor,\n# and the indices are a Mx3 torch integer tensor.\n# Optionally, for each vertex you can specify its UV coordinate for texture mapping,\n# and a normal for Phong interpolation.\n# Each shape also needs to be assigned a material using material id,\n# which is the index of the material in the material array.\nwith tf.device(pyredner.get_device_name()):\n    # tf.constant allocates arrays on host memory for int32 arrays (some tensorflow internal mess),\n    # but pyredner.Shape constructor automatically converts the memory to device if necessary.\n    shape_triangle = pyredner.Shape(\n        vertices = tf.Variable([[-1.7, 1.0, 0.0], [1.0, 1.0, 0.0], [-0.5, -1.0, 0.0]],\n            dtype=tf.float32),\n        indices = tf.constant([[0, 1, 2]], dtype=tf.int32),\n        uvs = None,\n        normals = None,\n        material_id = 0)\n    # Merely having a single triangle is not enough for physically-based rendering.\n    # We need to have a light source. Here we setup the shape of a quad area light source,\n    # similary to the previous triangle.\n    shape_light = pyredner.Shape(\n        vertices = tf.Variable([[-1.0, -1.0, -7.0],\n                                [ 1.0, -1.0, -7.0],\n                                [-1.0,  1.0, -7.0],\n                                [ 1.0,  1.0, -7.0]], dtype=tf.float32),\n        indices = tf.constant([[0, 1, 2],[1, 3, 2]], dtype=tf.int32),\n        uvs = None,\n        normals = None,\n        material_id = 0)\n# The shape list of the scene\nshapes = [shape_triangle, shape_light]\n\n# Now we assign some of the shapes in the scene as light sources.\n# Again, all the area light sources in the scene are stored in a Python list.\n# Each area light is attached to a shape using shape id, additionally we need to\n# assign the intensity of the light, which is a length 3 float tensor in CPU. \nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    light = pyredner.AreaLight(shape_id = 1, \n                               intensity = tf.Variable([20.0,20.0,20.0], dtype=tf.float32))\narea_lights = [light]\n\n# Finally we construct our scene using all the variables we setup previously.\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\n# All TensorFlow functions take a flat array of TensorFlow tensors as input,\n# therefore we need to serialize the scene into an array. The following\n# function is doing this. We also specify how many Monte Carlo samples we want to \n# use per pixel and the number of bounces for indirect illumination here\n# (one bounce means only direct illumination).\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\n\n# Render the scene as our target image.\n# To render the scene, we use our custom PyTorch function in pyredner/render_pytorch.py\n# First setup the alias of the render function\n\n# Render. The first argument is the seed for RNG in the renderer.\n# Redner automatically maps the devices in the render function, so no need to specify tf.device here.\nimg = pyredner.render(0, *scene_args)\n# Save the images.\npyredner.imwrite(img, \'results/test_single_triangle/target.exr\')\npyredner.imwrite(img, \'results/test_single_triangle/target.png\')\n# Read the target image we just saved.\ntarget = pyredner.imread(\'results/test_single_triangle/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.gpu()\n\n# Perturb the scene, this is our initial guess.\nwith tf.device(pyredner.get_device_name()):\n    shape_triangle.vertices = tf.Variable(\n        [[-2.0,1.5,0.3], [0.9,1.2,-0.3], [-0.4,-1.4,0.2]],\n        dtype=tf.float32,\n        trainable=True) # Set trainable to True since we want to optimize this\n# We need to serialize the scene again to get the new arguments.\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\n# Render the initial guess.\nimg = pyredner.render(1, *scene_args)\n# Save the images.\npyredner.imwrite(img, \'results/test_single_triangle/init.png\')\n# Compute the difference and save the images.\ndiff = tf.abs(target - img)\npyredner.imwrite(diff, \'results/test_single_triangle/init_diff.png\')\n\n# Optimize for triangle vertices.\noptimizer = tf.compat.v1.train.AdamOptimizer(5e-2)\n\ndef loss(output, target):\n    # Compute the loss function. Here it is L2.\n    error = output - target\n    return tf.reduce_sum(tf.square(error))\n\ndef optimize(scene_args, grads, lr=5e-2):\n    updates = []\n    for var, grad in zip(scene_args, grads):\n        if grad is None: \n            updates.append(var)\n            continue\n        # print(grad)\n        # var -= lr * grad\n        updates.append(var - lr * grad)\n\n    return updates\n\n# Run 200 Adam iterations.\nfor t in range(1, 201):\n    print(\'iteration:\', t)\n\n    with tf.GradientTape() as tape:\n        # Forward pass: render the image.\n        \n        # Important to use a different seed every iteration, otherwise the result\n        # would be biased.\n        scene_args = pyredner.serialize_scene(\n            scene = scene,\n            num_samples = 4, # We use less samples in the Adam loop.\n            max_bounces = 1)\n        img = pyredner.render(t, *scene_args)\n        loss_value = loss(img, target)\n\n    print(f""loss_value: {loss_value}"")\n    pyredner.imwrite(img, \'results/test_single_triangle/iter_{}.png\'.format(t))\n\n    grads = tape.gradient(loss_value, [shape_triangle.vertices])\n    print(grads)\n    optimizer.apply_gradients(zip(grads, [shape_triangle.vertices]))\n\n    print(\'grad:\', grads[0])\n    print(\'vertices:\', shape_triangle.vertices)\n\n# Render the final result.\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\nimg = pyredner.render(202, *scene_args)\n# Save the images and differences.\npyredner.imwrite(img, \'results/test_single_triangle/final.exr\')\npyredner.imwrite(img, \'results/test_single_triangle/final.png\')\npyredner.imwrite(tf.abs(target - img), \'results/test_single_triangle/final_diff.png\')\n\n# Convert the intermediate renderings to a video.\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_single_triangle/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_single_triangle/out.mp4""])\n'"
tests_tensorflow/test_single_triangle_background.py,27,"b'# Tensorflow by default allocates all GPU memory, leaving very little for rendering.\n# We set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true to enforce on demand\n# memory allocation to reduce page faults.\nimport os\nos.environ[\'TF_FORCE_GPU_ALLOW_GROWTH\'] = \'true\'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport pyredner_tensorflow as pyredner\nimport skimage.transform\nimport redner\n\n# Optimize three vertices of a single triangle, with a SIGGRAPH logo background\n\n# Use GPU if available\npyredner.set_use_gpu(tf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None))\n\n# Set up the pyredner scene for rendering:\n\n# Setup camera\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    cam = pyredner.Camera(position = tf.Variable([0.0, 0.0, -5.0], dtype=tf.float32),\n                          look_at = tf.Variable([0.0, 0.0, 0.0], dtype=tf.float32),\n                          up = tf.Variable([0.0, 1.0, 0.0], dtype=tf.float32),\n                          fov = tf.Variable([45.0], dtype=tf.float32), # in degree\n                          clip_near = 1e-2, # needs to > 0\n                          resolution = (256, 256),\n                          fisheye = False)\n\n# Setup materials\nwith tf.device(pyredner.get_device_name()):\n    mat_grey = pyredner.Material(\n        diffuse_reflectance = tf.Variable([0.5, 0.5, 0.5], dtype=tf.float32))\n    # The material list of the scene\n    materials = [mat_grey]\n\n    # Setup geometries\n    shape_triangle = pyredner.Shape(\n        vertices = tf.Variable([[-1.7, 1.0, 0.0], [1.0, 1.0, 0.0], [-0.5, -1.0, 0.0]],\n            dtype=tf.float32),\n        indices = tf.constant([[0, 1, 2]], dtype=tf.int32),\n        uvs = None,\n        normals = None,\n        material_id = 0)\n    # Setup light source shape\n    shape_light = pyredner.Shape(\n        vertices = tf.Variable([[-1.0, -1.0, -7.0],\n                                [ 1.0, -1.0, -7.0],\n                                [-1.0,  1.0, -7.0],\n                                [ 1.0,  1.0, -7.0]], dtype=tf.float32),\n        indices = tf.constant([[0, 1, 2],[1, 3, 2]], dtype=tf.int32),\n        uvs = None,\n        normals = None,\n        material_id = 0)\n# The shape list of the scene\nshapes = [shape_triangle, shape_light]\n\n# Setup light source\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    light = pyredner.AreaLight(shape_id = 1, \n                               intensity = tf.Variable([20.0,20.0,20.0], dtype=tf.float32))\narea_lights = [light]\n\n# Construct the scene\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\n# Serialize the scene\n# Here we specify the output channels as ""radiance"" and ""alpha""\n# Render the scene as our target image.\nscene_args = pyredner.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1,\n    channels = [redner.channels.radiance, redner.channels.alpha])\n\n# Render the scene as our target image.\n# Render. The first argument is the seed for RNG in the renderer.\nimg = pyredner.render(0, *scene_args)\n\nbackground = pyredner.imread(\'scenes/textures/siggraph.jpg\')\nbackground = tf.convert_to_tensor(skimage.transform.resize(background.numpy(), (256, 256, 3)), dtype=tf.float32)\nimg = img[:, :, :3] * img[:, :, 3:4] + background * (1 - img[:, :, 3:4])\n\n# Save the images.\n# The output image is in the GPU memory if you are using GPU.\npyredner.imwrite(img, \'results/test_single_triangle_background/target.exr\')\npyredner.imwrite(img, \'results/test_single_triangle_background/target.png\')\n# Read the target image we just saved.\ntarget = pyredner.imread(\'results/test_single_triangle_background/target.exr\')\n\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1,\n    channels = [redner.channels.radiance, redner.channels.alpha])\n\n# Render. The first argument is the seed for RNG in the renderer.\nimg = pyredner.render(0, *scene_args)\n# Since we specified alpha as output channel, img has 4 channels now\n# We blend the image with a background image\nimg = img[:, :, :3] * img[:, :, 3:4] + background * (1 - img[:, :, 3:4])\n\n# Save the images.\n# The output image is in the GPU memory if you are using GPU.\npyredner.imwrite(img, \'results/test_single_triangle_background/target.exr\')\npyredner.imwrite(img, \'results/test_single_triangle_background/target.png\')\n\n# Perturb the scene, this is our initial guess.\nwith tf.device(pyredner.get_device_name()):\n    shape_triangle.vertices = tf.Variable(\n        [[-2.0,1.5,0.3], [0.9,1.2,-0.3], [-0.4,-1.4,0.2]],\n        dtype=tf.float32,\n        trainable=True)\n# We need to serialize the scene again to get the new arguments.\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1,\n    channels = [redner.channels.radiance, redner.channels.alpha])\n\n# Render the initial guess.\nimg = pyredner.render(1, *scene_args)\n# Blend the image with a background image\nimg = img[:, :, :3] * img[:, :, 3:4] + background * (1 - img[:, :, 3:4])\n# Save the images.\npyredner.imwrite(img, \'results/test_single_triangle_background/init.png\')\n# Compute the difference and save the images.\ndiff = tf.abs(target - img)\npyredner.imwrite(diff, \'results/test_single_triangle_background/init_diff.png\')\n\n# Optimize for triangle vertices.\noptimizer = tf.compat.v1.train.AdamOptimizer(5e-2)\n# Run 300 Adam iterations.\nfor t in range(300):\n    print(\'iteration:\', t)\n\n    scene_args = pyredner.serialize_scene(\n            scene = scene,\n            num_samples = 4, # We use less samples in the Adam loop.\n            max_bounces = 1,\n            channels = [redner.channels.radiance, redner.channels.alpha])\n    \n    with tf.GradientTape() as tape:\n        # Forward pass: render the image.\n        # Important to use a different seed every iteration, otherwise the result\n        # would be biased.\n        img = pyredner.render(t+1, *scene_args)\n        img4c = tf.identity(img)\n        # Blend the image with a background image\n        img = img4c[:, :, :3] * img4c[:, :, 3:4] + background * (1 - img4c[:, :, 3:4])\n        # Save the intermediate render.\n        pyredner.imwrite(img, \'results/test_single_triangle_background/iter_{}.png\'.format(t))\n        # Compute the loss function. Here it is L2.\n        loss = tf.reduce_sum(tf.square(img - target))\n\n    print(\'loss:\', loss)\n\n    grads = tape.gradient(loss, [shape_triangle.vertices])\n    print(\'grad:\', grads)\n    optimizer.apply_gradients(zip(grads, [shape_triangle.vertices]))\n\n    # Print the current three vertices.\n    print(\'vertices:\', shape_triangle.vertices)\n\n# Render the final result.\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1,\n    channels = [redner.channels.radiance, redner.channels.alpha])\nimg = pyredner.render(302, *scene_args)\n# Blend the image with a background image\nimg = img[:, :, :3] * img[:, :, 3:4] + background * (1 - img[:, :, 3:4])\n# Save the images and differences.\npyredner.imwrite(img, \'results/test_single_triangle_background/final.exr\')\npyredner.imwrite(img, \'results/test_single_triangle_background/final.png\')\npyredner.imwrite(tf.abs(target - img), \'results/test_single_triangle_background/final_diff.png\')\n\n# Convert the intermediate renderings to a video.\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_single_triangle_background/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_single_triangle_background/out.mp4""])\n'"
tests_tensorflow/test_single_triangle_camera.py,25,"b'# Tensorflow by default allocates all GPU memory, leaving very little for rendering.\n# We set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true to enforce on demand\n# memory allocation to reduce page faults.\nimport os\nos.environ[\'TF_FORCE_GPU_ALLOW_GROWTH\'] = \'true\'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport pyredner_tensorflow as pyredner\n\n# Optimize camera parameters of a single triangle rendering\n\n# Use GPU if available\npyredner.set_use_gpu(tf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None))\n\n# Set up the scene\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    position = tf.Variable([0.0, 0.0, -5.0], dtype=tf.float32)\n    look_at = tf.Variable([0.0, 0.0, 0.0], dtype=tf.float32)\n    up = tf.Variable([0.0, 1.0, 0.0], dtype=tf.float32)\n    fov = tf.Variable([45.0], dtype=tf.float32) # in degree\n    clip_near = 1e-2 # needs to > 0\n    resolution = (256, 256)\n\n    cam = pyredner.Camera(position = position,\n                          look_at = look_at,\n                          up = up,\n                          fov = fov,\n                          clip_near = clip_near,\n                          resolution = (256, 256))\n\nwith tf.device(pyredner.get_device_name()):\n    mat_grey = pyredner.Material(\n        diffuse_reflectance = tf.Variable([0.5, 0.5, 0.5], dtype=tf.float32))\n    materials = [mat_grey]\n    vertices = tf.Variable([[-1.7,1.0,0.0], [1.0,1.0,0.0], [-0.5,-1.0,0.0]],\n        dtype=tf.float32)\n    indices = tf.constant([[0, 1, 2]], dtype=tf.int32)\n    shape_triangle = pyredner.Shape(vertices, indices, 0)\n    light_vertices = tf.Variable([[-1.0,-1.0,-9.0],[1.0,-1.0,-9.0],[-1.0,1.0,-9.0],[1.0,1.0,-9.0]],\n        dtype=tf.float32)\n    light_indices = tf.constant([[0,1,2],[1,3,2]], dtype=tf.int32)\n    shape_light = pyredner.Shape(light_vertices, light_indices, 0)\n    shapes = [shape_triangle, shape_light]\n\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    light_intensity = tf.Variable([30.0,30.0,30.0],dtype=tf.float32)\nlight = pyredner.AreaLight(1, light_intensity)\narea_lights = [light]\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\n\n# Alias of the render function\n\n# Render our target\nimg = pyredner.render(0, *scene_args)\npyredner.imwrite(img, \'results/test_single_triangle_camera/target.exr\')\npyredner.imwrite(img, \'results/test_single_triangle_camera/target.png\')\ntarget = pyredner.imread(\'results/test_single_triangle_camera/target.exr\')\n\n# Perturb the scene, this is our initial guess\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    position = tf.Variable([0.0,  0.0, -3.0], dtype=tf.float32, trainable=True)\n    look_at = tf.Variable([-0.5, -0.5,  0.0], dtype=tf.float32, trainable=True)\nscene.camera = pyredner.Camera(position = position,\n                               look_at = look_at,\n                               up = up,\n                               fov = fov,\n                               clip_near = clip_near,\n                               resolution = resolution)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\n# Render the initial guess\nimg = pyredner.render(1, *scene_args)\npyredner.imwrite(img, \'results/test_single_triangle_camera/init.png\')\ndiff = tf.abs(target - img)\npyredner.imwrite(diff, \'results/test_single_triangle_camera/init_diff.png\')\n\nscene.camera = pyredner.Camera(position = position,\n                               look_at = look_at,\n                               up = up,\n                               fov = fov,\n                               clip_near = clip_near,\n                               resolution = resolution)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 4,\n    max_bounces = 1)\n\n# Optimize for camera pose\noptimizer = tf.compat.v1.train.AdamOptimizer(2e-2)\nfor t in range(200):\n    print(\'iteration:\', t)\n    \n    with tf.GradientTape() as tape:\n        # Need to rerun the Camera constructor for autodiff to compute the derivatives\n        scene.camera = pyredner.Camera(position = position,\n                                       look_at = look_at,\n                                       up = up,\n                                       fov = fov,\n                                       clip_near = clip_near,\n                                       resolution = resolution)\n        scene_args = pyredner.serialize_scene(\n            scene = scene,\n            num_samples = 4,\n            max_bounces = 1)\n\n        img = pyredner.render(t+1, *scene_args)\n        pyredner.imwrite(img, \'results/test_single_triangle_camera/iter_{}.png\'.format(t))\n        loss = tf.reduce_sum(tf.square(img - target))\n    print(\'loss:\', loss)\n\n    grads = tape.gradient(loss, [position, look_at])\n\n    optimizer.apply_gradients(zip(grads, [position, look_at]))\n\n    print(\'position.grad:\', grads[0])\n    print(\'look_at.grad:\', grads[1])\n\n    print(\'position:\', position)\n    print(\'look_at:\', look_at)\n\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\nimg = pyredner.render(202, *scene_args)\npyredner.imwrite(img, \'results/test_single_triangle_camera/final.exr\')\npyredner.imwrite(img, \'results/test_single_triangle_camera/final.png\')\npyredner.imwrite(tf.abs(target - img), \'results/test_single_triangle_camera/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_single_triangle_camera/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_single_triangle_camera/out.mp4""])\n'"
tests_tensorflow/test_single_triangle_camera_fisheye.py,24,"b'# Tensorflow by default allocates all GPU memory, leaving very little for rendering.\n# We set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true to enforce on demand\n# memory allocation to reduce page faults.\nimport os\nos.environ[\'TF_FORCE_GPU_ALLOW_GROWTH\'] = \'true\'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport pyredner_tensorflow as pyredner\n\n# Optimize fisheye camera parameters of a single triangle rendering\n\n# Use GPU if available\npyredner.set_use_gpu(tf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None))\n\n# Set up the scene\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    position = tf.Variable([0.0, 0.0, -1.0], dtype=tf.float32)\n    look_at = tf.Variable([0.0, 0.0, 0.0], dtype=tf.float32)\n    up = tf.Variable([0.0, 1.0, 0.0], dtype=tf.float32)\n    fov = tf.Variable([45.0], dtype=tf.float32)\n    clip_near = 1e-2\n    resolution = (256, 256)\n    cam = pyredner.Camera(position = position,\n                          look_at = look_at,\n                          up = up,\n                          fov = fov,\n                          clip_near = clip_near,\n                          resolution = resolution,\n                          fisheye = True)\n\nwith tf.device(pyredner.get_device_name()):\n    mat_grey = pyredner.Material(diffuse_reflectance = \\\n        tf.Variable([0.5, 0.5, 0.5], dtype=tf.float32))\n    materials = [mat_grey]\n    vertices = tf.Variable([[-1.7,1.0,0.0], [1.0,1.0,0.0], [-0.5,-1.0,0.0]],\n        dtype=tf.float32)\n    indices = tf.constant([[0, 1, 2]], dtype=tf.int32)\n    shape_triangle = pyredner.Shape(vertices, indices, 0)\n    light_vertices = tf.Variable([[-1.0,-1.0,-9.0],[1.0,-1.0,-9.0],[-1.0,1.0,-9.0],[1.0,1.0,-9.0]],\n        dtype=tf.float32)\n    light_indices = tf.constant([[0,1,2],[1,3,2]], dtype=tf.int32)\n    shape_light = pyredner.Shape(light_vertices, light_indices, 0)\nshapes = [shape_triangle, shape_light]\n\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    light_intensity = tf.Variable([30.0,30.0,30.0], dtype=tf.float32)\nlight = pyredner.AreaLight(1, light_intensity)\narea_lights = [light]\n\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\n\n# Render our target\nimg = pyredner.render(0, *scene_args)\npyredner.imwrite(img, \'results/test_single_triangle_camera_fisheye/target.exr\')\npyredner.imwrite(img, \'results/test_single_triangle_camera_fisheye/target.png\')\ntarget = pyredner.imread(\'results/test_single_triangle_camera_fisheye/target.exr\')\n\n# Perturb the scene, this is our initial guess\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    position = tf.Variable([0.5, -0.5, -3.0], trainable=True)\nscene.camera = pyredner.Camera(position = position,\n                               look_at = look_at,\n                               up = up,\n                               fov = fov,\n                               clip_near = clip_near,\n                               resolution = resolution,\n                               fisheye = True)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\n# Render the initial guess\nimg = pyredner.render(1, *scene_args)\npyredner.imwrite(img, \'results/test_single_triangle_camera_fisheye/init.png\')\ndiff = tf.abs(target - img)\npyredner.imwrite(diff, \'results/test_single_triangle_camera_fisheye/init_diff.png\')\n\n# Optimize for camera pose\noptimizer = tf.compat.v1.train.AdamOptimizer(2e-2)\nfor t in range(200):\n    print(\'iteration:\', t)\n    \n    with tf.GradientTape() as tape:\n        # Need to rerun the Camera constructor for PyTorch autodiff to compute the derivatives\n        scene.camera = pyredner.Camera(position   = position,\n                                       look_at    = look_at,\n                                       up         = up,\n                                       fov        = fov,\n                                       clip_near  = clip_near,\n                                       resolution = resolution,\n                                       fisheye    = True)\n        scene_args = pyredner.serialize_scene(\n            scene = scene,\n            num_samples = 4,\n            max_bounces = 1)\n        img = pyredner.render(t+1, *scene_args)\n        pyredner.imwrite(img, \'results/test_single_triangle_camera_fisheye/iter_{}.png\'.format(t))\n        loss = tf.reduce_sum(tf.square(img - target))\n    \n    print(\'loss:\', loss)\n\n    grads = tape.gradient(loss, [position])\n\n    optimizer.apply_gradients(zip(grads, [position]))\n\n    print(\'d_position:\', grads[0])\n    print(\'position:\', position)\n\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\nimg = pyredner.render(202, *scene_args)\npyredner.imwrite(img, \'results/test_single_triangle_camera_fisheye/final.exr\')\npyredner.imwrite(img, \'results/test_single_triangle_camera_fisheye/final.png\')\npyredner.imwrite(tf.abs(target - img), \'results/test_single_triangle_camera_fisheye/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_single_triangle_camera_fisheye/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_single_triangle_camera_fisheye/out.mp4""])\n\n'"
tests_tensorflow/test_single_triangle_clipped.py,25,"b'# Tensorflow by default allocates all GPU memory, leaving very little for rendering.\n# We set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true to enforce on demand\n# memory allocation to reduce page faults.\nimport os\nos.environ[\'TF_FORCE_GPU_ALLOW_GROWTH\'] = \'true\'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport pyredner_tensorflow as pyredner\n\n# Optimize three vertices of a single triangle.\n# One of the vertices is behind the camera\n\n# Use GPU if available\npyredner.set_use_gpu(tf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None))\n\n# Set up the scene\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    position = tf.Variable([0.0, 0.0, -5.0], dtype=tf.float32)\n    look_at = tf.Variable([0.0, 0.0, 0.0], dtype=tf.float32)\n    up = tf.Variable([0.0, 1.0, 0.0], dtype=tf.float32)\n    fov = tf.Variable([45.0], dtype=tf.float32)\n    clip_near = 1e-2\n    resolution = (256, 256)\n    cam = pyredner.Camera(position = position,\n                          look_at = look_at,\n                          up = up,\n                          fov = fov,\n                          clip_near = clip_near,\n                          resolution = resolution)\n\nwith tf.device(pyredner.get_device_name()):\n    mat_grey = pyredner.Material(\n        diffuse_reflectance = tf.Variable([0.5, 0.5, 0.5], dtype=tf.float32))\n    materials = [mat_grey]\n    vertices = tf.Variable([[-1.3,1.0,0.0], [1.0,1.0,0.0], [-0.5,-2.0,-7.0]],\n        dtype=tf.float32)\n    indices = tf.constant([[0, 1, 2]], dtype=tf.int32)\n\n    shape_triangle = pyredner.Shape(vertices, indices, 0)\n    light_vertices = tf.Variable(\n        [[-1.0,-1.0,-7.0],[1.0,-1.0,-7.0],[-1.0,1.0,-7.0],[1.0,1.0,-7.0]], \n        dtype=tf.float32)\n    light_indices = tf.constant([[0,1,2],[1,3,2]], dtype=tf.int32)\n    shape_light = pyredner.Shape(light_vertices, light_indices, 0)\n    shapes = [shape_triangle, shape_light]\n\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    light_intensity = tf.Variable([20.0,20.0,20.0], dtype=tf.float32)\nlight = pyredner.AreaLight(1, light_intensity)\narea_lights = [light]\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\n\n# Render our target\nimg = pyredner.render(0, *scene_args)\npyredner.imwrite(img, \'results/test_single_triangle_clipped/target.exr\')\npyredner.imwrite(img, \'results/test_single_triangle_clipped/target.png\')\ntarget = pyredner.imread(\'results/test_single_triangle_clipped/target.exr\')\n\n# Perturb the scene, this is our initial guess\nwith tf.device(pyredner.get_device_name()):\n    shape_triangle.vertices = tf.Variable(\n        [[-1.0,1.5,0.3], [0.9,1.2,-0.3], [0.0,-3.0,-6.5]],\n        dtype=tf.float32,\n        trainable=True)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\n# Render the initial guess\nimg = pyredner.render(1, *scene_args)\npyredner.imwrite(img, \'results/test_single_triangle_clipped/init.png\')\ndiff = tf.abs(target - img)\npyredner.imwrite(diff, \'results/test_single_triangle_clipped/init_diff.png\')\n\ndef loss(output, target):\n    # Compute the loss function. Here it is L2.\n    error = output - target\n    return tf.reduce_sum(tf.square(error))\n\n# Optimize for triangle vertices\noptimizer = tf.compat.v1.train.AdamOptimizer(2e-2)\nfor t in range(200):\n    print(\'iteration:\', t)\n\n    with tf.GradientTape() as tape:\n        # Forward pass: render the image\n        scene_args = pyredner.serialize_scene(\n            scene = scene,\n            num_samples = 4,\n            max_bounces = 1)\n        img = pyredner.render(t+1, *scene_args)\n        loss_value = loss(img, target)\n    \n    pyredner.imwrite(img, \'results/test_single_triangle_clipped/iter_{}.png\'.format(t))\n    print(f""loss_value: {loss_value}"")    \n\n    grads = tape.gradient(loss_value, [shape_triangle.vertices])\n    optimizer.apply_gradients(zip(grads, [shape_triangle.vertices]))\n    print(\'shape_triangle.vertices.grad:\', grads[0])\n    print(\'vertices:\', shape_triangle.vertices)\n\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\nimg = pyredner.render(202, *scene_args)\npyredner.imwrite(img, \'results/test_single_triangle_clipped/final.exr\')\npyredner.imwrite(img, \'results/test_single_triangle_clipped/final.png\')\npyredner.imwrite(tf.abs(target - img), \'results/test_single_triangle_clipped/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_single_triangle_clipped/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_single_triangle_clipped/out.mp4""])\n'"
tests_tensorflow/test_sphere.py,3,"b""# Tensorflow by default allocates all GPU memory, leaving very little for rendering.\n# We set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true to enforce on demand\n# memory allocation to reduce page faults.\nimport os\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport pyredner_tensorflow as pyredner\n\nvertices, indices, uvs, normals = pyredner.generate_sphere(64, 128)\nm = pyredner.Material(diffuse_reflectance = tf.constant((0.5, 0.5, 0.5)))\nobj = pyredner.Object(vertices = vertices,\n                      indices = indices,\n                      uvs = uvs,\n                      normals = normals,\n                      material = m)\ncam = pyredner.automatic_camera_placement([obj], resolution = (480, 640))\nscene = pyredner.Scene(objects = [obj], camera = cam)\n\nimg = pyredner.render_g_buffer(scene, channels = [pyredner.channels.uv, pyredner.channels.shading_normal])\nuv_img = tf.concat([img[:, :, :2], tf.zeros((480, 640, 1))], axis=2)\nnormal_img = img[:, :, 2:]\npyredner.imwrite(uv_img, 'results/test_sphere/uv.png')\npyredner.imwrite(normal_img, 'results/test_sphere/normal.png')\n"""
tests_tensorflow/test_svbrdf.py,37,"b'# Tensorflow by default allocates all GPU memory, leaving very little for rendering.\n# We set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true to enforce on demand\n# memory allocation to reduce page faults.\nimport os\nos.environ[\'TF_FORCE_GPU_ALLOW_GROWTH\'] = \'true\'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport pyredner_tensorflow as pyredner\nimport numpy as np\n\n# Optimize texels of a textured patch\n\n# Perlin noise code taken from Stackoverflow\n# https://stackoverflow.com/questions/42147776/producing-2d-perlin-noise-with-numpy\ndef perlin(x, y, seed=0):\n    # permutation table\n    np.random.seed(seed)\n    p = np.arange(256, dtype=np.int32)\n    np.random.shuffle(p)\n    p = np.stack([p,p]).flatten()\n    # coordinates of the top-left\n    xi = x.astype(np.int32)\n    yi = y.astype(np.int32)\n    # internal coordinates\n    xf = (x - xi).astype(np.float32)\n    yf = (y - yi).astype(np.float32)\n    # fade factors\n    u = fade(xf)\n    v = fade(yf)\n    # noise components\n    n00 = gradient(p[p[xi]+yi],xf,yf)\n    n01 = gradient(p[p[xi]+yi+1],xf,yf-1)\n    n11 = gradient(p[p[xi+1]+yi+1],xf-1,yf-1)\n    n10 = gradient(p[p[xi+1]+yi],xf-1,yf)\n    # combine noises\n    x1 = lerp(n00, n10, u)\n    x2 = lerp(n01, n11, u)\n    return lerp(x1, x2, v)\n\ndef lerp(a,b,x):\n    return a + x * (b-a)\n\ndef fade(t):\n    return 6 * t**5 - 15 * t**4 + 10 * t**3\n\ndef gradient(h,x,y):\n    vectors = np.array([[0,1],[0,-1],[1,0],[-1,0]], dtype=np.float32)\n    g = vectors[h%4]\n    return g[:,:,0] * x + g[:,:,1] * y\n\nlin = np.linspace(0, 5, 256, endpoint=False, dtype=np.float32)\nx, y = np.meshgrid(lin, lin)\ndiffuse = perlin(x, y, seed=0)\ndiffuse = (diffuse - np.min(diffuse) + 1e-3) / (np.max(diffuse) - np.min(diffuse))\ndiffuse = tf.convert_to_tensor(np.tile(np.reshape(diffuse, (256, 256, 1)), (1, 1, 3)), dtype=tf.float32)\nspecular = perlin(x, y, seed=1)\nspecular = (specular - np.min(specular) + 1e-3) / (np.max(specular) - np.min(specular))\nspecular = tf.convert_to_tensor(np.tile(np.reshape(specular, (256, 256, 1)), (1, 1, 3)), dtype=tf.float32)\nroughness = perlin(x, y, seed=2)\nroughness = (roughness - np.min(roughness) + 1e-3) / (np.max(roughness) - np.min(roughness))\nroughness = tf.convert_to_tensor(np.reshape(roughness, (256, 256, 1)), dtype=tf.float32)\n\n# Use GPU if available\npyredner.set_use_gpu(tf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None))\n\n# Set up the scene using Pytorch tensor\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    position = tf.Variable([0.0, 0.0, -5.0], dtype=tf.float32)\n    look_at = tf.Variable([0.0, 0.0, 0.0], dtype=tf.float32)\n    up = tf.Variable([0.0, 1.0, 0.0], dtype=tf.float32)\n    fov = tf.Variable([45.0], dtype=tf.float32)\n    clip_near = 1e-2\n    resolution = (256, 256)\n    cam = pyredner.Camera(position = position,\n                          look_at = look_at,\n                          up = up,\n                          fov = fov,\n                          clip_near = clip_near,\n                          resolution = resolution)\n\nmat_perlin = pyredner.Material(\n    diffuse_reflectance = diffuse,\n    specular_reflectance = specular,\n    roughness = roughness)\nwith tf.device(pyredner.get_device_name()):\n    mat_black = pyredner.Material(\n        diffuse_reflectance = tf.Variable([0.0, 0.0, 0.0], dtype=tf.float32))\n    materials = [mat_perlin, mat_black]\n    vertices = tf.Variable([[-1.5,-1.5,0.0], [-1.5,1.5,0.0], [1.5,-1.5,0.0], [1.5,1.5,0.0]],\n        dtype=tf.float32)\n    indices = tf.constant([[0, 1, 2], [1, 3, 2]], dtype=tf.int32)\n    uvs = tf.Variable([[0.05, 0.05], [0.05, 0.95], [0.95, 0.05], [0.95, 0.95]],\n        dtype=tf.float32)\n    shape_plane = pyredner.Shape(vertices, indices, 0, uvs)\n    light_vertices = tf.Variable([[-1.0,-1.0,-7.0],[1.0,-1.0,-7.0],[-1.0,1.0,-7.0],[1.0,1.0,-7.0]],\n        dtype=tf.float32)\n    light_indices = tf.constant([[0,1,2],[1,3,2]], dtype=tf.int32)\n    shape_light = pyredner.Shape(light_vertices, light_indices, 1)\nshapes = [shape_plane, shape_light]\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    light_intensity = tf.Variable([20.0, 20.0, 20.0], dtype=tf.float32)\n# The first argument is the shape id of the light\nlight = pyredner.AreaLight(1, light_intensity)\narea_lights = [light]\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\n\n# Render our target\nimg = pyredner.render(0, *scene_args)\npyredner.imwrite(img, \'results/test_svbrdf/target.exr\')\npyredner.imwrite(img, \'results/test_svbrdf/target.png\')\ntarget = pyredner.imread(\'results/test_svbrdf/target.exr\')\n\n# Our initial guess is three gray textures \nwith tf.device(pyredner.get_device_name()):\n    diffuse_tex = tf.Variable(\n        tf.ones((256, 256, 3), dtype=np.float32) * 0.5,\n        trainable=True)\n    specular_tex = tf.Variable(\n        tf.ones((256, 256, 3), dtype=np.float32) * 0.5,\n        trainable=True)\n    roughness_tex = tf.Variable(\n        tf.ones((256, 256, 1), dtype=np.float32) * 0.5,\n        trainable=True)\nmat_perlin.diffuse_reflectance = pyredner.Texture(diffuse_tex)\nmat_perlin.specular_reflectance = pyredner.Texture(specular_tex)\nmat_perlin.roughness = pyredner.Texture(roughness_tex)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\n# Render the initial guess\nimg = pyredner.render(1, *scene_args)\npyredner.imwrite(img, \'results/test_svbrdf/init.png\')\ndiff = tf.abs(target - img)\npyredner.imwrite(diff, \'results/test_svbrdf/init_diff.png\')\n\n# Optimize for triangle vertices\noptimizer = tf.compat.v1.train.AdamOptimizer(1e-2)\nfor t in range(200):\n    print(\'iteration:\', t)\n    \n    with tf.GradientTape() as tape:\n        # Forward pass: render the image\n        # Need to rerun the mipmap generation for autodiff to flow through\n        mat_perlin.diffuse_reflectance = pyredner.Texture(diffuse_tex)\n        mat_perlin.specular_reflectance = pyredner.Texture(specular_tex)\n        mat_perlin.roughness = pyredner.Texture(roughness_tex)\n        scene_args = pyredner.serialize_scene(\n            scene = scene,\n            num_samples = 4,\n            max_bounces = 1)\n        img = pyredner.render(t+1, *scene_args)\n        pyredner.imwrite(img, \'results/test_svbrdf/iter_{}.png\'.format(t))\n        loss = tf.reduce_sum(tf.square(img - target))\n    print(\'loss:\', loss)\n\n    grads = tape.gradient(loss, [diffuse_tex, specular_tex, roughness_tex])\n    optimizer.apply_gradients(zip(grads, [diffuse_tex, specular_tex, roughness_tex]))\n    # Project textures back to valid range\n    diffuse_tex.assign(tf.maximum(diffuse_tex, 0))\n    specular_tex.assign(tf.maximum(specular_tex, 0))\n    roughness_tex.assign(tf.maximum(roughness_tex, 1e-5))\n\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\nimg = pyredner.render(202, *scene_args)\npyredner.imwrite(img, \'results/test_svbrdf/final.exr\')\npyredner.imwrite(img, \'results/test_svbrdf/final.png\')\npyredner.imwrite(tf.abs(target - img), \'results/test_svbrdf/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_svbrdf/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_svbrdf/out.mp4""])\n'"
tests_tensorflow/test_teapot_reflectance.py,30,"b'# Tensorflow by default allocates all GPU memory, leaving very little for rendering.\n# We set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true to enforce on demand\n# memory allocation to reduce page faults.\nimport os\nos.environ[\'TF_FORCE_GPU_ALLOW_GROWTH\'] = \'true\'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport pyredner_tensorflow as pyredner\nimport numpy as np\nimport scipy\n\n# Optimize for material parameters and camera pose\n\n# Use GPU if available\npyredner.set_use_gpu(tf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None))\n\n# Load the scene from a Mitsuba scene file\nscene = pyredner.load_mitsuba(\'scenes/teapot.xml\')\n\n# The last material is the teapot material, set it to the target\nwith tf.device(pyredner.get_device_name()):\n    scene.materials[-1].diffuse_reflectance = \\\n        pyredner.Texture(tf.Variable([0.3, 0.2, 0.2], dtype=tf.float32))\n    scene.materials[-1].specular_reflectance = \\\n        pyredner.Texture(tf.Variable([0.6, 0.6, 0.6], dtype=tf.float32))\n    scene.materials[-1].roughness = \\\n        pyredner.Texture(tf.Variable([0.05], dtype=tf.float32))\n\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 1024,\n    max_bounces = 2)\n\n# Render our target. The first argument is the seed for RNG in the renderer.\nimg = pyredner.render(0, *scene_args)\npyredner.imwrite(img, \'results/test_teapot_reflectance/target.exr\')\npyredner.imwrite(img, \'results/test_teapot_reflectance/target.png\')\ntarget = pyredner.imread(\'results/test_teapot_reflectance/target.exr\')\n\n# Perturb the scene, this is our initial guess\ncam = scene.camera\ncam_position = cam.position\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    cam_translation = tf.Variable([-0.2, 0.2, -0.2], dtype=tf.float32, trainable=True)\nwith tf.device(pyredner.get_device_name()):\n    diffuse_reflectance = tf.Variable([0.3, 0.3, 0.3], dtype=tf.float32, trainable=True)\n    specular_reflectance = tf.Variable([0.5, 0.5, 0.5], dtype=tf.float32, trainable=True)\n    roughness = tf.Variable([0.2], dtype=tf.float32, trainable=True)\nscene.materials[-1].diffuse_reflectance = pyredner.Texture(diffuse_reflectance)\nscene.materials[-1].specular_reflectance = pyredner.Texture(specular_reflectance)\nscene.materials[-1].roughness = pyredner.Texture(roughness)\nscene.camera = pyredner.Camera(position     = cam_position + cam_translation,\n                               look_at      = cam.look_at + cam_translation,\n                               up           = cam.up,\n                               fov          = cam.fov,\n                               clip_near    = cam.clip_near,\n                               resolution   = cam.resolution,\n                               fisheye      = False)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 1024,\n    max_bounces = 2)\n# Render the initial guess\nimg = pyredner.render(1, *scene_args)\npyredner.imwrite(img, \'results/test_teapot_reflectance/init.png\')\ndiff = tf.abs(target - img)\npyredner.imwrite(diff, \'results/test_teapot_reflectance/init_diff.png\')\n\nlr = 1e-2\noptimizer = tf.compat.v1.train.AdamOptimizer(lr)\nnum_iteration = 200\nfor t in range(num_iteration):\n    print(\'iteration:\', t)\n    \n    with tf.GradientTape() as tape:\n        # Forward pass: render the image\n        # need to rerun Camera constructor for autodiff \n        scene.camera = pyredner.Camera(position     = cam_position + cam_translation,\n                                       look_at      = cam.look_at + cam_translation,\n                                       up           = cam.up,\n                                       fov          = cam.fov,\n                                       clip_near    = cam.clip_near,\n                                       resolution   = cam.resolution,\n                                       fisheye      = False)\n        # need to rerun the reflectance for autodiff\n        scene.materials[-1].diffuse_reflectance = pyredner.Texture(diffuse_reflectance)\n        scene.materials[-1].specular_reflectance = pyredner.Texture(specular_reflectance)\n        scene.materials[-1].roughness = pyredner.Texture(roughness)\n        scene_args = pyredner.serialize_scene(\n            scene = scene,\n            num_samples = 4,\n            max_bounces = 2)\n        img = pyredner.render(t+1, *scene_args)\n        pyredner.imwrite(img, \'results/test_teapot_reflectance/iter_{}.png\'.format(t))\n\n        #loss = tf.reduce_sum(tf.square(img - target))\n        with tf.device(pyredner.get_device_name()):\n            dirac = np.zeros([7,7], dtype = np.float32)\n            dirac[3,3] = 1.0\n            f = np.zeros([7, 7, 3, 3], dtype = np.float32)\n            gf = scipy.ndimage.filters.gaussian_filter(dirac, 1.0)\n            f[:, :, 0, 0] = gf\n            f[:, :, 1, 1] = gf\n            f[:, :, 2, 2] = gf\n            f = tf.constant(f)\n            def conv(x, f):\n                \'\'\'\n                    m = torch.nn.AvgPool2d(2)\n                    m(torch.nn.functional.conv2d(diff_0, f, padding=3))\n    \n                Torch uses NCHW\n                TF uses NHWC\n                \'\'\'\n                padding = [\n                    [0,0], # Minibatch\n                    [3,3],\n                    [3,3],\n                    [0,0]\n                ]\n                y = tf.nn.conv2d(x, f, strides=1, padding=padding)\n                y = tf.nn.avg_pool2d(y, ksize=2, strides=2, padding=\'VALID\')\n                return y\n    \n            r = 256\n            # NOTE: `perm` must change according to the `data_format`\n            diff_0 = tf.transpose(tf.reshape(img - target, (1, r, r, 3)), perm=[0,2,1,3])\n            diff_1 = conv(diff_0, f)\n            diff_2 = conv(diff_1, f)\n            diff_3 = conv(diff_2, f)\n            diff_4 = conv(diff_3, f)\n            diff_5 = conv(diff_4, f)\n            \n            loss = tf.reduce_sum(tf.square(diff_0)) / (r*r) + \\\n                   tf.reduce_sum(tf.square(diff_1)) / ((r/2)*(r/2)) + \\\n                   tf.reduce_sum(tf.square(diff_2)) / ((r/4)*(r/4)) + \\\n                   tf.reduce_sum(tf.square(diff_3)) / ((r/8)*(r/8)) + \\\n                   tf.reduce_sum(tf.square(diff_4)) / ((r/16)*(r/16)) + \\\n                   tf.reduce_sum(tf.square(diff_5)) / ((r/32)*(r/32))\n    print(\'>>> LOSS:\', loss)\n\n    grads = tape.gradient(\n        loss, \n        [diffuse_reflectance, specular_reflectance, roughness, cam_translation]\n    )\n\n    print(grads)\n\n    print(\'diffuse_reflectance.grad:\', grads[0].numpy())\n    print(\'specular_reflectance.grad:\', grads[1].numpy())\n    print(\'roughness.grad:\', grads[2].numpy())\n    print(\'cam_translation.grad:\', grads[3].numpy())\n    grads[2] = tf.clip_by_norm(grads[2], 10)\n    grads[3] = tf.clip_by_norm(grads[3], 10)\n\n    optimizer.apply_gradients(\n        zip(grads, [diffuse_reflectance, specular_reflectance, roughness, cam_translation]))\n    print("">>> AFTER CLIPPING"")\n    print(\'diffuse_reflectance.grad:\', grads[0].numpy())\n    print(\'specular_reflectance.grad:\', grads[1].numpy())\n    print(\'roughness.grad:\', grads[2].numpy())\n    print(\'cam_translation.grad:\', grads[3].numpy())\n    \n    print(\'diffuse_reflectance:\', diffuse_reflectance.numpy())\n    print(\'specular_reflectance:\', specular_reflectance.numpy())\n    print(\'roughness:\', roughness.numpy())\n    print(\'cam_translation:\', cam_translation.numpy())\n\n    # Linearly reduce the learning rate\n    # lr = 1e-2 * float(num_iteration - t) / float(num_iteration)\n    # optimizer._lr = lr\n\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 1024,\n    max_bounces = 2)\nimg = pyredner.render(num_iteration + 2, *scene_args)\npyredner.imwrite(img, \'results/test_teapot_reflectance/final.exr\')\npyredner.imwrite(img, \'results/test_teapot_reflectance/final.png\')\npyredner.imwrite(tf.abs(target - img), \'results/test_teapot_reflectance/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_teapot_reflectance/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_teapot_reflectance/out.mp4""])\n'"
tests_tensorflow/test_teapot_specular.py,13,"b'# Tensorflow by default allocates all GPU memory, leaving very little for rendering.\n# We set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true to enforce on demand\n# memory allocation to reduce page faults.\nimport os\nos.environ[\'TF_FORCE_GPU_ALLOW_GROWTH\'] = \'true\'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport pyredner_tensorflow as pyredner\n\n# Optimize for a textured plane in a specular reflection\n\n# Use GPU if available\npyredner.set_use_gpu(tf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None))\n\n# Load the scene from a Mitsuba scene file\nscene = pyredner.load_mitsuba(\'scenes/teapot_specular.xml\')\n\n# The last material is the teapot material, set it to a specular material\nwith tf.device(pyredner.get_device_name()):\n    scene.materials[-1].diffuse_reflectance = \\\n        pyredner.Texture(tf.Variable([0.15, 0.2, 0.15], dtype=tf.float32))\n    scene.materials[-1].specular_reflectance = \\\n        pyredner.Texture(tf.Variable([0.8, 0.8, 0.8], dtype=tf.float32))\n    scene.materials[-1].roughness = \\\n        pyredner.Texture(tf.Variable([0.0001], dtype=tf.float32))\n\nscene_args=pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 512,\n    max_bounces = 2)\n\n# Render our target. The first argument is the seed for RNG in the renderer.\nimg = pyredner.render(0, *scene_args)\npyredner.imwrite(img, \'results/test_teapot_specular/target.exr\')\npyredner.imwrite(img, \'results/test_teapot_specular/target.png\')\ntarget = pyredner.imread(\'results/test_teapot_specular/target.exr\')\n\n# Perturb the scene, this is our initial guess\n# We perturb the last shape, which is the SIGGRAPH logo\nref_pos = scene.shapes[-1].vertices\nwith tf.device(pyredner.get_device_name()):\n    translation = tf.Variable([20.0, 0.0, 2.0], trainable=True)\n    scene.shapes[-1].vertices = ref_pos + translation\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 512,\n    max_bounces = 2)\n# Render the initial guess\nimg = pyredner.render(1, *scene_args)\npyredner.imwrite(img, \'results/test_teapot_specular/init.png\')\ndiff = tf.abs(target - img)\npyredner.imwrite(diff, \'results/test_teapot_specular/init_diff.png\')\n\noptimizer = tf.compat.v1.train.AdamOptimizer(0.5)\nnum_iteration = 400\nfor t in range(num_iteration):\n    print(\'iteration:\', t)\n    \n    with tf.GradientTape() as tape:\n        scene.shapes[-1].vertices = ref_pos + translation\n        scene_args = pyredner.serialize_scene(\n            scene = scene,\n            num_samples = 4,\n            max_bounces = 2)\n        img = pyredner.render(t+1, *scene_args)\n        pyredner.imwrite(img, \'results/test_teapot_specular/iter_{}.png\'.format(t))\n        loss = tf.reduce_sum(tf.square(img - target))\n    print(\'loss:\', loss)\n\n    grads = tape.gradient(loss, [translation])\n    print(\'grad:\', grads)\n\n    optimizer.apply_gradients(zip(grads, [translation]))\n\n    print(\'translation:\', translation)\n\n    # Linearly reduce the learning rate\n    lr = 0.5 * float(num_iteration - t) / float(num_iteration)\n    optimizer._lr = lr\n\nscene.shapes[-1].vertices = ref_pos + translation\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 512,\n    max_bounces = 2)\nimg = pyredner.render(num_iteration + 2, *scene_args)\npyredner.imwrite(img, \'results/test_teapot_specular/final.exr\')\npyredner.imwrite(img, \'results/test_teapot_specular/final.png\')\npyredner.imwrite(tf.abs(target - img), \'results/test_teapot_specular/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_teapot_specular/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_teapot_specular/out.mp4""])\n'"
tests_tensorflow/test_texture.py,27,"b'# Tensorflow by default allocates all GPU memory, leaving very little for rendering.\n# We set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true to enforce on demand\n# memory allocation to reduce page faults.\nimport os\nos.environ[\'TF_FORCE_GPU_ALLOW_GROWTH\'] = \'true\'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport pyredner_tensorflow as pyredner\n\n# Optimize four vertices of a textured patch\n\n# Use GPU if available\npyredner.set_use_gpu(tf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None))\n\n# Set up the scene using Pytorch tensor\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    position = tf.Variable([0.0, 0.0, -5.0], dtype=tf.float32)\n    look_at = tf.Variable([0.0, 0.0, 0.0], dtype=tf.float32)\n    up = tf.Variable([0.0, 1.0, 0.0], dtype=tf.float32)\n    fov = tf.Variable([45.0], dtype=tf.float32)\n    clip_near = 1e-2\n    resolution = (256, 256)\n    cam = pyredner.Camera(position = position,\n                          look_at = look_at,\n                          up = up,\n                          fov = fov,\n                          clip_near = clip_near,\n                          resolution = resolution)\n\nwith tf.device(pyredner.get_device_name()):\n    checkerboard_texture = pyredner.imread(\'checkerboard.exr\')\n    mat_checkerboard = pyredner.Material(\n        diffuse_reflectance = checkerboard_texture)\n    mat_black = pyredner.Material(\n        diffuse_reflectance = tf.Variable([0.0, 0.0, 0.0], dtype=tf.float32))\n    materials = [mat_checkerboard, mat_black]\n    vertices = tf.Variable([[-1.0,-1.0,0.0], [-1.0,1.0,0.0], [1.0,-1.0,0.0], [1.0,1.0,0.0]],\n        dtype=tf.float32)\n    indices = tf.constant([[0, 1, 2], [1, 3, 2]], dtype=tf.int32)\n    uvs = tf.Variable([[0.05, 0.05], [0.05, 0.95], [0.95, 0.05], [0.95, 0.95]],\n\tdtype=tf.float32)\n    shape_plane = pyredner.Shape(vertices, indices, 0, uvs)\n    light_vertices = tf.Variable([[-1.0,-1.0,-7.0],[1.0,-1.0,-7.0],[-1.0,1.0,-7.0],[1.0,1.0,-7.0]],\n        dtype=tf.float32)\n    light_indices = tf.constant([[0,1,2],[1,3,2]], dtype=tf.int32)\n    shape_light = pyredner.Shape(light_vertices, light_indices, 1)\n    shapes = [shape_plane, shape_light]\n\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    light_intensity = tf.Variable([20.0, 20.0, 20.0], dtype=tf.float32)\n# The first argument is the shape id of the light\nlight = pyredner.AreaLight(1, light_intensity)\narea_lights = [light]\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\n\n# Alias of the render function\n\n# Render our target\nimg = pyredner.render(0, *scene_args)\npyredner.imwrite(img, \'results/test_texture/target.exr\')\npyredner.imwrite(img, \'results/test_texture/target.png\')\ntarget = pyredner.imread(\'results/test_texture/target.exr\')\n\n# Perturb the scene, this is our initial guess\nwith tf.device(pyredner.get_device_name()):\n    shape_plane.vertices = tf.Variable(\n        [[-1.1,-1.2,0.0], [-1.3,1.1,0.0], [1.1,-1.1,0.0], [0.8,1.2,0.0]],\n        dtype=tf.float32,\n        trainable=True)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\n# Render the initial guess\nimg = pyredner.render(1, *scene_args)\npyredner.imwrite(img, \'results/test_texture/init.png\')\ndiff = tf.abs(target - img)\npyredner.imwrite(diff, \'results/test_texture/init_diff.png\')\n\n# Optimize for triangle vertices\noptimizer = tf.compat.v1.train.AdamOptimizer(5e-2)\nfor t in range(200):\n    print(\'iteration:\', t)\n\n    with tf.GradientTape() as tape:\n        # Forward pass: render the image\n        scene_args = pyredner.serialize_scene(\n            scene = scene,\n            num_samples = 4,\n            max_bounces = 1)\n        img = pyredner.render(t+1, *scene_args)\n        pyredner.imwrite(img, \'results/test_texture/iter_{}.png\'.format(t))\n        loss = tf.reduce_sum(tf.square(img - target))\n    \n    print(\'loss:\', loss)\n\n    grads = tape.gradient(loss, [shape_plane.vertices])\n    optimizer.apply_gradients(zip(grads, [shape_plane.vertices]))\n\n    print(\'grad:\', grads)\n    print(\'vertices:\', shape_plane.vertices)\n\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\nimg = pyredner.render(202, *scene_args)\npyredner.imwrite(img, \'results/test_texture/final.exr\')\npyredner.imwrite(img, \'results/test_texture/final.png\')\npyredner.imwrite(tf.abs(target - img), \'results/test_texture/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_texture/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_texture/out.mp4""])\n'"
tests_tensorflow/test_two_triangles.py,32,"b'# Tensorflow by default allocates all GPU memory, leaving very little for rendering.\n# We set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true to enforce on demand\n# memory allocation to reduce page faults.\nimport os\nos.environ[\'TF_FORCE_GPU_ALLOW_GROWTH\'] = \'true\'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport pyredner_tensorflow as pyredner\n\n# Optimize six vertices of a two triangles\n\n# Use GPU if available\npyredner.set_use_gpu(tf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None))\n\n# Set up the scene using Pytorch tensor\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    position = tf.Variable([0.0, 0.0, -5.0], dtype=tf.float32)\n    look_at = tf.Variable([0.0, 0.0, 0.0], dtype=tf.float32)\n    up = tf.Variable([0.0, 1.0, 0.0], dtype=tf.float32)\n    fov = tf.Variable([45.0], dtype=tf.float32)\n    clip_near = 1e-2\n    resolution = (256, 256)\n    cam = pyredner.Camera(position = position,\n                          look_at = look_at,\n                          up = up,\n                          fov = fov,\n                          clip_near = clip_near,\n                          resolution = resolution)\n\nwith tf.device(pyredner.get_device_name()):\n    mat_green = pyredner.Material(\n        diffuse_reflectance = tf.Variable([0.35, 0.75, 0.35], dtype=tf.float32))\n    mat_red = pyredner.Material(\n        diffuse_reflectance = tf.Variable([0.75, 0.35, 0.35], dtype=tf.float32))\n    mat_black = pyredner.Material(\n        diffuse_reflectance = tf.Variable([0.0, 0.0, 0.0], dtype=tf.float32))\n    materials = [mat_green,mat_red,mat_black]\n    tri0_vertices = tf.Variable(\n        [[-1.7,1.0,0.0], [1.0,1.0,0.0], [-0.5,-1.0,0.0]], dtype=tf.float32)\n    tri1_vertices = tf.Variable(\n        [[-1.0,1.5,1.0], [0.2,1.5,1.0], [0.2,-1.5,1.0]], dtype=tf.float32)\n    tri0_indices = tf.constant([[0, 1, 2]], dtype=tf.int32)\n    tri1_indices = tf.constant([[0, 1, 2]], dtype=tf.int32)\n    shape_tri0 = pyredner.Shape(tri0_vertices, tri0_indices, 0)\n    shape_tri1 = pyredner.Shape(tri1_vertices, tri1_indices, 1)\n    light_vertices = tf.Variable([[-1.0,-1.0,-7.0],[1.0,-1.0,-7.0],[-1.0,1.0,-7.0],[1.0,1.0,-7.0]],\n        dtype=tf.float32)\n    light_indices = tf.constant([[0,1,2],[1,3,2]], dtype=tf.int32)\n    shape_light = pyredner.Shape(light_vertices, light_indices, 2)\n    shapes = [shape_tri0, shape_tri1, shape_light]\n\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    light_intensity = tf.Variable([20.0,20.0,20.0], dtype=tf.float32)\n# The first argument is the shape id of the light\nlight = pyredner.AreaLight(2, light_intensity)\narea_lights = [light]\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\n\n# Render our target\nimg = pyredner.render(0, *scene_args)\npyredner.imwrite(img, \'results/test_two_triangles/target.exr\')\npyredner.imwrite(img, \'results/test_two_triangles/target.png\')\ntarget = pyredner.imread(\'results/test_two_triangles/target.exr\')\n\n# Perturb the scene, this is our initial guess\nwith tf.device(pyredner.get_device_name()):\n    shape_tri0.vertices = tf.Variable(\n        [[-1.3,1.5,0.1], [1.5,0.7,-0.2], [-0.8,-1.1,0.2]],\n        dtype=tf.float32,\n        trainable=True)\n    shape_tri1.vertices = tf.Variable(\n        [[-0.5,1.2,1.2], [0.3,1.7,1.0], [0.5,-1.8,1.3]],\n        dtype=tf.float32,\n        trainable=True)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\n# Render the initial guess\nimg = pyredner.render(1, *scene_args)\npyredner.imwrite(img, \'results/test_two_triangles/init.png\')\ndiff = tf.abs(target - img)\npyredner.imwrite(diff, \'results/test_two_triangles/init_diff.png\')\n\n# Optimize for triangle vertices\noptimizer = tf.compat.v1.train.AdamOptimizer(5e-2)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 4,\n    max_bounces = 1)\nfor t in range(200):\n    print(\'iteration:\', t)\n    \n    # Forward pass: render the image\n    with tf.GradientTape() as tape:\n        img = pyredner.render(t+1, *scene_args)\n\n        pyredner.imwrite(img, \'results/test_two_triangles/iter_{}.png\'.format(t))\n        loss = tf.reduce_sum(tf.square(img - target))\n        print(\'loss:\', loss)\n\n    grads = tape.gradient(loss, [shape_tri0.vertices, shape_tri1.vertices])\n\n    optimizer.apply_gradients(zip(grads, [shape_tri0.vertices, shape_tri1.vertices]))\n\n    print(\'tri0:\', shape_tri0.vertices)\n    print(\'tri1:\', shape_tri1.vertices)\n\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1)\nimg = pyredner.render(202, *scene_args)\npyredner.imwrite(img, \'results/test_two_triangles/final.exr\')\npyredner.imwrite(img, \'results/test_two_triangles/final.png\')\npyredner.imwrite(tf.abs(target - img), \'results/test_two_triangles/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_two_triangles/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_two_triangles/out.mp4""])\n'"
tests_tensorflow/test_vertex_color.py,18,"b'# Tensorflow by default allocates all GPU memory, leaving very little for rendering.\n# We set the environment variable TF_FORCE_GPU_ALLOW_GROWTH to true to enforce on demand\n# memory allocation to reduce page faults.\nimport os\nos.environ[\'TF_FORCE_GPU_ALLOW_GROWTH\'] = \'true\'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport pyredner_tensorflow as pyredner\nimport redner\n\n# Use GPU if available\npyredner.set_use_gpu(tf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None))\n\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    cam = pyredner.Camera(position = tf.Variable([0.0, 0.0, -5.0], dtype=tf.float32),\n                          look_at = tf.Variable([0.0, 0.0, 0.0], dtype=tf.float32),\n                          up = tf.Variable([0.0, 1.0, 0.0], dtype=tf.float32),\n                          fov = tf.Variable([45.0], dtype=tf.float32), # in degree\n                          clip_near = 1e-2, # needs to > 0\n                          resolution = (256, 256),\n                          fisheye = False)\n\nwith tf.device(pyredner.get_device_name()):\n    mat_vertex_color = pyredner.Material(use_vertex_color = True)\nmaterials = [mat_vertex_color]\n\nwith tf.device(pyredner.get_device_name()):\n    # For the target we randomize the vertex color.\n    vertices, indices, uvs, normals = pyredner.generate_sphere(128, 64)\n    vertex_color = tf.random.uniform(vertices.shape, 0.0, 1.0)\n    shape_sphere = pyredner.Shape(\n        vertices = vertices,\n        indices = indices,\n        uvs = uvs,\n        normals = normals,\n        colors = vertex_color,\n        material_id = 0)\nshapes = [shape_sphere]\n\nwith tf.device(pyredner.get_device_name()):\n    envmap = pyredner.imread(\'sunsky.exr\')\n    envmap = pyredner.EnvironmentMap(envmap)\nscene = pyredner.Scene(camera=cam,\n                       shapes=shapes,\n                       materials=materials,\n                       area_lights=[],\n                       envmap=envmap)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1,\n    channels = [redner.channels.radiance, redner.channels.vertex_color])\n\nimg = pyredner.render(0, *scene_args)\nimg_radiance = img[:, :, :3]\nimg_vertex_color = img[:, :, 3:]\npyredner.imwrite(img_radiance, \'results/test_vertex_color/target.exr\')\npyredner.imwrite(img_radiance, \'results/test_vertex_color/target.png\')\npyredner.imwrite(img_vertex_color, \'results/test_vertex_color/target_color.png\')\ntarget_radiance = pyredner.imread(\'results/test_vertex_color/target.exr\')\n\n# Initial guess. Set to 0.5 for all vertices.\nwith tf.device(pyredner.get_device_name()):\n    shape_sphere.colors = tf.Variable(tf.zeros_like(vertices) + 0.5)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1,\n    channels = [redner.channels.radiance, redner.channels.vertex_color])\nimg = pyredner.render(1, *scene_args)\nimg_radiance = img[:, :, :3]\nimg_vertex_color = img[:, :, 3:]\npyredner.imwrite(img_radiance, \'results/test_vertex_color/init.png\')\npyredner.imwrite(img_vertex_color, \'results/test_vertex_color/init_color.png\')\ndiff = tf.abs(target_radiance - img_radiance)\npyredner.imwrite(diff, \'results/test_vertex_color/init_diff.png\')\n\noptimizer = tf.compat.v1.train.AdamOptimizer(1e-2)\nfor t in range(100):\n    print(\'iteration:\', t)\n    with tf.GradientTape() as tape:\n        scene_args = pyredner.serialize_scene(\n            scene = scene,\n            num_samples = 4,\n            max_bounces = 1,\n            channels = [redner.channels.radiance, redner.channels.vertex_color])\n        img = pyredner.render(t+1, *scene_args)\n        img_radiance = img[:, :, :3]\n        img_vertex_color = img[:, :, 3:]\n\n        loss = tf.reduce_sum(tf.square(img_radiance - target_radiance))\n        print(\'loss:\', loss)\n\n    pyredner.imwrite(img_radiance, \'results/test_vertex_color/iter_{}.png\'.format(t))\n    pyredner.imwrite(img_vertex_color, \'results/test_vertex_color/iter_color_{}.png\'.format(t))\n\n    grads = tape.gradient(loss, [shape_sphere.colors])\n    optimizer.apply_gradients(zip(grads, [shape_sphere.colors]))\n\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 256,\n    max_bounces = 1,\n    channels = [redner.channels.radiance, redner.channels.vertex_color])\nimg = pyredner.render(102, *scene_args)\nimg_radiance = img[:, :, :3]\nimg_vertex_color = img[:, :, 3:]\npyredner.imwrite(img_radiance, \'results/test_vertex_color/final.exr\')\npyredner.imwrite(img_radiance, \'results/test_vertex_color/final.png\')\npyredner.imwrite(img_vertex_color, \'results/test_vertex_color/final_color.png\')\npyredner.imwrite(tf.abs(target_radiance - img_radiance), \'results/test_vertex_color/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_vertex_color/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_vertex_color/out.mp4""])\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_vertex_color/iter_color_%d.png"", ""-vb"", ""20M"",\n    ""results/test_vertex_color/out_color.mp4""])'"
tests_tensorflow/test_viewport.py,26,"b'import os\nos.environ[\'TF_FORCE_GPU_ALLOW_GROWTH\'] = \'true\'\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport pyredner_tensorflow as pyredner\n\n# From the test_single_triangle.py test case but with viewport\n\n# Use GPU if available\npyredner.set_use_gpu(tf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None))\n\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    cam = pyredner.Camera(position = tf.Variable([0.0, 0.0, -5.0], dtype=tf.float32),\n                          look_at = tf.Variable([0.0, 0.0, 0.0], dtype=tf.float32),\n                          up = tf.Variable([0.0, 1.0, 0.0], dtype=tf.float32),\n                          fov = tf.Variable([45.0], dtype=tf.float32), # in degree\n                          clip_near = 1e-2, # needs to > 0\n                          resolution = (1024, 1024),\n                          viewport = (200, 300, 700, 800))\n\nwith tf.device(pyredner.get_device_name()):\n    mat_grey = pyredner.Material(\n        diffuse_reflectance = tf.Variable([0.5, 0.5, 0.5], dtype=tf.float32))\nmaterials = [mat_grey]\n\nwith tf.device(pyredner.get_device_name()):\n    shape_triangle = pyredner.Shape(\n        vertices = tf.Variable([[-1.7, 1.0, 0.0], [1.0, 1.0, 0.0], [-0.5, -1.0, 0.0]],\n            dtype=tf.float32),\n        indices = tf.constant([[0, 1, 2]], dtype=tf.int32),\n        uvs = None,\n        normals = None,\n        material_id = 0)\n    shape_light = pyredner.Shape(\n        vertices = tf.Variable([[-1.0, -1.0, -7.0],\n                                [ 1.0, -1.0, -7.0],\n                                [-1.0,  1.0, -7.0],\n                                [ 1.0,  1.0, -7.0]], dtype=tf.float32),\n        indices = tf.constant([[0, 1, 2],[1, 3, 2]], dtype=tf.int32),\n        uvs = None,\n        normals = None,\n        material_id = 0)\nshapes = [shape_triangle, shape_light]\n\nwith tf.device(\'/device:cpu:\' + str(pyredner.get_cpu_device_id())):\n    light = pyredner.AreaLight(shape_id = 1, \n                               intensity = tf.Variable([20.0,20.0,20.0], dtype=tf.float32))\narea_lights = [light]\n\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\n\nimg = pyredner.render(0, *scene_args)\npyredner.imwrite(img, \'results/test_single_triangle/target.exr\')\npyredner.imwrite(img, \'results/test_single_triangle/target.png\')\ntarget = pyredner.imread(\'results/test_single_triangle/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.gpu()\n\nwith tf.device(pyredner.get_device_name()):\n    shape_triangle.vertices = tf.Variable(\n        [[-2.0,1.5,0.3], [0.9,1.2,-0.3], [-0.4,-1.4,0.2]],\n        dtype=tf.float32,\n        trainable=True) # Set trainable to True since we want to optimize this\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\nimg = pyredner.render(1, *scene_args)\npyredner.imwrite(img, \'results/test_single_triangle/init.png\')\ndiff = tf.abs(target - img)\npyredner.imwrite(diff, \'results/test_single_triangle/init_diff.png\')\n\noptimizer = tf.compat.v1.train.AdamOptimizer(5e-2)\n\ndef loss(output, target):\n    error = output - target\n    return tf.reduce_sum(tf.square(error))\n\ndef optimize(scene_args, grads, lr=5e-2):\n    updates = []\n    for var, grad in zip(scene_args, grads):\n        if grad is None: \n            updates.append(var)\n            continue\n        updates.append(var - lr * grad)\n\n    return updates\n\n# Run 200 Adam iterations.\nfor t in range(1, 201):\n    print(\'iteration:\', t)\n\n    with tf.GradientTape() as tape:\n        # Forward pass: render the image.\n        \n        # Important to use a different seed every iteration, otherwise the result\n        # would be biased.\n        scene_args = pyredner.serialize_scene(\n            scene = scene,\n            num_samples = 4, # We use less samples in the Adam loop.\n            max_bounces = 1)\n        img = pyredner.render(t, *scene_args)\n        loss_value = loss(img, target)\n\n    print(f""loss_value: {loss_value}"")\n    pyredner.imwrite(img, \'results/test_single_triangle/iter_{}.png\'.format(t))\n\n    grads = tape.gradient(loss_value, [shape_triangle.vertices])\n    optimizer.apply_gradients(zip(grads, [shape_triangle.vertices]))\n\n    print(\'grad:\', grads[0])\n    print(\'vertices:\', shape_triangle.vertices)\n\nscene_args = pyredner.serialize_scene(\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\nimg = pyredner.render(202, *scene_args)\npyredner.imwrite(img, \'results/test_single_triangle/final.exr\')\npyredner.imwrite(img, \'results/test_single_triangle/final.png\')\npyredner.imwrite(tf.abs(target - img), \'results/test_single_triangle/final_diff.png\')\n\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/test_single_triangle/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/test_single_triangle/out.mp4""])\n'"
tests_tensorflow/unit_tests.py,0,b'import pyrednertensorflow as pyredner\nimport redner\nimport numpy as np\n\ndef unit_tests():\n    redner.test_sample_primary_rays(False)\n    redner.test_scene_intersect(False)\n    redner.test_sample_point_on_light(False)\n    redner.test_active_pixels(False)\n    redner.test_camera_derivatives()\n    redner.test_d_bsdf()\n    redner.test_d_bsdf_sample()\n    redner.test_d_bsdf_pdf()\n    redner.test_d_intersect()\n    redner.test_d_sample_shape()\n\n    if False:\n        redner.test_sample_primary_rays(True)\n        redner.test_scene_intersect(True)\n        redner.test_sample_point_on_light(True)\n        redner.test_active_pixels(True)\n\nunit_tests()\n'
tests_tensorflow/utils.py,7,"b'from __future__ import absolute_import, division, print_function\nfrom typing import List, Set, Dict, Tuple, Optional, Callable, Union\n\nimport tensorflow as tf\nimport torch\n\nimport pyrednertensorflow as pyrednertorch\nimport pyrednertensorflow as pyredner\nimport numpy as np\n\nimport pdb\n\ndef is_same_tensor(a:tf.Tensor, b:torch.Tensor, atol=0.0001) -> bool:\n    is_same = False\n    try:\n        is_same = np.alltrue(a.numpy() == b.numpy()) or np.allclose(a.numpy(), b.numpy(), atol=atol)\n    except RuntimeError:\n        print(""Detach"")\n        is_same = np.alltrue(a.numpy() == b.detach().numpy()) \\\n            or np.allclose(a.numpy(), b.detach().numpy(), atol=atol)\n    finally:\n        return is_same\n\ndef is_same_grads(a:pyredner.SceneGrads, b:List) -> bool:\n    if not is_same_tensor(a.d_position, b[4]): \n        print(4); return False\n    if not is_same_tensor(a.d_look_at, b[5]): \n        print(5); return False\n    if not is_same_tensor(a.d_up, b[6]): \n        print(6); return False\n    if not is_same_tensor(a.d_ndc_to_cam, b[7]): \n        print(7); return False\n    if not is_same_tensor(a.d_cam_to_ndc, b[8]): \n        print(8); return False\n    if not is_same_container(a.d_vertices_list, [b[12]]): \n        print(12); return False\n    if not is_same_container(a.d_uvs_list, [b[14]]): \n        print(14); return False\n    if not is_same_container(a.d_normals_list, [b[15]]): \n        print(15); return False\n    if not is_same_container(a.d_diffuse_list, [b[18]]): \n        print(18); return False\n    if not is_same_container(a.d_specular_list, [b[20]]): \n        print(20); return False\n    if not is_same_container(a.d_roughness_list, [b[22]]): \n        print(22); return False\n    if not is_same_container(a.d_intensity_list, [b[26]]): \n        print(26); return False\n    if not is_same_tensor(a.d_envmap_values, b[28]): \n        print(28); return False\n    if not is_same_tensor(a.d_world_to_env, b[31]): \n        print(31); return False\n    return True\n\n\ndef is_same_optional(\n    a: Union[pyredner.EnvironmentMap, tf.Tensor, None], \n    b: Union[pyrednertorch.EnvironmentMap, torch.Tensor, None], \n    func:Callable, \n    atol=0.0001) -> bool:\n    \'\'\'Test for optional tensor parameters. \n\n    Args:\n        a(tf.Tensor or None)\n        b(torch.Tensor or None)\n        \n    \'\'\'\n    if is_only_one_element_none_in_pair(a, b):\n        return False\n    elif a is None and b is None:\n        return True\n    elif func == is_same_tensor:\n        return func(a,b, atol)\n    else:\n        return func(a,b)\n\ndef is_same_camera(a:pyredner.Camera, b:pyrednertorch.Camera):\n    if not is_same_tensor(a.position, b.position):\n        pdb.set_trace()\n    if not is_same_tensor(a.look_at, b.look_at):\n        pdb.set_trace()\n    if not is_same_tensor(a.up, b.up):\n        pdb.set_trace()\n    if not is_same_tensor(a.fov, b.fov):\n        pdb.set_trace()\n    if not is_same_tensor(a.cam_to_ndc, b.cam_to_ndc):\n        pdb.set_trace()\n    if not is_same_tensor(a.ndc_to_cam, b.ndc_to_cam):\n        pdb.set_trace()\n    return True \n\ndef is_same_pdf_norm(a:Union[tf.Tensor, float], b:float) -> bool:\n    if isinstance(a, tf.Tensor):\n        return  np.isclose(a.numpy(), b)\n    else:\n        return  np.isclose(a, b)\n\ndef is_same_envmap(a:pyredner.EnvironmentMap, b:pyredner.EnvironmentMap):\n    return is_same_texture(a.values, b.values) \\\n        and is_same_tensor(a.env_to_world, b.env_to_world) \\\n        and is_same_tensor(a.world_to_env, b.world_to_env) \\\n        and is_same_tensor(a.sample_cdf_xs, b.sample_cdf_xs) \\\n        and is_same_tensor(a.sample_cdf_ys, b.sample_cdf_ys) \\\n        and is_same_pdf_norm(a.pdf_norm, b.pdf_norm)\n        \n\ndef is_same_texture(a:pyredner.Texture, b:pyrednertorch.Texture) -> bool:\n    return is_same_tensor(a.texels, b.texels) \\\n        and is_same_tensor(a.mipmap, b.mipmap) \\\n        and is_same_tensor(a.uv_scale, b.uv_scale) \n\n\ndef is_same_material(a:pyredner.Material, b:pyrednertorch.Material):\n    if not is_same_texture(a.diffuse_reflectance, b.diffuse_reflectance):\n        return False\n\n    if not is_same_texture(a.specular_reflectance, b.specular_reflectance):\n        return False\n\n    if not is_same_texture(a.roughness, b.roughness):\n        return False\n\n    return a.two_sided == b.two_sided\n\ndef is_only_one_element_none_in_pair(a, b):\n    is_a_none_b_not_none = a is None and b is not None\n    is_a_not_none_b_none = a is not None and b is None\n\n    return is_a_none_b_not_none or is_a_not_none_b_none\n\n\ndef is_same_shape(a:pyredner.Shape, b:pyrednertorch.Shape):\n    if not is_same_tensor(a.vertices, b.vertices):\n        pdb.set_trace()\n    if not is_same_tensor(a.indices, b.indices):\n        pdb.set_trace()\n    \n    if not is_same_optional(a.normals, b.normals, is_same_tensor):\n        pdb.set_trace()\n\n    if not is_same_optional(a.uvs, b.uvs, is_same_tensor):\n        pdb.set_trace()\n\n    return True\n\ndef is_same_area_light(a:pyredner.AreaLight, b:pyrednertorch.AreaLight) -> bool:\n    return a.shape_id == b.shape_id \\\n        and is_same_tensor(a.intensity, b.intensity) \\\n        and a.two_sided == b.two_sided\n\ndef is_same_container(container1:List, container2:List) -> bool:\n    assert len(container1) == len(container2)\n\n    if len(container1) == 0:\n        return True\n\n    compare_func = None\n\n    if isinstance(container1[0], pyredner.Material):\n        compare_func = is_same_material\n    elif isinstance(container1[0], pyredner.AreaLight):\n        compare_func = is_same_area_light\n    elif isinstance(container1[0], pyredner.Shape):\n        compare_func = is_same_shape\n    elif isinstance(container1[0], tf.Tensor) or isinstance(container1[0], tf.Variable):\n        compare_func = is_same_tensor\n    else:\n        return False\n\n    for c1, c2 in zip(container1, container2):\n        if not compare_func(c1, c2):\n            return False\n\n    return True\n\ndef is_same_image(a: tf.Tensor, b:torch.Tensor) -> bool:\n    diff_channels = [\n        i for i in range(a.shape[2]) if not is_same_tensor(a[:,:,i], b[:,:,i])\n        ]\n    if len(diff_channels) == 0:\n        return True\n\n\n    diff_channels = [\n        \'RGB\'[i] for i in diff_channels\n    ]\n    \n    for c in diff_channels:\n        print(f\'{c} - channel is different\')\n\n    return False\n\n\ndef is_same_scene(scene1:pyredner.Scene, scene2:pyrednertorch.Scene) -> bool:        \n\n    if not is_same_optional(scene1.envmap, scene2.envmap, is_same_envmap):\n        return False\n\n    return is_same_camera(scene1.camera, scene2.camera) \\\n        and is_same_container(scene1.shapes, scene2.shapes) \\\n        and is_same_container(scene1.materials, scene2.materials) \\\n        and is_same_container(scene1.area_lights, scene2.area_lights)\n\ndef is_same_scene_args(args1:pyredner.SceneArgs, args2:List) -> bool:\n    i = 0\n    assert args1.num_shapes == args2[i]\n    i += 1\n    assert args1.num_materials == args2[i]\n    i += 1\n    assert args1.num_lights == args2[i]\n    i += 1\n    assert is_same_tensor(args1.position, args2[i])\n    i += 1\n    assert is_same_tensor(args1.look_at, args2[i])\n    i += 1\n    assert is_same_tensor(args1.up, args2[i])\n    i += 1\n    assert is_same_tensor(args1.ndc_to_cam, args2[i])\n    i += 1\n    assert is_same_tensor(args1.cam_to_ndc, args2[i])\n    i += 1\n    \n    assert args1.clip_near == args2[i]\n    i += 1\n    assert args1.resolution == args2[i]\n    i += 1\n    assert args1.fisheye == args2[i]\n    i += 1\n\n    for j in range(len(args1.shapes)):\n        \n        assert is_same_tensor(args1.shapes[j].vertices, args2[i])\n        i += 1\n        assert is_same_tensor(args1.shapes[j].indices, args2[i])\n        i += 1\n        assert is_same_optional(args1.shapes[j].uvs, args2[i], is_same_tensor)\n        i += 1\n        assert is_same_optional(args1.shapes[j].normals, args2[i], is_same_tensor)\n        i += 1\n        assert args1.shapes[j].material_id == args2[i]\n        i += 1\n        assert args1.shapes[j].light_id == args2[i]\n        i += 1\n        \n    for j in range(len(args1.materials)):\n        assert is_same_tensor(args1.materials[j].diffuse_reflectance.mipmap, args2[i])\n        i += 1\n        assert is_same_tensor(args1.materials[j].diffuse_reflectance.uv_scale, args2[i])\n        i += 1\n        assert is_same_tensor(args1.materials[j].specular_reflectance.mipmap, args2[i])\n        i += 1\n        assert is_same_tensor(args1.materials[j].specular_reflectance.uv_scale, args2[i])\n        i += 1\n        assert is_same_tensor(args1.materials[j].roughness.mipmap, args2[i])\n        i += 1\n        assert is_same_tensor(args1.materials[j].roughness.uv_scale, args2[i])\n        i += 1\n        assert args1.materials[j].two_sided == args2[i]\n        i += 1\n        \n    for j in range(len(args1.lights)):\n        assert args1.lights[j].shape_id == args2[i]\n        i += 1\n        assert is_same_tensor(args1.lights[j].intensity, args2[i])\n        i += 1\n        assert args1.lights[j].two_sided == args2[i]\n        i += 1\n        \n    if args2[i] is not None:\n        assert is_same_tensor(args1.envmap_mipmap, args2[i])\n        i += 1\n        assert is_same_tensor(args1.envmap_uv_scale, args2[i])\n        i += 1\n        assert is_same_tensor(args1.envmap_env_to_world, args2[i])\n        i += 1\n        assert is_same_tensor(args1.envmap_world_to_env, args2[i])\n        i += 1\n        assert is_same_tensor(args1.envmap_sample_cdf_ys, args2[i])\n        i += 1\n        assert is_same_tensor(args1.envmap_sample_cdf_xs, args2[i])\n        i += 1\n        assert is_same_pdf_norm(args1.envmap_pdf_norm, args2[i])\n        i += 1\n        \n    else:\n        i += 7\n\n\n\n    \n\n    assert args1.num_samples == args2[i]\n    i += 1\n    assert args1.max_bounces == args2[i]\n    i += 1\n    assert args1.channels == args2[i]\n    i += 1\n    assert args1.sampler_type == args2[i]\n    i += 1\n    assert args1.use_primary_edge_sampling == args2[i]\n    i += 1\n    assert args1.use_secondary_edge_sampling == args2[i]\n    i += 1\n    return True'"
tutorials/01_optimize_single_triangle.py,0,"b'# The Python interface of redner is defined in the pyredner package\nimport pyredner\nimport torch\n\n# Optimize three vertices of a single triangle\n# We first render a target image using redner,\n# then perturb the three vertices and optimize to match the target.\n\n# We need to tell redner first whether we are using GPU or not.\npyredner.set_use_gpu(torch.cuda.is_available())\n\n# Now we want to setup a 3D scene, represented in PyTorch tensors,\n# then feed it into redner to render an image.\n\n# First, we set up a camera.\n# redner assumes all the camera variables live in CPU memory,\n# so you should allocate torch tensors in CPU\ncam = pyredner.Camera(position = torch.tensor([0.0, 0.0, -5.0]),\n                      look_at = torch.tensor([0.0, 0.0, 0.0]),\n                      up = torch.tensor([0.0, 1.0, 0.0]),\n                      fov = torch.tensor([45.0]), # in degree\n                      clip_near = 1e-2, # needs to > 0\n                      resolution = (256, 256),\n                      fisheye = False)\n\n# Next, we setup the materials for the scene.\n# All materials in the scene are stored in a single Python list.\n# The index of a material in the list is its material id.\n# Our simple scene only has a single grey material with reflectance 0.5.\n# If you are using GPU, make sure to copy the reflectance to GPU memory.\nmat_grey = pyredner.Material(\\\n    diffuse_reflectance = \\\n        torch.tensor([0.5, 0.5, 0.5], device = pyredner.get_device()))\n# The material list of the scene\nmaterials = [mat_grey]\n\n# Next, we setup the geometry for the scene.\n# 3D objects in redner are called ""Shape"".\n# All shapes in the scene are stored in a single Python list,\n# the index of a shape in the list is its shape id.\n# Right now, a shape is always a triangle mesh, which has a list of\n# triangle vertices and a list of triangle indices.\n# The vertices are a Nx3 torch float tensor,\n# and the indices are a Mx3 torch integer tensor.\n# Optionally, for each vertex you can specify its UV coordinate for texture mapping,\n# and a normal for Phong interpolation.\n# Each shape also needs to be assigned a material using material id,\n# which is the index of the material in the material array.\n# If you are using GPU, make sure to store all tensors of the shape in GPU memory.\nshape_triangle = pyredner.Shape(\\\n    vertices = torch.tensor([[-1.7, 1.0, 0.0], [1.0, 1.0, 0.0], [-0.5, -1.0, 0.0]],\n        device = pyredner.get_device()),\n    indices = torch.tensor([[0, 1, 2]], dtype = torch.int32,\n        device = pyredner.get_device()),\n    uvs = None,\n    normals = None,\n    material_id = 0)\n# Merely having a single triangle is not enough for physically-based rendering.\n# We need to have a light source. Here we setup the shape of a quad area light source,\n# similary to the previous triangle.\nshape_light = pyredner.Shape(\\\n    vertices = torch.tensor([[-1.0, -1.0, -7.0],\n                             [ 1.0, -1.0, -7.0],\n                             [-1.0,  1.0, -7.0],\n                             [ 1.0,  1.0, -7.0]], device = pyredner.get_device()),\n    indices = torch.tensor([[0, 1, 2],[1, 3, 2]],\n        dtype = torch.int32, device = pyredner.get_device()),\n    uvs = None,\n    normals = None,\n    material_id = 0)\n# The shape list of our scene contains two shapes:\nshapes = [shape_triangle, shape_light]\n\n# Now we assign some of the shapes in the scene as light sources.\n# All area light sources in the scene are stored in a single Python list.\n# Each area light is attached to a shape using shape id, additionally we need to\n# assign the intensity of the light, which is a length 3 float tensor in CPU. \nlight = pyredner.AreaLight(shape_id = 1, \n                           intensity = torch.tensor([20.0,20.0,20.0]))\narea_lights = [light]\n# Finally we construct our scene using all the variables we setup previously.\nscene = pyredner.Scene(cam, shapes, materials, area_lights)\n# All PyTorch functions take a flat array of PyTorch tensors as input,\n# therefore we need to serialize the scene into an array. The following\n# function does this. We also specify how many Monte Carlo samples we want to \n# use per pixel and the number of bounces for indirect illumination here\n# (one bounce means only direct illumination).\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\n\n# Now we render the scene as our target image.\n# To render the scene, we use our custom PyTorch function in pyredner/render_pytorch.py\n# First setup the alias of the render function\nrender = pyredner.RenderFunction.apply\n# Next we call the render function to render.\n# The first argument is the seed for RNG in the renderer.\nimg = render(0, *scene_args)\n# This generates a PyTorch tensor with size [width, height, 3]. \n# The output image is in the GPU memory if you are using GPU.\n# Now we save the generated image to disk.\npyredner.imwrite(img.cpu(), \'results/optimize_single_triangle/target.exr\')\npyredner.imwrite(img.cpu(), \'results/optimize_single_triangle/target.png\')\n# Now we read back the target image we just saved, and copy to GPU if necessary\ntarget = pyredner.imread(\'results/optimize_single_triangle/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda()\n\n# Next we want to produce the initial guess. We do this by perturb the scene.\nshape_triangle.vertices = torch.tensor(\\\n    [[-2.0,1.5,0.3], [0.9,1.2,-0.3], [-0.4,-1.4,0.2]],\n    device = pyredner.get_device(),\n    requires_grad = True) # Set requires_grad to True since we want to optimize this\n# We need to serialize the scene again to get the new arguments.\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\n# Render the initial guess\nimg = render(1, *scene_args)\n# Save the image\npyredner.imwrite(img.cpu(), \'results/optimize_single_triangle/init.png\')\n# Compute the difference and save the images.\ndiff = torch.abs(target - img)\npyredner.imwrite(diff.cpu(), \'results/optimize_single_triangle/init_diff.png\')\n\n# Now we want to refine the initial guess using gradient-based optimization.\n# We use PyTorch\'s optimizer to do this. \noptimizer = torch.optim.Adam([shape_triangle.vertices], lr=5e-2)\n# Run 200 Adam iterations.\nfor t in range(200):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    # Forward pass: render the image\n    scene_args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4, # We use less samples in the Adam loop.\n        max_bounces = 1)\n    # Important to use a different seed every iteration, otherwise the result\n    # would be biased.\n    img = render(t+1, *scene_args)\n    # Save the intermediate render.\n    pyredner.imwrite(img.cpu(), \'results/optimize_single_triangle/iter_{}.png\'.format(t))\n    # Compute the loss function. Here it is L2.\n    loss = (img - target).pow(2).sum()\n    print(\'loss:\', loss.item())\n\n    # Backpropagate the gradients.\n    loss.backward()\n    # Print the gradients of the three vertices.\n    print(\'grad:\', shape_triangle.vertices.grad)\n\n    # Take a gradient descent step.\n    optimizer.step()\n    # Print the current three vertices.\n    print(\'vertices:\', shape_triangle.vertices)\n\n# Render the final result.\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 1)\nimg = render(202, *scene_args)\n# Save the images and differences.\npyredner.imwrite(img.cpu(), \'results/optimize_single_triangle/final.exr\')\npyredner.imwrite(img.cpu(), \'results/optimize_single_triangle/final.png\')\npyredner.imwrite(torch.abs(target - img).cpu(), \'results/optimize_single_triangle/final_diff.png\')\n\n# Convert the intermediate renderings to a video.\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/optimize_single_triangle/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/optimize_single_triangle/out.mp4""])\n'"
tutorials/02_pose_estimation.py,0,"b'import pyredner\nimport torch\n\n# Estimate the pose of a teapot object.\n# In this tutorial we will learn a few more advanced features of redner, \n# through a pose estimation example. In particular, we demonstrate:\n# 1. How to load a Wavefront object file\n# 2. How to generate smooth vertex normals\n# 3. apply environment lighting\n# 4. How to apply mesh operations (rotation and translation in this case) in PyTorch\n# Like the first tutorial, we first render a target image, then perturb the\n# rotation/translation parameters and optimize to match the target.\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\n# Load from the teapot Wavefront object file.\n# load_obj function returns three lists/dicts\n# material_map is a dict containing all the materials used in the obj file,\n# where the key is the name of material, and the value is a pyredner.Material\n#\n# mesh_list is a list containing all the meshes in the obj file, grouped by use_mtl calls.\n# Each element in the list is a tuple with length 2, the first is the name of material,\n# the second is a pyredner.TriangleMesh with mesh information.\n#\n# light_map is a Python dict, where the key is the material names with non zeros Ke,\n# and the values are the Ke\nmaterial_map, mesh_list, light_map = pyredner.load_obj(\'teapot.obj\')\n# The teapot we loaded is relatively low-poly and doesn\'t have vertex normal.\n# Fortunately we can compute the vertex normal from the neighbor vertices.\n# We can use pyredner.compute_vertex_normal for this task:\n# (Try commenting out the following two lines to see the differences in target images!)\nfor _, mesh in mesh_list:\n    mesh.normals = pyredner.compute_vertex_normal(mesh.vertices, mesh.indices)\n\n# Setup camera\ncam = pyredner.Camera(position = torch.tensor([0.0, 30.0, 200.0]),\n                      look_at = torch.tensor([0.0, 30.0, 0.0]),\n                      up = torch.tensor([0.0, 1.0, 0.0]),\n                      fov = torch.tensor([45.0]), # in degree\n                      clip_near = 1e-2, # needs to > 0\n                      resolution = (256, 256),\n                      fisheye = False)\n\n# Next, we convert the materials loaded from the Wavefront object files to a\n# Python list of material. At the same time we keep track of the id of the materials,\n# so that we can assign them to the shapes later.\nmaterial_id_map = {}\nmaterials = []\ncount = 0\nfor key, value in material_map.items():\n    material_id_map[key] = count\n    count += 1\n    materials.append(value)\n\n# Now we build a list of shapes using the list loaded from the Wavefront object file.\n# Meshes loaded from .obj files may have different indices for uvs and normals,\n# we use mesh.uv_indices and mesh.normal_indices to access them.\n# This mesh does not have normal_indices so the value is None.\nshapes = []\nfor mtl_name, mesh in mesh_list:\n    assert(mesh.normal_indices is None)\n    shapes.append(pyredner.Shape(\\\n        vertices = mesh.vertices,\n        indices = mesh.indices,\n        material_id = material_id_map[mtl_name],\n        uvs = mesh.uvs,\n        normals = mesh.normals,\n        uv_indices = mesh.uv_indices))\n\n# The previous tutorial used a mesh area light for the scene lighting, \n# here we use an environment light,\n# which is a texture representing infinitely far away light sources in \n# spherical coordinates.\nenvmap = pyredner.imread(\'sunsky.exr\')\nif pyredner.get_use_gpu():\n    envmap = envmap.cuda()\nenvmap = pyredner.EnvironmentMap(envmap)\n\n# Finally we construct our scene using all the variables we setup previously.\nscene = pyredner.Scene(cam, shapes, materials, area_lights = [], envmap = envmap)\n# Like the previous tutorial, we serialize and render the scene, \n# save it as our target\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 512,\n    max_bounces = 1)\nrender = pyredner.RenderFunction.apply\nimg = render(0, *scene_args)\npyredner.imwrite(img.cpu(), \'results/pose_estimation/target.exr\')\npyredner.imwrite(img.cpu(), \'results/pose_estimation/target.png\')\ntarget = pyredner.imread(\'results/pose_estimation/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda()\n\n# Now we want to generate the initial guess.\n# We want to rotate and translate the teapot. We do this by declaring\n# PyTorch tensors of translation and rotation parameters,\n# then apply them to all teapot vertices.\n# The translation and rotation parameters have very different ranges, so we normalize them\n# by multiplying the translation parameters 100 to map to the actual translation amounts.\ntranslation_params = torch.tensor([0.1, -0.1, 0.1],\n    device = pyredner.get_device(), requires_grad=True)\ntranslation = translation_params * 100.0\neuler_angles = torch.tensor([0.1, -0.1, 0.1], requires_grad=True)\n# We obtain the teapot vertices we want to apply the transformation on.\nshape0_vertices = shapes[0].vertices.clone()\nshape1_vertices = shapes[1].vertices.clone()\n# We can use pyredner.gen_rotate_matrix to generate 3x3 rotation matrices\nrotation_matrix = pyredner.gen_rotate_matrix(euler_angles)\nif pyredner.get_use_gpu():\n    rotation_matrix = rotation_matrix.cuda()\ncenter = torch.mean(torch.cat([shape0_vertices, shape1_vertices]), 0)\n# We shift the vertices to the center, apply rotation matrix,\n# then shift back to the original space.\nshapes[0].vertices = \\\n    (shape0_vertices - center) @ torch.t(rotation_matrix) + \\\n    center + translation\nshapes[1].vertices = \\\n    (shape1_vertices - center) @ torch.t(rotation_matrix) + \\\n    center + translation\n# Since we changed the vertices, we need to regenerate the shading normals\nshapes[0].normals = pyredner.compute_vertex_normal(shapes[0].vertices, shapes[0].indices)\nshapes[1].normals = pyredner.compute_vertex_normal(shapes[1].vertices, shapes[1].indices)\n# We need to serialize the scene again to get the new arguments.\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 512,\n    max_bounces = 1)\n# Render the initial guess.\nimg = render(1, *scene_args)\n# Save the images.\npyredner.imwrite(img.cpu(), \'results/pose_estimation/init.png\')\n# Compute the difference and save the images.\ndiff = torch.abs(target - img)\npyredner.imwrite(diff.cpu(), \'results/pose_estimation/init_diff.png\')\n\n# Optimize for pose parameters.\noptimizer = torch.optim.Adam([translation_params, euler_angles], lr=1e-2)\n# Run 200 Adam iterations.\nfor t in range(200):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    # Forward pass: apply the mesh operation and render the image.\n    translation = translation_params * 100.0\n    rotation_matrix = pyredner.gen_rotate_matrix(euler_angles)\n    if pyredner.get_use_gpu():\n        rotation_matrix = rotation_matrix.cuda()\n    center = torch.mean(torch.cat([shape0_vertices, shape1_vertices]), 0)\n    shapes[0].vertices = \\\n        (shape0_vertices - center) @ torch.t(rotation_matrix) + \\\n        center + translation\n    shapes[1].vertices = \\\n        (shape1_vertices - center) @ torch.t(rotation_matrix) + \\\n        center + translation\n    shapes[0].normals = pyredner.compute_vertex_normal(shapes[0].vertices, shapes[0].indices)\n    shapes[1].normals = pyredner.compute_vertex_normal(shapes[1].vertices, shapes[1].indices)\n    scene_args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4, # We use less samples in the Adam loop.\n        max_bounces = 1)\n    # Important to use a different seed every iteration, otherwise the result\n    # would be biased.\n    img = render(t+1, *scene_args)\n    # Save the intermediate render.\n    pyredner.imwrite(img.cpu(), \'results/pose_estimation/iter_{}.png\'.format(t))\n    # Compute the loss function. Here it is L2.\n    loss = (img - target).pow(2).sum()\n    print(\'loss:\', loss.item())\n\n    # Backpropagate the gradients.\n    loss.backward()\n    # Print the gradients\n    print(\'translation_params.grad:\', translation_params.grad)\n    print(\'euler_angles.grad:\', euler_angles.grad)\n\n    # Take a gradient descent step.\n    optimizer.step()\n    # Print the current pose parameters.\n    print(\'translation:\', translation)\n    print(\'euler_angles:\', euler_angles)\n\n# Render the final result.\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 512,\n    max_bounces = 1)\nimg = render(202, *scene_args)\n# Save the images and differences.\npyredner.imwrite(img.cpu(), \'results/pose_estimation/final.exr\')\npyredner.imwrite(img.cpu(), \'results/pose_estimation/final.png\')\npyredner.imwrite(torch.abs(target - img).cpu(), \'results/pose_estimation/final_diff.png\')\n\n# Convert the intermediate renderings to a video.\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/pose_estimation/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/pose_estimation/out.mp4""])\n'"
tutorials/04_fast_deferred_rendering.py,0,"b'import pyredner\nimport redner\nimport torch\nimport math\n\n# Estimate the pose of a teapot object.\n# This tutorial demonstrates:\n# 1. how to render G buffer, such as depth, normal, albedo\n# 2. how to use G buffer to do ""deferred rendering"" in pytorch, which bypasses the main path tracing\n#    process in redner, resulting in fast approximate rendering\n# You might want to read the wikipedia page first if you are not familiar with the concept\n# of deferred rendering: https://en.wikipedia.org/wiki/Deferred_shading\n#\n# Like the second tutorial, we first render a target image, then perturb the\n# rotation/translation parameters and optimize to match the target.\n\n# Use GPU if available\npyredner.set_use_gpu(torch.cuda.is_available())\n\n# Load from the teapot Wavefront object file just like tutorial 02\nmaterial_map, mesh_list, light_map = pyredner.load_obj(\'teapot.obj\')\n# Compute shading normal\nfor _, mesh in mesh_list:\n    mesh.normals = pyredner.compute_vertex_normal(mesh.vertices, mesh.indices)\n\n# Setup camera\ncam = pyredner.Camera(position = torch.tensor([0.0, 30.0, 200.0]),\n                      look_at = torch.tensor([0.0, 30.0, 0.0]),\n                      up = torch.tensor([0.0, 1.0, 0.0]),\n                      fov = torch.tensor([45.0]), # in degree\n                      clip_near = 1e-2, # needs to > 0\n                      resolution = (256, 256),\n                      fisheye = False)\n\n# Get a list of materials from material_map\nmaterial_id_map = {}\nmaterials = []\ncount = 0\nfor key, value in material_map.items():\n    material_id_map[key] = count\n    count += 1\n    materials.append(value)\n\n# Get a list of shapes\nshapes = []\nfor mtl_name, mesh in mesh_list:\n    shapes.append(pyredner.Shape(\\\n        vertices = mesh.vertices,\n        indices = mesh.indices,\n        uvs = mesh.uvs,\n        normals = mesh.normals,\n        material_id = material_id_map[mtl_name]))\n\n# Construct the scene\n# Unlike previous tutorials, here we don\'t setup any light sources\nscene = pyredner.Scene(cam, shapes, materials, area_lights = [], envmap = None)\n# Serialize the scene.\n# There are two difference comparing to previous tutorials.\n# 1. we set ""max_bounces"" to 0, so we do not perform path tracing at all.\n# 2. there is an extra argument ""channels"", specifying the output as position, shading normal, and albedo\n#    by default the channels is a list with a single item redner.channels.radiance, which contains\n#    the path tracing output.\n#    See channels.h for a full list of channels we support.\n#    The channels argument can also be useful for e.g. RGBD rendering.\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16, # Still need some samples for anti-aliasing\n    max_bounces = 0,\n    channels = [redner.channels.position,\n                redner.channels.shading_normal,\n                redner.channels.diffuse_reflectance])\nrender = pyredner.RenderFunction.apply\ng_buffer = render(0, *scene_args)\n# Now, since we specified the outputs to be position, normal, and albedo,\n# g_buffer is a 9-channels image\npos = g_buffer[:, :, :3]\nnormal = g_buffer[:, :, 3:6]\nalbedo = g_buffer[:, :, 6:9]\n# Next, we render the g-buffer into a final image\n# For this we define a deferred_render function:\ndef deferred_render(pos, normal, albedo):\n    # We assume a point light at the camera origin (0, 30, 200)\n    # The lighting consists of a geometry term cos/d^2, albedo, and the light intensity\n    light_pos = torch.tensor([0.0, 30.0, 200.0], device = pyredner.get_device())\n    light_pos = light_pos.view(1, 1, 3)\n    light_intensity = torch.tensor([10000.0, 10000.0, 10000.0], device = pyredner.get_device())\n    light_intensity = light_intensity.view(1, 1, 3)\n    light_dir = light_pos - pos\n    # the d^2 term:\n    light_dist_sq = torch.sum(light_dir * light_dir, 2, keepdim = True)\n    light_dist = torch.sqrt(light_dist_sq)\n    # Normalize light direction\n    light_dir = light_dir / light_dist\n    dot_l_n = torch.sum(light_dir * normal, 2, keepdim = True)\n    return light_intensity * dot_l_n * (albedo / math.pi) / light_dist_sq \nimg = deferred_render(pos, normal, albedo)\n# Save the images\npyredner.imwrite(img.cpu(), \'results/fast_deferred_rendering/target.exr\')\npyredner.imwrite(img.cpu(), \'results/fast_deferred_rendering/target.png\')\n# Load the targets back\ntarget = pyredner.imread(\'results/fast_deferred_rendering/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda()\n\n# Same as tutorial 2, perturb the scene by a translation and a rotation to the object\ntranslation_params = torch.tensor([0.1, -0.1, 0.1],\n    device = pyredner.get_device(), requires_grad=True)\ntranslation = translation_params * 100.0\neuler_angles = torch.tensor([0.1, -0.1, 0.1], requires_grad=True)\nshape0_vertices = shapes[0].vertices.clone()\nshape1_vertices = shapes[1].vertices.clone()\nrotation_matrix = pyredner.gen_rotate_matrix(euler_angles)\nif pyredner.get_use_gpu():\n    rotation_matrix = rotation_matrix.cuda()\ncenter = torch.mean(torch.cat([shape0_vertices, shape1_vertices]), 0)\nshapes[0].vertices = \\\n    (shape0_vertices - center) @ torch.t(rotation_matrix) + \\\n    center + translation\nshapes[1].vertices = \\\n    (shape1_vertices - center) @ torch.t(rotation_matrix) + \\\n    center + translation\n# Since we changed the vertices, we need to regenerate the shading normals\nshapes[0].normals = pyredner.compute_vertex_normal(shapes[0].vertices, shapes[0].indices)\nshapes[1].normals = pyredner.compute_vertex_normal(shapes[1].vertices, shapes[1].indices)\n# We need to serialize the scene again to get the new arguments.\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16, # Still need some samples for anti-aliasing\n    max_bounces = 0,\n    channels = [redner.channels.position,\n                redner.channels.shading_normal,\n                redner.channels.diffuse_reflectance])\n# Render the initial guess.\ng_buffer = render(1, *scene_args)\npos = g_buffer[:, :, :3]\nnormal = g_buffer[:, :, 3:6]\nalbedo = g_buffer[:, :, 6:9]\nimg = deferred_render(pos, normal, albedo)\n# Save the images\npyredner.imwrite(img.cpu(), \'results/fast_deferred_rendering/init.png\')\n# Compute the difference and save the images.\ndiff = torch.abs(target - img)\npyredner.imwrite(diff.cpu(), \'results/fast_deferred_rendering/init_diff.png\')\n\n# Optimize for pose parameters.\noptimizer = torch.optim.Adam([translation_params, euler_angles], lr=1e-2)\n# Run 200 Adam iterations.\nfor t in range(200):\n    print(\'iteration:\', t)\n    optimizer.zero_grad()\n    # Forward pass: apply the mesh operation and render the image.\n    translation = translation_params * 100.0\n    rotation_matrix = pyredner.gen_rotate_matrix(euler_angles)\n    if pyredner.get_use_gpu():\n        rotation_matrix = rotation_matrix.cuda()\n    center = torch.mean(torch.cat([shape0_vertices, shape1_vertices]), 0)\n    shapes[0].vertices = \\\n        (shape0_vertices - center) @ torch.t(rotation_matrix) + \\\n        center + translation\n    shapes[1].vertices = \\\n        (shape1_vertices - center) @ torch.t(rotation_matrix) + \\\n        center + translation\n    shapes[0].normals = pyredner.compute_vertex_normal(shapes[0].vertices, shapes[0].indices)\n    shapes[1].normals = pyredner.compute_vertex_normal(shapes[1].vertices, shapes[1].indices)\n    scene_args = pyredner.RenderFunction.serialize_scene(\\\n        scene = scene,\n        num_samples = 4, # Use less in Adam iteration\n        max_bounces = 0,\n        channels = [redner.channels.position,\n                    redner.channels.shading_normal,\n                    redner.channels.diffuse_reflectance])\n    # Important to use a different seed every iteration, otherwise the result\n    # would be biased.\n    g_buffer = render(t+1, *scene_args)\n    pos = g_buffer[:, :, :3]\n    normal = g_buffer[:, :, 3:6]\n    albedo = g_buffer[:, :, 6:9]\n    img = deferred_render(pos, normal, albedo)\n\n    # Save the intermediate render.\n    pyredner.imwrite(img.cpu(), \'results/fast_deferred_rendering/iter_{}.png\'.format(t))\n    # Compute the loss function. Here it is L2.\n    loss = (img - target).pow(2).sum()\n    print(\'loss:\', loss.item())\n\n    # Backpropagate the gradients.\n    loss.backward()\n    # Print the gradients\n    print(\'translation_params.grad:\', translation_params.grad)\n    print(\'euler_angles.grad:\', euler_angles.grad)\n\n    # Take a gradient descent step.\n    optimizer.step()\n    # Print the current pose parameters.\n    print(\'translation:\', translation)\n    print(\'euler_angles:\', euler_angles)\n\n# Render the final result.\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 16,\n    max_bounces = 0,\n    channels = [redner.channels.position,\n                redner.channels.shading_normal,\n                redner.channels.diffuse_reflectance])\ng_buffer = render(202, *scene_args)\npos = g_buffer[:, :, :3]\nnormal = g_buffer[:, :, 3:6]\nalbedo = g_buffer[:, :, 6:9]\nimg = deferred_render(pos, normal, albedo)\n# Save the images and differences.\npyredner.imwrite(img.cpu(), \'results/fast_deferred_rendering/final.exr\')\npyredner.imwrite(img.cpu(), \'results/fast_deferred_rendering/final.png\')\npyredner.imwrite(torch.abs(target - img).cpu(), \'results/fast_deferred_rendering/final_diff.png\')\n\n# Convert the intermediate renderings to a video.\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/fast_deferred_rendering/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/fast_deferred_rendering/out.mp4""])'"
tutorials/05_coarse_to_fine_estimation.py,0,"b'import pyredner\nimport torch\nimport scipy\nimport scipy.ndimage\nimport numpy as np\n\n# Joint material & camera pose estimation with global illumination\n# using coarse to fine estimation\n\n# This time we will learn the following through a Cornell box example.\n# - Loading from a Mitsuba scene file\n# - Global illumination and glossy/specular materials\n# - Coarse to fine estimation with a Gaussian pyramid loss\n# - Incorporate box constraints in your optimizer\n\n# In addition to Wavefront obj file, redner also supports loading from a Mitsuba\n# scene file. Currently we only support a limited amount of features. In particular\n# we only support two kinds of materials: diffuse and roughplastic. Note that the\n# ""alpha"" values in roughplastic is the square root of the roughness. See cbox.xml\n# for how a Mitsuba scene file should look like.\n# We can load a scene using pyredner.load_mitsuba() utility, and render it as usual.\nscene = pyredner.load_mitsuba(\'cbox/cbox.xml\')\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 512,\n    max_bounces = 5) # Set max_bounces = 5 for global illumination\nrender = pyredner.RenderFunction.apply\nimg = render(0, *scene_args)\npyredner.imwrite(img.cpu(), \'results/coarse_to_fine_estimation/target.exr\')\npyredner.imwrite(img.cpu(), \'results/coarse_to_fine_estimation/target.png\')\ntarget = pyredner.imread(\'results/coarse_to_fine_estimation/target.exr\')\nif pyredner.get_use_gpu():\n    target = target.cuda()\n\n# Now let\'s generate an initial guess by perturbing the reference.\n# Let\'s set all the diffuse color to gray by manipulating material.diffuse_reflectance.\n# We also store all the material variables to optimize in a list.\nmaterial_vars = []\nfor mi, m in enumerate(scene.materials):\n    var = torch.tensor([0.5, 0.5, 0.5],\n                       device = pyredner.get_device(),\n                       requires_grad = True)\n    material_vars.append(var)\n    m.diffuse_reflectance = pyredner.Texture(var)\n        \n# And let\'s also slightly perturb the camera up vector and field of view a bit\nup = torch.tensor([0.2, 0.8, -0.2], requires_grad = True)\nfov = torch.tensor([41.0], requires_grad = True)\ncam_vars = [up, fov]\nscene.camera = pyredner.Camera(\\\n    position = scene.camera.position,\n    look_at = scene.camera.look_at,\n    up = up,\n    fov = fov,\n    clip_near = scene.camera.clip_near,\n    resolution = scene.camera.resolution)\n# Serialize the scene and render the initial guess\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 512,\n    max_bounces = 5)\nimg = render(1, *scene_args)\npyredner.imwrite(img.cpu(), \'results/coarse_to_fine_estimation/init.png\')\n\n# Optimize for parameters.\noptimizer = torch.optim.Adam(material_vars + cam_vars, lr=5e-3)\n# We run a coarse-to-fine estimation here to prevent being trapped in local minimum\n# The final resolution is 256x256, but we will start from an 64x64 image\n# We can also tweak the number of iterations, samples per pixel, number of bounces,\n# learning rate for each resolution\nres = [64, 256]\niters = [400, 200]\nspp = [16, 4]\nbounces = [5, 5]\nlrs = [5e-3, 1e-3]\niter_count = 0\nfor ri, r in enumerate(res):\n    scene.camera.resolution = (r, r)\n    # Downsample the target to match our current resolution\n    resampled_target = target.cpu().numpy()\n    if r != 256:\n        resampled_target = scipy.ndimage.interpolation.zoom(\\\n            resampled_target, (r/256.0, r/256.0, 1.0),\n            order=1)\n    resampled_target = torch.from_numpy(resampled_target)\n    if pyredner.get_use_gpu():\n        resampled_target = resampled_target.cuda()\n\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lrs[ri]\n\n    for t in range(iters[ri]):\n        print(\'resolution:\', r, \', iteration:\', iter_count)\n        optimizer.zero_grad()\n        # Forward pass: serialize the scene and render the image\n        # Need to redefine the camera\n        scene.camera = pyredner.Camera(\\\n            position = scene.camera.position,\n            look_at = scene.camera.look_at,\n            up = up,\n            fov = fov,\n            clip_near = scene.camera.clip_near,\n            resolution = scene.camera.resolution)\n        scene_args = pyredner.RenderFunction.serialize_scene(\\\n            scene = scene,\n            num_samples = spp[ri],\n            max_bounces = bounces[ri])\n        # Important to use a different seed every iteration, otherwise the result\n        # would be biased.\n        img = render(iter_count+1, *scene_args)\n        # Save the intermediate render.\n        img_np = img.data.cpu().numpy()\n        img_np = scipy.ndimage.interpolation.zoom(\\\n            img_np, (256.0/r, 256.0/r, 1.0),\n            order=1)\n        pyredner.imwrite(torch.from_numpy(img_np),\n            \'results/coarse_to_fine_estimation/iter_{}.png\'.format(iter_count))\n        iter_count += 1\n\n        # Compute the loss function. Here we use a Gaussian pyramid loss function\n        # We also clamp the difference between -1 and 1 to prevent\n        # light source from being dominating the loss function\n        diff = (img - resampled_target).clamp(-1.0, 1.0)\n\n        # Now we convolve diff with Gaussian filter and downsample.\n        # We use PyTorch\'s conv2d function and AvgPool2d to achieve this.\n        # We need to first define a Gaussian kernel:\n        dirac = np.zeros((7,7), dtype = np.float32)\n        dirac[3,3] = 1.0\n        f = np.zeros([3, 3, 7, 7], dtype = np.float32)\n        gf = scipy.ndimage.filters.gaussian_filter(dirac, 1.0)\n        f[0, 0, :, :] = gf\n        f[1, 1, :, :] = gf\n        f[2, 2, :, :] = gf\n        f = torch.from_numpy(f)\n        m = torch.nn.AvgPool2d(2)\n        if pyredner.get_use_gpu():\n            f = f.cuda()\n\n        diff_0 = diff.view(1, r, r, 3).permute(0, 3, 2, 1)\n        # Convolve then downsample\n        diff_1 = m(torch.nn.functional.conv2d(diff_0, f, padding=3))\n        diff_2 = m(torch.nn.functional.conv2d(diff_1, f, padding=3))\n        diff_3 = m(torch.nn.functional.conv2d(diff_2, f, padding=3))\n        diff_4 = m(torch.nn.functional.conv2d(diff_3, f, padding=3))\n        loss = diff_0.pow(2).sum() / (r * r) + \\\n               diff_1.pow(2).sum() / ((r/2.)*(r/2.)) + \\\n               diff_2.pow(2).sum() / ((r/4.)*(r/4.)) + \\\n               diff_3.pow(2).sum() / ((r/8.)*(r/8.)) + \\\n               diff_4.pow(2).sum() / ((r/16.)*(r/16.))\n        print(\'loss:\', loss.item())\n\n        # Backpropagate the gradients.\n        loss.backward()\n\n        # Take a gradient descent step.\n        optimizer.step()\n        print(\'up:\', up)\n        print(\'fov:\', fov)\n\n        # Important: the material parameters has hard constraints: the\n        # reflectance and roughness cannot be negative. We enforce them here\n        # by projecting the values to the boundaries.\n        for var in material_vars:\n            var.data = var.data.clamp(1e-5, 1.0)\n            print(var)\n\n# Render the final result.\nscene.camera = pyredner.Camera(\\\n    position = scene.camera.position,\n    look_at = scene.camera.look_at,\n    up = up,\n    fov = fov,\n    clip_near = scene.camera.clip_near,\n    resolution = scene.camera.resolution)\nscene_args = pyredner.RenderFunction.serialize_scene(\\\n    scene = scene,\n    num_samples = 512,\n    max_bounces = 5)\nimg = render(402, *scene_args)\n# Save the images and differences.\npyredner.imwrite(img.cpu(), \'results/coarse_to_fine_estimation/final.exr\')\npyredner.imwrite(img.cpu(), \'results/coarse_to_fine_estimation/final.png\')\npyredner.imwrite(torch.abs(target - img).cpu(), \'results/coarse_to_fine_estimation/final_diff.png\')\n\n# Convert the intermediate renderings to a video.\nfrom subprocess import call\ncall([""ffmpeg"", ""-framerate"", ""24"", ""-i"",\n    ""results/coarse_to_fine_estimation/iter_%d.png"", ""-vb"", ""20M"",\n    ""results/coarse_to_fine_estimation/out.mp4""])\n'"
docs/source/conf.py,0,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\n# sys.path.insert(0, os.path.abspath(\'../..\'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'redner\'\ncopyright = \'2019, Tzu-Mao Li\'\nauthor = \'Tzu-Mao Li\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nimport sphinx_rtd_theme\nextensions = [\'sphinx.ext.autodoc\',\n\t\t\t  \'sphinx.ext.coverage\',\n\t\t\t  \'sphinx.ext.napoleon\',\n\t\t\t  \'sphinx_rtd_theme\',\n\t\t\t  \'autoapi.extension\']\n\nautoapi_dirs = [\'../../pyredner\', \'../../pyredner_tensorflow\']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\'\']\n\nautoapi_generate_api_docs = False\n\nmaster_doc = \'index\'\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n'"
