file_path,api_count,code
CRNN/config.py,0,"b'# Constants\n\n# Supported characters\nCHAR_VECTOR = ""0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ-\'.!?,\\""&""\n\n# Number of classes\nNUM_CLASSES = len(CHAR_VECTOR) + 1\n'"
CRNN/crnn.py,54,"b'import os\nimport time\nimport numpy as np\nimport tensorflow as tf\nfrom scipy.misc import imread, imresize, imsave\nfrom tensorflow.contrib import rnn\n\nfrom data_manager import DataManager\nfrom utils import (\n    sparse_tuple_from,\n    resize_image,\n    label_to_array,\n    ground_truth_to_word,\n    levenshtein,\n)\n\nos.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""3""\n\n\nclass CRNN(object):\n    def __init__(\n        self,\n        batch_size,\n        model_path,\n        examples_path,\n        max_image_width,\n        train_test_ratio,\n        restore,\n        char_set_string,\n        use_trdg,\n        language,\n    ):\n        self.step = 0\n        self.CHAR_VECTOR = char_set_string\n        self.NUM_CLASSES = len(self.CHAR_VECTOR) + 1\n\n        print(""CHAR_VECTOR {}"".format(self.CHAR_VECTOR))\n        print(""NUM_CLASSES {}"".format(self.NUM_CLASSES))\n\n        self.model_path = model_path\n        self.save_path = os.path.join(model_path, ""ckp"")\n\n        self.restore = restore\n\n        self.training_name = str(int(time.time()))\n        self.session = tf.Session()\n\n        # Building graph\n        with self.session.as_default():\n            (\n                self.inputs,\n                self.targets,\n                self.seq_len,\n                self.logits,\n                self.decoded,\n                self.optimizer,\n                self.acc,\n                self.cost,\n                self.max_char_count,\n                self.init,\n            ) = self.crnn(max_image_width)\n            self.init.run()\n\n        with self.session.as_default():\n            self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=10)\n            # Loading last save if needed\n            if self.restore:\n                print(""Restoring"")\n                ckpt = tf.train.latest_checkpoint(self.model_path)\n                if ckpt:\n                    print(""Checkpoint is valid"")\n                    self.step = int(ckpt.split(""-"")[1])\n                    self.saver.restore(self.session, ckpt)\n\n        # Creating data_manager\n        self.data_manager = DataManager(\n            batch_size,\n            model_path,\n            examples_path,\n            max_image_width,\n            train_test_ratio,\n            self.max_char_count,\n            self.CHAR_VECTOR,\n            use_trdg,\n            language,\n        )\n\n    def crnn(self, max_width):\n        def BidirectionnalRNN(inputs, seq_len):\n            """"""\n                Bidirectionnal LSTM Recurrent Neural Network part\n            """"""\n\n            with tf.variable_scope(None, default_name=""bidirectional-rnn-1""):\n                # Forward\n                lstm_fw_cell_1 = rnn.BasicLSTMCell(256)\n                # Backward\n                lstm_bw_cell_1 = rnn.BasicLSTMCell(256)\n\n                inter_output, _ = tf.nn.bidirectional_dynamic_rnn(\n                    lstm_fw_cell_1, lstm_bw_cell_1, inputs, seq_len, dtype=tf.float32\n                )\n\n                inter_output = tf.concat(inter_output, 2)\n\n            with tf.variable_scope(None, default_name=""bidirectional-rnn-2""):\n                # Forward\n                lstm_fw_cell_2 = rnn.BasicLSTMCell(256)\n                # Backward\n                lstm_bw_cell_2 = rnn.BasicLSTMCell(256)\n\n                outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n                    lstm_fw_cell_2,\n                    lstm_bw_cell_2,\n                    inter_output,\n                    seq_len,\n                    dtype=tf.float32,\n                )\n\n                outputs = tf.concat(outputs, 2)\n\n            return outputs\n\n        def CNN(inputs):\n            """"""\n                Convolutionnal Neural Network part\n            """"""\n\n            # 64 / 3 x 3 / 1 / 1\n            conv1 = tf.layers.conv2d(\n                inputs=inputs,\n                filters=64,\n                kernel_size=(3, 3),\n                padding=""same"",\n                activation=tf.nn.relu,\n            )\n\n            # 2 x 2 / 1\n            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n\n            # 128 / 3 x 3 / 1 / 1\n            conv2 = tf.layers.conv2d(\n                inputs=pool1,\n                filters=128,\n                kernel_size=(3, 3),\n                padding=""same"",\n                activation=tf.nn.relu,\n            )\n\n            # 2 x 2 / 1\n            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n\n            # 256 / 3 x 3 / 1 / 1\n            conv3 = tf.layers.conv2d(\n                inputs=pool2,\n                filters=256,\n                kernel_size=(3, 3),\n                padding=""same"",\n                activation=tf.nn.relu,\n            )\n\n            # Batch normalization layer\n            bnorm1 = tf.layers.batch_normalization(conv3)\n\n            # 256 / 3 x 3 / 1 / 1\n            conv4 = tf.layers.conv2d(\n                inputs=bnorm1,\n                filters=256,\n                kernel_size=(3, 3),\n                padding=""same"",\n                activation=tf.nn.relu,\n            )\n\n            # 1 x 2 / 1\n            pool3 = tf.layers.max_pooling2d(\n                inputs=conv4, pool_size=[2, 2], strides=[1, 2], padding=""same""\n            )\n\n            # 512 / 3 x 3 / 1 / 1\n            conv5 = tf.layers.conv2d(\n                inputs=pool3,\n                filters=512,\n                kernel_size=(3, 3),\n                padding=""same"",\n                activation=tf.nn.relu,\n            )\n\n            # Batch normalization layer\n            bnorm2 = tf.layers.batch_normalization(conv5)\n\n            # 512 / 3 x 3 / 1 / 1\n            conv6 = tf.layers.conv2d(\n                inputs=bnorm2,\n                filters=512,\n                kernel_size=(3, 3),\n                padding=""same"",\n                activation=tf.nn.relu,\n            )\n\n            # 1 x 2 / 2\n            pool4 = tf.layers.max_pooling2d(\n                inputs=conv6, pool_size=[2, 2], strides=[1, 2], padding=""same""\n            )\n\n            # 512 / 2 x 2 / 1 / 0\n            conv7 = tf.layers.conv2d(\n                inputs=pool4,\n                filters=512,\n                kernel_size=(2, 2),\n                padding=""valid"",\n                activation=tf.nn.relu,\n            )\n\n            return conv7\n\n        batch_size = None\n        inputs = tf.placeholder(\n            tf.float32, [batch_size, max_width, 32, 1], name=""input""\n        )\n\n        # Our target output\n        targets = tf.sparse_placeholder(tf.int32, name=""targets"")\n\n        # The length of the sequence\n        seq_len = tf.placeholder(tf.int32, [None], name=""seq_len"")\n\n        cnn_output = CNN(inputs)\n        reshaped_cnn_output = tf.squeeze(cnn_output, [2])\n        max_char_count = cnn_output.get_shape().as_list()[1]\n\n        crnn_model = BidirectionnalRNN(reshaped_cnn_output, seq_len)\n\n        logits = tf.reshape(crnn_model, [-1, 512])\n        W = tf.Variable(\n            tf.truncated_normal([512, self.NUM_CLASSES], stddev=0.1), name=""W""\n        )\n        b = tf.Variable(tf.constant(0.0, shape=[self.NUM_CLASSES]), name=""b"")\n\n        logits = tf.matmul(logits, W) + b\n        logits = tf.reshape(\n            logits, [tf.shape(cnn_output)[0], max_char_count, self.NUM_CLASSES]\n        )\n\n        # Final layer, the output of the BLSTM\n        logits = tf.transpose(logits, (1, 0, 2))\n\n        # Loss and cost calculation\n        loss = tf.nn.ctc_loss(\n            targets, logits, seq_len, ignore_longer_outputs_than_inputs=True\n        )\n\n        cost = tf.reduce_mean(loss)\n\n        # Training step\n        optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n\n        # The decoded answer\n        decoded, log_prob = tf.nn.ctc_beam_search_decoder(\n            logits, seq_len, merge_repeated=False\n        )\n        dense_decoded = tf.sparse_tensor_to_dense(\n            decoded[0], default_value=-1, name=""dense_decoded""\n        )\n\n        # The error rate\n        acc = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32), targets))\n\n        init = tf.global_variables_initializer()\n\n        return (\n            inputs,\n            targets,\n            seq_len,\n            logits,\n            dense_decoded,\n            optimizer,\n            acc,\n            cost,\n            max_char_count,\n            init,\n        )\n\n    def train(self, iteration_count):\n        with self.session.as_default():\n            print(""Training"")\n            for i in range(self.step, iteration_count + self.step):\n                batch_count = 0\n                iter_loss = 0\n                for batch_y, batch_dt, batch_x in self.data_manager.train_batches:\n                    op, decoded, loss_value, acc = self.session.run(\n                        [self.optimizer, self.decoded, self.cost, self.acc],\n                        feed_dict={\n                            self.inputs: batch_x,\n                            self.seq_len: [self.max_char_count]\n                            * self.data_manager.batch_size,\n                            self.targets: batch_dt,\n                        },\n                    )\n\n                    if i % 10 == 0:\n                        for j in range(2):\n                            pred = ground_truth_to_word(decoded[j], self.CHAR_VECTOR)\n                            print(""{} | {}"".format(batch_y[j], pred))\n                        print(""---- {} | {} ----"".format(i, batch_count))\n\n                    iter_loss += loss_value\n                    batch_count += 1\n                    if batch_count >= 100:\n                        break\n\n                self.saver.save(self.session, self.save_path, global_step=self.step)\n\n                self.save_frozen_model(""save/frozen.pb"")\n\n                print(""[{}] Iteration loss: {} Error rate: {}"".format(\n                    self.step, iter_loss, acc))\n\n                self.step += 1\n        return None\n\n    def test(self):\n        with self.session.as_default():\n            print(""Testing"")\n            for batch_y, _, batch_x in self.data_manager.test_batches:\n                decoded = self.session.run(\n                    self.decoded,\n                    feed_dict={\n                        self.inputs: batch_x,\n                        self.seq_len: [self.max_char_count]\n                        * self.data_manager.batch_size,\n                    },\n                )\n\n                for i, y in enumerate(batch_y):\n                    print(batch_y[i])\n                    print(ground_truth_to_word(decoded[i], self.CHAR_VECTOR))\n        return None\n\n    def save_frozen_model(\n        self,\n        path=None,\n        optimize=False,\n        input_nodes=[""input"", ""seq_len""],\n        output_nodes=[""dense_decoded""],\n    ):\n        if not path or len(path) == 0:\n            raise ValueError(""Save path for frozen model is not specified"")\n\n        tf.train.write_graph(\n            self.session.graph_def,\n            ""/"".join(path.split(""/"")[0:-1]),\n            path.split(""/"")[-1] + "".pbtxt"",\n        )\n\n        # get graph definitions with weights\n        output_graph_def = tf.graph_util.convert_variables_to_constants(\n            self.session,  # The session is used to retrieve the weights\n            self.session.graph.as_graph_def(),  # The graph_def is used to retrieve the nodes\n            output_nodes,  # The output node names are used to select the usefull nodes\n        )\n\n        # optimize graph\n        if optimize:\n            output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n                output_graph_def, input_nodes, output_nodes, tf.float32.as_datatype_enum\n            )\n\n        with open(path, ""wb"") as f:\n            f.write(output_graph_def.SerializeToString())\n\n        return True\n'"
CRNN/data_manager.py,0,"b'import re\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom multiprocessing import Queue, Process\nfrom utils import sparse_tuple_from, resize_image, label_to_array\n\nfrom scipy.misc import imread\nfrom trdg.generators import GeneratorFromDict\n\n\nclass DataManager(object):\n    def __init__(\n        self,\n        batch_size,\n        model_path,\n        examples_path,\n        max_image_width,\n        train_test_ratio,\n        max_char_count,\n        char_vector,\n        use_trdg,\n        language,\n    ):\n        if train_test_ratio > 1.0 or train_test_ratio < 0:\n            raise Exception(""Incoherent ratio!"")\n\n        self.char_vector = char_vector\n\n        self.train_test_ratio = train_test_ratio\n        self.max_image_width = max_image_width\n        self.batch_size = batch_size\n        self.model_path = model_path\n        self.current_train_offset = 0\n        self.examples_path = examples_path\n        self.max_char_count = max_char_count\n        self.use_trdg = use_trdg\n        self.language = language\n\n        if self.use_trdg:\n            self.train_batches = self.multiprocess_batch_generator()\n            self.test_batches = self.multiprocess_batch_generator()\n        else:\n            self.data, self.data_len = self.load_data()\n            self.test_offset = int(train_test_ratio * self.data_len)\n            self.current_test_offset = self.test_offset\n            self.train_batches = self.generate_all_train_batches()\n            self.test_batches = self.generate_all_test_batches()\n\n    def batch_generator(self, queue):\n        """"""Takes a queue and enqueue batches in it\n        """"""\n\n        generator = GeneratorFromDict(language=self.language)\n        while True:\n            batch = []\n            while len(batch) < self.batch_size:\n                img, lbl = generator.next()\n                batch.append(\n                    (\n                        resize_image(np.array(img.convert(""L"")), self.max_image_width)[\n                            0\n                        ],\n                        lbl,\n                        label_to_array(lbl, self.char_vector),\n                    )\n                )\n\n            raw_batch_x, raw_batch_y, raw_batch_la = zip(*batch)\n\n            batch_y = np.reshape(np.array(raw_batch_y), (-1))\n\n            batch_dt = sparse_tuple_from(np.reshape(np.array(raw_batch_la), (-1)))\n\n            raw_batch_x = np.swapaxes(raw_batch_x, 1, 2)\n\n            raw_batch_x = raw_batch_x / 255.0\n\n            batch_x = np.reshape(\n                np.array(raw_batch_x), (len(raw_batch_x), self.max_image_width, 32, 1)\n            )\n            if queue.qsize() < 20:\n                queue.put((batch_y, batch_dt, batch_x))\n            else:\n                pass\n\n    def multiprocess_batch_generator(self):\n        """"""Returns a batch generator to use in training\n        """"""\n\n        q = Queue()\n        processes = []\n        for i in range(2):\n            processes.append(Process(target=self.batch_generator, args=(q,)))\n            processes[-1].start()\n        while True:\n            yield q.get()\n\n    def load_data(self):\n        """"""Load all the images in the folder\n        """"""\n\n        print(""Loading data"")\n\n        examples = []\n\n        count = 0\n        skipped = 0\n        for f in os.listdir(self.examples_path):\n            if len(f.split(""_"")[0]) > self.max_char_count:\n                continue\n            arr, initial_len = resize_image(\n                imread(os.path.join(self.examples_path, f), mode=""L""),\n                self.max_image_width,\n            )\n            examples.append(\n                (\n                    arr,\n                    f.split(""_"")[0],\n                    label_to_array(f.split(""_"")[0], self.char_vector),\n                )\n            )\n            count += 1\n\n        return examples, len(examples)\n\n    def generate_all_train_batches(self):\n        train_batches = []\n        while not self.current_train_offset + self.batch_size > self.test_offset:\n            old_offset = self.current_train_offset\n\n            new_offset = self.current_train_offset + self.batch_size\n\n            self.current_train_offset = new_offset\n\n            raw_batch_x, raw_batch_y, raw_batch_la = zip(\n                *self.data[old_offset:new_offset]\n            )\n\n            batch_y = np.reshape(np.array(raw_batch_y), (-1))\n\n            batch_dt = sparse_tuple_from(np.reshape(np.array(raw_batch_la), (-1)))\n\n            raw_batch_x = np.swapaxes(raw_batch_x, 1, 2)\n\n            raw_batch_x = raw_batch_x / 255.0\n\n            batch_x = np.reshape(\n                np.array(raw_batch_x), (len(raw_batch_x), self.max_image_width, 32, 1)\n            )\n\n            train_batches.append((batch_y, batch_dt, batch_x))\n        return train_batches\n\n    def generate_all_test_batches(self):\n        test_batches = []\n        while not self.current_test_offset + self.batch_size > self.data_len:\n            old_offset = self.current_test_offset\n\n            new_offset = self.current_test_offset + self.batch_size\n\n            self.current_test_offset = new_offset\n\n            raw_batch_x, raw_batch_y, raw_batch_la = zip(\n                *self.data[old_offset:new_offset]\n            )\n\n            batch_y = np.reshape(np.array(raw_batch_y), (-1))\n\n            batch_dt = sparse_tuple_from(np.reshape(np.array(raw_batch_la), (-1)))\n\n            raw_batch_x = np.swapaxes(raw_batch_x, 1, 2)\n\n            raw_batch_x = raw_batch_x / 255.0\n\n            batch_x = np.reshape(\n                np.array(raw_batch_x), (len(raw_batch_x), self.max_image_width, 32, 1)\n            )\n\n            test_batches.append((batch_y, batch_dt, batch_x))\n        return test_batches\n'"
CRNN/run.py,0,"b'import argparse\nfrom crnn import CRNN\n\nCHAR_VECTOR = ""0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ-\'.!?,\\""&""\n\n\ndef parse_arguments():\n    """"""\n        Parse the command line arguments of the program.\n    """"""\n\n    parser = argparse.ArgumentParser(description=""Train or test the CRNN model."")\n\n    parser.add_argument(\n        ""--train"", action=""store_true"", help=""Define if we train the model""\n    )\n    parser.add_argument(\n        ""--test"", action=""store_true"", help=""Define if we test the model""\n    )\n    parser.add_argument(\n        ""-ttr"",\n        ""--train_test_ratio"",\n        type=float,\n        nargs=""?"",\n        help=""How the data will be split between training and testing"",\n        default=0.70,\n    )\n    parser.add_argument(\n        ""-m"",\n        ""--model_path"",\n        type=str,\n        nargs=""?"",\n        help=""The path where the pretrained model can be found or where the model will be saved"",\n        default=""./save/"",\n    )\n    parser.add_argument(\n        ""-ex"",\n        ""--examples_path"",\n        type=str,\n        nargs=""?"",\n        help=""The path to the file containing the examples (training samples)"",\n    )\n    parser.add_argument(\n        ""-bs"", ""--batch_size"", type=int, nargs=""?"", help=""Size of a batch"", default=64\n    )\n    parser.add_argument(\n        ""-it"",\n        ""--iteration_count"",\n        type=int,\n        nargs=""?"",\n        help=""How many iteration in training"",\n        default=10,\n    )\n    parser.add_argument(\n        ""-miw"",\n        ""--max_image_width"",\n        type=int,\n        nargs=""?"",\n        help=""Maximum width of an example before truncating"",\n        default=100,\n    )\n    parser.add_argument(\n        ""-r"",\n        ""--restore"",\n        action=""store_true"",\n        help=""Define if we try to load a checkpoint file from the save folder"",\n    )\n    parser.add_argument(\n        ""-cs"",\n        ""--char_set_string"",\n        type=str,\n        nargs=""?"",\n        help=""The charset string"",\n        default=CHAR_VECTOR,\n    )\n    parser.add_argument(\n        ""--use_trdg"",\n        action=""store_true"",\n        help=""Generate training data on the fly with TextRecognitionDataGenerator"",\n    )\n    parser.add_argument(\n        ""-l"",\n        ""--language"",\n        type=str,\n        nargs=""?"",\n        help=""Language to use with TRDG (Must be used with --use_trdg"",\n        default=""en"",\n    )\n\n    return parser.parse_args()\n\n\ndef main():\n    """"""\n        Entry point when using CRNN from the commandline\n    """"""\n\n    args = parse_arguments()\n\n    if not args.train and not args.test:\n        print(""If we are not training, and not testing, what is the point?"")\n\n    crnn = None\n\n    if args.train:\n        crnn = CRNN(\n            args.batch_size,\n            args.model_path,\n            args.examples_path,\n            args.max_image_width,\n            args.train_test_ratio,\n            args.restore,\n            args.char_set_string,\n            args.use_trdg,\n            args.language,\n        )\n\n        crnn.train(args.iteration_count)\n\n    if args.test:\n        if crnn is None:\n            crnn = CRNN(\n                args.batch_size,\n                args.model_path,\n                args.examples_path,\n                args.max_image_width,\n                0,\n                args.restore,\n                args.char_set_string,\n                args.use_trdg,\n                args.language,\n            )\n\n        crnn.test()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
CRNN/utils.py,0,"b'import numpy as np\nimport tensorflow as tf\n\nfrom scipy.misc import imread, imresize, imsave\n\n\ndef sparse_tuple_from(sequences, dtype=np.int32):\n    """"""\n        Inspired (copied) from https://github.com/igormq/ctc_tensorflow_example/blob/master/utils.py\n    """"""\n\n    indices = []\n    values = []\n\n    for n, seq in enumerate(sequences):\n        indices.extend(zip([n] * len(seq), [i for i in range(len(seq))]))\n        values.extend(seq)\n\n    indices = np.asarray(indices, dtype=np.int64)\n    values = np.asarray(values, dtype=dtype)\n    shape = np.asarray(\n        [len(sequences), np.asarray(indices).max(0)[1] + 1], dtype=np.int64\n    )\n\n    return indices, values, shape\n\n\ndef resize_image(im_arr, input_width):\n    """"""Resize an image to the ""good"" input size\n    """"""\n\n    r, c = np.shape(im_arr)\n    if c > input_width:\n        c = input_width\n        ratio = float(input_width) / c\n        final_arr = imresize(im_arr, (int(32 * ratio), input_width))\n    else:\n        final_arr = np.zeros((32, input_width))\n        ratio = 32.0 / r\n        im_arr_resized = imresize(im_arr, (32, int(c * ratio)))\n        final_arr[\n            :, 0 : min(input_width, np.shape(im_arr_resized)[1])\n        ] = im_arr_resized[:, 0:input_width]\n    return final_arr, c\n\n\ndef label_to_array(label, char_vector):\n    try:\n        return [char_vector.index(x) for x in label]\n    except Exception as ex:\n        print(label)\n        raise ex\n\n\ndef ground_truth_to_word(ground_truth, char_vector):\n    """"""\n        Return the word string based on the input ground_truth\n    """"""\n\n    try:\n        return """".join([char_vector[i] for i in ground_truth if i != -1])\n    except Exception as ex:\n        print(ground_truth)\n        print(ex)\n        input()\n\n\ndef levenshtein(s1, s2):\n    if len(s1) < len(s2):\n        return levenshtein(s2, s1)\n\n    # len(s1) >= len(s2)\n    if len(s2) == 0:\n        return len(s1)\n\n    previous_row = range(len(s2) + 1)\n    for i, c1 in enumerate(s1):\n        current_row = [i + 1]\n        for j, c2 in enumerate(s2):\n            insertions = previous_row[j + 1] + 1\n            deletions = current_row[j] + 1\n            substitutions = previous_row[j] + (c1 != c2)\n            current_row.append(min(insertions, deletions, substitutions))\n        previous_row = current_row\n\n    return previous_row[-1]\n'"
