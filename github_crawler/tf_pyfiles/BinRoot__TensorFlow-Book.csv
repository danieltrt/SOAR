file_path,api_count,code
ch02_basics/gradient.py,7,"b'import tensorflow as tf\n\ndef my_loss_function(var, data):\n    return tf.abs(tf.subtract(var, data))\n\ndef my_other_loss_function(var, data):\n    return tf.square(tf.subtract(var, data))\n\ndata = tf.placeholder(tf.float32)\nvar = tf.Variable(1.)\nloss = my_loss_function(var, data)\nvar_grad = tf.gradients(loss, [var])[0]\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    var_grad_val = sess.run(var_grad, feed_dict={data: 4})\n    print(var_grad_val)\n'"
ch02_basics/interactive_session.py,3,"b'import tensorflow as tf\nsess = tf.InteractiveSession()\n\nmatrix = tf.constant([[1., 2.]])\nnegMatrix = tf.neg(matrix)\n\nresult = negMatrix.eval()\nprint(result)\nsess.close()\n'"
ch02_basics/loading_vars.py,3,"b'# # Loading Variables in TensorFlow\n\nimport tensorflow as tf\nsess = tf.InteractiveSession()\n\n\n# Create a boolean vector called `spike` to locate a sudden spike in data.\n# \n# Since all variables must be initialized, initialize the variable by calling `run()` on its `initializer`.\n\nspikes = tf.Variable([False]*8, name=\'spikes\')\nsaver = tf.train.Saver()\n\nsaver.restore(sess, ""spikes.ckpt"")\nprint(spikes.eval())\n\nsess.close()\n'"
ch02_basics/log_example.py,3,"b'import tensorflow as tf\n\nmatrix = tf.constant([[1., 2.]])\nnegMatrix = tf.neg(matrix)\n\nwith tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n    result = sess.run(negMatrix)\n\nprint(result)\n'"
ch02_basics/logging_example.py,3,"b'import tensorflow as tf\n\nmatrix = tf.constant([[1, 2]])\nneg_matrix = tf.neg(matrix)\n\nwith tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n    result = sess.run(neg_matrix)\n\nprint result\n'"
ch02_basics/main.py,3,"b'import tensorflow as tf\nimport numpy as np\n\nx = tf.constant([[1, 2]])\nneg_x = tf.neg(x)\n\nprint(neg_x)\n\nwith tf.Session() as sess:\n    result = sess.run(neg_x)\nprint(result)\n'"
ch02_basics/moving_avg.py,10,"b'## Using TensorBoard\n\n# mkdir logs\n\nimport tensorflow as tf\nimport numpy as np\n\nraw_data = np.random.normal(10, 1, 100)\n\nalpha = tf.constant(0.05)\ncurr_value = tf.placeholder(tf.float32)\nprev_avg = tf.Variable(0.)\nupdate_avg = alpha * curr_value + (1 - alpha) * prev_avg\n\navg_hist = tf.summary.scalar(""running_avg"", update_avg)\nvalue_hist = tf.summary.scalar(""incoming_values"", curr_value)\nmerged = tf.summary.merge_all()\nwriter = tf.summary.FileWriter(\'./logs\')\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    for i in range(len(raw_data)):\n        summary_str, curr_avg = sess.run([merged, update_avg], feed_dict={curr_value: raw_data[i]})\n        sess.run(tf.assign(prev_avg, curr_avg))\n        print(raw_data[i], curr_avg)\n        writer.add_summary(summary_str, i)\n\nwriter.close()\n'"
ch02_basics/saving_vars.py,4,"b'# # Saving Variables in TensorFlow\n\nimport tensorflow as tf\nsess = tf.InteractiveSession()\n\n\n# Create a boolean vector called `spike` to locate a sudden spike in data.\n# \n# Since all variables must be initialized, initialize the variable by calling `run()` on its `initializer`.\n\nraw_data = [1., 2., 8., -1., 0., 5.5, 6., 13]\nspikes = tf.Variable([False] * len(raw_data), name=\'spikes\')\nspikes.initializer.run()\n\n\n# The saver op will enable saving and restoring\n\nsaver = tf.train.Saver()\n\n\n# Loop through the data and update the spike variable when there is a significant increase \n\nfor i in range(1, len(raw_data)):\n    if raw_data[i] - raw_data[i-1] > 5:\n        spikes_val = spikes.eval()\n        spikes_val[i] = True\n        updater = tf.assign(spikes, spikes_val)\n        updater.eval()\n\n\nsave_path = saver.save(sess, ""spikes.ckpt"")\nprint(""spikes data saved in file: %s"" % save_path)\n\n\nsess.close()\n'"
ch02_basics/spikes.py,4,"b'# # Using Variables in TensorFlow\n\nimport tensorflow as tf\nsess = tf.InteractiveSession()\n\n\n# Create a boolean variable called `spike` to detect sudden a sudden increase in a series of numbers.\n# \n# Since all variables must be initialized, initialize the variable by calling `run()` on its `initializer`.\n\nraw_data = [1., 2., 8., -1., 0., 5.5, 6., 13]\nspike = tf.Variable(False)\nspike.initializer.run()\n\n\n# Loop through the data and update the spike variable when there is a significant increase \n\nfor i in range(1, len(raw_data)):\n    if raw_data[i] - raw_data[i-1] > 5:\n        updater = tf.assign(spike, tf.constant(True))\n        updater.eval()\n    else:\n        tf.assign(spike, False).eval()\n    print(""Spike"", spike.eval())\n\nsess.close()\n\n\n'"
ch02_basics/types.py,4,"b'# # Tensor Types\n\nimport tensorflow as tf\nimport numpy as np\n\n\n# Define a 2x2 matrix in 3 different ways\n\nm1 = [[1.0, 2.0], [3.0, 4.0]]\nm2 = np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32)\nm3 = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n\n\nprint(type(m1))\nprint(type(m2))\nprint(type(m3))\n\n\n# Create tensor objects out of various types\n\nt1 = tf.convert_to_tensor(m1, dtype=tf.float32)\nt2 = tf.convert_to_tensor(m2, dtype=tf.float32)\nt3 = tf.convert_to_tensor(m3, dtype=tf.float32)\n\n\nprint(type(t1))\nprint(type(t2))\nprint(type(t3))\n'"
ch03_regression/data_reader.py,0,"b""import csv\nimport time\n\n# \n# crime14_freq = data_reader.read('crimes_2014.csv', 1, '%d-%b-%y %H:%M:%S', 2014)\n# freq = read('311.csv', 0, '%m/%d/%Y', 2014)\n\ndef read(filename, date_idx, date_parse, year, bucket=7):\n\n    days_in_year = 365\n\n    # Create initial frequency map\n    freq = {}\n    for period in range(0, int(days_in_year/bucket)):\n        freq[period] = 0\n\n    # Read data and aggregate crimes per day\n    with open(filename, 'rb') as csvfile:\n        csvreader = csv.reader(csvfile)\n        csvreader.next()\n        for row in csvreader:\n            if row[date_idx] == '':\n                continue\n            t = time.strptime(row[date_idx], date_parse)\n            if t.tm_year == year and t.tm_yday < (days_in_year-1):\n                freq[int(t.tm_yday / bucket)] += 1\n\n    return freq\n\nif __name__ == '__main__':\n    freq = read('311.csv', 0, '%m/%d/%Y', 2014)\n    print freq"""
ch04_classification/linear_1d.py,11,"b'import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx_label0 = np.random.normal(5, 1, 10)\nx_label1 = np.random.normal(2, 1, 10)\nxs = np.append(x_label0, x_label1)\nlabels = [0.] * len(x_label0) + [1.] * len(x_label1)\n\nplt.scatter(xs, labels)\n\nlearning_rate = 0.001\ntraining_epochs = 1000\n\nX = tf.placeholder(""float"")\nY = tf.placeholder(""float"")\n\ndef model(X, w):\n    return tf.add(tf.mul(w[1], tf.pow(X, 1)),\n                  tf.mul(w[0], tf.pow(X, 0)))\n\nw = tf.Variable([0., 0.], name=""parameters"")\ny_model = model(X, w)\ncost = tf.reduce_sum(tf.square(Y-y_model))\n\ntrain_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\ncorrect_prediction = tf.equal(Y, tf.to_float(tf.greater(y_model, 0.5)))\naccuracy = tf.reduce_mean(tf.to_float(correct_prediction))\n\nsess = tf.Session()\ninit = tf.initialize_all_variables()\nsess.run(init)\n\nfor epoch in range(training_epochs):\n    sess.run(train_op, feed_dict={X: xs, Y: labels})\n    current_cost = sess.run(cost, feed_dict={X: xs, Y: labels})\n    print(epoch, current_cost)\n\nw_val = sess.run(w)\nprint(\'learned parameters\', w_val)\n\nprint(\'accuracy\', sess.run(accuracy, feed_dict={X: xs, Y: labels}))\n\nsess.close()\n\nall_xs = np.linspace(0, 10, 100)\nplt.plot(all_xs, all_xs*w_val[1] + w_val[0])\nplt.show()'"
ch04_classification/logistic_1d.py,8,"b'import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nlearning_rate = 0.01\ntraining_epochs = 1000\n\ndef sigmoid(x):\n    return 1. / (1. + np.exp(-x))\n\nx1 = np.random.normal(-4, 2, 1000)\nx2 = np.random.normal(4, 2, 1000)\nxs = np.append(x1, x2)\nys = np.asarray([0.] * len(x1) + [1.] * len(x2))\n\nplt.scatter(xs, ys)\n\nX = tf.placeholder(tf.float32, shape=(None,), name=""x"")\nY = tf.placeholder(tf.float32, shape=(None,), name=""y"")\nw = tf.Variable([0., 0.], name=""parameter"", trainable=True)\ny_model = tf.sigmoid(-(w[1] * X + w[0]))\ncost = tf.reduce_mean(-tf.log(y_model * Y + (1 - y_model) * (1 - Y)))\n\ntrain_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    prev_err = 0\n    for epoch in range(training_epochs):\n        err, _ = sess.run([cost, train_op], {X: xs, Y: ys})\n        print(epoch, err)\n        if abs(prev_err - err) < 0.0001:\n            break\n        prev_err = err\n    w_val = sess.run(w, {X: xs, Y: ys})\n\nall_xs = np.linspace(-10, 10, 100)\nplt.plot(all_xs, sigmoid(all_xs * w_val[1] + w_val[0]))\nplt.show()\n'"
ch04_classification/logistic_2d.py,9,"b'import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nlearning_rate = 0.1\ntraining_epochs = 2000\n\n\ndef sigmoid(x):\n    return 1. / (1. + np.exp(-x))\n\nx1_label1 = np.random.normal(3, 1, 1000)\nx2_label1 = np.random.normal(2, 1, 1000)\nx1_label2 = np.random.normal(7, 1, 1000)\nx2_label2 = np.random.normal(6, 1, 1000)\nx1s = np.append(x1_label1, x1_label2)\nx2s = np.append(x2_label1, x2_label2)\nys = np.asarray([0.] * len(x1_label1) + [1.] * len(x1_label2))\n\nX1 = tf.placeholder(tf.float32, shape=(None,), name=""x1"")\nX2 = tf.placeholder(tf.float32, shape=(None,), name=""x2"")\nY = tf.placeholder(tf.float32, shape=(None,), name=""y"")\nw = tf.Variable([0., 0., 0.], name=""w"", trainable=True)\n\ny_model = tf.sigmoid(-(w[2] * X2 + w[1] * X1 + w[0]))\ncost = tf.reduce_mean(-tf.log(y_model) * Y -tf.log(1 - y_model) * (1 - Y))\ntrain_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    prev_err = 0\n    for epoch in range(training_epochs):\n        err, _ = sess.run([cost, train_op], {X1: x1s, X2: x2s, Y: ys})\n        print(epoch, err)\n        if abs(prev_err - err) < 0.0001:\n            break\n        prev_err = err\n\n    w_val = sess.run(w)\n\nx1_boundary, x2_boundary = [], []\nfor x1_test in np.linspace(0, 10, 100):\n    for x2_test in np.linspace(0, 10, 100):\n        z = sigmoid(-x2_test*w_val[2] - x1_test*w_val[1] - w_val[0])\n        if abs(z - 0.5) < 0.01:\n            x1_boundary.append(x1_test)\n            x2_boundary.append(x2_test)\n\nplt.scatter(x1_boundary, x2_boundary, c=\'b\', marker=\'o\', s=20)\nplt.scatter(x1_label1, x2_label1, c=\'r\', marker=\'x\', s=20)\nplt.scatter(x1_label2, x2_label2, c=\'g\', marker=\'1\', s=20)\n\nplt.show()\n'"
ch04_classification/softmax.py,11,"b'import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nlearning_rate = 0.01\ntraining_epochs = 1000\nnum_labels = 3\nbatch_size = 100\n\nx1_label0 = np.random.normal(1, 1, (100, 1))\nx2_label0 = np.random.normal(1, 1, (100, 1))\nx1_label1 = np.random.normal(5, 1, (100, 1))\nx2_label1 = np.random.normal(4, 1, (100, 1))\nx1_label2 = np.random.normal(8, 1, (100, 1))\nx2_label2 = np.random.normal(0, 1, (100, 1))\n\nplt.scatter(x1_label0, x2_label0, c=\'r\', marker=\'o\', s=60)\nplt.scatter(x1_label1, x2_label1, c=\'g\', marker=\'x\', s=60)\nplt.scatter(x1_label2, x2_label2, c=\'b\', marker=\'_\', s=60)\nplt.show()\n\nxs_label0 = np.hstack((x1_label0, x2_label0))\nxs_label1 = np.hstack((x1_label1, x2_label1))\nxs_label2 = np.hstack((x1_label2, x2_label2))\n\nxs = np.vstack((xs_label0, xs_label1, xs_label2))\nlabels = np.matrix([[1., 0., 0.]] * len(x1_label0) + [[0., 1., 0.]] * len(x1_label1) + [[0., 0., 1.]] * len(x1_label2))\n\narr = np.arange(xs.shape[0])\nnp.random.shuffle(arr)\nxs = xs[arr, :]\nlabels = labels[arr, :]\n\ntest_x1_label0 = np.random.normal(1, 1, (10, 1))\ntest_x2_label0 = np.random.normal(1, 1, (10, 1))\ntest_x1_label1 = np.random.normal(5, 1, (10, 1))\ntest_x2_label1 = np.random.normal(4, 1, (10, 1))\ntest_x1_label2 = np.random.normal(8, 1, (10, 1))\ntest_x2_label2 = np.random.normal(0, 1, (10, 1))\ntest_xs_label0 = np.hstack((test_x1_label0, test_x2_label0))\ntest_xs_label1 = np.hstack((test_x1_label1, test_x2_label1))\ntest_xs_label2 = np.hstack((test_x1_label2, test_x2_label2))\n\ntest_xs = np.vstack((test_xs_label0, test_xs_label1, test_xs_label2))\ntest_labels = np.matrix([[1., 0., 0.]] * 10 + [[0., 1., 0.]] * 10 + [[0., 0., 1.]] * 10)\n\ntrain_size, num_features = xs.shape\n\nX = tf.placeholder(""float"", shape=[None, num_features])\nY = tf.placeholder(""float"", shape=[None, num_labels])\n\nW = tf.Variable(tf.zeros([num_features, num_labels]))\nb = tf.Variable(tf.zeros([num_labels]))\ny_model = tf.nn.softmax(tf.matmul(X, W) + b)\n\ncost = -tf.reduce_sum(Y * tf.log(y_model))\ntrain_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n\ncorrect_prediction = tf.equal(tf.argmax(y_model, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))\n\nwith tf.Session() as sess:\n    tf.initialize_all_variables().run()\n\n    for step in xrange(training_epochs * train_size // batch_size):\n        offset = (step * batch_size) % train_size\n        batch_xs = xs[offset:(offset + batch_size), :]\n        batch_labels = labels[offset:(offset + batch_size)]\n        err, _ = sess.run([cost, train_op], feed_dict={X: batch_xs, Y: batch_labels})\n        print (step, err)\n\n    W_val = sess.run(W)\n    print(\'w\', W_val)\n    b_val = sess.run(b)\n    print(\'b\', b_val)\n    print ""accuracy"", accuracy.eval(feed_dict={X: test_xs, Y: test_labels})\n'"
ch05_clustering/audio_clustering.py,16,"b""import tensorflow as tf\nimport numpy as np\nfrom bregman.suite import *\n\nk = 2\nmax_iterations = 100\n\nfilenames = tf.train.match_filenames_once('./audio_dataset/*.wav')\ncount_num_files = tf.size(filenames)\nfilename_queue = tf.train.string_input_producer(filenames)\nreader = tf.WholeFileReader()\nfilename, file_contents = reader.read(filename_queue)\n\nchromo = tf.placeholder(tf.float32)\nmax_freqs = tf.argmax(chromo, 0)\n\n\ndef get_next_chromogram(sess):\n    audio_file = sess.run(filename)\n    F = Chromagram(audio_file, nfft=16384, wfft=8192, nhop=2205)\n    return F.X, audio_file\n\n\ndef extract_feature_vector(sess, chromo_data):\n    num_features, num_samples = np.shape(chromo_data)\n    freq_vals = sess.run(max_freqs, feed_dict={chromo: chromo_data})\n    hist, bins = np.histogram(freq_vals, bins=range(num_features + 1))\n    normalized_hist = hist.astype(float) / num_samples\n    return normalized_hist\n\n\ndef get_dataset(sess):\n    num_files = sess.run(count_num_files)\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(coord=coord)\n    xs = list()\n    names = list()\n    plt.figure()\n    for _ in range(num_files):\n        chromo_data, filename = get_next_chromogram(sess)\n\n        plt.subplot(1, 2, 1)\n        plt.imshow(chromo_data, cmap='Greys', interpolation='nearest')\n        plt.title('Visualization of Sound Spectrum')\n\n        plt.subplot(1, 2, 2)\n        freq_vals = sess.run(max_freqs, feed_dict={chromo: chromo_data})\n        plt.hist(freq_vals)\n        plt.title('Histogram of Notes')\n        plt.xlabel('Musical Note')\n        plt.ylabel('Count')\n        plt.savefig('{}.png'.format(filename))\n        plt.clf()\n\n        plt.clf()\n        names.append(filename)\n        x = extract_feature_vector(sess, chromo_data)\n        xs.append(x)\n    xs = np.asmatrix(xs)\n    return xs, names\n\n\ndef initial_cluster_centroids(X, k):\n    return X[0:k, :]\n\n\ndef assign_cluster(X, centroids):\n    expanded_vectors = tf.expand_dims(X, 0)\n    expanded_centroids = tf.expand_dims(centroids, 1)\n    distances = tf.reduce_sum(tf.square(tf.sub(expanded_vectors, expanded_centroids)), 2)\n    mins = tf.argmin(distances, 0)\n    return mins\n\n\ndef recompute_centroids(X, Y):\n    sums = tf.unsorted_segment_sum(X, Y, k)\n    counts = tf.unsorted_segment_sum(tf.ones_like(X), Y, k)\n    return sums / counts\n\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    X, names = get_dataset(sess)\n    centroids = initial_cluster_centroids(X, k)\n    i, converged = 0, False\n    while not converged and i < max_iterations:\n        i += 1\n        Y = assign_cluster(X, centroids)\n        centroids = sess.run(recompute_centroids(X, Y))\n    print(zip(sess.run(Y), names))\n"""
ch05_clustering/audio_segmentation.py,9,"b""import tensorflow as tf\nimport numpy as np\nfrom bregman.suite import *\n\nk = 4\nsegment_size = 50  # out of 24,526\nmax_iterations = 100\n\n\nchromo = tf.placeholder(tf.float32)\nmax_freqs = tf.argmax(chromo, 0)\n\ndef get_chromogram(audio_file):\n    F = Chromagram(audio_file, nfft=16384, wfft=8192, nhop=2205)\n    return F.X\n\ndef get_dataset(sess, audio_file):\n    chromo_data = get_chromogram(audio_file)\n    print('chromo_data', np.shape(chromo_data))\n    chromo_length = np.shape(chromo_data)[1]\n    xs = []\n    for i in range(chromo_length/segment_size):\n        chromo_segment = chromo_data[:, i*segment_size:(i+1)*segment_size]\n        x = extract_feature_vector(sess, chromo_segment)\n        if len(xs) == 0:\n            xs = x\n        else:\n            xs = np.vstack((xs, x))\n    return xs\n\n\ndef initial_cluster_centroids(X, k):\n    return X[0:k, :]\n\n\n# op\ndef assign_cluster(X, centroids):\n    expanded_vectors = tf.expand_dims(X, 0)\n    expanded_centroids = tf.expand_dims(centroids, 1)\n    distances = tf.reduce_sum(tf.square(tf.sub(expanded_vectors, expanded_centroids)), 2)\n    mins = tf.argmin(distances, 0)\n    return mins\n\n\n# op\ndef recompute_centroids(X, Y):\n    sums = tf.unsorted_segment_sum(X, Y, k)\n    counts = tf.unsorted_segment_sum(tf.ones_like(X), Y, k)\n    return sums / counts\n\n\ndef extract_feature_vector(sess, chromo_data):\n    num_features, num_samples = np.shape(chromo_data)\n    freq_vals = sess.run(max_freqs, feed_dict={chromo: chromo_data})\n    hist, bins = np.histogram(freq_vals, bins=range(num_features + 1))\n    return hist.astype(float) / num_samples\n\n\nwith tf.Session() as sess:\n    X = get_dataset(sess, 'sysk.wav')\n    print(np.shape(X))\n    centroids = initial_cluster_centroids(X, k)\n    i, converged = 0, False\n    # prev_Y = None\n    while not converged and i < max_iterations:\n        i += 1\n        Y = assign_cluster(X, centroids)\n        # if prev_Y == Y:\n        #     converged = True\n        #     break\n        # prev_Y = Y\n        centroids = sess.run(recompute_centroids(X, Y))\n        if i % 50 == 0:\n            print('iteration', i)\n    segments = sess.run(Y)\n    for i in range(len(segments)):\n        seconds = (i * segment_size) / float(10)\n        min, sec = divmod(seconds, 60)\n        time_str = str(min) + 'm ' + str(sec) + 's'\n        print(time_str, segments[i])\n\n"""
ch05_clustering/som.py,21,"b'import tensorflow as tf\nimport numpy as np\n\nclass SOM:\n    def __init__(self, width, height, dim):\n        self.num_iters = 100\n        self.width = width\n        self.height = height\n        self.dim = dim\n        self.node_locs = self.get_locs()\n\n        # Each node is a vector of dimension `dim`\n        # For a 2D grid, there are `width * height` nodes\n        nodes = tf.Variable(tf.random_normal([width*height, dim]))\n        self.nodes = nodes\n\n        # These two ops are inputs at each iteration\n        x = tf.placeholder(tf.float32, [dim])\n        iter = tf.placeholder(tf.float32)\n\n        self.x = x\n        self.iter = iter\n\n        # Find the node that matches closest to the input\n        bmu_loc = self.get_bmu_loc(x)\n\n        self.propagate_nodes = self.get_propagation(bmu_loc, x, iter)\n\n    def get_propagation(self, bmu_loc, x, iter):\n        num_nodes = self.width * self.height\n        rate = 1.0 - tf.div(iter, self.num_iters)\n        alpha = rate * 0.5\n        sigma = rate * tf.to_float(tf.maximum(self.width, self.height)) / 2.\n        expanded_bmu_loc = tf.expand_dims(tf.to_float(bmu_loc), 0)\n        sqr_dists_from_bmu = tf.reduce_sum(tf.square(tf.sub(expanded_bmu_loc, self.node_locs)), 1)\n        neigh_factor = tf.exp(-tf.div(sqr_dists_from_bmu, 2 * tf.square(sigma)))\n        rate = tf.mul(alpha, neigh_factor)\n        rate_factor = tf.pack([tf.tile(tf.slice(rate, [i], [1]), [self.dim]) for i in range(num_nodes)])\n        nodes_diff = tf.mul(rate_factor, tf.sub(tf.pack([x for i in range(num_nodes)]), self.nodes))\n        update_nodes = tf.add(self.nodes, nodes_diff)\n        return tf.assign(self.nodes, update_nodes)\n\n    def get_bmu_loc(self, x):\n        expanded_x = tf.expand_dims(x, 0)\n        sqr_diff = tf.square(tf.sub(expanded_x, self.nodes))\n        dists = tf.reduce_sum(sqr_diff, 1)\n        bmu_idx = tf.argmin(dists, 0)\n        bmu_loc = tf.pack([tf.mod(bmu_idx, self.width), tf.div(bmu_idx, self.width)])\n        return bmu_loc\n\n    def get_locs(self):\n        locs = [[x, y]\n                for y in range(self.height)\n                for x in range(self.width)]\n        return tf.to_float(locs)\n\n    def train(self, data):\n        with tf.Session() as sess:\n            sess.run(tf.initialize_all_variables())\n            for i in range(self.num_iters):\n                for data_x in data:\n                    sess.run(self.propagate_nodes, feed_dict={self.x: data_x, self.iter: i})\n            centroid_grid = [[] for i in range(self.width)]\n            self.nodes_val = list(sess.run(self.nodes))\n            self.locs_val = list(sess.run(self.node_locs))\n            for i, l in enumerate(self.locs_val):\n                centroid_grid[int(l[0])].append(self.nodes_val[i])\n            self.centroid_grid = centroid_grid'"
ch05_clustering/som_test.py,0,"b'#For plotting the images\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom som import SOM\n\ncolors = np.array(\n     [[0., 0., 1.],\n      [0., 0., 0.95],\n      [0., 0.05, 1.],\n      [0., 1., 0.],\n      [0., 0.95, 0.],\n      [0., 1, 0.05],\n      [1., 0., 0.],\n      [1., 0.05, 0.],\n      [1., 0., 0.05],\n      [1., 1., 0.]])\n\nsom = SOM(4, 4, 3)\nsom.train(colors)\n\nplt.imshow(som.centroid_grid)\nplt.show()\n'"
ch06_hmm/__init__.py,0,b''
ch06_hmm/forward.py,11,"b""import numpy as np\nimport tensorflow as tf\n\n\nclass HMM(object):\n    def __init__(self, initial_prob, trans_prob, obs_prob):\n        self.N = np.size(initial_prob)\n        self.initial_prob = initial_prob\n        self.trans_prob = trans_prob\n        self.emission = tf.constant(obs_prob)\n\n        assert self.initial_prob.shape == (self.N, 1)\n        assert self.trans_prob.shape == (self.N, self.N)\n        assert obs_prob.shape[0] == self.N\n\n        self.obs_idx = tf.placeholder(tf.int32)\n        self.fwd = tf.placeholder(tf.float64)\n\n    def get_emission(self, obs_idx):\n        slice_location = [0, obs_idx]\n        num_rows = tf.shape(self.emission)[0]\n        slice_shape = [num_rows, 1]\n        return tf.slice(self.emission, slice_location, slice_shape)\n\n    def forward_init_op(self):\n        obs_prob = self.get_emission(self.obs_idx)\n        fwd = tf.mul(self.initial_prob, obs_prob)\n        return fwd\n\n    def forward_op(self):\n        transitions = tf.matmul(self.fwd, tf.transpose(self.get_emission(self.obs_idx)))\n        weighted_transitions = transitions * self.trans_prob\n        fwd = tf.reduce_sum(weighted_transitions, 0)\n        return tf.reshape(fwd, tf.shape(self.fwd))\n\n\ndef forward_algorithm(sess, hmm, observations):\n    fwd = sess.run(hmm.forward_init_op(), feed_dict={hmm.obs_idx: observations[0]})\n    for t in range(1, len(observations)):\n        fwd = sess.run(hmm.forward_op(), feed_dict={hmm.obs_idx: observations[t], hmm.fwd: fwd})\n    prob = sess.run(tf.reduce_sum(fwd))\n    return prob\n\nif __name__ == '__main__':\n    initial_prob = np.array([[0.6], [0.4]])\n    trans_prob = np.array([[0.7, 0.3], [0.4, 0.6]])\n    obs_prob = np.array([[0.5, 0.4, 0.1], [0.1, 0.3, 0.6]])\n\n    hmm = HMM(initial_prob=initial_prob, trans_prob=trans_prob, obs_prob=obs_prob)\n\n    observations = [0, 1, 1, 2, 1]\n    with tf.Session() as sess:\n        prob = forward_algorithm(sess, hmm, observations)\n        print('Probability of observing {} is {}'.format(observations, prob))\n"""
ch06_hmm/hmm.py,17,"b""import numpy as np\nimport tensorflow as tf\n\n\n# initial parameters can be learned on training data\n# theory reference https://web.stanford.edu/~jurafsky/slp3/8.pdf\n# code reference https://phvu.net/2013/12/06/sweet-implementation-of-viterbi-in-python/\nclass HMM(object):\n    def __init__(self, initial_prob, trans_prob, obs_prob):\n        self.N = np.size(initial_prob)\n        self.initial_prob = initial_prob\n        self.trans_prob = trans_prob\n        self.obs_prob = obs_prob\n        self.emission = tf.constant(obs_prob)\n        assert self.initial_prob.shape == (self.N, 1)\n        assert self.trans_prob.shape == (self.N, self.N)\n        assert self.obs_prob.shape[0] == self.N\n        self.obs = tf.placeholder(tf.int32)\n        self.fwd = tf.placeholder(tf.float64)\n        self.viterbi = tf.placeholder(tf.float64)\n\n    def get_emission(self, obs_idx):\n        slice_location = [0, obs_idx]\n        num_rows = tf.shape(self.emission)[0]\n        slice_shape = [num_rows, 1]\n        return tf.slice(self.emission, slice_location, slice_shape)\n\n    def forward_init_op(self):\n        obs_prob = self.get_emission(self.obs)\n        fwd = tf.mul(self.initial_prob, obs_prob)\n        return fwd\n\n    def forward_op(self):\n        transitions = tf.matmul(self.fwd, tf.transpose(self.get_emission(self.obs)))\n        weighted_transitions = transitions * self.trans_prob\n        fwd = tf.reduce_sum(weighted_transitions, 0)\n        return tf.reshape(fwd, tf.shape(self.fwd))\n\n    def decode_op(self):\n        transitions = tf.matmul(self.viterbi, tf.transpose(self.get_emission(self.obs)))\n        weighted_transitions = transitions * self.trans_prob\n        viterbi = tf.reduce_max(weighted_transitions, 0)\n        return tf.reshape(viterbi, tf.shape(self.viterbi))\n\n    def backpt_op(self):\n        back_transitions = tf.matmul(self.viterbi, np.ones((1, self.N)))\n        weighted_back_transitions = back_transitions * self.trans_prob\n        return tf.argmax(weighted_back_transitions, 0)\n\n\ndef forward_algorithm(sess, hmm, observations):\n    fwd = sess.run(hmm.forward_init_op(), feed_dict={hmm.obs: observations[0]})\n    for t in range(1, len(observations)):\n        fwd = sess.run(hmm.forward_op(), feed_dict={hmm.obs: observations[t], hmm.fwd: fwd})\n    prob = sess.run(tf.reduce_sum(fwd))\n    return prob\n\ndef viterbi_decode(sess, hmm, observations):\n    viterbi = sess.run(hmm.forward_init_op(), feed_dict={hmm.obs: observations[0]})\n    backpts = np.ones((hmm.N, len(observations)), 'int32') * -1\n    for t in range(1, len(observations)):\n        viterbi, backpt = sess.run([hmm.decode_op(), hmm.backpt_op()],\n                                    feed_dict={hmm.obs: observations[t],\n                                               hmm.viterbi: viterbi})\n        backpts[:, t] = backpt\n    tokens = [viterbi[:, -1].argmax()]\n    for i in range(len(observations) - 1, 0, -1):\n        tokens.append(backpts[tokens[-1], i])\n    return tokens[::-1]\n\nif __name__ == '__main__':\n    states = ('Healthy', 'Fever')\n    observations = ('normal', 'cold', 'dizzy')\n    start_probability = {'Healthy': 0.6, 'Fever': 0.4}\n    transition_probability = {\n        'Healthy': {'Healthy': 0.7, 'Fever': 0.3},\n        'Fever': {'Healthy': 0.4, 'Fever': 0.6}\n    }\n    emission_probability = {\n        'Healthy': {'normal': 0.5, 'cold': 0.4, 'dizzy': 0.1},\n        'Fever': {'normal': 0.1, 'cold': 0.3, 'dizzy': 0.6}\n    }\n    initial_prob = np.array([[0.6], [0.4]])\n    trans_prob = np.array([[0.7, 0.3], [0.4, 0.6]])\n    obs_prob = np.array([[0.5, 0.4, 0.1], [0.1, 0.3, 0.6]])\n    hmm = HMM(initial_prob=initial_prob, trans_prob=trans_prob, obs_prob=obs_prob)\n\n    observations = [0, 1, 1, 2, 1]\n    with tf.Session() as sess:\n        prob = forward_algorithm(sess, hmm, observations)\n        print('Probability of observing {} is {}'.format(observations, prob))\n\n        seq = viterbi_decode(sess, hmm, observations)\n        print('Most likely hidden states are {}'.format(seq))\n\n"""
ch07_autoencoder/autoencoder.py,20,"b""import tensorflow as tf\nimport numpy as np\n\ndef get_batch(X, size):\n    a = np.random.choice(len(X), size, replace=False)\n    return X[a]\n\nclass Autoencoder:\n    def __init__(self, input_dim, hidden_dim, epoch=1000, batch_size=50, learning_rate=0.001):\n        self.epoch = epoch\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n\n        x = tf.placeholder(dtype=tf.float32, shape=[None, input_dim])\n        with tf.name_scope('encode'):\n            weights = tf.Variable(tf.random_normal([input_dim, hidden_dim], dtype=tf.float32), name='weights')\n            biases = tf.Variable(tf.zeros([hidden_dim]), name='biases')\n            encoded = tf.nn.sigmoid(tf.matmul(x, weights) + biases)\n        with tf.name_scope('decode'):\n            weights = tf.Variable(tf.random_normal([hidden_dim, input_dim], dtype=tf.float32), name='weights')\n            biases = tf.Variable(tf.zeros([input_dim]), name='biases')\n            decoded = tf.matmul(encoded, weights) + biases\n\n        self.x = x\n        self.encoded = encoded\n        self.decoded = decoded\n\n        self.loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(self.x, self.decoded))))\n        self.train_op = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)\n        \n        self.saver = tf.train.Saver()\n\n    def train(self, data):\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            for i in range(self.epoch):\n                for j in range(np.shape(data)[0] // self.batch_size):\n                    batch_data = get_batch(data, self.batch_size)\n                    l, _ = sess.run([self.loss, self.train_op], feed_dict={self.x: batch_data})\n                if i % 10 == 0:\n                    print('epoch {0}: loss = {1}'.format(i, l))\n                    self.saver.save(sess, './model.ckpt')\n            self.saver.save(sess, './model.ckpt')\n        \n    def test(self, data):\n        with tf.Session() as sess:\n            self.saver.restore(sess, './model.ckpt')\n            hidden, reconstructed = sess.run([self.encoded, self.decoded], feed_dict={self.x: data})\n        print('input', data)\n        print('compressed', hidden)\n        print('reconstructed', reconstructed)\n        return reconstructed\n\n    def get_params(self):\n        with tf.Session() as sess:\n            self.saver.restore(sess, './model.ckpt')\n            weights, biases = sess.run([self.weights1, self.biases1])\n        return weights, biases\n\n    def classify(self, data, labels):\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            self.saver.restore(sess, './model.ckpt')\n            hidden, reconstructed = sess.run([self.encoded, self.decoded], feed_dict={self.x: data})\n            reconstructed = reconstructed[0]\n            # loss = sess.run(self.all_loss, feed_dict={self.x: data})\n            print('data', np.shape(data))\n            print('reconstructed', np.shape(reconstructed))\n            loss = np.sqrt(np.mean(np.square(data - reconstructed), axis=1))\n            print('loss', np.shape(loss))\n            horse_indices = np.where(labels == 7)[0]\n            not_horse_indices = np.where(labels != 7)[0]\n            horse_loss = np.mean(loss[horse_indices])\n            not_horse_loss = np.mean(loss[not_horse_indices])\n            print('horse', horse_loss)\n            print('not horse', not_horse_loss)\n            return hidden\n\n    def decode(self, encoding):\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            self.saver.restore(sess, './model.ckpt')\n            reconstructed = sess.run(self.decoded, feed_dict={self.encoded: encoding})\n        img = np.reshape(reconstructed, (32, 32))\n        return img\n"""
ch07_autoencoder/autoencoder_batch.py,17,"b""import tensorflow as tf\nimport numpy as np\n\ndef get_batch(X, size):\n    a = np.random.choice(len(X), size, replace=False)\n    return X[a]\n\nclass Autoencoder:\n    def __init__(self, input_dim, hidden_dim, epoch=1000, batch_size=10, learning_rate=0.001):\n        self.epoch = epoch\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n\n        x = tf.placeholder(dtype=tf.float32, shape=[None, input_dim])\n        with tf.name_scope('encode'):\n            weights = tf.Variable(tf.random_normal([input_dim, hidden_dim], dtype=tf.float32), name='weights')\n            biases = tf.Variable(tf.zeros([hidden_dim]), name='biases')\n            encoded = tf.nn.sigmoid(tf.matmul(x, weights) + biases)\n        with tf.name_scope('decode'):\n            weights = tf.Variable(tf.random_normal([hidden_dim, input_dim], dtype=tf.float32), name='weights')\n            biases = tf.Variable(tf.zeros([input_dim]), name='biases')\n            decoded = tf.matmul(encoded, weights) + biases\n\n        self.x = x\n        self.encoded = encoded\n        self.decoded = decoded\n\n        self.loss = tf.sqrt(tf.reduce_mean(tf.square(tf.sub(self.x, self.decoded))))\n\n        self.all_loss = tf.sqrt(tf.reduce_mean(tf.square(tf.sub(self.x, self.decoded)), 1))\n        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n        self.saver = tf.train.Saver()\n\n    def train(self, data):\n        with tf.Session() as sess:\n            sess.run(tf.initialize_all_variables())\n            for i in range(self.epoch):\n                for j in range(500):\n                    batch_data = get_batch(data, self.batch_size)\n                    l, _ = sess.run([self.loss, self.train_op], feed_dict={self.x: batch_data})\n                if i % 10 == 0:\n                    print('epoch {0}: loss = {1}'.format(i, l))\n                    self.saver.save(sess, './model.ckpt')\n            self.saver.save(sess, './model.ckpt')\n        \n    def test(self, data):\n        with tf.Session() as sess:\n            self.saver.restore(sess, './model.ckpt')\n            hidden, reconstructed = sess.run([self.encoded, self.decoded], feed_dict={self.x: data})\n        print('input', data)\n        print('compressed', hidden)\n        print('reconstructed', reconstructed)\n        return reconstructed\n\n    def get_params(self):\n        with tf.Session() as sess:\n            self.saver.restore(sess, './model.ckpt')\n            weights, biases = sess.run([self.weights1, self.biases1])\n        return weights, biases\n\n"""
ch07_autoencoder/denoiser.py,17,"b""import tensorflow as tf\nimport numpy as np\nimport time\n\ndef get_batch(X, Xn, size):\n    a = np.random.choice(len(X), size, replace=False)\n    return X[a], Xn[a]\n\nclass Denoiser:\n\n    def __init__(self, input_dim, hidden_dim, epoch=10000, batch_size=50, learning_rate=0.001):\n        self.epoch = epoch\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n\n        self.x = tf.placeholder(dtype=tf.float32, shape=[None, input_dim], name='x')\n        self.x_noised = tf.placeholder(dtype=tf.float32, shape=[None, input_dim], name='x_noised')\n        with tf.name_scope('encode'):\n            self.weights1 = tf.Variable(tf.random_normal([input_dim, hidden_dim], dtype=tf.float32), name='weights')\n            self.biases1 = tf.Variable(tf.zeros([hidden_dim]), name='biases')\n            self.encoded = tf.nn.sigmoid(tf.matmul(self.x_noised, self.weights1) + self.biases1, name='encoded')\n        with tf.name_scope('decode'):\n            weights = tf.Variable(tf.random_normal([hidden_dim, input_dim], dtype=tf.float32), name='weights')\n            biases = tf.Variable(tf.zeros([input_dim]), name='biases')\n            self.decoded = tf.matmul(self.encoded, weights) + biases\n        self.loss = tf.sqrt(tf.reduce_mean(tf.square(tf.sub(self.x, self.decoded))))\n        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n        self.saver = tf.train.Saver()\n\n    def add_noise(self, data):\n        noise_type = 'mask-0.2'\n        if noise_type == 'gaussian':\n            n = np.random.normal(0, 0.1, np.shape(data))\n            return data + n\n        if 'mask' in noise_type:\n            frac = float(noise_type.split('-')[1])\n            temp = np.copy(data)\n            for i in temp:\n                n = np.random.choice(len(i), round(frac * len(i)), replace=False)\n                i[n] = 0\n            return temp\n\n    def train(self, data):\n        data_noised = self.add_noise(data)\n        with open('log.csv', 'w') as writer:\n            with tf.Session() as sess:\n                sess.run(tf.initialize_all_variables())\n                for i in range(self.epoch):\n                    for j in range(50):\n                        batch_data, batch_data_noised = get_batch(data, data_noised, self.batch_size)\n                        l, _ = sess.run([self.loss, self.train_op], feed_dict={self.x: batch_data, self.x_noised: batch_data_noised})\n                    if i % 10 == 0:\n                        print('epoch {0}: loss = {1}'.format(i, l))\n                        self.saver.save(sess, './model.ckpt')\n                        epoch_time = int(time.time())\n                        row_str = str(epoch_time) + ',' + str(i) + ',' + str(l) + '\\n'\n                        writer.write(row_str)\n                        writer.flush()\n                self.saver.save(sess, './model.ckpt')\n\n    def test(self, data):\n        with tf.Session() as sess:\n            self.saver.restore(sess, './model.ckpt')\n            hidden, reconstructed = sess.run([self.encoded, self.decoded], feed_dict={self.x: data})\n        print('input', data)\n        print('compressed', hidden)\n        print('reconstructed', reconstructed)\n        return reconstructed\n\n    def get_params(self):\n        with tf.Session() as sess:\n            self.saver.restore(sess, './model.ckpt')\n            weights, biases = sess.run([self.weights1, self.biases1])\n        return weights, biases\n"""
ch07_autoencoder/denoising_autoencoder.py,16,"b""import tensorflow as tf\nimport numpy as np\n\ndef get_batch(X, size):\n    a = np.random.choice(len(X), size, replace=False)\n    return X[a]\n\nclass Autoencoder:\n    def __init__(self, input_dim, hidden_dim, epoch=1000, batch_size=50, learning_rate=0.001):\n        self.epoch = epoch\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n\n        x = tf.placeholder(dtype=tf.float32, shape=[None, input_dim])\n        with tf.name_scope('encode'):\n            weights = tf.Variable(tf.random_normal([input_dim, hidden_dim], dtype=tf.float32))\n            biases = tf.Variable(tf.zeros([hidden_dim]))\n            encoded = tf.nn.sigmoid(tf.matmul(x, weights) + biases)\n        with tf.name_scope('decode'):\n            weights = tf.Variable(tf.random_normal([hidden_dim, input_dim], dtype=tf.float32))\n            biases = tf.Variable(tf.zeros([input_dim]))\n            decoded = tf.matmul(encoded, weights) + biases\n\n        self.x = x\n        self.encoded = encoded\n        self.decoded = decoded\n\n        self.loss = tf.sqrt(tf.reduce_mean(tf.square(tf.sub(self.x, self.decoded))))\n        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n        self.saver = tf.train.Saver()\n\n    def train(self, data):\n        with tf.Session() as sess:\n            sess.run(tf.initialize_all_variables())\n            for i in range(self.epoch):\n                for j in range(50):\n                    batch_data = get_batch(data, self.batch_size)\n                    l, _ = sess.run([self.loss, self.train_op], feed_dict={self.x: batch_data})\n                if i % 10 == 0:\n                    print('epoch {0}: loss = {1}'.format(i, l))\n                    self.saver.save(sess, './model.ckpt')\n            self.saver.save(sess, './model.ckpt')\n        \n    def test(self, data):\n        with tf.Session() as sess:\n            self.saver.restore(sess, './model.ckpt')\n            hidden, reconstructed = sess.run([self.encoded, self.decoded], feed_dict={self.x: data})\n        print('input', data)\n        print('compressed', hidden)\n        print('reconstructed', reconstructed)\n        return reconstructed\n\n    def get_params(self):\n        with tf.Session() as sess:\n            self.saver.restore(sess, './model.ckpt')\n            weights, biases = sess.run([self.weights1, self.biases1])\n        return weights, biases\n"""
ch07_autoencoder/export_parameters.py,0,"b""from autoencoder import Autoencoder\nfrom scipy.misc import imread, imresize, imsave\nimport numpy as np\nimport h5py\n\ndef zero_pad(num, pad):\n    return format(num, '0' + str(pad))\n\ndata_dir = '../vids/'\nfilename_prefix = 'raw_rgb_'\n\nhidden_dim = 1000\n\nfilepath = data_dir + str(1) + '/' + filename_prefix + zero_pad(20, 5) + '.png'\nimg = imresize(imread(filepath, True), 1. / 8.)\n\nimg_data = img.flatten()\n\nae = Autoencoder([img_data], hidden_dim)\n\nweights, biases = ae.get_params()\n\nprint(np.shape(weights))\nprint(np.shape([biases]))\n\nh5f_W = h5py.File('encoder_W.h5', 'w')\nh5f_W.create_dataset('dataset_1', data=weights)\nh5f_W.close()\n\nh5f_b = h5py.File('encoder_b.h5', 'w')\nh5f_b.create_dataset('dataset_1', data=[biases])\nh5f_b.close()\n"""
ch07_autoencoder/main.py,0,"b'from autoencoder import Autoencoder\nfrom sklearn import datasets\n\nhidden_dim = 1\ndata = datasets.load_iris().data\ninput_dim = len(data[0])\nae = Autoencoder(input_dim, hidden_dim)\nae.train(data)\nae.test([[8, 4, 6, 2]])\n'"
ch07_autoencoder/main_imgs.py,0,"b""# https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n\nimport cPickle\nimport numpy as np\nfrom autoencoder import Autoencoder\n#\n# def grayscale(x):\n#     gray = np.zeros(len(x)/3)\n#     for i in range(len(x)/3):\n#         gray[i] = (x[i] + x[2*i] + x[3*i]) / 3\n\n\ndef grayscale(a):\n    return a.reshape(a.shape[0], 3, 32, 32).mean(1).reshape(a.shape[0], -1)\n\n\ndef unpickle(file):\n    fo = open(file, 'rb')\n    dict = cPickle.load(fo)\n    fo.close()\n    return dict\n\nnames = unpickle('./cifar-10-batches-py/batches.meta')['label_names']\ndata, labels = [], []\nfor i in range(1, 6):\n    filename = './cifar-10-batches-py/data_batch_' + str(i)\n    batch_data = unpickle(filename)\n    if len(data) > 0:\n        data = np.vstack((data, batch_data['data']))\n        labels = np.vstack((labels, batch_data['labels']))\n    else:\n        data = batch_data['data']\n        labels = batch_data['labels']\n\ndata = grayscale(data)\n\nx = np.matrix(data)\ny = np.array(labels)\n\nhorse_indices = np.where(y == 7)[0]\n\nhorse_x = x[horse_indices]\n\nprint(np.shape(horse_x))  # (5000, 3072)\n\ninput_dim = np.shape(horse_x)[1]\nhidden_dim = 100\nae = Autoencoder(input_dim, hidden_dim)\nae.train(horse_x)\n\ntest_data = unpickle('./cifar-10-batches-py/test_batch')\ntest_x = grayscale(test_data['data'])\ntest_labels = np.array(test_data['labels'])\nencoding = ae.classify(test_x, test_labels)\nencoding = np.matrix(encoding)\nfrom matplotlib import pyplot as plt\n\n# encoding = np.matrix(np.random.choice([0, 1], size=(hidden_dim,)))\n\noriginal_img = np.reshape(test_x[7,:], (32,32))\nplt.imshow(original_img, cmap='Greys_r')\nplt.show()\n\nprint(np.size(encoding))\nwhile(True):\n    img = ae.decode(encoding)\n    plt.imshow(img, cmap='Greys_r')\n    plt.show()\n    rand_idx = np.random.randint(np.size(encoding))\n    encoding[0,rand_idx] = np.random.randint(2)\n"""
ch08_rl/rl.py,12,"b""from matplotlib import pyplot as plt\nimport numpy as np\nimport random\nimport tensorflow as tf\nimport random\nimport pandas as pd\npd.core.common.is_list_like = pd.api.types.is_list_like\nfrom pandas_datareader import data\nimport datetime\nimport requests_cache\n\nclass DecisionPolicy:\n    def select_action(self, current_state, step):\n        pass\n\n    def update_q(self, state, action, reward, next_state):\n        pass\n\n\nclass RandomDecisionPolicy(DecisionPolicy):\n    def __init__(self, actions):\n        self.actions = actions\n\n    def select_action(self, current_state, step):\n        action = self.actions[random.randint(0, len(self.actions) - 1)]\n        return action\n\n\nclass QLearningDecisionPolicy(DecisionPolicy):\n    def __init__(self, actions, input_dim):\n        self.epsilon = 0.9\n        self.gamma = 0.001\n        self.actions = actions\n        output_dim = len(actions)\n        h1_dim = 200\n\n        self.x = tf.placeholder(tf.float32, [None, input_dim])\n        self.y = tf.placeholder(tf.float32, [output_dim])\n        W1 = tf.Variable(tf.random_normal([input_dim, h1_dim]))\n        b1 = tf.Variable(tf.constant(0.1, shape=[h1_dim]))\n        h1 = tf.nn.relu(tf.matmul(self.x, W1) + b1)\n        W2 = tf.Variable(tf.random_normal([h1_dim, output_dim]))\n        b2 = tf.Variable(tf.constant(0.1, shape=[output_dim]))\n        self.q = tf.nn.relu(tf.matmul(h1, W2) + b2)\n\n        loss = tf.square(self.y - self.q)\n        self.train_op = tf.train.AdagradOptimizer(0.01).minimize(loss)\n        self.sess = tf.Session()\n        self.sess.run(tf.global_variables_initializer())\n\n    def select_action(self, current_state, step):\n        threshold = min(self.epsilon, step / 1000.)\n        if random.random() < threshold:\n            # Exploit best option with probability epsilon\n            action_q_vals = self.sess.run(self.q, feed_dict={self.x: current_state})\n            action_idx = np.argmax(action_q_vals)  # TODO: replace w/ tensorflow's argmax\n            action = self.actions[action_idx]\n        else:\n            # Explore random option with probability 1 - epsilon\n            action = self.actions[random.randint(0, len(self.actions) - 1)]\n        return action\n\n    def update_q(self, state, action, reward, next_state):\n        action_q_vals = self.sess.run(self.q, feed_dict={self.x: state})\n        next_action_q_vals = self.sess.run(self.q, feed_dict={self.x: next_state})\n        next_action_idx = np.argmax(next_action_q_vals)\n        action_q_vals[0, next_action_idx] = reward + self.gamma * next_action_q_vals[0, next_action_idx]\n        action_q_vals = np.squeeze(np.asarray(action_q_vals))\n        self.sess.run(self.train_op, feed_dict={self.x: state, self.y: action_q_vals})\n\n\ndef run_simulation(policy, initial_budget, initial_num_stocks, prices, hist, debug=False):\n    budget = initial_budget\n    num_stocks = initial_num_stocks\n    share_value = 0\n    transitions = list()\n    for i in range(len(prices) - hist - 1):\n        if i % 100 == 0:\n            print('progress {:.2f}%'.format(float(100*i) / (len(prices) - hist - 1)))\n        current_state = np.asmatrix(np.hstack((prices[i:i+hist], budget, num_stocks)))\n        current_portfolio = budget + num_stocks * share_value\n        action = policy.select_action(current_state, i)\n        share_value = float(prices[i + hist + 1])\n        if action == 'Buy' and budget >= share_value:\n            budget -= share_value\n            num_stocks += 1\n        elif action == 'Sell' and num_stocks > 0:\n            budget += share_value\n            num_stocks -= 1\n        else:\n            action = 'Hold'\n        new_portfolio = budget + num_stocks * share_value\n        reward = new_portfolio - current_portfolio\n        next_state = np.asmatrix(np.hstack((prices[i+1:i+hist+1], budget, num_stocks)))\n        transitions.append((current_state, action, reward, next_state))\n        policy.update_q(current_state, action, reward, next_state)\n\n    portfolio = budget + num_stocks * share_value\n    if debug:\n        print('${}\\t{} shares'.format(budget, num_stocks))\n    return portfolio\n\n\ndef run_simulations(policy, budget, num_stocks, prices, hist):\n    num_tries = 10\n    final_portfolios = list()\n    for i in range(num_tries):\n        final_portfolio = run_simulation(policy, budget, num_stocks, prices, hist)\n        final_portfolios.append(final_portfolio)\n    avg, std = np.mean(final_portfolios), np.std(final_portfolios)\n    return avg, std\n\n\ndef get_prices(share_symbol, start_date, end_date):\n    expire_after = datetime.timedelta(days=3)\n    session = requests_cache.CachedSession(cache_name='cache', backend='sqlite', expire_after=expire_after)\n    stock_hist = data.DataReader(share_symbol, 'iex', start_date, end_date, session=session)               \n    open_prices = stock_hist['open']    \n    return open_prices.values.tolist()\n\ndef plot_prices(prices):\n    plt.title('Opening stock prices')\n    plt.xlabel('day')\n    plt.ylabel('price ($)')\n    plt.plot(prices)\n    plt.savefig('prices.png')\n\n\nif __name__ == '__main__':\n    prices = get_prices('MSFT', '2013-07-22', '2018-07-22')\n    plot_prices(prices)\n    actions = ['Buy', 'Sell', 'Hold']\n    hist = 200\n    # policy = RandomDecisionPolicy(actions)\n    policy = QLearningDecisionPolicy(actions, hist + 2)\n    budget = 1000.0\n    num_stocks = 0\n    avg, std = run_simulations(policy, budget, num_stocks, prices, hist)\n    print(avg, std)\n\n"""
ch09_cnn/cifar_tools.py,0,"b""import pickle\nimport numpy as np\n\n\ndef unpickle(file):\n    fo = open(file, 'rb')\n    dict = pickle.load(fo, encoding='latin1')\n    fo.close()\n    return dict\n\n\ndef clean(data):\n    imgs = data.reshape(data.shape[0], 3, 32, 32)\n    grayscale_imgs = imgs.mean(1)\n    cropped_imgs = grayscale_imgs[:, 4:28, 4:28]\n    img_data = cropped_imgs.reshape(data.shape[0], -1)\n    img_size = np.shape(img_data)[1]\n    means = np.mean(img_data, axis=1)\n    meansT = means.reshape(len(means), 1)\n    stds = np.std(img_data, axis=1)\n    stdsT = stds.reshape(len(stds), 1)\n    adj_stds = np.maximum(stdsT, 1.0 / np.sqrt(img_size))\n    normalized = (img_data - meansT) / adj_stds\n    return normalized\n\n\ndef read_data(directory):\n    names = unpickle('{}/batches.meta'.format(directory))['label_names']\n    print('names', names)\n\n    data, labels = [], []\n    for i in range(1, 6):\n        filename = '{}/data_batch_{}'.format(directory, i)\n        batch_data = unpickle(filename)\n        if len(data) > 0:\n            data = np.vstack((data, batch_data['data']))\n            labels = np.hstack((labels, batch_data['labels']))\n        else:\n            data = batch_data['data']\n            labels = batch_data['labels']\n\n    print(np.shape(data), np.shape(labels))\n\n    data = clean(data)\n    data = data.astype(np.float32)\n    return names, data, labels\n"""
ch09_cnn/cnn.py,28,"b""import numpy as np\nimport matplotlib.pyplot as plt\nimport cifar_tools\nimport tensorflow as tf\n\nlearning_rate = 0.001\n\nnames, data, labels = \\\n    cifar_tools.read_data('/home/binroot/res/cifar-10-batches-py')\n\nx = tf.placeholder(tf.float32, [None, 24 * 24])\ny = tf.placeholder(tf.float32, [None, len(names)])\nW1 = tf.Variable(tf.random_normal([5, 5, 1, 64]))\nb1 = tf.Variable(tf.random_normal([64]))\nW2 = tf.Variable(tf.random_normal([5, 5, 64, 64]))\nb2 = tf.Variable(tf.random_normal([64]))\nW3 = tf.Variable(tf.random_normal([6*6*64, 1024]))\nb3 = tf.Variable(tf.random_normal([1024]))\nW_out = tf.Variable(tf.random_normal([1024, len(names)]))\nb_out = tf.Variable(tf.random_normal([len(names)]))\n\n\ndef conv_layer(x, W, b):\n    conv = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n    conv_with_b = tf.nn.bias_add(conv, b)\n    conv_out = tf.nn.relu(conv_with_b)\n    return conv_out\n\n\ndef maxpool_layer(conv, k=2):\n    return tf.nn.max_pool(conv, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n\n\ndef model():\n    x_reshaped = tf.reshape(x, shape=[-1, 24, 24, 1])\n\n    conv_out1 = conv_layer(x_reshaped, W1, b1)\n    maxpool_out1 = maxpool_layer(conv_out1)\n    norm1 = tf.nn.lrn(maxpool_out1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n    conv_out2 = conv_layer(norm1, W2, b2)\n    norm2 = tf.nn.lrn(conv_out2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n    maxpool_out2 = maxpool_layer(norm2)\n\n    maxpool_reshaped = tf.reshape(maxpool_out2, [-1, W3.get_shape().as_list()[0]])\n    local = tf.add(tf.matmul(maxpool_reshaped, W3), b3)\n    local_out = tf.nn.relu(local)\n\n    out = tf.add(tf.matmul(local_out, W_out), b_out)\n    return out\n\nmodel_op = model()\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(model_op, y))\ntrain_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\ncorrect_pred = tf.equal(tf.argmax(model_op, 1), tf.argmax(y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    onehot_labels = tf.one_hot(labels, len(names), on_value=1., off_value=0., axis=-1)\n    onehot_vals = sess.run(onehot_labels)\n    batch_size = len(data) / 200\n    print('batch size', batch_size)\n    for j in range(0, 1000):\n        print('EPOCH', j)\n        for i in range(0, len(data), batch_size):\n            batch_data = data[i:i+batch_size, :]\n            batch_onehot_vals = onehot_vals[i:i+batch_size, :]\n            _, accuracy_val = sess.run([train_op, accuracy], feed_dict={x: batch_data, y: batch_onehot_vals})\n            if i % 1000 == 0:\n                print(i, accuracy_val)\n        print('DONE WITH EPOCH')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"""
ch09_cnn/cnn_viz.py,32,"b""import numpy as np\nimport matplotlib.pyplot as plt\nimport cifar_tools\nimport tensorflow as tf\n\nlearning_rate = 0.001\n\nnames, data, labels = \\\n    cifar_tools.read_data('/home/binroot/res/cifar-10-batches-py')\n\nx = tf.placeholder(tf.float32, [None, 24 * 24], name='input')\ny = tf.placeholder(tf.float32, [None, len(names)], name='prediction')\nW1 = tf.Variable(tf.random_normal([5, 5, 1, 64]), name='W1')\nb1 = tf.Variable(tf.random_normal([64]), name='b1')\nW2 = tf.Variable(tf.random_normal([5, 5, 64, 64]), name='W2')\nb2 = tf.Variable(tf.random_normal([64]), name='b2')\nW3 = tf.Variable(tf.random_normal([6*6*64, 1024]), name='W3')\nb3 = tf.Variable(tf.random_normal([1024]), name='b3')\nW_out = tf.Variable(tf.random_normal([1024, len(names)]), name='W_out')\nb_out = tf.Variable(tf.random_normal([len(names)]), name='b_out')\n\n\nW1_summary = tf.image_summary('W1_img', W1)\n\ndef conv_layer(x, W, b):\n    conv = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n    conv_with_b = tf.nn.bias_add(conv, b)\n    conv_out = tf.nn.relu(conv_with_b)\n    return conv_out\n\n\ndef maxpool_layer(conv, k=2):\n    return tf.nn.max_pool(conv, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n\n\ndef model():\n    x_reshaped = tf.reshape(x, shape=[-1, 24, 24, 1])\n\n    conv_out1 = conv_layer(x_reshaped, W1, b1)\n    maxpool_out1 = maxpool_layer(conv_out1)\n    norm1 = tf.nn.lrn(maxpool_out1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n    conv_out2 = conv_layer(norm1, W2, b2)\n    norm2 = tf.nn.lrn(conv_out2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n    maxpool_out2 = maxpool_layer(norm2)\n\n    maxpool_reshaped = tf.reshape(maxpool_out2, [-1, W3.get_shape().as_list()[0]])\n    local = tf.add(tf.matmul(maxpool_reshaped, W3), b3)\n    local_out = tf.nn.relu(local)\n\n    out = tf.add(tf.matmul(local_out, W_out), b_out)\n    return out\n\nmodel_op = model()\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(model_op, y))\ntf.scalar_summary('cost', cost)\ntrain_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\ncorrect_pred = tf.equal(tf.argmax(model_op, 1), tf.argmax(y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\nmerged = tf.merge_all_summaries()\n\nwith tf.Session() as sess:\n    summary_writer = tf.train.SummaryWriter('summaries/train', sess.graph)\n    sess.run(tf.initialize_all_variables())\n    onehot_labels = tf.one_hot(labels, len(names), on_value=1., off_value=0., axis=-1)\n    onehot_vals = sess.run(onehot_labels)\n    batch_size = len(data) / 200\n    print('batch size', batch_size)\n    for j in range(0, 1000):\n        print('EPOCH', j)\n        for i in range(0, len(data), batch_size):\n            batch_data = data[i:i+batch_size, :]\n            batch_onehot_vals = onehot_vals[i:i+batch_size, :]\n            _, accuracy_val, summary = sess.run([train_op, accuracy, merged], feed_dict={x: batch_data, y: batch_onehot_vals})\n            summary_writer.add_summary(summary, i)\n            if i % 1000 == 0:\n                print(i, accuracy_val)\n        print('DONE WITH EPOCH')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"""
ch09_cnn/conv_visuals.py,9,"b""import numpy as np\nimport matplotlib.pyplot as plt\nimport cifar_tools\nimport tensorflow as tf\n\nnames, data, labels = \\\n    cifar_tools.read_data('/home/binroot/res/cifar-10-batches-py')\n\n\ndef show_conv_results(data, filename=None):\n    plt.figure()\n    rows, cols = 4, 8\n    for i in range(np.shape(data)[3]):\n        img = data[0, :, :, i]\n        plt.subplot(rows, cols, i + 1)\n        plt.imshow(img, cmap='Greys_r', interpolation='none')\n        plt.axis('off')\n    if filename:\n        plt.savefig(filename)\n    else:\n        plt.show()\n\n\ndef show_weights(W, filename=None):\n    plt.figure()\n    rows, cols = 4, 8\n    for i in range(np.shape(W)[3]):\n        img = W[:, :, 0, i]\n        plt.subplot(rows, cols, i + 1)\n        plt.imshow(img, cmap='Greys_r', interpolation='none')\n        plt.axis('off')\n    if filename:\n        plt.savefig(filename)\n    else:\n        plt.show()\n\nraw_data = data[4, :]\nraw_img = np.reshape(raw_data, (24, 24))\nplt.figure()\nplt.imshow(raw_img, cmap='Greys_r')\nplt.savefig('input_image.png')\n\nx = tf.reshape(raw_data, shape=[-1, 24, 24, 1])\nW = tf.Variable(tf.random_normal([5, 5, 1, 32]))\nb = tf.Variable(tf.random_normal([32]))\n\nconv = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\nconv_with_b = tf.nn.bias_add(conv, b)\nconv_out = tf.nn.relu(conv_with_b)\n\nk = 2\nmaxpool = tf.nn.max_pool(conv_out, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n\n    W_val = sess.run(W)\n    show_weights(W_val, 'step0_weights.png')\n\n    conv_val = sess.run(conv)\n    show_conv_results(conv_val, 'step1_convs.png')\n    print(np.shape(conv_val))\n\n    conv_out_val = sess.run(conv_out)\n    show_conv_results(conv_out_val, 'step2_conv_outs.png')\n    print(np.shape(conv_out_val))\n\n    maxpool_val = sess.run(maxpool)\n    show_conv_results(maxpool_val, 'step3_maxpool.png')\n    print(np.shape(maxpool_val))\n\n\n\n\n"""
ch09_cnn/using_cifar.py,0,"b""import numpy as np\nimport matplotlib.pyplot as plt\nimport cifar_tools\nimport random\n\nnames, data, labels = \\\n    cifar_tools.read_data('/home/binroot/res/cifar-10-batches-py')\n\nrandom.seed(1)\n\n\ndef show_some_examples(names, data, labels):\n    plt.figure()\n    rows, cols = 4, 4\n    random_idxs = random.sample(range(len(data)), rows * cols)\n    for i in range(rows * cols):\n        plt.subplot(rows, cols, i + 1)\n        j = random_idxs[i]\n        plt.title(names[labels[j]])\n        img = np.reshape(data[j, :], (24, 24))\n        plt.imshow(img, cmap='Greys_r')\n        plt.axis('off')\n    plt.tight_layout()\n    plt.savefig('cifar_examples.png')\n\nshow_some_examples(names, data, labels)"""
ch10_rnn/data_loader.py,0,"b""import csv\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef load_series(filename, series_idx=1):\n    try:\n        with open(filename) as csvfile:\n            csvreader = csv.reader(csvfile)\n            data = [float(row[series_idx]) for row in csvreader if len(row) > 0]\n            normalized_data = (data - np.mean(data)) / np.std(data)\n        return normalized_data\n    except IOError:\n        return None\n\n\ndef split_data(data, percent_train=0.80):\n    num_rows = len(data)\n    train_data, test_data = [], []\n    for idx, row in enumerate(data):\n        if idx < num_rows * percent_train:\n            train_data.append(row)\n        else:\n            test_data.append(row)\n    return train_data, test_data\n\n\nif __name__=='__main__':\n    # https://datamarket.com/data/set/22u3/international-airline-passengers-monthly-totals-in-thousands-jan-49-dec-60#!ds=22u3&display=line\n    timeseries = load_series('international-airline-passengers.csv')\n    print(np.shape(timeseries))\n\n    plt.figure()\n    plt.plot(timeseries)\n    plt.show()\n\n"""
ch10_rnn/regression.py,17,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.ops import rnn, rnn_cell\nimport data_loader\nimport matplotlib.pyplot as plt\n\nclass SeriesPredictor:\n\n    def __init__(self, input_dim, seq_size, hidden_dim):\n        # Hyperparameters\n        self.input_dim = input_dim\n        self.seq_size = seq_size\n        self.hidden_dim = hidden_dim\n\n        # Weight variables and input placeholders\n        self.W_out = tf.Variable(tf.random_normal([hidden_dim, 1]), name=\'W_out\')\n        self.b_out = tf.Variable(tf.random_normal([1]), name=\'b_out\')\n        self.x = tf.placeholder(tf.float32, [None, seq_size, input_dim])\n        self.y = tf.placeholder(tf.float32, [None, seq_size])\n\n        # Cost optimizer\n        self.cost = tf.reduce_mean(tf.square(self.model() - self.y))\n        self.train_op = tf.train.AdamOptimizer(learning_rate=0.003).minimize(self.cost)\n\n        # Auxiliary ops\n        self.saver = tf.train.Saver()\n\n    def model(self):\n        """"""\n        :param x: inputs of size [T, batch_size, input_size]\n        :param W: matrix of fully-connected output layer weights\n        :param b: vector of fully-connected output layer biases\n        """"""\n        cell = rnn_cell.BasicLSTMCell(self.hidden_dim)\n        outputs, states = rnn.dynamic_rnn(cell, self.x, dtype=tf.float32)\n        num_examples = tf.shape(self.x)[0]\n        W_repeated = tf.tile(tf.expand_dims(self.W_out, 0), [num_examples, 1, 1])\n        out = tf.batch_matmul(outputs, W_repeated) + self.b_out\n        out = tf.squeeze(out)\n        return out\n\n    def train(self, train_x, train_y, test_x, test_y):\n        with tf.Session() as sess:\n            tf.get_variable_scope().reuse_variables()\n            sess.run(tf.initialize_all_variables())\n            max_patience = 3\n            patience = max_patience\n            min_test_err = float(\'inf\')\n            step = 0\n            while patience > 0:\n                _, train_err = sess.run([self.train_op, self.cost], feed_dict={self.x: train_x, self.y: train_y})\n                if step % 100 == 0:\n                    test_err = sess.run(self.cost, feed_dict={self.x: test_x, self.y: test_y})\n                    print(\'step: {}\\t\\ttrain err: {}\\t\\ttest err: {}\'.format(step, train_err, test_err))\n                    if test_err < min_test_err:\n                        min_test_err = test_err\n                        patience = max_patience\n                    else:\n                        patience -= 1\n                step += 1\n            save_path = self.saver.save(sess, \'model.ckpt\')\n            print(\'Model saved to {}\'.format(save_path))\n\n    def test(self, sess, test_x):\n        tf.get_variable_scope().reuse_variables()\n        self.saver.restore(sess, \'model.ckpt\')\n        output = sess.run(self.model(), feed_dict={self.x: test_x})\n        return output\n\n\ndef plot_results(train_x, predictions, actual, filename):\n    plt.figure()\n    num_train = len(train_x)\n    plt.plot(list(range(num_train)), train_x, color=\'b\', label=\'training data\')\n    plt.plot(list(range(num_train, num_train + len(predictions))), predictions, color=\'r\', label=\'predicted\')\n    plt.plot(list(range(num_train, num_train + len(actual))), actual, color=\'g\', label=\'test data\')\n    plt.legend()\n    if filename is not None:\n        plt.savefig(filename)\n    else:\n        plt.show()\n\n\nif __name__ == \'__main__\':\n    seq_size = 5\n    predictor = SeriesPredictor(input_dim=1, seq_size=seq_size, hidden_dim=5)\n    data = data_loader.load_series(\'international-airline-passengers.csv\')\n    train_data, actual_vals = data_loader.split_data(data)\n\n    train_x, train_y = [], []\n    for i in range(len(train_data) - seq_size - 1):\n        train_x.append(np.expand_dims(train_data[i:i+seq_size], axis=1).tolist())\n        train_y.append(train_data[i+1:i+seq_size+1])\n\n    test_x, test_y = [], []\n    for i in range(len(actual_vals) - seq_size - 1):\n        test_x.append(np.expand_dims(actual_vals[i:i+seq_size], axis=1).tolist())\n        test_y.append(actual_vals[i+1:i+seq_size+1])\n\n    predictor.train(train_x, train_y, test_x, test_y)\n\n    with tf.Session() as sess:\n        predicted_vals = predictor.test(sess, test_x)[:,0]\n        print(\'predicted_vals\', np.shape(predicted_vals))\n        plot_results(train_data, predicted_vals, actual_vals, \'predictions.png\')\n\n        prev_seq = train_x[-1]\n        predicted_vals = []\n        for i in range(20):\n            next_seq = predictor.test(sess, [prev_seq])\n            predicted_vals.append(next_seq[-1])\n            prev_seq = np.vstack((prev_seq[1:], next_seq[-1]))\n        plot_results(train_data, predicted_vals, actual_vals, \'hallucinations.png\')\n'"
ch10_rnn/simple_regression.py,18,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.ops import rnn, rnn_cell\n\n\nclass SeriesPredictor:\n\n    def __init__(self, input_dim, seq_size, hidden_dim=10):\n        # Hyperparameters\n        self.input_dim = input_dim\n        self.seq_size = seq_size\n        self.hidden_dim = hidden_dim\n\n        # Weight variables and input placeholders\n        self.W_out = tf.Variable(tf.random_normal([hidden_dim, 1]), name=\'W_out\')\n        self.b_out = tf.Variable(tf.random_normal([1]), name=\'b_out\')\n        self.x = tf.placeholder(tf.float32, [None, seq_size, input_dim])\n        self.y = tf.placeholder(tf.float32, [None, seq_size])\n\n        # Cost optimizer\n        self.cost = tf.reduce_mean(tf.square(self.model() - self.y))\n        self.train_op = tf.train.AdamOptimizer().minimize(self.cost)\n\n        # Auxiliary ops\n        self.saver = tf.train.Saver()\n\n    def model(self):\n        """"""\n        :param x: inputs of size [T, batch_size, input_size]\n        :param W: matrix of fully-connected output layer weights\n        :param b: vector of fully-connected output layer biases\n        """"""\n        cell = rnn_cell.BasicLSTMCell(self.hidden_dim, reuse=tf.get_variable_scope().reuse)\n        outputs, states = rnn.dynamic_rnn(cell, self.x, dtype=tf.float32)\n        num_examples = tf.shape(self.x)[0]\n        W_repeated = tf.tile(tf.expand_dims(self.W_out, 0), [num_examples, 1, 1])\n        out = tf.batch_matmul(outputs, W_repeated) + self.b_out\n        out = tf.squeeze(out)\n        return out\n\n    def train(self, train_x, train_y):\n        with tf.Session() as sess:\n            tf.get_variable_scope().reuse_variables()\n            sess.run(tf.initialize_all_variables())\n            for i in range(1000):\n                _, mse = sess.run([self.train_op, self.cost], feed_dict={self.x: train_x, self.y: train_y})\n                if i % 100 == 0:\n                    print(i, mse)\n            save_path = self.saver.save(sess, \'model.ckpt\')\n            print(\'Model saved to {}\'.format(save_path))\n\n    def test(self, test_x):\n        with tf.Session() as sess:\n            tf.get_variable_scope().reuse_variables()\n            self.saver.restore(sess, \'model.ckpt\')\n            output = sess.run(self.model(), feed_dict={self.x: test_x})\n            print(output)\n\n\nif __name__ == \'__main__\':\n    predictor = SeriesPredictor(input_dim=1, seq_size=4, hidden_dim=10)\n    train_x = [[[1], [2], [5], [6]],\n               [[5], [7], [7], [8]],\n               [[3], [4], [5], [7]]]\n    train_y = [[1, 3, 7, 11],\n               [5, 12, 14, 15],\n               [3, 7, 9, 12]]\n    predictor.train(train_x, train_y)\n\n    test_x = [[[1], [2], [3], [4]],  # 1, 3, 5, 7\n              [[4], [5], [6], [7]]]  # 4, 9, 11, 13\n    predictor.test(test_x)\n'"
ch12_rank/imagenet_classes.py,0,"b'class_names = \'\'\'tench, Tinca tinca\ngoldfish, Carassius auratus\ngreat white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias\ntiger shark, Galeocerdo cuvieri\nhammerhead, hammerhead shark\nelectric ray, crampfish, numbfish, torpedo\nstingray\ncock\nhen\nostrich, Struthio camelus\nbrambling, Fringilla montifringilla\ngoldfinch, Carduelis carduelis\nhouse finch, linnet, Carpodacus mexicanus\njunco, snowbird\nindigo bunting, indigo finch, indigo bird, Passerina cyanea\nrobin, American robin, Turdus migratorius\nbulbul\njay\nmagpie\nchickadee\nwater ouzel, dipper\nkite\nbald eagle, American eagle, Haliaeetus leucocephalus\nvulture\ngreat grey owl, great gray owl, Strix nebulosa\nEuropean fire salamander, Salamandra salamandra\ncommon newt, Triturus vulgaris\neft\nspotted salamander, Ambystoma maculatum\naxolotl, mud puppy, Ambystoma mexicanum\nbullfrog, Rana catesbeiana\ntree frog, tree-frog\ntailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui\nloggerhead, loggerhead turtle, Caretta caretta\nleatherback turtle, leatherback, leathery turtle, Dermochelys coriacea\nmud turtle\nterrapin\nbox turtle, box tortoise\nbanded gecko\ncommon iguana, iguana, Iguana iguana\nAmerican chameleon, anole, Anolis carolinensis\nwhiptail, whiptail lizard\nagama\nfrilled lizard, Chlamydosaurus kingi\nalligator lizard\nGila monster, Heloderma suspectum\ngreen lizard, Lacerta viridis\nAfrican chameleon, Chamaeleo chamaeleon\nKomodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis\nAfrican crocodile, Nile crocodile, Crocodylus niloticus\nAmerican alligator, Alligator mississipiensis\ntriceratops\nthunder snake, worm snake, Carphophis amoenus\nringneck snake, ring-necked snake, ring snake\nhognose snake, puff adder, sand viper\ngreen snake, grass snake\nking snake, kingsnake\ngarter snake, grass snake\nwater snake\nvine snake\nnight snake, Hypsiglena torquata\nboa constrictor, Constrictor constrictor\nrock python, rock snake, Python sebae\nIndian cobra, Naja naja\ngreen mamba\nsea snake\nhorned viper, cerastes, sand viper, horned asp, Cerastes cornutus\ndiamondback, diamondback rattlesnake, Crotalus adamanteus\nsidewinder, horned rattlesnake, Crotalus cerastes\ntrilobite\nharvestman, daddy longlegs, Phalangium opilio\nscorpion\nblack and gold garden spider, Argiope aurantia\nbarn spider, Araneus cavaticus\ngarden spider, Aranea diademata\nblack widow, Latrodectus mactans\ntarantula\nwolf spider, hunting spider\ntick\ncentipede\nblack grouse\nptarmigan\nruffed grouse, partridge, Bonasa umbellus\nprairie chicken, prairie grouse, prairie fowl\npeacock\nquail\npartridge\nAfrican grey, African gray, Psittacus erithacus\nmacaw\nsulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita\nlorikeet\ncoucal\nbee eater\nhornbill\nhummingbird\njacamar\ntoucan\ndrake\nred-breasted merganser, Mergus serrator\ngoose\nblack swan, Cygnus atratus\ntusker\nechidna, spiny anteater, anteater\nplatypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus\nwallaby, brush kangaroo\nkoala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus\nwombat\njellyfish\nsea anemone, anemone\nbrain coral\nflatworm, platyhelminth\nnematode, nematode worm, roundworm\nconch\nsnail\nslug\nsea slug, nudibranch\nchiton, coat-of-mail shell, sea cradle, polyplacophore\nchambered nautilus, pearly nautilus, nautilus\nDungeness crab, Cancer magister\nrock crab, Cancer irroratus\nfiddler crab\nking crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica\nAmerican lobster, Northern lobster, Maine lobster, Homarus americanus\nspiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish\ncrayfish, crawfish, crawdad, crawdaddy\nhermit crab\nisopod\nwhite stork, Ciconia ciconia\nblack stork, Ciconia nigra\nspoonbill\nflamingo\nlittle blue heron, Egretta caerulea\nAmerican egret, great white heron, Egretta albus\nbittern\ncrane\nlimpkin, Aramus pictus\nEuropean gallinule, Porphyrio porphyrio\nAmerican coot, marsh hen, mud hen, water hen, Fulica americana\nbustard\nruddy turnstone, Arenaria interpres\nred-backed sandpiper, dunlin, Erolia alpina\nredshank, Tringa totanus\ndowitcher\noystercatcher, oyster catcher\npelican\nking penguin, Aptenodytes patagonica\nalbatross, mollymawk\ngrey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus\nkiller whale, killer, orca, grampus, sea wolf, Orcinus orca\ndugong, Dugong dugon\nsea lion\nChihuahua\nJapanese spaniel\nMaltese dog, Maltese terrier, Maltese\nPekinese, Pekingese, Peke\nShih-Tzu\nBlenheim spaniel\npapillon\ntoy terrier\nRhodesian ridgeback\nAfghan hound, Afghan\nbasset, basset hound\nbeagle\nbloodhound, sleuthhound\nbluetick\nblack-and-tan coonhound\nWalker hound, Walker foxhound\nEnglish foxhound\nredbone\nborzoi, Russian wolfhound\nIrish wolfhound\nItalian greyhound\nwhippet\nIbizan hound, Ibizan Podenco\nNorwegian elkhound, elkhound\notterhound, otter hound\nSaluki, gazelle hound\nScottish deerhound, deerhound\nWeimaraner\nStaffordshire bullterrier, Staffordshire bull terrier\nAmerican Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier\nBedlington terrier\nBorder terrier\nKerry blue terrier\nIrish terrier\nNorfolk terrier\nNorwich terrier\nYorkshire terrier\nwire-haired fox terrier\nLakeland terrier\nSealyham terrier, Sealyham\nAiredale, Airedale terrier\ncairn, cairn terrier\nAustralian terrier\nDandie Dinmont, Dandie Dinmont terrier\nBoston bull, Boston terrier\nminiature schnauzer\ngiant schnauzer\nstandard schnauzer\nScotch terrier, Scottish terrier, Scottie\nTibetan terrier, chrysanthemum dog\nsilky terrier, Sydney silky\nsoft-coated wheaten terrier\nWest Highland white terrier\nLhasa, Lhasa apso\nflat-coated retriever\ncurly-coated retriever\ngolden retriever\nLabrador retriever\nChesapeake Bay retriever\nGerman short-haired pointer\nvizsla, Hungarian pointer\nEnglish setter\nIrish setter, red setter\nGordon setter\nBrittany spaniel\nclumber, clumber spaniel\nEnglish springer, English springer spaniel\nWelsh springer spaniel\ncocker spaniel, English cocker spaniel, cocker\nSussex spaniel\nIrish water spaniel\nkuvasz\nschipperke\ngroenendael\nmalinois\nbriard\nkelpie\nkomondor\nOld English sheepdog, bobtail\nShetland sheepdog, Shetland sheep dog, Shetland\ncollie\nBorder collie\nBouvier des Flandres, Bouviers des Flandres\nRottweiler\nGerman shepherd, German shepherd dog, German police dog, alsatian\nDoberman, Doberman pinscher\nminiature pinscher\nGreater Swiss Mountain dog\nBernese mountain dog\nAppenzeller\nEntleBucher\nboxer\nbull mastiff\nTibetan mastiff\nFrench bulldog\nGreat Dane\nSaint Bernard, St Bernard\nEskimo dog, husky\nmalamute, malemute, Alaskan malamute\nSiberian husky\ndalmatian, coach dog, carriage dog\naffenpinscher, monkey pinscher, monkey dog\nbasenji\npug, pug-dog\nLeonberg\nNewfoundland, Newfoundland dog\nGreat Pyrenees\nSamoyed, Samoyede\nPomeranian\nchow, chow chow\nkeeshond\nBrabancon griffon\nPembroke, Pembroke Welsh corgi\nCardigan, Cardigan Welsh corgi\ntoy poodle\nminiature poodle\nstandard poodle\nMexican hairless\ntimber wolf, grey wolf, gray wolf, Canis lupus\nwhite wolf, Arctic wolf, Canis lupus tundrarum\nred wolf, maned wolf, Canis rufus, Canis niger\ncoyote, prairie wolf, brush wolf, Canis latrans\ndingo, warrigal, warragal, Canis dingo\ndhole, Cuon alpinus\nAfrican hunting dog, hyena dog, Cape hunting dog, Lycaon pictus\nhyena, hyaena\nred fox, Vulpes vulpes\nkit fox, Vulpes macrotis\nArctic fox, white fox, Alopex lagopus\ngrey fox, gray fox, Urocyon cinereoargenteus\ntabby, tabby cat\ntiger cat\nPersian cat\nSiamese cat, Siamese\nEgyptian cat\ncougar, puma, catamount, mountain lion, painter, panther, Felis concolor\nlynx, catamount\nleopard, Panthera pardus\nsnow leopard, ounce, Panthera uncia\njaguar, panther, Panthera onca, Felis onca\nlion, king of beasts, Panthera leo\ntiger, Panthera tigris\ncheetah, chetah, Acinonyx jubatus\nbrown bear, bruin, Ursus arctos\nAmerican black bear, black bear, Ursus americanus, Euarctos americanus\nice bear, polar bear, Ursus Maritimus, Thalarctos maritimus\nsloth bear, Melursus ursinus, Ursus ursinus\nmongoose\nmeerkat, mierkat\ntiger beetle\nladybug, ladybeetle, lady beetle, ladybird, ladybird beetle\nground beetle, carabid beetle\nlong-horned beetle, longicorn, longicorn beetle\nleaf beetle, chrysomelid\ndung beetle\nrhinoceros beetle\nweevil\nfly\nbee\nant, emmet, pismire\ngrasshopper, hopper\ncricket\nwalking stick, walkingstick, stick insect\ncockroach, roach\nmantis, mantid\ncicada, cicala\nleafhopper\nlacewing, lacewing fly\ndragonfly, darning needle, devil\'s darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\ndamselfly\nadmiral\nringlet, ringlet butterfly\nmonarch, monarch butterfly, milkweed butterfly, Danaus plexippus\ncabbage butterfly\nsulphur butterfly, sulfur butterfly\nlycaenid, lycaenid butterfly\nstarfish, sea star\nsea urchin\nsea cucumber, holothurian\nwood rabbit, cottontail, cottontail rabbit\nhare\nAngora, Angora rabbit\nhamster\nporcupine, hedgehog\nfox squirrel, eastern fox squirrel, Sciurus niger\nmarmot\nbeaver\nguinea pig, Cavia cobaya\nsorrel\nzebra\nhog, pig, grunter, squealer, Sus scrofa\nwild boar, boar, Sus scrofa\nwarthog\nhippopotamus, hippo, river horse, Hippopotamus amphibius\nox\nwater buffalo, water ox, Asiatic buffalo, Bubalus bubalis\nbison\nram, tup\nbighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis\nibex, Capra ibex\nhartebeest\nimpala, Aepyceros melampus\ngazelle\nArabian camel, dromedary, Camelus dromedarius\nllama\nweasel\nmink\npolecat, fitch, foulmart, foumart, Mustela putorius\nblack-footed ferret, ferret, Mustela nigripes\notter\nskunk, polecat, wood pussy\nbadger\narmadillo\nthree-toed sloth, ai, Bradypus tridactylus\norangutan, orang, orangutang, Pongo pygmaeus\ngorilla, Gorilla gorilla\nchimpanzee, chimp, Pan troglodytes\ngibbon, Hylobates lar\nsiamang, Hylobates syndactylus, Symphalangus syndactylus\nguenon, guenon monkey\npatas, hussar monkey, Erythrocebus patas\nbaboon\nmacaque\nlangur\ncolobus, colobus monkey\nproboscis monkey, Nasalis larvatus\nmarmoset\ncapuchin, ringtail, Cebus capucinus\nhowler monkey, howler\ntiti, titi monkey\nspider monkey, Ateles geoffroyi\nsquirrel monkey, Saimiri sciureus\nMadagascar cat, ring-tailed lemur, Lemur catta\nindri, indris, Indri indri, Indri brevicaudatus\nIndian elephant, Elephas maximus\nAfrican elephant, Loxodonta africana\nlesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens\ngiant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\nbarracouta, snoek\neel\ncoho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch\nrock beauty, Holocanthus tricolor\nanemone fish\nsturgeon\ngar, garfish, garpike, billfish, Lepisosteus osseus\nlionfish\npuffer, pufferfish, blowfish, globefish\nabacus\nabaya\nacademic gown, academic robe, judge\'s robe\naccordion, piano accordion, squeeze box\nacoustic guitar\naircraft carrier, carrier, flattop, attack aircraft carrier\nairliner\nairship, dirigible\naltar\nambulance\namphibian, amphibious vehicle\nanalog clock\napiary, bee house\napron\nashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin\nassault rifle, assault gun\nbackpack, back pack, knapsack, packsack, rucksack, haversack\nbakery, bakeshop, bakehouse\nbalance beam, beam\nballoon\nballpoint, ballpoint pen, ballpen, Biro\nBand Aid\nbanjo\nbannister, banister, balustrade, balusters, handrail\nbarbell\nbarber chair\nbarbershop\nbarn\nbarometer\nbarrel, cask\nbarrow, garden cart, lawn cart, wheelbarrow\nbaseball\nbasketball\nbassinet\nbassoon\nbathing cap, swimming cap\nbath towel\nbathtub, bathing tub, bath, tub\nbeach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon\nbeacon, lighthouse, beacon light, pharos\nbeaker\nbearskin, busby, shako\nbeer bottle\nbeer glass\nbell cote, bell cot\nbib\nbicycle-built-for-two, tandem bicycle, tandem\nbikini, two-piece\nbinder, ring-binder\nbinoculars, field glasses, opera glasses\nbirdhouse\nboathouse\nbobsled, bobsleigh, bob\nbolo tie, bolo, bola tie, bola\nbonnet, poke bonnet\nbookcase\nbookshop, bookstore, bookstall\nbottlecap\nbow\nbow tie, bow-tie, bowtie\nbrass, memorial tablet, plaque\nbrassiere, bra, bandeau\nbreakwater, groin, groyne, mole, bulwark, seawall, jetty\nbreastplate, aegis, egis\nbroom\nbucket, pail\nbuckle\nbulletproof vest\nbullet train, bullet\nbutcher shop, meat market\ncab, hack, taxi, taxicab\ncaldron, cauldron\ncandle, taper, wax light\ncannon\ncanoe\ncan opener, tin opener\ncardigan\ncar mirror\ncarousel, carrousel, merry-go-round, roundabout, whirligig\ncarpenter\'s kit, tool kit\ncarton\ncar wheel\ncash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM\ncassette\ncassette player\ncastle\ncatamaran\nCD player\ncello, violoncello\ncellular telephone, cellular phone, cellphone, cell, mobile phone\nchain\nchainlink fence\nchain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour\nchain saw, chainsaw\nchest\nchiffonier, commode\nchime, bell, gong\nchina cabinet, china closet\nChristmas stocking\nchurch, church building\ncinema, movie theater, movie theatre, movie house, picture palace\ncleaver, meat cleaver, chopper\ncliff dwelling\ncloak\nclog, geta, patten, sabot\ncocktail shaker\ncoffee mug\ncoffeepot\ncoil, spiral, volute, whorl, helix\ncombination lock\ncomputer keyboard, keypad\nconfectionery, confectionary, candy store\ncontainer ship, containership, container vessel\nconvertible\ncorkscrew, bottle screw\ncornet, horn, trumpet, trump\ncowboy boot\ncowboy hat, ten-gallon hat\ncradle\ncrane\ncrash helmet\ncrate\ncrib, cot\nCrock Pot\ncroquet ball\ncrutch\ncuirass\ndam, dike, dyke\ndesk\ndesktop computer\ndial telephone, dial phone\ndiaper, nappy, napkin\ndigital clock\ndigital watch\ndining table, board\ndishrag, dishcloth\ndishwasher, dish washer, dishwashing machine\ndisk brake, disc brake\ndock, dockage, docking facility\ndogsled, dog sled, dog sleigh\ndome\ndoormat, welcome mat\ndrilling platform, offshore rig\ndrum, membranophone, tympan\ndrumstick\ndumbbell\nDutch oven\nelectric fan, blower\nelectric guitar\nelectric locomotive\nentertainment center\nenvelope\nespresso maker\nface powder\nfeather boa, boa\nfile, file cabinet, filing cabinet\nfireboat\nfire engine, fire truck\nfire screen, fireguard\nflagpole, flagstaff\nflute, transverse flute\nfolding chair\nfootball helmet\nforklift\nfountain\nfountain pen\nfour-poster\nfreight car\nFrench horn, horn\nfrying pan, frypan, skillet\nfur coat\ngarbage truck, dustcart\ngasmask, respirator, gas helmet\ngas pump, gasoline pump, petrol pump, island dispenser\ngoblet\ngo-kart\ngolf ball\ngolfcart, golf cart\ngondola\ngong, tam-tam\ngown\ngrand piano, grand\ngreenhouse, nursery, glasshouse\ngrille, radiator grille\ngrocery store, grocery, food market, market\nguillotine\nhair slide\nhair spray\nhalf track\nhammer\nhamper\nhand blower, blow dryer, blow drier, hair dryer, hair drier\nhand-held computer, hand-held microcomputer\nhandkerchief, hankie, hanky, hankey\nhard disc, hard disk, fixed disk\nharmonica, mouth organ, harp, mouth harp\nharp\nharvester, reaper\nhatchet\nholster\nhome theater, home theatre\nhoneycomb\nhook, claw\nhoopskirt, crinoline\nhorizontal bar, high bar\nhorse cart, horse-cart\nhourglass\niPod\niron, smoothing iron\njack-o\'-lantern\njean, blue jean, denim\njeep, landrover\njersey, T-shirt, tee shirt\njigsaw puzzle\njinrikisha, ricksha, rickshaw\njoystick\nkimono\nknee pad\nknot\nlab coat, laboratory coat\nladle\nlampshade, lamp shade\nlaptop, laptop computer\nlawn mower, mower\nlens cap, lens cover\nletter opener, paper knife, paperknife\nlibrary\nlifeboat\nlighter, light, igniter, ignitor\nlimousine, limo\nliner, ocean liner\nlipstick, lip rouge\nLoafer\nlotion\nloudspeaker, speaker, speaker unit, loudspeaker system, speaker system\nloupe, jeweler\'s loupe\nlumbermill, sawmill\nmagnetic compass\nmailbag, postbag\nmailbox, letter box\nmaillot\nmaillot, tank suit\nmanhole cover\nmaraca\nmarimba, xylophone\nmask\nmatchstick\nmaypole\nmaze, labyrinth\nmeasuring cup\nmedicine chest, medicine cabinet\nmegalith, megalithic structure\nmicrophone, mike\nmicrowave, microwave oven\nmilitary uniform\nmilk can\nminibus\nminiskirt, mini\nminivan\nmissile\nmitten\nmixing bowl\nmobile home, manufactured home\nModel T\nmodem\nmonastery\nmonitor\nmoped\nmortar\nmortarboard\nmosque\nmosquito net\nmotor scooter, scooter\nmountain bike, all-terrain bike, off-roader\nmountain tent\nmouse, computer mouse\nmousetrap\nmoving van\nmuzzle\nnail\nneck brace\nnecklace\nnipple\nnotebook, notebook computer\nobelisk\noboe, hautboy, hautbois\nocarina, sweet potato\nodometer, hodometer, mileometer, milometer\noil filter\norgan, pipe organ\noscilloscope, scope, cathode-ray oscilloscope, CRO\noverskirt\noxcart\noxygen mask\npacket\npaddle, boat paddle\npaddlewheel, paddle wheel\npadlock\npaintbrush\npajama, pyjama, pj\'s, jammies\npalace\npanpipe, pandean pipe, syrinx\npaper towel\nparachute, chute\nparallel bars, bars\npark bench\nparking meter\npassenger car, coach, carriage\npatio, terrace\npay-phone, pay-station\npedestal, plinth, footstall\npencil box, pencil case\npencil sharpener\nperfume, essence\nPetri dish\nphotocopier\npick, plectrum, plectron\npickelhaube\npicket fence, paling\npickup, pickup truck\npier\npiggy bank, penny bank\npill bottle\npillow\nping-pong ball\npinwheel\npirate, pirate ship\npitcher, ewer\nplane, carpenter\'s plane, woodworking plane\nplanetarium\nplastic bag\nplate rack\nplow, plough\nplunger, plumber\'s helper\nPolaroid camera, Polaroid Land camera\npole\npolice van, police wagon, paddy wagon, patrol wagon, wagon, black Maria\nponcho\npool table, billiard table, snooker table\npop bottle, soda bottle\npot, flowerpot\npotter\'s wheel\npower drill\nprayer rug, prayer mat\nprinter\nprison, prison house\nprojectile, missile\nprojector\npuck, hockey puck\npunching bag, punch bag, punching ball, punchball\npurse\nquill, quill pen\nquilt, comforter, comfort, puff\nracer, race car, racing car\nracket, racquet\nradiator\nradio, wireless\nradio telescope, radio reflector\nrain barrel\nrecreational vehicle, RV, R.V.\nreel\nreflex camera\nrefrigerator, icebox\nremote control, remote\nrestaurant, eating house, eating place, eatery\nrevolver, six-gun, six-shooter\nrifle\nrocking chair, rocker\nrotisserie\nrubber eraser, rubber, pencil eraser\nrugby ball\nrule, ruler\nrunning shoe\nsafe\nsafety pin\nsaltshaker, salt shaker\nsandal\nsarong\nsax, saxophone\nscabbard\nscale, weighing machine\nschool bus\nschooner\nscoreboard\nscreen, CRT screen\nscrew\nscrewdriver\nseat belt, seatbelt\nsewing machine\nshield, buckler\nshoe shop, shoe-shop, shoe store\nshoji\nshopping basket\nshopping cart\nshovel\nshower cap\nshower curtain\nski\nski mask\nsleeping bag\nslide rule, slipstick\nsliding door\nslot, one-armed bandit\nsnorkel\nsnowmobile\nsnowplow, snowplough\nsoap dispenser\nsoccer ball\nsock\nsolar dish, solar collector, solar furnace\nsombrero\nsoup bowl\nspace bar\nspace heater\nspace shuttle\nspatula\nspeedboat\nspider web, spider\'s web\nspindle\nsports car, sport car\nspotlight, spot\nstage\nsteam locomotive\nsteel arch bridge\nsteel drum\nstethoscope\nstole\nstone wall\nstopwatch, stop watch\nstove\nstrainer\nstreetcar, tram, tramcar, trolley, trolley car\nstretcher\nstudio couch, day bed\nstupa, tope\nsubmarine, pigboat, sub, U-boat\nsuit, suit of clothes\nsundial\nsunglass\nsunglasses, dark glasses, shades\nsunscreen, sunblock, sun blocker\nsuspension bridge\nswab, swob, mop\nsweatshirt\nswimming trunks, bathing trunks\nswing\nswitch, electric switch, electrical switch\nsyringe\ntable lamp\ntank, army tank, armored combat vehicle, armoured combat vehicle\ntape player\nteapot\nteddy, teddy bear\ntelevision, television system\ntennis ball\nthatch, thatched roof\ntheater curtain, theatre curtain\nthimble\nthresher, thrasher, threshing machine\nthrone\ntile roof\ntoaster\ntobacco shop, tobacconist shop, tobacconist\ntoilet seat\ntorch\ntotem pole\ntow truck, tow car, wrecker\ntoyshop\ntractor\ntrailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi\ntray\ntrench coat\ntricycle, trike, velocipede\ntrimaran\ntripod\ntriumphal arch\ntrolleybus, trolley coach, trackless trolley\ntrombone\ntub, vat\nturnstile\ntypewriter keyboard\numbrella\nunicycle, monocycle\nupright, upright piano\nvacuum, vacuum cleaner\nvase\nvault\nvelvet\nvending machine\nvestment\nviaduct\nviolin, fiddle\nvolleyball\nwaffle iron\nwall clock\nwallet, billfold, notecase, pocketbook\nwardrobe, closet, press\nwarplane, military plane\nwashbasin, handbasin, washbowl, lavabo, wash-hand basin\nwasher, automatic washer, washing machine\nwater bottle\nwater jug\nwater tower\nwhiskey jug\nwhistle\nwig\nwindow screen\nwindow shade\nWindsor tie\nwine bottle\nwing\nwok\nwooden spoon\nwool, woolen, woollen\nworm fence, snake fence, snake-rail fence, Virginia fence\nwreck\nyawl\nyurt\nweb site, website, internet site, site\ncomic book\ncrossword puzzle, crossword\nstreet sign\ntraffic light, traffic signal, stoplight\nbook jacket, dust cover, dust jacket, dust wrapper\nmenu\nplate\nguacamole\nconsomme\nhot pot, hotpot\ntrifle\nice cream, icecream\nice lolly, lolly, lollipop, popsicle\nFrench loaf\nbagel, beigel\npretzel\ncheeseburger\nhotdog, hot dog, red hot\nmashed potato\nhead cabbage\nbroccoli\ncauliflower\nzucchini, courgette\nspaghetti squash\nacorn squash\nbutternut squash\ncucumber, cuke\nartichoke, globe artichoke\nbell pepper\ncardoon\nmushroom\nGranny Smith\nstrawberry\norange\nlemon\nfig\npineapple, ananas\nbanana\njackfruit, jak, jack\ncustard apple\npomegranate\nhay\ncarbonara\nchocolate sauce, chocolate syrup\ndough\nmeat loaf, meatloaf\npizza, pizza pie\npotpie\nburrito\nred wine\nespresso\ncup\neggnog\nalp\nbubble\ncliff, drop, drop-off\ncoral reef\ngeyser\nlakeside, lakeshore\npromontory, headland, head, foreland\nsandbar, sand bar\nseashore, coast, seacoast, sea-coast\nvalley, vale\nvolcano\nballplayer, baseball player\ngroom, bridegroom\nscuba diver\nrapeseed\ndaisy\nyellow lady\'s slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\ncorn\nacorn\nhip, rose hip, rosehip\nbuckeye, horse chestnut, conker\ncoral fungus\nagaric\ngyromitra\nstinkhorn, carrion fungus\nearthstar\nhen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa\nbolete\near, spike, capitulum\ntoilet tissue, toilet paper, bathroom tissue\'\'\'.split(""\\n"")'"
ch12_rank/vgg16.py,107,"b'########################################################################################\n# Davi Frossard, 2016                                                                  #\n# VGG16 implementation in TensorFlow                                                   #\n# Details:                                                                             #\n# http://www.cs.toronto.edu/~frossard/post/vgg16/                                      #\n#                                                                                      #\n# Model from https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md     #\n# Weights from Caffe converted using https://github.com/ethereon/caffe-tensorflow      #\n########################################################################################\n\nimport tensorflow as tf\nimport numpy as np\nfrom scipy.misc import imread, imresize\nfrom imagenet_classes import class_names\n\n\nclass vgg16:\n    def __init__(self, imgs, weights=None, sess=None):\n        self.imgs = imgs\n        tf.summary.image(""imgs"", self.imgs)\n        self.convlayers()\n        self.fc_layers()\n        self.probs = tf.nn.softmax(self.fc3l)\n        if weights is not None and sess is not None:\n            self.load_weights(weights, sess)\n        self.my_summaries = tf.summary.merge_all()\n        self.my_writer = tf.summary.FileWriter(\'tb_files\', sess.graph)\n\n    def convlayers(self):\n        self.parameters = []\n\n        # zero-mean input\n        with tf.name_scope(\'preprocess\') as scope:\n            mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 1, 3], name=\'img_mean\')\n            images = self.imgs-mean\n\n        # conv1_1\n        with tf.name_scope(\'conv1_1\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32,\n                                                     stddev=1e-1), name=\'weights\')\n            conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n                                 trainable=True, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv1_1 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # conv1_2\n        with tf.name_scope(\'conv1_2\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 64], dtype=tf.float32,\n                                                     stddev=1e-1), name=\'weights\')\n            conv = tf.nn.conv2d(self.conv1_1, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n                                 trainable=True, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv1_2 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # pool1\n        self.pool1 = tf.nn.max_pool(self.conv1_2,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=\'SAME\',\n                               name=\'pool1\')\n\n        # conv2_1\n        with tf.name_scope(\'conv2_1\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,\n                                                     stddev=1e-1), name=\'weights\')\n            conv = tf.nn.conv2d(self.pool1, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n                                 trainable=True, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv2_1 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # conv2_2\n        with tf.name_scope(\'conv2_2\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 128], dtype=tf.float32,\n                                                     stddev=1e-1), name=\'weights\')\n            conv = tf.nn.conv2d(self.conv2_1, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n                                 trainable=True, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv2_2 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # pool2\n        self.pool2 = tf.nn.max_pool(self.conv2_2,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=\'SAME\',\n                               name=\'pool2\')\n\n        # conv3_1\n        with tf.name_scope(\'conv3_1\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32,\n                                                     stddev=1e-1), name=\'weights\')\n            conv = tf.nn.conv2d(self.pool2, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n                                 trainable=True, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv3_1 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # conv3_2\n        with tf.name_scope(\'conv3_2\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n                                                     stddev=1e-1), name=\'weights\')\n            conv = tf.nn.conv2d(self.conv3_1, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n                                 trainable=True, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv3_2 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # conv3_3\n        with tf.name_scope(\'conv3_3\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n                                                     stddev=1e-1), name=\'weights\')\n            conv = tf.nn.conv2d(self.conv3_2, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n                                 trainable=True, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv3_3 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # pool3\n        self.pool3 = tf.nn.max_pool(self.conv3_3,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=\'SAME\',\n                               name=\'pool3\')\n\n        # conv4_1\n        with tf.name_scope(\'conv4_1\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32,\n                                                     stddev=1e-1), name=\'weights\')\n            conv = tf.nn.conv2d(self.pool3, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                                 trainable=True, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv4_1 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # conv4_2\n        with tf.name_scope(\'conv4_2\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n                                                     stddev=1e-1), name=\'weights\')\n            conv = tf.nn.conv2d(self.conv4_1, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                                 trainable=True, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv4_2 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # conv4_3\n        with tf.name_scope(\'conv4_3\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n                                                     stddev=1e-1), name=\'weights\')\n            conv = tf.nn.conv2d(self.conv4_2, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                                 trainable=True, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv4_3 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # pool4\n        self.pool4 = tf.nn.max_pool(self.conv4_3,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=\'SAME\',\n                               name=\'pool4\')\n\n        # conv5_1\n        with tf.name_scope(\'conv5_1\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n                                                     stddev=1e-1), name=\'weights\')\n            conv = tf.nn.conv2d(self.pool4, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                                 trainable=True, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv5_1 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # conv5_2\n        with tf.name_scope(\'conv5_2\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n                                                     stddev=1e-1), name=\'weights\')\n            conv = tf.nn.conv2d(self.conv5_1, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                                 trainable=True, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv5_2 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # conv5_3\n        with tf.name_scope(\'conv5_3\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n                                                     stddev=1e-1), name=\'weights\')\n            conv = tf.nn.conv2d(self.conv5_2, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                                 trainable=True, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv5_3 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # pool5\n        self.pool5 = tf.nn.max_pool(self.conv5_3,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=\'SAME\',\n                               name=\'pool4\')\n\n    def fc_layers(self):\n        # fc1\n        with tf.name_scope(\'fc1\') as scope:\n            shape = int(np.prod(self.pool5.get_shape()[1:]))\n            fc1w = tf.Variable(tf.truncated_normal([shape, 4096],\n                                                         dtype=tf.float32,\n                                                         stddev=1e-1), name=\'weights\')\n            fc1b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n                                 trainable=True, name=\'biases\')\n            pool5_flat = tf.reshape(self.pool5, [-1, shape])\n            fc1l = tf.nn.bias_add(tf.matmul(pool5_flat, fc1w), fc1b)\n            self.fc1 = tf.nn.relu(fc1l)\n            self.parameters += [fc1w, fc1b]\n\n        # fc2\n        with tf.name_scope(\'fc2\') as scope:\n            fc2w = tf.Variable(tf.truncated_normal([4096, 4096],\n                                                         dtype=tf.float32,\n                                                         stddev=1e-1), name=\'weights\')\n            fc2b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n                                 trainable=True, name=\'biases\')\n            fc2l = tf.nn.bias_add(tf.matmul(self.fc1, fc2w), fc2b)\n            self.fc2 = tf.nn.relu(fc2l)\n            self.parameters += [fc2w, fc2b]\n\n        # fc3\n        with tf.name_scope(\'fc3\') as scope:\n            fc3w = tf.Variable(tf.truncated_normal([4096, 1000],\n                                                         dtype=tf.float32,\n                                                         stddev=1e-1), name=\'weights\')\n            fc3b = tf.Variable(tf.constant(1.0, shape=[1000], dtype=tf.float32),\n                                 trainable=True, name=\'biases\')\n            self.fc3l = tf.nn.bias_add(tf.matmul(self.fc2, fc3w), fc3b)\n            self.parameters += [fc3w, fc3b]\n\n    def load_weights(self, weight_file, sess):\n        weights = np.load(weight_file)\n        keys = sorted(weights.keys())\n        for i, k in enumerate(keys):\n            print(i, k, np.shape(weights[k]))\n            sess.run(self.parameters[i].assign(weights[k]))\n'"
ch11_seq2seq/data/process_input.py,0,"b""import re\n\nNUM_CHARS = 30\n\nwith open('words_input.txt', 'r', encoding='ISO-8859-1') as fi_i, open('words_output.txt', 'r', encoding='ISO-8859-1') as fi_o, open('input_sentences.txt', 'w') as fo_i, open('output_sentences.txt', 'w') as fo_o:\n    fi_i_fixed = fi_i.read().encode('ascii', 'ignore').decode('UTF-8').lower()\n    fi_i_alpha = re.sub('[^a-z\\n]+', ' ', fi_i_fixed)\n    fi_o_fixed = fi_o.read().encode('ascii', 'ignore').decode('UTF-8').lower()\n    fi_o_alpha = re.sub('[^a-z\\n]+', ' ', fi_o_fixed)\n\n    for line in fi_i_alpha.split('\\n'):\n        fo_i.write(line[:NUM_CHARS] + '\\n')\n    for line in fi_o_alpha.split('\\n'):\n        fo_o.write(line[:NUM_CHARS] + '\\n')\n"""
