file_path,api_count,code
setup.py,0,"b'# Always prefer setuptools over distutils\nfrom setuptools import setup\n\n# To use a consistent encoding\nfrom codecs import open\nfrom os import path\n\nhere = path.abspath(path.dirname(__file__))\n\n# Get the long description from the README file\nwith open(path.join(here, ""README.md""), encoding=""utf-8"") as f:\n    long_description = f.read()\n\nsetup(\n    name=""tfpyth"",\n    # Versions should comply with PEP440.  For a discussion on single-sourcing\n    # the version across setup.py and the project code, see\n    # https://packaging.python.org/en/latest/single_source_version.html\n    version=""1.0.1"",\n    description=""Putting TensorFlow back in PyTorch, back in Tensorflow (differentiable TensorFlow PyTorch adapters)."",\n    # Fix windows newlines.\n    long_description=long_description.replace(""\\r\\n"", ""\\n""),\n    long_description_content_type=""text/markdown"",\n    # The project\'s main homepage.\n    url=""https://github.com/blackhc/tfpyth"",\n    # Author details\n    author=""Andreas @blackhc Kirsch"",\n    author_email=""blackhc+tfpyth@gmail.com"",\n    # Choose your license\n    license=""MIT"",\n    # See https://pypi.python.org/pypi?%3Aaction=list_classifiers\n    classifiers=[\n        # How mature is this project? Common values are\n        #   3 - Alpha\n        #   4 - Beta\n        #   5 - Production/Stable\n        ""Development Status :: 4 - Beta"",\n        # Indicate who your project is intended for\n        ""Intended Audience :: Developers"",\n        ""Intended Audience :: Science/Research"",\n        ""Topic :: Software Development :: Libraries :: Python Modules"",\n        # Pick your license as you wish (should match ""license"" above)\n        ""License :: OSI Approved :: MIT License"",\n        ""Programming Language :: Python :: 3.6"",\n    ],\n    # What does your project relate to?\n    keywords=""ml machine learning"",\n    # You can just specify the packages manually here if your project is\n    # simple. Or you can use find_packages().\n    packages=[""tfpyth""],\n    #package_dir={"""": """"},\n    # List run-time dependencies here.  These will be installed by pip when\n    # your project is installed. For an analysis of ""install_requires"" vs pip\'s\n    # requirements files see:\n    # https://packaging.python.org/en/latest/requirements.html\n    install_requires=[""tensorflow~=1.14"", ""torch~=1.1""],\n    # List additional groups of dependencies here (e.g. development\n    # dependencies). You can install these using the following syntax,\n    # for example:\n    # $ pip install -e .[dev,test]\n    extras_require={""dev"": [""check-manifest""], ""test"": [""coverage"", ""codecov"", ""pytest"", ""pytest-cov""]},\n    setup_requires=[""pytest-runner""],\n)\n'"
examples/torch_from_tensorflow.py,3,"b'import tensorflow as tf\nimport torch as th\nimport numpy as np\nimport tfpyth\n\nsession = tf.Session()\n\n\ndef get_torch_function():\n    a = tf.placeholder(tf.float32, name=""a"")\n    b = tf.placeholder(tf.float32, name=""b"")\n    c = 3 * a + 4 * b * b\n\n    f = tfpyth.torch_from_tensorflow(session, [a, b], c).apply\n    return f\n\n\nf = get_torch_function()\na = th.tensor(1, dtype=th.float32, requires_grad=True)\nb = th.tensor(3, dtype=th.float32, requires_grad=True)\nx = f(a, b)\n\nassert x == 39.0\n\nx.backward()\n\nassert np.allclose((a.grad, b.grad), (3.0, 24.0))\n'"
tests/test_adapters.py,13,"b'import tensorflow as tf\nimport torch as th\nimport numpy as np\nimport tfpyth\n\n\ndef test_pytorch_in_tensorflow_eager_mode():\n    tf.enable_eager_execution()\n    tfe = tf.contrib.eager\n\n    def pytorch_expr(a, b):\n        return 3 * a + 4 * b * b\n\n    x = tfpyth.eager_tensorflow_from_torch(pytorch_expr)\n\n    assert tf.math.equal(x(tf.convert_to_tensor(1.0), tf.convert_to_tensor(3.0)), 39.0)\n\n    dx = tfe.gradients_function(x)\n    assert all(tf.math.equal(dx(tf.convert_to_tensor(1.0), tf.convert_to_tensor(3.0)), [3.0, 24.0]))\n    tf.disable_eager_execution()\n\n\ndef test_pytorch_in_tensorflow_graph_mode():\n    session = tf.Session()\n\n    def pytorch_expr(a, b):\n        return 3 * a + 4 * b * b\n\n    a = tf.placeholder(tf.float32, name=""a"")\n    b = tf.placeholder(tf.float32, name=""b"")\n    c = tfpyth.tensorflow_from_torch(pytorch_expr, [a, b], tf.float32)\n    c_grad = tf.gradients([c], [a, b], unconnected_gradients=""zero"")\n\n    assert np.allclose(session.run([c, c_grad[0], c_grad[1]], {a: 1.0, b: 3.0}), [39.0, 3.0, 24.0])\n\n\ndef test_tensorflow_in_pytorch():\n    session = tf.Session()\n\n    def get_tf_function():\n        a = tf.placeholder(tf.float32, name=""a"")\n        b = tf.placeholder(tf.float32, name=""b"")\n        c = 3 * a + 4 * b * b\n\n        f = tfpyth.torch_from_tensorflow(session, [a, b], c).apply\n        return f\n\n    f = get_tf_function()\n    a_ = th.tensor(1, dtype=th.float32, requires_grad=True)\n    b_ = th.tensor(3, dtype=th.float32, requires_grad=True)\n    x = f(a_, b_)\n\n    assert x == 39.0\n\n    x.backward()\n\n    assert np.allclose((a_.grad, b_.grad), (3.0, 24.0))\n'"
tfpyth/__init__.py,8,"b'import tensorflow as tf\nimport torch as th\n\n\nclass TensorFlowFunction(th.autograd.Function):\n    """"""\n    Wrapper class for Tensorflow input/output nodes (incl gradient) in PyTorch.\n    """"""\n\n    inputs: list = None\n    output: tf.Tensor = None\n    gradient_placeholder = None\n    gradient_outputs = None\n\n\ndef torch_from_tensorflow(tf_session, tf_inputs, tf_output, tf_dtype=tf.float32):\n    """"""\n    Create a PyTorch TensorFlowFunction with forward and backward methods which executes evaluates the passed\n    TensorFlow tensors.\n\n    ```python\n    my_tensorflow_func = MyTensorFlowFunction.apply\n\n    result = my_tensorflow_func(th_a, th_b)\n    ```\n\n    :param tf_session: TensorFlow session to use\n    :param tf_inputs: TensorFlow input tensors/placeholders\n    :param tf_output: TensorFlow output tensor\n    :param tf_dtype: dtype to use for gradient placeholder.\n    :return: TensorflowFunction which can be applied to PyTorch tensors.\n    """"""\n    # create gradient placeholders\n    tf_gradient_placeholder = tf.placeholder(dtype=tf_dtype, name=f""gradient"")\n    tf_gradient_outputs = tf.gradients(\n        ys=tf_output, xs=tf_inputs, grad_ys=[tf_gradient_placeholder], unconnected_gradients=""zero""\n    )\n\n    class _TensorFlowFunction(TensorFlowFunction):\n        inputs = tf_inputs\n        output = tf_output\n        gradient_placeholder = tf_gradient_placeholder\n        gradient_outputs = tf_gradient_outputs\n\n        @staticmethod\n        def forward(ctx, *args):\n            assert len(args) == len(tf_inputs)\n\n            feed_dict = {tf_input: th_input.detach().numpy() for tf_input, th_input in zip(tf_inputs, args)}\n            output = tf_session.run(tf_output, feed_dict)\n\n            ctx.save_for_backward(*args)\n\n            th_output = th.as_tensor(output)\n            return th_output\n\n        # See https://www.janfreyberg.com/blog/2019-04-01-testing-pytorch-functions/ for why ""no cover""\n        @staticmethod\n        def backward(ctx, grad_output): # pragma: no cover\n            th_inputs = ctx.saved_tensors\n\n            feed_dict = {}\n            feed_dict.update({tf_input: th_input.detach().numpy() for tf_input, th_input in zip(tf_inputs, th_inputs)})\n            feed_dict.update({tf_gradient_placeholder: grad_output.detach().numpy()})\n\n            tf_gradients = tf_session.run(tf_gradient_outputs, feed_dict)\n            return tuple(th.as_tensor(tf_gradient) for tf_gradient in tf_gradients)\n\n    return _TensorFlowFunction()\n\n\ndef eager_tensorflow_from_torch(func):\n    """"""\n    Wraps a PyTorch function into a TensorFlow eager-mode function (ie can be executed within Tensorflow eager-mode).\n\n    :param func: Function that takes PyTorch tensors and returns a PyTorch tensor.\n    :return: Differentiable Tensorflow eager-mode function.\n    """"""\n\n    @tf.custom_gradient\n    def compute(*inputs):\n        th_inputs = [th.tensor(tf_input.numpy(), requires_grad=True) for tf_input in inputs]\n        th_output = func(*th_inputs)\n\n        def compute_grad(d_output):\n            th_d_output = th.tensor(d_output.numpy(), requires_grad=False)\n            th_gradients = th.autograd.grad([th_output], th_inputs, grad_outputs=[th_d_output], allow_unused=True)\n            tf_gradients = [tf.convert_to_tensor(th_gradient.numpy()) for th_gradient in th_gradients]\n            return tf_gradients\n\n        return tf.convert_to_tensor(th_output.detach().numpy()), compute_grad\n\n    return compute\n\n\ndef tensorflow_from_torch(func, inp, Tout, name=None):\n    """"""\n    Executes a PyTorch function into a TensorFlow op and output tensor (ie can be evaluated within Tensorflow).\\\n\n    :param func: Function that takes PyTorch tensors and returns a PyTorch tensor.\n    :param inp: TensorFlow input tensors\n    :param Tout: TensorFlow output dtype\n    :param name: Name of the output tensor\n    :return: Differentiable Tensorflow output tensor.\n    """"""\n    eager_compute = eager_tensorflow_from_torch(func)\n\n    return tf.py_function(eager_compute, inp, Tout, name=name)\n'"
