file_path,api_count,code
models/freeze_graph.py,0,"b'# EDITED on 10. 9. 2017 for meta graph freezing\n#\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#           http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Converts checkpoint variables into Const ops in a standalone GraphDef file.\n\nThis script is designed to take a GraphDef proto, a SaverDef proto, and a set of\nvariable values stored in a checkpoint file, and output a GraphDef with all of\nthe variable ops converted into const ops containing the values of the\nvariables.\n\nIt\'s useful to do this when we need to load a single file in C++, especially in\nenvironments like mobile or embedded where we may not have access to the\nRestoreTensor ops and file loading calls that they rely on.\n\nAn example of command-line usage is:\nbazel build tensorflow/python/tools:freeze_graph && \\\nbazel-bin/tensorflow/python/tools/freeze_graph \\\n--input_graph=some_graph_def.pb \\\n--input_checkpoint=model.ckpt-8361242 \\\n--output_graph=/tmp/frozen_graph.pb --output_node_names=softmax\n\nYou can also look at freeze_graph_test.py for an example of how to use it.\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\n\nfrom google.protobuf import text_format\n\nfrom tensorflow.contrib.saved_model.python.saved_model import reader\nfrom tensorflow.core.framework import graph_pb2\nfrom tensorflow.core.protobuf import saver_pb2\nfrom tensorflow.core.protobuf.meta_graph_pb2 import MetaGraphDef\nfrom tensorflow.python import pywrap_tensorflow\nfrom tensorflow.python.client import session\nfrom tensorflow.python.framework import graph_util\nfrom tensorflow.python.framework import importer\nfrom tensorflow.python.platform import app\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.python.saved_model import loader\nfrom tensorflow.python.saved_model import tag_constants\nfrom tensorflow.python.training import saver as saver_lib\n\nFLAGS = None\n\n\ndef freeze_graph_with_def_protos(input_graph_def,\n                                 input_saver_def,\n                                 input_checkpoint,\n                                 output_node_names,\n                                 restore_op_name,\n                                 filename_tensor_name,\n                                 output_graph,\n                                 clear_devices,\n                                 initializer_nodes,\n                                 variable_names_blacklist="""",\n                                 input_meta_graph_def=None,\n                                 input_saved_model_dir=None,\n                                 saved_model_tags=None):\n    """"""Converts all variables in a graph and checkpoint into constants.""""""\n    del restore_op_name, filename_tensor_name  # Unused by updated loading code.\n\n    # \'input_checkpoint\' may be a prefix if we\'re using Saver V2 format\n    if (not input_saved_model_dir and\n            not saver_lib.checkpoint_exists(input_checkpoint)):\n        print(""Input checkpoint \'"" + input_checkpoint + ""\' doesn\'t exist!"")\n        return -1\n\n    if not output_node_names:\n        print(""You need to supply the name of a node to --output_node_names."")\n        return -1\n\n    # Remove all the explicit device specifications for this node. This helps to\n    # make the graph more portable.\n    if clear_devices:\n        if input_meta_graph_def:\n            for node in input_meta_graph_def.graph_def.node:\n                node.device = """"\n        elif input_graph_def:\n            for node in input_graph_def.node:\n                node.device = """"\n\n    if input_graph_def:\n        _ = importer.import_graph_def(input_graph_def, name="""")\n    with session.Session() as sess:\n        if input_saver_def:\n            saver = saver_lib.Saver(saver_def=input_saver_def)\n            saver.restore(sess, input_checkpoint)\n        elif input_meta_graph_def:\n            restorer = saver_lib.import_meta_graph(\n                    input_meta_graph_def, clear_devices=True)\n            restorer.restore(sess, input_checkpoint)\n            if initializer_nodes:\n                sess.run(initializer_nodes.split("",""))\n        elif input_saved_model_dir:\n            if saved_model_tags is None:\n                saved_model_tags = []\n            loader.load(sess, saved_model_tags, input_saved_model_dir)\n        else:\n            var_list = {}\n            reader = pywrap_tensorflow.NewCheckpointReader(input_checkpoint)\n            var_to_shape_map = reader.get_variable_to_shape_map()\n            for key in var_to_shape_map:\n                try:\n                    tensor = sess.graph.get_tensor_by_name(key + "":0"")\n                except KeyError:\n                    # This tensor doesn\'t exist in the graph (for example it\'s\n                    # \'global_step\' or a similar housekeeping element) so skip it.\n                    continue\n                var_list[key] = tensor\n            saver = saver_lib.Saver(var_list=var_list)\n            saver.restore(sess, input_checkpoint)\n            if initializer_nodes:\n                sess.run(initializer_nodes.split("",""))\n\n        variable_names_blacklist = (variable_names_blacklist.split("","")\n                                                                if variable_names_blacklist else None)\n\n        if input_meta_graph_def:\n            output_graph_def = graph_util.convert_variables_to_constants(\n                    sess,\n                    input_meta_graph_def.graph_def,\n                    output_node_names.split("",""),\n                    variable_names_blacklist=variable_names_blacklist)\n        else:\n            output_graph_def = graph_util.convert_variables_to_constants(\n                    sess,\n                    input_graph_def,\n                    output_node_names.split("",""),\n                    variable_names_blacklist=variable_names_blacklist)\n\n    # Write GraphDef to file if output path has been given.\n    if output_graph:\n        with gfile.GFile(output_graph, ""wb"") as f:\n            f.write(output_graph_def.SerializeToString())\n\n    return output_graph_def\n\n\ndef _parse_input_graph_proto(input_graph, input_binary):\n    """"""Parser input tensorflow graph into GraphDef proto.""""""\n    if not gfile.Exists(input_graph):\n        print(""Input graph file \'"" + input_graph + ""\' does not exist!"")\n        return -1\n    input_graph_def = graph_pb2.GraphDef()\n    mode = ""rb"" if input_binary else ""r""\n    with gfile.FastGFile(input_graph, mode) as f:\n        if input_binary:\n            input_graph_def.ParseFromString(f.read())\n        else:\n            text_format.Merge(f.read(), input_graph_def)\n    return input_graph_def\n\n\ndef _parse_input_meta_graph_proto(input_graph, input_binary):\n    """"""Parser input tensorflow graph into MetaGraphDef proto.""""""\n    if not gfile.Exists(input_graph):\n        print(""Input meta graph file \'"" + input_graph + ""\' does not exist!"")\n        return -1\n    input_meta_graph_def = MetaGraphDef()\n    mode = ""rb"" if input_binary else ""r""\n    with gfile.FastGFile(input_graph, mode) as f:\n        if input_binary:\n            input_meta_graph_def.ParseFromString(f.read())\n        else:\n            text_format.Merge(f.read(), input_meta_graph_def)\n    print(""Loaded meta graph file \'"" + input_graph)\n    return input_meta_graph_def\n\n\ndef _parse_input_saver_proto(input_saver, input_binary):\n        """"""Parser input tensorflow Saver into SaverDef proto.""""""\n        if not gfile.Exists(input_saver):\n                print(""Input saver file \'"" + input_saver + ""\' does not exist!"")\n                return -1\n        mode = ""rb"" if input_binary else ""r""\n        with gfile.FastGFile(input_saver, mode) as f:\n                saver_def = saver_pb2.SaverDef()\n        if input_binary:\n                saver_def.ParseFromString(f.read())\n        else:\n                text_format.Merge(f.read(), saver_def)\n        return saver_def\n    \n\ndef get_meta_graph_def(saved_model_dir, tag_set):\n    """"""Gets MetaGraphDef from SavedModel.\n\n    Returns the MetaGraphDef for the given tag-set and SavedModel directory.\n\n    Args:\n        saved_model_dir: Directory containing the SavedModel to inspect or execute.\n        tag_set: Group of tag(s) of the MetaGraphDef to load, in string format,\n                separated by \',\'. For tag-set contains multiple tags, all tags must be\n                passed in.\n\n    Raises:\n        RuntimeError: An error when the given tag-set does not exist in the\n        SavedModel.\n\n    Returns:\n        A MetaGraphDef corresponding to the tag-set.\n    """"""\n    saved_model = reader.read_saved_model(saved_model_dir)\n    set_of_tags = set(tag_set.split(\',\'))\n    for meta_graph_def in saved_model.meta_graphs:\n        if set(meta_graph_def.meta_info_def.tags) == set_of_tags:\n            return meta_graph_def\n\n    raise RuntimeError(\'MetaGraphDef associated with tag-set \' + tag_set +\n                       \' could not be found in SavedModel\')\n\n\ndef freeze_graph(input_graph,\n                 input_saver,\n                 input_binary,\n                 input_checkpoint,\n                 output_node_names,\n                 restore_op_name,\n                 filename_tensor_name,\n                 output_graph,\n                 clear_devices,\n                 initializer_nodes,\n                 variable_names_blacklist="""",\n                 input_meta_graph=None,\n                 input_saved_model_dir=None,\n                 saved_model_tags=tag_constants.SERVING):\n        """"""Converts all variables in a graph and checkpoint into constants.""""""\n        input_graph_def = None\n        if input_saved_model_dir:\n                input_graph_def = get_meta_graph_def(\n                        input_saved_model_dir, saved_model_tags).graph_def\n        elif input_graph:\n                input_graph_def = _parse_input_graph_proto(input_graph, input_binary)\n        input_meta_graph_def = None\n        if input_meta_graph:\n                input_meta_graph_def = _parse_input_meta_graph_proto(\n                        input_meta_graph, input_binary)\n        input_saver_def = None\n        if input_saver:\n                input_saver_def = _parse_input_saver_proto(input_saver, input_binary)\n        freeze_graph_with_def_protos(\n                input_graph_def, input_saver_def, input_checkpoint, output_node_names,\n                restore_op_name, filename_tensor_name, output_graph, clear_devices,\n                initializer_nodes, variable_names_blacklist, input_meta_graph_def,\n                input_saved_model_dir, saved_model_tags.split("",""))\n'"
models/graph_optimizer.py,1,"b'""""""\nUsage:\npython graph_optimizer.py \\\n--tf_path ../../tensorflow/ \\\n--model_folder ""path_to_the_model_folder"" \\\n--output_names ""activation, accuracy"" \\\n--input_names ""x""\n""""""\n\nimport os, argparse\nfrom subprocess import call\n\nimport freeze_graph\nimport tensorflow as tf\n\ndir = os.path.dirname(os.path.realpath(__file__))\n\nfr_name = ""_frozen.pb""\nop_name = ""_optimized.pb""\n\n\ndef graph_freez(model_folder, output_names):\n    print(""Model folder"", model_folder)\n    checkpoint = tf.train.get_checkpoint_state(model_folder)\n    print(checkpoint)\n    checkpoint_path = checkpoint.model_checkpoint_path\n    output_graph_filename = checkpoint_path + fr_name\n\n    input_saver_def_path = """"\n    input_binary = True\n    output_node_names = output_names\n    restore_op_name = ""save/restore_all""\n    filename_tensor_name = ""save/Const:0""\n    clear_devices = False\n    input_meta_graph = checkpoint_path + "".meta""\n\n    freeze_graph.freeze_graph(\n        """", input_saver_def_path, input_binary, checkpoint_path,\n        output_node_names, restore_op_name, filename_tensor_name,\n        output_graph_filename, clear_devices, """", """", input_meta_graph)\n\n    return output_graph_filename\n\n\ndef graph_optimization(tf_path, graph_file, input_names, output_names):\n    output_file = graph_file[:-len(fr_name)] + op_name\n    tf_path += ""bazel-bin/tensorflow/tools/graph_transforms/transform_graph""\n\n    call([tf_path,\n          ""--in_graph="" + graph_file,\n          ""--out_graph="" + output_file,\n          ""--inputs="" + input_names,\n          ""--outputs="" + output_names,\n          """"""--transforms=\n          strip_unused_nodes(type=float, shape=""1,299,299,3"")\n          fold_constants(ignore_errors=true)\n          fold_batch_norms\n          fold_old_batch_norms""""""])\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(\n            ""Script freezes graph and optimize it for mobile usage"")\n    parser.add_argument(\n        ""--model"",\n        type=str,\n        help=""Path of folder + model name (folder_path/model_name)"")\n    parser.add_argument(\n        ""--input_names"",\n        type=str,\n        default="""",\n        help=""Input node names, comma separated."")\n    parser.add_argument(\n        ""--output_names"",\n        type=str,\n        default="""",\n        help=""Output node names, comma separated."")\n    parser.add_argument(\n        ""--tf_path"",\n        type=str,\n        default=""../../tensorflow/"",\n        help=""Path to the folder with tensorflow (requires bazel build of graph_transforms)"")\n\n    args = parser.parse_args()\n\n    graph = graph_freez(args.model, args.output_names)\n    graph_optimization(args.tf_path, graph, args.input_names, args.output_names)\n'"
src/__init__.py,0,b''
src/data/__init__.py,0,b''
src/data/create_csv.py,0,"b'import argparse\nimport csv\nimport glob\nimport os\nimport sys\n\nimport cv2\nimport numpy as np\nimport simplejson\n\nlocation = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.join(location, \'../\'))\nfrom ocr.viz import print_progress_bar\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    \'--sets\',\n    default=os.path.join(location, \'../../data/sets/\'),\n    help=""Folder with sets for converting to CSV."")\n\n\ndef create_csv(datadir):\n    print(\'Converting word images to CSV...\')\n    img_paths = {\n        \'train\': glob.glob(os.path.join(datadir, \'train\', \'*.png\')),\n        \'dev\': glob.glob(os.path.join(datadir, \'dev\', \'*.png\')),\n        \'test\': glob.glob(os.path.join(datadir, \'test\', \'*.png\'))}\n    \n    for split in [\'train\', \'dev\', \'test\']:\n        labels = np.array([\n            os.path.basename(name).split(\'_\')[0] for name in img_paths[split]])\n        length = len(img_paths[split])\n        images = np.empty(length, dtype=object)\n\n        for i, img in enumerate(img_paths[split]):\n            gaplines = \'None\'\n            if os.path.isfile(img[:-3] + \'txt\'):\n                with open(img[:-3] + \'txt\', \'r\') as fp:\n                    gaplines = str(simplejson.load(fp))[1:-1]\n            images[i] = (cv2.imread(img, 0), gaplines)\n            print_progress_bar(i, length)\n\n        with open(os.path.join(datadir, split + \'.csv\'), \'w\') as csvfile:\n            fieldnames = [\'label\', \'shape\', \'image\', \'gaplines\']\n            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n            writer.writeheader()\n            for i in range(length):\n                writer.writerow({\n                    fieldnames[0]: labels[i],\n                    fieldnames[1]: str(images[i][0].shape)[1:-1],\n                    fieldnames[2]: str(list(images[i][0].flatten()))[1:-1],\n                    fieldnames[3]: images[i][1]\n                })\n\n    print(\'\\tCSV files created!\')\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    create_csv(args.sets)\n'"
src/data/data_create_sets.py,0,"b'import argparse\nimport glob\nimport os\nimport random\nimport sys\nfrom shutil import copyfile\n\nimport cv2\nimport numpy as np\n\nlocation = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.join(location, \'../\'))\n\nfrom create_csv import create_csv\nfrom data_extractor import datasets\nfrom ocr.viz import print_progress_bar\n\n\nrandom.seed(17)  # Make the datasets split random, but reproducible\ndata_folder = \'words_final\'\noutput_folder = os.path.join(location, \'../../data/sets/\')\n\n# Sets percent distribution\ntest_set = 0.1\nvalidation_set = 0.1\n\n\nparser = argparse.ArgumentParser(\n    description=\'Script sliting processed words into train, validation and test sets.\')\nparser.add_argument(\n    \'-d\', \'--dataset\',\n    nargs=\'*\',\n    choices=datasets.keys(),\n    help=\'Pick dataset(s) to be used.\')\nparser.add_argument(\n    \'-p\', \'--path\',\n    nargs=\'*\',\n    default=[],\n    help=""""""Path to folder containing the dataset. For multiple datasets\n    provide path or \'\'. If not set, default paths will be used."""""")\nparser.add_argument(\n    \'--output\',\n    default=\'data-handwriting/sets\',\n    help=""Directory for normalized and split data"")\nparser.add_argument(\n    \'--csv\',\n    action=\'store_true\',\n    default=False,\n    help=""Include flag if you want to create csv files along with split."")\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    if args.dataset == [\'all\']:\n        args.dataset = list(datasets.keys())[:-1]\n\n    assert args.path == [] or len(args.dataset) == len(args.path), \\\n        ""provide same number of paths as datasets (use \'\' for default)""\n    if args.path != []:\n        for ds, path in zip(args.dataset, args.path):\n            datasets[ds][1] = path\n\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    imgs = []\n    for ds in args.dataset:\n        for loc, _, _ in os.walk(datasets[ds][1].replace(""raw"", ""processed"")):\n            imgs += glob.glob(os.path.join(loc, \'*.png\'))\n\n    imgs.sort()\n    random.shuffle(imgs)\n    \n    length = len(imgs)\n    sp1 = int((1 - test_set - validation_set) * length)\n    sp2 = int((1 - test_set) * length)\n    img_paths = {\'train\': imgs[:sp1], \'dev\': imgs[sp1:sp2], \'test\': imgs[sp2:]}\n    \n    i = 0\n    for split in [\'train\', \'dev\', \'test\']:\n        split_output = os.path.join(output_folder, split)\n        if not os.path.exists(split_output):\n            os.mkdir(split_output)\n        for im_path in img_paths[split]:\n            copyfile(im_path, os.path.join(split_output, os.path.basename(im_path)))\n            if \'_gaplines\' in im_path:\n                im_path = im_path[:-3] + \'txt\' \n                copyfile(\n                    im_path, os.path.join(split_output, os.path.basename(im_path)))\n\n            print_progress_bar(i, length)\n            i += 1\n\n        print(\n            ""\\n\\tNumber of %s words: %s"" % (split, len(os.listdir(split_output))))\n\n    if args.csv:\n        create_csv(output_folder)\n'"
src/data/data_extractor.py,0,"b'import argparse\nimport os\n\nfrom datasets import breta, camb, cvl, iam, orand\n\nlocation = os.path.dirname(os.path.abspath(__file__))\ndata_folder = os.path.join(location, \'../../data/raw/\')\ndatasets = {\n    \'breta\': [breta.extract, os.path.join(data_folder, \'breta\'), 1],\n    \'iam\': [iam.extract, os.path.join(data_folder, \'iam\'), 2],\n    \'cvl\': [cvl.extract, os.path.join(data_folder, \'cvl\'), 3],\n    \'orand\': [orand.extract, os.path.join(data_folder, \'orand\'), 4],\n    \'camb\': [camb.extract, os.path.join(data_folder, \'camb\'), 5],\n    \'all\': []}\n\noutput_folder = \'words_final\'\n\n\nparser = argparse.ArgumentParser(\n    description=\'Script extracting words from raw dataset.\')\nparser.add_argument(\n    \'-d\', \'--dataset\',\n    nargs=\'*\',\n    choices=datasets.keys(),\n    help=\'Pick dataset(s) to be used.\')\nparser.add_argument(\n    \'-p\', \'--path\',\n    nargs=\'*\',\n    default=[],\n    help=""""""Path to folder containing the dataset. For multiple datasets\n    provide path or \'\'. If not filled, default paths will be used."""""")\n\n    \nif __name__ == \'__main__\':\n    args = parser.parse_args() \n    if args.dataset == [\'all\']:\n        args.dataset = list(datasets.keys())[:-1]\n\n    assert args.path == [] or len(args.dataset) == len(args.path), \\\n        ""provide same number of paths as datasets (use \'\' for default)""\n    if args.path != []:\n        for ds, path in zip(args.dataset, args.path):\n            datasets[ds][1] = path\n    \n    for ds in args.dataset:\n        print(""Processing -"", ds)\n        entry = datasets[ds]\n        entry[0](entry[1], output_folder, entry[2])\n'"
src/data/data_normalization.py,0,"b'import argparse\nimport glob\nimport os\nimport sys\n\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nlocation = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.join(location, \'../\'))\n\nfrom data_extractor import datasets\nfrom ocr.normalization import word_normalization\nfrom ocr.viz import print_progress_bar\n\n\ndata_folder = \'words_final\'\noutput_folder = os.path.join(location, \'../../data/processed/\')\n\n\nparser = argparse.ArgumentParser(\n    description=\'Script normalizing words from datasts.\')\nparser.add_argument(\n    \'-d\', \'--dataset\',\n    nargs=\'*\',\n    choices=datasets.keys(),\n    help=\'Pick dataset(s) to be used.\')\nparser.add_argument(\n    \'-p\', \'--path\',\n    nargs=\'*\',\n    default=[],\n    help=""""""Path to folder containing the dataset. For multiple datasets\n    provide path or \'\'. If not set, default paths will be used."""""")\n\n\ndef words_norm(location, output):\n    output = os.path.join(location, output)\n    if os.path.exists(output):\n        print(""THIS DATASET IS BEING SKIPPED"")\n        print(""Output folder already exists:"", output)\n        return 1\n    else:\n        output = os.path.join(output, \'words_nolines\')\n        os.makedirs(output)\n        \n    imgs = glob.glob(os.path.join(location, data_folder, \'*.png\'))\n    length = len(imgs)\n\n    for i, img_path in enumerate(imgs):\n        image = cv2.imread(img_path)\n        # Simple check for invalid images\n        if image.shape[0] > 20:\n            cv2.imwrite(\n                os.path.join(output, os.path.basename(img_path)),\n                word_normalization(\n                    image,\n                    height=64,\n                    border=False,\n                    tilt=False,\n                    hyst_norm=False))\n        print_progress_bar(i, len(imgs))\n        \n    print(""\\tNumber of normalized words:"",\n          len([n for n in os.listdir(output)]))\n\n    \nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    if args.dataset == [\'all\']:\n        args.dataset = list(datasets.keys())[:-1]\n\n    assert args.path == [] or len(args.dataset) == len(args.path), \\\n        ""provide same number of paths as datasets (use \'\' for default)""\n    if args.path != []:\n        for ds, path in zip(args.dataset, args.path):\n            datasets[ds][1] = path\n    \n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n    \n    for ds in args.dataset:\n        print(""Processing -"", ds)\n        entry = datasets[ds]\n        words_norm(entry[1], os.path.join(output_folder, ds))\n'"
src/ocr/__init__.py,0,b''
src/ocr/characters.py,0,"b'# -*- coding: utf-8 -*-\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport cv2\nimport math\n\nfrom .helpers import *\nfrom .tfhelpers import Model\n\n# Preloading trained model with activation function\n# Loading is slow -> prevent multiple loads\nprint(""Loading segmentation models..."")\nlocation = os.path.dirname(os.path.abspath(__file__))\nCNN_model = Model(\n    os.path.join(location, \'../../models/gap-clas/CNN-CG\'))\nCNN_slider = (60, 30)\nRNN_model = Model(\n    os.path.join(location, \'../../models/gap-clas/RNN/Bi-RNN-new\'),\n    \'prediction\')\nRNN_slider = (60, 60)\n\n\ndef _classify(img, step=2, RNN=False, slider=(60, 60)):\n    """"""Slice the image and return raw output of classifier.""""""\n    length = (img.shape[1] - slider[1]) // 2 + 1\n    if RNN:\n        input_seq = np.zeros((1, length, slider[0]*slider[1]), dtype=np.float32)\n        input_seq[0][:] = [img[:, loc * step: loc * step + slider[1]].flatten()\n                           for loc in range(length)]\n        pred = RNN_model.eval_feed({\'inputs:0\': input_seq,\n                                    \'length:0\': [length],\n                                    \'keep_prob:0\': 1})[0]\n    else:\n        input_seq = np.zeros((length, slider[0]*slider[1]), dtype=np.float32)\n        input_seq[:] = [img[:, loc * step: loc * step + slider[1]].flatten()\n                        for loc in range(length)]\n        pred = CNN_model.run(input_seq)\n        \n    return pred\n    \n\ndef segment(img, step=2, RNN=False, debug=False):\n    """"""Take preprocessed image of word and\n    returns array of positions separating characters.\n    """"""\n    slider = CNN_slider\n    if RNN:\n        slider = RNN_slider\n    \n    # Run the classifier\n    pred = _classify(img, step=step, RNN=RNN, slider=slider)\n\n    # Finalize the gap positions from raw prediction\n    gaps = []\n    last_gap = 0\n    gap_count = 1\n    gap_position_sum = slider[1] / 2\n    first_gap = True\n    gap_block_first = 0\n    gap_block_last = slider[1] / 2\n\n    for i, p in enumerate(pred):\n        if p == 1:\n            gap_position_sum += i * step + slider[1] / 2\n            gap_block_last = i * step + slider[1] / 2\n            gap_count += 1\n            last_gap = 0\n            if gap_block_first == 0:\n                gap_block_first = i * step + slider[1] / 2\n        else:\n            if gap_count != 0 and last_gap >= 1:\n                if first_gap:\n                    gaps.append(int(gap_block_last))\n                    first_gap = False\n                else:\n                    gaps.append(int(gap_position_sum // gap_count))\n                gap_position_sum = 0\n                gap_count = 0\n            gap_block_first = 0\n            last_gap += 1\n\n    # Adding final gap position\n    if gap_block_first != 0:\n        gaps.append(int(gap_block_first))\n    else:\n        gap_position_sum += (len(pred) - 1) * 2 + slider[1]/2\n        gaps.append(int(gap_position_sum / (gap_count + 1)))\n        \n    if debug:\n        # Drawing lines\n        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n        for gap in gaps:\n            cv2.line(img,\n                     ((int)(gap), 0),\n                     ((int)(gap), slider[0]),\n                     (0, 255, 0), 1)\n        implt(img, t=""Separated characters"")\n        \n    return gaps'"
src/ocr/datahelpers.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nHelper functions for loading and creating datasets\n""""""\nimport numpy as np\nimport glob\nimport simplejson\nimport os\nimport cv2\nimport csv\nimport sys\nimport unidecode\n\nfrom .helpers import implt\nfrom .normalization import letter_normalization\nfrom .viz import print_progress_bar\n\n\nCHARS = [\'\', \'A\', \'B\', \'C\', \'D\', \'E\', \'F\', \'G\', \'H\', \'I\',\n         \'J\', \'K\', \'L\', \'M\', \'N\', \'O\', \'P\', \'Q\', \'R\', \'S\',\n         \'T\', \'U\', \'V\', \'W\', \'X\', \'Y\', \'Z\', \'a\', \'b\', \'c\',\n         \'d\', \'e\', \'f\', \'g\', \'h\', \'i\', \'j\', \'k\', \'l\', \'m\',\n         \'n\', \'o\', \'p\', \'q\', \'r\', \'s\', \'t\', \'u\', \'v\', \'w\',\n         \'x\', \'y\', \'z\', \'0\', \'1\', \'2\', \'3\', \'4\', \'5\', \'6\',\n         \'7\', \'8\', \'9\', \'.\', \'-\', \'+\', ""\'""]\nCHAR_SIZE = len(CHARS)\nidxs = [i for i in range(len(CHARS))]\nidx_2_chars = dict(zip(idxs, CHARS))\nchars_2_idx = dict(zip(CHARS, idxs))\n\ndef char2idx(c, sequence=False):\n    if sequence:\n        return chars_2_idx[c] + 1\n    return chars_2_idx[c]\n\ndef idx2char(idx, sequence=False):\n    if sequence:\n        return idx_2_chars[idx-1]\n    return idx_2_chars[idx]\n    \n\ndef load_words_data(dataloc=\'data/words/\', is_csv=False, load_gaplines=False):\n    """"""\n    Load word images with corresponding labels and gaplines (if load_gaplines == True).\n    Args:\n        dataloc: image folder location/CSV file - can be list of multiple locations\n        is_csv: using CSV files\n        load_gaplines: wheter or not load gaplines positions files\n    Returns:\n        (images, labels (, gaplines))\n    """"""\n    print(""Loading words..."")\n    if type(dataloc) is not list:\n        dataloc = [dataloc]\n\n    if is_csv:\n        csv.field_size_limit(sys.maxsize)\n        length = 0\n        for loc in dataloc:\n            with open(loc) as csvfile:\n                reader = csv.reader(csvfile)\n                length += max(sum(1 for row in csvfile)-1, 0)\n\n        labels = np.empty(length, dtype=object)\n        images = np.empty(length, dtype=object)\n        i = 0\n        for loc in dataloc:\n            print(loc)\n            with open(loc) as csvfile:\n                reader = csv.DictReader(csvfile)\n                for row in reader:\n                    shape = np.fromstring(\n                        row[\'shape\'],\n                        sep=\',\',\n                        dtype=int)\n                    img = np.fromstring(\n                        row[\'image\'],\n                        sep=\', \',\n                        dtype=np.uint8).reshape(shape)\n                    labels[i] = row[\'label\']\n                    images[i] = img\n                    \n                    print_progress_bar(i, length)\n                    i += 1\n    else:\n        img_list = []\n        tmp_labels = []\n        for loc in dataloc:\n            tmp_list = glob.glob(os.path.join(loc, \'*.png\'))\n            img_list += tmp_list\n            tmp_labels += [name[len(loc):].split(""_"")[0] for name in tmp_list]\n\n        labels = np.array(tmp_labels)\n        images = np.empty(len(img_list), dtype=object)\n\n        # Load grayscaled images\n        for i, img in enumerate(img_list):\n            images[i] = cv2.imread(img, 0)\n            print_progress_bar(i, len(img_list))\n\n        # Load gaplines (lines separating letters) from txt files\n        if load_gaplines:\n            gaplines = np.empty(len(img_list), dtype=object)\n            for i, name in enumerate(img_list):\n                with open(name[:-3] + \'txt\', \'r\') as fp:\n                    gaplines[i] = np.array(simplejson.load(fp))\n                \n    if load_gaplines:\n        assert len(labels) == len(images) == len(gaplines)\n    else:\n        assert len(labels) == len(images)\n    print(""-> Number of words:"", len(labels))\n    \n    if load_gaplines:\n        return (images, labels, gaplines)\n    return (images, labels)\n\n\ndef _words2chars(images, labels, gaplines):\n    """"""Transform word images with gaplines into individual chars.""""""\n    # Total number of chars\n    length = sum([len(l) for l in labels])\n    \n    imgs = np.empty(length, dtype=object)\n    new_labels = []\n    \n    height = images[0].shape[0]\n    \n    idx = 0;\n    for i, gaps in enumerate(gaplines):\n        for pos in range(len(gaps) - 1):\n            imgs[idx] = images[i][0:height, gaps[pos]:gaps[pos+1]]\n            new_labels.append(char2idx(labels[i][pos]))\n            idx += 1\n           \n    print(""Loaded chars from words:"", length)            \n    return imgs, new_labels\n\n\ndef load_chars_data(charloc=\'data/charclas/\', wordloc=\'data/words/\', lang=\'cz\'):\n    """"""\n    Load chars images with corresponding labels.\n    Args:\n        charloc: char images FOLDER LOCATION\n        wordloc: word images with gaplines FOLDER LOCATION\n    Returns:\n        (images, labels)\n    """"""\n    print(""Loading chars..."")\n    images = np.zeros((1, 4096))\n    labels = []\n\n    if charloc != \'\':\n        # Get subfolders with chars\n        dir_list = glob.glob(os.path.join(charloc, lang, ""*/""))\n        dir_list.sort()    \n\n        # if lang == \'en\':\n        chars = CHARS[:53]\n            \n        assert [d[-2] if d[-2] != \'0\' else \'\' for d in dir_list] == chars\n\n        # For every label load images and create corresponding labels\n        # cv2.imread(img, 0) - for loading images in grayscale\n        # Images are scaled to 64x64 = 4096 px\n        for i in range(len(chars)):\n            img_list = glob.glob(os.path.join(dir_list[i], \'*.jpg\'))\n            imgs = np.array([letter_normalization(cv2.imread(img, 0)) for img in img_list])\n            images = np.concatenate([images, imgs.reshape(len(imgs), 4096)])\n            labels.extend([i] * len(imgs))\n        \n    if wordloc != \'\':    \n        imgs, words, gaplines = load_words_data(wordloc, load_gaplines=True)\n        if lang != \'cz\':\n             words = np.array([unidecode.unidecode(w) for w in words])\n        imgs, chars = _words2chars(imgs, words, gaplines)\n        \n        labels.extend(chars)\n        images2 = np.zeros((len(imgs), 4096)) \n        for i in range(len(imgs)):\n            print_progress_bar(i, len(imgs))\n            images2[i] = letter_normalization(imgs[i]).reshape(1, 4096)\n\n        images = np.concatenate([images, images2])          \n\n    images = images[1:]\n    labels = np.array(labels)\n    \n    print(""-> Number of chars:"", len(labels))\n    return (images, labels)\n\n\ndef load_gap_data(loc=\'data/gapdet/large/\', slider=(60, 120), seq=False, flatten=True):\n    """""" \n    Load gap data from location with corresponding labels.\n    Args:\n        loc: location of folder with words separated into gap data\n             images have to by named as label_timestamp.jpg, label is 0 or 1\n        slider: dimensions of of output images\n        seq: Store images from one word as a sequence\n        flatten: Flatten the output images\n    Returns:\n        (images, labels)\n    """"""\n    print(\'Loading gap data...\')\n    dir_list = glob.glob(os.path.join(loc, ""*/""))\n    dir_list.sort()\n    \n    if slider[1] > 120:\n        # TODO Implement for higher dimmensions\n        slider[1] = 120\n        \n    cut_s = None if (120 - slider[1]) // 2 <= 0 else  (120 - slider[1]) // 2\n    cut_e = None if (120 - slider[1]) // 2 <= 0 else -(120 - slider[1]) // 2\n    \n    if seq:\n        images = np.empty(len(dir_list), dtype=object)\n        labels = np.empty(len(dir_list), dtype=object)\n        \n        for i, loc in enumerate(dir_list):\n            # TODO Check for empty directories\n            img_list = glob.glob(os.path.join(loc, \'*.jpg\'))\n            if (len(img_list) != 0):\n                img_list = sorted(imglist, key=lambda x: int(x[len(loc):].split(""_"")[1][:-4]))\n                images[i] = np.array([(cv2.imread(img, 0)[:, cut_s:cut_e].flatten() if flatten else\n                                       cv2.imread(img, 0)[:, cut_s:cut_e])\n                                      for img in img_list])\n                labels[i] = np.array([int(name[len(loc):].split(""_"")[0]) for name in img_list])\n        \n    else:\n        images = np.zeros((1, slider[0]*slider[1]))\n        labels = []\n\n        for i in range(len(dir_list)):\n            img_list = glob.glob(os.path.join(dir_list[i], \'*.jpg\'))\n            if (len(img_list) != 0):\n                imgs = np.array([cv2.imread(img, 0)[:, cut_s:cut_e] for img in img_list])\n                images = np.concatenate([images, imgs.reshape(len(imgs), slider[0]*slider[1])])\n                labels.extend([int(img[len(dirlist[i])]) for img in img_list])\n\n        images = images[1:]\n        labels = np.array(labels)\n    \n    if seq:\n        print(""-> Number of words / gaps and letters:"",\n              len(labels), \'/\', sum([len(l) for l in labels]))\n    else:\n        print(""-> Number of gaps and letters:"", len(labels))\n    return (images, labels)    \n\n\ndef corresponding_shuffle(a):\n    """""" \n    Shuffle array of numpy arrays such that\n    each pair a[x][i] and a[y][i] remains the same.\n    Args:\n        a: array of same length numpy arrays\n    Returns:\n        Array a with shuffled numpy arrays\n    """"""\n    assert all([len(a[0]) == len(a[i]) for i in range(len(a))])\n    p = np.random.permutation(len(a[0]))\n    for i in range(len(a)):\n        a[i] = a[i][p]\n    return a\n\n\ndef sequences_to_sparse(sequences):\n    """"""\n    Create a sparse representention of sequences.\n    Args:\n        sequences: a list of lists of type dtype where each element is a sequence\n    Returns:\n        A tuple with (indices, values, shape)\n    """"""\n    indices = []\n    values = []\n\n    for n, seq in enumerate(sequences):\n        indices.extend(zip([n]*len(seq), range(len(seq))))\n        values.extend(seq)\n        \n    indices = np.asarray(indices, dtype=np.int64)\n    values = np.asarray(values, dtype=np.int32)\n    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1]+1], dtype=np.int64)\n\n    return indices, values, shape\n'"
src/ocr/dataiterator.py,0,"b'# -*- coding: utf-8 -*-\n""""""Classes for feeding data during training.""""""\nimport numpy as np\nimport pandas as pd\nfrom .helpers import img_extend\nfrom .datahelpers import sequences_to_sparse\n\n\nclass BucketDataIterator():\n    """"""Iterator for feeding CTC model during training.""""""\n    def __init__(self,\n                 images,\n                 targets,\n                 num_buckets=5,\n                 slider=(60, 30),\n                 augmentation=None,\n                 dropout=0.0,\n                 train=True):\n\n        self.train = train\n        self.slider = slider\n        self.augmentation = augmentation\n        self.dropout = dropout\n        for i in range(len(images)):\n            images[i] = img_extend(\n                images[i],\n                (self.slider[0],\n\t\t         max(images[i].shape[1], self.slider[1])))\n        in_length = [image.shape[1] for image in images]\n        \n        # Create pandas dataFrame and sort it by images width (length)\n        self.dataFrame = pd.DataFrame({\n            \'in_length\': in_length,\n            \'images\': images,\n            \'targets\': targets}).sort_values(\'in_length\').reset_index(drop=True)\n\n        bsize = int(len(images) / num_buckets)\n        self.num_buckets = num_buckets\n        self.buckets = []\n        for bucket in range(num_buckets-1):\n            self.buckets.append(\n                self.dataFrame.iloc[bucket * bsize: (bucket+1) * bsize])\n        self.buckets.append(self.dataFrame.iloc[(num_buckets-1) * bsize:])\n\n        self.buckets_size = [len(bucket) for bucket in self.buckets]\n        self.cursor = np.array([0] * num_buckets)\n        self.bucket_order = np.random.permutation(num_buckets)\n        self.bucket_cursor = 0\n        self.shuffle()\n        print(""Iterator created."")\n\n\n    def shuffle(self, idx=None):\n        """"""Shuffle idx bucket or each bucket separately.""""""\n        for i in [idx] if idx is not None else range(self.num_buckets):\n            self.buckets[i] = self.buckets[i].sample(frac=1).reset_index(drop=True)\n            self.cursor[i] = 0\n\n\n    def next_batch(self, batch_size):\n        """"""Creates next training batch of size.\n        Args:\n            batch_size: size of next batch\n        Retruns:\n            (images, labels, images lengths, labels lengths)\n        """"""\n        i_bucket = self.bucket_order[self.bucket_cursor]\n        # Increment cursor and shuffle in case of new round\n        self.bucket_cursor = (self.bucket_cursor + 1) % self.num_buckets\n        if self.bucket_cursor == 0:\n            self.bucket_order = np.random.permutation(self.num_buckets)\n\n        if self.cursor[i_bucket] + batch_size > self.buckets_size[i_bucket]:\n            self.shuffle(i_bucket)\n\n        # Handle too big batch sizes\n        if (batch_size > self.buckets_size[i_bucket]):\n            batch_size = self.buckets_size[i_bucket]\n\n        res = self.buckets[i_bucket].iloc[self.cursor[i_bucket]:\n                                          self.cursor[i_bucket]+batch_size]\n        self.cursor[i_bucket] += batch_size\n\n        # PAD input sequence and output\n        input_max = max(res[\'in_length\'])\n\n        input_imgs = np.zeros(\n            (batch_size, self.slider[0], input_max, 1), dtype=np.uint8)\n        for i, img in enumerate(res[\'images\']):\n            input_imgs[i][:, :res[\'in_length\'].values[i], 0] = img\n            \n        if self.train:\n            input_imgs = self.augmentation.augment_images(input_imgs)\n        input_imgs = input_imgs.astype(np.float32)\n\n        targets = sequences_to_sparse(res[\'targets\'].values)\n        return input_imgs, targets, res[\'in_length\'].values\n\n'"
src/ocr/helpers.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nHelper functions for ocr project\n""""""\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\n\n\nSMALL_HEIGHT = 800\n\n\ndef implt(img, cmp=None, t=\'\'):\n    """"""Show image using plt.""""""\n    plt.imshow(img, cmap=cmp)\n    plt.title(t)\n    plt.show()\n\n\ndef resize(img, height=SMALL_HEIGHT, allways=False):\n    """"""Resize image to given height.""""""\n    if (img.shape[0] > height or allways):\n        rat = height / img.shape[0]\n        return cv2.resize(img, (int(rat * img.shape[1]), height))\n    \n    return img\n\n\ndef ratio(img, height=SMALL_HEIGHT):\n    """"""Getting scale ratio.""""""\n    return img.shape[0] / height\n\n\ndef img_extend(img, shape):\n    """"""Extend 2D image (numpy array) in vertical and horizontal direction.\n    Shape of result image will match \'shape\'\n    Args:\n        img: image to be extended\n        shape: shape (touple) of result image\n    Returns:\n        Extended image\n    """"""\n    x = np.zeros(shape, np.uint8)\n    x[:img.shape[0], :img.shape[1]] = img\n    return x'"
src/ocr/imgtransform.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nFunctions for transforming and preprocessing images for training\n""""""\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom scipy.ndimage.interpolation import map_coordinates\n\n\ndef coordinates_remap(image, factor_alpha, factor_sigma):\n    """"""Transforming image using remaping coordinates.""""""\n    alpha = image.shape[1] * factor_alpha\n    sigma = image.shape[1] * factor_sigma\n    shape = image.shape\n    \n    blur_size = int(4*sigma) | 1\n    dx = alpha * cv2.GaussianBlur((np.random.rand(*shape) * 2 - 1),\n                                  ksize=(blur_size, blur_size),\n                                  sigmaX=sigma)\n    dy = alpha * cv2.GaussianBlur((np.random.rand(*shape) * 2 - 1),\n                                  ksize=(blur_size, blur_size),\n                                  sigmaX=sigma)\n    \n    x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))\n    indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1))\n\n    # TODO use cv2.remap(image, dx, dy, interpolation=cv2.INTER_LINEAR)\n    return np.array(map_coordinates(image, indices, order=1, mode=\'constant\').reshape(shape)) '"
src/ocr/mlhelpers.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nClasses for controling machine learning processes\n""""""\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport csv\n\n\nclass TrainingPlot:\n    """"""\n    Creating live plot during training\n    REUIRES notebook backend: %matplotlib notebook\n    @TODO Migrate to Tensorboard\n    """"""\n    train_loss = []\n    train_acc = []\n    valid_acc = []\n    test_iter = 0\n    loss_iter = 0\n    interval = 0\n    ax1 = None\n    ax2 = None\n    fig = None\n\n    def __init__(self, steps, test_itr, loss_itr):\n        self.test_iter = test_itr\n        self.loss_iter = loss_itr\n        self.interval = steps\n\n        self.fig, self.ax1 = plt.subplots()\n        self.ax2 = self.ax1.twinx()\n        self.ax1.set_autoscaley_on(True)\n        plt.ion()\n\n        self._update_plot()\n\n        # Description\n        self.ax1.set_xlabel(\'Iteration\')\n        self.ax1.set_ylabel(\'Train Loss\')\n        self.ax2.set_ylabel(\'Valid. Accuracy\')\n\n        # Axes limits\n        self.ax1.set_ylim([0,10])\n\n    def _update_plot(self):\n        self.fig.canvas.draw()\n\n    def update_loss(self, loss_train, index):\n        self.trainLoss.append(loss_train)\n        if len(self.train_loss) == 1:\n            self.ax1.set_ylim([0, min(10, math.ceil(loss_train))])\n        self.ax1.plot(self.lossInterval * np.arange(len(self.train_loss)),\n                      self.train_loss, \'b\', linewidth=1.0)\n\n        self.updatePlot()\n\n    def update_acc(self, acc_val, acc_train, index):\n        self.validAcc.append(acc_val)\n        self.trainAcc.append(acc_train)\n\n        self.ax2.plot(self.test_iter * np.arange(len(self.valid_acc)),\n                      self.valid_acc, \'r\', linewidth=1.0)\n        self.ax2.plot(self.test_iter * np.arange(len(self.train_acc)),\n                      self.train_acc, \'g\',linewidth=1.0)\n\n        self.ax2.set_title(\'Valid. Accuracy: {:.4f}\'.format(self.valid_acc[-1]))\n\n        self.updatePlot()\n\n\nclass DataSet:\n    """"""Class for training data and feeding train function.""""""\n    images = None\n    labels = None\n    length = 0\n    index = 0\n\n    def __init__(self, img, lbl):\n        self.images = img\n        self.labels = lbl\n        self.length = len(img)\n        self.index = 0\n\n    def next_batch(self, batch_size):\n        """"""Return the next batch from the data set.""""""\n        start = self.index\n        self.index += batch_size\n\n        if self.index > self.length:\n            # Shuffle the data\n            perm = np.arange(self.length)\n            np.random.shuffle(perm)\n            self.images = self.images[perm]\n            self.labels = self.labels[perm]\n            # Start next epoch\n            start = 0\n            self.index = batch_size\n\n        end = self.index\n        return self.images[start:end], self.labels[start:end]\n'"
src/ocr/normalization.py,1,"b'# -*- coding: utf-8 -*-\n""""""\nInclude functions for normalizing images of words and letters\nMain functions: word_normalization, letter_normalization, image_standardization\n""""""\nimport numpy as np\nimport cv2\nimport math\n\nfrom .helpers import *\n\n\ndef image_standardization(image):\n    """"""Image standardization should result in same output \n    as tf.image.per_image_standardization.\n    """"""\n    return (image - np.mean(image)) / max(np.std(image), 1.0/math.sqrt(image.size))\n\n\ndef _crop_add_border(img, height, threshold=50, border=True, border_size=15):\n    """"""Crop and add border to word image of letter segmentation.""""""\n    # Clear small values\n    ret, img = cv2.threshold(img, threshold, 255, cv2.THRESH_TOZERO)\n\n    x0 = 0\n    y0 = 0\n    x1 = img.shape[1]\n    y1 = img.shape[0]\n\n    for i in range(img.shape[0]):\n        if np.count_nonzero(img[i, :]) > 1:\n            y0 = i\n            break\n    for i in reversed(range(img.shape[0])):\n        if np.count_nonzero(img[i, :]) > 1:\n            y1 = i+1\n            break\n    for i in range(img.shape[1]):\n        if np.count_nonzero(img[:, i]) > 1:\n            x0 = i\n            break\n    for i in reversed(range(img.shape[1])):\n        if np.count_nonzero(img[:, i]) > 1:\n            x1 = i+1\n            break\n\n    if height != 0:\n        img = resize(img[y0:y1, x0:x1], height, True)\n    else:\n        img = img[y0:y1, x0:x1]\n\n    if border:\n        return cv2.copyMakeBorder(img, 0, 0, border_size, border_size,\n                                  cv2.BORDER_CONSTANT,\n                                  value=[0, 0, 0])\n    return img\n\n\ndef _word_tilt(img, height, border=True, border_size=15):\n    """"""Detect the angle and tilt the image.""""""\n    edges = cv2.Canny(img, 50, 150, apertureSize = 3)\n    lines = cv2.HoughLines(edges, 1, np.pi/180, 30)\n\n    if lines is not None:\n        meanAngle = 0\n        # Set min number of valid lines (try higher)\n        numLines = np.sum(1 for l in lines if l[0][1] < 0.7 or l[0][1] > 2.6)\n        if numLines > 1:\n            meanAngle = np.mean([l[0][1] for l in lines if l[0][1] < 0.7 or l[0][1] > 2.6])\n\n        # Look for angle with correct value\n        if meanAngle != 0 and (meanAngle < 0.7 or meanAngle > 2.6):\n            img = _tilt_by_angle(img, meanAngle, height)\n    return _crop_add_border(img, height, 50, border, border_size)\n\n\ndef _tilt_by_angle(img, angle, height):\n    """"""Tilt the image by given angle.""""""\n    dist = np.tan(angle) * height\n    width = len(img[0])\n    sPoints = np.float32([[0,0], [0,height], [width,height], [width,0]])\n\n    # Dist is positive for angle < 0.7; negative for angle > 2.6\n    # Image must be shifed to right\n    if dist > 0:\n        tPoints = np.float32([[0,0],\n                              [dist,height],\n                              [width+dist,height],\n                              [width,0]])\n    else:\n        tPoints = np.float32([[-dist,0],\n                              [0,height],\n                              [width,height],\n                              [width-dist,0]])\n\n    M = cv2.getPerspectiveTransform(sPoints, tPoints)\n    return cv2.warpPerspective(img, M, (int(width+abs(dist)), height))\n\n\ndef _sobel_detect(channel):\n    """"""The Sobel Operator.""""""\n    sobelX = cv2.Sobel(channel, cv2.CV_16S, 1, 0)\n    sobelY = cv2.Sobel(channel, cv2.CV_16S, 0, 1)\n    # Combine x, y gradient magnitudes sqrt(x^2 + y^2)\n    sobel = np.hypot(sobelX, sobelY)\n    sobel[sobel > 255] = 255\n    return np.uint8(sobel)\n\n\nclass HysterThresh:\n    def __init__(self, img):\n        img = 255 - img\n        img = (img - np.min(img)) / (np.max(img) - np.min(img)) * 255\n        hist, bins = np.histogram(img.ravel(), 256, [0,256])\n\n        self.high = np.argmax(hist) + 65\n        self.low = np.argmax(hist) + 45\n        self.diff = 255 - self.high\n\n        self.img = img\n        self.im = np.zeros(img.shape, dtype=img.dtype)\n\n    def get_image(self):\n        self._hyster()\n        return np.uint8(self.im)\n\n    def _hyster_rec(self, r, c):\n        h, w = self.img.shape\n        for ri in range(r-1, r+2):\n            for ci in range(c-1, c+2):\n                if (h > ri >= 0\n                    and w > ci >= 0\n                    and self.im[ri, ci] == 0\n                    and self.high > self.img[ri, ci] >= self.low):\n                    self.im[ri, ci] = self.img[ri, ci] + self.diff\n                    self._hyster_rec(ri, ci)\n\n    def _hyster(self):\n        r, c = self.img.shape\n        for ri in range(r):\n            for ci in range(c):\n                if (self.img[ri, ci] >= self.high):\n                    self.im[ri, ci] = 255\n                    self.img[ri, ci] = 255\n                    self._hyster_rec(ri, ci)\n\n\ndef _hyst_word_norm(image):\n    """"""Word normalization using hystheresis thresholding.""""""\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n#     img = cv2.bilateralFilter(gray, 0, 10, 30)\n    img = cv2.bilateralFilter(gray, 10, 10, 30)\n    return HysterThresh(img).get_image()\n\n\ndef word_normalization(image, height, border=True, tilt=True, border_size=15, hyst_norm=False):\n    """""" Preprocess a word - resize, binarize, tilt world.""""""\n    image = resize(image, height, True)\n\n    if hyst_norm:\n        th = _hyst_word_norm(image)\n    else:\n        img = cv2.bilateralFilter(image, 10, 30, 30)\n        gray = 255 - cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        norm = cv2.normalize(gray, None, 0, 255, cv2.NORM_MINMAX)\n        ret,th = cv2.threshold(norm, 50, 255, cv2.THRESH_TOZERO)\n\n    if tilt:\n        return _word_tilt(th, height, border, border_size)\n    return _crop_add_border(th, height, border, border_size)\n\n\ndef _resize_letter(img, size = 56):\n    """"""Resize bigger side of the image to given size.""""""\n    if (img.shape[0] > img.shape[1]):\n        rat = size / img.shape[0]\n        return cv2.resize(img, (int(rat * img.shape[1]), size))\n    else:\n        rat = size / img.shape[1]\n        return cv2.resize(img, (size, int(rat * img.shape[0])))\n    return img\n\n\ndef letter_normalization(image, is_thresh=True, dim=False):\n    """"""Preprocess a letter - crop, resize""""""\n    if is_thresh and image.shape[0] > 0 and image.shape[1] > 0:\n        image = _crop_add_border(image, height=0, threshold=80, border=False)\n\n    resized = image\n    if image.shape[0] > 1 and image.shape[1] > 1:\n        resized = _resize_letter(image)\n\n    result = np.zeros((64, 64), np.uint8)\n    offset = [0, 0]\n    # Calculate offset for smaller size\n    if image.shape[0] > image.shape[1]:\n        offset = [int((result.shape[1] - resized.shape[1])/2), 4]\n    else:\n        offset = [4, int((result.shape[0] - resized.shape[0])/2)]\n    # Replace zeros by image \n    result[offset[1]:offset[1] + resized.shape[0],\n           offset[0]:offset[0] + resized.shape[1]] = resized\n\n    if dim:\n        return result, image.shape\n    return result\n'"
src/ocr/page.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCrop background and transform perspective from the photo of page\n""""""\nimport numpy as np\nimport cv2\n\nfrom .helpers import *\n\ndef detection(image):\n    """"""Finding Page.""""""\n    # Edge detection\n    image_edges = _edges_detection(image, 200, 250)\n    \n    # Close gaps between edges (double page clouse => rectangle kernel)\n    closed_edges = cv2.morphologyEx(image_edges, \n                                    cv2.MORPH_CLOSE, \n                                    np.ones((5, 11)))\n    # Countours\n    page_contour = _find_page_contours(closed_edges, resize(image))\n    # Recalculate to original scale\n    page_contour = page_contour.dot(ratio(image))    \n    # Transform prespective\n    new_image = _persp_transform(image, page_contour)\n    return new_image\n   \n\ndef _edges_detection(img, minVal, maxVal):\n    """"""Preprocessing (gray, thresh, filter, border) + Canny edge detection.""""""\n    img = cv2.cvtColor(resize(img), cv2.COLOR_BGR2GRAY)\n\n    img = cv2.bilateralFilter(img, 9, 75, 75)\n    img = cv2.adaptiveThreshold(img, 255,\n                                cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n                                cv2.THRESH_BINARY, 115, 4)\n\n    # Median blur replace center pixel by median of pixels under kelner\n    # => removes thin details\n    img = cv2.medianBlur(img, 11)\n\n    # Add black border - detection of border touching pages\n    img = cv2.copyMakeBorder(img, 5, 5, 5, 5,\n                             cv2.BORDER_CONSTANT,\n                             value=[0, 0, 0])\n    return cv2.Canny(img, minVal, maxVal)\n\n\ndef _four_corners_sort(pts):\n    """"""Sort corners in order: top-left, bot-left, bot-right, top-right.""""""\n    diff = np.diff(pts, axis=1)\n    summ = pts.sum(axis=1)\n    return np.array([pts[np.argmin(summ)],\n                     pts[np.argmax(diff)],\n                     pts[np.argmax(summ)],\n                     pts[np.argmin(diff)]])\n\n\ndef _contour_offset(cnt, offset):\n    """"""Offset contour because of 5px border.""""""\n    cnt += offset\n    cnt[cnt < 0] = 0\n    return cnt\n\n\ndef _find_page_contours(edges, img):\n    """"""Finding corner points of page contour.""""""\n    im2, contours, hierarchy = cv2.findContours(edges,\n                                                cv2.RETR_TREE,\n                                                cv2.CHAIN_APPROX_SIMPLE)\n    \n    # Finding biggest rectangle otherwise return original corners\n    height = edges.shape[0]\n    width = edges.shape[1]\n    MIN_COUNTOUR_AREA = height * width * 0.5\n    MAX_COUNTOUR_AREA = (width - 10) * (height - 10)\n\n    max_area = MIN_COUNTOUR_AREA\n    page_contour = np.array([[0, 0],\n                             [0, height-5],\n                             [width-5, height-5],\n                             [width-5, 0]])\n\n    for cnt in contours:\n        perimeter = cv2.arcLength(cnt, True)\n        approx = cv2.approxPolyDP(cnt, 0.03 * perimeter, True)\n\n        # Page has 4 corners and it is convex\n        if (len(approx) == 4 and\n                cv2.isContourConvex(approx) and\n                max_area < cv2.contourArea(approx) < MAX_COUNTOUR_AREA):\n            \n            max_area = cv2.contourArea(approx)\n            page_contour = approx[:, 0]\n\n    # Sort corners and offset them\n    page_contour = _four_corners_sort(page_contour)\n    return _contour_offset(page_contour, (-5, -5))\n\n\ndef _persp_transform(img, s_points):\n    """"""Transform perspective from start points to target points.""""""\n    # Euclidean distance - calculate maximum height and width\n    height = max(np.linalg.norm(s_points[0] - s_points[1]),\n                 np.linalg.norm(s_points[2] - s_points[3]))\n    width = max(np.linalg.norm(s_points[1] - s_points[2]),\n                 np.linalg.norm(s_points[3] - s_points[0]))\n    \n    # Create target points\n    t_points = np.array([[0, 0],\n                        [0, height],\n                        [width, height],\n                        [width, 0]], np.float32)\n    \n    # getPerspectiveTransform() needs float32\n    if s_points.dtype != np.float32:\n        s_points = s_points.astype(np.float32)\n    \n    M = cv2.getPerspectiveTransform(s_points, t_points) \n    return cv2.warpPerspective(img, M, (int(width), int(height)))'"
src/ocr/tfhelpers.py,3,"b'# -*- coding: utf-8 -*-\n""""""\nProvide functions and classes:\nModel       = Class for loading and using trained models from tensorflow\ncreate_cell = function for creatting RNN cells with wrappers\n""""""\nimport tensorflow as tf\nfrom tensorflow.python.ops.rnn_cell_impl import LSTMCell, ResidualWrapper, DropoutWrapper, MultiRNNCell\n\nclass Model():\n    """"""Loading and running isolated tf graph.""""""\n    def __init__(self, loc, operation=\'activation\', input_name=\'x\'):\n        """"""\n        loc: location of file containing saved model\n        operation: name of operation for running the model\n        input_name: name of input placeholder\n        """"""\n        self.input = input_name + "":0""\n        self.graph = tf.Graph()\n        self.sess = tf.Session(graph=self.graph)\n        with self.graph.as_default():\n            saver = tf.train.import_meta_graph(loc + \'.meta\', clear_devices=True)\n            saver.restore(self.sess, loc)\n            self.op = self.graph.get_operation_by_name(operation).outputs[0]\n\n    def run(self, data):\n        """"""Run the specified operation on given data.""""""\n        return self.sess.run(self.op, feed_dict={self.input: data})\n    \n    def eval_feed(self, feed):\n        """"""Run the specified operation with given feed.""""""\n        return self.sess.run(self.op, feed_dict=feed)\n    \n    def run_op(self, op, feed, output=True):\n        """"""Run given operation with the feed.""""""\n        if output:\n            return self.sess.run(\n                self.graph.get_operation_by_name(op).outputs[0],\n                feed_dict=feed)\n        else:\n            self.sess.run(\n                self.graph.get_operation_by_name(op),\n                feed_dict=feed)\n        \n    \n    \ndef _create_single_cell(cell_fn, num_units, is_residual=False, is_dropout=False, keep_prob=None):\n    """"""Create single RNN cell based on cell_fn.""""""\n    cell = cell_fn(num_units)\n    if is_dropout:\n        cell = DropoutWrapper(cell, input_keep_prob=keep_prob)\n    if is_residual:\n        cell = ResidualWrapper(cell)\n    return cell\n\n\ndef create_cell(num_units, num_layers, num_residual_layers, is_dropout=False, keep_prob=None, cell_fn=LSTMCell):\n    """"""Create corresponding number of RNN cells with given wrappers.""""""\n    cell_list = []\n    \n    for i in range(num_layers):\n        cell_list.append(_create_single_cell(\n            cell_fn=cell_fn,\n            num_units=num_units,\n            is_residual=(i >= num_layers - num_residual_layers),\n            is_dropout=is_dropout,\n            keep_prob=keep_prob\n        ))\n\n    if num_layers == 1:\n        return cell_list[0]\n    return MultiRNNCell(cell_list)'"
src/ocr/viz.py,0,"b'def print_progress_bar(iteration,\n                       total,\n                       prefix = \'\',\n                       suffix = \'\'):\n    """"""Call in a loop to create terminal progress bar.\n    Args:\n        iteration: current iteration (Int)\n        total: total iterations (Int)\n        prefix: prefix string (Str)\n        suffix: suffix string (Str)\n    """"""\n    # Printing slowes down the loop\n    if iteration % (total // 100) == 0:\n        length = 40\n        iteration += 1\n        percent = (100 * iteration) // (total * 99/100)\n        filled_length = int(length * percent / 100)\n        bar = \'\xe2\x96\x88\' * filled_length + \'-\' * (length - filled_length)\n        print(\'\\r%s |%s| %s%% %s\' % (prefix, bar, percent, suffix), end = \'\\r\')\n\n        if iteration >= total * 99/100:\n            print()\n'"
src/ocr/words.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nDetect words on the page\nreturn array of words\' bounding boxes\n""""""\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\n\nfrom .helpers import *\n\n\ndef detection(image, join=False):\n    """"""Detecting the words bounding boxes.\n    Return: numpy array of bounding boxes [x, y, x+w, y+h]\n    """"""\n    # Preprocess image for word detection\n    blurred = cv2.GaussianBlur(image, (5, 5), 18)\n    edge_img = _edge_detect(blurred)\n    ret, edge_img = cv2.threshold(edge_img, 50, 255, cv2.THRESH_BINARY)\n    bw_img = cv2.morphologyEx(edge_img, cv2.MORPH_CLOSE,\n                              np.ones((15,15), np.uint8))\n\n    return _text_detect(bw_img, image, join)\n\n\ndef sort_words(boxes):\n    """"""Sort boxes - (x, y, x+w, y+h) from left to right, top to bottom.""""""\n    mean_height = sum([y2 - y1 for _, y1, _, y2 in boxes]) / len(boxes)\n    \n    boxes.view(\'i8,i8,i8,i8\').sort(order=[\'f1\'], axis=0)\n    current_line = boxes[0][1]\n    lines = []\n    tmp_line = []\n    for box in boxes:\n        if box[1] > current_line + mean_height:\n            lines.append(tmp_line)\n            tmp_line = [box]\n            current_line = box[1]            \n            continue\n        tmp_line.append(box)\n    lines.append(tmp_line)\n        \n    for line in lines:\n        line.sort(key=lambda box: box[0])\n        \n    return lines\n\n\ndef _edge_detect(im):\n    """""" \n    Edge detection using sobel operator on each layer individually.\n    Sobel operator is applied for each image layer (RGB)\n    """"""\n    return np.max(np.array([_sobel_detect(im[:,:, 0]),\n                            _sobel_detect(im[:,:, 1]),\n                            _sobel_detect(im[:,:, 2])]), axis=0)\n\n\ndef _sobel_detect(channel):\n    """"""Sobel operator.""""""\n    sobelX = cv2.Sobel(channel, cv2.CV_16S, 1, 0)\n    sobelY = cv2.Sobel(channel, cv2.CV_16S, 0, 1)\n    sobel = np.hypot(sobelX, sobelY)\n    sobel[sobel > 255] = 255\n    return np.uint8(sobel)\n\n\ndef union(a,b):\n    x = min(a[0], b[0])\n    y = min(a[1], b[1])\n    w = max(a[0]+a[2], b[0]+b[2]) - x\n    h = max(a[1]+a[3], b[1]+b[3]) - y\n    return [x, y, w, h]\n\ndef _intersect(a,b):\n    x = max(a[0], b[0])\n    y = max(a[1], b[1])\n    w = min(a[0]+a[2], b[0]+b[2]) - x\n    h = min(a[1]+a[3], b[1]+b[3]) - y\n    if w<0 or h<0:\n        return False\n    return True\n\ndef _group_rectangles(rec):\n    """"""\n    Uion intersecting rectangles.\n    Args:\n        rec - list of rectangles in form [x, y, w, h]\n    Return:\n        list of grouped ractangles \n    """"""\n    tested = [False for i in range(len(rec))]\n    final = []\n    i = 0\n    while i < len(rec):\n        if not tested[i]:\n            j = i+1\n            while j < len(rec):\n                if not tested[j] and _intersect(rec[i], rec[j]):\n                    rec[i] = union(rec[i], rec[j])\n                    tested[j] = True\n                    j = i\n                j += 1\n            final += [rec[i]]\n        i += 1\n            \n    return final\n\n\ndef _text_detect(img, image, join=False):\n    """"""Text detection using contours.""""""\n    small = resize(img, 2000)\n    \n    # Finding contours\n    mask = np.zeros(small.shape, np.uint8)\n    im2, cnt, hierarchy = cv2.findContours(np.copy(small),\n                                           cv2.RETR_CCOMP,\n                                           cv2.CHAIN_APPROX_SIMPLE)\n    \n    index = 0    \n    boxes = []\n    # Go through all contours in top level\n    while (index >= 0):\n        x,y,w,h = cv2.boundingRect(cnt[index])\n        cv2.drawContours(mask, cnt, index, (255, 255, 255), cv2.FILLED)\n        maskROI = mask[y:y+h, x:x+w]\n        # Ratio of white pixels to area of bounding rectangle\n        r = cv2.countNonZero(maskROI) / (w * h)\n        \n        # Limits for text\n        if (r > 0.1\n            and 1600 > w > 10\n            and 1600 > h > 10\n            and h/w < 3\n            and w/h < 10\n            and (60 // h) * w < 1000):\n            boxes += [[x, y, w, h]]\n            \n        index = hierarchy[0][index][0]\n\n    if join:\n        # Need more work\n        boxes = _group_rectangles(boxes)\n\n    # image for drawing bounding boxes\n    small = cv2.cvtColor(small, cv2.COLOR_GRAY2RGB)\n    bounding_boxes = np.array([0,0,0,0])\n    for (x, y, w, h) in boxes:\n        cv2.rectangle(small, (x, y),(x+w,y+h), (0, 255, 0), 2)\n        bounding_boxes = np.vstack((bounding_boxes,\n                                    np.array([x, y, x+w, y+h])))\n        \n    implt(small, t=\'Bounding rectangles\')\n    \n    boxes = bounding_boxes.dot(ratio(image, small.shape[0])).astype(np.int64)\n    return boxes[1:]  \n    \n\ndef textDetectWatershed(thresh):\n    """"""NOT IN USE - Text detection using watershed algorithm.\n    Based on: http://docs.opencv.org/trunk/d3/db4/tutorial_py_watershed.html\n    """"""\n    img = cv2.cvtColor(cv2.imread(""data/textdet/%s.jpg"" % IMG),\n                       cv2.COLOR_BGR2RGB)\n    img = resize(img, 3000)\n    thresh = resize(thresh, 3000)\n    # noise removal\n    kernel = np.ones((3,3),np.uint8)\n    opening = cv2.morphologyEx(thresh,cv2.MORPH_OPEN,kernel, iterations = 3)\n    \n    # sure background area\n    sure_bg = cv2.dilate(opening,kernel,iterations=3)\n\n    # Finding sure foreground area\n    dist_transform = cv2.distanceTransform(opening,cv2.DIST_L2,5)\n    ret, sure_fg = cv2.threshold(dist_transform,\n                                 0.01*dist_transform.max(), 255, 0)\n\n    # Finding unknown region\n    sure_fg = np.uint8(sure_fg)\n    unknown = cv2.subtract(sure_bg,sure_fg)\n    \n    # Marker labelling\n    ret, markers = cv2.connectedComponents(sure_fg)\n\n    # Add one to all labels so that sure background is not 0, but 1\n    markers += 1\n\n    # Now, mark the region of unknown with zero\n    markers[unknown == 255] = 0\n    \n    markers = cv2.watershed(img, markers)\n    implt(markers, t=\'Markers\')\n    image = img.copy()\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \n    for mark in np.unique(markers):\n        # mark == 0 --> background\n        if mark == 0:\n            continue\n\n        # Draw it on mask and detect biggest contour\n        mask = np.zeros(gray.shape, dtype=""uint8"")\n        mask[markers == mark] = 255\n\n        cnts = cv2.findContours(mask.copy(),\n                                cv2.RETR_EXTERNAL,\n                                cv2.CHAIN_APPROX_SIMPLE)[-2]\n        c = max(cnts, key=cv2.contourArea)\n        \n        # Draw a bounding rectangle if it contains text\n        x,y,w,h = cv2.boundingRect(c)\n        cv2.drawContours(mask, c, 0, (255, 255, 255), cv2.FILLED)\n        maskROI = mask[y:y+h, x:x+w]\n        # Ratio of white pixels to area of bounding rectangle\n        r = cv2.countNonZero(maskROI) / (w * h)\n        \n        # Limits for text\n        if r > 0.2 and 2000 > w > 15 and 1500 > h > 15:\n            cv2.rectangle(image, (x, y),(x+w,y+h), (0, 255, 0), 2)\n        \n    implt(image)\n'"
src/data/data_creation/WordClassDM.py,0,"b'""""""\nLast: 5069\n\nScript with simple UI for creating gaplines data\nRun: python WordClassDM.py --index 0\nControls:\n    setting gaplines  - click and drag\n    saving gaplines   - \'s\' key\n    reseting gaplines - \'r\' key\n    skip to next img  - \'n\' key\n    delete last line  - \'d\' key\n""""""\n\nimport cv2\nimport os\nimport numpy as np\nimport glob\nimport argparse\nimport simplejson\nfrom ocr.normalization import imageNorm\nfrom ocr.viz import printProgressBar\n\n\ndef loadImages(dataloc, idx=0, num=None):\n    """""" Load images and labels """"""\n    print(""Loading words..."")\n\n    # Load images and short them from the oldest to the newest\n    imglist = glob.glob(os.path.join(dataloc, u\'*.jpg\'))\n    imglist.sort(key=lambda x: float(x.split(""_"")[-1][:-4]))\n    tmpLabels = [name[len(dataloc):] for name in imglist]\n\n    labels = np.array(tmpLabels)\n    images = np.empty(len(imglist), dtype=object)\n\n    if num is None:\n        upper = len(imglist)\n    else:\n        upper = min(idx + num, len(imglist))\n        num += idx\n\n    for i, img in enumerate(imglist):\n        # TODO Speed up loading - Normalization\n        if i >= idx and i < upper:\n            images[i] = imageNorm(\n                cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2RGB),\n                height=60,\n                border=False,\n                tilt=True,\n                hystNorm=True)\n            printProgressBar(i-idx, upper-idx-1)\n    print()\n    return (images[idx:num], labels[idx:num])\n\n\ndef locCheck(loc):\n    return loc + \'/\' if loc[-1] != \'/\' else loc\n\n\nclass Cycler:\n    drawing = False\n    scaleF = 4\n\n    def __init__(self, idx, data_loc, save_loc):\n        """""" Load images and starts from given index """"""\n        # self.images, self.labels = loadImages(loc, idx)\n        # Create save_loc directory if not exists\n        if not os.path.exists(save_loc):\n            os.makedirs(save_loc)\n            \n        self.data_loc = locCheck(data_loc)\n        self.save_loc = locCheck(save_loc)\n        \n        self.idx = 0\n        self.org_idx = idx\n\n        self.blockLoad()\n        self.image_act = self.images[self.idx]\n\n        cv2.namedWindow(\'image\')\n        cv2.setMouseCallback(\'image\', self.mouseHandler)\n        self.nextImage()\n\n        self.run()\n\n    def run(self):\n        while(1):\n            self.imageShow()\n            k = cv2.waitKey(1) & 0xFF\n            if k == ord(\'d\'):\n                # Delete last line\n                self.deleteLastLine()\n            elif k == ord(\'r\'):\n                # Clear current gaplines\n                self.nextImage()\n            elif k == ord(\'s\'):\n                # Save gaplines with image\n                if self.saveData():\n                    self.idx += 1\n                    if self.idx >= len(self.images):\n                        if not self.blockLoad():\n                            break\n                    self.nextImage()\n            elif k == ord(\'n\'):\n                # Skip to next image\n                self.idx += 1\n                if self.idx >= len(self.images):\n                    if not self.blockLoad():\n                        break\n                self.nextImage()\n            elif k == 27:\n                cv2.destroyAllWindows()\n                break\n\n        print(""End of labeling at INDEX: "" + str(self.org_idx + self.idx))\n\n    def blockLoad(self):\n        self.images, self.labels = loadImages(\n            self.data_loc, self.org_idx + self.idx, 100)\n        self.org_idx += self.idx\n        self.idx = 0\n        return len(self.images) is not 0\n\n    def imageShow(self):\n        cv2.imshow(\n            \'image\',\n            cv2.resize(\n                self.image_act,\n                (0,0),\n                fx=self.scaleF,\n                fy=self.scaleF,\n                interpolation=cv2.INTERSECT_NONE))\n\n    def nextImage(self):\n        self.image_act = cv2.cvtColor(self.images[self.idx], cv2.COLOR_GRAY2RGB)\n        self.label_act = self.labels[self.idx][:-4]\n        self.gaplines =  [0, self.image_act.shape[1]]\n        self.redrawLines()\n\n        print(self.org_idx + self.idx, "":"", self.label_act.split(""_"")[0])\n        self.imageShow();\n\n    def saveData(self):\n        self.gaplines.sort()\n        print(""Saving image with gaplines: "", self.gaplines)\n\n        try:\n            assert len(self.gaplines) - 1 == len(self.label_act.split(""_"")[0])\n\n            cv2.imwrite(\n                    self.save_loc + \'%s.jpg\' % (self.label_act),\n                    self.images[self.idx])\n            with open(self.save_loc + \'%s.txt\' % (self.label_act), \'w\') as fp:\n                simplejson.dump(self.gaplines, fp)\n            return True\n        except:\n            print(""Wront number of gaplines"")\n            return False\n\n        print()\n        self.nextImage()\n\n    def deleteLastLine(self):\n        if len(self.gaplines) > 0:\n            del self.gaplines[-1]\n        self.redrawLines()\n\n    def redrawLines(self):\n        self.image_act = cv2.cvtColor(self.images[self.idx], cv2.COLOR_GRAY2RGB)\n        for x in self.gaplines:\n            self.drawLine(x)\n\n    def drawLine(self, x):\n        cv2.line(\n            self.image_act, (x, 0), (x, self.image_act.shape[0]), (0,255,0), 1)\n\n    def mouseHandler(self, event, x, y, flags, param):\n        # Clip x into image width range\n        x = max(min(self.image_act.shape[1], x // self.scaleF), 0)\n\n        if event == cv2.EVENT_LBUTTONDOWN:\n            self.drawing = True\n            self.tmp = self.image_act.copy()\n            self.drawLine(x)\n        elif event == cv2.EVENT_MOUSEMOVE:\n            if self.drawing == True:\n                self.image_act = self.tmp.copy()\n                self.drawLine(x)\n        elif event == cv2.EVENT_LBUTTONUP:\n            self.drawing = False\n            if x not in self.gaplines:\n                self.gaplines.append(x)\n                self.image_act = self.tmp.copy()\n                self.drawLine(x)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(\n            ""Script creating UI for gaplines classification"")\n    parser.add_argument(\n        ""--index"",\n        type=int,\n        default=0,\n        help=""Index of starting image"")\n    \n    parser.add_argument(\n        ""--data"",\n        type=str,\n        default=\'data/words_raw\',\n        help=""Path to folder with images"")\n    \n    parser.add_argument(\n        ""--save"",\n        type=str,\n        default=\'data/words2\',\n        help=""Path to folder for saving images with gaplines"")\n\n    args = parser.parse_args()\n    Cycler(args.index, args.data, args.save)\n'"
src/data/datasets/__init__.py,0,b''
src/data/datasets/breta.py,0,"b'import os\nfrom PIL import Image\nimport time\nimport sys\n# Allow accesing files relative to this file\nlocation = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.join(location, \'../../\'))\nfrom ocr.viz import print_progress_bar\n\n\ndef extract(location, output, number=1):\n    output = os.path.join(location, output)\n    if not os.path.exists(output):\n        os.makedirs(output)\n\n    for sub in [\'words\', \'archive\', \'cz_raw\', \'en_raw\']:\n        folder = os.path.join(location, sub)\n\n        img_list = os.listdir(os.path.join(folder))\n        for i, data in enumerate(img_list):\n            word = data.split(\'_\')[0]\n            img = os.path.join(folder, data)\n            out = os.path.join(\n                output,\n                \'%s_%s_%s.png\' % (word, number, data.split(\'_\')[-1][:-4]))\n            Image.open(img).save(out)\n            print_progress_bar(i, len(img_list))\n\n    print(""\\tNumber of words:"", len([n for n in os.listdir(output)]))\n'"
src/data/datasets/camb.py,0,"b'import cv2\nimport glob\nimport numpy as np\nimport os\nimport sys\nimport time\nimport gzip\nimport shutil\n# Allow accesing files relative to this file\nlocation = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.join(location, \'../../\'))\nfrom ocr.viz import print_progress_bar\n\n\ndef extract(location, output, number=5):\n    output = os.path.join(location, output)\n    if not os.path.exists(output):\n        os.makedirs(output)\n\n    for sub in [\'lob\', \'numbers\']:\n        folder = os.path.join(location, sub)\n        seg_files = glob.glob(os.path.join(folder, \'*.seg\'))\n        length = sum([int(open(l, \'r\').readline()) for l in seg_files])\n\n        itr = 0\n        for fl in seg_files:\n            # Uncompressing tiff files\n            with gzip.open(fl[:-4] + \'.tiff.gz\', \'rb\') as f_in:\n                with open(fl[:-4] + \'.tiff\', \'wb\') as f_out:\n                    shutil.copyfileobj(f_in, f_out)\n            image = cv2.imread(fl[:-4] + "".tiff"")\n            with open(fl) as f:\n                f.readline()\n                for line in f:\n                    rect = [int(val) for val in line.strip().split(\' \')[1:]]\n                    word = line.split(\' \')[0].split(\'_\')[0]\n                    im = image[rect[2]:rect[3], rect[0]:rect[1]]\n\n                    if 0 not in im.shape:\n                        cv2.imwrite(\n                            os.path.join(\n                                output,\n                                \'%s_%s_%s.png\' % (word, number, time.time())),\n                            im)\n                    print_progress_bar(itr, length)\n                    itr += 1\n\n    print(""\\tNumber of words:"", len([n for n in os.listdir(output)]))\n'"
src/data/datasets/cvl.py,0,"b'import unidecode\nimport glob\nimport os\nimport sys\nimport time\nimport re\nfrom PIL import Image\n# Allow accesing files relative to this file\nlocation = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.join(location, \'../../\'))\nfrom ocr.viz import print_progress_bar\n\n\ndef extract(location, output, number=3):\n    output = os.path.join(location, output)\n    if not os.path.exists(output):\n        os.makedirs(output)\n\n    for sub in [\'cvl-database-1-1/testset\', \'cvl-database-1-1/trainset\']:\n        folder = os.path.join(location, sub)\n        images = glob.glob(os.path.join(folder, \'words\', \'*\', \'*.tif\'))\n\n        for i, im in enumerate(images):\n            word = re.search(\'\\/\\d+-\\d+-\\d+-\\d+-(.+?).tif\', im).group(1)\n            word = unidecode.unidecode(word)\n\n            if os.stat(im).st_size != 0:\n                outpath = os.path.join(\n                    output,\n                    \'%s_%s_%s.png\' % (word, number, time.time()))\n                Image.open(im).save(outpath)\n            print_progress_bar(i, len(images))\n\n    print(""\\tNumber of words:"", len([n for n in os.listdir(output)]))\n'"
src/data/datasets/iam.py,0,"b'import time\nimport os\nimport sys\nfrom shutil import copyfile\n# Allow accesing files relative to this file\nlocation = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.join(location, \'../../\'))\nfrom ocr.viz import print_progress_bar\n\n\n# Words with these characters are removed\n# you have to extend the alphabet in order to use them (ocr/datahelpers.py)\nprohibited = [\',\', \'(\', \')\', \';\', \':\', \'/\', \'\\\\\',\n              \'#\', \'""\', \'?\', \'!\', \'*\', \'_\', \'&\']\n\n\ndef extract(location, output, number=2):\n    output = os.path.join(location, output)\n    err_output = os.path.join(location, \'words_with_error\')\n    if not os.path.exists(output):\n        os.makedirs(output)\n    if not os.path.exists(err_output):\n        os.makedirs(err_output)\n\n    folder = os.path.join(location, \'words\')\n    label_file = os.path.join(location, \'words.txt\')\n    length = len(open(label_file).readlines())\n\n    with open(label_file) as fp:\n        for i, line in enumerate(fp):\n            if line[0] != \'#\':\n                l = line.strip().split("" "")\n                impath = os.path.join(\n                    folder,\n                    l[0].split(\'-\')[0],\n                    l[0].split(\'-\')[0] + \'-\' + l[0].split(\'-\')[1],\n                    l[0] + \'.png\')\n                word = l[-1]\n\n                if (os.stat(impath).st_size != 0\n                    and word not in [\'.\', \'-\', ""\'""]\n                    and not any(i in word for i in prohibited)):\n\n                    out = output if l[1] == \'ok\' else err_output\n                    outpath = os.path.join(\n                        out, ""%s_%s_%s.png"" % (word, number, time.time()))\n                    copyfile(impath, outpath)\n\n            print_progress_bar(i, length)\n    print(""\\tNumber of words:"", len([n for n in os.listdir(output)]))\n'"
src/data/datasets/orand.py,0,"b'import glob\nimport os\nimport sys\nfrom shutil import copyfile\nimport time\n# Allow accesing files relative to this file\nlocation = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.join(location, \'../../\'))\nfrom ocr.viz import print_progress_bar\n\n\ndef extract(location, output, number=4):\n    output = os.path.join(location, output)\n    if not os.path.exists(output):\n        os.makedirs(output)\n\n    for sub in [\'ORAND-CAR-2014/CAR-A\', \'ORAND-CAR-2014/CAR-B\']:\n        folder = os.path.join(location, sub)\n        l_files = glob.glob(os.path.join(folder, \'*.txt\'))\n        length = sum(1 for fl in l_files for line in open(fl))\n\n        itr = 0\n        for fl in l_files:\n            im_folder = fl[:-6] + \'images\'\n            with open(fl) as f:\n                for line in f:\n                    im, word = line.strip().split(\'\\t\')\n                    impath = os.path.join(im_folder, im)\n\n                    if os.stat(impath).st_size != 0:\n                        outpath = os.path.join(\n                            output,\n                            \'%s_%s_%s.png\' % (word, number, time.time()))\n                        copyfile(impath, outpath)\n                    print_progress_bar(itr, length)\n                    itr += 1\n\n    print(""\\tNumber of words:"", len([n for n in os.listdir(output)]))\n'"
