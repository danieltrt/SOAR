file_path,api_count,code
src/gpu_lock.py,0,"b'#!/usr/bin/python\n\n""""""\nA simple discretionary locking system for /dev/nvidia devices.\n\nIain Murray, November 2009, January 2010, January 2011.\n""""""\n\nimport os, os.path\n\n_dev_prefix = \'/dev/nvidia\'\n#URL = \'http://www.cs.toronto.edu/~murray/code/gpu_monitoring/\'\nURL = \'http://homepages.inf.ed.ac.uk/imurray2/code/gpu_monitoring/\'\n\n\n# Get ID\'s of NVIDIA boards. Should do this through a CUDA call, but this is\n# a quick and dirty way that works for now:\ndef board_ids():\n    """"""Returns integer board ids available on this machine.""""""\n    from glob import glob\n    board_devs = glob(_dev_prefix + \'[0-9]*\')\n    return list(range(len(board_devs)))\n\ndef _lock_file(id):\n    """"""lock file from integer id""""""\n    # /tmp is cleared on reboot on many systems, but it doesn\'t have to be\n    if os.path.exists(\'/dev/shm\'):\n        # /dev/shm on linux machines is a RAM disk, so is definitely cleared\n        return \'/dev/shm/gpu_lock_%d\' % id\n    else:\n        return \'/tmp/gpu_lock_%d\' % id\n\ndef owner_of_lock(id):\n    """"""Username that has locked the device id. (Empty string if no lock).""""""\n    import pwd\n    try:\n        statinfo = os.lstat(_lock_file(id))\n        return pwd.getpwuid(statinfo.st_uid).pw_name\n    except:\n        return """"\n\ndef _obtain_lock(id):\n    """"""Attempts to lock id, returning success as True/False.""""""\n#    print   id\n    try:\n        # On POSIX systems symlink creation is atomic, so this should be a\n        # robust locking operation:\n        os.symlink(\'/dev/null\', _lock_file(id))\n        return True\n    except:\n        return False\n\ndef _launch_reaper(id, pid):\n    """"""Start a process that will free a lock when process pid terminates""""""\n    from subprocess import Popen, PIPE\n    me = __file__\n    if me.endswith(\'.pyc\'):\n        me = me[:-1]\n    myloc = os.path.dirname(me)\n    if not myloc:\n        myloc = os.getcwd()\n    reaper_cmd = os.path.join(myloc, \'run_on_me_or_pid_quit\')\n    Popen([reaper_cmd, str(pid), me, \'--free\', str(id)],\n        stdout=open(\'/dev/null\', \'w\'))\n\ndef obtain_lock_id(pid = None):\n    """"""\n    Finds a free id, locks it and returns integer id, or -1 if none free.\n\n    A process is spawned that will free the lock automatically when the\n    process pid (by default the current python process) terminates.\n    """"""\n    id = -1\n    id = obtain_lock_id_to_hog()\n    try:\n        if id >= 0:\n            if pid is None:\n                pid = os.getpid()\n            _launch_reaper(id, pid)\n    except:\n        free_lock(id)\n        id = -1\n    return id\n\ndef obtain_lock_id_to_hog():\n    """"""\n    Finds a free id, locks it and returns integer id, or -1 if none free.\n\n    * Lock must be freed manually *\n    """"""\n    for id in board_ids():\n        if _obtain_lock(id):\n            return id\n    return -1\n\ndef free_lock(id):\n    """"""Attempts to free lock id, returning success as True/False.""""""\n    try:\n        filename = _lock_file(id)\n        # On POSIX systems os.rename is an atomic operation, so this is the safe\n        # way to delete a lock:\n        os.rename(filename, filename + \'.redundant\')\n        os.remove(filename + \'.redundant\')\n        return True\n    except:\n        return False\n\n\n# If run as a program:\nif __name__ == ""__main__"":\n    import sys\n    me = sys.argv[0]\n    # Report\n    if \'--id\' in sys.argv:\n        if len(sys.argv) > 2:\n            try:\n                pid = int(sys.argv[2])\n                print(pid, sys.argv[2])\n                assert(os.path.exists(\'/proc/%d\' % pid))\n            except:\n                print(\'Usage: %s --id [pid_to_wait_on]\' % me)\n                print(\'The optional process id must exist if specified.\')\n                print(\'Otherwise the id of the parent process is used.\')\n                sys.exit(1)\n        else:\n            pid = os.getppid()\n            print(pid)\n        print(obtain_lock_id(pid))\n    elif \'--id-to-hog\' in sys.argv:\n        print(obtain_lock_id_to_hog())\n    elif \'--free\' in sys.argv:\n        try:\n            id = int(sys.argv[2])\n        except:\n            print(\'Usage: %s --free <id>\' % me)\n            sys.exit(1)\n        if free_lock(id):\n            print(""Lock freed"")\n        else:\n            owner = owner_of_lock(id)\n            if owner:\n                print(""Failed to free lock id=%d owned by %s"" % (id, owner))\n            else:\n                print(""Failed to free lock, but it wasn\'t actually set?"")\n    else:\n        print(\'\\n  Usage instructions:\\n\')\n        print(\'  To obtain and lock an id: %s --id\' % me)\n        print(\'  The lock is automatically freed when the parent terminates\')\n        print()\n        print(""  To get an id that won\'t be freed: %s --id-to-hog"" % me)\n        print(""  You *must* manually free these ids: %s --free <id>\\n"" % me)\n        print(\'  More info: %s\\n\' % URL)\n        div = \'  \' + ""-""*60\n        print(\'\\n\' + div)\n        print(""  NVIDIA board users:"")\n        print(div)\n        for id in board_ids():\n            print(""      Board %d: %s"" % (id, owner_of_lock(id)))\n        print(div + \'\\n\')\n'"
src/run_keras_with_merlin_io.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://github.com/CSTR-Edinburgh/merlin\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\nimport os\nimport sys\nimport time\n\nfrom keras_lib import configuration\nfrom keras_lib import data_utils\nfrom keras_lib.train import TrainKerasModels\n\nclass KerasClass(object):\n\n    def __init__(self, cfg):\n\n        ###################################################\n        ########## User configurable variables ############\n        ###################################################\n\n        inp_feat_dir  = cfg.inp_feat_dir\n        out_feat_dir  = cfg.out_feat_dir\n        pred_feat_dir = cfg.pred_feat_dir\n\n        inp_file_ext = cfg.inp_file_ext\n        out_file_ext = cfg.out_file_ext\n\n        ### Input-Output ###\n\n        self.inp_dim = cfg.inp_dim\n        self.out_dim = cfg.out_dim\n\n        self.inp_norm = cfg.inp_norm\n        self.out_norm = cfg.out_norm\n\n        self.inp_stats_file = cfg.inp_stats_file\n        self.out_stats_file = cfg.out_stats_file\n\n        self.inp_scaler = None\n        self.out_scaler = None\n\n        #### define model params ####\n\n        self.hidden_layer_type = cfg.hidden_layer_type\n        self.hidden_layer_size = cfg.hidden_layer_size\n\n        self.sequential_training = cfg.sequential_training\n\n        self.stateful      = cfg.stateful\n        self.batch_size    = cfg.batch_size\n        self.seq_length    = cfg.seq_length\n\n        self.training_algo = cfg.training_algo\n        self.shuffle_data  = cfg.shuffle_data\n\n        self.output_layer_type = cfg.output_layer_type\n        self.loss_function     = cfg.loss_function\n        self.optimizer         = cfg.optimizer\n\n        self.rnn_params    = cfg.rnn_params\n        self.dropout_rate  = cfg.dropout_rate\n        self.num_of_epochs = cfg.num_of_epochs\n\n        self.json_model_file = cfg.json_model_file\n        self.h5_model_file   = cfg.h5_model_file\n\n        ### define train, valid, test ###\n\n        train_file_number = cfg.train_file_number\n        valid_file_number = cfg.valid_file_number\n        test_file_number  = cfg.test_file_number\n\n        file_id_scp  = cfg.file_id_scp\n        test_id_scp  = cfg.test_id_scp\n\n        #### main processess ####\n        \n        self.NORMDATA   = cfg.NORMDATA\n        self.TRAINMODEL = cfg.TRAINMODEL\n        self.TESTMODEL  = cfg.TESTMODEL\n\n        #### Generate only test list ####\n        self.GenTestList = cfg.GenTestList\n        \n        ###################################################\n        ####### End of user-defined conf variables ########\n        ###################################################\n\n        #### Create train, valid and test file lists ####\n        file_id_list = data_utils.read_file_list(file_id_scp)\n\n        train_id_list = file_id_list[0: train_file_number]\n        valid_id_list = file_id_list[train_file_number: train_file_number + valid_file_number]\n        test_id_list  = file_id_list[train_file_number + valid_file_number: train_file_number + valid_file_number + test_file_number]\n        \n        valid_test_id_list = file_id_list[train_file_number: train_file_number + valid_file_number + test_file_number]\n\n        self.inp_train_file_list = data_utils.prepare_file_path_list(train_id_list, inp_feat_dir, inp_file_ext)\n        self.out_train_file_list = data_utils.prepare_file_path_list(train_id_list, out_feat_dir, out_file_ext)\n\n        self.inp_valid_file_list = data_utils.prepare_file_path_list(valid_id_list, inp_feat_dir, inp_file_ext)\n        self.out_valid_file_list = data_utils.prepare_file_path_list(valid_id_list, out_feat_dir, out_file_ext)\n\n        self.inp_test_file_list = data_utils.prepare_file_path_list(valid_test_id_list, inp_feat_dir, inp_file_ext)\n        self.out_test_file_list = data_utils.prepare_file_path_list(valid_test_id_list, out_feat_dir, out_file_ext)\n\n        self.gen_test_file_list = data_utils.prepare_file_path_list(valid_test_id_list, pred_feat_dir, out_file_ext)\n\n        if self.GenTestList:\n            test_id_list = data_utils.read_file_list(test_id_scp)\n            self.inp_test_file_list = data_utils.prepare_file_path_list(test_id_list, inp_feat_dir, inp_file_ext)\n            self.gen_test_file_list = data_utils.prepare_file_path_list(test_id_list, pred_feat_dir, out_file_ext)\n\n        #### Define keras models class ####\n        self.keras_models = TrainKerasModels(self.inp_dim, self.hidden_layer_size, self.out_dim, self.hidden_layer_type,\n                                                output_type=self.output_layer_type, dropout_rate=self.dropout_rate,\n                                                loss_function=self.loss_function, optimizer=self.optimizer,\n                                                rnn_params=self.rnn_params)\n\n    def normlize_data(self):\n        ### normalize train data ###\n        if os.path.isfile(self.inp_stats_file) and os.path.isfile(self.out_stats_file):\n            self.inp_scaler = data_utils.load_norm_stats(self.inp_stats_file, self.inp_dim, method=self.inp_norm)\n            self.out_scaler = data_utils.load_norm_stats(self.out_stats_file, self.out_dim, method=self.out_norm)\n        else:\n            print(\'preparing train_x, train_y from input and output feature files...\')\n            train_x, train_y, train_flen = data_utils.read_data_from_file_list(self.inp_train_file_list, self.out_train_file_list,\n                                                                            self.inp_dim, self.out_dim, sequential_training=self.sequential_training)\n\n            print(\'computing norm stats for train_x...\')\n            inp_scaler = data_utils.compute_norm_stats(train_x, self.inp_stats_file, method=self.inp_norm)\n\n            print(\'computing norm stats for train_y...\')\n            out_scaler = data_utils.compute_norm_stats(train_y, self.out_stats_file, method=self.out_norm)\n\n\n    def train_keras_model(self):\n        #### define the model ####\n        if not self.sequential_training:\n            self.keras_models.define_feedforward_model()\n        elif self.stateful:\n            self.keras_models.define_stateful_model(batch_size=self.batch_size, seq_length=self.seq_length)\n        else:\n            self.keras_models.define_sequence_model()\n\n        #### load the data ####\n        print(\'preparing train_x, train_y from input and output feature files...\')\n        train_x, train_y, train_flen = data_utils.read_data_from_file_list(self.inp_train_file_list, self.out_train_file_list,\n                                                                            self.inp_dim, self.out_dim, sequential_training=self.sequential_training)\n        print(\'preparing valid_x, valid_y from input and output feature files...\')\n        valid_x, valid_y, valid_flen = data_utils.read_data_from_file_list(self.inp_valid_file_list, self.out_valid_file_list,\n                                                                            self.inp_dim, self.out_dim, sequential_training=self.sequential_training)\n\n        #### normalize the data ####\n        data_utils.norm_data(train_x, self.inp_scaler, sequential_training=self.sequential_training)\n        data_utils.norm_data(train_y, self.out_scaler, sequential_training=self.sequential_training)\n        data_utils.norm_data(valid_x, self.inp_scaler, sequential_training=self.sequential_training)\n        data_utils.norm_data(valid_y, self.out_scaler, sequential_training=self.sequential_training)\n\n        #### train the model ####\n        print(\'training...\')\n        if not self.sequential_training:\n            ### Train feedforward model ###\n            self.keras_models.train_feedforward_model(train_x, train_y, valid_x, valid_y, batch_size=self.batch_size, num_of_epochs=self.num_of_epochs, shuffle_data=self.shuffle_data)\n        else:\n            ### Train recurrent model ###\n            print((\'training algorithm: %d\' % (self.training_algo)))\n            self.keras_models.train_sequence_model(train_x, train_y, valid_x, valid_y, train_flen, batch_size=self.batch_size, num_of_epochs=self.num_of_epochs,\n                                                                                        shuffle_data=self.shuffle_data, training_algo=self.training_algo)\n\n        #### store the model ####\n        self.keras_models.save_model(self.json_model_file, self.h5_model_file)\n\n    def test_keras_model(self):\n        #### load the model ####\n        self.keras_models.load_model(self.json_model_file, self.h5_model_file)\n\n        #### load the data ####\n        print(\'preparing test_x from input feature files...\')\n        test_x, test_flen = data_utils.read_test_data_from_file_list(self.inp_test_file_list, self.inp_dim)\n\n        #### normalize the data ####\n        data_utils.norm_data(test_x, self.inp_scaler)\n\n        #### compute predictions ####\n        self.keras_models.predict(test_x, self.out_scaler, self.gen_test_file_list, self.sequential_training)\n\n    def main_function(self):\n        ### Implement each module ###\n        if self.NORMDATA:\n            self.normlize_data()\n\n        if self.TRAINMODEL:\n            self.train_keras_model()\n\n        if self.TESTMODEL:\n            self.test_keras_model()\n\nif __name__ == ""__main__"":\n\n    if len(sys.argv) != 2:\n        print(\'usage: python run_keras_with_merlin_io.py [config file name]\')\n        sys.exit(1)\n\n    # create a configuration instance\n    # and get a short name for this instance\n    cfg = configuration.configuration()\n\n    config_file = sys.argv[1]\n\n    config_file = os.path.abspath(config_file)\n    cfg.configure(config_file)\n\n    print(""--- Job started ---"")\n    start_time = time.time()\n\n    # main function\n    keras_instance = KerasClass(cfg)\n    keras_instance.main_function()\n\n    (m, s) = divmod(int(time.time() - start_time), 60)\n    print((""--- Job completion time: %d min. %d sec ---"" % (m, s)))\n\n    sys.exit(0)\n'"
src/run_merlin.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://github.com/CSTR-Edinburgh/merlin\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\nimport pickle\nimport gzip\nimport os, sys, errno\nimport time\nimport math\n\nimport subprocess\nimport socket # only for socket.getfqdn()\nimport multiprocessing\n\n#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)\nimport numpy\n#import gnumpy as gnp\n# we need to explicitly import this in some cases, not sure why this doesn\'t get imported with numpy itself\nimport numpy.distutils.__config__\n# and only after that can we import theano\nimport theano\n\nfrom utils.providers import ListDataProvider\n\nfrom frontend.label_normalisation import HTSLabelNormalisation\nfrom frontend.silence_remover import SilenceRemover\nfrom frontend.silence_remover import trim_silence\nfrom frontend.min_max_norm import MinMaxNormalisation\nfrom frontend.acoustic_composition import AcousticComposition\nfrom frontend.parameter_generation import ParameterGeneration\nfrom frontend.mean_variance_norm import MeanVarianceNorm\n\n# the new class for label composition and normalisation\nfrom frontend.label_composer import LabelComposer\nfrom frontend.label_modifier import HTSLabelModification\nfrom frontend.merge_features import MergeFeat\n\nimport configuration\nfrom models.deep_rnn import DeepRecurrentNetwork\n\nfrom utils.compute_distortion import DistortionComputation, IndividualDistortionComp\nfrom utils.generate import generate_wav\nfrom utils.acous_feat_extraction import acous_feat_extraction\nfrom utils.learn_rates import ExpDecreaseLearningRate\n\nfrom io_funcs.binary_io import  BinaryIOCollection\n\n# our custom logging class that can also plot\nfrom logplot.logging_plotting import LoggerPlotter, MultipleSeriesPlot, SingleWeightMatrixPlot\nimport logging # as logging\nimport logging.config\nimport io\nfrom utils.file_paths import FilePaths\nfrom utils.utils import read_file_list, prepare_file_path_list\n\n\ndef extract_file_id_list(file_list):\n    file_id_list = []\n    for file_name in file_list:\n        file_id = os.path.basename(os.path.splitext(file_name)[0])\n        file_id_list.append(file_id)\n\n    return  file_id_list\n\ndef make_output_file_list(out_dir, in_file_lists):\n    out_file_lists = []\n\n    for in_file_name in in_file_lists:\n        file_id = os.path.basename(in_file_name)\n        out_file_name = out_dir + \'/\' + file_id\n        out_file_lists.append(out_file_name)\n\n    return  out_file_lists\n\ndef visualize_dnn(dnn):\n\n    plotlogger = logging.getLogger(""plotting"")\n\n        # reference activation weights in layers\n    W = list(); layer_name = list()\n    for i in range(len(dnn.params)):\n        aa = dnn.params[i].get_value(borrow=True).T\n        print(aa.shape, aa.size)\n        if aa.size > aa.shape[0]:\n            W.append(aa)\n            layer_name.append(dnn.params[i].name)\n\n    ## plot activation weights including input and output\n    layer_num = len(W)\n    for i_layer in range(layer_num):\n        fig_name = \'Activation weights W\' + str(i_layer) + \'_\' + layer_name[i_layer]\n        fig_title = \'Activation weights of W\' + str(i_layer)\n        xlabel = \'Neuron index of hidden layer \' + str(i_layer)\n        ylabel = \'Neuron index of hidden layer \' + str(i_layer+1)\n        if i_layer == 0:\n            xlabel = \'Input feature index\'\n        if i_layer == layer_num-1:\n            ylabel = \'Output feature index\'\n        logger.create_plot(fig_name, SingleWeightMatrixPlot)\n        plotlogger.add_plot_point(fig_name, fig_name, W[i_layer])\n        plotlogger.save_plot(fig_name, title=fig_name, xlabel=xlabel, ylabel=ylabel)\n\n\ndef load_covariance(var_file_dict, out_dimension_dict):\n    var = {}\n    io_funcs = BinaryIOCollection()\n    for feature_name in list(var_file_dict.keys()):\n        var_values, dimension = io_funcs.load_binary_file_frame(var_file_dict[feature_name], 1)\n\n        var_values = numpy.reshape(var_values, (out_dimension_dict[feature_name], 1))\n\n        var[feature_name] = var_values\n\n    return  var\n\n\ndef train_DNN(train_xy_file_list, valid_xy_file_list, \\\n              nnets_file_name, n_ins, n_outs, ms_outs, hyper_params, buffer_size, plot=False, var_dict=None,\n              cmp_mean_vector = None, cmp_std_vector = None, init_dnn_model_file = None):\n\n    # get loggers for this function\n    # this one writes to both console and file\n    logger = logging.getLogger(""main.train_DNN"")\n    logger.debug(\'Starting train_DNN\')\n\n    if plot:\n        # this one takes care of plotting duties\n        plotlogger = logging.getLogger(""plotting"")\n        # create an (empty) plot of training convergence, ready to receive data points\n        logger.create_plot(\'training convergence\',MultipleSeriesPlot)\n\n    try:\n        assert numpy.sum(ms_outs) == n_outs\n    except AssertionError:\n        logger.critical(\'the summation of multi-stream outputs does not equal to %d\' %(n_outs))\n        raise\n\n    ####parameters#####\n    finetune_lr     = float(hyper_params[\'learning_rate\'])\n    training_epochs = int(hyper_params[\'training_epochs\'])\n    batch_size      = int(hyper_params[\'batch_size\'])\n    l1_reg          = float(hyper_params[\'l1_reg\'])\n    l2_reg          = float(hyper_params[\'l2_reg\'])\n    warmup_epoch    = int(hyper_params[\'warmup_epoch\'])\n    momentum        = float(hyper_params[\'momentum\'])\n    warmup_momentum = float(hyper_params[\'warmup_momentum\'])\n\n    hidden_layer_size = hyper_params[\'hidden_layer_size\']\n\n    buffer_utt_size = buffer_size\n    early_stop_epoch = int(hyper_params[\'early_stop_epochs\'])\n\n    hidden_activation = hyper_params[\'hidden_activation\']\n    output_activation = hyper_params[\'output_activation\']\n\n    model_type = hyper_params[\'model_type\']\n    hidden_layer_type  = hyper_params[\'hidden_layer_type\']\n\n    ## use a switch to turn on pretraining\n    ## pretraining may not help too much, if this case, we turn it off to save time\n    do_pretraining = hyper_params[\'do_pretraining\']\n    pretraining_epochs = int(hyper_params[\'pretraining_epochs\'])\n    pretraining_lr = float(hyper_params[\'pretraining_lr\'])\n\n    sequential_training = hyper_params[\'sequential_training\']\n    dropout_rate = hyper_params[\'dropout_rate\']\n\n    buffer_size = int(buffer_size / batch_size) * batch_size\n\n    ###################\n    (train_x_file_list, train_y_file_list) = train_xy_file_list\n    (valid_x_file_list, valid_y_file_list) = valid_xy_file_list\n\n    logger.debug(\'Creating training   data provider\')\n    train_data_reader = ListDataProvider(x_file_list = train_x_file_list, y_file_list = train_y_file_list,\n                            n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, \n                            sequential = sequential_training, shuffle = True)\n\n    logger.debug(\'Creating validation data provider\')\n    valid_data_reader = ListDataProvider(x_file_list = valid_x_file_list, y_file_list = valid_y_file_list,\n                            n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, \n                            sequential = sequential_training, shuffle = False)\n\n    if cfg.rnn_batch_training:\n        train_data_reader.set_rnn_params(training_algo=cfg.training_algo, batch_size=cfg.batch_size, seq_length=cfg.seq_length, merge_size=cfg.merge_size, bucket_range=cfg.bucket_range)\n        valid_data_reader.reshape_input_output()\n    \n    shared_train_set_xy, temp_train_set_x, temp_train_set_y = train_data_reader.load_one_partition()\n    train_set_x, train_set_y = shared_train_set_xy\n    shared_valid_set_xy, temp_valid_set_x, temp_valid_set_y = valid_data_reader.load_one_partition()\n    valid_set_x, valid_set_y = shared_valid_set_xy\n    train_data_reader.reset()\n    valid_data_reader.reset()\n\n\n    ##temporally we use the training set as pretrain_set_x.\n    ##we need to support any data for pretraining\n\n    # numpy random generator\n    numpy_rng = numpy.random.RandomState(123)\n    logger.info(\'building the model\')\n\n\n    dnn_model = None\n    pretrain_fn = None  ## not all the model support pretraining right now\n    train_fn = None\n    valid_fn = None\n    valid_model = None ## valid_fn and valid_model are the same. reserve to computer multi-stream distortion\n    if model_type == \'DNN\':\n        dnn_model = DeepRecurrentNetwork(n_in= n_ins, hidden_layer_size = hidden_layer_size, n_out = n_outs,\n                                         L1_reg = l1_reg, L2_reg = l2_reg, hidden_layer_type = hidden_layer_type, output_type = cfg.output_layer_type,\n                                         dropout_rate = dropout_rate, optimizer = cfg.optimizer, rnn_batch_training = cfg.rnn_batch_training)\n\n    else:\n        logger.critical(\'%s type NN model is not supported!\' %(model_type))\n        raise\n\n    ## Model adaptation -- fine tuning the existing model\n    ## We can\'t just unpickle the old model and use that because fine-tune functions\n    ## depend on opt_l2e option used in construction of initial model. One way around this\n    ## would be to unpickle, manually set unpickled_dnn_model.opt_l2e=True and then call\n    ## unpickled_dnn_model.build_finetne_function() again. This is another way, construct\n    ## new model from scratch with opt_l2e=True, then copy existing weights over:\n    use_lhuc = cfg.use_lhuc\n    if init_dnn_model_file != ""_"":\n        logger.info(\'load parameters from existing model: %s\' %(init_dnn_model_file))\n        if not os.path.isfile(init_dnn_model_file):\n            sys.exit(\'Model file %s does not exist\'%(init_dnn_model_file))\n        existing_dnn_model = pickle.load(open(init_dnn_model_file, \'rb\'))\n        if not use_lhuc and not len(existing_dnn_model.params) == len(dnn_model.params):\n            sys.exit(\'Old and new models have different numbers of weight matrices\')\n        elif use_lhuc and len(dnn_model.params) < len(existing_dnn_model.params):\n            sys.exit(\'In LHUC adaptation new model must have more parameters than old model.\')\n        # assign the existing dnn model parameters to the new dnn model\n        k = 0\n        for i in range(len(dnn_model.params)):\n            ## Added for LHUC ##\n            # In LHUC, we keep all the old parameters intact and learn only a small set of new\n            # parameters\n            if dnn_model.params[i].name == \'c\':\n                continue\n            else:\n                old_val = existing_dnn_model.params[k].get_value()\n                new_val = dnn_model.params[i].get_value()\n                if numpy.shape(old_val) == numpy.shape(new_val):\n                    dnn_model.params[i].set_value(old_val)\n                else:\n                    sys.exit(\'old and new weight matrices have different shapes\')\n                k = k + 1        \n    train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y), use_lhuc, layer_index=cfg.freeze_layers)  #, batch_size=batch_size\n    logger.info(\'fine-tuning the %s model\' %(model_type))\n\n    start_time = time.time()\n\n    best_dnn_model = dnn_model\n    best_validation_loss = sys.float_info.max\n    previous_loss = sys.float_info.max\n\n    lr_decay  = cfg.lr_decay\n    if lr_decay>0:\n        early_stop_epoch *= lr_decay\n\n    early_stop = 0\n    val_loss_counter = 0\n\n    previous_finetune_lr = finetune_lr\n\n    epoch = 0\n    while (epoch < training_epochs):\n        epoch = epoch + 1\n        \n        if lr_decay==0:\n            # fixed learning rate \n            reduce_lr = False\n        elif lr_decay<0:\n            # exponential decay\n            reduce_lr = False if epoch <= warmup_epoch else True\n        elif val_loss_counter > 0:\n            # linear decay\n            reduce_lr = False\n            if val_loss_counter%lr_decay==0:\n                reduce_lr = True\n                val_loss_counter = 0\n        else:\n            # no decay\n            reduce_lr = False\n\n        if reduce_lr:\n            current_finetune_lr = previous_finetune_lr * 0.5\n            current_momentum    = momentum\n        else:\n            current_finetune_lr = previous_finetune_lr\n            current_momentum    = warmup_momentum\n        \n        previous_finetune_lr = current_finetune_lr\n\n        train_error = []\n        sub_start_time = time.time()\n\n        logger.debug(""training params -- learning rate: %f, early_stop: %d/%d"" % (current_finetune_lr, early_stop, early_stop_epoch))\n        while (not train_data_reader.is_finish()):\n\n            _, temp_train_set_x, temp_train_set_y = train_data_reader.load_one_partition()\n\n            # if sequential training, the batch size will be the number of frames in an utterance\n            # batch_size for sequential training is considered only when rnn_batch_training is set to True\n            if sequential_training == True:\n                batch_size = temp_train_set_x.shape[0]\n\n            n_train_batches = temp_train_set_x.shape[0] // batch_size\n            for index in range(n_train_batches):\n                ## send a batch to the shared variable, rather than pass the batch size and batch index to the finetune function\n                train_set_x.set_value(numpy.asarray(temp_train_set_x[index*batch_size:(index + 1)*batch_size], dtype=theano.config.floatX), borrow=True)\n                train_set_y.set_value(numpy.asarray(temp_train_set_y[index*batch_size:(index + 1)*batch_size], dtype=theano.config.floatX), borrow=True)\n\n                this_train_error = train_fn(current_finetune_lr, current_momentum)\n\n                train_error.append(this_train_error)\n\n        train_data_reader.reset()\n\n        logger.debug(\'calculating validation loss\')\n        validation_losses = []\n        while (not valid_data_reader.is_finish()):\n            shared_valid_set_xy, temp_valid_set_x, temp_valid_set_y = valid_data_reader.load_one_partition()\n            valid_set_x.set_value(numpy.asarray(temp_valid_set_x, dtype=theano.config.floatX), borrow=True)\n            valid_set_y.set_value(numpy.asarray(temp_valid_set_y, dtype=theano.config.floatX), borrow=True)\n\n            this_valid_loss = valid_fn()\n\n            validation_losses.append(this_valid_loss)\n        valid_data_reader.reset()\n\n        this_validation_loss = numpy.mean(validation_losses)\n\n        this_train_valid_loss = numpy.mean(numpy.asarray(train_error))\n\n        sub_end_time = time.time()\n\n        loss_difference = this_validation_loss - previous_loss\n\n        logger.info(\'epoch %i, validation error %f, train error %f  time spent %.2f\' %(epoch, this_validation_loss, this_train_valid_loss, (sub_end_time - sub_start_time)))\n        if plot:\n            plotlogger.add_plot_point(\'training convergence\',\'validation set\',(epoch,this_validation_loss))\n            plotlogger.add_plot_point(\'training convergence\',\'training set\',(epoch,this_train_valid_loss))\n            plotlogger.save_plot(\'training convergence\',title=\'Progress of training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n\n        if this_validation_loss < best_validation_loss:\n            pickle.dump(best_dnn_model, open(nnets_file_name, \'wb\'))\n\n            best_dnn_model = dnn_model\n            best_validation_loss = this_validation_loss\n\n        if this_validation_loss >= previous_loss:\n            logger.debug(\'validation loss increased\')\n            val_loss_counter+=1\n            early_stop+=1\n\n        if epoch > 15 and early_stop > early_stop_epoch:\n            logger.debug(\'stopping early\')\n            break\n\n        if math.isnan(this_validation_loss):\n            break\n\n        previous_loss = this_validation_loss\n\n    end_time = time.time()\n\n    logger.info(\'overall  training time: %.2fm validation error %f\' % ((end_time - start_time) / 60., best_validation_loss))\n\n    if plot:\n        plotlogger.save_plot(\'training convergence\',title=\'Final training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n\n    return  best_validation_loss\n\n\ndef dnn_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list, reshape_io=False):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):  #file_number\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size // n_ins))]\n        test_set_x = features.reshape((-1, n_ins))\n        n_rows = test_set_x.shape[0]\n        \n        if reshape_io:\n            test_set_x = numpy.reshape(test_set_x, (1, test_set_x.shape[0], n_ins))\n            test_set_x = numpy.array(test_set_x, \'float32\')\n\n        predicted_parameter = dnn_model.parameter_prediction(test_set_x)\n        predicted_parameter = predicted_parameter.reshape(-1, n_outs)\n        predicted_parameter = predicted_parameter[0:n_rows]\n        \n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\n##generate bottleneck layer as features\ndef dnn_hidden_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list, bottleneck_index):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size // n_ins))]\n        features = features.reshape((-1, n_ins))\n        temp_set_x = features.tolist()\n        test_set_x = theano.shared(numpy.asarray(temp_set_x, dtype=theano.config.floatX))\n\n        predicted_parameter = dnn_model.generate_hidden_layer(test_set_x, bottleneck_index)\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\n\ndef perform_acoustic_composition_on_split(args):\n    """""" Performs acoustic composition on one chunk of data.\n        This is used as input for Pool.map to allow parallel acoustic composition.\n    """"""\n    (delta_win, acc_win, in_file_list_dict, nn_cmp_file_list, in_dimension_dict, out_dimension_dict) = args\n    acoustic_worker = AcousticComposition(delta_win = delta_win, acc_win = acc_win)\n    acoustic_worker.prepare_nn_data(in_file_list_dict, nn_cmp_file_list, in_dimension_dict, out_dimension_dict)\n\n\ndef perform_acoustic_composition(delta_win, acc_win, in_file_list_dict, nn_cmp_file_list, cfg, parallel=True):\n    """""" Runs acoustic composition from in_file_list_dict to nn_cmp_file_list.\n        If parallel is true, splits the data into multiple chunks and calls\n        perform_acoustic_composition_on_split for each chunk.\n    """"""\n    if parallel:\n        num_splits = multiprocessing.cpu_count()\n        pool = multiprocessing.Pool(num_splits)\n\n        # split data into a list of num_splits tuples with each tuple representing\n        # the parameters for perform_acoustic_compositon_on_split\n        splits_full = [\n             (delta_win,\n              acc_win,\n              {stream: in_file_list_dict[stream][i::num_splits] for stream in in_file_list_dict},\n              nn_cmp_file_list[i::num_splits],\n              cfg.in_dimension_dict,\n              cfg.out_dimension_dict\n             ) for i in range(num_splits) ]\n\n        pool.map(perform_acoustic_composition_on_split, splits_full)\n        pool.close()\n        pool.join()\n    else:\n        acoustic_worker = AcousticComposition(delta_win = delta_win, acc_win = acc_win)\n        acoustic_worker.prepare_nn_data(in_file_list_dict, nn_cmp_file_list, cfg.in_dimension_dict, cfg.out_dimension_dict)\n\n\ndef main_function(cfg):\n    file_paths = FilePaths(cfg)\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n    # get another logger to handle plotting duties\n    plotlogger = logging.getLogger(""plotting"")\n\n    # later, we might do this via a handler that is created, attached and configured\n    # using the standard config mechanism of the logging module\n    # but for now we need to do it manually\n    plotlogger.set_plot_path(cfg.plot_dir)\n\n    # create plot dir if set to True\n    if not os.path.exists(cfg.plot_dir) and cfg.plot:\n        os.makedirs(cfg.plot_dir)\n\n    #### parameter setting########\n    hidden_layer_size = cfg.hyper_params[\'hidden_layer_size\']\n\n    ####prepare environment\n    try:\n        file_id_list = read_file_list(cfg.file_id_scp)\n        logger.debug(\'Loaded file id list from %s\' % cfg.file_id_scp)\n    except IOError:\n        # this means that open(...) threw an error\n        logger.critical(\'Could not load file id list from %s\' % cfg.file_id_scp)\n        raise\n\n    ###total file number including training, development, and testing\n    total_file_number = len(file_id_list)\n\n    assert cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number == total_file_number, \'check train, valid, test file number\'\n\n    data_dir = cfg.data_dir\n\n    inter_data_dir = cfg.inter_data_dir\n    nn_cmp_dir       = file_paths.nn_cmp_dir\n    nn_cmp_norm_dir   = file_paths.nn_cmp_norm_dir\n    model_dir = file_paths.model_dir\n    gen_dir   = file_paths.gen_dir\n\n    in_file_list_dict = {}\n\n    for feature_name in list(cfg.in_dir_dict.keys()):\n        in_file_list_dict[feature_name] = prepare_file_path_list(file_id_list, cfg.in_dir_dict[feature_name], cfg.file_extension_dict[feature_name], False)\n\n    nn_cmp_file_list         = file_paths.get_nn_cmp_file_list()\n    nn_cmp_norm_file_list    = file_paths.get_nn_cmp_norm_file_list()\n\n    ###normalisation information\n    norm_info_file = file_paths.norm_info_file\n\n    ### normalise input full context label\n    # currently supporting two different forms of lingustic features\n    # later, we should generalise this\n\n    assert cfg.label_style == \'HTS\', \'Only HTS-style labels are now supported as input to Merlin\'\n\n    label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name, add_frame_features=cfg.add_frame_features, subphone_feats=cfg.subphone_feats)\n    add_feat_dim = sum(cfg.additional_features.values())\n    lab_dim = label_normaliser.dimension + add_feat_dim + cfg.appended_input_dim\n    if cfg.VoiceConversion:\n        lab_dim = cfg.cmp_dim\n    logger.info(\'Input label dimension is %d\' % lab_dim)\n    suffix=str(lab_dim)\n\n\n    if cfg.process_labels_in_work_dir:\n        inter_data_dir = cfg.work_dir\n\n    # the number can be removed\n    file_paths.set_label_dir(label_normaliser.dimension, suffix, lab_dim)\n    file_paths.set_label_file_list()\n\n    binary_label_dir      = file_paths.binary_label_dir\n    nn_label_dir          = file_paths.nn_label_dir\n    nn_label_norm_dir     = file_paths.nn_label_norm_dir\n\n    in_label_align_file_list = file_paths.in_label_align_file_list\n    binary_label_file_list   = file_paths.binary_label_file_list\n    nn_label_file_list       = file_paths.nn_label_file_list\n    nn_label_norm_file_list  = file_paths.nn_label_norm_file_list\n\n    min_max_normaliser = None\n\n    label_norm_file = file_paths.label_norm_file\n\n    test_id_list = file_paths.test_id_list\n\n    # Debug:----------------------------------\n    if cfg.ACFTEXTR:\n        logger.info(\'acoustic feature extraction\')\n        acous_feat_extraction(cfg.nat_wav_dir, file_id_list, cfg)\n        #generate_wav(gen_dir, file_id_list, cfg)     # generated speech\n\n\n\n    #-----------------------------------------\n\n    if cfg.NORMLAB:\n        # simple HTS labels\n        logger.info(\'preparing label data (input) using standard HTS style labels\')\n        label_normaliser.perform_normalisation(in_label_align_file_list, binary_label_file_list, label_type=cfg.label_type)\n\n        if cfg.additional_features:\n            out_feat_file_list = file_paths.out_feat_file_list\n            in_dim = label_normaliser.dimension\n\n            for new_feature, new_feature_dim in cfg.additional_features.items():\n                new_feat_dir  = os.path.join(data_dir, new_feature)\n                new_feat_file_list = prepare_file_path_list(file_id_list, new_feat_dir, \'.\'+new_feature)\n\n                merger = MergeFeat(lab_dim = in_dim, feat_dim = new_feature_dim)\n                merger.merge_data(binary_label_file_list, new_feat_file_list, out_feat_file_list)\n                in_dim += new_feature_dim\n\n                binary_label_file_list = out_feat_file_list\n\n        remover = SilenceRemover(n_cmp = lab_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type, remove_frame_features = cfg.add_frame_features, subphone_feats = cfg.subphone_feats)\n        remover.remove_silence(binary_label_file_list, in_label_align_file_list, nn_label_file_list)\n\n        min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99)\n\n        ###use only training data to find min-max information, then apply on the whole dataset\n        if cfg.GenTestList:\n            min_max_normaliser.load_min_max_values(label_norm_file)\n        else:\n            min_max_normaliser.find_min_max_values(nn_label_file_list[0:cfg.train_file_number])\n\n        ### enforce silence such that the normalization runs without removing silence: only for final synthesis\n        if cfg.GenTestList and cfg.enforce_silence:\n            min_max_normaliser.normalise_data(binary_label_file_list, nn_label_norm_file_list)\n        else:\n            min_max_normaliser.normalise_data(nn_label_file_list, nn_label_norm_file_list)\n\n\n\n    if min_max_normaliser != None and not cfg.GenTestList:\n        ### save label normalisation information for unseen testing labels\n        label_min_vector = min_max_normaliser.min_vector\n        label_max_vector = min_max_normaliser.max_vector\n        label_norm_info = numpy.concatenate((label_min_vector, label_max_vector), axis=0)\n\n        label_norm_info = numpy.array(label_norm_info, \'float32\')\n        fid = open(label_norm_file, \'wb\')\n        label_norm_info.tofile(fid)\n        fid.close()\n        logger.info(\'saved %s vectors to %s\' %(label_min_vector.size, label_norm_file))\n\n    ### make output duration data\n    if cfg.MAKEDUR:\n        logger.info(\'creating duration (output) features\')\n        label_normaliser.prepare_dur_data(in_label_align_file_list, file_paths.dur_file_list, cfg.label_type, cfg.dur_feature_type)\n\n    ### make output acoustic data\n    if cfg.MAKECMP:\n        logger.info(\'creating acoustic (output) features\')\n        delta_win = cfg.delta_win #[-0.5, 0.0, 0.5]\n        acc_win = cfg.acc_win     #[1.0, -2.0, 1.0]\n\n        if cfg.GenTestList:\n            for feature_name in list(cfg.in_dir_dict.keys()):\n                in_file_list_dict[feature_name] = prepare_file_path_list(test_id_list, cfg.in_dir_dict[feature_name], cfg.file_extension_dict[feature_name], False)\n            nn_cmp_file_list      = prepare_file_path_list(test_id_list, nn_cmp_dir, cfg.cmp_ext)\n            nn_cmp_norm_file_list = prepare_file_path_list(test_id_list, nn_cmp_norm_dir, cfg.cmp_ext)\n\n        if \'dur\' in list(cfg.in_dir_dict.keys()) and cfg.AcousticModel:\n            lf0_file_list = file_paths.get_lf0_file_list()\n            acoustic_worker = AcousticComposition(delta_win = delta_win, acc_win = acc_win)\n            acoustic_worker.make_equal_frames(dur_file_list, lf0_file_list, cfg.in_dimension_dict)\n            acoustic_worker.prepare_nn_data(in_file_list_dict, nn_cmp_file_list, cfg.in_dimension_dict, cfg.out_dimension_dict)\n        else:\n            perform_acoustic_composition(delta_win, acc_win, in_file_list_dict, nn_cmp_file_list, cfg, parallel=True)\n\n        if cfg.remove_silence_using_binary_labels:\n            ## do this to get lab_dim:\n            label_composer = LabelComposer()\n            label_composer.load_label_configuration(cfg.label_config_file)\n            lab_dim=label_composer.compute_label_dimension()\n\n            silence_feature = 0 ## use first feature in label -- hardcoded for now\n            logger.info(\'Silence removal from CMP using binary label file\')\n\n            ## overwrite the untrimmed audio with the trimmed version:\n            trim_silence(nn_cmp_file_list, nn_cmp_file_list, cfg.cmp_dim,\n                                binary_label_file_list, lab_dim, silence_feature)\n\n        elif cfg.remove_silence_using_hts_labels: \n            ## back off to previous method using HTS labels:\n            remover = SilenceRemover(n_cmp = cfg.cmp_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type, remove_frame_features = cfg.add_frame_features, subphone_feats = cfg.subphone_feats)\n            remover.remove_silence(nn_cmp_file_list, in_label_align_file_list, nn_cmp_file_list) # save to itself\n\n    ### save acoustic normalisation information for normalising the features back\n    var_dir  = file_paths.var_dir\n    var_file_dict = file_paths.get_var_dic()\n\n    ### normalise output acoustic data\n    if cfg.NORMCMP:\n        logger.info(\'normalising acoustic (output) features using method %s\' % cfg.output_feature_normalisation)\n        cmp_norm_info = None\n        if cfg.output_feature_normalisation == \'MVN\':\n            normaliser = MeanVarianceNorm(feature_dimension=cfg.cmp_dim)\n            if cfg.GenTestList:\n                # load mean std values\n                global_mean_vector, global_std_vector = normaliser.load_mean_std_values(norm_info_file)\n            else:\n                ###calculate mean and std vectors on the training data, and apply on the whole dataset\n                global_mean_vector = normaliser.compute_mean(nn_cmp_file_list[0:cfg.train_file_number], 0, cfg.cmp_dim)\n                global_std_vector = normaliser.compute_std(nn_cmp_file_list[0:cfg.train_file_number], global_mean_vector, 0, cfg.cmp_dim)\n                # for hmpd vocoder we don\'t need to normalize the \n                # pdd values\n                if cfg.vocoder_type == \'hmpd\':\n                    stream_start_index = {}\n                    dimension_index = 0\n                    recorded_vuv = False\n                    vuv_dimension = None\n                    for feature_name in cfg.out_dimension_dict.keys():\n                        if feature_name != \'vuv\':\n                            stream_start_index[feature_name] = dimension_index\n                        else:\n                            vuv_dimension = dimension_index\n                            recorded_vuv = True\n                        \n                        dimension_index += cfg.out_dimension_dict[feature_name]\n                    logger.info(\'hmpd pdd values are not normalized since they are in 0 to 1\')\n                    global_mean_vector[:,stream_start_index[\'pdd\']: stream_start_index[\'pdd\'] + cfg.out_dimension_dict[\'pdd\']] = 0\n                    global_std_vector[:,stream_start_index[\'pdd\']: stream_start_index[\'pdd\'] + cfg.out_dimension_dict[\'pdd\']] = 1\n            normaliser.feature_normalisation(nn_cmp_file_list, nn_cmp_norm_file_list)\n            cmp_norm_info = numpy.concatenate((global_mean_vector, global_std_vector), axis=0)\n\n        elif cfg.output_feature_normalisation == \'MINMAX\':\n            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim, min_value = 0.01, max_value = 0.99)\n            if cfg.GenTestList:\n                min_max_normaliser.load_min_max_values(norm_info_file)\n            else:\n                min_max_normaliser.find_min_max_values(nn_cmp_file_list[0:cfg.train_file_number])\n            min_max_normaliser.normalise_data(nn_cmp_file_list, nn_cmp_norm_file_list)\n\n            cmp_min_vector = min_max_normaliser.min_vector\n            cmp_max_vector = min_max_normaliser.max_vector\n            cmp_norm_info = numpy.concatenate((cmp_min_vector, cmp_max_vector), axis=0)\n\n        else:\n            logger.critical(\'Normalisation type %s is not supported!\\n\' %(cfg.output_feature_normalisation))\n            raise\n\n        if not cfg.GenTestList:\n            cmp_norm_info = numpy.array(cmp_norm_info, \'float32\')\n            fid = open(norm_info_file, \'wb\')\n            cmp_norm_info.tofile(fid)\n            fid.close()\n            logger.info(\'saved %s vectors to %s\' %(cfg.output_feature_normalisation, norm_info_file))\n\n            feature_index = 0\n            for feature_name in list(cfg.out_dimension_dict.keys()):\n                feature_std_vector = numpy.array(global_std_vector[:,feature_index:feature_index+cfg.out_dimension_dict[feature_name]], \'float32\')\n\n                fid = open(var_file_dict[feature_name], \'w\')\n                feature_var_vector = feature_std_vector**2\n                feature_var_vector.tofile(fid)\n                fid.close()\n\n                logger.info(\'saved %s variance vector to %s\' %(feature_name, var_file_dict[feature_name]))\n\n                feature_index += cfg.out_dimension_dict[feature_name]\n\n    train_x_file_list, train_y_file_list = file_paths.get_train_list_x_y()\n    valid_x_file_list, valid_y_file_list = file_paths.get_valid_list_x_y()\n    test_x_file_list, test_y_file_list = file_paths.get_test_list_x_y()\n\n    # we need to know the label dimension before training the DNN\n    # computing that requires us to look at the labels\n    #\n    label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name, add_frame_features=cfg.add_frame_features, subphone_feats=cfg.subphone_feats)\n    add_feat_dim = sum(cfg.additional_features.values())\n    lab_dim = label_normaliser.dimension + add_feat_dim + cfg.appended_input_dim\n    if cfg.VoiceConversion:\n        lab_dim = cfg.cmp_dim\n\n    logger.info(\'label dimension is %d\' % lab_dim)\n\n    combined_model_arch = str(len(hidden_layer_size))\n    for hid_size in hidden_layer_size:\n        combined_model_arch += \'_\' + str(hid_size)\n\n    nnets_file_name = file_paths.get_nnets_file_name()\n    temp_dir_name = file_paths.get_temp_nn_dir_name()\n\n    gen_dir = os.path.join(gen_dir, temp_dir_name)\n\n    if cfg.switch_to_keras or cfg.switch_to_tensorflow:\n        ### set configuration variables ###\n        cfg.inp_dim = lab_dim\n        cfg.out_dim = cfg.cmp_dim\n\n        cfg.inp_feat_dir  = nn_label_norm_dir\n        cfg.out_feat_dir  = nn_cmp_norm_dir\n        cfg.pred_feat_dir = gen_dir\n\n        if cfg.GenTestList and cfg.test_synth_dir!=""None"":\n            cfg.inp_feat_dir  = cfg.test_synth_dir\n            cfg.pred_feat_dir = cfg.test_synth_dir\n        \n    if cfg.switch_to_keras:\n        ### call kerasclass and use an instance ###\n        from run_keras_with_merlin_io import KerasClass\n        keras_instance = KerasClass(cfg)\n    \n    elif cfg.switch_to_tensorflow:\n        ### call Tensorflowclass and use an instance ###\n        from run_tensorflow_with_merlin_io import TensorflowClass\n        tf_instance = TensorflowClass(cfg)\n\n    ### DNN model training\n    if cfg.TRAINDNN:\n\n        var_dict = load_covariance(var_file_dict, cfg.out_dimension_dict)\n\n        logger.info(\'training DNN\')\n\n        fid = open(norm_info_file, \'rb\')\n        cmp_min_max = numpy.fromfile(fid, dtype=numpy.float32)\n        fid.close()\n        cmp_min_max = cmp_min_max.reshape((2, -1))\n        cmp_mean_vector = cmp_min_max[0, ]\n        cmp_std_vector  = cmp_min_max[1, ]\n\n\n        try:\n            os.makedirs(model_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create model directory %s\' % model_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        try:\n            if cfg.switch_to_keras:\n                keras_instance.train_keras_model()\n            elif cfg.switch_to_tensorflow:\n                tf_instance.train_tensorflow_model()\n            else:\n                train_DNN(train_xy_file_list = (train_x_file_list, train_y_file_list), \\\n                      valid_xy_file_list = (valid_x_file_list, valid_y_file_list), \\\n                      nnets_file_name = nnets_file_name, \\\n                      n_ins = lab_dim, n_outs = cfg.cmp_dim, ms_outs = cfg.multistream_outs, \\\n                      hyper_params = cfg.hyper_params, buffer_size = cfg.buffer_size, plot = cfg.plot, var_dict = var_dict,\n                      cmp_mean_vector = cmp_mean_vector, cmp_std_vector = cmp_std_vector,init_dnn_model_file=cfg.start_from_trained_model)\n        except KeyboardInterrupt:\n            logger.critical(\'train_DNN interrupted via keyboard\')\n            # Could \'raise\' the exception further, but that causes a deep traceback to be printed\n            # which we don\'t care about for a keyboard interrupt. So, just bail out immediately\n            sys.exit(1)\n        except:\n            logger.critical(\'train_DNN threw an exception\')\n            raise\n\n\n\n    if cfg.GENBNFEA:\n        # Please only tune on this step when you want to generate bottleneck features from DNN\n        gen_dir = file_paths.bottleneck_features\n\n        bottleneck_size = min(hidden_layer_size)\n        bottleneck_index = 0\n        for i in range(len(hidden_layer_size)):\n            if hidden_layer_size[i] == bottleneck_size:\n                bottleneck_index = i\n\n        logger.info(\'generating bottleneck features from DNN\')\n\n        try:\n            os.makedirs(gen_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create generation directory %s\' % gen_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        gen_file_id_list = file_id_list[0:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n        test_x_file_list = nn_label_norm_file_list[0:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n        gen_file_list = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.cmp_ext)\n\n        dnn_hidden_generation(test_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list, bottleneck_index)\n\n    ### generate parameters from DNN\n    gen_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n    test_x_file_list  = nn_label_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n    if cfg.GenTestList:\n        gen_file_id_list = test_id_list\n        test_x_file_list = nn_label_norm_file_list\n        if cfg.test_synth_dir!=""None"":\n            gen_dir = cfg.test_synth_dir\n\n    if cfg.DNNGEN:\n        logger.info(\'generating from DNN\')\n\n        try:\n            os.makedirs(gen_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create generation directory %s\' % gen_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        gen_file_list = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.cmp_ext)\n\n\n        if cfg.switch_to_keras:\n            keras_instance.test_keras_model()\n        elif cfg.switch_to_tensorflow:\n            tf_instance.test_tensorflow_model()\n        else:\n            reshape_io = True if cfg.rnn_batch_training else False\n            dnn_generation(test_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list, reshape_io)\n\n        logger.debug(\'denormalising generated output using method %s\' % cfg.output_feature_normalisation)\n\n        fid = open(norm_info_file, \'rb\')\n        cmp_min_max = numpy.fromfile(fid, dtype=numpy.float32)\n        fid.close()\n        cmp_min_max = cmp_min_max.reshape((2, -1))\n        cmp_min_vector = cmp_min_max[0, ]\n        cmp_max_vector = cmp_min_max[1, ]\n\n        if cfg.output_feature_normalisation == \'MVN\':\n            denormaliser = MeanVarianceNorm(feature_dimension = cfg.cmp_dim)\n            denormaliser.feature_denormalisation(gen_file_list, gen_file_list, cmp_min_vector, cmp_max_vector)\n\n        elif cfg.output_feature_normalisation == \'MINMAX\':\n            denormaliser = MinMaxNormalisation(cfg.cmp_dim, min_value = 0.01, max_value = 0.99, min_vector = cmp_min_vector, max_vector = cmp_max_vector)\n            denormaliser.denormalise_data(gen_file_list, gen_file_list)\n        else:\n            logger.critical(\'denormalising method %s is not supported!\\n\' %(cfg.output_feature_normalisation))\n            raise\n\n        if cfg.AcousticModel:\n            ##perform MLPG to smooth parameter trajectory\n            ## lf0 is included, the output features much have vuv.\n            generator = ParameterGeneration(gen_wav_features = cfg.gen_wav_features, enforce_silence = cfg.enforce_silence)\n            generator.acoustic_decomposition(gen_file_list, cfg.cmp_dim, cfg.out_dimension_dict, cfg.file_extension_dict, var_file_dict, do_MLPG=cfg.do_MLPG, cfg=cfg)\n\n        if cfg.DurationModel:\n            ### Perform duration normalization(min. state dur set to 1) ###\n            gen_dur_list   = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.dur_ext)\n            gen_label_list = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.lab_ext)\n            in_gen_label_align_file_list = prepare_file_path_list(gen_file_id_list, cfg.in_label_align_dir, cfg.lab_ext, False)\n\n            generator = ParameterGeneration(gen_wav_features = cfg.gen_wav_features)\n            generator.duration_decomposition(gen_file_list, cfg.cmp_dim, cfg.out_dimension_dict, cfg.file_extension_dict)\n\n            label_modifier = HTSLabelModification(silence_pattern = cfg.silence_pattern, label_type = cfg.label_type)\n            label_modifier.modify_duration_labels(in_gen_label_align_file_list, gen_dur_list, gen_label_list)\n\n\n    ### generate wav\n    if cfg.GENWAV:\n        logger.info(\'reconstructing waveform(s)\')\n        generate_wav(gen_dir, gen_file_id_list, cfg)     # generated speech\n#       generate_wav(nn_cmp_dir, gen_file_id_list, cfg)  # reference copy synthesis speech\n\n    ### setting back to original conditions before calculating objective scores ###\n    if cfg.GenTestList:\n        in_label_align_file_list = prepare_file_path_list(file_id_list, cfg.in_label_align_dir, cfg.lab_ext, False)\n        binary_label_file_list   = prepare_file_path_list(file_id_list, binary_label_dir, cfg.lab_ext)\n        gen_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n    ### evaluation: RMSE and CORR for duration\n    if cfg.CALMCD and cfg.DurationModel:\n        logger.info(\'calculating MCD\')\n\n        ref_data_dir = os.path.join(inter_data_dir, \'ref_data\')\n\n        ref_dur_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.dur_ext)\n\n        in_gen_label_align_file_list = in_label_align_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n        calculator = IndividualDistortionComp()\n\n        valid_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n        test_file_id_list  = file_id_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n        if cfg.remove_silence_using_binary_labels:\n            untrimmed_reference_data = in_file_list_dict[\'dur\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n            trim_silence(untrimmed_reference_data, ref_dur_list, cfg.dur_dim, \\\n                                untrimmed_test_labels, lab_dim, silence_feature)\n        else:\n            remover = SilenceRemover(n_cmp = cfg.dur_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type, remove_frame_features = cfg.add_frame_features)\n            remover.remove_silence(in_file_list_dict[\'dur\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_dur_list)\n\n        valid_dur_rmse, valid_dur_corr = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.dur_ext, cfg.dur_dim)\n        test_dur_rmse, test_dur_corr = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.dur_ext, cfg.dur_dim)\n\n        logger.info(\'Develop: DNN -- RMSE: %.3f frames/phoneme; CORR: %.3f; \' \\\n                    %(valid_dur_rmse, valid_dur_corr))\n        logger.info(\'Test: DNN -- RMSE: %.3f frames/phoneme; CORR: %.3f; \' \\\n                    %(test_dur_rmse, test_dur_corr))\n\n    ### evaluation: calculate distortion\n    if cfg.CALMCD and cfg.AcousticModel:\n        logger.info(\'calculating MCD\')\n\n        ref_data_dir = os.path.join(inter_data_dir, \'ref_data\')\n        ref_lf0_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.lf0_ext)\n        # for straight or world vocoders\n        ref_mgc_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.mgc_ext)\n        ref_bap_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.bap_ext)\n        # for magphase vocoder\n        ref_mag_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.mag_ext)\n        ref_real_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.real_ext)\n        ref_imag_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.imag_ext)\n        # for GlottDNN vocoder\n        ref_lsf_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.lsf_ext)\n        ref_slsf_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.slsf_ext)\n        ref_gain_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.gain_ext)\n        ref_hnr_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.hnr_ext)\n        # for pulsemodel vocoder\n        ref_pdd_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.pdd_ext)\n\n        in_gen_label_align_file_list = in_label_align_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n        calculator = IndividualDistortionComp()\n\n        spectral_distortion = 0.0\n        bap_mse             = 0.0\n        f0_mse              = 0.0\n        vuv_error           = 0.0\n\n        valid_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n        test_file_id_list  = file_id_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n        if cfg.remove_silence_using_binary_labels:\n            ## get lab_dim:\n            label_composer = LabelComposer()\n            label_composer.load_label_configuration(cfg.label_config_file)\n            lab_dim=label_composer.compute_label_dimension()\n\n            ## use first feature in label -- hardcoded for now\n            silence_feature = 0\n\n            ## Use these to trim silence:\n            untrimmed_test_labels = binary_label_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n\n        if \'mgc\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'mgc\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_mgc_list, cfg.mgc_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            elif cfg.remove_silence_using_hts_labels:\n                remover = SilenceRemover(n_cmp = cfg.mgc_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type)\n                remover.remove_silence(in_file_list_dict[\'mgc\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_mgc_list)\n            else:\n                ref_data_dir = os.path.join(data_dir, \'mgc\')\n            valid_spectral_distortion = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.mgc_ext, cfg.mgc_dim)\n            test_spectral_distortion  = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.mgc_ext, cfg.mgc_dim)\n            valid_spectral_distortion *= (10 /numpy.log(10)) * numpy.sqrt(2.0)    ##MCD\n            test_spectral_distortion  *= (10 /numpy.log(10)) * numpy.sqrt(2.0)    ##MCD\n\n\n        if \'bap\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'bap\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_bap_list, cfg.bap_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            elif cfg.remove_silence_using_hts_labels:\n                remover = SilenceRemover(n_cmp = cfg.bap_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type)\n                remover.remove_silence(in_file_list_dict[\'bap\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_bap_list)\n            else:\n                ref_data_dir = os.path.join(data_dir, \'bap\')\n            valid_bap_mse = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.bap_ext, cfg.bap_dim)\n            test_bap_mse  = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.bap_ext, cfg.bap_dim)\n            valid_bap_mse = valid_bap_mse / 10.0    ##Cassia\'s bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC\n            test_bap_mse  = test_bap_mse / 10.0    ##Cassia\'s bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC\n\n        if \'lf0\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'lf0\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_lf0_list, cfg.lf0_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            elif cfg.remove_silence_using_hts_labels:\n                remover = SilenceRemover(n_cmp = cfg.lf0_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type)\n                remover.remove_silence(in_file_list_dict[\'lf0\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_lf0_list)\n            else:\n                if cfg.vocoder_type == \'MAGPHASE\':\n                    ref_data_dir = os.path.join(data_dir, \'feats\')\n                else:\n                    ref_data_dir = os.path.join(data_dir, \'lf0\')\n            valid_f0_mse, valid_f0_corr, valid_vuv_error   = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.lf0_ext, cfg.lf0_dim)\n            test_f0_mse , test_f0_corr, test_vuv_error    = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.lf0_ext, cfg.lf0_dim)\n      \n        if \'mag\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_hts_labels:\n                remover = SilenceRemover(n_cmp = cfg.mag_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type)\n                remover.remove_silence(in_file_list_dict[\'mag\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_mag_list)\n            else:\n                ref_data_dir = os.path.join(data_dir, \'feats\')\n            valid_mag_mse = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.mag_ext, cfg.mag_dim)\n            test_mag_mse  = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.mag_ext, cfg.mag_dim)\n            valid_mag_mse = 10.0*numpy.log10(valid_mag_mse)    \n            test_mag_mse  = 10.0*numpy.log10(test_mag_mse)\n\n        if \'real\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_hts_labels:\n                remover = SilenceRemover(n_cmp = cfg.real_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type)\n                remover.remove_silence(in_file_list_dict[\'real\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_real_list)\n            else:\n                ref_data_dir = os.path.join(data_dir, \'feats\')\n            valid_real_mse = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.real_ext, cfg.real_dim)\n            test_real_mse = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.real_ext, cfg.real_dim)\n            valid_real_mse = 10.0*numpy.log10(valid_real_mse)    \n            test_real_mse  = 10.0*numpy.log10(test_real_mse)\n\n        if \'imag\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_hts_labels:\n                remover = SilenceRemover(n_cmp = cfg.imag_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type)\n                remover.remove_silence(in_file_list_dict[\'imag\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_imag_list)\n            else:\n                ref_data_dir = os.path.join(data_dir, \'feats\')\n            valid_imag_mse = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.imag_ext, cfg.imag_dim)\n            test_imag_mse  = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.imag_ext, cfg.imag_dim)\n            valid_imag_mse = 10.0*numpy.log10(valid_imag_mse)    \n            test_imag_mse  = 10.0*numpy.log10(test_imag_mse)\n\n        if \'lsf\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'lsf\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_lsf_list, cfg.lsf_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.lsf_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type)\n                remover.remove_silence(in_file_list_dict[\'lsf\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_lsf_list)\n            valid_spectral_distortion = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.lsf_ext, cfg.lsf_dim)\n            test_spectral_distortion  = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.lsf_ext, cfg.lsf_dim)\n        \n        if \'slsf\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'slsf\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_slsf_list, cfg.slsf_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.slsf_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type)\n                remover.remove_silence(in_file_list_dict[\'slsf\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_slsf_list)\n            valid_spectral_distortion = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.slsf_ext, cfg.slsf_dim)\n            test_spectral_distortion  = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.slsf_ext, cfg.slsf_dim)\n        \n        if \'hnr\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'hnr\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_hnr_list, cfg.hnr_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.hnr_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type)\n                remover.remove_silence(in_file_list_dict[\'hnr\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_hnr_list)\n            valid_spectral_distortion = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.hnr_ext, cfg.hnr_dim)\n            test_spectral_distortion  = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.hnr_ext, cfg.hnr_dim)\n        \n        if \'gain\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'gain\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_gain_list, cfg.gain_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.gain_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type)\n                remover.remove_silence(in_file_list_dict[\'gain\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_gain_list)\n            valid_spectral_distortion = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.gain_ext, cfg.gain_dim)\n            test_spectral_distortion  = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.gain_ext, cfg.gain_dim)\n        \n        if \'pdd\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'pdd\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_pdd_list, cfg.pdd_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.pdd_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type)\n                remover.remove_silence(in_file_list_dict[\'pdd\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_pdd_list)\n            valid_spectral_distortion = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.pdd_ext, cfg.pdd_dim)\n            test_spectral_distortion  = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.pdd_ext, cfg.pdd_dim)\n        \n\n        if cfg.vocoder_type == \'MAGPHASE\':\n            logger.info(\'Develop: DNN -- MAG: %.3f dB; REAL: %.3f dB; IMAG: %.3f dB; F0:- RMSE: %.3f Hz; CORR: %.3f; VUV: %.3f%%\' \\\n                    %(valid_mag_mse, valid_real_mse, valid_imag_mse, valid_f0_mse, valid_f0_corr, valid_vuv_error*100.))\n            logger.info(\'Test   : DNN -- MAG: %.3f dB; REAL: %.3f dB; IMAG: %.3f dB; F0:- RMSE: %.3f Hz; CORR: %.3f; VUV: %.3f%%\' \\\n                    %(test_mag_mse, test_real_mse, test_imag_mse , test_f0_mse , test_f0_corr, test_vuv_error*100.))\n        else:\n            logger.info(\'Develop: DNN -- MCD: %.3f dB; BAP: %.3f dB; F0:- RMSE: %.3f Hz; CORR: %.3f; VUV: %.3f%%\' \\\n                    %(valid_spectral_distortion, valid_bap_mse, valid_f0_mse, valid_f0_corr, valid_vuv_error*100.))\n            logger.info(\'Test   : DNN -- MCD: %.3f dB; BAP: %.3f dB; F0:- RMSE: %.3f Hz; CORR: %.3f; VUV: %.3f%%\' \\\n                    %(test_spectral_distortion , test_bap_mse , test_f0_mse , test_f0_corr, test_vuv_error*100.))\n\nif __name__ == \'__main__\':\n\n\n\n    # these things should be done even before trying to parse the command line\n\n    # create a configuration instance\n    # and get a short name for this instance\n    cfg=configuration.cfg\n\n    # set up logging to use our custom class\n    logging.setLoggerClass(LoggerPlotter)\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n\n    if len(sys.argv) != 2:\n        logger.critical(\'usage: run_merlin.sh [config file name]\')\n        sys.exit(1)\n\n    config_file = sys.argv[1]\n\n    config_file = os.path.abspath(config_file)\n    cfg.configure(config_file)\n\n\n    logger.info(\'Installation information:\')\n    logger.info(\'  Merlin directory: \'+os.path.abspath(os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir)))\n    logger.info(\'  PATH:\')\n    env_PATHs = os.getenv(\'PATH\')\n    if env_PATHs:\n        env_PATHs = env_PATHs.split(\':\')\n        for p in env_PATHs:\n            if len(p)>0: logger.info(\'      \'+p)\n    logger.info(\'  LD_LIBRARY_PATH:\')\n    env_LD_LIBRARY_PATHs = os.getenv(\'LD_LIBRARY_PATH\')\n    if env_LD_LIBRARY_PATHs:\n        env_LD_LIBRARY_PATHs = env_LD_LIBRARY_PATHs.split(\':\')\n        for p in env_LD_LIBRARY_PATHs:\n            if len(p)>0: logger.info(\'      \'+p)\n    logger.info(\'  Python version: \'+sys.version.replace(\'\\n\',\'\'))\n    logger.info(\'    PYTHONPATH:\')\n    env_PYTHONPATHs = os.getenv(\'PYTHONPATH\')\n    if env_PYTHONPATHs:\n        env_PYTHONPATHs = env_PYTHONPATHs.split(\':\')\n        for p in env_PYTHONPATHs:\n            if len(p)>0:\n                logger.info(\'      \'+p)\n    logger.info(\'  Numpy version: \'+numpy.version.version)\n    logger.info(\'  Theano version: \'+theano.version.version)\n    logger.info(\'    THEANO_FLAGS: \'+os.getenv(\'THEANO_FLAGS\'))\n    logger.info(\'    device: \'+theano.config.device)\n\n    # Check for the presence of git\n    ret = os.system(\'git status > /dev/null\')\n    if ret==0:\n        logger.info(\'  Git is available in the working directory:\')\n        git_describe = subprocess.Popen([\'git\', \'describe\', \'--tags\', \'--always\'], stdout=subprocess.PIPE).communicate()[0][:-1]\n        logger.info(\'    Merlin version: {}\'.format(git_describe))\n        git_branch = subprocess.Popen([\'git\', \'rev-parse\', \'--abbrev-ref\', \'HEAD\'], stdout=subprocess.PIPE).communicate()[0][:-1]\n        logger.info(\'    branch: {}\'.format(git_branch))\n        git_diff = subprocess.Popen([\'git\', \'diff\', \'--name-status\'], stdout=subprocess.PIPE).communicate()[0]\n        if sys.version_info.major >= 3:\n            git_diff = git_diff.decode(\'utf-8\')\n        git_diff = git_diff.replace(\'\\t\',\' \').split(\'\\n\')\n        logger.info(\'    diff to Merlin version:\')\n        for filediff in git_diff:\n            if len(filediff)>0: logger.info(\'      \'+filediff)\n        logger.info(\'      (all diffs logged in \'+os.path.basename(cfg.log_file)+\'.gitdiff\'+\')\')\n        os.system(\'git diff > \'+cfg.log_file+\'.gitdiff\')\n\n    logger.info(\'Execution information:\')\n    logger.info(\'  HOSTNAME: \'+socket.getfqdn())\n    logger.info(\'  USER: \'+os.getenv(\'USER\'))\n    logger.info(\'  PID: \'+str(os.getpid()))\n    PBS_JOBID = os.getenv(\'PBS_JOBID\')\n    if PBS_JOBID:\n        logger.info(\'  PBS_JOBID: \'+PBS_JOBID)\n\n\n    if cfg.profile:\n        logger.info(\'profiling is activated\')\n        import cProfile, pstats\n        cProfile.run(\'main_function(cfg)\', \'mainstats\')\n\n        # create a stream for the profiler to write to\n        profiling_output = io.StringIO()\n        p = pstats.Stats(\'mainstats\', stream=profiling_output)\n\n        # print stats to that stream\n        # here we just report the top 10 functions, sorted by total amount of time spent in each\n        p.strip_dirs().sort_stats(\'tottime\').print_stats(10)\n\n        # print the result to the log\n        logger.info(\'---Profiling result follows---\\n%s\' %  profiling_output.getvalue() )\n        profiling_output.close()\n        logger.info(\'---End of profiling result---\')\n\n    else:\n        main_function(cfg)\n\n    sys.exit(0)\n'"
src/run_merlin_hed.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://github.com/CSTR-Edinburgh/merlin\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\nimport pickle\nimport gzip\nimport os, sys, errno\nimport time\nimport math\n\nimport subprocess\nimport socket # only for socket.getfqdn()\n\n#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)\nimport numpy\n#import gnumpy as gnp\n# we need to explicitly import this in some cases, not sure why this doesn\'t get imported with numpy itself\nimport numpy.distutils.__config__\n# and only after that can we import theano\nimport theano\n\nfrom utils.providers import ListDataProvider\n\nfrom frontend.label_normalisation import HTSLabelNormalisation\nfrom frontend.silence_remover import SilenceRemover\nfrom frontend.silence_remover import trim_silence\nfrom frontend.min_max_norm import MinMaxNormalisation\nfrom frontend.acoustic_composition import AcousticComposition\nfrom frontend.parameter_generation import ParameterGeneration\nfrom frontend.mean_variance_norm import MeanVarianceNorm\n\n# the new class for label composition and normalisation\nfrom frontend.label_composer import LabelComposer\nfrom frontend.label_modifier import HTSLabelModification\nfrom frontend.merge_features import MergeFeat\n\nimport configuration\nfrom models.deep_rnn import DeepRecurrentNetwork\nfrom models.hed_rnn import DeepEncoderDecoderNetwork\n\nfrom utils.compute_distortion import DistortionComputation, IndividualDistortionComp\nfrom utils.generate import generate_wav\nfrom utils.learn_rates import ExpDecreaseLearningRate\n\nfrom io_funcs.binary_io import  BinaryIOCollection\n\n# our custom logging class that can also plot\nfrom logplot.logging_plotting import LoggerPlotter, MultipleSeriesPlot, SingleWeightMatrixPlot\nimport logging # as logging\nimport logging.config\nimport io\nfrom utils.file_paths import FilePaths\nfrom utils.utils import read_file_list, prepare_file_path_list\n\n\ndef extract_file_id_list(file_list):\n    file_id_list = []\n    for file_name in file_list:\n        file_id = os.path.basename(os.path.splitext(file_name)[0])\n        file_id_list.append(file_id)\n\n    return  file_id_list\n\ndef make_output_file_list(out_dir, in_file_lists):\n    out_file_lists = []\n\n    for in_file_name in in_file_lists:\n        file_id = os.path.basename(in_file_name)\n        out_file_name = out_dir + \'/\' + file_id\n        out_file_lists.append(out_file_name)\n\n    return  out_file_lists\n\ndef visualize_dnn(dnn):\n\n    plotlogger = logging.getLogger(""plotting"")\n\n        # reference activation weights in layers\n    W = list(); layer_name = list()\n    for i in range(len(dnn.params)):\n        aa = dnn.params[i].get_value(borrow=True).T\n        print(aa.shape, aa.size)\n        if aa.size > aa.shape[0]:\n            W.append(aa)\n            layer_name.append(dnn.params[i].name)\n\n    ## plot activation weights including input and output\n    layer_num = len(W)\n    for i_layer in range(layer_num):\n        fig_name = \'Activation weights W\' + str(i_layer) + \'_\' + layer_name[i_layer]\n        fig_title = \'Activation weights of W\' + str(i_layer)\n        xlabel = \'Neuron index of hidden layer \' + str(i_layer)\n        ylabel = \'Neuron index of hidden layer \' + str(i_layer+1)\n        if i_layer == 0:\n            xlabel = \'Input feature index\'\n        if i_layer == layer_num-1:\n            ylabel = \'Output feature index\'\n        logger.create_plot(fig_name, SingleWeightMatrixPlot)\n        plotlogger.add_plot_point(fig_name, fig_name, W[i_layer])\n        plotlogger.save_plot(fig_name, title=fig_name, xlabel=xlabel, ylabel=ylabel)\n\n\ndef load_covariance(var_file_dict, out_dimension_dict):\n    var = {}\n    io_funcs = BinaryIOCollection()\n    for feature_name in list(var_file_dict.keys()):\n        var_values, dimension = io_funcs.load_binary_file_frame(var_file_dict[feature_name], 1)\n\n        var_values = numpy.reshape(var_values, (out_dimension_dict[feature_name], 1))\n\n        var[feature_name] = var_values\n\n    return  var\n\n\ndef train_DNN(train_xy_file_list, valid_xy_file_list, \\\n              nnets_file_name, n_ins, n_outs, ms_outs, hyper_params, buffer_size, plot=False, var_dict=None,\n              cmp_mean_vector = None, cmp_std_vector = None, seq_dur_file_list = None, init_dnn_model_file = None):\n\n    # get loggers for this function\n    # this one writes to both console and file\n    logger = logging.getLogger(""main.train_DNN"")\n    logger.debug(\'Starting train_DNN\')\n\n    if plot:\n        # this one takes care of plotting duties\n        plotlogger = logging.getLogger(""plotting"")\n        # create an (empty) plot of training convergence, ready to receive data points\n        logger.create_plot(\'training convergence\',MultipleSeriesPlot)\n\n    try:\n        assert numpy.sum(ms_outs) == n_outs\n    except AssertionError:\n        logger.critical(\'the summation of multi-stream outputs does not equal to %d\' %(n_outs))\n        raise\n\n    ####parameters#####\n    finetune_lr     = float(hyper_params[\'learning_rate\'])\n    training_epochs = int(hyper_params[\'training_epochs\'])\n    batch_size      = int(hyper_params[\'batch_size\'])\n    l1_reg          = float(hyper_params[\'l1_reg\'])\n    l2_reg          = float(hyper_params[\'l2_reg\'])\n    warmup_epoch    = int(hyper_params[\'warmup_epoch\'])\n    momentum        = float(hyper_params[\'momentum\'])\n    warmup_momentum = float(hyper_params[\'warmup_momentum\'])\n\n    hidden_layer_size = hyper_params[\'hidden_layer_size\']\n\n    buffer_utt_size = buffer_size\n    early_stop_epoch = int(hyper_params[\'early_stop_epochs\'])\n\n    hidden_activation = hyper_params[\'hidden_activation\']\n    output_activation = hyper_params[\'output_activation\']\n\n    model_type = hyper_params[\'model_type\']\n    hidden_layer_type  = hyper_params[\'hidden_layer_type\']\n\n    ## use a switch to turn on pretraining\n    ## pretraining may not help too much, if this case, we turn it off to save time\n    do_pretraining = hyper_params[\'do_pretraining\']\n    pretraining_epochs = int(hyper_params[\'pretraining_epochs\'])\n    pretraining_lr = float(hyper_params[\'pretraining_lr\'])\n\n    sequential_training = hyper_params[\'sequential_training\']\n    dropout_rate = hyper_params[\'dropout_rate\']\n\n    buffer_size = int(buffer_size / batch_size) * batch_size\n\n    ###################\n    (train_x_file_list, train_y_file_list) = train_xy_file_list\n    (valid_x_file_list, valid_y_file_list) = valid_xy_file_list\n\n    if cfg.network_type != \'S2S\':\n        seq_dur_file_list = None\n\n    if not seq_dur_file_list:\n        train_dur_file_list = None\n        valid_dur_file_list = None\n    else:\n        label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name, subphone_feats=""coarse_coding"")\n        train_dur_file_list = seq_dur_file_list[0:cfg.train_file_number]\n        valid_dur_file_list = seq_dur_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n    \n    logger.debug(\'Creating training   data provider\')\n    train_data_reader = ListDataProvider(x_file_list = train_x_file_list, y_file_list = train_y_file_list, dur_file_list = train_dur_file_list, \n                            n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, \n                            sequential = sequential_training, network_type = cfg.network_type, shuffle = True)\n\n    logger.debug(\'Creating validation data provider\')\n    valid_data_reader = ListDataProvider(x_file_list = valid_x_file_list, y_file_list = valid_y_file_list, dur_file_list = valid_dur_file_list, \n                            n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, \n                            sequential = sequential_training, network_type = cfg.network_type, shuffle = False)\n\n    if cfg.rnn_batch_training:\n        train_data_reader.set_rnn_params(training_algo=cfg.training_algo, batch_size=cfg.batch_size, seq_length=cfg.seq_length, merge_size=cfg.merge_size, bucket_range=cfg.bucket_range)\n        valid_data_reader.reshape_input_output()\n    \n    if cfg.network_type == \'S2S\':\n        MLU_div = train_data_reader.set_s2s_division(cfg.linguistic_file_name)\n        MLU_div = valid_data_reader.set_s2s_division(cfg.linguistic_file_name)\n\n    if cfg.network_type == \'S2SD\':\n        shared_train_set_xyd, temp_train_set_x, temp_train_set_y, temp_train_set_d = train_data_reader.load_one_partition()\n        shared_valid_set_xyd, temp_valid_set_x, temp_valid_set_y, temp_valid_set_d = valid_data_reader.load_one_partition()\n        train_set_x, train_set_y, train_set_d = shared_train_set_xyd\n        valid_set_x, valid_set_y, valid_set_d = shared_valid_set_xyd\n        \n        temp_train_set_f = label_normaliser.extract_durational_features(dur_data=temp_train_set_d)\n        temp_valid_set_f = label_normaliser.extract_durational_features(dur_data=temp_valid_set_d)\n        train_set_f = theano.shared(numpy.asarray(temp_train_set_f, dtype=theano.config.floatX), name=\'f\', borrow=True)\n        valid_set_f = theano.shared(numpy.asarray(temp_valid_set_f, dtype=theano.config.floatX), name=\'f\', borrow=True)\n    elif cfg.network_type == \'S2S\':\n        shared_train_set_xyd, temp_train_set_x, temp_train_set_y, temp_train_set_d, temp_train_set_af = train_data_reader.load_one_partition()\n        shared_valid_set_xyd, temp_valid_set_x, temp_valid_set_y, temp_valid_set_d, temp_valid_set_af = valid_data_reader.load_one_partition()\n        train_set_x, train_set_y, train_set_d = shared_train_set_xyd\n        valid_set_x, valid_set_y, valid_set_d = shared_valid_set_xyd\n        \n        ### extract phone duration array for frame features ###\n        [num_train_words, n_ins] = temp_train_set_x.shape\n        num_train_syl   = sum(temp_train_set_d[0: num_train_words])\n        \n        [num_valid_words, n_ins] = temp_valid_set_x.shape\n        num_valid_syl   = sum(temp_valid_set_d[0: num_valid_words])\n        \n        temp_train_ph_dur_data = temp_train_set_d[num_train_words+num_train_syl:]\n        temp_valid_ph_dur_data = temp_valid_set_d[num_valid_words+num_valid_syl:]\n        \n        temp_train_set_f = label_normaliser.extract_durational_features(dur_data=temp_train_ph_dur_data)\n        temp_valid_set_f = label_normaliser.extract_durational_features(dur_data=temp_valid_ph_dur_data)\n        temp_train_set_af[:, -4:] = temp_train_set_f\n        temp_valid_set_af[:, -4:] = temp_valid_set_f\n        train_set_f = theano.shared(numpy.asarray(temp_train_set_af, dtype=theano.config.floatX), name=\'f\', borrow=True)\n        valid_set_f = theano.shared(numpy.asarray(temp_valid_set_af, dtype=theano.config.floatX), name=\'f\', borrow=True) \n    else: \n        shared_train_set_xy, temp_train_set_x, temp_train_set_y = train_data_reader.load_one_partition()\n        train_set_x, train_set_y = shared_train_set_xy\n        shared_valid_set_xy, temp_valid_set_x, temp_valid_set_y = valid_data_reader.load_one_partition()\n        valid_set_x, valid_set_y = shared_valid_set_xy\n    train_data_reader.reset()\n    valid_data_reader.reset()\n\n\n    ##temporally we use the training set as pretrain_set_x.\n    ##we need to support any data for pretraining\n\n    # numpy random generator\n    numpy_rng = numpy.random.RandomState(123)\n    logger.info(\'building the model\')\n\n\n    dnn_model = None\n    pretrain_fn = None  ## not all the model support pretraining right now\n    train_fn = None\n    valid_fn = None\n    valid_model = None ## valid_fn and valid_model are the same. reserve to computer multi-stream distortion\n    if model_type == \'DNN\':\n        if cfg.network_type == \'S2S\':\n            dnn_model = DeepEncoderDecoderNetwork(n_in= n_ins, hidden_layer_size = hidden_layer_size, n_out = n_outs,\n                                         L1_reg = l1_reg, L2_reg = l2_reg, hidden_layer_type = hidden_layer_type, output_type = cfg.output_layer_type,  \n                                         network_type=cfg.network_type, ed_type=\'HED\', MLU_div_lengths = MLU_div[\'length\'],  \n                                         dropout_rate = dropout_rate, optimizer = cfg.optimizer, rnn_batch_training = cfg.rnn_batch_training)\n        else:\n            dnn_model = DeepRecurrentNetwork(n_in= n_ins, hidden_layer_size = hidden_layer_size, n_out = n_outs,\n                                         L1_reg = l1_reg, L2_reg = l2_reg, hidden_layer_type = hidden_layer_type, output_type = cfg.output_layer_type,\n                                         dropout_rate = dropout_rate, optimizer = cfg.optimizer, rnn_batch_training = cfg.rnn_batch_training)\n\n    else:\n        logger.critical(\'%s type NN model is not supported!\' %(model_type))\n        raise\n\n    ## Model adaptation -- fine tuning the existing model\n    ## We can\'t just unpickle the old model and use that because fine-tune functions\n    ## depend on opt_l2e option used in construction of initial model. One way around this\n    ## would be to unpickle, manually set unpickled_dnn_model.opt_l2e=True and then call\n    ## unpickled_dnn_model.build_finetne_function() again. This is another way, construct\n    ## new model from scratch with opt_l2e=True, then copy existing weights over:\n    use_lhuc = cfg.use_lhuc\n    if init_dnn_model_file != ""_"":\n        logger.info(\'load parameters from existing model: %s\' %(init_dnn_model_file))\n        if not os.path.isfile(init_dnn_model_file):\n            sys.exit(\'Model file %s does not exist\'%(init_dnn_model_file))\n        existing_dnn_model = pickle.load(open(init_dnn_model_file, \'rb\'))\n        if not use_lhuc and not len(existing_dnn_model.params) == len(dnn_model.params):\n            sys.exit(\'Old and new models have different numbers of weight matrices\')\n        elif use_lhuc and len(dnn_model.params) < len(existing_dnn_model.params):\n            sys.exit(\'In LHUC adaptation new model must have more parameters than old model.\')\n        # assign the existing dnn model parameters to the new dnn model\n        k = 0\n        for i in range(len(dnn_model.params)):\n            ## Added for LHUC ##\n            # In LHUC, we keep all the old parameters intact and learn only a small set of new\n            # parameters\n            if dnn_model.params[i].name == \'c\':\n                continue\n            else:\n                old_val = existing_dnn_model.params[k].get_value()\n                new_val = dnn_model.params[i].get_value()\n                if numpy.shape(old_val) == numpy.shape(new_val):\n                    dnn_model.params[i].set_value(old_val)\n                else:\n                    sys.exit(\'old and new weight matrices have different shapes\')\n                k = k + 1        \n\n    if cfg.network_type == \'S2S\':\n        train_fn, valid_fn = dnn_model.build_finetune_functions_S2SPF(\n                        (train_set_x, train_set_y, train_set_d, train_set_f), (valid_set_x, valid_set_y, valid_set_d, valid_set_f))\n    else: \n        train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y), use_lhuc)  #, batch_size=batch_size\n    logger.info(\'fine-tuning the %s model\' %(model_type))\n\n    start_time = time.time()\n\n    best_dnn_model = dnn_model\n    best_validation_loss = sys.float_info.max\n    previous_loss = sys.float_info.max\n\n    lr_decay  = cfg.lr_decay\n    if lr_decay>0:\n        early_stop_epoch *= lr_decay\n\n    early_stop = 0\n    val_loss_counter = 0\n\n    previous_finetune_lr = finetune_lr\n\n    epoch = 0\n    while (epoch < training_epochs):\n        epoch = epoch + 1\n        \n        if lr_decay==0:\n            # fixed learning rate \n            reduce_lr = False\n        elif lr_decay<0:\n            # exponential decay\n            reduce_lr = False if epoch <= warmup_epoch else True\n        elif val_loss_counter > 0:\n            # linear decay\n            reduce_lr = False\n            if val_loss_counter%lr_decay==0:\n                reduce_lr = True\n                val_loss_counter = 0\n        else:\n            # no decay\n            reduce_lr = False\n\n        if reduce_lr:\n            current_finetune_lr = previous_finetune_lr * 0.5\n            current_momentum    = momentum\n        else:\n            current_finetune_lr = previous_finetune_lr\n            current_momentum    = warmup_momentum\n        \n        previous_finetune_lr = current_finetune_lr\n\n        train_error = []\n        sub_start_time = time.time()\n\n        logger.debug(""training params -- learning rate: %f, early_stop: %d/%d"" % (current_finetune_lr, early_stop, early_stop_epoch))\n        while (not train_data_reader.is_finish()):\n\n            if cfg.network_type == \'S2SD\':\n                shared_train_set_xyd, temp_train_set_x, temp_train_set_y, temp_train_set_d = train_data_reader.load_one_partition()\n                temp_train_set_f = label_normaliser.extract_durational_features(dur_data=temp_train_set_d)\n                train_set_d.set_value(numpy.asarray(temp_train_set_d, dtype=\'int32\'), borrow=True)\n                train_set_f.set_value(numpy.asarray(temp_train_set_f, dtype=theano.config.floatX), borrow=True)\n            elif cfg.network_type == \'S2S\':\n                shared_train_set_xyd, temp_train_set_x, temp_train_set_y, temp_train_set_d, temp_train_set_af = train_data_reader.load_one_partition()\n                [num_train_words, n_ins] = temp_train_set_x.shape\n                num_train_syl   = sum(temp_train_set_d[0: num_train_words])\n                temp_train_ph_dur_data = temp_train_set_d[num_train_words+num_train_syl:]\n                temp_train_set_f = label_normaliser.extract_durational_features(dur_data=temp_train_ph_dur_data)\n                temp_train_set_af[:, -4:] = temp_train_set_f\n                train_set_d.set_value(numpy.asarray(temp_train_set_d, dtype=\'int32\'), borrow=True)\n                train_set_f.set_value(numpy.asarray(temp_train_set_af, dtype=theano.config.floatX), borrow=True)\n            else:\n                _, temp_train_set_x, temp_train_set_y = train_data_reader.load_one_partition()\n\n            # if sequential training, the batch size will be the number of frames in an utterance\n            # batch_size for sequential training is considered only when rnn_batch_training is set to True\n            if sequential_training == True:\n                batch_size = temp_train_set_x.shape[0]\n\n            n_train_batches = temp_train_set_x.shape[0] // batch_size\n            for index in range(n_train_batches):\n                ## send a batch to the shared variable, rather than pass the batch size and batch index to the finetune function\n                train_set_x.set_value(numpy.asarray(temp_train_set_x, dtype=theano.config.floatX), borrow=True)\n                train_set_y.set_value(numpy.asarray(temp_train_set_y, dtype=theano.config.floatX), borrow=True)\n\n                this_train_error = train_fn(current_finetune_lr, current_momentum)\n\n                train_error.append(this_train_error)\n\n        train_data_reader.reset()\n\n        logger.debug(\'calculating validation loss\')\n        validation_losses = []\n        while (not valid_data_reader.is_finish()):\n            \n            if cfg.network_type == \'S2SD\':\n                shared_valid_set_xyd, temp_valid_set_x, temp_valid_set_y, temp_valid_set_d = valid_data_reader.load_one_partition()\n                temp_valid_set_f = label_normaliser.extract_durational_features(dur_data=temp_valid_set_d)\n                valid_set_d.set_value(numpy.asarray(temp_valid_set_d, dtype=\'int32\'), borrow=True)\n                valid_set_f.set_value(numpy.asarray(temp_valid_set_f, dtype=theano.config.floatX), borrow=True)\n            elif cfg.network_type == \'S2S\':    \n                shared_valid_set_xyd, temp_valid_set_x, temp_valid_set_y, temp_valid_set_d, temp_valid_set_af = valid_data_reader.load_one_partition()\n                [num_valid_words, n_ins] = temp_valid_set_x.shape\n                num_valid_syl   = sum(temp_valid_set_d[0: num_valid_words])\n                temp_valid_ph_dur_data = temp_valid_set_d[num_valid_words+num_valid_syl:]\n                temp_valid_set_f = label_normaliser.extract_durational_features(dur_data=temp_valid_ph_dur_data)\n                temp_valid_set_af[:, -4:] = temp_valid_set_f\n                valid_set_d.set_value(numpy.asarray(temp_valid_set_d, dtype=\'int32\'), borrow=True)\n                valid_set_f.set_value(numpy.asarray(temp_valid_set_af, dtype=theano.config.floatX), borrow=True)\n            else:\n                shared_valid_set_xy, temp_valid_set_x, temp_valid_set_y = valid_data_reader.load_one_partition()\n            valid_set_x.set_value(numpy.asarray(temp_valid_set_x, dtype=theano.config.floatX), borrow=True)\n            valid_set_y.set_value(numpy.asarray(temp_valid_set_y, dtype=theano.config.floatX), borrow=True)\n\n            this_valid_loss = valid_fn()\n\n            validation_losses.append(this_valid_loss)\n        valid_data_reader.reset()\n\n        this_validation_loss = numpy.mean(validation_losses)\n\n        this_train_valid_loss = numpy.mean(numpy.asarray(train_error))\n\n        sub_end_time = time.time()\n\n        loss_difference = this_validation_loss - previous_loss\n\n        logger.info(\'epoch %i, validation error %f, train error %f  time spent %.2f\' %(epoch, this_validation_loss, this_train_valid_loss, (sub_end_time - sub_start_time)))\n        if plot:\n            plotlogger.add_plot_point(\'training convergence\',\'validation set\',(epoch,this_validation_loss))\n            plotlogger.add_plot_point(\'training convergence\',\'training set\',(epoch,this_train_valid_loss))\n            plotlogger.save_plot(\'training convergence\',title=\'Progress of training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n\n        if this_validation_loss < best_validation_loss:\n            pickle.dump(best_dnn_model, open(nnets_file_name, \'wb\'))\n\n            best_dnn_model = dnn_model\n            best_validation_loss = this_validation_loss\n\n        if this_validation_loss >= previous_loss:\n            logger.debug(\'validation loss increased\')\n            val_loss_counter+=1\n            early_stop+=1\n\n        if epoch > 15 and early_stop > early_stop_epoch:\n            logger.debug(\'stopping early\')\n            break\n\n        if math.isnan(this_validation_loss):\n            break\n\n        previous_loss = this_validation_loss\n\n    end_time = time.time()\n\n    logger.info(\'overall  training time: %.2fm validation error %f\' % ((end_time - start_time) / 60., best_validation_loss))\n\n    if plot:\n        plotlogger.save_plot(\'training convergence\',title=\'Final training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n\n    return  best_validation_loss\n\n\ndef dnn_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list, reshape_io=False):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):  #file_number\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size // n_ins))]\n        test_set_x = features.reshape((-1, n_ins))\n        n_rows = test_set_x.shape[0]\n        \n        if reshape_io:\n            test_set_x = numpy.reshape(test_set_x, (1, test_set_x.shape[0], n_ins))\n            test_set_x = numpy.array(test_set_x, \'float32\')\n\n        predicted_parameter = dnn_model.parameter_prediction(test_set_x)\n        predicted_parameter = predicted_parameter.reshape(-1, n_outs)\n        predicted_parameter = predicted_parameter[0:n_rows]\n        \n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\ndef dnn_generation_S2S(valid_file_list, valid_dur_file_list, nnets_file_name, n_ins, n_outs, out_file_list):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    file_number = len(valid_file_list)\n\n    label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name, subphone_feats=""coarse_coding"")\n    for i in xrange(file_number):  #file_number\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        test_set_x = features.reshape((-1, n_ins))\n\n        fid_lab = open(valid_dur_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        test_set_d = features.astype(numpy.int32)\n        \n        dur_features = label_normaliser.extract_durational_features(dur_data=test_set_d)\n        test_set_f = dur_features.astype(numpy.float32)\n\n        predicted_parameter = dnn_model.parameter_prediction_S2SPF(test_set_x, test_set_d, test_set_f)\n\n        #print b_indices\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\ndef dnn_generation_S2SML(valid_file_list, valid_dur_file_list, nnets_file_name, n_ins, n_outs, MLU_div, out_file_list):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    file_number = len(valid_file_list)\n\n    label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name, subphone_feats=""coarse_coding"")\n    for i in xrange(file_number):  #file_number\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        test_set_MLU = features.reshape((-1, n_ins))\n\n        fid_lab = open(valid_dur_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        test_set_d = features.astype(numpy.int32)\n        \n        ### MLU features sub-division ###\n        test_set_phone = numpy.concatenate([test_set_MLU[:, MLU_div[\'phone\'][0]: MLU_div[\'phone\'][1]], test_set_MLU[:, MLU_div[\'phone\'][2]: MLU_div[\'phone\'][3]]], axis = 1)\n        test_set_syl   = numpy.concatenate([test_set_MLU[:, MLU_div[\'syl\'][0]: MLU_div[\'syl\'][1]], test_set_MLU[:, MLU_div[\'syl\'][2]: MLU_div[\'syl\'][3]]], axis = 1)\n        test_set_word  = numpy.concatenate([test_set_MLU[:, MLU_div[\'word\'][0]: MLU_div[\'word\'][1]], test_set_MLU[:, MLU_div[\'word\'][2]: MLU_div[\'word\'][3] ]], axis = 1)\n        \n        ### duration array sub-division ###\n        num_ph    = len(test_set_MLU)\n        \n        dur_word_syl = test_set_d[0: -num_ph]    \n        \n        num_syl   = (numpy.where(numpy.cumsum(dur_word_syl[::-1])==num_ph)[0][0] + 1)\n        num_words = len(dur_word_syl) - num_syl \n        \n        test_set_dur_phone = test_set_d[-num_ph:] \n        test_set_dur_word  = dur_word_syl[0: num_words]\n        test_set_dur_syl   = dur_word_syl[num_words: ]\n        \n        ### additional feature matrix (syllable+phone+frame) ###\n        num_frames = sum(test_set_dur_phone)\n        test_set_af = numpy.empty((num_frames, MLU_div[\'length\'][-1]))\n        \n        test_set_af[0: num_syl, MLU_div[\'length\'][0]: MLU_div[\'length\'][1] ] = test_set_syl[numpy.cumsum(test_set_dur_syl)-1]\n        test_set_af[0: num_ph, MLU_div[\'length\'][1]: MLU_div[\'length\'][2]] = test_set_phone\n        \n        ### input word feature matrix ###\n        test_set_dur_word_segments = numpy.zeros(num_words, dtype=\'int32\')\n        syl_bound = numpy.cumsum(test_set_dur_word)\n        for indx in xrange(num_words):\n            test_set_dur_word_segments[indx] = int(sum(test_set_dur_syl[0: syl_bound[indx]]))\n        test_set_x = test_set_word[test_set_dur_word_segments-1]\n\n        \n        dur_features = label_normaliser.extract_durational_features(dur_data=test_set_dur_phone)\n        test_set_af[:, -4:] = dur_features\n        test_set_f = test_set_af.astype(numpy.float32)\n\n        predicted_parameter = dnn_model.parameter_prediction_S2SPF(test_set_x, test_set_d, test_set_f)\n\n        #print b_indices\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n \n##generate bottleneck layer as features\ndef dnn_hidden_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list, bottleneck_index):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size // n_ins))]\n        features = features.reshape((-1, n_ins))\n        temp_set_x = features.tolist()\n        test_set_x = theano.shared(numpy.asarray(temp_set_x, dtype=theano.config.floatX))\n\n        predicted_parameter = dnn_model.generate_hidden_layer(test_set_x, bottleneck_index)\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\n\ndef main_function(cfg):\n    file_paths = FilePaths(cfg)\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n    # get another logger to handle plotting duties\n    plotlogger = logging.getLogger(""plotting"")\n\n    # later, we might do this via a handler that is created, attached and configured\n    # using the standard config mechanism of the logging module\n    # but for now we need to do it manually\n    plotlogger.set_plot_path(cfg.plot_dir)\n\n    # create plot dir if set to True\n    if not os.path.exists(cfg.plot_dir) and cfg.plot:\n        os.makedirs(cfg.plot_dir)\n\n    #### parameter setting########\n    hidden_layer_size = cfg.hyper_params[\'hidden_layer_size\']\n\n    ####prepare environment\n    try:\n        file_id_list = read_file_list(cfg.file_id_scp)\n        logger.debug(\'Loaded file id list from %s\' % cfg.file_id_scp)\n    except IOError:\n        # this means that open(...) threw an error\n        logger.critical(\'Could not load file id list from %s\' % cfg.file_id_scp)\n        raise\n\n    ###total file number including training, development, and testing\n    total_file_number = len(file_id_list)\n    assert cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number == total_file_number, \'check train, valid, test file number\'\n\n    data_dir = cfg.data_dir\n\n    inter_data_dir = cfg.inter_data_dir\n    nn_cmp_dir       = file_paths.nn_cmp_dir\n    nn_cmp_norm_dir   = file_paths.nn_cmp_norm_dir\n    model_dir = file_paths.model_dir\n    gen_dir   = file_paths.gen_dir\n\n    in_file_list_dict = {}\n\n    for feature_name in list(cfg.in_dir_dict.keys()):\n        in_file_list_dict[feature_name] = prepare_file_path_list(file_id_list, cfg.in_dir_dict[feature_name], cfg.file_extension_dict[feature_name], False)\n\n    nn_cmp_file_list         = file_paths.get_nn_cmp_file_list()\n    nn_cmp_norm_file_list    = file_paths.get_nn_cmp_norm_file_list()\n\n    ###normalisation information\n    norm_info_file = file_paths.norm_info_file\n\n    ### normalise input full context label\n    # currently supporting two different forms of lingustic features\n    # later, we should generalise this\n\n    assert cfg.label_style == \'HTS\', \'Only HTS-style labels are now supported as input to Merlin\'\n\n    label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name, add_frame_features=cfg.add_frame_features, subphone_feats=cfg.subphone_feats)\n    add_feat_dim = sum(cfg.additional_features.values())\n    lab_dim = label_normaliser.dimension + add_feat_dim + cfg.appended_input_dim\n    if cfg.VoiceConversion:\n        lab_dim = cfg.cmp_dim\n    logger.info(\'Input label dimension is %d\' % lab_dim)\n    suffix=str(lab_dim)\n\n\n    if cfg.process_labels_in_work_dir:\n        inter_data_dir = cfg.work_dir\n\n    # the number can be removed\n    file_paths.set_label_dir(label_normaliser.dimension, suffix, lab_dim)\n    file_paths.set_label_file_list()\n\n    binary_label_dir      = file_paths.binary_label_dir\n    nn_label_dir          = file_paths.nn_label_dir\n    nn_label_norm_dir     = file_paths.nn_label_norm_dir\n\n    in_label_align_file_list = file_paths.in_label_align_file_list\n    binary_label_file_list   = file_paths.binary_label_file_list\n    nn_label_file_list       = file_paths.nn_label_file_list\n    nn_label_norm_file_list  = file_paths.nn_label_norm_file_list\n\n    min_max_normaliser = None\n\n    label_norm_file = file_paths.label_norm_file\n\n    test_id_list = file_paths.test_id_list\n\n    if cfg.NORMLAB:\n        # simple HTS labels\n        logger.info(\'preparing label data (input) using standard HTS style labels\')\n        label_normaliser.perform_normalisation(in_label_align_file_list, binary_label_file_list, label_type=cfg.label_type)\n\n        if cfg.additional_features:\n            out_feat_file_list = file_paths.out_feat_file_list\n            in_dim = label_normaliser.dimension\n\n            for new_feature, new_feature_dim in cfg.additional_features.items():\n                new_feat_dir  = os.path.join(data_dir, new_feature)\n                new_feat_file_list = prepare_file_path_list(file_id_list, new_feat_dir, \'.\'+new_feature)\n\n                merger = MergeFeat(lab_dim = in_dim, feat_dim = new_feature_dim)\n                merger.merge_data(binary_label_file_list, new_feat_file_list, out_feat_file_list)\n                in_dim += new_feature_dim\n\n                binary_label_file_list = out_feat_file_list\n\n        remover = SilenceRemover(n_cmp = lab_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type, remove_frame_features = cfg.add_frame_features, subphone_feats = cfg.subphone_feats)\n        remover.remove_silence(binary_label_file_list, in_label_align_file_list, nn_label_file_list)\n\n        min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99)\n\n        ###use only training data to find min-max information, then apply on the whole dataset\n        if cfg.GenTestList:\n            min_max_normaliser.load_min_max_values(label_norm_file)\n        else:\n            min_max_normaliser.find_min_max_values(nn_label_file_list[0:cfg.train_file_number])\n\n        ### enforce silence such that the normalization runs without removing silence: only for final synthesis\n        if cfg.GenTestList and cfg.enforce_silence:\n            min_max_normaliser.normalise_data(binary_label_file_list, nn_label_norm_file_list)\n        else:\n            min_max_normaliser.normalise_data(nn_label_file_list, nn_label_norm_file_list)\n\n        ### make duration data for S2S network ###\n        if cfg.network_type == ""S2S"":\n            logger.info(\'creating duration (input) features for S2S network\')\n            label_normaliser.prepare_dur_data(in_label_align_file_list, file_paths.seq_dur_file_list, cfg.label_type, feature_type=cfg.dur_feature_type, unit_size=cfg.dur_unit_size, feat_size=cfg.dur_feat_size)\n\n            if cfg.remove_silence_from_dur:\n                remover = SilenceRemover(n_cmp = cfg.seq_dur_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type, remove_frame_features = False)\n                remover.remove_silence(file_paths.seq_dur_file_list, in_label_align_file_list, file_paths.seq_dur_file_list)\n        \n    if min_max_normaliser != None and not cfg.GenTestList:\n        ### save label normalisation information for unseen testing labels\n        label_min_vector = min_max_normaliser.min_vector\n        label_max_vector = min_max_normaliser.max_vector\n        label_norm_info = numpy.concatenate((label_min_vector, label_max_vector), axis=0)\n\n        label_norm_info = numpy.array(label_norm_info, \'float32\')\n        fid = open(label_norm_file, \'wb\')\n        label_norm_info.tofile(fid)\n        fid.close()\n        logger.info(\'saved %s vectors to %s\' %(label_min_vector.size, label_norm_file))\n\n    ### make output duration data\n    if cfg.MAKEDUR:\n        logger.info(\'creating duration (output) features\')\n        label_normaliser.prepare_dur_data(in_label_align_file_list, file_paths.dur_file_list, cfg.label_type, cfg.dur_feature_type)\n\n    ### make output acoustic data\n    if cfg.MAKECMP:\n        logger.info(\'creating acoustic (output) features\')\n        delta_win = cfg.delta_win #[-0.5, 0.0, 0.5]\n        acc_win = cfg.acc_win     #[1.0, -2.0, 1.0]\n\n        if cfg.GenTestList:\n            for feature_name in list(cfg.in_dir_dict.keys()):\n                in_file_list_dict[feature_name] = prepare_file_path_list(test_id_list, cfg.in_dir_dict[feature_name], cfg.file_extension_dict[feature_name], False)\n            nn_cmp_file_list      = prepare_file_path_list(test_id_list, nn_cmp_dir, cfg.cmp_ext)\n            nn_cmp_norm_file_list = prepare_file_path_list(test_id_list, nn_cmp_norm_dir, cfg.cmp_ext)\n        \n        acoustic_worker = AcousticComposition(delta_win = delta_win, acc_win = acc_win)\n\n        if \'dur\' in list(cfg.in_dir_dict.keys()) and cfg.AcousticModel:\n            lf0_file_list = file_paths.get_lf0_file_list()\n            acoustic_worker.make_equal_frames(dur_file_list, lf0_file_list, cfg.in_dimension_dict)\n\n        acoustic_worker.prepare_nn_data(in_file_list_dict, nn_cmp_file_list, cfg.in_dimension_dict, cfg.out_dimension_dict)\n\n        if cfg.remove_silence_using_binary_labels:\n            ## do this to get lab_dim:\n            label_composer = LabelComposer()\n            label_composer.load_label_configuration(cfg.label_config_file)\n            lab_dim=label_composer.compute_label_dimension()\n\n            silence_feature = 0 ## use first feature in label -- hardcoded for now\n            logger.info(\'Silence removal from CMP using binary label file\')\n\n            ## overwrite the untrimmed audio with the trimmed version:\n            trim_silence(nn_cmp_file_list, nn_cmp_file_list, cfg.cmp_dim,\n                                binary_label_file_list, lab_dim, silence_feature)\n\n        elif cfg.remove_silence_using_hts_labels: \n            ## back off to previous method using HTS labels:\n            remover = SilenceRemover(n_cmp = cfg.cmp_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type, remove_frame_features = cfg.add_frame_features, subphone_feats = cfg.subphone_feats)\n            remover.remove_silence(nn_cmp_file_list, in_label_align_file_list, nn_cmp_file_list) # save to itself\n\n    ### save acoustic normalisation information for normalising the features back\n    var_dir  = file_paths.var_dir\n    var_file_dict = file_paths.get_var_dic()\n\n    ### normalise output acoustic data\n    if cfg.NORMCMP:\n        logger.info(\'normalising acoustic (output) features using method %s\' % cfg.output_feature_normalisation)\n        cmp_norm_info = None\n        if cfg.output_feature_normalisation == \'MVN\':\n            normaliser = MeanVarianceNorm(feature_dimension=cfg.cmp_dim)\n            if cfg.GenTestList:\n                # load mean std values\n                global_mean_vector, global_std_vector = normaliser.load_mean_std_values(norm_info_file)\n            else:\n                ###calculate mean and std vectors on the training data, and apply on the whole dataset\n                global_mean_vector = normaliser.compute_mean(nn_cmp_file_list[0:cfg.train_file_number], 0, cfg.cmp_dim)\n                global_std_vector = normaliser.compute_std(nn_cmp_file_list[0:cfg.train_file_number], global_mean_vector, 0, cfg.cmp_dim)\n                # for hmpd vocoder we don\'t need to normalize the \n                # pdd values\n                if cfg.vocoder_type == \'hmpd\':\n                    stream_start_index = {}\n                    dimension_index = 0\n                    recorded_vuv = False\n                    vuv_dimension = None\n                    for feature_name in cfg.out_dimension_dict.keys():\n                        if feature_name != \'vuv\':\n                            stream_start_index[feature_name] = dimension_index\n                        else:\n                            vuv_dimension = dimension_index\n                            recorded_vuv = True\n                        \n                        dimension_index += cfg.out_dimension_dict[feature_name]\n                    logger.info(\'hmpd pdd values are not normalized since they are in 0 to 1\')\n                    global_mean_vector[:,stream_start_index[\'pdd\']: stream_start_index[\'pdd\'] + cfg.out_dimension_dict[\'pdd\']] = 0\n                    global_std_vector[:,stream_start_index[\'pdd\']: stream_start_index[\'pdd\'] + cfg.out_dimension_dict[\'pdd\']] = 1\n            normaliser.feature_normalisation(nn_cmp_file_list, nn_cmp_norm_file_list)\n            cmp_norm_info = numpy.concatenate((global_mean_vector, global_std_vector), axis=0)\n\n        elif cfg.output_feature_normalisation == \'MINMAX\':\n            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim, min_value = 0.01, max_value = 0.99)\n            if cfg.GenTestList:\n                min_max_normaliser.load_min_max_values(norm_info_file)\n            else:\n                min_max_normaliser.find_min_max_values(nn_cmp_file_list[0:cfg.train_file_number])\n            min_max_normaliser.normalise_data(nn_cmp_file_list, nn_cmp_norm_file_list)\n\n            cmp_min_vector = min_max_normaliser.min_vector\n            cmp_max_vector = min_max_normaliser.max_vector\n            cmp_norm_info = numpy.concatenate((cmp_min_vector, cmp_max_vector), axis=0)\n\n        else:\n            logger.critical(\'Normalisation type %s is not supported!\\n\' %(cfg.output_feature_normalisation))\n            raise\n\n        if not cfg.GenTestList:\n            cmp_norm_info = numpy.array(cmp_norm_info, \'float32\')\n            fid = open(norm_info_file, \'wb\')\n            cmp_norm_info.tofile(fid)\n            fid.close()\n            logger.info(\'saved %s vectors to %s\' %(cfg.output_feature_normalisation, norm_info_file))\n\n            feature_index = 0\n            for feature_name in list(cfg.out_dimension_dict.keys()):\n                feature_std_vector = numpy.array(global_std_vector[:,feature_index:feature_index+cfg.out_dimension_dict[feature_name]], \'float32\')\n\n                fid = open(var_file_dict[feature_name], \'w\')\n                feature_var_vector = feature_std_vector**2\n                feature_var_vector.tofile(fid)\n                fid.close()\n\n                logger.info(\'saved %s variance vector to %s\' %(feature_name, var_file_dict[feature_name]))\n\n                feature_index += cfg.out_dimension_dict[feature_name]\n\n    train_x_file_list, train_y_file_list = file_paths.get_train_list_x_y()\n    valid_x_file_list, valid_y_file_list = file_paths.get_valid_list_x_y()\n    test_x_file_list, test_y_file_list = file_paths.get_test_list_x_y()\n\n    # we need to know the label dimension before training the DNN\n    # computing that requires us to look at the labels\n    #\n    label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name, add_frame_features=cfg.add_frame_features, subphone_feats=cfg.subphone_feats)\n    add_feat_dim = sum(cfg.additional_features.values())\n    lab_dim = label_normaliser.dimension + add_feat_dim + cfg.appended_input_dim\n    \n    if cfg.VoiceConversion:\n        lab_dim = cfg.cmp_dim\n\n    logger.info(\'label dimension is %d\' % lab_dim)\n\n    combined_model_arch = str(len(hidden_layer_size))\n    for hid_size in hidden_layer_size:\n        combined_model_arch += \'_\' + str(hid_size)\n\n    nnets_file_name = file_paths.get_nnets_file_name()\n    temp_dir_name = file_paths.get_temp_nn_dir_name()\n\n    gen_dir = os.path.join(gen_dir, temp_dir_name)\n\n    if cfg.switch_to_keras or cfg.switch_to_tensorflow:\n        ### set configuration variables ###\n        cfg.inp_dim = lab_dim\n        cfg.out_dim = cfg.cmp_dim\n\n        cfg.inp_feat_dir  = nn_label_norm_dir\n        cfg.out_feat_dir  = nn_cmp_norm_dir\n        cfg.pred_feat_dir = gen_dir\n\n        if cfg.GenTestList and cfg.test_synth_dir!=""None"":\n            cfg.inp_feat_dir  = cfg.test_synth_dir\n            cfg.pred_feat_dir = cfg.test_synth_dir\n        \n    if cfg.switch_to_keras:\n        ### call kerasclass and use an instance ###\n        from run_keras_with_merlin_io import KerasClass\n        keras_instance = KerasClass(cfg)\n    \n    elif cfg.switch_to_tensorflow:\n        ### call Tensorflowclass and use an instance ###\n        from run_tensorflow_with_merlin_io import TensorflowClass\n        tf_instance = TensorflowClass(cfg)\n\n    ### DNN model training\n    if cfg.TRAINDNN:\n\n        var_dict = load_covariance(var_file_dict, cfg.out_dimension_dict)\n\n        logger.info(\'training DNN\')\n\n        fid = open(norm_info_file, \'rb\')\n        cmp_min_max = numpy.fromfile(fid, dtype=numpy.float32)\n        fid.close()\n        cmp_min_max = cmp_min_max.reshape((2, -1))\n        cmp_mean_vector = cmp_min_max[0, ]\n        cmp_std_vector  = cmp_min_max[1, ]\n\n\n        try:\n            os.makedirs(model_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create model directory %s\' % model_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        try:\n            if cfg.switch_to_keras:\n                keras_instance.train_keras_model()\n            elif cfg.switch_to_tensorflow:\n                tf_instance.train_tensorflow_model()\n            else:\n                train_DNN(train_xy_file_list = (train_x_file_list, train_y_file_list), \\\n                      valid_xy_file_list = (valid_x_file_list, valid_y_file_list), \\\n                      nnets_file_name = nnets_file_name, \\\n                      n_ins = lab_dim, n_outs = cfg.cmp_dim, ms_outs = cfg.multistream_outs, \\\n                      hyper_params = cfg.hyper_params, buffer_size = cfg.buffer_size, plot = cfg.plot, var_dict = var_dict,\n                      cmp_mean_vector = cmp_mean_vector, cmp_std_vector = cmp_std_vector, seq_dur_file_list = file_paths.seq_dur_file_list, init_dnn_model_file=cfg.start_from_trained_model)\n        except KeyboardInterrupt:\n            logger.critical(\'train_DNN interrupted via keyboard\')\n            # Could \'raise\' the exception further, but that causes a deep traceback to be printed\n            # which we don\'t care about for a keyboard interrupt. So, just bail out immediately\n            sys.exit(1)\n        except:\n            logger.critical(\'train_DNN threw an exception\')\n            raise\n\n\n\n    if cfg.GENBNFEA:\n        # Please only tune on this step when you want to generate bottleneck features from DNN\n        gen_dir = file_paths.bottleneck_features\n\n        bottleneck_size = min(hidden_layer_size)\n        bottleneck_index = 0\n        for i in range(len(hidden_layer_size)):\n            if hidden_layer_size[i] == bottleneck_size:\n                bottleneck_index = i\n\n        logger.info(\'generating bottleneck features from DNN\')\n\n        try:\n            os.makedirs(gen_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create generation directory %s\' % gen_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        gen_file_id_list = file_id_list[0:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n        test_x_file_list = nn_label_norm_file_list[0:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n        gen_file_list = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.cmp_ext)\n\n        dnn_hidden_generation(test_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list, bottleneck_index)\n\n    ### generate parameters from DNN\n    gen_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n    test_x_file_list  = nn_label_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n    test_y_file_list  = nn_cmp_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n    test_d_file_list = file_paths.seq_dur_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n    if cfg.GenTestList:\n        gen_file_id_list = test_id_list\n        test_x_file_list = nn_label_norm_file_list\n        if cfg.test_synth_dir!=""None"":\n            gen_dir = cfg.test_synth_dir\n\n    if cfg.DNNGEN:\n        logger.info(\'generating from DNN\')\n\n        try:\n            os.makedirs(gen_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create generation directory %s\' % gen_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        gen_file_list = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.cmp_ext)\n\n\n        if cfg.switch_to_keras:\n            keras_instance.test_keras_model()\n        elif cfg.switch_to_tensorflow:\n            tf_instance.test_tensorflow_model()\n        else:\n            reshape_io = True if cfg.rnn_batch_training else False\n            if cfg.network_type == ""S2SD"":\n                dnn_generation_S2S(test_x_file_list, test_d_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)\n            elif cfg.network_type == ""S2S"":\n                test_data_reader = ListDataProvider(x_file_list = test_x_file_list, y_file_list = test_y_file_list, dur_file_list = test_d_file_list) \n                MLU_div = test_data_reader.set_s2s_division(cfg.linguistic_file_name)\n                dnn_generation_S2SML(test_x_file_list, test_d_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, MLU_div, gen_file_list)\n            else:\n                dnn_generation(test_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list, reshape_io)\n\n        logger.debug(\'denormalising generated output using method %s\' % cfg.output_feature_normalisation)\n\n        fid = open(norm_info_file, \'rb\')\n        cmp_min_max = numpy.fromfile(fid, dtype=numpy.float32)\n        fid.close()\n        cmp_min_max = cmp_min_max.reshape((2, -1))\n        cmp_min_vector = cmp_min_max[0, ]\n        cmp_max_vector = cmp_min_max[1, ]\n\n        if cfg.output_feature_normalisation == \'MVN\':\n            denormaliser = MeanVarianceNorm(feature_dimension = cfg.cmp_dim)\n            denormaliser.feature_denormalisation(gen_file_list, gen_file_list, cmp_min_vector, cmp_max_vector)\n\n        elif cfg.output_feature_normalisation == \'MINMAX\':\n            denormaliser = MinMaxNormalisation(cfg.cmp_dim, min_value = 0.01, max_value = 0.99, min_vector = cmp_min_vector, max_vector = cmp_max_vector)\n            denormaliser.denormalise_data(gen_file_list, gen_file_list)\n        else:\n            logger.critical(\'denormalising method %s is not supported!\\n\' %(cfg.output_feature_normalisation))\n            raise\n\n        if cfg.AcousticModel:\n            ##perform MLPG to smooth parameter trajectory\n            ## lf0 is included, the output features much have vuv.\n            generator = ParameterGeneration(gen_wav_features = cfg.gen_wav_features, enforce_silence = cfg.enforce_silence)\n            generator.acoustic_decomposition(gen_file_list, cfg.cmp_dim, cfg.out_dimension_dict, cfg.file_extension_dict, var_file_dict, do_MLPG=cfg.do_MLPG, cfg=cfg)\n\n        if cfg.DurationModel:\n            ### Perform duration normalization(min. state dur set to 1) ###\n            gen_dur_list   = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.dur_ext)\n            gen_label_list = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.lab_ext)\n            in_gen_label_align_file_list = prepare_file_path_list(gen_file_id_list, cfg.in_label_align_dir, cfg.lab_ext, False)\n\n            generator = ParameterGeneration(gen_wav_features = cfg.gen_wav_features)\n            generator.duration_decomposition(gen_file_list, cfg.cmp_dim, cfg.out_dimension_dict, cfg.file_extension_dict)\n\n            label_modifier = HTSLabelModification(silence_pattern = cfg.silence_pattern, label_type = cfg.label_type)\n            label_modifier.modify_duration_labels(in_gen_label_align_file_list, gen_dur_list, gen_label_list)\n\n\n    ### generate wav\n    if cfg.GENWAV:\n        logger.info(\'reconstructing waveform(s)\')\n        generate_wav(gen_dir, gen_file_id_list, cfg)     # generated speech\n#       generate_wav(nn_cmp_dir, gen_file_id_list, cfg)  # reference copy synthesis speech\n\n    ### setting back to original conditions before calculating objective scores ###\n    if cfg.GenTestList:\n        in_label_align_file_list = prepare_file_path_list(file_id_list, cfg.in_label_align_dir, cfg.lab_ext, False)\n        binary_label_file_list   = prepare_file_path_list(file_id_list, binary_label_dir, cfg.lab_ext)\n        gen_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n    ### evaluation: RMSE and CORR for duration\n    if cfg.CALMCD and cfg.DurationModel:\n        logger.info(\'calculating MCD\')\n\n        ref_data_dir = os.path.join(inter_data_dir, \'ref_data\')\n\n        ref_dur_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.dur_ext)\n\n        in_gen_label_align_file_list = in_label_align_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n        calculator = IndividualDistortionComp()\n\n        valid_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n        test_file_id_list  = file_id_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n        if cfg.remove_silence_using_binary_labels:\n            untrimmed_reference_data = in_file_list_dict[\'dur\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n            trim_silence(untrimmed_reference_data, ref_dur_list, cfg.dur_dim, \\\n                                untrimmed_test_labels, lab_dim, silence_feature)\n        else:\n            remover = SilenceRemover(n_cmp = cfg.dur_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type, remove_frame_features = cfg.add_frame_features)\n            remover.remove_silence(in_file_list_dict[\'dur\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_dur_list)\n\n        valid_dur_rmse, valid_dur_corr = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.dur_ext, cfg.dur_dim)\n        test_dur_rmse, test_dur_corr = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.dur_ext, cfg.dur_dim)\n\n        logger.info(\'Develop: DNN -- RMSE: %.3f frames/phoneme; CORR: %.3f; \' \\\n                    %(valid_dur_rmse, valid_dur_corr))\n        logger.info(\'Test: DNN -- RMSE: %.3f frames/phoneme; CORR: %.3f; \' \\\n                    %(test_dur_rmse, test_dur_corr))\n\n    ### evaluation: calculate distortion\n    if cfg.CALMCD and cfg.AcousticModel:\n        logger.info(\'calculating MCD\')\n\n        ref_data_dir = os.path.join(inter_data_dir, \'ref_data\')\n        ref_lf0_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.lf0_ext)\n        # for straight or world vocoders\n        ref_mgc_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.mgc_ext)\n        ref_bap_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.bap_ext)\n        # for GlottDNN vocoder\n        ref_lsf_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.lsf_ext)\n        ref_slsf_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.slsf_ext)\n        ref_gain_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.gain_ext)\n        ref_hnr_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.hnr_ext)\n        # for pulsemodel vocoder\n        ref_pdd_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.pdd_ext)\n\n        in_gen_label_align_file_list = in_label_align_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n        calculator = IndividualDistortionComp()\n\n        spectral_distortion = 0.0\n        bap_mse             = 0.0\n        f0_mse              = 0.0\n        vuv_error           = 0.0\n\n        valid_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n        test_file_id_list  = file_id_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n        if cfg.remove_silence_using_binary_labels:\n            ## get lab_dim:\n            label_composer = LabelComposer()\n            label_composer.load_label_configuration(cfg.label_config_file)\n            lab_dim=label_composer.compute_label_dimension()\n\n            ## use first feature in label -- hardcoded for now\n            silence_feature = 0\n\n            ## Use these to trim silence:\n            untrimmed_test_labels = binary_label_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n\n        if \'mgc\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'mgc\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_mgc_list, cfg.mgc_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            elif cfg.remove_silence_using_hts_labels:\n                remover = SilenceRemover(n_cmp = cfg.mgc_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type)\n                remover.remove_silence(in_file_list_dict[\'mgc\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_mgc_list)\n            else:\n                ref_data_dir = os.path.join(data_dir, \'mgc\')\n            valid_spectral_distortion = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.mgc_ext, cfg.mgc_dim)\n            test_spectral_distortion  = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.mgc_ext, cfg.mgc_dim)\n            valid_spectral_distortion *= (10 /numpy.log(10)) * numpy.sqrt(2.0)    ##MCD\n            test_spectral_distortion  *= (10 /numpy.log(10)) * numpy.sqrt(2.0)    ##MCD\n\n\n        if \'bap\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'bap\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_bap_list, cfg.bap_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            elif cfg.remove_silence_using_hts_labels:\n                remover = SilenceRemover(n_cmp = cfg.bap_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type)\n                remover.remove_silence(in_file_list_dict[\'bap\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_bap_list)\n            else:\n                ref_data_dir = os.path.join(data_dir, \'bap\')\n            valid_bap_mse = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.bap_ext, cfg.bap_dim)\n            test_bap_mse  = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.bap_ext, cfg.bap_dim)\n            valid_bap_mse = valid_bap_mse / 10.0    ##Cassia\'s bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC\n            test_bap_mse  = test_bap_mse / 10.0    ##Cassia\'s bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC\n\n        if \'lf0\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'lf0\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_lf0_list, cfg.lf0_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            elif cfg.remove_silence_using_hts_labels:\n                remover = SilenceRemover(n_cmp = cfg.lf0_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type)\n                remover.remove_silence(in_file_list_dict[\'lf0\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_lf0_list)\n            else:\n                ref_data_dir = os.path.join(data_dir, \'lf0\')\n            valid_f0_mse, valid_f0_corr, valid_vuv_error   = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.lf0_ext, cfg.lf0_dim)\n            test_f0_mse , test_f0_corr, test_vuv_error    = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.lf0_ext, cfg.lf0_dim)\n        \n        if \'lsf\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'lsf\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_lsf_list, cfg.lsf_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.lsf_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type)\n                remover.remove_silence(in_file_list_dict[\'lsf\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_lsf_list)\n            valid_spectral_distortion = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.lsf_ext, cfg.lsf_dim)\n            test_spectral_distortion  = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.lsf_ext, cfg.lsf_dim)\n        \n        if \'slsf\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'slsf\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_slsf_list, cfg.slsf_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.slsf_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type)\n                remover.remove_silence(in_file_list_dict[\'slsf\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_slsf_list)\n            valid_spectral_distortion = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.slsf_ext, cfg.slsf_dim)\n            test_spectral_distortion  = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.slsf_ext, cfg.slsf_dim)\n        \n        if \'hnr\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'hnr\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_hnr_list, cfg.hnr_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.hnr_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type)\n                remover.remove_silence(in_file_list_dict[\'hnr\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_hnr_list)\n            valid_spectral_distortion = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.hnr_ext, cfg.hnr_dim)\n            test_spectral_distortion  = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.hnr_ext, cfg.hnr_dim)\n        \n        if \'gain\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'gain\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_gain_list, cfg.gain_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.gain_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type)\n                remover.remove_silence(in_file_list_dict[\'gain\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_gain_list)\n            valid_spectral_distortion = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.gain_ext, cfg.gain_dim)\n            test_spectral_distortion  = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.gain_ext, cfg.gain_dim)\n        \n        if \'pdd\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'pdd\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_pdd_list, cfg.pdd_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.pdd_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type)\n                remover.remove_silence(in_file_list_dict[\'pdd\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_pdd_list)\n            valid_spectral_distortion = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.pdd_ext, cfg.pdd_dim)\n            test_spectral_distortion  = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.pdd_ext, cfg.pdd_dim)\n        \n\n        logger.info(\'Develop: DNN -- MCD: %.3f dB; BAP: %.3f dB; F0:- RMSE: %.3f Hz; CORR: %.3f; VUV: %.3f%%\' \\\n                    %(valid_spectral_distortion, valid_bap_mse, valid_f0_mse, valid_f0_corr, valid_vuv_error*100.))\n        logger.info(\'Test   : DNN -- MCD: %.3f dB; BAP: %.3f dB; F0:- RMSE: %.3f Hz; CORR: %.3f; VUV: %.3f%%\' \\\n                    %(test_spectral_distortion , test_bap_mse , test_f0_mse , test_f0_corr, test_vuv_error*100.))\n\nif __name__ == \'__main__\':\n\n\n\n    # these things should be done even before trying to parse the command line\n\n    # create a configuration instance\n    # and get a short name for this instance\n    cfg=configuration.cfg\n\n    # set up logging to use our custom class\n    logging.setLoggerClass(LoggerPlotter)\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n\n    if len(sys.argv) != 2:\n        logger.critical(\'usage: run_merlin.sh [config file name]\')\n        sys.exit(1)\n\n    config_file = sys.argv[1]\n\n    config_file = os.path.abspath(config_file)\n    cfg.configure(config_file)\n\n\n    logger.info(\'Installation information:\')\n    logger.info(\'  Merlin directory: \'+os.path.abspath(os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir)))\n    logger.info(\'  PATH:\')\n    env_PATHs = os.getenv(\'PATH\')\n    if env_PATHs:\n        env_PATHs = env_PATHs.split(\':\')\n        for p in env_PATHs:\n            if len(p)>0: logger.info(\'      \'+p)\n    logger.info(\'  LD_LIBRARY_PATH:\')\n    env_LD_LIBRARY_PATHs = os.getenv(\'LD_LIBRARY_PATH\')\n    if env_LD_LIBRARY_PATHs:\n        env_LD_LIBRARY_PATHs = env_LD_LIBRARY_PATHs.split(\':\')\n        for p in env_LD_LIBRARY_PATHs:\n            if len(p)>0: logger.info(\'      \'+p)\n    logger.info(\'  Python version: \'+sys.version.replace(\'\\n\',\'\'))\n    logger.info(\'    PYTHONPATH:\')\n    env_PYTHONPATHs = os.getenv(\'PYTHONPATH\')\n    if env_PYTHONPATHs:\n        env_PYTHONPATHs = env_PYTHONPATHs.split(\':\')\n        for p in env_PYTHONPATHs:\n            if len(p)>0:\n                logger.info(\'      \'+p)\n    logger.info(\'  Numpy version: \'+numpy.version.version)\n    logger.info(\'  Theano version: \'+theano.version.version)\n    logger.info(\'    THEANO_FLAGS: \'+os.getenv(\'THEANO_FLAGS\'))\n    logger.info(\'    device: \'+theano.config.device)\n\n    # Check for the presence of git\n    ret = os.system(\'git status > /dev/null\')\n    if ret==0:\n        logger.info(\'  Git is available in the working directory:\')\n        git_describe = subprocess.Popen([\'git\', \'describe\', \'--tags\', \'--always\'], stdout=subprocess.PIPE).communicate()[0][:-1]\n        logger.info(\'    Merlin version: {}\'.format(git_describe))\n        git_branch = subprocess.Popen([\'git\', \'rev-parse\', \'--abbrev-ref\', \'HEAD\'], stdout=subprocess.PIPE).communicate()[0][:-1]\n        logger.info(\'    branch: {}\'.format(git_branch))\n        git_diff = subprocess.Popen([\'git\', \'diff\', \'--name-status\'], stdout=subprocess.PIPE).communicate()[0]\n        if sys.version_info.major >= 3:\n            git_diff = git_diff.decode(\'utf-8\')\n        git_diff = git_diff.replace(\'\\t\',\' \').split(\'\\n\')\n        logger.info(\'    diff to Merlin version:\')\n        for filediff in git_diff:\n            if len(filediff)>0: logger.info(\'      \'+filediff)\n        logger.info(\'      (all diffs logged in \'+os.path.basename(cfg.log_file)+\'.gitdiff\'+\')\')\n        os.system(\'git diff > \'+cfg.log_file+\'.gitdiff\')\n\n    logger.info(\'Execution information:\')\n    logger.info(\'  HOSTNAME: \'+socket.getfqdn())\n    logger.info(\'  USER: \'+os.getenv(\'USER\'))\n    logger.info(\'  PID: \'+str(os.getpid()))\n    PBS_JOBID = os.getenv(\'PBS_JOBID\')\n    if PBS_JOBID:\n        logger.info(\'  PBS_JOBID: \'+PBS_JOBID)\n\n\n    if cfg.profile:\n        logger.info(\'profiling is activated\')\n        import cProfile, pstats\n        cProfile.run(\'main_function(cfg)\', \'mainstats\')\n\n        # create a stream for the profiler to write to\n        profiling_output = io.StringIO()\n        p = pstats.Stats(\'mainstats\', stream=profiling_output)\n\n        # print stats to that stream\n        # here we just report the top 10 functions, sorted by total amount of time spent in each\n        p.strip_dirs().sort_stats(\'tottime\').print_stats(10)\n\n        # print the result to the log\n        logger.info(\'---Profiling result follows---\\n%s\' %  profiling_output.getvalue() )\n        profiling_output.close()\n        logger.info(\'---End of profiling result---\')\n\n    else:\n        main_function(cfg)\n\n    sys.exit(0)\n'"
src/run_tensorflow_with_merlin_io.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://github.com/CSTR-Edinburgh/merlin\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\nimport os\nimport sys\nimport time\nimport tensorflow as tf\nfrom tensorflow_lib import configuration\nfrom tensorflow_lib import data_utils\nfrom tensorflow_lib.train import TrainTensorflowModels,Train_Encoder_Decoder_Models\n\nclass TensorflowClass(object):\n\n    def __init__(self, cfg):\n\n        ###################################################\n        ########## User configurable variables ############\n        ###################################################\n\n        inp_feat_dir  = cfg.inp_feat_dir\n        out_feat_dir  = cfg.out_feat_dir\n        pred_feat_dir = cfg.pred_feat_dir\n\n        inp_file_ext = cfg.inp_file_ext\n        out_file_ext = cfg.out_file_ext\n\n        ### Input-Output ###\n\n        self.inp_dim = cfg.inp_dim\n        self.out_dim = cfg.out_dim\n\n        self.inp_norm = cfg.inp_norm\n        self.out_norm = cfg.out_norm\n\n        self.inp_stats_file = cfg.inp_stats_file\n        self.out_stats_file = cfg.out_stats_file\n\n        self.inp_scaler = None\n        self.out_scaler = None\n\n        #### define model params ####\n\n        self.hidden_layer_type = cfg.hidden_layer_type\n        self.hidden_layer_size = cfg.hidden_layer_size\n\n        self.sequential_training = cfg.sequential_training\n        self.encoder_decoder     = cfg.encoder_decoder\n        \n        self.attention    = cfg.attention\n        self.cbhg         = cfg.cbhg\n        self.batch_size   = cfg.batch_size\n        self.shuffle_data = cfg.shuffle_data\n\n        self.output_layer_type = cfg.output_layer_type\n        self.loss_function     = cfg.loss_function\n        self.optimizer         = cfg.optimizer\n\n        self.rnn_params    = cfg.rnn_params\n        self.dropout_rate  = cfg.dropout_rate\n        self.num_of_epochs = cfg.num_of_epochs\n\n        ### Define the work directory###\n        self.model_dir = cfg.model_dir\n\n        ### define train, valid, test ###\n\n        train_file_number = cfg.train_file_number\n        valid_file_number = cfg.valid_file_number\n        test_file_number  = cfg.test_file_number\n\n        file_id_scp  = cfg.file_id_scp\n        test_id_scp  = cfg.test_id_scp\n\n        #### main processess ####\n\n        self.NORMDATA   = cfg.NORMDATA\n        self.TRAINMODEL = cfg.TRAINMODEL\n        self.TESTMODEL  = cfg.TESTMODEL\n\n        #### Generate only test list ####\n        self.GenTestList = cfg.GenTestList\n        \n        ###################################################\n        ####### End of user-defined conf variables ########\n        ###################################################\n        \n        #### Create train, valid and test file lists ####\n        file_id_list = data_utils.read_file_list(file_id_scp)\n\n        train_id_list = file_id_list[0: train_file_number]\n        valid_id_list = file_id_list[train_file_number: train_file_number + valid_file_number]\n        test_id_list  = file_id_list[train_file_number + valid_file_number: train_file_number + valid_file_number + test_file_number]\n        \n        valid_test_id_list = file_id_list[train_file_number: train_file_number + valid_file_number + test_file_number]\n\n        self.inp_train_file_list = data_utils.prepare_file_path_list(train_id_list, inp_feat_dir, inp_file_ext)\n        self.out_train_file_list = data_utils.prepare_file_path_list(train_id_list, out_feat_dir, out_file_ext)\n\n        self.inp_valid_file_list = data_utils.prepare_file_path_list(valid_id_list, inp_feat_dir, inp_file_ext)\n        self.out_valid_file_list = data_utils.prepare_file_path_list(valid_id_list, out_feat_dir, out_file_ext)\n\n        self.inp_test_file_list = data_utils.prepare_file_path_list(valid_test_id_list, inp_feat_dir, inp_file_ext)\n        self.out_test_file_list = data_utils.prepare_file_path_list(valid_test_id_list, out_feat_dir, out_file_ext)\n\n        self.gen_test_file_list = data_utils.prepare_file_path_list(valid_test_id_list, pred_feat_dir, out_file_ext)\n\n        if self.GenTestList:\n            test_id_list = data_utils.read_file_list(test_id_scp)\n            self.inp_test_file_list = data_utils.prepare_file_path_list(test_id_list, inp_feat_dir, inp_file_ext)\n            self.gen_test_file_list = data_utils.prepare_file_path_list(test_id_list, pred_feat_dir, out_file_ext)\n\n        if not self.encoder_decoder:\n          self.tensorflow_models = TrainTensorflowModels(self.inp_dim, self.hidden_layer_size, self.out_dim, self.hidden_layer_type, self.model_dir, \n                                                output_type=self.output_layer_type, dropout_rate=self.dropout_rate,\n                                                loss_function=self.loss_function, optimizer=self.optimizer)\n        else:\n            self.encoder_decoder_models = Train_Encoder_Decoder_Models(self.inp_dim,self.hidden_layer_size,self.out_dim,self.hidden_layer_type,output_type=self.output_layer_type,\\\n                                                                     dropout_rate=self.dropout_rate,loss_function=self.loss_function,optimizer=self.optimizer,\\\n                                                                     attention=self.attention,cbhg=self.cbhg)\n    def normlize_data(self):\n        ### normalize train data ###\n        if os.path.isfile(self.inp_stats_file) and os.path.isfile(self.out_stats_file):\n            self.inp_scaler = data_utils.load_norm_stats(self.inp_stats_file, self.inp_dim, method=self.inp_norm)\n            self.out_scaler = data_utils.load_norm_stats(self.out_stats_file, self.out_dim, method=self.out_norm)\n        else:\n            print(\'preparing train_x, train_y from input and output feature files...\')\n            train_x, train_y, train_flen = data_utils.read_data_from_file_list(self.inp_train_file_list, self.out_train_file_list,\\\n                    self.inp_dim, self.out_dim, sequential_training=True if self.sequential_training or self.encoder_decoder else False)\n\n            print(\'computing norm stats for train_x...\')\n            inp_scaler = data_utils.compute_norm_stats(train_x, self.inp_stats_file, method=self.inp_norm)\n\n            print(\'computing norm stats for train_y...\')\n            out_scaler = data_utils.compute_norm_stats(train_y, self.out_stats_file, method=self.out_norm)\n\n    def train_tensorflow_model(self):\n        print(\'preparing train_x, train_y from input and output feature files...\')\n                 #### load the data ####\n\n        train_x, train_y, train_flen = data_utils.read_data_from_file_list(self.inp_train_file_list, self.out_train_file_list,\n                     self.inp_dim, self.out_dim, sequential_training=True if self.sequential_training or self.encoder_decoder else False)\n                #### normalize the data ####\n        data_utils.norm_data(train_x, self.inp_scaler, sequential_training=True if self.sequential_training or self.encoder_decoder else False)\n        data_utils.norm_data(train_y, self.out_scaler, sequential_training=True if self.sequential_training or self.encoder_decoder else False)\n\n        #### define the model ####\n        if self.sequential_training:\n           utt_length=train_flen[""utt2framenum""].values()\n           self.tensorflow_models.get_max_step(max(utt_length))\n           self.tensorflow_models.define_sequence_model()\n\n        elif self.encoder_decoder:\n             utt_length=train_flen[""utt2framenum""].values()\n             super(Train_Encoder_Decoder_Models,self.encoder_decoder_models).__setattr__(""max_step"",max(utt_length))\n             self.encoder_decoder_models.define_encoder_decoder()\n        else:\n            self.tensorflow_models.define_feedforward_model()\n\n        #### train the model ####\n        print(\'training...\')\n        if self.sequential_training:\n            ### Train feedforward model ###\n            self.tensorflow_models.train_sequence_model(train_x, train_y, batch_size=self.batch_size, num_of_epochs=self.num_of_epochs, shuffle_data=self.shuffle_data,utt_length=utt_length)\n\n        elif self.encoder_decoder:\n            self.encoder_decoder_models.train_encoder_decoder_model(train_x,train_y,batch_size=self.batch_size,num_of_epochs=self.num_of_epochs,shuffle_data=True,utt_length=utt_length)\n        else:\n            self.tensorflow_models.train_feedforward_model(train_x, train_y, batch_size=self.batch_size, num_of_epochs=self.num_of_epochs, shuffle_data=self.shuffle_data)\n\n    def test_tensorflow_model(self):\n         #### load the data ####\n         print(\'preparing test_x from input feature files...\')\n         test_x, test_flen = data_utils.read_test_data_from_file_list(self.inp_test_file_list, self.inp_dim)\n\n          #### normalize the data ####\n         data_utils.norm_data(test_x, self.inp_scaler)\n          #### compute predictions ####\n         if self.encoder_decoder:\n             self.encoder_decoder_models.predict(test_x,self.out_scaler,self.gen_test_file_list)\n         else:\n             self.tensorflow_models.predict(test_x, self.out_scaler, self.gen_test_file_list, self.sequential_training)\n\n    def main_function(self):\n         ### Implement each module ###\n         if self.NORMDATA:\n            self.normlize_data()\n\n         if self.TRAINMODEL:\n            self.train_tensorflow_model()\n\n         if self.TESTMODEL:\n            self.test_tensorflow_model()\n\nif __name__==""__main__"":\n\n    if len(sys.argv) != 2:\n        print(\'usage: python run_tensorflow_with_merlin_io.py [config file name]\')\n        sys.exit(1)\n\n    # create a configuration instance\n    # and get a short name for this instance\n    cfg = configuration.configuration()\n\n    config_file = sys.argv[1]\n\n    config_file = os.path.abspath(config_file)\n    cfg.configure(config_file)\n\n    print(""--- Job started ---"")\n    start_time = time.time()\n\n    # main function\n    tensorflow_instance = TensorflowClass(cfg)\n  # except:\n   #         print ""inp stats file is %s""%cfg.inp_stats_file\n    #        sys.exit(0)\n    tensorflow_instance.main_function()\n\n    (m, s) = divmod(int(time.time() - start_time), 60)\n    print(""--- Job completion time: %d min. %d sec ---"" % (m, s))\n\n    sys.exit(0)\n'"
src/validation.py,0,"b'#! /usr/bin/python2 -u\n# -*- coding: utf-8 -*-\n#\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Script to validate Merlin setup.\n""""""\n\n__author__ = \'pasindu@google.com (Pasindu De Silva)\'\n\nimport logging\nimport logging.config\nimport os\nimport sys\nimport configuration\nfrom utils.utils import read_file_list\n\nlogger = logging.getLogger(\'validation\')\n\n\nclass Validation(object):\n  """"""Runs Merlin validations\n  """"""\n  _is_valid = True\n\n  def __init__(self, cfg):\n    self.cfg = cfg\n\n  def is_valid(self):\n    """"""Returns whether the given configuration file is valid.""""""\n\n    self.validate_label_settings()\n    self.validate_acoustic_files()\n    return self._is_valid;\n\n  def validate_label_settings(self):\n    if self.cfg.label_style != \'HTS\':\n      self._is_valid = False\n      logging.error(\n          \'Only HTS-style labels are now supported as input to Merlin\')\n\n  def validate_acoustic_files(self):\n    """"""Validates that acoustic features exists in given path.\n\n    Args:\n      cfg: Merlin configuration.\n    """"""\n    file_types_to_check = [\n        {\n            \'name\': \'mgc\',\n            \'dir\': self.cfg.in_mgc_dir,\n            \'ext\': self.cfg.mgc_ext\n        },\n        {\n            \'name\': \'bap\',\n            \'dir\': self.cfg.in_bap_dir,\n            \'ext\': self.cfg.bap_ext\n        },\n        {\n            \'name\': \'lf0\',\n            \'dir\': self.cfg.in_lf0_dir,\n            \'ext\': self.cfg.lf0_ext\n        },\n        {\n            \'name\': \'label_align\',\n            \'dir\': self.cfg.in_label_align_dir,\n            \'ext\': self.cfg.lab_ext\n        },\n    ]\n\n    file_ids = read_file_list(self.cfg.file_id_scp)\n    actual_total = len(file_ids)\n\n    expected_total = self.cfg.train_file_number + self.cfg.valid_file_number + self.cfg.test_file_number\n\n    if expected_total > actual_total:\n      logger.error(\'Expected %d files but found %d files\', expected_total,\n                   actual_total)\n\n    for file_id in file_ids:\n      for path_info in file_types_to_check:\n        path = \'%s/%s%s\' % (path_info[\'dir\'], file_id, path_info[\'ext\'])\n        if not os.path.exists(path):\n          self._is_valid = False\n          logger.error(\'File id %s missing feature %s at %s\', file_id,\n                       path_info[\'name\'], path)\n\n\ndef main(args):\n  if len(args) <= 1:\n    sys.stderr.write(\'Usage - python src/validation path_to_conf1 path_to_conf2 ...\\n\')\n    exit(1)\n\n  for config_file in args[1:]:\n\n    logging.info(\'Validating %s configuration.\', config_file)\n\n    cfg = configuration.cfg\n    cfg.configure(config_file)\n    validation = Validation(cfg)\n\n    if validation.is_valid():\n      logging.info(\'Configuration file %s passed validation checks.\', config_file)\n    else:\n      logging.error(\'Configuration file %s contains errors.\', config_file)\n\nif __name__ == \'__main__\':\n  main(sys.argv)\n'"
test/test_classes.py,0,"b'# Test the classes used in Merlin pipeline\n# TODO run some very simple training on random data)\n\nimport sys\nimport os\nsys.path.append(\'../src\')\nimport errno\n\nimport numpy as np\nimport cPickle\nimport logging\n\ndef makedir(path):\n    try:\n        os.makedirs(path)\n    except OSError as exception:\n        if exception.errno != errno.EEXIST:\n            raise\n\ndef build_model(hidden_layer_type):\n    logger.info(\'  DeepRecurrentNetwork \'+str(hidden_layer_type))\n    nnmodel = DeepRecurrentNetwork(8, 16*np.ones(len(hidden_layer_type)), 4, L1_reg=0.0, L2_reg=0.00001, hidden_layer_type=hidden_layer_type)\n    \n    # Always try to save it and reload it\n    modelfile = \'log/model.pkl\'\n    makedir(\'log\')\n    cPickle.dump(nnmodel, open(modelfile, \'wb\'))\n    nnmodel = cPickle.load(open(modelfile, \'rb\'))\n\n    logger.info(\'    OK\')\n\n    return nnmodel\n\nif __name__ == \'__main__\':\n\n    # Get a logger for these tests\n    logging.basicConfig(format=\'%(asctime)s %(levelname)8s%(name)15s: %(message)s\')\n    logger = logging.getLogger(""test"")\n    logger.setLevel(logging.DEBUG)\n\n    logger.info(\'Testing Merlin classes\')\n\n    # Build various models\n    logger.info(\'Build models without training\')\n    from models.deep_rnn import DeepRecurrentNetwork\n    nnmodel = build_model([\'TANH\'])\n    del nnmodel\n    nnmodel = build_model([\'TANH\', \'TANH\'])\n    del nnmodel\n    nnmodel = build_model([\'LSTM\', \'LSTM\'])\n    del nnmodel\n    nnmodel = build_model([\'SLSTM\', \'SLSTM\'])\n    del nnmodel\n'"
test/test_compare.py,0,"b'import sys\nimport argparse\n\nimport numpy as np\n\ndef similar_reals(ref, test, tol, colnames=None):\n    \'\'\'\n    Compare vector test against vector ref with a tolerance of tol (common scalar or vector)\n    \'\'\'\n\n    ref = np.array(ref)\n    test = np.array(test)\n    tol = np.atleast_1d(tol)\n\n    if len(ref)!=len(test):\n        raise ValueError(\'Cannot compare arrays of different size\')\n\n    if len(tol)==1:\n        tol = tol*np.ones(len(ref))\n\n\n    row_format =""{:>10}"" * len(ref)\n\n    if colnames:\n        print(\'           \'+row_format.format(*colnames))\n    print(\'Reference: \'+row_format.format(*ref))\n    print(\'Test:      \'+row_format.format(*test))\n    print(\'Diff:      \'+row_format.format(*(ref-test)))\n    print(\'Tolerance: \'+row_format.format(*tol))\n    if any(abs(ref-test)>tol):\n        print(\'FAILED\')\n        return False\n\n    return True\n\nif __name__ == \'__main__\':\n    argpar = argparse.ArgumentParser()\n    argpar.add_argument(""--ref"", nargs=\'+\', type=float, default=None, help=""Reference values."")\n    argpar.add_argument(""--test"", nargs=\'+\', type=float, help=""Values to test against the references."")\n    argpar.add_argument(""--tol"", nargs=\'+\', type=float, default=0.1, help=""Accepted tolerance (if a single value is provided, it is used for all compared pairs"")\n    argpar.add_argument(""--colnames"", nargs=\'+\', default=None, help=""Names for each column"")\n    args = argpar.parse_args()\n\n    if not similar_reals(args.ref, args.test, args.tol, args.colnames):\n        sys.exit(1)\n'"
test/test_file_paths.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \'License\');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \'AS IS\' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests FilePaths class.\n""""""\n\n__author__ = \'pasindu@google.com (Pasindu De Silva)\'\n\nimport logging.config  # pylint: disable=unused-import\nimport sys\n# pylint: disable=g-import-not-at-top\nsys.path.append(\'../src\')\nimport configuration\nfrom utils.file_paths import FilePaths\n\n\ndef test_file_paths(cfg):\n  """"""Tests FilePaths constructor.\n\n  Args:\n    cfg: Merlin configuration\n  """"""\n  cfg.GenTestList = True\n  file_paths = FilePaths(cfg)\n\n  assert len(\n      file_paths.file_id_list) == 100, \'Number of files in file list incorrect\'\n  assert len(\n      file_paths.test_id_list) == 100, \'Number of test in file list incorrect\'\n\n\ndef test_nn_out_in_data_sets(cfg):\n  """"""Tests Train, Valid and Test filelists.\n\n  Args:\n    cfg: Merlin configuration\n  """"""\n  file_paths = FilePaths(cfg)\n  file_paths.set_label_dir(0, \'ext\', 0)\n  file_paths.set_label_file_list()\n\n  train_x_file_list, train_y_file_list = file_paths.get_train_list_x_y()\n  valid_x_file_list, valid_y_file_list = file_paths.get_valid_list_x_y()\n  test_x_file_list, test_y_file_list = file_paths.get_test_list_x_y()\n\n  assert len(train_x_file_list\n            ) == cfg.train_file_number, \'train set x axis dimension incorrect\'\n  assert len(valid_x_file_list\n            ) == cfg.valid_file_number, \'valid set x axis dimension incorrect\'\n  assert len(test_x_file_list\n            ) == cfg.test_file_number, \'test set x axis dimension incorrect\'\n\n  assert len(train_y_file_list\n            ) == cfg.train_file_number, \'train set y axis dimension incorrect\'\n  assert len(valid_y_file_list\n            ) == cfg.valid_file_number, \'valid set y axis dimension incorrect\'\n  assert len(test_y_file_list\n            ) == cfg.test_file_number, \'test set y axis dimension incorrect\'\n\n\ndef test_label_file_lists(cfg):\n  """"""Tests label filelists.\n\n  Args:\n    cfg: Merlin configuration\n  """"""\n  file_paths = FilePaths(cfg)\n  file_paths.set_label_dir(0, \'ext\', 0)\n  file_paths.set_label_file_list()\n\n  # Case 1: GenTestList = False and test_synth_dir = None\n  assert file_paths.in_label_align_file_list[\n      0] == \'/tmp/label_state_align/file1.lab\'\n  assert file_paths.binary_label_file_list[\n      0] == \'/tmp/inter_module/binary_label_0/file1.lab\'\n  assert file_paths.nn_label_file_list[\n      0] == \'/tmp/inter_module/nn_no_silence_lab_ext/file1.lab\'\n  assert file_paths.nn_label_norm_file_list[\n      0] == \'/tmp/inter_module/nn_no_silence_lab_norm_ext/file1.lab\'\n\n  # Case 2: GenTestList = True and test_synth_dir = None\n  cfg.GenTestList = True\n  file_paths = FilePaths(cfg)\n  file_paths.set_label_dir(0, \'ext\', 0)\n  file_paths.set_label_file_list()\n  assert file_paths.in_label_align_file_list[\n      0] == \'/tmp/label_state_align/test1.lab\'\n  assert file_paths.binary_label_file_list[\n      0] == \'/tmp/inter_module/binary_label_0/test1.lab\'\n  assert file_paths.nn_label_file_list[\n      0] == \'/tmp/inter_module/nn_no_silence_lab_ext/test1.lab\'\n  assert file_paths.nn_label_norm_file_list[\n      0] == \'/tmp/inter_module/nn_no_silence_lab_norm_ext/test1.lab\'\n\n  # Case 3: GenTestList = True and test_synth_dir = test_synth\n  cfg.GenTestList = True\n  cfg.test_synth_dir = \'test_synth\'\n  file_paths = FilePaths(cfg)\n  file_paths.set_label_dir(0, \'ext\', 0)\n  file_paths.set_label_file_list()\n  assert file_paths.in_label_align_file_list[\n      0] == \'/tmp/label_state_align/test1.lab\'\n  assert file_paths.binary_label_file_list[0] == \'test_synth/test1.lab\'\n  assert file_paths.nn_label_file_list[0] == \'test_synth/test1.lab\'\n  assert file_paths.nn_label_norm_file_list[0] == \'test_synth/test1.lab\'\n\n\ndef _get_config_file():\n  cfg = configuration.cfg\n  cfg.configure(\'test_data/test.conf\')\n  return cfg\n\n\ndef main():\n  test_file_paths(_get_config_file())\n  test_nn_out_in_data_sets(_get_config_file())\n  test_label_file_lists(_get_config_file())\n\n\nif __name__ == \'__main__\':\n  main()\n'"
src/configuration/__init__.py,0,"b""################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\nfrom . import configuration\n\n# instantiate one object of this class\ncfg = configuration.configuration()\n"""
src/configuration/configuration.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n\nimport math\nimport sys\nif sys.version_info.major >= 3:\n    import configparser\nelse:\n    import ConfigParser as configparser\nimport os\nimport logging\nimport io\nimport sys\nimport textwrap\nimport datetime\n\nclass configuration(object):\n\n    """"""Configuration settings. Any user-specific values are read from an external file\n    and parsed by an instance of the built-in ConfigParser class""""""\n\n    def __init__(self):\n        # doesn\'t do anything\n        pass\n\n\n    def configure(self, configFile=None, use_logging=True):\n\n        # get a logger\n        logger = logging.getLogger(""configuration"")\n        # this (and only this) logger needs to be configured immediately, otherwise it won\'t work\n        # we can\'t use the full user-supplied configuration mechanism in this particular case,\n        # because we haven\'t loaded it yet!\n        #\n        # so, just use simple console-only logging\n        logger.setLevel(logging.INFO) # this level is hardwired here - should change it to INFO\n        # add a handler & its formatter - will write only to console\n        ch = logging.StreamHandler()\n        logger.addHandler(ch)\n        formatter = logging.Formatter(\'%(asctime)s %(levelname)8s%(name)15s: %(message)s\')\n        ch.setFormatter(formatter)\n\n\n        # first, set up some default configuration values\n        self.initial_configuration()\n\n        # next, load in any user-supplied configuration values\n        # that might over-ride the default values\n        self.user_configuration(configFile)\n\n        # now that we have loaded the user\'s configuration, we can load the\n        # separate config file for logging (the name of that file will be specified in the config file)\n        if use_logging:\n            self.logging_configuration()\n\n        # finally, set up all remaining configuration values\n        # that depend upon either default or user-supplied values\n        self.complete_configuration()\n\n        logger.debug(\'configuration completed\')\n\n    def initial_configuration(self):\n\n        # to be called before loading any user specific values\n\n        # things to put here are\n        # 1. variables that the user cannot change\n        # 2. variables that need to be set before loading the user\'s config file\n\n        UTTID_REGEX = \'(.*)\\..*\'\n\n\n\n    def user_configuration(self,configFile=None):\n\n        # get a logger\n        logger = logging.getLogger(""configuration"")\n\n        # load and parse the provided configFile, if provided\n        if not configFile:\n            logger.warn(\'no user configuration file provided; using only built-in default settings\')\n            return\n\n        # load the config file\n        try:\n            cfgparser = configparser.ConfigParser()\n            cfgparser.readfp(open(configFile))\n            logger.debug(\'successfully read and parsed user configuration file %s\' % configFile)\n        except:\n            logger.fatal(\'error reading user configuration file %s\' % configFile)\n            raise\n\n\n        #work_dir must be provided before initialising other directories\n        self.work_dir = None\n\n        if self.work_dir == None:\n            try:\n                self.work_dir = cfgparser.get(\'Paths\', \'work\')\n\n            except (configparser.NoSectionError, configparser.NoOptionError):\n                if self.work_dir == None:\n                    logger.critical(\'Paths:work has no value!\')\n                    raise Exception\n\n        # look for those items that are user-configurable, and get their values\n        # sptk_bindir= ....\n\n        # default place for some data\n        self.data_dir       = os.path.join(self.work_dir, \'data\')\n        self.inter_data_dir = os.path.join(self.work_dir, \'inter_module\')\n\n        self.gen_dir     = os.path.join(self.work_dir, \'gen\')\n        self.model_dir   = os.path.join(self.work_dir, \'nnets_model\')\n        self.stats_dir   = os.path.join(self.work_dir, \'stats\')\n\n        self.def_inp_dir    = os.path.join(self.inter_data_dir, \'nn_no_silence_lab_norm_425\')\n        self.def_out_dir    = os.path.join(self.inter_data_dir, \'nn_norm_mgc_lf0_vuv_bap_187\')\n\n\n        # a list instead of a dict because OrderedDict is not available until 2.7\n        # and I don\'t want to import theano here just for that one class\n        # each entry is a tuple of (variable name, default value, section in config file, option name in config file)\n        #\n        # the type of the default value is important and controls the type that the corresponding\n        # variable will have\n        #\n        # to set a default value of \'undefined\' use an empty string\n        # or the special value \'impossible\', as appropriate\n        #\n        impossible_int=int(-99999)\n        impossible_float=float(-99999.0)\n\n        user_options = [\n\n            (\'work_dir\', self.work_dir, \'Paths\',\'work\'),\n            (\'data_dir\', self.data_dir, \'Paths\',\'data\'),\n            (\'inter_data_dir\', self.inter_data_dir, \'Paths\',\'inter_data\'),\n            (\'plot_dir\', \'\', \'Paths\',\'plot\'),\n\n            (\'inp_feat_dir\', self.def_inp_dir, \'Paths\', \'inp_feat\'),\n            (\'out_feat_dir\', self.def_out_dir, \'Paths\', \'out_feat\'),\n\n            (\'model_dir\', self.model_dir, \'Paths\', \'models\'),\n            (\'stats_dir\', self.stats_dir, \'Paths\', \'stats\'),\n            (\'gen_dir\'  ,   self.gen_dir, \'Paths\', \'gen\'),\n            (\'pred_feat_dir\',self.gen_dir, \'Paths\', \'pred_feat\'),\n\n            (\'plot\',      False, \'Utility\', \'plot\'),\n            (\'profile\',   False, \'Utility\', \'profile\'),\n\n            (\'file_id_scp\'       , os.path.join(self.work_dir, \'data/file_id_list.scp\')    , \'Paths\', \'file_id_list\'),\n            (\'test_id_scp\'       , os.path.join(self.work_dir, \'data/test_id_list.scp\')    , \'Paths\', \'test_id_list\'),\n\n            (\'GV_dir\'       , os.path.join(self.work_dir, \'data/GV\' )  , \'Paths\', \'GV_dir\'),\n\n            (\'in_stepw_dir\' , os.path.join(self.work_dir, \'data/stepw\'), \'Paths\', \'in_stepw_dir\'),\n            (\'in_mgc_dir\'   , os.path.join(self.work_dir, \'data/mgc\')  , \'Paths\', \'in_mgc_dir\'),\n            (\'in_lf0_dir\'   , os.path.join(self.work_dir, \'data/lf0\')  , \'Paths\', \'in_lf0_dir\'),\n            (\'in_bap_dir\'   , os.path.join(self.work_dir, \'data/bap\')  , \'Paths\', \'in_bap_dir\'),\n            (\'in_sp_dir\'    , os.path.join(self.work_dir, \'data/sp\' )  , \'Paths\', \'in_sp_dir\'),\n            (\'in_seglf0_dir\', os.path.join(self.work_dir, \'data/lf03\') , \'Paths\', \'in_seglf0_dir\'),\n\n            ## for glottHMM:\n            (\'in_F0_dir\'   , os.path.join(self.work_dir, \'data/F0\')  , \'Paths\', \'in_F0_dir\'),\n            (\'in_Gain_dir\'   , os.path.join(self.work_dir, \'data/Gain\')  , \'Paths\', \'in_Gain_dir\'),\n            (\'in_HNR_dir\'   , os.path.join(self.work_dir, \'data/HNR\')  , \'Paths\', \'in_HNR_dir\'),\n            (\'in_LSF_dir\'   , os.path.join(self.work_dir, \'data/LSF\')  , \'Paths\', \'in_LSF_dir\'),\n            (\'in_LSFsource_dir\'   , os.path.join(self.work_dir, \'data/LSFsource\')  , \'Paths\', \'in_LSFsource_dir\'),\n\n            ## for glottDNN:\n            (\'in_f0_dir\'   , os.path.join(self.work_dir, \'data/f0\')  , \'Paths\', \'in_f0_dir\'),\n            (\'in_gain_dir\'   , os.path.join(self.work_dir, \'data/gain\')  , \'Paths\', \'in_gain_dir\'),\n            (\'in_hnr_dir\'   , os.path.join(self.work_dir, \'data/hnr\')  , \'Paths\', \'in_hnr_dir\'),\n            (\'in_lsf_dir\'   , os.path.join(self.work_dir, \'data/lsf\')  , \'Paths\', \'in_lsf_dir\'),\n            (\'in_slsf_dir\'   , os.path.join(self.work_dir, \'data/slsf\')  , \'Paths\', \'in_slsf_dir\'),\n\n            ## for sinusoidal:\n            (\'in_pdd_dir\'   , os.path.join(self.work_dir, \'data/pdd\')  , \'Paths\', \'in_pdd_dir\'),\n\n            ## For MagPhase Vocoder:\n            (\'in_acous_feats_dir\' , os.path.join(self.work_dir, \'data/in_acoustic_feats\'), \'Paths\', \'in_acous_feats_dir\'),\n            (\'nat_wav_dir\'        , os.path.join(self.work_dir, \'data/nat_wavs\')         , \'Paths\', \'nat_wav_dir\'), # Containg natural speech waveforms (for acous feat extraction).\n\n            # Input-Output\n            (\'inp_dim\', 425, \'Input-Output\', \'inp_dim\'),\n            (\'out_dim\', 187, \'Input-Output\', \'out_dim\'),\n\n            (\'inp_file_ext\', \'.lab\', \'Input-Output\', \'inp_file_ext\'),\n            (\'out_file_ext\', \'.cmp\', \'Input-Output\', \'out_file_ext\'),\n\n            (\'inp_norm\', \'MINMAX\', \'Input-Output\', \'inp_norm\'),\n            (\'out_norm\', \'MINMAX\', \'Input-Output\', \'out_norm\'),\n\n            ## for joint duration\n            (\'in_seq_dur_dir\' , os.path.join(self.work_dir, \'data/S2S_dur\')  , \'Paths\', \'in_seq_dur_dir\'),\n            (\'in_dur_dir\'     , os.path.join(self.work_dir, \'data/dur\')      , \'Paths\', \'in_dur_dir\'),\n\n\n            (\'nn_norm_temp_dir\', os.path.join(self.work_dir, \'data/step_hidden9\'), \'Paths\', \'nn_norm_temp_dir\'),\n\n            (\'process_labels_in_work_dir\', False, \'Labels\', \'process_labels_in_work_dir\'),\n\n\n\n            (\'label_style\'        , \'HTS\'                                                 ,    \'Labels\', \'label_style\'),\n            (\'label_type\'         , \'state_align\'                                         ,    \'Labels\', \'label_type\'),\n            (\'in_label_align_dir\' , os.path.join(self.work_dir, \'data/label_state_align\') ,    \'Labels\', \'label_align\'),\n            (\'question_file_name\' , os.path.join(self.work_dir, \'data/questions.hed\')     ,    \'Labels\', \'question_file_name\'),\n            (\'linguistic_file_name\' , os.path.join(self.work_dir, \'data/hed_feats.txt\')   ,    \'Labels\', \'linguistic_file_name\'),\n            (\'silence_pattern\'    , [\'*-#+*\']                                             ,    \'Labels\', \'silence_pattern\'),\n            (\'subphone_feats\'     , \'full\'                                                ,    \'Labels\', \'subphone_feats\'),\n            (\'additional_features\', {}                                                    ,    \'Labels\', \'additional_features\'),\n\n            ## For MagPhase Vocoder:\n            #(\'label_align_orig_const_rate_dir\', os.path.join(self.work_dir, \'data/label_state_align\'), \'Labels\', \'label_align_orig_const_rate\'),\n\n            (\'xpath_file_name\',      os.path.join(self.work_dir, \'data/xml_labels/xpaths.txt\'), \'Labels\', \'xpath_file_name\'),\n\n            (\'label_config_file\',    \'configuration/examplelabelconfigfile.py\',                 \'Labels\', \'label_config\'),\n            (\'add_frame_features\',      True,                                                   \'Labels\', \'add_frame_features\'),\n            (\'fill_missing_values\',  False,                                                     \'Labels\', \'fill_missing_values\'),\n            (\'xpath_label_align_dir\', os.path.join(self.work_dir, \'data/label_state_align\'),    \'Labels\', \'xpath_label_align\'),\n\n            (\'enforce_silence\', False, \'Labels\', \'enforce_silence\'),\n            (\'remove_silence_using_binary_labels\', False, \'Labels\', \'remove_silence_using_binary_labels\'),\n            (\'remove_silence_using_hts_labels\', True, \'Labels\', \'remove_silence_using_hts_labels\'),\n\n            (\'precompile_xpaths\', True, \'Labels\', \'precompile_xpaths\'),\n            (\'iterate_over_frames\', True, \'Labels\', \'iterate_over_frames\'),\n\n            (\'appended_input_dim\'   ,  0                   ,  \'Labels\'       ,  \'appended_input_dim\'),\n\n            (\'buffer_size\', 200000, \'Data\', \'buffer_size\'),\n\n            (\'train_file_number\', impossible_int, \'Data\',\'train_file_number\'),\n            (\'valid_file_number\', impossible_int, \'Data\',\'valid_file_number\'),\n            (\'test_file_number\' , impossible_int, \'Data\',\'test_file_number\'),\n\n            (\'log_path\', os.path.join(self.work_dir, \'log\'), \'Paths\', \'log_path\'),\n            (\'log_file\', \'\', \'Paths\',\'log_file\'),\n            (\'log_config_file\', \'configuration/exampleloggingconfigfile.conf\', \'Paths\', \'log_config_file\'),\n\n            (\'sptk_bindir\'    , \'tools/bin/SPTK-3.9\'    , \'Paths\', \'sptk\'),\n            (\'straight_bindir\', \'tools/bin/straight\'    , \'Paths\', \'straight\'),\n            (\'world_bindir\'   , \'tools/bin/WORLD\'       , \'Paths\', \'world\'),\n            (\'glotthmm_bindir\', \'tools/bin/glotthmm\'    , \'Paths\', \'glotthmm\'),\n            (\'glottdnn_bindir\', \'tools/bin/glottdnn\'    , \'Paths\', \'glottdnn\'),\n            (\'hmpd_bindir\'    , \'tools/bin/hmpd\'        , \'Paths\', \'hmpd\'),\n            (\'magphase_bindir\', \'tools/bin/magphase/src\', \'Paths\', \'magphase\'),\n\n            (\'network_type\'           , \'RNN\'                                           , \'Architecture\', \'network_type\'),\n            (\'model_type\'           , \'DNN\'                                             , \'Architecture\', \'model_type\'),\n            (\'hidden_layer_type\'    , [\'TANH\', \'TANH\', \'TANH\', \'TANH\', \'TANH\', \'TANH\']  , \'Architecture\', \'hidden_layer_type\'),\n            (\'output_layer_type\'    , \'LINEAR\'                                          , \'Architecture\', \'output_layer_type\'),\n            (\'sequential_training\'  , False                                             , \'Architecture\', \'sequential_training\'),\n            (\'rnn_batch_training\'   , False                                             , \'Architecture\', \'rnn_batch_training\'),\n            (\'dropout_rate\'         , 0.0                                               , \'Architecture\', \'dropout_rate\'),\n            (\'switch_to_keras\'      , False                                             , \'Architecture\', \'switch_to_keras\'),\n            (\'switch_to_tensorflow\' , False                                             , \'Architecture\', \'switch_to_tensorflow\'),\n\n            ## some config variables for token projection DNN\n            (\'scheme\'               , \'stagewise\'                   , \'Architecture\', \'scheme\'),\n            (\'index_to_project\'    , 0       , \'Architecture\', \'index_to_project\'),\n            (\'projection_insize\'    , 10000        , \'Architecture\', \'projection_insize\'),\n            (\'projection_outsize\'    , 10        , \'Architecture\', \'projection_outsize\'),\n            (\'initial_projection_distrib\'    , \'gaussian\'    , \'Architecture\', \'initial_projection_distrib\'),\n            (\'projection_weights_output_dir\'    , \'some_path\', \'Architecture\', \'projection_weights_output_dir\'),\n            (\'layers_with_projection_input\'    , [0], \'Architecture\', \'layers_with_projection_input\'),\n            (\'projection_learning_rate_scaling\'    , 1.0, \'Architecture\', \'projection_learning_rate_scaling\'),\n\n            (\'num_of_epochs\',   1, \'Architecture\', \'training_epochs\'),\n\n            (\'optimizer\'        ,   \'sgd\', \'Architecture\', \'optimizer\'),\n            (\'loss_function\'    ,    \'mse\', \'Architecture\', \'loss_function\'),\n\n            # RNN\n            (\'model_file_name\'    , \'feed_forward_6_tanh\',\'Architecture\', \'model_file_name\'),\n            (\'stateful\'           , False, \'Architecture\', \'stateful\'),\n            (\'use_high_batch_size\', False, \'Architecture\', \'use_high_batch_size\'),\n\n            (\'training_algo\',   1, \'Architecture\', \'training_algo\'),\n            (\'merge_size\'   ,   1, \'Architecture\', \'merge_size\'),\n            (\'seq_length\'   , 200, \'Architecture\', \'seq_length\'),\n            (\'bucket_range\' , 100, \'Architecture\', \'bucket_range\'),\n\n            (\'encoder_decoder\'      , False                                           ,  \'Architecture\',\'encoder_decoder\'),\n            (\'attention\'            , False                                           ,  \'Architecture\', \'attention\'),\n            (""cbhg""                 , False                                           ,   ""Architecture"", ""cbhg""),\n            \n            # Data\n            (\'shuffle_data\', True, \'Data\', \'shuffle_data\'),\n\n            # Keras Processes\n            (\'NORMDATA\'  , False, \'Processes\', \'NORMDATA\'),\n            (\'TRAINMODEL\', False, \'Processes\', \'TRAINMODEL\'),\n            (\'TESTMODEL\' , False, \'Processes\', \'TESTMODEL\'),\n\n\n            (\'learning_rate\'        , 0.0002                          , \'Architecture\', \'learning_rate\'),\n            (\'lr_decay\'             , -1                              , \'Architecture\', \'lr_decay\'),\n            (\'l2_reg\'               , 0.00001                      , \'Architecture\', \'L2_regularization\'),\n            (\'l1_reg\'               , 0.0                           , \'Architecture\', \'L1_regularization\'),\n            (\'batch_size\'           , 16                            , \'Architecture\', \'batch_size\'),\n            (\'training_epochs\'      , 25                            , \'Architecture\', \'training_epochs\'),\n            (\'hidden_activation\'    , \'tanh\'                        , \'Architecture\', \'hidden_activation\'),\n            (\'output_activation\'    , \'linear\'                      , \'Architecture\', \'output_activation\'),\n            (\'hidden_layer_size\'  , [1024, 1024, 1024, 1024, 1024, 1024], \'Architecture\', \'hidden_layer_size\'),\n            (\'private_hidden_sizes\' , [1024]                         , \'Architecture\', \'private_hidden_sizes\'),\n            (\'stream_weights\'       , [1.0]                         , \'Architecture\', \'stream_weights\'),\n            (\'private_l2_reg\'       , 0.00001                       , \'Architecture\', \'private_l2_reg\'),\n            (\'warmup_epoch\'         , 5                             , \'Architecture\', \'warmup_epoch\'),\n\n            (\'warmup_momentum\' ,    0.3                           , \'Architecture\', \'warmup_momentum\'),\n            (\'momentum\' ,           0.9                           , \'Architecture\', \'momentum\'),\n            (\'warmup_epoch\' ,       5                             , \'Architecture\', \'warmup_epoch\'),\n            (\'mdn_component\',       1                             , \'Architecture\', \'mdn_component\'),\n            (\'var_floor\',           0.01                          , \'Architecture\', \'var_floor\'),\n            (\'beta_opt\',            False                         , \'Architecture\', \'beta_opt\'),\n            (\'eff_sample_size\',     0.8                           , \'Architecture\', \'eff_sample_size\'),\n            (\'mean_log_det\',        -100.0                        , \'Architecture\', \'mean_log_det\'),\n            (\'start_from_trained_model\',  \'_\'                     , \'Architecture\', \'start_from_trained_model\'),\n            (\'use_rprop\',           0                             , \'Architecture\', \'use_rprop\'),\n            (\'use_lhuc\',           False                             , \'Architecture\', \'use_lhuc\'),\n            (\'freeze_layers\',      0                              , \'Architecture\', \'freeze_layers\'),\n\n            (\'mgc_dim\' ,60     ,\'Outputs\',\'mgc\'),\n            (\'dmgc_dim\',60 * 3 ,\'Outputs\',\'dmgc\'),\n            (\'vuv_dim\' ,1      ,\'Outputs\',\'vuv\'),\n            (\'lf0_dim\' ,1      ,\'Outputs\',\'lf0\'),\n            (\'dlf0_dim\',1 * 3  ,\'Outputs\',\'dlf0\'),\n            (\'bap_dim\' ,25     ,\'Outputs\',\'bap\'),\n            (\'dbap_dim\',25 * 3 ,\'Outputs\',\'dbap\'),\n            (\'cmp_dim\'          ,(60 * 3) + 1 + (1 * 3) + (25 * 3) ,\'Outputs\',\'cmp\'),\n            (\'stepw_dim\'        , 55, \'Outputs\', \'stepw_dim\'),\n            (\'temp_sp_dim\'      , 1025, \'Outputs\', \'temp_sp_dim\'),\n            (\'seglf0_dim\'       , 7                 , \'Outputs\', \'seglf0_dim\'),\n            (\'delta_win\'        , [-0.5, 0.0, 0.5]  , \'Outputs\', \'delta_win\'),\n            (\'acc_win\'          , [1.0, -2.0, 1.0]  , \'Outputs\', \'acc_win\'),\n            (\'do_MLPG\'          , True              , \'Outputs\', \'do_MLPG\'),\n\n            ## for GlottHMM:\n            (\'F0_dim\' ,1     ,\'Outputs\',\'F0\'),\n            (\'dF0_dim\',1 * 3 ,\'Outputs\',\'dF0\'),\n            (\'Gain_dim\' ,1     ,\'Outputs\',\'Gain\'),\n            (\'dGain_dim\',1 * 3 ,\'Outputs\',\'dGain\'),\n            (\'HNR_dim\' ,5     ,\'Outputs\',\'HNR\'),\n            (\'dHNR_dim\',5 * 3 ,\'Outputs\',\'dHNR\'),\n            (\'LSF_dim\' ,30     ,\'Outputs\',\'LSF\'),\n            (\'dLSF_dim\',30 * 3 ,\'Outputs\',\'dLSF\'),\n            (\'LSFsource_dim\' ,10     ,\'Outputs\',\'LSFsource\'),\n            (\'dLSFsource_dim\',10 * 3 ,\'Outputs\',\'dLSFsource\'),\n\n            ## for GlottDNN:\n             (\'f0_dim\' ,1     ,\'Outputs\',\'f0\'),\n            (\'df0_dim\',1 * 3 ,\'Outputs\',\'df0\'),\n            (\'gain_dim\' ,1     ,\'Outputs\',\'gain\'),\n            (\'dgain_dim\',1 * 3 ,\'Outputs\',\'dgain\'),\n            (\'hnr_dim\' ,5     ,\'Outputs\',\'hnr\'),\n            (\'dhnr_dim\',5 * 3 ,\'Outputs\',\'dhnr\'),\n            (\'lsf_dim\' ,30     ,\'Outputs\',\'lsf\'),\n            (\'dlsf_dim\',30 * 3 ,\'Outputs\',\'dlsf\'),\n            (\'slsf_dim\' ,10     ,\'Outputs\',\'slsf\'),\n            (\'dslsf_dim\',10 * 3 ,\'Outputs\',\'dslsf\'),\n        \n            ## for sinusoidal:\n            (\'pdd_dim\', 25, \'Outputs\', \'pdd\'),\n            (\'dpdd_dim\', 25 * 3, \'Outputs\', \'dpdd\'),\n\n            ## For MagPhase Vocoder:\n            (\'mag_dim\'  , 60    , \'Outputs\', \'mag\'),\n            (\'dmag_dim\' , 60 * 3, \'Outputs\', \'dmag\'),\n            (\'real_dim\' , 45    , \'Outputs\', \'real\'),\n            (\'dreal_dim\', 45 * 3, \'Outputs\', \'dreal\'),\n            (\'imag_dim\' , 45    , \'Outputs\', \'imag\'),\n            (\'dimag_dim\', 45 * 3, \'Outputs\', \'dimag\'),\n\n        ## for joint dur:-\n            (\'seq_dur_dim\' ,1     ,\'Outputs\',\'seq_dur\'),\n            (\'remove_silence_from_dur\'  , True  , \'Outputs\', \'remove_silence_from_dur\'),\n            (\'dur_dim\' ,5     ,\'Outputs\',\'dur\'),\n            (\'dur_feature_type\' , \'numerical\' , \'Outputs\', \'dur_feature_type\'),\n            (\'dur_unit_size\' , \'phoneme\' , \'Outputs\', \'dur_unit_size\'),\n            (\'dur_feat_size\' , \'phoneme\' , \'Outputs\', \'dur_feat_size\'),\n\n            (\'output_feature_normalisation\', \'MVN\', \'Outputs\', \'output_feature_normalisation\'),\n\n            (\'multistream_switch\'  , False , \'Streams\', \'multistream_switch\'),\n#            (\'use_private_hidden\'  , False, \'Streams\', \'use_private_hidden\'),\n\n            (\'output_features\' , [\'mgc\',\'lf0\', \'vuv\', \'bap\'], \'Streams\', \'output_features\'),\n            (\'gen_wav_features\', [\'mgc\', \'bap\', \'lf0\']      , \'Streams\', \'gen_wav_features\'),\n\n            (\'vocoder_type\'     ,\'STRAIGHT\'            ,\'Waveform\'  , \'vocoder_type\'),\n            (\'sr\'               ,48000                 ,\'Waveform\'  , \'samplerate\'),\n            (\'fl\'               ,4096                  ,\'Waveform\'  , \'framelength\'),\n            (\'shift\'            ,1000 * 240 / 48000    ,\'Waveform\'  , \'frameshift\'),\n            (\'sp_dim\'           ,(4096 / 2) + 1        ,\'Waveform\'  , \'sp_dim\'),\n            # fw_alpha: \'Bark\' or \'ERB\' allowing deduction of alpha, or explicity float value (e.g. 0.77)\n            (\'fw_alpha\'         ,0.77                  ,\'Waveform\'  , \'fw_alpha\'),\n            (\'pf_coef\'          ,1.4                   ,\'Waveform\'  , \'postfilter_coef\'),\n            (\'co_coef\'          ,2047                  ,\'Waveform\'  , \'minimum_phase_order\'),\n            (\'use_cep_ap\'       ,True                  ,\'Waveform\'  , \'use_cep_ap\'),\n            (\'do_post_filtering\',True                  ,\'Waveform\'  , \'do_post_filtering\'),\n            (\'apply_GV\'         ,False                 ,\'Waveform\'  , \'apply_GV\'),\n            (\'test_synth_dir\'   ,\'test_synthesis/wav\'  ,\'Waveform\'  , \'test_synth_dir\'),\n\n            ## For MagPhase Vocoder:\n            #(\'use_magphase_pf\'  ,True                 ,\'Waveform\'  , \'use_magphase_pf\'), # Use MagPhase own Post-Filter (experimemental)\n            (\'magphase_pf_type\'   , [\'magphase\', \'no\', \'merlin\']  , \'Waveform\', \'magphase_pf_type\'),\n            (\'magphase_const_rate\', False                         , \'Waveform\', \'magphase_const_rate\'),\n\n\n            (\'DurationModel\'        , False, \'Processes\', \'DurationModel\'),\n            (\'AcousticModel\'        , False, \'Processes\', \'AcousticModel\'),\n            (\'VoiceConversion\'      , False, \'Processes\', \'VoiceConversion\'),\n            (\'GenTestList\'          , False, \'Processes\', \'GenTestList\'),\n\n            (\'ACFTEXTR\'        , False, \'Processes\', \'ACFTEXTR\'), # Acoustic feature extraction\n            (\'NORMLAB\'         , False, \'Processes\', \'NORMLAB\'),\n            (\'MAKEDUR\'         , False, \'Processes\', \'MAKEDUR\'),\n            (\'MAKECMP\'         , False, \'Processes\', \'MAKECMP\'),\n            (\'NORMCMP\'         , False, \'Processes\', \'NORMCMP\'),\n            (\'TRAINDNN\'        , False, \'Processes\', \'TRAINDNN\'),\n            (\'DNNGEN\'          , False, \'Processes\', \'DNNGEN\'),\n            (\'GENWAV\'          , False, \'Processes\', \'GENWAV\'),\n            (\'CALMCD\'          , False, \'Processes\', \'CALMCD\'),\n            (\'NORMSTEP\'        , False, \'Processes\', \'NORMSTEP\'),\n            (\'GENBNFEA\'        , False, \'Processes\', \'GENBNFEA\'),\n\n            (\'mgc_ext\'   , \'.mgc\'     , \'Extensions\', \'mgc_ext\'),\n            (\'bap_ext\'   , \'.bap\'     , \'Extensions\', \'bap_ext\'),\n            (\'lf0_ext\'   , \'.lf0\'     , \'Extensions\', \'lf0_ext\'),\n            (\'cmp_ext\'   , \'.cmp\'     , \'Extensions\', \'cmp_ext\'),\n            (\'lab_ext\'   , \'.lab\'     , \'Extensions\', \'lab_ext\'),\n            (\'utt_ext\'   , \'.utt\'     , \'Extensions\', \'utt_ext\'),\n            (\'stepw_ext\' , \'.stepw\'   , \'Extensions\', \'stepw_ext\'),\n            (\'sp_ext\'    , \'.sp\'      , \'Extensions\', \'sp_ext\'),\n\n\n            ## GlottHMM\n            (\'F0_ext\'   , \'.F0\'     , \'Extensions\', \'F0_ext\'),\n            (\'Gain_ext\'   , \'.Gain\'     , \'Extensions\', \'Gain_ext\'),\n            (\'HNR_ext\'   , \'.HNR\'     , \'Extensions\', \'HNR_ext\'),\n            (\'LSF_ext\'   , \'.LSF\'     , \'Extensions\', \'LSF_ext\'),\n            (\'LSFsource_ext\'   , \'.LSFsource\'     , \'Extensions\', \'LSFsource_ext\'),\n\n             ## GlottDNN\n            (\'f0_ext\'   , \'.f0\'     , \'Extensions\', \'f0_ext\'),\n            (\'gain_ext\'   , \'.gain\'     , \'Extensions\', \'gain_ext\'),\n            (\'hnr_ext\'   , \'.hnr\'     , \'Extensions\', \'hnr_ext\'),\n            (\'lsf_ext\'   , \'.lsf\'     , \'Extensions\', \'lsf_ext\'),\n            (\'slsf_ext\'   , \'.slsf\'     , \'Extensions\', \'slsf_ext\'),\n\n            ## sinusoidal\n            (\'pdd_ext\'  , \'.pdd\', \'Extensions\', \'pdd_ext\'),\n\n            ## For MagPhase Vocoder:\n            (\'mag_ext\'   , \'.mag\'     , \'Extensions\', \'mag_ext\'),\n            (\'real_ext\'  , \'.real\'    , \'Extensions\', \'real_ext\'),\n            (\'imag_ext\'  , \'.imag\'    , \'Extensions\', \'imag_ext\'),\n\n            ## joint dur\n            (\'dur_ext\'   , \'.dur\'     , \'Extensions\', \'dur_ext\'),\n\n        ]\n\n\n        # this uses exec(...) which is potentially dangerous since arbitrary code could be executed\n        for (variable,default,section,option) in user_options:\n            value=None\n\n            try:\n                # first, look for a user-set value for this variable in the config file\n                value = cfgparser.get(section,option)\n                user_or_default=\'user\'\n\n            except (configparser.NoSectionError, configparser.NoOptionError):\n                # use default value, if there is one\n                if (default == None) or \\\n                   (default == \'\')   or \\\n                   ((type(default) == int) and (default == impossible_int)) or \\\n                   ((type(default) == float) and (default == impossible_float))  :\n                    logger.critical(\'%20s has no value!\' % (section+"":""+option) )\n                    raise Exception\n                else:\n                    value = default\n                    user_or_default=\'default\'\n\n            if   type(default) == str:\n                exec(\'self.%s = ""%s""\'      % (variable,value))\n            elif type(default) == int:\n                exec(\'self.%s = int(%s)\'   % (variable,value))\n            elif type(default) == float:\n                exec(\'self.%s = float(%s)\' % (variable,value))\n            elif type(default) == bool:\n                exec(\'self.%s = bool(%s)\'  % (variable,value))\n            elif type(default) == list:\n                exec(\'self.%s = list(%s)\'  % (variable,value))\n            elif type(default) == dict:\n                exec(\'self.%s = dict(%s)\'  % (variable,value))\n            else:\n                logger.critical(\'Variable %s has default value of unsupported type %s\',variable,type(default))\n                raise Exception(\'Internal error in configuration settings: unsupported default type\')\n\n            logger.info(\'%20s has %7s value %s\' % (section+"":""+option,user_or_default,value) )\n\n\n        self.combined_feature_name = \'\'\n        for feature_name in self.output_features:\n            self.combined_feature_name += \'_\'\n            self.combined_feature_name += feature_name\n\n        self.combined_model_name = self.model_type\n        for hidden_type in self.hidden_layer_type:\n            self.combined_model_name += \'_\' + hidden_type\n\n        self.combined_model_name += \'_\' + self.output_layer_type\n\n\n    def complete_configuration(self):\n        # to be called after reading any user-specific settings\n        # because the values set here depend on those user-specific settings\n\n        # get a logger\n        logger = logging.getLogger(""configuration"")\n\n        # tools\n        self.SPTK = {\n            \'X2X\'    : os.path.join(self.sptk_bindir,\'x2x\'),\n            \'MERGE\'  : os.path.join(self.sptk_bindir,\'merge\'),\n            \'BCP\'    : os.path.join(self.sptk_bindir,\'bcp\'),\n            \'MLPG\'   : os.path.join(self.sptk_bindir,\'mlpg\'),\n            \'MGC2SP\' : os.path.join(self.sptk_bindir,\'mgc2sp\'),\n            \'VSUM\'   : os.path.join(self.sptk_bindir,\'vsum\'),\n            \'VSTAT\'  : os.path.join(self.sptk_bindir,\'vstat\'),\n            \'SOPR\'   : os.path.join(self.sptk_bindir,\'sopr\'),\n            \'VOPR\'   : os.path.join(self.sptk_bindir,\'vopr\'),\n            \'FREQT\'  : os.path.join(self.sptk_bindir,\'freqt\'),\n            \'C2ACR\'  : os.path.join(self.sptk_bindir,\'c2acr\'),\n            \'MC2B\'   : os.path.join(self.sptk_bindir,\'mc2b\'),\n            \'B2MC\'  : os.path.join(self.sptk_bindir,\'b2mc\')\n            }\n\n        self.STRAIGHT = {\n            \'SYNTHESIS_FFT\' : os.path.join(self.straight_bindir, \'synthesis_fft\'),\n            \'BNDAP2AP\'      : os.path.join(self.straight_bindir, \'bndap2ap\'),\n            }\n\n        self.WORLD = {\n            \'SYNTHESIS\'     : os.path.join(self.world_bindir, \'synth\'),\n            \'ANALYSIS\'      : os.path.join(self.world_bindir, \'analysis\'),\n            }\n        \n        self.GLOTTHMM= {\n            \'SYNTHESIS\'     : os.path.join(self.glotthmm_bindir, \'Synthesis\'),\n            \'config_file\'   : os.path.join(self.glotthmm_bindir, \'config_default_48\'),\n            \'config_file_16\'   : os.path.join(self.glotthmm_bindir, \'config_default_16\'),\n            }\n\n        self.GLOTTDNN = {\n            \'SYNTHESIS\'     : os.path.join(self.glottdnn_bindir, \'Synthesis\'),         \n            \'config_file\'   : os.path.join(self.glottdnn_bindir, \'config_default_48\'),\n            \'config_file_16\'   : os.path.join(self.glottdnn_bindir, \'config_default_16\'),\n            }\n\n        self.HMPD = {\n            \'SYNTHESIS\'     : os.path.join(self.hmpd_bindir, \'synthesis.py\'),\n           }\n\n\n        # set input extension same as output for voice conversion\n        if self.VoiceConversion:\n            self.remove_silence_using_hts_labels = False\n            self.lab_ext = self.cmp_ext\n\n        # check if any hidden layer is recurrent layer \n        list_of_RNNs = [\'RNN\', \'LSTM\', \'GRU\', \'BLSTM\', \'SLSTM\', \'SGRU\', \'BSLSTM\']\n        for hidden_type in self.hidden_layer_type:\n            if hidden_type in list_of_RNNs:\n                self.sequential_training = True\n                break\n\n        # switch to tensorflow\n        if self.switch_to_tensorflow:\n            ## create directories if not exists\n            self.model_dir = os.path.join(self.model_dir, ""tensorflow"")\n            self.model_dir = os.path.join(self.model_dir, self.model_file_name)\n            if not os.path.exists(self.model_dir):\n                os.makedirs(self.model_dir)\n\n        # switch to keras\n        if self.switch_to_keras:\n            ## create directories if not exists\n            self.model_dir = os.path.join(self.model_dir, ""keras"")\n            if not os.path.exists(self.model_dir):\n                os.makedirs(self.model_dir)\n\n            # model files\n            self.json_model_file = os.path.join(self.model_dir, self.model_file_name+\'.json\')\n            self.h5_model_file   = os.path.join(self.model_dir, self.model_file_name+\'.h5\')\n\n        if self.switch_to_keras and self.switch_to_tensorflow:\n            logger.critical(""Please switch to either tensorflow or keras, but not both!!"")\n            sys.exit(1)\n\n        if self.switch_to_keras or self.switch_to_tensorflow:\n            if not os.path.exists(self.gen_dir):\n                os.makedirs(self.gen_dir)\n\n            # input-output normalization stat files\n            self.inp_stats_file = os.path.join(self.stats_dir, ""input_%d_%s_%d.norm"" %(int(self.train_file_number), self.inp_norm, self.inp_dim))\n            self.out_stats_file = os.path.join(self.stats_dir, ""output_%d_%s_%d.norm"" %(int(self.train_file_number), self.out_norm, self.out_dim))\n\n            # define model file name\n            logger.info(\'model file: %s\' % (self.model_file_name))\n\n            # predicted features directory\n            self.pred_feat_dir = os.path.join(self.gen_dir, self.model_file_name)\n            if not os.path.exists(self.pred_feat_dir):\n                os.makedirs(self.pred_feat_dir)\n\n            # string.lower for some architecture values\n            self.output_layer_type = self.output_layer_type.lower()\n            self.optimizer         = self.optimizer.lower()\n            self.loss_function     = self.loss_function.lower()\n            for i in range(len(self.hidden_layer_type)):\n                self.hidden_layer_type[i] = self.hidden_layer_type[i].lower()\n\n            # force optimizer to adam if set to sgd\n            if self.optimizer == ""sgd"":\n                self.optimizer = \'adam\'\n\n            # set sequential training True if using LSTMs\n            if \'lstm\' in self.hidden_layer_type:\n                self.sequential_training = True\n\n            # set default seq length for duration model\n            if self.DurationModel and self.training_algo == 3 and self.seq_length>50:\n                self.seq_length = 20\n\n            # rnn params\n            self.rnn_params = {}\n            self.rnn_params[\'merge_size\']   = self.merge_size\n            self.rnn_params[\'seq_length\']   = self.seq_length\n            self.rnn_params[\'bucket_range\'] = self.bucket_range\n            self.rnn_params[\'stateful\']     = self.stateful\n\n    \n        ### RNN params\n        if self.sequential_training:\n            # batch training for RNNs\n            if self.batch_size>1:\n                self.rnn_batch_training = True\n\n            # set/limit batch size to 25\n            if self.batch_size>50:\n                if not self.use_high_batch_size:\n                    logger.info(\'reducing the batch size from %s to 25\' % (self.batch_size))\n                    self.batch_size = 25 ## num. of sentences in this case\n\n        ###dimensions for the output features\n        ### key name must follow the self.in_dimension_dict.\n        ### If do not want to include dynamic feature, just use the same dimension as that self.in_dimension_dict\n        ### if lf0 is one of the acoustic featues, the out_dimension_dict must have an additional \'vuv\' key\n        ### a bit confusing\n\n        ###need to control the order of the key?\n        self.in_dir_dict  = {}          ##dimensions for each raw acoustic (output of NN) feature\n        self.out_dimension_dict = {}\n        self.in_dimension_dict = {}\n\n        self.private_hidden_sizes = []\n        self.stream_weights = []\n\n        logger.debug(\'setting up output features\')\n        self.cmp_dim = 0\n        for feature_name in self.output_features:\n            logger.debug(\' %s\' % feature_name)\n\n            in_dimension = 0\n            out_dimension = 0\n            in_directory = \'\'\n#            current_stream_hidden_size = 0\n#            current_stream_weight = 0.0\n#            stream_lr_ratio = 0.0\n            if feature_name == \'mgc\':\n                in_dimension  = self.mgc_dim\n                out_dimension = self.dmgc_dim\n                in_directory  = self.in_mgc_dir\n\n#                current_stream_hidden_size = self.stream_mgc_hidden_size\n#                current_stream_weight      = self.stream_weight_mgc\n            elif feature_name == \'bap\':\n                in_dimension = self.bap_dim\n                out_dimension = self.dbap_dim\n                in_directory  = self.in_bap_dir\n\n#                current_stream_hidden_size = self.stream_bap_hidden_size\n#                current_stream_weight      = self.stream_weight_bap\n            elif feature_name == \'lf0\':\n                in_dimension = self.lf0_dim\n                out_dimension = self.dlf0_dim\n                in_directory  = self.in_lf0_dir\n                if self.vocoder_type == \'MAGPHASE\':\n                    in_directory = self.in_acous_feats_dir\n\n#                current_stream_hidden_size = self.stream_lf0_hidden_size\n#                current_stream_weight      = self.stream_weight_lf0\n            elif feature_name == \'vuv\':\n                out_dimension = 1\n\n#                current_stream_hidden_size = self.stream_vuv_hidden_size\n#                current_stream_weight      = self.stream_weight_vuv\n            elif feature_name == \'stepw\':\n                in_dimension = self.stepw_dim\n                out_dimension = self.stepw_dim\n                in_directory  = self.in_stepw_dir\n\n#                current_stream_hidden_size = self.stream_stepw_hidden_size\n#                current_stream_weight      = self.stream_weight_stepw\n            elif feature_name == \'sp\':\n                in_dimension = self.sp_dim\n                out_dimension = self.sp_dim\n                in_directory  = self.in_sp_dir\n\n#                current_stream_hidden_size = self.stream_sp_hidden_size\n#                current_stream_weight      = self.stream_weight_sp\n\n            elif feature_name == \'seglf0\':\n                in_dimension = self.seglf0_dim\n                out_dimension = self.seglf0_dim\n                in_directory = self.in_seglf0_dir\n\n#                current_stream_hidden_size = self.stream_seglf0_hidden_size\n#                current_stream_weight      = self.stream_weight_seglf0\n\n\n            ## for GlottHMM (start)\n            elif feature_name == \'F0\':\n                in_dimension = self.F0_dim\n                out_dimension = self.dF0_dim\n                in_directory  = self.in_F0_dir\n\n#                current_stream_hidden_size = self.stream_F0_hidden_size\n#                current_stream_weight      = self.stream_weight_F0\n\n            elif feature_name == \'Gain\':\n                in_dimension = self.Gain_dim\n                out_dimension = self.dGain_dim\n                in_directory  = self.in_Gain_dir\n\n#                current_stream_hidden_size = self.stream_Gain_hidden_size\n#                current_stream_weight      = self.stream_weight_Gain\n\n            elif feature_name == \'HNR\':\n                in_dimension = self.HNR_dim\n                out_dimension = self.dHNR_dim\n                in_directory  = self.in_HNR_dir\n\n#                current_stream_hidden_size = self.stream_HNR_hidden_size\n#                current_stream_weight      = self.stream_weight_HNR\n\n            elif feature_name == \'LSF\':\n                in_dimension = self.LSF_dim\n                out_dimension = self.dLSF_dim\n                in_directory  = self.in_LSF_dir\n\n#                current_stream_hidden_size = self.stream_LSF_hidden_size\n#                current_stream_weight      = self.stream_weight_LSF\n\n            elif feature_name == \'LSFsource\':\n                in_dimension = self.LSFsource_dim\n                out_dimension = self.dLSFsource_dim\n                in_directory  = self.in_LSFsource_dir\n\n#                current_stream_hidden_size = self.stream_LSFsource_hidden_size\n#                current_stream_weight      = self.stream_weight_LSFsource\n            ## for GlottHMM (end)\n\n            ## for GlottDNN (start)\n            elif feature_name == \'f0\':\n                in_dimension = self.f0_dim\n                out_dimension = self.df0_dim\n                in_directory  = self.in_f0_dir\n\n            elif feature_name == \'gain\':\n                in_dimension = self.gain_dim\n                out_dimension = self.dgain_dim\n                in_directory  = self.in_gain_dir\n\n            elif feature_name == \'hnr\':\n                in_dimension = self.hnr_dim\n                out_dimension = self.dhnr_dim\n                in_directory  = self.in_hnr_dir\n\n            elif feature_name == \'lsf\':\n                in_dimension = self.lsf_dim\n                out_dimension = self.dlsf_dim\n                in_directory  = self.in_lsf_dir\n\n            elif feature_name == \'slsf\':\n                in_dimension = self.slsf_dim\n                out_dimension = self.dslsf_dim\n                in_directory  = self.in_slsf_dir\n            ## for GlottDNN (end)\n\n            ## for HMPD (start)\n            elif feature_name == \'pdd\':\n                in_dimension = self.pdd_dim\n                out_dimension = self.dpdd_dim\n                in_directory  = self.in_pdd_dir\n            ## for HMPD (end)\n\n            ## For MagPhase Vocoder (start):\n            # Note: \'lf0\' is set before. See above.\n            elif feature_name == \'mag\':\n                in_dimension  = self.mag_dim\n                out_dimension = self.dmag_dim\n                in_directory  = self.in_acous_feats_dir\n\n            elif feature_name == \'real\':\n                in_dimension  = self.real_dim\n                out_dimension = self.dreal_dim\n                in_directory  = self.in_acous_feats_dir\n\n            elif feature_name == \'imag\':\n                in_dimension  = self.imag_dim\n                out_dimension = self.dimag_dim\n                in_directory  = self.in_acous_feats_dir\n            ## For MagPhase Vocoder (end)\n\n            ## for joint dur (start)\n            elif feature_name == \'dur\':\n                in_dimension = self.dur_dim\n                out_dimension = self.dur_dim\n                in_directory  = self.in_dur_dir\n\n#                current_stream_hidden_size = self.stream_dur_hidden_size\n#                current_stream_weight      = self.stream_weight_dur\n            ## for joint dur (end)\n\n\n            else:\n                logger.critical(\'%s feature is not supported right now. Please change the configuration.py to support it\' %(feature_name))\n                raise\n\n            logger.info(\'  in_dimension: %d\' % in_dimension)\n            logger.info(\'  out_dimension : %d\' % out_dimension)\n            logger.info(\'  in_directory : %s\' %  in_directory)\n#            logger.info(\'  current_stream_hidden_size: %d\' % current_stream_hidden_size)\n#            logger.info(\'  current_stream_weight: %d\' % current_stream_weight)\n\n            if in_dimension > 0:\n                self.in_dimension_dict[feature_name] = in_dimension\n                if in_directory == \'\':\n                    logger.critical(\'please provide the path for %s feature\' %(feature_name))\n                    raise\n                if out_dimension < in_dimension:\n                    logger.critical(\'the dimensionality setting for %s feature is not correct!\' %(feature_name))\n                    raise\n\n                self.in_dir_dict[feature_name] = in_directory\n\n\n            if out_dimension > 0:\n                self.out_dimension_dict[feature_name] = out_dimension\n\n#                if (current_stream_hidden_size <= 0 or current_stream_weight <= 0.0) and self.multistream_switch:\n#                    logger.critical(\'the hidden layer size or stream weight is not corrected setted for %s feature\' %(feature_name))\n#                    raise\n\n#                if self.multistream_switch:\n#                    self.private_hidden_sizes.append(current_stream_hidden_size)\n#                    self.stream_weights.append(current_stream_weight)\n\n                self.cmp_dim += out_dimension\n\n\n\n#        if not self.multistream_switch:\n#            self.private_hidden_sizes = []\n#            if self.stream_cmp_hidden_size > 0:\n#                self.private_hidden_sizes.append(self.stream_cmp_hidden_size)\n#            else:\n#                self.private_hidden_sizes.append(self.hidden_layer_size[-1])  ## use the same number of hidden layers if multi-stream is not supported\n#            self.stream_weights = []\n#            self.stream_weights.append(1.0)\n\n        self.stream_lr_weights = []\n\n        self.multistream_outs = []\n        if self.multistream_switch:\n            for feature_name in list(self.out_dimension_dict.keys()):\n                self.multistream_outs.append(self.out_dimension_dict[feature_name])\n\n#                stream_lr_ratio = 0.5\n#                if feature_name == \'lf0\':\n#                    stream_lr_ratio = self.stream_lf0_lr\n#                if feature_name == \'vuv\':\n#                    stream_lr_ratio = self.stream_vuv_lr\n#                self.stream_lr_weights.append(stream_lr_ratio)\n        else:\n            ### the new cmp is not the one for HTS, it includes all the features, such as that for main tasks and that for additional tasks\n            self.multistream_outs.append(self.cmp_dim)\n#            self.stream_lr_weights.append(0.5)\n\n        logger.info(\'multistream dimensions: %s\' %(self.multistream_outs))\n\n        # to check whether all the input and output features\' file extensions are here\n        self.file_extension_dict = {}\n        self.file_extension_dict[\'mgc\'] = self.mgc_ext\n        self.file_extension_dict[\'lf0\'] = self.lf0_ext\n        self.file_extension_dict[\'bap\'] = self.bap_ext\n        self.file_extension_dict[\'stepw\'] = self.stepw_ext\n        self.file_extension_dict[\'cmp\'] = self.cmp_ext\n        self.file_extension_dict[\'seglf0\'] = self.lf0_ext\n\n        ## gHMM:\n        self.file_extension_dict[\'F0\'] = self.F0_ext\n        self.file_extension_dict[\'Gain\'] = self.Gain_ext\n        self.file_extension_dict[\'HNR\'] = self.HNR_ext\n        self.file_extension_dict[\'LSF\'] = self.LSF_ext\n        self.file_extension_dict[\'LSFsource\'] = self.LSFsource_ext\n\n        ## gDNN\n        self.file_extension_dict[\'f0\'] = self.f0_ext\n        self.file_extension_dict[\'gain\'] = self.gain_ext\n        self.file_extension_dict[\'hnr\'] = self.hnr_ext\n        self.file_extension_dict[\'lsf\'] = self.lsf_ext\n        self.file_extension_dict[\'slsf\'] = self.slsf_ext\n        \n        ## HMPD\n        self.file_extension_dict[\'pdd\'] = self.pdd_ext\n\n        ## For MagPhase Vocoder:\n        # Note: \'lf0\' is set before. See above.\n        self.file_extension_dict[\'mag\']  = self.mag_ext\n        self.file_extension_dict[\'real\'] = self.real_ext\n        self.file_extension_dict[\'imag\'] = self.imag_ext\n\n        ## joint dur\n        self.file_extension_dict[\'dur\'] = self.dur_ext\n\n        ## hyper parameters for DNN. need to be setted by the user, as they depend on the architecture\n        self.hyper_params = { \'learning_rate\'      : \'0.0002\',        ###\n                              \'l2_reg\'             : \'0.00001\',\n                              \'l1_reg\'             : \'0.0\',\n                              \'batch_size\'         : \'16\',\n                              \'training_epochs\'    : \'25\',\n                              \'early_stop_epochs\'  : \'5\',\n                              \'hidden_activation\'  : \'tanh\',\n                              \'output_activation\'  : \'linear\',\n                              \'do_pretraining\'     : False,\n                              \'pretraining_epochs\' : \'10\',\n                              \'pretraining_lr\'     : \'0.0001\'}\n\n        self.hyper_params[\'warmup_momentum\']      = self.warmup_momentum\n        self.hyper_params[\'momentum\']             = self.momentum\n        self.hyper_params[\'warmup_epoch\']         = self.warmup_epoch\n\n\n        self.hyper_params[\'learning_rate\']         = self.learning_rate\n        self.hyper_params[\'l2_reg\']                = self.l2_reg\n        self.hyper_params[\'l1_reg\']                = self.l1_reg\n        self.hyper_params[\'batch_size\']            = self.batch_size\n        self.hyper_params[\'training_epochs\']       = self.training_epochs\n        self.hyper_params[\'hidden_activation\']     = self.hidden_activation\n        self.hyper_params[\'output_activation\']     = self.output_activation\n        self.hyper_params[\'hidden_layer_size\']   = self.hidden_layer_size\n        self.hyper_params[\'warmup_epoch\']          = self.warmup_epoch\n        self.hyper_params[\'use_rprop\']             = self.use_rprop\n\n        self.hyper_params[\'model_type\']            = self.model_type\n        self.hyper_params[\'hidden_layer_type\']     = self.hidden_layer_type\n\n        self.hyper_params[\'index_to_project\']     = self.index_to_project\n        self.hyper_params[\'projection_insize\']    = self.projection_insize\n        self.hyper_params[\'projection_outsize\']   = self.projection_outsize\n        self.hyper_params[\'initial_projection_distrib\']   = self.initial_projection_distrib\n        self.hyper_params[\'layers_with_projection_input\']   = self.layers_with_projection_input\n        self.hyper_params[\'projection_learning_rate_scaling\']   = self.projection_learning_rate_scaling\n\n        self.hyper_params[\'sequential_training\'] = self.sequential_training\n        self.hyper_params[\'dropout_rate\'] = self.dropout_rate\n\n\n        #To be recorded in the logging file for reference\n        for param_name in list(self.hyper_params.keys()):\n            logger.info(\'%s : %s\' %(param_name, str(self.hyper_params[param_name])))\n\n                # input files\n\n\n        # set up the label processing\n        # currently must be one of two styles\n        if self.label_style == \'HTS\':\n            # xpath_file_name is now obsolete - to remove\n            self.xpath_file_name=None\n        elif self.label_style == \'HTS_duration\':\n            self.xpath_file_name=None\n\n\n\n        elif self.label_style == \'composed\':\n            self.question_file_name=None\n\n        else:\n            logger.critical(\'unsupported label style requested: %s\' % self.label_style)\n            raise Exception\n\n\n    def logging_configuration(self):\n\n        # get a logger\n        logger = logging.getLogger(""configuration"")\n\n        # logging configuration, see here for format description\n        # https://docs.python.org/2/library/logging.config.html#logging-config-fileformat\n\n\n        # what we really want to do is this dicitonary-based configuration, but it\'s only available from Python 2.7 onwards\n        #    logging.config.dictConfig(cfg.logging_configuration)\n        # so we will settle for this file-based configuration procedure instead\n\n        try:\n            # open the logging configuration file\n            fp = open(self.log_config_file,\'r\')\n            logger.debug(""loading logging configuration from %s"" % self.log_config_file)\n            # load the logging configuration file into a string\n            config_string = fp.read()\n            fp.close()\n\n        except ValueError:\n            # this means that cfg.log_config_file does not exist and that no default was provided\n            # NOTE: currently this will never run\n            logging.warn(\'no logging configuration file provided - using default (console only, DEBUG level)\')\n\n            # set up a default level and default handlers\n            # first, get the root logger - all other loggers will inherit its configuration\n            rootogger = logging.getLogger("""")\n            # default logging level is DEBUG (a highly-verbose level)\n            rootlogger.setLevel(logging.DEBUG)\n            # add a handler to write to console\n            ch = logging.StreamHandler()\n            rootlogger.addHandler(ch)\n            # and a formatter\n            formatter = logging.Formatter(\'%(asctime)s %(levelname)8s%(name)15s: %(message)s\')\n            ch.setFormatter(formatter)\n\n        except IOError:\n            # this means that open(...) threw an error\n            logger.critical(\'could not load logging configuration file %s\' % self.log_config_file)\n            raise\n\n        else:\n\n            # inject the config lines for the file handler, now that we know the name of the file it will write to\n\n            if not os.path.exists(self.log_path):\n                os.makedirs(self.log_path, 0o755)\n            log_file_name = \'%s_%s.log\' %(self.model_file_name, datetime.datetime.now().strftime(""%I_%M%p_%B_%d_%Y""))\n\n            self.log_file = os.path.join(self.log_path, log_file_name)\n\n            to_inject=""""""\n                [handler_file]\n                class=FileHandler\n                formatter=file\n                args=(\'""""""+self.log_file+""""""\', \'w\')\n            """"""\n\n            # config file format doesn\'t allow leading white space on lines, so remove it with dedent\n            config_string = config_string + textwrap.dedent(to_inject)\n\n\n            try:\n                # pass that string as a filehandle\n                if sys.version_info.major < 3:\n                    config_string = unicode(config_string, ""utf-8"")\n                fh = io.StringIO(config_string)\n                logging.config.fileConfig(fh)\n                fh.close()\n                logger.info(""logging is now fully configured"")\n\n            except IOError:\n                logger.critical(\'could not configure logging: perhaps log file path is wrong?\')\n                sys.exit(1)\n'"
src/configuration/examplelabelconfigfile.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n# configuration for the input labels (features) for the DNN\n#\n# this currently supports\n# * input labels can be any combination of HTS and XML style input labels\n# * output features are numerical *only* (all strings are fully expanded into 1-of-n encodings, etc)\n#\n#\n#\n# this is all executable python code\n#  so we need to define things before using them\n#  that means the description is bottom-up\n\nimport logging\nlogger = logging.getLogger(""labels"")\n\n# we need to specify how any non-numerical (e.g., unicode string) features will be converted (mapped) into numerical feature vectors\n# (just some examples for now)\nmaps = {\n\n\'cplace_to_binary\':{\n    \'_UNSEEN_\' : [0,0,0,0,0,0,0],\n          \'NA\' : [0,0,0,0,0,0,0],\n        \'_NA_\' : [0,0,0,0,0,0,0],\n    \'alveolar\' : [1,0,0,0,0,0,0],\n      \'dental\' : [0,1,0,0,0,0,0],\n     \'glottal\' : [0,0,1,0,0,0,0],\n      \'labial\' : [0,0,0,1,0,0,0],\n \'labiodental\' : [0,0,0,0,1,0,0],\n     \'palatal\' : [0,0,0,0,0,1,0],\n       \'velar\' : [0,0,0,0,0,0,1]\n    },\n\n\'cmanner_to_binary\':{\n    \'_UNSEEN_\' : [0,0,0,0,0,0],\n          \'NA\' : [0,0,0,0,0,0],\n        \'_NA_\' : [0,0,0,0,0,0],\n   \'affricate\' : [1,0,0,0,0,0],\n \'approximant\' : [0,1,0,0,0,0],\n   \'fricative\' : [0,0,1,0,0,0],\n      \'liquid\' : [0,0,0,1,0,0],\n       \'nasal\' : [0,0,0,0,1,0],\n        \'stop\' : [0,0,0,0,0,1]\n    },\n\n\'cvoiced_to_binary\':{\n \'_UNSEEN_\' : [0,0],\n       \'NA\' : [0,0],\n     \'_NA_\' : [0,0],\n      \'yes\' : [1,0],\n       \'no\' : [0,1]\n    },\n\n\'vfront_to_binary\':{\n \'_UNSEEN_\' : [0,0,0],\n       \'NA\' : [0,0,0],\n     \'_NA_\' : [0,0,0],\n     \'back\' : [1,0,0],\n      \'mid\' : [0,1,0],\n    \'front\' : [0,0,1]\n    },\n\n\'vheight_to_binary\':{\n \'_UNSEEN_\' : [0,0,0],\n       \'NA\' : [0,0,0],\n     \'_NA_\' : [0,0,0],\n     \'high\' : [1,0,0],\n      \'mid\' : [0,1,0],\n      \'low\' : [0,0,1]\n    },\n\n\'vlength_to_binary\':{\n  \'_UNSEEN_\' : [0,0,0,0],\n        \'NA\' : [0,0,0,0],\n      \'_NA_\' : [0,0,0,0],\n \'diphthong\' : [1,0,0,0],\n      \'long\' : [0,1,0,0],\n     \'schwa\' : [0,0,1,0],\n     \'short\' : [0,0,0,1]\n    },\n\n\'vround_to_binary\':{\n \'_UNSEEN_\' : [0,0],\n       \'NA\' : [0,0],\n     \'_NA_\' : [0,0],\n      \'yes\' : [1,0],\n       \'no\' : [0,1]\n    },\n\n\'vowel_cons_to_binary\':{\n \'_UNSEEN_\' : [0,0],\n       \'NA\' : [0,0],\n     \'_NA_\' : [0,0],\n    \'vowel\' : [1,0],\n     \'cons\' : [0,1]\n    }\n}\n\n# read additional maps from external files and add them to the \'maps\' dictionary\n#  each such file must define a dictionary of dictionaries called maps, in the same format as above\n#  TO DO - avoid full paths here - import them from the main config file\nexternal_map_files=[\'/Users/simonk/data/dnn_tts/data/ossian/maps/segment_map.py\']\n\nimport imp\nfor fname in external_map_files:\n    # not sure this will work second time around - may not be able to import under the same module name ??\n    external_maps = imp.load_source(\'external_maps\',fname)\n    for k,v in external_maps.maps.items():\n        if k in maps:\n            logger.warning(\'Redefined map %s and over-wrote the previous map with the same name\' % k)\n        maps[k] = v\n\n# how to extract features\n# (just a few examples for now)\n#\n# each feature is a dictionary with various possible entries:\n#   xpath: an XPATH that will extract the required feature from a segment target node of an Ossian XML utterance tree\n#   hts:   a (list of) HTS pseudo regular expression(s) that match(es) part of an HTS label, resulting in a single boolean feature\n#   mapper:   an optional function or dictionary which converts the feature value (e.g., a string) to a (vector of) numerical value(s)\n#\n# the dictionary describes how to compute that feature\n# first, either xpath or hts describes how to extract the feature from a tree or label name\n# then, an optional mapping converts the feature via a lookup table (also a dictionary) into a numerical value or vector\n#\n# if no mapper is provided, then the feature must already be a single numerical or boolean value\n#\n# some XPATH-based features\n\n# in a future version, we could be more fleixble and allow more than one target_node type at once,\n# with a set of XPATHs for each target_node - it would not be very hard to modify the code to do this\n\n# the target nodes within the XML trees that the XPATH expressions apply to\ntarget_nodes = ""//segment""\n# target_nodes = ""//state"" ???\n\n        # <segment pronunciation=""t"" cmanner=""stop"" cplace=""alveolar"" cvoiced=""no"" vfront=""NA"" vheight=""NA"" vlength=""NA"" vowel_cons=""cons"" vround=""NA"" start=""1040"" end=""1090"" has_silence=""no"">\n\n\n# and the XPATH expressions to apply\n\nll_segment =      {\'xpath\':\'preceding::segment[2]/attribute::pronunciation\',   \'mapper\':maps[\'segment_to_binary\'] }\nl_segment  =      {\'xpath\':\'preceding::segment[1]/attribute::pronunciation\',   \'mapper\':maps[\'segment_to_binary\'] }\nc_segment  =      {\'xpath\':                    \'./attribute::pronunciation\',   \'mapper\':maps[\'segment_to_binary\'] }\nr_segment  =      {\'xpath\':\'following::segment[1]/attribute::pronunciation\',   \'mapper\':maps[\'segment_to_binary\'] }\nrr_segment =      {\'xpath\':\'following::segment[2]/attribute::pronunciation\',   \'mapper\':maps[\'segment_to_binary\'] }\n\ncmanner    =      {\'xpath\':                    \'./attribute::cmanner\',          \'mapper\':maps[\'cmanner_to_binary\'] }\ncplace     =      {\'xpath\':                    \'./attribute::cplace\',           \'mapper\':maps[\'cplace_to_binary\'] }\ncvoiced    =      {\'xpath\':                    \'./attribute::cvoiced\',          \'mapper\':maps[\'cvoiced_to_binary\'] }\n\nvfront     =      {\'xpath\':                    \'./attribute::vfront\',           \'mapper\':maps[\'vfront_to_binary\'] }\nvheight    =      {\'xpath\':                    \'./attribute::vheight\',          \'mapper\':maps[\'vheight_to_binary\'] }\nvlength    =      {\'xpath\':                    \'./attribute::vlength\',          \'mapper\':maps[\'vlength_to_binary\'] }\nvround     =      {\'xpath\':                    \'./attribute::vround\',           \'mapper\':maps[\'vround_to_binary\'] }\n\nvowel_cons =      {\'xpath\':                    \'./@vowel_cons\',                \'mapper\':maps[\'vowel_cons_to_binary\'] }\n\n\n# a composite ""vector"" of XPATH features\n#  this is just an ordered list of features, each of which is a dictionary describing how to compute this feature\n#  each feature may be a single numerical value or a vector of numerical values\nxpath_labels =[\n\nll_segment,\n l_segment,\n c_segment,\n r_segment,\nrr_segment,\n\ncmanner,\ncplace,\ncvoiced,\n\nvfront,\nvheight,\nvlength,\nvround,\n\nvowel_cons\n]\n\n\n# some HTS pseudo regular expression-based features\n# all of these evaluate to a single boolean value, which will be eventually represented numerically\n# note: names of features will need modifying to valid Python variable names (cannot contain ""-"", for example)\nC_Dental_Fricative = {\'hts\':\'{*-T+*,*-D+*}\'}\nC_Rounded_End      = {\'hts\':\'{*-9^+*,*-aU+*,*-o^+*,*-Or+*,*-QO+*,*-Q+*,*-@Ur+*,*-@U+*,*-O+*,*-u+*,*-U+*}\'}\nC_OI               = {\'hts\':\'{*-OI+*}\'}\n\n# a composite ""vector"" of HTS features\nhts_labels = [C_Dental_Fricative, C_Rounded_End, C_OI]\n\n\n\n\n# the full feature vector\nlabels = xpath_labels # + hts_labels\n'"
src/configuration/label_config_001.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n# configuration for the input labels (features) for the DNN\n#\n# this currently supports\n# * input labels can be any combination of HTS and XML style input labels\n# * output features are numerical *only* (all strings are fully expanded into 1-of-n encodings, etc)\n#\n#\n#\n# this is all executable python code\n#  so we need to define things before using them\n#  that means the description is bottom-up\n\nimport numpy\n\nimport logging\nlogger = logging.getLogger(""labels"")\n\n# we need to specify how any non-numerical (e.g., unicode string) features will be converted (mapped) into numerical feature vectors\n# (just some examples for now)\n\n\n\nmaps = {\n\n\'cplace_to_binary\':{\n    \'_UNSEEN_\' : [0,0,0,0,0,0,0],\n          \'NA\' : [0,0,0,0,0,0,0],\n        \'_NA_\' : [0,0,0,0,0,0,0],\n    \'alveolar\' : [1,0,0,0,0,0,0],\n      \'dental\' : [0,1,0,0,0,0,0],\n     \'glottal\' : [0,0,1,0,0,0,0],\n      \'labial\' : [0,0,0,1,0,0,0],\n \'labiodental\' : [0,0,0,0,1,0,0],\n     \'palatal\' : [0,0,0,0,0,1,0],\n       \'velar\' : [0,0,0,0,0,0,1]\n    },\n\n\'cmanner_to_binary\':{\n    \'_UNSEEN_\' : [0,0,0,0,0,0],\n          \'NA\' : [0,0,0,0,0,0],\n        \'_NA_\' : [0,0,0,0,0,0],\n   \'affricate\' : [1,0,0,0,0,0],\n \'approximant\' : [0,1,0,0,0,0],\n   \'fricative\' : [0,0,1,0,0,0],\n      \'liquid\' : [0,0,0,1,0,0],\n       \'nasal\' : [0,0,0,0,1,0],\n        \'stop\' : [0,0,0,0,0,1]\n    },\n\n\'cvoiced_to_binary\':{\n \'_UNSEEN_\' : [0,0],\n       \'NA\' : [0,0],\n     \'_NA_\' : [0,0],\n      \'yes\' : [1,0],\n       \'no\' : [0,1]\n    },\n\n\n\'vfront_to_binary\':{\n \'_UNSEEN_\' : [0,0,0],\n       \'NA\' : [0,0,0],\n     \'_NA_\' : [0,0,0],\n     \'back\' : [1,0,0],\n      \'mid\' : [0,1,0],\n    \'front\' : [0,0,1]\n    },\n\n\'vheight_to_binary\':{\n \'_UNSEEN_\' : [0,0,0],\n       \'NA\' : [0,0,0],\n     \'_NA_\' : [0,0,0],\n     \'high\' : [1,0,0],\n      \'mid\' : [0,1,0],\n      \'low\' : [0,0,1]\n    },\n\n\'vlength_to_binary\':{\n  \'_UNSEEN_\' : [0,0,0,0],\n        \'NA\' : [0,0,0,0],\n      \'_NA_\' : [0,0,0,0],\n \'diphthong\' : [1,0,0,0],\n      \'long\' : [0,1,0,0],\n     \'schwa\' : [0,0,1,0],\n     \'short\' : [0,0,0,1]\n    },\n\n\'vround_to_binary\':{\n \'_UNSEEN_\' : [0,0],\n       \'NA\' : [0,0],\n     \'_NA_\' : [0,0],\n      \'yes\' : [1,0],\n       \'no\' : [0,1]\n    },\n\n\'vowel_cons_to_binary\':{\n \'_UNSEEN_\' : [0,0],\n       \'NA\' : [0,0],\n     \'_NA_\' : [0,0],\n    \'vowel\' : [1,0],\n     \'cons\' : [0,1]\n    }\n}\n\n\n## osw -- also make some maps automatically, only specifying list of values for brevity:\n\ndef make_1_of_k_map(values):\n    ## strip special null values:\n    nulls = [\'_UNSEEN_\',\'NA\',\'_NA_\']\n    values = [val for val in values if val not in nulls]\n    map = {}\n    for (i, value) in enumerate(values):\n        vector = numpy.zeros(len(values))\n        vector[i] = 1\n        map[value] = vector.tolist()\n    for value in nulls:\n        map[value] = numpy.zeros(len(values)).tolist()\n    return map\n\n\nphone_names = [\'@\', \'@@\', \'@U\', \'A\', \'D\', \'E\', \'E@\', \'I\', \'I@\', \'N\', \'O\', \'OI\', \'Q\', \'S\', \'T\', \'U\', \'U@\', \'V\', \'Z\', \'a\', \'aI\', \'aU\', \'b\', \'d\', \'dZ\', \'eI\', \'f\', \'g\', \'h\', \'i\', \'j\', \'k\', \'l\', \'l!\', \'lw\', \'m\', \'m!\', \'n\', \'n!\', \'p\', \'r\', \'s\', \'sil\', \'t\', \'tS\', \'u\', \'v\', \'w\', \'z\']\n\nfine_POS_inventory = [\'_COMMA_\', \'_FULLSTOP_\', \'_SPACE_\', \'cc\', \'cd\', \'dt\', \'dt_VERTICALLINE_vbz\', \'ex\', \'ex_VERTICALLINE_vbz\', \'in\', \'jj\', \'jjr\', \'jjs\', \'md\', \'md_VERTICALLINE_rb\', \'nn\', \'nn_VERTICALLINE_pos\', \'nnp\', \'nnp_VERTICALLINE_pos\', \'nnps\', \'nns\', \'pdt\', \'prp\', \'prp_DOLLARSIGN_\', \'prp_VERTICALLINE_md\', \'prp_VERTICALLINE_vbp\', \'prp_VERTICALLINE_vbz\', \'rb\', \'rbr\', \'rbs\', \'rp\', \'to\', \'vb\', \'vb_VERTICALLINE_pos\', \'vb_VERTICALLINE_prp\', \'vbd\', \'vbd_VERTICALLINE_rb\', \'vbg\', \'vbn\', \'vbp\', \'vbp_VERTICALLINE_rb\', \'vbz\', \'vbz_VERTICALLINE_rb\', \'wdt\', \'wp\', \'wp_VERTICALLINE_vbz\', \'wrb\']\n\ncoarse_POS_inventory = [\'adj\', \'adv\', \'function\', \'noun\', \'punc\', \'space\', \'verb\']\n\nstress_inventory = [\'stress_0\', \'stress_1\', \'stress_2\']\n\nmaps[\'phone_to_binary\'] = make_1_of_k_map(phone_names)\nmaps[\'fine_POS_to_binary\'] = make_1_of_k_map(fine_POS_inventory)\nmaps[\'coarse_POS_to_binary\'] = make_1_of_k_map(coarse_POS_inventory)\nmaps[\'stress_to_binary\'] = make_1_of_k_map(stress_inventory)\n\n\n\n\n\n# read additional maps from external files and add them to the \'maps\' dictionary\n#  each such file must define a dictionary of dictionaries called maps, in the same format as above\n#  TO DO - avoid full paths here - import them from the main config file\nexternal_map_files=[]\n\nimport imp\nfor fname in external_map_files:\n    # not sure this will work second time around - may not be able to import under the same module name ??\n    external_maps = imp.load_source(\'external_maps\',fname)\n    for k,v in external_maps.maps.items():\n        if k in maps:\n            logger.warning(\'Redefined map %s and over-wrote the previous map with the same name\' % k)\n        maps[k] = v\n\n# how to extract features\n# (just a few examples for now)\n#\n# each feature is a dictionary with various possible entries:\n#   xpath: an XPATH that will extract the required feature from a segment target node of an Ossian XML utterance tree\n#   hts:   a (list of) HTS pseudo regular expression(s) that match(es) part of an HTS label, resulting in a single boolean feature\n#   mapper:   an optional function or dictionary which converts the feature value (e.g., a string) to a (vector of) numerical value(s)\n#\n# the dictionary describes how to compute that feature\n# first, either xpath or hts describes how to extract the feature from a tree or label name\n# then, an optional mapping converts the feature via a lookup table (also a dictionary) into a numerical value or vector\n#\n# if no mapper is provided, then the feature must already be a single numerical or boolean value\n#\n# some XPATH-based features\n\n# in a future version, we could be more fleixble and allow more than one target_node type at once,\n# with a set of XPATHs for each target_node - it would not be very hard to modify the code to do this\n\n# the target nodes within the XML trees that the XPATH expressions apply to\ntarget_nodes = ""//state""\n\n\n# and the XPATH expressions to apply\n\nxpath_labels = []\n\n## NB: first feature is for silence trimming only:\nxpath_labels.append({\'xpath\': ""./ancestor::segment/attribute::pronunciation = \'sil\'""})\n\nfor xpath in [\n\n    ""./ancestor::segment/preceding::segment[2]/attribute::pronunciation"",\n    ""./ancestor::segment/preceding::segment[1]/attribute::pronunciation"",\n    ""./ancestor::segment/attribute::pronunciation"",\n    ""./ancestor::segment/following::segment[1]/attribute::pronunciation"",\n    ""./ancestor::segment/following::segment[2]/attribute::pronunciation""]:\n\n    xpath_labels.append({\'xpath\': xpath, \'mapper\':maps[\'phone_to_binary\']})\n\n\nfor xpath in [\n\n    ""./ancestor::segment/preceding::segment[2]/attribute::vowel_cons"",\n    ""./ancestor::segment/preceding::segment[2]/attribute::vfront"",\n    ""./ancestor::segment/preceding::segment[2]/attribute::vheight"",\n    ""./ancestor::segment/preceding::segment[2]/attribute::vlength"",\n    ""./ancestor::segment/preceding::segment[2]/attribute::vround"",\n    ""./ancestor::segment/preceding::segment[2]/attribute::cmanner"",\n    ""./ancestor::segment/preceding::segment[2]/attribute::cplace"",\n    ""./ancestor::segment/preceding::segment[2]/attribute::cvoiced"",\n\n    ""./ancestor::segment/preceding::segment[1]/attribute::vowel_cons"",\n    ""./ancestor::segment/preceding::segment[1]/attribute::vfront"",\n    ""./ancestor::segment/preceding::segment[1]/attribute::vheight"",\n    ""./ancestor::segment/preceding::segment[1]/attribute::vlength"",\n    ""./ancestor::segment/preceding::segment[1]/attribute::vround"",\n    ""./ancestor::segment/preceding::segment[1]/attribute::cmanner"",\n    ""./ancestor::segment/preceding::segment[1]/attribute::cplace"",\n    ""./ancestor::segment/preceding::segment[1]/attribute::cvoiced"",\n\n    ""./ancestor::segment/attribute::vowel_cons"",\n    ""./ancestor::segment/attribute::vfront"",\n    ""./ancestor::segment/attribute::vheight"",\n    ""./ancestor::segment/attribute::vlength"",\n    ""./ancestor::segment/attribute::vround"",\n    ""./ancestor::segment/attribute::cmanner"",\n    ""./ancestor::segment/attribute::cplace"",\n    ""./ancestor::segment/attribute::cvoiced"",\n\n    ""./ancestor::segment/following::segment[1]/attribute::vowel_cons"",\n    ""./ancestor::segment/following::segment[1]/attribute::vfront"",\n    ""./ancestor::segment/following::segment[1]/attribute::vheight"",\n    ""./ancestor::segment/following::segment[1]/attribute::vlength"",\n    ""./ancestor::segment/following::segment[1]/attribute::vround"",\n    ""./ancestor::segment/following::segment[1]/attribute::cmanner"",\n    ""./ancestor::segment/following::segment[1]/attribute::cplace"",\n    ""./ancestor::segment/following::segment[1]/attribute::cvoiced"",\n\n    ""./ancestor::segment/following::segment[2]/attribute::vowel_cons"",\n    ""./ancestor::segment/following::segment[2]/attribute::vfront"",\n    ""./ancestor::segment/following::segment[2]/attribute::vheight"",\n    ""./ancestor::segment/following::segment[2]/attribute::vlength"",\n    ""./ancestor::segment/following::segment[2]/attribute::vround"",\n    ""./ancestor::segment/following::segment[2]/attribute::cmanner"",\n    ""./ancestor::segment/following::segment[2]/attribute::cplace"",\n    ""./ancestor::segment/following::segment[2]/attribute::cvoiced""]:\n\n    feature = xpath.split(\':\')[-1]\n    xpath_labels.append({\'xpath\': xpath, \'mapper\':maps[feature + \'_to_binary\']})\n\n\n## syll stress\nfor xpath in [\n        ""ancestor::syllable/preceding::syllable[1]/attribute::stress"",\n        ""ancestor::syllable/attribute::stress"",\n        ""ancestor::syllable/following::syllable[1]/attribute::stress""]:\n    xpath_labels.append({\'xpath\': xpath, \'mapper\': maps[\'stress_to_binary\']})\n\n\n## fine & coarse POS -- 3 word window\nfor xpath in [\n        ""ancestor::token/preceding::token[@token_class=\'word\'][1]/attribute::safe_pos"",\n        ""ancestor::token/attribute::safe_pos"",\n        ""ancestor::token/following::token[@token_class=\'word\'][1]/attribute::safe_pos""]:\n    xpath_labels.append({\'xpath\': xpath, \'mapper\': maps[\'fine_POS_to_binary\']})\n\nfor xpath in [\n        ""ancestor::token/preceding::token[@token_class=\'word\'][1]/attribute::coarse_pos"",\n        ""ancestor::token/attribute::coarse_pos"",\n        ""ancestor::token/following::token[@token_class=\'word\'][1]/attribute::coarse_pos""]:\n    xpath_labels.append({\'xpath\': xpath, \'mapper\': maps[\'coarse_POS_to_binary\']})\n\n\n## === SIZES and DISTANCES till start/end -- these are numeric and not mapped:\n\nfor xpath in [\n\n    ## state in segment -- number states is fixed, so exclude size and only count in 1 direction\n    ""count(./preceding-sibling::state)"",\n\n    ## segments in syll\n    ""count(ancestor::syllable/preceding::syllable[1]/descendant::segment)"",\n    ""count(ancestor::syllable/descendant::segment)"",\n    ""count(ancestor::syllable/following::syllable[1]/descendant::segment)"",\n    ""count(./ancestor::segment/preceding-sibling::segment)"",\n    ""count(./ancestor::segment/following-sibling::segment)"",\n\n    ## segments in word\n    ""count(ancestor::token/preceding::token[@token_class=\'word\'][1]/descendant::segment)"",\n    ""count(ancestor::token/descendant::segment)"",\n    ""count(ancestor::token/following::token[@token_class=\'word\'][1]/descendant::segment)"",\n    ""count(./ancestor::syllable/preceding-sibling::syllable/descendant::segment)"",\n    ""count(./ancestor::syllable/following-sibling::syllable/descendant::segment)"",\n\n    ## syll in word\n    ""count(ancestor::token/preceding::token[@token_class=\'word\'][1]/descendant::syllable)"",\n    ""count(ancestor::token/descendant::syllable)"",\n    ""count(ancestor::token/following::token[@token_class=\'word\'][1]/descendant::syllable)"",\n    ""count(./ancestor::syllable/preceding-sibling::syllable)"",\n    ""count(./ancestor::syllable/following-sibling::syllable)"",\n\n    ## word in phrase\n    ""count(ancestor::phrase/preceding::phrase[1]/descendant::token[@token_class=\'word\'])"",\n    ""count(ancestor::phrase/descendant::token[@token_class=\'word\'])"",\n    ""count(ancestor::phrase/following::phrase[1]/descendant::token[@token_class=\'word\'])"",\n    ""count(ancestor::token/preceding-sibling::token[@token_class=\'word\'])"",\n    ""count(ancestor::token/following-sibling::token[@token_class=\'word\'])"",\n\n    ## syll in phrase\n    ""count(ancestor::phrase/preceding::phrase[1]/descendant::syllable)"",\n    ""count(ancestor::phrase/descendant::syllable)"",\n    ""count(ancestor::phrase/following::phrase[1]/descendant::syllable)"",\n    ""count(ancestor::token/preceding-sibling::token/descendant::syllable)"",\n    ""count(ancestor::token/following-sibling::token/descendant::syllable)"",\n\n    ## segment in phrase\n    ""count(ancestor::phrase/preceding::phrase[1]/descendant::segment)"",\n    ""count(ancestor::phrase/descendant::segment)"",\n    ""count(ancestor::phrase/following::phrase[1]/descendant::segment)"",\n    ""count(ancestor::token/preceding-sibling::token/descendant::segment)"",\n    ""count(ancestor::token/following-sibling::token/descendant::segment)"",\n\n    ## X in utterance\n    ""count(preceding::segment)"",\n    ""count(preceding::syllable)"",\n    ""count(preceding::token[@token_class=\'word\'])"",\n    ""count(preceding::phrase)"",\n\n    ""count(following::segment)"",\n    ""count(following::syllable)"",\n    ""count(following::token[@token_class=\'word\'])"",\n    ""count(following::phrase)"",\n\n    ""count(ancestor::utt/descendant::segment)"",\n    ""count(ancestor::utt/descendant::syllable)"",\n    ""count(ancestor::utt/descendant::token[@token_class=\'word\'])"",\n    ""count(ancestor::utt/descendant::phrase)""\n    ]:\n    xpath_labels.append({\'xpath\': xpath})\n\n\n\n\n\n\n\n\n#\n# # a composite ""vector"" of XPATH features\n# #  this is just an ordered list of features, each of which is a dictionary describing how to compute this feature\n# #  each feature may be a single numerical value or a vector of numerical values\n# xpath_labels =[\n#\n# # ll_segment,\n# #  l_segment,\n# #  c_segment,\n# #  r_segment,\n# # rr_segment,\n#\n# cmanner,\n# cplace,\n# cvoiced,\n#\n# vfront,\n# vheight,\n# vlength,\n# vround,\n#\n# vowel_cons\n# ]\n#\n\n# some HTS pseudo regular expression-based features\n# all of these evaluate to a single boolean value, which will be eventually represented numerically\n# note: names of features will need modifying to valid Python variable names (cannot contain ""-"", for example)\nC_Dental_Fricative = {\'hts\':\'{*-T+*,*-D+*}\'}\nC_Rounded_End      = {\'hts\':\'{*-9^+*,*-aU+*,*-o^+*,*-Or+*,*-QO+*,*-Q+*,*-@Ur+*,*-@U+*,*-O+*,*-u+*,*-U+*}\'}\nC_OI               = {\'hts\':\'{*-OI+*}\'}\n\n# a composite ""vector"" of HTS features\nhts_labels = [C_Dental_Fricative, C_Rounded_End, C_OI]\n\n\n\n\n# the full feature vector\nlabels = xpath_labels # + hts_labels\n'"
src/frontend/__init__.py,0,b''
src/frontend/acoustic_base.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n\n\nimport numpy\nimport logging\n\nclass   AcousticBase(object):\n    def __init__(self, delta_win = [-0.5, 0.0, 0.5], acc_win = [1.0, -2.0, 1.0]):\n\n        ### whether dynamic features are needed for each data stream\n        self.compute_dynamic = {}\n        self.file_number = 0\n        self.data_stream_number = 0\n        self.data_stream_list = []\n\n        self.out_dimension = 0\n        self.record_vuv    = False\n\n        self.delta_win = delta_win\n        self.acc_win   = acc_win\n\n        self.logger = logging.getLogger(""acoustic_data"")\n\n    \'\'\'\n    in_file_list_dict: if there are multiple acoustic features,\n                       each feature has a key in the dict() and correspond to a list of file paths\n    out_file_list_dict: merge all the input files\n\n    three types of data:\n        CMP     : the one used for HTS training\n        DIY     : raw data without header, such the data to compose CMP files\n        CMP_DIY : mix of CMP and DIY data\n    \'\'\'\n    def prepare_nn_data(self, in_file_list_dict, out_file_list, in_dimension_dict, out_dimension_dict):\n\n        self.file_number = len(out_file_list)\n\n        for data_stream_name in list(in_file_list_dict.keys()):\n\n            try:\n                assert len(in_file_list_dict[data_stream_name]) == self.file_number\n            except AssertionError:\n                self.logger.critical(\'file number of stream %s is different from others: %d %d\' \\\n                                     %(data_stream_name, len(in_file_list_dict[data_stream_name]), self.file_number))\n                raise\n\n            try:\n                assert data_stream_name in in_dimension_dict\n            except AssertionError:\n                self.logger.critical(\'data stream %s is missing in  the input dimension dict!\' %(data_stream_name))\n                raise\n\n            try:\n                assert data_stream_name in out_dimension_dict\n            except AssertionError:\n                self.logger.critical(\'data stream %s is missing in  the output dimension dict!\' %(data_stream_name))\n                raise\n\n            ## we assume static+delta+delta-delta\n            if out_dimension_dict[data_stream_name] == 3 * in_dimension_dict[data_stream_name]:\n                self.compute_dynamic[data_stream_name] = True\n            elif out_dimension_dict[data_stream_name] == in_dimension_dict[data_stream_name]:\n                self.compute_dynamic[data_stream_name] = False\n            else:\n                self.logger.critical(\'output dimension of stream %s should be equal to or three times of input dimension: %d %d\'\n                                     %(data_stream_name, out_dimension_dict[data_stream_name], in_dimension_dict[data_stream_name]))\n                raise\n\n            self.data_stream_list.append(data_stream_name)\n\n        self.data_stream_number = len(self.data_stream_list)\n\n        if \'vuv\' in out_dimension_dict:\n            self.record_vuv = True\n\n            if not (\'lf0\' in in_dimension_dict or \'F0\' in in_dimension_dict):\n                self.logger.critical(""if voiced and unvoiced information are to be recorded, the \'lf0\' information must be provided"")\n                raise\n\n        for data_stream_name in list(out_dimension_dict.keys()):\n            self.out_dimension += out_dimension_dict[data_stream_name]\n\n        ### merge the data: like the cmp file\n        self.prepare_data(in_file_list_dict, out_file_list, in_dimension_dict, out_dimension_dict)\n\n    ### the real function to do the work\n    ### need to be implemented for a specific format\n    def prepare_data(self, in_file_list_dict, out_file_list, in_dimension_dict, out_dimension_dict):\n        pass\n\n    ### interpolate F0, if F0 has already been interpolated, nothing will be changed after passing this function\n    def interpolate_f0(self, data):\n\n        data = numpy.reshape(data, (data.size, 1))\n\n        vuv_vector = numpy.zeros((data.size, 1))\n        vuv_vector[data > 0.0] = 1.0\n        vuv_vector[data <= 0.0] = 0.0\n\n        ip_data = data\n\n        frame_number = data.size\n        last_value = 0.0\n        for i in range(frame_number):\n            if data[i] <= 0.0:\n                j = i+1\n                for j in range(i+1, frame_number):\n                    if data[j] > 0.0:\n                        break\n                if j < frame_number-1:\n                    if last_value > 0.0:\n                        step = (data[j] - data[i-1]) / float(j - i + 1)\n                        for k in range(i, j):\n                            ip_data[k] = data[i-1] + step * (k - i + 1)\n                    else:\n                        for k in range(i, j):\n                            ip_data[k] = data[j]\n                else:\n                    for k in range(i, frame_number):\n                        ip_data[k] = last_value\n            else:\n                ip_data[i] = data[i]\n                last_value = data[i]\n\n        return  ip_data, vuv_vector\n\n#        delta_win = [-0.5, 0.0, 0.5]\n#        acc_win   = [1.0, -2.0, 1.0]\n    def compute_dynamic_vector(self, vector, dynamic_win, frame_number):\n\n        vector = numpy.reshape(vector, (frame_number, 1))\n\n        win_length = len(dynamic_win)\n        win_width = int(win_length/2)\n        temp_vector = numpy.zeros((frame_number + 2 * win_width, 1))\n        delta_vector = numpy.zeros((frame_number, 1))\n\n        temp_vector[win_width:frame_number+win_width] = vector\n        for w in range(win_width):\n            temp_vector[w, 0] = vector[0, 0]\n            temp_vector[frame_number+win_width+w, 0] = vector[frame_number-1, 0]\n\n        for i in range(frame_number):\n            for w in range(win_length):\n                delta_vector[i] += temp_vector[i+w, 0] * dynamic_win[w]\n\n        return  delta_vector\n\n    ### compute dynamic features for a data matrix\n    def compute_dynamic_matrix(self, data_matrix, dynamic_win, frame_number, dimension):\n        dynamic_matrix = numpy.zeros((frame_number, dimension))\n\n        ###compute dynamic feature dimension by dimension\n        for dim in range(dimension):\n            dynamic_matrix[:, dim:dim+1] = self.compute_dynamic_vector(data_matrix[:, dim], dynamic_win, frame_number)\n\n        return  dynamic_matrix\n'"
src/frontend/acoustic_composition.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\nfrom io_funcs.binary_io import BinaryIOCollection\nimport numpy\nimport logging\nfrom .acoustic_base import AcousticBase\nimport os\n#io_funcs.\n\nclass   AcousticComposition(AcousticBase):\n\n    ###prepare_nn_data(self, in_file_list_dict, out_file_list, in_dimension_dict, out_dimension_dict):\n\n    \'\'\'\n    variables inheritate from AcousticBase:\n        self.compute_dynamic = {}\n        self.file_number = 0\n        self.data_stream_number = 0\n        self.data_stream_list = []\n\n        self.out_dimension = 0\n        self.record_vuv    = False\n    \'\'\'\n    def make_equal_frames(self, in_file_list, ref_file_list, in_dimension_dict):\n        logger = logging.getLogger(""acoustic_comp"")\n\n        logger.info(\'making equal number of lines...\')\n\n        io_funcs = BinaryIOCollection()\n\n        utt_number = len(in_file_list)\n\n        for i in range(utt_number):\n            in_file_name = in_file_list[i]\n            in_data_stream_name = in_file_name.split(\'.\')[-1]\n            in_feature_dim = in_dimension_dict[in_data_stream_name]\n            in_features, in_frame_number = io_funcs.load_binary_file_frame(in_file_name, in_feature_dim)\n\n            ref_file_name = ref_file_list[i]\n            ref_data_stream_name = ref_file_name.split(\'.\')[-1]\n            ref_feature_dim = in_dimension_dict[ref_data_stream_name]\n            ref_features, ref_frame_number = io_funcs.load_binary_file_frame(ref_file_name, ref_feature_dim)\n\n            target_features = numpy.zeros((ref_frame_number, in_feature_dim))\n            if in_frame_number == ref_frame_number:\n                continue;\n            elif in_frame_number > ref_frame_number:\n                target_features[0:ref_frame_number, ] = in_features[0:ref_frame_number, ]\n            elif in_frame_number < ref_frame_number:\n                target_features[0:in_frame_number, ] = in_features[0:in_frame_number, ]\n            io_funcs.array_to_binary_file(target_features, in_file_name)\n\n        logger.info(\'Finished: made equal rows in data stream %s with reference to data stream %s \' %(in_data_stream_name, ref_data_stream_name))\n\n\n    def prepare_data(self, in_file_list_dict, out_file_list, in_dimension_dict, out_dimension_dict):\n\n        logger = logging.getLogger(""acoustic_comp"")\n\n        stream_start_index = {}\n        stream_dim_index = 0\n        for stream_name in list(out_dimension_dict.keys()):\n            if stream_name not in stream_start_index:\n                stream_start_index[stream_name] = stream_dim_index\n\n            stream_dim_index += out_dimension_dict[stream_name]\n\n        io_funcs = BinaryIOCollection()\n\n        for i in range(self.file_number):\n            out_file_name = out_file_list[i]\n\n            #if os.path.isfile(out_file_name):\n            #    logger.info(\'processing file %4d of %4d : %s exists\' % (i+1, self.file_number, out_file_name))\n                    #    continue\n\n            logger.info(\'processing file %4d of %4d : %s\' % (i+1,self.file_number,out_file_name))\n\n            out_data_matrix = None\n            out_frame_number = 0\n\n\n            for k in range(self.data_stream_number):\n                data_stream_name = self.data_stream_list[k]\n                in_file_name   = in_file_list_dict[data_stream_name][i]\n                in_feature_dim = in_dimension_dict[data_stream_name]\n                features, frame_number = io_funcs.load_binary_file_frame(in_file_name, in_feature_dim)\n\n                if k == 0:\n                    out_frame_number = frame_number\n                    out_data_matrix = numpy.zeros((out_frame_number, self.out_dimension))\n\n                if frame_number > out_frame_number:\n                    features = features[0:out_frame_number, ]\n                    frame_number = out_frame_number\n\n                try:\n                    assert  out_frame_number == frame_number\n                except AssertionError:\n                    logger.critical(\'the frame number of data stream %s is not consistent with others: current %d others %d\'\n                                         %(data_stream_name, out_frame_number, frame_number))\n                    raise\n\n                dim_index = stream_start_index[data_stream_name]\n\n                if data_stream_name in [\'lf0\', \'F0\']:   ## F0 added for GlottHMM\n                    features, vuv_vector = self.interpolate_f0(features)\n\n                    ### if vuv information to be recorded, store it in corresponding column\n                    if self.record_vuv:\n                        out_data_matrix[0:out_frame_number, stream_start_index[\'vuv\']:stream_start_index[\'vuv\']+1] = vuv_vector\n\n                out_data_matrix[0:out_frame_number, dim_index:dim_index+in_feature_dim] = features\n                dim_index = dim_index+in_feature_dim\n\n                if self.compute_dynamic[data_stream_name]:\n\n                    delta_features = self.compute_dynamic_matrix(features, self.delta_win, frame_number, in_feature_dim)\n                    acc_features   = self.compute_dynamic_matrix(features, self.acc_win, frame_number, in_feature_dim)\n\n\n                    out_data_matrix[0:out_frame_number, dim_index:dim_index+in_feature_dim] = delta_features\n                    dim_index = dim_index+in_feature_dim\n\n                    out_data_matrix[0:out_frame_number, dim_index:dim_index+in_feature_dim] = acc_features\n\n            ### write data to file\n            io_funcs.array_to_binary_file(out_data_matrix, out_file_name)\n            logger.debug(\' wrote %d frames of features\',out_frame_number )\n\n    def acoustic_decomposition(self, in_file_list, out_dimension_dict, file_extension_dict):\n\n        stream_start_index = {}\n        dimension_index = 0\n        recorded_vuv = False\n        vuv_dimension = None\n        for feature_name in list(out_dimension_dict.keys()):\n            if feature_name != \'vuv\':\n                stream_start_index[feature_name] = dimension_index\n            else:\n                vuv_dimension = dimension_index\n                recorded_vuv = True\n\n            dimension_index += out_dimension_dict[feature_name]\n\n        for file_name in in_file_list:\n            dir_name = os.path.dirname(file_name)\n            file_id = os.path.splitext(os.path.basename(file_name))[0]\n\n\nif __name__ == \'__main__\':\n\n    acoustic_cmper = AcousticPreparation()\n\n    in_dimension_dict = { \'mgc\' : 50,\n                          \'lf0\' : 1,\n                          \'bap\' : 25}\n    out_dimension_dict = { \'mgc\' : 150,\n                           \'lf0\' : 3,\n                           \'vuv\' : 1,\n                           \'bap\' : 75}\n\n    in_file_list_dict = {}\n    in_file_list_dict[\'mgc\'] = [\'/afs/inf.ed.ac.uk/group/project/dnn_tts/data/nick/mgc/herald_001.mgc\', \'/afs/inf.ed.ac.uk/group/project/dnn_tts/data/nick/mgc/herald_002.mgc\']\n    in_file_list_dict[\'lf0\'] = [\'/afs/inf.ed.ac.uk/group/project/dnn_tts/data/nick/lf0/herald_001.lf0\', \'/afs/inf.ed.ac.uk/group/project/dnn_tts/data/nick/lf0/herald_002.lf0\']\n    in_file_list_dict[\'bap\'] = [\'/afs/inf.ed.ac.uk/group/project/dnn_tts/data/nick/bap/herald_001.bap\', \'/afs/inf.ed.ac.uk/group/project/dnn_tts/data/nick/bap/herald_002.bap\']\n\n    out_file_list = [\'/afs/inf.ed.ac.uk/group/project/dnn_tts/herald_001.cmp\', \'/afs/inf.ed.ac.uk/group/project/dnn_tts/herald_002.cmp\']\n\n    acoustic_cmper.prepare_nn_data(in_file_list_dict, out_file_list, in_dimension_dict, out_dimension_dict)\n'"
src/frontend/acoustic_normalisation.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n\nfrom io_funcs.htk_io import HTK_Parm_IO\nfrom io_funcs.binary_io import BinaryIOCollection\nimport numpy\nimport logging\n\nclass CMPNormalisation(object):\n    def __init__(self, mgc_dim=0, bap_dim=0, lf0_dim = 0):\n        self.mgc_dim = mgc_dim * 3\n        self.bap_dim = bap_dim * 3\n        self.lf0_dim = lf0_dim * 3\n\n    def load_cmp_file(self, file_name):\n\n        logger = logging.getLogger(""acoustic_norm"")\n\n        htk_reader = HTK_Parm_IO()\n        htk_reader.read_htk(file_name)\n\n        cmp_data = htk_reader.data\n\n        mgc_data = cmp_data[:, 0:self.mgc_dim]\n\n        # this only extracts the static lf0 because we need to interpolate it, then add deltas ourselves later\n        lf0_data = cmp_data[:, self.mgc_dim]\n\n        bap_data = cmp_data[:, self.mgc_dim+self.lf0_dim:self.mgc_dim+self.lf0_dim+self.bap_dim]\n\n        logger.debug(\'loaded %s of shape %s\' % (file_name, cmp_data.shape))\n        logger.debug(\'  with: %d mgc + %d lf0 + %d bap = %d\' % (self.mgc_dim,self.lf0_dim,self.bap_dim,self.mgc_dim+self.lf0_dim+self.bap_dim))\n\n        assert( (self.mgc_dim+self.lf0_dim+self.bap_dim) == cmp_data.shape[1])\n\n        return  mgc_data, bap_data, lf0_data\n\n    def interpolate_f0(self, data):\n\n        data = numpy.reshape(data, (data.size, 1))\n\n        vuv_vector = numpy.zeros((data.size, 1))\n        vuv_vector[data > 0.0] = 1.0\n        vuv_vector[data <= 0.0] = 0.0\n\n        ip_data = data\n\n        frame_number = data.size\n        last_value = 0.0\n        for i in range(frame_number):\n            if data[i] <= 0.0:\n                j = i+1\n                for j in range(i+1, frame_number):\n                    if data[j] > 0.0:\n                        break\n                if j < frame_number-1:\n                    if last_value > 0.0:\n                        step = (data[j] - data[i-1]) / float(j - i)\n                        for k in range(i, j):\n                            ip_data[k] = data[i-1] + step * (k - i + 1)\n                    else:\n                        for k in range(i, j):\n                            ip_data[k] = data[j]\n                else:\n                    for k in range(i, frame_number):\n                        ip_data[k] = last_value\n            else:\n                ip_data[i] = data[i]\n                last_value = data[i]\n\n        return  ip_data, vuv_vector\n\n    def compute_delta(self, vector, delta_win):\n#        delta_win = [-0.5, 0.0, 0.5]\n#        acc_win   = [1.0, -2.0, 1.0]\n\n        frame_number = vector.size\n        win_length = len(delta_win)\n        win_width = int(win_length/2)\n        temp_vector = numpy.zeros((frame_number + 2 * win_width, 1))\n        delta_vector = numpy.zeros((frame_number, 1))\n\n        temp_vector[win_width:frame_number+win_width, ] = vector\n        for w in range(win_width):\n            temp_vector[w, 0] = vector[0, 0]\n            temp_vector[frame_number+win_width+w, 0] = vector[frame_number-1, 0]\n\n        for i in range(frame_number):\n            for w in range(win_length):\n                delta_vector[i] += temp_vector[i+w, 0] * delta_win[w]\n\n        return  delta_vector\n\n    def produce_nn_cmp(self, in_file_list, out_file_list):\n\n\n        logger = logging.getLogger(""acoustic_norm"")\n\n        delta_win = [-0.5, 0.0, 0.5]\n        acc_win   = [1.0, -2.0, 1.0]\n\n        file_number = len(in_file_list)\n        logger.info(\'starting creation of %d files\' % file_number)\n\n        for i in range(file_number):\n\n            mgc_data, bap_data, lf0_data = self.load_cmp_file(in_file_list[i])\n            ip_lf0, vuv_vector = self.interpolate_f0(lf0_data)\n\n            delta_lf0 = self.compute_delta(ip_lf0, delta_win)\n            acc_lf0 = self.compute_delta(ip_lf0, acc_win)\n\n            frame_number = ip_lf0.size\n\n            cmp_data = numpy.concatenate((mgc_data, ip_lf0, delta_lf0, acc_lf0, vuv_vector, bap_data), axis=1)\n\n            io_funcs = BinaryIOCollection()\n            io_funcs.array_to_binary_file(cmp_data, out_file_list[i])\n\n        logger.info(\'finished creation of %d binary files\' % file_number)\n\nif __name__ == \'__main__\':\n    in_file_list = [\'/group/project/dnn_tts/data/nick/cmp/herald_001.cmp\']\n    out_file_list = [\'/group/project/dnn_tts/herald_001.out.cmp\']\n\n    cmp_norm = CMPNormalisation(mgc_dim=50, bap_dim=25, lf0_dim = 1)\n\n    cmp_norm.produce_nn_cmp(in_file_list, out_file_list)\n'"
src/frontend/feature_normalisation_base.py,0,"b""################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n\nimport numpy\nfrom io_funcs.binary_io import BinaryIOCollection\n\nimport logging\n\nclass FeatureNormBase(object):\n    '''\n    to normalise feature into specific range\n    to de-normalise feature back\n    support min-max norm, MVN,\n    this is a genetic class\n    '''\n    def __init__(self):\n        self.logger = logging.getLogger('feature_normalisation')\n\n        self.dimension_dict = {}\n        self.start_index_dict = {}\n        self.feature_dimension = 0\n\n    def feature_normalisation(self):\n        pass\n\n    def feature_denormalisation(self):\n        pass\n\n\n    def normal_standardization(self, in_file_list, out_file_list, feature_dimension):\n\n#        self.dimension_dict = dimension_dict\n        self.feature_dimension = feature_dimension\n\n        mean_vector = self.compute_mean(in_file_list, 0, feature_dimension)\n        std_vector = self.compute_std(in_file_list, mean_vector, 0, feature_dimension)\n\n        io_funcs = BinaryIOCollection()\n        file_number = len(in_file_list)\n\n        for i in range(file_number):\n\n            features, current_frame_number = io_funcs.load_binary_file_frame(in_file_list[i], self.feature_dimension)\n\n            mean_matrix = numpy.tile(mean_vector, (current_frame_number, 1))\n            std_matrix = numpy.tile(std_vector, (current_frame_number, 1))\n\n            norm_features = (features - mean_matrix) / std_matrix\n\n            io_funcs.array_to_binary_file(norm_features, out_file_list[i])\n\n        return  mean_vector, std_vector\n\n    def find_min_max_values(self, in_file_list, start_index, end_index):\n\n        local_feature_dimension = end_index - start_index\n\n        file_number = len(in_file_list)\n        min_value_matrix = numpy.zeros((file_number, local_feature_dimension))\n        max_value_matrix = numpy.zeros((file_number, local_feature_dimension))\n        io_funcs = BinaryIOCollection()\n        for i in range(file_number):\n            features = io_funcs.load_binary_file(in_file_list[i], self.feature_dimension)\n\n            temp_min = numpy.amin(features[:, start_index:end_index], axis = 0)\n            temp_max = numpy.amax(features[:, start_index:end_index], axis = 0)\n\n            min_value_matrix[i, ] = temp_min;\n            max_value_matrix[i, ] = temp_max;\n\n        self.min_vector = numpy.amin(min_value_matrix, axis = 0)\n        self.max_vector = numpy.amax(max_value_matrix, axis = 0)\n        self.min_vector = numpy.reshape(self.min_vector, (1, local_feature_dimension))\n        self.max_vector = numpy.reshape(self.max_vector, (1, local_feature_dimension))\n\n        # po=numpy.get_printoptions()\n        # numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)\n        self.logger.info('found min/max values of length %d:' % local_feature_dimension)\n        self.logger.info('  min: %s' % self.min_vector)\n        self.logger.info('  max: %s' % self.max_vector)\n        # restore the print options\n        # numpy.set_printoptions(po)\n\n    def compute_mean(self, file_list, start_index, end_index):\n\n        local_feature_dimension = end_index - start_index\n\n        mean_vector = numpy.zeros((1, local_feature_dimension))\n        all_frame_number = 0\n\n        io_funcs = BinaryIOCollection()\n        for file_name in file_list:\n            features, current_frame_number = io_funcs.load_binary_file_frame(file_name, self.feature_dimension)\n\n            mean_vector += numpy.reshape(numpy.sum(features[:, start_index:end_index], axis=0), (1, local_feature_dimension))\n            all_frame_number += current_frame_number\n\n        mean_vector /= float(all_frame_number)\n\n        # po=numpy.get_printoptions()\n        # numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)\n        self.logger.info('computed mean vector of length %d :' % mean_vector.shape[1] )\n        self.logger.info(' mean: %s' % mean_vector)\n        # restore the print options\n        # numpy.set_printoptions(po)\n\n        return  mean_vector\n\n    def compute_std(self, file_list, mean_vector, start_index, end_index):\n        local_feature_dimension = end_index - start_index\n\n        std_vector = numpy.zeros((1, self.feature_dimension))\n        all_frame_number = 0\n\n        io_funcs = BinaryIOCollection()\n        for file_name in file_list:\n            features, current_frame_number = io_funcs.load_binary_file_frame(file_name, self.feature_dimension)\n\n            mean_matrix = numpy.tile(mean_vector, (current_frame_number, 1))\n\n            std_vector += numpy.reshape(numpy.sum((features[:, start_index:end_index] - mean_matrix) ** 2, axis=0), (1, local_feature_dimension))\n            all_frame_number += current_frame_number\n\n        std_vector /= float(all_frame_number)\n\n        std_vector = std_vector ** 0.5\n\n        # po=numpy.get_printoptions()\n        # numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)\n        self.logger.info('computed  std vector of length %d' % std_vector.shape[1] )\n        self.logger.info('  std: %s' % std_vector)\n        # restore the print options\n        # numpy.set_printoptions(po)\n\n        return  std_vector\n"""
src/frontend/label_composer.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n\nimport logging\nimport imp\nimport numpy\nfrom io_funcs.binary_io import BinaryIOCollection\n\nfrom frontend.label_normalisation import HTSLabelNormalisation\n\n\n# context-dependent printing format for Numpy - should move this out to a utility file somewhere\nimport contextlib\n@contextlib.contextmanager\ndef printoptions(*args, **kwargs):\n    original = numpy.get_printoptions()\n    numpy.set_printoptions(*args, **kwargs)\n    yield\n    numpy.set_printoptions(**original)\n\n\nclass LabelComposer(object):\n\n    # a class that can compose input labels according to the user\'s specification, and convert them to numerical vectors\n\n    def __init__(self):\n\n        self.logger = logging.getLogger(""labels"")\n        self.configuration = None\n        self.label_dimension=None\n\n        # what label styles we find in the feature specification\n        # e.g., \'xpath\' , \'hts\'\n        self.label_styles={}\n\n        self.use_precompiled_xpaths = False ## will be set True if xpaths are compiled\n\n    def load_label_configuration(self, filename):\n\n        # load in a label specification, provided by the user\n        try:\n            self.configuration = imp.load_source(\'label_config\',filename)\n        except IOError:\n            self.logger.critical(\'failed to open label configuration file %s\' % filename)\n            raise\n        except:\n            self.logger.critical(\'error loading label configuration from %s\' % filename)\n            raise\n\n        # perform some sanity checks on it\n        #\n        # make sure \'labels\' is defined\n        try:\n            assert self.configuration.labels\n        except AssertionError:\n            logger.critical(\'loaded label configuration file %s, but it did not define ""labels"" !\' % filename)\n\n\n    def compute_label_dimension(self):\n\n        self.label_dimension=0\n\n        try:\n            assert self.configuration\n        except AssertionError:\n            self.logger.critical(\'no label configuration loaded, so cannot compute dimension\')\n            raise\n\n        for feature_specification in self.configuration.labels:\n            #osw# self.logger.debug(\'looking at feature %s\' % feature_specification )\n            # feature is a dictionary specifying how to construct this part of the input feature vector\n            if \'xpath\' in feature_specification:\n                # xpath and hts are mutually exclusive label styles\n                assert \'hts\' not in feature_specification\n\n                # if there is a mapper, then we will use that to convert the features to numbers\n                # we need to look at the mapper to deduce the dimensionality of vectors that it will produce\n                if \'mapper\' in feature_specification:\n\n                    # get an arbitrary item as the reference and measure its dimensionality\n                    try:\n                        l = len(next(iter(feature_specification[\'mapper\'].values())))\n                    except:\n                        logger.critical(\'Empty mapper for feature %s\' % feature_specification)\n\n                    for k,v in feature_specification[\'mapper\'].items():\n                        # make sure all other entries have the same dimension\n                        try:\n                            assert len(v) == l\n                        except AssertionError:\n                            logger.critical(\'Inconsistent dimensionality in mapper for feature %s\' % feature_specification)\n                    self.label_dimension = self.label_dimension + l\n                    #print \'   add %s    cum: %s\'%( str(l), self.label_dimension)\n\n\n                else:\n                    # without a mapper, features will be single numerical values\n                    self.label_dimension = self.label_dimension + 1\n                    #print \'   add 1    cum: %s\'%( self.label_dimension)\n\n                # we have seen at least one feature that will required xpath label files to be loaded\n                self.label_styles[\'xpath\'] = True\n\n\n            if \'hts\' in feature_specification:\n                assert \'xpath\' not in feature_specification\n                self.label_styles[\'hts\'] = False # will become True once implemented\n                # not yet implemented !\n                self.logger.warning(\'HTS features not implemented - ignoring them!\')\n\n        self.label_dimension += 1 ## for frame features -- TODO: decide how to handle this properly\n        #print \'   add 3   cum: %s\'%(  self.label_dimension)\n\n        return self.label_dimension\n\n\nif __name__ == \'__main__\':\n\n    logger = logging.getLogger(""labels"")\n    logger.setLevel(logging.DEBUG)\n    # a console handler\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.DEBUG)\n    logger.addHandler(ch)\n\n    label_composer = LabelComposer()\n    label_composer.load_label_configuration(\'configuration/labelconfigfile.conf\')\n\n    print(\'Loaded configuration, which is:\')\n    print(label_composer.configuration.labels)\n\n    d=label_composer.compute_label_dimension()\n    print(""label dimension will be"",d)\n\n    # not written test code for actual label processing - too complex and relies on config files\n'"
src/frontend/label_modifier.py,0,"b'\nimport os\nimport numpy, re, sys\nfrom io_funcs.binary_io import BinaryIOCollection\n\nimport logging\n# from logplot.logging_plotting import LoggerPlotter #, MultipleTimeSeriesPlot, SingleWeightMatrixPlot\n\nclass HTSLabelModification(object):\n    """"""This class is to modify HTS format labels with predicted duration.\n\n    Time alignments are expected in the HTS labels. Here is an example of the HTS labels:\n\n    3050000 3100000 xx~#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[2]\n\n    3100000 3150000 xx~#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[3]\n\n    3150000 3250000 xx~#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[4]\n\n    3250000 3350000 xx~#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[5]\n\n    3350000 3900000 xx~#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[6]\n\n    305000 310000 are the starting and ending time.\n    [2], [3], [4], [5], [6] mean the HMM state index.\n\n    """"""\n\n    def __init__(self, silence_pattern=[\'*-#+*\'], label_type=""state_align""):\n\n        logger = logging.getLogger(""labels"")\n\n        self.silence_pattern = silence_pattern\n        self.silence_pattern_size = len(silence_pattern)\n        self.label_type = label_type\n        self.state_number = 5\n\n    def check_silence_pattern(self, label):\n        for current_pattern in self.silence_pattern:\n            current_pattern = current_pattern.strip(\'*\')\n            if current_pattern in label:\n                return 1\n        return 0\n\n\n    def modify_duration_labels(self, in_gen_label_align_file_list, gen_dur_list, gen_label_list):\n        \'\'\'\n        modifying duration from label alignments with predicted duration.\n        \'\'\'\n        utt_number = len(gen_dur_list)\n        if utt_number != len(in_gen_label_align_file_list):\n            print(""the number of input and output files should be the same!\\n"");\n            sys.exit(1)\n\n        for i in range(utt_number):\n            if (self.label_type==""state_align""):\n                self.modify_dur_from_state_alignment_labels(in_gen_label_align_file_list[i], gen_dur_list[i], gen_label_list[i])\n            elif (self.label_type==""phone_align""):\n                self.modify_dur_from_phone_alignment_labels(in_gen_label_align_file_list[i], gen_dur_list[i], gen_label_list[i])\n            else:\n                logger.critical(""we don\'t support %s labels as of now!!"" % (self.label_type))\n                sys.exit(1)\n\n    def modify_dur_from_state_alignment_labels(self, label_file_name, gen_dur_file_name, gen_lab_file_name):\n        logger = logging.getLogger(""dur"")\n\n        state_number = self.state_number\n        dur_dim = state_number\n\n        io_funcs = BinaryIOCollection()\n        dur_features, frame_number = io_funcs.load_binary_file_frame(gen_dur_file_name, dur_dim)\n\n        fid = open(label_file_name)\n        utt_labels = fid.readlines()\n        fid.close()\n\n        label_number = len(utt_labels)\n        logger.info(\'loaded %s, %3d labels\' % (label_file_name, label_number) )\n\n        out_fid = open(gen_lab_file_name, \'w\')\n\n        current_index = 0\n        prev_end_time = 0\n        for line in utt_labels:\n            line = line.strip()\n\n            if len(line) < 1:\n                continue\n            temp_list = re.split(\'\\s+\', line)\n\n            if len(temp_list)==1:\n                start_time = 0 \n                end_time = 600000 ## hard-coded silence duration\n                full_label = temp_list[0]\n            else:\n                start_time = int(temp_list[0])\n                end_time = int(temp_list[1])\n                full_label = temp_list[2]\n\n                full_label_length = len(full_label) - 3  # remove state information [k]\n                state_index = full_label[full_label_length + 1]\n                state_index = int(state_index) - 1\n\n            label_binary_flag = self.check_silence_pattern(full_label)\n\n            if len(temp_list)==1:\n                for state_index in range(1, state_number+1):\n                    if label_binary_flag == 1:\n                        current_state_dur = end_time - start_time\n                    else:\n                        pred_state_dur = dur_features[current_index, state_index-1]\n                        current_state_dur = int(pred_state_dur)*5*10000\n                    out_fid.write(str(prev_end_time)+\' \'+str(prev_end_time+current_state_dur)+\' \'+full_label+\'[\'+str(state_index+1)+\']\\n\')\n                    prev_end_time = prev_end_time + current_state_dur\n            else:\n                if label_binary_flag == 1:\n                    current_state_dur = end_time - start_time\n                else:\n                    pred_state_dur = dur_features[current_index, state_index-1]\n                    current_state_dur = int(pred_state_dur)*5*10000\n                out_fid.write(str(prev_end_time)+\' \'+str(prev_end_time+current_state_dur)+\' \'+full_label+\'\\n\')\n                prev_end_time = prev_end_time + current_state_dur\n\n            if state_index == state_number and label_binary_flag!=1:\n                current_index += 1\n\n        logger.debug(\'modifed label with predicted duration of %d frames x %d features\' % dur_features.shape )\n\n    def modify_dur_from_phone_alignment_labels(self, label_file_name, gen_dur_file_name, gen_lab_file_name):\n        logger = logging.getLogger(""dur"")\n\n        dur_dim = 1\n\n        io_funcs = BinaryIOCollection()\n        dur_features, frame_number = io_funcs.load_binary_file_frame(gen_dur_file_name, dur_dim)\n\n        fid = open(label_file_name)\n        utt_labels = fid.readlines()\n        fid.close()\n\n        label_number = len(utt_labels)\n        logger.info(\'loaded %s, %3d labels\' % (label_file_name, label_number) )\n\n        out_fid = open(gen_lab_file_name, \'w\')\n\n        current_index = 0\n        prev_end_time = 0\n        for line in utt_labels:\n            line = line.strip()\n\n            if len(line) < 1:\n                continue\n            temp_list = re.split(\'\\s+\', line)\n            \n            if len(temp_list)==1:\n                start_time = 0 \n                end_time = 3000000 ## hard-coded silence duration\n                full_label = temp_list[0]\n            else:\n                start_time = int(temp_list[0])\n                end_time = int(temp_list[1])\n                full_label = temp_list[2]\n\n            label_binary_flag = self.check_silence_pattern(full_label)\n\n            if label_binary_flag == 1:\n                current_phone_dur = end_time - start_time\n                out_fid.write(str(prev_end_time)+\' \'+str(prev_end_time+current_phone_dur)+\' \'+full_label+\'\\n\')\n                prev_end_time = prev_end_time+current_phone_dur\n                continue;\n            else:\n                phone_dur = dur_features[current_index]\n                phone_dur = int(phone_dur)*5*10000\n                out_fid.write(str(prev_end_time)+\' \'+str(prev_end_time+phone_dur)+\' \'+full_label+\'\\n\')\n                prev_end_time = prev_end_time+phone_dur\n\n            current_index += 1\n\n        logger.debug(\'modifed label with predicted duration of %d frames x %d features\' % dur_features.shape )\n'"
src/frontend/label_normalisation.py,0,"b'\nimport os\nimport numpy, re, sys\nfrom multiprocessing import Pool\nfrom io_funcs.binary_io import BinaryIOCollection\nfrom .linguistic_base import LinguisticBase\n\nimport matplotlib.mlab as mlab\nimport math\n\nimport logging\n# from logplot.logging_plotting import LoggerPlotter #, MultipleTimeSeriesPlot, SingleWeightMatrixPlot\n\nclass LabelNormalisation(LinguisticBase):\n\n    # this class only knows how to deal with a single style of labels (XML or HTS)\n    # (to deal with composite labels, use LabelComposer instead)\n\n    def __init__(self, question_file_name=None,xpath_file_name=None):\n        pass\n\n    def extract_linguistic_features(self, in_file_name, out_file_name=None, label_type=""state_align"", dur_file_name=None):\n        if label_type==""phone_align"":\n            A = self.load_labels_with_phone_alignment(in_file_name, dur_file_name)\n        elif label_type==""state_align"":\n            A = self.load_labels_with_state_alignment(in_file_name)\n        else:\n            logger.critical(""we don\'t support %s labels as of now!!"" % (label_type))\n\n        if out_file_name:\n            io_funcs = BinaryIOCollection()\n            io_funcs.array_to_binary_file(A, out_file_name)\n        else:\n            return A\n\n#  -----------------------------\n\n\n\nclass HTSLabelNormalisation(LabelNormalisation):\n    """"""This class is to convert HTS format labels into continous or binary values, and store as binary format with float32 precision.\n\n    The class supports two kinds of questions: QS and CQS.\n        **QS**: is the same as that used in HTS\n\n        **CQS**: is the new defined question in the system.  Here is an example of the question: CQS C-Syl-Tone {_(\\d+)+}. regular expression is used for continous values.\n\n    Time alignments are expected in the HTS labels. Here is an example of the HTS labels:\n\n    3050000 3100000 xx~#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[2]\n\n    3100000 3150000 xx~#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[3]\n\n    3150000 3250000 xx~#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[4]\n\n    3250000 3350000 xx~#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[5]\n\n    3350000 3900000 xx~#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[6]\n\n    305000 310000 are the starting and ending time.\n    [2], [3], [4], [5], [6] mean the HMM state index.\n\n    """"""\n\n    # this subclass support HTS labels, which include time alignments\n\n    def __init__(self, question_file_name=None, add_frame_features=True, subphone_feats=\'full\', continuous_flag=True):\n\n        logger = logging.getLogger(""labels"")\n\n        self.question_dict = {}\n        self.ori_question_dict = {}\n        self.dict_size = 0\n        self.continuous_flag = continuous_flag\n        try:\n#            self.question_dict, self.ori_question_dict = self.load_question_set(question_file_name)\n            self.discrete_dict, self.continuous_dict = self.load_question_set_continous(question_file_name)\n        except:\n            logger.critical(\'error whilst loading HTS question set\')\n            raise\n\n        ###self.dict_size = len(self.question_dict)\n\n        self.dict_size = len(self.discrete_dict) + len(self.continuous_dict)\n        self.add_frame_features = add_frame_features\n        self.subphone_feats = subphone_feats\n\n        if self.subphone_feats == \'full\':\n            self.frame_feature_size = 9   ## zhizheng\'s original 5 state features + 4 phoneme features\n        elif self.subphone_feats == \'minimal_frame\':\n            self.frame_feature_size = 2   ## the minimal features necessary to go from a state-level to frame-level model\n        elif self.subphone_feats == \'state_only\':\n            self.frame_feature_size = 1   ## this is equivalent to a state-based system\n        elif self.subphone_feats == \'none\':\n            self.frame_feature_size = 0   ## the phoneme level features only\n        elif self.subphone_feats == \'frame_only\':\n            self.frame_feature_size = 1   ## this is equivalent to a frame-based system without relying on state-features\n        elif self.subphone_feats == \'uniform_state\':\n            self.frame_feature_size = 2   ## this is equivalent to a frame-based system with uniform state-features\n        elif self.subphone_feats == \'minimal_phoneme\':\n            self.frame_feature_size = 3   ## this is equivalent to a frame-based system with minimal features\n        elif self.subphone_feats == \'coarse_coding\':\n            self.frame_feature_size = 4   ## this is equivalent to a frame-based positioning system reported in Heiga Zen\'s work\n            self.cc_features = self.compute_coarse_coding_features(3)\n        else:\n            sys.exit(\'Unknown value for subphone_feats: %s\'%(subphone_feats))\n\n        self.dimension = self.dict_size + self.frame_feature_size\n\n        ### if user wants to define their own input, simply set the question set to empty.\n        if self.dict_size == 0:\n            self.dimension = 0\n\n        logger.debug(\'HTS-derived input feature dimension is %d + %d = %d\' % (self.dict_size, self.frame_feature_size, self.dimension) )\n\n    def prepare_dur_data(self, ori_file_list, output_file_list, label_type=""state_align"", feature_type=None, unit_size=None, feat_size=None):\n        \'\'\'\n        extracting duration binary features or numerical features.\n        \'\'\'\n        logger = logging.getLogger(""dur"")\n        utt_number = len(ori_file_list)\n        if utt_number != len(output_file_list):\n            print(""the number of input and output files should be the same!\\n"");\n            sys.exit(1)\n\n        ### set default feature type to numerical, if not assigned ###\n        if not feature_type:\n            feature_type = ""numerical""\n\n        ### set default unit size to state, if not assigned ###\n        if not unit_size:\n            unit_size = ""state""\n        if label_type==""phone_align"":\n            unit_size = ""phoneme""\n\n        ### set default feat size to frame or phoneme, if not assigned ###\n        if feature_type==""binary"":\n            if not feat_size:\n                feat_size = ""frame""\n        elif feature_type==""numerical"":\n            if not feat_size:\n                feat_size = ""phoneme""\n        else:\n            logger.critical(""Unknown feature type: %s \\n Please use one of the following: binary, numerical\\n"" %(feature_type))\n            sys.exit(1)\n\n        for i in range(utt_number):\n            self.extract_dur_features(ori_file_list[i], output_file_list[i], label_type, feature_type, unit_size, feat_size)\n\n    def extract_dur_features(self, in_file_name, out_file_name=None, label_type=""state_align"", feature_type=None, unit_size=None, feat_size=None):\n        logger = logging.getLogger(""dur"")\n        if label_type==""phone_align"":\n            A = self.extract_dur_from_phone_alignment_labels(in_file_name, feature_type, unit_size, feat_size)\n        elif label_type==""state_align"":\n            A = self.extract_dur_from_state_alignment_labels(in_file_name, feature_type, unit_size, feat_size)\n        else:\n            logger.critical(""we don\'t support %s labels as of now!!"" % (label_type))\n            sys.exit(1)\n\n        if out_file_name:\n            io_funcs = BinaryIOCollection()\n            io_funcs.array_to_binary_file(A, out_file_name)\n        else:\n            return A\n\n    def extract_dur_from_state_alignment_labels(self, file_name, feature_type, unit_size, feat_size):\n        logger = logging.getLogger(""dur"")\n\n        state_number = 5\n        dur_dim = state_number\n\n        if feature_type==""binary"":\n            dur_feature_matrix = numpy.empty((100000, 1))\n        elif feature_type==""numerical"":\n            if unit_size==""state"":\n                dur_feature_matrix = numpy.empty((100000, dur_dim))\n                current_dur_array = numpy.zeros((dur_dim, 1))\n            else: ## phoneme/syllable/word\n                dur_feature_matrix = numpy.empty((100000, 1))\n\n        fid = open(file_name)\n        utt_labels = fid.readlines()\n        fid.close()\n\n        label_number = len(utt_labels)\n        logger.info(\'loaded %s, %3d labels\' % (file_name, label_number) )\n\n        MLU_dur = [[],[],[]]\n        list_of_silences=[\'#\', \'sil\', \'pau\', \'SIL\']\n        current_index = 0\n        dur_feature_index = 0\n        syllable_duration = 0\n        word_duration = 0\n        for line in utt_labels:\n            line = line.strip()\n\n            if len(line) < 1:\n                continue\n            temp_list = re.split(\'\\s+\', line)\n            start_time = int(temp_list[0])\n            end_time = int(temp_list[1])\n\n            full_label = temp_list[2]\n            full_label_length = len(full_label) - 3  # remove state information [k]\n            state_index = full_label[full_label_length + 1]\n            state_index = int(state_index) - 1\n            current_phone = full_label[full_label.index(\'-\') + 1:full_label.index(\'+\')]\n\n            frame_number = int(end_time/50000) - int(start_time/50000)\n\n            if state_index == 1:\n                phone_duration = frame_number\n\n                for i in range(state_number - 1):\n                    line = utt_labels[current_index + i + 1].strip()\n                    temp_list = re.split(\'\\s+\', line)\n                    phone_duration += int((int(temp_list[1]) - int(temp_list[0]))/50000)\n                \n                syllable_duration+=phone_duration\n                word_duration+=phone_duration\n\n                ### for syllable and word positional information ###\n                label_binary_vector = self.pattern_matching_binary(full_label)\n                label_continuous_vector = self.pattern_matching_continous_position(full_label)\n\n                ### syllable ending information ###\n                syl_end = 0        \n                if(label_continuous_vector[0, 1]==1 or current_phone in list_of_silences): ##pos-bw and c-silences\n                    syl_end = 1\n\n                ### word ending information ###\n                word_end = 0        \n                if(syl_end and label_continuous_vector[0, 9]==1 or current_phone in list_of_silences):\n                    word_end = 1\n\n            if feature_type == ""binary"":\n                current_block_array = numpy.zeros((frame_number, 1))\n                if unit_size == ""state"":\n                    current_block_array[-1] = 1\n                elif unit_size == ""phoneme"":\n                    if state_index == state_number:\n                        current_block_array[-1] = 1\n                else:\n                    logger.critical(""Unknown unit size: %s \\n Please use one of the following: state, phoneme\\n"" %(unit_size))\n                    sys.exit(1)\n            elif feature_type == ""numerical"":\n                if unit_size == ""state"":\n                    current_dur_array[current_index%5] = frame_number\n                    if feat_size == ""phoneme"" and state_index == state_number:\n                        current_block_array =  current_dur_array.transpose()\n                    if feat_size == ""frame"":\n                        current_block_array = numpy.tile(current_dur_array.transpose(), (frame_number, 1))\n                elif state_index == state_number: \n                    if unit_size == ""phoneme"":\n                        current_block_array = numpy.array([phone_duration])\n                    elif unit_size == ""syllable"":\n                        current_block_array = numpy.array([syllable_duration])\n                    elif unit_size == ""word"":\n                        current_block_array = numpy.array([word_duration])\n                    if syl_end:\n                        syllable_duration = 0\n                    if word_end:\n                        word_duration = 0\n\n\n            ### writing into dur_feature_matrix ###\n            if feat_size == ""frame"":\n                dur_feature_matrix[dur_feature_index:dur_feature_index+frame_number,] = current_block_array\n                dur_feature_index = dur_feature_index + frame_number\n            elif state_index == state_number:\n                if feat_size == ""phoneme"":\n                    dur_feature_matrix[dur_feature_index:dur_feature_index+1,] = current_block_array\n                    dur_feature_index = dur_feature_index + 1\n                elif current_phone!=\'#\': ## removing silence here\n                    if feat_size == ""syllable"" and syl_end:\n                        dur_feature_matrix[dur_feature_index:dur_feature_index+1,] = current_block_array\n                        dur_feature_index = dur_feature_index + 1\n                    elif feat_size == ""word"" and word_end:\n                        dur_feature_matrix[dur_feature_index:dur_feature_index+1,] = current_block_array\n                        dur_feature_index = dur_feature_index + 1\n                    elif feat_size == ""MLU"":\n                        if word_end:\n                            if current_phone==\'pau\':\n                                MLU_dur[0].append(1)\n                            else:\n                                MLU_dur[0].append(int(label_continuous_vector[0, 24]))\n                        if syl_end:\n                            if current_phone==\'pau\':\n                                MLU_dur[1].append(1)\n                            else:\n                                MLU_dur[1].append(int(label_continuous_vector[0, 7]))\n                        MLU_dur[2].append(int(phone_duration))\n\n\n            current_index += 1\n\n        if feat_size == ""MLU"":\n            for seg_indx in xrange(len(MLU_dur)):\n                seg_len = len(MLU_dur[seg_indx])\n                current_block_array = numpy.reshape(numpy.array(MLU_dur[seg_indx]), (-1, 1))\n                dur_feature_matrix[dur_feature_index:dur_feature_index+seg_len, ] = current_block_array\n                dur_feature_index = dur_feature_index + seg_len\n        \n        dur_feature_matrix = dur_feature_matrix[0:dur_feature_index,]\n        logger.debug(\'made duration matrix of %d frames x %d features\' % dur_feature_matrix.shape )\n        return  dur_feature_matrix\n\n    def extract_dur_from_phone_alignment_labels(self, file_name, feature_type, unit_size, feat_size):\n        logger = logging.getLogger(""dur"")\n\n        dur_dim = 1 # hard coded here \n\n        if feature_type==""binary"":\n            dur_feature_matrix = numpy.empty((100000, dur_dim))\n        elif feature_type==""numerical"":\n            if unit_size==""phoneme"":\n                dur_feature_matrix = numpy.empty((100000, dur_dim))\n\n        fid = open(file_name)\n        utt_labels = fid.readlines()\n        fid.close()\n\n        label_number = len(utt_labels)\n        logger.info(\'loaded %s, %3d labels\' % (file_name, label_number) )\n\n        current_index = 0\n        dur_feature_index = 0\n        for line in utt_labels:\n            line = line.strip()\n\n            if len(line) < 1:\n                continue\n            temp_list = re.split(\'\\s+\', line)\n            start_time = int(temp_list[0])\n            end_time = int(temp_list[1])\n\n            full_label = temp_list[2]\n\n            frame_number = int(end_time/50000) - int(start_time/50000)\n\n            phone_duration = frame_number\n\n            if feature_type == ""binary"":\n                current_block_array = numpy.zeros((frame_number, 1))\n                if unit_size == ""phoneme"":\n                    current_block_array[-1] = 1\n                else:\n                    logger.critical(""Unknown unit size: %s \\n Please use one of the following: phoneme\\n"" %(unit_size))\n                    sys.exit(1)\n            elif feature_type == ""numerical"":\n                if unit_size == ""phoneme"":\n                    current_block_array = numpy.array([phone_duration])\n\n            ### writing into dur_feature_matrix ###\n            if feat_size == ""frame"":\n                dur_feature_matrix[dur_feature_index:dur_feature_index+frame_number,] = current_block_array\n                dur_feature_index = dur_feature_index + frame_number\n            elif feat_size == ""phoneme"":\n                dur_feature_matrix[dur_feature_index:dur_feature_index+1,] = current_block_array\n                dur_feature_index = dur_feature_index + 1\n\n            current_index += 1\n\n        dur_feature_matrix = dur_feature_matrix[0:dur_feature_index,]\n        logger.debug(\'made duration matrix of %d frames x %d features\' % dur_feature_matrix.shape )\n        return  dur_feature_matrix\n\n    def load_labels_with_phone_alignment(self, file_name, dur_file_name):\n\n        # this is not currently used ??? -- it works now :D\n        logger = logging.getLogger(""labels"")\n        #logger.critical(\'unused function ???\')\n        #raise Exception\n\n        if dur_file_name:\n            io_funcs = BinaryIOCollection()\n            dur_dim = 1 ## hard coded for now\n            manual_dur_data = io_funcs.load_binary_file(dur_file_name, dur_dim)\n\n        if self.add_frame_features:\n            assert self.dimension == self.dict_size+self.frame_feature_size\n        elif self.subphone_feats != \'none\':\n            assert self.dimension == self.dict_size+self.frame_feature_size\n        else:\n            assert self.dimension == self.dict_size\n\n        label_feature_matrix = numpy.empty((100000, self.dimension))\n\n        ph_count=0\n        label_feature_index = 0\n        with open(file_name) as fid:\n            all_data = fid.readlines()\n        for line in all_data:\n            line = line.strip()\n            if len(line) < 1:\n                continue\n            temp_list = re.split(\'\\s+\', line)\n            \n            if len(temp_list)==1:\n                frame_number = 0\n                full_label = temp_list[0]\n            else:\n                start_time = int(temp_list[0])\n                end_time = int(temp_list[1])\n                full_label = temp_list[2]\n\n                # to do - support different frame shift - currently hardwired to 5msec\n                # currently under beta testing: support different frame shift\n                if dur_file_name:\n                    frame_number = manual_dur_data[ph_count]\n                else:\n                    frame_number = int(end_time/50000) - int(start_time/50000)\n\n                if self.subphone_feats == ""coarse_coding"":\n                    cc_feat_matrix = self.extract_coarse_coding_features_relative(frame_number)\n\n            ph_count = ph_count+1\n            #label_binary_vector = self.pattern_matching(full_label)\n            label_binary_vector = self.pattern_matching_binary(full_label)\n\n            # if there is no CQS question, the label_continuous_vector will become to empty\n            label_continuous_vector = self.pattern_matching_continous_position(full_label)\n            label_vector = numpy.concatenate([label_binary_vector, label_continuous_vector], axis = 1)\n\n            if self.add_frame_features:\n                current_block_binary_array = numpy.zeros((frame_number, self.dict_size+self.frame_feature_size))\n                for i in range(frame_number):\n                    current_block_binary_array[i, 0:self.dict_size] = label_vector\n\n                    if self.subphone_feats == \'minimal_phoneme\':\n                        ## features which distinguish frame position in phoneme\n                        current_block_binary_array[i, self.dict_size] = float(i+1)/float(frame_number) # fraction through phone forwards\n                        current_block_binary_array[i, self.dict_size+1] = float(frame_number - i)/float(frame_number) # fraction through phone backwards\n                        current_block_binary_array[i, self.dict_size+2] = float(frame_number) # phone duration\n\n                    elif self.subphone_feats == \'coarse_coding\':\n                        ## features which distinguish frame position in phoneme using three continous numerical features\n                        current_block_binary_array[i, self.dict_size+0] = cc_feat_matrix[i, 0]\n                        current_block_binary_array[i, self.dict_size+1] = cc_feat_matrix[i, 1]\n                        current_block_binary_array[i, self.dict_size+2] = cc_feat_matrix[i, 2]\n                        current_block_binary_array[i, self.dict_size+3] = float(frame_number)\n\n                    elif self.subphone_feats == \'none\':\n                        pass\n\n                    else:\n                        sys.exit(\'unknown subphone_feats type\')\n\n                label_feature_matrix[label_feature_index:label_feature_index+frame_number,] = current_block_binary_array\n                label_feature_index = label_feature_index + frame_number\n\n            elif self.subphone_feats == \'none\':\n                current_block_binary_array = label_vector\n                label_feature_matrix[label_feature_index:label_feature_index+1,] = current_block_binary_array\n                label_feature_index = label_feature_index + 1\n\n        label_feature_matrix = label_feature_matrix[0:label_feature_index,]\n\n        logger.info(\'loaded %s, %3d labels\' % (file_name, ph_count) )\n        logger.debug(\'made label matrix of %d frames x %d labels\' % label_feature_matrix.shape )\n        return  label_feature_matrix\n\n\n    def load_labels_with_state_alignment(self, file_name):\n        ## setting add_frame_features to False performs either state/phoneme level normalisation\n\n        logger = logging.getLogger(""labels"")\n\n        if self.add_frame_features:\n            assert self.dimension == self.dict_size+self.frame_feature_size\n        elif self.subphone_feats != \'none\':\n            assert self.dimension == self.dict_size+self.frame_feature_size\n        else:\n            assert self.dimension == self.dict_size\n\n        # label_feature_matrix = numpy.empty((100000, self.dict_size+self.frame_feature_size))\n        label_feature_matrix = numpy.empty((100000, self.dimension))\n\n        label_feature_index = 0\n\n        state_number = 5\n\n        lab_binary_vector = numpy.zeros((1, self.dict_size))\n        fid = open(file_name)\n        utt_labels = fid.readlines()\n        fid.close()\n        current_index = 0\n        label_number = len(utt_labels)\n        logger.info(\'loaded %s, %3d labels\' % (file_name, label_number) )\n\n        phone_duration = 0\n        state_duration_base = 0\n        for line in utt_labels:\n            line = line.strip()\n\n            if len(line) < 1:\n                continue\n            temp_list = re.split(\'\\s+\', line)\n\n            if len(temp_list)==1:\n                frame_number = 0\n                state_index = 1\n                full_label = temp_list[0]\n            else:\n                start_time = int(temp_list[0])\n                end_time = int(temp_list[1])\n                frame_number = int(end_time/50000) - int(start_time/50000)\n                full_label = temp_list[2]\n            \n                full_label_length = len(full_label) - 3  # remove state information [k]\n                state_index = full_label[full_label_length + 1]\n\n                state_index = int(state_index) - 1\n                state_index_backward = 6 - state_index\n                full_label = full_label[0:full_label_length]\n\n            if state_index == 1:\n                current_frame_number = 0\n                phone_duration = frame_number\n                state_duration_base = 0\n\n#                label_binary_vector = self.pattern_matching(full_label)\n                label_binary_vector = self.pattern_matching_binary(full_label)\n\n                # if there is no CQS question, the label_continuous_vector will become to empty\n                label_continuous_vector = self.pattern_matching_continous_position(full_label)\n                label_vector = numpy.concatenate([label_binary_vector, label_continuous_vector], axis = 1)\n\n                if len(temp_list)==1:\n                    state_index = state_number\n                else:\n                    for i in range(state_number - 1):\n                        line = utt_labels[current_index + i + 1].strip()\n                        temp_list = re.split(\'\\s+\', line)\n                        phone_duration += int((int(temp_list[1]) - int(temp_list[0]))/50000)\n\n                    if self.subphone_feats == ""coarse_coding"":\n                        cc_feat_matrix = self.extract_coarse_coding_features_relative(phone_duration)\n\n            if self.add_frame_features:\n                current_block_binary_array = numpy.zeros((frame_number, self.dict_size+self.frame_feature_size))\n                for i in range(frame_number):\n                    current_block_binary_array[i, 0:self.dict_size] = label_vector\n\n                    if self.subphone_feats == \'full\':\n                        ## Zhizheng\'s original 9 subphone features:\n                        current_block_binary_array[i, self.dict_size] = float(i+1) / float(frame_number)   ## fraction through state (forwards)\n                        current_block_binary_array[i, self.dict_size+1] = float(frame_number - i) / float(frame_number)  ## fraction through state (backwards)\n                        current_block_binary_array[i, self.dict_size+2] = float(frame_number)  ## length of state in frames\n                        current_block_binary_array[i, self.dict_size+3] = float(state_index)   ## state index (counting forwards)\n                        current_block_binary_array[i, self.dict_size+4] = float(state_index_backward) ## state index (counting backwards)\n\n                        current_block_binary_array[i, self.dict_size+5] = float(phone_duration)   ## length of phone in frames\n                        current_block_binary_array[i, self.dict_size+6] = float(frame_number) / float(phone_duration)   ## fraction of the phone made up by current state\n                        current_block_binary_array[i, self.dict_size+7] = float(phone_duration - i - state_duration_base) / float(phone_duration) ## fraction through phone (backwards)\n                        current_block_binary_array[i, self.dict_size+8] = float(state_duration_base + i + 1) / float(phone_duration)  ## fraction through phone (forwards)\n\n                    elif self.subphone_feats == \'state_only\':\n                        ## features which only distinguish state:\n                        current_block_binary_array[i, self.dict_size] = float(state_index)   ## state index (counting forwards)\n\n                    elif self.subphone_feats == \'frame_only\':\n                        ## features which distinguish frame position in phoneme:\n                        current_frame_number += 1\n                        current_block_binary_array[i, self.dict_size] = float(current_frame_number) / float(phone_duration)   ## fraction through phone (counting forwards)\n\n                    elif self.subphone_feats == \'uniform_state\':\n                        ## features which distinguish frame position in phoneme:\n                        current_frame_number += 1\n                        current_block_binary_array[i, self.dict_size] = float(current_frame_number) / float(phone_duration)   ## fraction through phone (counting forwards)\n                        new_state_index = max(1, round(float(current_frame_number)/float(phone_duration)*5))\n                        current_block_binary_array[i, self.dict_size+1] = float(new_state_index)   ## state index (counting forwards)\n\n                    elif self.subphone_feats == ""coarse_coding"":\n                        ## features which distinguish frame position in phoneme using three continous numerical features\n                        current_block_binary_array[i, self.dict_size+0] = cc_feat_matrix[current_frame_number, 0]\n                        current_block_binary_array[i, self.dict_size+1] = cc_feat_matrix[current_frame_number, 1]\n                        current_block_binary_array[i, self.dict_size+2] = cc_feat_matrix[current_frame_number, 2]\n                        current_block_binary_array[i, self.dict_size+3] = float(phone_duration)\n                        current_frame_number += 1\n\n                    elif self.subphone_feats == \'minimal_frame\':\n                        ## features which distinguish state and minimally frame position in state:\n                        current_block_binary_array[i, self.dict_size] = float(i+1) / float(frame_number)   ## fraction through state (forwards)\n                        current_block_binary_array[i, self.dict_size+1] = float(state_index)   ## state index (counting forwards)\n                    elif self.subphone_feats == \'none\':\n                        pass\n                    else:\n                        sys.exit(\'unknown subphone_feats type\')\n\n                label_feature_matrix[label_feature_index:label_feature_index+frame_number,] = current_block_binary_array\n                label_feature_index = label_feature_index + frame_number\n            elif self.subphone_feats == \'state_only\' and state_index == state_number:\n                current_block_binary_array = numpy.zeros((state_number, self.dict_size+self.frame_feature_size))\n                for i in range(state_number):\n                    current_block_binary_array[i, 0:self.dict_size] = label_vector\n                    current_block_binary_array[i, self.dict_size] = float(i+1)   ## state index (counting forwards)\n                label_feature_matrix[label_feature_index:label_feature_index+state_number,] = current_block_binary_array\n                label_feature_index = label_feature_index + state_number\n            elif self.subphone_feats == \'none\' and state_index == state_number:\n                current_block_binary_array = label_vector\n                label_feature_matrix[label_feature_index:label_feature_index+1,] = current_block_binary_array\n                label_feature_index = label_feature_index + 1\n\n            state_duration_base += frame_number\n\n            current_index += 1\n\n        label_feature_matrix = label_feature_matrix[0:label_feature_index,]\n        logger.debug(\'made label matrix of %d frames x %d labels\' % label_feature_matrix.shape )\n        return  label_feature_matrix\n\n    def extract_durational_features(self, dur_file_name=None, dur_data=None):\n\n        if dur_file_name:\n            io_funcs = BinaryIOCollection()\n            dur_dim = 1 ## hard coded for now\n            dur_data = io_funcs.load_binary_file(dur_file_name, dur_dim)\n\n        ph_count = len(dur_data)\n        total_num_of_frames = int(sum(dur_data))\n\n        duration_feature_array = numpy.zeros((total_num_of_frames, self.frame_feature_size))\n\n        frame_index=0\n        for i in range(ph_count):\n            frame_number = int(dur_data[i])\n            if self.subphone_feats == ""coarse_coding"":\n                cc_feat_matrix = self.extract_coarse_coding_features_relative(frame_number)\n\n                for j in range(frame_number):\n                    duration_feature_array[frame_index, 0] = cc_feat_matrix[j, 0]\n                    duration_feature_array[frame_index, 1] = cc_feat_matrix[j, 1]\n                    duration_feature_array[frame_index, 2] = cc_feat_matrix[j, 2]\n                    duration_feature_array[frame_index, 3] = float(frame_number)\n                    frame_index+=1\n\n            elif self.subphone_feats == \'full\':\n                state_number = 5 # hard coded here \n                phone_duration = sum(dur_data[i, :])\n                state_duration_base = 0\n                for state_index in xrange(1, state_number+1):\n                    state_index_backward = (state_number - state_index) + 1\n                    frame_number = int(dur_data[i][state_index-1])\n                    for j in xrange(frame_number):\n                        duration_feature_array[frame_index, 0] = float(j+1) / float(frame_number)   ## fraction through state (forwards)\n                        duration_feature_array[frame_index, 1] = float(frame_number - j) / float(frame_number)  ## fraction through state (backwards)\n                        duration_feature_array[frame_index, 2] = float(frame_number)  ## length of state in frames\n                        duration_feature_array[frame_index, 3] = float(state_index)   ## state index (counting forwards)\n                        duration_feature_array[frame_index, 4] = float(state_index_backward) ## state index (counting backwards)\n    \n                        duration_feature_array[frame_index, 5] = float(phone_duration)   ## length of phone in frames\n                        duration_feature_array[frame_index, 6] = float(frame_number) / float(phone_duration)   ## fraction of the phone made up by current state\n                        duration_feature_array[frame_index, 7] = float(phone_duration - j - state_duration_base) / float(phone_duration) ## fraction through phone (forwards)\n                        duration_feature_array[frame_index, 8] = float(state_duration_base + j + 1) / float(phone_duration)  ## fraction through phone (backwards)\n                        frame_index+=1\n                    \n                    state_duration_base += frame_number\n\n        return duration_feature_array\n\n    def compute_coarse_coding_features(self, num_states):\n        assert num_states == 3\n\n        npoints = 600\n        cc_features = numpy.zeros((num_states, npoints))\n\n        x1 = numpy.linspace(-1.5, 1.5, npoints)\n        x2 = numpy.linspace(-1.0, 2.0, npoints)\n        x3 = numpy.linspace(-0.5, 2.5, npoints)\n\n        mu1 = 0.0\n        mu2 = 0.5\n        mu3 = 1.0\n\n        sigma = 0.4\n\n        cc_features[0, :] = mlab.normpdf(x1, mu1, sigma)\n        cc_features[1, :] = mlab.normpdf(x2, mu2, sigma)\n        cc_features[2, :] = mlab.normpdf(x3, mu3, sigma)\n\n        return cc_features\n\n    def extract_coarse_coding_features_relative(self, phone_duration):\n        dur = int(phone_duration)\n\n        cc_feat_matrix = numpy.zeros((dur, 3))\n\n        for i in range(dur):\n            rel_indx = int((200/float(dur))*i)\n            cc_feat_matrix[i,0] = self.cc_features[0, 300+rel_indx]\n            cc_feat_matrix[i,1] = self.cc_features[1, 200+rel_indx]\n            cc_feat_matrix[i,2] = self.cc_features[2, 100+rel_indx]\n\n        return cc_feat_matrix\n\n    ### this function is not used now\n    def extract_coarse_coding_features_absolute(self, phone_duration):\n        dur = int(phone_duration)\n\n        cc_feat_matrix = numpy.zeros((dur, 3))\n\n        npoints1 = (dur*2)*10+1\n        npoints2 = (dur-1)*10+1\n        npoints3 = (2*dur-1)*10+1\n\n        x1 = numpy.linspace(-dur, dur, npoints1)\n        x2 = numpy.linspace(1, dur, npoints2)\n        x3 = numpy.linspace(1, 2*dur-1, npoints3)\n\n        mu1 = 0\n        mu2 = (1+dur)/2\n        mu3 = dur\n        variance = 1\n        sigma = variance*((dur/10)+2)\n        sigma1 = sigma\n        sigma2 = sigma-1\n        sigma3 = sigma\n\n        y1 = mlab.normpdf(x1, mu1, sigma1)\n        y2 = mlab.normpdf(x2, mu2, sigma2)\n        y3 = mlab.normpdf(x3, mu3, sigma3)\n\n        for i in range(dur):\n            cc_feat_matrix[i,0] = y1[(dur+1+i)*10]\n            cc_feat_matrix[i,1] = y2[i*10]\n            cc_feat_matrix[i,2] = y3[i*10]\n\n        for i in range(3):\n            cc_feat_matrix[:,i] = cc_feat_matrix[:,i]/max(cc_feat_matrix[:,i])\n\n        return cc_feat_matrix\n\n\n    ### this function is not used now\n    def pattern_matching(self, label):\n        # this function is where most time is spent during label preparation\n        #\n        # it might be possible to speed it up by using pre-compiled regular expressions?\n        # (not trying this now, since we may change to to XML tree format for input instead of HTS labels)\n        #\n        label_size = len(label)\n\n        lab_binary_vector = numpy.zeros((1, self.dict_size))\n\n        for i in range(self.dict_size):\n            current_question_list = self.question_dict[str(i)]\n            binary_flag = 0\n            for iq in range(len(current_question_list)):\n                current_question = current_question_list[iq]\n                current_size = len(current_question)\n                if current_question[0] == \'*\' and current_question[current_size-1] == \'*\':\n                    temp_question = current_question[1:current_size-1]\n                    for il in range(1, label_size-current_size+2):\n                        if temp_question == label[il:il+current_size-2]:\n                            binary_flag = 1\n                elif current_question[current_size-1] != \'*\':\n                    temp_question = current_question[1:current_size]\n                    if temp_question == label[label_size-current_size+1:label_size]:\n                        binary_flag = 1\n                elif current_question[0] != \'*\':\n                    temp_question = current_question[0:current_size-1]\n                    if temp_question == label[0:current_size-1]:\n                        binary_flag = 1\n                if binary_flag == 1:\n                    break\n            lab_binary_vector[0, i] = binary_flag\n\n        return  lab_binary_vector\n\n    def pattern_matching_binary(self, label):\n\n        dict_size = len(self.discrete_dict)\n        lab_binary_vector = numpy.zeros((1, dict_size))\n\n        for i in range(dict_size):\n            current_question_list = self.discrete_dict[str(i)]\n            binary_flag = 0\n            for iq in range(len(current_question_list)):\n                current_compiled = current_question_list[iq]\n\n                ms = current_compiled.search(label)\n                if ms is not None:\n                    binary_flag = 1\n                    break\n            lab_binary_vector[0, i] = binary_flag\n\n        return   lab_binary_vector\n\n\n    def pattern_matching_continous_position(self, label):\n\n        dict_size = len(self.continuous_dict)\n\n        lab_continuous_vector = numpy.zeros((1, dict_size))\n\n        for i in range(dict_size):\n            continuous_value = -1.0\n\n            current_compiled = self.continuous_dict[str(i)]\n\n            ms = current_compiled.search(label)\n            if ms is not None:\n#                assert len(ms.group()) == 1\n                continuous_value = ms.group(1)\n\n            lab_continuous_vector[0, i] = continuous_value\n\n        return  lab_continuous_vector\n\n    def load_question_set(self, qs_file_name):\n        fid = open(qs_file_name)\n        question_index = 0\n        question_dict = {}\n        ori_question_dict = {}\n        for line in fid.readlines():\n            line = line.replace(\'\\n\', \'\')\n            if len(line) > 5:\n                temp_list = line.split(\'{\')\n                temp_line = temp_list[1]\n                temp_list = temp_line.split(\'}\')\n                temp_line = temp_list[0]\n                question_list = temp_line.split(\',\')\n                question_dict[str(question_index)] = question_list\n                ori_question_dict[str(question_index)] = line\n                question_index += 1\n        fid.close()\n\n        logger = logging.getLogger(""labels"")\n        logger.debug(\'loaded question set with %d questions\' % len(question_dict))\n\n        return  question_dict, ori_question_dict\n\n\n    def load_question_set_continous(self, qs_file_name):\n\n        logger = logging.getLogger(""labels"")\n\n        fid = open(qs_file_name)\n        binary_qs_index = 0\n        continuous_qs_index = 0\n        binary_dict = {}\n        continuous_dict = {}\n        LL=re.compile(re.escape(\'LL-\'))\n        LAST_QUESTION = re.compile(re.escape(\'(\\d+)\') + \'$\') # regex for last question\n\n        for line in fid.readlines():\n            line = line.replace(\'\\n\', \'\').replace(\'\\t\', \' \')\n\n            if len(line) > 5:\n                temp_list = line.split(\'{\')\n                temp_line = temp_list[1]\n                temp_list = temp_line.split(\'}\')\n                temp_line = temp_list[0]\n                temp_line = temp_line.strip()\n                question_list = temp_line.split(\',\')\n\n                temp_list = line.split(\' \')\n                question_key = temp_list[1]\n#                print   line\n                if temp_list[0] == \'CQS\':\n                    assert len(question_list) == 1\n                    processed_question = self.wildcards2regex(question_list[0], convert_number_pattern=True)\n                    if LAST_QUESTION.search(question_list[0]):\n                        processed_question = processed_question + \'$\' # last question must only match at end of HTS label string\n                    continuous_dict[str(continuous_qs_index)] = re.compile(processed_question) #save pre-compiled regular expression\n                    continuous_qs_index = continuous_qs_index + 1\n                elif temp_list[0] == \'QS\':\n                    re_list = []\n                    for temp_question in question_list:\n                        processed_question = self.wildcards2regex(temp_question)\n                        if LL.search(question_key):\n                            processed_question = \'^\'+processed_question\n                        re_list.append(re.compile(processed_question))\n\n                    binary_dict[str(binary_qs_index)] = re_list\n                    binary_qs_index = binary_qs_index + 1\n                else:\n                    logger.critical(\'The question set is not defined correctly: %s\' %(line))\n                    raise Exception\n\n#                question_index = question_index + 1\n        return  binary_dict, continuous_dict\n\n\n    def wildcards2regex(self, question, convert_number_pattern=False):\n        """"""\n        Convert HTK-style question into regular expression for searching labels.\n        If convert_number_pattern, keep the following sequences unescaped for\n        extracting continuous values):\n            (\\d+)       -- handles digit without decimal point\n            ([\\d\\.]+)   -- handles digits with and without decimal point\n        """"""\n\n        ## handle HTK wildcards (and lack of them) at ends of label:\n        prefix = """"\n        postfix = """"\n        if \'*\' in question:\n            if not question.startswith(\'*\'):\n                prefix = ""\\A""\n            if not question.endswith(\'*\'):\n                postfix = ""\\Z""\n        question = question.strip(\'*\')\n        question = re.escape(question)\n        ## convert remaining HTK wildcards * and ? to equivalent regex:\n        question = question.replace(\'\\\\*\', \'.*\')\n        question = question.replace(\'\\\\?\', \'.\')\n        question = prefix + question + postfix\n\n        if convert_number_pattern:\n            question = question.replace(\'\\\\(\\\\\\\\d\\\\+\\\\)\', \'(\\d+)\')\n            question = question.replace(\'\\\\(\\\\[\\\\\\\\d\\\\\\\\\\\\.\\\\]\\\\+\\\\)\', \'([\\d\\.]+)\')\n        return question\n\n\n\n\n\n\nclass HTSDurationLabelNormalisation(HTSLabelNormalisation):\n    """"""\n    Unlike HTSLabelNormalisation, HTSDurationLabelNormalisation does not accept timings.\n    One line of labels is converted into 1 datapoint, that is, the label is not \'unpacked\'\n    into frames. HTK state index [\\d] is not handled in any special way.\n    """"""\n    def __init__(self, question_file_name=None, subphone_feats=\'full\', continuous_flag=True):\n        super(HTSDurationLabelNormalisation, self).__init__(question_file_name=question_file_name, \\\n                                    subphone_feats=subphone_feats, continuous_flag=continuous_flag)\n        ## don\'t use extra features beyond those in questions for duration labels:\n        self.dimension = self.dict_size\n\n\n    def load_labels_with_state_alignment(self, file_name, add_frame_features=False):\n        ## add_frame_features not used in HTSLabelNormalisation -- only in XML version\n\n        logger = logging.getLogger(""labels"")\n\n        assert self.dimension == self.dict_size\n\n        label_feature_matrix = numpy.empty((100000, self.dimension))\n\n        label_feature_index = 0\n\n\n        lab_binary_vector = numpy.zeros((1, self.dict_size))\n        fid = open(file_name)\n        utt_labels = fid.readlines()\n        fid.close()\n        current_index = 0\n        label_number = len(utt_labels)\n        logger.info(\'loaded %s, %3d labels\' % (file_name, label_number) )\n\n        ## remove empty lines\n        utt_labels = [line for line in utt_labels if line != \'\']\n\n        for (line_number, line) in enumerate(utt_labels):\n            temp_list = re.split(\'\\s+\', line.strip())\n            full_label = temp_list[-1]  ## take last entry -- ignore timings if present\n\n            label_binary_vector = self.pattern_matching_binary(full_label)\n\n            # if there is no CQS question, the label_continuous_vector will become to empty\n            label_continuous_vector = self.pattern_matching_continous_position(full_label)\n            label_vector = numpy.concatenate([label_binary_vector, label_continuous_vector], axis = 1)\n\n            label_feature_matrix[line_number, :] = label_vector[:]\n\n\n        label_feature_matrix = label_feature_matrix[:line_number+1,:]\n        logger.debug(\'made label matrix of %d frames x %d labels\' % label_feature_matrix.shape )\n        return  label_feature_matrix\n\n\n#  -----------------------------\n\n\nif __name__ == \'__main__\':\n\n    qs_file_name = \'/afs/inf.ed.ac.uk/group/cstr/projects/blizzard_entries/blizzard2016/straight_voice/Hybrid_duration_experiments/dnn_tts_release/lstm_rnn/data/questions.hed\'\n\n    print(qs_file_name)\n\n    ori_file_list = [\'/afs/inf.ed.ac.uk/group/cstr/projects/blizzard_entries/blizzard2016/straight_voice/Hybrid_duration_experiments/dnn_tts_release/lstm_rnn/data/label_state_align/AMidsummerNightsDream_000_000.lab\']\n    output_file_list = [\'/afs/inf.ed.ac.uk/group/cstr/projects/blizzard_entries/blizzard2016/straight_voice/Hybrid_duration_experiments/dnn_tts_release/lstm_rnn/data/binary_label_601/AMidsummerNightsDream_000_000.lab\']\n    #output_file_list = [\'/afs/inf.ed.ac.uk/group/cstr/projects/blizzard_entries/blizzard2016/straight_voice/Hybrid_duration_experiments/dnn_tts_release/lstm_rnn/data/dur/AMidsummerNightsDream_000_000.dur\']\n\n    label_operater = HTSLabelNormalisation(qs_file_name)\n    label_operater.perform_normalisation(ori_file_list, output_file_list)\n    #feature_type=""binary""\n    #unit_size = ""phoneme""\n    #feat_size = ""phoneme""\n    #label_operater.prepare_dur_data(ori_file_list, output_file_list, feature_type, unit_size, feat_size)\n    #label_operater.prepare_dur_data(ori_file_list, output_file_list, feature_type)\n    print(label_operater.dimension)\n'"
src/frontend/linguistic_base.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n\nimport logging\nimport sys\nfrom multiprocessing.pool import ThreadPool as Pool\n\n\n## a generic class of linguistic feature extraction\n##\nclass LinguisticBase(object):\n    def __init__(self, dimension=0):\n        self.dimension = dimension  ##the feature dimensionality of output (should that read \'input\' ?)\n\n        ## the number of utterances to be normalised\n        self.utterance_num = 0\n\n    ## the ori_file_list contains the file paths of the raw linguistic data\n    ## the output_file_list contains the file paths of the normalised linguistic data\n    ##\n    def perform_normalisation(self, ori_file_list, output_file_list, label_type=""state_align"", dur_file_list=None):\n\n        logger = logging.getLogger(""perform_normalisation"")\n        logger.info(\'perform linguistic feature extraction\')\n        self.utterance_num = len(ori_file_list)\n        if self.utterance_num != len(output_file_list):\n            logger.error(\'the number of input and output linguistic files should be the same!\\n\')\n            sys.exit(1)\n\n        def _perform_normalisation(i):\n            if not dur_file_list:\n                self.extract_linguistic_features(ori_file_list[i], output_file_list[i], label_type)\n            else:\n                self.extract_linguistic_features(ori_file_list[i], output_file_list[i], label_type, dur_file_list[i])\n\n        pool = Pool()\n        pool.map(_perform_normalisation, range(self.utterance_num))\n        pool.close()\n        pool.join()\n\n    ## the exact function to do the work\n    ## need to be implemented in the specific class\n    ## the function will write the linguistic features directly to the output file\n    def extract_linguistic_features(self, in_file_name, out_file_name, label_type, dur_file_name=None):\n        pass\n'"
src/frontend/mean_variance_norm.py,0,"b""################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n\nfrom io_funcs.binary_io import BinaryIOCollection\nimport  logging\nimport  numpy\n\nfrom .feature_normalisation_base import FeatureNormBase\n\nclass   MeanVarianceNorm(FeatureNormBase):\n    '''\n    plan: 1: support normal MVN and denormalisation for both input and output\n          2: support stream-based operation: for example, some streams can use min-max, other streams use MVN, may need one more class\n    '''\n#    def __init__(self, feature_dimension):\n    def __init__(self, feature_dimension):\n\n        self.mean_vector = None\n        self.std_vector  = None\n        self.feature_dimension = feature_dimension\n\n    def feature_normalisation(self, in_file_list, out_file_list):\n        logger = logging.getLogger('feature_normalisation')\n\n#        self.feature_dimension = feature_dimension\n        try:\n            assert len(in_file_list) == len(out_file_list)\n        except  AssertionError:\n            logger.critical('The input and output file numbers are not the same! %d vs %d' %(len(in_file_list), len(out_file_list)))\n            raise\n\n        if self.mean_vector is None:\n            self.mean_vector = self.compute_mean(in_file_list, 0, self.feature_dimension)\n        if self.std_vector  is None:\n            self.std_vector = self.compute_std(in_file_list, self.mean_vector, 0, self.feature_dimension)\n\n        io_funcs = BinaryIOCollection()\n        file_number = len(in_file_list)\n        for i in range(file_number):\n            features, current_frame_number = io_funcs.load_binary_file_frame(in_file_list[i], self.feature_dimension)\n\n            mean_matrix = numpy.tile(self.mean_vector, (current_frame_number, 1))\n            std_matrix = numpy.tile(self.std_vector, (current_frame_number, 1))\n\n            norm_features = (features - mean_matrix) / std_matrix\n\n            io_funcs.array_to_binary_file(norm_features, out_file_list[i])\n\n        return  self.mean_vector, self.std_vector\n\n    def feature_denormalisation(self, in_file_list, out_file_list, mean_vector, std_vector):\n        io_funcs = BinaryIOCollection()\n        file_number = len(in_file_list)\n        try:\n            assert len(in_file_list) == len(out_file_list)\n        except  AssertionError:\n            logger.critical('The input and output file numbers are not the same! %d vs %d' %(len(in_file_list), len(out_file_list)))\n            raise\n\n        try:\n            assert  mean_vector.size == self.feature_dimension and std_vector.size == self.feature_dimension\n        except AssertionError:\n            logger.critical('the dimensionalities of the mean and standard derivation vectors are not the same as the dimensionality of the feature')\n            raise\n\n        for i in range(file_number):\n            features, current_frame_number = io_funcs.load_binary_file_frame(in_file_list[i], self.feature_dimension)\n\n            mean_matrix = numpy.tile(mean_vector, (current_frame_number, 1))\n            std_matrix = numpy.tile(std_vector, (current_frame_number, 1))\n\n            norm_features = features * std_matrix + mean_matrix\n\n            io_funcs.array_to_binary_file(norm_features, out_file_list[i])\n\n    def load_mean_std_values(self, acoustic_norm_file):\n\n        logger = logging.getLogger('feature_normalisation')\n\n        io_funcs = BinaryIOCollection()\n        mean_std_vector, frame_number = io_funcs.load_binary_file_frame(acoustic_norm_file, 1)\n        mean_std_vector = numpy.reshape(mean_std_vector, (-1, ))\n        self.mean_vector = mean_std_vector[0:frame_number//2]\n        self.std_vector = mean_std_vector[frame_number//2:]\n\n        logger.info('Loaded mean std values from the trained data for feature dimension of %d' % self.feature_dimension)\n        return self.mean_vector, self.std_vector\n\n    def compute_mean(self, file_list, start_index, end_index):\n\n        logger = logging.getLogger('feature_normalisation')\n\n        local_feature_dimension = end_index - start_index\n\n        mean_vector = numpy.zeros((1, local_feature_dimension))\n        all_frame_number = 0\n\n        io_funcs = BinaryIOCollection()\n        for file_name in file_list:\n            features, current_frame_number = io_funcs.load_binary_file_frame(file_name, self.feature_dimension)\n\n            mean_vector += numpy.reshape(numpy.sum(features[:, start_index:end_index], axis=0), (1, local_feature_dimension))\n            all_frame_number += current_frame_number\n\n        mean_vector /= float(all_frame_number)\n\n        # setting the print options in this way seems to break subsequent printing of numpy float32 types\n        # no idea what is going on - removed until this can be solved\n        # po=numpy.get_printoptions()\n        # numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)\n        logger.info('computed mean vector of length %d :' % mean_vector.shape[1] )\n        logger.info(' mean: %s' % mean_vector)\n        # restore the print options\n        # numpy.set_printoptions(po)\n\n        self.mean_vector = mean_vector\n\n        return  mean_vector\n\n    def compute_std(self, file_list, mean_vector, start_index, end_index):\n\n        logger = logging.getLogger('feature_normalisation')\n\n        local_feature_dimension = end_index - start_index\n\n        std_vector = numpy.zeros((1, self.feature_dimension))\n        all_frame_number = 0\n\n        io_funcs = BinaryIOCollection()\n        for file_name in file_list:\n            features, current_frame_number = io_funcs.load_binary_file_frame(file_name, self.feature_dimension)\n\n            mean_matrix = numpy.tile(mean_vector, (current_frame_number, 1))\n\n            std_vector += numpy.reshape(numpy.sum((features[:, start_index:end_index] - mean_matrix) ** 2, axis=0), (1, local_feature_dimension))\n            all_frame_number += current_frame_number\n\n        std_vector /= float(all_frame_number)\n\n        std_vector = std_vector ** 0.5\n\n        # setting the print options in this way seems to break subsequent printing of numpy float32 types\n        # no idea what is going on - removed until this can be solved\n        # po=numpy.get_printoptions()\n        # numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)\n        logger.info('computed  std vector of length %d' % std_vector.shape[1] )\n        logger.info('  std: %s' % std_vector)\n        # restore the print options\n        # numpy.set_printoptions(po)\n\n        self.std_vector = std_vector\n\n        return  std_vector\n"""
src/frontend/merge_features.py,0,"b'\nimport numpy, sys\nfrom io_funcs.binary_io import BinaryIOCollection\n\nimport logging\n\nclass MergeFeat(object):\n\n    def __init__(self, lab_dim = 481, feat_dim = 1):\n\n        self.logger = logging.getLogger(""labels"")\n\n        self.lab_dim = lab_dim\n        self.feat_dim = feat_dim\n\n\n\n    def merge_data(self, binary_label_file_list, new_feat_file_list, out_feat_file_list):\n        \'\'\'\n        merging new features with normalised label features\n        \'\'\'\n        utt_number = len(new_feat_file_list)\n        if utt_number != len(binary_label_file_list):\n            print(""the number of new feature input files and label files should be the same!\\n"");\n            sys.exit(1)\n\n        new_feat_ext   = new_feat_file_list[0].split(\'/\')[-1].split(\'.\')[1]\n\n        io_funcs = BinaryIOCollection()\n        for i in range(utt_number):\n            lab_file_name = binary_label_file_list[i]\n            new_feat_file_name = new_feat_file_list[i]\n            out_feat_file_name = out_feat_file_list[i]\n\n            lab_features, lab_frame_number  = io_funcs.load_binary_file_frame(lab_file_name, self.lab_dim)\n            new_features, feat_frame_number = io_funcs.load_binary_file_frame(new_feat_file_name, self.feat_dim)\n\n\n            if (lab_frame_number - feat_frame_number)>5:\n                base_file_name = new_feat_file_list[i].split(\'/\')[-1].split(\'.\')[0]\n                self.logger.critical(""the number of frames in label and new features are different: %d vs %d (%s)"" %(lab_frame_number, feat_frame_number, base_file_name))\n                raise\n\n            merged_features = numpy.zeros((lab_frame_number, self.lab_dim+self.feat_dim))\n\n            merged_features[0:lab_frame_number, 0:self.lab_dim] = lab_features\n            merged_features[0:feat_frame_number, self.lab_dim:self.lab_dim+self.feat_dim] = new_features[0:lab_frame_number, ]\n\n            io_funcs.array_to_binary_file(merged_features, out_feat_file_name)\n            self.logger.debug(\'merged new feature %s of %d frames with %d label features\' % (new_feat_ext, feat_frame_number,lab_frame_number) )\n'"
src/frontend/min_max_norm.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n\nimport numpy\nfrom io_funcs.binary_io import BinaryIOCollection\nimport logging\n\nclass MinMaxNormalisation(object):\n    def __init__(self, feature_dimension, min_value = 0.01, max_value = 0.99, min_vector = 0.0, max_vector = 0.0, exclude_columns=[]):\n\n        # this is the wrong name for this logger because we can also normalise labels here too\n        logger = logging.getLogger(""acoustic_norm"")\n\n        self.target_min_value = min_value\n        self.target_max_value = max_value\n\n        self.feature_dimension = feature_dimension\n\n        self.min_vector = min_vector\n        self.max_vector = max_vector\n\n        self.exclude_columns = exclude_columns\n\n        if type(min_vector) != float:\n            try:\n                assert( len(self.min_vector) == self.feature_dimension)\n            except AssertionError:\n                logger.critical(\'inconsistent feature_dimension (%d) and length of min_vector (%d)\' % (self.feature_dimension,len(self.min_vector)))\n                raise\n\n        if type(max_vector) != float:\n            try:\n                assert( len(self.max_vector) == self.feature_dimension)\n            except AssertionError:\n                logger.critical(\'inconsistent feature_dimension (%d) and length of max_vector (%d)\' % (self.feature_dimension,len(self.max_vector)))\n                raise\n\n        logger.debug(\'MinMaxNormalisation created for feature dimension of %d\' % self.feature_dimension)\n\n    def load_min_max_values(self, label_norm_file):\n\n        logger = logging.getLogger(""acoustic_norm"")\n\n        io_funcs = BinaryIOCollection()\n        min_max_vector, frame_number = io_funcs.load_binary_file_frame(label_norm_file, 1)\n        min_max_vector = numpy.reshape(min_max_vector, (-1, ))\n        self.min_vector = min_max_vector[0:frame_number//2]\n        self.max_vector = min_max_vector[frame_number//2:]\n\n        logger.info(\'Loaded min max values from the trained data for feature dimension of %d\' % self.feature_dimension)\n\n    def find_min_max_values(self, in_file_list):\n\n        logger = logging.getLogger(""acoustic_norm"")\n\n        file_number = len(in_file_list)\n        min_value_matrix = numpy.zeros((file_number, self.feature_dimension))\n        max_value_matrix = numpy.zeros((file_number, self.feature_dimension))\n        io_funcs = BinaryIOCollection()\n        for i in range(file_number):\n            features = io_funcs.load_binary_file(in_file_list[i], self.feature_dimension)\n\n            temp_min = numpy.amin(features, axis = 0)\n            temp_max = numpy.amax(features, axis = 0)\n\n            min_value_matrix[i, ] = temp_min;\n            max_value_matrix[i, ] = temp_max;\n\n        self.min_vector = numpy.amin(min_value_matrix, axis = 0)\n        self.max_vector = numpy.amax(max_value_matrix, axis = 0)\n        self.min_vector = numpy.reshape(self.min_vector, (1, self.feature_dimension))\n        self.max_vector = numpy.reshape(self.max_vector, (1, self.feature_dimension))\n\n        # po=numpy.get_printoptions()\n        # numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)\n        logger.info(\'across %d files found min/max values of length %d:\' % (file_number,self.feature_dimension) )\n        logger.info(\'  min: %s\' % self.min_vector)\n        logger.info(\'  max: %s\' % self.max_vector)\n        # restore the print options\n        # numpy.set_printoptions(po)\n\n    def normalise_data(self, in_file_list, out_file_list):\n        file_number = len(in_file_list)\n\n        fea_max_min_diff = self.max_vector - self.min_vector\n        diff_value = self.target_max_value - self.target_min_value\n        fea_max_min_diff = numpy.reshape(fea_max_min_diff, (1, self.feature_dimension))\n\n        target_max_min_diff = numpy.zeros((1, self.feature_dimension))\n        target_max_min_diff.fill(diff_value)\n\n        target_max_min_diff[fea_max_min_diff <= 0.0] = 1.0\n        fea_max_min_diff[fea_max_min_diff <= 0.0] = 1.0\n\n        io_funcs = BinaryIOCollection()\n        for i in range(file_number):\n            features = io_funcs.load_binary_file(in_file_list[i], self.feature_dimension)\n\n            frame_number = features.size // self.feature_dimension\n            fea_min_matrix = numpy.tile(self.min_vector, (frame_number, 1))\n            target_min_matrix = numpy.tile(self.target_min_value, (frame_number, self.feature_dimension))\n\n            fea_diff_matrix = numpy.tile(fea_max_min_diff, (frame_number, 1))\n            diff_norm_matrix = numpy.tile(target_max_min_diff, (frame_number, 1)) / fea_diff_matrix\n\n            norm_features = diff_norm_matrix * (features - fea_min_matrix) + target_min_matrix\n\n            ## If we are to keep some columns unnormalised, use advanced indexing to\n            ## reinstate original values:\n            m,n = numpy.shape(features)\n            for col in self.exclude_columns:\n                norm_features[list(range(m)),[col]*m] = features[list(range(m)),[col]*m]\n\n            io_funcs.array_to_binary_file(norm_features, out_file_list[i])\n\n#            norm_features = numpy.array(norm_features, \'float32\')\n#            fid = open(out_file_list[i], \'wb\')\n#            norm_features.tofile(fid)\n#            fid.close()\n\n    def denormalise_data(self, in_file_list, out_file_list):\n\n        logger = logging.getLogger(""acoustic_norm"")\n\n        file_number = len(in_file_list)\n        logger.info(\'MinMaxNormalisation.denormalise_data for %d files\' % file_number)\n\n        # print   self.max_vector, self.min_vector\n        fea_max_min_diff = self.max_vector - self.min_vector\n        diff_value = self.target_max_value - self.target_min_value\n        # logger.debug(\'reshaping fea_max_min_diff from shape %s to (1,%d)\' % (fea_max_min_diff.shape, self.feature_dimension) )\n\n        fea_max_min_diff = numpy.reshape(fea_max_min_diff, (1, self.feature_dimension))\n\n        target_max_min_diff = numpy.zeros((1, self.feature_dimension))\n        target_max_min_diff.fill(diff_value)\n\n        target_max_min_diff[fea_max_min_diff <= 0.0] = 1.0\n        fea_max_min_diff[fea_max_min_diff <= 0.0] = 1.0\n\n        io_funcs = BinaryIOCollection()\n        for i in range(file_number):\n            features = io_funcs.load_binary_file(in_file_list[i], self.feature_dimension)\n\n            frame_number = features.size // self.feature_dimension\n            fea_min_matrix = numpy.tile(self.min_vector, (frame_number, 1))\n            target_min_matrix = numpy.tile(self.target_min_value, (frame_number, self.feature_dimension))\n\n            fea_diff_matrix = numpy.tile(fea_max_min_diff, (frame_number, 1))\n            diff_norm_matrix = fea_diff_matrix / numpy.tile(target_max_min_diff, (frame_number, 1))\n            norm_features = diff_norm_matrix * (features - target_min_matrix) + fea_min_matrix\n            io_funcs.array_to_binary_file(norm_features, out_file_list[i])\n\n    def normal_standardization(self, in_file_list, out_file_list):\n        mean_vector = self.compute_mean(in_file_list)\n        std_vector = self.compute_std(in_file_list, mean_vector)\n\n        io_funcs = BinaryIOCollection()\n        file_number = len(in_file_list)\n        for i in range(file_number):\n            features = io_funcs.load_binary_file(in_file_list[i], self.feature_dimension)\n            current_frame_number = features.size // self.feature_dimension\n\n            mean_matrix = numpy.tile(mean_vector, (current_frame_number, 1))\n            std_matrix = numpy.tile(std_vector, (current_frame_number, 1))\n\n            norm_features = (features - mean_matrix) / std_matrix\n\n            io_funcs.array_to_binary_file(norm_features, out_file_list[i])\n\n    def compute_mean(self, file_list):\n\n        logger = logging.getLogger(""acoustic_norm"")\n\n        mean_vector = numpy.zeros((1, self.feature_dimension))\n        all_frame_number = 0\n\n        io_funcs = BinaryIOCollection()\n        for file_name in file_list:\n            features = io_funcs.load_binary_file(file_name, self.feature_dimension)\n            current_frame_number = features.size // self.feature_dimension\n            mean_vector += numpy.reshape(numpy.sum(features, axis=0), (1, self.feature_dimension))\n            all_frame_number += current_frame_number\n\n        mean_vector /= float(all_frame_number)\n\n        # po=numpy.get_printoptions()\n        # numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)\n        logger.info(\'computed mean vector of length %d :\' % mean_vector.shape[1] )\n        logger.info(\' mean: %s\' % mean_vector)\n        # restore the print options\n        # numpy.set_printoptions(po)\n\n        return  mean_vector\n\n    def compute_std(self, file_list, mean_vector):\n\n        logger = logging.getLogger(""acoustic_norm"")\n\n        std_vector = numpy.zeros((1, self.feature_dimension))\n        all_frame_number = 0\n\n        io_funcs = BinaryIOCollection()\n        for file_name in file_list:\n            features = io_funcs.load_binary_file(file_name, self.feature_dimension)\n            current_frame_number = features.size // self.feature_dimension\n            mean_matrix = numpy.tile(mean_vector, (current_frame_number, 1))\n\n            std_vector += numpy.reshape(numpy.sum((features - mean_matrix) ** 2, axis=0), (1, self.feature_dimension))\n            all_frame_number += current_frame_number\n\n        std_vector /= float(all_frame_number)\n\n        std_vector = std_vector ** 0.5\n\n        # po=numpy.get_printoptions()\n        # numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)\n        logger.info(\'computed  std vector of length %d\' % std_vector.shape[1] )\n        logger.info(\'  std: %s\' % std_vector)\n        # restore the print options\n        # numpy.set_printoptions(po)\n\n        return  std_vector\n\n\nif __name__ == \'__main__\':\n\n    in_file_list = [\'/group/project/dnn_tts/data/nick/sp/nick/herald_001.sp\']\n    out_file_list = [\'/group/project/dnn_tts/herald_001.sp\']\n    out_file_list1 = [\'/group/project/dnn_tts/herald_001.test.sp\']\n\n    feature_dimension = 1025\n\n    normaliser = MinMaxNormalisation(feature_dimension, min_value = 0.01, max_value = 0.99)\n    normaliser.find_min_max_values(in_file_list)\n    tmp_min_vector = normaliser.min_vector\n    tmp_max_vector = normaliser.max_vector\n    normaliser.normalise_data(in_file_list, out_file_list)\n\n    denormaliser = MinMaxNormalisation(feature_dimension, min_value = 0.01, max_value = 0.99, min_vector = tmp_min_vector, max_vector = tmp_max_vector)\n    denormaliser.denormalise_data(out_file_list, out_file_list1)\n'"
src/frontend/mlpg.py,0,"b""################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n\n## use theano to benefit from GPU computation\nfrom theano import tensor as T\nimport  theano\n\nimport numpy\nfrom numpy import dot\nimport logging\n\n\nclass MLParameterGeneration(object):\n    def __init__(self, delta_win = [-0.5, 0.0, 0.5], acc_win = [1.0, -2.0, 1.0]):\n        self.delta_win = delta_win\n        self.acc_win   = acc_win\n        ###assume the delta and acc windows have the same length\n        self.win_length = int(len(delta_win)/2)\n\n    def build_theano_function_wdw(self):\n\n        W_static = T.matrix('W_static')\n        W_delta  = T.matrix('W_delta')\n        W_acc    = T.matrix('W_acc')\n        D_static = T.matrix('D_static')\n        D_delta  = T.matrix('D_delta')\n        D_acc    = T.matrix('D_acc')\n\n        WDW = T.dot(T.dot(W_static.T, D_static), W_static) + T.dot(T.dot(W_delta.T, D_delta), W_delta) + T.dot(T.dot(W_acc.T, D_acc), W_acc)\n\n        fn = theano.function(inputs=[W_static, W_delta, W_acc, D_static, D_delta, D_acc], outputs=WDW)\n\n        return  fn\n\n    def build_theano_function_wdu(self):\n\n        W_static = T.matrix('W_static')\n        W_delta  = T.matrix('W_delta')\n        W_acc    = T.matrix('W_acc')\n        D_static = T.matrix('D_static')\n        D_delta  = T.matrix('D_delta')\n        D_acc    = T.matrix('D_acc')\n        U_static = T.matrix('U_static')\n        U_delta  = T.matrix('U_delta')\n        U_acc    = T.matrix('U_acc')\n\n        WDU = T.dot(T.dot(W_static.T, D_static), U_static) + T.dot(T.dot(W_delta.T, D_delta), U_delta) + T.dot(T.dot(W_acc.T, D_acc), U_acc)\n\n        fn = theano.function(inputs=[W_static, W_delta, W_acc, D_static, D_delta, D_acc, U_static, U_delta, U_acc], outputs=WDU)\n\n        return  fn\n\n    def generation(self, features, covariance, static_dimension):\n        '''\n        plan: use theano to do the parameter generation to benefit from GPU\n        '''\n\n        logger = logging.getLogger('param_generation')\n        logger.debug('starting MLParameterGeneration.generation')\n\n        frame_number = features.shape[0]\n\n        gen_parameter = numpy.zeros((frame_number, static_dimension))\n\n        W_static, W_delta, W_acc = self.prepare_window(frame_number)\n\n        WT_static = numpy.transpose(W_static)\n        WT_delta  = numpy.transpose(W_delta)\n        WT_acc    = numpy.transpose(W_acc)\n\n        fn_wdw = self.build_theano_function_wdw()\n        fn_wdu = self.build_theano_function_wdu()\n\n        for d in range(static_dimension):\n            logger.debug('static dimension %3d of %3d' % (d+1,static_dimension) )\n\n            D_static = self.prepare_D(frame_number, covariance[d, 0])\n            D_delta  = self.prepare_D(frame_number, covariance[static_dimension + d, 0])\n            D_acc    = self.prepare_D(frame_number, covariance[2*static_dimension + d, 0])\n\n            U_static = self.prepare_U(frame_number, features[:, d:d+1])\n            U_delta  = self.prepare_U(frame_number, features[:, static_dimension + d:static_dimension + d + 1])\n            U_acc    = self.prepare_U(frame_number, features[:, 2*static_dimension + d:2*static_dimension + d + 1])\n\n#            WDW = dot(dot(WT_static, D_static), W_static) + dot(dot(WT_delta, D_delta), W_delta) + dot(dot(WT_acc, D_acc), W_acc)\n#            WDU = dot(dot(WT_static, D_static), U_static) + dot(dot(WT_delta, D_delta), U_delta) + dot(dot(WT_acc, D_acc), U_acc)\n#            temp_obs = dot(numpy.linalg.inv(WDW), WDU)\n\n\n            WDW = fn_wdw(W_static, W_delta, W_acc, D_static, D_delta, D_acc)\n            WDU = fn_wdu(W_static, W_delta, W_acc, D_static, D_delta, D_acc, U_static, U_delta, U_acc)\n            ###only theano-dev version support matrix inversion\n            temp_obs = dot(numpy.linalg.inv(WDW), WDU)\n\n            gen_parameter[0:frame_number, d] = temp_obs[self.win_length:frame_number+self.win_length, 0]\n\n        return  gen_parameter\n\n\n    def prepare_window(self, frame_number):\n        win_length = self.win_length\n\n        w_static = numpy.zeros((frame_number+win_length*2, frame_number+win_length*2), dtype=theano.config.floatX)\n        w_delta  = numpy.zeros((frame_number+win_length*2, frame_number+win_length*2), dtype=theano.config.floatX)\n        w_acc    = numpy.zeros((frame_number+win_length*2, frame_number+win_length*2), dtype=theano.config.floatX)\n\n        for i in range(frame_number+win_length*2):\n            w_static[i, i] = 1.0\n            w_delta[i, i]  = self.delta_win[win_length]\n            w_acc[i, i]    = self.acc_win[win_length]\n\n            for j in range(win_length):\n                if i - j > 0:\n                    w_delta[i, i-j-1] = self.delta_win[win_length-j-1]\n                    w_acc[i, i-j-1]   = self.acc_win[win_length-j-1]\n\n                if i + j + 1 < frame_number+win_length*2:\n                    w_delta[i, i+j+1] = self.delta_win[win_length+j+1]\n                    w_acc[i, i+j+1]   = self.acc_win[win_length+j+1]\n\n        return  w_static, w_delta, w_acc\n\n    def prepare_D(self, frame_number, D_value):\n        win_length = self.win_length\n        D_matrix = numpy.zeros((frame_number+win_length*2, frame_number+win_length*2), dtype=theano.config.floatX)\n\n        for i in range(win_length):\n            D_matrix[i, i] = 1.0\n            D_matrix[frame_number+win_length+i, frame_number+win_length+i] = 1.0\n\n        for i in range(frame_number):\n            D_matrix[win_length+i, win_length+i] = 1.0 / D_value\n\n        return  D_matrix\n\n    def prepare_U(self, frame_number, U_vector):\n\n        win_length = self.win_length\n\n        U_expanded = numpy.zeros((frame_number+win_length*2, 1), dtype=theano.config.floatX)\n\n        U_expanded[win_length:frame_number+win_length, :] = U_vector\n\n        return  U_expanded\n"""
src/frontend/mlpg_fast.py,0,"b""################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n\n\nimport numpy as np\nfrom numpy import dot\nimport logging\nfrom numpy import float64\n\n\n# Adding this before the bandmat import lets us import .pyx files without running bandmat's setup.py:\n#import pyximport; pyximport.install()\n\n\n\nimport bandmat as bm\nimport bandmat.linalg as bla\n\nclass MLParameterGenerationFast(object):\n    def __init__(self, delta_win = [-0.5, 0.0, 0.5], acc_win = [1.0, -2.0, 1.0]):\n        self.delta_win = delta_win\n        self.acc_win   = acc_win\n        ###assume the delta and acc windows have the same length\n        self.win_length = int(len(delta_win)/2)\n\n    def build_win_mats(self, windows, frames):\n        win_mats = []\n        for l, u, win_coeff in windows:\n            assert l >= 0 and u >= 0\n            assert len(win_coeff) == l + u + 1\n            win_coeffs = np.tile(np.reshape(win_coeff, (l + u + 1, 1)), frames)\n            win_mat = bm.band_c_bm(u, l, win_coeffs).T\n            win_mats.append(win_mat)\n\n        return win_mats\n\n    def build_poe(self, b_frames, tau_frames, win_mats, sdw=None):\n#        tau_frames.astype('float64')\n\n        if sdw is None:\n            sdw = max([ win_mat.l + win_mat.u for win_mat in win_mats ])\n        num_windows = len(win_mats)\n        frames = len(b_frames)\n        assert np.shape(b_frames) == (frames, num_windows)\n        assert np.shape(tau_frames) == (frames, num_windows)\n        assert all([ win_mat.l + win_mat.u <= sdw for win_mat in win_mats ])\n\n        b = np.zeros((frames,))\n        prec = bm.zeros(sdw, sdw, frames)\n\n        for win_index, win_mat in enumerate(win_mats):\n            bm.dot_mv_plus_equals(win_mat.T, b_frames[:, win_index], target=b)\n            bm.dot_mm_plus_equals(win_mat.T, win_mat, target_bm=prec,\n                                  diag=float64(tau_frames[:, win_index]))\n\n        return b, prec\n\n    def generation(self, features, covariance, static_dimension):\n\n        windows = [\n            (0, 0, np.array([1.0])),\n            (1, 1, np.array([-0.5, 0.0, 0.5])),\n            (1, 1, np.array([1.0, -2.0, 1.0])),\n        ]\n        num_windows = len(windows)\n\n        frame_number = features.shape[0]\n\n        logger = logging.getLogger('param_generation')\n        logger.debug('starting MLParameterGeneration.generation')\n\n        gen_parameter = np.zeros((frame_number, static_dimension))\n\n        win_mats = self.build_win_mats(windows, frame_number)\n        mu_frames = np.zeros((frame_number, 3))\n        var_frames = np.zeros((frame_number, 3))\n\n        for d in range(static_dimension):\n            var_frames[:, 0] = covariance[:, d]\n            var_frames[:, 1] = covariance[:, static_dimension+d]\n            var_frames[:, 2] = covariance[:, static_dimension*2+d]\n            mu_frames[:, 0] = features[:, d]\n            mu_frames[:, 1] = features[:, static_dimension+d]\n            mu_frames[:, 2] = features[:, static_dimension*2+d]\n            var_frames[0, 1] = 100000000000;\n            var_frames[0, 2] = 100000000000;\n            var_frames[frame_number-1, 1] = 100000000000;\n            var_frames[frame_number-1, 2] = 100000000000;\n\n            b_frames = mu_frames / var_frames\n            tau_frames = 1.0 / var_frames\n\n            b, prec = self.build_poe(b_frames, tau_frames, win_mats)\n            mean_traj = bla.solveh(prec, b)\n\n            gen_parameter[0:frame_number, d] = mean_traj\n\n        return  gen_parameter\n"""
src/frontend/parameter_generation.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n## Added FAST_MLPG as a variable here, in case someone wants to use the slow version, but perhaps we\n## should always use the bandmat version?\nFAST_MLPG = True\n#io_funcs.\n\nfrom io_funcs.binary_io import  BinaryIOCollection\nimport os, re, numpy\nimport logging\n\nif FAST_MLPG:\n    from .mlpg_fast import MLParameterGenerationFast as MLParameterGeneration\n#    pass\nelse:\n    from .mlpg import MLParameterGeneration\n\nclass   ParameterGeneration(object):\n\n    def __init__(self, gen_wav_features = [\'mgc\', \'lf0\', \'bap\'], enforce_silence=False):\n        self.gen_wav_features = gen_wav_features\n        self.enforce_silence  = enforce_silence\n\n        # Debug:\n        self.inf_float = -1.0e+10\n        #self.inf_float = -50000\n\n        # not really necessary to have the logger rembered in the class - can easily obtain it by name instead\n        # self.logger = logging.getLogger(\'param_generation\')\n\n        self.var = {}\n\n    def duration_decomposition(self, in_file_list, dimension, out_dimension_dict, file_extension_dict):\n\n        logger = logging.getLogger(\'param_generation\')\n\n        logger.debug(\'duration_decomposition for %d files\' % len(in_file_list) )\n\n        state_number = 5  ## hard coding, try removing in future?\n\n        if len(list(out_dimension_dict.keys()))>1:\n            logger.critical(""we don\'t support any additional features along with duration as of now."")\n            sys.exit(1)\n        else:\n            feature_name = list(out_dimension_dict.keys())[0]\n\n        io_funcs = BinaryIOCollection()\n\n        findex=0\n        flen=len(in_file_list)\n        for file_name in in_file_list:\n\n            findex=findex+1\n\n            dir_name = os.path.dirname(file_name)\n            file_id = os.path.splitext(os.path.basename(file_name))[0]\n\n            features, frame_number = io_funcs.load_binary_file_frame(file_name, dimension)\n            gen_features = numpy.int32(numpy.round(features))\n            gen_features[gen_features<1]=1\n\n            if dimension > state_number:\n                gen_features = gen_features[:, state_number]\n\n            logger.info(\'processing %4d of %4d: %s\' % (findex,flen,file_name) )\n\n            new_file_name = os.path.join(dir_name, file_id + file_extension_dict[feature_name])\n            io_funcs.array_to_binary_file(gen_features, new_file_name)\n\n            logger.debug(\'wrote to file %s\' % new_file_name)\n\n    def acoustic_decomposition(self, in_file_list, dimension, out_dimension_dict, file_extension_dict, var_file_dict, do_MLPG=True, cfg=None):\n\n        logger = logging.getLogger(\'param_generation\')\n\n        logger.debug(\'acoustic_decomposition for %d files\' % len(in_file_list) )\n\n        self.load_covariance(var_file_dict, out_dimension_dict)\n\n        stream_start_index = {}\n        dimension_index = 0\n        recorded_vuv = False\n        vuv_dimension = None\n\n        for feature_name in list(out_dimension_dict.keys()):\n#            if feature_name != \'vuv\':\n            stream_start_index[feature_name] = dimension_index\n#            else:\n#                vuv_dimension = dimension_index\n#                recorded_vuv = True\n\n            dimension_index += out_dimension_dict[feature_name]\n\n        io_funcs = BinaryIOCollection()\n\n        mlpg_algo = MLParameterGeneration()\n\n        findex=0\n        flen=len(in_file_list)\n        for file_name in in_file_list:\n\n            findex=findex+1\n\n            dir_name = os.path.dirname(file_name)\n            file_id = os.path.splitext(os.path.basename(file_name))[0]\n\n            features, frame_number = io_funcs.load_binary_file_frame(file_name, dimension)\n\n            logger.info(\'processing %4d of %4d: %s\' % (findex,flen,file_name) )\n\n            for feature_name in self.gen_wav_features:\n\n                logger.debug(\' feature: %s\' % feature_name)\n\n                current_features = features[:, stream_start_index[feature_name]:stream_start_index[feature_name]+out_dimension_dict[feature_name]]\n                if FAST_MLPG:\n                    ### fast version wants variance per frame, not single global one:\n                    var = self.var[feature_name]\n                    var = numpy.transpose(numpy.tile(var,frame_number))\n                else:\n                    var = self.var[feature_name]\n\n#                print  var.shape[1]\n                if do_MLPG == False:\n                    gen_features = current_features\n                else:\n                    gen_features = mlpg_algo.generation(current_features, var, out_dimension_dict[feature_name]//3)\n#                else:\n#                    self.logger.critical(""the dimensions do not match for MLPG: %d vs %d"" %(var.shape[1], out_dimension_dict[feature_name]))\n#                    raise\n\n                logger.debug(\' feature dimensions: %d by %d\' %(gen_features.shape[0], gen_features.shape[1]))\n\n                if feature_name in [\'lf0\', \'F0\']:\n                    if \'vuv\' in stream_start_index:\n                        vuv_feature = features[:, stream_start_index[\'vuv\']:stream_start_index[\'vuv\']+1]\n\n                        for i in range(frame_number):\n                            if vuv_feature[i, 0] < 0.5 or gen_features[i, 0] < numpy.log(20):\n                                gen_features[i, 0] = self.inf_float\n\n                new_file_name = os.path.join(dir_name, file_id + file_extension_dict[feature_name])\n\n                if self.enforce_silence:\n                    silence_pattern = cfg.silence_pattern\n                    label_align_dir = cfg.in_label_align_dir\n                    in_f = open(label_align_dir+\'/\'+file_id+\'.lab\',\'r\')\n                    for line in in_f.readlines():\n                        line = line.strip()\n\n                        if len(line) < 1:\n                            continue\n                        temp_list  = re.split(\'\\s+\', line)\n                        start_time = int(int(temp_list[0])*(10**-4)/5)\n                        end_time   = int(int(temp_list[1])*(10**-4)/5)\n\n                        full_label = temp_list[2]\n\n                        label_binary_flag = self.check_silence_pattern(full_label, silence_pattern)\n\n                        if label_binary_flag:\n                            if feature_name in [\'lf0\', \'F0\', \'mag\']:\n                                gen_features[start_time:end_time, :] = self.inf_float\n                            else:\n                                gen_features[start_time:end_time, :] = 0.0\n\n                io_funcs.array_to_binary_file(gen_features, new_file_name)\n                logger.debug(\' wrote to file %s\' % new_file_name)\n\n\n    def load_covariance(self, var_file_dict, out_dimension_dict):\n\n        io_funcs = BinaryIOCollection()\n        for feature_name in list(var_file_dict.keys()):\n            var_values, dimension = io_funcs.load_binary_file_frame(var_file_dict[feature_name], 1)\n\n            var_values = numpy.reshape(var_values, (out_dimension_dict[feature_name], 1))\n\n            self.var[feature_name] = var_values\n\n\n    def check_silence_pattern(self, label, silence_pattern):\n        for current_pattern in silence_pattern:\n            current_pattern = current_pattern.strip(\'*\')\n            if current_pattern in label:\n                return 1\n        return 0\n\n\n\n\nif __name__ == \'__main__\':\n\n    in_file_list = [\'/afs/inf.ed.ac.uk/group/project/dnn_tts/mtl_dnn/gen/dnn_2500_601_229/hvd_678.cmp\']\n\n    out_dimension_dict = { \'mgc\' : 150,\n                           \'lf0\' : 3,\n                           \'vuv\' : 1,\n                           \'bap\' : 75}\n\n    file_extension_dict = {\'mgc\' : \'.mgc\',\n                           \'lf0\' : \'.lf0\',\n                           \'vuv\' : \'.vuv\',\n                           \'bap\' : \'.bap\'}\n\n    var_file_dict  = { \'mgc\' : \'/afs/inf.ed.ac.uk/group/project/dnn_tts/mtl_dnn/data/var/mgc\',\n                       \'lf0\' : \'/afs/inf.ed.ac.uk/group/project/dnn_tts/mtl_dnn/data/var/lf0\',\n                       \'bap\' : \'/afs/inf.ed.ac.uk/group/project/dnn_tts/mtl_dnn/data/var/bap\'}\n\n    generator = ParameterGeneration()\n\n    generator.acoustic_decomposition(in_file_list, 229, out_dimension_dict, file_extension_dict, var_file_dict)\n'"
src/frontend/silence_remover.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n\nimport sys, numpy, re, math\nfrom io_funcs.binary_io import BinaryIOCollection\nfrom multiprocessing.dummy import Pool as ThreadPool\n\n\nclass SilenceRemover(object):\n    def __init__(self, n_cmp, silence_pattern=[\'*-#+*\'], label_type=""state_align"", remove_frame_features=True,\n                 subphone_feats=""none""):\n        self.silence_pattern = silence_pattern\n        self.silence_pattern_size = len(silence_pattern)\n        self.label_type = label_type\n        self.remove_frame_features = remove_frame_features\n        self.subphone_feats = subphone_feats\n        self.n_cmp = n_cmp\n\n    def remove_silence(self, in_data_list, in_align_list, out_data_list, dur_file_list=None):\n        file_number = len(in_data_list)\n        align_file_number = len(in_align_list)\n\n        if file_number != align_file_number:\n            print(""The number of input and output files does not equal!\\n"")\n            sys.exit(1)\n        if file_number != len(out_data_list):\n            print(""The number of input and output files does not equal!\\n"")\n            sys.exit(1)\n\n        io_funcs = BinaryIOCollection()\n\n        def _remove_silence(i):\n            if self.label_type == ""phone_align"":\n                if dur_file_list:\n                    dur_file_name = dur_file_list[i]\n                else:\n                    dur_file_name = None\n                nonsilence_indices = self.load_phone_alignment(in_align_list[i], dur_file_name)\n            else:\n                nonsilence_indices = self.load_alignment(in_align_list[i])\n\n            ori_cmp_data = io_funcs.load_binary_file(in_data_list[i], self.n_cmp)\n\n            frame_number = ori_cmp_data.size / self.n_cmp\n\n            if len(nonsilence_indices) == frame_number:\n                print(\'WARNING: no silence found!\')\n                # previsouly: continue -- in fact we should keep non-silent data!\n\n            ## if labels have a few extra frames than audio, this can break the indexing, remove them:\n            nonsilence_indices = [ix for ix in nonsilence_indices if ix < frame_number]\n\n            new_cmp_data = ori_cmp_data[nonsilence_indices,]\n\n            io_funcs.array_to_binary_file(new_cmp_data, out_data_list[i])\n\n        pool = ThreadPool()\n        pool.map(_remove_silence, range(file_number))\n        pool.close()\n        pool.join()\n\n    ## OSW: rewrote above more succintly\n    def check_silence_pattern(self, label):\n        for current_pattern in self.silence_pattern:\n            current_pattern = current_pattern.strip(\'*\')\n            if current_pattern in label:\n                return 1\n        return 0\n\n    def load_phone_alignment(self, alignment_file_name, dur_file_name=None):\n\n        if dur_file_name:\n            io_funcs = BinaryIOCollection()\n            dur_dim = 1  ## hard coded for now\n            manual_dur_data = io_funcs.load_binary_file(dur_file_name, dur_dim)\n\n        ph_count = 0\n        base_frame_index = 0\n        nonsilence_frame_index_list = []\n        fid = open(alignment_file_name)\n        for line in fid.readlines():\n            line = line.strip()\n            if len(line) < 1:\n                continue\n            temp_list = re.split(\'\\s+\', line)\n\n            if len(temp_list) == 1:\n                full_label = temp_list[0]\n            else:\n                start_time = int(temp_list[0])\n                end_time = int(temp_list[1])\n                full_label = temp_list[2]\n\n                # to do - support different frame shift - currently hardwired to 5msec\n                # currently under beta testing: supports different frame shift\n                if dur_file_name:\n                    frame_number = manual_dur_data[ph_count]\n                    ph_count = ph_count + 1\n                else:\n                    frame_number = int((end_time - start_time) / 50000)\n\n            label_binary_flag = self.check_silence_pattern(full_label)\n\n            if self.remove_frame_features:\n                if label_binary_flag == 0:\n                    for frame_index in range(frame_number):\n                        nonsilence_frame_index_list.append(base_frame_index + frame_index)\n                base_frame_index = base_frame_index + frame_number\n            elif self.subphone_feats == \'none\':\n                if label_binary_flag == 0:\n                    nonsilence_frame_index_list.append(base_frame_index)\n                base_frame_index = base_frame_index + 1\n\n        fid.close()\n\n        return nonsilence_frame_index_list\n\n    def load_alignment(self, alignment_file_name, dur_file_name=None):\n\n        state_number = 5\n        base_frame_index = 0\n        nonsilence_frame_index_list = []\n        fid = open(alignment_file_name)\n        for line in fid.readlines():\n            line = line.strip()\n            if len(line) < 1:\n                continue\n            temp_list = re.split(\'\\s+\', line)\n            if len(temp_list) == 1:\n                state_index = state_number\n                full_label = temp_list[0]\n            else:\n                start_time = int(temp_list[0])\n                end_time = int(temp_list[1])\n                full_label = temp_list[2]\n                full_label_length = len(full_label) - 3  # remove state information [k]\n                state_index = full_label[full_label_length + 1]\n                state_index = int(state_index) - 1\n                frame_number = int((end_time - start_time) / 50000)\n\n            label_binary_flag = self.check_silence_pattern(full_label)\n\n            if self.remove_frame_features:\n                if label_binary_flag == 0:\n                    for frame_index in range(frame_number):\n                        nonsilence_frame_index_list.append(base_frame_index + frame_index)\n                base_frame_index = base_frame_index + frame_number\n            elif self.subphone_feats == \'state_only\':\n                if label_binary_flag == 0:\n                    nonsilence_frame_index_list.append(base_frame_index)\n                base_frame_index = base_frame_index + 1\n            elif self.subphone_feats == \'none\' and state_index == state_number:\n                if label_binary_flag == 0:\n                    nonsilence_frame_index_list.append(base_frame_index)\n                base_frame_index = base_frame_index + 1\n\n        fid.close()\n\n        return nonsilence_frame_index_list\n\n\n# def load_binary_file(self, file_name, dimension):\n\n#        fid_lab = open(file_name, \'rb\')\n#        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n#        fid_lab.close()\n#        features = features[:(dimension * (features.size / dimension))]\n#        features = features.reshape((-1, dimension))\n\n#        return  features\n\n\ndef trim_silence(in_list, out_list, in_dimension, label_list, label_dimension, \\\n                 silence_feature_index, percent_to_keep=0):\n    \'\'\'\n    Function to trim silence from binary label/speech files based on binary labels.\n        in_list: list of binary label/speech files to trim\n        out_list: trimmed files\n        in_dimension: dimension of data to trim\n        label_list: list of binary labels which contain trimming criterion\n        label_dimesion:\n        silence_feature_index: index of feature in labels which is silence: 1 means silence (trim), 0 means leave.\n    \'\'\'\n    assert len(in_list) == len(out_list) == len(label_list)\n    io_funcs = BinaryIOCollection()\n    for (infile, outfile, label_file) in zip(in_list, out_list, label_list):\n\n        data = io_funcs.load_binary_file(infile, in_dimension)\n        label = io_funcs.load_binary_file(label_file, label_dimension)\n\n        audio_label_difference = data.shape[0] - label.shape[0]\n        assert math.fabs(audio_label_difference) < 3, \'%s and %s contain different numbers of frames: %s %s\' % (\n            infile, label_file, data.shape[0], label.shape[0])\n\n        ## In case they are different, resize -- keep label fixed as we assume this has\n        ## already been processed. (This problem only arose with STRAIGHT features.)\n        if audio_label_difference < 0:  ## label is longer -- pad audio to match by repeating last frame:\n            print(\'audio too short -- pad\')\n            padding = numpy.vstack([data[-1, :]] * int(math.fabs(audio_label_difference)))\n            data = numpy.vstack([data, padding])\n        elif audio_label_difference > 0:  ## audio is longer -- cut it\n            print(\'audio too long -- trim\')\n            new_length = label.shape[0]\n            data = data[:new_length, :]\n        # else: -- expected case -- lengths match, so do nothing\n\n        silence_flag = label[:, silence_feature_index]\n        #         print silence_flag\n        if not (numpy.unique(silence_flag) == numpy.array([0, 1])).all():\n            ## if it\'s all 0s or 1s, that\'s ok:\n            assert (numpy.unique(silence_flag) == numpy.array([0]).all()) or \\\n                   (numpy.unique(silence_flag) == numpy.array([1]).all()), \\\n                \'dimension %s of %s contains values other than 0 and 1\' % (silence_feature_index, infile)\n        print(\'Remove %d%% of frames (%s frames) as silence... \' % (\n            100 * numpy.sum(silence_flag / float(len(silence_flag))), int(numpy.sum(silence_flag))))\n        non_silence_indices = numpy.nonzero(\n            silence_flag == 0)  ## get the indices where silence_flag == 0 is True (i.e. != 0)\n        if percent_to_keep != 0:\n            assert type(percent_to_keep) == int and percent_to_keep > 0\n            # print silence_flag\n            silence_indices = numpy.nonzero(silence_flag == 1)\n            ## nonzero returns a tuple of arrays, one for each dimension of input array\n            silence_indices = silence_indices[0]\n            every_nth = 100 / percent_to_keep\n            silence_indices_to_keep = silence_indices[::every_nth]  ## every_nth used +as step value in slice\n            ## -1 due to weird error with STRAIGHT features at line 144:\n            ## IndexError: index 445 is out of bounds for axis 0 with size 445\n            if len(silence_indices_to_keep) == 0:\n                silence_indices_to_keep = numpy.array([1])  ## avoid errors in case there is no silence\n            print(\'   Restore %s%% (every %sth frame: %s frames) of silent frames\' % (\n                percent_to_keep, every_nth, len(silence_indices_to_keep)))\n\n            ## Append to end of utt -- same function used for labels and audio\n            ## means that violation of temporal order doesn\'t matter -- will be consistent.\n            ## Later, frame shuffling will disperse silent frames evenly across minibatches:\n            non_silence_indices = (numpy.hstack([non_silence_indices[0], silence_indices_to_keep]))\n            ##  ^---- from tuple and back (see nonzero note above)\n\n        trimmed_data = data[non_silence_indices, :]  ## advanced integer indexing\n        io_funcs.array_to_binary_file(trimmed_data, outfile)\n\n\nif __name__ == \'__main__\':\n    cmp_file_list_name = \'\'\n    lab_file_list_name = \'\'\n    align_file_list_name = \'\'\n\n    n_cmp = 229\n    n_lab = 898\n\n    in_cmp_list = [\'/group/project/dnn_tts/data/nick/nn_cmp/nick/herald_001.cmp\']\n    in_lab_list = [\'/group/project/dnn_tts/data/nick/nn_new_lab/herald_001.lab\']\n    in_align_list = [\'/group/project/dnn_tts/data/cassia/nick_lab/herald_001.lab\']\n\n    out_cmp_list = [\'/group/project/dnn_tts/data/nick/nn_new_lab/herald_001.tmp.cmp\']\n    out_lab_list = [\'/group/project/dnn_tts/data/nick/nn_new_lab/herald_001.tmp.no.lab\']\n\n    remover = SilenceRemover(in_cmp_list, in_align_list, n_cmp, out_cmp_list)\n    remover.remove_silence()\n'"
src/io_funcs/__init__.py,0,b''
src/io_funcs/binary_io.py,0,"b""################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n\n\nimport numpy\n\nclass   BinaryIOCollection(object):\n\n    def load_binary_file(self, file_name, dimension):\n        fid_lab = open(file_name, 'rb')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        assert features.size % float(dimension) == 0.0,'specified dimension %s not compatible with data'%(dimension)\n        features = features[:(dimension * (features.size // dimension))]\n        features = features.reshape((-1, dimension))\n\n        return  features\n\n    def array_to_binary_file(self, data, output_file_name):\n        data = numpy.array(data, 'float32')\n\n        fid = open(output_file_name, 'wb')\n        data.tofile(fid)\n        fid.close()\n\n    def load_binary_file_frame(self, file_name, dimension):\n        fid_lab = open(file_name, 'rb')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        assert features.size % float(dimension) == 0.0,'specified dimension %s not compatible with data'%(dimension)\n        frame_number = features.size // dimension\n        features = features[:(dimension * frame_number)]\n        features = features.reshape((-1, dimension))\n\n        return  features, frame_number\n"""
src/io_funcs/htk_io.py,0,"b'\'\'\'\nCopyright 2011-2013 Pawel Swietojanski\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nTHIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\nWARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\nMERCHANTABLITY OR NON-INFRINGEMENT.\nSee the Apache 2 License for the specific language governing permissions and\nlimitations under the License.\n\nNot fully implemented [28 OCT 2011]\nTODO: support for options: _C, H_IREFC\n\n\'\'\'\n\nimport io, os, sys, numpy, struct, logging\n\nclass HTK_Parm_IO(object):\n    \'\'\'\n    For details look at the HTK book, Chapter 5.10 Storage of Parameter Files\n    \'\'\'\n\n    # HTK datatybes\n    H_WAVEFORM  = 0\n    H_LPC       = 1\n    H_LPREFC    = 2\n    H_LPCEPSTRA = 3\n    H_LPDELCEP  = 4\n    H_IREFC     = 5\n    H_MFCC      = 6\n    H_FBANK     = 7\n    H_MELSPEC   = 8\n    H_USER      = 9\n    H_DISCRETE  = 10\n    H_PLP       = 11\n    H_ANON      = 12\n\n    # Additional \'param kind\' options\n    _E = 0x0001 #has energy\n    _N = 0x0002 #absolute energy suppressed\n    _D = 0x0004 #has delta coefficients\n    _A = 0x0008 #has acceleration coefficients\n    _C = 0x0010 #is compressed\n    _Z = 0x0020 #has zero mean static coef.\n    _K = 0x0040 #has CRC checksum\n    _O = 0x0080 #has 0th cepstral coef.\n    _V = 0x0100 #has VQ data\n    _T = 0x0200 #has third differential coef.\n\n    MASK_H_DATATYPE = 0x003f # the first 6 bits contain datatype\n\n    def __init__(self, n_samples=0, samp_period=0, samp_size=0, param_kind = 0, data=None):\n        \'\'\'\n        \'\'\'\n\n        # HTK header\n        self.n_samples = n_samples # number of samples in file (4-byte integer)\n        self.samp_period = samp_period # sample period in 100ns units (4-byte integer)\n        self.samp_size = samp_size   # number of bytes per sample (2-byte integer)\n        self.param_kind = param_kind  # a code indicating the sample kind (2-byte integer)\n\n        self.data = data\n\n        return None\n\n    def htk_datatype(self):\n        return (self.param_kind & self.MASK_H_DATATYPE)\n\n\n    def set_htk_datatype(self, value):\n        self.param_kind = value | ~self.MASK_H_DATATYPE\n\n\n    def htk_datatype_has_option(self, option):\n        """"""Return True/False if the given options are set\n\n        :type option: int\n        :param option: one of the _E _N _D etc. flags\n\n        """"""\n        return (((self.param_kind>>6) & option)>0)\n\n\n    def set_htk_datatype_option(self, value):\n        self.param_kind = (value<<6) | self.param_kind\n\n    def read_htk(self, filename, reshape_to_matrix=True):\n        \'\'\'\n        \'\'\'\n        try:\n\n            f = open(filename, \'rb\')\n\n            self.n_samples = struct.unpack(\'<I\', f.read(4))[0]\n            self.samp_period = struct.unpack(\'<I\', f.read(4))[0]\n            self.samp_size = struct.unpack(\'<H\', f.read(2))[0]\n            self.param_kind = struct.unpack(\'<H\', f.read(2))[0]\n\n            if (self.htk_datatype_has_option(self._C)):\n                #TODO compression\n                #self.A = struct.unpack(\'>H\', f.read(2))[0]\n                #self.B = struct.unpack(\'>H\', f.read(2))[0]\n                raise Exception(""Compressed files not supported yet!"")\n\n            if (self.htk_datatype() == self.H_WAVEFORM):\n                self.data = numpy.fromfile(f, numpy.int16)\n            else:\n                self.data = numpy.fromfile(f, numpy.float32)\n#                print   ""world""\n                if reshape_to_matrix:\n                    self.data = self.data.reshape( (self.n_samples, -1) )\n\n#            if(sys.byteorder==\'little\'):\n#                print   ""hello""\n#                self.data.byteswap(True) # forces big-endian byte ordering\n\n            f.close()\n        except IOError as e:\n            logging.error(e)\n            raise Exception(e)\n\n        return None\n\n    def write_htk(self, filename):\n        \'\'\'\n        \'\'\'\n        try:\n\n            file = open(filename, \'wb\')\n\n            file.write(struct.pack(\'<I\', self.n_samples))\n            file.write(struct.pack(\'<I\', self.samp_period))\n            file.write(struct.pack(\'<H\', self.samp_size))\n            file.write(struct.pack(\'<H\', self.param_kind))\n\n            #if(sys.byteorder==\'little\'):\n            #    self.data.byteswap(True) # force big-endian byte ordering\n\n            self.data.tofile(file)\n\n        except IOError as e:\n            raise Exception(e)\n\n        return None\n\n    def print_info(self):\n\n        print(""Samples number: "", self.n_samples)\n        print(""Sample period: [100ns]"", self.samp_period)\n        print(""Bytes/sample:"", self.samp_size)\n        print(""ParamKind - datatype: "", self.htk_datatype())\n        print(""ParamKind - options: _E(%i), _D(%i), A(%i)"", self.htk_datatype_has_option(self._E), self.htk_datatype_has_option(self._D), self.htk_datatype_has_option(self._A))\n        print(""Features matrix shape"", self.data.shape)\n        print(""Features"", self.data)\n\n        return None\n\n    def get_data_size(self):\n        return self.data.size*self.data.itemsize\n\ndef test_HTK_Parm_IO():\n\n    #filename_src = ""../data/GE001_1.feat""\n    filename_src = ""../data/tr1.mfc""\n    filename_dst = ""../data/tr1_dst.mfc""\n\n    htk = HTK_Parm_IO()\n\n    try:\n        print(\'SOURCE FILE : \')\n        htk.read_htk(filename_src)\n        htk.print_info()\n        #print ""t"", htk.dupa, sys.byteorder\n\n        htk.writeHTK(filename_dst)\n\n        print(\'TARGET FILE : \')\n        htk2 = HTK_Parm_IO()\n        htk2.read_htk(filename_dst)\n        htk2.print_info()\n\n    except Exception as e:\n        print(e)\n\n    return None\n\n\nif __name__ == ""__main__"":\n    test_HTK_Parm_IO()\n'"
src/keras_lib/__init__.py,0,b''
src/keras_lib/configuration.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://github.com/CSTR-Edinburgh/merlin\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\nimport sys\nif sys.version_info.major >= 3:\n    import configparser\nelse:\n    import ConfigParser as configparser\nimport logging\nimport os\n\nclass configuration(object):\n\n    def __init__(self):\n        pass;\n\n    def configure(self, configFile=None):\n\n        # get a logger\n        logger = logging.getLogger(""configuration"")\n        # this (and only this) logger needs to be configured immediately, otherwise it won\'t work\n        # we can\'t use the full user-supplied configuration mechanism in this particular case,\n        # because we haven\'t loaded it yet!\n        #\n        # so, just use simple console-only logging\n        logger.setLevel(logging.DEBUG) # this level is hardwired here - should change it to INFO\n        # add a handler & its formatter - will write only to console\n        ch = logging.StreamHandler()\n        logger.addHandler(ch)\n        formatter = logging.Formatter(\'%(asctime)s %(levelname)8s%(name)15s: %(message)s\')\n        ch.setFormatter(formatter)\n\n        # first, set up some default configuration values\n        self.initial_configuration()\n\n        # next, load in any user-supplied configuration values\n        # that might over-ride the default values\n        self.user_configuration(configFile)\n\n        # finally, set up all remaining configuration values\n        # that depend upon either default or user-supplied values\n        self.complete_configuration()\n\n        logger.debug(\'configuration completed\')\n\n    def initial_configuration(self):\n\n        # to be called before loading any user specific values\n\n        # things to put here are\n        # 1. variables that the user cannot change\n        # 2. variables that need to be set before loading the user\'s config file\n\n        UTTID_REGEX = \'(.*)\\..*\'\n\n    def user_configuration(self,configFile=None):\n\n        # get a logger\n        logger = logging.getLogger(""configuration"")\n\n        # load and parse the provided configFile, if provided\n        if not configFile:\n            logger.warn(\'no user configuration file provided; using only built-in default settings\')\n            return\n\n        # load the config file\n        try:\n            cfgparser = configparser.ConfigParser()\n            cfgparser.readfp(open(configFile))\n            logger.debug(\'successfully read and parsed user configuration file %s\' % configFile)\n        except:\n            logger.fatal(\'error reading user configuration file %s\' % configFile)\n            raise\n\n        #work_dir must be provided before initialising other directories\n        self.work_dir = None\n\n        if self.work_dir == None:\n            try:\n                self.work_dir = cfgparser.get(\'Paths\', \'work\')\n\n            except (configparser.NoSectionError, configparser.NoOptionError):\n                if self.work_dir == None:\n                    logger.critical(\'Paths:work has no value!\')\n                    raise Exception\n\n        # default place for some data\n        self.data_dir    = os.path.join(self.work_dir, \'data\')\n        self.keras_dir   = os.path.join(self.work_dir, \'keras\')\n\n        self.gen_dir     = os.path.join(self.keras_dir, \'gen\')\n        self.model_dir   = os.path.join(self.keras_dir, \'models\')\n        self.stats_dir   = os.path.join(self.keras_dir, \'stats\')\n\n        self.inter_data_dir = os.path.join(self.work_dir, \'inter_module\')\n        self.def_inp_dir    = os.path.join(self.inter_data_dir, \'nn_no_silence_lab_norm_425\')\n        self.def_out_dir    = os.path.join(self.inter_data_dir, \'nn_norm_mgc_lf0_vuv_bap_187\')\n\n        impossible_int=int(-99999)\n        impossible_float=float(-99999.0)\n\n        user_options = [\n\n            # Paths\n            (\'work_dir\', self.work_dir, \'Paths\',\'work\'),\n            (\'data_dir\', self.data_dir, \'Paths\',\'data\'),\n\n            (\'inp_feat_dir\', self.def_inp_dir, \'Paths\', \'inp_feat\'),\n            (\'out_feat_dir\', self.def_out_dir, \'Paths\', \'out_feat\'),\n\n            (\'model_dir\', self.model_dir, \'Paths\', \'models\'),\n            (\'stats_dir\', self.stats_dir, \'Paths\', \'stats\'),\n            (\'gen_dir\'  ,   self.gen_dir, \'Paths\', \'gen\'),\n\n            (\'file_id_scp\', os.path.join(self.data_dir, \'file_id_list.scp\'), \'Paths\', \'file_id_list\'),\n            (\'test_id_scp\', os.path.join(self.data_dir, \'test_id_list.scp\'), \'Paths\', \'test_id_list\'),\n\n            # Input-Output\n            (\'inp_dim\', 425, \'Input-Output\', \'inp_dim\'),\n            (\'out_dim\', 187, \'Input-Output\', \'out_dim\'),\n\n            (\'inp_file_ext\', \'.lab\', \'Input-Output\', \'inp_file_ext\'),\n            (\'out_file_ext\', \'.cmp\', \'Input-Output\', \'out_file_ext\'),\n\n            (\'inp_norm\', \'MINMAX\', \'Input-Output\', \'inp_norm\'),\n            (\'out_norm\', \'MINMAX\', \'Input-Output\', \'out_norm\'),\n\n            # Architecture\n            (\'hidden_layer_type\', [\'TANH\', \'TANH\', \'TANH\', \'TANH\', \'TANH\', \'TANH\'], \'Architecture\', \'hidden_layer_type\'),\n            (\'hidden_layer_size\', [ 1024 ,  1024 ,  1024 ,  1024 ,  1024 ,   1024], \'Architecture\', \'hidden_layer_size\'),\n\n            (\'batch_size\'   , 256, \'Architecture\', \'batch_size\'),\n            (\'num_of_epochs\',   1, \'Architecture\', \'training_epochs\'),\n            (\'dropout_rate\' , 0.0, \'Architecture\', \'dropout_rate\'),\n\n            (\'output_layer_type\', \'linear\', \'Architecture\', \'output_layer_type\'),\n            (\'optimizer\'        ,   \'adam\', \'Architecture\', \'optimizer\'),\n            (\'loss_function\'    ,    \'mse\', \'Architecture\', \'loss_function\'),\n\n            # RNN\n            (\'sequential_training\', False, \'Architecture\', \'sequential_training\'),\n            (\'stateful\'           , False, \'Architecture\', \'stateful\'),\n            (\'use_high_batch_size\', False, \'Architecture\', \'use_high_batch_size\'),\n\n            (\'training_algo\',   1, \'Architecture\', \'training_algo\'),\n            (\'merge_size\'   ,   1, \'Architecture\', \'merge_size\'),\n            (\'seq_length\'   , 200, \'Architecture\', \'seq_length\'),\n            (\'bucket_range\' , 100, \'Architecture\', \'bucket_range\'),\n\n            # Data\n            (\'shuffle_data\', True, \'Data\', \'shuffle_data\'),\n\n            (\'train_file_number\', impossible_int, \'Data\',\'train_file_number\'),\n            (\'valid_file_number\', impossible_int, \'Data\',\'valid_file_number\'),\n            (\'test_file_number\' , impossible_int, \'Data\',\'test_file_number\'),\n\n            # Processes\n            (\'GenTestList\', False, \'Processes\', \'GenTestList\'),\n\n            (\'NORMDATA\'   , False, \'Processes\', \'NORMDATA\'),\n            (\'TRAINMODEL\' , False, \'Processes\', \'TRAINMODEL\'),\n            (\'TESTMODEL\'  , False, \'Processes\', \'TESTMODEL\')\n\n        ]\n\n        # this uses exec(...) which is potentially dangerous since arbitrary code could be executed\n        for (variable,default,section,option) in user_options:\n            # default value\n            value=None\n\n            try:\n                # first, look for a user-set value for this variable in the config file\n                value = cfgparser.get(section,option)\n                user_or_default=\'user\'\n\n            except (configparser.NoSectionError, configparser.NoOptionError):\n                # use default value, if there is one\n                if (default == None) or \\\n                   (default == \'\')   or \\\n                   ((type(default) == int) and (default == impossible_int)) or \\\n                   ((type(default) == float) and (default == impossible_float))  :\n                    logger.critical(\'%20s has no value!\' % (section+"":""+option) )\n                    raise Exception\n                else:\n                    value = default\n                    user_or_default=\'default\'\n\n\n            if type(default) == str:\n                exec(\'self.%s = ""%s""\'      % (variable,value))\n            elif type(default) == int:\n                exec(\'self.%s = int(%s)\'   % (variable,value))\n            elif type(default) == float:\n                exec(\'self.%s = float(%s)\' % (variable,value))\n            elif type(default) == bool:\n                exec(\'self.%s = bool(%s)\'  % (variable,value))\n            elif type(default) == list:\n                exec(\'self.%s = list(%s)\'  % (variable,value))\n            elif type(default) == dict:\n                exec(\'self.%s = dict(%s)\'  % (variable,value))\n            else:\n                logger.critical(\'Variable %s has default value of unsupported type %s\',variable,type(default))\n                raise Exception(\'Internal error in configuration settings: unsupported default type\')\n\n            logger.info(\'%20s has %7s value %s\' % (section+"":""+option,user_or_default,value) )\n\n\n    def complete_configuration(self):\n        # to be called after reading any user-specific settings\n        # because the values set here depend on those user-specific settings\n\n        # get a logger\n        logger = logging.getLogger(""configuration"")\n\n        ## create directories if not exists\n        if not os.path.exists(self.model_dir):\n            os.makedirs(self.model_dir)\n\n        if not os.path.exists(self.stats_dir):\n            os.makedirs(self.stats_dir)\n\n        if not os.path.exists(self.gen_dir):\n            os.makedirs(self.gen_dir)\n\n        # input-output normalization stat files\n        self.inp_stats_file = os.path.join(self.stats_dir, ""input_%d_%s_%d.norm"" %(int(self.train_file_number), self.inp_norm, self.inp_dim))\n        self.out_stats_file = os.path.join(self.stats_dir, ""output_%d_%s_%d.norm"" %(int(self.train_file_number), self.out_norm, self.out_dim))\n\n        # define model file name\n        if self.sequential_training:\n            self.combined_model_arch = \'RNN\'+str(self.training_algo)\n        else:\n            self.combined_model_arch = \'DNN\'\n\n        self.combined_model_arch += \'_\'+str(len(self.hidden_layer_size))\n        self.combined_model_arch += \'_\'+\'_\'.join(map(str, self.hidden_layer_size))\n        self.combined_model_arch += \'_\'+\'_\'.join(map(str, self.hidden_layer_type))\n\n        self.nnets_file_name = \'%s_%d_train_%d_%d_%d_%d_%d_model\' \\\n                          %(self.combined_model_arch, int(self.shuffle_data),\n                             self.inp_dim, self.out_dim, self.train_file_number, self.batch_size, self.num_of_epochs)\n\n        logger.info(\'model file: %s\' % (self.nnets_file_name))\n\n        # model files\n        self.json_model_file = os.path.join(self.model_dir, self.nnets_file_name+\'.json\')\n        self.h5_model_file   = os.path.join(self.model_dir, self.nnets_file_name+\'.h5\')\n\n        # predicted features directory\n        self.pred_feat_dir = os.path.join(self.gen_dir, self.nnets_file_name)\n        if not os.path.exists(self.pred_feat_dir):\n            os.makedirs(self.pred_feat_dir)\n\n        # string.lower for some architecture values\n        self.output_layer_type = self.output_layer_type.lower()\n        self.optimizer         = self.optimizer.lower()\n        self.loss_function     = self.loss_function.lower()\n        for i in range(len(self.hidden_layer_type)):\n            self.hidden_layer_type[i] = self.hidden_layer_type[i].lower()\n\n        # set sequential training True if using LSTMs\n        if \'lstm\' in self.hidden_layer_type:\n            self.sequential_training = True\n\n        # set/limit batch size to 25\n        if self.sequential_training and self.batch_size>50:\n            if not self.use_high_batch_size:\n                logger.info(\'reducing the batch size from %s to 25\' % (self.batch_size))\n                self.batch_size = 25 ## num. of sentences in this case\n\n        # rnn params\n        self.rnn_params = {}\n        self.rnn_params[\'merge_size\']   = self.merge_size\n        self.rnn_params[\'seq_length\']   = self.seq_length\n        self.rnn_params[\'bucket_range\'] = self.bucket_range\n        self.rnn_params[\'stateful\']     = self.stateful\n'"
src/keras_lib/data_utils.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://github.com/CSTR-Edinburgh/merlin\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\nimport os, sys\nimport time\nimport random\nimport numpy as np\n\nfrom sklearn import preprocessing\n\nfrom io_funcs.binary_io import BinaryIOCollection\n\n############################\n##### Memory variables #####\n############################\n\nUTT_BUFFER_SIZE   =   10000\nFRAME_BUFFER_SIZE = 3000000\n\n\ndef read_data_from_file_list(inp_file_list, out_file_list, inp_dim, out_dim, sequential_training=True):\n    io_funcs = BinaryIOCollection()\n\n    num_of_utt = len(inp_file_list)\n\n    file_length_dict = {\'framenum2utt\':{}, \'utt2framenum\':{}}\n\n    if sequential_training:\n        temp_set_x = {}\n        temp_set_y = {}\n    else:\n        temp_set_x = np.empty((FRAME_BUFFER_SIZE, inp_dim))\n        temp_set_y = np.empty((FRAME_BUFFER_SIZE, out_dim))\n\n    ### read file by file ###\n    current_index = 0\n    for i in range(num_of_utt):\n        inp_file_name = inp_file_list[i]\n        out_file_name = out_file_list[i]\n        inp_features, inp_frame_number = io_funcs.load_binary_file_frame(inp_file_name, inp_dim)\n        out_features, out_frame_number = io_funcs.load_binary_file_frame(out_file_name, out_dim)\n\n        base_file_name = os.path.basename(inp_file_name).split(""."")[0]\n\n        if abs(inp_frame_number-out_frame_number)>5:\n            print(\'the number of frames in input and output features are different: %d vs %d (%s)\' %(inp_frame_number, out_frame_number, base_file_name))\n            sys.exit(0)\n        else:\n            frame_number = min(inp_frame_number, out_frame_number)\n\n        if sequential_training:\n            temp_set_x[base_file_name] = inp_features[0:frame_number]\n            temp_set_y[base_file_name] = out_features[0:frame_number]\n        else:\n            temp_set_x[current_index:current_index+frame_number, ] = inp_features[0:frame_number]\n            temp_set_y[current_index:current_index+frame_number, ] = out_features[0:frame_number]\n            current_index += frame_number\n\n        if frame_number not in file_length_dict[\'framenum2utt\']:\n            file_length_dict[\'framenum2utt\'][frame_number] = [base_file_name]\n        else:\n            file_length_dict[\'framenum2utt\'][frame_number].append(base_file_name)\n\n        file_length_dict[\'utt2framenum\'][base_file_name] = frame_number\n\n        drawProgressBar(i+1, num_of_utt)\n\n    sys.stdout.write(""\\n"")\n\n    if not sequential_training:\n        temp_set_x = temp_set_x[0:current_index, ]\n        temp_set_y = temp_set_y[0:current_index, ]\n\n    return temp_set_x, temp_set_y, file_length_dict\n\ndef read_test_data_from_file_list(inp_file_list, inp_dim, sequential_training=True):\n    io_funcs = BinaryIOCollection()\n\n    num_of_utt = len(inp_file_list)\n\n    file_length_dict = {\'framenum2utt\':{}, \'utt2framenum\':{}}\n\n    if sequential_training:\n        temp_set_x = {}\n    else:\n        temp_set_x = np.empty((FRAME_BUFFER_SIZE, inp_dim))\n\n    ### read file by file ###\n    current_index = 0\n    for i in range(num_of_utt):\n        inp_file_name = inp_file_list[i]\n        inp_features, frame_number = io_funcs.load_binary_file_frame(inp_file_name, inp_dim)\n\n        base_file_name = os.path.basename(inp_file_name).split(""."")[0]\n\n        if sequential_training:\n            temp_set_x[base_file_name] = inp_features\n        else:\n            temp_set_x[current_index:current_index+frame_number, ] = inp_features[0:frame_number]\n            current_index += frame_number\n\n        if frame_number not in file_length_dict[\'framenum2utt\']:\n            file_length_dict[\'framenum2utt\'][frame_number] = [base_file_name]\n        else:\n            file_length_dict[\'framenum2utt\'][frame_number].append(base_file_name)\n\n        file_length_dict[\'utt2framenum\'][base_file_name] = frame_number\n\n        drawProgressBar(i+1, num_of_utt)\n\n    sys.stdout.write(""\\n"")\n\n    if not sequential_training:\n        temp_set_x = temp_set_x[0:current_index, ]\n\n    return temp_set_x, file_length_dict\n\ndef transform_data_to_3d_matrix(data, seq_length=200, max_length=0, merge_size=1, shuffle_data = True, shuffle_type = 1, padding=""right""):\n    num_of_utt = len(data)\n    feat_dim   = data[list(data.keys())[0]].shape[1]\n\n    if max_length > 0:\n        temp_set = np.zeros((num_of_utt, max_length, feat_dim))\n\n        ### read file by file ###\n        current_index = 0\n        for base_file_name, in_features in data.items():\n            frame_number = min(in_features.shape[0], max_length)\n            if padding==""right"":\n                temp_set[current_index, 0:frame_number, ] = in_features\n            else:\n                temp_set[current_index, -frame_number:, ] = in_features\n            current_index += 1\n\n    else:\n        temp_set = np.zeros((FRAME_BUFFER_SIZE, feat_dim))\n\n        train_idx_list = list(data.keys())\n        train_idx_list.sort()\n\n        if shuffle_data:\n            if shuffle_type == 1:\n                train_idx_list = shuffle_file_list(train_idx_list)\n            elif shuffle_type == 2:\n                train_idx_list = shuffle_file_list(train_idx_list, shuffle_type=2, merge_size=merge_size)\n\n        ### read file by file ###\n        current_index = 0\n        for file_number in range(num_of_utt):\n            base_file_name = train_idx_list[file_number]\n            in_features    = data[base_file_name]\n            frame_number   = in_features.shape[0]\n\n            temp_set[current_index:current_index+frame_number, ] = in_features\n            current_index += frame_number\n\n            if (file_number+1)%merge_size == 0:\n                current_index = seq_length * (int(np.ceil(float(current_index)/float(seq_length))))\n\n\n        num_of_samples = int(np.ceil(float(current_index)/float(seq_length)))\n\n        temp_set = temp_set[0: num_of_samples*seq_length, ]\n        temp_set = temp_set.reshape(-1, seq_length, feat_dim)\n\n    return temp_set\n\ndef read_and_transform_data_from_file_list(in_file_list, dim, seq_length=200, merge_size=1):\n    io_funcs = BinaryIOCollection()\n\n    num_of_utt = len(in_file_list)\n\n    temp_set = np.zeros((FRAME_BUFFER_SIZE, dim))\n\n    ### read file by file ###\n    current_index = 0\n    for i in range(num_of_utt):\n        in_file_name = in_file_list[i]\n        in_features, frame_number = io_funcs.load_binary_file_frame(in_file_name, dim)\n        base_file_name            = os.path.basename(in_file_name).split(""."")[0]\n\n        temp_set[current_index:current_index+frame_number, ] = in_features\n        current_index += frame_number\n\n        if (i+1)%merge_size == 0:\n            current_index = seq_length * (int(np.ceil(float(current_index)/float(seq_length))))\n\n        drawProgressBar(i+1, num_of_utt)\n\n    sys.stdout.write(""\\n"")\n\n    num_of_samples = int(np.ceil(float(current_index)/float(seq_length)))\n\n    temp_set = temp_set[0: num_of_samples*seq_length, ]\n    temp_set = temp_set.reshape(num_of_samples, seq_length)\n\n    return temp_set\n\ndef merge_data(train_x, train_y, merge_size):\n    temp_train_x = {}\n    temp_train_y = {}\n\n    train_id_list     = list(train_x.keys())\n    train_file_number = len(train_id_list)\n    train_id_list.sort()\n\n    inp_dim = train_x[train_id_list[0]].shape[1]\n    out_dim = train_y[train_id_list[0]].shape[1]\n\n    merged_features_x = np.zeros((0, inp_dim))\n    merged_features_y = np.zeros((0, out_dim))\n    new_file_count = 0\n    for file_index in range(1, train_file_number+1):\n        inp_features      = train_x[train_id_list[file_index-1]]\n        out_features      = train_y[train_id_list[file_index-1]]\n        merged_features_x = np.vstack((merged_features_x, inp_features))\n        merged_features_y = np.vstack((merged_features_y, out_features))\n\n        if file_index % merge_size == 0 or file_index==train_file_number:\n            base_file_name = ""new_utterance_%04d"" % (new_file_count)\n            temp_train_x[base_file_name] = merged_features_x\n            temp_train_y[base_file_name] = merged_features_y\n            new_file_count += 1\n            merged_features_x = np.zeros((0, inp_dim))\n            merged_features_y = np.zeros((0, out_dim))\n\n    return temp_train_x, temp_train_y\n\ndef shuffle_file_list(train_idx_list, shuffle_type=1, merge_size=5):\n    ### shuffle train id list ###\n    random.seed(271638)\n    train_file_number = len(train_idx_list)\n\n    if shuffle_type==1:  ## shuffle by sentence\n        random.shuffle(train_idx_list)\n        return train_idx_list\n\n    elif shuffle_type==2:  ## shuffle by a group of sentences\n        id_numbers = list(range(0, train_file_number, merge_size))\n        random.shuffle(id_numbers)\n        new_train_idx_list = []\n        for i in range(len(id_numbers)):\n            new_train_idx_list += train_idx_list[id_numbers[i]:id_numbers[i]+merge_size]\n        return new_train_idx_list\n\ndef get_stateful_data(train_x, train_y, batch_size):\n    num_of_batches = int(train_x.shape[0]/batch_size)\n    train_x   = train_x[0: num_of_batches*batch_size, ]\n    train_y   = train_y[0: num_of_batches*batch_size, ]\n\n    stateful_seq = np.zeros(num_of_batches*batch_size, dtype=""int32"")\n    for i in range(num_of_batches):\n        stateful_seq[i*batch_size:(i+1)*batch_size] = np.array(list(range(batch_size)))*num_of_batches+i\n\n    temp_train_x   = train_x[stateful_seq]\n    temp_train_y   = train_y[stateful_seq]\n\n    return temp_train_x, temp_train_y\n\ndef get_stateful_input(test_x, seq_length, batch_size=1):\n    [n_frames, n_dim] = test_x.shape\n\n    num_of_samples = batch_size*seq_length\n    num_of_batches = int(n_frames/num_of_samples) + 1\n    new_data_size  = num_of_batches*num_of_samples\n\n    temp_test_x = np.zeros((new_data_size, n_dim))\n    temp_test_x[0: n_frames, ] = test_x\n\n    temp_test_x = temp_test_x.reshape(-1, seq_length, n_dim)\n\n    return temp_test_x\n\ndef compute_norm_stats(data, stats_file, method=""MVN""):\n    #### normalize training data ####\n    io_funcs = BinaryIOCollection()\n\n    if method==""MVN"":\n        scaler = preprocessing.StandardScaler().fit(data)\n        norm_matrix = np.vstack((scaler.mean_, scaler.scale_))\n    elif method==""MINMAX"":\n        scaler = preprocessing.MinMaxScaler(feature_range=(0.01, 0.99)).fit(data)\n        norm_matrix = np.vstack((scaler.min_, scaler.scale_))\n\n    print(norm_matrix.shape)\n    io_funcs.array_to_binary_file(norm_matrix, stats_file)\n\n    return scaler\n\ndef load_norm_stats(stats_file, dim, method=""MVN""):\n    #### load norm stats ####\n    io_funcs = BinaryIOCollection()\n\n    norm_matrix, frame_number = io_funcs.load_binary_file_frame(stats_file, dim)\n    assert frame_number==2\n\n    if method==""MVN"":\n        scaler = preprocessing.StandardScaler()\n        scaler.mean_  = norm_matrix[0, :]\n        scaler.scale_ = norm_matrix[1, :]\n    elif method==""MINMAX"":\n        scaler = preprocessing.MinMaxScaler(feature_range=(0.01, 0.99))\n        scaler.min_   = norm_matrix[0, :]\n        scaler.scale_ = norm_matrix[1, :]\n\n    return scaler\n\ndef norm_data(data, scaler, sequential_training=True):\n    if scaler is None:\n        return;\n\n    #### normalize data ####\n    if not sequential_training:\n        data = scaler.transform(data)\n    else:\n        for filename, features in data.items():\n            data[filename] = scaler.transform(features)\n\ndef denorm_data(data, scaler):\n    if scaler is None:\n        return;\n\n    #### de-normalize data ####\n    data = scaler.inverse_transform(data)\n\ndef prepare_file_path_list(file_id_list, file_dir, file_extension, new_dir_switch=True):\n    if not os.path.exists(file_dir) and new_dir_switch:\n        os.makedirs(file_dir)\n    file_name_list = []\n    for file_id in file_id_list:\n        file_name = file_dir + \'/\' + file_id + file_extension\n        file_name_list.append(file_name)\n\n    return  file_name_list\n\ndef read_file_list(file_name):\n    file_lists = []\n    fid = open(file_name)\n    for line in fid.readlines():\n        line = line.strip()\n        if len(line) < 1:\n            continue\n        file_lists.append(line)\n    fid.close()\n\n    return  file_lists\n\ndef print_status(i, length):\n    pr = int(float(i)/float(length)*100)\n    st = int(float(pr)/7)\n    sys.stdout.write((""\\r%d/%d "")%(i,length)+(""[ %d""%pr+""% ] <<< "")+(\'=\'*st)+(\'\'*(100-st)))\n    sys.stdout.flush()\n\ndef drawProgressBar(indx, length, barLen = 20):\n    percent = float(indx)/length\n    sys.stdout.write(""\\r"")\n    progress = """"\n    for i in range(barLen):\n        if i < int(barLen * percent):\n            progress += ""=""\n        else:\n            progress += "" ""\n    sys.stdout.write(""[%s] <<< %d/%d (%d%%)"" % (progress, indx, length, percent * 100))\n    sys.stdout.flush()\n'"
src/keras_lib/model.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://github.com/CSTR-Edinburgh/merlin\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\nimport random\nimport numpy as np\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.models import model_from_json\nfrom keras.layers import Dense, SimpleRNN, GRU, LSTM\nfrom keras.layers import Dropout\n\nclass kerasModels(object):\n\n    def __init__(self, n_in, hidden_layer_size, n_out, hidden_layer_type, output_type=\'linear\', dropout_rate=0.0, loss_function=\'mse\', optimizer=\'adam\'):\n        """""" This function initialises a neural network\n\n        :param n_in: Dimensionality of input features\n        :param hidden_layer_size: The layer size for each hidden layer\n        :param n_out: Dimensionality of output features\n        :param hidden_layer_type: the activation types of each hidden layers, e.g., TANH, LSTM, GRU, BLSTM\n        :param output_type: the activation type of the output layer, by default is \'LINEAR\', linear regression.\n        :param dropout_rate: probability of dropout, a float number between 0 and 1.\n        :type n_in: Integer\n        :type hidden_layer_size: A list of integers\n        :type n_out: Integrer\n        """"""\n\n        self.n_in  = int(n_in)\n        self.n_out = int(n_out)\n\n        self.n_layers = len(hidden_layer_size)\n\n        self.hidden_layer_size = hidden_layer_size\n        self.hidden_layer_type = hidden_layer_type\n\n        assert len(self.hidden_layer_size) == len(self.hidden_layer_type)\n\n        self.output_type   = output_type\n        self.dropout_rate  = dropout_rate\n        self.loss_function = loss_function\n        self.optimizer     = optimizer\n\n        # create model\n        self.model = Sequential()\n\n    def define_feedforward_model(self):\n        seed = 12345\n        np.random.seed(seed)\n\n        # add hidden layers\n        for i in range(self.n_layers):\n            if i == 0:\n                input_size = self.n_in\n            else:\n                input_size = self.hidden_layer_size[i - 1]\n\n            self.model.add(Dense(\n                    units=self.hidden_layer_size[i],\n                    activation=self.hidden_layer_type[i],\n                    kernel_initializer=""normal"",\n                    input_dim=input_size))\n            self.model.add(Dropout(self.dropout_rate))\n\n        # add output layer\n        self.final_layer = self.model.add(Dense(\n            units=self.n_out,\n            activation=self.output_type.lower(),\n            kernel_initializer=""normal"",\n            input_dim=self.hidden_layer_size[-1]))\n\n        # Compile the model\n        self.compile_model()\n\n    def define_sequence_model(self):\n        seed = 12345\n        np.random.seed(seed)\n\n        # add hidden layers\n        for i in range(self.n_layers):\n            if i == 0:\n                input_size = self.n_in\n            else:\n                input_size = self.hidden_layer_size[i - 1]\n\n            if self.hidden_layer_type[i]==\'rnn\':\n                self.model.add(SimpleRNN(\n                        units=self.hidden_layer_size[i],\n                        input_shape=(None, input_size),\n                        return_sequences=True))\n            elif self.hidden_layer_type[i]==\'gru\':\n                self.model.add(GRU(\n                        units=self.hidden_layer_size[i],\n                        input_shape=(None, input_size),\n                        return_sequences=True))\n            elif self.hidden_layer_type[i]==\'lstm\':\n                self.model.add(LSTM(\n                        units=self.hidden_layer_size[i],\n                        input_shape=(None, input_size),\n                        return_sequences=True))\n            elif self.hidden_layer_type[i]==\'blstm\':\n                self.model.add(LSTM(\n                        units=self.hidden_layer_size[i],\n                        input_shape=(None, input_size),\n                        return_sequences=True,\n                        go_backwards=True))\n            else:\n                self.model.add(Dense(\n                        units=self.hidden_layer_size[i],\n                        activation=self.hidden_layer_type[i],\n                        kernel_initializer=""normal"",\n                        input_shape=(None, input_size)))\n\n        # add output layer\n        self.final_layer = self.model.add(Dense(\n            units=self.n_out,\n            input_dim=self.hidden_layer_size[-1],\n            kernel_initializer=\'normal\',\n            activation=self.output_type.lower()))\n\n        # Compile the model\n        self.compile_model()\n\n    def define_stateful_model(self, batch_size=25, seq_length=200):\n        seed = 12345\n        np.random.seed(seed)\n\n        # params\n        batch_size = batch_size\n        timesteps  = seq_length\n\n        # add hidden layers\n        for i in range(self.n_layers):\n            if i == 0:\n                input_size = self.n_in\n            else:\n                input_size = self.hidden_layer_size[i - 1]\n\n            if hidden_layer_type[i]==\'lstm\':\n                self.model.add(LSTM(\n                        units=self.hidden_layer_size[i],\n                        batch_input_shape=(batch_size, timesteps, input_size),\n                        return_sequences=True,\n                        stateful=True))   #go_backwards=True))\n            elif self.hidden_layer_type[i]==\'blstm\':\n                self.model.add(LSTM(\n                        units=self.hidden_layer_size[i],\n                        batch_input_shape=(batch_size, timesteps, input_size),\n                        return_sequences=True,\n                        stateful=True,\n                        go_backwards=True))\n            else:\n                self.model.add(Dense(\n                        units=self.hidden_layer_size[i],\n                        activation=self.hidden_layer_type[i],\n                        kernel_initializer=""normal"",\n                        batch_input_shape=(batch_size, timesteps, input_size)))\n\n        # add output layer\n        self.final_layer = self.model.add(Dense(\n            units=self.n_out,\n            input_dim=self.hidden_layer_size[-1],\n            kernel_initializer=\'normal\',\n            activation=self.output_type.lower()))\n\n        # Compile the model\n        self.compile_model()\n\n    def compile_model(self):\n        self.model.compile(loss=self.loss_function, optimizer=self.optimizer, metrics=[\'accuracy\'])\n\n    def save_model(self, json_model_file, h5_model_file):\n        # serialize model to JSON\n        model_json = self.model.to_json()\n        with open(json_model_file, ""w"") as json_file:\n            json_file.write(model_json)\n        # serialize weights to HDF5\n        self.model.save_weights(h5_model_file)\n        print(""Saved model to disk"")\n\n    def load_model(self, json_model_file, h5_model_file):\n        #### load the model ####\n        json_file = open(json_model_file, \'r\')\n        loaded_model_json = json_file.read()\n        json_file.close()\n        loaded_model = model_from_json(loaded_model_json)\n        loaded_model.load_weights(h5_model_file)\n        print(""Loaded model from disk"")\n\n        #### compile the model ####\n        self.model = loaded_model\n        self.compile_model()\n'"
src/keras_lib/train.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://github.com/CSTR-Edinburgh/merlin\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\nimport os, sys\nimport random\nimport numpy as np\n\nfrom io_funcs.binary_io import BinaryIOCollection\n\nfrom keras_lib.model import kerasModels\nfrom keras_lib import data_utils\n\nclass TrainKerasModels(kerasModels):\n\n    def __init__(self, n_in, hidden_layer_size, n_out, hidden_layer_type, output_type=\'linear\', dropout_rate=0.0, loss_function=\'mse\', optimizer=\'adam\', rnn_params=None):\n\n        kerasModels.__init__(self, n_in, hidden_layer_size, n_out, hidden_layer_type, output_type, dropout_rate, loss_function, optimizer)\n\n        #### TODO: Find a good way to pass below params ####\n        self.merge_size   = rnn_params[\'merge_size\']\n        self.seq_length   = rnn_params[\'seq_length\']\n        self.bucket_range = rnn_params[\'bucket_range\']\n\n        self.stateful = rnn_params[\'stateful\']\n\n        pass;\n\n    def train_feedforward_model(self, train_x, train_y, valid_x, valid_y, batch_size=256, num_of_epochs=10, shuffle_data=True):\n        self.model.fit(train_x, train_y, batch_size=batch_size, epochs=num_of_epochs, shuffle=shuffle_data)\n\n    def train_sequence_model(self, train_x, train_y, valid_x, valid_y, train_flen, batch_size=1, num_of_epochs=10, shuffle_data=True, training_algo=1):\n        if batch_size == 1:\n            self.train_recurrent_model_batchsize_one(train_x, train_y, valid_x, valid_y, num_of_epochs, shuffle_data)\n        else:\n            self.train_recurrent_model(train_x, train_y, valid_x, valid_y, train_flen, batch_size, num_of_epochs, shuffle_data, training_algo)\n\n    def train_recurrent_model_batchsize_one(self, train_x, train_y, valid_x, valid_y, num_of_epochs, shuffle_data):\n        ### if batch size is equal to 1 ###\n        train_idx_list = list(train_x.keys())\n        if shuffle_data:\n            random.seed(271638)\n            random.shuffle(train_idx_list)\n\n        train_file_number = len(train_idx_list)\n        for epoch_num in range(num_of_epochs):\n            print((\'Epoch: %d/%d \' %(epoch_num+1, num_of_epochs)))\n            file_num = 0\n            for file_name in train_idx_list:\n                temp_train_x = train_x[file_name]\n                temp_train_y = train_y[file_name]\n                temp_train_x = np.reshape(temp_train_x, (1, temp_train_x.shape[0], self.n_in))\n                temp_train_y = np.reshape(temp_train_y, (1, temp_train_y.shape[0], self.n_out))\n                self.model.train_on_batch(temp_train_x, temp_train_y)\n                #self.model.fit(temp_train_x, temp_train_y, epochs=1, shuffle=False, verbose=0)\n                file_num += 1\n                data_utils.drawProgressBar(file_num, train_file_number)\n\n            sys.stdout.write(""\\n"")\n\n    def train_recurrent_model(self, train_x, train_y, valid_x, valid_y, train_flen, batch_size, num_of_epochs, shuffle_data, training_algo):\n        ### if batch size more than 1 ###\n        if training_algo == 1:\n            self.train_padding_model(train_x, train_y, valid_x, valid_y, train_flen, batch_size, num_of_epochs, shuffle_data)\n        elif training_algo == 2:\n            self.train_bucket_model(train_x, train_y, valid_x, valid_y, train_flen, batch_size, num_of_epochs, shuffle_data)\n        elif training_algo == 3:\n            self.train_split_model(train_x, train_y, valid_x, valid_y, train_flen, batch_size, num_of_epochs, shuffle_data)\n        else:\n            print(""Choose training algorithm for batch training with RNNs:"")\n            print(""1. Padding model -- pad utterances with zeros to maximum sequence length"")\n            print(""2. Bucket model  -- form buckets with minimum and maximum sequence length"")\n            print(""3. Split model   -- split utterances to a fixed sequence length"")\n            sys.exit(1)\n\n    \n    def train_padding_model(self, train_x, train_y, valid_x, valid_y, train_flen, batch_size, num_of_epochs, shuffle_data):\n        ### Method 1 ###\n        train_id_list = list(train_flen[\'utt2framenum\'].keys())\n        if shuffle_data:\n            random.seed(271638)\n            random.shuffle(train_id_list)\n\n        train_file_number = len(train_id_list)\n        for epoch_num in range(num_of_epochs):\n            print((\'Epoch: %d/%d \' %(epoch_num+1, num_of_epochs)))\n            file_num = 0\n            while file_num < train_file_number:\n                train_idx_list = train_id_list[file_num: file_num + batch_size]\n                seq_len_arr    = [train_flen[\'utt2framenum\'][filename] for filename in train_idx_list]\n                max_seq_length = max(seq_len_arr)\n                sub_train_x    = dict((filename, train_x[filename]) for filename in train_idx_list)\n                sub_train_y    = dict((filename, train_y[filename]) for filename in train_idx_list)\n                temp_train_x   = data_utils.transform_data_to_3d_matrix(sub_train_x, max_length=max_seq_length)\n                temp_train_y   = data_utils.transform_data_to_3d_matrix(sub_train_y, max_length=max_seq_length)\n                self.model.train_on_batch(temp_train_x, temp_train_y)\n                file_num += len(train_idx_list)\n                data_utils.drawProgressBar(file_num, train_file_number)\n\n            print("" Validation error: %.3f"" % (self.get_validation_error(valid_x, valid_y)))\n    \n    def train_bucket_model(self, train_x, train_y, valid_x, valid_y, train_flen, batch_size, num_of_epochs, shuffle_data):\n        ### Method 2 ###\n        train_fnum_list  = np.array(list(train_flen[\'framenum2utt\'].keys()))\n        train_range_list = list(range(min(train_fnum_list), max(train_fnum_list)+1, self.bucket_range))\n        if shuffle_data:\n            random.seed(271638)\n            random.shuffle(train_range_list)\n\n        train_file_number = len(train_x)\n        for epoch_num in range(num_of_epochs):\n            print((\'Epoch: %d/%d \' %(epoch_num+1, num_of_epochs)))\n            file_num = 0\n            for frame_num in train_range_list:\n                min_seq_length = frame_num\n                max_seq_length = frame_num+self.bucket_range\n                sub_train_list = train_fnum_list[(train_fnum_list>=min_seq_length) & (train_fnum_list<max_seq_length)]\n                if len(sub_train_list)==0:\n                    continue;\n                train_idx_list = sum([train_flen[\'framenum2utt\'][framenum] for framenum in sub_train_list], [])\n                sub_train_x    = dict((filename, train_x[filename]) for filename in train_idx_list)\n                sub_train_y    = dict((filename, train_y[filename]) for filename in train_idx_list)\n                temp_train_x   = data_utils.transform_data_to_3d_matrix(sub_train_x, max_length=max_seq_length)\n                temp_train_y   = data_utils.transform_data_to_3d_matrix(sub_train_y, max_length=max_seq_length)\n                self.model.fit(temp_train_x, temp_train_y, batch_size=batch_size, shuffle=False, epochs=1, verbose=0)\n\n                file_num += len(train_idx_list)\n                data_utils.drawProgressBar(file_num, train_file_number)\n\n            print("" Validation error: %.3f"" % (self.get_validation_error(valid_x, valid_y)))\n\n    def train_split_model(self, train_x, train_y, valid_x, valid_y, train_flen, batch_size, num_of_epochs, shuffle_data):\n        ### Method 3 ###\n        train_id_list = list(train_flen[\'utt2framenum\'].keys())\n        if shuffle_data:\n            random.seed(271638)\n            random.shuffle(train_id_list)\n\n        train_file_number = len(train_id_list)\n        for epoch_num in range(num_of_epochs):\n            print((\'Epoch: %d/%d \' %(epoch_num+1, num_of_epochs)))\n            file_num = 0\n            while file_num < train_file_number:\n                train_idx_list = train_id_list[file_num: file_num + batch_size]\n                sub_train_x    = dict((filename, train_x[filename]) for filename in train_idx_list)\n                sub_train_y    = dict((filename, train_y[filename]) for filename in train_idx_list)\n                temp_train_x   = data_utils.transform_data_to_3d_matrix(sub_train_x, seq_length=self.seq_length, merge_size=self.merge_size)\n                temp_train_y   = data_utils.transform_data_to_3d_matrix(sub_train_y, seq_length=self.seq_length, merge_size=self.merge_size)\n    \n                self.model.train_on_batch(temp_train_x, temp_train_y)\n\n                file_num += len(train_idx_list)\n                data_utils.drawProgressBar(file_num, train_file_number)\n\n            print("" Validation error: %.3f"" % (self.get_validation_error(valid_x, valid_y)))\n\n    def train_split_model_keras_version(self, train_x, train_y, valid_x, valid_y, train_flen, batch_size, num_of_epochs, shuffle_data):\n        """"""This function is not used as of now \n        """"""\n        ### Method 3 ###\n        temp_train_x = data_utils.transform_data_to_3d_matrix(train_x, seq_length=self.seq_length, merge_size=self.merge_size, shuffle_data=shuffle_data)\n        print((""Input shape: ""+str(temp_train_x.shape)))\n        \n        temp_train_y = data_utils.transform_data_to_3d_matrix(train_y, seq_length=self.seq_length, merge_size=self.merge_size, shuffle_data=shuffle_data)\n        print((""Output shape: ""+str(temp_train_y.shape)))\n        \n        if self.stateful:\n            temp_train_x, temp_train_y = data_utils.get_stateful_data(temp_train_x, temp_train_y, batch_size)\n    \n        self.model.fit(temp_train_x, temp_train_y, batch_size=batch_size, epochs=num_of_epochs)\n    \n    def train_bucket_model_without_padding(self, train_x, train_y, valid_x, valid_y, train_flen, batch_size, num_of_epochs, shuffle_data):\n        """"""This function is not used as of now\n        """"""\n        ### Method 4 ###\n        train_count_list = list(train_flen[\'framenum2utt\'].keys())\n        if shuffle_data:\n            random.seed(271638)\n            random.shuffle(train_count_list)\n\n        train_file_number = len(train_x)\n        for epoch_num in range(num_of_epochs):\n            print((\'Epoch: %d/%d \' %(epoch_num+1, num_of_epochs)))\n            file_num = 0\n            for sequence_length in train_count_list:\n                train_idx_list = train_flen[\'framenum2utt\'][sequence_length]\n                sub_train_x    = dict((filename, train_x[filename]) for filename in train_idx_list)\n                sub_train_y    = dict((filename, train_y[filename]) for filename in train_idx_list)\n                temp_train_x   = data_utils.transform_data_to_3d_matrix(sub_train_x, max_length=sequence_length)\n                temp_train_y   = data_utils.transform_data_to_3d_matrix(sub_train_y, max_length=sequence_length)\n                self.model.fit(temp_train_x, temp_train_y, batch_size=batch_size, epochs=1, verbose=0)\n\n                file_num += len(train_idx_list)\n                data_utils.drawProgressBar(file_num, train_file_number)\n\n            sys.stdout.write(""\\n"")\n\n    def get_validation_error(self, valid_x, valid_y, sequential_training=True, stateful=False):\n        valid_id_list = list(valid_x.keys())\n        valid_id_list.sort()\n\n        valid_error = 0.0\n        valid_file_number = len(valid_id_list)\n        for utt_index in range(valid_file_number):\n            temp_valid_x = valid_x[valid_id_list[utt_index]]\n            temp_valid_y = valid_y[valid_id_list[utt_index]]\n            num_of_rows = temp_valid_x.shape[0]\n\n            if stateful:\n                temp_valid_x = data_utils.get_stateful_input(temp_valid_x, self.seq_length, self.batch_size)\n            elif sequential_training:\n                temp_valid_x = np.reshape(temp_valid_x, (1, num_of_rows, self.n_in))\n\n            predictions = self.model.predict(temp_valid_x)\n            if sequential_training:\n                predictions = np.reshape(predictions, (num_of_rows, self.n_out))\n\n            valid_error += np.mean(np.sum((predictions - temp_valid_y) ** 2, axis=1))\n\n        valid_error = valid_error/valid_file_number\n\n        return valid_error\n\n    def predict(self, test_x, out_scaler, gen_test_file_list, sequential_training=False, stateful=False):\n        #### compute predictions ####\n        io_funcs = BinaryIOCollection()\n\n        test_file_number = len(gen_test_file_list)\n        print(""generating features on held-out test data..."")\n        for utt_index in range(test_file_number):\n            gen_test_file_name = gen_test_file_list[utt_index]\n            test_id = os.path.splitext(os.path.basename(gen_test_file_name))[0]\n            temp_test_x        = test_x[test_id]\n            num_of_rows        = temp_test_x.shape[0]\n\n            if stateful:\n                temp_test_x = data_utils.get_stateful_input(temp_test_x, self.seq_length, self.batch_size)\n            elif sequential_training:\n                temp_test_x = np.reshape(temp_test_x, (1, num_of_rows, self.n_in))\n\n            predictions = self.model.predict(temp_test_x)\n            if sequential_training:\n                predictions = np.reshape(predictions, (num_of_rows, self.n_out))\n\n            data_utils.denorm_data(predictions, out_scaler)\n\n            io_funcs.array_to_binary_file(predictions, gen_test_file_name)\n            data_utils.drawProgressBar(utt_index+1, test_file_number)\n\n        sys.stdout.write(""\\n"")\n'"
src/layers/__init__.py,0,b''
src/layers/gating.py,0,"b'\n### refer Zhizheng and Simon\'s ICASSP\'16 paper for more details\n### http://www.zhizheng.org/papers/icassp2016_lstm.pdf\n\nimport numpy as np\nimport theano\nimport theano.tensor as T\nfrom theano import config\nfrom theano.tensor.shared_randomstreams import RandomStreams\n\nclass VanillaRNN(object):\n    """""" This class implements a standard recurrent neural network: h_{t} = f(W^{hx}x_{t} + W^{hh}h_{t-1}+b_{h})\n\n    """"""\n    def __init__(self, rng, x, n_in, n_h, p, training, rnn_batch_training=False):\n        """""" This is to initialise a standard RNN hidden unit\n\n        :param rng: random state, fixed value for randome state for reproducible objective results\n        :param x: input data to current layer\n        :param n_in: dimension of input data\n        :param n_h: number of hidden units/blocks\n        :param p: the probability of dropout\n        :param training: a binary value to indicate training or testing (for dropout training)\n        """"""\n        self.input = x\n\n        if p > 0.0:\n            if training==1:\n                srng = RandomStreams(seed=123456)\n                self.input = T.switch(srng.binomial(size=x.shape,p=p), x, 0)\n            else:\n                self.input =  (1-p) * x #(1-p) *\n\n        self.n_in = int(n_in)\n        self.n_h  = int(n_h)\n\n        self.rnn_batch_training = rnn_batch_training\n\n        # random initialisation\n        Wx_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_h)), dtype=config.floatX)\n        Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=config.floatX)\n\n        # Input gate weights\n        self.W_xi = theano.shared(value=Wx_value, name=\'W_xi\')\n        self.W_hi = theano.shared(value=Wh_value, name=\'W_hi\')\n\n        # bias\n        self.b_i = theano.shared(value=np.zeros((n_h, ), dtype=config.floatX), name=\'b_i\')\n\n\n        # initial value of hidden and cell state\n        if self.rnn_batch_training:\n            self.h0 = theano.shared(value=np.zeros((1, n_h), dtype = config.floatX), name = \'h0\')\n            self.c0 = theano.shared(value=np.zeros((1, n_h), dtype = config.floatX), name = \'c0\')\n\n            self.h0 = T.repeat(self.h0, x.shape[1], 0)\n            self.c0 = T.repeat(self.c0, x.shape[1], 0)\n        else:\n            self.h0 = theano.shared(value=np.zeros((n_h, ), dtype = config.floatX), name = \'h0\')\n            self.c0 = theano.shared(value=np.zeros((n_h, ), dtype = config.floatX), name = \'c0\')\n\n\n        self.Wix = T.dot(self.input, self.W_xi)\n\n        [self.h, self.c], _ = theano.scan(self.recurrent_as_activation_function, sequences = [self.Wix],\n                                                                      outputs_info = [self.h0, self.c0])\n\n        self.output = self.h\n\n        self.params = [self.W_xi, self.W_hi, self.b_i]\n\n        self.L2_cost = (self.W_xi ** 2).sum() + (self.W_hi ** 2).sum()\n\n\n    def recurrent_as_activation_function(self, Wix, h_tm1, c_tm1):\n        """""" Implement the recurrent unit as an activation function. This function is called by self.__init__().\n\n        :param Wix: it equals to W^{hx}x_{t}, as it does not relate with recurrent, pre-calculate the value for fast computation\n        :type Wix: matrix\n        :param h_tm1: contains the hidden activation from previous time step\n        :type h_tm1: matrix, each row means a hidden activation vector of a time step\n        :param c_tm1: this parameter is not used, just to keep the interface consistent with LSTM\n        :returns: h_t is the hidden activation of current time step\n        """"""\n\n        h_t = T.tanh(Wix + T.dot(h_tm1, self.W_hi) + self.b_i)  #\n\n        c_t = h_t\n\n        return h_t, c_t\n\nclass VanillaRNNDecoder(object):\n    """""" This class implements a standard recurrent neural network decoder:\n        h_{t} = f(W^{hx}x_{t} + W^{hh}h_{t-1}+ W^{yh}y_{t-1} + b_{h})\n        y_{t} = g(h_{t}W^{hy} + b_{y})\n\n    """"""\n    def __init__(self, rng, x, n_in, n_h, n_out, p, training, rnn_batch_training=False):\n        """""" This is to initialise a standard RNN hidden unit\n\n        :param rng: random state, fixed value for randome state for reproducible objective results\n        :param x: input data to current layer\n        :param n_in: dimension of input data\n        :param n_h: number of hidden units/blocks\n        :param n_out: dimension of output data\n        :param p: the probability of dropout\n        :param training: a binary value to indicate training or testing (for dropout training)\n        """"""\n        self.input = x\n\n        if p > 0.0:\n            if training==1:\n                srng = RandomStreams(seed=123456)\n                self.input = T.switch(srng.binomial(size=x.shape,p=p), x, 0)\n            else:\n                self.input =  (1-p) * x #(1-p) *\n\n        self.n_in  = int(n_in)\n        self.n_h   = int(n_h)\n        self.n_out = int(n_out)\n\n        self.rnn_batch_training = rnn_batch_training\n\n        # random initialisation\n        Wx_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_h)), dtype=config.floatX)\n        Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=config.floatX)\n        Wy_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_out), size=(n_out, n_h)), dtype=config.floatX)\n        Ux_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_out)), dtype=config.floatX)\n        Uh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_out)), dtype=config.floatX)\n        Uy_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_out), size=(n_out, n_out)), dtype=config.floatX)\n\n        # Input gate weights\n        self.W_xi = theano.shared(value=Wx_value, name=\'W_xi\')\n        self.W_hi = theano.shared(value=Wh_value, name=\'W_hi\')\n        self.W_yi = theano.shared(value=Wy_value, name=\'W_yi\')\n\n        # Output gate weights\n        self.U_xi = theano.shared(value=Ux_value, name=\'U_xi\')\n        self.U_hi = theano.shared(value=Uh_value, name=\'U_hi\')\n        self.U_yi = theano.shared(value=Uy_value, name=\'U_yi\')\n\n        # bias\n        self.b_i = theano.shared(value=np.zeros((n_h, ), dtype=config.floatX), name=\'b_i\')\n        self.b   = theano.shared(value=np.zeros((n_out, ), dtype=config.floatX), name=\'b\')\n\n\n        # initial value of hidden and cell state and output\n        if self.rnn_batch_training:\n            self.h0 = theano.shared(value=np.zeros((1, n_h), dtype = config.floatX), name = \'h0\')\n            self.c0 = theano.shared(value=np.zeros((1, n_h), dtype = config.floatX), name = \'c0\')\n            self.y0 = theano.shared(value=np.zeros((1, n_out), dtype = config.floatX), name = \'y0\')\n\n            self.h0 = T.repeat(self.h0, x.shape[1], 0)\n            self.c0 = T.repeat(self.c0, x.shape[1], 0)\n            self.y0 = T.repeat(self.c0, x.shape[1], 0)\n        else:\n            self.h0 = theano.shared(value=np.zeros((n_h, ), dtype = config.floatX), name = \'h0\')\n            self.c0 = theano.shared(value=np.zeros((n_h, ), dtype = config.floatX), name = \'c0\')\n            self.y0 = theano.shared(value=np.zeros((n_out, ), dtype = config.floatX), name = \'y0\')\n\n\n        self.Wix = T.dot(self.input, self.W_xi)\n\n        [self.h, self.c, self.y], _ = theano.scan(self.recurrent_as_activation_function, sequences = [self.Wix],\n                                                                      outputs_info = [self.h0, self.c0, self.y0])\n\n        self.output = self.y\n\n        self.params = [self.W_xi, self.W_hi, self.W_yi, self.U_hi, self.b_i, self.b]\n\n        self.L2_cost = (self.W_xi ** 2).sum() + (self.W_hi ** 2).sum() + (self.W_yi ** 2).sum() + (self.U_hi ** 2).sum()\n\n\n    def recurrent_as_activation_function(self, Wix, h_tm1, c_tm1, y_tm1):\n        """""" Implement the recurrent unit as an activation function. This function is called by self.__init__().\n\n        :param Wix: it equals to W^{hx}x_{t}, as it does not relate with recurrent, pre-calculate the value for fast computation\n        :type Wix: matrix\n        :param h_tm1: contains the hidden activation from previous time step\n        :type h_tm1: matrix, each row means a hidden activation vector of a time step\n        :param c_tm1: this parameter is not used, just to keep the interface consistent with LSTM\n        :returns: h_t is the hidden activation of current time step\n        """"""\n\n        h_t = T.tanh(Wix + T.dot(h_tm1, self.W_hi) + T.dot(y_tm1, self.W_yi) + self.b_i)  #\n\n        y_t = T.dot(h_t, self.U_hi) + self.b\n\n        c_t = h_t\n\n        return h_t, c_t, y_t\n\n\nclass LstmBase(object):\n    """""" This class provides as a base for all long short-term memory (LSTM) related classes.\n    Several variants of LSTM were investigated in (Wu & King, ICASSP 2016): Zhizheng Wu, Simon King, ""Investigating gated recurrent neural networks for speech synthesis"", ICASSP 2016\n\n    """"""\n\n    def __init__(self, rng, x, n_in, n_h, p=0.0, training=0, rnn_batch_training=False):\n        """""" Initialise all the components in a LSTM block, including input gate, output gate, forget gate, peephole connections\n\n        :param rng: random state, fixed value for randome state for reproducible objective results\n        :param x: input to a network\n        :param n_in: number of input features\n        :type n_in: integer\n        :param n_h: number of hidden units\n        :type n_h: integer\n        :param p: the probability of dropout\n        :param training: a binary value to indicate training or testing (for dropout training)\n        """"""\n\n        n_in = int(n_in)  # ensure sizes have integer type\n        n_h = int(n_h)# ensure sizes have integer type\n\n        self.input = x\n\n        if p > 0.0:\n            if training==1:\n                srng = RandomStreams(seed=123456)\n                self.input = T.switch(srng.binomial(size=x.shape,p=p), x, 0)\n            else:\n                self.input =  (1-p) * x\n\n        self.n_in = int(n_in)\n        self.n_h  = int(n_h)\n\n        self.rnn_batch_training = rnn_batch_training\n\n        # random initialisation\n        Wx_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_h)), dtype=config.floatX)\n        Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=config.floatX)\n        Wc_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, )), dtype=config.floatX)\n\n        # Input gate weights\n        self.W_xi = theano.shared(value=Wx_value, name=\'W_xi\')\n        self.W_hi = theano.shared(value=Wh_value, name=\'W_hi\')\n        self.w_ci = theano.shared(value=Wc_value, name=\'w_ci\')\n\n        # random initialisation\n        Wx_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_h)), dtype=config.floatX)\n        Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=config.floatX)\n        Wc_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, )), dtype=config.floatX)\n\n        # Forget gate weights\n        self.W_xf = theano.shared(value=Wx_value, name=\'W_xf\')\n        self.W_hf = theano.shared(value=Wh_value, name=\'W_hf\')\n        self.w_cf = theano.shared(value=Wc_value, name=\'w_cf\')\n\n        # random initialisation\n        Wx_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_h)), dtype=config.floatX)\n        Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=config.floatX)\n        Wc_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, )), dtype=config.floatX)\n\n        # Output gate weights\n        self.W_xo = theano.shared(value=Wx_value, name=\'W_xo\')\n        self.W_ho = theano.shared(value=Wh_value, name=\'W_ho\')\n        self.w_co = theano.shared(value=Wc_value, name=\'w_co\')\n\n        # random initialisation\n        Wx_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_h)), dtype=config.floatX)\n        Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=config.floatX)\n        Wc_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, )), dtype=config.floatX)\n\n        # Cell weights\n        self.W_xc = theano.shared(value=Wx_value, name=\'W_xc\')\n        self.W_hc = theano.shared(value=Wh_value, name=\'W_hc\')\n\n        # bias\n        self.b_i = theano.shared(value=np.zeros((n_h, ), dtype=config.floatX), name=\'b_i\')\n        self.b_f = theano.shared(value=np.zeros((n_h, ), dtype=config.floatX), name=\'b_f\')\n        self.b_o = theano.shared(value=np.zeros((n_h, ), dtype=config.floatX), name=\'b_o\')\n        self.b_c = theano.shared(value=np.zeros((n_h, ), dtype=config.floatX), name=\'b_c\')\n\n        ### make a layer\n\n        # initial value of hidden and cell state\n        if self.rnn_batch_training:\n            self.h0 = theano.shared(value=np.zeros((1, n_h), dtype = config.floatX), name = \'h0\')\n            self.c0 = theano.shared(value=np.zeros((1, n_h), dtype = config.floatX), name = \'c0\')\n\n            self.h0 = T.repeat(self.h0, x.shape[1], 0)\n            self.c0 = T.repeat(self.c0, x.shape[1], 0)\n        else:\n            self.h0 = theano.shared(value=np.zeros((n_h, ), dtype = config.floatX), name = \'h0\')\n            self.c0 = theano.shared(value=np.zeros((n_h, ), dtype = config.floatX), name = \'c0\')\n\n\n        self.Wix = T.dot(self.input, self.W_xi)\n        self.Wfx = T.dot(self.input, self.W_xf)\n        self.Wcx = T.dot(self.input, self.W_xc)\n        self.Wox = T.dot(self.input, self.W_xo)\n\n        [self.h, self.c], _ = theano.scan(self.recurrent_fn, sequences = [self.Wix, self.Wfx, self.Wcx, self.Wox],\n                                                             outputs_info = [self.h0, self.c0])\n\n        self.output = self.h\n\n\n    def recurrent_fn(self, Wix, Wfx, Wcx, Wox, h_tm1, c_tm1 = None):\n        """""" This implements a genetic recurrent function, called by self.__init__().\n\n        :param Wix: pre-computed matrix applying the weight matrix W on  the input units, for input gate\n        :param Wfx: Similar to Wix, but for forget gate\n        :param Wcx: Similar to Wix, but for cell memory\n        :param Wox: Similar to Wox, but for output gate\n        :param h_tm1: hidden activation from previous time step\n        :param c_tm1: activation from cell memory from previous time step\n        :returns: h_t is the hidden activation of current time step, and c_t is the activation for cell memory of current time step\n        """"""\n\n        h_t, c_t = self.lstm_as_activation_function(Wix, Wfx, Wcx, Wox, h_tm1, c_tm1)\n\n        return h_t, c_t\n\n    def lstm_as_activation_function(self):\n        """""" A genetic recurrent activation function for variants of LSTM architectures.\n        The function is called by self.recurrent_fn().\n\n        """"""\n        pass\n\nclass LstmDecoderBase(object):\n    """""" This class provides as a base for all long short-term memory (LSTM) related classes.\n    Several variants of LSTM were investigated in (Wu & King, ICASSP 2016): Zhizheng Wu, Simon King, ""Investigating gated recurrent neural networks for speech synthesis"", ICASSP 2016\n\n    """"""\n\n    def __init__(self, rng, x, n_in, n_h, n_out, p=0.0, training=0, rnn_batch_training=False):\n        """""" Initialise all the components in a LSTM block, including input gate, output gate, forget gate, peephole connections\n\n        :param rng: random state, fixed value for randome state for reproducible objective results\n        :param x: input to a network\n        :param n_in: number of input features\n        :type n_in: integer\n        :param n_h: number of hidden units\n        :type n_h: integer\n        :param p: the probability of dropout\n        :param training: a binary value to indicate training or testing (for dropout training)\n        """"""\n\n        self.input = x\n\n        if p > 0.0:\n            if training==1:\n                srng = RandomStreams(seed=123456)\n                self.input = T.switch(srng.binomial(size=x.shape,p=p), x, 0)\n            else:\n                self.input =  (1-p) * x\n\n        self.n_in = int(n_in)\n        self.n_h  = int(n_h)\n\n        self.rnn_batch_training = rnn_batch_training\n\n        # random initialisation\n        Wx_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_h)), dtype=config.floatX)\n        Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=config.floatX)\n        Wc_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, )), dtype=config.floatX)\n        Wy_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_out), size=(n_out, n_h)), dtype=config.floatX)\n\n        # Input gate weights\n        self.W_xi = theano.shared(value=Wx_value, name=\'W_xi\')\n        self.W_hi = theano.shared(value=Wh_value, name=\'W_hi\')\n        self.w_ci = theano.shared(value=Wc_value, name=\'w_ci\')\n        self.W_yi = theano.shared(value=Wy_value, name=\'W_yi\')\n\n        # random initialisation\n        Uh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_out)), dtype=config.floatX)\n\n        # Output gate weights\n        self.U_ho = theano.shared(value=Uh_value, name=\'U_ho\')\n\n        # random initialisation\n        Wx_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_h)), dtype=config.floatX)\n        Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=config.floatX)\n        Wc_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, )), dtype=config.floatX)\n\n        # Forget gate weights\n        self.W_xf = theano.shared(value=Wx_value, name=\'W_xf\')\n        self.W_hf = theano.shared(value=Wh_value, name=\'W_hf\')\n        self.w_cf = theano.shared(value=Wc_value, name=\'w_cf\')\n\n        # random initialisation\n        Wx_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_h)), dtype=config.floatX)\n        Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=config.floatX)\n        Wc_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, )), dtype=config.floatX)\n\n        # Output gate weights\n        self.W_xo = theano.shared(value=Wx_value, name=\'W_xo\')\n        self.W_ho = theano.shared(value=Wh_value, name=\'W_ho\')\n        self.w_co = theano.shared(value=Wc_value, name=\'w_co\')\n\n        # random initialisation\n        Wx_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_h)), dtype=config.floatX)\n        Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=config.floatX)\n        Wc_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, )), dtype=config.floatX)\n\n        # Cell weights\n        self.W_xc = theano.shared(value=Wx_value, name=\'W_xc\')\n        self.W_hc = theano.shared(value=Wh_value, name=\'W_hc\')\n\n        # bias\n        self.b_i = theano.shared(value=np.zeros((n_h, ), dtype=config.floatX), name=\'b_i\')\n        self.b_f = theano.shared(value=np.zeros((n_h, ), dtype=config.floatX), name=\'b_f\')\n        self.b_o = theano.shared(value=np.zeros((n_h, ), dtype=config.floatX), name=\'b_o\')\n        self.b_c = theano.shared(value=np.zeros((n_h, ), dtype=config.floatX), name=\'b_c\')\n        self.b   = theano.shared(value=np.zeros((n_out, ), dtype=config.floatX), name=\'b\')\n\n        ### make a layer\n\n        # initial value of hidden and cell state\n        if self.rnn_batch_training:\n            self.h0 = theano.shared(value=np.zeros((1, n_h), dtype = config.floatX), name = \'h0\')\n            self.c0 = theano.shared(value=np.zeros((1, n_h), dtype = config.floatX), name = \'c0\')\n            self.y0 = theano.shared(value=np.zeros((1, n_out), dtype = config.floatX), name = \'y0\')\n\n            self.h0 = T.repeat(self.h0, x.shape[1], 0)\n            self.c0 = T.repeat(self.c0, x.shape[1], 0)\n            self.y0 = T.repeat(self.c0, x.shape[1], 0)\n        else:\n            self.h0 = theano.shared(value=np.zeros((n_h, ), dtype = config.floatX), name = \'h0\')\n            self.c0 = theano.shared(value=np.zeros((n_h, ), dtype = config.floatX), name = \'c0\')\n            self.y0 = theano.shared(value=np.zeros((n_out, ), dtype = config.floatX), name = \'y0\')\n\n\n        self.Wix = T.dot(self.input, self.W_xi)\n        self.Wfx = T.dot(self.input, self.W_xf)\n        self.Wcx = T.dot(self.input, self.W_xc)\n        self.Wox = T.dot(self.input, self.W_xo)\n\n        [self.h, self.c, self.y], _ = theano.scan(self.recurrent_fn, sequences = [self.Wix, self.Wfx, self.Wcx, self.Wox],\n                                                             outputs_info = [self.h0, self.c0, self.y0])\n\n        self.output = self.y\n\n\n    def recurrent_fn(self, Wix, Wfx, Wcx, Wox, h_tm1, c_tm1=None, y_tm1=None):\n        """""" This implements a genetic recurrent function, called by self.__init__().\n\n        :param Wix: pre-computed matrix applying the weight matrix W on  the input units, for input gate\n        :param Wfx: Similar to Wix, but for forget gate\n        :param Wcx: Similar to Wix, but for cell memory\n        :param Wox: Similar to Wox, but for output gate\n        :param h_tm1: hidden activation from previous time step\n        :param c_tm1: activation from cell memory from previous time step\n        :returns: h_t is the hidden activation of current time step, and c_t is the activation for cell memory of current time step\n        """"""\n\n        h_t, c_t, y_t = self.lstm_as_activation_function(Wix, Wfx, Wcx, Wox, h_tm1, c_tm1, y_tm1)\n\n        return h_t, c_t, y_t\n\n    def lstm_as_activation_function(self):\n        """""" A genetic recurrent activation function for variants of LSTM architectures.\n        The function is called by self.recurrent_fn().\n\n        """"""\n        pass\n\nclass VanillaLstm(LstmBase):\n    """""" This class implements the standard LSTM block, inheriting the genetic class :class:`layers.gating.LstmBase`.\n\n    """"""\n\n\n    def __init__(self, rng, x, n_in, n_h, p=0.0, training=0, rnn_batch_training=False):\n        """""" Initialise a vanilla LSTM block\n\n        :param rng: random state, fixed value for randome state for reproducible objective results\n        :param x: input to a network\n        :param n_in: number of input features\n        :type n_in: integer\n        :param n_h: number of hidden units\n        :type n_h: integer\n        """"""\n\n        LstmBase.__init__(self, rng, x, n_in, n_h, p, training, rnn_batch_training)\n\n        self.params = [self.W_xi, self.W_hi, self.w_ci,\n                       self.W_xf, self.W_hf, self.w_cf,\n                       self.W_xo, self.W_ho, self.w_co,\n                       self.W_xc, self.W_hc,\n                       self.b_i, self.b_f, self.b_o, self.b_c]\n\n    def lstm_as_activation_function(self, Wix, Wfx, Wcx, Wox, h_tm1, c_tm1):\n        """""" This function treats the LSTM block as an activation function, and implements the standard LSTM activation function.\n            The meaning of each input and output parameters can be found in :func:`layers.gating.LstmBase.recurrent_fn`\n\n        """"""\n\n        i_t = T.nnet.sigmoid(Wix + T.dot(h_tm1, self.W_hi) + self.w_ci * c_tm1 + self.b_i)  #\n        f_t = T.nnet.sigmoid(Wfx + T.dot(h_tm1, self.W_hf) + self.w_cf * c_tm1 + self.b_f)  #\n\n        c_t = f_t * c_tm1 + i_t * T.tanh(Wcx + T.dot(h_tm1, self.W_hc) + self.b_c)\n\n        o_t = T.nnet.sigmoid(Wox + T.dot(h_tm1, self.W_ho) + self.w_co * c_t + self.b_o)\n\n        h_t = o_t * T.tanh(c_t)\n\n        return h_t, c_t#, i_t, f_t, o_t\n\nclass VanillaLstmDecoder(LstmDecoderBase):\n    """""" This class implements the standard LSTM block, inheriting the genetic class :class:`layers.gating.LstmBase`.\n\n    """"""\n\n\n    def __init__(self, rng, x, n_in, n_h, n_out, p=0.0, training=0, rnn_batch_training=False):\n        """""" Initialise a vanilla LSTM block\n\n        :param rng: random state, fixed value for randome state for reproducible objective results\n        :param x: input to a network\n        :param n_in: number of input features\n        :type n_in: integer\n        :param n_h: number of hidden units\n        :type n_h: integer\n        """"""\n\n        self.n_out = int(n_out)\n\n        LstmDecoderBase.__init__(self, rng, x, n_in, n_h, n_out, p, training, rnn_batch_training)\n\n        self.params = [self.W_xi, self.W_hi, self.w_ci, self.W_yi,\n                       self.W_xf, self.W_hf, self.w_cf,\n                       self.W_xo, self.W_ho, self.w_co,\n                       self.W_xc, self.W_hc,\n                       self.U_ho,\n                       self.b_i, self.b_f, self.b_o, self.b_c, self.b]\n\n    def lstm_as_activation_function(self, Wix, Wfx, Wcx, Wox, h_tm1, c_tm1, y_tm1):\n        """""" This function treats the LSTM block as an activation function, and implements the standard LSTM activation function.\n            The meaning of each input and output parameters can be found in :func:`layers.gating.LstmBase.recurrent_fn`\n\n        """"""\n\n        i_t = T.nnet.sigmoid(Wix + T.dot(h_tm1, self.W_hi) + self.w_ci * c_tm1 + self.b_i)  #\n        f_t = T.nnet.sigmoid(Wfx + T.dot(h_tm1, self.W_hf) + self.w_cf * c_tm1 + self.b_f)  #\n\n        c_t = f_t * c_tm1 + i_t * T.tanh(Wcx + T.dot(h_tm1, self.W_hc) + T.dot(y_tm1, self.W_yi) + self.b_c)\n\n        o_t = T.nnet.sigmoid(Wox + T.dot(h_tm1, self.W_ho) + self.w_co * c_t + self.b_o)\n\n        h_t = o_t * T.tanh(c_t)\n\n        y_t = T.dot(h_t, self.U_ho) + self.b\n\n        return h_t, c_t, y_t     #, i_t, f_t, o_t\n\nclass SimplifiedLstmDecoder(LstmDecoderBase):\n    """""" This class implements a simplified LSTM block which only keeps the forget gate, inheriting the genetic class :class:`layers.gating.LstmBase`.\n    \n    """"""\n\n    def __init__(self, rng, x, n_in, n_h, n_out, p=0.0, training=0, rnn_batch_training=False):\n        """""" Initialise a LSTM with only the forget gate\n        \n        :param rng: random state, fixed value for randome state for reproducible objective results\n        :param x: input to a network\n        :param n_in: number of input features\n        :type n_in: integer\n        :param n_h: number of hidden units\n        :type n_h: integer\n        """"""\n        \n        self.n_out = int(n_out)\n\n        LstmDecoderBase.__init__(self, rng, x, n_in, n_h, n_out, p, training, rnn_batch_training)\n\n        self.params = [self.W_yi,\n                       self.W_xf, self.W_hf,\n                       self.W_xc, self.W_hc,\n                       self.U_ho,\n                       self.b_f,  self.b_c, self.b]\n                       \n    def lstm_as_activation_function(self, Wix, Wfx, Wcx, Wox, h_tm1, c_tm1, y_tm1):\n        """""" This function treats the LSTM block as an activation function, and implements the LSTM (simplified LSTM) activation function.\n            The meaning of each input and output parameters can be found in :func:`layers.gating.LstmBase.recurrent_fn`\n        \n        """"""\n    \n        f_t = T.nnet.sigmoid(Wfx + T.dot(h_tm1, self.W_hf) + self.b_f)  #self.w_cf * c_tm1 \n    \n        c_t = f_t * c_tm1 + (1 - f_t) * T.tanh(Wcx + T.dot(h_tm1, self.W_hc) + T.dot(y_tm1, self.W_yi) + self.b_c) \n\n        h_t = T.tanh(c_t)\n\n        y_t = T.dot(h_t, self.U_ho) + self.b\n\n        return h_t, c_t, y_t\n\nclass LstmNFG(LstmBase):\n    """""" This class implements a LSTM block without the forget gate, inheriting the genetic class :class:`layers.gating.LstmBase`.\n\n    """"""\n    def __init__(self, rng, x, n_in, n_h, p=0.0, training=0, rnn_batch_training=False):\n        """""" Initialise a LSTM with the forget gate\n\n        :param rng: random state, fixed value for randome state for reproducible objective results\n        :param x: input to a network\n        :param n_in: number of input features\n        :type n_in: integer\n        :param n_h: number of hidden units\n        :type n_h: integer\n        """"""\n\n        LstmBase.__init__(self, rng, x, n_in, n_h, p, training, rnn_batch_training)\n\n        self.params = [self.W_xi, self.W_hi, self.w_ci,\n                       self.W_xo, self.W_ho, self.w_co,\n                       self.W_xc, self.W_hc,\n                       self.b_i, self.b_o, self.b_c]\n\n    def lstm_as_activation_function(self, Wix, Wfx, Wcx, Wox, h_tm1, c_tm1):\n        """""" This function treats the LSTM block as an activation function, and implements the LSTM (without the forget gate) activation function.\n            The meaning of each input and output parameters can be found in :func:`layers.gating.LstmBase.recurrent_fn`\n\n        """"""\n\n        i_t = T.nnet.sigmoid(Wix + T.dot(h_tm1, self.W_hi) + self.w_ci * c_tm1 + self.b_i)  #\n\n        c_t = c_tm1 + i_t * T.tanh(Wcx + T.dot(h_tm1, self.W_hc) + self.b_c)  #f_t *\n\n        o_t = T.nnet.sigmoid(Wox + T.dot(h_tm1, self.W_ho) + self.w_co * c_t + self.b_o)\n\n        h_t = o_t * T.tanh(c_t)\n\n        return h_t, c_t\n\nclass LstmNIG(LstmBase):\n    """""" This class implements a LSTM block without the input gate, inheriting the genetic class :class:`layers.gating.LstmBase`.\n\n    """"""\n\n    def __init__(self, rng, x, n_in, n_h, p=0.0, training=0, rnn_batch_training=False):\n        """""" Initialise a LSTM with the input gate\n\n        :param rng: random state, fixed value for randome state for reproducible objective results\n        :param x: input to a network\n        :param n_in: number of input features\n        :type n_in: integer\n        :param n_h: number of hidden units\n        :type n_h: integer\n        """"""\n\n        LstmBase.__init__(self, rng, x, n_in, n_h, p, training, rnn_batch_training)\n\n        self.params = [self.W_xf, self.W_hf, self.w_cf,\n                       self.W_xo, self.W_ho, self.w_co,\n                       self.W_xc, self.W_hc,\n                       self.b_f, self.b_o, self.b_c]\n\n    def lstm_as_activation_function(self, Wix, Wfx, Wcx, Wox, h_tm1, c_tm1):\n        """""" This function treats the LSTM block as an activation function, and implements the LSTM (without the input gate) activation function.\n            The meaning of each input and output parameters can be found in :func:`layers.gating.LstmBase.recurrent_fn`\n\n        """"""\n\n        f_t = T.nnet.sigmoid(Wfx + T.dot(h_tm1, self.W_hf) + self.w_cf * c_tm1 + self.b_f)  #\n\n        c_t = f_t * c_tm1 + T.tanh(Wcx + T.dot(h_tm1, self.W_hc) + self.b_c)  #i_t *\n\n        o_t = T.nnet.sigmoid(Wox + T.dot(h_tm1, self.W_ho) + self.w_co * c_t + self.b_o)\n\n        h_t = o_t * T.tanh(c_t)\n\n        return h_t, c_t\n\n\nclass LstmNOG(LstmBase):\n    """""" This class implements a LSTM block without the output gate, inheriting the genetic class :class:`layers.gating.LstmBase`.\n\n    """"""\n\n    def __init__(self, rng, x, n_in, n_h, p=0.0, training=0, rnn_batch_training=False):\n        """""" Initialise a LSTM with the output gate\n\n        :param rng: random state, fixed value for randome state for reproducible objective results\n        :param x: input to a network\n        :param n_in: number of input features\n        :type n_in: integer\n        :param n_h: number of hidden units\n        :type n_h: integer\n        """"""\n\n        LstmBase.__init__(self, rng, x, n_in, n_h, p, training, rnn_batch_training)\n\n        self.params = [self.W_xi, self.W_hi, self.w_ci,\n                       self.W_xf, self.W_hf, self.w_cf,\n                       self.W_xc, self.W_hc,\n                       self.b_i, self.b_f,\n                       self.b_c]\n\n    def lstm_as_activation_function(self, Wix, Wfx, Wcx, Wox, h_tm1, c_tm1):\n        """""" This function treats the LSTM block as an activation function, and implements the LSTM (without the output gate) activation function.\n            The meaning of each input and output parameters can be found in :func:`layers.gating.LstmBase.recurrent_fn`\n\n        """"""\n\n        i_t = T.nnet.sigmoid(Wix + T.dot(h_tm1, self.W_hi) + self.w_ci * c_tm1 + self.b_i)  #\n        f_t = T.nnet.sigmoid(Wfx + T.dot(h_tm1, self.W_hf) + self.w_cf * c_tm1 + self.b_f)  #\n\n        c_t = f_t * c_tm1 + i_t * T.tanh(Wcx + T.dot(h_tm1, self.W_hc) + self.b_c)  #i_t *\n\n        h_t = T.tanh(c_t)\n\n        return h_t, c_t\n\n\nclass LstmNoPeepholes(LstmBase):\n    """""" This class implements a LSTM block without the peephole connections, inheriting the genetic class :class:`layers.gating.LstmBase`.\n\n    """"""\n\n    def __init__(self, rng, x, n_in, n_h, p=0.0, training=0, rnn_batch_training=False):\n        """""" Initialise a LSTM with the peephole connections\n\n        :param rng: random state, fixed value for randome state for reproducible objective results\n        :param x: input to a network\n        :param n_in: number of input features\n        :type n_in: integer\n        :param n_h: number of hidden units\n        :type n_h: integer\n        """"""\n\n        LstmBase.__init__(self, rng, x, n_in, n_h, p, training, rnn_batch_training)\n\n        self.params = [self.W_xi, self.W_hi, #self.W_ci,\n                       self.W_xf, self.W_hf, #self.W_cf,\n                       self.W_xo, self.W_ho, #self.W_co,\n                       self.W_xc, self.W_hc,\n                       self.b_i, self.b_f,\n                       self.b_o, self.b_c]\n\n    def lstm_as_activation_function(self, Wix, Wfx, Wcx, Wox, h_tm1, c_tm1):\n        """""" This function treats the LSTM block as an activation function, and implements the LSTM (without the output gate) activation function.\n            The meaning of each input and output parameters can be found in :func:`layers.gating.LstmBase.recurrent_fn`\n\n        """"""\n\n        i_t = T.nnet.sigmoid(Wix + T.dot(h_tm1, self.W_hi) + self.b_i)\n        f_t = T.nnet.sigmoid(Wfx + T.dot(h_tm1, self.W_hf) + self.b_f)\n\n        c_t = f_t * c_tm1 + i_t * T.tanh(Wcx + T.dot(h_tm1, self.W_hc) + self.b_c)\n\n        o_t = T.nnet.sigmoid(Wox + T.dot(h_tm1, self.W_ho) + self.b_o)\n\n        h_t = o_t * T.tanh(c_t)\n\n        return h_t, c_t\n\n\nclass SimplifiedLstm(LstmBase):\n    """""" This class implements a simplified LSTM block which only keeps the forget gate, inheriting the genetic class :class:`layers.gating.LstmBase`.\n\n    """"""\n\n    def __init__(self, rng, x, n_in, n_h, p=0.0, training=0, rnn_batch_training=False):\n        """""" Initialise a LSTM with only the forget gate\n\n        :param rng: random state, fixed value for randome state for reproducible objective results\n        :param x: input to a network\n        :param n_in: number of input features\n        :type n_in: integer\n        :param n_h: number of hidden units\n        :type n_h: integer\n        """"""\n\n        LstmBase.__init__(self, rng, x, n_in, n_h, p, training, rnn_batch_training)\n\n        self.params = [self.W_xf, self.W_hf,\n                       self.W_xc, self.W_hc,\n                       self.b_f,  self.b_c]\n\n        self.L2_cost = (self.W_xf ** 2).sum() + (self.W_hf ** 2).sum() + (self.W_xc ** 2).sum() + (self.W_hc ** 2).sum()\n\n    def lstm_as_activation_function(self, Wix, Wfx, Wcx, Wox, h_tm1, c_tm1):\n        """""" This function treats the LSTM block as an activation function, and implements the LSTM (simplified LSTM) activation function.\n            The meaning of each input and output parameters can be found in :func:`layers.gating.LstmBase.recurrent_fn`\n\n        """"""\n\n        f_t = T.nnet.sigmoid(Wfx + T.dot(h_tm1, self.W_hf) + self.b_f)  #self.w_cf * c_tm1\n\n        c_t = f_t * c_tm1 + (1 - f_t) * T.tanh(Wcx + T.dot(h_tm1, self.W_hc) + self.b_c)\n\n        h_t = T.tanh(c_t)\n\n        return h_t, c_t\n\nclass SimplifiedGRU(LstmBase):\n    """""" This class implements a simplified GRU block which only keeps the forget gate, inheriting the genetic class :class:`layers.gating.LstmBase`.\n\n    """"""\n\n    def __init__(self, rng, x, n_in, n_h, p=0.0, training=0, rnn_batch_training=False):\n        """""" Initialise a LSTM with the the forget gate\n\n        :param rng: random state, fixed value for randome state for reproducible objective results\n        :param x: input to a network\n        :param n_in: number of input features\n        :type n_in: integer\n        :param n_h: number of hidden units\n        :type n_h: integer\n        """"""\n\n        LstmBase.__init__(self, rng, x, n_in, n_h, p, training, rnn_batch_training)\n\n        self.params = [self.W_xf, self.W_hf, self.w_cf,\n                       self.W_xc, self.W_hc,\n                       self.b_f,  self.b_c]\n\n        self.L2_cost = (self.W_xf ** 2).sum() + (self.W_hf ** 2).sum() + (self.W_xc ** 2).sum() + (self.W_hc ** 2).sum()\n\n    def lstm_as_activation_function(self, Wix, Wfx, Wcx, Wox, h_tm1, c_tm1):\n        """""" This function treats the LSTM block as an activation function, and implements the LSTM (simplified LSTM) activation function.\n            The meaning of each input and output parameters can be found in :func:`layers.gating.LstmBase.recurrent_fn`\n\n        """"""\n        ##can_h_t = T.tanh(Whx + r_t * T.dot(h_tm1, self.W_hh) + self.b_h)\n\n        f_t = T.nnet.sigmoid(Wfx + T.dot(h_tm1, self.W_hf) + self.b_f)  #self.w_cf * c_tm1\n\n        can_h_t = T.tanh(Wcx + f_t * T.dot(h_tm1, self.W_hc) + self.b_c)\n\n        h_t = self.w_cf * (1.0 - f_t) * h_tm1 + f_t * can_h_t\n        c_t = h_t\n\n#        c_t = f_t * c_tm1 + (1 - f_t) * T.tanh(Wcx + T.dot(h_tm1, self.W_hc) + self.b_c)\n\n#        h_t = T.tanh(c_t)\n\n        return h_t, c_t\n\nclass BidirectionSLstm(SimplifiedLstm):\n\n    def __init__(self, rng, x, n_in, n_h, n_out, p=0.0, training=0, rnn_batch_training=False):\n\n        fwd = SimplifiedLstm(rng, x, n_in, n_h, p, training, rnn_batch_training)\n        bwd = SimplifiedLstm(rng, x[::-1], n_in, n_h, p, training, rnn_batch_training)\n\n        self.params = fwd.params + bwd.params\n\n        self.output = T.concatenate([fwd.output, bwd.output[::-1]], axis=-1)\n\nclass BidirectionLstm(VanillaLstm):\n\n    def __init__(self, rng, x, n_in, n_h, n_out, p=0.0, training=0, rnn_batch_training=False):\n\n        fwd = VanillaLstm(rng, x, n_in, n_h, p, training, rnn_batch_training)\n        bwd = VanillaLstm(rng, x[::-1], n_in, n_h, p, training, rnn_batch_training)\n\n        self.params = fwd.params + bwd.params\n\n        self.output = T.concatenate([fwd.output, bwd.output[::-1]], axis=-1)\n\n\nclass RecurrentOutput(object):\n    def __init__(self, rng, x, n_in, n_out, p=0.0, training=0, rnn_batch_training=False):\n\n        self.W_h = theano.shared(value=np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_out), size=(n_in, n_out)), dtype=config.floatX), name=\'W_h\')\n        self.W_y = theano.shared(value=np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_out), size=(n_out, n_out)), dtype=config.floatX), name=\'W_y\')\n\n        self.b_y = theano.shared(value=np.zeros((n_out, ), dtype=config.floatX), name=\'b_y\')\n\n\n\n\n# Gated Recurrent Unit\nclass GatedRecurrentUnit(object):\n    """""" This class implements a gated recurrent unit (GRU), as proposed in Cho et al 2014 (http://arxiv.org/pdf/1406.1078.pdf).\n\n    """"""\n\n    def __init__(self, rng, x, n_in, n_h, p=0.0, training=0, rnn_batch_training=False):\n        """""" Initialise a gated recurrent unit\n\n        :param rng: random state, fixed value for randome state for reproducible objective results\n        :param x: input to a network\n        :param n_in: number of input features\n        :type n_in: integer\n        :param n_h: number of hidden units\n        :type n_h: integer\n        :param p: the probability of dropout\n        :param training: a binary value to indicate training or testing (for dropout training)\n        """"""\n\n        self.n_in = int(n_in)\n        self.n_h  = int(n_h)\n\n        self.rnn_batch_training = rnn_batch_training\n\n        self.input = x\n\n        if p > 0.0:\n            if training==1:\n                srng = RandomStreams(seed=123456)\n                self.input = T.switch(srng.binomial(size=x.shape,p=p), x, 0)\n            else:\n                self.input =  (1-p) * x\n\n        self.W_xz = theano.shared(value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in),\n                     size=(n_in, n_h)), dtype=config.floatX), name = \'W_xz\')\n        self.W_hz = theano.shared(value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h),\n                     size=(n_h, n_h)), dtype=config.floatX), name = \'W_hz\')\n\n        self.W_xr = theano.shared(value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in),\n                     size=(n_in, n_h)), dtype=config.floatX), name = \'W_xr\')\n        self.W_hr = theano.shared(value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h),\n                     size=(n_h, n_h)), dtype=config.floatX), name = \'W_hr\')\n\n        self.W_xh = theano.shared(value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in),\n                     size=(n_in, n_h)), dtype=config.floatX), name = \'W_xh\')\n        self.W_hh = theano.shared(value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h),\n                     size=(n_h, n_h)), dtype=config.floatX), name = \'W_hh\')\n\n        self.b_z = theano.shared(value = np.zeros((n_h, ), dtype = config.floatX), name = \'b_z\')\n\n        self.b_r = theano.shared(value = np.zeros((n_h, ), dtype = config.floatX), name = \'b_r\')\n\n        self.b_h = theano.shared(value = np.zeros((n_h, ), dtype = config.floatX), name = \'b_h\')\n\n        if self.rnn_batch_training:\n            self.h0 = theano.shared(value=np.zeros((1, n_h), dtype = config.floatX), name = \'h0\')\n            self.c0 = theano.shared(value=np.zeros((1, n_h), dtype = config.floatX), name = \'c0\')\n\n            self.h0 = T.repeat(self.h0, x.shape[1], 0)\n            self.c0 = T.repeat(self.c0, x.shape[1], 0)\n        else:\n            self.h0 = theano.shared(value=np.zeros((n_h, ), dtype = config.floatX), name = \'h0\')\n            self.c0 = theano.shared(value=np.zeros((n_h, ), dtype = config.floatX), name = \'c0\')\n\n\n        ## pre-compute these for fast computation\n        self.Wzx = T.dot(self.input, self.W_xz)\n        self.Wrx = T.dot(self.input, self.W_xr)\n        self.Whx = T.dot(self.input, self.W_xh)\n\n        [self.h, self.c], _ = theano.scan(self.gru_as_activation_function,\n                                               sequences = [self.Wzx, self.Wrx, self.Whx],\n                                               outputs_info = [self.h0, self.c0])  #\n\n\n        self.output = self.h\n\n        self.params = [self.W_xz, self.W_hz, self.W_xr, self.W_hr, self.W_xh, self.W_hh,\n                       self.b_z, self.b_r, self.b_h]\n\n        self.L2_cost = (self.W_xz ** 2).sum() + (self.W_hz ** 2).sum() + (self.W_xr ** 2).sum() + (self.W_hr ** 2).sum() + (self.W_xh ** 2).sum() + (self.W_hh ** 2).sum()\n\n    def gru_as_activation_function(self, Wzx, Wrx, Whx, h_tm1, c_tm1 = None):\n        """""" This function treats the GRU block as an activation function, and implements the GRU activation function.\n            This function is called by :func:`layers.gating.GatedRecurrentUnit.__init__`.\n            Wzx, Wrx, Whx have been pre-computed before passing to this function.\n\n            To make the same interface as LSTM, we keep a c_tm1 (means the cell state of previous time step, but GRU does not maintain a cell state).\n        """"""\n\n        z_t = T.nnet.sigmoid(Wzx + T.dot(h_tm1, self.W_hz) + self.b_z)\n        r_t = T.nnet.sigmoid(Wrx + T.dot(h_tm1, self.W_hr) + self.b_r)\n        can_h_t = T.tanh(Whx + r_t * T.dot(h_tm1, self.W_hh) + self.b_h)\n\n        h_t = (1 - z_t) * h_tm1 + z_t * can_h_t\n\n        c_t = h_t   ## in order to have the same interface as LSTM\n\n        return h_t, c_t\n'"
src/layers/layers.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n\nimport numpy, time, pickle, gzip, sys, os, copy\n\nimport theano\nimport theano.tensor as T\nfrom theano.tensor.shared_randomstreams import RandomStreams\nfrom theano.ifelse import ifelse \n\nimport logging\n\n\nclass MixtureDensityOutputLayer(object):\n    def __init__(self, rng, input, n_in, n_out, n_component, var_floor):\n        self.input = input\n\n        W_value = rng.normal(0.0, 1.0/numpy.sqrt(n_in), size=(n_in, n_out*n_component))\n        self.W_mu = theano.shared(value=numpy.asarray(W_value, dtype=theano.config.floatX), name=\'W_mu\', borrow=True)\n\n        self.W_sigma = theano.shared(value=numpy.asarray(W_value.copy(), dtype=theano.config.floatX), name=\'W_sigma\', borrow=True)\n\n        W_mix_value = rng.normal(0.0, 1.0/numpy.sqrt(n_in), size=(n_in, n_component))\n        self.W_mix = theano.shared(value=numpy.asarray(W_mix_value, dtype=theano.config.floatX), name=\'W_mix\', borrow=True)\n\n        self.mu = T.dot(self.input, self.W_mu) # assume linear output for mean vectors\n\n        #self.sigma = T.nnet.softplus(T.dot(self.input, self.W_sigma)) # + 0.0001\n        self.sigma = T.exp(T.dot(self.input, self.W_sigma)) # Zen et al. 2014\n        self.sigma = T.maximum(var_floor, self.sigma) # hard variance flooring\n        # note: sigma contains variances, so var_floor=0.01 means that\n        # the lowest possible standard deviation is 0.1\n\n        self.mix = T.nnet.softmax(T.dot(self.input, self.W_mix))\n\n        self.delta_W_mu    = theano.shared(value = numpy.zeros((n_in, n_out*n_component),\n                                           dtype=theano.config.floatX), name=\'delta_W_mu\')\n        self.delta_W_sigma = theano.shared(value = numpy.zeros((n_in, n_out*n_component),\n                                           dtype=theano.config.floatX), name=\'delta_W_sigma\')\n        self.delta_W_mix   = theano.shared(value = numpy.zeros((n_in, n_component),\n                                           dtype=theano.config.floatX), name=\'delta_W_mix\')\n\n\n        self.params = [self.W_mu, self.W_sigma, self.W_mix]\n        self.delta_params = [self.delta_W_mu, self.delta_W_sigma, self.delta_W_mix]\n\n\n\nclass LinearLayer(object):\n    def __init__(self, rng, input, n_in, n_out, W = None, b = None):\n        n_in = int(n_in)  # ensure sizes have integer type\n        n_out = int(n_out)# ensure sizes have integer type\n\n        self.input = input\n\n        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n        if W is None:\n            W_value = rng.normal(0.0, 1.0/numpy.sqrt(n_in), size=(n_in, n_out))\n            W = theano.shared(value=numpy.asarray(W_value, dtype=theano.config.floatX), name=\'W\', borrow=True)\n\n        if b is None:\n            b = theano.shared(value=numpy.zeros((n_out,),\n                                        dtype=theano.config.floatX),\n                                   name=\'b\', borrow=True)\n\n        self.W = W\n        self.b = b\n\n        self.delta_W = theano.shared(value = numpy.zeros((n_in,n_out),\n                                     dtype=theano.config.floatX), name=\'delta_W\')\n\n        self.delta_b = theano.shared(value = numpy.zeros_like(self.b.get_value(borrow=True),\n                                     dtype=theano.config.floatX), name=\'delta_b\')\n\n        self.output = T.dot(self.input, self.W) + self.b\n\n        self.params = [self.W, self.b]\n        self.delta_params = [self.delta_W, self.delta_b]\n\n    def errors(self, y):\n        L = T.sum( (self.output-y)*(self.output-y), axis=1 )\n        errors = T.mean(L)\n        return (errors)\n\n    def init_params(self, iparams):\n        updates = {}\n        for param, iparam in zip(self.params, iparams):\n            updates[param] = iparam\n        return updates\n\nclass SigmoidLayer(object):\n    def __init__(self, rng, x, n_in, n_out, W = None, b = None, activation = T.tanh, p=0.0, training=0):\n        n_in = int(n_in)  # ensure sizes have integer type\n        n_out = int(n_out)# ensure sizes have integer type\n\n        self.x = x\n\n        srng = RandomStreams(seed=123456)\n        \n        def _drop(srng,x, p):\n                mask = srng.binomial(n=1, p=1.0-p, size=x.shape)\n                return x * T.cast(mask, theano.config.floatX)\n                \n        if p > 0.0:\n            self.x = ifelse(T.eq(training,numpy.cast[\'int32\'](1)), _drop(srng,x,p) , numpy.cast[theano.config.floatX](1.0-p) * x )\n            \n        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n        if W is None:\n            W_value = numpy.asarray(rng.normal(0.0, 1.0/numpy.sqrt(n_in),\n                      size=(n_in, n_out)), dtype=theano.config.floatX)\n            W = theano.shared(value=W_value,\n                              name=\'W\', borrow=True)\n        if b is None:\n            b = theano.shared(value=numpy.zeros((n_out,),\n                              dtype=theano.config.floatX),\n                              name=\'b\', borrow=True)\n\n        self.W = W\n        self.b = b\n\n        self.delta_W = theano.shared(value = numpy.zeros((n_in,n_out),\n                                     dtype=theano.config.floatX), name=\'delta_W\')\n\n        self.delta_b = theano.shared(value = numpy.zeros_like(self.b.get_value(borrow=True),\n                                     dtype=theano.config.floatX), name=\'delta_b\')\n\n        self.output = T.dot(self.x, self.W) + self.b\n        self.output = activation(self.output)\n\n        self.params = [self.W, self.b]\n        self.delta_params = [self.delta_W, self.delta_b]\n\n    def errors(self, y):\n        L = T.sum( (self.output-y)*(self.output-y), axis=1 )\n        errors = T.mean(L)\n        return (errors)\n\n    def init_params(self, iparams):\n        updates = {}\n        for param, iparam in zip(self.params, iparams):\n            updates[param] = iparam\n        return updates\n\n\nclass GeneralLayer(object):\n\n    def __init__(self, rng, x, n_in, n_out, W = None, b = None, activation = \'linear\', p=0.0, training=0):\n        \'\'\'\n        General feed-forward layer with any activation\n        \'\'\'\n        logger = logging.getLogger(\'general_layer\')\n        \n        n_in  = int(n_in)  # ensure sizes have integer type\n        n_out = int(n_out)# ensure sizes have integer type\n\n        self.x = x\n\n        srng = RandomStreams(seed=123456)\n\n        def _drop(srng,x, p):\n                mask = srng.binomial(n=1, p=1.0-p, size=x.shape)\n                return x * T.cast(mask, theano.config.floatX)\n\n        if p > 0.0:\n            self.x = ifelse(T.eq(training,numpy.cast[\'int32\'](1)), _drop(srng,x,p) , numpy.cast[theano.config.floatX](1.0-p) * x )\n                    \n        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n        if W is None:\n            W_value = numpy.asarray(rng.normal(0.0, 1.0/numpy.sqrt(n_in),\n                      size=(n_in, n_out)), dtype=theano.config.floatX)\n            W = theano.shared(value=W_value,\n                              name=\'W\', borrow=True)\n        if b is None:\n            b = theano.shared(value=numpy.zeros((n_out,),\n                              dtype=theano.config.floatX),\n                              name=\'b\', borrow=True)\n\n        self.W = W\n        self.b = b\n\n        self.delta_W = theano.shared(value = numpy.zeros((n_in,n_out),\n                                     dtype=theano.config.floatX), name=\'delta_W\')\n\n        self.delta_b = theano.shared(value = numpy.zeros_like(self.b.get_value(borrow=True),\n                                     dtype=theano.config.floatX), name=\'delta_b\')\n\n        self.output = T.dot(self.x, self.W) + self.b\n\n        if activation == \'sigmoid\':\n            self.output = T.nnet.sigmoid(self.output)\n\n        elif activation == \'softmax\':\n            self.output = T.nnet.softmax(self.output)\n\n        elif activation == \'tanh\':\n            self.output = T.tanh(self.output)\n\n        elif activation == \'relu\':  ## rectifier linear unit\n            self.output = T.maximum(0.0, self.output)\n\n        elif activation == \'resu\':  ## rectifier smooth unit\n            self.output = numpy.log(1.0 + numpy.exp(self.output))\n\n        elif activation == \'linear\':\n            pass;\n\n        else:\n            logger.critical(\'the input activation function: %s is not supported right now. Please modify layers.py to support\' % (activation))\n            raise\n\n        # parameters of the model\n        self.params = [self.W, self.b]\n        self.delta_params = [self.delta_W, self.delta_b]\n    \n    def errors(self, y):\n        errors = T.mean(T.sum((self.output-y)**2, axis=1))\n\n        return errors\n\n    def init_params(self, iparams):\n        updates = {}\n        for param, iparam in zip(self.params, iparams):\n            updates[param] = iparam\n        return updates\n\n\nclass HiddenLayer(object):\n    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n                 activation=T.tanh, do_maxout = False, pool_size = 1,\n                 do_pnorm = False, pnorm_order = 1):\n        """""" Class for hidden layer """"""\n        self.input = input\n        self.n_in = n_in\n        self.n_out = n_out\n\n        if W is None:\n\n            W_values = numpy.asarray(rng.normal(0.0, 1.0/numpy.sqrt(n_in),\n                    size=(n_in, n_out)), dtype=theano.config.floatX)\n\n            W = theano.shared(value=W_values, name=\'W\', borrow=True)\n\n        if b is None:\n            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n            b = theano.shared(value=b_values, name=\'b\', borrow=True)\n\n        self.W = W\n        self.b = b\n\n        self.delta_W = theano.shared(value = numpy.zeros((n_in,n_out),\n                                     dtype=theano.config.floatX), name=\'delta_W\')\n\n        self.delta_b = theano.shared(value = numpy.zeros_like(self.b.get_value(borrow=True),\n                                     dtype=theano.config.floatX), name=\'delta_b\')\n\n        lin_output = T.dot(input, self.W) + self.b\n        if do_maxout == True:\n            self.last_start = n_out - pool_size\n            self.tmp_output = lin_output[:,0:self.last_start+1:pool_size]\n            for i in range(1, pool_size):\n                cur = lin_output[:,i:self.last_start+i+1:pool_size]\n                self.tmp_output = T.maximum(cur, self.tmp_output)\n            self.output = activation(self.tmp_output)\n        elif do_pnorm == True:\n            self.last_start = n_out - pool_size\n            self.tmp_output = abs(lin_output[:,0:self.last_start+1:pool_size]) ** pnorm_order\n            for i in range(1, pool_size):\n                cur = abs(lin_output[:,i:self.last_start+i+1:pool_size]) ** pnorm_order\n                self.tmp_output = self.tmp_output + cur\n            self.tmp_output = self.tmp_output ** (1.0 / pnorm_order)\n            self.output = activation(self.tmp_output)\n        else:\n            self.output = (lin_output if activation is None\n                           else activation(lin_output))\n\n#        self.output = self.rectifier_linear(lin_output)\n\n        # parameters of the model\n        self.params = [self.W, self.b]\n        self.delta_params = [self.delta_W, self.delta_b]\n\n    def rectifier_linear(self, x):\n        x = T.maximum(0.0, x)\n\n        return  x\n\n    def rectifier_smooth(self, x):\n        x = numpy.log(1.0 + numpy.exp(x))\n\n        return  x\n\n\nclass SplitHiddenLayer(object):\n    \'\'\'\n    The nin x nout matrix is vertically split into 2 portions which can be updated\n    independently.\n\n    n_in1 -- by convention, use this part for subword contexts\n    n_in2 -- by convention, use this part for word projections\n\n    Bias is not split in any way.\n    \'\'\'\n    def __init__(self, rng, input, n_in1, n_in2, n_out, W1=None, W2=None, b=None,\n                 activation=T.tanh, do_maxout = False, pool_size = 1,\n                 do_pnorm = False, pnorm_order = 1):\n        """""" Class for hidden layer """"""\n        self.input = input\n        #self.n_in = n_in\n        self.n_out = n_out\n\n        if W1 is None:\n\n            W1_values = numpy.asarray(rng.normal(0.0, 1.0/numpy.sqrt(n_in1),\n                    size=(n_in1, n_out)), dtype=theano.config.floatX)\n\n            W1 = theano.shared(value=W1_values, name=\'W1\', borrow=True)\n\n        if W2 is None:\n\n            W2_values = numpy.asarray(rng.normal(0.0, 1.0/numpy.sqrt(n_in1),\n                    size=(n_in2, n_out)), dtype=theano.config.floatX)\n\n            W2 = theano.shared(value=W2_values, name=\'W2\', borrow=True)\n\n        if b is None:\n            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n            b = theano.shared(value=b_values, name=\'b\', borrow=True)\n\n        self.W1 = W1\n        self.W2 = W2\n        self.b = b\n\n        self.delta_W1 = theano.shared(value = numpy.zeros((n_in1,n_out),\n                                     dtype=theano.config.floatX), name=\'delta_W1\')\n\n        self.delta_W2 = theano.shared(value = numpy.zeros((n_in2,n_out),\n                                     dtype=theano.config.floatX), name=\'delta_W2\')\n\n        self.delta_b = theano.shared(value = numpy.zeros_like(self.b.get_value(borrow=True),\n                                     dtype=theano.config.floatX), name=\'delta_b\')\n\n        lin_output = T.dot(input, T.concatenate([self.W1, self.W2])) + self.b\n        if do_maxout == True:\n            self.last_start = n_out - pool_size\n            self.tmp_output = lin_output[:,0:self.last_start+1:pool_size]\n            for i in range(1, pool_size):\n                cur = lin_output[:,i:self.last_start+i+1:pool_size]\n                self.tmp_output = T.maximum(cur, self.tmp_output)\n            self.output = activation(self.tmp_output)\n        elif do_pnorm == True:\n            self.last_start = n_out - pool_size\n            self.tmp_output = abs(lin_output[:,0:self.last_start+1:pool_size]) ** pnorm_order\n            for i in range(1, pool_size):\n                cur = abs(lin_output[:,i:self.last_start+i+1:pool_size]) ** pnorm_order\n                self.tmp_output = self.tmp_output + cur\n            self.tmp_output = self.tmp_output ** (1.0 / pnorm_order)\n            self.output = activation(self.tmp_output)\n        else:\n            self.output = (lin_output if activation is None\n                           else activation(lin_output))\n\n#        self.output = self.rectifier_linear(lin_output)\n\n        # parameters of the model\n        self.params = [self.W1, self.W2, self.b]\n        self.delta_params = [self.delta_W1, self.delta_W2, self.delta_b]\n\n    def rectifier_linear(self, x):\n        x = T.maximum(0.0, x)\n\n        return  x\n\n    def rectifier_smooth(self, x):\n        x = numpy.log(1.0 + numpy.exp(x))\n\n        return  x\n\n\n\n\n\n\nclass TokenProjectionLayer(object):\n    \'\'\'\n    A single projection, not shared. MErging or outputs with non-projected inputs is handled elsewhere.\n    \'\'\'\n\n    def __init__(self, rng, input, projection_insize, projection_outsize, initial_projection_distrib=\'gaussian\'):\n\n        self.input = input\n        self.params = []\n        self.delta_params = []\n        #self.n_in = n_in\n        self.projection_insize = projection_insize\n        self.projection_outsize = projection_outsize\n\n\n        if initial_projection_distrib == \'gaussian\':\n            W_values = numpy.asarray(rng.normal(0.0, 0.1,\n                                    size=(projection_insize, projection_outsize)),\n                                    dtype=theano.config.floatX)\n        elif initial_projection_distrib == \'uniform\':\n\n            # W_values = numpy.asarray(rng.uniform(low=-0.02, high=0.02,\n            W_values = numpy.asarray(rng.uniform(low=0.0, high=1.0,\n            size=(projection_insize, projection_outsize)),\n            dtype=theano.config.floatX)\n\n        elif initial_projection_distrib == \'zeros\':\n\n            W_values = numpy.zeros((projection_insize, projection_outsize),\n            dtype=theano.config.floatX)\n\n        elif initial_projection_distrib == \'4mix\':\n\n            ## TODO -- generalise to other n_modes and higher deimneionsal CVs\n            means = [(-0.5, -0.5), (0.5, 0.5), (0.5, -0.5), (-0.5, 0.5)]\n            var = (0.1, 0.1)\n            W_prelim = []\n            for mean in means:\n                W_prelim.append(\n                    numpy.asarray(rng.normal(mean, var,\n                    size=(projection_insize / len(means), projection_outsize)),\n                    dtype=theano.config.floatX)\n                    )\n            W_values = numpy.vstack(W_prelim)\n            rng.shuffle(W_values)\n\n        else:\n\n            sys.exit( \'initial_projection_distrib must be one of: gaussian, uniform\' )\n\n        W = theano.shared(value=W_values, name=\'W\', borrow=True)\n\n        delta_W = theano.shared(value = numpy.zeros((projection_insize, projection_outsize),\n                                 dtype=theano.config.floatX), name=\'delta_W\')\n\n        self.params.append(W)\n        self.delta_params.append(delta_W)\n\n        self.output = T.dot(self.input, W)\n\n\n\nclass dA(object):\n    def __init__(self, theano_rng = None, input = None,\n                 n_visible= None, n_hidden= None, W = None, bhid = None,\n                 bvis = None, activation=None, firstlayer = 1, variance   = None ):\n\n        self.n_visible = n_visible\n        self.n_hidden  = n_hidden\n\n        if not W:\n            initial_W = numpy.asarray(theano_rng.normal(0.0, 1.0/numpy.sqrt(n_in),\n                    size=(n_visible, n_hidden)), dtype=theano.config.floatX)\n            W = theano.shared(value = initial_W, name =\'W\')\n            #initial_W = numpy.asarray( numpy_rng.uniform(\n            #          low  = -4*numpy.sqrt(6./(n_hidden+n_visible)),\n            #          high =  4*numpy.sqrt(6./(n_hidden+n_visible)),\n            #          size = (n_visible, n_hidden)),\n            #                           dtype = theano.config.floatX)\n\n        if not bvis:\n            bvis = theano.shared(value = numpy.zeros(n_visible,\n                                         dtype = theano.config.floatX))\n\n        if not bhid:\n            bhid = theano.shared(value = numpy.zeros(n_hidden,\n                                dtype = theano.config.floatX), name =\'b\')\n\n\n        self.W = W\n        self.b = bhid\n        self.b_prime = bvis\n        self.W_prime = self.W.T\n        self.theano_rng = theano_rng\n        self.activation = activation\n\n        if input == None :\n            self.x = T.dmatrix(name = \'input\')\n        else:\n            self.x = input\n\n        self.params = [self.W, self.b, self.b_prime]\n\n        # first layer, use Gaussian noise\n        self.firstlayer = firstlayer\n\n        if self.firstlayer == 1 :\n            if variance == None :\n                self.var = T.vector(name = \'input\')\n            else :\n                self.var = variance\n        else :\n            self.var = None\n\n    def apply_activation(self, lin_output, activation):\n        if activation == \'SIGMOID\':\n            final_output = T.nnet.sigmoid(lin_output)\n\n        elif activation == \'TANH\':\n            final_output = T.tanh(lin_output)\n\n        elif activation == \'LINEAR\':\n            final_output = lin_output\n\n        elif activation == \'ReLU\':  ## rectifier linear unit\n            final_output = T.maximum(0.0, lin_output)\n\n        elif activation == \'ReSU\':  ## rectifier smooth unit\n            final_output = numpy.log(1.0 + numpy.exp(lin_output))\n\n        else:\n            self.logger.critical(\'the input activation function: %s is not supported right now. Please modify layers.py to support\' % (activation))\n            raise\n\n        return final_output\n\n    def get_corrupted_input(self, input, corruption_level):\n        if self.firstlayer == 0 :\n            return  self.theano_rng.binomial(\n                             size = input.shape,\n                             n = 1,\n                             p = 1 - corruption_level,\n                             dtype=theano.config.floatX) * input\n        else :\n            noise = self.theano_rng.normal( size = input.shape,\n                                            dtype = theano.config.floatX)\n            denoises = noise * self.var * corruption_level\n            return input+denoises\n\n    def get_hidden_values(self, input):\n        return self.apply_activation((T.dot(input, self.W) + self.b), self.activation)\n\n    def get_reconstructed_input(self, hidden):\n        if self.firstlayer == 1 :\n            return T.dot(hidden, self.W_prime) + self.b_prime\n        else :\n            return self.apply_activation((T.dot(hidden, self.W_prime) + self.b_prime), self.activation)\n\n    def get_cost_updates(self, corruption_level, learning_rate):\n        #if corruption_level == 0:\n        #    tilde_x = self.x\n        #else:\n        #    tilde_x = self.get_corrupted_input(self.x, corruption_level)\n        tilde_x = self.x\n\n        y = self.get_hidden_values(tilde_x)\n        z = self.get_reconstructed_input(y)\n\n        L = T.sum ( (self.x-z) * (self.x-z), axis=1 )\n        cost = T.mean(L) / 2\n\n        gparams = T.grad(cost, self.params)\n        updates = {}\n        for param, gparam in zip(self.params, gparams):\n            updates[param] = param -  learning_rate*gparam\n\n        return (cost, updates)\n\n    def init_params(self, iparams):\n        updates = {}\n        for param, iparam in zip(self.params, iparams):\n            updates[param] = iparam\n        return updates\n\n    def get_test_cost(self, corruption_level):\n        """""" This function computes the cost and the updates for one trainng\n        step of the dA """"""\n\n        # tilde_x = self.get_corrupted_input(self.x, corruption_level, 0.5)\n        y       = self.get_hidden_values( self.x )\n        z       = self.get_reconstructed_input(y)\n        L = T.sum ( (self.x-z) * (self.x-z), axis=1)\n        cost = T.mean(L)\n\n        return cost\n'"
src/layers/lhuc_layer.py,0,"b'\nimport numpy, time, pickle, gzip, sys, os, copy\n\nimport theano\nimport theano.tensor as T\nfrom theano.tensor.shared_randomstreams import RandomStreams\n\nimport logging\n\n\nclass SigmoidLayer_LHUC(object):\n    def __init__(self, rng, x, n_in, n_out, W = None, b = None, c = None, activation = T.tanh, p=0.0, training=0):\n\n        self.x = x\n\n        if p > 0.0:\n            if training==1:\n                srng = RandomStreams(seed=123456)\n                self.x = T.switch(srng.binomial(size=x.shape, p=p), x, 0)\n            else:\n                self.x =  (1-p) * x\n\n\n        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n        if W is None:\n            W_value = numpy.asarray(rng.normal(0.0, 1.0/numpy.sqrt(n_in),\n                      size=(n_in, n_out)), dtype=theano.config.floatX)\n            W = theano.shared(value=W_value,\n                              name=\'W\', borrow=True)\n        if b is None:\n            b = theano.shared(value=numpy.zeros((n_out,),\n                              dtype=theano.config.floatX),\n                              name=\'b\', borrow=True)\n        if c is None:\n            c_value = numpy.asarray(rng.normal(0.0, 1.0/numpy.sqrt(n_out),\n                      size=(n_out,)), dtype=theano.config.floatX)\n            c = theano.shared(value=c_value, name=\'c\', borrow=True)\n\n        self.W = W\n        self.b = b\n        self.c = c\n\n\n        self.delta_W = theano.shared(value = numpy.zeros((n_in,n_out),\n                                     dtype=theano.config.floatX), name=\'delta_W\')\n\n        self.delta_b = theano.shared(value = numpy.zeros_like(self.b.get_value(borrow=True),\n                                     dtype=theano.config.floatX), name=\'delta_b\')\n\n        self.delta_c = theano.shared(value=numpy.zeros((n_out),\n                                     dtype=theano.config.floatX), name=\'delta_c\')\n\n        self.output = T.dot(self.x, self.W) + self.b\n        self.output = activation(self.output)\n        self.output = 2.* T.nnet.sigmoid(self.c) * self.output\n\n        self.params = [self.W, self.b, self.c]\n        self.delta_params = [self.delta_W, self.delta_b, self.delta_c]\n\n    def errors(self, y):\n        L = T.sum( (self.output-y)*(self.output-y), axis=1 )\n        errors = T.mean(L)\n        return (errors)\n\n    def init_params(self, iparams):\n        updates = {}\n        for param, iparam in zip(self.params, iparams):\n            updates[param] = iparam\n        return updates\n\nclass LstmBase_LHUC(object):\n    """""" \n    Very similar to the LSTM layer in the gating file\n    Extra parameters are \'C\' for scaling the hidden value\n    """"""\n\n    def __init__(self, rng, x, n_in, n_h, p, training):\n        """""" Initialise all the components in a LSTM block, including input gate, output gate, forget gate, peephole connections\n        \n        :param rng: random state, fixed value for randome state for reproducible objective results\n        :param x: input to a network\n        :param n_in: number of input features\n        :type n_in: integer\n        :param n_h: number of hidden units\n        :type n_h: integer\n        :param p: the probability of dropout\n        :param training: a binary value to indicate training or testing (for dropout training)\n        """"""\n    \n        self.input = x\n                \n        if p > 0.0:\n            if training==1:\n                srng = RandomStreams(seed=123456)\n                self.input = T.switch(srng.binomial(size=x.shape,p=p), x, 0)\n            else:\n                self.input =  (1-p) * x \n\n        self.n_in = int(n_in)\n        self.n_h = int(n_h)\n        \n        # random initialisation \n        Wx_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_h)), dtype=theano.config.floatX)\n        Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=theano.config.floatX)\n        Wc_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, )), dtype=theano.config.floatX)\n\n        # Input gate weights\n        self.W_xi = theano.shared(value=Wx_value, name=\'W_xi\')\n        self.W_hi = theano.shared(value=Wh_value, name=\'W_hi\')\n        self.w_ci = theano.shared(value=Wc_value, name=\'w_ci\')\n\n        # random initialisation \n        Wx_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_h)), dtype=theano.config.floatX)\n        Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=theano.config.floatX)\n        Wc_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, )), dtype=theano.config.floatX)\n\n        # Forget gate weights\n        self.W_xf = theano.shared(value=Wx_value, name=\'W_xf\')\n        self.W_hf = theano.shared(value=Wh_value, name=\'W_hf\')\n        self.w_cf = theano.shared(value=Wc_value, name=\'w_cf\')\n\n        # random initialisation \n        Wx_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_h)), dtype=theano.config.floatX)\n        Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=theano.config.floatX)\n        Wc_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, )), dtype=theano.config.floatX)\n\n        # Output gate weights\n        self.W_xo = theano.shared(value=Wx_value, name=\'W_xo\')\n        self.W_ho = theano.shared(value=Wh_value, name=\'W_ho\')\n        self.w_co = theano.shared(value=Wc_value, name=\'w_co\')\n\n        # random initialisation \n        Wx_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_h)), dtype=theano.config.floatX)\n        Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=theano.config.floatX)\n        Wc_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, )), dtype=theano.config.floatX)\n\n        # Cell weights\n        self.W_xc = theano.shared(value=Wx_value, name=\'W_xc\')\n        self.W_hc = theano.shared(value=Wh_value, name=\'W_hc\')\n\n        # bias\n        self.b_i = theano.shared(value=np.zeros((n_h, ), dtype=theano.config.floatX), name=\'b_i\')\n        self.b_f = theano.shared(value=np.zeros((n_h, ), dtype=theano.config.floatX), name=\'b_f\')\n        self.b_o = theano.shared(value=np.zeros((n_h, ), dtype=theano.config.floatX), name=\'b_o\')\n        self.b_c = theano.shared(value=np.zeros((n_h, ), dtype=theano.config.floatX), name=\'b_c\')\n        \n\n        # scaling factor\n        c_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h)), dtype=theano.config.floatX)\n        self.C = theano.shared(value=c_value, name=\'c\')\n        ### make a layer\n        \n        # initial value of hidden and cell state\n        self.h0 = theano.shared(value=np.zeros((n_h, ), dtype = theano.config.floatX), name = \'h0\')\n        self.c0 = theano.shared(value=np.zeros((n_h, ), dtype = theano.config.floatX), name = \'c0\')\n\n\n        self.Wix = T.dot(self.input, self.W_xi)\n        self.Wfx = T.dot(self.input, self.W_xf)\n        self.Wcx = T.dot(self.input, self.W_xc)\n        self.Wox = T.dot(self.input, self.W_xo)\n\n\n        \n        [self.h, self.c], _ = theano.scan(self.recurrent_fn, sequences = [self.Wix, self.Wfx, self.Wcx, self.Wox],\n                                                             outputs_info = [self.h0, self.c0]) \n\n        self.output = 2. * T.nnet.sigmoid(self.C) * self.h\n        \n        \n    def recurrent_fn(self, Wix, Wfx, Wcx, Wox, h_tm1, c_tm1 = None):\n        """""" This implements a genetic recurrent function, called by self.__init__().\n        \n        :param Wix: pre-computed matrix applying the weight matrix W on  the input units, for input gate\n        :param Wfx: Similar to Wix, but for forget gate\n        :param Wcx: Similar to Wix, but for cell memory\n        :param Wox: Similar to Wox, but for output gate\n        :param h_tm1: hidden activation from previous time step\n        :param c_tm1: activation from cell memory from previous time step\n        :returns: h_t is the hidden activation of current time step, and c_t is the activation for cell memory of current time step\n        """"""\n        h_t, c_t = self.lstm_as_activation_function(Wix, Wfx, Wcx, Wox, h_tm1, c_tm1)\n            \n        return h_t, c_t\n\n    def lstm_as_activation_function(self):\n        """""" A genetic recurrent activation function for variants of LSTM architectures.\n        The function is called by self.recurrent_fn().\n        \n        """"""\n        pass\n\n\nclass VanillaLstm_LHUC(LstmBase_LHUC):\n    """""" This class implements the standard LSTM block, inheriting the genetic class :class:`layers.gating.LstmBase`.\n    \n    """"""\n\n\n    def __init__(self, rng, x, n_in, n_h, p=0.0, training=0):\n        """""" Initialise a vanilla LSTM block\n        \n        :param rng: random state, fixed value for randome state for reproducible objective results\n        :param x: input to a network\n        :param n_in: number of input features\n        :type n_in: integer\n        :param n_h: number of hidden units\n        :type n_h: integer\n        """"""\n        \n            \n        LstmBase_LHUC.__init__(self, rng, x, n_in, n_h, p, training)\n        \n        self.params = [self.W_xi, self.W_hi, self.w_ci,\n                       self.W_xf, self.W_hf, self.w_cf,\n                       self.W_xo, self.W_ho, self.w_co, \n                       self.W_xc, self.W_hc,\n                       self.b_i, self.b_f, self.b_o, self.b_c, self.C]\n                       \n    def lstm_as_activation_function(self, Wix, Wfx, Wcx, Wox, h_tm1, c_tm1):\n        """""" This function treats the LSTM block as an activation function, and implements the standard LSTM activation function.\n            The meaning of each input and output parameters can be found in :func:`layers.gating.LstmBase.recurrent_fn`\n        \n        """"""\n    \n        i_t = T.nnet.sigmoid(Wix + T.dot(h_tm1, self.W_hi) + self.w_ci * c_tm1 + self.b_i)  #\n        f_t = T.nnet.sigmoid(Wfx + T.dot(h_tm1, self.W_hf) + self.w_cf * c_tm1 + self.b_f)  # \n    \n        c_t = f_t * c_tm1 + i_t * T.tanh(Wcx + T.dot(h_tm1, self.W_hc) + self.b_c)\n\n        o_t = T.nnet.sigmoid(Wox + T.dot(h_tm1, self.W_ho) + self.w_co * c_t + self.b_o)\n                            \n        h_t = o_t * T.tanh(c_t)\n\n        return h_t, c_t#, i_t, f_t, o_t'"
src/layers/mdn_layers.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n\nimport numpy, time, pickle, gzip, sys, os, copy\n\nimport theano\nimport theano.tensor as T\nfrom theano.tensor.shared_randomstreams import RandomStreams\n\nimport logging\n\n\nclass MixtureDensityOutputLayer(object):\n    def __init__(self, rng, input, n_in, n_out, n_component):\n        self.input = input\n\n        W_value = rng.normal(0.0, 1.0/numpy.sqrt(n_in), size=(n_in, n_out*n_component))\n        self.W_mu = theano.shared(value=numpy.asarray(W_value, dtype=theano.config.floatX), name=\'W_mu\', borrow=True)\n\n        self.W_sigma = theano.shared(value=numpy.asarray(W_value.copy(), dtype=theano.config.floatX), name=\'W_sigma\', borrow=True)\n\n        W_mix_value = rng.normal(0.0, 1.0/numpy.sqrt(n_in), size=(n_in, n_component))\n        self.W_mix = theano.shared(value=numpy.asarray(W_mix_value, dtype=theano.config.floatX), name=\'W_mix\', borrow=True)\n\n        self.mu = T.dot(self.input, self.W_mu)    #assume linear output for mean vectors\n        self.sigma = T.nnet.softplus(T.dot(self.input, self.W_sigma)) # + 0.0001\n        #self.sigma = T.exp(T.dot(self.input, self.W_sigma)) # + 0.0001\n\n        self.mix = T.nnet.softmax(T.dot(self.input, self.W_mix))\n\n        self.delta_W_mu    = theano.shared(value = numpy.zeros((n_in, n_out*n_component),\n                                           dtype=theano.config.floatX), name=\'delta_W_mu\')\n        self.delta_W_sigma = theano.shared(value = numpy.zeros((n_in, n_out*n_component),\n                                           dtype=theano.config.floatX), name=\'delta_W_sigma\')\n        self.delta_W_mix   = theano.shared(value = numpy.zeros((n_in, n_component),\n                                           dtype=theano.config.floatX), name=\'delta_W_mix\')\n\n\n        self.params = [self.W_mu, self.W_sigma, self.W_mix]\n        self.delta_params = [self.delta_W_mu, self.delta_W_sigma, self.delta_W_mix]\n\nclass LinearLayer(object):\n    def __init__(self, rng, input, n_in, n_out, W = None, b = None):\n\n        self.input = input\n\n        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n        if W is None:\n            W_value = rng.normal(0.0, 1.0/numpy.sqrt(n_in), size=(n_in, n_out))\n            W = theano.shared(value=numpy.asarray(W_value, dtype=theano.config.floatX), name=\'W\', borrow=True)\n\n        if b is None:\n            b = theano.shared(value=numpy.zeros((n_out,),\n                                        dtype=theano.config.floatX),\n                                   name=\'b\', borrow=True)\n\n        self.W = W\n        self.b = b\n\n        self.delta_W = theano.shared(value = numpy.zeros((n_in,n_out),\n                                     dtype=theano.config.floatX), name=\'delta_W\')\n\n        self.delta_b = theano.shared(value = numpy.zeros_like(self.b.get_value(borrow=True),\n                                     dtype=theano.config.floatX), name=\'delta_b\')\n\n        self.output = T.dot(self.input, self.W) + self.b\n\n        self.params = [self.W, self.b]\n        self.delta_params = [self.delta_W, self.delta_b]\n\n    def errors(self, y):\n        L = T.sum( (self.output-y)*(self.output-y), axis=1 )\n        errors = T.mean(L)\n        return (errors)\n\n    def init_params(self, iparams):\n        updates = {}\n        for param, iparam in zip(self.params, iparams):\n            updates[param] = iparam\n        return updates\n\nclass SigmoidLayer(object):\n    def __init__(self, rng, input, n_in, n_out, W = None, b = None, activation = T.tanh):\n\n        self.input = input\n\n        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n        if W is None:\n            W_value = numpy.asarray(rng.normal(0.0, 1.0/numpy.sqrt(n_in),\n                      size=(n_in, n_out)), dtype=theano.config.floatX)\n            W = theano.shared(value=W_value,\n                              name=\'W\', borrow=True)\n        if b is None:\n            b = theano.shared(value=numpy.zeros((n_out,),\n                              dtype=theano.config.floatX),\n                              name=\'b\', borrow=True)\n\n        self.W = W\n        self.b = b\n\n        self.delta_W = theano.shared(value = numpy.zeros((n_in,n_out),\n                                     dtype=theano.config.floatX), name=\'delta_W\')\n\n        self.delta_b = theano.shared(value = numpy.zeros_like(self.b.get_value(borrow=True),\n                                     dtype=theano.config.floatX), name=\'delta_b\')\n\n        self.output = T.dot(self.input, self.W) + self.b\n        self.output = activation(self.output)\n\n        self.params = [self.W, self.b]\n        self.delta_params = [self.delta_W, self.delta_b]\n\n    def errors(self, y):\n        L = T.sum( (self.output-y)*(self.output-y), axis=1 )\n        errors = T.mean(L)\n        return (errors)\n\n    def init_params(self, iparams):\n        updates = {}\n        for param, iparam in zip(self.params, iparams):\n            updates[param] = iparam\n        return updates\n\n\nclass GeneralLayer(object):\n\n    def __init__(self, rng, input, n_in, n_out, W = None, b = None, activation = \'linear\'):\n\n        self.input = input\n        self.n_in = n_in\n        self.n_out = n_out\n\n        self.logger = logging.getLogger(\'general_layer\')\n\n        # randomly initialise the activation weights based on the input size, as advised by the \'tricks of neural network book\'\n        if W is None:\n            W_values = numpy.asarray(rng.normal(0.0, 1.0/numpy.sqrt(n_in),\n                    size=(n_in, n_out)), dtype=theano.config.floatX)\n            W = theano.shared(value=W_values, name=\'W\', borrow=True)\n\n        if b is None:\n            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n            b = theano.shared(value=b_values, name=\'b\', borrow=True)\n\n        self.W = W\n        self.b = b\n\n        self.delta_W = theano.shared(value = numpy.zeros((n_in,n_out),\n                                     dtype=theano.config.floatX), name=\'delta_W\')\n\n        self.delta_b = theano.shared(value = numpy.zeros_like(self.b.get_value(borrow=True),\n                                     dtype=theano.config.floatX), name=\'delta_b\')\n\n        lin_output = T.dot(input, self.W) + self.b\n        if activation == \'sigmoid\':\n            self.output = T.nnet.sigmoid(lin_output)\n\n        elif activation == \'tanh\':\n            self.output = T.tanh(lin_output)\n\n        elif activation == \'linear\':\n            self.output = lin_output\n\n        elif activation == \'ReLU\':  ## rectifier linear unit\n            self.output = T.maximum(0.0, lin_output)\n\n        elif activation == \'ReSU\':  ## rectifier smooth unit\n            self.output = numpy.log(1.0 + numpy.exp(lin_output))\n\n        else:\n            self.logger.critical(\'the input activation function: %s is not supported right now. Please modify layers.py to support\' % (activation))\n            raise\n\n        # parameters of the model\n\n        self.params = [self.W, self.b]\n        self.delta_params = [self.delta_W, self.delta_b]\n\n    def errors(self, y):\n        errors = T.mean(T.sum((self.output-y)**2, axis=1))\n\n        return errors\n\n    def init_params(self, iparams):\n        updates = {}\n        for param, iparam in zip(self.params, iparams):\n            updates[param] = iparam\n        return updates\n\n\nclass HiddenLayer(object):\n    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n                 activation=T.tanh, do_maxout = False, pool_size = 1,\n                 do_pnorm = False, pnorm_order = 1):\n        """""" Class for hidden layer """"""\n        self.input = input\n        self.n_in = n_in\n        self.n_out = n_out\n\n        if W is None:\n\n            W_values = numpy.asarray(rng.normal(0.0, 1.0/numpy.sqrt(n_in),\n                    size=(n_in, n_out)), dtype=theano.config.floatX)\n\n            W = theano.shared(value=W_values, name=\'W\', borrow=True)\n\n        if b is None:\n            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n            b = theano.shared(value=b_values, name=\'b\', borrow=True)\n\n        self.W = W\n        self.b = b\n\n        self.delta_W = theano.shared(value = numpy.zeros((n_in,n_out),\n                                     dtype=theano.config.floatX), name=\'delta_W\')\n\n        self.delta_b = theano.shared(value = numpy.zeros_like(self.b.get_value(borrow=True),\n                                     dtype=theano.config.floatX), name=\'delta_b\')\n\n        lin_output = T.dot(input, self.W) + self.b\n        if do_maxout == True:\n            self.last_start = n_out - pool_size\n            self.tmp_output = lin_output[:,0:self.last_start+1:pool_size]\n            for i in range(1, pool_size):\n                cur = lin_output[:,i:self.last_start+i+1:pool_size]\n                self.tmp_output = T.maximum(cur, self.tmp_output)\n            self.output = activation(self.tmp_output)\n        elif do_pnorm == True:\n            self.last_start = n_out - pool_size\n            self.tmp_output = abs(lin_output[:,0:self.last_start+1:pool_size]) ** pnorm_order\n            for i in range(1, pool_size):\n                cur = abs(lin_output[:,i:self.last_start+i+1:pool_size]) ** pnorm_order\n                self.tmp_output = self.tmp_output + cur\n            self.tmp_output = self.tmp_output ** (1.0 / pnorm_order)\n            self.output = activation(self.tmp_output)\n        else:\n            self.output = (lin_output if activation is None\n                           else activation(lin_output))\n\n#        self.output = self.rectifier_linear(lin_output)\n\n        # parameters of the model\n        self.params = [self.W, self.b]\n        self.delta_params = [self.delta_W, self.delta_b]\n\n    def rectifier_linear(self, x):\n        x = T.maximum(0.0, x)\n\n        return  x\n\n    def rectifier_smooth(self, x):\n        x = numpy.log(1.0 + numpy.exp(x))\n\n        return  x\n\nclass dA(object):\n    def __init__(self, numpy_rng, theano_rng = None, input = None,\n                 n_visible= None, n_hidden= None, W = None, bhid = None,\n                 bvis = None, firstlayer = 0, variance   = None ):\n\n        self.n_visible = n_visible\n        self.n_hidden  = n_hidden\n\n        # create a Theano random generator that gives symbolic random values\n        if not theano_rng :\n            theano_rng = RandomStreams(numpy_rng.randint(2**30))\n\n        if not W:\n            initial_W = numpy.asarray( numpy_rng.uniform(\n                      low  = -4*numpy.sqrt(6./(n_hidden+n_visible)),\n                      high =  4*numpy.sqrt(6./(n_hidden+n_visible)),\n                      size = (n_visible, n_hidden)),\n                                       dtype = theano.config.floatX)\n            W = theano.shared(value = initial_W, name =\'W\')\n\n        if not bvis:\n            bvis = theano.shared(value = numpy.zeros(n_visible,\n                                         dtype = theano.config.floatX))\n\n        if not bhid:\n            bhid = theano.shared(value = numpy.zeros(n_hidden,\n                                dtype = theano.config.floatX), name =\'b\')\n\n\n        self.W = W\n        self.b = bhid\n        self.b_prime = bvis\n        self.W_prime = self.W.T\n        self.theano_rng = theano_rng\n\n        if input == None :\n            self.x = T.dmatrix(name = \'input\')\n        else:\n            self.x = input\n\n        self.params = [self.W, self.b, self.b_prime]\n\n        # first layer, use Gaussian noise\n        self.firstlayer = firstlayer\n\n        if self.firstlayer == 1 :\n            if variance == None :\n                self.var = T.vector(name = \'input\')\n            else :\n                self.var = variance\n        else :\n            self.var = None\n\n    def get_corrupted_input(self, input, corruption_level):\n        if self.firstlayer == 0 :\n            return  self.theano_rng.binomial(\n                             size = input.shape,\n                             n = 1,\n                             p = 1 - corruption_level,\n                             dtype=theano.config.floatX) * input\n        else :\n            noise = self.theano_rng.normal( size = input.shape,\n                                            dtype = theano.config.floatX)\n            denoises = noise * self.var * corruption_level\n            return input+denoises\n\n    def get_hidden_values(self, input):\n        return T.nnet.sigmoid(T.dot(input, self.W) + self.b)\n\n    def get_reconstructed_input(self, hidden ):\n        if self.firstlayer == 1 :\n            return T.dot(hidden, self.W_prime) + self.b_prime\n        else :\n            return T.nnet.sigmoid(T.dot(hidden, self.W_prime) + self.b_prime)\n\n    def get_cost_updates(self, corruption_level, learning_rate):\n        tilde_x = self.get_corrupted_input(self.x, corruption_level)\n        y       = self.get_hidden_values( tilde_x )\n        z       = self.get_reconstructed_input(y)\n\n        L = T.sum ( (self.x-z) * (self.x-z), axis=1 )\n        cost = T.mean(L) / 2\n\n        gparams = T.grad(cost, self.params)\n        updates = {}\n        for param, gparam in zip(self.params, gparams):\n            updates[param] = param -  learning_rate*gparam\n\n        return (cost, updates)\n\n    def init_params(self, iparams):\n        updates = {}\n        for param, iparam in zip(self.params, iparams):\n            updates[param] = iparam\n        return updates\n\n    def get_test_cost(self, corruption_level):\n        """""" This function computes the cost and the updates for one trainng\n        step of the dA """"""\n\n        # tilde_x = self.get_corrupted_input(self.x, corruption_level, 0.5)\n        y       = self.get_hidden_values( self.x )\n        z       = self.get_reconstructed_input(y)\n        L = T.sum ( (self.x-z) * (self.x-z), axis=1)\n        cost = T.mean(L)\n\n        return cost\n'"
src/layers/recurrent_decoders.py,0,"b'\nimport numpy as np\nimport theano\nimport theano.tensor as T\nfrom theano import config\nfrom theano.tensor.shared_randomstreams import RandomStreams\n\nclass VanillaRNNDecoder(object):\n    """""" This class implements a standard recurrent neural network decoder:\n        h_{t} = f(W^{hx}x_{t} + W^{hh}h_{t-1}+ W^{yh}y_{t-1} + b_{h})\n        y_{t} = g(h_{t}W^{hy} + b_{y})\n\n    """"""\n    def __init__(self, rng, x, n_in, n_h, n_out, p, training, rnn_batch_training=False):\n        """""" This is to initialise a standard RNN hidden unit\n\n        :param rng: random state, fixed value for randome state for reproducible objective results\n        :param x: input data to current layer\n        :param n_in: dimension of input data\n        :param n_h: number of hidden units/blocks\n        :param n_out: dimension of output data\n        :param p: the probability of dropout\n        :param training: a binary value to indicate training or testing (for dropout training)\n        """"""\n        self.input = x\n\n        if p > 0.0:\n            if training==1:\n                srng = RandomStreams(seed=123456)\n                self.input = T.switch(srng.binomial(size=x.shape,p=p), x, 0)\n            else:\n                self.input =  (1-p) * x #(1-p) *\n\n        self.n_in  = int(n_in)\n        self.n_h   = int(n_h)\n        self.n_out = int(n_out)\n\n        self.rnn_batch_training = rnn_batch_training\n\n        # random initialisation\n        Wx_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_h)), dtype=config.floatX)\n        Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=config.floatX)\n        #Wy_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_out), size=(n_out, n_h)), dtype=config.floatX)\n        Ux_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_out)), dtype=config.floatX)\n        Uh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_out)), dtype=config.floatX)\n        #Uy_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_out), size=(n_out, n_out)), dtype=config.floatX)\n\n        # identity matrix initialisation\n        #Wh_value = np.asarray(np.eye(n_h, n_h), dtype=config.floatX)\n        Wy_value = np.asarray(np.eye(n_out, n_h), dtype=config.floatX)\n        #Uh_value = np.asarray(np.eye(n_in, n_out), dtype=config.floatX)\n        Uy_value = np.asarray(np.zeros(n_out, n_out), dtype=config.floatX)\n\n        # Input gate weights\n        self.W_xi = theano.shared(value=Wx_value, name=\'W_xi\')\n        self.W_hi = theano.shared(value=Wh_value, name=\'W_hi\')\n        self.W_yi = theano.shared(value=Wy_value, name=\'W_yi\')\n\n        # Output gate weights\n        self.U_xi = theano.shared(value=Ux_value, name=\'U_xi\')\n        self.U_hi = theano.shared(value=Uh_value, name=\'U_hi\')\n        self.U_yi = theano.shared(value=Uy_value, name=\'U_yi\')\n\n        # bias\n        self.b_i = theano.shared(value=np.zeros((n_h, ), dtype=config.floatX), name=\'b_i\')\n        self.b   = theano.shared(value=np.zeros((n_out, ), dtype=config.floatX), name=\'b\')\n\n\n        # initial value of hidden and cell state and output\n        if self.rnn_batch_training:\n            self.h0 = theano.shared(value=np.zeros((1, n_h), dtype = config.floatX), name = \'h0\')\n            self.c0 = theano.shared(value=np.zeros((1, n_h), dtype = config.floatX), name = \'c0\')\n            self.y0 = theano.shared(value=np.zeros((1, n_out), dtype = config.floatX), name = \'y0\')\n\n            self.h0 = T.repeat(self.h0, x.shape[1], 0)\n            self.c0 = T.repeat(self.c0, x.shape[1], 0)\n            self.y0 = T.repeat(self.c0, x.shape[1], 0)\n        else:\n            self.h0 = theano.shared(value=np.zeros((n_h, ), dtype = config.floatX), name = \'h0\')\n            self.c0 = theano.shared(value=np.zeros((n_h, ), dtype = config.floatX), name = \'c0\')\n            self.y0 = theano.shared(value=np.zeros((n_out, ), dtype = config.floatX), name = \'y0\')\n\n\n        self.Wix = T.dot(self.input, self.W_xi)\n        self.Uix = T.dot(self.input, self.U_xi)\n\n        [self.h, self.c, self.y], _ = theano.scan(self.recurrent_as_activation_function, sequences = [self.Wix, self.Uix],\n                                                                      outputs_info = [self.h0, self.c0, self.y0])\n\n        self.output = self.y\n\n        # simple recurrent decoder params\n        #self.params = [self.W_xi, self.W_hi, self.W_yi, self.U_hi, self.b_i, self.b]\n\n        # recurrent output params and additional input params\n        self.params = [self.W_xi, self.W_hi, self.W_yi, self.U_xi, self.U_hi, self.U_yi, self.b_i, self.b]\n\n        self.L2_cost = (self.W_xi ** 2).sum() + (self.W_hi ** 2).sum() + (self.W_yi ** 2).sum() + (self.U_hi ** 2).sum()\n\n\n    def recurrent_as_activation_function(self, Wix, Uix, h_tm1, c_tm1, y_tm1):\n        """""" Implement the recurrent unit as an activation function. This function is called by self.__init__().\n\n        :param Wix: it equals to W^{hx}x_{t}, as it does not relate with recurrent, pre-calculate the value for fast computation\n        :type Wix: matrix\n        :param h_tm1: contains the hidden activation from previous time step\n        :type h_tm1: matrix, each row means a hidden activation vector of a time step\n        :param c_tm1: this parameter is not used, just to keep the interface consistent with LSTM\n        :returns: h_t is the hidden activation of current time step\n        """"""\n\n        h_t = T.tanh(Wix + T.dot(h_tm1, self.W_hi) + T.dot(y_tm1, self.W_yi) + self.b_i)  #\n\n        # simple recurrent decoder\n        #y_t = T.dot(h_t, self.U_hi) + self.b\n\n        # recurrent output and additional input\n        y_t = Uix + T.dot(h_t, self.U_hi) + T.dot(y_tm1, self.U_yi) + self.b\n\n        c_t = h_t\n\n        return h_t, c_t, y_t\n\nclass ContextRNNDecoder(object):\n    """""" This class implements a standard recurrent neural network decoder:\n        h_{t} = f(W^{hx}x_{t} + W^{hh}h_{t-1}+ W^{yh}y_{t-1} + b_{h})\n        y_{t} = g(h_{t}W^{hy} + b_{y})\n\n    """"""\n    def __init__(self, rng, x, n_in, n_h, n_out, p, training, y=None, rnn_batch_training=False):\n        """""" This is to initialise a standard RNN hidden unit\n\n        :param rng: random state, fixed value for randome state for reproducible objective results\n        :param x: input data to current layer\n        :param n_in: dimension of input data\n        :param n_h: number of hidden units/blocks\n        :param n_out: dimension of output data\n        :param p: the probability of dropout\n        :param training: a binary value to indicate training or testing (for dropout training)\n        """"""\n        self.input = x\n        if y is not None:\n            self.groundtruth = y\n\n        if p > 0.0:\n            if training==1:\n                srng = RandomStreams(seed=123456)\n                self.input = T.switch(srng.binomial(size=x.shape,p=p), x, 0)\n            else:\n                self.input =  (1-p) * x #(1-p) *\n\n        self.n_in  = int(n_in)\n        self.n_h   = int(n_h)\n        self.n_out = int(n_out)\n\n        self.rnn_batch_training = rnn_batch_training\n\n        # random initialisation\n        Wx_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_h)), dtype=config.floatX)\n        #Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=config.floatX)\n        #Wy_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_out), size=(n_out, n_h)), dtype=config.floatX)\n        Ux_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_out)), dtype=config.floatX)\n        #Uh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_out)), dtype=config.floatX)\n        #Uy_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_out), size=(n_out, n_out)), dtype=config.floatX)\n\n        # identity matrix initialisation\n        Wh_value = np.asarray(np.eye(n_h, n_h), dtype=config.floatX)\n        Wy_value = np.asarray(np.eye(n_out, n_h), dtype=config.floatX)\n        Uh_value = np.asarray(np.eye(n_in, n_out), dtype=config.floatX)\n        Uy_value = np.asarray(np.zeros(n_out, n_out), dtype=config.floatX)\n\n        # Input gate weights\n        self.W_xi = theano.shared(value=Wx_value, name=\'W_xi\')\n        self.W_hi = theano.shared(value=Wh_value, name=\'W_hi\')\n        self.W_yi = theano.shared(value=Wy_value, name=\'W_yi\')\n\n        # Output gate weights\n        self.U_xi = theano.shared(value=Ux_value, name=\'U_xi\')\n        self.U_hi = theano.shared(value=Uh_value, name=\'U_hi\')\n        self.U_yi = theano.shared(value=Uy_value, name=\'U_yi\')\n\n        # bias\n        self.b_i = theano.shared(value=np.zeros((n_h, ), dtype=config.floatX), name=\'b_i\')\n        self.b   = theano.shared(value=np.zeros((n_out, ), dtype=config.floatX), name=\'b\')\n\n        # initial value of hidden and cell state and output\n        if self.rnn_batch_training:\n            self.h0 = theano.shared(value=np.zeros((1, n_h), dtype = config.floatX), name = \'h0\')\n            self.c0 = theano.shared(value=np.zeros((1, n_h), dtype = config.floatX), name = \'c0\')\n            self.y0 = theano.shared(value=np.zeros((1, n_out), dtype = config.floatX), name = \'y0\')\n\n            self.h0 = T.repeat(self.h0, x.shape[1], 0)\n            self.c0 = T.repeat(self.c0, x.shape[1], 0)\n            self.y0 = T.repeat(self.c0, x.shape[1], 0)\n        else:\n            self.h0 = theano.shared(value=np.zeros((n_h, ), dtype = config.floatX), name = \'h0\')\n            self.c0 = theano.shared(value=np.zeros((n_h, ), dtype = config.floatX), name = \'c0\')\n            self.y0 = theano.shared(value=np.zeros((n_out, ), dtype = config.floatX), name = \'y0\')\n\n        self.h0 = self.input[-1, 0:-4] # hard coded to remove coarse coding features\n\n        self.outytm1 = T.roll(self.groundtruth, 1, 0)\n        \n        self.Wix = T.dot(self.input, self.W_xi)\n        self.Uix = T.dot(self.input, self.U_xi)\n\n        [self.h, self.c], _ = theano.scan(self.recurrent_as_activation_function, sequences = [self.Wix, self.Wiy],\n                                                                      outputs_info = [self.h0, self.c0])\n\n        self.y = self.Uix + self.Uiy + T.dot(self.h, self.U_hi) + self.b\n        self.output = T.nnet.softmax(self.y)\n        \n        # recurrent output params and additional input params\n        self.params = [self.W_xi, self.W_hi, self.W_yi, self.U_xi, self.U_hi, self.U_yi, self.b_i, self.b]\n\n        self.L2_cost = (self.W_xi ** 2).sum() + (self.W_hi ** 2).sum() + (self.W_yi ** 2).sum() + (self.U_hi ** 2).sum()\n\n\n    def recurrent_as_activation_function(self, Wix, Wiy, h_tm1, c_tm1):\n        """""" Implement the recurrent unit as an activation function. This function is called by self.__init__().\n\n        :param Wix: it equals to W^{hx}x_{t}, as it does not relate with recurrent, pre-calculate the value for fast computation\n        :type Wix: matrix\n        :param h_tm1: contains the hidden activation from previous time step\n        :type h_tm1: matrix, each row means a hidden activation vector of a time step\n        :param c_tm1: this parameter is not used, just to keep the interface consistent with LSTM\n        :returns: h_t is the hidden activation of current time step\n        """"""\n\n        h_t = T.tanh(Wix + T.dot(h_tm1, self.W_hi) + Wiy + self.b_i)  #\n\n        c_t = h_t\n\n        return h_t, c_t\n\nclass LstmDecoderBase(object):\n    """""" This class provides as a base for all long short-term memory (LSTM) related classes.\n    Several variants of LSTM were investigated in (Wu & King, ICASSP 2016): Zhizheng Wu, Simon King, ""Investigating gated recurrent neural networks for speech synthesis"", ICASSP 2016\n\n    """"""\n\n    def __init__(self, rng, x, n_in, n_h, n_out, p=0.0, training=0, rnn_batch_training=False):\n        """""" Initialise all the components in a LSTM block, including input gate, output gate, forget gate, peephole connections\n\n        :param rng: random state, fixed value for randome state for reproducible objective results\n        :param x: input to a network\n        :param n_in: number of input features\n        :type n_in: integer\n        :param n_h: number of hidden units\n        :type n_h: integer\n        :param p: the probability of dropout\n        :param training: a binary value to indicate training or testing (for dropout training)\n        """"""\n\n        self.input = x\n\n        if p > 0.0:\n            if training==1:\n                srng = RandomStreams(seed=123456)\n                self.input = T.switch(srng.binomial(size=x.shape,p=p), x, 0)\n            else:\n                self.input =  (1-p) * x\n\n        self.n_in = int(n_in)\n        self.n_h  = int(n_h)\n\n        self.rnn_batch_training = rnn_batch_training\n\n        # random initialisation\n        Wx_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_h)), dtype=config.floatX)\n        Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=config.floatX)\n        Wc_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, )), dtype=config.floatX)\n        Wy_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_out), size=(n_out, n_h)), dtype=config.floatX)\n\n        # Input gate weights\n        self.W_xi = theano.shared(value=Wx_value, name=\'W_xi\')\n        self.W_hi = theano.shared(value=Wh_value, name=\'W_hi\')\n        self.w_ci = theano.shared(value=Wc_value, name=\'w_ci\')\n        self.W_yi = theano.shared(value=Wy_value, name=\'W_yi\')\n\n        # random initialisation\n        Uh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_out)), dtype=config.floatX)\n\n        # Output gate weights\n        self.U_ho = theano.shared(value=Uh_value, name=\'U_ho\')\n\n        # random initialisation\n        Wx_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_h)), dtype=config.floatX)\n        Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=config.floatX)\n        Wc_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, )), dtype=config.floatX)\n\n        # Forget gate weights\n        self.W_xf = theano.shared(value=Wx_value, name=\'W_xf\')\n        self.W_hf = theano.shared(value=Wh_value, name=\'W_hf\')\n        self.w_cf = theano.shared(value=Wc_value, name=\'w_cf\')\n\n        # random initialisation\n        Wx_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_h)), dtype=config.floatX)\n        Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=config.floatX)\n        Wc_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, )), dtype=config.floatX)\n\n        # Output gate weights\n        self.W_xo = theano.shared(value=Wx_value, name=\'W_xo\')\n        self.W_ho = theano.shared(value=Wh_value, name=\'W_ho\')\n        self.w_co = theano.shared(value=Wc_value, name=\'w_co\')\n\n        # random initialisation\n        Wx_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_h)), dtype=config.floatX)\n        Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=config.floatX)\n        Wc_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, )), dtype=config.floatX)\n\n        # Cell weights\n        self.W_xc = theano.shared(value=Wx_value, name=\'W_xc\')\n        self.W_hc = theano.shared(value=Wh_value, name=\'W_hc\')\n\n        # bias\n        self.b_i = theano.shared(value=np.zeros((n_h, ), dtype=config.floatX), name=\'b_i\')\n        self.b_f = theano.shared(value=np.zeros((n_h, ), dtype=config.floatX), name=\'b_f\')\n        self.b_o = theano.shared(value=np.zeros((n_h, ), dtype=config.floatX), name=\'b_o\')\n        self.b_c = theano.shared(value=np.zeros((n_h, ), dtype=config.floatX), name=\'b_c\')\n        self.b   = theano.shared(value=np.zeros((n_out, ), dtype=config.floatX), name=\'b\')\n\n        ### make a layer\n\n        # initial value of hidden and cell state\n        if self.rnn_batch_training:\n            self.h0 = theano.shared(value=np.zeros((1, n_h), dtype = config.floatX), name = \'h0\')\n            self.c0 = theano.shared(value=np.zeros((1, n_h), dtype = config.floatX), name = \'c0\')\n            self.y0 = theano.shared(value=np.zeros((1, n_out), dtype = config.floatX), name = \'y0\')\n\n            self.h0 = T.repeat(self.h0, x.shape[1], 0)\n            self.c0 = T.repeat(self.c0, x.shape[1], 0)\n            self.y0 = T.repeat(self.c0, x.shape[1], 0)\n        else:\n            self.h0 = theano.shared(value=np.zeros((n_h, ), dtype = config.floatX), name = \'h0\')\n            self.c0 = theano.shared(value=np.zeros((n_h, ), dtype = config.floatX), name = \'c0\')\n            self.y0 = theano.shared(value=np.zeros((n_out, ), dtype = config.floatX), name = \'y0\')\n\n\n        self.Wix = T.dot(self.input, self.W_xi)\n        self.Wfx = T.dot(self.input, self.W_xf)\n        self.Wcx = T.dot(self.input, self.W_xc)\n        self.Wox = T.dot(self.input, self.W_xo)\n\n        [self.h, self.c, self.y], _ = theano.scan(self.recurrent_fn, sequences = [self.Wix, self.Wfx, self.Wcx, self.Wox],\n                                                             outputs_info = [self.h0, self.c0, self.y0])\n\n        self.output = self.y\n\n\n    def recurrent_fn(self, Wix, Wfx, Wcx, Wox, h_tm1, c_tm1=None, y_tm1=None):\n        """""" This implements a genetic recurrent function, called by self.__init__().\n\n        :param Wix: pre-computed matrix applying the weight matrix W on  the input units, for input gate\n        :param Wfx: Similar to Wix, but for forget gate\n        :param Wcx: Similar to Wix, but for cell memory\n        :param Wox: Similar to Wox, but for output gate\n        :param h_tm1: hidden activation from previous time step\n        :param c_tm1: activation from cell memory from previous time step\n        :returns: h_t is the hidden activation of current time step, and c_t is the activation for cell memory of current time step\n        """"""\n\n        h_t, c_t, y_t = self.lstm_as_activation_function(Wix, Wfx, Wcx, Wox, h_tm1, c_tm1, y_tm1)\n\n        return h_t, c_t, y_t\n\n    def lstm_as_activation_function(self):\n        """""" A genetic recurrent activation function for variants of LSTM architectures.\n        The function is called by self.recurrent_fn().\n\n        """"""\n        pass\n\nclass VanillaLstmDecoder(LstmDecoderBase):\n    """""" This class implements the standard LSTM block, inheriting the genetic class :class:`layers.gating.LstmBase`.\n\n    """"""\n\n\n    def __init__(self, rng, x, n_in, n_h, n_out, p=0.0, training=0, rnn_batch_training=False):\n        """""" Initialise a vanilla LSTM block\n\n        :param rng: random state, fixed value for randome state for reproducible objective results\n        :param x: input to a network\n        :param n_in: number of input features\n        :type n_in: integer\n        :param n_h: number of hidden units\n        :type n_h: integer\n        """"""\n\n        self.n_out = int(n_out)\n\n        LstmDecoderBase.__init__(self, rng, x, n_in, n_h, n_out, p, training, rnn_batch_training)\n\n        self.params = [self.W_xi, self.W_hi, self.w_ci, self.W_yi,\n                       self.W_xf, self.W_hf, self.w_cf,\n                       self.W_xo, self.W_ho, self.w_co,\n                       self.W_xc, self.W_hc,\n                       self.U_ho,\n                       self.b_i, self.b_f, self.b_o, self.b_c, self.b]\n\n    def lstm_as_activation_function(self, Wix, Wfx, Wcx, Wox, h_tm1, c_tm1, y_tm1):\n        """""" This function treats the LSTM block as an activation function, and implements the standard LSTM activation function.\n            The meaning of each input and output parameters can be found in :func:`layers.gating.LstmBase.recurrent_fn`\n\n        """"""\n\n        i_t = T.nnet.sigmoid(Wix + T.dot(h_tm1, self.W_hi) + self.w_ci * c_tm1 + self.b_i)  #\n        f_t = T.nnet.sigmoid(Wfx + T.dot(h_tm1, self.W_hf) + self.w_cf * c_tm1 + self.b_f)  #\n\n        c_t = f_t * c_tm1 + i_t * T.tanh(Wcx + T.dot(h_tm1, self.W_hc) + T.dot(y_tm1, self.W_yi) + self.b_c)\n\n        o_t = T.nnet.sigmoid(Wox + T.dot(h_tm1, self.W_ho) + self.w_co * c_t + self.b_o)\n\n        h_t = o_t * T.tanh(c_t)\n\n        y_t = T.dot(h_t, self.U_ho) + self.b\n\n        return h_t, c_t, y_t     #, i_t, f_t, o_t\n\nclass SimplifiedLstmDecoder(LstmDecoderBase):\n    """""" This class implements a simplified LSTM block which only keeps the forget gate, inheriting the genetic class :class:`layers.gating.LstmBase`.\n    \n    """"""\n\n    def __init__(self, rng, x, n_in, n_h, n_out, p=0.0, training=0, rnn_batch_training=False):\n        """""" Initialise a LSTM with only the forget gate\n        \n        :param rng: random state, fixed value for randome state for reproducible objective results\n        :param x: input to a network\n        :param n_in: number of input features\n        :type n_in: integer\n        :param n_h: number of hidden units\n        :type n_h: integer\n        """"""\n        \n        self.n_out = int(n_out)\n\n        LstmDecoderBase.__init__(self, rng, x, n_in, n_h, n_out, p, training, rnn_batch_training)\n\n        self.params = [self.W_yi,\n                       self.W_xf, self.W_hf,\n                       self.W_xc, self.W_hc,\n                       self.U_ho,\n                       self.b_f,  self.b_c, self.b]\n                       \n    def lstm_as_activation_function(self, Wix, Wfx, Wcx, Wox, h_tm1, c_tm1, y_tm1):\n        """""" This function treats the LSTM block as an activation function, and implements the LSTM (simplified LSTM) activation function.\n            The meaning of each input and output parameters can be found in :func:`layers.gating.LstmBase.recurrent_fn`\n        \n        """"""\n    \n        f_t = T.nnet.sigmoid(Wfx + T.dot(h_tm1, self.W_hf) + self.b_f)  #self.w_cf * c_tm1 \n    \n        c_t = f_t * c_tm1 + (1 - f_t) * T.tanh(Wcx + T.dot(h_tm1, self.W_hc) + T.dot(y_tm1, self.W_yi) + self.b_c) \n\n        h_t = T.tanh(c_t)\n\n        y_t = T.dot(h_t, self.U_ho) + self.b\n\n        return h_t, c_t, y_t\n\nclass LstmBase(object):\n    """""" This class provides as a base for all long short-term memory (LSTM) related classes.\n    Several variants of LSTM were investigated in (Wu & King, ICASSP 2016): Zhizheng Wu, Simon King, ""Investigating gated recurrent neural networks for speech synthesis"", ICASSP 2016\n\n    """"""\n\n    def __init__(self, rng, x, n_in, n_h, p=0.0, training=0, rnn_batch_training=False):\n        """""" Initialise all the components in a LSTM block, including input gate, output gate, forget gate, peephole connections\n\n        :param rng: random state, fixed value for randome state for reproducible objective results\n        :param x: input to a network\n        :param n_in: number of input features\n        :type n_in: integer\n        :param n_h: number of hidden units\n        :type n_h: integer\n        :param p: the probability of dropout\n        :param training: a binary value to indicate training or testing (for dropout training)\n        """"""\n\n        n_in = int(n_in)  # ensure sizes have integer type\n        n_h = int(n_h)# ensure sizes have integer type\n\n        self.input = x\n\n        if p > 0.0:\n            if training==1:\n                srng = RandomStreams(seed=123456)\n                self.input = T.switch(srng.binomial(size=x.shape,p=p), x, 0)\n            else:\n                self.input =  (1-p) * x\n\n        self.n_in = int(n_in)\n        self.n_h  = int(n_h)\n\n        self.rnn_batch_training = rnn_batch_training\n\n        # random initialisation\n        Wx_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_h)), dtype=config.floatX)\n        Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=config.floatX)\n        Wc_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, )), dtype=config.floatX)\n\n        # Input gate weights\n        self.W_xi = theano.shared(value=Wx_value, name=\'W_xi\')\n        self.W_hi = theano.shared(value=Wh_value, name=\'W_hi\')\n        self.w_ci = theano.shared(value=Wc_value, name=\'w_ci\')\n\n        # random initialisation\n        Wx_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_h)), dtype=config.floatX)\n        Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=config.floatX)\n        Wc_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, )), dtype=config.floatX)\n\n        # Forget gate weights\n        self.W_xf = theano.shared(value=Wx_value, name=\'W_xf\')\n        self.W_hf = theano.shared(value=Wh_value, name=\'W_hf\')\n        self.w_cf = theano.shared(value=Wc_value, name=\'w_cf\')\n\n        # random initialisation\n        Wx_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_h)), dtype=config.floatX)\n        Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=config.floatX)\n        Wc_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, )), dtype=config.floatX)\n\n        # Output gate weights\n        self.W_xo = theano.shared(value=Wx_value, name=\'W_xo\')\n        self.W_ho = theano.shared(value=Wh_value, name=\'W_ho\')\n        self.w_co = theano.shared(value=Wc_value, name=\'w_co\')\n\n        # random initialisation\n        Wx_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_h)), dtype=config.floatX)\n        Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=config.floatX)\n        Wc_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, )), dtype=config.floatX)\n\n        # Cell weights\n        self.W_xc = theano.shared(value=Wx_value, name=\'W_xc\')\n        self.W_hc = theano.shared(value=Wh_value, name=\'W_hc\')\n\n        # bias\n        self.b_i = theano.shared(value=np.zeros((n_h, ), dtype=config.floatX), name=\'b_i\')\n        self.b_f = theano.shared(value=np.zeros((n_h, ), dtype=config.floatX), name=\'b_f\')\n        self.b_o = theano.shared(value=np.zeros((n_h, ), dtype=config.floatX), name=\'b_o\')\n        self.b_c = theano.shared(value=np.zeros((n_h, ), dtype=config.floatX), name=\'b_c\')\n\n        ### make a layer\n\n        # initial value of hidden and cell state\n        if self.rnn_batch_training:\n            self.h0 = theano.shared(value=np.zeros((1, n_h), dtype = config.floatX), name = \'h0\')\n            self.c0 = theano.shared(value=np.zeros((1, n_h), dtype = config.floatX), name = \'c0\')\n\n            self.h0 = T.repeat(self.h0, x.shape[1], 0)\n            self.c0 = T.repeat(self.c0, x.shape[1], 0)\n        else:\n            self.h0 = theano.shared(value=np.zeros((n_h, ), dtype = config.floatX), name = \'h0\')\n            self.c0 = theano.shared(value=np.zeros((n_h, ), dtype = config.floatX), name = \'c0\')\n\n        self.h0 = self.input[-1, 0:-4] # hard coded to remove coarse coding features\n\n        self.Wix = T.dot(self.input, self.W_xi)\n        self.Wfx = T.dot(self.input, self.W_xf)\n        self.Wcx = T.dot(self.input, self.W_xc)\n        self.Wox = T.dot(self.input, self.W_xo)\n\n        [self.h, self.c], _ = theano.scan(self.recurrent_fn, sequences = [self.Wix, self.Wfx, self.Wcx, self.Wox],\n                                                             outputs_info = [self.h0, self.c0])\n\n        self.output = self.h\n\n\n    def recurrent_fn(self, Wix, Wfx, Wcx, Wox, h_tm1, c_tm1 = None):\n        """""" This implements a genetic recurrent function, called by self.__init__().\n\n        :param Wix: pre-computed matrix applying the weight matrix W on  the input units, for input gate\n        :param Wfx: Similar to Wix, but for forget gate\n        :param Wcx: Similar to Wix, but for cell memory\n        :param Wox: Similar to Wox, but for output gate\n        :param h_tm1: hidden activation from previous time step\n        :param c_tm1: activation from cell memory from previous time step\n        :returns: h_t is the hidden activation of current time step, and c_t is the activation for cell memory of current time step\n        """"""\n\n        h_t, c_t = self.lstm_as_activation_function(Wix, Wfx, Wcx, Wox, h_tm1, c_tm1)\n\n        return h_t, c_t\n\n    def lstm_as_activation_function(self):\n        """""" A genetic recurrent activation function for variants of LSTM architectures.\n        The function is called by self.recurrent_fn().\n\n        """"""\n        pass\n\nclass ContextLstm(LstmBase):\n    """""" This class implements the standard LSTM block, inheriting the genetic class :class:`layers.gating.LstmBase`.\n\n    """"""\n\n\n    def __init__(self, rng, x, n_in, n_h, p=0.0, training=0, rnn_batch_training=False):\n        """""" Initialise a vanilla LSTM block\n\n        :param rng: random state, fixed value for randome state for reproducible objective results\n        :param x: input to a network\n        :param n_in: number of input features\n        :type n_in: integer\n        :param n_h: number of hidden units\n        :type n_h: integer\n        """"""\n\n        LstmBase.__init__(self, rng, x, n_in, n_h, p, training, rnn_batch_training)\n\n        self.params = [self.W_xi, self.W_hi, self.w_ci,\n                       self.W_xf, self.W_hf, self.w_cf,\n                       self.W_xo, self.W_ho, self.w_co,\n                       self.W_xc, self.W_hc,\n                       self.b_i, self.b_f, self.b_o, self.b_c]\n\n    def lstm_as_activation_function(self, Wix, Wfx, Wcx, Wox, h_tm1, c_tm1):\n        """""" This function treats the LSTM block as an activation function, and implements the standard LSTM activation function.\n            The meaning of each input and output parameters can be found in :func:`layers.gating.LstmBase.recurrent_fn`\n\n        """"""\n\n        i_t = T.nnet.sigmoid(Wix + T.dot(h_tm1, self.W_hi) + self.w_ci * c_tm1 + self.b_i)  #\n        f_t = T.nnet.sigmoid(Wfx + T.dot(h_tm1, self.W_hf) + self.w_cf * c_tm1 + self.b_f)  #\n\n        c_t = f_t * c_tm1 + i_t * T.tanh(Wcx + T.dot(h_tm1, self.W_hc) + self.b_c)\n\n        o_t = T.nnet.sigmoid(Wox + T.dot(h_tm1, self.W_ho) + self.w_co * c_t + self.b_o)\n\n        h_t = o_t * T.tanh(c_t)\n\n        return h_t, c_t#, i_t, f_t, o_t\n\n\n'"
src/layers/recurrent_output_layer.py,0,"b'import numpy as np\nimport theano\nimport theano.tensor as T\nfrom theano import config\nfrom theano.tensor.shared_randomstreams import RandomStreams\n\n\nclass RecurrentOutputLayer(object):\n    """""" This class implements a standard recurrent output layer:\n        y_{t} = g(h_{t}W^{hy} + y_{t}W^{yy} + b_{y})\n\n    """"""\n    def __init__(self, rng, x, n_in, n_out, p=0.0, training=1, rnn_batch_training=False):\n        """""" This is to initialise a standard RNN hidden unit\n\n        :param rng: random state, fixed value for randome state for reproducible objective results\n        :param x: input data to current layer\n        :param n_in: dimension of input data\n        :param n_out: dimension of output data\n        :param p: the probability of dropout\n        :param training: a binary value to indicate training or testing (for dropout training)\n        """"""\n        self.input = x\n\n        if p > 0.0:\n            if training==1:\n                srng = RandomStreams(seed=123456)\n                self.input = T.switch(srng.binomial(size=x.shape,p=p), x, 0)\n            else:\n                self.input =  (1-p) * x #(1-p) *\n\n        self.n_in = int(n_in)\n        self.n_out = int(n_out)\n\n        self.rnn_batch_training = rnn_batch_training\n\n        # random initialisation\n        Wx_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_in), size=(n_in, n_out)), dtype=config.floatX)\n        Wy_value = np.asarray(np.zeros((n_out, n_out)), dtype=config.floatX)\n\n        # Input gate weights\n        self.W_xi = theano.shared(value=Wx_value, name=\'W_xi\')\n        self.W_yi = theano.shared(value=Wy_value, name=\'W_yi\')\n\n        # bias\n        self.b_y = theano.shared(value=np.zeros((n_out, ), dtype=config.floatX), name=\'b_y\')\n\n        # initial value of output\n        if self.rnn_batch_training:\n            self.y0 = theano.shared(value=np.zeros((1, n_out), dtype = config.floatX), name = \'y0\')\n            self.y0 = T.repeat(self.y0, x.shape[1], 0)\n        else:\n            self.y0 = theano.shared(value=np.zeros((n_out, ), dtype = config.floatX), name = \'y0\')\n\n\n        self.Wix = T.dot(self.input, self.W_xi)\n\n        self.y, _ = theano.scan(self.recurrent_as_activation_function, sequences = self.Wix,\n                                                                      outputs_info = self.y0)\n\n        self.output = self.y\n\n        self.params = [self.W_xi, self.W_yi, self.b_y]\n\n    def recurrent_as_activation_function(self, Wix, y_tm1):\n        """""" Implement the recurrent unit as an activation function. This function is called by self.__init__().\n\n        :param Wix: it equals to W^{hx}x_{t}, as it does not relate with recurrent, pre-calculate the value for fast computation\n        :type Wix: matrix\n        :param y_tm1: contains the output from previous time step\n        :type y_tm1: matrix, each row means an output vector of a time step\n        """"""\n\n        y_t = Wix + T.dot(y_tm1, self.W_yi) + self.b_y  #\n\n        return y_t\n'"
src/logplot/__init__.py,0,b''
src/logplot/logging_plotting.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n# NOTES\n# still to consider: pygal, for HTML5 SVG plotting\n\nimport math\nimport string\nimport os\n\n# this module provides the base classes that we specialise here\nimport logging # as logging\n\n# for plotting\nimport matplotlib\n\n# should make this user-configurable - TO DO later\n# this line has to come before the import of matplotlib.pyplot\nmatplotlib.use(\'PDF\')\n\nimport matplotlib.pyplot as plt\nimport pylab\n\nfrom matplotlib.ticker import MultipleLocator, FormatStrFormatter\n\n# matplotlib needs to be passed numpy arrays\nimport numpy\n\n# for sorting tuples\nfrom operator import itemgetter, attrgetter\n\n\n# TO DO - this needs to be attached to the logging module so that it\'s available via config options\n# class PlotHandler(logging.FileHandler):\n#     """"""A handler for saving plots to disk""""""\n#     def __init__(self,filename):\n#         logging.FileHandler.__init__(self,filename, mode=\'a\', encoding=None, delay=False)\n\n\n\nclass PlotWithData(object):\n    # a generic plot object that contains both the underlying data and the plot itself\n    # this class needs to be subclassed for each specialised type of plot that we want\n\n    # the underlying data for the plot - a dictionary of data series\n    # each series is a list of data points of arbitrary type (e.g., tuples, arrays, ..)\n    data=None\n    # the plot generated from these data\n    plot=None\n\n    def __init__(self,name):\n        # clear the data series\n        self.data={}\n\n    def add_data_point(self,series_name,data_point):\n        # if there is no data series with this name yet, create an empty one\n        if series_name not in self.data:\n            self.data[series_name]=[]\n        # append this data point (e.g., it might be a tuple (x,y) )\n        # don\'t worry about data type or sorting - that is not our concern here\n        self.data[series_name].append(data_point)\n\n    def sort_and_validate(self):\n        # only applied if the data points are tuples, such as (x,y) values\n\n        # TO DO: first check that each series is a list of tuples, and that they have the same number of elements\n\n        # this method checks that all data series\n        # 1. have the same length\n        # 2. are sorted in ascending order of x\n        # 3. have identical values in their x series\n\n\n        # there has to be at least one data series\n        try:\n            assert len(self.data) > 0\n        except AssertionError:\n            logger.critical(\'No data series found in plot\')\n            raise\n\n        # check lengths are consistent, sort, then check x values are identical\n        l=-1\n        reference_x=None\n        # print ""starting with self.data="",self.data\n        for series_name,data_points in self.data.items():\n            if l > 0:\n                assert l == len(data_points)\n            else:\n                l = len(data_points)\n            # sort by ascending x value\n            data_points.sort(key=itemgetter(0))\n\n            if reference_x:\n                assert reference_x == [seq[0] for seq in data_points]\n            else:\n                # extract a list of just the x values\n                reference_x = [seq[0] for seq in data_points]\n\n\n        # print ""ending with self.data="",self.data\n\n    def generate_plot(self,**kwargs):\n        logger = logging.getLogger(""plotting"")\n        logger.error(\'Cannot generate a plot from abstract class: PlotWithData\' )\n        # raise an exception here?\n\nclass MultipleSeriesPlot(PlotWithData):\n\n    def generate_plot(self,filename,title=\'\',xlabel=\'\',ylabel=\'\',xlim=None,ylim=None):\n\n        logger = logging.getLogger(""plotting"")\n        logger.debug(\'MultipleSeriesPlot.generate_plot\')\n\n        # a plot with one or more time series sharing a common x axis:\n        # e.g., the training error and the validation error plotted against epochs\n\n        # sort the data series and make sure they are consistent\n        self.sort_and_validate()\n\n        # if there is a plot already in existence, we will clear it and re-use it;\n        # this avoids creating extraneous figures which will stay in memory\n        # (even if we are no longer referencing them)\n        if self.plot:\n            self.plot.clf()\n        else:\n            # create a plot\n            self.plot = plt.figure()\n\n        splt = self.plot.add_subplot(1, 1, 1)\n        splt.set_title(title)\n        splt.set_xlabel(xlabel)\n        splt.set_ylabel(ylabel)\n\n        if xlim:\n            pylab.xlim(xlim)\n        if ylim:\n            pylab.ylim(ylim)\n\n        for series_name,data_points in self.data.items():\n            xpoints=numpy.asarray([seq[0] for seq in data_points])\n            ypoints=numpy.asarray([seq[1] for seq in data_points])\n            line, = splt.plot(xpoints, ypoints, \'-\', linewidth=2)\n            logger.debug(\'set_label for %s\' % series_name)\n            line.set_label(series_name)\n\n        splt.legend()\n\n        # TO DO - better filename configuration for plots\n        self.plot.savefig(filename)\n\nclass SingleWeightMatrixPlot(PlotWithData):\n\n    def generate_plot(self, filename, title=\'\', xlabel=\'\', ylabel=\'\'):\n\n        data_keys = list(self.data.keys())\n        key_num = len(data_keys)\n\n        self.plot = plt.figure()\n        if key_num == 1:\n            splt = self.plot.add_subplot(1, 1, 1)\n            im_data = splt.imshow(numpy.flipud(self.data[data_keys[0]][0]), origin=\'lower\')\n            splt.set_xlabel(xlabel)\n            splt.set_ylabel(ylabel)\n            splt.set_title(title)\n        else:   ## still plotting multiple image in one figure still has problem. the visualization is not good\n            logger.error(\'no supported yet\')\n\n        self.plot.colorbar(im_data)\n        self.plot.savefig(filename)  #, bbox_inches=\'tight\'\n\n#class MultipleLinesPlot(PlotWithData):\n#    def generate_plot(self, filename, title=\'\', xlabel=\'\', ylabel=\'\'):\n\nclass LoggerPlotter(logging.getLoggerClass()):\n    """"""Based on the built-in logging class, with added capabilities including plotting""""""\n\n    # a dictionary to store all generated plots\n    # keys are plot names\n    # values are\n    plots ={}\n    # where the plots will be saved - a directory\n    plot_path=\'/tmp\' # default location\n\n    def __init__(self,name):\n        # initialise the logging parent class\n        # (should really use \'super\' here I think, but that fails - perhaps because the built in logger class is not derived from \'object\' ?)\n        logging.Logger.__init__(self,name)\n\n    def set_plot_path(self,path):\n        self.plot_path = path\n\n    def remove_all_plots(self):\n        self.plots={}\n\n    def create_plot(self,plot_name,plot_object):\n        self.plots[plot_name] = plot_object(plot_name)\n\n    def add_plot_point(self,plot_name,series_name,data_point):\n        # add a data point to a named plot\n        if plot_name not in self.plots:\n            self.plots[plot_name] = PlotWithData(plot_name)\n        self.plots[plot_name].add_data_point(series_name,data_point)\n\n    def save_plot(self,plot_name,**kwargs):\n        logger = logging.getLogger(""plotting"")\n        if plot_name not in self.plots:\n            logger.warn(\'Tried to generate a plot called %s that does not exist\' % plot_name)\n            # raise an exception here?\n        else:\n            # # the filename to save to is known by the handler, which needs to be assigned to this logger\n            # # look at the handlers attached to this logger instance\n            # ph=None\n            # for h in self.handlers:\n            #     # we want an instance of a PlotHandler - we\'ll take the first one we find\n            #     # (behaviour will be unpredictable if there is more than one handler of this type)\n            #     if isinstance(h,PlotHandler):\n            #         ph=h\n            #         break\n            # if ph:\n            # TO DO - need to be sure of safe file names\n            if not os.path.isdir(self.plot_path):\n                os.makedirs(self.plot_path)\n            filename = self.plot_path + ""/"" + string.replace(plot_name, "" "", ""_"") + "".pdf""\n            logger.info(\'Generating a plot in file %s\' % filename)\n            self.plots[plot_name].generate_plot(filename,**kwargs)\n            # else:\n            #     logger.warn(\'No handler of type PlotHandler is attached to this logger - cannot save plots\')\n\n\n\n\nclass ColouredFormatter(logging.Formatter):\n\n    # colourising formatter adapted from an answer to this question on Stack Overflow\n    # http://stackoverflow.com/questions/384076/how-can-i-color-python-logging-output\n\n    BLACK, RED, GREEN, YELLOW, BLUE, MAGENTA, CYAN, WHITE = list(range(8))\n\n    COLOURS = {\n        \'DEBUG\': BLUE,\n        \'INFO\': GREEN,\n        \'WARNING\': YELLOW,\n        \'ERROR\': RED,\n        \'CRITICAL\': MAGENTA\n    }\n\n    max_level_name_width = \'8\'\n\n    # terminal escape sequences\n    RESET_SEQ = ""\\033[0m""\n    COLOUR_SEQ = ""\\033[1;%dm""\n    BOLD_SEQ = ""\\033[1m""\n\n    def format(self, record):\n        if record.levelname in self.COLOURS:\n            # pad to fixed width - currently hardwired, should make this dynamic\n            # maximum width of level names, which is the 8 characters of ""CRITICAL""\n            fixed_width_levelname = \'{0:8s}\'.format(record.levelname)\n            record.name = \'{0:8s}\'.format(record.name)\n            # The background is set with 40 plus the number of the color, and the foreground with 30\n            record.levelname = self.COLOUR_SEQ % (30 + self.COLOURS[record.levelname]) + fixed_width_levelname + self.RESET_SEQ\n        return logging.Formatter.format(self, record)\n\n    def factory(fmt, datefmt):\n        default = logging.Formatter(fmt, datefmt)\n        return ColouredFormatter(default)\n\nif __name__ == \'__main__\':\n    # some simple tests\n\n    # tell the built-in logger module to use our custom class when instantiating any new logger\n    logging.setLoggerClass(LoggerPlotter)\n\n\n    logger = logging.getLogger(""test_logger"")\n    logger.setLevel(logging.DEBUG)\n\n    # a console handler\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.DEBUG)\n    formatter = ColouredFormatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n\n\n\n    print(""testing the logging code"")\n    logger.debug(\'A DEBUG message\')\n    logger.info(\'A INFO message\')\n    logger.warning(\'A WARN message\')\n    logger.error(\'A ERROR message\')\n    logger.critical(\'A CRITICAL message\')\n\n\n    plotlogger = logging.getLogger(""plotting"")\n    plotlogger.setLevel(logging.DEBUG)\n    # handler for plotting logger - will write only to console\n    plotlogger.addHandler(ch)\n\n\n    # # need a handler which will control where to save plots\n    # ph = PlotHandler(""/tmp/plot_test/testing.pdf"")\n    # logger.addHandler(ph)\n\n\n    print(""testing the plotting code"")\n\n    # the first argument is just a key for referring to this plot within the code\n    # the second argument says what kind of plot we will be making\n\n\n    plotlogger.set_plot_path(""./tmp"")\n\n    logger.create_plot(\'test plot\',MultipleTimeSeriesPlot)\n\n    plotlogger.add_plot_point(\'test plot\',\'validation\',(1,4))\n    plotlogger.add_plot_point(\'test plot\',\'validation\',(3,2))\n    plotlogger.add_plot_point(\'test plot\',\'validation\',(2,3))\n    plotlogger.add_plot_point(\'test plot\',\'validation\',(4,3))\n\n    plotlogger.add_plot_point(\'test plot\',\'training\',(1,3))\n    plotlogger.add_plot_point(\'test plot\',\'training\',(3,1))\n    plotlogger.add_plot_point(\'test plot\',\'training\',(2,2))\n    plotlogger.add_plot_point(\'test plot\',\'training\',(4,4))\n\n    plotlogger.save_plot(\'test plot\',title=\'Training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n\n    weights = [[1, 2, 3, 3], [1, 1, 2, 1], [2, 1, 2, 2]]\n    logger.create_plot(\'activation weight\', SingleWeightMatrixPlot)\n    plotlogger.add_plot_point(\'activation weight\', \'weight1\', weights)\n    plotlogger.add_plot_point(\'activation weight\', \'weight2\', weights)\n    plotlogger.add_plot_point(\'activation weight\', \'weight3\', weights)\n\n    plotlogger.save_plot(\'activation weight\', title=\'weight\', xlabel=\'dimension\', ylabel=\'dimension\')\n'"
src/models/__init__.py,0,b''
src/models/deep_rnn.py,0,"b'\nimport sys\n\nimport numpy as np\nfrom collections import OrderedDict\n\nimport theano\nimport theano.tensor as T\nfrom theano.tensor.shared_randomstreams import RandomStreams\n\nfrom layers.gating import SimplifiedLstm, BidirectionSLstm, VanillaLstm, BidirectionLstm, VanillaRNN, SimplifiedGRU, GatedRecurrentUnit, LstmNoPeepholes, LstmNOG, LstmNIG, LstmNFG\nfrom layers.layers import GeneralLayer, LinearLayer, SigmoidLayer\nfrom layers.recurrent_output_layer import RecurrentOutputLayer\nfrom layers.lhuc_layer import SigmoidLayer_LHUC, VanillaLstm_LHUC\n\nfrom training_schemes.rprop import compile_RPROP_train_function\nfrom training_schemes.adam_v2 import compile_ADAM_train_function\n\nimport logging\n\nclass DeepRecurrentNetwork(object):\n    """"""\n    This class is to assemble various neural network architectures. From basic feedforward neural network to bidirectional gated recurrent neural networks and hybrid architecture. **Hybrid** means a combination of feedforward and recurrent architecture.\n\n    """"""\n\n\n    def __init__(self, n_in, hidden_layer_size, n_out, L1_reg, L2_reg, hidden_layer_type, output_type=\'LINEAR\', dropout_rate=0.0, optimizer=\'sgd\', loss_function=\'MMSE\', rnn_batch_training=False):\n        """""" This function initialises a neural network\n\n        :param n_in: Dimensionality of input features\n        :type in: Integer\n        :param hidden_layer_size: The layer size for each hidden layer\n        :type hidden_layer_size: A list of integers\n        :param n_out: Dimensionality of output features\n        :type n_out: Integrer\n        :param hidden_layer_type: the activation types of each hidden layers, e.g., TANH, LSTM, GRU, BLSTM\n        :param L1_reg: the L1 regulasation weight\n        :param L2_reg: the L2 regulasation weight\n        :param output_type: the activation type of the output layer, by default is \'LINEAR\', linear regression.\n        :param dropout_rate: probability of dropout, a float number between 0 and 1.\n        """"""\n\n        logger = logging.getLogger(""DNN initialization"")\n\n        self.n_in = int(n_in)\n        self.n_out = int(n_out)\n\n        self.n_layers = len(hidden_layer_size)\n\n        self.dropout_rate = dropout_rate\n        self.optimizer = optimizer\n        self.loss_function = loss_function\n        self.is_train = T.iscalar(\'is_train\')\n        self.rnn_batch_training = rnn_batch_training\n\n        assert len(hidden_layer_size) == len(hidden_layer_type)\n\n        self.list_of_activations = [\'TANH\', \'SIGMOID\', \'SOFTMAX\', \'RELU\', \'RESU\']\n\n        if self.rnn_batch_training:\n            self.x = T.tensor3(\'x\')\n            self.y = T.tensor3(\'y\')\n        else:\n            self.x = T.matrix(\'x\')\n            self.y = T.matrix(\'y\')\n\n        self.L1_reg = L1_reg\n        self.L2_reg = L2_reg\n\n        self.rnn_layers = []\n        self.params = []\n        self.delta_params = []\n\n        rng = np.random.RandomState(123)\n\n        for i in range(self.n_layers):\n            if i == 0:\n                input_size = n_in\n            else:\n                input_size = hidden_layer_size[i-1]\n\n            if i == 0:\n                layer_input = self.x\n            else:\n                layer_input = self.rnn_layers[i-1].output\n                if hidden_layer_type[i-1]  == \'BSLSTM\' or hidden_layer_type[i-1]  == \'BLSTM\':\n                    input_size = hidden_layer_size[i-1]*2\n\n            \n            if hidden_layer_type[i] in self.list_of_activations:\n                hidden_activation = hidden_layer_type[i].lower()\n                hidden_layer = GeneralLayer(rng, layer_input, input_size, hidden_layer_size[i], activation=hidden_activation, p=self.dropout_rate, training=self.is_train)\n            elif hidden_layer_type[i] == \'TANH_LHUC\':\n                hidden_layer = SigmoidLayer_LHUC(rng, layer_input, input_size, hidden_layer_size[i], activation=T.tanh, p=self.dropout_rate, training=self.is_train)\n            elif hidden_layer_type[i] == \'SLSTM\':\n                hidden_layer = SimplifiedLstm(rng, layer_input, input_size, hidden_layer_size[i], p=self.dropout_rate, training=self.is_train, rnn_batch_training=self.rnn_batch_training)\n            elif hidden_layer_type[i] == \'SGRU\':\n                hidden_layer = SimplifiedGRU(rng, layer_input, input_size, hidden_layer_size[i], p=self.dropout_rate, training=self.is_train, rnn_batch_training=self.rnn_batch_training)\n            elif hidden_layer_type[i] == \'GRU\':\n                hidden_layer = GatedRecurrentUnit(rng, layer_input, input_size, hidden_layer_size[i], p=self.dropout_rate, training=self.is_train, rnn_batch_training=self.rnn_batch_training)\n            elif hidden_layer_type[i] == \'LSTM_NFG\':\n                hidden_layer = LstmNFG(rng, layer_input, input_size, hidden_layer_size[i], p=self.dropout_rate, training=self.is_train, rnn_batch_training=self.rnn_batch_training)\n            elif hidden_layer_type[i] == \'LSTM_NOG\':\n                hidden_layer = LstmNOG(rng, layer_input, input_size, hidden_layer_size[i], p=self.dropout_rate, training=self.is_train, rnn_batch_training=self.rnn_batch_training)\n            elif hidden_layer_type[i] == \'LSTM_NIG\':\n                hidden_layer = LstmNIG(rng, layer_input, input_size, hidden_layer_size[i], p=self.dropout_rate, training=self.is_train, rnn_batch_training=self.rnn_batch_training)\n            elif hidden_layer_type[i] == \'LSTM_NPH\':\n                hidden_layer = LstmNoPeepholes(rng, layer_input, input_size, hidden_layer_size[i], p=self.dropout_rate, training=self.is_train, rnn_batch_training=self.rnn_batch_training)\n            elif hidden_layer_type[i] == \'LSTM\':\n                hidden_layer = VanillaLstm(rng, layer_input, input_size, hidden_layer_size[i], p=self.dropout_rate, training=self.is_train, rnn_batch_training=self.rnn_batch_training)\n            elif hidden_layer_type[i] == \'BSLSTM\':\n                hidden_layer = BidirectionSLstm(rng, layer_input, input_size, hidden_layer_size[i], hidden_layer_size[i], p=self.dropout_rate, training=self.is_train, rnn_batch_training=self.rnn_batch_training)\n            elif hidden_layer_type[i] == \'BLSTM\':\n                hidden_layer = BidirectionLstm(rng, layer_input, input_size, hidden_layer_size[i], hidden_layer_size[i], p=self.dropout_rate, training=self.is_train, rnn_batch_training=self.rnn_batch_training)\n            elif hidden_layer_type[i] == \'RNN\':\n                hidden_layer = VanillaRNN(rng, layer_input, input_size, hidden_layer_size[i], p=self.dropout_rate, training=self.is_train, rnn_batch_training=self.rnn_batch_training)\n            elif hidden_layer_type[i] == \'LSTM_LHUC\':\n                hidden_layer = VanillaLstm_LHUC(rng, layer_input, input_size, hidden_layer_size[i], p=self.dropout_rate, training=self.is_train, rnn_batch_training=self.rnn_batch_training)\n            else:\n                logger.critical(""This hidden layer type: %s is not supported right now! \\n Please use one of the following: SLSTM, BSLSTM, TANH, SIGMOID\\n"" %(hidden_layer_type[i]))\n                sys.exit(1)\n\n            self.rnn_layers.append(hidden_layer)\n            self.params.extend(hidden_layer.params)\n\n        input_size = hidden_layer_size[-1]\n        if hidden_layer_type[-1]  == \'BSLSTM\' or hidden_layer_type[-1]  == \'BLSTM\':\n            input_size = hidden_layer_size[-1]*2\n\n        output_activation = output_type.lower()\n        if output_activation == \'linear\':\n            self.final_layer = LinearLayer(rng, self.rnn_layers[-1].output, input_size, self.n_out)\n        elif output_activation == \'recurrent\':\n            self.final_layer = RecurrentOutputLayer(rng, self.rnn_layers[-1].output, input_size, self.n_out, rnn_batch_training=self.rnn_batch_training)\n        elif output_type.upper() in self.list_of_activations:\n            self.final_layer = GeneralLayer(rng, self.rnn_layers[-1].output, input_size, self.n_out, activation=output_activation)\n        else:\n            logger.critical(""This output layer type: %s is not supported right now! \\n Please use one of the following: LINEAR, BSLSTM\\n"" %(output_type))\n            sys.exit(1)\n\n        self.params.extend(self.final_layer.params)\n\n        self.updates = {}\n        for param in self.params:\n            self.updates[param] = theano.shared(value = np.zeros(param.get_value(borrow = True).shape,\n                                                dtype = theano.config.floatX), name = \'updates\')\n\n        if self.loss_function == \'CCE\':\n            self.finetune_cost = self.categorical_crossentropy_loss(self.final_layer.output, self.y) \n            self.errors        = self.categorical_crossentropy_loss(self.final_layer.output, self.y) \n        elif self.loss_function == \'Hinge\':    \n            self.finetune_cost = self.multiclass_hinge_loss(self.final_layer.output, self.y)\n            self.errors        = self.multiclass_hinge_loss(self.final_layer.output, self.y)\n        elif self.loss_function == \'MMSE\':\n            if self.rnn_batch_training:\n                self.y_mod = T.reshape(self.y, (-1, n_out))\n                self.final_layer_output = T.reshape(self.final_layer.output, (-1, n_out))\n\n                nonzero_rows = T.any(self.y_mod, 1).nonzero()\n            \n                self.y_mod = self.y_mod[nonzero_rows]\n                self.final_layer_output = self.final_layer_output[nonzero_rows]\n            \n                self.finetune_cost = T.mean(T.sum((self.final_layer_output - self.y_mod) ** 2, axis=1))\n                self.errors = T.mean(T.sum((self.final_layer_output - self.y_mod) ** 2, axis=1))\n            else:\n                self.finetune_cost = T.mean(T.sum((self.final_layer.output - self.y) ** 2, axis=1))\n                self.errors = T.mean(T.sum((self.final_layer.output - self.y) ** 2, axis=1))\n\n    def categorical_crossentropy_loss(self, predictions, targets):\n        return T.nnet.categorical_crossentropy(predictions, targets).mean()\n\n    def multiclass_hinge_loss(self, predictions, targets, delta=1):\n        num_cls = predictions.shape[1]\n        if targets.ndim == predictions.ndim - 1:\n            targets = T.extra_ops.to_one_hot(targets, num_cls)\n        elif targets.ndim != predictions.ndim:\n            raise TypeError(\'rank mismatch between targets and predictions\')\n        corrects = predictions[targets.nonzero()]\n        rest = T.reshape(predictions[(1-targets).nonzero()],\n                                 (-1, num_cls-1))\n        rest = T.max(rest, axis=1)\n        return T.nnet.relu(rest - corrects + delta).mean()\n\n\n    def build_finetune_functions(self, train_shared_xy, valid_shared_xy, use_lhuc=False, layer_index=0):\n        """""" This function is to build finetune functions and to update gradients\n\n        :param train_shared_xy: theano shared variable for input and output training data\n        :type train_shared_xy: tuple of shared variable\n        :param valid_shared_xy: theano shared variable for input and output development data\n        :type valid_shared_xy: tuple of shared variable\n        :returns: finetune functions for training and development\n\n        """"""\n\n        logger = logging.getLogger(""DNN initialization"")\n\n        (train_set_x, train_set_y) = train_shared_xy\n        (valid_set_x, valid_set_y) = valid_shared_xy\n\n        lr = T.scalar(\'lr\', dtype = theano.config.floatX)\n        mom = T.scalar(\'mom\', dtype = theano.config.floatX)  # momentum\n\n        cost = self.finetune_cost #+ self.L2_reg * self.L2_sqr\n        \n        ## added for LHUC\n        if use_lhuc:\n            # In lhuc the parameters are only scaling parameters which have the name \'c\'\n            self.lhuc_params = []\n            for p in self.params:\n                if p.name == \'c\':\n                    self.lhuc_params.append(p)\n            params = self.lhuc_params\n            gparams = T.grad(cost, params)\n        else:\n            params = self.params\n            gparams = T.grad(cost, params)\n\n\n        freeze_params = 0\n        for layer in range(layer_index):\n            freeze_params += len(self.rnn_layers[layer].params)\n\n        # use optimizer\n        if self.optimizer==\'sgd\':\n            # zip just concatenate two lists\n            updates = OrderedDict()\n\n            for i, (param, gparam) in enumerate(zip(params, gparams)):\n                weight_update = self.updates[param]\n                upd = mom * weight_update - lr * gparam\n                updates[weight_update] = upd\n\n                # freeze layers and update weights\n                if i >= freeze_params:\n                    updates[param] = param + upd\n\n        elif self.optimizer==\'adam\':\n            updates = compile_ADAM_train_function(self, gparams, learning_rate=lr)\n        elif self.optimizer==\'rprop\':\n            updates = compile_RPROP_train_function(self, gparams)\n        else: \n            logger.critical(""This optimizer: %s is not supported right now! \\n Please use one of the following: sgd, adam, rprop\\n"" %(self.optimizer))\n            sys.exit(1)\n\n        train_model = theano.function(inputs = [lr, mom],  #index, batch_size\n                                      outputs = self.errors,\n                                      updates = updates,\n                                      givens = {self.x: train_set_x, #[index*batch_size:(index + 1)*batch_size]\n                                                self.y: train_set_y,\n                                                self.is_train: np.cast[\'int32\'](1)}, on_unused_input=\'ignore\')\n\n\n        valid_model = theano.function(inputs = [],\n                                      outputs = self.errors,\n                                      givens = {self.x: valid_set_x,\n                                                self.y: valid_set_y,\n                                                self.is_train: np.cast[\'int32\'](0)}, on_unused_input=\'ignore\')\n\n        return  train_model, valid_model\n\n    def parameter_prediction(self, test_set_x):  #, batch_size\n        """""" This function is to predict the output of NN\n\n        :param test_set_x: input features for a testing sentence\n        :type test_set_x: python array variable\n        :returns: predicted features\n\n        """"""\n\n\n        n_test_set_x = test_set_x.shape[0]\n\n        test_out = theano.function([], self.final_layer.output,\n              givens={self.x: test_set_x, self.is_train: np.cast[\'int32\'](0)}, on_unused_input=\'ignore\')\n\n        predict_parameter = test_out()\n\n        return predict_parameter\n    \n    ## the function to output activations at a hidden layer\n    def generate_hidden_layer(self, test_set_x, bn_layer_index):\n        """""" This function is to predict the bottleneck features of NN\n\n        :param test_set_x: input features for a testing sentence\n        :type test_set_x: python array variable\n        :returns: predicted bottleneck features\n\n        """"""\n\n        n_test_set_x = test_set_x.shape[0]\n\n        test_out = theano.function([], self.rnn_layers[bn_layer_index].output,\n                givens={self.x: test_set_x, self.is_train: np.cast[\'int32\'](0)}, on_unused_input=\'ignore\')\n\n        predict_parameter = test_out()\n\n        return predict_parameter\n\n'"
src/models/dnn.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n###THEANO_FLAGS=\'cuda.root=/opt/cuda-5.0.35,mode=FAST_RUN,device=gpu0,floatX=float32,exception_verbosity=high\' python dnn.py\n""""""\n""""""\nimport pickle\nimport os\nimport sys\nimport time\n\nimport numpy\nfrom collections import OrderedDict\n\nimport theano\nimport theano.tensor as T\nfrom theano.tensor.shared_randomstreams import RandomStreams\n\nfrom layers.layers import LinearLayer, SigmoidLayer, HiddenLayer\nfrom utils.providers import ListDataProvider\n\nfrom training_schemes.rprop import compile_RPROP_train_function\n\nimport logging\n\nclass DNN(object):\n\n    def __init__(self, numpy_rng, theano_rng=None, n_ins=784,\n                 n_outs=10, l1_reg = None, l2_reg = None,\n                 hidden_layers_sizes=[500, 500],\n                 hidden_activation=\'tanh\', output_activation=\'linear\',\n                 use_rprop=0, rprop_init_update=0.001):\n\n        logger = logging.getLogger(""DNN initialization"")\n\n        self.sigmoid_layers = []\n        self.params = []\n        self.delta_params   = []\n        self.n_layers = len(hidden_layers_sizes)\n\n        self.output_activation = output_activation\n\n        self.use_rprop = use_rprop\n        self.rprop_init_update = rprop_init_update\n\n        self.l1_reg = l1_reg\n        self.l2_reg = l2_reg\n\n        assert self.n_layers > 0\n\n        if not theano_rng:\n            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n\n        # allocate symbolic variables for the data\n        self.x = T.matrix(\'x\')\n        self.y = T.matrix(\'y\')\n\n        for i in range(self.n_layers):\n            if i == 0:\n                input_size = n_ins\n            else:\n                input_size = hidden_layers_sizes[i - 1]\n\n            if i == 0:\n                layer_input = self.x\n            else:\n                layer_input = self.sigmoid_layers[-1].output\n\n            sigmoid_layer = HiddenLayer(rng=numpy_rng,\n                                        input=layer_input,\n                                        n_in=input_size,\n                                        n_out=hidden_layers_sizes[i],\n                                        activation=T.tanh)  ##T.nnet.sigmoid)  #\n            self.sigmoid_layers.append(sigmoid_layer)\n            self.params.extend(sigmoid_layer.params)\n            self.delta_params.extend(sigmoid_layer.delta_params)\n\n        # add final layer\n        if self.output_activation == \'linear\':\n            self.final_layer = LinearLayer(rng = numpy_rng,\n                                           input=self.sigmoid_layers[-1].output,\n                                           n_in=hidden_layers_sizes[-1],\n                                           n_out=n_outs)\n        elif self.output_activation == \'sigmoid\':\n            self.final_layer = SigmoidLayer(\n                 rng = numpy_rng,\n                 input=self.sigmoid_layers[-1].output,\n                 n_in=hidden_layers_sizes[-1],\n                 n_out=n_outs, activation=T.nnet.sigmoid)\n        else:\n            logger.critical(""This output activation function: %s is not supported right now!"" %(self.output_activation))\n            sys.exit(1)\n\n        self.params.extend(self.final_layer.params)\n        self.delta_params.extend(self.final_layer.delta_params)\n\n        ### MSE\n        self.finetune_cost = T.mean(T.sum( (self.final_layer.output-self.y)*(self.final_layer.output-self.y), axis=1 ))\n\n        self.errors = T.mean(T.sum( (self.final_layer.output-self.y)*(self.final_layer.output-self.y), axis=1 ))\n\n        ### L1-norm\n        if self.l1_reg is not None:\n            for i in range(self.n_layers):\n                W = self.params[i * 2]\n                self.finetune_cost += self.l1_reg * (abs(W).sum())\n\n        ### L2-norm\n        if self.l2_reg is not None:\n            for i in range(self.n_layers):\n                W = self.params[i * 2]\n                self.finetune_cost += self.l2_reg * T.sqr(W).sum()\n\n\n    def build_finetune_functions(self, train_shared_xy, valid_shared_xy, batch_size, \\\n                                                     return_valid_score_i=False):\n\n        (train_set_x, train_set_y) = train_shared_xy\n        (valid_set_x, valid_set_y) = valid_shared_xy\n\n        # compute number of minibatches for training, validation and testing\n        n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n        n_valid_batches /= batch_size\n\n        index = T.lscalar(\'index\')  # index to a [mini]batch\n        learning_rate = T.fscalar(\'learning_rate\')\n        momentum = T.fscalar(\'momentum\')\n\n        layer_size = len(self.params)\n        lr_list = []\n        for i in range(layer_size):\n            lr_list.append(learning_rate)\n\n        ##top 2 layers use a smaller learning rate\n        ##hard-code now, change it later\n        if layer_size > 4:\n            for i in range(layer_size-4, layer_size):\n                lr_list[i] = learning_rate * 0.5\n\n        # compute list of fine-tuning updates\n        # compute the gradients with respect to the model parameters\n        gparams = T.grad(self.finetune_cost, self.params)\n\n        if self.use_rprop == 0:\n\n            updates = OrderedDict()\n            layer_index = 0\n            for dparam, gparam in zip(self.delta_params, gparams):\n                updates[dparam] = momentum * dparam - gparam * lr_list[layer_index]\n                layer_index += 1\n\n            for dparam, param in zip(self.delta_params, self.params):\n                updates[param] = param + updates[dparam]\n\n            on_unused_input_value = \'raise\'  ## Theano\'s default\n\n        elif self.use_rprop:\n            updates = compile_RPROP_train_function(self, gparams)\n            on_unused_input_value = \'warn\'\n\n\n        ## Retain learning rate and momentum to make interface backwards compatible,\n        ## even with RPROP where we don\'t use them, means we have to use on_unused_input=\'warn\'.\n\n        train_fn = theano.function(inputs=[index, theano.Param(learning_rate, default = 0.125),\n              theano.Param(momentum, default = 0.5)],\n              outputs=self.errors,\n              updates=updates,\n              on_unused_input=on_unused_input_value,\n              givens={self.x: train_set_x[index * batch_size:\n                                          (index + 1) * batch_size],\n                      self.y: train_set_y[index * batch_size:\n                                          (index + 1) * batch_size]})\n\n        valid_fn = theano.function([],\n              outputs=self.errors,\n              givens={self.x: valid_set_x,\n                      self.y: valid_set_y})\n\n        valid_score_i = theano.function([index],\n              outputs=self.errors,\n              givens={self.x: valid_set_x[index * batch_size:\n                                          (index + 1) * batch_size],\n                      self.y: valid_set_y[index * batch_size:\n                                          (index + 1) * batch_size]})\n\n        # Create a function that scans the entire validation set\n        def valid_score():\n            return [valid_score_i(i) for i in range(n_valid_batches)]\n\n        if return_valid_score_i:\n            return train_fn, valid_fn, valid_score_i\n        else:\n            return train_fn, valid_fn\n\n    def parameter_prediction(self, test_set_x):  #, batch_size\n\n        n_test_set_x = test_set_x.get_value(borrow=True).shape[0]\n\n        test_out = theano.function([], self.final_layer.output,\n              givens={self.x: test_set_x[0:n_test_set_x]})\n\n        predict_parameter = test_out()\n\n        return predict_parameter\n\n    ## the function to output activations at a hidden layer\n    def generate_top_hidden_layer(self, test_set_x, bn_layer_index):\n\n        n_test_set_x = test_set_x.get_value(borrow=True).shape[0]\n\n        test_out = theano.function([], self.sigmoid_layers[bn_layer_index].output,\n              givens={self.x: test_set_x[0:n_test_set_x]})\n\n        predict_parameter = test_out()\n\n        return predict_parameter\n\n\n\nif __name__ == \'__main__\':\n\n    train_scp = \'/afs/inf.ed.ac.uk/group/project/dnn_tts/data/nick/nn_scp/train.scp\'\n    valid_scp = \'/afs/inf.ed.ac.uk/group/project/dnn_tts/data/nick/nn_scp/gen.scp\'\n\n    model_dir = \'/afs/inf.ed.ac.uk/group/project/dnn_tts/practice/nnets_model\'\n\n    log_dir =  \'/afs/inf.ed.ac.uk/group/project/dnn_tts/practice/log\'\n\n    finetune_lr=0.01\n    pretraining_epochs=100\n    pretrain_lr=0.01\n    training_epochs=100\n    batch_size=32\n\n    n_ins = 898\n    n_outs = 229\n\n    hidden_layers_sizes = [512, 512, 512]\n\n#    test_DBN(train_scp, valid_scp, log_dir, model_dir, n_ins, n_outs, hidden_layers_sizes,\n#             finetune_lr, pretraining_epochs, pretrain_lr, training_epochs, batch_size)\n\n    dnn_generation()\n'"
src/models/dnn_cm.py,0,"b'###THEANO_FLAGS=\'cuda.root=/opt/cuda-5.0.35,mode=FAST_RUN,device=gpu0,floatX=float32,exception_verbosity=high\' python dnn.py\n""""""\n""""""\nimport pickle\nimport os\nimport sys\nimport time\n\nimport numpy# as np\nimport gnumpy as gnp\n\n#cudamat\n\n#import theano\n#import theano.tensor as T\n\nimport logging\n\nclass DNN(object):\n\n    def __init__(self, numpy_rng, n_ins=100,\n                 n_outs=100, l1_reg = None, l2_reg = None,\n                 hidden_layer_sizes=[500, 500],\n                 hidden_activation=\'tanh\', output_activation=\'linear\'):\n\n        logger = logging.getLogger(""DNN initialization"")\n\n        self.n_layers = len(hidden_layer_sizes)\n        self.l1_reg = l1_reg\n        self.l2_reg = l2_reg\n\n        assert self.n_layers > 0\n\n        self.W_params = []\n        self.b_params = []\n        self.mW_params = []\n        self.mb_params = []\n\n        for i in range(self.n_layers):\n            if i == 0:\n                input_size = n_ins\n            else:\n                input_size = hidden_layer_sizes[i-1]\n            W_value = gnp.garray(numpy_rng.normal(0.0, 1.0/numpy.sqrt(input_size), size=(input_size, hidden_layer_sizes[i])))\n            b_value = gnp.zeros(hidden_layer_sizes[i])\n            mW_value = gnp.zeros((input_size, hidden_layer_sizes[i]))\n            mb_value = gnp.zeros(hidden_layer_sizes[i])\n            self.W_params.append(W_value)\n            self.b_params.append(b_value)\n            self.mW_params.append(mW_value)\n            self.mb_params.append(mb_value)\n\n        #output layer\n        input_size = hidden_layer_sizes[self.n_layers-1]\n        W_value = gnp.garray(numpy_rng.normal(0.0, 1.0/numpy.sqrt(input_size), size=(input_size, n_outs)))\n        b_value = gnp.zeros(n_outs)\n        mW_value = gnp.zeros((input_size, n_outs))\n        mb_value = gnp.zeros(n_outs)\n        self.W_params.append(W_value)\n        self.b_params.append(b_value)\n        self.mW_params.append(mW_value)\n        self.mb_params.append(mb_value)\n\n    def backpropagation(self, train_set_y):\n#        (train_set_x, train_set_y) = train_xy\n\n        # assuming linear output and square error cost function\n        observation_error = self.final_layer_output - train_set_y\n\n        self.W_grads = []\n        self.b_grads = []\n        current_error = observation_error\n        current_activation = self.activations[-1]\n        current_W_grad = gnp.dot(current_activation.T, observation_error)\n        current_b_grad = gnp.dot(gnp.ones((1, observation_error.shape[0])), observation_error)\n        self.W_grads.append(current_W_grad)\n        self.b_grads.append(current_b_grad)\n\n        propagate_error = gnp.dot(observation_error, self.W_params[self.n_layers].T) # final layer is linear output, gradient is one\n        for i in reversed(list(range(self.n_layers))):\n            current_activation = self.activations[i]\n            current_gradient = 1.0 - current_activation ** 2\n            current_W_grad = gnp.dot(current_activation.T, propagate_error)\n            current_b_grad = gnp.dot(gnp.ones((1, propagate_error.shape[0])), propagate_error)\n            propagate_error = gnp.dot(propagate_error, self.W_params[i].T) * current_gradient\n\n            self.W_grads.insert(0, current_W_grad)\n            self.b_grads.insert(0, current_b_grad)\n\n\n    def feedforward(self, train_set_x):\n        self.activations = []\n\n        self.activations.append(train_set_x)\n\n        for i in range(self.n_layers):\n            current_activations = gnp.tanh(gnp.dot(self.activations[i], self.W_params[i]) + self.b_params[i])\n            self.activations.append(current_activations)\n\n        #output layers\n        self.final_layer_output = gnp.dot(self.activations[self.n_layers], self.W_params[self.n_layers]) + self.b_params[self.n_layers]\n\n    def gradient_update(self, batch_size, learning_rate, momentum):\n\n        multiplier = learning_rate / batch_size;\n        for i in range(len(self.W_grads)):\n\n            if i >= len(self.W_grads) - 2:\n                local_multiplier = multiplier * 0.5\n            else:\n                local_multiplier = multiplier\n\n            self.W_grads[i] = (self.W_grads[i] + self.W_params[i] * self.l2_reg) * local_multiplier\n            self.b_grads[i] = self.b_grads[i] * local_multiplier   # + self.b_params[i] * self.l2_reg\n\n            #update weights and record momentum weights\n            self.mW_params[i] = (self.mW_params[i] * momentum) - self.W_grads[i]\n            self.mb_params[i] = (self.mb_params[i] * momentum) - self.b_grads[i]\n            self.W_params[i] += self.mW_params[i]\n            self.b_params[i] += self.mb_params[i]\n#        print   self.W_params[0].shape, self.W_params[len(self.W_params)-1].shape\n\n    def finetune(self, train_xy, batch_size, learning_rate, momentum):\n        (train_set_x, train_set_y) = train_xy\n\n        train_set_x = gnp.as_garray(train_set_x)\n        train_set_y = gnp.as_garray(train_set_y)\n\n        self.feedforward(train_set_x)\n        self.backpropagation(train_set_y)\n        self.gradient_update(batch_size, learning_rate, momentum)\n\n        self.errors = gnp.sum((self.final_layer_output - train_set_y) ** 2, axis=1)\n\n        return  self.errors.as_numpy_array()\n\n    def parameter_prediction(self, test_set_x):\n        test_set_x = gnp.as_garray(test_set_x)\n\n        current_activations = test_set_x\n\n        for i in range(self.n_layers):\n            current_activations = gnp.tanh(gnp.dot(current_activations, self.W_params[i]) + self.b_params[i])\n\n        final_layer_output = gnp.dot(current_activations, self.W_params[self.n_layers]) + self.b_params[self.n_layers]\n\n        return  final_layer_output.as_numpy_array()\n\n#    def parameter_prediction(self, test_set_x):  #, batch_size\n\n#        n_test_set_x = test_set_x.get_value(borrow=True).shape[0]\n\n#        test_out = theano.function([], self.final_layer.output,\n#              givens={self.x: test_set_x[0:n_test_set_x]})\n#        predict_parameter = test_out()\n#        return predict_parameter\n\n\nif __name__ == \'__main__\':\n\n    train_scp = \'/afs/inf.ed.ac.uk/group/project/dnn_tts/data/nick/nn_scp/train.scp\'\n    valid_scp = \'/afs/inf.ed.ac.uk/group/project/dnn_tts/data/nick/nn_scp/gen.scp\'\n\n    model_dir = \'/afs/inf.ed.ac.uk/group/project/dnn_tts/practice/nnets_model\'\n\n    log_dir =  \'/afs/inf.ed.ac.uk/group/project/dnn_tts/practice/log\'\n\n    finetune_lr=0.01\n    pretraining_epochs=100\n    pretrain_lr=0.01\n    training_epochs=100\n    batch_size=32\n\n    n_ins = 898\n    n_outs = 229\n\n    hidden_layer_sizes = [512, 512, 512]\n\n#    test_DBN(train_scp, valid_scp, log_dir, model_dir, n_ins, n_outs, hidden_layer_sizes,\n#             finetune_lr, pretraining_epochs, pretrain_lr, training_epochs, batch_size)\n\n    dnn_generation()\n'"
src/models/hed_rnn.py,0,"b'\nimport sys\n\nimport numpy as np\nfrom collections import OrderedDict\n\nimport theano\nimport theano.tensor as T\nfrom theano.tensor.shared_randomstreams import RandomStreams\n\nfrom layers.gating import SimplifiedLstm, SimplifiedLstmDecoder, BidirectionSLstm, VanillaLstm, VanillaLstmDecoder, BidirectionLstm, VanillaRNN, VanillaRNNDecoder, SimplifiedGRU, GatedRecurrentUnit, LstmNoPeepholes, LstmNOG, LstmNIG, LstmNFG\nfrom layers.layers import GeneralLayer, LinearLayer, SigmoidLayer\nfrom layers.recurrent_output_layer import RecurrentOutputLayer\nfrom layers.lhuc_layer import SigmoidLayer_LHUC, VanillaLstm_LHUC\n\nfrom training_schemes.rprop import compile_RPROP_train_function\nfrom training_schemes.adam_v2 import compile_ADAM_train_function\n\nfrom models.seq2seq import VanillaSequenceEncoder, DistributedSequenceEncoder\n\nimport logging\n\nclass DeepEncoderDecoderNetwork(object):\n    """"""\n    This class is to assemble various neural network architectures. From basic feedforward neural network to bidirectional gated recurrent neural networks and hybrid architecture. **Hybrid** means a combination of feedforward and recurrent architecture.\n\n    """"""\n\n\n    def __init__(self, n_in, hidden_layer_size, n_out, L1_reg, L2_reg, hidden_layer_type, output_type=\'LINEAR\', network_type=\'S2S\', ed_type=\'HED\', dropout_rate=0.0, optimizer=\'sgd\', MLU_div_lengths = [], loss_function=\'MMSE\', rnn_batch_training=False):\n        """""" This function initialises a neural network\n\n        :param n_in: Dimensionality of input features\n        :type in: Integer\n        :param hidden_layer_size: The layer size for each hidden layer\n        :type hidden_layer_size: A list of integers\n        :param n_out: Dimensionality of output features\n        :type n_out: Integrer\n        :param hidden_layer_type: the activation types of each hidden layers, e.g., TANH, LSTM, GRU, BLSTM\n        :param L1_reg: the L1 regulasation weight\n        :param L2_reg: the L2 regulasation weight\n        :param output_type: the activation type of the output layer, by default is \'LINEAR\', linear regression.\n        :param dropout_rate: probability of dropout, a float number between 0 and 1.\n        """"""\n\n        logger = logging.getLogger(""DNN initialization"")\n\n        self.n_in = int(n_in)\n        self.n_out = int(n_out)\n\n        self.n_layers = len(hidden_layer_size)\n\n        self.dropout_rate = dropout_rate\n        self.optimizer = optimizer\n        self.loss_function = loss_function\n        self.is_train = T.iscalar(\'is_train\')\n        self.rnn_batch_training = rnn_batch_training\n\n        assert len(hidden_layer_size) == len(hidden_layer_type)\n\n        self.list_of_activations = [\'TANH\', \'SIGMOID\', \'SOFTMAX\', \'RELU\', \'RESU\']\n        \n        BLSTM_variants   = [\'BLSTM\', \'BSLSTM\', \'BLSTME\', \'BSLSTME\']\n        Encoder_variants = [\'RNNE\', \'LSTME\', \'BLSTME\', \'SLSTME\', \'TANHE\']\n        Decoder_variants = [\'RNND\', \'LSTMD\', \'SLSTMD\']\n\n        if self.rnn_batch_training:\n            self.x = T.tensor3(\'x\')\n            self.y = T.tensor3(\'y\')\n        else:\n            self.x = T.matrix(\'x\')\n            self.y = T.matrix(\'y\')\n        \n        if network_type == ""S2S"":\n            self.d = T.ivector(\'d\')\n            self.f = T.matrix(\'f\')\n\n        self.L1_reg = L1_reg\n        self.L2_reg = L2_reg\n\n        self.rnn_layers = []\n        self.params = []\n        self.delta_params = []\n\n        rng = np.random.RandomState(123)\n\n        prev_seg_end = 0\n        encoder_count = 0\n        MLU_div = MLU_div_lengths\n        for i in range(self.n_layers):\n            if i == 0:\n                input_size = n_in\n            else:\n                input_size = hidden_layer_size[i-1]\n                if hidden_layer_type[i-1] in BLSTM_variants:\n                    input_size = hidden_layer_size[i-1]*2\n\n            if i == 0:\n                layer_input = self.x\n            else:\n                layer_input = self.rnn_layers[i-1].output\n            \n            ### sequence-to-sequence mapping ###\n            if hidden_layer_type[i-1] in Encoder_variants:\n                dur_input        = self.d\n                frame_feat_input = self.f\n\n                # vanilla encoder-decoder (phone-level features)\n                if ed_type == ""VED"":\n                    seq2seq_model = DistributedSequenceEncoder(rng, layer_input, dur_input)\n                    layer_input   = T.concatenate((seq2seq_model.encoded_output, frame_feat_input), axis=1)\n                    input_size    = input_size+4\n                # hierarchical encoder-decoder\n                elif ed_type == ""HED"":\n                    seg_len       = layer_input.size//input_size\n                    seg_dur_input = dur_input[prev_seg_end: prev_seg_end+seg_len]\n                    num_of_segs   = T.sum(seg_dur_input)\n                    seq2seq_model = DistributedSequenceEncoder(rng, layer_input, seg_dur_input)\n                    addfeat_input = frame_feat_input[0:num_of_segs, MLU_div[encoder_count]:MLU_div[encoder_count+1]]  \n                    layer_input   = T.concatenate((seq2seq_model.encoded_output, addfeat_input), axis=1)\n                    input_size    = input_size + (MLU_div[encoder_count+1]-MLU_div[encoder_count])\n                    prev_seg_end  = prev_seg_end + seg_len\n                    encoder_count = encoder_count + 1\n\n            # hidden layer activation\n            if hidden_layer_type[i] in self.list_of_activations:\n                hidden_activation = hidden_layer_type[i].lower()\n                hidden_layer = GeneralLayer(rng, layer_input, input_size, hidden_layer_size[i], activation=hidden_activation, p=self.dropout_rate, training=self.is_train)\n            elif hidden_layer_type[i] == \'TANHE\' or hidden_layer_type[i] == \'SIGMOIDE\':\n                hidden_activation = hidden_layer_type[i][0:-1].lower()\n                hidden_layer = GeneralLayer(rng, layer_input, input_size, hidden_layer_size[i], activation=hidden_activation, p=self.dropout_rate, training=self.is_train)\n            elif hidden_layer_type[i] == \'TANH_LHUC\':\n                hidden_layer = SigmoidLayer_LHUC(rng, layer_input, input_size, hidden_layer_size[i], activation=T.tanh, p=self.dropout_rate, training=self.is_train)\n            elif hidden_layer_type[i] == \'SLSTM\' or hidden_layer_type[i] == \'SLSTME\':\n                hidden_layer = SimplifiedLstm(rng, layer_input, input_size, hidden_layer_size[i], p=self.dropout_rate, training=self.is_train, rnn_batch_training=self.rnn_batch_training)\n            elif hidden_layer_type[i] == \'SLSTMD\':\n                hidden_layer = SimplifiedLstmDecoder(rng, layer_input, input_size, hidden_layer_size[i], self.n_out, p=self.dropout_rate, training=self.is_train, rnn_batch_training=self.rnn_batch_training)\n            elif hidden_layer_type[i] == \'SGRU\':\n                hidden_layer = SimplifiedGRU(rng, layer_input, input_size, hidden_layer_size[i], p=self.dropout_rate, training=self.is_train, rnn_batch_training=self.rnn_batch_training)\n            elif hidden_layer_type[i] == \'GRU\':\n                hidden_layer = GatedRecurrentUnit(rng, layer_input, input_size, hidden_layer_size[i], p=self.dropout_rate, training=self.is_train, rnn_batch_training=self.rnn_batch_training)\n            elif hidden_layer_type[i] == \'LSTM\' or hidden_layer_type[i] == \'LSTME\':\n                hidden_layer = VanillaLstm(rng, layer_input, input_size, hidden_layer_size[i], p=self.dropout_rate, training=self.is_train, rnn_batch_training=self.rnn_batch_training)\n            elif hidden_layer_type[i] == \'LSTMD\':\n                hidden_layer = VanillaLstmDecoder(rng, layer_input, input_size, hidden_layer_size[i], self.n_out, p=self.dropout_rate, training=self.is_train, rnn_batch_training=self.rnn_batch_training)\n            elif hidden_layer_type[i] == \'BSLSTM\' or hidden_layer_type[i] == \'BSLSTME\':\n                hidden_layer = BidirectionSLstm(rng, layer_input, input_size, hidden_layer_size[i], hidden_layer_size[i], p=self.dropout_rate, training=self.is_train, rnn_batch_training=self.rnn_batch_training)\n            elif hidden_layer_type[i] == \'BLSTM\' or hidden_layer_type[i] == \'BLSTME\':\n                hidden_layer = BidirectionLstm(rng, layer_input, input_size, hidden_layer_size[i], hidden_layer_size[i], p=self.dropout_rate, training=self.is_train, rnn_batch_training=self.rnn_batch_training)\n            elif hidden_layer_type[i] == \'RNN\' or hidden_layer_type[i] == \'RNNE\':\n                hidden_layer = VanillaRNN(rng, layer_input, input_size, hidden_layer_size[i], p=self.dropout_rate, training=self.is_train, rnn_batch_training=self.rnn_batch_training)\n            elif hidden_layer_type[i] == \'RNND\':\n                hidden_layer = VanillaRNNDecoder(rng, layer_input, input_size, hidden_layer_size[i], self.n_out, p=self.dropout_rate, training=self.is_train, rnn_batch_training=self.rnn_batch_training)\n            elif hidden_layer_type[i] == \'LSTM_LHUC\':\n                hidden_layer = VanillaLstm_LHUC(rng, layer_input, input_size, hidden_layer_size[i], p=self.dropout_rate, training=self.is_train, rnn_batch_training=self.rnn_batch_training)\n            else:\n                logger.critical(""This hidden layer type: %s is not supported right now! \\n Please use one of the following: SLSTM, BSLSTM, TANH, SIGMOID\\n"" %(hidden_layer_type[i]))\n                sys.exit(1)\n\n            self.rnn_layers.append(hidden_layer)\n            self.params.extend(hidden_layer.params)\n\n        input_size = hidden_layer_size[-1]\n        if hidden_layer_type[-1] in BLSTM_variants:\n            input_size = hidden_layer_size[-1]*2\n\n        if hidden_layer_type[-1] in Decoder_variants:\n            self.final_layer = self.rnn_layers[-1]\n        else:\n            output_activation = output_type.lower()\n            if output_activation == \'linear\':\n                self.final_layer = LinearLayer(rng, self.rnn_layers[-1].output, input_size, self.n_out)\n            elif output_activation == \'recurrent\':\n                self.final_layer = RecurrentOutputLayer(rng, self.rnn_layers[-1].output, input_size, self.n_out, rnn_batch_training=self.rnn_batch_training)\n            elif output_type.upper() in self.list_of_activations:\n                self.final_layer = GeneralLayer(rng, self.rnn_layers[-1].output, input_size, self.n_out, activation=output_activation)\n            else:\n                logger.critical(""This output layer type: %s is not supported right now! \\n Please use one of the following: LINEAR, BSLSTM\\n"" %(output_type))\n                sys.exit(1)\n\n            self.params.extend(self.final_layer.params)\n\n        self.updates = {}\n        for param in self.params:\n            self.updates[param] = theano.shared(value = np.zeros(param.get_value(borrow = True).shape,\n                                                dtype = theano.config.floatX), name = \'updates\')\n\n        if self.loss_function == \'CCE\':\n            self.finetune_cost = self.categorical_crossentropy_loss(self.final_layer.output, self.y) \n            self.errors        = self.categorical_crossentropy_loss(self.final_layer.output, self.y) \n        elif self.loss_function == \'Hinge\':    \n            self.finetune_cost = self.multiclass_hinge_loss(self.final_layer.output, self.y)\n            self.errors        = self.multiclass_hinge_loss(self.final_layer.output, self.y)\n        elif self.loss_function == \'MMSE\':\n            if self.rnn_batch_training:\n                self.y_mod = T.reshape(self.y, (-1, n_out))\n                self.final_layer_output = T.reshape(self.final_layer.output, (-1, n_out))\n\n                nonzero_rows = T.any(self.y_mod, 1).nonzero()\n            \n                self.y_mod = self.y_mod[nonzero_rows]\n                self.final_layer_output = self.final_layer_output[nonzero_rows]\n            \n                self.finetune_cost = T.mean(T.sum((self.final_layer_output - self.y_mod) ** 2, axis=1))\n                self.errors = T.mean(T.sum((self.final_layer_output - self.y_mod) ** 2, axis=1))\n            else:\n                self.finetune_cost = T.mean(T.sum((self.final_layer.output - self.y) ** 2, axis=1))\n                self.errors = T.mean(T.sum((self.final_layer.output - self.y) ** 2, axis=1))\n\n    def categorical_crossentropy_loss(self, predictions, targets):\n        return T.nnet.categorical_crossentropy(predictions, targets).mean()\n\n    def multiclass_hinge_loss(self, predictions, targets, delta=1):\n        num_cls = predictions.shape[1]\n        if targets.ndim == predictions.ndim - 1:\n            targets = T.extra_ops.to_one_hot(targets, num_cls)\n        elif targets.ndim != predictions.ndim:\n            raise TypeError(\'rank mismatch between targets and predictions\')\n        corrects = predictions[targets.nonzero()]\n        rest = T.reshape(predictions[(1-targets).nonzero()],\n                                 (-1, num_cls-1))\n        rest = T.max(rest, axis=1)\n        return T.nnet.relu(rest - corrects + delta).mean()\n\n\n    def build_finetune_functions(self, train_shared_xy, valid_shared_xy, use_lhuc=False, layer_index=0):\n        """""" This function is to build finetune functions and to update gradients\n\n        :param train_shared_xy: theano shared variable for input and output training data\n        :type train_shared_xy: tuple of shared variable\n        :param valid_shared_xy: theano shared variable for input and output development data\n        :type valid_shared_xy: tuple of shared variable\n        :returns: finetune functions for training and development\n\n        """"""\n\n        logger = logging.getLogger(""DNN initialization"")\n\n        (train_set_x, train_set_y) = train_shared_xy\n        (valid_set_x, valid_set_y) = valid_shared_xy\n\n        lr = T.scalar(\'lr\', dtype = theano.config.floatX)\n        mom = T.scalar(\'mom\', dtype = theano.config.floatX)  # momentum\n\n        cost = self.finetune_cost #+ self.L2_reg * self.L2_sqr\n        \n        ## added for LHUC\n        if use_lhuc:\n            # In lhuc the parameters are only scaling parameters which have the name \'c\'\n            self.lhuc_params = []\n            for p in self.params:\n                if p.name == \'c\':\n                    self.lhuc_params.append(p)\n            params = self.lhuc_params\n            gparams = T.grad(cost, params)\n        else:\n            params = self.params\n            gparams = T.grad(cost, params)\n\n\n        freeze_params = 0\n        for layer in range(layer_index):\n            freeze_params += len(self.rnn_layers[layer].params)\n\n        # use optimizer\n        if self.optimizer==\'sgd\':\n            # zip just concatenate two lists\n            updates = OrderedDict()\n\n            for i, (param, gparam) in enumerate(zip(params, gparams)):\n                weight_update = self.updates[param]\n                upd = mom * weight_update - lr * gparam\n                updates[weight_update] = upd\n\n                # freeze layers and update weights\n                if i >= freeze_params:\n                    updates[param] = param + upd\n\n        elif self.optimizer==\'adam\':\n            updates = compile_ADAM_train_function(self, gparams, learning_rate=lr)\n        elif self.optimizer==\'rprop\':\n            updates = compile_RPROP_train_function(self, gparams)\n        else: \n            logger.critical(""This optimizer: %s is not supported right now! \\n Please use one of the following: sgd, adam, rprop\\n"" %(self.optimizer))\n            sys.exit(1)\n\n        train_model = theano.function(inputs = [lr, mom],  #index, batch_size\n                                      outputs = self.errors,\n                                      updates = updates,\n                                      givens = {self.x: train_set_x, #[index*batch_size:(index + 1)*batch_size]\n                                                self.y: train_set_y,\n                                                self.is_train: np.cast[\'int32\'](1)}, on_unused_input=\'ignore\')\n\n\n        valid_model = theano.function(inputs = [],\n                                      outputs = self.errors,\n                                      givens = {self.x: valid_set_x,\n                                                self.y: valid_set_y,\n                                                self.is_train: np.cast[\'int32\'](0)}, on_unused_input=\'ignore\')\n\n        return  train_model, valid_model\n\n    def build_finetune_functions_S2S(self, train_shared_xyd, valid_shared_xyd):\n        """""" This function is to build finetune functions and to update gradients\n        \n        :param train_shared_xy: theano shared variable for input and output training data \n        :type train_shared_xy: tuple of shared variable\n        :param valid_shared_xy: theano shared variable for input and output development data\n        :type valid_shared_xy: tuple of shared variable\n        :returns: finetune functions for training and development\n        \n        """"""\n\n        (train_set_x, train_set_y, train_set_d) = train_shared_xyd\n        (valid_set_x, valid_set_y, valid_set_d) = valid_shared_xyd\n\n        lr = T.scalar(\'lr\', dtype = theano.config.floatX)\n        mom = T.scalar(\'mom\', dtype = theano.config.floatX)  # momentum\n\n        cost = self.finetune_cost #+ self.L2_reg * self.L2_sqr\n\n        gparams = T.grad(cost, self.params)\n\n\n        # zip just concatenate two lists\n        updates = OrderedDict()\n\n        for param, gparam in zip(self.params, gparams):\n            weight_update = self.updates[param]\n            upd = mom * weight_update - lr * gparam\n            updates[weight_update] = upd\n            updates[param] = param + upd\n\n        train_model = theano.function(inputs = [lr, mom],  \n                                      outputs = self.errors,\n                                      updates = updates,\n                                      givens = {self.x: train_set_x, \n                                                self.y: train_set_y,\n                                                self.d: train_set_d,\n                                                self.is_train: np.cast[\'int32\'](1)}, on_unused_input=\'ignore\')\n\n\n        valid_model = theano.function(inputs = [],\n                                      outputs = self.errors,\n                                      givens = {self.x: valid_set_x,\n                                                self.y: valid_set_y,\n                                                self.d: valid_set_d,\n                                                self.is_train: np.cast[\'int32\'](0)}, on_unused_input=\'ignore\')\n \n        return  train_model, valid_model\n\n    def build_finetune_functions_S2SPF(self, train_shared_xydf, valid_shared_xydf, layer_index=6):\n        """""" This function is to build finetune functions and to update gradients\n        \n        :param train_shared_xy: theano shared variable for input and output training data \n        :type train_shared_xy: tuple of shared variable\n        :param valid_shared_xy: theano shared variable for input and output development data\n        :type valid_shared_xy: tuple of shared variable\n        :returns: finetune functions for training and development\n        \n        """"""\n\n        (train_set_x, train_set_y, train_set_d, train_set_f) = train_shared_xydf\n        (valid_set_x, valid_set_y, valid_set_d, valid_set_f) = valid_shared_xydf\n\n        lr = T.scalar(\'lr\', dtype = theano.config.floatX)\n        mom = T.scalar(\'mom\', dtype = theano.config.floatX)  # momentum\n\n        cost = self.finetune_cost #+ self.L2_reg * self.L2_sqr\n\n        params = self.params\n        gparams = T.grad(cost, params)\n\n        encoder_params = 0\n        for layer in range(layer_index):\n            encoder_params += len(self.rnn_layers[layer].params)\n\n        # use optimizer\n        if self.optimizer==\'sgd\':\n            # zip just concatenate two lists\n            updates = OrderedDict()\n\n            for i, (param, gparam) in enumerate(zip(params, gparams)):\n                weight_update = self.updates[param]\n                if i >= encoder_params:\n                    upd = mom * weight_update - lr * gparam\n                else:\n                    upd = mom * weight_update - (lr*2) * gparam\n                updates[weight_update] = upd\n                updates[param] = param + upd\n\n        elif self.optimizer==\'adam\':\n            updates = compile_ADAM_train_function(self, gparams, learning_rate=lr)\n        elif self.optimizer==\'rprop\':\n            updates = compile_RPROP_train_function(self, gparams)\n        else: \n            logger.critical(""This optimizer: %s is not supported right now! \\n Please use one of the following: sgd, adam, rprop\\n"" %(self.optimizer))\n            sys.exit(1)\n\n\n        train_model = theano.function(inputs = [lr, mom],  \n                                      outputs = self.errors,\n                                      updates = updates,\n                                      givens = {self.x: train_set_x, \n                                                self.y: train_set_y,\n                                                self.d: train_set_d,\n                                                self.f: train_set_f,\n                                                self.is_train: np.cast[\'int32\'](1)}, on_unused_input=\'ignore\')\n\n\n        valid_model = theano.function(inputs = [],\n                                      outputs = self.errors,\n                                      givens = {self.x: valid_set_x,\n                                                self.y: valid_set_y,\n                                                self.d: valid_set_d,\n                                                self.f: valid_set_f,\n                                                self.is_train: np.cast[\'int32\'](0)}, on_unused_input=\'ignore\')\n \n        return  train_model, valid_model\n\n    def parameter_prediction(self, test_set_x):  #, batch_size\n        """""" This function is to predict the output of NN\n        \n        :param test_set_x: input features for a testing sentence\n        :type test_set_x: python array variable\n        :returns: predicted features\n        \n        """"""\n    \n\n        n_test_set_x = test_set_x.shape[0]\n\n        test_out = theano.function([], self.final_layer.output,\n              givens={self.x: test_set_x[0:n_test_set_x], self.is_train: np.cast[\'int32\'](0)}, on_unused_input=\'ignore\')\n\n        predict_parameter = test_out()\n\n        return predict_parameter    \n\n    def parameter_prediction_S2S(self, test_set_x, test_set_d):  \n        """""" This function is to predict the output of NN\n        \n        :param test_set_x: input features for a testing sentence\n        :param test_set_d: phone durations for a testing sentence\n        :type test_set_x: python array variable\n        :type test_set_d: python array variable\n        :returns: predicted features\n        \n        """"""\n\n        n_test_set_x = test_set_x.shape[0]\n\n        test_out = theano.function([], self.final_layer.output,\n                givens={self.x: test_set_x[0:n_test_set_x], self.d: test_set_d[0:n_test_set_x], self.is_train: np.cast[\'int32\'](0)}, on_unused_input=\'ignore\')\n\n        predict_parameter = test_out()\n\n        return predict_parameter\n\n    def parameter_prediction_S2SPF(self, test_set_x, test_set_d, test_set_f):  \n        """""" This function is to predict the output of NN\n        \n        :param test_set_x: input features for a testing sentence\n        :param test_set_d: phone durations for a testing sentence\n        :type test_set_x: python array variable\n        :type test_set_d: python array variable\n        :returns: predicted features\n        \n        """"""\n\n        n_test_set_x  = test_set_x.shape[0]\n        num_of_frames = sum(test_set_d)\n\n        test_out = theano.function([], self.final_layer.output,\n                givens={self.x: test_set_x[0:n_test_set_x], self.d: test_set_d, self.f: test_set_f, self.is_train: np.cast[\'int32\'](0)}, on_unused_input=\'ignore\')\n\n        predict_parameter = test_out()\n\n        return predict_parameter\n\n    def parameter_prediction_CTC(self, test_set_x):  #, batch_size\n\n        n_test_set_x = test_set_x.shape[0]\n\n        test_out = theano.function([], self.rnn_layers[-1].output,\n              givens={self.x: test_set_x[0:n_test_set_x]})\n\n        predict_parameter = test_out()\n\n        return predict_parameter\n\n    def parameter_prediction_MDN(self, test_set_x):  #, batch_size\n\n        n_test_set_x = test_set_x.get_value(borrow=True).shape[0]\n\n        test_out = theano.function([], self.final_layer.mu,\n              givens={self.x: test_set_x[0:n_test_set_x]})\n\n        predict_parameter = test_out()\n\n        return predict_parameter\n\n    def parameter_prediction_mix(self, test_set_x):  #, batch_size\n\n        n_test_set_x = test_set_x.get_value(borrow=True).shape[0]\n\n        test_out = theano.function([], self.final_layer.mix,\n              givens={self.x: test_set_x[0:n_test_set_x]})\n\n        predict_parameter = test_out()\n\n        return predict_parameter\n\n    def parameter_prediction_sigma(self, test_set_x):  #, batch_size\n\n        n_test_set_x = test_set_x.get_value(borrow=True).shape[0]\n\n        test_out = theano.function([], self.final_layer.sigma,\n              givens={self.x: test_set_x[0:n_test_set_x]})\n\n        predict_parameter = test_out()\n\n        return predict_parameter\n    \n    ## the function to output activations at a hidden layer\n    def generate_hidden_layer(self, test_set_x, bn_layer_index):\n        """""" This function is to predict the bottleneck features of NN\n\n        :param test_set_x: input features for a testing sentence\n        :type test_set_x: python array variable\n        :returns: predicted bottleneck features\n\n        """"""\n\n        n_test_set_x = test_set_x.shape[0]\n\n        test_out = theano.function([], self.rnn_layers[bn_layer_index].output,\n                givens={self.x: test_set_x, self.is_train: np.cast[\'int32\'](0)}, on_unused_input=\'ignore\')\n\n        predict_parameter = test_out()\n\n        return predict_parameter\n\n'"
src/models/mdn.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n###THEANO_FLAGS=\'cuda.root=/opt/cuda-5.0.35,mode=FAST_RUN,device=gpu0,floatX=float32,exception_verbosity=high\' python dnn.py\n""""""\n""""""\nimport pickle\nimport os\nimport sys\nimport time\nimport math\n\nimport numpy\nfrom collections import OrderedDict\n\nimport theano\nimport theano.tensor as T\nfrom theano.tensor.shared_randomstreams import RandomStreams\n\nfrom layers.layers import LinearLayer, SigmoidLayer, HiddenLayer, GeneralLayer, MixtureDensityOutputLayer\nfrom utils.providers import ListDataProvider\n\nfrom training_schemes.rprop import compile_RPROP_train_function\n\nimport logging\n\nclass MixtureDensityNetwork(object):\n    def __init__(self, numpy_rng, n_ins=784, n_outs=24, l1_reg = None, l2_reg = None,\n                 hidden_layers_sizes=[500, 500],\n                 hidden_activation=\'tanh\', output_activation=\'linear\', var_floor=0.01,\n                 n_component=1, beta_opt=False, use_rprop=0, rprop_init_update=0.001,\n                 eff_sample_size=0.8, mean_log_det=-100.0):\n\n        logger = logging.getLogger(""Multi-stream DNN initialization"")\n\n        self.sigmoid_layers = []\n        self.params = []\n        self.delta_params   = []\n\n        self.final_layers = []\n\n        self.n_outs = n_outs\n\n        self.n_layers = len(hidden_layers_sizes)\n\n        self.output_activation = output_activation\n        self.var_floor = var_floor\n\n        self.use_rprop = use_rprop\n        self.rprop_init_update = rprop_init_update\n\n        self.l1_reg = l1_reg\n        self.l2_reg = l2_reg\n\n        self.beta_opt = beta_opt\n        self.eff_sample_size = eff_sample_size\n        self.mean_log_det = mean_log_det\n\n        assert self.n_layers > 0\n\n        # allocate symbolic variables for the data\n        self.x = T.matrix(\'x\')\n        self.y = T.matrix(\'y\')\n\n        for i in range(self.n_layers):\n            if i == 0:\n                input_size = n_ins\n            else:\n                input_size = hidden_layers_sizes[i - 1]\n\n            if i == 0:\n                layer_input = self.x\n            else:\n                layer_input = self.sigmoid_layers[-1].output\n\n            sigmoid_layer = HiddenLayer(rng=numpy_rng,\n                                        input=layer_input,\n                                        n_in=input_size,\n                                        n_out=hidden_layers_sizes[i],\n                                        activation=T.tanh)  ##T.nnet.sigmoid)  #\n            self.sigmoid_layers.append(sigmoid_layer)\n            self.params.extend(sigmoid_layer.params)\n            self.delta_params.extend(sigmoid_layer.delta_params)\n\n        hidden_output_size = hidden_layers_sizes[-1]\n\n        self.final_layer = MixtureDensityOutputLayer(rng = numpy_rng,\n                                                input = sigmoid_layer.output,\n                                                n_in = hidden_output_size,\n                                                n_out = self.n_outs,\n                                                n_component = n_component,\n                                                var_floor = self.var_floor)\n        self.params.extend(self.final_layer.params)\n        self.delta_params.extend(self.final_layer.delta_params)\n\n        ### Maximum likelihood\n        self.finetune_cost = 0.0\n\n        self.errors = 0.0\n\n        epsd = self.eff_sample_size**(-2.0/(n_outs + 2.0))\n        beta = (epsd - 1.0) + math.sqrt(epsd*(epsd - 1.0))\n\n        if self.beta_opt:\n            assert n_component == 1, ""beta optimisation only implemented for single-component MDNs""\n            for i in range(n_component):  #n_component\n                sigma = self.final_layer.sigma[:, i*n_outs:(i+1)*n_outs]\n                mu    = self.final_layer.mu[:, i*n_outs:(i+1)*n_outs]\n                mix_weight = self.final_layer.mix[:, i]\n\n                xEx = -0.5 * beta * T.sum(((self.y - mu)**2) * T.inv(sigma), axis=1)\n                exponent = (0.5 * (n_outs + 2.0) * T.log(1 + beta)) + xEx\n                point_fit = T.exp(exponent) - beta\n\n                log_det_mult = -0.5 * beta * T.sum(T.log(sigma), axis=1)\n\n                log_det_mult += (0.5 * beta * self.mean_log_det) # normalise by mean_log_det\n\n                beta_obj = (mix_weight**2) * point_fit * T.exp(log_det_mult)\n\n                self.finetune_cost += -T.mean(beta_obj)\n\n            # lines to compute debugging information for later printing\n            #self.errors = T.min(T.min(T.log(sigma), axis=1))\n            #self.errors = T.mean(T.sum(T.log(sigma), axis=1)) # computes mean_log_det\n            #self.errors = -xEx # (vector quantity) should be about 0.5 * beta * n_outs\n            #self.errors = point_fit  # (vector quantity) should be about one\n            #self.errors = T.mean(T.exp(exponent)) / T.exp(T.max(exponent)) # fraction of the data used, should be about efficiency\n            #self.errors = T.mean(point_fit) # should be about one\n            #self.errors = log_det_mult # (vector quantity) about zero, or always less if using Rprop\n            #self.errors = beta_obj # (vector quantity) objective function terms\n            #self.errors = self.finetune_cost # disable this line below when debugging\n        else:\n\n            all_mix_prob = []\n\n            print(n_component)\n            for i in range(n_component):  #n_component\n                sigma = self.final_layer.sigma[:, i*n_outs:(i+1)*n_outs]\n                mu    = self.final_layer.mu[:, i*n_outs:(i+1)*n_outs]\n                mix_weight = self.final_layer.mix[:, i]\n\n                xEx = -0.5 * T.sum(((self.y - mu)**2) * T.inv(sigma), axis=1)\n                normaliser = 0.5 * ( n_outs * T.log(2 * numpy.pi) + T.sum(T.log(sigma), axis=1))\n                exponent = xEx + T.log(mix_weight) - normaliser\n                all_mix_prob.append(exponent)\n\n            max_exponent = T.max(all_mix_prob, axis=0, keepdims=True)\n            mod_exponent = T.as_tensor_variable(all_mix_prob) - max_exponent\n\n            self.finetune_cost = - T.mean(max_exponent + T.log(T.sum(T.exp(mod_exponent), axis=0)))\n\n            #self.errors = self.finetune_cost\n\n\n        if self.l2_reg is not None:\n            for i in range(self.n_layers-1):\n                W = self.params[i * 2]\n                self.finetune_cost += self.l2_reg * T.sqr(W).sum()\n            self.finetune_cost += self.l2_reg * T.sqr(self.final_layer.W_mu).sum()\n            self.finetune_cost += self.l2_reg * T.sqr(self.final_layer.W_sigma).sum()\n            self.finetune_cost += self.l2_reg * T.sqr(self.final_layer.W_mix).sum()\n\n        self.errors = self.finetune_cost # disable this line if debugging beta_opt\n\n\n    def build_finetune_functions(self, train_shared_xy, valid_shared_xy, batch_size):\n\n        (train_set_x, train_set_y) = train_shared_xy\n        (valid_set_x, valid_set_y) = valid_shared_xy\n\n        # compute number of minibatches for training, validation and testing\n        n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n        n_valid_batches /= batch_size\n\n        index = T.lscalar(\'index\')  # index to a [mini]batch\n        learning_rate = T.fscalar(\'learning_rate\')\n        momentum = T.fscalar(\'momentum\')\n\n        layer_size = len(self.params)\n        lr_list = []\n        for i in range(layer_size):\n            lr_list.append(learning_rate)\n\n        ##top 2 layers use a smaller learning rate\n        if layer_size > 4:\n            for i in range(layer_size-4, layer_size):\n                lr_list[i] = learning_rate * 0.5\n\n        # compute list of fine-tuning updates\n        # compute the gradients with respect to the model parameters\n        gparams = T.grad(self.finetune_cost, self.params)\n\n        if self.use_rprop == 0:\n\n            updates = OrderedDict()\n            layer_index = 0\n            for dparam, gparam in zip(self.delta_params, gparams):\n                updates[dparam] = momentum * dparam - gparam * lr_list[layer_index]\n                layer_index += 1\n\n            for dparam, param in zip(self.delta_params, self.params):\n                updates[param] = param + updates[dparam]\n\n            train_fn = theano.function(inputs=[index, theano.Param(learning_rate, default = 0.0001),\n                  theano.Param(momentum, default = 0.5)],\n                  outputs=self.errors,\n                  updates=updates,\n                  on_unused_input=\'ignore\',\n                  givens={self.x: train_set_x[index * batch_size:\n                                              (index + 1) * batch_size],\n                          self.y: train_set_y[index * batch_size:\n                                              (index + 1) * batch_size]})\n\n        elif self.use_rprop:\n            updates = compile_RPROP_train_function(self, gparams)\n\n            ## retain learning rate and momentum to make interface backwards compatible,\n            ## but we won\'t use them, means we have to use on_unused_input=\'warn\'.\n            ## Otherwise same function for RPROP or otherwise -- can move this block outside if clause.\n            train_fn = theano.function(inputs=[index, theano.Param(learning_rate, default = 0.0001),\n                  theano.Param(momentum, default = 0.5)],\n                  outputs=self.errors,\n                  updates=updates,\n                  on_unused_input=\'warn\',\n                  givens={self.x: train_set_x[index * batch_size:\n                                              (index + 1) * batch_size],\n                          self.y: train_set_y[index * batch_size:\n                                              (index + 1) * batch_size]})\n\n        valid_fn = theano.function([],\n              outputs=self.errors,\n              on_unused_input=\'ignore\',\n              givens={self.x: valid_set_x,\n                      self.y: valid_set_y})\n\n        valid_score_i = theano.function([index],\n              outputs=self.errors,\n              on_unused_input=\'ignore\',\n              givens={self.x: valid_set_x[index * batch_size:\n                                          (index + 1) * batch_size],\n                      self.y: valid_set_y[index * batch_size:\n                                          (index + 1) * batch_size]})\n        # Create a function that scans the entire validation set\n        def valid_score():\n            return [valid_score_i(i) for i in range(n_valid_batches)]\n\n        return train_fn, valid_fn\n\n\n    def parameter_prediction(self, test_set_x):  #, batch_size\n\n        n_test_set_x = test_set_x.get_value(borrow=True).shape[0]\n\n        test_out = theano.function([], self.final_layer.mu,\n              givens={self.x: test_set_x[0:n_test_set_x]})\n\n        predict_parameter = test_out()\n\n        return predict_parameter\n\n    def parameter_prediction_mix(self, test_set_x):  #, batch_size\n\n        n_test_set_x = test_set_x.get_value(borrow=True).shape[0]\n\n        test_out = theano.function([], self.final_layer.mix,\n              givens={self.x: test_set_x[0:n_test_set_x]})\n\n        predict_parameter = test_out()\n\n        return predict_parameter\n\n    def parameter_prediction_sigma(self, test_set_x):  #, batch_size\n\n        n_test_set_x = test_set_x.get_value(borrow=True).shape[0]\n\n        test_out = theano.function([], self.final_layer.sigma,\n              givens={self.x: test_set_x[0:n_test_set_x]})\n\n        predict_parameter = test_out()\n\n        return predict_parameter\n\nif __name__ == \'__main__\':\n\n    train_scp = \'/afs/inf.ed.ac.uk/group/project/dnn_tts/data/nick/nn_scp/train.scp\'\n    valid_scp = \'/afs/inf.ed.ac.uk/group/project/dnn_tts/data/nick/nn_scp/gen.scp\'\n\n    model_dir = \'/afs/inf.ed.ac.uk/group/project/dnn_tts/practice/nnets_model\'\n\n    log_dir =  \'/afs/inf.ed.ac.uk/group/project/dnn_tts/practice/log\'\n\n    finetune_lr=0.01\n    pretraining_epochs=100\n    pretrain_lr=0.01\n    training_epochs=100\n    batch_size=32\n\n    n_ins = 898\n    n_outs = 229\n\n    hidden_layers_sizes = [512, 512, 512]\n\n#    test_DBN(train_scp, valid_scp, log_dir, model_dir, n_ins, n_outs, hidden_layers_sizes,\n#             finetune_lr, pretraining_epochs, pretrain_lr, training_epochs, batch_size)\n\n    dnn_generation()\n'"
src/models/seq2seq.py,0,"b'import numpy as np\nimport theano\nimport theano.tensor as T\nfrom theano import function\n\nclass VanillaSequenceEncoder(object):\n\n    def __init__(self, rng, x, d):\n\n        self.input = x\n        self.out_len = d\n        self.encoded_output = self.encode_final_state()\n\n    ### default seq-to-seq model: tile C as input to all frames ###\n    def encode_final_state(self):\n        context_vector       = self.input[-1, ]\n        tiled_context_vector = T.tile(context_vector, (self.out_len, 1))\n\n        return tiled_context_vector\n\nclass VanillaSequenceEncoderWithDur(object):\n\n    def __init__(self, rng, x, d):\n\n        self.input = x\n        self.dur_input = d\n        self.encoded_output = self.encode_final_state()\n\n    ### default seq-to-seq model: tile C as input to all frames ###\n    def encode_final_state(self):\n        context_vector       = self.input[-1, ]\n        tiled_context_vector = T.tile(context_vector, (T.sum(self.dur_input), 1))\n\n        return tiled_context_vector\n\nclass DistributedSequenceEncoder(object):\n\n    def __init__(self, rng, x, d):\n\n        self.input = x\n        self.dur_input = d\n        self.encoded_output = self.encode_all_states()\n\n    ### Distributed seq-to-seq model: tile C_1-C_n as input to corresponding decoder frames ###\n    def encode_all_states(self):\n        reps = T.repeat(T.arange(self.dur_input.size), self.dur_input)\n        dist_context_vector = self.input[reps]\n\n        return dist_context_vector\n'"
src/models/st_dnn_cm.py,0,"b'\nimport pickle\nimport os\nimport sys\nimport time\n\nimport numpy as np\nimport gnumpy as gnp\n\nfrom numpy import float64\n\nimport bandmat as bm\nimport bandmat.linalg as bla\n\nfrom guppy import hpy\n\nimport logging\n\nclass SequentialDNN(object):\n\n    def __init__(self, numpy_rng, n_ins=100,\n                 n_outs=100, l1_reg = None, l2_reg = None,\n                 hidden_layer_sizes=[500, 500],\n                 hidden_activation=\'tanh\', output_activation=\'linear\'):\n\n        logger = logging.getLogger(""DNN initialization"")\n\n        self.n_layers = len(hidden_layer_sizes)\n        self.l1_reg = l1_reg\n        self.l2_reg = l2_reg\n\n        assert self.n_layers > 0\n\n        self.W_params = []\n        self.b_params = []\n        self.mW_params = []\n        self.mb_params = []\n\n        for i in range(self.n_layers):\n            if i == 0:\n                input_size = n_ins\n            else:\n                input_size = hidden_layer_sizes[i-1]\n            W_value = gnp.garray(numpy_rng.normal(0.0, 1.0/np.sqrt(input_size), size=(input_size, hidden_layer_sizes[i])))\n            b_value = gnp.zeros(hidden_layer_sizes[i])\n            mW_value = gnp.zeros((input_size, hidden_layer_sizes[i]))\n            mb_value = gnp.zeros(hidden_layer_sizes[i])\n            self.W_params.append(W_value)\n            self.b_params.append(b_value)\n            self.mW_params.append(mW_value)\n            self.mb_params.append(mb_value)\n\n        #output layer\n        input_size = hidden_layer_sizes[self.n_layers-1]\n        W_value = gnp.garray(numpy_rng.normal(0.0, 1.0/np.sqrt(input_size), size=(input_size, n_outs)))\n        b_value = gnp.zeros(n_outs)\n        mW_value = gnp.zeros((input_size, n_outs))\n        mb_value = gnp.zeros(n_outs)\n        self.W_params.append(W_value)\n        self.b_params.append(b_value)\n        self.mW_params.append(mW_value)\n        self.mb_params.append(mb_value)\n\n\n    def backpropagation(self, train_set_y, mean_matrix, std_matrix):\n\n        final_layer_output = self.final_layer_output\n\n        final_layer_output = final_layer_output * gnp.garray(std_matrix) + gnp.garray(mean_matrix)\n        frame_number = final_layer_output.shape[0]\n\n        final_layer_output = final_layer_output.T\n        obs_mat = gnp.zeros((61, frame_number*3))\n        traj_err_mat = gnp.zeros((61, frame_number))\n        observation_error = gnp.zeros((frame_number, 259))\n\n        var_base = np.zeros((61, 3))\n        static_indice = []\n        delta_indice = []\n        acc_indice = []\n\n        for i in range(60):\n            static_indice.append(i)\n            delta_indice.append(i+60)\n            acc_indice.append(i+120)\n        static_indice.append(181)\n        delta_indice.append(182)\n        acc_indice.append(183)\n#        for i in xrange(25):\n#            static_indice.append(i+184)\n#            delta_indice.append(i+184+25)\n#            acc_indice.append(i+184+50)\n\n        obs_mat[:, 0:frame_number] = final_layer_output[static_indice, :]\n        obs_mat[:, frame_number:frame_number*2] = final_layer_output[delta_indice, :]\n        obs_mat[:, frame_number*2:frame_number*3] = final_layer_output[acc_indice, :]\n\n        var_base[:, 0] = std_matrix[0, static_indice].T\n        var_base[:, 1] = std_matrix[0, delta_indice].T\n        var_base[:, 2] = std_matrix[0, acc_indice].T\n        var_base = np.reshape(var_base, (61*3, 1))\n        var_base = var_base ** 2\n\n        sub_dim_list = []\n        for i in range(61):\n            sub_dim_list.append(1)\n\n        sub_dim_start = 0\n        for sub_dim in sub_dim_list:\n            wuw_mat, wu_mat = self.pre_wuw_wu(frame_number, sub_dim, var_base[sub_dim_start*3:sub_dim_start*3+sub_dim*3])\n\n            obs_mu = obs_mat[sub_dim_start:sub_dim_start+sub_dim, :].reshape((frame_number*3*sub_dim, 1))\n            wuwwu = gnp.dot(wuw_mat, wu_mat)\n\n            mlpg_traj = gnp.dot(wuwwu, obs_mu)\n\n            sub_std_mat = std_matrix[:, static_indice].T\n            sub_mu_mat  = mean_matrix[:, static_indice].T\n            sub_std_mat = sub_std_mat[sub_dim_start:sub_dim_start+sub_dim, :]\n\n#            print   sub_std_mat\n            sub_std_mat = sub_std_mat.reshape((frame_number*sub_dim, 1))\n            sub_mu_mat = sub_mu_mat[sub_dim_start:sub_dim_start+sub_dim, :].reshape((frame_number*sub_dim, 1))\n\n            sub_o_std_vec = var_base[sub_dim_start*3:sub_dim_start*3+sub_dim*3]\n            sub_o_std_mat = np.tile(sub_o_std_vec.T, (frame_number, 1))\n            sub_o_std_mat = (sub_o_std_mat.T) ** 0.5\n            sub_o_std_vec = sub_o_std_mat.reshape((frame_number*sub_dim*3, 1))\n#            print   sub_o_std_vec, var_base[sub_dim_start*3:sub_dim_start*3+sub_dim*3] ** 0.5\n\n            ref_y = train_set_y[:, static_indice].T\n            ref_y = ref_y[sub_dim_start:sub_dim_start+sub_dim, :].reshape((frame_number*sub_dim, 1))\n\n            ref_y = ref_y * sub_std_mat + sub_mu_mat\n            traj_err = (mlpg_traj - ref_y)\n\n            traj_err_mat[sub_dim_start:sub_dim_start+sub_dim] = traj_err.reshape((sub_dim, frame_number))\n\n            traj_err = traj_err / sub_std_mat\n\n            obs_err_vec = gnp.dot(wuwwu.T, traj_err)\n#            temp_obs_err_vec = gnp.dot(traj_err.T, wuwwu)\n#            print   obs_err_vec, temp_obs_err_vec\n#            print   obs_err_vec.shape, temp_obs_err_vec.shape\n            obs_err_vec = obs_err_vec * sub_o_std_vec\n#            print   obs_mu, mlpg_traj, ref_y\n#            print   obs_err_vec.shape, sub_o_std_vec.shape, frame_number, wuwwu.shape, traj_err.shape\n            obs_mat[sub_dim_start:sub_dim_start+sub_dim, :] = obs_err_vec.reshape((sub_dim, frame_number*3))\n\n            sub_dim_start = sub_dim_start + sub_dim\n\n        self.errors  = gnp.sum(traj_err_mat[0:60, :].T ** 2, axis=1)\n\n        observation_error[:, 0:60]    = obs_mat[0:60, 0:frame_number].T\n        observation_error[:, 60:120]  = obs_mat[0:60, frame_number:frame_number*2].T\n        observation_error[:, 120:180] = obs_mat[0:60, frame_number*2:frame_number*3].T\n        observation_error[:, 181]     = obs_mat[60, 0:frame_number].T\n        observation_error[:, 182]     = obs_mat[60, frame_number:frame_number*2].T\n        observation_error[:, 183]     = obs_mat[60, frame_number*2:frame_number*3].T\n\n        self.W_grads = []\n        self.b_grads = []\n        current_error = observation_error\n        current_activation = self.activations[-1]\n        current_W_grad = gnp.dot(current_activation.T, observation_error)\n        current_b_grad = gnp.dot(gnp.ones((1, observation_error.shape[0])), observation_error)\n        propagate_error = gnp.dot(observation_error, self.W_params[self.n_layers].T) # final layer is linear output, gradient is one\n        self.W_grads.append(current_W_grad)\n        self.b_grads.append(current_b_grad)\n        for i in reversed(list(range(self.n_layers))):\n            current_activation = self.activations[i]\n            current_gradient = 1.0 - current_activation ** 2\n            current_W_grad = gnp.dot(current_activation.T, propagate_error)\n            current_b_grad = gnp.dot(gnp.ones((1, propagate_error.shape[0])), propagate_error)\n            propagate_error = gnp.dot(propagate_error, self.W_params[i].T) * current_gradient\n\n            self.W_grads.insert(0, current_W_grad)\n            self.b_grads.insert(0, current_b_grad)\n\n    def feedforward(self, train_set_x):\n        self.activations = []\n\n        self.activations.append(train_set_x)\n\n        for i in range(self.n_layers):\n            input_data = self.activations[i]\n            current_activations = gnp.tanh(gnp.dot(input_data, self.W_params[i]) + self.b_params[i])\n            self.activations.append(current_activations)\n\n        #output layers\n        self.final_layer_output = gnp.dot(self.activations[self.n_layers], self.W_params[self.n_layers]) + self.b_params[self.n_layers]\n\n    def gradient_update(self, batch_size, learning_rate, momentum):\n\n        multiplier = learning_rate / batch_size;\n        for i in range(len(self.W_grads)):\n\n            if i >= len(self.W_grads) - 2:\n                local_multiplier = multiplier * 0.5\n            else:\n                local_multiplier = multiplier\n\n            self.W_grads[i] = (self.W_grads[i] + self.W_params[i] * self.l2_reg) * local_multiplier\n            self.b_grads[i] = self.b_grads[i] * local_multiplier   # + self.b_params[i] * self.l2_reg\n\n            #update weights and record momentum weights\n            self.mW_params[i] = (self.mW_params[i] * momentum) - self.W_grads[i]\n            self.mb_params[i] = (self.mb_params[i] * momentum) - self.b_grads[i]\n            self.W_params[i] += self.mW_params[i]\n            self.b_params[i] += self.mb_params[i]\n\n\n\n    def finetune(self, train_xy, batch_size, learning_rate, momentum, mean_matrix, std_matrix):\n        (train_set_x, train_set_y) = train_xy\n\n        train_set_x = gnp.as_garray(train_set_x)\n        train_set_y = gnp.as_garray(train_set_y)\n\n        self.feedforward(train_set_x)\n        self.backpropagation(train_set_y, mean_matrix, std_matrix)\n        self.gradient_update(batch_size, learning_rate, momentum)\n\n#        self.errors = gnp.sum((self.final_layer_output - train_set_y) ** 2, axis=1)\n\n        return  self.errors.as_numpy_array()\n\n    def parameter_prediction(self, test_set_x):\n        test_set_x = gnp.garray(test_set_x)\n\n        current_activations = test_set_x\n\n        for i in range(self.n_layers):\n            input_data = current_activations\n            current_activations = gnp.tanh(gnp.dot(input_data, self.W_params[i]) + self.b_params[i])\n\n        final_layer_output = gnp.dot(current_activations, self.W_params[self.n_layers]) + self.b_params[self.n_layers]\n\n        return  final_layer_output.as_numpy_array()\n\n    def parameter_prediction_trajectory(self, test_set_x, test_set_y, mean_matrix, std_matrix):\n        test_set_x = gnp.garray(test_set_x)\n\n        current_activations = test_set_x\n\n        for i in range(self.n_layers):\n            input_data = current_activations\n            current_activations = gnp.tanh(gnp.dot(input_data, self.W_params[i]) + self.b_params[i])\n\n        final_layer_output = gnp.dot(current_activations, self.W_params[self.n_layers]) + self.b_params[self.n_layers]\n\n        final_layer_output = final_layer_output * gnp.garray(std_matrix) + gnp.garray(mean_matrix)\n        frame_number = final_layer_output.shape[0]\n\n        final_layer_output = final_layer_output.T\n        obs_mat = gnp.zeros((60, frame_number*3))\n        traj_err_mat = gnp.zeros((60, frame_number))\n\n        var_base = np.zeros((60, 3))\n        static_indice = []\n        delta_indice = []\n        acc_indice = []\n\n        for i in range(60):\n            static_indice.append(i)\n            delta_indice.append(i+60)\n            acc_indice.append(i+120)\n\n        obs_mat[:, 0:frame_number] = final_layer_output[static_indice, :]\n        obs_mat[:, frame_number:frame_number*2] = final_layer_output[delta_indice, :]\n        obs_mat[:, frame_number*2:frame_number*3] = final_layer_output[acc_indice, :]\n\n        var_base[:, 0] = std_matrix[0, static_indice].T\n        var_base[:, 1] = std_matrix[0, delta_indice].T\n        var_base[:, 2] = std_matrix[0, acc_indice].T\n\n        var_base = np.reshape(var_base, (60*3, 1))\n        var_base = var_base ** 2\n\n        sub_dim_list = []\n        for i in range(60):\n            sub_dim_list.append(1)\n\n        sub_dim_start = 0\n        for sub_dim in sub_dim_list:\n            wuw_mat, wu_mat = self.pre_wuw_wu(frame_number, sub_dim, var_base[sub_dim_start*3:sub_dim_start*3+sub_dim*3])\n\n            obs_mu = obs_mat[sub_dim_start:sub_dim_start+sub_dim, :].reshape((frame_number*3*sub_dim, 1))\n            wuwwu = gnp.dot(wuw_mat, wu_mat)\n            mlpg_traj = gnp.dot(wuwwu, obs_mu)\n\n            sub_std_mat = std_matrix[:, static_indice].T\n            sub_mu_mat  = mean_matrix[:, static_indice].T\n            sub_std_mat = sub_std_mat[sub_dim_start:sub_dim_start+sub_dim, :].reshape((frame_number*sub_dim, 1))\n            sub_mu_mat = sub_mu_mat[sub_dim_start:sub_dim_start+sub_dim, :].reshape((frame_number*sub_dim, 1))\n\n            ref_y = test_set_y[:, static_indice].T\n            ref_y = ref_y[sub_dim_start:sub_dim_start+sub_dim, :].reshape((frame_number*sub_dim, 1))\n\n            ref_y = ref_y * sub_std_mat + sub_mu_mat\n            traj_err = (mlpg_traj - ref_y)  #mlpg_traj ref_y\n\n            traj_err_mat[sub_dim_start:sub_dim_start+sub_dim, :] = traj_err.reshape((sub_dim, frame_number))\n\n            sub_dim_start = sub_dim_start + sub_dim\n\n        validation_losses = gnp.sum(traj_err_mat[1:60, :].T ** 2, axis=1)\n        validation_losses = validation_losses ** 0.5\n\n        return  validation_losses.as_numpy_array()\n\n\n    def set_parameters(self, W_params, b_params):\n\n        assert len(self.W_params) == len(W_params)\n\n#        for i in xrange(len(self.W_params)):\n        for i in range(len(self.W_params)):\n            self.W_params[i] = W_params[i]\n            self.b_params[i] = b_params[i]\n\n    def set_delta_params(self, mW_params, mb_params):\n        assert len(self.mW_params) == len(mW_params)\n\n        for i in range(len(self.mW_params)):\n            self.mW_params[i] = mW_params[i]\n            self.mb_params[i] = mb_params[i]\n\n    \'\'\'\n    #############following function for MLPG##################\n    \'\'\'\n    def pre_wuw_wu(self, frame_number, static_dimension, var_base):\n\n        wuw_mat = gnp.zeros((frame_number*static_dimension, frame_number*static_dimension))\n        wu_mat  = gnp.zeros((frame_number*static_dimension, 3*frame_number*static_dimension))\n\n        for i in range(static_dimension):\n            temp_var_base = [var_base[i*3], var_base[i*3+1], var_base[i*3+2]]\n            temp_wuw, temp_wu = self.pre_compute_wuw(frame_number, temp_var_base)\n            wuw_mat[frame_number*i:frame_number*(i+1), frame_number*i:frame_number*(i+1)] = gnp.garray(temp_wuw[:])\n            wu_mat[frame_number*i:frame_number*(i+1), frame_number*i:frame_number*(i+3)] = gnp.garray(temp_wu[:])\n\n        return  wuw_mat, wu_mat\n\n    def pre_compute_wuw(self, frame_number, var_base):\n        windows = [\n            (0, 0, np.array([1.0])),\n            (1, 1, np.array([-0.5, 0.0, 0.5])),\n            (1, 1, np.array([1.0, -2.0, 1.0])),\n        ]\n        num_windows = len(windows)\n\n        win_mats = self.build_win_mats(windows, frame_number)\n\n        var_base = np.array(var_base)\n        var_base = np.reshape(var_base, (1, 3))\n\n        var_frames = np.tile(var_base, (frame_number, 1))\n        var_frames[0, 1] = 100000000000;\n        var_frames[0, 2] = 100000000000;\n        var_frames[frame_number-1, 1] = 100000000000;\n        var_frames[frame_number-1, 2] = 100000000000;\n\n\n        tau_frames = 1.0 / var_frames\n\n        prec = self.build_wuw(frame_number, tau_frames, win_mats)\n        inv_prec_full = bla.solveh(prec, np.eye(frame_number))\n\n        wu_list = self.build_wu(frame_number, tau_frames, win_mats)\n\n        wu_mat = np.zeros((frame_number, frame_number * 3))\n        wu_mat[:, 0:frame_number] = wu_list[0]\n        wu_mat[:, frame_number:frame_number*2] = wu_list[1]\n        wu_mat[:, frame_number*2:frame_number*3] = wu_list[2]\n\n\n        return  inv_prec_full, wu_mat\n\n\n    def build_wuw(self, frame_number, tau_frames, win_mats, sdw=None):\n        if sdw is None:\n            sdw = max([ win_mat.l + win_mat.u for win_mat in win_mats ])\n\n        prec = bm.zeros(sdw, sdw, frame_number)\n\n        for win_index, win_mat in enumerate(win_mats):\n            bm.dot_mm_plus_equals(win_mat.T, win_mat, target_bm=prec,\n                                  diag=float64(tau_frames[:, win_index]))\n\n        return prec\n\n    def build_wu(self, frame_number, tau_frames, win_mats, sdw=None):\n        if sdw is None:\n            sdw = max([ win_mat.l + win_mat.u for win_mat in win_mats ])\n\n        wu_list = []\n\n        for win_index, win_mat in enumerate(win_mats):\n            temp_wu =  bm.zeros(sdw, sdw, frame_number)\n            bm.dot_mm_plus_equals(win_mat.T, win_mats[0], target_bm=temp_wu,\n                                  diag=float64(tau_frames[:, win_index]))\n            wu_list.append(temp_wu.full())\n\n        return  wu_list\n\n    def build_win_mats(self, windows, frames):\n        win_mats = []\n        for l, u, win_coeff in windows:\n            assert l >= 0 and u >= 0\n            assert len(win_coeff) == l + u + 1\n            win_coeffs = np.tile(np.reshape(win_coeff, (l + u + 1, 1)), frames)\n            win_mat = bm.band_c_bm(u, l, win_coeffs).T\n            win_mats.append(win_mat)\n\n        return win_mats\n'"
src/tensorflow_lib/__init__.py,0,b''
src/tensorflow_lib/configuration.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://github.com/CSTR-Edinburgh/merlin\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\nimport ConfigParser\nimport logging\nimport os\nimport sys\n\nclass configuration(object):\n\n    def __init__(self):\n        pass;\n\n    def configure(self, configFile=None):\n\n        # get a logger\n        logger = logging.getLogger(""configuration"")\n        # this (and only this) logger needs to be configured immediately, otherwise it won\'t work\n        # we can\'t use the full user-supplied configuration mechanism in this particular case,\n        # because we haven\'t loaded it yet!\n        #\n        # so, just use simple console-only logging\n        logger.setLevel(logging.DEBUG) # this level is hardwired here - should change it to INFO\n        # add a handler & its formatter - will write only to console\n        ch = logging.StreamHandler()\n        logger.addHandler(ch)\n        formatter = logging.Formatter(\'%(asctime)s %(levelname)8s%(name)15s: %(message)s\')\n        ch.setFormatter(formatter)\n\n        # first, set up some default configuration values\n        self.initial_configuration()\n\n        # next, load in any user-supplied configuration values\n        # that might over-ride the default values\n        self.user_configuration(configFile)\n\n        # finally, set up all remaining configuration values\n        # that depend upon either default or user-supplied values\n        self.complete_configuration()\n        logger.debug(\'configuration completed\')\n\n    def initial_configuration(self):\n\n        # to be called before loading any user specific values\n\n        # things to put here are\n        # 1. variables that the user cannot change\n        # 2. variables that need to be set before loading the user\'s config file\n\n        UTTID_REGEX = \'(.*)\\..*\'\n\n    def user_configuration(self,configFile=None):\n\n        # get a logger\n        logger = logging.getLogger(""configuration"")\n\n        # load and parse the provided configFile, if provided\n        if not configFile:\n            logger.warn(\'no user configuration file provided; using only built-in default settings\')\n            return\n\n        # load the config file\n        try:\n            configparser = ConfigParser.ConfigParser()\n            configparser.readfp(open(configFile))\n            logger.debug(\'successfully read and parsed user configuration file %s\' % configFile)\n        except:\n            logger.fatal(\'error reading user configuration file %s\' % configFile)\n            raise\n\n        #work_dir must be provided before initialising other directories\n        self.work_dir = None\n\n        if self.work_dir == None:\n            try:\n                self.work_dir = configparser.get(\'Paths\', \'work\')\n\n            except (ConfigParser.NoSectionError, ConfigParser.NoOptionError):\n                if self.work_dir == None:\n                    logger.critical(\'Paths:work has no value!\')\n                    raise Exception\n\n        # default place for some data\n        self.data_dir    = os.path.join(self.work_dir, \'data\')\n        self.tensorflow_dir   = os.path.join(self.work_dir, \'tensorflow\')\n\n        self.gen_dir     = os.path.join(self.tensorflow_dir, \'gen\')\n        self.model_dir   = os.path.join(self.tensorflow_dir, \'models\')\n        self.stats_dir   = os.path.join(self.tensorflow_dir, \'stats\')\n\n        self.inter_data_dir = os.path.join(self.work_dir, \'inter_module\')\n        self.def_inp_dir    = os.path.join(self.inter_data_dir, \'nn_no_silence_lab_norm_425\')\n        self.def_out_dir    = os.path.join(self.inter_data_dir, \'nn_norm_mgc_lf0_vuv_bap_187\')\n\n        impossible_int=int(-99999)\n        impossible_float=float(-99999.0)\n\n        user_options = [\n\n            # Paths\n            (\'work_dir\', self.work_dir, \'Paths\',\'work\'),\n            (\'data_dir\', self.data_dir, \'Paths\',\'data\'),\n\n            (\'inp_feat_dir\', self.def_inp_dir, \'Paths\', \'inp_feat\'),\n            (\'out_feat_dir\', self.def_out_dir, \'Paths\', \'out_feat\'),\n\n            (\'model_dir\', self.model_dir, \'Paths\', \'models\'),\n            (\'stats_dir\', self.stats_dir, \'Paths\', \'stats\'),\n            (\'gen_dir\'  ,   self.gen_dir, \'Paths\', \'gen\'),\n\n            (\'file_id_scp\', os.path.join(self.data_dir, \'file_id_list.scp\'), \'Paths\', \'file_id_list\'),\n            (\'test_id_scp\', os.path.join(self.data_dir, \'test_id_list.scp\'), \'Paths\', \'test_id_list\'),\n\n            # Input-Output\n            (\'inp_dim\', 425, \'Input-Output\', \'inp_dim\'),\n            (\'out_dim\', 187, \'Input-Output\', \'out_dim\'),\n\n            (\'inp_file_ext\', \'.lab\', \'Input-Output\', \'inp_file_ext\'),\n            (\'out_file_ext\', \'.cmp\', \'Input-Output\', \'out_file_ext\'),\n\n            (\'inp_norm\', \'MINMAX\', \'Input-Output\', \'inp_norm\'),\n            (\'out_norm\', \'MINMAX\', \'Input-Output\', \'out_norm\'),\n\n            # Architecture\n            (\'hidden_layer_type\', [\'TANH\', \'TANH\', \'TANH\', \'TANH\', \'TANH\', \'TANH\'], \'Architecture\', \'hidden_layer_type\'),\n            (\'hidden_layer_size\', [ 1024 ,  1024 ,  1024 ,  1024 ,  1024 ,   1024], \'Architecture\', \'hidden_layer_size\'),\n\n            (\'batch_size\'   , 256, \'Architecture\', \'batch_size\'),\n            (\'num_of_epochs\',   1, \'Architecture\', \'training_epochs\'),\n            (\'dropout_rate\' , 0.0, \'Architecture\', \'dropout_rate\'),\n\n            (\'output_layer_type\', \'linear\', \'Architecture\', \'output_layer_type\'),\n            (\'optimizer\'        ,   \'adam\', \'Architecture\', \'optimizer\'),\n            (\'loss_function\'    ,    \'mse\', \'Architecture\', \'loss_function\'),\n\n            # RNN\n            (\'sequential_training\', False, \'Architecture\', \'sequential_training\'),\n            (\'stateful\'           , False, \'Architecture\', \'stateful\'),\n            (\'use_high_batch_size\', False, \'Architecture\', \'use_high_batch_size\'),\n\n            (\'training_algo\',   1, \'Architecture\', \'training_algo\'),\n            (\'merge_size\'   ,   1, \'Architecture\', \'merge_size\'),\n            (\'seq_length\'   , 200, \'Architecture\', \'seq_length\'),\n            (\'bucket_range\' , 100, \'Architecture\', \'bucket_range\'),\n            #encoder_decoder\n            (\'encoder_decoder\'      , False                                           ,  \'Architecture\',\'encoder_decoder\'),\n            (\'attention\'            , False                                           ,  \'Architecture\', \'attention\'),\n            (""cbhg""                 , False                                           ,   ""Architecture"", ""cbhg""),\n            # Data\n            (\'shuffle_data\', False, \'Data\', \'shuffle_data\'),\n\n            (\'train_file_number\', impossible_int, \'Data\',\'train_file_number\'),\n            (\'valid_file_number\', impossible_int, \'Data\',\'valid_file_number\'),\n            (\'test_file_number\' , impossible_int, \'Data\',\'test_file_number\'),\n\n            # Processes\n            (\'NORMDATA\'  , False, \'Processes\', \'NORMDATA\'),\n            (\'TRAINMODEL\', False, \'Processes\', \'TRAINMODEL\'),\n            (\'TESTMODEL\' , False, \'Processes\', \'TESTMODEL\')\n\n        ]\n\n        # this uses exec(...) which is potentially dangerous since arbitrary code could be executed\n        for (variable,default,section,option) in user_options:\n            # default value\n            value=None\n\n            try:\n                # first, look for a user-set value for this variable in the config file\n                value = configparser.get(section,option)\n                user_or_default=\'user\'\n\n            except (ConfigParser.NoSectionError, ConfigParser.NoOptionError):\n                # use default value, if there is one\n                if (default == None) or \\\n                   (default == \'\')   or \\\n                   ((type(default) == int) and (default == impossible_int)) or \\\n                   ((type(default) == float) and (default == impossible_float))  :\n                    logger.critical(\'%20s has no value!\' % (section+"":""+option) )\n                    raise Exception\n                else:\n                    value = default\n                    user_or_default=\'default\'\n\n\n            if type(default) == str:\n                exec(\'self.%s = ""%s""\'      % (variable,value))\n            elif type(default) == int:\n                exec(\'self.%s = int(%s)\'   % (variable,value))\n            elif type(default) == float:\n                exec(\'self.%s = float(%s)\' % (variable,value))\n            elif type(default) == bool:\n                exec(\'self.%s = bool(%s)\'  % (variable,value))\n            elif type(default) == list:\n                exec(\'self.%s = list(%s)\'  % (variable,value))\n            elif type(default) == dict:\n                exec(\'self.%s = dict(%s)\'  % (variable,value))\n            else:\n                logger.critical(\'Variable %s has default value of unsupported type %s\',variable,type(default))\n                raise Exception(\'Internal error in configuration settings: unsupported default type\')\n\n            logger.info(\'%20s has %7s value %s\' % (section+"":""+option,user_or_default,value) )\n\n\n    def complete_configuration(self):\n        # to be called after reading any user-specific settings\n        # because the values set here depend on those user-specific settings\n\n        # get a logger\n        logger = logging.getLogger(""configuration"")\n\n        ## create directories if not exists\n        if not os.path.exists(self.model_dir):\n            os.makedirs(self.model_dir)\n\n        if not os.path.exists(self.stats_dir):\n            os.makedirs(self.stats_dir)\n\n        if not os.path.exists(self.gen_dir):\n            os.makedirs(self.gen_dir)\n\n        # input-output normalization stat files\n        self.inp_stats_file = os.path.join(self.stats_dir, ""input_%d_%s_%d.norm"" %(int(self.train_file_number), self.inp_norm, self.inp_dim))\n        self.out_stats_file = os.path.join(self.stats_dir, ""output_%d_%s_%d.norm"" %(int(self.train_file_number), self.out_norm, self.out_dim))\n\n        # define model file name\n        if self.sequential_training:\n            self.combined_model_arch = \'RNN\'+str(self.training_algo)\n        else:\n            self.combined_model_arch = \'DNN\'\n\n        self.combined_model_arch += \'_\'+str(len(self.hidden_layer_size))\n        self.combined_model_arch += \'_\'+\'_\'.join(map(str, self.hidden_layer_size))\n        self.combined_model_arch += \'_\'+\'_\'.join(map(str, self.hidden_layer_type))\n\n        self.nnets_file_name = \'%s_%d_train_%d_%d_%d_%d_%d_model\' \\\n                          %(self.combined_model_arch, int(self.shuffle_data),\n                             self.inp_dim, self.out_dim, self.train_file_number, self.batch_size, self.num_of_epochs)\n\n        logger.info(\'model file: %s\' % (self.nnets_file_name))\n\n        # model files\n        self.json_model_file = os.path.join(self.model_dir, self.nnets_file_name+\'.json\')\n        self.h5_model_file   = os.path.join(self.model_dir, self.nnets_file_name+\'.h5\')\n\n        # predicted features directory\n        self.pred_feat_dir = os.path.join(self.gen_dir, self.nnets_file_name)\n        if not os.path.exists(self.pred_feat_dir):\n            os.makedirs(self.pred_feat_dir)\n\n        # string.lower for some architecture values\n        self.output_layer_type = self.output_layer_type.lower()\n        self.optimizer         = self.optimizer.lower()\n        self.loss_function     = self.loss_function.lower()\n        for i in range(len(self.hidden_layer_type)):\n            self.hidden_layer_type[i] = self.hidden_layer_type[i].lower()\n\n        # set sequential training True if using LSTMs\n        if \'lstm\' in self.hidden_layer_type:\n            self.sequential_training = True\n\n        # set/limit batch size to 25\n        if self.sequential_training and self.batch_size>50:\n            if not self.use_high_batch_size:\n                logger.info(\'reducing the batch size from %s to 25\' % (self.batch_size))\n                self.batch_size = 25 ## num. of sentences in this case\n\n        # rnn params\n        self.rnn_params = {}\n        self.rnn_params[\'merge_size\']   = self.merge_size\n        self.rnn_params[\'seq_length\']   = self.seq_length\n        self.rnn_params[\'bucket_range\'] = self.bucket_range\n        self.rnn_params[\'stateful\']     = self.stateful\n'"
src/tensorflow_lib/data_utils.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://github.com/CSTR-Edinburgh/merlin\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\nimport os, sys \nimport time\nimport random\nimport numpy as np\n\nfrom sklearn import preprocessing\n\nfrom io_funcs.binary_io import BinaryIOCollection\n\n############################\n##### Memory variables #####\n############################\n\nUTT_BUFFER_SIZE   =   10000\nFRAME_BUFFER_SIZE = 3000000\n\n\ndef read_data_from_file_list(inp_file_list, out_file_list, inp_dim, out_dim, sequential_training=True): \n    io_funcs = BinaryIOCollection()\n\n    num_of_utt = len(inp_file_list)\n\n    file_length_dict = {\'framenum2utt\':{}, \'utt2framenum\':{}}\n\n    if sequential_training:\n        temp_set_x = {}\n        temp_set_y = {}\n    else:\n        temp_set_x = np.empty((FRAME_BUFFER_SIZE, inp_dim))\n        temp_set_y = np.empty((FRAME_BUFFER_SIZE, out_dim))\n     \n    ### read file by file ###\n    current_index = 0\n    for i in xrange(num_of_utt):    \n        inp_file_name = inp_file_list[i]\n        out_file_name = out_file_list[i]\n        inp_features, inp_frame_number = io_funcs.load_binary_file_frame(inp_file_name, inp_dim)\n        out_features, out_frame_number = io_funcs.load_binary_file_frame(out_file_name, out_dim)\n\n        base_file_name = os.path.basename(inp_file_name).split(""."")[0]\n\n        if abs(inp_frame_number-out_frame_number)>5:\n            print \'the number of frames in input and output features are different: %d vs %d (%s)\' %(inp_frame_number, out_frame_number, base_file_name)\n            sys.exit(0)\n        else:\n            frame_number = min(inp_frame_number, out_frame_number)\n  \n        if sequential_training:\n            temp_set_x[base_file_name] = inp_features[0:frame_number] \n            temp_set_y[base_file_name] = out_features[0:frame_number] \n        else:\n            temp_set_x[current_index:current_index+frame_number, ] = inp_features[0:frame_number]\n            temp_set_y[current_index:current_index+frame_number, ] = out_features[0:frame_number]\n            current_index += frame_number\n        \n        if frame_number not in file_length_dict[\'framenum2utt\']:\n            file_length_dict[\'framenum2utt\'][frame_number] = [base_file_name]\n        else:\n            file_length_dict[\'framenum2utt\'][frame_number].append(base_file_name)\n\n        file_length_dict[\'utt2framenum\'][base_file_name] = frame_number\n    \n        drawProgressBar(i+1, num_of_utt)\n        \n    sys.stdout.write(""\\n"")\n        \n    if not sequential_training:\n        temp_set_x = temp_set_x[0:current_index, ]\n        temp_set_y = temp_set_y[0:current_index, ]\n    \n    return temp_set_x, temp_set_y, file_length_dict\n\ndef read_test_data_from_file_list(inp_file_list, inp_dim, sequential_training=True): \n    io_funcs = BinaryIOCollection()\n\n    num_of_utt = len(inp_file_list)\n\n    file_length_dict = {\'framenum2utt\':{}, \'utt2framenum\':{}}\n\n    if sequential_training:\n        temp_set_x = {}\n    else:\n        temp_set_x = np.empty((FRAME_BUFFER_SIZE, inp_dim))\n     \n    ### read file by file ###\n    current_index = 0\n    for i in xrange(num_of_utt):    \n        inp_file_name = inp_file_list[i]\n        inp_features, frame_number = io_funcs.load_binary_file_frame(inp_file_name, inp_dim)\n\n        base_file_name = os.path.basename(inp_file_name).split(""."")[0]\n\n        if sequential_training:\n            temp_set_x[base_file_name] = inp_features\n        else:\n            temp_set_x[current_index:current_index+frame_number, ] = inp_features[0:frame_number]\n            current_index += frame_number\n        \n        if frame_number not in file_length_dict[\'framenum2utt\']:\n            file_length_dict[\'framenum2utt\'][frame_number] = [base_file_name]\n        else:\n            file_length_dict[\'framenum2utt\'][frame_number].append(base_file_name)\n\n        file_length_dict[\'utt2framenum\'][base_file_name] = frame_number\n    \n        drawProgressBar(i+1, num_of_utt)\n        \n    sys.stdout.write(""\\n"")\n        \n    if not sequential_training:\n        temp_set_x = temp_set_x[0:current_index, ]\n    \n    return temp_set_x, file_length_dict\n\ndef transform_data_to_3d_matrix(data, seq_length=200, max_length=0, merge_size=1, shuffle_data = True, shuffle_type = 1, padding=""right""):\n    num_of_utt = len(data)\n    feat_dim   = data[data.keys()[0]].shape[1]\n\n    if max_length > 0:\n        temp_set = np.zeros((num_of_utt, max_length, feat_dim))\n        \n        ### read file by file ###\n        current_index = 0\n        for base_file_name, in_features in data.iteritems():\n            frame_number = min(in_features.shape[0], max_length)\n            if padding==""right"":\n                temp_set[current_index, 0:frame_number, ] = in_features\n            else:\n                temp_set[current_index, -frame_number:, ] = in_features\n            current_index += 1\n\n    else:\n        temp_set = np.zeros((FRAME_BUFFER_SIZE, feat_dim))\n\n        train_idx_list = data.keys()\n        train_idx_list.sort()\n        \n        if shuffle_data:\n            if shuffle_type == 1:\n                train_idx_list = shuffle_file_list(train_idx_list)\n            elif shuffle_type == 2:\n                train_idx_list = shuffle_file_list(train_idx_list, shuffle_type=2, merge_size=merge_size)\n        \n        ### read file by file ###\n        current_index = 0\n        for file_number in xrange(num_of_utt):\n            base_file_name = train_idx_list[file_number]\n            in_features    = data[base_file_name]\n            frame_number   = in_features.shape[0]\n            \n            temp_set[current_index:current_index+frame_number, ] = in_features\n            current_index += frame_number\n    \n            if (file_number+1)%merge_size == 0:\n                current_index = seq_length * (int(np.ceil(float(current_index)/float(seq_length))))\n            \n        \n        num_of_samples = int(np.ceil(float(current_index)/float(seq_length)))\n    \n        temp_set = temp_set[0: num_of_samples*seq_length, ]\n        temp_set = temp_set.reshape(-1, seq_length, feat_dim)\n     \n    return temp_set\n\ndef read_and_transform_data_from_file_list(in_file_list, dim, seq_length=200, merge_size=1):\n    io_funcs = BinaryIOCollection()\n\n    num_of_utt = len(in_file_list)\n\n    temp_set = np.zeros((FRAME_BUFFER_SIZE, dim))\n\n    ### read file by file ###\n    current_index = 0\n    for i in range(num_of_utt):\n        in_file_name = in_file_list[i]\n        in_features, frame_number = io_funcs.load_binary_file_frame(in_file_name, dim)\n        base_file_name            = os.path.basename(in_file_name).split(""."")[0]\n\n        temp_set[current_index:current_index+frame_number, ] = in_features\n        current_index += frame_number\n\n        if (i+1)%merge_size == 0:\n            current_index = seq_length * (int(np.ceil(float(current_index)/float(seq_length))))\n            \n        drawProgressBar(i+1, num_of_utt)\n        \n    sys.stdout.write(""\\n"")\n\n    num_of_samples = int(np.ceil(float(current_index)/float(seq_length)))\n\n    temp_set = temp_set[0: num_of_samples*seq_length, ]\n    temp_set = temp_set.reshape(num_of_samples, seq_length)\n\n    return temp_set\n\ndef merge_data(train_x, train_y, merge_size):\n    temp_train_x = {}\n    temp_train_y = {}\n\n    train_id_list     = train_x.keys()\n    train_file_number = len(train_id_list)\n    train_id_list.sort()\n\n    inp_dim = train_x[train_id_list[0]].shape[1]\n    out_dim = train_y[train_id_list[0]].shape[1]\n    \n    merged_features_x = np.zeros((0, inp_dim))\n    merged_features_y = np.zeros((0, out_dim))\n    new_file_count = 0\n    for file_index in xrange(1, train_file_number+1):\n        inp_features      = train_x[train_id_list[file_index-1]]\n        out_features      = train_y[train_id_list[file_index-1]]\n        merged_features_x = np.vstack((merged_features_x, inp_features))\n        merged_features_y = np.vstack((merged_features_y, out_features))\n        \n        if file_index % merge_size == 0 or file_index==train_file_number:\n            base_file_name = ""new_utterance_%04d"" % (new_file_count)\n            temp_train_x[base_file_name] = merged_features_x\n            temp_train_y[base_file_name] = merged_features_y\n            new_file_count += 1\n            merged_features_x = np.zeros((0, inp_dim))\n            merged_features_y = np.zeros((0, out_dim))\n\n    return temp_train_x, temp_train_y\n\ndef shuffle_file_list(train_idx_list, shuffle_type=1, merge_size=5):\n    ### shuffle train id list ###\n    random.seed(271638)\n    train_file_number = len(train_idx_list)\n    \n    if shuffle_type==1:  ## shuffle by sentence\n        random.shuffle(train_idx_list)\n        return train_idx_list\n     \n    elif shuffle_type==2:  ## shuffle by a group of sentences\n        id_numbers = range(0, train_file_number, merge_size)\n        random.shuffle(id_numbers)\n        new_train_idx_list = []\n        for i in xrange(len(id_numbers)):\n            new_train_idx_list += train_idx_list[id_numbers[i]:id_numbers[i]+merge_size]\n        return new_train_idx_list\n\ndef get_stateful_data(train_x, train_y, batch_size):\n    num_of_batches = int(train_x.shape[0]/batch_size) \n    train_x   = train_x[0: num_of_batches*batch_size, ]\n    train_y   = train_y[0: num_of_batches*batch_size, ]\n\n    stateful_seq = np.zeros(num_of_batches*batch_size, dtype=""int32"")\n    for i in xrange(num_of_batches):\n        stateful_seq[i*batch_size:(i+1)*batch_size] = np.array(range(batch_size))*num_of_batches+i\n\n    temp_train_x   = train_x[stateful_seq]\n    temp_train_y   = train_y[stateful_seq]\n\n    return temp_train_x, temp_train_y\n\ndef get_stateful_input(test_x, seq_length, batch_size=1):\n    [n_frames, n_dim] = test_x.shape\n\n    num_of_samples = batch_size*seq_length\n    num_of_batches = int(n_frames/num_of_samples) + 1\n    new_data_size  = num_of_batches*num_of_samples\n\n    temp_test_x = np.zeros((new_data_size, n_dim))\n    temp_test_x[0: n_frames, ] = test_x\n\n    temp_test_x = temp_test_x.reshape(-1, seq_length, n_dim)\n\n    return temp_test_x\n    \ndef compute_norm_stats(data, stats_file, method=""MVN""):\n    #### normalize training data ####\n    io_funcs = BinaryIOCollection()\n\n    if method==""MVN"":\n        scaler = preprocessing.StandardScaler().fit(data)\n        norm_matrix = np.vstack((scaler.mean_, scaler.scale_))\n    elif method==""MINMAX"":\n        scaler = preprocessing.MinMaxScaler(feature_range=(0.01, 0.99)).fit(data)\n        norm_matrix = np.vstack((scaler.min_, scaler.scale_))\n    \n    print norm_matrix.shape\n    io_funcs.array_to_binary_file(norm_matrix, stats_file)\n\n    return scaler\n\ndef load_norm_stats(stats_file, dim, method=""MVN""):\n    #### load norm stats ####\n    io_funcs = BinaryIOCollection()\n\n    norm_matrix, frame_number = io_funcs.load_binary_file_frame(stats_file, dim)\n    assert frame_number==2\n\n    if method==""MVN"":\n        scaler = preprocessing.StandardScaler()\n        scaler.mean_  = norm_matrix[0, :]\n        scaler.scale_ = norm_matrix[1, :]\n    elif method==""MINMAX"":\n        scaler = preprocessing.MinMaxScaler(feature_range=(0.01, 0.99))\n        scaler.min_   = norm_matrix[0, :]\n        scaler.scale_ = norm_matrix[1, :]\n\n    return scaler\n\ndef norm_data(data, scaler, sequential_training=True):\n    if scaler is None:\n        return;\n    \n    #### normalize data ####\n    if not sequential_training:\n        data = scaler.transform(data) \n    else:\n        for filename, features in data.iteritems():\n            data[filename] = scaler.transform(features)\n\ndef denorm_data(data, scaler):\n    if scaler is None:\n        return;\n    \n    #### de-normalize data ####\n    data = scaler.inverse_transform(data) \n    \ndef prepare_file_path_list(file_id_list, file_dir, file_extension, new_dir_switch=True):\n    if not os.path.exists(file_dir) and new_dir_switch:\n        os.makedirs(file_dir)\n    file_name_list = []\n    for file_id in file_id_list:\n        file_name = file_dir + \'/\' + file_id + file_extension\n        file_name_list.append(file_name)\n\n    return  file_name_list\n\ndef read_file_list(file_name):\n    file_lists = []\n    fid = open(file_name)\n    for line in fid.readlines():\n        line = line.strip()\n        if len(line) < 1:\n            continue\n        file_lists.append(line)\n    fid.close()\n\n    return  file_lists\n\ndef print_status(i, length): \n    pr = int(float(i)/float(length)*100)\n    st = int(float(pr)/7)\n    sys.stdout.write((""\\r%d/%d "")%(i,length)+(""[ %d""%pr+""% ] <<< "")+(\'=\'*st)+(\'\'*(100-st)))\n    sys.stdout.flush()\n    \ndef drawProgressBar(indx, length, barLen = 20):\n    percent = float(indx)/length\n    sys.stdout.write(""\\r"")\n    progress = """"\n    for i in range(barLen):\n        if i < int(barLen * percent):\n            progress += ""=""\n        else:\n            progress += "" ""\n    sys.stdout.write(""[%s] <<< %d/%d (%d%%)"" % (progress, indx, length, percent * 100))\n    sys.stdout.flush()\n'"
src/tensorflow_lib/model.py,79,"b'#!/usr/bin/env python\n################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://github.com/CSTR-Edinburgh/merlin\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.layers import fully_connected, batch_norm\nfrom tensorflow.contrib.layers import dropout\nfrom tensorflow.contrib.rnn import MultiRNNCell,RNNCell, BasicRNNCell, BasicLSTMCell,GRUCell, LayerNormBasicLSTMCell, DropoutWrapper,\\\nResidualWrapper\nfrom tensorflow.python.ops import rnn_cell_impl\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.ops import init_ops,math_ops\n\nclass TensorflowModels(object):\n\n  def __init__ (self,n_in,hidden_layer_size,n_out,hidden_layer_type,output_type=""linear"",dropout_rate=0,loss_function=""mse"",optimizer=""adam""):\n\n        #self.session=tf.InteractiveSession()\n        self.n_in  = int(n_in)\n        self.n_out = int(n_out)\n\n        self.n_layers = len(hidden_layer_size)\n\n        self.hidden_layer_size = hidden_layer_size\n        self.hidden_layer_type = hidden_layer_type\n\n        assert len(self.hidden_layer_size) == len(self.hidden_layer_type)\n\n        self.output_type   = output_type\n        self.dropout_rate  = dropout_rate\n        self.loss_function = loss_function\n        self.optimizer     = optimizer\n        #self.activation    ={""tanh"":tf.nn.tanh,""sigmoid"":tf.nn.sigmoid}\n        self.graph=tf.Graph()\n        #self.saver=tf.train.Saver()\n\n\n  def define_feedforward_model(self):\n      layer_list=[]\n      with self.graph.as_default() as g:\n          is_training_batch=tf.placeholder(tf.bool,shape=(),name=""is_training_batch"")\n          bn_params={""is_training"":is_training_batch,""decay"":0.99,""updates_collections"":None}\n          g.add_to_collection(""is_training_batch"",is_training_batch)\n          with tf.name_scope(""input""):\n              input_layer=tf.placeholder(dtype=tf.float32,shape=(None,self.n_in),name=""input_layer"")\n              if self.dropout_rate!=0.0:\n                 print ""Using dropout to avoid overfitting and the dropout rate is"",self.dropout_rate\n                 is_training_drop=tf.placeholder(dtype=tf.bool,shape=(),name=""is_training_drop"")\n                 input_layer_drop=dropout(input_layer,self.dropout_rate,is_training=is_training_drop)\n                 layer_list.append(input_layer_drop)\n                 g.add_to_collection(name=""is_training_drop"",value=is_training_drop)\n              else:\n                 layer_list.append(input_layer)\n          g.add_to_collection(""input_layer"",layer_list[0])\n          for i in xrange(len(self.hidden_layer_size)):\n              with tf.name_scope(""hidden_layer_""+str(i+1)):\n                if self.dropout_rate!=0.0:\n                    last_layer=layer_list[-1]\n                    if self.hidden_layer_type[i]==""tanh"":\n                       new_layer=fully_connected(last_layer,self.hidden_layer_size[i],activation_fn=tf.nn.tanh,normalizer_fn=batch_norm,\\\n                                  normalizer_params=bn_params)\n                    if self.hidden_layer_type[i]==""sigmoid"":\n                        new_layer=fully_connected(last_layer,self.hidden_layer_size[i],activation_fn=tf.nn.sigmoid,normalizer_fn=batch_norm,\\\n                                  normalizer_params=bn_params)\n                    new_layer_drop=dropout(new_layer,self.dropout_rate,is_training=is_training_drop)\n                    layer_list.append(new_layer_drop)\n                else:\n                    last_layer=layer_list[-1]\n                    if self.hidden_layer_type[i]==""tanh"":\n                       new_layer=fully_connected(last_layer,self.hidden_layer_size[i],activation_fn=tf.nn.tanh,normalizer_fn=batch_norm,\\\n                                 normalizer_params=bn_params)\n                    if self.hidden_layer_type[i]==""sigmoid"":\n                       new_layer=fully_connected(last_layer,self.hidden_layer_size[i],activation_fn=tf.nn.sigmoid,normalizer_fn=batch_norm,\\\n                                 normalizer_params=bn_params)\n                    layer_list.append(new_layer)\n          with tf.name_scope(""output_layer""):\n              if self.output_type==""linear"":\n                 output_layer=fully_connected(layer_list[-1],self.n_out,activation_fn=None)\n              if self.output_type==""tanh"":\n                 output_layer=fully_connected(layer_list[-1],self.n_out,activation_fn=tf.nn.tanh)\n              g.add_to_collection(name=""output_layer"",value=output_layer)\n          with tf.name_scope(""training_op""):\n               if self.optimizer==""adam"":\n                  self.training_op=tf.train.AdamOptimizer()\n\n  def define_sequence_model(self):\n      seed=12345\n      np.random.seed(12345)\n      layer_list=[]\n      with self.graph.as_default() as g:\n          utt_length=tf.placeholder(tf.int32,shape=(None))\n          g.add_to_collection(name=""utt_length"",value=utt_length)\n          with tf.name_scope(""input""):\n               input_layer=tf.placeholder(dtype=tf.float32,shape=(None,None,self.n_in),name=""input_layer"")\n               if self.dropout_rate!=0.0:\n                  print ""Using dropout to avoid overfitting and the dropout rate is"",self.dropout_rate\n                  is_training_drop=tf.placeholder(dtype=tf.bool,shape=(),name=""is_training_drop"")\n                  input_layer_drop=dropout(input_layer,self.dropout_rate,is_training=is_training_drop)\n                  layer_list.append(input_layer_drop)\n                  g.add_to_collection(name=""is_training_drop"",value=is_training_drop)\n               else:\n                  layer_list.append(input_layer)\n          g.add_to_collection(""input_layer"",layer_list[0])\n          with tf.name_scope(""hidden_layer""):\n             basic_cell=[]\n             if ""tanh"" in self.hidden_layer_type:\n                 is_training_batch=tf.placeholder(dtype=tf.bool,shape=(),name=""is_training_batch"")\n                 bn_params={""is_training"":is_training_batch,""decay"":0.99,""updates_collections"":None}\n                 g.add_to_collection(""is_training_batch"",is_training_batch)\n             for i in xrange(len(self.hidden_layer_type)):\n                 if self.dropout_rate!=0.0:\n                     if self.hidden_layer_type[i]==""tanh"":\n                         new_layer=fully_connected(layer_list[-1],self.hidden_layer_size[i],activation_fn=tf.nn.tanh,normalizer_fn=batch_norm,normalizer_params=bn_params)\n                         new_layer_drop=dropout(new_layer,self.dropout_rate,is_training=is_training_drop)\n                         layer_list.append(new_layer_drop)\n                     if self.hidden_layer_type[i]==""lstm"":\n                         basic_cell.append(MyDropoutWrapper(BasicLSTMCell(num_units=self.hidden_layer_size[i]),self.dropout_rate,self.dropout_rate,is_training=is_training_drop))\n                     if self.hidden_layer_type[i]==""gru"":\n                         basic_cell.append(MyDropoutWrapper(GRUCell(num_units=self.hidden_layer_size[i]),self.dropout_rate,self.dropout_rate,is_training=is_training_drop))\n                 else:\n                     if self.hidden_layer_type[i]==""tanh"":\n                        new_layer=fully_connected(layer_list[-1],self.hidden_layer_size[i],activation_fn=tf.nn.tanh,normalizer_fn=batch_norm,normalizer_params=bn_params)\n                        layer_list.append(new_layer)\n                     if self.hidden_layer_type[i]==""lstm"":\n                        basic_cell.append(LayerNormBasicLSTMCell(num_units=self.hidden_layer_size[i]))\n                     if self.hidden_layer_type[i]==""gru"":\n                        basic_cell.append(LayerNormGRUCell(num_units=self.hidden_layer_size[i]))\n             multi_cell=MultiRNNCell(basic_cell)\n             rnn_outputs,rnn_states=tf.nn.dynamic_rnn(multi_cell,layer_list[-1],dtype=tf.float32,sequence_length=utt_length)\n             layer_list.append(rnn_outputs)\n          with tf.name_scope(""output_layer""):\n               if self.output_type==""linear"" :\n                   output_layer=tf.layers.dense(rnn_outputs,self.n_out)\n                #  stacked_rnn_outputs=tf.reshape(rnn_outputs,[-1,self.n_out])\n                #  stacked_outputs=tf.layers.dense(stacked_rnn_outputs,self.n_out)\n                #  output_layer=tf.reshape(stacked_outputs,[-1,utt_length,self.n_out])\n               g.add_to_collection(name=""output_layer"",value=output_layer)\n          with tf.name_scope(""training_op""):\n               if self.optimizer==""adam"":\n                   self.training_op=tf.train.AdamOptimizer()\n\n  def get_max_step(self,max_step):\n       ##This method is only used when a sequence model is TrainTensorflowModels\n       self.max_step=max_step\n\nclass MyDropoutWrapper(DropoutWrapper):\n\n    def __init__(self, cell, is_training,input_keep_prob=1.0, output_keep_prob=1.0,\n               state_keep_prob=1.0, variational_recurrent=False,\n                input_size=None, dtype=None, seed=None):\n        DropoutWrapper.__init__(self, cell, input_keep_prob=1.0, output_keep_prob=1.0,\n               state_keep_prob=1.0, variational_recurrent=False,\n               input_size=None, dtype=None, seed=None)\n        self.is_training=is_training\n\n    def __call__(self, inputs, state, scope=None):\n\n        return tf.cond(self.is_training,\\\n                    lambda: DropoutWrapper(self._cell,self._input_keep_prob,self._output_keep_prob).__call__(inputs,state,scope=None),\\\n                    lambda: DropoutWrapper(self._cell,1.0,1.0).__call__(inputs,state,scope=None))\n           #return self._cell(dropout(inputs,self._input_keep_prob,is_training=self.is_training,scope=None),state,scope=None)\n\nclass Encoder_Decoder_Models(TensorflowModels):\n\n      def __init__(self,n_in,encoder_layer_size,n_out,encoder_layer_type,output_type=""linear"",dropout_rate=0,loss_function=""mse"",optimizer=""adam"",attention=False,cbhg=False):\n          TensorflowModels.__init__(self,n_in,encoder_layer_size,n_out,encoder_layer_type,output_type=""linear"",dropout_rate=0,loss_function=""mse"",optimizer=""adam"")\n          self.encoder_layer_size=self.hidden_layer_size\n          self.encoder_layer_type=self.hidden_layer_type\n          self.attention=attention\n          self.cbhg=cbhg\n\n      def convbank(self,inputs,conv_bank_size,scope=""convbank""):\n          with tf.variable_scope(scope,reuse=None):\n              outputs=tf.layers.conv1d(inputs,self.n_in//2,1)\n              for k in range(2,conv_bank_size+1):\n                  with tf.variable_scope(""num_{0}"".format(k)):\n                    k_output=tf.layers.conv1d(inputs,self.n_in//2,k,padding=""same"",activation=tf.nn.relu)\n                    outputs=tf.concat((outputs,k_output),-1)\n          return outputs\n\n      def pooling(self,conv_outputs,pooling_window,stride,scope=""pooling""):\n            with tf.variable_scope(scope,reuse=None):\n                pooling_outputs=tf.layers.max_pooling1d(conv_outputs,pooling_window,stride)\n        #    print pooling_outputs.shape\n            return pooling_outputs\n\n      def convproject(self,inputs,filters,width,scope=""convproject""):\n           with tf.variable_scope(scope,reuse=None):\n               projection_layer=tf.layers.conv1d(inputs,filters,width,padding=""same"",activation=tf.nn.relu)\n          # print projection_layer.shape\n           return projection_layer\n\n      def deep_feedfoward(self,project_outputs,num_units,layers=4,scope=""feedforward""):\n          with tf.variable_scope(scope,reuse=None):\n               layer_list=[project_outputs]\n               for l in range(layers):\n                     layer_list.append(fully_connected(layer_list[-1],num_units,activation_fn=tf.nn.relu))\n        #  print layer_list[-1].shape\n          return layer_list[-1]\n\n      def encoder(self,inputs,inputs_sequence_length):\n           with tf.variable_scope(""encoder""):\n                basic_cell=[]\n                for i in xrange(len(self.hidden_layer_size)):\n                    if self.hidden_layer_type[i]==""tanh"":\n                        basic_cell.append(tf.contrib.rnn.BasicRNNCell(num_units=self.encoder_layer_size[i]))\n                    if self.hidden_layer_type[i]==""lstm"":\n                        basic_cell.append(tf.contrib.rnn.BasicLSTMCell(num_units=self.encoder_layer_size[i]))\n                    if self.hidden_layer_type[i]==""gru"":\n                         basic_cell.append(GRUCell(num_units=self.encoder_layer_size[i]))\n                multicell=MultiRNNCell(basic_cell)\n                enc_output, enc_state=tf.nn.bidirectional_dynamic_rnn(cell_fw=multicell,cell_bw=multicell,inputs=inputs,\\\n                             sequence_length=inputs_sequence_length,dtype=tf.float32)\n                enc_output=tf.concat(enc_output,2)\n                #enc_state=(tf.concat(enc_state[0])\n                return enc_output, enc_state\n\n      def process_decoder_input(self,target_sequence):\n          decode_input=tf.concat((tf.zeros_like(target_sequence[:, :1, :]), target_sequence[:, :-1, :]), 1)\n\n          return decode_input\n\n      def decoder(self,decoder_inputs,enc_output,enc_states,target_sequence_length):\n          """"""Memory is a tuple containing the forward and backward final states (output_states_fw,output_states_bw)""""""\n          with tf.variable_scope(""decoder""):\n              basic_cell=[]\n              for i in xrange(len(self.hidden_layer_size)):\n                    if self.hidden_layer_type[i]==""tanh"":\n                        basic_cell.append(tf.contrib.rnn.BasicRNNCell(num_units=self.encoder_layer_size[i]))\n                    if self.hidden_layer_type[i]==""lstm"":\n                        basic_cell.append(tf.contrib.rnn.BasicLSTMCell(num_units=self.encoder_layer_size[i]))\n                    if self.hidden_layer_type[i]==""gru"":\n                         basic_cell.append(GRUCell(num_units=self.encoder_layer_size[i]))\n              multicell=MultiRNNCell(basic_cell)\n          if not self.attention:\n              dec_output,_=tf.nn.bidirectional_dynamic_rnn(cell_fw=multicell,cell_bw=multicell,inputs=decoder_inputs,initial_state_fw=enc_states[0],\\\n                                                           sequence_length=target_sequence_length,initial_state_bw=enc_states[1])\n          else:\n              attention_size=decoder_inputs.get_shape().as_list()[-1]\n              attention_mechanism=tf.contrib.seq2seq.BahdanauAttention(attention_size,enc_output,target_sequence_length,normalize=True,probability_fn=tf.nn.softmax)\n              cell_with_attention=tf.contrib.seq2seq.AttentionWrapper(multicell,attention_mechanism,attention_size)\n              dec_output,_=tf.nn.bidirectional_dynamic_rnn(cell_fw=cell_with_attention,cell_bw=cell_with_attention,inputs=decoder_inputs,dtype=tf.float32)\n          return dec_output\n\n      def define_encoder_decoder(self):\n          with self.graph.as_default() as g:\n              with tf.name_scope(""encoder_input""):\n                  inputs_data=tf.placeholder(dtype=tf.float32,shape=[None,None,self.n_in],name=""inputs_data"")\n                  if self.cbhg:\n                      conv_bank=self.convbank(inputs_data,16)\n                      max_pooled_=self.pooling(conv_bank,2,1)\n                      conv_project=self.convproject(max_pooled_,self.n_in//2,3)\n                      encoder_inputs=self.deep_feedfoward(conv_project,self.n_in//2,4)\n                  else:\n                      inputs_sequence_length=tf.placeholder(tf.int32,shape=[None],name=""inputs_sequence_length"")\n                      g.add_to_collection(""inputs_sequence_length"",inputs_sequence_length)\n                  g.add_to_collection(""inputs_data"",inputs_data)\n              with tf.name_scope(""target_sequence""):\n                   targets=tf.placeholder(dtype=tf.float32,shape=[None,None,self.n_out],name=""targets"")\n                   target_sequence_length=tf.placeholder(tf.int32,[None],name=""target_sequence_length"")\n                   g.add_to_collection(""targets"",targets)\n                   g.add_to_collection(""target_sequence_length"",target_sequence_length)\n\n              with tf.name_scope(""encoder_output""):\n                   if self.cbhg:\n                       enc_out,enc_states=self.encoder(encoder_inputs,None)\n                   else:\n                       enc_out,enc_states=self.encoder(inputs_data,inputs_sequence_length)\n              with tf.name_scope(""decoder_inputs""):\n                   dec_inputs=self.process_decoder_input(targets)\n              with tf.name_scope(""decoder_outputs""):\n                   dec_output=self.decoder(dec_inputs,enc_out,enc_states,target_sequence_length)\n                   dec_output=tf.concat(dec_output,2)\n              with tf.name_scope(""outputs""):\n                  if self.output_type==""linear"":\n                     outputs=tf.layers.dense(dec_output,self.n_out)\n                  g.add_to_collection(name=""decoder_outputs"",value=outputs)\n              with tf.name_scope(""training_op""):\n                   if self.optimizer==""adam"":\n                      self.training_op=tf.train.AdamOptimizer(0.002)\n\ndef layer_normalization(inputs,epsilon = 1e-5,scope=None):\n    mean,var=tf.nn.moments(inputs,[1],keep_dims=True)\n    with tf.variable_scope(scope+""LN"",reuse=None):\n          scale=tf.get_variable(name=""scale"",shape=[inputs.get_shape()[1]],initializer=tf.constant_initializer(1))\n          shift=tf.get_variable(name=""shift"",shape=[inputs.get_shape()[1]],initializer=tf.constant_initializer(0))\n    LN_output=scale*(inputs-mean)/tf.sqrt(var + epsilon) + shift\n    return LN_output\n\n\nclass LayerNormGRUCell(RNNCell):\n  """"""Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).""""""\n\n  def __init__(self,\n               num_units,\n               activation=None,\n               reuse=None,\n               kernel_initializer=None,\n               bias_initializer=None):\n    super(LayerNormGRUCell, self).__init__(_reuse=reuse)\n    self._num_units = num_units\n    self._activation = activation or math_ops.tanh\n    self._kernel_initializer = kernel_initializer\n    self._bias_initializer = bias_initializer\n\n  @property\n  def state_size(self):\n    return self._num_units\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  def __call__(self, inputs, state):\n    """"""Gated recurrent unit (GRU) with nunits cells.""""""\n    with vs.variable_scope(""gates""):  # Reset gate and update gate.\n      # We start with bias of 1.0 to not reset and not update.\n      bias_ones = self._bias_initializer\n      if self._bias_initializer is None:\n        dtype = [a.dtype for a in [inputs, state]][0]\n        bias_ones = init_ops.constant_initializer(1.0, dtype=dtype)\n      value = rnn_cell_impl._linear([inputs, state], 2 * self._num_units, True, bias_ones,\\\n                  self._kernel_initializer)\n      r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1)\n      r,u=layer_normalization(r,scope=""r/""),layer_normalization(u,scope=""u/"")\n      r,u=math_ops.sigmoid(r),math_ops.sigmoid(u)\n    with vs.variable_scope(""candidate""):\n      c = self._activation(rnn_cell_impl._linear([inputs, r * state], self._num_units, True, self._bias_initializer, self._kernel_initializer))\n    new_h = u * state + (1 - u) * c\n    return new_h, new_h\n'"
src/tensorflow_lib/train.py,40,"b'#!/usr/bin/env python\n################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://github.com/CSTR-Edinburgh/merlin\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\nimport tensorflow as tf\nimport numpy as np\nimport random, os ,sys\nfrom io_funcs.binary_io import BinaryIOCollection\nfrom tensorflow_lib.model import TensorflowModels, Encoder_Decoder_Models\nfrom tensorflow_lib import data_utils\n\n\nclass TrainTensorflowModels(TensorflowModels):\n\n    def __init__(self, n_in, hidden_layer_size, n_out, hidden_layer_type, model_dir,output_type=\'linear\', dropout_rate=0.0, loss_function=\'mse\', optimizer=\'adam\', rnn_params=None):\n\n        TensorflowModels.__init__(self, n_in, hidden_layer_size, n_out, hidden_layer_type, output_type, dropout_rate, loss_function, optimizer)\n\n        #### TODO: Find a good way to pass below params ####\n        self.ckpt_dir = model_dir\n\n    def train_feedforward_model(self, train_x, train_y, batch_size=256, num_of_epochs=10, shuffle_data=True):\n        seed=12345\n        np.random.seed(seed)\n        print train_x.shape\n        with self.graph.as_default() as g:\n           output_data=tf.placeholder(dtype=tf.float32,shape=(None,self.n_out),name=""output_data"")\n           input_layer=g.get_collection(name=""input_layer"")[0]\n           is_training_batch=g.get_collection(name=""is_training_batch"")[0]\n           if self.dropout_rate!=0.0:\n              is_training_drop=g.get_collection(name=""is_training_drop"")[0]\n           with tf.name_scope(""loss""):\n               output_layer=g.get_collection(name=""output_layer"")[0]\n               loss=tf.reduce_mean(tf.square(output_layer-output_data),name=""loss"")\n           with tf.name_scope(""train""):\n                self.training_op=self.training_op.minimize(loss)\n           init=tf.global_variables_initializer()\n           self.saver=tf.train.Saver()\n           with tf.Session() as sess:\n             init.run();summary_writer=tf.summary.FileWriter(os.path.join(self.ckpt_dir,""losslog""),sess.graph)\n             for epoch in xrange(num_of_epochs):\n                 L=1;overall_loss=0\n                 for iteration in range(int(train_x.shape[0]/batch_size)+1):\n                    if (iteration+1)*batch_size>train_x.shape[0]:\n                        x_batch,y_batch=train_x[iteration*batch_size:],train_y[iteration*batch_size:]\n                        if x_batch!=[]:\n                           L+=1\n                        else:continue\n                    else:\n                        x_batch,y_batch=train_x[iteration*batch_size:(iteration+1)*batch_size,], train_y[iteration*batch_size:(iteration+1)*batch_size]\n                        L+=1\n                    if self.dropout_rate!=0.0:\n                       _,batch_loss=sess.run([self.training_op,loss],feed_dict={input_layer:x_batch,output_data:y_batch,is_training_drop:True,is_training_batch:True})\n                       #rs=sess.run(merged,feed_dict={input_layer:x_batch,output_data:y_batch,is_training_drop:True,is_training_batch:True})\n                    else:\n                       _,batch_loss=sess.run([self.training_op,loss],feed_dict={input_layer:x_batch,output_data:y_batch,is_training_batch:True})\n                       #rs=sess.run(merged,feed_dict={input_layer:x_batch,output_data:y_batch,is_training_batch:True})\n                    overall_loss+=batch_loss\n            #if self.dropout_rate!=0.0:\n            #        training_loss=loss.eval(feed_dict={input_layer:train_x,output_data:train_y,is_training_drop:False,is_training_batch:False})\n            #     else:\n            #        training_loss=loss.eval(feed_dict={input_layer:train_x,output_data:train_y,is_training_batch:False})\n                 print ""Epoch "",epoch+1, ""Finishes"",""Training loss:"",overall_loss/L\n             self.saver.save(sess,os.path.join(self.ckpt_dir,""mymodel.ckpt""))\n             print ""The model parameters are saved""\n\n    def get_batch(self,train_x,train_y,start,batch_size=50):\n        utt_keys=train_x.keys()\n        if (start+1)*batch_size>len(utt_keys):\n            batch_keys=utt_keys[start*batch_size:]\n        else:\n           batch_keys=utt_keys[start*batch_size:(start+1)*batch_size]\n        batch_x_dict=dict([(k,train_x[k]) for k  in batch_keys])\n        batch_y_dict=dict([(k,train_y[k]) for k in batch_keys])\n        utt_len_batch=[len(batch_x_dict[k])for k in batch_x_dict.keys()]\n        return batch_x_dict, batch_y_dict, utt_len_batch\n\n    def train_sequence_model(self,train_x,train_y,utt_length,batch_size=256,num_of_epochs=10,shuffle_data=False):\n        seed=12345\n        np.random.seed(seed)\n        #Data Preparation\n        temp_train_x = data_utils.transform_data_to_3d_matrix(train_x, max_length=self.max_step, shuffle_data=False)\n        print(""Input shape: ""+str(temp_train_x.shape))\n        temp_train_y = data_utils.transform_data_to_3d_matrix(train_y, max_length=self.max_step, shuffle_data=False)\n        print(""Output shape: ""+str(temp_train_y.shape))\n        #Shuffle the data\n\n        with self.graph.as_default() as g:\n            output_layer=g.get_collection(name=""output_layer"")[0]\n            input_layer=g.get_collection(name=""input_layer"")[0]\n            utt_length_placeholder=g.get_collection(name=""utt_length"")[0]\n            hybrid=0\n            if ""tanh"" in self.hidden_layer_type:\n                hybrid=1\n                is_training_batch=g.get_collection(name=""is_training_batch"")[0]\n            if self.dropout_rate!=0.0:\n                is_training_drop=g.get_collection(name=""is_training_drop"")[0]\n            with tf.name_scope(""output_data""):\n               output_data=tf.placeholder(tf.float32,shape=(None,None,self.n_out))\n            with tf.name_scope(""loss""):\n                error=output_data-output_layer\n                loss=tf.reduce_mean(tf.square(error),name=""loss"")\n            with tf.name_scope(""train""):\n                self.training_op=self.training_op.minimize(loss)\n            init=tf.global_variables_initializer()\n            self.saver=tf.train.Saver()\n            #overall_loss=tf.summary.scalar(""training loss"",overall_loss)\n            with tf.Session() as sess:\n                 init.run();summary_writer=tf.summary.FileWriter(os.path.join(self.ckpt_dir,""losslog""),sess.graph)\n                 for epoch in xrange(num_of_epochs):\n                     L=1;overall_loss=0\n                     for iteration in range(int(len(train_x.keys())/batch_size)+1):\n                        x_batch,y_batch,utt_length_batch=self.get_batch(train_x,train_y,iteration,batch_size)\n                        if utt_length_batch==[]:\n                            continue\n                        else:L+=1\n                        max_length_batch=max(utt_length_batch)\n                        x_batch=data_utils.transform_data_to_3d_matrix(x_batch, max_length=max_length_batch, shuffle_data=False)\n                        y_batch=data_utils.transform_data_to_3d_matrix(y_batch, max_length=max_length_batch, shuffle_data=False)\n                        if self.dropout_rate!=0.0:\n                           if hybrid:\n                              _,batch_loss=sess.run([self.training_op,loss],feed_dict={input_layer:x_batch,output_data:y_batch,utt_length_placeholder:utt_length_batch,\\\n                                       is_training_drop:True,is_training_batch:True})\n                           else:\n                             _,batch_loss=sess.run([self.training_op,loss],feed_dict={input_layer:x_batch,output_data:y_batch,utt_length_placeholder:utt_length_batch,\\\n                                       is_training_drop:True})\n                        elif hybrid:\n                           _,batch_loss=sess.run([self.training_op,loss],feed_dict={input_layer:x_batch,output_data:y_batch,utt_length_placeholder:utt_length_batch,is_training_batch:True})\n                        else:\n                           _,batch_loss=sess.run([self.training_op,loss],feed_dict={input_layer:x_batch,output_data:y_batch,utt_length_placeholder:utt_length_batch})\n                     overall_loss+=batch_loss\n                     #summary_writer.add_summary(overall_loss,epoch)\n                     #if self.dropout_rate!=0.0:\n                     #    if hybrid:\n                     #       training_loss=loss.eval(feed_dict={input_layer:temp_train_x,output_data:temp_train_y,utt_length_placeholder:utt_length,\\\n                     #       is_training_drop:False,is_training_batch:False})\n                     #    else:\n                     #       training_loss=loss.eval(feed_dict={input_layer:temp_train_x,output_data:temp_train_y,utt_length_placeholder:utt_length,\\\n                     #       is_training_drop:False})\n                     #elif hybrid:\n                     #    training_loss=loss.eval(feed_dict={input_layer:temp_train_x,output_data:temp_train_y,utt_length_placeholder:utt_length,is_training_batch:False})\n                     #else:\n                     #    training_loss=loss.eval(feed_dict={input_layer:temp_train_x,output_data:temp_train_y,utt_length_placeholder:utt_length})\n                     print ""Epoch "",epoch+1,""Training loss:"",overall_loss/L\n                 #model_name=""sequence_model""+"" hybrid.ckpt"" if hybrid==1 else ""sequence_model.ckpt""\n                 self.saver.save(sess,os.path.join(self.ckpt_dir,""mymodel.ckpt""))\n                 print ""The model parameters are saved""\n\n    def predict(self, test_x, out_scaler, gen_test_file_list, sequential_training=False, stateful=False):\n        #### compute predictions ####\n\n        io_funcs = BinaryIOCollection()\n\n        test_id_list = test_x.keys()\n        test_id_list.sort()\n\n        test_file_number = len(test_id_list)\n\n        print(""generating features on held-out test data..."")\n        with tf.Session() as sess:\n           new_saver=tf.train.import_meta_graph(os.path.join(self.ckpt_dir,""mymodel.ckpt.meta""))\n           print ""loading the model parameters...""\n           output_layer=tf.get_collection(""output_layer"")[0]\n           input_layer=tf.get_collection(""input_layer"")[0]\n           new_saver.restore(sess,os.path.join(self.ckpt_dir,""mymodel.ckpt""))\n           print ""The model parameters are successfully restored""\n           for utt_index in xrange(test_file_number):\n               gen_test_file_name = gen_test_file_list[utt_index]\n               temp_test_x        = test_x[test_id_list[utt_index]]\n               num_of_rows        = temp_test_x.shape[0]\n               if not sequential_training:\n                   is_training_batch=tf.get_collection(""is_training_batch"")[0]\n                   if self.dropout_rate!=0.0:\n                        is_training_drop=tf.get_collection(""is_training_drop"")[0]\n                        y_predict=sess.run(output_layer,feed_dict={input_layer:temp_test_x,is_training_drop:False,is_training_batch:False})\n                   else:\n                        y_predict=sess.run(output_layer,feed_dict={input_layer:temp_test_x,is_training_batch:False})\n               else:\n                        temp_test_x=np.reshape(temp_test_x,[1,num_of_rows,self.n_in])\n                        hybrid=0\n                        utt_length_placeholder=tf.get_collection(""utt_length"")[0]\n                        if ""tanh"" in self.hidden_layer_type:\n                            hybrid=1\n                            is_training_batch=tf.get_collection(""is_training_batch"")[0]\n                        if self.dropout_rate!=0.0:\n                           is_training_drop=tf.get_collection(""is_training_drop"")[0]\n                           if hybrid:\n                              y_predict=sess.run(output_layer,feed_dict={input_layer:temp_test_x,utt_length_placeholder:[num_of_rows],is_training_drop:False,is_training_batch:False})\n                           else:\n                              y_predict=sess.run(output_layer,feed_dict={input_layer:temp_test_x,utt_length_placeholder:[num_of_rows],is_training_drop:False})\n                        elif hybrid:\n                              y_predict=sess.run(output_layer,feed_dict={input_layer:temp_test_x,utt_length_placeholder:[num_of_rows],is_training_batch:False})\n                        else:\n                              y_predict=sess.run(output_layer,feed_dict={input_layer:temp_test_x,utt_length_placeholder:[num_of_rows]})\n               data_utils.denorm_data(y_predict, out_scaler)\n               io_funcs.array_to_binary_file(y_predict, gen_test_file_name)\n               data_utils.drawProgressBar(utt_index+1, test_file_number)\n    sys.stdout.write(""\\n"")\n\nclass Train_Encoder_Decoder_Models(Encoder_Decoder_Models):\n\n      def __init__(self,n_in, hidden_layer_size, n_out, hidden_layer_type, model_dir,output_type=\'linear\', dropout_rate=0.0, loss_function=\'mse\', optimizer=\'adam\',attention=False,cbhg=False):\n          Encoder_Decoder_Models.__init__(self,n_in, hidden_layer_size, n_out, hidden_layer_type, output_type=\'linear\', dropout_rate=0.0, loss_function=\'mse\', \\\n                                optimizer=\'adam\',attention=attention,cbhg=cbhg)\n          self.ckpt_dir=os.path.join(model_dir,""temp_checkpoint_file"")\n\n      def get_batch(self,train_x,train_y,start,batch_size):\n\n             utt_keys=train_x.keys()\n             if (start+1)*batch_size>len(utt_keys):\n                 batch_keys=utt_keys[start*batch_size:]\n             else:\n                batch_keys=utt_keys[start*batch_size:(start+1)*batch_size]\n             batch_x_dict=dict([(k,train_x[k]) for k  in batch_keys])\n             batch_y_dict=dict([(k,train_y[k]) for k in batch_keys])\n             utt_len_batch=[len(batch_x_dict[k])for k in batch_x_dict.keys()]\n             return batch_x_dict, batch_y_dict, utt_len_batch\n\n\n      def train_encoder_decoder_model(self,train_x,train_y,utt_length,batch_size=1,num_of_epochs=10,shuffle_data=False):\n          temp_train_x = data_utils.transform_data_to_3d_matrix(train_x, max_length=self.max_step,shuffle_data=False)\n          print(""Input shape: ""+str(temp_train_x.shape))\n          temp_train_y = data_utils.transform_data_to_3d_matrix(train_y, max_length=self.max_step,shuffle_data=False)\n          print(""Output shape: ""+str(temp_train_y.shape))\n\n          with self.graph.as_default() as g:\n               outputs=g.get_collection(name=""decoder_outputs"")[0]\n               var=g.get_collection(name=""trainable_variables"")\n               targets=g.get_tensor_by_name(""target_sequence/targets:0"")\n               inputs_data=g.get_tensor_by_name(""encoder_input/inputs_data:0"")\n               if not self.cbhg:\n                  inputs_sequence_length=g.get_tensor_by_name(""encoder_input/inputs_sequence_length:0"")\n               target_sequence_length=g.get_tensor_by_name(""target_sequence/target_sequence_length:0"")\n               with tf.name_scope(""loss""):\n                   error=targets-outputs\n                   loss=tf.reduce_mean(tf.square(error))\n               gradients=self.training_op.compute_gradients(loss)\n               capped_gradients=[(tf.clip_by_value(grad,-5.,5.),var) for grad,var in gradients if grad is not None]\n               self.training_op=self.training_op.apply_gradients(capped_gradients)\n               init=tf.global_variables_initializer()\n               self.saver=tf.train.Saver()\n               overall_loss=0;tf.summary.scalar(""training_loss"",overall_loss)\n               with tf.Session() as sess:\n                 init.run();tf.summary_writer=tf.summary.FileWriter(os.path.join(self.ckpt_dir,""losslog""),sess.graph)\n                 for epoch in xrange(num_of_epochs):\n                     L=1\n                     for iteration in range(int(temp_train_x.shape[0]/batch_size)+1):\n                        x_batch_dict,y_batch_dict,utt_length_batch=self.get_batch(train_x,train_y,iteration,batch_size)\n                        if utt_length_batch==[]:\n                            continue\n                        else:L+=1\n                        assert [len(v) for v in x_batch_dict.values()]==[len(v) for v in y_batch_dict.values()]\n                        assert x_batch_dict.keys()==y_batch_dict.keys()\n                        max_length_batch=max(utt_length_batch)\n                        x_batch=data_utils.transform_data_to_3d_matrix(x_batch_dict, max_length=max_length_batch, shuffle_data=False)\n                        y_batch=data_utils.transform_data_to_3d_matrix(y_batch_dict, max_length=max_length_batch, shuffle_data=False)\n                        if self.cbhg:\n                           _,batch_loss=sess.run([self.training_op,loss],{inputs_data:x_batch,targets:y_batch,target_sequence_length:utt_length_batch})\n                        else:\n                           _,batch_loss=sess.run([self.training_op,loss],{inputs_data:x_batch,targets:y_batch,inputs_sequence_length:utt_length_batch,target_sequence_length:utt_length_batch})\n                        overall_loss+=batch_loss\n                     #if self.cbhg:\n                     #    training_loss=loss.eval(feed_dict={inputs_data:temp_train_x,targets:temp_train_y,target_sequence_length:utt_length})\n                     #else:\n                     #    training_loss=loss.eval(feed_dict={inputs_data:temp_train_x,targets:temp_train_y,inputs_sequence_length:utt_length,target_sequence_length:utt_length})\n                     print ""Epoch:"",epoch+1, ""Training loss:"",overall_loss/L\n                     summary_writer.add_summary(str(overall_loss),epoch)\n                 self.saver.save(sess,os.path.join(self.ckpt_dir,""mymodel.ckpt""))\n                 print ""The model parameters are saved""\n\n      def predict(self,test_x, out_scaler, gen_test_file_list):\n          #### compute predictions ####\n\n         io_funcs = BinaryIOCollection()\n\n         test_id_list = test_x.keys()\n         test_id_list.sort()\n         inference_batch_size=len(test_id_list)\n         test_file_number = len(test_id_list)\n         with tf.Session(graph=self.graph) as sess:\n             new_saver=tf.train.import_meta_graph(self.ckpt_dir,""mymodel.ckpt.meta"")\n             """"""Notice change targets=tf.get_collection(""targets"")[0]""""""\n             inputs_data=self.graph.get_collection(""inputs_data"")[0]\n             """"""Notice Change decoder_outputs=tf.get_collection(""decoder_outputs"")[0]""""""\n             inputs_sequence_length=self.graph.get_collection(""inputs_sequence_length"")[0]\n             target_sequence_length=self.graph.get_collection(""target_sequence_length"")[0]\n             print ""loading the model parameters...""\n             new_saver.restore(sess,os.path.join(self.ckpt_dir,""mymodel.ckpt""))\n             print ""Model parameters are successfully restored""\n             print(""generating features on held-out test data..."")\n             for utt_index in xrange(test_file_number):\n               gen_test_file_name = gen_test_file_list[utt_index]\n               temp_test_x        = test_x[test_id_list[utt_index]]\n               num_of_rows        = temp_test_x.shape[0]\n\n         #utt_length=[len(utt) for utt in test_x.values()]\n         #max_step=max(utt_length)\n               temp_test_x = tf.reshape(temp_test_x,[1,num_of_rows,self.n_in])\n\n               outputs=np.zeros(shape=[len(test_x),max_step,self.n_out],dtype=np.float32)\n                #dec_cell=self.graph.get_collection(""decoder_cell"")[0]\n               print ""Generating speech parameters ...""\n               for t in range(num_of_rows):\n                 #  outputs=sess.run(inference_output,{inputs_data:temp_test_x,inputs_sequence_length:utt_length,\\\n                #            target_sequence_length:utt_length})\n                   _outputs=sess.run(decoder_outputs,feed_dict={inputs_data:temp_test_x,targets:outputs,inputs_sequence_length:[num_of_rows],\\\n                             target_sequence_length:[num_of_rows]})\n                #   #print _outputs[:,t,:]\n                   outputs[:,t,:]=_outputs[:,t,:]\n\n               data_utils.denorm_data(outputs, out_scaler)\n               io_funcs.array_to_binary_file(outputs, gen_test_file_name)\n               data_utils.drawProgressBar(utt_index+1, test_file_number)\n      sys.stdout.write(""\\n"")\n'"
src/training_schemes/__init__.py,0,b''
src/training_schemes/adam.py,0,"b'import theano\nimport theano.tensor as T\n\nimport numpy as np\nfrom collections import OrderedDict\n\ndef compile_ADAM_train_function(model, gparams, learning_rate=0.001, b1=0.9, b2=0.999, e=1e-8,\n         gamma=1-1e-8):\n    """"""\n    ADAM update rules\n    Default values are taken from [Kingma2014]\n\n    References:\n    [Kingma2014] Kingma, Diederik, and Jimmy Ba.\n    ""Adam: A Method for Stochastic Optimization.""\n    arXiv preprint arXiv:1412.6980 (2014).\n    http://arxiv.org/pdf/1412.6980v4.pdf\n\n    """"""\n    updates = OrderedDict()\n    all_params = model.params\n    all_grads = gparams\n    alpha = learning_rate\n    t = theano.shared(np.float32(1))\n    b1_t = b1*gamma**(t-1)   #(Decay the first moment running average coefficient)\n\n    for theta_previous, g in zip(all_params, all_grads):\n        m_previous = theano.shared(np.zeros(theta_previous.get_value().shape,\n                                            dtype=theano.config.floatX))\n        v_previous = theano.shared(np.zeros(theta_previous.get_value().shape,\n                                            dtype=theano.config.floatX))\n\n        m = b1_t*m_previous + (1 - b1_t)*g                             # (Update biased first moment estimate)\n        v = b2*v_previous + (1 - b2)*g**2                              # (Update biased second raw moment estimate)\n        m_hat = m / (1-b1**t)                                          # (Compute bias-corrected first moment estimate)\n        v_hat = v / (1-b2**t)                                          # (Compute bias-corrected second raw moment estimate)\n        theta = theta_previous - (alpha * m_hat) / (T.sqrt(v_hat) + e) #(Update parameters)\n\n        #updates.append((m_previous, m))\n        #updates.append((v_previous, v))\n        #updates.append((theta_previous, theta) )\n        updates[m_previous] = m\n        updates[v_previous] = v\n        updates[theta_previous] = theta\n\n    updates[t] = t + 1.\n    \n    return updates\n\n'"
src/training_schemes/adam_v2.py,0,"b'""""""\nThe MIT License (MIT)\n\nCopyright (c) 2015 Alec Radford\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the ""Software""), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n""""""    \n\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nfrom collections import OrderedDict\n\ndef compile_ADAM_train_function(model, gparams, learning_rate=0.0002, b1=0.1, b2=0.001, e=1e-8):\n    updates = OrderedDict()\n    params = model.params\n    grads = gparams\n    lr = learning_rate\n    i = theano.shared(np.float32(0.))\n    i_t = i + 1.\n    fix1 = 1. - (1. - b1)**i_t\n    fix2 = 1. - (1. - b2)**i_t\n    lr_t = lr * (T.sqrt(fix2) / fix1)\n    for p, g in zip(params, grads):\n        m = theano.shared(np.zeros(p.get_value().shape).astype(dtype=theano.config.floatX))\n        v = theano.shared(np.zeros(p.get_value().shape).astype(dtype=theano.config.floatX))\n        m_t = (b1 * g) + ((1. - b1) * m)\n        v_t = (b2 * T.sqr(g)) + ((1. - b2) * v)\n        g_t = m_t / (T.sqrt(v_t) + e)\n        p_t = p - (lr_t * g_t)\n        #updates.append((m, m_t))\n        #updates.append((v, v_t))\n        #updates.append((p, p_t))\n        updates[m] = m_t\n        updates[v] = v_t\n        updates[p] = p_t\n    \n    updates[i] = i_t\n\n    return updates\n'"
src/training_schemes/rprop.py,0,"b""################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\nimport numpy\nimport theano\nimport theano.tensor as T\n\nimport matplotlib\n# Force matplotlib to not use any Xwindows backend.\nmatplotlib.use('Agg')\nimport  matplotlib.pyplot as plt\nfrom collections import OrderedDict\n\ndef compile_RPROP_train_function(model, gparams, learning_rate=0.001, rprop_algo=2, params_to_update=[]):\n\n    if params_to_update == []:  ## then update all by default\n        params_to_update = list(range(len(gparams)))\n\n    ## 1, 2, 3, 4: Rprop+ Rprop- iRprop+ iRprop-\n    ##    in Igel 2003 'Empirical evaluation of the improved Rprop learning algorithms'\n\n    ## It would be easier to follow if these things were defined in __init__, but\n    ## they are here to keep all RPROP-specific stuff in one place.\n    ## Also, make some or all\n    ## rprop_init_update is configured during __init__, all of the others are hardcoded here\n    ## for now:-\n\n    model.eta_plus = 1.2\n    model.eta_minus = 0.5\n    model.max_update = 50.0\n    model.min_update = 0.0000001\n\n    model.use_rprop = rprop_algo\n    model.rprop_init_update = learning_rate\n\n    model.previous_gparams = []\n    model.update_values = []\n\n    model.update_change_DEBUG = []\n\n    for (i, weights) in enumerate(model.params):\n        model.previous_gparams.append(theano.shared(value = numpy.zeros((numpy.shape(weights.get_value())),\n                        dtype=theano.config.floatX), name='pg_%s'%(i)))\n        model.update_values.append(theano.shared(value =\n                (numpy.ones(numpy.shape(weights.get_value()), \\\n                        dtype=theano.config.floatX) * model.rprop_init_update ), name='uv_%s'%(i)))\n\n        model.update_change_DEBUG.append(theano.shared(value = numpy.zeros((numpy.shape(weights.get_value())),\n                        dtype=theano.config.floatX), name='pcd_%s'%(i)))\n\n\n    if model.use_rprop in [2,4]:\n\n        updates = OrderedDict()\n\n        for (i, (prev_gparam, gparam, update_step, param))  in enumerate(zip(model.previous_gparams, gparams, \\\n                                                        model.update_values, model.params)):\n            if i in params_to_update:\n                ## first update update_values:\n                sign_change_test = prev_gparam * gparam\n                increase_update_size = T.gt(sign_change_test, 0.0) * model.eta_plus\n                decrease_update_size = T.lt(sign_change_test, 0.0) * model.eta_minus\n                retain_update_size   = T.eq(sign_change_test, 0.0)\n                update_changes = increase_update_size + decrease_update_size + retain_update_size\n                new_update_step = update_step * update_changes\n                ## apply floor/ceiling to updates:\n                new_update_step = T.minimum(model.max_update, T.maximum(model.min_update, new_update_step))\n                updates[update_step] = new_update_step\n\n                if model.use_rprop == 4:\n                    ## zero gradients where sign changed: reduce step size but don't change weight\n                    gparam = gparam * (T.gt(sign_change_test, 0.0) + T.eq(sign_change_test, 0.0))\n\n                ## then update params:\n                updates[param] = param - T.sgn(gparam) * new_update_step\n\n                ## store previous iteration gradient to check for sign change in next iteration:\n                updates[prev_gparam] = gparam\n\n                updates[model.update_change_DEBUG[i]] = param  # gparam # sign_change_test #  update_changes    #\n\n    else:\n        sys.exit('RPROP version %s not implemented'%(model.use_rprop))\n\n    return updates\n\n\ndef check_rprop_values(model):\n    print('=== Update steps: ===')\n    for (i, update_step) in enumerate(model.update_values):\n        print('   param no. %s'%(i))\n        print(get_stats(update_step))\n        v = update_step.get_value()\n        if len(v.shape) == 2:\n            print(v[:4, :4])\n        else:\n            print(v[:4])\n        print('   Update changes:--')\n        u = model.update_change_DEBUG[i].get_value()\n        if len(u.shape) == 2:\n            print(u[:4, :4])\n        else:\n            print(u[:4])\n\n\ndef get_stats(theano_shared_params):\n    vals = theano_shared_params.get_value()\n    #m,n = numpy.shape(vals)\n    print('   shape, minm max, mean, 5th and 95th percentile')\n    print('   %s %s %s %s %s %s'%(numpy.shape(vals),vals.min(), vals.max(),vals.mean(), numpy.percentile(vals, 5), numpy.percentile(vals, 95)))\n\n## This is generic, and not specific to RPROP:\ndef plot_weight_histogram(model, outfile, lower=-0.25, upper=0.25):\n    n = len(model.params)\n    plt.clf()\n    for (i, theano_shared_params) in enumerate(model.params):\n        weights = theano_shared_params.get_value()\n        values = weights.flatten()\n        plt.subplot(n,1,i+1)\n        frame = plt.gca()\n        frame.axes.get_yaxis().set_ticks([])\n        if i != n-1:  ## only keep bottom one\n            frame.axes.get_xaxis().set_ticks([])\n        plt.hist(values, 100)\n        plt.xlim(lower, upper)\n        print('   param no. %s'%(i))\n        print(get_stats(theano_shared_params))\n    plt.savefig(outfile)\n    print('Made plot %s'%(outfile))\n"""
src/utils/__init__.py,0,b''
src/utils/acous_feat_extraction.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n#/usr/bin/python -u\n\n\'\'\'\nThis script assumes c-version STRAIGHT which is not available to public. Please use your\nown vocoder to replace this script.\n\'\'\'\nimport sys, os\n#from utils import GlobalCfg\nimport logging\n\n\ndef feat_extraction_magphase(in_wav_dir, file_id_list, cfg, logger, b_multiproc=False):\n    sys.path.append(cfg.magphase_bindir)\n    import libutils as lu\n    import magphase as mp\n\n    def feat_extraction_magphase_one_file(in_wav_dir, file_name_token, acous_feats_dir, cfg, logger):\n\n        # Logging:\n        logger.info(\'Analysing waveform: %s.wav\' % (file_name_token))\n\n        # File setup:\n        wav_file = os.path.join(in_wav_dir, file_name_token + \'.wav\')\n\n        # Feat extraction:\n        mp.analysis_for_acoustic_modelling(wav_file, out_dir=acous_feats_dir, mag_dim=cfg.mag_dim,\n                                                            phase_dim=cfg.real_dim, b_const_rate=cfg.magphase_const_rate)\n\n        return\n\n\n    if b_multiproc:\n        lu.run_multithreaded(feat_extraction_magphase_one_file, in_wav_dir, file_id_list, cfg.acous_feats_dir, cfg, logger)\n    else:\n        for file_name_token in file_id_list:\n            feat_extraction_magphase_one_file(in_wav_dir, file_name_token, cfg.acous_feats_dir, cfg, logger)\n\n\n    return\n\n\ndef acous_feat_extraction(in_wav_dir, file_id_list, cfg):\n\n    logger = logging.getLogger(""acous_feat_extraction"")\n\n    ## MagPhase Vocoder:\n    if cfg.vocoder_type==\'MAGPHASE\':\n        feat_extraction_magphase(in_wav_dir, file_id_list, cfg, logger)\n\n\n    # TODO: Add WORLD and STRAIGHT\n\n    # If vocoder is not supported:\n    else:\n        logger.critical(\'The vocoder %s is not supported for feature extraction yet!\\n\' % cfg.vocoder_type )\n        raise\n\n    return'"
src/utils/compute_distortion.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n\nimport sys, numpy\nfrom io_funcs.binary_io import BinaryIOCollection\nimport  logging\nfrom scipy.stats.stats import pearsonr\n\nclass   DistortionComputation(object):\n    def __init__(self, cmp_dim, mgc_dim, bap_dim, lf0_dim):\n        self.total_frame_number = 0\n        self.distortion = 0.0\n        self.bap_distortion = 0.0\n        self.f0_distortion = 0.0\n        self.vuv_error = 0.0\n\n        self.cmp_dim = cmp_dim\n        self.mgc_dim = mgc_dim\n        self.bap_dim = bap_dim\n        self.lf0_dim = lf0_dim\n\n    def compute_distortion(self, file_id_list, reference_dir, generation_dir, cmp_ext, mgc_ext, bap_ext, lf0_ext):\n\n        total_voiced_frame_number = 0\n        for file_id in file_id_list:\n            reference_file_name = reference_dir + \'/\' + file_id + cmp_ext\n            mgc_file_name = generation_dir + \'/\' + file_id + mgc_ext\n            bap_file_name = generation_dir + \'/\' + file_id + bap_ext\n            lf0_file_name = generation_dir + \'/\' + file_id + lf0_ext\n\n            reference_cmp, ref_frame_number = self.load_binary_file(reference_file_name, self.cmp_dim)\n            generation_mgc, mgc_frame_number = self.load_binary_file(mgc_file_name, self.mgc_dim)\n            generation_bap, bap_frame_number = self.load_binary_file(bap_file_name, self.bap_dim)\n            generation_lf0, lf0_frame_number = self.load_binary_file(lf0_file_name, self.lf0_dim)\n\n            if ref_frame_number != mgc_frame_number:\n                print(""The number of frames is not the same: %d vs %d (%s). Error in compute_distortion.py\\n."" %(ref_frame_number, mgc_frame_number, file_id))\n                sys.exit(1)\n\n            reference_mgc = reference_cmp[:, 0:self.mgc_dim]\n            reference_lf0 = reference_cmp[:, self.mgc_dim*3:self.mgc_dim*3+self.lf0_dim]\n            reference_vuv = reference_cmp[:, self.mgc_dim*3+self.lf0_dim*3:self.mgc_dim*3+self.lf0_dim*3+1]\n            reference_bap = reference_cmp[:, self.mgc_dim*3+self.lf0_dim*3+1:self.mgc_dim*3+self.lf0_dim*3+1+self.bap_dim]\n\n            reference_lf0[reference_vuv<0.5] = 0.0\n#            print   reference_vuv\n            temp_distortion = self.compute_mse(reference_mgc[:, 1:self.mgc_dim], generation_mgc[:, 1:self.mgc_dim])\n            self.distortion += temp_distortion * (10 /numpy.log(10)) * numpy.sqrt(2.0)\n\n            temp_bap_distortion = self.compute_mse(reference_bap, generation_bap)\n            self.bap_distortion += temp_bap_distortion * (10 /numpy.log(10)) * numpy.sqrt(2.0)\n\n            temp_f0_distortion, temp_vuv_error, voiced_frame_number = self.compute_f0_mse(reference_lf0, generation_lf0)\n            self.f0_distortion += temp_f0_distortion\n            self.vuv_error += temp_vuv_error\n\n            self.total_frame_number += ref_frame_number\n            total_voiced_frame_number += voiced_frame_number\n\n        self.distortion /= float(self.total_frame_number)\n        self.bap_distortion /= float(self.total_frame_number)\n\n        self.f0_distortion /= total_voiced_frame_number\n        self.f0_distortion = numpy.sqrt(self.f0_distortion)\n\n        self.vuv_error /= float(self.total_frame_number)\n\n        return  self.distortion, self.bap_distortion, self.f0_distortion, self.vuv_error\n\n    def compute_f0_mse(self, ref_data, gen_data):\n        ref_vuv_vector = numpy.zeros((ref_data.size, 1))\n        gen_vuv_vector = numpy.zeros((ref_data.size, 1))\n\n        ref_vuv_vector[ref_data > 0.0] = 1.0\n        gen_vuv_vector[gen_data > 0.0] = 1.0\n\n        sum_ref_gen_vector = ref_vuv_vector + gen_vuv_vector\n        voiced_ref_data = ref_data[sum_ref_gen_vector == 2.0]\n        voiced_gen_data = gen_data[sum_ref_gen_vector == 2.0]\n        voiced_frame_number = voiced_gen_data.size\n\n        f0_mse = numpy.sum(((numpy.exp(voiced_ref_data) - numpy.exp(voiced_gen_data)) ** 2))\n#        f0_mse = numpy.sum((((voiced_ref_data) - (voiced_gen_data)) ** 2))\n\n        vuv_error_vector = sum_ref_gen_vector[sum_ref_gen_vector == 0.0]\n        vuv_error = numpy.sum(sum_ref_gen_vector[sum_ref_gen_vector == 1.0])\n\n        return  f0_mse, vuv_error, voiced_frame_number\n\n    def compute_mse(self, ref_data, gen_data):\n        diff = (ref_data - gen_data) ** 2\n        sum_diff = numpy.sum(diff, axis=1)\n        sum_diff = numpy.sqrt(sum_diff)       # ** 0.5\n        sum_diff = numpy.sum(sum_diff, axis=0)\n\n        return  sum_diff\n\n    def load_binary_file(self, file_name, dimension):\n        fid_lab = open(file_name, \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        frame_number = features.size / dimension\n        features = features[:(dimension * frame_number)]\n        features = features.reshape((-1, dimension))\n\n        return  features, frame_number\n\n\n\'\'\'\nto be refined. genertic class for various features\n\'\'\'\nclass IndividualDistortionComp(object):\n\n    def __init__(self):\n        self.logger = logging.getLogger(\'computer_distortion\')\n\n    def compute_distortion(self, file_id_list, reference_dir, generation_dir, file_ext, feature_dim):\n        total_voiced_frame_number = 0\n\n        distortion = 0.0\n        vuv_error = 0\n        total_frame_number = 0\n\n        io_funcs = BinaryIOCollection()\n\n        ref_all_files_data = numpy.reshape(numpy.array([]), (-1,1))\n        gen_all_files_data = numpy.reshape(numpy.array([]), (-1,1))\n        for file_id in file_id_list:\n            ref_file_name  = reference_dir + \'/\' + file_id + file_ext\n            gen_file_name  = generation_dir + \'/\' + file_id + file_ext\n\n            ref_data, ref_frame_number = io_funcs.load_binary_file_frame(ref_file_name, feature_dim)\n            gen_data, gen_frame_number = io_funcs.load_binary_file_frame(gen_file_name, feature_dim)\n\n            # accept the difference upto two frames\n            if abs(ref_frame_number - gen_frame_number) <= 2:\n                ref_frame_number = min(ref_frame_number, gen_frame_number)\n                gen_frame_number = min(ref_frame_number, gen_frame_number)\n                ref_data = ref_data[0:ref_frame_number, ]\n                gen_data = gen_data[0:gen_frame_number, ]\n            \n            if ref_frame_number != gen_frame_number:\n                self.logger.critical(""The number of frames is not the same: %d vs %d (%s). Error in compute_distortion.py\\n."" %(ref_frame_number, gen_frame_number, file_id))\n                raise\n\n            if file_ext == \'.lf0\':\n                ref_all_files_data = numpy.concatenate((ref_all_files_data, ref_data), axis=0)\n                gen_all_files_data = numpy.concatenate((gen_all_files_data, gen_data), axis=0)\n                temp_distortion, temp_vuv_error, voiced_frame_number = self.compute_f0_mse(ref_data, gen_data)\n                vuv_error += temp_vuv_error\n                total_voiced_frame_number += voiced_frame_number\n            elif file_ext == \'.dur\':\n                ref_data = numpy.reshape(numpy.sum(ref_data, axis=1), (-1, 1))\n                gen_data = numpy.reshape(numpy.sum(gen_data, axis=1), (-1, 1))\n                ref_all_files_data = numpy.concatenate((ref_all_files_data, ref_data), axis=0)\n                gen_all_files_data = numpy.concatenate((gen_all_files_data, gen_data), axis=0)\n                continue;\n            elif file_ext == \'.mgc\':\n                temp_distortion = self.compute_mse(ref_data[:, 1:feature_dim], gen_data[:, 1:feature_dim])\n            else:\n                temp_distortion = self.compute_mse(ref_data, gen_data)\n\n            distortion += temp_distortion\n\n            total_frame_number += ref_frame_number\n\n        if file_ext == \'.dur\':\n            dur_rmse = self.compute_rmse(ref_all_files_data, gen_all_files_data)\n            dur_corr = self.compute_corr(ref_all_files_data, gen_all_files_data)\n\n            return dur_rmse, dur_corr\n        elif file_ext == \'.lf0\':\n            distortion /= float(total_voiced_frame_number)\n            vuv_error  /= float(total_frame_number)\n\n            distortion = numpy.sqrt(distortion)\n            f0_corr = self.compute_f0_corr(ref_all_files_data, gen_all_files_data)\n\n            return  distortion, f0_corr, vuv_error\n        else:\n            distortion /= float(total_frame_number)\n\n            return  distortion\n\n    def compute_f0_mse(self, ref_data, gen_data):\n        ref_vuv_vector = numpy.zeros((ref_data.size, 1))\n        gen_vuv_vector = numpy.zeros((ref_data.size, 1))\n\n        ref_vuv_vector[ref_data > 0.0] = 1.0\n        gen_vuv_vector[gen_data > 0.0] = 1.0\n\n        sum_ref_gen_vector = ref_vuv_vector + gen_vuv_vector\n        voiced_ref_data = ref_data[sum_ref_gen_vector == 2.0]\n        voiced_gen_data = gen_data[sum_ref_gen_vector == 2.0]\n        voiced_frame_number = voiced_gen_data.size\n\n        f0_mse = (numpy.exp(voiced_ref_data) - numpy.exp(voiced_gen_data)) ** 2\n        f0_mse = numpy.sum((f0_mse))\n\n        vuv_error_vector = sum_ref_gen_vector[sum_ref_gen_vector == 0.0]\n        vuv_error = numpy.sum(sum_ref_gen_vector[sum_ref_gen_vector == 1.0])\n\n        return  f0_mse, vuv_error, voiced_frame_number\n\n    def compute_f0_corr(self, ref_data, gen_data):\n        ref_vuv_vector = numpy.zeros((ref_data.size, 1))\n        gen_vuv_vector = numpy.zeros((ref_data.size, 1))\n\n        ref_vuv_vector[ref_data > 0.0] = 1.0\n        gen_vuv_vector[gen_data > 0.0] = 1.0\n\n        sum_ref_gen_vector = ref_vuv_vector + gen_vuv_vector\n        voiced_ref_data = ref_data[sum_ref_gen_vector == 2.0]\n        voiced_gen_data = gen_data[sum_ref_gen_vector == 2.0]\n        f0_corr = self.compute_corr(numpy.exp(voiced_ref_data), numpy.exp(voiced_gen_data))\n\n        return f0_corr\n\n    def compute_corr(self, ref_data, gen_data):\n        corr_coef = pearsonr(ref_data, gen_data)\n\n        return corr_coef[0]\n\n    def compute_rmse(self, ref_data, gen_data):\n        diff = (ref_data - gen_data) ** 2\n        total_frame_number = ref_data.size\n        sum_diff = numpy.sum(diff)\n        rmse = numpy.sqrt(sum_diff/total_frame_number)\n\n        return rmse\n\n    def compute_mse(self, ref_data, gen_data):\n        diff = (ref_data - gen_data) ** 2\n        sum_diff = numpy.sum(diff, axis=1)\n        sum_diff = numpy.sqrt(sum_diff)       # ** 0.5\n        sum_diff = numpy.sum(sum_diff, axis=0)\n\n        return  sum_diff\n'"
src/utils/file_paths.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \'License\');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \'AS IS\' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Class to handle file paths.\n""""""\n__author__ = \'pasindu@google.com (Pasindu De Silva)\'\n\nimport os\nfrom .utils import prepare_file_path_list\nfrom .utils import read_file_list\n\n\nclass FilePaths(object):\n  _NORM_INFO_FILE_NAME = \'norm_info_%s_%d_%s.dat\'\n  nn_cmp_dir = \'\'\n  nn_cmp_norm_dir = \'\'\n  model_dir = \'\'\n  gen_dir = \'\'\n  inter_data_dir = \'\'\n  norm_info_file = \'\'\n  var_dir = \'\'\n  file_id_list = []\n  test_id_list = []\n  binary_label_dir = \'\'\n  nn_label_dir = \'\'\n  nn_label_norm_dir = \'\'\n  bottleneck_features = \'\'\n  binary_label_file_list = []\n  nn_label_file_list = []\n  nn_label_norm_file_list = []\n  in_label_align_file_list = []\n  dur_file_list = []\n  seq_dur_file_list = []\n  nn_cmp_norm_file_list = []\n\n  def __init__(self, cfg):\n    self.cfg = cfg\n\n    self.inter_data_dir = cfg.inter_data_dir\n    if not os.path.exists(self.inter_data_dir):\n      os.makedirs(self.inter_data_dir)\n\n    self.nn_cmp_dir = os.path.join(\n        self.inter_data_dir,\n        \'nn\' + self.cfg.combined_feature_name + \'_\' + str(self.cfg.cmp_dim))\n    self.nn_cmp_norm_dir = os.path.join(\n        self.inter_data_dir, \'nn_norm\' + self.cfg.combined_feature_name + \'_\' +\n        str(self.cfg.cmp_dim))\n    self.model_dir = os.path.join(self.cfg.work_dir, \'nnets_model\')\n    self.gen_dir = os.path.join(self.cfg.work_dir, \'gen\')\n    self.file_id_list = read_file_list(self.cfg.file_id_scp)\n    self.bottleneck_features = os.path.join(self.gen_dir, \'bottleneck_features\')\n\n    if self.cfg.GenTestList:\n      self.test_id_list = read_file_list(cfg.test_id_scp)\n\n    self.norm_info_file = os.path.join(self.inter_data_dir,\n                                       self._NORM_INFO_FILE_NAME %\n                                       (cfg.combined_feature_name, cfg.cmp_dim,\n                                        cfg.output_feature_normalisation))\n\n    ### save acoustic normalisation information for normalising the features back\n    self.var_dir = os.path.join(self.inter_data_dir, \'var\')\n    if not os.path.exists(self.var_dir):\n      os.makedirs(self.var_dir)\n\n    if self.cfg.MAKEDUR:\n      self.dur_file_list = prepare_file_path_list(\n          self.file_id_list, self.cfg.in_dur_dir, self.cfg.dur_ext)\n\n    if self.cfg.network_type==""S2S"":\n      self.seq_dur_file_list  = prepare_file_path_list(\n          self.file_id_list, self.cfg.in_seq_dur_dir, self.cfg.dur_ext)\n\n    self.nn_cmp_norm_file_list = prepare_file_path_list(\n        self.file_id_list, self.nn_cmp_norm_dir, self.cfg.cmp_ext)\n\n  def get_nnets_file_name(self):\n    return \'%s/%s.model\' % (self.model_dir, self.cfg.model_file_name)\n\n  def get_temp_nn_dir_name(self):\n    return self.cfg.model_file_name\n\n  def get_var_dic(self):\n    var_file_dict = {}\n    for feature_name in list(self.cfg.out_dimension_dict.keys()):\n      var_file_dict[feature_name] = self._get_var_file_name(feature_name)\n    return var_file_dict\n\n  def get_train_list_x_y(self):\n    start = 0\n    end = self.cfg.train_file_number\n    return self.nn_label_norm_file_list[start:end], self.nn_cmp_norm_file_list[\n        start:end]\n\n  def get_valid_list_x_y(self):\n    start = self.cfg.train_file_number\n    end = self.cfg.train_file_number + self.cfg.valid_file_number\n    return self.nn_label_norm_file_list[start:end], self.nn_cmp_norm_file_list[\n        start:end]\n\n  def get_test_list_x_y(self):\n    start = self.cfg.train_file_number + self.cfg.valid_file_number\n    end = self.cfg.train_file_number + self.cfg.valid_file_number + self.cfg.test_file_number\n    return self.nn_label_norm_file_list[start:end], self.nn_cmp_norm_file_list[\n        start:end]\n\n  def _get_var_file_name(self, feature_name):\n    return os.path.join(\n        self.var_dir,\n        feature_name + \'_\' + str(self.cfg.out_dimension_dict[feature_name]))\n\n  def set_label_dir(self, dimension, suffix, lab_dim):\n    self.binary_label_dir = os.path.join(self.inter_data_dir,\n                                         \'binary_label_\' + str(dimension))\n    self.nn_label_dir = os.path.join(self.inter_data_dir,\n                                     \'nn_no_silence_lab_\' + suffix)\n    self.nn_label_norm_dir = os.path.join(self.inter_data_dir,\n                                          \'nn_no_silence_lab_norm_\' + suffix)\n\n    label_norm_file = \'label_norm_%s_%d.dat\' % (self.cfg.label_style, lab_dim)\n    self.label_norm_file = os.path.join(self.inter_data_dir, label_norm_file)\n\n    out_feat_dir = os.path.join(self.inter_data_dir, \'binary_label_\' + suffix)\n    self.out_feat_file_list = prepare_file_path_list(\n        self.file_id_list, out_feat_dir, self.cfg.lab_ext)\n\n  def get_nn_cmp_file_list(self):\n    return prepare_file_path_list(self.file_id_list, self.nn_cmp_dir,\n                                  self.cfg.cmp_ext)\n\n  def get_nn_cmp_norm_file_list(self):\n    return self.nn_cmp_norm_file_list\n\n  def get_lf0_file_list(self):\n    return prepare_file_path_list(self.file_id_list, self.cfg.in_lf0_dir,\n                                  self.cfg.lf0_ext)\n\n  def set_label_file_list(self):\n    if self.cfg.GenTestList:\n      self.in_label_align_file_list = prepare_file_path_list(\n          self.test_id_list, self.cfg.in_label_align_dir, self.cfg.lab_ext,\n          False)\n    else:\n      self.in_label_align_file_list = prepare_file_path_list(\n          self.file_id_list, self.cfg.in_label_align_dir, self.cfg.lab_ext,\n          False)\n\n    if self.cfg.GenTestList and self.cfg.test_synth_dir != \'None\' and not self.cfg.VoiceConversion:\n      test_binary_file_list = self._prepare_test_binary_label_file_path_list(\n          self.cfg.test_synth_dir)\n      test_file_list = self._prepare_test_label_file_path_list(\n          self.cfg.test_synth_dir)\n      self.binary_label_file_list = test_binary_file_list\n      self.nn_label_file_list = test_file_list\n      self.nn_label_norm_file_list = test_file_list\n    elif self.cfg.GenTestList:\n      self.binary_label_file_list = self._prepare_test_label_file_path_list(\n          self.binary_label_dir)\n      self.nn_label_file_list = self._prepare_test_label_file_path_list(\n          self.nn_label_dir)\n      self.nn_label_norm_file_list = self._prepare_test_label_file_path_list(\n          self.nn_label_norm_dir)\n    else:\n      self.binary_label_file_list = self._prepare_file_label_file_path_list(\n          self.binary_label_dir)\n      self.nn_label_file_list = self._prepare_file_label_file_path_list(\n          self.nn_label_dir)\n      self.nn_label_norm_file_list = self._prepare_file_label_file_path_list(\n          self.nn_label_norm_dir)\n\n  def _prepare_file_label_file_path_list(self, list_dir):\n    return prepare_file_path_list(self.file_id_list, list_dir, self.cfg.lab_ext)\n\n  def _prepare_test_label_file_path_list(self, list_dir):\n    return prepare_file_path_list(self.test_id_list, list_dir, self.cfg.lab_ext)\n\n  def _prepare_test_binary_label_file_path_list(self, list_dir):\n    return prepare_file_path_list(self.test_id_list, list_dir, self.cfg.lab_ext+\'bin\')\n'"
src/utils/generate.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n#/usr/bin/python -u\n\n\'\'\'\nThis script assumes c-version STRAIGHT which is not available to public. Please use your\nown vocoder to replace this script.\n\'\'\'\nimport sys, os, subprocess, glob, subprocess\n#from utils import GlobalCfg\n\nfrom io_funcs.binary_io import  BinaryIOCollection\nimport numpy as np\n\nimport logging\n\n#import configuration\n\n# cannot have these outside a function - if you do that, they get executed as soon\n# as this file is imported, but that can happen before the configuration is set up properly\n# SPTK     = cfg.SPTK\n# NND      = cfg.NND\n# STRAIGHT = cfg.STRAIGHT\n\n\ndef run_process(args,log=True):\n\n    logger = logging.getLogger(""subprocess"")\n\n    # a convenience function instead of calling subprocess directly\n    # this is so that we can do some logging and catch exceptions\n\n    # we don\'t always want debug logging, even when logging level is DEBUG\n    # especially if calling a lot of external functions\n    # so we can disable it by force, where necessary\n    if log:\n        logger.debug(\'%s\' % args)\n\n    try:\n        # the following is only available in later versions of Python\n        # rval = subprocess.check_output(args)\n\n        # bufsize=-1 enables buffering and may improve performance compared to the unbuffered case\n        p = subprocess.Popen(args, bufsize=-1, shell=True,\n                        stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n                        close_fds=True, env=os.environ)\n        # better to use communicate() than read() and write() - this avoids deadlocks\n        (stdoutdata, stderrdata) = p.communicate()\n\n        if p.returncode != 0:\n            # for critical things, we always log, even if log==False\n            logger.critical(\'exit status %d\' % p.returncode )\n            logger.critical(\' for command: %s\' % args )\n            logger.critical(\'      stderr: %s\' % stderrdata )\n            logger.critical(\'      stdout: %s\' % stdoutdata )\n            raise OSError\n\n        return (stdoutdata, stderrdata)\n\n    except subprocess.CalledProcessError as e:\n        # not sure under what circumstances this exception would be raised in Python 2.6\n        logger.critical(\'exit status %d\' % e.returncode )\n        logger.critical(\' for command: %s\' % args )\n        # not sure if there is an \'output\' attribute under 2.6 ? still need to test this...\n        logger.critical(\'  output: %s\' % e.output )\n        raise\n\n    except ValueError:\n        logger.critical(\'ValueError for %s\' % args )\n        raise\n\n    except OSError:\n        logger.critical(\'OSError for %s\' % args )\n        raise\n\n    except KeyboardInterrupt:\n        logger.critical(\'KeyboardInterrupt during %s\' % args )\n        try:\n            # try to kill the subprocess, if it exists\n            p.kill()\n        except UnboundLocalError:\n            # this means that p was undefined at the moment of the keyboard interrupt\n            # (and we do nothing)\n            pass\n\n        raise KeyboardInterrupt\n\n\ndef bark_alpha(sr):\n    return 0.8517*np.sqrt(np.arctan(0.06583*sr/1000.0))-0.1916\n\ndef erb_alpha(sr):\n    return 0.5941*np.sqrt(np.arctan(0.1418*sr/1000.0))+0.03237\n\ndef post_filter(mgc_file_in, mgc_file_out, mgc_dim, pf_coef, fw_coef, co_coef, fl_coef, gen_dir, cfg):\n\n    SPTK = cfg.SPTK\n\n    line = ""echo 1 1 ""\n    for i in range(2, mgc_dim):\n        line = line + str(pf_coef) + "" ""\n\n    run_process(\'{line} | {x2x} +af > {weight}\'\n                .format(line=line, x2x=SPTK[\'X2X\'], weight=os.path.join(gen_dir, \'weight\')))\n\n    run_process(\'{freqt} -m {order} -a {fw} -M {co} -A 0 < {mgc} | {c2acr} -m {co} -M 0 -l {fl} > {base_r0}\'\n                .format(freqt=SPTK[\'FREQT\'], order=mgc_dim-1, fw=fw_coef, co=co_coef, mgc=mgc_file_in, c2acr=SPTK[\'C2ACR\'], fl=fl_coef, base_r0=mgc_file_in+\'_r0\'))\n\n    run_process(\'{vopr} -m -n {order} < {mgc} {weight} | {freqt} -m {order} -a {fw} -M {co} -A 0 | {c2acr} -m {co} -M 0 -l {fl} > {base_p_r0}\'\n                .format(vopr=SPTK[\'VOPR\'], order=mgc_dim-1, mgc=mgc_file_in, weight=os.path.join(gen_dir, \'weight\'),\n                        freqt=SPTK[\'FREQT\'], fw=fw_coef, co=co_coef,\n                        c2acr=SPTK[\'C2ACR\'], fl=fl_coef, base_p_r0=mgc_file_in+\'_p_r0\'))\n\n    run_process(\'{vopr} -m -n {order} < {mgc} {weight} | {mc2b} -m {order} -a {fw} | {bcp} -n {order} -s 0 -e 0 > {base_b0}\'\n                .format(vopr=SPTK[\'VOPR\'], order=mgc_dim-1, mgc=mgc_file_in, weight=os.path.join(gen_dir, \'weight\'),\n                        mc2b=SPTK[\'MC2B\'], fw=fw_coef,\n                        bcp=SPTK[\'BCP\'], base_b0=mgc_file_in+\'_b0\'))\n\n    run_process(\'{vopr} -d < {base_r0} {base_p_r0} | {sopr} -LN -d 2 | {vopr} -a {base_b0} > {base_p_b0}\'\n                .format(vopr=SPTK[\'VOPR\'], base_r0=mgc_file_in+\'_r0\', base_p_r0=mgc_file_in+\'_p_r0\',\n                        sopr=SPTK[\'SOPR\'],\n                        base_b0=mgc_file_in+\'_b0\', base_p_b0=mgc_file_in+\'_p_b0\'))\n\n    run_process(\'{vopr} -m -n {order} < {mgc} {weight} | {mc2b} -m {order} -a {fw} | {bcp} -n {order} -s 1 -e {order} | {merge} -n {order2} -s 0 -N 0 {base_p_b0} | {b2mc} -m {order} -a {fw} > {base_p_mgc}\'\n                .format(vopr=SPTK[\'VOPR\'], order=mgc_dim-1, mgc=mgc_file_in, weight=os.path.join(gen_dir, \'weight\'),\n                        mc2b=SPTK[\'MC2B\'],  fw=fw_coef,\n                        bcp=SPTK[\'BCP\'],\n                        merge=SPTK[\'MERGE\'], order2=mgc_dim-2, base_p_b0=mgc_file_in+\'_p_b0\',\n                        b2mc=SPTK[\'B2MC\'], base_p_mgc=mgc_file_out))\n\n    return\n\ndef wavgen_straight_type_vocoder(gen_dir, file_id_list, cfg, logger):\n    \'\'\'\n    Waveform generation with STRAIGHT or WORLD vocoders.\n    (whose acoustic parameters are: mgc, bap, and lf0)\n    \'\'\'\n\n    SPTK     = cfg.SPTK\n#    NND      = cfg.NND\n    STRAIGHT = cfg.STRAIGHT\n    WORLD    = cfg.WORLD\n\n    ## to be moved\n    pf_coef = cfg.pf_coef\n    if isinstance(cfg.fw_alpha, str):\n        if cfg.fw_alpha==\'Bark\':\n            fw_coef = bark_alpha(cfg.sr)\n        elif cfg.fw_alpha==\'ERB\':\n            fw_coef = bark_alpha(cfg.sr)\n        else:\n            raise ValueError(\'cfg.fw_alpha=\'+cfg.fw_alpha+\' not implemented, the frequency warping coefficient ""fw_coef"" cannot be deduced.\')\n    else:\n        fw_coef = cfg.fw_alpha\n    co_coef = cfg.co_coef\n    fl_coef = cfg.fl\n\n    if cfg.apply_GV:\n        io_funcs = BinaryIOCollection()\n\n        logger.info(\'loading global variance stats from %s\' % (cfg.GV_dir))\n\n        ref_gv_mean_file = os.path.join(cfg.GV_dir, \'ref_gv.mean\')\n        gen_gv_mean_file = os.path.join(cfg.GV_dir, \'gen_gv.mean\')\n        ref_gv_std_file  = os.path.join(cfg.GV_dir, \'ref_gv.std\')\n        gen_gv_std_file  = os.path.join(cfg.GV_dir, \'gen_gv.std\')\n\n        ref_gv_mean, frame_number = io_funcs.load_binary_file_frame(ref_gv_mean_file, 1)\n        gen_gv_mean, frame_number = io_funcs.load_binary_file_frame(gen_gv_mean_file, 1)\n        ref_gv_std, frame_number = io_funcs.load_binary_file_frame(ref_gv_std_file, 1)\n        gen_gv_std, frame_number = io_funcs.load_binary_file_frame(gen_gv_std_file, 1)\n\n    counter=1\n    max_counter = len(file_id_list)\n\n    for filename in file_id_list:\n\n        logger.info(\'creating waveform for %4d of %4d: %s\' % (counter,max_counter,filename) )\n        counter=counter+1\n        base   = filename\n        files = {\'sp\'  : base + cfg.sp_ext,\n                 \'mgc\' : base + cfg.mgc_ext,\n                 \'f0\'  : base + \'.f0\',\n                 \'lf0\' : base + cfg.lf0_ext,\n                 \'ap\'  : base + \'.ap\',\n                 \'bap\' : base + cfg.bap_ext,\n                 \'wav\' : base + \'.wav\'}\n\n        mgc_file_name = files[\'mgc\']\n        bap_file_name = files[\'bap\']\n\n        cur_dir = os.getcwd()\n        os.chdir(gen_dir)\n\n        ### post-filtering\n        if cfg.do_post_filtering:\n\n            mgc_file_name = files[\'mgc\']+\'_p_mgc\'\n            post_filter(files[\'mgc\'], mgc_file_name, cfg.mgc_dim, pf_coef, fw_coef, co_coef, fl_coef, gen_dir, cfg)\n\n        if cfg.vocoder_type == ""STRAIGHT"" and cfg.apply_GV:\n            gen_mgc, frame_number = io_funcs.load_binary_file_frame(mgc_file_name, cfg.mgc_dim)\n\n            gen_mu  = np.reshape(np.mean(gen_mgc, axis=0), (-1, 1))\n            gen_std = np.reshape(np.std(gen_mgc, axis=0), (-1, 1))\n\n            local_gv = (ref_gv_std/gen_gv_std) * (gen_std - gen_gv_mean) + ref_gv_mean;\n\n            enhanced_mgc = np.repeat(local_gv, frame_number, 1).T / np.repeat(gen_std, frame_number, 1).T * (gen_mgc - np.repeat(gen_mu, frame_number, 1).T) + np.repeat(gen_mu, frame_number, 1).T;\n\n            new_mgc_file_name = files[\'mgc\']+\'_p_mgc\'\n            io_funcs.array_to_binary_file(enhanced_mgc, new_mgc_file_name)\n\n            mgc_file_name = files[\'mgc\']+\'_p_mgc\'\n\n        if cfg.do_post_filtering and cfg.apply_GV:\n            logger.critical(\'Both smoothing techniques together can\\\'t be applied!!\\n\' )\n            raise\n\n        ###mgc to sp to wav\n        if cfg.vocoder_type == \'STRAIGHT\':\n            run_process(\'{mgc2sp} -a {alpha} -g 0 -m {order} -l {fl} -o 2 {mgc} > {sp}\'\n                        .format(mgc2sp=SPTK[\'MGC2SP\'], alpha=cfg.fw_alpha, order=cfg.mgc_dim-1, fl=cfg.fl, mgc=mgc_file_name, sp=files[\'sp\']))\n            run_process(\'{sopr} -magic -1.0E+10 -EXP -MAGIC 0.0 {lf0} > {f0}\'.format(sopr=SPTK[\'SOPR\'], lf0=files[\'lf0\'], f0=files[\'f0\']))\n            run_process(\'{x2x} +fa {f0} > {f0a}\'.format(x2x=SPTK[\'X2X\'], f0=files[\'f0\'], f0a=files[\'f0\'] + \'.a\'))\n\n            if cfg.use_cep_ap:\n                run_process(\'{mgc2sp} -a {alpha} -g 0 -m {order} -l {fl} -o 0 {bap} > {ap}\'\n                            .format(mgc2sp=SPTK[\'MGC2SP\'], alpha=cfg.fw_alpha, order=cfg.bap_dim-1, fl=cfg.fl, bap=files[\'bap\'], ap=files[\'ap\']))\n            else:\n                run_process(\'{bndap2ap} {bap} > {ap}\'\n                             .format(bndap2ap=STRAIGHT[\'BNDAP2AP\'], bap=files[\'bap\'], ap=files[\'ap\']))\n\n            run_process(\'{synfft} -f {sr} -spec -fftl {fl} -shift {shift} -sigp 1.2 -cornf 4000 -float -apfile {ap} {f0a} {sp} {wav}\'\n                        .format(synfft=STRAIGHT[\'SYNTHESIS_FFT\'], sr=cfg.sr, fl=cfg.fl, shift=cfg.shift, ap=files[\'ap\'], f0a=files[\'f0\']+\'.a\', sp=files[\'sp\'], wav=files[\'wav\']))\n\n            run_process(\'rm -f {sp} {f0} {f0a} {ap}\'\n                        .format(sp=files[\'sp\'],f0=files[\'f0\'],f0a=files[\'f0\']+\'.a\',ap=files[\'ap\']))\n        elif cfg.vocoder_type == \'WORLD\':\n\n            run_process(\'{sopr} -magic -1.0E+10 -EXP -MAGIC 0.0 {lf0} | {x2x} +fd > {f0}\'.format(sopr=SPTK[\'SOPR\'], lf0=files[\'lf0\'], x2x=SPTK[\'X2X\'], f0=files[\'f0\']))\n\n            run_process(\'{sopr} -c 0 {bap} | {x2x} +fd > {ap}\'.format(sopr=SPTK[\'SOPR\'],bap=files[\'bap\'],x2x=SPTK[\'X2X\'],ap=files[\'ap\']))\n\n            ### If using world v2, please comment above line and uncomment this\n            #run_process(\'{mgc2sp} -a {alpha} -g 0 -m {order} -l {fl} -o 0 {bap} | {sopr} -d 32768.0 -P | {x2x} +fd > {ap}\'\n            #            .format(mgc2sp=SPTK[\'MGC2SP\'], alpha=cfg.fw_alpha, order=cfg.bap_dim, fl=cfg.fl, bap=bap_file_name, sopr=SPTK[\'SOPR\'], x2x=SPTK[\'X2X\'], ap=files[\'ap\']))\n\n            run_process(\'{mgc2sp} -a {alpha} -g 0 -m {order} -l {fl} -o 2 {mgc} | {sopr} -d 32768.0 -P | {x2x} +fd > {sp}\'\n                        .format(mgc2sp=SPTK[\'MGC2SP\'], alpha=cfg.fw_alpha, order=cfg.mgc_dim-1, fl=cfg.fl, mgc=mgc_file_name, sopr=SPTK[\'SOPR\'], x2x=SPTK[\'X2X\'], sp=files[\'sp\']))\n\n            run_process(\'{synworld} {fl} {sr} {f0} {sp} {ap} {wav}\'\n                         .format(synworld=WORLD[\'SYNTHESIS\'], fl=cfg.fl, sr=cfg.sr, f0=files[\'f0\'], sp=files[\'sp\'], ap=files[\'ap\'], wav=files[\'wav\']))\n\n            run_process(\'rm -f {ap} {sp} {f0}\'.format(ap=files[\'ap\'],sp=files[\'sp\'],f0=files[\'f0\']))\n\n        os.chdir(cur_dir)\n\n\ndef wavgen_magphase(gen_dir, file_id_list, cfg, logger):\n\n    # Import MagPhase and libraries:\n    sys.path.append(cfg.magphase_bindir)\n    import libutils as lu\n    import libaudio as la\n    import magphase as mp\n\n    nfiles = len(file_id_list)\n    for nxf in xrange(nfiles):\n        filename_token = file_id_list[nxf]\n        logger.info(\'Creating waveform for %4d of %4d: %s\' % (nxf+1, nfiles, filename_token))\n\n        for pf_type in cfg.magphase_pf_type:\n            gen_wav_dir = os.path.join(gen_dir + \'_wav_pf_\' + pf_type)\n            lu.mkdir(gen_wav_dir)\n            mp.synthesis_from_acoustic_modelling(gen_dir, filename_token, gen_wav_dir, cfg.mag_dim, cfg.real_dim,\n                                                            cfg.sr, pf_type=pf_type, b_const_rate=cfg.magphase_const_rate)\n\n    return\n\ndef generate_wav(gen_dir, file_id_list, cfg):\n\n    logger = logging.getLogger(""wav_generation"")\n\n    ## STRAIGHT or WORLD vocoders:\n    if (cfg.vocoder_type==\'STRAIGHT\') or (cfg.vocoder_type==\'WORLD\'):\n        wavgen_straight_type_vocoder(gen_dir, file_id_list, cfg, logger)\n\n    ## MagPhase Vocoder:\n    elif cfg.vocoder_type==\'MAGPHASE\':\n        wavgen_magphase(gen_dir, file_id_list, cfg, logger)\n\n    # Add your favorite vocoder here.\n\n    # If vocoder is not supported:\n    else:\n        logger.critical(\'The vocoder %s is not supported yet!\\n\' % cfg.vocoder_type )\n        raise\n\n    return'"
src/utils/learn_rates.py,0,"b""################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n\nimport numpy\n\nclass LearningRate(object):\n\n    def __init__(self):\n        '''constructor'''\n\n    def get_rate(self):\n        pass\n\n    def get_next_rate(self, current_error):\n        pass\n\nclass LearningRateConstant(LearningRate):\n\n    def __init__(self, learning_rate = 0.08, epoch_num = 20):\n\n        self.learning_rate = learning_rate\n        self.epoch = 1\n        self.epoch_num = epoch_num\n        self.rate = learning_rate\n\n    def get_rate(self):\n        return self.rate\n\n    def get_next_rate(self, current_error):\n\n        if ( self.epoch >=  self.epoch_num):\n            self.rate = 0.0\n        else:\n            self.rate = self.learning_rate\n        self.epoch += 1\n\n        return self.rate\n\nclass LearningRateExpDecay(LearningRate):\n\n    def __init__(self, start_rate = 0.08, scale_by = 0.5,\n                 min_derror_decay_start = 0.05, min_derror_stop = 0.05, init_error = 100,\n                 decay=False, min_epoch_decay_start=15, zero_rate = 0.0):\n\n        self.start_rate = start_rate\n        self.init_error = init_error\n\n        self.rate = start_rate\n        self.scale_by = scale_by\n        self.min_derror_decay_start = min_derror_decay_start\n        self.min_derror_stop = min_derror_stop\n        self.lowest_error = init_error\n\n        self.epoch = 1\n        self.decay = decay\n        self.zero_rate = zero_rate\n\n        self.min_epoch_decay_start = min_epoch_decay_start\n\n\n    def get_rate(self):\n        return self.rate\n\n    def get_next_rate(self, current_error):\n        diff_error = 0.0\n        diff_error = self.lowest_error - current_error\n\n        if (current_error < self.lowest_error):\n            self.lowest_error = current_error\n\n        if (self.decay):\n            if (diff_error < self.min_derror_stop):\n                self.rate = 0.0\n            else:\n                self.rate *= self.scale_by\n        else:\n            if ((diff_error < self.min_derror_decay_start) and (self.epoch > self.min_epoch_decay_start)):\n                self.decay = True\n                self.rate *= self.scale_by\n\n        self.epoch += 1\n        return self.rate\n\n\nclass LearningMinLrate(LearningRate):\n\n    def __init__(self, start_rate = 0.08, scale_by = 0.5,\n                 min_lrate_stop = 0.0002, init_error = 100,\n                 decay=False, min_epoch_decay_start=15):\n\n        self.start_rate = start_rate\n        self.init_error = init_error\n\n        self.rate = start_rate\n        self.scale_by = scale_by\n        self.max_epochs = max_epochs\n        self.min_lrate_stop = min_lrate_stop\n        self.lowest_error = init_error\n\n        self.epoch = 1\n        self.decay = decay\n        self.min_epoch_decay_start = min_epoch_decay_start\n\n    def get_rate(self):\n        return self.rate\n\n    def get_next_rate(self, current_error):\n        diff_error = 0.0\n\n        diff_error = self.lowest_error - current_error\n\n        if (current_error < self.lowest_error):\n            self.lowest_error = current_error\n\n        if (self.decay):\n            if (self.rate < self.min_lrate_stop):\n                self.rate = 0.0\n            else:\n                self.rate *= self.scale_by\n        else:\n            if (self.epoch >= self.min_epoch_decay_start):\n                self.decay = True\n                self.rate *= self.scale_by\n\n        self.epoch += 1\n        return self.rate\n\nclass   ExpDecreaseLearningRate(object):\n    def __init__(self, start_rate = 0.02, end_rate = 0.001, maximum_epoch = 5):\n        self.start_rate = start_rate\n        self.end_rate = end_rate\n        self.maximum_epoch = maximum_epoch\n\n        self.rate_diff = self.start_rate - self.end_rate\n\n        self.decrease_ratio = numpy.zeros((1, maximum_epoch+1))\n        for i in range(maximum_epoch):\n            self.decrease_ratio[0, i+1] = maximum_epoch - i\n\n        self.decrease_ratio = numpy.exp(self.decrease_ratio)\n        self.decrease_ratio /= numpy.sum(self.decrease_ratio)\n\n        self.decrease_ratio[0, 0] = 1.0\n\n    def get_rate(self, epoch):\n\n        if epoch < 0:\n            epoch = 0\n\n        current_rate = self.end_rate\n        if epoch <= self.maximum_epoch:\n            current_rate = self.end_rate + self.decrease_ratio[0, epoch] * self.rate_diff\n\n        return  float(current_rate)\n"""
src/utils/providers.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\nimport os, sys\nimport numpy, theano, random\nfrom io_funcs.binary_io import BinaryIOCollection\nimport logging\nfrom frontend.label_normalisation import HTSLabelNormalisation\n\nclass ListDataProvider(object):\n    """""" This class provides an interface to load data into CPU/GPU memory utterance by utterance or block by block.\n\n    In speech synthesis, usually we are not able to load all the training data/evaluation data into RAMs, we will do the following three steps:\n\n    - Step 1: a data provide will load part of the data into a buffer\n\n    - Step 2: training a DNN by using the data from the buffer\n\n    - Step 3: Iterate step 1 and 2 until all the data are used for DNN training. Until now, one epoch of DNN training is finished.\n\n    The utterance-by-utterance data loading will be useful when sequential training is used, while block-by-block loading will be used when the order of frames is not important.\n\n    This provide assumes binary format with float32 precision without any header (e.g. HTK header).\n\n    """"""\n    def __init__(self, x_file_list, y_file_list, dur_file_list=None, n_ins=0, n_outs=0, buffer_size=500000, sequential=False, network_type=None, shuffle=False):\n        """"""Initialise a data provider\n\n        :param x_file_list: list of file names for the input files to DNN\n        :type x_file_list: python list\n        :param y_file_list: list of files for the output files to DNN\n        :param n_ins: the dimensionality for input feature\n        :param n_outs: the dimensionality for output features\n        :param buffer_size: the size of the buffer, indicating the number of frames in the buffer. The value depends on the memory size of RAM/GPU.\n        :param shuffle: True/False. To indicate whether the file list will be shuffled. When loading data block by block, the data in the buffer will be shuffle no matter this value is True or False.\n        """"""\n\n        self.logger = logging.getLogger(""ListDataProvider"")\n\n        self.n_ins = n_ins\n        self.n_outs = n_outs\n\n        self.buffer_size = buffer_size\n\n        self.sequential = sequential\n        self.network_type = network_type\n\n        self.rnn_batch_training = False\n        self.reshape_io = False\n\n        #remove potential empty lines and end of line signs\n\n        try:\n            assert len(x_file_list) > 0\n        except AssertionError:\n            self.logger.critical(\'first list is empty\')\n            raise\n\n        try:\n            assert len(y_file_list) > 0\n        except AssertionError:\n            self.logger.critical(\'second list is empty\')\n            raise\n\n        try:\n            assert len(x_file_list) == len(y_file_list)\n        except AssertionError:\n            self.logger.critical(\'two lists are of differing lengths: %d versus %d\',len(x_file_list),len(y_file_list))\n            raise\n\n        if dur_file_list:\n            try:\n                assert len(x_file_list) == len(dur_file_list)\n            except AssertionError:\n                self.logger.critical(\'two lists are of differing lengths: %d versus %d\',len(x_file_list),len(y_file_list))\n                raise\n\n        self.x_files_list = x_file_list\n        self.y_files_list = y_file_list\n        self.dur_files_list = dur_file_list\n\n        self.logger.debug(\'first  list of items from ...%s to ...%s\' % (self.x_files_list[0].rjust(20)[-20:],self.x_files_list[-1].rjust(20)[-20:]) )\n        self.logger.debug(\'second list of items from ...%s to ...%s\' % (self.y_files_list[0].rjust(20)[-20:],self.y_files_list[-1].rjust(20)[-20:]) )\n\n        if shuffle:\n            random.seed(271638)\n            random.shuffle(self.x_files_list)\n            random.seed(271638)\n            random.shuffle(self.y_files_list)\n            if self.dur_files_list:\n                random.seed(271638)\n                random.shuffle(self.dur_files_list)\n\n        self.file_index = 0\n        self.list_size = len(self.x_files_list)\n\n        self.remain_data_x = numpy.empty((0, self.n_ins))\n        self.remain_data_y = numpy.empty((0, self.n_outs))\n        self.remain_frame_number = 0\n\n        self.end_reading = False\n\n        self.logger.debug(\'initialised\')\n\n    def __iter__(self):\n        return self\n\n    def reset(self):\n        """"""When all the files in the file list have been used for DNN training, reset the data provider to start a new epoch.\n\n        """"""\n        self.file_index = 0\n        self.end_reading = False\n\n        self.remain_frame_number = 0\n        \n        self.bucket_index = 0\n        self.bucket_file_index = 0\n        self.current_bucket_size = 0\n\n        self.logger.debug(\'reset\')\n\n    def make_shared(self, data_set, data_name):\n        """"""To make data shared for theano implementation. If you want to know why we make it shared, please refer the theano documentation: http://deeplearning.net/software/theano/library/compile/shared.html\n\n        :param data_set: normal data in CPU memory\n        :param data_name: indicate the name of the data (e.g., \'x\', \'y\', etc)\n        :returns: shared dataset -- data_set\n        """"""\n        data_set = theano.shared(numpy.asarray(data_set, dtype=theano.config.floatX), name=data_name, borrow=True)\n\n        return  data_set\n\n    def set_rnn_params(self, training_algo=1, batch_size=25, seq_length=200, merge_size=1, bucket_range=100):\n        # get file lengths\n        self.get_file_lengths()\n\n        # set training algo\n        self.training_algo = training_algo\n\n        # set batch size\n        self.batch_size = batch_size\n\n        # set RNN batch training True\n        self.rnn_batch_training = True\n\n        # set params for each training algo\n        if(self.training_algo == 1):\n            self.merge_size = 1\n        elif(self.training_algo == 2):\n            self.merge_size = 1\n            self.bucket_index = 0\n            self.bucket_file_index = 0\n            self.current_bucket_size = 0\n            self.bucket_range = bucket_range\n            self.x_frame_list = numpy.array(list(self.file_length_dict[\'framenum2utt\'].keys()))\n            self.list_of_buckets = list(range(min(self.x_frame_list), max(self.x_frame_list)+1, self.bucket_range))\n        elif(self.training_algo == 3):\n            self.seq_length = seq_length\n            self.merge_size = merge_size\n        else:\n            self.logger.critical(""Choose training algorithm for batch training with RNNs:"")\n            self.logger.critical(""1. Padding model -- pad utterances with zeros to maximum sequence length"")\n            self.logger.critical(""2. Bucket model  -- form buckets with minimum and maximum sequence length"")\n            self.logger.critical(""3. Split model   -- split utterances to a fixed sequence length"")\n            sys.exit(1)\n\n    def reshape_input_output(self):\n        self.reshape_io = True\n\n    def get_file_lengths(self):\n        io_funcs = BinaryIOCollection()\n\n        self.file_length_dict = {\'framenum2utt\':{}, \'utt2framenum\':{}, \'utt2index\':{}}\n\n        ### read file by file ###\n        while True:\n            if  self.file_index >= self.list_size:\n                self.end_reading = True\n                self.file_index = 0\n                break\n\n            in_features, lab_frame_number = io_funcs.load_binary_file_frame(self.x_files_list[self.file_index], self.n_ins)\n            out_features, out_frame_number = io_funcs.load_binary_file_frame(self.y_files_list[self.file_index], self.n_outs)\n         \n            base_file_name = os.path.basename(self.x_files_list[self.file_index]).split(\'.\')[0]\n            if abs(lab_frame_number - out_frame_number) < 5:    ## we allow small difference here. may not be correct, but sometimes, there is one/two frames difference\n                frame_number = min(lab_frame_number, out_frame_number)\n            else:\n                self.logger.critical(""the number of frames in label and acoustic features are different: %d vs %d (%s)"" %(lab_frame_number, out_frame_number, base_file_name))\n                raise\n\n            if frame_number not in self.file_length_dict[\'framenum2utt\']:\n                self.file_length_dict[\'framenum2utt\'][frame_number] = [base_file_name]\n            else:\n                self.file_length_dict[\'framenum2utt\'][frame_number].append(base_file_name)\n\n            self.file_length_dict[\'utt2framenum\'][base_file_name] = frame_number\n            self.file_length_dict[\'utt2index\'][base_file_name] = self.file_index\n            self.file_index += 1\n\n        self.reset()\n\n    def set_seq_length_from_current_batch(self):\n        temp_list = []\n        for indx in range(self.batch_size):\n            if  self.file_index+indx >= self.list_size:\n                break\n            base_file_name = os.path.basename(self.x_files_list[self.file_index+indx]).split(\'.\')[0]\n            temp_list.append(self.file_length_dict[\'utt2framenum\'][base_file_name])\n\n        self.seq_length = max(temp_list)\n\n    def get_next_bucket(self):\n        min_seq_length = self.list_of_buckets[self.bucket_index]\n        max_seq_length = self.list_of_buckets[self.bucket_index] + self.bucket_range\n        \n        current_bucket = self.x_frame_list[(self.x_frame_list >= min_seq_length) & (self.x_frame_list < max_seq_length)]\n        self.current_bucket_list  = sum([self.file_length_dict[\'framenum2utt\'][framenum] for framenum in current_bucket], [])\n        \n        self.bucket_file_index   = 0  \n        self.current_bucket_size = len(self.current_bucket_list)\n        \n        self.seq_length   = max_seq_length\n        self.bucket_index = self.bucket_index + 1\n\n    def set_s2s_division(self, linguistic_feats_file=None, frame_length=4):\n        self.MLU_div = {}\n        in_f = open(linguistic_feats_file, \'r\')\n        for newline in in_f.readlines():\n            temp_list = newline.strip().split()\n            unit  = temp_list[0]\n            feat1 = temp_list[1][1:-1].split(\'-\')\n            feat2 = temp_list[2][1:-1].split(\'-\')\n\n            self.MLU_div[unit] = [int(feat1[0]), int(feat1[1]), int(feat2[0]), int(feat2[1])]\n       \n        syl_length = (self.MLU_div[\'syl\'][1] - self.MLU_div[\'syl\'][0])+ (self.MLU_div[\'syl\'][3] - self.MLU_div[\'syl\'][2])\n        phone_length = (self.MLU_div[\'phone\'][1] - self.MLU_div[\'phone\'][0]) + (self.MLU_div[\'phone\'][3] - self.MLU_div[\'phone\'][2])\n        self.MLU_div[\'length\'] = [0, syl_length, syl_length+phone_length, syl_length+phone_length+frame_length]\n\n        return self.MLU_div\n\n    def load_one_partition(self):\n        if self.sequential == True:\n            if not self.network_type or self.network_type==""RNN"":\n                if self.rnn_batch_training:\n                    shared_set_xy, temp_set_x, temp_set_y = self.load_next_batch()\n                else:\n                    shared_set_xy, temp_set_x, temp_set_y = self.load_next_utterance()\n            elif self.network_type==""CTC"":\n                shared_set_xy, temp_set_x, temp_set_y = self.load_next_utterance_CTC()\n            elif self.network_type==""S2S"":\n                shared_set_xyd, temp_set_x, temp_set_y, temp_set_d, temp_set_af = self.load_next_utterance_S2SML()\n                return  shared_set_xyd, temp_set_x, temp_set_y, temp_set_d, temp_set_af\n            else:\n                logger.critical(""Unknown network type: %s \\n Please use one of the following: DNN, RNN, S2S, CTC\\n"" %(self.network_type))\n                sys.exit(1)\n        else:\n            shared_set_xy, temp_set_x, temp_set_y = self.load_next_partition()\n\n        return  shared_set_xy, temp_set_x, temp_set_y\n\n    def load_next_batch(self):\n        io_funcs = BinaryIOCollection()\n\n        ## set sequence length for batch training \n        if(self.training_algo == 1):\n            # set seq length to maximum seq length from current batch\n            self.set_seq_length_from_current_batch()\n        elif(self.training_algo == 2):\n            # set seq length to maximum seq length from current bucket\n            while not self.current_bucket_size:\n                self.get_next_bucket()\n        elif(self.training_algo == 3):\n            # seq length is set based on default/user configuration \n            pass;\n            \n        temp_set_x = numpy.zeros((self.buffer_size, self.n_ins))\n        temp_set_y = numpy.zeros((self.buffer_size, self.n_outs))\n\n        ### read file by file ###\n        current_index = 0\n        while True:\n            if current_index >= self.buffer_size:\n                print(\'buffer size reached by file index %d\' %(self.file_index))\n                break\n\n            if self.training_algo == 2:\n                # choose utterance from current bucket list\n                base_file_name = self.current_bucket_list[self.bucket_file_index]\n                self.utt_index = self.file_length_dict[\'utt2index\'][base_file_name] \n            else: \n                # choose utterance randomly from current file list \n                #self.utt_index = numpy.random.randint(self.list_size)\n                ## choose utterance in serial order\n                self.utt_index = self.file_index \n                base_file_name = os.path.basename(self.x_files_list[self.utt_index]).split(\'.\')[0]\n\n            in_features, lab_frame_number = io_funcs.load_binary_file_frame(self.x_files_list[self.utt_index], self.n_ins)\n            out_features, out_frame_number = io_funcs.load_binary_file_frame(self.y_files_list[self.utt_index], self.n_outs)\n         \n            frame_number = self.file_length_dict[\'utt2framenum\'][base_file_name]\n\n            temp_set_x[current_index:current_index+frame_number, ] = in_features\n            temp_set_y[current_index:current_index+frame_number, ] = out_features\n            current_index += frame_number\n\n            if((self.file_index+1)%self.merge_size == 0):\n                num_of_samples = int(numpy.ceil(float(current_index)/float(self.seq_length)))\n                current_index = self.seq_length * num_of_samples\n                \n            self.file_index += 1\n            \n            # break for any of the below conditions\n            if self.training_algo == 2:\n                self.bucket_file_index += 1\n                if(self.bucket_file_index >= self.current_bucket_size):\n                    self.current_bucket_size = 0\n                    break;\n                if(self.bucket_file_index%self.batch_size==0):\n                    break;\n            else:  \n                if(self.file_index%self.batch_size==0) or (self.file_index >= self.list_size):\n                    break\n        \n        if  self.file_index >= self.list_size:\n            self.end_reading = True\n            self.file_index = 0\n        \n        num_of_samples = int(numpy.ceil(float(current_index)/float(self.seq_length)))\n\n        temp_set_x = temp_set_x[0: num_of_samples*self.seq_length, ]\n        temp_set_y = temp_set_y[0: num_of_samples*self.seq_length, ]\n        \n        temp_set_x = temp_set_x.reshape(num_of_samples, self.seq_length, self.n_ins)\n        temp_set_y = temp_set_y.reshape(num_of_samples, self.seq_length, self.n_outs)\n\n        shared_set_x = self.make_shared(temp_set_x, \'x\')\n        shared_set_y = self.make_shared(temp_set_y, \'y\')\n\n        shared_set_xy = (shared_set_x, shared_set_y)\n\n        return shared_set_xy, temp_set_x, temp_set_y\n        \n    def load_next_utterance(self):\n        """"""Load the data for one utterance. This function will be called when utterance-by-utterance loading is required (e.g., sequential training).\n\n        """"""\n\n        temp_set_x = numpy.empty((self.buffer_size, self.n_ins))\n        temp_set_y = numpy.empty((self.buffer_size, self.n_outs))\n\n        io_fun = BinaryIOCollection()\n\n        in_features, lab_frame_number = io_fun.load_binary_file_frame(self.x_files_list[self.file_index], self.n_ins)\n        out_features, out_frame_number = io_fun.load_binary_file_frame(self.y_files_list[self.file_index], self.n_outs)\n\n        frame_number = lab_frame_number\n        if abs(lab_frame_number - out_frame_number) < 5:    ## we allow small difference here. may not be correct, but sometimes, there is one/two frames difference\n            if lab_frame_number > out_frame_number:\n                frame_number = out_frame_number\n        else:\n            base_file_name = os.path.basename(self.x_files_list[self.file_index]).split(\'.\')[0]\n            self.logger.critical(""the number of frames in label and acoustic features are different: %d vs %d (%s)"" %(lab_frame_number, out_frame_number, base_file_name))\n            raise\n\n        temp_set_y = out_features[0:frame_number, ]\n        temp_set_x = in_features[0:frame_number, ]\n\n        self.file_index += 1\n\n        if  self.file_index >= self.list_size:\n            self.end_reading = True\n            self.file_index = 0\n       \n        # reshape input-output\n        if self.reshape_io:\n            temp_set_x = numpy.reshape(temp_set_x, (1, temp_set_x.shape[0], self.n_ins))\n            temp_set_y = numpy.reshape(temp_set_y, (1, temp_set_y.shape[0], self.n_outs))\n        \n            temp_set_x = numpy.array(temp_set_x, \'float32\')\n            temp_set_y = numpy.array(temp_set_y, \'float32\')\n\n        shared_set_x = self.make_shared(temp_set_x, \'x\')\n        shared_set_y = self.make_shared(temp_set_y, \'y\')\n\n        shared_set_xy = (shared_set_x, shared_set_y)\n\n        return shared_set_xy, temp_set_x, temp_set_y\n\n    def load_next_utterance_S2S(self):\n        """"""Load the data for one utterance. This function will be called when utterance-by-utterance loading is required (e.g., sequential training).\n\n        """"""\n\n        temp_set_x = numpy.empty((self.buffer_size, self.n_ins))\n        temp_set_y = numpy.empty((self.buffer_size, self.n_outs))\n\n        io_fun = BinaryIOCollection()\n\n        in_features, lab_frame_number = io_fun.load_binary_file_frame(self.x_files_list[self.file_index], self.n_ins)\n        out_features, out_frame_number = io_fun.load_binary_file_frame(self.y_files_list[self.file_index], self.n_outs)\n\n        temp_set_x = in_features[0:lab_frame_number, ]\n        temp_set_y = out_features[0:out_frame_number, ]\n\n        if not self.dur_files_list:\n            dur_frame_number = out_frame_number\n            dur_features = numpy.array([dur_frame_number])\n        else:\n            dur_features, dur_frame_number = io_fun.load_binary_file_frame(self.dur_files_list[self.file_index], 1)\n            assert sum(dur_features) == out_frame_number\n           \n        dur_features = numpy.reshape(dur_features, (-1, ))\n        temp_set_d = dur_features.astype(int)   \n        \n        self.file_index += 1\n\n        if  self.file_index >= self.list_size:\n            self.end_reading = True\n            self.file_index = 0\n\n        shared_set_x = self.make_shared(temp_set_x, \'x\')\n        shared_set_y = self.make_shared(temp_set_y, \'y\')\n        shared_set_d = theano.shared(numpy.asarray(temp_set_d, dtype=\'int32\'), name=\'d\', borrow=True)\n\n        shared_set_xyd = (shared_set_x, shared_set_y, shared_set_d)\n\n        return shared_set_xyd, temp_set_x, temp_set_y, temp_set_d\n\n    def load_next_utterance_S2SML(self):\n        """"""Load the data for one utterance. This function will be called when utterance-by-utterance loading is required (e.g., sequential training).\n        \n        """"""\n        \n        io_fun = BinaryIOCollection()\n\n        in_features, lab_frame_number = io_fun.load_binary_file_frame(self.x_files_list[self.file_index], self.n_ins)\n        out_features, out_frame_number = io_fun.load_binary_file_frame(self.y_files_list[self.file_index], self.n_outs)\n        dur_features, dur_frame_number = io_fun.load_binary_file_frame(self.dur_files_list[self.file_index], 1)\n      \n        ### MLU features sub-division ###\n        temp_set_MLU = in_features[0:lab_frame_number, ]\n        temp_set_y   = out_features[0:out_frame_number, ]\n      \n        temp_set_phone = numpy.concatenate([temp_set_MLU[:, self.MLU_div[\'phone\'][0]: self.MLU_div[\'phone\'][1]], temp_set_MLU[:, self.MLU_div[\'phone\'][2]: self.MLU_div[\'phone\'][3]]], axis = 1)\n        temp_set_syl   = numpy.concatenate([temp_set_MLU[:, self.MLU_div[\'syl\'][0]: self.MLU_div[\'syl\'][1]], temp_set_MLU[:, self.MLU_div[\'syl\'][2]: self.MLU_div[\'syl\'][3]]], axis = 1)\n        temp_set_word  = numpy.concatenate([temp_set_MLU[:, self.MLU_div[\'word\'][0]: self.MLU_div[\'word\'][1]], temp_set_MLU[:, self.MLU_div[\'word\'][2]: self.MLU_div[\'word\'][3] ]], axis = 1)\n        \n        ### duration array sub-division ###\n        dur_features = numpy.reshape(dur_features, (-1, ))\n        temp_set_d   = dur_features.astype(int)   \n        dur_word_syl = temp_set_d[0: -lab_frame_number]    \n        \n        num_ph    = lab_frame_number\n        num_syl   = (numpy.where(numpy.cumsum(dur_word_syl[::-1])==lab_frame_number)[0][0] + 1)\n        num_words = len(dur_word_syl) - num_syl \n        \n        temp_set_dur_phone = temp_set_d[-num_ph:] \n        temp_set_dur_word  = dur_word_syl[0: num_words]\n        temp_set_dur_syl   = dur_word_syl[num_words: ]\n        \n        ### additional feature matrix (syllable+phone+frame=432) ###\n        num_frames = sum(temp_set_dur_phone)\n        temp_set_af = numpy.empty((num_frames, self.MLU_div[\'length\'][-1]))\n        \n        temp_set_af[0: num_syl, self.MLU_div[\'length\'][0]: self.MLU_div[\'length\'][1] ] = temp_set_syl[numpy.cumsum(temp_set_dur_syl)-1]\n        temp_set_af[0: num_ph, self.MLU_div[\'length\'][1]: self.MLU_div[\'length\'][2]] = temp_set_phone\n        \n        ### input word feature matrix ###\n        temp_set_dur_word_segments = numpy.zeros(num_words, dtype=\'int32\')\n        syl_bound = numpy.cumsum(temp_set_dur_word)\n        for indx in xrange(num_words):\n            temp_set_dur_word_segments[indx] = int(sum(temp_set_dur_syl[0: syl_bound[indx]]))\n        temp_set_x = temp_set_word[temp_set_dur_word_segments-1]\n        \n        ### rest of the code similar to S2S ###\n        self.file_index += 1\n\n        if  self.file_index >= self.list_size:\n            self.end_reading = True\n            self.file_index = 0\n\n        shared_set_x  = self.make_shared(temp_set_x, \'x\')\n        shared_set_y  = self.make_shared(temp_set_y, \'y\')\n        shared_set_d  = theano.shared(numpy.asarray(temp_set_d, dtype=\'int32\'), name=\'d\', borrow=True)\n\n        shared_set_xyd = (shared_set_x, shared_set_y, shared_set_d)\n        \n        return shared_set_xyd, temp_set_x, temp_set_y, temp_set_d, temp_set_af\n\n    def load_next_batch_S2S(self):\n        """"""Load the data for one utterance. This function will be called when utterance-by-utterance loading is required (e.g., sequential training).\n        \n        """"""\n\n        temp_set_x = numpy.empty((self.buffer_size, self.n_ins))\n        temp_set_y = numpy.empty((self.buffer_size, self.n_outs))\n        temp_set_d = numpy.empty((self.buffer_size, 1))\n\n        io_fun = BinaryIOCollection()\n\n        lab_start_frame_number = 0\n        lab_end_frame_number   = 0\n\n        out_start_frame_number = 0\n        out_end_frame_number   = 0\n\n        new_x_files_list = self.x_files_list[self.file_index].split(\',\')\n        new_y_files_list = self.y_files_list[self.file_index].split(\',\')\n        new_dur_files_list = self.dur_files_list[self.file_index].split(\',\')\n\n        for new_file_index in xrange(len(new_x_files_list)):\n            in_features, lab_frame_number = io_fun.load_binary_file_frame(new_x_files_list[new_file_index], self.n_ins)\n            out_features, out_frame_number = io_fun.load_binary_file_frame(new_y_files_list[new_file_index], self.n_outs)\n            \n            lab_end_frame_number+=lab_frame_number\n            out_end_frame_number+=out_frame_number\n\n            temp_set_x[lab_start_frame_number: lab_end_frame_number, ] = in_features[0:lab_frame_number, ]\n            temp_set_y[out_start_frame_number: out_end_frame_number, ] = out_features[0:out_frame_number, ]\n            if not self.dur_files_list:\n                dur_frame_number = out_end_frame_number\n                temp_set_d = numpy.array([dur_frame_number])\n            else:\n                dur_features, dur_frame_number = io_fun.load_binary_file_frame(new_dur_files_list[new_file_index], 1)\n                assert sum(dur_features) == out_frame_number\n                temp_set_d[lab_start_frame_number: lab_end_frame_number, ] = dur_features[0:lab_frame_number, ]\n\n            lab_start_frame_number = lab_end_frame_number\n            out_start_frame_number = out_end_frame_number\n\n        temp_set_x = temp_set_x[0:lab_end_frame_number, ]\n        temp_set_y = temp_set_y[0:out_end_frame_number, ]\n\n        temp_set_d = temp_set_d[0:lab_end_frame_number, ]\n        temp_set_d = numpy.reshape(temp_set_d, (-1, ))\n        temp_set_d = temp_set_d.astype(int)   \n        \n        self.file_index += 1\n\n        if  self.file_index >= self.list_size:\n            self.end_reading = True\n            self.file_index = 0\n\n        shared_set_x = self.make_shared(temp_set_x, \'x\')\n        shared_set_y = self.make_shared(temp_set_y, \'y\')\n        shared_set_d = theano.shared(numpy.asarray(temp_set_d, dtype=\'int32\'), name=\'d\', borrow=True)\n\n        shared_set_xyd = (shared_set_x, shared_set_y, shared_set_d)\n\n        return shared_set_xyd, temp_set_x, temp_set_y, temp_set_d\n\n    def load_next_batch_S2SML(self):\n        """"""Load the data for one utterance. This function will be called when utterance-by-utterance loading is required (e.g., sequential training).\n        \n        """"""\n       \n        inp_length = (self.MLU_div[\'word\'][1] - self.MLU_div[\'word\'][0]) + (self.MLU_div[\'word\'][3] - self.MLU_div[\'word\'][2])\n        af_length = self.MLU_div[\'length\'][-1]\n\n        new_temp_set_x  = numpy.empty((self.buffer_size, inp_length))\n        new_temp_set_y  = numpy.empty((self.buffer_size, self.n_outs))\n        new_temp_set_af = numpy.empty((self.buffer_size, af_length))\n        new_temp_set_d  = [numpy.array([], \'int32\'),numpy.array([], \'int32\'),numpy.array([], \'int32\')]\n\n        io_fun = BinaryIOCollection()\n\n        lab_start_frame_number = 0\n        lab_end_frame_number   = 0\n\n        out_start_frame_number = 0\n        out_end_frame_number   = 0\n\n        new_x_files_list = self.x_files_list[self.file_index].split(\',\')\n        new_y_files_list = self.y_files_list[self.file_index].split(\',\')\n        new_dur_files_list = self.dur_files_list[self.file_index].split(\',\')\n\n        for new_file_index in xrange(len(new_x_files_list)):\n            in_features, lab_frame_number = io_fun.load_binary_file_frame(new_x_files_list[new_file_index], self.n_ins)\n            out_features, out_frame_number = io_fun.load_binary_file_frame(new_y_files_list[new_file_index], self.n_outs)\n            dur_features, dur_frame_number = io_fun.load_binary_file_frame(new_dur_files_list[new_file_index], 1)\n            \n            ### MLU features sub-division ###\n            temp_set_MLU = in_features[0:lab_frame_number, ]\n            temp_set_y   = out_features[0:out_frame_number, ]\n        \n            temp_set_phone = numpy.concatenate([temp_set_MLU[:, self.MLU_div[\'phone\'][0]: self.MLU_div[\'phone\'][1]], temp_set_MLU[:, self.MLU_div[\'phone\'][2]: self.MLU_div[\'phone\'][3]]], axis = 1)\n            temp_set_syl   = numpy.concatenate([temp_set_MLU[:, self.MLU_div[\'syl\'][0]: self.MLU_div[\'syl\'][1]], temp_set_MLU[:, self.MLU_div[\'syl\'][2]: self.MLU_div[\'syl\'][3]]], axis = 1)\n            temp_set_word  = numpy.concatenate([temp_set_MLU[:, self.MLU_div[\'word\'][0]: self.MLU_div[\'word\'][1]], temp_set_MLU[:, self.MLU_div[\'word\'][2]: self.MLU_div[\'word\'][3] ]], axis = 1)\n        \n            ### duration array sub-division ###\n            dur_features = numpy.reshape(dur_features, (-1, ))\n            temp_set_d   = dur_features.astype(int)   \n            dur_word_syl = temp_set_d[0: -lab_frame_number]    \n        \n            num_ph    = lab_frame_number\n            num_syl   = (numpy.where(numpy.cumsum(dur_word_syl[::-1])==lab_frame_number)[0][0] + 1)\n            num_words = len(dur_word_syl) - num_syl \n        \n            temp_set_dur_phone = temp_set_d[-num_ph:] \n            temp_set_dur_word  = dur_word_syl[0: num_words]\n            temp_set_dur_syl   = dur_word_syl[num_words: ]\n        \n            ### additional feature matrix (syllable+phone+frame=432) ###\n            num_frames = sum(temp_set_dur_phone)\n            temp_set_af = numpy.empty((num_frames, self.MLU_div[\'length\'][-1]))\n        \n            temp_set_af[0: num_syl, self.MLU_div[\'length\'][0]: self.MLU_div[\'length\'][1] ] = temp_set_syl[numpy.cumsum(temp_set_dur_syl)-1]\n            temp_set_af[0: num_ph, self.MLU_div[\'length\'][1]: self.MLU_div[\'length\'][2]] = temp_set_phone\n        \n            ### input word feature matrix ###\n            temp_set_dur_word_segments = numpy.zeros(num_words, dtype=\'int32\')\n            syl_bound = numpy.cumsum(temp_set_dur_word)\n            for indx in xrange(num_words):\n                temp_set_dur_word_segments[indx] = int(sum(temp_set_dur_syl[0: syl_bound[indx]]))\n            temp_set_x = temp_set_word[temp_set_dur_word_segments-1]\n        \n            ### for batch processing ###\n            lab_end_frame_number+=num_words\n            out_end_frame_number+=out_frame_number\n      \n            new_temp_set_x[lab_start_frame_number: lab_end_frame_number, ] = temp_set_x[0:num_words, ]\n            new_temp_set_y[out_start_frame_number: out_end_frame_number, ] = temp_set_y[0:out_frame_number, ]\n            new_temp_set_af[out_start_frame_number: out_end_frame_number, ] = temp_set_af[0:out_frame_number, ]\n\n            new_temp_set_d[0] = numpy.append(new_temp_set_d[0], temp_set_dur_word)\n            new_temp_set_d[1] = numpy.append(new_temp_set_d[1], temp_set_dur_syl)\n            new_temp_set_d[2] = numpy.append(new_temp_set_d[2], temp_set_dur_phone)\n\n            lab_start_frame_number = lab_end_frame_number\n            out_start_frame_number = out_end_frame_number\n        \n        new_temp_set_x = new_temp_set_x[0:lab_end_frame_number, ]\n        new_temp_set_y = new_temp_set_y[0:out_end_frame_number, ]\n        new_temp_set_af = new_temp_set_af[0:out_end_frame_number, ]\n        \n        new_temp_set_d = numpy.concatenate((new_temp_set_d[0], new_temp_set_d[1], new_temp_set_d[2]))\n        \n        ### rest of the code similar to S2S ###\n        self.file_index += 1\n\n        if  self.file_index >= self.list_size:\n            self.end_reading = True\n            self.file_index = 0\n\n        shared_set_x  = self.make_shared(new_temp_set_x, \'x\')\n        shared_set_y  = self.make_shared(new_temp_set_y, \'y\')\n        shared_set_d  = theano.shared(numpy.asarray(new_temp_set_d, dtype=\'int32\'), name=\'d\', borrow=True)\n\n        shared_set_xyd = (shared_set_x, shared_set_y, shared_set_d)\n        \n        return shared_set_xyd, new_temp_set_x, new_temp_set_y, new_temp_set_d, new_temp_set_af\n\n    def load_next_utterance_CTC(self):\n\n        temp_set_x = numpy.empty((self.buffer_size, self.n_ins))\n        temp_set_y = numpy.empty(self.buffer_size)\n\n        io_fun = BinaryIOCollection()\n\n        in_features, lab_frame_number = io_fun.load_binary_file_frame(self.x_files_list[self.file_index], self.n_ins)\n        out_features, out_frame_number = io_fun.load_binary_file_frame(self.y_files_list[self.file_index], self.n_outs)\n\n        frame_number = lab_frame_number\n        temp_set_x = in_features[0:frame_number, ]\n\n        temp_set_y = numpy.array([self.n_outs])\n        for il in numpy.argmax(out_features, axis=1):\n            temp_set_y = numpy.concatenate((temp_set_y, [il, self.n_outs]), axis=0)\n\n        self.file_index += 1\n\n        if  self.file_index >= self.list_size:\n            self.end_reading = True\n            self.file_index = 0\n\n        shared_set_x = self.make_shared(temp_set_x, \'x\')\n        shared_set_y = theano.shared(numpy.asarray(temp_set_y, dtype=\'int32\'), name=\'y\', borrow=True)\n\n        shared_set_xy = (shared_set_x, shared_set_y)\n\n        return shared_set_xy, temp_set_x, temp_set_y\n\n\n    def load_next_partition(self):\n        """"""Load one block data. The number of frames will be the buffer size set during intialisation.\n\n        """"""\n\n        self.logger.debug(\'loading next partition\')\n\n        temp_set_x = numpy.empty((self.buffer_size, self.n_ins))\n        temp_set_y = numpy.empty((self.buffer_size, self.n_outs))\n        current_index = 0\n\n        ### first check whether there are remaining data from previous utterance\n        if self.remain_frame_number > 0:\n            temp_set_x[current_index:self.remain_frame_number, ] = self.remain_data_x\n            temp_set_y[current_index:self.remain_frame_number, ] = self.remain_data_y\n            current_index += self.remain_frame_number\n\n            self.remain_frame_number = 0\n\n        io_fun = BinaryIOCollection()\n        while True:\n            if current_index >= self.buffer_size:\n                break\n            if  self.file_index >= self.list_size:\n                self.end_reading = True\n                self.file_index = 0\n                break\n\n            in_features, lab_frame_number = io_fun.load_binary_file_frame(self.x_files_list[self.file_index], self.n_ins)\n            out_features, out_frame_number = io_fun.load_binary_file_frame(self.y_files_list[self.file_index], self.n_outs)\n\n            frame_number = lab_frame_number\n            if abs(lab_frame_number - out_frame_number) < 5:    ## we allow small difference here. may not be correct, but sometimes, there is one/two frames difference\n                if lab_frame_number > out_frame_number:\n                    frame_number = out_frame_number\n            else:\n                base_file_name = os.path.basename(self.x_files_list[self.file_index]).split(\'.\')[0]\n                self.logger.critical(""the number of frames in label and acoustic features are different: %d vs %d (%s)"" %(lab_frame_number, out_frame_number, base_file_name))\n                raise\n\n            out_features = out_features[0:frame_number, ]\n            in_features = in_features[0:frame_number, ]\n\n            if current_index + frame_number <= self.buffer_size:\n                temp_set_x[current_index:current_index+frame_number, ] = in_features\n                temp_set_y[current_index:current_index+frame_number, ] = out_features\n\n                current_index = current_index + frame_number\n            else:   ## if current utterance cannot be stored in the block, then leave the remaining part for the next block\n                used_frame_number = self.buffer_size - current_index\n                temp_set_x[current_index:self.buffer_size, ] = in_features[0:used_frame_number, ]\n                temp_set_y[current_index:self.buffer_size, ] = out_features[0:used_frame_number, ]\n                current_index = self.buffer_size\n\n                self.remain_data_x = in_features[used_frame_number:frame_number, ]\n                self.remain_data_y = out_features[used_frame_number:frame_number, ]\n                self.remain_frame_number = frame_number - used_frame_number\n\n            self.file_index += 1\n\n        temp_set_x = temp_set_x[0:current_index, ]\n        temp_set_y = temp_set_y[0:current_index, ]\n\n        numpy.random.seed(271639)\n        numpy.random.shuffle(temp_set_x)\n        numpy.random.seed(271639)\n        numpy.random.shuffle(temp_set_y)\n\n        shared_set_x = self.make_shared(temp_set_x, \'x\')\n        shared_set_y = self.make_shared(temp_set_y, \'y\')\n\n        shared_set_xy = (shared_set_x, shared_set_y)\n#        temp_set_x = self.make_shared(temp_set_x, \'x\')\n#        temp_set_y = self.make_shared(temp_set_y, \'y\')\n\n        return shared_set_xy, temp_set_x, temp_set_y\n\n    def is_finish(self):\n        return self.end_reading\n\n\nclass ListDataProviderWithProjectionIndex(ListDataProvider):\n    \'\'\'\n    Added kwarg index_to_project to __init__\n    \'\'\'\n\n    def __init__(self, x_file_list, y_file_list, n_ins=0, n_outs=0, \\\n            buffer_size = 500000, shuffle=False, index_to_project=1, projection_insize=10000, indexes_only=False):\n        ##ListDataProvider.__init__(x_file_list, \\\n        ##         y_file_list, n_ins=0, n_outs=0, buffer_size = 500000, shuffle=False)\n        super( ListDataProviderWithProjectionIndex, self ).__init__(x_file_list, \\\n                 y_file_list, n_ins=n_ins, n_outs=n_outs, buffer_size=buffer_size, shuffle=shuffle)\n        self.index_to_project = index_to_project\n        self.projection_insize = projection_insize\n        self.indexes_only = indexes_only\n\n    def load_next_partition_with_projection(self):\n\n        shared_set_xy, temp_set_x, temp_set_y = self.load_next_partition()\n\n        if self.indexes_only:\n            temp_set_x, p_indexes = get_unexpanded_projection_inputs(temp_set_x, self.index_to_project, \\\n                                                            self.projection_insize)\n            shared_set_x_proj = theano.shared(p_indexes, name=\'x_proj\', borrow=True)\n        else:\n            temp_set_x, one_hot = expand_projection_inputs(temp_set_x, self.index_to_project, \\\n                                                            self.projection_insize)\n            shared_set_x_proj = self.make_shared(one_hot, \'x_proj\')\n\n        shared_set_x = self.make_shared(temp_set_x, \'x\')\n        shared_set_y = self.make_shared(temp_set_y, \'y\')\n\n        shared_set_xy = (shared_set_x, shared_set_x_proj, shared_set_y)\n\n        if self.indexes_only:\n            return shared_set_xy, temp_set_x, p_indexes, temp_set_y\n        else:\n            return shared_set_xy, temp_set_x, one_hot, temp_set_y\n\n## Put this function at global level so it can be imported for use in dnn_generation\ndef expand_projection_inputs(temp_set_x, index_to_project, projection_insize):\n    ## Turn indexes to words, syllables etc. to one-hot data:\n    m,n = numpy.shape(temp_set_x)\n    projection_indices = temp_set_x[:, index_to_project]\n    #print projection_indices.tolist()\n    assert projection_indices.max() < projection_insize,\'projection_insize is %s but there is an index %s in the data\'%(projection_insize, projection_indices.max())\n    one_hot = numpy.zeros((m, projection_insize))\n\n    ## Used advanced indexing to turn the relevant features on:\n    projection_indices = projection_indices.astype(int) ## check conversion???!?!?!\n    #     print projection_indices.tolist()\n    #     print \'            ^--- proj indices\'\n    #     print\n    one_hot[list(range(m)),projection_indices] = 1.0\n    ## Effectively remove the index from the original data by setting to 0:\n    temp_set_x[:, index_to_project] = 0.0\n    return temp_set_x, one_hot\n\ndef get_unexpanded_projection_inputs(temp_set_x, index_to_project, projection_insize):\n    ## Turn indexes to words, syllables etc. to one-hot data:\n    m,n = numpy.shape(temp_set_x)\n    projection_indices = temp_set_x[:, index_to_project]\n    #print projection_indices.tolist()\n    assert projection_indices.max() < projection_insize,\'projection_insize is %s but there is an index %s in the data\'%(projection_insize, projection_indices.max())\n\n    projection_indices = projection_indices.astype(\'int32\') ## check conversion???!?!?!\n\n    temp_set_x[:, index_to_project] = 0.0\n    return temp_set_x, projection_indices\n'"
src/utils/utils.py,0,"b""################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://github.com/CSTR-Edinburgh/merlin\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\nimport logging\nimport os\n\n\ndef read_file_list(file_name):\n  logger = logging.getLogger('read_file_list')\n\n  file_lists = []\n  fid = open(file_name)\n  for line in fid.readlines():\n    line = line.strip()\n    if len(line) < 1:\n      continue\n    file_lists.append(line)\n  fid.close()\n\n  logger.info('Read file list from %s', file_name)\n  return file_lists\n\n\ndef prepare_file_path_list(file_id_list,\n                           file_dir,\n                           file_extension,\n                           new_dir_switch=True):\n  logger = logging.getLogger('prepare_file_path_list')\n\n  if not os.path.exists(file_dir) and new_dir_switch:\n    os.makedirs(file_dir)\n\n  logger.info('Preparing file_list for %s in dir \\n%s', file_extension,\n              file_dir)\n\n  return [\n      os.path.join(file_dir, file_id + file_extension)\n      for file_id in file_id_list\n  ]\n"""
src/utils/view.py,0,"b""################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n#  quick and dirty utility to print out binary files, for debugging\n\nimport sys\n# import numpy\nfrom io_funcs.binary_io import BinaryIOCollection\n\nif __name__ == '__main__':\n\n    ## shall we read the logging config file from command line?\n    if len(sys.argv) < 3:\n        print('usage: python view.py dimension filename(s)')\n        sys.exit(1)\n\n    dimension = int(sys.argv[1])\n    fnames = sys.argv[2:]\n\n    print(fnames)\n\n    io_funcs = BinaryIOCollection()\n    for f in fnames:\n        features = io_funcs.load_binary_file(f, dimension)\n\n    print(features.shape)\n    # print features\n"""
src/work_in_progress/run_dnn.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n\nimport pickle\nimport gzip\nimport os, sys, errno\nimport time\nimport math\n\n#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)\nimport numpy\n# we need to explicitly import this in some cases, not sure why this doesn\'t get imported with numpy itself\nimport numpy.distutils.__config__\n# and only after that can we import theano\nimport theano\n\nfrom utils.providers import ListDataProvider\n\nfrom frontend.label_normalisation import HTSLabelNormalisation, HTSDurationLabelNormalisation, XMLLabelNormalisation\nfrom frontend.silence_remover import SilenceRemover\nfrom frontend.silence_remover import trim_silence\nfrom frontend.min_max_norm import MinMaxNormalisation\n#from frontend.acoustic_normalisation import CMPNormalisation\nfrom frontend.acoustic_composition import AcousticComposition\nfrom frontend.parameter_generation import ParameterGeneration\n#from frontend.feature_normalisation_base import FeatureNormBase\nfrom frontend.mean_variance_norm import MeanVarianceNorm\n\n# the new class for label composition and normalisation\nfrom frontend.label_composer import LabelComposer\n\nimport configuration\n\nfrom models.dnn import DNN\n#from models.ms_dnn import MultiStreamDNN\n#from models.ms_dnn_gv import MultiStreamDNNGv\n#from models.sdae import StackedDenoiseAutoEncoder\n\nfrom utils.compute_distortion import DistortionComputation, IndividualDistortionComp\nfrom utils.generate import generate_wav\nfrom utils.learn_rates import ExpDecreaseLearningRate\n\n\n#import matplotlib.pyplot as plt\n# our custom logging class that can also plot\n#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot\nfrom logplot.logging_plotting import LoggerPlotter, MultipleSeriesPlot, SingleWeightMatrixPlot\nimport logging # as logging\nimport logging.config\nimport io\n\n\ndef extract_file_id_list(file_list):\n    file_id_list = []\n    for file_name in file_list:\n        file_id = os.path.basename(os.path.splitext(file_name)[0])\n        file_id_list.append(file_id)\n\n    return  file_id_list\n\ndef read_file_list(file_name):\n\n    logger = logging.getLogger(""read_file_list"")\n\n    file_lists = []\n    fid = open(file_name)\n    for line in fid.readlines():\n        line = line.strip()\n        if len(line) < 1:\n            continue\n        file_lists.append(line)\n    fid.close()\n\n    logger.debug(\'Read file list from %s\' % file_name)\n    return  file_lists\n\n\ndef make_output_file_list(out_dir, in_file_lists):\n    out_file_lists = []\n\n    for in_file_name in in_file_lists:\n        file_id = os.path.basename(in_file_name)\n        out_file_name = out_dir + \'/\' + file_id\n        out_file_lists.append(out_file_name)\n\n    return  out_file_lists\n\ndef prepare_file_path_list(file_id_list, file_dir, file_extension, new_dir_switch=True):\n    if not os.path.exists(file_dir) and new_dir_switch:\n        os.makedirs(file_dir)\n    file_name_list = []\n    for file_id in file_id_list:\n        file_name = file_dir + \'/\' + file_id + file_extension\n        file_name_list.append(file_name)\n\n    return  file_name_list\n\n\n\ndef visualize_dnn(dnn):\n\n    layer_num = len(dnn.params) / 2     ## including input and output\n\n    for i in range(layer_num):\n        fig_name = \'Activation weights W\' + str(i)\n        fig_title = \'Activation weights of W\' + str(i)\n        xlabel = \'Neuron index of hidden layer \' + str(i)\n        ylabel = \'Neuron index of hidden layer \' + str(i+1)\n        if i == 0:\n            xlabel = \'Input feature index\'\n        if i == layer_num-1:\n            ylabel = \'Output feature index\'\n\n        logger.create_plot(fig_name, SingleWeightMatrixPlot)\n        plotlogger.add_plot_point(fig_name, fig_name, dnn.params[i*2].get_value(borrow=True).T)\n        plotlogger.save_plot(fig_name, title=fig_name, xlabel=xlabel, ylabel=ylabel)\n\n\ndef train_DNN(train_xy_file_list, valid_xy_file_list, \\\n              nnets_file_name, n_ins, n_outs, ms_outs, hyper_params, buffer_size, plot=False):\n\n    # get loggers for this function\n    # this one writes to both console and file\n    logger = logging.getLogger(""main.train_DNN"")\n    logger.debug(\'Starting train_DNN\')\n\n    if plot:\n        # this one takes care of plotting duties\n        plotlogger = logging.getLogger(""plotting"")\n        # create an (empty) plot of training convergence, ready to receive data points\n        logger.create_plot(\'training convergence\',MultipleSeriesPlot)\n\n    try:\n        assert numpy.sum(ms_outs) == n_outs\n    except AssertionError:\n        logger.critical(\'the summation of multi-stream outputs does not equal to %d\' %(n_outs))\n        raise\n\n    ####parameters#####\n    finetune_lr     = numpy.asarray(hyper_params[\'learning_rate\'],  dtype=\'float32\')\n    training_epochs = int(hyper_params[\'training_epochs\'])\n    batch_size      = int(hyper_params[\'batch_size\'])\n    l1_reg          = float(hyper_params[\'l1_reg\'])\n    l2_reg          = float(hyper_params[\'l2_reg\'])\n#     private_l2_reg  = float(hyper_params[\'private_l2_reg\'])\n    warmup_epoch    = int(hyper_params[\'warmup_epoch\'])\n    momentum        = float(hyper_params[\'momentum\'])\n    warmup_momentum = float(hyper_params[\'warmup_momentum\'])\n\n    use_rprop = int(hyper_params[\'use_rprop\'])\n\n    use_rprop = int(hyper_params[\'use_rprop\'])\n\n    hidden_layers_sizes = hyper_params[\'hidden_layer_size\']\n\n#     stream_weights       = hyper_params[\'stream_weights\']\n#     private_hidden_sizes = hyper_params[\'private_hidden_sizes\']\n\n    buffer_utt_size = buffer_size\n    early_stop_epoch = int(hyper_params[\'early_stop_epochs\'])\n\n    hidden_activation = hyper_params[\'hidden_activation\']\n    output_activation = hyper_params[\'output_activation\']\n\n#     stream_lr_weights = hyper_params[\'stream_lr_weights\']\n#     use_private_hidden = hyper_params[\'use_private_hidden\']\n\n    model_type = hyper_params[\'model_type\']\n\n    ## use a switch to turn on pretraining\n    ## pretraining may not help too much, if this case, we turn it off to save time\n    do_pretraining = hyper_params[\'do_pretraining\']\n    pretraining_epochs = int(hyper_params[\'pretraining_epochs\'])\n    pretraining_lr = float(hyper_params[\'pretraining_lr\'])\n\n\n    buffer_size = int(buffer_size / batch_size) * batch_size\n\n    ###################\n    (train_x_file_list, train_y_file_list) = train_xy_file_list\n    (valid_x_file_list, valid_y_file_list) = valid_xy_file_list\n\n    logger.debug(\'Creating training   data provider\')\n    train_data_reader = ListDataProvider(x_file_list = train_x_file_list, y_file_list = train_y_file_list, n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, shuffle = True)\n\n    logger.debug(\'Creating validation data provider\')\n    valid_data_reader = ListDataProvider(x_file_list = valid_x_file_list, y_file_list = valid_y_file_list, n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, shuffle = False)\n\n    shared_train_set_xy, temp_train_set_x, temp_train_set_y = train_data_reader.load_next_partition()\n    train_set_x, train_set_y = shared_train_set_xy\n    shared_valid_set_xy, temp_valid_set_x, temp_valid_set_y = valid_data_reader.load_next_partition()\n    valid_set_x, valid_set_y = shared_valid_set_xy\n    train_data_reader.reset()\n    valid_data_reader.reset()\n\n    ##temporally we use the training set as pretrain_set_x.\n    ##we need to support any data for pretraining\n    pretrain_set_x = train_set_x\n\n    # numpy random generator\n    numpy_rng = numpy.random.RandomState(123)\n    logger.info(\'building the model\')\n\n\n    dnn_model = None\n    pretrain_fn = None  ## not all the model support pretraining right now\n    train_fn = None\n    valid_fn = None\n    valid_model = None ## valid_fn and valid_model are the same. reserve to computer multi-stream distortion\n    if model_type == \'DNN\':\n        dnn_model = DNN(numpy_rng=numpy_rng, n_ins=n_ins, n_outs = n_outs,\n                        l1_reg = l1_reg, l2_reg = l2_reg,\n                         hidden_layers_sizes = hidden_layers_sizes,\n                          hidden_activation = hidden_activation,\n                          output_activation = output_activation,\n                          use_rprop = use_rprop, rprop_init_update=finetune_lr)\n        train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y), batch_size=batch_size)\n\n    else:\n        logger.critical(\'%s type NN model is not supported!\' %(model_type))\n        raise\n\n    logger.info(\'fine-tuning the %s model\' %(model_type))\n\n    start_time = time.clock()\n\n    best_dnn_model = dnn_model\n    best_validation_loss = sys.float_info.max\n    previous_loss = sys.float_info.max\n\n    early_stop = 0\n    epoch = 0\n    previous_finetune_lr = finetune_lr\n\n    while (epoch < training_epochs):\n        epoch = epoch + 1\n\n        current_momentum = momentum\n        current_finetune_lr = finetune_lr\n        if epoch <= warmup_epoch:\n            current_finetune_lr = finetune_lr\n            current_momentum = warmup_momentum\n        else:\n            current_finetune_lr = previous_finetune_lr * 0.5\n\n        previous_finetune_lr = current_finetune_lr\n\n        train_error = []\n        sub_start_time = time.clock()\n\n        while (not train_data_reader.is_finish()):\n            shared_train_set_xy, temp_train_set_x, temp_train_set_y = train_data_reader.load_next_partition()\n            train_set_x.set_value(numpy.asarray(temp_train_set_x, dtype=theano.config.floatX), borrow=True)\n            train_set_y.set_value(numpy.asarray(temp_train_set_y, dtype=theano.config.floatX), borrow=True)\n\n            n_train_batches = train_set_x.get_value().shape[0] / batch_size\n\n            logger.debug(\'this partition: %d frames (divided into %d batches of size %d)\' %(train_set_x.get_value(borrow=True).shape[0], n_train_batches, batch_size) )\n\n            for minibatch_index in range(n_train_batches):\n                this_train_error = train_fn(minibatch_index, current_finetune_lr, current_momentum)\n                train_error.append(this_train_error)\n\n                if numpy.isnan(this_train_error):\n                    logger.warning(\'training error over minibatch %d of %d was %s\' % (minibatch_index+1,n_train_batches,this_train_error) )\n\n        train_data_reader.reset()\n\n        logger.debug(\'calculating validation loss\')\n        validation_losses = valid_fn()\n        this_validation_loss = numpy.mean(validation_losses)\n\n        # this has a possible bias if the minibatches were not all of identical size\n        # but it should not be siginficant if minibatches are small\n        this_train_valid_loss = numpy.mean(train_error)\n\n        sub_end_time = time.clock()\n\n        loss_difference = this_validation_loss - previous_loss\n\n        logger.info(\'epoch %i, validation error %f, train error %f  time spent %.2f\' %(epoch, this_validation_loss, this_train_valid_loss, (sub_end_time - sub_start_time)))\n        if plot:\n            plotlogger.add_plot_point(\'training convergence\',\'validation set\',(epoch,this_validation_loss))\n            plotlogger.add_plot_point(\'training convergence\',\'training set\',(epoch,this_train_valid_loss))\n            plotlogger.save_plot(\'training convergence\',title=\'Progress of training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n\n        if this_validation_loss < best_validation_loss:\n            best_dnn_model = dnn_model\n            best_validation_loss = this_validation_loss\n            logger.debug(\'validation loss decreased, so saving model\')\n            early_stop = 0\n        else:\n            logger.debug(\'validation loss did not improve\')\n            dbn = best_dnn_model\n            early_stop += 1\n\n        if early_stop >= early_stop_epoch:\n            # too many consecutive epochs without surpassing the best model\n            logger.debug(\'stopping early\')\n            break\n\n        if math.isnan(this_validation_loss):\n            break\n\n        previous_loss = this_validation_loss\n\n    end_time = time.clock()\n    pickle.dump(best_dnn_model, open(nnets_file_name, \'wb\'))\n\n    logger.info(\'overall  training time: %.2fm validation error %f\' % ((end_time - start_time) / 60., best_validation_loss))\n\n    if plot:\n        plotlogger.save_plot(\'training convergence\',title=\'Final training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n\n    return  best_validation_loss\n\n\ndef dnn_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n#    visualize_dnn(dnn_model)\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        features = features.reshape((-1, n_ins))\n        temp_set_x = features.tolist()\n        test_set_x = theano.shared(numpy.asarray(temp_set_x, dtype=theano.config.floatX))\n\n        predicted_parameter = dnn_model.parameter_prediction(test_set_x=test_set_x)\n#        predicted_parameter = test_out()\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\n##generate bottleneck layer as festures\ndef dnn_hidden_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        features = features.reshape((-1, n_ins))\n        temp_set_x = features.tolist()\n        test_set_x = theano.shared(numpy.asarray(temp_set_x, dtype=theano.config.floatX))\n\n        predicted_parameter = dnn_model.generate_top_hidden_layer(test_set_x=test_set_x)\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\n\ndef main_function(cfg):\n\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n    # get another logger to handle plotting duties\n    plotlogger = logging.getLogger(""plotting"")\n\n    # later, we might do this via a handler that is created, attached and configured\n    # using the standard config mechanism of the logging module\n    # but for now we need to do it manually\n    plotlogger.set_plot_path(cfg.plot_dir)\n\n    #### parameter setting########\n    hidden_layers_sizes = cfg.hyper_params[\'hidden_layer_size\']\n\n\n    ####prepare environment\n\n    try:\n        file_id_list = read_file_list(cfg.file_id_scp)\n        logger.debug(\'Loaded file id list from %s\' % cfg.file_id_scp)\n    except IOError:\n        # this means that open(...) threw an error\n        logger.critical(\'Could not load file id list from %s\' % cfg.file_id_scp)\n        raise\n\n    ###total file number including training, development, and testing\n    total_file_number = len(file_id_list)\n\n    data_dir = cfg.data_dir\n\n    nn_cmp_dir       = os.path.join(data_dir, \'nn\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim))\n    nn_cmp_nosil_dir  = os.path.join(data_dir, \'nn_nosil\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim))\n    nn_cmp_norm_dir   = os.path.join(data_dir, \'nn_norm\'  + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim))\n\n    model_dir = os.path.join(cfg.work_dir, \'nnets_model\')\n    gen_dir   = os.path.join(cfg.work_dir, \'gen\')\n\n    in_file_list_dict = {}\n\n    for feature_name in list(cfg.in_dir_dict.keys()):\n        in_file_list_dict[feature_name] = prepare_file_path_list(file_id_list, cfg.in_dir_dict[feature_name], cfg.file_extension_dict[feature_name], False)\n\n    nn_cmp_file_list         = prepare_file_path_list(file_id_list, nn_cmp_dir, cfg.cmp_ext)\n    nn_cmp_nosil_file_list  = prepare_file_path_list(file_id_list, nn_cmp_nosil_dir, cfg.cmp_ext)\n    nn_cmp_norm_file_list    = prepare_file_path_list(file_id_list, nn_cmp_norm_dir, cfg.cmp_ext)\n\n    ###normalisation information\n    norm_info_file = os.path.join(data_dir, \'norm_info\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim) + \'_\' + cfg.output_feature_normalisation + \'.dat\')\n\n    ### normalise input full context label\n\n    # currently supporting two different forms of lingustic features\n    # later, we should generalise this\n\n    if cfg.label_style == \'HTS\':\n        label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name)\n        lab_dim = label_normaliser.dimension + cfg.appended_input_dim\n        logger.info(\'Input label dimension is %d\' % lab_dim)\n        suffix=str(lab_dim)\n    elif cfg.label_style == \'HTS_duration\':\n        label_normaliser = HTSDurationLabelNormalisation(question_file_name=cfg.question_file_name)\n        lab_dim = label_normaliser.dimension ## + cfg.appended_input_dim\n        logger.info(\'Input label dimension is %d\' % lab_dim)\n        suffix=str(lab_dim)\n    # no longer supported - use new ""composed"" style labels instead\n    elif cfg.label_style == \'composed\':\n        # label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)\n        suffix=\'composed\'\n\n    if cfg.process_labels_in_work_dir:\n        label_data_dir = cfg.work_dir\n    else:\n        label_data_dir = data_dir\n\n    # the number can be removed\n    binary_label_dir      = os.path.join(label_data_dir, \'binary_label_\'+suffix)\n    nn_label_dir          = os.path.join(label_data_dir, \'nn_no_silence_lab_\'+suffix)\n    nn_label_norm_dir     = os.path.join(label_data_dir, \'nn_no_silence_lab_norm_\'+suffix)\n#    nn_label_norm_mvn_dir = os.path.join(data_dir, \'nn_no_silence_lab_norm_\'+suffix)\n\n    in_label_align_file_list = prepare_file_path_list(file_id_list, cfg.in_label_align_dir, cfg.lab_ext, False)\n    binary_label_file_list   = prepare_file_path_list(file_id_list, binary_label_dir, cfg.lab_ext)\n    nn_label_file_list       = prepare_file_path_list(file_id_list, nn_label_dir, cfg.lab_ext)\n    nn_label_norm_file_list  = prepare_file_path_list(file_id_list, nn_label_norm_dir, cfg.lab_ext)\n\n    # to do - sanity check the label dimension here?\n\n\n\n    min_max_normaliser = None\n    label_norm_file = \'label_norm_%s.dat\' %(cfg.label_style)\n    label_norm_file = os.path.join(label_data_dir, label_norm_file)\n\n    if cfg.NORMLAB and (cfg.label_style in [\'HTS\', \'HTS_duration\']):\n        # simple HTS labels\n        logger.info(\'preparing label data (input) using standard HTS style labels\')\n        label_normaliser.perform_normalisation(in_label_align_file_list, binary_label_file_list)\n\n        if cfg.label_style == \'HTS\':\n            remover = SilenceRemover(n_cmp = lab_dim, silence_pattern = cfg.silence_pattern)\n            remover.remove_silence(binary_label_file_list, in_label_align_file_list, nn_label_file_list)\n        elif cfg.label_style == \'HTS_duration\':\n            ## don\'t remove silences for duration\n            nn_label_file_list = binary_label_file_list\n\n\n        min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99)\n        ###use only training data to find min-max information, then apply on the whole dataset\n        min_max_normaliser.find_min_max_values(nn_label_file_list[0:cfg.train_file_number])\n        min_max_normaliser.normalise_data(nn_label_file_list, nn_label_norm_file_list)\n\n\n    if cfg.NORMLAB and (cfg.label_style == \'composed\'):\n        # new flexible label preprocessor\n\n        logger.info(\'preparing label data (input) using ""composed"" style labels\')\n        label_composer = LabelComposer()\n        label_composer.load_label_configuration(cfg.label_config_file)\n\n        logger.info(\'Loaded label configuration\')\n        # logger.info(\'%s\' % label_composer.configuration.labels )\n\n        lab_dim=label_composer.compute_label_dimension()\n        logger.info(\'label dimension will be %d\' % lab_dim)\n\n        if cfg.precompile_xpaths:\n            label_composer.precompile_xpaths()\n\n        # there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)\n        # create all the lists of these, ready to pass to the label composer\n\n        in_label_align_file_list = {}\n        for label_style, label_style_required in label_composer.label_styles.items():\n            if label_style_required:\n                logger.info(\'labels of style %s are required - constructing file paths for them\' % label_style)\n                if label_style == \'xpath\':\n                    in_label_align_file_list[\'xpath\'] = prepare_file_path_list(file_id_list, cfg.xpath_label_align_dir, cfg.utt_ext, False)\n                elif label_style == \'hts\':\n                    in_label_align_file_list[\'hts\'] = prepare_file_path_list(file_id_list, cfg.hts_label_align_dir, cfg.lab_ext, False)\n                else:\n                    logger.critical(\'unsupported label style %s specified in label configuration\' % label_style)\n                    raise Exception\n\n            # now iterate through the files, one at a time, constructing the labels for them\n            num_files=len(file_id_list)\n            logger.info(\'the label styles required are %s\' % label_composer.label_styles)\n\n            for i in range(num_files):\n                logger.info(\'making input label features for %4d of %4d\' % (i+1,num_files))\n\n                # iterate through the required label styles and open each corresponding label file\n\n                # a dictionary of file descriptors, pointing at the required files\n                required_labels={}\n\n                for label_style, label_style_required in label_composer.label_styles.items():\n\n                    # the files will be a parallel set of files for a single utterance\n                    # e.g., the XML tree and an HTS label file\n                    if label_style_required:\n                        required_labels[label_style] = open(in_label_align_file_list[label_style][i] , \'r\')\n                        logger.debug(\' opening label file %s\' % in_label_align_file_list[label_style][i])\n\n                logger.debug(\'label styles with open files: %s\' % required_labels)\n                label_composer.make_labels(required_labels,out_file_name=binary_label_file_list[i],fill_missing_values=cfg.fill_missing_values,iterate_over_frames=cfg.iterate_over_frames)\n\n                # now close all opened files\n                for fd in required_labels.values():\n                    fd.close()\n\n\n        # silence removal\n        if cfg.remove_silence_using_binary_labels:\n            silence_feature = 0 ## use first feature in label -- hardcoded for now\n            logger.info(\'Silence removal from label using silence feature: %s\'%(label_composer.configuration.labels[silence_feature]))\n            logger.info(\'Silence will be removed from CMP files in same way\')\n            ## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:\n            trim_silence(binary_label_file_list, nn_label_file_list, lab_dim, \\\n                                binary_label_file_list, lab_dim, silence_feature, percent_to_keep=5)\n        else:\n            logger.info(\'No silence removal done\')\n            # start from the labels we have just produced, not trimmed versions\n            nn_label_file_list = binary_label_file_list\n\n        min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99)\n        ###use only training data to find min-max information, then apply on the whole dataset\n        min_max_normaliser.find_min_max_values(nn_label_file_list[0:cfg.train_file_number])\n        min_max_normaliser.normalise_data(nn_label_file_list, nn_label_norm_file_list)\n\n    if min_max_normaliser != None:\n        ### save label normalisation information for unseen testing labels\n        label_min_vector = min_max_normaliser.min_vector\n        label_max_vector = min_max_normaliser.max_vector\n        label_norm_info = numpy.concatenate((label_min_vector, label_max_vector), axis=0)\n\n        label_norm_info = numpy.array(label_norm_info, \'float32\')\n        fid = open(label_norm_file, \'wb\')\n        label_norm_info.tofile(fid)\n        fid.close()\n        logger.info(\'saved %s vectors to %s\' %(label_min_vector.size, label_norm_file))\n\n\n\n\n    ### make output acoustic data\n    if cfg.MAKECMP:\n        logger.info(\'creating acoustic (output) features\')\n        delta_win = cfg.delta_win #[-0.5, 0.0, 0.5]\n        acc_win = cfg.acc_win     #[1.0, -2.0, 1.0]\n\n        acoustic_worker = AcousticComposition(delta_win = delta_win, acc_win = acc_win)\n        acoustic_worker.prepare_nn_data(in_file_list_dict, nn_cmp_file_list, cfg.in_dimension_dict, cfg.out_dimension_dict)\n\n        if cfg.label_style == \'HTS\':\n\n            if cfg.remove_silence_using_binary_labels:\n                ## do this to get lab_dim:\n                label_composer = LabelComposer()\n                label_composer.load_label_configuration(cfg.label_config_file)\n                lab_dim=label_composer.compute_label_dimension()\n\n                silence_feature = 0 ## use first feature in label -- hardcoded for now\n                logger.info(\'Silence removal from CMP using binary label file\')\n\n                ## overwrite the untrimmed audio with the trimmed version:\n                trim_silence(nn_cmp_file_list, nn_cmp_nosil_file_list, cfg.cmp_dim,\n                                    binary_label_file_list, lab_dim, silence_feature)\n\n            else: ## back off to previous method using HTS labels:\n                remover = SilenceRemover(n_cmp = cfg.cmp_dim, silence_pattern = cfg.silence_pattern)\n                remover.remove_silence(nn_cmp_file_list[0:cfg.train_file_number+cfg.valid_file_number],\n                                       in_label_align_file_list[0:cfg.train_file_number+cfg.valid_file_number],\n                                       nn_cmp_nosil_file_list[0:cfg.train_file_number+cfg.valid_file_number]) # save to itself\n\n\n        elif cfg.label_style == \'HTS_duration\':\n            ## don\'t remove silences for duration\n            nn_cmp_nosil_file_list = nn_cmp_file_list\n            pass\n\n\n    ### save acoustic normalisation information for normalising the features back\n    var_dir   = os.path.join(data_dir, \'var\')\n    if not os.path.exists(var_dir):\n        os.makedirs(var_dir)\n\n    var_file_dict = {}\n    for feature_name in list(cfg.out_dimension_dict.keys()):\n        var_file_dict[feature_name] = os.path.join(var_dir, feature_name + \'_\' + str(cfg.out_dimension_dict[feature_name]))\n\n    ### normalise output acoustic data\n    if cfg.NORMCMP:\n        logger.info(\'normalising acoustic (output) features using method %s\' % cfg.output_feature_normalisation)\n        cmp_norm_info = None\n        if cfg.output_feature_normalisation == \'MVN\':\n            normaliser = MeanVarianceNorm(feature_dimension=cfg.cmp_dim)\n            ###calculate mean and std vectors on the training data, and apply on the whole dataset\n            global_mean_vector = normaliser.compute_mean(nn_cmp_nosil_file_list[0:cfg.train_file_number], 0, cfg.cmp_dim)\n            global_std_vector = normaliser.compute_std(nn_cmp_nosil_file_list[0:cfg.train_file_number], global_mean_vector, 0, cfg.cmp_dim)\n\n            normaliser.feature_normalisation(nn_cmp_nosil_file_list[0:cfg.train_file_number+cfg.valid_file_number],\n                                             nn_cmp_norm_file_list[0:cfg.train_file_number+cfg.valid_file_number])\n            cmp_norm_info = numpy.concatenate((global_mean_vector, global_std_vector), axis=0)\n\n        elif cfg.output_feature_normalisation == \'MINMAX\':\n            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim)\n            global_mean_vector = min_max_normaliser.compute_mean(nn_cmp_nosil_file_list[0:cfg.train_file_number])\n            global_std_vector = min_max_normaliser.compute_std(nn_cmp_nosil_file_list[0:cfg.train_file_number], global_mean_vector)\n\n            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim, min_value = 0.01, max_value = 0.99)\n            min_max_normaliser.find_min_max_values(nn_cmp_nosil_file_list[0:cfg.train_file_number])\n            min_max_normaliser.normalise_data(nn_cmp_nosil_file_list, nn_cmp_norm_file_list)\n\n            cmp_min_vector = min_max_normaliser.min_vector\n            cmp_max_vector = min_max_normaliser.max_vector\n            cmp_norm_info = numpy.concatenate((cmp_min_vector, cmp_max_vector), axis=0)\n\n        else:\n            logger.critical(\'Normalisation type %s is not supported!\\n\' %(cfg.output_feature_normalisation))\n            raise\n\n        cmp_norm_info = numpy.array(cmp_norm_info, \'float32\')\n        fid = open(norm_info_file, \'wb\')\n        cmp_norm_info.tofile(fid)\n        fid.close()\n        logger.info(\'saved %s vectors to %s\' %(cfg.output_feature_normalisation, norm_info_file))\n        # logger.debug(\' value was\\n%s\' % cmp_norm_info)\n\n        feature_index = 0\n        for feature_name in list(cfg.out_dimension_dict.keys()):\n            feature_std_vector = numpy.array(global_std_vector[:,feature_index:feature_index+cfg.out_dimension_dict[feature_name]], \'float32\')\n\n            fid = open(var_file_dict[feature_name], \'w\')\n            feature_std_vector.tofile(fid)\n            fid.close()\n\n            logger.info(\'saved %s variance vector to %s\' %(feature_name, var_file_dict[feature_name]))\n            # logger.debug(\' value was\\n%s\' % feature_std_vector)\n\n            feature_index += cfg.out_dimension_dict[feature_name]\n\n    train_x_file_list = nn_label_norm_file_list[0:cfg.train_file_number]\n    train_y_file_list = nn_cmp_norm_file_list[0:cfg.train_file_number]\n    valid_x_file_list = nn_label_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n    valid_y_file_list = nn_cmp_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n    test_x_file_list  = nn_label_norm_file_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n    test_y_file_list  = nn_cmp_norm_file_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n\n    # we need to know the label dimension before training the DNN\n    # computing that requires us to look at the labels\n    #\n    # currently, there are two ways to do this\n    if cfg.label_style == \'HTS\':\n        label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name)\n        lab_dim = label_normaliser.dimension + cfg.appended_input_dim\n\n    elif cfg.label_style == \'composed\':\n        label_composer = LabelComposer()\n        label_composer.load_label_configuration(cfg.label_config_file)\n        lab_dim=label_composer.compute_label_dimension()\n\n    logger.info(\'label dimension is %d\' % lab_dim)\n\n    combined_model_arch = str(len(hidden_layers_sizes))\n    for hid_size in hidden_layers_sizes:\n        combined_model_arch += \'_\' + str(hid_size)\n\n    nnets_file_name = \'%s/%s_%s_%d_%s_%d.%d.train.%d.model\' \\\n                      %(model_dir, cfg.model_type, cfg.combined_feature_name, int(cfg.multistream_switch),\n                        combined_model_arch, lab_dim, cfg.cmp_dim, cfg.train_file_number)\n\n    ### DNN model training\n    if cfg.TRAINDNN:\n\n        logger.info(\'training DNN\')\n\n        try:\n            os.makedirs(model_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create model directory %s\' % model_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        try:\n            # print   \'start DNN\'\n            train_DNN(train_xy_file_list = (train_x_file_list, train_y_file_list), \\\n                      valid_xy_file_list = (valid_x_file_list, valid_y_file_list), \\\n                      nnets_file_name = nnets_file_name, \\\n                      n_ins = lab_dim, n_outs = cfg.cmp_dim, ms_outs = cfg.multistream_outs, \\\n                      hyper_params = cfg.hyper_params, buffer_size = cfg.buffer_size, plot = cfg.plot)\n        except KeyboardInterrupt:\n            logger.critical(\'train_DNN interrupted via keyboard\')\n            # Could \'raise\' the exception further, but that causes a deep traceback to be printed\n            # which we don\'t care about for a keyboard interrupt. So, just bail out immediately\n            sys.exit(1)\n        except:\n            logger.critical(\'train_DNN threw an exception\')\n            raise\n\n    ### generate parameters from DNN\n    temp_dir_name = \'%s_%s_%d_%d_%d_%d_%d_%d\' \\\n                    %(cfg.model_type, cfg.combined_feature_name, int(cfg.do_post_filtering), \\\n                      cfg.train_file_number, lab_dim, cfg.cmp_dim, \\\n                      len(hidden_layers_sizes), hidden_layers_sizes[0])\n    gen_dir = os.path.join(gen_dir, temp_dir_name)\n\n    gen_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n    test_x_file_list  = nn_label_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n    if cfg.DNNGEN:\n        logger.info(\'generating from DNN\')\n\n        try:\n            os.makedirs(gen_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create generation directory %s\' % gen_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        gen_file_list = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.cmp_ext)\n\n#        dnn_generation(valid_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)\n        dnn_generation(test_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)\n\n        logger.debug(\'denormalising generated output using method %s\' % cfg.output_feature_normalisation)\n\n        fid = open(norm_info_file, \'rb\')\n        cmp_min_max = numpy.fromfile(fid, dtype=numpy.float32)\n        fid.close()\n        cmp_min_max = cmp_min_max.reshape((2, -1))\n        cmp_min_vector = cmp_min_max[0, ]\n        cmp_max_vector = cmp_min_max[1, ]\n\n        if cfg.output_feature_normalisation == \'MVN\':\n            denormaliser = MeanVarianceNorm(feature_dimension = cfg.cmp_dim)\n            denormaliser.feature_denormalisation(gen_file_list, gen_file_list, cmp_min_vector, cmp_max_vector)\n\n        elif cfg.output_feature_normalisation == \'MINMAX\':\n            denormaliser = MinMaxNormalisation(cfg.cmp_dim, min_value = 0.01, max_value = 0.99, min_vector = cmp_min_vector, max_vector = cmp_max_vector)\n            denormaliser.denormalise_data(gen_file_list, gen_file_list)\n        else:\n            logger.critical(\'denormalising method %s is not supported!\\n\' %(cfg.output_feature_normalisation))\n            raise\n\n        ##perform MLPG to smooth parameter trajectory\n        ## lf0 is included, the output features much have vuv.\n        generator = ParameterGeneration(gen_wav_features = cfg.gen_wav_features)\n        generator.acoustic_decomposition(gen_file_list, cfg.cmp_dim, cfg.out_dimension_dict, cfg.file_extension_dict, var_file_dict)\n\n    ### generate wav\n    if cfg.GENWAV:\n        logger.info(\'reconstructing waveform(s)\')\n        generate_wav(gen_dir, gen_file_id_list, cfg)     # generated speech\n#       generate_wav(nn_cmp_dir, gen_file_id_list)  # reference copy synthesis speech\n\n    ### evaluation: calculate distortion\n    if cfg.CALMCD:\n        logger.info(\'calculating MCD\')\n\n        ref_data_dir = os.path.join(data_dir, \'ref_data\')\n\n        ref_mgc_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.mgc_ext)\n        ref_bap_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.bap_ext)\n        ref_lf0_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.lf0_ext)\n\n        in_gen_label_align_file_list = in_label_align_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n        calculator = IndividualDistortionComp()\n\n        spectral_distortion = 0.0\n        bap_mse             = 0.0\n        f0_mse              = 0.0\n        vuv_error           = 0.0\n\n        valid_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n        test_file_id_list  = file_id_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n        if cfg.remove_silence_using_binary_labels:\n            ## get lab_dim:\n            label_composer = LabelComposer()\n            label_composer.load_label_configuration(cfg.label_config_file)\n            lab_dim=label_composer.compute_label_dimension()\n\n            ## use first feature in label -- hardcoded for now\n            silence_feature = 0\n\n            ## Use these to trim silence:\n            untrimmed_test_labels = binary_label_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n\n        if \'mgc\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'mgc\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_mgc_list, cfg.mgc_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.mgc_dim, silence_pattern =  cfg.silence_pattern)\n                remover.remove_silence(in_file_list_dict[\'mgc\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_mgc_list)\n            valid_spectral_distortion = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.mgc_ext, cfg.mgc_dim)\n            test_spectral_distortion  = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.mgc_ext, cfg.mgc_dim)\n            valid_spectral_distortion *= (10 /numpy.log(10)) * numpy.sqrt(2.0)    ##MCD\n            test_spectral_distortion  *= (10 /numpy.log(10)) * numpy.sqrt(2.0)    ##MCD\n\n\n        if \'bap\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'bap\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_bap_list, cfg.bap_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.bap_dim, silence_pattern =  cfg.silence_pattern)\n                remover.remove_silence(in_file_list_dict[\'bap\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_bap_list)\n            valid_bap_mse        = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.bap_ext, cfg.bap_dim)\n            test_bap_mse         = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.bap_ext, cfg.bap_dim)\n            valid_bap_mse = valid_bap_mse / 10.0\n            test_bap_mse  = test_bap_mse / 10.0\n\n        if \'lf0\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'lf0\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_lf0_list, cfg.lf0_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.lf0_dim, silence_pattern = [\'*-#+*\'])\n                remover.remove_silence(in_file_list_dict[\'lf0\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_lf0_list)\n            valid_f0_mse, valid_f0_corr, valid_vuv_error   = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.lf0_ext, cfg.lf0_dim)\n            test_f0_mse , test_f0_corr, test_vuv_error    = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.lf0_ext, cfg.lf0_dim)\n\n        logger.info(\'Develop: DNN -- MCD: %.3f dB; BAP: %.3f dB; F0:- RMSE: %.3f Hz; CORR: %.3f; VUV: %.3f%%\' \\\n                    %(valid_spectral_distortion, valid_bap_mse, valid_f0_mse, valid_f0_corr, valid_vuv_error*100.))\n        logger.info(\'Test   : DNN -- MCD: %.3f dB; BAP: %.3f dB; F0:- RMSE: %.3f Hz; CORR: %.3f; VUV: %.3f%%\' \\\n                    %(test_spectral_distortion , test_bap_mse , test_f0_mse , test_f0_corr, test_vuv_error*100.))\n\nif __name__ == \'__main__\':\n\n\n\n    # these things should be done even before trying to parse the command line\n\n    # create a configuration instance\n    # and get a short name for this instance\n    cfg=configuration.cfg\n\n    # set up logging to use our custom class\n    logging.setLoggerClass(LoggerPlotter)\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n\n    if len(sys.argv) != 2:\n        logger.critical(\'usage: run_dnn.sh [config file name]\')\n        sys.exit(1)\n\n    config_file = sys.argv[1]\n\n    config_file = os.path.abspath(config_file)\n    cfg.configure(config_file)\n\n    if cfg.profile:\n        logger.info(\'profiling is activated\')\n        import cProfile, pstats\n        cProfile.run(\'main_function(cfg)\', \'mainstats\')\n\n        # create a stream for the profiler to write to\n        profiling_output = io.StringIO()\n        p = pstats.Stats(\'mainstats\', stream=profiling_output)\n\n        # print stats to that stream\n        # here we just report the top 10 functions, sorted by total amount of time spent in each\n        p.strip_dirs().sort_stats(\'tottime\').print_stats(10)\n\n        # print the result to the log\n        logger.info(\'---Profiling result follows---\\n%s\' %  profiling_output.getvalue() )\n        profiling_output.close()\n        logger.info(\'---End of profiling result---\')\n\n    else:\n        main_function(cfg)\n\n    sys.exit(0)\n'"
src/work_in_progress/run_dnn_bottleneck.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n\nimport pickle\nimport gzip\nimport os, sys, errno\nimport time\nimport math\n\n#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)\nimport numpy\n# we need to explicitly import this in some cases, not sure why this doesn\'t get imported with numpy itself\nimport numpy.distutils.__config__\n# and only after that can we import theano\nimport theano\n\nfrom utils.providers import ListDataProvider\n\nfrom frontend.label_normalisation import HTSLabelNormalisation, XMLLabelNormalisation\nfrom frontend.silence_remover import SilenceRemover\nfrom frontend.silence_remover import trim_silence\nfrom frontend.min_max_norm import MinMaxNormalisation\n#from frontend.acoustic_normalisation import CMPNormalisation\nfrom frontend.acoustic_composition import AcousticComposition\nfrom frontend.parameter_generation import ParameterGeneration\n#from frontend.feature_normalisation_base import FeatureNormBase\nfrom frontend.mean_variance_norm import MeanVarianceNorm\n\n# the new class for label composition and normalisation\nfrom frontend.label_composer import LabelComposer\n\nimport configuration\n\nfrom models.dnn import DNN\nfrom models.ms_dnn import MultiStreamDNN\nfrom models.ms_dnn_gv import MultiStreamDNNGv\nfrom models.sdae import StackedDenoiseAutoEncoder\n\nfrom utils.compute_distortion import DistortionComputation, IndividualDistortionComp\nfrom utils.generate import generate_wav\nfrom utils.learn_rates import ExpDecreaseLearningRate\n\n\n#import matplotlib.pyplot as plt\n# our custom logging class that can also plot\n#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot\nfrom logplot.logging_plotting import LoggerPlotter, MultipleSeriesPlot, SingleWeightMatrixPlot\nimport logging # as logging\nimport logging.config\nimport io\n\n\ndef extract_file_id_list(file_list):\n    file_id_list = []\n    for file_name in file_list:\n        file_id = os.path.basename(os.path.splitext(file_name)[0])\n        file_id_list.append(file_id)\n\n    return  file_id_list\n\ndef read_file_list(file_name):\n\n    logger = logging.getLogger(""read_file_list"")\n\n    file_lists = []\n    fid = open(file_name)\n    for line in fid.readlines():\n        line = line.strip()\n        if len(line) < 1:\n            continue\n        file_lists.append(line)\n    fid.close()\n\n    logger.debug(\'Read file list from %s\' % file_name)\n    return  file_lists\n\n\ndef make_output_file_list(out_dir, in_file_lists):\n    out_file_lists = []\n\n    for in_file_name in in_file_lists:\n        file_id = os.path.basename(in_file_name)\n        out_file_name = out_dir + \'/\' + file_id\n        out_file_lists.append(out_file_name)\n\n    return  out_file_lists\n\ndef prepare_file_path_list(file_id_list, file_dir, file_extension, new_dir_switch=True):\n    if not os.path.exists(file_dir) and new_dir_switch:\n        os.makedirs(file_dir)\n    file_name_list = []\n    for file_id in file_id_list:\n        file_name = file_dir + \'/\' + file_id + file_extension\n        file_name_list.append(file_name)\n\n    return  file_name_list\n\n\n\ndef visualize_dnn(dnn):\n\n    layer_num = len(dnn.params) / 2     ## including input and output\n\n    for i in range(layer_num):\n        fig_name = \'Activation weights W\' + str(i)\n        fig_title = \'Activation weights of W\' + str(i)\n        xlabel = \'Neuron index of hidden layer \' + str(i)\n        ylabel = \'Neuron index of hidden layer \' + str(i+1)\n        if i == 0:\n            xlabel = \'Input feature index\'\n        if i == layer_num-1:\n            ylabel = \'Output feature index\'\n\n        logger.create_plot(fig_name, SingleWeightMatrixPlot)\n        plotlogger.add_plot_point(fig_name, fig_name, dnn.params[i*2].get_value(borrow=True).T)\n        plotlogger.save_plot(fig_name, title=fig_name, xlabel=xlabel, ylabel=ylabel)\n\n\ndef train_DNN(train_xy_file_list, valid_xy_file_list, \\\n              nnets_file_name, n_ins, n_outs, ms_outs, hyper_params, buffer_size, plot=False):\n\n    # get loggers for this function\n    # this one writes to both console and file\n    logger = logging.getLogger(""main.train_DNN"")\n    logger.debug(\'Starting train_DNN\')\n\n    if plot:\n        # this one takes care of plotting duties\n        plotlogger = logging.getLogger(""plotting"")\n        # create an (empty) plot of training convergence, ready to receive data points\n        logger.create_plot(\'training convergence\',MultipleSeriesPlot)\n\n    try:\n        assert numpy.sum(ms_outs) == n_outs\n    except AssertionError:\n        logger.critical(\'the summation of multi-stream outputs does not equal to %d\' %(n_outs))\n        raise\n\n    ####parameters#####\n    finetune_lr     = float(hyper_params[\'learning_rate\'])\n    training_epochs = int(hyper_params[\'training_epochs\'])\n    batch_size      = int(hyper_params[\'batch_size\'])\n    l1_reg          = float(hyper_params[\'l1_reg\'])\n    l2_reg          = float(hyper_params[\'l2_reg\'])\n    private_l2_reg  = float(hyper_params[\'private_l2_reg\'])\n    warmup_epoch    = int(hyper_params[\'warmup_epoch\'])\n    momentum        = float(hyper_params[\'momentum\'])\n    warmup_momentum = float(hyper_params[\'warmup_momentum\'])\n\n    hidden_layers_sizes = hyper_params[\'hidden_layers_sizes\']\n\n    stream_weights       = hyper_params[\'stream_weights\']\n    private_hidden_sizes = hyper_params[\'private_hidden_sizes\']\n\n    buffer_utt_size = buffer_size\n    early_stop_epoch = int(hyper_params[\'early_stop_epochs\'])\n\n    hidden_activation = hyper_params[\'hidden_activation\']\n    output_activation = hyper_params[\'output_activation\']\n\n    stream_lr_weights = hyper_params[\'stream_lr_weights\']\n    use_private_hidden = hyper_params[\'use_private_hidden\']\n\n    model_type = hyper_params[\'model_type\']\n\n    ## use a switch to turn on pretraining\n    ## pretraining may not help too much, if this case, we turn it off to save time\n    do_pretraining = hyper_params[\'do_pretraining\']\n    pretraining_epochs = int(hyper_params[\'pretraining_epochs\'])\n    pretraining_lr = float(hyper_params[\'pretraining_lr\'])\n\n\n    buffer_size = int(buffer_size / batch_size) * batch_size\n\n    ###################\n    (train_x_file_list, train_y_file_list) = train_xy_file_list\n    (valid_x_file_list, valid_y_file_list) = valid_xy_file_list\n\n    logger.debug(\'Creating training   data provider\')\n    train_data_reader = ListDataProvider(x_file_list = train_x_file_list, y_file_list = train_y_file_list, n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, shuffle = True)\n\n    logger.debug(\'Creating validation data provider\')\n    valid_data_reader = ListDataProvider(x_file_list = valid_x_file_list, y_file_list = valid_y_file_list, n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, shuffle = False)\n\n    shared_train_set_xy, temp_train_set_x, temp_train_set_y = train_data_reader.load_next_partition()\n    train_set_x, train_set_y = shared_train_set_xy\n    shared_valid_set_xy, temp_valid_set_x, temp_valid_set_y = valid_data_reader.load_next_partition()\n    valid_set_x, valid_set_y = shared_valid_set_xy\n    train_data_reader.reset()\n    valid_data_reader.reset()\n\n    ##temporally we use the training set as pretrain_set_x.\n    ##we need to support any data for pretraining\n    pretrain_set_x = train_set_x\n\n    # numpy random generator\n    numpy_rng = numpy.random.RandomState(123)\n    logger.info(\'building the model\')\n\n\n    dnn_model = None\n    pretrain_fn = None  ## not all the model support pretraining right now\n    train_fn = None\n    valid_fn = None\n    valid_model = None ## valid_fn and valid_model are the same. reserve to computer multi-stream distortion\n    if model_type == \'DNN\':\n        dnn_model = DNN(numpy_rng=numpy_rng, n_ins=n_ins, n_outs = n_outs,\n                        l1_reg = l1_reg, l2_reg = l2_reg,\n                         hidden_layers_sizes = hidden_layers_sizes,\n                          hidden_activation = hidden_activation,\n                          output_activation = output_activation)\n        train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y), batch_size=batch_size)\n\n    elif model_type == \'SDAE\':\n        ##basic model is ready.\n        ##if corruption levels is set to zero. it becomes normal autoencoder\n        dnn_model = StackedDenoiseAutoEncoder(numpy_rng=numpy_rng, n_ins=n_ins, n_outs = n_outs,\n                          l1_reg = l1_reg, l2_reg = l2_reg,\n                          hidden_layers_sizes = hidden_layers_sizes)\n\n        if do_pretraining:\n            pretraining_fn = dnn_model.pretraining_functions(pretrain_set_x, batch_size)\n\n        train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y), batch_size=batch_size)\n\n    elif model_type == \'MSDNN\': ##model is ready, but the hyper-parameters are not optimised.\n        dnn_model = MultiStreamDNN(numpy_rng=numpy_rng, n_ins=n_ins, ms_outs=ms_outs,\n                          l1_reg = l1_reg, l2_reg = l2_reg,\n                          hidden_layers_sizes = hidden_layers_sizes,\n                          stream_weights = stream_weights,\n                          hidden_activation = hidden_activation,\n                          output_activation = output_activation)\n\n        train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y),\n                    batch_size=batch_size, lr_weights = stream_lr_weights)\n    elif model_type == \'MSDNN_GV\':  ## not fully ready\n        dnn_model = MultiStreamDNNGv(numpy_rng=numpy_rng, n_ins=n_ins, ms_outs=ms_outs,\n                          l1_reg = l1_reg, l2_reg = l2_reg,\n                          hidden_layers_sizes = hidden_layers_sizes,\n                          stream_weights = stream_weights,\n                          hidden_activation = hidden_activation,\n                          output_activation = output_activation)\n\n        train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y),\n                    batch_size=batch_size, lr_weights = stream_lr_weights)\n    else:\n        logger.critical(\'%s type NN model is not supported!\' %(model_type))\n        raise\n\n    ## if pretraining is supported in one model, add the switch here\n    ## be careful to use autoencoder for pretraining here:\n    ## for SDAE, currently only sigmoid function is supported in the hidden layers, as our input is scaled to [0, 1]\n    ## however, tanh works better and converge fast in finetuning\n    ##\n    ## Will extend this soon...\n    if do_pretraining and model_type == \'SDAE\':\n        logger.info(\'pretraining the %s model\' %(model_type))\n\n        corruption_level = 0.0\n        ## in SDAE we do layer-wise pretraining using autoencoders\n        for i in range(dnn_model.n_layers):\n            for epoch in range(pretraining_epochs):\n                sub_start_time = time.clock()\n\n                pretrain_loss = []\n                while (not train_data_reader.is_finish()):\n                    shared_train_set_xy, temp_train_set_x, temp_train_set_y = train_data_reader.load_next_partition()\n                    pretrain_set_x.set_value(numpy.asarray(temp_train_set_x, dtype=theano.config.floatX), borrow=True)\n\n                    n_train_batches = pretrain_set_x.get_value().shape[0] / batch_size\n\n                    for batch_index in range(n_train_batches):\n                        pretrain_loss.append(pretraining_fn[i](index=batch_index,\n                                                               corruption=corruption_level,\n                                                               learning_rate=pretraining_lr))\n\n                sub_end_time = time.clock()\n                logger.info(\'Pre-training layer %i, epoch %d, cost %s, time spent%.2f\' % (i+1, epoch+1, numpy.mean(pretrain_loss), (sub_end_time - sub_start_time)))\n                train_data_reader.reset()\n\n\n    logger.info(\'fine-tuning the %s model\' %(model_type))\n\n    start_time = time.clock()\n\n    best_dnn_model = dnn_model\n    best_validation_loss = sys.float_info.max\n    previous_loss = sys.float_info.max\n\n    early_stop = 0\n    epoch = 0\n    previous_finetune_lr = finetune_lr\n    while (epoch < training_epochs):\n        epoch = epoch + 1\n\n        current_momentum = momentum\n        current_finetune_lr = finetune_lr\n        if epoch <= warmup_epoch:\n            current_finetune_lr = finetune_lr\n            current_momentum = warmup_momentum\n        else:\n            current_finetune_lr = previous_finetune_lr * 0.5\n\n        previous_finetune_lr = current_finetune_lr\n\n        train_error = []\n        sub_start_time = time.clock()\n\n        while (not train_data_reader.is_finish()):\n            shared_train_set_xy, temp_train_set_x, temp_train_set_y = train_data_reader.load_next_partition()\n            train_set_x.set_value(numpy.asarray(temp_train_set_x, dtype=theano.config.floatX), borrow=True)\n            train_set_y.set_value(numpy.asarray(temp_train_set_y, dtype=theano.config.floatX), borrow=True)\n\n            n_train_batches = train_set_x.get_value().shape[0] / batch_size\n\n            logger.debug(\'this partition: %d frames (divided into %d batches of size %d)\' %(train_set_x.get_value(borrow=True).shape[0], n_train_batches, batch_size) )\n\n            for minibatch_index in range(n_train_batches):\n                this_train_error = train_fn(minibatch_index, current_finetune_lr, current_momentum)\n                train_error.append(this_train_error)\n\n                if numpy.isnan(this_train_error):\n                    logger.warning(\'training error over minibatch %d of %d was %s\' % (minibatch_index+1,n_train_batches,this_train_error) )\n\n        train_data_reader.reset()\n\n        logger.debug(\'calculating validation loss\')\n        validation_losses = valid_fn()\n        this_validation_loss = numpy.mean(validation_losses)\n\n        # this has a possible bias if the minibatches were not all of identical size\n        # but it should not be siginficant if minibatches are small\n        this_train_valid_loss = numpy.mean(train_error)\n\n        sub_end_time = time.clock()\n\n        loss_difference = this_validation_loss - previous_loss\n\n        logger.info(\'epoch %i, validation error %f, train error %f  time spent %.2f\' %(epoch, this_validation_loss, this_train_valid_loss, (sub_end_time - sub_start_time)))\n        if plot:\n            plotlogger.add_plot_point(\'training convergence\',\'validation set\',(epoch,this_validation_loss))\n            plotlogger.add_plot_point(\'training convergence\',\'training set\',(epoch,this_train_valid_loss))\n            plotlogger.save_plot(\'training convergence\',title=\'Progress of training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n\n        if this_validation_loss < best_validation_loss:\n            best_dnn_model = dnn_model\n            best_validation_loss = this_validation_loss\n            logger.debug(\'validation loss decreased, so saving model\')\n            early_stop = 0\n        else:\n            logger.debug(\'validation loss did not improve\')\n            dbn = best_dnn_model\n            early_stop += 1\n\n        if early_stop >= early_stop_epoch:\n            # too many consecutive epochs without surpassing the best model\n            logger.debug(\'stopping early\')\n            break\n\n        if math.isnan(this_validation_loss):\n            break\n\n        previous_loss = this_validation_loss\n\n    end_time = time.clock()\n    pickle.dump(best_dnn_model, open(nnets_file_name, \'wb\'))\n\n    logger.info(\'overall  training time: %.2fm validation error %f\' % ((end_time - start_time) / 60., best_validation_loss))\n\n    if plot:\n        plotlogger.save_plot(\'training convergence\',title=\'Final training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n\n    return  best_validation_loss\n\n\ndef dnn_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n#    visualize_dnn(dbn)\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        features = features.reshape((-1, n_ins))\n        temp_set_x = features.tolist()\n        test_set_x = theano.shared(numpy.asarray(temp_set_x, dtype=theano.config.floatX))\n\n        predicted_parameter = dnn_model.parameter_prediction(test_set_x=test_set_x)\n#        predicted_parameter = test_out()\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\n##generate bottleneck layer as festures\ndef dnn_hidden_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list, bottleneck_index):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        features = features.reshape((-1, n_ins))\n        temp_set_x = features.tolist()\n        test_set_x = theano.shared(numpy.asarray(temp_set_x, dtype=theano.config.floatX))\n\n        predicted_parameter = dnn_model.generate_top_hidden_layer(test_set_x=test_set_x, bn_layer_index=bottleneck_index)\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\n\ndef main_function(cfg):\n\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n    # get another logger to handle plotting duties\n    plotlogger = logging.getLogger(""plotting"")\n\n    # later, we might do this via a handler that is created, attached and configured\n    # using the standard config mechanism of the logging module\n    # but for now we need to do it manually\n    plotlogger.set_plot_path(cfg.plot_dir)\n\n    #### parameter setting########\n    hidden_layers_sizes = cfg.hyper_params[\'hidden_layers_sizes\']\n\n\n    ####prepare environment\n\n    try:\n        file_id_list = read_file_list(cfg.file_id_scp)\n        logger.debug(\'Loaded file id list from %s\' % cfg.file_id_scp)\n    except IOError:\n        # this means that open(...) threw an error\n        logger.critical(\'Could not load file id list from %s\' % cfg.file_id_scp)\n        raise\n\n    ###total file number including training, development, and testing\n    total_file_number = len(file_id_list)\n\n    data_dir = cfg.data_dir\n\n    nn_cmp_dir       = os.path.join(data_dir, \'nn\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim))\n    nn_cmp_norm_dir   = os.path.join(data_dir, \'nn_norm\'  + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim))\n\n    model_dir = os.path.join(cfg.work_dir, \'nnets_model\')\n    gen_dir   = os.path.join(cfg.work_dir, \'gen\')\n\n    in_file_list_dict = {}\n\n    for feature_name in list(cfg.in_dir_dict.keys()):\n        in_file_list_dict[feature_name] = prepare_file_path_list(file_id_list, cfg.in_dir_dict[feature_name], cfg.file_extension_dict[feature_name], False)\n\n    nn_cmp_file_list         = prepare_file_path_list(file_id_list, nn_cmp_dir, cfg.cmp_ext)\n    nn_cmp_norm_file_list    = prepare_file_path_list(file_id_list, nn_cmp_norm_dir, cfg.cmp_ext)\n\n    ###normalisation information\n    norm_info_file = os.path.join(data_dir, \'norm_info\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim) + \'_\' + cfg.output_feature_normalisation + \'.dat\')\n\n    ### normalise input full context label\n\n    # currently supporting two different forms of lingustic features\n    # later, we should generalise this\n\n    if cfg.label_style == \'HTS\':\n        label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name)\n        lab_dim = label_normaliser.dimension\n        logger.info(\'Input label dimension is %d\' % lab_dim)\n        suffix=str(lab_dim)\n    # no longer supported - use new ""composed"" style labels instead\n    elif cfg.label_style == \'composed\':\n        # label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)\n        suffix=\'composed\'\n\n    if cfg.process_labels_in_work_dir:\n        label_data_dir = cfg.work_dir\n    else:\n        label_data_dir = data_dir\n\n    # the number can be removed\n    binary_label_dir      = os.path.join(label_data_dir, \'binary_label_\'+suffix)\n    nn_label_dir          = os.path.join(label_data_dir, \'nn_no_silence_lab_\'+suffix)\n    nn_label_norm_dir     = os.path.join(label_data_dir, \'nn_no_silence_lab_norm_\'+suffix)\n#    nn_label_norm_mvn_dir = os.path.join(data_dir, \'nn_no_silence_lab_norm_\'+suffix)\n\n    in_label_align_file_list = prepare_file_path_list(file_id_list, cfg.in_label_align_dir, cfg.lab_ext, False)\n    binary_label_file_list   = prepare_file_path_list(file_id_list, binary_label_dir, cfg.lab_ext)\n    nn_label_file_list       = prepare_file_path_list(file_id_list, nn_label_dir, cfg.lab_ext)\n    nn_label_norm_file_list  = prepare_file_path_list(file_id_list, nn_label_norm_dir, cfg.lab_ext)\n\n    # to do - sanity check the label dimension here?\n\n\n\n    min_max_normaliser = None\n    label_norm_file = \'label_norm_%s.dat\' %(cfg.label_style)\n    label_norm_file = os.path.join(label_data_dir, label_norm_file)\n\n    if cfg.NORMLAB and (cfg.label_style == \'HTS\'):\n        # simple HTS labels\n        logger.info(\'preparing label data (input) using standard HTS style labels\')\n#        label_normaliser.perform_normalisation(in_label_align_file_list, binary_label_file_list)\n\n#        remover = SilenceRemover(n_cmp = lab_dim, silence_pattern = [\'*-#+*\'])\n#        remover.remove_silence(binary_label_file_list, in_label_align_file_list, nn_label_file_list)\n\n        min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99)\n        ###use only training data to find min-max information, then apply on the whole dataset\n        min_max_normaliser.find_min_max_values(nn_label_file_list[0:cfg.train_file_number])\n        min_max_normaliser.normalise_data(nn_label_file_list, nn_label_norm_file_list)\n\n\n    if cfg.NORMLAB and (cfg.label_style == \'composed\'):\n        # new flexible label preprocessor\n\n        logger.info(\'preparing label data (input) using ""composed"" style labels\')\n        label_composer = LabelComposer()\n        label_composer.load_label_configuration(cfg.label_config_file)\n\n        logger.info(\'Loaded label configuration\')\n        # logger.info(\'%s\' % label_composer.configuration.labels )\n\n        lab_dim=label_composer.compute_label_dimension()\n        logger.info(\'label dimension will be %d\' % lab_dim)\n\n        if cfg.precompile_xpaths:\n            label_composer.precompile_xpaths()\n\n        # there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)\n        # create all the lists of these, ready to pass to the label composer\n\n        in_label_align_file_list = {}\n        for label_style, label_style_required in label_composer.label_styles.items():\n            if label_style_required:\n                logger.info(\'labels of style %s are required - constructing file paths for them\' % label_style)\n                if label_style == \'xpath\':\n                    in_label_align_file_list[\'xpath\'] = prepare_file_path_list(file_id_list, cfg.xpath_label_align_dir, cfg.utt_ext, False)\n                elif label_style == \'hts\':\n                    in_label_align_file_list[\'hts\'] = prepare_file_path_list(file_id_list, cfg.hts_label_align_dir, cfg.lab_ext, False)\n                else:\n                    logger.critical(\'unsupported label style %s specified in label configuration\' % label_style)\n                    raise Exception\n\n            # now iterate through the files, one at a time, constructing the labels for them\n            num_files=len(file_id_list)\n            logger.info(\'the label styles required are %s\' % label_composer.label_styles)\n\n            for i in range(num_files):\n                logger.info(\'making input label features for %4d of %4d\' % (i+1,num_files))\n\n                # iterate through the required label styles and open each corresponding label file\n\n                # a dictionary of file descriptors, pointing at the required files\n                required_labels={}\n\n                for label_style, label_style_required in label_composer.label_styles.items():\n\n                    # the files will be a parallel set of files for a single utterance\n                    # e.g., the XML tree and an HTS label file\n                    if label_style_required:\n                        required_labels[label_style] = open(in_label_align_file_list[label_style][i] , \'r\')\n                        logger.debug(\' opening label file %s\' % in_label_align_file_list[label_style][i])\n\n                logger.debug(\'label styles with open files: %s\' % required_labels)\n                label_composer.make_labels(required_labels,out_file_name=binary_label_file_list[i],fill_missing_values=cfg.fill_missing_values,iterate_over_frames=cfg.iterate_over_frames)\n\n                # now close all opened files\n                for fd in required_labels.values():\n                    fd.close()\n\n\n        # silence removal\n        if cfg.remove_silence_using_binary_labels:\n            silence_feature = 0 ## use first feature in label -- hardcoded for now\n            logger.info(\'Silence removal from label using silence feature: %s\'%(label_composer.configuration.labels[silence_feature]))\n            logger.info(\'Silence will be removed from CMP files in same way\')\n            ## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:\n            trim_silence(binary_label_file_list, nn_label_file_list, lab_dim, \\\n                                binary_label_file_list, lab_dim, silence_feature)\n        else:\n            logger.info(\'No silence removal done\')\n            # start from the labels we have just produced, not trimmed versions\n            nn_label_file_list = binary_label_file_list\n\n        min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99)\n        ###use only training data to find min-max information, then apply on the whole dataset\n        min_max_normaliser.find_min_max_values(nn_label_file_list[0:cfg.train_file_number])\n        min_max_normaliser.normalise_data(nn_label_file_list, nn_label_norm_file_list)\n\n    if min_max_normaliser != None:\n        ### save label normalisation information for unseen testing labels\n        label_min_vector = min_max_normaliser.min_vector\n        label_max_vector = min_max_normaliser.max_vector\n        label_norm_info = numpy.concatenate((label_min_vector, label_max_vector), axis=0)\n\n        label_norm_info = numpy.array(label_norm_info, \'float32\')\n        fid = open(label_norm_file, \'wb\')\n        label_norm_info.tofile(fid)\n        fid.close()\n        logger.info(\'saved %s vectors to %s\' %(label_min_vector.size, label_norm_file))\n\n\n\n\n    ### make output acoustic data\n    if cfg.MAKECMP:\n        logger.info(\'creating acoustic (output) features\')\n        delta_win = [-0.5, 0.0, 0.5]\n        acc_win = [1.0, -2.0, 1.0]\n\n        acoustic_worker = AcousticComposition(delta_win = delta_win, acc_win = acc_win)\n        acoustic_worker.prepare_nn_data(in_file_list_dict, nn_cmp_file_list, cfg.in_dimension_dict, cfg.out_dimension_dict)\n\n        if cfg.remove_silence_using_binary_labels:\n            ## do this to get lab_dim:\n            label_composer = LabelComposer()\n            label_composer.load_label_configuration(cfg.label_config_file)\n            lab_dim=label_composer.compute_label_dimension()\n\n            silence_feature = 0 ## use first feature in label -- hardcoded for now\n            logger.info(\'Silence removal from CMP using binary label file\')\n\n            ## overwrite the untrimmed audio with the trimmed version:\n            trim_silence(nn_cmp_file_list, nn_cmp_file_list, cfg.cmp_dim, \\\n                                binary_label_file_list, lab_dim, silence_feature)\n\n        else: ## back off to previous method using HTS labels:\n            remover = SilenceRemover(n_cmp = cfg.cmp_dim, silence_pattern = [\'*-#+*\'])\n            remover.remove_silence(nn_cmp_file_list, in_label_align_file_list, nn_cmp_file_list) # save to itself\n\n    ### save acoustic normalisation information for normalising the features back\n    var_dir   = os.path.join(data_dir, \'var\')\n    if not os.path.exists(var_dir):\n        os.makedirs(var_dir)\n\n    var_file_dict = {}\n    for feature_name in list(cfg.out_dimension_dict.keys()):\n        var_file_dict[feature_name] = os.path.join(var_dir, feature_name + \'_\' + str(cfg.out_dimension_dict[feature_name]))\n\n    ### normalise output acoustic data\n    if cfg.NORMCMP:\n        logger.info(\'normalising acoustic (output) features using method %s\' % cfg.output_feature_normalisation)\n        cmp_norm_info = None\n        if cfg.output_feature_normalisation == \'MVN\':\n            normaliser = MeanVarianceNorm(feature_dimension=cfg.cmp_dim)\n            ###calculate mean and std vectors on the training data, and apply on the whole dataset\n            global_mean_vector = normaliser.compute_mean(nn_cmp_file_list[0:cfg.train_file_number], 0, cfg.cmp_dim)\n            global_std_vector = normaliser.compute_std(nn_cmp_file_list[0:cfg.train_file_number], global_mean_vector, 0, cfg.cmp_dim)\n\n            normaliser.feature_normalisation(nn_cmp_file_list, nn_cmp_norm_file_list)\n            cmp_norm_info = numpy.concatenate((global_mean_vector, global_std_vector), axis=0)\n\n        elif cfg.output_feature_normalisation == \'MINMAX\':\n            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim)\n            global_mean_vector = min_max_normaliser.compute_mean(nn_cmp_file_list[0:cfg.train_file_number])\n            global_std_vector = min_max_normaliser.compute_std(nn_cmp_file_list[0:cfg.train_file_number], global_mean_vector)\n\n            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim, min_value = 0.01, max_value = 0.99)\n            min_max_normaliser.find_min_max_values(nn_cmp_file_list[0:cfg.train_file_number])\n            min_max_normaliser.normalise_data(nn_cmp_file_list, nn_cmp_norm_file_list)\n\n            cmp_min_vector = min_max_normaliser.min_vector\n            cmp_max_vector = min_max_normaliser.max_vector\n            cmp_norm_info = numpy.concatenate((cmp_min_vector, cmp_max_vector), axis=0)\n\n        else:\n            logger.critical(\'Normalisation type %s is not supported!\\n\' %(cfg.output_feature_normalisation))\n            raise\n\n        cmp_norm_info = numpy.array(cmp_norm_info, \'float32\')\n        fid = open(norm_info_file, \'wb\')\n        cmp_norm_info.tofile(fid)\n        fid.close()\n        logger.info(\'saved %s vectors to %s\' %(cfg.output_feature_normalisation, norm_info_file))\n        # logger.debug(\' value was\\n%s\' % cmp_norm_info)\n\n        feature_index = 0\n        for feature_name in list(cfg.out_dimension_dict.keys()):\n            feature_std_vector = numpy.array(global_std_vector[:,feature_index:feature_index+cfg.out_dimension_dict[feature_name]], \'float32\')\n\n            fid = open(var_file_dict[feature_name], \'w\')\n            feature_std_vector.tofile(fid)\n            fid.close()\n\n            logger.info(\'saved %s variance vector to %s\' %(feature_name, var_file_dict[feature_name]))\n            # logger.debug(\' value was\\n%s\' % feature_std_vector)\n\n            feature_index += cfg.out_dimension_dict[feature_name]\n\n    train_x_file_list = nn_label_norm_file_list[0:cfg.train_file_number]\n    train_y_file_list = nn_cmp_norm_file_list[0:cfg.train_file_number]\n    valid_x_file_list = nn_label_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n    valid_y_file_list = nn_cmp_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n    test_x_file_list  = nn_label_norm_file_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n    test_y_file_list  = nn_cmp_norm_file_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n\n    # we need to know the label dimension before training the DNN\n    # computing that requires us to look at the labels\n    #\n    # currently, there are two ways to do this\n    if cfg.label_style == \'HTS\':\n        label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name)\n        lab_dim = label_normaliser.dimension\n\n    elif cfg.label_style == \'composed\':\n        label_composer = LabelComposer()\n        label_composer.load_label_configuration(cfg.label_config_file)\n        lab_dim=label_composer.compute_label_dimension()\n\n    logger.info(\'label dimension is %d\' % lab_dim)\n\n    combined_model_arch = str(len(hidden_layers_sizes))\n    for hid_size in hidden_layers_sizes:\n        combined_model_arch += \'_\' + str(hid_size)\n\n#    nnets_file_name = \'%s/%s_%s_%d.%d.%d.%d.%d.train.%d.model\' \\\n#                       %(model_dir, cfg.model_type, cfg.combined_feature_name, int(cfg.multistream_switch),\n#                        len(hidden_layers_sizes), hidden_layers_sizes[0],\n#                        lab_dim, cfg.cmp_dim, cfg.train_file_number)\n\n    nnets_file_name = \'%s/%s_%s_%d_%s_%d.%d.train.%d.model\' \\\n                      %(model_dir, cfg.model_type, cfg.combined_feature_name, int(cfg.multistream_switch),\n                        combined_model_arch, lab_dim, cfg.cmp_dim, cfg.train_file_number)\n\n    ### DNN model training\n    if cfg.TRAINDNN:\n\n        logger.info(\'training DNN\')\n\n        try:\n            os.makedirs(model_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create model directory %s\' % model_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        try:\n            # print   \'start DNN\'\n            train_DNN(train_xy_file_list = (train_x_file_list, train_y_file_list), \\\n                      valid_xy_file_list = (valid_x_file_list, valid_y_file_list), \\\n                      nnets_file_name = nnets_file_name, \\\n                      n_ins = lab_dim, n_outs = cfg.cmp_dim, ms_outs = cfg.multistream_outs, \\\n                      hyper_params = cfg.hyper_params, buffer_size = cfg.buffer_size, plot = cfg.plot)\n        except KeyboardInterrupt:\n            logger.critical(\'train_DNN interrupted via keyboard\')\n            # Could \'raise\' the exception further, but that causes a deep traceback to be printed\n            # which we don\'t care about for a keyboard interrupt. So, just bail out immediately\n            sys.exit(1)\n        except:\n            logger.critical(\'train_DNN threw an exception\')\n            raise\n\n    ### generate parameters from DNN\n\n    if cfg.GENBNFEA:\n\n        temp_dir_name = \'%s_%s_%d_%d_%d_%d_%d_%s_hidden\' \\\n                        %(cfg.model_type, cfg.combined_feature_name, int(cfg.do_post_filtering), \\\n                          cfg.train_file_number, lab_dim, cfg.cmp_dim, \\\n                          len(hidden_layers_sizes), combined_model_arch)\n        gen_dir = os.path.join(gen_dir, temp_dir_name)\n\n\n        bottleneck_size = min(hidden_layers_sizes)\n        bottleneck_index = 0\n        for i in range(len(hidden_layers_sizes)):\n            if hidden_layers_sizes[i] == bottleneck_size:\n                bottleneck_index = i\n\n        logger.info(\'generating from DNN\')\n\n        try:\n            os.makedirs(gen_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create generation directory %s\' % gen_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        gen_file_id_list = file_id_list[0:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n        test_x_file_list  = nn_label_norm_file_list[0:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n        gen_file_list = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.cmp_ext)\n\n        dnn_hidden_generation(test_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list, bottleneck_index)\n\n\n    gen_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n    test_x_file_list  = nn_label_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n\n    temp_dir_name = \'%s_%s_%d_%d_%d_%d_%d_%d_%d\' \\\n                    %(cfg.model_type, cfg.combined_feature_name, int(cfg.do_post_filtering), \\\n                      cfg.train_file_number, lab_dim, cfg.cmp_dim, \\\n                      len(hidden_layers_sizes), max(hidden_layers_sizes), min(hidden_layers_sizes))\n    gen_dir = os.path.join(gen_dir, temp_dir_name)\n\n    if cfg.DNNGEN:\n        logger.info(\'generating from DNN\')\n\n        try:\n            os.makedirs(gen_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create generation directory %s\' % gen_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        gen_file_list = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.cmp_ext)\n\n#        dnn_generation(valid_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)\n        dnn_generation(test_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)\n\n        logger.debug(\'denormalising generated output using method %s\' % cfg.output_feature_normalisation)\n\n        fid = open(norm_info_file, \'rb\')\n        cmp_min_max = numpy.fromfile(fid, dtype=numpy.float32)\n        fid.close()\n        cmp_min_max = cmp_min_max.reshape((2, -1))\n        cmp_min_vector = cmp_min_max[0, ]\n        cmp_max_vector = cmp_min_max[1, ]\n\n        if cfg.output_feature_normalisation == \'MVN\':\n            denormaliser = MeanVarianceNorm(feature_dimension = cfg.cmp_dim)\n            denormaliser.feature_denormalisation(gen_file_list, gen_file_list, cmp_min_vector, cmp_max_vector)\n\n        elif cfg.output_feature_normalisation == \'MINMAX\':\n            denormaliser = MinMaxNormalisation(cfg.cmp_dim, min_value = 0.01, max_value = 0.99, min_vector = cmp_min_vector, max_vector = cmp_max_vector)\n            denormaliser.denormalise_data(gen_file_list, gen_file_list)\n        else:\n            logger.critical(\'denormalising method %s is not supported!\\n\' %(cfg.output_feature_normalisation))\n            raise\n\n        ##perform MLPG to smooth parameter trajectory\n        ## lf0 is included, the output features much have vuv.\n        generator = ParameterGeneration(gen_wav_features = cfg.gen_wav_features)\n        generator.acoustic_decomposition(gen_file_list, cfg.cmp_dim, cfg.out_dimension_dict, cfg.file_extension_dict, var_file_dict)\n\n    ### generate wav\n    if cfg.GENWAV:\n        logger.info(\'reconstructing waveform(s)\')\n        generate_wav(gen_dir, gen_file_id_list, cfg)     # generated speech\n#       generate_wav(nn_cmp_dir, gen_file_id_list)  # reference copy synthesis speech\n\n    ### evaluation: calculate distortion\n    if cfg.CALMCD:\n        logger.info(\'calculating MCD\')\n\n        ref_data_dir = os.path.join(data_dir, \'ref_data\')\n\n        ref_mgc_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.mgc_ext)\n        ref_bap_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.bap_ext)\n        ref_lf0_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.lf0_ext)\n\n        in_gen_label_align_file_list = in_label_align_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n        calculator = IndividualDistortionComp()\n\n        spectral_distortion = 0.0\n        bap_mse             = 0.0\n        f0_mse              = 0.0\n        vuv_error           = 0.0\n\n        valid_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n        test_file_id_list  = file_id_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n        if cfg.remove_silence_using_binary_labels:\n            ## get lab_dim:\n            label_composer = LabelComposer()\n            label_composer.load_label_configuration(cfg.label_config_file)\n            lab_dim=label_composer.compute_label_dimension()\n\n            ## use first feature in label -- hardcoded for now\n            silence_feature = 0\n\n            ## Use these to trim silence:\n            untrimmed_test_labels = binary_label_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n\n        if \'mgc\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'mgc\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_mgc_list, cfg.mgc_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.mgc_dim, silence_pattern = [\'*-#+*\'])\n                remover.remove_silence(in_file_list_dict[\'mgc\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_mgc_list)\n            valid_spectral_distortion = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.mgc_ext, cfg.mgc_dim)\n            test_spectral_distortion  = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.mgc_ext, cfg.mgc_dim)\n            valid_spectral_distortion *= (10 /numpy.log(10)) * numpy.sqrt(2.0)    ##MCD\n            test_spectral_distortion  *= (10 /numpy.log(10)) * numpy.sqrt(2.0)    ##MCD\n\n\n        if \'bap\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'bap\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_bap_list, cfg.bap_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.bap_dim, silence_pattern = [\'*-#+*\'])\n                remover.remove_silence(in_file_list_dict[\'bap\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_bap_list)\n            valid_bap_mse        = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.bap_ext, cfg.bap_dim)\n            test_bap_mse         = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.bap_ext, cfg.bap_dim)\n            valid_bap_mse = valid_bap_mse / 10.0    ##Cassia\'s bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC\n            test_bap_mse  = test_bap_mse / 10.0    ##Cassia\'s bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC\n\n        if \'lf0\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'lf0\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_lf0_list, cfg.lf0_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.lf0_dim, silence_pattern = [\'*-#+*\'])\n                remover.remove_silence(in_file_list_dict[\'lf0\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_lf0_list)\n            valid_f0_mse, valid_vuv_error   = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.lf0_ext, cfg.lf0_dim)\n            test_f0_mse , test_vuv_error    = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.lf0_ext, cfg.lf0_dim)\n\n        logger.info(\'Develop: DNN -- MCD: %.3f dB; BAP: %.3f dB; F0: %.3f Hz; VUV: %.3f%%\' \\\n                    %(valid_spectral_distortion, valid_bap_mse, valid_f0_mse, valid_vuv_error*100.))\n        logger.info(\'Test   : DNN -- MCD: %.3f dB; BAP: %.3f dB; F0: %.3f Hz; VUV: %.3f%%\' \\\n                    %(test_spectral_distortion , test_bap_mse , test_f0_mse , test_vuv_error*100.))\n\n        # this can be removed\n        #\n        if  0: #to calculate distortion of HMM baseline\n            hmm_gen_no_silence_dir = \'/afs/inf.ed.ac.uk/group/project/dnn_tts/data/nick/nick_hmm_pf_2400_no_silence\'\n            hmm_gen_dir = \'/afs/inf.ed.ac.uk/group/project/dnn_tts/data/nick/nick_hmm_pf_2400\'\n\n            if 1:\n                hmm_mgc_list = prepare_file_path_list(gen_file_id_list, hmm_gen_dir, cfg.mgc_ext)\n                hmm_bap_list = prepare_file_path_list(gen_file_id_list, hmm_gen_dir, cfg.bap_ext)\n                hmm_lf0_list = prepare_file_path_list(gen_file_id_list, hmm_gen_dir, cfg.lf0_ext)\n\n                hmm_mgc_no_silence_list = prepare_file_path_list(gen_file_id_list, hmm_gen_no_silence_dir, cfg.mgc_ext)\n                hmm_bap_no_silence_list = prepare_file_path_list(gen_file_id_list, hmm_gen_no_silence_dir, cfg.bap_ext)\n                hmm_lf0_no_silence_list = prepare_file_path_list(gen_file_id_list, hmm_gen_no_silence_dir, cfg.lf0_ext)\n\n                in_gen_label_align_file_list = in_label_align_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                remover = SilenceRemover(n_cmp = cfg.mgc_dim, silence_pattern = [\'*-#+*\'])\n                remover.remove_silence(hmm_mgc_list, in_gen_label_align_file_list, hmm_mgc_no_silence_list)\n\n                remover = SilenceRemover(n_cmp = cfg.bap_dim, silence_pattern = [\'*-#+*\'])\n                remover.remove_silence(hmm_bap_list, in_gen_label_align_file_list, hmm_bap_no_silence_list)\n\n                remover = SilenceRemover(n_cmp = cfg.lf0_dim, silence_pattern = [\'*-#+*\'])\n                remover.remove_silence(hmm_lf0_list, in_gen_label_align_file_list, hmm_lf0_no_silence_list)\n\n            calculator = IndividualDistortionComp()\n\n            spectral_distortion = calculator.compute_distortion(valid_file_id_list, ref_data_dir, hmm_gen_no_silence_dir, cfg.mgc_ext, cfg.mgc_dim)\n            bap_mse             = calculator.compute_distortion(valid_file_id_list, ref_data_dir, hmm_gen_no_silence_dir, cfg.bap_ext, cfg.bap_dim)\n            f0_mse, vuv_error   = calculator.compute_distortion(valid_file_id_list, ref_data_dir, hmm_gen_no_silence_dir, cfg.lf0_ext, cfg.lf0_dim)\n\n            spectral_distortion *= (10 /numpy.log(10)) * numpy.sqrt(2.0)\n            bap_mse = bap_mse / 10.0\n\n            logger.info(\'Develop: HMM -- MCD: %.3f dB; BAP: %.3f dB; F0: %.3f Hz; VUV: %.3f%%\' %(spectral_distortion, bap_mse, f0_mse, vuv_error*100.))\n\n            spectral_distortion = calculator.compute_distortion(test_file_id_list, ref_data_dir, hmm_gen_no_silence_dir, cfg.mgc_ext, cfg.mgc_dim)\n            bap_mse             = calculator.compute_distortion(test_file_id_list, ref_data_dir, hmm_gen_no_silence_dir, cfg.bap_ext, cfg.bap_dim)\n            f0_mse, vuv_error   = calculator.compute_distortion(test_file_id_list, ref_data_dir, hmm_gen_no_silence_dir, cfg.lf0_ext, cfg.lf0_dim)\n\n            spectral_distortion *= (10 /numpy.log(10)) * numpy.sqrt(2.0)\n            bap_mse = bap_mse / 10.0\n\n            logger.info(\'Test   : HMM -- MCD: %.3f dB; BAP: %.3f dB; F0: %.3f Hz; VUV: %.3f%%\' %(spectral_distortion, bap_mse, f0_mse, vuv_error*100.))\n\nif __name__ == \'__main__\':\n\n\n\n    # these things should be done even before trying to parse the command line\n\n    # create a configuration instance\n    # and get a short name for this instance\n    cfg=configuration.cfg\n\n    # set up logging to use our custom class\n    logging.setLoggerClass(LoggerPlotter)\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n\n    if len(sys.argv) != 2:\n        logger.critical(\'usage: run_dnn.sh [config file name]\')\n        sys.exit(1)\n\n    config_file = sys.argv[1]\n\n    config_file = os.path.abspath(config_file)\n    cfg.configure(config_file)\n\n    if cfg.profile:\n        logger.info(\'profiling is activated\')\n        import cProfile, pstats\n        cProfile.run(\'main_function(cfg)\', \'mainstats\')\n\n        # create a stream for the profiler to write to\n        profiling_output = io.StringIO()\n        p = pstats.Stats(\'mainstats\', stream=profiling_output)\n\n        # print stats to that stream\n        # here we just report the top 10 functions, sorted by total amount of time spent in each\n        p.strip_dirs().sort_stats(\'tottime\').print_stats(10)\n\n        # print the result to the log\n        logger.info(\'---Profiling result follows---\\n%s\' %  profiling_output.getvalue() )\n        profiling_output.close()\n        logger.info(\'---End of profiling result---\')\n\n    else:\n        main_function(cfg)\n\n    sys.exit(0)\n'"
src/work_in_progress/run_mdn.py,0,"b'################################################################################\n#           The Neural Network (NN) based Speech Synthesis System\n#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/\n#\n#                Centre for Speech Technology Research\n#                     University of Edinburgh, UK\n#                      Copyright (c) 2014-2015\n#                        All Rights Reserved.\n#\n# The system as a whole and most of the files in it are distributed\n# under the following copyright and conditions\n#\n#  Permission is hereby granted, free of charge, to use and distribute\n#  this software and its documentation without restriction, including\n#  without limitation the rights to use, copy, modify, merge, publish,\n#  distribute, sublicense, and/or sell copies of this work, and to\n#  permit persons to whom this work is furnished to do so, subject to\n#  the following conditions:\n#\n#   - Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   - Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   - The authors\' names may not be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK\n#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT\n#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE\n#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN\n#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,\n#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\n#  THIS SOFTWARE.\n################################################################################\n\n\nimport pickle\nimport gzip\nimport os, sys, errno\nimport time\nimport math\n\n#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)\nimport numpy\n# we need to explicitly import this in some cases, not sure why this doesn\'t get imported with numpy itself\nimport numpy.distutils.__config__\n# and only after that can we import theano\nimport theano\n\nfrom utils.providers import ListDataProvider\n\nfrom frontend.label_normalisation import HTSLabelNormalisation, XMLLabelNormalisation\nfrom frontend.silence_remover import SilenceRemover\nfrom frontend.silence_remover import trim_silence\nfrom frontend.min_max_norm import MinMaxNormalisation\n#from frontend.acoustic_normalisation import CMPNormalisation\nfrom frontend.acoustic_composition import AcousticComposition\nfrom frontend.parameter_generation import ParameterGeneration\n#from frontend.feature_normalisation_base import FeatureNormBase\nfrom frontend.mean_variance_norm import MeanVarianceNorm\nfrom frontend.mlpg_fast import MLParameterGenerationFast\n\n# the new class for label composition and normalisation\nfrom frontend.label_composer import LabelComposer\n\nimport configuration\n\nfrom models.dnn import DNN\n#from models.ms_dnn import MultiStreamDNN\n#from models.ms_dnn_gv import MultiStreamDNNGv\n#from models.sdae import StackedDenoiseAutoEncoder\nfrom models.mdn import MixtureDensityNetwork\n\nfrom utils.compute_distortion import DistortionComputation, IndividualDistortionComp\nfrom utils.generate import generate_wav\nfrom utils.learn_rates import ExpDecreaseLearningRate\nfrom io_funcs.binary_io import  BinaryIOCollection\n\n#import matplotlib.pyplot as plt\n# our custom logging class that can also plot\n#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot\nfrom logplot.logging_plotting import LoggerPlotter, MultipleSeriesPlot, SingleWeightMatrixPlot\nimport logging # as logging\nimport logging.config\nimport io\n\n\ndef extract_file_id_list(file_list):\n    file_id_list = []\n    for file_name in file_list:\n        file_id = os.path.basename(os.path.splitext(file_name)[0])\n        file_id_list.append(file_id)\n\n    return  file_id_list\n\ndef read_file_list(file_name):\n\n    logger = logging.getLogger(""read_file_list"")\n\n    file_lists = []\n    fid = open(file_name)\n    for line in fid.readlines():\n        line = line.strip()\n        if len(line) < 1:\n            continue\n        file_lists.append(line)\n    fid.close()\n\n    logger.debug(\'Read file list from %s\' % file_name)\n    return  file_lists\n\n\ndef make_output_file_list(out_dir, in_file_lists):\n    out_file_lists = []\n\n    for in_file_name in in_file_lists:\n        file_id = os.path.basename(in_file_name)\n        out_file_name = out_dir + \'/\' + file_id\n        out_file_lists.append(out_file_name)\n\n    return  out_file_lists\n\ndef prepare_file_path_list(file_id_list, file_dir, file_extension, new_dir_switch=True):\n    if not os.path.exists(file_dir) and new_dir_switch:\n        os.makedirs(file_dir)\n    file_name_list = []\n    for file_id in file_id_list:\n        file_name = file_dir + \'/\' + file_id + file_extension\n        file_name_list.append(file_name)\n\n    return  file_name_list\n\n\n\ndef visualize_dnn(dnn):\n\n    layer_num = len(dnn.params) / 2     ## including input and output\n\n    for i in range(layer_num):\n        fig_name = \'Activation weights W\' + str(i)\n        fig_title = \'Activation weights of W\' + str(i)\n        xlabel = \'Neuron index of hidden layer \' + str(i)\n        ylabel = \'Neuron index of hidden layer \' + str(i+1)\n        if i == 0:\n            xlabel = \'Input feature index\'\n        if i == layer_num-1:\n            ylabel = \'Output feature index\'\n\n        logger.create_plot(fig_name, SingleWeightMatrixPlot)\n        plotlogger.add_plot_point(fig_name, fig_name, dnn.params[i*2].get_value(borrow=True).T)\n        plotlogger.save_plot(fig_name, title=fig_name, xlabel=xlabel, ylabel=ylabel)\n\n\ndef train_DNN(train_xy_file_list, valid_xy_file_list, \\\n              nnets_file_name, n_ins, n_outs, ms_outs, hyper_params, buffer_size, \\\n              mdn_component, var_floor=0.01, beta_opt=False, eff_sample_size=0.8, mean_log_det=-100.0, \\\n              plot=False, start_from_trained_model=\'_\'):\n\n    # get loggers for this function\n    # this one writes to both console and file\n    logger = logging.getLogger(""main.train_DNN"")\n    logger.debug(\'Starting train_DNN\')\n\n    if plot:\n        # this one takes care of plotting duties\n        plotlogger = logging.getLogger(""plotting"")\n        # create an (empty) plot of training convergence, ready to receive data points\n        logger.create_plot(\'training convergence\',MultipleSeriesPlot)\n\n    try:\n        assert numpy.sum(ms_outs) == n_outs\n    except AssertionError:\n        logger.critical(\'the summation of multi-stream outputs does not equal to %d\' %(n_outs))\n        raise\n\n    ####parameters#####\n    finetune_lr     = float(hyper_params[\'learning_rate\'])\n    training_epochs = int(hyper_params[\'training_epochs\'])\n    batch_size      = int(hyper_params[\'batch_size\'])\n    l1_reg          = float(hyper_params[\'l1_reg\'])\n    l2_reg          = float(hyper_params[\'l2_reg\'])\n    private_l2_reg  = float(hyper_params[\'private_l2_reg\'])\n    warmup_epoch    = int(hyper_params[\'warmup_epoch\'])\n    momentum        = float(hyper_params[\'momentum\'])\n    warmup_momentum = float(hyper_params[\'warmup_momentum\'])\n\n    use_rprop = int(hyper_params[\'use_rprop\'])\n\n    hidden_layers_sizes = hyper_params[\'hidden_layer_size\']\n\n    stream_weights       = hyper_params[\'stream_weights\']\n    private_hidden_sizes = hyper_params[\'private_hidden_sizes\']\n\n    buffer_utt_size = buffer_size\n    early_stop_epoch = int(hyper_params[\'early_stop_epochs\'])\n\n    hidden_activation = hyper_params[\'hidden_activation\']\n    output_activation = hyper_params[\'output_activation\']\n\n    stream_lr_weights = hyper_params[\'stream_lr_weights\']\n    use_private_hidden = hyper_params[\'use_private_hidden\']\n\n    model_type = hyper_params[\'model_type\']\n\n    ## use a switch to turn on pretraining\n    ## pretraining may not help too much, if this case, we turn it off to save time\n    do_pretraining = hyper_params[\'do_pretraining\']\n    pretraining_epochs = int(hyper_params[\'pretraining_epochs\'])\n    pretraining_lr = float(hyper_params[\'pretraining_lr\'])\n\n\n    buffer_size = int(buffer_size / batch_size) * batch_size\n\n    ###################\n    (train_x_file_list, train_y_file_list) = train_xy_file_list\n    (valid_x_file_list, valid_y_file_list) = valid_xy_file_list\n\n    logger.debug(\'Creating training   data provider\')\n    train_data_reader = ListDataProvider(x_file_list = train_x_file_list, y_file_list = train_y_file_list, n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, shuffle = True)\n\n    logger.debug(\'Creating validation data provider\')\n    valid_data_reader = ListDataProvider(x_file_list = valid_x_file_list, y_file_list = valid_y_file_list, n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, shuffle = False)\n\n    shared_train_set_xy, temp_train_set_x, temp_train_set_y = train_data_reader.load_next_partition()\n    train_set_x, train_set_y = shared_train_set_xy\n    shared_valid_set_xy, temp_valid_set_x, temp_valid_set_y = valid_data_reader.load_next_partition()\n    valid_set_x, valid_set_y = shared_valid_set_xy\n    train_data_reader.reset()\n    valid_data_reader.reset()\n\n    ##temporally we use the training set as pretrain_set_x.\n    ##we need to support any data for pretraining\n    pretrain_set_x = train_set_x\n\n    # numpy random generator\n    numpy_rng = numpy.random.RandomState(123)\n    logger.info(\'building the model\')\n\n\n    dnn_model = None\n    pretrain_fn = None  ## not all the model support pretraining right now\n    train_fn = None\n    valid_fn = None\n    valid_model = None ## valid_fn and valid_model are the same. reserve to computer multi-stream distortion\n\n\n    if model_type == \'DNN\':\n\n        dnn_model = MixtureDensityNetwork(numpy_rng=numpy_rng, n_ins=n_ins, n_outs = n_outs,\n                        l1_reg = l1_reg, l2_reg = l2_reg,\n                         hidden_layers_sizes = hidden_layers_sizes,\n                          hidden_activation = hidden_activation,\n                          output_activation = output_activation, var_floor = var_floor,\n                          n_component = mdn_component,\n                          use_rprop = use_rprop, rprop_init_update=finetune_lr,\n                          beta_opt=beta_opt, eff_sample_size=eff_sample_size, mean_log_det=mean_log_det)\n#        dnn_model = DNN(numpy_rng=numpy_rng, n_ins=n_ins, n_outs = n_outs,\n#                        l1_reg = l1_reg, l2_reg = l2_reg,\n#                         hidden_layers_sizes = hidden_layers_sizes,\n#                          hidden_activation = hidden_activation,\n#                          output_activation = output_activation)\n        train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y), batch_size=batch_size)\n\n    else:\n        logger.critical(\'%s type NN model is not supported!\' %(model_type))\n        raise\n\n    ## We can\'t just unpickle the old model and use that because fine-tune functions\n    ## depend on opt_l2e option used in construction of initial model. One way around this\n    ## would be to unpickle, manually set unpickled_dnn_model.opt_l2e=True and then call\n    ## unpickled_dnn_model.build_finetne_function() again. This is another way, construct\n    ## new model from scratch with opt_l2e=True, then copy existing weights over:\n\n    if start_from_trained_model != \'_\':\n        logger.info(\'load parameters from existing model: %s\' %(start_from_trained_model))\n        if not os.path.isfile(start_from_trained_model):\n            sys.exit(\'Model file %s does not exist\'%(start_from_trained_model))\n        existing_dnn_model = pickle.load(open(start_from_trained_model, \'rb\'))\n        if not len(existing_dnn_model.params) == len(dnn_model.params):\n            sys.exit(\'Old and new models have different numbers of weight matrices\')\n        for (old_weight, new_weight) in zip(existing_dnn_model.params, dnn_model.params):\n            old_val = old_weight.get_value()\n            new_val = new_weight.get_value()\n            if numpy.shape(old_val) == numpy.shape(new_val):\n                new_weight.set_value(old_val)\n            else:\n                sys.exit(\'old and new weight matrices have different shapes\')\n\n\n    logger.info(\'fine-tuning the %s model\' %(model_type))\n\n    start_time = time.clock()\n\n    best_dnn_model = dnn_model\n    best_validation_loss = sys.float_info.max\n    previous_loss = sys.float_info.max\n\n    early_stop = 0\n    epoch = 0\n    previous_finetune_lr = finetune_lr\n    while (epoch < training_epochs): #training_epochs\n        epoch = epoch + 1\n\n        current_momentum = momentum\n        current_finetune_lr = finetune_lr\n        if epoch <= warmup_epoch:\n            current_finetune_lr = finetune_lr\n            current_momentum = warmup_momentum\n        else:\n            current_finetune_lr = previous_finetune_lr * 0.5\n\n        previous_finetune_lr = current_finetune_lr\n\n        train_error = []\n        sub_start_time = time.clock()\n\n        while (not train_data_reader.is_finish()):\n            shared_train_set_xy, temp_train_set_x, temp_train_set_y = train_data_reader.load_next_partition()\n            train_set_x.set_value(numpy.asarray(temp_train_set_x, dtype=theano.config.floatX), borrow=True)\n            train_set_y.set_value(numpy.asarray(temp_train_set_y, dtype=theano.config.floatX), borrow=True)\n\n            n_train_batches = train_set_x.get_value().shape[0] / batch_size\n\n            logger.debug(\'this partition: %d frames (divided into %d batches of size %d)\' %(train_set_x.get_value(borrow=True).shape[0], n_train_batches, batch_size) )\n\n            for minibatch_index in range(n_train_batches):\n                this_train_error = train_fn(minibatch_index, current_finetune_lr, current_momentum)\n                train_error.append(this_train_error)\n\n                if numpy.isnan(this_train_error):\n                    logger.warning(\'training error over minibatch %d of %d was %s\' % (minibatch_index+1,n_train_batches,this_train_error) )\n\n        train_data_reader.reset()\n\n        logger.debug(\'calculating validation loss\')\n        validation_losses = valid_fn()\n        this_validation_loss = numpy.mean(validation_losses)\n\n        # this has a possible bias if the minibatches were not all of identical size\n        # but it should not be siginficant if minibatches are small\n        this_train_valid_loss = numpy.mean(train_error)\n\n        sub_end_time = time.clock()\n\n        loss_difference = this_validation_loss - previous_loss\n\n        logger.info(\'epoch %i, validation error %f, train error %f  time spent %.2f\' %(epoch, this_validation_loss, this_train_valid_loss, (sub_end_time - sub_start_time)))\n        if plot:\n            plotlogger.add_plot_point(\'training convergence\',\'validation set\',(epoch,this_validation_loss))\n            plotlogger.add_plot_point(\'training convergence\',\'training set\',(epoch,this_train_valid_loss))\n            plotlogger.save_plot(\'training convergence\',title=\'Optimisation progress\',xlabel=\'training epochs\',ylabel=\'objective function\')\n\n        if this_validation_loss < best_validation_loss:\n            best_dnn_model = dnn_model\n            best_validation_loss = this_validation_loss\n            logger.debug(\'validation loss decreased, so saving model\')\n            early_stop = 0\n        else:\n            logger.debug(\'validation loss did not improve\')\n            dbn = best_dnn_model\n            early_stop += 1\n\n        if early_stop >= early_stop_epoch:\n            # too many consecutive epochs without surpassing the best model\n            logger.debug(\'stopping early\')\n            break\n\n        if math.isnan(this_validation_loss):\n            break\n\n        previous_loss = this_validation_loss\n\n    end_time = time.clock()\n    pickle.dump(best_dnn_model, open(nnets_file_name, \'wb\'))\n\n    logger.info(\'overall  training time: %.2fm validation error %f\' % ((end_time - start_time) / 60., best_validation_loss))\n\n    if plot:\n        plotlogger.save_plot(\'training convergence\',title=\'Final training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n\n    return  best_validation_loss\n\n\ndef dnn_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n#    visualize_dnn(dbn)\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        features = features.reshape((-1, n_ins))\n        temp_set_x = features.tolist()\n        test_set_x = theano.shared(numpy.asarray(temp_set_x, dtype=theano.config.floatX))\n\n        predicted_parameter = dnn_model.parameter_prediction(test_set_x=test_set_x)\n#        predicted_parameter = test_out()\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\n### multiple Gaussian components\ndef dnn_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list, target_mean_vector, target_std_vector, out_dimension_dict, file_extension_dict):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    inf_float = -1.0e+10\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    gen_wav_features = [\'mgc\', \'lf0\', \'bap\']\n    stream_start_index = {}\n    dimension_index = 0\n    for feature_name in list(out_dimension_dict.keys()):\n        stream_start_index[feature_name] = dimension_index\n        dimension_index += out_dimension_dict[feature_name]\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    file_number = len(valid_file_list)\n    io_funcs = BinaryIOCollection()\n\n    mlpg = MLParameterGenerationFast()\n\n    for i in range(file_number):\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        features = features.reshape((-1, n_ins))\n\n        frame_number = features.shape[0]\n\n        test_set_x = theano.shared(numpy.asarray(features, dtype=theano.config.floatX))\n\n        mean_matrix = numpy.tile(target_mean_vector, (features.shape[0], 1))\n        std_matrix = numpy.tile(target_std_vector, (features.shape[0], 1))\n\n        predicted_mix   = dnn_model.parameter_prediction_mix(test_set_x = test_set_x)\n        max_index = numpy.argmax(predicted_mix, axis=1)\n\n        temp_predicted_mu = dnn_model.parameter_prediction(test_set_x=test_set_x)\n        temp_predicted_sigma = dnn_model.parameter_prediction_sigma(test_set_x = test_set_x)\n        predicted_mu = numpy.zeros((temp_predicted_mu.shape[0], n_outs))\n        predicted_sigma = numpy.zeros((temp_predicted_sigma.shape[0], n_outs))\n        for kk in range(temp_predicted_mu.shape[0]):\n            predicted_mu[kk, :] = temp_predicted_mu[kk, max_index[kk]*n_outs:(max_index[kk]+1)*n_outs]\n            predicted_sigma[kk, :] = temp_predicted_sigma[kk, max_index[kk]*n_outs:(max_index[kk]+1)*n_outs]\n#        print   predicted_mu.shape\n#        predicted_mu = predicted_mu[aa*n_outs:(aa+1)*n_outs]\n        predicted_mu = predicted_mu * std_matrix + mean_matrix\n        predicted_sigma = ((predicted_sigma ** 0.5) * std_matrix ) ** 2\n\n        dir_name = os.path.dirname(out_file_list[i])\n        file_id = os.path.splitext(os.path.basename(out_file_list[i]))[0]\n\n        mlpg = MLParameterGenerationFast()\n        for feature_name in gen_wav_features:\n            current_features = predicted_mu[:, stream_start_index[feature_name]:stream_start_index[feature_name]+out_dimension_dict[feature_name]]\n            current_sigma    = predicted_sigma[:, stream_start_index[feature_name]:stream_start_index[feature_name]+out_dimension_dict[feature_name]]\n\n            gen_features = mlpg.generation(current_features, current_sigma, out_dimension_dict[feature_name]/3)\n\n            if feature_name == \'lf0\':\n                if \'vuv\' in stream_start_index:\n                    vuv_feature = predicted_mu[:, stream_start_index[\'vuv\']:stream_start_index[\'vuv\']+1]\n                    for i in range(frame_number):\n                        if vuv_feature[i, 0] < 0.5:\n                            gen_features[i, 0] = inf_float\n#                print   gen_features\n            new_file_name = os.path.join(dir_name, file_id + file_extension_dict[feature_name])\n\n            io_funcs.array_to_binary_file(gen_features, new_file_name)\n\n\n##generate bottleneck layer as festures\ndef dnn_hidden_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        features = features.reshape((-1, n_ins))\n        temp_set_x = features.tolist()\n        test_set_x = theano.shared(numpy.asarray(temp_set_x, dtype=theano.config.floatX))\n\n        predicted_parameter = dnn_model.generate_top_hidden_layer(test_set_x=test_set_x)\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\n\ndef main_function(cfg):\n\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n    # get another logger to handle plotting duties\n    plotlogger = logging.getLogger(""plotting"")\n\n    # later, we might do this via a handler that is created, attached and configured\n    # using the standard config mechanism of the logging module\n    # but for now we need to do it manually\n    plotlogger.set_plot_path(cfg.plot_dir)\n\n    #### parameter setting########\n    hidden_layers_sizes = cfg.hyper_params[\'hidden_layer_size\']\n\n\n    ####prepare environment\n\n    try:\n        file_id_list = read_file_list(cfg.file_id_scp)\n        logger.debug(\'Loaded file id list from %s\' % cfg.file_id_scp)\n    except IOError:\n        # this means that open(...) threw an error\n        logger.critical(\'Could not load file id list from %s\' % cfg.file_id_scp)\n        raise\n\n    ###total file number including training, development, and testing\n    total_file_number = len(file_id_list)\n\n    data_dir = cfg.data_dir\n\n    nn_cmp_dir       = os.path.join(data_dir, \'nn\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim))\n    nn_cmp_norm_dir   = os.path.join(data_dir, \'nn_norm\'  + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim))\n\n    model_dir = os.path.join(cfg.work_dir, \'nnets_model\')\n    gen_dir   = os.path.join(cfg.work_dir, \'gen\')\n\n    in_file_list_dict = {}\n\n    for feature_name in list(cfg.in_dir_dict.keys()):\n        in_file_list_dict[feature_name] = prepare_file_path_list(file_id_list, cfg.in_dir_dict[feature_name], cfg.file_extension_dict[feature_name], False)\n\n    nn_cmp_file_list         = prepare_file_path_list(file_id_list, nn_cmp_dir, cfg.cmp_ext)\n    nn_cmp_norm_file_list    = prepare_file_path_list(file_id_list, nn_cmp_norm_dir, cfg.cmp_ext)\n\n    ###normalisation information\n    norm_info_file = os.path.join(data_dir, \'norm_info\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim) + \'_\' + cfg.output_feature_normalisation + \'.dat\')\n\n    ### normalise input full context label\n\n    # currently supporting two different forms of lingustic features\n    # later, we should generalise this\n\n    if cfg.label_style == \'HTS\':\n        label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name)\n        lab_dim = label_normaliser.dimension\n        logger.info(\'Input label dimension is %d\' % lab_dim)\n        suffix=str(lab_dim)\n    # no longer supported - use new ""composed"" style labels instead\n    elif cfg.label_style == \'composed\':\n        # label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)\n        suffix=\'composed\'\n\n    if cfg.process_labels_in_work_dir:\n        label_data_dir = cfg.work_dir\n    else:\n        label_data_dir = data_dir\n\n    # the number can be removed\n    binary_label_dir      = os.path.join(label_data_dir, \'binary_label_\'+suffix)\n    nn_label_dir          = os.path.join(label_data_dir, \'nn_no_silence_lab_\'+suffix)\n    nn_label_norm_dir     = os.path.join(label_data_dir, \'nn_no_silence_lab_norm_\'+suffix)\n#    nn_label_norm_mvn_dir = os.path.join(data_dir, \'nn_no_silence_lab_norm_\'+suffix)\n\n    in_label_align_file_list = prepare_file_path_list(file_id_list, cfg.in_label_align_dir, cfg.lab_ext, False)\n    binary_label_file_list   = prepare_file_path_list(file_id_list, binary_label_dir, cfg.lab_ext)\n    nn_label_file_list       = prepare_file_path_list(file_id_list, nn_label_dir, cfg.lab_ext)\n    nn_label_norm_file_list  = prepare_file_path_list(file_id_list, nn_label_norm_dir, cfg.lab_ext)\n\n    # to do - sanity check the label dimension here?\n\n\n\n    min_max_normaliser = None\n    label_norm_file = \'label_norm_%s.dat\' %(cfg.label_style)\n    label_norm_file = os.path.join(label_data_dir, label_norm_file)\n\n    if cfg.NORMLAB and (cfg.label_style == \'HTS\'):\n        # simple HTS labels\n        logger.info(\'preparing label data (input) using standard HTS style labels\')\n        label_normaliser.perform_normalisation(in_label_align_file_list, binary_label_file_list)\n\n        remover = SilenceRemover(n_cmp = lab_dim, silence_pattern = [\'*-#+*\'])\n        remover.remove_silence(binary_label_file_list, in_label_align_file_list, nn_label_file_list)\n\n        min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99)\n        ###use only training data to find min-max information, then apply on the whole dataset\n        min_max_normaliser.find_min_max_values(nn_label_file_list[0:cfg.train_file_number])\n        min_max_normaliser.normalise_data(nn_label_file_list, nn_label_norm_file_list)\n\n\n    if cfg.NORMLAB and (cfg.label_style == \'composed\'):\n        # new flexible label preprocessor\n\n        logger.info(\'preparing label data (input) using ""composed"" style labels\')\n        label_composer = LabelComposer()\n        label_composer.load_label_configuration(cfg.label_config_file)\n\n        logger.info(\'Loaded label configuration\')\n        # logger.info(\'%s\' % label_composer.configuration.labels )\n\n        lab_dim=label_composer.compute_label_dimension()\n        logger.info(\'label dimension will be %d\' % lab_dim)\n\n        if cfg.precompile_xpaths:\n            label_composer.precompile_xpaths()\n\n        # there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)\n        # create all the lists of these, ready to pass to the label composer\n\n        in_label_align_file_list = {}\n        for label_style, label_style_required in label_composer.label_styles.items():\n            if label_style_required:\n                logger.info(\'labels of style %s are required - constructing file paths for them\' % label_style)\n                if label_style == \'xpath\':\n                    in_label_align_file_list[\'xpath\'] = prepare_file_path_list(file_id_list, cfg.xpath_label_align_dir, cfg.utt_ext, False)\n                elif label_style == \'hts\':\n                    in_label_align_file_list[\'hts\'] = prepare_file_path_list(file_id_list, cfg.hts_label_align_dir, cfg.lab_ext, False)\n                else:\n                    logger.critical(\'unsupported label style %s specified in label configuration\' % label_style)\n                    raise Exception\n\n            # now iterate through the files, one at a time, constructing the labels for them\n            num_files=len(file_id_list)\n            logger.info(\'the label styles required are %s\' % label_composer.label_styles)\n\n            for i in range(num_files):\n                logger.info(\'making input label features for %4d of %4d\' % (i+1,num_files))\n\n                # iterate through the required label styles and open each corresponding label file\n\n                # a dictionary of file descriptors, pointing at the required files\n                required_labels={}\n\n                for label_style, label_style_required in label_composer.label_styles.items():\n\n                    # the files will be a parallel set of files for a single utterance\n                    # e.g., the XML tree and an HTS label file\n                    if label_style_required:\n                        required_labels[label_style] = open(in_label_align_file_list[label_style][i] , \'r\')\n                        logger.debug(\' opening label file %s\' % in_label_align_file_list[label_style][i])\n\n                logger.debug(\'label styles with open files: %s\' % required_labels)\n                label_composer.make_labels(required_labels,out_file_name=binary_label_file_list[i],fill_missing_values=cfg.fill_missing_values,iterate_over_frames=cfg.iterate_over_frames)\n\n                # now close all opened files\n                for fd in required_labels.values():\n                    fd.close()\n\n\n        # silence removal\n        if cfg.remove_silence_using_binary_labels:\n            silence_feature = 0 ## use first feature in label -- hardcoded for now\n            logger.info(\'Silence removal from label using silence feature: %s\'%(label_composer.configuration.labels[silence_feature]))\n            logger.info(\'Silence will be removed from CMP files in same way\')\n            ## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:\n            trim_silence(binary_label_file_list, nn_label_file_list, lab_dim, \\\n                                binary_label_file_list, lab_dim, silence_feature, percent_to_keep=5)\n        else:\n            logger.info(\'No silence removal done\')\n            # start from the labels we have just produced, not trimmed versions\n            nn_label_file_list = binary_label_file_list\n\n        min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99)\n        ###use only training data to find min-max information, then apply on the whole dataset\n        min_max_normaliser.find_min_max_values(nn_label_file_list[0:cfg.train_file_number])\n        min_max_normaliser.normalise_data(nn_label_file_list, nn_label_norm_file_list)\n\n    if min_max_normaliser != None:\n        ### save label normalisation information for unseen testing labels\n        label_min_vector = min_max_normaliser.min_vector\n        label_max_vector = min_max_normaliser.max_vector\n        label_norm_info = numpy.concatenate((label_min_vector, label_max_vector), axis=0)\n\n        label_norm_info = numpy.array(label_norm_info, \'float32\')\n        fid = open(label_norm_file, \'wb\')\n        label_norm_info.tofile(fid)\n        fid.close()\n        logger.info(\'saved %s vectors to %s\' %(label_min_vector.size, label_norm_file))\n\n\n\n\n    ### make output acoustic data\n    if cfg.MAKECMP:\n        logger.info(\'creating acoustic (output) features\')\n        delta_win = [-0.5, 0.0, 0.5]\n        acc_win = [1.0, -2.0, 1.0]\n\n        acoustic_worker = AcousticComposition(delta_win = delta_win, acc_win = acc_win)\n        acoustic_worker.prepare_nn_data(in_file_list_dict, nn_cmp_file_list, cfg.in_dimension_dict, cfg.out_dimension_dict)\n\n        if cfg.remove_silence_using_binary_labels:\n            ## do this to get lab_dim:\n            label_composer = LabelComposer()\n            label_composer.load_label_configuration(cfg.label_config_file)\n            lab_dim=label_composer.compute_label_dimension()\n\n            silence_feature = 0 ## use first feature in label -- hardcoded for now\n            logger.info(\'Silence removal from CMP using binary label file\')\n\n            ## overwrite the untrimmed audio with the trimmed version:\n            trim_silence(nn_cmp_file_list, nn_cmp_file_list, cfg.cmp_dim, \\\n                                binary_label_file_list, lab_dim, silence_feature, percent_to_keep=5)\n\n        else: ## back off to previous method using HTS labels:\n            remover = SilenceRemover(n_cmp = cfg.cmp_dim, silence_pattern = [\'*-#+*\'])\n            remover.remove_silence(nn_cmp_file_list, in_label_align_file_list, nn_cmp_file_list) # save to itself\n\n    ### save acoustic normalisation information for normalising the features back\n    var_dir   = os.path.join(data_dir, \'var\')\n    if not os.path.exists(var_dir):\n        os.makedirs(var_dir)\n\n    var_file_dict = {}\n    for feature_name in list(cfg.out_dimension_dict.keys()):\n        var_file_dict[feature_name] = os.path.join(var_dir, feature_name + \'_\' + str(cfg.out_dimension_dict[feature_name]))\n\n    ### normalise output acoustic data\n    if cfg.NORMCMP:\n        logger.info(\'normalising acoustic (output) features using method %s\' % cfg.output_feature_normalisation)\n        cmp_norm_info = None\n        if cfg.output_feature_normalisation == \'MVN\':\n            normaliser = MeanVarianceNorm(feature_dimension=cfg.cmp_dim)\n            ###calculate mean and std vectors on the training data, and apply on the whole dataset\n            global_mean_vector = normaliser.compute_mean(nn_cmp_file_list[0:cfg.train_file_number], 0, cfg.cmp_dim)\n            global_std_vector = normaliser.compute_std(nn_cmp_file_list[0:cfg.train_file_number], global_mean_vector, 0, cfg.cmp_dim)\n\n            normaliser.feature_normalisation(nn_cmp_file_list, nn_cmp_norm_file_list)\n            cmp_norm_info = numpy.concatenate((global_mean_vector, global_std_vector), axis=0)\n\n        elif cfg.output_feature_normalisation == \'MINMAX\':\n            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim)\n            global_mean_vector = min_max_normaliser.compute_mean(nn_cmp_file_list[0:cfg.train_file_number])\n            global_std_vector = min_max_normaliser.compute_std(nn_cmp_file_list[0:cfg.train_file_number], global_mean_vector)\n\n            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim, min_value = 0.01, max_value = 0.99)\n            min_max_normaliser.find_min_max_values(nn_cmp_file_list[0:cfg.train_file_number])\n            min_max_normaliser.normalise_data(nn_cmp_file_list, nn_cmp_norm_file_list)\n\n            cmp_min_vector = min_max_normaliser.min_vector\n            cmp_max_vector = min_max_normaliser.max_vector\n            cmp_norm_info = numpy.concatenate((cmp_min_vector, cmp_max_vector), axis=0)\n\n        else:\n            logger.critical(\'Normalisation type %s is not supported!\\n\' %(cfg.output_feature_normalisation))\n            raise\n\n        cmp_norm_info = numpy.array(cmp_norm_info, \'float32\')\n        fid = open(norm_info_file, \'wb\')\n        cmp_norm_info.tofile(fid)\n        fid.close()\n        logger.info(\'saved %s vectors to %s\' %(cfg.output_feature_normalisation, norm_info_file))\n        # logger.debug(\' value was\\n%s\' % cmp_norm_info)\n\n        feature_index = 0\n        for feature_name in list(cfg.out_dimension_dict.keys()):\n            feature_std_vector = numpy.array(global_std_vector[:,feature_index:feature_index+cfg.out_dimension_dict[feature_name]], \'float32\')\n\n            fid = open(var_file_dict[feature_name], \'w\')\n            feature_std_vector.tofile(fid)\n            fid.close()\n\n            logger.info(\'saved %s variance vector to %s\' %(feature_name, var_file_dict[feature_name]))\n            # logger.debug(\' value was\\n%s\' % feature_std_vector)\n\n            feature_index += cfg.out_dimension_dict[feature_name]\n\n    train_x_file_list = nn_label_norm_file_list[0:cfg.train_file_number]\n    train_y_file_list = nn_cmp_norm_file_list[0:cfg.train_file_number]\n    valid_x_file_list = nn_label_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n    valid_y_file_list = nn_cmp_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n    test_x_file_list  = nn_label_norm_file_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n    test_y_file_list  = nn_cmp_norm_file_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n\n    # we need to know the label dimension before training the DNN\n    # computing that requires us to look at the labels\n    #\n    # currently, there are two ways to do this\n    if cfg.label_style == \'HTS\':\n        label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name)\n        lab_dim = label_normaliser.dimension\n\n    elif cfg.label_style == \'composed\':\n        label_composer = LabelComposer()\n        label_composer.load_label_configuration(cfg.label_config_file)\n        lab_dim=label_composer.compute_label_dimension()\n\n    logger.info(\'label dimension is %d\' % lab_dim)\n\n    combined_model_arch = str(len(hidden_layers_sizes))\n    for hid_size in hidden_layers_sizes:\n        combined_model_arch += \'_\' + str(hid_size)\n\n#    nnets_file_name = \'%s/%s_%s_%d.%d.%d.%d.%d.train.%d.model\' \\\n#                       %(model_dir, cfg.model_type, cfg.combined_feature_name, int(cfg.multistream_switch),\n#                        len(hidden_layers_sizes), hidden_layers_sizes[0],\n#                        lab_dim, cfg.cmp_dim, cfg.train_file_number)\n\n    nnets_file_name = \'%s/%s_%s_%d_%s_%d.%d.train.%d.mdn.model\' \\\n                      %(model_dir, cfg.model_type, cfg.combined_feature_name, int(cfg.multistream_switch),\n                        combined_model_arch, lab_dim, cfg.cmp_dim, cfg.train_file_number)\n\n    ### DNN model training\n    if cfg.TRAINDNN:\n\n        logger.info(\'training DNN\')\n\n        try:\n            os.makedirs(model_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create model directory %s\' % model_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        try:\n            # print   \'start DNN\'\n            train_DNN(train_xy_file_list = (train_x_file_list, train_y_file_list), \\\n                      valid_xy_file_list = (valid_x_file_list, valid_y_file_list), \\\n                      nnets_file_name = nnets_file_name, \\\n                      n_ins = lab_dim, n_outs = cfg.cmp_dim, ms_outs = cfg.multistream_outs, \\\n                      hyper_params = cfg.hyper_params, buffer_size = cfg.buffer_size, \\\n                      mdn_component=cfg.mdn_component, var_floor=cfg.var_floor, \\\n                      plot = cfg.plot, beta_opt=cfg.beta_opt, \\\n                      eff_sample_size=cfg.eff_sample_size, mean_log_det=cfg.mean_log_det, \\\n                      start_from_trained_model=cfg.start_from_trained_model)\n        except KeyboardInterrupt:\n            logger.critical(\'train_DNN interrupted via keyboard\')\n            # Could \'raise\' the exception further, but that causes a deep traceback to be printed\n            # which we don\'t care about for a keyboard interrupt. So, just bail out immediately\n            sys.exit(1)\n        except:\n            logger.critical(\'train_DNN threw an exception\')\n            raise\n\n    ### generate parameters from DNN\n    temp_dir_name = \'%s_%s_%d_%d_%d_%d_%d_%d\' \\\n                    %(cfg.model_type, cfg.combined_feature_name, int(cfg.do_post_filtering), \\\n                      cfg.train_file_number, lab_dim, cfg.cmp_dim, \\\n                      len(hidden_layers_sizes), hidden_layers_sizes[0])\n    gen_dir = os.path.join(gen_dir, temp_dir_name)\n\n    gen_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n    test_x_file_list  = nn_label_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n    if cfg.DNNGEN:\n        logger.info(\'generating from DNN\')\n\n        try:\n            os.makedirs(gen_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create generation directory %s\' % gen_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        gen_file_list = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.cmp_ext)\n\n        fid = open(norm_info_file, \'rb\')\n        cmp_min_max = numpy.fromfile(fid, dtype=numpy.float32)\n        fid.close()\n        cmp_min_max = cmp_min_max.reshape((2, -1))\n        target_mean_vector = cmp_min_max[0, ]\n        target_std_vector = cmp_min_max[1, ]\n\n#        dnn_generation(valid_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)\n#        dnn_generation(test_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)\n        dnn_generation(test_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list, target_mean_vector, target_std_vector, cfg.out_dimension_dict, cfg.file_extension_dict)\n\n\n    ### generate wav\n    if cfg.GENWAV:\n        logger.info(\'reconstructing waveform(s)\')\n        generate_wav(gen_dir, gen_file_id_list, cfg)     # generated speech\n#       generate_wav(nn_cmp_dir, gen_file_id_list)  # reference copy synthesis speech\n\n    ### evaluation: calculate distortion\n    if cfg.CALMCD:\n        logger.info(\'calculating MCD\')\n\n        ref_data_dir = os.path.join(data_dir, \'ref_data\')\n\n        ref_mgc_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.mgc_ext)\n        ref_bap_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.bap_ext)\n        ref_lf0_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.lf0_ext)\n\n        in_gen_label_align_file_list = in_label_align_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n        calculator = IndividualDistortionComp()\n\n        spectral_distortion = 0.0\n        bap_mse             = 0.0\n        f0_mse              = 0.0\n        vuv_error           = 0.0\n\n        valid_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n        test_file_id_list  = file_id_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n        if cfg.remove_silence_using_binary_labels:\n            ## get lab_dim:\n            label_composer = LabelComposer()\n            label_composer.load_label_configuration(cfg.label_config_file)\n            lab_dim=label_composer.compute_label_dimension()\n\n            ## use first feature in label -- hardcoded for now\n            silence_feature = 0\n\n            ## Use these to trim silence:\n            untrimmed_test_labels = binary_label_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n\n        if \'mgc\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'mgc\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_mgc_list, cfg.mgc_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.mgc_dim, silence_pattern = [\'*-#+*\'])\n                remover.remove_silence(in_file_list_dict[\'mgc\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_mgc_list)\n            valid_spectral_distortion = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.mgc_ext, cfg.mgc_dim)\n            test_spectral_distortion  = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.mgc_ext, cfg.mgc_dim)\n            valid_spectral_distortion *= (10 /numpy.log(10)) * numpy.sqrt(2.0)    ##MCD\n            test_spectral_distortion  *= (10 /numpy.log(10)) * numpy.sqrt(2.0)    ##MCD\n\n\n        if \'bap\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'bap\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_bap_list, cfg.bap_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.bap_dim, silence_pattern = [\'*-#+*\'])\n                remover.remove_silence(in_file_list_dict[\'bap\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_bap_list)\n            valid_bap_mse        = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.bap_ext, cfg.bap_dim)\n            test_bap_mse         = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.bap_ext, cfg.bap_dim)\n            valid_bap_mse = valid_bap_mse / 10.0    ##Cassia\'s bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC\n            test_bap_mse  = test_bap_mse / 10.0    ##Cassia\'s bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC\n\n        if \'lf0\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'lf0\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_lf0_list, cfg.lf0_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.lf0_dim, silence_pattern = [\'*-#+*\'])\n                remover.remove_silence(in_file_list_dict[\'lf0\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_lf0_list)\n            valid_f0_mse, valid_vuv_error   = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.lf0_ext, cfg.lf0_dim)\n            test_f0_mse , test_vuv_error    = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.lf0_ext, cfg.lf0_dim)\n\n        logger.info(\'Develop: DNN -- MCD: %.3f dB; BAP: %.3f dB; F0: %.3f Hz; VUV: %.3f%%\' \\\n                    %(valid_spectral_distortion, valid_bap_mse, valid_f0_mse, valid_vuv_error*100.))\n        logger.info(\'Test   : DNN -- MCD: %.3f dB; BAP: %.3f dB; F0: %.3f Hz; VUV: %.3f%%\' \\\n                    %(test_spectral_distortion , test_bap_mse , test_f0_mse , test_vuv_error*100.))\n\n        # this can be removed\n        #\n        if  0: #to calculate distortion of HMM baseline\n            hmm_gen_no_silence_dir = \'/afs/inf.ed.ac.uk/group/project/dnn_tts/data/nick/nick_hmm_pf_2400_no_silence\'\n            hmm_gen_dir = \'/afs/inf.ed.ac.uk/group/project/dnn_tts/data/nick/nick_hmm_pf_2400\'\n\n            if 1:\n                hmm_mgc_list = prepare_file_path_list(gen_file_id_list, hmm_gen_dir, cfg.mgc_ext)\n                hmm_bap_list = prepare_file_path_list(gen_file_id_list, hmm_gen_dir, cfg.bap_ext)\n                hmm_lf0_list = prepare_file_path_list(gen_file_id_list, hmm_gen_dir, cfg.lf0_ext)\n\n                hmm_mgc_no_silence_list = prepare_file_path_list(gen_file_id_list, hmm_gen_no_silence_dir, cfg.mgc_ext)\n                hmm_bap_no_silence_list = prepare_file_path_list(gen_file_id_list, hmm_gen_no_silence_dir, cfg.bap_ext)\n                hmm_lf0_no_silence_list = prepare_file_path_list(gen_file_id_list, hmm_gen_no_silence_dir, cfg.lf0_ext)\n\n                in_gen_label_align_file_list = in_label_align_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                remover = SilenceRemover(n_cmp = cfg.mgc_dim, silence_pattern = [\'*-#+*\'])\n                remover.remove_silence(hmm_mgc_list, in_gen_label_align_file_list, hmm_mgc_no_silence_list)\n\n                remover = SilenceRemover(n_cmp = cfg.bap_dim, silence_pattern = [\'*-#+*\'])\n                remover.remove_silence(hmm_bap_list, in_gen_label_align_file_list, hmm_bap_no_silence_list)\n\n                remover = SilenceRemover(n_cmp = cfg.lf0_dim, silence_pattern = [\'*-#+*\'])\n                remover.remove_silence(hmm_lf0_list, in_gen_label_align_file_list, hmm_lf0_no_silence_list)\n\n            calculator = IndividualDistortionComp()\n\n            spectral_distortion = calculator.compute_distortion(valid_file_id_list, ref_data_dir, hmm_gen_no_silence_dir, cfg.mgc_ext, cfg.mgc_dim)\n            bap_mse             = calculator.compute_distortion(valid_file_id_list, ref_data_dir, hmm_gen_no_silence_dir, cfg.bap_ext, cfg.bap_dim)\n            f0_mse, vuv_error   = calculator.compute_distortion(valid_file_id_list, ref_data_dir, hmm_gen_no_silence_dir, cfg.lf0_ext, cfg.lf0_dim)\n\n            spectral_distortion *= (10 /numpy.log(10)) * numpy.sqrt(2.0)\n            bap_mse = bap_mse / 10.0\n\n            logger.info(\'Develop: HMM -- MCD: %.3f dB; BAP: %.3f dB; F0: %.3f Hz; VUV: %.3f%%\' %(spectral_distortion, bap_mse, f0_mse, vuv_error*100.))\n\n            spectral_distortion = calculator.compute_distortion(test_file_id_list, ref_data_dir, hmm_gen_no_silence_dir, cfg.mgc_ext, cfg.mgc_dim)\n            bap_mse             = calculator.compute_distortion(test_file_id_list, ref_data_dir, hmm_gen_no_silence_dir, cfg.bap_ext, cfg.bap_dim)\n            f0_mse, vuv_error   = calculator.compute_distortion(test_file_id_list, ref_data_dir, hmm_gen_no_silence_dir, cfg.lf0_ext, cfg.lf0_dim)\n\n            spectral_distortion *= (10 /numpy.log(10)) * numpy.sqrt(2.0)\n            bap_mse = bap_mse / 10.0\n\n            logger.info(\'Test   : HMM -- MCD: %.3f dB; BAP: %.3f dB; F0: %.3f Hz; VUV: %.3f%%\' %(spectral_distortion, bap_mse, f0_mse, vuv_error*100.))\n\nif __name__ == \'__main__\':\n\n\n\n    # these things should be done even before trying to parse the command line\n\n    # create a configuration instance\n    # and get a short name for this instance\n    cfg=configuration.cfg\n\n    # set up logging to use our custom class\n    logging.setLoggerClass(LoggerPlotter)\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n\n    if len(sys.argv) != 2:\n        logger.critical(\'usage: run_dnn.sh [config file name]\')\n        sys.exit(1)\n\n    config_file = sys.argv[1]\n\n    config_file = os.path.abspath(config_file)\n    cfg.configure(config_file)\n\n    if cfg.profile:\n        logger.info(\'profiling is activated\')\n        import cProfile, pstats\n        cProfile.run(\'main_function(cfg)\', \'mainstats\')\n\n        # create a stream for the profiler to write to\n        profiling_output = io.StringIO()\n        p = pstats.Stats(\'mainstats\', stream=profiling_output)\n\n        # print stats to that stream\n        # here we just report the top 10 functions, sorted by total amount of time spent in each\n        p.strip_dirs().sort_stats(\'tottime\').print_stats(10)\n\n        # print the result to the log\n        logger.info(\'---Profiling result follows---\\n%s\' %  profiling_output.getvalue() )\n        profiling_output.close()\n        logger.info(\'---End of profiling result---\')\n\n    else:\n        main_function(cfg)\n\n    sys.exit(0)\n'"
src/work_in_progress/run_merlin_v2.py,0,"b'\nimport pickle\nimport gzip\nimport os, sys, errno\nimport time\nimport math\n\n#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)\nimport numpy\n#import gnumpy as gnp\n# we need to explicitly import this in some cases, not sure why this doesn\'t get imported with numpy itself\nimport numpy.distutils.__config__\n# and only after that can we import theano\nimport theano\n\nfrom utils.providers import ListDataProvider\n\nfrom frontend.label_normalisation import HTSLabelNormalisation, XMLLabelNormalisation\nfrom frontend.silence_remover import SilenceRemover\nfrom frontend.silence_remover import trim_silence\nfrom frontend.min_max_norm import MinMaxNormalisation\nfrom frontend.acoustic_composition import AcousticComposition\nfrom frontend.parameter_generation import ParameterGeneration\nfrom frontend.mean_variance_norm import MeanVarianceNorm\n\n# the new class for label composition and normalisation\nfrom frontend.label_composer import LabelComposer\nfrom frontend.label_modifier import HTSLabelModification\n#from frontend.mlpg_fast import MLParameterGenerationFast\n\n#from frontend.mlpg_fast_layer import MLParameterGenerationFastLayer\n\n\nimport configuration\nfrom models.deep_rnn import DeepRecurrentNetwork\n\nfrom utils.compute_distortion import DistortionComputation, IndividualDistortionComp\nfrom utils.generate import generate_wav\nfrom utils.learn_rates import ExpDecreaseLearningRate\n\nfrom io_funcs.binary_io import  BinaryIOCollection\n\n#import matplotlib.pyplot as plt\n# our custom logging class that can also plot\n#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot\nfrom logplot.logging_plotting import LoggerPlotter, MultipleSeriesPlot, SingleWeightMatrixPlot\nimport logging # as logging\nimport logging.config\nimport io\n\n\ndef extract_file_id_list(file_list):\n    file_id_list = []\n    for file_name in file_list:\n        file_id = os.path.basename(os.path.splitext(file_name)[0])\n        file_id_list.append(file_id)\n\n    return  file_id_list\n\ndef read_file_list(file_name):\n\n    logger = logging.getLogger(""read_file_list"")\n\n    file_lists = []\n    fid = open(file_name)\n    for line in fid.readlines():\n        line = line.strip()\n        if len(line) < 1:\n            continue\n        file_lists.append(line)\n    fid.close()\n\n    logger.debug(\'Read file list from %s\' % file_name)\n    return  file_lists\n\n\ndef make_output_file_list(out_dir, in_file_lists):\n    out_file_lists = []\n\n    for in_file_name in in_file_lists:\n        file_id = os.path.basename(in_file_name)\n        out_file_name = out_dir + \'/\' + file_id\n        out_file_lists.append(out_file_name)\n\n    return  out_file_lists\n\ndef prepare_file_path_list(file_id_list, file_dir, file_extension, new_dir_switch=True):\n    if not os.path.exists(file_dir) and new_dir_switch:\n        os.makedirs(file_dir)\n    file_name_list = []\n    for file_id in file_id_list:\n        file_name = file_dir + \'/\' + file_id + file_extension\n        file_name_list.append(file_name)\n\n    return  file_name_list\n\n\n\ndef visualize_dnn(dnn):\n\n    layer_num = len(dnn.params)     ## including input and output\n    plotlogger = logging.getLogger(""plotting"")\n\n    for i in range(layer_num):\n        fig_name = \'Activation weights W\' + str(i) + \'_\' + dnn.params[i].name\n        fig_title = \'Activation weights of W\' + str(i)\n        xlabel = \'Neuron index of hidden layer \' + str(i)\n        ylabel = \'Neuron index of hidden layer \' + str(i+1)\n        if i == 0:\n            xlabel = \'Input feature index\'\n        if i == layer_num-1:\n            ylabel = \'Output feature index\'\n\n        aa = dnn.params[i].get_value(borrow=True).T\n        print(aa.shape, aa.size)\n        if aa.size > aa.shape[0]:\n            logger.create_plot(fig_name, SingleWeightMatrixPlot)\n            plotlogger.add_plot_point(fig_name, fig_name, dnn.params[i].get_value(borrow=True).T)\n            plotlogger.save_plot(fig_name, title=fig_name, xlabel=xlabel, ylabel=ylabel)\n\ndef load_covariance(var_file_dict, out_dimension_dict):\n    var = {}\n    io_funcs = BinaryIOCollection()\n    for feature_name in list(var_file_dict.keys()):\n        var_values, dimension = io_funcs.load_binary_file_frame(var_file_dict[feature_name], 1)\n\n        var_values = numpy.reshape(var_values, (out_dimension_dict[feature_name], 1))\n\n        var[feature_name] = var_values\n\n    return  var\n\n\ndef train_DNN(train_xy_file_list, valid_xy_file_list, \\\n              nnets_file_name, n_ins, n_outs, ms_outs, hyper_params, buffer_size, plot=False, var_dict=None,\n              cmp_mean_vector = None, cmp_std_vector = None, init_dnn_model_file = None, seq_dur_file_list = None):\n\n    # get loggers for this function\n    # this one writes to both console and file\n    logger = logging.getLogger(""main.train_DNN"")\n    logger.debug(\'Starting train_DNN\')\n\n    if plot:\n        # this one takes care of plotting duties\n        plotlogger = logging.getLogger(""plotting"")\n        # create an (empty) plot of training convergence, ready to receive data points\n        logger.create_plot(\'training convergence\',MultipleSeriesPlot)\n\n    try:\n        assert numpy.sum(ms_outs) == n_outs\n    except AssertionError:\n        logger.critical(\'the summation of multi-stream outputs does not equal to %d\' %(n_outs))\n        raise\n\n    ####parameters#####\n    finetune_lr     = float(hyper_params[\'learning_rate\'])\n    training_epochs = int(hyper_params[\'training_epochs\'])\n    batch_size      = int(hyper_params[\'batch_size\'])\n    l1_reg          = float(hyper_params[\'l1_reg\'])\n    l2_reg          = float(hyper_params[\'l2_reg\'])\n    warmup_epoch    = int(hyper_params[\'warmup_epoch\'])\n    momentum        = float(hyper_params[\'momentum\'])\n    warmup_momentum = float(hyper_params[\'warmup_momentum\'])\n\n    hidden_layer_size = hyper_params[\'hidden_layer_size\']\n\n    buffer_utt_size = buffer_size\n    early_stop_epoch = int(hyper_params[\'early_stop_epochs\'])\n\n    hidden_activation = hyper_params[\'hidden_activation\']\n    output_activation = hyper_params[\'output_activation\']\n\n    model_type = hyper_params[\'model_type\']\n    hidden_layer_type  = hyper_params[\'hidden_layer_type\']\n\n    ## use a switch to turn on pretraining\n    ## pretraining may not help too much, if this case, we turn it off to save time\n    do_pretraining = hyper_params[\'do_pretraining\']\n    pretraining_epochs = int(hyper_params[\'pretraining_epochs\'])\n    pretraining_lr = float(hyper_params[\'pretraining_lr\'])\n\n    sequential_training = hyper_params[\'sequential_training\']\n    dropout_rate = hyper_params[\'dropout_rate\']\n\n#    sequential_training = True\n\n    buffer_size = int(buffer_size / batch_size) * batch_size\n\n    ###################\n    (train_x_file_list, train_y_file_list) = train_xy_file_list\n    (valid_x_file_list, valid_y_file_list) = valid_xy_file_list\n\n    if cfg.network_type != \'S2S\':\n        seq_dur_file_list = None\n\n    if not seq_dur_file_list:\n        train_dur_file_list = None\n        valid_dur_file_list = None\n    else:\n        label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name, subphone_feats=""coarse_coding"")\n        train_dur_file_list = seq_dur_file_list[0:cfg.train_file_number]\n        valid_dur_file_list = seq_dur_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n\n    logger.debug(\'Creating training   data provider\')\n    train_data_reader = ListDataProvider(x_file_list = train_x_file_list, y_file_list = train_y_file_list, dur_file_list = train_dur_file_list,\n                            n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, sequential = sequential_training, network_type=cfg.network_type, shuffle = True)\n\n    logger.debug(\'Creating validation data provider\')\n    valid_data_reader = ListDataProvider(x_file_list = valid_x_file_list, y_file_list = valid_y_file_list, dur_file_list = valid_dur_file_list,\n                            n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, sequential = sequential_training, network_type=cfg.network_type, shuffle = False)\n\n\n    if cfg.network_type == \'S2S\':\n        shared_train_set_xyd, temp_train_set_x, temp_train_set_y, temp_train_set_d = train_data_reader.load_one_partition()\n        shared_valid_set_xyd, temp_valid_set_x, temp_valid_set_y, temp_valid_set_d = valid_data_reader.load_one_partition()\n        train_set_x, train_set_y, train_set_d = shared_train_set_xyd\n        valid_set_x, valid_set_y, valid_set_d = shared_valid_set_xyd\n\n        temp_train_set_f = label_normaliser.extract_durational_features(dur_data=temp_train_set_d)\n        temp_valid_set_f = label_normaliser.extract_durational_features(dur_data=temp_valid_set_d)\n        train_set_f = theano.shared(numpy.asarray(temp_train_set_f, dtype=theano.config.floatX), name=\'f\', borrow=True)\n        valid_set_f = theano.shared(numpy.asarray(temp_valid_set_f, dtype=theano.config.floatX), name=\'f\', borrow=True)\n    else:\n        shared_train_set_xy, temp_train_set_x, temp_train_set_y = train_data_reader.load_one_partition()\n        shared_valid_set_xy, temp_valid_set_x, temp_valid_set_y = valid_data_reader.load_one_partition()\n        train_set_x, train_set_y = shared_train_set_xy\n        valid_set_x, valid_set_y = shared_valid_set_xy\n    train_data_reader.reset()\n    valid_data_reader.reset()\n\n\n    ##temporally we use the training set as pretrain_set_x.\n    ##we need to support any data for pretraining\n#    pretrain_set_x = train_set_x\n\n    # numpy random generator\n    numpy_rng = numpy.random.RandomState(123)\n    logger.info(\'building the model\')\n\n\n    dnn_model = None\n    pretrain_fn = None  ## not all the model support pretraining right now\n    train_fn = None\n    valid_fn = None\n    valid_model = None ## valid_fn and valid_model are the same. reserve to computer multi-stream distortion\n    if model_type == \'DNN\':\n        dnn_model = DeepRecurrentNetwork(n_in= n_ins, hidden_layer_size = hidden_layer_size, n_out = n_outs,\n                                         L1_reg = l1_reg, L2_reg = l2_reg, hidden_layer_type = hidden_layer_type, output_type=cfg.output_layer_type, network_type=cfg.network_type, dropout_rate = dropout_rate)\n        if cfg.network_type == \'S2S\':\n            train_fn, valid_fn = dnn_model.build_finetune_functions_S2SPF(\n                        (train_set_x, train_set_y, train_set_d, train_set_f), (valid_set_x, valid_set_y, valid_set_d, valid_set_f))\n        else:\n            train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y))  #, batch_size=batch_size\n\n    else:\n        logger.critical(\'%s type NN model is not supported!\' %(model_type))\n        raise\n\n    logger.info(\'fine-tuning the %s model\' %(model_type))\n\n    start_time = time.time()\n\n    best_dnn_model = dnn_model\n    best_validation_loss = sys.float_info.max\n    previous_loss = sys.float_info.max\n\n    early_stop = 0\n    epoch = 0\n\n#    finetune_lr = 0.000125\n    previous_finetune_lr = finetune_lr\n\n    print(finetune_lr)\n\n    while (epoch < training_epochs):\n        epoch = epoch + 1\n\n        current_momentum = momentum\n        current_finetune_lr = finetune_lr\n        if epoch <= warmup_epoch:\n            current_finetune_lr = finetune_lr\n            current_momentum = warmup_momentum\n        else:\n            current_finetune_lr = previous_finetune_lr * 0.5\n\n        previous_finetune_lr = current_finetune_lr\n\n        train_error = []\n        sub_start_time = time.time()\n\n        while (not train_data_reader.is_finish()):\n\n            if cfg.network_type == \'S2S\':\n                shared_train_set_xyd, temp_train_set_x, temp_train_set_y, temp_train_set_d = train_data_reader.load_one_partition()\n                temp_train_set_f = label_normaliser.extract_durational_features(dur_data=temp_train_set_d)\n                train_set_d.set_value(numpy.asarray(temp_train_set_d, dtype=\'int32\'), borrow=True)\n                train_set_f.set_value(numpy.asarray(temp_train_set_f, dtype=theano.config.floatX), borrow=True)\n            else:\n                shared_train_set_xy, temp_train_set_x, temp_train_set_y = train_data_reader.load_one_partition()\n\n            # if sequential training, the batch size will be the number of frames in an utterance\n\n            if sequential_training == True:\n                #batch_size = temp_train_set_x.shape[0]\n\n                train_set_x.set_value(numpy.asarray(temp_train_set_x, dtype=theano.config.floatX), borrow=True)\n                train_set_y.set_value(numpy.asarray(temp_train_set_y, dtype=theano.config.floatX), borrow=True)\n\n                this_train_error = train_fn(current_finetune_lr, current_momentum)\n                train_error.append(this_train_error)\n                #print train_set_x.eval().shape, train_set_y.eval().shape, this_train_error\n\n            else:\n                n_train_batches = temp_train_set_x.shape[0] / batch_size\n                for index in range(n_train_batches):\n                    ## send a batch to the shared variable, rather than pass the batch size and batch index to the finetune function\n                    train_set_x.set_value(numpy.asarray(temp_train_set_x[index*batch_size:(index + 1)*batch_size], dtype=theano.config.floatX), borrow=True)\n                    train_set_y.set_value(numpy.asarray(temp_train_set_y[index*batch_size:(index + 1)*batch_size], dtype=theano.config.floatX), borrow=True)\n\n                    this_train_error = train_fn(current_finetune_lr, current_momentum)\n                    train_error.append(this_train_error)\n\n        train_data_reader.reset()\n\n        logger.debug(\'calculating validation loss\')\n        validation_losses = []\n        while (not valid_data_reader.is_finish()):\n\n            if cfg.network_type == \'S2S\':\n                shared_valid_set_xyd, temp_valid_set_x, temp_valid_set_y, temp_valid_set_d = valid_data_reader.load_one_partition()\n                temp_valid_set_f = label_normaliser.extract_durational_features(dur_data=temp_valid_set_d)\n                valid_set_d.set_value(numpy.asarray(temp_valid_set_d, dtype=\'int32\'), borrow=True)\n                valid_set_f.set_value(numpy.asarray(temp_valid_set_f, dtype=theano.config.floatX), borrow=True)\n            else:\n                shared_valid_set_xy, temp_valid_set_x, temp_valid_set_y = valid_data_reader.load_one_partition()\n\n            valid_set_x.set_value(numpy.asarray(temp_valid_set_x, dtype=theano.config.floatX), borrow=True)\n            valid_set_y.set_value(numpy.asarray(temp_valid_set_y, dtype=theano.config.floatX), borrow=True)\n\n            this_valid_loss = valid_fn()\n\n            validation_losses.append(this_valid_loss)\n        valid_data_reader.reset()\n\n        this_validation_loss = numpy.mean(validation_losses)\n\n        this_train_valid_loss = numpy.mean(numpy.asarray(train_error))\n\n        sub_end_time = time.time()\n\n        loss_difference = this_validation_loss - previous_loss\n\n        logger.info(\'epoch %i, validation error %f, train error %f  time spent %.2f\' %(epoch, this_validation_loss, this_train_valid_loss, (sub_end_time - sub_start_time)))\n        if plot:\n            plotlogger.add_plot_point(\'training convergence\',\'validation set\',(epoch,this_validation_loss))\n            plotlogger.add_plot_point(\'training convergence\',\'training set\',(epoch,this_train_valid_loss))\n            plotlogger.save_plot(\'training convergence\',title=\'Progress of training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n\n        if this_validation_loss < best_validation_loss:\n            if epoch > 5:\n                pickle.dump(best_dnn_model, open(nnets_file_name, \'wb\'))\n\n            best_dnn_model = dnn_model\n            best_validation_loss = this_validation_loss\n#            logger.debug(\'validation loss decreased, so saving model\')\n\n        if this_validation_loss >= previous_loss:\n            logger.debug(\'validation loss increased\')\n\n#            dbn = best_dnn_model\n            early_stop += 1\n\n        if epoch > 15 and early_stop > early_stop_epoch:\n            logger.debug(\'stopping early\')\n            break\n\n        if math.isnan(this_validation_loss):\n            break\n\n        previous_loss = this_validation_loss\n\n    end_time = time.time()\n#    cPickle.dump(best_dnn_model, open(nnets_file_name, \'wb\'))\n\n    logger.info(\'overall  training time: %.2fm validation error %f\' % ((end_time - start_time) / 60., best_validation_loss))\n\n    if plot:\n        plotlogger.save_plot(\'training convergence\',title=\'Final training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n\n    return  best_validation_loss\n\n\ndef dnn_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):  #file_number\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        test_set_x = features.reshape((-1, n_ins))\n\n        predicted_parameter = dnn_model.parameter_prediction(test_set_x)\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\ndef dnn_generation_S2S(valid_file_list, valid_dur_file_list, nnets_file_name, n_ins, n_outs, out_file_list):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    file_number = len(valid_file_list)\n\n    label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name, subphone_feats=""coarse_coding"")\n    for i in range(file_number):  #file_number\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        test_set_x = features.reshape((-1, n_ins))\n\n        fid_lab = open(valid_dur_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        test_set_d = features.astype(numpy.int32)\n\n        dur_features = label_normaliser.extract_durational_features(dur_data=test_set_d)\n        test_set_f = dur_features.astype(numpy.float32)\n\n        predicted_parameter = dnn_model.parameter_prediction_S2SPF(test_set_x, test_set_d, test_set_f)\n\n        #print b_indices\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\ndef dnn_generation_lstm(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    visualize_dnn(dnn_model)\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):  #file_number\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        test_set_x = features.reshape((-1, n_ins))\n\n        predicted_parameter = dnn_model.parameter_prediction_lstm(test_set_x)\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\n##generate bottleneck layer as festures\ndef dnn_hidden_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        features = features.reshape((-1, n_ins))\n        temp_set_x = features.tolist()\n        test_set_x = theano.shared(numpy.asarray(temp_set_x, dtype=theano.config.floatX))\n\n        predicted_parameter = dnn_model.generate_top_hidden_layer(test_set_x=test_set_x)\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\n\ndef main_function(cfg):\n\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n    # get another logger to handle plotting duties\n    plotlogger = logging.getLogger(""plotting"")\n\n    # later, we might do this via a handler that is created, attached and configured\n    # using the standard config mechanism of the logging module\n    # but for now we need to do it manually\n    plotlogger.set_plot_path(cfg.plot_dir)\n\n    #### parameter setting########\n    hidden_layer_size = cfg.hyper_params[\'hidden_layer_size\']\n\n\n    ####prepare environment\n\n    try:\n        file_id_list = read_file_list(cfg.file_id_scp)\n        logger.debug(\'Loaded file id list from %s\' % cfg.file_id_scp)\n    except IOError:\n        # this means that open(...) threw an error\n        logger.critical(\'Could not load file id list from %s\' % cfg.file_id_scp)\n        raise\n\n    ###total file number including training, development, and testing\n    total_file_number = len(file_id_list)\n\n    data_dir = cfg.data_dir\n\n    nn_cmp_dir       = os.path.join(data_dir, \'nn\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim))\n    nn_cmp_norm_dir   = os.path.join(data_dir, \'nn_norm\'  + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim))\n\n    model_dir = os.path.join(cfg.work_dir, \'nnets_model\')\n    gen_dir   = os.path.join(cfg.work_dir, \'gen\')\n\n    in_file_list_dict = {}\n\n    for feature_name in list(cfg.in_dir_dict.keys()):\n        in_file_list_dict[feature_name] = prepare_file_path_list(file_id_list, cfg.in_dir_dict[feature_name], cfg.file_extension_dict[feature_name], False)\n\n    nn_cmp_file_list         = prepare_file_path_list(file_id_list, nn_cmp_dir, cfg.cmp_ext)\n    nn_cmp_norm_file_list    = prepare_file_path_list(file_id_list, nn_cmp_norm_dir, cfg.cmp_ext)\n\n    ###normalisation information\n    norm_info_file = os.path.join(data_dir, \'norm_info\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim) + \'_\' + cfg.output_feature_normalisation + \'.dat\')\n\n    ### normalise input full context label\n    # currently supporting two different forms of lingustic features\n    # later, we should generalise this\n\n    if cfg.label_style == \'HTS\':\n        label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name, add_frame_features=cfg.add_frame_features, subphone_feats=cfg.subphone_feats)\n        lab_dim = label_normaliser.dimension + cfg.appended_input_dim\n        logger.info(\'Input label dimension is %d\' % lab_dim)\n        suffix=str(lab_dim)\n    # no longer supported - use new ""composed"" style labels instead\n    elif cfg.label_style == \'composed\':\n        # label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)\n        suffix=\'composed\'\n\n    if cfg.process_labels_in_work_dir:\n        label_data_dir = cfg.work_dir\n    else:\n        label_data_dir = data_dir\n\n    # the number can be removed\n    binary_label_dir      = os.path.join(label_data_dir, \'binary_label_\'+suffix)\n    nn_label_dir          = os.path.join(label_data_dir, \'nn_no_silence_lab_\'+suffix)\n    nn_label_norm_dir     = os.path.join(label_data_dir, \'nn_no_silence_lab_norm_\'+suffix)\n\n    in_label_align_file_list = prepare_file_path_list(file_id_list, cfg.in_label_align_dir, cfg.lab_ext, False)\n    binary_label_file_list   = prepare_file_path_list(file_id_list, binary_label_dir, cfg.lab_ext)\n    nn_label_file_list       = prepare_file_path_list(file_id_list, nn_label_dir, cfg.lab_ext)\n    nn_label_norm_file_list  = prepare_file_path_list(file_id_list, nn_label_norm_dir, cfg.lab_ext)\n    dur_file_list            = prepare_file_path_list(file_id_list, cfg.in_dur_dir, cfg.dur_ext)\n    seq_dur_file_list = prepare_file_path_list(file_id_list, cfg.in_seq_dur_dir, cfg.dur_ext)\n    lf0_file_list            = prepare_file_path_list(file_id_list, cfg.in_lf0_dir, cfg.lf0_ext)\n\n    # to do - sanity check the label dimension here?\n\n\n\n    min_max_normaliser = None\n    label_norm_file = \'label_norm_%s_%d.dat\' %(cfg.label_style, lab_dim)\n    label_norm_file = os.path.join(label_data_dir, label_norm_file)\n\n    if cfg.GenTestList:\n        try:\n            test_id_list = read_file_list(cfg.test_id_scp)\n            logger.debug(\'Loaded file id list from %s\' % cfg.test_id_scp)\n        except IOError:\n            # this means that open(...) threw an error\n            logger.critical(\'Could not load file id list from %s\' % cfg.test_id_scp)\n            raise\n\n        in_label_align_file_list = prepare_file_path_list(test_id_list, cfg.in_label_align_dir, cfg.lab_ext, False)\n        binary_label_file_list   = prepare_file_path_list(test_id_list, binary_label_dir, cfg.lab_ext)\n        nn_label_file_list       = prepare_file_path_list(test_id_list, nn_label_dir, cfg.lab_ext)\n        nn_label_norm_file_list  = prepare_file_path_list(test_id_list, nn_label_norm_dir, cfg.lab_ext)\n\n    if cfg.NORMLAB and (cfg.label_style == \'HTS\'):\n        # simple HTS labels\n        logger.info(\'preparing label data (input) using standard HTS style labels\')\n        label_normaliser.perform_normalisation(in_label_align_file_list, binary_label_file_list, label_type=cfg.label_type)\n\n        remover = SilenceRemover(n_cmp = lab_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type, remove_frame_features = cfg.add_frame_features, subphone_feats = cfg.subphone_feats)\n        remover.remove_silence(binary_label_file_list, in_label_align_file_list, nn_label_file_list)\n\n        min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99)\n        ###use only training data to find min-max information, then apply on the whole dataset\n        if cfg.GenTestList:\n            min_max_normaliser.load_min_max_values(label_norm_file)\n        else:\n            min_max_normaliser.find_min_max_values(nn_label_file_list[0:cfg.train_file_number])\n        min_max_normaliser.normalise_data(nn_label_file_list, nn_label_norm_file_list)\n\n        ### make duration data for S2S network ###\n        if cfg.network_type == ""S2S"":\n            logger.info(\'creating duration (input) features for S2S network\')\n            label_normaliser.prepare_dur_data(in_label_align_file_list, seq_dur_file_list, feature_type=""numerical"", unit_size=""phoneme"")\n\n            if cfg.remove_silence_from_dur:\n                remover = SilenceRemover(n_cmp = cfg.seq_dur_dim, silence_pattern = cfg.silence_pattern, remove_frame_features = cfg.add_frame_features)\n                remover.remove_silence(seq_dur_file_list, in_label_align_file_list, seq_dur_file_list)\n\n    if cfg.NORMLAB and (cfg.label_style == \'composed\'):\n        # new flexible label preprocessor\n\n        logger.info(\'preparing label data (input) using ""composed"" style labels\')\n        label_composer = LabelComposer()\n        label_composer.load_label_configuration(cfg.label_config_file)\n\n        logger.info(\'Loaded label configuration\')\n        # logger.info(\'%s\' % label_composer.configuration.labels )\n\n        lab_dim=label_composer.compute_label_dimension()\n        logger.info(\'label dimension will be %d\' % lab_dim)\n\n        if cfg.precompile_xpaths:\n            label_composer.precompile_xpaths()\n\n        # there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)\n        # create all the lists of these, ready to pass to the label composer\n\n        in_label_align_file_list = {}\n        for label_style, label_style_required in label_composer.label_styles.items():\n            if label_style_required:\n                logger.info(\'labels of style %s are required - constructing file paths for them\' % label_style)\n                if label_style == \'xpath\':\n                    in_label_align_file_list[\'xpath\'] = prepare_file_path_list(file_id_list, cfg.xpath_label_align_dir, cfg.utt_ext, False)\n                elif label_style == \'hts\':\n                    in_label_align_file_list[\'hts\'] = prepare_file_path_list(file_id_list, cfg.hts_label_align_dir, cfg.lab_ext, False)\n                else:\n                    logger.critical(\'unsupported label style %s specified in label configuration\' % label_style)\n                    raise Exception\n\n            # now iterate through the files, one at a time, constructing the labels for them\n            num_files=len(file_id_list)\n            logger.info(\'the label styles required are %s\' % label_composer.label_styles)\n\n            for i in range(num_files):\n                logger.info(\'making input label features for %4d of %4d\' % (i+1,num_files))\n\n                # iterate through the required label styles and open each corresponding label file\n\n                # a dictionary of file descriptors, pointing at the required files\n                required_labels={}\n\n                for label_style, label_style_required in label_composer.label_styles.items():\n\n                    # the files will be a parallel set of files for a single utterance\n                    # e.g., the XML tree and an HTS label file\n                    if label_style_required:\n                        required_labels[label_style] = open(in_label_align_file_list[label_style][i] , \'r\')\n                        logger.debug(\' opening label file %s\' % in_label_align_file_list[label_style][i])\n\n                logger.debug(\'label styles with open files: %s\' % required_labels)\n                label_composer.make_labels(required_labels,out_file_name=binary_label_file_list[i],fill_missing_values=cfg.fill_missing_values,iterate_over_frames=cfg.iterate_over_frames)\n\n                # now close all opened files\n                for fd in required_labels.values():\n                    fd.close()\n\n\n        # silence removal\n        if cfg.remove_silence_using_binary_labels:\n            silence_feature = 0 ## use first feature in label -- hardcoded for now\n            logger.info(\'Silence removal from label using silence feature: %s\'%(label_composer.configuration.labels[silence_feature]))\n            logger.info(\'Silence will be removed from CMP files in same way\')\n            ## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:\n            trim_silence(binary_label_file_list, nn_label_file_list, lab_dim, \\\n                                binary_label_file_list, lab_dim, silence_feature)\n        else:\n            logger.info(\'No silence removal done\')\n            # start from the labels we have just produced, not trimmed versions\n            nn_label_file_list = binary_label_file_list\n\n        min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99)\n        ###use only training data to find min-max information, then apply on the whole dataset\n        min_max_normaliser.find_min_max_values(nn_label_file_list[0:cfg.train_file_number])\n        min_max_normaliser.normalise_data(nn_label_file_list, nn_label_norm_file_list)\n\n    if min_max_normaliser != None and not cfg.GenTestList:\n        ### save label normalisation information for unseen testing labels\n        label_min_vector = min_max_normaliser.min_vector\n        label_max_vector = min_max_normaliser.max_vector\n        label_norm_info = numpy.concatenate((label_min_vector, label_max_vector), axis=0)\n\n        label_norm_info = numpy.array(label_norm_info, \'float32\')\n        fid = open(label_norm_file, \'wb\')\n        label_norm_info.tofile(fid)\n        fid.close()\n        logger.info(\'saved %s vectors to %s\' %(label_min_vector.size, label_norm_file))\n\n\n    ### make output duration data\n    if cfg.MAKEDUR:\n        logger.info(\'creating duration (output) features\')\n        feature_type = cfg.dur_feature_type\n        label_normaliser.prepare_dur_data(in_label_align_file_list, dur_file_list, feature_type)\n\n\n    ### make output acoustic data\n    if cfg.MAKECMP:\n        logger.info(\'creating acoustic (output) features\')\n        delta_win = cfg.delta_win #[-0.5, 0.0, 0.5]\n        acc_win = cfg.acc_win     #[1.0, -2.0, 1.0]\n\n        acoustic_worker = AcousticComposition(delta_win = delta_win, acc_win = acc_win)\n        if \'dur\' in list(cfg.in_dir_dict.keys()) and cfg.AcousticModel:\n            acoustic_worker.make_equal_frames(dur_file_list, lf0_file_list, cfg.in_dimension_dict)\n        acoustic_worker.prepare_nn_data(in_file_list_dict, nn_cmp_file_list, cfg.in_dimension_dict, cfg.out_dimension_dict)\n\n        if cfg.remove_silence_using_binary_labels:\n            ## do this to get lab_dim:\n            label_composer = LabelComposer()\n            label_composer.load_label_configuration(cfg.label_config_file)\n            lab_dim=label_composer.compute_label_dimension()\n\n            silence_feature = 0 ## use first feature in label -- hardcoded for now\n            logger.info(\'Silence removal from CMP using binary label file\')\n\n            ## overwrite the untrimmed audio with the trimmed version:\n            trim_silence(nn_cmp_file_list, nn_cmp_file_list, cfg.cmp_dim,\n                                binary_label_file_list, lab_dim, silence_feature)\n\n        else: ## back off to previous method using HTS labels:\n            remover = SilenceRemover(n_cmp = cfg.cmp_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type, remove_frame_features = cfg.add_frame_features, subphone_feats = cfg.subphone_feats)\n            remover.remove_silence(nn_cmp_file_list[0:cfg.train_file_number+cfg.valid_file_number],\n                                   in_label_align_file_list[0:cfg.train_file_number+cfg.valid_file_number],\n                                   nn_cmp_file_list[0:cfg.train_file_number+cfg.valid_file_number]) # save to itself\n\n    ### save acoustic normalisation information for normalising the features back\n    var_dir   = os.path.join(data_dir, \'var\')\n    if not os.path.exists(var_dir):\n        os.makedirs(var_dir)\n\n    var_file_dict = {}\n    for feature_name in list(cfg.out_dimension_dict.keys()):\n        var_file_dict[feature_name] = os.path.join(var_dir, feature_name + \'_\' + str(cfg.out_dimension_dict[feature_name]))\n\n    ### normalise output acoustic data\n    if cfg.NORMCMP:\n        logger.info(\'normalising acoustic (output) features using method %s\' % cfg.output_feature_normalisation)\n        cmp_norm_info = None\n        if cfg.output_feature_normalisation == \'MVN\':\n            normaliser = MeanVarianceNorm(feature_dimension=cfg.cmp_dim)\n            ###calculate mean and std vectors on the training data, and apply on the whole dataset\n            global_mean_vector = normaliser.compute_mean(nn_cmp_file_list[0:cfg.train_file_number], 0, cfg.cmp_dim)\n            global_std_vector = normaliser.compute_std(nn_cmp_file_list[0:cfg.train_file_number], global_mean_vector, 0, cfg.cmp_dim)\n\n            normaliser.feature_normalisation(nn_cmp_file_list[0:cfg.train_file_number+cfg.valid_file_number],\n                                             nn_cmp_norm_file_list[0:cfg.train_file_number+cfg.valid_file_number])\n            cmp_norm_info = numpy.concatenate((global_mean_vector, global_std_vector), axis=0)\n\n        elif cfg.output_feature_normalisation == \'MINMAX\':\n            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim)\n            global_mean_vector = min_max_normaliser.compute_mean(nn_cmp_file_list[0:cfg.train_file_number])\n            global_std_vector = min_max_normaliser.compute_std(nn_cmp_file_list[0:cfg.train_file_number], global_mean_vector)\n\n            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim, min_value = 0.01, max_value = 0.99)\n            min_max_normaliser.find_min_max_values(nn_cmp_file_list[0:cfg.train_file_number])\n            min_max_normaliser.normalise_data(nn_cmp_file_list, nn_cmp_norm_file_list)\n\n            cmp_min_vector = min_max_normaliser.min_vector\n            cmp_max_vector = min_max_normaliser.max_vector\n            cmp_norm_info = numpy.concatenate((cmp_min_vector, cmp_max_vector), axis=0)\n\n        else:\n            logger.critical(\'Normalisation type %s is not supported!\\n\' %(cfg.output_feature_normalisation))\n            raise\n\n        cmp_norm_info = numpy.array(cmp_norm_info, \'float32\')\n        fid = open(norm_info_file, \'wb\')\n        cmp_norm_info.tofile(fid)\n        fid.close()\n        logger.info(\'saved %s vectors to %s\' %(cfg.output_feature_normalisation, norm_info_file))\n\n        feature_index = 0\n        for feature_name in list(cfg.out_dimension_dict.keys()):\n            feature_std_vector = numpy.array(global_std_vector[:,feature_index:feature_index+cfg.out_dimension_dict[feature_name]], \'float32\')\n\n            fid = open(var_file_dict[feature_name], \'w\')\n            feature_var_vector = feature_std_vector**2\n            feature_var_vector.tofile(fid)\n            fid.close()\n\n            logger.info(\'saved %s variance vector to %s\' %(feature_name, var_file_dict[feature_name]))\n\n            feature_index += cfg.out_dimension_dict[feature_name]\n\n    train_x_file_list = nn_label_norm_file_list[0:cfg.train_file_number]\n    train_y_file_list = nn_cmp_norm_file_list[0:cfg.train_file_number]\n    valid_x_file_list = nn_label_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n    valid_y_file_list = nn_cmp_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n    test_x_file_list  = nn_label_norm_file_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n    test_y_file_list  = nn_cmp_norm_file_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n\n    # we need to know the label dimension before training the DNN\n    # computing that requires us to look at the labels\n    #\n    # currently, there are two ways to do this\n    if cfg.label_style == \'HTS\':\n        label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name, add_frame_features=cfg.add_frame_features, subphone_feats=cfg.subphone_feats)\n        lab_dim = label_normaliser.dimension + cfg.appended_input_dim\n\n    elif cfg.label_style == \'composed\':\n        label_composer = LabelComposer()\n        label_composer.load_label_configuration(cfg.label_config_file)\n        lab_dim=label_composer.compute_label_dimension()\n\n    logger.info(\'label dimension is %d\' % lab_dim)\n\n    combined_model_arch = str(len(hidden_layer_size))\n    for hid_size in hidden_layer_size:\n        combined_model_arch += \'_\' + str(hid_size)\n\n    nnets_file_name = \'%s/%s_%s_%d_%s_%d.%d.train.%d.%f.rnn.model\' \\\n                      %(model_dir, cfg.combined_model_name, cfg.combined_feature_name, int(cfg.multistream_switch),\n                        combined_model_arch, lab_dim, cfg.cmp_dim, cfg.train_file_number, cfg.hyper_params[\'learning_rate\'])\n\n\n    ### DNN model training\n    if cfg.TRAINDNN:\n\n        var_dict = load_covariance(var_file_dict, cfg.out_dimension_dict)\n\n        logger.info(\'training DNN\')\n\n        fid = open(norm_info_file, \'rb\')\n        cmp_min_max = numpy.fromfile(fid, dtype=numpy.float32)\n        fid.close()\n        cmp_min_max = cmp_min_max.reshape((2, -1))\n        cmp_mean_vector = cmp_min_max[0, ]\n        cmp_std_vector  = cmp_min_max[1, ]\n\n\n        try:\n            os.makedirs(model_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create model directory %s\' % model_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        try:\n            train_DNN(train_xy_file_list = (train_x_file_list, train_y_file_list), \\\n                      valid_xy_file_list = (valid_x_file_list, valid_y_file_list), \\\n                      nnets_file_name = nnets_file_name, \\\n                      n_ins = lab_dim, n_outs = cfg.cmp_dim, ms_outs = cfg.multistream_outs, \\\n                      hyper_params = cfg.hyper_params, buffer_size = cfg.buffer_size, plot = cfg.plot, var_dict = var_dict,\n                      cmp_mean_vector = cmp_mean_vector, cmp_std_vector = cmp_std_vector, seq_dur_file_list=seq_dur_file_list)\n        except KeyboardInterrupt:\n            logger.critical(\'train_DNN interrupted via keyboard\')\n            # Could \'raise\' the exception further, but that causes a deep traceback to be printed\n            # which we don\'t care about for a keyboard interrupt. So, just bail out immediately\n            sys.exit(1)\n        except:\n            logger.critical(\'train_DNN threw an exception\')\n            raise\n\n\n\n    if cfg.GENBNFEA:\n        \'\'\'\n        Please only tune on this step when you want to generate bottleneck features from DNN\n        \'\'\'\n        temp_dir_name = \'%s_%s_%d_%d_%d_%d_%s_hidden\' \\\n                        %(cfg.model_type, cfg.combined_feature_name, \\\n                          cfg.train_file_number, lab_dim, cfg.cmp_dim, \\\n                          len(hidden_layers_sizes), combined_model_arch)\n        gen_dir = os.path.join(gen_dir, temp_dir_name)\n\n        bottleneck_size = min(hidden_layers_sizes)\n        bottleneck_index = 0\n        for i in range(len(hidden_layers_sizes)):\n            if hidden_layers_sizes(i) == bottleneck_size:\n                bottleneck_index = i\n\n        logger.info(\'generating bottleneck features from DNN\')\n\n        try:\n            os.makedirs(gen_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create generation directory %s\' % gen_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        gen_file_id_list = file_id_list[0:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n        test_x_file_list  = nn_label_norm_file_list[0:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n        test_d_file_list  = seq_dur_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n        gen_file_list = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.cmp_ext)\n\n        dnn_hidden_generation(test_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list, bottleneck_index)\n\n    ### generate parameters from DNN\n    temp_dir_name = \'%s_%s_%d_%d_%d_%d_%d_%d_%d\' \\\n                    %(cfg.combined_model_name, cfg.combined_feature_name, int(cfg.do_post_filtering), \\\n                      cfg.train_file_number, lab_dim, cfg.cmp_dim, \\\n                      len(hidden_layer_size), hidden_layer_size[0], hidden_layer_size[-1])\n    gen_dir = os.path.join(gen_dir, temp_dir_name)\n\n    gen_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n    test_x_file_list = nn_label_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n    test_d_file_list = seq_dur_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n    if cfg.GenTestList:\n        gen_file_id_list = test_id_list\n        test_x_file_list = nn_label_norm_file_list\n        test_d_file_list = seq_dur_file_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n    if cfg.DNNGEN:\n        logger.info(\'generating from DNN\')\n\n        try:\n            os.makedirs(gen_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create generation directory %s\' % gen_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        gen_file_list = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.cmp_ext)\n        if cfg.network_type == ""S2S"":\n            dnn_generation_S2S(test_x_file_list, test_d_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)\n        else:\n            dnn_generation(test_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)\n\n        logger.debug(\'denormalising generated output using method %s\' % cfg.output_feature_normalisation)\n\n        fid = open(norm_info_file, \'rb\')\n        cmp_min_max = numpy.fromfile(fid, dtype=numpy.float32)\n        fid.close()\n        cmp_min_max = cmp_min_max.reshape((2, -1))\n        cmp_min_vector = cmp_min_max[0, ]\n        cmp_max_vector = cmp_min_max[1, ]\n\n        if cfg.output_feature_normalisation == \'MVN\':\n            denormaliser = MeanVarianceNorm(feature_dimension = cfg.cmp_dim)\n            denormaliser.feature_denormalisation(gen_file_list, gen_file_list, cmp_min_vector, cmp_max_vector)\n\n        elif cfg.output_feature_normalisation == \'MINMAX\':\n            denormaliser = MinMaxNormalisation(cfg.cmp_dim, min_value = 0.01, max_value = 0.99, min_vector = cmp_min_vector, max_vector = cmp_max_vector)\n            denormaliser.denormalise_data(gen_file_list, gen_file_list)\n        else:\n            logger.critical(\'denormalising method %s is not supported!\\n\' %(cfg.output_feature_normalisation))\n            raise\n\n        if cfg.AcousticModel:\n            ##perform MLPG to smooth parameter trajectory\n            ## lf0 is included, the output features much have vuv.\n            generator = ParameterGeneration(gen_wav_features = cfg.gen_wav_features)\n            generator.acoustic_decomposition(gen_file_list, cfg.cmp_dim, cfg.out_dimension_dict, cfg.file_extension_dict, var_file_dict, do_MLPG=cfg.do_MLPG)\n\n        if cfg.DurationModel:\n            ### Perform duration normalization(min. state dur set to 1) ###\n            gen_dur_list   = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.dur_ext)\n            gen_label_list = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.lab_ext)\n            in_gen_label_align_file_list = prepare_file_path_list(gen_file_id_list, cfg.in_label_align_dir, cfg.lab_ext, False)\n\n            generator = ParameterGeneration(gen_wav_features = cfg.gen_wav_features)\n            generator.duration_decomposition(gen_file_list, cfg.cmp_dim, cfg.out_dimension_dict, cfg.file_extension_dict)\n\n            label_modifier = HTSLabelModification(silence_pattern = cfg.silence_pattern)\n            label_modifier.modify_duration_labels(in_gen_label_align_file_list, gen_dur_list, gen_label_list)\n\n\n    ### generate wav\n    if cfg.GENWAV:\n        logger.info(\'reconstructing waveform(s)\')\n        print(len(gen_file_id_list))\n        generate_wav(gen_dir, gen_file_id_list, cfg)     # generated speech\n#       generate_wav(nn_cmp_dir, gen_file_id_list, cfg)  # reference copy synthesis speech\n\n    ### setting back to original conditions before calculating objective scores ###\n    if cfg.GenTestList:\n        in_label_align_file_list = prepare_file_path_list(file_id_list, cfg.in_label_align_dir, cfg.lab_ext, False)\n        binary_label_file_list   = prepare_file_path_list(file_id_list, binary_label_dir, cfg.lab_ext)\n        gen_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n    ### evaluation: RMSE and CORR for duration\n    if cfg.CALMCD and cfg.DurationModel:\n        logger.info(\'calculating MCD\')\n\n        ref_data_dir = os.path.join(data_dir, \'ref_data\')\n\n        ref_dur_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.dur_ext)\n\n        in_gen_label_align_file_list = in_label_align_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n        calculator = IndividualDistortionComp()\n\n        valid_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n        test_file_id_list  = file_id_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n        if cfg.remove_silence_using_binary_labels:\n            untrimmed_reference_data = in_file_list_dict[\'dur\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n            trim_silence(untrimmed_reference_data, ref_dur_list, cfg.dur_dim, \\\n                                untrimmed_test_labels, lab_dim, silence_feature)\n        else:\n            remover = SilenceRemover(n_cmp = cfg.dur_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type, remove_frame_features = cfg.add_frame_features)\n            remover.remove_silence(in_file_list_dict[\'dur\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_dur_list)\n\n        valid_dur_rmse, valid_dur_corr = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.dur_ext, cfg.dur_dim)\n        test_dur_rmse, test_dur_corr = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.dur_ext, cfg.dur_dim)\n\n        logger.info(\'Develop: DNN -- RMSE: %.3f frames/phoneme; CORR: %.3f; \' \\\n                    %(valid_dur_rmse, valid_dur_corr))\n        logger.info(\'Test: DNN -- RMSE: %.3f frames/phoneme; CORR: %.3f; \' \\\n                    %(test_dur_rmse, test_dur_corr))\n\n    ### evaluation: calculate distortion\n    if cfg.CALMCD and cfg.AcousticModel:\n        logger.info(\'calculating MCD\')\n\n        ref_data_dir = os.path.join(data_dir, \'ref_data\')\n\n        ref_mgc_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.mgc_ext)\n        ref_bap_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.bap_ext)\n        ref_lf0_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.lf0_ext)\n\n        in_gen_label_align_file_list = in_label_align_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n        calculator = IndividualDistortionComp()\n\n        spectral_distortion = 0.0\n        bap_mse             = 0.0\n        f0_mse              = 0.0\n        vuv_error           = 0.0\n\n        valid_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n        test_file_id_list  = file_id_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n        if cfg.remove_silence_using_binary_labels:\n            ## get lab_dim:\n            label_composer = LabelComposer()\n            label_composer.load_label_configuration(cfg.label_config_file)\n            lab_dim=label_composer.compute_label_dimension()\n\n            ## use first feature in label -- hardcoded for now\n            silence_feature = 0\n\n            ## Use these to trim silence:\n            untrimmed_test_labels = binary_label_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n\n        if \'mgc\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'mgc\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_mgc_list, cfg.mgc_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.mgc_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type)\n                remover.remove_silence(in_file_list_dict[\'mgc\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_mgc_list)\n            valid_spectral_distortion = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.mgc_ext, cfg.mgc_dim)\n            test_spectral_distortion  = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.mgc_ext, cfg.mgc_dim)\n            valid_spectral_distortion *= (10 /numpy.log(10)) * numpy.sqrt(2.0)    ##MCD\n            test_spectral_distortion  *= (10 /numpy.log(10)) * numpy.sqrt(2.0)    ##MCD\n\n\n        if \'bap\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'bap\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_bap_list, cfg.bap_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.bap_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type)\n                remover.remove_silence(in_file_list_dict[\'bap\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_bap_list)\n            valid_bap_mse        = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.bap_ext, cfg.bap_dim)\n            test_bap_mse         = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.bap_ext, cfg.bap_dim)\n            valid_bap_mse = valid_bap_mse / 10.0    ##Cassia\'s bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC\n            test_bap_mse  = test_bap_mse / 10.0    ##Cassia\'s bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC\n\n        if \'lf0\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'lf0\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_lf0_list, cfg.lf0_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.lf0_dim, silence_pattern = cfg.silence_pattern, label_type=cfg.label_type)\n                remover.remove_silence(in_file_list_dict[\'lf0\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_lf0_list)\n            valid_f0_mse, valid_f0_corr, valid_vuv_error   = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.lf0_ext, cfg.lf0_dim)\n            test_f0_mse , test_f0_corr, test_vuv_error    = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.lf0_ext, cfg.lf0_dim)\n\n        logger.info(\'Develop: DNN -- MCD: %.3f dB; BAP: %.3f dB; F0:- RMSE: %.3f Hz; CORR: %.3f; VUV: %.3f%%\' \\\n                    %(valid_spectral_distortion, valid_bap_mse, valid_f0_mse, valid_f0_corr, valid_vuv_error*100.))\n        logger.info(\'Test   : DNN -- MCD: %.3f dB; BAP: %.3f dB; F0:- RMSE: %.3f Hz; CORR: %.3f; VUV: %.3f%%\' \\\n                    %(test_spectral_distortion , test_bap_mse , test_f0_mse , test_f0_corr, test_vuv_error*100.))\n\nif __name__ == \'__main__\':\n\n\n\n    # these things should be done even before trying to parse the command line\n\n    # create a configuration instance\n    # and get a short name for this instance\n    cfg=configuration.cfg\n\n    # set up logging to use our custom class\n    logging.setLoggerClass(LoggerPlotter)\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n\n    if len(sys.argv) != 2:\n        logger.critical(\'usage: run_dnn.sh [config file name]\')\n        sys.exit(1)\n\n    config_file = sys.argv[1]\n\n    config_file = os.path.abspath(config_file)\n    cfg.configure(config_file)\n\n    if cfg.profile:\n        logger.info(\'profiling is activated\')\n        import cProfile, pstats\n        cProfile.run(\'main_function(cfg)\', \'mainstats\')\n\n        # create a stream for the profiler to write to\n        profiling_output = io.StringIO()\n        p = pstats.Stats(\'mainstats\', stream=profiling_output)\n\n        # print stats to that stream\n        # here we just report the top 10 functions, sorted by total amount of time spent in each\n        p.strip_dirs().sort_stats(\'tottime\').print_stats(10)\n\n        # print the result to the log\n        logger.info(\'---Profiling result follows---\\n%s\' %  profiling_output.getvalue() )\n        profiling_output.close()\n        logger.info(\'---End of profiling result---\')\n\n    else:\n        main_function(cfg)\n\n#    if gnp._boardId is not None:\n#        import gpu_lock\n#        gpu_lock.free_lock(gnp._boardId)\n\n    sys.exit(0)\n'"
src/work_in_progress/run_sdae.py,0,"b'\nimport pickle\nimport gzip\nimport os, sys, errno\nimport time\nimport math\n\n#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)\nimport numpy\n#import gnumpy as gnp\n# we need to explicitly import this in some cases, not sure why this doesn\'t get imported with numpy itself\nimport numpy.distutils.__config__\n# and only after that can we import theano\nimport theano\n\nfrom utils.providers import ListDataProvider\n\nfrom frontend.label_normalisation import HTSLabelNormalisation, XMLLabelNormalisation\nfrom frontend.silence_remover import SilenceRemover\nfrom frontend.silence_remover import trim_silence\nfrom frontend.min_max_norm import MinMaxNormalisation\nfrom frontend.acoustic_composition import AcousticComposition\nfrom frontend.parameter_generation import ParameterGeneration\nfrom frontend.mean_variance_norm import MeanVarianceNorm\n\n# the new class for label composition and normalisation\nfrom frontend.label_composer import LabelComposer\nfrom frontend.label_modifier import HTSLabelModification\n#from frontend.mlpg_fast import MLParameterGenerationFast\n\n#from frontend.mlpg_fast_layer import MLParameterGenerationFastLayer\n\n\nimport configuration\nfrom models.deep_rnn import DeepRecurrentNetwork\nfrom models.sdae import StackedDenoiseAutoEncoder\n\nfrom utils.compute_distortion import DistortionComputation, IndividualDistortionComp\nfrom utils.generate import generate_wav\nfrom utils.learn_rates import ExpDecreaseLearningRate\n\nfrom io_funcs.binary_io import  BinaryIOCollection\n\n#import matplotlib.pyplot as plt\n# our custom logging class that can also plot\n#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot\nfrom logplot.logging_plotting import LoggerPlotter, MultipleSeriesPlot, SingleWeightMatrixPlot\nimport logging # as logging\nimport logging.config\nimport io\n\n\ndef extract_file_id_list(file_list):\n    file_id_list = []\n    for file_name in file_list:\n        file_id = os.path.basename(os.path.splitext(file_name)[0])\n        file_id_list.append(file_id)\n\n    return  file_id_list\n\ndef read_file_list(file_name):\n\n    logger = logging.getLogger(""read_file_list"")\n\n    file_lists = []\n    fid = open(file_name)\n    for line in fid.readlines():\n        line = line.strip()\n        if len(line) < 1:\n            continue\n        file_lists.append(line)\n    fid.close()\n\n    logger.debug(\'Read file list from %s\' % file_name)\n    return  file_lists\n\n\ndef make_output_file_list(out_dir, in_file_lists):\n    out_file_lists = []\n\n    for in_file_name in in_file_lists:\n        file_id = os.path.basename(in_file_name)\n        out_file_name = out_dir + \'/\' + file_id\n        out_file_lists.append(out_file_name)\n\n    return  out_file_lists\n\ndef prepare_file_path_list(file_id_list, file_dir, file_extension, new_dir_switch=True):\n    if not os.path.exists(file_dir) and new_dir_switch:\n        os.makedirs(file_dir)\n    file_name_list = []\n    for file_id in file_id_list:\n        file_name = file_dir + \'/\' + file_id + file_extension\n        file_name_list.append(file_name)\n\n    return  file_name_list\n\n\n\ndef visualize_dnn(dnn):\n\n    layer_num = len(dnn.params)     ## including input and output\n    plotlogger = logging.getLogger(""plotting"")\n\n    for i in range(layer_num):\n        fig_name = \'Activation weights W\' + str(i) + \'_\' + dnn.params[i].name\n        fig_title = \'Activation weights of W\' + str(i)\n        xlabel = \'Neuron index of hidden layer \' + str(i)\n        ylabel = \'Neuron index of hidden layer \' + str(i+1)\n        if i == 0:\n            xlabel = \'Input feature index\'\n        if i == layer_num-1:\n            ylabel = \'Output feature index\'\n\n        aa = dnn.params[i].get_value(borrow=True).T\n        print(aa.shape, aa.size)\n        if aa.size > aa.shape[0]:\n            logger.create_plot(fig_name, SingleWeightMatrixPlot)\n            plotlogger.add_plot_point(fig_name, fig_name, dnn.params[i].get_value(borrow=True).T)\n            plotlogger.save_plot(fig_name, title=fig_name, xlabel=xlabel, ylabel=ylabel)\n\ndef load_covariance(var_file_dict, out_dimension_dict):\n    var = {}\n    io_funcs = BinaryIOCollection()\n    for feature_name in list(var_file_dict.keys()):\n        var_values, dimension = io_funcs.load_binary_file_frame(var_file_dict[feature_name], 1)\n\n        var_values = numpy.reshape(var_values, (out_dimension_dict[feature_name], 1))\n\n        var[feature_name] = var_values\n\n    return  var\n\n\ndef train_DNN(train_xy_file_list, valid_xy_file_list, \\\n              nnets_file_name, n_ins, n_outs, ms_outs, hyper_params, buffer_size, plot=False, var_dict=None,\n              cmp_mean_vector = None, cmp_std_vector = None, init_dnn_model_file = None):\n\n    # get loggers for this function\n    # this one writes to both console and file\n    logger = logging.getLogger(""main.train_DNN"")\n    logger.debug(\'Starting train_DNN\')\n\n    if plot:\n        # this one takes care of plotting duties\n        plotlogger = logging.getLogger(""plotting"")\n        # create an (empty) plot of training convergence, ready to receive data points\n        logger.create_plot(\'training convergence\',MultipleSeriesPlot)\n\n    try:\n        assert numpy.sum(ms_outs) == n_outs\n    except AssertionError:\n        logger.critical(\'the summation of multi-stream outputs does not equal to %d\' %(n_outs))\n        raise\n\n    ####parameters#####\n    finetune_lr     = float(hyper_params[\'learning_rate\'])\n    training_epochs = int(hyper_params[\'training_epochs\'])\n    batch_size      = int(hyper_params[\'batch_size\'])\n    l1_reg          = float(hyper_params[\'l1_reg\'])\n    l2_reg          = float(hyper_params[\'l2_reg\'])\n    warmup_epoch    = int(hyper_params[\'warmup_epoch\'])\n    momentum        = float(hyper_params[\'momentum\'])\n    warmup_momentum = float(hyper_params[\'warmup_momentum\'])\n\n    hidden_layer_size = hyper_params[\'hidden_layer_size\']\n\n    buffer_utt_size = buffer_size\n    early_stop_epoch = int(hyper_params[\'early_stop_epochs\'])\n\n    hidden_activation = hyper_params[\'hidden_activation\']\n    output_activation = hyper_params[\'output_activation\']\n\n    model_type = hyper_params[\'model_type\']\n    hidden_layer_type  = hyper_params[\'hidden_layer_type\']\n\n    ## use a switch to turn on pretraining\n    ## pretraining may not help too much, if this case, we turn it off to save time\n    do_pretraining = hyper_params[\'do_pretraining\']\n    pretraining_epochs = int(hyper_params[\'pretraining_epochs\'])\n    pretraining_lr = float(hyper_params[\'pretraining_lr\'])\n\n    sequential_training = hyper_params[\'sequential_training\']\n    dropout_rate = hyper_params[\'dropout_rate\']\n\n#    sequential_training = True\n\n    buffer_size = int(buffer_size / batch_size) * batch_size\n\n    ###################\n    (train_x_file_list, train_y_file_list) = train_xy_file_list\n    (valid_x_file_list, valid_y_file_list) = valid_xy_file_list\n\n    logger.debug(\'Creating training   data provider\')\n    train_data_reader = ListDataProvider(x_file_list = train_x_file_list, y_file_list = train_y_file_list,\n                            n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, sequential = sequential_training, shuffle = True)\n\n    logger.debug(\'Creating validation data provider\')\n    valid_data_reader = ListDataProvider(x_file_list = valid_x_file_list, y_file_list = valid_y_file_list,\n                            n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, sequential = sequential_training, shuffle = False)\n\n    shared_train_set_xy, temp_train_set_x, temp_train_set_y = train_data_reader.load_one_partition()\n    train_set_x, train_set_y = shared_train_set_xy\n    shared_valid_set_xy, valid_set_x, valid_set_y = valid_data_reader.load_one_partition()   #validation data is still read block by block\n    valid_set_x, valid_set_y = shared_valid_set_xy\n    train_data_reader.reset()\n    valid_data_reader.reset()\n\n\n    ##temporally we use the training set as pretrain_set_x.\n    ##we need to support any data for pretraining\n#    pretrain_set_x = train_set_x\n\n    # numpy random generator\n    numpy_rng = numpy.random.RandomState(123)\n    logger.info(\'building the model\')\n\n\n    dnn_model = None\n    pretrain_fn = None  ## not all the model support pretraining right now\n    train_fn = None\n    valid_fn = None\n    valid_model = None ## valid_fn and valid_model are the same. reserve to computer multi-stream distortion\n    if model_type == \'DNN\':\n        dnn_model = DeepRecurrentNetwork(n_in= n_ins, hidden_layer_size = hidden_layer_size, n_out = n_outs,\n                                         L1_reg = l1_reg, L2_reg = l2_reg, hidden_layer_type = hidden_layer_type, dropout_rate = dropout_rate)\n        train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y))  #, batch_size=batch_size\n    elif model_type == \'SDAE\':\n        dnn_model = StackedDenoiseAutoEncoder(n_in= n_ins, hidden_layer_size = hidden_layer_size, n_out = n_outs,\n                                         L1_reg = l1_reg, L2_reg = l2_reg, hidden_layer_type = hidden_layer_type, dropout_rate = dropout_rate)\n\n        if do_pretraining:\n            #temporally we use the training set as pretrain_set_x.\n            #we need to support any data for pretraining\n            pretrain_set_x = train_set_x\n            pretraining_fn = dnn_model.pretraining_functions(pretrain_set_x)\n\n        train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y))  #, batch_size=batch_size\n\n    else:\n        logger.critical(\'%s type NN model is not supported!\' %(model_type))\n        raise\n\n    ## if pretraining is supported more than one model, add the switch here\n    ## be careful to use autoencoder for pretraining here:\n    if do_pretraining and model_type == \'SDAE\':\n        logger.info(\'pretraining the %s model\' %(model_type))\n\n        corruption_level = 0.0\n        ## in SDAE we do layer-wise pretraining using autoencoders\n        for i in range(dnn_model.n_layers):\n            for epoch in range(pretraining_epochs):\n                sub_start_time = time.clock()\n\n                pretrain_loss = []\n                while (not train_data_reader.is_finish()):\n                    shared_train_set_xy, temp_train_set_x, temp_train_set_y = train_data_reader.load_one_partition()\n                    # if sequential training, the batch size will be the number of frames in an utterance\n                    if sequential_training == True:\n                        batch_size = temp_train_set_x.shape[0]\n\n                    n_train_batches = temp_train_set_x.shape[0] / batch_size\n                    for index in range(n_train_batches):\n                    ## send a batch to the shared variable, rather than pass the batch size and batch index to the finetune function\n                        pretrain_set_x.set_value(numpy.asarray(temp_train_set_x[index*batch_size:(index + 1)*batch_size], dtype=theano.config.floatX), borrow=True)\n\n                        pretrain_loss.append(pretraining_fn[i](corruption=corruption_level,\n                                                               learning_rate=pretraining_lr))\n\n                sub_end_time = time.clock()\n                logger.info(\'Pre-training layer %i, epoch %d, cost %s, time spent%.2f\' % (i+1, epoch+1, numpy.mean(pretrain_loss), (sub_end_time - sub_start_time)))\n                train_data_reader.reset()\n\n\n    logger.info(\'fine-tuning the %s model\' %(model_type))\n\n    start_time = time.time()\n\n    best_dnn_model = dnn_model\n    best_validation_loss = sys.float_info.max\n    previous_loss = sys.float_info.max\n\n    early_stop = 0\n    epoch = 0\n\n#    finetune_lr = 0.000125\n    previous_finetune_lr = finetune_lr\n\n    print(finetune_lr)\n\n    while (epoch < training_epochs):\n        epoch = epoch + 1\n\n        current_momentum = momentum\n        current_finetune_lr = finetune_lr\n        if epoch <= warmup_epoch:\n            current_finetune_lr = finetune_lr\n            current_momentum = warmup_momentum\n        else:\n            current_finetune_lr = previous_finetune_lr * 0.5\n\n        previous_finetune_lr = current_finetune_lr\n\n        train_error = []\n        sub_start_time = time.time()\n\n        while (not train_data_reader.is_finish()):\n\n            shared_train_set_xy, temp_train_set_x, temp_train_set_y = train_data_reader.load_one_partition()\n#            train_set_x.set_value(numpy.asarray(temp_train_set_x, dtype=theano.config.floatX), borrow=True)\n#            train_set_y.set_value(numpy.asarray(temp_train_set_y, dtype=theano.config.floatX), borrow=True)\n\n            # if sequential training, the batch size will be the number of frames in an utterance\n            if sequential_training == True:\n                batch_size = temp_train_set_x.shape[0]\n\n            n_train_batches = temp_train_set_x.shape[0] / batch_size\n            for index in range(n_train_batches):\n                ## send a batch to the shared variable, rather than pass the batch size and batch index to the finetune function\n                train_set_x.set_value(numpy.asarray(temp_train_set_x[index*batch_size:(index + 1)*batch_size], dtype=theano.config.floatX), borrow=True)\n                train_set_y.set_value(numpy.asarray(temp_train_set_y[index*batch_size:(index + 1)*batch_size], dtype=theano.config.floatX), borrow=True)\n\n                this_train_error = train_fn(current_finetune_lr, current_momentum)\n\n                train_error.append(this_train_error)\n\n        train_data_reader.reset()\n\n        logger.debug(\'calculating validation loss\')\n        validation_losses = []\n        while (not valid_data_reader.is_finish()):\n            shared_valid_set_xy, temp_valid_set_x, temp_valid_set_y = valid_data_reader.load_one_partition()\n            valid_set_x.set_value(numpy.asarray(temp_valid_set_x, dtype=theano.config.floatX), borrow=True)\n            valid_set_y.set_value(numpy.asarray(temp_valid_set_y, dtype=theano.config.floatX), borrow=True)\n\n            this_valid_loss = valid_fn()\n\n            validation_losses.append(this_valid_loss)\n        valid_data_reader.reset()\n\n        this_validation_loss = numpy.mean(validation_losses)\n\n        this_train_valid_loss = numpy.mean(numpy.asarray(train_error))\n\n        sub_end_time = time.time()\n\n        loss_difference = this_validation_loss - previous_loss\n\n        logger.info(\'epoch %i, validation error %f, train error %f  time spent %.2f\' %(epoch, this_validation_loss, this_train_valid_loss, (sub_end_time - sub_start_time)))\n        if plot:\n            plotlogger.add_plot_point(\'training convergence\',\'validation set\',(epoch,this_validation_loss))\n            plotlogger.add_plot_point(\'training convergence\',\'training set\',(epoch,this_train_valid_loss))\n            plotlogger.save_plot(\'training convergence\',title=\'Progress of training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n\n        if this_validation_loss < best_validation_loss:\n            if epoch > 5:\n                pickle.dump(best_dnn_model, open(nnets_file_name, \'wb\'))\n\n            best_dnn_model = dnn_model\n            best_validation_loss = this_validation_loss\n#            logger.debug(\'validation loss decreased, so saving model\')\n\n        if this_validation_loss >= previous_loss:\n            logger.debug(\'validation loss increased\')\n\n#            dbn = best_dnn_model\n            early_stop += 1\n\n        if epoch > 15 and early_stop > early_stop_epoch:\n            logger.debug(\'stopping early\')\n            break\n\n        if math.isnan(this_validation_loss):\n            break\n\n        previous_loss = this_validation_loss\n\n    end_time = time.time()\n#    cPickle.dump(best_dnn_model, open(nnets_file_name, \'wb\'))\n\n    logger.info(\'overall  training time: %.2fm validation error %f\' % ((end_time - start_time) / 60., best_validation_loss))\n\n    if plot:\n        plotlogger.save_plot(\'training convergence\',title=\'Final training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n\n    return  best_validation_loss\n\n\ndef dnn_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):  #file_number\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        test_set_x = features.reshape((-1, n_ins))\n\n        predicted_parameter = dnn_model.parameter_prediction(test_set_x)\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\ndef dnn_generation_lstm(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    visualize_dnn(dnn_model)\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):  #file_number\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        test_set_x = features.reshape((-1, n_ins))\n\n        predicted_parameter = dnn_model.parameter_prediction_lstm(test_set_x)\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\n##generate bottleneck layer as festures\ndef dnn_hidden_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        features = features.reshape((-1, n_ins))\n        temp_set_x = features.tolist()\n        test_set_x = theano.shared(numpy.asarray(temp_set_x, dtype=theano.config.floatX))\n\n        predicted_parameter = dnn_model.generate_top_hidden_layer(test_set_x=test_set_x)\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\n\ndef main_function(cfg):\n\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n    # get another logger to handle plotting duties\n    plotlogger = logging.getLogger(""plotting"")\n\n    # later, we might do this via a handler that is created, attached and configured\n    # using the standard config mechanism of the logging module\n    # but for now we need to do it manually\n    plotlogger.set_plot_path(cfg.plot_dir)\n\n    #### parameter setting########\n    hidden_layer_size = cfg.hyper_params[\'hidden_layer_size\']\n\n\n    ####prepare environment\n\n    try:\n        file_id_list = read_file_list(cfg.file_id_scp)\n        logger.debug(\'Loaded file id list from %s\' % cfg.file_id_scp)\n    except IOError:\n        # this means that open(...) threw an error\n        logger.critical(\'Could not load file id list from %s\' % cfg.file_id_scp)\n        raise\n\n    ###total file number including training, development, and testing\n    total_file_number = len(file_id_list)\n\n    data_dir = cfg.data_dir\n\n    nn_cmp_dir       = os.path.join(data_dir, \'nn\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim))\n    nn_cmp_norm_dir   = os.path.join(data_dir, \'nn_norm\'  + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim))\n\n    model_dir = os.path.join(cfg.work_dir, \'nnets_model\')\n    gen_dir   = os.path.join(cfg.work_dir, \'gen\')\n\n    in_file_list_dict = {}\n\n    for feature_name in list(cfg.in_dir_dict.keys()):\n        in_file_list_dict[feature_name] = prepare_file_path_list(file_id_list, cfg.in_dir_dict[feature_name], cfg.file_extension_dict[feature_name], False)\n\n    nn_cmp_file_list         = prepare_file_path_list(file_id_list, nn_cmp_dir, cfg.cmp_ext)\n    nn_cmp_norm_file_list    = prepare_file_path_list(file_id_list, nn_cmp_norm_dir, cfg.cmp_ext)\n\n    ###normalisation information\n    norm_info_file = os.path.join(data_dir, \'norm_info\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim) + \'_\' + cfg.output_feature_normalisation + \'.dat\')\n\n    ### normalise input full context label\n    # currently supporting two different forms of lingustic features\n    # later, we should generalise this\n\n    if cfg.label_style == \'HTS\':\n        label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name, add_frame_features=cfg.add_frame_features, subphone_feats=cfg.subphone_feats)\n        lab_dim = label_normaliser.dimension + cfg.appended_input_dim\n        logger.info(\'Input label dimension is %d\' % lab_dim)\n        suffix=str(lab_dim)\n    # no longer supported - use new ""composed"" style labels instead\n    elif cfg.label_style == \'composed\':\n        # label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)\n        suffix=\'composed\'\n\n    if cfg.process_labels_in_work_dir:\n        label_data_dir = cfg.work_dir\n    else:\n        label_data_dir = data_dir\n\n    # the number can be removed\n    binary_label_dir      = os.path.join(label_data_dir, \'binary_label_\'+suffix)\n    nn_label_dir          = os.path.join(label_data_dir, \'nn_no_silence_lab_\'+suffix)\n    nn_label_norm_dir     = os.path.join(label_data_dir, \'nn_no_silence_lab_norm_\'+suffix)\n\n    in_label_align_file_list = prepare_file_path_list(file_id_list, cfg.in_label_align_dir, cfg.lab_ext, False)\n    binary_label_file_list   = prepare_file_path_list(file_id_list, binary_label_dir, cfg.lab_ext)\n    nn_label_file_list       = prepare_file_path_list(file_id_list, nn_label_dir, cfg.lab_ext)\n    nn_label_norm_file_list  = prepare_file_path_list(file_id_list, nn_label_norm_dir, cfg.lab_ext)\n    dur_file_list            = prepare_file_path_list(file_id_list, cfg.in_dur_dir, cfg.dur_ext)\n    lf0_file_list            = prepare_file_path_list(file_id_list, cfg.in_lf0_dir, cfg.lf0_ext)\n\n    # to do - sanity check the label dimension here?\n\n\n\n    min_max_normaliser = None\n    label_norm_file = \'label_norm_%s_%d.dat\' %(cfg.label_style, lab_dim)\n    label_norm_file = os.path.join(label_data_dir, label_norm_file)\n\n    if cfg.GenTestList:\n        try:\n            test_id_list = read_file_list(cfg.test_id_scp)\n            logger.debug(\'Loaded file id list from %s\' % cfg.test_id_scp)\n        except IOError:\n            # this means that open(...) threw an error\n            logger.critical(\'Could not load file id list from %s\' % cfg.test_id_scp)\n            raise\n\n        in_label_align_file_list = prepare_file_path_list(test_id_list, cfg.in_label_align_dir, cfg.lab_ext, False)\n        binary_label_file_list   = prepare_file_path_list(test_id_list, binary_label_dir, cfg.lab_ext)\n        nn_label_file_list       = prepare_file_path_list(test_id_list, nn_label_dir, cfg.lab_ext)\n        nn_label_norm_file_list  = prepare_file_path_list(test_id_list, nn_label_norm_dir, cfg.lab_ext)\n\n    if cfg.NORMLAB and (cfg.label_style == \'HTS\'):\n        # simple HTS labels\n        logger.info(\'preparing label data (input) using standard HTS style labels\')\n        label_normaliser.perform_normalisation(in_label_align_file_list, binary_label_file_list)\n\n        remover = SilenceRemover(n_cmp = lab_dim, silence_pattern = cfg.silence_pattern, remove_frame_features = cfg.add_frame_features, subphone_feats = cfg.subphone_feats)\n        remover.remove_silence(binary_label_file_list, in_label_align_file_list, nn_label_file_list)\n\n        min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99)\n        ###use only training data to find min-max information, then apply on the whole dataset\n        if cfg.GenTestList:\n            min_max_normaliser.load_min_max_values(label_norm_file)\n        else:\n            min_max_normaliser.find_min_max_values(nn_label_file_list[0:cfg.train_file_number])\n        min_max_normaliser.normalise_data(nn_label_file_list, nn_label_norm_file_list)\n\n\n    if cfg.NORMLAB and (cfg.label_style == \'composed\'):\n        # new flexible label preprocessor\n\n        logger.info(\'preparing label data (input) using ""composed"" style labels\')\n        label_composer = LabelComposer()\n        label_composer.load_label_configuration(cfg.label_config_file)\n\n        logger.info(\'Loaded label configuration\')\n        # logger.info(\'%s\' % label_composer.configuration.labels )\n\n        lab_dim=label_composer.compute_label_dimension()\n        logger.info(\'label dimension will be %d\' % lab_dim)\n\n        if cfg.precompile_xpaths:\n            label_composer.precompile_xpaths()\n\n        # there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)\n        # create all the lists of these, ready to pass to the label composer\n\n        in_label_align_file_list = {}\n        for label_style, label_style_required in label_composer.label_styles.items():\n            if label_style_required:\n                logger.info(\'labels of style %s are required - constructing file paths for them\' % label_style)\n                if label_style == \'xpath\':\n                    in_label_align_file_list[\'xpath\'] = prepare_file_path_list(file_id_list, cfg.xpath_label_align_dir, cfg.utt_ext, False)\n                elif label_style == \'hts\':\n                    in_label_align_file_list[\'hts\'] = prepare_file_path_list(file_id_list, cfg.hts_label_align_dir, cfg.lab_ext, False)\n                else:\n                    logger.critical(\'unsupported label style %s specified in label configuration\' % label_style)\n                    raise Exception\n\n            # now iterate through the files, one at a time, constructing the labels for them\n            num_files=len(file_id_list)\n            logger.info(\'the label styles required are %s\' % label_composer.label_styles)\n\n            for i in range(num_files):\n                logger.info(\'making input label features for %4d of %4d\' % (i+1,num_files))\n\n                # iterate through the required label styles and open each corresponding label file\n\n                # a dictionary of file descriptors, pointing at the required files\n                required_labels={}\n\n                for label_style, label_style_required in label_composer.label_styles.items():\n\n                    # the files will be a parallel set of files for a single utterance\n                    # e.g., the XML tree and an HTS label file\n                    if label_style_required:\n                        required_labels[label_style] = open(in_label_align_file_list[label_style][i] , \'r\')\n                        logger.debug(\' opening label file %s\' % in_label_align_file_list[label_style][i])\n\n                logger.debug(\'label styles with open files: %s\' % required_labels)\n                label_composer.make_labels(required_labels,out_file_name=binary_label_file_list[i],fill_missing_values=cfg.fill_missing_values,iterate_over_frames=cfg.iterate_over_frames)\n\n                # now close all opened files\n                for fd in required_labels.values():\n                    fd.close()\n\n\n        # silence removal\n        if cfg.remove_silence_using_binary_labels:\n            silence_feature = 0 ## use first feature in label -- hardcoded for now\n            logger.info(\'Silence removal from label using silence feature: %s\'%(label_composer.configuration.labels[silence_feature]))\n            logger.info(\'Silence will be removed from CMP files in same way\')\n            ## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:\n            trim_silence(binary_label_file_list, nn_label_file_list, lab_dim, \\\n                                binary_label_file_list, lab_dim, silence_feature)\n        else:\n            logger.info(\'No silence removal done\')\n            # start from the labels we have just produced, not trimmed versions\n            nn_label_file_list = binary_label_file_list\n\n        min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99)\n        ###use only training data to find min-max information, then apply on the whole dataset\n        min_max_normaliser.find_min_max_values(nn_label_file_list[0:cfg.train_file_number])\n        min_max_normaliser.normalise_data(nn_label_file_list, nn_label_norm_file_list)\n\n    if min_max_normaliser != None and not cfg.GenTestList:\n        ### save label normalisation information for unseen testing labels\n        label_min_vector = min_max_normaliser.min_vector\n        label_max_vector = min_max_normaliser.max_vector\n        label_norm_info = numpy.concatenate((label_min_vector, label_max_vector), axis=0)\n\n        label_norm_info = numpy.array(label_norm_info, \'float32\')\n        fid = open(label_norm_file, \'wb\')\n        label_norm_info.tofile(fid)\n        fid.close()\n        logger.info(\'saved %s vectors to %s\' %(label_min_vector.size, label_norm_file))\n\n\n    ### make output duration data\n    if cfg.MAKEDUR:\n        logger.info(\'creating duration (output) features\')\n        feature_type = cfg.dur_feature_type\n        label_normaliser.prepare_dur_data(in_label_align_file_list, dur_file_list, feature_type)\n\n\n    ### make output acoustic data\n    if cfg.MAKECMP:\n        logger.info(\'creating acoustic (output) features\')\n        delta_win = cfg.delta_win #[-0.5, 0.0, 0.5]\n        acc_win = cfg.acc_win     #[1.0, -2.0, 1.0]\n\n        acoustic_worker = AcousticComposition(delta_win = delta_win, acc_win = acc_win)\n        if \'dur\' in list(cfg.in_dir_dict.keys()) and cfg.AcousticModel:\n            acoustic_worker.make_equal_frames(dur_file_list, lf0_file_list, cfg.in_dimension_dict)\n        acoustic_worker.prepare_nn_data(in_file_list_dict, nn_cmp_file_list, cfg.in_dimension_dict, cfg.out_dimension_dict)\n\n        if cfg.remove_silence_using_binary_labels:\n            ## do this to get lab_dim:\n            label_composer = LabelComposer()\n            label_composer.load_label_configuration(cfg.label_config_file)\n            lab_dim=label_composer.compute_label_dimension()\n\n            silence_feature = 0 ## use first feature in label -- hardcoded for now\n            logger.info(\'Silence removal from CMP using binary label file\')\n\n            ## overwrite the untrimmed audio with the trimmed version:\n            trim_silence(nn_cmp_file_list, nn_cmp_file_list, cfg.cmp_dim,\n                                binary_label_file_list, lab_dim, silence_feature)\n\n        else: ## back off to previous method using HTS labels:\n            remover = SilenceRemover(n_cmp = cfg.cmp_dim, silence_pattern = cfg.silence_pattern, remove_frame_features = cfg.add_frame_features, subphone_feats = cfg.subphone_feats)\n            remover.remove_silence(nn_cmp_file_list[0:cfg.train_file_number+cfg.valid_file_number],\n                                   in_label_align_file_list[0:cfg.train_file_number+cfg.valid_file_number],\n                                   nn_cmp_file_list[0:cfg.train_file_number+cfg.valid_file_number]) # save to itself\n\n    ### save acoustic normalisation information for normalising the features back\n    var_dir   = os.path.join(data_dir, \'var\')\n    if not os.path.exists(var_dir):\n        os.makedirs(var_dir)\n\n    var_file_dict = {}\n    for feature_name in list(cfg.out_dimension_dict.keys()):\n        var_file_dict[feature_name] = os.path.join(var_dir, feature_name + \'_\' + str(cfg.out_dimension_dict[feature_name]))\n\n    ### normalise output acoustic data\n    if cfg.NORMCMP:\n        logger.info(\'normalising acoustic (output) features using method %s\' % cfg.output_feature_normalisation)\n        cmp_norm_info = None\n        if cfg.output_feature_normalisation == \'MVN\':\n            normaliser = MeanVarianceNorm(feature_dimension=cfg.cmp_dim)\n            ###calculate mean and std vectors on the training data, and apply on the whole dataset\n            global_mean_vector = normaliser.compute_mean(nn_cmp_file_list[0:cfg.train_file_number], 0, cfg.cmp_dim)\n            global_std_vector = normaliser.compute_std(nn_cmp_file_list[0:cfg.train_file_number], global_mean_vector, 0, cfg.cmp_dim)\n\n            normaliser.feature_normalisation(nn_cmp_file_list[0:cfg.train_file_number+cfg.valid_file_number],\n                                             nn_cmp_norm_file_list[0:cfg.train_file_number+cfg.valid_file_number])\n            cmp_norm_info = numpy.concatenate((global_mean_vector, global_std_vector), axis=0)\n\n        elif cfg.output_feature_normalisation == \'MINMAX\':\n            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim)\n            global_mean_vector = min_max_normaliser.compute_mean(nn_cmp_file_list[0:cfg.train_file_number])\n            global_std_vector = min_max_normaliser.compute_std(nn_cmp_file_list[0:cfg.train_file_number], global_mean_vector)\n\n            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim, min_value = 0.01, max_value = 0.99)\n            min_max_normaliser.find_min_max_values(nn_cmp_file_list[0:cfg.train_file_number])\n            min_max_normaliser.normalise_data(nn_cmp_file_list, nn_cmp_norm_file_list)\n\n            cmp_min_vector = min_max_normaliser.min_vector\n            cmp_max_vector = min_max_normaliser.max_vector\n            cmp_norm_info = numpy.concatenate((cmp_min_vector, cmp_max_vector), axis=0)\n\n        else:\n            logger.critical(\'Normalisation type %s is not supported!\\n\' %(cfg.output_feature_normalisation))\n            raise\n\n        cmp_norm_info = numpy.array(cmp_norm_info, \'float32\')\n        fid = open(norm_info_file, \'wb\')\n        cmp_norm_info.tofile(fid)\n        fid.close()\n        logger.info(\'saved %s vectors to %s\' %(cfg.output_feature_normalisation, norm_info_file))\n\n        feature_index = 0\n        for feature_name in list(cfg.out_dimension_dict.keys()):\n            feature_std_vector = numpy.array(global_std_vector[:,feature_index:feature_index+cfg.out_dimension_dict[feature_name]], \'float32\')\n\n            fid = open(var_file_dict[feature_name], \'w\')\n            feature_std_vector.tofile(fid)\n            fid.close()\n\n            logger.info(\'saved %s variance vector to %s\' %(feature_name, var_file_dict[feature_name]))\n\n            feature_index += cfg.out_dimension_dict[feature_name]\n\n    train_x_file_list = nn_label_norm_file_list[0:cfg.train_file_number]\n    train_y_file_list = nn_cmp_norm_file_list[0:cfg.train_file_number]\n    valid_x_file_list = nn_label_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n    valid_y_file_list = nn_cmp_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n    test_x_file_list  = nn_label_norm_file_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n    test_y_file_list  = nn_cmp_norm_file_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n\n    # we need to know the label dimension before training the DNN\n    # computing that requires us to look at the labels\n    #\n    # currently, there are two ways to do this\n    if cfg.label_style == \'HTS\':\n        label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name, add_frame_features=cfg.add_frame_features, subphone_feats=cfg.subphone_feats)\n        lab_dim = label_normaliser.dimension + cfg.appended_input_dim\n\n    elif cfg.label_style == \'composed\':\n        label_composer = LabelComposer()\n        label_composer.load_label_configuration(cfg.label_config_file)\n        lab_dim=label_composer.compute_label_dimension()\n\n    logger.info(\'label dimension is %d\' % lab_dim)\n\n    combined_model_arch = str(len(hidden_layer_size))\n    for hid_size in hidden_layer_size:\n        combined_model_arch += \'_\' + str(hid_size)\n\n    nnets_file_name = \'%s/%s_%s_%d_%s_%d.%d.train.%d.%f.rnn.model\' \\\n                      %(model_dir, cfg.combined_model_name, cfg.combined_feature_name, int(cfg.multistream_switch),\n                        combined_model_arch, lab_dim, cfg.cmp_dim, cfg.train_file_number, cfg.hyper_params[\'learning_rate\'])\n\n\n    ### DNN model training\n    if cfg.TRAINDNN:\n\n        var_dict = load_covariance(var_file_dict, cfg.out_dimension_dict)\n\n        logger.info(\'training DNN\')\n\n        fid = open(norm_info_file, \'rb\')\n        cmp_min_max = numpy.fromfile(fid, dtype=numpy.float32)\n        fid.close()\n        cmp_min_max = cmp_min_max.reshape((2, -1))\n        cmp_mean_vector = cmp_min_max[0, ]\n        cmp_std_vector  = cmp_min_max[1, ]\n\n\n        try:\n            os.makedirs(model_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create model directory %s\' % model_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        try:\n            train_DNN(train_xy_file_list = (train_x_file_list, train_y_file_list), \\\n                      valid_xy_file_list = (valid_x_file_list, valid_y_file_list), \\\n                      nnets_file_name = nnets_file_name, \\\n                      n_ins = lab_dim, n_outs = cfg.cmp_dim, ms_outs = cfg.multistream_outs, \\\n                      hyper_params = cfg.hyper_params, buffer_size = cfg.buffer_size, plot = cfg.plot, var_dict = var_dict,\n                      cmp_mean_vector = cmp_mean_vector, cmp_std_vector = cmp_std_vector)\n        except KeyboardInterrupt:\n            logger.critical(\'train_DNN interrupted via keyboard\')\n            # Could \'raise\' the exception further, but that causes a deep traceback to be printed\n            # which we don\'t care about for a keyboard interrupt. So, just bail out immediately\n            sys.exit(1)\n        except:\n            logger.critical(\'train_DNN threw an exception\')\n            raise\n\n\n\n    if cfg.GENBNFEA:\n        \'\'\'\n        Please only tune on this step when you want to generate bottleneck features from DNN\n        \'\'\'\n        temp_dir_name = \'%s_%s_%d_%d_%d_%d_%s_hidden\' \\\n                        %(cfg.model_type, cfg.combined_feature_name, \\\n                          cfg.train_file_number, lab_dim, cfg.cmp_dim, \\\n                          len(hidden_layers_sizes), combined_model_arch)\n        gen_dir = os.path.join(gen_dir, temp_dir_name)\n\n        bottleneck_size = min(hidden_layers_sizes)\n        bottleneck_index = 0\n        for i in range(len(hidden_layers_sizes)):\n            if hidden_layers_sizes(i) == bottleneck_size:\n                bottleneck_index = i\n\n        logger.info(\'generating bottleneck features from DNN\')\n\n        try:\n            os.makedirs(gen_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create generation directory %s\' % gen_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        gen_file_id_list = file_id_list[0:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n        test_x_file_list  = nn_label_norm_file_list[0:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n        gen_file_list = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.cmp_ext)\n\n        dnn_hidden_generation(test_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list, bottleneck_index)\n\n    ### generate parameters from DNN\n    temp_dir_name = \'%s_%s_%d_%d_%d_%d_%d_%d_%d\' \\\n                    %(cfg.combined_model_name, cfg.combined_feature_name, int(cfg.do_post_filtering), \\\n                      cfg.train_file_number, lab_dim, cfg.cmp_dim, \\\n                      len(hidden_layer_size), hidden_layer_size[0], hidden_layer_size[-1])\n    gen_dir = os.path.join(gen_dir, temp_dir_name)\n\n    gen_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n    test_x_file_list  = nn_label_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n    if cfg.GenTestList:\n        gen_file_id_list = test_id_list\n        test_x_file_list = nn_label_norm_file_list\n\n    if cfg.DNNGEN:\n        logger.info(\'generating from DNN\')\n\n        try:\n            os.makedirs(gen_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create generation directory %s\' % gen_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        gen_file_list = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.cmp_ext)\n        dnn_generation(test_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)\n\n        logger.debug(\'denormalising generated output using method %s\' % cfg.output_feature_normalisation)\n\n        fid = open(norm_info_file, \'rb\')\n        cmp_min_max = numpy.fromfile(fid, dtype=numpy.float32)\n        fid.close()\n        cmp_min_max = cmp_min_max.reshape((2, -1))\n        cmp_min_vector = cmp_min_max[0, ]\n        cmp_max_vector = cmp_min_max[1, ]\n\n        if cfg.output_feature_normalisation == \'MVN\':\n            denormaliser = MeanVarianceNorm(feature_dimension = cfg.cmp_dim)\n            denormaliser.feature_denormalisation(gen_file_list, gen_file_list, cmp_min_vector, cmp_max_vector)\n\n        elif cfg.output_feature_normalisation == \'MINMAX\':\n            denormaliser = MinMaxNormalisation(cfg.cmp_dim, min_value = 0.01, max_value = 0.99, min_vector = cmp_min_vector, max_vector = cmp_max_vector)\n            denormaliser.denormalise_data(gen_file_list, gen_file_list)\n        else:\n            logger.critical(\'denormalising method %s is not supported!\\n\' %(cfg.output_feature_normalisation))\n            raise\n\n        if cfg.AcousticModel:\n            ##perform MLPG to smooth parameter trajectory\n            ## lf0 is included, the output features much have vuv.\n            generator = ParameterGeneration(gen_wav_features = cfg.gen_wav_features)\n            generator.acoustic_decomposition(gen_file_list, cfg.cmp_dim, cfg.out_dimension_dict, cfg.file_extension_dict, var_file_dict, do_MLPG=cfg.do_MLPG)\n\n        if cfg.DurationModel:\n            ### Perform duration normalization(min. state dur set to 1) ###\n            gen_dur_list   = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.dur_ext)\n            gen_label_list = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.lab_ext)\n            in_gen_label_align_file_list = prepare_file_path_list(gen_file_id_list, cfg.in_label_align_dir, cfg.lab_ext, False)\n\n            generator = ParameterGeneration(gen_wav_features = cfg.gen_wav_features)\n            generator.duration_decomposition(gen_file_list, cfg.cmp_dim, cfg.out_dimension_dict, cfg.file_extension_dict)\n\n            label_modifier = HTSLabelModification(silence_pattern = cfg.silence_pattern)\n            label_modifier.modify_duration_labels(in_gen_label_align_file_list, gen_dur_list, gen_label_list)\n\n\n    ### generate wav\n    if cfg.GENWAV:\n        logger.info(\'reconstructing waveform(s)\')\n        print(len(gen_file_id_list))\n        generate_wav(gen_dir, gen_file_id_list, cfg)     # generated speech\n#       generate_wav(nn_cmp_dir, gen_file_id_list, cfg)  # reference copy synthesis speech\n\n    ### setting back to original conditions before calculating objective scores ###\n    if cfg.GenTestList:\n        in_label_align_file_list = prepare_file_path_list(file_id_list, cfg.in_label_align_dir, cfg.lab_ext, False)\n        binary_label_file_list   = prepare_file_path_list(file_id_list, binary_label_dir, cfg.lab_ext)\n        gen_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n    ### evaluation: RMSE and CORR for duration\n    if cfg.CALMCD and cfg.DurationModel:\n        logger.info(\'calculating MCD\')\n\n        ref_data_dir = os.path.join(data_dir, \'ref_data\')\n\n        ref_dur_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.dur_ext)\n\n        in_gen_label_align_file_list = in_label_align_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n        calculator = IndividualDistortionComp()\n\n        valid_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n        test_file_id_list  = file_id_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n        if cfg.remove_silence_using_binary_labels:\n            untrimmed_reference_data = in_file_list_dict[\'dur\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n            trim_silence(untrimmed_reference_data, ref_dur_list, cfg.dur_dim, \\\n                                untrimmed_test_labels, lab_dim, silence_feature)\n        else:\n            remover = SilenceRemover(n_cmp = cfg.dur_dim, silence_pattern = cfg.silence_pattern, remove_frame_features = cfg.add_frame_features)\n            remover.remove_silence(in_file_list_dict[\'dur\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_dur_list)\n\n        valid_dur_rmse, valid_dur_corr = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.dur_ext, cfg.dur_dim)\n        test_dur_rmse, test_dur_corr = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.dur_ext, cfg.dur_dim)\n\n        logger.info(\'Develop: DNN -- RMSE: %.3f frames/phoneme; CORR: %.3f; \' \\\n                    %(valid_dur_rmse, valid_dur_corr))\n        logger.info(\'Test: DNN -- RMSE: %.3f frames/phoneme; CORR: %.3f; \' \\\n                    %(test_dur_rmse, test_dur_corr))\n\n    ### evaluation: calculate distortion\n    if cfg.CALMCD and cfg.AcousticModel:\n        logger.info(\'calculating MCD\')\n\n        ref_data_dir = os.path.join(data_dir, \'ref_data\')\n\n        ref_mgc_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.mgc_ext)\n        ref_bap_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.bap_ext)\n        ref_lf0_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.lf0_ext)\n\n        in_gen_label_align_file_list = in_label_align_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n        calculator = IndividualDistortionComp()\n\n        spectral_distortion = 0.0\n        bap_mse             = 0.0\n        f0_mse              = 0.0\n        vuv_error           = 0.0\n\n        valid_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n        test_file_id_list  = file_id_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n        if cfg.remove_silence_using_binary_labels:\n            ## get lab_dim:\n            label_composer = LabelComposer()\n            label_composer.load_label_configuration(cfg.label_config_file)\n            lab_dim=label_composer.compute_label_dimension()\n\n            ## use first feature in label -- hardcoded for now\n            silence_feature = 0\n\n            ## Use these to trim silence:\n            untrimmed_test_labels = binary_label_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n\n        if \'mgc\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'mgc\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_mgc_list, cfg.mgc_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.mgc_dim, silence_pattern = cfg.silence_pattern)\n                remover.remove_silence(in_file_list_dict[\'mgc\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_mgc_list)\n            valid_spectral_distortion = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.mgc_ext, cfg.mgc_dim)\n            test_spectral_distortion  = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.mgc_ext, cfg.mgc_dim)\n            valid_spectral_distortion *= (10 /numpy.log(10)) * numpy.sqrt(2.0)    ##MCD\n            test_spectral_distortion  *= (10 /numpy.log(10)) * numpy.sqrt(2.0)    ##MCD\n\n\n        if \'bap\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'bap\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_bap_list, cfg.bap_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.bap_dim, silence_pattern = cfg.silence_pattern)\n                remover.remove_silence(in_file_list_dict[\'bap\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_bap_list)\n            valid_bap_mse        = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.bap_ext, cfg.bap_dim)\n            test_bap_mse         = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.bap_ext, cfg.bap_dim)\n            valid_bap_mse = valid_bap_mse / 10.0    ##Cassia\'s bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC\n            test_bap_mse  = test_bap_mse / 10.0    ##Cassia\'s bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC\n\n        if \'lf0\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'lf0\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_lf0_list, cfg.lf0_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.lf0_dim, silence_pattern = cfg.silence_pattern)\n                remover.remove_silence(in_file_list_dict[\'lf0\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_lf0_list)\n            valid_f0_mse, valid_f0_corr, valid_vuv_error   = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.lf0_ext, cfg.lf0_dim)\n            test_f0_mse , test_f0_corr, test_vuv_error    = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.lf0_ext, cfg.lf0_dim)\n\n        logger.info(\'Develop: DNN -- MCD: %.3f dB; BAP: %.3f dB; F0:- RMSE: %.3f Hz; CORR: %.3f; VUV: %.3f%%\' \\\n                    %(valid_spectral_distortion, valid_bap_mse, valid_f0_mse, valid_f0_corr, valid_vuv_error*100.))\n        logger.info(\'Test   : DNN -- MCD: %.3f dB; BAP: %.3f dB; F0:- RMSE: %.3f Hz; CORR: %.3f; VUV: %.3f%%\' \\\n                    %(test_spectral_distortion , test_bap_mse , test_f0_mse , test_f0_corr, test_vuv_error*100.))\n\nif __name__ == \'__main__\':\n\n\n\n    # these things should be done even before trying to parse the command line\n\n    # create a configuration instance\n    # and get a short name for this instance\n    cfg=configuration.cfg\n\n    # set up logging to use our custom class\n    logging.setLoggerClass(LoggerPlotter)\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n\n    if len(sys.argv) != 2:\n        logger.critical(\'usage: run_dnn.sh [config file name]\')\n        sys.exit(1)\n\n    config_file = sys.argv[1]\n\n    config_file = os.path.abspath(config_file)\n    cfg.configure(config_file)\n\n    if cfg.profile:\n        logger.info(\'profiling is activated\')\n        import cProfile, pstats\n        cProfile.run(\'main_function(cfg)\', \'mainstats\')\n\n        # create a stream for the profiler to write to\n        profiling_output = io.StringIO()\n        p = pstats.Stats(\'mainstats\', stream=profiling_output)\n\n        # print stats to that stream\n        # here we just report the top 10 functions, sorted by total amount of time spent in each\n        p.strip_dirs().sort_stats(\'tottime\').print_stats(10)\n\n        # print the result to the log\n        logger.info(\'---Profiling result follows---\\n%s\' %  profiling_output.getvalue() )\n        profiling_output.close()\n        logger.info(\'---End of profiling result---\')\n\n    else:\n        main_function(cfg)\n\n#    if gnp._boardId is not None:\n#        import gpu_lock\n#        gpu_lock.free_lock(gnp._boardId)\n\n    sys.exit(0)\n'"
src/work_in_progress/store_model.py,0,"b'\nimport pickle\nimport gzip\nimport os, sys, errno\nimport time\nimport math\n\n#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)\nimport numpy\n#import gnumpy as gnp\n# we need to explicitly import this in some cases, not sure why this doesn\'t get imported with numpy itself\nimport numpy.distutils.__config__\n# and only after that can we import theano\nimport theano\n#import theano.tensor as T\n\n\nfrom utils.providers import ListDataProvider\n\nfrom frontend.label_normalisation import HTSLabelNormalisation, HTSDurationLabelNormalisation, XMLLabelNormalisation\nfrom frontend.silence_remover import SilenceRemover\nfrom frontend.silence_remover import trim_silence\nfrom frontend.min_max_norm import MinMaxNormalisation\nfrom frontend.acoustic_composition import AcousticComposition\nfrom frontend.parameter_generation import ParameterGeneration\nfrom frontend.mean_variance_norm import MeanVarianceNorm\n\n# the new class for label composition and normalisation\nfrom frontend.label_composer import LabelComposer\n#from frontend.mlpg_fast import MLParameterGenerationFast\n\n#from frontend.mlpg_fast_layer import MLParameterGenerationFastLayer\n\n\nimport configuration\nfrom models.deep_rnn import DeepRecurrentNetwork\n\nfrom utils.compute_distortion import DistortionComputation, IndividualDistortionComp\nfrom utils.generate import generate_wav\nfrom utils.learn_rates import ExpDecreaseLearningRate\n\nfrom io_funcs.binary_io import  BinaryIOCollection\n\n#import matplotlib.pyplot as plt\n# our custom logging class that can also plot\n#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot\nfrom logplot.logging_plotting import LoggerPlotter, MultipleSeriesPlot, SingleWeightMatrixPlot\nimport logging # as logging\nimport logging.config\nimport io\n\n\ndef store_network(nnets_file_name, outdir):\n    print(\'store network\')\n\n    if not os.path.isdir(outdir):\n        os.makedirs(outdir)\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n\n    names = [p.name for p in dnn_model.params]\n    param_vals = [p.get_value(borrow=True) for p in dnn_model.params]\n    shapes = [numpy.shape(p) for p in param_vals]\n    print(cfg.hidden_layer_size)\n    layer_types = cfg.hidden_layer_type\n    if cfg.output_activation == \'linear\':\n        layer_types.append(\'LINEAR\')\n    else:\n        sys.exit(\'unsupported output activation\')\n    assert len(param_vals) == len(layer_types) * 2 ##  W and b for each layer\n    print(names)\n\n\n    p_ix = 0\n    for (l_ix, layer_type) in enumerate(layer_types):\n        layer_name = \'LAYER_\' + str(l_ix+1).zfill(3) + \'_\' + layer_type + \'_\'\n        #print layer_name\n        for part in [\'W\',\'b\']:\n            assert names[p_ix] == part\n            fname = layer_name + part\n            print(fname)\n            #numpy.savetxt(os.path.join(outdir, fname + \'.txt\'), param_vals[p_ix])\n            numpy.save(os.path.join(outdir, fname + \'.npy\'), param_vals[p_ix])\n\n            p_ix += 1\n\n    ### Input normalisation:-\n    if cfg.process_labels_in_work_dir:\n        label_data_dir = cfg.work_dir\n    else:\n        label_data_dir = cfg.data_dir\n    label_norm_file = \'label_norm_%s.dat\' %(cfg.label_style)\n    label_norm_file = os.path.join(label_data_dir, label_norm_file)\n\n    lab_norm_data = numpy.fromfile(label_norm_file, \'float32\')\n    labsize = numpy.shape(lab_norm_data)[0]\n\n    min_vect = lab_norm_data[:(labsize/2)]\n    max_vect = lab_norm_data[(labsize/2):]\n\n    print(min_vect)\n    print(max_vect)\n\n    fname = \'NORM_INPUT_MIN\'\n    numpy.save(os.path.join(outdir, fname + \'.npy\'), min_vect)\n    fname = \'NORM_INPUT_MAX\'\n    numpy.save(os.path.join(outdir, fname + \'.npy\'), max_vect)\n\n\n    ## output norm\n    assert cfg.output_feature_normalisation == \'MVN\'\n    norm_info_file = os.path.join(cfg.data_dir, \\\n        \'norm_info\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim) + \'_\' + cfg.output_feature_normalisation + \'.dat\')\n\n    out_norm_data = numpy.fromfile(norm_info_file, \'float32\')\n    outsize = numpy.shape(out_norm_data)[0]\n\n    mean_vect = out_norm_data[:(outsize/2)]\n    std_vect = out_norm_data[(outsize/2):]\n\n    print(mean_vect)\n    print(std_vect)\n\n    fname = \'NORM_OUTPUT_MEAN\'\n    numpy.save(os.path.join(outdir, fname + \'.npy\'), mean_vect)\n    fname = \'NORM_OUTPUT_STD\'\n    numpy.save(os.path.join(outdir, fname + \'.npy\'), std_vect)\n\n\n    in_streams = list(cfg.in_dimension_dict.keys())\n    indims = [str(cfg.in_dimension_dict[s]) for s in in_streams]\n    out_streams = list(cfg.out_dimension_dict.keys())\n    outdims = [str(cfg.out_dimension_dict[s]) for s in out_streams]\n\n    f = open(os.path.join(outdir, \'stream_info.txt\'), \'w\')\n    f.write(\' \'.join(in_streams) + \'\\n\')\n    f.write(\' \'.join(indims) + \'\\n\')\n    f.write(\' \'.join(out_streams) + \'\\n\')\n    f.write(\' \'.join(outdims) + \'\\n\')\n    f.close()\n\ndef main_function(cfg, outdir, model_pickle_file=None):\n\n    hidden_layer_size = cfg.hyper_params[\'hidden_layer_size\']\n    data_dir = cfg.data_dir\n    model_dir = os.path.join(cfg.work_dir, \'nnets_model\')\n#     norm_info_file = os.path.join(data_dir, \'norm_info\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim) + \'_\' + cfg.output_feature_normalisation + \'.dat\')\n\n    ### normalise input full context label\n    if cfg.label_style == \'HTS\':\n        label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name)\n        lab_dim = label_normaliser.dimension + cfg.appended_input_dim\n        print((\'Input label dimension is %d\' % lab_dim))\n        suffix=str(lab_dim)\n    elif cfg.label_style == \'HTS_duration\':\n        label_normaliser = HTSDurationLabelNormalisation(question_file_name=cfg.question_file_name)\n        lab_dim = label_normaliser.dimension ## + cfg.appended_input_dim\n        print((\'Input label dimension is %d\' % lab_dim))\n        suffix=str(lab_dim)\n    # no longer supported - use new ""composed"" style labels instead\n    elif cfg.label_style == \'composed\':\n        # label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)\n        suffix=\'composed\'\n\n\n\n    combined_model_arch = str(len(hidden_layer_size))\n    for hid_size in hidden_layer_size:\n        combined_model_arch += \'_\' + str(hid_size)\n\n    ## if made with run_lstm:--\n    \'\'\'\n    nnets_file_name = \'%s/%s_%s_%d_%s_%d.%d.train.%d.%f.rnn.model\' \\\n                      %(model_dir, cfg.combined_model_name, cfg.combined_feature_name, int(cfg.multistream_switch),\n                        combined_model_arch, lab_dim, cfg.cmp_dim, cfg.train_file_number, cfg.hyper_params[\'learning_rate\'])\n    \'\'\'\n\n    ## if made with run_dnn:--\n    nnets_file_name = \'%s/%s_%s_%d_%s_%d.%d.train.%d.model\' \\\n                      %(model_dir, cfg.model_type, cfg.combined_feature_name, int(cfg.multistream_switch),\n                        combined_model_arch, lab_dim, cfg.cmp_dim, cfg.train_file_number)\n\n    ## override the name computed from config variables if model_pickle_file specified:\n    if model_pickle_file != None:\n        nnets_file_name = model_pickle_file\n\n    print(\'store DNN\')\n\n\n    store_network(nnets_file_name, outdir)\n\n\nif __name__ == \'__main__\':\n    cfg=configuration.cfg\n    if len(sys.argv) not in [3, 4]:\n        print(\'usage: run_dnn.sh [config file name]\')\n        sys.exit(1)\n\n    if len(sys.argv) == 3:\n        config_file = sys.argv[1]\n        outdir = sys.argv[2]\n\n        model_pickle_file = None\n\n\n\n    elif len(sys.argv) == 4:\n        config_file = sys.argv[1]\n        model_pickle_file = sys.argv[2]\n        outdir = sys.argv[3]\n\n\n    config_file = os.path.abspath(config_file)\n    cfg.configure(config_file)\n\n    main_function(cfg, outdir, model_pickle_file=model_pickle_file)\n'"
egs/slt_arctic/s2/run_demo.py,0,"b'#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n""""""\n\n@author: Felipe Espic\n""""""\nfrom shutil import copytree, copy2\nimport scripts.label_st_align_to_var_rate as ltvr\nfrom os.path import join, dirname, realpath, isdir\nimport sys\nthis_dir = dirname(realpath(__file__))\nsys.path.append(realpath(this_dir + \'/../../../tools/magphase/src\'))\nimport libutils as lu\nimport magphase as mp\nimport configparser # Install it with pip (it\'s not the same as \'ConfigParser\' (old version))\nfrom subprocess import call\n\n\ndef feat_extraction(in_wav_dir, file_name_token, out_feats_dir, d_opts):\n\n    # Display:\n    print(""\\nAnalysing file: "" + file_name_token + \'.wav............................\')\n\n    # File setup:\n    wav_file = join(in_wav_dir, file_name_token + \'.wav\')\n\n    mp.analysis_for_acoustic_modelling(wav_file, out_feats_dir,\n                                        mag_dim=d_opts[\'mag_dim\'],\n                                        phase_dim=d_opts[\'phase_dim\'],\n                                        b_const_rate=d_opts[\'b_const_rate\'])\n    return\n\n\ndef open_config_file(configfile_path):\n    parser = configparser.ConfigParser()\n    parser.optionxform = str\n    parser.read([configfile_path])\n    return parser\n\ndef save_config(parser, file_path):\n    with open(file_path, \'wb\') as file:\n        parser.write(file)\n    return\n\ndef mod_acoustic_config(parser, merlin_path, exper_path, exper_type, d_mp_opts):\n    parser[\'DEFAULT\'][\'Merlin\']   = merlin_path\n    parser[\'DEFAULT\'][\'TOPLEVEL\'] = exper_path\n\n    parser[\'Outputs\'][\'mag\' ] = \'%d\' %  d_mp_opts[\'mag_dim\']\n    parser[\'Outputs\'][\'dmag\'] = \'%d\' % (d_mp_opts[\'mag_dim\']*3)\n\n    parser[\'Outputs\'][\'real\' ] = \'%d\' %  d_mp_opts[\'phase_dim\']\n    parser[\'Outputs\'][\'imag\' ] = \'%d\' %  d_mp_opts[\'phase_dim\']\n    parser[\'Outputs\'][\'dreal\'] = \'%d\' % (d_mp_opts[\'phase_dim\']*3)\n    parser[\'Outputs\'][\'dimag\'] = \'%d\' % (d_mp_opts[\'phase_dim\']*3)\n\n    if exper_type==\'full\':\n        parser[\'Architecture\'][\'hidden_layer_size\'] = ""[1024, 1024, 1024, 1024, 1024, 1024]""\n        parser[\'Architecture\'][\'hidden_layer_type\'] = ""[\'TANH\', \'TANH\', \'TANH\', \'TANH\', \'TANH\', \'TANH\']""\n        parser[\'Architecture\'][\'model_file_name\']   = ""feed_forward_6_tanh""\n\n    if d_mp_opts[\'b_const_rate\']:\n        parser[\'Labels\'][\'label_align\'] = \'%(TOPLEVEL)s/acoustic_model/data/label_state_align\'\n\n    parser = mod_number_of_utts(parser, exper_type)\n\n    return parser\n\ndef mod_duration_config(parser, merlin_path, exper_path, exper_type, d_mp_opts):\n    parser[\'DEFAULT\'][\'Merlin\']   = merlin_path\n    parser[\'DEFAULT\'][\'TOPLEVEL\'] = exper_path\n\n    if exper_type==\'full\':\n        parser[\'Architecture\'][\'hidden_layer_size\'] = ""[1024, 1024, 1024, 1024, 1024, 1024]""\n        parser[\'Architecture\'][\'hidden_layer_type\'] = ""[\'TANH\', \'TANH\', \'TANH\', \'TANH\', \'TANH\', \'TANH\']""\n        parser[\'Architecture\'][\'model_file_name\']   = ""feed_forward_6_tanh""\n\n    if d_mp_opts[\'b_const_rate\']:\n        parser[\'Labels\'][\'label_align\'] = \'%(TOPLEVEL)s/acoustic_model/data/label_state_align\'\n\n    parser = mod_number_of_utts(parser, exper_type)\n\n    return parser\n\ndef mod_number_of_utts(parser, exper_type):\n\n    if exper_type==\'full\':\n        parser[\'Paths\'][\'file_id_list\'] = \'%(data)s/file_id_list_full.scp\'\n        parser[\'Data\'][\'train_file_number\'] = \'%d\' % 1000\n        parser[\'Data\'][\'valid_file_number\'] = \'%d\' % 66\n        parser[\'Data\'][\'test_file_number\' ] = \'%d\' % 65\n\n    elif exper_type==\'demo\':\n        pass\n\n    return parser\n\n\nif __name__ == \'__main__\':\n\n    # INPUT:===================================================================================================\n\n    # Experiment type:-----------------------------------------------------------------------\n    exper_type = \'demo\'  #  \'demo\' (50 training utts) or \'full\' (1k training utts)\n\n    # Steps:---------------------------------------------------------------------------------\n    b_download_data  = 1 # Downloads wavs and label data.\n    b_setup_data     = 1 # Copies downloaded data into the experiment directory. Plus, makes a backup copy of this script.\n    b_config_merlin  = 1 # Saves new configuration files for Merlin.\n    b_feat_extr      = 1 # Performs acoustic feature extraction using the MagPhase vocoder\n    b_conv_labs_rate = 1 # Converts the state aligned labels to variable rate if running in variable frame rate mode (d_mp_opts[\'b_const_rate\'] = False)\n    b_dur_train      = 1 # Merlin: Training of duration model.\n    b_acous_train    = 1 # Merlin: Training of acoustic model.\n    b_dur_syn        = 1 # Merlin: Generation of state durations using the duration model.\n    b_acous_syn      = 1 # Merlin: Waveform generation for the utterances provided in ./test_synthesis/prompt-lab\n\n    # MagPhase Vocoder:-----------------------------------------------------------------------\n    d_mp_opts = {}                     # Dictionary containing internal options for the MagPhase vocoder (mp).\n    d_mp_opts[\'mag_dim\'   ] = 100       # Number of coefficients (bins) for magnitude feature M.\n    d_mp_opts[\'phase_dim\' ] = 10       # Number of coefficients (bins) for phase features R and I.\n    d_mp_opts[\'b_const_rate\'] = False  # To work in constant frame rate mode.\n    d_mp_opts[\'l_pf_type\'   ] = [ \'no\', \'magphase\', \'merlin\'] #  List containing the postfilters to apply during waveform generation.\n    # You need to choose at least one: \'magphase\' (magphase-tailored postfilter), \'merlin\' (Merlin\'s style postfilter), \'no\' (no postfilter)\n\n    b_feat_ext_multiproc      = 1     # Acoustic feature extraction done in multiprocessing mode (faster).\n\n\n    # PROCESS:===================================================================================================\n    # Pre setup:-------------------------------------------------------------------------------\n    exper_name  = \'slt_arctic_magphase_%s_mag_dim_%s_phase_dim_%d_const_rate_%d\' % (exper_type, d_mp_opts[\'mag_dim\'], d_mp_opts[\'phase_dim\'], d_mp_opts[\'b_const_rate\'])\n    exper_path  = join(this_dir, \'experiments\' , exper_name)\n    merlin_path = realpath(this_dir + \'/../../..\')\n    submit_path     = join(this_dir, \'scripts\', \'submit.sh\')\n    run_merlin_path = join(merlin_path, \'src\', \'run_merlin.py\')\n    dur_model_conf_path   = join(exper_path, \'duration_model\', \'conf\')\n    acous_model_conf_path = join(exper_path, \'acoustic_model\'   , \'conf\')\n\n    # Build config parsers:-------------------------------------------------------------------\n\n    # Duration training config file:\n    pars_dur_train = open_config_file(join(this_dir, \'conf_base\', \'dur_train_base.conf\'))\n    pars_dur_train = mod_duration_config(pars_dur_train, merlin_path, exper_path, exper_type, d_mp_opts)\n\n    # Duration synthesis:\n    pars_dur_synth = open_config_file(join(this_dir, \'conf_base\', \'dur_synth_base.conf\'))\n    pars_dur_synth = mod_duration_config(pars_dur_synth, merlin_path, exper_path, exper_type, d_mp_opts)\n\n    # Acoustic training:\n    pars_acous_train = open_config_file(join(this_dir, \'conf_base\', \'acous_train_base.conf\'))\n    pars_acous_train = mod_acoustic_config(pars_acous_train, merlin_path, exper_path, exper_type, d_mp_opts)\n\n    # Acoustic synth:\n    pars_acous_synth = open_config_file(join(this_dir, \'conf_base\', \'acous_synth_base.conf\'))\n    pars_acous_synth = mod_acoustic_config(pars_acous_synth, merlin_path, exper_path, exper_type, d_mp_opts)\n\n    # Download Data:--------------------------------------------------------------------------\n    if b_download_data:\n        data_zip_file = join(this_dir, \'slt_arctic_%s_data.zip\' % exper_type)\n        call([\'wget\', \'http://felipeespic.com/depot/databases/merlin_demos/slt_arctic_%s_data.zip\' % exper_type , \'-O\', data_zip_file])\n        call([\'unzip\', \'-o\', \'-q\', data_zip_file, \'-d\', this_dir])\n\n    # Setup Data:-----------------------------------------------------------------------------\n    if b_setup_data:\n        copytree(join(this_dir, \'slt_arctic_\' + exper_type + \'_data\', \'exper\'), exper_path)\n        copy2(__file__, join(exper_path, \'run_demo_backup.py\'))\n\n    # Configure Merlin:-----------------------------------------------------------------------\n    if b_config_merlin:\n        save_config(pars_dur_train,   join(dur_model_conf_path  , \'dur_train.conf\'))\n        save_config(pars_dur_synth,   join(dur_model_conf_path  , \'dur_synth.conf\'))\n        save_config(pars_acous_train, join(acous_model_conf_path, \'acous_train.conf\'))\n        save_config(pars_acous_synth, join(acous_model_conf_path, \'acous_synth.conf\'))\n\n        copy2(join(this_dir, \'conf_base\', \'logging_config.conf\'), join(exper_path, \'acoustic_model\', \'conf\', \'logging_config.conf\'))\n\n    # Read file list:\n    file_id_list = pars_acous_train[\'Paths\'][\'file_id_list\']\n    l_file_tokns = lu.read_text_file2(file_id_list, dtype=\'string\', comments=\'#\').tolist()\n    acoustic_feats_path = pars_acous_train[\'Paths\'][\'in_acous_feats_dir\']\n\n    # Acoustic Feature Extraction:-------------------------------------------------------------\n    if b_feat_extr:\n        # Extract features:\n        lu.mkdir(acoustic_feats_path)\n\n        if b_feat_ext_multiproc:\n            lu.run_multithreaded(feat_extraction, join(exper_path, \'acoustic_model\', \'data\', \'wav\'), l_file_tokns, acoustic_feats_path, d_mp_opts)\n        else:\n            for file_name_token in l_file_tokns:\n                feat_extraction(join(exper_path, \'acoustic_model\', \'data\', \'wav\'), file_name_token, acoustic_feats_path, d_mp_opts)\n\n    # Labels Conversion to Variable Frame Rate:------------------------------------------------\n    if b_conv_labs_rate and not d_mp_opts[\'b_const_rate\']: # NOTE: The script ./script/label_st_align_to_var_rate.py can be also called from comand line directly.\n        label_state_align = join(exper_path, \'acoustic_model\', \'data\', \'label_state_align\')\n        label_state_align_var_rate = pars_acous_train[\'Labels\'][\'label_align\']\n        fs = int(pars_acous_train[\'Waveform\'][\'samplerate\'])\n        ltvr.convert(file_id_list,label_state_align, acoustic_feats_path, fs, label_state_align_var_rate)\n\n    # Run duration training:-------------------------------------------------------------------\n    if b_dur_train:\n        call([submit_path, run_merlin_path, join(dur_model_conf_path, \'dur_train.conf\')])\n\n    # Run acoustic train:----------------------------------------------------------------------\n    if b_acous_train:\n        call([submit_path,run_merlin_path, join(acous_model_conf_path, \'acous_train.conf\')])\n\n    # Run duration syn:------------------------------------------------------------------------\n    if b_dur_syn:\n        call([submit_path, run_merlin_path, join(dur_model_conf_path, \'dur_synth.conf\')])\n\n    # Run acoustic synth:----------------------------------------------------------------------\n    if b_acous_syn:\n        call([submit_path, run_merlin_path, join(acous_model_conf_path, \'acous_synth.conf\')])\n\n\n    print(""Done!"")\n\n\n\n\n\n\n'"
misc/recipes/MGE/run_dnn_cm.py,0,"b'\nimport pickle\nimport gzip\nimport os, sys, errno\nimport time\nimport math\n\n#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)\nimport numpy\n#import gnumpy as gnp\n# we need to explicitly import this in some cases, not sure why this doesn\'t get imported with numpy itself\nimport numpy.distutils.__config__\n# and only after that can we import theano\n#import theano\n\nfrom utils.providers import ListDataProvider\n\nfrom frontend.label_normalisation import HTSLabelNormalisation, XMLLabelNormalisation\nfrom frontend.silence_remover import SilenceRemover\nfrom frontend.silence_remover import trim_silence\nfrom frontend.min_max_norm import MinMaxNormalisation\nfrom frontend.acoustic_composition import AcousticComposition\nfrom frontend.parameter_generation import ParameterGeneration\nfrom frontend.mean_variance_norm import MeanVarianceNorm\n\n# the new class for label composition and normalisation\nfrom frontend.label_composer import LabelComposer\n#from frontend.mlpg_fast import MLParameterGenerationFast\n\n#from frontend.mlpg_fast_layer import MLParameterGenerationFastLayer\n\n\nimport configuration\nfrom models.dnn_cm import DNN\n\nfrom utils.compute_distortion import DistortionComputation, IndividualDistortionComp\nfrom utils.generate import generate_wav\nfrom utils.learn_rates import ExpDecreaseLearningRate\n\nfrom io_funcs.binary_io import  BinaryIOCollection\n\n#import matplotlib.pyplot as plt\n# our custom logging class that can also plot\n#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot\nfrom logplot.logging_plotting import LoggerPlotter, MultipleSeriesPlot, SingleWeightMatrixPlot\nimport logging # as logging\nimport logging.config\nimport io\n\n\ndef extract_file_id_list(file_list):\n    file_id_list = []\n    for file_name in file_list:\n        file_id = os.path.basename(os.path.splitext(file_name)[0])\n        file_id_list.append(file_id)\n\n    return  file_id_list\n\ndef read_file_list(file_name):\n\n    logger = logging.getLogger(""read_file_list"")\n\n    file_lists = []\n    fid = open(file_name)\n    for line in fid.readlines():\n        line = line.strip()\n        if len(line) < 1:\n            continue\n        file_lists.append(line)\n    fid.close()\n\n    logger.debug(\'Read file list from %s\' % file_name)\n    return  file_lists\n\n\ndef make_output_file_list(out_dir, in_file_lists):\n    out_file_lists = []\n\n    for in_file_name in in_file_lists:\n        file_id = os.path.basename(in_file_name)\n        out_file_name = out_dir + \'/\' + file_id\n        out_file_lists.append(out_file_name)\n\n    return  out_file_lists\n\ndef prepare_file_path_list(file_id_list, file_dir, file_extension, new_dir_switch=True):\n    if not os.path.exists(file_dir) and new_dir_switch:\n        os.makedirs(file_dir)\n    file_name_list = []\n    for file_id in file_id_list:\n        file_name = file_dir + \'/\' + file_id + file_extension\n        file_name_list.append(file_name)\n\n    return  file_name_list\n\n\n\ndef visualize_dnn(dnn):\n\n    layer_num = len(dnn.params)     ## including input and output\n    plotlogger = logging.getLogger(""plotting"")\n\n    for i in range(layer_num):\n        fig_name = \'Activation weights W\' + str(i) + \'_\' + dnn.params[i].name\n        fig_title = \'Activation weights of W\' + str(i)\n        xlabel = \'Neuron index of hidden layer \' + str(i)\n        ylabel = \'Neuron index of hidden layer \' + str(i+1)\n        if i == 0:\n            xlabel = \'Input feature index\'\n        if i == layer_num-1:\n            ylabel = \'Output feature index\'\n\n        aa = dnn.params[i].get_value(borrow=True).T\n        print(aa.shape, aa.size)\n        if aa.size > aa.shape[0]:\n            logger.create_plot(fig_name, SingleWeightMatrixPlot)\n            plotlogger.add_plot_point(fig_name, fig_name, dnn.params[i].get_value(borrow=True).T)\n            plotlogger.save_plot(fig_name, title=fig_name, xlabel=xlabel, ylabel=ylabel)\n\ndef load_covariance(var_file_dict, out_dimension_dict):\n    var = {}\n    io_funcs = BinaryIOCollection()\n    for feature_name in list(var_file_dict.keys()):\n        var_values, dimension = io_funcs.load_binary_file_frame(var_file_dict[feature_name], 1)\n\n        var_values = numpy.reshape(var_values, (out_dimension_dict[feature_name], 1))\n\n        var[feature_name] = var_values\n\n    return  var\n\n\ndef train_DNN(train_xy_file_list, valid_xy_file_list, \\\n              nnets_file_name, n_ins, n_outs, ms_outs, hyper_params, buffer_size, plot=False, var_dict=None,\n              cmp_mean_vector = None, cmp_std_vector = None, init_dnn_model_file = None):\n\n    # get loggers for this function\n    # this one writes to both console and file\n    logger = logging.getLogger(""main.train_DNN"")\n    logger.debug(\'Starting train_DNN\')\n\n    if plot:\n        # this one takes care of plotting duties\n        plotlogger = logging.getLogger(""plotting"")\n        # create an (empty) plot of training convergence, ready to receive data points\n        logger.create_plot(\'training convergence\',MultipleSeriesPlot)\n\n    try:\n        assert numpy.sum(ms_outs) == n_outs\n    except AssertionError:\n        logger.critical(\'the summation of multi-stream outputs does not equal to %d\' %(n_outs))\n        raise\n\n    ####parameters#####\n    finetune_lr     = float(hyper_params[\'learning_rate\'])\n    training_epochs = int(hyper_params[\'training_epochs\'])\n    batch_size      = int(hyper_params[\'batch_size\'])\n    l1_reg          = float(hyper_params[\'l1_reg\'])\n    l2_reg          = float(hyper_params[\'l2_reg\'])\n    warmup_epoch    = int(hyper_params[\'warmup_epoch\'])\n    momentum        = float(hyper_params[\'momentum\'])\n    warmup_momentum = float(hyper_params[\'warmup_momentum\'])\n\n    hidden_layer_size = hyper_params[\'hidden_layer_size\']\n\n    buffer_utt_size = buffer_size\n    early_stop_epoch = int(hyper_params[\'early_stop_epochs\'])\n\n    hidden_activation = hyper_params[\'hidden_activation\']\n    output_activation = hyper_params[\'output_activation\']\n\n    model_type = hyper_params[\'model_type\']\n    hidden_layer_type  = hyper_params[\'hidden_layer_type\']\n\n    ## use a switch to turn on pretraining\n    ## pretraining may not help too much, if this case, we turn it off to save time\n    do_pretraining = hyper_params[\'do_pretraining\']\n    pretraining_epochs = int(hyper_params[\'pretraining_epochs\'])\n    pretraining_lr = float(hyper_params[\'pretraining_lr\'])\n\n    sequential_training = hyper_params[\'sequential_training\']\n\n#    sequential_training = True\n\n    buffer_size = int(buffer_size / batch_size) * batch_size\n\n    ###################\n    (train_x_file_list, train_y_file_list) = train_xy_file_list\n    (valid_x_file_list, valid_y_file_list) = valid_xy_file_list\n\n    logger.debug(\'Creating training   data provider\')\n    train_data_reader = ListDataProvider(x_file_list = train_x_file_list, y_file_list = train_y_file_list,\n                            n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, sequential = sequential_training, shuffle = True)\n\n    logger.debug(\'Creating validation data provider\')\n    valid_data_reader = ListDataProvider(x_file_list = valid_x_file_list, y_file_list = valid_y_file_list,\n                            n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, sequential = sequential_training, shuffle = False)\n\n    shared_train_set_xy, temp_train_set_x, temp_train_set_y = train_data_reader.load_one_partition()\n    train_set_x, train_set_y = shared_train_set_xy\n    shared_valid_set_xy, valid_set_x, valid_set_y = valid_data_reader.load_one_partition()   #validation data is still read block by block\n    valid_set_x, valid_set_y = shared_valid_set_xy\n    train_data_reader.reset()\n    valid_data_reader.reset()\n\n\n    ##temporally we use the training set as pretrain_set_x.\n    ##we need to support any data for pretraining\n#    pretrain_set_x = train_set_x\n\n    # numpy random generator\n    numpy_rng = numpy.random.RandomState(123)\n    logger.info(\'building the model\')\n\n\n    dnn_model = None\n    pretrain_fn = None  ## not all the model support pretraining right now\n    train_fn = None\n    valid_fn = None\n    valid_model = None ## valid_fn and valid_model are the same. reserve to computer multi-stream distortion\n    if model_type == \'DNN\':\n#        dnn_model = DeepRecurrentNetwork(n_in= n_ins, hidden_layer_size = hidden_layer_size, n_out = n_outs, L1_reg = l1_reg, L2_reg = l2_reg, hidden_layer_type = hidden_layer_type)\n\n#        dnn_model = SequentialDNN(numpy_rng=numpy_rng, n_ins=n_ins, n_outs = n_outs,\n#                        l1_reg = l1_reg, l2_reg = l2_reg,\n#                         hidden_layer_sizes = hidden_layer_size)\n        dnn_model = DNN(numpy_rng=numpy_rng, n_ins=n_ins, n_outs = n_outs,\n                        l1_reg = l1_reg, l2_reg = l2_reg,\n                         hidden_layer_sizes = hidden_layer_size)\n\n    else:\n        logger.critical(\'%s type NN model is not supported!\' %(model_type))\n        raise\n\n    logger.info(\'fine-tuning the %s model\' %(model_type))\n\n\n    start_time = time.time()\n\n    best_dnn_model = dnn_model\n    best_validation_loss = sys.float_info.max\n    previous_loss = sys.float_info.max\n\n    early_stop = 0\n    epoch = 0\n\n#    finetune_lr = 0.000125\n    previous_finetune_lr = finetune_lr\n\n    print(finetune_lr)\n\n    while (epoch < training_epochs):\n        epoch = epoch + 1\n\n        current_momentum = momentum\n        current_finetune_lr = finetune_lr\n        if epoch <= warmup_epoch:\n            current_finetune_lr = finetune_lr\n            current_momentum = warmup_momentum\n        else:\n            current_finetune_lr = previous_finetune_lr * 0.5\n\n        previous_finetune_lr = current_finetune_lr\n\n        train_error = []\n        sub_start_time = time.time()\n\n        while (not train_data_reader.is_finish()):\n            shared_train_set_xy, train_set_x, train_set_y = train_data_reader.load_one_partition()\n\n            n_train_batches = train_set_x.shape[0] / batch_size\n\n            logger.debug(\'this partition: %d frames (divided into %d batches of size %d)\' %(train_set_x.shape[0], n_train_batches, batch_size) )\n\n            all_batches = all_batches + n_train_batches\n\n            for minibatch_index in range(n_train_batches):\n                this_train_error = dnn_model.finetune((train_set_x[minibatch_index*batch_size:(minibatch_index+1)*batch_size, :], \\\n                                                       train_set_y[minibatch_index*batch_size:(minibatch_index+1)*batch_size, :]), batch_size, current_finetune_lr, current_momentum)\n                train_error.extend(this_train_error)\n\n        train_data_reader.reset()\n\n\n        logger.debug(\'calculating validation loss\')\n        predicted_parameter = dnn_model.parameter_prediction(valid_set_x) #, valid_set_y\n        validation_losses = numpy.sum((predicted_parameter - valid_set_y) ** 2, axis=1)\n        this_validation_loss = numpy.mean(validation_losses)\n\n\n        this_train_valid_loss = numpy.mean(numpy.asarray(train_error))\n\n        sub_end_time = time.time()\n\n        loss_difference = this_validation_loss - previous_loss\n\n        logger.info(\'epoch %i, validation error %f, train error %f  time spent %.2f\' %(epoch, this_validation_loss, this_train_valid_loss, (sub_end_time - sub_start_time)))\n        if plot:\n            plotlogger.add_plot_point(\'training convergence\',\'validation set\',(epoch,this_validation_loss))\n            plotlogger.add_plot_point(\'training convergence\',\'training set\',(epoch,this_train_valid_loss))\n            plotlogger.save_plot(\'training convergence\',title=\'Progress of training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n\n        if this_validation_loss < best_validation_loss:\n            if epoch > 10:\n                pickle.dump(best_dnn_model, open(nnets_file_name, \'wb\'))\n\n            best_dnn_model = dnn_model\n            best_validation_loss = this_validation_loss\n#            logger.debug(\'validation loss decreased, so saving model\')\n\n        if this_validation_loss >= previous_loss:\n            logger.debug(\'validation loss increased\')\n\n#            dbn = best_dnn_model\n            early_stop += 1\n\n#        if early_stop > early_stop_epoch:\n#            logger.debug(\'stopping early\')\n#            break\n\n        if math.isnan(this_validation_loss):\n            break\n\n        previous_loss = this_validation_loss\n\n    end_time = time.time()\n#    cPickle.dump(best_dnn_model, open(nnets_file_name, \'wb\'))\n\n    logger.info(\'overall  training time: %.2fm validation error %f\' % ((end_time - start_time) / 60., best_validation_loss))\n\n    if plot:\n        plotlogger.save_plot(\'training convergence\',title=\'Final training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n\n    return  best_validation_loss\n\n\ndef dnn_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):  #file_number\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        test_set_x = features.reshape((-1, n_ins))\n\n        predicted_parameter = dnn_model.parameter_prediction(test_set_x)\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\ndef dnn_generation_lstm(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    visualize_dnn(dnn_model)\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):  #file_number\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        test_set_x = features.reshape((-1, n_ins))\n\n        predicted_parameter = dnn_model.parameter_prediction_lstm(test_set_x)\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\ndef main_function(cfg):\n\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n    # get another logger to handle plotting duties\n    plotlogger = logging.getLogger(""plotting"")\n\n    # later, we might do this via a handler that is created, attached and configured\n    # using the standard config mechanism of the logging module\n    # but for now we need to do it manually\n    plotlogger.set_plot_path(cfg.plot_dir)\n\n    #### parameter setting########\n    hidden_layer_size = cfg.hyper_params[\'hidden_layer_size\']\n\n\n    ####prepare environment\n\n    try:\n        file_id_list = read_file_list(cfg.file_id_scp)\n        logger.debug(\'Loaded file id list from %s\' % cfg.file_id_scp)\n    except IOError:\n        # this means that open(...) threw an error\n        logger.critical(\'Could not load file id list from %s\' % cfg.file_id_scp)\n        raise\n\n    ###total file number including training, development, and testing\n    total_file_number = len(file_id_list)\n\n    data_dir = cfg.data_dir\n\n    nn_cmp_dir       = os.path.join(data_dir, \'nn\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim))\n    nn_cmp_norm_dir   = os.path.join(data_dir, \'nn_norm\'  + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim))\n\n    model_dir = os.path.join(cfg.work_dir, \'nnets_model\')\n    gen_dir   = os.path.join(cfg.work_dir, \'gen\')\n\n    in_file_list_dict = {}\n\n    for feature_name in list(cfg.in_dir_dict.keys()):\n        in_file_list_dict[feature_name] = prepare_file_path_list(file_id_list, cfg.in_dir_dict[feature_name], cfg.file_extension_dict[feature_name], False)\n\n    nn_cmp_file_list         = prepare_file_path_list(file_id_list, nn_cmp_dir, cfg.cmp_ext)\n    nn_cmp_norm_file_list    = prepare_file_path_list(file_id_list, nn_cmp_norm_dir, cfg.cmp_ext)\n\n    ###normalisation information\n    norm_info_file = os.path.join(data_dir, \'norm_info\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim) + \'_\' + cfg.output_feature_normalisation + \'.dat\')\n\n    ### normalise input full context label\n    # currently supporting two different forms of lingustic features\n    # later, we should generalise this\n\n    if cfg.label_style == \'HTS\':\n        label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name)\n        lab_dim = label_normaliser.dimension + cfg.appended_input_dim\n        logger.info(\'Input label dimension is %d\' % lab_dim)\n        suffix=str(lab_dim)\n    # no longer supported - use new ""composed"" style labels instead\n    elif cfg.label_style == \'composed\':\n        # label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)\n        suffix=\'composed\'\n\n    if cfg.process_labels_in_work_dir:\n        label_data_dir = cfg.work_dir\n    else:\n        label_data_dir = data_dir\n\n    # the number can be removed\n    binary_label_dir      = os.path.join(label_data_dir, \'binary_label_\'+suffix)\n    nn_label_dir          = os.path.join(label_data_dir, \'nn_no_silence_lab_\'+suffix)\n    nn_label_norm_dir     = os.path.join(label_data_dir, \'nn_no_silence_lab_norm_\'+suffix)\n\n    in_label_align_file_list = prepare_file_path_list(file_id_list, cfg.in_label_align_dir, cfg.lab_ext, False)\n    binary_label_file_list   = prepare_file_path_list(file_id_list, binary_label_dir, cfg.lab_ext)\n    nn_label_file_list       = prepare_file_path_list(file_id_list, nn_label_dir, cfg.lab_ext)\n    nn_label_norm_file_list  = prepare_file_path_list(file_id_list, nn_label_norm_dir, cfg.lab_ext)\n\n    # to do - sanity check the label dimension here?\n\n\n\n    min_max_normaliser = None\n    label_norm_file = \'label_norm_%s.dat\' %(cfg.label_style)\n    label_norm_file = os.path.join(label_data_dir, label_norm_file)\n\n    if cfg.NORMLAB and (cfg.label_style == \'HTS\'):\n        # simple HTS labels\n        logger.info(\'preparing label data (input) using standard HTS style labels\')\n        label_normaliser.perform_normalisation(in_label_align_file_list, binary_label_file_list)\n\n        remover = SilenceRemover(n_cmp = lab_dim, silence_pattern = cfg.silence_pattern)\n        remover.remove_silence(binary_label_file_list, in_label_align_file_list, nn_label_file_list)\n\n        min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99)\n        ###use only training data to find min-max information, then apply on the whole dataset\n        min_max_normaliser.find_min_max_values(nn_label_file_list[0:cfg.train_file_number])\n        min_max_normaliser.normalise_data(nn_label_file_list, nn_label_norm_file_list)\n\n\n    if cfg.NORMLAB and (cfg.label_style == \'composed\'):\n        # new flexible label preprocessor\n\n        logger.info(\'preparing label data (input) using ""composed"" style labels\')\n        label_composer = LabelComposer()\n        label_composer.load_label_configuration(cfg.label_config_file)\n\n        logger.info(\'Loaded label configuration\')\n        # logger.info(\'%s\' % label_composer.configuration.labels )\n\n        lab_dim=label_composer.compute_label_dimension()\n        logger.info(\'label dimension will be %d\' % lab_dim)\n\n        if cfg.precompile_xpaths:\n            label_composer.precompile_xpaths()\n\n        # there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)\n        # create all the lists of these, ready to pass to the label composer\n\n        in_label_align_file_list = {}\n        for label_style, label_style_required in label_composer.label_styles.items():\n            if label_style_required:\n                logger.info(\'labels of style %s are required - constructing file paths for them\' % label_style)\n                if label_style == \'xpath\':\n                    in_label_align_file_list[\'xpath\'] = prepare_file_path_list(file_id_list, cfg.xpath_label_align_dir, cfg.utt_ext, False)\n                elif label_style == \'hts\':\n                    in_label_align_file_list[\'hts\'] = prepare_file_path_list(file_id_list, cfg.hts_label_align_dir, cfg.lab_ext, False)\n                else:\n                    logger.critical(\'unsupported label style %s specified in label configuration\' % label_style)\n                    raise Exception\n\n            # now iterate through the files, one at a time, constructing the labels for them\n            num_files=len(file_id_list)\n            logger.info(\'the label styles required are %s\' % label_composer.label_styles)\n\n            for i in range(num_files):\n                logger.info(\'making input label features for %4d of %4d\' % (i+1,num_files))\n\n                # iterate through the required label styles and open each corresponding label file\n\n                # a dictionary of file descriptors, pointing at the required files\n                required_labels={}\n\n                for label_style, label_style_required in label_composer.label_styles.items():\n\n                    # the files will be a parallel set of files for a single utterance\n                    # e.g., the XML tree and an HTS label file\n                    if label_style_required:\n                        required_labels[label_style] = open(in_label_align_file_list[label_style][i] , \'r\')\n                        logger.debug(\' opening label file %s\' % in_label_align_file_list[label_style][i])\n\n                logger.debug(\'label styles with open files: %s\' % required_labels)\n                label_composer.make_labels(required_labels,out_file_name=binary_label_file_list[i],fill_missing_values=cfg.fill_missing_values,iterate_over_frames=cfg.iterate_over_frames)\n\n                # now close all opened files\n                for fd in required_labels.values():\n                    fd.close()\n\n\n        # silence removal\n        if cfg.remove_silence_using_binary_labels:\n            silence_feature = 0 ## use first feature in label -- hardcoded for now\n            logger.info(\'Silence removal from label using silence feature: %s\'%(label_composer.configuration.labels[silence_feature]))\n            logger.info(\'Silence will be removed from CMP files in same way\')\n            ## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:\n            trim_silence(binary_label_file_list, nn_label_file_list, lab_dim, \\\n                                binary_label_file_list, lab_dim, silence_feature)\n        else:\n            logger.info(\'No silence removal done\')\n            # start from the labels we have just produced, not trimmed versions\n            nn_label_file_list = binary_label_file_list\n\n        min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99)\n        ###use only training data to find min-max information, then apply on the whole dataset\n        min_max_normaliser.find_min_max_values(nn_label_file_list[0:cfg.train_file_number])\n        min_max_normaliser.normalise_data(nn_label_file_list, nn_label_norm_file_list)\n\n    if min_max_normaliser != None:\n        ### save label normalisation information for unseen testing labels\n        label_min_vector = min_max_normaliser.min_vector\n        label_max_vector = min_max_normaliser.max_vector\n        label_norm_info = numpy.concatenate((label_min_vector, label_max_vector), axis=0)\n\n        label_norm_info = numpy.array(label_norm_info, \'float32\')\n        fid = open(label_norm_file, \'wb\')\n        label_norm_info.tofile(fid)\n        fid.close()\n        logger.info(\'saved %s vectors to %s\' %(label_min_vector.size, label_norm_file))\n\n\n\n\n    ### make output acoustic data\n    if cfg.MAKECMP:\n        logger.info(\'creating acoustic (output) features\')\n        delta_win = cfg.delta_win #[-0.5, 0.0, 0.5]\n        acc_win = cfg.acc_win     #[1.0, -2.0, 1.0]\n\n        acoustic_worker = AcousticComposition(delta_win = delta_win, acc_win = acc_win)\n        acoustic_worker.prepare_nn_data(in_file_list_dict, nn_cmp_file_list, cfg.in_dimension_dict, cfg.out_dimension_dict)\n\n        if cfg.remove_silence_using_binary_labels:\n            ## do this to get lab_dim:\n            label_composer = LabelComposer()\n            label_composer.load_label_configuration(cfg.label_config_file)\n            lab_dim=label_composer.compute_label_dimension()\n\n            silence_feature = 0 ## use first feature in label -- hardcoded for now\n            logger.info(\'Silence removal from CMP using binary label file\')\n\n            ## overwrite the untrimmed audio with the trimmed version:\n            trim_silence(nn_cmp_file_list, nn_cmp_file_list, cfg.cmp_dim,\n                                binary_label_file_list, lab_dim, silence_feature)\n\n        else: ## back off to previous method using HTS labels:\n            remover = SilenceRemover(n_cmp = cfg.cmp_dim, silence_pattern = cfg.silence_pattern)\n            remover.remove_silence(nn_cmp_file_list[0:cfg.train_file_number+cfg.valid_file_number],\n                                   in_label_align_file_list[0:cfg.train_file_number+cfg.valid_file_number],\n                                   nn_cmp_file_list[0:cfg.train_file_number+cfg.valid_file_number]) # save to itself\n\n    ### save acoustic normalisation information for normalising the features back\n    var_dir   = os.path.join(data_dir, \'var\')\n    if not os.path.exists(var_dir):\n        os.makedirs(var_dir)\n\n    var_file_dict = {}\n    for feature_name in list(cfg.out_dimension_dict.keys()):\n        var_file_dict[feature_name] = os.path.join(var_dir, feature_name + \'_\' + str(cfg.out_dimension_dict[feature_name]))\n\n    ### normalise output acoustic data\n    if cfg.NORMCMP:\n        logger.info(\'normalising acoustic (output) features using method %s\' % cfg.output_feature_normalisation)\n        cmp_norm_info = None\n        if cfg.output_feature_normalisation == \'MVN\':\n            normaliser = MeanVarianceNorm(feature_dimension=cfg.cmp_dim)\n            ###calculate mean and std vectors on the training data, and apply on the whole dataset\n            global_mean_vector = normaliser.compute_mean(nn_cmp_file_list[0:cfg.train_file_number], 0, cfg.cmp_dim)\n            global_std_vector = normaliser.compute_std(nn_cmp_file_list[0:cfg.train_file_number], global_mean_vector, 0, cfg.cmp_dim)\n\n            normaliser.feature_normalisation(nn_cmp_file_list[0:cfg.train_file_number+cfg.valid_file_number],\n                                             nn_cmp_norm_file_list[0:cfg.train_file_number+cfg.valid_file_number])\n            cmp_norm_info = numpy.concatenate((global_mean_vector, global_std_vector), axis=0)\n\n        elif cfg.output_feature_normalisation == \'MINMAX\':\n            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim)\n            global_mean_vector = min_max_normaliser.compute_mean(nn_cmp_file_list[0:cfg.train_file_number])\n            global_std_vector = min_max_normaliser.compute_std(nn_cmp_file_list[0:cfg.train_file_number], global_mean_vector)\n\n            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim, min_value = 0.01, max_value = 0.99)\n            min_max_normaliser.find_min_max_values(nn_cmp_file_list[0:cfg.train_file_number])\n            min_max_normaliser.normalise_data(nn_cmp_file_list, nn_cmp_norm_file_list)\n\n            cmp_min_vector = min_max_normaliser.min_vector\n            cmp_max_vector = min_max_normaliser.max_vector\n            cmp_norm_info = numpy.concatenate((cmp_min_vector, cmp_max_vector), axis=0)\n\n        else:\n            logger.critical(\'Normalisation type %s is not supported!\\n\' %(cfg.output_feature_normalisation))\n            raise\n\n        cmp_norm_info = numpy.array(cmp_norm_info, \'float32\')\n        fid = open(norm_info_file, \'wb\')\n        cmp_norm_info.tofile(fid)\n        fid.close()\n        logger.info(\'saved %s vectors to %s\' %(cfg.output_feature_normalisation, norm_info_file))\n\n        feature_index = 0\n        for feature_name in list(cfg.out_dimension_dict.keys()):\n            feature_std_vector = numpy.array(global_std_vector[:,feature_index:feature_index+cfg.out_dimension_dict[feature_name]], \'float32\')\n\n            fid = open(var_file_dict[feature_name], \'w\')\n            feature_std_vector.tofile(fid)\n            fid.close()\n\n            logger.info(\'saved %s variance vector to %s\' %(feature_name, var_file_dict[feature_name]))\n\n            feature_index += cfg.out_dimension_dict[feature_name]\n\n    train_x_file_list = nn_label_norm_file_list[0:cfg.train_file_number]\n    train_y_file_list = nn_cmp_norm_file_list[0:cfg.train_file_number]\n    valid_x_file_list = nn_label_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n    valid_y_file_list = nn_cmp_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n    test_x_file_list  = nn_label_norm_file_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n    test_y_file_list  = nn_cmp_norm_file_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n\n    # we need to know the label dimension before training the DNN\n    # computing that requires us to look at the labels\n    #\n    # currently, there are two ways to do this\n    if cfg.label_style == \'HTS\':\n        label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name)\n        lab_dim = label_normaliser.dimension + cfg.appended_input_dim\n\n    elif cfg.label_style == \'composed\':\n        label_composer = LabelComposer()\n        label_composer.load_label_configuration(cfg.label_config_file)\n        lab_dim=label_composer.compute_label_dimension()\n\n    logger.info(\'label dimension is %d\' % lab_dim)\n\n    combined_model_arch = str(len(hidden_layer_size))\n    for hid_size in hidden_layer_size:\n        combined_model_arch += \'_\' + str(hid_size)\n\n    nnets_file_name = \'%s/%s_%s_%d_%s_%d.%d.train.%d.%f.nn.model\' \\\n                      %(model_dir, cfg.combined_model_name, cfg.combined_feature_name, int(cfg.multistream_switch),\n                        combined_model_arch, lab_dim, cfg.cmp_dim, cfg.train_file_number, cfg.hyper_params[\'learning_rate\'])\n\n\n    ### DNN model training\n    if cfg.TRAINDNN:\n\n        var_dict = load_covariance(var_file_dict, cfg.out_dimension_dict)\n\n        logger.info(\'training DNN\')\n\n        fid = open(norm_info_file, \'rb\')\n        cmp_min_max = numpy.fromfile(fid, dtype=numpy.float32)\n        fid.close()\n        cmp_min_max = cmp_min_max.reshape((2, -1))\n        cmp_mean_vector = cmp_min_max[0, ]\n        cmp_std_vector  = cmp_min_max[1, ]\n\n\n        try:\n            os.makedirs(model_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create model directory %s\' % model_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        try:\n            train_DNN(train_xy_file_list = (train_x_file_list, train_y_file_list), \\\n                      valid_xy_file_list = (valid_x_file_list, valid_y_file_list), \\\n                      nnets_file_name = nnets_file_name, \\\n                      n_ins = lab_dim, n_outs = cfg.cmp_dim, ms_outs = cfg.multistream_outs, \\\n                      hyper_params = cfg.hyper_params, buffer_size = cfg.buffer_size, plot = cfg.plot, var_dict = var_dict,\n                      cmp_mean_vector = cmp_mean_vector, cmp_std_vector = cmp_std_vector)\n        except KeyboardInterrupt:\n            logger.critical(\'train_DNN interrupted via keyboard\')\n            # Could \'raise\' the exception further, but that causes a deep traceback to be printed\n            # which we don\'t care about for a keyboard interrupt. So, just bail out immediately\n            sys.exit(1)\n        except:\n            logger.critical(\'train_DNN threw an exception\')\n            raise\n\n    ### generate parameters from DNN\n    temp_dir_name = \'%s_%s_%d_%d_%d_%d_%d_%d\' \\\n                    %(cfg.combined_model_name, cfg.combined_feature_name, int(cfg.do_post_filtering), \\\n                      cfg.train_file_number, lab_dim, cfg.cmp_dim, \\\n                      len(hidden_layer_size), hidden_layer_size[0])\n    gen_dir = os.path.join(gen_dir, temp_dir_name)\n\n    gen_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n    test_x_file_list  = nn_label_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n    if cfg.DNNGEN:\n        logger.info(\'generating from DNN\')\n\n        try:\n            os.makedirs(gen_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create generation directory %s\' % gen_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        gen_file_list = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.cmp_ext)\n        dnn_generation(test_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)\n\n        logger.debug(\'denormalising generated output using method %s\' % cfg.output_feature_normalisation)\n\n        fid = open(norm_info_file, \'rb\')\n        cmp_min_max = numpy.fromfile(fid, dtype=numpy.float32)\n        fid.close()\n        cmp_min_max = cmp_min_max.reshape((2, -1))\n        cmp_min_vector = cmp_min_max[0, ]\n        cmp_max_vector = cmp_min_max[1, ]\n\n        if cfg.output_feature_normalisation == \'MVN\':\n            denormaliser = MeanVarianceNorm(feature_dimension = cfg.cmp_dim)\n            denormaliser.feature_denormalisation(gen_file_list, gen_file_list, cmp_min_vector, cmp_max_vector)\n\n        elif cfg.output_feature_normalisation == \'MINMAX\':\n            denormaliser = MinMaxNormalisation(cfg.cmp_dim, min_value = 0.01, max_value = 0.99, min_vector = cmp_min_vector, max_vector = cmp_max_vector)\n            denormaliser.denormalise_data(gen_file_list, gen_file_list)\n        else:\n            logger.critical(\'denormalising method %s is not supported!\\n\' %(cfg.output_feature_normalisation))\n            raise\n\n        ##perform MLPG to smooth parameter trajectory\n        ## lf0 is included, the output features much have vuv.\n        generator = ParameterGeneration(gen_wav_features = cfg.gen_wav_features)\n        generator.acoustic_decomposition(gen_file_list, cfg.cmp_dim, cfg.out_dimension_dict, cfg.file_extension_dict, var_file_dict)\n\n\n    ### generate wav\n    if cfg.GENWAV:\n        logger.info(\'reconstructing waveform(s)\')\n        print(len(gen_file_id_list))\n        generate_wav(gen_dir, gen_file_id_list[cfg.valid_file_number:cfg.valid_file_number+cfg.test_file_number], cfg)     # generated speech\n#       generate_wav(nn_cmp_dir, gen_file_id_list)  # reference copy synthesis speech\n\n    ### evaluation: calculate distortion\n    if cfg.CALMCD:\n        logger.info(\'calculating MCD\')\n\n        ref_data_dir = os.path.join(data_dir, \'ref_data\')\n\n        ref_mgc_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.mgc_ext)\n        ref_bap_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.bap_ext)\n        ref_lf0_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.lf0_ext)\n\n        in_gen_label_align_file_list = in_label_align_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n        calculator = IndividualDistortionComp()\n\n        spectral_distortion = 0.0\n        bap_mse             = 0.0\n        f0_mse              = 0.0\n        vuv_error           = 0.0\n\n        valid_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n        test_file_id_list  = file_id_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n        if cfg.remove_silence_using_binary_labels:\n            ## get lab_dim:\n            label_composer = LabelComposer()\n            label_composer.load_label_configuration(cfg.label_config_file)\n            lab_dim=label_composer.compute_label_dimension()\n\n            ## use first feature in label -- hardcoded for now\n            silence_feature = 0\n\n            ## Use these to trim silence:\n            untrimmed_test_labels = binary_label_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n\n        if \'mgc\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'mgc\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_mgc_list, cfg.mgc_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.mgc_dim, silence_pattern = cfg.silence_pattern)\n                remover.remove_silence(in_file_list_dict[\'mgc\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_mgc_list)\n            valid_spectral_distortion = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.mgc_ext, cfg.mgc_dim)\n            test_spectral_distortion  = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.mgc_ext, cfg.mgc_dim)\n            valid_spectral_distortion *= (10 /numpy.log(10)) * numpy.sqrt(2.0)    ##MCD\n            test_spectral_distortion  *= (10 /numpy.log(10)) * numpy.sqrt(2.0)    ##MCD\n\n\n        if \'bap\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'bap\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_bap_list, cfg.bap_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.bap_dim, silence_pattern = cfg.silence_pattern)\n                remover.remove_silence(in_file_list_dict[\'bap\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_bap_list)\n            valid_bap_mse        = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.bap_ext, cfg.bap_dim)\n            test_bap_mse         = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.bap_ext, cfg.bap_dim)\n            valid_bap_mse = valid_bap_mse / 10.0    ##Cassia\'s bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC\n            test_bap_mse  = test_bap_mse / 10.0    ##Cassia\'s bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC\n\n        if \'lf0\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'lf0\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_lf0_list, cfg.lf0_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.lf0_dim, silence_pattern = cfg.silence_pattern)\n                remover.remove_silence(in_file_list_dict[\'lf0\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_lf0_list)\n            valid_f0_mse, valid_vuv_error   = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.lf0_ext, cfg.lf0_dim)\n            test_f0_mse , test_vuv_error    = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.lf0_ext, cfg.lf0_dim)\n\n        logger.info(\'Develop: DNN -- MCD: %.3f dB; BAP: %.3f dB; F0: %.3f Hz; VUV: %.3f%%\' \\\n                    %(valid_spectral_distortion, valid_bap_mse, valid_f0_mse, valid_vuv_error*100.))\n        logger.info(\'Test   : DNN -- MCD: %.3f dB; BAP: %.3f dB; F0: %.3f Hz; VUV: %.3f%%\' \\\n                    %(test_spectral_distortion , test_bap_mse , test_f0_mse , test_vuv_error*100.))\n\nif __name__ == \'__main__\':\n\n\n\n    # these things should be done even before trying to parse the command line\n\n    # create a configuration instance\n    # and get a short name for this instance\n    cfg=configuration.cfg\n\n    # set up logging to use our custom class\n    logging.setLoggerClass(LoggerPlotter)\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n\n    if len(sys.argv) != 2:\n        logger.critical(\'usage: run_dnn.sh [config file name]\')\n        sys.exit(1)\n\n    config_file = sys.argv[1]\n\n    config_file = os.path.abspath(config_file)\n    cfg.configure(config_file)\n\n    if cfg.profile:\n        logger.info(\'profiling is activated\')\n        import cProfile, pstats\n        cProfile.run(\'main_function(cfg)\', \'mainstats\')\n\n        # create a stream for the profiler to write to\n        profiling_output = io.StringIO()\n        p = pstats.Stats(\'mainstats\', stream=profiling_output)\n\n        # print stats to that stream\n        # here we just report the top 10 functions, sorted by total amount of time spent in each\n        p.strip_dirs().sort_stats(\'tottime\').print_stats(10)\n\n        # print the result to the log\n        logger.info(\'---Profiling result follows---\\n%s\' %  profiling_output.getvalue() )\n        profiling_output.close()\n        logger.info(\'---End of profiling result---\')\n\n    else:\n        main_function(cfg)\n\n#    if gnp._boardId is not None:\n#        import gpu_lock\n#        gpu_lock.free_lock(gnp._boardId)\n\n    sys.exit(0)\n'"
misc/recipes/MGE/run_mge_dnn.py,0,"b'\nimport pickle\nimport gzip\nimport os, sys, errno\nimport time\nimport math\n\n#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)\nimport numpy\n#import gnumpy as gnp\n# we need to explicitly import this in some cases, not sure why this doesn\'t get imported with numpy itself\nimport numpy.distutils.__config__\n# and only after that can we import theano\n#import theano\n\nfrom utils.providers import ListDataProvider\n\nfrom frontend.label_normalisation import HTSLabelNormalisation, XMLLabelNormalisation\nfrom frontend.silence_remover import SilenceRemover\nfrom frontend.silence_remover import trim_silence\nfrom frontend.min_max_norm import MinMaxNormalisation\nfrom frontend.acoustic_composition import AcousticComposition\nfrom frontend.parameter_generation import ParameterGeneration\nfrom frontend.mean_variance_norm import MeanVarianceNorm\n\n# the new class for label composition and normalisation\nfrom frontend.label_composer import LabelComposer\n#from frontend.mlpg_fast import MLParameterGenerationFast\n\n#from frontend.mlpg_fast_layer import MLParameterGenerationFastLayer\n\n\nimport configuration\nfrom models.st_dnn_cm import SequentialDNN\n\nfrom utils.compute_distortion import DistortionComputation, IndividualDistortionComp\nfrom utils.generate import generate_wav\nfrom utils.learn_rates import ExpDecreaseLearningRate\n\nfrom io_funcs.binary_io import  BinaryIOCollection\n\n#import matplotlib.pyplot as plt\n# our custom logging class that can also plot\n#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot\nfrom logplot.logging_plotting import LoggerPlotter, MultipleSeriesPlot, SingleWeightMatrixPlot\nimport logging # as logging\nimport logging.config\nimport io\n\n\ndef extract_file_id_list(file_list):\n    file_id_list = []\n    for file_name in file_list:\n        file_id = os.path.basename(os.path.splitext(file_name)[0])\n        file_id_list.append(file_id)\n\n    return  file_id_list\n\ndef read_file_list(file_name):\n\n    logger = logging.getLogger(""read_file_list"")\n\n    file_lists = []\n    fid = open(file_name)\n    for line in fid.readlines():\n        line = line.strip()\n        if len(line) < 1:\n            continue\n        file_lists.append(line)\n    fid.close()\n\n    logger.debug(\'Read file list from %s\' % file_name)\n    return  file_lists\n\n\ndef make_output_file_list(out_dir, in_file_lists):\n    out_file_lists = []\n\n    for in_file_name in in_file_lists:\n        file_id = os.path.basename(in_file_name)\n        out_file_name = out_dir + \'/\' + file_id\n        out_file_lists.append(out_file_name)\n\n    return  out_file_lists\n\ndef prepare_file_path_list(file_id_list, file_dir, file_extension, new_dir_switch=True):\n    if not os.path.exists(file_dir) and new_dir_switch:\n        os.makedirs(file_dir)\n    file_name_list = []\n    for file_id in file_id_list:\n        file_name = file_dir + \'/\' + file_id + file_extension\n        file_name_list.append(file_name)\n\n    return  file_name_list\n\n\n\ndef visualize_dnn(dnn):\n\n    layer_num = len(dnn.params)     ## including input and output\n    plotlogger = logging.getLogger(""plotting"")\n\n    for i in range(layer_num):\n        fig_name = \'Activation weights W\' + str(i) + \'_\' + dnn.params[i].name\n        fig_title = \'Activation weights of W\' + str(i)\n        xlabel = \'Neuron index of hidden layer \' + str(i)\n        ylabel = \'Neuron index of hidden layer \' + str(i+1)\n        if i == 0:\n            xlabel = \'Input feature index\'\n        if i == layer_num-1:\n            ylabel = \'Output feature index\'\n\n        aa = dnn.params[i].get_value(borrow=True).T\n        print(aa.shape, aa.size)\n        if aa.size > aa.shape[0]:\n            logger.create_plot(fig_name, SingleWeightMatrixPlot)\n            plotlogger.add_plot_point(fig_name, fig_name, dnn.params[i].get_value(borrow=True).T)\n            plotlogger.save_plot(fig_name, title=fig_name, xlabel=xlabel, ylabel=ylabel)\n\ndef load_covariance(var_file_dict, out_dimension_dict):\n    var = {}\n    io_funcs = BinaryIOCollection()\n    for feature_name in list(var_file_dict.keys()):\n        var_values, dimension = io_funcs.load_binary_file_frame(var_file_dict[feature_name], 1)\n\n        var_values = numpy.reshape(var_values, (out_dimension_dict[feature_name], 1))\n\n        var[feature_name] = var_values\n\n    return  var\n\n\ndef train_DNN(train_xy_file_list, valid_xy_file_list, \\\n              nnets_file_name, n_ins, n_outs, ms_outs, hyper_params, buffer_size, plot=False, var_dict=None,\n              cmp_mean_vector = None, cmp_std_vector = None, init_dnn_model_file = None):\n\n    # get loggers for this function\n    # this one writes to both console and file\n    logger = logging.getLogger(""main.train_DNN"")\n    logger.debug(\'Starting train_DNN\')\n\n    if plot:\n        # this one takes care of plotting duties\n        plotlogger = logging.getLogger(""plotting"")\n        # create an (empty) plot of training convergence, ready to receive data points\n        logger.create_plot(\'training convergence\',MultipleSeriesPlot)\n\n    try:\n        assert numpy.sum(ms_outs) == n_outs\n    except AssertionError:\n        logger.critical(\'the summation of multi-stream outputs does not equal to %d\' %(n_outs))\n        raise\n\n    ####parameters#####\n    finetune_lr     = float(hyper_params[\'learning_rate\'])\n    training_epochs = int(hyper_params[\'training_epochs\'])\n    batch_size      = int(hyper_params[\'batch_size\'])\n    l1_reg          = float(hyper_params[\'l1_reg\'])\n    l2_reg          = float(hyper_params[\'l2_reg\'])\n    warmup_epoch    = int(hyper_params[\'warmup_epoch\'])\n    momentum        = float(hyper_params[\'momentum\'])\n    warmup_momentum = float(hyper_params[\'warmup_momentum\'])\n\n    hidden_layer_size = hyper_params[\'hidden_layer_size\']\n\n    buffer_utt_size = buffer_size\n    early_stop_epoch = int(hyper_params[\'early_stop_epochs\'])\n\n    hidden_activation = hyper_params[\'hidden_activation\']\n    output_activation = hyper_params[\'output_activation\']\n\n    model_type = hyper_params[\'model_type\']\n    hidden_layer_type  = hyper_params[\'hidden_layer_type\']\n\n    ## use a switch to turn on pretraining\n    ## pretraining may not help too much, if this case, we turn it off to save time\n    do_pretraining = hyper_params[\'do_pretraining\']\n    pretraining_epochs = int(hyper_params[\'pretraining_epochs\'])\n    pretraining_lr = float(hyper_params[\'pretraining_lr\'])\n\n    sequential_training = hyper_params[\'sequential_training\']\n\n#    sequential_training = True\n\n    buffer_size = int(buffer_size / batch_size) * batch_size\n\n    ###################\n    (train_x_file_list, train_y_file_list) = train_xy_file_list\n    (valid_x_file_list, valid_y_file_list) = valid_xy_file_list\n\n    logger.debug(\'Creating training   data provider\')\n    train_data_reader = ListDataProvider(x_file_list = train_x_file_list, y_file_list = train_y_file_list,\n                            n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, sequential = sequential_training, shuffle = True)\n\n    logger.debug(\'Creating validation data provider\')\n    valid_data_reader = ListDataProvider(x_file_list = valid_x_file_list, y_file_list = valid_y_file_list,\n                            n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, sequential = sequential_training, shuffle = False)\n\n    shared_train_set_xy, temp_train_set_x, temp_train_set_y = train_data_reader.load_one_partition()\n    train_set_x, train_set_y = shared_train_set_xy\n    shared_valid_set_xy, valid_set_x, valid_set_y = valid_data_reader.load_one_partition()   #validation data is still read block by block\n    valid_set_x, valid_set_y = shared_valid_set_xy\n    train_data_reader.reset()\n    valid_data_reader.reset()\n\n\n    ##temporally we use the training set as pretrain_set_x.\n    ##we need to support any data for pretraining\n#    pretrain_set_x = train_set_x\n\n    # numpy random generator\n    numpy_rng = numpy.random.RandomState(123)\n    logger.info(\'building the model\')\n\n\n    dnn_model = None\n    pretrain_fn = None  ## not all the model support pretraining right now\n    train_fn = None\n    valid_fn = None\n    valid_model = None ## valid_fn and valid_model are the same. reserve to computer multi-stream distortion\n    if model_type == \'DNN\':\n#        dnn_model = DeepRecurrentNetwork(n_in= n_ins, hidden_layer_size = hidden_layer_size, n_out = n_outs, L1_reg = l1_reg, L2_reg = l2_reg, hidden_layer_type = hidden_layer_type)\n\n        dnn_model = SequentialDNN(numpy_rng=numpy_rng, n_ins=n_ins, n_outs = n_outs,\n                        l1_reg = l1_reg, l2_reg = l2_reg,\n                         hidden_layer_sizes = hidden_layer_size)\n\n    else:\n        logger.critical(\'%s type NN model is not supported!\' %(model_type))\n        raise\n\n    logger.info(\'fine-tuning the %s model\' %(model_type))\n\n    init_dnn_model = pickle.load(open(init_dnn_model_file, \'rb\'))\n\n    dnn_model.set_parameters(init_dnn_model.W_params, init_dnn_model.b_params)\n\n\n    start_time = time.time()\n\n    best_dnn_model = dnn_model\n    best_validation_loss = sys.float_info.max\n    previous_loss = sys.float_info.max\n\n    early_stop = 0\n    epoch = 0\n\n#    finetune_lr = 0.000125\n    previous_finetune_lr = finetune_lr\n\n    print(finetune_lr)\n\n    while (epoch < training_epochs):\n        epoch = epoch + 1\n\n        current_momentum = momentum\n        current_finetune_lr = finetune_lr\n        if epoch <= warmup_epoch:\n            current_finetune_lr = finetune_lr\n            current_momentum = warmup_momentum\n        else:\n            current_finetune_lr = previous_finetune_lr * 0.5\n\n        previous_finetune_lr = current_finetune_lr\n\n        train_error = []\n        sub_start_time = time.time()\n\n        while (not train_data_reader.is_finish()):\n            shared_train_set_xy, train_set_x, train_set_y = train_data_reader.load_one_partition()\n\n            n_train_batches = train_set_x.shape[0]\n            current_frame_number = train_set_x.shape[0]\n\n            mean_matrix = numpy.tile(cmp_mean_vector, (current_frame_number, 1))\n            std_matrix = numpy.tile(cmp_std_vector, (current_frame_number, 1))\n\n            logger.debug(\'this partition: %d frames (divided into %d batches )\' %(train_set_x.shape[0], n_train_batches) )\n\n            this_train_error = dnn_model.finetune((train_set_x, train_set_y), current_frame_number, current_finetune_lr, current_momentum, mean_matrix, std_matrix)\n            train_error.extend(this_train_error.tolist())\n\n        train_data_reader.reset()\n\n\n        logger.debug(\'calculating validation loss\')\n        validation_losses = []\n        validation_losses2 = []\n        while (not valid_data_reader.is_finish()):\n            shared_valid_set_xy, valid_set_x, valid_set_y = valid_data_reader.load_one_partition()\n\n            current_frame_number = valid_set_x.shape[0]\n            mean_matrix = numpy.tile(cmp_mean_vector, (current_frame_number, 1))\n            std_matrix = numpy.tile(cmp_std_vector, (current_frame_number, 1))\n\n            this_valid_loss = dnn_model.parameter_prediction_trajectory(valid_set_x, valid_set_y, mean_matrix, std_matrix)\n            validation_losses.extend(this_valid_loss.tolist())\n\n            predicted_para = dnn_model.parameter_prediction(valid_set_x)\n            temp_loss = numpy.sum(((predicted_para[:, 0:60] - valid_set_y[:, 0:60]) * std_matrix[:, 0:60]) ** 2, axis=1)\n            temp_loss = temp_loss ** 0.5\n            validation_losses2.extend(temp_loss.tolist())\n        valid_data_reader.reset()\n\n        this_validation_loss = numpy.mean(validation_losses)\n\n        this_train_valid_loss = numpy.mean(numpy.asarray(train_error))\n\n        sub_end_time = time.time()\n\n        loss_difference = this_validation_loss - previous_loss\n\n        logger.info(\'epoch %i, validation error %f, train error %f  time spent %.2f\' %(epoch, this_validation_loss, this_train_valid_loss, (sub_end_time - sub_start_time)))\n        if plot:\n            plotlogger.add_plot_point(\'training convergence\',\'validation set\',(epoch,this_validation_loss))\n            plotlogger.add_plot_point(\'training convergence\',\'training set\',(epoch,this_train_valid_loss))\n            plotlogger.save_plot(\'training convergence\',title=\'Progress of training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n\n        if this_validation_loss < best_validation_loss:\n            if epoch > 10:\n                pickle.dump(best_dnn_model, open(nnets_file_name, \'wb\'))\n\n            best_dnn_model = dnn_model\n            best_validation_loss = this_validation_loss\n#            logger.debug(\'validation loss decreased, so saving model\')\n\n        if this_validation_loss >= previous_loss:\n            logger.debug(\'validation loss increased\')\n\n#            dbn = best_dnn_model\n            early_stop += 1\n\n#        if early_stop > early_stop_epoch:\n#            logger.debug(\'stopping early\')\n#            break\n\n        if math.isnan(this_validation_loss):\n            break\n\n        previous_loss = this_validation_loss\n\n    end_time = time.time()\n#    cPickle.dump(best_dnn_model, open(nnets_file_name, \'wb\'))\n\n    logger.info(\'overall  training time: %.2fm validation error %f\' % ((end_time - start_time) / 60., best_validation_loss))\n\n    if plot:\n        plotlogger.save_plot(\'training convergence\',title=\'Final training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n\n    return  best_validation_loss\n\n\ndef dnn_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):  #file_number\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        test_set_x = features.reshape((-1, n_ins))\n\n        predicted_parameter = dnn_model.parameter_prediction(test_set_x)\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\ndef dnn_generation_lstm(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    visualize_dnn(dnn_model)\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):  #file_number\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        test_set_x = features.reshape((-1, n_ins))\n\n        predicted_parameter = dnn_model.parameter_prediction_lstm(test_set_x)\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\ndef main_function(cfg):\n\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n    # get another logger to handle plotting duties\n    plotlogger = logging.getLogger(""plotting"")\n\n    # later, we might do this via a handler that is created, attached and configured\n    # using the standard config mechanism of the logging module\n    # but for now we need to do it manually\n    plotlogger.set_plot_path(cfg.plot_dir)\n\n    #### parameter setting########\n    hidden_layer_size = cfg.hyper_params[\'hidden_layer_size\']\n\n\n    ####prepare environment\n\n    try:\n        file_id_list = read_file_list(cfg.file_id_scp)\n        logger.debug(\'Loaded file id list from %s\' % cfg.file_id_scp)\n    except IOError:\n        # this means that open(...) threw an error\n        logger.critical(\'Could not load file id list from %s\' % cfg.file_id_scp)\n        raise\n\n    ###total file number including training, development, and testing\n    total_file_number = len(file_id_list)\n\n    data_dir = cfg.data_dir\n\n    nn_cmp_dir       = os.path.join(data_dir, \'nn\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim))\n    nn_cmp_norm_dir   = os.path.join(data_dir, \'nn_norm\'  + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim))\n\n    model_dir = os.path.join(cfg.work_dir, \'nnets_model\')\n    gen_dir   = os.path.join(cfg.work_dir, \'gen\')\n\n    in_file_list_dict = {}\n\n    for feature_name in list(cfg.in_dir_dict.keys()):\n        in_file_list_dict[feature_name] = prepare_file_path_list(file_id_list, cfg.in_dir_dict[feature_name], cfg.file_extension_dict[feature_name], False)\n\n    nn_cmp_file_list         = prepare_file_path_list(file_id_list, nn_cmp_dir, cfg.cmp_ext)\n    nn_cmp_norm_file_list    = prepare_file_path_list(file_id_list, nn_cmp_norm_dir, cfg.cmp_ext)\n\n    ###normalisation information\n    norm_info_file = os.path.join(data_dir, \'norm_info\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim) + \'_\' + cfg.output_feature_normalisation + \'.dat\')\n\n    ### normalise input full context label\n    # currently supporting two different forms of lingustic features\n    # later, we should generalise this\n\n    if cfg.label_style == \'HTS\':\n        label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name)\n        lab_dim = label_normaliser.dimension + cfg.appended_input_dim\n        logger.info(\'Input label dimension is %d\' % lab_dim)\n        suffix=str(lab_dim)\n    # no longer supported - use new ""composed"" style labels instead\n    elif cfg.label_style == \'composed\':\n        # label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)\n        suffix=\'composed\'\n\n    if cfg.process_labels_in_work_dir:\n        label_data_dir = cfg.work_dir\n    else:\n        label_data_dir = data_dir\n\n    # the number can be removed\n    binary_label_dir      = os.path.join(label_data_dir, \'binary_label_\'+suffix)\n    nn_label_dir          = os.path.join(label_data_dir, \'nn_no_silence_lab_\'+suffix)\n    nn_label_norm_dir     = os.path.join(label_data_dir, \'nn_no_silence_lab_norm_\'+suffix)\n\n    in_label_align_file_list = prepare_file_path_list(file_id_list, cfg.in_label_align_dir, cfg.lab_ext, False)\n    binary_label_file_list   = prepare_file_path_list(file_id_list, binary_label_dir, cfg.lab_ext)\n    nn_label_file_list       = prepare_file_path_list(file_id_list, nn_label_dir, cfg.lab_ext)\n    nn_label_norm_file_list  = prepare_file_path_list(file_id_list, nn_label_norm_dir, cfg.lab_ext)\n\n    # to do - sanity check the label dimension here?\n\n\n\n    min_max_normaliser = None\n    label_norm_file = \'label_norm_%s.dat\' %(cfg.label_style)\n    label_norm_file = os.path.join(label_data_dir, label_norm_file)\n\n    if cfg.NORMLAB and (cfg.label_style == \'HTS\'):\n        # simple HTS labels\n        logger.info(\'preparing label data (input) using standard HTS style labels\')\n        label_normaliser.perform_normalisation(in_label_align_file_list, binary_label_file_list)\n\n        remover = SilenceRemover(n_cmp = lab_dim, silence_pattern = cfg.silence_pattern)\n        remover.remove_silence(binary_label_file_list, in_label_align_file_list, nn_label_file_list)\n\n        min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99)\n        ###use only training data to find min-max information, then apply on the whole dataset\n        min_max_normaliser.find_min_max_values(nn_label_file_list[0:cfg.train_file_number])\n        min_max_normaliser.normalise_data(nn_label_file_list, nn_label_norm_file_list)\n\n\n    if cfg.NORMLAB and (cfg.label_style == \'composed\'):\n        # new flexible label preprocessor\n\n        logger.info(\'preparing label data (input) using ""composed"" style labels\')\n        label_composer = LabelComposer()\n        label_composer.load_label_configuration(cfg.label_config_file)\n\n        logger.info(\'Loaded label configuration\')\n        # logger.info(\'%s\' % label_composer.configuration.labels )\n\n        lab_dim=label_composer.compute_label_dimension()\n        logger.info(\'label dimension will be %d\' % lab_dim)\n\n        if cfg.precompile_xpaths:\n            label_composer.precompile_xpaths()\n\n        # there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)\n        # create all the lists of these, ready to pass to the label composer\n\n        in_label_align_file_list = {}\n        for label_style, label_style_required in label_composer.label_styles.items():\n            if label_style_required:\n                logger.info(\'labels of style %s are required - constructing file paths for them\' % label_style)\n                if label_style == \'xpath\':\n                    in_label_align_file_list[\'xpath\'] = prepare_file_path_list(file_id_list, cfg.xpath_label_align_dir, cfg.utt_ext, False)\n                elif label_style == \'hts\':\n                    in_label_align_file_list[\'hts\'] = prepare_file_path_list(file_id_list, cfg.hts_label_align_dir, cfg.lab_ext, False)\n                else:\n                    logger.critical(\'unsupported label style %s specified in label configuration\' % label_style)\n                    raise Exception\n\n            # now iterate through the files, one at a time, constructing the labels for them\n            num_files=len(file_id_list)\n            logger.info(\'the label styles required are %s\' % label_composer.label_styles)\n\n            for i in range(num_files):\n                logger.info(\'making input label features for %4d of %4d\' % (i+1,num_files))\n\n                # iterate through the required label styles and open each corresponding label file\n\n                # a dictionary of file descriptors, pointing at the required files\n                required_labels={}\n\n                for label_style, label_style_required in label_composer.label_styles.items():\n\n                    # the files will be a parallel set of files for a single utterance\n                    # e.g., the XML tree and an HTS label file\n                    if label_style_required:\n                        required_labels[label_style] = open(in_label_align_file_list[label_style][i] , \'r\')\n                        logger.debug(\' opening label file %s\' % in_label_align_file_list[label_style][i])\n\n                logger.debug(\'label styles with open files: %s\' % required_labels)\n                label_composer.make_labels(required_labels,out_file_name=binary_label_file_list[i],fill_missing_values=cfg.fill_missing_values,iterate_over_frames=cfg.iterate_over_frames)\n\n                # now close all opened files\n                for fd in required_labels.values():\n                    fd.close()\n\n\n        # silence removal\n        if cfg.remove_silence_using_binary_labels:\n            silence_feature = 0 ## use first feature in label -- hardcoded for now\n            logger.info(\'Silence removal from label using silence feature: %s\'%(label_composer.configuration.labels[silence_feature]))\n            logger.info(\'Silence will be removed from CMP files in same way\')\n            ## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:\n            trim_silence(binary_label_file_list, nn_label_file_list, lab_dim, \\\n                                binary_label_file_list, lab_dim, silence_feature)\n        else:\n            logger.info(\'No silence removal done\')\n            # start from the labels we have just produced, not trimmed versions\n            nn_label_file_list = binary_label_file_list\n\n        min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99)\n        ###use only training data to find min-max information, then apply on the whole dataset\n        min_max_normaliser.find_min_max_values(nn_label_file_list[0:cfg.train_file_number])\n        min_max_normaliser.normalise_data(nn_label_file_list, nn_label_norm_file_list)\n\n    if min_max_normaliser != None:\n        ### save label normalisation information for unseen testing labels\n        label_min_vector = min_max_normaliser.min_vector\n        label_max_vector = min_max_normaliser.max_vector\n        label_norm_info = numpy.concatenate((label_min_vector, label_max_vector), axis=0)\n\n        label_norm_info = numpy.array(label_norm_info, \'float32\')\n        fid = open(label_norm_file, \'wb\')\n        label_norm_info.tofile(fid)\n        fid.close()\n        logger.info(\'saved %s vectors to %s\' %(label_min_vector.size, label_norm_file))\n\n\n\n\n    ### make output acoustic data\n    if cfg.MAKECMP:\n        logger.info(\'creating acoustic (output) features\')\n        delta_win = cfg.delta_win #[-0.5, 0.0, 0.5]\n        acc_win = cfg.acc_win     #[1.0, -2.0, 1.0]\n\n        acoustic_worker = AcousticComposition(delta_win = delta_win, acc_win = acc_win)\n        acoustic_worker.prepare_nn_data(in_file_list_dict, nn_cmp_file_list, cfg.in_dimension_dict, cfg.out_dimension_dict)\n\n        if cfg.remove_silence_using_binary_labels:\n            ## do this to get lab_dim:\n            label_composer = LabelComposer()\n            label_composer.load_label_configuration(cfg.label_config_file)\n            lab_dim=label_composer.compute_label_dimension()\n\n            silence_feature = 0 ## use first feature in label -- hardcoded for now\n            logger.info(\'Silence removal from CMP using binary label file\')\n\n            ## overwrite the untrimmed audio with the trimmed version:\n            trim_silence(nn_cmp_file_list, nn_cmp_file_list, cfg.cmp_dim,\n                                binary_label_file_list, lab_dim, silence_feature)\n\n        else: ## back off to previous method using HTS labels:\n            remover = SilenceRemover(n_cmp = cfg.cmp_dim, silence_pattern = cfg.silence_pattern)\n            remover.remove_silence(nn_cmp_file_list[0:cfg.train_file_number+cfg.valid_file_number],\n                                   in_label_align_file_list[0:cfg.train_file_number+cfg.valid_file_number],\n                                   nn_cmp_file_list[0:cfg.train_file_number+cfg.valid_file_number]) # save to itself\n\n    ### save acoustic normalisation information for normalising the features back\n    var_dir   = os.path.join(data_dir, \'var\')\n    if not os.path.exists(var_dir):\n        os.makedirs(var_dir)\n\n    var_file_dict = {}\n    for feature_name in list(cfg.out_dimension_dict.keys()):\n        var_file_dict[feature_name] = os.path.join(var_dir, feature_name + \'_\' + str(cfg.out_dimension_dict[feature_name]))\n\n    ### normalise output acoustic data\n    if cfg.NORMCMP:\n        logger.info(\'normalising acoustic (output) features using method %s\' % cfg.output_feature_normalisation)\n        cmp_norm_info = None\n        if cfg.output_feature_normalisation == \'MVN\':\n            normaliser = MeanVarianceNorm(feature_dimension=cfg.cmp_dim)\n            ###calculate mean and std vectors on the training data, and apply on the whole dataset\n            global_mean_vector = normaliser.compute_mean(nn_cmp_file_list[0:cfg.train_file_number], 0, cfg.cmp_dim)\n            global_std_vector = normaliser.compute_std(nn_cmp_file_list[0:cfg.train_file_number], global_mean_vector, 0, cfg.cmp_dim)\n\n            normaliser.feature_normalisation(nn_cmp_file_list[0:cfg.train_file_number+cfg.valid_file_number],\n                                             nn_cmp_norm_file_list[0:cfg.train_file_number+cfg.valid_file_number])\n            cmp_norm_info = numpy.concatenate((global_mean_vector, global_std_vector), axis=0)\n\n        elif cfg.output_feature_normalisation == \'MINMAX\':\n            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim)\n            global_mean_vector = min_max_normaliser.compute_mean(nn_cmp_file_list[0:cfg.train_file_number])\n            global_std_vector = min_max_normaliser.compute_std(nn_cmp_file_list[0:cfg.train_file_number], global_mean_vector)\n\n            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim, min_value = 0.01, max_value = 0.99)\n            min_max_normaliser.find_min_max_values(nn_cmp_file_list[0:cfg.train_file_number])\n            min_max_normaliser.normalise_data(nn_cmp_file_list, nn_cmp_norm_file_list)\n\n            cmp_min_vector = min_max_normaliser.min_vector\n            cmp_max_vector = min_max_normaliser.max_vector\n            cmp_norm_info = numpy.concatenate((cmp_min_vector, cmp_max_vector), axis=0)\n\n        else:\n            logger.critical(\'Normalisation type %s is not supported!\\n\' %(cfg.output_feature_normalisation))\n            raise\n\n        cmp_norm_info = numpy.array(cmp_norm_info, \'float32\')\n        fid = open(norm_info_file, \'wb\')\n        cmp_norm_info.tofile(fid)\n        fid.close()\n        logger.info(\'saved %s vectors to %s\' %(cfg.output_feature_normalisation, norm_info_file))\n\n        feature_index = 0\n        for feature_name in list(cfg.out_dimension_dict.keys()):\n            feature_std_vector = numpy.array(global_std_vector[:,feature_index:feature_index+cfg.out_dimension_dict[feature_name]], \'float32\')\n\n            fid = open(var_file_dict[feature_name], \'w\')\n            feature_std_vector.tofile(fid)\n            fid.close()\n\n            logger.info(\'saved %s variance vector to %s\' %(feature_name, var_file_dict[feature_name]))\n\n            feature_index += cfg.out_dimension_dict[feature_name]\n\n    train_x_file_list = nn_label_norm_file_list[0:cfg.train_file_number]\n    train_y_file_list = nn_cmp_norm_file_list[0:cfg.train_file_number]\n    valid_x_file_list = nn_label_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n    valid_y_file_list = nn_cmp_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n    test_x_file_list  = nn_label_norm_file_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n    test_y_file_list  = nn_cmp_norm_file_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n\n    # we need to know the label dimension before training the DNN\n    # computing that requires us to look at the labels\n    #\n    # currently, there are two ways to do this\n    if cfg.label_style == \'HTS\':\n        label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name)\n        lab_dim = label_normaliser.dimension + cfg.appended_input_dim\n\n    elif cfg.label_style == \'composed\':\n        label_composer = LabelComposer()\n        label_composer.load_label_configuration(cfg.label_config_file)\n        lab_dim=label_composer.compute_label_dimension()\n\n    logger.info(\'label dimension is %d\' % lab_dim)\n\n    combined_model_arch = str(len(hidden_layer_size))\n    for hid_size in hidden_layer_size:\n        combined_model_arch += \'_\' + str(hid_size)\n\n    nnets_file_name = \'%s/%s_%s_%d_%s_%d.%d.train.%d.%f.nn.model\' \\\n                      %(model_dir, cfg.combined_model_name, cfg.combined_feature_name, int(cfg.multistream_switch),\n                        combined_model_arch, lab_dim, cfg.cmp_dim, cfg.train_file_number, cfg.hyper_params[\'learning_rate\'])\n\n\n    ### DNN model training\n    if cfg.TRAINDNN:\n\n        var_dict = load_covariance(var_file_dict, cfg.out_dimension_dict)\n\n        logger.info(\'training DNN\')\n\n        fid = open(norm_info_file, \'rb\')\n        cmp_min_max = numpy.fromfile(fid, dtype=numpy.float32)\n        fid.close()\n        cmp_min_max = cmp_min_max.reshape((2, -1))\n        cmp_mean_vector = cmp_min_max[0, ]\n        cmp_std_vector  = cmp_min_max[1, ]\n\n\n        try:\n            os.makedirs(model_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create model directory %s\' % model_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        try:\n            train_DNN(train_xy_file_list = (train_x_file_list, train_y_file_list), \\\n                      valid_xy_file_list = (valid_x_file_list, valid_y_file_list), \\\n                      nnets_file_name = nnets_file_name, \\\n                      n_ins = lab_dim, n_outs = cfg.cmp_dim, ms_outs = cfg.multistream_outs, \\\n                      hyper_params = cfg.hyper_params, buffer_size = cfg.buffer_size, plot = cfg.plot, var_dict = var_dict,\n                      cmp_mean_vector = cmp_mean_vector, cmp_std_vector = cmp_std_vector, init_dnn_model_file = cfg.start_from_trained_model)\n        except KeyboardInterrupt:\n            logger.critical(\'train_DNN interrupted via keyboard\')\n            # Could \'raise\' the exception further, but that causes a deep traceback to be printed\n            # which we don\'t care about for a keyboard interrupt. So, just bail out immediately\n            sys.exit(1)\n        except:\n            logger.critical(\'train_DNN threw an exception\')\n            raise\n\n    ### generate parameters from DNN\n    temp_dir_name = \'%s_%s_%d_%d_%d_%d_%d_%d\' \\\n                    %(cfg.combined_model_name, cfg.combined_feature_name, int(cfg.do_post_filtering), \\\n                      cfg.train_file_number, lab_dim, cfg.cmp_dim, \\\n                      len(hidden_layer_size), hidden_layer_size[0])\n    gen_dir = os.path.join(gen_dir, temp_dir_name)\n\n    gen_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n    test_x_file_list  = nn_label_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n    if cfg.DNNGEN:\n        logger.info(\'generating from DNN\')\n\n        try:\n            os.makedirs(gen_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create generation directory %s\' % gen_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        gen_file_list = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.cmp_ext)\n        dnn_generation(test_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)\n\n        logger.debug(\'denormalising generated output using method %s\' % cfg.output_feature_normalisation)\n\n        fid = open(norm_info_file, \'rb\')\n        cmp_min_max = numpy.fromfile(fid, dtype=numpy.float32)\n        fid.close()\n        cmp_min_max = cmp_min_max.reshape((2, -1))\n        cmp_min_vector = cmp_min_max[0, ]\n        cmp_max_vector = cmp_min_max[1, ]\n\n        if cfg.output_feature_normalisation == \'MVN\':\n            denormaliser = MeanVarianceNorm(feature_dimension = cfg.cmp_dim)\n            denormaliser.feature_denormalisation(gen_file_list, gen_file_list, cmp_min_vector, cmp_max_vector)\n\n        elif cfg.output_feature_normalisation == \'MINMAX\':\n            denormaliser = MinMaxNormalisation(cfg.cmp_dim, min_value = 0.01, max_value = 0.99, min_vector = cmp_min_vector, max_vector = cmp_max_vector)\n            denormaliser.denormalise_data(gen_file_list, gen_file_list)\n        else:\n            logger.critical(\'denormalising method %s is not supported!\\n\' %(cfg.output_feature_normalisation))\n            raise\n\n        ##perform MLPG to smooth parameter trajectory\n        ## lf0 is included, the output features much have vuv.\n        generator = ParameterGeneration(gen_wav_features = cfg.gen_wav_features)\n        generator.acoustic_decomposition(gen_file_list, cfg.cmp_dim, cfg.out_dimension_dict, cfg.file_extension_dict, var_file_dict)\n\n\n    ### generate wav\n    if cfg.GENWAV:\n        logger.info(\'reconstructing waveform(s)\')\n        print(len(gen_file_id_list))\n        generate_wav(gen_dir, gen_file_id_list[cfg.valid_file_number:cfg.valid_file_number+cfg.test_file_number], cfg)     # generated speech\n#       generate_wav(nn_cmp_dir, gen_file_id_list)  # reference copy synthesis speech\n\n    ### evaluation: calculate distortion\n    if cfg.CALMCD:\n        logger.info(\'calculating MCD\')\n\n        ref_data_dir = os.path.join(data_dir, \'ref_data\')\n\n        ref_mgc_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.mgc_ext)\n        ref_bap_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.bap_ext)\n        ref_lf0_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.lf0_ext)\n\n        in_gen_label_align_file_list = in_label_align_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n        calculator = IndividualDistortionComp()\n\n        spectral_distortion = 0.0\n        bap_mse             = 0.0\n        f0_mse              = 0.0\n        vuv_error           = 0.0\n\n        valid_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n        test_file_id_list  = file_id_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n        if cfg.remove_silence_using_binary_labels:\n            ## get lab_dim:\n            label_composer = LabelComposer()\n            label_composer.load_label_configuration(cfg.label_config_file)\n            lab_dim=label_composer.compute_label_dimension()\n\n            ## use first feature in label -- hardcoded for now\n            silence_feature = 0\n\n            ## Use these to trim silence:\n            untrimmed_test_labels = binary_label_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n\n        if \'mgc\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'mgc\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_mgc_list, cfg.mgc_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.mgc_dim, silence_pattern = cfg.silence_pattern)\n                remover.remove_silence(in_file_list_dict[\'mgc\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_mgc_list)\n            valid_spectral_distortion = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.mgc_ext, cfg.mgc_dim)\n            test_spectral_distortion  = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.mgc_ext, cfg.mgc_dim)\n            valid_spectral_distortion *= (10 /numpy.log(10)) * numpy.sqrt(2.0)    ##MCD\n            test_spectral_distortion  *= (10 /numpy.log(10)) * numpy.sqrt(2.0)    ##MCD\n\n\n        if \'bap\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'bap\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_bap_list, cfg.bap_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.bap_dim, silence_pattern = cfg.silence_pattern)\n                remover.remove_silence(in_file_list_dict[\'bap\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_bap_list)\n            valid_bap_mse        = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.bap_ext, cfg.bap_dim)\n            test_bap_mse         = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.bap_ext, cfg.bap_dim)\n            valid_bap_mse = valid_bap_mse / 10.0    ##Cassia\'s bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC\n            test_bap_mse  = test_bap_mse / 10.0    ##Cassia\'s bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC\n\n        if \'lf0\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'lf0\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_lf0_list, cfg.lf0_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.lf0_dim, silence_pattern = cfg.silence_pattern)\n                remover.remove_silence(in_file_list_dict[\'lf0\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_lf0_list)\n            valid_f0_mse, valid_vuv_error   = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.lf0_ext, cfg.lf0_dim)\n            test_f0_mse , test_vuv_error    = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.lf0_ext, cfg.lf0_dim)\n\n        logger.info(\'Develop: DNN -- MCD: %.3f dB; BAP: %.3f dB; F0: %.3f Hz; VUV: %.3f%%\' \\\n                    %(valid_spectral_distortion, valid_bap_mse, valid_f0_mse, valid_vuv_error*100.))\n        logger.info(\'Test   : DNN -- MCD: %.3f dB; BAP: %.3f dB; F0: %.3f Hz; VUV: %.3f%%\' \\\n                    %(test_spectral_distortion , test_bap_mse , test_f0_mse , test_vuv_error*100.))\n\nif __name__ == \'__main__\':\n\n\n\n    # these things should be done even before trying to parse the command line\n\n    # create a configuration instance\n    # and get a short name for this instance\n    cfg=configuration.cfg\n\n    # set up logging to use our custom class\n    logging.setLoggerClass(LoggerPlotter)\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n\n    if len(sys.argv) != 2:\n        logger.critical(\'usage: run_dnn.sh [config file name]\')\n        sys.exit(1)\n\n    config_file = sys.argv[1]\n\n    config_file = os.path.abspath(config_file)\n    cfg.configure(config_file)\n\n    if cfg.profile:\n        logger.info(\'profiling is activated\')\n        import cProfile, pstats\n        cProfile.run(\'main_function(cfg)\', \'mainstats\')\n\n        # create a stream for the profiler to write to\n        profiling_output = io.StringIO()\n        p = pstats.Stats(\'mainstats\', stream=profiling_output)\n\n        # print stats to that stream\n        # here we just report the top 10 functions, sorted by total amount of time spent in each\n        p.strip_dirs().sort_stats(\'tottime\').print_stats(10)\n\n        # print the result to the log\n        logger.info(\'---Profiling result follows---\\n%s\' %  profiling_output.getvalue() )\n        profiling_output.close()\n        logger.info(\'---End of profiling result---\')\n\n    else:\n        main_function(cfg)\n\n#    if gnp._boardId is not None:\n#        import gpu_lock\n#        gpu_lock.free_lock(gnp._boardId)\n\n    sys.exit(0)\n'"
misc/scripts/hybrid_voice/compute_tcoef_features.py,6,"b'#!/usr/bin/env python\n\nimport os, sys\nimport re\nimport numpy\n\nimport processHybridInfo\n\nmstohtk        = 10000\nsectoms        = 1000\nframeshift     = 5\nnumHybridSec   = 4\n\ndef findHybridParamRichContexts(file_id_list, feat_dict, vfloor, data_dir, tcoef_dir, lab_dir, sil_identifier, ignoreSilence=True):\n    ### create tcoef dir if not exists ###\n    if not os.path.isdir(tcoef_dir):\n        os.makedirs(tcoef_dir)\n\n    #print ""vfloor: {0}"".format(vfloor)\n    for file_index in range(len(file_id_list)):\n        file_name  = file_id_list[file_index]\n        label_file = os.path.join(lab_dir, file_name+\'.lab\')\n        tcoef_file = os.path.join(tcoef_dir, file_name+\'.tcoef\')\n\n        label_info = processHybridInfo.readHybridLabelFile(label_file, file_index, sil_identifier, ignoreSilence)\n        hybridInfo = processHybridInfo.convertToHybridLabel(label_info, numHybridSec)\n\n        feat_index = 0\n        tempFeats  = [[] for x in range(len(feat_dict))]\n        for feat_ext, feat_dim in feat_dict.items():\n            in_feat_dir = os.path.join(data_dir, feat_ext)\n            feat_file   = os.path.join(in_feat_dir, file_name+\'.\'+feat_ext)\n\n            tempFeats[feat_index] = processHybridInfo.readBottleneckFeatures(feat_file, feat_dim)\n            if feat_ext == \'lf0\':\n                tempFeats[feat_index] = numpy.exp(tempFeats[feat_index])\n            feat_index = feat_index + 1\n\n        features = numpy.hstack(tempFeats)\n\n        outf = open(tcoef_file, \'w\')\n        outf.write(\'EST_File Track\\nDataType ascii\\nNumFrames {0}\\nNumChannels {1}\\nNumAuxChannels 0\\nfile_type 14\\nEST_Header_End\\n\'.format(len(hybridInfo[1]),len(features[0])*2))\n\n        temp     = [[] for x in range(len(hybridInfo[1]))]\n        silMeans = numpy.zeros(len(features[0]))\n        silVars  = numpy.ones(len(features[0]))\n\n        for x in range(len(hybridInfo[1])):\n            outf.write(\'{0}\'.format(float(hybridInfo[4][x])/sectoms))\n            if sil_identifier in hybridInfo[3][x]:\n                tempMeans = silMeans\n                tempVars  = silVars\n            else:\n                if int(hybridInfo[1][x])==int(hybridInfo[2][x]):\n                    #set to the frame value if there is no range!\n                    temp[x] = features[hybridInfo[1][x]:hybridInfo[2][x]+1]\n                else:\n                    temp[x] = features[hybridInfo[1][x]:hybridInfo[2][x]]\n\n                tempContext = processHybridInfo.ContextInfo(float(hybridInfo[0][x]), hybridInfo[1][x], hybridInfo[2][x], hybridInfo[3][x], temp[x])\n                tempDist    = tempContext.getFeatsDistribution()\n\n                tempDist.enforceVFloor(vfloor)\n                tempMeans = tempDist.getArrayMeans()\n                tempVars  = tempDist.getArrayVariances()\n\n            for y in tempMeans:\n                outf.write(\'\\t{0}\'.format(y))\n            for y in tempVars:\n                outf.write(\'\\t{0}\'.format(y))\n            outf.write(\'\\n\')\n\n        outf.close()\n        print_status(file_index, len(file_id_list))\n\n    sys.stdout.write(""\\n"")\n\n    return tempFeats\n\ndef print_status(i, length):\n    pr = int(float(i+1)/float(length)*100)\n    st = int(float(pr)/7)\n    sys.stdout.write((""\\r%d/%d "")%(i+1,length)+(""[ %d""%pr+""% ] <<< "")+(\'=\'*st)+(\'\'*(100-st)))\n    sys.stdout.flush()\n\ndef prepare_file_path_list(file_id_list, file_dir, file_extension, new_dir_switch=True):\n    if not os.path.exists(file_dir) and new_dir_switch:\n        os.makedirs(file_dir)\n    file_name_list = []\n    for file_id in file_id_list:\n        file_name = file_dir + \'/\' + file_id + \'.\' + file_extension\n        file_name_list.append(file_name)\n\n    return  file_name_list\n\ndef read_file_list(file_name):\n    file_lists = []\n    fid = open(file_name)\n    for line in fid.readlines():\n        line = line.strip()\n        if len(line) < 1:\n            continue\n        file_lists.append(line)\n    fid.close()\n\n    return  file_lists\n\nif __name__==\'__main__\':\n\n    #### User configurable variables ####\n\n    merlin_dir = ""/work/smg/v-srikanth/merlin""\n    data_dir   = os.path.join(merlin_dir, ""egs/slt_arctic/s1/experiments/slt_arctic_demo/acoustic_model/data"")\n\n    feat_dict = {\'mgc\':60, \'lf0\':1, \'bap\':1}\n\n    sil_identifier = \'sil\'\n    in_lab_dir = os.path.join(data_dir, \'lab\')\n\n    out_dir    = os.path.join(data_dir, \'hybrid_voice_data\')\n    vfloor_dir = os.path.join(out_dir,  \'vfloor\')\n    tcoef_dir  = os.path.join(out_dir,  \'tcoef\')\n\n    if not os.path.isdir(vfloor_dir):\n        os.makedirs(vfloor_dir)\n\n    if not os.path.isdir(tcoef_dir):\n        os.makedirs(tcoef_dir)\n\n    tcoef_train_dir = os.path.join(tcoef_dir, \'train\')\n    tcoef_test_dir  = os.path.join(tcoef_dir, \'test\')\n\n    #### Train and test file lists ####\n\n    train_id_scp  = os.path.join(data_dir, \'train_id_list.scp\')\n    train_id_list = read_file_list(train_id_scp)\n\n    test_id_scp   = os.path.join(data_dir, \'test_id_list.scp\')\n    test_id_list  = read_file_list(test_id_scp)\n\n    #### calculate variance flooring for each feature (from only training files) ####\n\n    feat_index = 0\n    vf=[[] for x in range(len(feat_dict))]\n\n    for feat_ext, feat_dim in feat_dict.items():\n        filename = feat_ext+\'_\'+str(feat_dim)+\'_vfloor\'\n        var_file = os.path.join(vfloor_dir, filename)\n\n        if not os.path.isfile(var_file):\n            print(\'Calculating variance flooring for \'+feat_ext+\'...\')\n            in_feat_dir    = os.path.join(data_dir, feat_ext)\n            feat_file_list = prepare_file_path_list(train_id_list, in_feat_dir, feat_ext)\n\n            vf[feat_index] = processHybridInfo.calculateParamGV(feat_file_list, feat_dim)\n            vf[feat_index] = vf[feat_index]*0.01\n\n            numpy.savetxt(var_file, vf[feat_index])\n        else:\n            vf[feat_index] = numpy.loadtxt(var_file)\n\n        feat_index = feat_index + 1\n\n    vfloor = numpy.hstack(vf)\n\n    #### calculate tcoef features ####\n\n    print(\'computing tcoef features for training data...\')\n    tempFeats = findHybridParamRichContexts(train_id_list, feat_dict, vfloor, data_dir, tcoef_train_dir, in_lab_dir, sil_identifier)\n\n    print(\'computing tcoef features for test data...\')\n    tempFeats = findHybridParamRichContexts(test_id_list , feat_dict, vfloor, data_dir, tcoef_test_dir,  in_lab_dir, sil_identifier)\n'"
misc/scripts/hybrid_voice/convert_hts_label_format_to_festival.py,0,"b'import os\nimport sys\nimport re\n\ndef change_label_format(inp_label_file_list, out_label_file_list, label_style=""state_align""):\n\n    utt_len = len(inp_label_file_list)\n\n    ### read file by file ###\n    for i in range(utt_len):\n        inp_label_file_name = inp_label_file_list[i]\n        out_label_file_name = out_label_file_list[i]\n\n        label_info = convert_hts_lab_to_festival_lab(inp_label_file_name, out_label_file_name, label_style)\n\n        print_status(i, utt_len)\n\n    sys.stdout.write(""\\n"")\n\ndef convert_hts_lab_to_festival_lab(inp_label_file_name, out_label_file_name, label_style):\n    ### read label file ###\n    fid = open(inp_label_file_name)\n    utt_labels = fid.readlines()\n    fid.close()\n\n    dur = 0.0\n    lab_info = [[],[]]\n\n    ### process label file ###\n    for line in utt_labels:\n        line = line.strip()\n\n        if len(line) < 1:\n            continue\n        temp_list = re.split(\'\\s+\', line)\n        full_label = temp_list[2]\n\n        if label_style == ""state_align"":\n            full_label_length = len(full_label) - 3  # remove state information [k]\n            state_index = full_label[full_label_length + 1]\n\n            state_index = int(state_index) - 1\n            if state_index == 1:\n                ph_start_time = temp_list[0]\n            if state_index == 5:\n                ph_end_time   = temp_list[1]\n                full_label    = full_label[0:full_label_length]\n                current_phone = full_label[full_label.index(\'-\') + 1:full_label.index(\'+\')]\n                dur = dur + ((float(ph_end_time)-float(ph_start_time))*(10**-7))\n                lab_info[0].append(dur)\n                lab_info[1].append(current_phone)\n        elif label_style == ""phone_align"":\n            ph_start_time = temp_list[0]\n            ph_end_time   = temp_list[1]\n            current_phone = full_label[full_label.index(\'-\') + 1:full_label.index(\'+\')]\n            dur = dur + ((float(ph_end_time)-float(ph_start_time))*(10**-7))\n            lab_info[0].append(dur)\n            lab_info[1].append(current_phone)\n\n    out_f = open(out_label_file_name, \'w\')\n    out_f.write(\'#\\n\')\n    for j in range(len(lab_info[0])):\n        dur = lab_info[0][j]\n        ph  = lab_info[1][j]\n        out_f.write(str(dur)+\' 125 \'+ph+\'\\n\')\n    out_f.close()\n\n    return lab_info\n\ndef print_status(i, length):\n    pr = int(float(i+1)/float(length)*100)\n    st = int(float(pr)/7)\n    sys.stdout.write((""\\r%d/%d "")%(i+1,length)+(""[ %d""%pr+""% ] <<< "")+(\'=\'*st)+(\'\'*(100-st)))\n    sys.stdout.flush()\n\ndef prepare_file_path_list(file_id_list, file_dir, file_extension, new_dir_switch=True):\n    if not os.path.exists(file_dir) and new_dir_switch:\n        os.makedirs(file_dir)\n    file_name_list = []\n    for file_id in file_id_list:\n        file_name = file_dir + \'/\' + file_id + file_extension\n        file_name_list.append(file_name)\n\n    return  file_name_list\n\ndef read_file_list(file_name):\n    file_lists = []\n    fid = open(file_name)\n    for line in fid.readlines():\n        line = line.strip()\n        if len(line) < 1:\n            continue\n        file_lists.append(line)\n    fid.close()\n\n    return  file_lists\n\nif __name__==\'__main__\':\n\n    if len(sys.argv)!=5:\n        print(\'Usage: python convert_hts_label_format_to_festival.py <input_folder> <output_folder> <file_list> <label_style: state_align/phone_align>\\n\')\n        sys.exit(1)\n\n    inp_lab_dir  = sys.argv[1]\n    out_lab_dir  = sys.argv[2]\n\n    file_id_scp  = sys.argv[3]\n    file_id_list = read_file_list(file_id_scp)\n\n    label_style  = sys.argv[4]\n\n    inp_label_file_list = prepare_file_path_list(file_id_list, inp_lab_dir, \'.lab\')\n    out_label_file_list = prepare_file_path_list(file_id_list, out_lab_dir, \'.lab\')\n\n    print(\'changing HTS label format to festival...\')\n    change_label_format(inp_label_file_list, out_label_file_list, label_style)\n'"
misc/scripts/hybrid_voice/processHybridInfo.py,0,"b""#!/usr/bin/env python\n\nimport os,numpy,re\n\nmstohtk=10000\nsectoms=1000\nframeshift=5\n\nclass ContextInfo:\n    def __init__(self, fileID, frameStart, frameEnd, context, btlnkFeats=None):\n        self.fid       = numpy.array([int(fileID)])\n        self.sframe    = numpy.array([int(frameStart)])\n        self.eframe    = numpy.array([int(frameEnd)])\n        self.context   = context\n        self.feats     = numpy.array(btlnkFeats)\n        self.featsDist = DistributionInfo(1) # one stream\n\n        if btlnkFeats.any():\n            self.featsDist.setMean(numpy.mean(self.feats,0))\n            self.featsDist.setVariance(numpy.var(self.feats,0))\n\n    def getContext(self):\n        return self.context\n\n    def getFeats(self):\n        return self.feats\n\n    def getFeatsDistribution(self):\n        return self.featsDist\n\n    def getId(self):\n        return self.fid\n\n    def getStartFrame(self):\n        return self.sframe\n\n    def getEndFrame(self):\n        return self.eframe\n\n    def setStartFrame(self, frameStart):\n        self.sframe = frameStart\n\n    def setEndFrame(self, frameEnd):\n        self.eframe = frameEnd\n\n    def sameContext(self, altContext):\n        if self.context==altContext:\n            print('match found: {0}'.format(altContext))\n        return self.context==altContext\n\n    def addContextInstance(self, fileID, frameStart, frameEnd, btlnkFeats):\n        self.fid    = numpy.hstack([self.fid,int(fileID)])\n        self.sframe = numpy.hstack([self.sframe,int(frameStart)])\n        self.eframe = numpy.hstack([self.eframe,int(frameEnd)])\n        self.feats  = numpy.vstack([self.feats,btlnkFeats])\n\n        self.featsDist.setMean(numpy.mean(self.feats,0))\n        self.featsDist.setVariance(numpy.var(self.feats,0))\n\n    def contextMatch(self, expr):\n        res = expr.search(self.context)\n        return res.group(1)\n\nclass DistributionInfo:\n    def __init__(self, mixNum=1):\n        self.mean      = [None for x in range(mixNum)]\n        self.var       = [None for x in range(mixNum)]\n        self.mixWeight = [None for x in range(mixNum)]\n\n    def setVariance(self, variance, index=0):\n        self.var[index] = numpy.array(variance, dtype=numpy.float)\n\n    def setMean(self, mean, index=0):\n        self.mean[index] = numpy.array(mean, dtype=numpy.float)\n\n    def setMixWeight(self, weight, index=0):\n        self.mixWeight[index] = weight\n\n    def getCovariance(self, index=0):\n        covariance = numpy.zeros((len(self.var[index]), len(self.var[index])))\n        for i in range(len(self.var[index])):\n            covariance[i, i] = self.var[index][i]\n        return covariance\n\n    def getInverseCovariance(self, index=0):\n        covariance = numpy.zeros((len(self.var[index]), len(self.var[index])))\n        for i in range(len(self.var[index])):\n            covariance[i,i] = 1.0/self.var[index][i]\n        return covariance\n\n    def getDimensionality(self, index=0):\n        return len(self.var[index])\n\n    def getMeans(self, index=0):\n        meanMatrix = numpy.transpose(numpy.matrix(self.mean[index]))\n        return meanMatrix\n\n    def getArrayVariances(self, index=0):\n        return self.var[index]\n\n    def getArrayMeans(self, index=0):\n        return self.mean[index]\n\n    def getMixWeight(self, index=0):\n        return self.mixWeight[index]\n\n    def enforceVFloor(self, varFloor, index=0):\n        count=0\n        for x in range(len(self.var[index])):\n            if self.var[index][x]<varFloor[x]:\n                self.var[index][x] = varFloor[x]\n                count = count+1\n        return count\n\n\ndef readBottleneckFeatures(fname, featNum=32):\n    data = numpy.fromfile(fname, 'float32')\n    data = data.reshape(-1,featNum)\n    return data\n\ndef calculateParamGV(feat_file_list, feat_dim=32):\n    data = numpy.empty((1, feat_dim))\n    for file_index in range(len(feat_file_list)):\n        file_name   = feat_file_list[file_index]\n        (junk, ext) = feat_file_list[file_index].split('.')\n\n        features    = readBottleneckFeatures(file_name, feat_dim)\n\n        if ext == 'lf0': #remove unvoiced values\n            features = features[numpy.where(features != -1.*(10**(10)))[0]]\n            features = numpy.exp(features) #convert to linear scale\n\n        if file_index==0:\n            data=features\n        else:\n            data=numpy.concatenate((data,features),0)\n\n    gv = numpy.var(data, 0)\n    return gv\n\ndef readHybridLabelFile(fname, idnum, sil_identifier='#', ignoreSilence=True):\n    fid  = open(fname, 'r')\n    data = fid.readlines()\n    fid.close()\n\n    lines = [[data[x].split()[0], data[x].split()[2]] for x in range(1,len(data))] #exclude first line!\n\n    columns = [[] for x in range(len(lines[0]))]\n    for line in lines:\n        for i, item in enumerate(line):\n            columns[i].append(item)\n\n    idarr   = numpy.ones(len(columns[0]))*idnum\n    stime   = numpy.hstack((0,numpy.array(columns[0][:-1],dtype=numpy.float64)))\n    columns = numpy.vstack((idarr,stime,columns))\n\n    if ignoreSilence:\n        keep = [not(bool(re.search(sil_identifier,x))) for x in columns[3]]\n    else:\n        keep = [bool(1) for x in range(len(columns[3]))]\n\n    toInc  = numpy.where(keep)[0]\n    gap    = numpy.array(columns[2][toInc], dtype=numpy.float64)-numpy.array(columns[1][toInc], dtype=numpy.float64)\n    frames = (gap*sectoms)/frameshift\n\n    frameEnd   = numpy.cumsum(frames)\n    frameEnd   = numpy.round(frameEnd,0)\n    frameStart = numpy.append(0,frameEnd[:-1])\n\n    allFrameStart = numpy.ones(len(columns[2]))*-1\n    allFrameEnd   = numpy.ones(len(columns[2]))*-1\n\n    for point in range(len(toInc)):\n        allFrameEnd[toInc[point]]   = frameEnd[point]\n        allFrameStart[toInc[point]] = frameStart[point]\n\n    data = [columns[0],allFrameStart,allFrameEnd,columns[3],numpy.array(columns[1], dtype=numpy.float64)*sectoms,numpy.array(columns[2], dtype=numpy.float64)*sectoms]\n\n    return data\n\ndef convertToHybridLabel(labData, numHybridSec):\n    hybridData   = [[] for x in range(len(labData))]\n    labDurations = labData[2]-labData[1]\n\n    tDur = labData[5]-labData[4]\n    for i in range(len(labData[0])):\n        #keep as frames or convert to time?! Currently kept in frames\n        sectionLen = float(labDurations[i])/numHybridSec\n\n        tLen = float(tDur[i])/numHybridSec\n        for j in range(numHybridSec):\n            hybridData[0].append(labData[0][0])\n            hybridData[1].append(int(labData[1][i]+numpy.floor((j)*sectionLen)))\n            hybridData[2].append(int(labData[1][i]+numpy.floor((j+1)*sectionLen)))\n            hybridData[3].append(labData[3][i]+'[{0}]'.format(j))\n            hybridData[5].append(int(labData[4][i]+numpy.floor((j+1)*tLen)))\n\n    hybridData[1] = numpy.array(hybridData[1])\n    hybridData[2] = numpy.array(hybridData[2])\n    hybridData[3] = numpy.array(hybridData[3])\n    hybridData[4] = numpy.append(labData[4][0],hybridData[5][0:len(hybridData[3])-1])\n    hybridData[5] = numpy.array(hybridData[5])\n\n    return hybridData\n"""
misc/scripts/voice_conversion/align_feats.py,0,"b""\nimport numpy\n\nfrom binary_io import BinaryIOCollection\n\nclass AlignFeats(object):\n    def __init__(self):\n        self.io_funcs = BinaryIOCollection()\n\n    def align_src_feats(self, src_feat_file, src_aligned_feat_file, feat_dim, dtw_path_dict):\n        '''\n        align source feats as per the dtw path (matching target length)\n        '''\n        src_features, frame_number = self.io_funcs.load_binary_file_frame(src_feat_file, feat_dim)\n        \n        tgt_length = len(dtw_path_dict)\n        src_aligned_features = numpy.zeros((tgt_length, feat_dim))\n        \n        for i in range(tgt_length):\n            src_aligned_features[i, ] = src_features[dtw_path_dict[i]]\n\n        self.io_funcs.array_to_binary_file(src_aligned_features, src_aligned_feat_file)\n"""
misc/scripts/voice_conversion/binary_io.py,0,"b'\n\nimport numpy\n\nclass   BinaryIOCollection(object):\n\n    def load_binary_file(self, file_name, dimension):\n        fid_lab = open(file_name, \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        assert features.size % float(dimension) == 0.0,\'specified dimension not compatible with data\'\n        features = features[:(dimension * (features.size // dimension))]\n        features = features.reshape((-1, dimension))\n\n        return  features\n\n    def array_to_binary_file(self, data, output_file_name):\n        data = numpy.array(data, \'float32\')\n\n        fid = open(output_file_name, \'wb\')\n        data.tofile(fid)\n        fid.close()\n\n    def load_binary_file_frame(self, file_name, dimension):\n        fid_lab = open(file_name, \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        assert features.size % float(dimension) == 0.0,\'specified dimension not compatible with data\'\n        frame_number = features.size // dimension\n        features = features[:(dimension * frame_number)]\n        features = features.reshape((-1, dimension))\n\n        return  features, frame_number\n\n    def load_binary_dtw_file(self, file_name, dimension=2):\n        fid_lab = open(file_name, \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=""int32"")\n        fid_lab.close()\n        assert features.size % float(dimension) == 0.0,\'specified dimension not compatible with data\'\n        frame_number = features.size // dimension\n        features = features[:(dimension * frame_number)]\n        features = features.reshape((-1, dimension))\n        \n        feat_path_dict = {}\n        for i in range(frame_number):\n            feat_path_dict[features[i][1]] = features[i][0]\n        \n        return  feat_path_dict\n    \n    def load_ascii_dtw_file(self, file_name):\n        fid_lab = open(file_name, \'r\')\n        data = fid_lab.readlines()\n        fid_lab.close()\n\n        feat_path_dict = {}\n        for newline in data[0:-1]:\n            temp_list = newline.strip().split()\n            feat_path_dict[int(temp_list[0])] = int(temp_list[1])\n        \n        return  feat_path_dict\n'"
misc/scripts/voice_conversion/compute_lf0_stats.py,0,"b'import os,sys\nimport numpy\nimport itertools    \n\nfrom binary_io import BinaryIOCollection\n\nio_funcs = BinaryIOCollection()\n\ndef compute_mean_and_std(lf0_file_list):\n    all_files_lf0_arr = numpy.zeros(200000)\n    \n    current_index = 0\n    for lf0_file in lf0_file_list:\n        lf0_arr, frame_number = io_funcs.load_binary_file_frame(lf0_file, 1)\n        for lf0_value in lf0_arr:\n            all_files_lf0_arr[current_index] = numpy.exp(lf0_value)\n            current_index+=1\n\n    all_files_lf0_arr = all_files_lf0_arr[all_files_lf0_arr>0]\n    all_files_lf0_arr = numpy.log(all_files_lf0_arr)\n    \n    mean_f0 = numpy.mean(all_files_lf0_arr)\n    std_f0  = numpy.std(all_files_lf0_arr)\n\n    return mean_f0, std_f0\n\ndef get_lf0_filelist(lf0_dir):\n    lf0_files = []\n    for file in os.listdir(lf0_dir):\n        whole_filepath = os.path.join(lf0_dir,file)\n        if os.path.isfile(whole_filepath) and str(whole_filepath).endswith("".lf0""):\n            lf0_files.append(whole_filepath)\n        elif os.path.isdir(whole_filepath):\n            lf0_files += get_lf0_filelist(whole_filepath)\n\n    lf0_files.sort()\n\n    return lf0_files\n\n\nif __name__ == ""__main__"":\n    # parse the arguments\n    lf0_dir = sys.argv[1]\n    lf0_stats_file = sys.argv[2]\n\n    lf0_file_list   = get_lf0_filelist(lf0_dir)\n    mean_f0, std_f0 = compute_mean_and_std(lf0_file_list)\n   \n    out_f = open(lf0_stats_file, \'w\')\n    out_f.write(\'%f %f\\n\' %(mean_f0, std_f0))\n    out_f.close()\n    \n\n'"
misc/scripts/voice_conversion/dtw_aligner.py,0,"b'#!/usr/bin/env python\nimport os\nimport sys\nimport time\nimport shutil\nimport multiprocessing as mp\n\nimport fastdtw\n\nfrom binary_io import BinaryIOCollection\nfrom align_feats import AlignFeats\n\nif len(sys.argv)!=6:\n    print(""Usage: python dtw_aligner.py <path_to_tools_dir> <path_to_src_feat_dir> <path_to_tgt_feat_dir> <path_to_src_aligned_feat_dir> <bap_dim>"")\n    sys.exit(1)\n\n### Arguments\n\n# tools directory\ntools_dir = sys.argv[1]\n\n# Source features directory\nsrc_feat_dir = sys.argv[2]\n\n# Target features directory\ntgt_feat_dir = sys.argv[3]\n\n# Source-aligned features directory\nsrc_aligned_feat_dir = sys.argv[4]\n\n# bap dimension\nbap_dim = int(sys.argv[5])\n\nif not os.path.exists(src_aligned_feat_dir):\n    os.makedirs(src_aligned_feat_dir)\n\n### Define variables\nmgc_dim = 60\nlf0_dim = 1\n\nsrc_mgc_dir = os.path.join(src_feat_dir, ""mgc"")\ntgt_mgc_dir = os.path.join(tgt_feat_dir, ""mgc"")\n\nsrc_bap_dir = os.path.join(src_feat_dir, ""bap"")\ntgt_bap_dir = os.path.join(tgt_feat_dir, ""bap"")\n\nsrc_lf0_dir = os.path.join(src_feat_dir, ""lf0"")\ntgt_lf0_dir = os.path.join(tgt_feat_dir, ""lf0"")\n\n### create outut directories\nsrc_aligned_mgc_dir = os.path.join(src_aligned_feat_dir, ""mgc"")\nsrc_aligned_bap_dir = os.path.join(src_aligned_feat_dir, ""bap"")\nsrc_aligned_lf0_dir = os.path.join(src_aligned_feat_dir, ""lf0"")\n\nif not os.path.exists(src_aligned_mgc_dir):\n    os.mkdir(src_aligned_mgc_dir)\n\nif not os.path.exists(src_aligned_bap_dir):\n    os.mkdir(src_aligned_bap_dir)\n\nif not os.path.exists(src_aligned_lf0_dir):\n    os.mkdir(src_aligned_lf0_dir)\n\n#################################################################\n######## align source feats with target feats using dtw ## ######\n#################################################################\n\nio_funcs = BinaryIOCollection()\naligner  = AlignFeats()\n\ndef get_mgc_filelist(mgc_dir):\n    mgc_files = []\n    for file in os.listdir(mgc_dir):\n        whole_filepath = os.path.join(mgc_dir,file)\n        if os.path.isfile(whole_filepath) and str(whole_filepath).endswith("".mgc""):\n            mgc_files.append(whole_filepath)\n        elif os.path.isdir(whole_filepath):\n            mgc_files += get_mgc_filelist(whole_filepath)\n\n    mgc_files.sort()\n\n    return mgc_files\n\ndef load_dtw_path(dtw_path):\n    dtw_path_dict = {}\n    nframes = len(dtw_path)\n\n    for item,i in zip(dtw_path, range(nframes)):\n        if item[1] not in dtw_path_dict:\n            dtw_path_dict[item[1]] = item[0]\n\n    return dtw_path_dict\n\ndef process(filename):\n    \'\'\'\n    The function derives dtw alignment path given source mgc and target mgc\n    :param filename: path to src mgc file\n    \'\'\'\n    file_id = os.path.basename(filename).split(""."")[0]\n    print(file_id)\n\n    ### DTW alignment -- align source with target parameters ###\n    src_mgc_file = os.path.join(src_mgc_dir, file_id+ "".mgc"")\n    tgt_mgc_file = os.path.join(tgt_mgc_dir, file_id+ "".mgc"")\n\n    src_features, src_frame_number = io_funcs.load_binary_file_frame(src_mgc_file, mgc_dim)\n    tgt_features, tgt_frame_number = io_funcs.load_binary_file_frame(tgt_mgc_file, mgc_dim)\n\n    ### dtw align src with tgt ###\n    distance, dtw_path = fastdtw.fastdtw(src_features, tgt_features)\n\n    ### load dtw path\n    dtw_path_dict = load_dtw_path(dtw_path)\n    assert len(dtw_path_dict)==tgt_frame_number   # dtw length not matched\n\n    ### align features\n    aligner.align_src_feats(os.path.join(src_mgc_dir, file_id+ "".mgc""), os.path.join(src_aligned_mgc_dir, file_id+ "".mgc""), mgc_dim, dtw_path_dict)\n    aligner.align_src_feats(os.path.join(src_bap_dir, file_id+ "".bap""), os.path.join(src_aligned_bap_dir, file_id+ "".bap""), bap_dim, dtw_path_dict)\n    aligner.align_src_feats(os.path.join(src_lf0_dir, file_id+ "".lf0""), os.path.join(src_aligned_lf0_dir, file_id+ "".lf0""), lf0_dim, dtw_path_dict)\n\nprint(""--- DTW alignment started ---"")\nstart_time = time.time()\n\n# get mgc files list\nmgc_files = get_mgc_filelist(src_mgc_dir)\n\n# do multi-processing\npool = mp.Pool(mp.cpu_count())\npool.map(process, mgc_files)\n\n(m, s) = divmod(int(time.time() - start_time), 60)\nprint((""--- DTW alignment completion time: %d min. %d sec ---"" % (m, s)))\n\nif not os.path.exists(src_aligned_mgc_dir):\n    print(""DTW alignment unsucessful!!"")\nelse:\n    print(""You should have your src feats(aligned with target) ready in: %s"" % (src_aligned_feat_dir))    \n\n'"
misc/scripts/voice_conversion/dtw_aligner_festvox.py,0,"b'#!/usr/bin/env python\nimport os\nimport sys\nimport time\nimport shutil\nimport multiprocessing as mp\n\nfrom binary_io import BinaryIOCollection\nfrom align_feats import AlignFeats\n\nif len(sys.argv)!=6:\n    print(""Usage: python dtw_aligner_festvox.py <path_to_tools_dir> <path_to_src_feat_dir> <path_to_tgt_feat_dir> <path_to_src_aligned_feat_dir> <bap_dim>"")\n    sys.exit(1)\n\n### Arguments\n\n# tools directory\ntools_dir = sys.argv[1]\n\n# Source features directory\nsrc_feat_dir = sys.argv[2]\n\n# Target features directory\ntgt_feat_dir = sys.argv[3]\n\n# Source-aligned features directory\nsrc_aligned_feat_dir = sys.argv[4]\n\n# bap dimension\nbap_dim = int(sys.argv[5])\n\nif not os.path.exists(src_aligned_feat_dir):\n    os.makedirs(src_aligned_feat_dir)\n\n# path to tools\nsptk = os.path.join(tools_dir, ""bin/SPTK-3.9"")\nspeech_tools = os.path.join(tools_dir, ""speech_tools/bin"")\nfestvox = os.path.join(tools_dir, ""festvox"")\n\n### Define variables\nmgc_dim = 60\nlf0_dim = 1\n\nsrc_mgc_dir = os.path.join(src_feat_dir, ""mgc"")\ntgt_mgc_dir = os.path.join(tgt_feat_dir, ""mgc"")\n\nsrc_bap_dir = os.path.join(src_feat_dir, ""bap"")\ntgt_bap_dir = os.path.join(tgt_feat_dir, ""bap"")\n\nsrc_lf0_dir = os.path.join(src_feat_dir, ""lf0"")\ntgt_lf0_dir = os.path.join(tgt_feat_dir, ""lf0"")\n\n### create outut directories\nalignments_dir = os.path.join(src_aligned_feat_dir, ""../dtw_alignments"")\ntemp_dir = os.path.join(src_aligned_feat_dir, ""../temp"")\n\nsrc_aligned_mgc_dir = os.path.join(src_aligned_feat_dir, ""mgc"")\nsrc_aligned_bap_dir = os.path.join(src_aligned_feat_dir, ""bap"")\nsrc_aligned_lf0_dir = os.path.join(src_aligned_feat_dir, ""lf0"")\n\nif not os.path.exists(alignments_dir):\n    os.mkdir(alignments_dir)\n\nif not os.path.exists(temp_dir):\n    os.mkdir(temp_dir)\n\nif not os.path.exists(src_aligned_mgc_dir):\n    os.mkdir(src_aligned_mgc_dir)\n\nif not os.path.exists(src_aligned_bap_dir):\n    os.mkdir(src_aligned_bap_dir)\n\nif not os.path.exists(src_aligned_lf0_dir):\n    os.mkdir(src_aligned_lf0_dir)\n\n#################################################################\n######## align source feats with target feats using dtw ## ######\n#################################################################\n\nio_funcs = BinaryIOCollection()\naligner  = AlignFeats()\n\n# create dummy lab files\nos.system(""touch %s/i.lab"" % (temp_dir))\nos.system(""touch %s/o.lab"" % (temp_dir))\n\ndef get_mgc_filelist(mgc_dir):\n    mgc_files = []\n    for file in os.listdir(mgc_dir):\n        whole_filepath = os.path.join(mgc_dir,file)\n        if os.path.isfile(whole_filepath) and str(whole_filepath).endswith("".mgc""):\n            mgc_files.append(whole_filepath)\n        elif os.path.isdir(whole_filepath):\n            mgc_files += get_mgc_filelist(whole_filepath)\n\n    mgc_files.sort()\n\n    return mgc_files\n\ndef process(filename):\n    \'\'\'\n    The function derives dtw alignment path given source mgc and target mgc\n    :param filename: path to src mgc file\n    :return: .dtw files\n    \'\'\'\n    file_id = os.path.basename(filename).split(""."")[0]\n    print(file_id)\n\n    ### DTW alignment -- align source with target parameters ###\n    src_mgc_file = os.path.join(src_mgc_dir, file_id+ "".mgc"")\n    tgt_mgc_file = os.path.join(tgt_mgc_dir, file_id+ "".mgc"")\n\n    src_features, src_frame_number = io_funcs.load_binary_file_frame(src_mgc_file, mgc_dim)\n    tgt_features, tgt_frame_number = io_funcs.load_binary_file_frame(tgt_mgc_file, mgc_dim)\n\n    ### dtw align src with tgt ###\n    dtw_alignment_file = os.path.join(alignments_dir, file_id+ "".dtw"")\n    \n    x2x_cmd1 = ""%s +fa %s | xargs -n%d > %s"" %(os.path.join(sptk, ""x2x""), src_mgc_file, mgc_dim, os.path.join(temp_dir, file_id+ ""_src_ascii.mgc""))\n    x2x_cmd2 = ""%s +fa %s | xargs -n%d > %s"" %(os.path.join(sptk, ""x2x""), tgt_mgc_file, mgc_dim, os.path.join(temp_dir, file_id+ ""_tgt_ascii.mgc""))\n    \n    os.system(x2x_cmd1)\n    os.system(x2x_cmd2)\n    \n    chtrack_cmd1 = ""%s -s 0.005 -otype est_binary %s -o %s"" %(os.path.join(speech_tools, ""ch_track""), \\\n                                                                os.path.join(temp_dir, file_id+ ""_src_ascii.mgc""), \\\n                                                                os.path.join(temp_dir, file_id+ ""_src_binary.mgc""))\n    \n    os.system(chtrack_cmd1)\n\n    chtrack_cmd2 = ""%s -s 0.005 -otype est_binary %s -o %s"" %(os.path.join(speech_tools, ""ch_track""), \\\n                                                                os.path.join(temp_dir, file_id+ ""_tgt_ascii.mgc""), \\\n                                                                os.path.join(temp_dir, file_id+ ""_tgt_binary.mgc""))\n    os.system(chtrack_cmd2)\n    \n    phone_align_cmd = ""%s -itrack %s -otrack %s -ilabel %s -olabel %s -verbose -withcosts > %s"" %(os.path.join(festvox, ""src/general/phonealign""), \\\n                                                            os.path.join(temp_dir, file_id+ ""_tgt_binary.mgc""), os.path.join(temp_dir, file_id+ ""_src_binary.mgc""), \\\n                                                            os.path.join(temp_dir, ""i.lab""), os.path.join(temp_dir, ""o.lab""), dtw_alignment_file)\n    os.system(phone_align_cmd)\n\n    ### load dtw path\n    dtw_path_dict = io_funcs.load_ascii_dtw_file(dtw_alignment_file)\n    assert len(dtw_path_dict)==tgt_frame_number   # dtw length not matched\n\n    ### align features\n    aligner.align_src_feats(os.path.join(src_mgc_dir, file_id+ "".mgc""), os.path.join(src_aligned_mgc_dir, file_id+ "".mgc""), mgc_dim, dtw_path_dict)\n    aligner.align_src_feats(os.path.join(src_bap_dir, file_id+ "".bap""), os.path.join(src_aligned_bap_dir, file_id+ "".bap""), bap_dim, dtw_path_dict)\n    aligner.align_src_feats(os.path.join(src_lf0_dir, file_id+ "".lf0""), os.path.join(src_aligned_lf0_dir, file_id+ "".lf0""), lf0_dim, dtw_path_dict)\n\nprint(""--- DTW alignment started ---"")\nstart_time = time.time()\n\n# get mgc files list\nmgc_files = get_mgc_filelist(src_mgc_dir)\n\n# do multi-processing\npool = mp.Pool(mp.cpu_count())\npool.map(process, mgc_files)\n\n# clean temporal files\nshutil.rmtree(alignments_dir, ignore_errors=True)\nshutil.rmtree(temp_dir, ignore_errors=True)\n\n(m, s) = divmod(int(time.time() - start_time), 60)\nprint((""--- DTW alignment completion time: %d min. %d sec ---"" % (m, s)))\n\nif not os.path.exists(src_aligned_mgc_dir):\n    print(""DTW alignment unsucessful!!"")\nelse:\n    print(""You should have your src feats(aligned with target) ready in: %s"" % (src_aligned_feat_dir))    \n\n'"
misc/scripts/voice_conversion/dtw_aligner_festvox_magphase.py,0,"b'#!/usr/bin/env python\nimport os\nimport sys\nimport time\nimport shutil\nimport multiprocessing as mp\n\nfrom binary_io import BinaryIOCollection\nfrom align_feats import AlignFeats\n\nif len(sys.argv)!=5:\n    print(""Usage: python dtw_aligner_festvox_magphase.py <path_to_tools_dir> <path_to_src_feat_dir> <path_to_tgt_feat_dir> <path_to_src_aligned_feat_dir>"")\n    sys.exit(1)\n\n### Arguments\n\n# tools directory\ntools_dir = sys.argv[1]\n\n# Source features directory\nsrc_feat_dir = sys.argv[2]\n\n# Target features directory\ntgt_feat_dir = sys.argv[3]\n\n# Source-aligned features directory\nsrc_aligned_feat_dir = sys.argv[4]\n\n# bap dimension\n#bap_dim = int(sys.argv[5])\n\nif not os.path.exists(src_aligned_feat_dir):\n    os.makedirs(src_aligned_feat_dir)\n\n# path to tools\nsptk = os.path.join(tools_dir, ""bin/SPTK-3.9"")\nspeech_tools = os.path.join(tools_dir, ""speech_tools/bin"")\nfestvox = os.path.join(tools_dir, ""festvox"")\n\n### Define variables. TODO: read from config file (void hardcoding)\nmag_dim  = 60\nreal_dim = 45\nimag_dim = 45\nlf0_dim  = 1\n\n#src_mag_dir = os.path.join(src_feat_dir, ""mag"")\n#tgt_mag_dir = os.path.join(tgt_feat_dir, ""mag"")\n\n#src_bap_dir = os.path.join(src_feat_dir, ""bap"")\n#tgt_bap_dir = os.path.join(tgt_feat_dir, ""bap"")\n\n#src_lf0_dir = os.path.join(src_feat_dir, ""lf0"")\n#tgt_lf0_dir = os.path.join(tgt_feat_dir, ""lf0"")\n\n### create outut directories\nalignments_dir = os.path.join(src_aligned_feat_dir, ""../dtw_alignments"")\ntemp_dir = os.path.join(src_aligned_feat_dir, ""../temp"")\n\n#src_aligned_mag_dir = os.path.join(src_aligned_feat_dir, ""mag"")\n#src_aligned_bap_dir = os.path.join(src_aligned_feat_dir, ""bap"")\n#src_aligned_lf0_dir = os.path.join(src_aligned_feat_dir, ""lf0"")\n\nif not os.path.exists(alignments_dir):\n    os.mkdir(alignments_dir)\n\nif not os.path.exists(temp_dir):\n    os.mkdir(temp_dir)\n\n#if not os.path.exists(src_aligned_mag_dir):\n#    os.mkdir(src_aligned_mag_dir)\n\n#if not os.path.exists(src_aligned_bap_dir):\n#    os.mkdir(src_aligned_bap_dir)\n\n#if not os.path.exists(src_aligned_lf0_dir):\n#    os.mkdir(src_aligned_lf0_dir)\n\n#################################################################\n######## align source feats with target feats using dtw ## ######\n#################################################################\n\nio_funcs = BinaryIOCollection()\naligner  = AlignFeats()\n\n# create dummy lab files\nos.system(""touch %s/i.lab"" % (temp_dir))\nos.system(""touch %s/o.lab"" % (temp_dir))\n\ndef get_mag_filelist(mag_dir):\n    mag_files = []\n    for file in os.listdir(mag_dir):\n        whole_filepath = os.path.join(mag_dir,file)\n        if os.path.isfile(whole_filepath) and str(whole_filepath).endswith("".mag""):\n            mag_files.append(whole_filepath)\n        elif os.path.isdir(whole_filepath):\n            mag_files += get_mag_filelist(whole_filepath)\n\n    mag_files.sort()\n\n    return mag_files\n\ndef process(filename):\n    \'\'\'\n    The function derives dtw alignment path given source mag and target mag\n    :param filename: path to src mag file\n    :return: .dtw files\n    \'\'\'\n    file_id = os.path.basename(filename).split(""."")[0]\n    print(file_id)\n\n    ### DTW alignment -- align source with target parameters ###\n    src_mag_file = os.path.join(src_feat_dir, file_id + "".mag"")\n    tgt_mag_file = os.path.join(tgt_feat_dir, file_id + "".mag"")\n\n    src_features, src_frame_number = io_funcs.load_binary_file_frame(src_mag_file, mag_dim)\n    tgt_features, tgt_frame_number = io_funcs.load_binary_file_frame(tgt_mag_file, mag_dim)\n\n    ### dtw align src with tgt ###\n    dtw_alignment_file = os.path.join(alignments_dir, file_id+ "".dtw"")\n    \n    x2x_cmd1 = ""%s +fa %s | xargs -n%d > %s"" %(os.path.join(sptk, ""x2x""), src_mag_file, mag_dim, os.path.join(temp_dir, file_id+ ""_src_ascii.mag""))\n    x2x_cmd2 = ""%s +fa %s | xargs -n%d > %s"" %(os.path.join(sptk, ""x2x""), tgt_mag_file, mag_dim, os.path.join(temp_dir, file_id+ ""_tgt_ascii.mag""))\n    \n    os.system(x2x_cmd1)\n    os.system(x2x_cmd2)\n    \n    chtrack_cmd1 = ""%s -s 0.005 -otype est_binary %s -o %s"" %(os.path.join(speech_tools, ""ch_track""), \\\n                                                                os.path.join(temp_dir, file_id+ ""_src_ascii.mag""), \\\n                                                                os.path.join(temp_dir, file_id+ ""_src_binary.mag""))\n    \n    os.system(chtrack_cmd1)\n\n    chtrack_cmd2 = ""%s -s 0.005 -otype est_binary %s -o %s"" %(os.path.join(speech_tools, ""ch_track""), \\\n                                                                os.path.join(temp_dir, file_id+ ""_tgt_ascii.mag""), \\\n                                                                os.path.join(temp_dir, file_id+ ""_tgt_binary.mag""))\n    os.system(chtrack_cmd2)\n    \n    phone_align_cmd = ""%s -itrack %s -otrack %s -ilabel %s -olabel %s -verbose -withcosts > %s"" %(os.path.join(festvox, ""src/general/phonealign""), \\\n                                                            os.path.join(temp_dir, file_id+ ""_tgt_binary.mag""), os.path.join(temp_dir, file_id+ ""_src_binary.mag""), \\\n                                                            os.path.join(temp_dir, ""i.lab""), os.path.join(temp_dir, ""o.lab""), dtw_alignment_file)\n    os.system(phone_align_cmd)\n\n    ### load dtw path\n    dtw_path_dict = io_funcs.load_ascii_dtw_file(dtw_alignment_file)\n    assert len(dtw_path_dict)==tgt_frame_number   # dtw length not matched\n\n    ### align features\n    aligner.align_src_feats(os.path.join(src_feat_dir, file_id + "".mag"") , os.path.join(src_aligned_feat_dir, file_id + "".mag""), mag_dim , dtw_path_dict)\n    aligner.align_src_feats(os.path.join(src_feat_dir, file_id + "".real""), os.path.join(src_aligned_feat_dir, file_id + "".real""), real_dim, dtw_path_dict)\n    aligner.align_src_feats(os.path.join(src_feat_dir, file_id + "".imag""), os.path.join(src_aligned_feat_dir, file_id + "".imag""), imag_dim, dtw_path_dict)\n    aligner.align_src_feats(os.path.join(src_feat_dir, file_id + "".lf0"") , os.path.join(src_aligned_feat_dir, file_id + "".lf0""), lf0_dim , dtw_path_dict)\n\nprint(""--- DTW alignment started ---"")\nstart_time = time.time()\n\n# get mag files list\nmag_files = get_mag_filelist(src_feat_dir)\n\n# do multi-processing\npool = mp.Pool(mp.cpu_count())\npool.map(process, mag_files)\n\n# clean temporal files\nshutil.rmtree(alignments_dir, ignore_errors=True)\nshutil.rmtree(temp_dir, ignore_errors=True)\n\n(m, s) = divmod(int(time.time() - start_time), 60)\nprint((""--- DTW alignment completion time: %d min. %d sec ---"" % (m, s)))\n\nif not os.path.exists(src_aligned_feat_dir):\n    print(""DTW alignment unsucessful!!"")\nelse:\n    print(""You should have your src feats(aligned with target) ready in: %s"" % (src_aligned_feat_dir))    \n\n'"
misc/scripts/voice_conversion/dtw_aligner_magphase.py,0,"b'#!/usr/bin/env python\nimport os\nimport sys\nimport time\n#import shutil\nimport multiprocessing as mp\n\nimport fastdtw\n\nfrom binary_io import BinaryIOCollection\nfrom align_feats import AlignFeats\n\nif len(sys.argv)!=5:\n    print(""Usage: python dtw_aligner_magphase.py <path_to_tools_dir> <path_to_src_feat_dir> <path_to_tgt_feat_dir> <path_to_src_aligned_feat_dir>"")\n    sys.exit(1)\n\n### Arguments\n\n# tools directory\ntools_dir = sys.argv[1]\n\n# Source features directory\nsrc_feat_dir = sys.argv[2]\n\n# Target features directory\ntgt_feat_dir = sys.argv[3]\n\n# Source-aligned features directory\nsrc_aligned_feat_dir = sys.argv[4]\n\nif not os.path.exists(src_aligned_feat_dir):\n    os.makedirs(src_aligned_feat_dir)\n\n### Define variables\nmag_dim  = 60 # TODO: Change this (avoid hardcoded)\nreal_dim = 10\nimag_dim = 10\nlf0_dim  = 1\n\n#src_mag_dir = src_feat_dir\n#tgt_mag_dir = tgt_feat_dir\n\n#src_lf0_dir = os.path.join(src_feat_dir, ""lf0"")\n#tgt_lf0_dir = os.path.join(tgt_feat_dir, ""lf0"")\n\n### create outut directories\n#src_aligned_mag_dir = os.path.join(src_aligned_feat_dir, ""mag"")\n#src_aligned_bap_dir = os.path.join(src_aligned_feat_dir, ""bap"")\n#src_aligned_lf0_dir = os.path.join(src_aligned_feat_dir, ""lf0"")\n\n#if not os.path.exists(src_aligned_mag_dir):\n#    os.mkdir(src_aligned_mag_dir)\n\n#if not os.path.exists(src_aligned_bap_dir):\n#    os.mkdir(src_aligned_bap_dir)\n\n#if not os.path.exists(src_aligned_lf0_dir):\n#    os.mkdir(src_aligned_lf0_dir)\n\n#################################################################\n######## align source feats with target feats using dtw ## ######\n#################################################################\n\nio_funcs = BinaryIOCollection()\naligner  = AlignFeats()\n\ndef get_mag_filelist(mag_dir):\n    mag_files = []\n    for file in os.listdir(mag_dir):\n        whole_filepath = os.path.join(mag_dir,file)\n        if os.path.isfile(whole_filepath) and str(whole_filepath).endswith("".mag""):\n            mag_files.append(whole_filepath)\n        elif os.path.isdir(whole_filepath):\n            mag_files += get_mag_filelist(whole_filepath)\n\n    mag_files.sort()\n\n    return mag_files\n\ndef load_dtw_path(dtw_path):\n    dtw_path_dict = {}\n    nframes = len(dtw_path)\n\n    for item,i in zip(dtw_path, range(nframes)):\n        if item[1] not in dtw_path_dict:\n            dtw_path_dict[item[1]] = item[0]\n\n    return dtw_path_dict\n\ndef process(filename):\n    \'\'\'\n    The function derives dtw alignment path given source mag and target mag\n    :param filename: path to src mag file\n    \'\'\'\n    file_id = os.path.basename(filename).split(""."")[0]\n    print(file_id)\n\n    ### DTW alignment -- align source with target parameters ###\n    src_mag_file = os.path.join(src_feat_dir, file_id + "".mag"")\n    tgt_mag_file = os.path.join(tgt_feat_dir, file_id + "".mag"")\n\n    src_features, src_frame_number = io_funcs.load_binary_file_frame(src_mag_file, mag_dim)\n    tgt_features, tgt_frame_number = io_funcs.load_binary_file_frame(tgt_mag_file, mag_dim)\n\n    ### dtw align src with tgt ###\n    distance, dtw_path = fastdtw.fastdtw(src_features, tgt_features)\n\n    ### load dtw path\n    dtw_path_dict = load_dtw_path(dtw_path)\n    assert len(dtw_path_dict)==tgt_frame_number   # dtw length not matched\n\n    ### align features\n    aligner.align_src_feats(os.path.join(src_feat_dir, file_id + "".mag"") , os.path.join(src_aligned_feat_dir, file_id + "".mag"") , mag_dim , dtw_path_dict)\n    aligner.align_src_feats(os.path.join(src_feat_dir, file_id + "".real""), os.path.join(src_aligned_feat_dir, file_id + "".real""), real_dim, dtw_path_dict)\n    aligner.align_src_feats(os.path.join(src_feat_dir, file_id + "".imag""), os.path.join(src_aligned_feat_dir, file_id + "".imag""), imag_dim, dtw_path_dict)\n    aligner.align_src_feats(os.path.join(src_feat_dir, file_id + "".lf0"") , os.path.join(src_aligned_feat_dir, file_id + "".lf0"") , lf0_dim , dtw_path_dict)\n\nprint(""--- DTW alignment started ---"")\nstart_time = time.time()\n\n# get mag files list\nmag_files = get_mag_filelist(src_feat_dir)\n\n# do multi-processing\npool = mp.Pool(mp.cpu_count())\npool.map(process, mag_files)\n\n(m, s) = divmod(int(time.time() - start_time), 60)\nprint((""--- DTW alignment completion time: %d min. %d sec ---"" % (m, s)))\n\nif not os.path.exists(src_aligned_feat_dir):\n    print(""DTW alignment unsucessful!!"")\nelse:\n    print(""You should have your src feats(aligned with target) ready in: %s"" % (src_aligned_feat_dir))    \n\n\n'"
misc/scripts/voice_conversion/transform_f0.py,0,"b'import os,sys\nimport numpy\nimport argparse\n    \nfrom binary_io import BinaryIOCollection\n\nio_funcs = BinaryIOCollection()\n \ndef read_file_list(file_name):\n    file_lists = []\n    fid = open(file_name)\n    for line in fid.readlines():\n        line = line.strip()\n        if len(line) < 1:\n            continue\n        file_lists.append(line)\n    fid.close()\n\n    return  file_lists\n\ndef prepare_file_path_list(file_id_list, file_dir, file_extension, new_dir_switch=True):\n    if not os.path.exists(file_dir) and new_dir_switch:\n        os.makedirs(file_dir)\n    file_name_list = []\n    for file_id in file_id_list:\n        file_name = file_dir + \'/\' + file_id + file_extension\n        file_name_list.append(file_name)\n\n    return  file_name_list\n\ndef transform_f0(src_lf0_arr, stats_dict):\n    mu_src = stats_dict[\'mu_src\']\n    mu_tgt = stats_dict[\'mu_tgt\']\n\n    std_src = stats_dict[\'std_src\']\n    std_tgt = stats_dict[\'std_tgt\']\n\n    tgt_lf0_arr = numpy.zeros(len(src_lf0_arr))\n    for i in range(len(src_lf0_arr)):\n        lf0_src = src_lf0_arr[i]\n        f0_src  = numpy.exp(lf0_src)\n        if f0_src <= 0:\n            tgt_lf0_arr[i] = lf0_src\n        else:\n            tgt_lf0_arr[i] = (mu_tgt + (std_tgt/std_src)*(lf0_src - mu_src)) \n    \n    return tgt_lf0_arr\n\ndef transform_lf0_dir(src_lf0_file_list, tgt_lf0_file_list, stats_dict):\n    for i in range(len(src_lf0_file_list)):\n        src_lf0_file = src_lf0_file_list[i]\n        tgt_lf0_file = tgt_lf0_file_list[i]\n        transform_lf0_file(src_lf0_file, tgt_lf0_file, stats_dict)\n\ndef transform_lf0_file(src_lf0_file, tgt_lf0_file, stats_dict):\n    src_lf0_arr, frame_number = io_funcs.load_binary_file_frame(src_lf0_file, 1)\n    tgt_lf0_arr = transform_f0(src_lf0_arr, stats_dict)\n    io_funcs.array_to_binary_file(tgt_lf0_arr, tgt_lf0_file)\n\ndef get_lf0_filelist(lf0_dir):\n    lf0_files = []\n    for file in os.listdir(lf0_dir):\n        whole_filepath = os.path.join(lf0_dir,file)\n        if os.path.isfile(whole_filepath) and str(whole_filepath).endswith("".lf0""):\n            lf0_files.append(whole_filepath)\n        elif os.path.isdir(whole_filepath):\n            lf0_files += get_lf0_filelist(whole_filepath)\n\n    lf0_files.sort()\n\n    return lf0_files\n\n\nif __name__ == ""__main__"":\n    # parse the arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--srcstatsfile\', required=True, help=\'path to source lf0 stats file\')\n    parser.add_argument(\'--tgtstatsfile\', required=True, help=\'path to target lf0 stats file\')\n    parser.add_argument(\'--srcdir\', type=str, help=\'path to source lf0 data directory\')\n    parser.add_argument(\'--tgtdir\', type=str, help=\'path to target lf0 data directory\')\n    parser.add_argument(\'--filelist\', type=str, help=\'path to file ID list\')\n    parser.add_argument(\'--srcfile\', type=str, help=\'path to source lf0 data file\')\n    parser.add_argument(\'--tgtfile\', type=str, help=\'path to target lf0 data file\')\n    opt = parser.parse_args()\n    \n    if opt.srcdir is None and opt.srcfile is None:\n        print(""at least one of --srcdir and --srcfile is required"")\n        sys.exit(1)\n\n    if opt.tgtdir is None and opt.tgtfile is None:\n        print(""at least one of --tgtdir and --tgtfile is required"")\n        sys.exit(1)\n    \n    if opt.srcdir is not None and opt.filelist is None:\n        print(""file ID list is required"")\n        sys.exit(1)\n    \n    src_lf0_stats_file = opt.srcstatsfile\n    tgt_lf0_stats_file = opt.tgtstatsfile\n    \n    if os.path.isfile(src_lf0_stats_file):\n        in_f = open(src_lf0_stats_file, \'r\')\n        data = in_f.readlines()\n        in_f.close()\n\n        [src_mean_f0, src_std_f0] = map(float, data[0].strip().split())\n    else:\n        print(""File doesn\'t exist!! Please check path: %s"" %(src_lf0_stats_file))\n        \n    if os.path.isfile(tgt_lf0_stats_file):\n        in_f = open(tgt_lf0_stats_file, \'r\')\n        data = in_f.readlines()\n        in_f.close()\n\n        [tgt_mean_f0, tgt_std_f0] = map(float, data[0].strip().split())\n    else:\n        print(""File doesn\'t exist!! Please check path: %s"" %(tgt_lf0_stats_file))\n\n    #print(src_mean_f0, src_std_f0)\n    #print(tgt_mean_f0, tgt_std_f0)\n   \n    stats_dict = {}\n\n    stats_dict[\'mu_src\'] = src_mean_f0\n    stats_dict[\'mu_tgt\'] = tgt_mean_f0\n\n    stats_dict[\'std_src\'] = src_std_f0\n    stats_dict[\'std_tgt\'] = tgt_std_f0\n\n    if opt.srcdir is not None and opt.tgtdir is not None:\n        file_id_list = read_file_list(opt.filelist)\n        src_lf0_file_list = prepare_file_path_list(file_id_list, opt.srcdir, \'.lf0\')\n        tgt_lf0_file_list = prepare_file_path_list(file_id_list, opt.tgtdir, \'.lf0\')\n        \n        transform_lf0_dir(src_lf0_file_list, tgt_lf0_file_list, stats_dict)\n\n    elif opt.srcfile is not None and opt.tgtfile is not None:\n\n        transform_lf0_file(opt.srcfile, opt.tgtfile, stats_dict)\n        \n'"
src/work_in_progress/oliver/dnn_synth.py,0,"b'\nimport pickle\nimport gzip\nimport os, sys, errno\nimport time\nimport math\nimport glob\nimport struct\n\nfile_location = os.path.split(os.path.realpath(os.path.abspath(os.path.dirname(__file__))))[0]+\'/\'\nsys.path.append(file_location + \'/../\')\n\n\n#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)\nimport numpy\n# we need to explicitly import this in some cases, not sure why this doesn\'t get imported with numpy itself\nimport numpy.distutils.__config__\n# and only after that can we import theano\nimport theano\n\nfrom utils.providers import ListDataProvider\n\nfrom frontend.label_normalisation import HTSLabelNormalisation, XMLLabelNormalisation\nfrom frontend.silence_remover import SilenceRemover\nfrom frontend.silence_remover import trim_silence\nfrom frontend.min_max_norm import MinMaxNormalisation\n#from frontend.acoustic_normalisation import CMPNormalisation\nfrom frontend.acoustic_composition import AcousticComposition\nfrom frontend.parameter_generation import ParameterGeneration\n#from frontend.feature_normalisation_base import FeatureNormBase\nfrom frontend.mean_variance_norm import MeanVarianceNorm\n\nfrom io_funcs.binary_io import  BinaryIOCollection\n\n# the new class for label composition and normalisation\nfrom frontend.label_composer import LabelComposer\n\nimport configuration\n\nfrom models.dnn import DNN\n#from models.ms_dnn import MultiStreamDNN\n#from models.ms_dnn_gv import MultiStreamDNNGv\n#from models.sdae import StackedDenoiseAutoEncoder\n\nfrom utils.compute_distortion import DistortionComputation, IndividualDistortionComp\nfrom utils.generate import generate_wav\nfrom utils.learn_rates import ExpDecreaseLearningRate\n\n\n#import matplotlib.pyplot as plt\n# our custom logging class that can also plot\n#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot\nfrom logplot.logging_plotting import LoggerPlotter, MultipleSeriesPlot, SingleWeightMatrixPlot\nimport logging # as logging\nimport logging.config\nimport io\n\n\ndef extract_file_id_list(file_list):\n    file_id_list = []\n    for file_name in file_list:\n        file_id = os.path.basename(os.path.splitext(file_name)[0])\n        file_id_list.append(file_id)\n\n    return  file_id_list\n\ndef read_file_list(file_name):\n\n    logger = logging.getLogger(""read_file_list"")\n\n    file_lists = []\n    fid = open(file_name)\n    for line in fid.readlines():\n        line = line.strip()\n        if len(line) < 1:\n            continue\n        file_lists.append(line)\n    fid.close()\n\n    logger.debug(\'Read file list from %s\' % file_name)\n    return  file_lists\n\n\ndef make_output_file_list(out_dir, in_file_lists):\n    out_file_lists = []\n\n    for in_file_name in in_file_lists:\n        file_id = os.path.basename(in_file_name)\n        out_file_name = out_dir + \'/\' + file_id\n        out_file_lists.append(out_file_name)\n\n    return  out_file_lists\n\ndef prepare_file_path_list(file_id_list, file_dir, file_extension, new_dir_switch=True):\n    if not os.path.exists(file_dir) and new_dir_switch:\n        os.makedirs(file_dir)\n    file_name_list = []\n    for file_id in file_id_list:\n        file_name = file_dir + \'/\' + file_id + file_extension\n        file_name_list.append(file_name)\n\n    return  file_name_list\n\n\n\ndef visualize_dnn(dnn):\n\n    layer_num = len(dnn.params) / 2     ## including input and output\n\n    for i in range(layer_num):\n        fig_name = \'Activation weights W\' + str(i)\n        fig_title = \'Activation weights of W\' + str(i)\n        xlabel = \'Neuron index of hidden layer \' + str(i)\n        ylabel = \'Neuron index of hidden layer \' + str(i+1)\n        if i == 0:\n            xlabel = \'Input feature index\'\n        if i == layer_num-1:\n            ylabel = \'Output feature index\'\n\n        logger.create_plot(fig_name, SingleWeightMatrixPlot)\n        plotlogger.add_plot_point(fig_name, fig_name, dnn.params[i*2].get_value(borrow=True).T)\n        plotlogger.save_plot(fig_name, title=fig_name, xlabel=xlabel, ylabel=ylabel)\n\n\n\ndef dnn_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n#    visualize_dnn(dbn)\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        features = features.reshape((-1, n_ins))\n        temp_set_x = features.tolist()\n        test_set_x = theano.shared(numpy.asarray(temp_set_x, dtype=theano.config.floatX))\n\n        predicted_parameter = dnn_model.parameter_prediction(test_set_x=test_set_x)\n#        predicted_parameter = test_out()\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\n##generate bottleneck layer as festures\ndef dnn_hidden_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        features = features.reshape((-1, n_ins))\n        temp_set_x = features.tolist()\n        test_set_x = theano.shared(numpy.asarray(temp_set_x, dtype=theano.config.floatX))\n\n        predicted_parameter = dnn_model.generate_top_hidden_layer(test_set_x=test_set_x)\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\n\ndef main_function(cfg, in_dir, out_dir):\n\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n    # get another logger to handle plotting duties\n    plotlogger = logging.getLogger(""plotting"")\n\n\n    #### parameter setting########\n    hidden_layers_sizes = cfg.hyper_params[\'hidden_layer_size\']\n\n    file_id_list = []\n\n    if cfg.label_style == \'HTS\':\n        ext = \'.lab\'\n    else:\n        ext = \'.utt\'\n\n    synth_utts = glob.glob(in_dir + \'/*\' + ext)\n    for fname in synth_utts:\n        junk,name = os.path.split(fname)\n        file_id_list.append(name.replace(ext,\'\'))\n\n    if not os.path.isdir(out_dir):\n        os.mkdir(out_dir)\n\n    ###total file number including training, development, and testing\n    #total_file_number = len(file_id_list)\n\n    data_dir = cfg.data_dir\n\n    #nn_cmp_dir       = os.path.join(data_dir, \'nn\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim))\n    #nn_cmp_norm_dir   = os.path.join(data_dir, \'nn_norm\'  + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim))\n\n    model_dir = os.path.join(cfg.work_dir, \'nnets_model\')\n    gen_dir   = os.path.join(out_dir, \'gen\')\n\n    #in_file_list_dict = {}\n\n    #for feature_name in cfg.in_dir_dict.keys():\n    #    in_file_list_dict[feature_name] = prepare_file_path_list(file_id_list, cfg.in_dir_dict[feature_name], cfg.file_extension_dict[feature_name], False)\n\n    #nn_cmp_file_list         = prepare_file_path_list(file_id_list, nn_cmp_dir, cfg.cmp_ext)\n    #nn_cmp_norm_file_list    = prepare_file_path_list(file_id_list, nn_cmp_norm_dir, cfg.cmp_ext)\n\n    ###normalisation information\n    norm_info_file = os.path.join(data_dir, \'norm_info\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim) + \'_\' + cfg.output_feature_normalisation + \'.dat\')\n\n    ### normalise input full context label\n\n    # currently supporting two different forms of lingustic features\n    # later, we should generalise this\n\n    if cfg.label_style == \'HTS\':\n        label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name)\n        lab_dim = label_normaliser.dimension\n        logger.info(\'Input label dimension is %d\' % lab_dim)\n        suffix=str(lab_dim)\n    # no longer supported - use new ""composed"" style labels instead\n    elif cfg.label_style == \'composed\':\n        # label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)\n        suffix=\'composed\'\n\n    # the number can be removed\n    binary_label_dir      = os.path.join(out_dir, \'lab_bin\')\n    nn_label_norm_dir     = os.path.join(out_dir, \'lab_bin_norm\')\n\n\n    in_label_align_file_list = prepare_file_path_list(file_id_list, in_dir, cfg.lab_ext)\n    binary_label_file_list   = prepare_file_path_list(file_id_list, binary_label_dir, cfg.lab_ext)\n    nn_label_norm_file_list  = prepare_file_path_list(file_id_list, nn_label_norm_dir, cfg.lab_ext)\n\n    ## need this to find normalisation info:\n    if cfg.process_labels_in_work_dir:\n        label_data_dir = cfg.work_dir\n    else:\n        label_data_dir = data_dir\n\n    min_max_normaliser = None\n    label_norm_file = \'label_norm_%s.dat\' %(cfg.label_style)\n    label_norm_file = os.path.join(label_data_dir, label_norm_file)\n\n    if cfg.label_style == \'HTS\':\n        # simple HTS labels\n        logger.info(\'preparing label data (input) using standard HTS style labels\')\n        label_normaliser.perform_normalisation(in_label_align_file_list, binary_label_file_list)\n\n    else:\n\n        logger.info(\'preparing label data (input) using ""composed"" style labels\')\n        label_composer = LabelComposer()\n        label_composer.load_label_configuration(cfg.label_config_file)\n\n        logger.info(\'Loaded label configuration\')\n        # logger.info(\'%s\' % label_composer.configuration.labels )\n\n        lab_dim=label_composer.compute_label_dimension()\n        logger.info(\'label dimension will be %d\' % lab_dim)\n\n        if cfg.precompile_xpaths:\n            label_composer.precompile_xpaths()\n\n        # there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)\n        # create all the lists of these, ready to pass to the label composer\n\n        in_label_align_file_list = {}\n        for label_style, label_style_required in label_composer.label_styles.items():\n            if label_style_required:\n                logger.info(\'labels of style %s are required - constructing file paths for them\' % label_style)\n                if label_style == \'xpath\':\n                    in_label_align_file_list[\'xpath\'] = prepare_file_path_list(file_id_list, in_dir, cfg.utt_ext, False)\n                elif label_style == \'hts\':\n                    logger.critical(\'script not tested with HTS labels\')\n                else:\n                    logger.critical(\'unsupported label style %s specified in label configuration\' % label_style)\n                    raise Exception\n\n            # now iterate through the files, one at a time, constructing the labels for them\n            num_files=len(file_id_list)\n            logger.info(\'the label styles required are %s\' % label_composer.label_styles)\n\n            for i in range(num_files):\n                logger.info(\'making input label features for %4d of %4d\' % (i+1,num_files))\n\n                # iterate through the required label styles and open each corresponding label file\n\n                # a dictionary of file descriptors, pointing at the required files\n                required_labels={}\n\n                for label_style, label_style_required in label_composer.label_styles.items():\n\n                    # the files will be a parallel set of files for a single utterance\n                    # e.g., the XML tree and an HTS label file\n                    if label_style_required:\n                        required_labels[label_style] = open(in_label_align_file_list[label_style][i] , \'r\')\n                        logger.debug(\' opening label file %s\' % in_label_align_file_list[label_style][i])\n\n                logger.debug(\'label styles with open files: %s\' % required_labels)\n                label_composer.make_labels(required_labels,out_file_name=binary_label_file_list[i],fill_missing_values=cfg.fill_missing_values,iterate_over_frames=cfg.iterate_over_frames)\n\n                # now close all opened files\n                for fd in required_labels.values():\n                    fd.close()\n\n\n    # no silence removal for synthesis ...\n\n    ## minmax norm:\n    min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99)\n\n    # reload stored minmax values: (TODO -- move reading and writing into MinMaxNormalisation class)\n    fid = open(label_norm_file, \'rb\')\n\n    ## This doesn\'t work -- precision is lost -- reads in as float64\n    #label_norm_info = numpy.fromfile(fid)  ## label_norm_info = numpy.array(label_norm_info, \'float32\')\n\n    ## use struct to enforce float32:\n    nbytes = os.stat(label_norm_file)[6]  # length in bytes\n    data = fid.read(nbytes)               # = read until bytes run out\n    fid.close()\n    m = nbytes / 4  ## number 32 bit floats\n    format = str(m)+""f""\n    label_norm_info = struct.unpack(format, data)\n    label_norm_info = numpy.array(label_norm_info)\n\n    min_max_normaliser.min_vector = label_norm_info[:m/2]\n    min_max_normaliser.max_vector = label_norm_info[m/2:]\n\n    ###  apply precompuated min-max to the whole dataset\n    min_max_normaliser.normalise_data(binary_label_file_list, nn_label_norm_file_list)\n\n\n\n    ### make output acoustic data\n#    if cfg.MAKECMP:\n\n    ### retrieve acoustic normalisation information for normalising the features back\n    var_dir   = os.path.join(data_dir, \'var\')\n    var_file_dict = {}\n    for feature_name in list(cfg.out_dimension_dict.keys()):\n        var_file_dict[feature_name] = os.path.join(var_dir, feature_name + \'_\' + str(cfg.out_dimension_dict[feature_name]))\n\n\n    ### normalise output acoustic data\n#    if cfg.NORMCMP:\n\n    combined_model_arch = str(len(hidden_layers_sizes))\n    for hid_size in hidden_layers_sizes:\n        combined_model_arch += \'_\' + str(hid_size)\n    nnets_file_name = \'%s/%s_%s_%d_%s_%d.%d.train.%d.model\' \\\n                      %(model_dir, cfg.model_type, cfg.combined_feature_name, int(cfg.multistream_switch),\n                        combined_model_arch, lab_dim, cfg.cmp_dim, cfg.train_file_number)\n\n    ### DNN model training\n#    if cfg.TRAINDNN:\n\n    ##if cfg.DNNGEN:\n    logger.info(\'generating from DNN\')\n\n    try:\n        os.makedirs(gen_dir)\n    except OSError as e:\n        if e.errno == errno.EEXIST:\n            # not an error - just means directory already exists\n            pass\n        else:\n            logger.critical(\'Failed to create generation directory %s\' % gen_dir)\n            logger.critical(\' OS error was: %s\' % e.strerror)\n            raise\n\n    gen_file_list = prepare_file_path_list(file_id_list, gen_dir, cfg.cmp_ext)\n\n    dnn_generation(nn_label_norm_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)\n\n    logger.debug(\'denormalising generated output using method %s\' % cfg.output_feature_normalisation)\n\n    fid = open(norm_info_file, \'rb\')\n    cmp_min_max = numpy.fromfile(fid, dtype=numpy.float32)\n    fid.close()\n    cmp_min_max = cmp_min_max.reshape((2, -1))\n    cmp_min_vector = cmp_min_max[0, ]\n    cmp_max_vector = cmp_min_max[1, ]\n\n    if cfg.output_feature_normalisation == \'MVN\':\n        denormaliser = MeanVarianceNorm(feature_dimension = cfg.cmp_dim)\n        denormaliser.feature_denormalisation(gen_file_list, gen_file_list, cmp_min_vector, cmp_max_vector)\n\n    elif cfg.output_feature_normalisation == \'MINMAX\':\n        denormaliser = MinMaxNormalisation(cfg.cmp_dim, min_value = 0.01, max_value = 0.99, min_vector = cmp_min_vector, max_vector = cmp_max_vector)\n        denormaliser.denormalise_data(gen_file_list, gen_file_list)\n    else:\n        logger.critical(\'denormalising method %s is not supported!\\n\' %(cfg.output_feature_normalisation))\n        raise\n\n    ##perform MLPG to smooth parameter trajectory\n    ## lf0 is included, the output features much have vuv.\n    generator = ParameterGeneration(gen_wav_features = cfg.gen_wav_features)\n    generator.acoustic_decomposition(gen_file_list, cfg.cmp_dim, cfg.out_dimension_dict, cfg.file_extension_dict, var_file_dict)\n\n\n    logger.info(\'Simple variance expansion\')\n    test_var_scaling=False\n    scaled_dir = gen_dir + \'_scaled\'\n    if test_var_scaling:\n        file_id_list = simple_scale_variance_CONTINUUM(gen_dir, scaled_dir, var_file_dict, cfg.out_dimension_dict, file_id_list)\n    else:\n        simple_scale_variance(gen_dir, scaled_dir, var_file_dict, cfg.out_dimension_dict, file_id_list, gv_weight=1.0)  ## gv_weight hard coded here!\n\n    ### generate wav ----\n    #if cfg.GENWAV:\n    logger.info(\'reconstructing waveform(s)\')\n    #generate_wav_glottHMM(scaled_dir, file_id_list)\n    generate_wav(scaled_dir, file_id_list, cfg)\n\n\ndef simple_scale_variance(indir, outdir, var_file_dict, out_dimension_dict, file_id_list, gv_weight=1.0):\n    ## simple variance scaling (silen et al. 2012, paragraph 3.1)\n    ## TODO: Lots of things like stream names hardcoded here; 3 for delta + delta-delta; ...\n#     all_streams = [\'cmp\',\'HNR\',\'F0\',\'LSF\',\'Gain\',\'LSFsource\']\n#     streams_to_scale = [\'LSF\']\n    all_streams = [\'cmp\',\'mgc\',\'lf0\',\'bap\']\n    streams_to_scale = [\'mgc\']\n\n    static_variances = {}\n\n    static_dimension_dict = {}\n    for (feature_name,size) in list(out_dimension_dict.items()):\n        static_dimension_dict[feature_name] = size/3\n\n    io_funcs = BinaryIOCollection()\n    for feature_name in list(var_file_dict.keys()):\n        var_values, dimension = io_funcs.load_binary_file_frame(var_file_dict[feature_name], 1)\n        static_var_values = var_values[:static_dimension_dict[feature_name], :]\n        static_variances[feature_name] = static_var_values\n\n    if not os.path.isdir(outdir):\n        os.makedirs(outdir)\n\n    assert gv_weight <= 1.0 and gv_weight >= 0.0\n    local_weight = 1.0 - gv_weight\n\n    for uttname in file_id_list:\n        for stream in all_streams:\n            infile = os.path.join(indir, uttname + \'.\' + stream)\n            outfile = os.path.join(outdir, uttname + \'.\' + stream)\n            if not os.path.isfile(infile):\n                sys.exit(infile + \' does not exist\')\n            if stream in streams_to_scale:\n                speech, dimension = io_funcs.load_binary_file_frame(infile, static_dimension_dict[stream])\n                utt_mean = numpy.mean(speech, axis=0)\n                utt_std =  numpy.std(speech, axis=0)\n\n                global_std = numpy.transpose((static_variances[stream]))\n                weighted_global_std = (gv_weight * global_std) + (local_weight * utt_std)\n                std_ratio = weighted_global_std / utt_std\n\n                nframes, ndim = numpy.shape(speech)\n                utt_mean_matrix = numpy.tile(utt_mean, (nframes,1))\n                std_ratio_matrix = numpy.tile(std_ratio, (nframes,1))\n\n                scaled_speech = ((speech - utt_mean_matrix) * std_ratio_matrix) + utt_mean_matrix\n                io_funcs.array_to_binary_file(scaled_speech, outfile)\n\n\n            else:\n                os.system(\'cp %s %s\'%(infile, outfile))\n\n\n\ndef simple_scale_variance_CONTINUUM(indir, outdir, var_file_dict, out_dimension_dict, file_id_list):\n    ## Try range of interpolation weights for combining global & local variance\n    all_streams = [\'cmp\',\'HNR\',\'F0\',\'LSF\',\'Gain\',\'LSFsource\']\n    streams_to_scale = [\'LSF\']\n\n    static_variances = {}\n\n    static_dimension_dict = {}\n    for (feature_name,size) in list(out_dimension_dict.items()):\n        static_dimension_dict[feature_name] = size/3\n\n    io_funcs = BinaryIOCollection()\n    for feature_name in list(var_file_dict.keys()):\n        var_values, dimension = io_funcs.load_binary_file_frame(var_file_dict[feature_name], 1)\n        static_var_values = var_values[:static_dimension_dict[feature_name], :]\n        static_variances[feature_name] = static_var_values\n\n    if not os.path.isdir(outdir):\n        os.makedirs(outdir)\n\n    file_id_list_out = []\n    for uttname in file_id_list:\n        for gv_weight in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n            local_weight = 1.0 - gv_weight\n            for stream in all_streams:\n                infile = os.path.join(indir, uttname + \'.\' + stream)\n                extended_uttname = uttname + \'_gv\' + str(gv_weight)\n                print(extended_uttname)\n                outfile = os.path.join(outdir, extended_uttname + \'.\' + stream)\n                if not os.path.isfile(infile):\n                    sys.exit(infile + \' does not exist\')\n                if stream in streams_to_scale:\n                    speech, dimension = io_funcs.load_binary_file_frame(infile, static_dimension_dict[stream])\n                    utt_mean = numpy.mean(speech, axis=0)\n                    utt_std =  numpy.std(speech, axis=0)\n\n                    global_std = numpy.transpose((static_variances[stream]))\n\n                    weighted_global_std = (gv_weight * global_std) + (local_weight * utt_std)\n\n                    std_ratio = weighted_global_std / utt_std\n\n                    nframes, ndim = numpy.shape(speech)\n                    utt_mean_matrix = numpy.tile(utt_mean, (nframes,1))\n                    std_ratio_matrix = numpy.tile(std_ratio, (nframes,1))\n\n                    scaled_speech = ((speech - utt_mean_matrix) * std_ratio_matrix) + utt_mean_matrix\n                    io_funcs.array_to_binary_file(scaled_speech, outfile)\n\n\n                else:\n                    os.system(\'cp %s %s\'%(infile, outfile))\n            file_id_list_out.append(extended_uttname)\n    return file_id_list_out\n\n\ndef log_to_hertz(infile, outfile):\n    f = open(infile, \'r\')\n    log_values = [float(val) for val in f.readlines()]\n    f.close()\n\n    def m2h(l):\n        h = math.exp(l)\n        return h\n\n    hertz = [m2h(l) for l in log_values]\n    f = open(outfile, \'w\')\n    for val in hertz:\n        if val > 0:\n            f.write(str(val) + \'\\n\')\n        else:\n            f.write(\'0.0\\n\')\n    f.close()\n\n\ndef generate_wav_glottHMM(gen_dir, gen_file_id_list):\n\n    x2x=\'~/repos/simple4all/CSTRVoiceClone/trunk/bin/x2x\'\n    synthesis=\'~/sim2/oliver/nst_repos/OSSIAN/ossian-v.1.3/tools/GlottHMM/Synthesis\'\n    general_glott_conf = \'~/sim2/oliver/nst_repos/OSSIAN/ossian-v.1.3/voices/en/ky_02_toy/english_gold_basic_glott_KY/processors/speech_feature_extractor/main_config.cfg\'\n    user_glott_conf = \'~/sim2/oliver/nst_repos/OSSIAN/ossian-v.1.3/voices/en/ky_02_toy/english_gold_basic_glott_KY/processors/speech_feature_extractor/user_config.cfg\'\n\n    exports = \'export LIBCONFIG_INSTALL_DIR=/afs/inf.ed.ac.uk/user/o/owatts/sim2/oliver/nst_repos/OSSIAN/ossian-v.1.3/tools/GlottHMM//libconfig-1.4.9 ; export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$LIBCONFIG_INSTALL_DIR/lib/.libs ; export LIBRARY_PATH=$LIBRARY_PATH:$LIBCONFIG_INSTALL_DIR/lib/.libs ; export CPATH=$CPATH:$LIBCONFIG_INSTALL_DIR/lib ;\'\n\n\n    streams = [\'cmp\',\'HNR\',\'F0\',\'LSF\',\'Gain\',\'LSFsource\']\n    for uttname in gen_file_id_list:\n        all_present = True\n        for stream in streams:\n            if not os.path.isfile(os.path.join(gen_dir, uttname + \'.\' + stream)):\n                all_present = False\n        if all_present:\n            for stream in streams:\n                extra = \'\'\n                if stream == \'F0\':\n                    extra = \'.NEGVALS\'\n                fname = os.path.join(gen_dir, uttname + \'.\' + stream)\n                fname_txt = os.path.join(gen_dir, uttname + \'.txt.\' + stream + extra)\n                comm = \'%s +fa %s > %s\'%(x2x, fname, fname_txt)\n                os.system(comm)\n            log_to_hertz(os.path.join(gen_dir, uttname + \'.txt.F0.NEGVALS\'), \\\n                                        os.path.join(gen_dir, uttname + \'.txt.F0\'))\n\n            stem_name = os.path.join(gen_dir, uttname + \'.txt\')\n            comm = \'%s %s %s %s %s\'%(exports, synthesis, stem_name, general_glott_conf, user_glott_conf)\n            print(comm)\n            os.system(comm)\n\n\n\n        else:\n            print(\'missing stream(s) for utterance \' + uttname)\n\n\n\nif __name__ == \'__main__\':\n\n\n\n    # these things should be done even before trying to parse the command line\n\n    # create a configuration instance\n    # and get a short name for this instance\n    cfg=configuration.cfg\n#\n#     # set up logging to use our custom class\n#     logging.setLoggerClass(LoggerPlotter)\n#\n#     # get a logger for this main function\n#     logger = logging.getLogger(""main"")\n\n\n    if len(sys.argv) != 4:\n        print(\'usage: run_dnn.sh config_file_name in_dir out_dir\')\n        #logger.critical(\'usage: run_dnn.sh config_file_name utt_dir\')\n        sys.exit(1)\n\n    config_file = sys.argv[1]\n    in_dir = sys.argv[2]\n    out_dir = sys.argv[3]\n\n    config_file = os.path.abspath(config_file)\n    cfg.configure(config_file)\n\n    main_function(cfg, in_dir, out_dir)\n\n    sys.exit(0)\n'"
src/work_in_progress/oliver/dnn_synth_PROJECTION.py,0,"b'\nimport pickle\nimport gzip\nimport os, sys, errno\nimport time\nimport math\nimport glob\nimport struct\n\nimport copy\n\nfrom lxml import etree\n\n#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)\nimport numpy\n# we need to explicitly import this in some cases, not sure why this doesn\'t get imported with numpy itself\nimport numpy.distutils.__config__\n# and only after that can we import theano\nimport theano\n\nfrom utils.providers import ListDataProviderWithProjectionIndex, expand_projection_inputs, get_unexpanded_projection_inputs # ListDataProvider\n\nfrom frontend.label_normalisation import HTSLabelNormalisation, XMLLabelNormalisation\nfrom frontend.silence_remover import SilenceRemover\nfrom frontend.silence_remover import trim_silence\nfrom frontend.min_max_norm import MinMaxNormalisation\n#from frontend.acoustic_normalisation import CMPNormalisation\nfrom frontend.acoustic_composition import AcousticComposition\nfrom frontend.parameter_generation import ParameterGeneration\n#from frontend.feature_normalisation_base import FeatureNormBase\nfrom frontend.mean_variance_norm import MeanVarianceNorm\n\nfrom io_funcs.binary_io import  BinaryIOCollection\n\n# the new class for label composition and normalisation\nfrom frontend.label_composer import LabelComposer\n\nimport configuration\n\nfrom models.dnn import DNN\nfrom models.tpdnn import TokenProjectionDNN\nfrom models.ms_dnn import MultiStreamDNN\nfrom models.ms_dnn_gv import MultiStreamDNNGv\nfrom models.sdae import StackedDenoiseAutoEncoder\n\nfrom utils.compute_distortion import DistortionComputation, IndividualDistortionComp\nfrom utils.generate import generate_wav\nfrom utils.learn_rates import ExpDecreaseLearningRate\n\n\n#import matplotlib.pyplot as plt\n# our custom logging class that can also plot\n#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot\nfrom logplot.logging_plotting import LoggerPlotter, MultipleSeriesPlot, SingleWeightMatrixPlot\nimport logging # as logging\nimport logging.config\nimport io\n\n\n\n\n## This should always be True -- tidy up later\nexpand_by_minibatch = True\n\nif expand_by_minibatch:\n    proj_type = \'int32\'\nelse:\n    proj_type = theano.config.floatX\n\n\n\n\ndef extract_file_id_list(file_list):\n    file_id_list = []\n    for file_name in file_list:\n        file_id = os.path.basename(os.path.splitext(file_name)[0])\n        file_id_list.append(file_id)\n\n    return  file_id_list\n\ndef read_file_list(file_name):\n\n    logger = logging.getLogger(""read_file_list"")\n\n    file_lists = []\n    fid = open(file_name)\n    for line in fid.readlines():\n        line = line.strip()\n        if len(line) < 1:\n            continue\n        file_lists.append(line)\n    fid.close()\n\n    logger.debug(\'Read file list from %s\' % file_name)\n    return  file_lists\n\n\ndef make_output_file_list(out_dir, in_file_lists):\n    out_file_lists = []\n\n    for in_file_name in in_file_lists:\n        file_id = os.path.basename(in_file_name)\n        out_file_name = out_dir + \'/\' + file_id\n        out_file_lists.append(out_file_name)\n\n    return  out_file_lists\n\ndef prepare_file_path_list(file_id_list, file_dir, file_extension, new_dir_switch=True):\n    if not os.path.exists(file_dir) and new_dir_switch:\n        os.makedirs(file_dir)\n    file_name_list = []\n    for file_id in file_id_list:\n        file_name = file_dir + \'/\' + file_id + file_extension\n        file_name_list.append(file_name)\n\n    return  file_name_list\n\n\n\ndef visualize_dnn(dnn):\n\n    layer_num = len(dnn.params) / 2     ## including input and output\n\n    for i in range(layer_num):\n        fig_name = \'Activation weights W\' + str(i)\n        fig_title = \'Activation weights of W\' + str(i)\n        xlabel = \'Neuron index of hidden layer \' + str(i)\n        ylabel = \'Neuron index of hidden layer \' + str(i+1)\n        if i == 0:\n            xlabel = \'Input feature index\'\n        if i == layer_num-1:\n            ylabel = \'Output feature index\'\n\n        logger.create_plot(fig_name, SingleWeightMatrixPlot)\n        plotlogger.add_plot_point(fig_name, fig_name, dnn.params[i*2].get_value(borrow=True).T)\n        plotlogger.save_plot(fig_name, title=fig_name, xlabel=xlabel, ylabel=ylabel)\n\n\n\n\n\ndef infer_projections(train_xy_file_list, valid_xy_file_list, \\\n              nnets_file_name, n_ins, n_outs, ms_outs, hyper_params, buffer_size, plot=False):\n\n    \'\'\'\n    Unlike the same function in run_tpdnn.py this *DOESN\'T* save model at the\n    end -- just returns array of the learned projection weights\n    \'\'\'\n\n    ####parameters#####\n    finetune_lr     = float(hyper_params[\'learning_rate\'])\n    training_epochs = int(hyper_params[\'training_epochs\'])\n    batch_size      = int(hyper_params[\'batch_size\'])\n    l1_reg          = float(hyper_params[\'l1_reg\'])\n    l2_reg          = float(hyper_params[\'l2_reg\'])\n    private_l2_reg  = float(hyper_params[\'private_l2_reg\'])\n    warmup_epoch    = int(hyper_params[\'warmup_epoch\'])\n    momentum        = float(hyper_params[\'momentum\'])\n    warmup_momentum = float(hyper_params[\'warmup_momentum\'])\n\n    hidden_layers_sizes = hyper_params[\'hidden_layers_sizes\']\n\n    stream_weights       = hyper_params[\'stream_weights\']\n    private_hidden_sizes = hyper_params[\'private_hidden_sizes\']\n\n    buffer_utt_size = buffer_size\n    early_stop_epoch = int(hyper_params[\'early_stop_epochs\'])\n\n    hidden_activation = hyper_params[\'hidden_activation\']\n    output_activation = hyper_params[\'output_activation\']\n\n    stream_lr_weights = hyper_params[\'stream_lr_weights\']\n    use_private_hidden = hyper_params[\'use_private_hidden\']\n\n    model_type = hyper_params[\'model_type\']\n\n    index_to_project = hyper_params[\'index_to_project\']\n    projection_insize = hyper_params[\'projection_insize\']\n    projection_outsize = hyper_params[\'projection_outsize\']\n\n    ######### data providers ##########\n    (train_x_file_list, train_y_file_list) = train_xy_file_list\n    (valid_x_file_list, valid_y_file_list) = valid_xy_file_list\n\n    logger.debug(\'Creating training   data provider\')\n    train_data_reader = ListDataProviderWithProjectionIndex(x_file_list = train_x_file_list, y_file_list = train_y_file_list, n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, shuffle = True, index_to_project=index_to_project, projection_insize=projection_insize, indexes_only=expand_by_minibatch)\n\n    logger.debug(\'Creating validation data provider\')\n    valid_data_reader = ListDataProviderWithProjectionIndex(x_file_list = valid_x_file_list, y_file_list = valid_y_file_list, n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, shuffle = False, index_to_project=index_to_project, projection_insize=projection_insize, indexes_only=expand_by_minibatch)\n\n    shared_train_set_xy, temp_train_set_x, temp_train_set_x_proj, temp_train_set_y = train_data_reader.load_next_partition_with_projection()\n    train_set_x, train_set_x_proj, train_set_y = shared_train_set_xy\n    shared_valid_set_xy, temp_valid_set_x, temp_valid_set_x_proj, temp_valid_set_y = valid_data_reader.load_next_partition_with_projection()\n    valid_set_x, valid_set_x_proj, valid_set_y = shared_valid_set_xy\n    train_data_reader.reset()\n    valid_data_reader.reset()\n    ####################################\n\n\n    # numpy random generator\n    numpy_rng = numpy.random.RandomState(123)\n    logger.info(\'building the model\')\n\n\n    ############## load existing dnn #####\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n    train_all_fn, train_subword_fn, train_word_fn, infer_projections_fn, valid_fn, valid_score_i = \\\n                    dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_x_proj, train_set_y),\n                    (valid_set_x, valid_set_x_proj, valid_set_y), batch_size=batch_size)\n    ####################################\n\n    logger.info(\'fine-tuning the %s model\' %(model_type))\n\n    start_time = time.clock()\n\n    best_dnn_model = dnn_model\n    best_validation_loss = sys.float_info.max\n    previous_loss = sys.float_info.max\n\n    early_stop = 0\n    epoch = 0\n    previous_finetune_lr = finetune_lr\n\n    logger.info(\'fine-tuning the %s model\' %(model_type))\n\n\n\n    dnn_model.initialise_projection_weights()\n\n    inference_epochs = 20  ## <-------- hard coded !!!!!!!!!!\n\n    current_finetune_lr = previous_finetune_lr = finetune_lr\n    warmup_epoch_3 = 10 # 10  ## <-------- hard coded !!!!!!!!!!\n\n    #warmup_epoch_3 = epoch + warmup_epoch_3\n    #inference_epochs += epoch\n    while (epoch < inference_epochs):\n\n        epoch = epoch + 1\n\n        current_momentum = momentum\n\n        if epoch > warmup_epoch_3:\n            previous_finetune_lr = current_finetune_lr\n            current_finetune_lr = previous_finetune_lr * 0.5\n\n\n\n        dev_error = []\n        sub_start_time = time.clock()\n\n        ## osw -- inferring word reps on validation set in a forward pass in a single batch\n        ##        exausts memory when using 20k projected vocab -- also use minibatches\n        logger.debug(\'infer word representations for validation set\')\n        valid_error = []\n        n_valid_batches = valid_set_x.get_value().shape[0] / batch_size\n        for minibatch_index in range(n_valid_batches):\n            v_loss = infer_projections_fn(minibatch_index, current_finetune_lr, current_momentum)\n            valid_error.append(v_loss)\n\n        this_validation_loss = numpy.mean(valid_error)\n\n\n        #valid_error = infer_projections_fn(current_finetune_lr, current_momentum)\n        #this_validation_loss = numpy.mean(valid_error)\n\n#        if plot:\n#            ## add dummy validation loss so that plot works:\n#            plotlogger.add_plot_point(\'training convergence\',\'validation set\',(epoch,this_validation_loss))\n#            plotlogger.add_plot_point(\'training convergence\',\'training set\',(epoch,this_train_valid_loss))\n#\n\n        sub_end_time = time.clock()\n\n\n        logger.info(\'INFERENCE epoch %i, validation error %f, time spent %.2f\' %(epoch, this_validation_loss, (sub_end_time - sub_start_time)))\n\n\n#        if cfg.hyper_params[\'model_type\'] == \'TPDNN\':\n#            if not os.path.isdir(cfg.projection_weights_output_dir):\n#                os.mkdir(cfg.projection_weights_output_dir)\n#            weights = dnn_model.get_projection_weights()\n#            fname = os.path.join(cfg.projection_weights_output_dir, \'proj_INFERENCE_epoch_%s\'%(epoch))\n#            numpy.savetxt(fname, weights)\n#\n\n        best_dnn_model = dnn_model  ## always update\n\n    end_time = time.clock()\n    ##cPickle.dump(best_dnn_model, open(nnets_file_name, \'wb\'))\n    final_weights = dnn_model.get_projection_weights()\n\n\n    logger.info(\'overall  training time: %.2fm validation error %f\' % ((end_time - start_time) / 60., best_validation_loss))\n\n#    if plot:\n#        plotlogger.save_plot(\'training convergence\',title=\'Final training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n#\n\n    ### ========================================================\n\n\n\n\n#    if cfg.hyper_params[\'model_type\'] == \'TPDNN\':\n#        os.system(\'python %s %s\'%(\'/afs/inf.ed.ac.uk/user/o/owatts/scripts_NEW/plot_weights_multiple_phases.py\', cfg.projection_weights_output_dir))\n\n    return  final_weights\n\n\ndef dnn_generation_PROJECTION(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list, cfg=None, synth_mode=\'constant\', projection_end=0, projection_weights_to_use=None, save_weights_to_file=None):\n    \'\'\'\n    Use the (training/dev/test) projections learned in training, but shuffled, for test tokens.\n\n    -- projection_end is *real* value for last projection index (or some lower value)\n    -- this is so the samples / means are of real values learned on training data\n    \'\'\'\n\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation_PROJECTION\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    ## \'remove\' word representations by randomising them. As model is unpickled and\n    ## not re-saved, this does not throw trained parameters away.\n\n    if synth_mode == \'sampled_training\':\n        ## use randomly chosen training projection -- shuffle in-place = same as sampling wihtout replacement\n        P = dnn_model.get_projection_weights()\n        numpy.random.shuffle(P[:,:projection_end])  ## shuffle in place along 1st dim (reorder rows)\n        dnn_model.params[0].set_value(P, borrow=True)\n    elif synth_mode == \'uniform\':\n        ##  generate utt embeddings uniformly at random within the min-max of the training set (i.e. from a (hyper)-rectangle)\n        P = dnn_model.get_projection_weights()\n\n        column_min = numpy.min(P[:,:projection_end], axis=0)  ## vector like a row of P with min of its columns\n        column_max = numpy.max(P[:,:projection_end], axis=0)\n\n        random_proj = numpy.random.uniform(low=column_min, high=column_max, size=numpy.shape(P))\n        random_proj = random_proj.astype(numpy.float32)\n\n        dnn_model.params[0].set_value(random_proj, borrow=True)\n\n    elif synth_mode == \'constant\':\n        ## use mean projection\n        P = dnn_model.get_projection_weights()\n        mean_row = P[:,:projection_end].mean(axis=0)\n        print(\'mean row used for projection:\')\n        print(mean_row)\n        P = numpy.ones(numpy.shape(P), dtype=numpy.float32) * mean_row   ## stack mean rows\n        dnn_model.params[0].set_value(P, borrow=True)\n    elif synth_mode == \'inferred\':\n        ## DEBUG\n        assert projection_weights_to_use != None\n        old_weights = dnn_model.get_projection_weights()\n        ## DEBUG:=========\n        #projection_weights_to_use = old_weights # numpy.array(numpy.random.uniform(low=-0.3, high=0.3, size=numpy.shape(old_weights)),  dtype=numpy.float32)\n        ## =============\n        assert numpy.shape(old_weights) == numpy.shape(projection_weights_to_use),  [numpy.shape(old_weights), numpy.shape(projection_weights_to_use)]\n        dnn_model.params[0].set_value(projection_weights_to_use, borrow=True)\n\n    elif synth_mode == \'single_sentence_demo\':\n        ##  generate utt embeddings from a uniform 10 x 10 grid within the min-max of the training set (i.e. from a rectangle)\n        P = dnn_model.get_projection_weights()\n\n        column_min = numpy.min(P[:,:projection_end], axis=0)  ## vector like a row of P with min of its columns\n        column_max = numpy.max(P[:,:projection_end], axis=0)\n        assert len(column_min) == 2, \'Only 2D projections supported in mode single_sentence_demo\'\n\n        ranges = column_max - column_min\n        nstep = 10\n        steps = ranges / (nstep-1)\n\n        grid_params = [numpy.array([1.0, 1.0])]  ## pading to handle 0 index (reserved for defaults)\n        for x in range(nstep):\n            for y in range(nstep):\n                grid_params.append( column_min + (numpy.array([x, y]) * steps) )\n        stacked_params = numpy.vstack(grid_params)\n        print(stacked_params)\n        print(numpy.shape(stacked_params))\n        print()\n        print()\n\n\n        proj = numpy.ones(numpy.shape(P))\n        proj[:101, :] = stacked_params\n\n        proj = proj.astype(numpy.float32)\n\n        dnn_model.params[0].set_value(proj, borrow=True)\n\n    elif  synth_mode == \'uniform_sampled_within_std_1\':\n        ## points uniformly sampled from between the 1.8 - 2.0 stds of a diagonal covariance gaussian fitted to the data\n        P = dnn_model.get_projection_weights()\n\n        column_min = numpy.min(P[:,:projection_end], axis=0)  ## vector like a row of P with min of its columns\n        column_max = numpy.max(P[:,:projection_end], axis=0)\n\n        std_val = numpy.std(P[:,:projection_end], axis=0)\n\n        dots = numpy.random.uniform(low=column_min, high=column_max, size=(100000, 2))\n        dots = within_circle(dots, radius=std_val*2.0)\n        dots = outside_circle(dots, radius=std_val*1.8)\n\n        m,n = numpy.shape(P)\n        dots = dots[:m, :]\n\n        dots = dots.astype(numpy.float32)\n        dnn_model.params[0].set_value(dots, borrow=True)\n\n\n    elif  synth_mode == \'uniform_sampled_within_std_2\':\n        ## points uniformly sampled from between the 1.8 - 2.0 stds of a diagonal covariance gaussian fitted to the data\n        P = dnn_model.get_projection_weights()\n\n        column_min = numpy.min(P[:,:projection_end], axis=0)  ## vector like a row of P with min of its columns\n        column_max = numpy.max(P[:,:projection_end], axis=0)\n\n        std_val = numpy.std(P[:,:projection_end], axis=0)\n\n        dots = numpy.random.uniform(low=column_min, high=column_max, size=(100000, 2))\n        dots = within_circle(dots, radius=std_val*3.0)\n        dots = outside_circle(dots, radius=std_val*2.8)\n\n        m,n = numpy.shape(P)\n        dots = dots[:m, :]\n\n        dots = dots.astype(numpy.float32)\n        dnn_model.params[0].set_value(dots, borrow=True)\n\n\n    elif  synth_mode == \'uniform_sampled_within_std_3\':\n        ## points uniformly sampled from between the 1.8 - 2.0 stds of a diagonal covariance gaussian fitted to the data\n        P = dnn_model.get_projection_weights()\n\n        column_min = numpy.min(P[:,:projection_end], axis=0)  ## vector like a row of P with min of its columns\n        column_max = numpy.max(P[:,:projection_end], axis=0)\n\n        std_val = numpy.std(P[:,:projection_end], axis=0)\n\n        dots = numpy.random.uniform(low=column_min, high=column_max, size=(100000, 2))\n        dots = within_circle(dots, radius=std_val*4.0)\n        dots = outside_circle(dots, radius=std_val*3.8)\n\n        m,n = numpy.shape(P)\n        dots = dots[:m, :]\n\n        dots = dots.astype(numpy.float32)\n        dnn_model.params[0].set_value(dots, borrow=True)\n\n    else:\n        sys.exit(\'unknow mode: %s\'%(synth_mode))\n\n    ##  save used weights for future reference:\n    if save_weights_to_file:\n        weights = dnn_model.get_projection_weights()\n        numpy.savetxt(save_weights_to_file, weights)\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        features = features.reshape((-1, n_ins))\n\n        #features, features_proj = expand_projection_inputs(features, cfg.index_to_project, \\\n        #                                                         cfg.projection_insize)\n        features, features_proj = get_unexpanded_projection_inputs(features, cfg.index_to_project, \\\n                                                                 cfg.projection_insize)\n        #temp_set_x = features.tolist()  ## osw - why list conversion necessary?\n        test_set_x = theano.shared(numpy.asarray(features, dtype=theano.config.floatX))\n        test_set_x_proj = theano.shared(numpy.asarray(features_proj, dtype=\'int32\'))\n\n        predicted_parameter = dnn_model.parameter_prediction(test_set_x=test_set_x, test_set_x_proj=test_set_x_proj)\n#        predicted_parameter = test_out()\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\n\n\n## define a couple of functions for circular rejection sampling:\ndef within_circle(dots, radius=1.0):\n    standardised_dots = (dots - numpy.mean(dots)) / radius\n    ## if x^2 + y^2 <= 1, point is within unit circle\n    within_circle = (standardised_dots[:,0]*standardised_dots[:,0]) + (standardised_dots[:,1]*standardised_dots[:,1]) <= 1.0\n    return dots[within_circle]\n##\ndef outside_circle(dots, radius=1.0):\n    standardised_dots = (dots - numpy.mean(dots)) / radius\n    ## if x^2 + y^2 <= 1, point is within unit circle\n    within_circle = (standardised_dots[:,0]*standardised_dots[:,0]) + (standardised_dots[:,1]*standardised_dots[:,1]) > 1.0\n    return dots[within_circle]\n\n\n##generate bottleneck layer as festures\ndef dnn_hidden_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        features = features.reshape((-1, n_ins))\n        temp_set_x = features.tolist()\n        test_set_x = theano.shared(numpy.asarray(temp_set_x, dtype=theano.config.floatX))\n\n        predicted_parameter = dnn_model.generate_top_hidden_layer(test_set_x=test_set_x)\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\n\ndef add_projection_indices(uttlist, token_xpath, attrib_name, outdir):\n    ## Taken from: ~/proj/dnn_tts/script/add_token_index.py\n    \'\'\'\n    For utts in uttlist, add attribute called <attrib_name> to all nodes\n    matching <token_xpath> with a corpus-unique integer value > 0. Add default\n    0-valued attrib at root node.\n    \'\'\'\n    i = 1\n    for uttfile in uttlist:\n        utt = etree.parse(uttfile)\n        ## clear target attribute name from all nodes to be safe:\n        for node in utt.xpath(\'//*\'): ## all nodes\n            if attrib_name in node.attrib:\n                del node.attrib[attrib_name]\n        root_node = utt.getroot()\n        root_node.attrib[attrib_name] = \'0\'   ## 0 is the defualt \'n/a\' value -- *some* ancestor of all nodes will have the relevant attibute to fall back on\n        for node in utt.xpath(token_xpath):\n            node.attrib[attrib_name] = str(i)\n            i += 1\n        junk,fname = os.path.split(uttfile)\n        outfile = os.path.join(outdir, fname)\n        utt.write(outfile, encoding=\'utf-8\', pretty_print=True)\n\ndef add_projection_indices_with_replicates(uttlist, token_xpath, attrib_name, outdir, nreplicates):\n    ## Taken from: ~/proj/dnn_tts/script/add_token_index.py\n    \'\'\'\n    For utts in uttlist, add attribute called <attrib_name> to all nodes\n    matching <token_xpath> with a corpus-unique integer value > 0. Add default\n    0-valued attrib at root node.\n    \'\'\'\n    assert len(uttlist) == 1\n    uttfile = uttlist[0]\n\n    i = 1\n\n    master_utt = etree.parse(uttfile)\n\n    new_utt_names = []\n\n    while i < nreplicates + 1:\n\n        utt = copy.copy(master_utt)\n\n        ## clear target attribute name from all nodes to be safe:\n        for node in utt.xpath(\'//*\'): ## all nodes\n            if attrib_name in node.attrib:\n                del node.attrib[attrib_name]\n        root_node = utt.getroot()\n        root_node.attrib[attrib_name] = \'0\'   ## 0 is the defualt \'n/a\' value -- *some* ancestor of all nodes will have the relevant attibute to fall back on\n        assert len(utt.xpath(token_xpath)) == 1\n        for node in utt.xpath(token_xpath):\n            node.attrib[attrib_name] = str(i)\n        junk,fname = os.path.split(uttfile)\n        new_utt_name = fname.replace(\'.utt\', \'_rep_%s.utt\'%(i))\n        new_utt_names.append(new_utt_name)\n        outfile = os.path.join(outdir, new_utt_name)\n        utt.write(outfile, encoding=\'utf-8\', pretty_print=True)\n        i += 1\n\n    return new_utt_names\n\n\ndef retrieve_normalisation_values(norm_file):\n    ## TODO -- move reading and writing into MinMaxNormalisation class\n\n    if not os.path.isfile(norm_file):\n        sys.exit(\'Normalisation file %s does not exist \'%(norm_file))\n\n    # reload stored minmax values:\n    fid = open(norm_file, \'rb\')\n\n    ## This doesn\'t work -- precision is lost -- reads in as float64\n    #label_norm_info = numpy.fromfile(fid)  ## label_norm_info = numpy.array(label_norm_info, \'float32\')\n\n    ## use struct to enforce float32:\n    nbytes = os.stat(norm_file)[6]  # length in bytes\n    data = fid.read(nbytes)               # = read until bytes run out\n    fid.close()\n    m = nbytes / 4  ## number of 32 bit floats\n    format = str(m)+""f""\n    label_norm_info = struct.unpack(format, data)\n    label_norm_info = numpy.array(label_norm_info)\n\n    ## values can be min + max or mean + std, hence non-descript variable names:\n    first_vector = label_norm_info[:m/2]\n    second_vector = label_norm_info[m/2:]\n\n    return (first_vector, second_vector)\n\n\ndef main_function(cfg, in_dir, out_dir, token_xpath, index_attrib_name, synth_mode, cmp_dir, projection_end):\n    ## TODO: token_xpath & index_attrib_name   should be in config\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n    # get another logger to handle plotting duties\n    plotlogger = logging.getLogger(""plotting"")\n\n    # later, we might do this via a handler that is created, attached and configured\n    # but for now we need to do it manually\n    plotlogger.set_plot_path(cfg.plot_dir)\n\n    #### parameter setting########\n    hidden_layers_sizes = cfg.hyper_params[\'hidden_layers_sizes\']\n\n    ####prepare environment\n    synth_utts_input = glob.glob(in_dir + \'/*.utt\')\n    ###synth_utts_input = synth_utts_input[:10]   ### temp!!!!!\n\n    if synth_mode == \'single_sentence_demo\':\n        synth_utts_input = synth_utts_input[:1]\n        print()\n        print(\'mode: single_sentence_demo\')\n        print(synth_utts_input)\n        print()\n\n    indexed_utt_dir = os.path.join(out_dir, \'utt\') ## place to put test utts with tokens labelled with projection indices\n    direcs = [out_dir, indexed_utt_dir]\n    for direc in direcs:\n        if not os.path.isdir(direc):\n            os.mkdir(direc)\n\n\n    ## was below -- see comment\n    if synth_mode == \'single_sentence_demo\':\n        synth_utts_input = add_projection_indices_with_replicates(synth_utts_input, token_xpath, index_attrib_name, indexed_utt_dir, 100)\n    else:\n        add_projection_indices(synth_utts_input, token_xpath, index_attrib_name, indexed_utt_dir)\n\n\n\n\n    file_id_list = []\n    for fname in synth_utts_input:\n        junk,name = os.path.split(fname)\n        file_id_list.append(name.replace(\'.utt\',\'\'))\n\n\n    data_dir = cfg.data_dir\n\n    model_dir = os.path.join(cfg.work_dir, \'nnets_model\')\n    gen_dir   = os.path.join(out_dir, \'gen\')\n\n    ###normalisation information\n    norm_info_file = os.path.join(data_dir, \'norm_info\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim) + \'_\' + cfg.output_feature_normalisation + \'.dat\')\n\n    ### normalise input full context label\n    if cfg.label_style == \'HTS\':\n        sys.exit(\'only ossian utts supported\')\n    elif cfg.label_style == \'composed\':\n        suffix=\'composed\'\n\n    # the number can be removed\n    binary_label_dir      = os.path.join(out_dir, \'lab_bin\')\n    nn_label_norm_dir     = os.path.join(out_dir, \'lab_bin_norm\')\n\n    binary_label_file_list   = prepare_file_path_list(file_id_list, binary_label_dir, cfg.lab_ext)\n    nn_label_norm_file_list  = prepare_file_path_list(file_id_list, nn_label_norm_dir, cfg.lab_ext)\n\n    ## need this to find normalisation info:\n    if cfg.process_labels_in_work_dir:\n        label_data_dir = cfg.work_dir\n    else:\n        label_data_dir = data_dir\n\n    min_max_normaliser = None\n    label_norm_file = \'label_norm_%s.dat\' %(cfg.label_style)\n    label_norm_file = os.path.join(label_data_dir, label_norm_file)\n\n    if cfg.label_style == \'HTS\':\n        sys.exit(\'script not tested with HTS labels\')\n\n\n    ## always do this in synth:\n    ## if cfg.NORMLAB and (cfg.label_style == \'composed\'):\n    logger.info(\'add projection indices to tokens in test utts\')\n\n    ## add_projection_indices was here\n\n    logger.info(\'preparing label data (input) using ""composed"" style labels\')\n    label_composer = LabelComposer()\n    label_composer.load_label_configuration(cfg.label_config_file)\n\n    logger.info(\'Loaded label configuration\')\n\n    lab_dim=label_composer.compute_label_dimension()\n    logger.info(\'label dimension will be %d\' % lab_dim)\n\n    if cfg.precompile_xpaths:\n        label_composer.precompile_xpaths()\n\n    # there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)\n    # create all the lists of these, ready to pass to the label composer\n\n    in_label_align_file_list = {}\n    for label_style, label_style_required in label_composer.label_styles.items():\n        if label_style_required:\n            logger.info(\'labels of style %s are required - constructing file paths for them\' % label_style)\n            if label_style == \'xpath\':\n                in_label_align_file_list[\'xpath\'] = prepare_file_path_list(file_id_list, indexed_utt_dir, cfg.utt_ext, False)\n            elif label_style == \'hts\':\n                logger.critical(\'script not tested with HTS labels\')\n            else:\n                logger.critical(\'unsupported label style %s specified in label configuration\' % label_style)\n                raise Exception\n\n        # now iterate through the files, one at a time, constructing the labels for them\n        num_files=len(file_id_list)\n        logger.info(\'the label styles required are %s\' % label_composer.label_styles)\n\n        for i in range(num_files):\n            logger.info(\'making input label features for %4d of %4d\' % (i+1,num_files))\n\n            # iterate through the required label styles and open each corresponding label file\n\n            # a dictionary of file descriptors, pointing at the required files\n            required_labels={}\n\n            for label_style, label_style_required in label_composer.label_styles.items():\n\n                # the files will be a parallel set of files for a single utterance\n                # e.g., the XML tree and an HTS label file\n                if label_style_required:\n                    required_labels[label_style] = open(in_label_align_file_list[label_style][i] , \'r\')\n                    logger.debug(\' opening label file %s\' % in_label_align_file_list[label_style][i])\n\n            logger.debug(\'label styles with open files: %s\' % required_labels)\n            label_composer.make_labels(required_labels,out_file_name=binary_label_file_list[i],fill_missing_values=cfg.fill_missing_values,iterate_over_frames=cfg.iterate_over_frames)\n\n            # now close all opened files\n            for fd in required_labels.values():\n                fd.close()\n\n    # no silence removal for synthesis ...\n\n    ## minmax norm:\n    min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99, exclude_columns=[cfg.index_to_project])\n\n    (min_vector, max_vector) = retrieve_normalisation_values(label_norm_file)\n    min_max_normaliser.min_vector = min_vector\n    min_max_normaliser.max_vector = max_vector\n\n    ###  apply precompuated and stored min-max to the whole dataset\n    min_max_normaliser.normalise_data(binary_label_file_list, nn_label_norm_file_list)\n\n\n### DEBUG\n    if synth_mode == \'inferred\':\n\n        ## set up paths -- write CMP data to infer from in outdir:\n        nn_cmp_dir = os.path.join(out_dir, \'nn\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim))\n        nn_cmp_norm_dir = os.path.join(out_dir, \'nn_norm\'  + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim))\n\n        in_file_list_dict = {}\n        for feature_name in list(cfg.in_dir_dict.keys()):\n            in_direc = os.path.join(cmp_dir, feature_name)\n            assert os.path.isdir(in_direc), in_direc\n            in_file_list_dict[feature_name] = prepare_file_path_list(file_id_list, in_direc, cfg.file_extension_dict[feature_name], False)\n\n        nn_cmp_file_list         = prepare_file_path_list(file_id_list, nn_cmp_dir, cfg.cmp_ext)\n        nn_cmp_norm_file_list    = prepare_file_path_list(file_id_list, nn_cmp_norm_dir, cfg.cmp_ext)\n\n\n\n        ### make output acoustic data\n        #    if cfg.MAKECMP:\n        logger.info(\'creating acoustic (output) features\')\n        delta_win = [-0.5, 0.0, 0.5]\n        acc_win = [1.0, -2.0, 1.0]\n\n        acoustic_worker = AcousticComposition(delta_win = delta_win, acc_win = acc_win)\n        acoustic_worker.prepare_nn_data(in_file_list_dict, nn_cmp_file_list, cfg.in_dimension_dict, cfg.out_dimension_dict)\n\n        ## skip silence removal for inference -- need to match labels, which are\n        ## not silence removed either\n\n\n\n    ### retrieve acoustic normalisation information for normalising the features back\n    var_dir   = os.path.join(data_dir, \'var\')\n    var_file_dict = {}\n    for feature_name in list(cfg.out_dimension_dict.keys()):\n        var_file_dict[feature_name] = os.path.join(var_dir, feature_name + \'_\' + str(cfg.out_dimension_dict[feature_name]))\n\n\n    ### normalise output acoustic data\n#    if cfg.NORMCMP:\n\n\n#### DEBUG\n    if synth_mode == \'inferred\':\n\n\n        logger.info(\'normalising acoustic (output) features using method %s\' % cfg.output_feature_normalisation)\n        cmp_norm_info = None\n        if cfg.output_feature_normalisation == \'MVN\':\n            normaliser = MeanVarianceNorm(feature_dimension=cfg.cmp_dim)\n\n            (mean_vector,std_vector) = retrieve_normalisation_values(norm_info_file)\n            normaliser.mean_vector = mean_vector\n            normaliser.std_vector = std_vector\n\n            ###  apply precompuated and stored mean and std to the whole dataset\n            normaliser.feature_normalisation(nn_cmp_file_list, nn_cmp_norm_file_list)\n\n        elif cfg.output_feature_normalisation == \'MINMAX\':\n            sys.exit(\'not implemented\')\n            #            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim)\n            #            global_mean_vector = min_max_normaliser.compute_mean(nn_cmp_file_list[0:cfg.train_file_number])\n            #            global_std_vector = min_max_normaliser.compute_std(nn_cmp_file_list[0:cfg.train_file_number], global_mean_vector)\n\n            #            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim, min_value = 0.01, max_value = 0.99)\n            #            min_max_normaliser.find_min_max_values(nn_cmp_file_list[0:cfg.train_file_number])\n            #            min_max_normaliser.normalise_data(nn_cmp_file_list, nn_cmp_norm_file_list)\n\n            #            cmp_min_vector = min_max_normaliser.min_vector\n            #            cmp_max_vector = min_max_normaliser.max_vector\n            #            cmp_norm_info = numpy.concatenate((cmp_min_vector, cmp_max_vector), axis=0)\n\n        else:\n            logger.critical(\'Normalisation type %s is not supported!\\n\' %(cfg.output_feature_normalisation))\n            raise\n\n\n    combined_model_arch = str(len(hidden_layers_sizes))\n    for hid_size in hidden_layers_sizes:\n        combined_model_arch += \'_\' + str(hid_size)\n    nnets_file_name = \'%s/%s_%s_%d_%s_%d.%d.train.%d.model\' \\\n                      %(model_dir, cfg.model_type, cfg.combined_feature_name, int(cfg.multistream_switch),\n                        combined_model_arch, lab_dim, cfg.cmp_dim, cfg.train_file_number)\n\n    ### DNN model training\n#    if cfg.TRAINDNN: always do this in synth\n\n\n\n\n\n\n#### DEBUG\n    inferred_weights = None ## default, for non-inferring synth methods\n    if synth_mode == \'inferred\':\n\n        ## infer control values from TESTING data\n\n        ## identical lists (our test data) for \'train\' and \'valid\' -- this is just to\n        ##   keep the infer_projections_fn theano function happy -- operates on\n        ##    validation set. \'Train\' set shouldn\'t be used here.\n        train_x_file_list = copy.copy(nn_label_norm_file_list)\n        train_y_file_list = copy.copy(nn_cmp_norm_file_list)\n        valid_x_file_list = copy.copy(nn_label_norm_file_list)\n        valid_y_file_list = copy.copy(nn_cmp_norm_file_list)\n\n        print(\'FILELIST for inferr:\')\n        print(train_x_file_list)\n        print()\n\n        try:\n            inferred_weights = infer_projections(train_xy_file_list = (train_x_file_list, train_y_file_list), \\\n                        valid_xy_file_list = (valid_x_file_list, valid_y_file_list), \\\n                        nnets_file_name = nnets_file_name, \\\n                        n_ins = lab_dim, n_outs = cfg.cmp_dim, ms_outs = cfg.multistream_outs, \\\n                        hyper_params = cfg.hyper_params, buffer_size = cfg.buffer_size, plot = cfg.plot)\n\n        except KeyboardInterrupt:\n            logger.critical(\'train_DNN interrupted via keyboard\')\n            # Could \'raise\' the exception further, but that causes a deep traceback to be printed\n            # which we don\'t care about for a keyboard interrupt. So, just bail out immediately\n            sys.exit(1)\n        except:\n            logger.critical(\'train_DNN threw an exception\')\n            raise\n\n\n\n\n\n\n    ## if cfg.DNNGEN:\n    logger.info(\'generating from DNN\')\n\n    try:\n        os.makedirs(gen_dir)\n    except OSError as e:\n        if e.errno == errno.EEXIST:\n            # not an error - just means directory already exists\n            pass\n        else:\n            logger.critical(\'Failed to create generation directory %s\' % gen_dir)\n            logger.critical(\' OS error was: %s\' % e.strerror)\n            raise\n\n\n\n    gen_file_list = prepare_file_path_list(file_id_list, gen_dir, cfg.cmp_ext)\n\n    #print nn_label_norm_file_list  ## <-- this WAS mangled in inferred due to copying of file list to trainlist_x etc. which is then shuffled. Now use copy.copy\n    #print gen_file_list\n\n    weights_outfile = os.path.join(out_dir, \'projection_weights_for_synth.txt\')\n    dnn_generation_PROJECTION(nn_label_norm_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list, cfg=cfg, synth_mode=synth_mode, projection_end=projection_end, projection_weights_to_use=inferred_weights, save_weights_to_file=weights_outfile )\n\n    logger.debug(\'denormalising generated output using method %s\' % cfg.output_feature_normalisation)\n    ## DNNGEN\n\n    fid = open(norm_info_file, \'rb\')\n    cmp_min_max = numpy.fromfile(fid, dtype=numpy.float32)\n    fid.close()\n    cmp_min_max = cmp_min_max.reshape((2, -1))\n    cmp_min_vector = cmp_min_max[0, ]\n    cmp_max_vector = cmp_min_max[1, ]\n\n    if cfg.output_feature_normalisation == \'MVN\':\n        denormaliser = MeanVarianceNorm(feature_dimension = cfg.cmp_dim)\n        denormaliser.feature_denormalisation(gen_file_list, gen_file_list, cmp_min_vector, cmp_max_vector)\n\n    elif cfg.output_feature_normalisation == \'MINMAX\':\n        denormaliser = MinMaxNormalisation(cfg.cmp_dim, min_value = 0.01, max_value = 0.99, min_vector = cmp_min_vector, max_vector = cmp_max_vector)\n        denormaliser.denormalise_data(gen_file_list, gen_file_list)\n    else:\n        logger.critical(\'denormalising method %s is not supported!\\n\' %(cfg.output_feature_normalisation))\n        raise\n\n    ##perform MLPG to smooth parameter trajectory\n    ## lf0 is included, the output features much have vuv.\n    generator = ParameterGeneration(gen_wav_features = cfg.gen_wav_features)\n    generator.acoustic_decomposition(gen_file_list, cfg.cmp_dim, cfg.out_dimension_dict, cfg.file_extension_dict, var_file_dict)\n\n            ## osw: skip MLPG:\n#            split_cmp(gen_file_list, [\'mgc\', \'lf0\', \'bap\'], cfg.cmp_dim, cfg.out_dimension_dict, cfg.file_extension_dict)\n\n    ## Variance scaling:\n    scaled_dir = gen_dir + \'_scaled\'\n    simple_scale_variance(gen_dir, scaled_dir, var_file_dict, cfg.out_dimension_dict, file_id_list, gv_weight=0.5)  ## gv_weight hardcoded\n\n    ### generate wav ---- glottHMM only!!!\n    #if cfg.GENWAV:\n    logger.info(\'reconstructing waveform(s)\')\n    generate_wav_glottHMM(scaled_dir, file_id_list)   # generated speech\n\n\ndef simple_scale_variance(indir, outdir, var_file_dict, out_dimension_dict, file_id_list, gv_weight=1.0):\n    ## simple variance scaling (silen et al. 2012, paragraph 3.1)\n    ## TODO: Lots of things like stream names hardcoded here; 3 for delta + delta-delta; ...\n    all_streams = [\'cmp\',\'HNR\',\'F0\',\'LSF\',\'Gain\',\'LSFsource\']\n    streams_to_scale = [\'LSF\']\n\n    static_variances = {}\n\n    static_dimension_dict = {}\n    for (feature_name,size) in list(out_dimension_dict.items()):\n        static_dimension_dict[feature_name] = size/3\n\n    io_funcs = BinaryIOCollection()\n    for feature_name in list(var_file_dict.keys()):\n        var_values, dimension = io_funcs.load_binary_file_frame(var_file_dict[feature_name], 1)\n        static_var_values = var_values[:static_dimension_dict[feature_name], :]\n        static_variances[feature_name] = static_var_values\n\n    if not os.path.isdir(outdir):\n        os.makedirs(outdir)\n\n    assert gv_weight <= 1.0 and gv_weight >= 0.0\n    local_weight = 1.0 - gv_weight\n\n    for uttname in file_id_list:\n        for stream in all_streams:\n            infile = os.path.join(indir, uttname + \'.\' + stream)\n            outfile = os.path.join(outdir, uttname + \'.\' + stream)\n            if not os.path.isfile(infile):\n                sys.exit(infile + \' does not exist\')\n            if stream in streams_to_scale:\n                speech, dimension = io_funcs.load_binary_file_frame(infile, static_dimension_dict[stream])\n                utt_mean = numpy.mean(speech, axis=0)\n                utt_std =  numpy.std(speech, axis=0)\n\n                global_std = numpy.transpose((static_variances[stream]))\n                weighted_global_std = (gv_weight * global_std) + (local_weight * utt_std)\n                std_ratio = weighted_global_std / utt_std\n\n                nframes, ndim = numpy.shape(speech)\n                utt_mean_matrix = numpy.tile(utt_mean, (nframes,1))\n                std_ratio_matrix = numpy.tile(std_ratio, (nframes,1))\n\n                scaled_speech = ((speech - utt_mean_matrix) * std_ratio_matrix) + utt_mean_matrix\n                io_funcs.array_to_binary_file(scaled_speech, outfile)\n\n\n            else:\n                os.system(\'cp %s %s\'%(infile, outfile))\n\n\ndef log_to_hertz(infile, outfile):\n    f = open(infile, \'r\')\n    log_values = [float(val) for val in f.readlines()]\n    f.close()\n\n    def m2h(l):\n        h = math.exp(l)\n        return h\n\n    hertz = [m2h(l) for l in log_values]\n    f = open(outfile, \'w\')\n    for val in hertz:\n        if val > 0:\n            f.write(str(val) + \'\\n\')\n        else:\n            f.write(\'0.0\\n\')\n    f.close()\n\ndef generate_wav_glottHMM(gen_dir, gen_file_id_list):\n\n    x2x=\'~/repos/simple4all/CSTRVoiceClone/trunk/bin/x2x\'\n    synthesis=\'~/sim2/oliver/nst_repos/OSSIAN/ossian-v.1.3/tools/GlottHMM/Synthesis\'\n    general_glott_conf = \'~/sim2/oliver/nst_repos/OSSIAN/ossian-v.1.3/voices/en/ky_02_toy/english_gold_basic_glott_KY/processors/speech_feature_extractor/main_config.cfg\'\n    user_glott_conf = \'~/sim2/oliver/nst_repos/OSSIAN/ossian-v.1.3/voices/en/ky_02_toy/english_gold_basic_glott_KY/processors/speech_feature_extractor/user_config.cfg\'\n\n    exports = \'export LIBCONFIG_INSTALL_DIR=/afs/inf.ed.ac.uk/user/o/owatts/sim2/oliver/nst_repos/OSSIAN/ossian-v.1.3/tools/GlottHMM//libconfig-1.4.9 ; export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$LIBCONFIG_INSTALL_DIR/lib/.libs ; export LIBRARY_PATH=$LIBRARY_PATH:$LIBCONFIG_INSTALL_DIR/lib/.libs ; export CPATH=$CPATH:$LIBCONFIG_INSTALL_DIR/lib ;\'\n\n\n    streams = [\'cmp\',\'HNR\',\'F0\',\'LSF\',\'Gain\',\'LSFsource\']\n    for uttname in gen_file_id_list:\n        all_present = True\n        for stream in streams:\n            if not os.path.isfile(os.path.join(gen_dir, uttname + \'.\' + stream)):\n                all_present = False\n        if all_present:\n            for stream in streams:\n                extra = \'\'\n                if stream == \'F0\':\n                    extra = \'.NEGVALS\'\n                fname = os.path.join(gen_dir, uttname + \'.\' + stream)\n                fname_txt = os.path.join(gen_dir, uttname + \'.txt.\' + stream + extra)\n                comm = \'%s +fa %s > %s\'%(x2x, fname, fname_txt)\n                os.system(comm)\n            log_to_hertz(os.path.join(gen_dir, uttname + \'.txt.F0.NEGVALS\'), \\\n                                        os.path.join(gen_dir, uttname + \'.txt.F0\'))\n\n            stem_name = os.path.join(gen_dir, uttname + \'.txt\')\n            comm = \'%s %s %s %s %s\'%(exports, synthesis, stem_name, general_glott_conf, user_glott_conf)\n            print(comm)\n            os.system(comm)\n\n\n\n        else:\n            print(\'missing stream(s) for utterance \' + uttname)\n\n\n\nif __name__ == \'__main__\':\n\n\n\n    # these things should be done even before trying to parse the command line\n\n    # create a configuration instance\n    # and get a short name for this instance\n    cfg=configuration.cfg\n\n\n\n    # set up logging to use our custom class\n    logging.setLoggerClass(LoggerPlotter)\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n    if len(sys.argv) not in [8,9]:\n        print(sys.argv)\n        sys.exit(\'usage: run_dnn.sh config_file_name utt_dir\')\n\n    config_file = sys.argv[1]\n    in_dir = sys.argv[2]\n    out_dir = sys.argv[3]\n    token_xpath = sys.argv[4]\n    index_attrib_name = sys.argv[5]\n    synth_mode = sys.argv[6]\n    projection_end = int(sys.argv[7])\n\n    assert synth_mode in [\'constant\', \'sampled_training\', \'inferred\', \'uniform\', \'single_sentence_demo\', \'uniform_sampled_within_std_1\', \'uniform_sampled_within_std_2\', \'uniform_sampled_within_std_3\']\n\n    cmp_dir = None\n    if synth_mode == \'inferred\':\n        cmp_dir = sys.argv[8]\n\n\n    config_file = os.path.abspath(config_file)\n    cfg.configure(config_file)\n\n#    if cfg.profile:\n#        logger.info(\'profiling is activated\')\n#        import cProfile, pstats\n#        cProfile.run(\'main_function(cfg)\', \'mainstats\')\n\n#        # create a stream for the profiler to write to\n#        profiling_output = StringIO.StringIO()\n#        p = pstats.Stats(\'mainstats\', stream=profiling_output)\n\n#        # print stats to that stream\n#        # here we just report the top 10 functions, sorted by total amount of time spent in each\n#        p.strip_dirs().sort_stats(\'tottime\').print_stats(10)\n\n#        # print the result to the log\n#        logger.info(\'---Profiling result follows---\\n%s\' %  profiling_output.getvalue() )\n#        profiling_output.close()\n#        logger.info(\'---End of profiling result---\')\n#\n#    else:\n    main_function(cfg, in_dir, out_dir, token_xpath, index_attrib_name, synth_mode, cmp_dir, projection_end)\n\n\n\n    sys.exit(0)\n'"
src/work_in_progress/oliver/mdn_synth.py,0,"b'\nimport pickle\nimport gzip\nimport os, sys, errno\nimport time\nimport math\nimport glob\nimport struct\n\n#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)\nimport numpy\n# we need to explicitly import this in some cases, not sure why this doesn\'t get imported with numpy itself\nimport numpy.distutils.__config__\n# and only after that can we import theano\nimport theano\n\nfrom utils.providers import ListDataProvider\n\nfrom frontend.label_normalisation import HTSLabelNormalisation, XMLLabelNormalisation\nfrom frontend.silence_remover import SilenceRemover\nfrom frontend.silence_remover import trim_silence\nfrom frontend.min_max_norm import MinMaxNormalisation\n#from frontend.acoustic_normalisation import CMPNormalisation\nfrom frontend.acoustic_composition import AcousticComposition\nfrom frontend.parameter_generation import ParameterGeneration\n#from frontend.feature_normalisation_base import FeatureNormBase\nfrom frontend.mean_variance_norm import MeanVarianceNorm\n##from frontend.mlpg_fast import MLParameterGenerationFast\nfrom frontend.mlpg import MLParameterGeneration as MLParameterGenerationFast  ## osw temp\n\nfrom io_funcs.binary_io import  BinaryIOCollection\n\n# the new class for label composition and normalisation\nfrom frontend.label_composer import LabelComposer\n\nimport configuration\n\nfrom models.dnn import DNN\nfrom models.ms_dnn import MultiStreamDNN\nfrom models.ms_dnn_gv import MultiStreamDNNGv\nfrom models.sdae import StackedDenoiseAutoEncoder\nfrom models.mdn import MixtureDensityNetwork\n\nfrom utils.compute_distortion import DistortionComputation, IndividualDistortionComp\nfrom utils.generate import generate_wav\nfrom utils.learn_rates import ExpDecreaseLearningRate\n\n\n#import matplotlib.pyplot as plt\n# our custom logging class that can also plot\n#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot\nfrom logplot.logging_plotting import LoggerPlotter, MultipleSeriesPlot, SingleWeightMatrixPlot\nimport logging # as logging\nimport logging.config\nimport io\n\n\ndef extract_file_id_list(file_list):\n    file_id_list = []\n    for file_name in file_list:\n        file_id = os.path.basename(os.path.splitext(file_name)[0])\n        file_id_list.append(file_id)\n\n    return  file_id_list\n\ndef read_file_list(file_name):\n\n    logger = logging.getLogger(""read_file_list"")\n\n    file_lists = []\n    fid = open(file_name)\n    for line in fid.readlines():\n        line = line.strip()\n        if len(line) < 1:\n            continue\n        file_lists.append(line)\n    fid.close()\n\n    logger.debug(\'Read file list from %s\' % file_name)\n    return  file_lists\n\n\ndef make_output_file_list(out_dir, in_file_lists):\n    out_file_lists = []\n\n    for in_file_name in in_file_lists:\n        file_id = os.path.basename(in_file_name)\n        out_file_name = out_dir + \'/\' + file_id\n        out_file_lists.append(out_file_name)\n\n    return  out_file_lists\n\ndef prepare_file_path_list(file_id_list, file_dir, file_extension, new_dir_switch=True):\n    if not os.path.exists(file_dir) and new_dir_switch:\n        os.makedirs(file_dir)\n    file_name_list = []\n    for file_id in file_id_list:\n        file_name = file_dir + \'/\' + file_id + file_extension\n        file_name_list.append(file_name)\n\n    return  file_name_list\n\n\n\ndef visualize_dnn(dnn):\n\n    layer_num = len(dnn.params) / 2     ## including input and output\n\n    for i in range(layer_num):\n        fig_name = \'Activation weights W\' + str(i)\n        fig_title = \'Activation weights of W\' + str(i)\n        xlabel = \'Neuron index of hidden layer \' + str(i)\n        ylabel = \'Neuron index of hidden layer \' + str(i+1)\n        if i == 0:\n            xlabel = \'Input feature index\'\n        if i == layer_num-1:\n            ylabel = \'Output feature index\'\n\n        logger.create_plot(fig_name, SingleWeightMatrixPlot)\n        plotlogger.add_plot_point(fig_name, fig_name, dnn.params[i*2].get_value(borrow=True).T)\n        plotlogger.save_plot(fig_name, title=fig_name, xlabel=xlabel, ylabel=ylabel)\n\n\n### plain DNN case\n# def dnn_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list):\n#     logger = logging.getLogger(""dnn_generation"")\n#     logger.debug(\'Starting dnn_generation\')\n#\n#     plotlogger = logging.getLogger(""plotting"")\n#\n#     dnn_model = cPickle.load(open(nnets_file_name, \'rb\'))\n#\n# #    visualize_dnn(dbn)\n#\n#     file_number = len(valid_file_list)\n#\n#     for i in xrange(file_number):\n#         logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n#         fid_lab = open(valid_file_list[i], \'rb\')\n#         features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n#         fid_lab.close()\n#         features = features[:(n_ins * (features.size / n_ins))]\n#         features = features.reshape((-1, n_ins))\n#         temp_set_x = features.tolist()\n#         test_set_x = theano.shared(numpy.asarray(temp_set_x, dtype=theano.config.floatX))\n#\n#         predicted_parameter = dnn_model.parameter_prediction(test_set_x=test_set_x)\n# #        predicted_parameter = test_out()\n#\n#         ### write to cmp file\n#         predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n#         temp_parameter = predicted_parameter\n#         fid = open(out_file_list[i], \'wb\')\n#         predicted_parameter.tofile(fid)\n#         logger.debug(\'saved to %s\' % out_file_list[i])\n#         fid.close()\n#\n\n### multiple Gaussian components\ndef dnn_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list, target_mean_vector, target_std_vector, out_dimension_dict, file_extension_dict, vocoder=\'straight\'):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    inf_float = -1.0e+10\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    cfg.gen_wav_features\n\n    if vocoder == \'straight\':\n        gen_wav_features = [\'mgc\', \'lf0\', \'bap\']\n    elif vocoder == \'glotthmm\':\n        gen_wav_features = [\'F0\', \'Gain\', \'HNR\', \'LSF\',\'LSFsource\']  ## TODO: take this from config\n    else:\n        sys.exit(\'unsupported vocoder %s !\'%(vocoder))\n\n    stream_start_index = {}\n    dimension_index = 0\n    for feature_name in list(out_dimension_dict.keys()):\n        stream_start_index[feature_name] = dimension_index\n        dimension_index += out_dimension_dict[feature_name]\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    file_number = len(valid_file_list)\n    io_funcs = BinaryIOCollection()\n\n    mlpg = MLParameterGenerationFast()\n\n    for i in range(file_number):\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        features = features.reshape((-1, n_ins))\n\n        frame_number = features.shape[0]\n\n        test_set_x = theano.shared(numpy.asarray(features, dtype=theano.config.floatX))\n\n        mean_matrix = numpy.tile(target_mean_vector, (features.shape[0], 1))\n        std_matrix = numpy.tile(target_std_vector, (features.shape[0], 1))\n\n        predicted_mix   = dnn_model.parameter_prediction_mix(test_set_x = test_set_x)\n        max_index = numpy.argmax(predicted_mix, axis=1)\n\n        temp_predicted_mu = dnn_model.parameter_prediction(test_set_x=test_set_x)\n        temp_predicted_sigma = dnn_model.parameter_prediction_sigma(test_set_x = test_set_x)\n        predicted_mu = numpy.zeros((temp_predicted_mu.shape[0], n_outs))\n        predicted_sigma = numpy.zeros((temp_predicted_sigma.shape[0], n_outs))\n        for kk in range(temp_predicted_mu.shape[0]):\n            predicted_mu[kk, :] = temp_predicted_mu[kk, max_index[kk]*n_outs:(max_index[kk]+1)*n_outs]\n            predicted_sigma[kk, :] = temp_predicted_sigma[kk, max_index[kk]*n_outs:(max_index[kk]+1)*n_outs]\n#        print   predicted_mu.shape\n#        predicted_mu = predicted_mu[aa*n_outs:(aa+1)*n_outs]\n        predicted_mu = predicted_mu * std_matrix + mean_matrix\n        predicted_sigma = ((predicted_sigma ** 0.5) * std_matrix ) ** 2\n\n        dir_name = os.path.dirname(out_file_list[i])\n        file_id = os.path.splitext(os.path.basename(out_file_list[i]))[0]\n\n        mlpg = MLParameterGenerationFast()\n        for feature_name in gen_wav_features:\n            current_features = predicted_mu[:, stream_start_index[feature_name]:stream_start_index[feature_name]+out_dimension_dict[feature_name]]\n            current_sigma    = predicted_sigma[:, stream_start_index[feature_name]:stream_start_index[feature_name]+out_dimension_dict[feature_name]]\n\n            gen_features = mlpg.generation(current_features, current_sigma, out_dimension_dict[feature_name]/3)\n\n            if feature_name in [\'lf0\', \'F0\']:\n                if \'vuv\' in stream_start_index:\n                    vuv_feature = predicted_mu[:, stream_start_index[\'vuv\']:stream_start_index[\'vuv\']+1]\n                    for i in range(frame_number):\n                        if vuv_feature[i, 0] < 0.5:\n                            gen_features[i, 0] = inf_float\n#                print   gen_features\n            new_file_name = os.path.join(dir_name, file_id + file_extension_dict[feature_name])\n\n            io_funcs.array_to_binary_file(gen_features, new_file_name)\n\n\n\n\n##generate bottleneck layer as festures\ndef dnn_hidden_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        features = features.reshape((-1, n_ins))\n        temp_set_x = features.tolist()\n        test_set_x = theano.shared(numpy.asarray(temp_set_x, dtype=theano.config.floatX))\n\n        predicted_parameter = dnn_model.generate_top_hidden_layer(test_set_x=test_set_x)\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\n\ndef main_function(cfg, in_dir, out_dir):\n\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n    # get another logger to handle plotting duties\n    plotlogger = logging.getLogger(""plotting"")\n\n    # later, we might do this via a handler that is created, attached and configured\n    # using the standard config mechanism of the logging module\n    # but for now we need to do it manually\n    plotlogger.set_plot_path(cfg.plot_dir)\n\n    #### parameter setting########\n    hidden_layers_sizes = cfg.hyper_params[\'hidden_layers_sizes\']\n\n\n    synth_utts = glob.glob(in_dir + \'/*.utt\')\n\n    file_id_list = []\n    for fname in synth_utts:\n        junk,name = os.path.split(fname)\n        file_id_list.append(name.replace(\'.utt\',\'\'))\n\n    if not os.path.isdir(out_dir):\n        os.mkdir(out_dir)\n\n    ###total file number including training, development, and testing\n    #total_file_number = len(file_id_list)\n\n    data_dir = cfg.data_dir\n\n    #nn_cmp_dir       = os.path.join(data_dir, \'nn\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim))\n    #nn_cmp_norm_dir   = os.path.join(data_dir, \'nn_norm\'  + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim))\n\n    model_dir = os.path.join(cfg.work_dir, \'nnets_model\')\n    gen_dir   = os.path.join(out_dir, \'gen\')\n\n    #in_file_list_dict = {}\n\n    #for feature_name in cfg.in_dir_dict.keys():\n    #    in_file_list_dict[feature_name] = prepare_file_path_list(file_id_list, cfg.in_dir_dict[feature_name], cfg.file_extension_dict[feature_name], False)\n\n    #nn_cmp_file_list         = prepare_file_path_list(file_id_list, nn_cmp_dir, cfg.cmp_ext)\n    #nn_cmp_norm_file_list    = prepare_file_path_list(file_id_list, nn_cmp_norm_dir, cfg.cmp_ext)\n\n    ###normalisation information\n    norm_info_file = os.path.join(data_dir, \'norm_info\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim) + \'_\' + cfg.output_feature_normalisation + \'.dat\')\n\n    ### normalise input full context label\n\n    # currently supporting two different forms of lingustic features\n    # later, we should generalise this\n\n    if cfg.label_style == \'HTS\':\n        label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name)\n        lab_dim = label_normaliser.dimension\n        logger.info(\'Input label dimension is %d\' % lab_dim)\n        suffix=str(lab_dim)\n    # no longer supported - use new ""composed"" style labels instead\n    elif cfg.label_style == \'composed\':\n        # label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)\n        suffix=\'composed\'\n\n    # the number can be removed\n    binary_label_dir      = os.path.join(out_dir, \'lab_bin\')\n    nn_label_norm_dir     = os.path.join(out_dir, \'lab_bin_norm\')\n\n\n\n    binary_label_file_list   = prepare_file_path_list(file_id_list, binary_label_dir, cfg.lab_ext)\n    nn_label_norm_file_list  = prepare_file_path_list(file_id_list, nn_label_norm_dir, cfg.lab_ext)\n\n    ## need this to find normalisation info:\n    if cfg.process_labels_in_work_dir:\n        label_data_dir = cfg.work_dir\n    else:\n        label_data_dir = data_dir\n\n    min_max_normaliser = None\n    label_norm_file = \'label_norm_%s.dat\' %(cfg.label_style)\n    label_norm_file = os.path.join(label_data_dir, label_norm_file)\n\n    if cfg.label_style == \'HTS\':\n        sys.exit(\'script not tested with HTS labels\')\n        # simple HTS labels\n        #        logger.info(\'preparing label data (input) using standard HTS style labels\')\n        #        label_normaliser.perform_normalisation(in_label_align_file_list, binary_label_file_list)\n\n        #        remover = SilenceRemover(n_cmp = lab_dim, silence_pattern = [\'*-#+*\'])\n        #        remover.remove_silence(binary_label_file_list, in_label_align_file_list, nn_label_file_list)\n\n        #        min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99)\n        #        ###use only training data to find min-max information, then apply on the whole dataset\n        #        min_max_normaliser.find_min_max_values(nn_label_file_list[0:cfg.train_file_number])\n        #        min_max_normaliser.normalise_data(nn_label_file_list, nn_label_norm_file_list)\n\n\n\n    logger.info(\'preparing label data (input) using ""composed"" style labels\')\n    label_composer = LabelComposer()\n    label_composer.load_label_configuration(cfg.label_config_file)\n\n    logger.info(\'Loaded label configuration\')\n    # logger.info(\'%s\' % label_composer.configuration.labels )\n\n    lab_dim=label_composer.compute_label_dimension()\n    logger.info(\'label dimension will be %d\' % lab_dim)\n\n    if cfg.precompile_xpaths:\n        label_composer.precompile_xpaths()\n\n    # there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)\n    # create all the lists of these, ready to pass to the label composer\n\n    in_label_align_file_list = {}\n    for label_style, label_style_required in label_composer.label_styles.items():\n        if label_style_required:\n            logger.info(\'labels of style %s are required - constructing file paths for them\' % label_style)\n            if label_style == \'xpath\':\n                in_label_align_file_list[\'xpath\'] = prepare_file_path_list(file_id_list, in_dir, cfg.utt_ext, False)\n            elif label_style == \'hts\':\n                logger.critical(\'script not tested with HTS labels\')\n            else:\n                logger.critical(\'unsupported label style %s specified in label configuration\' % label_style)\n                raise Exception\n\n        # now iterate through the files, one at a time, constructing the labels for them\n        num_files=len(file_id_list)\n        logger.info(\'the label styles required are %s\' % label_composer.label_styles)\n\n        for i in range(num_files):\n            logger.info(\'making input label features for %4d of %4d\' % (i+1,num_files))\n\n            # iterate through the required label styles and open each corresponding label file\n\n            # a dictionary of file descriptors, pointing at the required files\n            required_labels={}\n\n            for label_style, label_style_required in label_composer.label_styles.items():\n\n                # the files will be a parallel set of files for a single utterance\n                # e.g., the XML tree and an HTS label file\n                if label_style_required:\n                    required_labels[label_style] = open(in_label_align_file_list[label_style][i] , \'r\')\n                    logger.debug(\' opening label file %s\' % in_label_align_file_list[label_style][i])\n\n            logger.debug(\'label styles with open files: %s\' % required_labels)\n            label_composer.make_labels(required_labels,out_file_name=binary_label_file_list[i],fill_missing_values=cfg.fill_missing_values,iterate_over_frames=cfg.iterate_over_frames)\n\n            # now close all opened files\n            for fd in required_labels.values():\n                fd.close()\n\n\n    # no silence removal for synthesis ...\n\n    ## minmax norm:\n    min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99)\n\n    # reload stored minmax values: (TODO -- move reading and writing into MinMaxNormalisation class)\n    fid = open(label_norm_file, \'rb\')\n\n    ## This doesn\'t work -- precision is lost -- reads in as float64\n    #label_norm_info = numpy.fromfile(fid)  ## label_norm_info = numpy.array(label_norm_info, \'float32\')\n\n    ## use struct to enforce float32:\n    nbytes = os.stat(label_norm_file)[6]  # length in bytes\n    data = fid.read(nbytes)               # = read until bytes run out\n    fid.close()\n    m = nbytes / 4  ## number 32 bit floats\n    format = str(m)+""f""\n    label_norm_info = struct.unpack(format, data)\n    label_norm_info = numpy.array(label_norm_info)\n\n    min_max_normaliser.min_vector = label_norm_info[:m/2]\n    min_max_normaliser.max_vector = label_norm_info[m/2:]\n\n    ###  apply precompuated min-max to the whole dataset\n    min_max_normaliser.normalise_data(binary_label_file_list, nn_label_norm_file_list)\n\n\n\n    ### make output acoustic data\n#    if cfg.MAKECMP:\n\n    ### retrieve acoustic normalisation information for normalising the features back\n    var_dir   = os.path.join(data_dir, \'var\')\n    var_file_dict = {}\n    for feature_name in list(cfg.out_dimension_dict.keys()):\n        var_file_dict[feature_name] = os.path.join(var_dir, feature_name + \'_\' + str(cfg.out_dimension_dict[feature_name]))\n\n\n    ### normalise output acoustic data\n#    if cfg.NORMCMP:\n\n    combined_model_arch = str(len(hidden_layers_sizes))\n    for hid_size in hidden_layers_sizes:\n        combined_model_arch += \'_\' + str(hid_size)\n    nnets_file_name = \'%s/%s_%s_%d_%s_%d.%d.train.%d.mdn.model\' \\\n                      %(model_dir, cfg.model_type, cfg.combined_feature_name, int(cfg.multistream_switch),\n                        combined_model_arch, lab_dim, cfg.cmp_dim, cfg.train_file_number)\n\n    ### DNN model training\n#    if cfg.TRAINDNN:\n\n    ##if cfg.DNNGEN:\n    logger.info(\'generating from DNN\')\n\n    try:\n        os.makedirs(gen_dir)\n    except OSError as e:\n        if e.errno == errno.EEXIST:\n            # not an error - just means directory already exists\n            pass\n        else:\n            logger.critical(\'Failed to create generation directory %s\' % gen_dir)\n            logger.critical(\' OS error was: %s\' % e.strerror)\n            raise\n\n    gen_file_list = prepare_file_path_list(file_id_list, gen_dir, cfg.cmp_ext)\n\n\n\n    assert cfg.output_feature_normalisation == \'MVN\'\n\n    #gen_file_list = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.cmp_ext)\n\n    fid = open(norm_info_file, \'rb\')\n    cmp_min_max = numpy.fromfile(fid, dtype=numpy.float32)\n    fid.close()\n    cmp_min_max = cmp_min_max.reshape((2, -1))\n    target_mean_vector = cmp_min_max[0, ]\n    target_std_vector = cmp_min_max[1, ]\n\n#        dnn_generation(valid_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)\n#        dnn_generation(test_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)\n    dnn_generation(nn_label_norm_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list, target_mean_vector, target_std_vector, cfg.out_dimension_dict, cfg.file_extension_dict, vocoder=\'glotthmm\')\n\n\n\n    ## Variance scaling:\n    test_var_scaling=False\n    scaled_dir = gen_dir + \'_scaled\'\n    if test_var_scaling:\n        file_id_list = simple_scale_variance_CONTINUUM(gen_dir, scaled_dir, var_file_dict, cfg.out_dimension_dict, file_id_list)\n    else:\n        simple_scale_variance(gen_dir, scaled_dir, var_file_dict, cfg.out_dimension_dict, file_id_list, gv_weight=0.5)  ## gv_weight hard coded here!\n\n    ### generate wav ---- glottHMM only!!!\n    #if cfg.GENWAV:\n    logger.info(\'reconstructing waveform(s)\')\n    generate_wav_glottHMM(scaled_dir, file_id_list)   # generated speech\n\n\ndef simple_scale_variance(indir, outdir, var_file_dict, out_dimension_dict, file_id_list, gv_weight=1.0):\n    ## simple variance scaling (silen et al. 2012, paragraph 3.1)\n    ## TODO: Lots of things like stream names hardcoded here; 3 for delta + delta-delta; ...\n    all_streams = [\'HNR\',\'F0\',\'LSF\',\'Gain\',\'LSFsource\']\n    streams_to_scale = [\'LSF\']\n\n    static_variances = {}\n\n    static_dimension_dict = {}\n    for (feature_name,size) in list(out_dimension_dict.items()):\n        static_dimension_dict[feature_name] = size/3\n\n    io_funcs = BinaryIOCollection()\n    for feature_name in list(var_file_dict.keys()):\n        var_values, dimension = io_funcs.load_binary_file_frame(var_file_dict[feature_name], 1)\n        static_var_values = var_values[:static_dimension_dict[feature_name], :]\n        static_variances[feature_name] = static_var_values\n\n    if not os.path.isdir(outdir):\n        os.makedirs(outdir)\n\n    assert gv_weight <= 1.0 and gv_weight >= 0.0\n    local_weight = 1.0 - gv_weight\n\n    for uttname in file_id_list:\n        for stream in all_streams:\n            infile = os.path.join(indir, uttname + \'.\' + stream)\n            outfile = os.path.join(outdir, uttname + \'.\' + stream)\n            if not os.path.isfile(infile):\n                sys.exit(infile + \' does not exist\')\n            if stream in streams_to_scale:\n                speech, dimension = io_funcs.load_binary_file_frame(infile, static_dimension_dict[stream])\n                utt_mean = numpy.mean(speech, axis=0)\n                utt_std =  numpy.std(speech, axis=0)\n\n                global_std = numpy.transpose((static_variances[stream]))\n                weighted_global_std = (gv_weight * global_std) + (local_weight * utt_std)\n                std_ratio = weighted_global_std / utt_std\n\n                nframes, ndim = numpy.shape(speech)\n                utt_mean_matrix = numpy.tile(utt_mean, (nframes,1))\n                std_ratio_matrix = numpy.tile(std_ratio, (nframes,1))\n\n                scaled_speech = ((speech - utt_mean_matrix) * std_ratio_matrix) + utt_mean_matrix\n                io_funcs.array_to_binary_file(scaled_speech, outfile)\n\n\n            else:\n                os.system(\'cp %s %s\'%(infile, outfile))\n\n\n\ndef simple_scale_variance_CONTINUUM(indir, outdir, var_file_dict, out_dimension_dict, file_id_list):\n    ## Try range of interpolation weights for combining global & local variance\n    all_streams = [\'cmp\',\'HNR\',\'F0\',\'LSF\',\'Gain\',\'LSFsource\']\n    streams_to_scale = [\'LSF\']\n\n    static_variances = {}\n\n    static_dimension_dict = {}\n    for (feature_name,size) in list(out_dimension_dict.items()):\n        static_dimension_dict[feature_name] = size/3\n\n    io_funcs = BinaryIOCollection()\n    for feature_name in list(var_file_dict.keys()):\n        var_values, dimension = io_funcs.load_binary_file_frame(var_file_dict[feature_name], 1)\n        static_var_values = var_values[:static_dimension_dict[feature_name], :]\n        static_variances[feature_name] = static_var_values\n\n    if not os.path.isdir(outdir):\n        os.makedirs(outdir)\n\n    file_id_list_out = []\n    for uttname in file_id_list:\n        for gv_weight in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n            local_weight = 1.0 - gv_weight\n            for stream in all_streams:\n                infile = os.path.join(indir, uttname + \'.\' + stream)\n                extended_uttname = uttname + \'_gv\' + str(gv_weight)\n                print(extended_uttname)\n                outfile = os.path.join(outdir, extended_uttname + \'.\' + stream)\n                if not os.path.isfile(infile):\n                    sys.exit(infile + \' does not exist\')\n                if stream in streams_to_scale:\n                    speech, dimension = io_funcs.load_binary_file_frame(infile, static_dimension_dict[stream])\n                    utt_mean = numpy.mean(speech, axis=0)\n                    utt_std =  numpy.std(speech, axis=0)\n\n                    global_std = numpy.transpose((static_variances[stream]))\n\n                    weighted_global_std = (gv_weight * global_std) + (local_weight * utt_std)\n\n                    std_ratio = weighted_global_std / utt_std\n\n                    nframes, ndim = numpy.shape(speech)\n                    utt_mean_matrix = numpy.tile(utt_mean, (nframes,1))\n                    std_ratio_matrix = numpy.tile(std_ratio, (nframes,1))\n\n                    scaled_speech = ((speech - utt_mean_matrix) * std_ratio_matrix) + utt_mean_matrix\n                    io_funcs.array_to_binary_file(scaled_speech, outfile)\n\n\n                else:\n                    os.system(\'cp %s %s\'%(infile, outfile))\n            file_id_list_out.append(extended_uttname)\n    return file_id_list_out\n\n\ndef log_to_hertz(infile, outfile):\n    f = open(infile, \'r\')\n    log_values = [float(val) for val in f.readlines()]\n    f.close()\n\n    def m2h(l):\n        h = math.exp(l)\n        return h\n\n    hertz = [m2h(l) for l in log_values]\n    f = open(outfile, \'w\')\n    for val in hertz:\n        if val > 0:\n            f.write(str(val) + \'\\n\')\n        else:\n            f.write(\'0.0\\n\')\n    f.close()\n\n\ndef generate_wav_glottHMM(gen_dir, gen_file_id_list):\n\n    x2x=\'~/repos/simple4all/CSTRVoiceClone/trunk/bin/x2x\'\n    synthesis=\'~/sim2/oliver/nst_repos/OSSIAN/ossian-v.1.3/tools/GlottHMM/Synthesis\'\n    general_glott_conf = \'~/sim2/oliver/nst_repos/OSSIAN/ossian-v.1.3/voices/en/ky_02_toy/english_gold_basic_glott_KY/processors/speech_feature_extractor/main_config.cfg\'\n    user_glott_conf = \'~/sim2/oliver/nst_repos/OSSIAN/ossian-v.1.3/voices/en/ky_02_toy/english_gold_basic_glott_KY/processors/speech_feature_extractor/user_config.cfg\'\n\n    exports = \'export LIBCONFIG_INSTALL_DIR=/afs/inf.ed.ac.uk/user/o/owatts/sim2/oliver/nst_repos/OSSIAN/ossian-v.1.3/tools/GlottHMM//libconfig-1.4.9 ; export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$LIBCONFIG_INSTALL_DIR/lib/.libs ; export LIBRARY_PATH=$LIBRARY_PATH:$LIBCONFIG_INSTALL_DIR/lib/.libs ; export CPATH=$CPATH:$LIBCONFIG_INSTALL_DIR/lib ;\'\n\n\n    streams = [\'HNR\',\'F0\',\'LSF\',\'Gain\',\'LSFsource\']\n    for uttname in gen_file_id_list:\n        all_present = True\n        for stream in streams:\n            if not os.path.isfile(os.path.join(gen_dir, uttname + \'.\' + stream)):\n                all_present = False\n        if all_present:\n            for stream in streams:\n                extra = \'\'\n                if stream == \'F0\':\n                    extra = \'.NEGVALS\'\n                fname = os.path.join(gen_dir, uttname + \'.\' + stream)\n                fname_txt = os.path.join(gen_dir, uttname + \'.txt.\' + stream + extra)\n                comm = \'%s +fa %s > %s\'%(x2x, fname, fname_txt)\n                os.system(comm)\n            log_to_hertz(os.path.join(gen_dir, uttname + \'.txt.F0.NEGVALS\'), \\\n                                        os.path.join(gen_dir, uttname + \'.txt.F0\'))\n\n            stem_name = os.path.join(gen_dir, uttname + \'.txt\')\n            comm = \'%s %s %s %s %s\'%(exports, synthesis, stem_name, general_glott_conf, user_glott_conf)\n            print(comm)\n            os.system(comm)\n\n\n\n        else:\n            print(\'missing stream(s) for utterance \' + uttname)\n\n\n\nif __name__ == \'__main__\':\n\n\n\n    # these things should be done even before trying to parse the command line\n\n    # create a configuration instance\n    # and get a short name for this instance\n    cfg=configuration.cfg\n\n    # set up logging to use our custom class\n    logging.setLoggerClass(LoggerPlotter)\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n\n    if len(sys.argv) != 4:\n        logger.critical(\'usage: run_dnn.sh config_file_name utt_dir\')\n        sys.exit(1)\n\n    config_file = sys.argv[1]\n    in_dir = sys.argv[2]\n    out_dir = sys.argv[3]\n\n    config_file = os.path.abspath(config_file)\n    cfg.configure(config_file)\n\n#    if cfg.profile:\n#        logger.info(\'profiling is activated\')\n#        import cProfile, pstats\n#        cProfile.run(\'main_function(cfg)\', \'mainstats\')\n\n#        # create a stream for the profiler to write to\n#        profiling_output = StringIO.StringIO()\n#        p = pstats.Stats(\'mainstats\', stream=profiling_output)\n\n#        # print stats to that stream\n#        # here we just report the top 10 functions, sorted by total amount of time spent in each\n#        p.strip_dirs().sort_stats(\'tottime\').print_stats(10)\n\n#        # print the result to the log\n#        logger.info(\'---Profiling result follows---\\n%s\' %  profiling_output.getvalue() )\n#        profiling_output.close()\n#        logger.info(\'---End of profiling result---\')\n#\n#    else:\n    main_function(cfg, in_dir, out_dir)\n\n    sys.exit(0)\n'"
src/work_in_progress/oliver/run_dnn_hourly_check.py,0,"b'\nimport pickle\nimport gzip\nimport os, sys, errno\nimport time\nimport math\n\n#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)\nimport numpy\n# we need to explicitly import this in some cases, not sure why this doesn\'t get imported with numpy itself\nimport numpy.distutils.__config__\n# and only after that can we import theano\nimport theano\n\nfrom utils.providers import ListDataProvider\n\nfrom frontend.label_normalisation import HTSLabelNormalisation, XMLLabelNormalisation\nfrom frontend.silence_remover import SilenceRemover\nfrom frontend.silence_remover import trim_silence\nfrom frontend.min_max_norm import MinMaxNormalisation\n#from frontend.acoustic_normalisation import CMPNormalisation\nfrom frontend.acoustic_composition import AcousticComposition\nfrom frontend.parameter_generation import ParameterGeneration\n#from frontend.feature_normalisation_base import FeatureNormBase\nfrom frontend.mean_variance_norm import MeanVarianceNorm\n\n# the new class for label composition and normalisation\nfrom frontend.label_composer import LabelComposer\n\nimport configuration\n\nfrom models.dnn import DNN\nfrom models.ms_dnn import MultiStreamDNN\nfrom models.ms_dnn_gv import MultiStreamDNNGv\nfrom models.sdae import StackedDenoiseAutoEncoder\n\nfrom utils.compute_distortion import DistortionComputation, IndividualDistortionComp\nfrom utils.generate import generate_wav\nfrom utils.learn_rates import ExpDecreaseLearningRate\n\n\n#import matplotlib.pyplot as plt\n# our custom logging class that can also plot\n#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot\nfrom logplot.logging_plotting import LoggerPlotter, MultipleSeriesPlot, SingleWeightMatrixPlot\nimport logging # as logging\nimport logging.config\nimport io\n\n\ndef extract_file_id_list(file_list):\n    file_id_list = []\n    for file_name in file_list:\n        file_id = os.path.basename(os.path.splitext(file_name)[0])\n        file_id_list.append(file_id)\n\n    return  file_id_list\n\ndef read_file_list(file_name):\n\n    logger = logging.getLogger(""read_file_list"")\n\n    file_lists = []\n    fid = open(file_name)\n    for line in fid.readlines():\n        line = line.strip()\n        if len(line) < 1:\n            continue\n        file_lists.append(line)\n    fid.close()\n\n    logger.debug(\'Read file list from %s\' % file_name)\n    return  file_lists\n\n\ndef make_output_file_list(out_dir, in_file_lists):\n    out_file_lists = []\n\n    for in_file_name in in_file_lists:\n        file_id = os.path.basename(in_file_name)\n        out_file_name = out_dir + \'/\' + file_id\n        out_file_lists.append(out_file_name)\n\n    return  out_file_lists\n\ndef prepare_file_path_list(file_id_list, file_dir, file_extension, new_dir_switch=True):\n    if not os.path.exists(file_dir) and new_dir_switch:\n        os.makedirs(file_dir)\n    file_name_list = []\n    for file_id in file_id_list:\n        file_name = file_dir + \'/\' + file_id + file_extension\n        file_name_list.append(file_name)\n\n    return  file_name_list\n\n\n\ndef visualize_dnn(dnn):\n\n    layer_num = len(dnn.params) / 2     ## including input and output\n\n    for i in range(layer_num):\n        fig_name = \'Activation weights W\' + str(i)\n        fig_title = \'Activation weights of W\' + str(i)\n        xlabel = \'Neuron index of hidden layer \' + str(i)\n        ylabel = \'Neuron index of hidden layer \' + str(i+1)\n        if i == 0:\n            xlabel = \'Input feature index\'\n        if i == layer_num-1:\n            ylabel = \'Output feature index\'\n\n        logger.create_plot(fig_name, SingleWeightMatrixPlot)\n        plotlogger.add_plot_point(fig_name, fig_name, dnn.params[i*2].get_value(borrow=True).T)\n        plotlogger.save_plot(fig_name, title=fig_name, xlabel=xlabel, ylabel=ylabel)\n\n\ndef train_DNN(train_xy_file_list, valid_xy_file_list, \\\n              nnets_file_name, n_ins, n_outs, ms_outs, hyper_params, buffer_size, plot=False):\n\n    # get loggers for this function\n    # this one writes to both console and file\n    logger = logging.getLogger(""main.train_DNN"")\n    logger.debug(\'Starting train_DNN\')\n\n    if plot:\n        # this one takes care of plotting duties\n        plotlogger = logging.getLogger(""plotting"")\n        # create an (empty) plot of training convergence, ready to receive data points\n        logger.create_plot(\'training convergence\',MultipleSeriesPlot)\n\n    try:\n        assert numpy.sum(ms_outs) == n_outs\n    except AssertionError:\n        logger.critical(\'the summation of multi-stream outputs does not equal to %d\' %(n_outs))\n        raise\n\n    ####parameters#####\n    finetune_lr     = float(hyper_params[\'learning_rate\'])\n    training_epochs = int(hyper_params[\'training_epochs\'])\n    batch_size      = int(hyper_params[\'batch_size\'])\n    l1_reg          = float(hyper_params[\'l1_reg\'])\n    l2_reg          = float(hyper_params[\'l2_reg\'])\n    private_l2_reg  = float(hyper_params[\'private_l2_reg\'])\n    warmup_epoch    = int(hyper_params[\'warmup_epoch\'])\n    momentum        = float(hyper_params[\'momentum\'])\n    warmup_momentum = float(hyper_params[\'warmup_momentum\'])\n\n    use_rprop = int(hyper_params[\'use_rprop\'])\n\n    use_rprop = int(hyper_params[\'use_rprop\'])\n\n    hidden_layers_sizes = hyper_params[\'hidden_layers_sizes\']\n\n    stream_weights       = hyper_params[\'stream_weights\']\n    private_hidden_sizes = hyper_params[\'private_hidden_sizes\']\n\n    buffer_utt_size = buffer_size\n    early_stop_epoch = int(hyper_params[\'early_stop_epochs\'])\n\n    hidden_activation = hyper_params[\'hidden_activation\']\n    output_activation = hyper_params[\'output_activation\']\n\n    stream_lr_weights = hyper_params[\'stream_lr_weights\']\n    use_private_hidden = hyper_params[\'use_private_hidden\']\n\n    model_type = hyper_params[\'model_type\']\n\n    ## use a switch to turn on pretraining\n    ## pretraining may not help too much, if this case, we turn it off to save time\n    do_pretraining = hyper_params[\'do_pretraining\']\n    pretraining_epochs = int(hyper_params[\'pretraining_epochs\'])\n    pretraining_lr = float(hyper_params[\'pretraining_lr\'])\n\n\n    buffer_size = int(buffer_size / batch_size) * batch_size\n\n    ###################\n    (train_x_file_list, train_y_file_list) = train_xy_file_list\n    (valid_x_file_list, valid_y_file_list) = valid_xy_file_list\n\n    logger.debug(\'Creating training   data provider\')\n    train_data_reader = ListDataProvider(x_file_list = train_x_file_list, y_file_list = train_y_file_list, n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, shuffle = True)\n\n    logger.debug(\'Creating validation data provider\')\n    valid_data_reader = ListDataProvider(x_file_list = valid_x_file_list, y_file_list = valid_y_file_list, n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, shuffle = False)\n\n    shared_train_set_xy, temp_train_set_x, temp_train_set_y = train_data_reader.load_next_partition()\n    train_set_x, train_set_y = shared_train_set_xy\n    shared_valid_set_xy, temp_valid_set_x, temp_valid_set_y = valid_data_reader.load_next_partition()\n    valid_set_x, valid_set_y = shared_valid_set_xy\n    train_data_reader.reset()\n    valid_data_reader.reset()\n\n#     frames_per_hour = 720000.0\n#     tframes = train_set_x.get_value().shape[0]\n#     vframes = valid_set_x.get_value().shape[0]\n#     print \'Training frames: %s (%s hours)\'%(tframes, tframes / frames_per_hour)\n#     print \'Validation frames: %s (%s hours)\'%(tframes, tframes / frames_per_hour)\n#     sys.exit(\'999\')\n\n\n    ##temporally we use the training set as pretrain_set_x.\n    ##we need to support any data for pretraining\n    pretrain_set_x = train_set_x\n\n    # numpy random generator\n    numpy_rng = numpy.random.RandomState(123)\n    logger.info(\'building the model\')\n\n\n    dnn_model = None\n    pretrain_fn = None  ## not all the model support pretraining right now\n    train_fn = None\n    valid_fn = None\n    valid_model = None ## valid_fn and valid_model are the same. reserve to computer multi-stream distortion\n    if model_type == \'DNN\':\n        dnn_model = DNN(numpy_rng=numpy_rng, n_ins=n_ins, n_outs = n_outs,\n                        l1_reg = l1_reg, l2_reg = l2_reg,\n                         hidden_layers_sizes = hidden_layers_sizes,\n                          hidden_activation = hidden_activation,\n                          output_activation = output_activation,\n                          use_rprop = use_rprop, rprop_init_update=finetune_lr)\n\n        train_fn, valid_fn, valid_score_i = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y), batch_size=batch_size, return_valid_score_i=True)\n\n\n    elif model_type == \'SDAE\':\n        ##basic model is ready.\n        ##if corruption levels is set to zero. it becomes normal autoencoder\n        dnn_model = StackedDenoiseAutoEncoder(numpy_rng=numpy_rng, n_ins=n_ins, n_outs = n_outs,\n                          l1_reg = l1_reg, l2_reg = l2_reg,\n                          hidden_layers_sizes = hidden_layers_sizes)\n\n        if do_pretraining:\n            pretraining_fn = dnn_model.pretraining_functions(pretrain_set_x, batch_size)\n\n        train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y), batch_size=batch_size)\n\n    elif model_type == \'MSDNN\': ##model is ready, but the hyper-parameters are not optimised.\n        dnn_model = MultiStreamDNN(numpy_rng=numpy_rng, n_ins=n_ins, ms_outs=ms_outs,\n                          l1_reg = l1_reg, l2_reg = l2_reg,\n                          hidden_layers_sizes = hidden_layers_sizes,\n                          stream_weights = stream_weights,\n                          hidden_activation = hidden_activation,\n                          output_activation = output_activation)\n\n        train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y),\n                    batch_size=batch_size, lr_weights = stream_lr_weights)\n    elif model_type == \'MSDNN_GV\':  ## not fully ready\n        dnn_model = MultiStreamDNNGv(numpy_rng=numpy_rng, n_ins=n_ins, ms_outs=ms_outs,\n                          l1_reg = l1_reg, l2_reg = l2_reg,\n                          hidden_layers_sizes = hidden_layers_sizes,\n                          stream_weights = stream_weights,\n                          hidden_activation = hidden_activation,\n                          output_activation = output_activation)\n\n        train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y),\n                    batch_size=batch_size, lr_weights = stream_lr_weights)\n    else:\n        logger.critical(\'%s type NN model is not supported!\' %(model_type))\n        raise\n\n    ## if pretraining is supported in one model, add the switch here\n    ## be careful to use autoencoder for pretraining here:\n    ## for SDAE, currently only sigmoid function is supported in the hidden layers, as our input is scaled to [0, 1]\n    ## however, tanh works better and converge fast in finetuning\n    ##\n    ## Will extend this soon...\n    if do_pretraining and model_type == \'SDAE\':\n        logger.info(\'pretraining the %s model\' %(model_type))\n\n        corruption_level = 0.0\n        ## in SDAE we do layer-wise pretraining using autoencoders\n        for i in range(dnn_model.n_layers):\n            for epoch in range(pretraining_epochs):\n                sub_start_time = time.clock()\n\n                pretrain_loss = []\n                while (not train_data_reader.is_finish()):\n                    shared_train_set_xy, temp_train_set_x, temp_train_set_y = train_data_reader.load_next_partition()\n                    pretrain_set_x.set_value(numpy.asarray(temp_train_set_x, dtype=theano.config.floatX), borrow=True)\n\n                    n_train_batches = pretrain_set_x.get_value().shape[0] / batch_size\n\n                    for batch_index in range(n_train_batches):\n                        pretrain_loss.append(pretraining_fn[i](index=batch_index,\n                                                               corruption=corruption_level,\n                                                               learning_rate=pretraining_lr))\n\n                sub_end_time = time.clock()\n                logger.info(\'Pre-training layer %i, epoch %d, cost %s, time spent%.2f\' % (i+1, epoch+1, numpy.mean(pretrain_loss), (sub_end_time - sub_start_time)))\n                train_data_reader.reset()\n\n\n    logger.info(\'fine-tuning the %s model\' %(model_type))\n\n    start_time = time.clock()\n\n    best_dnn_model = dnn_model\n    best_validation_loss = sys.float_info.max\n    previous_loss = sys.float_info.max\n\n    early_stop = 0\n    epoch = 0\n    previous_finetune_lr = finetune_lr\n\n    hours_seen = 0\n    seen_frames = 0\n    train_error = []\n    sub_start_time = time.clock()\n\n    # =============================================================================\n    # The original script (run_dnn.py) has a training routine that looks like this:\n    #\n    #     foreach epoch:\n    #        foreach partition:\n    #            foreach minibatch:\n    #                train_model\n    #        validate_performance_and_stop_if_converged\n    #\n    # The current script\'s rountine looks like this:\n    #\n    #     foreach epoch:\n    #        foreach partition:\n    #            foreach minibatch:\n    #                train_model\n    #                if we\'ve seen another hour of data:\n    #                     validate_performance_and_stop_if_converged\n    #\n    # In order to jump out of these multiple loops when converged, we\'ll use this variable:\n    #\n\n    break_main_loop = False\n\n    while (epoch < training_epochs):\n        epoch = epoch + 1\n\n        current_momentum = momentum\n        current_finetune_lr = finetune_lr\n        if epoch <= warmup_epoch:\n            current_finetune_lr = finetune_lr\n            current_momentum = warmup_momentum\n        else:\n            current_finetune_lr = previous_finetune_lr * 0.5\n\n        previous_finetune_lr = current_finetune_lr\n\n\n\n\n        while (not train_data_reader.is_finish()):\n            shared_train_set_xy, temp_train_set_x, temp_train_set_y = train_data_reader.load_next_partition()\n            train_set_x.set_value(numpy.asarray(temp_train_set_x, dtype=theano.config.floatX), borrow=True)\n            train_set_y.set_value(numpy.asarray(temp_train_set_y, dtype=theano.config.floatX), borrow=True)\n\n            n_train_batches = train_set_x.get_value().shape[0] / batch_size\n\n            logger.debug(\'this partition: %d frames (divided into %d batches of size %d)\' %(train_set_x.get_value(borrow=True).shape[0], n_train_batches, batch_size) )\n\n            for minibatch_index in range(n_train_batches):\n                this_train_error = train_fn(minibatch_index, current_finetune_lr, current_momentum)\n                train_error.append(this_train_error)\n\n                if numpy.isnan(this_train_error):\n                    logger.warning(\'training error over minibatch %d of %d was %s\' % (minibatch_index+1,n_train_batches,this_train_error) )\n\n                seen_frames += batch_size\n\n\n                if seen_frames >= 720000:  ## Hardcoded checking intervals and framerate: 720000 frames per hour at 5ms frame rate\n\n                    hours_seen += 1\n                    logger.debug(\'seen %s hour(s) of data -- calculating validation loss\'%(hours_seen))\n\n\n                    ### calculation validation error in 1 big batch can fail for big data --\n                    ### use minibatches\n\n                    #validation_losses = valid_fn()\n                    #this_validation_loss = numpy.mean(validation_losses)\n\n                    valid_error = []\n                    valid_data_reader.reset()\n                    while (not valid_data_reader.is_finish()):\n                        shared_valid_set_xy, temp_valid_set_x, temp_valid_set_y = valid_data_reader.load_next_partition()\n                        valid_set_x.set_value(numpy.asarray(temp_valid_set_x, dtype=theano.config.floatX), borrow=True)\n                        valid_set_y.set_value(numpy.asarray(temp_valid_set_y, dtype=theano.config.floatX), borrow=True)\n                        n_valid_batches = valid_set_x.get_value().shape[0] / batch_size\n                        for minibatch_index in range(n_valid_batches):\n                            v_loss = valid_score_i(minibatch_index)\n                            valid_error.append(v_loss)\n                            #print \'   validation for batch %s (%s frames): %s\'%(minibatch_index, batch_size, v_loss)\n                    this_validation_loss = numpy.mean(valid_error)\n                    this_validation_loss_std = numpy.std(valid_error)\n                    print(\'Mean validation loss: %s, std over minibatches: %s\'%(this_validation_loss, this_validation_loss_std))\n\n                    # this has a possible bias if the minibatches were not all of identical size\n                    # but it should not be siginficant if minibatches are small\n                    this_train_valid_loss = numpy.mean(train_error)\n\n                    ## It might also be interesting to look at how consistent performance is across minibatches:\n                    this_train_valid_loss_std = numpy.std(train_error)\n\n                    sub_end_time = time.clock()\n\n                    loss_difference = this_validation_loss - previous_loss\n\n                    logger.info(\'epoch %i, validation error %f (std: %f), train error %f (std: %f)  time spent %.2f\' %(epoch, this_validation_loss, this_validation_loss_std, this_train_valid_loss, this_train_valid_loss_std, (sub_end_time - sub_start_time)))\n                    if plot:\n                        plotlogger.add_plot_point(\'training convergence\',\'validation set\',(hours_seen,this_validation_loss))\n                        plotlogger.add_plot_point(\'training convergence\',\'training set\',(hours_seen,this_train_valid_loss))\n                        plotlogger.save_plot(\'training convergence\',title=\'Progress of training and validation error\',xlabel=\'hours of data seen\',ylabel=\'error\')\n\n                    if this_validation_loss < best_validation_loss:\n                        best_dnn_model = dnn_model\n                        best_validation_loss = this_validation_loss\n                        logger.debug(\'validation loss decreased, so saving model\')\n                        early_stop = 0\n                    else:\n                        logger.debug(\'validation loss did not improve\')\n                        dbn = best_dnn_model\n                        early_stop += 1\n\n                    if early_stop > early_stop_epoch:\n                        # too many consecutive checks without surpassing the best model\n                        logger.debug(\'stopping early\')\n                        break_main_loop = True\n                        break\n\n                    if math.isnan(this_validation_loss):\n                        break_main_loop = True\n                        break\n\n                    previous_loss = this_validation_loss\n\n                    sub_start_time = time.clock()\n                    seen_frames = 0\n\n                    train_error = []\n\n            if break_main_loop:\n                break\n\n        if break_main_loop:\n            break\n\n        train_data_reader.reset()\n\n\n    end_time = time.clock()\n    pickle.dump(best_dnn_model, open(nnets_file_name, \'wb\'))\n\n    logger.info(\'overall  training time: %.2fm validation error %f\' % ((end_time - start_time) / 60., best_validation_loss))\n\n    if plot:\n        plotlogger.save_plot(\'training convergence\',title=\'Final training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n\n    return  best_validation_loss\n\n\ndef dnn_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n#    visualize_dnn(dbn)\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        features = features.reshape((-1, n_ins))\n        temp_set_x = features.tolist()\n        test_set_x = theano.shared(numpy.asarray(temp_set_x, dtype=theano.config.floatX))\n\n        predicted_parameter = dnn_model.parameter_prediction(test_set_x=test_set_x)\n#        predicted_parameter = test_out()\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\n##generate bottleneck layer as festures\ndef dnn_hidden_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        features = features.reshape((-1, n_ins))\n        temp_set_x = features.tolist()\n        test_set_x = theano.shared(numpy.asarray(temp_set_x, dtype=theano.config.floatX))\n\n        predicted_parameter = dnn_model.generate_top_hidden_layer(test_set_x=test_set_x)\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\n\ndef main_function(cfg):\n\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n    # get another logger to handle plotting duties\n    plotlogger = logging.getLogger(""plotting"")\n\n    # later, we might do this via a handler that is created, attached and configured\n    # using the standard config mechanism of the logging module\n    # but for now we need to do it manually\n    plotlogger.set_plot_path(cfg.plot_dir)\n\n    #### parameter setting########\n    hidden_layers_sizes = cfg.hyper_params[\'hidden_layers_sizes\']\n\n\n    ####prepare environment\n\n    try:\n        file_id_list = read_file_list(cfg.file_id_scp)\n        logger.debug(\'Loaded file id list from %s\' % cfg.file_id_scp)\n    except IOError:\n        # this means that open(...) threw an error\n        logger.critical(\'Could not load file id list from %s\' % cfg.file_id_scp)\n        raise\n\n    ###total file number including training, development, and testing\n    total_file_number = len(file_id_list)\n\n    data_dir = cfg.data_dir\n\n    nn_cmp_dir       = os.path.join(data_dir, \'nn\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim))\n    nn_cmp_norm_dir   = os.path.join(data_dir, \'nn_norm\'  + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim))\n\n    model_dir = os.path.join(cfg.work_dir, \'nnets_model\')\n    gen_dir   = os.path.join(cfg.work_dir, \'gen\')\n\n    in_file_list_dict = {}\n\n    for feature_name in list(cfg.in_dir_dict.keys()):\n        in_file_list_dict[feature_name] = prepare_file_path_list(file_id_list, cfg.in_dir_dict[feature_name], cfg.file_extension_dict[feature_name], False)\n\n    nn_cmp_file_list         = prepare_file_path_list(file_id_list, nn_cmp_dir, cfg.cmp_ext)\n    nn_cmp_norm_file_list    = prepare_file_path_list(file_id_list, nn_cmp_norm_dir, cfg.cmp_ext)\n\n    ###normalisation information\n    norm_info_file = os.path.join(data_dir, \'norm_info\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim) + \'_\' + cfg.output_feature_normalisation + \'.dat\')\n\n    ### normalise input full context label\n\n    # currently supporting two different forms of lingustic features\n    # later, we should generalise this\n\n    if cfg.label_style == \'HTS\':\n        label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name)\n        lab_dim = label_normaliser.dimension\n        logger.info(\'Input label dimension is %d\' % lab_dim)\n        suffix=str(lab_dim)\n    # no longer supported - use new ""composed"" style labels instead\n    elif cfg.label_style == \'composed\':\n        # label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)\n        suffix=\'composed\'\n\n    if cfg.process_labels_in_work_dir:\n        label_data_dir = cfg.work_dir\n    else:\n        label_data_dir = data_dir\n\n    # the number can be removed\n    binary_label_dir      = os.path.join(label_data_dir, \'binary_label_\'+suffix)\n    nn_label_dir          = os.path.join(label_data_dir, \'nn_no_silence_lab_\'+suffix)\n    nn_label_norm_dir     = os.path.join(label_data_dir, \'nn_no_silence_lab_norm_\'+suffix)\n#    nn_label_norm_mvn_dir = os.path.join(data_dir, \'nn_no_silence_lab_norm_\'+suffix)\n\n    in_label_align_file_list = prepare_file_path_list(file_id_list, cfg.in_label_align_dir, cfg.lab_ext, False)\n    binary_label_file_list   = prepare_file_path_list(file_id_list, binary_label_dir, cfg.lab_ext)\n    nn_label_file_list       = prepare_file_path_list(file_id_list, nn_label_dir, cfg.lab_ext)\n    nn_label_norm_file_list  = prepare_file_path_list(file_id_list, nn_label_norm_dir, cfg.lab_ext)\n\n    # to do - sanity check the label dimension here?\n\n\n\n    min_max_normaliser = None\n    label_norm_file = \'label_norm_%s.dat\' %(cfg.label_style)\n    label_norm_file = os.path.join(label_data_dir, label_norm_file)\n\n    if cfg.NORMLAB and (cfg.label_style == \'HTS\'):\n        # simple HTS labels\n        logger.info(\'preparing label data (input) using standard HTS style labels\')\n        label_normaliser.perform_normalisation(in_label_align_file_list, binary_label_file_list)\n\n        remover = SilenceRemover(n_cmp = lab_dim, silence_pattern = [\'*-#+*\'])\n        remover.remove_silence(binary_label_file_list, in_label_align_file_list, nn_label_file_list)\n\n        min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99)\n        ###use only training data to find min-max information, then apply on the whole dataset\n        min_max_normaliser.find_min_max_values(nn_label_file_list[0:cfg.train_file_number])\n        min_max_normaliser.normalise_data(nn_label_file_list, nn_label_norm_file_list)\n\n\n    if cfg.NORMLAB and (cfg.label_style == \'composed\'):\n        # new flexible label preprocessor\n\n        logger.info(\'preparing label data (input) using ""composed"" style labels\')\n        label_composer = LabelComposer()\n        label_composer.load_label_configuration(cfg.label_config_file)\n\n        logger.info(\'Loaded label configuration\')\n        # logger.info(\'%s\' % label_composer.configuration.labels )\n\n        lab_dim=label_composer.compute_label_dimension()\n        logger.info(\'label dimension will be %d\' % lab_dim)\n\n        if cfg.precompile_xpaths:\n            label_composer.precompile_xpaths()\n\n        # there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)\n        # create all the lists of these, ready to pass to the label composer\n\n        in_label_align_file_list = {}\n        for label_style, label_style_required in label_composer.label_styles.items():\n            if label_style_required:\n                logger.info(\'labels of style %s are required - constructing file paths for them\' % label_style)\n                if label_style == \'xpath\':\n                    in_label_align_file_list[\'xpath\'] = prepare_file_path_list(file_id_list, cfg.xpath_label_align_dir, cfg.utt_ext, False)\n                elif label_style == \'hts\':\n                    in_label_align_file_list[\'hts\'] = prepare_file_path_list(file_id_list, cfg.hts_label_align_dir, cfg.lab_ext, False)\n                else:\n                    logger.critical(\'unsupported label style %s specified in label configuration\' % label_style)\n                    raise Exception\n\n            # now iterate through the files, one at a time, constructing the labels for them\n            num_files=len(file_id_list)\n            logger.info(\'the label styles required are %s\' % label_composer.label_styles)\n\n            for i in range(num_files):\n                logger.info(\'making input label features for %4d of %4d\' % (i+1,num_files))\n\n                # iterate through the required label styles and open each corresponding label file\n\n                # a dictionary of file descriptors, pointing at the required files\n                required_labels={}\n\n                for label_style, label_style_required in label_composer.label_styles.items():\n\n                    # the files will be a parallel set of files for a single utterance\n                    # e.g., the XML tree and an HTS label file\n                    if label_style_required:\n                        required_labels[label_style] = open(in_label_align_file_list[label_style][i] , \'r\')\n                        logger.debug(\' opening label file %s\' % in_label_align_file_list[label_style][i])\n\n                logger.debug(\'label styles with open files: %s\' % required_labels)\n                label_composer.make_labels(required_labels,out_file_name=binary_label_file_list[i],fill_missing_values=cfg.fill_missing_values,iterate_over_frames=cfg.iterate_over_frames)\n\n                # now close all opened files\n                for fd in required_labels.values():\n                    fd.close()\n\n\n        # silence removal\n        if cfg.remove_silence_using_binary_labels:\n            silence_feature = 0 ## use first feature in label -- hardcoded for now\n            logger.info(\'Silence removal from label using silence feature: %s\'%(label_composer.configuration.labels[silence_feature]))\n            logger.info(\'Silence will be removed from CMP files in same way\')\n            ## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:\n            trim_silence(binary_label_file_list, nn_label_file_list, lab_dim, \\\n                                binary_label_file_list, lab_dim, silence_feature, percent_to_keep=5)\n        else:\n            logger.info(\'No silence removal done\')\n            # start from the labels we have just produced, not trimmed versions\n            nn_label_file_list = binary_label_file_list\n\n        min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99)\n        ###use only training data to find min-max information, then apply on the whole dataset\n        min_max_normaliser.find_min_max_values(nn_label_file_list[0:cfg.train_file_number])\n        min_max_normaliser.normalise_data(nn_label_file_list, nn_label_norm_file_list)\n\n    if min_max_normaliser != None:\n        ### save label normalisation information for unseen testing labels\n        label_min_vector = min_max_normaliser.min_vector\n        label_max_vector = min_max_normaliser.max_vector\n        label_norm_info = numpy.concatenate((label_min_vector, label_max_vector), axis=0)\n\n        label_norm_info = numpy.array(label_norm_info, \'float32\')\n        fid = open(label_norm_file, \'wb\')\n        label_norm_info.tofile(fid)\n        fid.close()\n        logger.info(\'saved %s vectors to %s\' %(label_min_vector.size, label_norm_file))\n\n\n\n\n    ### make output acoustic data\n    if cfg.MAKECMP:\n        logger.info(\'creating acoustic (output) features\')\n        delta_win = [-0.5, 0.0, 0.5]\n        acc_win = [1.0, -2.0, 1.0]\n\n        acoustic_worker = AcousticComposition(delta_win = delta_win, acc_win = acc_win)\n        acoustic_worker.prepare_nn_data(in_file_list_dict, nn_cmp_file_list, cfg.in_dimension_dict, cfg.out_dimension_dict)\n\n        if cfg.remove_silence_using_binary_labels:\n            ## do this to get lab_dim:\n            label_composer = LabelComposer()\n            label_composer.load_label_configuration(cfg.label_config_file)\n            lab_dim=label_composer.compute_label_dimension()\n\n            silence_feature = 0 ## use first feature in label -- hardcoded for now\n            logger.info(\'Silence removal from CMP using binary label file\')\n\n            ## overwrite the untrimmed audio with the trimmed version:\n            trim_silence(nn_cmp_file_list, nn_cmp_file_list, cfg.cmp_dim, \\\n                                binary_label_file_list, lab_dim, silence_feature, percent_to_keep=5)\n\n        else: ## back off to previous method using HTS labels:\n            remover = SilenceRemover(n_cmp = cfg.cmp_dim, silence_pattern = [\'*-#+*\'])\n            remover.remove_silence(nn_cmp_file_list, in_label_align_file_list, nn_cmp_file_list) # save to itself\n\n    ### save acoustic normalisation information for normalising the features back\n    var_dir   = os.path.join(data_dir, \'var\')\n    if not os.path.exists(var_dir):\n        os.makedirs(var_dir)\n\n    var_file_dict = {}\n    for feature_name in list(cfg.out_dimension_dict.keys()):\n        var_file_dict[feature_name] = os.path.join(var_dir, feature_name + \'_\' + str(cfg.out_dimension_dict[feature_name]))\n\n    ### normalise output acoustic data\n    if cfg.NORMCMP:\n        logger.info(\'normalising acoustic (output) features using method %s\' % cfg.output_feature_normalisation)\n        cmp_norm_info = None\n        if cfg.output_feature_normalisation == \'MVN\':\n            normaliser = MeanVarianceNorm(feature_dimension=cfg.cmp_dim)\n            ###calculate mean and std vectors on the training data, and apply on the whole dataset\n            global_mean_vector = normaliser.compute_mean(nn_cmp_file_list[0:cfg.train_file_number], 0, cfg.cmp_dim)\n            global_std_vector = normaliser.compute_std(nn_cmp_file_list[0:cfg.train_file_number], global_mean_vector, 0, cfg.cmp_dim)\n\n            normaliser.feature_normalisation(nn_cmp_file_list, nn_cmp_norm_file_list)\n            cmp_norm_info = numpy.concatenate((global_mean_vector, global_std_vector), axis=0)\n\n        elif cfg.output_feature_normalisation == \'MINMAX\':\n            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim)\n            global_mean_vector = min_max_normaliser.compute_mean(nn_cmp_file_list[0:cfg.train_file_number])\n            global_std_vector = min_max_normaliser.compute_std(nn_cmp_file_list[0:cfg.train_file_number], global_mean_vector)\n\n            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim, min_value = 0.01, max_value = 0.99)\n            min_max_normaliser.find_min_max_values(nn_cmp_file_list[0:cfg.train_file_number])\n            min_max_normaliser.normalise_data(nn_cmp_file_list, nn_cmp_norm_file_list)\n\n            cmp_min_vector = min_max_normaliser.min_vector\n            cmp_max_vector = min_max_normaliser.max_vector\n            cmp_norm_info = numpy.concatenate((cmp_min_vector, cmp_max_vector), axis=0)\n\n        else:\n            logger.critical(\'Normalisation type %s is not supported!\\n\' %(cfg.output_feature_normalisation))\n            raise\n\n        cmp_norm_info = numpy.array(cmp_norm_info, \'float32\')\n        fid = open(norm_info_file, \'wb\')\n        cmp_norm_info.tofile(fid)\n        fid.close()\n        logger.info(\'saved %s vectors to %s\' %(cfg.output_feature_normalisation, norm_info_file))\n        # logger.debug(\' value was\\n%s\' % cmp_norm_info)\n\n        feature_index = 0\n        for feature_name in list(cfg.out_dimension_dict.keys()):\n            feature_std_vector = numpy.array(global_std_vector[:,feature_index:feature_index+cfg.out_dimension_dict[feature_name]], \'float32\')\n\n            fid = open(var_file_dict[feature_name], \'w\')\n            feature_std_vector.tofile(fid)\n            fid.close()\n\n            logger.info(\'saved %s variance vector to %s\' %(feature_name, var_file_dict[feature_name]))\n            # logger.debug(\' value was\\n%s\' % feature_std_vector)\n\n            feature_index += cfg.out_dimension_dict[feature_name]\n\n    train_x_file_list = nn_label_norm_file_list[0:cfg.train_file_number]\n    train_y_file_list = nn_cmp_norm_file_list[0:cfg.train_file_number]\n    valid_x_file_list = nn_label_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n    valid_y_file_list = nn_cmp_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n    test_x_file_list  = nn_label_norm_file_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n    test_y_file_list  = nn_cmp_norm_file_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n\n    # we need to know the label dimension before training the DNN\n    # computing that requires us to look at the labels\n    #\n    # currently, there are two ways to do this\n    if cfg.label_style == \'HTS\':\n        label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name)\n        lab_dim = label_normaliser.dimension\n\n    elif cfg.label_style == \'composed\':\n        label_composer = LabelComposer()\n        label_composer.load_label_configuration(cfg.label_config_file)\n        lab_dim=label_composer.compute_label_dimension()\n\n    logger.info(\'label dimension is %d\' % lab_dim)\n\n    combined_model_arch = str(len(hidden_layers_sizes))\n    for hid_size in hidden_layers_sizes:\n        combined_model_arch += \'_\' + str(hid_size)\n\n#    nnets_file_name = \'%s/%s_%s_%d.%d.%d.%d.%d.train.%d.model\' \\\n#                       %(model_dir, cfg.model_type, cfg.combined_feature_name, int(cfg.multistream_switch),\n#                        len(hidden_layers_sizes), hidden_layers_sizes[0],\n#                        lab_dim, cfg.cmp_dim, cfg.train_file_number)\n\n    nnets_file_name = \'%s/%s_%s_%d_%s_%d.%d.train.%d.model\' \\\n                      %(model_dir, cfg.model_type, cfg.combined_feature_name, int(cfg.multistream_switch),\n                        combined_model_arch, lab_dim, cfg.cmp_dim, cfg.train_file_number)\n\n    ### DNN model training\n    if cfg.TRAINDNN:\n\n        logger.info(\'training DNN\')\n\n        try:\n            os.makedirs(model_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create model directory %s\' % model_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        try:\n            # print   \'start DNN\'\n            train_DNN(train_xy_file_list = (train_x_file_list, train_y_file_list), \\\n                      valid_xy_file_list = (valid_x_file_list, valid_y_file_list), \\\n                      nnets_file_name = nnets_file_name, \\\n                      n_ins = lab_dim, n_outs = cfg.cmp_dim, ms_outs = cfg.multistream_outs, \\\n                      hyper_params = cfg.hyper_params, buffer_size = cfg.buffer_size, plot = cfg.plot)\n        except KeyboardInterrupt:\n            logger.critical(\'train_DNN interrupted via keyboard\')\n            # Could \'raise\' the exception further, but that causes a deep traceback to be printed\n            # which we don\'t care about for a keyboard interrupt. So, just bail out immediately\n            sys.exit(1)\n        except:\n            logger.critical(\'train_DNN threw an exception\')\n            raise\n\n    ### generate parameters from DNN\n    temp_dir_name = \'%s_%s_%d_%d_%d_%d_%d_%d\' \\\n                    %(cfg.model_type, cfg.combined_feature_name, int(cfg.do_post_filtering), \\\n                      cfg.train_file_number, lab_dim, cfg.cmp_dim, \\\n                      len(hidden_layers_sizes), hidden_layers_sizes[0])\n    gen_dir = os.path.join(gen_dir, temp_dir_name)\n\n    gen_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n    test_x_file_list  = nn_label_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n    if cfg.DNNGEN:\n        logger.info(\'generating from DNN\')\n\n        try:\n            os.makedirs(gen_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create generation directory %s\' % gen_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        gen_file_list = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.cmp_ext)\n\n#        dnn_generation(valid_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)\n        dnn_generation(test_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)\n\n        logger.debug(\'denormalising generated output using method %s\' % cfg.output_feature_normalisation)\n\n        fid = open(norm_info_file, \'rb\')\n        cmp_min_max = numpy.fromfile(fid, dtype=numpy.float32)\n        fid.close()\n        cmp_min_max = cmp_min_max.reshape((2, -1))\n        cmp_min_vector = cmp_min_max[0, ]\n        cmp_max_vector = cmp_min_max[1, ]\n\n        if cfg.output_feature_normalisation == \'MVN\':\n            denormaliser = MeanVarianceNorm(feature_dimension = cfg.cmp_dim)\n            denormaliser.feature_denormalisation(gen_file_list, gen_file_list, cmp_min_vector, cmp_max_vector)\n\n        elif cfg.output_feature_normalisation == \'MINMAX\':\n            denormaliser = MinMaxNormalisation(cfg.cmp_dim, min_value = 0.01, max_value = 0.99, min_vector = cmp_min_vector, max_vector = cmp_max_vector)\n            denormaliser.denormalise_data(gen_file_list, gen_file_list)\n        else:\n            logger.critical(\'denormalising method %s is not supported!\\n\' %(cfg.output_feature_normalisation))\n            raise\n\n        ##perform MLPG to smooth parameter trajectory\n        ## lf0 is included, the output features much have vuv.\n        generator = ParameterGeneration(gen_wav_features = cfg.gen_wav_features)\n        generator.acoustic_decomposition(gen_file_list, cfg.cmp_dim, cfg.out_dimension_dict, cfg.file_extension_dict, var_file_dict)\n\n    ### generate wav\n    if cfg.GENWAV:\n        logger.info(\'reconstructing waveform(s)\')\n        generate_wav(gen_dir, gen_file_id_list, cfg)     # generated speech\n#       generate_wav(nn_cmp_dir, gen_file_id_list)  # reference copy synthesis speech\n\n    ### evaluation: calculate distortion\n    if cfg.CALMCD:\n        logger.info(\'calculating MCD\')\n\n        ref_data_dir = os.path.join(data_dir, \'ref_data\')\n\n        ref_mgc_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.mgc_ext)\n        ref_bap_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.bap_ext)\n        ref_lf0_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.lf0_ext)\n\n        in_gen_label_align_file_list = in_label_align_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n        calculator = IndividualDistortionComp()\n\n        spectral_distortion = 0.0\n        bap_mse             = 0.0\n        f0_mse              = 0.0\n        vuv_error           = 0.0\n\n        valid_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n        test_file_id_list  = file_id_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n        if cfg.remove_silence_using_binary_labels:\n            ## get lab_dim:\n            label_composer = LabelComposer()\n            label_composer.load_label_configuration(cfg.label_config_file)\n            lab_dim=label_composer.compute_label_dimension()\n\n            ## use first feature in label -- hardcoded for now\n            silence_feature = 0\n\n            ## Use these to trim silence:\n            untrimmed_test_labels = binary_label_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n\n        if \'mgc\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'mgc\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_mgc_list, cfg.mgc_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.mgc_dim, silence_pattern = [\'*-#+*\'])\n                remover.remove_silence(in_file_list_dict[\'mgc\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_mgc_list)\n            valid_spectral_distortion = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.mgc_ext, cfg.mgc_dim)\n            test_spectral_distortion  = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.mgc_ext, cfg.mgc_dim)\n            valid_spectral_distortion *= (10 /numpy.log(10)) * numpy.sqrt(2.0)    ##MCD\n            test_spectral_distortion  *= (10 /numpy.log(10)) * numpy.sqrt(2.0)    ##MCD\n\n\n        if \'bap\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'bap\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_bap_list, cfg.bap_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.bap_dim, silence_pattern = [\'*-#+*\'])\n                remover.remove_silence(in_file_list_dict[\'bap\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_bap_list)\n            valid_bap_mse        = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.bap_ext, cfg.bap_dim)\n            test_bap_mse         = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.bap_ext, cfg.bap_dim)\n            valid_bap_mse = valid_bap_mse / 10.0    ##Cassia\'s bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC\n            test_bap_mse  = test_bap_mse / 10.0    ##Cassia\'s bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC\n\n        if \'lf0\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'lf0\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_lf0_list, cfg.lf0_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.lf0_dim, silence_pattern = [\'*-#+*\'])\n                remover.remove_silence(in_file_list_dict[\'lf0\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_lf0_list)\n            valid_f0_mse, valid_vuv_error   = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.lf0_ext, cfg.lf0_dim)\n            test_f0_mse , test_vuv_error    = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.lf0_ext, cfg.lf0_dim)\n\n        logger.info(\'Develop: DNN -- MCD: %.3f dB; BAP: %.3f dB; F0: %.3f Hz; VUV: %.3f%%\' \\\n                    %(valid_spectral_distortion, valid_bap_mse, valid_f0_mse, valid_vuv_error*100.))\n        logger.info(\'Test   : DNN -- MCD: %.3f dB; BAP: %.3f dB; F0: %.3f Hz; VUV: %.3f%%\' \\\n                    %(test_spectral_distortion , test_bap_mse , test_f0_mse , test_vuv_error*100.))\n\n        # this can be removed\n        #\n        if  0: #to calculate distortion of HMM baseline\n            hmm_gen_no_silence_dir = \'/afs/inf.ed.ac.uk/group/project/dnn_tts/data/nick/nick_hmm_pf_2400_no_silence\'\n            hmm_gen_dir = \'/afs/inf.ed.ac.uk/group/project/dnn_tts/data/nick/nick_hmm_pf_2400\'\n\n            if 1:\n                hmm_mgc_list = prepare_file_path_list(gen_file_id_list, hmm_gen_dir, cfg.mgc_ext)\n                hmm_bap_list = prepare_file_path_list(gen_file_id_list, hmm_gen_dir, cfg.bap_ext)\n                hmm_lf0_list = prepare_file_path_list(gen_file_id_list, hmm_gen_dir, cfg.lf0_ext)\n\n                hmm_mgc_no_silence_list = prepare_file_path_list(gen_file_id_list, hmm_gen_no_silence_dir, cfg.mgc_ext)\n                hmm_bap_no_silence_list = prepare_file_path_list(gen_file_id_list, hmm_gen_no_silence_dir, cfg.bap_ext)\n                hmm_lf0_no_silence_list = prepare_file_path_list(gen_file_id_list, hmm_gen_no_silence_dir, cfg.lf0_ext)\n\n                in_gen_label_align_file_list = in_label_align_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                remover = SilenceRemover(n_cmp = cfg.mgc_dim, silence_pattern = [\'*-#+*\'])\n                remover.remove_silence(hmm_mgc_list, in_gen_label_align_file_list, hmm_mgc_no_silence_list)\n\n                remover = SilenceRemover(n_cmp = cfg.bap_dim, silence_pattern = [\'*-#+*\'])\n                remover.remove_silence(hmm_bap_list, in_gen_label_align_file_list, hmm_bap_no_silence_list)\n\n                remover = SilenceRemover(n_cmp = cfg.lf0_dim, silence_pattern = [\'*-#+*\'])\n                remover.remove_silence(hmm_lf0_list, in_gen_label_align_file_list, hmm_lf0_no_silence_list)\n\n            calculator = IndividualDistortionComp()\n\n            spectral_distortion = calculator.compute_distortion(valid_file_id_list, ref_data_dir, hmm_gen_no_silence_dir, cfg.mgc_ext, cfg.mgc_dim)\n            bap_mse             = calculator.compute_distortion(valid_file_id_list, ref_data_dir, hmm_gen_no_silence_dir, cfg.bap_ext, cfg.bap_dim)\n            f0_mse, vuv_error   = calculator.compute_distortion(valid_file_id_list, ref_data_dir, hmm_gen_no_silence_dir, cfg.lf0_ext, cfg.lf0_dim)\n\n            spectral_distortion *= (10 /numpy.log(10)) * numpy.sqrt(2.0)\n            bap_mse = bap_mse / 10.0\n\n            logger.info(\'Develop: HMM -- MCD: %.3f dB; BAP: %.3f dB; F0: %.3f Hz; VUV: %.3f%%\' %(spectral_distortion, bap_mse, f0_mse, vuv_error*100.))\n\n            spectral_distortion = calculator.compute_distortion(test_file_id_list, ref_data_dir, hmm_gen_no_silence_dir, cfg.mgc_ext, cfg.mgc_dim)\n            bap_mse             = calculator.compute_distortion(test_file_id_list, ref_data_dir, hmm_gen_no_silence_dir, cfg.bap_ext, cfg.bap_dim)\n            f0_mse, vuv_error   = calculator.compute_distortion(test_file_id_list, ref_data_dir, hmm_gen_no_silence_dir, cfg.lf0_ext, cfg.lf0_dim)\n\n            spectral_distortion *= (10 /numpy.log(10)) * numpy.sqrt(2.0)\n            bap_mse = bap_mse / 10.0\n\n            logger.info(\'Test   : HMM -- MCD: %.3f dB; BAP: %.3f dB; F0: %.3f Hz; VUV: %.3f%%\' %(spectral_distortion, bap_mse, f0_mse, vuv_error*100.))\n\nif __name__ == \'__main__\':\n\n\n\n    # these things should be done even before trying to parse the command line\n\n    # create a configuration instance\n    # and get a short name for this instance\n    cfg=configuration.cfg\n\n    # set up logging to use our custom class\n    logging.setLoggerClass(LoggerPlotter)\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n\n    if len(sys.argv) != 2:\n        logger.critical(\'usage: run_dnn.sh [config file name]\')\n        sys.exit(1)\n\n    config_file = sys.argv[1]\n\n    config_file = os.path.abspath(config_file)\n    cfg.configure(config_file)\n\n    if cfg.profile:\n        logger.info(\'profiling is activated\')\n        import cProfile, pstats\n        cProfile.run(\'main_function(cfg)\', \'mainstats\')\n\n        # create a stream for the profiler to write to\n        profiling_output = io.StringIO()\n        p = pstats.Stats(\'mainstats\', stream=profiling_output)\n\n        # print stats to that stream\n        # here we just report the top 10 functions, sorted by total amount of time spent in each\n        p.strip_dirs().sort_stats(\'tottime\').print_stats(10)\n\n        # print the result to the log\n        logger.info(\'---Profiling result follows---\\n%s\' %  profiling_output.getvalue() )\n        profiling_output.close()\n        logger.info(\'---End of profiling result---\')\n\n    else:\n        main_function(cfg)\n\n    sys.exit(0)\n'"
src/work_in_progress/oliver/run_tpdnn.py,0,"b'\nimport pickle\nimport gzip\nimport os, sys, errno\nimport time\nimport math\n\n#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)\nimport numpy\n# we need to explicitly import this in some cases, not sure why this doesn\'t get imported with numpy itself\nimport numpy.distutils.__config__\n# and only after that can we import theano\nimport theano\n\nfrom utils.providers import ListDataProviderWithProjectionIndex, expand_projection_inputs, get_unexpanded_projection_inputs # ListDataProvider\n\nfrom frontend.label_normalisation import HTSLabelNormalisation, XMLLabelNormalisation\nfrom frontend.silence_remover import SilenceRemover\nfrom frontend.silence_remover import trim_silence\nfrom frontend.min_max_norm import MinMaxNormalisation\n#from frontend.acoustic_normalisation import CMPNormalisation\nfrom frontend.acoustic_composition import AcousticComposition\nfrom frontend.parameter_generation import ParameterGeneration\n#from frontend.feature_normalisation_base import FeatureNormBase\nfrom frontend.mean_variance_norm import MeanVarianceNorm\n\n# the new class for label composition and normalisation\nfrom frontend.label_composer import LabelComposer\n\nimport configuration\n\nfrom models.dnn import DNN\nfrom models.tpdnn import TokenProjectionDNN\nfrom models.ms_dnn import MultiStreamDNN\nfrom models.ms_dnn_gv import MultiStreamDNNGv\nfrom models.sdae import StackedDenoiseAutoEncoder\n\nfrom utils.compute_distortion import DistortionComputation, IndividualDistortionComp\nfrom utils.generate import generate_wav\nfrom utils.learn_rates import ExpDecreaseLearningRate\n\n\n#import matplotlib.pyplot as plt\n# our custom logging class that can also plot\n#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot\nfrom logplot.logging_plotting import LoggerPlotter, MultipleSeriesPlot, SingleWeightMatrixPlot\nimport logging # as logging\nimport logging.config\nimport io\n\n\n\n\n## This should always be True -- tidy up later\nexpand_by_minibatch = True\n\nif expand_by_minibatch:\n    proj_type = \'int32\'\nelse:\n    proj_type = theano.config.floatX\n\n\n\n\ndef extract_file_id_list(file_list):\n    file_id_list = []\n    for file_name in file_list:\n        file_id = os.path.basename(os.path.splitext(file_name)[0])\n        file_id_list.append(file_id)\n\n    return  file_id_list\n\ndef read_file_list(file_name):\n\n    logger = logging.getLogger(""read_file_list"")\n\n    file_lists = []\n    fid = open(file_name)\n    for line in fid.readlines():\n        line = line.strip()\n        if len(line) < 1:\n            continue\n        file_lists.append(line)\n    fid.close()\n\n    logger.debug(\'Read file list from %s\' % file_name)\n    return  file_lists\n\n\ndef make_output_file_list(out_dir, in_file_lists):\n    out_file_lists = []\n\n    for in_file_name in in_file_lists:\n        file_id = os.path.basename(in_file_name)\n        out_file_name = out_dir + \'/\' + file_id\n        out_file_lists.append(out_file_name)\n\n    return  out_file_lists\n\ndef prepare_file_path_list(file_id_list, file_dir, file_extension, new_dir_switch=True):\n    if not os.path.exists(file_dir) and new_dir_switch:\n        os.makedirs(file_dir)\n    file_name_list = []\n    for file_id in file_id_list:\n        file_name = file_dir + \'/\' + file_id + file_extension\n        file_name_list.append(file_name)\n\n    return  file_name_list\n\n\n\ndef visualize_dnn(dnn):\n\n    layer_num = len(dnn.params) / 2     ## including input and output\n\n    for i in range(layer_num):\n        fig_name = \'Activation weights W\' + str(i)\n        fig_title = \'Activation weights of W\' + str(i)\n        xlabel = \'Neuron index of hidden layer \' + str(i)\n        ylabel = \'Neuron index of hidden layer \' + str(i+1)\n        if i == 0:\n            xlabel = \'Input feature index\'\n        if i == layer_num-1:\n            ylabel = \'Output feature index\'\n\n        logger.create_plot(fig_name, SingleWeightMatrixPlot)\n        plotlogger.add_plot_point(fig_name, fig_name, dnn.params[i*2].get_value(borrow=True).T)\n        plotlogger.save_plot(fig_name, title=fig_name, xlabel=xlabel, ylabel=ylabel)\n\n\n\n\n## Function for training projection and non-projection parts at same time\ndef train_DNN(train_xy_file_list, valid_xy_file_list, \\\n              nnets_file_name, n_ins, n_outs, ms_outs, hyper_params, buffer_size, plot=False):\n\n    # get loggers for this function\n    # this one writes to both console and file\n    logger = logging.getLogger(""main.train_DNN"")\n    logger.debug(\'Starting train_DNN\')\n\n    if plot:\n        # this one takes care of plotting duties\n        plotlogger = logging.getLogger(""plotting"")\n        # create an (empty) plot of training convergence, ready to receive data points\n        logger.create_plot(\'training convergence\',MultipleSeriesPlot)\n\n    try:\n        assert numpy.sum(ms_outs) == n_outs\n    except AssertionError:\n        logger.critical(\'the summation of multi-stream outputs does not equal to %d\' %(n_outs))\n        raise\n\n    ####parameters#####\n    finetune_lr     = float(hyper_params[\'learning_rate\'])\n    training_epochs = int(hyper_params[\'training_epochs\'])\n    batch_size      = int(hyper_params[\'batch_size\'])\n    l1_reg          = float(hyper_params[\'l1_reg\'])\n    l2_reg          = float(hyper_params[\'l2_reg\'])\n    private_l2_reg  = float(hyper_params[\'private_l2_reg\'])\n    warmup_epoch    = int(hyper_params[\'warmup_epoch\'])\n    momentum        = float(hyper_params[\'momentum\'])\n    warmup_momentum = float(hyper_params[\'warmup_momentum\'])\n\n    hidden_layers_sizes = hyper_params[\'hidden_layers_sizes\']\n\n    stream_weights       = hyper_params[\'stream_weights\']\n    private_hidden_sizes = hyper_params[\'private_hidden_sizes\']\n\n    buffer_utt_size = buffer_size\n    early_stop_epoch = int(hyper_params[\'early_stop_epochs\'])\n\n    hidden_activation = hyper_params[\'hidden_activation\']\n    output_activation = hyper_params[\'output_activation\']\n\n    stream_lr_weights = hyper_params[\'stream_lr_weights\']\n    use_private_hidden = hyper_params[\'use_private_hidden\']\n\n    model_type = hyper_params[\'model_type\']\n\n    index_to_project = hyper_params[\'index_to_project\']\n    projection_insize = hyper_params[\'projection_insize\']\n    projection_outsize = hyper_params[\'projection_outsize\']\n\n    ## use a switch to turn on pretraining\n    ## pretraining may not help too much, if this case, we turn it off to save time\n    do_pretraining = hyper_params[\'do_pretraining\']\n    pretraining_epochs = int(hyper_params[\'pretraining_epochs\'])\n    pretraining_lr = float(hyper_params[\'pretraining_lr\'])\n    initial_projection_distrib = hyper_params[\'initial_projection_distrib\']\n\n    buffer_size = int(buffer_size / batch_size) * batch_size\n\n    ###################\n    (train_x_file_list, train_y_file_list) = train_xy_file_list\n    (valid_x_file_list, valid_y_file_list) = valid_xy_file_list\n\n    logger.debug(\'Creating training   data provider\')\n    train_data_reader = ListDataProviderWithProjectionIndex(x_file_list = train_x_file_list, y_file_list = train_y_file_list, n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, shuffle = True, index_to_project=index_to_project, projection_insize=projection_insize, indexes_only=expand_by_minibatch)\n\n    logger.debug(\'Creating validation data provider\')\n    valid_data_reader = ListDataProviderWithProjectionIndex(x_file_list = valid_x_file_list, y_file_list = valid_y_file_list, n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, shuffle = False, index_to_project=index_to_project, projection_insize=projection_insize, indexes_only=expand_by_minibatch)\n\n    shared_train_set_xy, temp_train_set_x, temp_train_set_x_proj, temp_train_set_y = train_data_reader.load_next_partition_with_projection()\n    train_set_x, train_set_x_proj, train_set_y = shared_train_set_xy\n    shared_valid_set_xy, temp_valid_set_x, temp_valid_set_x_proj, temp_valid_set_y = valid_data_reader.load_next_partition_with_projection()\n    valid_set_x, valid_set_x_proj, valid_set_y = shared_valid_set_xy\n    train_data_reader.reset()\n    valid_data_reader.reset()\n\n    ##temporally we use the training set as pretrain_set_x.\n    ##we need to support any data for pretraining\n    pretrain_set_x = train_set_x\n\n    # numpy random generator\n    numpy_rng = numpy.random.RandomState(123)\n    logger.info(\'building the model\')\n\n\n    dnn_model = None\n    pretrain_fn = None  ## not all the model support pretraining right now\n    train_fn = None\n    valid_fn = None\n    valid_model = None ## valid_fn and valid_model are the same. reserve to computer multi-stream distortion\n    if model_type == \'DNN\':\n        dnn_model = DNN(numpy_rng=numpy_rng, n_ins=n_ins, n_outs = n_outs,\n                        l1_reg = l1_reg, l2_reg = l2_reg,\n                         hidden_layers_sizes = hidden_layers_sizes,\n                          hidden_activation = hidden_activation,\n                          output_activation = output_activation)\n        train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y), batch_size=batch_size)\n\n    elif model_type == \'TPDNN\':\n        dnn_model = TokenProjectionDNN(numpy_rng=numpy_rng, n_ins=n_ins, n_outs = n_outs,\n                        l1_reg = l1_reg, l2_reg = l2_reg,\n                         hidden_layers_sizes = hidden_layers_sizes,\n                          hidden_activation = hidden_activation,\n                          output_activation = output_activation,\n                          projection_insize=projection_insize, projection_outsize=projection_outsize,\n                          expand_by_minibatch=expand_by_minibatch, initial_projection_distrib=initial_projection_distrib)\n        train_all_fn, train_subword_fn, train_word_fn, infer_projections_fn, valid_fn, valid_score_i = \\\n                    dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_x_proj, train_set_y),\n                    (valid_set_x, valid_set_x_proj, valid_set_y), batch_size=batch_size)\n\n    elif model_type == \'SDAE\':\n        ##basic model is ready.\n        ##if corruption levels is set to zero. it becomes normal autoencoder\n        dnn_model = StackedDenoiseAutoEncoder(numpy_rng=numpy_rng, n_ins=n_ins, n_outs = n_outs,\n                          l1_reg = l1_reg, l2_reg = l2_reg,\n                          hidden_layers_sizes = hidden_layers_sizes)\n\n        if do_pretraining:\n            pretraining_fn = dnn_model.pretraining_functions(pretrain_set_x, batch_size)\n\n        train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y), batch_size=batch_size)\n\n    elif model_type == \'MSDNN\': ##model is ready, but the hyper-parameters are not optimised.\n        dnn_model = MultiStreamDNN(numpy_rng=numpy_rng, n_ins=n_ins, ms_outs=ms_outs,\n                          l1_reg = l1_reg, l2_reg = l2_reg,\n                          hidden_layers_sizes = hidden_layers_sizes,\n                          stream_weights = stream_weights,\n                          hidden_activation = hidden_activation,\n                          output_activation = output_activation)\n\n        train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y),\n                    batch_size=batch_size, lr_weights = stream_lr_weights)\n    elif model_type == \'MSDNN_GV\':  ## not fully ready\n        dnn_model = MultiStreamDNNGv(numpy_rng=numpy_rng, n_ins=n_ins, ms_outs=ms_outs,\n                          l1_reg = l1_reg, l2_reg = l2_reg,\n                          hidden_layers_sizes = hidden_layers_sizes,\n                          stream_weights = stream_weights,\n                          hidden_activation = hidden_activation,\n                          output_activation = output_activation)\n\n        train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y),\n                    batch_size=batch_size, lr_weights = stream_lr_weights)\n    else:\n        logger.critical(\'%s type NN model is not supported!\' %(model_type))\n        raise\n\n    ## if pretraining is supported in one model, add the switch here\n    ## be careful to use autoencoder for pretraining here:\n    ## for SDAE, currently only sigmoid function is supported in the hidden layers, as our input is scaled to [0, 1]\n    ## however, tanh works better and converge fast in finetuning\n    ##\n    ## Will extend this soon...\n    if do_pretraining and model_type == \'SDAE\':\n        logger.info(\'pretraining the %s model\' %(model_type))\n\n        corruption_level = 0.0\n        ## in SDAE we do layer-wise pretraining using autoencoders\n        for i in range(dnn_model.n_layers):\n            for epoch in range(pretraining_epochs):\n                sub_start_time = time.clock()\n\n                pretrain_loss = []\n                while (not train_data_reader.is_finish()):\n                    shared_train_set_xy, temp_train_set_x, temp_train_set_y = train_data_reader.load_next_partition()\n                    pretrain_set_x.set_value(numpy.asarray(temp_train_set_x, dtype=theano.config.floatX), borrow=True)\n\n                    n_train_batches = pretrain_set_x.get_value().shape[0] / batch_size\n\n                    for batch_index in range(n_train_batches):\n                        pretrain_loss.append(pretraining_fn[i](index=batch_index,\n                                                               corruption=corruption_level,\n                                                               learning_rate=pretraining_lr))\n\n                sub_end_time = time.clock()\n                logger.info(\'Pre-training layer %i, epoch %d, cost %s, time spent%.2f\' % (i+1, epoch+1, numpy.mean(pretrain_loss), (sub_end_time - sub_start_time)))\n                train_data_reader.reset()\n\n\n    logger.info(\'fine-tuning the %s model\' %(model_type))\n\n    start_time = time.clock()\n\n    best_dnn_model = dnn_model\n    best_validation_loss = sys.float_info.max\n    previous_loss = sys.float_info.max\n\n    early_stop = 0\n    epoch = 0\n    previous_finetune_lr = finetune_lr\n    while (epoch < training_epochs):\n        epoch = epoch + 1\n\n        current_momentum = momentum\n        current_finetune_lr = finetune_lr\n        if epoch <= warmup_epoch:\n            current_finetune_lr = finetune_lr\n            current_momentum = warmup_momentum\n        else:\n            current_finetune_lr = previous_finetune_lr * 0.5\n\n        previous_finetune_lr = current_finetune_lr\n\n        train_error = []\n        sub_start_time = time.clock()\n\n        while (not train_data_reader.is_finish()):\n            shared_train_set_xy, temp_train_set_x, temp_train_set_x_proj, temp_train_set_y = train_data_reader.load_next_partition_with_projection()\n            train_set_x.set_value(numpy.asarray(temp_train_set_x, dtype=theano.config.floatX), borrow=True)\n            train_set_x_proj.set_value(numpy.asarray(temp_train_set_x_proj, dtype=proj_type), borrow=True)\n            train_set_y.set_value(numpy.asarray(temp_train_set_y, dtype=theano.config.floatX), borrow=True)\n\n            n_train_batches = train_set_x.get_value().shape[0] / batch_size\n\n            logger.debug(\'this partition: %d frames (divided into %d batches of size %d)\' %(train_set_x.get_value(borrow=True).shape[0], n_train_batches, batch_size) )\n\n            for minibatch_index in range(n_train_batches):\n                this_train_error = train_all_fn(minibatch_index, current_finetune_lr, current_momentum)\n                train_error.append(this_train_error)\n\n                if numpy.isnan(this_train_error):\n                    logger.warning(\'training error over minibatch %d of %d was %s\' % (minibatch_index+1,n_train_batches,this_train_error) )\n\n        train_data_reader.reset()\n\n        ## osw -- getting validation error from a forward pass in a single batch\n        ##        exausts memory when using 20k projected vocab -- also use minibatches\n        logger.debug(\'calculating validation loss\')\n        valid_error = []\n        n_valid_batches = valid_set_x.get_value().shape[0] / batch_size\n        for minibatch_index in range(n_valid_batches):\n            v_loss = valid_score_i(minibatch_index)\n            valid_error.append(v_loss)\n\n        this_validation_loss = numpy.mean(valid_error)\n\n        # this has a possible bias if the minibatches were not all of identical size\n        # but it should not be siginficant if minibatches are small\n        this_train_valid_loss = numpy.mean(train_error)\n\n        sub_end_time = time.clock()\n\n        loss_difference = this_validation_loss - previous_loss\n\n        logger.info(\'BASIC epoch %i, validation error %f, train error %f  time spent %.2f\' %(epoch, this_validation_loss, this_train_valid_loss, (sub_end_time - sub_start_time)))\n        if plot:\n            plotlogger.add_plot_point(\'training convergence\',\'validation set\',(epoch,this_validation_loss))\n            plotlogger.add_plot_point(\'training convergence\',\'training set\',(epoch,this_train_valid_loss))\n            plotlogger.save_plot(\'training convergence\',title=\'Progress of training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n\n        if this_validation_loss < best_validation_loss:\n            best_dnn_model = dnn_model\n            best_validation_loss = this_validation_loss\n            logger.debug(\'validation loss decreased, so saving model\')\n            early_stop = 0\n        else:\n            logger.debug(\'validation loss did not improve\')\n            dbn = best_dnn_model\n            early_stop += 1\n\n        if early_stop > early_stop_epoch:\n            # too many consecutive epochs without surpassing the best model\n            logger.debug(\'stopping early\')\n            break\n\n        if math.isnan(this_validation_loss):\n            break\n\n        previous_loss = this_validation_loss\n\n\n        ### Save projection values:\n        if cfg.hyper_params[\'model_type\'] == \'TPDNN\':\n            if not os.path.isdir(cfg.projection_weights_output_dir):\n                os.mkdir(cfg.projection_weights_output_dir)\n            weights = dnn_model.get_projection_weights()\n            fname = os.path.join(cfg.projection_weights_output_dir, \'proj_BASIC_epoch_%s\'%(epoch))\n            numpy.savetxt(fname, weights)\n\n    end_time = time.clock()\n    pickle.dump(best_dnn_model, open(nnets_file_name, \'wb\'))\n\n    logger.info(\'overall  training time: %.2fm validation error %f\' % ((end_time - start_time) / 60., best_validation_loss))\n\n    if plot:\n        plotlogger.save_plot(\'training convergence\',title=\'Final training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n\n\n\n\n## Function for training all model on train data as well as simultaneously\n## inferring proj weights on dev data.\n# in each epoch do:\n#   train_all_fn()\n#   infer_projections_fn()    ## <-- updates proj for devset and gives validation loss\ndef train_DNN_and_traindev_projections(train_xy_file_list, valid_xy_file_list, \\\n              nnets_file_name, n_ins, n_outs, ms_outs, hyper_params, buffer_size, plot=False):\n\n    # get loggers for this function\n    # this one writes to both console and file\n    logger = logging.getLogger(""main.train_DNN"")\n    logger.debug(\'Starting train_DNN\')\n\n    if plot:\n        # this one takes care of plotting duties\n        plotlogger = logging.getLogger(""plotting"")\n        # create an (empty) plot of training convergence, ready to receive data points\n        logger.create_plot(\'training convergence\',MultipleSeriesPlot)\n\n    try:\n        assert numpy.sum(ms_outs) == n_outs\n    except AssertionError:\n        logger.critical(\'the summation of multi-stream outputs does not equal to %d\' %(n_outs))\n        raise\n\n    ####parameters#####\n    finetune_lr     = float(hyper_params[\'learning_rate\'])\n    training_epochs = int(hyper_params[\'training_epochs\'])\n    batch_size      = int(hyper_params[\'batch_size\'])\n    l1_reg          = float(hyper_params[\'l1_reg\'])\n    l2_reg          = float(hyper_params[\'l2_reg\'])\n    private_l2_reg  = float(hyper_params[\'private_l2_reg\'])\n    warmup_epoch    = int(hyper_params[\'warmup_epoch\'])\n    momentum        = float(hyper_params[\'momentum\'])\n    warmup_momentum = float(hyper_params[\'warmup_momentum\'])\n\n    hidden_layers_sizes = hyper_params[\'hidden_layers_sizes\']\n\n    stream_weights       = hyper_params[\'stream_weights\']\n    private_hidden_sizes = hyper_params[\'private_hidden_sizes\']\n\n    buffer_utt_size = buffer_size\n    early_stop_epoch = int(hyper_params[\'early_stop_epochs\'])\n\n    hidden_activation = hyper_params[\'hidden_activation\']\n    output_activation = hyper_params[\'output_activation\']\n\n    stream_lr_weights = hyper_params[\'stream_lr_weights\']\n    use_private_hidden = hyper_params[\'use_private_hidden\']\n\n    model_type = hyper_params[\'model_type\']\n\n    index_to_project = hyper_params[\'index_to_project\']\n    projection_insize = hyper_params[\'projection_insize\']\n    projection_outsize = hyper_params[\'projection_outsize\']\n\n    ## use a switch to turn on pretraining\n    ## pretraining may not help too much, if this case, we turn it off to save time\n    do_pretraining = hyper_params[\'do_pretraining\']\n    pretraining_epochs = int(hyper_params[\'pretraining_epochs\'])\n    pretraining_lr = float(hyper_params[\'pretraining_lr\'])\n    initial_projection_distrib = hyper_params[\'initial_projection_distrib\']\n\n    buffer_size = int(buffer_size / batch_size) * batch_size\n\n    ###################\n    (train_x_file_list, train_y_file_list) = train_xy_file_list\n    (valid_x_file_list, valid_y_file_list) = valid_xy_file_list\n\n    logger.debug(\'Creating training   data provider\')\n    train_data_reader = ListDataProviderWithProjectionIndex(x_file_list = train_x_file_list, y_file_list = train_y_file_list, n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, shuffle = True, index_to_project=index_to_project, projection_insize=projection_insize, indexes_only=expand_by_minibatch)\n\n    logger.debug(\'Creating validation data provider\')\n    valid_data_reader = ListDataProviderWithProjectionIndex(x_file_list = valid_x_file_list, y_file_list = valid_y_file_list, n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, shuffle = False, index_to_project=index_to_project, projection_insize=projection_insize, indexes_only=expand_by_minibatch)\n\n    shared_train_set_xy, temp_train_set_x, temp_train_set_x_proj, temp_train_set_y = train_data_reader.load_next_partition_with_projection()\n    train_set_x, train_set_x_proj, train_set_y = shared_train_set_xy\n    shared_valid_set_xy, temp_valid_set_x, temp_valid_set_x_proj, temp_valid_set_y = valid_data_reader.load_next_partition_with_projection()\n    valid_set_x, valid_set_x_proj, valid_set_y = shared_valid_set_xy\n    train_data_reader.reset()\n    valid_data_reader.reset()\n\n    ##temporally we use the training set as pretrain_set_x.\n    ##we need to support any data for pretraining\n    pretrain_set_x = train_set_x\n\n    # numpy random generator\n    numpy_rng = numpy.random.RandomState(123)\n    logger.info(\'building the model\')\n\n\n    dnn_model = None\n    pretrain_fn = None  ## not all the model support pretraining right now\n    train_fn = None\n    valid_fn = None\n    valid_model = None ## valid_fn and valid_model are the same. reserve to computer multi-stream distortion\n    if model_type == \'DNN\':\n        dnn_model = DNN(numpy_rng=numpy_rng, n_ins=n_ins, n_outs = n_outs,\n                        l1_reg = l1_reg, l2_reg = l2_reg,\n                         hidden_layers_sizes = hidden_layers_sizes,\n                          hidden_activation = hidden_activation,\n                          output_activation = output_activation)\n        train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y), batch_size=batch_size)\n\n    elif model_type == \'TPDNN\':\n        dnn_model = TokenProjectionDNN(numpy_rng=numpy_rng, n_ins=n_ins, n_outs = n_outs,\n                        l1_reg = l1_reg, l2_reg = l2_reg,\n                         hidden_layers_sizes = hidden_layers_sizes,\n                          hidden_activation = hidden_activation,\n                          output_activation = output_activation,\n                          projection_insize=projection_insize, projection_outsize=projection_outsize,\n                          expand_by_minibatch=expand_by_minibatch, initial_projection_distrib=initial_projection_distrib)\n        train_all_fn, train_subword_fn, train_word_fn, infer_projections_fn, valid_fn, valid_score_i = \\\n                    dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_x_proj, train_set_y),\n                    (valid_set_x, valid_set_x_proj, valid_set_y), batch_size=batch_size)\n\n    elif model_type == \'SDAE\':\n        ##basic model is ready.\n        ##if corruption levels is set to zero. it becomes normal autoencoder\n        dnn_model = StackedDenoiseAutoEncoder(numpy_rng=numpy_rng, n_ins=n_ins, n_outs = n_outs,\n                          l1_reg = l1_reg, l2_reg = l2_reg,\n                          hidden_layers_sizes = hidden_layers_sizes)\n\n        if do_pretraining:\n            pretraining_fn = dnn_model.pretraining_functions(pretrain_set_x, batch_size)\n\n        train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y), batch_size=batch_size)\n\n    elif model_type == \'MSDNN\': ##model is ready, but the hyper-parameters are not optimised.\n        dnn_model = MultiStreamDNN(numpy_rng=numpy_rng, n_ins=n_ins, ms_outs=ms_outs,\n                          l1_reg = l1_reg, l2_reg = l2_reg,\n                          hidden_layers_sizes = hidden_layers_sizes,\n                          stream_weights = stream_weights,\n                          hidden_activation = hidden_activation,\n                          output_activation = output_activation)\n\n        train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y),\n                    batch_size=batch_size, lr_weights = stream_lr_weights)\n    elif model_type == \'MSDNN_GV\':  ## not fully ready\n        dnn_model = MultiStreamDNNGv(numpy_rng=numpy_rng, n_ins=n_ins, ms_outs=ms_outs,\n                          l1_reg = l1_reg, l2_reg = l2_reg,\n                          hidden_layers_sizes = hidden_layers_sizes,\n                          stream_weights = stream_weights,\n                          hidden_activation = hidden_activation,\n                          output_activation = output_activation)\n\n        train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y),\n                    batch_size=batch_size, lr_weights = stream_lr_weights)\n    else:\n        logger.critical(\'%s type NN model is not supported!\' %(model_type))\n        raise\n\n    ## if pretraining is supported in one model, add the switch here\n    ## be careful to use autoencoder for pretraining here:\n    ## for SDAE, currently only sigmoid function is supported in the hidden layers, as our input is scaled to [0, 1]\n    ## however, tanh works better and converge fast in finetuning\n    ##\n    ## Will extend this soon...\n    if do_pretraining and model_type == \'SDAE\':\n        logger.info(\'pretraining the %s model\' %(model_type))\n\n        corruption_level = 0.0\n        ## in SDAE we do layer-wise pretraining using autoencoders\n        for i in range(dnn_model.n_layers):\n            for epoch in range(pretraining_epochs):\n                sub_start_time = time.clock()\n\n                pretrain_loss = []\n                while (not train_data_reader.is_finish()):\n                    shared_train_set_xy, temp_train_set_x, temp_train_set_y = train_data_reader.load_next_partition()\n                    pretrain_set_x.set_value(numpy.asarray(temp_train_set_x, dtype=theano.config.floatX), borrow=True)\n\n                    n_train_batches = pretrain_set_x.get_value().shape[0] / batch_size\n\n                    for batch_index in range(n_train_batches):\n                        pretrain_loss.append(pretraining_fn[i](index=batch_index,\n                                                               corruption=corruption_level,\n                                                               learning_rate=pretraining_lr))\n\n                sub_end_time = time.clock()\n                logger.info(\'Pre-training layer %i, epoch %d, cost %s, time spent%.2f\' % (i+1, epoch+1, numpy.mean(pretrain_loss), (sub_end_time - sub_start_time)))\n                train_data_reader.reset()\n\n\n    logger.info(\'fine-tuning the %s model\' %(model_type))\n\n    start_time = time.clock()\n\n    best_dnn_model = dnn_model\n    best_validation_loss = sys.float_info.max\n    previous_loss = sys.float_info.max\n\n    early_stop = 0\n    epoch = 0\n    previous_finetune_lr = finetune_lr\n\n    ##dnn_model.zero_projection_weights()\n\n    while (epoch < training_epochs):\n        epoch = epoch + 1\n\n        current_momentum = momentum\n        current_finetune_lr = finetune_lr\n        if epoch <= warmup_epoch:\n            current_finetune_lr = finetune_lr\n            current_momentum = warmup_momentum\n        else:\n            current_finetune_lr = previous_finetune_lr * 0.5\n\n        previous_finetune_lr = current_finetune_lr\n\n        train_error = []\n        sub_start_time = time.clock()\n\n        while (not train_data_reader.is_finish()):\n            shared_train_set_xy, temp_train_set_x, temp_train_set_x_proj, temp_train_set_y = train_data_reader.load_next_partition_with_projection()\n            train_set_x.set_value(numpy.asarray(temp_train_set_x, dtype=theano.config.floatX), borrow=True)\n            train_set_x_proj.set_value(numpy.asarray(temp_train_set_x_proj, dtype=proj_type), borrow=True)\n            train_set_y.set_value(numpy.asarray(temp_train_set_y, dtype=theano.config.floatX), borrow=True)\n\n            n_train_batches = train_set_x.get_value().shape[0] / batch_size\n\n            logger.debug(\'this partition: %d frames (divided into %d batches of size %d)\' %(train_set_x.get_value(borrow=True).shape[0], n_train_batches, batch_size) )\n\n            for minibatch_index in range(n_train_batches):\n                this_train_error = train_all_fn(minibatch_index, current_finetune_lr, current_momentum)\n                train_error.append(this_train_error)\n\n                if numpy.isnan(this_train_error):\n                    logger.warning(\'training error over minibatch %d of %d was %s\' % (minibatch_index+1,n_train_batches,this_train_error) )\n\n        train_data_reader.reset()\n\n\n        ## infer validation weights before getting validation error:\n        ## osw -- inferring word reps on validation set in a forward pass in a single batch\n        ##        exausts memory when using 20k projected vocab -- also use minibatches\n        logger.debug(\'infer word representations for validation set\')\n        valid_error = []\n        n_valid_batches = valid_set_x.get_value().shape[0] / batch_size\n        for minibatch_index in range(n_valid_batches):\n            v_loss = infer_projections_fn(minibatch_index, current_finetune_lr, current_momentum)\n            valid_error.append(v_loss)\n\n        ## this function also give us validation loss:\n        this_validation_loss = numpy.mean(valid_error)\n\n\n        \'\'\'\n        ## osw -- getting validation error from a forward pass in a single batch\n        ##        exausts memory when using 20k projected vocab -- also use minibatches\n        logger.debug(\'calculating validation loss\')\n        valid_error = []\n        n_valid_batches = valid_set_x.get_value().shape[0] / batch_size\n        for minibatch_index in xrange(n_valid_batches):\n            v_loss = valid_score_i(minibatch_index)\n            valid_error.append(v_loss)\n\n        this_validation_loss = numpy.mean(valid_error)\n        \'\'\'\n\n\n        # this has a possible bias if the minibatches were not all of identical size\n        # but it should not be siginficant if minibatches are small\n        this_train_valid_loss = numpy.mean(train_error)\n\n        sub_end_time = time.clock()\n\n        loss_difference = this_validation_loss - previous_loss\n\n        logger.info(\'BASIC epoch %i, validation error %f, train error %f  time spent %.2f\' %(epoch, this_validation_loss, this_train_valid_loss, (sub_end_time - sub_start_time)))\n        if plot:\n            plotlogger.add_plot_point(\'training convergence\',\'validation set\',(epoch,this_validation_loss))\n            plotlogger.add_plot_point(\'training convergence\',\'training set\',(epoch,this_train_valid_loss))\n            plotlogger.save_plot(\'training convergence\',title=\'Progress of training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n\n        if this_validation_loss < best_validation_loss:\n            best_dnn_model = dnn_model\n            best_validation_loss = this_validation_loss\n            logger.debug(\'validation loss decreased, so saving model\')\n            early_stop = 0\n        else:\n            logger.debug(\'validation loss did not improve\')\n            dbn = best_dnn_model\n            early_stop += 1\n\n        if early_stop > early_stop_epoch:\n            # too many consecutive epochs without surpassing the best model\n            logger.debug(\'stopping early\')\n            break\n\n        if math.isnan(this_validation_loss):\n            break\n\n        previous_loss = this_validation_loss\n\n\n        ### Save projection values:\n        if cfg.hyper_params[\'model_type\'] == \'TPDNN\':\n            if not os.path.isdir(cfg.projection_weights_output_dir):\n                os.mkdir(cfg.projection_weights_output_dir)\n            weights = dnn_model.get_projection_weights()\n            fname = os.path.join(cfg.projection_weights_output_dir, \'proj_BASIC_epoch_%s\'%(epoch))\n            numpy.savetxt(fname, weights)\n\n    end_time = time.clock()\n    pickle.dump(best_dnn_model, open(nnets_file_name, \'wb\'))\n\n    logger.info(\'overall  training time: %.2fm validation error %f\' % ((end_time - start_time) / 60., best_validation_loss))\n\n    if plot:\n        plotlogger.save_plot(\'training convergence\',title=\'Final training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n\n\n\n\n\n## Function for training the non-projection part only\ndef train_basic_DNN(train_xy_file_list, valid_xy_file_list, \\\n              nnets_file_name, n_ins, n_outs, ms_outs, hyper_params, buffer_size, plot=False):\n\n    # get loggers for this function\n    # this one writes to both console and file\n    logger = logging.getLogger(""main.train_DNN"")\n    logger.debug(\'Starting train_DNN\')\n\n    if plot:\n        # this one takes care of plotting duties\n        plotlogger = logging.getLogger(""plotting"")\n        # create an (empty) plot of training convergence, ready to receive data points\n        logger.create_plot(\'training convergence\',MultipleSeriesPlot)\n\n    try:\n        assert numpy.sum(ms_outs) == n_outs\n    except AssertionError:\n        logger.critical(\'the summation of multi-stream outputs does not equal to %d\' %(n_outs))\n        raise\n\n    ####parameters#####\n    finetune_lr     = float(hyper_params[\'learning_rate\'])\n    training_epochs = int(hyper_params[\'training_epochs\'])\n    batch_size      = int(hyper_params[\'batch_size\'])\n    l1_reg          = float(hyper_params[\'l1_reg\'])\n    l2_reg          = float(hyper_params[\'l2_reg\'])\n    private_l2_reg  = float(hyper_params[\'private_l2_reg\'])\n    warmup_epoch    = int(hyper_params[\'warmup_epoch\'])\n    momentum        = float(hyper_params[\'momentum\'])\n    warmup_momentum = float(hyper_params[\'warmup_momentum\'])\n\n    hidden_layers_sizes = hyper_params[\'hidden_layers_sizes\']\n\n    stream_weights       = hyper_params[\'stream_weights\']\n    private_hidden_sizes = hyper_params[\'private_hidden_sizes\']\n\n    buffer_utt_size = buffer_size\n    early_stop_epoch = int(hyper_params[\'early_stop_epochs\'])\n\n    hidden_activation = hyper_params[\'hidden_activation\']\n    output_activation = hyper_params[\'output_activation\']\n\n    stream_lr_weights = hyper_params[\'stream_lr_weights\']\n    use_private_hidden = hyper_params[\'use_private_hidden\']\n\n    model_type = hyper_params[\'model_type\']\n\n    index_to_project = hyper_params[\'index_to_project\']\n    projection_insize = hyper_params[\'projection_insize\']\n    projection_outsize = hyper_params[\'projection_outsize\']\n\n    ## use a switch to turn on pretraining\n    ## pretraining may not help too much, if this case, we turn it off to save time\n    do_pretraining = hyper_params[\'do_pretraining\']\n    pretraining_epochs = int(hyper_params[\'pretraining_epochs\'])\n    pretraining_lr = float(hyper_params[\'pretraining_lr\'])\n    initial_projection_distrib = hyper_params[\'initial_projection_distrib\']\n\n    buffer_size = int(buffer_size / batch_size) * batch_size\n\n    ###################\n    (train_x_file_list, train_y_file_list) = train_xy_file_list\n    (valid_x_file_list, valid_y_file_list) = valid_xy_file_list\n\n    logger.debug(\'Creating training   data provider\')\n    train_data_reader = ListDataProviderWithProjectionIndex(x_file_list = train_x_file_list, y_file_list = train_y_file_list, n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, shuffle = True, index_to_project=index_to_project, projection_insize=projection_insize, indexes_only=expand_by_minibatch)\n\n    logger.debug(\'Creating validation data provider\')\n    valid_data_reader = ListDataProviderWithProjectionIndex(x_file_list = valid_x_file_list, y_file_list = valid_y_file_list, n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, shuffle = False, index_to_project=index_to_project, projection_insize=projection_insize, indexes_only=expand_by_minibatch)\n\n    shared_train_set_xy, temp_train_set_x, temp_train_set_x_proj, temp_train_set_y = train_data_reader.load_next_partition_with_projection()\n    train_set_x, train_set_x_proj, train_set_y = shared_train_set_xy\n    shared_valid_set_xy, temp_valid_set_x, temp_valid_set_x_proj, temp_valid_set_y = valid_data_reader.load_next_partition_with_projection()\n    valid_set_x, valid_set_x_proj, valid_set_y = shared_valid_set_xy\n    train_data_reader.reset()\n    valid_data_reader.reset()\n\n    ##temporally we use the training set as pretrain_set_x.\n    ##we need to support any data for pretraining\n    pretrain_set_x = train_set_x\n\n    # numpy random generator\n    numpy_rng = numpy.random.RandomState(123)\n    logger.info(\'building the model\')\n\n\n    dnn_model = None\n    pretrain_fn = None  ## not all the model support pretraining right now\n    train_fn = None\n    valid_fn = None\n    valid_model = None ## valid_fn and valid_model are the same. reserve to computer multi-stream distortion\n    if model_type == \'DNN\':\n        dnn_model = DNN(numpy_rng=numpy_rng, n_ins=n_ins, n_outs = n_outs,\n                        l1_reg = l1_reg, l2_reg = l2_reg,\n                         hidden_layers_sizes = hidden_layers_sizes,\n                          hidden_activation = hidden_activation,\n                          output_activation = output_activation)\n        train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y), batch_size=batch_size)\n\n    elif model_type == \'TPDNN\':\n\n        dnn_model = TokenProjectionDNN(numpy_rng=numpy_rng, n_ins=n_ins, n_outs = n_outs,\n                        l1_reg = l1_reg, l2_reg = l2_reg,\n                         hidden_layers_sizes = hidden_layers_sizes,\n                          hidden_activation = hidden_activation,\n                          output_activation = output_activation,\n                          projection_insize=projection_insize, projection_outsize=projection_outsize,\n                          expand_by_minibatch=expand_by_minibatch, initial_projection_distrib=initial_projection_distrib)\n        train_all_fn, train_subword_fn, train_word_fn, infer_projections_fn, valid_fn, valid_score_i = \\\n                    dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_x_proj, train_set_y),\n                    (valid_set_x, valid_set_x_proj, valid_set_y), batch_size=batch_size)\n\n    elif model_type == \'SDAE\':\n        ##basic model is ready.\n        ##if corruption levels is set to zero. it becomes normal autoencoder\n        dnn_model = StackedDenoiseAutoEncoder(numpy_rng=numpy_rng, n_ins=n_ins, n_outs = n_outs,\n                          l1_reg = l1_reg, l2_reg = l2_reg,\n                          hidden_layers_sizes = hidden_layers_sizes)\n\n        if do_pretraining:\n            pretraining_fn = dnn_model.pretraining_functions(pretrain_set_x, batch_size)\n\n        train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y), batch_size=batch_size)\n\n    elif model_type == \'MSDNN\': ##model is ready, but the hyper-parameters are not optimised.\n        dnn_model = MultiStreamDNN(numpy_rng=numpy_rng, n_ins=n_ins, ms_outs=ms_outs,\n                          l1_reg = l1_reg, l2_reg = l2_reg,\n                          hidden_layers_sizes = hidden_layers_sizes,\n                          stream_weights = stream_weights,\n                          hidden_activation = hidden_activation,\n                          output_activation = output_activation)\n\n        train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y),\n                    batch_size=batch_size, lr_weights = stream_lr_weights)\n    elif model_type == \'MSDNN_GV\':  ## not fully ready\n        dnn_model = MultiStreamDNNGv(numpy_rng=numpy_rng, n_ins=n_ins, ms_outs=ms_outs,\n                          l1_reg = l1_reg, l2_reg = l2_reg,\n                          hidden_layers_sizes = hidden_layers_sizes,\n                          stream_weights = stream_weights,\n                          hidden_activation = hidden_activation,\n                          output_activation = output_activation)\n\n        train_fn, valid_fn = dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_y), (valid_set_x, valid_set_y),\n                    batch_size=batch_size, lr_weights = stream_lr_weights)\n    else:\n        logger.critical(\'%s type NN model is not supported!\' %(model_type))\n        raise\n\n    ## if pretraining is supported in one model, add the switch here\n    ## be careful to use autoencoder for pretraining here:\n    ## for SDAE, currently only sigmoid function is supported in the hidden layers, as our input is scaled to [0, 1]\n    ## however, tanh works better and converge fast in finetuning\n    ##\n    ## Will extend this soon...\n    if do_pretraining and model_type == \'SDAE\':\n        logger.info(\'pretraining the %s model\' %(model_type))\n\n        corruption_level = 0.0\n        ## in SDAE we do layer-wise pretraining using autoencoders\n        for i in range(dnn_model.n_layers):\n            for epoch in range(pretraining_epochs):\n                sub_start_time = time.clock()\n\n                pretrain_loss = []\n                while (not train_data_reader.is_finish()):\n                    shared_train_set_xy, temp_train_set_x, temp_train_set_y = train_data_reader.load_next_partition()\n                    pretrain_set_x.set_value(numpy.asarray(temp_train_set_x, dtype=theano.config.floatX), borrow=True)\n\n                    n_train_batches = pretrain_set_x.get_value().shape[0] / batch_size\n\n                    for batch_index in range(n_train_batches):\n                        pretrain_loss.append(pretraining_fn[i](index=batch_index,\n                                                               corruption=corruption_level,\n                                                               learning_rate=pretraining_lr))\n\n                sub_end_time = time.clock()\n                logger.info(\'Pre-training layer %i, epoch %d, cost %s, time spent%.2f\' % (i+1, epoch+1, numpy.mean(pretrain_loss), (sub_end_time - sub_start_time)))\n                train_data_reader.reset()\n\n\n    logger.info(\'fine-tuning the %s model\' %(model_type))\n\n    start_time = time.clock()\n\n    best_dnn_model = dnn_model\n    best_validation_loss = sys.float_info.max\n    previous_loss = sys.float_info.max\n\n    early_stop = 0\n    epoch = 0\n    previous_finetune_lr = finetune_lr\n\n    dnn_model.zero_projection_weights()\n\n    while (epoch < training_epochs):\n        epoch = epoch + 1\n\n        current_momentum = momentum\n        current_finetune_lr = finetune_lr\n        if epoch <= warmup_epoch:\n            current_finetune_lr = finetune_lr\n            current_momentum = warmup_momentum\n        else:\n            current_finetune_lr = previous_finetune_lr * 0.5\n\n        previous_finetune_lr = current_finetune_lr\n\n        train_error = []\n        sub_start_time = time.clock()\n\n        while (not train_data_reader.is_finish()):\n            shared_train_set_xy, temp_train_set_x, temp_train_set_x_proj, temp_train_set_y = train_data_reader.load_next_partition_with_projection()\n            train_set_x.set_value(numpy.asarray(temp_train_set_x, dtype=theano.config.floatX), borrow=True)\n            train_set_x_proj.set_value(numpy.asarray(temp_train_set_x_proj, dtype=proj_type), borrow=True)\n            train_set_y.set_value(numpy.asarray(temp_train_set_y, dtype=theano.config.floatX), borrow=True)\n\n            n_train_batches = train_set_x.get_value().shape[0] / batch_size\n\n            logger.debug(\'this partition: %d frames (divided into %d batches of size %d)\' %(train_set_x.get_value(borrow=True).shape[0], n_train_batches, batch_size) )\n\n            for minibatch_index in range(n_train_batches):\n                this_train_error = train_subword_fn(minibatch_index, current_finetune_lr, current_momentum)\n                train_error.append(this_train_error)\n\n                if numpy.isnan(this_train_error):\n                    logger.warning(\'training error over minibatch %d of %d was %s\' % (minibatch_index+1,n_train_batches,this_train_error) )\n\n        train_data_reader.reset()\n\n        ## osw -- getting validation error from a forward pass in a single batch\n        ##        exausts memory when using 20k projected vocab -- also use minibatches\n        logger.debug(\'calculating validation loss\')\n        valid_error = []\n        n_valid_batches = valid_set_x.get_value().shape[0] / batch_size\n        for minibatch_index in range(n_valid_batches):\n            v_loss = valid_score_i(minibatch_index)\n            valid_error.append(v_loss)\n\n        this_validation_loss = numpy.mean(valid_error)\n\n        # this has a possible bias if the minibatches were not all of identical size\n        # but it should not be siginficant if minibatches are small\n        this_train_valid_loss = numpy.mean(train_error)\n\n        sub_end_time = time.clock()\n\n        loss_difference = this_validation_loss - previous_loss\n\n        logger.info(\'BASIC epoch %i, validation error %f, train error %f  time spent %.2f\' %(epoch, this_validation_loss, this_train_valid_loss, (sub_end_time - sub_start_time)))\n        if plot:\n            plotlogger.add_plot_point(\'training convergence\',\'validation set\',(epoch,this_validation_loss))\n            plotlogger.add_plot_point(\'training convergence\',\'training set\',(epoch,this_train_valid_loss))\n            plotlogger.save_plot(\'training convergence\',title=\'Progress of training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n\n        if this_validation_loss < best_validation_loss:\n            best_dnn_model = dnn_model\n            best_validation_loss = this_validation_loss\n            logger.debug(\'validation loss decreased, so saving model\')\n            early_stop = 0\n        else:\n            logger.debug(\'validation loss did not improve\')\n            dbn = best_dnn_model\n            early_stop += 1\n\n        if early_stop > early_stop_epoch:\n            # too many consecutive epochs without surpassing the best model\n            logger.debug(\'stopping early\')\n            break\n\n        if math.isnan(this_validation_loss):\n            break\n\n        previous_loss = this_validation_loss\n\n\n        ### Save projection values:\n        if cfg.hyper_params[\'model_type\'] == \'TPDNN\':\n            if not os.path.isdir(cfg.projection_weights_output_dir):\n                os.mkdir(cfg.projection_weights_output_dir)\n            weights = dnn_model.get_projection_weights()\n            fname = os.path.join(cfg.projection_weights_output_dir, \'proj_BASIC_epoch_%s\'%(epoch))\n            numpy.savetxt(fname, weights)\n\n    end_time = time.clock()\n    pickle.dump(best_dnn_model, open(nnets_file_name, \'wb\'))\n\n    logger.info(\'overall  training time: %.2fm validation error %f\' % ((end_time - start_time) / 60., best_validation_loss))\n\n    if plot:\n        plotlogger.save_plot(\'training convergence\',title=\'Final training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n\n\n### ========== now train the word residual ============\ndef train_DNN_with_projections(train_xy_file_list, valid_xy_file_list, \\\n              nnets_file_name, n_ins, n_outs, ms_outs, hyper_params, buffer_size, plot=False):\n\n    ####parameters#####\n    finetune_lr     = float(hyper_params[\'learning_rate\'])\n    training_epochs = int(hyper_params[\'training_epochs\'])\n    batch_size      = int(hyper_params[\'batch_size\'])\n    l1_reg          = float(hyper_params[\'l1_reg\'])\n    l2_reg          = float(hyper_params[\'l2_reg\'])\n    private_l2_reg  = float(hyper_params[\'private_l2_reg\'])\n    warmup_epoch    = int(hyper_params[\'warmup_epoch\'])\n    momentum        = float(hyper_params[\'momentum\'])\n    warmup_momentum = float(hyper_params[\'warmup_momentum\'])\n\n    hidden_layers_sizes = hyper_params[\'hidden_layers_sizes\']\n\n    stream_weights       = hyper_params[\'stream_weights\']\n    private_hidden_sizes = hyper_params[\'private_hidden_sizes\']\n\n    buffer_utt_size = buffer_size\n    early_stop_epoch = int(hyper_params[\'early_stop_epochs\'])\n\n    hidden_activation = hyper_params[\'hidden_activation\']\n    output_activation = hyper_params[\'output_activation\']\n\n    stream_lr_weights = hyper_params[\'stream_lr_weights\']\n    use_private_hidden = hyper_params[\'use_private_hidden\']\n\n    model_type = hyper_params[\'model_type\']\n\n    index_to_project = hyper_params[\'index_to_project\']\n    projection_insize = hyper_params[\'projection_insize\']\n    projection_outsize = hyper_params[\'projection_outsize\']\n\n\n    ######### data providers ##########\n    (train_x_file_list, train_y_file_list) = train_xy_file_list\n    (valid_x_file_list, valid_y_file_list) = valid_xy_file_list\n\n    logger.debug(\'Creating training   data provider\')\n    train_data_reader = ListDataProviderWithProjectionIndex(x_file_list = train_x_file_list, y_file_list = train_y_file_list, n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, shuffle = True, index_to_project=index_to_project, projection_insize=projection_insize, indexes_only=expand_by_minibatch)\n\n    logger.debug(\'Creating validation data provider\')\n    valid_data_reader = ListDataProviderWithProjectionIndex(x_file_list = valid_x_file_list, y_file_list = valid_y_file_list, n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, shuffle = False, index_to_project=index_to_project, projection_insize=projection_insize, indexes_only=expand_by_minibatch)\n\n    shared_train_set_xy, temp_train_set_x, temp_train_set_x_proj, temp_train_set_y = train_data_reader.load_next_partition_with_projection()\n    train_set_x, train_set_x_proj, train_set_y = shared_train_set_xy\n    shared_valid_set_xy, temp_valid_set_x, temp_valid_set_x_proj, temp_valid_set_y = valid_data_reader.load_next_partition_with_projection()\n    valid_set_x, valid_set_x_proj, valid_set_y = shared_valid_set_xy\n    train_data_reader.reset()\n    valid_data_reader.reset()\n    ####################################\n\n\n    # numpy random generator\n    numpy_rng = numpy.random.RandomState(123)\n    logger.info(\'building the model\')\n\n\n    ############## load existing dnn #####\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n    train_all_fn, train_subword_fn, train_word_fn, infer_projections_fn, valid_fn, valid_score_i = \\\n                    dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_x_proj, train_set_y),\n                    (valid_set_x, valid_set_x_proj, valid_set_y), batch_size=batch_size)\n    ####################################\n\n    logger.info(\'fine-tuning the %s model\' %(model_type))\n\n    start_time = time.clock()\n\n    best_dnn_model = dnn_model\n    best_validation_loss = sys.float_info.max\n    previous_loss = sys.float_info.max\n\n    early_stop = 0\n    epoch = 0\n    previous_finetune_lr = finetune_lr\n\n\n    dnn_model.initialise_projection_weights()\n\n    all_epochs = 20 ## 100  ## <-------- hard coded !!!!!!!!!!\n\n    current_finetune_lr = previous_finetune_lr = finetune_lr\n    warmup_epoch_2 = 10 # 10  ## <-------- hard coded !!!!!!!!!!\n\n    while (epoch < all_epochs):\n        epoch = epoch + 1\n\n        current_momentum = momentum\n\n        if epoch > warmup_epoch_2:\n            previous_finetune_lr = current_finetune_lr\n            current_finetune_lr = previous_finetune_lr * 0.5\n\n        train_error = []\n        sub_start_time = time.clock()\n\n        while (not train_data_reader.is_finish()):\n            shared_train_set_xy, temp_train_set_x, temp_train_set_x_proj, temp_train_set_y = train_data_reader.load_next_partition_with_projection()\n            train_set_x.set_value(numpy.asarray(temp_train_set_x, dtype=theano.config.floatX), borrow=True)\n            train_set_x_proj.set_value(numpy.asarray(temp_train_set_x_proj, dtype=proj_type), borrow=True)\n            train_set_y.set_value(numpy.asarray(temp_train_set_y, dtype=theano.config.floatX), borrow=True)\n\n            n_train_batches = train_set_x.get_value().shape[0] / batch_size\n\n            logger.debug(\'this partition: %d frames (divided into %d batches of size %d)\' %(train_set_x.get_value(borrow=True).shape[0], n_train_batches, batch_size) )\n\n            for minibatch_index in range(n_train_batches):\n                this_train_error = train_word_fn(minibatch_index, current_finetune_lr, current_momentum)\n                train_error.append(this_train_error)\n\n                if numpy.isnan(this_train_error):\n                    logger.warning(\'training error over minibatch %d of %d was %s\' % (minibatch_index+1,n_train_batches,this_train_error) )\n\n        train_data_reader.reset()\n\n\n        ### COULD REMOVE THIS LATER\n        ## osw -- getting validation error from a forward pass in a single batch\n        ##        exausts memory when using 20k projected vocab -- also use minibatches\n        logger.debug(\'calculating validation loss\')\n        valid_error = []\n        n_valid_batches = valid_set_x.get_value().shape[0] / batch_size\n        for minibatch_index in range(n_valid_batches):\n            v_loss = valid_score_i(minibatch_index)\n            valid_error.append(v_loss)\n        this_validation_loss = numpy.mean(valid_error)\n\n\n        # this has a possible bias if the minibatches were not all of identical size\n        # but it should not be siginficant if minibatches are small\n        this_train_valid_loss = numpy.mean(train_error)\n\n#        if plot:\n#            ## add dummy validation loss so that plot works:\n#            plotlogger.add_plot_point(\'training convergence\',\'validation set\',(epoch,this_validation_loss))\n#            plotlogger.add_plot_point(\'training convergence\',\'training set\',(epoch,this_train_valid_loss))\n#\n\n        sub_end_time = time.clock()\n\n\n        logger.info(\'TOKEN epoch %i, validation error %f, train error %f  time spent %.2f\' %(epoch, this_validation_loss, this_train_valid_loss, (sub_end_time - sub_start_time)))\n\n        if cfg.hyper_params[\'model_type\'] == \'TPDNN\':\n            if not os.path.isdir(cfg.projection_weights_output_dir):\n                os.mkdir(cfg.projection_weights_output_dir)\n            weights = dnn_model.get_projection_weights()\n            fname = os.path.join(cfg.projection_weights_output_dir, \'proj_TOKEN_epoch_%s\'%(epoch))\n            numpy.savetxt(fname, weights)\n\n\n        best_dnn_model = dnn_model  ## always update\n\n    end_time = time.clock()\n    pickle.dump(best_dnn_model, open(nnets_file_name, \'wb\'))\n\n    logger.info(\'overall  training time: %.2fm validation error %f\' % ((end_time - start_time) / 60., best_validation_loss))\n\n#    if plot:\n#        plotlogger.save_plot(\'training convergence\',title=\'Final training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n#\n\n    ### ========================================================\n\n\n    ### ========== now infer word represntations for out-of-training (dev) data ============\n#\n#    ### TEMP-- restarted!!! ### ~~~~~~~\n#    epoch = 50\n#    dnn_model = cPickle.load(open(nnets_file_name, \'rb\'))\n#    train_all_fn, train_subword_fn, train_word_fn, infer_projections_fn, valid_fn, valid_score_i = \\\n#                    dnn_model.build_finetune_functions(\n#                    (train_set_x, train_set_x_proj, train_set_y),\n#                    (valid_set_x, valid_set_x_proj, valid_set_y), batch_size=batch_size)\n#    this_train_valid_loss = 198.0 ## approx value\n#    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\ndef infer_projections(train_xy_file_list, valid_xy_file_list, \\\n              nnets_file_name, n_ins, n_outs, ms_outs, hyper_params, buffer_size, plot=False):\n\n    ####parameters#####\n    finetune_lr     = float(hyper_params[\'learning_rate\'])\n    training_epochs = int(hyper_params[\'training_epochs\'])\n    batch_size      = int(hyper_params[\'batch_size\'])\n    l1_reg          = float(hyper_params[\'l1_reg\'])\n    l2_reg          = float(hyper_params[\'l2_reg\'])\n    private_l2_reg  = float(hyper_params[\'private_l2_reg\'])\n    warmup_epoch    = int(hyper_params[\'warmup_epoch\'])\n    momentum        = float(hyper_params[\'momentum\'])\n    warmup_momentum = float(hyper_params[\'warmup_momentum\'])\n\n    hidden_layers_sizes = hyper_params[\'hidden_layers_sizes\']\n\n    stream_weights       = hyper_params[\'stream_weights\']\n    private_hidden_sizes = hyper_params[\'private_hidden_sizes\']\n\n    buffer_utt_size = buffer_size\n    early_stop_epoch = int(hyper_params[\'early_stop_epochs\'])\n\n    hidden_activation = hyper_params[\'hidden_activation\']\n    output_activation = hyper_params[\'output_activation\']\n\n    stream_lr_weights = hyper_params[\'stream_lr_weights\']\n    use_private_hidden = hyper_params[\'use_private_hidden\']\n\n    model_type = hyper_params[\'model_type\']\n\n    index_to_project = hyper_params[\'index_to_project\']\n    projection_insize = hyper_params[\'projection_insize\']\n    projection_outsize = hyper_params[\'projection_outsize\']\n\n    ######### data providers ##########\n    (train_x_file_list, train_y_file_list) = train_xy_file_list\n    (valid_x_file_list, valid_y_file_list) = valid_xy_file_list\n\n    logger.debug(\'Creating training   data provider\')\n    train_data_reader = ListDataProviderWithProjectionIndex(x_file_list = train_x_file_list, y_file_list = train_y_file_list, n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, shuffle = True, index_to_project=index_to_project, projection_insize=projection_insize, indexes_only=expand_by_minibatch)\n\n    logger.debug(\'Creating validation data provider\')\n    valid_data_reader = ListDataProviderWithProjectionIndex(x_file_list = valid_x_file_list, y_file_list = valid_y_file_list, n_ins = n_ins, n_outs = n_outs, buffer_size = buffer_size, shuffle = False, index_to_project=index_to_project, projection_insize=projection_insize, indexes_only=expand_by_minibatch)\n\n    shared_train_set_xy, temp_train_set_x, temp_train_set_x_proj, temp_train_set_y = train_data_reader.load_next_partition_with_projection()\n    train_set_x, train_set_x_proj, train_set_y = shared_train_set_xy\n    shared_valid_set_xy, temp_valid_set_x, temp_valid_set_x_proj, temp_valid_set_y = valid_data_reader.load_next_partition_with_projection()\n    valid_set_x, valid_set_x_proj, valid_set_y = shared_valid_set_xy\n    train_data_reader.reset()\n    valid_data_reader.reset()\n    ####################################\n\n\n    # numpy random generator\n    numpy_rng = numpy.random.RandomState(123)\n    logger.info(\'building the model\')\n\n\n    ############## load existing dnn #####\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n    train_all_fn, train_subword_fn, train_word_fn, infer_projections_fn, valid_fn, valid_score_i = \\\n                    dnn_model.build_finetune_functions(\n                    (train_set_x, train_set_x_proj, train_set_y),\n                    (valid_set_x, valid_set_x_proj, valid_set_y), batch_size=batch_size)\n    ####################################\n\n    logger.info(\'fine-tuning the %s model\' %(model_type))\n\n    start_time = time.clock()\n\n    best_dnn_model = dnn_model\n    best_validation_loss = sys.float_info.max\n    previous_loss = sys.float_info.max\n\n    early_stop = 0\n    epoch = 0\n    previous_finetune_lr = finetune_lr\n\n    logger.info(\'fine-tuning the %s model\' %(model_type))\n\n\n\n    #dnn_model.initialise_projection_weights()\n\n    inference_epochs = 20  ## <-------- hard coded !!!!!!!!!!\n\n\n\n    current_finetune_lr = previous_finetune_lr = finetune_lr\n    warmup_epoch_3 = 10 # 10  ## <-------- hard coded !!!!!!!!!!\n\n    #warmup_epoch_3 = epoch + warmup_epoch_3\n    #inference_epochs += epoch\n    while (epoch < inference_epochs):\n\n        epoch = epoch + 1\n\n        current_momentum = momentum\n\n        if epoch > warmup_epoch_3:\n            previous_finetune_lr = current_finetune_lr\n            current_finetune_lr = previous_finetune_lr * 0.5\n\n\n\n        dev_error = []\n        sub_start_time = time.clock()\n\n        ## osw -- inferring word reps on validation set in a forward pass in a single batch\n        ##        exausts memory when using 20k projected vocab -- also use minibatches\n        logger.debug(\'infer word representations for validation set\')\n        valid_error = []\n        n_valid_batches = valid_set_x.get_value().shape[0] / batch_size\n        for minibatch_index in range(n_valid_batches):\n            v_loss = infer_projections_fn(minibatch_index, current_finetune_lr, current_momentum)\n            valid_error.append(v_loss)\n\n        this_validation_loss = numpy.mean(valid_error)\n\n\n        #valid_error = infer_projections_fn(current_finetune_lr, current_momentum)\n        #this_validation_loss = numpy.mean(valid_error)\n\n#        if plot:\n#            ## add dummy validation loss so that plot works:\n#            plotlogger.add_plot_point(\'training convergence\',\'validation set\',(epoch,this_validation_loss))\n#            plotlogger.add_plot_point(\'training convergence\',\'training set\',(epoch,this_train_valid_loss))\n#\n\n        sub_end_time = time.clock()\n\n\n        logger.info(\'INFERENCE epoch %i, validation error %f, time spent %.2f\' %(epoch, this_validation_loss, (sub_end_time - sub_start_time)))\n\n\n        if cfg.hyper_params[\'model_type\'] == \'TPDNN\':\n            if not os.path.isdir(cfg.projection_weights_output_dir):\n                os.mkdir(cfg.projection_weights_output_dir)\n            weights = dnn_model.get_projection_weights()\n            fname = os.path.join(cfg.projection_weights_output_dir, \'proj_INFERENCE_epoch_%s\'%(epoch))\n            numpy.savetxt(fname, weights)\n\n\n        best_dnn_model = dnn_model  ## always update\n\n    end_time = time.clock()\n    pickle.dump(best_dnn_model, open(nnets_file_name, \'wb\'))\n\n    logger.info(\'overall  training time: %.2fm validation error %f\' % ((end_time - start_time) / 60., best_validation_loss))\n\n#    if plot:\n#        plotlogger.save_plot(\'training convergence\',title=\'Final training and validation error\',xlabel=\'epochs\',ylabel=\'error\')\n#\n\n    ### ========================================================\n\n\n\n\n    if cfg.hyper_params[\'model_type\'] == \'TPDNN\':\n        os.system(\'python %s %s\'%(\'/afs/inf.ed.ac.uk/user/o/owatts/scripts_NEW/plot_weights_multiple_phases.py\', cfg.projection_weights_output_dir))\n\n    return  best_validation_loss\n\n\ndef dnn_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list, cfg=None, use_word_projections=True):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    ## \'remove\' word representations by randomising them. As model is unpickled and\n    ## no re-saved, this does not throw trained parameters away.\n    if not use_word_projections:\n        dnn_model.initialise_projection_weights()\n\n#    visualize_dnn(dbn)\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        features = features.reshape((-1, n_ins))\n\n        #features, features_proj = expand_projection_inputs(features, cfg.index_to_project, \\\n        #                                                         cfg.projection_insize)\n        features, features_proj = get_unexpanded_projection_inputs(features, cfg.index_to_project, \\\n                                                                 cfg.projection_insize)\n        #temp_set_x = features.tolist()  ## osw - why list conversion necessary?\n        #print temp_set_x\n        test_set_x = theano.shared(numpy.asarray(features, dtype=theano.config.floatX))\n        test_set_x_proj = theano.shared(numpy.asarray(features_proj, dtype=\'int32\'))\n\n        predicted_parameter = dnn_model.parameter_prediction(test_set_x=test_set_x, test_set_x_proj=test_set_x_proj)\n#        predicted_parameter = test_out()\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\n##generate bottleneck layer as festures\ndef dnn_hidden_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list):\n    logger = logging.getLogger(""dnn_generation"")\n    logger.debug(\'Starting dnn_generation\')\n\n    plotlogger = logging.getLogger(""plotting"")\n\n    dnn_model = pickle.load(open(nnets_file_name, \'rb\'))\n\n    file_number = len(valid_file_list)\n\n    for i in range(file_number):\n        logger.info(\'generating %4d of %4d: %s\' % (i+1,file_number,valid_file_list[i]) )\n        fid_lab = open(valid_file_list[i], \'rb\')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        features = features[:(n_ins * (features.size / n_ins))]\n        features = features.reshape((-1, n_ins))\n        temp_set_x = features.tolist()\n        test_set_x = theano.shared(numpy.asarray(temp_set_x, dtype=theano.config.floatX))\n\n        predicted_parameter = dnn_model.generate_top_hidden_layer(test_set_x=test_set_x)\n\n        ### write to cmp file\n        predicted_parameter = numpy.array(predicted_parameter, \'float32\')\n        temp_parameter = predicted_parameter\n        fid = open(out_file_list[i], \'wb\')\n        predicted_parameter.tofile(fid)\n        logger.debug(\'saved to %s\' % out_file_list[i])\n        fid.close()\n\n\ndef main_function(cfg):\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n    # get another logger to handle plotting duties\n    plotlogger = logging.getLogger(""plotting"")\n\n    # later, we might do this via a handler that is created, attached and configured\n    # using the standard config mechanism of the logging module\n    # but for now we need to do it manually\n    plotlogger.set_plot_path(cfg.plot_dir)\n\n    #### parameter setting########\n    hidden_layers_sizes = cfg.hyper_params[\'hidden_layers_sizes\']\n\n    ####prepare environment\n\n    try:\n        file_id_list = read_file_list(cfg.file_id_scp)\n        logger.debug(\'Loaded file id list from %s\' % cfg.file_id_scp)\n    except IOError:\n        # this means that open(...) threw an error\n        logger.critical(\'Could not load file id list from %s\' % cfg.file_id_scp)\n        raise\n\n    ###total file number including training, development, and testing\n    total_file_number = len(file_id_list)\n\n    data_dir = cfg.data_dir\n\n    nn_cmp_dir       = os.path.join(data_dir, \'nn\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim))\n    nn_cmp_norm_dir   = os.path.join(data_dir, \'nn_norm\'  + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim))\n\n    model_dir = os.path.join(cfg.work_dir, \'nnets_model\')\n    gen_dir   = os.path.join(cfg.work_dir, \'gen\')\n\n    in_file_list_dict = {}\n\n    for feature_name in list(cfg.in_dir_dict.keys()):\n        in_file_list_dict[feature_name] = prepare_file_path_list(file_id_list, cfg.in_dir_dict[feature_name], cfg.file_extension_dict[feature_name], False)\n\n    nn_cmp_file_list         = prepare_file_path_list(file_id_list, nn_cmp_dir, cfg.cmp_ext)\n    nn_cmp_norm_file_list    = prepare_file_path_list(file_id_list, nn_cmp_norm_dir, cfg.cmp_ext)\n\n    ###normalisation information\n    norm_info_file = os.path.join(data_dir, \'norm_info\' + cfg.combined_feature_name + \'_\' + str(cfg.cmp_dim) + \'_\' + cfg.output_feature_normalisation + \'.dat\')\n\n    ### normalise input full context label\n\n    # currently supporting two different forms of lingustic features\n    # later, we should generalise this\n\n    if cfg.label_style == \'HTS\':\n        label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name)\n        lab_dim = label_normaliser.dimension\n        logger.info(\'Input label dimension is %d\' % lab_dim)\n        suffix=str(lab_dim)\n    # no longer supported - use new ""composed"" style labels instead\n    elif cfg.label_style == \'composed\':\n        # label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)\n        suffix=\'composed\'\n\n    if cfg.process_labels_in_work_dir:\n        label_data_dir = cfg.work_dir\n    else:\n        label_data_dir = data_dir\n\n    # the number can be removed\n    binary_label_dir      = os.path.join(label_data_dir, \'binary_label_\'+suffix)\n    nn_label_dir          = os.path.join(label_data_dir, \'nn_no_silence_lab_\'+suffix)\n    nn_label_norm_dir     = os.path.join(label_data_dir, \'nn_no_silence_lab_norm_\'+suffix)\n#    nn_label_norm_mvn_dir = os.path.join(data_dir, \'nn_no_silence_lab_norm_\'+suffix)\n\n    in_label_align_file_list = prepare_file_path_list(file_id_list, cfg.in_label_align_dir, cfg.lab_ext, False)\n    binary_label_file_list   = prepare_file_path_list(file_id_list, binary_label_dir, cfg.lab_ext)\n    nn_label_file_list       = prepare_file_path_list(file_id_list, nn_label_dir, cfg.lab_ext)\n    nn_label_norm_file_list  = prepare_file_path_list(file_id_list, nn_label_norm_dir, cfg.lab_ext)\n\n    # to do - sanity check the label dimension here?\n\n\n\n    min_max_normaliser = None\n    label_norm_file = \'label_norm_%s.dat\' %(cfg.label_style)\n    label_norm_file = os.path.join(label_data_dir, label_norm_file)\n\n    if cfg.NORMLAB and (cfg.label_style == \'HTS\'):\n        # simple HTS labels\n        logger.info(\'preparing label data (input) using standard HTS style labels\')\n        label_normaliser.perform_normalisation(in_label_align_file_list, binary_label_file_list)\n\n        remover = SilenceRemover(n_cmp = lab_dim, silence_pattern = [\'*-#+*\'])\n        remover.remove_silence(binary_label_file_list, in_label_align_file_list, nn_label_file_list)\n\n        min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99)\n        ###use only training data to find min-max information, then apply on the whole dataset\n        min_max_normaliser.find_min_max_values(nn_label_file_list[0:cfg.train_file_number])\n        min_max_normaliser.normalise_data(nn_label_file_list, nn_label_norm_file_list)\n\n\n    if cfg.NORMLAB and (cfg.label_style == \'composed\'):\n        # new flexible label preprocessor\n\n        logger.info(\'preparing label data (input) using ""composed"" style labels\')\n        label_composer = LabelComposer()\n        label_composer.load_label_configuration(cfg.label_config_file)\n\n        logger.info(\'Loaded label configuration\')\n        # logger.info(\'%s\' % label_composer.configuration.labels )\n\n        lab_dim=label_composer.compute_label_dimension()\n        logger.info(\'label dimension will be %d\' % lab_dim)\n\n        if cfg.precompile_xpaths:\n            label_composer.precompile_xpaths()\n\n        # there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)\n        # create all the lists of these, ready to pass to the label composer\n\n        in_label_align_file_list = {}\n        for label_style, label_style_required in label_composer.label_styles.items():\n            if label_style_required:\n                logger.info(\'labels of style %s are required - constructing file paths for them\' % label_style)\n                if label_style == \'xpath\':\n                    in_label_align_file_list[\'xpath\'] = prepare_file_path_list(file_id_list, cfg.xpath_label_align_dir, cfg.utt_ext, False)\n                elif label_style == \'hts\':\n                    in_label_align_file_list[\'hts\'] = prepare_file_path_list(file_id_list, cfg.hts_label_align_dir, cfg.lab_ext, False)\n                else:\n                    logger.critical(\'unsupported label style %s specified in label configuration\' % label_style)\n                    raise Exception\n\n            # now iterate through the files, one at a time, constructing the labels for them\n            num_files=len(file_id_list)\n            logger.info(\'the label styles required are %s\' % label_composer.label_styles)\n\n            for i in range(num_files):\n                logger.info(\'making input label features for %4d of %4d\' % (i+1,num_files))\n\n                # iterate through the required label styles and open each corresponding label file\n\n                # a dictionary of file descriptors, pointing at the required files\n                required_labels={}\n\n                for label_style, label_style_required in label_composer.label_styles.items():\n\n                    # the files will be a parallel set of files for a single utterance\n                    # e.g., the XML tree and an HTS label file\n                    if label_style_required:\n                        required_labels[label_style] = open(in_label_align_file_list[label_style][i] , \'r\')\n                        logger.debug(\' opening label file %s\' % in_label_align_file_list[label_style][i])\n\n                logger.debug(\'label styles with open files: %s\' % required_labels)\n                label_composer.make_labels(required_labels,out_file_name=binary_label_file_list[i],fill_missing_values=cfg.fill_missing_values,iterate_over_frames=cfg.iterate_over_frames)\n\n                # now close all opened files\n                for fd in required_labels.values():\n                    fd.close()\n\n\n        # silence removal\n        if cfg.remove_silence_using_binary_labels:\n            silence_feature = 0 ## use first feature in label -- hardcoded for now\n            logger.info(\'Silence removal from label using silence feature: %s\'%(label_composer.configuration.labels[silence_feature]))\n            logger.info(\'Silence will be removed from CMP files in same way\')\n            ## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:\n            trim_silence(binary_label_file_list, nn_label_file_list, lab_dim, \\\n                                binary_label_file_list, lab_dim, silence_feature, percent_to_keep=5)\n        else:\n            logger.info(\'No silence removal done\')\n            # start from the labels we have just produced, not trimmed versions\n            nn_label_file_list = binary_label_file_list\n\n        min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99, exclude_columns=[cfg.index_to_project])\n        ###use only training data to find min-max information, then apply on the whole dataset\n        min_max_normaliser.find_min_max_values(nn_label_file_list[0:cfg.train_file_number])\n        min_max_normaliser.normalise_data(nn_label_file_list, nn_label_norm_file_list)\n\n    if min_max_normaliser != None:\n        ### save label normalisation information for unseen testing labels\n        label_min_vector = min_max_normaliser.min_vector\n        label_max_vector = min_max_normaliser.max_vector\n        label_norm_info = numpy.concatenate((label_min_vector, label_max_vector), axis=0)\n\n        label_norm_info = numpy.array(label_norm_info, \'float32\')\n        fid = open(label_norm_file, \'wb\')\n        label_norm_info.tofile(fid)\n        fid.close()\n        logger.info(\'saved %s vectors to %s\' %(label_min_vector.size, label_norm_file))\n\n\n\n\n    ### make output acoustic data\n    if cfg.MAKECMP:\n        logger.info(\'creating acoustic (output) features\')\n        delta_win = [-0.5, 0.0, 0.5]\n        acc_win = [1.0, -2.0, 1.0]\n\n        acoustic_worker = AcousticComposition(delta_win = delta_win, acc_win = acc_win)\n        acoustic_worker.prepare_nn_data(in_file_list_dict, nn_cmp_file_list, cfg.in_dimension_dict, cfg.out_dimension_dict)\n\n        if cfg.remove_silence_using_binary_labels:\n            ## do this to get lab_dim:\n            label_composer = LabelComposer()\n            label_composer.load_label_configuration(cfg.label_config_file)\n            lab_dim=label_composer.compute_label_dimension()\n\n            silence_feature = 0 ## use first feature in label -- hardcoded for now\n            logger.info(\'Silence removal from CMP using binary label file\')\n\n            ## overwrite the untrimmed audio with the trimmed version:\n            trim_silence(nn_cmp_file_list, nn_cmp_file_list, cfg.cmp_dim, \\\n                                binary_label_file_list, lab_dim, silence_feature, percent_to_keep=5)\n\n        else: ## back off to previous method using HTS labels:\n            remover = SilenceRemover(n_cmp = cfg.cmp_dim, silence_pattern = [\'*-#+*\'])\n            remover.remove_silence(nn_cmp_file_list, in_label_align_file_list, nn_cmp_file_list) # save to itself\n\n    ### save acoustic normalisation information for normalising the features back\n    var_dir   = os.path.join(data_dir, \'var\')\n    if not os.path.exists(var_dir):\n        os.makedirs(var_dir)\n\n    var_file_dict = {}\n    for feature_name in list(cfg.out_dimension_dict.keys()):\n        var_file_dict[feature_name] = os.path.join(var_dir, feature_name + \'_\' + str(cfg.out_dimension_dict[feature_name]))\n\n    ### normalise output acoustic data\n    if cfg.NORMCMP:\n        logger.info(\'normalising acoustic (output) features using method %s\' % cfg.output_feature_normalisation)\n        cmp_norm_info = None\n        if cfg.output_feature_normalisation == \'MVN\':\n            normaliser = MeanVarianceNorm(feature_dimension=cfg.cmp_dim)\n            ###calculate mean and std vectors on the training data, and apply on the whole dataset\n            global_mean_vector = normaliser.compute_mean(nn_cmp_file_list[0:cfg.train_file_number], 0, cfg.cmp_dim)\n            global_std_vector = normaliser.compute_std(nn_cmp_file_list[0:cfg.train_file_number], global_mean_vector, 0, cfg.cmp_dim)\n\n            normaliser.feature_normalisation(nn_cmp_file_list, nn_cmp_norm_file_list)\n            cmp_norm_info = numpy.concatenate((global_mean_vector, global_std_vector), axis=0)\n\n        elif cfg.output_feature_normalisation == \'MINMAX\':\n            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim)\n            global_mean_vector = min_max_normaliser.compute_mean(nn_cmp_file_list[0:cfg.train_file_number])\n            global_std_vector = min_max_normaliser.compute_std(nn_cmp_file_list[0:cfg.train_file_number], global_mean_vector)\n\n            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim, min_value = 0.01, max_value = 0.99)\n            min_max_normaliser.find_min_max_values(nn_cmp_file_list[0:cfg.train_file_number])\n            min_max_normaliser.normalise_data(nn_cmp_file_list, nn_cmp_norm_file_list)\n\n            cmp_min_vector = min_max_normaliser.min_vector\n            cmp_max_vector = min_max_normaliser.max_vector\n            cmp_norm_info = numpy.concatenate((cmp_min_vector, cmp_max_vector), axis=0)\n\n        else:\n            logger.critical(\'Normalisation type %s is not supported!\\n\' %(cfg.output_feature_normalisation))\n            raise\n\n        cmp_norm_info = numpy.array(cmp_norm_info, \'float32\')\n        fid = open(norm_info_file, \'wb\')\n        cmp_norm_info.tofile(fid)\n        fid.close()\n        logger.info(\'saved %s vectors to %s\' %(cfg.output_feature_normalisation, norm_info_file))\n        # logger.debug(\' value was\\n%s\' % cmp_norm_info)\n\n        feature_index = 0\n        for feature_name in list(cfg.out_dimension_dict.keys()):\n            feature_std_vector = numpy.array(global_std_vector[:,feature_index:feature_index+cfg.out_dimension_dict[feature_name]], \'float32\')\n\n            fid = open(var_file_dict[feature_name], \'w\')\n            feature_std_vector.tofile(fid)\n            fid.close()\n\n            logger.info(\'saved %s variance vector to %s\' %(feature_name, var_file_dict[feature_name]))\n            # logger.debug(\' value was\\n%s\' % feature_std_vector)\n\n            feature_index += cfg.out_dimension_dict[feature_name]\n\n    train_x_file_list = nn_label_norm_file_list[0:cfg.train_file_number]\n    train_y_file_list = nn_cmp_norm_file_list[0:cfg.train_file_number]\n    valid_x_file_list = nn_label_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n    valid_y_file_list = nn_cmp_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n    test_x_file_list  = nn_label_norm_file_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n    test_y_file_list  = nn_cmp_norm_file_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n\n    # we need to know the label dimension before training the DNN\n    # computing that requires us to look at the labels\n    #\n    # currently, there are two ways to do this\n    if cfg.label_style == \'HTS\':\n        label_normaliser = HTSLabelNormalisation(question_file_name=cfg.question_file_name)\n        lab_dim = label_normaliser.dimension\n\n    elif cfg.label_style == \'composed\':\n        label_composer = LabelComposer()\n        label_composer.load_label_configuration(cfg.label_config_file)\n        lab_dim=label_composer.compute_label_dimension()\n\n    logger.info(\'label dimension is %d\' % lab_dim)\n\n    combined_model_arch = str(len(hidden_layers_sizes))\n    for hid_size in hidden_layers_sizes:\n        combined_model_arch += \'_\' + str(hid_size)\n\n#    nnets_file_name = \'%s/%s_%s_%d.%d.%d.%d.%d.train.%d.model\' \\\n#                       %(model_dir, cfg.model_type, cfg.combined_feature_name, int(cfg.multistream_switch),\n#                        len(hidden_layers_sizes), hidden_layers_sizes[0],\n#                        lab_dim, cfg.cmp_dim, cfg.train_file_number)\n\n    nnets_file_name = \'%s/%s_%s_%d_%s_%d.%d.train.%d.model\' \\\n                      %(model_dir, cfg.model_type, cfg.combined_feature_name, int(cfg.multistream_switch),\n                        combined_model_arch, lab_dim, cfg.cmp_dim, cfg.train_file_number)\n\n    ### DNN model training\n    if cfg.TRAINDNN:\n\n        logger.info(\'training DNN\')\n\n        try:\n            os.makedirs(model_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create model directory %s\' % model_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        try:\n            if cfg.scheme == \'stagwise\':\n                train_basic_DNN(train_xy_file_list = (train_x_file_list, train_y_file_list), \\\n                          valid_xy_file_list = (valid_x_file_list, valid_y_file_list), \\\n                          nnets_file_name = nnets_file_name, \\\n                          n_ins = lab_dim, n_outs = cfg.cmp_dim, ms_outs = cfg.multistream_outs, \\\n                          hyper_params = cfg.hyper_params, buffer_size = cfg.buffer_size, plot = cfg.plot)\n                train_DNN_with_projections(train_xy_file_list = (train_x_file_list, train_y_file_list), \\\n                          valid_xy_file_list = (valid_x_file_list, valid_y_file_list), \\\n                          nnets_file_name = nnets_file_name, \\\n                          n_ins = lab_dim, n_outs = cfg.cmp_dim, ms_outs = cfg.multistream_outs, \\\n                          hyper_params = cfg.hyper_params, buffer_size = cfg.buffer_size, plot = cfg.plot)\n                infer_projections(train_xy_file_list = (train_x_file_list, train_y_file_list), \\\n                          valid_xy_file_list = (valid_x_file_list, valid_y_file_list), \\\n                          nnets_file_name = nnets_file_name, \\\n                          n_ins = lab_dim, n_outs = cfg.cmp_dim, ms_outs = cfg.multistream_outs, \\\n                          hyper_params = cfg.hyper_params, buffer_size = cfg.buffer_size, plot = cfg.plot)\n            elif cfg.scheme == \'simultaneous\':\n                train_DNN_and_traindev_projections(train_xy_file_list = (train_x_file_list, train_y_file_list), \\\n                          valid_xy_file_list = (valid_x_file_list, valid_y_file_list), \\\n                          nnets_file_name = nnets_file_name, \\\n                          n_ins = lab_dim, n_outs = cfg.cmp_dim, ms_outs = cfg.multistream_outs, \\\n                          hyper_params = cfg.hyper_params, buffer_size = cfg.buffer_size, plot = cfg.plot)\n            else:\n                sys.exit(\'unknown scheme!\')\n#                train_DNN(train_xy_file_list = (train_x_file_list, train_y_file_list), \\\n#                          valid_xy_file_list = (valid_x_file_list, valid_y_file_list), \\\n#                          nnets_file_name = nnets_file_name, \\\n#                          n_ins = lab_dim, n_outs = cfg.cmp_dim, ms_outs = cfg.multistream_outs, \\\n#                          hyper_params = cfg.hyper_params, buffer_size = cfg.buffer_size, plot = cfg.plot)\n#                infer_projections(train_xy_file_list = (train_x_file_list, train_y_file_list), \\\n#                          valid_xy_file_list = (valid_x_file_list, valid_y_file_list), \\\n#                          nnets_file_name = nnets_file_name, \\\n#                          n_ins = lab_dim, n_outs = cfg.cmp_dim, ms_outs = cfg.multistream_outs, \\\n#                          hyper_params = cfg.hyper_params, buffer_size = cfg.buffer_size, plot = cfg.plot)\n\n        except KeyboardInterrupt:\n            logger.critical(\'train_DNN interrupted via keyboard\')\n            # Could \'raise\' the exception further, but that causes a deep traceback to be printed\n            # which we don\'t care about for a keyboard interrupt. So, just bail out immediately\n            sys.exit(1)\n        except:\n            logger.critical(\'train_DNN threw an exception\')\n            raise\n\n    ### generate parameters from DNN (with random token reps and inferred ones -- NOTOKENS & TOKENS)\n    temp_dir_name_NOTOKENS = \'%s_%s_%d_%d_%d_%d_%d_%d_NOTOKENS\' \\\n                    %(cfg.model_type, cfg.combined_feature_name, int(cfg.do_post_filtering), \\\n                      cfg.train_file_number, lab_dim, cfg.cmp_dim, \\\n                      len(hidden_layers_sizes), hidden_layers_sizes[0])\n    gen_dir_NOTOKENS = os.path.join(gen_dir, temp_dir_name_NOTOKENS)\n\n    temp_dir_name_TOKENS = \'%s_%s_%d_%d_%d_%d_%d_%d_TOKENS\' \\\n                    %(cfg.model_type, cfg.combined_feature_name, int(cfg.do_post_filtering), \\\n                      cfg.train_file_number, lab_dim, cfg.cmp_dim, \\\n                      len(hidden_layers_sizes), hidden_layers_sizes[0])\n    gen_dir_TOKENS = os.path.join(gen_dir, temp_dir_name_TOKENS)\n\n    gen_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n    test_x_file_list  = nn_label_norm_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n    if cfg.DNNGEN:\n        logger.info(\'generating from DNN\')\n\n        try:\n            os.makedirs(gen_dir)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                # not an error - just means directory already exists\n                pass\n            else:\n                logger.critical(\'Failed to create generation directory %s\' % gen_dir)\n                logger.critical(\' OS error was: %s\' % e.strerror)\n                raise\n\n        ## Without words embeddings:\n        gen_file_list_NOTOKENS = prepare_file_path_list(gen_file_id_list, gen_dir_NOTOKENS, cfg.cmp_ext)\n        dnn_generation(test_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list_NOTOKENS, cfg=cfg, use_word_projections=False)\n\n        ## With word embeddings:\n        gen_file_list_TOKENS = prepare_file_path_list(gen_file_id_list, gen_dir_TOKENS, cfg.cmp_ext)\n        dnn_generation(test_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list_TOKENS, cfg=cfg, use_word_projections=True)\n\n        logger.debug(\'denormalising generated output using method %s\' % cfg.output_feature_normalisation)\n\n        for gen_file_list in [gen_file_list_NOTOKENS, gen_file_list_TOKENS]:\n\n            fid = open(norm_info_file, \'rb\')\n            cmp_min_max = numpy.fromfile(fid, dtype=numpy.float32)\n            fid.close()\n            cmp_min_max = cmp_min_max.reshape((2, -1))\n            cmp_min_vector = cmp_min_max[0, ]\n            cmp_max_vector = cmp_min_max[1, ]\n\n            if cfg.output_feature_normalisation == \'MVN\':\n                denormaliser = MeanVarianceNorm(feature_dimension = cfg.cmp_dim)\n                denormaliser.feature_denormalisation(gen_file_list, gen_file_list, cmp_min_vector, cmp_max_vector)\n\n            elif cfg.output_feature_normalisation == \'MINMAX\':\n                denormaliser = MinMaxNormalisation(cfg.cmp_dim, min_value = 0.01, max_value = 0.99, min_vector = cmp_min_vector, max_vector = cmp_max_vector)\n                denormaliser.denormalise_data(gen_file_list, gen_file_list)\n            else:\n                logger.critical(\'denormalising method %s is not supported!\\n\' %(cfg.output_feature_normalisation))\n                raise\n\n            ##perform MLPG to smooth parameter trajectory\n            ## lf0 is included, the output features much have vuv.\n            generator = ParameterGeneration(gen_wav_features = cfg.gen_wav_features)\n            generator.acoustic_decomposition(gen_file_list, cfg.cmp_dim, cfg.out_dimension_dict, cfg.file_extension_dict, var_file_dict)\n\n            ## osw: skip MLPG:\n#            split_cmp(gen_file_list, [\'mgc\', \'lf0\', \'bap\'], cfg.cmp_dim, cfg.out_dimension_dict, cfg.file_extension_dict)\n\n\n    ### generate wav\n    if cfg.GENWAV:\n        logger.info(\'reconstructing waveform(s)\')\n        for gen_dir in [gen_dir_NOTOKENS, gen_dir_TOKENS]:\n            generate_wav(gen_dir, gen_file_id_list, cfg)     # generated speech\n    #           generate_wav(nn_cmp_dir, gen_file_id_list)  # reference copy synthesis speech\n\n    ### evaluation: calculate distortion\n    if cfg.CALMCD:\n        logger.info(\'calculating MCD\')\n\n        ref_data_dir = os.path.join(data_dir, \'ref_data\')\n\n        ref_mgc_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.mgc_ext)\n        ref_bap_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.bap_ext)\n        ref_lf0_list = prepare_file_path_list(gen_file_id_list, ref_data_dir, cfg.lf0_ext)\n\n        in_gen_label_align_file_list = in_label_align_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n        calculator = IndividualDistortionComp()\n\n        spectral_distortion = 0.0\n        bap_mse             = 0.0\n        f0_mse              = 0.0\n        vuv_error           = 0.0\n\n        valid_file_id_list = file_id_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number]\n        test_file_id_list  = file_id_list[cfg.train_file_number+cfg.valid_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n        if cfg.remove_silence_using_binary_labels:\n            ## get lab_dim:\n            label_composer = LabelComposer()\n            label_composer.load_label_configuration(cfg.label_config_file)\n            lab_dim=label_composer.compute_label_dimension()\n\n            ## use first feature in label -- hardcoded for now\n            silence_feature = 0\n\n            ## Use these to trim silence:\n            untrimmed_test_labels = binary_label_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n\n\n        if \'mgc\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'mgc\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_mgc_list, cfg.mgc_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.mgc_dim, silence_pattern = [\'*-#+*\'])\n                remover.remove_silence(in_file_list_dict[\'mgc\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_mgc_list)\n            valid_spectral_distortion = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.mgc_ext, cfg.mgc_dim)\n            test_spectral_distortion  = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.mgc_ext, cfg.mgc_dim)\n            valid_spectral_distortion *= (10 /numpy.log(10)) * numpy.sqrt(2.0)    ##MCD\n            test_spectral_distortion  *= (10 /numpy.log(10)) * numpy.sqrt(2.0)    ##MCD\n\n\n        if \'bap\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'bap\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_bap_list, cfg.bap_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.bap_dim, silence_pattern = [\'*-#+*\'])\n                remover.remove_silence(in_file_list_dict[\'bap\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_bap_list)\n            valid_bap_mse        = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.bap_ext, cfg.bap_dim)\n            test_bap_mse         = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.bap_ext, cfg.bap_dim)\n            valid_bap_mse = valid_bap_mse / 10.0    ##Cassia\'s bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC\n            test_bap_mse  = test_bap_mse / 10.0    ##Cassia\'s bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC\n\n        if \'lf0\' in cfg.in_dimension_dict:\n            if cfg.remove_silence_using_binary_labels:\n                untrimmed_reference_data = in_file_list_dict[\'lf0\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                trim_silence(untrimmed_reference_data, ref_lf0_list, cfg.lf0_dim, \\\n                                    untrimmed_test_labels, lab_dim, silence_feature)\n            else:\n                remover = SilenceRemover(n_cmp = cfg.lf0_dim, silence_pattern = [\'*-#+*\'])\n                remover.remove_silence(in_file_list_dict[\'lf0\'][cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number], in_gen_label_align_file_list, ref_lf0_list)\n            valid_f0_mse, valid_vuv_error   = calculator.compute_distortion(valid_file_id_list, ref_data_dir, gen_dir, cfg.lf0_ext, cfg.lf0_dim)\n            test_f0_mse , test_vuv_error    = calculator.compute_distortion(test_file_id_list , ref_data_dir, gen_dir, cfg.lf0_ext, cfg.lf0_dim)\n\n        logger.info(\'Develop: DNN -- MCD: %.3f dB; BAP: %.3f dB; F0: %.3f Hz; VUV: %.3f%%\' \\\n                    %(valid_spectral_distortion, valid_bap_mse, valid_f0_mse, valid_vuv_error*100.))\n        logger.info(\'Test   : DNN -- MCD: %.3f dB; BAP: %.3f dB; F0: %.3f Hz; VUV: %.3f%%\' \\\n                    %(test_spectral_distortion , test_bap_mse , test_f0_mse , test_vuv_error*100.))\n\n        # this can be removed\n        #\n        if  0: #to calculate distortion of HMM baseline\n            hmm_gen_no_silence_dir = \'/afs/inf.ed.ac.uk/group/project/dnn_tts/data/nick/nick_hmm_pf_2400_no_silence\'\n            hmm_gen_dir = \'/afs/inf.ed.ac.uk/group/project/dnn_tts/data/nick/nick_hmm_pf_2400\'\n\n            if 1:\n                hmm_mgc_list = prepare_file_path_list(gen_file_id_list, hmm_gen_dir, cfg.mgc_ext)\n                hmm_bap_list = prepare_file_path_list(gen_file_id_list, hmm_gen_dir, cfg.bap_ext)\n                hmm_lf0_list = prepare_file_path_list(gen_file_id_list, hmm_gen_dir, cfg.lf0_ext)\n\n                hmm_mgc_no_silence_list = prepare_file_path_list(gen_file_id_list, hmm_gen_no_silence_dir, cfg.mgc_ext)\n                hmm_bap_no_silence_list = prepare_file_path_list(gen_file_id_list, hmm_gen_no_silence_dir, cfg.bap_ext)\n                hmm_lf0_no_silence_list = prepare_file_path_list(gen_file_id_list, hmm_gen_no_silence_dir, cfg.lf0_ext)\n\n                in_gen_label_align_file_list = in_label_align_file_list[cfg.train_file_number:cfg.train_file_number+cfg.valid_file_number+cfg.test_file_number]\n                remover = SilenceRemover(n_cmp = cfg.mgc_dim, silence_pattern = [\'*-#+*\'])\n                remover.remove_silence(hmm_mgc_list, in_gen_label_align_file_list, hmm_mgc_no_silence_list)\n\n                remover = SilenceRemover(n_cmp = cfg.bap_dim, silence_pattern = [\'*-#+*\'])\n                remover.remove_silence(hmm_bap_list, in_gen_label_align_file_list, hmm_bap_no_silence_list)\n\n                remover = SilenceRemover(n_cmp = cfg.lf0_dim, silence_pattern = [\'*-#+*\'])\n                remover.remove_silence(hmm_lf0_list, in_gen_label_align_file_list, hmm_lf0_no_silence_list)\n\n            calculator = IndividualDistortionComp()\n\n            spectral_distortion = calculator.compute_distortion(valid_file_id_list, ref_data_dir, hmm_gen_no_silence_dir, cfg.mgc_ext, cfg.mgc_dim)\n            bap_mse             = calculator.compute_distortion(valid_file_id_list, ref_data_dir, hmm_gen_no_silence_dir, cfg.bap_ext, cfg.bap_dim)\n            f0_mse, vuv_error   = calculator.compute_distortion(valid_file_id_list, ref_data_dir, hmm_gen_no_silence_dir, cfg.lf0_ext, cfg.lf0_dim)\n\n            spectral_distortion *= (10 /numpy.log(10)) * numpy.sqrt(2.0)\n            bap_mse = bap_mse / 10.0\n\n            logger.info(\'Develop: HMM -- MCD: %.3f dB; BAP: %.3f dB; F0: %.3f Hz; VUV: %.3f%%\' %(spectral_distortion, bap_mse, f0_mse, vuv_error*100.))\n\n            spectral_distortion = calculator.compute_distortion(test_file_id_list, ref_data_dir, hmm_gen_no_silence_dir, cfg.mgc_ext, cfg.mgc_dim)\n            bap_mse             = calculator.compute_distortion(test_file_id_list, ref_data_dir, hmm_gen_no_silence_dir, cfg.bap_ext, cfg.bap_dim)\n            f0_mse, vuv_error   = calculator.compute_distortion(test_file_id_list, ref_data_dir, hmm_gen_no_silence_dir, cfg.lf0_ext, cfg.lf0_dim)\n\n            spectral_distortion *= (10 /numpy.log(10)) * numpy.sqrt(2.0)\n            bap_mse = bap_mse / 10.0\n\n            logger.info(\'Test   : HMM -- MCD: %.3f dB; BAP: %.3f dB; F0: %.3f Hz; VUV: %.3f%%\' %(spectral_distortion, bap_mse, f0_mse, vuv_error*100.))\n\nif __name__ == \'__main__\':\n\n\n\n    # these things should be done even before trying to parse the command line\n\n    # create a configuration instance\n    # and get a short name for this instance\n    cfg=configuration.cfg\n\n    # set up logging to use our custom class\n    logging.setLoggerClass(LoggerPlotter)\n\n    # get a logger for this main function\n    logger = logging.getLogger(""main"")\n\n\n    if len(sys.argv) != 2:\n        logger.critical(\'usage: run_dnn.sh [config file name]\')\n        sys.exit(1)\n\n    config_file = sys.argv[1]\n\n    config_file = os.path.abspath(config_file)\n    cfg.configure(config_file)\n\n    if cfg.profile:\n        logger.info(\'profiling is activated\')\n        import cProfile, pstats\n        cProfile.run(\'main_function(cfg)\', \'mainstats\')\n\n        # create a stream for the profiler to write to\n        profiling_output = io.StringIO()\n        p = pstats.Stats(\'mainstats\', stream=profiling_output)\n\n        # print stats to that stream\n        # here we just report the top 10 functions, sorted by total amount of time spent in each\n        p.strip_dirs().sort_stats(\'tottime\').print_stats(10)\n\n        # print the result to the log\n        logger.info(\'---Profiling result follows---\\n%s\' %  profiling_output.getvalue() )\n        profiling_output.close()\n        logger.info(\'---End of profiling result---\')\n\n    else:\n        main_function(cfg)\n\n    sys.exit(0)\n'"
egs/slt_arctic/s2/scripts/__init__.py,0,b''
egs/slt_arctic/s2/scripts/gpu_lock.py,0,"b'#!/usr/bin/python\n\n""""""\nA simple discretionary locking system for /dev/nvidia devices.\n\nIain Murray, November 2009, January 2010, January 2011.\n""""""\n\nimport os, os.path\n\n_dev_prefix = \'/dev/nvidia\'\n#URL = \'http://www.cs.toronto.edu/~murray/code/gpu_monitoring/\'\nURL = \'http://homepages.inf.ed.ac.uk/imurray2/code/gpu_monitoring/\'\n\n\n# Get ID\'s of NVIDIA boards. Should do this through a CUDA call, but this is\n# a quick and dirty way that works for now:\ndef board_ids():\n    """"""Returns integer board ids available on this machine.""""""\n    from glob import glob\n    board_devs = glob(_dev_prefix + \'[0-9]*\')\n    return list(range(len(board_devs)))\n\ndef _lock_file(id):\n    """"""lock file from integer id""""""\n    # /tmp is cleared on reboot on many systems, but it doesn\'t have to be\n    if os.path.exists(\'/dev/shm\'):\n        # /dev/shm on linux machines is a RAM disk, so is definitely cleared\n        return \'/dev/shm/gpu_lock_%d\' % id\n    else:\n        return \'/tmp/gpu_lock_%d\' % id\n\ndef owner_of_lock(id):\n    """"""Username that has locked the device id. (Empty string if no lock).""""""\n    import pwd\n    try:\n        statinfo = os.lstat(_lock_file(id))\n        return pwd.getpwuid(statinfo.st_uid).pw_name\n    except:\n        return """"\n\ndef _obtain_lock(id):\n    """"""Attempts to lock id, returning success as True/False.""""""\n#    print   id\n    try:\n        # On POSIX systems symlink creation is atomic, so this should be a\n        # robust locking operation:\n        os.symlink(\'/dev/null\', _lock_file(id))\n        return True\n    except:\n        return False\n\ndef _launch_reaper(id, pid):\n    """"""Start a process that will free a lock when process pid terminates""""""\n    from subprocess import Popen, PIPE\n    me = __file__\n    if me.endswith(\'.pyc\'):\n        me = me[:-1]\n    myloc = os.path.dirname(me)\n    if not myloc:\n        myloc = os.getcwd()\n    reaper_cmd = os.path.join(myloc, \'run_on_me_or_pid_quit\')\n    Popen([reaper_cmd, str(pid), me, \'--free\', str(id)],\n        stdout=open(\'/dev/null\', \'w\'))\n\ndef obtain_lock_id(pid = None):\n    """"""\n    Finds a free id, locks it and returns integer id, or -1 if none free.\n\n    A process is spawned that will free the lock automatically when the\n    process pid (by default the current python process) terminates.\n    """"""\n    id = -1\n    id = obtain_lock_id_to_hog()\n    try:\n        if id >= 0:\n            if pid is None:\n                pid = os.getpid()\n            _launch_reaper(id, pid)\n    except:\n        free_lock(id)\n        id = -1\n    return id\n\ndef obtain_lock_id_to_hog():\n    """"""\n    Finds a free id, locks it and returns integer id, or -1 if none free.\n\n    * Lock must be freed manually *\n    """"""\n    for id in board_ids():\n        if _obtain_lock(id):\n            return id\n    return -1\n\ndef free_lock(id):\n    """"""Attempts to free lock id, returning success as True/False.""""""\n    try:\n        filename = _lock_file(id)\n        # On POSIX systems os.rename is an atomic operation, so this is the safe\n        # way to delete a lock:\n        os.rename(filename, filename + \'.redundant\')\n        os.remove(filename + \'.redundant\')\n        return True\n    except:\n        return False\n\n\n# If run as a program:\nif __name__ == ""__main__"":\n    import sys\n    me = sys.argv[0]\n    # Report\n    if \'--id\' in sys.argv:\n        if len(sys.argv) > 2:\n            try:\n                pid = int(sys.argv[2])\n                print(pid, sys.argv[2])\n                assert(os.path.exists(\'/proc/%d\' % pid))\n            except:\n                print(\'Usage: %s --id [pid_to_wait_on]\' % me)\n                print(\'The optional process id must exist if specified.\')\n                print(\'Otherwise the id of the parent process is used.\')\n                sys.exit(1)\n        else:\n            pid = os.getppid()\n            print(pid)\n        print(obtain_lock_id(pid))\n    elif \'--id-to-hog\' in sys.argv:\n        print(obtain_lock_id_to_hog())\n    elif \'--free\' in sys.argv:\n        try:\n            id = int(sys.argv[2])\n        except:\n            print(\'Usage: %s --free <id>\' % me)\n            sys.exit(1)\n        if free_lock(id):\n            print(""Lock freed"")\n        else:\n            owner = owner_of_lock(id)\n            if owner:\n                print(""Failed to free lock id=%d owned by %s"" % (id, owner))\n            else:\n                print(""Failed to free lock, but it wasn\'t actually set?"")\n    else:\n        print(\'\\n  Usage instructions:\\n\')\n        print(\'  To obtain and lock an id: %s --id\' % me)\n        print(\'  The lock is automatically freed when the parent terminates\')\n        print()\n        print(""  To get an id that won\'t be freed: %s --id-to-hog"" % me)\n        print(""  You *must* manually free these ids: %s --free <id>\\n"" % me)\n        print(\'  More info: %s\\n\' % URL)\n        div = \'  \' + ""-""*60\n        print(\'\\n\' + div)\n        print(""  NVIDIA board users:"")\n        print(div)\n        for id in board_ids():\n            print(""      Board %d: %s"" % (id, owner_of_lock(id)))\n        print(div + \'\\n\')\n'"
egs/slt_arctic/s2/scripts/label_st_align_to_var_rate.py,0,"b'#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n""""""\n@author: Felipe Espic\n\nDESCRIPTION:\nAs Merlin works at a constant frame rate, but MagPhase runs at a variable frame\nrate, it is needed to trick Merlin by warping the time durations in the label files.\nThis script converts the original constant-frame-rate state aligned labels to\nvariable-frame-rate labels, thus compensating for the frame rate missmatch.\nThis script acts as a workaround, so it should be removed when Merlin natively\nsupport variable frame rates.\n\nUSE:\npyhthon <this_script_name.py> <merlin_acoustic_training_config_file.conf>\n\nNOTES:\n1.- This script needs "".shift"" files extracted by MagPhase, even though they are not\n    used for acoustic modelling (only .mag, .real, .imag, and .lf0 files are used in training/synthesis).\n\n2.- The file crashlist_file stores the list of utterances that were not possible\n    to convert. This could happen if for example some phonemes had no frames assigned.\n    It rarelly occurs.\n""""""\n\nimport sys, os\n\nthis_dir = os.path.dirname(__file__)\nsys.path.append(os.path.realpath(this_dir + \'/../../../../tools/magphase/src\'))\nimport libutils as lu\nimport magphase as mp\nimport libaudio as la\n\ndef convert(file_id_list, in_lab_dir, in_feats_dir, fs, out_lab_dir, b_prevent_zeros=False):\n\n    \'\'\'\n    b_prevent_zeros: True if you want to ensure that all the phonemes have one frame at least.\n    (not recommended, only useful when there are too many utterances crashed)\n    \'\'\'\n\n    # Conversion:\n    lu.mkdir(out_lab_dir)\n    v_filenames = lu.read_text_file2(file_id_list, dtype=\'string\', comments=\'#\')\n\n    crashlist_file = lu.ins_pid(\'crash_file_list.scp\')\n    for filename in v_filenames:\n\n        # Display:\n        print(\'\\nConverting lab file: \' + filename + \'................................\')\n\n        # Current i/o files:\n        in_lab_file   = os.path.join(in_lab_dir  , filename + \'.lab\')\n        out_lab_file  = os.path.join(out_lab_dir , filename + \'.lab\')\n\n        in_shift_file = os.path.join(in_feats_dir, filename + \'.shift\')\n\n\n        # Debug:\n        \'\'\'\n        v_shift  = lu.read_binfile(in_shift_file, dim=1)\n        v_n_frms = mp.get_num_of_frms_per_state(v_shift, in_lab_file, fs, b_prevent_zeros=b_prevent_zeros)\n        la.convert_label_state_align_to_var_frame_rate(in_lab_file, v_n_frms, out_lab_file)\n        #\'\'\'\n\n        try:\n            v_shift  = lu.read_binfile(in_shift_file, dim=1)\n            v_n_frms = mp.get_num_of_frms_per_state(v_shift, in_lab_file, fs, b_prevent_zeros=b_prevent_zeros)\n\n            la.convert_label_state_align_to_var_frame_rate(in_lab_file, v_n_frms, out_lab_file)\n\n        except (KeyboardInterrupt, SystemExit):\n            raise\n\n        except:\n            with open(crashlist_file, ""a"") as crashlistlog:\n                crashlistlog.write(filename + \'\\n\')\n\n    print(\'Done!\')\n\n\nif __name__ == \'__main__\':\n\n    # Parsing input arg:\n    file_id_list = sys.argv[1]\n    in_lab_dir   = sys.argv[2]\n    in_feats_dir = sys.argv[3]\n    fs           = int(sys.argv[4])\n    out_lab_dir  = sys.argv[5]\n\n    convert(file_id_list, in_lab_dir, in_feats_dir, fs, out_lab_dir, b_prevent_zeros=False)\n'"
misc/scripts/alignment/state_align/binary_io.py,0,"b""\n\nimport numpy\n\nclass   BinaryIOCollection(object):\n\n    def load_binary_file(self, file_name, dimension):\n        fid_lab = open(file_name, 'rb')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        assert features.size % float(dimension) == 0.0,'specified dimension not compatible with data'\n        features = features[:(dimension * (features.size // dimension))]\n        features = features.reshape((-1, dimension))\n\n        return  features\n\n    def array_to_binary_file(self, data, output_file_name):\n        data = numpy.array(data, 'float32')\n\n        fid = open(output_file_name, 'wb')\n        data.tofile(fid)\n        fid.close()\n\n    def load_binary_file_frame(self, file_name, dimension):\n        fid_lab = open(file_name, 'rb')\n        features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n        fid_lab.close()\n        assert features.size % float(dimension) == 0.0,'specified dimension not compatible with data'\n        frame_number = features.size // dimension\n        features = features[:(dimension * frame_number)]\n        features = features.reshape((-1, dimension))\n\n        return  features, frame_number\n"""
misc/scripts/alignment/state_align/forced_alignment.py,0,"b'import os, sys\nimport time\n\nfrom sys import argv, stderr\nfrom subprocess import check_call, Popen, CalledProcessError, PIPE\nfrom mean_variance_norm import MeanVarianceNorm\n\n# string constants for various shell calls\nSTATE_NUM=5\nF = str(0.01)\nSFAC = str(5.0)\nPRUNING = [str(i) for i in (250., 150., 2000.)]\n\nMACROS = \'macros\'\nHMMDEFS = \'hmmdefs\'\nVFLOORS = \'vFloors\'\n\n##\nHTKDIR = path/to/tools/htk\nHCompV = os.path.join(HTKDIR, \'HCompV\')\nHCopy  = os.path.join(HTKDIR, \'HCopy\' )\nHERest = os.path.join(HTKDIR, \'HERest\')\nHHEd   = os.path.join(HTKDIR, \'HHEd\'  )\nHVite  = os.path.join(HTKDIR, \'HVite\' )\n\nclass ForcedAlignment(object):\n\n    def __init__(self):\n        self.proto = None\n        self.phoneme_mlf = None\n\n    def _make_proto(self):\n        ## make proto\n        fid = open(self.proto, \'w\')\n        means = \' \'.join([\'0.0\' for _ in range(39)])\n        varg  = \' \'.join([\'1.0\' for _ in range(39)])\n        fid.write(""""""~o <VECSIZE> 39 <USER>\n~h ""proto""\n<BEGINHMM>\n<NUMSTATES> 7\n"""""")\n        for i in range(2, STATE_NUM+2):\n            fid.write(\'<STATE> {0}\\n<MEAN> 39\\n{1}\\n\'.format(i, means))\n            fid.write(\'<VARIANCE> 39\\n{0}\\n\'.format(varg))\n        fid.write(""""""<TRANSP> 7\n 0.0 1.0 0.0 0.0 0.0 0.0 0.0\n 0.0 0.6 0.4 0.0 0.0 0.0 0.0\n 0.0 0.0 0.6 0.4 0.0 0.0 0.0\n 0.0 0.0 0.0 0.6 0.4 0.0 0.0\n 0.0 0.0 0.0 0.0 0.6 0.4 0.0\n 0.0 0.0 0.0 0.0 0.0 0.7 0.3\n 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n<ENDHMM>\n"""""")\n        fid.close()\n\n        ## make vFloors\n        check_call([HCompV, \'-f\', F, \'-C\', self.cfg,\n                              \'-S\', self.train_scp,\n                              \'-M\', self.cur_dir, self.proto])\n        ## make local macro\n        # get first three lines from local proto\n        fid = open(os.path.join(self.cur_dir, MACROS), \'w\')\n        source = open(os.path.join(self.cur_dir,\n                      os.path.split(self.proto)[1]), \'r\')\n        for _ in range(3):\n            fid.write(source.readline())\n        source.close()\n        # get remaining lines from vFloors\n        fid.writelines(open(os.path.join(self.cur_dir,\n                                          VFLOORS), \'r\').readlines())\n        fid.close()\n        ## make hmmdefs\n        fid = open(os.path.join(self.cur_dir, HMMDEFS), \'w\')\n        for phone in open(self.phonemes, \'r\'):\n            source = open(self.proto, \'r\')\n            # ignore\n            source.readline()\n            source.readline()\n            # the header\n            fid.write(\'~h ""{0}""\\n\'.format(phone.rstrip()))\n            # the rest\n            fid.writelines(source.readlines())\n            source.close()\n        fid.close()\n\n    def _read_file_list(self, file_name):\n\n        file_lists = []\n        fid = open(file_name)\n        for line in fid.readlines():\n            line = line.strip()\n            if len(line) < 1:\n                continue\n            file_lists.append(line)\n        fid.close()\n\n        return  file_lists\n\n    def _full_to_mono(self, full_file_name, mono_file_name, phoneme_dict):\n        fre = open(full_file_name, \'r\')\n        fwe = open(mono_file_name, \'w\')\n        for line in fre.readlines():\n            line = line.strip()\n            if len(line) < 1:\n                continue\n            tmp_list = line.split(\'-\')\n            tmp_list = tmp_list[1].split(\'+\')\n            mono_phone = tmp_list[0]\n            fwe.write(\'{0}\\n\'.format(mono_phone))\n            if mono_phone not in phoneme_dict:\n                phoneme_dict[mono_phone] = 1\n            phoneme_dict[mono_phone] += 1\n        fwe.close()\n        fre.close()\n\n    def _check_data(self, file_id_list, multiple_speaker):\n\n        copy_scp = open(self.copy_scp, \'w\')\n        check_scp = open(self.train_scp, \'w\')\n        i = 0\n\n        phoneme_dict = {}\n        speaker_utt_dict = {}\n\n        for file_id in file_id_list:\n            wav_file = os.path.join(self.wav_dir, file_id + \'.wav\')\n            lab_file = os.path.join(self.lab_dir, file_id + \'.lab\')\n            mfc_file = os.path.join(self.mfc_dir, file_id + \'.mfc\')\n            mono_lab_file = os.path.join(self.mono_lab_dir, file_id + \'.lab\')\n\n            mfc_sub_dir = os.path.dirname(mfc_file)\n            if os.path.exists(wav_file) and os.path.exists(lab_file):\n                if not os.path.exists(mfc_sub_dir):\n                    os.makedirs(mfc_sub_dir)\n\n                copy_scp.write(\'{0} {1}\\n\'.format(wav_file, mfc_file))\n                check_scp.write(\'{0}\\n\'.format(mfc_file))\n\n\n                if multiple_speaker:\n                    tmp_list = file_id.split(\'/\')\n                    speaker_name = tmp_list[0]\n                    if speaker_name not in speaker_utt_dict:\n                        speaker_utt_dict[speaker_name] = []\n                    speaker_utt_dict[speaker_name].append(mfc_file)\n                else:\n                    if \'only_one\' not in speaker_utt_dict:\n                        speaker_utt_dict[\'only_one\'] = []\n                    speaker_utt_dict[\'only_one\'].append(mfc_file)\n\n\n                self._full_to_mono(lab_file, mono_lab_file, phoneme_dict)\n        copy_scp.close()\n        check_scp.close()\n\n        fid = open(self.phonemes, \'w\')\n        fmap = open(self.phoneme_map, \'w\')\n        for phoneme in list(phoneme_dict.keys()):\n            fid.write(\'{0}\\n\'.format(phoneme))\n            fmap.write(\'{0} {0}\\n\'.format(phoneme))\n        fmap.close()\n        fid.close()\n\n        self.phoneme_mlf = os.path.join(self.cfg_dir, \'mono_phone.mlf\')\n        fid = open(self.phoneme_mlf, \'w\')\n        fid.write(\'#!MLF!#\\n\')\n        fid.write(\'""*/*.lab"" -> ""\' + self.mono_lab_dir + \'""\\n\')\n        fid.close()\n\n        return  speaker_utt_dict\n\n    def _HCopy(self):\n        """"""\n        Compute MFCCs\n        """"""\n        # write a CFG for extracting MFCCs\n        open(self.cfg, \'w\').write(""""""SOURCEKIND = WAVEFORM\nSOURCEFORMAT = WAVE\nTARGETRATE = 50000.0\nTARGETKIND = MFCC_D_A_0\nWINDOWSIZE = 250000.0\nPREEMCOEF = 0.97\nUSEHAMMING = T\nENORMALIZE = T\nCEPLIFTER = 22\nNUMCHANS = 20\nNUMCEPS = 12\n"""""")\n        check_call([HCopy, \'-C\', self.cfg, \'-S\', self.copy_scp])\n        # write a CFG for what we just built\n        open(self.cfg, \'w\').write(""""""TARGETRATE = 50000.0\nTARGETKIND = USER\nWINDOWSIZE = 250000.0\nPREEMCOEF = 0.97\nUSEHAMMING = T\nENORMALIZE = T\nCEPLIFTER = 22\nNUMCHANS = 20\nNUMCEPS = 12\n"""""")\n\n    def _nxt_dir(self):\n        """"""\n        Get the next HMM directory\n        """"""\n        # pass on the previously new one to the old one\n        self.cur_dir = self.nxt_dir\n        # increment\n        self.n += 1\n        # compute the path for the new one\n        self.nxt_dir = os.path.join(self.hmm_dir, str(self.n).zfill(3))\n        # make the new directory\n        os.mkdir(self.nxt_dir)\n\n    def prepare_training(self, file_id_list_name, wav_dir, lab_dir, work_dir, multiple_speaker):\n\n        print(\'---preparing enverionment\')\n        self.cfg_dir = os.path.join(work_dir, \'config\')\n        self.model_dir = os.path.join(work_dir, \'model\')\n        self.cur_dir = os.path.join(self.model_dir, \'hmm0\')\n        if not os.path.exists(self.cfg_dir):\n            os.makedirs(self.cfg_dir)\n        if not os.path.exists(self.cur_dir):\n            os.makedirs(self.cur_dir)\n\n        self.phonemes = os.path.join(work_dir, \'mono_phone.list\')\n        self.phoneme_map = os.path.join(work_dir, \'phoneme_map.dict\')\n        # HMMs\n        self.proto = os.path.join(self.cfg_dir, \'proto\')\n        # SCP files\n        self.copy_scp = os.path.join(self.cfg_dir, \'copy.scp\')\n        self.test_scp = os.path.join(self.cfg_dir, \'test.scp\')\n        self.train_scp = os.path.join(self.cfg_dir, \'train.scp\')\n        # CFG\n        self.cfg = os.path.join(self.cfg_dir, \'cfg\')\n\n        self.wav_dir=wav_dir\n        self.lab_dir = lab_dir\n        self.mfc_dir = os.path.join(work_dir, \'mfc\')\n        if not os.path.exists(self.mfc_dir):\n            os.makedirs(self.mfc_dir)\n\n        self.mono_lab_dir = os.path.join(work_dir, \'mono_no_align\')\n        if not os.path.exists(self.mono_lab_dir):\n            os.makedirs(self.mono_lab_dir)\n\n        file_id_list = self._read_file_list(file_id_list_name)\n        print(\'---checking data\')\n        speaker_utt_dict = self._check_data(file_id_list, multiple_speaker)\n\n        print(\'---extracting features\')\n        self._HCopy()\n        print(time.strftime(""%c""))\n\n        print(\'---feature_normalisation\')\n        normaliser = MeanVarianceNorm(39)\n        for key_name in list(speaker_utt_dict.keys()):\n            normaliser.feature_normalisation(speaker_utt_dict[key_name], speaker_utt_dict[key_name])  ## save to itself\n        print(time.strftime(""%c""))\n\n        print(\'---making proto\')\n        self._make_proto()\n\n    def train_hmm(self, niter, num_mix):\n        """"""\n        Perform one or more rounds of estimation\n        """"""\n\n        print(time.strftime(""%c""))\n        print(\'---training HMM models\')\n        done = 0\n        mix = 1\n        while mix <= num_mix and done == 0:\n            for i in range(niter):\n                next_dir = os.path.join(self.model_dir, \'hmm_mix_\' + str(mix) + \'_iter_\' + str(i+1))\n                if not os.path.exists(next_dir):\n                    os.makedirs(next_dir)\n                check_call([HERest, \'-C\', self.cfg, \'-S\', self.train_scp,\n                            \'-I\', self.phoneme_mlf,\n                            \'-M\', next_dir,\n                            \'-H\', os.path.join(self.cur_dir, MACROS),\n                            \'-H\', os.path.join(self.cur_dir, HMMDEFS),\n                            \'-t\'] + PRUNING + [self.phonemes],\n                           stdout=PIPE)\n                self.cur_dir = next_dir\n\n            if mix * 2 <= num_mix:\n                ##increase mixture number\n                hed_file = os.path.join(self.cfg_dir, \'mix_\' + str(mix * 2) + \'.hed\')\n                fid = open(hed_file, \'w\')\n                fid.write(\'MU \' + str(mix * 2) + \' {*.state[2-\'+str(STATE_NUM+2)+\'].mix}\\n\')\n                fid.close()\n\n                next_dir = os.path.join(self.model_dir, \'hmm_mix_\' + str(mix * 2) + \'_iter_0\')\n                if not os.path.exists(next_dir):\n                    os.makedirs(next_dir)\n\n                check_call( [HHEd, \'-A\',\n                             \'-H\', os.path.join(self.cur_dir, MACROS),\n                             \'-H\', os.path.join(self.cur_dir, HMMDEFS),\n                             \'-M\', next_dir] + [hed_file] + [self.phonemes])\n\n                self.cur_dir = next_dir\n                mix = mix * 2\n            else:\n                done = 1\n\n    def align(self, work_dir, lab_align_dir):\n        """"""\n        Align using the models in self.cur_dir and MLF to path\n        """"""\n        print(\'---aligning data\')\n        print(time.strftime(""%c""))\n        self.align_mlf = os.path.join(work_dir, \'mono_align.mlf\')\n\n        check_call([HVite, \'-a\', \'-f\', \'-m\', \'-y\', \'lab\', \'-o\', \'SM\',\n                    \'-i\', self.align_mlf, \'-L\', self.mono_lab_dir,\n                    \'-C\', self.cfg, \'-S\', self.train_scp,\n                    \'-H\', os.path.join(self.cur_dir, MACROS),\n                    \'-H\', os.path.join(self.cur_dir, HMMDEFS),\n                    \'-I\', self.phoneme_mlf, \'-t\'] + PRUNING +\n                   [\'-s\', SFAC, self.phoneme_map, self.phonemes])\n\n        self._postprocess(self.align_mlf, lab_align_dir)\n\n    def _postprocess(self, mlf, lab_align_dir):\n        if not os.path.exists(lab_align_dir):\n            os.makedirs(lab_align_dir)\n\n        state_num = STATE_NUM\n        fid = open(mlf, \'r\')\n        line = fid.readline()\n        while True:\n            line = fid.readline()\n            line = line.strip()\n            if len(line) < 1:\n                break\n            line = line.replace(\'""\', \'\')\n            file_base = os.path.basename(line)\n            flab = open(os.path.join(self.lab_dir, file_base), \'r\')\n            fw   = open(os.path.join(lab_align_dir, file_base), \'w\')\n            for full_lab in flab.readlines():\n                full_lab = full_lab.strip()\n                for i in range(state_num):\n                    line = fid.readline()\n                    line = line.strip()\n                    tmp_list = line.split()\n                    fw.write(\'{0} {1} {2}[{3}]\\n\'.format(tmp_list[0], tmp_list[1], full_lab, i+2))\n\n            fw.close()\n            flab.close()\n            line = fid.readline()\n            line = line.strip()\n            if line != \'.\':\n                print(\'The two files are not matched!\\n\')\n                sys.exit(1)\n        fid.close()\n\n\nif __name__ == \'__main__\':\n\n    work_dir = os.getcwd()\n\n    wav_dir = os.path.join(work_dir, \'slt_wav\')\n    lab_dir = os.path.join(work_dir, \'label_no_align\')\n    lab_align_dir = os.path.join(work_dir, \'label_state_align\')\n\n    file_id_list_name = os.path.join(work_dir, \'file_id_list.scp\')\n\n    ## if multiple_speaker is tuned on. the file_id_list.scp has to reflact this\n    ## for example\n    ## speaker_1/0001\n    ## speaker_2/0001\n    ## This is to do speaker-dependent normalisation\n    multiple_speaker = False\n\n    aligner = ForcedAlignment()\n    aligner.prepare_training(file_id_list_name, wav_dir, lab_dir, work_dir, multiple_speaker)\n\n    aligner.train_hmm(7, 32)\n    aligner.align(work_dir, lab_align_dir)\n    print(\'---done!\')\n'"
misc/scripts/alignment/state_align/htk_io.py,0,"b'\'\'\'\nCopyright 2011-2013 Pawel Swietojanski\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nTHIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\nWARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\nMERCHANTABLITY OR NON-INFRINGEMENT.\nSee the Apache 2 License for the specific language governing permissions and\nlimitations under the License.\n\nNot fully implemented [28 OCT 2011]\nTODO: support for options: _C, H_IREFC\n\n\'\'\'\n\nimport io, os, sys, numpy, struct, logging\n\nclass HTK_Parm_IO(object):\n    \'\'\'\n    For details look at the HTK book, Chapter 5.10 Storage of Parameter Files\n    \'\'\'\n\n    # HTK datatybes\n    H_WAVEFORM  = 0\n    H_LPC       = 1\n    H_LPREFC    = 2\n    H_LPCEPSTRA = 3\n    H_LPDELCEP  = 4\n    H_IREFC     = 5\n    H_MFCC      = 6\n    H_FBANK     = 7\n    H_MELSPEC   = 8\n    H_USER      = 9\n    H_DISCRETE  = 10\n    H_PLP       = 11\n    H_ANON      = 12\n\n    # Additional \'param kind\' options\n    _E = 0x0001 #has energy\n    _N = 0x0002 #absolute energy suppressed\n    _D = 0x0004 #has delta coefficients\n    _A = 0x0008 #has acceleration coefficients\n    _C = 0x0010 #is compressed\n    _Z = 0x0020 #has zero mean static coef.\n    _K = 0x0040 #has CRC checksum\n    _O = 0x0080 #has 0th cepstral coef.\n    _V = 0x0100 #has VQ data\n    _T = 0x0200 #has third differential coef.\n\n    MASK_H_DATATYPE = 0x003f # the first 6 bits contain datatype\n\n    def __init__(self, n_samples=0, samp_period=0, samp_size=0, param_kind = 0, data=None):\n        \'\'\'\n        \'\'\'\n\n        # HTK header\n        self.n_samples = n_samples # number of samples in file (4-byte integer)\n        self.samp_period = samp_period # sample period in 100ns units (4-byte integer)\n        self.samp_size = samp_size   # number of bytes per sample (2-byte integer)\n        self.param_kind = param_kind  # a code indicating the sample kind (2-byte integer)\n\n        self.data = data\n\n        return None\n\n    def htk_datatype(self):\n        return (self.param_kind & self.MASK_H_DATATYPE)\n\n\n    def set_htk_datatype(self, value):\n        self.param_kind = value | ~self.MASK_H_DATATYPE\n\n\n    def htk_datatype_has_option(self, option):\n        """"""Return True/False if the given options are set\n\n        :type option: int\n        :param option: one of the _E _N _D etc. flags\n\n        """"""\n        return (((self.param_kind>>6) & option)>0)\n\n\n    def set_htk_datatype_option(self, value):\n        self.param_kind = (value<<6) | self.param_kind\n\n    def read_htk(self, filename, reshape_to_matrix=True):\n        \'\'\'\n        \'\'\'\n        try:\n\n            f = open(filename, \'rb\')\n\n            self.n_samples = struct.unpack(\'<I\', f.read(4))[0]\n            self.samp_period = struct.unpack(\'<I\', f.read(4))[0]\n            self.samp_size = struct.unpack(\'<H\', f.read(2))[0]\n            self.param_kind = struct.unpack(\'<H\', f.read(2))[0]\n\n            print(self.n_samples, self.samp_size)\n            print(self.param_kind, self._C)\n\n            if (self.htk_datatype_has_option(self._C)):\n                #TODO compression\n                #self.A = struct.unpack(\'>H\', f.read(2))[0]\n                #self.B = struct.unpack(\'>H\', f.read(2))[0]\n                raise Exception(""Compressed files not supported yet!"")\n\n            if (self.htk_datatype() == self.H_WAVEFORM):\n                self.data = numpy.fromfile(f, numpy.int16)\n            else:\n                self.data = numpy.fromfile(f, numpy.float32)\n#                print   ""world""\n                if reshape_to_matrix:\n                    self.data = self.data.reshape( (self.n_samples, -1) )\n\n            if(sys.byteorder==\'little\'):\n#                print   ""hello""\n                self.data.byteswap(True) # forces big-endian byte ordering\n\n            f.close()\n        except IOError as e:\n            logging.error(e)\n            raise Exception(e)\n\n        return None\n\n    def write_htk(self, filename):\n        \'\'\'\n        \'\'\'\n        try:\n\n            file = open(filename, \'wb\')\n\n            file.write(struct.pack(\'>I\', self.n_samples))\n            file.write(struct.pack(\'>I\', self.samp_period))\n            file.write(struct.pack(\'>H\', self.samp_size))\n            file.write(struct.pack(\'>H\', self.param_kind))\n\n            if(sys.byteorder==\'little\'):\n                self.data.byteswap(True) # force big-endian byte ordering\n\n            self.data.tofile(file)\n\n        except IOError as e:\n            raise Exception(e)\n\n        return None\n\n    def print_info(self):\n\n        print(""Samples number: "", self.n_samples)\n        print(""Sample period: [100ns]"", self.samp_period)\n        print(""Bytes/sample:"", self.samp_size)\n        print(""ParamKind - datatype: "", self.htk_datatype())\n        print(""ParamKind - options: _E(%i), _D(%i), A(%i)"", self.htk_datatype_has_option(self._E), self.htk_datatype_has_option(self._D), self.htk_datatype_has_option(self._A))\n        print(""Features matrix shape"", self.data.shape)\n        print(""Features"", self.data)\n\n        return None\n\n    def get_data_size(self):\n        return self.data.size*self.data.itemsize\n\ndef test_HTK_Parm_IO():\n\n    #filename_src = ""../data/GE001_1.feat""\n    filename_src = ""../data/tr1.mfc""\n    filename_dst = ""../data/tr1_dst.mfc""\n\n    htk = HTK_Parm_IO()\n\n    try:\n        print(\'SOURCE FILE : \')\n        htk.read_htk(filename_src)\n        htk.print_info()\n        #print ""t"", htk.dupa, sys.byteorder\n\n        htk.writeHTK(filename_dst)\n\n        print(\'TARGET FILE : \')\n        htk2 = HTK_Parm_IO()\n        htk2.read_htk(filename_dst)\n        htk2.print_info()\n\n    except Exception as e:\n        print(e)\n\n    return None\n\n\nif __name__ == ""__main__"":\n    test_HTK_Parm_IO()\n'"
misc/scripts/alignment/state_align/htkmfc.py,0,"b'# Copyright (c) 2007 Carnegie Mellon University\n#\n# You may copy and modify this freely under the same terms as\n# Sphinx-III\n\n""""""Read and write HTK feature files.\n\nThis module reads and writes the acoustic feature files used by HTK\n""""""\n\n__author__ = ""David Huggins-Daines <dhuggins@cs.cmu.edu>""\n__version__ = ""$Revision $""\n\nfrom struct import unpack, pack\nimport numpy\n\nLPC = 1\nLPCREFC = 2\nLPCEPSTRA = 3\nLPCDELCEP = 4\nIREFC = 5\nMFCC = 6\nFBANK = 7\nMELSPEC = 8\nUSER = 9\nDISCRETE = 10\nPLP = 11\n\n_E = 0o000100 # has energy\n_N = 0o000200 # absolute energy supressed\n_D = 0o000400 # has delta coefficients\n_A = 0o001000 # has acceleration (delta-delta) coefficients\n_C = 0o002000 # is compressed\n_Z = 0o004000 # has zero mean static coefficients\n_K = 0o010000 # has CRC checksum\n_O = 0o020000 # has 0th cepstral coefficient\n_V = 0o040000 # has VQ data\n_T = 0o100000 # has third differential coefficients\n\ndef open_htk_file(f, mode=None, veclen=13):\n    """"""Open an HTK format feature file for reading or writing.\n    The mode parameter is \'rb\' (reading) or \'wb\' (writing).""""""\n    if mode is None:\n        if hasattr(f, \'mode\'):\n            mode = f.mode\n        else:\n            mode = \'rb\'\n    if mode in (\'r\', \'rb\'):\n        return HTKFeat_read(f) # veclen is ignored since it\'s in the file\n    elif mode in (\'w\', \'wb\'):\n        return HTKFeat_write(f, veclen)\n    else:\n        raise Exception(""mode must be \'r\', \'rb\', \'w\', or \'wb\'"")\n\nclass HTKFeat_read(object):\n    ""Read HTK format feature files""\n    def __init__(self, filename=None):\n        self.swap = (unpack(\'=i\', pack(\'>i\', 42))[0] != 42)\n        if (filename != None):\n            self.open_file(filename)\n\n    def __iter__(self):\n        self.fh.seek(12,0)\n        return self\n\n    def open_file(self, filename):\n        self.filename = filename\n        self.fh = open(filename, ""rb"")\n        self.readheader()\n\n    def readheader(self):\n        self.fh.seek(0,0)\n        spam = self.fh.read(12)\n        self.nSamples, self.sampPeriod, self.sampSize, self.parmKind = \\\n                       unpack("">IIHH"", spam)\n        # Get coefficients for compressed data\n        if self.parmKind & _C:\n            self.dtype = \'h\'\n            self.veclen = self.sampSize / 2\n            if self.parmKind & 0x3f == IREFC:\n                self.A = 32767\n                self.B = 0\n            else:\n                self.A = numpy.fromfile(self.fh, \'f\', self.veclen)\n                self.B = numpy.fromfile(self.fh, \'f\', self.veclen)\n                if self.swap:\n                    self.A = self.A.byteswap()\n                    self.B = self.B.byteswap()\n        else:\n            self.dtype = \'f\'\n            self.veclen = self.sampSize / 4\n        self.hdrlen = self.fh.tell()\n        self.veclen = int(self.veclen)\n\n    def seek(self, idx):\n        self.fh.seek(self.hdrlen + idx * self.sampSize, 0)\n\n    def __next__(self):\n        vec = numpy.fromfile(self.fh, self.dtype, self.veclen)\n        if len(vec) == 0:\n            raise StopIteration\n        if self.swap:\n            vec = vec.byteswap()\n        # Uncompress data to floats if required\n        if self.parmKind & _C:\n            vec = (vec.astype(\'f\') + self.B) / self.A\n        return vec\n\n    def readvec(self):\n        return next(self)\n\n    def getall(self, filename):\n        self.open_file(filename)\n        self.readheader()\n\n#        print   self.nSamples, self.veclen\n\n#        print   self.parmKind, self.sampPeriod\n\n        self.seek(0)\n        data = numpy.fromfile(self.fh, self.dtype)\n#        print   len(data), data.shape\n#        if self.parmKind & _K: # Remove and ignore checksum\n#            data = data[:-1]\n#        print   data.shape\n        data = data.reshape((-1, self.veclen))\n#        data = tmp_data.reshape((len(tmp_data)/self.veclen, self.veclen))\n        if self.swap:\n            data = data.byteswap()\n        # Uncompress data to floats if required\n        if self.parmKind & _C:\n            data = (data.astype(\'f\') + self.B) / self.A\n        return data, self.nSamples\n\nclass HTKFeat_write(object):\n    ""Write Sphinx-II format feature files""\n    def __init__(self, filename=None,\n                 veclen=13, sampPeriod=100000,\n                 paramKind = (MFCC | _O)):\n        self.veclen = veclen\n        self.sampPeriod = sampPeriod\n        self.sampSize = veclen * 4\n        self.paramKind = paramKind\n        self.dtype = \'f\'\n        self.filesize = 0\n        self.swap = (unpack(\'=i\', pack(\'>i\', 42))[0] != 42)\n        if (filename != None):\n            self.open_file(filename)\n\n    def __del__(self):\n        self.close()\n\n    def open_file(self, filename):\n        self.filename = filename\n        self.fh = open(filename, ""wb"")\n        self.writeheader()\n\n    def close(self):\n        self.writeheader()\n\n    def writeheader(self):\n        self.fh.seek(0,0)\n        self.fh.write(pack("">IIHH"", self.filesize,\n                           self.sampPeriod,\n                           self.sampSize,\n                           self.paramKind))\n\n    def writevec(self, vec):\n        if len(vec) != self.veclen:\n            raise Exception(""Vector length must be %d"" % self.veclen)\n        if self.swap:\n            numpy.array(vec, self.dtype).byteswap().tofile(self.fh)\n        else:\n            numpy.array(vec, self.dtype).tofile(self.fh)\n        self.filesize = self.filesize + self.veclen\n\n    def writeall(self, arr, filename):\n        self.open_file(filename)\n        for row in arr:\n            self.writevec(row)\n\n        self.close()\n'"
misc/scripts/alignment/state_align/mean_variance_norm.py,0,"b""\nfrom htk_io import HTK_Parm_IO\nfrom htkmfc import HTKFeat_read,HTKFeat_write\nimport  logging\nimport  numpy\n\nclass   MeanVarianceNorm():\n    '''\n    plan: 1: support normal MVN and denormalisation for both input and output\n          2: support stream-based operation: for example, some streams can use min-max, other streams use MVN, may need one more class\n    '''\n    def __init__(self, feature_dimension):\n\n        self.mean_vector = None\n        self.std_vector  = None\n        self.feature_dimension = feature_dimension\n\n    def feature_normalisation(self, in_file_list, out_file_list):\n        logger = logging.getLogger('feature_normalisation')\n\n#        self.feature_dimension = feature_dimension\n        try:\n            assert len(in_file_list) == len(out_file_list)\n        except  AssertionError:\n            logger.critical('The input and output file numbers are not the same! %d vs %d' %(len(in_file_list), len(out_file_list)))\n            raise\n\n        if self.mean_vector == None:\n            self.mean_vector = self.compute_mean(in_file_list, 0, self.feature_dimension)\n        if self.std_vector  == None:\n            self.std_vector = self.compute_std(in_file_list, self.mean_vector, 0, self.feature_dimension)\n\n        io_funcs = HTKFeat_read()\n        file_number = len(in_file_list)\n        for i in range(file_number):\n            features, current_frame_number = io_funcs.getall(in_file_list[i])\n#            print   current_frame_number\n#            features = io_funcs.data\n#            current_frame_number = io_funcs.n_samples\n\n            mean_matrix = numpy.tile(self.mean_vector, (current_frame_number, 1))\n            std_matrix = numpy.tile(self.std_vector, (current_frame_number, 1))\n\n            norm_features = (features - mean_matrix) / std_matrix\n\n            htk_writer  = HTKFeat_write(veclen=io_funcs.veclen, sampPeriod=io_funcs.sampPeriod, paramKind=9)\n            htk_writer.writeall(norm_features, out_file_list[i])\n\n#            htk_writter = HTK_Parm_IO(n_samples=io_funcs.n_samples, samp_period=io_funcs.samp_period, samp_size=io_funcs.samp_size, param_kind=io_funcs.param_kind, data=norm_features)\n#            htk_writter.write_htk(out_file_list[i])\n\n        return  self.mean_vector, self.std_vector\n\n    def feature_denormalisation(self, in_file_list, out_file_list, mean_vector, std_vector):\n        io_funcs = BinaryIOCollection()\n        file_number = len(in_file_list)\n        try:\n            assert len(in_file_list) == len(out_file_list)\n        except  AssertionError:\n            logger.critical('The input and output file numbers are not the same! %d vs %d' %(len(in_file_list), len(out_file_list)))\n            raise\n\n        try:\n            assert  mean_vector.size == self.feature_dimension and std_vector.size == self.feature_dimension\n        except AssertionError:\n            logger.critical('the dimensionalities of the mean and standard derivation vectors are not the same as the dimensionality of the feature')\n            raise\n\n        for i in range(file_number):\n            features, current_frame_number = io_funcs.load_binary_file_frame(in_file_list[i], self.feature_dimension)\n\n            mean_matrix = numpy.tile(mean_vector, (current_frame_number, 1))\n            std_matrix = numpy.tile(std_vector, (current_frame_number, 1))\n\n            norm_features = features * std_matrix + mean_matrix\n\n            io_funcs.array_to_binary_file(norm_features, out_file_list[i])\n\n    def compute_mean(self, file_list, start_index, end_index):\n\n        logger = logging.getLogger('feature_normalisation')\n\n        local_feature_dimension = end_index - start_index\n\n        mean_vector = numpy.zeros((1, local_feature_dimension))\n        all_frame_number = 0\n\n        io_funcs = HTKFeat_read()\n        for file_name in file_list:\n            features, current_frame_number = io_funcs.getall(file_name)\n#            io_funcs = HTK_Parm_IO()\n#            io_funcs.read_htk(file_name)\n#            features = io_funcs.data\n#            current_frame_number = io_funcs.n_samples\n\n            mean_vector += numpy.reshape(numpy.sum(features[:, start_index:end_index], axis=0), (1, local_feature_dimension))\n            all_frame_number += current_frame_number\n\n        mean_vector /= float(all_frame_number)\n\n        # setting the print options in this way seems to break subsequent printing of numpy float32 types\n        # no idea what is going on - removed until this can be solved\n        # po=numpy.get_printoptions()\n        # numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)\n        logger.info('computed mean vector of length %d :' % mean_vector.shape[1] )\n        logger.info(' mean: %s' % mean_vector)\n        # restore the print options\n        # numpy.set_printoptions(po)\n\n        self.mean_vector = mean_vector\n\n        return  mean_vector\n\n    def compute_std(self, file_list, mean_vector, start_index, end_index):\n\n        logger = logging.getLogger('feature_normalisation')\n\n        local_feature_dimension = end_index - start_index\n\n        std_vector = numpy.zeros((1, self.feature_dimension))\n        all_frame_number = 0\n\n        io_funcs = HTKFeat_read()\n        for file_name in file_list:\n            features, current_frame_number = io_funcs.getall(file_name)\n\n            mean_matrix = numpy.tile(mean_vector, (current_frame_number, 1))\n\n            std_vector += numpy.reshape(numpy.sum((features[:, start_index:end_index] - mean_matrix) ** 2, axis=0), (1, local_feature_dimension))\n            all_frame_number += current_frame_number\n\n        std_vector /= float(all_frame_number)\n\n        std_vector = std_vector ** 0.5\n\n        # setting the print options in this way seems to break subsequent printing of numpy float32 types\n        # no idea what is going on - removed until this can be solved\n        # po=numpy.get_printoptions()\n        # numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)\n        logger.info('computed  std vector of length %d' % std_vector.shape[1] )\n        logger.info('  std: %s' % std_vector)\n        # restore the print options\n        # numpy.set_printoptions(po)\n\n        self.std_vector = std_vector\n\n        return  std_vector\n"""
misc/scripts/frontend/utils/genScmFile.py,0,"b'import os,sys,glob\nimport collections\n\ndef readtext(fname):\n    f = open(fname, \'r\')\n    data = f.read()\n    data = data.strip(\' \\n\')\n    f.close()\n    return data\n\ndef create_dictionary_from_txt_dir(txt_dir):\n    utt_text = {}\n    textfiles = glob.glob(txt_dir + \'/*.txt\')\n\n    num_of_files = len(textfiles)\n\n    for i in range(num_of_files):\n        textfile = textfiles[i]\n        junk,filename = os.path.split(textfile)\n        filename = filename.split(\'.\')[0]\n\n        text = readtext(textfile)\n        utt_text[filename] = text\n\n    return utt_text\n\ndef create_dictionary_from_txt_file(txt_file):\n    utt_text = {}\n    in_f = open(txt_file, \'r\')\n    for newline in in_f.readlines():\n        newline = newline.strip()\n        newline = newline.replace(\'(\', \'\')\n        newline = newline.replace(\')\', \'\')\n\n        text_parts = newline.split()\n        filename = text_parts[0]\n\n        text = \' \'.join(text_parts[1:])\n        text = text[1:-1] ## remove begining and end double quotes\n\n        utt_text[filename] = text\n\n    return utt_text\n\nif __name__ == ""__main__"":\n\n    if len(sys.argv)!=5:\n        print(\'Usage: python genScmFile.py <in_txt_dir/in_txt_file> <out_utt_dir> <out_scm_file> <out_file_id_list>\')\n        sys.exit(1)\n\n    out_utt_dir  = sys.argv[2]\n    out_scm_file = sys.argv[3]\n    out_id_file  = sys.argv[4]\n\n    if not os.path.exists(out_utt_dir):\n        os.makedirs(out_utt_dir)\n\n    if os.path.isdir(sys.argv[1]):\n        print(""creating a scheme file from text directory"")\n        in_txt_dir = sys.argv[1]\n        utt_text   = create_dictionary_from_txt_dir(in_txt_dir)\n\n    elif os.path.isfile(sys.argv[1]):\n        print(""creating a scheme file from text file"")\n        in_txt_file = sys.argv[1]\n        utt_text    = create_dictionary_from_txt_file(in_txt_file)\n\n    sorted_utt_text = collections.OrderedDict(sorted(utt_text.items()))\n\n    out_f1 = open(out_scm_file, \'w\')\n    out_f2 = open(out_id_file, \'w\')\n\n    ### if you want to use a particular voice\n    #out_f1.write(""(voice_cstr_edi_fls_multisyn)\\n"")\n\n    for utt_name, sentence in sorted_utt_text.items():\n        out_file_name = os.path.join(out_utt_dir, utt_name+\'.utt\')\n        sentence = sentence.replace(\'""\', \'\\\\""\')\n        out_f1.write(""(utt.save (utt.synth (Utterance Text \\""""+sentence+""\\"" )) \\""""+out_file_name+""\\"")\\n"")\n        out_f2.write(utt_name+""\\n"")\n\n    out_f1.close()\n    out_f2.close()\n'"
misc/scripts/frontend/utils/normalize_lab_for_merlin.py,0,"b'import sys,os\nimport numpy as np\n\ndef divide_into_states(st_dur, fn_dur, num_states):\n    state_dur = np.zeros((2, num_states), np.int64)\n\n    state_dur[0][0] = st_dur\n    state_dur[1][num_states-1] = fn_dur\n\n    num_of_frames  = (fn_dur-st_dur)/50000\n    nof_each_state = num_of_frames/num_states\n\n    #if nof_each_state<1:\n    #    print \'warning: some states are with zero duration\'\n\n    for k in range(num_states-1):\n        state_dur[1][k]   = state_dur[0][k]+(nof_each_state*50000)\n        state_dur[0][k+1] = state_dur[1][k]\n\n    return state_dur\n\ndef normalize_dur(dur):\n    rem_t = dur%50000\n\n    if rem_t<=25000:\n        dur = dur - rem_t\n    else:\n        dur = dur + (50000-rem_t)\n\n    return dur\n\ndef normalize_label_files(in_lab_file, out_lab_file, label_style, write_time_stamps):\n    out_f = open(out_lab_file,\'w\')\n\n    in_f = open(in_lab_file,\'r\')\n    data = in_f.readlines()\n    in_f.close()\n\n    ph_arr=[]\n    for i in data:\n        fstr = i.strip().split()\n        ftag = fstr[2]\n        ph = ftag[ftag.index(\'-\')+1:ftag.index(\'+\')]\n        if(ph==\'pau\'):\n            continue;\n        ph_arr.append(ph)\n    count=0;prev_ph=\'\'\n    merged_data = [[],[],[]]\n    for i in data:\n        fstr = i.strip().split()\n        start_time = fstr[0]\n        end_time   = fstr[1]\n        ftag = fstr[2]\n        mid_indx = ftag.index(\':\')\n        p1 = ftag[0:mid_indx]\n        p2 = ftag[mid_indx:]\n        ph = ftag[ftag.index(\'-\')+1:ftag.index(\'+\')]\n        #print ph\n        if(ph!=\'pau\'):\n            count=count+1\n        if(prev_ph==\'pau\' and ph==\'pau\'):\n            continue;\n        if(count<=2 and \'pau\' in p1) or (count>len(ph_arr)-2 and \'pau\' in p1):\n            p1 = p1.replace(\'pau\',\'sil\')\n            ftag = p1+p2\n        if(count>=1 and count<len(ph_arr)):\n            if \'-sil+\' in ftag:\n                ftag = ftag.replace(\'-sil+\',\'-pau+\')\n        merged_data[0].append(start_time)\n        merged_data[1].append(end_time)\n        merged_data[2].append(ftag)\n        prev_ph=ph\n\n    num_states = 5\n    tot_num_ph = len(merged_data[0])\n    for j in range(tot_num_ph):\n        if j<tot_num_ph-1:\n            ph_end = normalize_dur(int(merged_data[0][j+1]))\n            merged_data[0][j+1] = str(ph_end)\n            merged_data[1][j]   = merged_data[0][j+1]\n        else:\n            end_time = normalize_dur(int(end_time))\n            merged_data[1][j]=str(end_time)\n\n        if (int(merged_data[1][j])-int(merged_data[0][j]))==0:\n            print(\'Error: zero duration for this phone\')\n            raise\n\n        if label_style == ""phone_align"":\n            if write_time_stamps:\n                out_f.write(merged_data[0][j]+\' \'+merged_data[1][j]+\' \'+merged_data[2][j]+\'\\n\')\n            else:\n                out_f.write(merged_data[2][j]+\'\\n\')\n        elif label_style == ""state_align"":\n            if write_time_stamps:\n                for k in range(num_states):\n                    state_dur = divide_into_states(int(merged_data[0][j]), int(merged_data[1][j]), num_states)\n                    out_f.write(str(state_dur[0][k])+\' \'+str(state_dur[1][k])+\' \'+merged_data[2][j]+\'[\'+str(k+2)+\']\\n\')\n            else:\n                out_f.write(merged_data[2][j]+\'\\n\')\n\n    out_f.close()\n\nif __name__ == ""__main__"":\n\n    if len(sys.argv)<5:\n        print(\'Usage: python normalize_lab_for_merlin.py <input_lab_dir> <output_lab_dir> <label_style> <file_id_list_scp> <optional: write_time_stamps (1/0)>\\n\')\n        sys.exit(0)\n\n    in_lab_dir   = sys.argv[1]\n    out_lab_dir  = sys.argv[2]\n    label_style  = sys.argv[3]\n    file_id_list = sys.argv[4]\n\n    write_time_stamps = True\n    if len(sys.argv)==6:\n        if int(sys.argv[5])==0:\n            write_time_stamps = False\n\n    if label_style!=""phone_align"" and label_style!=""state_align"":\n        print(""These labels %s are not supported as of now...please use state_align or phone_align!!"" % (label_style))\n        sys.exit(0)\n\n    if not os.path.exists(out_lab_dir):\n        os.makedirs(out_lab_dir)\n\n    in_f = open(file_id_list,\'r\')\n\n    for i in in_f.readlines():\n        filename = i.strip()+\'.lab\'\n        print(filename)\n        in_lab_file  = os.path.join(in_lab_dir, filename)\n        out_lab_file = os.path.join(out_lab_dir, filename)\n        normalize_label_files(in_lab_file, out_lab_file, label_style, write_time_stamps)\n        #break;\n\n    in_f.close()\n'"
misc/scripts/frontend/utils/prepare_txt_done_data_file.py,0,"b'import os\nimport sys\nimport numpy as np\n\nif __name__ == ""__main__"":\n\n    if len(sys.argv)!=3:\n        print(\'Usage: python src/prepare_txt_done_data_file.py <txt_dir> <txt.done.data>\\n\')\n        sys.exit(0)\n\n    txt_dir  = sys.argv[1]\n    out_file = sys.argv[2]\n\n    out_f = open(out_file,\'w\')\n\n    for txtfile in os.listdir(txt_dir):\n        if txtfile is not None:\n            file_id = os.path.basename(txtfile).split(""."")[0]\n            txtfile = os.path.join(txt_dir, txtfile)\n            with open(txtfile, \'r\') as myfile:\n                data = myfile.read().replace(\'\\n\', \'\')\n            data = data.replace(\'""\', \'\\\\""\')\n            out_f.write(""( ""+file_id+"" \\"" ""+data+"" \\"")\\n"")\n\n    out_f.close()\n'"
misc/scripts/vocoder/magphase/extract_features_for_merlin.py,0,"b'#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n""""""\n@author: Felipe Espic\n\nDESCRIPTION:\nThis script extracts low-dimensional acoustic features from a batch of wav files intended for using with the Merlin toolkit.\nIt runs the extraction in parallel mode, using all the cores available in the system.\n\nThe acoustic features extracted and used by Merlin are:\n- \'<file>.mag\'  : Mel-scaled Log-Mag (dim=nbins_mel,   usually 60).\n- \'<file>.real\' : Mel-scaled real    (dim=nbins_phase, usually 45).\n- \'<file>.imag\' : Mel-scaled imag    (dim=nbins_phase, usually 45).\n- \'<file>.lf0\'  : Log-F0 (dim=1).\n\nAlso, this script extracts the additional files:\n- \'<file>.est\'  : File generated by REAPER containing epoch locations and voi/unvoi decisions (remove them if wanted).\n- \'<file>.shift\': File that contains the shifts (hop-sizes) for each extracted frame (variable frame rate).\n                  It is used to modify the label files in Merlin. Se .... for more information.\n\nINSTRUCTIONS:\nThis demo should work out of the box. Just run it by typing: python <script name>\nIf wanted, you can modify the input options (directories, input files, etc.) See the main function below for details.\n""""""\n\nimport sys, os\n\nif len(sys.argv)!=5:\n    print(""Usage: "")\n    print(""python extract_features_for_merlin.py <path_to_merlin_dir> <path_to_wav_dir> <path_to_feat_dir> <sampling rate>"")\n    sys.exit(1)\n\n# top merlin directory\nmerlin_dir = sys.argv[1]\n\n# input audio directory\nwav_dir = sys.argv[2]\n\n# Output features directory\nout_dir = sys.argv[3]\n\n# Expected sample rate\nfs_expected = int(sys.argv[4])\n\n# Magphase directory\nmagphase = os.path.join(merlin_dir, \'tools\', \'magphase\', \'src\')\nsys.path.append(os.path.realpath(magphase))\nimport libutils as lu\nimport magphase as mp\n\n\ndef feat_extraction(wav_file, out_feats_dir):\n\n    # Parsing path:\n    file_name_token = os.path.basename(os.path.splitext(wav_file)[0])\n\n    # Display:\n    print(""\\nAnalysing file: "" + file_name_token + \'.wav\' + \'................................\')\n\n    mp.analysis_for_acoustic_modelling(wav_file, out_feats_dir)\n    return\n\n\ndef get_wav_filelist(wav_dir):\n    wav_files = []\n    for file in os.listdir(wav_dir):\n        whole_filepath = os.path.join(wav_dir, file)\n        if os.path.isfile(whole_filepath) and str(whole_filepath).endswith("".wav""):\n            wav_files.append(whole_filepath)\n        elif os.path.isdir(whole_filepath):\n            wav_files += get_wav_filelist(whole_filepath)\n\n    wav_files.sort()\n\n    return wav_files\n\n# FILES SETUP:========================================================================\nlu.mkdir(out_dir)\nl_wavfiles = get_wav_filelist(wav_dir)\n\n# MULTIPROCESSING EXTRACTION:==========================================================\nlu.run_multithreaded(feat_extraction, l_wavfiles, out_dir)\n\n# For debugging (don\'t delete):\n#for wavfile in l_wavfiles:\n#    feat_extraction(wavfile, out_dir)\n\n\nprint(\'Done!\')\n        \n'"
misc/scripts/vocoder/straight/extract_features_for_merlin.py,0,"b'import os\nimport sys\nimport shutil\nimport glob\nimport time\nimport multiprocessing as mp\n\nif len(sys.argv)!=5:\n    print(""Usage: "")\n    print(""python extract_features_for_merlin.py <path_to_merlin_dir> <path_to_wav_dir> <path_to_feat_dir> <sampling rate>"")\n    sys.exit(1)\n\n# top merlin directory\nmerlin_dir = sys.argv[1]\n\n# input audio directory\nwav_dir = sys.argv[2]\n\n# Output features directory\nout_dir = sys.argv[3]\n\n# initializations\nfs = int(sys.argv[4])\n\n# tools directory\nstraight = os.path.join(merlin_dir, ""tools/bin/straight"")\nsptk     = os.path.join(merlin_dir, ""tools/bin/SPTK-3.9"")\n\nraw_dir = os.path.join(out_dir, \'raw\' )\nsp_dir  = os.path.join(out_dir, \'sp\' )\nmgc_dir = os.path.join(out_dir, \'mgc\')\nbap_dir = os.path.join(out_dir, \'bap\')\nap_dir  = os.path.join(out_dir, \'ap\')\nf0_dir  = os.path.join(out_dir, \'f0\' )\nlf0_dir = os.path.join(out_dir, \'lf0\')\n\nif not os.path.exists(out_dir):\n    os.mkdir(out_dir)\n\nif not os.path.exists(raw_dir):\n    os.mkdir(raw_dir)\n\nif not os.path.exists(sp_dir):\n    os.mkdir(sp_dir)\n\nif not os.path.exists(mgc_dir):\n    os.mkdir(mgc_dir)\n\nif not os.path.exists(bap_dir):\n    os.mkdir(bap_dir)\n\nif not os.path.exists(ap_dir):\n    os.mkdir(ap_dir)\n\nif not os.path.exists(f0_dir):\n    os.mkdir(f0_dir)\n\nif not os.path.exists(lf0_dir):\n    os.mkdir(lf0_dir)\n\nif fs == 16000:\n    nFFT = 1024\n    alpha = 0.58\n\nelif fs == 48000:\n    nFFT = 4096\n    alpha = 0.77\n\nelse:\n    print(""As of now, we don\'t support %d Hz sampling rate."" %(fs))\n    print(""Please consider either downsampling to 16000 Hz or upsampling to 48000 Hz"")\n    sys.exit(1)\n\nmcsize = 59\norder = 24\nnFFTHalf = 1 + nFFT / 2\nfshift = 5\n\ndef get_wav_filelist(wav_dir):\n    wav_files = []\n    for file in os.listdir(wav_dir):\n        whole_filepath = os.path.join(wav_dir,file)\n        if os.path.isfile(whole_filepath) and str(whole_filepath).endswith("".wav""):\n            wav_files.append(whole_filepath)\n        elif os.path.isdir(whole_filepath):\n            wav_files += get_wav_filelist(whole_filepath)\n\n    wav_files.sort()\n\n    return wav_files\n\ndef process(filename):\n    \'\'\'\n    The function decomposes a wav file into F0, mel-cepstral coefficients, and band aperiodicity\n    :param filename: path to wav file\n    :return: .lf0, .mgc and .bap files\n    \'\'\'\n    file_id = os.path.basename(filename).split(""."")[0]\n    print(file_id)\n\n    sox_wav_2_raw_cmd = \'sox %s -b 16 -c 1 -r %s -t raw %s\' % (filename,\\\n                                                               fs,\\\n                                                               os.path.join(raw_dir, file_id + \'.raw\'))\n    os.system(sox_wav_2_raw_cmd)\n\n    ### STRAIGHT ANALYSIS -- extract vocoder parameters ###\n    ### extract f0, sp, ap ###\n\n    straight_f0_analysis_cmd = ""%s -nmsg -maxf0 400 -uf0 400 -minf0 50 -lf0 50 -f0shift %s -f %s -raw %s %s"" % (os.path.join(straight, \'tempo\'), \\\n                                                                                                        fshift, fs, \\\n                                                                                                        os.path.join(raw_dir, file_id + \'.raw\'), \\\n                                                                                                        os.path.join(f0_dir, file_id + \'.f0\'))\n    os.system(straight_f0_analysis_cmd)\n\n    straight_ap_analysis_cmd = ""%s -nmsg -f %s -fftl %s -apord %s -shift %s -f0shift %s -float -f0file %s -raw %s %s"" % (os.path.join(straight, \'straight_bndap\'),\\\n                                                                                                                          fs, nFFT, nFFTHalf, fshift, fshift,\\\n                                                                                                                          os.path.join(f0_dir, file_id + \'.f0\'), \\\n                                                                                                                          os.path.join(raw_dir, file_id + \'.raw\'), \\\n                                                                                                                          os.path.join(ap_dir, file_id + \'.ap\'))\n    os.system(straight_ap_analysis_cmd)\n\n    straight_sp_analysis_cmd = ""%s -nmsg -f %s -fftl %s -apord %s -shift %s -f0shift %s -order %s -f0file %s -pow -float -raw %s %s"" % (os.path.join(straight, \'straight_mcep\'),\\\n                                                                  fs, nFFT, nFFTHalf, fshift, fshift, mcsize, \\\n    os.path.join(f0_dir,file_id + \'.f0\'), \\\n    os.path.join(raw_dir,file_id + \'.raw\'), \\\n    os.path.join(sp_dir,file_id + \'.sp\'))\n\n    os.system(straight_sp_analysis_cmd)\n\n    ### convert f0 to lf0 ###\n\n    sptk_x2x_af_cmd = ""%s +af %s | %s > %s "" % (os.path.join(sptk, \'x2x\'), \\\n                                                os.path.join(f0_dir, file_id + \'.f0\'), \\\n                                                os.path.join(sptk, \'sopr\') + \' -magic 0.0 -LN -MAGIC -1.0E+10\', \\\n                                                os.path.join(lf0_dir, file_id + \'.lf0\'))\n    os.system(sptk_x2x_af_cmd)\n\n    ### convert sp to mgc ###\n    sptk_mcep = ""%s -a %s -m %s -l %s -e 1.0E-8 -j 0 -f 0.0 -q 3 %s > %s"" % (os.path.join(sptk, \'mcep\'),\\\n                                       alpha, mcsize, nFFT,\\\n                                                                             os.path.join(sp_dir, file_id+\'.sp\'),\\\n                                                                             os.path.join(mgc_dir, file_id+\'.mgc\'))\n    os.system(sptk_mcep)\n\n    ### convert ap to bap ###\n    sptk_mcep = ""%s -a %s -m %s -l %s -e 1.0E-8 -j 0 -f 0.0 -q 1 %s > %s"" % (os.path.join(sptk, \'mcep\'),\\\n                                       alpha, order, nFFT,\\\n                                                                             os.path.join(ap_dir, file_id+\'.ap\'),\\\n                                                                             os.path.join(bap_dir, file_id+\'.bap\'))\n\n    os.system(sptk_mcep)\n\nprint(""--- Feature extraction started ---"")\nstart_time = time.time()\n\n# get wav files list\nwav_files = get_wav_filelist(wav_dir)\n\n# do multi-processing\npool = mp.Pool(mp.cpu_count())\npool.map(process, wav_files)\n\n# clean temporal files\nshutil.rmtree(raw_dir, ignore_errors=True)\nshutil.rmtree(sp_dir, ignore_errors=True)\nshutil.rmtree(f0_dir, ignore_errors=True)\nshutil.rmtree(ap_dir, ignore_errors=True)\n\nprint(""You should have your features ready in: ""+out_dir)    \n\n(m, s) = divmod(int(time.time() - start_time), 60)\nprint((""--- Feature extraction completion time: %d min. %d sec ---"" % (m, s)))\n\n'"
misc/scripts/vocoder/world/extract_features_for_merlin.py,0,"b'import os\nimport sys\nimport shutil\nimport glob\nimport time\nimport multiprocessing as mp\nimport numpy as np\n\nif len(sys.argv)!=5:\n    print(""Usage: "")\n    print(""python extract_features_for_merlin.py <path_to_merlin_dir> <path_to_wav_dir> <path_to_feat_dir> <sampling rate>"")\n    sys.exit(1)\n\n# top merlin directory\nmerlin_dir = sys.argv[1]\n\n# input audio directory\nwav_dir = sys.argv[2]\n\n# Output features directory\nout_dir = sys.argv[3]\n\n# initializations\nfs = int(sys.argv[4])\n\n# tools directory\nworld  = os.path.join(merlin_dir, ""tools/bin/WORLD"")\nsptk   = os.path.join(merlin_dir, ""tools/bin/SPTK-3.9"")\nreaper = os.path.join(merlin_dir, ""tools/bin/REAPER"")\n\nsp_dir  = os.path.join(out_dir, \'sp\' )\nmgc_dir = os.path.join(out_dir, \'mgc\')\nap_dir  = os.path.join(out_dir, \'ap\' )\nbap_dir = os.path.join(out_dir, \'bap\')\nf0_dir  = os.path.join(out_dir, \'f0\' )\nlf0_dir = os.path.join(out_dir, \'lf0\')\n\nif not os.path.exists(out_dir):\n    os.mkdir(out_dir)\n\nif not os.path.exists(sp_dir):\n    os.mkdir(sp_dir)\n\nif not os.path.exists(mgc_dir):\n    os.mkdir(mgc_dir)\n\nif not os.path.exists(bap_dir):\n    os.mkdir(bap_dir)\n\nif not os.path.exists(f0_dir):\n    os.mkdir(f0_dir)\n\nif not os.path.exists(lf0_dir):\n    os.mkdir(lf0_dir)\n\nif fs == 16000:\n    nFFTHalf = 1024\n    alpha = 0.58\n\nelif fs == 22050:\n    nFFTHalf = 1024\n    alpha = 0.65\n\nelif fs == 44100:\n    nFFTHalf = 2048\n    alpha = 0.76\n\nelif fs == 48000:\n    nFFTHalf = 2048\n    alpha = 0.77\n\nelse:\n    print(""As of now, we don\'t support %d Hz sampling rate."" %(fs))\n    print(""Please consider either downsampling to 16000 Hz or upsampling to 48000 Hz"")\n    sys.exit(1)\n\n#bap order depends on sampling rate.\nmcsize=59\nb_use_reaper=True # If True: Reaper is used for f0 extraction. If False: The vocoder is used for f0 extraction.\n\ndef get_wav_filelist(wav_dir):\n    wav_files = []\n    for file in os.listdir(wav_dir):\n        whole_filepath = os.path.join(wav_dir,file)\n        if os.path.isfile(whole_filepath) and str(whole_filepath).endswith("".wav""):\n            wav_files.append(whole_filepath)\n        elif os.path.isdir(whole_filepath):\n            wav_files += get_wav_filelist(whole_filepath)\n\n    wav_files.sort()\n\n    return wav_files\n\n\ndef read_binfile(filename, dim=60, dtype=np.float64):\n    \'\'\'\n    Reads binary file into numpy array.\n    \'\'\'\n    fid = open(filename, \'rb\')\n    v_data = np.fromfile(fid, dtype=dtype)\n    fid.close()\n    if np.mod(v_data.size, dim) != 0:\n        raise ValueError(\'Dimension provided not compatible with file size.\')\n    m_data = v_data.reshape((-1, dim)).astype(\'float64\') # This is to keep compatibility with numpy default dtype.\n    m_data = np.squeeze(m_data)\n    return  m_data\n\ndef write_binfile(m_data, filename, dtype=np.float64):\n    \'\'\'\n    Writes numpy array into binary file.\n    \'\'\'\n    m_data = np.array(m_data, dtype)\n    fid = open(filename, \'wb\')\n    m_data.tofile(fid)\n    fid.close()\n    return\n\ndef read_reaper_f0_file(est_file, skiprows=7):\n    \'\'\'\n    Reads f0 track into numpy array from EST file generated by REAPER.\n    \'\'\'\n    v_f0 = np.loadtxt(est_file, skiprows=skiprows, usecols=[2])\n    v_f0[v_f0<0] = 0\n    return v_f0\n\ndef reaper_f0_extract(in_wavfile, f0_file_ref, f0_file_out, frame_shift_ms=5.0):\n    \'\'\'\n    Extracts f0 track using REAPER.\n    To keep consistency with the vocoder, it also fixes for the difference in number\n    of frames between the REAPER f0 track and the acoustic parameters extracted by the vocoder.\n    f0_file_ref: f0 extracted by the vocoder. It is used as a reference to fix the number of frames, as explained.\n    \'\'\'\n\n    # Run REAPER:\n    print(""Running REAPER f0 extraction..."")\n    cmd = ""%s -a -s -x 400 -m 50 -u %1.4f -i %s -f %s"" % (os.path.join(reaper, \'reaper\'), frame_shift_ms / 1000.0, in_wavfile, f0_file_out + ""_reaper"")\n    os.system(cmd)\n\n    # Protection - number of frames:\n    v_f0_ref = read_binfile(f0_file_ref, dim=1)\n    v_f0     = read_reaper_f0_file(f0_file_out + ""_reaper"")\n    frm_diff = v_f0.size - v_f0_ref.size\n    if frm_diff<0:\n        v_f0 = np.r_[ v_f0, np.zeros(-frm_diff) + v_f0[-1]]\n    if frm_diff>0:\n        v_f0 = v_f0[:-frm_diff]\n\n    # Save f0 file:\n    write_binfile(v_f0, f0_file_out)\n    return\n\n\ndef process(filename):\n    \'\'\'\n    The function decomposes a wav file into F0, mel-cepstral coefficients, and aperiodicity\n    :param filename: path to wav file\n    :return: .lf0, .mgc and .bap files\n    \'\'\'\n\n    file_id = os.path.basename(filename).split(""."")[0]\n    print(\'\\n\' + file_id)\n\n    ### WORLD ANALYSIS -- extract vocoder parameters ###\n    ### extract sp, ap ###\n    f0_file = os.path.join(f0_dir, file_id + \'.f0\')\n    f0_world_file = f0_file\n    if b_use_reaper:\n        f0_world_file = f0_file + ""_world""\n\n    world_analysis_cmd = ""%s %s %s %s %s"" % (os.path.join(world, \'analysis\'), \\\n                                             filename,\n                                             f0_world_file, \\\n                                             os.path.join(sp_dir, file_id + \'.sp\'), \\\n                                             os.path.join(bap_dir, file_id + \'.bapd\'))\n    os.system(world_analysis_cmd)\n\n    ### Extract f0 using reaper ###\n    if b_use_reaper:\n        reaper_f0_extract(filename, f0_world_file, f0_file)\n\n    ### convert f0 to lf0 ###\n    sptk_x2x_da_cmd = ""%s +da %s > %s"" % (os.path.join(sptk, \'x2x\'), f0_file, \\\n                                          os.path.join(f0_dir, file_id + \'.f0a\'))\n    os.system(sptk_x2x_da_cmd)\n\n    sptk_x2x_af_cmd = ""%s +af %s | %s > %s "" % (os.path.join(sptk, \'x2x\'), \\\n                                                os.path.join(f0_dir, file_id + \'.f0a\'), \\\n                                                os.path.join(sptk, \'sopr\') + \' -magic 0.0 -LN -MAGIC -1.0E+10\', \\\n                                                os.path.join(lf0_dir, file_id + \'.lf0\'))\n    os.system(sptk_x2x_af_cmd)\n\n    ### convert sp to mgc ###\n    sptk_x2x_df_cmd1 = ""%s +df %s | %s | %s >%s"" % (os.path.join(sptk, \'x2x\'), \\\n                                                    os.path.join(sp_dir, file_id + \'.sp\'), \\\n                                                    os.path.join(sptk, \'sopr\') + \' -R -m 32768.0\', \\\n                                                    os.path.join(sptk, \'mcep\') + \' -a \' + str(alpha) + \' -m \' + str(\n                                                        mcsize) + \' -l \' + str(\n                                                        nFFTHalf) + \' -e 1.0E-8 -j 0 -f 0.0 -q 3 \', \\\n                                                    os.path.join(mgc_dir, file_id + \'.mgc\'))\n    os.system(sptk_x2x_df_cmd1)\n\n    ### convert bapd to bap ###\n    sptk_x2x_df_cmd2 = ""%s +df %s > %s "" % (os.path.join(sptk, ""x2x""), \\\n                                            os.path.join(bap_dir, file_id + "".bapd""), \\\n                                            os.path.join(bap_dir, file_id + \'.bap\'))\n    os.system(sptk_x2x_df_cmd2)\n\nprint(""--- Feature extraction started ---"")\nstart_time = time.time()\n\n# get wav files list\nwav_files = get_wav_filelist(wav_dir)\n\n# do multi-processing\npool = mp.Pool(mp.cpu_count())\npool.map(process, wav_files)\n\n# DEBUG:\n#for nxf in xrange(len(wav_files)):\n#    process(wav_files[nxf])\n\n# clean temporal files\nshutil.rmtree(sp_dir, ignore_errors=True)\nshutil.rmtree(f0_dir, ignore_errors=True)\n\n\nfor zippath in glob.iglob(os.path.join(bap_dir, \'*.bapd\')):\n    os.remove(zippath)\n\nprint(""You should have your features ready in: ""+out_dir)    \n\n(m, s) = divmod(int(time.time() - start_time), 60)\nprint((""--- Feature extraction completion time: %d min. %d sec ---"" % (m, s)))\n\n'"
misc/scripts/vocoder/world/synthesis.py,0,"b'import os\nimport sys\n\nif len(sys.argv)!=6:\n    print(""Usage: "")\n    print(""python synthesis.py <path_to_merlin_dir> <path_to_feat_dir> <path_to_out_wav_dir> <sampling rate> <file_id_list>"")\n    sys.exit(1)\n\n# top merlin directory\nmerlin_dir = sys.argv[1]\n\n# input feat directory\nfeat_dir = sys.argv[2]\n\n# Output audio directory\nout_dir = sys.argv[3]\n\n# initializations\nfs = int(sys.argv[4])\n\n# file ID list\nfile_id_scp = sys.argv[5]\n\n# feat directories\nmgc_dir = os.path.join(feat_dir, \'mgc\')\nbap_dir = os.path.join(feat_dir, \'bap\')\nlf0_dir = os.path.join(feat_dir, \'lf0\')\n\nif not os.path.exists(mgc_dir):\n    mgc_dir = feat_dir\n\nif not os.path.exists(bap_dir):\n    bap_dir = feat_dir\n\nif not os.path.exists(lf0_dir):\n    lf0_dir = feat_dir\n\n# tools directory\nworld = os.path.join(merlin_dir, ""tools/bin/WORLD"")\nsptk  = os.path.join(merlin_dir, ""tools/bin/SPTK-3.9"")\n\nif not os.path.exists(out_dir):\n    os.mkdir(out_dir)\n\nif fs == 16000:\n    nFFTHalf = 1024\n    alpha = 0.58\n\nelif fs == 22050:\n    nFFTHalf = 1024\n    alpha = 0.65\n\nelif fs == 44100:\n    nFFTHalf = 2048\n    alpha = 0.76\n\nelif fs == 48000:\n    nFFTHalf = 2048\n    alpha = 0.77\n\nelse:\n    print(""As of now, we don\'t support %d Hz sampling rate."" %(fs))\n    print(""Please consider either downsampling to 16000 Hz or upsampling to 48000 Hz"")\n    sys.exit(1)\n\nmcsize=59\n\n# set to True if synthesizing generated files\npost_filtering = False\n\n# this coefficient depends on voice\npf_coef = 1.07 \n\nf = open(file_id_scp)\nfile_id_list = [newline.strip() for newline in f.readlines()]\nf.close()\n\nfor file_id in file_id_list:\n    ### WORLD Re-synthesis -- reconstruction of parameters ###\n    print(file_id)\n\n    ### convert lf0 to f0 ###\n    sopr_cmd = ""%s -magic -1.0E+10 -EXP -MAGIC 0.0 %s | %s +fa > %s"" %  (os.path.join(sptk, \'sopr\'), \\\n                                                                             os.path.join(lf0_dir, file_id+"".lf0""), \\\n                                                                             os.path.join(sptk, ""x2x""), \\\n                                                                             os.path.join(out_dir, file_id+"".f0a""))\n    os.system(sopr_cmd)\n\n    x2x_cmd1 = ""%s +ad %s > %s"" % (os.path.join(sptk, ""x2x""), \\\n                                      os.path.join(out_dir, file_id+"".f0a""), \\\n                                      os.path.join(out_dir, file_id+"".f0""))\n    os.system(x2x_cmd1)\n\n    mgc_file = os.path.join(mgc_dir, file_id+"".mgc"")\n    if post_filtering:\n        ### post-filtering mgc ###\n        mgc_file = os.path.join(out_dir, file_id+"".mgc_p"")\n        mcpf_cmd = ""%s -m %d -b %f %s > %s"" % (os.path.join(sptk, ""mcpf""), \n                                                mcsize, pf_coef, \\\n                                                os.path.join(mgc_dir, file_id+"".mgc""), \\\n                                                os.path.join(out_dir, file_id+"".mgc_p""))\n        os.system(mcpf_cmd)\n\n\n    ### convert mgc to sp ###\n    mgc2sp_cmd = ""%s -a %f -g 0 -m %d -l %d -o 2 %s | %s -d 32768.0 -P | %s +fd > %s"" % (os.path.join(sptk, ""mgc2sp""), \n                                                                                            alpha, mcsize, nFFTHalf, \\\n                                                                                            mgc_file, \\\n                                                                                            os.path.join(sptk, ""sopr""), \\\n                                                                                            os.path.join(sptk, ""x2x""), \\\n                                                                                            os.path.join(out_dir, file_id+"".sp""))\n    os.system(mgc2sp_cmd)\n\n    ### convert bapd to bap ###\n    x2x_cmd2 = ""%s +fd %s > %s"" % (os.path.join(sptk, ""x2x""), \\\n                                     os.path.join(bap_dir, file_id+"".bap""), \\\n                                     os.path.join(out_dir, file_id+"".bapd""))\n    os.system(x2x_cmd2)\n\n    # Final synthesis using WORLD\n    synth_cmd = ""%s %d %d %s %s %s %s"" % (os.path.join(world, ""synth""), \\\n                                            nFFTHalf, fs, \\\n                                            os.path.join(out_dir, file_id+"".f0""), \\\n                                            os.path.join(out_dir, file_id+"".sp""), \\\n                                            os.path.join(out_dir, file_id+"".bapd""), \\\n                                            os.path.join(out_dir, file_id+"".wav""))\n    os.system(synth_cmd)\n\n'"
src/work_in_progress/srikanth/bottleneck_scripts/merge_data.py,0,"b'\nimport numpy, os\nimport matplotlib.pyplot as plt\n\ndef load_binary_file(file_name, dimension):\n    fid_lab = open(file_name, \'rb\')\n    features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n    fid_lab.close()\n    frame_number = features.size / dimension\n    features = features[:(dimension * (features.size / dimension))]\n    features = features.reshape((-1, dimension))\n\n    return  features, frame_number\n\ndef read_file_list(dir_name):\n\n    file_paths = []\n    filenames = []\n    for root, directories, files in os.walk(dir_name):\n        for filename in files:\n            filepath = os.path.join(root, filename)\n            file_paths.append(filepath)\n            filenames.append(filename)\n\n    return  file_paths, filenames\n\n\ndef generate_context_feature(in_data_dir1, in_data_dir2, out_data_dir, dimension1, dimension2):\n\n    if not os.path.exists(out_data_dir):\n        os.makedirs(out_data_dir)\n\n    file_paths, filenames = read_file_list(in_data_dir1)\n\n    context_features = numpy\n\n    i = 0\n    for file_path, filename in zip(file_paths, filenames):\n        features1, frame_number1 = load_binary_file(file_path, dimension1)\n        features2, frame_number2 = load_binary_file(os.path.join(in_data_dir2, filename), dimension2)\n        if frame_number1 != frame_number2:\n            print(dimension2)\n            print(filename)\n            print(""%s %d != %d"" %(filename, frame_number1, frame_number2))\n            print(features1.shape, features2.shape)\n            os.exit(1)\n\n        context_features = numpy.zeros((frame_number1, dimension1+dimension2))\n\n        context_features[0:frame_number1, 0:dimension1] = features1\n        context_features[0:frame_number2, dimension1:dimension1+dimension2] = features2\n\n        print(filename, features1.shape, features2.shape, context_features.shape)\n\n        context_filename = out_data_dir + \'/\' + filename\n\n        context_features = numpy.asarray(context_features, \'float32\')\n        fid = open(context_filename, \'wb\')\n        context_features.tofile(fid)\n        fid.close()\n\n\nif __name__ == \'__main__\':\n    in_dir1 = \'/afs/inf.ed.ac.uk/group/cstr/projects/phd/s1432486/work/Merlin/test_version/dnn_tts/experiments/acoustic_model/data/nn_no_silence_lab_norm_490\'\n    in_dir2 = \'/afs/inf.ed.ac.uk/group/cstr/projects/phd/s1432486/work/Merlin/test_version/dnn_tts/experiments/acoustic_model/gen/DNN_TANH_TANH_TANH_TANH_LINEAR__mgc_lf0_vuv_bap_1_200_490_259_4_512_512_hidden_stacked/\'\n\n    dimension1 = 490\n    dimension2 = 32*21 # 128 * 1\n\n    out_dir = \'/afs/inf.ed.ac.uk/group/cstr/projects/phd/s1432486/work/Merlin/test_version/dnn_tts/experiments/acoustic_model/data/nn_no_silence_lab_norm_1162\'\n\n    generate_context_feature(in_dir1, in_dir2, out_dir, dimension1, dimension2)\n'"
src/work_in_progress/srikanth/bottleneck_scripts/prepare_context_feature.py,0,"b""#!/usr/bin/env python\n\nimport numpy, os\nimport matplotlib.pyplot as plt\n\ndef load_binary_file(file_name, dimension):\n    fid_lab = open(file_name, 'rb')\n    features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n    fid_lab.close()\n    frame_number = features.size / dimension\n    features = features[:(dimension * (features.size / dimension))]\n    features = features.reshape((-1, dimension))\n\n    return  features, frame_number\n\ndef read_file_list(dir_name):\n\n    #file_paths = []\n    #filenames = []\n    #for root, directories, files in os.walk(dir_name):\n    #    for filename in files:\n    #        filepath = os.path.join(root, filename)\n    #        file_paths.append(filepath)\n    #        filenames.append(filename)\n\n    #filenames=filter(os.path.isfile, os.listdir(dir_name))\n    filenames=[ f for f in os.listdir(dir_name) if os.path.isfile(dir_name+'/'+f) ]\n    #for f in os.listdir(dir_name):\n    #    if os.path.isfile(dir_name+'/'+f):\n    #        print dir_name+'/'+f+' is a file'\n    #    else:\n    #        print dir_name+'/'+f+' is not a file'\n    #print filenames\n    #file_paths=[ dir_name+'/'+f for f in os.listdir(dir_name) if os.path.isfile(f) ]\n    file_paths=[ dir_name+'/'+f for f in filenames ]\n\n    return  file_paths, filenames\n\n\ndef generate_context_feature(in_data_dir, out_data_dir, context_width, dimension):\n\n    if not os.path.exists(out_data_dir):\n        os.makedirs(out_data_dir)\n\n    #print in_data_dir\n    file_paths, filenames = read_file_list(in_data_dir)\n\n    window_size = context_width * 2 + 1\n\n    file_index = 0\n    i = 0\n    for file_path, filename in zip(file_paths, filenames):\n        features, frame_number = load_binary_file(file_path, dimension)\n\n        print(file_index, features.shape, filename)\n        expand_features = numpy.zeros((frame_number+window_size-1, dimension))\n        for ci in range(context_width):\n            expand_features[ci, :] = features[0, :]\n            expand_features[frame_number+context_width+ci, :] = features[frame_number-1, :]\n\n        expand_features[context_width:frame_number+context_width, :] = features\n\n        context_features = numpy.zeros((frame_number, dimension*window_size))\n\n        for wi in range(window_size):\n            context_features[0:frame_number, wi*dimension:(wi+1)*dimension] = expand_features[wi:frame_number+wi, :]\n\n        context_filename = out_data_dir + '/' + os.path.splitext(filename)[0] + '.lab'\n\n        context_features = numpy.asarray(context_features, 'float32')\n        fid = open(context_filename, 'wb')\n        context_features.tofile(fid)\n        fid.close()\n\n        file_index = file_index + 1\n\nif __name__ == '__main__':\n    in_dir = '/afs/inf.ed.ac.uk/group/cstr/projects/phd/s1432486/work/Merlin/test_version/dnn_tts/experiments/acoustic_model/gen/DNN_TANH_TANH_TANH_TANH_LINEAR__mgc_lf0_vuv_bap_1_200_490_259_4_512_512_hidden/'\n\n    dimension = 32 # 128\n    context_width = 10 # 0\n    out_dir = '/afs/inf.ed.ac.uk/group/cstr/projects/phd/s1432486/work/Merlin/test_version/dnn_tts/experiments/acoustic_model/gen/DNN_TANH_TANH_TANH_TANH_LINEAR__mgc_lf0_vuv_bap_1_200_490_259_4_512_512_hidden_stacked/'\n\n    generate_context_feature(in_dir, out_dir, context_width, dimension)\n"""
