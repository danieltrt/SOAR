file_path,api_count,code
Stereo_Online_Adaptation.py,25,"b'import tensorflow as tf\nimport numpy as np\nimport argparse\nimport Nets\nimport os\nimport sys\nimport time\nimport cv2\nimport json\nimport datetime\nimport shutil\nfrom matplotlib import pyplot as plt\nfrom Data_utils import data_reader,weights_utils,preprocessing\nfrom Losses import loss_factory\nfrom Sampler import sampler_factory\n\n\n#static params\nMAX_DISP=256\nPIXEL_TH = 3\n\ndef scale_tensor(tensor,scale):\n\treturn preprocessing.rescale_image(tensor,[tf.shape(tensor)[1]//scale,tf.shape(tensor)[2]//scale])\n\ndef softmax(x):\n\t""""""Compute softmax values for each sets of scores in x.""""""\n\treturn np.exp(x) / np.sum(np.exp(x), axis=0)\n\n\ndef main(args):\n\t#load json file config\n\twith open(args.blockConfig) as json_data:\n\t\ttrain_config = json.load(json_data)\n\t\n\t#read input data\n\twith tf.variable_scope(\'input_reader\'):\n\t\tdata_set = data_reader.dataset(\n\t\t\targs.list,\n\t\t\tbatch_size = 1,\n\t\t\tcrop_shape=args.imageShape,\n\t\t\tnum_epochs=1,\n\t\t\taugment=False,\n\t\t\tis_training=False,\n\t\t\tshuffle=False\n\t\t)\n\t\tleft_img_batch, right_img_batch, gt_image_batch = data_set.get_batch()\n\t\tinputs={\n\t\t\t\'left\':left_img_batch,\n\t\t\t\'right\':right_img_batch,\n\t\t\t\'target\':gt_image_batch\n\t\t}\n\n\t#build inference network\n\twith tf.variable_scope(\'model\'):\n\t\tnet_args = {}\n\t\tnet_args[\'left_img\'] = left_img_batch\n\t\tnet_args[\'right_img\'] = right_img_batch\n\t\tnet_args[\'split_layers\'] = [None]\n\t\tnet_args[\'sequence\'] = True\n\t\tnet_args[\'train_portion\'] = \'BEGIN\'\n\t\tnet_args[\'bulkhead\'] = True if args.mode==\'MAD\' else False\n\t\tstereo_net = Nets.get_stereo_net(args.modelName, net_args)\n\t\tprint(\'Stereo Prediction Model:\\n\', stereo_net)\n\t\tpredictions = stereo_net.get_disparities()\n\t\tfull_res_disp = predictions[-1]\n\t\n\t#build real full resolution loss\n\twith tf.variable_scope(\'full_res_loss\'):\n\t\t# reconstruction loss between warped right image and original left image\n\t\tfull_reconstruction_loss =  loss_factory.get_reprojection_loss(\'mean_SSIM_l1\',reduced=True)(predictions,inputs)\n\n\n\t#build validation ops\n\twith tf.variable_scope(\'validation_error\'):\n\t\t# compute error against gt\n\t\tabs_err = tf.abs(full_res_disp - gt_image_batch)\n\t\tvalid_map = tf.where(tf.equal(gt_image_batch, 0), tf.zeros_like(gt_image_batch, dtype=tf.float32), tf.ones_like(gt_image_batch, dtype=tf.float32))\n\t\tfiltered_error = abs_err * valid_map\n\n\t\tabs_err = tf.reduce_sum(filtered_error) / tf.reduce_sum(valid_map)\n\t\tbad_pixel_abs = tf.where(tf.greater(filtered_error, PIXEL_TH), tf.ones_like(filtered_error, dtype=tf.float32), tf.zeros_like(filtered_error, dtype=tf.float32))\n\t\tbad_pixel_perc = tf.reduce_sum(bad_pixel_abs) / tf.reduce_sum(valid_map)\n\t\n\t#build train ops\n\tdisparity_trainer = tf.train.MomentumOptimizer(args.lr,0.9)\n\ttrain_ops = []\n\tif args.mode == \'MAD\':\n\t\t#build train ops for separate portion of the network\n\t\tpredictions = predictions[:-1] #remove full res disp\n\t\t\n\t\tinputs_modules = {\n\t\t\t\'left\':scale_tensor(left_img_batch,args.reprojectionScale),\n\t\t\t\'right\':scale_tensor(right_img_batch,args.reprojectionScale),\n\t\t\t\'target\':scale_tensor(gt_image_batch,args.reprojectionScale)/args.reprojectionScale\n\t\t}\n\t\t\n\t\tassert(len(predictions)==len(train_config))\n\t\tfor counter,p in enumerate(predictions):\n\t\t\tprint(\'Build train ops for disparity {}\'.format(counter))\n\t\t\t\t\t\n\t\t\t#rescale predictions to proper resolution\n\t\t\tmultiplier = tf.cast(tf.shape(left_img_batch)[1]//tf.shape(p)[1],tf.float32)\n\t\t\tp = preprocessing.resize_to_prediction(p,inputs_modules[\'left\'])*multiplier\n\n\t\t\t#compute reprojection error\n\t\t\twith tf.variable_scope(\'reprojection_\'+str(counter)):\n\t\t\t\treconstruction_loss = loss_factory.get_reprojection_loss(\'mean_SSIM_l1\',reduced=True)([p],inputs_modules)\n\n\t\t\t#build train op\n\t\t\tlayer_to_train = train_config[counter]\n\t\t\tprint(\'Going to train on {}\'.format(layer_to_train))\n\t\t\tvar_accumulator=[]\n\t\t\tfor name in layer_to_train:\n\t\t\t\tvar_accumulator+=stereo_net.get_variables(name)\n\t\t\tprint(\'Number of variable to train: {}\'.format(len(var_accumulator)))\n\t\t\t\t\n\t\t\t#add new training op\n\t\t\ttrain_ops.append(disparity_trainer.minimize(reconstruction_loss,var_list=var_accumulator))\n\n\t\t\tprint(\'Done\')\n\t\t\tprint(\'=\'*50)\n\t\t\n\t\t#create Sampler to fetch portions to train\n\t\tsampler = sampler_factory.get_sampler(args.sampleMode,args.numBlocks,args.fixedID)\n\t\t\n\telif args.mode==\'FULL\':\n\t\t#build single train op for the full network\n\t\ttrain_ops.append(disparity_trainer.minimize(full_reconstruction_loss))\n\t\n\n\tif args.summary:\n\t\t#add summaries\n\t\ttf.summary.scalar(\'EPE\',abs_err)\n\t\ttf.summary.scalar(\'bad3\',bad_pixel_perc)\n\t\ttf.summary.image(\'full_res_disp\',preprocessing.colorize_img(full_res_disp,cmap=\'jet\'),max_outputs=1)\n\t\ttf.summary.image(\'gt_disp\',preprocessing.colorize_img(gt_image_batch,cmap=\'jet\'),max_outputs=1)\n\n\t\t#create summary logger\n\t\tsummary_op = tf.summary.merge_all()\n\t\tlogger = tf.summary.FileWriter(args.output)\n\n\n\t#start session\n\tgpu_options = tf.GPUOptions(allow_growth=True)\n\twith tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n\t\t#init stuff\n\t\tsess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\n\n\t\t#restore disparity inference weights\n\t\tvar_to_restore = weights_utils.get_var_to_restore_list(args.weights, [])\n\t\tassert(len(var_to_restore)>0)\n\t\trestorer = tf.train.Saver(var_list=var_to_restore)\n\t\trestorer.restore(sess,args.weights)\n\t\tprint(\'Disparity Net Restored?: {}, number of restored variables: {}\'.format(True,len(var_to_restore)))\n\n\t\tnum_actions=len(train_ops)\n\t\tif args.mode==\'FULL\':\n\t\t\tselected_train_ops = train_ops\n\t\telse:\n\t\t\tselected_train_ops = [tf.no_op()]\n\n\t\tepe_accumulator = []\n\t\tbad3_accumulator = []\n\t\ttime_accumulator = []\n\t\texec_time = 0\n\t\tfetch_counter=[0]*num_actions\n\t\tsample_distribution=np.zeros(shape=[num_actions])\n\t\ttemp_score = np.zeros(shape=[num_actions])\n\t\tloss_t_2 = 0\n\t\tloss_t_1 = 0\n\t\texpected_loss = 0\n\t\tlast_trained_blocks = []\n\t\treset_counter=0\n\t\tstep=0\n\t\tmax_steps=data_set.get_max_steps()\n\t\ttry:\t\n\t\t\tstart_time = time.time()\n\t\t\twhile True:\n\n\t\t\t\t#fetch new network portion to train\n\t\t\t\tif step%args.sampleFrequency==0 and args.mode==\'MAD\':\n\t\t\t\t\t#Sample \n\t\t\t\t\tdistribution = softmax(sample_distribution)\n\t\t\t\t\tblocks_to_train = sampler.sample(distribution)\n\t\t\t\t\tselected_train_ops = [train_ops[i] for i in blocks_to_train]\n\n\t\t\t\t\t#accumulate sampling statistics\n\t\t\t\t\tfor l in blocks_to_train:\n\t\t\t\t\t\tfetch_counter[l]+=1\n\n\t\t\t\t#build list of tensorflow operations that needs to be executed\n\n\t\t\t\t#errors and full resolution loss\n\t\t\t\ttf_fetches = [abs_err,bad_pixel_perc,full_reconstruction_loss]\n\n\t\t\t\tif args.summary and step%100==0:\n\t\t\t\t\t#summaries\n\t\t\t\t\ttf_fetches = tf_fetches + [summary_op]\n\n\t\t\t\t#update ops\n\t\t\t\ttf_fetches = tf_fetches+selected_train_ops\n\n\t\t\t\tif args.logDispStep!=-1 and step%args.logDispStep==0:\n\t\t\t\t\t#prediction for serialization to disk\n\t\t\t\t\ttf_fetches=tf_fetches + [full_res_disp]\n\n\t\t\t\t#run network\n\t\t\t\tfetches = sess.run(tf_fetches)\n\t\t\t\tnew_loss = fetches[2]\n\n\t\t\t\tif args.mode == \'MAD\':\n\t\t\t\t\t#update sampling probabilities\n\t\t\t\t\tif step==0:\n\t\t\t\t\t\tloss_t_2 = new_loss\n\t\t\t\t\t\tloss_t_1 = new_loss\n\t\t\t\t\texpected_loss = 2*loss_t_1-loss_t_2\t\n\t\t\t\t\tgain_loss=expected_loss-new_loss\n\t\t\t\t\tsample_distribution = 0.99*sample_distribution\n\t\t\t\t\tfor i in last_trained_blocks:\n\t\t\t\t\t\tsample_distribution[i] += 0.01*gain_loss\n\n\t\t\t\t\tlast_trained_blocks=blocks_to_train\n\t\t\t\t\tloss_t_2 = loss_t_1\n\t\t\t\t\tloss_t_1 = new_loss\n\t\t\t\t\t\n\t\t\t\t#accumulate performance metrics\n\t\t\t\tepe_accumulator.append(fetches[0])\n\t\t\t\tbad3_accumulator.append(fetches[1])\n\n\t\t\t\tif step%100==0:\n\t\t\t\t\t#log on terminal\n\t\t\t\t\tfbTime = (time.time()-start_time)\n\t\t\t\t\texec_time += fbTime\n\t\t\t\t\tfbTime = fbTime/100\n\t\t\t\t\tif args.summary:\n\t\t\t\t\t\tlogger.add_summary(fetches[3],global_step=step)\n\t\t\t\t\tmissing_time=(max_steps-step)*fbTime\n\t\t\t\t\tprint(\'Step:{:4d}\\tbad3:{:.2f}\\tEPE:{:.2f}\\tSSIM:{:.2f}\\tf/b time:{:3f}\\tMissing time:{}\'.format(step,fetches[1], fetches[0],new_loss,fbTime,datetime.timedelta(seconds=missing_time)))\n\t\t\t\t\tstart_time = time.time()\n\t\t\t\t\n\t\t\t\t#reset network if necessary\n\t\t\t\tif new_loss>args.SSIMTh:\n\t\t\t\t\trestorer.restore(sess,args.weights)\n\t\t\t\t\treset_counter+=1\n\t\t\t\t\n\t\t\t\t#save disparity if requested\n\t\t\t\tif args.logDispStep!=-1 and step%args.logDispStep==0:\n\t\t\t\t\tdispy=fetches[-1]\n\t\t\t\t\tdispy_to_save = np.clip(dispy[0], 0, MAX_DISP)\n\t\t\t\t\tdispy_to_save = (dispy_to_save*256.0).astype(np.uint16)\n\t\t\t\t\tcv2.imwrite(os.path.join(args.output, \'disparities/disparity_{}.png\'.format(step)), dispy_to_save)\n\n\t\t\t\tstep+=1\n\n\t\texcept tf.errors.OutOfRangeError:\n\t\t\tpass\n\t\tfinally:\n\t\t\tepe_array = epe_accumulator\n\t\t\tbad3_array = bad3_accumulator\n\t\t\tepe_accumulator = np.sum(epe_accumulator)\n\t\t\tbad3_accumulator = np.sum(bad3_accumulator)\n\t\t\twith open(os.path.join(args.output, \'stats.csv\'), \'w+\') as f_out:\n\t\t\t\t# report series\n\t\t\t\tf_out.write(\'Metrics,cumulative,average\\n\')\n\t\t\t\tf_out.write(\'EPE,{},{}\\n\'.format(epe_accumulator,epe_accumulator/step))\n\t\t\t\tf_out.write(\'bad3,{},{}\\n\'.format(bad3_accumulator,bad3_accumulator/step))\n\t\t\t\tf_out.write(\'time,{},{}\\n\'.format(exec_time,exec_time/step))\n\t\t\t\tf_out.write(\'FPS,{}\\n\'.format(1/(exec_time/step)))\n\t\t\t\tf_out.write(\'#resets,{}\\n\'.format(reset_counter))\n\t\t\t\tf_out.write(\'Blocks\')\n\t\t\t\tfor n in range(len(predictions)):\n\t\t\t\t\tf_out.write(\',{}\'.format(n))\n\t\t\t\tf_out.write(\',final\\n\')\n\t\t\t\tf_out.write(\'fetch_counter\')\n\t\t\t\tfor c in fetch_counter:\n\t\t\t\t\tf_out.write(\',{}\'.format(c))\n\t\t\t\tf_out.write(\'\\n\')\n\t\t\t\tfor c in sample_distribution:\n\t\t\t\t\tf_out.write(\',{}\'.format(c))\n\t\t\t\tf_out.write(\'\\n\')\n\t\t\t\n\t\t\tstep_time = exec_time/step\n\t\t\ttime_array = [str(x*step_time) for x in range(len(epe_array))]\n\t\t\t\n\t\t\twith open(os.path.join(args.output,\'series.csv\'),\'w+\') as f_out:\n\t\t\t\tf_out.write(\'Iteration,Time,EPE,bad3\\n\')\n\t\t\t\tfor i,(t,e,b) in enumerate(zip(time_array,epe_array,bad3_array)):\n\t\t\t\t\tf_out.write(\'{},{},{},{}\\n\'.format(i,t,e,b))\n\t\t\t\n\t\t\tprint(\'Result saved in {}\'.format(args.output))\n\t\t\t\n\t\t\tprint(\'All Done, Bye Bye!\')\n\nif __name__==\'__main__\':\n\tparser=argparse.ArgumentParser(description=\'Script for online Adaptation of a Deep Stereo Network\')\n\tparser.add_argument(""-l"",""--list"", help=\'path to the list file with frames to be processed\', required=True)\n\tparser.add_argument(""-o"",""--output"", help=""path to the output folder where the results will be saved"", required=True)\n\tparser.add_argument(""--weights"",help=""path to the initial weights for the disparity estimation network"",required=True)\n\tparser.add_argument(""--modelName"", help=""name of the stereo model to be used"", default=""Dispnet"", choices=Nets.STEREO_FACTORY.keys())\n\tparser.add_argument(""--numBlocks"", help=""number of CNN portions to train at each iteration"",type=int,default=1)\n\tparser.add_argument(""--lr"", help=""value for learning rate"",default=0.0001, type=float)\n\tparser.add_argument(""--blockConfig"",help=""path to the block_config json file"",required=True)\n\tparser.add_argument(""--sampleMode"",help=""choose the sampling heuristic to use"",choices=sampler_factory.AVAILABLE_SAMPLER,default=\'SAMPLE\')\n\tparser.add_argument(""--fixedID"",help=""index of the portions of network to train, used only if sampleMode=FIXED"",type=int,nargs=\'+\',default=[0])\n\tparser.add_argument(""--reprojectionScale"",help=""compute all loss function at 1/reprojectionScale"",default=1,type=int)\n\tparser.add_argument(""--summary"",help=\'flag to enable tensorboard summaries\',action=\'store_true\')\n\tparser.add_argument(""--imageShape"", help=\'two int for the size of the crop extracted from each image [height,width]\', nargs=\'+\', type=int, default=[320,1216])\n\tparser.add_argument(""--SSIMTh"",help=""reset network to initial configuration if loss is above this value"",type=float,default=0.5)\n\tparser.add_argument(""--sampleFrequency"",help=""sample new network portions to train every K frame"",type=int,default=1)\n\tparser.add_argument(""--mode"",help=""online adaptation mode: NONE - perform only inference, FULL - full online backprop, MAD - backprop only on portions of the network"", choices=[\'NONE\',\'FULL\',\'MAD\'], default=\'MAD\')\n\tparser.add_argument(""--logDispStep"", help=""save disparity every K step, -1 to disable"", default=-1, type=int)\n\targs=parser.parse_args()\n\n\tif not os.path.exists(args.output):\n\t\tos.makedirs(args.output)\n\tif args.logDispStep!=-1 and not os.path.exists(os.path.join(args.output, \'disparities\')):\n\t\tos.makedirs(os.path.join(args.output, \'disparities\'))\n\tshutil.copy(args.blockConfig,os.path.join(args.output,\'config.json\'))\n\twith open(os.path.join(args.output, \'params.sh\'), \'w+\') as out:\n\t\tsys.argv[0] = os.path.join(os.getcwd(), sys.argv[0])\n\t\tout.write(\'#!/bin/bash\\n\')\n\t\tout.write(\'python3 \')\n\t\tout.write(\' \'.join(sys.argv))\n\t\tout.write(\'\\n\')\n\tmain(args)\n'"
Train.py,29,"b'import tensorflow as tf\nimport numpy as np\nimport argparse\nimport Nets\nimport os\nimport sys\nimport time\nimport cv2\nimport json\nimport datetime\nimport shutil\nfrom matplotlib import pyplot as plt\nfrom Data_utils import data_reader,weights_utils,preprocessing\nfrom Losses import loss_factory\nfrom Sampler import sampler_factory\n\n\n#static params\nPIXEL_TH = 3\nMAX_DISP = 192\n\ndef main(args):\n\t#read input data\n\twith tf.name_scope(\'input_reader\'):\n\t\twith tf.name_scope(\'training_set_reader\'):\n\t\t\tdata_set = data_reader.dataset(\n\t\t\t\targs.trainingSet,\n\t\t\t\tbatch_size = args.batchSize,\n\t\t\t\tcrop_shape=args.imageShape,\n\t\t\t\tnum_epochs=args.numEpochs,\n\t\t\t\taugment=args.augment,\n\t\t\t\tis_training=True,\n\t\t\t\tshuffle=True\n\t\t\t)\n\t\t\tleft_img_batch, right_img_batch, gt_image_batch = data_set.get_batch()\n\t\t\tinputs={\n\t\t\t\t\'left\':left_img_batch,\n\t\t\t\t\'right\':right_img_batch,\n\t\t\t\t\'target\':gt_image_batch\n\t\t\t}\n\t\tif args.validationSet is not None:\n\t\t\twith tf.name_scope(\'validation_set_reader\'):\n\t\t\t\tvalidation_set = data_reader.dataset(\n\t\t\t\t\targs.validationSet,\n\t\t\t\t\tbatch_size = args.batchSize,\n\t\t\t\t\taugment=False,\n\t\t\t\t\tis_training=False,\n\t\t\t\t\tshuffle=True\n\t\t\t\t)\n\t\t\t\tleft_val_batch, right_val_batch, gt_val_batch = validation_set.get_batch()\n\t\t\t\tprint(left_val_batch.shape,right_val_batch.shape)\n\n\t#build network\n\twith tf.variable_scope(\'model\') as scope:\n\t\tnet_args = {}\n\t\tnet_args[\'left_img\'] = left_img_batch\n\t\tnet_args[\'right_img\'] = right_img_batch\n\t\tnet_args[\'split_layers\'] = [None]\n\t\tnet_args[\'sequence\'] = True\n\t\tnet_args[\'train_portion\'] = \'BEGIN\'\n\t\tnet_args[\'bulkhead\'] = False\n\t\tstereo_net = Nets.get_stereo_net(args.modelName, net_args)\n\t\tprint(\'Stereo Prediction Model:\\n\', stereo_net)\n\t\tpredictions = stereo_net.get_disparities()\n\t\tfull_res_disp = predictions[-1]\n\n\t\tif args.validationSet is not None:\n\t\t\tscope.reuse_variables()\n\t\t\tnet_args[\'left_img\']=left_val_batch\n\t\t\tnet_args[\'right_img\']=right_val_batch\n\t\t\tval_stereo_net = Nets.get_stereo_net(args.modelName, net_args)\n\t\t\tval_prediction = val_stereo_net.get_disparities()[-1]\n\t\n\tif args.validationSet is not None:\n\t\t#build validation ops\n\t\twith tf.variable_scope(\'validation_error\'):\n\t\t\t# compute error against gt\n\t\t\tabs_err = tf.abs(val_prediction - gt_val_batch)\n\t\t\tvalid_map = tf.where(tf.equal(gt_val_batch, 0), tf.zeros_like(gt_val_batch, dtype=tf.float32), tf.ones_like(gt_val_batch, dtype=tf.float32))\n\t\t\tfiltered_error = abs_err * valid_map\n\n\t\t\tabs_err = tf.reduce_sum(filtered_error) / tf.reduce_sum(valid_map)\n\t\t\tbad_pixel_abs = tf.where(tf.greater(filtered_error, PIXEL_TH), tf.ones_like(filtered_error, dtype=tf.float32), tf.zeros_like(filtered_error, dtype=tf.float32))\n\t\t\tbad_pixel_perc = tf.reduce_sum(bad_pixel_abs) / tf.reduce_sum(valid_map)\n\n\t\t\ttf.summary.scalar(\'EPE\',abs_err)\n\t\t\ttf.summary.scalar(\'bad3\',bad_pixel_perc)\n\t\t\ttf.summary.image(\'val_prediction\',preprocessing.colorize_img(val_prediction,cmap=\'jet\'),max_outputs=1)\n\t\t\ttf.summary.image(\'val_gt\',preprocessing.colorize_img(gt_val_batch,cmap=\'jet\'),max_outputs=1)\n\t\n\twith tf.name_scope(\'training_error\'):\n\t\t#build train ops\n\t\tglobal_step = tf.Variable(0,trainable=False)\n\t\tlearning_rate = tf.train.exponential_decay(args.lr,global_step,args.decayStep, 0.5, staircase=True)\n\t\tdisparity_trainer = tf.train.AdamOptimizer(args.lr,0.9)\n\t\t\n\t\t#l1 regression loss for each scale mutiplied by the corresponding weight\n\t\tif args.lossWeights is not None and len(args.lossWeights)==len(predictions):\n\t\t\traise ValueError(\'Wrong number of loss weights provide, should provide {}\'.format(len(predictions)))\n\t\tfull_reconstruction_loss = loss_factory.get_supervised_loss(args.lossType,multiScale=True,logs=False,weights=args.lossWeights,max_disp=MAX_DISP)(predictions,inputs)\n\n\t\ttrain_op = disparity_trainer.minimize(full_reconstruction_loss,global_step=global_step)\n\n\t\t#add summaries\n\t\ttf.summary.image(\'full_res_disp\',preprocessing.colorize_img(full_res_disp,cmap=\'jet\'),max_outputs=1)\n\t\ttf.summary.image(\'gt_disp\',preprocessing.colorize_img(gt_image_batch,cmap=\'jet\'),max_outputs=1)\n\t\ttf.summary.scalar(\'full_reconstruction_loss\',full_reconstruction_loss)\n\n\t#create summary logger\n\tsummary_op = tf.summary.merge_all()\n\tlogger = tf.summary.FileWriter(args.output)\n\n\t#create saver\n\tmain_saver = tf.train.Saver(max_to_keep=2)\n\n\t#start session\n\tgpu_options = tf.GPUOptions(allow_growth=True)\n\tmax_steps = data_set.get_max_steps()\n\texec_time=0\n\twith tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n\t\t#init stuff\n\t\tsess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\n\n\t\t#restore disparity inference weights\n\t\trestored,step_eval = weights_utils.check_for_weights_or_restore_them(args.output,sess,initial_weights=args.weights)\n\t\tprint(\'Disparity Net Restored?: {} from step {}\'.format(restored,step_eval))\n\n\t\tsess.run(global_step.assign(step_eval))\n\t\ttry:\t\n\t\t\tstart_time = time.time()\n\t\t\twhile True:\n\t\t\t\ttf_fetches = [global_step,train_op,full_reconstruction_loss]\n\n\t\t\t\tif step_eval%100==0:\n\t\t\t\t\t#summaries\n\t\t\t\t\ttf_fetches = tf_fetches + [summary_op]\n\n\t\t\t\t#run network\n\t\t\t\trun_options = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n\t\t\t\tfetches = sess.run(tf_fetches,options=run_options)\n\t\t\t\t\n\t\t\t\tif step_eval%100==0:\n\t\t\t\t\t#log on terminal\n\t\t\t\t\tfbTime = (time.time()-start_time)\n\t\t\t\t\texec_time += fbTime\n\t\t\t\t\tfbTime = fbTime/100\n\t\t\t\t\tlogger.add_summary(fetches[-1],global_step=step_eval)\n\t\t\t\t\tmissing_time=(max_steps-step_eval)*fbTime\n\t\t\t\t\tprint(\'Step:{:4d}\\tLoss:{:.2f}\\tf/b time:{:3f}\\tMissing time:{}\'.format(step_eval,fetches[2],fbTime,datetime.timedelta(seconds=missing_time)))\n\t\t\t\t\tstart_time = time.time()\n\t\t\t\t\n\t\t\t\tif step_eval%10000==0:\n\t\t\t\t\tckpt = os.path.join(args.output,\'weights.ckpt\')\n\t\t\t\t\tmain_saver.save(sess,ckpt,global_step=step_eval)\n\n\t\t\t\tstep_eval = fetches[0]\n\t\texcept tf.errors.OutOfRangeError:\n\t\t\tpass\n\t\tfinally:\t\n\t\t\tprint(\'All Done, Bye Bye!\')\n\nif __name__==\'__main__\':\n\tparser=argparse.ArgumentParser(description=\'Script for training of a Deep Stereo Network\')\n\tparser.add_argument(""--trainingSet"", help=\'path to the list file with training set\', required=True)\n\tparser.add_argument(""--validationSet"", help=""path to the list file with the validation set"", default=None, type=str)\n\tparser.add_argument(""-o"",""--output"", help=""path to the output folder where the results will be saved"", required=True)\n\tparser.add_argument(""--weights"",help=""path to the initial weights for the disparity estimation network (OPTIONAL)"")\n\tparser.add_argument(""--modelName"", help=""name of the stereo model to be used"", default=""Dispnet"", choices=Nets.STEREO_FACTORY.keys())\n\tparser.add_argument(""--lr"", help=""initial value for learning rate"",default=0.0001, type=float)\n\tparser.add_argument(""--imageShape"", help=\'two int for the size of the crop extracted from each image [height,width]\', nargs=\'+\', type=int, default=[320,1216])\n\tparser.add_argument(""--batchSize"", help=\'batch size to use during training\',type=int,default=4)\n\tparser.add_argument(""--numEpochs"", help=\'number of training epochs\',type=int,default=50)\n\tparser.add_argument(""--augment"", help=""flag to enable data augmentation"", action=\'store_true\')\n\tparser.add_argument(""--lossWeights"", help=""weights for loss at different resolution from full to lower res"", nargs=\'+\', default=None, type=float)\n\tparser.add_argument(\'--lossType\', help=""Type of supervised loss to use"", choices=loss_factory.SUPERVISED_LOSS.keys(), default=""mean_l1"",type=str)\n\tparser.add_argument(""--decayStep"", help=""halve learning rate after this many steps"", type=int, default=500000)\n\targs=parser.parse_args()\n\n\tif not os.path.exists(args.output):\n\t\tos.makedirs(args.output)\n\n\twith open(os.path.join(args.output, \'params.sh\'), \'w+\') as out:\n\t\tsys.argv[0] = os.path.join(os.getcwd(), sys.argv[0])\n\t\tout.write(\'#!/bin/bash\\n\')\n\t\tout.write(\'python3 \')\n\t\tout.write(\' \'.join(sys.argv))\n\t\tout.write(\'\\n\')\n\tmain(args)\n'"
Data_utils/__init__.py,0,b''
Data_utils/data_reader.py,12,"b'import tensorflow as tf\nimport numpy as np\nimport cv2\nimport re\nimport os\nfrom collections import defaultdict\n\nfrom Data_utils import preprocessing\n\n\ndef readPFM(file):\n    """"""\n    Load a pfm file as a numpy array\n    Args:\n        file: path to the file to be loaded\n    Returns:\n        content of the file as a numpy array\n    """"""\n    file = open(file, \'rb\')\n\n    color = None\n    width = None\n    height = None\n    scale = None\n    endian = None\n\n    header = file.readline().rstrip()\n    if header == b\'PF\':\n        color = True\n    elif header == b\'Pf\':\n        color = False\n    else:\n        raise Exception(\'Not a PFM file.\')\n\n    dims = file.readline()\n    try:\n        width, height = list(map(int, dims.split()))\n    except:\n        raise Exception(\'Malformed PFM header.\')\n\n    scale = float(file.readline().rstrip())\n    if scale < 0:  # little-endian\n        endian = \'<\'\n        scale = -scale\n    else:\n        endian = \'>\'  # big-endian\n\n    data = np.fromfile(file, endian + \'f\')\n    shape = (height, width, 3) if color else (height, width, 1)\n\n    data = np.reshape(data, shape)\n    data = np.flipud(data)\n    return data, scale\n\ndef read_list_file(path_file):\n    """"""\n    Read dataset description file encoded as left;right;disp;conf\n    Args:\n        path_file: path to the file encoding the database\n    Returns:\n        [left,right,gt,conf] 4 list containing the images to be loaded\n    """"""\n    with open(path_file,\'r\') as f_in:\n        lines = f_in.readlines()\n    lines = [x for x in lines if not x.strip()[0] == \'#\']\n    left_file_list = []\n    right_file_list = []\n    gt_file_list = []\n    conf_file_list = []\n    for l in lines:\n        to_load = re.split(\',|;\',l.strip())\n        left_file_list.append(to_load[0])\n        right_file_list.append(to_load[1])\n        if len(to_load)>2:\n            gt_file_list.append(to_load[2])\n        if len(to_load)>3:\n            conf_file_list.append(to_load[3])\n    return left_file_list,right_file_list,gt_file_list,conf_file_list\n\ndef read_image_from_disc(image_path,shape=None,dtype=tf.uint8):\n    """"""\n    Create a queue to hoold the paths of files to be loaded, then create meta op to read and decode image\n    Args:\n        image_path: metaop with path of the image to be loaded\n        shape: optional shape for the image\n    Returns:\n        meta_op with image_data\n    """"""         \n    image_raw = tf.read_file(image_path)\n    if dtype==tf.uint8:\n        image = tf.image.decode_image(image_raw)\n    else:\n        image = tf.image.decode_png(image_raw,dtype=dtype)\n    if shape is None:\n        image.set_shape([None,None,3])\n    else:\n        image.set_shape(shape)\n    return tf.cast(image, dtype=tf.float32)\n\n\nclass dataset():\n    """"""\n    Class that reads a dataset for deep stereo\n    """"""\n    def __init__(\n        self,\n        path_file,\n        batch_size=4,\n        crop_shape=[320,1216],\n        num_epochs=None,\n        augment=False,\n        is_training=True,\n        shuffle=True):\n    \n        if not os.path.exists(path_file):\n            raise Exception(\'File not found during dataset construction\')\n    \n        self._path_file = path_file\n        self._batch_size=batch_size\n        self._crop_shape=crop_shape\n        self._num_epochs=num_epochs\n        self._augment=augment\n        self._shuffle=shuffle\n        self._is_training = is_training\n\n        self._build_input_pipeline()\n    \n    def _load_image(self, files):\n        left_file_name = files[0]\n        right_file_name = files[1]\n        gt_file_name = files[2]\n        left_image = read_image_from_disc(left_file_name)\n        right_image = read_image_from_disc(right_file_name)\n        if self._usePfm:\n            gt_image = tf.py_func(lambda x: readPFM(x)[0], [gt_file_name], tf.float32)\n            gt_image.set_shape([None,None,1])\n        else:\n            read_type = tf.uint16 if self._double_prec_gt else tf.uint8\n            gt_image = read_image_from_disc(gt_file_name,shape=[None,None,1], dtype=read_type)\n            gt_image = tf.cast(gt_image,tf.float32)\n            if self._double_prec_gt:\n                gt_image = gt_image/256.0\n        \n        #crop gt to fit with image (SGM add some paddings who know why...)\n        gt_image = gt_image[:,:tf.shape(left_image)[1],:]\n        \n        if self._is_training:\n            left_image,right_image,gt_image = preprocessing.random_crop(self._crop_shape, [left_image,right_image,gt_image])\n        else:\n            (left_image,right_image,gt_image) = [tf.image.resize_image_with_crop_or_pad(x,self._crop_shape[0],self._crop_shape[1]) for x in [left_image,right_image,gt_image]]\n        \n        if self._augment:\n            left_image,right_image=preprocessing.augment(left_image,right_image)\n        return [left_image,right_image,gt_image]\n    \n    def _build_input_pipeline(self):\n        left_files, right_files, gt_files, _ = read_list_file(self._path_file)\n        self._couples = [[l, r, gt] for l, r, gt in zip(left_files, right_files, gt_files)]\n        #flags \n        self._usePfm = gt_files[0].endswith(\'pfm\') or gt_files[0].endswith(\'PFM\')\n        if not self._usePfm:\n            gg = cv2.imread(gt_files[0],-1)\n            self._double_prec_gt = (gg.dtype == np.uint16)\n\n        #create dataset\n        dataset = tf.data.Dataset.from_tensor_slices(self._couples).repeat(self._num_epochs)\n        if self._shuffle:\n            dataset = dataset.shuffle(self._batch_size*50)\n        \n        #load images\n        dataset = dataset.map(self._load_image)\n\n        #transform data\n        dataset = dataset.batch(self._batch_size, drop_remainder=True)\n        dataset = dataset.prefetch(buffer_size=30)\n\n        #get iterator and batches\n        iterator = dataset.make_one_shot_iterator()\n        images = iterator.get_next()\n        self._left_batch = images[0]\n        self._right_batch = images[1]\n        self._gt_batch = images[2]\n\n    ################# PUBLIC METHOD #######################\n\n    def __len__(self):\n        return len(self._couples)\n    \n    def get_max_steps(self):\n        return (len(self)*self._num_epochs)//self._batch_size\n\n    def get_batch(self):\n        return self._left_batch,self._right_batch,self._gt_batch\n    \n    def get_couples(self):\n        return self._couples\n\n'"
Data_utils/preprocessing.py,82,"b'import tensorflow as tf\nfrom matplotlib import cm\nimport numpy as np\n\nFULLY_DIFFERENTIABLE=False\n\ndef pad_image(immy,down_factor = 256,dynamic=False):\n    """"""\n    pad image with a proper number of 0 to prevent problem when concatenating after upconv\n    Args:\n        immy: metaop that produces an image\n        down_factor: downgrade resolution that should be respected before feeding the image to the network\n        dynamic: if dynamic is True use dynamic shape of immy, otherway use static shape\n    """"""\n    if dynamic:\n        immy_shape = tf.shape(immy)\n        new_height = tf.where(tf.equal(immy_shape[-3]%down_factor,0),x=immy_shape[-3],y=(tf.floordiv(immy_shape[-3],down_factor)+1)*down_factor)\n        new_width = tf.where(tf.equal(immy_shape[-2]%down_factor,0),x=immy_shape[-2],y=(tf.floordiv(immy_shape[-2],down_factor)+1)*down_factor)\n    else:\n        immy_shape = immy.get_shape().as_list()\n        new_height = immy_shape[-3] if immy_shape[-3]%down_factor==0 else ((immy_shape[-3]//down_factor)+1)*down_factor\n        new_width = immy_shape[-2] if immy_shape[-2]%down_factor==0 else ((immy_shape[-2]//down_factor)+1)*down_factor\n    \n    pad_height_left = (new_height-immy_shape[-3])//2\n    pad_height_right = (new_height-immy_shape[-3]+1)//2\n    pad_width_left = (new_width-immy_shape[-2])//2\n    pad_width_right = (new_width-immy_shape[-2]+1)//2\n    immy = tf.pad(immy,[[0,0],[pad_height_left,pad_height_right],[pad_width_left,pad_width_right],[0,0]],mode=""REFLECT"")\n    return immy\n\ndef random_crop(crop_shape, tensor_list):\n\t""""""\n\tPerform an alligned random crop on the list of tensors passed as arguments l r and gt\n\t""""""\n\timage_shape = tf.shape(tensor_list[0])\n\tmax_row = image_shape[0]-crop_shape[0]-1\n\tmax_col = image_shape[1]-crop_shape[1]-1\n\tmax_row = tf.cond(max_row>0, lambda: max_row, lambda: 1)\n\tmax_col = tf.cond(max_col>0, lambda: max_col, lambda: 1)\n\tstart_row = tf.random_uniform([],minval=0,maxval=max_row,dtype=tf.int32)\n\tstart_col = tf.random_uniform([],minval=0,maxval=max_col,dtype=tf.int32)\n\tresult=[]\n\tfor x in tensor_list:\n\t\tstatic_shape = x.get_shape().as_list()\n\t\tif len(static_shape)==3:\n\t\t\t#crop\n\t\t\ttemp = x[start_row:start_row+crop_shape[0],start_col:start_col+crop_shape[1],:]\n\t\t\t#force shape\n\t\t\ttemp.set_shape([crop_shape[0],crop_shape[1],static_shape[-1]])\n\t\telse:\n\t\t\t#crop\n\t\t\ttemp = x[:,start_row:start_row+crop_shape[0],start_col:start_col+crop_shape[1],:]\n\t\t\t#force shape\n\t\t\ttemp.set_shape([static_shape[0],crop_shape[0],crop_shape[1],static_shape[-1]])\n\t\tresult.append(temp)\n\treturn result\n\n\t\n\n\ndef augment(left_img, right_img):\n    active = tf.random_uniform(shape=[4], minval=0, maxval=1, dtype=tf.float32)\n    left_img = tf.cast(left_img,tf.float32)\n    right_img = tf.cast(right_img,tf.float32)\n\n    # random gamma\n    # random_gamma = tf.random_uniform(shape=(),minval=0.95,maxval=1.05,dtype=tf.float32)\n    # left_img = tf.where(active[0]>0.5,left_img,tf.image.adjust_gamma(left_img,random_gamma))\n    # right_img = tf.where(active[0]>0.5,right_img,tf.image.adjust_gamma(right_img,random_gamma))\n\n    # random brightness\n    random_delta = tf.random_uniform(shape=(), minval=-0.05, maxval=0.05, dtype=tf.float32)\n    left_img = tf.where(active[1] > 0.5, left_img, tf.image.adjust_brightness(left_img, random_delta))\n    right_img = tf.where(active[1] > 0.5, right_img, tf.image.adjust_brightness(right_img, random_delta))\n\n    # random contrast\n    random_contrast = tf.random_uniform(shape=(), minval=0.8, maxval=1.2, dtype=tf.float32)\n    left_img = tf.where(active[2] > 0.5, left_img, tf.image.adjust_contrast(left_img, random_contrast))\n    right_img = tf.where(active[2] > 0.5, right_img, tf.image.adjust_contrast(right_img, random_contrast))\n\n    # random hue\n    random_hue = tf.random_uniform(shape=(), minval=0.8, maxval=1.2, dtype=tf.float32)\n    left_img = tf.where(active[3] > 0.5, left_img,tf.image.adjust_hue(left_img, random_hue))\n    right_img = tf.where(active[3] > 0.5, right_img, tf.image.adjust_hue(right_img, random_hue))\n\n    left_img = tf.clip_by_value(left_img,0,255)\n    right_img = tf.clip_by_value(right_img,0,255)\n\n    return left_img,right_img\n\ndef colorize_img(value, vmin=None, vmax=None, cmap=None):\n    """"""\n    A utility function for TensorFlow that maps a grayscale image to a matplotlib colormap for use with TensorBoard image summaries.\n    By default it will normalize the input value to the range 0..1 before mapping to a grayscale colormap.\n    Arguments:\n      - value: 4D Tensor of shape [batch_size,height, width,1]\n      - vmin: the minimum value of the range used for normalization. (Default: value minimum)\n      - vmax: the maximum value of the range used for normalization. (Default: value maximum)\n      - cmap: a valid cmap named for use with matplotlib\'s \'get_cmap\'.(Default: \'gray\')\n    \n    Returns a 3D tensor of shape [batch_size,height, width,3].\n    """"""\n\n    # normalize\n    vmin = tf.reduce_min(value) if vmin is None else vmin\n    vmax = tf.reduce_max(value) if vmax is None else vmax\n    value = (value - vmin) / (vmax - vmin) # vmin..vmax\n\n    # quantize\n    indices = tf.to_int32(tf.round(value[:,:,:,0]*255))\n\n    # gather\n    color_map = cm.get_cmap(cmap if cmap is not None else \'gray\')\n    colors = color_map(np.arange(256))[:,:3]\n    colors = tf.constant(colors, dtype=tf.float32)\n    value = tf.gather(colors, indices)\n    return value\n\n###PER LOSS RIPROIEZIONE###\n\ndef bilinear_sampler(imgs, coords):\n\t""""""\n\tConstruct a new image by bilinear sampling from the input image.\n\tPoints falling outside the source image boundary have value 0.\n\tArgs:\n\t\timgs: source image to be sampled from [batch, height_s, width_s, channels]\n\t\tcoords: coordinates of source pixels to sample from [batch, height_t,width_t, 2]. height_t/width_t correspond to the dimensions of the outputimage (don\'t need to be the same as height_s/width_s). The two channels correspond to x and y coordinates respectively.\n\tReturns:\n\t\tA new sampled image [batch, height_t, width_t, channels]\n\t""""""\n\n\tdef _repeat(x, n_repeats):\n\t\trep = tf.transpose(\n\t\t\ttf.expand_dims(tf.ones(shape=tf.stack([\n\t\t\t\tn_repeats,\n\t\t\t])), 1), [1, 0])\n\t\trep = tf.cast(rep, \'float32\')\n\t\tx = tf.matmul(tf.reshape(x, (-1, 1)), rep)\n\t\treturn tf.reshape(x, [-1])\n\n\twith tf.name_scope(\'image_sampling\'):\n\t\tcoords_x, coords_y = tf.split(coords, [1, 1], axis=3)\n\t\tinp_size = tf.shape(imgs)\n\t\tcoord_size = tf.shape(coords)\n\t\tout_size = [coord_size[0],coord_size[1],coord_size[2],inp_size[3]]\n\n\t\tcoords_x = tf.cast(coords_x, \'float32\')\n\t\tcoords_y = tf.cast(coords_y, \'float32\')\n\n\t\tx0 = tf.floor(coords_x)\n\t\tx1 = x0 + 1\n\t\ty0 = tf.floor(coords_y)\n\t\ty1 = y0 + 1\n\n\t\ty_max = tf.cast(inp_size[1] - 1, \'float32\')\n\t\tx_max = tf.cast(inp_size[2] - 1, \'float32\')\n\t\tzero = tf.zeros([1], dtype=\'float32\')\n\n\t\twt_x0 = x1 - coords_x\n\t\twt_x1 = coords_x - x0\n\t\twt_y0 = y1 - coords_y\n\t\twt_y1 = coords_y - y0\n\n\t\tx0_safe = tf.clip_by_value(x0, zero[0], x_max)\n\t\ty0_safe = tf.clip_by_value(y0, zero[0], y_max)\n\t\tx1_safe = tf.clip_by_value(x1, zero[0], x_max)\n\t\ty1_safe = tf.clip_by_value(y1, zero[0], y_max)\n\n\t\t## indices in the flat image to sample from\n\t\tdim2 = tf.cast(inp_size[2], \'float32\')\n\t\tdim1 = tf.cast(inp_size[2] * inp_size[1], \'float32\')\n\t\tbase = tf.reshape(_repeat(tf.cast(tf.range(coord_size[0]), \'float32\') * dim1,coord_size[1] * coord_size[2]),[out_size[0], out_size[1], out_size[2], 1])\n\n\t\tbase_y0 = base + y0_safe * dim2\n\t\tbase_y1 = base + y1_safe * dim2\n\t\tidx00 = x0_safe + base_y0\n\t\tidx01 = x0_safe + base_y1\n\t\tidx10 = x1_safe + base_y0\n\t\tidx11 = x1_safe + base_y1\n\n\t\t## sample from imgs\n\t\timgs_flat = tf.reshape(imgs, tf.stack([-1, inp_size[3]]))\n\t\timgs_flat = tf.cast(imgs_flat, \'float32\')\n\t\tim00 = tf.reshape(tf.gather(imgs_flat, tf.cast(idx00, \'int32\')), out_size)\n\t\tim01 = tf.reshape(tf.gather(imgs_flat, tf.cast(idx01, \'int32\')), out_size)\n\t\tim10 = tf.reshape(tf.gather(imgs_flat, tf.cast(idx10, \'int32\')), out_size)\n\t\tim11 = tf.reshape(tf.gather(imgs_flat, tf.cast(idx11, \'int32\')), out_size)\n\n\t\tw00 = wt_x0 * wt_y0\n\t\tw01 = wt_x0 * wt_y1\n\t\tw10 = wt_x1 * wt_y0\n\t\tw11 = wt_x1 * wt_y1\n\n\t\toutput = tf.add_n([\n\t\t\tw00 * im00, w01 * im01,\n\t\t\tw10 * im10, w11 * im11\n\t\t])\n\n\t\treturn output\n\ndef warp_image(img, flow):\n\t""""""\n\tGiven an image and a flow generate the warped image, for stereo img is the right image, flow is the disparity alligned with left\n\timg: image that needs to be warped\n\tflow: Generic optical flow or disparity\n\t""""""\n\n\tdef build_coords(immy):   \n\t\tmax_height = 2048\n\t\tmax_width = 2048\n\t\tpixel_coords = np.ones((1, max_height, max_width, 2))\n\n\t\t# build pixel coordinates and their disparity\n\t\tfor i in range(0, max_height):\n\t\t\tfor j in range(0, max_width):\n\t\t\t\tpixel_coords[0][i][j][0] = j\n\t\t\t\tpixel_coords[0][i][j][1] = i\n\n\t\tpixel_coords = tf.constant(pixel_coords, tf.float32)\n\t\treal_height = tf.shape(immy)[1]\n\t\treal_width = tf.shape(immy)[2]\n\t\treal_pixel_coord = pixel_coords[:,0:real_height,0:real_width,:]\n\t\timmy = tf.concat([immy, tf.zeros_like(immy)], axis=-1)\n\t\toutput = real_pixel_coord - immy\n\n\t\treturn output\n\n\tcoords = build_coords(flow)\n\twarped = bilinear_sampler(img, coords)\n\treturn warped\n\ndef _rescale_tf(img,out_shape):\n\t""""""\n\tRescale image using bilinear upsampling\n\t""""""\n\t#print(out_shape)\n\tdef _build_coords(immy,out_shape):\n\t\tbatch_size = tf.shape(immy)[0]\n\t\tin_height = tf.cast(tf.shape(immy)[1],tf.float32)-1\n\t\tin_width = tf.cast(tf.shape(immy)[2],tf.float32)-1\n\n\t\tout_height = out_shape[0]\n\t\tout_width = out_shape[1]\n\n\t\tdelta_x = in_width/tf.cast(out_width-1,tf.float32)\n\t\tdelta_y = in_height/tf.cast(out_height-1,tf.float32)\n\n\t\tcoord_x = tf.concat([tf.range(in_width-1E-4,delta=delta_x,dtype=tf.float32),[in_width]],axis=0)\n\t\tcoord_x = tf.expand_dims(coord_x,axis=0)\n\t\tcoord_x_tile = tf.tile(coord_x,[out_height,1])\n\n\t\tcoord_y = tf.concat([tf.range(in_height-1E-4,delta=delta_y,dtype=tf.float32),[in_height]],axis=0)\n\t\tcoord_y = tf.expand_dims(coord_y,axis=1)\n\t\tcoord_y_tile = tf.tile(coord_y,[1,out_width])\n\n\t\tcoord = tf.stack([coord_x_tile,coord_y_tile],axis=-1)\n\t\t#coord = tf.Print(coord,[coord[:,:,0]],summarize=1000)\n\t\tcoord = tf.expand_dims(coord,axis=0)\n\t\tcoord = tf.tile(coord,[batch_size,1,1,1])\n\n\t\treturn coord\n\t\n\tcoord = _build_coords(img,out_shape)\n\twarped = bilinear_sampler(img,coord)\n\tinput_shape = img.get_shape().as_list()\n\twarped.set_shape([input_shape[0],None,None,input_shape[-1]])\n\treturn warped\n\ndef rescale_image(img,out_shape):\n\tif FULLY_DIFFERENTIABLE:\n\t\treturn _rescale_tf(img,out_shape)\n\telse:\n\t\treturn tf.image.resize_images(img,out_shape,method=tf.image.ResizeMethod.BILINEAR)\n\n\ndef resize_to_prediction(x, pred):\n    return rescale_image(x,tf.shape(pred)[1:3])\n'"
Data_utils/weights_utils.py,6,"b'import tensorflow as tf\nimport os\n\ndef get_var_to_restore_list(ckpt_path, mask=[], prefix="""", ignore_list=[]):\n    """"""\n    Get all the variable defined in a ckpt file and add them to the returned var_to_restore list. Allows for partially defined model to be restored fomr ckpt files.\n    Args:\n        ckpt_path: path to the ckpt model to be restored\n        mask: list of layers to skip\n        prefix: prefix string before the actual layer name in the graph definition\n    """"""\n    variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n    variables_dict = {}\n    for v in variables:\n        name = v.name[:-2]\n        #print(name)\n        skip=False\n        #check for skip\n        for m in mask:\n            if m in name:\n                skip=True\n                continue\n        if not skip:\n            variables_dict[v.name[:-2]] = v\n\n    #print(\'====================================================\')\n    reader = tf.train.NewCheckpointReader(ckpt_path)\n    var_to_shape_map = reader.get_variable_to_shape_map()\n    var_to_restore = {}\n    for key in var_to_shape_map:\n        t_key=key\n        #print(key)\n        for ig in ignore_list:\n            t_key=t_key.replace(ig,\'\')\n        if prefix+t_key in variables_dict.keys():\n            var_to_restore[key] = variables_dict[prefix+t_key]\n    \n    return var_to_restore\n\n\ndef check_for_weights_or_restore_them(logdir, session, initial_weights=None, prefix=\'\', ignore_list=[]):\n    """"""\n    Check for the existance of a previous checkpoint in logdir, if not found and weights is set to a valid path restore that model instead.\n    Args:\n        log_dir: dir where to look for previous checkpoints\n        session: tensorflow session to restore weights\n        initial_weights: optional fall back weights to be used if no available weight as been found\n        prefix: prefix to be putted before variable names in the ckpt file\n    Returns:\n        A boolean that states if the weights have been restored or not and the number of step restored (if any)\n    """"""\n    ckpt = tf.train.latest_checkpoint(logdir)\n    if ckpt:\n        print(\'Found valid checkpoint file: {}\'.format(ckpt))\n        var_to_restore = get_var_to_restore_list(ckpt, [], prefix="""")\n        restorer = tf.train.Saver(var_list=var_to_restore)\n        restorer.restore(session,ckpt)\n        step = int(ckpt.split(\'-\')[-1])\n        return True,step\n    elif initial_weights is not None:\n        if os.path.isdir(initial_weights):\n            #if its a directory fetch the last checkpoint\n            initial_weights = tf.train.latest_checkpoint(initial_weights)\n        step = 0\n        var_to_restore = get_var_to_restore_list(initial_weights, [], prefix=prefix, ignore_list=ignore_list)\n        print(\'Found {} variables to restore in {}\'.format(len(var_to_restore),initial_weights))\n        if len(var_to_restore)>0:\n            restorer = tf.train.Saver(var_list=var_to_restore)\n            restorer.restore(session, initial_weights)\n            return True,0\n        else:\n            return False,0\n    else:\n        print(\'Unable to restore any weight\')\n        return False,0\n'"
Demo/Live_Adaptation_Demo.py,0,"b'import os,sys,inspect\ncurrentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\nparentdir = os.path.dirname(currentdir)\nsys.path.insert(0,parentdir) \n\nimport queue\nimport argparse\nimport grabber\nimport cv2\nimport demo_model\n\nimport Nets\n\n\nif __name__==\'__main__\':\n    parser=argparse.ArgumentParser(description=\'Real-time stereo live demo\')\n    parser.add_argument(""--modelName"", help=""name of the stereo model to be used"", default=""MADNet"", choices=Nets.STEREO_FACTORY.keys())\n    parser.add_argument(""--weights"",help=""path to the initial weights for the disparity estimation network"",default=None)\n    parser.add_argument(""--mode"",help=""online adaptation mode: NONE - perform only inference, FULL - full online backprop, MAD - backprop only on portions of the network"", choices=[\'NONE\',\'FULL\',\'MAD\'], default=\'NONE\')\n    parser.add_argument(""--lr"", help=""value for learning rate"",default=0.0001, type=float)\n    parser.add_argument(""--blockConfig"",help=""path to the block_config json file"",default=\'../block_config/MadNet_full.json\')\n    parser.add_argument(""--imageShape"", help=\'two int, reshape input images to this shape [height,width], -1 to disable\', nargs=\'+\', type=int, default=[480,640])\n    parser.add_argument(""--cropShape"", help=\'two int, crop input images to this shape [height,width], -1 to disable\', nargs=\'+\', type=int, default=[320,512]), \n    parser.add_argument(""--SSIMTh"",help=""reset network to initial configuration if loss is above this value"",type=float,default=0.5)\n    parser.add_argument(""--cameraConfig"", help=""path to a configuration file for the camera"", default=\'/home/alessio/code/Real-time-self-adaptive-deep-stereo/Demo/configuration.json\')\n    parser.add_argument(""--cameraName"", help=""name of the camera grabber to build"", default=""ZED_Mini"", choices=grabber.get_available_camera())\n    args = parser.parse_args()\n\n    assert(os.path.exists(args.cameraConfig))\n    assert(len(args.imageShape)==2)\n\n    #setup shared queue for readed frame\n    camera_frames = queue.Queue(1)\n\n    #create camera grabber and disparity network\n    dd = demo_model.RealTimeStereo(\n        camera_frames,\n        model_name=args.modelName,\n        weight_path=args.weights,\n        learning_rate=args.lr,\n        block_config_path=args.blockConfig,\n        image_shape = args.imageShape,\n        crop_shape=args.cropShape,\n        SSIMTh = args.SSIMTh,\n        mode = args.mode\n        )\n    gg = grabber.get_camera(\n        args.cameraName,\n        camera_frames,\n        config=args.cameraConfig, \n        framerate=30)\n\n    print(\'Threads ready to start\')\n    #unleash the thread\n    gg.start()\n    dd.start()\n\n    #print(\'Going To sleep\')\n    a=input(\'Press something to stop\')\n    \n    print(\'Requesting Stops\')\n\n    gg.stop()\n    gg.join()\n    print(\'Camera grabber stopped\')\n\n\n    dd.stop()\n    dd.join()\n    print(\'detector stopped\')\n\n    print(\'Goodbye\')'"
Demo/demo_model.py,15,"b'import threading\nimport tensorflow as tf \nimport numpy as np \nimport json\nimport Nets \nimport cv2\n\nfrom Data_utils import preprocessing, weights_utils\nfrom Sampler import sampler_factory\nfrom Losses import loss_factory\n\nclass RealTimeStereo(threading.Thread):\n    """"""\n    Class that implement real time self adaptive stereo\n    """"""\n    _camera_window_name_left = ""left frame""\n    _camera_window_name_right = ""right frame""\n    _disparity_window_name = ""disparity prediction""\n    def __init__(\n        self,\n        camera_buffer,\n        model_name=\'MADNet\',\n        weight_path=None,\n        learning_rate=0.0001,\n        block_config_path=\'../block_config/MadNet_full.json\',\n        image_shape=[480,640],\n        crop_shape=[None,None],\n        SSIMTh=0.5,\n        mode=\'MAD\'):\n        """"""\n        Create a self adaptive deep stereo system\n        Args\n        ---\n        camera_buffer: queue\n            queue to read left and right thread\n        model_name: string\n            name of the stereo CNN to create\n        weight_path: string\n            path to the initial weights for the Stereo CNN, None for random initialization\n        learning_rate: float\n            learning rate for online adaptation\n        block_config_path: string\n            path to the block_config json file\n        image_shape: list of two int\n            height and width of the stereo frames\n        crop_shape: list of two int\n            crop input to this dimension before feeding them to the network\n        SSIMTh: float\n            reset the network to the initial configuration if current SSIM > SSIMTh\n        mode: string\n            nline adaptation mode: NONE - perform only inference, FULL - full online backprop, MAD - backprop only on portions of the network\n        """"""\n        self._camera_buffer = camera_buffer\n        self._model_name = model_name\n        self._weight_path = weight_path\n        self._learning_rate = learning_rate\n        self._block_config_path = block_config_path\n        self._image_shape = image_shape\n        self._crop_shape = crop_shape\n        self._SSIMTh = SSIMTh\n        self._mode = mode \n        self._stop_flag = False\n        self._ready = self._setup_graph()\n        self._ready &= self._initialize_model()\n        threading.Thread.__init__(self)\n\n    def _load_block_config(self):\n        #load json file config\n        with open(self._block_config_path) as json_data:\n            self._train_config = json.load(json_data)\n\n    def _build_input_ops(self):\n        #input placeholder ops\n        self._left_placeholder = tf.placeholder(tf.float32,shape=[1,None, None,3], name=\'left_input\')\n        self._right_placeholder = tf.placeholder(tf.float32,shape=[1,None, None,3], name=\'right_input\')\n\n        self._left_input = self._left_placeholder\n        self._right_input = self._right_placeholder\n        \n        if self._image_shape[0] is not None:\n            self._left_input = preprocessing.rescale_image(self._left_input, self._image_shape)\n            self._right_input = preprocessing.rescale_image(self._right_input, self._image_shape)\n        \n        if self._crop_shape[0] is not None:\n            self._left_input = tf.image.resize_image_with_crop_or_pad(self._left_input, self._crop_shape[0], self._crop_shape[1])\n            self._right_input = tf.image.resize_image_with_crop_or_pad(self._right_input, self._crop_shape[0], self._crop_shape[1])\n\n    def _build_network(self):\n        #network model\n        with tf.variable_scope(\'model\'):\n            net_args = {}\n            net_args[\'left_img\'] = self._left_input\n            net_args[\'right_img\'] = self._right_input\n            net_args[\'split_layers\'] = [None]\n            net_args[\'sequence\'] = True\n            net_args[\'train_portion\'] = \'BEGIN\'\n            net_args[\'bulkhead\'] = True if self._mode==\'MAD\' else False\n            self._net = Nets.get_stereo_net(self._model_name, net_args)\n            self._predictions = self._net.get_disparities()\n            self._full_res_disp = self._predictions[-1]\n\n            self._inputs = {\n                \'left\':self._left_input,\n                \'right\':self._right_input,\n                \'target\':tf.zeros([1,self._image_shape[0],self._image_shape[1],1],dtype=tf.float32)\n            }\n\n            #full resolution loss between warped right image and original left image\n            self._loss =  loss_factory.get_reprojection_loss(\'mean_SSIM_l1\',reduced=True)(self._predictions,self._inputs)\n    \n    def _MAD_adaptation_ops(self):\n        #build train ops for separate portions of the network\n        self._load_block_config()\n\n        #keep all predictions except full res\n        predictions = self._predictions[:-1] \n        \n        inputs_modules = self._inputs\n        \n        assert(len(predictions)==len(self._train_config))\n        for counter,p in enumerate(predictions):\n            print(\'Build train ops for disparity {}\'.format(counter))\n                    \n            #rescale predictions to proper resolution\n            multiplier = tf.cast(tf.shape(self._left_input)[1]//tf.shape(p)[1],tf.float32)\n            p = preprocessing.resize_to_prediction(p,inputs_modules[\'left\'])*multiplier\n\n            #compute reprojection error\n            with tf.variable_scope(\'reprojection_\'+str(counter)):\n                reconstruction_loss = loss_factory.get_reprojection_loss(\'mean_SSIM_l1\',reduced=True)([p],inputs_modules)\n\n            #build train op\n            layer_to_train = self._train_config[counter]\n            print(\'Going to train on {}\'.format(layer_to_train))\n            var_accumulator=[]\n            for name in layer_to_train:\n                var_accumulator+=self._net.get_variables(name)\n            print(\'Number of variable to train: {}\'.format(len(var_accumulator)))\n                \n            #add new training op\n            self._train_ops.append(self._trainer.minimize(reconstruction_loss,var_list=var_accumulator))\n\n            print(\'Done\')\n            print(\'=\'*50)\n        \n        #create Sampler to fetch portions to train\n        self._sampler = sampler_factory.get_sampler(\'PROBABILITY\',1,0)\n    \n    def _Full_adaptation_ops(self):\n        self._train_ops.append(self._trainer.minimize(self._loss))\n        self._sampler = sampler_factory.get_sampler(\'FIXED\',1,0)\n    \n    def _no_adaptation_ops(self):\n        #mock ops that don\'t do anything\n        self._train_ops.append(tf.no_op())\n        self._sampler = sampler_factory.get_sampler(\'FIXED\',1,0)\n\n    def _build_adaptation_ops(self):\n        """"""\n        Populate self._train_ops\n        """"""\n        #self._trainer = tf.train.MomentumOptimizer(self._learning_rate,0.9)\n        self._trainer = tf.train.AdamOptimizer(self._learning_rate)\n        self._train_ops = []\n        if self._mode == \'MAD\':\n            self._MAD_adaptation_ops()\n        elif self._mode == \'FULL\':\n            self._Full_adaptation_ops()\n        elif self._mode == \'NONE\':\n            self._no_adaptation_ops()\n\n    def _setup_graph(self):\n        """"""\n        Build tensorflow graph and ops\n        """"""\n        self._build_input_ops()\n\n        self._build_network()\n\n        self._build_adaptation_ops()\n\n        return True\n    \n    def _initialize_model(self):\n        """"""\n        Create tensorflow session and initialize the network\n        """"""\n        #session\n        gpu_options = tf.GPUOptions(allow_growth=True)\n        self._session = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n\n        #variable initialization\n        initializers = [tf.global_variables_initializer(),tf.local_variables_initializer()]\n        self._session.run(initializers)\n\n        #restore disparity inference weights and populate self._restore_op\n        if self._weight_path is not None:\n            var_to_restore = weights_utils.get_var_to_restore_list(self._weight_path, [])\n            self._restorer = tf.train.Saver(var_list=var_to_restore)\n            self._restore_op = lambda: self._restorer.restore(self._session, self._weight_path)\n            self._restore_op()\n            print(\'Disparity Net Restored?: {}, number of restored variables: {}\'.format(True,len(var_to_restore)))\n        else:\n            self._restore_op = lambda: self._session.run(initializers)\n\n        #operation to select the different train ops\n        num_actions=len(self._train_ops)\n        self._sample_distribution=np.zeros(shape=[num_actions])\n        self._temp_score = np.zeros(shape=[num_actions])\n        self._loss_t_2 = 0\n        self._loss_t_1 = 0\n        self._expected_loss = 0\n        self._last_trained_blocks = 0\n\n        print(\'Network Ready\')\n\n        return True\n\n    def _setup_gui(self):\n        """"""\n        Create a simple OpenCV GUI\n        """"""\n        cv2.namedWindow(self._camera_window_name_left, cv2.WINDOW_NORMAL)\n        cv2.namedWindow(self._camera_window_name_right, cv2.WINDOW_NORMAL)\n        cv2.namedWindow(self._disparity_window_name, cv2.WINDOW_NORMAL)\n    \n    def stop(self):\n        """"""\n        Stop the prediciton and kill the thread\n        """"""\n        self._stop_flag =True\n\n    def run(self):\n        self._setup_gui()\n        first=True\n        it=0\n        while not self._stop_flag:\n            #fetch frames\n            frames = self._camera_buffer.get(block=True)\n            left_frame = frames[:1]\n            right_frame = frames[1:]\n\n            #Fetch portion of network to train\n            #softmax\n            exp = np.exp(self._sample_distribution)\n            distribution = exp/np.sum(exp,axis=0)\n            train_op_id = self._sampler.sample(distribution)[0]\n            selected_train_op = self._train_ops[train_op_id]\n\n            #build list of tensorflow operations that needs to be executed + feed dict\n            tf_fetches = [self._loss, selected_train_op, self._full_res_disp, self._left_input, self._right_input]\n            fd = {\n                self._left_placeholder: left_frame,\n                self._right_placeholder: right_frame\n            }\n\n            #run network\n            full_ssim, _, disp_prediction, lefty, righty = self._session.run(tf_fetches, feed_dict=fd)\n            print(\'Step {}: {}\'.format(it,full_ssim))\n\n            if self._mode == \'MAD\':\n                #update sampling probabilities\n                if first:\n                    self._loss_t_2 = full_ssim\n                    self._loss_t_1 = full_ssim\n                self._expected_loss = 2*self._loss_t_1-self._loss_t_2\t\n                gain_loss=self._expected_loss-full_ssim\n                self._sample_distribution = 0.99*self._sample_distribution\n                self._sample_distribution[self._last_trained_blocks] += 0.01*gain_loss\n\n                self._last_trained_blocks = train_op_id\n                self._loss_t_2 = self._loss_t_1\n                self._loss_t_1 = full_ssim\n            \n            if full_ssim>self._SSIMTh:\n                print(\'Resetting Network...\')\n                self._restore_op()\n            \n            #show current detection\n            cv2.imshow(self._camera_window_name_left, lefty[0].astype(np.uint8))\n            cv2.imshow(self._camera_window_name_right, righty[0].astype(np.uint8))\n            cv2.imshow(self._disparity_window_name, cv2.applyColorMap(disp_prediction[0].astype(np.uint8),cv2.COLORMAP_JET))\n            cv2.waitKey(1)\n            it+=1\n        \n        #close session\n        self._session.close()\n        cv2.destroyAllWindows()'"
Demo/grabber.py,0,"b'import threading\nimport time\nimport numpy as np\nimport abc\nimport json\n\n############################################################\n###########          CAMERA FACTORY              ###########\n###########################################################\n\n_GRABBER_FACTORY={}\n\ndef get_camera(name, frame_queue, config=None, framerate=30):\n    """"""\n    factory method to construct a camera wrapper\n    """"""\n    if name not in _GRABBER_FACTORY:\n        raise Exception(\'Unrecognized camera type: {}\'.format(name))\n    else:\n        return _GRABBER_FACTORY[name](frame_queue, config=config, framerate=framerate)\n\ndef get_available_camera():\n    return _GRABBER_FACTORY.keys()\n\ndef register_camera_to_factory():\n    def decorator(cls):\n        _GRABBER_FACTORY[cls._name]=cls\n        return cls\n    return decorator\n\n#####################################################################\n##############           ABSTRACT CAMERA                #############\n####################################################################\n\n\nclass ImageGrabber(threading.Thread):\n    """"""\n    Thread to grab frames from the camera and load them in frame_queue\n    """"""\n    __metaclass__=abc.ABCMeta\n    def __init__(self, frame_queue, config=None, framerate=30):\n        """"""\n        Args\n        -----\n        frame_queue: queue\n            synchronized queue where left and right frames will be loaded\n        config: path\n            path to json file for calibration and/or other configuration parameters\n        framerate: int\n            target frame per second\n        """"""\n        threading.Thread.__init__(self)\n        self._config = config\n        self._connect_to_camera()\n        self._buffer = frame_queue\n        self._sleeptime = 1/framerate\n        self._stop_acquire=False\n    \n    def stop(self):\n        """"""\n        Stop the acquisition of new frames from the camera and kill the thread\n        """"""\n        self._stop_acquire=True\n    \n    def run(self):\n        """"""\n        Main body method, grab frames from camera and put them on buffer as a [2,h,w,c] numpy array\n        """"""\n        while not self._stop_acquire:\n            l,r = self._read_frame()\n            self._buffer.put(np.stack([l,r],axis=0))\n            time.sleep(self._sleeptime)\n        \n        self._disconnect_from_camera()\n\n    @abc.abstractmethod\n    def _read_frame(self):\n        """"""\n        Read left and right rectified frame and return them\n        """"""\n    \n    @abc.abstractmethod\n    def _connect_to_camera(self):\n        """"""\n        Connect to external camera\n        """"""\n    \n    @abc.abstractmethod\n    def _disconnect_from_camera(self):\n        """"""\n        Disconnect from external camera\n        """"""\n\n\n#########################################################################\n#################           ZED MINI                    #################\n#########################################################################\n\nimport pyzed.sl as sl\n\n@register_camera_to_factory()\nclass ZEDMini(ImageGrabber):\n    _name = \'ZED_Mini\'\n    _key_to_res = {\n        \'2K\' : sl.RESOLUTION.RESOLUTION_HD2K,\n        \'1080p\' : sl.RESOLUTION.RESOLUTION_HD1080,\n        \'720p\' : sl.RESOLUTION.RESOLUTION_HD720,\n        \'VGA\' : sl.RESOLUTION.RESOLUTION_VGA\n    }\n\n    """""" Read Stereo frames from a ZED Mini stereo camera. """"""\n    def _read_frame(self):\n        err = self._cam.grab(self._runtime)\n        if err == sl.ERROR_CODE.SUCCESS:\n            self._cam.retrieve_image(self._left_frame, sl.VIEW.VIEW_LEFT)\n            self._cam.retrieve_image(self._right_frame, sl.VIEW.VIEW_RIGHT)\n            return self._left_frame.get_data()[:,:,:3], self._right_frame.get_data()[:,:,:3]\n    \n    def _connect_to_camera(self):\n        # road option from config file\n        with open(self._config) as f_in:\n            self._config = json.load(f_in)\n\n        self._params = sl.InitParameters()\n        \n        if \'resolution\' in self._config:\n            self._params.camera_resolution = self._key_to_res[self._config[\'resolution\']]\n        else:\n            self._params.camera_resolution = sl.RESOLUTION.RESOLUTION_HD720\n        \n        if \'fps\' in self._config:\n            self._params.camera_fps = self._config[\'fps\']\n        else:\n            self._params.camera_fps = 30\n        \n        self._cam = sl.Camera()\n        status = self._cam.open(self._params)\n        if status != sl.ERROR_CODE.SUCCESS:\n            print(status)\n            raise Exception(\'Unable to connect to Stereo Camera\')\n        self._runtime = sl.RuntimeParameters()\n        self._left_frame = sl.Mat()\n        self._right_frame = sl.Mat()\n\n    def _disconnect_from_camera(self):\n        self._cam.close()        \n\n\n#########################################################################\n#################           SMATT CAM                   #################\n#########################################################################\n\n#Example of frame grabber for a custom camera\n\n# from stereocam import StereoCamera \n\n# @register_camera_to_factory()\n# class SmattCam(ImageGrabber):\n#     _name=\'SmattCam\'\n#     """"""\n#     Read frames from smart camera from Mattoccia et al.\n#     """"""\n#     def _read_frame(self):\n#         left,right =  self._cam.grab_frames()\n#         left = np.repeat(left, 3, axis=-1)\n#         right = np.repeat(right, 3, axis=-1)\n#         return left,right\n    \n#     def _connect_to_camera(self):\n#         self._cam = StereoCamera(self._config)\n#         self._cam.calibrate()\n\n    \n#     def _disconnect_from_camera(self):\n#         pass'"
Losses/__init__.py,0,b''
Losses/loss_factory.py,63,"b'import tensorflow as tf\nimport numpy as np\n\ndef l1(x,y,mask=None):\n\t""""""\n\tpixelwise reconstruction error\n\tArgs:\n\t\tx: predicted image\n\t\ty: target image\n\t\tmask: compute only on this points\n\t""""""\n\tif mask is None:\n\t\tmask=tf.ones_like(x, dtype=tf.float32)\n\treturn mask*tf.abs(x-y)\n\ndef l2(x,y,mask=None):\n\t""""""\n\tPixelWise squarred error\n\tArgs:\n\t\tx: predicted image\n\t\ty: target image\n\t\tmask: compute only on this points\n\t""""""\n\tif mask is None:\n\t\tmask=tf.ones_like(x, dtype=tf.float32)\n\treturn mask*tf.square(x-y)\n\ndef mean_l1(x,y,mask=None):\n\t""""""\n\tMean reconstruction error\n\tArgs:\n\t\tx: predicted image\n\t\ty: target image\n\t\tmask: compute only on this points\n\t""""""\n\tif mask is None:\n\t\tmask=tf.ones_like(x, dtype=tf.float32)\n\treturn tf.reduce_sum(mask*tf.abs(x-y))/tf.reduce_sum(mask)\n\ndef mean_l2(x,y,mask=None):\n\t""""""\n\tMean squarred error\n\tArgs:\n\t\tx: predicted image\n\t\ty: target image\n\t\tmask: compute only on this points\n\t""""""\n\tif mask is None:\n\t\tmask=tf.ones_like(x, dtype=tf.float32)\n\treturn tf.reduce_sum(mask*tf.square(x-y))/tf.reduce_sum(mask)\n\ndef huber(x,y,c=1.0):\n\tdiff = x-y\n\tl2 = tf.square(diff)\n\tl1 = tf.abs(diff)\n\t#c = (ratio)*tf.reduce_max(diff)\n\tdiff = tf.where(tf.greater(diff,c),0.5*tf.square(c)+c*(l1-c),0.5*l2)\n\treturn diff \n\ndef mean_huber(x,y,mask=None):\n\t""""""\n\tMean huber loss\n\tArgs:\n\t\tx: predicted image\n\t\ty: target image\n\t\tmask: compute only on this points\n\t""""""\n\tif mask is None:\n\t\tmask=tf.ones_like(x, dtype=tf.float32)\n\t\n\treturn tf.reduce_mean(huber(x,y)*mask)\n\ndef sum_huber(x,y,mask=None):\n\t""""""\n\tSum huber loss\n\tArgs:\n\t\tx: predicted image\n\t\ty: target image\n\t\tmask: compute only on this points\n\t""""""\n\tif mask is None:\n\t\tmask=tf.ones_like(x, dtype=tf.float32)\n\t\n\treturn tf.reduce_sum(huber(x,y)*mask)\n\ndef sum_l1(x,y,mask=None):\n\t""""""\n\tSum of the reconstruction error\n\tArgs:\n\t\tx: predicted image\n\t\ty: target image\n\t\tmask: compute only on this points\n\t""""""\n\tif mask is None:\n\t\tmask=tf.ones_like(x, dtype=tf.float32)\n\treturn tf.reduce_sum(mask*tf.abs(x-y))\n\ndef sum_l2(x,y,mask=None):\n\t""""""\n\tSum squarred error\n\tArgs:\n\t\tx: predicted image\n\t\ty: target image\n\t\tmask: compute only on those points\n\t""""""\n\tif mask is None:\n\t\tmask=tf.ones_like(x, dtype=tf.float32)\n\treturn tf.reduce_sum(mask*tf.square(x-y))\n\ndef zncc(x,y):\n\t""""""\n\tZNCC dissimilarity measure\n\tArgs:\n\t\tx: predicted image\n\t\ty: target image\n\t""""""\n\tmean_x = tf.reduce_mean(x)\n\tmean_y = tf.reduce_mean(y)\n\tnorm_x = x-mean_x\n\tnorm_y = y-mean_y\n\tvariance_x = tf.sqrt(tf.reduce_sum(tf.square(norm_x)))\n\tvariance_y = tf.sqrt(tf.reduce_sum(tf.square(norm_y)))\n\n\tzncc = tf.reduce_sum(norm_x*norm_y)/(variance_x*variance_y)\n\treturn 1-zncc\n\n\ndef SSIM(x, y):\n\t""""""\n\tSSIM dissimilarity measure\n\tArgs:\n\t\tx: predicted image\n\t\ty: target image\n\t""""""\n\tC1 = 0.01**2\n\tC2 = 0.03**2\n\tmu_x = tf.nn.avg_pool(x,[1,3,3,1],[1,1,1,1],padding=\'VALID\')\n\tmu_y = tf.nn.avg_pool(y,[1,3,3,1],[1,1,1,1],padding=\'VALID\')\n\t\n\tsigma_x = tf.nn.avg_pool(x**2, [1,3,3,1],[1,1,1,1],padding=\'VALID\') - mu_x**2\n\tsigma_y = tf.nn.avg_pool(y**2, [1,3,3,1],[1,1,1,1],padding=\'VALID\') - mu_y**2\n\tsigma_xy = tf.nn.avg_pool(x*y, [1,3,3,1],[1,1,1,1],padding=\'VALID\') - mu_x * mu_y\n\n\tSSIM_n = (2 * mu_x * mu_y + C1) * (2 * sigma_xy + C2)\n\tSSIM_d = (mu_x ** 2 + mu_y ** 2 + C1) * (sigma_x + sigma_y + C2)\n\n\tSSIM = SSIM_n / SSIM_d\n\n\treturn tf.clip_by_value((1-SSIM)/2, 0 ,1)\n\ndef ssim_l1(x,y,alpha=0.85):\n\tss = tf.pad(SSIM(x,y),[[0,0],[1,1],[1,1],[0,0]])\n\tll = l1(x,y)\n\treturn alpha*ss+(1-alpha)*ll\n\ndef mean_SSIM(x,y):\n\t""""""\n\tMean error over SSIM reconstruction\n\t""""""\n\treturn tf.reduce_mean(SSIM(x,y))\n\n\ndef mean_SSIM_L1(x, y):\n\treturn 0.85* mean_SSIM(x, y) + 0.15 * mean_l1(x, y)\n\n\ndef sign_and_elementwise(x,y):\n\t""""""\n\tReturn the elementwise and of the sign between vectors\n\t""""""\n\telement_wise_sign = tf.sigmoid(10*(tf.sign(x)*tf.sign(y)))\n\treturn tf.reduce_mean(tf.sigmoid(element_wise_sign))\n\ndef cos_similarity(x,y,normalize=False):\n\t""""""\n\tReturn the cosine similarity between (normalized) vectors\n\t""""""\n\tif normalize:\n\t\tx = tf.nn.l2_normalize(x)\n\t\ty = tf.nn.l2_normalize(y)\n\treturn tf.reduce_sum(x*y)\n\ndef smoothness(x, y):\n\t""""""\n\tSmoothness constraint between predicted and image \n\tArgs:\n\t\tx: disparity\n\t\ty: image\n\t""""""\n\tdef gradient_x(image):\n\t\tsobel_x = tf.Variable(initial_value=[[1,0,-1],[2,0,-2],[1,0,-1]],trainable=False,dtype=tf.float32)\n\t\tsobel_x = tf.reshape(sobel_x,[3,3,1,1])\n\t\tif image.get_shape()[-1].value==3:\n\t\t\tsobel_x = tf.concat([sobel_x,sobel_x,sobel_x],axis=2)\n\t\treturn tf.nn.conv2d(image,sobel_x,[1,1,1,1],padding=\'SAME\')\n\t\n\tdef gradient_y(image):\n\t\tsobel_y = tf.Variable(initial_value=[[1,2,-1],[0,0,0],[-1,-2,-1]],trainable=False,dtype=tf.float32)\n\t\tsobel_y = tf.reshape(sobel_y,[3,3,1,1])\n\t\tif image.get_shape()[-1].value==3:\n\t\t\tsobel_y = tf.concat([sobel_y,sobel_y,sobel_y],axis=2)\n\t\treturn tf.nn.conv2d(image,sobel_y,[1,1,1,1],padding=\'SAME\')\n\t\n\t#normalize image and disp in a fixed range\n\tx = x/255\n\ty = y/255\n\n\tdisp_gradients_x = gradient_x(x)\n\tdisp_gradients_y = gradient_y(x)\n\n\timage_gradients_x = tf.reduce_mean(gradient_x(y), axis=-1, keepdims=True) \n\timage_gradients_y = tf.reduce_mean(gradient_y(y), axis=-1, keepdims=True)\n\n\tweights_x = tf.exp(-tf.reduce_mean(tf.abs(image_gradients_x), 3, keepdims=True)) \n\tweights_y = tf.exp(-tf.reduce_mean(tf.abs(image_gradients_y), 3, keepdims=True))\n\n\tsmoothness_x = tf.abs(disp_gradients_x) * weights_x\n\tsmoothness_y = tf.abs(disp_gradients_y) * weights_y\n\n\treturn tf.reduce_mean(smoothness_x + smoothness_y)\n\n\n\n\n\n###################################################################################################################################################################\n\nfrom Data_utils import preprocessing\n\nSUPERVISED_LOSS={\n\t\'mean_l1\':mean_l1,\n\t\'sum_l1\':sum_l1,\n\t\'mean_l2\':mean_l2,\n\t\'sum_l2\':sum_l2,\n\t\'mean_SSIM\':mean_SSIM,\n\t\'mean_SSIM_l1\':mean_SSIM_L1,\n\t\'ZNCC\':zncc,\n\t\'cos_similarity\':cos_similarity,\n\t\'smoothness\':smoothness,\n\t\'mean_huber\':mean_huber,\n\t\'sum_huber\':sum_huber\n}\n\nPIXELWISE_LOSSES={\n\t\'l1\':l1,\n\t\'l2\':l2,\n\t\'SSIM\':SSIM,\n\t\'huber\':huber,\n\t\'ssim_l1\':ssim_l1\n}\n\nALL_LOSSES = dict(SUPERVISED_LOSS)\nALL_LOSSES.update(PIXELWISE_LOSSES)\n\n\ndef get_supervised_loss(name, multiScale=False, logs=False, weights=None, reduced=True, max_disp=None):\n\t""""""\n\tBuild a lambda op to be used to compute a loss function\n\tArgs:\n\t\tname: name of the loss function to build\n\t\tmultiScale: if True compute multiple loss, one for each scale at which disparities are predicted\n\t\tlogs: if True enable tf summary\n\t\tweights: array of weights to be multiplied for the losses at different resolution\n\t\treduced: if true return the sum of the loss across the different scales, false to return an array with the different losses\n\t\tmax_disp: if different from None clip max disparity to be this one\n\t""""""\n\tif name not in ALL_LOSSES.keys():\n\t\tprint(\'Unrecognized loss function, pick one among: {}\'.format(ALL_LOSSES.keys()))\n\t\traise Exception(\'Unknown loss function selected\')\n\t\n\tbase_loss_function = ALL_LOSSES[name]\n\tif weights is None:\n\t\tweights = [1]*10\n\tif max_disp is None:\n\t\tmax_disp=1000\n\tdef compute_loss(disparities,inputs):\n\t\tleft = inputs[\'left\']\n\t\tright = inputs[\'right\']\n\t\ttargets = inputs[\'target\']\n\t\taccumulator=[]\n\t\tif multiScale:\n\t\t\tdisp_to_test=len(disparities)\n\t\telse:\n\t\t\tdisp_to_test=1\n\t\tvalid_map = tf.where(tf.logical_or(tf.equal(targets, 0), tf.greater_equal(targets,max_disp)), tf.zeros_like(targets, dtype=tf.float32), tf.ones_like(targets, dtype=tf.float32))\n\t\t\n\t\tfor i in range(0,disp_to_test):\n\t\t\t#upsample prediction\n\t\t\tcurrent_disp = disparities[-(i+1)]\n\t\t\tdisparity_scale_factor = tf.cast(tf.shape(left)[2],tf.float32)/tf.cast(tf.shape(current_disp)[2],tf.float32)\n\t\t\tresized_disp = preprocessing.resize_to_prediction(current_disp,targets) * disparity_scale_factor\n\n\t\t\tpartial_loss = base_loss_function(resized_disp,targets,valid_map)\n\t\t\t#partial_loss = tf.Print(partial_loss,[disparity_scale_factor,tf.shape(valid_map),tf.reduce_sum(valid_map), tf.reduce_sum(valid_map*resized_disp)/tf.reduce_sum(valid_map), tf.reduce_sum(valid_map*targets)/tf.reduce_sum(valid_map)],summarize=10000)\n\t\t\tif logs:\n\t\t\t\ttf.summary.scalar(\'Loss_resolution_{}\'.format(i),partial_loss)\n\t\t\taccumulator.append(weights[i]*partial_loss)\n\t\tif reduced:\n\t\t\treturn tf.reduce_sum(accumulator)\n\t\telse:\n\t\t\treturn accumulator\n\treturn compute_loss\n\ndef get_reprojection_loss(reconstruction_loss,multiScale=False, logs=False, weights=None,reduced=True):\n\t""""""\n\tBuild a lmbda op to be used to compute a loss function using reprojection between left and right frame\n\tArgs:\n\t\treconstruction_loss: name of the loss function used to compare reprojected and real image\n\t\tmultiScale: if True compute multiple loss, one for each scale at which disparities are predicted\n\t\tlogs: if True enable tf summary\n\t\tweights: array of weights to be multiplied for the losses at different resolution\n\t\treduced: if true return the sum of the loss across the different scales, false to return an array with the different losses\n\t""""""\n\tif reconstruction_loss not in ALL_LOSSES.keys():\n\t\tprint(\'Unrecognized loss function, pick one among: {}\'.format(ALL_LOSSES.keys()))\n\t\traise Exception(\'Unknown loss function selected\')\n\tbase_loss_function = ALL_LOSSES[reconstruction_loss]\n\tif weights is None:\n\t\tweights = [1]*10\n\tdef compute_loss(disparities,inputs):\n\t\tleft = inputs[\'left\']\n\t\tright = inputs[\'right\']\n\t\t#normalize image to be between 0 and 1 \n\t\tleft = tf.cast(left,dtype=tf.float32)/256.0\n\t\tright = tf.cast(right,dtype=tf.float32)/256.0\n\t\taccumulator=[]\n\t\tif multiScale:\n\t\t\tdisp_to_test=len(disparities)\n\t\telse:\n\t\t\tdisp_to_test=1\n\t\tfor i in range(disp_to_test):\n\t\t\t#rescale prediction to full resolution\n\t\t\tcurrent_disp = disparities[-(i+1)]\n\t\t\tdisparity_scale_factor = tf.cast(tf.shape(left)[2],tf.float32)/tf.cast(tf.shape(current_disp)[2],tf.float32)\n\t\t\tresized_disp = preprocessing.resize_to_prediction(current_disp, left) * disparity_scale_factor\n\n\t\t\treprojected_left = preprocessing.warp_image(right, resized_disp)\n\t\t\tpartial_loss = base_loss_function(reprojected_left,left)\n\t\t\tif logs:\n\t\t\t\ttf.summary.scalar(\'Loss_resolution_{}\'.format(i),partial_loss)\n\t\t\taccumulator.append(weights[i]*partial_loss)\n\t\tif reduced:\n\t\t\treturn tf.reduce_sum(accumulator)\n\t\telse:\n\t\t\treturn accumulator\n\treturn compute_loss\n'"
Nets/DispNet.py,15,"b'import tensorflow as tf\n\nfrom Nets import Stereo_net\nfrom Nets import sharedLayers\nfrom Data_utils import preprocessing\n\nMAX_DISP = 40\n\nclass DispNet(Stereo_net.StereoNet):\n    _valid_args = [\n        (""left_img"", ""meta op for left image batch""),\n        (""right_img"", ""meta op for right image batch""),\n        (""correlation"", ""flag to enable the use of the correlation layer"")\n    ] + Stereo_net.StereoNet._valid_args\n    _netName = ""Dispnet""\n\n    def __init__(self, **kwargs):\n        """"""\n        Creation of a DispNet CNN\n        """"""\n        super(DispNet, self).__init__(**kwargs)\n\n    def _validate_args(self, args):\n        """"""\n        Check that args contains everything that is needed\n        Valid Keys for args:\n            left_img: left image op\n            right_img: right image op\n            correlation: boolean, if True use correlation layer, defaults to True\n        """"""\n        super(DispNet, self)._validate_args(args)\n        if (""left_img"" not in args) or (""right_img"" not in args):\n            raise Exception(\'Missing input op for left and right images\')\n        if ""correlation"" not in args:\n            print(\'WARNING: Correlation unspecified, setting to True\')\n            args[\'correlation\'] = True\n        return args\n\n    def _make_disp(self,op):\n        scale = self._left_input_batch.get_shape()[2].value/op.get_shape()[2].value\n        op = tf.image.resize_images(tf.nn.relu(op*scale), [self._left_input_batch.get_shape()[1].value, self._left_input_batch.get_shape()[2].value])\n        op = tf.image.resize_image_with_crop_or_pad(op, self._restore_shape[0], self._restore_shape[1])\n        return op\n\n    def _upsampling_block(self, bottom, skip_connection, input_channels, output_channels, skip_input_channels, name=\'upsample\', reuse=False):\n        with tf.variable_scope(name, reuse=reuse):\n            self._add_to_layers(name + \'/deconv\', sharedLayers.conv2d_transpose(\n                bottom, [4, 4, output_channels, input_channels], strides=2, name=\'deconv\'))\n            self._add_to_layers(name + \'/predict\', sharedLayers.conv2d(bottom, [\n                                3, 3, input_channels, 1], strides=1, activation=lambda x: x, name=\'predict\'))\n            self._disparities.append(self._make_disp(self._layers[name + \'/predict\']))\n            self._add_to_layers(name + \'/up_predict\', sharedLayers.conv2d_transpose(self._get_layer_as_input(\n                name + \'/predict\'), [4, 4, 1, 1], strides=2, activation=lambda x: x, name=\'up_predict\'))\n            with tf.variable_scope(\'join_skip\'):    \n                concat_inputs = tf.concat([skip_connection, self._get_layer_as_input(name + \'/deconv\'), self._get_layer_as_input(name + \'/up_predict\')], axis=3)\n            self._add_to_layers(name + \'/concat\', sharedLayers.conv2d(concat_inputs, [\n                                3, 3, output_channels + skip_input_channels + 1, output_channels], strides=1, activation=lambda x: x, name=\'concat\'))\n\n    def _preprocess_inputs(self, args):\n        self._left_input_batch = args[\'left_img\']\n        self._restore_shape = tf.shape(args[\'left_img\'])[1:3]\n        self._left_input_batch = tf.cast(\n            self._left_input_batch, dtype=tf.float32) / 255.0\n        self._left_input_batch = self._left_input_batch - (100.0 / 255)\n        self._left_input_batch = preprocessing.pad_image(\n            self._left_input_batch, 64, dynamic=True)\n\n        self._right_input_batch = args[\'right_img\']\n        self._right_input_batch = tf.cast(\n            self._right_input_batch, dtype=tf.float32) / 255.0\n        self._right_input_batch = self._right_input_batch - (100.0 / 255)\n        self._right_input_batch = preprocessing.pad_image(\n            self._right_input_batch, 64, dynamic=True)\n\n    def _build_network(self, args):\n        if args[\'correlation\']:\n            self._add_to_layers(\'conv1a\', sharedLayers.conv2d(\n                self._left_input_batch, [7, 7, 3, 64], strides=2, name=\'conv1\'))\n            self._add_to_layers(\'conv1b\', sharedLayers.conv2d(self._right_input_batch, [\n                                7, 7, 3, 64], strides=2, name=\'conv1\', reuse=True))\n\n            self._add_to_layers(\'conv2a\', sharedLayers.conv2d(\n                self._get_layer_as_input(\'conv1a\'), [5, 5, 64, 128], strides=2, name=\'conv2\'))\n            self._add_to_layers(\'conv2b\', sharedLayers.conv2d(self._get_layer_as_input(\n                \'conv1b\'), [5, 5, 64, 128], strides=2, name=\'conv2\', reuse=True))\n\n            self._add_to_layers(\'conv_redir\', sharedLayers.conv2d(self._get_layer_as_input(\n                \'conv2a\'), [1, 1, 128, 64], strides=1, name=\'conv_redir\'))\n            self._add_to_layers(\'corr\', sharedLayers.correlation(self._get_layer_as_input(\n                \'conv2a\'), self._get_layer_as_input(\'conv2b\'), max_disp=MAX_DISP))\n\n            self._add_to_layers(\'conv3\', sharedLayers.conv2d(tf.concat([self._get_layer_as_input(\'corr\'), self._get_layer_as_input(\n                \'conv_redir\')], axis=3), [5, 5, MAX_DISP * 2 + 1 + 64, 256], strides=2, name=\'conv3\'))\n        else:\n            concat_inputs = tf.concat(\n                [self._left_img_batch, self._right_input_batch], axis=-1)\n            self._add_to_layers(\'conv1\', sharedLayers.conv2d(\n                concat_inputs, [7, 7, 6, 64], strides=2, name=\'conv1\'))\n            self._add_to_layers(\'conv2\', sharedLayers.conv2d(\n                self._get_layer_as_input(\'conv1\'), [5, 5, 64, 128], strides=2, name=\'conv2\'))\n            self._add_to_layers(\'conv3\', sharedLayers.conv2d(\n                self._get_layer_as_input(\'conv2\'), [5, 5, 128, 256], strides=2, name=\'conv3\'))\n\n        self._add_to_layers(\'conv3/1\', sharedLayers.conv2d(\n            self._get_layer_as_input(\'conv3\'), [3, 3, 256, 256], strides=1, name=\'conv3/1\'))\n        self._add_to_layers(\'conv4\', sharedLayers.conv2d(self._get_layer_as_input(\n            \'conv3/1\'), [3, 3, 256, 512], strides=2, name=\'conv4\'))\n        self._add_to_layers(\'conv4/1\', sharedLayers.conv2d(\n            self._get_layer_as_input(\'conv4\'), [3, 3, 512, 512], strides=1, name=\'conv4/1\'))\n        self._add_to_layers(\'conv5\', sharedLayers.conv2d(self._get_layer_as_input(\n            \'conv4/1\'), [3, 3, 512, 512], strides=2, name=\'conv5\'))\n        self._add_to_layers(\'conv5/1\', sharedLayers.conv2d(\n            self._get_layer_as_input(\'conv5\'), [3, 3, 512, 512], strides=1, name=\'conv5/1\'))\n        self._add_to_layers(\'conv6\', sharedLayers.conv2d(self._get_layer_as_input(\n            \'conv5/1\'), [3, 3, 512, 1024], strides=2, name=\'conv6\'))\n        self._add_to_layers(\'conv6/1\', sharedLayers.conv2d(self._get_layer_as_input(\n            \'conv6\'), [3, 3, 1024, 1024], strides=1, name=\'conv6/1\'))\n\n        self._upsampling_block(self._get_layer_as_input(\n            \'conv6/1\'), self._get_layer_as_input(\'conv5/1\'), 1024, 512, 512, name=\'up5\')\n\n        self._upsampling_block(self._get_layer_as_input(\n            \'up5/concat\'), self._get_layer_as_input(\'conv4/1\'), 512, 256, 512, name=\'up4\')\n\n        self._upsampling_block(self._get_layer_as_input(\n            \'up4/concat\'), self._get_layer_as_input(\'conv3/1\'), 256, 128, 256, name=\'up3\')\n\n        if args[\'correlation\']:\n            self._upsampling_block(self._get_layer_as_input(\n                \'up3/concat\'), self._get_layer_as_input(\'conv2a\'), 128, 64, 128, name=\'up2\')\n        else:\n            self._upsampling_block(self._get_layer_as_input(\n                \'up3/concat\'), self._get_layer_as_input(\'conv2\'), 128, 64, 128, name=\'up2\')\n\n        if args[\'correlation\']:\n            self._upsampling_block(self._get_layer_as_input(\n                \'up2/concat\'), self._get_layer_as_input(\'conv1a\'), 64, 32, 64, name=\'up1\')\n        else:\n            self._upsampling_block(self._get_layer_as_input(\n                \'up2/concat\'), self._get_layer_as_input(\'conv1\'), 64, 32, 64, name=\'up1\')\n\n        self._add_to_layers(\'prediction\', sharedLayers.conv2d(self._get_layer_as_input(\n            \'up1/concat\'), [3, 3, 32, 1], strides=1, activation=lambda x: x, name=\'prediction\'))\n        self._disparities.append(self._make_disp(self._layers[\'prediction\']))\n\n         ############ LOOK BELOW IF DISPNET GIVES WRONG RESULTS #########################\n        # rescaled_prediction = -preprocessing.rescale_image(self._layers[\'prediction\'], tf.shape(self._left_input_batch)[1:3])\n        ################################################################################\n        rescaled_prediction = tf.image.resize_images(self._layers[\'prediction\'], tf.shape(self._left_input_batch)[1:3]) * 2\n        \n        self._layers[\'rescaled_prediction\'] = tf.image.resize_image_with_crop_or_pad(rescaled_prediction, self._restore_shape[0], self._restore_shape[1])\n        self._disparities.append(self._layers[\'rescaled_prediction\'])\n'"
Nets/MadNet.py,61,"b'import tensorflow as tf\nimport numpy as np\n\nfrom Nets import Stereo_net\nfrom Nets import sharedLayers\nfrom Data_utils import preprocessing\n\nclass MadNet(Stereo_net.StereoNet):\n    _valid_args = [\n        (""left_img"", ""meta op for left image batch""),\n        (""right_img"", ""meta op for right image batch""),\n        (""warping"", ""flag to enable warping""),\n        (""context_net"", ""flag to enable context_net""),\n        (""radius_d"", ""size f the patch using for correlation""),\n        (""stride"", ""stride used for correlation""),\n        (""bulkhead"", ""flag to stop gradient propagation among different resolution"")\n    ] + Stereo_net.StereoNet._valid_args\n    _netName = ""MADNet""\n\n    def __init__(self, **kwargs):\n        """"""\n        Creation of a MadNet for stereo prediction\n        """"""\n        super(MadNet, self).__init__(**kwargs)\n\n    def _validate_args(self, args):\n        """"""\n        Check that args contains everything that is needed\n        Valid Keys for args:\n            left_img: left image op\n            right_img: right image op\n            warping: boolean to enable or disable warping\n            context_net: boolean to enable or disable context_net\n            radius_d: kernel side used for computing correlation map\n            stride: stride used to compute correlation map\n        """"""\n        super(MadNet, self)._validate_args(args)\n        if (\'left_img\' not in args) or (\'right_img\' not in args):\n            raise Exception(\'Missing input op for left and right images\')\n        if \'warping\' not in args:\n            print(\'WARNING: warping flag not setted, setting default True value\')\n            args[\'warping\'] = True\n        if \'context_net\' not in args:\n            print(\'WARNING: context_net flag not setted, setting default True value\')\n            args[\'context_net\'] = True\n        if \'radius_d\' not in args:\n            print(\'WARNING: radius_d not setted, setting default value 2\')\n            args[\'radius_d\'] = 2\n        if \'stride\' not in args:\n            print(\'WARNING: stride not setted, setting default value 1\')\n            args[\'stride\'] = 1\n        if \'bulkhead\' not in args:\n            args[\'bulkhead\']=False\n        return args\n\n    def _preprocess_inputs(self, args):\n        self._left_input_batch = args[\'left_img\']\n        self._restore_shape = tf.shape(args[\'left_img\'])[1:3]\n        self._left_input_batch = tf.cast(self._left_input_batch, tf.float32)\n        self._left_input_batch = preprocessing.pad_image(\n            self._left_input_batch, 64)\n\n        self._right_input_batch = args[\'right_img\']\n        self._right_input_batch = tf.cast(self._right_input_batch, tf.float32)\n        self._right_input_batch = preprocessing.pad_image(\n            self._right_input_batch, 64)\n    \n    def _make_disp(self,op,scale):\n        op = tf.image.resize_images(tf.nn.relu(op * -20), [self._left_input_batch.get_shape()[1].value, self._left_input_batch.get_shape()[2].value]) \n        op = tf.image.resize_image_with_crop_or_pad(op, self._restore_shape[0], self._restore_shape[1])\n        return op\n\n    def _stereo_estimator(self, costs, upsampled_disp=None, scope=\'fgc-volume-filtering\'):\n        activation = self._leaky_relu()\n        with tf.variable_scope(scope):\n            # create initial cost volume\n            if upsampled_disp is not None:\n                volume = tf.concat([costs, upsampled_disp], -1)\n            else:\n                volume = costs\n\n            names = []\n\n            # disp-1\n            names.append(\'{}/disp1\'.format(scope))\n            input_layer = volume\n            self._add_to_layers(names[-1], sharedLayers.conv2d(input_layer, [3, 3, volume.get_shape(\n            ).as_list()[3], 128], name=\'disp-1\', bName=\'biases\', activation=activation))\n\n            # disp-2:\n            names.append(\'{}/disp2\'.format(scope))\n            input_layer = self._get_layer_as_input(names[-2])\n            self._add_to_layers(names[-1], sharedLayers.conv2d(input_layer, [\n                                3, 3, 128, 128], name=\'disp-2\', bName=\'biases\', activation=activation))\n\n            # disp-3\n            names.append(\'{}/disp3\'.format(scope))\n            input_layer = self._get_layer_as_input(names[-2])\n            self._add_to_layers(names[-1], sharedLayers.conv2d(input_layer, [\n                                3, 3, 128, 96], name=\'disp-3\', bName=\'biases\', activation=activation))\n\n            # disp-4\n            names.append(\'{}/disp4\'.format(scope))\n            input_layer = self._get_layer_as_input(names[-2])\n            self._add_to_layers(names[-1], sharedLayers.conv2d(input_layer, [\n                                3, 3, 96, 64], name=\'disp-4\', bName=\'biases\', activation=activation))\n\n            # disp-5\n            names.append(\'{}/disp5\'.format(scope))\n            input_layer = self._get_layer_as_input(names[-2])\n            self._add_to_layers(names[-1], sharedLayers.conv2d(input_layer, [\n                                3, 3, 64, 32], name=\'disp-5\', bName=\'biases\', activation=activation))\n\n            # disp-6\n            names.append(\'{}/disp6\'.format(scope))\n            input_layer = self._get_layer_as_input(names[-2])\n            self._add_to_layers(names[-1], sharedLayers.conv2d(input_layer, [\n                                3, 3, 32, 1], name=\'disp-6\', bName=\'biases\', activation=lambda x: x))\n\n            return self._get_layer_as_input(names[-1])\n\n    def _stereo_context_net(self, input, disp):\n        volume = tf.concat([input, disp], -1)\n        att = self._leaky_relu()\n        names = []\n        # context-1\n        names.append(\'context1\')\n        input_layer = volume\n        self._add_to_layers(names[-1], sharedLayers.dilated_conv2d(input_layer, [\n                            3, 3, input_layer.get_shape().as_list()[-1], 128], name=\'context-1\', rate=1, activation=att))\n\n        # context-2\n        names.append(\'context2\')\n        input_layer = self._get_layer_as_input(names[-2])\n        self._add_to_layers(names[-1], sharedLayers.dilated_conv2d(\n            input_layer, [3, 3, 128, 128], name=\'context-2\', rate=2, activation=att))\n\n        # context-3\n        names.append(\'context3\')\n        input_layer = self._get_layer_as_input(names[-2])\n        self._add_to_layers(names[-1], sharedLayers.dilated_conv2d(\n            input_layer, [3, 3, 128, 128], name=\'context-3\', rate=4, activation=att))\n\n        # context-4\n        names.append(\'context4\')\n        input_layer = self._get_layer_as_input(names[-2])\n        self._add_to_layers(names[-1], sharedLayers.dilated_conv2d(\n            input_layer, [3, 3, 128, 96], name=\'context-4\', rate=8, activation=att))\n\n        # context-5\n        names.append(\'context5\')\n        input_layer = self._get_layer_as_input(names[-2])\n        self._add_to_layers(names[-1], sharedLayers.dilated_conv2d(\n            input_layer, [3, 3, 96, 64], name=\'context-5\', rate=16, activation=att))\n\n        # context-6\n        names.append(\'context6\')\n        input_layer = self._get_layer_as_input(names[-2])\n        self._add_to_layers(names[-1], sharedLayers.dilated_conv2d(\n            input_layer, [3, 3, 64, 32], name=\'context-6\', rate=1, activation=att))\n\n        # context-7\n        names.append(\'context7\')\n        input_layer = self._get_layer_as_input(names[-2])\n        self._add_to_layers(names[-1], sharedLayers.dilated_conv2d(\n            input_layer, [3, 3, 32, 1], name=\'context-7\', rate=1, activation=lambda x: x))\n\n        final_disp = disp + self._get_layer_as_input(names[-1])\n        self._add_to_layers(\'final_disp\', final_disp)\n        \n        return final_disp\n\n    def _pyramid_features(self, input_batch, scope=\'pyramid\', reuse=False, layer_prefix=\'pyramid\'):\n        with tf.variable_scope(scope, reuse=reuse):\n\n            names = []\n            activation = self._leaky_relu()\n\n            # conv1\n            names.append(\'{}/conv1\'.format(layer_prefix))\n            input_layer = input_batch\n            self._add_to_layers(names[-1], sharedLayers.conv2d(input_layer, [3, 3, input_batch.get_shape(\n            )[-1].value, 16], strides=2, name=\'conv1\', bName=\'biases\', activation=activation))\n\n            # conv2\n            names.append(\'{}/conv2\'.format(layer_prefix))\n            input_layer = self._get_layer_as_input(names[-2])\n            self._add_to_layers(names[-1], sharedLayers.conv2d(input_layer, [\n                                3, 3, 16, 16], strides=1, name=\'conv2\', bName=\'biases\', activation=activation))\n\n            # conv3\n            names.append(\'{}/conv3\'.format(layer_prefix))\n            input_layer = self._get_layer_as_input(names[-2])\n            self._add_to_layers(names[-1], sharedLayers.conv2d(input_layer, [\n                                3, 3, 16, 32], strides=2, name=\'conv3\', bName=\'biases\', activation=activation))\n\n            # conv4\n            names.append(\'{}/conv4\'.format(layer_prefix))\n            input_layer = self._get_layer_as_input(names[-2])\n            self._add_to_layers(names[-1], sharedLayers.conv2d(input_layer, [\n                                3, 3, 32, 32], strides=1, name=\'conv4\', bName=\'biases\', activation=activation))\n\n            # conv5\n            names.append(\'{}/conv5\'.format(layer_prefix))\n            input_layer = self._get_layer_as_input(names[-2])\n            self._add_to_layers(names[-1], sharedLayers.conv2d(input_layer, [\n                                3, 3, 32, 64], strides=2, name=\'conv5\', bName=\'biases\', activation=activation))\n\n            # conv6\n            names.append(\'{}/conv6\'.format(layer_prefix))\n            input_layer = self._get_layer_as_input(names[-2])\n            self._add_to_layers(names[-1], sharedLayers.conv2d(input_layer, [\n                                3, 3, 64, 64], strides=1, name=\'conv6\', bName=\'biases\', activation=activation))\n\n            # conv7\n            names.append(\'{}/conv7\'.format(layer_prefix))\n            input_layer = self._get_layer_as_input(names[-2])\n            self._add_to_layers(names[-1], sharedLayers.conv2d(input_layer, [\n                                3, 3, 64, 96], strides=2, name=\'conv7\', bName=\'biases\', activation=activation))\n\n            # conv8\n            names.append(\'{}/conv8\'.format(layer_prefix))\n            input_layer = self._get_layer_as_input(names[-2])\n            self._add_to_layers(names[-1], sharedLayers.conv2d(input_layer, [\n                                3, 3, 96, 96], strides=1, name=\'conv8\', bName=\'biases\', activation=activation))\n\n            # conv9\n            names.append(\'{}/conv9\'.format(layer_prefix))\n            input_layer = self._get_layer_as_input(names[-2])\n            self._add_to_layers(names[-1], sharedLayers.conv2d(input_layer, [\n                                3, 3, 96, 128], strides=2, name=\'conv9\', bName=\'biases\', activation=activation))\n\n            # conv10\n            names.append(\'{}/conv10\'.format(layer_prefix))\n            input_layer = self._get_layer_as_input(names[-2])\n            self._add_to_layers(names[-1], sharedLayers.conv2d(input_layer, [\n                                3, 3, 128, 128], strides=1, name=\'conv10\', bName=\'biases\', activation=activation))\n\n            # conv11\n            names.append(\'{}/conv11\'.format(layer_prefix))\n            input_layer = self._get_layer_as_input(names[-2])\n            self._add_to_layers(names[-1], sharedLayers.conv2d(input_layer, [\n                                3, 3, 128, 192], strides=2, name=\'conv11\', bName=\'biases\', activation=activation))\n\n            # conv12\n            names.append(\'{}/conv12\'.format(layer_prefix))\n            input_layer = self._get_layer_as_input(names[-2])\n            self._add_to_layers(names[-1], sharedLayers.conv2d(input_layer, [\n                                3, 3, 192, 192], strides=1, name=\'conv12\', bName=\'biases\', activation=activation))\n\n    def _build_network(self, args):\n        image_height = self._left_input_batch.get_shape()[1].value\n        image_width = self._left_input_batch.get_shape()[2].value\n        scales = [1, 2, 4, 8, 16, 32, 64]\n\n        #######################PYRAMID FEATURES###############################\n        # setup pyramid features left\n        self._pyramid_features(self._left_input_batch,scope=\'gc-read-pyramid\', layer_prefix=\'left\')\n        # setup pyramid features right\n        self._pyramid_features(self._right_input_batch, scope=\'gc-read-pyramid\', reuse=True, layer_prefix=\'right\')\n\n        #############################SCALE 6#################################\n        with tf.variable_scope(""G6""):\n            with tf.variable_scope(""unary-6""):\n                left_0_sample_6 = self._get_layer_as_input(\'left/conv12\')\n                right_0_sample_6 = self._get_layer_as_input(\'right/conv12\')\n\n            with tf.variable_scope(""fgc-volume-creator-6""):\n                dsi_6 = self._stereo_cost_volume_correlation(left_0_sample_6, right_0_sample_6, args[\'radius_d\'], args[\'stride\'])\n\n            V6 = self._stereo_estimator(dsi_6, scope=""fgc-volume-filtering-6"")\n            real_disp_v6 =self._make_disp(V6,scales[6])\n            self._disparities.append(real_disp_v6)\n            u5 = tf.image.resize_images(V6, [image_height // scales[5], image_width // scales[5]]) * 20. / scales[5]\n            if args[\'bulkhead\']:\n                u5=tf.stop_gradient(u5)\n\n        ############################SCALE 5###################################\n        with tf.variable_scope(""G5""):\n            with tf.variable_scope(""unary-5""):\n                left_0_sample_5 = self._get_layer_as_input(\'left/conv10\')\n                if args[\'warping\']:\n                    right_0_sample_5 = self._linear_warping(self._get_layer_as_input(\'right/conv10\'), self._build_indeces(tf.concat([u5, tf.zeros_like(u5)], -1)))\n                else:\n                    right_0_sample_5 = self._get_layer_as_input(\'right/conv10\')\n\n            with tf.variable_scope(""fgc-volume-creator-5""):\n                dsi_5 = self._stereo_cost_volume_correlation(left_0_sample_5, right_0_sample_5, args[\'radius_d\'], args[\'stride\'])\n\n            V5 = self._stereo_estimator(dsi_5, upsampled_disp=u5, scope=""fgc-volume-filtering-5"")\n            real_disp_v5 =self._make_disp(V5,scales[5])\n            self._disparities.append(real_disp_v5)\n            u4 = tf.image.resize_images(V5, [image_height // scales[4], image_width // scales[4]]) * 20. / scales[4]\n            if args[\'bulkhead\']:\n                u4=tf.stop_gradient(u4)\n\n        ############################SCALE 4###################################\n        with tf.variable_scope(\'G4\'):\n            with tf.variable_scope(\'unary-4\'):\n                left_0_sample_4 = self._get_layer_as_input(\'left/conv8\')\n                if args[\'warping\']:\n                    right_0_sample_4 = self._linear_warping(self._get_layer_as_input(\'right/conv8\'), self._build_indeces(tf.concat([u4, tf.zeros_like(u4)], -1)))\n                else:\n                    right_0_sample_4 = self._get_layer_as_input(\'right/conv8\')\n\n            with tf.variable_scope(""fgc-volume-creator-4""):\n                dsi_4 = self._stereo_cost_volume_correlation(left_0_sample_4, right_0_sample_4, args[\'radius_d\'], args[\'stride\'])\n\n            V4 = self._stereo_estimator(dsi_4, upsampled_disp=u4, scope=""fgc-volume-filtering-4"")\n            real_disp_v4 =self._make_disp(V4,scales[4])\n            self._disparities.append(real_disp_v4)\n            u3 = tf.image.resize_images(V4, [image_height // scales[3], image_width // scales[3]]) * 20. / scales[3]\n            if args[\'bulkhead\']:\n                u3=tf.stop_gradient(u3)\n\n        ############################SCALE 3###################################\n        with tf.variable_scope(\'G3\'):\n            with tf.variable_scope(\'unary-3\'):\n                left_0_sample_3 = self._get_layer_as_input(\'left/conv6\')\n                if args[\'warping\']:\n                    right_0_sample_3 = self._linear_warping(self._get_layer_as_input(\'right/conv6\'), self._build_indeces(tf.concat([u3, tf.zeros_like(u3)], -1)))\n                else:\n                    right_0_sample_3 = self._get_layer_as_input(\'right/conv6\')\n\n            with tf.variable_scope(""fgc-volume-creator-3""):\n                dsi_3 = self._stereo_cost_volume_correlation(left_0_sample_3, right_0_sample_3, args[\'radius_d\'], args[\'stride\'])\n\n            V3 = self._stereo_estimator(dsi_3, upsampled_disp=u3, scope=""fgc-volume-filtering-3"")\n            real_disp_v3 =self._make_disp(V3,scales[3])\n            self._disparities.append(real_disp_v3)\n            u2 = tf.image.resize_images(V3, [image_height // scales[2], image_width // scales[2]]) * 20. / scales[2]\n            if args[\'bulkhead\']:\n                u2=tf.stop_gradient(u2)\n\n        #################################SCALE 2###############################\n        with tf.variable_scope(\'G2\'):\n            with tf.variable_scope(\'unary-2\'):\n                left_0_sample_2 = self._get_layer_as_input(\'left/conv4\')\n                if args[\'warping\']:\n                    right_0_sample_2 = self._linear_warping(self._get_layer_as_input(\'right/conv4\'), self._build_indeces(tf.concat([u2, tf.zeros_like(u2)], -1)))\n                else:\n                    right_0_sample_2 = self._get_layer_as_input(\'right/conv4\')\n\n            with tf.variable_scope(""fgc-volume-creator-2""):\n                dsi_2 = self._stereo_cost_volume_correlation(left_0_sample_2, right_0_sample_2, args[\'radius_d\'], args[\'stride\'])\n\n            self._stereo_estimator(dsi_2, upsampled_disp=u2, scope=""fgc-volume-filtering-2"")\n            V2_init = self._get_layer_as_input(\'fgc-volume-filtering-2/disp6\')\n            ##################################################\n            #real_disp_v2 = self._make_disp(V2_init,scales[2])\n            #self._disparities.append(real_disp_v2)\n\n        if args[\'context_net\']:\n            V2 = self._stereo_context_net(left_0_sample_2, V2_init)\n            real_disp_v2_context = self._make_disp(V2,scales[2])\n            self._disparities.append(real_disp_v2_context)\n        else:\n            V2 = V2_init\n            self._add_to_layers(\'final_disp\', V2)\n            self._disparities.append(real_disp_v2)\n\n        rescaled_prediction = tf.nn.relu(tf.image.resize_images(self._get_layer_as_input(\'final_disp\'), [image_height, image_width]) * -20.)\n        self._layers[\'rescaled_prediction\'] = tf.image.resize_image_with_crop_or_pad(rescaled_prediction, self._restore_shape[0], self._restore_shape[1])\n        self._disparities.append(self._layers[\'rescaled_prediction\'])\n    \n    def _leaky_relu(self):\n        return lambda x: tf.maximum(0.2 * x, x)\n\n    # Utility functions\n    def _stereo_cost_volume_correlation(self,reference, target, radius_x, stride=1):\n\n        cost_curve = sharedLayers.correlation(reference,target,radius_x,stride=stride)\n        cost_curve = tf.concat([reference, cost_curve], axis=3)\n\n        return cost_curve\n\n\n    def _build_indeces(self,coords):\n        batches = coords.get_shape().as_list()[0]\n\n        height = coords.get_shape().as_list()[1]\n        width = coords.get_shape().as_list()[2]\n        pixel_coords = np.ones((1, height, width, 2))\n        batches_coords = np.ones((batches, height, width, 1))\n\n        for i in range(0, batches):\n            batches_coords[i][:][:][:] = i\n        # build pixel coordinates and their disparity\n        for i in range(0, height):\n            for j in range(0, width):\n                pixel_coords[0][i][j][0] = j\n                pixel_coords[0][i][j][1] = i\n\n        pixel_coords = tf.constant(pixel_coords, tf.float32)\n        output = tf.concat([batches_coords, pixel_coords + coords], -1)\n\n        return output\n\n\n    def _linear_warping(self,imgs, coords):\n        shape = coords.get_shape().as_list()\n\n        #coords = tf.reshape(coords, [shape[1], shape[2], shape[0], shape[3]])\n        coord_b, coords_x, coords_y = tf.split(coords, [1, 1, 1], axis=3)\n\n        coords_x = tf.cast(coords_x, \'float32\')\n        coords_y = tf.cast(coords_y, \'float32\')\n\n        x0 = tf.floor(coords_x)\n        x1 = x0 + 1\n        y0 = tf.floor(coords_y)\n\n        y_max = tf.cast(tf.shape(imgs)[1] - 1, \'float32\')\n        x_max = tf.cast(tf.shape(imgs)[2] - 1, \'float32\')\n        zero = tf.zeros([1],dtype=tf.float32)\n\n        x0_safe = tf.clip_by_value(x0, zero[0], x_max)\n        y0_safe = tf.clip_by_value(y0, zero[0], y_max)\n        x1_safe = tf.clip_by_value(x1, zero[0], x_max)\n\n        # bilinear interp weights, with points outside the grid having weight 0\n        wt_x0 = (x1 - coords_x) * tf.cast(tf.equal(x0, x0_safe), \'float32\')\n        wt_x1 = (coords_x - x0) * tf.cast(tf.equal(x1, x1_safe), \'float32\')\n\n        # print(x0_safe.get_shape().as_list())\n\n        im00 = tf.cast(tf.gather_nd(imgs, tf.cast(\n            tf.concat([coord_b, y0_safe, x0_safe], -1), \'int32\')), \'float32\')\n        im01 = tf.cast(tf.gather_nd(imgs, tf.cast(\n            tf.concat([coord_b, y0_safe, x1_safe], -1), \'int32\')), \'float32\')\n\n        output = tf.add_n([\n            wt_x0 * im00, wt_x1 * im01\n        ])\n\n        return output\n'"
Nets/Stereo_net.py,4,"b'import tensorflow as tf\nimport abc\nfrom collections import OrderedDict\n\n\nclass StereoNet(object):\n    __metaclass__ = abc.ABCMeta\n    """"""\n    Meta parent class for all the convnets\n    """"""\n    #=======================Static Class Fields=============\n    _valid_args = [\n        (""split_layer"", ""name of the layer where the network will be splitted""),\n        (""sequence"", ""flag to use network on a video sequence instead of on single images""),\n        (""train_portion"", ""one among \'BEGIN\' or \'END\' specify which portion of the network will be trained, respectivelly before and after split""),\n        (""is_training"", ""boolean or placeholder to specify if the network is in train or inference mode"")\n    ]\n    _netName=""stereoNet""\n    #=====================Static Class Methods==============\n\n    @classmethod\n    def getPossibleArsg(cls):\n        return cls._valid_args\n\n    #==================PRIVATE METHODS======================\n    def __init__(self, **kwargs):\n        self._layers = OrderedDict()\n        self._disparities = []\n        self._placeholders = []\n        self._placeholderable = []\n        self._trainable_variables = OrderedDict() \n        self._layer_to_var = {}\n        self._after_split = False\n        print(\'=\' * 50)\n        print(\'Starting Creation of {}\'.format(self._netName))\n        print(\'=\' * 50)\n\n        args = self._validate_args(kwargs)\n        print(\'Args Validated, setting up graph\')\n\n        self._preprocess_inputs(args)\n        print(\'Meta op to preprocess data created\')\n\n        self._build_network(args)\n        print(\'Network ready\')\n        print(\'=\' * 50)\n\n    def _get_placeholder_name(self, name):\n        """"""\n        convert a layer name to its placehodler version and return it\n        """"""\n        return name + \'_placeholder\'\n\n    def _add_to_layers(self, name, op):\n        """"""\n        Add the layer to the network ones and check if name is among the layer where the network should split, if so create a placeholder and return it, otherways return the real layer. Add teh variables to the trainable colelction \n        Args:\n            name: name of the layer that need to be addded to the network collection\n            op: tensorflow op \n        """"""\n        self._layers[name] = op\n\n        # extract variables\n        scope = \'/\'.join(op.name.split(\'/\')[0:-1])\n        variables = tf.get_collection(\n            tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope) \n        self._layer_to_var[name] = variables\n\n        if not self._after_split:\n            # append layer name among those that can be turned into placeholder\n            self._placeholderable.append(name)\n        if self._after_split != self._train_beginning:  # XOR\n            # add variables in scope to the one that needs to be trained\n            for v in variables:\n                self._trainable_variables[v] = True\n\n        if name in self._split_layers_list:\n            # enable flag to mark the first layer after split\n            self._after_split = True\n\n    def _get_layer_as_input(self, name):\n        # Check if a placeholder for this layer already exist\n        if self._get_placeholder_name(name) in self._layers:\n            return self._layers[self._get_placeholder_name(name)]\n        # check if layer need to be transformed into a placeholder\n        elif self._after_split and (not self._sequence) and name in self._placeholderable:\n            real_op = self._layers[name]\n            placeholder_op = tf.placeholder(\n                tf.float32, shape=real_op.get_shape())\n            self._layers[self._get_placeholder_name(name)] = placeholder_op\n            self._placeholders.append((real_op, placeholder_op))\n            return self._layers[self._get_placeholder_name(name)]\n        # check if real layer exist\n        elif name in self._layers:\n            return self._layers[name]\n        else:\n            raise Exception(\'Trying to fetch an unknown layer!\')\n\n    def __str__(self):\n        """"""to string method""""""\n        ss = """"\n        for k, l in self._layers.items():\n            if l in self._disparities:\n                ss += ""Prediction Layer {}: {}\\n"".format(k, str(l.shape))\n            else:\n                ss += ""Layer {}: {}\\n"".format(k, str(l.shape))\n        return ss\n\n    def __repr__(self):\n        """"""to string method""""""\n        return self.__str__()\n\n    def __getitem__(self, key):\n        """"""\n        Returns a layer by name\n        """"""\n        return self._layers[key]\n\n    #========================ABSTRACT METHODs============================\n    @abc.abstractmethod\n    def _preprocess_inputs(self, args):\n        """"""\n        Abstract method to create metaop that preprocess data before feeding them in the network\n        """"""\n        pass\n\n    @abc.abstractmethod\n    def _build_network(self, args):\n        """"""\n        Should build the elaboration graph\n        """"""\n        pass\n\n    @abc.abstractmethod\n    def _validate_args(self, args):\n        """"""\n        Should validate the argument and add default values\n        """"""\n        portion_options = [\'BEGIN\', \'END\']\n        # Check common args\n        if \'split_layers\' not in args:\n            print(\n                \'WARNING: no split points selected, the network will flow without interruption\')\n            args[\'split_layers\'] = [None]\n        if \'train_portion\' not in args:\n            print(\'WARNING: train_portion not specified, using default END\')\n            args[\'train_portion\'] = \'END\' if args[\'split_layers\'] != [\n                None] else \'BEGIN\'\n        elif args[\'train_portion\'] not in portion_options:\n            raise Exception(\'Invalid portion options {}\'.format(\n                args[\'train_portion\']))\n        if \'sequence\' not in args:\n            print(\'WARNING: sequence flag not setted, configuring the network for single image adaptation\')\n            args[\'sequence\'] = False\n        if \'is_training\' not in args:\n            print(\'WARNING: flag for trainign not setted, using default False\')\n            args[\'is_training\']=False\n\n        # save args value\n        self._split_layers_list = args[\'split_layers\']\n        self._train_beginning = (args[\'train_portion\'] == \'BEGIN\')\n        self._sequence = args[\'sequence\']\n        self._isTraining=False\n\n    #==============================PUBLIC METHODS==================================\n    def get_placeholders(self):\n        """"""\n        Get all the placeholder defined internally in the network\n        Returns:\n            list of couples of layers that became placeholder, each couple is (real_layer,placeholder)\n        """"""\n        return self._placeholders\n\n    def get_placeholder(self, name):\n        """"""\n        Return the placeholder corresponding to the layer named name\n        Args:\n            name of the layer where there should be a placeholder\n        Returns:\n            placeholder for the layer\n        """"""\n        placeholder_name = self._get_placeholder_name(name)\n        if placeholder_name not in self._layers:\n            raise Exception(\n                \'Unable to find placeholder for layer {}\'.format(placeholder_name))\n        else:\n            return self._layers[placeholder_name]\n\n    def get_all_layers(self):\n        """"""\n        Returns all network layers\n        """"""\n        return self._layers\n    \n    def get_layers_names(self):\n        """"""\n        Returns all layers name\n        """"""\n        return self._layers.keys()\n\n    def get_disparities(self):\n        """"""\n        Return all the disparity predicted with increasing resolution\n        """"""\n        return self._disparities\n\n    def get_trainable_variables(self):\n        """"""\n        Returns the list of trainable variables\n        """"""\n        return list(self._trainable_variables.keys())\n\n    def get_variables(self, layer_name):\n        """"""\n        Returns the colelction of variables associated to layer_name\n        Args:\n        layer_name: name of the layer for which we want to access variables\n        """"""\n        if layer_name in self._layers and layer_name not in self._layer_to_var:\n            return []\n        else:\n            return self._layer_to_var[layer_name]\n'"
Nets/__init__.py,0,"b""import Nets.DispNet\nimport Nets.MadNet\n\nSTEREO_FACTORY = {\n    Nets.DispNet.DispNet._netName: Nets.DispNet.DispNet,\n    Nets.MadNet.MadNet._netName: Nets.MadNet.MadNet\n}\n\ndef get_stereo_net(name,args):\n    if name not in STEREO_FACTORY:\n        raise Exception('Unrecognized network name: {}'.format(name))\n    else:\n        return STEREO_FACTORY[name](**args)"""
Nets/sharedLayers.py,61,"b'import tensorflow as tf\nimport os\n\nINITIALIZER_CONV = tf.contrib.layers.xavier_initializer()\nINITIALIZER_BIAS = tf.constant_initializer(0.0)\nMODE = \'TF\'\n\n####################################################\n#### Uncomment lines below for cuda correlation ####\n####################################################\n\n#REPO_DIR = os.path.dirname(os.path.abspath(__file__))\n#shift_corr_module = tf.load_op_library(os.path.join(REPO_DIR, \'Native/shift_corr.so\'))\n\n#@tf.RegisterGradient(""ShiftCorr"")\n#def _ShiftCorrOpGrad(op, grad):\n#\treturn shift_corr_module.shift_corr_grad(op.inputs[0], op.inputs[1], grad, max_disp=op.get_attr(\'max_disp\'))\n\n# MODE=\'CUDA\'\n\n#######################################################\n\ndef correlation(x,y,max_disp, name=\'corr\', mode=MODE,stride=1):\n\tif mode == \'TF\':\n\t\treturn correlation_tf(x,y,max_disp,name=name,stride=stride)\n\telse:\n\t\tif stride!=1:\n\t\t\traise Exception(\'Cuda implementation cannot handle stride different than 1\')\n\t\treturn correlation_native(x,y,max_disp,name=name)\n\ndef correlation_native(x, y, max_disp, name=\'corr\'):\n\twith tf.variable_scope(name):\n\t\tinput_shape = x.get_shape().as_list()\n\t\tx = tf.pad(x, [[0, 0], [0, 0], [max_disp, max_disp], [0, 0]], ""CONSTANT"")\n\t\ty = tf.pad(y, [[0, 0], [0, 0], [max_disp, max_disp], [0, 0]], ""CONSTANT"")\n\t\tcorr = shift_corr_module.shift_corr(x, y, max_disp=max_disp)\n\t\tcorr = tf.transpose(corr, perm=[0, 2, 3, 1])\n\t\tcorr.set_shape([input_shape[0],input_shape[1],input_shape[2],2*max_disp+1])\n\t\treturn corr\n\ndef correlation_tf(x, y, max_disp, stride=1, name=\'corr\'):\n\twith tf.variable_scope(name):\n\t\tcorr_tensors = []\n\t\ty_shape = tf.shape(y)\n\t\ty_feature = tf.pad(y,[[0,0],[0,0],[max_disp,max_disp],[0,0]])\n\t\tfor i in range(-max_disp, max_disp+1,stride):\n\t\t\tshifted = tf.slice(y_feature, [0, 0, i + max_disp, 0], [-1, y_shape[1], y_shape[2], -1])\n\t\t\tcorr_tensors.append(tf.reduce_mean(shifted*x, axis=-1, keepdims=True))\n\n\t\tresult = tf.concat(corr_tensors,axis=-1)\n\t\treturn result\n\n\ndef conv2d(x, kernel_shape, strides=1, activation=lambda x: tf.maximum(0.1 * x, x), padding=\'SAME\', name=\'conv\', reuse=False, wName=\'weights\', bName=\'bias\', batch_norm=False, training=False):\n    with tf.variable_scope(name, reuse=reuse):\n        W = tf.get_variable(wName, kernel_shape, initializer=INITIALIZER_CONV)\n        b = tf.get_variable(bName, kernel_shape[3], initializer=INITIALIZER_BIAS)\n        x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding=padding)\n        x = tf.nn.bias_add(x, b)\n        if batch_norm:\n            x = tf.layers.batch_normalization(x,training=training,momentum=0.99)\n        x = activation(x)\n        return x\n\n\ndef dilated_conv2d(x, kernel_shape, rate=1, activation=lambda x: tf.maximum(0.1 * x, x), padding=\'SAME\', name=\'dilated_conv\', reuse=False, wName=\'weights\', bName=\'biases\',  batch_norm=False, training=False):\n    with tf.variable_scope(name, reuse=reuse):\n        weights = tf.get_variable(\n            wName, kernel_shape, initializer=INITIALIZER_CONV)\n        biases = tf.get_variable(\n            bName, kernel_shape[3], initializer=INITIALIZER_BIAS)\n        x = tf.nn.atrous_conv2d(x, weights, rate=rate, padding=padding)\n        x = tf.nn.bias_add(x, biases)\n        if batch_norm:\n            x = tf.layers.batch_normalization(x,training=training,momentum=0.99)\n        x = activation(x)\n        return x\n\n\ndef conv2d_transpose(x, kernel_shape, strides=1, activation=lambda x: tf.maximum(0.1 * x, x), name=\'conv\', reuse=False, wName=\'weights\', bName=\'bias\',  batch_norm=False, training=False):\n    with tf.variable_scope(name, reuse=reuse):\n        W = tf.get_variable(wName, kernel_shape,initializer=INITIALIZER_CONV)\n        tf.add_to_collection(tf.GraphKeys.WEIGHTS, W)\n        b = tf.get_variable(bName, kernel_shape[2], initializer=INITIALIZER_BIAS)\n        x_shape = tf.shape(x)\n        output_shape = [x_shape[0], x_shape[1] * strides,x_shape[2] * strides, kernel_shape[2]]\n        x = tf.nn.conv2d_transpose(x, W, output_shape, strides=[1, strides, strides, 1], padding=\'SAME\')\n        x = tf.nn.bias_add(x, b)\n        if batch_norm:\n            x = tf.layers.batch_normalization(x,training=training,momentum=0.99)\n        x = activation(x)\n        return x\n\ndef depthwise_conv(x, kernel_shape, strides=1, activation=lambda x: tf.maximum(0.1 * x, x), padding=\'SAME\', name=\'conv\', reuse=False, wName=\'weights\', bName=\'bias\', batch_norm=False, training=False):\n    with tf.variable_scope(name, reuse=reuse):\n        w = tf.get_variable(wName, kernel_shape, initializer=INITIALIZER_CONV)\n        b = tf.get_variable(bName, kernel_shape[3]*kernel_shape[2], initializer=INITIALIZER_BIAS)\n        x = tf.nn.depthwise_conv2d(x, w, strides=[1, strides, strides, 1], padding=padding)\n        x = tf.nn.bias_add(x, b)\n        if batch_norm:\n            x = tf.layers.batch_normalization(x, training=training,momentum=0.99)\n        x = activation(x)\n        return x\n\ndef separable_conv2d(x, kernel_shape, channel_multiplier=1, strides=1, activation=lambda x: tf.maximum(0.1 * x, x), padding=\'SAME\', name=\'conv\', reuse=False, wName=\'weights\', bName=\'bias\', batch_norm=True, training=False):\n    with tf.variable_scope(name, reuse=reuse):\n        #detpthwise conv\n        depthwise_conv_kernel = [kernel_shape[0],kernel_shape[1],kernel_shape[2],channel_multiplier]\n        x = depthwise_conv(x,depthwise_conv_kernel,strides=strides,activation=lambda x: tf.maximum(0.1 * x, x),padding=padding,name=\'depthwise_conv\',reuse=reuse,wName=wName,bName=bName,batch_norm=batch_norm, training=training)\n\n        #pointwise_conv\n        pointwise_conv_kernel = [1,1,x.get_shape()[-1].value,kernel_shape[-1]]\n        x = conv2d(x,pointwise_conv_kernel,strides=strides,activation=activation,padding=padding,name=\'pointwise_conv\',reuse=reuse,wName=wName,bName=bName,batch_norm=batch_norm, training=training)\n\n        return x\n\ndef grouped_conv2d(x, kernel_shape, num_groups=1, strides=1, activation=lambda x: tf.maximum(0.1 * x, x), padding=\'SAME\', name=\'conv\', reuse=False, wName=\'weights\', bName=\'bias\', batch_norm=True, training=False):\n    with tf.variable_scope(name,reuse=reuse):\n        w = tf.get_variable(wName,shape=kernel_shape,initializer=INITIALIZER_CONV)\n        b = tf.get_variable(bName, kernel_shape[3], initializer=INITIALIZER_BIAS)\n\n        input_groups = tf.split(x,num_or_size_splits=num_groups,axis=-1)\n        kernel_groups = tf.split(w, num_or_size_splits=num_groups, axis=2)\n        bias_group = tf.split(b,num_or_size_splits=num_groups,axis=-1)\n        output_groups = [tf.nn.conv2d(i, k,[1,strides,strides,1],padding=padding)+bb for i, k,bb in zip(input_groups, kernel_groups,bias_group)]\n\t\t# Concatenate the groups\n        x = tf.concat(output_groups,axis=3)\n        if batch_norm:\n            x = tf.layers.batch_normalization(x,training=training,momentum=0.99)\n        x = activation(x)  \n        return x\n\ndef channel_shuffle_inside_group(x, num_groups, name=\'shuffle\'):\n    with tf.variable_scope(name):\n        _, h, w, c = x.shape.as_list()\n        x_reshaped = tf.reshape(x, [-1, h, w, num_groups, c // num_groups])\n        x_transposed = tf.transpose(x_reshaped, [0, 1, 2, 4, 3])\n        output = tf.reshape(x_transposed, [-1, h, w, c])\n    return output\n'"
Sampler/__init__.py,0,b''
Sampler/sampler_factory.py,0,"b'import abc\nimport numpy as np\n\nclass meta_sampler():\n\t""""""Sampler for MAD adaptation""""""\n\t__metaclass__ = abc.ABCMeta\n\n\tdef __init__(self,blocks_to_fetch):\n\t\t""""""\n\t\tArgs:\n\t\t\tblocks_to_fetch: number of blocks to fetch for each call of the sample() function\n\t\t""""""\n\t\tself._blocks_to_fetch = blocks_to_fetch\n\t\n\t@abc.abstractmethod\n\tdef sample(self, distribution):\n\t\t""""""\n\t\tArgs:\n\t\t\tdistribution: perform sampling from this distribution.\n\t\t""""""\n\t\tpass\n\nclass fixed_sampler(meta_sampler):\n\t""""""\n\tReturn always the same fixed group, it does not perform sampling\n\t""""""\n\tdef __init__(self,blocks_to_fetch,fixed_id):\n\t\t""""""\n\t\tArgs:\n\t\t\tblocks_to_fetch: number of blocks to fetch for each call of the sample() function\n\t\t\tfixed_id: value to return when calling sample()\n\t\t""""""\n\t\tsuper(fixed_sampler, self).__init__(blocks_to_fetch)\n\t\tself._fixed_id = fixed_id\n\t\n\tdef sample(self,distribution):\n\t\treturn [self._fixed_id]\n\nclass random_sampler(meta_sampler):\n\t""""""\n\tPerform random sampling \n\t""""""\n\tdef sample(self,distribution):\n\t\treturn np.random.choice(range(distribution.shape[0]),size=self._blocks_to_fetch,replace=False)\n\nclass argmax_sampler(meta_sampler):\n\t""""""\n\tPerform sampling according to an argmax of the sampling distribution\n\t"""""" \n\tdef sample(self,distribution):\n\t\treturn np.argpartition(np.squeeze(distribution),-self._blocks_to_fetch)[-self._blocks_to_fetch:]\n\nclass sequential_sampler(meta_sampler):\n\t""""""\n\tSample block to train according to a round robin schema\n\t""""""\n\tdef __init__(self,blocks_to_fetch):\n\t\tsuper(sequential_sampler, self).__init__(blocks_to_fetch)\n\t\tself._sample_counter=0\n\t\n\tdef sample(self,distribution):\n\t\tbase_block = self._sample_counter%distribution.shape[0]\n\t\tresult = [(base_block+i)%distribution.shape[0] for i in range(self._blocks_to_fetch)]\n\t\tself._sample_counter+=1\n\t\treturn result\n\nclass probabilistic_sampler(meta_sampler):\n\t""""""\n\tSample according to the current probability distribution \n\t""""""\n\tdef sample(self,distribution):\n\t\treturn np.random.choice(range(distribution.shape[0]),size=self._blocks_to_fetch,replace=False,p=np.squeeze(distribution))\n\n###############################################################################\nSAMPLER_FACTORY = {\n\t\'FIXED\':fixed_sampler,\n\t\'RANDOM\':random_sampler,\n\t\'ARGMAX\':argmax_sampler,\n\t\'SEQUENTIAL\':sequential_sampler,\n\t\'PROBABILITY\':probabilistic_sampler\n}\n\nAVAILABLE_SAMPLER = SAMPLER_FACTORY.keys()\n\ndef get_sampler(name, blocks_to_fetch, fixed_id=0):\n\tassert(name in AVAILABLE_SAMPLER)\n\tif name==\'FIXED\':\n\t\treturn SAMPLER_FACTORY[name](blocks_to_fetch,fixed_id)\n\telse:\n\t\treturn SAMPLER_FACTORY[name](blocks_to_fetch)'"
