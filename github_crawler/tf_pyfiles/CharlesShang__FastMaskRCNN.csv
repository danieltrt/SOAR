file_path,api_count,code
download_and_convert_data.py,4,"b'#!/usr/bin/env python\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom libs.datasets import download_and_convert_coco\nfrom libs.configs import config_v1\n\nFLAGS = tf.app.flags.FLAGS\n\n# tf.app.flags.DEFINE_string(\n#     \'dataset_name\', \'coco\',\n#     \'The name of the dataset to convert, one of ""coco"", ""cifar10"", ""flowers"", ""mnist"".\')\n\n# tf.app.flags.DEFINE_string(\n#     \'dataset_dir\', \'data/coco\',\n#     \'The directory where the output TFRecords and temporary files are saved.\')\n\n\ndef main(_):\n  if not os.path.isdir(\'./output/mask_rcnn\'):\n    os.makedirs(\'./output/mask_rcnn\')\n  if not FLAGS.dataset_name:\n    raise ValueError(\'You must supply the dataset name with --dataset_name\')\n  if not FLAGS.dataset_dir:\n    raise ValueError(\'You must supply the dataset directory with --dataset_dir\')\n\n  elif FLAGS.dataset_name == \'coco\':\n    download_and_convert_coco.run(FLAGS.dataset_dir, FLAGS.dataset_split_name)\n  else:\n    raise ValueError(\n        \'dataset_name [%s] was not recognized.\' % FLAGS.dataset_dir)\n\nif __name__ == \'__main__\':\n  tf.app.run()\n\n'"
libs/__init__.py,0,b''
libs/memory_util.py,8,"b'import os\nimport re\nimport sys\nimport tempfile\nimport tensorflow as tf\n\ndebug_messages = False\n\ndef vlog(level):\n  os.environ[\'TF_CPP_MIN_VLOG_LEVEL\'] = str(level)\n\n# this helper is here in case we later want to capture huge stderr that doesn\'t fit in RAM\nclass TemporaryFileHelper:\n  """"""Provides a way to fetch contents of temporary file."""""" \n  def __init__(self, temporary_file):\n    self.temporary_file = temporary_file\n  def getvalue(self):\n    return open(self.temporary_file.name).read() \n\n\nSTDOUT=1\nSTDERR=2\nclass capture_stderr:\n  """"""Utility to capture output, use as follows\n     with util.capture_stderr() as stderr:\n        sess = tf.Session()\n    print(""Captured:"", stderr.getvalue()).\n    """"""\n\n  def __init__(self, fd=STDERR):\n    self.fd = fd\n    self.prevfd = None\n\n  def __enter__(self):\n    t = tempfile.NamedTemporaryFile()\n    self.prevfd = os.dup(self.fd)\n    os.dup2(t.fileno(), self.fd)\n    return TemporaryFileHelper(t)\n\n  def __exit__(self, exc_type, exc_value, traceback):\n    os.dup2(self.prevfd, self.fd)\n\n\n################################################################################\n# LOG_MEMORY_PARSING\n################################################################################\n# Until https://github.com/tensorflow/tensorflow/issues/6716 is resolved, the\n# reliable way to get access to tensor deallocation information is to parse\n# __LOG_MEMORY__ from VLOG print statements. This is sensitive to print order\n# run unbuffered to prevent interleaving:\n#   python -u script.py\n\n# Regex\'es to parse __LOG_MEMORY__ statements\n# Each regex is preceded by an example of line it\'s meant to pass\n\n# I 5143420588.000000 file tensorflow/core/framework/log_memory.cc:41] __LOG_MEMORY__ MemoryLogTensorAllocation { step_id: -6 kernel_name: ""Unknown (from Proto)"" tensor { dtype: DT_INT32 shape { dim { size: 3 } } allocation_description { requested_bytes: 12 allocated_bytes: 12 allocator_name: ""cpu"" allocation_id: 3 has_single_reference: true ptr: 29496256 } } }\ntensor_allocation_regex = re.compile(""""""MemoryLogTensorAllocation.*?step_id: (?P<step_id>[-0123456789]+).*kernel_name: \\""(?P<kernel_name>[^""]+)\\"".*?allocated_bytes: (?P<allocated_bytes>\\d+).*allocator_name: \\""(?P<allocator_name>[^""]+)\\"".*allocation_id: (?P<allocation_id>\\d+).*"""""")\n\n# I 6795349363.000000 file tensorflow/core/framework/log_memory.cc:41] __LOG_MEMORY__ MemoryLogRawAllocation { step_id: -3 operation: ""TF_AllocateTensor"" num_bytes: 1000000 ptr: 80910752 allocation_id: 99 allocator_name: ""cpu"" }\nraw_allocation_regex = re.compile(""""""MemoryLogRawAllocation.*?step_id: (?P<step_id>[-0123456789]+).*operation: \\""(?P<kernel_name>[^""]+)\\"".*?num_bytes: (?P<allocated_bytes>\\d+).*allocation_id: (?P<allocation_id>\\d+).*allocator_name: ""(?P<allocator_name>[^""]+)"".*"""""")\n\n# I 5143420588.000000 file tensorflow/core/framework/log_memory.cc:41] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: 1 kernel_name: ""Const"" tensor { dtype: DT_INT32 shape { dim { size: 3 } } allocation_description { requested_bytes: 12 allocated_bytes: 12 allocator_name: ""cpu"" allocation_id: 3 ptr: 29496256 } } }\n# 2017-01-26 10:13:30: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: 2 kernel_name: ""a0"" tensor { dtype: DT_FLOAT shape { dim { size: 250000 } } allocation_description { requested_bytes: 1000000 allocated_bytes: 1000192 allocator_name: ""gpu_bfc"" allocation_id: 3 ptr: 30076651520 } } }\n#tensor_output_regex = re.compile(""""""MemoryLogTensorOutput.* step_id: (?P<step_id>[-0123456789]+) kernel_name: \\""(?P<kernel_name>[^""]+).*allocated_bytes: (?P<allocated_bytes>\\d+).*allocation_id: (?P<allocation_id>\\d+).*"""""")   \ntensor_output_regex = re.compile(""""""MemoryLogTensorOutput.* step_id: (?P<step_id>[-0123456789]+) kernel_name: \\""(?P<kernel_name>[^""]+).*allocated_bytes: (?P<allocated_bytes>\\d+).*allocator_name: \\""(?P<allocator_name>[^""]+)\\"".*allocation_id: (?P<allocation_id>\\d+).*"""""")\n\n# some Shape lines are missing bytes info so have separate regex for them\n# I 5162643141.000000 file tensorflow/core/framework/log_memory.cc:41] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: 5 kernel_name: ""gradients/Shape"" tensor { dtype: DT_INT32 shape { dim { } } } }\ntensor_output_regex_no_bytes = re.compile(""""""MemoryLogTensorOutput.* step_id: (?P<step_id>[-0123456789]+) kernel_name: \\""(?P<kernel_name>[^""]+).*"""""")\n\n\n# 5143420588.000000 file tensorflow/core/framework/log_memory.cc:41] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 2 allocator_name: ""cpu"" }\ntensor_deallocation_regex = re.compile(""""""allocation_id: (?P<allocation_id>\\d+).*allocator_name: \\""(?P<allocator_name>[^""]+)\\"".*"""""")\n\n# I 6796000229.000000 file tensorflow/core/framework/log_memory.cc:41] __LOG_MEMORY__ MemoryLogRawDeallocation { step_id: -3 operation: ""TensorFlow C Api"" allocation_id: 177 allocator_name: ""cpu"" }\nraw_deallocation_regex = re.compile(""""""allocation_id: (?P<allocation_id>\\d+).*allocator_name: \\""(?P<allocator_name>[^""]+)\\"".*"""""")\n\n# I 5143420588.000000 file tensorflow/core/framework/log_memory.cc:41] __LOG_MEMORY__ MemoryLogStep { step_id: 1 handle: ""->Print:0//0/;0"" }\ntensor_logstep_regex = re.compile(""""""MemoryLogStep.*?step_id: (?P<step_id>[-0123456789]+).*"""""")\n\n\ndef _parse_logline(l):\n    if \'MemoryLogTensorOutput\' in l:\n        m = tensor_output_regex.search(l)\n        if not m:\n            m = tensor_output_regex_no_bytes.search(l)\n\n        assert m, l\n        d = m.groupdict()\n        d[""type""] = ""MemoryLogTensorOutput""\n            \n    elif \'MemoryLogTensorAllocation\' in l:\n        m = tensor_allocation_regex.search(l)\n\n        # Broadcast args give weird allocation messages without size, ignore\n        # I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorAllocation { step_id: 2 kernel_name: ""gradients/node_5_grad/BroadcastGradientArgs"" tensor { dtype: DT_INT32 shape { dim { } } } }\n        if not m:\n            return {""type"": ""MemoryLogTensorAllocation"", ""line"": l,\n                    ""allocation_id"": ""-1""}\n\n        assert m, l\n        d = m.groupdict()\n        d[""type""] = ""MemoryLogTensorAllocation""\n        if debug_messages:\n            print(""Got allocation for %s, %s""%(d[""allocation_id""], d[""kernel_name""]))\n    elif \'MemoryLogTensorDeallocation\' in l:\n        m = tensor_deallocation_regex.search(l)\n        assert m, l\n        d = m.groupdict()\n        d[""type""] = ""MemoryLogTensorDeallocation""\n        if debug_messages:\n            print(""Got deallocation for %s""%(d[""allocation_id""]))\n    elif \'MemoryLogStep\' in l:\n        m = tensor_logstep_regex.search(l)\n        assert m, l\n        d = m.groupdict()\n        d[""type""] = ""MemoryLogStep""\n    elif \'MemoryLogRawAllocation\' in l:\n        m = raw_allocation_regex.search(l)\n        assert m, l\n        d = m.groupdict()\n        d[""type""] = ""MemoryLogRawAllocation""\n    elif \'MemoryLogRawDeallocation\' in l:\n        m = raw_deallocation_regex.search(l)\n        assert m, l\n        d = m.groupdict()\n        d[""type""] = ""MemoryLogRawDeallocation""\n    else:\n        assert False, ""Unknown log line: ""+l\n        \n    if not ""allocation_id"" in d:\n        d[""allocation_id""] = ""-1""\n\n    d[""line""] = l\n    return d\n\ndef memory_timeline(log):\n    if hasattr(log, \'getvalue\'):\n        log = log.getvalue()\n    \n    def unique_alloc_id(line):\n        if line[""allocation_id""] == ""-1"":\n            return ""-1""\n        return line[""allocation_id""]+""-""+line[""allocator_name""]\n    \n    def get_alloc_names(line):\n        alloc_id = unique_alloc_id(line)\n        for entry in reversed(allocation_map.get(alloc_id, [])):\n            kernel_name = entry.get(""kernel_name"", ""unknown"")\n            if not ""unknown"" in kernel_name:\n                return kernel_name+""(""+unique_alloc_id(line)+"")""\n        # couldn\'t find an allocation message with name of kernel\n        return ""(""+alloc_id+"")""\n\n    def get_alloc_bytes(line):\n        for entry in allocation_map.get(unique_alloc_id(line), []):\n            if ""allocated_bytes"" in entry:\n                return entry[""allocated_bytes""]\n        return ""0""\n\n    def get_alloc_type(line):\n        for entry in allocation_map.get(unique_alloc_id(line), []):\n            if ""allocator_name"" in entry:\n                return entry[""allocator_name""]\n        return ""0""\n\n    parsed_lines = []\n    for l in log.split(""\\n""):\n        if \'LOG_MEMORY\' in l: # and not \'step_id: -6\' in l:\n            parsed_lines.append(_parse_logline(l))\n\n    allocation_map = {} # map of <allocation_id>-<allocator_name>->parsed_logline of allocation\n    for line in parsed_lines:\n        if (line[""type""] == ""MemoryLogTensorAllocation"" or line[""type""] == ""MemoryLogRawAllocation"" or\n            line[""type""] == ""MemoryLogTensorOutput""):\n            allocation_map.setdefault(unique_alloc_id(line), []).append(line)\n    if debug_messages:\n        print(allocation_map)\n    result = []\n    for i, line in enumerate(parsed_lines):\n        # skip lines without allocation_id, ie lines like\n        # I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogStep { step_id: 2 handle: ""->/gradients/a1_grad/TanhGrad/0/;1"" }\n\n        if int(line[""allocation_id""]) == -1:\n            continue\n        alloc_names = get_alloc_names(line)\n        # if line doesn\'t specify bytes, look in history if there was corresponding TensorOutput or TensorAllocation msg\n        if int(line.get(\'allocated_bytes\', -1)) < 0:\n            alloc_bytes = get_alloc_bytes(line)\n        else:\n            alloc_bytes = line.get(\'allocated_bytes\', -1)\n        alloc_type = get_alloc_type(line)\n        if line[""type""] == ""MemoryLogTensorOutput"":\n            continue\n        if line[""type""] == ""MemoryLogTensorDeallocation"" or line[""type""]==""MemoryLogRawDeallocation"":\n            alloc_bytes = ""-"" + alloc_bytes\n        result.append((i, alloc_names, alloc_bytes, alloc_type))\n    return result\n\ndef peak_memory(log, gpu_only=False):\n    """"""Peak memory used across all devices.""""""\n    peak_memory = -123456789 # to catch bugs\n    total_memory = 0\n    for record in memory_timeline(log):\n        i, kernel_name, allocated_bytes, allocator_type = record\n        allocated_bytes = int(allocated_bytes)\n        if gpu_only:\n            if not allocator_type.startswith(""gpu""):\n                continue\n        total_memory += allocated_bytes\n        peak_memory = max(total_memory, peak_memory)\n    return peak_memory\n    \ndef print_memory_timeline(log, gpu_only=False, ignore_less_than_bytes=0):\n      \n    total_memory = 0\n    for record in memory_timeline(log):\n        i, kernel_name, allocated_bytes, allocator_type = record\n        allocated_bytes = int(allocated_bytes)\n        if gpu_only:\n            if not allocator_type.startswith(""gpu""):\n                continue\n        if abs(allocated_bytes)<ignore_less_than_bytes:\n            continue  # ignore small allocations\n        total_memory += allocated_bytes\n        print(""%9d %42s %11d %11d %s""%(i, kernel_name, allocated_bytes, total_memory, allocator_type))\n\nimport matplotlib.pyplot as plt\ndef plot_memory_timeline(log, gpu_only=False, ignore_less_than_bytes=1000):\n      \n    total_memory = 0\n    timestamps = []\n    data = []\n    current_time = 0\n    for record in memory_timeline(log):\n        timestamp, kernel_name, allocated_bytes, allocator_type = record\n        allocated_bytes = int(allocated_bytes)\n        \n        if abs(allocated_bytes)<ignore_less_than_bytes:\n            continue  # ignore small allocations\n        if gpu_only:\n            if not record[3].startswith(""gpu""):\n                continue\n        timestamps.append(current_time-.00000001)\n        data.append(total_memory)\n        total_memory += int(record[2])\n        timestamps.append(current_time)\n        data.append(total_memory)\n        current_time+=1\n    plt.plot(timestamps, data)\n\n################################################################################\n# smart initialize\n################################################################################\n\ndef smart_initialize(variables=None, sess=None):\n  """"""Initializes all uninitialized variables in correct order. Initializers\n  are only run for uninitialized variables, so it\'s safe to run this multiple\n  times.\n  Args:\n      sess: session to use. Use default session if None.\n  """"""\n\n  from tensorflow.contrib import graph_editor as ge\n  def make_initializer(var): \n    def f():\n      return tf.assign(var, var.initial_value).op\n    return f\n  \n  def make_noop(): return tf.no_op()\n\n  def make_safe_initializer(var):\n    """"""Returns initializer op that only runs for uninitialized ops.""""""\n    return tf.cond(tf.is_variable_initialized(var), make_noop,\n                   make_initializer(var), name=""safe_init_""+var.op.name).op\n\n  if not sess:\n    sess = tf.get_default_session()\n  g = tf.get_default_graph()\n\n  if not variables:\n    variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n      \n  safe_initializers = {}\n  for v in variables:\n    safe_initializers[v.op.name] = make_safe_initializer(v)\n      \n  # initializers access variable vaue through read-only value cached in\n  # <varname>/read, so add control dependency to trigger safe_initializer\n  # on read access\n  for v in variables:\n    var_name = v.op.name\n    var_cache = g.get_operation_by_name(var_name+""/read"")\n    ge.reroute.add_control_inputs(var_cache, [safe_initializers[var_name]])\n\n  sess.run(tf.group(*safe_initializers.values()))\n    \n  # remove initializer dependencies to avoid slowing down future variable reads\n  for v in variables:\n    var_name = v.op.name\n    var_cache = g.get_operation_by_name(var_name+""/read"")\n    ge.reroute.remove_control_inputs(var_cache, [safe_initializers[var_name]])\n'"
libs/setup.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nimport numpy as np\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\n\ndef find_in_path(name, path):\n    ""Find a file in a search path""\n    #adapted fom http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\ndef locate_cuda():\n    """"""Locate the CUDA environment on the system\n\n    Returns a dict with keys \'home\', \'nvcc\', \'include\', and \'lib64\'\n    and values giving the absolute path to each directory.\n\n    Starts by looking for the CUDAHOME env variable. If not found, everything\n    is based on finding \'nvcc\' in the PATH.\n    """"""\n\n    # first check if the CUDAHOME env variable is in use\n    if \'CUDAHOME\' in os.environ:\n        home = os.environ[\'CUDAHOME\']\n        nvcc = pjoin(home, \'bin\', \'nvcc\')\n    else:\n        # otherwise, search the PATH for NVCC\n        default_path = pjoin(os.sep, \'usr\', \'local\', \'cuda\', \'bin\')\n        nvcc = find_in_path(\'nvcc\', os.environ[\'PATH\'] + os.pathsep + default_path)\n        if nvcc is None:\n            raise EnvironmentError(\'The nvcc binary could not be \'\n                \'located in your $PATH. Either add it to your path, or set $CUDAHOME\')\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {\'home\':home, \'nvcc\':nvcc,\n                  \'include\': pjoin(home, \'include\'),\n                  \'lib64\': pjoin(home, \'lib64\')}\n    for k, v in cudaconfig.iteritems():\n        if not os.path.exists(v):\n            raise EnvironmentError(\'The CUDA %s path could not be located in %s\' % (k, v))\n\n    return cudaconfig\nCUDA = locate_cuda()\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        print extra_postargs\n        if os.path.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', CUDA[\'nvcc\'])\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\next_modules = [\n    Extension(\n        ""boxes.cython_bbox"",\n        [""boxes/bbox.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs = [numpy_include]\n    ),\n    Extension(\n        ""boxes.cython_anchor"",\n        [""boxes/cython_anchor.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs = [numpy_include]\n    ),\n    Extension(\n      ""boxes.cython_bbox_transform"",\n      [""boxes/cython_bbox_transform.pyx""],\n      extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n      include_dirs=[numpy_include]\n    ),\n    Extension(\n        ""boxes.cython_nms"",\n        [""boxes/nms.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs = [numpy_include]\n    ),\n    Extension(\n        ""nms.cpu_nms"",\n        [""nms/cpu_nms.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs = [numpy_include]\n    ),\n    Extension(\n        \'nms.gpu_nms\',\n        [\'nms/nms_kernel.cu\', \'nms/gpu_nms.pyx\'],\n        library_dirs=[CUDA[\'lib64\']],\n        libraries=[\'cudart\'],\n        language=\'c++\',\n        runtime_library_dirs=[CUDA[\'lib64\']],\n        # this syntax is specific to this build system\n        # we\'re only going to use certain compiler args with nvcc and not with gcc\n        # the implementation of this trick is in customize_compiler() below\n        extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                            \'nvcc\': [\'-arch=sm_52\',\n                                     \'--ptxas-options=-v\',\n                                     \'-c\',\n                                     \'--compiler-options\',\n                                     ""\'-fPIC\'""]},\n        include_dirs = [numpy_include, CUDA[\'include\']]\n    ),\n]\n\nsetup(\n    name=\'fast_rcnn\',\n    ext_modules=ext_modules,\n    # inject our custom trigger\n    cmdclass={\'build_ext\': custom_build_ext},\n)\n'"
train/__init__.py,0,b'#!/usr/bin/env python\n# coding=utf-8\n\nfrom . import train_utils\n'
train/train.py,41,"b'#!/usr/bin/env python\n# coding=utf-8\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n \nimport functools\nimport os, sys\nimport time\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom time import gmtime, strftime\n\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), \'..\'))\nimport libs.configs.config_v1 as cfg\nimport libs.datasets.dataset_factory as datasets\nimport libs.nets.nets_factory as network \n\nimport libs.preprocessings.coco_v1 as coco_preprocess\nimport libs.nets.pyramid_network as pyramid_network\nimport libs.nets.resnet_v1 as resnet_v1\n\nfrom train.train_utils import _configure_learning_rate, _configure_optimizer, \\\n  _get_variables_to_train, _get_init_fn, get_var_list_to_restore\n\nfrom PIL import Image, ImageFont, ImageDraw, ImageEnhance\nfrom libs.datasets import download_and_convert_coco\n#from libs.datasets.download_and_convert_coco import _cat_id_to_cls_name\nfrom libs.visualization.pil_utils import cat_id_to_cls_name, draw_img, draw_bbox\n\nFLAGS = tf.app.flags.FLAGS\nresnet50 = resnet_v1.resnet_v1_50\n\ndef solve(global_step):\n    """"""add solver to losses""""""\n    # learning reate\n    lr = _configure_learning_rate(82783, global_step)\n    optimizer = _configure_optimizer(lr)\n    tf.summary.scalar(\'learning_rate\', lr)\n\n    # compute and apply gradient\n    losses = tf.get_collection(tf.GraphKeys.LOSSES)\n    regular_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n    regular_loss = tf.add_n(regular_losses)\n    out_loss = tf.add_n(losses)\n    total_loss = tf.add_n(losses + regular_losses)\n\n    tf.summary.scalar(\'total_loss\', total_loss)\n    tf.summary.scalar(\'out_loss\', out_loss)\n    tf.summary.scalar(\'regular_loss\', regular_loss)\n\n    update_ops = []\n    variables_to_train = _get_variables_to_train()\n    # update_op = optimizer.minimize(total_loss)\n    gradients = optimizer.compute_gradients(total_loss, var_list=variables_to_train)\n    grad_updates = optimizer.apply_gradients(gradients, \n            global_step=global_step)\n    update_ops.append(grad_updates)\n    \n    # update moving mean and variance\n    if FLAGS.update_bn:\n        update_bns = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        update_bn = tf.group(*update_bns)\n        update_ops.append(update_bn)\n\n    return tf.group(*update_ops)\n\ndef restore(sess):\n     """"""choose which param to restore""""""\n     if FLAGS.restore_previous_if_exists:\n        try:\n            checkpoint_path = tf.train.latest_checkpoint(FLAGS.train_dir)\n            ###########\n            restorer = tf.train.Saver()\n            ###########\n\n            ###########\n            # not_restore = [ \'pyramid/fully_connected/weights:0\', \n            #                 \'pyramid/fully_connected/biases:0\',\n            #                 \'pyramid/fully_connected/weights:0\', \n            #                 \'pyramid/fully_connected_1/biases:0\',\n            #                 \'pyramid/fully_connected_1/weights:0\', \n            #                 \'pyramid/fully_connected_2/weights:0\', \n            #                 \'pyramid/fully_connected_2/biases:0\',\n            #                 \'pyramid/fully_connected_3/weights:0\', \n            #                 \'pyramid/fully_connected_3/biases:0\',\n            #                 \'pyramid/Conv/weights:0\', \n            #                 \'pyramid/Conv/biases:0\',\n            #                 \'pyramid/Conv_1/weights:0\', \n            #                 \'pyramid/Conv_1/biases:0\', \n            #                 \'pyramid/Conv_2/weights:0\', \n            #                 \'pyramid/Conv_2/biases:0\', \n            #                 \'pyramid/Conv_3/weights:0\', \n            #                 \'pyramid/Conv_3/biases:0\',\n            #                 \'pyramid/Conv2d_transpose/weights:0\', \n            #                 \'pyramid/Conv2d_transpose/biases:0\', \n            #                 \'pyramid/Conv_4/weights:0\',\n            #                 \'pyramid/Conv_4/biases:0\',\n            #                 \'pyramid/fully_connected/weights/Momentum:0\', \n            #                 \'pyramid/fully_connected/biases/Momentum:0\',\n            #                 \'pyramid/fully_connected/weights/Momentum:0\', \n            #                 \'pyramid/fully_connected_1/biases/Momentum:0\',\n            #                 \'pyramid/fully_connected_1/weights/Momentum:0\', \n            #                 \'pyramid/fully_connected_2/weights/Momentum:0\', \n            #                 \'pyramid/fully_connected_2/biases/Momentum:0\',\n            #                 \'pyramid/fully_connected_3/weights/Momentum:0\', \n            #                 \'pyramid/fully_connected_3/biases/Momentum:0\',\n            #                 \'pyramid/Conv/weights/Momentum:0\', \n            #                 \'pyramid/Conv/biases/Momentum:0\',\n            #                 \'pyramid/Conv_1/weights/Momentum:0\', \n            #                 \'pyramid/Conv_1/biases/Momentum:0\', \n            #                 \'pyramid/Conv_2/weights/Momentum:0\', \n            #                 \'pyramid/Conv_2/biases/Momentum:0\', \n            #                 \'pyramid/Conv_3/weights/Momentum:0\', \n            #                 \'pyramid/Conv_3/biases/Momentum:0\',\n            #                 \'pyramid/Conv2d_transpose/weights/Momentum:0\', \n            #                 \'pyramid/Conv2d_transpose/biases/Momentum:0\', \n            #                 \'pyramid/Conv_4/weights/Momentum:0\',\n            #                 \'pyramid/Conv_4/biases/Momentum:0\',]\n            # vars_to_restore = [v for v in  tf.all_variables()if v.name not in not_restore]\n            # restorer = tf.train.Saver(vars_to_restore)\n            # for var in vars_to_restore:\n            #     print (\'restoring \', var.name)\n            ############\n\n            restorer.restore(sess, checkpoint_path)\n            print (\'restored previous model %s from %s\'\\\n                    %(checkpoint_path, FLAGS.train_dir))\n            time.sleep(2)\n            return\n        except:\n            print (\'--restore_previous_if_exists is set, but failed to restore in %s %s\'\\\n                    % (FLAGS.train_dir, checkpoint_path))\n            time.sleep(2)\n\n     if FLAGS.pretrained_model:\n        if tf.gfile.IsDirectory(FLAGS.pretrained_model):\n            checkpoint_path = tf.train.latest_checkpoint(FLAGS.pretrained_model)\n        else:\n            checkpoint_path = FLAGS.pretrained_model\n\n        if FLAGS.checkpoint_exclude_scopes is None:\n            FLAGS.checkpoint_exclude_scopes=\'pyramid\'\n        if FLAGS.checkpoint_include_scopes is None:\n            FLAGS.checkpoint_include_scopes=\'resnet_v1_50\'\n\n        vars_to_restore = get_var_list_to_restore()\n        for var in vars_to_restore:\n            print (\'restoring \', var.name)\n      \n        try:\n           restorer = tf.train.Saver(vars_to_restore)\n           restorer.restore(sess, checkpoint_path)\n           print (\'Restored %d(%d) vars from %s\' %(\n               len(vars_to_restore), len(tf.global_variables()),\n               checkpoint_path ))\n        except:\n           print (\'Checking your params %s\' %(checkpoint_path))\n           raise\n    \ndef train():\n    """"""The main function that runs training""""""\n\n    ## data\n    image, ih, iw, gt_boxes, gt_masks, num_instances, img_id = \\\n        datasets.get_dataset(FLAGS.dataset_name, \n                             FLAGS.dataset_split_name, \n                             FLAGS.dataset_dir, \n                             FLAGS.im_batch,\n                             is_training=True)\n\n    data_queue = tf.RandomShuffleQueue(capacity=32, min_after_dequeue=16,\n            dtypes=(\n                image.dtype, ih.dtype, iw.dtype, \n                gt_boxes.dtype, gt_masks.dtype, \n                num_instances.dtype, img_id.dtype)) \n    enqueue_op = data_queue.enqueue((image, ih, iw, gt_boxes, gt_masks, num_instances, img_id))\n    data_queue_runner = tf.train.QueueRunner(data_queue, [enqueue_op] * 4)\n    tf.add_to_collection(tf.GraphKeys.QUEUE_RUNNERS, data_queue_runner)\n    (image, ih, iw, gt_boxes, gt_masks, num_instances, img_id) =  data_queue.dequeue()\n    im_shape = tf.shape(image)\n    image = tf.reshape(image, (im_shape[0], im_shape[1], im_shape[2], 3))\n\n    ## network\n    logits, end_points, pyramid_map = network.get_network(FLAGS.network, image,\n            weight_decay=FLAGS.weight_decay, is_training=True)\n    outputs = pyramid_network.build(end_points, im_shape[1], im_shape[2], pyramid_map,\n            num_classes=81,\n            base_anchors=9,\n            is_training=True,\n            gt_boxes=gt_boxes, gt_masks=gt_masks,\n            loss_weights=[0.2, 0.2, 1.0, 0.2, 1.0])\n\n\n    total_loss = outputs[\'total_loss\']\n    losses  = outputs[\'losses\']\n    batch_info = outputs[\'batch_info\']\n    regular_loss = tf.add_n(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n    \n    input_image = end_points[\'input\']\n    final_box = outputs[\'final_boxes\'][\'box\']\n    final_cls = outputs[\'final_boxes\'][\'cls\']\n    final_prob = outputs[\'final_boxes\'][\'prob\']\n    final_gt_cls = outputs[\'final_boxes\'][\'gt_cls\']\n    gt = outputs[\'gt\']\n\n    #############################\n    tmp_0 = outputs[\'losses\']\n    tmp_1 = outputs[\'losses\']\n    tmp_2 = outputs[\'losses\']\n    tmp_3 = outputs[\'losses\']\n    tmp_4 = outputs[\'losses\']\n\n    # tmp_0 = outputs[\'tmp_0\']\n    # tmp_1 = outputs[\'tmp_1\']\n    # tmp_2 = outputs[\'tmp_2\']\n    tmp_3 = outputs[\'tmp_3\']\n    tmp_4 = outputs[\'tmp_4\']\n    ############################\n\n\n    ## solvers\n    global_step = slim.create_global_step()\n    update_op = solve(global_step)\n\n    cropped_rois = tf.get_collection(\'__CROPPED__\')[0]\n    transposed = tf.get_collection(\'__TRANSPOSED__\')[0]\n    \n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.95)\n    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n    init_op = tf.group(\n            tf.global_variables_initializer(),\n            tf.local_variables_initializer()\n            )\n    sess.run(init_op)\n\n    summary_op = tf.summary.merge_all()\n    logdir = os.path.join(FLAGS.train_dir, strftime(\'%Y%m%d%H%M%S\', gmtime()))\n    if not os.path.exists(logdir):\n        os.makedirs(logdir)\n    summary_writer = tf.summary.FileWriter(logdir, graph=sess.graph)\n\n    ## restore\n    restore(sess)\n\n    ## main loop\n    coord = tf.train.Coordinator()\n    threads = []\n    # print (tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS))\n    for qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):\n        threads.extend(qr.create_threads(sess, coord=coord, daemon=True,\n                                         start=True))\n\n    tf.train.start_queue_runners(sess=sess, coord=coord)\n    saver = tf.train.Saver(max_to_keep=20)\n\n    for step in range(FLAGS.max_iters):\n        \n        start_time = time.time()\n\n        s_, tot_loss, reg_lossnp, img_id_str, \\\n        rpn_box_loss, rpn_cls_loss, refined_box_loss, refined_cls_loss, mask_loss, \\\n        gt_boxesnp, \\\n        rpn_batch_pos, rpn_batch, refine_batch_pos, refine_batch, mask_batch_pos, mask_batch, \\\n        input_imagenp, final_boxnp, final_clsnp, final_probnp, final_gt_clsnp, gtnp, tmp_0np, tmp_1np, tmp_2np, tmp_3np, tmp_4np= \\\n                     sess.run([update_op, total_loss, regular_loss, img_id] + \n                              losses + \n                              [gt_boxes] + \n                              batch_info + \n                              [input_image] + [final_box] + [final_cls] + [final_prob] + [final_gt_cls] + [gt] + [tmp_0] + [tmp_1] + [tmp_2] + [tmp_3] + [tmp_4])\n\n        duration_time = time.time() - start_time\n        if step % 1 == 0: \n            print ( """"""iter %d: image-id:%07d, time:%.3f(sec), regular_loss: %.6f, """"""\n                    """"""total-loss %.4f(%.4f, %.4f, %.6f, %.4f, %.4f), """"""\n                    """"""instances: %d, """"""\n                    """"""batch:(%d|%d, %d|%d, %d|%d)"""""" \n                   % (step, img_id_str, duration_time, reg_lossnp, \n                      tot_loss, rpn_box_loss, rpn_cls_loss, refined_box_loss, refined_cls_loss, mask_loss,\n                      gt_boxesnp.shape[0], \n                      rpn_batch_pos, rpn_batch, refine_batch_pos, refine_batch, mask_batch_pos, mask_batch))\n\n            # draw_bbox(step, \n            #           np.uint8((np.array(input_imagenp[0])/2.0+0.5)*255.0), \n            #           name=\'est\', \n            #           bbox=final_boxnp, \n            #           label=final_clsnp, \n            #           prob=final_probnp,\n            #           gt_label=np.argmax(np.asarray(final_gt_clsnp),axis=1),\n            #           )\n\n            # draw_bbox(step, \n            #           np.uint8((np.array(input_imagenp[0])/2.0+0.5)*255.0), \n            #           name=\'gt\', \n            #           bbox=gtnp[:,0:4], \n            #           label=np.asarray(gtnp[:,4], dtype=np.uint8),\n            #           )\n            \n            print (""labels"")\n            # print (cat_id_to_cls_name(np.unique(np.argmax(np.asarray(final_gt_clsnp),axis=1)))[1:])\n            # print (cat_id_to_cls_name(np.unique(np.asarray(gt_boxesnp, dtype=np.uint8)[:,4])))\n            print (cat_id_to_cls_name(np.unique(np.argmax(np.asarray(tmp_3np),axis=1)))[1:])\n            #print (cat_id_to_cls_name(np.unique(np.argmax(np.asarray(gt_boxesnp)[:,4],axis=1))))\n            print (""classes"")\n            print (cat_id_to_cls_name(np.unique(np.argmax(np.array(tmp_4np),axis=1))))\n            # print (np.asanyarray(tmp_3np))\n\n            #print (""ordered rois"")\n            #print (np.asarray(tmp_0np)[0])\n            #print (""pyramid_feature"")\n            #print ()\n             #print(np.unique(np.argmax(np.array(final_probnp),axis=1)))\n            #for var, val in zip(tmp_2, tmp_2np):\n            #    print(var.name)  \n            #print(np.argmax(np.array(tmp_0np),axis=1))\n            \n            \n            if np.isnan(tot_loss) or np.isinf(tot_loss):\n                print (gt_boxesnp)\n                raise\n          \n        if step % 100 == 0:\n            summary_str = sess.run(summary_op)\n            summary_writer.add_summary(summary_str, step)\n            summary_writer.flush()\n\n        if (step % 10000 == 0 or step + 1 == FLAGS.max_iters) and step != 0:\n            checkpoint_path = os.path.join(FLAGS.train_dir, \n                                           FLAGS.dataset_name + \'_\' + FLAGS.network + \'_model.ckpt\')\n            saver.save(sess, checkpoint_path, global_step=step)\n\n        if coord.should_stop():\n            coord.request_stop()\n            coord.join(threads)\n\n\nif __name__ == \'__main__\':\n    train()\n'"
train/train_utils.py,21,"b'#!/usr/bin/env python\n# coding=utf-8\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport tensorflow as tf\nimport libs.configs.config_v1 as cfg\n\nslim = tf.contrib.slim\nFLAGS = tf.app.flags.FLAGS\n\ndef _configure_optimizer(learning_rate):\n  """"""Configures the optimizer used for training.\n\n  Args:\n    learning_rate: A scalar or `Tensor` learning rate.\n\n  Returns:\n    An instance of an optimizer.\n\n  Raises:\n    ValueError: if FLAGS.optimizer is not recognized.\n  """"""\n  if FLAGS.optimizer == \'adadelta\':\n    optimizer = tf.train.AdadeltaOptimizer(\n        learning_rate,\n        rho=FLAGS.adadelta_rho,\n        epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == \'adagrad\':\n    optimizer = tf.train.AdagradOptimizer(\n        learning_rate,\n        initial_accumulator_value=FLAGS.adagrad_initial_accumulator_value)\n  elif FLAGS.optimizer == \'adam\':\n    optimizer = tf.train.AdamOptimizer(\n        learning_rate,\n        beta1=FLAGS.adam_beta1,\n        beta2=FLAGS.adam_beta2,\n        epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == \'ftrl\':\n    optimizer = tf.train.FtrlOptimizer(\n        learning_rate,\n        learning_rate_power=FLAGS.ftrl_learning_rate_power,\n        initial_accumulator_value=FLAGS.ftrl_initial_accumulator_value,\n        l1_regularization_strength=FLAGS.ftrl_l1,\n        l2_regularization_strength=FLAGS.ftrl_l2)\n  elif FLAGS.optimizer == \'momentum\':\n    optimizer = tf.train.MomentumOptimizer(\n        learning_rate,\n        momentum=FLAGS.momentum,\n        name=\'Momentum\')\n  elif FLAGS.optimizer == \'rmsprop\':\n    optimizer = tf.train.RMSPropOptimizer(\n        learning_rate,\n        decay=FLAGS.rmsprop_decay,\n        momentum=FLAGS.rmsprop_momentum,\n        epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == \'sgd\':\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n  else:\n    raise ValueError(\'Optimizer [%s] was not recognized\', FLAGS.optimizer)\n  return optimizer\n\ndef _configure_learning_rate(num_samples_per_epoch, global_step):\n  """"""Configures the learning rate.\n\n  Args:\n    num_samples_per_epoch: The number of samples in each epoch of training.\n    global_step: The global_step tensor.\n\n  Returns:\n    A `Tensor` representing the learning rate.\n\n  Raises:\n    ValueError: if\n  """"""\n  decay_steps = int(num_samples_per_epoch / FLAGS.batch_size *\n                    FLAGS.num_epochs_per_decay)\n  if FLAGS.sync_replicas:\n    decay_steps /= FLAGS.replicas_to_aggregate\n\n  if FLAGS.learning_rate_decay_type == \'exponential\':\n    return tf.train.exponential_decay(FLAGS.learning_rate,\n                                      global_step,\n                                      decay_steps,\n                                      FLAGS.learning_rate_decay_factor,\n                                      staircase=True,\n                                      name=\'exponential_decay_learning_rate\')\n  elif FLAGS.learning_rate_decay_type == \'fixed\':\n    return tf.constant(FLAGS.learning_rate, name=\'fixed_learning_rate\')\n  elif FLAGS.learning_rate_decay_type == \'polynomial\':\n    return tf.train.polynomial_decay(FLAGS.learning_rate,\n                                     global_step,\n                                     decay_steps,\n                                     FLAGS.end_learning_rate,\n                                     power=0.9,\n                                     cycle=False,\n                                     name=\'polynomial_decay_learning_rate\')\n  else:\n    raise ValueError(\'learning_rate_decay_type [%s] was not recognized\',\n                     FLAGS.learning_rate_decay_type)\n\ndef _get_variables_to_train():\n  """"""Returns a list of variables to train.\n\n  Returns:\n    A list of variables to train by the optimizer.\n  """"""\n  if FLAGS.trainable_scopes is None:\n    return tf.trainable_variables()\n  else:\n    scopes = [scope.strip() for scope in FLAGS.trainable_scopes.split(\',\')]\n\n  variables_to_train = []\n  for scope in scopes:\n    variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n    variables_to_train.extend(variables)\n  return variables_to_train\n\ndef _get_init_fn():\n  """"""Returns a function run by the chief worker to warm-start the training.\n\n  Note that the init_fn is only run when initializing the model during the very\n  first global step.\n\n  Returns:\n    An init function run by the supervisor.\n  """"""\n  if FLAGS.checkpoint_path is None:\n    return None\n\n  # Warn the user if a checkpoint exists in the train_dir. Then we\'ll\n  # ignore the checkpoint anyway.\n  if tf.train.latest_checkpoint(FLAGS.train_dir):\n    tf.logging.info(\n        \'Ignoring --checkpoint_path because a checkpoint already exists in %s\'\n        % FLAGS.train_dir)\n    return None\n\n  exclusions = []\n  if FLAGS.checkpoint_exclude_scopes:\n    exclusions = [scope.strip()\n                  for scope in FLAGS.checkpoint_exclude_scopes.split(\',\')]\n\n  # TODO(sguada) variables.filter_variables()\n  variables_to_restore = []\n  for var in slim.get_model_variables():\n    for exclusion in exclusions:\n      if var.op.name.startswith(exclusion):\n        break\n    else:\n      variables_to_restore.append(var)\n\n  if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\n    checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n  else:\n    checkpoint_path = FLAGS.checkpoint_path\n\n  tf.logging.info(\'Fine-tuning from %s\' % checkpoint_path)\n\n  return slim.assign_from_checkpoint_fn(\n      checkpoint_path,\n      variables_to_restore,\n      ignore_missing_vars=FLAGS.ignore_missing_vars)\n\ndef get_var_list_to_restore():\n  """"""Choose which vars to restore, ignore vars by setting --checkpoint_exclude_scopes """"""\n\n  variables_to_restore = []\n  if FLAGS.checkpoint_exclude_scopes is not None:\n    exclusions = [scope.strip()\n                  for scope in FLAGS.checkpoint_exclude_scopes.split(\',\')]\n\n    # build restore list\n    for var in tf.model_variables():\n      for exclusion in exclusions:\n        if var.name.startswith(exclusion):\n          break\n      else:\n        variables_to_restore.append(var)\n  else:\n    variables_to_restore = tf.model_variables()\n\n  variables_to_restore_final = []\n  if FLAGS.checkpoint_include_scopes is not None:\n      includes = [\n              scope.strip()\n              for scope in FLAGS.checkpoint_include_scopes.split(\',\')\n              ]\n      for var in variables_to_restore:\n          for include in includes:\n              if var.name.startswith(include):\n                  variables_to_restore_final.append(var)\n                  break\n  else:\n      variables_to_restore_final = variables_to_restore\n\n  return variables_to_restore_final\n'"
unit_test/__init__.py,0,b''
unit_test/data_test.py,8,"b""#!/usr/bin/env python\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport functools\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..'))\nimport numpy as np\nimport PIL.Image as Image\nfrom PIL import ImageDraw\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom libs.logs.log import LOG\nimport libs.configs.config_v1 as cfg\nimport libs.nets.resnet_v1 as resnet_v1\nimport libs.datasets.dataset_factory as dataset_factory\nimport libs.datasets.coco as coco\nimport libs.preprocessings.coco_v1 as preprocess_coco\nfrom libs.layers import ROIAlign\n\nresnet50 = resnet_v1.resnet_v1_50\nFLAGS = tf.app.flags.FLAGS\n\nwith tf.Graph().as_default():\n\n  image, ih, iw, gt_boxes, gt_masks, num_instances, img_id = \\\n    coco.read('./data/coco/records/coco_trainval2014_00000-of-00048.tfrecord')\n  \n  image, gt_boxes, gt_masks = \\\n    preprocess_coco.preprocess_image(image, gt_boxes, gt_masks)\n\n  \n\n  sess = tf.Session()\n  init_op = tf.group(tf.global_variables_initializer(),\n                     tf.local_variables_initializer())\n  # init_op = tf.initialize_all_variables()\n\n  boxes = [[100, 100, 200, 200],\n           [50, 50, 100, 100],\n           [100, 100, 750, 750],\n           [50, 50, 60, 60]]\n  # boxes = np.zeros((0, 4))\n  boxes = tf.constant(boxes, tf.float32)\n  feat = ROIAlign(image, boxes, False, 16, 7, 7)\n  sess.run(init_op)\n\n  tf.train.start_queue_runners(sess=sess)\n  with sess.as_default():\n      for i in range(20000):\n        image_np, ih_np, iw_np, gt_boxes_np, gt_masks_np, num_instances_np, img_id_np, \\\n        feat_np = \\\n            sess.run([image, ih, iw, gt_boxes, gt_masks, num_instances, img_id,\n                feat])\n        # print (image_np.shape, gt_boxes_np.shape, gt_masks_np.shape)\n            \n        if i % 100 == 0:\n            print ('%d, image_id: %s, instances: %d'%  (i, str(img_id_np), num_instances_np))\n            image_np = 256 * (image_np * 0.5 + 0.5)\n            image_np = image_np.astype(np.uint8)\n            image_np = np.squeeze(image_np)\n            print (image_np.shape, ih_np, iw_np)\n            print (feat_np.shape)\n            im = Image.fromarray(image_np)\n            imd = ImageDraw.Draw(im)\n            for i in range(gt_boxes_np.shape[0]):\n                imd.rectangle(gt_boxes_np[i, :])\n            im.save(str(img_id_np) + '.png')\n            # print (gt_boxes_np)\n  sess.close()\n"""
unit_test/layer_test.py,10,"b""#!/usr/bin/env python \n# coding=utf-8\n\nimport numpy as np\nimport sys\nimport os\nimport tensorflow as tf \n\nsys.path.append(os.path.join(os.path.dirname(__file__), '..'))\nfrom libs.boxes.roi import roi_cropping\nfrom libs.layers import anchor_encoder\nfrom libs.layers import anchor_decoder\nfrom libs.layers import roi_encoder\nfrom libs.layers import roi_decoder\nfrom libs.layers import mask_encoder\nfrom libs.layers import mask_decoder\nfrom libs.layers import gen_all_anchors\nfrom libs.layers import ROIAlign\nfrom libs.layers import sample_rpn_outputs\nfrom libs.layers import assign_boxes\nimport  libs.configs.config_v1 as cfg\n\nclass layer_test(object):\n\n    def __init__(self, N, num_classes, height, width, gt_boxes=None, gt_masks=None, rois=None, classes=None):\n        self.N = N\n        self.num_classes = num_classes \n        self.height = height\n        self.width = width \n        if gt_boxes is not None:\n            self.gt_boxes = gt_boxes\n            self.gt_masks = gt_masks \n            self.rois = rois \n            self.classes = classes\n        else:\n            self.gt_boxes = np.random.randint(0, 50, (self.N, 2))\n            s = np.random.randint(30, 40, (self.N, 2))\n            c = np.random.randint(1, self.num_classes, (self.N, 1))\n            self.gt_boxes = np.hstack((self.gt_boxes, self.gt_boxes + s, c))\n            self.gt_boxes = self.gt_boxes.astype(np.float32)\n            gt_masks = np.zeros((self.N, height, width), dtype=np.int32)\n            for i in range(self.N):\n                gt_masks[i, int(self.gt_boxes[i, 1]):int(self.gt_boxes[i, 3]), int(self.gt_boxes[i, 0]):int(self.gt_boxes[i, 2])] = 1\n            self.gt_masks = gt_masks\n\n            # rois \n            noises = np.random.randint(-3, 3, (self.N, 4))\n            self.rois = self.gt_boxes[:, :4] + noises \n\n            # classes \n            self.classes = self.gt_boxes[:, -1]\n\n    def test(self):\n        return\n\n\nclass anchor_test(layer_test):\n\n    def __init__(self, N, num_classes, height, width, gt_boxes=None, gt_masks=None, rois=None, classes=None):\n        super(anchor_test, self).__init__(N, num_classes, height, width, gt_boxes, gt_masks, rois, classes)\n    def test(self):\n        cfg.FLAGS.fg_threshold = 0.7\n        with tf.Session() as sess:\n            all_anchors = gen_all_anchors(self.height / 4, self.width / 4, stride = 4, scales = 2**np.arange(1,5))\n            all_anchors = tf.reshape(all_anchors, [-1, 4])\n            self.all_anchors =  np.reshape(all_anchors.eval(), (-1, 4))\n            labels, bbox_targets, bbox_inside_weights = \\\n                    anchor_encoder(self.gt_boxes, all_anchors, self.height / 4, self.width / 4, 4)\n            self.labels = labels.eval()\n            self.bbox_targets = bbox_targets.eval()\n            self.bbox_inside_weights = bbox_inside_weights.eval()\n            print (self.labels.shape)\n            print (self.bbox_targets.shape)\n            print (self.bbox_inside_weights.shape)\n            print (self.gt_boxes)\n            # print (self.all_anchors[0:120:15, ])\n            np_labels = self.labels.reshape((-1,))\n            np_bbox_targets = self.bbox_targets.reshape((-1, 4))\n            np_bbox_inside_weights = self.bbox_inside_weights.reshape((-1, 4))\n            encoded_gt_boxes = []\n            for i in range(np_labels.shape[0]):\n                if np_labels[i] >= 1:\n                    # print (self.all_anchors[i, :], np_bbox_targets[i, :], np_bbox_inside_weights[i, :])\n                    encoded_gt_boxes.append (np_bbox_targets[i, :])\n            encoded_gt_boxes = np.asarray(encoded_gt_boxes, dtype = np.float32)\n            encoded_gt_boxes = encoded_gt_boxes.reshape((-1, 4))\n            # print (np.max(np_labels))\n            # print (np.sum(np_labels >= 1))\n            scores = np.zeros((np_labels.shape[0], 2), dtype=np.float32)\n            for i in range(np_labels.shape[0]):\n                if np_labels[i] > 0:\n                    scores[i, 0] = 0\n                    scores[i, 1] = 1\n            scores = scores.astype(np.float32)\n            boxes, classes, scores = \\\n                    anchor_decoder(self.bbox_targets, scores, all_anchors, self.height, self.width)\n            self.npboxes = boxes.eval().reshape((-1, 4))\n            npscores = scores.eval().reshape((-1, 1))\n            self.npboxes = np.hstack((self.npboxes, npscores))\n            # print (self.npboxes.shape, npscores.shape)\n            bbox_targets_np = self.bbox_targets.reshape([-1, 4])\n            all_anchors_np = all_anchors.eval().reshape([-1, 4])\n            for i in range(self.npboxes.shape[0]):\n                if self.npboxes[i, 4] >= 1:\n                    print (bbox_targets_np[i], self.npboxes[i], all_anchors_np[i])\n\nclass roi_test(layer_test):\n\n    def __init__(self, N, num_classes, height, width, gt_boxes=None, gt_masks=None, rois=None, classes=None):\n        super(roi_test, self).__init__(N, num_classes, height, width, gt_boxes, gt_masks, rois, classes)\n\n    def test(self):\n        import time\n        print (self.gt_boxes)\n        # time.sleep(10)\n        with tf.Session() as sess:\n            rois = self.gt_boxes[:, :4]\n            rois = rois + np.random.randint(-3, 3, (self.N, 4))\n            bgs = np.random.randint(0, 60, (self.N + 2, 2))\n            bgs = np.hstack((bgs, bgs + np.random.randint(20, 30, (self.N + 2, 2))))\n            bgs = bgs.astype(np.float32)\n            rois = np.vstack((rois, bgs))\n            self.rois = rois\n            print (rois)\n            print (self.gt_boxes)\n            labels, bbox_targets, bbox_inside_weights = \\\n                    roi_encoder(self.gt_boxes, self.rois, self.num_classes)\n            self.labels = labels.eval()\n            self.bbox_targets = bbox_targets.eval()\n            self.bbox_inside_weights = bbox_inside_weights.eval()\n\n            print (self.labels.shape)\n            print (self.labels)\n            print (self.bbox_targets.shape)\n            print (self.bbox_inside_weights.shape)\n            print ('learning targets:')\n            for i in range(self.labels.size):\n                s = int(4 * self.labels[i])\n                e = s + 4\n                print (self.labels[i], self.bbox_targets[i, s:e], self.bbox_inside_weights[i, s:e])\n\n            scores = np.random.rand(self.rois.shape[0], self.num_classes)\n            scores = scores.astype(np.float32)\n            final_boxes, classes, scores = \\\n                    roi_decoder(self.bbox_targets, scores, self.rois, 100, 100)\n            self.final_boxes = final_boxes.eval()\n            self.scores = scores.eval()\n            self.classes = classes.eval()\n            print ('rois:')\n            print (self.rois)\n            print ('final_boxes:')\n            print (self.final_boxes)\n\n\nclass mask_test(layer_test):\n\n    def __init__(self, N, num_classes, height, width, gt_boxes=None, gt_masks=None, rois=None, classes=None):\n        super(mask_test, self).__init__(N, num_classes, height, width, gt_boxes, gt_masks, rois, classes)\n\n    def test(self):\n        # mask \n        with tf.Session() as sess:\n            gt_masks = np.zeros((self.N, 100, 100), dtype=np.int32)\n         \n            rois = self.gt_boxes[:, :4]\n            rois = rois + np.random.randint(-5, 5, (self.N, 4))\n            rois[rois < 0] = 0\n            bgs = np.random.randint(0, 60, (self.N + 2, 2))\n            bgs = np.hstack((bgs, bgs + np.random.randint(20, 30, (self.N + 2, 2))))\n            bgs = bgs.astype(np.float32)\n            rois = np.vstack((rois, bgs))\n            print (rois)\n\n            for i in range(self.N):\n                x1, y1 = int(self.gt_boxes[i, 0] + 2), int(self.gt_boxes[i, 1] + 2)\n                x2, y2 = int(self.gt_boxes[i, 2] - 1), int(self.gt_boxes[i, 3] - 1)\n                gt_masks[i, y1:y2, x1:x2] = 1\n            self.gt_masks = gt_masks\n\n            labels, mask_targets, mask_inside_weights = \\\n                    mask_encoder(self.gt_masks, self.gt_boxes, rois, self.num_classes, 15, 15)\n            self.labels = labels.eval()\n            self.mask_targets = mask_targets.eval()\n            self.mask_inside_weights = mask_inside_weights.eval()\n\n            # print (self.mask_targets)\n            print (self.labels)\n            for i in range(rois.shape[0]):\n                print(i, 'label:', self.labels[i])\n                print (self.mask_targets[i, :, :, int(self.labels[i])])\n\nclass sample_test(layer_test):\n\n    def __init__(self, N, num_classes, height, width, gt_boxes=None, gt_masks=None, rois=None, classes=None):\n        super(sample_test, self).__init__(N, num_classes, height, width, gt_boxes, gt_masks, rois, classes)\n\n    def test(self):\n        with tf.Session() as sess:\n            boxes = np.random.randint(0, 50, [self.N, 2])\n            s = np.random.randint(20, 30, [self.N, 2])\n            boxes = np.hstack((boxes, boxes + s)).astype(np.float32)\n\n            scores = np.random.rand(self.N, 1).astype(np.float32)\n            boxes, scores, batch_inds= \\\n                    sample_rpn_outputs(boxes, scores, is_training=False,) \n            self.boxes = boxes.eval()\n            self.scores = scores.eval()\n            bs = np.hstack((self.boxes, self.scores))\n            np.set_printoptions(precision=3, suppress=True)\n            print (bs)\n\nclass ROIAlign_test(layer_test):\n\n    def __init__(self, N, num_classes, height, width, gt_boxes=None, gt_masks=None, rois=None, classes=None):\n        super(ROIAlign_test, self).__init__(N, num_classes, height, width, gt_boxes, gt_masks, rois, classes)\n\n    def test(self):\n        with tf.Session() as sess:\n            npimg = np.random.rand(1, self.height, self.width, 2).astype(np.float32)\n            npimg = np.zeros((1, self.height, self.width, 1), dtype=np.float32)\n\n            boxes = np.random.randint(0, 50, [self.N, 2])\n            s = np.random.randint(20, 30, [self.N, 2])\n            boxes = np.hstack((boxes, boxes + s)).astype(np.float32)\n\n            stride = 2.0 \n            for i in range(self.N):\n                b = boxes[i, :] / stride\n                npimg[:, \n                      int(b[1]):int(b[3]+1),\n                      int(b[0]):int(b[2]+1),\n                      :] = 1 \n                \n            img = tf.constant(npimg)\n            pooled_height = 5\n            pooled_width = 5\n            batch_inds = np.zeros((self.N, ), dtype=np.int32)\n            batch_inds = tf.convert_to_tensor(batch_inds)\n            feats = ROIAlign(img, boxes, batch_inds, stride=stride, pooled_height=pooled_height, pooled_width=pooled_width,)\n            self.feats = feats.eval()\n            print (self.feats.shape)\n            print (self.feats.reshape((self.N, pooled_height, pooled_width)))\n\nclass assign_test(layer_test):\n\n    def __init__(self, N, num_classes, height, width, gt_boxes=None, gt_masks=None, rois=None, classes=None):\n        super(assign_test, self).__init__(N, num_classes, height, width, gt_boxes, gt_masks, rois, classes)\n    def test(self):\n\n        self.gt_boxes = np.random.randint(0, int(self.width/1.5), (self.N, 2))\n        s = np.random.randint(30, int(self.width/1), (self.N, 2))\n        c = np.random.randint(1, self.num_classes, (self.N, 1))\n        self.gt_boxes = np.hstack((self.gt_boxes, self.gt_boxes + s, c))\n        batch_inds = np.zeros((self.N, ), np.int32)\n        with tf.Session() as sess:\n\n            batch_inds = tf.convert_to_tensor(batch_inds)\n            [assigned_boxes, assigned_batch, inds] = \\\n                assign_boxes(self.gt_boxes, [self.gt_boxes, batch_inds], [2,3,4,5])\n            [b1, b2, b3, b4] = assigned_boxes\n            [ind1, ind2, ind3, ind4] = assigned_batch\n            b1n, b2n, b3n, b4n, indsn, ind1n, ind2n, ind3n, ind4n= \\\n                    sess.run([b1, b2, b3, b4, inds, ind1, ind2, ind3, ind4])\n            print (b1n)\n            print (b2n)\n            print (b3n)\n            print (b4n)\n            print (np.hstack((self.gt_boxes, indsn[:, np.newaxis])))\n\n            print (ind1n, ind2n, ind3n, ind4n)\n\nif __name__ == '__main__':\n    print ('##############################')\n    print ('Anchor Test')\n    print ('##############################')\n    an_test = anchor_test(0, 5, 100, 100)\n    an_test.test()\n    an_test = anchor_test(5, 5, 100, 100)\n    an_test.test()\n    print ('##############################')\n    print ('ROI Test')\n    print ('##############################')\n    r_test  = roi_test(10, 9, 100, 100)\n    r_test.test()\n    r_test  = roi_test(0, 9, 100, 100)\n    r_test.test()\n    print ('##############################')\n    print ('Mask Test')\n    print ('##############################')\n    m_test  = mask_test(5, 4, 100, 100)\n    m_test.test()\n    m_test  = mask_test(0, 4, 100, 100)\n    m_test.test()\n    print ('##############################')\n    print ('Sample Test')\n    print ('##############################')\n    s_test  = sample_test(20, 4, 100, 100)\n    s_test.test()\n    print ('##############################')\n    print ('ROIAlign Test')\n    print ('##############################')\n    c_test  = ROIAlign_test(8, 4, 100, 100)\n    c_test.test()\n    c_test  = ROIAlign_test(0, 4, 100, 100)\n    c_test.test()\n    print ('##############################')\n    print ('Assign Test')\n    print ('##############################')\n    c_test  = assign_test(15, 2, 800, 800)\n    c_test.test()\n"""
unit_test/preprocessing_test.py,6,"b""#!/usr/bin/env python\n# coding=utf-8\n\nimport numpy as np\nimport sys\nimport os\nimport tensorflow as tf \nsys.path.append(os.path.join(os.path.dirname(__file__), '..'))\n\nimport libs.preprocessings.coco_v1 as coco_preprocess\nimport  libs.configs.config_v1 as cfg\n\nih, iw, ic = 400,500, 3\nN = 3\nimage = np.random.randint(0, 255, (ih, iw, ic)).astype(np.uint8)\ngt_masks = np.zeros((N, ih, iw)).astype(np.int32)\nxy = np.random.randint(0, min(iw, ih)-100, (N, 2)).astype(np.float32)\nwh = np.random.randint(20, 40, (N, 2)).astype(np.float32)\ncls = np.random.randint(1, 6, (N, 1)).astype(np.float32)\ngt_boxes = np.hstack((xy, xy + wh, cls)).astype(np.float32)\ngt_boxes_np = gt_boxes \nimage_np = image \ngt_masks_np = gt_masks \n\nfor i in range(N):\n    box = gt_boxes[i, 0:4]\n    gt_masks[i, int(box[1]):int(box[3]),\n                int(box[0]):int(box[2])] = 1\nimage = tf.constant(image)\ngt_boxes = tf.constant(gt_boxes)\ngt_masks = tf.constant(gt_masks)\n\nimage, gt_boxes, gt_masks = \\\n        coco_preprocess.preprocess_image(image, gt_boxes, gt_masks, is_training=True)\n\nwith tf.Session() as sess:\n    # print(image.eval())\n    image_tf, gt_boxes_tf, gt_masks_tf = \\\n            sess.run([image, gt_boxes, gt_masks])\n    print ('#######################')\n    print ('DATA PREPROCESSING TEST')\n    print ('#######################')\n    print ('gt_boxes shape:', gt_boxes_tf.shape)\n    print('mask shape:', gt_masks_tf.shape)\n    print(gt_boxes_tf)\n    for i in range(N):\n        box = np.round(gt_boxes_tf[i, 0:4])\n        box = box.astype(np.int32)\n        m = gt_masks_tf[i, box[1]:box[3], box[0]:box[2]]\n        print ('after:', box)\n        print (np.sum(m)/ (0.0 + m.size))\n        print (m)\n        box = np.round(gt_boxes_np[i, 0:4])\n        box = box.astype(np.int32)\n        m = gt_masks_np[i, box[1]:box[3], box[0]:box[2]]\n        print ('ori box:', box)\n        print (np.sum(m)/ (0.0 + m.size))\n"""
unit_test/resnet50_test.py,30,"b'#!/usr/bin/env python\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport functools\nimport os, sys\nimport time\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\nimport numpy as np\nfrom time import gmtime, strftime\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport libs.configs.config_v1 as cfg\nimport libs.datasets.coco as coco\nimport libs.preprocessings.coco_v1 as coco_preprocess\nimport libs.nets.pyramid_network as pyramid_network\nimport libs.nets.resnet_v1 as resnet_v1\nfrom train.train_utils import _configure_learning_rate, _configure_optimizer, \\\n  _get_variables_to_train, _get_init_fn, get_var_list_to_restore\n\nresnet50 = resnet_v1.resnet_v1_50\nFLAGS = tf.app.flags.FLAGS\n\nDEBUG = False\n\nwith tf.Graph().as_default():\n  gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8, \n                              allow_growth=True,\n                              )\n  with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,\n                                        allow_soft_placement=True)) as sess:\n      global_step = slim.create_global_step()\n      \n      ## data\n      image, ih, iw, gt_boxes, gt_masks, num_instances, img_id = \\\n        coco.read(\'./data/coco/records/coco_train2014_00000-of-00040.tfrecord\')\n      with tf.control_dependencies([image, gt_boxes, gt_masks]):\n        image, gt_boxes, gt_masks = coco_preprocess.preprocess_image(image, gt_boxes, gt_masks, is_training=True)\n      \n      ##  network\n      with slim.arg_scope(resnet_v1.resnet_arg_scope(weight_decay=0.0001)):\n        logits, end_points = resnet50(image, 1000, is_training=False)\n      end_points[\'inputs\'] = image\n      \n      for x in sorted(end_points.keys()):\n        print (x, end_points[x].name, end_points[x].shape)\n        \n      pyramid = pyramid_network.build_pyramid(\'resnet50\', end_points)\n      # for p in pyramid:\n      #   print (p, pyramid[p])\n\n      summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n      for p in pyramid:\n        summaries.add(tf.summary.histogram(\'pyramid/hist/\' + p, pyramid[p]))\n        summaries.add(tf.summary.scalar(\'pyramid/means/\'+ p, tf.reduce_mean(tf.abs(pyramid[p]))))\n        \n      outputs = pyramid_network.build_heads(pyramid, ih, iw, num_classes=81, base_anchors=9, is_training=True, gt_boxes=gt_boxes)\n      \n      ## losses\n      loss, losses, batch_info = pyramid_network.build_losses(pyramid, outputs,\n                                             gt_boxes, gt_masks,\n                                             num_classes=81, base_anchors=9, \n                                             rpn_box_lw =0.1, rpn_cls_lw = 0.2,\n                                             refined_box_lw=2.0, refined_cls_lw=0.1,\n                                             mask_lw=0.2)\n\n      ## optimization\n      learning_rate = _configure_learning_rate(82783, global_step)\n      optimizer = _configure_optimizer(learning_rate)\n      summaries.add(tf.summary.scalar(\'learning_rate\', learning_rate))\n      for loss in tf.get_collection(tf.GraphKeys.LOSSES):\n        summaries.add(tf.summary.scalar(\'losses/%s\' % loss.op.name, loss))\n\n      loss = tf.get_collection(tf.GraphKeys.LOSSES)\n      regular_loss = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n      total_loss = tf.add_n(loss + regular_loss)\n      reg_loss = tf.add_n(regular_loss)\n      summaries.add(tf.summary.scalar(\'total_loss\', total_loss))\n      summaries.add(tf.summary.scalar(\'regular_loss\', reg_loss))\n      \n      variables_to_train = _get_variables_to_train()\n      update_op = optimizer.minimize(total_loss)\n      # gradients = optimizer.compute_gradients(total_loss, var_list=variables_to_train)\n      # grad_updates = optimizer.apply_gradients(gradients,\n      #                                          global_step=global_step)\n      # update_op = tf.group(grad_updates)\n      \n      # summary_op = tf.summary.merge(list(summaries), name=\'summary_op\')\n      summary_op = tf.summary.merge_all()\n      logdir = os.path.join(FLAGS.train_dir, strftime(\'%Y%m%d%H%M%S\', gmtime()))\n      if not os.path.exists(logdir):\n        os.makedirs(logdir)\n      summary_writer = tf.summary.FileWriter(\n            logdir,\n            graph=sess.graph)\n      \n      \n      init_op = tf.group(tf.global_variables_initializer(),\n                         tf.local_variables_initializer())\n      \n      sess.run(init_op)\n      coord = tf.train.Coordinator()\n      tf.train.start_queue_runners(sess=sess, coord=coord)\n\n      ## restore pretrained model\n      # FLAGS.pretrained_model = None\n      if FLAGS.pretrained_model:\n          if tf.gfile.IsDirectory(FLAGS.pretrained_model):\n              checkpoint_path = tf.train.latest_checkpoint(FLAGS.pretrained_model)\n          else:\n              checkpoint_path = FLAGS.pretrained_model\n          FLAGS.checkpoint_exclude_scopes=\'pyramid\'\n          FLAGS.checkpoint_include_scopes=\'resnet_v1_50\'\n          vars_to_restore = get_var_list_to_restore()\n          for var in vars_to_restore:\n              print (\'restoring \', var.name)\n          \n          try:\n              restorer = tf.train.Saver(vars_to_restore)\n              restorer.restore(sess, checkpoint_path)\n              print (\'Restored %d(%d) vars from %s\' %(\n                  len(vars_to_restore), len(tf.global_variables()),\n                  checkpoint_path ))\n          except:\n              print (\'Checking your params %s\' %(checkpoint_path))\n              raise\n      \n      # import libs.memory_util as memory_util\n      # memory_util.vlog(1)\n      # with memory_util.capture_stderr() as stderr:\n      #     sess.run([update_op])\n      # memory_util.print_memory_timeline(stderr, ignore_less_than_bytes=1000)\n      \n      ## training loop\n      saver = tf.train.Saver(max_to_keep=20)\n      for step in range(FLAGS.max_iters):\n        start_time = time.time()\n        \n        _, tot_loss, reg_lossnp, img_id_str, \\\n        rpn_box_loss, rpn_cls_loss, refined_box_loss, refined_cls_loss, mask_loss, \\\n        gt_boxesnp, \\\n        rpn_batch_pos, rpn_batch, refine_batch_pos, refine_batch, mask_batch_pos, mask_batch = \\\n                     sess.run([update_op, total_loss, reg_loss,  img_id] + \n                              losses + \n                              [gt_boxes] + \n                              batch_info)\n      # TODO: sampling strategy\n\n        duration_time = time.time() - start_time\n        if step % 1 == 0: \n            print ( """"""iter %d: image-id:%07d, time:%.3f(sec), regular_loss: %.6f, """"""\n                    """"""total-loss %.4f(%.4f, %.4f, %.6f, %.4f, %.4f), """"""\n                    """"""instances: %d, """"""\n                    """"""batch:(%d|%d, %d|%d, %d|%d)"""""" \n                   % (step, img_id_str, duration_time, reg_lossnp, \n                      tot_loss, rpn_box_loss, rpn_cls_loss, refined_box_loss, refined_cls_loss, mask_loss,\n                      gt_boxesnp.shape[0], \n                      rpn_batch_pos, rpn_batch, refine_batch_pos, refine_batch, mask_batch_pos, mask_batch))\n\n            if np.isnan(tot_loss) or np.isinf(tot_loss):\n                print (gt_boxesnp)\n                raise\n          \n        if step % 100 == 0:\n           summary_str = sess.run(summary_op)\n           summary_writer.add_summary(summary_str, step)\n\n        if (step % 1000 == 0 or step + 1 == FLAGS.max_iters) and step != 0:\n          checkpoint_path = os.path.join(FLAGS.train_dir, \n                                         FLAGS.dataset_name + \'_model.ckpt\')\n          saver.save(sess, checkpoint_path, global_step=step)\n\n        if coord.should_stop():\n              coord.request_stop()\n              coord.join(threads)\n'"
libs/boxes/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\nfrom . import cython_nms\nfrom . import cython_bbox\nimport nms\nimport timer\nfrom .anchor import anchors\nfrom .anchor import anchors_plane\nfrom .roi import roi_cropping\nfrom .roi import roi_cropping\nfrom . import cython_anchor\nfrom . import cython_bbox_transform'
libs/boxes/anchor.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom libs.boxes import cython_anchor\n\ndef anchors(scales=[2, 4, 8, 16, 32], ratios=[0.5, 1, 2.0], base=16):\n  """"""Get a set of anchors at one position """"""\n  return generate_anchors(base_size=base, scales=np.asarray(scales, np.int32), ratios=ratios)\n\ndef anchors_plane(height, width, stride = 1.0, \n        scales=[2, 4, 8, 16, 32], ratios=[0.5, 1, 2.0], base=16):\n  """"""Get a complete set of anchors in a spatial plane,\n  height, width are plane dimensions\n  stride is scale ratio of\n  """"""\n  # TODO: implement in C, or pre-compute them, or set to a fixed input-shape\n  # enum all anchors in a plane\n  # scales = kwargs.setdefault(\'scales\', [2, 4, 8, 16, 32])\n  # ratios = kwargs.setdefault(\'ratios\', [0.5, 1, 2.0])\n  # base = kwargs.setdefault(\'base\', 16)\n  anc = anchors(scales, ratios, base)\n  all_anchors = cython_anchor.anchors_plane(height, width, stride, anc)\n  return all_anchors\n\n# Written by Ross Girshick and Sean Bell\ndef generate_anchors(base_size=16, ratios=[0.5, 1, 2],\n                     scales=2 ** np.arange(3, 6)):\n  """"""\n  Generate anchor (reference) windows by enumerating aspect ratios X\n  scales wrt a reference (0, 0, 15, 15) window.\n  """"""\n\n  base_anchor = np.array([1, 1, base_size, base_size]) - 1\n  ratio_anchors = _ratio_enum(base_anchor, ratios)\n  anchors = np.vstack([_scale_enum(ratio_anchors[i, :], scales)\n                       for i in xrange(ratio_anchors.shape[0])])\n  return anchors\n\ndef _whctrs(anchor):\n  """"""\n  Return width, height, x center, and y center for an anchor (window).\n  """"""\n\n  w = anchor[2] - anchor[0] + 1\n  h = anchor[3] - anchor[1] + 1\n  x_ctr = anchor[0] + 0.5 * (w - 1)\n  y_ctr = anchor[1] + 0.5 * (h - 1)\n  return w, h, x_ctr, y_ctr\n\n\ndef _mkanchors(ws, hs, x_ctr, y_ctr):\n  """"""\n  Given a vector of widths (ws) and heights (hs) around a center\n  (x_ctr, y_ctr), output a set of anchors (windows).\n  """"""\n  \n  ws = ws[:, np.newaxis]\n  hs = hs[:, np.newaxis]\n  anchors = np.hstack((x_ctr - 0.5 * (ws - 1),\n                       y_ctr - 0.5 * (hs - 1),\n                       x_ctr + 0.5 * (ws - 1),\n                       y_ctr + 0.5 * (hs - 1)))\n  return anchors\n\n\ndef _ratio_enum(anchor, ratios):\n  """"""\n  Enumerate a set of anchors for each aspect ratio wrt an anchor.\n  """"""\n  \n  w, h, x_ctr, y_ctr = _whctrs(anchor)\n  size = w * h\n  size_ratios = size / ratios\n  ws = np.round(np.sqrt(size_ratios))\n  hs = np.round(ws * ratios)\n  anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n  return anchors\n\n\ndef _scale_enum(anchor, scales):\n  """"""\n  Enumerate a set of anchors for each scale wrt an anchor.\n  """"""\n  \n  w, h, x_ctr, y_ctr = _whctrs(anchor)\n  ws = w * scales\n  hs = h * scales\n  anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n  return anchors\n\ndef _unmap(data, count, inds, fill=0):\n  """""" Unmap a subset of item (data) back to the original set of items (of\n  size count) """"""\n  if len(data.shape) == 1:\n    ret = np.empty((count,), dtype=np.float32)\n    ret.fill(fill)\n    ret[inds] = data\n  else:\n    ret = np.empty((count,) + data.shape[1:], dtype=np.float32)\n    ret.fill(fill)\n    ret[inds, :] = data\n  return ret\n\nif __name__ == \'__main__\':\n  import time\n  \n  t = time.time()\n  a = anchors()\n  num_anchors = 0\n\n  # all_anchors = anchors_plane(200, 250, stride=4, boarder=0)\n  # num_anchors += all_anchors.shape[0]\n  for i in range(10):\n    ancs = anchors()\n    all_anchors = cython_anchor.anchors_plane(200, 250, 4, ancs)\n    num_anchors += all_anchors.shape[0] * all_anchors.shape[1] * all_anchors.shape[2]\n    all_anchors = cython_anchor.anchors_plane(100, 125, 8, ancs)\n    num_anchors += all_anchors.shape[0] * all_anchors.shape[1] * all_anchors.shape[2]\n    all_anchors = cython_anchor.anchors_plane(50, 63, 16, ancs)\n    num_anchors += all_anchors.shape[0] * all_anchors.shape[1] * all_anchors.shape[2]\n    all_anchors = cython_anchor.anchors_plane(25, 32, 32, ancs)\n    num_anchors += all_anchors.shape[0] * all_anchors.shape[1] * all_anchors.shape[2]\n  print(\'average time: %f\' % ((time.time() - t) / 10))\n  print(\'anchors: %d\' % (num_anchors / 10))\n  print(a.shape, \'\\n\', a)\n  print (all_anchors.shape)\n  # from IPython import embed\n  # embed()\n'"
libs/boxes/bbox_transform.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\nimport warnings\n\ndef bbox_transform(ex_rois, gt_rois):\n    """"""\n    computes the distance from ground-truth boxes to the given boxes, normed by their size\n    :param ex_rois: n * 4 numpy array, given boxes\n    :param gt_rois: n * 4 numpy array, ground-truth boxes\n    :return: deltas: n * 4 numpy array, ground-truth boxes\n    """"""\n    ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0\n    ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0\n    ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n    ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n\n    # assert np.min(ex_widths) > 0.1 and np.min(ex_heights) > 0.1, \\\n        # \'Invalid boxes found: {} {}\'. \\\n            # format(ex_rois[np.argmin(ex_widths), :], ex_rois[np.argmin(ex_heights), :])\n\n    gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + 1.0\n    gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + 1.0\n    gt_ctr_x = gt_rois[:, 0] + 0.5 * gt_widths\n    gt_ctr_y = gt_rois[:, 1] + 0.5 * gt_heights\n\n    # warnings.catch_warnings()\n    # warnings.filterwarnings(\'error\')\n    targets_dx = 10.0 * (gt_ctr_x - ex_ctr_x) / ex_widths\n    targets_dy = 10.0 * (gt_ctr_y - ex_ctr_y) / ex_heights\n    targets_dw = 5.0 * np.log(gt_widths / ex_widths)\n    targets_dh = 5.0 * np.log(gt_heights / ex_heights)\n\n    targets = np.vstack(\n        (targets_dx, targets_dy, targets_dw, targets_dh)).transpose()\n    return targets\n\ndef bbox_transform_inv(boxes, deltas):\n    if boxes.shape[0] == 0:\n        return np.zeros((0, deltas.shape[1]), dtype=deltas.dtype)\n\n    boxes = boxes.astype(deltas.dtype, copy=False)\n\n    widths = boxes[:, 2] - boxes[:, 0] + 1.0\n    heights = boxes[:, 3] - boxes[:, 1] + 1.0\n    ctr_x = boxes[:, 0] + 0.5 * widths\n    ctr_y = boxes[:, 1] + 0.5 * heights\n\n    dx = deltas[:, 0::4] * 0.1\n    dy = deltas[:, 1::4] * 0.1\n    dw = deltas[:, 2::4] * 0.2\n    dh = deltas[:, 3::4] * 0.2\n\n    pred_ctr_x = dx * widths[:, np.newaxis] + ctr_x[:, np.newaxis]\n    pred_ctr_y = dy * heights[:, np.newaxis] + ctr_y[:, np.newaxis]\n    # pred_w = np.exp(dw) * widths[:, np.newaxis]\n    # pred_h = np.exp(dh) * heights[:, np.newaxis]\n\n    pred_w = np.exp(dw + np.log(widths[:, np.newaxis]))\n    pred_h = np.exp(dh + np.log(heights[:, np.newaxis]))\n\n\n    #pred_w = np.exp(dw + np.log(widths[:, np.newaxis]))\n    #pred_h = np.exp(dh + np.log(heights[:, np.newaxis]))\n\n\n    pred_boxes = np.zeros(deltas.shape, dtype=deltas.dtype)\n    # x1\n    pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n    # y1\n    pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n    # x2\n    pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w - 1\n    # y2\n    pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h - 1\n\n    return pred_boxes\n\ndef clip_boxes(boxes, im_shape):\n    """"""\n    Clip boxes to image boundaries.\n    """"""\n\n    # x1 >= 0\n    boxes[:, 0::4] = np.maximum(np.minimum(boxes[:, 0::4], im_shape[1] - 1), 0)\n    # y1 >= 0\n    boxes[:, 1::4] = np.maximum(np.minimum(boxes[:, 1::4], im_shape[0] - 1), 0)\n    # x2 < im_shape[1]\n    boxes[:, 2::4] = np.maximum(np.minimum(boxes[:, 2::4], im_shape[1] - 1), 0)\n    # y2 < im_shape[0]\n    boxes[:, 3::4] = np.maximum(np.minimum(boxes[:, 3::4], im_shape[0] - 1), 0)\n    return boxes\n'"
libs/boxes/blob.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Blob helper functions.""""""\n\nimport numpy as np\nimport cv2\nfrom ..fast_rcnn.config import cfg\n\ndef im_list_to_blob(ims):\n    """"""Convert a list of images into a network input.\n\n    Assumes images are already prepared (means subtracted, BGR order, ...).\n    """"""\n    max_shape = np.array([im.shape for im in ims]).max(axis=0)\n    num_images = len(ims)\n    blob = np.zeros((num_images, max_shape[0], max_shape[1], 3),\n                    dtype=np.float32)\n    for i in xrange(num_images):\n        im = ims[i]\n        blob[i, 0:im.shape[0], 0:im.shape[1], :] = im\n\n    return blob\n\ndef prep_im_for_blob(im, pixel_means, target_size, max_size):\n    """"""Mean subtract and scale an image for use in a blob.""""""\n    im = im.astype(np.float32, copy=False)\n    im -= pixel_means\n    im_shape = im.shape\n    im_size_min = np.min(im_shape[0:2])\n    im_size_max = np.max(im_shape[0:2])\n    im_scale = float(target_size) / float(im_size_min)\n    # Prevent the biggest axis from being more than MAX_SIZE\n    if np.round(im_scale * im_size_max) > max_size:\n        im_scale = float(max_size) / float(im_size_max)\n    if cfg.TRAIN.RANDOM_DOWNSAMPLE:\n        r = 0.6 + np.random.rand() * 0.4\n        im_scale *= r\n    im = cv2.resize(im, None, None, fx=im_scale, fy=im_scale,\n                    interpolation=cv2.INTER_LINEAR)\n\n    return im, im_scale\n'"
libs/boxes/cython_anchor.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys, pkg_resources, imp\n\ndef __bootstrap__():\n  global __bootstrap__, __loader__, __file__\n  __file__ = pkg_resources.resource_filename(__name__, 'cython_anchor.so')\n  __loader__ = None\n  del __bootstrap__, __loader__\n  imp.load_dynamic(__name__, __file__)\n\n__bootstrap__()"""
libs/boxes/cython_bbox.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys, pkg_resources, imp\n\ndef __bootstrap__():\n  global __bootstrap__, __loader__, __file__\n  __file__ = pkg_resources.resource_filename(__name__, 'cython_bbox.so')\n  __loader__ = None\n  del __bootstrap__, __loader__\n  imp.load_dynamic(__name__, __file__)\n\n__bootstrap__()"""
libs/boxes/cython_bbox_transform.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys, pkg_resources, imp\n\ndef __bootstrap__():\n  global __bootstrap__, __loader__, __file__\n  __file__ = pkg_resources.resource_filename(__name__, 'cython_bbox_transform.so')\n  __loader__ = None\n  del __bootstrap__, __loader__\n  imp.load_dynamic(__name__, __file__)\n\n__bootstrap__()"""
libs/boxes/cython_nms.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys, pkg_resources, imp\n\ndef __bootstrap__():\n  global __bootstrap__, __loader__, __file__\n  __file__ = pkg_resources.resource_filename(__name__, 'cython_nms.so')\n  __loader__ = None\n  del __bootstrap__, __loader__\n  imp.load_dynamic(__name__, __file__)\n\n__bootstrap__()"""
libs/boxes/gprof2dot.py,0,"b'#!/usr/bin/env python\n#\n# Copyright 2008-2014 Jose Fonseca\n#\n# This program is free software: you can redistribute it and/or modify it\n# under the terms of the GNU Lesser General Public License as published\n# by the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n\n""""""Generate a dot graph from the output of several profilers.""""""\n\n__author__ = ""Jose Fonseca et al""\n\n\nimport sys\nimport math\nimport os.path\nimport re\nimport textwrap\nimport optparse\nimport xml.parsers.expat\nimport collections\nimport locale\nimport json\n\n\n# Python 2.x/3.x compatibility\nif sys.version_info[0] >= 3:\n    PYTHON_3 = True\n    def compat_iteritems(x): return x.items()  # No iteritems() in Python 3\n    def compat_itervalues(x): return x.values()  # No itervalues() in Python 3\n    def compat_keys(x): return list(x.keys())  # keys() is a generator in Python 3\n    basestring = str  # No class basestring in Python 3\n    unichr = chr # No unichr in Python 3\n    xrange = range # No xrange in Python 3\nelse:\n    PYTHON_3 = False\n    def compat_iteritems(x): return x.iteritems()\n    def compat_itervalues(x): return x.itervalues()\n    def compat_keys(x): return x.keys()\n\n\ntry:\n    # Debugging helper module\n    import debug\nexcept ImportError:\n    pass\n\n\n\n########################################################################\n# Model\n\n\nMULTIPLICATION_SIGN = unichr(0xd7)\n\n\ndef times(x):\n    return ""%u%s"" % (x, MULTIPLICATION_SIGN)\n\ndef percentage(p):\n    return ""%.02f%%"" % (p*100.0,)\n\ndef add(a, b):\n    return a + b\n\ndef fail(a, b):\n    assert False\n\n\ntol = 2 ** -23\n\ndef ratio(numerator, denominator):\n    try:\n        ratio = float(numerator)/float(denominator)\n    except ZeroDivisionError:\n        # 0/0 is undefined, but 1.0 yields more useful results\n        return 1.0\n    if ratio < 0.0:\n        if ratio < -tol:\n            sys.stderr.write(\'warning: negative ratio (%s/%s)\\n\' % (numerator, denominator))\n        return 0.0\n    if ratio > 1.0:\n        if ratio > 1.0 + tol:\n            sys.stderr.write(\'warning: ratio greater than one (%s/%s)\\n\' % (numerator, denominator))\n        return 1.0\n    return ratio\n\n\nclass UndefinedEvent(Exception):\n    """"""Raised when attempting to get an event which is undefined.""""""\n    \n    def __init__(self, event):\n        Exception.__init__(self)\n        self.event = event\n\n    def __str__(self):\n        return \'unspecified event %s\' % self.event.name\n\n\nclass Event(object):\n    """"""Describe a kind of event, and its basic operations.""""""\n\n    def __init__(self, name, null, aggregator, formatter = str):\n        self.name = name\n        self._null = null\n        self._aggregator = aggregator\n        self._formatter = formatter\n\n    def __eq__(self, other):\n        return self is other\n\n    def __hash__(self):\n        return id(self)\n\n    def null(self):\n        return self._null\n\n    def aggregate(self, val1, val2):\n        """"""Aggregate two event values.""""""\n        assert val1 is not None\n        assert val2 is not None\n        return self._aggregator(val1, val2)\n    \n    def format(self, val):\n        """"""Format an event value.""""""\n        assert val is not None\n        return self._formatter(val)\n\n\nCALLS = Event(""Calls"", 0, add, times)\nSAMPLES = Event(""Samples"", 0, add, times)\nSAMPLES2 = Event(""Samples"", 0, add, times)\n\n# Count of samples where a given function was either executing or on the stack.\n# This is used to calculate the total time ratio according to the\n# straightforward method described in Mike Dunlavey\'s answer to\n# stackoverflow.com/questions/1777556/alternatives-to-gprof, item 4 (the myth\n# ""that recursion is a tricky confusing issue""), last edited 2012-08-30: it\'s\n# just the ratio of TOTAL_SAMPLES over the number of samples in the profile.\n#\n# Used only when totalMethod == callstacks\nTOTAL_SAMPLES = Event(""Samples"", 0, add, times)\n\nTIME = Event(""Time"", 0.0, add, lambda x: \'(\' + str(x) + \')\')\nTIME_RATIO = Event(""Time ratio"", 0.0, add, lambda x: \'(\' + percentage(x) + \')\')\nTOTAL_TIME = Event(""Total time"", 0.0, fail)\nTOTAL_TIME_RATIO = Event(""Total time ratio"", 0.0, fail, percentage)\n\ntotalMethod = \'callratios\'\n\n\nclass Object(object):\n    """"""Base class for all objects in profile which can store events.""""""\n\n    def __init__(self, events=None):\n        if events is None:\n            self.events = {}\n        else:\n            self.events = events\n\n    def __hash__(self):\n        return id(self)\n\n    def __eq__(self, other):\n        return self is other\n\n    def __lt__(self, other):\n        return id(self) < id(other)\n\n    def __contains__(self, event):\n        return event in self.events\n    \n    def __getitem__(self, event):\n        try:\n            return self.events[event]\n        except KeyError:\n            raise UndefinedEvent(event)\n    \n    def __setitem__(self, event, value):\n        if value is None:\n            if event in self.events:\n                del self.events[event]\n        else:\n            self.events[event] = value\n\n\nclass Call(Object):\n    """"""A call between functions.\n    \n    There should be at most one call object for every pair of functions.\n    """"""\n\n    def __init__(self, callee_id):\n        Object.__init__(self)\n        self.callee_id = callee_id\n        self.ratio = None\n        self.weight = None\n\n\nclass Function(Object):\n    """"""A function.""""""\n\n    def __init__(self, id, name):\n        Object.__init__(self)\n        self.id = id\n        self.name = name\n        self.module = None\n        self.process = None\n        self.calls = {}\n        self.called = None\n        self.weight = None\n        self.cycle = None\n        self.filename = None\n    \n    def add_call(self, call):\n        if call.callee_id in self.calls:\n            sys.stderr.write(\'warning: overwriting call from function %s to %s\\n\' % (str(self.id), str(call.callee_id)))\n        self.calls[call.callee_id] = call\n\n    def get_call(self, callee_id):\n        if not callee_id in self.calls:\n            call = Call(callee_id)\n            call[SAMPLES] = 0\n            call[SAMPLES2] = 0\n            call[CALLS] = 0\n            self.calls[callee_id] = call\n        return self.calls[callee_id]\n\n    _parenthesis_re = re.compile(r\'\\([^()]*\\)\')\n    _angles_re = re.compile(r\'<[^<>]*>\')\n    _const_re = re.compile(r\'\\s+const$\')\n\n    def stripped_name(self):\n        """"""Remove extraneous information from C++ demangled function names.""""""\n\n        name = self.name\n\n        # Strip function parameters from name by recursively removing paired parenthesis\n        while True:\n            name, n = self._parenthesis_re.subn(\'\', name)\n            if not n:\n                break\n\n        # Strip const qualifier\n        name = self._const_re.sub(\'\', name)\n\n        # Strip template parameters from name by recursively removing paired angles\n        while True:\n            name, n = self._angles_re.subn(\'\', name)\n            if not n:\n                break\n\n        return name\n\n    # TODO: write utility functions\n\n    def __repr__(self):\n        return self.name\n\n\nclass Cycle(Object):\n    """"""A cycle made from recursive function calls.""""""\n\n    def __init__(self):\n        Object.__init__(self)\n        self.functions = set()\n\n    def add_function(self, function):\n        assert function not in self.functions\n        self.functions.add(function)\n        if function.cycle is not None:\n            for other in function.cycle.functions:\n                if function not in self.functions:\n                    self.add_function(other)\n        function.cycle = self\n\n\nclass Profile(Object):\n    """"""The whole profile.""""""\n\n    def __init__(self):\n        Object.__init__(self)\n        self.functions = {}\n        self.cycles = []\n\n    def add_function(self, function):\n        if function.id in self.functions:\n            sys.stderr.write(\'warning: overwriting function %s (id %s)\\n\' % (function.name, str(function.id)))\n        self.functions[function.id] = function\n\n    def add_cycle(self, cycle):\n        self.cycles.append(cycle)\n\n    def validate(self):\n        """"""Validate the edges.""""""\n\n        for function in compat_itervalues(self.functions):\n            for callee_id in compat_keys(function.calls):\n                assert function.calls[callee_id].callee_id == callee_id\n                if callee_id not in self.functions:\n                    sys.stderr.write(\'warning: call to undefined function %s from function %s\\n\' % (str(callee_id), function.name))\n                    del function.calls[callee_id]\n\n    def find_cycles(self):\n        """"""Find cycles using Tarjan\'s strongly connected components algorithm.""""""\n\n        # Apply the Tarjan\'s algorithm successively until all functions are visited\n        stack = []\n        data = {}\n        order = 0\n        for function in compat_itervalues(self.functions):\n            order = self._tarjan(function, order, stack, data)\n        cycles = []\n        for function in compat_itervalues(self.functions):\n            if function.cycle is not None and function.cycle not in cycles:\n                cycles.append(function.cycle)\n        self.cycles = cycles\n        if 0:\n            for cycle in cycles:\n                sys.stderr.write(""Cycle:\\n"")\n                for member in cycle.functions:\n                    sys.stderr.write(""\\tFunction %s\\n"" % member.name)\n\n    def prune_root(self, root):\n        visited = set()\n        frontier = set([root])\n        while len(frontier) > 0:\n            node = frontier.pop()\n            visited.add(node)\n            f = self.functions[node]\n            newNodes = f.calls.keys()\n            frontier = frontier.union(set(newNodes) - visited)\n        subtreeFunctions = {}\n        for n in visited:\n            subtreeFunctions[n] = self.functions[n]\n        self.functions = subtreeFunctions\n\n    def prune_leaf(self, leaf):\n        edgesUp = collections.defaultdict(set)\n        for f in self.functions.keys():\n            for n in self.functions[f].calls.keys():\n                edgesUp[n].add(f)\n        # build the tree up\n        visited = set()\n        frontier = set([leaf])\n        while len(frontier) > 0:\n            node = frontier.pop()\n            visited.add(node)\n            frontier = frontier.union(edgesUp[node] - visited)\n        downTree = set(self.functions.keys())\n        upTree = visited\n        path = downTree.intersection(upTree)\n        pathFunctions = {}\n        for n in path:\n            f = self.functions[n]\n            newCalls = {}\n            for c in f.calls.keys():\n                if c in path:\n                    newCalls[c] = f.calls[c]\n            f.calls = newCalls\n            pathFunctions[n] = f\n        self.functions = pathFunctions\n\n\n    def getFunctionId(self, funcName):\n        for f in self.functions:\n            if self.functions[f].name == funcName:\n                return f\n        return False\n\n    class _TarjanData:\n        def __init__(self, order):\n            self.order = order\n            self.lowlink = order\n            self.onstack = False\n\n    def _tarjan(self, function, order, stack, data):\n        """"""Tarjan\'s strongly connected components algorithm.\n\n        See also:\n        - http://en.wikipedia.org/wiki/Tarjan\'s_strongly_connected_components_algorithm\n        """"""\n\n        try:\n            func_data = data[function.id]\n            return order\n        except KeyError:\n            func_data = self._TarjanData(order)\n            data[function.id] = func_data\n        order += 1\n        pos = len(stack)\n        stack.append(function)\n        func_data.onstack = True\n        for call in compat_itervalues(function.calls):\n            try:\n                callee_data = data[call.callee_id]\n                if callee_data.onstack:\n                    func_data.lowlink = min(func_data.lowlink, callee_data.order)\n            except KeyError:\n                callee = self.functions[call.callee_id]\n                order = self._tarjan(callee, order, stack, data)\n                callee_data = data[call.callee_id]\n                func_data.lowlink = min(func_data.lowlink, callee_data.lowlink)\n        if func_data.lowlink == func_data.order:\n            # Strongly connected component found\n            members = stack[pos:]\n            del stack[pos:]\n            if len(members) > 1:\n                cycle = Cycle()\n                for member in members:\n                    cycle.add_function(member)\n                    data[member.id].onstack = False\n            else:\n                for member in members:\n                    data[member.id].onstack = False\n        return order\n\n    def call_ratios(self, event):\n        # Aggregate for incoming calls\n        cycle_totals = {}\n        for cycle in self.cycles:\n            cycle_totals[cycle] = 0.0\n        function_totals = {}\n        for function in compat_itervalues(self.functions):\n            function_totals[function] = 0.0\n\n        # Pass 1:  function_total gets the sum of call[event] for all\n        #          incoming arrows.  Same for cycle_total for all arrows\n        #          that are coming into the *cycle* but are not part of it.\n        for function in compat_itervalues(self.functions):\n            for call in compat_itervalues(function.calls):\n                if call.callee_id != function.id:\n                    callee = self.functions[call.callee_id]\n                    if event in call.events:\n                        function_totals[callee] += call[event]\n                        if callee.cycle is not None and callee.cycle is not function.cycle:\n                            cycle_totals[callee.cycle] += call[event]\n                    else:\n                        sys.stderr.write(""call_ratios: No data for "" + function.name + "" call to "" + callee.name + ""\\n"")\n\n        # Pass 2:  Compute the ratios.  Each call[event] is scaled by the\n        #          function_total of the callee.  Calls into cycles use the\n        #          cycle_total, but not calls within cycles.\n        for function in compat_itervalues(self.functions):\n            for call in compat_itervalues(function.calls):\n                assert call.ratio is None\n                if call.callee_id != function.id:\n                    callee = self.functions[call.callee_id]\n                    if event in call.events:\n                        if callee.cycle is not None and callee.cycle is not function.cycle:\n                            total = cycle_totals[callee.cycle]\n                        else:\n                            total = function_totals[callee]\n                        call.ratio = ratio(call[event], total)\n                    else:\n                        # Warnings here would only repeat those issued above.\n                        call.ratio = 0.0\n\n    def integrate(self, outevent, inevent):\n        """"""Propagate function time ratio along the function calls.\n\n        Must be called after finding the cycles.\n\n        See also:\n        - http://citeseer.ist.psu.edu/graham82gprof.html\n        """"""\n\n        # Sanity checking\n        assert outevent not in self\n        for function in compat_itervalues(self.functions):\n            assert outevent not in function\n            assert inevent in function\n            for call in compat_itervalues(function.calls):\n                assert outevent not in call\n                if call.callee_id != function.id:\n                    assert call.ratio is not None\n\n        # Aggregate the input for each cycle \n        for cycle in self.cycles:\n            total = inevent.null()\n            for function in compat_itervalues(self.functions):\n                total = inevent.aggregate(total, function[inevent])\n            self[inevent] = total\n\n        # Integrate along the edges\n        total = inevent.null()\n        for function in compat_itervalues(self.functions):\n            total = inevent.aggregate(total, function[inevent])\n            self._integrate_function(function, outevent, inevent)\n        self[outevent] = total\n\n    def _integrate_function(self, function, outevent, inevent):\n        if function.cycle is not None:\n            return self._integrate_cycle(function.cycle, outevent, inevent)\n        else:\n            if outevent not in function:\n                total = function[inevent]\n                for call in compat_itervalues(function.calls):\n                    if call.callee_id != function.id:\n                        total += self._integrate_call(call, outevent, inevent)\n                function[outevent] = total\n            return function[outevent]\n    \n    def _integrate_call(self, call, outevent, inevent):\n        assert outevent not in call\n        assert call.ratio is not None\n        callee = self.functions[call.callee_id]\n        subtotal = call.ratio *self._integrate_function(callee, outevent, inevent)\n        call[outevent] = subtotal\n        return subtotal\n\n    def _integrate_cycle(self, cycle, outevent, inevent):\n        if outevent not in cycle:\n\n            # Compute the outevent for the whole cycle\n            total = inevent.null()\n            for member in cycle.functions:\n                subtotal = member[inevent]\n                for call in compat_itervalues(member.calls):\n                    callee = self.functions[call.callee_id]\n                    if callee.cycle is not cycle:\n                        subtotal += self._integrate_call(call, outevent, inevent)\n                total += subtotal\n            cycle[outevent] = total\n            \n            # Compute the time propagated to callers of this cycle\n            callees = {}\n            for function in compat_itervalues(self.functions):\n                if function.cycle is not cycle:\n                    for call in compat_itervalues(function.calls):\n                        callee = self.functions[call.callee_id]\n                        if callee.cycle is cycle:\n                            try:\n                                callees[callee] += call.ratio\n                            except KeyError:\n                                callees[callee] = call.ratio\n            \n            for member in cycle.functions:\n                member[outevent] = outevent.null()\n\n            for callee, call_ratio in compat_iteritems(callees):\n                ranks = {}\n                call_ratios = {}\n                partials = {}\n                self._rank_cycle_function(cycle, callee, ranks)\n                self._call_ratios_cycle(cycle, callee, ranks, call_ratios, set())\n                partial = self._integrate_cycle_function(cycle, callee, call_ratio, partials, ranks, call_ratios, outevent, inevent)\n                assert partial == max(partials.values())\n                assert abs(call_ratio*total - partial) <= 0.001*call_ratio*total\n\n        return cycle[outevent]\n\n    def _rank_cycle_function(self, cycle, function, ranks):\n        """"""Dijkstra\'s shortest paths algorithm.\n\n        See also:\n        - http://en.wikipedia.org/wiki/Dijkstra\'s_algorithm\n        """"""\n\n        import heapq\n        Q = []\n        Qd = {}\n        p = {}\n        visited = set([function])\n\n        ranks[function] = 0\n        for call in compat_itervalues(function.calls):\n            if call.callee_id != function.id:\n                callee = self.functions[call.callee_id]\n                if callee.cycle is cycle:\n                    ranks[callee] = 1\n                    item = [ranks[callee], function, callee]\n                    heapq.heappush(Q, item)\n                    Qd[callee] = item\n\n        while Q:\n            cost, parent, member = heapq.heappop(Q)\n            if member not in visited:\n                p[member]= parent\n                visited.add(member)\n                for call in compat_itervalues(member.calls):\n                    if call.callee_id != member.id:\n                        callee = self.functions[call.callee_id]\n                        if callee.cycle is cycle:\n                            member_rank = ranks[member]\n                            rank = ranks.get(callee)\n                            if rank is not None:\n                                if rank > 1 + member_rank:\n                                    rank = 1 + member_rank\n                                    ranks[callee] = rank\n                                    Qd_callee = Qd[callee]\n                                    Qd_callee[0] = rank\n                                    Qd_callee[1] = member\n                                    heapq._siftdown(Q, 0, Q.index(Qd_callee))\n                            else:\n                                rank = 1 + member_rank\n                                ranks[callee] = rank\n                                item = [rank, member, callee]\n                                heapq.heappush(Q, item)\n                                Qd[callee] = item\n\n    def _call_ratios_cycle(self, cycle, function, ranks, call_ratios, visited):\n        if function not in visited:\n            visited.add(function)\n            for call in compat_itervalues(function.calls):\n                if call.callee_id != function.id:\n                    callee = self.functions[call.callee_id]\n                    if callee.cycle is cycle:\n                        if ranks[callee] > ranks[function]:\n                            call_ratios[callee] = call_ratios.get(callee, 0.0) + call.ratio\n                            self._call_ratios_cycle(cycle, callee, ranks, call_ratios, visited)\n\n    def _integrate_cycle_function(self, cycle, function, partial_ratio, partials, ranks, call_ratios, outevent, inevent):\n        if function not in partials:\n            partial = partial_ratio*function[inevent]\n            for call in compat_itervalues(function.calls):\n                if call.callee_id != function.id:\n                    callee = self.functions[call.callee_id]\n                    if callee.cycle is not cycle:\n                        assert outevent in call\n                        partial += partial_ratio*call[outevent]\n                    else:\n                        if ranks[callee] > ranks[function]:\n                            callee_partial = self._integrate_cycle_function(cycle, callee, partial_ratio, partials, ranks, call_ratios, outevent, inevent)\n                            call_ratio = ratio(call.ratio, call_ratios[callee])\n                            call_partial = call_ratio*callee_partial\n                            try:\n                                call[outevent] += call_partial\n                            except UndefinedEvent:\n                                call[outevent] = call_partial\n                            partial += call_partial\n            partials[function] = partial\n            try:\n                function[outevent] += partial\n            except UndefinedEvent:\n                function[outevent] = partial\n        return partials[function]\n\n    def aggregate(self, event):\n        """"""Aggregate an event for the whole profile.""""""\n\n        total = event.null()\n        for function in compat_itervalues(self.functions):\n            try:\n                total = event.aggregate(total, function[event])\n            except UndefinedEvent:\n                return\n        self[event] = total\n\n    def ratio(self, outevent, inevent):\n        assert outevent not in self\n        assert inevent in self\n        for function in compat_itervalues(self.functions):\n            assert outevent not in function\n            assert inevent in function\n            function[outevent] = ratio(function[inevent], self[inevent])\n            for call in compat_itervalues(function.calls):\n                assert outevent not in call\n                if inevent in call:\n                    call[outevent] = ratio(call[inevent], self[inevent])\n        self[outevent] = 1.0\n\n    def prune(self, node_thres, edge_thres, colour_nodes_by_selftime):\n        """"""Prune the profile""""""\n\n        # compute the prune ratios\n        for function in compat_itervalues(self.functions):\n            try:\n                function.weight = function[TOTAL_TIME_RATIO]\n            except UndefinedEvent:\n                pass\n\n            for call in compat_itervalues(function.calls):\n                callee = self.functions[call.callee_id]\n\n                if TOTAL_TIME_RATIO in call:\n                    # handle exact cases first\n                    call.weight = call[TOTAL_TIME_RATIO] \n                else:\n                    try:\n                        # make a safe estimate\n                        call.weight = min(function[TOTAL_TIME_RATIO], callee[TOTAL_TIME_RATIO]) \n                    except UndefinedEvent:\n                        pass\n\n        # prune the nodes\n        for function_id in compat_keys(self.functions):\n            function = self.functions[function_id]\n            if function.weight is not None:\n                if function.weight < node_thres:\n                    del self.functions[function_id]\n\n        # prune the egdes\n        for function in compat_itervalues(self.functions):\n            for callee_id in compat_keys(function.calls):\n                call = function.calls[callee_id]\n                if callee_id not in self.functions or call.weight is not None and call.weight < edge_thres:\n                    del function.calls[callee_id]\n\n        if colour_nodes_by_selftime:\n            weights = []\n            for function in compat_itervalues(self.functions):\n                try:\n                    weights.append(function[TIME_RATIO])\n                except UndefinedEvent:\n                    pass\n            max_ratio = max(weights or [1])\n\n            # apply rescaled weights for coloriung\n            for function in compat_itervalues(self.functions):\n                try:\n                    function.weight = function[TIME_RATIO] / max_ratio\n                except (ZeroDivisionError, UndefinedEvent):\n                    pass\n    \n    def dump(self):\n        for function in compat_itervalues(self.functions):\n            sys.stderr.write(\'Function %s:\\n\' % (function.name,))\n            self._dump_events(function.events)\n            for call in compat_itervalues(function.calls):\n                callee = self.functions[call.callee_id]\n                sys.stderr.write(\'  Call %s:\\n\' % (callee.name,))\n                self._dump_events(call.events)\n        for cycle in self.cycles:\n            sys.stderr.write(\'Cycle:\\n\')\n            self._dump_events(cycle.events)\n            for function in cycle.functions:\n                sys.stderr.write(\'  Function %s\\n\' % (function.name,))\n\n    def _dump_events(self, events):\n        for event, value in compat_iteritems(events):\n            sys.stderr.write(\'    %s: %s\\n\' % (event.name, event.format(value)))\n\n\n\n########################################################################\n# Parsers\n\n\nclass Struct:\n    """"""Masquerade a dictionary with a structure-like behavior.""""""\n\n    def __init__(self, attrs = None):\n        if attrs is None:\n            attrs = {}\n        self.__dict__[\'_attrs\'] = attrs\n    \n    def __getattr__(self, name):\n        try:\n            return self._attrs[name]\n        except KeyError:\n            raise AttributeError(name)\n\n    def __setattr__(self, name, value):\n        self._attrs[name] = value\n\n    def __str__(self):\n        return str(self._attrs)\n\n    def __repr__(self):\n        return repr(self._attrs)\n    \n\nclass ParseError(Exception):\n    """"""Raised when parsing to signal mismatches.""""""\n\n    def __init__(self, msg, line):\n        Exception.__init__(self)\n        self.msg = msg\n        # TODO: store more source line information\n        self.line = line\n\n    def __str__(self):\n        return \'%s: %r\' % (self.msg, self.line)\n\n\nclass Parser:\n    """"""Parser interface.""""""\n\n    stdinInput = True\n    multipleInput = False\n\n    def __init__(self):\n        pass\n\n    def parse(self):\n        raise NotImplementedError\n\n    \nclass JsonParser(Parser):\n    """"""Parser for a custom JSON representation of profile data.\n\n    See schema.json for details.\n    """"""\n\n\n    def __init__(self, stream):\n        Parser.__init__(self)\n        self.stream = stream\n\n    def parse(self):\n\n        obj = json.load(self.stream)\n\n        assert obj[\'version\'] == 0\n\n        profile = Profile()\n        profile[SAMPLES] = 0\n\n        fns = obj[\'functions\']\n\n        for functionIndex in range(len(fns)):\n            fn = fns[functionIndex]\n            function = Function(functionIndex, fn[\'name\'])\n            try:\n                function.module = fn[\'module\']\n            except KeyError:\n                pass\n            try:\n                function.process = fn[\'process\']\n            except KeyError:\n                pass\n            function[SAMPLES] = 0\n            profile.add_function(function)\n\n        for event in obj[\'events\']:\n            callchain = []\n\n            for functionIndex in event[\'callchain\']:\n                function = profile.functions[functionIndex]\n                callchain.append(function)\n\n            cost = event[\'cost\'][0]\n\n            callee = callchain[0]\n            callee[SAMPLES] += cost\n            profile[SAMPLES] += cost\n\n            for caller in callchain[1:]:\n                try:\n                    call = caller.calls[callee.id]\n                except KeyError:\n                    call = Call(callee.id)\n                    call[SAMPLES2] = cost\n                    caller.add_call(call)\n                else:\n                    call[SAMPLES2] += cost\n\n                callee = caller\n\n        if False:\n            profile.dump()\n\n        # compute derived data\n        profile.validate()\n        profile.find_cycles()\n        profile.ratio(TIME_RATIO, SAMPLES)\n        profile.call_ratios(SAMPLES2)\n        profile.integrate(TOTAL_TIME_RATIO, TIME_RATIO)\n\n        return profile\n\n\nclass LineParser(Parser):\n    """"""Base class for parsers that read line-based formats.""""""\n\n    def __init__(self, stream):\n        Parser.__init__(self)\n        self._stream = stream\n        self.__line = None\n        self.__eof = False\n        self.line_no = 0\n\n    def readline(self):\n        line = self._stream.readline()\n        if not line:\n            self.__line = \'\'\n            self.__eof = True\n        else:\n            self.line_no += 1\n        line = line.rstrip(\'\\r\\n\')\n        if not PYTHON_3:\n            encoding = self._stream.encoding\n            if encoding is None:\n                encoding = locale.getpreferredencoding()\n            line = line.decode(encoding)\n        self.__line = line\n\n    def lookahead(self):\n        assert self.__line is not None\n        return self.__line\n\n    def consume(self):\n        assert self.__line is not None\n        line = self.__line\n        self.readline()\n        return line\n\n    def eof(self):\n        assert self.__line is not None\n        return self.__eof\n\n\nXML_ELEMENT_START, XML_ELEMENT_END, XML_CHARACTER_DATA, XML_EOF = range(4)\n\n\nclass XmlToken:\n\n    def __init__(self, type, name_or_data, attrs = None, line = None, column = None):\n        assert type in (XML_ELEMENT_START, XML_ELEMENT_END, XML_CHARACTER_DATA, XML_EOF)\n        self.type = type\n        self.name_or_data = name_or_data\n        self.attrs = attrs\n        self.line = line\n        self.column = column\n\n    def __str__(self):\n        if self.type == XML_ELEMENT_START:\n            return \'<\' + self.name_or_data + \' ...>\'\n        if self.type == XML_ELEMENT_END:\n            return \'</\' + self.name_or_data + \'>\'\n        if self.type == XML_CHARACTER_DATA:\n            return self.name_or_data\n        if self.type == XML_EOF:\n            return \'end of file\'\n        assert 0\n\n\nclass XmlTokenizer:\n    """"""Expat based XML tokenizer.""""""\n\n    def __init__(self, fp, skip_ws = True):\n        self.fp = fp\n        self.tokens = []\n        self.index = 0\n        self.final = False\n        self.skip_ws = skip_ws\n        \n        self.character_pos = 0, 0\n        self.character_data = \'\'\n        \n        self.parser = xml.parsers.expat.ParserCreate()\n        self.parser.StartElementHandler  = self.handle_element_start\n        self.parser.EndElementHandler    = self.handle_element_end\n        self.parser.CharacterDataHandler = self.handle_character_data\n    \n    def handle_element_start(self, name, attributes):\n        self.finish_character_data()\n        line, column = self.pos()\n        token = XmlToken(XML_ELEMENT_START, name, attributes, line, column)\n        self.tokens.append(token)\n    \n    def handle_element_end(self, name):\n        self.finish_character_data()\n        line, column = self.pos()\n        token = XmlToken(XML_ELEMENT_END, name, None, line, column)\n        self.tokens.append(token)\n\n    def handle_character_data(self, data):\n        if not self.character_data:\n            self.character_pos = self.pos()\n        self.character_data += data\n    \n    def finish_character_data(self):\n        if self.character_data:\n            if not self.skip_ws or not self.character_data.isspace(): \n                line, column = self.character_pos\n                token = XmlToken(XML_CHARACTER_DATA, self.character_data, None, line, column)\n                self.tokens.append(token)\n            self.character_data = \'\'\n    \n    def next(self):\n        size = 16*1024\n        while self.index >= len(self.tokens) and not self.final:\n            self.tokens = []\n            self.index = 0\n            data = self.fp.read(size)\n            self.final = len(data) < size\n            self.parser.Parse(data, self.final)\n        if self.index >= len(self.tokens):\n            line, column = self.pos()\n            token = XmlToken(XML_EOF, None, None, line, column)\n        else:\n            token = self.tokens[self.index]\n            self.index += 1\n        return token\n\n    def pos(self):\n        return self.parser.CurrentLineNumber, self.parser.CurrentColumnNumber\n\n\nclass XmlTokenMismatch(Exception):\n\n    def __init__(self, expected, found):\n        Exception.__init__(self)\n        self.expected = expected\n        self.found = found\n\n    def __str__(self):\n        return \'%u:%u: %s expected, %s found\' % (self.found.line, self.found.column, str(self.expected), str(self.found))\n\n\nclass XmlParser(Parser):\n    """"""Base XML document parser.""""""\n\n    def __init__(self, fp):\n        Parser.__init__(self)\n        self.tokenizer = XmlTokenizer(fp)\n        self.consume()\n    \n    def consume(self):\n        self.token = self.tokenizer.next()\n\n    def match_element_start(self, name):\n        return self.token.type == XML_ELEMENT_START and self.token.name_or_data == name\n    \n    def match_element_end(self, name):\n        return self.token.type == XML_ELEMENT_END and self.token.name_or_data == name\n\n    def element_start(self, name):\n        while self.token.type == XML_CHARACTER_DATA:\n            self.consume()\n        if self.token.type != XML_ELEMENT_START:\n            raise XmlTokenMismatch(XmlToken(XML_ELEMENT_START, name), self.token)\n        if self.token.name_or_data != name:\n            raise XmlTokenMismatch(XmlToken(XML_ELEMENT_START, name), self.token)\n        attrs = self.token.attrs\n        self.consume()\n        return attrs\n    \n    def element_end(self, name):\n        while self.token.type == XML_CHARACTER_DATA:\n            self.consume()\n        if self.token.type != XML_ELEMENT_END:\n            raise XmlTokenMismatch(XmlToken(XML_ELEMENT_END, name), self.token)\n        if self.token.name_or_data != name:\n            raise XmlTokenMismatch(XmlToken(XML_ELEMENT_END, name), self.token)\n        self.consume()\n\n    def character_data(self, strip = True):\n        data = \'\'\n        while self.token.type == XML_CHARACTER_DATA:\n            data += self.token.name_or_data\n            self.consume()\n        if strip:\n            data = data.strip()\n        return data\n\n\nclass GprofParser(Parser):\n    """"""Parser for GNU gprof output.\n\n    See also:\n    - Chapter ""Interpreting gprof\'s Output"" from the GNU gprof manual\n      http://sourceware.org/binutils/docs-2.18/gprof/Call-Graph.html#Call-Graph\n    - File ""cg_print.c"" from the GNU gprof source code\n      http://sourceware.org/cgi-bin/cvsweb.cgi/~checkout~/src/gprof/cg_print.c?rev=1.12&cvsroot=src\n    """"""\n\n    def __init__(self, fp):\n        Parser.__init__(self)\n        self.fp = fp\n        self.functions = {}\n        self.cycles = {}\n\n    def readline(self):\n        line = self.fp.readline()\n        if not line:\n            sys.stderr.write(\'error: unexpected end of file\\n\')\n            sys.exit(1)\n        line = line.rstrip(\'\\r\\n\')\n        return line\n\n    _int_re = re.compile(r\'^\\d+$\')\n    _float_re = re.compile(r\'^\\d+\\.\\d+$\')\n\n    def translate(self, mo):\n        """"""Extract a structure from a match object, while translating the types in the process.""""""\n        attrs = {}\n        groupdict = mo.groupdict()\n        for name, value in compat_iteritems(groupdict):\n            if value is None:\n                value = None\n            elif self._int_re.match(value):\n                value = int(value)\n            elif self._float_re.match(value):\n                value = float(value)\n            attrs[name] = (value)\n        return Struct(attrs)\n\n    _cg_header_re = re.compile(\n        # original gprof header\n        r\'^\\s+called/total\\s+parents\\s*$|\' +\n        r\'^index\\s+%time\\s+self\\s+descendents\\s+called\\+self\\s+name\\s+index\\s*$|\' +\n        r\'^\\s+called/total\\s+children\\s*$|\' +\n        # GNU gprof header\n        r\'^index\\s+%\\s+time\\s+self\\s+children\\s+called\\s+name\\s*$\'\n    )\n\n    _cg_ignore_re = re.compile(\n        # spontaneous\n        r\'^\\s+<spontaneous>\\s*$|\'\n        # internal calls (such as ""mcount"")\n        r\'^.*\\((\\d+)\\)$\'\n    )\n\n    _cg_primary_re = re.compile(\n        r\'^\\[(?P<index>\\d+)\\]?\' + \n        r\'\\s+(?P<percentage_time>\\d+\\.\\d+)\' + \n        r\'\\s+(?P<self>\\d+\\.\\d+)\' + \n        r\'\\s+(?P<descendants>\\d+\\.\\d+)\' + \n        r\'\\s+(?:(?P<called>\\d+)(?:\\+(?P<called_self>\\d+))?)?\' + \n        r\'\\s+(?P<name>\\S.*?)\' +\n        r\'(?:\\s+<cycle\\s(?P<cycle>\\d+)>)?\' +\n        r\'\\s\\[(\\d+)\\]$\'\n    )\n\n    _cg_parent_re = re.compile(\n        r\'^\\s+(?P<self>\\d+\\.\\d+)?\' + \n        r\'\\s+(?P<descendants>\\d+\\.\\d+)?\' + \n        r\'\\s+(?P<called>\\d+)(?:/(?P<called_total>\\d+))?\' + \n        r\'\\s+(?P<name>\\S.*?)\' +\n        r\'(?:\\s+<cycle\\s(?P<cycle>\\d+)>)?\' +\n        r\'\\s\\[(?P<index>\\d+)\\]$\'\n    )\n\n    _cg_child_re = _cg_parent_re\n\n    _cg_cycle_header_re = re.compile(\n        r\'^\\[(?P<index>\\d+)\\]?\' + \n        r\'\\s+(?P<percentage_time>\\d+\\.\\d+)\' + \n        r\'\\s+(?P<self>\\d+\\.\\d+)\' + \n        r\'\\s+(?P<descendants>\\d+\\.\\d+)\' + \n        r\'\\s+(?:(?P<called>\\d+)(?:\\+(?P<called_self>\\d+))?)?\' + \n        r\'\\s+<cycle\\s(?P<cycle>\\d+)\\sas\\sa\\swhole>\' +\n        r\'\\s\\[(\\d+)\\]$\'\n    )\n\n    _cg_cycle_member_re = re.compile(\n        r\'^\\s+(?P<self>\\d+\\.\\d+)?\' + \n        r\'\\s+(?P<descendants>\\d+\\.\\d+)?\' + \n        r\'\\s+(?P<called>\\d+)(?:\\+(?P<called_self>\\d+))?\' + \n        r\'\\s+(?P<name>\\S.*?)\' +\n        r\'(?:\\s+<cycle\\s(?P<cycle>\\d+)>)?\' +\n        r\'\\s\\[(?P<index>\\d+)\\]$\'\n    )\n\n    _cg_sep_re = re.compile(r\'^--+$\')\n\n    def parse_function_entry(self, lines):\n        parents = []\n        children = []\n\n        while True:\n            if not lines:\n                sys.stderr.write(\'warning: unexpected end of entry\\n\')\n            line = lines.pop(0)\n            if line.startswith(\'[\'):\n                break\n        \n            # read function parent line\n            mo = self._cg_parent_re.match(line)\n            if not mo:\n                if self._cg_ignore_re.match(line):\n                    continue\n                sys.stderr.write(\'warning: unrecognized call graph entry: %r\\n\' % line)\n            else:\n                parent = self.translate(mo)\n                parents.append(parent)\n\n        # read primary line\n        mo = self._cg_primary_re.match(line)\n        if not mo:\n            sys.stderr.write(\'warning: unrecognized call graph entry: %r\\n\' % line)\n            return\n        else:\n            function = self.translate(mo)\n\n        while lines:\n            line = lines.pop(0)\n            \n            # read function subroutine line\n            mo = self._cg_child_re.match(line)\n            if not mo:\n                if self._cg_ignore_re.match(line):\n                    continue\n                sys.stderr.write(\'warning: unrecognized call graph entry: %r\\n\' % line)\n            else:\n                child = self.translate(mo)\n                children.append(child)\n        \n        function.parents = parents\n        function.children = children\n\n        self.functions[function.index] = function\n\n    def parse_cycle_entry(self, lines):\n\n        # read cycle header line\n        line = lines[0]\n        mo = self._cg_cycle_header_re.match(line)\n        if not mo:\n            sys.stderr.write(\'warning: unrecognized call graph entry: %r\\n\' % line)\n            return\n        cycle = self.translate(mo)\n\n        # read cycle member lines\n        cycle.functions = []\n        for line in lines[1:]:\n            mo = self._cg_cycle_member_re.match(line)\n            if not mo:\n                sys.stderr.write(\'warning: unrecognized call graph entry: %r\\n\' % line)\n                continue\n            call = self.translate(mo)\n            cycle.functions.append(call)\n        \n        self.cycles[cycle.cycle] = cycle\n\n    def parse_cg_entry(self, lines):\n        if lines[0].startswith(""[""):\n            self.parse_cycle_entry(lines)\n        else:\n            self.parse_function_entry(lines)\n\n    def parse_cg(self):\n        """"""Parse the call graph.""""""\n\n        # skip call graph header\n        while not self._cg_header_re.match(self.readline()):\n            pass\n        line = self.readline()\n        while self._cg_header_re.match(line):\n            line = self.readline()\n\n        # process call graph entries\n        entry_lines = []\n        while line != \'\\014\': # form feed\n            if line and not line.isspace():\n                if self._cg_sep_re.match(line):\n                    self.parse_cg_entry(entry_lines)\n                    entry_lines = []\n                else:\n                    entry_lines.append(line)            \n            line = self.readline()\n    \n    def parse(self):\n        self.parse_cg()\n        self.fp.close()\n\n        profile = Profile()\n        profile[TIME] = 0.0\n        \n        cycles = {}\n        for index in self.cycles:\n            cycles[index] = Cycle()\n\n        for entry in compat_itervalues(self.functions):\n            # populate the function\n            function = Function(entry.index, entry.name)\n            function[TIME] = entry.self\n            if entry.called is not None:\n                function.called = entry.called\n            if entry.called_self is not None:\n                call = Call(entry.index)\n                call[CALLS] = entry.called_self\n                function.called += entry.called_self\n            \n            # populate the function calls\n            for child in entry.children:\n                call = Call(child.index)\n                \n                assert child.called is not None\n                call[CALLS] = child.called\n\n                if child.index not in self.functions:\n                    # NOTE: functions that were never called but were discovered by gprof\'s \n                    # static call graph analysis dont have a call graph entry so we need\n                    # to add them here\n                    missing = Function(child.index, child.name)\n                    function[TIME] = 0.0\n                    function.called = 0\n                    profile.add_function(missing)\n\n                function.add_call(call)\n\n            profile.add_function(function)\n\n            if entry.cycle is not None:\n                try:\n                    cycle = cycles[entry.cycle]\n                except KeyError:\n                    sys.stderr.write(\'warning: <cycle %u as a whole> entry missing\\n\' % entry.cycle) \n                    cycle = Cycle()\n                    cycles[entry.cycle] = cycle\n                cycle.add_function(function)\n\n            profile[TIME] = profile[TIME] + function[TIME]\n\n        for cycle in compat_itervalues(cycles):\n            profile.add_cycle(cycle)\n\n        # Compute derived events\n        profile.validate()\n        profile.ratio(TIME_RATIO, TIME)\n        profile.call_ratios(CALLS)\n        profile.integrate(TOTAL_TIME, TIME)\n        profile.ratio(TOTAL_TIME_RATIO, TOTAL_TIME)\n\n        return profile\n\n\n# Clone&hack of GprofParser for VTune Amplifier XE 2013 gprof-cc output.\n# Tested only with AXE 2013 for Windows.\n#   - Use total times as reported by AXE.\n#   - In the absence of call counts, call ratios are faked from the relative\n#     proportions of total time.  This affects only the weighting of the calls.\n#   - Different header, separator, and end marker.\n#   - Extra whitespace after function names.\n#   - You get a full entry for <spontaneous>, which does not have parents.\n#   - Cycles do have parents.  These are saved but unused (as they are\n#     for functions).\n#   - Disambiguated ""unrecognized call graph entry"" error messages.\n# Notes:\n#   - Total time of functions as reported by AXE passes the val3 test.\n#   - CPU Time:Children in the input is sometimes a negative number.  This\n#     value goes to the variable descendants, which is unused.\n#   - The format of gprof-cc reports is unaffected by the use of\n#       -knob enable-call-counts=true (no call counts, ever), or\n#       -show-as=samples (results are quoted in seconds regardless).\nclass AXEParser(Parser):\n    ""Parser for VTune Amplifier XE 2013 gprof-cc report output.""\n\n    def __init__(self, fp):\n        Parser.__init__(self)\n        self.fp = fp\n        self.functions = {}\n        self.cycles = {}\n\n    def readline(self):\n        line = self.fp.readline()\n        if not line:\n            sys.stderr.write(\'error: unexpected end of file\\n\')\n            sys.exit(1)\n        line = line.rstrip(\'\\r\\n\')\n        return line\n\n    _int_re = re.compile(r\'^\\d+$\')\n    _float_re = re.compile(r\'^\\d+\\.\\d+$\')\n\n    def translate(self, mo):\n        """"""Extract a structure from a match object, while translating the types in the process.""""""\n        attrs = {}\n        groupdict = mo.groupdict()\n        for name, value in compat_iteritems(groupdict):\n            if value is None:\n                value = None\n            elif self._int_re.match(value):\n                value = int(value)\n            elif self._float_re.match(value):\n                value = float(value)\n            attrs[name] = (value)\n        return Struct(attrs)\n\n    _cg_header_re = re.compile(\n        \'^Index |\'\n        \'^-----+ \'\n    )\n\n    _cg_footer_re = re.compile(r\'^Index\\s+Function\\s*$\')\n\n    _cg_primary_re = re.compile(\n        r\'^\\[(?P<index>\\d+)\\]?\' + \n        r\'\\s+(?P<percentage_time>\\d+\\.\\d+)\' + \n        r\'\\s+(?P<self>\\d+\\.\\d+)\' + \n        r\'\\s+(?P<descendants>\\d+\\.\\d+)\' + \n        r\'\\s+(?P<name>\\S.*?)\' +\n        r\'(?:\\s+<cycle\\s(?P<cycle>\\d+)>)?\' +\n        r\'\\s+\\[(\\d+)\\]\' +\n        r\'\\s*$\'\n    )\n\n    _cg_parent_re = re.compile(\n        r\'^\\s+(?P<self>\\d+\\.\\d+)?\' + \n        r\'\\s+(?P<descendants>\\d+\\.\\d+)?\' + \n        r\'\\s+(?P<name>\\S.*?)\' +\n        r\'(?:\\s+<cycle\\s(?P<cycle>\\d+)>)?\' +\n        r\'(?:\\s+\\[(?P<index>\\d+)\\]\\s*)?\' +\n        r\'\\s*$\'\n    )\n\n    _cg_child_re = _cg_parent_re\n\n    _cg_cycle_header_re = re.compile(\n        r\'^\\[(?P<index>\\d+)\\]?\' + \n        r\'\\s+(?P<percentage_time>\\d+\\.\\d+)\' + \n        r\'\\s+(?P<self>\\d+\\.\\d+)\' + \n        r\'\\s+(?P<descendants>\\d+\\.\\d+)\' + \n        r\'\\s+<cycle\\s(?P<cycle>\\d+)\\sas\\sa\\swhole>\' +\n        r\'\\s+\\[(\\d+)\\]\' +\n        r\'\\s*$\'\n    )\n\n    _cg_cycle_member_re = re.compile(\n        r\'^\\s+(?P<self>\\d+\\.\\d+)?\' + \n        r\'\\s+(?P<descendants>\\d+\\.\\d+)?\' + \n        r\'\\s+(?P<name>\\S.*?)\' +\n        r\'(?:\\s+<cycle\\s(?P<cycle>\\d+)>)?\' +\n        r\'\\s+\\[(?P<index>\\d+)\\]\' +\n        r\'\\s*$\'\n    )\n\n    def parse_function_entry(self, lines):\n        parents = []\n        children = []\n\n        while True:\n            if not lines:\n                sys.stderr.write(\'warning: unexpected end of entry\\n\')\n                return\n            line = lines.pop(0)\n            if line.startswith(\'[\'):\n                break\n        \n            # read function parent line\n            mo = self._cg_parent_re.match(line)\n            if not mo:\n                sys.stderr.write(\'warning: unrecognized call graph entry (1): %r\\n\' % line)\n            else:\n                parent = self.translate(mo)\n                if parent.name != \'<spontaneous>\':\n                    parents.append(parent)\n\n        # read primary line\n        mo = self._cg_primary_re.match(line)\n        if not mo:\n            sys.stderr.write(\'warning: unrecognized call graph entry (2): %r\\n\' % line)\n            return\n        else:\n            function = self.translate(mo)\n\n        while lines:\n            line = lines.pop(0)\n            \n            # read function subroutine line\n            mo = self._cg_child_re.match(line)\n            if not mo:\n                sys.stderr.write(\'warning: unrecognized call graph entry (3): %r\\n\' % line)\n            else:\n                child = self.translate(mo)\n                if child.name != \'<spontaneous>\':\n                    children.append(child)\n\n        if function.name != \'<spontaneous>\':\n            function.parents = parents\n            function.children = children\n\n            self.functions[function.index] = function\n\n    def parse_cycle_entry(self, lines):\n\n        # Process the parents that were not there in gprof format.\n        parents = []\n        while True:\n            if not lines:\n                sys.stderr.write(\'warning: unexpected end of cycle entry\\n\')\n                return\n            line = lines.pop(0)\n            if line.startswith(\'[\'):\n                break\n            mo = self._cg_parent_re.match(line)\n            if not mo:\n                sys.stderr.write(\'warning: unrecognized call graph entry (6): %r\\n\' % line)\n            else:\n                parent = self.translate(mo)\n                if parent.name != \'<spontaneous>\':\n                    parents.append(parent)\n\n        # read cycle header line\n        mo = self._cg_cycle_header_re.match(line)\n        if not mo:\n            sys.stderr.write(\'warning: unrecognized call graph entry (4): %r\\n\' % line)\n            return\n        cycle = self.translate(mo)\n\n        # read cycle member lines\n        cycle.functions = []\n        for line in lines[1:]:\n            mo = self._cg_cycle_member_re.match(line)\n            if not mo:\n                sys.stderr.write(\'warning: unrecognized call graph entry (5): %r\\n\' % line)\n                continue\n            call = self.translate(mo)\n            cycle.functions.append(call)\n        \n        cycle.parents = parents\n        self.cycles[cycle.cycle] = cycle\n\n    def parse_cg_entry(self, lines):\n        if any(""as a whole"" in linelooper for linelooper in lines):\n            self.parse_cycle_entry(lines)\n        else:\n            self.parse_function_entry(lines)\n\n    def parse_cg(self):\n        """"""Parse the call graph.""""""\n\n        # skip call graph header\n        line = self.readline()\n        while self._cg_header_re.match(line):\n            line = self.readline()\n\n        # process call graph entries\n        entry_lines = []\n        # An EOF in readline terminates the program without returning.\n        while not self._cg_footer_re.match(line):\n            if line.isspace():\n                self.parse_cg_entry(entry_lines)\n                entry_lines = []\n            else:\n                entry_lines.append(line)            \n            line = self.readline()\n\n    def parse(self):\n        sys.stderr.write(\'warning: for axe format, edge weights are unreliable estimates derived from function total times.\\n\')\n        self.parse_cg()\n        self.fp.close()\n\n        profile = Profile()\n        profile[TIME] = 0.0\n        \n        cycles = {}\n        for index in self.cycles:\n            cycles[index] = Cycle()\n\n        for entry in compat_itervalues(self.functions):\n            # populate the function\n            function = Function(entry.index, entry.name)\n            function[TIME] = entry.self\n            function[TOTAL_TIME_RATIO] = entry.percentage_time / 100.0\n            \n            # populate the function calls\n            for child in entry.children:\n                call = Call(child.index)\n                # The following bogus value affects only the weighting of\n                # the calls.\n                call[TOTAL_TIME_RATIO] = function[TOTAL_TIME_RATIO]\n\n                if child.index not in self.functions:\n                    # NOTE: functions that were never called but were discovered by gprof\'s \n                    # static call graph analysis dont have a call graph entry so we need\n                    # to add them here\n                    # FIXME: Is this applicable?\n                    missing = Function(child.index, child.name)\n                    function[TIME] = 0.0\n                    profile.add_function(missing)\n\n                function.add_call(call)\n\n            profile.add_function(function)\n\n            if entry.cycle is not None:\n                try:\n                    cycle = cycles[entry.cycle]\n                except KeyError:\n                    sys.stderr.write(\'warning: <cycle %u as a whole> entry missing\\n\' % entry.cycle) \n                    cycle = Cycle()\n                    cycles[entry.cycle] = cycle\n                cycle.add_function(function)\n\n            profile[TIME] = profile[TIME] + function[TIME]\n\n        for cycle in compat_itervalues(cycles):\n            profile.add_cycle(cycle)\n\n        # Compute derived events.\n        profile.validate()\n        profile.ratio(TIME_RATIO, TIME)\n        # Lacking call counts, fake call ratios based on total times.\n        profile.call_ratios(TOTAL_TIME_RATIO)\n        # The TOTAL_TIME_RATIO of functions is already set.  Propagate that\n        # total time to the calls.  (TOTAL_TIME is neither set nor used.)\n        for function in compat_itervalues(profile.functions):\n            for call in compat_itervalues(function.calls):\n                if call.ratio is not None:\n                    callee = profile.functions[call.callee_id]\n                    call[TOTAL_TIME_RATIO] = call.ratio * callee[TOTAL_TIME_RATIO]\n\n        return profile\n\n\nclass CallgrindParser(LineParser):\n    """"""Parser for valgrind\'s callgrind tool.\n    \n    See also:\n    - http://valgrind.org/docs/manual/cl-format.html\n    """"""\n\n    _call_re = re.compile(r\'^calls=\\s*(\\d+)\\s+((\\d+|\\+\\d+|-\\d+|\\*)\\s+)+$\')\n\n    def __init__(self, infile):\n        LineParser.__init__(self, infile)\n\n        # Textual positions\n        self.position_ids = {}\n        self.positions = {}\n\n        # Numeric positions\n        self.num_positions = 1\n        self.cost_positions = [\'line\']\n        self.last_positions = [0]\n\n        # Events\n        self.num_events = 0\n        self.cost_events = []\n\n        self.profile = Profile()\n        self.profile[SAMPLES] = 0\n\n    def parse(self):\n        # read lookahead\n        self.readline()\n\n        self.parse_key(\'version\')\n        self.parse_key(\'creator\')\n        while self.parse_part():\n            pass\n        if not self.eof():\n            sys.stderr.write(\'warning: line %u: unexpected line\\n\' % self.line_no)\n            sys.stderr.write(\'%s\\n\' % self.lookahead())\n\n        # compute derived data\n        self.profile.validate()\n        self.profile.find_cycles()\n        self.profile.ratio(TIME_RATIO, SAMPLES)\n        self.profile.call_ratios(SAMPLES2)\n        self.profile.integrate(TOTAL_TIME_RATIO, TIME_RATIO)\n\n        return self.profile\n\n    def parse_part(self):\n        if not self.parse_header_line():\n            return False\n        while self.parse_header_line():\n            pass\n        if not self.parse_body_line():\n            return False\n        while self.parse_body_line():\n            pass\n        return True\n\n    def parse_header_line(self):\n        return \\\n            self.parse_empty() or \\\n            self.parse_comment() or \\\n            self.parse_part_detail() or \\\n            self.parse_description() or \\\n            self.parse_event_specification() or \\\n            self.parse_cost_line_def() or \\\n            self.parse_cost_summary()\n\n    _detail_keys = set((\'cmd\', \'pid\', \'thread\', \'part\'))\n\n    def parse_part_detail(self):\n        return self.parse_keys(self._detail_keys)\n\n    def parse_description(self):\n        return self.parse_key(\'desc\') is not None\n\n    def parse_event_specification(self):\n        event = self.parse_key(\'event\')\n        if event is None:\n            return False\n        return True\n\n    def parse_cost_line_def(self):\n        pair = self.parse_keys((\'events\', \'positions\'))\n        if pair is None:\n            return False\n        key, value = pair\n        items = value.split()\n        if key == \'events\':\n            self.num_events = len(items)\n            self.cost_events = items\n        if key == \'positions\':\n            self.num_positions = len(items)\n            self.cost_positions = items\n            self.last_positions = [0]*self.num_positions\n        return True\n\n    def parse_cost_summary(self):\n        pair = self.parse_keys((\'summary\', \'totals\'))\n        if pair is None:\n            return False\n        return True\n\n    def parse_body_line(self):\n        return \\\n            self.parse_empty() or \\\n            self.parse_comment() or \\\n            self.parse_cost_line() or \\\n            self.parse_position_spec() or \\\n            self.parse_association_spec()\n\n    __subpos_re = r\'(0x[0-9a-fA-F]+|\\d+|\\+\\d+|-\\d+|\\*)\'\n    _cost_re = re.compile(r\'^\' + \n        __subpos_re + r\'( +\' + __subpos_re + r\')*\' +\n        r\'( +\\d+)*\' +\n    \'$\')\n\n    def parse_cost_line(self, calls=None):\n        line = self.lookahead().rstrip()\n        mo = self._cost_re.match(line)\n        if not mo:\n            return False\n\n        function = self.get_function()\n\n        if calls is None:\n            # Unlike other aspects, call object (cob) is relative not to the\n            # last call object, but to the caller\'s object (ob), so try to\n            # update it when processing a functions cost line\n            try:\n                self.positions[\'cob\'] = self.positions[\'ob\']\n            except KeyError:\n                pass\n\n        values = line.split()\n        assert len(values) <= self.num_positions + self.num_events\n\n        positions = values[0 : self.num_positions]\n        events = values[self.num_positions : ]\n        events += [\'0\']*(self.num_events - len(events))\n\n        for i in range(self.num_positions):\n            position = positions[i]\n            if position == \'*\':\n                position = self.last_positions[i]\n            elif position[0] in \'-+\':\n                position = self.last_positions[i] + int(position)\n            elif position.startswith(\'0x\'):\n                position = int(position, 16)\n            else:\n                position = int(position)\n            self.last_positions[i] = position\n\n        events = [float(event) for event in events]\n\n        if calls is None:\n            function[SAMPLES] += events[0] \n            self.profile[SAMPLES] += events[0]\n        else:\n            callee = self.get_callee()\n            callee.called += calls\n    \n            try:\n                call = function.calls[callee.id]\n            except KeyError:\n                call = Call(callee.id)\n                call[CALLS] = calls\n                call[SAMPLES2] = events[0]\n                function.add_call(call)\n            else:\n                call[CALLS] += calls\n                call[SAMPLES2] += events[0]\n\n        self.consume()\n        return True\n\n    def parse_association_spec(self):\n        line = self.lookahead()\n        if not line.startswith(\'calls=\'):\n            return False\n\n        _, values = line.split(\'=\', 1)\n        values = values.strip().split()\n        calls = int(values[0])\n        call_position = values[1:]\n        self.consume()\n\n        self.parse_cost_line(calls)\n\n        return True\n\n    _position_re = re.compile(r\'^(?P<position>[cj]?(?:ob|fl|fi|fe|fn))=\\s*(?:\\((?P<id>\\d+)\\))?(?:\\s*(?P<name>.+))?\')\n\n    _position_table_map = {\n        \'ob\': \'ob\',\n        \'fl\': \'fl\',\n        \'fi\': \'fl\',\n        \'fe\': \'fl\',\n        \'fn\': \'fn\',\n        \'cob\': \'ob\',\n        \'cfl\': \'fl\',\n        \'cfi\': \'fl\',\n        \'cfe\': \'fl\',\n        \'cfn\': \'fn\',\n        \'jfi\': \'fl\',\n    }\n\n    _position_map = {\n        \'ob\': \'ob\',\n        \'fl\': \'fl\',\n        \'fi\': \'fl\',\n        \'fe\': \'fl\',\n        \'fn\': \'fn\',\n        \'cob\': \'cob\',\n        \'cfl\': \'cfl\',\n        \'cfi\': \'cfl\',\n        \'cfe\': \'cfl\',\n        \'cfn\': \'cfn\',\n        \'jfi\': \'jfi\',\n    }\n\n    def parse_position_spec(self):\n        line = self.lookahead()\n        \n        if line.startswith(\'jump=\') or line.startswith(\'jcnd=\'):\n            self.consume()\n            return True\n\n        mo = self._position_re.match(line)\n        if not mo:\n            return False\n\n        position, id, name = mo.groups()\n        if id:\n            table = self._position_table_map[position]\n            if name:\n                self.position_ids[(table, id)] = name\n            else:\n                name = self.position_ids.get((table, id), \'\')\n        self.positions[self._position_map[position]] = name\n\n        self.consume()\n        return True\n\n    def parse_empty(self):\n        if self.eof():\n            return False\n        line = self.lookahead()\n        if line.strip():\n            return False\n        self.consume()\n        return True\n\n    def parse_comment(self):\n        line = self.lookahead()\n        if not line.startswith(\'#\'):\n            return False\n        self.consume()\n        return True\n\n    _key_re = re.compile(r\'^(\\w+):\')\n\n    def parse_key(self, key):\n        pair = self.parse_keys((key,))\n        if not pair:\n            return None\n        key, value = pair\n        return value\n\n    def parse_keys(self, keys):\n        line = self.lookahead()\n        mo = self._key_re.match(line)\n        if not mo:\n            return None\n        key, value = line.split(\':\', 1)\n        if key not in keys:\n            return None\n        value = value.strip()\n        self.consume()\n        return key, value\n\n    def make_function(self, module, filename, name):\n        # FIXME: module and filename are not being tracked reliably\n        #id = \'|\'.join((module, filename, name))\n        id = name\n        try:\n            function = self.profile.functions[id]\n        except KeyError:\n            function = Function(id, name)\n            if module:\n                function.module = os.path.basename(module)\n            function[SAMPLES] = 0\n            function.called = 0\n            self.profile.add_function(function)\n        return function\n\n    def get_function(self):\n        module = self.positions.get(\'ob\', \'\')\n        filename = self.positions.get(\'fl\', \'\') \n        function = self.positions.get(\'fn\', \'\') \n        return self.make_function(module, filename, function)\n\n    def get_callee(self):\n        module = self.positions.get(\'cob\', \'\')\n        filename = self.positions.get(\'cfi\', \'\') \n        function = self.positions.get(\'cfn\', \'\') \n        return self.make_function(module, filename, function)\n\n\nclass PerfParser(LineParser):\n    """"""Parser for linux perf callgraph output.\n\n    It expects output generated with\n\n        perf record -g\n        perf script | gprof2dot.py --format=perf\n    """"""\n\n    def __init__(self, infile):\n        LineParser.__init__(self, infile)\n        self.profile = Profile()\n\n    def readline(self):\n        # Override LineParser.readline to ignore comment lines\n        while True:\n            LineParser.readline(self)\n            if self.eof() or not self.lookahead().startswith(\'#\'):\n                break\n\n    def parse(self):\n        # read lookahead\n        self.readline()\n\n        profile = self.profile\n        profile[SAMPLES] = 0\n        while not self.eof():\n            self.parse_event()\n\n        # compute derived data\n        profile.validate()\n        profile.find_cycles()\n        profile.ratio(TIME_RATIO, SAMPLES)\n        profile.call_ratios(SAMPLES2)\n        if totalMethod == ""callratios"":\n            # Heuristic approach.  TOTAL_SAMPLES is unused.\n            profile.integrate(TOTAL_TIME_RATIO, TIME_RATIO)\n        elif totalMethod == ""callstacks"":\n            # Use the actual call chains for functions.\n            profile[TOTAL_SAMPLES] = profile[SAMPLES]\n            profile.ratio(TOTAL_TIME_RATIO, TOTAL_SAMPLES)\n            # Then propagate that total time to the calls.\n            for function in compat_itervalues(profile.functions):\n                for call in compat_itervalues(function.calls):\n                    if call.ratio is not None:\n                        callee = profile.functions[call.callee_id]\n                        call[TOTAL_TIME_RATIO] = call.ratio * callee[TOTAL_TIME_RATIO]\n        else:\n            assert False\n\n        return profile\n\n    def parse_event(self):\n        if self.eof():\n            return\n\n        line = self.consume()\n        assert line\n\n        callchain = self.parse_callchain()\n        if not callchain:\n            return\n\n        callee = callchain[0]\n        callee[SAMPLES] += 1\n        self.profile[SAMPLES] += 1\n\n        for caller in callchain[1:]:\n            try:\n                call = caller.calls[callee.id]\n            except KeyError:\n                call = Call(callee.id)\n                call[SAMPLES2] = 1\n                caller.add_call(call)\n            else:\n                call[SAMPLES2] += 1\n\n            callee = caller\n\n        # Increment TOTAL_SAMPLES only once on each function.\n        stack = set(callchain)\n        for function in stack:\n            function[TOTAL_SAMPLES] += 1\n\n    def parse_callchain(self):\n        callchain = []\n        while self.lookahead():\n            function = self.parse_call()\n            if function is None:\n                break\n            callchain.append(function)\n        if self.lookahead() == \'\':\n            self.consume()\n        return callchain\n\n    call_re = re.compile(r\'^\\s+(?P<address>[0-9a-fA-F]+)\\s+(?P<symbol>.*)\\s+\\((?P<module>.*)\\)$\')\n\n    def parse_call(self):\n        line = self.consume()\n        mo = self.call_re.match(line)\n        assert mo\n        if not mo:\n            return None\n\n        function_name = mo.group(\'symbol\')\n        if not function_name or function_name == \'[unknown]\':\n            function_name = mo.group(\'address\')\n\n        module = mo.group(\'module\')\n\n        function_id = function_name + \':\' + module\n\n        try:\n            function = self.profile.functions[function_id]\n        except KeyError:\n            function = Function(function_id, function_name)\n            function.module = os.path.basename(module)\n            function[SAMPLES] = 0\n            function[TOTAL_SAMPLES] = 0\n            self.profile.add_function(function)\n\n        return function\n\n\nclass OprofileParser(LineParser):\n    """"""Parser for oprofile callgraph output.\n    \n    See also:\n    - http://oprofile.sourceforge.net/doc/opreport.html#opreport-callgraph\n    """"""\n\n    _fields_re = {\n        \'samples\': r\'(\\d+)\',\n        \'%\': r\'(\\S+)\',\n        \'linenr info\': r\'(?P<source>\\(no location information\\)|\\S+:\\d+)\',\n        \'image name\': r\'(?P<image>\\S+(?:\\s\\(tgid:[^)]*\\))?)\',\n        \'app name\': r\'(?P<application>\\S+)\',\n        \'symbol name\': r\'(?P<symbol>\\(no symbols\\)|.+?)\',\n    }\n\n    def __init__(self, infile):\n        LineParser.__init__(self, infile)\n        self.entries = {}\n        self.entry_re = None\n\n    def add_entry(self, callers, function, callees):\n        try:\n            entry = self.entries[function.id]\n        except KeyError:\n            self.entries[function.id] = (callers, function, callees)\n        else:\n            callers_total, function_total, callees_total = entry\n            self.update_subentries_dict(callers_total, callers)\n            function_total.samples += function.samples\n            self.update_subentries_dict(callees_total, callees)\n    \n    def update_subentries_dict(self, totals, partials):\n        for partial in compat_itervalues(partials):\n            try:\n                total = totals[partial.id]\n            except KeyError:\n                totals[partial.id] = partial\n            else:\n                total.samples += partial.samples\n        \n    def parse(self):\n        # read lookahead\n        self.readline()\n\n        self.parse_header()\n        while self.lookahead():\n            self.parse_entry()\n\n        profile = Profile()\n\n        reverse_call_samples = {}\n        \n        # populate the profile\n        profile[SAMPLES] = 0\n        for _callers, _function, _callees in compat_itervalues(self.entries):\n            function = Function(_function.id, _function.name)\n            function[SAMPLES] = _function.samples\n            profile.add_function(function)\n            profile[SAMPLES] += _function.samples\n\n            if _function.application:\n                function.process = os.path.basename(_function.application)\n            if _function.image:\n                function.module = os.path.basename(_function.image)\n\n            total_callee_samples = 0\n            for _callee in compat_itervalues(_callees):\n                total_callee_samples += _callee.samples\n\n            for _callee in compat_itervalues(_callees):\n                if not _callee.self:\n                    call = Call(_callee.id)\n                    call[SAMPLES2] = _callee.samples\n                    function.add_call(call)\n                \n        # compute derived data\n        profile.validate()\n        profile.find_cycles()\n        profile.ratio(TIME_RATIO, SAMPLES)\n        profile.call_ratios(SAMPLES2)\n        profile.integrate(TOTAL_TIME_RATIO, TIME_RATIO)\n\n        return profile\n\n    def parse_header(self):\n        while not self.match_header():\n            self.consume()\n        line = self.lookahead()\n        fields = re.split(r\'\\s\\s+\', line)\n        entry_re = r\'^\\s*\' + r\'\\s+\'.join([self._fields_re[field] for field in fields]) + r\'(?P<self>\\s+\\[self\\])?$\'\n        self.entry_re = re.compile(entry_re)\n        self.skip_separator()\n\n    def parse_entry(self):\n        callers = self.parse_subentries()\n        if self.match_primary():\n            function = self.parse_subentry()\n            if function is not None:\n                callees = self.parse_subentries()\n                self.add_entry(callers, function, callees)\n        self.skip_separator()\n\n    def parse_subentries(self):\n        subentries = {}\n        while self.match_secondary():\n            subentry = self.parse_subentry()\n            subentries[subentry.id] = subentry\n        return subentries\n\n    def parse_subentry(self):\n        entry = Struct()\n        line = self.consume()\n        mo = self.entry_re.match(line)\n        if not mo:\n            raise ParseError(\'failed to parse\', line)\n        fields = mo.groupdict()\n        entry.samples = int(mo.group(1))\n        if \'source\' in fields and fields[\'source\'] != \'(no location information)\':\n            source = fields[\'source\']\n            filename, lineno = source.split(\':\')\n            entry.filename = filename\n            entry.lineno = int(lineno)\n        else:\n            source = \'\'\n            entry.filename = None\n            entry.lineno = None\n        entry.image = fields.get(\'image\', \'\')\n        entry.application = fields.get(\'application\', \'\')\n        if \'symbol\' in fields and fields[\'symbol\'] != \'(no symbols)\':\n            entry.symbol = fields[\'symbol\']\n        else:\n            entry.symbol = \'\'\n        if entry.symbol.startswith(\'""\') and entry.symbol.endswith(\'""\'):\n            entry.symbol = entry.symbol[1:-1]\n        entry.id = \':\'.join((entry.application, entry.image, source, entry.symbol))\n        entry.self = fields.get(\'self\', None) != None\n        if entry.self:\n            entry.id += \':self\'\n        if entry.symbol:\n            entry.name = entry.symbol\n        else:\n            entry.name = entry.image\n        return entry\n\n    def skip_separator(self):\n        while not self.match_separator():\n            self.consume()\n        self.consume()\n\n    def match_header(self):\n        line = self.lookahead()\n        return line.startswith(\'samples\')\n\n    def match_separator(self):\n        line = self.lookahead()\n        return line == \'-\'*len(line)\n\n    def match_primary(self):\n        line = self.lookahead()\n        return not line[:1].isspace()\n    \n    def match_secondary(self):\n        line = self.lookahead()\n        return line[:1].isspace()\n\n\nclass HProfParser(LineParser):\n    """"""Parser for java hprof output\n    \n    See also:\n    - http://java.sun.com/developer/technicalArticles/Programming/HPROF.html\n    """"""\n\n    trace_re = re.compile(r\'\\t(.*)\\((.*):(.*)\\)\')\n    trace_id_re = re.compile(r\'^TRACE (\\d+):$\')\n\n    def __init__(self, infile):\n        LineParser.__init__(self, infile)\n        self.traces = {}\n        self.samples = {}\n\n    def parse(self):\n        # read lookahead\n        self.readline()\n\n        while not self.lookahead().startswith(\'------\'): self.consume()\n        while not self.lookahead().startswith(\'TRACE \'): self.consume()\n\n        self.parse_traces()\n\n        while not self.lookahead().startswith(\'CPU\'):\n            self.consume()\n\n        self.parse_samples()\n\n        # populate the profile\n        profile = Profile()\n        profile[SAMPLES] = 0\n\n        functions = {}\n\n        # build up callgraph\n        for id, trace in compat_iteritems(self.traces):\n            if not id in self.samples: continue\n            mtime = self.samples[id][0]\n            last = None\n\n            for func, file, line in trace:\n                if not func in functions:\n                    function = Function(func, func)\n                    function[SAMPLES] = 0\n                    profile.add_function(function)\n                    functions[func] = function\n\n                function = functions[func]\n                # allocate time to the deepest method in the trace\n                if not last:\n                    function[SAMPLES] += mtime\n                    profile[SAMPLES] += mtime\n                else:\n                    c = function.get_call(last)\n                    c[SAMPLES2] += mtime\n\n                last = func\n\n        # compute derived data\n        profile.validate()\n        profile.find_cycles()\n        profile.ratio(TIME_RATIO, SAMPLES)\n        profile.call_ratios(SAMPLES2)\n        profile.integrate(TOTAL_TIME_RATIO, TIME_RATIO)\n\n        return profile\n\n    def parse_traces(self):\n        while self.lookahead().startswith(\'TRACE \'):\n            self.parse_trace()\n\n    def parse_trace(self):\n        l = self.consume()\n        mo = self.trace_id_re.match(l)\n        tid = mo.group(1)\n        last = None\n        trace = []\n\n        while self.lookahead().startswith(\'\\t\'):\n            l = self.consume()\n            match = self.trace_re.search(l)\n            if not match:\n                #sys.stderr.write(\'Invalid line: %s\\n\' % l)\n                break\n            else:\n                function_name, file, line = match.groups()\n                trace += [(function_name, file, line)]\n\n        self.traces[int(tid)] = trace\n\n    def parse_samples(self):\n        self.consume()\n        self.consume()\n\n        while not self.lookahead().startswith(\'CPU\'):\n            rank, percent_self, percent_accum, count, traceid, method = self.lookahead().split()\n            self.samples[int(traceid)] = (int(count), method)\n            self.consume()\n\n\nclass SysprofParser(XmlParser):\n\n    def __init__(self, stream):\n        XmlParser.__init__(self, stream)\n\n    def parse(self):\n        objects = {}\n        nodes = {}\n\n        self.element_start(\'profile\')\n        while self.token.type == XML_ELEMENT_START:\n            if self.token.name_or_data == \'objects\':\n                assert not objects\n                objects = self.parse_items(\'objects\')\n            elif self.token.name_or_data == \'nodes\':\n                assert not nodes\n                nodes = self.parse_items(\'nodes\')\n            else:\n                self.parse_value(self.token.name_or_data)\n        self.element_end(\'profile\')\n\n        return self.build_profile(objects, nodes)\n\n    def parse_items(self, name):\n        assert name[-1] == \'s\'\n        items = {}\n        self.element_start(name)\n        while self.token.type == XML_ELEMENT_START:\n            id, values = self.parse_item(name[:-1])\n            assert id not in items\n            items[id] = values\n        self.element_end(name)\n        return items\n\n    def parse_item(self, name):\n        attrs = self.element_start(name)\n        id = int(attrs[\'id\'])\n        values = self.parse_values()\n        self.element_end(name)\n        return id, values\n\n    def parse_values(self):\n        values = {}\n        while self.token.type == XML_ELEMENT_START:\n            name = self.token.name_or_data\n            value = self.parse_value(name)\n            assert name not in values\n            values[name] = value\n        return values\n\n    def parse_value(self, tag):\n        self.element_start(tag)\n        value = self.character_data()\n        self.element_end(tag)\n        if value.isdigit():\n            return int(value)\n        if value.startswith(\'""\') and value.endswith(\'""\'):\n            return value[1:-1]\n        return value\n\n    def build_profile(self, objects, nodes):\n        profile = Profile()\n        \n        profile[SAMPLES] = 0\n        for id, object in compat_iteritems(objects):\n            # Ignore fake objects (process names, modules, ""Everything"", ""kernel"", etc.)\n            if object[\'self\'] == 0:\n                continue\n\n            function = Function(id, object[\'name\'])\n            function[SAMPLES] = object[\'self\']\n            profile.add_function(function)\n            profile[SAMPLES] += function[SAMPLES]\n\n        for id, node in compat_iteritems(nodes):\n            # Ignore fake calls\n            if node[\'self\'] == 0:\n                continue\n\n            # Find a non-ignored parent\n            parent_id = node[\'parent\']\n            while parent_id != 0:\n                parent = nodes[parent_id]\n                caller_id = parent[\'object\']\n                if objects[caller_id][\'self\'] != 0:\n                    break\n                parent_id = parent[\'parent\']\n            if parent_id == 0:\n                continue\n\n            callee_id = node[\'object\']\n\n            assert objects[caller_id][\'self\']\n            assert objects[callee_id][\'self\']\n\n            function = profile.functions[caller_id]\n\n            samples = node[\'self\']\n            try:\n                call = function.calls[callee_id]\n            except KeyError:\n                call = Call(callee_id)\n                call[SAMPLES2] = samples\n                function.add_call(call)\n            else:\n                call[SAMPLES2] += samples\n\n        # Compute derived events\n        profile.validate()\n        profile.find_cycles()\n        profile.ratio(TIME_RATIO, SAMPLES)\n        profile.call_ratios(SAMPLES2)\n        profile.integrate(TOTAL_TIME_RATIO, TIME_RATIO)\n\n        return profile\n\n\nclass XPerfParser(Parser):\n    """"""Parser for CSVs generted by XPerf, from Microsoft Windows Performance Tools.\n    """"""\n\n    def __init__(self, stream):\n        Parser.__init__(self)\n        self.stream = stream\n        self.profile = Profile()\n        self.profile[SAMPLES] = 0\n        self.column = {}\n\n    def parse(self):\n        import csv\n        reader = csv.reader(\n            self.stream, \n            delimiter = \',\',\n            quotechar = None,\n            escapechar = None,\n            doublequote = False,\n            skipinitialspace = True,\n            lineterminator = \'\\r\\n\',\n            quoting = csv.QUOTE_NONE)\n        header = True\n        for row in reader:\n            if header:\n                self.parse_header(row)\n                header = False\n            else:\n                self.parse_row(row)\n                \n        # compute derived data\n        self.profile.validate()\n        self.profile.find_cycles()\n        self.profile.ratio(TIME_RATIO, SAMPLES)\n        self.profile.call_ratios(SAMPLES2)\n        self.profile.integrate(TOTAL_TIME_RATIO, TIME_RATIO)\n\n        return self.profile\n\n    def parse_header(self, row):\n        for column in range(len(row)):\n            name = row[column]\n            assert name not in self.column\n            self.column[name] = column\n\n    def parse_row(self, row):\n        fields = {}\n        for name, column in compat_iteritems(self.column):\n            value = row[column]\n            for factory in int, float:\n                try:\n                    value = factory(value)\n                except ValueError:\n                    pass\n                else:\n                    break\n            fields[name] = value\n        \n        process = fields[\'Process Name\']\n        symbol = fields[\'Module\'] + \'!\' + fields[\'Function\']\n        weight = fields[\'Weight\']\n        count = fields[\'Count\']\n\n        if process == \'Idle\':\n            return\n\n        function = self.get_function(process, symbol)\n        function[SAMPLES] += weight * count\n        self.profile[SAMPLES] += weight * count\n\n        stack = fields[\'Stack\']\n        if stack != \'?\':\n            stack = stack.split(\'/\')\n            assert stack[0] == \'[Root]\'\n            if stack[-1] != symbol:\n                # XXX: some cases the sampled function does not appear in the stack\n                stack.append(symbol)\n            caller = None\n            for symbol in stack[1:]:\n                callee = self.get_function(process, symbol)\n                if caller is not None:\n                    try:\n                        call = caller.calls[callee.id]\n                    except KeyError:\n                        call = Call(callee.id)\n                        call[SAMPLES2] = count\n                        caller.add_call(call)\n                    else:\n                        call[SAMPLES2] += count\n                caller = callee\n\n    def get_function(self, process, symbol):\n        function_id = process + \'!\' + symbol\n\n        try:\n            function = self.profile.functions[function_id]\n        except KeyError:\n            module, name = symbol.split(\'!\', 1)\n            function = Function(function_id, name)\n            function.process = process\n            function.module = module\n            function[SAMPLES] = 0\n            self.profile.add_function(function)\n\n        return function\n\n\nclass SleepyParser(Parser):\n    """"""Parser for GNU gprof output.\n\n    See also:\n    - http://www.codersnotes.com/sleepy/\n    - http://sleepygraph.sourceforge.net/\n    """"""\n\n    stdinInput = False\n\n    def __init__(self, filename):\n        Parser.__init__(self)\n\n        from zipfile import ZipFile\n\n        self.database = ZipFile(filename)\n\n        self.symbols = {}\n        self.calls = {}\n\n        self.profile = Profile()\n    \n    _symbol_re = re.compile(\n        r\'^(?P<id>\\w+)\' + \n        r\'\\s+""(?P<module>[^""]*)""\' + \n        r\'\\s+""(?P<procname>[^""]*)""\' + \n        r\'\\s+""(?P<sourcefile>[^""]*)""\' + \n        r\'\\s+(?P<sourceline>\\d+)$\'\n    )\n\n    def openEntry(self, name):\n        # Some versions of verysleepy use lowercase filenames\n        for database_name in self.database.namelist():\n            if name.lower() == database_name.lower():\n                name = database_name\n                break\n\n        return self.database.open(name, \'rU\')\n\n    def parse_symbols(self):\n        for line in self.openEntry(\'Symbols.txt\'):\n            line = line.decode(\'UTF-8\')\n\n            mo = self._symbol_re.match(line)\n            if mo:\n                symbol_id, module, procname, sourcefile, sourceline = mo.groups()\n    \n                function_id = \':\'.join([module, procname])\n\n                try:\n                    function = self.profile.functions[function_id]\n                except KeyError:\n                    function = Function(function_id, procname)\n                    function.module = module\n                    function[SAMPLES] = 0\n                    self.profile.add_function(function)\n\n                self.symbols[symbol_id] = function\n\n    def parse_callstacks(self):\n        for line in self.openEntry(\'Callstacks.txt\'):\n            line = line.decode(\'UTF-8\')\n\n            fields = line.split()\n            samples = float(fields[0])\n            callstack = fields[1:]\n\n            callstack = [self.symbols[symbol_id] for symbol_id in callstack]\n\n            callee = callstack[0]\n\n            callee[SAMPLES] += samples\n            self.profile[SAMPLES] += samples\n            \n            for caller in callstack[1:]:\n                try:\n                    call = caller.calls[callee.id]\n                except KeyError:\n                    call = Call(callee.id)\n                    call[SAMPLES2] = samples\n                    caller.add_call(call)\n                else:\n                    call[SAMPLES2] += samples\n\n                callee = caller\n\n    def parse(self):\n        profile = self.profile\n        profile[SAMPLES] = 0\n\n        self.parse_symbols()\n        self.parse_callstacks()\n\n        # Compute derived events\n        profile.validate()\n        profile.find_cycles()\n        profile.ratio(TIME_RATIO, SAMPLES)\n        profile.call_ratios(SAMPLES2)\n        profile.integrate(TOTAL_TIME_RATIO, TIME_RATIO)\n\n        return profile\n\n\nclass PstatsParser:\n    """"""Parser python profiling statistics saved with te pstats module.""""""\n\n    stdinInput = False\n    multipleInput = True\n\n    def __init__(self, *filename):\n        import pstats\n        try:\n            self.stats = pstats.Stats(*filename)\n        except ValueError:\n            if PYTHON_3:\n                sys.stderr.write(\'error: failed to load %s\\n\' % \', \'.join(filename))\n                sys.exit(1)\n            import hotshot.stats\n            self.stats = hotshot.stats.load(filename[0])\n        self.profile = Profile()\n        self.function_ids = {}\n\n    def get_function_name(self, key):\n        filename, line, name = key\n        module = os.path.splitext(filename)[0]\n        module = os.path.basename(module)\n        return ""%s:%d:%s"" % (module, line, name)\n\n    def get_function(self, key):\n        try:\n            id = self.function_ids[key]\n        except KeyError:\n            id = len(self.function_ids)\n            name = self.get_function_name(key)\n            function = Function(id, name)\n            function.filename = key[0]\n            self.profile.functions[id] = function\n            self.function_ids[key] = id\n        else:\n            function = self.profile.functions[id]\n        return function\n\n    def parse(self):\n        self.profile[TIME] = 0.0\n        self.profile[TOTAL_TIME] = self.stats.total_tt\n        for fn, (cc, nc, tt, ct, callers) in compat_iteritems(self.stats.stats):\n            callee = self.get_function(fn)\n            callee.called = nc\n            callee[TOTAL_TIME] = ct\n            callee[TIME] = tt\n            self.profile[TIME] += tt\n            self.profile[TOTAL_TIME] = max(self.profile[TOTAL_TIME], ct)\n            for fn, value in compat_iteritems(callers):\n                caller = self.get_function(fn)\n                call = Call(callee.id)\n                if isinstance(value, tuple):\n                    for i in xrange(0, len(value), 4):\n                        nc, cc, tt, ct = value[i:i+4]\n                        if CALLS in call:\n                            call[CALLS] += cc\n                        else:\n                            call[CALLS] = cc\n\n                        if TOTAL_TIME in call:\n                            call[TOTAL_TIME] += ct\n                        else:\n                            call[TOTAL_TIME] = ct\n\n                else:\n                    call[CALLS] = value\n                    call[TOTAL_TIME] = ratio(value, nc)*ct\n\n                caller.add_call(call)\n\n        if False:\n            self.stats.print_stats()\n            self.stats.print_callees()\n\n        # Compute derived events\n        self.profile.validate()\n        self.profile.ratio(TIME_RATIO, TIME)\n        self.profile.ratio(TOTAL_TIME_RATIO, TOTAL_TIME)\n\n        return self.profile\n\n\nformats = {\n    ""axe"": AXEParser,\n    ""callgrind"": CallgrindParser,\n    ""hprof"": HProfParser,\n    ""json"": JsonParser,\n    ""oprofile"": OprofileParser,\n    ""perf"": PerfParser,\n    ""prof"": GprofParser,\n    ""pstats"": PstatsParser,\n    ""sleepy"": SleepyParser,\n    ""sysprof"": SysprofParser,\n    ""xperf"": XPerfParser,\n}\n\n\n########################################################################\n# Output\n\n\nclass Theme:\n\n    def __init__(self, \n            bgcolor = (0.0, 0.0, 1.0),\n            mincolor = (0.0, 0.0, 0.0),\n            maxcolor = (0.0, 0.0, 1.0),\n            fontname = ""Arial"",\n            fontcolor = ""white"",\n            nodestyle = ""filled"",\n            minfontsize = 10.0,\n            maxfontsize = 10.0,\n            minpenwidth = 0.5,\n            maxpenwidth = 4.0,\n            gamma = 2.2,\n            skew = 1.0):\n        self.bgcolor = bgcolor\n        self.mincolor = mincolor\n        self.maxcolor = maxcolor\n        self.fontname = fontname\n        self.fontcolor = fontcolor\n        self.nodestyle = nodestyle\n        self.minfontsize = minfontsize\n        self.maxfontsize = maxfontsize\n        self.minpenwidth = minpenwidth\n        self.maxpenwidth = maxpenwidth\n        self.gamma = gamma\n        self.skew = skew\n\n    def graph_bgcolor(self):\n        return self.hsl_to_rgb(*self.bgcolor)\n\n    def graph_fontname(self):\n        return self.fontname\n\n    def graph_fontcolor(self):\n        return self.fontcolor\n\n    def graph_fontsize(self):\n        return self.minfontsize\n\n    def node_bgcolor(self, weight):\n        return self.color(weight)\n\n    def node_fgcolor(self, weight):\n        if self.nodestyle == ""filled"":\n            return self.graph_bgcolor()\n        else:\n            return self.color(weight)\n\n    def node_fontsize(self, weight):\n        return self.fontsize(weight)\n\n    def node_style(self):\n        return self.nodestyle\n\n    def edge_color(self, weight):\n        return self.color(weight)\n\n    def edge_fontsize(self, weight):\n        return self.fontsize(weight)\n\n    def edge_penwidth(self, weight):\n        return max(weight*self.maxpenwidth, self.minpenwidth)\n\n    def edge_arrowsize(self, weight):\n        return 0.5 * math.sqrt(self.edge_penwidth(weight))\n\n    def fontsize(self, weight):\n        return max(weight**2 * self.maxfontsize, self.minfontsize)\n\n    def color(self, weight):\n        weight = min(max(weight, 0.0), 1.0)\n    \n        hmin, smin, lmin = self.mincolor\n        hmax, smax, lmax = self.maxcolor\n        \n        if self.skew < 0:\n            raise ValueError(""Skew must be greater than 0"")\n        elif self.skew == 1.0:\n            h = hmin + weight*(hmax - hmin)\n            s = smin + weight*(smax - smin)\n            l = lmin + weight*(lmax - lmin)\n        else:\n            base = self.skew\n            h = hmin + ((hmax-hmin)*(-1.0 + (base ** weight)) / (base - 1.0))\n            s = smin + ((smax-smin)*(-1.0 + (base ** weight)) / (base - 1.0))\n            l = lmin + ((lmax-lmin)*(-1.0 + (base ** weight)) / (base - 1.0))\n\n        return self.hsl_to_rgb(h, s, l)\n\n    def hsl_to_rgb(self, h, s, l):\n        """"""Convert a color from HSL color-model to RGB.\n\n        See also:\n        - http://www.w3.org/TR/css3-color/#hsl-color\n        """"""\n\n        h = h % 1.0\n        s = min(max(s, 0.0), 1.0)\n        l = min(max(l, 0.0), 1.0)\n\n        if l <= 0.5:\n            m2 = l*(s + 1.0)\n        else:\n            m2 = l + s - l*s\n        m1 = l*2.0 - m2\n        r = self._hue_to_rgb(m1, m2, h + 1.0/3.0)\n        g = self._hue_to_rgb(m1, m2, h)\n        b = self._hue_to_rgb(m1, m2, h - 1.0/3.0)\n\n        # Apply gamma correction\n        r **= self.gamma\n        g **= self.gamma\n        b **= self.gamma\n\n        return (r, g, b)\n\n    def _hue_to_rgb(self, m1, m2, h):\n        if h < 0.0:\n            h += 1.0\n        elif h > 1.0:\n            h -= 1.0\n        if h*6 < 1.0:\n            return m1 + (m2 - m1)*h*6.0\n        elif h*2 < 1.0:\n            return m2\n        elif h*3 < 2.0:\n            return m1 + (m2 - m1)*(2.0/3.0 - h)*6.0\n        else:\n            return m1\n\n\nTEMPERATURE_COLORMAP = Theme(\n    mincolor = (2.0/3.0, 0.80, 0.25), # dark blue\n    maxcolor = (0.0, 1.0, 0.5), # satured red\n    gamma = 1.0\n)\n\nPINK_COLORMAP = Theme(\n    mincolor = (0.0, 1.0, 0.90), # pink\n    maxcolor = (0.0, 1.0, 0.5), # satured red\n)\n\nGRAY_COLORMAP = Theme(\n    mincolor = (0.0, 0.0, 0.85), # light gray\n    maxcolor = (0.0, 0.0, 0.0), # black\n)\n\nBW_COLORMAP = Theme(\n    minfontsize = 8.0,\n    maxfontsize = 24.0,\n    mincolor = (0.0, 0.0, 0.0), # black\n    maxcolor = (0.0, 0.0, 0.0), # black\n    minpenwidth = 0.1,\n    maxpenwidth = 8.0,\n)\n\nPRINT_COLORMAP = Theme(\n    minfontsize = 18.0,\n    maxfontsize = 30.0,\n    fontcolor = ""black"",\n    nodestyle = ""solid"",\n    mincolor = (0.0, 0.0, 0.0), # black\n    maxcolor = (0.0, 0.0, 0.0), # black\n    minpenwidth = 0.1,\n    maxpenwidth = 8.0,\n)\n\n\nthemes = {\n    ""color"": TEMPERATURE_COLORMAP,\n    ""pink"": PINK_COLORMAP,\n    ""gray"": GRAY_COLORMAP,\n    ""bw"": BW_COLORMAP,\n    ""print"": PRINT_COLORMAP,\n}\n\n\ndef sorted_iteritems(d):\n    # Used mostly for result reproducibility (while testing.)\n    keys = compat_keys(d)\n    keys.sort()\n    for key in keys:\n        value = d[key]\n        yield key, value\n\n\nclass DotWriter:\n    """"""Writer for the DOT language.\n\n    See also:\n    - ""The DOT Language"" specification\n      http://www.graphviz.org/doc/info/lang.html\n    """"""\n\n    strip = False\n    wrap = False\n\n    def __init__(self, fp):\n        self.fp = fp\n\n    def wrap_function_name(self, name):\n        """"""Split the function name on multiple lines.""""""\n\n        if len(name) > 32:\n            ratio = 2.0/3.0\n            height = max(int(len(name)/(1.0 - ratio) + 0.5), 1)\n            width = max(len(name)/height, 32)\n            # TODO: break lines in symbols\n            name = textwrap.fill(name, width, break_long_words=False)\n\n        # Take away spaces\n        name = name.replace("", "", "","")\n        name = name.replace(""> >"", "">>"")\n        name = name.replace(""> >"", "">>"") # catch consecutive\n\n        return name\n\n    show_function_events = [TOTAL_TIME_RATIO, TIME_RATIO]\n    show_edge_events = [TOTAL_TIME_RATIO, CALLS]\n\n    def graph(self, profile, theme):\n        self.begin_graph()\n\n        fontname = theme.graph_fontname()\n        fontcolor = theme.graph_fontcolor()\n        nodestyle = theme.node_style()\n\n        self.attr(\'graph\', fontname=fontname, ranksep=0.25, nodesep=0.125)\n        self.attr(\'node\', fontname=fontname, shape=""box"", style=nodestyle, fontcolor=fontcolor, width=0, height=0)\n        self.attr(\'edge\', fontname=fontname)\n\n        for _, function in sorted_iteritems(profile.functions):\n            labels = []\n            if function.process is not None:\n                labels.append(function.process)\n            if function.module is not None:\n                labels.append(function.module)\n\n            if self.strip:\n                function_name = function.stripped_name()\n            else:\n                function_name = function.name\n            if self.wrap:\n                function_name = self.wrap_function_name(function_name)\n            labels.append(function_name)\n\n            for event in self.show_function_events:\n                if event in function.events:\n                    label = event.format(function[event])\n                    labels.append(label)\n            if function.called is not None:\n                labels.append(""%u%s"" % (function.called, MULTIPLICATION_SIGN))\n\n            if function.weight is not None:\n                weight = function.weight\n            else:\n                weight = 0.0\n\n            label = \'\\n\'.join(labels)\n            self.node(function.id, \n                label = label, \n                color = self.color(theme.node_bgcolor(weight)), \n                fontcolor = self.color(theme.node_fgcolor(weight)), \n                fontsize = ""%.2f"" % theme.node_fontsize(weight),\n                tooltip = function.filename,\n            )\n\n            for _, call in sorted_iteritems(function.calls):\n                callee = profile.functions[call.callee_id]\n\n                labels = []\n                for event in self.show_edge_events:\n                    if event in call.events:\n                        label = event.format(call[event])\n                        labels.append(label)\n\n                if call.weight is not None:\n                    weight = call.weight\n                elif callee.weight is not None:\n                    weight = callee.weight\n                else:\n                    weight = 0.0\n\n                label = \'\\n\'.join(labels)\n\n                self.edge(function.id, call.callee_id, \n                    label = label, \n                    color = self.color(theme.edge_color(weight)), \n                    fontcolor = self.color(theme.edge_color(weight)),\n                    fontsize = ""%.2f"" % theme.edge_fontsize(weight), \n                    penwidth = ""%.2f"" % theme.edge_penwidth(weight), \n                    labeldistance = ""%.2f"" % theme.edge_penwidth(weight), \n                    arrowsize = ""%.2f"" % theme.edge_arrowsize(weight),\n                )\n\n        self.end_graph()\n\n    def begin_graph(self):\n        self.write(\'digraph {\\n\')\n\n    def end_graph(self):\n        self.write(\'}\\n\')\n\n    def attr(self, what, **attrs):\n        self.write(""\\t"")\n        self.write(what)\n        self.attr_list(attrs)\n        self.write("";\\n"")\n\n    def node(self, node, **attrs):\n        self.write(""\\t"")\n        self.id(node)\n        self.attr_list(attrs)\n        self.write("";\\n"")\n\n    def edge(self, src, dst, **attrs):\n        self.write(""\\t"")\n        self.id(src)\n        self.write("" -> "")\n        self.id(dst)\n        self.attr_list(attrs)\n        self.write("";\\n"")\n\n    def attr_list(self, attrs):\n        if not attrs:\n            return\n        self.write(\' [\')\n        first = True\n        for name, value in sorted_iteritems(attrs):\n            if value is None:\n                continue\n            if first:\n                first = False\n            else:\n                self.write("", "")\n            self.id(name)\n            self.write(\'=\')\n            self.id(value)\n        self.write(\']\')\n\n    def id(self, id):\n        if isinstance(id, (int, float)):\n            s = str(id)\n        elif isinstance(id, basestring):\n            if id.isalnum() and not id.startswith(\'0x\'):\n                s = id\n            else:\n                s = self.escape(id)\n        else:\n            raise TypeError\n        self.write(s)\n\n    def color(self, rgb):\n        r, g, b = rgb\n\n        def float2int(f):\n            if f <= 0.0:\n                return 0\n            if f >= 1.0:\n                return 255\n            return int(255.0*f + 0.5)\n\n        return ""#"" + """".join([""%02x"" % float2int(c) for c in (r, g, b)])\n\n    def escape(self, s):\n        if not PYTHON_3:\n            s = s.encode(\'utf-8\')\n        s = s.replace(\'\\\\\', r\'\\\\\')\n        s = s.replace(\'\\n\', r\'\\n\')\n        s = s.replace(\'\\t\', r\'\\t\')\n        s = s.replace(\'""\', r\'\\""\')\n        return \'""\' + s + \'""\'\n\n    def write(self, s):\n        self.fp.write(s)\n\n\n\n########################################################################\n# Main program\n\n\ndef naturalJoin(values):\n    if len(values) >= 2:\n        return \', \'.join(values[:-1]) + \' or \' + values[-1]\n\n    else:\n        return \'\'.join(values)\n\n\ndef main():\n    """"""Main program.""""""\n\n    global totalMethod\n\n    formatNames = list(formats.keys())\n    formatNames.sort()\n\n    optparser = optparse.OptionParser(\n        usage=""\\n\\t%prog [options] [file] ..."")\n    optparser.add_option(\n        \'-o\', \'--output\', metavar=\'FILE\',\n        type=""string"", dest=""output"",\n        help=""output filename [stdout]"")\n    optparser.add_option(\n        \'-n\', \'--node-thres\', metavar=\'PERCENTAGE\',\n        type=""float"", dest=""node_thres"", default=0.5,\n        help=""eliminate nodes below this threshold [default: %default]"")\n    optparser.add_option(\n        \'-e\', \'--edge-thres\', metavar=\'PERCENTAGE\',\n        type=""float"", dest=""edge_thres"", default=0.1,\n        help=""eliminate edges below this threshold [default: %default]"")\n    optparser.add_option(\n        \'-f\', \'--format\',\n        type=""choice"", choices=formatNames,\n        dest=""format"", default=""prof"",\n        help=""profile format: %s [default: %%default]"" % naturalJoin(formatNames))\n    optparser.add_option(\n        \'--total\',\n        type=""choice"", choices=(\'callratios\', \'callstacks\'),\n        dest=""totalMethod"", default=totalMethod,\n        help=""preferred method of calculating total time: callratios or callstacks (currently affects only perf format) [default: %default]"")\n    optparser.add_option(\n        \'-c\', \'--colormap\',\n        type=""choice"", choices=(\'color\', \'pink\', \'gray\', \'bw\', \'print\'),\n        dest=""theme"", default=""color"",\n        help=""color map: color, pink, gray, bw, or print [default: %default]"")\n    optparser.add_option(\n        \'-s\', \'--strip\',\n        action=""store_true"",\n        dest=""strip"", default=False,\n        help=""strip function parameters, template parameters, and const modifiers from demangled C++ function names"")\n    optparser.add_option(\n        \'--colour-nodes-by-selftime\',\n        action=""store_true"",\n        dest=""colour_nodes_by_selftime"", default=False,\n        help=""colour nodes by self time, rather than by total time (sum of self and descendants)"")\n    optparser.add_option(\n        \'-w\', \'--wrap\',\n        action=""store_true"",\n        dest=""wrap"", default=False,\n        help=""wrap function names"")\n    optparser.add_option(\n        \'--show-samples\',\n        action=""store_true"",\n        dest=""show_samples"", default=False,\n        help=""show function samples"")\n    # add option to create subtree or show paths\n    optparser.add_option(\n        \'-z\', \'--root\',\n        type=""string"",\n        dest=""root"", default="""",\n        help=""prune call graph to show only descendants of specified root function"")\n    optparser.add_option(\n        \'-l\', \'--leaf\',\n        type=""string"",\n        dest=""leaf"", default="""",\n        help=""prune call graph to show only ancestors of specified leaf function"")\n    # add a new option to control skew of the colorization curve\n    optparser.add_option(\n        \'--skew\',\n        type=""float"", dest=""theme_skew"", default=1.0,\n        help=""skew the colorization curve.  Values < 1.0 give more variety to lower percentages.  Values > 1.0 give less variety to lower percentages"")\n    (options, args) = optparser.parse_args(sys.argv[1:])\n\n    if len(args) > 1 and options.format != \'pstats\':\n        optparser.error(\'incorrect number of arguments\')\n\n    try:\n        theme = themes[options.theme]\n    except KeyError:\n        optparser.error(\'invalid colormap \\\'%s\\\'\' % options.theme)\n\n    # set skew on the theme now that it has been picked.\n    if options.theme_skew:\n        theme.skew = options.theme_skew\n\n    totalMethod = options.totalMethod\n\n    try:\n        Format = formats[options.format]\n    except KeyError:\n        optparser.error(\'invalid format \\\'%s\\\'\' % options.format)\n\n    if Format.stdinInput:\n        if not args:\n            fp = sys.stdin\n        elif PYTHON_3:\n            fp = open(args[0], \'rt\', encoding=\'UTF-8\')\n        else:\n            fp = open(args[0], \'rt\')\n        parser = Format(fp)\n    elif Format.multipleInput:\n        if not args:\n            optparser.error(\'at least a file must be specified for %s input\' % options.format)\n        parser = Format(*args)\n    else:\n        if len(args) != 1:\n            optparser.error(\'exactly one file must be specified for %s input\' % options.format)\n        parser = Format(args[0])\n\n    profile = parser.parse()\n\n    if options.output is None:\n        if PYTHON_3:\n            output = open(sys.stdout.fileno(), mode=\'wt\', encoding=\'UTF-8\', closefd=False)\n        else:\n            output = sys.stdout\n    else:\n        if PYTHON_3:\n            output = open(options.output, \'wt\', encoding=\'UTF-8\')\n        else:\n            output = open(options.output, \'wt\')\n\n    dot = DotWriter(output)\n    dot.strip = options.strip\n    dot.wrap = options.wrap\n    if options.show_samples:\n        dot.show_function_events.append(SAMPLES)\n\n    profile = profile\n    profile.prune(options.node_thres/100.0, options.edge_thres/100.0, options.colour_nodes_by_selftime)\n\n    if options.root:\n        rootId = profile.getFunctionId(options.root)\n        if not rootId:\n            sys.stderr.write(\'root node \' + options.root + \' not found (might already be pruned : try -e0 -n0 flags)\\n\')\n            sys.exit(1)\n        profile.prune_root(rootId)\n    if options.leaf:\n        leafId = profile.getFunctionId(options.leaf)\n        if not leafId:\n            sys.stderr.write(\'leaf node \' + options.leaf + \' not found (maybe already pruned : try -e0 -n0 flags)\\n\')\n            sys.exit(1)\n        profile.prune_leaf(leafId)\n\n    dot.graph(profile, theme)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
libs/boxes/nms.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\n\ndef nms(dets, thresh):\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n'"
libs/boxes/nms_wrapper.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\nimport libs.configs.config_v1 as cfg\nimport libs.nms.gpu_nms as gpu_nms\nimport libs.nms.cpu_nms as cpu_nms\n\ndef nms(dets, thresh, force_cpu=False):\n    """"""Dispatch to either CPU or GPU NMS implementations.""""""\n\n    if dets.shape[0] == 0:\n        return []\n    return gpu_nms.gpu_nms(dets, thresh, device_id=0)\n\ndef nms_wrapper(scores, boxes, threshold = 0.7, class_sets = None):\n    """"""\n    post-process the results of im_detect\n    :param boxes: N * (K * 4) numpy\n    :param scores: N * K numpy\n    :param class_sets: e.g. CLASSES = (\'__background__\',\'person\',\'bike\',\'motorbike\',\'car\',\'bus\')\n    :return: a list of K-1 dicts, no background, each is {\'class\': classname, \'dets\': None | [[x1,y1,x2,y2,score],...]}\n    """"""\n    num_class = scores.shape[1] if class_sets is None else len(class_sets)\n    assert num_class * 4 == boxes.shape[1],\\\n        \'Detection scores and boxes dont match %d vs %d\' % (num_class, boxes.shape[1])\n    class_sets = [\'class_\' + str(i) for i in range(0, num_class)] if class_sets is None else class_sets\n\n    res = []\n    for ind, cls in enumerate(class_sets[1:]):\n        ind += 1 # skip background\n        cls_boxes =  boxes[:, 4*ind : 4*(ind+1)]\n        cls_scores = scores[:, ind]\n        dets = np.hstack((cls_boxes, cls_scores[:, np.newaxis])).astype(np.float32)\n        keep = nms(dets, thresh=0.3)\n        dets = dets[keep, :]\n        dets = dets[np.where(dets[:, 4] > threshold)]\n        r = {}\n        if dets.shape[0] > 0:\n            r[\'class\'], r[\'dets\'] = cls, dets\n        else:\n            r[\'class\'], r[\'dets\'] = cls, None\n        res.append(r)\n    return res\n\nif __name__==\'__main__\':\n  \n  score = np.random.rand(10, 21)\n  boxes = np.random.randint(0, 100, (10, 21, 2))\n  s = np.random.randint(0, 100, (10, 21, 2))\n  s = boxes + s\n  boxes = np.concatenate((boxes, s), axis=2)\n  boxes = np.reshape(boxes, [boxes.shape[0], -1])\n  # score = np.reshape(score, [score.shape[0], -1])\n  res = nms_wrapper(score, boxes)\n  print (res)'"
libs/boxes/roi.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport functools\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\ndef roi_align(feat, boxes):\n  """"""Given features and boxes, This function crops feature """"""\n  return\n\ndef roi_cropping(feat, boxes, clses, anchors, spatial_scale=1.0/16):\n  """"""This function computes final rpn boxes\n   And crops areas from the incoming features\n  """"""\n  return'"
libs/boxes/timer.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport time\n\nclass Timer(object):\n    """"""A simple timer.""""""\n    def __init__(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n\n    def tic(self):\n        # using time.time instead of time.clock because time time.clock\n        # does not normalize for multithreading\n        self.start_time = time.time()\n\n    def toc(self, average=True):\n        self.diff = time.time() - self.start_time\n        self.total_time += self.diff\n        self.calls += 1\n        self.average_time = self.total_time / self.calls\n        if average:\n            return self.average_time\n        else:\n            return self.diff\n'"
libs/configs/__init__.py,0,b''
libs/configs/config_v1.py,67,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n##########################\n#                  restore\n##########################\ntf.app.flags.DEFINE_string(\n    \'train_dir\', \'./output/mask_rcnn/\',\n    \'Directory where checkpoints and event logs are written to.\')\n\ntf.app.flags.DEFINE_string(\n    \'pretrained_model\', \'./data/pretrained_models/resnet_v1_50.ckpt\',\n    \'Path to pretrained model\')\n\n##########################\n#                  network\n##########################\ntf.app.flags.DEFINE_string(\n    \'network\', \'resnet50\',\n    \'name of backbone network\')\n\n##########################\n#                  dataset\n##########################\ntf.app.flags.DEFINE_bool(\n    \'update_bn\', False,\n    \'Whether or not to update bacth normalization layer\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_readers\', 4,\n    \'The number of parallel readers that read data from the dataset.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'coco\',\n    \'The name of the dataset to load.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_split_name\', \'train2014\',\n    \'The name of the train/test/val split.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_dir\', \'data/coco/\',\n    \'The directory where the dataset files are stored.\')\n\ntf.app.flags.DEFINE_integer(\n    \'im_batch\', 1,\n    \'number of images in a mini-batch\')\n\n\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 4,\n    \'The number of threads used to create the batches.\')\n\ntf.app.flags.DEFINE_integer(\n    \'log_every_n_steps\', 10,\n    \'The frequency with which logs are print.\')\n\ntf.app.flags.DEFINE_integer(\n    \'save_summaries_secs\', 60,\n    \'The frequency with which summaries are saved, in seconds.\')\n\ntf.app.flags.DEFINE_integer(\n    \'save_interval_secs\', 7200,\n    \'The frequency with which the model is saved, in seconds.\')\n\ntf.app.flags.DEFINE_integer(\n    \'max_iters\', 2500000,\n    \'max iterations\')\n\n######################\n# Optimization Flags #\n######################\n\ntf.app.flags.DEFINE_float(\n    \'weight_decay\', 0.00005, \'The weight decay on the model weights.\')\n\ntf.app.flags.DEFINE_string(\n    \'optimizer\', \'momentum\',\n    \'The name of the optimizer, one of ""adadelta"", ""adagrad"", ""adam"",\'\n    \'""ftrl"", ""momentum"", ""sgd"" or ""rmsprop"".\')\n\ntf.app.flags.DEFINE_float(\n    \'adadelta_rho\', 0.95,\n    \'The decay rate for adadelta.\')\n\ntf.app.flags.DEFINE_float(\n    \'adagrad_initial_accumulator_value\', 0.1,\n    \'Starting value for the AdaGrad accumulators.\')\n\ntf.app.flags.DEFINE_float(\n    \'adam_beta1\', 0.9,\n    \'The exponential decay rate for the 1st moment estimates.\')\n\ntf.app.flags.DEFINE_float(\n    \'adam_beta2\', 0.999,\n    \'The exponential decay rate for the 2nd moment estimates.\')\n\ntf.app.flags.DEFINE_float(\'opt_epsilon\', 1.0, \'Epsilon term for the optimizer.\')\n\ntf.app.flags.DEFINE_float(\'ftrl_learning_rate_power\', -0.5,\n                          \'The learning rate power.\')\n\ntf.app.flags.DEFINE_float(\n    \'ftrl_initial_accumulator_value\', 0.1,\n    \'Starting value for the FTRL accumulators.\')\n\ntf.app.flags.DEFINE_float(\n    \'ftrl_l1\', 0.0, \'The FTRL l1 regularization strength.\')\n\ntf.app.flags.DEFINE_float(\n    \'ftrl_l2\', 0.0, \'The FTRL l2 regularization strength.\')\n\ntf.app.flags.DEFINE_float(\n    \'momentum\', 0.99,\n    \'The momentum for the MomentumOptimizer and RMSPropOptimizer.\')\n\ntf.app.flags.DEFINE_float(\'rmsprop_momentum\', 0.99, \'Momentum.\')\n\ntf.app.flags.DEFINE_float(\'rmsprop_decay\', 0.99, \'Decay term for RMSProp.\')\n\n#######################\n# Learning Rate Flags #\n#######################\n\ntf.app.flags.DEFINE_string(\n    \'learning_rate_decay_type\', \'exponential\',\n    \'Specifies how the learning rate is decayed. One of ""fixed"", ""exponential"",\'\n    \' or ""polynomial""\')\n\ntf.app.flags.DEFINE_float(\'learning_rate\', 0.002,\n                          \'Initial learning rate.\')\n\ntf.app.flags.DEFINE_float(\n    \'end_learning_rate\', 0.00001,\n    \'The minimal end learning rate used by a polynomial decay learning rate.\')\n\ntf.app.flags.DEFINE_float(\n    \'label_smoothing\', 0.0, \'The amount of label smoothing.\')\n\ntf.app.flags.DEFINE_float(\n    \'learning_rate_decay_factor\', 0.94, \'Learning rate decay factor.\')\n\ntf.app.flags.DEFINE_float(\n    \'num_epochs_per_decay\', 2.0,\n    \'Number of epochs after which learning rate decays.\')\n\ntf.app.flags.DEFINE_bool(\n    \'sync_replicas\', False,\n    \'Whether or not to synchronize the replicas during training.\')\n\ntf.app.flags.DEFINE_integer(\n    \'replicas_to_aggregate\', 1,\n    \'The Number of gradients to collect before updating params.\')\n\ntf.app.flags.DEFINE_float(\n    \'moving_average_decay\', None,\n    \'The decay to use for the moving average.\'\n    \'If left as None, then moving averages are not used.\')\n\n#######################\n# Dataset Flags #\n#######################\n\n\ntf.app.flags.DEFINE_string(\n    \'model_name\', \'resnet50\',\n    \'The name of the architecture to train.\')\n\ntf.app.flags.DEFINE_string(\n    \'preprocessing_name\', \'coco\',\n    \'The name of the preprocessing to use. If left \'\n    \'as `None`, then the model_name flag is used.\')\n\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 1,\n    \'The number of samples in each batch.\')\n\ntf.app.flags.DEFINE_integer(\n    \'train_image_size\', None, \'Train image size\')\n\ntf.app.flags.DEFINE_integer(\'max_number_of_steps\', None,\n                            \'The maximum number of training steps.\')\n\ntf.app.flags.DEFINE_string(\n    \'classes\', None,\n    \'The classes to classify.\')\n\ntf.app.flags.DEFINE_integer(\n    \'image_min_size\', 640,\n    \'resize image so that the min edge equals to image_min_size\')\n\n#####################\n# Fine-Tuning Flags #\n#####################\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', None,\n    \'The path to a checkpoint from which to fine-tune.\')\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_exclude_scopes\', None,\n    \'Comma-separated list of scopes of variables to exclude when restoring \'\n    \'from a checkpoint.\')\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_include_scopes\', None,\n    \'Comma-separated list of scopes of variables to include when restoring \'\n    \'from a checkpoint.\')\n\ntf.app.flags.DEFINE_string(\n    \'trainable_scopes\', None,\n    \'Comma-separated list of scopes to filter the set of variables to train.\'\n    \'By default, None would train all the variables.\')\n\ntf.app.flags.DEFINE_boolean(\n    \'ignore_missing_vars\', False,\n    \'When restoring a checkpoint would ignore missing variables.\')\n\ntf.app.flags.DEFINE_boolean(\n    \'restore_previous_if_exists\', True,\n    \'When restoring a checkpoint would ignore missing variables.\')\n\n#######################\n# BOX Flags #\n#######################\ntf.app.flags.DEFINE_float(\n    \'rpn_bg_threshold\', 0.3,\n    \'Only regions which intersection is larger than fg_threshold are considered to be fg\')\n\ntf.app.flags.DEFINE_float(\n    \'rpn_fg_threshold\', 0.7,\n    \'Only regions which intersection is larger than fg_threshold are considered to be fg\')\n\ntf.app.flags.DEFINE_float(\n    \'fg_threshold\', 0.7,\n    \'Only regions which intersection is larger than fg_threshold are considered to be fg\')\n\ntf.app.flags.DEFINE_float(\n    \'bg_threshold\', 0.3,\n    \'Only regions which intersection is less than bg_threshold are considered to be bg\')\n\ntf.app.flags.DEFINE_integer(\n    \'rois_per_image\', 256,\n    \'Number of rois that should be sampled to train this network\')\n\ntf.app.flags.DEFINE_float(\n    \'fg_roi_fraction\', 0.25,\n    \'Number of rois that should be sampled to train this network\')\n\ntf.app.flags.DEFINE_float(\n    \'fg_rpn_fraction\', 0.25,\n    \'Number of rois that should be sampled to train this network\')\n\ntf.app.flags.DEFINE_integer(\n    \'rpn_batch_size\', 500,\n    \'Number of rpn anchors that should be sampled to train this network\')\n\ntf.app.flags.DEFINE_integer(\n    \'allow_border\', 10,\n    \'How many pixels out of an image\')\n\n##################################\n#            NMS                #\n##################################\n\ntf.app.flags.DEFINE_integer(\n    \'pre_nms_top_n\', 12000,\n    \'Number of rpn anchors that should be sampled before nms\')\n\ntf.app.flags.DEFINE_integer(\n    \'post_nms_top_n\', 2000,\n    \'Number of rpn anchors that should be sampled after nms\')\n\ntf.app.flags.DEFINE_float(\n    \'rpn_nms_threshold\', 0.7,\n    \'NMS threshold\')\n\n##################################\n#            Mask                #\n##################################\n\ntf.app.flags.DEFINE_boolean(\n    \'mask_allow_bg\', True,\n    \'Allow to add bg masks in the masking stage\')\n\ntf.app.flags.DEFINE_float(\n    \'mask_threshold\', 0.50,\n    \'Least intersection of a positive mask\')\ntf.app.flags.DEFINE_integer(\n    \'masks_per_image\', 64,\n    \'Number of rois that should be sampled to train this network\')\n\ntf.app.flags.DEFINE_float(\n    \'min_size\', 2,\n    \'minimum size of an object\')\n\nFLAGS = tf.app.flags.FLAGS\n'"
libs/datasets/__init__.py,0,b''
libs/datasets/coco.py,49,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.python.lib.io.tf_record import TFRecordCompressionType\n\n_FILE_PATTERN = 'coco_%s_*.tfrecord'\n\nSPLITS_TO_SIZES = {'train2014': 82783, 'val2014': 40504}\n\n_NUM_CLASSES = 81\n\n_ITEMS_TO_DESCRIPTIONS = {\n    'image': 'A color image of varying size.',\n    'label': 'An annotation image of varying size. (pixel-level masks)',\n    'gt_masks': 'masks of instances in this image. (instance-level masks), of shape (N, image_height, image_width)',\n    'gt_boxes': 'bounding boxes and classes of instances in this image, of shape (N, 5), each entry is (x1, y1, x2, y2)',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  if split_name not in SPLITS_TO_SIZES:\n    raise ValueError('split name %s was not recognized.' % split_name)\n  \n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, 'records', file_pattern % split_name)\n  \n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n  \n  keys_to_features = {\n    'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n    'image/format': tf.FixedLenFeature((), tf.string, default_value='jpeg'),\n    'label/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n    'label/format': tf.FixedLenFeature((), tf.string, default_value='png'),\n    'image/height': tf.FixedLenFeature((), tf.int64),\n    'image/width': tf.FixedLenFeature((), tf.int64),\n    \n    'label/num_instances': tf.FixedLenFeature((), tf.int64),\n    'label/gt_boxes': tf.FixedLenFeature((), tf.string),\n    'label/gt_masks': tf.FixedLenFeature((), tf.string),\n  }\n  \n  def _masks_decoder(keys_to_tensors):\n    masks = tf.decode_raw(keys_to_tensors['label/gt_masks'], tf.uint8)\n    width = tf.cast(keys_to_tensors['image/width'], tf.int32)\n    height = tf.cast(keys_to_tensors['image/height'], tf.int32)\n    instances = tf.cast(keys_to_tensors['label/num_instances'], tf.int32)\n    mask_shape = tf.stack([instances, height, width])\n    return tf.reshape(masks, mask_shape)\n  \n  def _gt_boxes_decoder(keys_to_tensors):\n    bboxes = tf.decode_raw(keys_to_tensors['label/gt_boxes'], tf.float32)\n    instances = tf.cast(keys_to_tensors['label/num_instances'], tf.int32)\n    bboxes_shape = tf.stack([instances, 5])\n    return tf.reshape(bboxes, bboxes_shape)\n  \n  def _width_decoder(keys_to_tensors):\n    width = keys_to_tensors['image/width']\n    return tf.cast(width, tf.int32)\n  \n  def _height_decoder(keys_to_tensors):\n    height = keys_to_tensors['image/height']\n    return tf.cast(height, tf.int32)\n  \n  items_to_handlers = {\n    'image': slim.tfexample_decoder.Image('image/encoded', 'image/format'),\n    'label': slim.tfexample_decoder.Image('label/encoded', 'label/format', channels=1),\n    'gt_masks': slim.tfexample_decoder.ItemHandlerCallback(\n                ['label/gt_masks', 'label/num_instances', 'image/width', 'image/height'], _masks_decoder),\n    'gt_boxes': slim.tfexample_decoder.ItemHandlerCallback(['label/gt_boxes', 'label/num_instances'], _gt_boxes_decoder),\n    'width': slim.tfexample_decoder.ItemHandlerCallback(['image/width'], _width_decoder),\n    'height': slim.tfexample_decoder.ItemHandlerCallback(['image/height'], _height_decoder),\n  }\n  \n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n    keys_to_features, items_to_handlers)\n  \n  return slim.dataset.Dataset(\n    data_sources=file_pattern,\n    reader=reader,\n    decoder=decoder,\n    num_samples=SPLITS_TO_SIZES[split_name],\n    items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n    num_classes=_NUM_CLASSES)\n\ndef read(tfrecords_filename):\n\n  if not isinstance(tfrecords_filename, list):\n    tfrecords_filename = [tfrecords_filename]\n  filename_queue = tf.train.string_input_producer(\n    tfrecords_filename, num_epochs=100)\n\n  options = tf.python_io.TFRecordOptions(TFRecordCompressionType.ZLIB)\n  reader = tf.TFRecordReader(options=options)\n  _, serialized_example = reader.read(filename_queue)\n  features = tf.parse_single_example(\n    serialized_example,\n    features={\n      'image/img_id': tf.FixedLenFeature([], tf.int64),\n      'image/encoded': tf.FixedLenFeature([], tf.string),\n      'image/height': tf.FixedLenFeature([], tf.int64),\n      'image/width': tf.FixedLenFeature([], tf.int64),\n      'label/num_instances': tf.FixedLenFeature([], tf.int64),\n      'label/gt_masks': tf.FixedLenFeature([], tf.string),\n      'label/gt_boxes': tf.FixedLenFeature([], tf.string),\n      'label/encoded': tf.FixedLenFeature([], tf.string),\n      })\n  # image = tf.image.decode_jpeg(features['image/encoded'], channels=3)\n  img_id = tf.cast(features['image/img_id'], tf.int32)\n  ih = tf.cast(features['image/height'], tf.int32)\n  iw = tf.cast(features['image/width'], tf.int32)\n  num_instances = tf.cast(features['label/num_instances'], tf.int32)\n  image = tf.decode_raw(features['image/encoded'], tf.uint8)\n  imsize = tf.size(image)\n  image = tf.cond(tf.equal(imsize, ih * iw), \\\n          lambda: tf.image.grayscale_to_rgb(tf.reshape(image, (ih, iw, 1))), \\\n          lambda: tf.reshape(image, (ih, iw, 3)))\n\n  gt_boxes = tf.decode_raw(features['label/gt_boxes'], tf.float32)\n  gt_boxes = tf.reshape(gt_boxes, [num_instances, 5])\n  gt_masks = tf.decode_raw(features['label/gt_masks'], tf.uint8)\n  gt_masks = tf.cast(gt_masks, tf.int32)\n  gt_masks = tf.reshape(gt_masks, [num_instances, ih, iw])\n  \n  return image, ih, iw, gt_boxes, gt_masks, num_instances, img_id\n\n"""
libs/datasets/dataset_factory.py,1,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom libs.visualization.summary_utils import visualize_input\nimport glob\nfrom libs.datasets import coco\n\nimport libs.preprocessings.coco_v1 as coco_preprocess\n\ndef get_dataset(dataset_name, split_name, dataset_dir, \n        im_batch=1, is_training=False, file_pattern=None, reader=None):\n    """"""""""""\n    if file_pattern is None:\n        file_pattern = dataset_name + \'_\' + split_name + \'*.tfrecord\' \n\n    tfrecords = glob.glob(dataset_dir + \'/records/\' + file_pattern)\n    image, ih, iw, gt_boxes, gt_masks, num_instances, img_id = coco.read(tfrecords)\n\n    image, gt_boxes, gt_masks = coco_preprocess.preprocess_image(image, gt_boxes, gt_masks, is_training)\n    #visualize_input(gt_boxes, image, tf.expand_dims(gt_masks, axis=3))\n\n    return image, ih, iw, gt_boxes, gt_masks, num_instances, img_id\n\n'"
libs/datasets/download_and_convert_coco.py,27,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport math\nimport zipfile\nimport time\nimport numpy as np\nimport tensorflow as tf\nfrom six.moves import urllib\nfrom PIL import Image\nimport skimage.io as io\nfrom matplotlib import pyplot as plt\n\nfrom libs.datasets.pycocotools.coco import COCO\nfrom tensorflow.python.lib.io.tf_record import TFRecordCompressionType\nfrom libs.logs.log import LOG\n\n# The URL where the coco data can be downloaded.\n\n_TRAIN_DATA_URL=""https://msvocds.blob.core.windows.net/coco2014/train2014.zip""\n_VAL_DATA_URL=""https://msvocds.blob.core.windows.net/coco2014/val2014.zip""\n_INS_LABEL_URL=""https://msvocds.blob.core.windows.net/annotations-1-0-3/instances_train-val2014.zip""\n_KPT_LABEL_URL=""https://msvocds.blob.core.windows.net/annotations-1-0-3/person_keypoints_trainval2014.zip""\n_CPT_LABEL_URL=""https://msvocds.blob.core.windows.net/annotations-1-0-3/captions_train-val2014.zip""\n_DATA_URLS=[\n  _TRAIN_DATA_URL, _VAL_DATA_URL,\n  _INS_LABEL_URL, _KPT_LABEL_URL, _CPT_LABEL_URL,\n]\n\nFLAGS = tf.app.flags.FLAGS\ntf.app.flags.DEFINE_boolean(\'vis\',  False,\n                          \'Show some visual masks\')\n\n\ndef download_and_uncompress_zip(zip_url, dataset_dir):\n  """"""Downloads the `zip_url` and uncompresses it locally.\n     From: https://github.com/tensorflow/models/blob/master/slim/datasets/dataset_utils.py\n\n  Args:\n    zip_url: The URL of a zip file.\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  filename = zip_url.split(\'/\')[-1]\n  filepath = os.path.join(dataset_dir, filename)\n\n  def _progress(count, block_size, total_size):\n    sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' % (\n        filename, float(count * block_size) / float(total_size) * 100.0))\n    sys.stdout.flush()\n\n  if tf.gfile.Exists(filepath):\n    print(\'Zip file already exist. Skip download..\', filepath)\n  else:\n    filepath, _ = urllib.request.urlretrieve(zip_url, filepath, _progress)\n    print()\n    statinfo = os.stat(filepath)\n    print(\'Successfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n\n  with zipfile.ZipFile(filepath) as f:\n    print(\'Extracting \', filepath)\n    f.extractall(dataset_dir)\n    print(\'Successfully extracted\')\n\ndef _real_id_to_cat_id(catId):\n  """"""Note coco has 80 classes, but the catId ranges from 1 to 90!""""""\n  real_id_to_cat_id = \\\n    {1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 13, 13: 14, 14: 15, 15: 16, 16: 17,\n     17: 18, 18: 19, 19: 20, 20: 21, 21: 22, 22: 23, 23: 24, 24: 25, 25: 27, 26: 28, 27: 31, 28: 32, 29: 33, 30: 34,\n     31: 35, 32: 36, 33: 37, 34: 38, 35: 39, 36: 40, 37: 41, 38: 42, 39: 43, 40: 44, 41: 46, 42: 47, 43: 48, 44: 49,\n     45: 50, 46: 51, 47: 52, 48: 53, 49: 54, 50: 55, 51: 56, 52: 57, 53: 58, 54: 59, 55: 60, 56: 61, 57: 62, 58: 63,\n     59: 64, 60: 65, 61: 67, 62: 70, 63: 72, 64: 73, 65: 74, 66: 75, 67: 76, 68: 77, 69: 78, 70: 79, 71: 80, 72: 81,\n     73: 82, 74: 84, 75: 85, 76: 86, 77: 87, 78: 88, 79: 89, 80: 90}\n  return real_id_to_cat_id[catId]\n\ndef _cat_id_to_real_id(readId):\n  """"""Note coco has 80 classes, but the catId ranges from 1 to 90!""""""\n  cat_id_to_real_id = \\\n    {1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16,\n     18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 27: 25, 28: 26, 31: 27, 32: 28, 33: 29, 34: 30,\n     35: 31, 36: 32, 37: 33, 38: 34, 39: 35, 40: 36, 41: 37, 42: 38, 43: 39, 44: 40, 46: 41, 47: 42, 48: 43, 49: 44,\n     50: 45, 51: 46, 52: 47, 53: 48, 54: 49, 55: 50, 56: 51, 57: 52, 58: 53, 59: 54, 60: 55, 61: 56, 62: 57, 63: 58,\n     64: 59, 65: 60, 67: 61, 70: 62, 72: 63, 73: 64, 74: 65, 75: 66, 76: 67, 77: 68, 78: 69, 79: 70, 80: 71, 81: 72,\n     82: 73, 84: 74, 85: 75, 86: 76, 87: 77, 88: 78, 89: 79, 90: 80}\n  return cat_id_to_real_id[readId]\n  \n\nclass ImageReader(object):\n  def __init__(self):\n    self._decode_data = tf.placeholder(dtype=tf.string)\n    self._decode_jpeg = tf.image.decode_jpeg(self._decode_data, channels=3)\n    self._decode_png = tf.image.decode_png(self._decode_data)\n\n  def read_jpeg_dims(self, sess, image_data):\n    image = self.decode_jpeg(sess, image_data)\n    return image.shape\n\n  def read_png_dims(self, sess, image_data):\n    image = self.decode_png(sess, image_data)\n    return image.shape\n\n  def decode_jpeg(self, sess, image_data):\n    image = sess.run(self._decode_jpeg,\n                     feed_dict={self._decode_data: image_data})\n    assert len(image.shape) == 3\n    assert image.shape[2] == 3\n    return image\n\n  def decode_png(self, sess, image_data):\n    image = sess.run(self._decode_png,\n                     feed_dict={self._decode_data: image_data})\n    assert len(image.shape) == 3\n    assert image.shape[2] == 1\n    return image\n\n\ndef _get_dataset_filename(dataset_dir, split_name, shard_id, num_shards):\n  output_filename = \'coco_%s_%05d-of-%05d.tfrecord\' % (\n      split_name, shard_id, num_shards)\n  return os.path.join(dataset_dir, output_filename)\n\n\ndef _get_image_filenames(image_dir):\n  return sorted(os.listdir(image_dir))\n\n\ndef _int64_feature(values):\n  if not isinstance(values, (tuple, list)):\n    values = [values]\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n\ndef _bytes_feature(values):\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))\n\n\ndef _to_tfexample(image_data, image_format, label_data, label_format, height, width):\n  """"""Encode only masks """"""\n  return tf.train.Example(features=tf.train.Features(feature={\n      \'image/encoded\': _bytes_feature(image_data),\n      \'image/format\': _bytes_feature(image_format),\n      \'image/height\': _int64_feature(height),\n      \'image/width\': _int64_feature(width),\n      \'label/encoded\': _bytes_feature(label_data),\n      \'label/format\': _bytes_feature(label_format),\n      \'label/height\': _int64_feature(height),\n      \'label/width\': _int64_feature(width),\n  }))\n\ndef _to_tfexample_coco(image_data, image_format, label_data, label_format,\n                       height, width,\n                       num_instances, gt_boxes, masks):\n  \n  return tf.train.Example(features=tf.train.Features(feature={\n      \'image/encoded\': _bytes_feature(image_data),\n      \'image/format\': _bytes_feature(image_format),\n      \'image/height\': _int64_feature(height),\n      \'image/width\': _int64_feature(width),\n  \n      \'label/num_instances\': _int64_feature(num_instances), # N\n      \'label/gt_boxes\': _bytes_feature(gt_boxes), # of shape (N, 5), (x1, y1, x2, y2, classid)\n      \'label/gt_masks\': _bytes_feature(masks),       # of shape (N, height, width)\n    \n      \'label/encoded\': _bytes_feature(label_data),  # deprecated, this is used for pixel-level segmentation\n      \'label/format\': _bytes_feature(label_format),\n  }))\n\n\ndef _to_tfexample_coco_raw(image_id, image_data, label_data,\n                           height, width,\n                           num_instances, gt_boxes, masks):\n  """""" just write a raw input""""""\n  return tf.train.Example(features=tf.train.Features(feature={\n    \'image/img_id\': _int64_feature(image_id),\n    \'image/encoded\': _bytes_feature(image_data),\n    \'image/height\': _int64_feature(height),\n    \'image/width\': _int64_feature(width),\n    \'label/num_instances\': _int64_feature(num_instances),  # N\n    \'label/gt_boxes\': _bytes_feature(gt_boxes),  # of shape (N, 5), (x1, y1, x2, y2, classid)\n    \'label/gt_masks\': _bytes_feature(masks),  # of shape (N, height, width)\n    \'label/encoded\': _bytes_feature(label_data),  # deprecated, this is used for pixel-level segmentation\n  }))\n\n\ndef _get_coco_masks(coco, img_id, height, width, img_name):\n  """""" get the masks for all the instances\n  Note: some images are not annotated\n  Return:\n    masks, mxhxw numpy array\n    classes, mx1\n    bboxes, mx4\n  """"""\n  annIds = coco.getAnnIds(imgIds=[img_id], iscrowd=None)\n  # assert  annIds is not None and annIds > 0, \'No annotaion for %s\' % str(img_id)\n  anns = coco.loadAnns(annIds)\n  # assert len(anns) > 0, \'No annotaion for %s\' % str(img_id)\n  masks = []\n  classes = []\n  bboxes = []\n  mask = np.zeros((height, width), dtype=np.float32)\n  segmentations = []\n  for ann in anns:\n    m = coco.annToMask(ann) # zero one mask\n    assert m.shape[0] == height and m.shape[1] == width, \\\n            \'image %s and ann %s dont match\' % (img_id, ann)\n    masks.append(m)\n    cat_id = _cat_id_to_real_id(ann[\'category_id\'])\n    classes.append(cat_id)\n    bboxes.append(ann[\'bbox\'])\n    m = m.astype(np.float32) * cat_id\n    mask[m > 0] = m[m > 0]\n\n  masks = np.asarray(masks)\n  classes = np.asarray(classes)\n  bboxes = np.asarray(bboxes)\n  # to x1, y1, x2, y2\n  if bboxes.shape[0] <= 0:\n    bboxes = np.zeros([0, 4], dtype=np.float32)\n    classes = np.zeros([0], dtype=np.float32)\n    print (\'None Annotations %s\' % img_name)\n    LOG(\'None Annotations %s\' % img_name)\n  bboxes[:, 2] = bboxes[:, 0] + bboxes[:, 2]\n  bboxes[:, 3] = bboxes[:, 1] + bboxes[:, 3]\n  gt_boxes = np.hstack((bboxes, classes[:, np.newaxis]))\n  gt_boxes = gt_boxes.astype(np.float32)\n  masks = masks.astype(np.uint8)\n  mask = mask.astype(np.uint8)\n  assert masks.shape[0] == gt_boxes.shape[0], \'Shape Error\'\n  \n  return gt_boxes, masks, mask\n  \n\n\ndef _add_to_tfrecord(record_dir, image_dir, annotation_dir, split_name):\n  """"""Loads image files and writes files to a TFRecord.\n  Note: masks and bboxes will lose shape info after converting to string.\n  """"""\n\n  assert split_name in [\'train2014\', \'val2014\', \'valminusminival2014\', \'minival2014\']\n  annFile = os.path.join(annotation_dir, \'instances_%s.json\' % (split_name))\n  \n  coco = COCO(annFile)\n\n  cats = coco.loadCats(coco.getCatIds())\n  print (\'%s has %d images\' %(split_name, len(coco.imgs)))\n  imgs = [(img_id, coco.imgs[img_id]) for img_id in coco.imgs]\n  \n  num_shards = int(len(imgs) / 2500)\n  num_per_shard = int(math.ceil(len(imgs) / float(num_shards)))\n  \n  with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n    image_reader = ImageReader()\n    \n    # encode mask to png_string\n    mask_placeholder = tf.placeholder(dtype=tf.uint8)\n    encoded_image = tf.image.encode_png(mask_placeholder)\n    \n    with tf.Session(\'\') as sess:\n      for shard_id in range(num_shards):\n        record_filename = _get_dataset_filename(record_dir, split_name, shard_id, num_shards)\n        options = tf.python_io.TFRecordOptions(TFRecordCompressionType.ZLIB)\n        with tf.python_io.TFRecordWriter(record_filename, options=options) as tfrecord_writer:\n          start_ndx = shard_id * num_per_shard\n          end_ndx = min((shard_id + 1) * num_per_shard, len(imgs))\n          for i in range(start_ndx, end_ndx):\n            if i % 50 == 0:\n                sys.stdout.write(\'\\r>> Converting image %d/%d shard %d\\n\' % (\n                  i + 1, len(imgs), shard_id))\n                sys.stdout.flush()\n            \n            # image id and path\n            img_id = imgs[i][0]\n            img_name = imgs[i][1][\'file_name\']\n            split = img_name.split(\'_\')[1]\n            img_name = os.path.join(image_dir, split, img_name)\n            \n            if FLAGS.vis:\n              im = Image.open(img_name)\n              im.save(\'img.png\')\n              plt.figure(0)\n              plt.axis(\'off\')\n              plt.imshow(im)\n              # plt.show()\n              # plt.close()\n            \n            # jump over the damaged images\n            if str(img_id) == \'320612\':\n              continue\n            \n            # process anns\n            height, width = imgs[i][1][\'height\'], imgs[i][1][\'width\']\n            gt_boxes, masks, mask = _get_coco_masks(coco, img_id, height, width, img_name)\n            \n            # read image as RGB numpy\n            img = np.array(Image.open(img_name))\n            if img.size == height * width:\n                print (\'Gray Image %s\' % str(img_id))\n                im = np.empty((height, width, 3), dtype=np.uint8)\n                im[:, :, :] = img[:, :, np.newaxis]\n                img = im\n\n            img = img.astype(np.uint8)\n            assert img.size == width * height * 3, \'%s\' % str(img_id)\n\n            img_raw = img.tostring()\n            mask_raw = mask.tostring()\n            \n            example = _to_tfexample_coco_raw(\n              img_id,\n              img_raw,\n              mask_raw,\n              height, width, gt_boxes.shape[0],\n              gt_boxes.tostring(), masks.tostring())\n            \n            tfrecord_writer.write(example.SerializeToString())\n  sys.stdout.write(\'\\n\')\n  sys.stdout.flush()\n\ndef _add_to_tfrecord_trainvalsplit(record_dir, image_dir, annotation_dir, split_name):\n  """"""Loads image files and writes files to a TFRecord.\n  Note: masks and bboxes will lose shape info after converting to string.\n  """"""\n\n  assert split_name in [\'trainval2014\', \'minival2014\']\n  # NOTE: this instances_minival2014.json file cannot be processed by official COCO API,\n  # so just use its id list, [\'images\'][\'id\']\n  minival_path = os.path.join(annotation_dir, \'instances_minival2014.json\')\n  minival2014_url=\'https://dl.dropboxusercontent.com/s/o43o90bna78omob/instances_minival2014.json.zip?dl=0\'\n  assert os.path.exists(minival_path), \'need to download %s minival split to %s\' %(minival2014_url, minival_path)\n  \n  import ujson as json\n  with open(minival_path, \'r\') as f:\n      minival = json.load(f)\n\n  def is_in_minival(img_id, minival):\n      for img in minival[\'images\']:\n          if (img[\'id\']) == (img_id):\n              return True\n      return False\n\n  annFile = os.path.join(annotation_dir, \'instances_train2014.json\')\n  coco_train = COCO(annFile)\n  annFile = os.path.join(annotation_dir, \'instances_val2014.json\')\n  coco_val = COCO(annFile)\n  cats = coco_train.loadCats(coco_train.getCatIds())\n  # imgs = [(img_id, coco_train.imgs[img_id]) for img_id in coco_train.imgs] + \\ \n  #        [(img_id, coco_val.imgs[img_id]) for img_id in coco_val.imgs]\n  \n  imgs1 = [(img_id, coco_train.imgs[img_id]) for img_id in coco_train.imgs]\n  imgs2 = [(img_id, coco_val.imgs[img_id]) for img_id in coco_val.imgs]\n  imgs = imgs1 + imgs2\n\n  num_of_train = len(coco_train.imgs)\n  num_of_all   = len(imgs)\n\n  num_per_shard = 2500\n  num_shards = int(np.ceil((len(imgs) + 0.0 - len(minival[\'images\'])) / num_per_shard))\n  if split_name == \'minival2014\':\n    num_shards = int(np.ceil((len(minival[\'images\']) + 0.0) / num_per_shard))\n      \n  with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n  \n    image_reader = ImageReader()\n    \n    # encode mask to png_string\n    mask_placeholder = tf.placeholder(dtype=tf.uint8)\n    encoded_image = tf.image.encode_png(mask_placeholder)\n    \n    with tf.Session(\'\') as sess:\n        \n        cnt = 0\n        shard_id = -1\n        for i in range(len(imgs)):\n            img_id = imgs[i][0]\n            img_name = imgs[i][1][\'file_name\']\n            split = img_name.split(\'_\')[1]\n            img_name = os.path.join(image_dir, split, img_name)\n\n            if str(img_id) == \'320612\':\n                continue\n            is_minival = is_in_minival(img_id, minival)\n\n            if split_name == \'trainval2014\' and is_minival:\n                continue\n            if split_name == \'minival2014\' and not is_minival:\n                continue\n\n            cnt += 1\n            \n            if cnt % num_per_shard == 1:\n                shard_id += 1\n                record_filename = _get_dataset_filename(record_dir, split_name, shard_id, num_shards)\n                options = tf.python_io.TFRecordOptions(TFRecordCompressionType.ZLIB)\n                tfrecord_writer = tf.python_io.TFRecordWriter(record_filename, options=options) \n\n            if cnt % 100 == 1:\n                print (\'%d (image_id: %d) of %d, split: %s, shard_id: %d\' %(i, img_id, len(imgs), split_name, shard_id))\n\n            \n            # process anns\n            height, width = imgs[i][1][\'height\'], imgs[i][1][\'width\']\n            coco = coco_train if i < num_of_train else coco_val\n\n            gt_boxes, masks, mask = _get_coco_masks(coco, img_id, height, width, img_name)\n            \n            # read image as RGB numpy\n            img = np.array(Image.open(img_name))\n            if img.size == height * width:\n                print (\'Gray Image %s\' % str(img_id))\n                im = np.empty((height, width, 3), dtype=np.uint8)\n                im[:, :, :] = img[:, :, np.newaxis]\n                img = im\n\n            img = img.astype(np.uint8)\n            assert img.size == width * height * 3, \'%s\' % str(img_id)\n\n            img_raw = img.tostring()\n            mask_raw = mask.tostring()\n            \n            example = _to_tfexample_coco_raw(\n              img_id,\n              img_raw,\n              mask_raw,\n              height, width, gt_boxes.shape[0],\n              gt_boxes.tostring(), masks.tostring())\n            \n            tfrecord_writer.write(example.SerializeToString())\n\n            if cnt % num_per_shard == 0 or i == len(imgs)-1:\n                tfrecord_writer.close()\n\n\ndef run(dataset_dir, dataset_split_name=\'train2014\'):\n  """"""Runs the download and conversion operation.\n\n  Args:\n    dataset_dir: The dataset directory where the dataset is stored.\n  """"""\n  if not tf.gfile.Exists(dataset_dir):\n    tf.gfile.MakeDirs(dataset_dir)\n\n  # for url in _DATA_URLS:\n  #   download_and_uncompress_zip(url, dataset_dir)\n\n  record_dir     = os.path.join(dataset_dir, \'records\')\n  annotation_dir = os.path.join(dataset_dir, \'annotations\')\n\n  if not tf.gfile.Exists(record_dir):\n    tf.gfile.MakeDirs(record_dir)\n\n  # process the training, validation data:\n  if dataset_split_name in [\'train2014\', \'val2014\']:\n      _add_to_tfrecord(record_dir,\n                       dataset_dir,\n                       annotation_dir,\n                       dataset_split_name)\n\n  if dataset_split_name in [\'trainval2014\', \'minival2014\']:\n      _add_to_tfrecord_trainvalsplit(\n                        record_dir,\n                        dataset_dir,\n                        annotation_dir,\n                        dataset_split_name)\n  \n  print(\'\\nFinished converting the coco dataset!\')\n'"
libs/layers/__init__.py,0,b'# --------------------------------------------------------\n# Mask RCNN\n# Written by CharlesShang@github\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom .wrapper import anchor_decoder\nfrom .wrapper import anchor_encoder\nfrom .wrapper import roi_decoder\nfrom .wrapper import roi_encoder\nfrom .wrapper import mask_decoder\nfrom .wrapper import mask_encoder\nfrom .wrapper import sample_wrapper as sample_rpn_outputs\nfrom .wrapper import sample_with_gt_wrapper as sample_rpn_outputs_with_gt\nfrom .wrapper import gen_all_anchors\nfrom .wrapper import assign_boxes\nfrom .crop import crop as ROIAlign\nfrom .crop import crop_ as ROIAlign_\n'
libs/layers/anchor.py,1,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nimport libs.boxes.cython_bbox as cython_bbox\nimport libs.configs.config_v1 as cfg\nfrom libs.boxes.bbox_transform import bbox_transform, bbox_transform_inv, clip_boxes\nfrom libs.boxes.anchor import anchors_plane\nfrom libs.logs.log import LOG\n# FLAGS = tf.app.flags.FLAGS\n\n_DEBUG = False\n\ndef encode(gt_boxes, all_anchors, height, width, stride):\n  """"""Matching and Encoding groundtruth into learning targets\n  Sampling\n  \n  Parameters\n  ---------\n  gt_boxes: an array of shape (G x 5), [x1, y1, x2, y2, class]\n  all_anchors: an array of shape (h, w, A, 4),\n  width: width of feature\n  height: height of feature\n  stride: downscale factor w.r.t the input size, e.g., [4, 8, 16, 32]\n  Returns\n  --------\n  labels:   Nx1 array in [0, num_classes]\n  bbox_targets: N x (4) regression targets\n  bbox_inside_weights: N x (4), in {0, 1} indicating to which class is assigned.\n  """"""\n  # TODO: speedup this module\n  # if all_anchors is None:\n  #   all_anchors = anchors_plane(height, width, stride=stride)\n\n  # # anchors, inds_inside, total_anchors\n  # border = cfg.FLAGS.allow_border\n  # all_anchors = all_anchors.reshape((-1, 4))\n  # inds_inside = np.where(\n  #   (all_anchors[:, 0] >= -border) &\n  #   (all_anchors[:, 1] >= -border) &\n  #   (all_anchors[:, 2] < (width * stride) + border) &\n  #   (all_anchors[:, 3] < (height * stride) + border))[0]\n  # anchors = all_anchors[inds_inside, :]\n  all_anchors = all_anchors.reshape([-1, 4])\n  anchors = all_anchors\n  total_anchors = all_anchors.shape[0]\n\n  # labels = np.zeros((anchors.shape[0], ), dtype=np.float32)\n  labels = np.empty((anchors.shape[0], ), dtype=np.float32)\n  labels.fill(-1)\n\n  if gt_boxes.size > 0:\n      overlaps = cython_bbox.bbox_overlaps(\n                 np.ascontiguousarray(anchors, dtype=np.float),\n                 np.ascontiguousarray(gt_boxes[:, :4], dtype=np.float))\n\n      # if _DEBUG:\n      #     print (\'gt_boxes shape: \', gt_boxes.shape)\n      #     print (\'anchors shape: \', anchors.shape)\n      #     print (\'overlaps shape: \', overlaps.shape)\n\n      gt_assignment = overlaps.argmax(axis=1)  # (A)\n      max_overlaps = overlaps[np.arange(total_anchors), gt_assignment]\n      gt_argmax_overlaps = overlaps.argmax(axis=0)  # G\n      gt_max_overlaps = overlaps[gt_argmax_overlaps,\n                                 np.arange(overlaps.shape[1])]\n\n      labels[max_overlaps < cfg.FLAGS.rpn_bg_threshold] = 0\n      \n      if True:\n        # this is sentive to boxes of little overlaps, no need!\n        # gt_argmax_overlaps = np.where(overlaps == gt_max_overlaps)[0]\n\n        # fg label: for each gt, hard-assign anchor with highest overlap despite its overlaps\n        labels[gt_argmax_overlaps] = 1\n\n        # exclude examples with little overlaps\n        # added later\n        # excludes = np.where(gt_max_overlaps < cfg.FLAGS.bg_threshold)[0]\n        # labels[gt_argmax_overlaps[excludes]] = -1\n\n        if _DEBUG:\n           min_ov = np.min(gt_max_overlaps)\n           max_ov = np.max(gt_max_overlaps)\n           mean_ov = np.mean(gt_max_overlaps)\n           if min_ov < cfg.FLAGS.bg_threshold:\n               LOG(\'ANCHOREncoder: overlaps: (min %.3f mean:%.3f max:%.3f), stride: %d, shape:(h:%d, w:%d)\' \n                       % (min_ov, mean_ov, max_ov, stride, height, width))\n               worst = gt_boxes[np.argmin(gt_max_overlaps)]\n               anc = anchors[gt_argmax_overlaps[np.argmin(gt_max_overlaps)], :]\n               LOG(\'ANCHOREncoder: worst case: overlap: %.3f, box:(%.1f, %.1f, %.1f, %.1f %d), anchor:(%.1f, %.1f, %.1f, %.1f)\'\n                       % (min_ov, worst[0], worst[1], worst[2], worst[3], worst[4],\n                          anc[0], anc[1], anc[2], anc[3]))\n           \n\n      # fg label: above threshold IOU\n      labels[max_overlaps >= cfg.FLAGS.rpn_fg_threshold] = 1\n      # print (np.min(labels), np.max(labels))\n\n      # subsample positive labels if there are too many\n      num_fg = int(cfg.FLAGS.fg_rpn_fraction * cfg.FLAGS.rpn_batch_size)\n      fg_inds = np.where(labels == 1)[0]\n      if len(fg_inds) > num_fg:\n        disable_inds = np.random.choice(fg_inds, size=(len(fg_inds) - num_fg), replace=False)\n        labels[disable_inds] = -1\n  else:\n      # if there is no gt\n      labels[:] = 0\n\n  # TODO: mild hard negative mining\n  # subsample negative labels if there are too many\n  num_fg = np.sum(labels == 1)\n  num_bg = max(min(cfg.FLAGS.rpn_batch_size - num_fg, num_fg * 3), 8)\n  bg_inds = np.where(labels == 0)[0]\n  if len(bg_inds) > num_bg:\n    disable_inds = np.random.choice(bg_inds, size=(len(bg_inds) - num_bg), replace=False)\n    labels[disable_inds] = -1\n\n  bbox_targets = np.zeros((total_anchors, 4), dtype=np.float32)\n  if gt_boxes.size > 0:\n    bbox_targets = _compute_targets(anchors, gt_boxes[gt_assignment, :])\n  bbox_inside_weights = np.zeros((total_anchors, 4), dtype=np.float32)\n  bbox_inside_weights[labels == 1, :] = 0.1\n\n  # # mapping to whole outputs\n  # labels = _unmap(labels, total_anchors, inds_inside, fill=-1)\n  # bbox_targets = _unmap(bbox_targets, total_anchors, inds_inside, fill=0)\n  # bbox_inside_weights = _unmap(bbox_inside_weights, total_anchors, inds_inside, fill=0)\n\n  labels = labels.reshape((1, height, width, -1))\n  bbox_targets = bbox_targets.reshape((1, height, width, -1))\n  bbox_inside_weights = bbox_inside_weights.reshape((1, height, width, -1))\n\n  return labels, bbox_targets, bbox_inside_weights\n\ndef decode(boxes, scores, all_anchors, ih, iw):\n  """"""Decode outputs into boxes\n  Parameters\n  ---------\n  boxes: an array of shape (1, h, w, Ax4)\n  scores: an array of shape (1, h, w, Ax2),\n  all_anchors: an array of shape (1, h, w, Ax4), [x1, y1, x2, y2]\n  \n  Returns\n  --------\n  final_boxes: of shape (R x 4)\n  classes: of shape (R) in {0,1,2,3... K-1}\n  scores: of shape (R) in [0 ~ 1]\n  """"""\n  # h, w = boxes.shape[1], boxes.shape[2]\n  # if all_anchors is  None:\n  #   stride = 2 ** int(round(np.log2((iw + 0.0) / w)))\n  #   all_anchors = anchors_plane(h, w, stride=stride)\n  all_anchors = all_anchors.reshape((-1, 4))\n  boxes = boxes.reshape((-1, 4))\n  scores = scores.reshape((-1, 2))\n  assert scores.shape[0] == boxes.shape[0] == all_anchors.shape[0], \\\n    \'Anchor layer shape error %d vs %d vs %d\' % (scores.shape[0],boxes.shape[0],all_anchors.reshape[0])\n  boxes = bbox_transform_inv(all_anchors, boxes)\n  classes = np.argmax(scores, axis=1)\n  scores = scores[:, 1]\n  final_boxes = boxes  \n  final_boxes = clip_boxes(final_boxes, (ih, iw))\n  classes = classes.astype(np.int32)\n  return final_boxes, classes, scores\n\ndef sample(boxes, scores, ih, iw, is_training):\n  """"""\n  Sampling the anchor layer outputs for next stage, mask or roi prediction or roi\n  \n  Params\n  ----------\n  boxes:  of shape (? ,4)\n  scores: foreground prob\n  ih:     image height\n  iw:     image width\n  is_training:  \'test\' or \'train\'\n  \n  Returns\n  ----------\n  rois: of shape (N, 4)\n  scores: of shape (N, 1)\n  batch_ids:\n  """"""\n  return\n\n\ndef _unmap(data, count, inds, fill=0):\n  """""" Unmap a subset of item (data) back to the original set of items (of\n  size count) """"""\n  if len(data.shape) == 1:\n    ret = np.empty((count,), dtype=np.float32)\n    ret.fill(fill)\n    ret[inds] = data\n  else:\n    ret = np.empty((count,) + data.shape[1:], dtype=np.float32)\n    ret.fill(fill)\n    ret[inds, :] = data\n  return ret\n  \ndef _compute_targets(ex_rois, gt_rois):\n  """"""Compute bounding-box regression targets for an image.""""""\n\n  assert ex_rois.shape[0] == gt_rois.shape[0]\n  assert ex_rois.shape[1] == 4\n  assert gt_rois.shape[1] == 5\n\n  return bbox_transform(ex_rois, gt_rois[:, :4]).astype(np.float32, copy=False)\n\nif __name__ == \'__main__\':\n  \n  import time\n  t = time.time()\n  \n  for i in range(10):\n    cfg.FLAGS.fg_threshold = 0.1\n    classes = np.random.randint(0, 3, (50, 1))\n    boxes = np.random.randint(10, 50, (50, 2))\n    s = np.random.randint(20, 50, (50, 2))\n    s = boxes + s\n    boxes = np.concatenate((boxes, s), axis=1)\n    gt_boxes = np.hstack((boxes, classes))\n    # gt_boxes = boxes\n    rois = np.random.randint(10, 50, (20, 2))\n    s = np.random.randint(0, 20, (20, 2))\n    s = rois + s\n    rois = np.concatenate((rois, s), axis=1)\n    labels, bbox_targets, bbox_inside_weights = encode(gt_boxes, all_anchors=None, height=200, width=300, stride=4)\n    labels, bbox_targets, bbox_inside_weights = encode(gt_boxes, all_anchors=None, height=100, width=150, stride=8)\n    labels, bbox_targets, bbox_inside_weights = encode(gt_boxes, all_anchors=None, height=50, width=75, stride=16)\n    labels, bbox_targets, bbox_inside_weights = encode(gt_boxes, all_anchors=None, height=25, width=37, stride=32)\n    # anchors, _, _ = anchors_plane(200, 300, stride=4, boarder=0)\n  \n  print(\'average time: %f\' % ((time.time() - t)/10.0))\n'"
libs/layers/assign.py,1,"b'#!/usr/bin/env python\n# coding=utf-8\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nimport libs.boxes.cython_bbox as cython_bbox\nimport libs.configs.config_v1 as cfg\nfrom libs.boxes.bbox_transform import bbox_transform, bbox_transform_inv, clip_boxes\nfrom libs.boxes.anchor import anchors_plane\nfrom libs.logs.log import LOG\n# FLAGS = tf.app.flags.FLAGS\n\n_DEBUG = False\n\ndef assign_boxes(gt_boxes, min_k=2, max_k=5):\n    """"""assigning boxes to layers in a pyramid according to its area\n    Params\n    -----\n    gt_boxes: of shape (N, 5), each entry is [x1, y1, x2, y2, cls]\n    strides:  the stride of each layer, like [4, 8, 16, 32]\n\n    Returns\n    -----\n    layer_ids: of shape (N,), each entry is a id indicating the assigned layer id\n    """"""\n    k0 = 4\n    if gt_boxes.size > 0:\n        layer_ids = np.zeros((gt_boxes.shape[0], ), dtype=np.int32)\n        ws = gt_boxes[:, 2] - gt_boxes[:, 0]\n        hs = gt_boxes[:, 3] - gt_boxes[:, 1]\n        areas = ws * hs\n        k = np.floor(k0 + np.log2(np.sqrt(areas) / 224))\n        inds = np.where(k < min_k)[0]\n        k[inds] = min_k\n        inds = np.where(k > max_k)[0]\n        k[inds] = max_k\n        if _DEBUG: \n            print (""### boxes and layer ids"")\n            print (np.hstack((gt_boxes[:, 0:4], k[:, np.newaxis])))\n        return k.astype(np.int32)\n\n    else:\n        return np.asarray([], dtype=np.int32)\n'"
libs/layers/crop.py,30,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\ndef crop(images, boxes, batch_inds, stride = 1, pooled_height = 7, pooled_width = 7, scope=\'ROIAlign\'):\n  """"""Cropping areas of features into fixed size\n  Params:\n  --------\n  images: a 4-d Tensor of shape (N, H, W, C)\n  boxes: rois in the original image, of shape (N, ..., 4), [x1, y1, x2, y2]\n  batch_inds: \n\n  Returns:\n  --------\n  A Tensor of shape (N, pooled_height, pooled_width, C)\n  """"""\n  with tf.name_scope(scope):\n    #\n    boxes = boxes / (stride + 0.0)\n    boxes = tf.reshape(boxes, [-1, 4])\n\n    # normalize the boxes and swap x y dimensions\n    shape = tf.shape(images)\n    boxes = tf.reshape(boxes, [-1, 2]) # to (x, y)\n    xs = boxes[:, 0] \n    ys = boxes[:, 1]\n    xs = xs / tf.cast(shape[2], tf.float32)\n    ys = ys / tf.cast(shape[1], tf.float32)\n    boxes = tf.concat([ys[:, tf.newaxis], xs[:, tf.newaxis]], axis=1)\n    boxes = tf.reshape(boxes, [-1, 4])  # to (y1, x1, y2, x2)\n    \n    # if batch_inds is False:\n    #   num_boxes = tf.shape(boxes)[0]\n    #   batch_inds = tf.zeros([num_boxes], dtype=tf.int32, name=\'batch_inds\')\n    # batch_inds = boxes[:, 0] * 0\n    # batch_inds = tf.cast(batch_inds, tf.int32)\n\n    # assert_op = tf.Assert(tf.greater(tf.shape(images)[0], tf.reduce_max(batch_inds)), [images, batch_inds])\n    assert_op = tf.Assert(tf.greater(tf.size(images), 0), [images, batch_inds])\n    with tf.control_dependencies([assert_op, images, batch_inds]):\n        return  tf.image.crop_and_resize(images, boxes, batch_inds,\n                                         [pooled_height, pooled_width],\n                                         method=\'bilinear\',\n                                         name=\'Crop\')\n\ndef crop_(images, boxes, batch_inds, ih, iw, stride = 1, pooled_height = 7, pooled_width = 7, scope=\'ROIAlign\'):\n  """"""Cropping areas of features into fixed size\n  Params:\n  --------\n  images: a 4-d Tensor of shape (N, H, W, C)\n  boxes: rois in the original image, of shape (N, ..., 4), [x1, y1, x2, y2]\n  batch_inds: \n\n  Returns:\n  --------\n  A Tensor of shape (N, pooled_height, pooled_width, C)\n  """"""\n  with tf.name_scope(scope):\n    #\n    boxes = boxes / (stride + 0.0)\n    boxes = tf.reshape(boxes, [-1, 4])\n\n    # normalize the boxes and swap x y dimensions\n    shape = tf.shape(images)\n    boxes = tf.reshape(boxes, [-1, 2]) # to (x, y)\n    xs = boxes[:, 0] \n    ys = boxes[:, 1]\n    xs = xs / tf.cast(shape[2], tf.float32)\n    ys = ys / tf.cast(shape[1], tf.float32)\n    boxes = tf.concat([ys[:, tf.newaxis], xs[:, tf.newaxis]], axis=1)\n    boxes = tf.reshape(boxes, [-1, 4])  # to (y1, x1, y2, x2)\n    \n    # if batch_inds is False:\n    #   num_boxes = tf.shape(boxes)[0]\n    #   batch_inds = tf.zeros([num_boxes], dtype=tf.int32, name=\'batch_inds\')\n    # batch_inds = boxes[:, 0] * 0\n    # batch_inds = tf.cast(batch_inds, tf.int32)\n\n    # assert_op = tf.Assert(tf.greater(tf.shape(images)[0], tf.reduce_max(batch_inds)), [images, batch_inds])\n    assert_op = tf.Assert(tf.greater(tf.size(images), 0), [images, batch_inds])\n    with tf.control_dependencies([assert_op, images, batch_inds]):\n        return  [tf.image.crop_and_resize(images, boxes, batch_inds,\n                                         [pooled_height, pooled_width],\n                                         method=\'bilinear\',\n                                         name=\'Crop\')] + [boxes]\n\n'"
libs/layers/mask.py,0,"b'# coding=utf-8\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport cv2\nimport libs.boxes.cython_bbox as cython_bbox\nimport libs.configs.config_v1 as cfg\nfrom libs.logs.log import LOG\nfrom libs.boxes.bbox_transform import bbox_transform, bbox_transform_inv, clip_boxes\n\n_DEBUG = False \ndef encode(gt_masks, gt_boxes, rois, num_classes, mask_height, mask_width):\n  """"""Encode masks groundtruth into learnable targets\n  Sample some exmaples\n  \n  Params\n  ------\n  gt_masks: image_height x image_width {0, 1} matrix, of shape (G, imh, imw)\n  gt_boxes: ground-truth boxes of shape (G, 5), each raw is [x1, y1, x2, y2, class]\n  rois:     the bounding boxes of shape (N, 4),\n  ## scores:   scores of shape (N, 1)\n  num_classes; K\n  mask_height, mask_width: height and width of output masks\n  \n  Returns\n  -------\n  # rois: boxes sampled for cropping masks, of shape (M, 4)\n  labels: class-ids of shape (M, 1)\n  mask_targets: learning targets of shape (M, pooled_height, pooled_width, K) in {0, 1} values\n  mask_inside_weights: of shape (M, pooled_height, pooled_width, K) in {0, 1}\xc3\x8d indicating which mask is sampled\n  """"""\n  total_masks = rois.shape[0]\n  if gt_boxes.size > 0: \n      # B x G\n      overlaps = cython_bbox.bbox_overlaps(\n          np.ascontiguousarray(rois[:, 0:4], dtype=np.float),\n          np.ascontiguousarray(gt_boxes[:, :4], dtype=np.float))\n      gt_assignment = overlaps.argmax(axis=1)  # shape is N\n      max_overlaps = overlaps[np.arange(len(gt_assignment)), gt_assignment] # N\n      # note: this will assign every rois with a positive label \n      # labels = gt_boxes[gt_assignment, 4] # N\n      labels = np.zeros((total_masks, ), np.float32)\n      labels[:] = -1\n\n      # sample positive rois which intersection is more than 0.5\n      keep_inds = np.where(max_overlaps >= cfg.FLAGS.mask_threshold)[0]\n      num_masks = int(min(keep_inds.size, cfg.FLAGS.masks_per_image))\n      if keep_inds.size > 0 and num_masks < keep_inds.size:\n        keep_inds = np.random.choice(keep_inds, size=num_masks, replace=False)\n        LOG(\'Masks: %d of %d rois are considered positive mask. Number of masks %d\'\\\n                     %(num_masks, rois.shape[0], gt_masks.shape[0]))\n\n      labels[keep_inds] = gt_boxes[gt_assignment[keep_inds], -1]\n        \n      # rois = rois[inds]\n      # labels = labels[inds].astype(np.int32)\n      # gt_assignment = gt_assignment[inds]\n\n      # ignore rois with overlaps between fg_threshold and bg_threshold \n      # mask are only defined on positive rois\n      ignore_inds = np.where((max_overlaps < cfg.FLAGS.fg_threshold))[0]\n      labels[ignore_inds] = -1 \n\n      mask_targets = np.zeros((total_masks, mask_height, mask_width, num_classes), dtype=np.int32)\n      mask_inside_weights = np.zeros((total_masks, mask_height, mask_width, num_classes), dtype=np.float32)\n      rois [rois < 0] = 0\n      \n      # TODO: speed bottleneck?\n      for i in keep_inds:\n        roi = rois[i, :4]\n        cropped = gt_masks[gt_assignment[i], int(roi[1]):int(roi[3])+1, int(roi[0]):int(roi[2])+1]\n        cropped = cv2.resize(cropped, (mask_width, mask_height), interpolation=cv2.INTER_NEAREST)\n        \n        mask_targets[i, :, :, int(labels[i])] = cropped\n        mask_inside_weights[i, :, :, int(labels[i])] = 1\n  else:\n      # there is no gt\n      labels = np.zeros((total_masks, ), np.float32)\n      labels[:] = -1\n      mask_targets = np.zeros((total_masks, mask_height, mask_width, num_classes), dtype=np.int32)\n      mask_inside_weights = np.zeros((total_masks, mask_height, mask_height, num_classes), dtype=np.float32)\n  return labels, mask_targets, mask_inside_weights\n\ndef decode(mask_targets, rois, classes, ih, iw):\n  """"""Decode outputs into final masks\n  Params\n  ------\n  mask_targets: of shape (N, h, w, K)\n  rois: of shape (N, 4) [x1, y1, x2, y2]\n  classes: of shape (N, 1) the class-id of each roi\n  height: image height\n  width:  image width\n  \n  Returns\n  ------\n  M: a painted image with all masks, of shape (height, width), in [0, K]\n  """"""\n  Mask = np.zeros((ih, iw), dtype=np.float32)\n  assert rois.shape[0] == mask_targets.shape[0], \\\n    \'%s rois vs %d masks\' %(rois.shape[0], mask_targets.shape[0])\n  num = rois.shape[0]\n  rois = clip_boxes(rois, (ih, iw))\n  for i in np.arange(num):\n    k = classes[i]\n    mask = mask_targets[i, :, :, k]\n    h, w = rois[i, 3] - rois[i, 1] + 1, rois[i, 2] - rois[i, 0] + 1\n    x, y = rois[i, 0], rois[i, 1]\n    mask = cv2.resize(mask, (w, h), interpolation=cv2.INTER_NEAREST)\n    mask *= k\n    \n    # paint\n    Mask[y:y+h, x:x+w] = mask\n  \n  return Mask\n\n\n\nif __name__ == \'__main__\':\n  \n  import time\n  import matplotlib.pyplot as plt\n  \n  t = time.time()\n  \n  for i in range(10):\n    cfg.FLAGS.mask_threshold = 0.2\n    N = 50\n    W, H = 200, 200\n    M = 50\n    \n    gt_masks = np.zeros((2, H, W), dtype=np.int32)\n    gt_masks[0, 50:150, 50:150] = 1\n    gt_masks[1, 100:150, 50:150] = 1\n    gt_boxes = np.asarray(\n      [\n        [20, 20, 100, 100, 1],\n        [100, 100, 180, 180, 2]\n      ])\n    rois = gt_boxes[:, :4]\n    print (rois)\n    rois, labels, mask_targets, mask_inside_weights = encode(gt_masks, gt_boxes, rois, 3, 7, 7)\n    print (rois)\n    Mask = decode(mask_targets, rois, labels, H, W)\n    if True:\n      plt.figure(1)\n      plt.imshow(Mask)\n      plt.show()\n      time.sleep(2)\n  print(labels)\n  print(\'average time: %f\' % ((time.time() - t) / 10.0))\n  \n'"
libs/layers/roi.py,1,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nimport libs.boxes.cython_bbox as cython_bbox\nimport libs.configs.config_v1 as cfg\nfrom libs.boxes.bbox_transform import bbox_transform, bbox_transform_inv, clip_boxes\nfrom libs.logs.log import LOG \n\n# FLAGS = tf.app.flags.FLAGS\n\n_DEBUG = False \n\ndef encode(gt_boxes, rois, num_classes):\n  """"""Matching and Encoding groundtruth boxes (gt_boxes) into learning targets to boxes\n  Sampling\n  Parameters\n  ---------\n  gt_boxes an array of shape (G x 5), [x1, y1, x2, y2, class]\n  rois an array of shape (R x 4), [x1, y1, x2, y2]\n  num_classes: scalar, number of classes\n  \n  Returns\n  --------\n  labels: Nx1 array in [0, num_classes)\n  bbox_targets: of shape (N, Kx4) regression targets\n  bbox_inside_weights: of shape (N, Kx4), in {0, 1} indicating which class is assigned.\n  """"""\n  \n  all_rois = rois\n  num_rois = rois.shape[0]\n  if gt_boxes.size > 0: \n      # R x G matrix\n      overlaps = cython_bbox.bbox_overlaps(\n        np.ascontiguousarray(all_rois[:, 0:4], dtype=np.float),\n        np.ascontiguousarray(gt_boxes[:, :4], dtype=np.float))\n      gt_assignment = overlaps.argmax(axis=1)  # R\n      # max_overlaps = overlaps.max(axis=1)      # R\n      max_overlaps = overlaps[np.arange(rois.shape[0]), gt_assignment]\n      # note: this will assign every rois with a positive label \n      # labels = gt_boxes[gt_assignment, 4]\n      labels = np.zeros([num_rois], dtype=np.float32)\n      labels[:] = -1\n\n      # if _DEBUG:\n      #     print (\'gt_assignment\')\n      #     print (gt_assignment)\n\n      # sample rois as to 1:3\n      fg_inds = np.where(max_overlaps >= cfg.FLAGS.fg_threshold)[0]\n      fg_rois = int(min(fg_inds.size, cfg.FLAGS.rois_per_image * cfg.FLAGS.fg_roi_fraction))\n      if fg_inds.size > 0 and fg_rois < fg_inds.size:\n        fg_inds = np.random.choice(fg_inds, size=fg_rois, replace=False)\n      labels[fg_inds] = gt_boxes[gt_assignment[fg_inds], 4] \n      \n      # TODO: sampling strategy\n      bg_inds = np.where((max_overlaps < cfg.FLAGS.bg_threshold))[0]\n      bg_rois = max(min(cfg.FLAGS.rois_per_image - fg_rois, fg_rois * 3), 64)\n      if bg_inds.size > 0 and bg_rois < bg_inds.size:\n        bg_inds = np.random.choice(bg_inds, size=bg_rois, replace=False)\n      labels[bg_inds] = 0\n      \n      # ignore rois with overlaps between fg_threshold and bg_threshold \n      ignore_inds = np.where(((max_overlaps > cfg.FLAGS.bg_threshold) &\\\n              (max_overlaps < cfg.FLAGS.fg_threshold)))[0]\n      labels[ignore_inds] = -1 \n\n      keep_inds = np.append(fg_inds, bg_inds)\n      if _DEBUG: \n          print (\'keep_inds\')\n          print (keep_inds)\n          print (\'fg_inds\')\n          print (fg_inds)\n          print (\'bg_inds\')\n          print (bg_inds)\n          print (\'bg_rois:\', bg_rois)\n          print (\'cfg.FLAGS.bg_threshold:\', cfg.FLAGS.bg_threshold)\n          # print (max_overlaps)\n\n          LOG(\'ROIEncoder: %d positive rois, %d negative rois\' % (len(fg_inds), len(bg_inds)))\n\n      bbox_targets, bbox_inside_weights = _compute_targets(\n        rois[keep_inds, 0:4], gt_boxes[gt_assignment[keep_inds], :4], labels[keep_inds], num_classes)\n      bbox_targets = _unmap(bbox_targets, num_rois, keep_inds, 0)\n      bbox_inside_weights = _unmap(bbox_inside_weights, num_rois, keep_inds, 0)\n   \n  else:\n      # there is no gt\n      labels = np.zeros((num_rois, ), np.float32)\n      bbox_targets = np.zeros((num_rois, 4 * num_classes), np.float32)\n      bbox_inside_weights = np.zeros((num_rois, 4 * num_classes), np.float32)\n      bg_rois  = min(int(cfg.FLAGS.rois_per_image * (1 - cfg.FLAGS.fg_roi_fraction)), 64)\n      if bg_rois < num_rois:\n          bg_inds = np.arange(num_rois)\n          ignore_inds = np.random.choice(bg_inds, size=num_rois - bg_rois, replace=False)\n          labels[ignore_inds] = -1 \n\n  return labels, bbox_targets, bbox_inside_weights\n\ndef decode(boxes, scores, rois, ih, iw):\n  """"""Decode prediction targets into boxes and only keep only one boxes of greatest possibility for each rois\n    Parameters\n  ---------\n  boxes: an array of shape (R, Kx4), [x1, y1, x2, y2, x1, x2, y1, y2]\n  scores: an array of shape (R, K),\n  rois: an array of shape (R, 4), [x1, y1, x2, y2]\n  \n  Returns\n  --------\n  final_boxes: of shape (R x 4)\n  classes: of shape (R) in {0,1,2,3... K-1}\n  scores: of shape (R) in [0 ~ 1]\n  """"""\n  boxes = bbox_transform_inv(rois, deltas=boxes)\n  classes = np.argmax(scores, axis=1)\n  classes = classes.astype(np.int32)\n  scores = np.max(scores, axis=1)\n  final_boxes = np.zeros((boxes.shape[0], 4), dtype=np.float32)\n  for i in np.arange(0, boxes.shape[0]):\n    ind = classes[i]*4\n    final_boxes[i, 0:4] = boxes[i, ind:ind+4]\n  final_boxes = clip_boxes(final_boxes, (ih, iw))\n  return final_boxes, classes, scores\n\ndef _compute_targets(ex_rois, gt_rois, labels, num_classes):\n  """"""\n  This function expands those targets into the 4-of-4*K representation used\n  by the network (i.e. only one class has non-zero targets).\n  \n  Returns:\n    bbox_target (ndarray): N x 4K blob of regression targets\n    bbox_inside_weights (ndarray): N x 4K blob of loss weights\n  """"""\n\n  assert ex_rois.shape[0] == gt_rois.shape[0]\n  assert ex_rois.shape[1] == 4\n  assert gt_rois.shape[1] == 4\n\n  targets = bbox_transform(ex_rois, gt_rois)\n\n  clss = labels\n  bbox_targets = np.zeros((clss.size, 4 * num_classes), dtype=np.float32)\n  bbox_inside_weights = np.zeros(bbox_targets.shape, dtype=np.float32)\n  inds = np.where(clss > 0)[0]\n  for ind in inds:\n    cls = int(clss[ind])\n    start = 4 * cls\n    end = start + 4\n    bbox_targets[ind, start:end] = targets[ind, 0:4]\n    bbox_inside_weights[ind, start:end] = 1\n  return bbox_targets, bbox_inside_weights\n\ndef _unmap(data, count, inds, fill=0):\n  """""" Unmap a subset of item (data) back to the original set of items (of\n  size count) """"""\n  if len(data.shape) == 1:\n    ret = np.empty((count,), dtype=np.float32)\n    ret.fill(fill)\n    ret[inds] = data\n  else:\n    ret = np.empty((count,) + data.shape[1:], dtype=np.float32)\n    ret.fill(fill)\n    ret[inds, :] = data\n  return ret\n\nif __name__ == \'__main__\':\n  cfg.FLAGS.fg_threshold = 0.1\n  classes = np.random.randint(0, 3, (10, 1))\n  boxes = np.random.randint(10, 50, (10, 2))\n  s = np.random.randint(10, 20, (10, 2))\n  s = boxes + s\n  boxes = np.concatenate((boxes, s), axis=1)\n  gt_boxes = np.hstack((boxes, classes))\n  noise = np.random.randint(-3, 3, (10, 4))\n  rois = gt_boxes[:, :4] + noise\n  labels, rois, bbox_targets, bbox_inside_weights = encode(gt_boxes, rois, num_classes=3)\n  print (labels)\n  print (bbox_inside_weights)\n  \n  ls = np.zeros((labels.shape[0], 3))\n  for i in range(labels.shape[0]):\n    ls[i, labels[i]] = 1\n  final_boxes, classes, scores = decode(bbox_targets, ls, rois, 100, 100)\n  print(\'gt_boxes:\\n\', gt_boxes)\n  print (\'final boxes:\\n\', np.hstack((final_boxes, np.expand_dims(classes, axis=1))).astype(np.int32))\n  # print (final_boxes.astype(np.int32))\n'"
libs/layers/sample.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\n\nimport libs.configs.config_v1 as cfg\nimport libs.boxes.nms_wrapper as nms_wrapper\nimport libs.boxes.cython_bbox as cython_bbox\nfrom libs.boxes.bbox_transform import bbox_transform, bbox_transform_inv, clip_boxes\nfrom libs.logs.log import LOG\n\n_DEBUG=False\n\ndef sample_rpn_outputs(boxes, scores, is_training=False, only_positive=False):\n  """"""Sample boxes according to scores and some learning strategies\n  assuming the first class is background\n  Params:\n  boxes: of shape (..., Ax4), each entry is [x1, y1, x2, y2], the last axis has k*4 dims\n  scores: of shape (..., A), probs of fg, in [0, 1]\n  """"""\n  min_size = cfg.FLAGS.min_size\n  rpn_nms_threshold = cfg.FLAGS.rpn_nms_threshold\n  pre_nms_top_n = cfg.FLAGS.pre_nms_top_n\n  post_nms_top_n = cfg.FLAGS.post_nms_top_n\n\n  # training: 12000, 2000\n  # testing: 6000, 400\n  if not is_training:\n    pre_nms_top_n = int(pre_nms_top_n / 2)\n    post_nms_top_n = int(post_nms_top_n / 5)\n    \n  boxes = boxes.reshape((-1, 4))\n  scores = scores.reshape((-1, 1))\n  assert scores.shape[0] == boxes.shape[0], \'scores and boxes dont match\'\n  \n  # filter backgrounds\n  # Hope this will filter most of background anchors, since a argsort is too slow..\n  if only_positive:\n    keeps = np.where(scores > 0.5)[0]\n    boxes = boxes[keeps, :]\n    scores = scores[keeps]\n  \n  # filter minimum size\n  keeps = _filter_boxes(boxes, min_size=min_size)\n  boxes = boxes[keeps, :]\n  scores = scores[keeps]\n  \n  # filter with scores\n  order = scores.ravel().argsort()[::-1]\n  if pre_nms_top_n > 0:\n    order = order[:pre_nms_top_n]\n  boxes = boxes[order, :]\n  scores = scores[order]\n\n  # filter with nms\n  det = np.hstack((boxes, scores)).astype(np.float32)\n  keeps = nms_wrapper.nms(det, rpn_nms_threshold)\n  \n  if post_nms_top_n > 0:\n    keeps = keeps[:post_nms_top_n]\n  boxes = boxes[keeps, :]\n  scores = scores[keeps]\n  batch_inds = np.zeros([boxes.shape[0]], dtype=np.int32)\n\n  # # random sample boxes\n  ## try early sample later\n  # fg_inds = np.where(scores > 0.5)[0]\n  # num_fgs = min(len(fg_inds.size), int(rois_per_image * fg_roi_fraction))\n\n  if _DEBUG:\n    LOG(\'SAMPLE: %d rois has been choosen\' % len(scores))\n    LOG(\'SAMPLE: a positive box: %d %d %d %d %.4f\' % (boxes[0, 0], boxes[0, 1], boxes[0, 2], boxes[0, 3], scores[0]))\n    LOG(\'SAMPLE: a negative box: %d %d %d %d %.4f\' % (boxes[-1, 0], boxes[-1, 1], boxes[-1, 2], boxes[-1, 3], scores[-1]))\n    hs = boxes[:, 3] - boxes[:, 1]\n    ws = boxes[:, 2] - boxes[:, 0]\n    assert min(np.min(hs), np.min(ws)) > 0, \'invalid boxes\'\n  \n  return boxes, scores.astype(np.float32), batch_inds\n\ndef sample_rpn_outputs_wrt_gt_boxes(boxes, scores, gt_boxes, is_training=False, only_positive=False):\n    """"""sample boxes for refined output""""""\n    boxes, scores, batch_inds = sample_rpn_outputs(boxes, scores, is_training, only_positive)\n\n    if gt_boxes.size > 0:\n        overlaps = cython_bbox.bbox_overlaps(\n                np.ascontiguousarray(boxes[:, 0:4], dtype=np.float),\n                np.ascontiguousarray(gt_boxes[:, 0:4], dtype=np.float))\n        gt_assignment = overlaps.argmax(axis=1) # B\n        max_overlaps = overlaps[np.arange(boxes.shape[0]), gt_assignment] # B\n        fg_inds = np.where(max_overlaps >= cfg.FLAGS.fg_threshold)[0]\n        if _DEBUG and np.argmax(overlaps[fg_inds],axis=1).size < gt_boxes.size/5.0:\n            print(""gt_size"")\n            print(gt_boxes)\n            gt_height = (gt_boxes[:,2]-gt_boxes[:,0])\n            gt_width = (gt_boxes[:,3]-gt_boxes[:,1])\n            gt_dim = np.vstack((gt_height, gt_width))\n            print(np.transpose(gt_dim))\n            #print(gt_height)\n            #print(gt_width)\n\n            print(\'SAMPLE: %d after overlaps by %s\' % (len(fg_inds),cfg.FLAGS.fg_threshold))\n            print(""detected object no."")\n            print(np.argmax(overlaps[fg_inds],axis=1))\n            print(""total object"")\n            print(gt_boxes.size/5.0)\n\n        mask_fg_inds = np.where(max_overlaps >= cfg.FLAGS.mask_threshold)[0]\n        if mask_fg_inds.size > cfg.FLAGS.masks_per_image:\n            mask_fg_inds = np.random.choice(mask_fg_inds, size=cfg.FLAGS.masks_per_image, replace=False)\n\n        if True:\n            gt_argmax_overlaps = overlaps.argmax(axis=0) # G\n            fg_inds = np.union1d(gt_argmax_overlaps, fg_inds)\n\n\tfg_rois = int(min(fg_inds.size, cfg.FLAGS.rois_per_image * cfg.FLAGS.fg_roi_fraction))\n      \tif fg_inds.size > 0 and fg_rois < fg_inds.size:\n       \t   fg_inds = np.random.choice(fg_inds, size=fg_rois, replace=False)\n      \t\n\t# TODO: sampling strategy\n      \tbg_inds = np.where((max_overlaps < cfg.FLAGS.bg_threshold))[0]\n      \tbg_rois = max(min(cfg.FLAGS.rois_per_image - fg_rois, fg_rois * 3), 8)#64\n      \tif bg_inds.size > 0 and bg_rois < bg_inds.size:\n           bg_inds = np.random.choice(bg_inds, size=bg_rois, replace=False)\n\n        keep_inds = np.append(fg_inds, bg_inds)\n        #print(gt_boxes[np.argmax(overlaps[fg_inds],axis=1),4])\n    else:\n        bg_inds = np.arange(boxes.shape[0])\n        bg_rois = min(int(cfg.FLAGS.rois_per_image * (1-cfg.FLAGS.fg_roi_fraction)), 8)#64\n        if bg_rois < bg_inds.size:\n            bg_inds = np.random.choice(bg_inds, size=bg_rois, replace=False)\n\n        keep_inds = bg_inds\n        mask_fg_inds = np.arange(0)\n    \n    return boxes[keep_inds, :], scores[keep_inds], batch_inds[keep_inds],\\\n           boxes[mask_fg_inds, :], scores[mask_fg_inds], batch_inds[mask_fg_inds]\n\ndef _jitter_boxes(boxes, jitter=0.1):\n    """""" jitter the boxes before appending them into rois\n    """"""\n    jittered_boxes = boxes.copy()\n    ws = jittered_boxes[:, 2] - jittered_boxes[:, 0] + 1.0\n    hs = jittered_boxes[:, 3] - jittered_boxes[:, 1] + 1.0\n    width_offset = (np.random.rand(jittered_boxes.shape[0]) - 0.5) * jitter * ws\n    height_offset = (np.random.rand(jittered_boxes.shape[0]) - 0.5) * jitter * hs\n    jittered_boxes[:, 0] += width_offset\n    jittered_boxes[:, 2] += width_offset\n    jittered_boxes[:, 1] += height_offset\n    jittered_boxes[:, 3] += height_offset\n\n    return jittered_boxes\n\ndef _filter_boxes(boxes, min_size):\n  """"""Remove all boxes with any side smaller than min_size.""""""\n  ws = boxes[:, 2] - boxes[:, 0] + 1\n  hs = boxes[:, 3] - boxes[:, 1] + 1\n  keep = np.where((ws >= min_size) & (hs >= min_size))[0]\n  return keep\n\ndef _apply_nms(boxes, scores, threshold = 0.5):\n  """"""After this only positive boxes are left\n  Applying this class-wise\n  """"""\n  num_class = scores.shape[1]\n  assert boxes.shape[0] == scores.shape[0], \\\n    \'Shape dismatch {} vs {}\'.format(boxes.shape, scores.shape)\n  \n  final_boxes = []\n  final_scores = []\n  for cls in np.arange(1, num_class):\n    cls_boxes = boxes[:, 4*cls: 4*cls+4]\n    cls_scores = scores[:, cls]\n    dets = np.hstack((cls_boxes, cls_scores[:, np.newaxis]))\n    keep = nms_wrapper.nms(dets, thresh=0.3)\n    dets = dets[keep, :]\n    dets = dets[np.where(dets[:, 4] > threshold)]\n    final_boxes.append(dets[:, :4])\n    final_scores.append(dets[:, 4])\n  \n  final_boxes = np.vstack(final_boxes)\n  final_scores = np.vstack(final_scores)\n  \n  return final_boxes, final_scores\n\nif __name__ == \'__main__\':\n  import time\n  t = time.time()\n  \n  for i in range(10):\n    N = 200000\n    boxes = np.random.randint(0, 50, (N, 2))\n    s = np.random.randint(10, 40, (N, 2))\n    s = boxes + s\n    boxes = np.hstack((boxes, s))\n    \n    scores = np.random.rand(N, 1)\n    # scores_ = 1 - np.random.rand(N, 1)\n    # scores = np.hstack((scores, scores_))\n  \n    boxes, scores = sample_rpn_outputs(boxes, scores, only_positive=False)\n  \n  print (\'average time %f\' % ((time.time() - t) / 10))\n'"
libs/layers/wrapper.py,78,"b""# --------------------------------------------------------\n# Mask RCNN\n# Written by CharlesShang@github\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom . import anchor\nfrom . import roi\nfrom . import mask\nfrom . import sample\nfrom . import assign\nfrom libs.boxes.anchor import anchors_plane\n\ndef anchor_encoder(gt_boxes, all_anchors, height, width, stride, scope='AnchorEncoder'):\n  \n  with tf.name_scope(scope) as sc:\n    labels, bbox_targets, bbox_inside_weights = \\\n      tf.py_func(anchor.encode,\n                 [gt_boxes, all_anchors, height, width, stride],\n                 [tf.float32, tf.float32, tf.float32])\n    labels = tf.convert_to_tensor(tf.cast(labels, tf.int32), name='labels')\n    bbox_targets = tf.convert_to_tensor(bbox_targets, name='bbox_targets')\n    bbox_inside_weights = tf.convert_to_tensor(bbox_inside_weights, name='bbox_inside_weights')\n    labels = tf.reshape(labels, (1, height, width, -1))\n    bbox_targets = tf.reshape(bbox_targets, (1, height, width, -1))\n    bbox_inside_weights = tf.reshape(bbox_inside_weights, (1, height, width, -1))\n  \n  return labels, bbox_targets, bbox_inside_weights\n\n\ndef anchor_decoder(boxes, scores, all_anchors, ih, iw, scope='AnchorDecoder'):\n  \n  with tf.name_scope(scope) as sc:\n    final_boxes, classes, scores = \\\n      tf.py_func(anchor.decode,\n                 [boxes, scores, all_anchors, ih, iw],\n                 [tf.float32, tf.int32, tf.float32])\n    final_boxes = tf.convert_to_tensor(final_boxes, name='boxes')\n    classes = tf.convert_to_tensor(tf.cast(classes, tf.int32), name='classes')\n    scores = tf.convert_to_tensor(scores, name='scores')\n    final_boxes = tf.reshape(final_boxes, (-1, 4))\n    classes = tf.reshape(classes, (-1, ))\n    scores = tf.reshape(scores, (-1, ))\n  \n  return final_boxes, classes, scores\n\n\ndef roi_encoder(gt_boxes, rois, num_classes, scope='ROIEncoder'):\n  \n  with tf.name_scope(scope) as sc:\n    labels, bbox_targets, bbox_inside_weights = \\\n      tf.py_func(roi.encode,\n                [gt_boxes, rois, num_classes],\n                [tf.float32, tf.float32, tf.float32])\n    labels = tf.convert_to_tensor(tf.cast(labels, tf.int32), name='labels')\n    bbox_targets = tf.convert_to_tensor(bbox_targets, name='bbox_targets')\n    bbox_inside_weights = tf.convert_to_tensor(bbox_inside_weights, name='bbox_inside_weights')\n    labels = tf.reshape(labels, (-1, ))\n    bbox_targets = tf.reshape(bbox_targets, (-1, num_classes * 4))\n    bbox_inside_weights = tf.reshape(bbox_inside_weights, (-1, num_classes * 4))\n  \n  return labels, bbox_targets, bbox_inside_weights\n\n\ndef roi_decoder(boxes, scores, rois, ih, iw, scope='ROIDecoder'):\n  \n  with tf.name_scope(scope) as sc:\n    final_boxes, classes, scores = \\\n      tf.py_func(roi.decode,\n                 [boxes, scores, rois, ih, iw],\n                 [tf.float32, tf.int32, tf.float32])\n    final_boxes = tf.convert_to_tensor(final_boxes, name='boxes')\n    classes = tf.convert_to_tensor(tf.cast(classes, tf.int32), name='classes')\n    scores = tf.convert_to_tensor(scores, name='scores')\n    final_boxes = tf.reshape(final_boxes, (-1, 4))\n    \n  return final_boxes, classes, scores\n\ndef mask_encoder(gt_masks, gt_boxes, rois, num_classes, mask_height, mask_width, scope='MaskEncoder'):\n  \n  with tf.name_scope(scope) as sc:\n    labels, mask_targets, mask_inside_weights = \\\n      tf.py_func(mask.encode,\n                 [gt_masks, gt_boxes, rois, num_classes, mask_height, mask_width],\n                 [tf.float32, tf.int32, tf.float32])\n    labels = tf.convert_to_tensor(tf.cast(labels, tf.int32), name='classes')\n    mask_targets = tf.convert_to_tensor(mask_targets, name='mask_targets')\n    mask_inside_weights = tf.convert_to_tensor(mask_inside_weights, name='mask_inside_weights')\n    labels = tf.reshape(labels, (-1,))\n    mask_targets = tf.reshape(mask_targets, (-1, mask_height, mask_width, num_classes))\n    mask_inside_weights = tf.reshape(mask_inside_weights, (-1, mask_height, mask_width, num_classes))\n  \n  return labels, mask_targets, mask_inside_weights\n\ndef mask_decoder(mask_targets, rois, classes, ih, iw, scope='MaskDecoder'):\n  \n  with tf.name_scope(scope) as sc:\n    Mask = \\\n      tf.py_func(mask.decode,\n                 [mask_targets, rois, classes, ih, iw,],\n                 [tf.float32])\n    Mask = tf.convert_to_tensor(Mask, name='MaskImage')\n    Mask = tf.reshape(Mask, (ih, iw))\n  \n  return Mask\n\n\ndef sample_wrapper(boxes, scores, is_training=True, scope='SampleBoxes'):\n  \n  with tf.name_scope(scope) as sc:\n    boxes, scores, batch_inds = \\\n      tf.py_func(sample.sample_rpn_outputs,\n                 [boxes, scores, is_training],\n                 [tf.float32, tf.float32, tf.int32])\n    boxes = tf.convert_to_tensor(boxes, name='Boxes')\n    scores = tf.convert_to_tensor(scores, name='Scores')\n    batch_inds = tf.convert_to_tensor(batch_inds, name='BatchInds')\n    boxes = tf.reshape(boxes, (-1, 4))\n    batch_inds = tf.reshape(batch_inds, [-1])\n  \n  return boxes, scores, batch_inds\n\ndef sample_with_gt_wrapper(boxes, scores, gt_boxes, is_training=True, scope='SampleBoxesWithGT'):\n  \n  with tf.name_scope(scope) as sc:\n    boxes, scores, batch_inds, mask_boxes, mask_scores, mask_batch_inds = \\\n      tf.py_func(sample.sample_rpn_outputs_wrt_gt_boxes,\n                 [boxes, scores, gt_boxes, is_training],\n                 [tf.float32, tf.float32, tf.int32, tf.float32, tf.float32, tf.int32])\n    boxes = tf.convert_to_tensor(boxes, name='Boxes')\n    scores = tf.convert_to_tensor(scores, name='Scores')\n    batch_inds = tf.convert_to_tensor(batch_inds, name='BatchInds')\n    \n    mask_boxes = tf.convert_to_tensor(mask_boxes, name='MaskBoxes')\n    mask_scores = tf.convert_to_tensor(mask_scores, name='MaskScores')\n    mask_batch_inds = tf.convert_to_tensor(mask_batch_inds, name='MaskBatchInds')\n  \n  return boxes, scores, batch_inds, mask_boxes, mask_scores, mask_batch_inds\n\ndef gen_all_anchors(height, width, stride, scales, scope='GenAnchors'):\n  \n  with tf.name_scope(scope) as sc:\n    all_anchors = \\\n      tf.py_func(anchors_plane,\n                 [height, width, stride, scales],\n                 [tf.float64]\n                 )\n    all_anchors = tf.convert_to_tensor(tf.cast(all_anchors, tf.float32), name='AllAnchors')\n    all_anchors = tf.reshape(all_anchors, (height, width, -1))\n    \n    return all_anchors\n\ndef assign_boxes(gt_boxes, tensors, layers, scope='AssignGTBoxes'):\n\n    with tf.name_scope(scope) as sc:\n        min_k = layers[0]\n        max_k = layers[-1]\n        assigned_layers = \\\n            tf.py_func(assign.assign_boxes, \n                     [ gt_boxes, min_k, max_k ],\n                     tf.int32)\n        assigned_layers = tf.reshape(assigned_layers, [-1])\n\n        assigned_tensors = []\n        for t in tensors:\n            split_tensors = []\n            for l in layers:\n                tf.cast(l, tf.int32)\n                inds = tf.where(tf.equal(assigned_layers, l))\n                inds = tf.reshape(inds, [-1])\n                split_tensors.append(tf.gather(t, inds))\n            assigned_tensors.append(split_tensors)\n\n        return assigned_tensors + [assigned_layers]"""
libs/logs/__init__.py,0,b''
libs/logs/log.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport libs.configs.config_v1 as cfg\n\ndef LOG(mssg):\n  logging.basicConfig(filename=cfg.FLAGS.train_dir + '/maskrcnn.log',\n                      level=logging.INFO,\n                      datefmt='%m/%d/%Y %I:%M:%S %p', format='%(asctime)s %(message)s')\n  logging.info(mssg)"""
libs/nets/__init__.py,0,b''
libs/nets/nets_factory.py,1,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport functools\n\nimport tensorflow as tf\n\nfrom . import resnet_v1\nfrom .resnet_v1 import resnet_v1_50 as resnet50\nfrom .resnet_utils import resnet_arg_scope\nfrom .resnet_v1 import resnet_v1_101 as resnet101\n\nslim = tf.contrib.slim\n\npyramid_maps = {\n  'resnet50': {'C1':'resnet_v1_50/conv1/Relu:0',\n               'C2':'resnet_v1_50/block1/unit_2/bottleneck_v1',\n               'C3':'resnet_v1_50/block2/unit_3/bottleneck_v1',\n               'C4':'resnet_v1_50/block3/unit_5/bottleneck_v1',\n               'C5':'resnet_v1_50/block4/unit_3/bottleneck_v1',\n               },\n  'resnet101': {'C1': '', 'C2': '',\n                'C3': '', 'C4': '',\n                'C5': '',\n               }\n}\n\ndef get_network(name, image, weight_decay=0.000005, is_training=False):\n\n    if name == 'resnet50':\n        with slim.arg_scope(resnet_v1.resnet_arg_scope(weight_decay=weight_decay)):\n            logits, end_points = resnet50(image, 1000, is_training=is_training)\n    \n    if name == 'resnet101':\n        with slim.arg_scope(resnet_v1.resnet_arg_scope(weight_decay=weight_decay)):\n            logits, end_points = resnet50(image, 1000, is_training=is_training)\n\n    if name == 'resnext50':\n        name\n\n    end_points['input'] = image\n    return logits, end_points, pyramid_maps[name]\n"""
libs/nets/pyramid_network.py,133,"b'# coding=utf-8\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom libs.boxes.roi import roi_cropping\nfrom libs.layers import anchor_encoder\nfrom libs.layers import anchor_decoder\nfrom libs.layers import roi_encoder\nfrom libs.layers import roi_decoder\nfrom libs.layers import mask_encoder\nfrom libs.layers import mask_decoder\nfrom libs.layers import gen_all_anchors\nfrom libs.layers import ROIAlign\nfrom libs.layers import ROIAlign_\nfrom libs.layers import sample_rpn_outputs\nfrom libs.layers import sample_rpn_outputs_with_gt\nfrom libs.layers import assign_boxes\nfrom libs.visualization.summary_utils import visualize_bb, visualize_final_predictions, visualize_input\n\n_TRAIN_MASK = True\n\n# mapping each stage to its\' tensor features\n_networks_map = {\n  \'resnet50\': {\'C1\':\'resnet_v1_50/conv1/Relu:0\',\n               \'C2\':\'resnet_v1_50/block1/unit_2/bottleneck_v1\',\n               \'C3\':\'resnet_v1_50/block2/unit_3/bottleneck_v1\',\n               \'C4\':\'resnet_v1_50/block3/unit_5/bottleneck_v1\',\n               \'C5\':\'resnet_v1_50/block4/unit_3/bottleneck_v1\',\n               },\n  \'resnet101\': {\'C1\': \'\', \'C2\': \'\',\n                \'C3\': \'\', \'C4\': \'\',\n                \'C5\': \'\',\n               }\n}\n\ndef _extra_conv_arg_scope_with_bn(weight_decay=0.00001,\n                     activation_fn=None,\n                     batch_norm_decay=0.997,\n                     batch_norm_epsilon=1e-5,\n                     batch_norm_scale=True):\n\n  batch_norm_params = {\n      \'decay\': batch_norm_decay,\n      \'epsilon\': batch_norm_epsilon,\n      \'scale\': batch_norm_scale,\n      \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n  }\n\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=slim.variance_scaling_initializer(),\n      activation_fn=tf.nn.relu,\n      normalizer_fn=slim.batch_norm,\n      normalizer_params=batch_norm_params):\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n      with slim.arg_scope([slim.max_pool2d], padding=\'SAME\') as arg_sc:\n        return arg_sc\n\ndef _extra_conv_arg_scope(weight_decay=0.00001, activation_fn=None, normalizer_fn=None):\n\n  with slim.arg_scope(\n      [slim.conv2d, slim.conv2d_transpose],\n      padding=\'SAME\',\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=tf.truncated_normal_initializer(stddev=0.001),\n      activation_fn=activation_fn,\n      normalizer_fn=normalizer_fn,) as arg_sc:\n    with slim.arg_scope(\n      [slim.fully_connected],\n          weights_regularizer=slim.l2_regularizer(weight_decay),\n          weights_initializer=tf.truncated_normal_initializer(stddev=0.001),\n          activation_fn=activation_fn,\n          normalizer_fn=normalizer_fn) as arg_sc:\n          return arg_sc\n\ndef my_sigmoid(x):\n    """"""add an active function for the box output layer, which is linear around 0""""""\n    return (tf.nn.sigmoid(x) - tf.cast(0.5, tf.float32)) * 6.0\n\ndef _smooth_l1_dist(x, y, sigma2=9.0, name=\'smooth_l1_dist\'):\n  """"""Smooth L1 loss\n  Returns\n  ------\n  dist: element-wise distance, as the same shape of x, y\n  """"""\n  deltas = x - y\n  with tf.name_scope(name=name) as scope:\n    deltas_abs = tf.abs(deltas)\n    smoothL1_sign = tf.cast(tf.less(deltas_abs, 1.0 / sigma2), tf.float32)\n    return tf.square(deltas) * 0.5 * sigma2 * smoothL1_sign + \\\n           (deltas_abs - 0.5 / sigma2) * tf.abs(smoothL1_sign - 1)\n\ndef _get_valid_sample_fraction(labels, p=0):\n    """"""return fraction of non-negative examples, the ignored examples have been marked as negative""""""\n    num_valid = tf.reduce_sum(tf.cast(tf.greater_equal(labels, p), tf.float32))\n    num_example = tf.cast(tf.size(labels), tf.float32)\n    frac = tf.cond(tf.greater(num_example, 0), lambda:num_valid / num_example,  \n            lambda: tf.cast(0, tf.float32))\n    frac_ = tf.cond(tf.greater(num_valid, 0), lambda:num_example / num_valid, \n            lambda: tf.cast(0, tf.float32))\n    return frac, frac_\n\n\ndef _filter_negative_samples(labels, tensors):\n    """"""keeps only samples with none-negative labels \n    Params:\n    -----\n    labels: of shape (N,)\n    tensors: a list of tensors, each of shape (N, .., ..) the first axis is sample number\n\n    Returns:\n    -----\n    tensors: filtered tensors\n    """"""\n    # return tensors\n    keeps = tf.where(tf.greater_equal(labels, 0))\n    keeps = tf.reshape(keeps, [-1])\n\n    filtered = []\n    for t in tensors:\n        tf.assert_equal(tf.shape(t)[0], tf.shape(labels)[0])\n        f = tf.gather(t, keeps)\n        filtered.append(f)\n\n    return filtered\n        \ndef _add_jittered_boxes(rois, scores, batch_inds, gt_boxes, jitter=0.1):\n    ws = gt_boxes[:, 2] - gt_boxes[:, 0]\n    hs = gt_boxes[:, 3] - gt_boxes[:, 1]\n    shape = tf.shape(gt_boxes)[0]\n    jitter = tf.random_uniform([shape, 1], minval = -jitter, maxval = jitter)\n    jitter = tf.reshape(jitter, [-1])\n    ws_offset = ws * jitter\n    hs_offset = hs * jitter\n    x1s = gt_boxes[:, 0] + ws_offset\n    x2s = gt_boxes[:, 2] + ws_offset\n    y1s = gt_boxes[:, 1] + hs_offset\n    y2s = gt_boxes[:, 3] + hs_offset\n    boxes = tf.concat(\n            values=[\n                x1s[:, tf.newaxis],\n                y1s[:, tf.newaxis],\n                x2s[:, tf.newaxis],\n                y2s[:, tf.newaxis]],\n            axis=1)\n    new_scores = tf.ones([shape], tf.float32)\n    new_batch_inds = tf.zeros([shape], tf.int32)\n\n    return tf.concat(values=[rois, boxes], axis=0), \\\n           tf.concat(values=[scores, new_scores], axis=0), \\\n           tf.concat(values=[batch_inds, new_batch_inds], axis=0)\n\ndef build_pyramid(net_name, end_points, bilinear=True):\n  """"""build pyramid features from a typical network,\n  assume each stage is 2 time larger than its top feature\n  Returns:\n    returns several endpoints\n  """"""\n  pyramid = {}\n  if isinstance(net_name, str):\n    pyramid_map = _networks_map[net_name]\n  else:\n    pyramid_map = net_name\n  # pyramid[\'inputs\'] = end_points[\'inputs\']\n  #arg_scope = _extra_conv_arg_scope()\n  arg_scope = _extra_conv_arg_scope_with_bn()\n  with tf.variable_scope(\'pyramid\'):\n    with slim.arg_scope(arg_scope):\n      \n      pyramid[\'P5\'] = \\\n        slim.conv2d(end_points[pyramid_map[\'C5\']], 256, [1, 1], stride=1, scope=\'C5\')\n      \n      for c in range(4, 1, -1):\n        s, s_ = pyramid[\'P%d\'%(c+1)], end_points[pyramid_map[\'C%d\' % (c)]]\n\n        # s_ = slim.conv2d(s_, 256, [3, 3], stride=1, scope=\'C%d\'%c)\n        \n        up_shape = tf.shape(s_)\n        # out_shape = tf.stack((up_shape[1], up_shape[2]))\n        # s = slim.conv2d(s, 256, [3, 3], stride=1, scope=\'C%d\'%c)\n        s = tf.image.resize_bilinear(s, [up_shape[1], up_shape[2]], name=\'C%d/upscale\'%c)\n        s_ = slim.conv2d(s_, 256, [1,1], stride=1, scope=\'C%d\'%c)\n        \n        s = tf.add(s, s_, name=\'C%d/addition\'%c)\n        s = slim.conv2d(s, 256, [3,3], stride=1, scope=\'C%d/fusion\'%c)\n        \n        pyramid[\'P%d\'%(c)] = s\n      \n      return pyramid\n  \ndef build_heads(pyramid, ih, iw, num_classes, base_anchors, is_training=False, gt_boxes=None):\n  """"""Build the 3-way outputs, i.e., class, box and mask in the pyramid\n  Algo\n  ----\n  For each layer:\n    1. Build anchor layer\n    2. Process the results of anchor layer, decode the output into rois \n    3. Sample rois \n    4. Build roi layer\n    5. Process the results of roi layer, decode the output into boxes\n    6. Build the mask layer\n    7. Build losses\n  """"""\n  outputs = {}\n  #arg_scope = _extra_conv_arg_scope(activation_fn=None)\n  arg_scope = _extra_conv_arg_scope_with_bn(activation_fn=None)\n  my_sigmoid = None\n  with slim.arg_scope(arg_scope):\n    with tf.variable_scope(\'pyramid\'):\n        # for p in pyramid:\n        outputs[\'rpn\'] = {}\n        for i in range(5, 1, -1):\n          p = \'P%d\'%i\n          stride = 2 ** i\n          \n          ## rpn head\n          shape = tf.shape(pyramid[p])\n          height, width = shape[1], shape[2]\n          rpn = slim.conv2d(pyramid[p], 256, [3, 3], stride=1, activation_fn=tf.nn.relu, scope=\'%s/rpn\'%p)\n          box = slim.conv2d(rpn, base_anchors * 4, [1, 1], stride=1, scope=\'%s/rpn/box\' % p, \\\n                  weights_initializer=tf.truncated_normal_initializer(stddev=0.001), activation_fn=my_sigmoid)\n          cls = slim.conv2d(rpn, base_anchors * 2, [1, 1], stride=1, scope=\'%s/rpn/cls\' % p, \\\n                  weights_initializer=tf.truncated_normal_initializer(stddev=0.01))\n\n          anchor_scales = [2 **(i-2), 2 ** (i-1), 2 **(i)]\n          print(""anchor_scales = "" , anchor_scales)\n          all_anchors = gen_all_anchors(height, width, stride, anchor_scales)\n          outputs[\'rpn\'][p]={\'box\':box, \'cls\':cls, \'anchor\':all_anchors}\n\n        ## gather all rois\n        # print (outputs[\'rpn\'])\n        rpn_boxes = [tf.reshape(outputs[\'rpn\'][\'P%d\'%p][\'box\'], [-1, 4]) for p in range(5, 1, -1)]  \n        rpn_clses = [tf.reshape(outputs[\'rpn\'][\'P%d\'%p][\'cls\'], [-1, 1]) for p in range(5, 1, -1)]  \n        rpn_anchors = [tf.reshape(outputs[\'rpn\'][\'P%d\'%p][\'anchor\'], [-1, 4]) for p in range(5, 1, -1)]  \n        rpn_boxes = tf.concat(values=rpn_boxes, axis=0)\n        rpn_clses = tf.concat(values=rpn_clses, axis=0)\n        rpn_anchors = tf.concat(values=rpn_anchors, axis=0)\n\n        outputs[\'rpn\'][\'box\'] = rpn_boxes\n        outputs[\'rpn\'][\'cls\'] = rpn_clses\n        outputs[\'rpn\'][\'anchor\'] = rpn_anchors\n        # outputs[\'rpn\'] = {\'box\': rpn_boxes, \'cls\': rpn_clses, \'anchor\': rpn_anchors}\n        \n        rpn_probs = tf.nn.softmax(tf.reshape(rpn_clses, [-1, 2]))\n        rois, roi_clses, scores, = anchor_decoder(rpn_boxes, rpn_probs, rpn_anchors, ih, iw)\n        # rois, scores, batch_inds = sample_rpn_outputs(rois, rpn_probs[:, 1])\n        rois, scores, batch_inds, mask_rois, mask_scores, mask_batch_inds = \\\n                sample_rpn_outputs_with_gt(rois, rpn_probs[:, 1], gt_boxes, is_training=is_training)\n\n        # if is_training:\n        #     # rois, scores, batch_inds = _add_jittered_boxes(rois, scores, batch_inds, gt_boxes)\n        #     rois, scores, batch_inds = _add_jittered_boxes(rois, scores, batch_inds, gt_boxes, jitter=0.2)\n        \n        outputs[\'roi\'] = {\'box\': rois, \'score\': scores}\n\n        ## cropping regions\n        [assigned_rois, assigned_batch_inds, assigned_layer_inds] = \\\n                assign_boxes(rois, [rois, batch_inds], [2, 3, 4, 5])\n\n        outputs[\'assigned_rois\'] = assigned_rois\n        outputs[\'assigned_layer_inds\'] = assigned_layer_inds\n\n        cropped_rois = []\n        ordered_rois = []\n        pyramid_feature = []\n        for i in range(5, 1, -1):\n            print(i)\n            p = \'P%d\'%i\n            splitted_rois = assigned_rois[i-2]\n            batch_inds = assigned_batch_inds[i-2]\n            cropped, boxes_in_crop = ROIAlign_(pyramid[p], splitted_rois, batch_inds, ih, iw, stride=2**i,\n                               pooled_height=14, pooled_width=14)\n            # cropped = ROIAlign(pyramid[p], splitted_rois, batch_inds, stride=2**i,\n            #                    pooled_height=14, pooled_width=14)\n            cropped_rois.append(cropped)\n            ordered_rois.append(splitted_rois)\n            pyramid_feature.append(tf.transpose(pyramid[p],[0,3,1,2]))\n            # if i is 5:\n            #     outputs[\'tmp_0\'] = tf.transpose(pyramid[p],[0,3,1,2])\n            #     outputs[\'tmp_1\'] = splitted_rois\n            #     outputs[\'tmp_2\'] = tf.transpose(cropped,[0,3,1,2])\n            #     outputs[\'tmp_3\'] = boxes_in_crop\n            #     outputs[\'tmp_4\'] = [ih, iw]\n            \n        cropped_rois = tf.concat(values=cropped_rois, axis=0)\n        ordered_rois = tf.concat(values=ordered_rois, axis=0)\n\n\n        outputs[\'ordered_rois\'] = ordered_rois\n        outputs[\'pyramid_feature\'] = pyramid_feature\n\n        outputs[\'roi\'][\'cropped_rois\'] = cropped_rois\n        tf.add_to_collection(\'__CROPPED__\', cropped_rois)\n\n        ## refine head\n        # to 7 x 7\n        cropped_regions = slim.max_pool2d(cropped_rois, [3, 3], stride=2, padding=\'SAME\')\n        refine = slim.flatten(cropped_regions)\n        refine = slim.fully_connected(refine, 1024, activation_fn=tf.nn.relu)\n        refine = slim.dropout(refine, keep_prob=0.75, is_training=is_training)\n        refine = slim.fully_connected(refine,  1024, activation_fn=tf.nn.relu)\n        refine = slim.dropout(refine, keep_prob=0.75, is_training=is_training)\n        cls2 = slim.fully_connected(refine, num_classes, activation_fn=None, \n                weights_initializer=tf.truncated_normal_initializer(stddev=0.05))\n        box = slim.fully_connected(refine, num_classes*4, activation_fn=my_sigmoid, \n                weights_initializer=tf.truncated_normal_initializer(stddev=0.05))\n\n        outputs[\'refined\'] = {\'box\': box, \'cls\': cls2}\n        \n        ## decode refine net outputs\n        cls2_prob = tf.nn.softmax(cls2)\n        final_boxes, classes, scores = \\\n                roi_decoder(box, cls2_prob, ordered_rois, ih, iw)\n\n        #outputs[\'tmp_0\'] = ordered_rois\n        #outputs[\'tmp_1\'] = assigned_rois\n        #outputs[\'tmp_2\'] = box\n        #outputs[\'tmp_3\'] = final_boxes\n        #outputs[\'tmp_4\'] = cls2_prob\n\n        #outputs[\'final_boxes\'] = {\'box\': final_boxes, \'cls\': classes}\n        outputs[\'final_boxes\'] = {\'box\': final_boxes, \'cls\': classes, \'prob\': cls2_prob}\n        ## for testing, maskrcnn takes refined boxes as inputs\n        if not is_training:\n          rois = final_boxes\n          # [assigned_rois, assigned_batch_inds, assigned_layer_inds] = \\\n          #       assign_boxes(rois, [rois, batch_inds], [2, 3, 4, 5])\n          for i in range(5, 1, -1):\n            p = \'P%d\'%i\n            splitted_rois = assigned_rois[i-2]\n            batch_inds = assigned_batch_inds[i-2]\n            cropped = ROIAlign(pyramid[p], splitted_rois, batch_inds, stride=2**i,\n                               pooled_height=14, pooled_width=14)\n            cropped_rois.append(cropped)\n            ordered_rois.append(splitted_rois)\n          cropped_rois = tf.concat(values=cropped_rois, axis=0)\n          ordered_rois = tf.concat(values=ordered_rois, axis=0)\n          \n        ## mask head\n        m = cropped_rois\n        for _ in range(4):\n            m = slim.conv2d(m, 256, [3, 3], stride=1, padding=\'SAME\', activation_fn=tf.nn.relu)\n        # to 28 x 28\n        m = slim.conv2d_transpose(m, 256, 2, stride=2, padding=\'VALID\', activation_fn=tf.nn.relu)\n        tf.add_to_collection(\'__TRANSPOSED__\', m)\n        m = slim.conv2d(m, num_classes, [1, 1], stride=1, padding=\'VALID\', activation_fn=None)\n          \n        # add a mask, given the predicted boxes and classes\n        outputs[\'mask\'] = {\'mask\':m, \'cls\': classes, \'score\': scores}\n          \n  return outputs\n\ndef build_losses(pyramid, outputs, gt_boxes, gt_masks,\n                 num_classes, base_anchors,\n                 rpn_box_lw =1.0, rpn_cls_lw = 1.0,\n                 refined_box_lw=1.0, refined_cls_lw=1.0,\n                 mask_lw=1.0):\n  """"""Building 3-way output losses, totally 5 losses\n  Params:\n  ------\n  outputs: output of build_heads\n  gt_boxes: A tensor of shape (G, 5), [x1, y1, x2, y2, class]\n  gt_masks: A tensor of shape (G, ih, iw),  {0, 1}\xc3\x8c[Ma\xc3\x8c[Ma\xc3\x8c]]\n  *_lw: loss weight of rpn, refined and mask losses\n  \n  Returns:\n  -------\n  l: a loss tensor\n  """"""\n\n  # losses for pyramid\n  losses = []\n  rpn_box_losses, rpn_cls_losses = [], []\n  refined_box_losses, refined_cls_losses = [], []\n  mask_losses = []\n  \n  # watch some info during training\n  rpn_batch = []\n  refine_batch = []\n  mask_batch = []\n  rpn_batch_pos = []\n  refine_batch_pos = []\n  mask_batch_pos = []\n\n  #arg_scope = _extra_conv_arg_scope(activation_fn=None)\n  arg_scope = _extra_conv_arg_scope_with_bn(activation_fn=None)\n  with slim.arg_scope(arg_scope):\n      with tf.variable_scope(\'pyramid\'):\n\n        ## assigning gt_boxes\n        [assigned_gt_boxes, assigned_layer_inds] = assign_boxes(gt_boxes, [gt_boxes], [2, 3, 4, 5])\n\n        ## build losses for PFN\n\n        for i in range(5, 1, -1):\n            p = \'P%d\' % i\n            stride = 2 ** i\n            shape = tf.shape(pyramid[p])\n            height, width = shape[1], shape[2]\n\n            splitted_gt_boxes = assigned_gt_boxes[i-2]\n            \n            ### rpn losses\n            # 1. encode ground truth\n            # 2. compute distances\n            # anchor_scales = [2 **(i-2), 2 ** (i-1), 2 **(i)]\n            # all_anchors = gen_all_anchors(height, width, stride, anchor_scales)\n            all_anchors = outputs[\'rpn\'][p][\'anchor\']\n            labels, bbox_targets, bbox_inside_weights = \\\n              anchor_encoder(splitted_gt_boxes, all_anchors, height, width, stride, scope=\'AnchorEncoder\')\n            boxes = outputs[\'rpn\'][p][\'box\']\n            classes = tf.reshape(outputs[\'rpn\'][p][\'cls\'], (1, height, width, base_anchors, 2))\n\n            labels, classes, boxes, bbox_targets, bbox_inside_weights = \\\n                    _filter_negative_samples(tf.reshape(labels, [-1]), [\n                        tf.reshape(labels, [-1]),\n                        tf.reshape(classes, [-1, 2]),\n                        tf.reshape(boxes, [-1, 4]),\n                        tf.reshape(bbox_targets, [-1, 4]),\n                        tf.reshape(bbox_inside_weights, [-1, 4])\n                        ])\n            # _, frac_ = _get_valid_sample_fraction(labels)\n            rpn_batch.append(\n                    tf.reduce_sum(tf.cast(\n                        tf.greater_equal(labels, 0), tf.float32\n                        )))\n            rpn_batch_pos.append(\n                    tf.reduce_sum(tf.cast(\n                        tf.greater_equal(labels, 1), tf.float32\n                        )))\n            rpn_box_loss = bbox_inside_weights * _smooth_l1_dist(boxes, bbox_targets)\n            rpn_box_loss = tf.reshape(rpn_box_loss, [-1, 4])\n            rpn_box_loss = tf.reduce_sum(rpn_box_loss, axis=1)\n            rpn_box_loss = rpn_box_lw * tf.reduce_mean(rpn_box_loss) \n            tf.add_to_collection(tf.GraphKeys.LOSSES, rpn_box_loss)\n            rpn_box_losses.append(rpn_box_loss)\n\n            # NOTE: examples with negative labels are ignore when compute one_hot_encoding and entropy losses \n            # BUT these examples still count when computing the average of softmax_cross_entropy, \n            # the loss become smaller by a factor (None_negtive_labels / all_labels)\n            # the BEST practise still should be gathering all none-negative examples\n            labels = slim.one_hot_encoding(labels, 2, on_value=1.0, off_value=0.0) # this will set -1 label to all zeros\n            rpn_cls_loss = rpn_cls_lw * tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=classes) \n            rpn_cls_loss = tf.reduce_mean(rpn_cls_loss) \n            tf.add_to_collection(tf.GraphKeys.LOSSES, rpn_cls_loss)\n            rpn_cls_losses.append(rpn_cls_loss)\n            \n\n        ### refined loss\n        # 1. encode ground truth\n        # 2. compute distances\n        ordered_rois = outputs[\'ordered_rois\']\n        #rois = outputs[\'roi\'][\'box\']\n        \n        boxes = outputs[\'refined\'][\'box\']\n        classes = outputs[\'refined\'][\'cls\']\n\n        labels, bbox_targets, bbox_inside_weights = \\\n          roi_encoder(gt_boxes, ordered_rois, num_classes, scope=\'ROIEncoder\')\n\n        outputs[\'final_boxes\'][\'gt_cls\'] = slim.one_hot_encoding(labels, num_classes, on_value=1.0, off_value=0.0)\n        outputs[\'gt\'] = gt_boxes\n        labels, classes, boxes, bbox_targets, bbox_inside_weights = \\\n                _filter_negative_samples(tf.reshape(labels, [-1]),[\n                    tf.reshape(labels, [-1]),\n                    tf.reshape(classes, [-1, num_classes]),\n                    tf.reshape(boxes, [-1, num_classes * 4]),\n                    tf.reshape(bbox_targets, [-1, num_classes * 4]),\n                    tf.reshape(bbox_inside_weights, [-1, num_classes * 4])\n                    ] )\n        # frac, frac_ = _get_valid_sample_fraction(labels, 1)\n        refine_batch.append(\n                tf.reduce_sum(tf.cast(\n                    tf.greater_equal(labels, 0), tf.float32\n                    )))\n        refine_batch_pos.append(\n                tf.reduce_sum(tf.cast(\n                    tf.greater_equal(labels, 1), tf.float32\n                    )))\n\n        refined_box_loss = bbox_inside_weights * _smooth_l1_dist(boxes, bbox_targets)\n        refined_box_loss = tf.reshape(refined_box_loss, [-1, 4])\n        refined_box_loss = tf.reduce_sum(refined_box_loss, axis=1)\n        refined_box_loss = refined_box_lw * tf.reduce_mean(refined_box_loss) # * frac_\n        tf.add_to_collection(tf.GraphKeys.LOSSES, refined_box_loss)\n        refined_box_losses.append(refined_box_loss)\n\n        labels = slim.one_hot_encoding(labels, num_classes, on_value=1.0, off_value=0.0)\n        refined_cls_loss = refined_cls_lw * tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=classes) \n        refined_cls_loss = tf.reduce_mean(refined_cls_loss) # * frac_\n        tf.add_to_collection(tf.GraphKeys.LOSSES, refined_cls_loss)\n        refined_cls_losses.append(refined_cls_loss)\n\n        outputs[\'tmp_3\'] = labels\n        outputs[\'tmp_4\'] = classes\n\n        # outputs[\'tmp_0\'] = outputs[\'ordered_rois\']\n        # outputs[\'tmp_1\'] = outputs[\'pyramid_feature\']\n        # outputs[\'tmp_2\'] = tf.transpose(outputs[\'roi\'][\'cropped_rois\'],[0,3,1,2])\n        # outputs[\'tmp_3\'] = outputs[\'assigned_rois\']\n        \n\n        ### mask loss\n        # mask of shape (N, h, w, num_classes)\n        masks = outputs[\'mask\'][\'mask\']\n        # mask_shape = tf.shape(masks)\n        # masks = tf.reshape(masks, (mask_shape[0], mask_shape[1],\n        #                            mask_shape[2], tf.cast(mask_shape[3]/2, tf.int32), 2))\n        labels, mask_targets, mask_inside_weights = \\\n          mask_encoder(gt_masks, gt_boxes, ordered_rois, num_classes, 28, 28, scope=\'MaskEncoder\')\n        labels, masks, mask_targets, mask_inside_weights = \\\n                _filter_negative_samples(tf.reshape(labels, [-1]), [\n                    tf.reshape(labels, [-1]),\n                    masks,\n                    mask_targets, \n                    mask_inside_weights, \n                    ])\n        # _, frac_ = _get_valid_sample_fraction(labels)\n        mask_batch.append(\n                tf.reduce_sum(tf.cast(\n                    tf.greater_equal(labels, 0), tf.float32\n                    )))\n        mask_batch_pos.append(\n                tf.reduce_sum(tf.cast(\n                    tf.greater_equal(labels, 1), tf.float32\n                    )))\n        # mask_targets = slim.one_hot_encoding(mask_targets, 2, on_value=1.0, off_value=0.0)\n        # mask_binary_loss = mask_lw * tf.losses.softmax_cross_entropy(mask_targets, masks)\n        # NOTE: w/o competition between classes. \n        mask_targets = tf.cast(mask_targets, tf.float32)\n        mask_loss = mask_lw * tf.nn.sigmoid_cross_entropy_with_logits(labels=mask_targets, logits=masks) \n        mask_loss = tf.reduce_mean(mask_loss) \n        mask_loss = tf.cond(tf.greater(tf.size(labels), 0), lambda: mask_loss, lambda: tf.constant(0.0))\n        tf.add_to_collection(tf.GraphKeys.LOSSES, mask_loss)\n        mask_losses.append(mask_loss)\n\n  rpn_box_losses = tf.add_n(rpn_box_losses)\n  rpn_cls_losses = tf.add_n(rpn_cls_losses)\n  refined_box_losses = tf.add_n(refined_box_losses)\n  refined_cls_losses = tf.add_n(refined_cls_losses)\n  mask_losses = tf.add_n(mask_losses)\n  losses = [rpn_box_losses, rpn_cls_losses, refined_box_losses, refined_cls_losses, mask_losses]\n  total_loss = tf.add_n(losses)\n\n  rpn_batch = tf.cast(tf.add_n(rpn_batch), tf.float32)\n  refine_batch = tf.cast(tf.add_n(refine_batch), tf.float32)\n  mask_batch = tf.cast(tf.add_n(mask_batch), tf.float32)\n  rpn_batch_pos = tf.cast(tf.add_n(rpn_batch_pos), tf.float32)\n  refine_batch_pos = tf.cast(tf.add_n(refine_batch_pos), tf.float32)\n  mask_batch_pos = tf.cast(tf.add_n(mask_batch_pos), tf.float32)\n    \n  return total_loss, losses, [rpn_batch_pos, rpn_batch, \\\n                              refine_batch_pos, refine_batch, \\\n                              mask_batch_pos, mask_batch]\n\ndef decode_output(outputs):\n    """"""decode outputs into boxes and masks""""""\n    return [], [], []\n\ndef build(end_points, image_height, image_width, pyramid_map, \n        num_classes,\n        base_anchors,\n        is_training,\n        gt_boxes,\n        gt_masks, \n        loss_weights=[0.5, 0.5, 1.0, 0.5, 0.1]):\n    \n    pyramid = build_pyramid(pyramid_map, end_points)\n\n    for p in pyramid:\n        print (p)\n\n    outputs = \\\n        build_heads(pyramid, image_height, image_width, num_classes, base_anchors, \n                    is_training=is_training, gt_boxes=gt_boxes)\n\n    if is_training:\n        loss, losses, batch_info = build_losses(pyramid, outputs, \n                        gt_boxes, gt_masks,\n                        num_classes=num_classes, base_anchors=base_anchors,\n                        rpn_box_lw=loss_weights[0], rpn_cls_lw=loss_weights[1],\n                        refined_box_lw=loss_weights[2], refined_cls_lw=loss_weights[3],\n                        mask_lw=loss_weights[4])\n\n        outputs[\'losses\'] = losses\n        outputs[\'total_loss\'] = loss\n        outputs[\'batch_info\'] = batch_info\n\n    ## just decode outputs into readable prediction\n    pred_boxes, pred_classes, pred_masks = decode_output(outputs)\n    outputs[\'pred_boxes\'] = pred_boxes\n    outputs[\'pred_classes\'] = pred_classes\n    outputs[\'pred_masks\'] = pred_masks\n\n    # image and gt visualization\n    visualize_input(gt_boxes, end_points[""input""], tf.expand_dims(gt_masks, axis=3))\n\n    # rpn visualization\n    visualize_bb(end_points[""input""], outputs[\'roi\'][""box""], name=""rpn_bb_visualization"")\n\n    # final network visualization\n    first_mask = outputs[\'mask\'][\'mask\'][:1]\n    first_mask = tf.transpose(first_mask, [3, 1, 2, 0])\n\n    visualize_final_predictions(outputs[\'final_boxes\'][""box""], end_points[""input""], first_mask)\n\n    return outputs\n'"
libs/nets/resnet_utils.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains building blocks for various versions of Residual Networks.\n\nResidual networks (ResNets) were proposed in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Deep Residual Learning for Image Recognition. arXiv:1512.03385, 2015\n\nMore variants were introduced in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Identity Mappings in Deep Residual Networks. arXiv: 1603.05027, 2016\n\nWe can obtain different ResNet variants by changing the network depth, width,\nand form of residual unit. This module implements the infrastructure for\nbuilding them. Concrete ResNet units and full ResNet networks are implemented in\nthe accompanying resnet_v1.py and resnet_v2.py modules.\n\nCompared to https://github.com/KaimingHe/deep-residual-networks, in the current\nimplementation we subsample the output activations in the last residual unit of\neach block, instead of subsampling the input activations in the first residual\nunit of each block. The two implementations give identical results but our\nimplementation is more memory efficient.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport tensorflow as tf\n\n# slim = tf.contrib.slim\nimport tensorflow.contrib.slim as slim\n\n\nclass Block(collections.namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\n  """"""A named tuple describing a ResNet block.\n\n  Its parts are:\n    scope: The scope of the `Block`.\n    unit_fn: The ResNet unit function which takes as input a `Tensor` and\n      returns another `Tensor` with the output of the ResNet unit.\n    args: A list of length equal to the number of units in the `Block`. The list\n      contains one (depth, depth_bottleneck, stride) tuple for each unit in the\n      block to serve as argument to unit_fn.\n  """"""\n\n\ndef subsample(inputs, factor, scope=None):\n  """"""Subsamples the input along the spatial dimensions.\n\n  Args:\n    inputs: A `Tensor` of size [batch, height_in, width_in, channels].\n    factor: The subsampling factor.\n    scope: Optional variable_scope.\n\n  Returns:\n    output: A `Tensor` of size [batch, height_out, width_out, channels] with the\n      input, either intact (if factor == 1) or subsampled (if factor > 1).\n  """"""\n  if factor == 1:\n    return inputs\n  else:\n    return slim.max_pool2d(inputs, [1, 1], stride=factor, scope=scope)\n\n\ndef conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):\n  """"""Strided 2-D convolution with \'SAME\' padding.\n\n  When stride > 1, then we do explicit zero-padding, followed by conv2d with\n  \'VALID\' padding.\n\n  Note that\n\n     net = conv2d_same(inputs, num_outputs, 3, stride=stride)\n\n  is equivalent to\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=1, padding=\'SAME\')\n     net = subsample(net, factor=stride)\n\n  whereas\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=stride, padding=\'SAME\')\n\n  is different when the input\'s height or width is even, which is why we add the\n  current function. For more details, see ResnetUtilsTest.testConv2DSameEven().\n\n  Args:\n    inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\n    num_outputs: An integer, the number of output filters.\n    kernel_size: An int with the kernel_size of the filters.\n    stride: An integer, the output stride.\n    rate: An integer, rate for atrous convolution.\n    scope: Scope.\n\n  Returns:\n    output: A 4-D tensor of size [batch, height_out, width_out, channels] with\n      the convolution output.\n  """"""\n  if stride == 1:\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=1, rate=rate,\n                       padding=\'SAME\', scope=scope)\n  else:\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    inputs = tf.pad(inputs,\n                    [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=stride,\n                       rate=rate, padding=\'VALID\', scope=scope)\n\n\n@slim.add_arg_scope\ndef stack_blocks_dense(net, blocks, output_stride=None,\n                       outputs_collections=None):\n  """"""Stacks ResNet `Blocks` and controls output feature density.\n\n  First, this function creates scopes for the ResNet in the form of\n  \'block_name/unit_1\', \'block_name/unit_2\', etc.\n\n  Second, this function allows the user to explicitly control the ResNet\n  output_stride, which is the ratio of the input to output spatial resolution.\n  This is useful for dense prediction tasks such as semantic segmentation or\n  object detection.\n\n  Most ResNets consist of 4 ResNet blocks and subsample the activations by a\n  factor of 2 when transitioning between consecutive ResNet blocks. This results\n  to a nominal ResNet output_stride equal to 8. If we set the output_stride to\n  half the nominal network stride (e.g., output_stride=4), then we compute\n  responses twice.\n\n  Control of the output feature density is implemented by atrous convolution.\n\n  Args:\n    net: A `Tensor` of size [batch, height, width, channels].\n    blocks: A list of length equal to the number of ResNet `Blocks`. Each\n      element is a ResNet `Block` object describing the units in the `Block`.\n    output_stride: If `None`, then the output will be computed at the nominal\n      network stride. If output_stride is not `None`, it specifies the requested\n      ratio of input to output spatial resolution, which needs to be equal to\n      the product of unit strides from the start up to some level of the ResNet.\n      For example, if the ResNet employs units with strides 1, 2, 1, 3, 4, 1,\n      then valid values for the output_stride are 1, 2, 6, 24 or None (which\n      is equivalent to output_stride=24).\n    outputs_collections: Collection to add the ResNet block outputs.\n\n  Returns:\n    net: Output tensor with stride equal to the specified output_stride.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  # The current_stride variable keeps track of the effective stride of the\n  # activations. This allows us to invoke atrous convolution whenever applying\n  # the next residual unit would result in the activations having stride larger\n  # than the target output_stride.\n  current_stride = 1\n\n  # The atrous convolution rate parameter.\n  rate = 1\n\n  for block in blocks:\n    with tf.variable_scope(block.scope, \'block\', [net]) as sc:\n      for i, unit in enumerate(block.args):\n        if output_stride is not None and current_stride > output_stride:\n          raise ValueError(\'The target output_stride cannot be reached.\')\n\n        with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n          unit_depth, unit_depth_bottleneck, unit_stride = unit\n\n          # If we have reached the target output_stride, then we need to employ\n          # atrous convolution with stride=1 and multiply the atrous rate by the\n          # current unit\'s stride for use in subsequent layers.\n          if output_stride is not None and current_stride == output_stride:\n            net = block.unit_fn(net,\n                                depth=unit_depth,\n                                depth_bottleneck=unit_depth_bottleneck,\n                                stride=1,\n                                rate=rate)\n            rate *= unit_stride\n\n          else:\n            net = block.unit_fn(net,\n                                depth=unit_depth,\n                                depth_bottleneck=unit_depth_bottleneck,\n                                stride=unit_stride,\n                                rate=1)\n            current_stride *= unit_stride\n      net = slim.utils.collect_named_outputs(outputs_collections, sc.name, net)\n\n  if output_stride is not None and current_stride != output_stride:\n    raise ValueError(\'The target output_stride cannot be reached.\')\n\n  return net\n\n\ndef resnet_arg_scope(weight_decay=0.0001,\n                     batch_norm_decay=0.997,\n                     batch_norm_epsilon=1e-5,\n                     batch_norm_scale=True):\n  """"""Defines the default ResNet arg scope.\n\n  TODO(gpapan): The batch-normalization related default values above are\n    appropriate for use in conjunction with the reference ResNet models\n    released at https://github.com/KaimingHe/deep-residual-networks. When\n    training ResNets from scratch, they might need to be tuned.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    batch_norm_decay: The moving average decay when estimating layer activation\n      statistics in batch normalization.\n    batch_norm_epsilon: Small constant to prevent division by zero when\n      normalizing activations by their variance in batch normalization.\n    batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\n      activations in the batch normalization layer.\n\n  Returns:\n    An `arg_scope` to use for the resnet models.\n  """"""\n  batch_norm_params = {\n      \'decay\': batch_norm_decay,\n      \'epsilon\': batch_norm_epsilon,\n      \'scale\': batch_norm_scale,\n      \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n  }\n\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=slim.variance_scaling_initializer(),\n      activation_fn=tf.nn.relu,\n      normalizer_fn=slim.batch_norm,\n      normalizer_params=batch_norm_params):\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n      # The following implies padding=\'SAME\' for pool1, which makes feature\n      # alignment easier for dense prediction tasks. This is also used in\n      # https://github.com/facebook/fb.resnet.torch. However the accompanying\n      # code of \'Deep Residual Learning for Image Recognition\' uses\n      # padding=\'VALID\' for pool1. You can switch to that choice by setting\n      # slim.arg_scope([slim.max_pool2d], padding=\'VALID\').\n      with slim.arg_scope([slim.max_pool2d], padding=\'SAME\') as arg_sc:\n        return arg_sc\n'"
libs/nets/resnet_v1.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the original form of Residual Networks.\n\nThe \'v1\' residual networks (ResNets) implemented in this module were proposed\nby:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nOther variants were introduced in:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe networks defined in this module utilize the bottleneck building block of\n[1] with projection shortcuts only for increasing depths. They employ batch\nnormalization *after* every weight layer. This is the architecture used by\nMSRA in the Imagenet and MSCOCO 2016 competition models ResNet-101 and\nResNet-152. See [2; Fig. 1a] for a comparison between the current \'v1\'\narchitecture and the alternative \'v2\' architecture of [2] which uses batch\nnormalization *before* every weight layer in the so-called full pre-activation\nunits.\n\nTypical use:\n\n   from tensorflow.contrib.slim.nets import resnet_v1\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom libs.nets import resnet_utils\n\n\nresnet_arg_scope = resnet_utils.resnet_arg_scope\nslim = tf.contrib.slim\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n  """"""Bottleneck residual unit variant with BN after convolutions.\n\n  This is the original residual unit proposed in [1]. See Fig. 1(a) of [2] for\n  its definition. Note that we use here the bottleneck variant which has an\n  extra bottleneck layer.\n\n  When putting together two consecutive ResNet blocks that use this unit, one\n  should use stride = 2 in the last unit of the first block.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    depth: The depth of the ResNet unit output.\n    depth_bottleneck: The depth of the bottleneck layers.\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n      the units output compared to its input.\n    rate: An integer, rate for atrous convolution.\n    outputs_collections: Collection to add the ResNet unit output.\n    scope: Optional variable_scope.\n\n  Returns:\n    The ResNet unit\'s output.\n  """"""\n  with tf.variable_scope(scope, \'bottleneck_v1\', [inputs]) as sc:\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(inputs, depth, [1, 1], stride=stride,\n                             activation_fn=None, scope=\'shortcut\')\n\n    residual = slim.conv2d(inputs, depth_bottleneck, [1, 1], stride=1,\n                           scope=\'conv1\')\n    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                        rate=rate, scope=\'conv2\')\n    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                           activation_fn=None, scope=\'conv3\')\n\n    output = tf.nn.relu(shortcut + residual)\n\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope,\n                                            output)\n\n\ndef resnet_v1(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=True,\n              reuse=None,\n              scope=None):\n  """"""Generator for v1 ResNet models.\n\n  This function generates a family of ResNet v1 models. See the resnet_v1_*()\n  methods for specific model instantiations, obtained by selecting different\n  block instantiations that produce ResNets of various depths.\n\n  Training for image classification on Imagenet is usually done with [224, 224]\n  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n  block for the ResNets defined in [1] that have nominal stride equal to 32.\n  However, for dense prediction tasks we advise that one uses inputs with\n  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n  this case the feature maps at the ResNet output will have spatial shape\n  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n  and corners exactly aligned with the input image corners, which greatly\n  facilitates alignment of the features to the image. Using as input [225, 225]\n  images results in [8, 8] feature maps at the output of the last ResNet block.\n\n  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n  have nominal stride equal to 32 and a good choice in FCN mode is to use\n  output_stride=16 in order to increase the density of the computed features at\n  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    blocks: A list of length equal to the number of ResNet blocks. Each element\n      is a resnet_utils.Block object describing the units in the block.\n    num_classes: Number of predicted classes for classification tasks. If None\n      we return the features before the logit layer.\n    is_training: whether is training or not.\n    global_pool: If True, we perform global average pooling before computing the\n      logits. Set to True for image classification, False for dense prediction.\n    output_stride: If None, then the output will be computed at the nominal\n      network stride. If output_stride is not None, it specifies the requested\n      ratio of input to output spatial resolution.\n    include_root_block: If True, include the initial convolution followed by\n      max-pooling, if False excludes it.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n      If global_pool is False, then height_out and width_out are reduced by a\n      factor of output_stride compared to the respective height_in and width_in,\n      else both height_out and width_out equal one. If num_classes is None, then\n      net is the output of the last ResNet block, potentially after global\n      average pooling. If num_classes is not None, net contains the pre-softmax\n      activations.\n    end_points: A dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  with tf.variable_scope(scope, \'resnet_v1\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, bottleneck,\n                         resnet_utils.stack_blocks_dense],\n                        outputs_collections=end_points_collection):\n      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n        if num_classes is not None:\n          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                            normalizer_fn=None, scope=\'logits\')\n        if spatial_squeeze:\n          logits = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n        if num_classes is not None:\n          end_points[\'predictions\'] = slim.softmax(logits, scope=\'predictions\')\n        return logits, end_points\nresnet_v1.default_image_size = 224\n\n\ndef resnet_v1_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 reuse=None,\n                 scope=\'resnet_v1_50\'):\n  """"""ResNet-50 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 5 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\nresnet_v1_50.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v1_101\'):\n  """"""ResNet-101 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 22 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\nresnet_v1_101.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v1_152\'):\n  """"""ResNet-152 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 7 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\nresnet_v1_152.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v1_200\'):\n  """"""ResNet-200 model of [2]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 23 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\nresnet_v1_200.default_image_size = resnet_v1.default_image_size\n'"
libs/nets/train_utils.py,21,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport tensorflow as tf\nimport libs.configs.config_v1 as cfg\n\nslim = tf.contrib.slim\nFLAGS = tf.app.flags.FLAGS\n\ndef _configure_optimizer(learning_rate):\n  """"""Configures the optimizer used for training.\n\n  Args:\n    learning_rate: A scalar or `Tensor` learning rate.\n\n  Returns:\n    An instance of an optimizer.\n\n  Raises:\n    ValueError: if FLAGS.optimizer is not recognized.\n  """"""\n  if FLAGS.optimizer == \'adadelta\':\n    optimizer = tf.train.AdadeltaOptimizer(\n        learning_rate,\n        rho=FLAGS.adadelta_rho,\n        epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == \'adagrad\':\n    optimizer = tf.train.AdagradOptimizer(\n        learning_rate,\n        initial_accumulator_value=FLAGS.adagrad_initial_accumulator_value)\n  elif FLAGS.optimizer == \'adam\':\n    optimizer = tf.train.AdamOptimizer(\n        learning_rate,\n        beta1=FLAGS.adam_beta1,\n        beta2=FLAGS.adam_beta2,\n        epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == \'ftrl\':\n    optimizer = tf.train.FtrlOptimizer(\n        learning_rate,\n        learning_rate_power=FLAGS.ftrl_learning_rate_power,\n        initial_accumulator_value=FLAGS.ftrl_initial_accumulator_value,\n        l1_regularization_strength=FLAGS.ftrl_l1,\n        l2_regularization_strength=FLAGS.ftrl_l2)\n  elif FLAGS.optimizer == \'momentum\':\n    optimizer = tf.train.MomentumOptimizer(\n        learning_rate,\n        momentum=FLAGS.momentum,\n        name=\'Momentum\')\n  elif FLAGS.optimizer == \'rmsprop\':\n    optimizer = tf.train.RMSPropOptimizer(\n        learning_rate,\n        decay=FLAGS.rmsprop_decay,\n        momentum=FLAGS.rmsprop_momentum,\n        epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == \'sgd\':\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n  else:\n    raise ValueError(\'Optimizer [%s] was not recognized\', FLAGS.optimizer)\n  return optimizer\n\ndef _configure_learning_rate(num_samples_per_epoch, global_step):\n  """"""Configures the learning rate.\n\n  Args:\n    num_samples_per_epoch: The number of samples in each epoch of training.\n    global_step: The global_step tensor.\n\n  Returns:\n    A `Tensor` representing the learning rate.\n\n  Raises:\n    ValueError: if\n  """"""\n  decay_steps = int(num_samples_per_epoch / FLAGS.batch_size *\n                    FLAGS.num_epochs_per_decay)\n  if FLAGS.sync_replicas:\n    decay_steps /= FLAGS.replicas_to_aggregate\n\n  if FLAGS.learning_rate_decay_type == \'exponential\':\n    return tf.train.exponential_decay(FLAGS.learning_rate,\n                                      global_step,\n                                      decay_steps,\n                                      FLAGS.learning_rate_decay_factor,\n                                      staircase=True,\n                                      name=\'exponential_decay_learning_rate\')\n  elif FLAGS.learning_rate_decay_type == \'fixed\':\n    return tf.constant(FLAGS.learning_rate, name=\'fixed_learning_rate\')\n  elif FLAGS.learning_rate_decay_type == \'polynomial\':\n    return tf.train.polynomial_decay(FLAGS.learning_rate,\n                                     global_step,\n                                     decay_steps,\n                                     FLAGS.end_learning_rate,\n                                     power=0.9,\n                                     cycle=False,\n                                     name=\'polynomial_decay_learning_rate\')\n  else:\n    raise ValueError(\'learning_rate_decay_type [%s] was not recognized\',\n                     FLAGS.learning_rate_decay_type)\n  \ndef _get_variables_to_train():\n  """"""Returns a list of variables to train.\n\n  Returns:\n    A list of variables to train by the optimizer.\n  """"""\n  if FLAGS.trainable_scopes is None:\n    return tf.trainable_variables()\n  else:\n    scopes = [scope.strip() for scope in FLAGS.trainable_scopes.split(\',\')]\n\n  variables_to_train = []\n  for scope in scopes:\n    variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n    variables_to_train.extend(variables)\n  return variables_to_train\n\ndef _get_init_fn():\n  """"""Returns a function run by the chief worker to warm-start the training.\n\n  Note that the init_fn is only run when initializing the model during the very\n  first global step.\n\n  Returns:\n    An init function run by the supervisor.\n  """"""\n  if FLAGS.checkpoint_path is None:\n    return None\n\n  # Warn the user if a checkpoint exists in the train_dir. Then we\'ll be\n  # ignoring the checkpoint anyway.\n  if tf.train.latest_checkpoint(FLAGS.train_dir):\n    tf.logging.info(\n        \'Ignoring --checkpoint_path because a checkpoint already exists in %s\'\n        % FLAGS.train_dir)\n    return None\n\n  exclusions = []\n  if FLAGS.checkpoint_exclude_scopes:\n    exclusions = [scope.strip()\n                  for scope in FLAGS.checkpoint_exclude_scopes.split(\',\')]\n\n  # TODO(sguada) variables.filter_variables()\n  variables_to_restore = []\n  for var in slim.get_model_variables():\n    excluded = False\n    for exclusion in exclusions:\n      if var.op.name.startswith(exclusion):\n        excluded = True\n        break\n    if not excluded:\n      variables_to_restore.append(var)\n\n  if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\n    checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n  else:\n    checkpoint_path = FLAGS.checkpoint_path\n\n  tf.logging.info(\'Fine-tuning from %s\' % checkpoint_path)\n\n  return slim.assign_from_checkpoint_fn(\n      checkpoint_path,\n      variables_to_restore,\n      ignore_missing_vars=FLAGS.ignore_missing_vars)\n\ndef get_var_list_to_restore():\n  """"""Choosing which vars to restore, ignore vars by setting --checkpoint_exclude_scopes """"""\n\n  variables_to_restore = []\n  if FLAGS.checkpoint_exclude_scopes is not None:\n    exclusions = [scope.strip()\n                  for scope in FLAGS.checkpoint_exclude_scopes.split(\',\')]\n\n    # build restore list\n    for var in tf.model_variables():\n      excluded = False\n      for exclusion in exclusions:\n        if var.name.startswith(exclusion):\n          excluded = True\n          break\n      if not excluded:\n        variables_to_restore.append(var)\n  else:\n    variables_to_restore = tf.model_variables()\n\n  variables_to_restore_final = []\n  if FLAGS.checkpoint_include_scopes is not None:\n      includes = [\n              scope.strip()\n              for scope in FLAGS.checkpoint_include_scopes.split(\',\')\n              ]\n      for var in variables_to_restore:\n          included = False\n          for include in includes:\n              if var.name.startswith(include):\n                  included = True\n                  break\n          if included:\n              variables_to_restore_final.append(var)\n  else:\n      variables_to_restore_final = variables_to_restore\n\n  return variables_to_restore_final\n'"
libs/nms/__init__.py,0,b''
libs/nms/py_cpu_nms.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\n\ndef py_cpu_nms(dets, thresh):\n    """"""Pure Python NMS baseline.""""""\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n'"
libs/preprocessings/__init__.py,0,b''
libs/preprocessings/coco_v1.py,33,"b'#!/usr/bin/env python\n# coding=utf-8\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\nimport tensorflow as tf\nimport libs.configs.config_v1 as cfg\nfrom . import utils as preprocess_utils\n\nFLAGS = tf.app.flags.FLAGS \n\ndef preprocess_image(image, gt_boxes, gt_masks, is_training=False):\n    """"""preprocess image for coco\n    1. random flipping\n    2. min size resizing\n    3. zero mean \n    4. ... \n    """"""\n    if is_training:\n        return preprocess_for_training(image, gt_boxes, gt_masks)\n    else:\n        return preprocess_for_test(image, gt_boxes, gt_masks)\n\n\ndef preprocess_for_training(image, gt_boxes, gt_masks):\n    \n    ih, iw = tf.shape(image)[0], tf.shape(image)[1]\n    ## random flipping\n    coin = tf.to_float(tf.random_uniform([1]))[0]\n    image, gt_boxes, gt_masks =\\\n            tf.cond(tf.greater_equal(coin, 0.5), \n                    lambda: (preprocess_utils.flip_image(image),\n                            preprocess_utils.flip_gt_boxes(gt_boxes, ih, iw),\n                            preprocess_utils.flip_gt_masks(gt_masks)),\n                    lambda: (image, gt_boxes, gt_masks))\n\n    ## min size resizing\n    new_ih, new_iw = preprocess_utils._smallest_size_at_least(ih, iw, cfg.FLAGS.image_min_size)\n    image = tf.expand_dims(image, 0)\n    image = tf.image.resize_bilinear(image, [new_ih, new_iw], align_corners=False)\n    image = tf.squeeze(image, axis=[0])\n\n    gt_masks = tf.expand_dims(gt_masks, -1)\n    gt_masks = tf.cast(gt_masks, tf.float32)\n    gt_masks = tf.image.resize_nearest_neighbor(gt_masks, [new_ih, new_iw], align_corners=False)\n    gt_masks = tf.cast(gt_masks, tf.int32)\n    gt_masks = tf.squeeze(gt_masks, axis=[-1])\n\n    scale_ratio = tf.to_float(new_ih) / tf.to_float(ih)\n    gt_boxes = preprocess_utils.resize_gt_boxes(gt_boxes, scale_ratio)\n\n    ## random flip image\n    # val_lr = tf.to_float(tf.random_uniform([1]))[0]\n    # image = tf.cond(val_lr > 0.5, lambda: preprocess_utils.flip_image(image), lambda: image)\n    # gt_masks = tf.cond(val_lr > 0.5, lambda: preprocess_utils.flip_gt_masks(gt_masks), lambda: gt_masks)\n    # gt_boxes = tf.cond(val_lr > 0.5, lambda: preprocess_utils.flip_gt_boxes(gt_boxes, new_ih, new_iw), lambda: gt_boxes)\n\n    ## zero mean image\n    image = tf.cast(image, tf.float32)\n    image = image / 256.0\n    image = (image - 0.5) * 2.0\n    image = tf.expand_dims(image, axis=0)\n\n    ## rgb to bgr\n    image = tf.reverse(image, axis=[-1])\n\n    return image, gt_boxes, gt_masks \n\ndef preprocess_for_test(image, gt_boxes, gt_masks):\n\n\n    ih, iw = tf.shape(image)[0], tf.shape(image)[1]\n\n    ## min size resizing\n    new_ih, new_iw = preprocess_utils._smallest_size_at_least(ih, iw, cfg.FLAGS.image_min_size)\n    image = tf.expand_dims(image, 0)\n    image = tf.image.resize_bilinear(image, [new_ih, new_iw], align_corners=False)\n    image = tf.squeeze(image, axis=[0])\n\n    gt_masks = tf.expand_dims(gt_masks, -1)\n    gt_masks = tf.cast(gt_masks, tf.float32)\n    gt_masks = tf.image.resize_nearest_neighbor(gt_masks, [new_ih, new_iw], align_corners=False)\n    gt_masks = tf.cast(gt_masks, tf.int32)\n    gt_masks = tf.squeeze(gt_masks, axis=[-1])\n\n    scale_ratio = tf.to_float(new_ih) / tf.to_float(ih)\n    gt_boxes = preprocess_utils.resize_gt_boxes(gt_boxes, scale_ratio)\n    \n    ## zero mean image\n    image = tf.cast(image, tf.float32)\n    image = image / 256.0\n    image = (image - 0.5) * 2.0\n    image = tf.expand_dims(image, axis=0)\n\n    ## rgb to bgr\n    image = tf.reverse(image, axis=[-1])\n\n    return image, gt_boxes, gt_masks \n'"
libs/preprocessings/utils.py,59,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.contrib import slim\n\n\ndef _crop(image, offset_height, offset_width, crop_height, crop_width):\n  original_shape = tf.shape(image)\n\n  rank_assertion = tf.Assert(\n      tf.equal(tf.rank(image), 3),\n      ['Rank of image must be equal to 3.'])\n  cropped_shape = control_flow_ops.with_dependencies(\n      [rank_assertion],\n      tf.stack([crop_height, crop_width, original_shape[2]]))\n\n  size_assertion = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(original_shape[0], crop_height),\n          tf.greater_equal(original_shape[1], crop_width)),\n      ['Crop size greater than the image size.'])\n\n  offsets = tf.to_int32(tf.stack([offset_height, offset_width, 0]))\n\n  # Use tf.slice instead of crop_to_bounding box as it accepts tensors to\n  # define the crop size.\n  image = control_flow_ops.with_dependencies(\n      [size_assertion],\n      tf.slice(image, offsets, cropped_shape))\n  return tf.reshape(image, cropped_shape)\n\n\ndef _random_crop(image_list, label_list, crop_height, crop_width):\n  if not image_list:\n    raise ValueError('Empty image_list.')\n\n  # Compute the rank assertions.\n  rank_assertions = []\n  for i in range(len(image_list)):\n    image_rank = tf.rank(image_list[i])\n    rank_assert = tf.Assert(\n        tf.equal(image_rank, 3),\n        ['Wrong rank for tensor  %s [expected] [actual]',\n         image_list[i].name, 3, image_rank])\n    rank_assertions.append(rank_assert)\n\n  image_shape = control_flow_ops.with_dependencies(\n      [rank_assertions[0]],\n      tf.shape(image_list[0]))\n  image_height = image_shape[0]\n  image_width = image_shape[1]\n  crop_size_assert = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(image_height, crop_height),\n          tf.greater_equal(image_width, crop_width)),\n      ['Crop size greater than the image size.', image_height, image_width, crop_height, crop_width])\n\n  asserts = [rank_assertions[0], crop_size_assert]\n\n  for i in range(1, len(image_list)):\n    image = image_list[i]\n    asserts.append(rank_assertions[i])\n    shape = control_flow_ops.with_dependencies([rank_assertions[i]],\n                                               tf.shape(image))\n    height = shape[0]\n    width = shape[1]\n\n    height_assert = tf.Assert(\n        tf.equal(height, image_height),\n        ['Wrong height for tensor %s [expected][actual]',\n         image.name, height, image_height])\n    width_assert = tf.Assert(\n        tf.equal(width, image_width),\n        ['Wrong width for tensor %s [expected][actual]',\n         image.name, width, image_width])\n    asserts.extend([height_assert, width_assert])\n\n  # Create a random bounding box.\n  #\n  # Use tf.random_uniform and not numpy.random.rand as doing the former would\n  # generate random numbers at graph eval time, unlike the latter which\n  # generates random numbers at graph definition time.\n  max_offset_height = control_flow_ops.with_dependencies(\n      asserts, tf.reshape(image_height - crop_height + 1, []))\n  max_offset_width = control_flow_ops.with_dependencies(\n      asserts, tf.reshape(image_width - crop_width + 1, []))\n  offset_height = tf.random_uniform(\n      [], maxval=max_offset_height, dtype=tf.int32)\n  offset_width = tf.random_uniform(\n      [], maxval=max_offset_width, dtype=tf.int32)\n\n  cropped_images = [_crop(image, offset_height, offset_width,\n                          crop_height, crop_width) for image in image_list]\n  cropped_labels = [_crop(label, offset_height, offset_width,\n                          crop_height, crop_width) for label in label_list]\n  return cropped_images, cropped_labels\n\n\ndef _central_crop(image_list, label_list, crop_height, crop_width):\n  output_images = []\n  output_labels = []\n  for image, label in zip(image_list, label_list):\n    image_height = tf.shape(image)[0]\n    image_width = tf.shape(image)[1]\n\n    offset_height = (image_height - crop_height) / 2\n    offset_width = (image_width - crop_width) / 2\n\n    output_images.append(_crop(image, offset_height, offset_width,\n                               crop_height, crop_width))\n    output_labels.append(_crop(label, offset_height, offset_width,\n                               crop_height, crop_width))\n  return output_images, output_labels\n\n\ndef _smallest_size_at_least(height, width, smallest_side):\n  smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)\n\n  height = tf.to_float(height)\n  width = tf.to_float(width)\n  smallest_side = tf.to_float(smallest_side)\n\n  scale = tf.cond(tf.greater(height, width),\n                  lambda: smallest_side / width,\n                  lambda: smallest_side / height)\n  new_height = tf.to_int32(height * scale)\n  new_width = tf.to_int32(width * scale)\n  return new_height, new_width\n\ndef _aspect_preserving_resize(image, label, smallest_side):\n  smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)\n\n  shape = tf.shape(image)\n  height = shape[0]\n  width = shape[1]\n  new_height, new_width = _smallest_size_at_least(height, width, smallest_side)\n\n  image = tf.expand_dims(image, 0)\n  resized_image = tf.image.resize_bilinear(image, [new_height, new_width],\n                                           align_corners=False)\n  resized_image = tf.squeeze(resized_image, axis=[0])\n  resized_image.set_shape([None, None, 3])\n\n  label = tf.expand_dims(label, 0)\n  resized_label = tf.image.resize_nearest_neighbor(label, [new_height, new_width],\n                                                   align_corners=False)\n  resized_label = tf.squeeze(resized_label, axis=[0])\n  resized_label.set_shape([None, None, 1])\n  return resized_image, resized_label\n\ndef flip_gt_boxes(gt_boxes, ih, iw):\n    x1s, y1s, x2s, y2s, cls = \\\n            gt_boxes[:, 0], gt_boxes[:, 1], gt_boxes[:, 2], gt_boxes[:, 3], gt_boxes[:, 4]\n    x1s = tf.to_float(iw) - x1s\n    x2s = tf.to_float(iw) - x2s\n    return tf.concat(values=(x2s[:, tf.newaxis], \n                             y1s[:, tf.newaxis], \n                             x1s[:, tf.newaxis], \n                             y2s[:, tf.newaxis], \n                             cls[:, tf.newaxis]), axis=1)\n\ndef flip_gt_masks(gt_masks):\n    return tf.reverse(gt_masks, axis=[2])\n\ndef flip_image(image):\n    return tf.reverse(image, axis=[1])\n\ndef resize_gt_boxes(gt_boxes, scale_ratio):\n    xys, cls = \\\n            gt_boxes[:, 0:4], gt_boxes[:, 4]\n    xys = xys * scale_ratio \n    return tf.concat(values=(xys, cls[:, tf.newaxis]), axis=1)\n\n"""
libs/visualization/__init__.py,0,b''
libs/visualization/pil_utils.py,1,"b'import numpy as np\nimport tensorflow as tf\nfrom PIL import Image, ImageFont, ImageDraw, ImageEnhance\n\nFLAGS = tf.app.flags.FLAGS\n_DEBUG = False\n\ndef draw_img(step, image, name=\'\', image_height=1, image_width=1, rois=None):\n    #print(""image"")\n    #print(image)\n    #norm_image = np.uint8(image/np.max(np.abs(image))*255.0)\n    norm_image = np.uint8(image/0.1*127.0 + 127.0)\n    #print(""norm_image"")\n    #print(norm_image)\n    source_img = Image.fromarray(norm_image)\n    return source_img.save(FLAGS.train_dir + \'test_\' + name + \'_\' +  str(step) +\'.jpg\', \'JPEG\')\n\ndef draw_bbox(step, image, name=\'\', image_height=1, image_width=1, bbox=None, label=None, gt_label=None, prob=None):\n    #print(prob[:,label])\n    source_img = Image.fromarray(image)\n    b, g, r = source_img.split()\n    source_img = Image.merge(""RGB"", (r, g, b))\n    draw = ImageDraw.Draw(source_img)\n    color = \'#0000ff\'\n    if bbox is not None:\n        for i, box in enumerate(bbox):\n            if label is not None:\n                if prob is not None:\n                    if (prob[i,label[i]] > 0.5) and (label[i] > 0):\n                        if gt_label is not None:\n                            text  = cat_id_to_cls_name(label[i]) + \' : \' + cat_id_to_cls_name(gt_label[i])\n                            if label[i] != gt_label[i]:\n                                color = \'#ff0000\'#draw.text((2+bbox[i,0], 2+bbox[i,1]), cat_id_to_cls_name(label[i]) + \' : \' + cat_id_to_cls_name(gt_label[i]), fill=\'#ff0000\')\n                            else:\n                                color = \'#0000ff\'  \n                        else: \n                            text = cat_id_to_cls_name(label[i])\n                        draw.text((2+bbox[i,0], 2+bbox[i,1]), text, fill=color)\n                        if _DEBUG is True:\n                            print(""plot"",label[i], prob[i,label[i]])\n                        draw.rectangle(box,fill=None,outline=color)\n                    else: \n                        if _DEBUG is True:\n                            print(""skip"",label[i], prob[i,label[i]])\n                else:\n                    text = cat_id_to_cls_name(label[i])\n                    draw.text((2+bbox[i,0], 2+bbox[i,1]), text, fill=color)\n                    draw.rectangle(box,fill=None,outline=color)\n\n\n    return source_img.save(FLAGS.train_dir + \'/est_imgs/test_\' + name + \'_\' +  str(step) +\'.jpg\', \'JPEG\')\n\ndef cat_id_to_cls_name(catId):\n    cls_name = np.array([  \'background\', \'person\', \'bicycle\', \'car\', \'motorcycle\', \'airplane\',\n                       \'bus\', \'train\', \'truck\', \'boat\', \'traffic light\', \'fire hydrant\',\n                       \'stop sign\', \'parking meter\', \'bench\', \'bird\', \'cat\', \'dog\',\n                       \'horse\', \'sheep\', \'cow\', \'elephant\', \'bear\', \'zebra\', \'giraffe\',\n                       \'backpack\', \'umbrella\', \'handbag\', \'tie\', \'suitcase\', \'frisbee\',\n                       \'skis\', \'snowboard\', \'sports ball\', \'kite\', \'baseball bat\',\n                       \'baseball glove\', \'skateboard\', \'surfboard\', \'tennis racket\',\n                       \'bottle\', \'wine glass\', \'cup\', \'fork\', \'knife\', \'spoon\', \'bowl\',\n                       \'banana\', \'apple\', \'sandwich\', \'orange\', \'broccoli\', \'carrot\',\n                       \'hot dog\', \'pizza\', \'donut\', \'cake\', \'chair\', \'couch\',\n                       \'potted plant\', \'bed\', \'dining table\', \'toilet\', \'tv\', \'laptop\',\n                       \'mouse\', \'remote\', \'keyboard\', \'cell phone\', \'microwave\', \'oven\',\n                       \'toaster\', \'sink\', \'refrigerator\', \'book\', \'clock\', \'vase\',\n                       \'scissors\', \'teddy bear\', \'hair drier\', \'toothbrush\'])\n    return cls_name[catId]'"
libs/visualization/summary_utils.py,11,"b'import tensorflow as tf\n\n\ndef visualize_input(boxes, image, masks):\n    image_sum_sample = image[:1]\n    visualize_masks(masks, ""input_image_gt_mask"")\n    visualize_bb(image, boxes, ""input_image_gt_bb"")\n    visualize_input_image(image_sum_sample)\n\n\ndef visualize_rpn_predictions(boxes, image):\n    image_sum_sample = image[:1]\n    visualize_bb(image_sum_sample, boxes, ""rpn_pred_bb"")\n\n# TODO: Present all masks in different colors\ndef visualize_masks(masks, name):\n    masks = tf.cast(masks, tf.float32)\n    tf.summary.image(name=name, tensor=masks, max_outputs=1)\n\n\ndef visualize_bb(image, boxes, name):\n    image_sum_sample_shape = tf.shape(image)[1:]\n    gt_x_min = boxes[:, 0] / tf.cast(image_sum_sample_shape[1], tf.float32)\n    gt_y_min = boxes[:, 1] / tf.cast(image_sum_sample_shape[0], tf.float32)\n    gt_x_max = boxes[:, 2] / tf.cast(image_sum_sample_shape[1], tf.float32)\n    gt_y_max = boxes[:, 3] / tf.cast(image_sum_sample_shape[0], tf.float32)\n    bb = tf.stack([gt_y_min, gt_x_min, gt_y_max, gt_x_max], axis=1)\n    tf.summary.image(name=name,\n                     tensor=tf.image.draw_bounding_boxes(image, tf.expand_dims(bb, 0), name=None),\n                     max_outputs=1)\n\n\ndef visualize_input_image(image):\n    tf.summary.image(name=""input_image"", tensor=image, max_outputs=1)\n\n\ndef visualize_final_predictions(boxes, image, masks):\n    visualize_masks(masks, ""pred_mask"")\n    visualize_bb(image, boxes, ""final_bb_pred"")\n'"
libs/datasets/pycocotools/__init__.py,0,"b""__author__ = 'tylin'\n"""
libs/datasets/pycocotools/coco.py,0,"b'__author__ = \'tylin\'\n__version__ = \'2.0\'\n# Interface for accessing the Microsoft COCO dataset.\n\n# Microsoft COCO is a large image dataset designed for object detection,\n# segmentation, and caption generation. pycocotools is a Python API that\n# assists in loading, parsing and visualizing the annotations in COCO.\n# Please visit http://mscoco.org/ for more information on COCO, including\n# for the data, paper, and tutorials. The exact format of the annotations\n# is also described on the COCO website. For example usage of the pycocotools\n# please see pycocotools_demo.ipynb. In addition to this API, please download both\n# the COCO images and annotations in order to run the demo.\n\n# An alternative to using the API is to load the annotations directly\n# into Python dictionary\n# Using the API provides additional utility functions. Note that this API\n# supports both *instance* and *caption* annotations. In the case of\n# captions not all functions are defined (e.g. categories are undefined).\n\n# The following API functions are defined:\n#  COCO       - COCO api class that loads COCO annotation file and prepare data structures.\n#  decodeMask - Decode binary mask M encoded via run-length encoding.\n#  encodeMask - Encode binary mask M using run-length encoding.\n#  getAnnIds  - Get ann ids that satisfy given filter conditions.\n#  getCatIds  - Get cat ids that satisfy given filter conditions.\n#  getImgIds  - Get img ids that satisfy given filter conditions.\n#  loadAnns   - Load anns with the specified ids.\n#  loadCats   - Load cats with the specified ids.\n#  loadImgs   - Load imgs with the specified ids.\n#  annToMask  - Convert segmentation in an annotation to binary mask.\n#  showAnns   - Display the specified annotations.\n#  loadRes    - Load algorithm results and create API for accessing them.\n#  download   - Download COCO images from mscoco.org server.\n# Throughout the API ""ann""=annotation, ""cat""=category, and ""img""=image.\n# Help on each functions can be accessed by: ""help COCO>function"".\n\n# See also COCO>decodeMask,\n# COCO>encodeMask, COCO>getAnnIds, COCO>getCatIds,\n# COCO>getImgIds, COCO>loadAnns, COCO>loadCats,\n# COCO>loadImgs, COCO>annToMask, COCO>showAnns\n\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2014.\n# Licensed under the Simplified BSD License [see bsd.txt]\n\nimport json\nimport time\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import PatchCollection\nfrom matplotlib.patches import Polygon\nimport numpy as np\nimport copy\nimport itertools\nfrom libs.datasets.pycocotools import mask as maskUtils # change importing\nimport os\nfrom collections import defaultdict\nimport sys\nPYTHON_VERSION = sys.version_info[0]\nif PYTHON_VERSION == 2:\n    from urllib import urlretrieve\nelif PYTHON_VERSION == 3:\n    from urllib.request import urlretrieve\n\nclass COCO:\n    def __init__(self, annotation_file=None):\n        """"""\n        Constructor of Microsoft COCO helper class for reading and visualizing annotations.\n        :param annotation_file (str): location of annotation file\n        :param image_folder (str): location to the folder that hosts images.\n        :return:\n        """"""\n        # load dataset\n        self.dataset,self.anns,self.cats,self.imgs = dict(),dict(),dict(),dict()\n        self.imgToAnns, self.catToImgs = defaultdict(list), defaultdict(list)\n        if not annotation_file == None:\n            print(\'loading annotations into memory...\')\n            tic = time.time()\n            dataset = json.load(open(annotation_file, \'r\'))\n            assert type(dataset)==dict, \'annotation file format {} not supported\'.format(type(dataset))\n            print(\'Done (t={:0.2f}s)\'.format(time.time()- tic))\n            self.dataset = dataset\n            self.createIndex()\n\n    def createIndex(self):\n        # create index\n        print(\'creating index...\')\n        anns, cats, imgs = {}, {}, {}\n        imgToAnns,catToImgs = defaultdict(list),defaultdict(list)\n        if \'annotations\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                imgToAnns[ann[\'image_id\']].append(ann)\n                anns[ann[\'id\']] = ann\n\n        if \'images\' in self.dataset:\n            for img in self.dataset[\'images\']:\n                imgs[img[\'id\']] = img\n\n        if \'categories\' in self.dataset:\n            for cat in self.dataset[\'categories\']:\n                cats[cat[\'id\']] = cat\n\n        if \'annotations\' in self.dataset and \'categories\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                catToImgs[ann[\'category_id\']].append(ann[\'image_id\'])\n\n        print(\'index created!\')\n\n        # create class members\n        self.anns = anns\n        self.imgToAnns = imgToAnns\n        self.catToImgs = catToImgs\n        self.imgs = imgs\n        self.cats = cats\n\n    def info(self):\n        """"""\n        Print information about the annotation file.\n        :return:\n        """"""\n        for key, value in self.dataset[\'info\'].items():\n            print(\'{}: {}\'.format(key, value))\n\n    def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n        """"""\n        Get ann ids that satisfy given filter conditions. default skips that filter\n        :param imgIds  (int array)     : get anns for given imgs\n               catIds  (int array)     : get anns for given cats\n               areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n               iscrowd (boolean)       : get anns for given crowd label (False or True)\n        :return: ids (int array)       : integer array of ann ids\n        """"""\n        imgIds = imgIds if type(imgIds) == list else [imgIds]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(imgIds) == len(catIds) == len(areaRng) == 0:\n            anns = self.dataset[\'annotations\']\n        else:\n            if not len(imgIds) == 0:\n                lists = [self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns]\n                anns = list(itertools.chain.from_iterable(lists))\n            else:\n                anns = self.dataset[\'annotations\']\n            anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann[\'category_id\'] in catIds]\n            anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann[\'area\'] > areaRng[0] and ann[\'area\'] < areaRng[1]]\n        if not iscrowd == None:\n            ids = [ann[\'id\'] for ann in anns if ann[\'iscrowd\'] == iscrowd]\n        else:\n            ids = [ann[\'id\'] for ann in anns]\n        return ids\n\n    def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n        """"""\n        filtering parameters. default skips that filter.\n        :param catNms (str array)  : get cats for given cat names\n        :param supNms (str array)  : get cats for given supercategory names\n        :param catIds (int array)  : get cats for given cat ids\n        :return: ids (int array)   : integer array of cat ids\n        """"""\n        catNms = catNms if type(catNms) == list else [catNms]\n        supNms = supNms if type(supNms) == list else [supNms]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(catNms) == len(supNms) == len(catIds) == 0:\n            cats = self.dataset[\'categories\']\n        else:\n            cats = self.dataset[\'categories\']\n            cats = cats if len(catNms) == 0 else [cat for cat in cats if cat[\'name\']          in catNms]\n            cats = cats if len(supNms) == 0 else [cat for cat in cats if cat[\'supercategory\'] in supNms]\n            cats = cats if len(catIds) == 0 else [cat for cat in cats if cat[\'id\']            in catIds]\n        ids = [cat[\'id\'] for cat in cats]\n        return ids\n\n    def getImgIds(self, imgIds=[], catIds=[]):\n        \'\'\'\n        Get img ids that satisfy given filter conditions.\n        :param imgIds (int array) : get imgs for given ids\n        :param catIds (int array) : get imgs with all given cats\n        :return: ids (int array)  : integer array of img ids\n        \'\'\'\n        imgIds = imgIds if type(imgIds) == list else [imgIds]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(imgIds) == len(catIds) == 0:\n            ids = self.imgs.keys()\n        else:\n            ids = set(imgIds)\n            for i, catId in enumerate(catIds):\n                if i == 0 and len(ids) == 0:\n                    ids = set(self.catToImgs[catId])\n                else:\n                    ids &= set(self.catToImgs[catId])\n        return list(ids)\n\n    def loadAnns(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying anns\n        :return: anns (object array) : loaded ann objects\n        """"""\n        if type(ids) == list:\n            return [self.anns[id] for id in ids]\n        elif type(ids) == int:\n            return [self.anns[ids]]\n\n    def loadCats(self, ids=[]):\n        """"""\n        Load cats with the specified ids.\n        :param ids (int array)       : integer ids specifying cats\n        :return: cats (object array) : loaded cat objects\n        """"""\n        if type(ids) == list:\n            return [self.cats[id] for id in ids]\n        elif type(ids) == int:\n            return [self.cats[ids]]\n\n    def loadImgs(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying img\n        :return: imgs (object array) : loaded img objects\n        """"""\n        if type(ids) == list:\n            return [self.imgs[id] for id in ids]\n        elif type(ids) == int:\n            return [self.imgs[ids]]\n\n    def showAnns(self, anns):\n        """"""\n        Display the specified annotations.\n        :param anns (array of object): annotations to display\n        :return: None\n        """"""\n        if len(anns) == 0:\n            return 0\n        if \'segmentation\' in anns[0] or \'keypoints\' in anns[0]:\n            datasetType = \'instances\'\n        elif \'caption\' in anns[0]:\n            datasetType = \'captions\'\n        else:\n            raise Exception(\'datasetType not supported\')\n        if datasetType == \'instances\':\n            ax = plt.gca()\n            ax.set_autoscale_on(False)\n            polygons = []\n            color = []\n            for ann in anns:\n                c = (np.random.random((1, 3))*0.6+0.4).tolist()[0]\n                if \'segmentation\' in ann:\n                    if type(ann[\'segmentation\']) == list:\n                        # polygon\n                        for seg in ann[\'segmentation\']:\n                            poly = np.array(seg).reshape((int(len(seg)/2), 2))\n                            polygons.append(Polygon(poly))\n                            color.append(c)\n                    else:\n                        # mask\n                        t = self.imgs[ann[\'image_id\']]\n                        if type(ann[\'segmentation\'][\'counts\']) == list:\n                            rle = maskUtils.frPyObjects([ann[\'segmentation\']], t[\'height\'], t[\'width\'])\n                        else:\n                            rle = [ann[\'segmentation\']]\n                        m = maskUtils.decode(rle)\n                        img = np.ones( (m.shape[0], m.shape[1], 3) )\n                        if ann[\'iscrowd\'] == 1:\n                            color_mask = np.array([2.0,166.0,101.0])/255\n                        if ann[\'iscrowd\'] == 0:\n                            color_mask = np.random.random((1, 3)).tolist()[0]\n                        for i in range(3):\n                            img[:,:,i] = color_mask[i]\n                        ax.imshow(np.dstack( (img, m*0.5) ))\n                if \'keypoints\' in ann and type(ann[\'keypoints\']) == list:\n                    # turn skeleton into zero-based index\n                    sks = np.array(self.loadCats(ann[\'category_id\'])[0][\'skeleton\'])-1\n                    kp = np.array(ann[\'keypoints\'])\n                    x = kp[0::3]\n                    y = kp[1::3]\n                    v = kp[2::3]\n                    for sk in sks:\n                        if np.all(v[sk]>0):\n                            plt.plot(x[sk],y[sk], linewidth=3, color=c)\n                    plt.plot(x[v>0], y[v>0],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=\'k\',markeredgewidth=2)\n                    plt.plot(x[v>1], y[v>1],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=c, markeredgewidth=2)\n            p = PatchCollection(polygons, facecolor=color, linewidths=0, alpha=0.4)\n            ax.add_collection(p)\n            p = PatchCollection(polygons, facecolor=\'none\', edgecolors=color, linewidths=2)\n            ax.add_collection(p)\n        elif datasetType == \'captions\':\n            for ann in anns:\n                print(ann[\'caption\'])\n\n    def loadRes(self, resFile):\n        """"""\n        Load result file and return a result api object.\n        :param   resFile (str)     : file name of result file\n        :return: res (obj)         : result api object\n        """"""\n        res = COCO()\n        res.dataset[\'images\'] = [img for img in self.dataset[\'images\']]\n\n        print(\'Loading and preparing results...\')\n        tic = time.time()\n        if type(resFile) == str or type(resFile) == unicode:\n            anns = json.load(open(resFile))\n        elif type(resFile) == np.ndarray:\n            anns = self.loadNumpyAnnotations(resFile)\n        else:\n            anns = resFile\n        assert type(anns) == list, \'results in not an array of objects\'\n        annsImgIds = [ann[\'image_id\'] for ann in anns]\n        assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n               \'Results do not correspond to current coco set\'\n        if \'caption\' in anns[0]:\n            imgIds = set([img[\'id\'] for img in res.dataset[\'images\']]) & set([ann[\'image_id\'] for ann in anns])\n            res.dataset[\'images\'] = [img for img in res.dataset[\'images\'] if img[\'id\'] in imgIds]\n            for id, ann in enumerate(anns):\n                ann[\'id\'] = id+1\n        elif \'bbox\' in anns[0] and not anns[0][\'bbox\'] == []:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                bb = ann[\'bbox\']\n                x1, x2, y1, y2 = [bb[0], bb[0]+bb[2], bb[1], bb[1]+bb[3]]\n                if not \'segmentation\' in ann:\n                    ann[\'segmentation\'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n                ann[\'area\'] = bb[2]*bb[3]\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'segmentation\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                # now only support compressed RLE format as segmentation results\n                ann[\'area\'] = maskUtils.area(ann[\'segmentation\'])\n                if not \'bbox\' in ann:\n                    ann[\'bbox\'] = maskUtils.toBbox(ann[\'segmentation\'])\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'keypoints\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                s = ann[\'keypoints\']\n                x = s[0::3]\n                y = s[1::3]\n                x0,x1,y0,y1 = np.min(x), np.max(x), np.min(y), np.max(y)\n                ann[\'area\'] = (x1-x0)*(y1-y0)\n                ann[\'id\'] = id + 1\n                ann[\'bbox\'] = [x0,y0,x1-x0,y1-y0]\n        print(\'DONE (t={:0.2f}s)\'.format(time.time()- tic))\n\n        res.dataset[\'annotations\'] = anns\n        res.createIndex()\n        return res\n\n    def download(self, tarDir = None, imgIds = [] ):\n        \'\'\'\n        Download COCO images from mscoco.org server.\n        :param tarDir (str): COCO results directory name\n               imgIds (list): images to be downloaded\n        :return:\n        \'\'\'\n        if tarDir is None:\n            print(\'Please specify target directory\')\n            return -1\n        if len(imgIds) == 0:\n            imgs = self.imgs.values()\n        else:\n            imgs = self.loadImgs(imgIds)\n        N = len(imgs)\n        if not os.path.exists(tarDir):\n            os.makedirs(tarDir)\n        for i, img in enumerate(imgs):\n            tic = time.time()\n            fname = os.path.join(tarDir, img[\'file_name\'])\n            if not os.path.exists(fname):\n                urlretrieve(img[\'coco_url\'], fname)\n            print(\'downloaded {}/{} images (t={:0.1f}s)\'.format(i, N, time.time()- tic))\n\n    def loadNumpyAnnotations(self, data):\n        """"""\n        Convert result data from a numpy array [Nx7] where each row contains {imageID,x1,y1,w,h,score,class}\n        :param  data (numpy.ndarray)\n        :return: annotations (python nested list)\n        """"""\n        print(\'Converting ndarray to lists...\')\n        assert(type(data) == np.ndarray)\n        print(data.shape)\n        assert(data.shape[1] == 7)\n        N = data.shape[0]\n        ann = []\n        for i in range(N):\n            if i % 1000000 == 0:\n                print(\'{}/{}\'.format(i,N))\n            ann += [{\n                \'image_id\'  : int(data[i, 0]),\n                \'bbox\'  : [ data[i, 1], data[i, 2], data[i, 3], data[i, 4] ],\n                \'score\' : data[i, 5],\n                \'category_id\': int(data[i, 6]),\n                }]\n        return ann\n\n    def annToRLE(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE to RLE.\n        :return: binary mask (numpy 2D array)\n        """"""\n        t = self.imgs[ann[\'image_id\']]\n        h, w = t[\'height\'], t[\'width\']\n        segm = ann[\'segmentation\']\n        if type(segm) == list:\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            rles = maskUtils.frPyObjects(segm, h, w)\n            rle = maskUtils.merge(rles)\n        elif type(segm[\'counts\']) == list:\n            # uncompressed RLE\n            rle = maskUtils.frPyObjects(segm, h, w)\n        else:\n            # rle\n            rle = ann[\'segmentation\']\n        return rle\n\n    def annToMask(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n        :return: binary mask (numpy 2D array)\n        """"""\n        rle = self.annToRLE(ann)\n        m = maskUtils.decode(rle)\n        return m'"
libs/datasets/pycocotools/cocoeval.py,0,"b'__author__ = \'tsungyi\'\n\nimport numpy as np\nimport datetime\nimport time\nfrom collections import defaultdict\n# from . import mask as maskUtils\nfrom libs.datasets.pycocotools import  mask as maskUtils\nimport copy\n\nclass COCOeval:\n    # Interface for evaluating detection on the Microsoft COCO dataset.\n    #\n    # The usage for CocoEval is as follows:\n    #  cocoGt=..., cocoDt=...       # load dataset and results\n    #  E = CocoEval(cocoGt,cocoDt); # initialize CocoEval object\n    #  E.params.recThrs = ...;      # set parameters as desired\n    #  E.evaluate();                # run per image evaluation\n    #  E.accumulate();              # accumulate per image results\n    #  E.summarize();               # display summary metrics of results\n    # For example usage see evalDemo.m and http://mscoco.org/.\n    #\n    # The evaluation parameters are as follows (defaults in brackets):\n    #  imgIds     - [all] N img ids to use for evaluation\n    #  catIds     - [all] K cat ids to use for evaluation\n    #  iouThrs    - [.5:.05:.95] T=10 IoU thresholds for evaluation\n    #  recThrs    - [0:.01:1] R=101 recall thresholds for evaluation\n    #  areaRng    - [...] A=4 object area ranges for evaluation\n    #  maxDets    - [1 10 100] M=3 thresholds on max detections per image\n    #  iouType    - [\'segm\'] set iouType to \'segm\', \'bbox\' or \'keypoints\'\n    #  iouType replaced the now DEPRECATED useSegm parameter.\n    #  useCats    - [1] if true use category labels for evaluation\n    # Note: if useCats=0 category labels are ignored as in proposal scoring.\n    # Note: multiple areaRngs [Ax2] and maxDets [Mx1] can be specified.\n    #\n    # evaluate(): evaluates detections on every image and every category and\n    # concats the results into the ""evalImgs"" with fields:\n    #  dtIds      - [1xD] id for each of the D detections (dt)\n    #  gtIds      - [1xG] id for each of the G ground truths (gt)\n    #  dtMatches  - [TxD] matching gt id at each IoU or 0\n    #  gtMatches  - [TxG] matching dt id at each IoU or 0\n    #  dtScores   - [1xD] confidence of each dt\n    #  gtIgnore   - [1xG] ignore flag for each gt\n    #  dtIgnore   - [TxD] ignore flag for each dt at each IoU\n    #\n    # accumulate(): accumulates the per-image, per-category evaluation\n    # results in ""evalImgs"" into the dictionary ""eval"" with fields:\n    #  params     - parameters used for evaluation\n    #  date       - date evaluation was performed\n    #  counts     - [T,R,K,A,M] parameter dimensions (see above)\n    #  precision  - [TxRxKxAxM] precision for every evaluation setting\n    #  recall     - [TxKxAxM] max recall for every evaluation setting\n    # Note: precision and recall==-1 for settings with no gt objects.\n    #\n    # See also coco, mask, pycocoDemo, pycocoEvalDemo\n    #\n    # Microsoft COCO Toolbox.      version 2.0\n    # Data, paper, and tutorials available at:  http://mscoco.org/\n    # Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n    # Licensed under the Simplified BSD License [see coco/license.txt]\n    def __init__(self, cocoGt=None, cocoDt=None, iouType=\'segm\'):\n        \'\'\'\n        Initialize CocoEval using coco APIs for gt and dt\n        :param cocoGt: coco object with ground truth annotations\n        :param cocoDt: coco object with detection results\n        :return: None\n        \'\'\'\n        if not iouType:\n            print(\'iouType not specified. use default iouType segm\')\n        self.cocoGt   = cocoGt              # ground truth COCO API\n        self.cocoDt   = cocoDt              # detections COCO API\n        self.params   = {}                  # evaluation parameters\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results [KxAxI] elements\n        self.eval     = {}                  # accumulated evaluation results\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        self.params = Params(iouType=iouType) # parameters\n        self._paramsEval = {}               # parameters for evaluation\n        self.stats = []                     # result summarization\n        self.ious = {}                      # ious between all gts and dts\n        if not cocoGt is None:\n            self.params.imgIds = sorted(cocoGt.getImgIds())\n            self.params.catIds = sorted(cocoGt.getCatIds())\n\n\n    def _prepare(self):\n        \'\'\'\n        Prepare ._gts and ._dts for evaluation based on params\n        :return: None\n        \'\'\'\n        def _toMask(anns, coco):\n            # modify ann[\'segmentation\'] by reference\n            for ann in anns:\n                rle = coco.annToRLE(ann)\n                ann[\'segmentation\'] = rle\n        p = self.params\n        if p.useCats:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n        else:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds))\n\n        # convert ground truth to mask if iouType == \'segm\'\n        if p.iouType == \'segm\':\n            _toMask(gts, self.cocoGt)\n            _toMask(dts, self.cocoDt)\n        # set ignore flag\n        for gt in gts:\n            gt[\'ignore\'] = gt[\'ignore\'] if \'ignore\' in gt else 0\n            gt[\'ignore\'] = \'iscrowd\' in gt and gt[\'iscrowd\']\n            if p.iouType == \'keypoints\':\n                gt[\'ignore\'] = (gt[\'num_keypoints\'] == 0) or gt[\'ignore\']\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        for gt in gts:\n            self._gts[gt[\'image_id\'], gt[\'category_id\']].append(gt)\n        for dt in dts:\n            self._dts[dt[\'image_id\'], dt[\'category_id\']].append(dt)\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results\n        self.eval     = {}                  # accumulated evaluation results\n\n    def evaluate(self):\n        \'\'\'\n        Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n        :return: None\n        \'\'\'\n        tic = time.time()\n        print(\'Running per image evaluation...\')\n        p = self.params\n        # add backward compatibility if useSegm is specified in params\n        if not p.useSegm is None:\n            p.iouType = \'segm\' if p.useSegm == 1 else \'bbox\'\n            print(\'useSegm (deprecated) is not None. Running {} evaluation\'.format(p.iouType))\n        print(\'Evaluate annotation type *{}*\'.format(p.iouType))\n        p.imgIds = list(np.unique(p.imgIds))\n        if p.useCats:\n            p.catIds = list(np.unique(p.catIds))\n        p.maxDets = sorted(p.maxDets)\n        self.params=p\n\n        self._prepare()\n        # loop through images, area range, max detection number\n        catIds = p.catIds if p.useCats else [-1]\n\n        if p.iouType == \'segm\' or p.iouType == \'bbox\':\n            computeIoU = self.computeIoU\n        elif p.iouType == \'keypoints\':\n            computeIoU = self.computeOks\n        self.ious = {(imgId, catId): computeIoU(imgId, catId) \\\n                        for imgId in p.imgIds\n                        for catId in catIds}\n\n        evaluateImg = self.evaluateImg\n        maxDet = p.maxDets[-1]\n        self.evalImgs = [evaluateImg(imgId, catId, areaRng, maxDet)\n                 for catId in catIds\n                 for areaRng in p.areaRng\n                 for imgId in p.imgIds\n             ]\n        self._paramsEval = copy.deepcopy(self.params)\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format(toc-tic))\n\n    def computeIoU(self, imgId, catId):\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return []\n        inds = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in inds]\n        if len(dt) > p.maxDets[-1]:\n            dt=dt[0:p.maxDets[-1]]\n\n        if p.iouType == \'segm\':\n            g = [g[\'segmentation\'] for g in gt]\n            d = [d[\'segmentation\'] for d in dt]\n        elif p.iouType == \'bbox\':\n            g = [g[\'bbox\'] for g in gt]\n            d = [d[\'bbox\'] for d in dt]\n        else:\n            raise Exception(\'unknown iouType for iou computation\')\n\n        # compute iou between each dt and gt region\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        ious = maskUtils.iou(d,g,iscrowd)\n        return ious\n\n    def computeOks(self, imgId, catId):\n        p = self.params\n        # dimention here should be Nxm\n        gts = self._gts[imgId, catId]\n        dts = self._dts[imgId, catId]\n        inds = np.argsort([-d[\'score\'] for d in dts], kind=\'mergesort\')\n        dts = [dts[i] for i in inds]\n        if len(dts) > p.maxDets[-1]:\n            dts = dts[0:p.maxDets[-1]]\n        # if len(gts) == 0 and len(dts) == 0:\n        if len(gts) == 0 or len(dts) == 0:\n            return []\n        ious = np.zeros((len(dts), len(gts)))\n        sigmas = np.array([.26, .25, .25, .35, .35, .79, .79, .72, .72, .62,.62, 1.07, 1.07, .87, .87, .89, .89])/10.0\n        vars = (sigmas * 2)**2\n        k = len(sigmas)\n        # compute oks between each detection and ground truth object\n        for j, gt in enumerate(gts):\n            # create bounds for ignore regions(double the gt bbox)\n            g = np.array(gt[\'keypoints\'])\n            xg = g[0::3]; yg = g[1::3]; vg = g[2::3]\n            k1 = np.count_nonzero(vg > 0)\n            bb = gt[\'bbox\']\n            x0 = bb[0] - bb[2]; x1 = bb[0] + bb[2] * 2\n            y0 = bb[1] - bb[3]; y1 = bb[1] + bb[3] * 2\n            for i, dt in enumerate(dts):\n                d = np.array(dt[\'keypoints\'])\n                xd = d[0::3]; yd = d[1::3]\n                if k1>0:\n                    # measure the per-keypoint distance if keypoints visible\n                    dx = xd - xg\n                    dy = yd - yg\n                else:\n                    # measure minimum distance to keypoints in (x0,y0) & (x1,y1)\n                    z = np.zeros((k))\n                    dx = np.max((z, x0-xd),axis=0)+np.max((z, xd-x1),axis=0)\n                    dy = np.max((z, y0-yd),axis=0)+np.max((z, yd-y1),axis=0)\n                e = (dx**2 + dy**2) / vars / (gt[\'area\']+np.spacing(1)) / 2\n                if k1 > 0:\n                    e=e[vg > 0]\n                ious[i, j] = np.sum(np.exp(-e)) / e.shape[0]\n        return ious\n\n    def evaluateImg(self, imgId, catId, aRng, maxDet):\n        \'\'\'\n        perform evaluation for single category and image\n        :return: dict (single image results)\n        \'\'\'\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return None\n\n        for g in gt:\n            if g[\'ignore\'] or (g[\'area\']<aRng[0] or g[\'area\']>aRng[1]):\n                g[\'_ignore\'] = 1\n            else:\n                g[\'_ignore\'] = 0\n\n        # sort dt highest score first, sort gt ignore last\n        gtind = np.argsort([g[\'_ignore\'] for g in gt], kind=\'mergesort\')\n        gt = [gt[i] for i in gtind]\n        dtind = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in dtind[0:maxDet]]\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        # load computed ious\n        ious = self.ious[imgId, catId][:, gtind] if len(self.ious[imgId, catId]) > 0 else self.ious[imgId, catId]\n\n        T = len(p.iouThrs)\n        G = len(gt)\n        D = len(dt)\n        gtm  = np.zeros((T,G))\n        dtm  = np.zeros((T,D))\n        gtIg = np.array([g[\'_ignore\'] for g in gt])\n        dtIg = np.zeros((T,D))\n        if not len(ious)==0:\n            for tind, t in enumerate(p.iouThrs):\n                for dind, d in enumerate(dt):\n                    # information about best match so far (m=-1 -> unmatched)\n                    iou = min([t,1-1e-10])\n                    m   = -1\n                    for gind, g in enumerate(gt):\n                        # if this gt already matched, and not a crowd, continue\n                        if gtm[tind,gind]>0 and not iscrowd[gind]:\n                            continue\n                        # if dt matched to reg gt, and on ignore gt, stop\n                        if m>-1 and gtIg[m]==0 and gtIg[gind]==1:\n                            break\n                        # continue to next gt unless better match made\n                        if ious[dind,gind] < iou:\n                            continue\n                        # if match successful and best so far, store appropriately\n                        iou=ious[dind,gind]\n                        m=gind\n                    # if match made store id of match for both dt and gt\n                    if m ==-1:\n                        continue\n                    dtIg[tind,dind] = gtIg[m]\n                    dtm[tind,dind]  = gt[m][\'id\']\n                    gtm[tind,m]     = d[\'id\']\n        # set unmatched detections outside of area range to ignore\n        a = np.array([d[\'area\']<aRng[0] or d[\'area\']>aRng[1] for d in dt]).reshape((1, len(dt)))\n        dtIg = np.logical_or(dtIg, np.logical_and(dtm==0, np.repeat(a,T,0)))\n        # store results for given image and category\n        return {\n                \'image_id\':     imgId,\n                \'category_id\':  catId,\n                \'aRng\':         aRng,\n                \'maxDet\':       maxDet,\n                \'dtIds\':        [d[\'id\'] for d in dt],\n                \'gtIds\':        [g[\'id\'] for g in gt],\n                \'dtMatches\':    dtm,\n                \'gtMatches\':    gtm,\n                \'dtScores\':     [d[\'score\'] for d in dt],\n                \'gtIgnore\':     gtIg,\n                \'dtIgnore\':     dtIg,\n            }\n\n    def accumulate(self, p = None):\n        \'\'\'\n        Accumulate per image evaluation results and store the result in self.eval\n        :param p: input params for evaluation\n        :return: None\n        \'\'\'\n        print(\'Accumulating evaluation results...\')\n        tic = time.time()\n        if not self.evalImgs:\n            print(\'Please run evaluate() first\')\n        # allows input customized parameters\n        if p is None:\n            p = self.params\n        p.catIds = p.catIds if p.useCats == 1 else [-1]\n        T           = len(p.iouThrs)\n        R           = len(p.recThrs)\n        K           = len(p.catIds) if p.useCats else 1\n        A           = len(p.areaRng)\n        M           = len(p.maxDets)\n        precision   = -np.ones((T,R,K,A,M)) # -1 for the precision of absent categories\n        recall      = -np.ones((T,K,A,M))\n\n        # create dictionary for future indexing\n        _pe = self._paramsEval\n        catIds = _pe.catIds if _pe.useCats else [-1]\n        setK = set(catIds)\n        setA = set(map(tuple, _pe.areaRng))\n        setM = set(_pe.maxDets)\n        setI = set(_pe.imgIds)\n        # get inds to evaluate\n        k_list = [n for n, k in enumerate(p.catIds)  if k in setK]\n        m_list = [m for n, m in enumerate(p.maxDets) if m in setM]\n        a_list = [n for n, a in enumerate(map(lambda x: tuple(x), p.areaRng)) if a in setA]\n        i_list = [n for n, i in enumerate(p.imgIds)  if i in setI]\n        I0 = len(_pe.imgIds)\n        A0 = len(_pe.areaRng)\n        # retrieve E at each category, area range, and max number of detections\n        for k, k0 in enumerate(k_list):\n            Nk = k0*A0*I0\n            for a, a0 in enumerate(a_list):\n                Na = a0*I0\n                for m, maxDet in enumerate(m_list):\n                    E = [self.evalImgs[Nk + Na + i] for i in i_list]\n                    E = [e for e in E if not e is None]\n                    if len(E) == 0:\n                        continue\n                    dtScores = np.concatenate([e[\'dtScores\'][0:maxDet] for e in E])\n\n                    # different sorting method generates slightly different results.\n                    # mergesort is used to be consistent as Matlab implementation.\n                    inds = np.argsort(-dtScores, kind=\'mergesort\')\n\n                    dtm  = np.concatenate([e[\'dtMatches\'][:,0:maxDet] for e in E], axis=1)[:,inds]\n                    dtIg = np.concatenate([e[\'dtIgnore\'][:,0:maxDet]  for e in E], axis=1)[:,inds]\n                    gtIg = np.concatenate([e[\'gtIgnore\'] for e in E])\n                    npig = np.count_nonzero(gtIg==0 )\n                    if npig == 0:\n                        continue\n                    tps = np.logical_and(               dtm,  np.logical_not(dtIg) )\n                    fps = np.logical_and(np.logical_not(dtm), np.logical_not(dtIg) )\n\n                    tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\n                    fp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)\n                    for t, (tp, fp) in enumerate(zip(tp_sum, fp_sum)):\n                        tp = np.array(tp)\n                        fp = np.array(fp)\n                        nd = len(tp)\n                        rc = tp / npig\n                        pr = tp / (fp+tp+np.spacing(1))\n                        q  = np.zeros((R,))\n\n                        if nd:\n                            recall[t,k,a,m] = rc[-1]\n                        else:\n                            recall[t,k,a,m] = 0\n\n                        # numpy is slow without cython optimization for accessing elements\n                        # use python array gets significant speed improvement\n                        pr = pr.tolist(); q = q.tolist()\n\n                        for i in range(nd-1, 0, -1):\n                            if pr[i] > pr[i-1]:\n                                pr[i-1] = pr[i]\n\n                        inds = np.searchsorted(rc, p.recThrs, side=\'left\')\n                        try:\n                            for ri, pi in enumerate(inds):\n                                q[ri] = pr[pi]\n                        except:\n                            pass\n                        precision[t,:,k,a,m] = np.array(q)\n        self.eval = {\n            \'params\': p,\n            \'counts\': [T, R, K, A, M],\n            \'date\': datetime.datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\'),\n            \'precision\': precision,\n            \'recall\':   recall,\n        }\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format( toc-tic))\n\n    def summarize(self):\n        \'\'\'\n        Compute and display summary metrics for evaluation results.\n        Note this functin can *only* be applied on the default parameter setting\n        \'\'\'\n        def _summarize( ap=1, iouThr=None, areaRng=\'all\', maxDets=100 ):\n            p = self.params\n            iStr = \' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}\'\n            titleStr = \'Average Precision\' if ap == 1 else \'Average Recall\'\n            typeStr = \'(AP)\' if ap==1 else \'(AR)\'\n            iouStr = \'{:0.2f}:{:0.2f}\'.format(p.iouThrs[0], p.iouThrs[-1]) \\\n                if iouThr is None else \'{:0.2f}\'.format(iouThr)\n\n            aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]\n            mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]\n            if ap == 1:\n                # dimension of precision: [TxRxKxAxM]\n                s = self.eval[\'precision\']\n                # IoU\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,:,aind,mind]\n            else:\n                # dimension of recall: [TxKxAxM]\n                s = self.eval[\'recall\']\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,aind,mind]\n            if len(s[s>-1])==0:\n                mean_s = -1\n            else:\n                mean_s = np.mean(s[s>-1])\n            print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n            return mean_s\n        def _summarizeDets():\n            stats = np.zeros((12,))\n            stats[0] = _summarize(1)\n            stats[1] = _summarize(1, iouThr=.5, maxDets=self.params.maxDets[2])\n            stats[2] = _summarize(1, iouThr=.75, maxDets=self.params.maxDets[2])\n            stats[3] = _summarize(1, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[4] = _summarize(1, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[5] = _summarize(1, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            stats[6] = _summarize(0, maxDets=self.params.maxDets[0])\n            stats[7] = _summarize(0, maxDets=self.params.maxDets[1])\n            stats[8] = _summarize(0, maxDets=self.params.maxDets[2])\n            stats[9] = _summarize(0, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[10] = _summarize(0, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[11] = _summarize(0, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            return stats\n        def _summarizeKps():\n            stats = np.zeros((10,))\n            stats[0] = _summarize(1, maxDets=20)\n            stats[1] = _summarize(1, maxDets=20, iouThr=.5)\n            stats[2] = _summarize(1, maxDets=20, iouThr=.75)\n            stats[3] = _summarize(1, maxDets=20, areaRng=\'medium\')\n            stats[4] = _summarize(1, maxDets=20, areaRng=\'large\')\n            stats[5] = _summarize(0, maxDets=20)\n            stats[6] = _summarize(0, maxDets=20, iouThr=.5)\n            stats[7] = _summarize(0, maxDets=20, iouThr=.75)\n            stats[8] = _summarize(0, maxDets=20, areaRng=\'medium\')\n            stats[9] = _summarize(0, maxDets=20, areaRng=\'large\')\n            return stats\n        if not self.eval:\n            raise Exception(\'Please run accumulate() first\')\n        iouType = self.params.iouType\n        if iouType == \'segm\' or iouType == \'bbox\':\n            summarize = _summarizeDets\n        elif iouType == \'keypoints\':\n            summarize = _summarizeKps\n        self.stats = summarize()\n\n    def __str__(self):\n        self.summarize()\n\nclass Params:\n    \'\'\'\n    Params for coco evaluation api\n    \'\'\'\n    def setDetParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [1, 10, 100]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [0 ** 2, 32 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'small\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def setKpParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [20]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def __init__(self, iouType=\'segm\'):\n        if iouType == \'segm\' or iouType == \'bbox\':\n            self.setDetParams()\n        elif iouType == \'keypoints\':\n            self.setKpParams()\n        else:\n            raise Exception(\'iouType not supported\')\n        self.iouType = iouType\n        # useSegm is deprecated\n        self.useSegm = None'"
libs/datasets/pycocotools/mask.py,0,"b'__author__ = \'tsungyi\'\n\nimport libs.datasets.pycocotools._mask as _mask\n\n# Interface for manipulating masks stored in RLE format.\n#\n# RLE is a simple yet efficient format for storing binary masks. RLE\n# first divides a vector (or vectorized image) into a series of piecewise\n# constant regions and then for each piece simply stores the length of\n# that piece. For example, given M=[0 0 1 1 1 0 1] the RLE counts would\n# be [2 3 1 1], or for M=[1 1 1 1 1 1 0] the counts would be [0 6 1]\n# (note that the odd counts are always the numbers of zeros). Instead of\n# storing the counts directly, additional compression is achieved with a\n# variable bitrate representation based on a common scheme called LEB128.\n#\n# Compression is greatest given large piecewise constant regions.\n# Specifically, the size of the RLE is proportional to the number of\n# *boundaries* in M (or for an image the number of boundaries in the y\n# direction). Assuming fairly simple shapes, the RLE representation is\n# O(sqrt(n)) where n is number of pixels in the object. Hence space usage\n# is substantially lower, especially for large simple objects (large n).\n#\n# Many common operations on masks can be computed directly using the RLE\n# (without need for decoding). This includes computations such as area,\n# union, intersection, etc. All of these operations are linear in the\n# size of the RLE, in other words they are O(sqrt(n)) where n is the area\n# of the object. Computing these operations on the original mask is O(n).\n# Thus, using the RLE can result in substantial computational savings.\n#\n# The following API functions are defined:\n#  encode         - Encode binary masks using RLE.\n#  decode         - Decode binary masks encoded via RLE.\n#  merge          - Compute union or intersection of encoded masks.\n#  iou            - Compute intersection over union between masks.\n#  area           - Compute area of encoded masks.\n#  toBbox         - Get bounding boxes surrounding encoded masks.\n#  frPyObjects    - Convert polygon, bbox, and uncompressed RLE to encoded RLE mask.\n#\n# Usage:\n#  Rs     = encode( masks )\n#  masks  = decode( Rs )\n#  R      = merge( Rs, intersect=false )\n#  o      = iou( dt, gt, iscrowd )\n#  a      = area( Rs )\n#  bbs    = toBbox( Rs )\n#  Rs     = frPyObjects( [pyObjects], h, w )\n#\n# In the API the following formats are used:\n#  Rs      - [dict] Run-length encoding of binary masks\n#  R       - dict Run-length encoding of binary mask\n#  masks   - [hxwxn] Binary mask(s) (must have type np.ndarray(dtype=uint8) in column-major order)\n#  iscrowd - [nx1] list of np.ndarray. 1 indicates corresponding gt image has crowd region to ignore\n#  bbs     - [nx4] Bounding box(es) stored as [x y w h]\n#  poly    - Polygon stored as [[x1 y1 x2 y2...],[x1 y1 ...],...] (2D list)\n#  dt,gt   - May be either bounding boxes or encoded masks\n# Both poly and bbs are 0-indexed (bbox=[0 0 1 1] encloses first pixel).\n#\n# Finally, a note about the intersection over union (iou) computation.\n# The standard iou of a ground truth (gt) and detected (dt) object is\n#  iou(gt,dt) = area(intersect(gt,dt)) / area(union(gt,dt))\n# For ""crowd"" regions, we use a modified criteria. If a gt object is\n# marked as ""iscrowd"", we allow a dt to match any subregion of the gt.\n# Choosing gt\' in the crowd gt that best matches the dt can be done using\n# gt\'=intersect(dt,gt). Since by definition union(gt\',dt)=dt, computing\n#  iou(gt,dt,iscrowd) = iou(gt\',dt) = area(intersect(gt,dt)) / area(dt)\n# For crowd gt regions we use this modified criteria above for the iou.\n#\n# To compile run ""python setup.py build_ext --inplace""\n# Please do not contact us for help with compiling.\n#\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n# Licensed under the Simplified BSD License [see coco/license.txt]\n\niou         = _mask.iou\nmerge       = _mask.merge\nfrPyObjects = _mask.frPyObjects\n\ndef encode(bimask):\n    if len(bimask.shape) == 3:\n        return _mask.encode(bimask)\n    elif len(bimask.shape) == 2:\n        h, w = bimask.shape\n        return _mask.encode(bimask.reshape((h, w, 1), order=\'F\'))[0]\n\ndef decode(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.decode(rleObjs)\n    else:\n        return _mask.decode([rleObjs])[:,:,0]\n\ndef area(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.area(rleObjs)\n    else:\n        return _mask.area([rleObjs])[0]\n\ndef toBbox(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.toBbox(rleObjs)\n    else:\n        return _mask.toBbox([rleObjs])[0]'"
libs/datasets/pycocotools/setup.py,0,"b'from distutils.core import setup\nfrom Cython.Build import cythonize\nfrom distutils.extension import Extension\nimport numpy as np\n\n# To compile and install locally run ""python setup.py build_ext --inplace""\n# To install library to Python site-packages run ""python setup.py build_ext install""\n\next_modules = [\n    Extension(\n        \'_mask\',\n        sources=[\'./common/maskApi.c\', \'_mask.pyx\'],\n        include_dirs = [np.get_include(), \'./common\'],\n        extra_compile_args=[\'-Wno-cpp\', \'-Wno-unused-function\', \'-std=c99\'],\n    )\n]\n\nsetup(name=\'pycocotools\',\n      packages=[\'pycocotools\'],\n      package_dir = {\'pycocotools\': \'pycocotools\'},\n      version=\'2.0\',\n      ext_modules=\n          cythonize(ext_modules)\n      )'"
