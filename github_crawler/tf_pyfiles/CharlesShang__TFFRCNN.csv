file_path,api_count,code
faster_rcnn/__init__.py,0,"b""import sys\nsys.path.insert(0, '..')"""
faster_rcnn/demo.py,2,"b'import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os, sys, cv2\nimport argparse\nimport os.path as osp\nimport glob\n\nthis_dir = osp.dirname(__file__)\nprint(this_dir)\n\nfrom lib.networks.factory import get_network\nfrom lib.fast_rcnn.config import cfg\nfrom lib.fast_rcnn.test import im_detect\nfrom lib.fast_rcnn.nms_wrapper import nms\nfrom lib.utils.timer import Timer\n\nCLASSES = (\'__background__\',\n           \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n           \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n           \'cow\', \'diningtable\', \'dog\', \'horse\',\n           \'motorbike\', \'person\', \'pottedplant\',\n           \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\n\n\n# CLASSES = (\'__background__\',\'person\',\'bike\',\'motorbike\',\'car\',\'bus\')\n\ndef vis_detections(im, class_name, dets, ax, thresh=0.5):\n    """"""Draw detected bounding boxes.""""""\n    inds = np.where(dets[:, -1] >= thresh)[0]\n    if len(inds) == 0:\n        return\n\n    for i in inds:\n        bbox = dets[i, :4]\n        score = dets[i, -1]\n\n        ax.add_patch(\n            plt.Rectangle((bbox[0], bbox[1]),\n                          bbox[2] - bbox[0],\n                          bbox[3] - bbox[1], fill=False,\n                          edgecolor=\'red\', linewidth=3.5)\n        )\n        ax.text(bbox[0], bbox[1] - 2,\n                \'{:s} {:.3f}\'.format(class_name, score),\n                bbox=dict(facecolor=\'blue\', alpha=0.5),\n                fontsize=14, color=\'white\')\n\n    ax.set_title((\'{} detections with \'\n                  \'p({} | box) >= {:.1f}\').format(class_name, class_name,\n                                                  thresh),\n                 fontsize=14)\n    plt.axis(\'off\')\n    plt.tight_layout()\n    plt.draw()\n\n\ndef demo(sess, net, image_name):\n    """"""Detect object classes in an image using pre-computed object proposals.""""""\n\n    # Load the demo image\n    im = cv2.imread(image_name)\n\n    # Detect all object classes and regress object bounds\n    timer = Timer()\n    timer.tic()\n    scores, boxes = im_detect(sess, net, im)\n    timer.toc()\n    print (\'Detection took {:.3f}s for \'\n           \'{:d} object proposals\').format(timer.total_time, boxes.shape[0])\n\n    # Visualize detections for each class\n    im = im[:, :, (2, 1, 0)]\n    fig, ax = plt.subplots(figsize=(12, 12))\n    ax.imshow(im, aspect=\'equal\')\n\n    CONF_THRESH = 0.8\n    NMS_THRESH = 0.3\n    for cls_ind, cls in enumerate(CLASSES[1:]):\n        cls_ind += 1  # because we skipped background\n        cls_boxes = boxes[:, 4 * cls_ind:4 * (cls_ind + 1)]\n        cls_scores = scores[:, cls_ind]\n        dets = np.hstack((cls_boxes,\n                          cls_scores[:, np.newaxis])).astype(np.float32)\n        keep = nms(dets, NMS_THRESH)\n        dets = dets[keep, :]\n        vis_detections(im, cls, dets, ax, thresh=CONF_THRESH)\n\n\ndef parse_args():\n    """"""Parse input arguments.""""""\n    parser = argparse.ArgumentParser(description=\'Faster R-CNN demo\')\n    parser.add_argument(\'--gpu\', dest=\'gpu_id\', help=\'GPU device id to use [0]\',\n                        default=0, type=int)\n    parser.add_argument(\'--cpu\', dest=\'cpu_mode\',\n                        help=\'Use CPU mode (overrides --gpu)\',\n                        action=\'store_true\')\n    parser.add_argument(\'--net\', dest=\'demo_net\', help=\'Network to use [vgg16]\',\n                        default=\'VGGnet_test\')\n    parser.add_argument(\'--model\', dest=\'model\', help=\'Model path\',\n                        default=\' \')\n\n    args = parser.parse_args()\n\n    return args\n\n\nif __name__ == \'__main__\':\n    cfg.TEST.HAS_RPN = True  # Use RPN for proposals\n\n    args = parse_args()\n\n    if args.model == \' \' or not os.path.exists(args.model):\n        print (\'current path is \' + os.path.abspath(__file__))\n        raise IOError((\'Error: Model not found.\\n\'))\n\n    # init session\n    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n    # load network\n    net = get_network(args.demo_net)\n    # load model\n    print (\'Loading network {:s}... \'.format(args.demo_net)),\n    saver = tf.train.Saver()\n    saver.restore(sess, args.model)\n    print (\' done.\')\n\n    # Warmup on a dummy image\n    im = 128 * np.ones((300, 300, 3), dtype=np.uint8)\n    for i in xrange(2):\n        _, _ = im_detect(sess, net, im)\n\n    im_names = glob.glob(os.path.join(cfg.DATA_DIR, \'demo\', \'*.png\')) + \\\n               glob.glob(os.path.join(cfg.DATA_DIR, \'demo\', \'*.jpg\'))\n\n    for im_name in im_names:\n        print \'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\'\n        print \'Demo for {:s}\'.format(im_name)\n        demo(sess, net, im_name)\n\n    plt.show()\n\n'"
faster_rcnn/test_net.py,2,"b'#!/usr/bin/env python\n\n# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Test a Fast R-CNN network on an image database.""""""\nimport sys,os\nthis_dir = os.path.dirname(__file__)\nsys.path.insert(0, this_dir + \'/..\')\n# import _init_paths\nfrom lib.fast_rcnn.test import test_net\nfrom lib.fast_rcnn.config import cfg, cfg_from_file\nfrom lib.datasets.factory import get_imdb\nfrom lib.networks.factory import get_network\nimport argparse\nimport pprint\nimport time\nimport tensorflow as tf\n\ndef parse_args():\n    """"""\n    Parse input arguments\n    """"""\n    parser = argparse.ArgumentParser(description=\'Test a Fast R-CNN network\')\n    parser.add_argument(\'--gpu\', dest=\'gpu_id\', help=\'GPU id to use\',\n                        default=0, type=int)\n    parser.add_argument(\'--def\', dest=\'prototxt\',\n                        help=\'prototxt file defining the network\',\n                        default=None, type=str)\n    parser.add_argument(\'--weights\', dest=\'model\',\n                        help=\'model to test\',\n                        default=None, type=str)\n    parser.add_argument(\'--cfg\', dest=\'cfg_file\',\n                        help=\'optional config file\', default=None, type=str)\n    parser.add_argument(\'--wait\', dest=\'wait\',\n                        help=\'wait until net file exists\',\n                        default=True, type=bool)\n    parser.add_argument(\'--imdb\', dest=\'imdb_name\',\n                        help=\'dataset to test\',\n                        default=\'voc_2007_test\', type=str)\n    parser.add_argument(\'--comp\', dest=\'comp_mode\', help=\'competition mode\',\n                        action=\'store_true\')\n    parser.add_argument(\'--network\', dest=\'network_name\',\n                        help=\'name of the network\',\n                        default=None, type=str)\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n\n    args = parser.parse_args()\n    return args\n\nif __name__ == \'__main__\':\n    args = parse_args()\n\n    print(\'Called with args:\')\n    print(args)\n\n    if args.cfg_file is not None:\n        cfg_from_file(args.cfg_file)\n\n    print(\'Using config:\')\n    pprint.pprint(cfg)\n\n    while not os.path.exists(args.model) and args.wait:\n        print(\'Waiting for {} to exist...\'.format(args.model))\n        time.sleep(1000)\n\n    weights_filename = os.path.splitext(os.path.basename(args.model))[0]\n\n    imdb = get_imdb(args.imdb_name)\n    imdb.competition_mode(args.comp_mode)\n\n    device_name = \'/gpu:{:d}\'.format(args.gpu_id)\n    print device_name\n\n    network = get_network(args.network_name)\n    print \'Use network `{:s}` in training\'.format(args.network_name)\n\n    cfg.GPU_ID = args.gpu_id\n\n    # start a session\n    saver = tf.train.Saver()\n    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n    saver.restore(sess, args.model)\n    print (\'Loading model weights from {:s}\').format(args.model)\n\n    test_net(sess, network, imdb, weights_filename)\n'"
faster_rcnn/train_net.py,0,"b'\n# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Train a Fast R-CNN network on a region of interest database.""""""\n\nimport argparse\nimport pprint\nimport numpy as np\nimport pdb\nimport sys\nimport os.path\n\nthis_dir = os.path.dirname(__file__)\nsys.path.insert(0, this_dir + \'/..\')\n# for p in sys.path: print p\n# print (this_dir)\n\nfrom lib.fast_rcnn.train import get_training_roidb, train_net\nfrom lib.fast_rcnn.config import cfg, cfg_from_file, cfg_from_list, get_output_dir, get_log_dir\nfrom lib.datasets.factory import get_imdb\nfrom lib.networks.factory import get_network\nfrom lib.fast_rcnn.config import cfg\n\ndef parse_args():\n    """"""\n    Parse input arguments\n    """"""\n    parser = argparse.ArgumentParser(description=\'Train a Fast R-CNN network\')\n    parser.add_argument(\'--gpu\', dest=\'gpu_id\',\n                        help=\'GPU device id to use [0]\',\n                        default=0, type=int)\n    parser.add_argument(\'--solver\', dest=\'solver\',\n                        help=\'solver prototxt\',\n                        default=None, type=str)\n    parser.add_argument(\'--iters\', dest=\'max_iters\',\n                        help=\'number of iterations to train\',\n                        default=70000, type=int)\n    parser.add_argument(\'--weights\', dest=\'pretrained_model\',\n                        help=\'initialize with pretrained model weights\',\n                        default=None, type=str)\n    parser.add_argument(\'--cfg\', dest=\'cfg_file\',\n                        help=\'optional config file\',\n                        default=None, type=str)\n    parser.add_argument(\'--imdb\', dest=\'imdb_name\',\n                        help=\'dataset to train on\',\n                        default=\'kitti_train\', type=str)\n    parser.add_argument(\'--rand\', dest=\'randomize\',\n                        help=\'randomize (do not use a fixed seed)\',\n                        action=\'store_true\')\n    parser.add_argument(\'--network\', dest=\'network_name\',\n                        help=\'name of the network\',\n                        default=None, type=str)\n    parser.add_argument(\'--set\', dest=\'set_cfgs\',\n                        help=\'set config keys\', default=None,\n                        nargs=argparse.REMAINDER)\n    parser.add_argument(\'--restore\', dest=\'restore\',\n                        help=\'restore or not\',\n                        default=1, type=int)\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        # sys.exit(1)\n\n    args = parser.parse_args()\n    return args\n\nif __name__ == \'__main__\':\n    args = parse_args()\n\n    print(\'Called with args:\')\n    print(args)\n\n    if args.cfg_file is not None:\n        cfg_from_file(args.cfg_file)\n    if args.set_cfgs is not None:\n        cfg_from_list(args.set_cfgs)\n\n    print(\'Using config:\')\n    pprint.pprint(cfg)\n\n    if not args.randomize:\n        # fix the random seeds (numpy and caffe) for reproducibility\n        np.random.seed(cfg.RNG_SEED)\n    imdb = get_imdb(args.imdb_name)\n    print \'Loaded dataset `{:s}` for training\'.format(imdb.name)\n    roidb = get_training_roidb(imdb)\n\n    output_dir = get_output_dir(imdb, None)\n    log_dir = get_log_dir(imdb)\n    print \'Output will be saved to `{:s}`\'.format(output_dir)\n    print \'Logs will be saved to `{:s}`\'.format(log_dir)\n\n    device_name = \'/gpu:{:d}\'.format(args.gpu_id)\n    print device_name\n\n    network = get_network(args.network_name)\n    print \'Use network `{:s}` in training\'.format(args.network_name)\n\n    train_net(network, imdb, roidb,\n              output_dir=output_dir,\n              log_dir=log_dir,\n              pretrained_model=args.pretrained_model,\n              max_iters=args.max_iters,\n              restore=bool(int(args.restore)))\n'"
lib/__init__.py,0,b'import fast_rcnn'
lib/setup.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nimport numpy as np\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\n\ndef find_in_path(name, path):\n    ""Find a file in a search path""\n    #adapted fom http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\ndef locate_cuda():\n    """"""Locate the CUDA environment on the system\n\n    Returns a dict with keys \'home\', \'nvcc\', \'include\', and \'lib64\'\n    and values giving the absolute path to each directory.\n\n    Starts by looking for the CUDAHOME env variable. If not found, everything\n    is based on finding \'nvcc\' in the PATH.\n    """"""\n\n    # first check if the CUDAHOME env variable is in use\n    if \'CUDAHOME\' in os.environ:\n        home = os.environ[\'CUDAHOME\']\n        nvcc = pjoin(home, \'bin\', \'nvcc\')\n    else:\n        # otherwise, search the PATH for NVCC\n        default_path = pjoin(os.sep, \'usr\', \'local\', \'cuda\', \'bin\')\n        nvcc = find_in_path(\'nvcc\', os.environ[\'PATH\'] + os.pathsep + default_path)\n        if nvcc is None:\n            raise EnvironmentError(\'The nvcc binary could not be \'\n                \'located in your $PATH. Either add it to your path, or set $CUDAHOME\')\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {\'home\':home, \'nvcc\':nvcc,\n                  \'include\': pjoin(home, \'include\'),\n                  \'lib64\': pjoin(home, \'lib64\')}\n    for k, v in cudaconfig.iteritems():\n        if not os.path.exists(v):\n            raise EnvironmentError(\'The CUDA %s path could not be located in %s\' % (k, v))\n\n    return cudaconfig\nCUDA = locate_cuda()\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        print extra_postargs\n        if os.path.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', CUDA[\'nvcc\'])\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\next_modules = [\n    Extension(\n        ""utils.cython_bbox"",\n        [""utils/bbox.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n\tinclude_dirs = [numpy_include]\n\t),\n    Extension(\n        ""utils.cython_nms"",\n        [""utils/nms.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs = [numpy_include]\n    ),\n    Extension(\n        ""nms.cpu_nms"",\n        [""nms/cpu_nms.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs = [numpy_include]\n    ),\n    Extension(\'nms.gpu_nms\',\n        [\'nms/nms_kernel.cu\', \'nms/gpu_nms.pyx\'],\n        library_dirs=[CUDA[\'lib64\']],\n        libraries=[\'cudart\'],\n        language=\'c++\',\n        runtime_library_dirs=[CUDA[\'lib64\']],\n        # this syntax is specific to this build system\n        # we\'re only going to use certain compiler args with nvcc and not with gcc\n        # the implementation of this trick is in customize_compiler() below\n        extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                            \'nvcc\': [\'-arch=sm_35\',\n                                     \'--ptxas-options=-v\',\n                                     \'-c\',\n                                     \'--compiler-options\',\n                                     ""\'-fPIC\'""]},\n        include_dirs = [numpy_include, CUDA[\'include\']]\n    ),\n    Extension(\n        \'pycocotools._mask\',\n        sources=[\'pycocotools/maskApi.c\', \'pycocotools/_mask.pyx\'],\n        include_dirs = [numpy_include, \'pycocotools\'],\n        extra_compile_args={\n            \'gcc\': [\'-Wno-cpp\', \'-Wno-unused-function\', \'-std=c99\']},\n    ),\n]\n\nsetup(\n    name=\'fast_rcnn\',\n    ext_modules=ext_modules,\n    # inject our custom trigger\n    cmdclass={\'build_ext\': custom_build_ext},\n)\n'"
experiments/profiling/gprof2dot.py,0,"b'#!/usr/bin/env python\n#\n# Copyright 2008-2014 Jose Fonseca\n#\n# This program is free software: you can redistribute it and/or modify it\n# under the terms of the GNU Lesser General Public License as published\n# by the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n\n""""""Generate a dot graph from the output of several profilers.""""""\n\n__author__ = ""Jose Fonseca et al""\n\n\nimport sys\nimport math\nimport os.path\nimport re\nimport textwrap\nimport optparse\nimport xml.parsers.expat\nimport collections\nimport locale\nimport json\n\n\n# Python 2.x/3.x compatibility\nif sys.version_info[0] >= 3:\n    PYTHON_3 = True\n    def compat_iteritems(x): return x.items()  # No iteritems() in Python 3\n    def compat_itervalues(x): return x.values()  # No itervalues() in Python 3\n    def compat_keys(x): return list(x.keys())  # keys() is a generator in Python 3\n    basestring = str  # No class basestring in Python 3\n    unichr = chr # No unichr in Python 3\n    xrange = range # No xrange in Python 3\nelse:\n    PYTHON_3 = False\n    def compat_iteritems(x): return x.iteritems()\n    def compat_itervalues(x): return x.itervalues()\n    def compat_keys(x): return x.keys()\n\n\ntry:\n    # Debugging helper module\n    import debug\nexcept ImportError:\n    pass\n\n\n\n########################################################################\n# Model\n\n\nMULTIPLICATION_SIGN = unichr(0xd7)\n\n\ndef times(x):\n    return ""%u%s"" % (x, MULTIPLICATION_SIGN)\n\ndef percentage(p):\n    return ""%.02f%%"" % (p*100.0,)\n\ndef add(a, b):\n    return a + b\n\ndef fail(a, b):\n    assert False\n\n\ntol = 2 ** -23\n\ndef ratio(numerator, denominator):\n    try:\n        ratio = float(numerator)/float(denominator)\n    except ZeroDivisionError:\n        # 0/0 is undefined, but 1.0 yields more useful results\n        return 1.0\n    if ratio < 0.0:\n        if ratio < -tol:\n            sys.stderr.write(\'warning: negative ratio (%s/%s)\\n\' % (numerator, denominator))\n        return 0.0\n    if ratio > 1.0:\n        if ratio > 1.0 + tol:\n            sys.stderr.write(\'warning: ratio greater than one (%s/%s)\\n\' % (numerator, denominator))\n        return 1.0\n    return ratio\n\n\nclass UndefinedEvent(Exception):\n    """"""Raised when attempting to get an event which is undefined.""""""\n    \n    def __init__(self, event):\n        Exception.__init__(self)\n        self.event = event\n\n    def __str__(self):\n        return \'unspecified event %s\' % self.event.name\n\n\nclass Event(object):\n    """"""Describe a kind of event, and its basic operations.""""""\n\n    def __init__(self, name, null, aggregator, formatter = str):\n        self.name = name\n        self._null = null\n        self._aggregator = aggregator\n        self._formatter = formatter\n\n    def __eq__(self, other):\n        return self is other\n\n    def __hash__(self):\n        return id(self)\n\n    def null(self):\n        return self._null\n\n    def aggregate(self, val1, val2):\n        """"""Aggregate two event values.""""""\n        assert val1 is not None\n        assert val2 is not None\n        return self._aggregator(val1, val2)\n    \n    def format(self, val):\n        """"""Format an event value.""""""\n        assert val is not None\n        return self._formatter(val)\n\n\nCALLS = Event(""Calls"", 0, add, times)\nSAMPLES = Event(""Samples"", 0, add, times)\nSAMPLES2 = Event(""Samples"", 0, add, times)\n\n# Count of samples where a given function was either executing or on the stack.\n# This is used to calculate the total time ratio according to the\n# straightforward method described in Mike Dunlavey\'s answer to\n# stackoverflow.com/questions/1777556/alternatives-to-gprof, item 4 (the myth\n# ""that recursion is a tricky confusing issue""), last edited 2012-08-30: it\'s\n# just the ratio of TOTAL_SAMPLES over the number of samples in the profile.\n#\n# Used only when totalMethod == callstacks\nTOTAL_SAMPLES = Event(""Samples"", 0, add, times)\n\nTIME = Event(""Time"", 0.0, add, lambda x: \'(\' + str(x) + \')\')\nTIME_RATIO = Event(""Time ratio"", 0.0, add, lambda x: \'(\' + percentage(x) + \')\')\nTOTAL_TIME = Event(""Total time"", 0.0, fail)\nTOTAL_TIME_RATIO = Event(""Total time ratio"", 0.0, fail, percentage)\n\ntotalMethod = \'callratios\'\n\n\nclass Object(object):\n    """"""Base class for all objects in profile which can store events.""""""\n\n    def __init__(self, events=None):\n        if events is None:\n            self.events = {}\n        else:\n            self.events = events\n\n    def __hash__(self):\n        return id(self)\n\n    def __eq__(self, other):\n        return self is other\n\n    def __lt__(self, other):\n        return id(self) < id(other)\n\n    def __contains__(self, event):\n        return event in self.events\n    \n    def __getitem__(self, event):\n        try:\n            return self.events[event]\n        except KeyError:\n            raise UndefinedEvent(event)\n    \n    def __setitem__(self, event, value):\n        if value is None:\n            if event in self.events:\n                del self.events[event]\n        else:\n            self.events[event] = value\n\n\nclass Call(Object):\n    """"""A call between functions.\n    \n    There should be at most one call object for every pair of functions.\n    """"""\n\n    def __init__(self, callee_id):\n        Object.__init__(self)\n        self.callee_id = callee_id\n        self.ratio = None\n        self.weight = None\n\n\nclass Function(Object):\n    """"""A function.""""""\n\n    def __init__(self, id, name):\n        Object.__init__(self)\n        self.id = id\n        self.name = name\n        self.module = None\n        self.process = None\n        self.calls = {}\n        self.called = None\n        self.weight = None\n        self.cycle = None\n        self.filename = None\n    \n    def add_call(self, call):\n        if call.callee_id in self.calls:\n            sys.stderr.write(\'warning: overwriting call from function %s to %s\\n\' % (str(self.id), str(call.callee_id)))\n        self.calls[call.callee_id] = call\n\n    def get_call(self, callee_id):\n        if not callee_id in self.calls:\n            call = Call(callee_id)\n            call[SAMPLES] = 0\n            call[SAMPLES2] = 0\n            call[CALLS] = 0\n            self.calls[callee_id] = call\n        return self.calls[callee_id]\n\n    _parenthesis_re = re.compile(r\'\\([^()]*\\)\')\n    _angles_re = re.compile(r\'<[^<>]*>\')\n    _const_re = re.compile(r\'\\s+const$\')\n\n    def stripped_name(self):\n        """"""Remove extraneous information from C++ demangled function names.""""""\n\n        name = self.name\n\n        # Strip function parameters from name by recursively removing paired parenthesis\n        while True:\n            name, n = self._parenthesis_re.subn(\'\', name)\n            if not n:\n                break\n\n        # Strip const qualifier\n        name = self._const_re.sub(\'\', name)\n\n        # Strip template parameters from name by recursively removing paired angles\n        while True:\n            name, n = self._angles_re.subn(\'\', name)\n            if not n:\n                break\n\n        return name\n\n    # TODO: write utility functions\n\n    def __repr__(self):\n        return self.name\n\n\nclass Cycle(Object):\n    """"""A cycle made from recursive function calls.""""""\n\n    def __init__(self):\n        Object.__init__(self)\n        self.functions = set()\n\n    def add_function(self, function):\n        assert function not in self.functions\n        self.functions.add(function)\n        if function.cycle is not None:\n            for other in function.cycle.functions:\n                if function not in self.functions:\n                    self.add_function(other)\n        function.cycle = self\n\n\nclass Profile(Object):\n    """"""The whole profile.""""""\n\n    def __init__(self):\n        Object.__init__(self)\n        self.functions = {}\n        self.cycles = []\n\n    def add_function(self, function):\n        if function.id in self.functions:\n            sys.stderr.write(\'warning: overwriting function %s (id %s)\\n\' % (function.name, str(function.id)))\n        self.functions[function.id] = function\n\n    def add_cycle(self, cycle):\n        self.cycles.append(cycle)\n\n    def validate(self):\n        """"""Validate the edges.""""""\n\n        for function in compat_itervalues(self.functions):\n            for callee_id in compat_keys(function.calls):\n                assert function.calls[callee_id].callee_id == callee_id\n                if callee_id not in self.functions:\n                    sys.stderr.write(\'warning: call to undefined function %s from function %s\\n\' % (str(callee_id), function.name))\n                    del function.calls[callee_id]\n\n    def find_cycles(self):\n        """"""Find cycles using Tarjan\'s strongly connected components algorithm.""""""\n\n        # Apply the Tarjan\'s algorithm successively until all functions are visited\n        stack = []\n        data = {}\n        order = 0\n        for function in compat_itervalues(self.functions):\n            order = self._tarjan(function, order, stack, data)\n        cycles = []\n        for function in compat_itervalues(self.functions):\n            if function.cycle is not None and function.cycle not in cycles:\n                cycles.append(function.cycle)\n        self.cycles = cycles\n        if 0:\n            for cycle in cycles:\n                sys.stderr.write(""Cycle:\\n"")\n                for member in cycle.functions:\n                    sys.stderr.write(""\\tFunction %s\\n"" % member.name)\n\n    def prune_root(self, root):\n        visited = set()\n        frontier = set([root])\n        while len(frontier) > 0:\n            node = frontier.pop()\n            visited.add(node)\n            f = self.functions[node]\n            newNodes = f.calls.keys()\n            frontier = frontier.union(set(newNodes) - visited)\n        subtreeFunctions = {}\n        for n in visited:\n            subtreeFunctions[n] = self.functions[n]\n        self.functions = subtreeFunctions\n\n    def prune_leaf(self, leaf):\n        edgesUp = collections.defaultdict(set)\n        for f in self.functions.keys():\n            for n in self.functions[f].calls.keys():\n                edgesUp[n].add(f)\n        # build the tree up\n        visited = set()\n        frontier = set([leaf])\n        while len(frontier) > 0:\n            node = frontier.pop()\n            visited.add(node)\n            frontier = frontier.union(edgesUp[node] - visited)\n        downTree = set(self.functions.keys())\n        upTree = visited\n        path = downTree.intersection(upTree)\n        pathFunctions = {}\n        for n in path:\n            f = self.functions[n]\n            newCalls = {}\n            for c in f.calls.keys():\n                if c in path:\n                    newCalls[c] = f.calls[c]\n            f.calls = newCalls\n            pathFunctions[n] = f\n        self.functions = pathFunctions\n\n\n    def getFunctionId(self, funcName):\n        for f in self.functions:\n            if self.functions[f].name == funcName:\n                return f\n        return False\n\n    class _TarjanData:\n        def __init__(self, order):\n            self.order = order\n            self.lowlink = order\n            self.onstack = False\n\n    def _tarjan(self, function, order, stack, data):\n        """"""Tarjan\'s strongly connected components algorithm.\n\n        See also:\n        - http://en.wikipedia.org/wiki/Tarjan\'s_strongly_connected_components_algorithm\n        """"""\n\n        try:\n            func_data = data[function.id]\n            return order\n        except KeyError:\n            func_data = self._TarjanData(order)\n            data[function.id] = func_data\n        order += 1\n        pos = len(stack)\n        stack.append(function)\n        func_data.onstack = True\n        for call in compat_itervalues(function.calls):\n            try:\n                callee_data = data[call.callee_id]\n                if callee_data.onstack:\n                    func_data.lowlink = min(func_data.lowlink, callee_data.order)\n            except KeyError:\n                callee = self.functions[call.callee_id]\n                order = self._tarjan(callee, order, stack, data)\n                callee_data = data[call.callee_id]\n                func_data.lowlink = min(func_data.lowlink, callee_data.lowlink)\n        if func_data.lowlink == func_data.order:\n            # Strongly connected component found\n            members = stack[pos:]\n            del stack[pos:]\n            if len(members) > 1:\n                cycle = Cycle()\n                for member in members:\n                    cycle.add_function(member)\n                    data[member.id].onstack = False\n            else:\n                for member in members:\n                    data[member.id].onstack = False\n        return order\n\n    def call_ratios(self, event):\n        # Aggregate for incoming calls\n        cycle_totals = {}\n        for cycle in self.cycles:\n            cycle_totals[cycle] = 0.0\n        function_totals = {}\n        for function in compat_itervalues(self.functions):\n            function_totals[function] = 0.0\n\n        # Pass 1:  function_total gets the sum of call[event] for all\n        #          incoming arrows.  Same for cycle_total for all arrows\n        #          that are coming into the *cycle* but are not part of it.\n        for function in compat_itervalues(self.functions):\n            for call in compat_itervalues(function.calls):\n                if call.callee_id != function.id:\n                    callee = self.functions[call.callee_id]\n                    if event in call.events:\n                        function_totals[callee] += call[event]\n                        if callee.cycle is not None and callee.cycle is not function.cycle:\n                            cycle_totals[callee.cycle] += call[event]\n                    else:\n                        sys.stderr.write(""call_ratios: No data for "" + function.name + "" call to "" + callee.name + ""\\n"")\n\n        # Pass 2:  Compute the ratios.  Each call[event] is scaled by the\n        #          function_total of the callee.  Calls into cycles use the\n        #          cycle_total, but not calls within cycles.\n        for function in compat_itervalues(self.functions):\n            for call in compat_itervalues(function.calls):\n                assert call.ratio is None\n                if call.callee_id != function.id:\n                    callee = self.functions[call.callee_id]\n                    if event in call.events:\n                        if callee.cycle is not None and callee.cycle is not function.cycle:\n                            total = cycle_totals[callee.cycle]\n                        else:\n                            total = function_totals[callee]\n                        call.ratio = ratio(call[event], total)\n                    else:\n                        # Warnings here would only repeat those issued above.\n                        call.ratio = 0.0\n\n    def integrate(self, outevent, inevent):\n        """"""Propagate function time ratio along the function calls.\n\n        Must be called after finding the cycles.\n\n        See also:\n        - http://citeseer.ist.psu.edu/graham82gprof.html\n        """"""\n\n        # Sanity checking\n        assert outevent not in self\n        for function in compat_itervalues(self.functions):\n            assert outevent not in function\n            assert inevent in function\n            for call in compat_itervalues(function.calls):\n                assert outevent not in call\n                if call.callee_id != function.id:\n                    assert call.ratio is not None\n\n        # Aggregate the input for each cycle \n        for cycle in self.cycles:\n            total = inevent.null()\n            for function in compat_itervalues(self.functions):\n                total = inevent.aggregate(total, function[inevent])\n            self[inevent] = total\n\n        # Integrate along the edges\n        total = inevent.null()\n        for function in compat_itervalues(self.functions):\n            total = inevent.aggregate(total, function[inevent])\n            self._integrate_function(function, outevent, inevent)\n        self[outevent] = total\n\n    def _integrate_function(self, function, outevent, inevent):\n        if function.cycle is not None:\n            return self._integrate_cycle(function.cycle, outevent, inevent)\n        else:\n            if outevent not in function:\n                total = function[inevent]\n                for call in compat_itervalues(function.calls):\n                    if call.callee_id != function.id:\n                        total += self._integrate_call(call, outevent, inevent)\n                function[outevent] = total\n            return function[outevent]\n    \n    def _integrate_call(self, call, outevent, inevent):\n        assert outevent not in call\n        assert call.ratio is not None\n        callee = self.functions[call.callee_id]\n        subtotal = call.ratio *self._integrate_function(callee, outevent, inevent)\n        call[outevent] = subtotal\n        return subtotal\n\n    def _integrate_cycle(self, cycle, outevent, inevent):\n        if outevent not in cycle:\n\n            # Compute the outevent for the whole cycle\n            total = inevent.null()\n            for member in cycle.functions:\n                subtotal = member[inevent]\n                for call in compat_itervalues(member.calls):\n                    callee = self.functions[call.callee_id]\n                    if callee.cycle is not cycle:\n                        subtotal += self._integrate_call(call, outevent, inevent)\n                total += subtotal\n            cycle[outevent] = total\n            \n            # Compute the time propagated to callers of this cycle\n            callees = {}\n            for function in compat_itervalues(self.functions):\n                if function.cycle is not cycle:\n                    for call in compat_itervalues(function.calls):\n                        callee = self.functions[call.callee_id]\n                        if callee.cycle is cycle:\n                            try:\n                                callees[callee] += call.ratio\n                            except KeyError:\n                                callees[callee] = call.ratio\n            \n            for member in cycle.functions:\n                member[outevent] = outevent.null()\n\n            for callee, call_ratio in compat_iteritems(callees):\n                ranks = {}\n                call_ratios = {}\n                partials = {}\n                self._rank_cycle_function(cycle, callee, ranks)\n                self._call_ratios_cycle(cycle, callee, ranks, call_ratios, set())\n                partial = self._integrate_cycle_function(cycle, callee, call_ratio, partials, ranks, call_ratios, outevent, inevent)\n                assert partial == max(partials.values())\n                assert abs(call_ratio*total - partial) <= 0.001*call_ratio*total\n\n        return cycle[outevent]\n\n    def _rank_cycle_function(self, cycle, function, ranks):\n        """"""Dijkstra\'s shortest paths algorithm.\n\n        See also:\n        - http://en.wikipedia.org/wiki/Dijkstra\'s_algorithm\n        """"""\n\n        import heapq\n        Q = []\n        Qd = {}\n        p = {}\n        visited = set([function])\n\n        ranks[function] = 0\n        for call in compat_itervalues(function.calls):\n            if call.callee_id != function.id:\n                callee = self.functions[call.callee_id]\n                if callee.cycle is cycle:\n                    ranks[callee] = 1\n                    item = [ranks[callee], function, callee]\n                    heapq.heappush(Q, item)\n                    Qd[callee] = item\n\n        while Q:\n            cost, parent, member = heapq.heappop(Q)\n            if member not in visited:\n                p[member]= parent\n                visited.add(member)\n                for call in compat_itervalues(member.calls):\n                    if call.callee_id != member.id:\n                        callee = self.functions[call.callee_id]\n                        if callee.cycle is cycle:\n                            member_rank = ranks[member]\n                            rank = ranks.get(callee)\n                            if rank is not None:\n                                if rank > 1 + member_rank:\n                                    rank = 1 + member_rank\n                                    ranks[callee] = rank\n                                    Qd_callee = Qd[callee]\n                                    Qd_callee[0] = rank\n                                    Qd_callee[1] = member\n                                    heapq._siftdown(Q, 0, Q.index(Qd_callee))\n                            else:\n                                rank = 1 + member_rank\n                                ranks[callee] = rank\n                                item = [rank, member, callee]\n                                heapq.heappush(Q, item)\n                                Qd[callee] = item\n\n    def _call_ratios_cycle(self, cycle, function, ranks, call_ratios, visited):\n        if function not in visited:\n            visited.add(function)\n            for call in compat_itervalues(function.calls):\n                if call.callee_id != function.id:\n                    callee = self.functions[call.callee_id]\n                    if callee.cycle is cycle:\n                        if ranks[callee] > ranks[function]:\n                            call_ratios[callee] = call_ratios.get(callee, 0.0) + call.ratio\n                            self._call_ratios_cycle(cycle, callee, ranks, call_ratios, visited)\n\n    def _integrate_cycle_function(self, cycle, function, partial_ratio, partials, ranks, call_ratios, outevent, inevent):\n        if function not in partials:\n            partial = partial_ratio*function[inevent]\n            for call in compat_itervalues(function.calls):\n                if call.callee_id != function.id:\n                    callee = self.functions[call.callee_id]\n                    if callee.cycle is not cycle:\n                        assert outevent in call\n                        partial += partial_ratio*call[outevent]\n                    else:\n                        if ranks[callee] > ranks[function]:\n                            callee_partial = self._integrate_cycle_function(cycle, callee, partial_ratio, partials, ranks, call_ratios, outevent, inevent)\n                            call_ratio = ratio(call.ratio, call_ratios[callee])\n                            call_partial = call_ratio*callee_partial\n                            try:\n                                call[outevent] += call_partial\n                            except UndefinedEvent:\n                                call[outevent] = call_partial\n                            partial += call_partial\n            partials[function] = partial\n            try:\n                function[outevent] += partial\n            except UndefinedEvent:\n                function[outevent] = partial\n        return partials[function]\n\n    def aggregate(self, event):\n        """"""Aggregate an event for the whole profile.""""""\n\n        total = event.null()\n        for function in compat_itervalues(self.functions):\n            try:\n                total = event.aggregate(total, function[event])\n            except UndefinedEvent:\n                return\n        self[event] = total\n\n    def ratio(self, outevent, inevent):\n        assert outevent not in self\n        assert inevent in self\n        for function in compat_itervalues(self.functions):\n            assert outevent not in function\n            assert inevent in function\n            function[outevent] = ratio(function[inevent], self[inevent])\n            for call in compat_itervalues(function.calls):\n                assert outevent not in call\n                if inevent in call:\n                    call[outevent] = ratio(call[inevent], self[inevent])\n        self[outevent] = 1.0\n\n    def prune(self, node_thres, edge_thres, colour_nodes_by_selftime):\n        """"""Prune the profile""""""\n\n        # compute the prune ratios\n        for function in compat_itervalues(self.functions):\n            try:\n                function.weight = function[TOTAL_TIME_RATIO]\n            except UndefinedEvent:\n                pass\n\n            for call in compat_itervalues(function.calls):\n                callee = self.functions[call.callee_id]\n\n                if TOTAL_TIME_RATIO in call:\n                    # handle exact cases first\n                    call.weight = call[TOTAL_TIME_RATIO] \n                else:\n                    try:\n                        # make a safe estimate\n                        call.weight = min(function[TOTAL_TIME_RATIO], callee[TOTAL_TIME_RATIO]) \n                    except UndefinedEvent:\n                        pass\n\n        # prune the nodes\n        for function_id in compat_keys(self.functions):\n            function = self.functions[function_id]\n            if function.weight is not None:\n                if function.weight < node_thres:\n                    del self.functions[function_id]\n\n        # prune the egdes\n        for function in compat_itervalues(self.functions):\n            for callee_id in compat_keys(function.calls):\n                call = function.calls[callee_id]\n                if callee_id not in self.functions or call.weight is not None and call.weight < edge_thres:\n                    del function.calls[callee_id]\n\n        if colour_nodes_by_selftime:\n            weights = []\n            for function in compat_itervalues(self.functions):\n                try:\n                    weights.append(function[TIME_RATIO])\n                except UndefinedEvent:\n                    pass\n            max_ratio = max(weights or [1])\n\n            # apply rescaled weights for coloriung\n            for function in compat_itervalues(self.functions):\n                try:\n                    function.weight = function[TIME_RATIO] / max_ratio\n                except (ZeroDivisionError, UndefinedEvent):\n                    pass\n    \n    def dump(self):\n        for function in compat_itervalues(self.functions):\n            sys.stderr.write(\'Function %s:\\n\' % (function.name,))\n            self._dump_events(function.events)\n            for call in compat_itervalues(function.calls):\n                callee = self.functions[call.callee_id]\n                sys.stderr.write(\'  Call %s:\\n\' % (callee.name,))\n                self._dump_events(call.events)\n        for cycle in self.cycles:\n            sys.stderr.write(\'Cycle:\\n\')\n            self._dump_events(cycle.events)\n            for function in cycle.functions:\n                sys.stderr.write(\'  Function %s\\n\' % (function.name,))\n\n    def _dump_events(self, events):\n        for event, value in compat_iteritems(events):\n            sys.stderr.write(\'    %s: %s\\n\' % (event.name, event.format(value)))\n\n\n\n########################################################################\n# Parsers\n\n\nclass Struct:\n    """"""Masquerade a dictionary with a structure-like behavior.""""""\n\n    def __init__(self, attrs = None):\n        if attrs is None:\n            attrs = {}\n        self.__dict__[\'_attrs\'] = attrs\n    \n    def __getattr__(self, name):\n        try:\n            return self._attrs[name]\n        except KeyError:\n            raise AttributeError(name)\n\n    def __setattr__(self, name, value):\n        self._attrs[name] = value\n\n    def __str__(self):\n        return str(self._attrs)\n\n    def __repr__(self):\n        return repr(self._attrs)\n    \n\nclass ParseError(Exception):\n    """"""Raised when parsing to signal mismatches.""""""\n\n    def __init__(self, msg, line):\n        Exception.__init__(self)\n        self.msg = msg\n        # TODO: store more source line information\n        self.line = line\n\n    def __str__(self):\n        return \'%s: %r\' % (self.msg, self.line)\n\n\nclass Parser:\n    """"""Parser interface.""""""\n\n    stdinInput = True\n    multipleInput = False\n\n    def __init__(self):\n        pass\n\n    def parse(self):\n        raise NotImplementedError\n\n    \nclass JsonParser(Parser):\n    """"""Parser for a custom JSON representation of profile data.\n\n    See schema.json for details.\n    """"""\n\n\n    def __init__(self, stream):\n        Parser.__init__(self)\n        self.stream = stream\n\n    def parse(self):\n\n        obj = json.load(self.stream)\n\n        assert obj[\'version\'] == 0\n\n        profile = Profile()\n        profile[SAMPLES] = 0\n\n        fns = obj[\'functions\']\n\n        for functionIndex in range(len(fns)):\n            fn = fns[functionIndex]\n            function = Function(functionIndex, fn[\'name\'])\n            try:\n                function.module = fn[\'module\']\n            except KeyError:\n                pass\n            try:\n                function.process = fn[\'process\']\n            except KeyError:\n                pass\n            function[SAMPLES] = 0\n            profile.add_function(function)\n\n        for event in obj[\'events\']:\n            callchain = []\n\n            for functionIndex in event[\'callchain\']:\n                function = profile.functions[functionIndex]\n                callchain.append(function)\n\n            cost = event[\'cost\'][0]\n\n            callee = callchain[0]\n            callee[SAMPLES] += cost\n            profile[SAMPLES] += cost\n\n            for caller in callchain[1:]:\n                try:\n                    call = caller.calls[callee.id]\n                except KeyError:\n                    call = Call(callee.id)\n                    call[SAMPLES2] = cost\n                    caller.add_call(call)\n                else:\n                    call[SAMPLES2] += cost\n\n                callee = caller\n\n        if False:\n            profile.dump()\n\n        # compute derived data\n        profile.validate()\n        profile.find_cycles()\n        profile.ratio(TIME_RATIO, SAMPLES)\n        profile.call_ratios(SAMPLES2)\n        profile.integrate(TOTAL_TIME_RATIO, TIME_RATIO)\n\n        return profile\n\n\nclass LineParser(Parser):\n    """"""Base class for parsers that read line-based formats.""""""\n\n    def __init__(self, stream):\n        Parser.__init__(self)\n        self._stream = stream\n        self.__line = None\n        self.__eof = False\n        self.line_no = 0\n\n    def readline(self):\n        line = self._stream.readline()\n        if not line:\n            self.__line = \'\'\n            self.__eof = True\n        else:\n            self.line_no += 1\n        line = line.rstrip(\'\\r\\n\')\n        if not PYTHON_3:\n            encoding = self._stream.encoding\n            if encoding is None:\n                encoding = locale.getpreferredencoding()\n            line = line.decode(encoding)\n        self.__line = line\n\n    def lookahead(self):\n        assert self.__line is not None\n        return self.__line\n\n    def consume(self):\n        assert self.__line is not None\n        line = self.__line\n        self.readline()\n        return line\n\n    def eof(self):\n        assert self.__line is not None\n        return self.__eof\n\n\nXML_ELEMENT_START, XML_ELEMENT_END, XML_CHARACTER_DATA, XML_EOF = range(4)\n\n\nclass XmlToken:\n\n    def __init__(self, type, name_or_data, attrs = None, line = None, column = None):\n        assert type in (XML_ELEMENT_START, XML_ELEMENT_END, XML_CHARACTER_DATA, XML_EOF)\n        self.type = type\n        self.name_or_data = name_or_data\n        self.attrs = attrs\n        self.line = line\n        self.column = column\n\n    def __str__(self):\n        if self.type == XML_ELEMENT_START:\n            return \'<\' + self.name_or_data + \' ...>\'\n        if self.type == XML_ELEMENT_END:\n            return \'</\' + self.name_or_data + \'>\'\n        if self.type == XML_CHARACTER_DATA:\n            return self.name_or_data\n        if self.type == XML_EOF:\n            return \'end of file\'\n        assert 0\n\n\nclass XmlTokenizer:\n    """"""Expat based XML tokenizer.""""""\n\n    def __init__(self, fp, skip_ws = True):\n        self.fp = fp\n        self.tokens = []\n        self.index = 0\n        self.final = False\n        self.skip_ws = skip_ws\n        \n        self.character_pos = 0, 0\n        self.character_data = \'\'\n        \n        self.parser = xml.parsers.expat.ParserCreate()\n        self.parser.StartElementHandler  = self.handle_element_start\n        self.parser.EndElementHandler    = self.handle_element_end\n        self.parser.CharacterDataHandler = self.handle_character_data\n    \n    def handle_element_start(self, name, attributes):\n        self.finish_character_data()\n        line, column = self.pos()\n        token = XmlToken(XML_ELEMENT_START, name, attributes, line, column)\n        self.tokens.append(token)\n    \n    def handle_element_end(self, name):\n        self.finish_character_data()\n        line, column = self.pos()\n        token = XmlToken(XML_ELEMENT_END, name, None, line, column)\n        self.tokens.append(token)\n\n    def handle_character_data(self, data):\n        if not self.character_data:\n            self.character_pos = self.pos()\n        self.character_data += data\n    \n    def finish_character_data(self):\n        if self.character_data:\n            if not self.skip_ws or not self.character_data.isspace(): \n                line, column = self.character_pos\n                token = XmlToken(XML_CHARACTER_DATA, self.character_data, None, line, column)\n                self.tokens.append(token)\n            self.character_data = \'\'\n    \n    def next(self):\n        size = 16*1024\n        while self.index >= len(self.tokens) and not self.final:\n            self.tokens = []\n            self.index = 0\n            data = self.fp.read(size)\n            self.final = len(data) < size\n            self.parser.Parse(data, self.final)\n        if self.index >= len(self.tokens):\n            line, column = self.pos()\n            token = XmlToken(XML_EOF, None, None, line, column)\n        else:\n            token = self.tokens[self.index]\n            self.index += 1\n        return token\n\n    def pos(self):\n        return self.parser.CurrentLineNumber, self.parser.CurrentColumnNumber\n\n\nclass XmlTokenMismatch(Exception):\n\n    def __init__(self, expected, found):\n        Exception.__init__(self)\n        self.expected = expected\n        self.found = found\n\n    def __str__(self):\n        return \'%u:%u: %s expected, %s found\' % (self.found.line, self.found.column, str(self.expected), str(self.found))\n\n\nclass XmlParser(Parser):\n    """"""Base XML document parser.""""""\n\n    def __init__(self, fp):\n        Parser.__init__(self)\n        self.tokenizer = XmlTokenizer(fp)\n        self.consume()\n    \n    def consume(self):\n        self.token = self.tokenizer.next()\n\n    def match_element_start(self, name):\n        return self.token.type == XML_ELEMENT_START and self.token.name_or_data == name\n    \n    def match_element_end(self, name):\n        return self.token.type == XML_ELEMENT_END and self.token.name_or_data == name\n\n    def element_start(self, name):\n        while self.token.type == XML_CHARACTER_DATA:\n            self.consume()\n        if self.token.type != XML_ELEMENT_START:\n            raise XmlTokenMismatch(XmlToken(XML_ELEMENT_START, name), self.token)\n        if self.token.name_or_data != name:\n            raise XmlTokenMismatch(XmlToken(XML_ELEMENT_START, name), self.token)\n        attrs = self.token.attrs\n        self.consume()\n        return attrs\n    \n    def element_end(self, name):\n        while self.token.type == XML_CHARACTER_DATA:\n            self.consume()\n        if self.token.type != XML_ELEMENT_END:\n            raise XmlTokenMismatch(XmlToken(XML_ELEMENT_END, name), self.token)\n        if self.token.name_or_data != name:\n            raise XmlTokenMismatch(XmlToken(XML_ELEMENT_END, name), self.token)\n        self.consume()\n\n    def character_data(self, strip = True):\n        data = \'\'\n        while self.token.type == XML_CHARACTER_DATA:\n            data += self.token.name_or_data\n            self.consume()\n        if strip:\n            data = data.strip()\n        return data\n\n\nclass GprofParser(Parser):\n    """"""Parser for GNU gprof output.\n\n    See also:\n    - Chapter ""Interpreting gprof\'s Output"" from the GNU gprof manual\n      http://sourceware.org/binutils/docs-2.18/gprof/Call-Graph.html#Call-Graph\n    - File ""cg_print.c"" from the GNU gprof source code\n      http://sourceware.org/cgi-bin/cvsweb.cgi/~checkout~/src/gprof/cg_print.c?rev=1.12&cvsroot=src\n    """"""\n\n    def __init__(self, fp):\n        Parser.__init__(self)\n        self.fp = fp\n        self.functions = {}\n        self.cycles = {}\n\n    def readline(self):\n        line = self.fp.readline()\n        if not line:\n            sys.stderr.write(\'error: unexpected end of file\\n\')\n            sys.exit(1)\n        line = line.rstrip(\'\\r\\n\')\n        return line\n\n    _int_re = re.compile(r\'^\\d+$\')\n    _float_re = re.compile(r\'^\\d+\\.\\d+$\')\n\n    def translate(self, mo):\n        """"""Extract a structure from a match object, while translating the types in the process.""""""\n        attrs = {}\n        groupdict = mo.groupdict()\n        for name, value in compat_iteritems(groupdict):\n            if value is None:\n                value = None\n            elif self._int_re.match(value):\n                value = int(value)\n            elif self._float_re.match(value):\n                value = float(value)\n            attrs[name] = (value)\n        return Struct(attrs)\n\n    _cg_header_re = re.compile(\n        # original gprof header\n        r\'^\\s+called/total\\s+parents\\s*$|\' +\n        r\'^index\\s+%time\\s+self\\s+descendents\\s+called\\+self\\s+name\\s+index\\s*$|\' +\n        r\'^\\s+called/total\\s+children\\s*$|\' +\n        # GNU gprof header\n        r\'^index\\s+%\\s+time\\s+self\\s+children\\s+called\\s+name\\s*$\'\n    )\n\n    _cg_ignore_re = re.compile(\n        # spontaneous\n        r\'^\\s+<spontaneous>\\s*$|\'\n        # internal calls (such as ""mcount"")\n        r\'^.*\\((\\d+)\\)$\'\n    )\n\n    _cg_primary_re = re.compile(\n        r\'^\\[(?P<index>\\d+)\\]?\' + \n        r\'\\s+(?P<percentage_time>\\d+\\.\\d+)\' + \n        r\'\\s+(?P<self>\\d+\\.\\d+)\' + \n        r\'\\s+(?P<descendants>\\d+\\.\\d+)\' + \n        r\'\\s+(?:(?P<called>\\d+)(?:\\+(?P<called_self>\\d+))?)?\' + \n        r\'\\s+(?P<name>\\S.*?)\' +\n        r\'(?:\\s+<cycle\\s(?P<cycle>\\d+)>)?\' +\n        r\'\\s\\[(\\d+)\\]$\'\n    )\n\n    _cg_parent_re = re.compile(\n        r\'^\\s+(?P<self>\\d+\\.\\d+)?\' + \n        r\'\\s+(?P<descendants>\\d+\\.\\d+)?\' + \n        r\'\\s+(?P<called>\\d+)(?:/(?P<called_total>\\d+))?\' + \n        r\'\\s+(?P<name>\\S.*?)\' +\n        r\'(?:\\s+<cycle\\s(?P<cycle>\\d+)>)?\' +\n        r\'\\s\\[(?P<index>\\d+)\\]$\'\n    )\n\n    _cg_child_re = _cg_parent_re\n\n    _cg_cycle_header_re = re.compile(\n        r\'^\\[(?P<index>\\d+)\\]?\' + \n        r\'\\s+(?P<percentage_time>\\d+\\.\\d+)\' + \n        r\'\\s+(?P<self>\\d+\\.\\d+)\' + \n        r\'\\s+(?P<descendants>\\d+\\.\\d+)\' + \n        r\'\\s+(?:(?P<called>\\d+)(?:\\+(?P<called_self>\\d+))?)?\' + \n        r\'\\s+<cycle\\s(?P<cycle>\\d+)\\sas\\sa\\swhole>\' +\n        r\'\\s\\[(\\d+)\\]$\'\n    )\n\n    _cg_cycle_member_re = re.compile(\n        r\'^\\s+(?P<self>\\d+\\.\\d+)?\' + \n        r\'\\s+(?P<descendants>\\d+\\.\\d+)?\' + \n        r\'\\s+(?P<called>\\d+)(?:\\+(?P<called_self>\\d+))?\' + \n        r\'\\s+(?P<name>\\S.*?)\' +\n        r\'(?:\\s+<cycle\\s(?P<cycle>\\d+)>)?\' +\n        r\'\\s\\[(?P<index>\\d+)\\]$\'\n    )\n\n    _cg_sep_re = re.compile(r\'^--+$\')\n\n    def parse_function_entry(self, lines):\n        parents = []\n        children = []\n\n        while True:\n            if not lines:\n                sys.stderr.write(\'warning: unexpected end of entry\\n\')\n            line = lines.pop(0)\n            if line.startswith(\'[\'):\n                break\n        \n            # read function parent line\n            mo = self._cg_parent_re.match(line)\n            if not mo:\n                if self._cg_ignore_re.match(line):\n                    continue\n                sys.stderr.write(\'warning: unrecognized call graph entry: %r\\n\' % line)\n            else:\n                parent = self.translate(mo)\n                parents.append(parent)\n\n        # read primary line\n        mo = self._cg_primary_re.match(line)\n        if not mo:\n            sys.stderr.write(\'warning: unrecognized call graph entry: %r\\n\' % line)\n            return\n        else:\n            function = self.translate(mo)\n\n        while lines:\n            line = lines.pop(0)\n            \n            # read function subroutine line\n            mo = self._cg_child_re.match(line)\n            if not mo:\n                if self._cg_ignore_re.match(line):\n                    continue\n                sys.stderr.write(\'warning: unrecognized call graph entry: %r\\n\' % line)\n            else:\n                child = self.translate(mo)\n                children.append(child)\n        \n        function.parents = parents\n        function.children = children\n\n        self.functions[function.index] = function\n\n    def parse_cycle_entry(self, lines):\n\n        # read cycle header line\n        line = lines[0]\n        mo = self._cg_cycle_header_re.match(line)\n        if not mo:\n            sys.stderr.write(\'warning: unrecognized call graph entry: %r\\n\' % line)\n            return\n        cycle = self.translate(mo)\n\n        # read cycle member lines\n        cycle.functions = []\n        for line in lines[1:]:\n            mo = self._cg_cycle_member_re.match(line)\n            if not mo:\n                sys.stderr.write(\'warning: unrecognized call graph entry: %r\\n\' % line)\n                continue\n            call = self.translate(mo)\n            cycle.functions.append(call)\n        \n        self.cycles[cycle.cycle] = cycle\n\n    def parse_cg_entry(self, lines):\n        if lines[0].startswith(""[""):\n            self.parse_cycle_entry(lines)\n        else:\n            self.parse_function_entry(lines)\n\n    def parse_cg(self):\n        """"""Parse the call graph.""""""\n\n        # skip call graph header\n        while not self._cg_header_re.match(self.readline()):\n            pass\n        line = self.readline()\n        while self._cg_header_re.match(line):\n            line = self.readline()\n\n        # process call graph entries\n        entry_lines = []\n        while line != \'\\014\': # form feed\n            if line and not line.isspace():\n                if self._cg_sep_re.match(line):\n                    self.parse_cg_entry(entry_lines)\n                    entry_lines = []\n                else:\n                    entry_lines.append(line)            \n            line = self.readline()\n    \n    def parse(self):\n        self.parse_cg()\n        self.fp.close()\n\n        profile = Profile()\n        profile[TIME] = 0.0\n        \n        cycles = {}\n        for index in self.cycles:\n            cycles[index] = Cycle()\n\n        for entry in compat_itervalues(self.functions):\n            # populate the function\n            function = Function(entry.index, entry.name)\n            function[TIME] = entry.self\n            if entry.called is not None:\n                function.called = entry.called\n            if entry.called_self is not None:\n                call = Call(entry.index)\n                call[CALLS] = entry.called_self\n                function.called += entry.called_self\n            \n            # populate the function calls\n            for child in entry.children:\n                call = Call(child.index)\n                \n                assert child.called is not None\n                call[CALLS] = child.called\n\n                if child.index not in self.functions:\n                    # NOTE: functions that were never called but were discovered by gprof\'s \n                    # static call graph analysis dont have a call graph entry so we need\n                    # to add them here\n                    missing = Function(child.index, child.name)\n                    function[TIME] = 0.0\n                    function.called = 0\n                    profile.add_function(missing)\n\n                function.add_call(call)\n\n            profile.add_function(function)\n\n            if entry.cycle is not None:\n                try:\n                    cycle = cycles[entry.cycle]\n                except KeyError:\n                    sys.stderr.write(\'warning: <cycle %u as a whole> entry missing\\n\' % entry.cycle) \n                    cycle = Cycle()\n                    cycles[entry.cycle] = cycle\n                cycle.add_function(function)\n\n            profile[TIME] = profile[TIME] + function[TIME]\n\n        for cycle in compat_itervalues(cycles):\n            profile.add_cycle(cycle)\n\n        # Compute derived events\n        profile.validate()\n        profile.ratio(TIME_RATIO, TIME)\n        profile.call_ratios(CALLS)\n        profile.integrate(TOTAL_TIME, TIME)\n        profile.ratio(TOTAL_TIME_RATIO, TOTAL_TIME)\n\n        return profile\n\n\n# Clone&hack of GprofParser for VTune Amplifier XE 2013 gprof-cc output.\n# Tested only with AXE 2013 for Windows.\n#   - Use total times as reported by AXE.\n#   - In the absence of call counts, call ratios are faked from the relative\n#     proportions of total time.  This affects only the weighting of the calls.\n#   - Different header, separator, and end marker.\n#   - Extra whitespace after function names.\n#   - You get a full entry for <spontaneous>, which does not have parents.\n#   - Cycles do have parents.  These are saved but unused (as they are\n#     for functions).\n#   - Disambiguated ""unrecognized call graph entry"" error messages.\n# Notes:\n#   - Total time of functions as reported by AXE passes the val3 test.\n#   - CPU Time:Children in the input is sometimes a negative number.  This\n#     value goes to the variable descendants, which is unused.\n#   - The format of gprof-cc reports is unaffected by the use of\n#       -knob enable-call-counts=true (no call counts, ever), or\n#       -show-as=samples (results are quoted in seconds regardless).\nclass AXEParser(Parser):\n    ""Parser for VTune Amplifier XE 2013 gprof-cc report output.""\n\n    def __init__(self, fp):\n        Parser.__init__(self)\n        self.fp = fp\n        self.functions = {}\n        self.cycles = {}\n\n    def readline(self):\n        line = self.fp.readline()\n        if not line:\n            sys.stderr.write(\'error: unexpected end of file\\n\')\n            sys.exit(1)\n        line = line.rstrip(\'\\r\\n\')\n        return line\n\n    _int_re = re.compile(r\'^\\d+$\')\n    _float_re = re.compile(r\'^\\d+\\.\\d+$\')\n\n    def translate(self, mo):\n        """"""Extract a structure from a match object, while translating the types in the process.""""""\n        attrs = {}\n        groupdict = mo.groupdict()\n        for name, value in compat_iteritems(groupdict):\n            if value is None:\n                value = None\n            elif self._int_re.match(value):\n                value = int(value)\n            elif self._float_re.match(value):\n                value = float(value)\n            attrs[name] = (value)\n        return Struct(attrs)\n\n    _cg_header_re = re.compile(\n        \'^Index |\'\n        \'^-----+ \'\n    )\n\n    _cg_footer_re = re.compile(r\'^Index\\s+Function\\s*$\')\n\n    _cg_primary_re = re.compile(\n        r\'^\\[(?P<index>\\d+)\\]?\' + \n        r\'\\s+(?P<percentage_time>\\d+\\.\\d+)\' + \n        r\'\\s+(?P<self>\\d+\\.\\d+)\' + \n        r\'\\s+(?P<descendants>\\d+\\.\\d+)\' + \n        r\'\\s+(?P<name>\\S.*?)\' +\n        r\'(?:\\s+<cycle\\s(?P<cycle>\\d+)>)?\' +\n        r\'\\s+\\[(\\d+)\\]\' +\n        r\'\\s*$\'\n    )\n\n    _cg_parent_re = re.compile(\n        r\'^\\s+(?P<self>\\d+\\.\\d+)?\' + \n        r\'\\s+(?P<descendants>\\d+\\.\\d+)?\' + \n        r\'\\s+(?P<name>\\S.*?)\' +\n        r\'(?:\\s+<cycle\\s(?P<cycle>\\d+)>)?\' +\n        r\'(?:\\s+\\[(?P<index>\\d+)\\]\\s*)?\' +\n        r\'\\s*$\'\n    )\n\n    _cg_child_re = _cg_parent_re\n\n    _cg_cycle_header_re = re.compile(\n        r\'^\\[(?P<index>\\d+)\\]?\' + \n        r\'\\s+(?P<percentage_time>\\d+\\.\\d+)\' + \n        r\'\\s+(?P<self>\\d+\\.\\d+)\' + \n        r\'\\s+(?P<descendants>\\d+\\.\\d+)\' + \n        r\'\\s+<cycle\\s(?P<cycle>\\d+)\\sas\\sa\\swhole>\' +\n        r\'\\s+\\[(\\d+)\\]\' +\n        r\'\\s*$\'\n    )\n\n    _cg_cycle_member_re = re.compile(\n        r\'^\\s+(?P<self>\\d+\\.\\d+)?\' + \n        r\'\\s+(?P<descendants>\\d+\\.\\d+)?\' + \n        r\'\\s+(?P<name>\\S.*?)\' +\n        r\'(?:\\s+<cycle\\s(?P<cycle>\\d+)>)?\' +\n        r\'\\s+\\[(?P<index>\\d+)\\]\' +\n        r\'\\s*$\'\n    )\n\n    def parse_function_entry(self, lines):\n        parents = []\n        children = []\n\n        while True:\n            if not lines:\n                sys.stderr.write(\'warning: unexpected end of entry\\n\')\n                return\n            line = lines.pop(0)\n            if line.startswith(\'[\'):\n                break\n        \n            # read function parent line\n            mo = self._cg_parent_re.match(line)\n            if not mo:\n                sys.stderr.write(\'warning: unrecognized call graph entry (1): %r\\n\' % line)\n            else:\n                parent = self.translate(mo)\n                if parent.name != \'<spontaneous>\':\n                    parents.append(parent)\n\n        # read primary line\n        mo = self._cg_primary_re.match(line)\n        if not mo:\n            sys.stderr.write(\'warning: unrecognized call graph entry (2): %r\\n\' % line)\n            return\n        else:\n            function = self.translate(mo)\n\n        while lines:\n            line = lines.pop(0)\n            \n            # read function subroutine line\n            mo = self._cg_child_re.match(line)\n            if not mo:\n                sys.stderr.write(\'warning: unrecognized call graph entry (3): %r\\n\' % line)\n            else:\n                child = self.translate(mo)\n                if child.name != \'<spontaneous>\':\n                    children.append(child)\n\n        if function.name != \'<spontaneous>\':\n            function.parents = parents\n            function.children = children\n\n            self.functions[function.index] = function\n\n    def parse_cycle_entry(self, lines):\n\n        # Process the parents that were not there in gprof format.\n        parents = []\n        while True:\n            if not lines:\n                sys.stderr.write(\'warning: unexpected end of cycle entry\\n\')\n                return\n            line = lines.pop(0)\n            if line.startswith(\'[\'):\n                break\n            mo = self._cg_parent_re.match(line)\n            if not mo:\n                sys.stderr.write(\'warning: unrecognized call graph entry (6): %r\\n\' % line)\n            else:\n                parent = self.translate(mo)\n                if parent.name != \'<spontaneous>\':\n                    parents.append(parent)\n\n        # read cycle header line\n        mo = self._cg_cycle_header_re.match(line)\n        if not mo:\n            sys.stderr.write(\'warning: unrecognized call graph entry (4): %r\\n\' % line)\n            return\n        cycle = self.translate(mo)\n\n        # read cycle member lines\n        cycle.functions = []\n        for line in lines[1:]:\n            mo = self._cg_cycle_member_re.match(line)\n            if not mo:\n                sys.stderr.write(\'warning: unrecognized call graph entry (5): %r\\n\' % line)\n                continue\n            call = self.translate(mo)\n            cycle.functions.append(call)\n        \n        cycle.parents = parents\n        self.cycles[cycle.cycle] = cycle\n\n    def parse_cg_entry(self, lines):\n        if any(""as a whole"" in linelooper for linelooper in lines):\n            self.parse_cycle_entry(lines)\n        else:\n            self.parse_function_entry(lines)\n\n    def parse_cg(self):\n        """"""Parse the call graph.""""""\n\n        # skip call graph header\n        line = self.readline()\n        while self._cg_header_re.match(line):\n            line = self.readline()\n\n        # process call graph entries\n        entry_lines = []\n        # An EOF in readline terminates the program without returning.\n        while not self._cg_footer_re.match(line):\n            if line.isspace():\n                self.parse_cg_entry(entry_lines)\n                entry_lines = []\n            else:\n                entry_lines.append(line)            \n            line = self.readline()\n\n    def parse(self):\n        sys.stderr.write(\'warning: for axe format, edge weights are unreliable estimates derived from function total times.\\n\')\n        self.parse_cg()\n        self.fp.close()\n\n        profile = Profile()\n        profile[TIME] = 0.0\n        \n        cycles = {}\n        for index in self.cycles:\n            cycles[index] = Cycle()\n\n        for entry in compat_itervalues(self.functions):\n            # populate the function\n            function = Function(entry.index, entry.name)\n            function[TIME] = entry.self\n            function[TOTAL_TIME_RATIO] = entry.percentage_time / 100.0\n            \n            # populate the function calls\n            for child in entry.children:\n                call = Call(child.index)\n                # The following bogus value affects only the weighting of\n                # the calls.\n                call[TOTAL_TIME_RATIO] = function[TOTAL_TIME_RATIO]\n\n                if child.index not in self.functions:\n                    # NOTE: functions that were never called but were discovered by gprof\'s \n                    # static call graph analysis dont have a call graph entry so we need\n                    # to add them here\n                    # FIXME: Is this applicable?\n                    missing = Function(child.index, child.name)\n                    function[TIME] = 0.0\n                    profile.add_function(missing)\n\n                function.add_call(call)\n\n            profile.add_function(function)\n\n            if entry.cycle is not None:\n                try:\n                    cycle = cycles[entry.cycle]\n                except KeyError:\n                    sys.stderr.write(\'warning: <cycle %u as a whole> entry missing\\n\' % entry.cycle) \n                    cycle = Cycle()\n                    cycles[entry.cycle] = cycle\n                cycle.add_function(function)\n\n            profile[TIME] = profile[TIME] + function[TIME]\n\n        for cycle in compat_itervalues(cycles):\n            profile.add_cycle(cycle)\n\n        # Compute derived events.\n        profile.validate()\n        profile.ratio(TIME_RATIO, TIME)\n        # Lacking call counts, fake call ratios based on total times.\n        profile.call_ratios(TOTAL_TIME_RATIO)\n        # The TOTAL_TIME_RATIO of functions is already set.  Propagate that\n        # total time to the calls.  (TOTAL_TIME is neither set nor used.)\n        for function in compat_itervalues(profile.functions):\n            for call in compat_itervalues(function.calls):\n                if call.ratio is not None:\n                    callee = profile.functions[call.callee_id]\n                    call[TOTAL_TIME_RATIO] = call.ratio * callee[TOTAL_TIME_RATIO]\n\n        return profile\n\n\nclass CallgrindParser(LineParser):\n    """"""Parser for valgrind\'s callgrind tool.\n    \n    See also:\n    - http://valgrind.org/docs/manual/cl-format.html\n    """"""\n\n    _call_re = re.compile(r\'^calls=\\s*(\\d+)\\s+((\\d+|\\+\\d+|-\\d+|\\*)\\s+)+$\')\n\n    def __init__(self, infile):\n        LineParser.__init__(self, infile)\n\n        # Textual positions\n        self.position_ids = {}\n        self.positions = {}\n\n        # Numeric positions\n        self.num_positions = 1\n        self.cost_positions = [\'line\']\n        self.last_positions = [0]\n\n        # Events\n        self.num_events = 0\n        self.cost_events = []\n\n        self.profile = Profile()\n        self.profile[SAMPLES] = 0\n\n    def parse(self):\n        # read lookahead\n        self.readline()\n\n        self.parse_key(\'version\')\n        self.parse_key(\'creator\')\n        while self.parse_part():\n            pass\n        if not self.eof():\n            sys.stderr.write(\'warning: line %u: unexpected line\\n\' % self.line_no)\n            sys.stderr.write(\'%s\\n\' % self.lookahead())\n\n        # compute derived data\n        self.profile.validate()\n        self.profile.find_cycles()\n        self.profile.ratio(TIME_RATIO, SAMPLES)\n        self.profile.call_ratios(SAMPLES2)\n        self.profile.integrate(TOTAL_TIME_RATIO, TIME_RATIO)\n\n        return self.profile\n\n    def parse_part(self):\n        if not self.parse_header_line():\n            return False\n        while self.parse_header_line():\n            pass\n        if not self.parse_body_line():\n            return False\n        while self.parse_body_line():\n            pass\n        return True\n\n    def parse_header_line(self):\n        return \\\n            self.parse_empty() or \\\n            self.parse_comment() or \\\n            self.parse_part_detail() or \\\n            self.parse_description() or \\\n            self.parse_event_specification() or \\\n            self.parse_cost_line_def() or \\\n            self.parse_cost_summary()\n\n    _detail_keys = set((\'cmd\', \'pid\', \'thread\', \'part\'))\n\n    def parse_part_detail(self):\n        return self.parse_keys(self._detail_keys)\n\n    def parse_description(self):\n        return self.parse_key(\'desc\') is not None\n\n    def parse_event_specification(self):\n        event = self.parse_key(\'event\')\n        if event is None:\n            return False\n        return True\n\n    def parse_cost_line_def(self):\n        pair = self.parse_keys((\'events\', \'positions\'))\n        if pair is None:\n            return False\n        key, value = pair\n        items = value.split()\n        if key == \'events\':\n            self.num_events = len(items)\n            self.cost_events = items\n        if key == \'positions\':\n            self.num_positions = len(items)\n            self.cost_positions = items\n            self.last_positions = [0]*self.num_positions\n        return True\n\n    def parse_cost_summary(self):\n        pair = self.parse_keys((\'summary\', \'totals\'))\n        if pair is None:\n            return False\n        return True\n\n    def parse_body_line(self):\n        return \\\n            self.parse_empty() or \\\n            self.parse_comment() or \\\n            self.parse_cost_line() or \\\n            self.parse_position_spec() or \\\n            self.parse_association_spec()\n\n    __subpos_re = r\'(0x[0-9a-fA-F]+|\\d+|\\+\\d+|-\\d+|\\*)\'\n    _cost_re = re.compile(r\'^\' + \n        __subpos_re + r\'( +\' + __subpos_re + r\')*\' +\n        r\'( +\\d+)*\' +\n    \'$\')\n\n    def parse_cost_line(self, calls=None):\n        line = self.lookahead().rstrip()\n        mo = self._cost_re.match(line)\n        if not mo:\n            return False\n\n        function = self.get_function()\n\n        if calls is None:\n            # Unlike other aspects, call object (cob) is relative not to the\n            # last call object, but to the caller\'s object (ob), so try to\n            # update it when processing a functions cost line\n            try:\n                self.positions[\'cob\'] = self.positions[\'ob\']\n            except KeyError:\n                pass\n\n        values = line.split()\n        assert len(values) <= self.num_positions + self.num_events\n\n        positions = values[0 : self.num_positions]\n        events = values[self.num_positions : ]\n        events += [\'0\']*(self.num_events - len(events))\n\n        for i in range(self.num_positions):\n            position = positions[i]\n            if position == \'*\':\n                position = self.last_positions[i]\n            elif position[0] in \'-+\':\n                position = self.last_positions[i] + int(position)\n            elif position.startswith(\'0x\'):\n                position = int(position, 16)\n            else:\n                position = int(position)\n            self.last_positions[i] = position\n\n        events = [float(event) for event in events]\n\n        if calls is None:\n            function[SAMPLES] += events[0] \n            self.profile[SAMPLES] += events[0]\n        else:\n            callee = self.get_callee()\n            callee.called += calls\n    \n            try:\n                call = function.calls[callee.id]\n            except KeyError:\n                call = Call(callee.id)\n                call[CALLS] = calls\n                call[SAMPLES2] = events[0]\n                function.add_call(call)\n            else:\n                call[CALLS] += calls\n                call[SAMPLES2] += events[0]\n\n        self.consume()\n        return True\n\n    def parse_association_spec(self):\n        line = self.lookahead()\n        if not line.startswith(\'calls=\'):\n            return False\n\n        _, values = line.split(\'=\', 1)\n        values = values.strip().split()\n        calls = int(values[0])\n        call_position = values[1:]\n        self.consume()\n\n        self.parse_cost_line(calls)\n\n        return True\n\n    _position_re = re.compile(r\'^(?P<position>[cj]?(?:ob|fl|fi|fe|fn))=\\s*(?:\\((?P<id>\\d+)\\))?(?:\\s*(?P<name>.+))?\')\n\n    _position_table_map = {\n        \'ob\': \'ob\',\n        \'fl\': \'fl\',\n        \'fi\': \'fl\',\n        \'fe\': \'fl\',\n        \'fn\': \'fn\',\n        \'cob\': \'ob\',\n        \'cfl\': \'fl\',\n        \'cfi\': \'fl\',\n        \'cfe\': \'fl\',\n        \'cfn\': \'fn\',\n        \'jfi\': \'fl\',\n    }\n\n    _position_map = {\n        \'ob\': \'ob\',\n        \'fl\': \'fl\',\n        \'fi\': \'fl\',\n        \'fe\': \'fl\',\n        \'fn\': \'fn\',\n        \'cob\': \'cob\',\n        \'cfl\': \'cfl\',\n        \'cfi\': \'cfl\',\n        \'cfe\': \'cfl\',\n        \'cfn\': \'cfn\',\n        \'jfi\': \'jfi\',\n    }\n\n    def parse_position_spec(self):\n        line = self.lookahead()\n        \n        if line.startswith(\'jump=\') or line.startswith(\'jcnd=\'):\n            self.consume()\n            return True\n\n        mo = self._position_re.match(line)\n        if not mo:\n            return False\n\n        position, id, name = mo.groups()\n        if id:\n            table = self._position_table_map[position]\n            if name:\n                self.position_ids[(table, id)] = name\n            else:\n                name = self.position_ids.get((table, id), \'\')\n        self.positions[self._position_map[position]] = name\n\n        self.consume()\n        return True\n\n    def parse_empty(self):\n        if self.eof():\n            return False\n        line = self.lookahead()\n        if line.strip():\n            return False\n        self.consume()\n        return True\n\n    def parse_comment(self):\n        line = self.lookahead()\n        if not line.startswith(\'#\'):\n            return False\n        self.consume()\n        return True\n\n    _key_re = re.compile(r\'^(\\w+):\')\n\n    def parse_key(self, key):\n        pair = self.parse_keys((key,))\n        if not pair:\n            return None\n        key, value = pair\n        return value\n\n    def parse_keys(self, keys):\n        line = self.lookahead()\n        mo = self._key_re.match(line)\n        if not mo:\n            return None\n        key, value = line.split(\':\', 1)\n        if key not in keys:\n            return None\n        value = value.strip()\n        self.consume()\n        return key, value\n\n    def make_function(self, module, filename, name):\n        # FIXME: module and filename are not being tracked reliably\n        #id = \'|\'.join((module, filename, name))\n        id = name\n        try:\n            function = self.profile.functions[id]\n        except KeyError:\n            function = Function(id, name)\n            if module:\n                function.module = os.path.basename(module)\n            function[SAMPLES] = 0\n            function.called = 0\n            self.profile.add_function(function)\n        return function\n\n    def get_function(self):\n        module = self.positions.get(\'ob\', \'\')\n        filename = self.positions.get(\'fl\', \'\') \n        function = self.positions.get(\'fn\', \'\') \n        return self.make_function(module, filename, function)\n\n    def get_callee(self):\n        module = self.positions.get(\'cob\', \'\')\n        filename = self.positions.get(\'cfi\', \'\') \n        function = self.positions.get(\'cfn\', \'\') \n        return self.make_function(module, filename, function)\n\n\nclass PerfParser(LineParser):\n    """"""Parser for linux perf callgraph output.\n\n    It expects output generated with\n\n        perf record -g\n        perf script | gprof2dot.py --format=perf\n    """"""\n\n    def __init__(self, infile):\n        LineParser.__init__(self, infile)\n        self.profile = Profile()\n\n    def readline(self):\n        # Override LineParser.readline to ignore comment lines\n        while True:\n            LineParser.readline(self)\n            if self.eof() or not self.lookahead().startswith(\'#\'):\n                break\n\n    def parse(self):\n        # read lookahead\n        self.readline()\n\n        profile = self.profile\n        profile[SAMPLES] = 0\n        while not self.eof():\n            self.parse_event()\n\n        # compute derived data\n        profile.validate()\n        profile.find_cycles()\n        profile.ratio(TIME_RATIO, SAMPLES)\n        profile.call_ratios(SAMPLES2)\n        if totalMethod == ""callratios"":\n            # Heuristic approach.  TOTAL_SAMPLES is unused.\n            profile.integrate(TOTAL_TIME_RATIO, TIME_RATIO)\n        elif totalMethod == ""callstacks"":\n            # Use the actual call chains for functions.\n            profile[TOTAL_SAMPLES] = profile[SAMPLES]\n            profile.ratio(TOTAL_TIME_RATIO, TOTAL_SAMPLES)\n            # Then propagate that total time to the calls.\n            for function in compat_itervalues(profile.functions):\n                for call in compat_itervalues(function.calls):\n                    if call.ratio is not None:\n                        callee = profile.functions[call.callee_id]\n                        call[TOTAL_TIME_RATIO] = call.ratio * callee[TOTAL_TIME_RATIO]\n        else:\n            assert False\n\n        return profile\n\n    def parse_event(self):\n        if self.eof():\n            return\n\n        line = self.consume()\n        assert line\n\n        callchain = self.parse_callchain()\n        if not callchain:\n            return\n\n        callee = callchain[0]\n        callee[SAMPLES] += 1\n        self.profile[SAMPLES] += 1\n\n        for caller in callchain[1:]:\n            try:\n                call = caller.calls[callee.id]\n            except KeyError:\n                call = Call(callee.id)\n                call[SAMPLES2] = 1\n                caller.add_call(call)\n            else:\n                call[SAMPLES2] += 1\n\n            callee = caller\n\n        # Increment TOTAL_SAMPLES only once on each function.\n        stack = set(callchain)\n        for function in stack:\n            function[TOTAL_SAMPLES] += 1\n\n    def parse_callchain(self):\n        callchain = []\n        while self.lookahead():\n            function = self.parse_call()\n            if function is None:\n                break\n            callchain.append(function)\n        if self.lookahead() == \'\':\n            self.consume()\n        return callchain\n\n    call_re = re.compile(r\'^\\s+(?P<address>[0-9a-fA-F]+)\\s+(?P<symbol>.*)\\s+\\((?P<module>.*)\\)$\')\n\n    def parse_call(self):\n        line = self.consume()\n        mo = self.call_re.match(line)\n        assert mo\n        if not mo:\n            return None\n\n        function_name = mo.group(\'symbol\')\n        if not function_name or function_name == \'[unknown]\':\n            function_name = mo.group(\'address\')\n\n        module = mo.group(\'module\')\n\n        function_id = function_name + \':\' + module\n\n        try:\n            function = self.profile.functions[function_id]\n        except KeyError:\n            function = Function(function_id, function_name)\n            function.module = os.path.basename(module)\n            function[SAMPLES] = 0\n            function[TOTAL_SAMPLES] = 0\n            self.profile.add_function(function)\n\n        return function\n\n\nclass OprofileParser(LineParser):\n    """"""Parser for oprofile callgraph output.\n    \n    See also:\n    - http://oprofile.sourceforge.net/doc/opreport.html#opreport-callgraph\n    """"""\n\n    _fields_re = {\n        \'samples\': r\'(\\d+)\',\n        \'%\': r\'(\\S+)\',\n        \'linenr info\': r\'(?P<source>\\(no location information\\)|\\S+:\\d+)\',\n        \'image name\': r\'(?P<image>\\S+(?:\\s\\(tgid:[^)]*\\))?)\',\n        \'app name\': r\'(?P<application>\\S+)\',\n        \'symbol name\': r\'(?P<symbol>\\(no symbols\\)|.+?)\',\n    }\n\n    def __init__(self, infile):\n        LineParser.__init__(self, infile)\n        self.entries = {}\n        self.entry_re = None\n\n    def add_entry(self, callers, function, callees):\n        try:\n            entry = self.entries[function.id]\n        except KeyError:\n            self.entries[function.id] = (callers, function, callees)\n        else:\n            callers_total, function_total, callees_total = entry\n            self.update_subentries_dict(callers_total, callers)\n            function_total.samples += function.samples\n            self.update_subentries_dict(callees_total, callees)\n    \n    def update_subentries_dict(self, totals, partials):\n        for partial in compat_itervalues(partials):\n            try:\n                total = totals[partial.id]\n            except KeyError:\n                totals[partial.id] = partial\n            else:\n                total.samples += partial.samples\n        \n    def parse(self):\n        # read lookahead\n        self.readline()\n\n        self.parse_header()\n        while self.lookahead():\n            self.parse_entry()\n\n        profile = Profile()\n\n        reverse_call_samples = {}\n        \n        # populate the profile\n        profile[SAMPLES] = 0\n        for _callers, _function, _callees in compat_itervalues(self.entries):\n            function = Function(_function.id, _function.name)\n            function[SAMPLES] = _function.samples\n            profile.add_function(function)\n            profile[SAMPLES] += _function.samples\n\n            if _function.application:\n                function.process = os.path.basename(_function.application)\n            if _function.image:\n                function.module = os.path.basename(_function.image)\n\n            total_callee_samples = 0\n            for _callee in compat_itervalues(_callees):\n                total_callee_samples += _callee.samples\n\n            for _callee in compat_itervalues(_callees):\n                if not _callee.self:\n                    call = Call(_callee.id)\n                    call[SAMPLES2] = _callee.samples\n                    function.add_call(call)\n                \n        # compute derived data\n        profile.validate()\n        profile.find_cycles()\n        profile.ratio(TIME_RATIO, SAMPLES)\n        profile.call_ratios(SAMPLES2)\n        profile.integrate(TOTAL_TIME_RATIO, TIME_RATIO)\n\n        return profile\n\n    def parse_header(self):\n        while not self.match_header():\n            self.consume()\n        line = self.lookahead()\n        fields = re.split(r\'\\s\\s+\', line)\n        entry_re = r\'^\\s*\' + r\'\\s+\'.join([self._fields_re[field] for field in fields]) + r\'(?P<self>\\s+\\[self\\])?$\'\n        self.entry_re = re.compile(entry_re)\n        self.skip_separator()\n\n    def parse_entry(self):\n        callers = self.parse_subentries()\n        if self.match_primary():\n            function = self.parse_subentry()\n            if function is not None:\n                callees = self.parse_subentries()\n                self.add_entry(callers, function, callees)\n        self.skip_separator()\n\n    def parse_subentries(self):\n        subentries = {}\n        while self.match_secondary():\n            subentry = self.parse_subentry()\n            subentries[subentry.id] = subentry\n        return subentries\n\n    def parse_subentry(self):\n        entry = Struct()\n        line = self.consume()\n        mo = self.entry_re.match(line)\n        if not mo:\n            raise ParseError(\'failed to parse\', line)\n        fields = mo.groupdict()\n        entry.samples = int(mo.group(1))\n        if \'source\' in fields and fields[\'source\'] != \'(no location information)\':\n            source = fields[\'source\']\n            filename, lineno = source.split(\':\')\n            entry.filename = filename\n            entry.lineno = int(lineno)\n        else:\n            source = \'\'\n            entry.filename = None\n            entry.lineno = None\n        entry.image = fields.get(\'image\', \'\')\n        entry.application = fields.get(\'application\', \'\')\n        if \'symbol\' in fields and fields[\'symbol\'] != \'(no symbols)\':\n            entry.symbol = fields[\'symbol\']\n        else:\n            entry.symbol = \'\'\n        if entry.symbol.startswith(\'""\') and entry.symbol.endswith(\'""\'):\n            entry.symbol = entry.symbol[1:-1]\n        entry.id = \':\'.join((entry.application, entry.image, source, entry.symbol))\n        entry.self = fields.get(\'self\', None) != None\n        if entry.self:\n            entry.id += \':self\'\n        if entry.symbol:\n            entry.name = entry.symbol\n        else:\n            entry.name = entry.image\n        return entry\n\n    def skip_separator(self):\n        while not self.match_separator():\n            self.consume()\n        self.consume()\n\n    def match_header(self):\n        line = self.lookahead()\n        return line.startswith(\'samples\')\n\n    def match_separator(self):\n        line = self.lookahead()\n        return line == \'-\'*len(line)\n\n    def match_primary(self):\n        line = self.lookahead()\n        return not line[:1].isspace()\n    \n    def match_secondary(self):\n        line = self.lookahead()\n        return line[:1].isspace()\n\n\nclass HProfParser(LineParser):\n    """"""Parser for java hprof output\n    \n    See also:\n    - http://java.sun.com/developer/technicalArticles/Programming/HPROF.html\n    """"""\n\n    trace_re = re.compile(r\'\\t(.*)\\((.*):(.*)\\)\')\n    trace_id_re = re.compile(r\'^TRACE (\\d+):$\')\n\n    def __init__(self, infile):\n        LineParser.__init__(self, infile)\n        self.traces = {}\n        self.samples = {}\n\n    def parse(self):\n        # read lookahead\n        self.readline()\n\n        while not self.lookahead().startswith(\'------\'): self.consume()\n        while not self.lookahead().startswith(\'TRACE \'): self.consume()\n\n        self.parse_traces()\n\n        while not self.lookahead().startswith(\'CPU\'):\n            self.consume()\n\n        self.parse_samples()\n\n        # populate the profile\n        profile = Profile()\n        profile[SAMPLES] = 0\n\n        functions = {}\n\n        # build up callgraph\n        for id, trace in compat_iteritems(self.traces):\n            if not id in self.samples: continue\n            mtime = self.samples[id][0]\n            last = None\n\n            for func, file, line in trace:\n                if not func in functions:\n                    function = Function(func, func)\n                    function[SAMPLES] = 0\n                    profile.add_function(function)\n                    functions[func] = function\n\n                function = functions[func]\n                # allocate time to the deepest method in the trace\n                if not last:\n                    function[SAMPLES] += mtime\n                    profile[SAMPLES] += mtime\n                else:\n                    c = function.get_call(last)\n                    c[SAMPLES2] += mtime\n\n                last = func\n\n        # compute derived data\n        profile.validate()\n        profile.find_cycles()\n        profile.ratio(TIME_RATIO, SAMPLES)\n        profile.call_ratios(SAMPLES2)\n        profile.integrate(TOTAL_TIME_RATIO, TIME_RATIO)\n\n        return profile\n\n    def parse_traces(self):\n        while self.lookahead().startswith(\'TRACE \'):\n            self.parse_trace()\n\n    def parse_trace(self):\n        l = self.consume()\n        mo = self.trace_id_re.match(l)\n        tid = mo.group(1)\n        last = None\n        trace = []\n\n        while self.lookahead().startswith(\'\\t\'):\n            l = self.consume()\n            match = self.trace_re.search(l)\n            if not match:\n                #sys.stderr.write(\'Invalid line: %s\\n\' % l)\n                break\n            else:\n                function_name, file, line = match.groups()\n                trace += [(function_name, file, line)]\n\n        self.traces[int(tid)] = trace\n\n    def parse_samples(self):\n        self.consume()\n        self.consume()\n\n        while not self.lookahead().startswith(\'CPU\'):\n            rank, percent_self, percent_accum, count, traceid, method = self.lookahead().split()\n            self.samples[int(traceid)] = (int(count), method)\n            self.consume()\n\n\nclass SysprofParser(XmlParser):\n\n    def __init__(self, stream):\n        XmlParser.__init__(self, stream)\n\n    def parse(self):\n        objects = {}\n        nodes = {}\n\n        self.element_start(\'profile\')\n        while self.token.type == XML_ELEMENT_START:\n            if self.token.name_or_data == \'objects\':\n                assert not objects\n                objects = self.parse_items(\'objects\')\n            elif self.token.name_or_data == \'nodes\':\n                assert not nodes\n                nodes = self.parse_items(\'nodes\')\n            else:\n                self.parse_value(self.token.name_or_data)\n        self.element_end(\'profile\')\n\n        return self.build_profile(objects, nodes)\n\n    def parse_items(self, name):\n        assert name[-1] == \'s\'\n        items = {}\n        self.element_start(name)\n        while self.token.type == XML_ELEMENT_START:\n            id, values = self.parse_item(name[:-1])\n            assert id not in items\n            items[id] = values\n        self.element_end(name)\n        return items\n\n    def parse_item(self, name):\n        attrs = self.element_start(name)\n        id = int(attrs[\'id\'])\n        values = self.parse_values()\n        self.element_end(name)\n        return id, values\n\n    def parse_values(self):\n        values = {}\n        while self.token.type == XML_ELEMENT_START:\n            name = self.token.name_or_data\n            value = self.parse_value(name)\n            assert name not in values\n            values[name] = value\n        return values\n\n    def parse_value(self, tag):\n        self.element_start(tag)\n        value = self.character_data()\n        self.element_end(tag)\n        if value.isdigit():\n            return int(value)\n        if value.startswith(\'""\') and value.endswith(\'""\'):\n            return value[1:-1]\n        return value\n\n    def build_profile(self, objects, nodes):\n        profile = Profile()\n        \n        profile[SAMPLES] = 0\n        for id, object in compat_iteritems(objects):\n            # Ignore fake objects (process names, modules, ""Everything"", ""kernel"", etc.)\n            if object[\'self\'] == 0:\n                continue\n\n            function = Function(id, object[\'name\'])\n            function[SAMPLES] = object[\'self\']\n            profile.add_function(function)\n            profile[SAMPLES] += function[SAMPLES]\n\n        for id, node in compat_iteritems(nodes):\n            # Ignore fake calls\n            if node[\'self\'] == 0:\n                continue\n\n            # Find a non-ignored parent\n            parent_id = node[\'parent\']\n            while parent_id != 0:\n                parent = nodes[parent_id]\n                caller_id = parent[\'object\']\n                if objects[caller_id][\'self\'] != 0:\n                    break\n                parent_id = parent[\'parent\']\n            if parent_id == 0:\n                continue\n\n            callee_id = node[\'object\']\n\n            assert objects[caller_id][\'self\']\n            assert objects[callee_id][\'self\']\n\n            function = profile.functions[caller_id]\n\n            samples = node[\'self\']\n            try:\n                call = function.calls[callee_id]\n            except KeyError:\n                call = Call(callee_id)\n                call[SAMPLES2] = samples\n                function.add_call(call)\n            else:\n                call[SAMPLES2] += samples\n\n        # Compute derived events\n        profile.validate()\n        profile.find_cycles()\n        profile.ratio(TIME_RATIO, SAMPLES)\n        profile.call_ratios(SAMPLES2)\n        profile.integrate(TOTAL_TIME_RATIO, TIME_RATIO)\n\n        return profile\n\n\nclass XPerfParser(Parser):\n    """"""Parser for CSVs generted by XPerf, from Microsoft Windows Performance Tools.\n    """"""\n\n    def __init__(self, stream):\n        Parser.__init__(self)\n        self.stream = stream\n        self.profile = Profile()\n        self.profile[SAMPLES] = 0\n        self.column = {}\n\n    def parse(self):\n        import csv\n        reader = csv.reader(\n            self.stream, \n            delimiter = \',\',\n            quotechar = None,\n            escapechar = None,\n            doublequote = False,\n            skipinitialspace = True,\n            lineterminator = \'\\r\\n\',\n            quoting = csv.QUOTE_NONE)\n        header = True\n        for row in reader:\n            if header:\n                self.parse_header(row)\n                header = False\n            else:\n                self.parse_row(row)\n                \n        # compute derived data\n        self.profile.validate()\n        self.profile.find_cycles()\n        self.profile.ratio(TIME_RATIO, SAMPLES)\n        self.profile.call_ratios(SAMPLES2)\n        self.profile.integrate(TOTAL_TIME_RATIO, TIME_RATIO)\n\n        return self.profile\n\n    def parse_header(self, row):\n        for column in range(len(row)):\n            name = row[column]\n            assert name not in self.column\n            self.column[name] = column\n\n    def parse_row(self, row):\n        fields = {}\n        for name, column in compat_iteritems(self.column):\n            value = row[column]\n            for factory in int, float:\n                try:\n                    value = factory(value)\n                except ValueError:\n                    pass\n                else:\n                    break\n            fields[name] = value\n        \n        process = fields[\'Process Name\']\n        symbol = fields[\'Module\'] + \'!\' + fields[\'Function\']\n        weight = fields[\'Weight\']\n        count = fields[\'Count\']\n\n        if process == \'Idle\':\n            return\n\n        function = self.get_function(process, symbol)\n        function[SAMPLES] += weight * count\n        self.profile[SAMPLES] += weight * count\n\n        stack = fields[\'Stack\']\n        if stack != \'?\':\n            stack = stack.split(\'/\')\n            assert stack[0] == \'[Root]\'\n            if stack[-1] != symbol:\n                # XXX: some cases the sampled function does not appear in the stack\n                stack.append(symbol)\n            caller = None\n            for symbol in stack[1:]:\n                callee = self.get_function(process, symbol)\n                if caller is not None:\n                    try:\n                        call = caller.calls[callee.id]\n                    except KeyError:\n                        call = Call(callee.id)\n                        call[SAMPLES2] = count\n                        caller.add_call(call)\n                    else:\n                        call[SAMPLES2] += count\n                caller = callee\n\n    def get_function(self, process, symbol):\n        function_id = process + \'!\' + symbol\n\n        try:\n            function = self.profile.functions[function_id]\n        except KeyError:\n            module, name = symbol.split(\'!\', 1)\n            function = Function(function_id, name)\n            function.process = process\n            function.module = module\n            function[SAMPLES] = 0\n            self.profile.add_function(function)\n\n        return function\n\n\nclass SleepyParser(Parser):\n    """"""Parser for GNU gprof output.\n\n    See also:\n    - http://www.codersnotes.com/sleepy/\n    - http://sleepygraph.sourceforge.net/\n    """"""\n\n    stdinInput = False\n\n    def __init__(self, filename):\n        Parser.__init__(self)\n\n        from zipfile import ZipFile\n\n        self.database = ZipFile(filename)\n\n        self.symbols = {}\n        self.calls = {}\n\n        self.profile = Profile()\n    \n    _symbol_re = re.compile(\n        r\'^(?P<id>\\w+)\' + \n        r\'\\s+""(?P<module>[^""]*)""\' + \n        r\'\\s+""(?P<procname>[^""]*)""\' + \n        r\'\\s+""(?P<sourcefile>[^""]*)""\' + \n        r\'\\s+(?P<sourceline>\\d+)$\'\n    )\n\n    def openEntry(self, name):\n        # Some versions of verysleepy use lowercase filenames\n        for database_name in self.database.namelist():\n            if name.lower() == database_name.lower():\n                name = database_name\n                break\n\n        return self.database.open(name, \'rU\')\n\n    def parse_symbols(self):\n        for line in self.openEntry(\'Symbols.txt\'):\n            line = line.decode(\'UTF-8\')\n\n            mo = self._symbol_re.match(line)\n            if mo:\n                symbol_id, module, procname, sourcefile, sourceline = mo.groups()\n    \n                function_id = \':\'.join([module, procname])\n\n                try:\n                    function = self.profile.functions[function_id]\n                except KeyError:\n                    function = Function(function_id, procname)\n                    function.module = module\n                    function[SAMPLES] = 0\n                    self.profile.add_function(function)\n\n                self.symbols[symbol_id] = function\n\n    def parse_callstacks(self):\n        for line in self.openEntry(\'Callstacks.txt\'):\n            line = line.decode(\'UTF-8\')\n\n            fields = line.split()\n            samples = float(fields[0])\n            callstack = fields[1:]\n\n            callstack = [self.symbols[symbol_id] for symbol_id in callstack]\n\n            callee = callstack[0]\n\n            callee[SAMPLES] += samples\n            self.profile[SAMPLES] += samples\n            \n            for caller in callstack[1:]:\n                try:\n                    call = caller.calls[callee.id]\n                except KeyError:\n                    call = Call(callee.id)\n                    call[SAMPLES2] = samples\n                    caller.add_call(call)\n                else:\n                    call[SAMPLES2] += samples\n\n                callee = caller\n\n    def parse(self):\n        profile = self.profile\n        profile[SAMPLES] = 0\n\n        self.parse_symbols()\n        self.parse_callstacks()\n\n        # Compute derived events\n        profile.validate()\n        profile.find_cycles()\n        profile.ratio(TIME_RATIO, SAMPLES)\n        profile.call_ratios(SAMPLES2)\n        profile.integrate(TOTAL_TIME_RATIO, TIME_RATIO)\n\n        return profile\n\n\nclass PstatsParser:\n    """"""Parser python profiling statistics saved with te pstats module.""""""\n\n    stdinInput = False\n    multipleInput = True\n\n    def __init__(self, *filename):\n        import pstats\n        try:\n            self.stats = pstats.Stats(*filename)\n        except ValueError:\n            if PYTHON_3:\n                sys.stderr.write(\'error: failed to load %s\\n\' % \', \'.join(filename))\n                sys.exit(1)\n            import hotshot.stats\n            self.stats = hotshot.stats.load(filename[0])\n        self.profile = Profile()\n        self.function_ids = {}\n\n    def get_function_name(self, key):\n        filename, line, name = key\n        module = os.path.splitext(filename)[0]\n        module = os.path.basename(module)\n        return ""%s:%d:%s"" % (module, line, name)\n\n    def get_function(self, key):\n        try:\n            id = self.function_ids[key]\n        except KeyError:\n            id = len(self.function_ids)\n            name = self.get_function_name(key)\n            function = Function(id, name)\n            function.filename = key[0]\n            self.profile.functions[id] = function\n            self.function_ids[key] = id\n        else:\n            function = self.profile.functions[id]\n        return function\n\n    def parse(self):\n        self.profile[TIME] = 0.0\n        self.profile[TOTAL_TIME] = self.stats.total_tt\n        for fn, (cc, nc, tt, ct, callers) in compat_iteritems(self.stats.stats):\n            callee = self.get_function(fn)\n            callee.called = nc\n            callee[TOTAL_TIME] = ct\n            callee[TIME] = tt\n            self.profile[TIME] += tt\n            self.profile[TOTAL_TIME] = max(self.profile[TOTAL_TIME], ct)\n            for fn, value in compat_iteritems(callers):\n                caller = self.get_function(fn)\n                call = Call(callee.id)\n                if isinstance(value, tuple):\n                    for i in xrange(0, len(value), 4):\n                        nc, cc, tt, ct = value[i:i+4]\n                        if CALLS in call:\n                            call[CALLS] += cc\n                        else:\n                            call[CALLS] = cc\n\n                        if TOTAL_TIME in call:\n                            call[TOTAL_TIME] += ct\n                        else:\n                            call[TOTAL_TIME] = ct\n\n                else:\n                    call[CALLS] = value\n                    call[TOTAL_TIME] = ratio(value, nc)*ct\n\n                caller.add_call(call)\n\n        if False:\n            self.stats.print_stats()\n            self.stats.print_callees()\n\n        # Compute derived events\n        self.profile.validate()\n        self.profile.ratio(TIME_RATIO, TIME)\n        self.profile.ratio(TOTAL_TIME_RATIO, TOTAL_TIME)\n\n        return self.profile\n\n\nformats = {\n    ""axe"": AXEParser,\n    ""callgrind"": CallgrindParser,\n    ""hprof"": HProfParser,\n    ""json"": JsonParser,\n    ""oprofile"": OprofileParser,\n    ""perf"": PerfParser,\n    ""prof"": GprofParser,\n    ""pstats"": PstatsParser,\n    ""sleepy"": SleepyParser,\n    ""sysprof"": SysprofParser,\n    ""xperf"": XPerfParser,\n}\n\n\n########################################################################\n# Output\n\n\nclass Theme:\n\n    def __init__(self, \n            bgcolor = (0.0, 0.0, 1.0),\n            mincolor = (0.0, 0.0, 0.0),\n            maxcolor = (0.0, 0.0, 1.0),\n            fontname = ""Arial"",\n            fontcolor = ""white"",\n            nodestyle = ""filled"",\n            minfontsize = 10.0,\n            maxfontsize = 10.0,\n            minpenwidth = 0.5,\n            maxpenwidth = 4.0,\n            gamma = 2.2,\n            skew = 1.0):\n        self.bgcolor = bgcolor\n        self.mincolor = mincolor\n        self.maxcolor = maxcolor\n        self.fontname = fontname\n        self.fontcolor = fontcolor\n        self.nodestyle = nodestyle\n        self.minfontsize = minfontsize\n        self.maxfontsize = maxfontsize\n        self.minpenwidth = minpenwidth\n        self.maxpenwidth = maxpenwidth\n        self.gamma = gamma\n        self.skew = skew\n\n    def graph_bgcolor(self):\n        return self.hsl_to_rgb(*self.bgcolor)\n\n    def graph_fontname(self):\n        return self.fontname\n\n    def graph_fontcolor(self):\n        return self.fontcolor\n\n    def graph_fontsize(self):\n        return self.minfontsize\n\n    def node_bgcolor(self, weight):\n        return self.color(weight)\n\n    def node_fgcolor(self, weight):\n        if self.nodestyle == ""filled"":\n            return self.graph_bgcolor()\n        else:\n            return self.color(weight)\n\n    def node_fontsize(self, weight):\n        return self.fontsize(weight)\n\n    def node_style(self):\n        return self.nodestyle\n\n    def edge_color(self, weight):\n        return self.color(weight)\n\n    def edge_fontsize(self, weight):\n        return self.fontsize(weight)\n\n    def edge_penwidth(self, weight):\n        return max(weight*self.maxpenwidth, self.minpenwidth)\n\n    def edge_arrowsize(self, weight):\n        return 0.5 * math.sqrt(self.edge_penwidth(weight))\n\n    def fontsize(self, weight):\n        return max(weight**2 * self.maxfontsize, self.minfontsize)\n\n    def color(self, weight):\n        weight = min(max(weight, 0.0), 1.0)\n    \n        hmin, smin, lmin = self.mincolor\n        hmax, smax, lmax = self.maxcolor\n        \n        if self.skew < 0:\n            raise ValueError(""Skew must be greater than 0"")\n        elif self.skew == 1.0:\n            h = hmin + weight*(hmax - hmin)\n            s = smin + weight*(smax - smin)\n            l = lmin + weight*(lmax - lmin)\n        else:\n            base = self.skew\n            h = hmin + ((hmax-hmin)*(-1.0 + (base ** weight)) / (base - 1.0))\n            s = smin + ((smax-smin)*(-1.0 + (base ** weight)) / (base - 1.0))\n            l = lmin + ((lmax-lmin)*(-1.0 + (base ** weight)) / (base - 1.0))\n\n        return self.hsl_to_rgb(h, s, l)\n\n    def hsl_to_rgb(self, h, s, l):\n        """"""Convert a color from HSL color-model to RGB.\n\n        See also:\n        - http://www.w3.org/TR/css3-color/#hsl-color\n        """"""\n\n        h = h % 1.0\n        s = min(max(s, 0.0), 1.0)\n        l = min(max(l, 0.0), 1.0)\n\n        if l <= 0.5:\n            m2 = l*(s + 1.0)\n        else:\n            m2 = l + s - l*s\n        m1 = l*2.0 - m2\n        r = self._hue_to_rgb(m1, m2, h + 1.0/3.0)\n        g = self._hue_to_rgb(m1, m2, h)\n        b = self._hue_to_rgb(m1, m2, h - 1.0/3.0)\n\n        # Apply gamma correction\n        r **= self.gamma\n        g **= self.gamma\n        b **= self.gamma\n\n        return (r, g, b)\n\n    def _hue_to_rgb(self, m1, m2, h):\n        if h < 0.0:\n            h += 1.0\n        elif h > 1.0:\n            h -= 1.0\n        if h*6 < 1.0:\n            return m1 + (m2 - m1)*h*6.0\n        elif h*2 < 1.0:\n            return m2\n        elif h*3 < 2.0:\n            return m1 + (m2 - m1)*(2.0/3.0 - h)*6.0\n        else:\n            return m1\n\n\nTEMPERATURE_COLORMAP = Theme(\n    mincolor = (2.0/3.0, 0.80, 0.25), # dark blue\n    maxcolor = (0.0, 1.0, 0.5), # satured red\n    gamma = 1.0\n)\n\nPINK_COLORMAP = Theme(\n    mincolor = (0.0, 1.0, 0.90), # pink\n    maxcolor = (0.0, 1.0, 0.5), # satured red\n)\n\nGRAY_COLORMAP = Theme(\n    mincolor = (0.0, 0.0, 0.85), # light gray\n    maxcolor = (0.0, 0.0, 0.0), # black\n)\n\nBW_COLORMAP = Theme(\n    minfontsize = 8.0,\n    maxfontsize = 24.0,\n    mincolor = (0.0, 0.0, 0.0), # black\n    maxcolor = (0.0, 0.0, 0.0), # black\n    minpenwidth = 0.1,\n    maxpenwidth = 8.0,\n)\n\nPRINT_COLORMAP = Theme(\n    minfontsize = 18.0,\n    maxfontsize = 30.0,\n    fontcolor = ""black"",\n    nodestyle = ""solid"",\n    mincolor = (0.0, 0.0, 0.0), # black\n    maxcolor = (0.0, 0.0, 0.0), # black\n    minpenwidth = 0.1,\n    maxpenwidth = 8.0,\n)\n\n\nthemes = {\n    ""color"": TEMPERATURE_COLORMAP,\n    ""pink"": PINK_COLORMAP,\n    ""gray"": GRAY_COLORMAP,\n    ""bw"": BW_COLORMAP,\n    ""print"": PRINT_COLORMAP,\n}\n\n\ndef sorted_iteritems(d):\n    # Used mostly for result reproducibility (while testing.)\n    keys = compat_keys(d)\n    keys.sort()\n    for key in keys:\n        value = d[key]\n        yield key, value\n\n\nclass DotWriter:\n    """"""Writer for the DOT language.\n\n    See also:\n    - ""The DOT Language"" specification\n      http://www.graphviz.org/doc/info/lang.html\n    """"""\n\n    strip = False\n    wrap = False\n\n    def __init__(self, fp):\n        self.fp = fp\n\n    def wrap_function_name(self, name):\n        """"""Split the function name on multiple lines.""""""\n\n        if len(name) > 32:\n            ratio = 2.0/3.0\n            height = max(int(len(name)/(1.0 - ratio) + 0.5), 1)\n            width = max(len(name)/height, 32)\n            # TODO: break lines in symbols\n            name = textwrap.fill(name, width, break_long_words=False)\n\n        # Take away spaces\n        name = name.replace("", "", "","")\n        name = name.replace(""> >"", "">>"")\n        name = name.replace(""> >"", "">>"") # catch consecutive\n\n        return name\n\n    show_function_events = [TOTAL_TIME_RATIO, TIME_RATIO]\n    show_edge_events = [TOTAL_TIME_RATIO, CALLS]\n\n    def graph(self, profile, theme):\n        self.begin_graph()\n\n        fontname = theme.graph_fontname()\n        fontcolor = theme.graph_fontcolor()\n        nodestyle = theme.node_style()\n\n        self.attr(\'graph\', fontname=fontname, ranksep=0.25, nodesep=0.125)\n        self.attr(\'node\', fontname=fontname, shape=""box"", style=nodestyle, fontcolor=fontcolor, width=0, height=0)\n        self.attr(\'edge\', fontname=fontname)\n\n        for _, function in sorted_iteritems(profile.functions):\n            labels = []\n            if function.process is not None:\n                labels.append(function.process)\n            if function.module is not None:\n                labels.append(function.module)\n\n            if self.strip:\n                function_name = function.stripped_name()\n            else:\n                function_name = function.name\n            if self.wrap:\n                function_name = self.wrap_function_name(function_name)\n            labels.append(function_name)\n\n            for event in self.show_function_events:\n                if event in function.events:\n                    label = event.format(function[event])\n                    labels.append(label)\n            if function.called is not None:\n                labels.append(""%u%s"" % (function.called, MULTIPLICATION_SIGN))\n\n            if function.weight is not None:\n                weight = function.weight\n            else:\n                weight = 0.0\n\n            label = \'\\n\'.join(labels)\n            self.node(function.id, \n                label = label, \n                color = self.color(theme.node_bgcolor(weight)), \n                fontcolor = self.color(theme.node_fgcolor(weight)), \n                fontsize = ""%.2f"" % theme.node_fontsize(weight),\n                tooltip = function.filename,\n            )\n\n            for _, call in sorted_iteritems(function.calls):\n                callee = profile.functions[call.callee_id]\n\n                labels = []\n                for event in self.show_edge_events:\n                    if event in call.events:\n                        label = event.format(call[event])\n                        labels.append(label)\n\n                if call.weight is not None:\n                    weight = call.weight\n                elif callee.weight is not None:\n                    weight = callee.weight\n                else:\n                    weight = 0.0\n\n                label = \'\\n\'.join(labels)\n\n                self.edge(function.id, call.callee_id, \n                    label = label, \n                    color = self.color(theme.edge_color(weight)), \n                    fontcolor = self.color(theme.edge_color(weight)),\n                    fontsize = ""%.2f"" % theme.edge_fontsize(weight), \n                    penwidth = ""%.2f"" % theme.edge_penwidth(weight), \n                    labeldistance = ""%.2f"" % theme.edge_penwidth(weight), \n                    arrowsize = ""%.2f"" % theme.edge_arrowsize(weight),\n                )\n\n        self.end_graph()\n\n    def begin_graph(self):\n        self.write(\'digraph {\\n\')\n\n    def end_graph(self):\n        self.write(\'}\\n\')\n\n    def attr(self, what, **attrs):\n        self.write(""\\t"")\n        self.write(what)\n        self.attr_list(attrs)\n        self.write("";\\n"")\n\n    def node(self, node, **attrs):\n        self.write(""\\t"")\n        self.id(node)\n        self.attr_list(attrs)\n        self.write("";\\n"")\n\n    def edge(self, src, dst, **attrs):\n        self.write(""\\t"")\n        self.id(src)\n        self.write("" -> "")\n        self.id(dst)\n        self.attr_list(attrs)\n        self.write("";\\n"")\n\n    def attr_list(self, attrs):\n        if not attrs:\n            return\n        self.write(\' [\')\n        first = True\n        for name, value in sorted_iteritems(attrs):\n            if value is None:\n                continue\n            if first:\n                first = False\n            else:\n                self.write("", "")\n            self.id(name)\n            self.write(\'=\')\n            self.id(value)\n        self.write(\']\')\n\n    def id(self, id):\n        if isinstance(id, (int, float)):\n            s = str(id)\n        elif isinstance(id, basestring):\n            if id.isalnum() and not id.startswith(\'0x\'):\n                s = id\n            else:\n                s = self.escape(id)\n        else:\n            raise TypeError\n        self.write(s)\n\n    def color(self, rgb):\n        r, g, b = rgb\n\n        def float2int(f):\n            if f <= 0.0:\n                return 0\n            if f >= 1.0:\n                return 255\n            return int(255.0*f + 0.5)\n\n        return ""#"" + """".join([""%02x"" % float2int(c) for c in (r, g, b)])\n\n    def escape(self, s):\n        if not PYTHON_3:\n            s = s.encode(\'utf-8\')\n        s = s.replace(\'\\\\\', r\'\\\\\')\n        s = s.replace(\'\\n\', r\'\\n\')\n        s = s.replace(\'\\t\', r\'\\t\')\n        s = s.replace(\'""\', r\'\\""\')\n        return \'""\' + s + \'""\'\n\n    def write(self, s):\n        self.fp.write(s)\n\n\n\n########################################################################\n# Main program\n\n\ndef naturalJoin(values):\n    if len(values) >= 2:\n        return \', \'.join(values[:-1]) + \' or \' + values[-1]\n\n    else:\n        return \'\'.join(values)\n\n\ndef main():\n    """"""Main program.""""""\n\n    global totalMethod\n\n    formatNames = list(formats.keys())\n    formatNames.sort()\n\n    optparser = optparse.OptionParser(\n        usage=""\\n\\t%prog [options] [file] ..."")\n    optparser.add_option(\n        \'-o\', \'--output\', metavar=\'FILE\',\n        type=""string"", dest=""output"",\n        help=""output filename [stdout]"")\n    optparser.add_option(\n        \'-n\', \'--node-thres\', metavar=\'PERCENTAGE\',\n        type=""float"", dest=""node_thres"", default=0.5,\n        help=""eliminate nodes below this threshold [default: %default]"")\n    optparser.add_option(\n        \'-e\', \'--edge-thres\', metavar=\'PERCENTAGE\',\n        type=""float"", dest=""edge_thres"", default=0.1,\n        help=""eliminate edges below this threshold [default: %default]"")\n    optparser.add_option(\n        \'-f\', \'--format\',\n        type=""choice"", choices=formatNames,\n        dest=""format"", default=""prof"",\n        help=""profile format: %s [default: %%default]"" % naturalJoin(formatNames))\n    optparser.add_option(\n        \'--total\',\n        type=""choice"", choices=(\'callratios\', \'callstacks\'),\n        dest=""totalMethod"", default=totalMethod,\n        help=""preferred method of calculating total time: callratios or callstacks (currently affects only perf format) [default: %default]"")\n    optparser.add_option(\n        \'-c\', \'--colormap\',\n        type=""choice"", choices=(\'color\', \'pink\', \'gray\', \'bw\', \'print\'),\n        dest=""theme"", default=""color"",\n        help=""color map: color, pink, gray, bw, or print [default: %default]"")\n    optparser.add_option(\n        \'-s\', \'--strip\',\n        action=""store_true"",\n        dest=""strip"", default=False,\n        help=""strip function parameters, template parameters, and const modifiers from demangled C++ function names"")\n    optparser.add_option(\n        \'--colour-nodes-by-selftime\',\n        action=""store_true"",\n        dest=""colour_nodes_by_selftime"", default=False,\n        help=""colour nodes by self time, rather than by total time (sum of self and descendants)"")\n    optparser.add_option(\n        \'-w\', \'--wrap\',\n        action=""store_true"",\n        dest=""wrap"", default=False,\n        help=""wrap function names"")\n    optparser.add_option(\n        \'--show-samples\',\n        action=""store_true"",\n        dest=""show_samples"", default=False,\n        help=""show function samples"")\n    # add option to create subtree or show paths\n    optparser.add_option(\n        \'-z\', \'--root\',\n        type=""string"",\n        dest=""root"", default="""",\n        help=""prune call graph to show only descendants of specified root function"")\n    optparser.add_option(\n        \'-l\', \'--leaf\',\n        type=""string"",\n        dest=""leaf"", default="""",\n        help=""prune call graph to show only ancestors of specified leaf function"")\n    # add a new option to control skew of the colorization curve\n    optparser.add_option(\n        \'--skew\',\n        type=""float"", dest=""theme_skew"", default=1.0,\n        help=""skew the colorization curve.  Values < 1.0 give more variety to lower percentages.  Values > 1.0 give less variety to lower percentages"")\n    (options, args) = optparser.parse_args(sys.argv[1:])\n\n    if len(args) > 1 and options.format != \'pstats\':\n        optparser.error(\'incorrect number of arguments\')\n\n    try:\n        theme = themes[options.theme]\n    except KeyError:\n        optparser.error(\'invalid colormap \\\'%s\\\'\' % options.theme)\n\n    # set skew on the theme now that it has been picked.\n    if options.theme_skew:\n        theme.skew = options.theme_skew\n\n    totalMethod = options.totalMethod\n\n    try:\n        Format = formats[options.format]\n    except KeyError:\n        optparser.error(\'invalid format \\\'%s\\\'\' % options.format)\n\n    if Format.stdinInput:\n        if not args:\n            fp = sys.stdin\n        elif PYTHON_3:\n            fp = open(args[0], \'rt\', encoding=\'UTF-8\')\n        else:\n            fp = open(args[0], \'rt\')\n        parser = Format(fp)\n    elif Format.multipleInput:\n        if not args:\n            optparser.error(\'at least a file must be specified for %s input\' % options.format)\n        parser = Format(*args)\n    else:\n        if len(args) != 1:\n            optparser.error(\'exactly one file must be specified for %s input\' % options.format)\n        parser = Format(args[0])\n\n    profile = parser.parse()\n\n    if options.output is None:\n        if PYTHON_3:\n            output = open(sys.stdout.fileno(), mode=\'wt\', encoding=\'UTF-8\', closefd=False)\n        else:\n            output = sys.stdout\n    else:\n        if PYTHON_3:\n            output = open(options.output, \'wt\', encoding=\'UTF-8\')\n        else:\n            output = open(options.output, \'wt\')\n\n    dot = DotWriter(output)\n    dot.strip = options.strip\n    dot.wrap = options.wrap\n    if options.show_samples:\n        dot.show_function_events.append(SAMPLES)\n\n    profile = profile\n    profile.prune(options.node_thres/100.0, options.edge_thres/100.0, options.colour_nodes_by_selftime)\n\n    if options.root:\n        rootId = profile.getFunctionId(options.root)\n        if not rootId:\n            sys.stderr.write(\'root node \' + options.root + \' not found (might already be pruned : try -e0 -n0 flags)\\n\')\n            sys.exit(1)\n        profile.prune_root(rootId)\n    if options.leaf:\n        leafId = profile.getFunctionId(options.leaf)\n        if not leafId:\n            sys.stderr.write(\'leaf node \' + options.leaf + \' not found (maybe already pruned : try -e0 -n0 flags)\\n\')\n            sys.exit(1)\n        profile.prune_leaf(leafId)\n\n    dot.graph(profile, theme)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
experiments/scripts/kitti2pascalvoc.py,0,"b'import sys\nimport argparse\nfrom xml.dom.minidom import Document\nimport cv2, os\nimport glob\nimport xml.etree.ElementTree as ET\nimport shutil\nimport numpy as np\n\ndef generate_xml(name, lines, img_size = (370, 1224, 3), \\\n                 class_sets = (\'pedestrian\', \'car\', \'cyclist\'), \\\n                 doncateothers = True):\n    """"""\n    Write annotations into voc xml format.\n    Examples:\n        In: 0000001.txt\n            cls        truncated    occlusion   angle   boxes                         3d annotation...\n            Pedestrian 0.00         0           -0.20   712.40 143.00 810.73 307.92   1.89 0.48 1.20 1.84 1.47 8.41 0.01\n        Out: 0000001.xml\n            <annotation>\n                <folder>VOC2007</folder>\n\t            <filename>000001.jpg</filename>\n\t            <source>\n\t            ...\n\t            <object>\n                    <name>Pedestrian</name>\n                    <pose>Left</pose>\n                    <truncated>1</truncated>\n                    <difficult>0</difficult>\n                    <bndbox>\n                        <xmin>x1</xmin>\n                        <ymin>y1</ymin>\n                        <xmax>x2</xmax>\n                        <ymax>y2</ymax>\n                    </bndbox>\n            \t</object>\n            </annotation>\n    :param name: stem name of an image, example: 0000001\n    :param lines: lines in kitti annotation txt\n    :param img_size: [height, width, channle]\n    :param class_sets: (\'Pedestrian\', \'Car\', \'Cyclist\')\n    :return:\n    """"""\n\n    doc = Document()\n\n    def append_xml_node_attr(child, parent = None, text = None):\n        ele = doc.createElement(child)\n        if not text is None:\n            text_node = doc.createTextNode(text)\n            ele.appendChild(text_node)\n        parent = doc if parent is None else parent\n        parent.appendChild(ele)\n        return ele\n\n    img_name = name+\'.jpg\'\n\n    # create header\n    annotation = append_xml_node_attr(\'annotation\')\n    append_xml_node_attr(\'folder\', parent = annotation, text=\'KITTI\')\n    append_xml_node_attr(\'filename\', parent = annotation, text=img_name)\n    source = append_xml_node_attr(\'source\', parent=annotation)\n    append_xml_node_attr(\'database\', parent=source, text=\'KITTI\')\n    append_xml_node_attr(\'annotation\', parent=source, text=\'KITTI\')\n    append_xml_node_attr(\'image\', parent=source, text=\'KITTI\')\n    append_xml_node_attr(\'flickrid\', parent=source, text=\'000000\')\n    owner = append_xml_node_attr(\'owner\', parent=annotation)\n    append_xml_node_attr(\'url\', parent=owner, text = \'http://www.cvlibs.net/datasets/kitti/index.php\')\n    size = append_xml_node_attr(\'size\', annotation)\n    append_xml_node_attr(\'width\', size, str(img_size[1]))\n    append_xml_node_attr(\'height\', size, str(img_size[0]))\n    append_xml_node_attr(\'depth\', size, str(img_size[2]))\n    append_xml_node_attr(\'segmented\', parent=annotation, text=\'0\')\n\n    # create objects\n    objs = []\n    for line in lines:\n        splitted_line = line.strip().lower().split()\n        cls = splitted_line[0].lower()\n        if not doncateothers and cls not in class_sets:\n            continue\n        cls = \'dontcare\' if cls not in class_sets else cls\n        obj = append_xml_node_attr(\'object\', parent=annotation)\n        occlusion = int(float(splitted_line[2]))\n        x1, y1, x2, y2 = int(float(splitted_line[4]) + 1), int(float(splitted_line[5]) + 1), \\\n                         int(float(splitted_line[6]) + 1), int(float(splitted_line[7]) + 1)\n        truncation = float(splitted_line[1])\n        difficult = 1 if _is_hard(cls, truncation, occlusion, x1, y1, x2, y2) else 0\n        truncted = 0 if truncation < 0.5 else 1\n\n        append_xml_node_attr(\'name\', parent=obj, text=cls)\n        append_xml_node_attr(\'pose\', parent=obj, text=\'Left\')\n        append_xml_node_attr(\'truncated\', parent=obj, text=str(truncted))\n        append_xml_node_attr(\'difficult\', parent=obj, text=str(int(difficult)))\n        bb = append_xml_node_attr(\'bndbox\', parent=obj)\n        append_xml_node_attr(\'xmin\', parent=bb, text=str(x1))\n        append_xml_node_attr(\'ymin\', parent=bb, text=str(y1))\n        append_xml_node_attr(\'xmax\', parent=bb, text=str(x2))\n        append_xml_node_attr(\'ymax\', parent=bb, text=str(y2))\n\n        o = {\'class\': cls, \'box\': np.asarray([x1, y1, x2, y2], dtype=float), \\\n             \'truncation\': truncation, \'difficult\': difficult, \'occlusion\': occlusion}\n        objs.append(o)\n\n    return  doc, objs\n\ndef _is_hard(cls, truncation, occlusion, x1, y1, x2, y2):\n    # Easy: Min. bounding box height: 40 Px, Max. occlusion level: Fully visible, Max. truncation: 15 %\n    # Moderate: Min. bounding box height: 25 Px, Max. occlusion level: Partly occluded, Max. truncation: 30 %\n    # Hard: Min. bounding box height: 25 Px, Max. occlusion level: Difficult to see, Max. truncation: 50 %\n    hard = False\n    if y2 - y1 < 25 and occlusion >= 2:\n        hard = True\n        return hard\n    if occlusion >= 3:\n        hard = True\n        return hard\n    if truncation > 0.8:\n        hard = True\n        return hard\n    return hard\n\ndef parse_args():\n    """"""\n    Parse input arguments\n    """"""\n    parser = argparse.ArgumentParser(description=\'Convert KITTI dataset into Pascal voc format\')\n    parser.add_argument(\'--kitti\', dest=\'kitti\',\n                        help=\'path to kitti root\',\n                        default=\'./data/KITTI\', type=str)\n    parser.add_argument(\'--out\', dest=\'outdir\',\n                        help=\'path to voc-kitti\',\n                        default=\'./data/KITTIVOC\', type=str)\n    parser.add_argument(\'--draw\', dest=\'draw\',\n                        help=\'draw rects on images\',\n                        default=0, type=int)\n    parser.add_argument(\'--dontcareothers\', dest=\'dontcareothers\',\n                        help=\'ignore other categories, add them to dontcare rsgions\',\n                        default=1, type=int)\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        # sys.exit(1)\n    args = parser.parse_args()\n    return args\n\ndef build_voc_dirs(outdir):\n    """"""\n    Build voc dir structure:\n        VOC2007\n            |-- Annotations\n                    |-- ***.xml\n            |-- ImageSets\n                    |-- Layout\n                            |-- [test|train|trainval|val].txt\n                    |-- Main\n                            |-- class_[test|train|trainval|val].txt\n                    |-- Segmentation\n                            |-- [test|train|trainval|val].txt\n            |-- JPEGImages\n                    |-- ***.jpg\n            |-- SegmentationClass\n                    [empty]\n            |-- SegmentationObject\n                    [empty]\n    """"""\n    mkdir = lambda dir: os.makedirs(dir) if not os.path.exists(dir) else None\n    mkdir(outdir)\n    mkdir(os.path.join(outdir, \'Annotations\'))\n    mkdir(os.path.join(outdir, \'ImageSets\'))\n    mkdir(os.path.join(outdir, \'ImageSets\', \'Layout\'))\n    mkdir(os.path.join(outdir, \'ImageSets\', \'Main\'))\n    mkdir(os.path.join(outdir, \'ImageSets\', \'Segmentation\'))\n    mkdir(os.path.join(outdir, \'JPEGImages\'))\n    mkdir(os.path.join(outdir, \'SegmentationClass\'))\n    mkdir(os.path.join(outdir, \'SegmentationObject\'))\n\n    return os.path.join(outdir, \'Annotations\'), os.path.join(outdir, \'JPEGImages\'), os.path.join(outdir, \'ImageSets\', \'Main\')\n\ndef _draw_on_image(img, objs, class_sets_dict):\n    colors = [(86, 0, 240), (173, 225, 61), (54, 137, 255),\\\n              (151, 0, 255), (243, 223, 48), (0, 117, 255),\\\n              (58, 184, 14), (86, 67, 140), (121, 82, 6),\\\n              (174, 29, 128), (115, 154, 81), (86, 255, 234)]\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    for ind, obj in enumerate(objs):\n        if obj[\'box\'] is None: continue\n        x1, y1, x2, y2 = obj[\'box\'].astype(int)\n        cls_id = class_sets_dict[obj[\'class\']]\n        if obj[\'class\'] == \'dontcare\':\n            cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 1)\n            continue\n        cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), colors[cls_id % len(colors)], 1)\n        text = \'{:s}*|\'.format(obj[\'class\'][:3]) if obj[\'difficult\'] == 1 else \'{:s}|\'.format(obj[\'class\'][:3])\n        text += \'{:.1f}|\'.format(obj[\'truncation\'])\n        text += str(obj[\'occlusion\'])\n        cv2.putText(img, text, (x1-2, y2-2), font, 0.5, (255, 0, 255), 1)\n    return img\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n\n    _kittidir = args.kitti\n    _outdir = args.outdir\n    _draw = bool(args.draw)\n    _dest_label_dir, _dest_img_dir, _dest_set_dir = build_voc_dirs(_outdir)\n    _doncateothers = bool(args.dontcareothers)\n\n    # for kitti only provides training labels\n    for dset in [\'train\']:\n\n        _labeldir = os.path.join(_kittidir, \'training\', \'label_2\')\n        _imagedir = os.path.join(_kittidir, \'training\', \'image_2\')\n        """"""\n        class_sets = (\'pedestrian\', \'cyclist\', \'car\', \'person_sitting\', \'van\', \'truck\', \'tram\', \'misc\', \'dontcare\')\n        """"""\n        class_sets = (\'pedestrian\', \'cyclist\', \'car\', \'dontcare\')\n        class_sets_dict = dict((k, i) for i, k in enumerate(class_sets))\n        allclasses = {}\n        fs = [open(os.path.join(_dest_set_dir, cls + \'_\' + dset + \'.txt\'), \'w\') for cls in class_sets ]\n        ftrain = open(os.path.join(_dest_set_dir, dset + \'.txt\'), \'w\')\n\n        files = glob.glob(os.path.join(_labeldir, \'*.txt\'))\n        files.sort()\n        for file in files:\n            path, basename = os.path.split(file)\n            stem, ext = os.path.splitext(basename)\n            with open(file, \'r\') as f:\n                lines = f.readlines()\n            img_file = os.path.join(_imagedir, stem + \'.png\')\n            img = cv2.imread(img_file)\n            img_size = img.shape\n\n            doc, objs = generate_xml(stem, lines, img_size, class_sets=class_sets, doncateothers=_doncateothers)\n            if _draw:\n                _draw_on_image(img, objs, class_sets_dict)\n\n            cv2.imwrite(os.path.join(_dest_img_dir, stem + \'.jpg\'), img)\n            xmlfile = os.path.join(_dest_label_dir, stem + \'.xml\')\n            with open(xmlfile, \'w\') as f:\n                f.write(doc.toprettyxml(indent=\'\t\'))\n\n            ftrain.writelines(stem + \'\\n\')\n\n            # build [cls_train.txt]\n            # Car_train.txt: 0000xxx [1 | -1]\n            cls_in_image = set([o[\'class\'] for o in objs])\n\n            for obj in objs:\n                cls = obj[\'class\']\n                allclasses[cls] = 0 \\\n                    if not cls in allclasses.keys() else allclasses[cls] + 1\n\n            for cls in cls_in_image:\n                if cls in class_sets:\n                    fs[class_sets_dict[cls]].writelines(stem + \' 1\\n\')\n            for cls in class_sets:\n                if cls not in cls_in_image:\n                    fs[class_sets_dict[cls]].writelines(stem + \' -1\\n\')\n\n            if int(stem) % 100 == 0:\n                print(file)\n\n        (f.close() for f in fs)\n        ftrain.close()\n\n        print \'~~~~~~~~~~~~~~~~~~~\'\n        print allclasses\n        print \'~~~~~~~~~~~~~~~~~~~\'\n        shutil.copyfile(os.path.join(_dest_set_dir, \'train.txt\'), os.path.join(_dest_set_dir, \'val.txt\'))\n        shutil.copyfile(os.path.join(_dest_set_dir, \'train.txt\'), os.path.join(_dest_set_dir, \'trainval.txt\'))\n        for cls in class_sets:\n            shutil.copyfile(os.path.join(_dest_set_dir, cls + \'_train.txt\'),\n                            os.path.join(_dest_set_dir, cls + \'_trainval.txt\'))\n            shutil.copyfile(os.path.join(_dest_set_dir, cls + \'_train.txt\'),\n                            os.path.join(_dest_set_dir, cls + \'_val.txt\'))\n'"
lib/datasets/__init__.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n# TODO: make this fold self-contained, only depends on utils package\n\nfrom .imdb import imdb\nfrom .pascal_voc import pascal_voc\nfrom .pascal3d import pascal3d\nfrom .imagenet3d import imagenet3d\nfrom .kitti import kitti\nfrom .kitti_tracking import kitti_tracking\nfrom .nissan import nissan\nfrom .nthu import nthu\nfrom . import factory\n\n## NOTE: obsolete\nimport os.path as osp\nfrom .imdb import ROOT_DIR\nfrom .imdb import MATLAB\n\n# http://stackoverflow.com/questions/377017/test-if-executable-exists-in-python\ndef _which(program):\n    import os\n    def is_exe(fpath):\n        return os.path.isfile(fpath) and os.access(fpath, os.X_OK)\n\n    fpath, fname = os.path.split(program)\n    if fpath:\n        if is_exe(program):\n            return program\n    else:\n        for path in os.environ[""PATH""].split(os.pathsep):\n            path = path.strip(\'""\')\n            exe_file = os.path.join(path, program)\n            if is_exe(exe_file):\n                return exe_file\n\n    return None\n""""""\nif _which(MATLAB) is None:\n    msg = (""MATLAB command \'{}\' not found. ""\n           ""Please add \'{}\' to your PATH."").format(MATLAB, MATLAB)\n    raise EnvironmentError(msg)\n""""""\n'"
lib/datasets/coco.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n# from datasets.imdb import imdb\n# import datasets.ds_utils as ds_utils\n# from fast_rcnn.config import cfg\nimport os.path as osp\nimport sys\nimport os\nimport numpy as np\nimport scipy.sparse\nimport scipy.io as sio\nimport cPickle\nimport json\nimport uuid\n# COCO API\n# TODO: add this part into this project\nfrom ..pycocotools.coco import COCO\nfrom ..pycocotools.cocoeval import COCOeval\nfrom ..pycocotools import mask as COCOmask\n\nfrom .imdb import imdb\nimport ds_utils\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\n\ndef _filter_crowd_proposals(roidb, crowd_thresh):\n    """"""\n    Finds proposals that are inside crowd regions and marks them with\n    overlap = -1 (for all gt rois), which means they will be excluded from\n    training.\n    """"""\n    for ix, entry in enumerate(roidb):\n        overlaps = entry[\'gt_overlaps\'].toarray()\n        crowd_inds = np.where(overlaps.max(axis=1) == -1)[0]\n        non_gt_inds = np.where(entry[\'gt_classes\'] == 0)[0]\n        if len(crowd_inds) == 0 or len(non_gt_inds) == 0:\n            continue\n        iscrowd = [int(True) for _ in xrange(len(crowd_inds))]\n        crowd_boxes = ds_utils.xyxy_to_xywh(entry[\'boxes\'][crowd_inds, :])\n        non_gt_boxes = ds_utils.xyxy_to_xywh(entry[\'boxes\'][non_gt_inds, :])\n        ious = COCOmask.iou(non_gt_boxes, crowd_boxes, iscrowd)\n        bad_inds = np.where(ious.max(axis=1) > crowd_thresh)[0]\n        overlaps[non_gt_inds[bad_inds], :] = -1\n        roidb[ix][\'gt_overlaps\'] = scipy.sparse.csr_matrix(overlaps)\n    return roidb\n\nclass coco(imdb):\n    def __init__(self, image_set, year):\n        imdb.__init__(self, \'coco_\' + year + \'_\' + image_set)\n        # COCO specific config options\n        self.config = {\'top_k\' : 2000,\n                       \'use_salt\' : True,\n                       \'cleanup\' : True,\n                       \'crowd_thresh\' : 0.7,\n                       \'min_size\' : 2}\n        # name, paths\n        self._year = year\n        self._image_set = image_set\n        self._data_path = osp.join(cfg.DATA_DIR, \'coco\')\n        # load COCO API, classes, class <-> id mappings\n        self._COCO = COCO(self._get_ann_file())\n        cats = self._COCO.loadCats(self._COCO.getCatIds())\n        self._classes = tuple([\'__background__\'] + [c[\'name\'] for c in cats])\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._class_to_coco_cat_id = dict(zip([c[\'name\'] for c in cats],\n                                              self._COCO.getCatIds()))\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        self.set_proposal_method(\'selective_search\')\n        self.competition_mode(False)\n\n        # Some image sets are ""views"" (i.e. subsets) into others.\n        # For example, minival2014 is a random 5000 image subset of val2014.\n        # This mapping tells us where the view\'s images and proposals come from.\n        self._view_map = {\n            \'minival2014\' : \'val2014\',          # 5k val2014 subset\n            \'valminusminival2014\' : \'val2014\',  # val2014 \\setminus minival2014\n        }\n        coco_name = image_set + year  # e.g., ""val2014""\n        self._data_name = (self._view_map[coco_name]\n                           if self._view_map.has_key(coco_name)\n                           else coco_name)\n        # Dataset splits that have ground-truth annotations (test splits\n        # do not have gt annotations)\n        self._gt_splits = (\'train\', \'val\', \'minival\')\n\n    def _get_ann_file(self):\n        prefix = \'instances\' if self._image_set.find(\'test\') == -1 \\\n                             else \'image_info\'\n        return osp.join(self._data_path, \'annotations\',\n                        prefix + \'_\' + self._image_set + self._year + \'.json\')\n\n    def _load_image_set_index(self):\n        """"""\n        Load image ids.\n        """"""\n        image_ids = self._COCO.getImgIds()\n        return image_ids\n\n    def _get_widths(self):\n        anns = self._COCO.loadImgs(self._image_index)\n        widths = [ann[\'width\'] for ann in anns]\n        return widths\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self._image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        # Example image path for index=119993:\n        #   images/train2014/COCO_train2014_000000119993.jpg\n        file_name = (\'COCO_\' + self._data_name + \'_\' +\n                     str(index).zfill(12) + \'.jpg\')\n        image_path = osp.join(self._data_path, \'images\',\n                              self._data_name, file_name)\n        assert osp.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def selective_search_roidb(self):\n        return self._roidb_from_proposals(\'selective_search\')\n\n    def edge_boxes_roidb(self):\n        return self._roidb_from_proposals(\'edge_boxes_AR\')\n\n    def mcg_roidb(self):\n        return self._roidb_from_proposals(\'MCG\')\n\n    def _roidb_from_proposals(self, method):\n        """"""\n        Creates a roidb from pre-computed proposals of a particular methods.\n        """"""\n        top_k = self.config[\'top_k\']\n        cache_file = osp.join(self.cache_path, self.name +\n                              \'_{:s}_top{:d}\'.format(method, top_k) +\n                              \'_roidb.pkl\')\n\n        if osp.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{:s} {:s} roidb loaded from {:s}\'.format(self.name, method,\n                                                            cache_file)\n            return roidb\n\n        if self._image_set in self._gt_splits:\n            gt_roidb = self.gt_roidb()\n            method_roidb = self._load_proposals(method, gt_roidb)\n            roidb = imdb.merge_roidbs(gt_roidb, method_roidb)\n            # Make sure we don\'t use proposals that are contained in crowds\n            roidb = _filter_crowd_proposals(roidb, self.config[\'crowd_thresh\'])\n        else:\n            roidb = self._load_proposals(method, None)\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote {:s} roidb to {:s}\'.format(method, cache_file)\n        return roidb\n\n    def _load_proposals(self, method, gt_roidb):\n        """"""\n        Load pre-computed proposals in the format provided by Jan Hosang:\n        http://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-\n          computing/research/object-recognition-and-scene-understanding/how-\n          good-are-detection-proposals-really/\n        For MCG, use boxes from http://www.eecs.berkeley.edu/Research/Projects/\n          CS/vision/grouping/mcg/ and convert the file layout using\n        lib/datasets/tools/mcg_munge.py.\n        """"""\n        box_list = []\n        top_k = self.config[\'top_k\']\n        valid_methods = [\n            \'MCG\',\n            \'selective_search\',\n            \'edge_boxes_AR\',\n            \'edge_boxes_70\']\n        assert method in valid_methods\n\n        print \'Loading {} boxes\'.format(method)\n        for i, index in enumerate(self._image_index):\n            if i % 1000 == 0:\n                print \'{:d} / {:d}\'.format(i + 1, len(self._image_index))\n\n            box_file = osp.join(\n                cfg.DATA_DIR, \'coco_proposals\', method, \'mat\',\n                self._get_box_file(index))\n\n            raw_data = sio.loadmat(box_file)[\'boxes\']\n            boxes = np.maximum(raw_data - 1, 0).astype(np.uint16)\n            if method == \'MCG\':\n                # Boxes from the MCG website are in (y1, x1, y2, x2) order\n                boxes = boxes[:, (1, 0, 3, 2)]\n            # Remove duplicate boxes and very small boxes and then take top k\n            keep = ds_utils.unique_boxes(boxes)\n            boxes = boxes[keep, :]\n            keep = ds_utils.filter_small_boxes(boxes, self.config[\'min_size\'])\n            boxes = boxes[keep, :]\n            boxes = boxes[:top_k, :]\n            box_list.append(boxes)\n            # Sanity check\n            im_ann = self._COCO.loadImgs(index)[0]\n            width = im_ann[\'width\']\n            height = im_ann[\'height\']\n            ds_utils.validate_boxes(boxes, width=width, height=height)\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = osp.join(self.cache_path, self.name + \'_gt_roidb.pkl\')\n        if osp.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} gt roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = [self._load_coco_annotation(index)\n                    for index in self._image_index]\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote gt roidb to {}\'.format(cache_file)\n        return gt_roidb\n\n    def _load_coco_annotation(self, index):\n        """"""\n        Loads COCO bounding-box instance annotations. Crowd instances are\n        handled by marking their overlaps (with all categories) to -1. This\n        overlap value means that crowd ""instances"" are excluded from training.\n        """"""\n        im_ann = self._COCO.loadImgs(index)[0]\n        width = im_ann[\'width\']\n        height = im_ann[\'height\']\n\n        annIds = self._COCO.getAnnIds(imgIds=index, iscrowd=None)\n        objs = self._COCO.loadAnns(annIds)\n        # Sanitize bboxes -- some are invalid\n        valid_objs = []\n        for obj in objs:\n            x1 = np.max((0, obj[\'bbox\'][0]))\n            y1 = np.max((0, obj[\'bbox\'][1]))\n            x2 = np.min((width - 1, x1 + np.max((0, obj[\'bbox\'][2] - 1))))\n            y2 = np.min((height - 1, y1 + np.max((0, obj[\'bbox\'][3] - 1))))\n            if obj[\'area\'] > 0 and x2 >= x1 and y2 >= y1:\n                obj[\'clean_bbox\'] = [x1, y1, x2, y2]\n                valid_objs.append(obj)\n        objs = valid_objs\n        num_objs = len(objs)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        seg_areas = np.zeros((num_objs), dtype=np.float32)\n\n        # Lookup table to map from COCO category ids to our internal class\n        # indices\n        coco_cat_id_to_class_ind = dict([(self._class_to_coco_cat_id[cls],\n                                          self._class_to_ind[cls])\n                                         for cls in self._classes[1:]])\n\n        for ix, obj in enumerate(objs):\n            cls = coco_cat_id_to_class_ind[obj[\'category_id\']]\n            boxes[ix, :] = obj[\'clean_bbox\']\n            gt_classes[ix] = cls\n            seg_areas[ix] = obj[\'area\']\n            if obj[\'iscrowd\']:\n                # Set overlap to -1 for all classes for crowd objects\n                # so they will be excluded during training\n                overlaps[ix, :] = -1.0\n            else:\n                overlaps[ix, cls] = 1.0\n\n        ds_utils.validate_boxes(boxes, width=width, height=height)\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_overlaps\' : overlaps,\n                \'flipped\' : False,\n                \'seg_areas\' : seg_areas}\n\n    def _get_box_file(self, index):\n        # first 14 chars / first 22 chars / all chars + .mat\n        # COCO_val2014_0/COCO_val2014_000000447/COCO_val2014_000000447991.mat\n        file_name = (\'COCO_\' + self._data_name +\n                     \'_\' + str(index).zfill(12) + \'.mat\')\n        return osp.join(file_name[:14], file_name[:22], file_name)\n\n    def _print_detection_eval_metrics(self, coco_eval):\n        IoU_lo_thresh = 0.5\n        IoU_hi_thresh = 0.95\n        def _get_thr_ind(coco_eval, thr):\n            ind = np.where((coco_eval.params.iouThrs > thr - 1e-5) &\n                           (coco_eval.params.iouThrs < thr + 1e-5))[0][0]\n            iou_thr = coco_eval.params.iouThrs[ind]\n            assert np.isclose(iou_thr, thr)\n            return ind\n\n        ind_lo = _get_thr_ind(coco_eval, IoU_lo_thresh)\n        ind_hi = _get_thr_ind(coco_eval, IoU_hi_thresh)\n        # precision has dims (iou, recall, cls, area range, max dets)\n        # area range index 0: all area ranges\n        # max dets index 2: 100 per image\n        precision = \\\n            coco_eval.eval[\'precision\'][ind_lo:(ind_hi + 1), :, :, 0, 2]\n        ap_default = np.mean(precision[precision > -1])\n        print (\'~~~~ Mean and per-category AP @ IoU=[{:.2f},{:.2f}] \'\n               \'~~~~\').format(IoU_lo_thresh, IoU_hi_thresh)\n        print \'{:.1f}\'.format(100 * ap_default)\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == \'__background__\':\n                continue\n            # minus 1 because of __background__\n            precision = coco_eval.eval[\'precision\'][ind_lo:(ind_hi + 1), :, cls_ind - 1, 0, 2]\n            ap = np.mean(precision[precision > -1])\n            print \'{:.1f}\'.format(100 * ap)\n\n        print \'~~~~ Summary metrics ~~~~\'\n        coco_eval.summarize()\n\n    def _do_detection_eval(self, res_file, output_dir):\n        ann_type = \'bbox\'\n        coco_dt = self._COCO.loadRes(res_file)\n        coco_eval = COCOeval(self._COCO, coco_dt)\n        coco_eval.params.useSegm = (ann_type == \'segm\')\n        coco_eval.evaluate()\n        coco_eval.accumulate()\n        self._print_detection_eval_metrics(coco_eval)\n        eval_file = osp.join(output_dir, \'detection_results.pkl\')\n        with open(eval_file, \'wb\') as fid:\n            cPickle.dump(coco_eval, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'Wrote COCO eval results to: {}\'.format(eval_file)\n\n    def _coco_results_one_category(self, boxes, cat_id):\n        results = []\n        for im_ind, index in enumerate(self.image_index):\n            dets = boxes[im_ind].astype(np.float)\n            if dets == []:\n                continue\n            scores = dets[:, -1]\n            xs = dets[:, 0]\n            ys = dets[:, 1]\n            ws = dets[:, 2] - xs + 1\n            hs = dets[:, 3] - ys + 1\n            results.extend(\n              [{\'image_id\' : index,\n                \'category_id\' : cat_id,\n                \'bbox\' : [xs[k], ys[k], ws[k], hs[k]],\n                \'score\' : scores[k]} for k in xrange(dets.shape[0])])\n        return results\n\n    def _write_coco_results_file(self, all_boxes, res_file):\n        # [{""image_id"": 42,\n        #   ""category_id"": 18,\n        #   ""bbox"": [258.15,41.29,348.26,243.78],\n        #   ""score"": 0.236}, ...]\n        results = []\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == \'__background__\':\n                continue\n            print \'Collecting {} results ({:d}/{:d})\'.format(cls, cls_ind,\n                                                          self.num_classes - 1)\n            coco_cat_id = self._class_to_coco_cat_id[cls]\n            results.extend(self._coco_results_one_category(all_boxes[cls_ind],\n                                                           coco_cat_id))\n        print \'Writing results json to {}\'.format(res_file)\n        with open(res_file, \'w\') as fid:\n            json.dump(results, fid)\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        res_file = osp.join(output_dir, (\'detections_\' +\n                                         self._image_set +\n                                         self._year +\n                                         \'_results\'))\n        if self.config[\'use_salt\']:\n            res_file += \'_{}\'.format(str(uuid.uuid4()))\n        res_file += \'.json\'\n        self._write_coco_results_file(all_boxes, res_file)\n        # Only do evaluation on non-test sets\n        if self._image_set.find(\'test\') == -1:\n            self._do_detection_eval(res_file, output_dir)\n        # Optionally cleanup results json file\n        if self.config[\'cleanup\']:\n            os.remove(res_file)\n\n    def competition_mode(self, on):\n        if on:\n            self.config[\'use_salt\'] = False\n            self.config[\'cleanup\'] = False\n        else:\n            self.config[\'use_salt\'] = True\n            self.config[\'cleanup\'] = True\n'"
lib/datasets/ds_utils.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\n\ndef unique_boxes(boxes, scale=1.0):\n    """"""Return indices of unique boxes.""""""\n    v = np.array([1, 1e3, 1e6, 1e9])\n    hashes = np.round(boxes * scale).dot(v)\n    _, index = np.unique(hashes, return_index=True)\n    return np.sort(index)\n\ndef xywh_to_xyxy(boxes):\n    """"""Convert [x y w h] box format to [x1 y1 x2 y2] format.""""""\n    return np.hstack((boxes[:, 0:2], boxes[:, 0:2] + boxes[:, 2:4] - 1))\n\ndef xyxy_to_xywh(boxes):\n    """"""Convert [x1 y1 x2 y2] box format to [x y w h] format.""""""\n    return np.hstack((boxes[:, 0:2], boxes[:, 2:4] - boxes[:, 0:2] + 1))\n\ndef validate_boxes(boxes, width=0, height=0):\n    """"""Check that a set of boxes are valid.""""""\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    assert (x1 >= 0).all()\n    assert (y1 >= 0).all()\n    assert (x2 >= x1).all()\n    assert (y2 >= y1).all()\n    assert (x2 < width).all()\n    assert (y2 < height).all()\n\ndef filter_small_boxes(boxes, min_size):\n    w = boxes[:, 2] - boxes[:, 0]\n    h = boxes[:, 3] - boxes[:, 1]\n    keep = np.where((w >= min_size) & (h > min_size))[0]\n    return keep\n'"
lib/datasets/factory.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Factory method for easily getting imdbs by name.""""""\n\n__sets = {}\n\nimport numpy as np\n\nfrom .pascal_voc import pascal_voc\nfrom .imagenet3d import imagenet3d\nfrom .kitti import kitti\nfrom .kitti_tracking import kitti_tracking\nfrom .nthu import nthu\nfrom .coco import coco\nfrom .kittivoc import kittivoc\n\ndef _selective_search_IJCV_top_k(split, year, top_k):\n    """"""Return an imdb that uses the top k proposals from the selective search\n    IJCV code.\n    """"""\n    imdb = pascal_voc(split, year)\n    imdb.roidb_handler = imdb.selective_search_IJCV_roidb\n    imdb.config[\'top_k\'] = top_k\n    return imdb\n\n# Set up voc_<year>_<split> using selective search ""fast"" mode\nfor year in [\'2007\', \'2012\', \'0712\']:\n    for split in [\'train\', \'val\', \'trainval\', \'test\']:\n        name = \'voc_{}_{}\'.format(year, split)\n        __sets[name] = (lambda split=split, year=year:\n                pascal_voc(split, year))\n\n\n# Set up kittivoc\n    for split in [\'train\', \'val\', \'trainval\', \'test\']:\n        name = \'kittivoc_{}\'.format(split)\n        print name\n        __sets[name] = (lambda split=split: kittivoc(split))\n\n# # KITTI dataset\n# for split in [\'train\', \'val\', \'trainval\', \'test\']:\n#     name = \'kitti_{}\'.format(split)\n#     print name\n#     __sets[name] = (lambda split=split: kitti(split))\n\n# Set up coco_2014_<split>\nfor year in [\'2014\']:\n    for split in [\'train\', \'val\', \'minival\', \'valminusminival\']:\n        name = \'coco_{}_{}\'.format(year, split)\n        __sets[name] = (lambda split=split, year=year: coco(split, year))\n\n# Set up coco_2015_<split>\nfor year in [\'2015\']:\n    for split in [\'test\', \'test-dev\']:\n        name = \'coco_{}_{}\'.format(year, split)\n        __sets[name] = (lambda split=split, year=year: coco(split, year))\n\n# NTHU dataset\nfor split in [\'71\', \'370\']:\n    name = \'nthu_{}\'.format(split)\n    print name\n    __sets[name] = (lambda split=split: nthu(split))\n\n\ndef get_imdb(name):\n    """"""Get an imdb (image database) by name.""""""\n    if not __sets.has_key(name):\n        print (list_imdbs())\n        raise KeyError(\'Unknown dataset: {}\'.format(name))\n    return __sets[name]()\n\ndef list_imdbs():\n    """"""List all registered imdbs.""""""\n    return __sets.keys()\n'"
lib/datasets/imagenet3d.py,0,"b'import imagenet3d\nimport os\nimport PIL\nimport numpy as np\nimport scipy.sparse\nimport subprocess\nimport cPickle\nimport math\nimport sys\n\nfrom .imdb import imdb\nfrom .imdb import ROOT_DIR\nfrom ..utils.cython_bbox import bbox_overlaps\nfrom ..utils.boxes_grid import get_boxes_grid\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\nfrom ..rpn_msr.generate_anchors import generate_anchors\n# <<<< obsolete\n\n\nclass imagenet3d(imdb):\n    def __init__(self, image_set, imagenet3d_path=None):\n        imdb.__init__(self, \'imagenet3d_\' + image_set)\n        self._image_set = image_set\n        self._imagenet3d_path = self._get_default_path() if imagenet3d_path is None \\\n                            else imagenet3d_path\n        self._data_path = os.path.join(self._imagenet3d_path, \'Images\')\n        self._classes = (\'__background__\', \'aeroplane\', \'ashtray\', \'backpack\', \'basket\', \\\n             \'bed\', \'bench\', \'bicycle\', \'blackboard\', \'boat\', \'bookshelf\', \'bottle\', \'bucket\', \\\n             \'bus\', \'cabinet\', \'calculator\', \'camera\', \'can\', \'cap\', \'car\', \'cellphone\', \'chair\', \\\n             \'clock\', \'coffee_maker\', \'comb\', \'computer\', \'cup\', \'desk_lamp\', \'diningtable\', \\\n             \'dishwasher\', \'door\', \'eraser\', \'eyeglasses\', \'fan\', \'faucet\', \'filing_cabinet\', \\\n             \'fire_extinguisher\', \'fish_tank\', \'flashlight\', \'fork\', \'guitar\', \'hair_dryer\', \\\n             \'hammer\', \'headphone\', \'helmet\', \'iron\', \'jar\', \'kettle\', \'key\', \'keyboard\', \'knife\', \\\n             \'laptop\', \'lighter\', \'mailbox\', \'microphone\', \'microwave\', \'motorbike\', \'mouse\', \\\n             \'paintbrush\', \'pan\', \'pen\', \'pencil\', \'piano\', \'pillow\', \'plate\', \'pot\', \'printer\', \\\n             \'racket\', \'refrigerator\', \'remote_control\', \'rifle\', \'road_pole\', \'satellite_dish\', \\\n             \'scissors\', \'screwdriver\', \'shoe\', \'shovel\', \'sign\', \'skate\', \'skateboard\', \'slipper\', \\\n             \'sofa\', \'speaker\', \'spoon\', \'stapler\', \'stove\', \'suitcase\', \'teapot\', \'telephone\', \\\n             \'toaster\', \'toilet\', \'toothbrush\', \'train\', \'trash_bin\', \'trophy\', \'tub\', \'tvmonitor\', \\\n             \'vending_machine\', \'washing_machine\', \'watch\', \'wheelchair\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.JPEG\'\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        if cfg.IS_RPN:\n            self._roidb_handler = self.gt_roidb\n        else:\n            self._roidb_handler = self.region_proposal_roidb\n\n        self.config = {\'top_k\': 100000}\n\n        # statistics for computing recall\n        self._num_boxes_all = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_covered = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_proposal = 0\n\n        assert os.path.exists(self._imagenet3d_path), \\\n                \'imagenet3d path does not exist: {}\'.format(self._imagenet3d_path)\n        assert os.path.exists(self._data_path), \\\n                \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self.image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n\n        image_path = os.path.join(self._data_path, index + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        image_set_file = os.path.join(self._imagenet3d_path, \'Image_sets\', self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n                \'Path does not exist: {}\'.format(image_set_file)\n\n        with open(image_set_file) as f:\n            image_index = [x.rstrip(\'\\n\') for x in f.readlines()]\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where imagenet3d is expected to be installed.\n        """"""\n        return os.path.join(ROOT_DIR, \'data\', \'ImageNet3D\')\n\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n\n        cache_file = os.path.join(self.cache_path, self.name + \'_\' + cfg.SUBCLS_NAME + \'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} gt roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = [self._load_imagenet3d_annotation(index)\n                    for index in self.image_index]\n\n        if cfg.IS_RPN:\n            # print out recall\n            for i in xrange(1, self.num_classes):\n                print \'{}: Total number of boxes {:d}\'.format(self.classes[i], self._num_boxes_all[i])\n                print \'{}: Number of boxes covered {:d}\'.format(self.classes[i], self._num_boxes_covered[i])\n                print \'{}: Recall {:f}\'.format(self.classes[i], float(self._num_boxes_covered[i]) / float(self._num_boxes_all[i]))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote gt roidb to {}\'.format(cache_file)\n\n        return gt_roidb\n\n\n    def _load_imagenet3d_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from txt file in the imagenet3d format.\n        """"""\n\n        if self._image_set == \'test\' or self._image_set == \'test_1\' or self._image_set == \'test_2\':\n            lines = []\n        else:\n            filename = os.path.join(self._imagenet3d_path, \'Labels\', index + \'.txt\')\n            lines = []\n            with open(filename) as f:\n                for line in f:\n                    lines.append(line)\n\n        num_objs = len(lines)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.float32)\n        viewpoints = np.zeros((num_objs, 3), dtype=np.float32)          # azimuth, elevation, in-plane rotation\n        viewpoints_flipped = np.zeros((num_objs, 3), dtype=np.float32)  # azimuth, elevation, in-plane rotation\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n\n        for ix, line in enumerate(lines):\n            words = line.split()\n            assert len(words) == 5 or len(words) == 8, \'Wrong label format: {}\'.format(index)\n            cls = self._class_to_ind[words[0]]\n            boxes[ix, :] = [float(n) for n in words[1:5]]\n            gt_classes[ix] = cls\n            overlaps[ix, cls] = 1.0\n            if len(words) == 8:\n                viewpoints[ix, :] = [float(n) for n in words[5:8]]\n                # flip the viewpoint\n                viewpoints_flipped[ix, 0] = -viewpoints[ix, 0]  # azimuth\n                viewpoints_flipped[ix, 1] = viewpoints[ix, 1]   # elevation\n                viewpoints_flipped[ix, 2] = -viewpoints[ix, 2]  # in-plane rotation\n            else:\n                viewpoints[ix, :] = np.inf\n                viewpoints_flipped[ix, :] = np.inf\n\n        gt_subclasses = np.zeros((num_objs), dtype=np.int32)\n        gt_subclasses_flipped = np.zeros((num_objs), dtype=np.int32)\n        subindexes = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes_flipped = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        viewindexes_azimuth = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        viewindexes_azimuth_flipped = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        viewindexes_elevation = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        viewindexes_elevation_flipped = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        viewindexes_rotation = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        viewindexes_rotation_flipped = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        subindexes = scipy.sparse.csr_matrix(subindexes)\n        subindexes_flipped = scipy.sparse.csr_matrix(subindexes_flipped)\n        viewindexes_azimuth = scipy.sparse.csr_matrix(viewindexes_azimuth)\n        viewindexes_azimuth_flipped = scipy.sparse.csr_matrix(viewindexes_azimuth_flipped)\n        viewindexes_elevation = scipy.sparse.csr_matrix(viewindexes_elevation)\n        viewindexes_elevation_flipped = scipy.sparse.csr_matrix(viewindexes_elevation_flipped)\n        viewindexes_rotation = scipy.sparse.csr_matrix(viewindexes_rotation)\n        viewindexes_rotation_flipped = scipy.sparse.csr_matrix(viewindexes_rotation_flipped)\n\n        if cfg.IS_RPN:\n            if cfg.IS_MULTISCALE:\n                # compute overlaps between grid boxes and gt boxes in multi-scales\n                # rescale the gt boxes\n                boxes_all = np.zeros((0, 4), dtype=np.float32)\n                for scale in cfg.TRAIN.SCALES:\n                    boxes_all = np.vstack((boxes_all, boxes * scale))\n                gt_classes_all = np.tile(gt_classes, len(cfg.TRAIN.SCALES))\n\n                # compute grid boxes\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n                boxes_grid, _, _ = get_boxes_grid(image_height, image_width)\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(boxes_grid.astype(np.float), boxes_all.astype(np.float))\n        \n                # check how many gt boxes are covered by grids\n                if num_objs != 0:\n                    index = np.tile(range(num_objs), len(cfg.TRAIN.SCALES))\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes_all == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n                    index_covered = np.unique(index[fg_inds])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[index_covered] == i)[0])\n            else:\n                assert len(cfg.TRAIN.SCALES_BASE) == 1\n                scale = cfg.TRAIN.SCALES_BASE[0]\n                feat_stride = 16\n                # faster rcnn region proposal\n                base_size = 16\n                ratios = cfg.TRAIN.RPN_ASPECTS\n                scales = cfg.TRAIN.RPN_SCALES\n                anchors = generate_anchors(base_size, ratios, scales)\n                num_anchors = anchors.shape[0]\n\n                # image size\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n\n                # height and width of the heatmap\n                height = np.round((image_height * scale - 1) / 4.0 + 1)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n\n                width = np.round((image_width * scale - 1) / 4.0 + 1)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n\n                # gt boxes\n                gt_boxes = boxes * scale\n\n                # 1. Generate proposals from bbox deltas and shifted anchors\n                shift_x = np.arange(0, width) * feat_stride\n                shift_y = np.arange(0, height) * feat_stride\n                shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n                shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n                # add A anchors (1, A, 4) to\n                # cell K shifts (K, 1, 4) to get\n                # shift anchors (K, A, 4)\n                # reshape to (K*A, 4) shifted anchors\n                A = num_anchors\n                K = shifts.shape[0]\n                all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n                all_anchors = all_anchors.reshape((K * A, 4))\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(all_anchors.astype(np.float), gt_boxes.astype(np.float))\n        \n                # check how many gt boxes are covered by anchors\n                if num_objs != 0:\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_viewpoints\': viewpoints,\n                \'gt_viewpoints_flipped\': viewpoints_flipped,\n                \'gt_viewindexes_azimuth\': viewindexes_azimuth,\n                \'gt_viewindexes_azimuth_flipped\': viewindexes_azimuth_flipped,\n                \'gt_viewindexes_elevation\': viewindexes_elevation,\n                \'gt_viewindexes_elevation_flipped\': viewindexes_elevation_flipped,\n                \'gt_viewindexes_rotation\': viewindexes_rotation,\n                \'gt_viewindexes_rotation_flipped\': viewindexes_rotation_flipped,\n                \'gt_subclasses\': gt_subclasses,\n                \'gt_subclasses_flipped\': gt_subclasses_flipped,\n                \'gt_overlaps\' : overlaps,\n                \'gt_subindexes\': subindexes,\n                \'gt_subindexes_flipped\': subindexes_flipped,\n                \'flipped\' : False}\n\n\n    def region_proposal_roidb(self):\n        """"""\n        Return the database of regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_\' + cfg.REGION_PROPOSAL + \'_region_proposal_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n\n            print \'Loading region proposal network boxes...\'\n            model = cfg.REGION_PROPOSAL\n            rpn_roidb = self._load_rpn_roidb(gt_roidb, model)\n            print \'Region proposal network boxes loaded\'\n            roidb = imdb.merge_roidbs(rpn_roidb, gt_roidb)\n        else:\n            print \'Loading region proposal network boxes...\'\n            model = cfg.REGION_PROPOSAL\n            roidb = self._load_rpn_roidb(None, model)\n            print \'Region proposal network boxes loaded\'\n\n        print \'{} region proposals per image\'.format(self._num_boxes_proposal / len(self.image_index))\n\n        # print out recall\n        if self._image_set != \'test\':\n            for i in xrange(1, self.num_classes):\n                print \'{}: Total number of boxes {:d}\'.format(self.classes[i], self._num_boxes_all[i])\n                print \'{}: Number of boxes covered {:d}\'.format(self.classes[i], self._num_boxes_covered[i])\n                if self._num_boxes_all[i] > 0:\n                    print \'{}: Recall {:f}\'.format(self.classes[i], float(self._num_boxes_covered[i]) / float(self._num_boxes_all[i]))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb, model):\n\n        box_list = []\n        for ix, index in enumerate(self.image_index):\n            filename = os.path.join(self._imagenet3d_path, \'region_proposals\', model, index + \'.txt\')\n            assert os.path.exists(filename), \\\n                \'{} data not found at: {}\'.format(model, filename)\n            raw_data = np.loadtxt(filename, dtype=float)\n            if len(raw_data.shape) == 1:\n                if raw_data.size == 0:\n                    raw_data = raw_data.reshape((0, 5))\n                else:\n                    raw_data = raw_data.reshape((1, 5))\n\n            if model == \'selective_search\' or model == \'mcg\':\n                x1 = raw_data[:, 1].copy()\n                y1 = raw_data[:, 0].copy()\n                x2 = raw_data[:, 3].copy()\n                y2 = raw_data[:, 2].copy()\n            elif model == \'edge_boxes\':\n                x1 = raw_data[:, 0].copy()\n                y1 = raw_data[:, 1].copy()\n                x2 = raw_data[:, 2].copy() + raw_data[:, 0].copy()\n                y2 = raw_data[:, 3].copy() + raw_data[:, 1].copy()\n            elif model == \'rpn_caffenet\' or model == \'rpn_vgg16\':\n                x1 = raw_data[:, 0].copy()\n                y1 = raw_data[:, 1].copy()\n                x2 = raw_data[:, 2].copy()\n                y2 = raw_data[:, 3].copy()\n            else:\n                assert 1, \'region proposal not supported: {}\'.format(model)\n\n            inds = np.where((x2 > x1) & (y2 > y1))[0]\n            raw_data[:, 0] = x1\n            raw_data[:, 1] = y1\n            raw_data[:, 2] = x2\n            raw_data[:, 3] = y2\n            raw_data = raw_data[inds,:4]\n\n            self._num_boxes_proposal += raw_data.shape[0]\n            box_list.append(raw_data)\n            print \'load {}: {}\'.format(model, index)\n\n            if gt_roidb is not None:\n                # compute overlaps between region proposals and gt boxes\n                boxes = gt_roidb[ix][\'boxes\'].copy()\n                gt_classes = gt_roidb[ix][\'gt_classes\'].copy()\n                # compute overlap\n                overlaps = bbox_overlaps(raw_data.astype(np.float), boxes.astype(np.float))\n                # check how many gt boxes are covered by anchors\n                if raw_data.shape[0] != 0:\n                    max_overlaps = overlaps.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n\n    def evaluate_detections(self, all_boxes, output_dir):\n\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing imagenet3d results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    # detection and viewpoint\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:s} {:f} {:f} {:f} {:f} {:.32f} {:f} {:f} {:f}\\n\'.format(\\\n                                 cls, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4], dets[k, 6], dets[k, 7], dets[k, 8]))\n\n    # write detection results into one file\n    def evaluate_detections_one_file(self, all_boxes, output_dir):\n\n        # for each class\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == \'__background__\':\n                continue\n            # open results file\n            filename = os.path.join(output_dir, \'detections_{}.txt\'.format(cls))\n            print \'Writing imagenet3d results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each image\n                for im_ind, index in enumerate(self.image_index):\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    # detection and viewpoint\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:s} {:f} {:f} {:f} {:f} {:.32f} {:f} {:f} {:f}\\n\'.format(\\\n                                 index, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4], dets[k, 6], dets[k, 7], dets[k, 8]))\n\n    def evaluate_proposals(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing imagenet3d results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(\\\n                                 dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    def evaluate_proposals_msr(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing imagenet3d results to file \' + filename\n            with open(filename, \'wt\') as f:\n                dets = all_boxes[im_ind]\n                if dets == []:\n                    continue\n                for k in xrange(dets.shape[0]):\n                    f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n\nif __name__ == \'__main__\':\n    d = imagenet3d(\'trainval\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
lib/datasets/imdb.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nimport os.path as osp\nimport PIL\nimport numpy as np\nimport scipy.sparse\n\nfrom ..utils.cython_bbox import bbox_overlaps\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\n\nROOT_DIR = osp.join(osp.dirname(__file__), \'..\', \'..\')\nMATLAB = \'matlab_r2013b\'\n\nclass imdb(object):\n    """"""Image database.""""""\n\n    def __init__(self, name):\n        self._name = name\n        self._num_classes = 0\n        self._classes = []\n        self._image_index = []\n        self._obj_proposer = \'selective_search\'\n        self._roidb = None\n        print self.default_roidb\n        self._roidb_handler = self.default_roidb\n        # Use this dict for storing dataset specific config options\n        self.config = {}\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def num_classes(self):\n        return len(self._classes)\n\n    @property\n    def classes(self):\n        return self._classes\n\n    @property\n    def image_index(self):\n        return self._image_index\n\n    @property\n    def roidb_handler(self):\n        return self._roidb_handler\n\n    @roidb_handler.setter\n    def roidb_handler(self, val):\n        self._roidb_handler = val\n\n    def set_proposal_method(self, method):\n        method = eval(\'self.\' + method + \'_roidb\')\n        self.roidb_handler = method\n\n    @property\n    def roidb(self):\n        # A roidb is a list of dictionaries, each with the following keys:\n        #   boxes\n        #   gt_overlaps\n        #   gt_classes\n        #   flipped\n        if self._roidb is not None:\n            return self._roidb\n        self._roidb = self.roidb_handler()\n        return self._roidb\n\n    @property\n    def cache_path(self):\n        cache_path = osp.abspath(osp.join(cfg.DATA_DIR, \'cache\'))\n        if not os.path.exists(cache_path):\n            os.makedirs(cache_path)\n        return cache_path\n\n    @property\n    def num_images(self):\n      return len(self.image_index)\n\n    def image_path_at(self, i):\n        raise NotImplementedError\n\n    def default_roidb(self):\n        raise NotImplementedError\n\n    def evaluate_detections(self, all_boxes, output_dir=None):\n        """"""\n        all_boxes is a list of length number-of-classes.\n        Each list element is a list of length number-of-images.\n        Each of those list elements is either an empty list []\n        or a numpy array of detection.\n\n        all_boxes[class][image] = [] or np.array of shape #dets x 5\n        """"""\n        raise NotImplementedError\n\n    def _get_widths(self):\n      return [PIL.Image.open(self.image_path_at(i)).size[0]\n              for i in xrange(self.num_images)]\n\n    def append_flipped_images(self):\n        num_images = self.num_images\n        widths = self._get_widths()\n        for i in xrange(num_images):\n            boxes = self.roidb[i][\'boxes\'].copy()\n            oldx1 = boxes[:, 0].copy()\n            oldx2 = boxes[:, 2].copy()\n            boxes[:, 0] = widths[i] - oldx2 - 1\n            boxes[:, 2] = widths[i] - oldx1 - 1\n            assert (boxes[:, 2] >= boxes[:, 0]).all()\n            entry = {\'boxes\' : boxes,\n                     \'gt_overlaps\' : self.roidb[i][\'gt_overlaps\'],\n                     \'gt_classes\' : self.roidb[i][\'gt_classes\'],\n                     \'flipped\' : True}\n\n            if \'gt_ishard\' in self.roidb[i] and \'dontcare_areas\' in self.roidb[i]:\n                entry[\'gt_ishard\'] = self.roidb[i][\'gt_ishard\'].copy()\n                dontcare_areas = self.roidb[i][\'dontcare_areas\'].copy()\n                oldx1 = dontcare_areas[:, 0].copy()\n                oldx2 = dontcare_areas[:, 2].copy()\n                dontcare_areas[:, 0] = widths[i] - oldx2 - 1\n                dontcare_areas[:, 2] = widths[i] - oldx1 - 1\n                entry[\'dontcare_areas\'] = dontcare_areas\n\n            self.roidb.append(entry)\n\n        self._image_index = self._image_index * 2\n\n    def evaluate_recall(self, candidate_boxes=None, thresholds=None,\n                        area=\'all\', limit=None):\n        """"""Evaluate detection proposal recall metrics.\n\n        Returns:\n            results: dictionary of results with keys\n                \'ar\': average recall\n                \'recalls\': vector recalls at each IoU overlap threshold\n                \'thresholds\': vector of IoU overlap thresholds\n                \'gt_overlaps\': vector of all ground-truth overlaps\n        """"""\n        # Record max overlap value for each gt box\n        # Return vector of overlap values\n        areas = { \'all\': 0, \'small\': 1, \'medium\': 2, \'large\': 3,\n                  \'96-128\': 4, \'128-256\': 5, \'256-512\': 6, \'512-inf\': 7}\n        area_ranges = [ [0**2, 1e5**2],    # all\n                        [0**2, 32**2],     # small\n                        [32**2, 96**2],    # medium\n                        [96**2, 1e5**2],   # large\n                        [96**2, 128**2],   # 96-128\n                        [128**2, 256**2],  # 128-256\n                        [256**2, 512**2],  # 256-512\n                        [512**2, 1e5**2],  # 512-inf\n                      ]\n        assert areas.has_key(area), \'unknown area range: {}\'.format(area)\n        area_range = area_ranges[areas[area]]\n        gt_overlaps = np.zeros(0)\n        num_pos = 0\n        for i in xrange(self.num_images):\n            # Checking for max_overlaps == 1 avoids including crowd annotations\n            # (...pretty hacking :/)\n            max_gt_overlaps = self.roidb[i][\'gt_overlaps\'].toarray().max(axis=1)\n            gt_inds = np.where((self.roidb[i][\'gt_classes\'] > 0) &\n                               (max_gt_overlaps == 1))[0]\n            gt_boxes = self.roidb[i][\'boxes\'][gt_inds, :]\n            gt_areas = self.roidb[i][\'seg_areas\'][gt_inds]\n            valid_gt_inds = np.where((gt_areas >= area_range[0]) &\n                                     (gt_areas <= area_range[1]))[0]\n            gt_boxes = gt_boxes[valid_gt_inds, :]\n            num_pos += len(valid_gt_inds)\n\n            if candidate_boxes is None:\n                # If candidate_boxes is not supplied, the default is to use the\n                # non-ground-truth boxes from this roidb\n                non_gt_inds = np.where(self.roidb[i][\'gt_classes\'] == 0)[0]\n                boxes = self.roidb[i][\'boxes\'][non_gt_inds, :]\n            else:\n                boxes = candidate_boxes[i]\n            if boxes.shape[0] == 0:\n                continue\n            if limit is not None and boxes.shape[0] > limit:\n                boxes = boxes[:limit, :]\n\n            overlaps = bbox_overlaps(boxes.astype(np.float),\n                                     gt_boxes.astype(np.float))\n\n            _gt_overlaps = np.zeros((gt_boxes.shape[0]))\n            for j in xrange(gt_boxes.shape[0]):\n                # find which proposal box maximally covers each gt box\n                argmax_overlaps = overlaps.argmax(axis=0)\n                # and get the iou amount of coverage for each gt box\n                max_overlaps = overlaps.max(axis=0)\n                # find which gt box is \'best\' covered (i.e. \'best\' = most iou)\n                gt_ind = max_overlaps.argmax()\n                gt_ovr = max_overlaps.max()\n                assert(gt_ovr >= 0)\n                # find the proposal box that covers the best covered gt box\n                box_ind = argmax_overlaps[gt_ind]\n                # record the iou coverage of this gt box\n                _gt_overlaps[j] = overlaps[box_ind, gt_ind]\n                assert(_gt_overlaps[j] == gt_ovr)\n                # mark the proposal box and the gt box as used\n                overlaps[box_ind, :] = -1\n                overlaps[:, gt_ind] = -1\n            # append recorded iou coverage level\n            gt_overlaps = np.hstack((gt_overlaps, _gt_overlaps))\n\n        gt_overlaps = np.sort(gt_overlaps)\n        if thresholds is None:\n            step = 0.05\n            thresholds = np.arange(0.5, 0.95 + 1e-5, step)\n        recalls = np.zeros_like(thresholds)\n        # compute recall for each iou threshold\n        for i, t in enumerate(thresholds):\n            recalls[i] = (gt_overlaps >= t).sum() / float(num_pos)\n        # ar = 2 * np.trapz(recalls, thresholds)\n        ar = recalls.mean()\n        return {\'ar\': ar, \'recalls\': recalls, \'thresholds\': thresholds,\n                \'gt_overlaps\': gt_overlaps}\n\n    def create_roidb_from_box_list(self, box_list, gt_roidb):\n        assert len(box_list) == self.num_images, \\\n                \'Number of boxes must match number of ground-truth images\'\n        roidb = []\n        for i in xrange(self.num_images):\n            boxes = box_list[i]\n            num_boxes = boxes.shape[0]\n            overlaps = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n\n            if gt_roidb is not None and gt_roidb[i][\'boxes\'].size > 0:\n                gt_boxes = gt_roidb[i][\'boxes\']\n                gt_classes = gt_roidb[i][\'gt_classes\']\n                gt_overlaps = bbox_overlaps(boxes.astype(np.float),\n                                            gt_boxes.astype(np.float))\n                argmaxes = gt_overlaps.argmax(axis=1)\n                maxes = gt_overlaps.max(axis=1)\n                I = np.where(maxes > 0)[0]\n                overlaps[I, gt_classes[argmaxes[I]]] = maxes[I]\n\n            overlaps = scipy.sparse.csr_matrix(overlaps)\n            roidb.append({\n                \'boxes\' : boxes,\n                \'gt_classes\' : np.zeros((num_boxes,), dtype=np.int32),\n                \'gt_overlaps\' : overlaps,\n                \'flipped\' : False,\n                \'seg_areas\' : np.zeros((num_boxes,), dtype=np.float32),\n            })\n        return roidb\n\n    @staticmethod\n    def merge_roidbs(a, b):\n        assert len(a) == len(b)\n        for i in xrange(len(a)):\n            a[i][\'boxes\'] = np.vstack((a[i][\'boxes\'], b[i][\'boxes\']))\n            a[i][\'gt_classes\'] = np.hstack((a[i][\'gt_classes\'],\n                                            b[i][\'gt_classes\']))\n            a[i][\'gt_overlaps\'] = scipy.sparse.vstack([a[i][\'gt_overlaps\'],\n                                                       b[i][\'gt_overlaps\']])\n            a[i][\'seg_areas\'] = np.hstack((a[i][\'seg_areas\'],\n                                           b[i][\'seg_areas\']))\n        return a\n\n    def competition_mode(self, on):\n        """"""Turn competition mode on or off.""""""\n        pass\n'"
lib/datasets/imdb2.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nimport os.path as osp\nimport PIL\nimport numpy as np\nimport scipy.sparse\n\nfrom . import ROOT_DIR\nfrom ..utils.cython_bbox import bbox_overlaps\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\n\nclass imdb(object):\n    """"""Image database.""""""\n\n    def __init__(self, name):\n        self._name = name\n        self._num_classes = 0\n        self._num_subclasses = 0\n        self._subclass_mapping = []\n        self._classes = []\n        self._image_index = []\n        self._obj_proposer = \'selective_search\'\n        self._roidb = None\n        self._roidb_handler = self.default_roidb\n        # Use this dict for storing dataset specific config options\n        self.config = {}\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def num_classes(self):\n        return len(self._classes)\n\n    @property\n    def num_subclasses(self):\n        return self._num_subclasses\n\n    @property\n    def subclass_mapping(self):\n        return self._subclass_mapping\n\n    @property\n    def classes(self):\n        return self._classes\n\n    @property\n    def image_index(self):\n        return self._image_index\n\n    @property\n    def roidb_handler(self):\n        return self._roidb_handler\n\n    @roidb_handler.setter\n    def roidb_handler(self, val):\n        self._roidb_handler = val\n\n    @property\n    def roidb(self):\n        # A roidb is a list of dictionaries, each with the following keys:\n        #   boxes\n        #   gt_overlaps\n        #   gt_classes\n        #   flipped\n        if self._roidb is not None:\n            return self._roidb\n        self._roidb = self.roidb_handler()\n        return self._roidb\n\n    @property\n    def cache_path(self):\n        cache_path = osp.abspath(osp.join(ROOT_DIR, \'data\', \'cache\'))\n        if not os.path.exists(cache_path):\n            os.makedirs(cache_path)\n        return cache_path\n\n    @property\n    def num_images(self):\n      return len(self.image_index)\n\n    def image_path_at(self, i):\n        raise NotImplementedError\n\n    def default_roidb(self):\n        raise NotImplementedError\n\n    def evaluate_detections(self, all_boxes, output_dir=None):\n        """"""\n        all_boxes is a list of length number-of-classes.\n        Each list element is a list of length number-of-images.\n        Each of those list elements is either an empty list []\n        or a numpy array of detection.\n\n        all_boxes[class][image] = [] or np.array of shape #dets x 5\n        """"""\n        raise NotImplementedError\n\n    def evaluate_proposals(self, all_boxes, output_dir=None):\n        """"""\n        all_boxes is a list of length number-of-classes.\n        Each list element is a list of length number-of-images.\n        Each of those list elements is either an empty list []\n        or a numpy array of detection.\n\n        all_boxes[class][image] = [] or np.array of shape #dets x 5\n        """"""\n        raise NotImplementedError\n\n    def append_flipped_images(self):\n        num_images = self.num_images\n        widths = [PIL.Image.open(self.image_path_at(i)).size[0]\n                  for i in xrange(num_images)]\n        for i in xrange(num_images):\n            boxes = self.roidb[i][\'boxes\'].copy()\n            oldx1 = boxes[:, 0].copy()\n            oldx2 = boxes[:, 2].copy()\n            boxes[:, 0] = widths[i] - oldx2 - 1\n            boxes[:, 2] = widths[i] - oldx1 - 1\n            assert (boxes[:, 2] >= boxes[:, 0]).all()\n            assert (self.roidb[i][\'gt_subindexes\'].shape == self.roidb[i][\'gt_subindexes_flipped\'].shape), \\\n                \'gt_subindexes {}, gt_subindexes_flip {}\'.format(self.roidb[i][\'gt_subindexes\'].shape, \n                                                                 self.roidb[i][\'gt_subindexes_flipped\'].shape)\n\n            if cfg.TRAIN.VIEWPOINT == True or cfg.TEST.VIEWPOINT == True:\n                entry = {\'boxes\' : boxes,\n                         \'gt_overlaps\' : self.roidb[i][\'gt_overlaps\'],\n                         \'gt_classes\' : self.roidb[i][\'gt_classes\'],\n                         \'gt_viewpoints\' : self.roidb[i][\'gt_viewpoints_flipped\'],\n                         \'gt_viewpoints_flipped\' : self.roidb[i][\'gt_viewpoints\'],\n                         \'gt_viewindexes_azimuth\' : self.roidb[i][\'gt_viewindexes_azimuth_flipped\'],\n                         \'gt_viewindexes_azimuth_flipped\' : self.roidb[i][\'gt_viewindexes_azimuth\'],\n                         \'gt_viewindexes_elevation\' : self.roidb[i][\'gt_viewindexes_elevation_flipped\'],\n                         \'gt_viewindexes_elevation_flipped\' : self.roidb[i][\'gt_viewindexes_elevation\'],\n                         \'gt_viewindexes_rotation\' : self.roidb[i][\'gt_viewindexes_rotation_flipped\'],\n                         \'gt_viewindexes_rotation_flipped\' : self.roidb[i][\'gt_viewindexes_rotation\'],\n                         \'gt_subclasses\' : self.roidb[i][\'gt_subclasses_flipped\'],\n                         \'gt_subclasses_flipped\' : self.roidb[i][\'gt_subclasses\'],\n                         \'gt_subindexes\' : self.roidb[i][\'gt_subindexes_flipped\'],\n                         \'gt_subindexes_flipped\' : self.roidb[i][\'gt_subindexes\'],\n                         \'flipped\' : True}\n            else:\n                entry = {\'boxes\' : boxes,\n                         \'gt_overlaps\' : self.roidb[i][\'gt_overlaps\'],\n                         \'gt_classes\' : self.roidb[i][\'gt_classes\'],\n                         \'gt_subclasses\' : self.roidb[i][\'gt_subclasses_flipped\'],\n                         \'gt_subclasses_flipped\' : self.roidb[i][\'gt_subclasses\'],\n                         \'gt_subindexes\' : self.roidb[i][\'gt_subindexes_flipped\'],\n                         \'gt_subindexes_flipped\' : self.roidb[i][\'gt_subindexes\'],\n                         \'flipped\' : True}\n            self.roidb.append(entry)\n        self._image_index = self._image_index * 2\n        print \'finish appending flipped images\'\n\n    def evaluate_recall(self, candidate_boxes, ar_thresh=0.5):\n        # Record max overlap value for each gt box\n        # Return vector of overlap values\n        gt_overlaps = np.zeros(0)\n        for i in xrange(self.num_images):\n            gt_inds = np.where(self.roidb[i][\'gt_classes\'] > 0)[0]\n            gt_boxes = self.roidb[i][\'boxes\'][gt_inds, :]\n\n            boxes = candidate_boxes[i]\n            if boxes.shape[0] == 0:\n                continue\n            overlaps = bbox_overlaps(boxes.astype(np.float),\n                                     gt_boxes.astype(np.float))\n\n            # gt_overlaps = np.hstack((gt_overlaps, overlaps.max(axis=0)))\n            _gt_overlaps = np.zeros((gt_boxes.shape[0]))\n            for j in xrange(gt_boxes.shape[0]):\n                argmax_overlaps = overlaps.argmax(axis=0)\n                max_overlaps = overlaps.max(axis=0)\n                gt_ind = max_overlaps.argmax()\n                gt_ovr = max_overlaps.max()\n                assert(gt_ovr >= 0)\n                box_ind = argmax_overlaps[gt_ind]\n                _gt_overlaps[j] = overlaps[box_ind, gt_ind]\n                assert(_gt_overlaps[j] == gt_ovr)\n                overlaps[box_ind, :] = -1\n                overlaps[:, gt_ind] = -1\n\n            gt_overlaps = np.hstack((gt_overlaps, _gt_overlaps))\n\n        num_pos = gt_overlaps.size\n        gt_overlaps = np.sort(gt_overlaps)\n        step = 0.001\n        thresholds = np.minimum(np.arange(0.5, 1.0 + step, step), 1.0)\n        recalls = np.zeros_like(thresholds)\n        for i, t in enumerate(thresholds):\n            recalls[i] = (gt_overlaps >= t).sum() / float(num_pos)\n        ar = 2 * np.trapz(recalls, thresholds)\n\n        return ar, gt_overlaps, recalls, thresholds\n\n    def create_roidb_from_box_list(self, box_list, gt_roidb):\n        assert len(box_list) == self.num_images, \\\n                \'Number of boxes must match number of ground-truth images\'\n        roidb = []\n        for i in xrange(self.num_images):\n            boxes = box_list[i]\n            num_boxes = boxes.shape[0]\n            overlaps = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n            subindexes = np.zeros((num_boxes, self.num_classes), dtype=np.int32)\n            subindexes_flipped = np.zeros((num_boxes, self.num_classes), dtype=np.int32)\n            if cfg.TRAIN.VIEWPOINT == True or cfg.TEST.VIEWPOINT == True:\n                viewindexes_azimuth = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n                viewindexes_azimuth_flipped = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n                viewindexes_elevation = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n                viewindexes_elevation_flipped = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n                viewindexes_rotation = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n                viewindexes_rotation_flipped = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n\n            if gt_roidb is not None:\n                gt_boxes = gt_roidb[i][\'boxes\']\n                if gt_boxes.shape[0] != 0 and num_boxes != 0:\n                    gt_classes = gt_roidb[i][\'gt_classes\']\n                    gt_subclasses = gt_roidb[i][\'gt_subclasses\']\n                    gt_subclasses_flipped = gt_roidb[i][\'gt_subclasses_flipped\']\n                    if cfg.TRAIN.VIEWPOINT == True or cfg.TEST.VIEWPOINT == True:\n                        gt_viewpoints = gt_roidb[i][\'gt_viewpoints\']\n                        gt_viewpoints_flipped = gt_roidb[i][\'gt_viewpoints_flipped\']\n                    gt_overlaps = bbox_overlaps(boxes.astype(np.float),\n                                            gt_boxes.astype(np.float))\n                    argmaxes = gt_overlaps.argmax(axis=1)\n                    maxes = gt_overlaps.max(axis=1)\n                    I = np.where(maxes > 0)[0]\n                    overlaps[I, gt_classes[argmaxes[I]]] = maxes[I]\n                    subindexes[I, gt_classes[argmaxes[I]]] = gt_subclasses[argmaxes[I]]\n                    subindexes_flipped[I, gt_classes[argmaxes[I]]] = gt_subclasses_flipped[argmaxes[I]]\n                    if cfg.TRAIN.VIEWPOINT == True or cfg.TEST.VIEWPOINT == True:\n                        viewindexes_azimuth[I, gt_classes[argmaxes[I]]] = gt_viewpoints[argmaxes[I], 0]\n                        viewindexes_azimuth_flipped[I, gt_classes[argmaxes[I]]] = gt_viewpoints_flipped[argmaxes[I], 0]\n                        viewindexes_elevation[I, gt_classes[argmaxes[I]]] = gt_viewpoints[argmaxes[I], 1]\n                        viewindexes_elevation_flipped[I, gt_classes[argmaxes[I]]] = gt_viewpoints_flipped[argmaxes[I], 1]\n                        viewindexes_rotation[I, gt_classes[argmaxes[I]]] = gt_viewpoints[argmaxes[I], 2]\n                        viewindexes_rotation_flipped[I, gt_classes[argmaxes[I]]] = gt_viewpoints_flipped[argmaxes[I], 2]\n\n            overlaps = scipy.sparse.csr_matrix(overlaps)\n            subindexes = scipy.sparse.csr_matrix(subindexes)\n            subindexes_flipped = scipy.sparse.csr_matrix(subindexes_flipped)\n\n            if cfg.TRAIN.VIEWPOINT == True or cfg.TEST.VIEWPOINT == True:\n                viewindexes_azimuth = scipy.sparse.csr_matrix(viewindexes_azimuth)\n                viewindexes_azimuth_flipped = scipy.sparse.csr_matrix(viewindexes_azimuth_flipped)\n                viewindexes_elevation = scipy.sparse.csr_matrix(viewindexes_elevation)\n                viewindexes_elevation_flipped = scipy.sparse.csr_matrix(viewindexes_elevation_flipped)\n                viewindexes_rotation = scipy.sparse.csr_matrix(viewindexes_rotation)\n                viewindexes_rotation_flipped = scipy.sparse.csr_matrix(viewindexes_rotation_flipped)\n                roidb.append({\'boxes\' : boxes,\n                              \'gt_classes\' : np.zeros((num_boxes,), dtype=np.int32),\n                              \'gt_viewpoints\': np.zeros((num_boxes, 3), dtype=np.float32),\n                              \'gt_viewpoints_flipped\': np.zeros((num_boxes, 3), dtype=np.float32),\n                              \'gt_viewindexes_azimuth\': viewindexes_azimuth,\n                              \'gt_viewindexes_azimuth_flipped\': viewindexes_azimuth_flipped,\n                              \'gt_viewindexes_elevation\': viewindexes_elevation,\n                              \'gt_viewindexes_elevation_flipped\': viewindexes_elevation_flipped,\n                              \'gt_viewindexes_rotation\': viewindexes_rotation,\n                              \'gt_viewindexes_rotation_flipped\': viewindexes_rotation_flipped,\n                              \'gt_subclasses\' : np.zeros((num_boxes,), dtype=np.int32),\n                              \'gt_subclasses_flipped\' : np.zeros((num_boxes,), dtype=np.int32),\n                              \'gt_overlaps\' : overlaps,\n                              \'gt_subindexes\': subindexes,\n                              \'gt_subindexes_flipped\': subindexes_flipped,\n                              \'flipped\' : False})\n            else:\n                roidb.append({\'boxes\' : boxes,\n                              \'gt_classes\' : np.zeros((num_boxes,), dtype=np.int32),\n                              \'gt_subclasses\' : np.zeros((num_boxes,), dtype=np.int32),\n                              \'gt_subclasses_flipped\' : np.zeros((num_boxes,), dtype=np.int32),\n                              \'gt_overlaps\' : overlaps,\n                              \'gt_subindexes\': subindexes,\n                              \'gt_subindexes_flipped\': subindexes_flipped,\n                              \'flipped\' : False})\n        return roidb\n\n    @staticmethod\n    def merge_roidbs(a, b):\n        assert len(a) == len(b)\n        for i in xrange(len(a)):\n            a[i][\'boxes\'] = np.vstack((a[i][\'boxes\'], b[i][\'boxes\']))\n            a[i][\'gt_classes\'] = np.hstack((a[i][\'gt_classes\'],\n                                            b[i][\'gt_classes\']))\n            if cfg.TRAIN.VIEWPOINT == True or cfg.TEST.VIEWPOINT == True:\n                a[i][\'gt_viewpoints\'] = np.vstack((a[i][\'gt_viewpoints\'],\n                                                b[i][\'gt_viewpoints\']))\n                a[i][\'gt_viewpoints_flipped\'] = np.vstack((a[i][\'gt_viewpoints_flipped\'],\n                                                b[i][\'gt_viewpoints_flipped\']))\n                a[i][\'gt_viewindexes_azimuth\'] = scipy.sparse.vstack((a[i][\'gt_viewindexes_azimuth\'],\n                                                b[i][\'gt_viewindexes_azimuth\']))\n                a[i][\'gt_viewindexes_azimuth_flipped\'] = scipy.sparse.vstack((a[i][\'gt_viewindexes_azimuth_flipped\'],\n                                                b[i][\'gt_viewindexes_azimuth_flipped\']))\n                a[i][\'gt_viewindexes_elevation\'] = scipy.sparse.vstack((a[i][\'gt_viewindexes_elevation\'],\n                                                b[i][\'gt_viewindexes_elevation\']))\n                a[i][\'gt_viewindexes_elevation_flipped\'] = scipy.sparse.vstack((a[i][\'gt_viewindexes_elevation_flipped\'],\n                                                b[i][\'gt_viewindexes_elevation_flipped\']))\n                a[i][\'gt_viewindexes_rotation\'] = scipy.sparse.vstack((a[i][\'gt_viewindexes_rotation\'],\n                                                b[i][\'gt_viewindexes_rotation\']))\n                a[i][\'gt_viewindexes_rotation_flipped\'] = scipy.sparse.vstack((a[i][\'gt_viewindexes_rotation_flipped\'],\n                                                b[i][\'gt_viewindexes_rotation_flipped\']))\n\n            a[i][\'gt_subclasses\'] = np.hstack((a[i][\'gt_subclasses\'],\n                                            b[i][\'gt_subclasses\']))\n            a[i][\'gt_subclasses_flipped\'] = np.hstack((a[i][\'gt_subclasses_flipped\'],\n                                            b[i][\'gt_subclasses_flipped\']))\n            a[i][\'gt_overlaps\'] = scipy.sparse.vstack([a[i][\'gt_overlaps\'],\n                                                       b[i][\'gt_overlaps\']])\n            a[i][\'gt_subindexes\'] = scipy.sparse.vstack((a[i][\'gt_subindexes\'],\n                                            b[i][\'gt_subindexes\']))\n            a[i][\'gt_subindexes_flipped\'] = scipy.sparse.vstack((a[i][\'gt_subindexes_flipped\'],\n                                            b[i][\'gt_subindexes_flipped\']))\n        return a\n\n    def competition_mode(self, on):\n        """"""Turn competition mode on or off.""""""\n        pass\n'"
lib/datasets/kitti.py,0,"b'import os\nimport PIL\nimport numpy as np\nimport scipy.sparse\nimport subprocess\nimport cPickle\nimport math\nimport glob\n\nfrom .imdb import imdb\nfrom .imdb import ROOT_DIR\n\nfrom ..utils.cython_bbox import bbox_overlaps\nfrom ..utils.boxes_grid import get_boxes_grid\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\nfrom ..rpn_msr.generate_anchors import generate_anchors\n# <<<< obsolete\n\nclass kitti(imdb):\n    def __init__(self, image_set, kitti_path=None):\n\n        imdb.__init__(self, \'kitti_\' + image_set)\n        self._image_set = image_set\n        self._kitti_path = self._get_default_path() if kitti_path is None \\\n                            else kitti_path\n        self._data_path = os.path.join(self._kitti_path, \'data_object_image_2\')\n        self._classes = (\'__background__\', \'Car\', \'Pedestrian\', \'Cyclist\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.png\'\n        self._image_index = self._load_image_set_index_new()\n        # Default to roidb handler\n        if cfg.IS_RPN:\n            self._roidb_handler = self.gt_roidb\n        else:\n            self._roidb_handler = self.region_proposal_roidb\n\n        # num of subclasses\n        if image_set == \'train\' or image_set == \'val\':\n            self._num_subclasses = 125 + 24 + 24 + 1\n            prefix = \'validation\'\n        else:\n            self._num_subclasses = 227 + 36 + 36 + 1\n            prefix = \'test\'\n\n        self.config = {\'top_k\': 100000}\n\n        # statistics for computing recall\n        self._num_boxes_all = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_covered = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_proposal = 0\n\n        assert os.path.exists(self._kitti_path), \\\n                \'KITTI path does not exist: {}\'.format(self._kitti_path)\n        assert os.path.exists(self._data_path), \\\n                \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self.image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        # set the prefix\n        if self._image_set == \'test\':\n            prefix = \'testing/image_2\'\n        else:\n            prefix = \'training/image_2\'\n\n        image_path = os.path.join(self._data_path, prefix, index + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        obsolete, using _load_image_set_index_new instead\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        image_set_file = os.path.join(self._kitti_path, self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n                \'Path does not exist: {}\'.format(image_set_file)\n\n        with open(image_set_file) as f:\n            image_index = [x.rstrip(\'\\n\') for x in f.readlines()]\n        return image_index\n\n    def _load_image_set_index_new(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        image_set_file = os.path.join(self._kitti_path, \'training/image_2/\')\n        assert os.path.exists(image_set_file), \\\n                \'Path does not exist: {}\'.format(image_set_file)\n\n        image_index = os.listdir(image_set_file)\n        image_set_file = image_set_file + \'*.png\'\n        image_index = glob.glob(image_set_file)\n\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where KITTI is expected to be installed.\n        """"""\n        return os.path.join(cfg.DATA_DIR, \'KITTI\')\n\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n\n        cache_file = os.path.join(self.cache_path, self.name + \'_\' + cfg.SUBCLS_NAME + \'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} gt roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = [self._load_kitti_voxel_exemplar_annotation(index)\n                    for index in self.image_index]\n\n        if cfg.IS_RPN:\n            # print out recall\n            for i in xrange(1, self.num_classes):\n                print \'{}: Total number of boxes {:d}\'.format(self.classes[i], self._num_boxes_all[i])\n                print \'{}: Number of boxes covered {:d}\'.format(self.classes[i], self._num_boxes_covered[i])\n                print \'{}: Recall {:f}\'.format(self.classes[i], float(self._num_boxes_covered[i]) / float(self._num_boxes_all[i]))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote gt roidb to {}\'.format(cache_file)\n\n        return gt_roidb\n\n\n    def _load_kitti_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from txt file in the KITTI format.\n        """"""\n\n        if self._image_set == \'test\':\n            lines = []\n        else:\n            filename = os.path.join(self._data_path, \'training\', \'label_2\', index + \'.txt\')\n            lines = []\n            with open(filename) as f:\n                for line in f:\n                    line = line.replace(\'Van\', \'Car\')\n                    words = line.split()\n                    cls = words[0]\n                    truncation = float(words[1])\n                    occlusion = int(words[2])\n                    height = float(words[7]) - float(words[5])\n                    if cls in self._class_to_ind and truncation < 0.5 and occlusion < 3 and height > 25:\n                        lines.append(line)\n\n        num_objs = len(lines)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.float32)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n\n        for ix, line in enumerate(lines):\n            words = line.split()\n            cls = self._class_to_ind[words[0]]\n            boxes[ix, :] = [float(n) for n in words[4:8]]\n            gt_classes[ix] = cls\n            overlaps[ix, cls] = 1.0\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        gt_subclasses = np.zeros((num_objs), dtype=np.int32)\n        gt_subclasses_flipped = np.zeros((num_objs), dtype=np.int32)\n        subindexes = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes_flipped = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes = scipy.sparse.csr_matrix(subindexes)\n        subindexes_flipped = scipy.sparse.csr_matrix(subindexes_flipped)\n\n        if cfg.IS_RPN:\n            if cfg.IS_MULTISCALE:\n                # compute overlaps between grid boxes and gt boxes in multi-scales\n                # rescale the gt boxes\n                boxes_all = np.zeros((0, 4), dtype=np.float32)\n                for scale in cfg.TRAIN.SCALES:\n                    boxes_all = np.vstack((boxes_all, boxes * scale))\n                gt_classes_all = np.tile(gt_classes, len(cfg.TRAIN.SCALES))\n\n                # compute grid boxes\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n                boxes_grid, _, _ = get_boxes_grid(image_height, image_width)\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(boxes_grid.astype(np.float), boxes_all.astype(np.float))\n        \n                # check how many gt boxes are covered by grids\n                if num_objs != 0:\n                    index = np.tile(range(num_objs), len(cfg.TRAIN.SCALES))\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes_all == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n                    index_covered = np.unique(index[fg_inds])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[index_covered] == i)[0])\n            else:\n                assert len(cfg.TRAIN.SCALES_BASE) == 1\n                scale = cfg.TRAIN.SCALES_BASE[0]\n                feat_stride = 16\n                # faster rcnn region proposal\n                anchors = generate_anchors()\n                num_anchors = anchors.shape[0]\n\n                # image size\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n\n                # height and width of the heatmap\n                height = np.round((image_height * scale - 1) / 4.0 + 1)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n\n                width = np.round((image_width * scale - 1) / 4.0 + 1)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n\n                # gt boxes\n                gt_boxes = boxes * scale\n\n                # 1. Generate proposals from bbox deltas and shifted anchors\n                shift_x = np.arange(0, width) * feat_stride\n                shift_y = np.arange(0, height) * feat_stride\n                shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n                shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n                # add A anchors (1, A, 4) to\n                # cell K shifts (K, 1, 4) to get\n                # shift anchors (K, A, 4)\n                # reshape to (K*A, 4) shifted anchors\n                A = num_anchors\n                K = shifts.shape[0]\n                all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n                all_anchors = all_anchors.reshape((K * A, 4))\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(all_anchors.astype(np.float), gt_boxes.astype(np.float))\n        \n                # check how many gt boxes are covered by anchors\n                if num_objs != 0:\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_subclasses\': gt_subclasses,\n                \'gt_subclasses_flipped\': gt_subclasses_flipped,\n                \'gt_overlaps\' : overlaps,\n                \'gt_subindexes\': subindexes,\n                \'gt_subindexes_flipped\': subindexes_flipped,\n                \'flipped\' : False}\n\n    def _load_kitti_voxel_exemplar_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from txt file in the KITTI voxel exemplar format.\n        """"""\n        if self._image_set == \'train\':\n            prefix = \'validation\'\n        elif self._image_set == \'trainval\':\n            prefix = \'test\'\n        else:\n            return self._load_kitti_annotation(index)\n\n        filename = os.path.join(self._kitti_path, cfg.SUBCLS_NAME, prefix, index + \'.txt\')\n        assert os.path.exists(filename), \\\n                \'Path does not exist: {}\'.format(filename)\n\n        # the annotation file contains flipped objects    \n        lines = []\n        lines_flipped = []\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[1])\n                is_flip = int(words[2])\n                if subcls != -1:\n                    if is_flip == 0:\n                        lines.append(line)\n                    else:\n                        lines_flipped.append(line)\n        \n        num_objs = len(lines)\n\n        # store information of flipped objects\n        assert (num_objs == len(lines_flipped)), \'The number of flipped objects is not the same!\'\n        gt_subclasses_flipped = np.zeros((num_objs), dtype=np.int32)\n        \n        for ix, line in enumerate(lines_flipped):\n            words = line.split()\n            subcls = int(words[1])\n            gt_subclasses_flipped[ix] = subcls\n\n        boxes = np.zeros((num_objs, 4), dtype=np.float32)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        gt_subclasses = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        subindexes = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes_flipped = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n\n        for ix, line in enumerate(lines):\n            words = line.split()\n            cls = self._class_to_ind[words[0]]\n            subcls = int(words[1])\n            boxes[ix, :] = [float(n) for n in words[3:7]]\n            gt_classes[ix] = cls\n            gt_subclasses[ix] = subcls\n            overlaps[ix, cls] = 1.0\n            subindexes[ix, cls] = subcls\n            subindexes_flipped[ix, cls] = gt_subclasses_flipped[ix]\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        subindexes = scipy.sparse.csr_matrix(subindexes)\n        subindexes_flipped = scipy.sparse.csr_matrix(subindexes_flipped)\n\n        if cfg.IS_RPN:\n            if cfg.IS_MULTISCALE:\n                # compute overlaps between grid boxes and gt boxes in multi-scales\n                # rescale the gt boxes\n                boxes_all = np.zeros((0, 4), dtype=np.float32)\n                for scale in cfg.TRAIN.SCALES:\n                    boxes_all = np.vstack((boxes_all, boxes * scale))\n                gt_classes_all = np.tile(gt_classes, len(cfg.TRAIN.SCALES))\n\n                # compute grid boxes\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n                boxes_grid, _, _ = get_boxes_grid(image_height, image_width)\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(boxes_grid.astype(np.float), boxes_all.astype(np.float))\n        \n                # check how many gt boxes are covered by grids\n                if num_objs != 0:\n                    index = np.tile(range(num_objs), len(cfg.TRAIN.SCALES))\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes_all == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n                    index_covered = np.unique(index[fg_inds])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[index_covered] == i)[0])\n            else:\n                assert len(cfg.TRAIN.SCALES_BASE) == 1\n                scale = cfg.TRAIN.SCALES_BASE[0]\n                feat_stride = 16\n                # faster rcnn region proposal\n                base_size = 16\n                ratios = [3.0, 2.0, 1.5, 1.0, 0.75, 0.5, 0.25]\n                scales = 2**np.arange(1, 6, 0.5)\n                anchors = generate_anchors(base_size, ratios, scales)\n                num_anchors = anchors.shape[0]\n\n                # image size\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n\n                # height and width of the heatmap\n                height = np.round((image_height * scale - 1) / 4.0 + 1)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n\n                width = np.round((image_width * scale - 1) / 4.0 + 1)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n\n                # gt boxes\n                gt_boxes = boxes * scale\n\n                # 1. Generate proposals from bbox deltas and shifted anchors\n                shift_x = np.arange(0, width) * feat_stride\n                shift_y = np.arange(0, height) * feat_stride\n                shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n                shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n                # add A anchors (1, A, 4) to\n                # cell K shifts (K, 1, 4) to get\n                # shift anchors (K, A, 4)\n                # reshape to (K*A, 4) shifted anchors\n                A = num_anchors\n                K = shifts.shape[0]\n                all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n                all_anchors = all_anchors.reshape((K * A, 4))\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(all_anchors.astype(np.float), gt_boxes.astype(np.float))\n        \n                # check how many gt boxes are covered by anchors\n                if num_objs != 0:\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_subclasses\': gt_subclasses,\n                \'gt_subclasses_flipped\': gt_subclasses_flipped,\n                \'gt_overlaps\': overlaps,\n                \'gt_subindexes\': subindexes, \n                \'gt_subindexes_flipped\': subindexes_flipped, \n                \'flipped\' : False}\n\n    def region_proposal_roidb(self):\n        """"""\n        Return the database of regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_\' + cfg.SUBCLS_NAME + \'_\' + cfg.REGION_PROPOSAL + \'_region_proposal_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n\n            print \'Loading region proposal network boxes...\'\n            if self._image_set == \'trainval\':\n                model = cfg.REGION_PROPOSAL + \'_227/\'\n            else:\n                model = cfg.REGION_PROPOSAL + \'_125/\'\n            rpn_roidb = self._load_rpn_roidb(gt_roidb, model)\n            print \'Region proposal network boxes loaded\'\n            roidb = imdb.merge_roidbs(rpn_roidb, gt_roidb)\n\n            # print \'Loading voxel pattern boxes...\'\n            # if self._image_set == \'trainval\':\n            #    model = \'3DVP_227\'\n            # else:\n            #    model = \'3DVP_125/\'\n            # vp_roidb = self._load_voxel_pattern_roidb(gt_roidb, model)\n            # print \'Voxel pattern boxes loaded\'\n            # roidb = imdb.merge_roidbs(vp_roidb, gt_roidb)\n\n            # print \'Loading selective search boxes...\'\n            # ss_roidb = self._load_selective_search_roidb(gt_roidb)\n            # print \'Selective search boxes loaded\'\n\n            # print \'Loading ACF boxes...\'\n            # acf_roidb = self._load_acf_roidb(gt_roidb)\n            # print \'ACF boxes loaded\'\n\n            # roidb = imdb.merge_roidbs(ss_roidb, gt_roidb)\n            # roidb = imdb.merge_roidbs(roidb, acf_roidb)\n        else:\n            print \'Loading region proposal network boxes...\'\n            model = cfg.REGION_PROPOSAL + \'_227/\'\n            roidb = self._load_rpn_roidb(None, model)\n            print \'Region proposal network boxes loaded\'\n\n            # print \'Loading voxel pattern boxes...\'\n            # model = \'3DVP_227/\'\n            # roidb = self._load_voxel_pattern_roidb(None, model)\n            # print \'Voxel pattern boxes loaded\'\n\n            # print \'Loading selective search boxes...\'\n            # roidb = self._load_selective_search_roidb(None)\n            # print \'Selective search boxes loaded\'\n\n            # print \'Loading ACF boxes...\'\n            # acf_roidb = self._load_acf_roidb(None)\n            # print \'ACF boxes loaded\'\n\n            # roidb = imdb.merge_roidbs(roidb, acf_roidb)\n        print \'{} region proposals per image\'.format(self._num_boxes_proposal / len(self.image_index))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb, model):\n        # set the prefix\n        if self._image_set == \'test\':\n            prefix = model + \'testing\'\n        else:\n            prefix = model + \'training\'\n\n        box_list = []\n        for index in self.image_index:\n            filename = os.path.join(self._kitti_path, \'region_proposals\',  prefix, index + \'.txt\')\n            assert os.path.exists(filename), \\\n                \'RPN data not found at: {}\'.format(filename)\n            raw_data = np.loadtxt(filename, dtype=float)\n            if len(raw_data.shape) == 1:\n                if raw_data.size == 0:\n                    raw_data = raw_data.reshape((0, 5))\n                else:\n                    raw_data = raw_data.reshape((1, 5))\n\n            x1 = raw_data[:, 0]\n            y1 = raw_data[:, 1]\n            x2 = raw_data[:, 2]\n            y2 = raw_data[:, 3]\n            score = raw_data[:, 4]\n            inds = np.where((x2 > x1) & (y2 > y1))[0]\n            raw_data = raw_data[inds,:4]\n            self._num_boxes_proposal += raw_data.shape[0]\n            box_list.append(raw_data)\n            print \'load {}: {}\'.format(model, index)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def _load_voxel_pattern_roidb(self, gt_roidb, model):\n        # set the prefix\n        if self._image_set == \'test\':\n            prefix = model + \'testing\'\n        else:\n            prefix = model + \'training\'\n\n        box_list = []\n        for index in self.image_index:\n            filename = os.path.join(self._kitti_path, \'region_proposals\',  prefix, index + \'.txt\')\n            assert os.path.exists(filename), \\\n                \'Voxel pattern data not found at: {}\'.format(filename)\n            raw_data = np.loadtxt(filename, dtype=float)\n            if len(raw_data.shape) == 1:\n                if raw_data.size == 0:\n                    raw_data = raw_data.reshape((0, 4))\n                else:\n                    raw_data = raw_data.reshape((1, 4))\n            self._num_boxes_proposal += raw_data.shape[0]\n            box_list.append(raw_data)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def _load_selective_search_roidb(self, gt_roidb):\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_selective_search_box_list.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                box_list = cPickle.load(fid)\n            print \'{} boxes loaded from {}\'.format(self.name, cache_file)\n        else:\n            # set the prefix\n            model = \'selective_search/\'\n            if self._image_set == \'test\':\n                prefix = model + \'testing\'\n            else:\n                prefix = model + \'training\'\n\n            box_list = []\n            for index in self.image_index:\n                filename = os.path.join(self._kitti_path, \'region_proposals\', prefix, index + \'.txt\')\n                assert os.path.exists(filename), \\\n                    \'Selective search data not found at: {}\'.format(filename)\n                raw_data = np.loadtxt(filename, dtype=float)\n                box_list.append(raw_data[:min(self.config[\'top_k\'], raw_data.shape[0]), 1:])\n\n            with open(cache_file, \'wb\') as fid:\n                cPickle.dump(box_list, fid, cPickle.HIGHEST_PROTOCOL)\n            print \'wrote selective search boxes to {}\'.format(cache_file)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def _load_acf_roidb(self, gt_roidb):\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_acf_box_list.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                box_list = cPickle.load(fid)\n            print \'{} boxes loaded from {}\'.format(self.name, cache_file)\n        else:\n            # set the prefix\n            model = \'ACF/\'\n            if self._image_set == \'test\':\n                prefix = model + \'testing\'\n            else:\n                prefix = model + \'training\'\n\n            box_list = []\n            for index in self.image_index:\n                filename = os.path.join(self._kitti_path, \'region_proposals\', prefix, index + \'.txt\')\n                assert os.path.exists(filename), \\\n                    \'ACF data not found at: {}\'.format(filename)\n                raw_data = np.loadtxt(filename, usecols=(2,3,4,5), dtype=float)\n                box_list.append(raw_data[:min(self.config[\'top_k\'], raw_data.shape[0]), :])\n\n            with open(cache_file, \'wb\') as fid:\n                cPickle.dump(box_list, fid, cPickle.HIGHEST_PROTOCOL)\n            print \'wrote ACF boxes to {}\'.format(cache_file)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        # load the mapping for subcalss the alpha (viewpoint)\n        if self._image_set == \'val\':\n            prefix = \'validation\'\n        elif self._image_set == \'test\':\n            prefix = \'test\'\n        else:\n            prefix = \'\'\n\n        filename = os.path.join(self._kitti_path, cfg.SUBCLS_NAME, prefix, \'mapping.txt\')\n        assert os.path.exists(filename), \\\n                \'Path does not exist: {}\'.format(filename)\n\n        mapping = np.zeros(self._num_subclasses, dtype=np.float)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = float(words[3])\n\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing KITTI results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        if cfg.TEST.SUBCLS:\n                            subcls = int(dets[k, 5])\n                            cls_name = self.classes[self.subclass_mapping[subcls]]\n                            assert (cls_name == cls), \'subclass not in class\'\n                            alpha = mapping[subcls]\n                        else:\n                            alpha = -10\n                        f.write(\'{:s} -1 -1 {:f} {:f} {:f} {:f} {:f} -1 -1 -1 -1 -1 -1 -1 {:.32f}\\n\'.format(\\\n                                 cls, alpha, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    # write detection results into one file\n    def evaluate_detections_one_file(self, all_boxes, output_dir):\n        # open results file\n        filename = os.path.join(output_dir, \'detections.txt\')\n        print \'Writing all KITTI results to file \' + filename\n        with open(filename, \'wt\') as f:\n            # for each image\n            for im_ind, index in enumerate(self.image_index):\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        if cfg.TEST.SUBCLS:\n                            subcls = int(dets[k, 5])\n                            cls_name = self.classes[self.subclass_mapping[subcls]]\n                            assert (cls_name == cls), \'subclass not in class\'\n                        else:\n                            subcls = -1\n                        f.write(\'{:s} {:s} {:f} {:f} {:f} {:f} {:d} {:f}\\n\'.format(\\\n                                 index, cls, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], subcls, dets[k, 4]))\n\n    def evaluate_proposals(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing KITTI results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(\\\n                                 dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    def evaluate_proposals_msr(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing KITTI results to file \' + filename\n            with open(filename, \'wt\') as f:\n                dets = all_boxes[im_ind]\n                if dets == []:\n                    continue\n                for k in xrange(dets.shape[0]):\n                    f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n\nif __name__ == \'__main__\':\n    d = kitti(\'train\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
lib/datasets/kitti_tracking.py,0,"b'\nimport os\nimport PIL\nimport numpy as np\nimport scipy.sparse\nimport subprocess\nimport cPickle\nimport math\n\nfrom .imdb import imdb\nfrom .imdb import ROOT_DIR\n\nfrom ..utils.cython_bbox import bbox_overlaps\nfrom ..utils.boxes_grid import get_boxes_grid\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\nfrom ..rpn_msr.generate_anchors import generate_anchors\n# <<<< obsolete\n\nclass kitti_tracking(imdb):\n    def __init__(self, image_set, seq_name, kitti_tracking_path=None):\n        imdb.__init__(self, \'kitti_tracking_\' + image_set + \'_\' + seq_name)\n        self._image_set = image_set\n        self._seq_name = seq_name\n        self._kitti_tracking_path = self._get_default_path() if kitti_tracking_path is None \\\n                            else kitti_tracking_path\n        self._data_path = os.path.join(self._kitti_tracking_path, image_set, \'image_02\')\n        self._classes = (\'__background__\', \'Car\', \'Pedestrian\', \'Cyclist\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.png\'\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        if cfg.IS_RPN:\n            self._roidb_handler = self.gt_roidb\n        else:\n            self._roidb_handler = self.region_proposal_roidb\n\n        # num of subclasses\n        if image_set == \'training\' and seq_name != \'trainval\':\n            self._num_subclasses = 220 + 1\n        else:\n            self._num_subclasses = 472 + 1\n\n        # load the mapping for subcalss to class\n        if image_set == \'training\' and seq_name != \'trainval\':\n            filename = os.path.join(self._kitti_tracking_path, \'voxel_exemplars\', \'train\', \'mapping.txt\')\n        else:\n            filename = os.path.join(self._kitti_tracking_path, \'voxel_exemplars\', \'trainval\', \'mapping.txt\')\n        assert os.path.exists(filename), \'Path does not exist: {}\'.format(filename)\n        \n        mapping = np.zeros(self._num_subclasses, dtype=np.int)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = self._class_to_ind[words[1]]\n        self._subclass_mapping = mapping\n\n        self.config = {\'top_k\': 100000}\n\n        # statistics for computing recall\n        self._num_boxes_all = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_covered = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_proposal = 0\n\n        assert os.path.exists(self._kitti_tracking_path), \\\n                \'kitti_tracking path does not exist: {}\'.format(self._kitti_tracking_path)\n        assert os.path.exists(self._data_path), \\\n                \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self.image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n\n        image_path = os.path.join(self._data_path, index + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n\n        kitti_train_nums = [154, 447, 233, 144, 314, 297, 270, 800, 390, 803, 294, \\\n                            373, 78, 340, 106, 376, 209, 145, 339, 1059, 837]\n\n        kitti_test_nums = [465, 147, 243, 257, 421, 809, 114, 215, 165, 349, 1176, \\\n                           774, 694, 152, 850, 701, 510, 305, 180, 404, 173, 203, \\\n                           436, 430, 316, 176, 170, 85, 175]\n\n        if self._seq_name == \'train\' or self._seq_name == \'trainval\':\n\n            assert self._image_set == \'training\', \'Use train set or trainval set in testing\'\n\n            if self._seq_name == \'train\':\n                seq_index = [0, 1, 2, 3, 4, 5, 12, 13, 14, 15, 16]\n            else:\n                seq_index = range(0, 21)\n\n            # for each sequence\n            image_index = []\n            for i in xrange(len(seq_index)):\n                seq_idx = seq_index[i]\n                num = kitti_train_nums[seq_idx]\n                for j in xrange(num):\n                    image_index.append(\'{:04d}/{:06d}\'.format(seq_idx, j))\n        else:\n            # a single sequence\n            seq_num = int(self._seq_name)\n            if self._image_set == \'training\':\n                num = kitti_train_nums[seq_num]\n            else:\n                num = kitti_test_nums[seq_num]\n            image_index = []\n            for i in xrange(num):\n                image_index.append(\'{:04d}/{:06d}\'.format(seq_num, i))\n\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where kitti_tracking is expected to be installed.\n        """"""\n        return os.path.join(ROOT_DIR, \'data\', \'KITTI_Tracking\')\n\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n        """"""\n\n        cache_file = os.path.join(self.cache_path, self.name + \'_\' + cfg.SUBCLS_NAME + \'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} gt roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = [self._load_kitti_voxel_exemplar_annotation(index)\n                    for index in self.image_index]\n\n        if cfg.IS_RPN:\n            # print out recall\n            for i in xrange(1, self.num_classes):\n                print \'{}: Total number of boxes {:d}\'.format(self.classes[i], self._num_boxes_all[i])\n                print \'{}: Number of boxes covered {:d}\'.format(self.classes[i], self._num_boxes_covered[i])\n                print \'{}: Recall {:f}\'.format(self.classes[i], float(self._num_boxes_covered[i]) / float(self._num_boxes_all[i]))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote gt roidb to {}\'.format(cache_file)\n\n        return gt_roidb\n\n\n    def _load_kitti_voxel_exemplar_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from txt file in the KITTI voxel exemplar format.\n        """"""\n        if self._image_set == \'training\' and self._seq_name != \'trainval\':\n            prefix = \'train\'\n        elif self._image_set == \'training\':\n            prefix = \'trainval\'\n        else:\n            prefix = \'\'\n\n        if prefix == \'\':\n            lines = []\n            lines_flipped = []\n        else:\n            filename = os.path.join(self._kitti_tracking_path, cfg.SUBCLS_NAME, prefix, index + \'.txt\')\n            if os.path.exists(filename):\n                print filename\n\n                # the annotation file contains flipped objects    \n                lines = []\n                lines_flipped = []\n                with open(filename) as f:\n                    for line in f:\n                        words = line.split()\n                        subcls = int(words[1])\n                        is_flip = int(words[2])\n                        if subcls != -1:\n                            if is_flip == 0:\n                                lines.append(line)\n                            else:\n                                lines_flipped.append(line)\n            else:\n                lines = []\n                lines_flipped = []\n        \n        num_objs = len(lines)\n\n        # store information of flipped objects\n        assert (num_objs == len(lines_flipped)), \'The number of flipped objects is not the same!\'\n        gt_subclasses_flipped = np.zeros((num_objs), dtype=np.int32)\n        \n        for ix, line in enumerate(lines_flipped):\n            words = line.split()\n            subcls = int(words[1])\n            gt_subclasses_flipped[ix] = subcls\n\n        boxes = np.zeros((num_objs, 4), dtype=np.float32)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        gt_subclasses = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        subindexes = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes_flipped = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n\n        for ix, line in enumerate(lines):\n            words = line.split()\n            cls = self._class_to_ind[words[0]]\n            subcls = int(words[1])\n            boxes[ix, :] = [float(n) for n in words[3:7]]\n            gt_classes[ix] = cls\n            gt_subclasses[ix] = subcls\n            overlaps[ix, cls] = 1.0\n            subindexes[ix, cls] = subcls\n            subindexes_flipped[ix, cls] = gt_subclasses_flipped[ix]\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        subindexes = scipy.sparse.csr_matrix(subindexes)\n        subindexes_flipped = scipy.sparse.csr_matrix(subindexes_flipped)\n\n        if cfg.IS_RPN:\n            if cfg.IS_MULTISCALE:\n                # compute overlaps between grid boxes and gt boxes in multi-scales\n                # rescale the gt boxes\n                boxes_all = np.zeros((0, 4), dtype=np.float32)\n                for scale in cfg.TRAIN.SCALES:\n                    boxes_all = np.vstack((boxes_all, boxes * scale))\n                gt_classes_all = np.tile(gt_classes, len(cfg.TRAIN.SCALES))\n\n                # compute grid boxes\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n                boxes_grid, _, _ = get_boxes_grid(image_height, image_width)\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(boxes_grid.astype(np.float), boxes_all.astype(np.float))\n        \n                # check how many gt boxes are covered by grids\n                if num_objs != 0:\n                    index = np.tile(range(num_objs), len(cfg.TRAIN.SCALES))\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes_all == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n                    index_covered = np.unique(index[fg_inds])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[index_covered] == i)[0])\n            else:\n                assert len(cfg.TRAIN.SCALES_BASE) == 1\n                scale = cfg.TRAIN.SCALES_BASE[0]\n                feat_stride = 16\n                # faster rcnn region proposal\n                base_size = 16\n                ratios = [3.0, 2.0, 1.5, 1.0, 0.75, 0.5, 0.25]\n                scales = 2**np.arange(1, 6, 0.5)\n                anchors = generate_anchors(base_size, ratios, scales)\n                num_anchors = anchors.shape[0]\n\n                # image size\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n\n                # height and width of the heatmap\n                height = np.round((image_height * scale - 1) / 4.0 + 1)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n\n                width = np.round((image_width * scale - 1) / 4.0 + 1)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n\n                # gt boxes\n                gt_boxes = boxes * scale\n\n                # 1. Generate proposals from bbox deltas and shifted anchors\n                shift_x = np.arange(0, width) * feat_stride\n                shift_y = np.arange(0, height) * feat_stride\n                shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n                shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n                # add A anchors (1, A, 4) to\n                # cell K shifts (K, 1, 4) to get\n                # shift anchors (K, A, 4)\n                # reshape to (K*A, 4) shifted anchors\n                A = num_anchors\n                K = shifts.shape[0]\n                all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n                all_anchors = all_anchors.reshape((K * A, 4))\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(all_anchors.astype(np.float), gt_boxes.astype(np.float))\n        \n                # check how many gt boxes are covered by anchors\n                if num_objs != 0:\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_subclasses\': gt_subclasses,\n                \'gt_subclasses_flipped\': gt_subclasses_flipped,\n                \'gt_overlaps\': overlaps,\n                \'gt_subindexes\': subindexes, \n                \'gt_subindexes_flipped\': subindexes_flipped, \n                \'flipped\' : False}\n\n\n    def region_proposal_roidb(self):\n        """"""\n        Return the database of regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_\' + cfg.SUBCLS_NAME + \'_\' + cfg.REGION_PROPOSAL + \'_region_proposal_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if self._image_set != \'testing\':\n            gt_roidb = self.gt_roidb()\n\n            print \'Loading region proposal network boxes...\'\n            if self._image_set == \'trainval\':\n                model = cfg.REGION_PROPOSAL + \'_trainval/\'\n            else:\n                model = cfg.REGION_PROPOSAL + \'_train/\'\n            rpn_roidb = self._load_rpn_roidb(gt_roidb, model)\n            print \'Region proposal network boxes loaded\'\n            roidb = imdb.merge_roidbs(rpn_roidb, gt_roidb)\n        else:\n            print \'Loading region proposal network boxes...\'\n            model = cfg.REGION_PROPOSAL + \'_trainval/\'\n            roidb = self._load_rpn_roidb(None, model)\n            print \'Region proposal network boxes loaded\'\n\n        print \'{} region proposals per image\'.format(self._num_boxes_proposal / len(self.image_index))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote roidb to {}\'.format(cache_file)\n\n        return roidb\n\n\n    def _load_rpn_roidb(self, gt_roidb, model):\n        # set the prefix\n        prefix = model\n\n        box_list = []\n        for index in self.image_index:\n            filename = os.path.join(self._kitti_tracking_path, \'region_proposals\',  prefix, self._image_set, index + \'.txt\')\n            assert os.path.exists(filename), \\\n                \'RPN data not found at: {}\'.format(filename)\n            print filename\n            raw_data = np.loadtxt(filename, dtype=float)\n            if len(raw_data.shape) == 1:\n                if raw_data.size == 0:\n                    raw_data = raw_data.reshape((0, 5))\n                else:\n                    raw_data = raw_data.reshape((1, 5))\n\n            x1 = raw_data[:, 0]\n            y1 = raw_data[:, 1]\n            x2 = raw_data[:, 2]\n            y2 = raw_data[:, 3]\n            score = raw_data[:, 4]\n            inds = np.where((x2 > x1) & (y2 > y1))[0]\n            raw_data = raw_data[inds,:4]\n            self._num_boxes_proposal += raw_data.shape[0]\n            box_list.append(raw_data)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        # load the mapping for subcalss the alpha (viewpoint)\n        if self._image_set == \'training\' and self._seq_name != \'trainval\':\n            filename = os.path.join(self._kitti_tracking_path, \'voxel_exemplars\', \'train\', \'mapping.txt\')\n        else:\n            filename = os.path.join(self._kitti_tracking_path, \'voxel_exemplars\', \'trainval\', \'mapping.txt\')\n        assert os.path.exists(filename), \'Path does not exist: {}\'.format(filename)\n\n        mapping = np.zeros(self._num_subclasses, dtype=np.float)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = float(words[3])\n\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index[5:] + \'.txt\')\n            print \'Writing kitti_tracking results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        subcls = int(dets[k, 5])\n                        cls_name = self.classes[self.subclass_mapping[subcls]]\n                        assert (cls_name == cls), \'subclass not in class\'\n                        alpha = mapping[subcls]\n                        f.write(\'{:s} -1 -1 {:f} {:f} {:f} {:f} {:f} -1 -1 -1 -1 -1 -1 -1 {:.32f}\\n\'.format(\\\n                                 cls, alpha, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    # write detection results into one file\n    def evaluate_detections_one_file(self, all_boxes, output_dir):\n        # load the mapping for subcalss the alpha (viewpoint)\n        if self._image_set == \'training\' and self._seq_name != \'trainval\':\n            filename = os.path.join(self._kitti_tracking_path, \'voxel_exemplars\', \'train\', \'mapping.txt\')\n        else:\n            filename = os.path.join(self._kitti_tracking_path, \'voxel_exemplars\', \'trainval\', \'mapping.txt\')\n        assert os.path.exists(filename), \'Path does not exist: {}\'.format(filename)\n\n        mapping = np.zeros(self._num_subclasses, dtype=np.float)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = float(words[3])\n\n        # open results file\n        filename = os.path.join(output_dir, self._seq_name+\'.txt\')\n        print \'Writing all kitti_tracking results to file \' + filename\n        with open(filename, \'wt\') as f:\n            # for each image\n            for im_ind, index in enumerate(self.image_index):\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        subcls = int(dets[k, 5])\n                        cls_name = self.classes[self.subclass_mapping[subcls]]\n                        assert (cls_name == cls), \'subclass not in class\'\n                        alpha = mapping[subcls]\n                        f.write(\'{:d} -1 {:s} -1 -1 {:f} {:f} {:f} {:f} {:f} -1 -1 -1 -1000 -1000 -1000 -10 {:f}\\n\'.format(\\\n                                 im_ind, cls, alpha, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    def evaluate_proposals(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index[5:] + \'.txt\')\n            print \'Writing kitti_tracking results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(\\\n                                 dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    def evaluate_proposals_msr(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing kitti_tracking results to file \' + filename\n            with open(filename, \'wt\') as f:\n                dets = all_boxes[im_ind]\n                if dets == []:\n                    continue\n                for k in xrange(dets.shape[0]):\n                    f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n\nif __name__ == \'__main__\':\n    d = kitti_tracking(\'training\', \'0000\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
lib/datasets/kittivoc.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport xml.dom.minidom as minidom\n\nimport os\nimport PIL\nimport numpy as np\nimport scipy.sparse\nimport subprocess\nimport cPickle\nimport math\nimport glob\nimport uuid\nimport scipy.io as sio\nimport xml.etree.ElementTree as ET\n\nfrom .imdb import imdb\nfrom .imdb import ROOT_DIR\nimport ds_utils\nfrom .voc_eval import voc_eval\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\n\n\nclass kittivoc(imdb):\n    def __init__(self, image_set, devkit_path=None):\n        imdb.__init__(self, \'kittivoc_\' + image_set)\n        self._image_set = image_set\n        self._devkit_path = self._get_default_path() if devkit_path is None \\\n                            else devkit_path\n        self._data_path = self._devkit_path\n        self._classes = (\'__background__\', # always index 0\n                         \'pedestrian\', \'car\', \'cyclist\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.jpg\'\n        self._image_index = self._load_image_set_index()\n        self._remove_empty_samples()\n        # Default to roidb handler\n        #self._roidb_handler = self.selective_search_roidb\n        self._roidb_handler = self.gt_roidb\n        self._salt = str(uuid.uuid4())\n        self._comp_id = \'comp4\'\n        self._year = \'\'\n        # PASCAL specific config options\n        self.config = {\'cleanup\'     : True,\n                       \'use_salt\'    : True,\n                       \'use_diff\'    : False, # using difficult samples\n                       \'matlab_eval\' : False,\n                       \'rpn_file\'    : None,\n                       \'min_size\'    : 2}\n\n        assert os.path.exists(self._devkit_path), \\\n                \'VOCdevkit path does not exist: {}\'.format(self._devkit_path)\n        assert os.path.exists(self._data_path), \\\n                \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self._image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier\n        :param index filename stem e.g. 000000\n        :return filepath\n        """"""\n        image_path = os.path.join(self._data_path, \'JPEGImages\',\n                                  index + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        # Example path to image set file:\n        image_set_file = os.path.join(self._data_path, \'ImageSets\', \'Main\',\n                                      self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n                \'Path does not exist: {}\'.format(image_set_file)\n        with open(image_set_file) as f:\n            image_index = [x.strip() for x in f.readlines()]\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where PASCAL VOC is expected to be installed.\n        """"""\n        return os.path.join(cfg.DATA_DIR, \'KITTIVOC\')\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest, aka, the annotations.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path, self.name + \'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} gt roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = [self._load_pascal_annotation(index)\n                    for index in self.image_index]\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote gt roidb to {}\'.format(cache_file)\n\n        return gt_roidb\n\n    def selective_search_roidb(self):\n        """"""\n        Return the database of selective search regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_selective_search_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} ss roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n            ss_roidb = self._load_selective_search_roidb(gt_roidb)\n            roidb = imdb.merge_roidbs(gt_roidb, ss_roidb)\n        else:\n            roidb = self._load_selective_search_roidb(None)\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote ss roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def rpn_roidb(self):\n        if self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n            rpn_roidb = self._load_rpn_roidb(gt_roidb)\n            roidb = imdb.merge_roidbs(gt_roidb, rpn_roidb)\n        else:\n            roidb = self._load_rpn_roidb(None)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb):\n        filename = self.config[\'rpn_file\']\n        print \'loading {}\'.format(filename)\n        assert os.path.exists(filename), \\\n               \'rpn data not found at: {}\'.format(filename)\n        with open(filename, \'rb\') as f:\n            box_list = cPickle.load(f)\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def _load_selective_search_roidb(self, gt_roidb):\n        filename = os.path.abspath(os.path.join(self._data_path,\n                                                \'selective_search_data\',\n                                                self.name + \'.mat\'))\n        assert os.path.exists(filename), \\\n               \'Selective search data not found at: {}\'.format(filename)\n        raw_data = sio.loadmat(filename)[\'boxes\'].ravel()\n\n        box_list = []\n        for i in xrange(raw_data.shape[0]):\n            boxes = raw_data[i][:, (1, 0, 3, 2)] - 1\n            keep = ds_utils.unique_boxes(boxes)\n            boxes = boxes[keep, :]\n            keep = ds_utils.filter_small_boxes(boxes, self.config[\'min_size\'])\n            boxes = boxes[keep, :]\n            box_list.append(boxes)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def _remove_empty_samples(self):\n        """"""\n        Remove images with zero annotation ()\n        """"""\n        print \'Remove empty annotations: \',\n        for i in range(len(self._image_index)-1, -1, -1):\n            index = self._image_index[i]\n            filename = os.path.join(self._data_path, \'Annotations\', index + \'.xml\')\n            tree = ET.parse(filename)\n            objs = tree.findall(\'object\')\n            non_diff_objs = [\n                obj for obj in objs if \\\n                    int(obj.find(\'difficult\').text) == 0 and obj.find(\'name\').text.lower().strip() != \'dontcare\']\n            num_objs = len(non_diff_objs)\n            if num_objs == 0:\n                print index,\n                self._image_index.pop(i)\n        print \'Done. \'\n\n    def _load_pascal_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from XML file in the PASCAL VOC\n        format.\n        """"""\n        filename = os.path.join(self._data_path, \'Annotations\', index + \'.xml\')\n        tree = ET.parse(filename)\n        objs = tree.findall(\'object\')\n        # if not self.config[\'use_diff\']:\n        #     # Exclude the samples labeled as difficult\n        #     non_diff_objs = [\n        #         obj for obj in objs if int(obj.find(\'difficult\').text) == 0]\n        #     objs = non_diff_objs\n        num_objs = len(objs)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.int32)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        # just the same as gt_classes\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        # ""Seg"" area for pascal is just the box area\n        seg_areas = np.zeros((num_objs), dtype=np.float32)\n\n        ishards = np.zeros((num_objs), dtype=np.int32)\n        care_inds = np.empty((0), dtype=np.int32)\n        dontcare_inds = np.empty((0), dtype=np.int32)\n\n        # Load object bounding boxes into a data frame.\n        for ix, obj in enumerate(objs):\n            bbox = obj.find(\'bndbox\')\n            # Make pixel indexes 0-based\n            x1 = max(float(bbox.find(\'xmin\').text) - 1, 0)\n            y1 = max(float(bbox.find(\'ymin\').text) - 1, 0)\n            x2 = float(bbox.find(\'xmax\').text) - 1\n            y2 = float(bbox.find(\'ymax\').text) - 1\n\n            diffc = obj.find(\'difficult\')\n            difficult = 0 if diffc == None else int(diffc.text)\n            ishards[ix] = difficult\n\n            class_name = obj.find(\'name\').text.lower().strip()\n            if class_name != \'dontcare\':\n                care_inds = np.append(care_inds, np.asarray([ix], dtype=np.int32))\n            if class_name == \'dontcare\':\n                dontcare_inds = np.append(dontcare_inds, np.asarray([ix], dtype=np.int32))\n                boxes[ix, :] = [x1, y1, x2, y2]\n                continue\n            cls = self._class_to_ind[class_name]\n            boxes[ix, :] = [x1, y1, x2, y2]\n            gt_classes[ix] = cls\n            overlaps[ix, cls] = 1.0\n            seg_areas[ix] = (x2 - x1 + 1) * (y2 - y1 + 1)\n\n        # deal with dontcare areas\n        dontcare_areas = boxes[dontcare_inds, :]\n        boxes = boxes[care_inds, :]\n        gt_classes = gt_classes[care_inds]\n        overlaps = overlaps[care_inds, :]\n        seg_areas = seg_areas[care_inds]\n        ishards = ishards[care_inds]\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_ishard\' : ishards,\n                \'dontcare_areas\' : dontcare_areas,\n                \'gt_overlaps\' : overlaps,\n                \'flipped\' : False,\n                \'seg_areas\' : seg_areas}\n\n    def _get_comp_id(self):\n        comp_id = (self._comp_id + \'_\' + self._salt if self.config[\'use_salt\']\n            else self._comp_id)\n        return comp_id\n\n    def _get_voc_results_file_template(self):\n        # VOCdevkit/results/VOC2007/Main/<comp_id>_det_test_aeroplane.txt\n        filename = self._get_comp_id() + \'_det_\' + self._image_set + \'_{:s}.txt\'\n        filedir = os.path.join(self._devkit_path, \'results\', \'KITTI\', \'Main\')\n        if not os.path.exists(filedir):\n            os.makedirs(filedir)\n        path = os.path.join(filedir, filename)\n        return path\n\n    def _write_voc_results_file(self, all_boxes):\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == \'__background__\':\n                continue\n            print \'Writing {} VOC results file\'.format(cls)\n            filename = self._get_voc_results_file_template().format(cls)\n            with open(filename, \'wt\') as f:\n                for im_ind, index in enumerate(self.image_index):\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    # the VOCdevkit expects 1-based indices\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                            format(index, dets[k, -1],              # filename(stem), score\n                                   dets[k, 0] + 1, dets[k, 1] + 1,  # x1, y1, x2, y2\n                                   dets[k, 2] + 1, dets[k, 3] + 1))\n\n    def _do_python_eval(self, output_dir = \'output\'):\n        annopath = os.path.join(\n            self._devkit_path,\n            \'Annotations\', \'{:s}.xml\')\n        imagesetfile = os.path.join(\n            self._devkit_path,\n            \'ImageSets\', \'Main\',\n            self._image_set + \'.txt\')\n        cachedir = os.path.join(self._devkit_path, \'annotations_cache\')\n        aps = []\n        # The PASCAL VOC metric changed in 2010\n        use_07_metric = False\n        print \'VOC07 metric? \' + (\'Yes\' if use_07_metric else \'No\')\n        if not os.path.isdir(output_dir):\n            os.mkdir(output_dir)\n        for i, cls in enumerate(self._classes):\n            if cls == \'__background__\':\n                continue\n            filename = self._get_voc_results_file_template().format(cls)\n            rec, prec, ap = voc_eval(filename, annopath, imagesetfile, cls, cachedir,\n                                     ovthresh=0.5, use_07_metric = use_07_metric)\n            aps += [ap]\n            print(\'AP for {} = {:.4f}\'.format(cls, ap))\n            with open(os.path.join(output_dir, cls + \'_pr.pkl\'), \'w\') as f:\n                cPickle.dump({\'rec\': rec, \'prec\': prec, \'ap\': ap}, f)\n        print(\'Mean AP = {:.4f}\'.format(np.mean(aps)))\n        print(\'~~~~~~~~\')\n        print(\'Results:\')\n        for ap in aps:\n            print(\'{:.3f}\'.format(ap))\n        print(\'{:.3f}\'.format(np.mean(aps)))\n        print(\'~~~~~~~~\')\n        print(\'\')\n        print(\'--------------------------------------------------------------\')\n        print(\'Results computed with the **unofficial** Python eval code.\')\n        print(\'Results should be very close to the official MATLAB eval code.\')\n        print(\'Recompute with `./tools/reval.py --matlab ...` for your paper.\')\n        print(\'-- Thanks, The Management\')\n        print(\'--------------------------------------------------------------\')\n\n    def _do_matlab_eval(self, output_dir=\'output\'):\n        print \'-----------------------------------------------------\'\n        print \'Computing results with the official MATLAB eval code.\'\n        print \'-----------------------------------------------------\'\n        path = os.path.join(cfg.ROOT_DIR, \'lib\', \'datasets\',\n                            \'VOCdevkit-matlab-wrapper\')\n        cmd = \'cd {} && \'.format(path)\n        cmd += \'{:s} -nodisplay -nodesktop \'.format(cfg.MATLAB)\n        cmd += \'-r ""dbstop if error; \'\n        cmd += \'voc_eval(\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\'); quit;""\' \\\n               .format(self._devkit_path, self._get_comp_id(),\n                       self._image_set, output_dir)\n        print(\'Running:\\n{}\'.format(cmd))\n        status = subprocess.call(cmd, shell=True)\n\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        self._write_voc_results_file(all_boxes)\n        self._do_python_eval(output_dir)\n        if self.config[\'matlab_eval\']:\n            self._do_matlab_eval(output_dir)\n        if self.config[\'cleanup\']:\n            for cls in self._classes:\n                if cls == \'__background__\':\n                    continue\n                filename = self._get_voc_results_file_template().format(cls)\n                os.remove(filename)\n\n    def competition_mode(self, on):\n        if on:\n            self.config[\'use_salt\'] = False\n            self.config[\'cleanup\'] = False\n        else:\n            self.config[\'use_salt\'] = True\n            self.config[\'cleanup\'] = True\n\nif __name__ == \'__main__\':\n    d = kittivoc(\'trainval\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
lib/datasets/nissan.py,0,"b'import os\nimport PIL\nimport numpy as np\nimport scipy.sparse\nimport subprocess\nimport cPickle\nimport math\nimport glob\n\nfrom .imdb import imdb\nfrom .imdb import ROOT_DIR\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\n\nclass nissan(imdb):\n    def __init__(self, image_set, nissan_path=None):\n        imdb.__init__(self, \'nissan_\' + image_set)\n        self._image_set = image_set\n        self._nissan_path = self._get_default_path() if nissan_path is None \\\n                            else nissan_path\n        self._data_path = os.path.join(self._nissan_path, \'Images\')\n        self._classes = (\'__background__\', \'Car\', \'Pedestrian\', \'Cyclist\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.png\'\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        if cfg.IS_RPN:\n            self._roidb_handler = self.gt_roidb\n        else:\n            self._roidb_handler = self.region_proposal_roidb\n\n        # num of subclasses\n        self._num_subclasses = 227 + 36 + 36 + 1\n\n        # load the mapping for subcalss to class\n        filename = os.path.join(self._nissan_path, \'mapping.txt\')\n        assert os.path.exists(filename), \'Path does not exist: {}\'.format(filename)\n        \n        mapping = np.zeros(self._num_subclasses, dtype=np.int)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = self._class_to_ind[words[1]]\n        self._subclass_mapping = mapping\n\n        self.config = {\'top_k\': 100000}\n\n        # statistics for computing recall\n        self._num_boxes_all = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_covered = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_proposal = 0\n\n        assert os.path.exists(self._nissan_path), \\\n                \'Nissan path does not exist: {}\'.format(self._nissan_path)\n        assert os.path.exists(self._data_path), \\\n                \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self.image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        # set the prefix\n        prefix = self._image_set\n\n        image_path = os.path.join(self._data_path, prefix, index + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        image_set_file = os.path.join(self._data_path, self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n                \'Path does not exist: {}\'.format(image_set_file)\n\n        with open(image_set_file) as f:\n            image_index = [x.rstrip(\'\\n\') for x in f.readlines()]\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where NISSAN is expected to be installed.\n        """"""\n        return os.path.join(ROOT_DIR, \'data\', \'NISSAN\')\n\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n        No implementation.\n        """"""\n\n        gt_roidb = []\n        return gt_roidb\n\n    def region_proposal_roidb(self):\n        """"""\n        Return the database of regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_\' + cfg.REGION_PROPOSAL + \'_region_proposal_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        print \'Loading region proposal network boxes...\'\n        model = cfg.REGION_PROPOSAL\n        roidb = self._load_rpn_roidb(None, model)\n        print \'Region proposal network boxes loaded\'\n        print \'{} region proposals per image\'.format(self._num_boxes_proposal / len(self.image_index))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb, model):\n        # set the prefix\n        prefix = model\n\n        box_list = []\n        for index in self.image_index:\n            filename = os.path.join(self._nissan_path, \'region_proposals\',  prefix, self._image_set, index + \'.txt\')\n            assert os.path.exists(filename), \\\n                \'RPN data not found at: {}\'.format(filename)\n            raw_data = np.loadtxt(filename, dtype=float)\n            if len(raw_data.shape) == 1:\n                if raw_data.size == 0:\n                    raw_data = raw_data.reshape((0, 5))\n                else:\n                    raw_data = raw_data.reshape((1, 5))\n\n            x1 = raw_data[:, 0]\n            y1 = raw_data[:, 1]\n            x2 = raw_data[:, 2]\n            y2 = raw_data[:, 3]\n            score = raw_data[:, 4]\n            inds = np.where((x2 > x1) & (y2 > y1))[0]\n            raw_data = raw_data[inds,:4]\n            self._num_boxes_proposal += raw_data.shape[0]\n            box_list.append(raw_data)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        # load the mapping for subcalss the alpha (viewpoint)\n        filename = os.path.join(self._nissan_path, \'mapping.txt\')\n        assert os.path.exists(filename), \\\n                \'Path does not exist: {}\'.format(filename)\n\n        mapping = np.zeros(self._num_subclasses, dtype=np.float)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = float(words[3])\n\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing NISSAN results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        subcls = int(dets[k, 5])\n                        cls_name = self.classes[self.subclass_mapping[subcls]]\n                        assert (cls_name == cls), \'subclass not in class\'\n                        alpha = mapping[subcls]\n                        f.write(\'{:s} -1 -1 {:f} {:f} {:f} {:f} {:f} -1 -1 -1 -1 -1 -1 -1 {:.32f}\\n\'.format(\\\n                                 cls, alpha, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    # write detection results into one file\n    def evaluate_detections_one_file(self, all_boxes, output_dir):\n        # open results file\n        filename = os.path.join(output_dir, \'detections.txt\')\n        print \'Writing all NISSAN results to file \' + filename\n        with open(filename, \'wt\') as f:\n            # for each image\n            for im_ind, index in enumerate(self.image_index):\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        subcls = int(dets[k, 5])\n                        cls_name = self.classes[self.subclass_mapping[subcls]]\n                        assert (cls_name == cls), \'subclass not in class\'\n                        f.write(\'{:s} {:s} {:f} {:f} {:f} {:f} {:d} {:f}\\n\'.format(\\\n                                 index, cls, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], subcls, dets[k, 4]))\n\n    def evaluate_proposals(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing NISSAN results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(\\\n                                 dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    def evaluate_proposals_msr(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing NISSAN results to file \' + filename\n            with open(filename, \'wt\') as f:\n                dets = all_boxes[im_ind]\n                if dets == []:\n                    continue\n                for k in xrange(dets.shape[0]):\n                    f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n\nif __name__ == \'__main__\':\n    d = nissan(\'2015-10-21-16-25-12\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
lib/datasets/nthu.py,0,"b'import os\nimport PIL\nimport numpy as np\nimport scipy.sparse\nimport subprocess\nimport cPickle\nimport math\nimport glob\n\nfrom .imdb import imdb\nfrom .imdb import ROOT_DIR\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\n\nclass nthu(imdb):\n    def __init__(self, image_set, nthu_path=None):\n        imdb.__init__(self, \'nthu_\' + image_set)\n        self._image_set = image_set\n        self._nthu_path = self._get_default_path() if nthu_path is None \\\n                            else nthu_path\n        self._data_path = os.path.join(self._nthu_path, \'data\')\n        self._classes = (\'__background__\', \'Car\', \'Pedestrian\', \'Cyclist\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.jpg\'\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        if cfg.IS_RPN:\n            self._roidb_handler = self.gt_roidb\n        else:\n            self._roidb_handler = self.region_proposal_roidb\n\n        # num of subclasses\n        self._num_subclasses = 227 + 36 + 36 + 1\n\n        # load the mapping for subcalss to class\n        filename = os.path.join(self._nthu_path, \'mapping.txt\')\n        assert os.path.exists(filename), \'Path does not exist: {}\'.format(filename)\n        \n        mapping = np.zeros(self._num_subclasses, dtype=np.int)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = self._class_to_ind[words[1]]\n        self._subclass_mapping = mapping\n\n        self.config = {\'top_k\': 100000}\n\n        # statistics for computing recall\n        self._num_boxes_all = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_covered = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_proposal = 0\n\n        assert os.path.exists(self._nthu_path), \\\n                \'NTHU path does not exist: {}\'.format(self._nthu_path)\n        assert os.path.exists(self._data_path), \\\n                \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self.image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        # set the prefix\n        prefix = self._image_set\n\n        image_path = os.path.join(self._data_path, prefix, index + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        image_set_file = os.path.join(self._data_path, self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n                \'Path does not exist: {}\'.format(image_set_file)\n\n        with open(image_set_file) as f:\n            image_index = [x.rstrip(\'\\n\') for x in f.readlines()]\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where nthu is expected to be installed.\n        """"""\n        return os.path.join(ROOT_DIR, \'data\', \'NTHU\')\n\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n        No implementation.\n        """"""\n\n        gt_roidb = []\n        return gt_roidb\n\n    def region_proposal_roidb(self):\n        """"""\n        Return the database of regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_\' + cfg.REGION_PROPOSAL + \'_region_proposal_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        print \'Loading region proposal network boxes...\'\n        model = cfg.REGION_PROPOSAL\n        roidb = self._load_rpn_roidb(None, model)\n        print \'Region proposal network boxes loaded\'\n        print \'{} region proposals per image\'.format(self._num_boxes_proposal / len(self.image_index))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb, model):\n        # set the prefix\n        prefix = model\n\n        box_list = []\n        for index in self.image_index:\n            filename = os.path.join(self._nthu_path, \'region_proposals\',  prefix, self._image_set, index + \'.txt\')\n            assert os.path.exists(filename), \\\n                \'RPN data not found at: {}\'.format(filename)\n            raw_data = np.loadtxt(filename, dtype=float)\n            if len(raw_data.shape) == 1:\n                if raw_data.size == 0:\n                    raw_data = raw_data.reshape((0, 5))\n                else:\n                    raw_data = raw_data.reshape((1, 5))\n\n            x1 = raw_data[:, 0]\n            y1 = raw_data[:, 1]\n            x2 = raw_data[:, 2]\n            y2 = raw_data[:, 3]\n            score = raw_data[:, 4]\n            inds = np.where((x2 > x1) & (y2 > y1))[0]\n            raw_data = raw_data[inds,:4]\n            self._num_boxes_proposal += raw_data.shape[0]\n            box_list.append(raw_data)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        # load the mapping for subcalss the alpha (viewpoint)\n        filename = os.path.join(self._nthu_path, \'mapping.txt\')\n        assert os.path.exists(filename), \\\n                \'Path does not exist: {}\'.format(filename)\n\n        mapping = np.zeros(self._num_subclasses, dtype=np.float)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = float(words[3])\n\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing nthu results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        subcls = int(dets[k, 5])\n                        cls_name = self.classes[self.subclass_mapping[subcls]]\n                        assert (cls_name == cls), \'subclass not in class\'\n                        alpha = mapping[subcls]\n                        f.write(\'{:s} -1 -1 {:f} {:f} {:f} {:f} {:f} -1 -1 -1 -1 -1 -1 -1 {:.32f}\\n\'.format(\\\n                                 cls, alpha, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    # write detection results into one file\n    def evaluate_detections_one_file(self, all_boxes, output_dir):\n        # open results file\n        filename = os.path.join(output_dir, \'detections.txt\')\n        print \'Writing all nthu results to file \' + filename\n        with open(filename, \'wt\') as f:\n            # for each image\n            for im_ind, index in enumerate(self.image_index):\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        subcls = int(dets[k, 5])\n                        cls_name = self.classes[self.subclass_mapping[subcls]]\n                        assert (cls_name == cls), \'subclass not in class\'\n                        f.write(\'{:s} {:s} {:f} {:f} {:f} {:f} {:d} {:f}\\n\'.format(\\\n                                 index, cls, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], subcls, dets[k, 4]))\n\n    def evaluate_proposals(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing nthu results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(\\\n                                 dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    def evaluate_proposals_msr(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing nthu results to file \' + filename\n            with open(filename, \'wt\') as f:\n                dets = all_boxes[im_ind]\n                if dets == []:\n                    continue\n                for k in xrange(dets.shape[0]):\n                    f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n\nif __name__ == \'__main__\':\n    d = nthu(\'71\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
lib/datasets/pascal3d.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport xml.dom.minidom as minidom\n\nimport os\nimport PIL\nimport numpy as np\nimport scipy.sparse\nimport subprocess\nimport cPickle\nimport math\nimport glob\nimport scipy.io as sio\n\nfrom .imdb import imdb\nfrom .imdb import ROOT_DIR\n\nfrom ..utils.cython_bbox import bbox_overlaps\nfrom ..utils.boxes_grid import get_boxes_grid\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\nfrom ..rpn_msr.generate_anchors import generate_anchors\n# <<<< obsolete\n\nclass pascal3d(imdb):\n    def __init__(self, image_set, pascal3d_path = None):\n        imdb.__init__(self, \'pascal3d_\' + image_set)\n        self._year = \'2012\'\n        self._image_set = image_set\n        self._pascal3d_path = self._get_default_path() if pascal3d_path is None \\\n                            else pascal3d_path\n        self._data_path = os.path.join(self._pascal3d_path, \'VOCdevkit\' + self._year, \'VOC\' + self._year)\n        self._classes = (\'__background__\', # always index 0\n                         \'aeroplane\', \'bicycle\', \'boat\',\n                         \'bottle\', \'bus\', \'car\', \'chair\',\n                         \'diningtable\', \'motorbike\',\n                         \'sofa\', \'train\', \'tvmonitor\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.jpg\'\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        if cfg.IS_RPN:\n            self._roidb_handler = self.gt_roidb\n        else:\n            self._roidb_handler = self.region_proposal_roidb\n\n        # num of subclasses\n        if cfg.SUBCLS_NAME == \'voxel_exemplars\':\n            self._num_subclasses = 337 + 1\n        elif cfg.SUBCLS_NAME == \'pose_exemplars\':\n            self._num_subclasses = 260 + 1\n        else:\n            assert (1), \'cfg.SUBCLS_NAME not supported!\'\n\n        # load the mapping for subcalss to class\n        filename = os.path.join(self._pascal3d_path, cfg.SUBCLS_NAME, \'mapping.txt\')\n        assert os.path.exists(filename), \'Path does not exist: {}\'.format(filename)\n        \n        mapping = np.zeros(self._num_subclasses, dtype=np.int)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = self._class_to_ind[words[1]]\n        self._subclass_mapping = mapping\n\n        # PASCAL specific config options\n        self.config = {\'cleanup\'  : True,\n                       \'use_salt\' : True,\n                       \'top_k\'    : 2000}\n\n        # statistics for computing recall\n        self._num_boxes_all = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_covered = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_proposal = 0\n\n        assert os.path.exists(self._pascal3d_path), \\\n                \'PASCAL3D path does not exist: {}\'.format(self._pascal3d_path)\n        assert os.path.exists(self._data_path), \\\n                \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self._image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        image_path = os.path.join(self._data_path, \'JPEGImages\',\n                                  index + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        # Example path to image set file:\n        # self._pascal3d_path + /VOCdevkit2012/VOC2012/ImageSets/Main/val.txt\n        image_set_file = os.path.join(self._data_path, \'ImageSets\', \'Main\',\n                                      self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n                \'Path does not exist: {}\'.format(image_set_file)\n        with open(image_set_file) as f:\n            image_index = [x.strip() for x in f.readlines()]\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where PASCAL3D is expected to be installed.\n        """"""\n        return os.path.join(ROOT_DIR, \'data\', \'PASCAL3D\')\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path, self.name + \'_\' + cfg.SUBCLS_NAME + \'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} gt roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = [self._load_pascal3d_voxel_exemplar_annotation(index)\n                    for index in self.image_index]\n\n        if cfg.IS_RPN:\n            # print out recall\n            for i in xrange(1, self.num_classes):\n                print \'{}: Total number of boxes {:d}\'.format(self.classes[i], self._num_boxes_all[i])\n                print \'{}: Number of boxes covered {:d}\'.format(self.classes[i], self._num_boxes_covered[i])\n                print \'{}: Recall {:f}\'.format(self.classes[i], float(self._num_boxes_covered[i]) / float(self._num_boxes_all[i]))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote gt roidb to {}\'.format(cache_file)\n\n        return gt_roidb\n\n    def _load_pascal_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from XML file in the PASCAL VOC\n        format.\n        """"""\n        filename = os.path.join(self._data_path, \'Annotations\', index + \'.xml\')\n        # print \'Loading: {}\'.format(filename)\n        def get_data_from_tag(node, tag):\n            return node.getElementsByTagName(tag)[0].childNodes[0].data\n\n        with open(filename) as f:\n            data = minidom.parseString(f.read())\n\n        objs = data.getElementsByTagName(\'object\')\n        num_objs = len(objs)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n\n        # Load object bounding boxes into a data frame.\n        for ix, obj in enumerate(objs):\n            # Make pixel indexes 0-based\n            x1 = float(get_data_from_tag(obj, \'xmin\')) - 1\n            y1 = float(get_data_from_tag(obj, \'ymin\')) - 1\n            x2 = float(get_data_from_tag(obj, \'xmax\')) - 1\n            y2 = float(get_data_from_tag(obj, \'ymax\')) - 1\n            name =  str(get_data_from_tag(obj, ""name"")).lower().strip()\n            if name in self._classes:\n                cls = self._class_to_ind[name]\n            else:\n                cls = 0\n            boxes[ix, :] = [x1, y1, x2, y2]\n            gt_classes[ix] = cls\n            overlaps[ix, cls] = 1.0\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        gt_subclasses = np.zeros((num_objs), dtype=np.int32)\n        gt_subclasses_flipped = np.zeros((num_objs), dtype=np.int32)\n        subindexes = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes_flipped = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes = scipy.sparse.csr_matrix(subindexes)\n        subindexes_flipped = scipy.sparse.csr_matrix(subindexes_flipped)\n\n        if cfg.IS_RPN:\n            if cfg.IS_MULTISCALE:\n                # compute overlaps between grid boxes and gt boxes in multi-scales\n                # rescale the gt boxes\n                boxes_all = np.zeros((0, 4), dtype=np.float32)\n                for scale in cfg.TRAIN.SCALES:\n                    boxes_all = np.vstack((boxes_all, boxes * scale))\n                gt_classes_all = np.tile(gt_classes, len(cfg.TRAIN.SCALES))\n\n                # compute grid boxes\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n                boxes_grid, _, _ = get_boxes_grid(image_height, image_width)\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(boxes_grid.astype(np.float), boxes_all.astype(np.float))\n        \n                # check how many gt boxes are covered by grids\n                if num_objs != 0:\n                    index = np.tile(range(num_objs), len(cfg.TRAIN.SCALES))\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes_all == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n                    index_covered = np.unique(index[fg_inds])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[index_covered] == i)[0])\n            else:\n                assert len(cfg.TRAIN.SCALES_BASE) == 1\n                scale = cfg.TRAIN.SCALES_BASE[0]\n                feat_stride = 16\n                # faster rcnn region proposal\n                anchors = generate_anchors()\n                num_anchors = anchors.shape[0]\n\n                # image size\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n\n                # height and width of the heatmap\n                height = np.round((image_height * scale - 1) / 4.0 + 1)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n\n                width = np.round((image_width * scale - 1) / 4.0 + 1)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n\n                # gt boxes\n                gt_boxes = boxes * scale\n\n                # 1. Generate proposals from bbox deltas and shifted anchors\n                shift_x = np.arange(0, width) * feat_stride\n                shift_y = np.arange(0, height) * feat_stride\n                shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n                shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n                # add A anchors (1, A, 4) to\n                # cell K shifts (K, 1, 4) to get\n                # shift anchors (K, A, 4)\n                # reshape to (K*A, 4) shifted anchors\n                A = num_anchors\n                K = shifts.shape[0]\n                all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n                all_anchors = all_anchors.reshape((K * A, 4))\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(all_anchors.astype(np.float), gt_boxes.astype(np.float))\n        \n                # check how many gt boxes are covered by anchors\n                if num_objs != 0:\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_subclasses\': gt_subclasses,\n                \'gt_subclasses_flipped\': gt_subclasses_flipped,\n                \'gt_overlaps\' : overlaps,\n                \'gt_subindexes\': subindexes,\n                \'gt_subindexes_flipped\': subindexes_flipped,\n                \'flipped\' : False}\n\n\n    def _load_pascal3d_voxel_exemplar_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from txt file in the pascal subcategory exemplar format.\n        """"""\n\n        if self._image_set == \'val\':\n            return self._load_pascal_annotation(index)\n\n        filename = os.path.join(self._pascal3d_path, cfg.SUBCLS_NAME, index + \'.txt\')\n        assert os.path.exists(filename), \\\n                \'Path does not exist: {}\'.format(filename)\n\n        # the annotation file contains flipped objects    \n        lines = []\n        lines_flipped = []\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[1])\n                is_flip = int(words[2])\n                if subcls != -1:\n                    if is_flip == 0:\n                        lines.append(line)\n                    else:\n                        lines_flipped.append(line)\n        \n        num_objs = len(lines)\n\n        # store information of flipped objects\n        assert (num_objs == len(lines_flipped)), \'The number of flipped objects is not the same!\'\n        gt_subclasses_flipped = np.zeros((num_objs), dtype=np.int32)\n        \n        for ix, line in enumerate(lines_flipped):\n            words = line.split()\n            subcls = int(words[1])\n            gt_subclasses_flipped[ix] = subcls\n\n        boxes = np.zeros((num_objs, 4), dtype=np.float32)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        gt_subclasses = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        subindexes = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes_flipped = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n\n        for ix, line in enumerate(lines):\n            words = line.split()\n            cls = self._class_to_ind[words[0]]\n            subcls = int(words[1])\n            # Make pixel indexes 0-based\n            boxes[ix, :] = [float(n)-1 for n in words[3:7]]\n            gt_classes[ix] = cls\n            gt_subclasses[ix] = subcls\n            overlaps[ix, cls] = 1.0\n            subindexes[ix, cls] = subcls\n            subindexes_flipped[ix, cls] = gt_subclasses_flipped[ix]\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        subindexes = scipy.sparse.csr_matrix(subindexes)\n        subindexes_flipped = scipy.sparse.csr_matrix(subindexes_flipped)\n\n        if cfg.IS_RPN:\n            if cfg.IS_MULTISCALE:\n                # compute overlaps between grid boxes and gt boxes in multi-scales\n                # rescale the gt boxes\n                boxes_all = np.zeros((0, 4), dtype=np.float32)\n                for scale in cfg.TRAIN.SCALES:\n                    boxes_all = np.vstack((boxes_all, boxes * scale))\n                gt_classes_all = np.tile(gt_classes, len(cfg.TRAIN.SCALES))\n\n                # compute grid boxes\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n                boxes_grid, _, _ = get_boxes_grid(image_height, image_width)\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(boxes_grid.astype(np.float), boxes_all.astype(np.float))\n        \n                # check how many gt boxes are covered by grids\n                if num_objs != 0:\n                    index = np.tile(range(num_objs), len(cfg.TRAIN.SCALES))\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes_all == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n                    index_covered = np.unique(index[fg_inds])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[index_covered] == i)[0])\n            else:\n                assert len(cfg.TRAIN.SCALES_BASE) == 1\n                scale = cfg.TRAIN.SCALES_BASE[0]\n                feat_stride = 16\n                # faster rcnn region proposal\n                base_size = 16\n                ratios = [3.0, 2.0, 1.5, 1.0, 0.75, 0.5, 0.25]\n                scales = 2**np.arange(1, 6, 0.5)\n                anchors = generate_anchors(base_size, ratios, scales)\n                num_anchors = anchors.shape[0]\n\n                # image size\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n\n                # height and width of the heatmap\n                height = np.round((image_height * scale - 1) / 4.0 + 1)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n\n                width = np.round((image_width * scale - 1) / 4.0 + 1)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n\n                # gt boxes\n                gt_boxes = boxes * scale\n\n                # 1. Generate proposals from bbox deltas and shifted anchors\n                shift_x = np.arange(0, width) * feat_stride\n                shift_y = np.arange(0, height) * feat_stride\n                shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n                shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n                # add A anchors (1, A, 4) to\n                # cell K shifts (K, 1, 4) to get\n                # shift anchors (K, A, 4)\n                # reshape to (K*A, 4) shifted anchors\n                A = num_anchors\n                K = shifts.shape[0]\n                all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n                all_anchors = all_anchors.reshape((K * A, 4))\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(all_anchors.astype(np.float), gt_boxes.astype(np.float))\n        \n                # check how many gt boxes are covered by anchors\n                if num_objs != 0:\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_subclasses\': gt_subclasses,\n                \'gt_subclasses_flipped\': gt_subclasses_flipped,\n                \'gt_overlaps\': overlaps,\n                \'gt_subindexes\': subindexes, \n                \'gt_subindexes_flipped\': subindexes_flipped, \n                \'flipped\' : False}\n\n    def region_proposal_roidb(self):\n        """"""\n        Return the database of regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_\' + cfg.SUBCLS_NAME + \'_\' + cfg.REGION_PROPOSAL + \'_region_proposal_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n\n            print \'Loading region proposal network boxes...\'\n            model = cfg.REGION_PROPOSAL\n            rpn_roidb = self._load_rpn_roidb(gt_roidb, model)\n            print \'Region proposal network boxes loaded\'\n            roidb = imdb.merge_roidbs(rpn_roidb, gt_roidb)\n        else:\n            print \'Loading region proposal network boxes...\'\n            model = cfg.REGION_PROPOSAL\n            roidb = self._load_rpn_roidb(None, model)\n            print \'Region proposal network boxes loaded\'\n\n        print \'{} region proposals per image\'.format(self._num_boxes_proposal / len(self.image_index))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb, model):\n        # set the prefix\n        if self._image_set == \'val\':\n            prefix = model + \'/validation\'\n        elif self._image_set == \'train\':\n            prefix = model + \'/training\'\n        else:\n            predix = \'\'\n\n        box_list = []\n        for index in self.image_index:\n            filename = os.path.join(self._pascal3d_path, \'region_proposals\',  prefix, index + \'.txt\')\n            assert os.path.exists(filename), \\\n                \'RPN data not found at: {}\'.format(filename)\n            raw_data = np.loadtxt(filename, dtype=float)\n            if len(raw_data.shape) == 1:\n                if raw_data.size == 0:\n                    raw_data = raw_data.reshape((0, 5))\n                else:\n                    raw_data = raw_data.reshape((1, 5))\n\n            x1 = raw_data[:, 0]\n            y1 = raw_data[:, 1]\n            x2 = raw_data[:, 2]\n            y2 = raw_data[:, 3]\n            score = raw_data[:, 4]\n            inds = np.where((x2 > x1) & (y2 > y1))[0]\n            raw_data = raw_data[inds,:4]\n            self._num_boxes_proposal += raw_data.shape[0]\n            box_list.append(raw_data)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n\n    def selective_search_roidb(self):\n        """"""\n        Return the database of selective search regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_selective_search_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} ss roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if int(self._year) == 2007 or self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n            ss_roidb = self._load_selective_search_roidb(gt_roidb)\n            roidb = imdb.merge_roidbs(gt_roidb, ss_roidb)\n        else:\n            roidb = self._load_selective_search_roidb(None)\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote ss roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_selective_search_roidb(self, gt_roidb):\n        filename = os.path.abspath(os.path.join(self.cache_path, \'..\',\n                                                \'selective_search_data\',\n                                                self.name + \'.mat\'))\n        assert os.path.exists(filename), \\\n               \'Selective search data not found at: {}\'.format(filename)\n        raw_data = sio.loadmat(filename)[\'boxes\'].ravel()\n\n        box_list = []\n        for i in xrange(raw_data.shape[0]):\n            box_list.append(raw_data[i][:, (1, 0, 3, 2)] - 1)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def selective_search_IJCV_roidb(self):\n        """"""\n        Return the database of selective search regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                \'{:s}_selective_search_IJCV_top_{:d}_roidb.pkl\'.\n                format(self.name, self.config[\'top_k\']))\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} ss roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = self.gt_roidb()\n        ss_roidb = self._load_selective_search_IJCV_roidb(gt_roidb)\n        roidb = imdb.merge_roidbs(gt_roidb, ss_roidb)\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote ss roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_selective_search_IJCV_roidb(self, gt_roidb):\n        IJCV_path = os.path.abspath(os.path.join(self.cache_path, \'..\',\n                                                 \'selective_search_IJCV_data\',\n                                                 \'voc_\' + self._year))\n        assert os.path.exists(IJCV_path), \\\n               \'Selective search IJCV data not found at: {}\'.format(IJCV_path)\n\n        top_k = self.config[\'top_k\']\n        box_list = []\n        for i in xrange(self.num_images):\n            filename = os.path.join(IJCV_path, self.image_index[i] + \'.mat\')\n            raw_data = sio.loadmat(filename)\n            box_list.append((raw_data[\'boxes\'][:top_k, :]-1).astype(np.uint16))\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    # evaluate detection results\n    def evaluate_detections(self, all_boxes, output_dir):\n        # load the mapping for subcalss the azimuth (viewpoint)\n        filename = os.path.join(self._pascal3d_path, cfg.SUBCLS_NAME, \'mapping.txt\')\n        assert os.path.exists(filename), \\\n                \'Path does not exist: {}\'.format(filename)\n\n        mapping = np.zeros(self._num_subclasses, dtype=np.float)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = float(words[2])\n\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == \'__background__\':\n                continue\n            print \'Writing {} VOC results file\'.format(cls)\n            filename = os.path.join(output_dir, \'det_\' + self._image_set + \'_\' + cls + \'.txt\')\n            print filename\n\n            with open(filename, \'wt\') as f:\n                for im_ind, index in enumerate(self.image_index):\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    # the VOCdevkit expects 1-based indices\n                    for k in xrange(dets.shape[0]):\n                        subcls = int(dets[k, 5])\n                        cls_name = self.classes[self.subclass_mapping[subcls]]\n                        assert (cls_name == cls), \'subclass not in class\'\n                        azimuth = mapping[subcls]\n                        f.write(\'{:s} {:.3f} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                                format(index, dets[k, 4], azimuth,\n                                       dets[k, 0] + 1, dets[k, 1] + 1,\n                                       dets[k, 2] + 1, dets[k, 3] + 1))\n\n    # evaluate detection results\n    def evaluate_detections_one_file(self, all_boxes, output_dir):\n        # open results file\n        filename = os.path.join(output_dir, \'detections.txt\')\n        print \'Writing all PASCAL3D results to file \' + filename\n        with open(filename, \'wt\') as f:\n            for im_ind, index in enumerate(self.image_index):\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    # the VOCdevkit expects 1-based indices\n                    for k in xrange(dets.shape[0]):\n                        subcls = int(dets[k, 5])\n                        cls_name = self.classes[self.subclass_mapping[subcls]]\n                        assert (cls_name == cls), \'subclass not in class\'\n                        f.write(\'{:s} {:s} {:f} {:f} {:f} {:f} {:d} {:f}\\n\'.\n                                format(index, cls, dets[k, 0] + 1, dets[k, 1] + 1,\n                                       dets[k, 2] + 1, dets[k, 3] + 1, subcls, dets[k, 4]))\n\n\n    def evaluate_proposals(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing PASCAL results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(\\\n                                 dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    def evaluate_proposals_msr(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing PASCAL results to file \' + filename\n            with open(filename, \'wt\') as f:\n                dets = all_boxes[im_ind]\n                if dets == []:\n                    continue\n                for k in xrange(dets.shape[0]):\n                    f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n\n    def competition_mode(self, on):\n        if on:\n            self.config[\'use_salt\'] = False\n            self.config[\'cleanup\'] = False\n        else:\n            self.config[\'use_salt\'] = True\n            self.config[\'cleanup\'] = True\n\nif __name__ == \'__main__\':\n    d = pascal3d(\'train\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
lib/datasets/pascal_voc.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport xml.dom.minidom as minidom\n\nimport os\nimport PIL\nimport numpy as np\nimport scipy.sparse\nimport subprocess\nimport cPickle\nimport math\nimport glob\nimport uuid\nimport scipy.io as sio\nimport xml.etree.ElementTree as ET\n\nfrom .imdb import imdb\nfrom .imdb import ROOT_DIR\nimport ds_utils\nfrom .voc_eval import voc_eval\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\n\n\nclass pascal_voc(imdb):\n    def __init__(self, image_set, year, devkit_path=None):\n        imdb.__init__(self, \'voc_\' + year + \'_\' + image_set)\n        self._year = year\n        self._image_set = image_set\n        self._devkit_path = self._get_default_path() if devkit_path is None \\\n                            else devkit_path\n        self._data_path = os.path.join(self._devkit_path, \'VOC\' + self._year)\n        self._classes = (\'__background__\', # always index 0\n                         \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n                         \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n                         \'cow\', \'diningtable\', \'dog\', \'horse\',\n                         \'motorbike\', \'person\', \'pottedplant\',\n                         \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.jpg\'\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        #self._roidb_handler = self.selective_search_roidb\n        self._roidb_handler = self.gt_roidb\n        self._salt = str(uuid.uuid4())\n        self._comp_id = \'comp4\'\n\n        # PASCAL specific config options\n        self.config = {\'cleanup\'     : True,\n                       \'use_salt\'    : True,\n                       \'use_diff\'    : False,\n                       \'matlab_eval\' : False,\n                       \'rpn_file\'    : None,\n                       \'min_size\'    : 2}\n\n        assert os.path.exists(self._devkit_path), \\\n                \'VOCdevkit path does not exist: {}\'.format(self._devkit_path)\n        assert os.path.exists(self._data_path), \\\n                \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self._image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        image_path = os.path.join(self._data_path, \'JPEGImages\',\n                                  index + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        # Example path to image set file:\n        # self._devkit_path + /VOCdevkit2007/VOC2007/ImageSets/Main/val.txt\n        image_set_file = os.path.join(self._data_path, \'ImageSets\', \'Main\',\n                                      self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n                \'Path does not exist: {}\'.format(image_set_file)\n        with open(image_set_file) as f:\n            image_index = [x.strip() for x in f.readlines()]\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where PASCAL VOC is expected to be installed.\n        """"""\n        return os.path.join(cfg.DATA_DIR, \'VOCdevkit\' + self._year)\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path, self.name + \'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} gt roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = [self._load_pascal_annotation(index)\n                    for index in self.image_index]\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote gt roidb to {}\'.format(cache_file)\n\n        return gt_roidb\n\n    def selective_search_roidb(self):\n        """"""\n        Return the database of selective search regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_selective_search_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} ss roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if int(self._year) == 2007 or self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n            ss_roidb = self._load_selective_search_roidb(gt_roidb)\n            roidb = imdb.merge_roidbs(gt_roidb, ss_roidb)\n        else:\n            roidb = self._load_selective_search_roidb(None)\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote ss roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def rpn_roidb(self):\n        if int(self._year) == 2007 or self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n            rpn_roidb = self._load_rpn_roidb(gt_roidb)\n            roidb = imdb.merge_roidbs(gt_roidb, rpn_roidb)\n        else:\n            roidb = self._load_rpn_roidb(None)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb):\n        filename = self.config[\'rpn_file\']\n        print \'loading {}\'.format(filename)\n        assert os.path.exists(filename), \\\n               \'rpn data not found at: {}\'.format(filename)\n        with open(filename, \'rb\') as f:\n            box_list = cPickle.load(f)\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def _load_selective_search_roidb(self, gt_roidb):\n        filename = os.path.abspath(os.path.join(cfg.DATA_DIR,\n                                                \'selective_search_data\',\n                                                self.name + \'.mat\'))\n        assert os.path.exists(filename), \\\n               \'Selective search data not found at: {}\'.format(filename)\n        raw_data = sio.loadmat(filename)[\'boxes\'].ravel()\n\n        box_list = []\n        for i in xrange(raw_data.shape[0]):\n            boxes = raw_data[i][:, (1, 0, 3, 2)] - 1\n            keep = ds_utils.unique_boxes(boxes)\n            boxes = boxes[keep, :]\n            keep = ds_utils.filter_small_boxes(boxes, self.config[\'min_size\'])\n            boxes = boxes[keep, :]\n            box_list.append(boxes)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def _load_pascal_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from XML file in the PASCAL VOC\n        format.\n        """"""\n        filename = os.path.join(self._data_path, \'Annotations\', index + \'.xml\')\n        tree = ET.parse(filename)\n        objs = tree.findall(\'object\')\n        # if not self.config[\'use_diff\']:\n        #     # Exclude the samples labeled as difficult\n        #     non_diff_objs = [\n        #         obj for obj in objs if int(obj.find(\'difficult\').text) == 0]\n        #     # if len(non_diff_objs) != len(objs):\n        #     #     print \'Removed {} difficult objects\'.format(\n        #     #         len(objs) - len(non_diff_objs))\n        #     objs = non_diff_objs\n        num_objs = len(objs)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        # ""Seg"" area for pascal is just the box area\n        seg_areas = np.zeros((num_objs), dtype=np.float32)\n        ishards = np.zeros((num_objs), dtype=np.int32)\n\n        # Load object bounding boxes into a data frame.\n        for ix, obj in enumerate(objs):\n            bbox = obj.find(\'bndbox\')\n            # Make pixel indexes 0-based\n            x1 = float(bbox.find(\'xmin\').text) - 1\n            y1 = float(bbox.find(\'ymin\').text) - 1\n            x2 = float(bbox.find(\'xmax\').text) - 1\n            y2 = float(bbox.find(\'ymax\').text) - 1\n\n            diffc = obj.find(\'difficult\')\n            difficult = 0 if diffc == None else int(diffc.text)\n            ishards[ix] = difficult\n\n            cls = self._class_to_ind[obj.find(\'name\').text.lower().strip()]\n            boxes[ix, :] = [x1, y1, x2, y2]\n            gt_classes[ix] = cls\n            overlaps[ix, cls] = 1.0\n            seg_areas[ix] = (x2 - x1 + 1) * (y2 - y1 + 1)\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_ishard\': ishards,\n                \'gt_overlaps\' : overlaps,\n                \'flipped\' : False,\n                \'seg_areas\' : seg_areas}\n\n    def _get_comp_id(self):\n        comp_id = (self._comp_id + \'_\' + self._salt if self.config[\'use_salt\']\n            else self._comp_id)\n        return comp_id\n\n    def _get_voc_results_file_template(self):\n        # VOCdevkit/results/VOC2007/Main/<comp_id>_det_test_aeroplane.txt\n        filename = self._get_comp_id() + \'_det_\' + self._image_set + \'_{:s}.txt\'\n        filedir = os.path.join(self._devkit_path, \'results\', \'VOC\' + self._year, \'Main\')\n        if not os.path.exists(filedir):\n            os.makedirs(filedir)\n        path = os.path.join(filedir, filename)\n        return path\n\n    def _write_voc_results_file(self, all_boxes):\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == \'__background__\':\n                continue\n            print \'Writing {} VOC results file\'.format(cls)\n            filename = self._get_voc_results_file_template().format(cls)\n            with open(filename, \'wt\') as f:\n                for im_ind, index in enumerate(self.image_index):\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    # the VOCdevkit expects 1-based indices\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                                format(index, dets[k, -1],\n                                       dets[k, 0] + 1, dets[k, 1] + 1,\n                                       dets[k, 2] + 1, dets[k, 3] + 1))\n\n    def _do_python_eval(self, output_dir = \'output\'):\n        annopath = os.path.join(\n            self._devkit_path,\n            \'VOC\' + self._year,\n            \'Annotations\',\n            \'{:s}.xml\')\n        imagesetfile = os.path.join(\n            self._devkit_path,\n            \'VOC\' + self._year,\n            \'ImageSets\',\n            \'Main\',\n            self._image_set + \'.txt\')\n        cachedir = os.path.join(self._devkit_path, \'annotations_cache\')\n        aps = []\n        # The PASCAL VOC metric changed in 2010\n        use_07_metric = True if int(self._year) < 2010 else False\n        print \'VOC07 metric? \' + (\'Yes\' if use_07_metric else \'No\')\n        if not os.path.isdir(output_dir):\n            os.mkdir(output_dir)\n        for i, cls in enumerate(self._classes):\n            if cls == \'__background__\':\n                continue\n            filename = self._get_voc_results_file_template().format(cls)\n            rec, prec, ap = voc_eval(\n                filename, annopath, imagesetfile, cls, cachedir, ovthresh=0.5,\n                use_07_metric=use_07_metric)\n            aps += [ap]\n            print(\'AP for {} = {:.4f}\'.format(cls, ap))\n            with open(os.path.join(output_dir, cls + \'_pr.pkl\'), \'w\') as f:\n                cPickle.dump({\'rec\': rec, \'prec\': prec, \'ap\': ap}, f)\n        print(\'Mean AP = {:.4f}\'.format(np.mean(aps)))\n        print(\'~~~~~~~~\')\n        print(\'Results:\')\n        for ap in aps:\n            print(\'{:.3f}\'.format(ap))\n        print(\'{:.3f}\'.format(np.mean(aps)))\n        print(\'~~~~~~~~\')\n        print(\'\')\n        print(\'--------------------------------------------------------------\')\n        print(\'Results computed with the **unofficial** Python eval code.\')\n        print(\'Results should be very close to the official MATLAB eval code.\')\n        print(\'Recompute with `./tools/reval.py --matlab ...` for your paper.\')\n        print(\'-- Thanks, The Management\')\n        print(\'--------------------------------------------------------------\')\n\n    def _do_matlab_eval(self, output_dir=\'output\'):\n        print \'-----------------------------------------------------\'\n        print \'Computing results with the official MATLAB eval code.\'\n        print \'-----------------------------------------------------\'\n        path = os.path.join(cfg.ROOT_DIR, \'lib\', \'datasets\',\n                            \'VOCdevkit-matlab-wrapper\')\n        cmd = \'cd {} && \'.format(path)\n        cmd += \'{:s} -nodisplay -nodesktop \'.format(cfg.MATLAB)\n        cmd += \'-r ""dbstop if error; \'\n        cmd += \'voc_eval(\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\'); quit;""\' \\\n               .format(self._devkit_path, self._get_comp_id(),\n                       self._image_set, output_dir)\n        print(\'Running:\\n{}\'.format(cmd))\n        status = subprocess.call(cmd, shell=True)\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        self._write_voc_results_file(all_boxes)\n        self._do_python_eval(output_dir)\n        if self.config[\'matlab_eval\']:\n            self._do_matlab_eval(output_dir)\n        if self.config[\'cleanup\']:\n            for cls in self._classes:\n                if cls == \'__background__\':\n                    continue\n                filename = self._get_voc_results_file_template().format(cls)\n                os.remove(filename)\n\n    def competition_mode(self, on):\n        if on:\n            self.config[\'use_salt\'] = False\n            self.config[\'cleanup\'] = False\n        else:\n            self.config[\'use_salt\'] = True\n            self.config[\'cleanup\'] = True\n\nif __name__ == \'__main__\':\n    d = pascal_voc(\'trainval\', \'2007\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
lib/datasets/pascal_voc2.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport xml.dom.minidom as minidom\n\nimport os\nimport PIL\nimport numpy as np\nimport scipy.sparse\nimport subprocess\nimport cPickle\nimport math\nimport glob\nimport uuid\nimport scipy.io as sio\nimport xml.etree.ElementTree as ET\n\nfrom .imdb import imdb\nfrom .imdb import ROOT_DIR\nfrom .imdb import MATLAB\n\nfrom ..utils.cython_bbox import bbox_overlaps\nfrom ..utils.boxes_grid import get_boxes_grid\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\nfrom ..rpn_msr.generate_anchors import generate_anchors\n# <<<< obsolete\n\nclass pascal_voc(imdb):\n    def __init__(self, image_set, year, pascal_path=None):\n        imdb.__init__(self, \'voc_\' + year + \'_\' + image_set)\n        self._year = year\n        self._image_set = image_set\n        self._pascal_path = self._get_default_path() if pascal_path is None \\\n                            else pascal_path\n        self._data_path = os.path.join(self._pascal_path, \'VOCdevkit\' + self._year, \'VOC\' + self._year)\n        self._classes = (\'__background__\', # always index 0\n                         \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n                         \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n                         \'cow\', \'diningtable\', \'dog\', \'horse\',\n                         \'motorbike\', \'person\', \'pottedplant\',\n                         \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.jpg\'\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        if cfg.IS_RPN:\n            self._roidb_handler = self.gt_roidb\n        else:\n            self._roidb_handler = self.region_proposal_roidb\n\n        # num of subclasses\n        self._num_subclasses = 240 + 1\n\n        # load the mapping for subcalss to class\n        filename = os.path.join(self._pascal_path, \'subcategory_exemplars\', \'mapping.txt\')\n        assert os.path.exists(filename), \'Path does not exist: {}\'.format(filename)\n        \n        mapping = np.zeros(self._num_subclasses, dtype=np.int)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = self._class_to_ind[words[1]]\n        self._subclass_mapping = mapping\n\n        # PASCAL specific config options\n        self.config = {\'cleanup\'  : True,\n                       \'use_salt\' : True,\n                       \'top_k\'    : 2000}\n\n        # statistics for computing recall\n        self._num_boxes_all = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_covered = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_proposal = 0\n\n        assert os.path.exists(self._pascal_path), \\\n                \'PASCAL path does not exist: {}\'.format(self._pascal_path)\n        assert os.path.exists(self._data_path), \\\n                \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self._image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        image_path = os.path.join(self._data_path, \'JPEGImages\',\n                                  index + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        # Example path to image set file:\n        # self._pascal_path + /VOCdevkit2007/VOC2007/ImageSets/Main/val.txt\n        image_set_file = os.path.join(self._data_path, \'ImageSets\', \'Main\',\n                                      self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n                \'Path does not exist: {}\'.format(image_set_file)\n        with open(image_set_file) as f:\n            image_index = [x.strip() for x in f.readlines()]\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where PASCAL VOC is expected to be installed.\n        """"""\n        return os.path.join(ROOT_DIR, \'data\', \'PASCAL\')\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path, self.name + \'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} gt roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = [self._load_pascal_subcategory_exemplar_annotation(index)\n                    for index in self.image_index]\n\n        if cfg.IS_RPN:\n            # print out recall\n            for i in xrange(1, self.num_classes):\n                print \'{}: Total number of boxes {:d}\'.format(self.classes[i], self._num_boxes_all[i])\n                print \'{}: Number of boxes covered {:d}\'.format(self.classes[i], self._num_boxes_covered[i])\n                print \'{}: Recall {:f}\'.format(self.classes[i], float(self._num_boxes_covered[i]) / float(self._num_boxes_all[i]))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote gt roidb to {}\'.format(cache_file)\n\n        return gt_roidb\n\n\n    def _load_pascal_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from XML file in the PASCAL VOC\n        format.\n        """"""\n        filename = os.path.join(self._data_path, \'Annotations\', index + \'.xml\')\n        # print \'Loading: {}\'.format(filename)\n        def get_data_from_tag(node, tag):\n            return node.getElementsByTagName(tag)[0].childNodes[0].data\n\n        with open(filename) as f:\n            data = minidom.parseString(f.read())\n\n        objs = data.getElementsByTagName(\'object\')\n        num_objs = len(objs)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n\n        # Load object bounding boxes into a data frame.\n        for ix, obj in enumerate(objs):\n            # Make pixel indexes 0-based\n            x1 = float(get_data_from_tag(obj, \'xmin\')) - 1\n            y1 = float(get_data_from_tag(obj, \'ymin\')) - 1\n            x2 = float(get_data_from_tag(obj, \'xmax\')) - 1\n            y2 = float(get_data_from_tag(obj, \'ymax\')) - 1\n            cls = self._class_to_ind[\n                    str(get_data_from_tag(obj, ""name"")).lower().strip()]\n            boxes[ix, :] = [x1, y1, x2, y2]\n            gt_classes[ix] = cls\n            overlaps[ix, cls] = 1.0\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        gt_subclasses = np.zeros((num_objs), dtype=np.int32)\n        gt_subclasses_flipped = np.zeros((num_objs), dtype=np.int32)\n        subindexes = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes_flipped = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n\n        if cfg.IS_RPN:\n            if cfg.IS_MULTISCALE:\n                # compute overlaps between grid boxes and gt boxes in multi-scales\n                # rescale the gt boxes\n                boxes_all = np.zeros((0, 4), dtype=np.float32)\n                for scale in cfg.TRAIN.SCALES:\n                    boxes_all = np.vstack((boxes_all, boxes * scale))\n                gt_classes_all = np.tile(gt_classes, len(cfg.TRAIN.SCALES))\n\n                # compute grid boxes\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n                boxes_grid, _, _ = get_boxes_grid(image_height, image_width)\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(boxes_grid.astype(np.float), boxes_all.astype(np.float))\n        \n                # check how many gt boxes are covered by grids\n                if num_objs != 0:\n                    index = np.tile(range(num_objs), len(cfg.TRAIN.SCALES))\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes_all == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n                    index_covered = np.unique(index[fg_inds])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[index_covered] == i)[0])\n            else:\n                assert len(cfg.TRAIN.SCALES_BASE) == 1\n                scale = cfg.TRAIN.SCALES_BASE[0]\n                feat_stride = 16\n                # faster rcnn region proposal\n                anchors = generate_anchors()\n                num_anchors = anchors.shape[0]\n\n                # image size\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n\n                # height and width of the heatmap\n                height = np.round((image_height * scale - 1) / 4.0 + 1)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n\n                width = np.round((image_width * scale - 1) / 4.0 + 1)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n\n                # gt boxes\n                gt_boxes = boxes * scale\n\n                # 1. Generate proposals from bbox deltas and shifted anchors\n                shift_x = np.arange(0, width) * feat_stride\n                shift_y = np.arange(0, height) * feat_stride\n                shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n                shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n                # add A anchors (1, A, 4) to\n                # cell K shifts (K, 1, 4) to get\n                # shift anchors (K, A, 4)\n                # reshape to (K*A, 4) shifted anchors\n                A = num_anchors\n                K = shifts.shape[0]\n                all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n                all_anchors = all_anchors.reshape((K * A, 4))\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(all_anchors.astype(np.float), gt_boxes.astype(np.float))\n        \n                # check how many gt boxes are covered by anchors\n                if num_objs != 0:\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_subclasses\': gt_subclasses,\n                \'gt_subclasses_flipped\': gt_subclasses_flipped,\n                \'gt_overlaps\' : overlaps,\n                \'gt_subindexes\': subindexes,\n                \'gt_subindexes_flipped\': subindexes_flipped,\n                \'flipped\' : False}\n\n\n    def _load_pascal_subcategory_exemplar_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from txt file in the pascal subcategory exemplar format.\n        """"""\n        if self._image_set == \'test\':\n            return self._load_pascal_annotation(index)\n\n        filename = os.path.join(self._pascal_path, \'subcategory_exemplars\', index + \'.txt\')\n        assert os.path.exists(filename), \\\n                \'Path does not exist: {}\'.format(filename)\n\n        # the annotation file contains flipped objects    \n        lines = []\n        lines_flipped = []\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[1])\n                is_flip = int(words[2])\n                if subcls != -1:\n                    if is_flip == 0:\n                        lines.append(line)\n                    else:\n                        lines_flipped.append(line)\n        \n        num_objs = len(lines)\n\n        # store information of flipped objects\n        assert (num_objs == len(lines_flipped)), \'The number of flipped objects is not the same!\'\n        gt_subclasses_flipped = np.zeros((num_objs), dtype=np.int32)\n        \n        for ix, line in enumerate(lines_flipped):\n            words = line.split()\n            subcls = int(words[1])\n            gt_subclasses_flipped[ix] = subcls\n\n        boxes = np.zeros((num_objs, 4), dtype=np.float32)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        gt_subclasses = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        subindexes = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes_flipped = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n\n        for ix, line in enumerate(lines):\n            words = line.split()\n            cls = self._class_to_ind[words[0]]\n            subcls = int(words[1])\n            # Make pixel indexes 0-based\n            boxes[ix, :] = [float(n)-1 for n in words[3:7]]\n            gt_classes[ix] = cls\n            gt_subclasses[ix] = subcls\n            overlaps[ix, cls] = 1.0\n            subindexes[ix, cls] = subcls\n            subindexes_flipped[ix, cls] = gt_subclasses_flipped[ix]\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n\n        if cfg.IS_RPN:\n            if cfg.IS_MULTISCALE:\n                # compute overlaps between grid boxes and gt boxes in multi-scales\n                # rescale the gt boxes\n                boxes_all = np.zeros((0, 4), dtype=np.float32)\n                for scale in cfg.TRAIN.SCALES:\n                    boxes_all = np.vstack((boxes_all, boxes * scale))\n                gt_classes_all = np.tile(gt_classes, len(cfg.TRAIN.SCALES))\n\n                # compute grid boxes\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n                boxes_grid, _, _ = get_boxes_grid(image_height, image_width)\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(boxes_grid.astype(np.float), boxes_all.astype(np.float))\n        \n                # check how many gt boxes are covered by grids\n                if num_objs != 0:\n                    index = np.tile(range(num_objs), len(cfg.TRAIN.SCALES))\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes_all == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n                    index_covered = np.unique(index[fg_inds])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[index_covered] == i)[0])\n            else:\n                assert len(cfg.TRAIN.SCALES_BASE) == 1\n                scale = cfg.TRAIN.SCALES_BASE[0]\n                feat_stride = 16\n                # faster rcnn region proposal\n                base_size = 16\n                ratios = [3.0, 2.0, 1.5, 1.0, 0.75, 0.5, 0.25]\n                scales = 2**np.arange(1, 6, 0.5)\n                anchors = generate_anchors(base_size, ratios, scales)\n                num_anchors = anchors.shape[0]\n\n                # image size\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n\n                # height and width of the heatmap\n                height = np.round((image_height * scale - 1) / 4.0 + 1)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n\n                width = np.round((image_width * scale - 1) / 4.0 + 1)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n\n                # gt boxes\n                gt_boxes = boxes * scale\n\n                # 1. Generate proposals from bbox deltas and shifted anchors\n                shift_x = np.arange(0, width) * feat_stride\n                shift_y = np.arange(0, height) * feat_stride\n                shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n                shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n                # add A anchors (1, A, 4) to\n                # cell K shifts (K, 1, 4) to get\n                # shift anchors (K, A, 4)\n                # reshape to (K*A, 4) shifted anchors\n                A = num_anchors\n                K = shifts.shape[0]\n                all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n                all_anchors = all_anchors.reshape((K * A, 4))\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(all_anchors.astype(np.float), gt_boxes.astype(np.float))\n        \n                # check how many gt boxes are covered by anchors\n                if num_objs != 0:\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_subclasses\': gt_subclasses,\n                \'gt_subclasses_flipped\': gt_subclasses_flipped,\n                \'gt_overlaps\': overlaps,\n                \'gt_subindexes\': subindexes, \n                \'gt_subindexes_flipped\': subindexes_flipped, \n                \'flipped\' : False}\n\n    def region_proposal_roidb(self):\n        """"""\n        Return the database of regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_\' + cfg.REGION_PROPOSAL + \'_region_proposal_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n\n            print \'Loading region proposal network boxes...\'\n            model = cfg.REGION_PROPOSAL\n            rpn_roidb = self._load_rpn_roidb(gt_roidb, model)\n            print \'Region proposal network boxes loaded\'\n            roidb = imdb.merge_roidbs(rpn_roidb, gt_roidb)\n        else:\n            print \'Loading region proposal network boxes...\'\n            model = cfg.REGION_PROPOSAL\n            roidb = self._load_rpn_roidb(None, model)\n            print \'Region proposal network boxes loaded\'\n\n        print \'{} region proposals per image\'.format(self._num_boxes_proposal / len(self.image_index))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb, model):\n        # set the prefix\n        if self._image_set == \'test\':\n            prefix = model + \'/testing\'\n        else:\n            prefix = model + \'/training\'\n\n        box_list = []\n        for index in self.image_index:\n            filename = os.path.join(self._pascal_path, \'region_proposals\',  prefix, index + \'.txt\')\n            assert os.path.exists(filename), \\\n                \'RPN data not found at: {}\'.format(filename)\n            raw_data = np.loadtxt(filename, dtype=float)\n            if len(raw_data.shape) == 1:\n                if raw_data.size == 0:\n                    raw_data = raw_data.reshape((0, 5))\n                else:\n                    raw_data = raw_data.reshape((1, 5))\n\n            x1 = raw_data[:, 0]\n            y1 = raw_data[:, 1]\n            x2 = raw_data[:, 2]\n            y2 = raw_data[:, 3]\n            score = raw_data[:, 4]\n            inds = np.where((x2 > x1) & (y2 > y1))[0]\n            raw_data = raw_data[inds,:4]\n            self._num_boxes_proposal += raw_data.shape[0]\n            box_list.append(raw_data)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n\n    def selective_search_roidb(self):\n        """"""\n        Return the database of selective search regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_selective_search_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} ss roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if int(self._year) == 2007 or self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n            ss_roidb = self._load_selective_search_roidb(gt_roidb)\n            roidb = imdb.merge_roidbs(gt_roidb, ss_roidb)\n        else:\n            roidb = self._load_selective_search_roidb(None)\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote ss roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_selective_search_roidb(self, gt_roidb):\n        filename = os.path.abspath(os.path.join(self.cache_path, \'..\',\n                                                \'selective_search_data\',\n                                                self.name + \'.mat\'))\n        assert os.path.exists(filename), \\\n               \'Selective search data not found at: {}\'.format(filename)\n        raw_data = sio.loadmat(filename)[\'boxes\'].ravel()\n\n        box_list = []\n        for i in xrange(raw_data.shape[0]):\n            box_list.append(raw_data[i][:, (1, 0, 3, 2)] - 1)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def selective_search_IJCV_roidb(self):\n        """"""\n        Return the database of selective search regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                \'{:s}_selective_search_IJCV_top_{:d}_roidb.pkl\'.\n                format(self.name, self.config[\'top_k\']))\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} ss roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = self.gt_roidb()\n        ss_roidb = self._load_selective_search_IJCV_roidb(gt_roidb)\n        roidb = imdb.merge_roidbs(gt_roidb, ss_roidb)\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote ss roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_selective_search_IJCV_roidb(self, gt_roidb):\n        IJCV_path = os.path.abspath(os.path.join(self.cache_path, \'..\',\n                                                 \'selective_search_IJCV_data\',\n                                                 \'voc_\' + self._year))\n        assert os.path.exists(IJCV_path), \\\n               \'Selective search IJCV data not found at: {}\'.format(IJCV_path)\n\n        top_k = self.config[\'top_k\']\n        box_list = []\n        for i in xrange(self.num_images):\n            filename = os.path.join(IJCV_path, self.image_index[i] + \'.mat\')\n            raw_data = sio.loadmat(filename)\n            box_list.append((raw_data[\'boxes\'][:top_k, :]-1).astype(np.uint16))\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n\n    def _write_voc_results_file(self, all_boxes):\n        use_salt = self.config[\'use_salt\']\n        comp_id = \'comp4\'\n        if use_salt:\n            comp_id += \'-{}\'.format(os.getpid())\n\n        # VOCdevkit/results/VOC2007/Main/comp4-44503_det_test_aeroplane.txt\n        path = os.path.join(self._pascal_path, \'VOCdevkit\' + self._year, \'results\', \'VOC\' + self._year,\n                            \'Main\', comp_id + \'_\')\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == \'__background__\':\n                continue\n            print \'Writing {} VOC results file\'.format(cls)\n            filename = path + \'det_\' + self._image_set + \'_\' + cls + \'.txt\'\n            print filename\n            with open(filename, \'wt\') as f:\n                for im_ind, index in enumerate(self.image_index):\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    # the VOCdevkit expects 1-based indices\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                                format(index, dets[k, 4],\n                                       dets[k, 0] + 1, dets[k, 1] + 1,\n                                       dets[k, 2] + 1, dets[k, 3] + 1))\n        return comp_id\n\n    def _do_matlab_eval(self, comp_id, output_dir=\'output\'):\n        rm_results = self.config[\'cleanup\']\n\n        path = os.path.join(os.path.dirname(__file__),\n                            \'VOCdevkit-matlab-wrapper\')\n        cmd = \'cd {} && \'.format(path)\n        cmd += \'{:s} -nodisplay -nodesktop \'.format(MATLAB)\n        cmd += \'-r ""dbstop if error; \'\n        cmd += \'voc_eval(\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\',{:d}); quit;""\' \\\n               .format(self._pascal_path + \'/VOCdevkit\' + self._year, comp_id,\n                       self._image_set, output_dir, int(rm_results))\n        print(\'Running:\\n{}\'.format(cmd))\n        status = subprocess.call(cmd, shell=True)\n\n    # evaluate detection results\n    def evaluate_detections(self, all_boxes, output_dir):\n        comp_id = self._write_voc_results_file(all_boxes)\n        self._do_matlab_eval(comp_id, output_dir)\n\n    def evaluate_proposals(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing PASCAL results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(\\\n                                 dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    def evaluate_proposals_msr(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing PASCAL results to file \' + filename\n            with open(filename, \'wt\') as f:\n                dets = all_boxes[im_ind]\n                if dets == []:\n                    continue\n                for k in xrange(dets.shape[0]):\n                    f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n\n    def competition_mode(self, on):\n        if on:\n            self.config[\'use_salt\'] = False\n            self.config[\'cleanup\'] = False\n        else:\n            self.config[\'use_salt\'] = True\n            self.config[\'cleanup\'] = True\n\nif __name__ == \'__main__\':\n    d = pascal_voc(\'trainval\', \'2007\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
lib/datasets/voc_eval.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Bharath Hariharan\n# --------------------------------------------------------\n\nimport xml.etree.ElementTree as ET\nimport os\nimport cPickle\nimport numpy as np\nimport pdb\ndef parse_rec(filename):\n    """""" Parse a PASCAL VOC xml file """"""\n    tree = ET.parse(filename)\n    objects = []\n    for obj in tree.findall(\'object\'):\n        obj_struct = {}\n        obj_struct[\'name\'] = obj.find(\'name\').text\n        obj_struct[\'pose\'] = obj.find(\'pose\').text\n        obj_struct[\'truncated\'] = int(obj.find(\'truncated\').text)\n        obj_struct[\'difficult\'] = int(obj.find(\'difficult\').text)\n        bbox = obj.find(\'bndbox\')\n        obj_struct[\'bbox\'] = [int(bbox.find(\'xmin\').text),\n                              int(bbox.find(\'ymin\').text),\n                              int(bbox.find(\'xmax\').text),\n                              int(bbox.find(\'ymax\').text)]\n        objects.append(obj_struct)\n\n    return objects\n\ndef voc_ap(rec, prec, use_07_metric=False):\n    """""" ap = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:False).\n    """"""\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\ndef voc_eval(detpath,\n             annopath,\n             imagesetfile,\n             classname,\n             cachedir,\n             ovthresh=0.5,\n             use_07_metric=False):\n    """"""rec, prec, ap = voc_eval(detpath,\n                                annopath,\n                                imagesetfile,\n                                classname,\n                                [ovthresh],\n                                [use_07_metric])\n\n    Top level function that does the PASCAL VOC evaluation.\n\n    detpath: Path to detections\n        detpath.format(classname) should produce the detection results file.\n    annopath: Path to annotations\n        annopath.format(imagename) should be the xml annotations file.\n    imagesetfile: Text file containing the list of images, one image per line.\n    classname: Category name (duh)\n    cachedir: Directory for caching the annotations\n    [ovthresh]: Overlap threshold (default = 0.5)\n    [use_07_metric]: Whether to use VOC07\'s 11 point AP computation\n        (default False)\n    """"""\n    # assumes detections are in detpath.format(classname)\n    # assumes annotations are in annopath.format(imagename)\n    # assumes imagesetfile is a text file with each line an image name\n    # cachedir caches the annotations in a pickle file\n\n    # first load gt\n    if not os.path.isdir(cachedir):\n        os.mkdir(cachedir)\n    cachefile = os.path.join(cachedir, \'annots.pkl\')\n    # read list of images\n    with open(imagesetfile, \'r\') as f:\n        lines = f.readlines()\n    imagenames = [x.strip() for x in lines]\n\n    if not os.path.isfile(cachefile):\n        # load annots\n        recs = {}\n        for i, imagename in enumerate(imagenames):\n            recs[imagename] = parse_rec(annopath.format(imagename))\n            if i % 100 == 0:\n                print \'Reading annotation for {:d}/{:d}\'.format(\n                    i + 1, len(imagenames))\n        # save\n        print \'Saving cached annotations to {:s}\'.format(cachefile)\n        with open(cachefile, \'w\') as f:\n            cPickle.dump(recs, f)\n    else:\n        # load\n        with open(cachefile, \'r\') as f:\n            recs = cPickle.load(f)\n\n    # extract gt objects for this class\n    class_recs = {}\n    npos = 0\n    for imagename in imagenames:\n        R = [obj for obj in recs[imagename] if obj[\'name\'] == classname]\n        bbox = np.array([x[\'bbox\'] for x in R])\n        difficult = np.array([x[\'difficult\'] for x in R]).astype(np.bool)\n        det = [False] * len(R)\n        npos = npos + sum(~difficult)\n        class_recs[imagename] = {\'bbox\': bbox,\n                                 \'difficult\': difficult,\n                                 \'det\': det}\n\n    # read dets\n    detfile = detpath.format(classname)\n    with open(detfile, \'r\') as f:\n        lines = f.readlines()\n    if any(lines) == 1:\n\n        splitlines = [x.strip().split(\' \') for x in lines]\n        image_ids = [x[0] for x in splitlines]\n        confidence = np.array([float(x[1]) for x in splitlines])\n        BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n\n        # sort by confidence\n        sorted_ind = np.argsort(-confidence)\n        sorted_scores = np.sort(-confidence)\n        BB = BB[sorted_ind, :]\n        image_ids = [image_ids[x] for x in sorted_ind]\n\n        # go down dets and mark TPs and FPs\n        nd = len(image_ids)\n        tp = np.zeros(nd)\n        fp = np.zeros(nd)\n        for d in range(nd):\n            R = class_recs[image_ids[d]]\n            bb = BB[d, :].astype(float)\n            ovmax = -np.inf\n            BBGT = R[\'bbox\'].astype(float)\n\n            if BBGT.size > 0:\n                # compute overlaps\n                # intersection\n                ixmin = np.maximum(BBGT[:, 0], bb[0])\n                iymin = np.maximum(BBGT[:, 1], bb[1])\n                ixmax = np.minimum(BBGT[:, 2], bb[2])\n                iymax = np.minimum(BBGT[:, 3], bb[3])\n                iw = np.maximum(ixmax - ixmin + 1., 0.)\n                ih = np.maximum(iymax - iymin + 1., 0.)\n                inters = iw * ih\n\n                # union\n                uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n                       (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n                       (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n\n                overlaps = inters / uni\n                ovmax = np.max(overlaps)\n                jmax = np.argmax(overlaps)\n\n            if ovmax > ovthresh:\n                if not R[\'difficult\'][jmax]:\n                    if not R[\'det\'][jmax]:\n                        tp[d] = 1.\n                        R[\'det\'][jmax] = 1\n                    else:\n                        fp[d] = 1.\n            else:\n                fp[d] = 1.\n\n        # compute precision recall\n        fp = np.cumsum(fp)\n        tp = np.cumsum(tp)\n        rec = tp / float(npos)\n        # avoid divide by zero in case the first detection matches a difficult\n        # ground truth\n        prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n        ap = voc_ap(rec, prec, use_07_metric)\n    else:\n         rec = -1\n         prec = -1\n         ap = -1\n\n    return rec, prec, ap\n'"
lib/fast_rcnn/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nfrom . import config\nfrom . import train\nfrom . import test\nfrom . import nms_wrapper\n# from nms_wrapper import nms'
lib/fast_rcnn/bbox_transform.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\nimport warnings\n\ndef bbox_transform(ex_rois, gt_rois):\n    """"""\n    computes the distance from ground-truth boxes to the given boxes, normed by their size\n    :param ex_rois: n * 4 numpy array, given boxes\n    :param gt_rois: n * 4 numpy array, ground-truth boxes\n    :return: deltas: n * 4 numpy array, ground-truth boxes\n    """"""\n    ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0\n    ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0\n    ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n    ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n\n    assert np.min(ex_widths) > 0.1 and np.min(ex_heights) > 0.1, \\\n        \'Invalid boxes found: {} {}\'. \\\n            format(ex_rois[np.argmin(ex_widths), :], ex_rois[np.argmin(ex_heights), :])\n\n    gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + 1.0\n    gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + 1.0\n    gt_ctr_x = gt_rois[:, 0] + 0.5 * gt_widths\n    gt_ctr_y = gt_rois[:, 1] + 0.5 * gt_heights\n\n    # warnings.catch_warnings()\n    # warnings.filterwarnings(\'error\')\n    targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n    targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n    targets_dw = np.log(gt_widths / ex_widths)\n    targets_dh = np.log(gt_heights / ex_heights)\n\n    targets = np.vstack(\n        (targets_dx, targets_dy, targets_dw, targets_dh)).transpose()\n    return targets\n\ndef bbox_transform_inv(boxes, deltas):\n    if boxes.shape[0] == 0:\n        return np.zeros((0, deltas.shape[1]), dtype=deltas.dtype)\n\n    boxes = boxes.astype(deltas.dtype, copy=False)\n\n    widths = boxes[:, 2] - boxes[:, 0] + 1.0\n    heights = boxes[:, 3] - boxes[:, 1] + 1.0\n    ctr_x = boxes[:, 0] + 0.5 * widths\n    ctr_y = boxes[:, 1] + 0.5 * heights\n\n    dx = deltas[:, 0::4]\n    dy = deltas[:, 1::4]\n    dw = deltas[:, 2::4]\n    dh = deltas[:, 3::4]\n\n    pred_ctr_x = dx * widths[:, np.newaxis] + ctr_x[:, np.newaxis]\n    pred_ctr_y = dy * heights[:, np.newaxis] + ctr_y[:, np.newaxis]\n    pred_w = np.exp(dw) * widths[:, np.newaxis]\n    pred_h = np.exp(dh) * heights[:, np.newaxis]\n\n    pred_boxes = np.zeros(deltas.shape, dtype=deltas.dtype)\n    # x1\n    pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n    # y1\n    pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n    # x2\n    pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n    # y2\n    pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n    return pred_boxes\n\ndef clip_boxes(boxes, im_shape):\n    """"""\n    Clip boxes to image boundaries.\n    """"""\n\n    # x1 >= 0\n    boxes[:, 0::4] = np.maximum(np.minimum(boxes[:, 0::4], im_shape[1] - 1), 0)\n    # y1 >= 0\n    boxes[:, 1::4] = np.maximum(np.minimum(boxes[:, 1::4], im_shape[0] - 1), 0)\n    # x2 < im_shape[1]\n    boxes[:, 2::4] = np.maximum(np.minimum(boxes[:, 2::4], im_shape[1] - 1), 0)\n    # y2 < im_shape[0]\n    boxes[:, 3::4] = np.maximum(np.minimum(boxes[:, 3::4], im_shape[0] - 1), 0)\n    return boxes\n'"
lib/fast_rcnn/config.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Fast R-CNN config system.\n\nThis file specifies default config options for Fast R-CNN. You should not\nchange values in this file. Instead, you should write a config file (in yaml)\nand use cfg_from_file(yaml_file) to load it and override the default options.\n\nMost tools in $ROOT/tools take a --cfg option to specify an override file.\n    - See tools/{train,test}_net.py for example code that uses cfg_from_file()\n    - See experiments/cfgs/*.yml for example YAML config override files\n""""""\n\nimport os\nimport os.path as osp\nimport numpy as np\nfrom time import strftime, localtime\nfrom easydict import EasyDict as edict\n\n__C = edict()\n# Consumers can get config by:\n#   from fast_rcnn_config import cfg\ncfg = __C\n\n#\n# Training options\n#\n\n# region proposal network (RPN) or not\n__C.IS_RPN = True\n__C.ANCHOR_SCALES = [8, 16, 32]\n__C.NCLASSES = 21\n\n# multiscale training and testing\n__C.IS_MULTISCALE = False\n__C.IS_EXTRAPOLATING = True\n\n__C.REGION_PROPOSAL = \'RPN\'\n\n__C.NET_NAME = \'VGGnet\'\n__C.SUBCLS_NAME = \'voxel_exemplars\'\n\n__C.TRAIN = edict()\n# Adam, Momentum, RMS\n__C.TRAIN.SOLVER = \'Momentum\'\n# learning rate\n__C.TRAIN.WEIGHT_DECAY = 0.0005\n__C.TRAIN.LEARNING_RATE = 0.001\n__C.TRAIN.MOMENTUM = 0.9\n__C.TRAIN.GAMMA = 0.1\n__C.TRAIN.STEPSIZE = 50000\n__C.TRAIN.DISPLAY = 10\n__C.TRAIN.LOG_IMAGE_ITERS = 100\n__C.TRAIN.OHEM = False\n__C.TRAIN.RANDOM_DOWNSAMPLE = False\n\n# Scales to compute real features\n__C.TRAIN.SCALES_BASE = (0.25, 0.5, 1.0, 2.0, 3.0)\n# __C.TRAIN.SCALES_BASE = (1.0,)\n\n# parameters for ROI generating\n#__C.TRAIN.SPATIAL_SCALE = 0.0625\n__C.TRAIN.KERNEL_SIZE = 5\n\n# Aspect ratio to use during training\n# __C.TRAIN.ASPECTS = (1, 0.75, 0.5, 0.25)\n__C.TRAIN.ASPECTS= (1,)\n\n\n# Scales to use during training (can list multiple scales)\n# Each scale is the pixel size of an image\'s shortest side\n__C.TRAIN.SCALES = (600,)\n\n# Max pixel size of the longest side of a scaled input image\n__C.TRAIN.MAX_SIZE = 1000\n\n# Images to use per minibatch\n__C.TRAIN.IMS_PER_BATCH = 2\n\n# Minibatch size (number of regions of interest [ROIs])\n__C.TRAIN.BATCH_SIZE = 128\n\n# Fraction of minibatch that is labeled foreground (i.e. class > 0)\n__C.TRAIN.FG_FRACTION = 0.25\n\n# Overlap threshold for a ROI to be considered foreground (if >= FG_THRESH)\n__C.TRAIN.FG_THRESH = 0.5\n\n# Overlap threshold for a ROI to be considered background (class = 0 if\n# overlap in [LO, HI))\n__C.TRAIN.BG_THRESH_HI = 0.5\n__C.TRAIN.BG_THRESH_LO = 0.1\n\n# Use horizontally-flipped images during training?\n__C.TRAIN.USE_FLIPPED = True\n\n# Train bounding-box regressors\n__C.TRAIN.BBOX_REG = True\n\n# Overlap required between a ROI and ground-truth box in order for that ROI to\n# be used as a bounding-box regression training example\n__C.TRAIN.BBOX_THRESH = 0.5\n\n# Iterations between snapshots\n__C.TRAIN.SNAPSHOT_ITERS = 5000\n\n# solver.prototxt specifies the snapshot path prefix, this adds an optional\n# infix to yield the path: <prefix>[_<infix>]_iters_XYZ.caffemodel\n__C.TRAIN.SNAPSHOT_PREFIX = \'VGGnet_fast_rcnn\'\n__C.TRAIN.SNAPSHOT_INFIX = \'\'\n\n# Use a prefetch thread in roi_data_layer.layer\n# So far I haven\'t found this useful; likely more engineering work is required\n__C.TRAIN.USE_PREFETCH = False\n\n# Normalize the targets (subtract empirical mean, divide by empirical stddev)\n__C.TRAIN.BBOX_NORMALIZE_TARGETS = True\n# Deprecated (inside weights)\n# used for assigning weights for each coords (x1, y1, w, h)\n__C.TRAIN.BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n# Normalize the targets using ""precomputed"" (or made up) means and stdevs\n# (BBOX_NORMALIZE_TARGETS must also be True)\n__C.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED = True\n__C.TRAIN.BBOX_NORMALIZE_MEANS = (0.0, 0.0, 0.0, 0.0)\n__C.TRAIN.BBOX_NORMALIZE_STDS = (0.1, 0.1, 0.2, 0.2)\n# faster rcnn dont use pre-generated rois by selective search\n# __C.TRAIN.BBOX_NORMALIZE_STDS = (1, 1, 1, 1)\n\n# Train using these proposals\n__C.TRAIN.PROPOSAL_METHOD = \'selective_search\'\n\n# Make minibatches from images that have similar aspect ratios (i.e. both\n# tall and thin or both short and wide) in order to avoid wasting computation\n# on zero-padding.\n__C.TRAIN.ASPECT_GROUPING = True\n# preclude rois intersected with dontcare areas above the value\n__C.TRAIN.DONTCARE_AREA_INTERSECTION_HI = 0.5\n__C.TRAIN.PRECLUDE_HARD_SAMPLES = True\n# Use RPN to detect objects\n__C.TRAIN.HAS_RPN = True\n# IOU >= thresh: positive example\n__C.TRAIN.RPN_POSITIVE_OVERLAP = 0.7\n# IOU < thresh: negative example\n__C.TRAIN.RPN_NEGATIVE_OVERLAP = 0.3\n# If an anchor statisfied by positive and negative conditions set to negative\n__C.TRAIN.RPN_CLOBBER_POSITIVES = False\n# Max number of foreground examples\n__C.TRAIN.RPN_FG_FRACTION = 0.5\n# Total number of examples\n__C.TRAIN.RPN_BATCHSIZE = 256\n# NMS threshold used on RPN proposals\n__C.TRAIN.RPN_NMS_THRESH = 0.7\n# Number of top scoring boxes to keep before apply NMS to RPN proposals\n__C.TRAIN.RPN_PRE_NMS_TOP_N = 12000\n# Number of top scoring boxes to keep after applying NMS to RPN proposals\n__C.TRAIN.RPN_POST_NMS_TOP_N = 2000\n# Proposal height and width both need to be greater than RPN_MIN_SIZE (at orig image scale)\n__C.TRAIN.RPN_MIN_SIZE = 16\n# Deprecated (outside weights)\n__C.TRAIN.RPN_BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n# Give the positive RPN examples weight of p * 1 / {num positives}\n# and give negatives a weight of (1 - p)\n# Set to -1.0 to use uniform example weighting\n__C.TRAIN.RPN_POSITIVE_WEIGHT = -1.0\n# __C.TRAIN.RPN_POSITIVE_WEIGHT = 0.5\n\n\n#\n# Testing options\n#\n\n__C.TEST = edict()\n\n# Scales to use during testing (can list multiple scales)\n# Each scale is the pixel size of an image\'s shortest side\n__C.TEST.SCALES = (600,)\n\n# Max pixel size of the longest side of a scaled input image\n__C.TEST.MAX_SIZE = 1000\n\n# Overlap threshold used for non-maximum suppression (suppress boxes with\n# IoU >= this threshold)\n__C.TEST.NMS = 0.3\n\n# Experimental: treat the (K+1) units in the cls_score layer as linear\n# predictors (trained, eg, with one-vs-rest SVMs).\n__C.TEST.SVM = False\n\n# Test using bounding-box regressors\n__C.TEST.BBOX_REG = True\n\n# Propose boxes\n__C.TEST.HAS_RPN = True\n\n# Test using these proposals\n__C.TEST.PROPOSAL_METHOD = \'selective_search\'\n\n## NMS threshold used on RPN proposals\n__C.TEST.RPN_NMS_THRESH = 0.7\n## Number of top scoring boxes to keep before apply NMS to RPN proposals\n__C.TEST.RPN_PRE_NMS_TOP_N = 6000\n#__C.TEST.RPN_PRE_NMS_TOP_N = 12000\n## Number of top scoring boxes to keep after applying NMS to RPN proposals\n__C.TEST.RPN_POST_NMS_TOP_N = 300\n#__C.TEST.RPN_POST_NMS_TOP_N = 2000\n# Proposal height and width both need to be greater than RPN_MIN_SIZE (at orig image scale)\n__C.TEST.RPN_MIN_SIZE = 16\n\n\n#\n# MISC\n#\n\n# The mapping from image coordinates to feature map coordinates might cause\n# some boxes that are distinct in image space to become identical in feature\n# coordinates. If DEDUP_BOXES > 0, then DEDUP_BOXES is used as the scale factor\n# for identifying duplicate boxes.\n# 1/16 is correct for {Alex,Caffe}Net, VGG_CNN_M_1024, and VGG16\n__C.DEDUP_BOXES = 1./16.\n\n# Pixel mean values (BGR order) as a (1, 1, 3) array\n# We use the same pixel mean for all networks even though it\'s not exactly what\n# they were trained with\n__C.PIXEL_MEANS = np.array([[[102.9801, 115.9465, 122.7717]]])\n\n# For reproducibility\n__C.RNG_SEED = 3\n\n# A small number that\'s used many times\n__C.EPS = 1e-14\n\n# Root directory of project\n__C.ROOT_DIR = osp.abspath(osp.join(osp.dirname(__file__), \'..\', \'..\'))\n\n# Data directory\n__C.DATA_DIR = osp.abspath(osp.join(__C.ROOT_DIR, \'data\'))\n\n# Model directory\n__C.MODELS_DIR = osp.abspath(osp.join(__C.ROOT_DIR, \'models\', \'pascal_voc\'))\n\n# Name (or path to) the matlab executable\n__C.MATLAB = \'matlab\'\n\n# Place outputs under an experiments directory\n__C.EXP_DIR = \'default\'\n__C.LOG_DIR = \'default\'\n\n# Use GPU implementation of non-maximum suppression\n__C.USE_GPU_NMS = True\n\n# Default GPU device id\n__C.GPU_ID = 0\n\ndef get_output_dir(imdb, weights_filename):\n    """"""Return the directory where experimental artifacts are placed.\n    If the directory does not exist, it is created.\n\n    A canonical path is built using the name from an imdb and a network\n    (if not None).\n    """"""\n    outdir = osp.abspath(osp.join(__C.ROOT_DIR, \'output\', __C.EXP_DIR, imdb.name))\n    if weights_filename is not None:\n        outdir = osp.join(outdir, weights_filename)\n    if not os.path.exists(outdir):\n        os.makedirs(outdir)\n    return outdir\n\ndef get_log_dir(imdb):\n    """"""Return the directory where experimental artifacts are placed.\n    If the directory does not exist, it is created.\n    A canonical path is built using the name from an imdb and a network\n    (if not None).\n    """"""\n    log_dir = osp.abspath(\\\n        osp.join(__C.ROOT_DIR, \'logs\', __C.LOG_DIR, imdb.name, strftime(""%Y-%m-%d-%H-%M-%S"", localtime())))\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n    return log_dir\n\ndef _merge_a_into_b(a, b):\n    """"""Merge config dictionary a into config dictionary b, clobbering the\n    options in b whenever they are also specified in a.\n    """"""\n    if type(a) is not edict:\n        return\n\n    for k, v in a.iteritems():\n        # a must specify keys that are in b\n        if not b.has_key(k):\n            raise KeyError(\'{} is not a valid config key\'.format(k))\n\n        # the types must match, too\n        old_type = type(b[k])\n        if old_type is not type(v):\n            if isinstance(b[k], np.ndarray):\n                v = np.array(v, dtype=b[k].dtype)\n            else:\n                raise ValueError((\'Type mismatch ({} vs. {}) \'\n                                \'for config key: {}\').format(type(b[k]),\n                                                            type(v), k))\n\n        # recursively merge dicts\n        if type(v) is edict:\n            try:\n                _merge_a_into_b(a[k], b[k])\n            except:\n                print(\'Error under config key: {}\'.format(k))\n                raise\n        else:\n            b[k] = v\n\ndef cfg_from_file(filename):\n    """"""Load a config file and merge it into the default options.""""""\n    import yaml\n    with open(filename, \'r\') as f:\n        yaml_cfg = edict(yaml.load(f))\n\n    _merge_a_into_b(yaml_cfg, __C)\n\ndef cfg_from_list(cfg_list):\n    """"""Set config keys via list (e.g., from command line).""""""\n    from ast import literal_eval\n    assert len(cfg_list) % 2 == 0\n    for k, v in zip(cfg_list[0::2], cfg_list[1::2]):\n        key_list = k.split(\'.\')\n        d = __C\n        for subkey in key_list[:-1]:\n            assert d.has_key(subkey)\n            d = d[subkey]\n        subkey = key_list[-1]\n        assert d.has_key(subkey)\n        try:\n            value = literal_eval(v)\n        except:\n            # handle the case when v is a string literal\n            value = v\n        assert type(value) == type(d[subkey]), \\\n            \'type {} does not match original type {}\'.format(\n            type(value), type(d[subkey]))\n        d[subkey] = value\n'"
lib/fast_rcnn/config2.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Fast R-CNN config system.\n\nThis file specifies default config options for Fast R-CNN. You should not\nchange values in this file. Instead, you should write a config file (in yaml)\nand use cfg_from_file(yaml_file) to load it and override the default options.\n\nMost tools in $ROOT/tools take a --cfg option to specify an override file.\n    - See tools/{train,test}_net.py for example code that uses cfg_from_file()\n    - See experiments/cfgs/*.yml for example YAML config override files\n""""""\n\nimport os\nimport os.path as osp\nimport numpy as np\nimport math\n# `pip install easydict` if you don\'t have it\nfrom easydict import EasyDict as edict\n\n__C = edict()\n# Consumers can get config by:\n#   from fast_rcnn_config import cfg\ncfg = __C\n\n# region proposal network (RPN) or not\n__C.IS_RPN = True\n\n# multiscale training and testing\n__C.IS_MULTISCALE = False\n__C.IS_EXTRAPOLATING = True\n\n#\n__C.REGION_PROPOSAL = \'RPN\'\n\n__C.NET_NAME = \'CaffeNet\'\n__C.SUBCLS_NAME = \'voxel_exemplars\'\n\n#\n# Training options\n#\n\n__C.TRAIN = edict()\n\n# learning rate\n__C.TRAIN.LEARNING_RATE = 0.001\n__C.TRAIN.MOMENTUM = 0.9\n__C.TRAIN.GAMMA = 0.1\n__C.TRAIN.STEPSIZE = 30000\n\n# Scales to compute real features\n__C.TRAIN.SCALES_BASE = (0.25, 0.5, 1.0, 2.0, 3.0)\n\n# The number of scales per octave in the image pyramid\n# An octave is the set of scales up to half of the initial scale\n__C.TRAIN.NUM_PER_OCTAVE = 4\n\n# parameters for ROI generating\n__C.TRAIN.SPATIAL_SCALE = 0.0625\n__C.TRAIN.KERNEL_SIZE = 5\n\n# Aspect ratio to use during training\n__C.TRAIN.ASPECTS = (1, 0.75, 0.5, 0.25)\n\n# Images to use per minibatch\n__C.TRAIN.IMS_PER_BATCH = 2\n\n# Minibatch size (number of regions of interest [ROIs])\n__C.TRAIN.BATCH_SIZE = 128\n\n# Fraction of minibatch that is labeled foreground (i.e. class > 0)\n__C.TRAIN.FG_FRACTION = 0.25\n\n# Overlap threshold for a ROI to be considered foreground (if >= FG_THRESH)\n__C.TRAIN.FG_THRESH = (0.5,)\n\n# Overlap threshold for a ROI to be considered background (class = 0 if\n# overlap in [LO, HI))\n__C.TRAIN.BG_THRESH_HI = (0.5,)\n__C.TRAIN.BG_THRESH_LO = (0.1,)\n\n# Use horizontally-flipped images during training?\n__C.TRAIN.USE_FLIPPED = True\n\n# Train bounding-box regressors\n__C.TRAIN.BBOX_REG = True\n\n# Overlap required between a ROI and ground-truth box in order for that ROI to\n# be used as a bounding-box regression training example\n__C.TRAIN.BBOX_THRESH = (0.5,)\n\n# Iterations between snapshots\n__C.TRAIN.SNAPSHOT_ITERS = 10000\n\n# solver.prototxt specifies the snapshot path prefix, this adds an optional\n# infix to yield the path: <prefix>[_<infix>]_iters_XYZ.caffemodel\n__C.TRAIN.SNAPSHOT_PREFIX = \'caffenet_fast_rcnn\'\n__C.TRAIN.SNAPSHOT_INFIX = \'\'\n\n# Use a prefetch thread in roi_data_layer.layer\n# So far I haven\'t found this useful; likely more engineering work is required\n__C.TRAIN.USE_PREFETCH = False\n\n# Train using subclasses\n__C.TRAIN.SUBCLS = True\n\n# Train using viewpoint\n__C.TRAIN.VIEWPOINT = False\n\n# Threshold of ROIs in training RCNN\n__C.TRAIN.ROI_THRESHOLD = 0.1\n\n__C.TRAIN.DISPLAY = 20\n\n\n# IOU >= thresh: positive example\n__C.TRAIN.RPN_POSITIVE_OVERLAP = 0.7\n# IOU < thresh: negative example\n__C.TRAIN.RPN_NEGATIVE_OVERLAP = 0.3\n# If an anchor statisfied by positive and negative conditions set to negative\n__C.TRAIN.RPN_CLOBBER_POSITIVES = False\n# Max number of foreground examples\n__C.TRAIN.RPN_FG_FRACTION = 0.5\n# Total number of examples\n__C.TRAIN.RPN_BATCHSIZE = 256\n# NMS threshold used on RPN proposals\n__C.TRAIN.RPN_NMS_THRESH = 0.7\n# Number of top scoring boxes to keep before apply NMS to RPN proposals\n__C.TRAIN.RPN_PRE_NMS_TOP_N = 12000\n# Number of top scoring boxes to keep after applying NMS to RPN proposals\n__C.TRAIN.RPN_POST_NMS_TOP_N = 2000\n# Proposal height and width both need to be greater than RPN_MIN_SIZE (at orig image scale)\n__C.TRAIN.RPN_MIN_SIZE = 16\n# Deprecated (outside weights)\n__C.TRAIN.RPN_BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n# Give the positive RPN examples weight of p * 1 / {num positives}\n# and give negatives a weight of (1 - p)\n# Set to -1.0 to use uniform example weighting\n__C.TRAIN.RPN_POSITIVE_WEIGHT = -1.0\n\n__C.TRAIN.RPN_BASE_SIZE = 16\n__C.TRAIN.RPN_ASPECTS = [0.25, 0.5, 0.75, 1, 1.5, 2, 3]  # 7 aspects\n__C.TRAIN.RPN_SCALES = [2, 2.82842712, 4, 5.65685425, 8, 11.3137085, 16, 22.627417, 32, 45.254834] # 2**np.arange(1, 6, 0.5), 10 scales\n\n#\n# Testing options\n#\n\n__C.TEST = edict()\n\n# Scales to compute real features\n#__C.TEST.SCALES_BASE = (0.25, 0.5, 1.0, 2.0, 3.0)\n__C.TEST.SCALES_BASE = (1.0,)\n\n# The number of scales per octave in the image pyramid\n# An octave is the set of scales up to half of the initial scale\n__C.TEST.NUM_PER_OCTAVE = 4\n\n# Aspect ratio to use during testing\n__C.TEST.ASPECTS = (1, 0.75, 0.5, 0.25)\n\n# parameters for ROI generating\n__C.TEST.SPATIAL_SCALE = 0.0625\n__C.TEST.KERNEL_SIZE = 5\n\n# Overlap threshold used for non-maximum suppression (suppress boxes with\n# IoU >= this threshold)\n__C.TEST.NMS = 0.5\n\n# Experimental: treat the (K+1) units in the cls_score layer as linear\n# predictors (trained, eg, with one-vs-rest SVMs).\n__C.TEST.SVM = False\n\n# Test using bounding-box regressors\n__C.TEST.BBOX_REG = True\n\n# Test using subclass\n__C.TEST.SUBCLS = True\n\n# Train using viewpoint\n__C.TEST.VIEWPOINT = False\n\n# Threshold of ROIs in testing\n__C.TEST.ROI_THRESHOLD = 0.1\n__C.TEST.ROI_NUM = 2000\n__C.TEST.DET_THRESHOLD = 0.0001\n\n## NMS threshold used on RPN proposals\n__C.TEST.RPN_NMS_THRESH = 0.7\n## Number of top scoring boxes to keep before apply NMS to RPN proposals\n__C.TEST.RPN_PRE_NMS_TOP_N = 6000\n## Number of top scoring boxes to keep after applying NMS to RPN proposals\n__C.TEST.RPN_POST_NMS_TOP_N = 300\n# Proposal height and width both need to be greater than RPN_MIN_SIZE (at orig image scale)\n__C.TEST.RPN_MIN_SIZE = 16\n\n#\n# MISC\n#\n\n# The mapping from image coordinates to feature map coordinates might cause\n# some boxes that are distinct in image space to become identical in feature\n# coordinates. If DEDUP_BOXES > 0, then DEDUP_BOXES is used as the scale factor\n# for identifying duplicate boxes.\n# 1/16 is correct for {Alex,Caffe}Net, VGG_CNN_M_1024, and VGG16\n__C.DEDUP_BOXES = 1./16.\n\n# Pixel mean values (BGR order) as a (1, 1, 3) array\n# These are the values originally used for training VGG16\n__C.PIXEL_MEANS = np.array([[[102.9801, 115.9465, 122.7717]]])\n\n# For reproducibility\n__C.RNG_SEED = 3\n\n# A small number that\'s used many times\n__C.EPS = 1e-14\n\n# Root directory of project\n__C.ROOT_DIR = osp.abspath(osp.join(osp.dirname(__file__), \'..\', \'..\'))\n\n# Place outputs under an experiments directory\n__C.EXP_DIR = \'default\'\n\n# Use GPU implementation of non-maximum suppression\n__C.USE_GPU_NMS = True\n\n# Default GPU device id\n__C.GPU_ID = 0\n\ndef get_output_dir(imdb, net):\n    """"""Return the directory where experimental artifacts are placed.\n\n    A canonical path is built using the name from an imdb and a network\n    (if not None).\n    """"""\n    path = osp.abspath(osp.join(__C.ROOT_DIR, \'output\', __C.EXP_DIR, imdb.name))\n    if net is None:\n        return path\n    else:\n        return osp.join(path, net)\n\ndef _add_more_info(is_train):\n    # compute all the scales\n    if is_train:\n        scales_base = __C.TRAIN.SCALES_BASE\n        num_per_octave = __C.TRAIN.NUM_PER_OCTAVE\n    else:\n        scales_base = __C.TEST.SCALES_BASE\n        num_per_octave = __C.TEST.NUM_PER_OCTAVE\n\n    num_scale_base = len(scales_base)\n    num = (num_scale_base - 1) * num_per_octave + 1\n    scales = []\n    for i in xrange(num):\n        index_scale_base = i / num_per_octave\n        sbase = scales_base[index_scale_base]\n        j = i % num_per_octave\n        if j == 0:\n            scales.append(sbase)\n        else:\n            sbase_next = scales_base[index_scale_base+1]\n            step = (sbase_next - sbase) / num_per_octave\n            scales.append(sbase + j * step)\n\n    if is_train:\n        __C.TRAIN.SCALES = scales\n    else:\n        __C.TEST.SCALES = scales\n    print scales\n\n\n    # map the scales to scales for RoI pooling of classification\n    if is_train:\n        kernel_size = __C.TRAIN.KERNEL_SIZE / __C.TRAIN.SPATIAL_SCALE\n    else:\n        kernel_size = __C.TEST.KERNEL_SIZE / __C.TEST.SPATIAL_SCALE\n\n    area = kernel_size * kernel_size\n    scales = np.array(scales)\n    areas = np.repeat(area, num) / (scales ** 2)\n    scaled_areas = areas[:, np.newaxis] * (scales[np.newaxis, :] ** 2)\n    diff_areas = np.abs(scaled_areas - 224 * 224)\n    levels = diff_areas.argmin(axis=1)\n\n    if is_train:\n        __C.TRAIN.SCALE_MAPPING = levels\n    else:\n        __C.TEST.SCALE_MAPPING = levels\n\n    # compute width and height of grid box\n    if is_train:\n        area = __C.TRAIN.KERNEL_SIZE * __C.TRAIN.KERNEL_SIZE\n        aspect = __C.TRAIN.ASPECTS  # height / width\n    else:\n        area = __C.TEST.KERNEL_SIZE * __C.TEST.KERNEL_SIZE\n        aspect = __C.TEST.ASPECTS  # height / width\n\n    num_aspect = len(aspect)\n    widths = np.zeros((num_aspect), dtype=np.float32)\n    heights = np.zeros((num_aspect), dtype=np.float32)\n    for i in xrange(num_aspect):\n        widths[i] = math.sqrt(area / aspect[i])\n        heights[i] = widths[i] * aspect[i]\n\n    if is_train:\n        __C.TRAIN.ASPECT_WIDTHS = widths\n        __C.TRAIN.ASPECT_HEIGHTS = heights\n        __C.TRAIN.RPN_SCALES = np.array(__C.TRAIN.RPN_SCALES)\n    else:\n        __C.TEST.ASPECT_WIDTHS = widths\n        __C.TEST.ASPECT_HEIGHTS = heights\n\ndef _merge_a_into_b(a, b):\n    """"""Merge config dictionary a into config dictionary b, clobbering the\n    options in b whenever they are also specified in a.\n    """"""\n    if type(a) is not edict:\n        return\n\n    for k, v in a.iteritems():\n        # a must specify keys that are in b\n        if not b.has_key(k):\n            raise KeyError(\'{} is not a valid config key\'.format(k))\n\n        # the types must match, too\n        if type(b[k]) is not type(v):\n            raise ValueError((\'Type mismatch ({} vs. {}) \'\n                              \'for config key: {}\').format(type(b[k]),\n                                                           type(v), k))\n\n        # recursively merge dicts\n        if type(v) is edict:\n            try:\n                _merge_a_into_b(a[k], b[k])\n            except:\n                print(\'Error under config key: {}\'.format(k))\n                raise\n        else:\n            b[k] = v\n\ndef cfg_from_file(filename):\n    """"""Load a config file and merge it into the default options.""""""\n    import yaml\n    with open(filename, \'r\') as f:\n        yaml_cfg = edict(yaml.load(f))\n\n    _merge_a_into_b(yaml_cfg, __C)\n    _add_more_info(1)\n    _add_more_info(0)\n\ndef cfg_from_list(cfg_list):\n    """"""Set config keys via list (e.g., from command line).""""""\n    from ast import literal_eval\n    assert len(cfg_list) % 2 == 0\n    for k, v in zip(cfg_list[0::2], cfg_list[1::2]):\n        key_list = k.split(\'.\')\n        d = __C\n        for subkey in key_list[:-1]:\n            assert d.has_key(subkey)\n            d = d[subkey]\n        subkey = key_list[-1]\n        assert d.has_key(subkey)\n        try:\n            value = literal_eval(v)\n        except:\n            # handle the case when v is a string literal\n            value = v\n        assert type(value) == type(d[subkey]), \\\n            \'type {} does not match original type {}\'.format(\n            type(value), type(d[subkey]))\n        d[subkey] = value'"
lib/fast_rcnn/nms_wrapper.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\nfrom .config import cfg\nfrom ..nms.gpu_nms import gpu_nms\nfrom ..nms.cpu_nms import cpu_nms\n\ndef nms(dets, thresh, force_cpu=False):\n    """"""Dispatch to either CPU or GPU NMS implementations.""""""\n\n    if dets.shape[0] == 0:\n        return []\n    if cfg.USE_GPU_NMS and not force_cpu:\n        return gpu_nms(dets, thresh, device_id=cfg.GPU_ID)\n    else:\n        return cpu_nms(dets, thresh)\n\ndef nms_wrapper(scores, boxes, threshold = 0.7, class_sets = None):\n    """"""\n    post-process the results of im_detect\n    :param scores: N * (K * 4) numpy\n    :param boxes: N * K numpy\n    :param class_sets: e.g. CLASSES = (\'__background__\',\'person\',\'bike\',\'motorbike\',\'car\',\'bus\')\n    :return: a list of K-1 dicts, no background, each is {\'class\': classname, \'dets\': None | [[x1,y1,x2,y2,score],...]}\n    """"""\n    num_class = scores.shape[1] if class_sets is None else len(class_sets)\n    assert num_class * 4 == boxes.shape[1],\\\n        \'Detection scores and boxes dont match\'\n    class_sets = [\'class_\' + str(i) for i in range(0, num_class)] if class_sets is None else class_sets\n\n    res = []\n    for ind, cls in enumerate(class_sets[1:]):\n        ind += 1 # skip background\n        cls_boxes =  boxes[:, 4*ind : 4*(ind+1)]\n        cls_scores = scores[:, ind]\n        dets = np.hstack((cls_boxes, cls_scores[:, np.newaxis])).astype(np.float32)\n        keep = nms(dets, thresh=0.3)\n        dets = dets[keep, :]\n        dets = dets[np.where(dets[:, 4] > threshold)]\n        r = {}\n        if dets.shape[0] > 0:\n            r[\'class\'], r[\'dets\'] = cls, dets\n        else:\n            r[\'class\'], r[\'dets\'] = cls, None\n        res.append(r)\n    return res'"
lib/fast_rcnn/test.py,0,"b'import argparse\nimport numpy as np\nimport cv2\nimport cPickle\nimport heapq\nimport os\nimport math\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom .config import cfg, get_output_dir\n\nfrom ..utils.timer import Timer\nfrom ..utils.cython_nms import nms, nms_new\nfrom ..utils.blob import im_list_to_blob\nfrom ..utils.boxes_grid import get_boxes_grid\n\nfrom ..fast_rcnn.bbox_transform import clip_boxes, bbox_transform_inv\n\n\ndef _get_image_blob(im):\n    """"""Converts an image into a network input.\n    Arguments:\n        im (ndarray): a color image in BGR order\n    Returns:\n        blob (ndarray): a data blob holding an image pyramid\n        im_scale_factors (list): list of image scales (relative to im) used\n            in the image pyramid\n    """"""\n    im_orig = im.astype(np.float32, copy=True)\n    im_orig -= cfg.PIXEL_MEANS\n\n    im_shape = im_orig.shape\n    im_size_min = np.min(im_shape[0:2])\n    im_size_max = np.max(im_shape[0:2])\n\n    processed_ims = []\n    im_scale_factors = []\n\n    for target_size in cfg.TEST.SCALES:\n        im_scale = float(target_size) / float(im_size_min)\n        # Prevent the biggest axis from being more than MAX_SIZE\n        if np.round(im_scale * im_size_max) > cfg.TEST.MAX_SIZE:\n            im_scale = float(cfg.TEST.MAX_SIZE) / float(im_size_max)\n        im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale,\n                        interpolation=cv2.INTER_LINEAR)\n        im_scale_factors.append(im_scale)\n        processed_ims.append(im)\n\n    # Create a blob to hold the input images\n    blob = im_list_to_blob(processed_ims)\n\n    return blob, np.array(im_scale_factors)\n\ndef _get_rois_blob(im_rois, im_scale_factors):\n    """"""Converts RoIs into network inputs.\n    Arguments:\n        im_rois (ndarray): R x 4 matrix of RoIs in original image coordinates\n        im_scale_factors (list): scale factors as returned by _get_image_blob\n    Returns:\n        blob (ndarray): R x 5 matrix of RoIs in the image pyramid\n    """"""\n    rois, levels = _project_im_rois(im_rois, im_scale_factors)\n    rois_blob = np.hstack((levels, rois))\n    return rois_blob.astype(np.float32, copy=False)\n\ndef _project_im_rois(im_rois, scales):\n    """"""Project image RoIs into the image pyramid built by _get_image_blob.\n    Arguments:\n        im_rois (ndarray): R x 4 matrix of RoIs in original image coordinates\n        scales (list): scale factors as returned by _get_image_blob\n    Returns:\n        rois (ndarray): R x 4 matrix of projected RoI coordinates\n        levels (list): image pyramid levels used by each projected RoI\n    """"""\n    im_rois = im_rois.astype(np.float, copy=False)\n    scales = np.array(scales)\n\n    if len(scales) > 1:\n        widths = im_rois[:, 2] - im_rois[:, 0] + 1\n        heights = im_rois[:, 3] - im_rois[:, 1] + 1\n\n        areas = widths * heights\n        scaled_areas = areas[:, np.newaxis] * (scales[np.newaxis, :] ** 2)\n        diff_areas = np.abs(scaled_areas - 224 * 224)\n        levels = diff_areas.argmin(axis=1)[:, np.newaxis]\n    else:\n        levels = np.zeros((im_rois.shape[0], 1), dtype=np.int)\n\n    rois = im_rois * scales[levels]\n\n    return rois, levels\n\ndef _get_blobs(im, rois):\n    """"""Convert an image and RoIs within that image into network inputs.""""""\n    if cfg.TEST.HAS_RPN:\n        blobs = {\'data\' : None, \'rois\' : None}\n        blobs[\'data\'], im_scale_factors = _get_image_blob(im)\n    else:\n        blobs = {\'data\' : None, \'rois\' : None}\n        blobs[\'data\'], im_scale_factors = _get_image_blob(im)\n        if cfg.IS_MULTISCALE:\n            if cfg.IS_EXTRAPOLATING:\n                blobs[\'rois\'] = _get_rois_blob(rois, cfg.TEST.SCALES)\n            else:\n                blobs[\'rois\'] = _get_rois_blob(rois, cfg.TEST.SCALES_BASE)\n        else:\n            blobs[\'rois\'] = _get_rois_blob(rois, cfg.TEST.SCALES_BASE)\n\n    return blobs, im_scale_factors\n\ndef _clip_boxes(boxes, im_shape):\n    """"""Clip boxes to image boundaries.""""""\n    # x1 >= 0\n    boxes[:, 0::4] = np.maximum(boxes[:, 0::4], 0)\n    # y1 >= 0\n    boxes[:, 1::4] = np.maximum(boxes[:, 1::4], 0)\n    # x2 < im_shape[1]\n    boxes[:, 2::4] = np.minimum(boxes[:, 2::4], im_shape[1] - 1)\n    # y2 < im_shape[0]\n    boxes[:, 3::4] = np.minimum(boxes[:, 3::4], im_shape[0] - 1)\n    return boxes\n\n\ndef _rescale_boxes(boxes, inds, scales):\n    """"""Rescale boxes according to image rescaling.""""""\n\n    for i in xrange(boxes.shape[0]):\n        boxes[i,:] = boxes[i,:] / scales[int(inds[i])]\n\n    return boxes\n\n\ndef im_detect(sess, net, im, boxes=None):\n    """"""Detect object classes in an image given object proposals.\n    Arguments:\n        net (caffe.Net): Fast R-CNN network to use\n        im (ndarray): color image to test (in BGR order)\n        boxes (ndarray): R x 4 array of object proposals\n    Returns:\n        scores (ndarray): R x K array of object class scores (K includes\n            background as object category 0)\n        boxes (ndarray): R x (4*K) array of predicted bounding boxes\n    """"""\n\n    blobs, im_scales = _get_blobs(im, boxes)\n\n    # When mapping from image ROIs to feature map ROIs, there\'s some aliasing\n    # (some distinct image ROIs get mapped to the same feature ROI).\n    # Here, we identify duplicate feature ROIs, so we only compute features\n    # on the unique subset.\n    if cfg.DEDUP_BOXES > 0 and not cfg.TEST.HAS_RPN:\n        v = np.array([1, 1e3, 1e6, 1e9, 1e12])\n        hashes = np.round(blobs[\'rois\'] * cfg.DEDUP_BOXES).dot(v)\n        _, index, inv_index = np.unique(hashes, return_index=True,\n                                        return_inverse=True)\n        blobs[\'rois\'] = blobs[\'rois\'][index, :]\n        boxes = boxes[index, :]\n\n    if cfg.TEST.HAS_RPN:\n        im_blob = blobs[\'data\']\n        blobs[\'im_info\'] = np.array(\n            [[im_blob.shape[1], im_blob.shape[2], im_scales[0]]],\n            dtype=np.float32)\n    # forward pass\n    if cfg.TEST.HAS_RPN:\n        feed_dict={net.data: blobs[\'data\'], net.im_info: blobs[\'im_info\'], net.keep_prob: 1.0}\n    else:\n        feed_dict={net.data: blobs[\'data\'], net.rois: blobs[\'rois\'], net.keep_prob: 1.0}\n\n    cls_score, cls_prob, bbox_pred, rois = \\\n        sess.run([net.get_output(\'cls_score\'), net.get_output(\'cls_prob\'), net.get_output(\'bbox_pred\'),net.get_output(\'rois\')],\\\n                 feed_dict=feed_dict)\n    \n    if cfg.TEST.HAS_RPN:\n        assert len(im_scales) == 1, ""Only single-image batch implemented""\n        boxes = rois[:, 1:5] / im_scales[0]\n\n\n    if cfg.TEST.SVM:\n        # use the raw scores before softmax under the assumption they\n        # were trained as linear SVMs\n        scores = cls_score\n    else:\n        # use softmax estimated probabilities\n        scores = cls_prob\n\n    if cfg.TEST.BBOX_REG:\n        # Apply bounding-box regression deltas\n        box_deltas = bbox_pred\n        pred_boxes = bbox_transform_inv(boxes, box_deltas)\n        pred_boxes = _clip_boxes(pred_boxes, im.shape)\n    else:\n        # Simply repeat the boxes, once for each class\n        pred_boxes = np.tile(boxes, (1, scores.shape[1]))\n\n    if cfg.DEDUP_BOXES > 0 and not cfg.TEST.HAS_RPN:\n        # Map scores and predictions back to the original set of boxes\n        scores = scores[inv_index, :]\n        pred_boxes = pred_boxes[inv_index, :]\n\n    return scores, pred_boxes\n\ndef vis_detections(im, class_name, dets, thresh=0.8):\n    """"""Visual debugging of detections.""""""\n    import matplotlib.pyplot as plt \n    #im = im[:, :, (2, 1, 0)]\n    for i in xrange(np.minimum(10, dets.shape[0])):\n        bbox = dets[i, :4] \n        score = dets[i, -1] \n        if score > thresh:\n            #plt.cla()\n            #plt.imshow(im)\n            plt.gca().add_patch(\n                plt.Rectangle((bbox[0], bbox[1]),\n                              bbox[2] - bbox[0],\n                              bbox[3] - bbox[1], fill=False,\n                              edgecolor=\'g\', linewidth=3)\n                )\n            plt.gca().text(bbox[0], bbox[1] - 2,\n                 \'{:s} {:.3f}\'.format(class_name, score),\n                 bbox=dict(facecolor=\'blue\', alpha=0.5),\n                 fontsize=14, color=\'white\')\n\n            plt.title(\'{}  {:.3f}\'.format(class_name, score))\n    #plt.show()\n\ndef apply_nms(all_boxes, thresh):\n    """"""Apply non-maximum suppression to all predicted boxes output by the\n    test_net method.\n    """"""\n    num_classes = len(all_boxes)\n    num_images = len(all_boxes[0])\n    nms_boxes = [[[] for _ in xrange(num_images)]\n                 for _ in xrange(num_classes)]\n    for cls_ind in xrange(num_classes):\n        for im_ind in xrange(num_images):\n            dets = all_boxes[cls_ind][im_ind]\n            if dets == []:\n                continue\n\n            x1 = dets[:, 0]\n            y1 = dets[:, 1]\n            x2 = dets[:, 2]\n            y2 = dets[:, 3]\n            scores = dets[:, 4]\n            inds = np.where((x2 > x1) & (y2 > y1) & (scores > cfg.TEST.DET_THRESHOLD))[0]\n            dets = dets[inds,:]\n            if dets == []:\n                continue\n\n            keep = nms(dets, thresh)\n            if len(keep) == 0:\n                continue\n            nms_boxes[cls_ind][im_ind] = dets[keep, :].copy()\n    return nms_boxes\n\n\ndef test_net(sess, net, imdb, weights_filename , max_per_image=300, thresh=0.05, vis=False):\n    """"""Test a Fast R-CNN network on an image database.""""""\n    num_images = len(imdb.image_index)\n    # all detections are collected into:\n    #    all_boxes[cls][image] = N x 5 array of detections in\n    #    (x1, y1, x2, y2, score)\n    all_boxes = [[[] for _ in xrange(num_images)]\n                 for _ in xrange(imdb.num_classes)]\n\n    output_dir = get_output_dir(imdb, weights_filename)\n    # timers\n    _t = {\'im_detect\' : Timer(), \'misc\' : Timer()}\n\n    if not cfg.TEST.HAS_RPN:\n        roidb = imdb.roidb\n\n    det_file = os.path.join(output_dir, \'detections.pkl\')\n    # if os.path.exists(det_file):\n    #     with open(det_file, \'rb\') as f:\n    #         all_boxes = cPickle.load(f)\n\n    for i in xrange(num_images):\n        # filter out any ground truth boxes\n        if cfg.TEST.HAS_RPN:\n            box_proposals = None\n        else:\n            # The roidb may contain ground-truth rois (for example, if the roidb\n            # comes from the training or val split). We only want to evaluate\n            # detection on the *non*-ground-truth rois. We select those the rois\n            # that have the gt_classes field set to 0, which means there\'s no\n            # ground truth.\n            box_proposals = roidb[i][\'boxes\'][roidb[i][\'gt_classes\'] == 0]\n\n        im = cv2.imread(imdb.image_path_at(i))\n        _t[\'im_detect\'].tic()\n        scores, boxes = im_detect(sess, net, im, box_proposals)\n        detect_time = _t[\'im_detect\'].toc(average=False)\n\n        _t[\'misc\'].tic()\n        if vis:\n            image = im[:, :, (2, 1, 0)] \n            plt.cla()\n            plt.imshow(image)\n\n        # skip j = 0, because it\'s the background class\n        for j in xrange(1, imdb.num_classes):\n            inds = np.where(scores[:, j] > thresh)[0]\n            cls_scores = scores[inds, j]\n            cls_boxes = boxes[inds, j*4:(j+1)*4]\n            cls_dets = np.hstack((cls_boxes, cls_scores[:, np.newaxis])) \\\n                .astype(np.float32, copy=False)\n            keep = nms(cls_dets, cfg.TEST.NMS)\n            cls_dets = cls_dets[keep, :]\n            if vis:\n                vis_detections(image, imdb.classes[j], cls_dets)\n            all_boxes[j][i] = cls_dets\n        if vis:\n           plt.show()\n        # Limit to max_per_image detections *over all classes*\n        if max_per_image > 0:\n            image_scores = np.hstack([all_boxes[j][i][:, -1]\n                                      for j in xrange(1, imdb.num_classes)])\n            if len(image_scores) > max_per_image:\n                image_thresh = np.sort(image_scores)[-max_per_image]\n                for j in xrange(1, imdb.num_classes):\n                    keep = np.where(all_boxes[j][i][:, -1] >= image_thresh)[0]\n                    all_boxes[j][i] = all_boxes[j][i][keep, :]\n        nms_time = _t[\'misc\'].toc(average=False)\n\n        print \'im_detect: {:d}/{:d} {:.3f}s {:.3f}s\' \\\n              .format(i + 1, num_images, detect_time, nms_time)\n\n\n    with open(det_file, \'wb\') as f:\n        cPickle.dump(all_boxes, f, cPickle.HIGHEST_PROTOCOL)\n\n    print \'Evaluating detections\'\n    imdb.evaluate_detections(all_boxes, output_dir)\n\n'"
lib/fast_rcnn/train.py,33,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Train a Fast R-CNN network.""""""\nimport numpy as np\nimport os\nimport tensorflow as tf\nfrom tensorflow.python.client import timeline\nimport cv2\n\nfrom .nms_wrapper import nms_wrapper\nfrom ..roi_data_layer.layer import RoIDataLayer\nfrom ..utils.timer import Timer\nfrom ..gt_data_layer import roidb as gdl_roidb\nfrom ..roi_data_layer import roidb as rdl_roidb\n\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\nfrom ..fast_rcnn.bbox_transform import clip_boxes, bbox_transform_inv\n# <<<< obsolete\n\n_DEBUG = False\n\nclass SolverWrapper(object):\n    """"""A simple wrapper around Caffe\'s solver.\n    This wrapper gives us control over he snapshotting process, which we\n    use to unnormalize the learned bounding-box regression weights.\n    """"""\n\n    def __init__(self, sess, network, imdb, roidb, output_dir, logdir, pretrained_model=None):\n        """"""Initialize the SolverWrapper.""""""\n        self.net = network\n        self.imdb = imdb\n        self.roidb = roidb\n        self.output_dir = output_dir\n        self.pretrained_model = pretrained_model\n\n        print \'Computing bounding-box regression targets...\'\n        if cfg.TRAIN.BBOX_REG:\n            self.bbox_means, self.bbox_stds = rdl_roidb.add_bbox_regression_targets(roidb)\n        print \'done\'\n\n        # For checkpoint\n        self.saver = tf.train.Saver(max_to_keep=100)\n        self.writer = tf.summary.FileWriter(logdir=logdir,\n                                             graph=tf.get_default_graph(),\n                                             flush_secs=5)\n\n    def snapshot(self, sess, iter):\n        """"""Take a snapshot of the network after unnormalizing the learned\n        bounding-box regression weights. This enables easy use at test-time.\n        """"""\n        net = self.net\n\n        if cfg.TRAIN.BBOX_REG and net.layers.has_key(\'bbox_pred\') and cfg.TRAIN.BBOX_NORMALIZE_TARGETS:\n            # save original values\n            with tf.variable_scope(\'bbox_pred\', reuse=True):\n                weights = tf.get_variable(""weights"")\n                biases = tf.get_variable(""biases"")\n\n            orig_0 = weights.eval()\n            orig_1 = biases.eval()\n\n            # scale and shift with bbox reg unnormalization; then save snapshot\n            weights_shape = weights.get_shape().as_list()\n            sess.run(weights.assign(orig_0 * np.tile(self.bbox_stds, (weights_shape[0],1))))\n            sess.run(biases.assign(orig_1 * self.bbox_stds + self.bbox_means))\n\n        if not os.path.exists(self.output_dir):\n            os.makedirs(self.output_dir)\n\n        infix = (\'_\' + cfg.TRAIN.SNAPSHOT_INFIX\n                 if cfg.TRAIN.SNAPSHOT_INFIX != \'\' else \'\')\n        filename = (cfg.TRAIN.SNAPSHOT_PREFIX + infix +\n                    \'_iter_{:d}\'.format(iter+1) + \'.ckpt\')\n        filename = os.path.join(self.output_dir, filename)\n\n        self.saver.save(sess, filename)\n        print \'Wrote snapshot to: {:s}\'.format(filename)\n\n        if cfg.TRAIN.BBOX_REG and net.layers.has_key(\'bbox_pred\'):\n            # restore net to original state\n            sess.run(weights.assign(orig_0))\n            sess.run(biases.assign(orig_1))\n\n    def build_image_summary(self):\n        """"""\n        A simple graph for write image summary\n        :return:\n        """"""\n        log_image_data = tf.placeholder(tf.uint8, [None, None, 3])\n        log_image_name = tf.placeholder(tf.string)\n        # import tensorflow.python.ops.gen_logging_ops as logging_ops\n        from tensorflow.python.ops import gen_logging_ops\n        from tensorflow.python.framework import ops as _ops\n        log_image = gen_logging_ops._image_summary(log_image_name, tf.expand_dims(log_image_data, 0), max_images=1)\n        _ops.add_to_collection(_ops.GraphKeys.SUMMARIES, log_image)\n        # log_image = tf.summary.image(log_image_name, tf.expand_dims(log_image_data, 0), max_outputs=1)\n        return log_image, log_image_data, log_image_name\n\n\n    def train_model(self, sess, max_iters, restore=False):\n        """"""Network training loop.""""""\n\n        data_layer = get_data_layer(self.roidb, self.imdb.num_classes)\n\n        loss, cross_entropy, loss_box, rpn_cross_entropy, rpn_loss_box = \\\n            self.net.build_loss(ohem=cfg.TRAIN.OHEM)\n\n        # scalar summary\n        tf.summary.scalar(\'rpn_rgs_loss\', rpn_loss_box)\n        tf.summary.scalar(\'rpn_cls_loss\', rpn_cross_entropy)\n        tf.summary.scalar(\'cls_loss\', cross_entropy)\n        tf.summary.scalar(\'rgs_loss\', loss_box)\n        tf.summary.scalar(\'loss\', loss)\n        summary_op = tf.summary.merge_all()\n\n        # image writer\n        # NOTE: this image is independent to summary_op\n        log_image, log_image_data, log_image_name =\\\n            self.build_image_summary()\n\n        # optimizer\n        if cfg.TRAIN.SOLVER == \'Adam\':\n            opt = tf.train.AdamOptimizer(cfg.TRAIN.LEARNING_RATE)\n        elif cfg.TRAIN.SOLVER == \'RMS\':\n            opt = tf.train.RMSPropOptimizer(cfg.TRAIN.LEARNING_RATE)\n        else:\n            lr = tf.Variable(cfg.TRAIN.LEARNING_RATE, trainable=False)\n            # lr = tf.Variable(0.0, trainable=False)\n            momentum = cfg.TRAIN.MOMENTUM\n            opt = tf.train.MomentumOptimizer(lr, momentum)\n\n        global_step = tf.Variable(0, trainable=False)\n        with_clip = True\n        if with_clip:\n            tvars = tf.trainable_variables()\n            grads, norm = tf.clip_by_global_norm(tf.gradients(loss, tvars), 10.0)\n            train_op = opt.apply_gradients(zip(grads, tvars), global_step=global_step)\n        else:\n            train_op = opt.minimize(loss, global_step=global_step)\n\n        # intialize variables\n        sess.run(tf.global_variables_initializer())\n        restore_iter = 0\n\n        # load vgg16\n        if self.pretrained_model is not None and not restore:\n            try:\n                print (\'Loading pretrained model \'\n                   \'weights from {:s}\').format(self.pretrained_model)\n                self.net.load(self.pretrained_model, sess, True)\n            except:\n                raise \'Check your pretrained model {:s}\'.format(self.pretrained_model)\n\n        # resuming a trainer\n        if restore:\n            try:\n                ckpt = tf.train.get_checkpoint_state(self.output_dir)\n                print \'Restoring from {}...\'.format(ckpt.model_checkpoint_path),\n                self.saver.restore(sess, ckpt.model_checkpoint_path)\n                stem = os.path.splitext(os.path.basename(ckpt.model_checkpoint_path))[0]\n                restore_iter = int(stem.split(\'_\')[-1])\n                sess.run(global_step.assign(restore_iter))\n                print \'done\'\n            except:\n                raise \'Check your pretrained {:s}\'.format(ckpt.model_checkpoint_path)\n\n        last_snapshot_iter = -1\n        timer = Timer()\n        # for iter in range(max_iters):\n        for iter in range(restore_iter, max_iters):\n            timer.tic()\n\n            # learning rate\n            if iter != 0 and iter % cfg.TRAIN.STEPSIZE == 0:\n                sess.run(tf.assign(lr, lr.eval() * cfg.TRAIN.GAMMA))\n                # sess.run(tf.assign(lr, 0.0))\n\n            # get one batch\n            blobs = data_layer.forward()\n\n            if (iter + 1) % (cfg.TRAIN.DISPLAY) == 0:\n                print \'image: %s\' %(blobs[\'im_name\']),\n\n            feed_dict={\n                self.net.data: blobs[\'data\'],\n                self.net.im_info: blobs[\'im_info\'],\n                self.net.keep_prob: 0.5,\n                self.net.gt_boxes: blobs[\'gt_boxes\'],\n                self.net.gt_ishard: blobs[\'gt_ishard\'],\n                self.net.dontcare_areas: blobs[\'dontcare_areas\']\n            }\n\n            res_fetches = [self.net.get_output(\'cls_prob\'),  # FRCNN class prob\n                           self.net.get_output(\'bbox_pred\'), # FRCNN rgs output\n                           self.net.get_output(\'rois\')]  # RPN rgs output\n\n            fetch_list = [rpn_cross_entropy,\n                          rpn_loss_box,\n                          cross_entropy,\n                          loss_box,\n                          summary_op,\n                          train_op] + res_fetches\n\n            if _DEBUG:\n\n                # add profiling\n                # link libcupti.so in LD_LIBRARY_PATH\n                #\n                # run_metadata = tf.RunMetadata()\n                # rpn_loss_cls_value, rpn_loss_box_value,loss_cls_value, loss_box_value,\\\n                #     summary_str, _, \\\n                #     cls_prob, bbox_pred, rois, \\\n                #      =  sess.run(fetches=fetch_list,\n                #                  feed_dict=feed_dict,\n                #                  options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),\n                #                  run_metadata=run_metadata\n                #                  )\n                #\n                # # write profiling\n                # trace = timeline.Timeline(step_stats=run_metadata.step_stats)\n                # with open(\'timeline.ctf.json\', \'w\') as trace_file:\n                #     trace_file.write(trace.generate_chrome_trace_format())\n\n                fetch_list = [rpn_cross_entropy,\n                              rpn_loss_box,\n                              cross_entropy,\n                              loss_box,\n                              summary_op] + res_fetches\n\n                fetch_list += [self.net.get_output(\'rpn_cls_score_reshape\'), self.net.get_output(\'rpn_cls_prob_reshape\')]\n\n                fetch_list += []\n                rpn_loss_cls_value, rpn_loss_box_value, loss_cls_value, loss_box_value, \\\n                summary_str, \\\n                cls_prob, bbox_pred, rois, \\\n                rpn_cls_score_reshape_np, rpn_cls_prob_reshape_np\\\n                        =  sess.run(fetches=fetch_list, feed_dict=feed_dict)\n            else:\n                fetch_list = [rpn_cross_entropy,\n                              rpn_loss_box,\n                              cross_entropy,\n                              loss_box,\n                              summary_op,\n                              train_op] + res_fetches\n\n                fetch_list += []\n                rpn_loss_cls_value, rpn_loss_box_value, loss_cls_value, loss_box_value, \\\n                summary_str, _, \\\n                cls_prob, bbox_pred, rois =  sess.run(fetches=fetch_list, feed_dict=feed_dict)\n\n            self.writer.add_summary(summary=summary_str, global_step=global_step.eval())\n\n            _diff_time = timer.toc(average=False)\n\n            # image summary\n            if (iter) % cfg.TRAIN.LOG_IMAGE_ITERS == 0:\n                # plus mean\n                ori_im = np.squeeze(blobs[\'data\']) + cfg.PIXEL_MEANS\n                ori_im = ori_im.astype(dtype=np.uint8, copy=False)\n                ori_im = _draw_gt_to_image(ori_im, blobs[\'gt_boxes\'], blobs[\'gt_ishard\'])\n                ori_im = _draw_dontcare_to_image(ori_im, blobs[\'dontcare_areas\'])\n                # draw rects\n                # print \'rois:\', rois.shape[0]\n                if cfg.TRAIN.BBOX_REG and cfg.TRAIN.BBOX_NORMALIZE_TARGETS:\n                    bbox_pred = bbox_pred * np.tile(self.bbox_stds, (bbox_pred.shape[0], 1)) + \\\n                                np.tile(self.bbox_means, (bbox_pred.shape[0], 1))\n                boxes, scores = _process_boxes_scores(cls_prob, bbox_pred, rois, blobs[\'im_info\'][0][2], ori_im.shape)\n                res = nms_wrapper(scores, boxes, threshold=0.7)\n                image = cv2.cvtColor(_draw_boxes_to_image(ori_im, res), cv2.COLOR_BGR2RGB)\n                log_image_name_str = (\'%06d_\' % iter ) + blobs[\'im_name\']\n                log_image_summary_op = \\\n                    sess.run(log_image, \\\n                             feed_dict={log_image_name: log_image_name_str,\\\n                                        log_image_data: image})\n                self.writer.add_summary(log_image_summary_op, global_step=global_step.eval())\n\n            if (iter) % (cfg.TRAIN.DISPLAY) == 0:\n                print \'iter: %d / %d, total loss: %.4f, rpn_loss_cls: %.4f, rpn_loss_box: %.4f, loss_cls: %.4f, loss_box: %.4f, lr: %f\'%\\\n                        (iter, max_iters, rpn_loss_cls_value + rpn_loss_box_value + loss_cls_value + loss_box_value ,\\\n                         rpn_loss_cls_value, rpn_loss_box_value,loss_cls_value, loss_box_value, lr.eval())\n                print \'speed: {:.3f}s / iter\'.format(_diff_time)\n\n            if (iter+1) % cfg.TRAIN.SNAPSHOT_ITERS == 0:\n                last_snapshot_iter = iter\n                self.snapshot(sess, iter)\n\n        if last_snapshot_iter != iter:\n            self.snapshot(sess, iter)\n\ndef get_training_roidb(imdb):\n    """"""Returns a roidb (Region of Interest database) for use in training.""""""\n    if cfg.TRAIN.USE_FLIPPED:\n        print \'Appending horizontally-flipped training examples...\'\n        imdb.append_flipped_images()\n        print \'done\'\n\n    print \'Preparing training data...\'\n    if cfg.TRAIN.HAS_RPN:\n        if cfg.IS_MULTISCALE:\n            # TODO: fix multiscale training (single scale is already a good trade-off)\n            print (\'#### warning: multi-scale has not been tested.\')\n            print (\'#### warning: using single scale by setting IS_MULTISCALE: False.\')\n            gdl_roidb.prepare_roidb(imdb)\n        else:\n            rdl_roidb.prepare_roidb(imdb)\n    else:\n        rdl_roidb.prepare_roidb(imdb)\n    print \'done\'\n\n    return imdb.roidb\n\n\ndef get_data_layer(roidb, num_classes):\n    """"""return a data layer.""""""\n    if cfg.TRAIN.HAS_RPN:\n        if cfg.IS_MULTISCALE:\n            # obsolete\n            # layer = GtDataLayer(roidb)\n            raise ""Calling caffe modules...""\n        else:\n            layer = RoIDataLayer(roidb, num_classes)\n    else:\n        layer = RoIDataLayer(roidb, num_classes)\n\n    return layer\n\ndef _process_boxes_scores(cls_prob, bbox_pred, rois, im_scale, im_shape):\n    """"""\n    process the output tensors, to get the boxes and scores\n    """"""\n    assert rois.shape[0] == bbox_pred.shape[0],\\\n        \'rois and bbox_pred must have the same shape\'\n    boxes = rois[:, 1:5]\n    scores = cls_prob\n    if cfg.TEST.BBOX_REG:\n        pred_boxes = bbox_transform_inv(boxes, deltas=bbox_pred)\n        pred_boxes = clip_boxes(pred_boxes, im_shape)\n    else:\n        # Simply repeat the boxes, once for each class\n        # boxes = np.tile(boxes, (1, scores.shape[1]))\n\n        pred_boxes = clip_boxes(boxes, im_shape)\n    return pred_boxes, scores\n\ndef _draw_boxes_to_image(im, res):\n    colors = [(86, 0, 240), (173, 225, 61), (54, 137, 255),\\\n              (151, 0, 255), (243, 223, 48), (0, 117, 255),\\\n              (58, 184, 14), (86, 67, 140), (121, 82, 6),\\\n              (174, 29, 128), (115, 154, 81), (86, 255, 234)]\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    image = np.copy(im)\n    cnt = 0\n    for ind, r in enumerate(res):\n        if r[\'dets\'] is None: continue\n        dets = r[\'dets\']\n        for i in range(0, dets.shape[0]):\n            (x1, y1, x2, y2, score) = dets[i, :]\n            cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), colors[ind % len(colors)], 2)\n            text = \'{:s} {:.2f}\'.format(r[\'class\'], score)\n            cv2.putText(image, text, (x1, y1), font, 0.6, colors[ind % len(colors)], 1)\n            cnt = (cnt + 1)\n    return image\n\ndef _draw_gt_to_image(im, gt_boxes, gt_ishard):\n    image = np.copy(im)\n\n    for i in range(0, gt_boxes.shape[0]):\n        (x1, y1, x2, y2, score) = gt_boxes[i, :]\n        if gt_ishard[i] == 0:\n            cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (255, 255, 255), 2)\n        else:\n            cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n    return image\n\ndef _draw_dontcare_to_image(im, dontcare):\n    image = np.copy(im)\n\n    for i in range(0, dontcare.shape[0]):\n        (x1, y1, x2, y2) = dontcare[i, :]\n        cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 2)\n    return image\n\n\n\ndef train_net(network, imdb, roidb, output_dir, log_dir, pretrained_model=None, max_iters=40000, restore=False):\n    """"""Train a Fast R-CNN network.""""""\n\n    config = tf.ConfigProto(allow_soft_placement=True)\n    config.gpu_options.allocator_type = \'BFC\'\n    config.gpu_options.per_process_gpu_memory_fraction = 0.40\n    with tf.Session(config=config) as sess:\n        sw = SolverWrapper(sess, network, imdb, roidb, output_dir, logdir= log_dir, pretrained_model=pretrained_model)\n        print \'Solving...\'\n        sw.train_model(sess, max_iters, restore=restore)\n        print \'done solving\'\n'"
lib/gt_data_layer/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\nimport roidb\n# from layer import GtDataLayer'
lib/gt_data_layer/layer.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""The data layer used during training to train a Fast R-CNN network.\n\nGtDataLayer implements a Caffe Python layer.\n""""""\n# TODO: make caffe irrelevant, or remove caffe backend from this projcet\nimport caffe\n\nimport numpy as np\nimport yaml\nfrom multiprocessing import Process, Queue\n\nfrom .minibatch import get_minibatch\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\n\nclass GtDataLayer(caffe.Layer):\n    """"""Fast R-CNN data layer used for training.""""""\n\n    def _shuffle_roidb_inds(self):\n        """"""Randomly permute the training roidb.""""""\n        self._perm = np.random.permutation(np.arange(len(self._roidb)))\n        self._cur = 0\n\n    def _get_next_minibatch_inds(self):\n        """"""Return the roidb indices for the next minibatch.""""""\n        if self._cur + cfg.TRAIN.IMS_PER_BATCH >= len(self._roidb):\n            self._shuffle_roidb_inds()\n\n        db_inds = self._perm[self._cur:self._cur + cfg.TRAIN.IMS_PER_BATCH]\n        self._cur += cfg.TRAIN.IMS_PER_BATCH\n\n        """"""\n        # sample images with gt objects\n        db_inds = np.zeros((cfg.TRAIN.IMS_PER_BATCH), dtype=np.int32)\n        i = 0\n        while (i < cfg.TRAIN.IMS_PER_BATCH):\n            ind = self._perm[self._cur]\n            num_objs = self._roidb[ind][\'boxes\'].shape[0]\n            if num_objs != 0:\n                db_inds[i] = ind\n                i += 1\n\n            self._cur += 1\n            if self._cur >= len(self._roidb):\n                self._shuffle_roidb_inds()\n        """"""\n\n        return db_inds\n\n    def _get_next_minibatch(self):\n        """"""Return the blobs to be used for the next minibatch.""""""\n        db_inds = self._get_next_minibatch_inds()\n        minibatch_db = [self._roidb[i] for i in db_inds]\n        return get_minibatch(minibatch_db, self._num_classes)\n\n    # this function is called in training the net\n    def set_roidb(self, roidb):\n        """"""Set the roidb to be used by this layer during training.""""""\n        self._roidb = roidb\n        self._shuffle_roidb_inds()\n\n    def setup(self, bottom, top):\n        """"""Setup the GtDataLayer.""""""\n\n        # parse the layer parameter string, which must be valid YAML\n        layer_params = yaml.load(self.param_str_)\n\n        self._num_classes = layer_params[\'num_classes\']\n\n        self._name_to_top_map = {\n            \'data\': 0,\n            \'info_boxes\': 1,\n            \'parameters\': 2}\n\n        # data blob: holds a batch of N images, each with 3 channels\n        # The height and width (100 x 100) are dummy values\n        num_scale_base = len(cfg.TRAIN.SCALES_BASE)\n        top[0].reshape(num_scale_base, 3, 100, 100)\n\n        # info boxes blob\n        top[1].reshape(1, 18)\n\n        # parameters blob\n        num_scale = len(cfg.TRAIN.SCALES)\n        num_aspect = len(cfg.TRAIN.ASPECTS)\n        top[2].reshape(2 + 2*num_scale + 2*num_aspect)\n            \n    def forward(self, bottom, top):\n        """"""Get blobs and copy them into this layer\'s top blob vector.""""""\n        blobs = self._get_next_minibatch()\n\n        for blob_name, blob in blobs.iteritems():\n            top_ind = self._name_to_top_map[blob_name]\n            # Reshape net\'s input blobs\n            top[top_ind].reshape(*(blob.shape))\n            # Copy data into net\'s input blobs\n            top[top_ind].data[...] = blob.astype(np.float32, copy=False)\n\n    def backward(self, top, propagate_down, bottom):\n        """"""This layer does not propagate gradients.""""""\n        pass\n\n    def reshape(self, bottom, top):\n        """"""Reshaping happens during the call to forward.""""""\n        pass\n'"
lib/gt_data_layer/minibatch.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Compute minibatch blobs for training a Fast R-CNN network.""""""\n\nimport numpy as np\nimport numpy.random as npr\nimport cv2\n\nfrom ..utils.blob import prep_im_for_blob, im_list_to_blob\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\n\ndef get_minibatch(roidb, num_classes):\n    """"""Given a roidb, construct a minibatch sampled from it.""""""\n    num_images = len(roidb)\n    assert(cfg.TRAIN.BATCH_SIZE % num_images == 0), \\\n        \'num_images ({}) must divide BATCH_SIZE ({})\'. \\\n        format(num_images, cfg.TRAIN.BATCH_SIZE)\n\n    # Get the input image blob, formatted for caffe\n    im_blob = _get_image_blob(roidb)\n\n    # build the box information blob\n    info_boxes_blob = np.zeros((0, 18), dtype=np.float32)\n    num_scale = len(cfg.TRAIN.SCALES)\n    for i in xrange(num_images):\n        info_boxes = roidb[i][\'info_boxes\']\n\n        # change the batch index\n        info_boxes[:,2] += i * num_scale\n        info_boxes[:,7] += i * num_scale\n\n        info_boxes_blob = np.vstack((info_boxes_blob, info_boxes))\n\n    # build the parameter blob\n    num_aspect = len(cfg.TRAIN.ASPECTS)\n    num = 2 + 2 * num_scale + 2 * num_aspect\n    parameters_blob = np.zeros((num), dtype=np.float32)\n    parameters_blob[0] = num_scale\n    parameters_blob[1] = num_aspect\n    parameters_blob[2:2+num_scale] = cfg.TRAIN.SCALES\n    parameters_blob[2+num_scale:2+2*num_scale] = cfg.TRAIN.SCALE_MAPPING\n    parameters_blob[2+2*num_scale:2+2*num_scale+num_aspect] = cfg.TRAIN.ASPECT_HEIGHTS\n    parameters_blob[2+2*num_scale+num_aspect:2+2*num_scale+2*num_aspect] = cfg.TRAIN.ASPECT_WIDTHS\n\n    # For debug visualizations\n    # _vis_minibatch(im_blob, rois_blob, labels_blob, sublabels_blob)\n\n    blobs = {\'data\': im_blob,\n             \'info_boxes\': info_boxes_blob,\n             \'parameters\': parameters_blob}\n\n    return blobs\n\ndef _get_image_blob(roidb):\n    """"""Builds an input blob from the images in the roidb at the different scales.\n    """"""\n    num_images = len(roidb)\n    processed_ims = []\n\n    for i in xrange(num_images):\n        # read image\n        im = cv2.imread(roidb[i][\'image\'])\n        if roidb[i][\'flipped\']:\n            im = im[:, ::-1, :]\n\n        im_orig = im.astype(np.float32, copy=True)\n        im_orig -= cfg.PIXEL_MEANS\n\n        # build image pyramid\n        for im_scale in cfg.TRAIN.SCALES_BASE:\n            im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale,\n                        interpolation=cv2.INTER_LINEAR)\n\n            processed_ims.append(im)\n\n    # Create a blob to hold the input images\n    blob = im_list_to_blob(processed_ims)\n\n    return blob\n\ndef _project_im_rois(im_rois, im_scale_factor):\n    """"""Project image RoIs into the rescaled training image.""""""\n    rois = im_rois * im_scale_factor\n    return rois\n\ndef _get_bbox_regression_labels(bbox_target_data, num_classes):\n    """"""Bounding-box regression targets are stored in a compact form in the\n    roidb.\n\n    This function expands those targets into the 4-of-4*K representation used\n    by the network (i.e. only one class has non-zero targets). The loss weights\n    are similarly expanded.\n\n    Returns:\n        bbox_target_data (ndarray): N x 4K blob of regression targets\n        bbox_loss_weights (ndarray): N x 4K blob of loss weights\n    """"""\n    clss = bbox_target_data[:, 0]\n    bbox_targets = np.zeros((clss.size, 4 * num_classes), dtype=np.float32)\n    bbox_loss_weights = np.zeros(bbox_targets.shape, dtype=np.float32)\n    inds = np.where(clss > 0)[0]\n    for ind in inds:\n        cls = clss[ind]\n        start = 4 * cls\n        end = start + 4\n        bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]\n        bbox_loss_weights[ind, start:end] = [1., 1., 1., 1.]\n    return bbox_targets, bbox_loss_weights\n\n\ndef _vis_minibatch(im_blob, rois_blob, labels_blob, sublabels_blob):\n    """"""Visualize a mini-batch for debugging.""""""\n    import matplotlib.pyplot as plt\n    for i in xrange(rois_blob.shape[0]):\n        rois = rois_blob[i, :]\n        im_ind = rois[0]\n        roi = rois[2:]\n        im = im_blob[im_ind, :, :, :].transpose((1, 2, 0)).copy()\n        im += cfg.PIXEL_MEANS\n        im = im[:, :, (2, 1, 0)]\n        im = im.astype(np.uint8)\n        cls = labels_blob[i]\n        subcls = sublabels_blob[i]\n        plt.imshow(im)\n        print \'class: \', cls, \' subclass: \', subcls\n        plt.gca().add_patch(\n            plt.Rectangle((roi[0], roi[1]), roi[2] - roi[0],\n                          roi[3] - roi[1], fill=False,\n                          edgecolor=\'r\', linewidth=3)\n            )\n        plt.show()\n'"
lib/gt_data_layer/roidb.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Transform a roidb into a trainable roidb by adding a bunch of metadata.""""""\n\nimport numpy as np\nimport scipy.sparse\nimport PIL\nimport math\nimport os\nimport cPickle\nimport pdb\n\nfrom ..utils.cython_bbox import bbox_overlaps\nfrom ..utils.boxes_grid import get_boxes_grid\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\n\n\ndef prepare_roidb(imdb):\n    """"""Enrich the imdb\'s roidb by adding some derived quantities that\n    are useful for training. This function precomputes the maximum\n    overlap, taken over ground-truth boxes, between each ROI and\n    each ground-truth box. The class with maximum overlap is also\n    recorded.\n    """"""\n    cache_file = os.path.join(imdb.cache_path, imdb.name + \'_gt_roidb_prepared.pkl\')\n    if os.path.exists(cache_file):\n        with open(cache_file, \'rb\') as fid:\n            imdb._roidb = cPickle.load(fid)\n        print \'{} gt roidb prepared loaded from {}\'.format(imdb.name, cache_file)\n        return\n\n    roidb = imdb.roidb\n    for i in xrange(len(imdb.image_index)):\n        roidb[i][\'image\'] = imdb.image_path_at(i)\n        boxes = roidb[i][\'boxes\']\n        labels = roidb[i][\'gt_classes\']\n        info_boxes = np.zeros((0, 18), dtype=np.float32)\n\n        if boxes.shape[0] == 0:\n            roidb[i][\'info_boxes\'] = info_boxes\n            continue\n\n        # compute grid boxes\n        s = PIL.Image.open(imdb.image_path_at(i)).size\n        image_height = s[1]\n        image_width = s[0]\n        boxes_grid, cx, cy = get_boxes_grid(image_height, image_width)\n        \n        # for each scale\n        for scale_ind, scale in enumerate(cfg.TRAIN.SCALES):\n            boxes_rescaled = boxes * scale\n\n            # compute overlap\n            overlaps = bbox_overlaps(boxes_grid.astype(np.float), boxes_rescaled.astype(np.float))\n            max_overlaps = overlaps.max(axis = 1)\n            argmax_overlaps = overlaps.argmax(axis = 1)\n            max_classes = labels[argmax_overlaps]\n\n            # select positive boxes\n            fg_inds = []\n            for k in xrange(1, imdb.num_classes):\n                fg_inds.extend(np.where((max_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH))[0])\n\n            if len(fg_inds) > 0:\n                gt_inds = argmax_overlaps[fg_inds]\n                # bounding box regression targets\n                gt_targets = _compute_targets(boxes_grid[fg_inds,:], boxes_rescaled[gt_inds,:])\n                # scale mapping for RoI pooling\n                scale_ind_map = cfg.TRAIN.SCALE_MAPPING[scale_ind]\n                scale_map = cfg.TRAIN.SCALES[scale_ind_map]\n                # contruct the list of positive boxes\n                # (cx, cy, scale_ind, box, scale_ind_map, box_map, gt_label, gt_sublabel, target)\n                info_box = np.zeros((len(fg_inds), 18), dtype=np.float32)\n                info_box[:, 0] = cx[fg_inds]\n                info_box[:, 1] = cy[fg_inds]\n                info_box[:, 2] = scale_ind\n                info_box[:, 3:7] = boxes_grid[fg_inds,:]\n                info_box[:, 7] = scale_ind_map\n                info_box[:, 8:12] = boxes_grid[fg_inds,:] * scale_map / scale\n                info_box[:, 12] = labels[gt_inds]\n                info_box[:, 14:] = gt_targets\n                info_boxes = np.vstack((info_boxes, info_box))\n\n        roidb[i][\'info_boxes\'] = info_boxes\n\n    with open(cache_file, \'wb\') as fid:\n        cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n    print \'wrote gt roidb prepared to {}\'.format(cache_file)\n\ndef add_bbox_regression_targets(roidb):\n    """"""Add information needed to train bounding-box regressors.""""""\n    assert len(roidb) > 0\n    assert \'info_boxes\' in roidb[0], \'Did you call prepare_roidb first?\'\n\n    num_images = len(roidb)\n    # Infer number of classes from the number of columns in gt_overlaps\n    num_classes = roidb[0][\'gt_overlaps\'].shape[1]\n\n    # Compute values needed for means and stds\n    # var(x) = E(x^2) - E(x)^2\n    class_counts = np.zeros((num_classes, 1)) + cfg.EPS\n    sums = np.zeros((num_classes, 4))\n    squared_sums = np.zeros((num_classes, 4))\n    for im_i in xrange(num_images):\n        targets = roidb[im_i][\'info_boxes\']\n        for cls in xrange(1, num_classes):\n            cls_inds = np.where(targets[:, 12] == cls)[0]\n            if cls_inds.size > 0:\n                class_counts[cls] += cls_inds.size\n                sums[cls, :] += targets[cls_inds, 14:].sum(axis=0)\n                squared_sums[cls, :] += (targets[cls_inds, 14:] ** 2).sum(axis=0)\n\n    means = sums / class_counts\n    stds = np.sqrt(squared_sums / class_counts - means ** 2)\n\n    # Normalize targets\n    for im_i in xrange(num_images):\n        targets = roidb[im_i][\'info_boxes\']\n        for cls in xrange(1, num_classes):\n            cls_inds = np.where(targets[:, 12] == cls)[0]\n            roidb[im_i][\'info_boxes\'][cls_inds, 14:] -= means[cls, :]\n            if stds[cls, 0] != 0:\n                roidb[im_i][\'info_boxes\'][cls_inds, 14:] /= stds[cls, :]\n\n    # These values will be needed for making predictions\n    # (the predicts will need to be unnormalized and uncentered)\n    return means.ravel(), stds.ravel()\n\ndef _compute_targets(ex_rois, gt_rois):\n    """"""Compute bounding-box regression targets for an image. The targets are scale invariance""""""\n\n    ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + cfg.EPS\n    ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + cfg.EPS\n    ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n    ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n\n    gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + cfg.EPS\n    gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + cfg.EPS\n    gt_ctr_x = gt_rois[:, 0] + 0.5 * gt_widths\n    gt_ctr_y = gt_rois[:, 1] + 0.5 * gt_heights\n\n    targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n    targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n    targets_dw = np.log(gt_widths / ex_widths)\n    targets_dh = np.log(gt_heights / ex_heights)\n\n    targets = np.zeros((ex_rois.shape[0], 4), dtype=np.float32)\n    targets[:, 0] = targets_dx\n    targets[:, 1] = targets_dy\n    targets[:, 2] = targets_dw\n    targets[:, 3] = targets_dh\n    return targets\n'"
lib/networks/PVAnet_test.py,4,"b""# --------------------------------------------------------\n# TFFRCNN - Resnet50\n# Copyright (c) 2016\n# Licensed under The MIT License [see LICENSE for details]\n# Written by miraclebiu\n# --------------------------------------------------------\nimport tensorflow as tf\nfrom .network import Network\nfrom ..fast_rcnn.config import cfg\n\n\nclass PVAnet_test(Network):\n    def __init__(self, trainable=True):\n        self.inputs = []\n        self.data = tf.placeholder(tf.float32, shape=[None, None, None, 3])\n        self.im_info = tf.placeholder(tf.float32, shape=[None, 3])\n        self.keep_prob = tf.placeholder(tf.float32)\n        self.layers = dict({'data': self.data, 'im_info': self.im_info})\n        self.trainable = trainable\n        self.setup()\n\n    def setup(self):\n        n_classes = cfg.NCLASSES\n        # anchor_scales = [8, 16, 32]\n        anchor_scales = cfg.ANCHOR_SCALES\n        _feat_stride = [16, ]\n\n        (self.feed('data')\n         .pva_negation_block(7, 7, 16, 2, 2, name='conv1_1', negation=True)  # downsample\n         .max_pool(3, 3, 2, 2, padding='VALID', name='pool1')  # downsample\n         .conv(1, 1, 24, 1, 1, name='conv2_1/1/conv', biased=True, relu=False)\n         .pva_negation_block_v2(3, 3, 24, 1, 1, 24, name='conv2_1/2', negation=False)\n         .pva_negation_block_v2(1, 1, 64, 1, 1, 24, name='conv2_1/3', negation=True))\n\n        (self.feed('pool1')\n         .conv(1, 1, 64, 1, 1, name='conv2_1/proj', relu=True))\n\n        (self.feed('conv2_1/3', 'conv2_1/proj')\n         .add(name='conv2_1')\n         .pva_negation_block_v2(1, 1, 24, 1, 1, 64, name='conv2_2/1', negation=False)\n         .pva_negation_block_v2(3, 3, 24, 1, 1, 24, name='conv2_2/2', negation=False)\n         .pva_negation_block_v2(1, 1, 64, 1, 1, 24, name='conv2_2/3', negation=True))\n\n        (self.feed('conv2_2/3', 'conv2_1')\n         .add(name='conv2_2')\n         .pva_negation_block_v2(1, 1, 24, 1, 1, 64, name='conv2_3/1', negation=False)\n         .pva_negation_block_v2(3, 3, 24, 1, 1, 24, name='conv2_3/2', negation=False)\n         .pva_negation_block_v2(1, 1, 64, 1, 1, 24, name='conv2_3/3', negation=True))\n\n        (self.feed('conv2_3/3', 'conv2_2')\n         .add(name='conv2_3')\n         .pva_negation_block_v2(1, 1, 48, 2, 2, 64, name='conv3_1/1', negation=False)  # downsample\n         .pva_negation_block_v2(3, 3, 48, 1, 1, 48, name='conv3_1/2', negation=False)\n         .pva_negation_block_v2(1, 1, 128, 1, 1, 48, name='conv3_1/3', negation=True))\n\n        (self.feed('conv3_1/1/relu')\n         .conv(1, 1, 128, 2, 2, name='conv3_1/proj', relu=True))\n\n        (self.feed('conv3_1/3', 'conv3_1/proj')  # 128\n         .add(name='conv3_1')\n         .pva_negation_block_v2(1, 1, 48, 1, 1, 128, name='conv3_2/1', negation=False)\n         .pva_negation_block_v2(3, 3, 48, 1, 1, 48, name='conv3_2/2', negation=False)\n         .pva_negation_block_v2(1, 1, 128, 1, 1, 48, name='conv3_2/3', negation=True))\n\n        (self.feed('conv3_2/3', 'conv3_1')  # 128\n         .add(name='conv3_2')\n         .pva_negation_block_v2(1, 1, 48, 1, 1, 128, name='conv3_3/1', negation=False)\n         .pva_negation_block_v2(3, 3, 48, 1, 1, 48, name='conv3_3/2', negation=False)\n         .pva_negation_block_v2(1, 1, 128, 1, 1, 48, name='conv3_3/3', negation=True))\n\n        (self.feed('conv3_3/3', 'conv3_2')  # 128\n         .add(name='conv3_3')\n         .pva_negation_block_v2(1, 1, 48, 1, 1, 128, name='conv3_4/1', negation=False)\n         .pva_negation_block_v2(3, 3, 48, 1, 1, 48, name='conv3_4/2', negation=False)\n         .pva_negation_block_v2(1, 1, 128, 1, 1, 48, name='conv3_4/3', negation=True))\n\n        (self.feed('conv3_4/3', 'conv3_3')  # 128\n         .add(name='conv3_4')\n         .max_pool(3, 3, 2, 2, padding='SAME', name='downsample'))  # downsample\n\n        (self.feed('conv3_4')\n         .pva_inception_res_block(name='conv4_4', name_prefix='conv4_', type='a')  # downsample\n         .pva_inception_res_block(name='conv5_4', name_prefix='conv5_', type='b')  # downsample\n         .batch_normalization(name='conv5_4/last_bn', relu=False)\n         .scale(c_in=384, name='conv5_4/last_bn_scale')\n         .relu(name='conv5_4/last_relu'))\n\n        (self.feed('conv5_4/last_relu')\n         .upconv(tf.shape(self.layers['downsample']),\n                 384, 4, 2, name='upsample', biased=False, relu=False, trainable=True))  # upsample\n\n        (self.feed('downsample', 'conv4_4', 'upsample')\n         .concat(axis=3, name='concat'))\n\n        # ========= RPN ============\n        (self.feed('concat')\n         .conv(1, 1, 128, 1, 1, name='convf_rpn', biased=True, relu=True)\n         .conv(3, 3, 384, 1, 1, name='rpn_conv/3x3', biased=True, relu=True)\n         .conv(1, 1, len(anchor_scales) * 3 * 2, 1, 1, padding='VALID', relu=False, name='rpn_cls_score'))\n\n        (self.feed('rpn_conv/3x3')\n         .conv(1, 1, len(anchor_scales) * 3 * 4, 1, 1, padding='VALID', relu=False, name='rpn_bbox_pred'))\n\n        # ========= RoI Proposal ============\n        (self.feed('rpn_cls_score')\n         .spatial_reshape_layer(2, name='rpn_cls_score_reshape')\n         .spatial_softmax(name='rpn_cls_prob'))\n\n        (self.feed('rpn_cls_prob')\n         .spatial_reshape_layer(len(anchor_scales) * 3 * 2, name='rpn_cls_prob_reshape'))\n\n        (self.feed('rpn_cls_prob_reshape', 'rpn_bbox_pred', 'im_info')\n         .proposal_layer(_feat_stride, anchor_scales, 'TEST', name='rois'))\n\n        # ========= RCNN ============\n        (self.feed('concat')\n         .conv(1, 1, 384, 1, 1, name='convf_2', biased=True, relu=True))\n        (self.feed('convf_rpn', 'convf_2')\n         .concat(axis=3, name='convf'))\n\n        (self.feed('convf', 'rois')\n         .roi_pool(7, 7, 1.0 / 16, name='roi_pooling')\n         .fc(4096, name='fc6', relu=False)\n         .bn_scale_combo(c_in = 4096, name='fc6', relu=True)\n         .fc(4096, name='fc7', relu=False)\n         .bn_scale_combo(c_in=4096, name='fc7', relu=True)\n         .fc(n_classes, relu=False, name='cls_score')\n         .softmax(name='cls_prob'))\n\n        (self.feed('fc7')\n         .fc(n_classes * 4, relu=False, name='bbox_pred'))"""
lib/networks/PVAnet_train.py,9,"b'# --------------------------------------------------------\n# TFFRCNN - Resnet50\n# Copyright (c) 2016\n# Licensed under The MIT License [see LICENSE for details]\n# Written by miraclebiu\n# --------------------------------------------------------\n\nimport tensorflow as tf\nfrom .network import Network\nfrom ..fast_rcnn.config import cfg\nimport numpy as np\n\n\nclass PVAnet_train(Network):\n    def __init__(self, trainable=True):\n        self.inputs = []\n        self.data = tf.placeholder(tf.float32, shape=[None, None, None, 3], name=\'data\')\n        self.im_info = tf.placeholder(tf.float32, shape=[None, 3], name=\'im_info\')\n        self.gt_boxes = tf.placeholder(tf.float32, shape=[None, 5], name=\'gt_boxes\')\n        self.gt_ishard = tf.placeholder(tf.int32, shape=[None], name=\'gt_ishard\')\n        self.dontcare_areas = tf.placeholder(tf.float32, shape=[None, 4], name=\'dontcare_areas\')\n        self.keep_prob = tf.placeholder(tf.float32)\n        self.layers = dict({\'data\': self.data, \'im_info\': self.im_info, \'gt_boxes\': self.gt_boxes, \\\n                            \'gt_ishard\': self.gt_ishard, \'dontcare_areas\': self.dontcare_areas})\n        self.trainable = trainable\n        self.setup()\n\n    def setup(self):\n        n_classes = cfg.NCLASSES\n        # anchor_scales = [8, 16, 32]\n        anchor_scales = cfg.ANCHOR_SCALES\n        _feat_stride = [16, ]\n        (self.feed(\'data\')\n         .pva_negation_block(7, 7, 16, 2, 2, name=\'conv1_1\', negation=True)         # downsample\n         .max_pool(3, 3, 2, 2, padding=\'VALID\', name=\'pool1\')                       # downsample\n         .conv(1, 1, 24, 1, 1, name=\'conv2_1/1/conv\', biased=True, relu=False)\n         .pva_negation_block_v2(3, 3, 24, 1, 1, 24, name=\'conv2_1/2\', negation=False)\n         .pva_negation_block_v2(1, 1, 64, 1, 1, 24, name=\'conv2_1/3\', negation=True))\n\n        (self.feed(\'pool1\')\n         .conv(1,1, 64, 1, 1, name=\'conv2_1/proj\', relu=True))\n\n        (self.feed(\'conv2_1/3\', \'conv2_1/proj\')\n         .add(name=\'conv2_1\')\n         .pva_negation_block_v2(1, 1, 24, 1, 1, 64, name=\'conv2_2/1\', negation = False)\n         .pva_negation_block_v2(3, 3, 24, 1, 1, 24, name=\'conv2_2/2\', negation = False)\n         .pva_negation_block_v2(1, 1, 64, 1, 1, 24, name=\'conv2_2/3\', negation = True))\n\n        (self.feed(\'conv2_2/3\', \'conv2_1\')\n         .add(name=\'conv2_2\')\n         .pva_negation_block_v2(1, 1, 24, 1, 1, 64, name=\'conv2_3/1\', negation=False)\n         .pva_negation_block_v2(3, 3, 24, 1, 1, 24, name=\'conv2_3/2\', negation=False)\n         .pva_negation_block_v2(1, 1, 64, 1, 1, 24, name=\'conv2_3/3\', negation=True))\n\n        (self.feed(\'conv2_3/3\', \'conv2_2\')\n         .add(name=\'conv2_3\')\n         .pva_negation_block_v2(1, 1, 48, 2, 2, 64, name=\'conv3_1/1\', negation=False) # downsample\n         .pva_negation_block_v2(3, 3, 48, 1, 1, 48, name=\'conv3_1/2\', negation=False)\n         .pva_negation_block_v2(1, 1, 128, 1, 1, 48, name=\'conv3_1/3\', negation=True))\n\n        (self.feed(\'conv3_1/1/relu\')\n         .conv(1, 1, 128, 2, 2, name=\'conv3_1/proj\', relu=True))\n\n        (self.feed(\'conv3_1/3\', \'conv3_1/proj\')  # 128\n         .add(name=\'conv3_1\')\n         .pva_negation_block_v2(1, 1, 48, 1, 1, 128, name=\'conv3_2/1\', negation=False)\n         .pva_negation_block_v2(3, 3, 48, 1, 1, 48, name=\'conv3_2/2\', negation=False)\n         .pva_negation_block_v2(1, 1, 128, 1, 1, 48, name=\'conv3_2/3\', negation=True))\n\n        (self.feed(\'conv3_2/3\', \'conv3_1\')  # 128\n         .add(name=\'conv3_2\')\n         .pva_negation_block_v2(1, 1, 48, 1, 1, 128, name=\'conv3_3/1\', negation=False)\n         .pva_negation_block_v2(3, 3, 48, 1, 1, 48, name=\'conv3_3/2\', negation=False)\n         .pva_negation_block_v2(1, 1, 128, 1, 1, 48, name=\'conv3_3/3\', negation=True))\n\n        (self.feed(\'conv3_3/3\', \'conv3_2\')  # 128\n         .add(name=\'conv3_3\')\n         .pva_negation_block_v2(1, 1, 48, 1, 1, 128, name=\'conv3_4/1\', negation=False)\n         .pva_negation_block_v2(3, 3, 48, 1, 1, 48, name=\'conv3_4/2\', negation=False)\n         .pva_negation_block_v2(1, 1, 128, 1, 1, 48, name=\'conv3_4/3\', negation=True))\n\n        (self.feed(\'conv3_4/3\', \'conv3_3\')  # 128\n         .add(name=\'conv3_4\')\n         .max_pool(3, 3, 2, 2, padding=\'SAME\', name=\'downsample\')) # downsample\n\n        (self.feed(\'conv3_4\')\n         .pva_inception_res_block(name = \'conv4_4\', name_prefix = \'conv4_\', type=\'a\') # downsample\n         .pva_inception_res_block(name=\'conv5_4\', name_prefix=\'conv5_\', type=\'b\')     # downsample\n         .batch_normalization(name=\'conv5_4/last_bn\', relu=False)\n         .relu(name=\'conv5_4/last_relu\'))\n\n        (self.feed(\'conv5_4/last_relu\')\n         .upconv(tf.shape(self.layers[\'downsample\']),\n                 384, 4, 2, name = \'upsample\', biased= False, relu=False, trainable=True)) # upsample\n\n        (self.feed(\'downsample\', \'conv4_4\')\n         .concat(axis=3, name=\'concat\'))\n\n        # ========= RPN ============\n        (self.feed(\'concat\')\n         .conv(1, 1, 128, 1, 1, name=\'convf_rpn\', biased=True, relu=True)\n         .conv(3, 3, 384, 1, 1, name=\'rpn_conv/3x3\', biased=True, relu=True)\n         .conv(1, 1, len(anchor_scales) * 3 * 2, 1, 1, padding=\'VALID\', relu=False, name=\'rpn_cls_score\'))\n\n        (self.feed(\'rpn_cls_score\', \'gt_boxes\', \'gt_ishard\', \'dontcare_areas\', \'im_info\')\n         .anchor_target_layer(_feat_stride, anchor_scales, name=\'rpn-data\'))\n        # Loss of rpn_cls & rpn_boxes\n\n        (self.feed(\'rpn_conv/3x3\')\n         .conv(1, 1, len(anchor_scales) * 3 * 4, 1, 1, padding=\'VALID\', relu=False, name=\'rpn_bbox_pred\'))\n\n        # ========= RoI Proposal ============\n        (self.feed(\'rpn_cls_score\')\n         .spatial_reshape_layer(2, name=\'rpn_cls_score_reshape\')\n         .spatial_softmax(name=\'rpn_cls_prob\'))\n\n        (self.feed(\'rpn_cls_prob\')\n         .spatial_reshape_layer(len(anchor_scales) * 3 * 2, name=\'rpn_cls_prob_reshape\'))\n\n        (self.feed(\'rpn_cls_prob_reshape\', \'rpn_bbox_pred\', \'im_info\')\n         .proposal_layer(_feat_stride, anchor_scales, \'TRAIN\', name=\'rpn_rois\'))\n\n        (self.feed(\'rpn_rois\', \'gt_boxes\', \'gt_ishard\', \'dontcare_areas\')\n         .proposal_target_layer(n_classes, name=\'roi-data\'))\n\n        # ========= RCNN ============\n        (self.feed(\'concat\')\n         .conv(1, 1, 384, 1, 1, name=\'convf_2\', biased=True, relu=True))\n\n        # (self.feed(\'convf_rpn\', \'convf_2\')\n        #  .concat(axis=3, name=\'convf\'))\n\n        (self.feed(\'convf_2\', \'roi-data\')\n         .roi_pool(6, 6, 1.0 / 16, name=\'roi_pooling\')\n         .fc(4096, name=\'fc6\', relu=False)\n         .bn_scale_combo(c_in = 4096, name=\'fc6\', relu=True)\n         .dropout(0.5, name=\'fc6/drop6\')\n         .fc(4096, name=\'fc7\', relu=False)\n         .bn_scale_combo(c_in=4096, name=\'fc7\', relu=True)\n         .dropout(0.5, name=\'fc7/drop7\')\n         .fc(n_classes, relu=False, name=\'cls_score\')\n         .softmax(name=\'cls_prob\'))\n\n        (self.feed(\'fc7/drop7\')\n         .fc(n_classes * 4, relu=False, name=\'bbox_pred\'))\n\n    def load(self, data_path, session, ignore_missing=False):\n        data_dict = np.load(data_path).item()\n        # print (data_dict.keys())\n        for key in sorted(data_dict.keys(), reverse=True):\n            with tf.variable_scope(key, reuse=True):\n                for subkey in data_dict[key]:\n                    try:\n                        var = tf.get_variable(subkey)\n                        session.run(var.assign(data_dict[key][subkey]))\n                        print ""assign pretrain model "" + subkey + "" to "" + key + \'/\' + subkey\n                    except ValueError:\n                        print ""ignore "" + key + \'/\' + subkey + \' shape:\', data_dict[key][subkey].shape\n                        if not ignore_missing:\n\n                            raise'"
lib/networks/Resnet101_test.py,3,"b""# --------------------------------------------------------\n# TFFRCNN - Resnet50\n# Copyright (c) 2016\n# Licensed under The MIT License [see LICENSE for details]\n# Written by miraclebiu\n# --------------------------------------------------------\nimport tensorflow as tf\nfrom .network import Network\nimport pdb\nfrom ..fast_rcnn.config import cfg\n\nclass Resnet101_test(Network):\n    def __init__(self, trainable=True):\n        self.inputs = []\n        self.data = tf.placeholder(tf.float32, shape=[None, None, None, 3])\n        self.im_info = tf.placeholder(tf.float32, shape=[None, 3])\n        self.keep_prob = tf.placeholder(tf.float32)\n        self.layers = dict({'data':self.data, 'im_info':self.im_info})\n        self.trainable = trainable\n        self.setup()\n\n    def setup(self):\n\n        n_classes = cfg.NCLASSES\n        # anchor_scales = [8, 16, 32]\n        anchor_scales = cfg.ANCHOR_SCALES\n        _feat_stride = [16, ]\n\n        (self.feed('data')\n             .conv(7, 7, 64, 2, 2, biased=False, relu=False, name='conv1')\n             .batch_normalization(relu=True, name='bn_conv1', is_training=False)\n             .max_pool(3, 3, 2, 2, padding='VALID',name='pool1')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res2a_branch1')\n             .batch_normalization(name='bn2a_branch1',is_training=False,relu=False))\n\n        (self.feed('pool1')\n             .conv(1, 1, 64, 1, 1, biased=False, relu=False, name='res2a_branch2a')\n             .batch_normalization(relu=True, name='bn2a_branch2a',is_training=False)\n             .conv(3, 3, 64, 1, 1, biased=False, relu=False, name='res2a_branch2b')\n             .batch_normalization(relu=True, name='bn2a_branch2b',is_training=False)\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res2a_branch2c')\n             .batch_normalization(name='bn2a_branch2c',is_training=False,relu=False))\n\n        (self.feed('bn2a_branch1', \n                   'bn2a_branch2c')\n             .add(name='res2a')\n             .relu(name='res2a_relu')\n             .conv(1, 1, 64, 1, 1, biased=False, relu=False, name='res2b_branch2a')\n             .batch_normalization(relu=True, name='bn2b_branch2a',is_training=False)\n             .conv(3, 3, 64, 1, 1, biased=False, relu=False, name='res2b_branch2b')\n             .batch_normalization(relu=True, name='bn2b_branch2b',is_training=False)\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res2b_branch2c')\n             .batch_normalization(name='bn2b_branch2c',is_training=False,relu=False))\n\n        (self.feed('res2a_relu', \n                   'bn2b_branch2c')\n             .add(name='res2b')\n             .relu(name='res2b_relu')\n             .conv(1, 1, 64, 1, 1, biased=False, relu=False, name='res2c_branch2a')\n             .batch_normalization(relu=True, name='bn2c_branch2a',is_training=False)\n             .conv(3, 3, 64, 1, 1, biased=False, relu=False, name='res2c_branch2b')\n             .batch_normalization(relu=True, name='bn2c_branch2b',is_training=False)\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res2c_branch2c')\n             .batch_normalization(name='bn2c_branch2c',is_training=False,relu=False))\n\n        (self.feed('res2b_relu', \n                   'bn2c_branch2c')\n             .add(name='res2c')\n             .relu(name='res2c_relu')\n             .conv(1, 1, 512, 2, 2, biased=False, relu=False, name='res3a_branch1',padding='VALID')\n             .batch_normalization(name='bn3a_branch1',is_training=False,relu=False))\n\n        (self.feed('res2c_relu')\n             .conv(1, 1, 128, 2, 2, biased=False, relu=False, name='res3a_branch2a',padding='VALID')\n             .batch_normalization(relu=True, name='bn3a_branch2a',is_training=False)\n             .conv(3, 3, 128, 1, 1, biased=False, relu=False, name='res3a_branch2b')\n             .batch_normalization(relu=True, name='bn3a_branch2b',is_training=False)\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res3a_branch2c')\n             .batch_normalization(name='bn3a_branch2c',is_training=False,relu=False))\n\n        (self.feed('bn3a_branch1', \n                   'bn3a_branch2c')\n             .add(name='res3a')\n             .relu(name='res3a_relu')\n             .conv(1, 1, 128, 1, 1, biased=False, relu=False, name='res3b1_branch2a')\n             .batch_normalization(relu=True, name='bn3b1_branch2a',is_training=False)\n             .conv(3, 3, 128, 1, 1, biased=False, relu=False, name='res3b1_branch2b')\n             .batch_normalization(relu=True, name='bn3b1_branch2b',is_training=False)\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res3b1_branch2c')\n             .batch_normalization(name='bn3b1_branch2c',is_training=False,relu=False))\n\n        (self.feed('res3a_relu', \n                   'bn3b1_branch2c')\n             .add(name='res3b1')\n             .relu(name='res3b1_relu')\n             .conv(1, 1, 128, 1, 1, biased=False, relu=False, name='res3b2_branch2a')\n             .batch_normalization(relu=True, name='bn3b2_branch2a',is_training=False)\n             .conv(3, 3, 128, 1, 1, biased=False, relu=False, name='res3b2_branch2b')\n             .batch_normalization(relu=True, name='bn3b2_branch2b',is_training=False)\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res3b2_branch2c')\n             .batch_normalization(name='bn3b2_branch2c',is_training=False,relu=False))\n\n        (self.feed('res3b1_relu', \n                   'bn3b2_branch2c')\n             .add(name='res3b2')\n             .relu(name='res3b2_relu')\n             .conv(1, 1, 128, 1, 1, biased=False, relu=False, name='res3b3_branch2a')\n             .batch_normalization(relu=True, name='bn3b3_branch2a',is_training=False)\n             .conv(3, 3, 128, 1, 1, biased=False, relu=False, name='res3b3_branch2b')\n             .batch_normalization(relu=True, name='bn3b3_branch2b',is_training=False)\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res3b3_branch2c')\n             .batch_normalization(name='bn3b3_branch2c',is_training=False,relu=False))\n\n        (self.feed('res3b2_relu', \n                   'bn3b3_branch2c')\n             .add(name='res3b3')\n             .relu(name='res3b3_relu')\n             .conv(1, 1, 1024, 2, 2, biased=False, relu=False, name='res4a_branch1',padding='VALID')\n             .batch_normalization(name='bn4a_branch1',is_training=False,relu=False))\n\n        (self.feed('res3b3_relu')\n             .conv(1, 1, 256, 2, 2, biased=False, relu=False, name='res4a_branch2a',padding='VALID')\n             .batch_normalization(relu=True, name='bn4a_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4a_branch2b')\n             .batch_normalization(relu=True, name='bn4a_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4a_branch2c')\n             .batch_normalization(name='bn4a_branch2c',is_training=False,relu=False))\n\n        (self.feed('bn4a_branch1', \n                   'bn4a_branch2c')\n             .add(name='res4a')\n             .relu(name='res4a_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b1_branch2a')\n             .batch_normalization(relu=True, name='bn4b1_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b1_branch2b')\n             .batch_normalization(relu=True, name='bn4b1_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b1_branch2c')\n             .batch_normalization(name='bn4b1_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4a_relu', \n                   'bn4b1_branch2c')\n             .add(name='res4b1')\n             .relu(name='res4b1_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b2_branch2a')\n             .batch_normalization(relu=True, name='bn4b2_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b2_branch2b')\n             .batch_normalization(relu=True, name='bn4b2_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b2_branch2c')\n             .batch_normalization(name='bn4b2_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b1_relu', \n                   'bn4b2_branch2c')\n             .add(name='res4b2')\n             .relu(name='res4b2_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b3_branch2a')\n             .batch_normalization(relu=True, name='bn4b3_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b3_branch2b')\n             .batch_normalization(relu=True, name='bn4b3_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b3_branch2c')\n             .batch_normalization(name='bn4b3_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b2_relu', \n                   'bn4b3_branch2c')\n             .add(name='res4b3')\n             .relu(name='res4b3_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b4_branch2a')\n             .batch_normalization(relu=True, name='bn4b4_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b4_branch2b')\n             .batch_normalization(relu=True, name='bn4b4_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b4_branch2c')\n             .batch_normalization(name='bn4b4_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b3_relu', \n                   'bn4b4_branch2c')\n             .add(name='res4b4')\n             .relu(name='res4b4_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b5_branch2a')\n             .batch_normalization(relu=True, name='bn4b5_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b5_branch2b')\n             .batch_normalization(relu=True, name='bn4b5_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b5_branch2c')\n             .batch_normalization(name='bn4b5_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b4_relu', \n                   'bn4b5_branch2c')\n             .add(name='res4b5')\n             .relu(name='res4b5_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b6_branch2a')\n             .batch_normalization(relu=True, name='bn4b6_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b6_branch2b')\n             .batch_normalization(relu=True, name='bn4b6_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b6_branch2c')\n             .batch_normalization(name='bn4b6_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b5_relu', \n                   'bn4b6_branch2c')\n             .add(name='res4b6')\n             .relu(name='res4b6_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b7_branch2a')\n             .batch_normalization(relu=True, name='bn4b7_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b7_branch2b')\n             .batch_normalization(relu=True, name='bn4b7_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b7_branch2c')\n             .batch_normalization(name='bn4b7_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b6_relu', \n                   'bn4b7_branch2c')\n             .add(name='res4b7')\n             .relu(name='res4b7_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b8_branch2a')\n             .batch_normalization(relu=True, name='bn4b8_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b8_branch2b')\n             .batch_normalization(relu=True, name='bn4b8_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b8_branch2c')\n             .batch_normalization(name='bn4b8_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b7_relu', \n                   'bn4b8_branch2c')\n             .add(name='res4b8')\n             .relu(name='res4b8_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b9_branch2a')\n             .batch_normalization(relu=True, name='bn4b9_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b9_branch2b')\n             .batch_normalization(relu=True, name='bn4b9_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b9_branch2c')\n             .batch_normalization(name='bn4b9_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b8_relu', \n                   'bn4b9_branch2c')\n             .add(name='res4b9')\n             .relu(name='res4b9_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b10_branch2a')\n             .batch_normalization(relu=True, name='bn4b10_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b10_branch2b')\n             .batch_normalization(relu=True, name='bn4b10_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b10_branch2c')\n             .batch_normalization(name='bn4b10_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b9_relu', \n                   'bn4b10_branch2c')\n             .add(name='res4b10')\n             .relu(name='res4b10_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b11_branch2a')\n             .batch_normalization(relu=True, name='bn4b11_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b11_branch2b')\n             .batch_normalization(relu=True, name='bn4b11_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b11_branch2c')\n             .batch_normalization(name='bn4b11_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b10_relu', \n                   'bn4b11_branch2c')\n             .add(name='res4b11')\n             .relu(name='res4b11_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b12_branch2a')\n             .batch_normalization(relu=True, name='bn4b12_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b12_branch2b')\n             .batch_normalization(relu=True, name='bn4b12_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b12_branch2c')\n             .batch_normalization(name='bn4b12_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b11_relu', \n                   'bn4b12_branch2c')\n             .add(name='res4b12')\n             .relu(name='res4b12_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b13_branch2a')\n             .batch_normalization(relu=True, name='bn4b13_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b13_branch2b')\n             .batch_normalization(relu=True, name='bn4b13_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b13_branch2c')\n             .batch_normalization(name='bn4b13_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b12_relu', \n                   'bn4b13_branch2c')\n             .add(name='res4b13')\n             .relu(name='res4b13_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b14_branch2a')\n             .batch_normalization(relu=True, name='bn4b14_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b14_branch2b')\n             .batch_normalization(relu=True, name='bn4b14_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b14_branch2c')\n             .batch_normalization(name='bn4b14_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b13_relu', \n                   'bn4b14_branch2c')\n             .add(name='res4b14')\n             .relu(name='res4b14_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b15_branch2a')\n             .batch_normalization(relu=True, name='bn4b15_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b15_branch2b')\n             .batch_normalization(relu=True, name='bn4b15_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b15_branch2c')\n             .batch_normalization(name='bn4b15_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b14_relu', \n                   'bn4b15_branch2c')\n             .add(name='res4b15')\n             .relu(name='res4b15_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b16_branch2a')\n             .batch_normalization(relu=True, name='bn4b16_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b16_branch2b')\n             .batch_normalization(relu=True, name='bn4b16_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b16_branch2c')\n             .batch_normalization(name='bn4b16_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b15_relu', \n                   'bn4b16_branch2c')\n             .add(name='res4b16')\n             .relu(name='res4b16_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b17_branch2a')\n             .batch_normalization(relu=True, name='bn4b17_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b17_branch2b')\n             .batch_normalization(relu=True, name='bn4b17_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b17_branch2c')\n             .batch_normalization(name='bn4b17_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b16_relu', \n                   'bn4b17_branch2c')\n             .add(name='res4b17')\n             .relu(name='res4b17_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b18_branch2a')\n             .batch_normalization(relu=True, name='bn4b18_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b18_branch2b')\n             .batch_normalization(relu=True, name='bn4b18_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b18_branch2c')\n             .batch_normalization(name='bn4b18_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b17_relu', \n                   'bn4b18_branch2c')\n             .add(name='res4b18')\n             .relu(name='res4b18_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b19_branch2a')\n             .batch_normalization(relu=True, name='bn4b19_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b19_branch2b')\n             .batch_normalization(relu=True, name='bn4b19_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b19_branch2c')\n             .batch_normalization(name='bn4b19_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b18_relu', \n                   'bn4b19_branch2c')\n             .add(name='res4b19')\n             .relu(name='res4b19_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b20_branch2a')\n             .batch_normalization(relu=True, name='bn4b20_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b20_branch2b')\n             .batch_normalization(relu=True, name='bn4b20_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b20_branch2c')\n             .batch_normalization(name='bn4b20_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b19_relu', \n                   'bn4b20_branch2c')\n             .add(name='res4b20')\n             .relu(name='res4b20_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b21_branch2a')\n             .batch_normalization(relu=True, name='bn4b21_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b21_branch2b')\n             .batch_normalization(relu=True, name='bn4b21_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b21_branch2c')\n             .batch_normalization(name='bn4b21_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b20_relu', \n                   'bn4b21_branch2c')\n             .add(name='res4b21')\n             .relu(name='res4b21_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b22_branch2a')\n             .batch_normalization(relu=True, name='bn4b22_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b22_branch2b')\n             .batch_normalization(relu=True, name='bn4b22_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b22_branch2c')\n             .batch_normalization(name='bn4b22_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b21_relu', \n                   'bn4b22_branch2c')\n             .add(name='res4b22')\n             .relu(name='res4b22_relu'))\n\n        #========= RPN ============\n        (self.feed('res4b22_relu')\n             .conv(3,3,512,1,1,name='rpn_conv/3x3')\n             .conv(1,1,len(anchor_scales)*3*2 ,1 , 1, padding='VALID', relu = False, name='rpn_cls_score'))\n  \n\n        (self.feed('rpn_conv/3x3')\n             .conv(1,1,len(anchor_scales)*3*4, 1, 1, padding='VALID', relu = False, name='rpn_bbox_pred'))\n\n        #========= RoI Proposal ============\n        (self.feed('rpn_cls_score')\n             .spatial_reshape_layer(2, name = 'rpn_cls_score_reshape')\n             .spatial_softmax(name='rpn_cls_prob'))\n\n        (self.feed('rpn_cls_prob')\n             .spatial_reshape_layer(len(anchor_scales)*3*2, name = 'rpn_cls_prob_reshape'))\n\n        (self.feed('rpn_cls_prob_reshape','rpn_bbox_pred','im_info')\n             .proposal_layer(_feat_stride, anchor_scales, 'TEST',name = 'rois'))\n\n\n\n        #========= RCNN ============ \n        (self.feed('res4b22_relu','rois')\n             .roi_pool(7,7,1.0/16,name='res5a_branch2a_roipooling')\n             .conv(1, 1, 512, 2, 2, biased=False, relu=False, name='res5a_branch2a',padding='VALID')\n             .batch_normalization(relu=True, name='bn5a_branch2a',is_training=False)\n             .conv(3, 3, 512, 1, 1, biased=False, relu=False, name='res5a_branch2b')\n             .batch_normalization(relu=True, name='bn5a_branch2b',is_training=False)\n             .conv(1, 1, 2048, 1, 1, biased=False, relu=False, name='res5a_branch2c')\n             .batch_normalization(name='bn5a_branch2c',is_training=False,relu=False))\n\n        (self.feed('res5a_branch2a_roipooling')\n             .conv(1, 1, 2048, 2, 2, biased=False, relu=False, name='res5a_branch1',padding='VALID')\n             .batch_normalization(name='bn5a_branch1',is_training=False,relu=False))\n\n\n        (self.feed('bn5a_branch1', \n                   'bn5a_branch2c')\n             .add(name='res5a')\n             .relu(name='res5a_relu')\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res5b_branch2a')\n             .batch_normalization(relu=True, name='bn5b_branch2a',is_training=False)\n             .conv(3, 3, 512, 1, 1, biased=False, relu=False, name='res5b_branch2b')\n             .batch_normalization(relu=True, name='bn5b_branch2b',is_training=False)\n             .conv(1, 1, 2048, 1, 1, biased=False, relu=False, name='res5b_branch2c')\n             .batch_normalization(name='bn5b_branch2c',is_training=False,relu=False))\n\n        (self.feed('res5a_relu', \n                   'bn5b_branch2c')\n             .add(name='res5b')\n             .relu(name='res5b_relu')\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res5c_branch2a')\n             .batch_normalization(relu=True, name='bn5c_branch2a',is_training=False)\n             .conv(3, 3, 512, 1, 1, biased=False, relu=False, name='res5c_branch2b')\n             .batch_normalization(relu=True, name='bn5c_branch2b',is_training=False)\n             .conv(1, 1, 2048, 1, 1, biased=False, relu=False, name='res5c_branch2c')\n             .batch_normalization(name='bn5c_branch2c',is_training=False,relu=False))\n\n        (self.feed('res5b_relu', \n                   'bn5c_branch2c')\n             .add(name='res5c')\n             .relu(name='res5c_relu')\n             .avg_pool(4, 4, 1, 1, padding='VALID', name='pool5')\n             .fc(1000, relu=False, name='fc1000')\n             .softmax(name='prob'))"""
lib/networks/Resnet101_train.py,6,"b""# --------------------------------------------------------\n# TFFRCNN - Resnet50\n# Copyright (c) 2016\n# Licensed under The MIT License [see LICENSE for details]\n# Written by miraclebiu\n# --------------------------------------------------------\n\nimport tensorflow as tf\nfrom .network import Network\nfrom ..fast_rcnn.config import cfg\n\nclass Resnet101_train(Network):\n    def __init__(self, trainable=True):\n        self.inputs = []\n        self.data = tf.placeholder(tf.float32, shape=[None, None, None, 3], name='data')\n        self.im_info = tf.placeholder(tf.float32, shape=[None, 3], name='im_info')\n        self.gt_boxes = tf.placeholder(tf.float32, shape=[None, 5], name='gt_boxes')\n        self.gt_ishard = tf.placeholder(tf.int32, shape=[None], name='gt_ishard')\n        self.dontcare_areas = tf.placeholder(tf.float32, shape=[None, 4], name='dontcare_areas')\n        self.keep_prob = tf.placeholder(tf.float32)\n        self.layers = dict({'data':self.data, 'im_info':self.im_info, 'gt_boxes':self.gt_boxes,\\\n                            'gt_ishard': self.gt_ishard, 'dontcare_areas': self.dontcare_areas})\n        self.trainable = trainable\n        self.setup()\n\n    def setup(self):\n\n        n_classes = cfg.NCLASSES\n        # anchor_scales = [8, 16, 32]\n        anchor_scales = cfg.ANCHOR_SCALES\n        _feat_stride = [16, ]\n\n        (self.feed('data')\n             .conv(7, 7, 64, 2, 2, biased=False, relu=False, name='conv1')\n             .batch_normalization(relu=True, name='bn_conv1', is_training=False)\n             .max_pool(3, 3, 2, 2, padding='VALID',name='pool1')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res2a_branch1')\n             .batch_normalization(name='bn2a_branch1',is_training=False,relu=False))\n\n        (self.feed('pool1')\n             .conv(1, 1, 64, 1, 1, biased=False, relu=False, name='res2a_branch2a')\n             .batch_normalization(relu=True, name='bn2a_branch2a',is_training=False)\n             .conv(3, 3, 64, 1, 1, biased=False, relu=False, name='res2a_branch2b')\n             .batch_normalization(relu=True, name='bn2a_branch2b',is_training=False)\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res2a_branch2c')\n             .batch_normalization(name='bn2a_branch2c',is_training=False,relu=False))\n\n        (self.feed('bn2a_branch1', \n                   'bn2a_branch2c')\n             .add(name='res2a')\n             .relu(name='res2a_relu')\n             .conv(1, 1, 64, 1, 1, biased=False, relu=False, name='res2b_branch2a')\n             .batch_normalization(relu=True, name='bn2b_branch2a',is_training=False)\n             .conv(3, 3, 64, 1, 1, biased=False, relu=False, name='res2b_branch2b')\n             .batch_normalization(relu=True, name='bn2b_branch2b',is_training=False)\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res2b_branch2c')\n             .batch_normalization(name='bn2b_branch2c',is_training=False,relu=False))\n\n        (self.feed('res2a_relu', \n                   'bn2b_branch2c')\n             .add(name='res2b')\n             .relu(name='res2b_relu')\n             .conv(1, 1, 64, 1, 1, biased=False, relu=False, name='res2c_branch2a')\n             .batch_normalization(relu=True, name='bn2c_branch2a',is_training=False)\n             .conv(3, 3, 64, 1, 1, biased=False, relu=False, name='res2c_branch2b')\n             .batch_normalization(relu=True, name='bn2c_branch2b',is_training=False)\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res2c_branch2c')\n             .batch_normalization(name='bn2c_branch2c',is_training=False,relu=False))\n\n        (self.feed('res2b_relu', \n                   'bn2c_branch2c')\n             .add(name='res2c')\n             .relu(name='res2c_relu')\n             .conv(1, 1, 512, 2, 2, biased=False, relu=False, name='res3a_branch1',padding='VALID')\n             .batch_normalization(name='bn3a_branch1',is_training=False,relu=False))\n\n        (self.feed('res2c_relu')\n             .conv(1, 1, 128, 2, 2, biased=False, relu=False, name='res3a_branch2a',padding='VALID')\n             .batch_normalization(relu=True, name='bn3a_branch2a',is_training=False)\n             .conv(3, 3, 128, 1, 1, biased=False, relu=False, name='res3a_branch2b')\n             .batch_normalization(relu=True, name='bn3a_branch2b',is_training=False)\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res3a_branch2c')\n             .batch_normalization(name='bn3a_branch2c',is_training=False,relu=False))\n\n        (self.feed('bn3a_branch1', \n                   'bn3a_branch2c')\n             .add(name='res3a')\n             .relu(name='res3a_relu')\n             .conv(1, 1, 128, 1, 1, biased=False, relu=False, name='res3b1_branch2a')\n             .batch_normalization(relu=True, name='bn3b1_branch2a',is_training=False)\n             .conv(3, 3, 128, 1, 1, biased=False, relu=False, name='res3b1_branch2b')\n             .batch_normalization(relu=True, name='bn3b1_branch2b',is_training=False)\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res3b1_branch2c')\n             .batch_normalization(name='bn3b1_branch2c',is_training=False,relu=False))\n\n        (self.feed('res3a_relu', \n                   'bn3b1_branch2c')\n             .add(name='res3b1')\n             .relu(name='res3b1_relu')\n             .conv(1, 1, 128, 1, 1, biased=False, relu=False, name='res3b2_branch2a')\n             .batch_normalization(relu=True, name='bn3b2_branch2a',is_training=False)\n             .conv(3, 3, 128, 1, 1, biased=False, relu=False, name='res3b2_branch2b')\n             .batch_normalization(relu=True, name='bn3b2_branch2b',is_training=False)\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res3b2_branch2c')\n             .batch_normalization(name='bn3b2_branch2c',is_training=False,relu=False))\n\n        (self.feed('res3b1_relu', \n                   'bn3b2_branch2c')\n             .add(name='res3b2')\n             .relu(name='res3b2_relu')\n             .conv(1, 1, 128, 1, 1, biased=False, relu=False, name='res3b3_branch2a')\n             .batch_normalization(relu=True, name='bn3b3_branch2a',is_training=False)\n             .conv(3, 3, 128, 1, 1, biased=False, relu=False, name='res3b3_branch2b')\n             .batch_normalization(relu=True, name='bn3b3_branch2b',is_training=False)\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res3b3_branch2c')\n             .batch_normalization(name='bn3b3_branch2c',is_training=False,relu=False))\n\n        (self.feed('res3b2_relu', \n                   'bn3b3_branch2c')\n             .add(name='res3b3')\n             .relu(name='res3b3_relu')\n             .conv(1, 1, 1024, 2, 2, biased=False, relu=False, name='res4a_branch1',padding='VALID')\n             .batch_normalization(name='bn4a_branch1',is_training=False,relu=False))\n\n        (self.feed('res3b3_relu')\n             .conv(1, 1, 256, 2, 2, biased=False, relu=False, name='res4a_branch2a',padding='VALID')\n             .batch_normalization(relu=True, name='bn4a_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4a_branch2b')\n             .batch_normalization(relu=True, name='bn4a_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4a_branch2c')\n             .batch_normalization(name='bn4a_branch2c',is_training=False,relu=False))\n\n        (self.feed('bn4a_branch1', \n                   'bn4a_branch2c')\n             .add(name='res4a')\n             .relu(name='res4a_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b1_branch2a')\n             .batch_normalization(relu=True, name='bn4b1_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b1_branch2b')\n             .batch_normalization(relu=True, name='bn4b1_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b1_branch2c')\n             .batch_normalization(name='bn4b1_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4a_relu', \n                   'bn4b1_branch2c')\n             .add(name='res4b1')\n             .relu(name='res4b1_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b2_branch2a')\n             .batch_normalization(relu=True, name='bn4b2_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b2_branch2b')\n             .batch_normalization(relu=True, name='bn4b2_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b2_branch2c')\n             .batch_normalization(name='bn4b2_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b1_relu', \n                   'bn4b2_branch2c')\n             .add(name='res4b2')\n             .relu(name='res4b2_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b3_branch2a')\n             .batch_normalization(relu=True, name='bn4b3_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b3_branch2b')\n             .batch_normalization(relu=True, name='bn4b3_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b3_branch2c')\n             .batch_normalization(name='bn4b3_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b2_relu', \n                   'bn4b3_branch2c')\n             .add(name='res4b3')\n             .relu(name='res4b3_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b4_branch2a')\n             .batch_normalization(relu=True, name='bn4b4_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b4_branch2b')\n             .batch_normalization(relu=True, name='bn4b4_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b4_branch2c')\n             .batch_normalization(name='bn4b4_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b3_relu', \n                   'bn4b4_branch2c')\n             .add(name='res4b4')\n             .relu(name='res4b4_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b5_branch2a')\n             .batch_normalization(relu=True, name='bn4b5_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b5_branch2b')\n             .batch_normalization(relu=True, name='bn4b5_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b5_branch2c')\n             .batch_normalization(name='bn4b5_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b4_relu', \n                   'bn4b5_branch2c')\n             .add(name='res4b5')\n             .relu(name='res4b5_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b6_branch2a')\n             .batch_normalization(relu=True, name='bn4b6_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b6_branch2b')\n             .batch_normalization(relu=True, name='bn4b6_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b6_branch2c')\n             .batch_normalization(name='bn4b6_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b5_relu', \n                   'bn4b6_branch2c')\n             .add(name='res4b6')\n             .relu(name='res4b6_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b7_branch2a')\n             .batch_normalization(relu=True, name='bn4b7_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b7_branch2b')\n             .batch_normalization(relu=True, name='bn4b7_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b7_branch2c')\n             .batch_normalization(name='bn4b7_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b6_relu', \n                   'bn4b7_branch2c')\n             .add(name='res4b7')\n             .relu(name='res4b7_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b8_branch2a')\n             .batch_normalization(relu=True, name='bn4b8_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b8_branch2b')\n             .batch_normalization(relu=True, name='bn4b8_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b8_branch2c')\n             .batch_normalization(name='bn4b8_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b7_relu', \n                   'bn4b8_branch2c')\n             .add(name='res4b8')\n             .relu(name='res4b8_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b9_branch2a')\n             .batch_normalization(relu=True, name='bn4b9_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b9_branch2b')\n             .batch_normalization(relu=True, name='bn4b9_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b9_branch2c')\n             .batch_normalization(name='bn4b9_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b8_relu', \n                   'bn4b9_branch2c')\n             .add(name='res4b9')\n             .relu(name='res4b9_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b10_branch2a')\n             .batch_normalization(relu=True, name='bn4b10_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b10_branch2b')\n             .batch_normalization(relu=True, name='bn4b10_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b10_branch2c')\n             .batch_normalization(name='bn4b10_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b9_relu', \n                   'bn4b10_branch2c')\n             .add(name='res4b10')\n             .relu(name='res4b10_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b11_branch2a')\n             .batch_normalization(relu=True, name='bn4b11_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b11_branch2b')\n             .batch_normalization(relu=True, name='bn4b11_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b11_branch2c')\n             .batch_normalization(name='bn4b11_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b10_relu', \n                   'bn4b11_branch2c')\n             .add(name='res4b11')\n             .relu(name='res4b11_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b12_branch2a')\n             .batch_normalization(relu=True, name='bn4b12_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b12_branch2b')\n             .batch_normalization(relu=True, name='bn4b12_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b12_branch2c')\n             .batch_normalization(name='bn4b12_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b11_relu', \n                   'bn4b12_branch2c')\n             .add(name='res4b12')\n             .relu(name='res4b12_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b13_branch2a')\n             .batch_normalization(relu=True, name='bn4b13_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b13_branch2b')\n             .batch_normalization(relu=True, name='bn4b13_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b13_branch2c')\n             .batch_normalization(name='bn4b13_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b12_relu', \n                   'bn4b13_branch2c')\n             .add(name='res4b13')\n             .relu(name='res4b13_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b14_branch2a')\n             .batch_normalization(relu=True, name='bn4b14_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b14_branch2b')\n             .batch_normalization(relu=True, name='bn4b14_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b14_branch2c')\n             .batch_normalization(name='bn4b14_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b13_relu', \n                   'bn4b14_branch2c')\n             .add(name='res4b14')\n             .relu(name='res4b14_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b15_branch2a')\n             .batch_normalization(relu=True, name='bn4b15_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b15_branch2b')\n             .batch_normalization(relu=True, name='bn4b15_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b15_branch2c')\n             .batch_normalization(name='bn4b15_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b14_relu', \n                   'bn4b15_branch2c')\n             .add(name='res4b15')\n             .relu(name='res4b15_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b16_branch2a')\n             .batch_normalization(relu=True, name='bn4b16_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b16_branch2b')\n             .batch_normalization(relu=True, name='bn4b16_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b16_branch2c')\n             .batch_normalization(name='bn4b16_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b15_relu', \n                   'bn4b16_branch2c')\n             .add(name='res4b16')\n             .relu(name='res4b16_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b17_branch2a')\n             .batch_normalization(relu=True, name='bn4b17_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b17_branch2b')\n             .batch_normalization(relu=True, name='bn4b17_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b17_branch2c')\n             .batch_normalization(name='bn4b17_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b16_relu', \n                   'bn4b17_branch2c')\n             .add(name='res4b17')\n             .relu(name='res4b17_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b18_branch2a')\n             .batch_normalization(relu=True, name='bn4b18_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b18_branch2b')\n             .batch_normalization(relu=True, name='bn4b18_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b18_branch2c')\n             .batch_normalization(name='bn4b18_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b17_relu', \n                   'bn4b18_branch2c')\n             .add(name='res4b18')\n             .relu(name='res4b18_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b19_branch2a')\n             .batch_normalization(relu=True, name='bn4b19_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b19_branch2b')\n             .batch_normalization(relu=True, name='bn4b19_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b19_branch2c')\n             .batch_normalization(name='bn4b19_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b18_relu', \n                   'bn4b19_branch2c')\n             .add(name='res4b19')\n             .relu(name='res4b19_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b20_branch2a')\n             .batch_normalization(relu=True, name='bn4b20_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b20_branch2b')\n             .batch_normalization(relu=True, name='bn4b20_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b20_branch2c')\n             .batch_normalization(name='bn4b20_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b19_relu', \n                   'bn4b20_branch2c')\n             .add(name='res4b20')\n             .relu(name='res4b20_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b21_branch2a')\n             .batch_normalization(relu=True, name='bn4b21_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b21_branch2b')\n             .batch_normalization(relu=True, name='bn4b21_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b21_branch2c')\n             .batch_normalization(name='bn4b21_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b20_relu', \n                   'bn4b21_branch2c')\n             .add(name='res4b21')\n             .relu(name='res4b21_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b22_branch2a')\n             .batch_normalization(relu=True, name='bn4b22_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b22_branch2b')\n             .batch_normalization(relu=True, name='bn4b22_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b22_branch2c')\n             .batch_normalization(name='bn4b22_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b21_relu', \n                   'bn4b22_branch2c')\n             .add(name='res4b22')\n             .relu(name='res4b22_relu'))\n\n        #========= RPN ============\n        (self.feed('res4b22_relu')\n             .conv(3,3,512,1,1,name='rpn_conv/3x3')\n             .conv(1,1,len(anchor_scales)*3*2 ,1 , 1, padding='VALID', relu = False, name='rpn_cls_score'))\n  \n        (self.feed('rpn_cls_score', 'gt_boxes', 'gt_ishard', 'dontcare_areas', 'im_info')\n             .anchor_target_layer(_feat_stride, anchor_scales, name = 'rpn-data' ))\n        # Loss of rpn_cls & rpn_boxes\n\n        (self.feed('rpn_conv/3x3')\n             .conv(1,1,len(anchor_scales)*3*4, 1, 1, padding='VALID', relu = False, name='rpn_bbox_pred'))\n\n        #========= RoI Proposal ============\n        (self.feed('rpn_cls_score')\n             .spatial_reshape_layer(2, name = 'rpn_cls_score_reshape')\n             .spatial_softmax(name='rpn_cls_prob'))\n\n        (self.feed('rpn_cls_prob')\n             .spatial_reshape_layer(len(anchor_scales)*3*2, name = 'rpn_cls_prob_reshape'))\n\n        (self.feed('rpn_cls_prob_reshape','rpn_bbox_pred','im_info')\n             .proposal_layer(_feat_stride, anchor_scales, 'TRAIN',name = 'rpn_rois'))\n\n        (self.feed('rpn_rois','gt_boxes', 'gt_ishard', 'dontcare_areas')\n             .proposal_target_layer(n_classes,name = 'roi-data'))\n\n\n        #========= RCNN ============ \n        (self.feed('res4b22_relu','roi-data')\n             .roi_pool(7,7,1.0/16,name='res5a_branch2a_roipooling')\n             .conv(1, 1, 512, 2, 2, biased=False, relu=False, name='res5a_branch2a',padding='VALID')\n             .batch_normalization(relu=True, name='bn5a_branch2a',is_training=False)\n             .conv(3, 3, 512, 1, 1, biased=False, relu=False, name='res5a_branch2b')\n             .batch_normalization(relu=True, name='bn5a_branch2b',is_training=False)\n             .conv(1, 1, 2048, 1, 1, biased=False, relu=False, name='res5a_branch2c')\n             .batch_normalization(name='bn5a_branch2c',is_training=False,relu=False))\n\n        (self.feed('res5a_branch2a_roipooling')\n             .conv(1, 1, 2048, 2, 2, biased=False, relu=False, name='res5a_branch1',padding='VALID')\n             .batch_normalization(name='bn5a_branch1',is_training=False,relu=False))\n\n\n        (self.feed('bn5a_branch1', \n                   'bn5a_branch2c')\n             .add(name='res5a')\n             .relu(name='res5a_relu')\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res5b_branch2a')\n             .batch_normalization(relu=True, name='bn5b_branch2a',is_training=False)\n             .conv(3, 3, 512, 1, 1, biased=False, relu=False, name='res5b_branch2b')\n             .batch_normalization(relu=True, name='bn5b_branch2b',is_training=False)\n             .conv(1, 1, 2048, 1, 1, biased=False, relu=False, name='res5b_branch2c')\n             .batch_normalization(name='bn5b_branch2c',is_training=False,relu=False))\n\n        (self.feed('res5a_relu', \n                   'bn5b_branch2c')\n             .add(name='res5b')\n             .relu(name='res5b_relu')\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res5c_branch2a')\n             .batch_normalization(relu=True, name='bn5c_branch2a',is_training=False)\n             .conv(3, 3, 512, 1, 1, biased=False, relu=False, name='res5c_branch2b')\n             .batch_normalization(relu=True, name='bn5c_branch2b',is_training=False)\n             .conv(1, 1, 2048, 1, 1, biased=False, relu=False, name='res5c_branch2c')\n             .batch_normalization(name='bn5c_branch2c',is_training=False,relu=False))\n\n        (self.feed('res5b_relu', \n                   'bn5c_branch2c')\n             .add(name='res5c')\n             .relu(name='res5c_relu')\n             .avg_pool(4, 4, 1, 1, padding='VALID', name='pool5')\n             .fc(1000, relu=False, name='fc1000')\n             .softmax(name='prob'))"""
lib/networks/Resnet50_test.py,3,"b""# --------------------------------------------------------\n# TFFRCNN - Resnet50\n# Copyright (c) 2016\n# Licensed under The MIT License [see LICENSE for details]\n# Written by miraclebiu\n# --------------------------------------------------------\nimport tensorflow as tf\nfrom .network import Network\nfrom ..fast_rcnn.config import cfg\n\n\nclass Resnet50_test(Network):\n    def __init__(self, trainable=True):\n        self.inputs = []\n        self.data = tf.placeholder(tf.float32, shape=[None, None, None, 3])\n        self.im_info = tf.placeholder(tf.float32, shape=[None, 3])\n        self.keep_prob = tf.placeholder(tf.float32)\n        self.layers = dict({'data':self.data, 'im_info':self.im_info})\n        self.trainable = trainable\n        self.setup()\n\n    def setup(self):\n        n_classes = cfg.NCLASSES\n        anchor_scales = cfg.ANCHOR_SCALES\n        _feat_stride = [16, ]\n\n\n        (self.feed('data')\n             .conv(7, 7, 64, 2, 2, relu=False, name='conv1')\n             .batch_normalization(relu=True, name='bn_conv1',is_training=False)\n             .max_pool(3, 3, 2, 2, padding='VALID',name='pool1')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res2a_branch1')\n             .batch_normalization(name='bn2a_branch1',is_training=False,relu=False))\n\n        (self.feed('pool1')\n             .conv(1, 1, 64, 1, 1, biased=False, relu=False, name='res2a_branch2a')\n             .batch_normalization(relu=True, name='bn2a_branch2a',is_training=False)\n             .conv(3, 3, 64, 1, 1, biased=False, relu=False, name='res2a_branch2b')\n             .batch_normalization(relu=True, name='bn2a_branch2b',is_training=False)\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res2a_branch2c')\n             .batch_normalization(name='bn2a_branch2c',is_training=False,relu=False))\n\n        (self.feed('bn2a_branch1', \n                   'bn2a_branch2c')\n             .add(name='res2a')\n             .relu(name='res2a_relu')\n             .conv(1, 1, 64, 1, 1, biased=False, relu=False, name='res2b_branch2a')\n             .batch_normalization(relu=True, name='bn2b_branch2a',is_training=False)\n             .conv(3, 3, 64, 1, 1, biased=False, relu=False, name='res2b_branch2b')\n             .batch_normalization(relu=True, name='bn2b_branch2b',is_training=False)\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res2b_branch2c')\n             .batch_normalization(name='bn2b_branch2c',is_training=False,relu=False))\n\n        (self.feed('res2a_relu', \n                   'bn2b_branch2c')\n             .add(name='res2b')\n             .relu(name='res2b_relu')\n             .conv(1, 1, 64, 1, 1, biased=False, relu=False, name='res2c_branch2a')\n             .batch_normalization(relu=True, name='bn2c_branch2a',is_training=False)\n             .conv(3, 3, 64, 1, 1, biased=False, relu=False, name='res2c_branch2b')\n             .batch_normalization(relu=True, name='bn2c_branch2b',is_training=False)\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res2c_branch2c')\n             .batch_normalization(name='bn2c_branch2c',is_training=False,relu=False))\n\n        (self.feed('res2b_relu', \n                   'bn2c_branch2c')\n             .add(name='res2c')\n             .relu(name='res2c_relu')\n             .conv(1, 1, 512, 2, 2, biased=False, relu=False, name='res3a_branch1', padding='VALID')\n             .batch_normalization(name='bn3a_branch1',is_training=False,relu=False))\n\n        (self.feed('res2c_relu')\n             .conv(1, 1, 128, 2, 2, biased=False, relu=False, name='res3a_branch2a', padding='VALID')\n             .batch_normalization(relu=True, name='bn3a_branch2a',is_training=False)\n             .conv(3, 3, 128, 1, 1, biased=False, relu=False, name='res3a_branch2b')\n             .batch_normalization(relu=True, name='bn3a_branch2b',is_training=False)\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res3a_branch2c')\n             .batch_normalization(name='bn3a_branch2c',is_training=False,relu=False))\n\n        (self.feed('bn3a_branch1', \n                   'bn3a_branch2c')\n             .add(name='res3a')\n             .relu(name='res3a_relu')\n             .conv(1, 1, 128, 1, 1, biased=False, relu=False, name='res3b_branch2a')\n             .batch_normalization(relu=True, name='bn3b_branch2a',is_training=False)\n             .conv(3, 3, 128, 1, 1, biased=False, relu=False, name='res3b_branch2b')\n             .batch_normalization(relu=True, name='bn3b_branch2b',is_training=False)\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res3b_branch2c')\n             .batch_normalization(name='bn3b_branch2c',is_training=False,relu=False))\n\n        (self.feed('res3a_relu', \n                   'bn3b_branch2c')\n             .add(name='res3b')\n             .relu(name='res3b_relu')\n             .conv(1, 1, 128, 1, 1, biased=False, relu=False, name='res3c_branch2a')\n             .batch_normalization(relu=True, name='bn3c_branch2a',is_training=False)\n             .conv(3, 3, 128, 1, 1, biased=False, relu=False, name='res3c_branch2b')\n             .batch_normalization(relu=True, name='bn3c_branch2b',is_training=False)\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res3c_branch2c')\n             .batch_normalization(name='bn3c_branch2c',is_training=False,relu=False))\n\n        (self.feed('res3b_relu', \n                   'bn3c_branch2c')\n             .add(name='res3c')\n             .relu(name='res3c_relu')\n             .conv(1, 1, 128, 1, 1, biased=False, relu=False, name='res3d_branch2a')\n             .batch_normalization(relu=True, name='bn3d_branch2a',is_training=False)\n             .conv(3, 3, 128, 1, 1, biased=False, relu=False, name='res3d_branch2b')\n             .batch_normalization(relu=True, name='bn3d_branch2b',is_training=False)\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res3d_branch2c')\n             .batch_normalization(name='bn3d_branch2c',is_training=False,relu=False))\n\n        (self.feed('res3c_relu', \n                   'bn3d_branch2c')\n             .add(name='res3d')\n             .relu(name='res3d_relu')\n             .conv(1, 1, 1024, 2, 2, biased=False, relu=False, name='res4a_branch1', padding='VALID')\n             .batch_normalization(name='bn4a_branch1',is_training=False,relu=False))\n\n        (self.feed('res3d_relu')\n             .conv(1, 1, 256, 2, 2, biased=False, relu=False, name='res4a_branch2a', padding='VALID')\n             .batch_normalization(relu=True, name='bn4a_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4a_branch2b')\n             .batch_normalization(relu=True, name='bn4a_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4a_branch2c')\n             .batch_normalization(name='bn4a_branch2c',is_training=False,relu=False))\n\n        (self.feed('bn4a_branch1', \n                   'bn4a_branch2c')\n             .add(name='res4a')\n             .relu(name='res4a_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b_branch2a')\n             .batch_normalization(relu=True, name='bn4b_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b_branch2b')\n             .batch_normalization(relu=True, name='bn4b_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b_branch2c')\n             .batch_normalization(name='bn4b_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4a_relu', \n                   'bn4b_branch2c')\n             .add(name='res4b')\n             .relu(name='res4b_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4c_branch2a')\n             .batch_normalization(relu=True, name='bn4c_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4c_branch2b')\n             .batch_normalization(relu=True, name='bn4c_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4c_branch2c')\n             .batch_normalization(name='bn4c_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b_relu', \n                   'bn4c_branch2c')\n             .add(name='res4c')\n             .relu(name='res4c_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4d_branch2a')\n             .batch_normalization(relu=True, name='bn4d_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4d_branch2b')\n             .batch_normalization(relu=True, name='bn4d_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4d_branch2c')\n             .batch_normalization(name='bn4d_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4c_relu', \n                   'bn4d_branch2c')\n             .add(name='res4d')\n             .relu(name='res4d_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4e_branch2a')\n             .batch_normalization(relu=True, name='bn4e_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4e_branch2b')\n             .batch_normalization(relu=True, name='bn4e_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4e_branch2c')\n             .batch_normalization(name='bn4e_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4d_relu', \n                   'bn4e_branch2c')\n             .add(name='res4e')\n             .relu(name='res4e_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4f_branch2a')\n             .batch_normalization(relu=True, name='bn4f_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4f_branch2b')\n             .batch_normalization(relu=True, name='bn4f_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4f_branch2c')\n             .batch_normalization(name='bn4f_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4e_relu', \n                   'bn4f_branch2c')\n             .add(name='res4f')\n             .relu(name='res4f_relu'))\n\n        \n\n        #========= RPN ============\n        (self.feed('res4f_relu')\n             .conv(3,3,512,1,1,name='rpn_conv/3x3')\n             .conv(1,1,len(anchor_scales)*3*2 ,1 , 1, padding='VALID', relu = False, name='rpn_cls_score'))\n\n\n        (self.feed('rpn_conv/3x3')\n             .conv(1,1,len(anchor_scales)*3*4, 1, 1, padding='VALID', relu = False, name='rpn_bbox_pred'))\n\n        #========= RoI Proposal ============\n        (self.feed('rpn_cls_score')\n             .spatial_reshape_layer(2,name = 'rpn_cls_score_reshape')\n             .spatial_softmax(name='rpn_cls_prob'))\n\n        (self.feed('rpn_cls_prob')\n             .spatial_reshape_layer(len(anchor_scales)*3*2,name = 'rpn_cls_prob_reshape'))\n\n        (self.feed('rpn_cls_prob_reshape','rpn_bbox_pred','im_info')\n             .proposal_layer(_feat_stride, anchor_scales, 'TEST',name = 'rois'))\n\n        #========= RCNN ============        \n        (self.feed('res4f_relu','rois')\n        \t .roi_pool(7,7,1.0/16,name='res5a_branch2a_roipooling')\n             .conv(1, 1, 512, 2, 2, biased=False, relu=False, name='res5a_branch2a', padding='VALID')\n             .batch_normalization(relu=True, name='bn5a_branch2a',is_training=False)\n             .conv(3, 3, 512, 1, 1, biased=False, relu=False, name='res5a_branch2b')\n             .batch_normalization(relu=True, name='bn5a_branch2b',is_training=False)\n             .conv(1, 1, 2048, 1, 1, biased=False, relu=False, name='res5a_branch2c')\n             .batch_normalization(name='bn5a_branch2c',is_training=False,relu=False))\n\n\n        (self.feed('res5a_branch2a_roipooling')\n             .conv(1,1,2048,2,2,biased=False, relu=False, name='res5a_branch1', padding='VALID')\n             .batch_normalization(name='bn5a_branch1',is_training=False,relu=False))\n\n        (self.feed('bn5a_branch2c','bn5a_branch1')\n             .add(name='res5a')\n             .relu(name='res5a_relu')\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res5b_branch2a')\n             .batch_normalization(relu=True, name='bn5b_branch2a',is_training=False)\n             .conv(3, 3, 512, 1, 1, biased=False, relu=False, name='res5b_branch2b')\n             .batch_normalization(relu=True, name='bn5b_branch2b',is_training=False)\n             .conv(1, 1, 2048, 1, 1, biased=False, relu=False, name='res5b_branch2c')\n             .batch_normalization(name='bn5b_branch2c',is_training=False,relu=False))\n\n        (self.feed('res5a_relu', \n                   'bn5b_branch2c')\n             .add(name='res5b')\n             .relu(name='res5b_relu')\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res5c_branch2a')\n             .batch_normalization(relu=True, name='bn5c_branch2a',is_training=False)\n             .conv(3, 3, 512, 1, 1, biased=False, relu=False, name='res5c_branch2b')\n             .batch_normalization(relu=True, name='bn5c_branch2b',is_training=False)\n             .conv(1, 1, 2048, 1, 1, biased=False, relu=False, name='res5c_branch2c')\n             .batch_normalization(name='bn5c_branch2c',is_training=False,relu=False))\n\n        (self.feed('res5b_relu', \n                   'bn5c_branch2c')\n             .add(name='res5c')\n             .relu(name='res5c_relu')\n             .fc(n_classes, relu=False, name='cls_score')\n             .softmax(name='cls_prob'))\n\n\n        (self.feed('res5c_relu')\n             .fc(n_classes*4, relu=False, name='bbox_pred'))"""
lib/networks/Resnet50_train.py,6,"b""# --------------------------------------------------------\n# TFFRCNN - Resnet50\n# Copyright (c) 2016\n# Licensed under The MIT License [see LICENSE for details]\n# Written by miraclebiu\n# --------------------------------------------------------\n\nimport tensorflow as tf\nfrom .network import Network\nfrom ..fast_rcnn.config import cfg\n\n\nclass Resnet50_train(Network):\n    def __init__(self, trainable=True):\n        self.inputs = []\n        self.data = tf.placeholder(tf.float32, shape=[None, None, None, 3], name='data')\n        self.im_info = tf.placeholder(tf.float32, shape=[None, 3], name='im_info')\n        self.gt_boxes = tf.placeholder(tf.float32, shape=[None, 5], name='gt_boxes')\n        self.gt_ishard = tf.placeholder(tf.int32, shape=[None], name='gt_ishard')\n        self.dontcare_areas = tf.placeholder(tf.float32, shape=[None, 4], name='dontcare_areas')\n        self.keep_prob = tf.placeholder(tf.float32)\n        self.layers = dict({'data':self.data, 'im_info':self.im_info, 'gt_boxes':self.gt_boxes,\\\n                            'gt_ishard': self.gt_ishard, 'dontcare_areas': self.dontcare_areas})\n        self.trainable = trainable\n        self.setup()\n\n    def setup(self):\n\n        n_classes = cfg.NCLASSES\n        # anchor_scales = [8, 16, 32]\n        anchor_scales = cfg.ANCHOR_SCALES\n        _feat_stride = [16, ]\n        (self.feed('data')\n             .conv(7, 7, 64, 2, 2, relu=False, name='conv1')\n             .batch_normalization(relu=True, name='bn_conv1', is_training=False)\n             .max_pool(3, 3, 2, 2, padding='VALID',name='pool1')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res2a_branch1')\n             .batch_normalization(name='bn2a_branch1',is_training=False,relu=False))\n\n        (self.feed('pool1')\n             .conv(1, 1, 64, 1, 1, biased=False, relu=False, name='res2a_branch2a')\n             .batch_normalization(relu=True, name='bn2a_branch2a',is_training=False)\n             .conv(3, 3, 64, 1, 1, biased=False, relu=False, name='res2a_branch2b')\n             .batch_normalization(relu=True, name='bn2a_branch2b',is_training=False)\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res2a_branch2c')\n             .batch_normalization(name='bn2a_branch2c',is_training=False,relu=False))\n\n        (self.feed('bn2a_branch1', \n                   'bn2a_branch2c')\n             .add(name='res2a')\n             .relu(name='res2a_relu')\n             .conv(1, 1, 64, 1, 1, biased=False, relu=False, name='res2b_branch2a')\n             .batch_normalization(relu=True, name='bn2b_branch2a',is_training=False)\n             .conv(3, 3, 64, 1, 1, biased=False, relu=False, name='res2b_branch2b')\n             .batch_normalization(relu=True, name='bn2b_branch2b',is_training=False)\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res2b_branch2c')\n             .batch_normalization(name='bn2b_branch2c',is_training=False,relu=False))\n\n        (self.feed('res2a_relu', \n                   'bn2b_branch2c')\n             .add(name='res2b')\n             .relu(name='res2b_relu')\n             .conv(1, 1, 64, 1, 1, biased=False, relu=False, name='res2c_branch2a')\n             .batch_normalization(relu=True, name='bn2c_branch2a',is_training=False)\n             .conv(3, 3, 64, 1, 1, biased=False, relu=False, name='res2c_branch2b')\n             .batch_normalization(relu=True, name='bn2c_branch2b',is_training=False)\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res2c_branch2c')\n             .batch_normalization(name='bn2c_branch2c',is_training=False,relu=False))\n\n        (self.feed('res2b_relu', \n                   'bn2c_branch2c')\n             .add(name='res2c')\n             .relu(name='res2c_relu')\n             .conv(1, 1, 512, 2, 2, biased=False, relu=False, name='res3a_branch1', padding='VALID')\n             .batch_normalization(name='bn3a_branch1',is_training=False,relu=False))\n\n        (self.feed('res2c_relu')\n             .conv(1, 1, 128, 2, 2, biased=False, relu=False, name='res3a_branch2a', padding='VALID')\n             .batch_normalization(relu=True, name='bn3a_branch2a',is_training=False)\n             .conv(3, 3, 128, 1, 1, biased=False, relu=False, name='res3a_branch2b')\n             .batch_normalization(relu=True, name='bn3a_branch2b',is_training=False)\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res3a_branch2c')\n             .batch_normalization(name='bn3a_branch2c',is_training=False,relu=False))\n\n        (self.feed('bn3a_branch1', \n                   'bn3a_branch2c')\n             .add(name='res3a')\n             .relu(name='res3a_relu')\n             .conv(1, 1, 128, 1, 1, biased=False, relu=False, name='res3b_branch2a')\n             .batch_normalization(relu=True, name='bn3b_branch2a',is_training=False)\n             .conv(3, 3, 128, 1, 1, biased=False, relu=False, name='res3b_branch2b')\n             .batch_normalization(relu=True, name='bn3b_branch2b',is_training=False)\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res3b_branch2c')\n             .batch_normalization(name='bn3b_branch2c',is_training=False,relu=False))\n\n        (self.feed('res3a_relu', \n                   'bn3b_branch2c')\n             .add(name='res3b')\n             .relu(name='res3b_relu')\n             .conv(1, 1, 128, 1, 1, biased=False, relu=False, name='res3c_branch2a')\n             .batch_normalization(relu=True, name='bn3c_branch2a',is_training=False)\n             .conv(3, 3, 128, 1, 1, biased=False, relu=False, name='res3c_branch2b')\n             .batch_normalization(relu=True, name='bn3c_branch2b',is_training=False)\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res3c_branch2c')\n             .batch_normalization(name='bn3c_branch2c',is_training=False,relu=False))\n\n        (self.feed('res3b_relu', \n                   'bn3c_branch2c')\n             .add(name='res3c')\n             .relu(name='res3c_relu')\n             .conv(1, 1, 128, 1, 1, biased=False, relu=False, name='res3d_branch2a')\n             .batch_normalization(relu=True, name='bn3d_branch2a',is_training=False)\n             .conv(3, 3, 128, 1, 1, biased=False, relu=False, name='res3d_branch2b')\n             .batch_normalization(relu=True, name='bn3d_branch2b',is_training=False)\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res3d_branch2c')\n             .batch_normalization(name='bn3d_branch2c',is_training=False,relu=False))\n\n        (self.feed('res3c_relu', \n                   'bn3d_branch2c')\n             .add(name='res3d')\n             .relu(name='res3d_relu')\n             .conv(1, 1, 1024, 2, 2, biased=False, relu=False, name='res4a_branch1', padding='VALID')\n             .batch_normalization(name='bn4a_branch1',is_training=False,relu=False))\n\n        (self.feed('res3d_relu')\n             .conv(1, 1, 256, 2, 2, biased=False, relu=False, name='res4a_branch2a', padding='VALID')\n             .batch_normalization(relu=True, name='bn4a_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4a_branch2b')\n             .batch_normalization(relu=True, name='bn4a_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4a_branch2c')\n             .batch_normalization(name='bn4a_branch2c',is_training=False,relu=False))\n\n        (self.feed('bn4a_branch1', \n                   'bn4a_branch2c')\n             .add(name='res4a')\n             .relu(name='res4a_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b_branch2a')\n             .batch_normalization(relu=True, name='bn4b_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b_branch2b')\n             .batch_normalization(relu=True, name='bn4b_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b_branch2c')\n             .batch_normalization(name='bn4b_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4a_relu', \n                   'bn4b_branch2c')\n             .add(name='res4b')\n             .relu(name='res4b_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4c_branch2a')\n             .batch_normalization(relu=True, name='bn4c_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4c_branch2b')\n             .batch_normalization(relu=True, name='bn4c_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4c_branch2c')\n             .batch_normalization(name='bn4c_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4b_relu', \n                   'bn4c_branch2c')\n             .add(name='res4c')\n             .relu(name='res4c_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4d_branch2a')\n             .batch_normalization(relu=True, name='bn4d_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4d_branch2b')\n             .batch_normalization(relu=True, name='bn4d_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4d_branch2c')\n             .batch_normalization(name='bn4d_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4c_relu', \n                   'bn4d_branch2c')\n             .add(name='res4d')\n             .relu(name='res4d_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4e_branch2a')\n             .batch_normalization(relu=True, name='bn4e_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4e_branch2b')\n             .batch_normalization(relu=True, name='bn4e_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4e_branch2c')\n             .batch_normalization(name='bn4e_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4d_relu', \n                   'bn4e_branch2c')\n             .add(name='res4e')\n             .relu(name='res4e_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4f_branch2a')\n             .batch_normalization(relu=True, name='bn4f_branch2a',is_training=False)\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4f_branch2b')\n             .batch_normalization(relu=True, name='bn4f_branch2b',is_training=False)\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4f_branch2c')\n             .batch_normalization(name='bn4f_branch2c',is_training=False,relu=False))\n\n        (self.feed('res4e_relu', \n                   'bn4f_branch2c')\n             .add(name='res4f')\n             .relu(name='res4f_relu'))\n\n        \n\n        #========= RPN ============\n        (self.feed('res4f_relu')\n             .conv(3,3,512,1,1,name='rpn_conv/3x3')\n             .conv(1,1,len(anchor_scales)*3*2 ,1 , 1, padding='VALID', relu = False, name='rpn_cls_score'))\n  \n        (self.feed('rpn_cls_score', 'gt_boxes', 'gt_ishard', 'dontcare_areas', 'im_info')\n             .anchor_target_layer(_feat_stride, anchor_scales, name = 'rpn-data' ))\n        # Loss of rpn_cls & rpn_boxes\n\n        (self.feed('rpn_conv/3x3')\n             .conv(1,1,len(anchor_scales)*3*4, 1, 1, padding='VALID', relu = False, name='rpn_bbox_pred'))\n\n        #========= RoI Proposal ============\n        (self.feed('rpn_cls_score')\n             .spatial_reshape_layer(2, name = 'rpn_cls_score_reshape')\n             .spatial_softmax(name='rpn_cls_prob'))\n\n        (self.feed('rpn_cls_prob')\n             .spatial_reshape_layer(len(anchor_scales)*3*2, name = 'rpn_cls_prob_reshape'))\n\n        (self.feed('rpn_cls_prob_reshape','rpn_bbox_pred','im_info')\n             .proposal_layer(_feat_stride, anchor_scales, 'TRAIN',name = 'rpn_rois'))\n\n        (self.feed('rpn_rois','gt_boxes', 'gt_ishard', 'dontcare_areas')\n             .proposal_target_layer(n_classes,name = 'roi-data'))\n\n        #========= RCNN ============        \n        (self.feed('res4f_relu','roi-data')\n             .roi_pool(7,7,1.0/16,name='res5a_branch2a_roipooling')\n             .conv(1, 1, 512, 2, 2, biased=False, relu=False, name='res5a_branch2a', padding='VALID')\n             .batch_normalization(relu=True, name='bn5a_branch2a',is_training=False)\n             .conv(3, 3, 512, 1, 1, biased=False, relu=False, name='res5a_branch2b')\n             .batch_normalization(relu=True, name='bn5a_branch2b',is_training=False)\n             .conv(1, 1, 2048, 1, 1, biased=False, relu=False, name='res5a_branch2c')\n             .batch_normalization(name='bn5a_branch2c',is_training=False,relu=False))\n\n        (self.feed('res5a_branch2a_roipooling')\n             .conv(1,1,2048,2,2,biased=False, relu=False, name='res5a_branch1', padding='VALID')\n             .batch_normalization(name='bn5a_branch1',is_training=False,relu=False))\n\n        (self.feed('bn5a_branch2c','bn5a_branch1')\n             .add(name='res5a')\n             .relu(name='res5a_relu')\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res5b_branch2a')\n             .batch_normalization(relu=True, name='bn5b_branch2a',is_training=False)\n             .conv(3, 3, 512, 1, 1, biased=False, relu=False, name='res5b_branch2b')\n             .batch_normalization(relu=True, name='bn5b_branch2b',is_training=False)\n             .conv(1, 1, 2048, 1, 1, biased=False, relu=False, name='res5b_branch2c')\n             .batch_normalization(name='bn5b_branch2c',is_training=False,relu=False))\n        #pdb.set_trace()\n        (self.feed('res5a_relu', \n                   'bn5b_branch2c')\n             .add(name='res5b')\n             .relu(name='res5b_relu')\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res5c_branch2a')\n             .batch_normalization(relu=True, name='bn5c_branch2a',is_training=False)\n             .conv(3, 3, 512, 1, 1, biased=False, relu=False, name='res5c_branch2b')\n             .batch_normalization(relu=True, name='bn5c_branch2b',is_training=False)\n             .conv(1, 1, 2048, 1, 1, biased=False, relu=False, name='res5c_branch2c')\n             .batch_normalization(name='bn5c_branch2c',is_training=False,relu=False))\n        #pdb.set_trace()\n        (self.feed('res5b_relu',\n        \t       'bn5c_branch2c')\n             .add(name='res5c')\n             .relu(name='res5c_relu')\n             .fc(n_classes, relu=False, name='cls_score')\n             .softmax(name='cls_prob'))\n\n        (self.feed('res5c_relu')\n             .fc(n_classes*4, relu=False, name='bbox_pred'))\n"""
lib/networks/VGGnet_test.py,3,"b""import tensorflow as tf\nfrom .network import Network\nfrom ..fast_rcnn.config import cfg\n\n\nclass VGGnet_test(Network):\n    def __init__(self, trainable=True):\n        self.inputs = []\n        self.data = tf.placeholder(tf.float32, shape=[None, None, None, 3])\n        self.im_info = tf.placeholder(tf.float32, shape=[None, 3])\n        self.keep_prob = tf.placeholder(tf.float32)\n        self.layers = dict({'data': self.data, 'im_info': self.im_info})\n        self.trainable = trainable\n        self.setup()\n\n    def setup(self):\n        # n_classes = 21\n        n_classes = cfg.NCLASSES\n        # anchor_scales = [8, 16, 32]\n        anchor_scales = cfg.ANCHOR_SCALES\n        _feat_stride = [16, ]\n\n        (self.feed('data')\n         .conv(3, 3, 64, 1, 1, name='conv1_1', trainable=False)\n         .conv(3, 3, 64, 1, 1, name='conv1_2', trainable=False)\n         .max_pool(2, 2, 2, 2, padding='VALID', name='pool1')\n         .conv(3, 3, 128, 1, 1, name='conv2_1', trainable=False)\n         .conv(3, 3, 128, 1, 1, name='conv2_2', trainable=False)\n         .max_pool(2, 2, 2, 2, padding='VALID', name='pool2')\n         .conv(3, 3, 256, 1, 1, name='conv3_1')\n         .conv(3, 3, 256, 1, 1, name='conv3_2')\n         .conv(3, 3, 256, 1, 1, name='conv3_3')\n         .max_pool(2, 2, 2, 2, padding='VALID', name='pool3')\n         .conv(3, 3, 512, 1, 1, name='conv4_1')\n         .conv(3, 3, 512, 1, 1, name='conv4_2')\n         .conv(3, 3, 512, 1, 1, name='conv4_3')\n         .max_pool(2, 2, 2, 2, padding='VALID', name='pool4')\n         .conv(3, 3, 512, 1, 1, name='conv5_1')\n         .conv(3, 3, 512, 1, 1, name='conv5_2')\n         .conv(3, 3, 512, 1, 1, name='conv5_3'))\n\n        (self.feed('conv5_3')\n         .conv(3, 3, 512, 1, 1, name='rpn_conv/3x3')\n         .conv(1, 1, len(anchor_scales) * 3 * 2, 1, 1, padding='VALID', relu=False, name='rpn_cls_score'))\n\n        (self.feed('rpn_conv/3x3')\n         .conv(1, 1, len(anchor_scales) * 3 * 4, 1, 1, padding='VALID', relu=False, name='rpn_bbox_pred'))\n\n        #  shape is (1, H, W, Ax2) -> (1, H, WxA, 2)\n        (self.feed('rpn_cls_score')\n         .spatial_reshape_layer(2, name='rpn_cls_score_reshape')\n         .spatial_softmax(name='rpn_cls_prob'))\n\n        # shape is (1, H, WxA, 2) -> (1, H, W, Ax2)\n        (self.feed('rpn_cls_prob')\n         .spatial_reshape_layer(len(anchor_scales) * 3 * 2, name='rpn_cls_prob_reshape'))\n\n        (self.feed('rpn_cls_prob_reshape', 'rpn_bbox_pred', 'im_info')\n         .proposal_layer(_feat_stride, anchor_scales, 'TEST', name='rois'))\n\n        (self.feed('conv5_3', 'rois')\n         .roi_pool(7, 7, 1.0 / 16, name='pool_5')\n         .fc(4096, name='fc6')\n         .fc(4096, name='fc7')\n         .fc(n_classes, relu=False, name='cls_score')\n         .softmax(name='cls_prob'))\n\n        (self.feed('fc7')\n         .fc(n_classes * 4, relu=False, name='bbox_pred'))\n\n"""
lib/networks/VGGnet_testold.py,3,"b""import tensorflow as tf\nfrom .network import Network\nfrom ..fast_rcnn.config import cfg\n\nclass VGGnet_testold(Network):\n    def __init__(self, trainable=True):\n        self.inputs = []\n        self.data = tf.placeholder(tf.float32, shape=[None, None, None, 3])\n        self.im_info = tf.placeholder(tf.float32, shape=[None, 3])\n        self.keep_prob = tf.placeholder(tf.float32)\n        self.layers = dict({'data':self.data, 'im_info':self.im_info})\n        self.trainable = trainable\n        self.setup()\n\n    def setup(self):\n        # n_classes = 21\n        n_classes = cfg.NCLASSES\n        # anchor_scales = [8, 16, 32]\n        anchor_scales = cfg.ANCHOR_SCALES\n        _feat_stride = [16, ]\n\n        (self.feed('data')\n             .conv(3, 3, 64, 1, 1, name='conv1_1', trainable=False)\n             .conv(3, 3, 64, 1, 1, name='conv1_2', trainable=False)\n             .max_pool(2, 2, 2, 2, padding='VALID', name='pool1')\n             .conv(3, 3, 128, 1, 1, name='conv2_1', trainable=False)\n             .conv(3, 3, 128, 1, 1, name='conv2_2', trainable=False)\n             .max_pool(2, 2, 2, 2, padding='VALID', name='pool2')\n             .conv(3, 3, 256, 1, 1, name='conv3_1')\n             .conv(3, 3, 256, 1, 1, name='conv3_2')\n             .conv(3, 3, 256, 1, 1, name='conv3_3')\n             .max_pool(2, 2, 2, 2, padding='VALID', name='pool3')\n             .conv(3, 3, 512, 1, 1, name='conv4_1')\n             .conv(3, 3, 512, 1, 1, name='conv4_2')\n             .conv(3, 3, 512, 1, 1, name='conv4_3')\n             .max_pool(2, 2, 2, 2, padding='VALID', name='pool4')\n             .conv(3, 3, 512, 1, 1, name='conv5_1')\n             .conv(3, 3, 512, 1, 1, name='conv5_2')\n             .conv(3, 3, 512, 1, 1, name='conv5_3'))\n\n        (self.feed('conv5_3')\n             .conv(3,3,512,1,1,name='rpn_conv/3x3')\n             .conv(1,1,len(anchor_scales)*3*2,1,1,padding='VALID',relu = False,name='rpn_cls_score')) # (1, H, W, A x 2)\n\n        (self.feed('rpn_conv/3x3')\n             .conv(1,1,len(anchor_scales)*3*4,1,1,padding='VALID',relu = False,name='rpn_bbox_pred'))\n\n        (self.feed('rpn_cls_score')\n             .reshape_layer(2,name = 'rpn_cls_score_reshape') # (1, AxH, W, 2)\n             .softmax(name='rpn_cls_prob')) # (1, AxH, W, 2)\n\n        (self.feed('rpn_cls_prob')\n             .reshape_layer(len(anchor_scales)*3*2,name = 'rpn_cls_prob_reshape')) # (1, H, W, 2xA)!!\n\n        (self.feed('rpn_cls_prob_reshape','rpn_bbox_pred','im_info')\n             .proposal_layer(_feat_stride, anchor_scales, 'TEST', name = 'rois'))\n        \n        (self.feed('conv5_3', 'rois')\n             .roi_pool(7, 7, 1.0/16, name='pool_5')\n             .fc(4096, name='fc6')\n             .fc(4096, name='fc7')\n             .fc(n_classes, relu=False, name='cls_score')\n             .softmax(name='cls_prob'))\n\n        (self.feed('fc7')\n             .fc(n_classes*4, relu=False, name='bbox_pred'))\n\n"""
lib/networks/VGGnet_train.py,6,"b""import tensorflow as tf\nfrom network import Network\nfrom ..fast_rcnn.config import cfg\n\nclass VGGnet_train(Network):\n    def __init__(self, trainable=True):\n        self.inputs = []\n        self.data = tf.placeholder(tf.float32, shape=[None, None, None, 3], name='data')\n        self.im_info = tf.placeholder(tf.float32, shape=[None, 3], name='im_info')\n        self.gt_boxes = tf.placeholder(tf.float32, shape=[None, 5], name='gt_boxes')\n        self.gt_ishard = tf.placeholder(tf.int32, shape=[None], name='gt_ishard')\n        self.dontcare_areas = tf.placeholder(tf.float32, shape=[None, 4], name='dontcare_areas')\n        self.keep_prob = tf.placeholder(tf.float32)\n        self.layers = dict({'data':self.data, 'im_info':self.im_info, 'gt_boxes':self.gt_boxes,\\\n                            'gt_ishard': self.gt_ishard, 'dontcare_areas': self.dontcare_areas})\n        self.trainable = trainable\n        self.setup()\n\n    def setup(self):\n\n        # n_classes = 21\n        n_classes = cfg.NCLASSES\n        # anchor_scales = [8, 16, 32]\n        anchor_scales = cfg.ANCHOR_SCALES\n        _feat_stride = [16, ]\n\n        (self.feed('data')\n             .conv(3, 3, 64, 1, 1, name='conv1_1', trainable=False)\n             .conv(3, 3, 64, 1, 1, name='conv1_2', trainable=False)\n             .max_pool(2, 2, 2, 2, padding='VALID', name='pool1')\n             .conv(3, 3, 128, 1, 1, name='conv2_1', trainable=False)\n             .conv(3, 3, 128, 1, 1, name='conv2_2', trainable=False)\n             .max_pool(2, 2, 2, 2, padding='VALID', name='pool2')\n             .conv(3, 3, 256, 1, 1, name='conv3_1')\n             .conv(3, 3, 256, 1, 1, name='conv3_2')\n             .conv(3, 3, 256, 1, 1, name='conv3_3')\n             .max_pool(2, 2, 2, 2, padding='VALID', name='pool3')\n             .conv(3, 3, 512, 1, 1, name='conv4_1')\n             .conv(3, 3, 512, 1, 1, name='conv4_2')\n             .conv(3, 3, 512, 1, 1, name='conv4_3')\n             .max_pool(2, 2, 2, 2, padding='VALID', name='pool4')\n             .conv(3, 3, 512, 1, 1, name='conv5_1')\n             .conv(3, 3, 512, 1, 1, name='conv5_2')\n             .conv(3, 3, 512, 1, 1, name='conv5_3'))\n        #========= RPN ============\n        (self.feed('conv5_3')\n             .conv(3,3,512,1,1,name='rpn_conv/3x3'))\n\n        # Loss of rpn_cls & rpn_boxes\n        # shape is (1, H, W, A x 4) and (1, H, W, A x 2)\n        (self.feed('rpn_conv/3x3')\n             .conv(1,1,len(anchor_scales) * 3 * 4, 1, 1, padding='VALID', relu = False, name='rpn_bbox_pred'))\n        (self.feed('rpn_conv/3x3')\n             .conv(1, 1, len(anchor_scales) * 3 * 2, 1, 1, padding='VALID', relu=False, name='rpn_cls_score'))\n\n        # generating training labels on the fly\n        # output: rpn_labels(HxWxA, 2) rpn_bbox_targets(HxWxA, 4) rpn_bbox_inside_weights rpn_bbox_outside_weights\n        (self.feed('rpn_cls_score', 'gt_boxes', 'gt_ishard', 'dontcare_areas', 'im_info')\n             .anchor_target_layer(_feat_stride, anchor_scales, name = 'rpn-data' ))\n\n        # shape is (1, H, W, Ax2) -> (1, H, WxA, 2)\n        (self.feed('rpn_cls_score')\n             .spatial_reshape_layer(2, name = 'rpn_cls_score_reshape')\n             .spatial_softmax(name='rpn_cls_prob'))\n\n        # shape is (1, H, WxA, 2) -> (1, H, W, Ax2)\n        (self.feed('rpn_cls_prob')\n             .spatial_reshape_layer(len(anchor_scales)*3*2, name = 'rpn_cls_prob_reshape'))\n\n        # ========= RoI Proposal ============\n        # add the delta(output) to anchors then\n        # choose some reasonabel boxes, considering scores, ratios, size and iou\n        # rpn_rois <- (1 x H x W x A, 5) e.g. [0, x1, y1, x2, y2]\n        (self.feed('rpn_cls_prob_reshape','rpn_bbox_pred','im_info')\n             .proposal_layer(_feat_stride, anchor_scales, 'TRAIN', name = 'rpn_rois'))\n\n        # matching boxes and groundtruth,\n        # and randomly sample some rois and labels for RCNN\n        (self.feed('rpn_rois','gt_boxes', 'gt_ishard', 'dontcare_areas')\n             .proposal_target_layer(n_classes,name = 'roi-data'))\n\n        #========= RCNN ============        \n        (self.feed('conv5_3', 'rois')\n             .roi_pool(7, 7, 1.0/16, name='pool_5')\n             .fc(4096, name='fc6')\n             .dropout(0.5, name='drop6')\n             .fc(4096, name='fc7')\n             .dropout(0.5, name='drop7')\n             .fc(n_classes, relu=False, name='cls_score')\n             .softmax(name='cls_prob'))\n\n        (self.feed('drop7')\n             .fc(n_classes*4, relu=False, name='bbox_pred'))\n"""
lib/networks/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nfrom .VGGnet_train import VGGnet_train\nfrom .VGGnet_test import VGGnet_test\nfrom .Resnet50_train import Resnet50_train\nfrom .Resnet50_test import Resnet50_test\nfrom .Resnet101_train import Resnet101_train\nfrom .Resnet101_test import Resnet101_test\nfrom . import factory\n'
lib/networks/caffenet.py,3,"b""import tensorflow as tf\nfrom .network import Network\n\nclass caffenet(Network):\n    def __init__(self, trainable=True):\n        self.inputs = []\n        self.data = tf.placeholder(tf.float32, shape=[None, None, None, 3])\n        self.rois = tf.placeholder(tf.float32, shape=[None, 5])\n        self.keep_prob = tf.placeholder(tf.float32)\n        self.layers = dict({'data':self.data, 'rois':self.rois})\n        self.trainable = trainable\n        self.setup()\n\n    def setup(self):\n        (self.feed('data')\n             .conv(11, 11, 96, 4, 4, padding='VALID', name='conv1', trainable=False)\n             .max_pool(3, 3, 2, 2, padding='VALID', name='pool1')\n             .lrn(2, 2e-05, 0.75, name='norm1')\n             .conv(5, 5, 256, 1, 1, group=2, name='conv2')\n             .max_pool(3, 3, 2, 2, padding='VALID', name='pool2')\n             .lrn(2, 2e-05, 0.75, name='norm2')\n             .conv(3, 3, 384, 1, 1, name='conv3')\n             .conv(3, 3, 384, 1, 1, group=2, name='conv4')\n             .conv(3, 3, 256, 1, 1, group=2, name='conv5')\n             .feature_extrapolating([1.0, 2.0, 3.0, 4.0], 4, 4, name='conv5_feature'))\n\n        (self.feed('conv5_feature','im_info')\n             .conv(3,3,)\n\n        (self.feed('conv5_feature', 'rois')\n             .roi_pool(6, 6, 1.0/16, name='pool5')\n             .fc(4096, name='fc6')\n             .dropout(self.keep_prob, name='drop6')\n             .fc(4096, name='fc7')\n             .dropout(self.keep_prob, name='drop7')\n             .fc(174, relu=False, name='subcls_score')\n             .softmax(name='subcls_prob'))\n\n        (self.feed('subcls_score')\n             .fc(4, relu=False, name='cls_score')\n             .softmax(name='cls_prob'))\n\n        (self.feed('subcls_score')\n             .fc(16, relu=False, name='bbox_pred')))\n"""
lib/networks/factory.py,0,"b'# --------------------------------------------------------\n# SubCNN_TF\n# Copyright (c) 2016 CVGL Stanford\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Yu Xiang\n# --------------------------------------------------------\n\n""""""Factory method for easily getting imdbs by name.""""""\n\n__sets = {}\n\nfrom .VGGnet_test import VGGnet_test\nfrom .VGGnet_testold import VGGnet_testold\nfrom .VGGnet_train import VGGnet_train\nfrom .Resnet50_test import Resnet50_test\nfrom .Resnet50_train import Resnet50_train\nfrom .Resnet101_test import Resnet101_test\nfrom .Resnet101_train import Resnet101_train\nfrom .PVAnet_train import PVAnet_train\nfrom .PVAnet_test import PVAnet_test\n\n\ndef get_network(name):\n    """"""Get a network by name.""""""\n    if name.split(\'_\')[0] == \'VGGnet\':\n        if name.split(\'_\')[1] == \'test\':\n           return VGGnet_test()\n        elif name.split(\'_\')[1] == \'train\':\n           return VGGnet_train()\n        elif name.split(\'_\')[1] == \'testold\':\n            return VGGnet_testold()\n        else:\n           raise KeyError(\'Unknown dataset: {}\'.format(name))\n    elif name.split(\'_\')[0] == \'Resnet50\':\n        if name.split(\'_\')[1] == \'test\':\n            return Resnet50_test()\n        elif name.split(\'_\')[1] == \'train\':\n            return Resnet50_train()\n        else:\n            raise KeyError(\'Unknown dataset: {}\'.format(name))\n    elif name.split(\'_\')[0] == \'Resnet101\':\n        if name.split(\'_\')[1] == \'test\':\n            return Resnet101_test()\n        elif name.split(\'_\')[1] == \'train\':\n            return Resnet101_train()\n        else:\n            raise KeyError(\'Unknown dataset: {}\'.format(name))\n    elif name.split(\'_\')[0] == \'PVAnet\':\n        if name.split(\'_\')[1] == \'test\':\n           return PVAnet_test()\n        elif name.split(\'_\')[1] == \'train\':\n           return PVAnet_train()\n        else:\n            raise KeyError(\'Unknown dataset: {}\'.format(name))\n    else:\n        raise KeyError(\'Unknown dataset: {}\'.format(name))\n\ndef list_networks():\n    """"""List all registered imdbs.""""""\n    return __sets.keys()\n'"
lib/networks/network.py,169,"b'import numpy as np\nimport tensorflow as tf\n\nfrom ..fast_rcnn.config import cfg\nfrom ..roi_pooling_layer import roi_pooling_op as roi_pool_op\nfrom ..rpn_msr.proposal_layer_tf import proposal_layer as proposal_layer_py\nfrom ..rpn_msr.anchor_target_layer_tf import anchor_target_layer as anchor_target_layer_py\nfrom ..rpn_msr.proposal_target_layer_tf import proposal_target_layer as proposal_target_layer_py\n# FCN pooling\nfrom ..psroi_pooling_layer import psroi_pooling_op as psroi_pooling_op\n\n\nDEFAULT_PADDING = \'SAME\'\n\ndef include_original(dec):\n    """""" Meta decorator, which make the original function callable (via f._original() )""""""\n    def meta_decorator(f):\n        decorated = dec(f)\n        decorated._original = f\n        return decorated\n    return meta_decorator\n\n@include_original\ndef layer(op):\n    def layer_decorated(self, *args, **kwargs):\n        # Automatically set a name if not provided.\n        name = kwargs.setdefault(\'name\', self.get_unique_name(op.__name__))\n        # Figure out the layer inputs.\n        if len(self.inputs)==0:\n            raise RuntimeError(\'No input variables found for layer %s.\'%name)\n        elif len(self.inputs)==1:\n            layer_input = self.inputs[0]\n        else:\n            layer_input = list(self.inputs)\n        # Perform the operation and get the output.\n        layer_output = op(self, layer_input, *args, **kwargs)\n        # Add to layer LUT.\n        self.layers[name] = layer_output\n        # This output is now the input for the next layer.\n        self.feed(layer_output)\n        # Return self for chained calls.\n        return self\n    return layer_decorated\n\nclass Network(object):\n    def __init__(self, inputs, trainable=True):\n        self.inputs = []\n        self.layers = dict(inputs)\n        self.trainable = trainable\n        self.setup()\n\n    def setup(self):\n        raise NotImplementedError(\'Must be subclassed.\')\n\n    def load(self, data_path, session, ignore_missing=False):\n        data_dict = np.load(data_path).item()\n        for key in data_dict:\n            with tf.variable_scope(key, reuse=True):\n                for subkey in data_dict[key]:\n                    try:\n                        var = tf.get_variable(subkey)\n                        session.run(var.assign(data_dict[key][subkey]))\n                        print ""assign pretrain model ""+subkey+ "" to ""+key\n                    except ValueError:\n                        print ""ignore ""+key\n                        if not ignore_missing:\n\n                            raise\n\n    def feed(self, *args):\n        assert len(args)!=0\n        self.inputs = []\n        for layer in args:\n            if isinstance(layer, basestring):\n                try:\n                    layer = self.layers[layer]\n                    print layer\n                except KeyError:\n                    print self.layers.keys()\n                    raise KeyError(\'Unknown layer name fed: %s\'%layer)\n            self.inputs.append(layer)\n        return self\n\n    def get_output(self, layer):\n        try:\n            layer = self.layers[layer]\n        except KeyError:\n            print self.layers.keys()\n            raise KeyError(\'Unknown layer name fed: %s\'%layer)\n        return layer\n\n    def get_unique_name(self, prefix):\n        id = sum(t.startswith(prefix) for t,_ in self.layers.items())+1\n        return \'%s_%d\'%(prefix, id)\n\n    def make_var(self, name, shape, initializer=None, trainable=True, regularizer=None):\n        return tf.get_variable(name, shape, initializer=initializer, trainable=trainable, regularizer=regularizer)\n\n    def validate_padding(self, padding):\n        assert padding in (\'SAME\', \'VALID\')\n\n    """"""\n    @layer\n    def conv(self, input, k_h, k_w, c_o, s_h, s_w, name, relu=True, padding=DEFAULT_PADDING, group=1, trainable=True):\n        self.validate_padding(padding)\n        c_i = input.get_shape()[-1]\n        assert c_i%group==0\n        assert c_o%group==0\n        convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n        with tf.variable_scope(name) as scope:\n\n            init_weights = tf.truncated_normal_initializer(0.0, stddev=0.01)\n            init_biases = tf.constant_initializer(0.0)\n            kernel = self.make_var(\'weights\', [k_h, k_w, c_i/group, c_o], init_weights, trainable, \\\n                                   regularizer=self.l2_regularizer(cfg.TRAIN.WEIGHT_DECAY))\n            biases = self.make_var(\'biases\', [c_o], init_biases, trainable)\n\n            if group==1:\n                conv = convolve(input, kernel)\n            else:\n                input_groups = tf.split(3, group, input)\n                kernel_groups = tf.split(3, group, kernel)\n                output_groups = [convolve(i, k) for i,k in zip(input_groups, kernel_groups)]\n                conv = tf.concat(3, output_groups)\n            if relu:\n                bias = tf.nn.bias_add(conv, biases)\n                return tf.nn.relu(bias, name=scope.name)\n            return tf.nn.bias_add(conv, biases, name=scope.name)\n            """"""\n\n    @layer\n    def conv(self, input, k_h, k_w, c_o, s_h, s_w, name, biased=True,relu=True, padding=DEFAULT_PADDING, trainable=True):\n        """""" contribution by miraclebiu, and biased option""""""\n        self.validate_padding(padding)\n        c_i = input.get_shape()[-1]\n        convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n        with tf.variable_scope(name) as scope:\n\n            # init_weights = tf.truncated_normal_initializer(0.0, stddev=0.001)\n            init_weights = tf.contrib.layers.variance_scaling_initializer(factor=0.01, mode=\'FAN_AVG\', uniform=False)\n            init_biases = tf.constant_initializer(0.0)\n            kernel = self.make_var(\'weights\', [k_h, k_w, c_i, c_o], init_weights, trainable, \\\n                                   regularizer=self.l2_regularizer(cfg.TRAIN.WEIGHT_DECAY))\n            if biased:\n                biases = self.make_var(\'biases\', [c_o], init_biases, trainable)\n                conv = convolve(input, kernel)\n                if relu:\n                    bias = tf.nn.bias_add(conv, biases)\n                    return tf.nn.relu(bias)\n                return tf.nn.bias_add(conv, biases)\n            else:\n                conv = convolve(input, kernel)\n                if relu:\n                    return tf.nn.relu(conv)\n                return conv\n\n    @layer\n    def upconv(self, input, shape, c_o, ksize=4, stride = 2, name = \'upconv\', biased=False, relu=True, padding=DEFAULT_PADDING,\n             trainable=True):\n        """""" up-conv""""""\n        self.validate_padding(padding)\n\n        c_in = input.get_shape()[3].value\n        in_shape = tf.shape(input)\n        if shape is None:\n            # h = ((in_shape[1] - 1) * stride) + 1\n            # w = ((in_shape[2] - 1) * stride) + 1\n            h = ((in_shape[1] ) * stride)\n            w = ((in_shape[2] ) * stride)\n            new_shape = [in_shape[0], h, w, c_o]\n        else:\n            new_shape = [in_shape[0], shape[1], shape[2], c_o]\n        output_shape = tf.stack(new_shape)\n\n        filter_shape = [ksize, ksize, c_o, c_in]\n\n        with tf.variable_scope(name) as scope:\n            # init_weights = tf.truncated_normal_initializer(0.0, stddev=0.01)\n            init_weights = tf.contrib.layers.variance_scaling_initializer(factor=0.01, mode=\'FAN_AVG\', uniform=False)\n            filters = self.make_var(\'weights\', filter_shape, init_weights, trainable, \\\n                                   regularizer=self.l2_regularizer(cfg.TRAIN.WEIGHT_DECAY))\n            deconv = tf.nn.conv2d_transpose(input, filters, output_shape,\n                                            strides=[1, stride, stride, 1], padding=DEFAULT_PADDING, name=scope.name)\n            # coz de-conv losses shape info, use reshape to re-gain shape\n            deconv = tf.reshape(deconv, new_shape)\n\n            if biased:\n                init_biases = tf.constant_initializer(0.0)\n                biases = self.make_var(\'biases\', [c_o], init_biases, trainable)\n                if relu:\n                    bias = tf.nn.bias_add(deconv, biases)\n                    return tf.nn.relu(bias)\n                return tf.nn.bias_add(deconv, biases)\n            else:\n                if relu:\n                    return tf.nn.relu(deconv)\n                return deconv\n\n    @layer\n    def relu(self, input, name):\n        return tf.nn.relu(input, name=name)\n\n    @layer\n    def max_pool(self, input, k_h, k_w, s_h, s_w, name, padding=DEFAULT_PADDING):\n        self.validate_padding(padding)\n        return tf.nn.max_pool(input,\n                              ksize=[1, k_h, k_w, 1],\n                              strides=[1, s_h, s_w, 1],\n                              padding=padding,\n                              name=name)\n\n    @layer\n    def avg_pool(self, input, k_h, k_w, s_h, s_w, name, padding=DEFAULT_PADDING):\n        self.validate_padding(padding)\n        return tf.nn.avg_pool(input,\n                              ksize=[1, k_h, k_w, 1],\n                              strides=[1, s_h, s_w, 1],\n                              padding=padding,\n                              name=name)\n\n    @layer\n    def roi_pool(self, input, pooled_height, pooled_width, spatial_scale, name):\n        # only use the first input\n        if isinstance(input[0], tuple):\n            input[0] = input[0][0]\n\n        if isinstance(input[1], tuple):\n            input[1] = input[1][0]\n\n        print input\n        return roi_pool_op.roi_pool(input[0], input[1],\n                                    pooled_height,\n                                    pooled_width,\n                                    spatial_scale,\n                                    name=name)[0]\n\n    @layer\n    def psroi_pool(self, input, output_dim, group_size, spatial_scale, name):\n        """"""contribution by miraclebiu""""""\n        # only use the first input\n        if isinstance(input[0], tuple):\n            input[0] = input[0][0]\n\n        if isinstance(input[1], tuple):\n            input[1] = input[1][0]\n\n        return psroi_pooling_op.psroi_pool(input[0], input[1],\n                                           output_dim=output_dim,\n                                           group_size=group_size,\n                                           spatial_scale=spatial_scale,\n                                           name=name)[0]\n\n    @layer\n    def proposal_layer(self, input, _feat_stride, anchor_scales, cfg_key, name):\n        if isinstance(input[0], tuple):\n            input[0] = input[0][0]\n            # input[0] shape is (1, H, W, Ax2)\n            # rpn_rois <- (1 x H x W x A, 5) [0, x1, y1, x2, y2]\n        return tf.reshape(tf.py_func(proposal_layer_py,\\\n                                     [input[0],input[1],input[2], cfg_key, _feat_stride, anchor_scales],\\\n                                     [tf.float32]),\n                          [-1,5],name =name)\n\n    @layer\n    def anchor_target_layer(self, input, _feat_stride, anchor_scales, name):\n        if isinstance(input[0], tuple):\n            input[0] = input[0][0]\n\n        with tf.variable_scope(name) as scope:\n            # \'rpn_cls_score\', \'gt_boxes\', \'gt_ishard\', \'dontcare_areas\', \'im_info\'\n            rpn_labels,rpn_bbox_targets,rpn_bbox_inside_weights,rpn_bbox_outside_weights = \\\n                tf.py_func(anchor_target_layer_py,\n                           [input[0],input[1],input[2],input[3],input[4], _feat_stride, anchor_scales],\n                           [tf.float32,tf.float32,tf.float32,tf.float32])\n\n            rpn_labels = tf.convert_to_tensor(tf.cast(rpn_labels,tf.int32), name = \'rpn_labels\') # shape is (1 x H x W x A, 2)\n            rpn_bbox_targets = tf.convert_to_tensor(rpn_bbox_targets, name = \'rpn_bbox_targets\') # shape is (1 x H x W x A, 4)\n            rpn_bbox_inside_weights = tf.convert_to_tensor(rpn_bbox_inside_weights , name = \'rpn_bbox_inside_weights\') # shape is (1 x H x W x A, 4)\n            rpn_bbox_outside_weights = tf.convert_to_tensor(rpn_bbox_outside_weights , name = \'rpn_bbox_outside_weights\') # shape is (1 x H x W x A, 4)\n\n\n            return rpn_labels, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights\n\n\n    @layer\n    def proposal_target_layer(self, input, classes, name):\n        if isinstance(input[0], tuple):\n            input[0] = input[0][0]\n        with tf.variable_scope(name) as scope:\n            #inputs: \'rpn_rois\',\'gt_boxes\', \'gt_ishard\', \'dontcare_areas\'\n            rois,labels,bbox_targets,bbox_inside_weights,bbox_outside_weights \\\n                = tf.py_func(proposal_target_layer_py,\n                             [input[0],input[1],input[2],input[3],classes],\n                             [tf.float32,tf.float32,tf.float32,tf.float32,tf.float32])\n            # rois <- (1 x H x W x A, 5) e.g. [0, x1, y1, x2, y2]\n            # rois = tf.convert_to_tensor(rois, name=\'rois\')\n            rois = tf.reshape(rois, [-1, 5], name=\'rois\') # goes to roi_pooling\n            labels = tf.convert_to_tensor(tf.cast(labels,tf.int32), name = \'labels\') # goes to FRCNN loss\n            bbox_targets = tf.convert_to_tensor(bbox_targets, name = \'bbox_targets\') # goes to FRCNN loss\n            bbox_inside_weights = tf.convert_to_tensor(bbox_inside_weights, name = \'bbox_inside_weights\')\n            bbox_outside_weights = tf.convert_to_tensor(bbox_outside_weights, name = \'bbox_outside_weights\')\n\n            self.layers[\'rois\'] = rois\n\n            return rois, labels, bbox_targets, bbox_inside_weights, bbox_outside_weights\n\n\n    @layer\n    def reshape_layer(self, input, d, name):\n        input_shape = tf.shape(input)\n        if name == \'rpn_cls_prob_reshape\':\n            #\n            # transpose: (1, AxH, W, 2) -> (1, 2, AxH, W)\n            # reshape: (1, 2xA, H, W)\n            # transpose: -> (1, H, W, 2xA)\n             return tf.transpose(tf.reshape(tf.transpose(input,[0,3,1,2]),\n                                            [   input_shape[0],\n                                                int(d),\n                                                tf.cast(tf.cast(input_shape[1],tf.float32)/tf.cast(d,tf.float32)*tf.cast(input_shape[3],tf.float32),tf.int32),\n                                                input_shape[2]\n                                            ]),\n                                 [0,2,3,1],name=name)\n        else:\n             return tf.transpose(tf.reshape(tf.transpose(input,[0,3,1,2]),\n                                        [   input_shape[0],\n                                            int(d),\n                                            tf.cast(tf.cast(input_shape[1],tf.float32)*(tf.cast(input_shape[3],tf.float32)/tf.cast(d,tf.float32)),tf.int32),\n                                            input_shape[2]\n                                        ]),\n                                 [0,2,3,1],name=name)\n\n    @layer\n    def spatial_reshape_layer(self, input, d, name):\n        input_shape = tf.shape(input)\n        # transpose: (1, H, W, A x d) -> (1, H, WxA, d)\n        return tf.reshape(input,\\\n                               [input_shape[0],\\\n                                input_shape[1], \\\n                                -1,\\\n                                int(d)])\n\n    # @layer\n    # def feature_extrapolating(self, input, scales_base, num_scale_base, num_per_octave, name):\n    #     return feature_extrapolating_op.feature_extrapolating(input,\n    #                           scales_base,\n    #                           num_scale_base,\n    #                           num_per_octave,\n    #                           name=name)\n\n    @layer\n    def lrn(self, input, radius, alpha, beta, name, bias=1.0):\n        return tf.nn.local_response_normalization(input,\n                                                  depth_radius=radius,\n                                                  alpha=alpha,\n                                                  beta=beta,\n                                                  bias=bias,\n                                                  name=name)\n\n    @layer\n    def concat(self, inputs, axis, name):\n        return tf.concat(axis=axis, values=inputs, name=name)\n\n    @layer\n    def fc(self, input, num_out, name, relu=True, trainable=True):\n        with tf.variable_scope(name) as scope:\n            # only use the first input\n            if isinstance(input, tuple):\n                input = input[0]\n\n            input_shape = input.get_shape()\n            if input_shape.ndims == 4:\n                dim = 1\n                for d in input_shape[1:].as_list():\n                    dim *= d\n                feed_in = tf.reshape(tf.transpose(input,[0,3,1,2]), [-1, dim])\n            else:\n                feed_in, dim = (input, int(input_shape[-1]))\n\n            if name == \'bbox_pred\':\n                init_weights = tf.truncated_normal_initializer(0.0, stddev=0.001)\n                init_biases = tf.constant_initializer(0.0)\n            else:\n                init_weights = tf.truncated_normal_initializer(0.0, stddev=0.01)\n                init_biases = tf.constant_initializer(0.0)\n\n            weights = self.make_var(\'weights\', [dim, num_out], init_weights, trainable, \\\n                                    regularizer=self.l2_regularizer(cfg.TRAIN.WEIGHT_DECAY))\n            biases = self.make_var(\'biases\', [num_out], init_biases, trainable)\n\n            op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b\n            fc = op(feed_in, weights, biases, name=scope.name)\n            return fc\n\n    @layer\n    def softmax(self, input, name):\n        input_shape = tf.shape(input)\n        if name == \'rpn_cls_prob\':\n            return tf.reshape(tf.nn.softmax(tf.reshape(input,[-1,input_shape[3]])),[-1,input_shape[1],input_shape[2],input_shape[3]],name=name)\n        else:\n            return tf.nn.softmax(input,name=name)\n\n    @layer\n    def spatial_softmax(self, input, name):\n        input_shape = tf.shape(input)\n        # d = input.get_shape()[-1]\n        return tf.reshape(tf.nn.softmax(tf.reshape(input, [-1, input_shape[3]])),\n                          [-1, input_shape[1], input_shape[2], input_shape[3]], name=name)\n\n    @layer\n    def add(self,input,name):\n        """"""contribution by miraclebiu""""""\n        return tf.add(input[0],input[1], name=name)\n\n    @layer\n    def batch_normalization(self,input,name,relu=True, is_training=False):\n        """"""contribution by miraclebiu""""""\n        if relu:\n            temp_layer=tf.contrib.layers.batch_norm(input,scale=True,center=True,is_training=is_training,scope=name)\n            return tf.nn.relu(temp_layer)\n        else:\n            return tf.contrib.layers.batch_norm(input,scale=True,center=True,is_training=is_training,scope=name)\n\n    @layer\n    def negation(self, input, name):\n        """""" simply multiplies -1 to the tensor""""""\n        return tf.multiply(input, -1.0, name=name)\n\n    @layer\n    def bn_scale_combo(self, input, c_in, name, relu=True):\n        """""" PVA net BN -> Scale -> Relu""""""\n        with tf.variable_scope(name) as scope:\n            bn = self.batch_normalization._original(self, input, name=\'bn\', relu=False, is_training=False)\n            # alpha = tf.get_variable(\'bn_scale/alpha\', shape=[c_in, ], dtype=tf.float32,\n            #                     initializer=tf.constant_initializer(1.0), trainable=True,\n            #                     regularizer=self.l2_regularizer(0.00001))\n            # beta = tf.get_variable(\'bn_scale/beta\', shape=[c_in, ], dtype=tf.float32,\n            #                    initializer=tf.constant_initializer(0.0), trainable=True,\n            #                    regularizer=self.l2_regularizer(0.00001))\n            # bn = tf.add(tf.mul(bn, alpha), beta)\n            if relu:\n                bn = tf.nn.relu(bn, name=\'relu\')\n            return bn\n\n    @layer\n    def pva_negation_block(self, input, k_h, k_w, c_o, s_h, s_w, name, biased=True, padding=DEFAULT_PADDING, trainable=True,\n                           scale = True, negation = True):\n        """""" for PVA net, Conv -> BN -> Neg -> Concat -> Scale -> Relu""""""\n        with tf.variable_scope(name) as scope:\n            conv = self.conv._original(self, input, k_h, k_w, c_o, s_h, s_w, biased=biased, relu=False, name=\'conv\', padding=padding, trainable=trainable)\n            conv = self.batch_normalization._original(self, conv, name=\'bn\', relu=False, is_training=False)\n            c_in = c_o\n            if negation:\n                conv_neg = self.negation._original(self, conv, name=\'neg\')\n                conv = tf.concat(axis=3, values=[conv, conv_neg], name=\'concat\')\n                c_in += c_in\n            if scale:\n                # y = \\alpha * x + \\beta\n                alpha = tf.get_variable(\'scale/alpha\', shape=[c_in,], dtype=tf.float32,\n                                        initializer=tf.constant_initializer(1.0), trainable=True, regularizer=self.l2_regularizer(0.00001))\n                beta = tf.get_variable(\'scale/beta\', shape=[c_in, ], dtype=tf.float32,\n                                        initializer=tf.constant_initializer(0.0), trainable=True, regularizer=self.l2_regularizer(0.00001))\n                # conv = conv * alpha + beta\n                conv = tf.add(tf.multiply(conv, alpha), beta)\n            return tf.nn.relu(conv, name=\'relu\')\n\n    @layer\n    def pva_negation_block_v2(self, input, k_h, k_w, c_o, s_h, s_w, c_in, name, biased=True, padding=DEFAULT_PADDING, trainable=True,\n                           scale = True, negation = True):\n        """""" for PVA net, BN -> [Neg -> Concat ->] Scale -> Relu -> Conv""""""\n        with tf.variable_scope(name) as scope:\n            bn = self.batch_normalization._original(self, input, name=\'bn\', relu=False, is_training=False)\n            if negation:\n                bn_neg = self.negation._original(self, bn, name=\'neg\')\n                bn = tf.concat(axis=3, values=[bn, bn_neg], name=\'concat\')\n                c_in += c_in\n                # y = \\alpha * x + \\beta\n                alpha = tf.get_variable(\'scale/alpha\', shape=[c_in,], dtype=tf.float32,\n                                        initializer=tf.constant_initializer(1.0), trainable=True, regularizer=self.l2_regularizer(0.00004))\n                beta = tf.get_variable(\'scale/beta\', shape=[c_in, ], dtype=tf.float32,\n                                        initializer=tf.constant_initializer(0.0), trainable=True, regularizer=self.l2_regularizer(0.00004))\n                bn = tf.add(tf.multiply(bn, alpha), beta)\n            bn = tf.nn.relu(bn, name=\'relu\')\n            if name == \'conv3_1/1\': self.layers[\'conv3_1/1/relu\'] = bn\n\n            conv = self.conv._original(self, bn, k_h, k_w, c_o, s_h, s_w, biased=biased, relu=False, name=\'conv\', padding=padding,\n                         trainable=trainable)\n            return conv\n\n    @layer\n    def pva_inception_res_stack(self, input, c_in, name, block_start = False, type = \'a\'):\n\n        if type == \'a\':\n            (c_0, c_1, c_2, c_pool, c_out) = (64, 64, 24, 128, 256)\n        elif type == \'b\':\n            (c_0, c_1, c_2, c_pool, c_out) = (64, 96, 32, 128, 384)\n        else:\n            raise (\'Unexpected inception-res type\')\n        if block_start:\n            stride = 2\n        else:\n            stride = 1\n        with tf.variable_scope(name+\'/incep\') as scope:\n            bn = self.batch_normalization._original(self, input, name=\'bn\', relu=False, is_training=False)\n            bn_scale = self.scale._original(self, bn, c_in, name=\'bn_scale\')\n            ## 1 x 1\n\n            conv = self.conv._original(self, bn_scale, 1, 1, c_0, stride, stride, name=\'0/conv\', biased = False, relu=False)\n            conv_0 = self.bn_scale_combo._original(self, conv, c_in=c_0, name =\'0\', relu=True)\n\n            ## 3 x 3\n            bn_relu = tf.nn.relu(bn_scale, name=\'relu\')\n            if name == \'conv4_1\': tmp_c = c_1; c_1 = 48\n            conv = self.conv._original(self, bn_relu, 1, 1, c_1, stride, stride, name=\'1_reduce/conv\', biased = False, relu=False)\n            conv = self.bn_scale_combo._original(self, conv, c_in=c_1, name=\'1_reduce\', relu=True)\n            if name == \'conv4_1\': c_1 = tmp_c\n            conv = self.conv._original(self, conv, 3, 3, c_1 * 2, 1, 1, name=\'1_0/conv\', biased = False, relu=False)\n            conv_1 = self.bn_scale_combo._original(self, conv, c_in=c_1 * 2, name=\'1_0\', relu=True)\n\n            ## 5 x 5\n            conv = self.conv._original(self, bn_scale, 1, 1, c_2, stride, stride, name=\'2_reduce/conv\', biased = False, relu=False)\n            conv = self.bn_scale_combo._original(self, conv, c_in=c_2, name=\'2_reduce\', relu=True)\n            conv = self.conv._original(self, conv, 3, 3, c_2 * 2, 1, 1, name=\'2_0/conv\', biased = False, relu=False)\n            conv = self.bn_scale_combo._original(self, conv, c_in=c_2 * 2, name=\'2_0\', relu=True)\n            conv = self.conv._original(self, conv, 3, 3, c_2 * 2, 1, 1, name=\'2_1/conv\', biased = False, relu=False)\n            conv_2 = self.bn_scale_combo._original(self, conv, c_in=c_2 * 2, name=\'2_1\', relu=True)\n\n            ## pool\n            if block_start:\n                pool = self.max_pool._original(self, bn_scale, 3, 3, 2, 2, padding=DEFAULT_PADDING, name=\'pool\')\n                pool = self.conv._original(self, pool, 1, 1, c_pool, 1, 1, name=\'poolproj/conv\', biased = False, relu=False)\n                pool = self.bn_scale_combo._original(self, pool, c_in=c_pool, name=\'poolproj\', relu=True)\n\n        with tf.variable_scope(name) as scope:\n            if block_start:\n                concat = tf.concat(axis=3, values=[conv_0, conv_1, conv_2, pool], name=\'concat\')\n                proj = self.conv._original(self, input, 1, 1, c_out, 2, 2, name=\'proj\', biased=True,\n                                           relu=False)\n            else:\n                concat = tf.concat(axis=3, values=[conv_0, conv_1, conv_2], name=\'concat\')\n                proj = input\n\n            conv = self.conv._original(self, concat, 1, 1, c_out, 1, 1, name=\'out/conv\', relu=False)\n            if name == \'conv5_4\':\n                conv = self.bn_scale_combo._original(self, conv, c_in=c_out, name=\'out\', relu=False)\n            conv = self.add._original(self, [conv, proj], name=\'sum\')\n        return  conv\n\n    @layer\n    def pva_inception_res_block(self, input, name, name_prefix = \'conv4_\', type = \'a\'):\n        """"""build inception block""""""\n        node = input\n        if type == \'a\':\n            c_ins = (128, 256, 256, 256, 256, )\n        else:\n            c_ins = (256, 384, 384, 384, 384, )\n        for i in range(1, 5):\n            node = self.pva_inception_res_stack._original(self, node, c_in = c_ins[i-1],\n                                                          name = name_prefix + str(i), block_start=(i==1), type=type)\n        return node\n\n    @layer\n    def scale(self, input, c_in, name):\n        with tf.variable_scope(name) as scope:\n\n            alpha = tf.get_variable(\'alpha\', shape=[c_in, ], dtype=tf.float32,\n                                    initializer=tf.constant_initializer(1.0), trainable=True,\n                                    regularizer=self.l2_regularizer(0.00001))\n            beta = tf.get_variable(\'beta\', shape=[c_in, ], dtype=tf.float32,\n                                   initializer=tf.constant_initializer(0.0), trainable=True,\n                                   regularizer=self.l2_regularizer(0.00001))\n            return tf.add(tf.multiply(input, alpha), beta)\n\n\n\n\n\n\n    @layer\n    def dropout(self, input, keep_prob, name):\n        return tf.nn.dropout(input, keep_prob, name=name)\n\n    def l2_regularizer(self, weight_decay=0.0005, scope=None):\n        def regularizer(tensor):\n            with tf.name_scope(scope, default_name=\'l2_regularizer\', values=[tensor]):\n                l2_weight = tf.convert_to_tensor(weight_decay,\n                                       dtype=tensor.dtype.base_dtype,\n                                       name=\'weight_decay\')\n                return tf.multiply(l2_weight, tf.nn.l2_loss(tensor), name=\'value\')\n        return regularizer\n\n    def smooth_l1_dist(self, deltas, sigma2=9.0, name=\'smooth_l1_dist\'):\n        with tf.name_scope(name=name) as scope:\n            deltas_abs = tf.abs(deltas)\n            smoothL1_sign = tf.cast(tf.less(deltas_abs, 1.0/sigma2), tf.float32)\n            return tf.square(deltas) * 0.5 * sigma2 * smoothL1_sign + \\\n                        (deltas_abs - 0.5 / sigma2) * tf.abs(smoothL1_sign - 1)\n\n\n\n    def build_loss(self, ohem=False):\n        ############# RPN\n        # classification loss\n        rpn_cls_score = tf.reshape(self.get_output(\'rpn_cls_score_reshape\'), [-1, 2])  # shape (HxWxA, 2)\n        rpn_label = tf.reshape(self.get_output(\'rpn-data\')[0], [-1])  # shape (HxWxA)\n        # ignore_label(-1)\n        fg_keep = tf.equal(rpn_label, 1)\n        rpn_keep = tf.where(tf.not_equal(rpn_label, -1))\n        rpn_cls_score = tf.reshape(tf.gather(rpn_cls_score, rpn_keep), [-1, 2]) # shape (N, 2)\n        rpn_label = tf.reshape(tf.gather(rpn_label, rpn_keep), [-1])\n        rpn_cross_entropy_n = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=rpn_cls_score, labels=rpn_label)\n        rpn_cross_entropy = tf.reduce_mean(rpn_cross_entropy_n)\n\n        # box loss\n        rpn_bbox_pred = self.get_output(\'rpn_bbox_pred\') # shape (1, H, W, Ax4)\n        rpn_bbox_targets = self.get_output(\'rpn-data\')[1]\n        rpn_bbox_inside_weights = self.get_output(\'rpn-data\')[2]\n        rpn_bbox_outside_weights = self.get_output(\'rpn-data\')[3]\n        rpn_bbox_pred = tf.reshape(tf.gather(tf.reshape(rpn_bbox_pred, [-1, 4]), rpn_keep), [-1, 4]) # shape (N, 4)\n        rpn_bbox_targets = tf.reshape(tf.gather(tf.reshape(rpn_bbox_targets, [-1, 4]), rpn_keep), [-1, 4])\n        rpn_bbox_inside_weights = tf.reshape(tf.gather(tf.reshape(rpn_bbox_inside_weights, [-1, 4]), rpn_keep), [-1, 4])\n        rpn_bbox_outside_weights = tf.reshape(tf.gather(tf.reshape(rpn_bbox_outside_weights, [-1, 4]), rpn_keep), [-1, 4])\n\n        rpn_loss_box_n = tf.reduce_sum(self.smooth_l1_dist(\n            rpn_bbox_inside_weights * (rpn_bbox_pred - rpn_bbox_targets)), axis=[1])\n\n        # rpn_loss_n = tf.reshape(rpn_cross_entropy_n + rpn_loss_box_n * 5, [-1])\n\n        if ohem:\n            # k = tf.minimum(tf.shape(rpn_cross_entropy_n)[0] / 2, 300)\n            # # k = tf.shape(rpn_loss_n)[0] / 2\n            # rpn_loss_n, top_k_indices = tf.nn.top_k(rpn_cross_entropy_n, k=k, sorted=False)\n            # rpn_cross_entropy_n = tf.gather(rpn_cross_entropy_n, top_k_indices)\n            # rpn_loss_box_n = tf.gather(rpn_loss_box_n, top_k_indices)\n\n            # strategy: keeps all the positive samples\n            fg_ = tf.equal(rpn_label, 1)\n            bg_ = tf.equal(rpn_label, 0)\n            pos_inds = tf.where(fg_)\n            neg_inds = tf.where(bg_)\n            rpn_cross_entropy_n_pos = tf.reshape(tf.gather(rpn_cross_entropy_n, pos_inds), [-1])\n            rpn_cross_entropy_n_neg = tf.reshape(tf.gather(rpn_cross_entropy_n, neg_inds), [-1])\n            top_k = tf.cast(tf.minimum(tf.shape(rpn_cross_entropy_n_neg)[0], 300), tf.int32)\n            rpn_cross_entropy_n_neg, _ = tf.nn.top_k(rpn_cross_entropy_n_neg, k=top_k)\n            rpn_cross_entropy = tf.reduce_sum(rpn_cross_entropy_n_neg) / (tf.reduce_sum(tf.cast(bg_, tf.float32)) + 1.0) \\\n                                + tf.reduce_sum(rpn_cross_entropy_n_pos) / (tf.reduce_sum(tf.cast(fg_, tf.float32)) + 1.0)\n\n            rpn_loss_box_n = tf.reshape(tf.gather(rpn_loss_box_n, pos_inds), [-1])\n            # rpn_cross_entropy_n = tf.concat(0, (rpn_cross_entropy_n_pos, rpn_cross_entropy_n_neg))\n\n        # rpn_loss_box = 1 * tf.reduce_mean(rpn_loss_box_n)\n        rpn_loss_box = tf.reduce_sum(rpn_loss_box_n) / (tf.reduce_sum(tf.cast(fg_keep, tf.float32)) + 1.0)\n\n        ############# R-CNN\n        # classification loss\n        cls_score = self.get_output(\'cls_score\') # (R, C+1)\n        label = tf.reshape(self.get_output(\'roi-data\')[1], [-1]) # (R)\n        cross_entropy_n = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=cls_score, labels=label)\n\n        # bounding box regression L1 loss\n        bbox_pred = self.get_output(\'bbox_pred\') # (R, (C+1)x4)\n        bbox_targets = self.get_output(\'roi-data\')[2] # (R, (C+1)x4)\n        # each element is {0, 1}, represents background (0), objects (1)\n        bbox_inside_weights = self.get_output(\'roi-data\')[3] # (R, (C+1)x4)\n        bbox_outside_weights = self.get_output(\'roi-data\')[4] # (R, (C+1)x4)\n\n        loss_box_n = tf.reduce_sum( \\\n            bbox_outside_weights * self.smooth_l1_dist(bbox_inside_weights * (bbox_pred - bbox_targets)), \\\n            axis=[1])\n\n        loss_n = loss_box_n + cross_entropy_n\n        loss_n = tf.reshape(loss_n, [-1])\n\n        # if ohem:\n        #     # top_k = 100\n        #     top_k = tf.minimum(tf.shape(loss_n)[0] / 2, 500)\n        #     loss_n, top_k_indices = tf.nn.top_k(loss_n, k=top_k, sorted=False)\n        #     loss_box_n = tf.gather(loss_box_n, top_k_indices)\n        #     cross_entropy_n = tf.gather(cross_entropy_n, top_k_indices)\n\n        loss_box = tf.reduce_mean(loss_box_n)\n        cross_entropy = tf.reduce_mean(cross_entropy_n)\n\n        loss = cross_entropy + loss_box + rpn_cross_entropy + rpn_loss_box\n\n        # add regularizer\n        if cfg.TRAIN.WEIGHT_DECAY > 0:\n            regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n            loss = tf.add_n(regularization_losses) + loss\n\n        return loss, cross_entropy, loss_box, rpn_cross_entropy, rpn_loss_box'"
lib/nms/__init__.py,0,b''
lib/nms/py_cpu_nms.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\n\ndef py_cpu_nms(dets, thresh):\n    """"""Pure Python NMS baseline.""""""\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n'"
lib/psroi_pooling_layer/__init__.py,0,b'# --------------------------------------------------------\n# R-FCN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Revised by Minyue Jiang\n# --------------------------------------------------------\n'
lib/psroi_pooling_layer/psroi_pooling_op.py,1,"b""import tensorflow as tf\nimport os.path as osp\n\nfilename = osp.join(osp.dirname(__file__), 'psroi_pooling.so')\n_psroi_pooling_module = tf.load_op_library(filename)\npsroi_pool = _psroi_pooling_module.psroi_pool\npsroi_pool_grad = _psroi_pooling_module.psroi_pool_grad"""
lib/psroi_pooling_layer/psroi_pooling_op_grad.py,2,"b'import tensorflow as tf\nfrom tensorflow.python.framework import ops\nimport psroi_pooling_op\nimport pdb\n\n\n@tf.RegisterShape(""PSROIPool"")\ndef _psroi_pool_shape(op):\n  """"""Shape function for the PSROIPool op.\n\n  """"""\n  dims_data = op.inputs[0].get_shape().as_list()\n  channels = dims_data[3]\n  dims_rois = op.inputs[1].get_shape().as_list()\n  num_rois = dims_rois[0]\n  output_dim = op.get_attr(\'output_dim\')\n  group_size  = op.get_attr(\'group_size\')\n  pooled_height = group_size\n  pooled_width = group_size\n\n  output_shape = tf.TensorShape([num_rois, pooled_height, pooled_width, output_dim])\n  return [output_shape, output_shape]\n\n@ops.RegisterGradient(""PSROIPool"")\ndef _psroi_pool_grad(op, grad, _):\n  """"""The gradients for `PSROI_pool`.\n  Args:\n    op: The `roi_pool` `Operation` that we are differentiating, which we can use\n      to find the inputs and outputs of the original op.\n    grad: Gradient with respect to the output of the `roi_pool` op.\n  Returns:\n    Gradients with respect to the input of `zero_out`.\n  """"""\n  \n  data = op.inputs[0]\n  rois = op.inputs[1]\n  mapping_channel = op.outputs[1]\n  spatial_scale = op.get_attr(\'spatial_scale\')\n\n  # compute gradient\n  #data_grad = psroi_pooling_op.psroi_pool_grad(data, rois, argmax, grad, pooled_height, pooled_width, spatial_scale)\n  data_grad = psroi_pooling_op.psroi_pool_grad(data, rois, mapping_channel, grad, spatial_scale)  \n\n  return [data_grad, None]  # List of one Tensor, since we have one input\n\n'"
lib/psroi_pooling_layer/psroi_pooling_op_test.py,3,"b'import tensorflow as tf\nimport numpy as np\nimport psroi_pooling_op\nimport psroi_pooling_op_grad\nimport pdb\n\npdb.set_trace()\n\nrois = tf.convert_to_tensor([ [0, 0, 0, 4, 4]], dtype=tf.float32)\nhh=tf.convert_to_tensor(np.random.rand(1,5,5,25),dtype=tf.float32)\n[y2, channels] = psroi_pooling_op.psroi_pool(hh, rois, output_dim=1, group_size=5, spatial_scale=1.0)\n\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\nprint( sess.run(hh))\nprint( sess.run(y2))\npdb.set_trace()\n'"
lib/pycocotools/__init__.py,0,"b""__author__ = 'tylin'\n"""
lib/pycocotools/coco.py,0,"b'__author__ = \'tylin\'\n__version__ = \'1.0.1\'\n# Interface for accessing the Microsoft COCO dataset.\n\n# Microsoft COCO is a large image dataset designed for object detection,\n# segmentation, and caption generation. pycocotools is a Python API that\n# assists in loading, parsing and visualizing the annotations in COCO.\n# Please visit http://mscoco.org/ for more information on COCO, including\n# for the data, paper, and tutorials. The exact format of the annotations\n# is also described on the COCO website. For example usage of the pycocotools\n# please see pycocotools_demo.ipynb. In addition to this API, please download both\n# the COCO images and annotations in order to run the demo.\n\n# An alternative to using the API is to load the annotations directly\n# into Python dictionary\n# Using the API provides additional utility functions. Note that this API\n# supports both *instance* and *caption* annotations. In the case of\n# captions not all functions are defined (e.g. categories are undefined).\n\n# The following API functions are defined:\n#  COCO       - COCO api class that loads COCO annotation file and prepare data structures.\n#  decodeMask - Decode binary mask M encoded via run-length encoding.\n#  encodeMask - Encode binary mask M using run-length encoding.\n#  getAnnIds  - Get ann ids that satisfy given filter conditions.\n#  getCatIds  - Get cat ids that satisfy given filter conditions.\n#  getImgIds  - Get img ids that satisfy given filter conditions.\n#  loadAnns   - Load anns with the specified ids.\n#  loadCats   - Load cats with the specified ids.\n#  loadImgs   - Load imgs with the specified ids.\n#  segToMask  - Convert polygon segmentation to binary mask.\n#  showAnns   - Display the specified annotations.\n#  loadRes    - Load algorithm results and create API for accessing them.\n#  download   - Download COCO images from mscoco.org server.\n# Throughout the API ""ann""=annotation, ""cat""=category, and ""img""=image.\n# Help on each functions can be accessed by: ""help COCO>function"".\n\n# See also COCO>decodeMask,\n# COCO>encodeMask, COCO>getAnnIds, COCO>getCatIds,\n# COCO>getImgIds, COCO>loadAnns, COCO>loadCats,\n# COCO>loadImgs, COCO>segToMask, COCO>showAnns\n\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2014.\n# Licensed under the Simplified BSD License [see bsd.txt]\n\nimport json\nimport datetime\nimport time\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import PatchCollection\nfrom matplotlib.patches import Polygon\nimport numpy as np\nfrom skimage.draw import polygon\nimport urllib\nimport copy\nimport itertools\nimport mask\nimport os\n\nclass COCO:\n    def __init__(self, annotation_file=None):\n        """"""\n        Constructor of Microsoft COCO helper class for reading and visualizing annotations.\n        :param annotation_file (str): location of annotation file\n        :param image_folder (str): location to the folder that hosts images.\n        :return:\n        """"""\n        # load dataset\n        self.dataset = {}\n        self.anns = []\n        self.imgToAnns = {}\n        self.catToImgs = {}\n        self.imgs = {}\n        self.cats = {}\n        if not annotation_file == None:\n            print \'loading annotations into memory...\'\n            tic = time.time()\n            dataset = json.load(open(annotation_file, \'r\'))\n            print \'Done (t=%0.2fs)\'%(time.time()- tic)\n            self.dataset = dataset\n            self.createIndex()\n\n    def createIndex(self):\n        # create index\n        print \'creating index...\'\n        anns = {}\n        imgToAnns = {}\n        catToImgs = {}\n        cats = {}\n        imgs = {}\n        if \'annotations\' in self.dataset:\n            imgToAnns = {ann[\'image_id\']: [] for ann in self.dataset[\'annotations\']}\n            anns =      {ann[\'id\']:       [] for ann in self.dataset[\'annotations\']}\n            for ann in self.dataset[\'annotations\']:\n                imgToAnns[ann[\'image_id\']] += [ann]\n                anns[ann[\'id\']] = ann\n\n        if \'images\' in self.dataset:\n            imgs      = {im[\'id\']: {} for im in self.dataset[\'images\']}\n            for img in self.dataset[\'images\']:\n                imgs[img[\'id\']] = img\n\n        if \'categories\' in self.dataset:\n            cats = {cat[\'id\']: [] for cat in self.dataset[\'categories\']}\n            for cat in self.dataset[\'categories\']:\n                cats[cat[\'id\']] = cat\n            catToImgs = {cat[\'id\']: [] for cat in self.dataset[\'categories\']}\n            if \'annotations\' in self.dataset:\n                for ann in self.dataset[\'annotations\']:\n                    catToImgs[ann[\'category_id\']] += [ann[\'image_id\']]\n\n        print \'index created!\'\n\n        # create class members\n        self.anns = anns\n        self.imgToAnns = imgToAnns\n        self.catToImgs = catToImgs\n        self.imgs = imgs\n        self.cats = cats\n\n    def info(self):\n        """"""\n        Print information about the annotation file.\n        :return:\n        """"""\n        for key, value in self.dataset[\'info\'].items():\n            print \'%s: %s\'%(key, value)\n\n    def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n        """"""\n        Get ann ids that satisfy given filter conditions. default skips that filter\n        :param imgIds  (int array)     : get anns for given imgs\n               catIds  (int array)     : get anns for given cats\n               areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n               iscrowd (boolean)       : get anns for given crowd label (False or True)\n        :return: ids (int array)       : integer array of ann ids\n        """"""\n        imgIds = imgIds if type(imgIds) == list else [imgIds]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(imgIds) == len(catIds) == len(areaRng) == 0:\n            anns = self.dataset[\'annotations\']\n        else:\n            if not len(imgIds) == 0:\n                # this can be changed by defaultdict\n                lists = [self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns]\n                anns = list(itertools.chain.from_iterable(lists))\n            else:\n                anns = self.dataset[\'annotations\']\n            anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann[\'category_id\'] in catIds]\n            anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann[\'area\'] > areaRng[0] and ann[\'area\'] < areaRng[1]]\n        if not iscrowd == None:\n            ids = [ann[\'id\'] for ann in anns if ann[\'iscrowd\'] == iscrowd]\n        else:\n            ids = [ann[\'id\'] for ann in anns]\n        return ids\n\n    def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n        """"""\n        filtering parameters. default skips that filter.\n        :param catNms (str array)  : get cats for given cat names\n        :param supNms (str array)  : get cats for given supercategory names\n        :param catIds (int array)  : get cats for given cat ids\n        :return: ids (int array)   : integer array of cat ids\n        """"""\n        catNms = catNms if type(catNms) == list else [catNms]\n        supNms = supNms if type(supNms) == list else [supNms]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(catNms) == len(supNms) == len(catIds) == 0:\n            cats = self.dataset[\'categories\']\n        else:\n            cats = self.dataset[\'categories\']\n            cats = cats if len(catNms) == 0 else [cat for cat in cats if cat[\'name\']          in catNms]\n            cats = cats if len(supNms) == 0 else [cat for cat in cats if cat[\'supercategory\'] in supNms]\n            cats = cats if len(catIds) == 0 else [cat for cat in cats if cat[\'id\']            in catIds]\n        ids = [cat[\'id\'] for cat in cats]\n        return ids\n\n    def getImgIds(self, imgIds=[], catIds=[]):\n        \'\'\'\n        Get img ids that satisfy given filter conditions.\n        :param imgIds (int array) : get imgs for given ids\n        :param catIds (int array) : get imgs with all given cats\n        :return: ids (int array)  : integer array of img ids\n        \'\'\'\n        imgIds = imgIds if type(imgIds) == list else [imgIds]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(imgIds) == len(catIds) == 0:\n            ids = self.imgs.keys()\n        else:\n            ids = set(imgIds)\n            for i, catId in enumerate(catIds):\n                if i == 0 and len(ids) == 0:\n                    ids = set(self.catToImgs[catId])\n                else:\n                    ids &= set(self.catToImgs[catId])\n        return list(ids)\n\n    def loadAnns(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying anns\n        :return: anns (object array) : loaded ann objects\n        """"""\n        if type(ids) == list:\n            return [self.anns[id] for id in ids]\n        elif type(ids) == int:\n            return [self.anns[ids]]\n\n    def loadCats(self, ids=[]):\n        """"""\n        Load cats with the specified ids.\n        :param ids (int array)       : integer ids specifying cats\n        :return: cats (object array) : loaded cat objects\n        """"""\n        if type(ids) == list:\n            return [self.cats[id] for id in ids]\n        elif type(ids) == int:\n            return [self.cats[ids]]\n\n    def loadImgs(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying img\n        :return: imgs (object array) : loaded img objects\n        """"""\n        if type(ids) == list:\n            return [self.imgs[id] for id in ids]\n        elif type(ids) == int:\n            return [self.imgs[ids]]\n\n    def showAnns(self, anns):\n        """"""\n        Display the specified annotations.\n        :param anns (array of object): annotations to display\n        :return: None\n        """"""\n        if len(anns) == 0:\n            return 0\n        if \'segmentation\' in anns[0]:\n            datasetType = \'instances\'\n        elif \'caption\' in anns[0]:\n            datasetType = \'captions\'\n        if datasetType == \'instances\':\n            ax = plt.gca()\n            polygons = []\n            color = []\n            for ann in anns:\n                c = np.random.random((1, 3)).tolist()[0]\n                if type(ann[\'segmentation\']) == list:\n                    # polygon\n                    for seg in ann[\'segmentation\']:\n                        poly = np.array(seg).reshape((len(seg)/2, 2))\n                        polygons.append(Polygon(poly, True,alpha=0.4))\n                        color.append(c)\n                else:\n                    # mask\n                    t = self.imgs[ann[\'image_id\']]\n                    if type(ann[\'segmentation\'][\'counts\']) == list:\n                        rle = mask.frPyObjects([ann[\'segmentation\']], t[\'height\'], t[\'width\'])\n                    else:\n                        rle = [ann[\'segmentation\']]\n                    m = mask.decode(rle)\n                    img = np.ones( (m.shape[0], m.shape[1], 3) )\n                    if ann[\'iscrowd\'] == 1:\n                        color_mask = np.array([2.0,166.0,101.0])/255\n                    if ann[\'iscrowd\'] == 0:\n                        color_mask = np.random.random((1, 3)).tolist()[0]\n                    for i in range(3):\n                        img[:,:,i] = color_mask[i]\n                    ax.imshow(np.dstack( (img, m*0.5) ))\n            p = PatchCollection(polygons, facecolors=color, edgecolors=(0,0,0,1), linewidths=3, alpha=0.4)\n            ax.add_collection(p)\n        elif datasetType == \'captions\':\n            for ann in anns:\n                print ann[\'caption\']\n\n    def loadRes(self, resFile):\n        """"""\n        Load result file and return a result api object.\n        :param   resFile (str)     : file name of result file\n        :return: res (obj)         : result api object\n        """"""\n        res = COCO()\n        res.dataset[\'images\'] = [img for img in self.dataset[\'images\']]\n        # res.dataset[\'info\'] = copy.deepcopy(self.dataset[\'info\'])\n        # res.dataset[\'licenses\'] = copy.deepcopy(self.dataset[\'licenses\'])\n\n        print \'Loading and preparing results...     \'\n        tic = time.time()\n        anns    = json.load(open(resFile))\n        assert type(anns) == list, \'results in not an array of objects\'\n        annsImgIds = [ann[\'image_id\'] for ann in anns]\n        assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n               \'Results do not correspond to current coco set\'\n        if \'caption\' in anns[0]:\n            imgIds = set([img[\'id\'] for img in res.dataset[\'images\']]) & set([ann[\'image_id\'] for ann in anns])\n            res.dataset[\'images\'] = [img for img in res.dataset[\'images\'] if img[\'id\'] in imgIds]\n            for id, ann in enumerate(anns):\n                ann[\'id\'] = id+1\n        elif \'bbox\' in anns[0] and not anns[0][\'bbox\'] == []:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                bb = ann[\'bbox\']\n                x1, x2, y1, y2 = [bb[0], bb[0]+bb[2], bb[1], bb[1]+bb[3]]\n                if not \'segmentation\' in ann:\n                    ann[\'segmentation\'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n                ann[\'area\'] = bb[2]*bb[3]\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'segmentation\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                # now only support compressed RLE format as segmentation results\n                ann[\'area\'] = mask.area([ann[\'segmentation\']])[0]\n                if not \'bbox\' in ann:\n                    ann[\'bbox\'] = mask.toBbox([ann[\'segmentation\']])[0]\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        print \'DONE (t=%0.2fs)\'%(time.time()- tic)\n\n        res.dataset[\'annotations\'] = anns\n        res.createIndex()\n        return res\n\n    def download( self, tarDir = None, imgIds = [] ):\n        \'\'\'\n        Download COCO images from mscoco.org server.\n        :param tarDir (str): COCO results directory name\n               imgIds (list): images to be downloaded\n        :return:\n        \'\'\'\n        if tarDir is None:\n            print \'Please specify target directory\'\n            return -1\n        if len(imgIds) == 0:\n            imgs = self.imgs.values()\n        else:\n            imgs = self.loadImgs(imgIds)\n        N = len(imgs)\n        if not os.path.exists(tarDir):\n            os.makedirs(tarDir)\n        for i, img in enumerate(imgs):\n            tic = time.time()\n            fname = os.path.join(tarDir, img[\'file_name\'])\n            if not os.path.exists(fname):\n                urllib.urlretrieve(img[\'coco_url\'], fname)\n            print \'downloaded %d/%d images (t=%.1fs)\'%(i, N, time.time()- tic)\n'"
lib/pycocotools/cocoeval.py,0,"b'__author__ = \'tsungyi\'\n\nimport numpy as np\nimport datetime\nimport time\nfrom collections import defaultdict\nimport mask\nimport copy\n\nclass COCOeval:\n    # Interface for evaluating detection on the Microsoft COCO dataset.\n    #\n    # The usage for CocoEval is as follows:\n    #  cocoGt=..., cocoDt=...       # load dataset and results\n    #  E = CocoEval(cocoGt,cocoDt); # initialize CocoEval object\n    #  E.params.recThrs = ...;      # set parameters as desired\n    #  E.evaluate();                # run per image evaluation\n    #  E.accumulate();              # accumulate per image results\n    #  E.summarize();               # display summary metrics of results\n    # For example usage see evalDemo.m and http://mscoco.org/.\n    #\n    # The evaluation parameters are as follows (defaults in brackets):\n    #  imgIds     - [all] N img ids to use for evaluation\n    #  catIds     - [all] K cat ids to use for evaluation\n    #  iouThrs    - [.5:.05:.95] T=10 IoU thresholds for evaluation\n    #  recThrs    - [0:.01:1] R=101 recall thresholds for evaluation\n    #  areaRng    - [...] A=4 object area ranges for evaluation\n    #  maxDets    - [1 10 100] M=3 thresholds on max detections per image\n    #  useSegm    - [1] if true evaluate against ground-truth segments\n    #  useCats    - [1] if true use category labels for evaluation    # Note: if useSegm=0 the evaluation is run on bounding boxes.\n    # Note: if useCats=0 category labels are ignored as in proposal scoring.\n    # Note: multiple areaRngs [Ax2] and maxDets [Mx1] can be specified.\n    #\n    # evaluate(): evaluates detections on every image and every category and\n    # concats the results into the ""evalImgs"" with fields:\n    #  dtIds      - [1xD] id for each of the D detections (dt)\n    #  gtIds      - [1xG] id for each of the G ground truths (gt)\n    #  dtMatches  - [TxD] matching gt id at each IoU or 0\n    #  gtMatches  - [TxG] matching dt id at each IoU or 0\n    #  dtScores   - [1xD] confidence of each dt\n    #  gtIgnore   - [1xG] ignore flag for each gt\n    #  dtIgnore   - [TxD] ignore flag for each dt at each IoU\n    #\n    # accumulate(): accumulates the per-image, per-category evaluation\n    # results in ""evalImgs"" into the dictionary ""eval"" with fields:\n    #  params     - parameters used for evaluation\n    #  date       - date evaluation was performed\n    #  counts     - [T,R,K,A,M] parameter dimensions (see above)\n    #  precision  - [TxRxKxAxM] precision for every evaluation setting\n    #  recall     - [TxKxAxM] max recall for every evaluation setting\n    # Note: precision and recall==-1 for settings with no gt objects.\n    #\n    # See also coco, mask, pycocoDemo, pycocoEvalDemo\n    #\n    # Microsoft COCO Toolbox.      version 2.0\n    # Data, paper, and tutorials available at:  http://mscoco.org/\n    # Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n    # Licensed under the Simplified BSD License [see coco/license.txt]\n    def __init__(self, cocoGt=None, cocoDt=None):\n        \'\'\'\n        Initialize CocoEval using coco APIs for gt and dt\n        :param cocoGt: coco object with ground truth annotations\n        :param cocoDt: coco object with detection results\n        :return: None\n        \'\'\'\n        self.cocoGt   = cocoGt              # ground truth COCO API\n        self.cocoDt   = cocoDt              # detections COCO API\n        self.params   = {}                  # evaluation parameters\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results [KxAxI] elements\n        self.eval     = {}                  # accumulated evaluation results\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        self.params = Params()              # parameters\n        self._paramsEval = {}               # parameters for evaluation\n        self.stats = []                     # result summarization\n        self.ious = {}                      # ious between all gts and dts\n        if not cocoGt is None:\n            self.params.imgIds = sorted(cocoGt.getImgIds())\n            self.params.catIds = sorted(cocoGt.getCatIds())\n\n\n    def _prepare(self):\n        \'\'\'\n        Prepare ._gts and ._dts for evaluation based on params\n        :return: None\n        \'\'\'\n        #\n        def _toMask(objs, coco):\n            # modify segmentation by reference\n            for obj in objs:\n                t = coco.imgs[obj[\'image_id\']]\n                if type(obj[\'segmentation\']) == list:\n                    if type(obj[\'segmentation\'][0]) == dict:\n                        print \'debug\'\n                    obj[\'segmentation\'] = mask.frPyObjects(obj[\'segmentation\'],t[\'height\'],t[\'width\'])\n                    if len(obj[\'segmentation\']) == 1:\n                        obj[\'segmentation\'] = obj[\'segmentation\'][0]\n                    else:\n                        # an object can have multiple polygon regions\n                        # merge them into one RLE mask\n                        obj[\'segmentation\'] = mask.merge(obj[\'segmentation\'])\n                elif type(obj[\'segmentation\']) == dict and type(obj[\'segmentation\'][\'counts\']) == list:\n                    obj[\'segmentation\'] = mask.frPyObjects([obj[\'segmentation\']],t[\'height\'],t[\'width\'])[0]\n                elif type(obj[\'segmentation\']) == dict and \\\n                     type(obj[\'segmentation\'][\'counts\'] == unicode or type(obj[\'segmentation\'][\'counts\']) == str):\n                    pass\n                else:\n                    raise Exception(\'segmentation format not supported.\')\n        p = self.params\n        if p.useCats:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n        else:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds))\n\n        if p.useSegm:\n            _toMask(gts, self.cocoGt)\n            _toMask(dts, self.cocoDt)\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        for gt in gts:\n            self._gts[gt[\'image_id\'], gt[\'category_id\']].append(gt)\n        for dt in dts:\n            self._dts[dt[\'image_id\'], dt[\'category_id\']].append(dt)\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results\n        self.eval     = {}                  # accumulated evaluation results\n\n    def evaluate(self):\n        \'\'\'\n        Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n        :return: None\n        \'\'\'\n        tic = time.time()\n        print \'Running per image evaluation...      \'\n        p = self.params\n        p.imgIds = list(np.unique(p.imgIds))\n        if p.useCats:\n            p.catIds = list(np.unique(p.catIds))\n        p.maxDets = sorted(p.maxDets)\n        self.params=p\n\n        self._prepare()\n        # loop through images, area range, max detection number\n        catIds = p.catIds if p.useCats else [-1]\n\n        computeIoU = self.computeIoU\n        self.ious = {(imgId, catId): computeIoU(imgId, catId) \\\n                        for imgId in p.imgIds\n                        for catId in catIds}\n\n        evaluateImg = self.evaluateImg\n        maxDet = p.maxDets[-1]\n        self.evalImgs = [evaluateImg(imgId, catId, areaRng, maxDet)\n                 for catId in catIds\n                 for areaRng in p.areaRng\n                 for imgId in p.imgIds\n             ]\n        self._paramsEval = copy.deepcopy(self.params)\n        toc = time.time()\n        print \'DONE (t=%0.2fs).\'%(toc-tic)\n\n    def computeIoU(self, imgId, catId):\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return []\n        dt = sorted(dt, key=lambda x: -x[\'score\'])\n        if len(dt) > p.maxDets[-1]:\n            dt=dt[0:p.maxDets[-1]]\n\n        if p.useSegm:\n            g = [g[\'segmentation\'] for g in gt]\n            d = [d[\'segmentation\'] for d in dt]\n        else:\n            g = [g[\'bbox\'] for g in gt]\n            d = [d[\'bbox\'] for d in dt]\n\n        # compute iou between each dt and gt region\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        ious = mask.iou(d,g,iscrowd)\n        return ious\n\n    def evaluateImg(self, imgId, catId, aRng, maxDet):\n        \'\'\'\n        perform evaluation for single category and image\n        :return: dict (single image results)\n        \'\'\'\n        #\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return None\n\n        for g in gt:\n            if \'ignore\' not in g:\n                g[\'ignore\'] = 0\n            if g[\'iscrowd\'] == 1 or g[\'ignore\'] or (g[\'area\']<aRng[0] or g[\'area\']>aRng[1]):\n                g[\'_ignore\'] = 1\n            else:\n                g[\'_ignore\'] = 0\n\n        # sort dt highest score first, sort gt ignore last\n        # gt = sorted(gt, key=lambda x: x[\'_ignore\'])\n        gtind = [ind for (ind, g) in sorted(enumerate(gt), key=lambda (ind, g): g[\'_ignore\']) ]\n\n        gt = [gt[ind] for ind in gtind]\n        dt = sorted(dt, key=lambda x: -x[\'score\'])[0:maxDet]\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        # load computed ious\n        N_iou = len(self.ious[imgId, catId])\n        ious = self.ious[imgId, catId][0:maxDet, np.array(gtind)] if N_iou >0 else self.ious[imgId, catId]\n\n        T = len(p.iouThrs)\n        G = len(gt)\n        D = len(dt)\n        gtm  = np.zeros((T,G))\n        dtm  = np.zeros((T,D))\n        gtIg = np.array([g[\'_ignore\'] for g in gt])\n        dtIg = np.zeros((T,D))\n        if not len(ious)==0:\n            for tind, t in enumerate(p.iouThrs):\n                for dind, d in enumerate(dt):\n                    # information about best match so far (m=-1 -> unmatched)\n                    iou = min([t,1-1e-10])\n                    m   = -1\n                    for gind, g in enumerate(gt):\n                        # if this gt already matched, and not a crowd, continue\n                        if gtm[tind,gind]>0 and not iscrowd[gind]:\n                            continue\n                        # if dt matched to reg gt, and on ignore gt, stop\n                        if m>-1 and gtIg[m]==0 and gtIg[gind]==1:\n                            break\n                        # continue to next gt unless better match made\n                        if ious[dind,gind] < iou:\n                            continue\n                        # match successful and best so far, store appropriately\n                        iou=ious[dind,gind]\n                        m=gind\n                    # if match made store id of match for both dt and gt\n                    if m ==-1:\n                        continue\n                    dtIg[tind,dind] = gtIg[m]\n                    dtm[tind,dind]  = gt[m][\'id\']\n                    gtm[tind,m]     = d[\'id\']\n        # set unmatched detections outside of area range to ignore\n        a = np.array([d[\'area\']<aRng[0] or d[\'area\']>aRng[1] for d in dt]).reshape((1, len(dt)))\n        dtIg = np.logical_or(dtIg, np.logical_and(dtm==0, np.repeat(a,T,0)))\n        # store results for given image and category\n        return {\n                \'image_id\':     imgId,\n                \'category_id\':  catId,\n                \'aRng\':         aRng,\n                \'maxDet\':       maxDet,\n                \'dtIds\':        [d[\'id\'] for d in dt],\n                \'gtIds\':        [g[\'id\'] for g in gt],\n                \'dtMatches\':    dtm,\n                \'gtMatches\':    gtm,\n                \'dtScores\':     [d[\'score\'] for d in dt],\n                \'gtIgnore\':     gtIg,\n                \'dtIgnore\':     dtIg,\n            }\n\n    def accumulate(self, p = None):\n        \'\'\'\n        Accumulate per image evaluation results and store the result in self.eval\n        :param p: input params for evaluation\n        :return: None\n        \'\'\'\n        print \'Accumulating evaluation results...   \'\n        tic = time.time()\n        if not self.evalImgs:\n            print \'Please run evaluate() first\'\n        # allows input customized parameters\n        if p is None:\n            p = self.params\n        p.catIds = p.catIds if p.useCats == 1 else [-1]\n        T           = len(p.iouThrs)\n        R           = len(p.recThrs)\n        K           = len(p.catIds) if p.useCats else 1\n        A           = len(p.areaRng)\n        M           = len(p.maxDets)\n        precision   = -np.ones((T,R,K,A,M)) # -1 for the precision of absent categories\n        recall      = -np.ones((T,K,A,M))\n\n        # create dictionary for future indexing\n        _pe = self._paramsEval\n        catIds = _pe.catIds if _pe.useCats else [-1]\n        setK = set(catIds)\n        setA = set(map(tuple, _pe.areaRng))\n        setM = set(_pe.maxDets)\n        setI = set(_pe.imgIds)\n        # get inds to evaluate\n        k_list = [n for n, k in enumerate(p.catIds)  if k in setK]\n        m_list = [m for n, m in enumerate(p.maxDets) if m in setM]\n        a_list = [n for n, a in enumerate(map(lambda x: tuple(x), p.areaRng)) if a in setA]\n        i_list = [n for n, i in enumerate(p.imgIds)  if i in setI]\n        # K0 = len(_pe.catIds)\n        I0 = len(_pe.imgIds)\n        A0 = len(_pe.areaRng)\n        # retrieve E at each category, area range, and max number of detections\n        for k, k0 in enumerate(k_list):\n            Nk = k0*A0*I0\n            for a, a0 in enumerate(a_list):\n                Na = a0*I0\n                for m, maxDet in enumerate(m_list):\n                    E = [self.evalImgs[Nk+Na+i] for i in i_list]\n                    E = filter(None, E)\n                    if len(E) == 0:\n                        continue\n                    dtScores = np.concatenate([e[\'dtScores\'][0:maxDet] for e in E])\n\n                    # different sorting method generates slightly different results.\n                    # mergesort is used to be consistent as Matlab implementation.\n                    inds = np.argsort(-dtScores, kind=\'mergesort\')\n\n                    dtm  = np.concatenate([e[\'dtMatches\'][:,0:maxDet] for e in E], axis=1)[:,inds]\n                    dtIg = np.concatenate([e[\'dtIgnore\'][:,0:maxDet]  for e in E], axis=1)[:,inds]\n                    gtIg = np.concatenate([e[\'gtIgnore\']  for e in E])\n                    npig = len([ig for ig in gtIg if ig == 0])\n                    if npig == 0:\n                        continue\n                    tps = np.logical_and(               dtm,  np.logical_not(dtIg) )\n                    fps = np.logical_and(np.logical_not(dtm), np.logical_not(dtIg) )\n\n                    tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\n                    fp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)\n                    for t, (tp, fp) in enumerate(zip(tp_sum, fp_sum)):\n                        tp = np.array(tp)\n                        fp = np.array(fp)\n                        nd = len(tp)\n                        rc = tp / npig\n                        pr = tp / (fp+tp+np.spacing(1))\n                        q  = np.zeros((R,))\n\n                        if nd:\n                            recall[t,k,a,m] = rc[-1]\n                        else:\n                            recall[t,k,a,m] = 0\n\n                        # numpy is slow without cython optimization for accessing elements\n                        # use python array gets significant speed improvement\n                        pr = pr.tolist(); q = q.tolist()\n\n                        for i in range(nd-1, 0, -1):\n                            if pr[i] > pr[i-1]:\n                                pr[i-1] = pr[i]\n\n                        inds = np.searchsorted(rc, p.recThrs)\n                        try:\n                            for ri, pi in enumerate(inds):\n                                q[ri] = pr[pi]\n                        except:\n                            pass\n                        precision[t,:,k,a,m] = np.array(q)\n        self.eval = {\n            \'params\': p,\n            \'counts\': [T, R, K, A, M],\n            \'date\': datetime.datetime.now().strftime(""%Y-%m-%d %H:%M:%S""),\n            \'precision\': precision,\n            \'recall\':   recall,\n        }\n        toc = time.time()\n        print \'DONE (t=%0.2fs).\'%( toc-tic )\n\n    def summarize(self):\n        \'\'\'\n        Compute and display summary metrics for evaluation results.\n        Note this functin can *only* be applied on the default parameter setting\n        \'\'\'\n        def _summarize( ap=1, iouThr=None, areaRng=\'all\', maxDets=100 ):\n            p = self.params\n            iStr        = \' {:<18} {} @[ IoU={:<9} | area={:>6} | maxDets={:>3} ] = {}\'\n            titleStr    = \'Average Precision\' if ap == 1 else \'Average Recall\'\n            typeStr     = \'(AP)\' if ap==1 else \'(AR)\'\n            iouStr      = \'%0.2f:%0.2f\'%(p.iouThrs[0], p.iouThrs[-1]) if iouThr is None else \'%0.2f\'%(iouThr)\n            areaStr     = areaRng\n            maxDetsStr  = \'%d\'%(maxDets)\n\n            aind = [i for i, aRng in enumerate([\'all\', \'small\', \'medium\', \'large\']) if aRng == areaRng]\n            mind = [i for i, mDet in enumerate([1, 10, 100]) if mDet == maxDets]\n            if ap == 1:\n                # dimension of precision: [TxRxKxAxM]\n                s = self.eval[\'precision\']\n                # IoU\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                # areaRng\n                s = s[:,:,:,aind,mind]\n            else:\n                # dimension of recall: [TxKxAxM]\n                s = self.eval[\'recall\']\n                s = s[:,:,aind,mind]\n            if len(s[s>-1])==0:\n                mean_s = -1\n            else:\n                mean_s = np.mean(s[s>-1])\n            print iStr.format(titleStr, typeStr, iouStr, areaStr, maxDetsStr, \'%.3f\'%(float(mean_s)))\n            return mean_s\n\n        if not self.eval:\n            raise Exception(\'Please run accumulate() first\')\n        self.stats = np.zeros((12,))\n        self.stats[0] = _summarize(1)\n        self.stats[1] = _summarize(1,iouThr=.5)\n        self.stats[2] = _summarize(1,iouThr=.75)\n        self.stats[3] = _summarize(1,areaRng=\'small\')\n        self.stats[4] = _summarize(1,areaRng=\'medium\')\n        self.stats[5] = _summarize(1,areaRng=\'large\')\n        self.stats[6] = _summarize(0,maxDets=1)\n        self.stats[7] = _summarize(0,maxDets=10)\n        self.stats[8] = _summarize(0,maxDets=100)\n        self.stats[9]  = _summarize(0,areaRng=\'small\')\n        self.stats[10] = _summarize(0,areaRng=\'medium\')\n        self.stats[11] = _summarize(0,areaRng=\'large\')\n\n    def __str__(self):\n        self.summarize()\n\nclass Params:\n    \'\'\'\n    Params for coco evaluation api\n    \'\'\'\n    def __init__(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95-.5)/.05)+1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00-.0)/.01)+1, endpoint=True)\n        self.maxDets = [1,10,100]\n        self.areaRng = [ [0**2,1e5**2], [0**2, 32**2], [32**2, 96**2], [96**2, 1e5**2] ]\n        self.useSegm = 0\n        self.useCats = 1'"
lib/pycocotools/mask.py,0,"b'__author__ = \'tsungyi\'\n\nfrom . import _mask\n\n# Interface for manipulating masks stored in RLE format.\n#\n# RLE is a simple yet efficient format for storing binary masks. RLE\n# first divides a vector (or vectorized image) into a series of piecewise\n# constant regions and then for each piece simply stores the length of\n# that piece. For example, given M=[0 0 1 1 1 0 1] the RLE counts would\n# be [2 3 1 1], or for M=[1 1 1 1 1 1 0] the counts would be [0 6 1]\n# (note that the odd counts are always the numbers of zeros). Instead of\n# storing the counts directly, additional compression is achieved with a\n# variable bitrate representation based on a common scheme called LEB128.\n#\n# Compression is greatest given large piecewise constant regions.\n# Specifically, the size of the RLE is proportional to the number of\n# *boundaries* in M (or for an image the number of boundaries in the y\n# direction). Assuming fairly simple shapes, the RLE representation is\n# O(sqrt(n)) where n is number of pixels in the object. Hence space usage\n# is substantially lower, especially for large simple objects (large n).\n#\n# Many common operations on masks can be computed directly using the RLE\n# (without need for decoding). This includes computations such as area,\n# union, intersection, etc. All of these operations are linear in the\n# size of the RLE, in other words they are O(sqrt(n)) where n is the area\n# of the object. Computing these operations on the original mask is O(n).\n# Thus, using the RLE can result in substantial computational savings.\n#\n# The following API functions are defined:\n#  encode         - Encode binary masks using RLE.\n#  decode         - Decode binary masks encoded via RLE.\n#  merge          - Compute union or intersection of encoded masks.\n#  iou            - Compute intersection over union between masks.\n#  area           - Compute area of encoded masks.\n#  toBbox         - Get bounding boxes surrounding encoded masks.\n#  frPyObjects    - Convert polygon, bbox, and uncompressed RLE to encoded RLE mask.\n#\n# Usage:\n#  Rs     = encode( masks )\n#  masks  = decode( Rs )\n#  R      = merge( Rs, intersect=false )\n#  o      = iou( dt, gt, iscrowd )\n#  a      = area( Rs )\n#  bbs    = toBbox( Rs )\n#  Rs     = frPyObjects( [pyObjects], h, w )\n#\n# In the API the following formats are used:\n#  Rs      - [dict] Run-length encoding of binary masks\n#  R       - dict Run-length encoding of binary mask\n#  masks   - [hxwxn] Binary mask(s) (must have type np.ndarray(dtype=uint8) in column-major order)\n#  iscrowd - [nx1] list of np.ndarray. 1 indicates corresponding gt image has crowd region to ignore\n#  bbs     - [nx4] Bounding box(es) stored as [x y w h]\n#  poly    - Polygon stored as [[x1 y1 x2 y2...],[x1 y1 ...],...] (2D list)\n#  dt,gt   - May be either bounding boxes or encoded masks\n# Both poly and bbs are 0-indexed (bbox=[0 0 1 1] encloses first pixel).\n#\n# Finally, a note about the intersection over union (iou) computation.\n# The standard iou of a ground truth (gt) and detected (dt) object is\n#  iou(gt,dt) = area(intersect(gt,dt)) / area(union(gt,dt))\n# For ""crowd"" regions, we use a modified criteria. If a gt object is\n# marked as ""iscrowd"", we allow a dt to match any subregion of the gt.\n# Choosing gt\' in the crowd gt that best matches the dt can be done using\n# gt\'=intersect(dt,gt). Since by definition union(gt\',dt)=dt, computing\n#  iou(gt,dt,iscrowd) = iou(gt\',dt) = area(intersect(gt,dt)) / area(dt)\n# For crowd gt regions we use this modified criteria above for the iou.\n#\n# To compile run ""python setup.py build_ext --inplace""\n# Please do not contact us for help with compiling.\n#\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n# Licensed under the Simplified BSD License [see coco/license.txt]\n\nencode      = _mask.encode\ndecode      = _mask.decode\niou         = _mask.iou\nmerge       = _mask.merge\narea        = _mask.area\ntoBbox      = _mask.toBbox\nfrPyObjects = _mask.frPyObjects'"
lib/roi_data_layer/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\nimport roidb'
lib/roi_data_layer/layer.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""The data layer used during training to train a Fast R-CNN network.\n\nRoIDataLayer implements a Caffe Python layer.\n""""""\n\nimport numpy as np\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\nfrom ..roi_data_layer.minibatch import get_minibatch\n\nclass RoIDataLayer(object):\n    """"""Fast R-CNN data layer used for training.""""""\n\n    def __init__(self, roidb, num_classes):\n        """"""Set the roidb to be used by this layer during training.""""""\n        self._roidb = roidb\n        self._num_classes = num_classes\n        self._shuffle_roidb_inds()\n\n    def _shuffle_roidb_inds(self):\n        """"""Randomly permute the training roidb.""""""\n        self._perm = np.random.permutation(np.arange(len(self._roidb)))\n        self._cur = 0\n\n    def _get_next_minibatch_inds(self):\n        """"""Return the roidb indices for the next minibatch.""""""\n        \n        if cfg.TRAIN.HAS_RPN:\n            if self._cur + cfg.TRAIN.IMS_PER_BATCH >= len(self._roidb):\n                self._shuffle_roidb_inds()\n\n            db_inds = self._perm[self._cur:self._cur + cfg.TRAIN.IMS_PER_BATCH]\n            self._cur += cfg.TRAIN.IMS_PER_BATCH\n        else:\n            # sample images\n            db_inds = np.zeros((cfg.TRAIN.IMS_PER_BATCH), dtype=np.int32)\n            i = 0\n            while (i < cfg.TRAIN.IMS_PER_BATCH):\n                ind = self._perm[self._cur]\n                num_objs = self._roidb[ind][\'boxes\'].shape[0]\n                if num_objs != 0:\n                    db_inds[i] = ind\n                    i += 1\n\n                self._cur += 1\n                if self._cur >= len(self._roidb):\n                    self._shuffle_roidb_inds()\n\n        return db_inds\n\n    def _get_next_minibatch(self):\n        """"""Return the blobs to be used for the next minibatch.\n\n        If cfg.TRAIN.USE_PREFETCH is True, then blobs will be computed in a\n        separate process and made available through self._blob_queue.\n        """"""\n        db_inds = self._get_next_minibatch_inds()\n        minibatch_db = [self._roidb[i] for i in db_inds]\n        return get_minibatch(minibatch_db, self._num_classes)\n            \n    def forward(self):\n        """"""Get blobs and copy them into this layer\'s top blob vector.""""""\n        blobs = self._get_next_minibatch()\n        return blobs\n'"
lib/roi_data_layer/minibatch.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Compute minibatch blobs for training a Fast R-CNN network.""""""\n\nimport numpy as np\nimport numpy.random as npr\nimport cv2\nimport os\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\nfrom ..utils.blob import prep_im_for_blob, im_list_to_blob\n\ndef get_minibatch(roidb, num_classes):\n    """"""Given a roidb, construct a minibatch sampled from it.""""""\n    num_images = len(roidb)\n    # Sample random scales to use for each image in this batch\n    random_scale_inds = npr.randint(0, high=len(cfg.TRAIN.SCALES),\n                                    size=num_images)\n    assert(cfg.TRAIN.BATCH_SIZE % num_images == 0), \\\n        \'num_images ({}) must divide BATCH_SIZE ({})\'. \\\n        format(num_images, cfg.TRAIN.BATCH_SIZE)\n    rois_per_image = cfg.TRAIN.BATCH_SIZE / num_images\n    fg_rois_per_image = np.round(cfg.TRAIN.FG_FRACTION * rois_per_image)\n\n    # Get the input image blob, formatted for caffe\n    im_blob, im_scales = _get_image_blob(roidb, random_scale_inds)\n\n    blobs = {\'data\': im_blob}\n\n    if cfg.TRAIN.HAS_RPN:\n        assert len(im_scales) == 1, ""Single batch only""\n        assert len(roidb) == 1, ""Single batch only""\n        # gt boxes: (x1, y1, x2, y2, cls)\n        gt_inds = np.where(roidb[0][\'gt_classes\'] != 0)[0]\n        gt_boxes = np.empty((len(gt_inds), 5), dtype=np.float32)\n        gt_boxes[:, 0:4] = roidb[0][\'boxes\'][gt_inds, :] * im_scales[0]\n        gt_boxes[:, 4] = roidb[0][\'gt_classes\'][gt_inds]\n        blobs[\'gt_boxes\'] = gt_boxes\n        blobs[\'gt_ishard\'] = roidb[0][\'gt_ishard\'][gt_inds]  \\\n            if \'gt_ishard\' in roidb[0] else np.zeros(gt_inds.size, dtype=int)\n        # blobs[\'gt_ishard\'] = roidb[0][\'gt_ishard\'][gt_inds]\n        blobs[\'dontcare_areas\'] = roidb[0][\'dontcare_areas\'] * im_scales[0] \\\n            if \'dontcare_areas\' in roidb[0] else np.zeros([0, 4], dtype=float)\n        blobs[\'im_info\'] = np.array(\n            [[im_blob.shape[1], im_blob.shape[2], im_scales[0]]],\n            dtype=np.float32)\n        blobs[\'im_name\'] = os.path.basename(roidb[0][\'image\'])\n\n    else: # not using RPN\n        # Now, build the region of interest and label blobs\n        rois_blob = np.zeros((0, 5), dtype=np.float32)\n        labels_blob = np.zeros((0), dtype=np.float32)\n        bbox_targets_blob = np.zeros((0, 4 * num_classes), dtype=np.float32)\n        bbox_inside_blob = np.zeros(bbox_targets_blob.shape, dtype=np.float32)\n        # all_overlaps = []\n        for im_i in xrange(num_images):\n            labels, overlaps, im_rois, bbox_targets, bbox_inside_weights \\\n                = _sample_rois(roidb[im_i], fg_rois_per_image, rois_per_image,\n                               num_classes)\n\n            # Add to RoIs blob\n            rois = _project_im_rois(im_rois, im_scales[im_i])\n            batch_ind = im_i * np.ones((rois.shape[0], 1))\n            rois_blob_this_image = np.hstack((batch_ind, rois))\n            rois_blob = np.vstack((rois_blob, rois_blob_this_image))\n\n            # Add to labels, bbox targets, and bbox loss blobs\n            labels_blob = np.hstack((labels_blob, labels))\n            bbox_targets_blob = np.vstack((bbox_targets_blob, bbox_targets))\n            bbox_inside_blob = np.vstack((bbox_inside_blob, bbox_inside_weights))\n            # all_overlaps = np.hstack((all_overlaps, overlaps))\n\n        # For debug visualizations\n        # _vis_minibatch(im_blob, rois_blob, labels_blob, all_overlaps)\n\n        blobs[\'rois\'] = rois_blob\n        blobs[\'labels\'] = labels_blob\n\n        if cfg.TRAIN.BBOX_REG:\n            blobs[\'bbox_targets\'] = bbox_targets_blob\n            blobs[\'bbox_inside_weights\'] = bbox_inside_blob\n            blobs[\'bbox_outside_weights\'] = \\\n                np.array(bbox_inside_blob > 0).astype(np.float32)\n\n    return blobs\n\ndef _sample_rois(roidb, fg_rois_per_image, rois_per_image, num_classes):\n    """"""Generate a random sample of RoIs comprising foreground and background\n    examples.\n    """"""\n    # label = class RoI has max overlap with\n    labels = roidb[\'max_classes\']\n    overlaps = roidb[\'max_overlaps\']\n    rois = roidb[\'boxes\']\n\n    # Select foreground RoIs as those with >= FG_THRESH overlap\n    fg_inds = np.where(overlaps >= cfg.TRAIN.FG_THRESH)[0]\n    # Guard against the case when an image has fewer than fg_rois_per_image\n    # foreground RoIs\n    fg_rois_per_this_image = np.minimum(fg_rois_per_image, fg_inds.size)\n    # Sample foreground regions without replacement\n    if fg_inds.size > 0:\n        fg_inds = npr.choice(\n                fg_inds, size=fg_rois_per_this_image, replace=False)\n\n    # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n    bg_inds = np.where((overlaps < cfg.TRAIN.BG_THRESH_HI) &\n                       (overlaps >= cfg.TRAIN.BG_THRESH_LO))[0]\n    # Compute number of background RoIs to take from this image (guarding\n    # against there being fewer than desired)\n    bg_rois_per_this_image = rois_per_image - fg_rois_per_this_image\n    bg_rois_per_this_image = np.minimum(bg_rois_per_this_image,\n                                        bg_inds.size)\n    # Sample foreground regions without replacement\n    if bg_inds.size > 0:\n        bg_inds = npr.choice(\n                bg_inds, size=bg_rois_per_this_image, replace=False)\n\n    # The indices that we\'re selecting (both fg and bg)\n    keep_inds = np.append(fg_inds, bg_inds)\n    # Select sampled values from various arrays:\n    labels = labels[keep_inds]\n    # Clamp labels for the background RoIs to 0\n    labels[fg_rois_per_this_image:] = 0\n    overlaps = overlaps[keep_inds]\n    rois = rois[keep_inds]\n\n    bbox_targets, bbox_inside_weights = _get_bbox_regression_labels(\n            roidb[\'bbox_targets\'][keep_inds, :], num_classes)\n\n    return labels, overlaps, rois, bbox_targets, bbox_inside_weights\n\ndef _get_image_blob(roidb, scale_inds):\n    """"""Builds an input blob from the images in the roidb at the specified\n    scales.\n    """"""\n    num_images = len(roidb)\n    processed_ims = []\n    im_scales = []\n    for i in xrange(num_images):\n        im = cv2.imread(roidb[i][\'image\'])\n        if roidb[i][\'flipped\']:\n            im = im[:, ::-1, :]\n        target_size = cfg.TRAIN.SCALES[scale_inds[i]]\n        im, im_scale = prep_im_for_blob(im, cfg.PIXEL_MEANS, target_size,\n                                        cfg.TRAIN.MAX_SIZE)\n        im_scales.append(im_scale)\n        processed_ims.append(im)\n\n    # Create a blob to hold the input images\n    blob = im_list_to_blob(processed_ims)\n\n    return blob, im_scales\n\ndef _project_im_rois(im_rois, im_scale_factor):\n    """"""Project image RoIs into the rescaled training image.""""""\n    rois = im_rois * im_scale_factor\n    return rois\n\ndef _get_bbox_regression_labels(bbox_target_data, num_classes):\n    """"""Bounding-box regression targets are stored in a compact form in the\n    roidb.\n\n    This function expands those targets into the 4-of-4*K representation used\n    by the network (i.e. only one class has non-zero targets). The loss weights\n    are similarly expanded.\n\n    Returns:\n        bbox_target_data (ndarray): N x 4K blob of regression targets\n        bbox_inside_weights (ndarray): N x 4K blob of loss weights\n    """"""\n    clss = bbox_target_data[:, 0]\n    bbox_targets = np.zeros((clss.size, 4 * num_classes), dtype=np.float32)\n    bbox_inside_weights = np.zeros(bbox_targets.shape, dtype=np.float32)\n    inds = np.where(clss > 0)[0]\n    for ind in inds:\n        cls = clss[ind]\n        start = 4 * cls\n        end = start + 4\n        bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]\n        bbox_inside_weights[ind, start:end] = cfg.TRAIN.BBOX_INSIDE_WEIGHTS\n    return bbox_targets, bbox_inside_weights\n\ndef _vis_minibatch(im_blob, rois_blob, labels_blob, overlaps):\n    """"""Visualize a mini-batch for debugging.""""""\n    import matplotlib.pyplot as plt\n    for i in xrange(rois_blob.shape[0]):\n        rois = rois_blob[i, :]\n        im_ind = rois[0]\n        roi = rois[1:]\n        im = im_blob[im_ind, :, :, :].transpose((1, 2, 0)).copy()\n        im += cfg.PIXEL_MEANS\n        im = im[:, :, (2, 1, 0)]\n        im = im.astype(np.uint8)\n        cls = labels_blob[i]\n        plt.imshow(im)\n        print \'class: \', cls, \' overlap: \', overlaps[i]\n        plt.gca().add_patch(\n            plt.Rectangle((roi[0], roi[1]), roi[2] - roi[0],\n                          roi[3] - roi[1], fill=False,\n                          edgecolor=\'r\', linewidth=3)\n            )\n        plt.show()\n'"
lib/roi_data_layer/minibatch2.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Compute minibatch blobs for training a Fast R-CNN network.""""""\n\nimport numpy as np\nimport numpy.random as npr\nimport cv2\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\nfrom ..utils.blob import prep_im_for_blob, im_list_to_blob\n\ndef get_minibatch(roidb, num_classes):\n    """"""Given a roidb, construct a minibatch sampled from it.""""""\n    num_images = len(roidb)\n\n    assert(cfg.TRAIN.BATCH_SIZE % num_images == 0), \\\n        \'num_images ({}) must divide BATCH_SIZE ({})\'. \\\n        format(num_images, cfg.TRAIN.BATCH_SIZE)\n    rois_per_image = cfg.TRAIN.BATCH_SIZE / num_images\n    fg_rois_per_image = np.round(cfg.TRAIN.FG_FRACTION * rois_per_image)\n\n    if cfg.IS_MULTISCALE:\n        im_blob, im_scales = _get_image_blob_multiscale(roidb)\n    else:\n        # Get the input image blob, formatted for caffe\n        # Sample random scales to use for each image in this batch\n        random_scale_inds = npr.randint(0, high=len(cfg.TRAIN.SCALES_BASE), size=num_images)\n        im_blob, im_scales = _get_image_blob(roidb, random_scale_inds)\n\n    blobs = {\'data\': im_blob}\n\n    if cfg.TRAIN.HAS_RPN:\n        assert len(im_scales) == 1, ""Single batch only""\n        assert len(roidb) == 1, ""Single batch only""\n        # gt boxes: (x1, y1, x2, y2, cls)\n        gt_inds = np.where(roidb[0][\'gt_classes\'] != 0)[0]\n        gt_boxes = np.empty((len(gt_inds), 5), dtype=np.float32)\n        gt_boxes[:, 0:4] = roidb[0][\'boxes\'][gt_inds, :] * im_scales[0]\n        gt_boxes[:, 4] = roidb[0][\'gt_classes\'][gt_inds]\n        blobs[\'gt_boxes\'] = gt_boxes\n        blobs[\'im_info\'] = np.array(\n            [[im_blob.shape[1], im_blob.shape[2], im_scales[0]]],\n            dtype=np.float32)\n\n\n    else:\n        # Now, build the region of interest and label blobs\n        rois_blob = np.zeros((0, 5), dtype=np.float32)\n        labels_blob = np.zeros((0), dtype=np.float32)\n        bbox_targets_blob = np.zeros((0, 4 * num_classes), dtype=np.float32)\n        bbox_inside_blob = np.zeros(bbox_targets_blob.shape, dtype=np.float32)\n\n        # all_overlaps = []\n        for im_i in xrange(num_images):\n            labels, overlaps, im_rois, bbox_targets, bbox_inside_weights, sublabels \\\n                    = _sample_rois(roidb[im_i], fg_rois_per_image, rois_per_image, num_classes)\n\n            # Add to RoIs blob\n            if cfg.IS_MULTISCALE:\n                if cfg.IS_EXTRAPOLATING:\n                    rois, levels = _project_im_rois_multiscale(im_rois, cfg.TRAIN.SCALES)\n                    batch_ind = im_i * len(cfg.TRAIN.SCALES) + levels\n                else:\n                    rois, levels = _project_im_rois_multiscale(im_rois, cfg.TRAIN.SCALES_BASE)\n                    batch_ind = im_i * len(cfg.TRAIN.SCALES_BASE) + levels\n            else:\n                rois = _project_im_rois(im_rois, im_scales[im_i])\n                batch_ind = im_i * np.ones((rois.shape[0], 1))\n\n            rois_blob_this_image = np.hstack((batch_ind, rois))\n            rois_blob = np.vstack((rois_blob, rois_blob_this_image))\n\n            # Add to labels, bbox targets, and bbox loss blobs\n            labels_blob = np.hstack((labels_blob, labels))\n            bbox_targets_blob = np.vstack((bbox_targets_blob, bbox_targets))\n            bbox_inside_blob = np.vstack((bbox_inside_blob, bbox_inside_weights))\n\n            # all_overlaps = np.hstack((all_overlaps, overlaps))\n\n        # For debug visualizations\n        # _vis_minibatch(im_blob, rois_blob, labels_blob, all_overlaps, sublabels_blob, view_targets_blob, view_inside_blob)\n        # _vis_minibatch(im_blob, rois_blob, labels_blob, all_overlaps, sublabels_blob)\n\n        blobs[\'rois\'] = rois_blob\n        blobs[\'labels\'] = labels_blob\n\n        if cfg.TRAIN.BBOX_REG:\n            blobs[\'bbox_targets\'] = bbox_targets_blob\n            blobs[\'bbox_inside_weights\'] = bbox_inside_blob\n            blobs[\'bbox_outside_weights\'] = np.array(bbox_inside_blob > 0).astype(np.float32)\n\n    return blobs\n\ndef _sample_rois(roidb, fg_rois_per_image, rois_per_image, num_classes):\n    """"""Generate a random sample of RoIs comprising foreground and background\n    examples.\n    """"""\n    # label = class RoI has max overlap with\n    labels = roidb[\'max_classes\']\n    overlaps = roidb[\'max_overlaps\']\n    rois = roidb[\'boxes\']\n\n    # Select foreground RoIs as those with >= FG_THRESH overlap\n    fg_inds = []\n    for i in xrange(1, num_classes):\n        fg_inds.extend(np.where((labels == i) & (overlaps >= cfg.TRAIN.FG_THRESH))[0])\n    fg_inds = np.array(fg_inds)\n\n    # Guard against the case when an image has fewer than fg_rois_per_image\n    # foreground RoIs\n    fg_rois_per_this_image = np.minimum(fg_rois_per_image, fg_inds.size)\n    # Sample foreground regions without replacement\n    if fg_inds.size > 0:\n        fg_inds = npr.choice(fg_inds, size=fg_rois_per_this_image,\n                             replace=False)\n\n    bg_rois_per_this_image = rois_per_image - fg_rois_per_this_image\n    # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n    bg_inds = []\n    for i in xrange(1, num_classes):\n        bg_inds.extend( np.where((labels == i) & (overlaps < cfg.TRAIN.BG_THRESH_HI) &\n                        (overlaps >= cfg.TRAIN.BG_THRESH_LO))[0] )\n\n    if len(bg_inds) < bg_rois_per_this_image:\n        for i in xrange(1, num_classes):\n            bg_inds.extend( np.where((labels == i) & (overlaps < cfg.TRAIN.BG_THRESH_HI))[0] )\n\n    if len(bg_inds) < bg_rois_per_this_image:\n        bg_inds.extend( np.where(overlaps < cfg.TRAIN.BG_THRESH_HI)[0] )\n    bg_inds = np.array(bg_inds, dtype=np.int32)\n\n    # Compute number of background RoIs to take from this image (guarding\n    # against there being fewer than desired)\n    bg_rois_per_this_image = np.minimum(bg_rois_per_this_image,\n                                        bg_inds.size)\n    # Sample foreground regions without replacement\n    if bg_inds.size > 0:\n        bg_inds = npr.choice(bg_inds, size=bg_rois_per_this_image,\n                             replace=False)\n\n    # The indices that we\'re selecting (both fg and bg)\n    keep_inds = np.append(fg_inds, bg_inds).astype(int)\n    # print \'{} foregrounds and {} backgrounds\'.format(fg_inds.size, bg_inds.size)\n    # Select sampled values from various arrays:\n    labels = labels[keep_inds]\n    # Clamp labels for the background RoIs to 0\n    labels[fg_rois_per_this_image:] = 0\n    overlaps = overlaps[keep_inds]\n    rois = rois[keep_inds]\n    sublabels = sublabels[keep_inds]\n    sublabels[fg_rois_per_this_image:] = 0\n\n    bbox_targets, bbox_loss_weights = \\\n            _get_bbox_regression_labels(roidb[\'bbox_targets\'][keep_inds, :],\n                                        num_classes)\n\n    if cfg.TRAIN.VIEWPOINT or cfg.TEST.VIEWPOINT:\n        viewpoints = viewpoints[keep_inds]\n        view_targets, view_loss_weights = \\\n                _get_viewpoint_estimation_labels(viewpoints, labels, num_classes)\n        return labels, overlaps, rois, bbox_targets, bbox_loss_weights, sublabels, view_targets, view_loss_weights\n\n    return labels, overlaps, rois, bbox_targets, bbox_loss_weights, sublabels\n\ndef _get_image_blob(roidb, scale_inds):\n    """"""Builds an input blob from the images in the roidb at the specified\n    scales.\n    """"""\n    num_images = len(roidb)\n    processed_ims = []\n    im_scales = []\n    for i in xrange(num_images):\n        im = cv2.imread(roidb[i][\'image\'])\n        if roidb[i][\'flipped\']:\n            im = im[:, ::-1, :]\n\n        im_orig = im.astype(np.float32, copy=True)\n        im_orig -= cfg.PIXEL_MEANS\n\n        im_scale = cfg.TRAIN.SCALES_BASE[scale_inds[i]]\n        im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale, interpolation=cv2.INTER_LINEAR)\n\n        im_scales.append(im_scale)\n        processed_ims.append(im)\n\n    # Create a blob to hold the input images\n    blob = im_list_to_blob(processed_ims)\n\n    return blob, im_scales\n\n\ndef _get_image_blob_multiscale(roidb):\n    """"""Builds an input blob from the images in the roidb at multiscales.\n    """"""\n    num_images = len(roidb)\n    processed_ims = []\n    im_scales = []\n    scales = cfg.TRAIN.SCALES_BASE\n    for i in xrange(num_images):\n        im = cv2.imread(roidb[i][\'image\'])\n        if roidb[i][\'flipped\']:\n            im = im[:, ::-1, :]\n\n        im_orig = im.astype(np.float32, copy=True)\n        im_orig -= cfg.PIXEL_MEANS\n\n        for im_scale in scales:\n            im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale, interpolation=cv2.INTER_LINEAR)\n            im_scales.append(im_scale)\n            processed_ims.append(im)\n\n    # Create a blob to hold the input images\n    blob = im_list_to_blob(processed_ims)\n\n    return blob, im_scales\n\n\ndef _project_im_rois(im_rois, im_scale_factor):\n    """"""Project image RoIs into the rescaled training image.""""""\n    rois = im_rois * im_scale_factor\n    return rois\n\n\ndef _project_im_rois_multiscale(im_rois, scales):\n    """"""Project image RoIs into the image pyramid built by _get_image_blob.\n\n    Arguments:\n        im_rois (ndarray): R x 4 matrix of RoIs in original image coordinates\n        scales (list): scale factors as returned by _get_image_blob\n\n    Returns:\n        rois (ndarray): R x 4 matrix of projected RoI coordinates\n        levels (list): image pyramid levels used by each projected RoI\n    """"""\n    im_rois = im_rois.astype(np.float, copy=False)\n    scales = np.array(scales)\n\n    if len(scales) > 1:\n        widths = im_rois[:, 2] - im_rois[:, 0] + 1\n        heights = im_rois[:, 3] - im_rois[:, 1] + 1\n\n        areas = widths * heights\n        scaled_areas = areas[:, np.newaxis] * (scales[np.newaxis, :] ** 2)\n        diff_areas = np.abs(scaled_areas - 224 * 224)\n        levels = diff_areas.argmin(axis=1)[:, np.newaxis]\n    else:\n        levels = np.zeros((im_rois.shape[0], 1), dtype=np.int)\n\n    rois = im_rois * scales[levels]\n\n    return rois, levels\n\n\ndef _get_bbox_regression_labels(bbox_target_data, num_classes):\n    """"""Bounding-box regression targets are stored in a compact form in the\n    roidb.\n\n    This function expands those targets into the 4-of-4*K representation used\n    by the network (i.e. only one class has non-zero targets). The loss weights\n    are similarly expanded.\n\n    Returns:\n        bbox_target_data (ndarray): N x 4K blob of regression targets\n        bbox_loss_weights (ndarray): N x 4K blob of loss weights\n    """"""\n    clss = bbox_target_data[:, 0]\n    bbox_targets = np.zeros((clss.size, 4 * num_classes), dtype=np.float32)\n    bbox_loss_weights = np.zeros(bbox_targets.shape, dtype=np.float32)\n    inds = np.where(clss > 0)[0]\n    for ind in inds:\n        cls = clss[ind]\n        start = 4 * cls\n        end = start + 4\n        bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]\n        bbox_loss_weights[ind, start:end] = [1., 1., 1., 1.]\n    return bbox_targets, bbox_loss_weights\n\n\ndef _get_viewpoint_estimation_labels(viewpoint_data, clss, num_classes):\n    """"""Bounding-box regression targets are stored in a compact form in the\n    roidb.\n\n    This function expands those targets into the 4-of-4*K representation used\n    by the network (i.e. only one class has non-zero targets). The loss weights\n    are similarly expanded.\n\n    Returns:\n        view_target_data (ndarray): N x 3K blob of regression targets\n        view_loss_weights (ndarray): N x 3K blob of loss weights\n    """"""\n    view_targets = np.zeros((clss.size, 3 * num_classes), dtype=np.float32)\n    view_loss_weights = np.zeros(view_targets.shape, dtype=np.float32)\n    inds = np.where( (clss > 0) & np.isfinite(viewpoint_data[:,0]) & np.isfinite(viewpoint_data[:,1]) & np.isfinite(viewpoint_data[:,2]) )[0]\n    for ind in inds:\n        cls = clss[ind]\n        start = 3 * cls\n        end = start + 3\n        view_targets[ind, start:end] = viewpoint_data[ind, :]\n        view_loss_weights[ind, start:end] = [1., 1., 1.]\n\n    assert not np.isinf(view_targets).any(), \'viewpoint undefined\'\n    return view_targets, view_loss_weights\n\n\ndef _vis_minibatch(im_blob, rois_blob, labels_blob, overlaps, sublabels_blob, view_targets_blob=None, view_inside_blob=None):\n    """"""Visualize a mini-batch for debugging.""""""\n    import matplotlib.pyplot as plt\n    import math\n    for i in xrange(min(rois_blob.shape[0], 10)):\n        rois = rois_blob[i, :]\n        im_ind = rois[0]\n        roi = rois[1:]\n        im = im_blob[im_ind, :, :, :].transpose((1, 2, 0)).copy()\n        im += cfg.PIXEL_MEANS\n        im = im[:, :, (2, 1, 0)]\n        im = im.astype(np.uint8)\n        cls = labels_blob[i]\n        subcls = sublabels_blob[i]\n        plt.imshow(im)\n        print \'class: \', cls, \' subclass: \', subcls, \' overlap: \', overlaps[i]\n\n        start = 3 * cls\n        end = start + 3\n        # print \'view: \', view_targets_blob[i, start:end] * 180 / math.pi\n        # print \'view weights: \', view_inside_blob[i, start:end]\n\n        plt.gca().add_patch(\n            plt.Rectangle((roi[0], roi[1]), roi[2] - roi[0],\n                          roi[3] - roi[1], fill=False,\n                          edgecolor=\'r\', linewidth=3)\n            )\n        plt.show()\n'"
lib/roi_data_layer/roidb.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Transform a roidb into a trainable roidb by adding a bunch of metadata.""""""\n\nimport numpy as np\n\nimport PIL\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\nfrom ..fast_rcnn.bbox_transform import bbox_transform\n# <<<< obsolete\nfrom ..utils.cython_bbox import bbox_overlaps\n\ndef prepare_roidb(imdb):\n    """"""Enrich the imdb\'s roidb by adding some derived quantities that\n    are useful for training. This function precomputes the maximum\n    overlap, taken over ground-truth boxes, between each ROI and\n    each ground-truth box. The class with maximum overlap is also\n    recorded.\n    """"""\n    sizes = [PIL.Image.open(imdb.image_path_at(i)).size\n             for i in xrange(imdb.num_images)]\n    roidb = imdb.roidb\n    for i in xrange(len(imdb.image_index)):\n        roidb[i][\'image\'] = imdb.image_path_at(i)\n        roidb[i][\'width\'] = sizes[i][0]\n        roidb[i][\'height\'] = sizes[i][1]\n        # need gt_overlaps as a dense array for argmax\n        gt_overlaps = roidb[i][\'gt_overlaps\'].toarray()\n        # max overlap with gt over classes (columns)\n        max_overlaps = gt_overlaps.max(axis=1)\n        # gt class that had the max overlap\n        max_classes = gt_overlaps.argmax(axis=1)\n        roidb[i][\'max_classes\'] = max_classes\n        roidb[i][\'max_overlaps\'] = max_overlaps\n        # sanity checks\n        # max overlap of 0 => class should be zero (background)\n        zero_inds = np.where(max_overlaps == 0)[0]\n        assert all(max_classes[zero_inds] == 0)\n        # max overlap > 0 => class should not be zero (must be a fg class)\n        nonzero_inds = np.where(max_overlaps > 0)[0]\n        assert all(max_classes[nonzero_inds] != 0)\n\ndef add_bbox_regression_targets(roidb):\n    """"""\n    Add information needed to train bounding-box regressors.\n    For each roi find the corresponding gt box, and compute the distance.\n    then normalize the distance into Gaussian by minus mean and divided by std\n    """"""\n    assert len(roidb) > 0\n    assert \'max_classes\' in roidb[0], \'Did you call prepare_roidb first?\'\n\n    num_images = len(roidb)\n    # Infer number of classes from the number of columns in gt_overlaps\n    num_classes = roidb[0][\'gt_overlaps\'].shape[1]\n    for im_i in xrange(num_images):\n        rois = roidb[im_i][\'boxes\']\n        max_overlaps = roidb[im_i][\'max_overlaps\']\n        max_classes = roidb[im_i][\'max_classes\']\n        roidb[im_i][\'bbox_targets\'] = \\\n                _compute_targets(rois, max_overlaps, max_classes)\n\n    if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n        # Use fixed / precomputed ""means"" and ""stds"" instead of empirical values\n        means = np.tile(\n                np.array(cfg.TRAIN.BBOX_NORMALIZE_MEANS), (num_classes, 1))\n        stds = np.tile(\n                np.array(cfg.TRAIN.BBOX_NORMALIZE_STDS), (num_classes, 1))\n    else:\n        # Compute values needed for means and stds\n        # var(x) = E(x^2) - E(x)^2\n        class_counts = np.zeros((num_classes, 1)) + cfg.EPS\n        sums = np.zeros((num_classes, 4))\n        squared_sums = np.zeros((num_classes, 4))\n        for im_i in xrange(num_images):\n            targets = roidb[im_i][\'bbox_targets\']\n            for cls in xrange(1, num_classes):\n                cls_inds = np.where(targets[:, 0] == cls)[0]\n                if cls_inds.size > 0:\n                    class_counts[cls] += cls_inds.size\n                    sums[cls, :] += targets[cls_inds, 1:].sum(axis=0)\n                    squared_sums[cls, :] += \\\n                            (targets[cls_inds, 1:] ** 2).sum(axis=0)\n\n        means = sums / class_counts\n        stds = np.sqrt(squared_sums / class_counts - means ** 2)\n        # too small number will cause nan error\n        assert np.min(stds) < 0.01, \\\n            \'Boxes std is too small, std:{}\'.format(stds)\n\n    print \'bbox target means:\'\n    print means\n    print means[1:, :].mean(axis=0) # ignore bg class\n    print \'bbox target stdevs:\'\n    print stds\n    print stds[1:, :].mean(axis=0) # ignore bg class\n\n    # Normalize targets\n    if cfg.TRAIN.BBOX_NORMALIZE_TARGETS:\n        print ""Normalizing targets""\n        for im_i in xrange(num_images):\n            targets = roidb[im_i][\'bbox_targets\']\n            for cls in xrange(1, num_classes):\n                cls_inds = np.where(targets[:, 0] == cls)[0]\n                roidb[im_i][\'bbox_targets\'][cls_inds, 1:] -= means[cls, :]\n                roidb[im_i][\'bbox_targets\'][cls_inds, 1:] /= stds[cls, :]\n    else:\n        print ""NOT normalizing targets""\n\n    # These values will be needed for making predictions\n    # (the predicts will need to be unnormalized and uncentered)\n    return means.ravel(), stds.ravel()\n\ndef _compute_targets(rois, overlaps, labels):\n    """"""\n    Compute bounding-box regression targets for an image.\n    for each roi find the corresponding gt_box, then compute the distance.\n    """"""\n    # Indices of ground-truth ROIs\n    gt_inds = np.where(overlaps == 1)[0]\n    if len(gt_inds) == 0:\n        # Bail if the image has no ground-truth ROIs\n        return np.zeros((rois.shape[0], 5), dtype=np.float32)\n    # Indices of examples for which we try to make predictions\n    ex_inds = np.where(overlaps >= cfg.TRAIN.BBOX_THRESH)[0]\n\n    # Get IoU overlap between each ex ROI and gt ROI\n    ex_gt_overlaps = bbox_overlaps(\n        np.ascontiguousarray(rois[ex_inds, :], dtype=np.float),\n        np.ascontiguousarray(rois[gt_inds, :], dtype=np.float))\n\n    # Find which gt ROI each ex ROI has max overlap with:\n    # this will be the ex ROI\'s gt target\n    gt_assignment = ex_gt_overlaps.argmax(axis=1)\n    gt_rois = rois[gt_inds[gt_assignment], :]\n    ex_rois = rois[ex_inds, :]\n\n    targets = np.zeros((rois.shape[0], 5), dtype=np.float32)\n    targets[ex_inds, 0] = labels[ex_inds]\n    targets[ex_inds, 1:] = bbox_transform(ex_rois, gt_rois)\n    return targets\n'"
lib/roi_data_layer/roidb2.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Transform a roidb into a trainable roidb by adding a bunch of metadata.""""""\n\nimport numpy as np\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\nfrom ..fast_rcnn.bbox_transform import bbox_transform\n# <<<< obsolete\nfrom ..utils.cython_bbox import bbox_overlaps\n\ndef prepare_roidb(imdb):\n    """"""Enrich the imdb\'s roidb by adding some derived quantities that\n    are useful for training. This function precomputes the maximum\n    overlap, taken over ground-truth boxes, between each ROI and\n    each ground-truth box. The class with maximum overlap is also\n    recorded.\n    """"""\n    roidb = imdb.roidb\n    for i in xrange(len(imdb.image_index)):\n        roidb[i][\'image\'] = imdb.image_path_at(i)\n        # need gt_overlaps as a dense array for argmax\n        gt_overlaps = roidb[i][\'gt_overlaps\'].toarray()\n        # max overlap with gt over classes (columns)\n        max_overlaps = gt_overlaps.max(axis=1)\n        # gt class that had the max overlap\n        max_classes = gt_overlaps.argmax(axis=1)\n\n        roidb[i][\'max_classes\'] = max_classes\n        roidb[i][\'max_overlaps\'] = max_overlaps\n\n        # sanity checks\n        # max overlap of 0 => class should be zero (background)\n        zero_inds = np.where(max_overlaps == 0)[0]\n        assert all(max_classes[zero_inds] == 0)\n        # max overlap > 0 => class should not be zero (must be a fg class)\n        nonzero_inds = np.where(max_overlaps > 0)[0]\n        assert all(max_classes[nonzero_inds] != 0)\n\ndef add_bbox_regression_targets(roidb):\n    """"""Add information needed to train bounding-box regressors.""""""\n    assert len(roidb) > 0\n    assert \'max_classes\' in roidb[0], \'Did you call prepare_roidb first?\'\n\n    num_images = len(roidb)\n    # Infer number of classes from the number of columns in gt_overlaps\n    num_classes = roidb[0][\'gt_overlaps\'].shape[1]\n    for im_i in xrange(num_images):\n        rois = roidb[im_i][\'boxes\']\n        max_overlaps = roidb[im_i][\'max_overlaps\']\n        max_classes = roidb[im_i][\'max_classes\']\n        roidb[im_i][\'bbox_targets\'] = \\\n                _compute_targets(rois, max_overlaps, max_classes, num_classes)\n\n    # Compute values needed for means and stds\n    # var(x) = E(x^2) - E(x)^2\n    class_counts = np.zeros((num_classes, 1)) + cfg.EPS\n    sums = np.zeros((num_classes, 4))\n    squared_sums = np.zeros((num_classes, 4))\n    for im_i in xrange(num_images):\n        targets = roidb[im_i][\'bbox_targets\']\n        for cls in xrange(1, num_classes):\n            cls_inds = np.where(targets[:, 0] == cls)[0]\n            if cls_inds.size > 0:\n                class_counts[cls] += cls_inds.size\n                sums[cls, :] += targets[cls_inds, 1:].sum(axis=0)\n                squared_sums[cls, :] += (targets[cls_inds, 1:] ** 2).sum(axis=0)\n\n    means = sums / class_counts\n    stds = np.sqrt(squared_sums / class_counts - means ** 2)\n\n    # Normalize targets\n    for im_i in xrange(num_images):\n        targets = roidb[im_i][\'bbox_targets\']\n        for cls in xrange(1, num_classes):\n            cls_inds = np.where(targets[:, 0] == cls)[0]\n            roidb[im_i][\'bbox_targets\'][cls_inds, 1:] -= means[cls, :]\n            if stds[cls, 0] != 0:\n                roidb[im_i][\'bbox_targets\'][cls_inds, 1:] /= stds[cls, :]\n\n    # These values will be needed for making predictions\n    # (the predicts will need to be unnormalized and uncentered)\n    return means.ravel(), stds.ravel()\n\ndef _compute_targets(rois, overlaps, labels, num_classes):\n    """"""Compute bounding-box regression targets for an image.""""""\n    # Ensure ROIs are floats\n    rois = rois.astype(np.float, copy=False)\n\n    # Indices of ground-truth ROIs\n    gt_inds = np.where(overlaps == 1)[0]\n    # Indices of examples for which we try to make predictions\n    ex_inds = []\n    for i in xrange(1, num_classes):\n        ex_inds.extend( np.where((labels == i) & (overlaps >= cfg.TRAIN.BBOX_THRESH))[0] )\n\n    # Get IoU overlap between each ex ROI and gt ROI\n    ex_gt_overlaps = utils.cython_bbox.bbox_overlaps(rois[ex_inds, :],\n                                                     rois[gt_inds, :])\n\n    # Find which gt ROI each ex ROI has max overlap with:\n    # this will be the ex ROI\'s gt target\n    if ex_gt_overlaps.shape[0] != 0:\n        gt_assignment = ex_gt_overlaps.argmax(axis=1)\n    else:\n        gt_assignment = []\n    gt_rois = rois[gt_inds[gt_assignment], :]\n    ex_rois = rois[ex_inds, :]\n\n    ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + cfg.EPS\n    ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + cfg.EPS\n    ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n    ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n\n    gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + cfg.EPS\n    gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + cfg.EPS\n    gt_ctr_x = gt_rois[:, 0] + 0.5 * gt_widths\n    gt_ctr_y = gt_rois[:, 1] + 0.5 * gt_heights\n\n    targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n    targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n    targets_dw = np.log(gt_widths / ex_widths)\n    targets_dh = np.log(gt_heights / ex_heights)\n\n    targets = np.zeros((rois.shape[0], 5), dtype=np.float32)\n    targets[ex_inds, 0] = labels[ex_inds]\n    targets[ex_inds, 1] = targets_dx\n    targets[ex_inds, 2] = targets_dy\n    targets[ex_inds, 3] = targets_dw\n    targets[ex_inds, 4] = targets_dh\n    return targets\n'"
lib/roi_pooling_layer/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\nimport roi_pooling_op\nimport roi_pooling_op_grad'
lib/roi_pooling_layer/roi_pooling_op.py,1,"b""import tensorflow as tf\nimport os.path as osp\n\nfilename = osp.join(osp.dirname(__file__), 'roi_pooling.so')\n_roi_pooling_module = tf.load_op_library(filename)\nroi_pool = _roi_pooling_module.roi_pool\nroi_pool_grad = _roi_pooling_module.roi_pool_grad\n"""
lib/roi_pooling_layer/roi_pooling_op_grad.py,0,"b'import tensorflow as tf\nfrom tensorflow.python.framework import ops\nimport roi_pooling_op\n\n@ops.RegisterGradient(""RoiPool"")\ndef _roi_pool_grad(op, grad, _):\n  """"""The gradients for `roi_pool`.\n  Args:\n    op: The `roi_pool` `Operation` that we are differentiating, which we can use\n      to find the inputs and outputs of the original op.\n    grad: Gradient with respect to the output of the `roi_pool` op.\n  Returns:\n    Gradients with respect to the input of `zero_out`.\n  """"""\n  data = op.inputs[0]\n  rois = op.inputs[1]\n  argmax = op.outputs[1]\n  pooled_height = op.get_attr(\'pooled_height\')\n  pooled_width = op.get_attr(\'pooled_width\')\n  spatial_scale = op.get_attr(\'spatial_scale\')\n\n  # compute gradient\n  data_grad = roi_pooling_op.roi_pool_grad(data, rois, argmax, grad, pooled_height, pooled_width, spatial_scale)\n\n  return [data_grad, None]  # List of one Tensor, since we have one input\n'"
lib/roi_pooling_layer/roi_pooling_op_test.py,12,"b""import tensorflow as tf\nimport numpy as np\nimport roi_pooling_op\nimport roi_pooling_op_grad\nimport tensorflow as tf\nimport pdb\n\n\ndef weight_variable(shape):\n  initial = tf.truncated_normal(shape, stddev=0.1)\n  return tf.Variable(initial)\n\ndef conv2d(x, W):\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\narray = np.random.rand(32, 100, 100, 3)\ndata = tf.convert_to_tensor(array, dtype=tf.float32)\nrois = tf.convert_to_tensor([[0, 10, 10, 20, 20], [31, 30, 30, 40, 40]], dtype=tf.float32)\n\nW = weight_variable([3, 3, 3, 1])\nh = conv2d(data, W)\n\n[y, argmax] = roi_pooling_op.roi_pool(h, rois, 6, 6, 1.0/3)\npdb.set_trace()\ny_data = tf.convert_to_tensor(np.ones((2, 6, 6, 1)), dtype=tf.float32)\nprint y_data, y, argmax\n\n# Minimize the mean squared errors.\nloss = tf.reduce_mean(tf.square(y - y_data))\noptimizer = tf.train.GradientDescentOptimizer(0.5)\ntrain = optimizer.minimize(loss)\n\ninit = tf.global_variables_initializer()\n\n# Launch the graph.\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\nsess.run(init)\npdb.set_trace()\nfor step in xrange(10):\n    sess.run(train)\n    print(step, sess.run(W))\n    print(sess.run(y))\n\n#with tf.device('/gpu:0'):\n#  result = module.roi_pool(data, rois, 1, 1, 1.0/1)\n#  print result.eval()\n#with tf.device('/cpu:0'):\n#  run(init)\n"""
lib/rpn_msr/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n'
lib/rpn_msr/anchor_target_layer.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\nimport caffe\n\nimport os\nimport yaml\nimport numpy as np\nimport numpy.random as npr\n\nfrom .generate_anchors import generate_anchors\nfrom ..utils.cython_bbox import bbox_overlaps\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\nfrom ..fast_rcnn.bbox_transform import bbox_transform\n# <<<< obsolete\n\n\nDEBUG = False \n\nclass AnchorTargetLayer(caffe.Layer):\n    """"""\n    Assign anchors to ground-truth targets. Produces anchor classification\n    labels and bounding-box regression targets.\n    """"""\n\n    def setup(self, bottom, top):\n        self._anchors = generate_anchors(cfg.TRAIN.RPN_BASE_SIZE, cfg.TRAIN.RPN_ASPECTS, cfg.TRAIN.RPN_SCALES)\n        self._num_anchors = self._anchors.shape[0]\n\n        if DEBUG:\n            print \'anchors:\'\n            print self._anchors\n            print \'anchor shapes:\'\n            print np.hstack((\n                self._anchors[:, 2::4] - self._anchors[:, 0::4],\n                self._anchors[:, 3::4] - self._anchors[:, 1::4],\n            ))\n            self._counts = cfg.EPS\n            self._sums = np.zeros((1, 4))\n            self._squared_sums = np.zeros((1, 4))\n            self._fg_sum = 0\n            self._bg_sum = 0\n            self._count = 0\n\n        layer_params = yaml.load(self.param_str_)\n        self._feat_stride = layer_params[\'feat_stride\']\n\n        # allow boxes to sit over the edge by a small amount\n        self._allowed_border = layer_params.get(\'allowed_border\', 0)\n\n        height, width = bottom[0].data.shape[-2:]\n        if DEBUG:\n            print \'AnchorTargetLayer: height\', height, \'width\', width\n\n        A = self._num_anchors\n        # labels\n        top[0].reshape(1, 1, A * height, width)\n        # bbox_targets\n        top[1].reshape(1, A * 4, height, width)\n        # bbox_inside_weights\n        top[2].reshape(1, A * 4, height, width)\n        # bbox_outside_weights\n        top[3].reshape(1, A * 4, height, width)\n\n    def forward(self, bottom, top):\n        # Algorithm:\n        #\n        # for each (H, W) location i\n        #   generate 9 anchor boxes centered on cell i\n        #   apply predicted bbox deltas at cell i to each of the 9 anchors\n        # filter out-of-image anchors\n        # measure GT overlap\n\n        assert bottom[0].data.shape[0] == 1, \\\n            \'Only single item batches are supported\'\n\n        # map of shape (..., H, W)\n        height, width = bottom[0].data.shape[-2:]\n        # GT boxes (x1, y1, x2, y2, label)\n        gt_boxes = bottom[1].data\n        # im_info\n        im_info = bottom[2].data[0, :]\n\n        if DEBUG:\n            print \'\'\n            print \'im_size: ({}, {})\'.format(im_info[0], im_info[1])\n            print \'scale: {}\'.format(im_info[2])\n            print \'height, width: ({}, {})\'.format(height, width)\n            print \'rpn: gt_boxes.shape\', gt_boxes.shape\n            print \'rpn: gt_boxes\', gt_boxes\n\n        # 1. Generate proposals from bbox deltas and shifted anchors\n        shift_x = np.arange(0, width) * self._feat_stride\n        shift_y = np.arange(0, height) * self._feat_stride\n        shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n        shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n        # add A anchors (1, A, 4) to\n        # cell K shifts (K, 1, 4) to get\n        # shift anchors (K, A, 4)\n        # reshape to (K*A, 4) shifted anchors\n        A = self._num_anchors\n        K = shifts.shape[0]\n        all_anchors = (self._anchors.reshape((1, A, 4)) +\n                       shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n        all_anchors = all_anchors.reshape((K * A, 4))\n        total_anchors = int(K * A)\n\n        # only keep anchors inside the image\n        inds_inside = np.where(\n            (all_anchors[:, 0] >= -self._allowed_border) &\n            (all_anchors[:, 1] >= -self._allowed_border) &\n            (all_anchors[:, 2] < im_info[1] + self._allowed_border) &  # width\n            (all_anchors[:, 3] < im_info[0] + self._allowed_border)    # height\n        )[0]\n\n        if DEBUG:\n            print \'total_anchors\', total_anchors\n            print \'inds_inside\', len(inds_inside)\n\n        # keep only inside anchors\n        anchors = all_anchors[inds_inside, :]\n        if DEBUG:\n            print \'anchors.shape\', anchors.shape\n\n        # label: 1 is positive, 0 is negative, -1 is dont care\n        labels = np.empty((len(inds_inside), ), dtype=np.float32)\n        labels.fill(-1)\n\n        # overlaps between the anchors and the gt boxes\n        # overlaps (ex, gt)\n        if gt_boxes.shape[0] != 0:\n            overlaps = bbox_overlaps(\n                np.ascontiguousarray(anchors, dtype=np.float),\n                np.ascontiguousarray(gt_boxes, dtype=np.float))\n            argmax_overlaps = overlaps.argmax(axis=1)\n            max_overlaps = overlaps[np.arange(len(inds_inside)), argmax_overlaps]\n            gt_argmax_overlaps = overlaps.argmax(axis=0)\n            gt_max_overlaps = overlaps[gt_argmax_overlaps, np.arange(overlaps.shape[1])]\n            gt_argmax_overlaps = np.where(overlaps == gt_max_overlaps)[0]\n\n            if not cfg.TRAIN.RPN_CLOBBER_POSITIVES:\n                # assign bg labels first so that positive labels can clobber them\n                labels[max_overlaps < cfg.TRAIN.RPN_NEGATIVE_OVERLAP] = 0\n\n            # fg label: for each gt, anchor with highest overlap\n            labels[gt_argmax_overlaps] = 1\n\n            # fg label: above threshold IOU\n            labels[max_overlaps >= cfg.TRAIN.RPN_POSITIVE_OVERLAP] = 1\n\n            if cfg.TRAIN.RPN_CLOBBER_POSITIVES:\n                # assign bg labels last so that negative labels can clobber positives\n                labels[max_overlaps < cfg.TRAIN.RPN_NEGATIVE_OVERLAP] = 0\n        else:\n            labels.fill(0)\n\n        # subsample positive labels if we have too many\n        num_fg = int(cfg.TRAIN.RPN_FG_FRACTION * cfg.TRAIN.RPN_BATCHSIZE)\n        fg_inds = np.where(labels == 1)[0]\n        if len(fg_inds) > num_fg:\n            disable_inds = npr.choice(\n                fg_inds, size=(len(fg_inds) - num_fg), replace=False)\n            labels[disable_inds] = -1\n\n        # subsample negative labels if we have too many\n        num_bg = cfg.TRAIN.RPN_BATCHSIZE - np.sum(labels == 1)\n        bg_inds = np.where(labels == 0)[0]\n        if len(bg_inds) > num_bg:\n            disable_inds = npr.choice(\n                bg_inds, size=(len(bg_inds) - num_bg), replace=False)\n            labels[disable_inds] = -1\n            #print ""was %s inds, disabling %s, now %s inds"" % (\n                #len(bg_inds), len(disable_inds), np.sum(labels == 0))\n\n        bbox_targets = np.zeros((len(inds_inside), 4), dtype=np.float32)\n        if gt_boxes.shape[0] != 0:\n            bbox_targets = _compute_targets(anchors, gt_boxes[argmax_overlaps, :])\n\n        bbox_inside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n        bbox_inside_weights[labels == 1, :] = np.array(cfg.TRAIN.RPN_BBOX_INSIDE_WEIGHTS)\n\n        bbox_outside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n        if cfg.TRAIN.RPN_POSITIVE_WEIGHT < 0:\n            # uniform weighting of examples (given non-uniform sampling)\n            num_examples = np.sum(labels >= 0)\n            positive_weights = np.ones((1, 4)) * 1.0 / num_examples\n            negative_weights = np.ones((1, 4)) * 1.0 / num_examples\n        else:\n            assert ((cfg.TRAIN.RPN_POSITIVE_WEIGHT > 0) &\n                    (cfg.TRAIN.RPN_POSITIVE_WEIGHT < 1))\n            positive_weights = (cfg.TRAIN.RPN_POSITIVE_WEIGHT /\n                                np.sum(labels == 1))\n            negative_weights = ((1.0 - cfg.TRAIN.RPN_POSITIVE_WEIGHT) /\n                                np.sum(labels == 0))\n        bbox_outside_weights[labels == 1, :] = positive_weights\n        bbox_outside_weights[labels == 0, :] = negative_weights\n\n        if DEBUG:\n            self._sums += bbox_targets[labels == 1, :].sum(axis=0)\n            self._squared_sums += (bbox_targets[labels == 1, :] ** 2).sum(axis=0)\n            self._counts += np.sum(labels == 1)\n            means = self._sums / self._counts\n            stds = np.sqrt(self._squared_sums / self._counts - means ** 2)\n            print \'means:\'\n            print means\n            print \'stdevs:\'\n            print stds\n\n        # map up to original set of anchors\n        labels = _unmap(labels, total_anchors, inds_inside, fill=-1)\n        bbox_targets = _unmap(bbox_targets, total_anchors, inds_inside, fill=0)\n        bbox_inside_weights = _unmap(bbox_inside_weights, total_anchors, inds_inside, fill=0)\n        bbox_outside_weights = _unmap(bbox_outside_weights, total_anchors, inds_inside, fill=0)\n\n        if DEBUG:\n            if gt_boxes.shape[0] != 0:\n                print \'rpn: max max_overlap\', np.max(max_overlaps)\n            else:\n                print \'rpn: max max_overlap\', 0\n            print \'rpn: num_positive\', np.sum(labels == 1)\n            print \'rpn: num_negative\', np.sum(labels == 0)\n            self._fg_sum += np.sum(labels == 1)\n            self._bg_sum += np.sum(labels == 0)\n            self._count += 1\n            print \'rpn: num_positive avg\', self._fg_sum / self._count\n            print \'rpn: num_negative avg\', self._bg_sum / self._count\n\n        # labels\n        labels = labels.reshape((1, height, width, A)).transpose(0, 3, 1, 2)\n        labels = labels.reshape((1, 1, A * height, width))\n        top[0].reshape(*labels.shape)\n        top[0].data[...] = labels\n\n        # bbox_targets\n        bbox_targets = bbox_targets \\\n            .reshape((1, height, width, A * 4)).transpose(0, 3, 1, 2)\n        top[1].reshape(*bbox_targets.shape)\n        top[1].data[...] = bbox_targets\n\n        # bbox_inside_weights\n        bbox_inside_weights = bbox_inside_weights \\\n            .reshape((1, height, width, A * 4)).transpose(0, 3, 1, 2)\n        assert bbox_inside_weights.shape[2] == height\n        assert bbox_inside_weights.shape[3] == width\n        top[2].reshape(*bbox_inside_weights.shape)\n        top[2].data[...] = bbox_inside_weights\n\n        # bbox_outside_weights\n        bbox_outside_weights = bbox_outside_weights \\\n            .reshape((1, height, width, A * 4)).transpose(0, 3, 1, 2)\n        assert bbox_outside_weights.shape[2] == height\n        assert bbox_outside_weights.shape[3] == width\n        top[3].reshape(*bbox_outside_weights.shape)\n        top[3].data[...] = bbox_outside_weights\n\n    def backward(self, top, propagate_down, bottom):\n        """"""This layer does not propagate gradients.""""""\n        pass\n\n    def reshape(self, bottom, top):\n        """"""Reshaping happens during the call to forward.""""""\n        pass\n\n\ndef _unmap(data, count, inds, fill=0):\n    """""" Unmap a subset of item (data) back to the original set of items (of\n    size count) """"""\n    if len(data.shape) == 1:\n        ret = np.empty((count, ), dtype=np.float32)\n        ret.fill(fill)\n        ret[inds] = data\n    else:\n        ret = np.empty((count, ) + data.shape[1:], dtype=np.float32)\n        ret.fill(fill)\n        ret[inds, :] = data\n    return ret\n\n\ndef _compute_targets(ex_rois, gt_rois):\n    """"""Compute bounding-box regression targets for an image.""""""\n\n    assert ex_rois.shape[0] == gt_rois.shape[0]\n    assert ex_rois.shape[1] == 4\n    assert gt_rois.shape[1] == 5\n\n    return bbox_transform(ex_rois, gt_rois[:, :4]).astype(np.float32, copy=False)\n'"
lib/rpn_msr/anchor_target_layer_tf.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n\nimport os\nimport yaml\nimport numpy as np\nimport numpy.random as npr\n\nfrom .generate_anchors import generate_anchors\nfrom ..utils.cython_bbox import bbox_overlaps, bbox_intersections\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\nfrom ..fast_rcnn.bbox_transform import bbox_transform\n# <<<< obsolete\n\nDEBUG = False\n\ndef anchor_target_layer(rpn_cls_score, gt_boxes, gt_ishard, dontcare_areas, im_info, _feat_stride = [16,], anchor_scales = [4 ,8, 16, 32]):\n    """"""\n    Assign anchors to ground-truth targets. Produces anchor classification\n    labels and bounding-box regression targets.\n    Parameters\n    ----------\n    rpn_cls_score: (1, H, W, Ax2) bg/fg scores of previous conv layer\n    gt_boxes: (G, 5) vstack of [x1, y1, x2, y2, class]\n    gt_ishard: (G, 1), 1 or 0 indicates difficult or not\n    dontcare_areas: (D, 4), some areas may contains small objs but no labelling. D may be 0\n    im_info: a list of [image_height, image_width, scale_ratios]\n    _feat_stride: the downsampling ratio of feature map to the original input image\n    anchor_scales: the scales to the basic_anchor (basic anchor is [16, 16])\n    ----------\n    Returns\n    ----------\n    rpn_labels : (HxWxA, 1), for each anchor, 0 denotes bg, 1 fg, -1 dontcare\n    rpn_bbox_targets: (HxWxA, 4), distances of the anchors to the gt_boxes(may contains some transform)\n                            that are the regression objectives\n    rpn_bbox_inside_weights: (HxWxA, 4) weights of each boxes, mainly accepts hyper param in cfg\n    rpn_bbox_outside_weights: (HxWxA, 4) used to balance the fg/bg,\n                            beacuse the numbers of bgs and fgs mays significiantly different\n    """"""\n    _anchors = generate_anchors(scales=np.array(anchor_scales))\n    _num_anchors = _anchors.shape[0]\n\n    if DEBUG:\n        print \'anchors:\'\n        print _anchors\n        print \'anchor shapes:\'\n        print np.hstack((\n            _anchors[:, 2::4] - _anchors[:, 0::4],\n            _anchors[:, 3::4] - _anchors[:, 1::4],\n        ))\n        _counts = cfg.EPS\n        _sums = np.zeros((1, 4))\n        _squared_sums = np.zeros((1, 4))\n        _fg_sum = 0\n        _bg_sum = 0\n        _count = 0\n\n    # allow boxes to sit over the edge by a small amount\n    _allowed_border =  0\n    # map of shape (..., H, W)\n    #height, width = rpn_cls_score.shape[1:3]\n\n    im_info = im_info[0]\n\n    # Algorithm:\n    #\n    # for each (H, W) location i\n    #   generate 9 anchor boxes centered on cell i\n    #   apply predicted bbox deltas at cell i to each of the 9 anchors\n    # filter out-of-image anchors\n    # measure GT overlap\n\n    assert rpn_cls_score.shape[0] == 1, \\\n        \'Only single item batches are supported\'\n\n    # map of shape (..., H, W)\n    height, width = rpn_cls_score.shape[1:3]\n\n    if DEBUG:\n        print \'AnchorTargetLayer: height\', height, \'width\', width\n        print \'\'\n        print \'im_size: ({}, {})\'.format(im_info[0], im_info[1])\n        print \'scale: {}\'.format(im_info[2])\n        print \'height, width: ({}, {})\'.format(height, width)\n        print \'rpn: gt_boxes.shape\', gt_boxes.shape\n        print \'rpn: gt_boxes\', gt_boxes\n\n    # 1. Generate proposals from bbox deltas and shifted anchors\n    shift_x = np.arange(0, width) * _feat_stride\n    shift_y = np.arange(0, height) * _feat_stride\n    shift_x, shift_y = np.meshgrid(shift_x, shift_y) # in W H order\n    # K is H x W\n    shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                        shift_x.ravel(), shift_y.ravel())).transpose()\n    # add A anchors (1, A, 4) to\n    # cell K shifts (K, 1, 4) to get\n    # shift anchors (K, A, 4)\n    # reshape to (K*A, 4) shifted anchors\n    A = _num_anchors\n    K = shifts.shape[0]\n    all_anchors = (_anchors.reshape((1, A, 4)) +\n                   shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n    all_anchors = all_anchors.reshape((K * A, 4))\n    total_anchors = int(K * A)\n\n    # only keep anchors inside the image\n    inds_inside = np.where(\n        (all_anchors[:, 0] >= -_allowed_border) &\n        (all_anchors[:, 1] >= -_allowed_border) &\n        (all_anchors[:, 2] < im_info[1] + _allowed_border) &  # width\n        (all_anchors[:, 3] < im_info[0] + _allowed_border)    # height\n    )[0]\n\n    if DEBUG:\n        print \'total_anchors\', total_anchors\n        print \'inds_inside\', len(inds_inside)\n\n    # keep only inside anchors\n    anchors = all_anchors[inds_inside, :]\n    if DEBUG:\n        print \'anchors.shape\', anchors.shape\n\n    # label: 1 is positive, 0 is negative, -1 is dont care\n    # (A)\n    labels = np.empty((len(inds_inside), ), dtype=np.float32)\n    labels.fill(-1)\n\n    # overlaps between the anchors and the gt boxes\n    # overlaps (ex, gt), shape is A x G\n    overlaps = bbox_overlaps(\n        np.ascontiguousarray(anchors, dtype=np.float),\n        np.ascontiguousarray(gt_boxes, dtype=np.float))\n    argmax_overlaps = overlaps.argmax(axis=1) # (A)\n    max_overlaps = overlaps[np.arange(len(inds_inside)), argmax_overlaps]\n    gt_argmax_overlaps = overlaps.argmax(axis=0) # G\n    gt_max_overlaps = overlaps[gt_argmax_overlaps,\n                               np.arange(overlaps.shape[1])]\n    gt_argmax_overlaps = np.where(overlaps == gt_max_overlaps)[0]\n\n    if not cfg.TRAIN.RPN_CLOBBER_POSITIVES:\n        # assign bg labels first so that positive labels can clobber them\n        labels[max_overlaps < cfg.TRAIN.RPN_NEGATIVE_OVERLAP] = 0\n\n    # fg label: for each gt, anchor with highest overlap\n    labels[gt_argmax_overlaps] = 1\n    # fg label: above threshold IOU\n    labels[max_overlaps >= cfg.TRAIN.RPN_POSITIVE_OVERLAP] = 1\n\n    if cfg.TRAIN.RPN_CLOBBER_POSITIVES:\n        # assign bg labels last so that negative labels can clobber positives\n        labels[max_overlaps < cfg.TRAIN.RPN_NEGATIVE_OVERLAP] = 0\n\n    # preclude dontcare areas\n    if dontcare_areas is not None and dontcare_areas.shape[0] > 0:\n        # intersec shape is D x A\n        intersecs = bbox_intersections(\n            np.ascontiguousarray(dontcare_areas, dtype=np.float), # D x 4\n            np.ascontiguousarray(anchors, dtype=np.float) # A x 4\n        )\n        intersecs_ = intersecs.sum(axis=0) # A x 1\n        labels[intersecs_ > cfg.TRAIN.DONTCARE_AREA_INTERSECTION_HI] = -1\n\n    # preclude hard samples that are highly occlusioned, truncated or difficult to see\n    if cfg.TRAIN.PRECLUDE_HARD_SAMPLES and gt_ishard is not None and gt_ishard.shape[0] > 0:\n        assert gt_ishard.shape[0] == gt_boxes.shape[0]\n        gt_ishard = gt_ishard.astype(int)\n        gt_hardboxes = gt_boxes[gt_ishard == 1, :]\n        if gt_hardboxes.shape[0] > 0:\n            # H x A\n            hard_overlaps = bbox_overlaps(\n                np.ascontiguousarray(gt_hardboxes, dtype=np.float), # H x 4\n                np.ascontiguousarray(anchors, dtype=np.float)) # A x 4\n            hard_max_overlaps = hard_overlaps.max(axis=0)  # (A)\n            labels[hard_max_overlaps >= cfg.TRAIN.RPN_POSITIVE_OVERLAP] = -1\n            max_intersec_label_inds = hard_overlaps.argmax(axis=1) # H x 1\n            labels[max_intersec_label_inds] = -1 #\n\n    # subsample positive labels if we have too many\n    num_fg = int(cfg.TRAIN.RPN_FG_FRACTION * cfg.TRAIN.RPN_BATCHSIZE)\n    fg_inds = np.where(labels == 1)[0]\n    if len(fg_inds) > num_fg:\n        disable_inds = npr.choice(\n            fg_inds, size=(len(fg_inds) - num_fg), replace=False)\n        labels[disable_inds] = -1\n\n    # subsample negative labels if we have too many\n    num_bg = cfg.TRAIN.RPN_BATCHSIZE - np.sum(labels == 1)\n    bg_inds = np.where(labels == 0)[0]\n    if len(bg_inds) > num_bg:\n        disable_inds = npr.choice(\n            bg_inds, size=(len(bg_inds) - num_bg), replace=False)\n        labels[disable_inds] = -1\n        #print ""was %s inds, disabling %s, now %s inds"" % (\n            #len(bg_inds), len(disable_inds), np.sum(labels == 0))\n\n    bbox_targets = np.zeros((len(inds_inside), 4), dtype=np.float32)\n    bbox_targets = _compute_targets(anchors, gt_boxes[argmax_overlaps, :])\n\n    bbox_inside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n    bbox_inside_weights[labels == 1, :] = np.array(cfg.TRAIN.RPN_BBOX_INSIDE_WEIGHTS)\n\n    bbox_outside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n    if cfg.TRAIN.RPN_POSITIVE_WEIGHT < 0:\n        # uniform weighting of examples (given non-uniform sampling)\n        num_examples = np.sum(labels >= 0) + 1\n        # positive_weights = np.ones((1, 4)) * 1.0 / num_examples\n        # negative_weights = np.ones((1, 4)) * 1.0 / num_examples\n        positive_weights = np.ones((1, 4))\n        negative_weights = np.zeros((1, 4))\n    else:\n        assert ((cfg.TRAIN.RPN_POSITIVE_WEIGHT > 0) &\n                (cfg.TRAIN.RPN_POSITIVE_WEIGHT < 1))\n        positive_weights = (cfg.TRAIN.RPN_POSITIVE_WEIGHT /\n                            (np.sum(labels == 1)) + 1)\n        negative_weights = ((1.0 - cfg.TRAIN.RPN_POSITIVE_WEIGHT) /\n                            (np.sum(labels == 0)) + 1)\n    bbox_outside_weights[labels == 1, :] = positive_weights\n    bbox_outside_weights[labels == 0, :] = negative_weights\n\n    if DEBUG:\n        _sums += bbox_targets[labels == 1, :].sum(axis=0)\n        _squared_sums += (bbox_targets[labels == 1, :] ** 2).sum(axis=0)\n        _counts += np.sum(labels == 1)\n        means = _sums / _counts\n        stds = np.sqrt(_squared_sums / _counts - means ** 2)\n        print \'means:\'\n        print means\n        print \'stdevs:\'\n        print stds\n\n    # map up to original set of anchors\n    labels = _unmap(labels, total_anchors, inds_inside, fill=-1)\n    bbox_targets = _unmap(bbox_targets, total_anchors, inds_inside, fill=0)\n    bbox_inside_weights = _unmap(bbox_inside_weights, total_anchors, inds_inside, fill=0)\n    bbox_outside_weights = _unmap(bbox_outside_weights, total_anchors, inds_inside, fill=0)\n\n    if DEBUG:\n        print \'rpn: max max_overlap\', np.max(max_overlaps)\n        print \'rpn: num_positive\', np.sum(labels == 1)\n        print \'rpn: num_negative\', np.sum(labels == 0)\n        _fg_sum += np.sum(labels == 1)\n        _bg_sum += np.sum(labels == 0)\n        _count += 1\n        print \'rpn: num_positive avg\', _fg_sum / _count\n        print \'rpn: num_negative avg\', _bg_sum / _count\n\n    # labels\n    #pdb.set_trace()\n    labels = labels.reshape((1, height, width, A))\n    rpn_labels = labels\n\n    # bbox_targets\n    bbox_targets = bbox_targets \\\n        .reshape((1, height, width, A * 4))\n\n    rpn_bbox_targets = bbox_targets\n    # bbox_inside_weights\n    bbox_inside_weights = bbox_inside_weights \\\n        .reshape((1, height, width, A * 4))\n    #assert bbox_inside_weights.shape[2] == height\n    #assert bbox_inside_weights.shape[3] == width\n\n    rpn_bbox_inside_weights = bbox_inside_weights\n\n    # bbox_outside_weights\n    bbox_outside_weights = bbox_outside_weights \\\n        .reshape((1, height, width, A * 4))\n    #assert bbox_outside_weights.shape[2] == height\n    #assert bbox_outside_weights.shape[3] == width\n\n    rpn_bbox_outside_weights = bbox_outside_weights\n\n    return rpn_labels, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights\n\n\n\ndef _unmap(data, count, inds, fill=0):\n    """""" Unmap a subset of item (data) back to the original set of items (of\n    size count) """"""\n    if len(data.shape) == 1:\n        ret = np.empty((count, ), dtype=np.float32)\n        ret.fill(fill)\n        ret[inds] = data\n    else:\n        ret = np.empty((count, ) + data.shape[1:], dtype=np.float32)\n        ret.fill(fill)\n        ret[inds, :] = data\n    return ret\n\n\ndef _compute_targets(ex_rois, gt_rois):\n    """"""Compute bounding-box regression targets for an image.""""""\n\n    assert ex_rois.shape[0] == gt_rois.shape[0]\n    assert ex_rois.shape[1] == 4\n    assert gt_rois.shape[1] == 5\n\n    return bbox_transform(ex_rois, gt_rois[:, :4]).astype(np.float32, copy=False)\n'"
lib/rpn_msr/generate.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\nfrom ..utils.blob import im_list_to_blob\nfrom ..utils.timer import Timer\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\n\n\ndef _vis_proposals(im, dets, thresh=0.5):\n    """"""Draw detected bounding boxes.""""""\n    inds = np.where(dets[:, -1] >= thresh)[0]\n    if len(inds) == 0:\n        return\n\n    class_name = \'obj\'\n    im = im[:, :, (2, 1, 0)]\n    fig, ax = plt.subplots(figsize=(12, 12))\n    ax.imshow(im, aspect=\'equal\')\n    for i in inds:\n        bbox = dets[i, :4]\n        score = dets[i, -1]\n\n        ax.add_patch(\n            plt.Rectangle((bbox[0], bbox[1]),\n                          bbox[2] - bbox[0],\n                          bbox[3] - bbox[1], fill=False,\n                          edgecolor=\'red\', linewidth=3.5)\n            )\n        ax.text(bbox[0], bbox[1] - 2,\n                \'{:s} {:.3f}\'.format(class_name, score),\n                bbox=dict(facecolor=\'blue\', alpha=0.5),\n                fontsize=14, color=\'white\')\n\n    ax.set_title((\'{} detections with \'\n                  \'p({} | box) >= {:.1f}\').format(class_name, class_name,\n                                                  thresh),\n                  fontsize=14)\n    plt.axis(\'off\')\n    plt.tight_layout()\n    plt.draw()\n\ndef _get_image_blob(im):\n    """"""Converts an image into a network input.\n\n    Arguments:\n        im (ndarray): a color image in BGR order\n\n    Returns:\n        blob (ndarray): a data blob holding an image pyramid\n        im_scale_factors (list): list of image scales (relative to im) used\n            in the image pyramid\n    """"""\n    im_orig = im.astype(np.float32, copy=True)\n    im_orig -= cfg.PIXEL_MEANS\n\n    processed_ims = []\n\n    assert len(cfg.TEST.SCALES_BASE) == 1\n    im_scale = cfg.TRAIN.SCALES_BASE[0]\n\n    im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale,\n                    interpolation=cv2.INTER_LINEAR)\n    im_info = np.hstack((im.shape[:2], im_scale))[np.newaxis, :]\n    processed_ims.append(im)\n\n    # Create a blob to hold the input images\n    blob = im_list_to_blob(processed_ims)\n\n    return blob, im_info\n\ndef im_proposals(net, im):\n    """"""Generate RPN proposals on a single image.""""""\n    blobs = {}\n    blobs[\'data\'], blobs[\'im_info\'] = _get_image_blob(im)\n    net.blobs[\'data\'].reshape(*(blobs[\'data\'].shape))\n    net.blobs[\'im_info\'].reshape(*(blobs[\'im_info\'].shape))\n    blobs_out = net.forward(\n            data=blobs[\'data\'].astype(np.float32, copy=False),\n            im_info=blobs[\'im_info\'].astype(np.float32, copy=False))\n\n    scale = blobs[\'im_info\'][0, 2]\n    boxes = blobs_out[\'rois\'][:, 1:].copy() / scale\n    scores = blobs_out[\'scores\'].copy()\n    return boxes, scores\n\ndef imdb_proposals(net, imdb):\n    """"""Generate RPN proposals on all images in an imdb.""""""\n\n    _t = Timer()\n    imdb_boxes = [[] for _ in xrange(imdb.num_images)]\n    for i in xrange(imdb.num_images):\n        im = cv2.imread(imdb.image_path_at(i))\n        _t.tic()\n        imdb_boxes[i], scores = im_proposals(net, im)\n        _t.toc()\n        print \'im_proposals: {:d}/{:d} {:.3f}s\' \\\n              .format(i + 1, imdb.num_images, _t.average_time)\n        if 0:\n            dets = np.hstack((imdb_boxes[i], scores))\n            # from IPython import embed; embed()\n            _vis_proposals(im, dets[:3, :], thresh=0.9)\n            plt.show()\n\n    return imdb_boxes\n\ndef imdb_proposals_det(net, imdb):\n    """"""Generate RPN proposals on all images in an imdb.""""""\n\n    _t = Timer()\n    imdb_boxes = [[] for _ in xrange(imdb.num_images)]\n    for i in xrange(imdb.num_images):\n        im = cv2.imread(imdb.image_path_at(i))\n        _t.tic()\n        boxes, scores = im_proposals(net, im)\n        _t.toc()\n        print \'im_proposals: {:d}/{:d} {:.3f}s\' \\\n              .format(i + 1, imdb.num_images, _t.average_time)\n        dets = np.hstack((boxes, scores))\n        imdb_boxes[i] = dets\n\n        if 0:            \n            # from IPython import embed; embed()\n            _vis_proposals(im, dets[:3, :], thresh=0.9)\n            plt.show()\n\n    return imdb_boxes\n'"
lib/rpn_msr/generate_anchors.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n\nimport numpy as np\n\n# Verify that we compute the same anchors as Shaoqing\'s matlab implementation:\n#\n#    >> load output/rpn_cachedir/faster_rcnn_VOC2007_ZF_stage1_rpn/anchors.mat\n#    >> anchors\n#\n#    anchors =\n#\n#       -83   -39   100    56\n#      -175   -87   192   104\n#      -359  -183   376   200\n#       -55   -55    72    72\n#      -119  -119   136   136\n#      -247  -247   264   264\n#       -35   -79    52    96\n#       -79  -167    96   184\n#      -167  -343   184   360\n\n#array([[ -83.,  -39.,  100.,   56.],\n#       [-175.,  -87.,  192.,  104.],\n#       [-359., -183.,  376.,  200.],\n#       [ -55.,  -55.,   72.,   72.],\n#       [-119., -119.,  136.,  136.],\n#       [-247., -247.,  264.,  264.],\n#       [ -35.,  -79.,   52.,   96.],\n#       [ -79., -167.,   96.,  184.],\n#       [-167., -343.,  184.,  360.]])\n\ndef generate_anchors(base_size=16, ratios=[0.5, 1, 2],\n                     scales=2**np.arange(3, 6)):\n    """"""\n    Generate anchor (reference) windows by enumerating aspect ratios X\n    scales wrt a reference (0, 0, 15, 15) window.\n    """"""\n\n    base_anchor = np.array([1, 1, base_size, base_size]) - 1\n    ratio_anchors = _ratio_enum(base_anchor, ratios)\n    anchors = np.vstack([_scale_enum(ratio_anchors[i, :], scales)\n                         for i in xrange(ratio_anchors.shape[0])])\n    return anchors\n\ndef _whctrs(anchor):\n    """"""\n    Return width, height, x center, and y center for an anchor (window).\n    """"""\n\n    w = anchor[2] - anchor[0] + 1\n    h = anchor[3] - anchor[1] + 1\n    x_ctr = anchor[0] + 0.5 * (w - 1)\n    y_ctr = anchor[1] + 0.5 * (h - 1)\n    return w, h, x_ctr, y_ctr\n\ndef _mkanchors(ws, hs, x_ctr, y_ctr):\n    """"""\n    Given a vector of widths (ws) and heights (hs) around a center\n    (x_ctr, y_ctr), output a set of anchors (windows).\n    """"""\n\n    ws = ws[:, np.newaxis]\n    hs = hs[:, np.newaxis]\n    anchors = np.hstack((x_ctr - 0.5 * (ws - 1),\n                         y_ctr - 0.5 * (hs - 1),\n                         x_ctr + 0.5 * (ws - 1),\n                         y_ctr + 0.5 * (hs - 1)))\n    return anchors\n\ndef _ratio_enum(anchor, ratios):\n    """"""\n    Enumerate a set of anchors for each aspect ratio wrt an anchor.\n    """"""\n\n    w, h, x_ctr, y_ctr = _whctrs(anchor)\n    size = w * h\n    size_ratios = size / ratios\n    ws = np.round(np.sqrt(size_ratios))\n    hs = np.round(ws * ratios)\n    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n    return anchors\n\ndef _scale_enum(anchor, scales):\n    """"""\n    Enumerate a set of anchors for each scale wrt an anchor.\n    """"""\n\n    w, h, x_ctr, y_ctr = _whctrs(anchor)\n    ws = w * scales\n    hs = h * scales\n    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n    return anchors\n\nif __name__ == \'__main__\':\n    import time\n    t = time.time()\n    a = generate_anchors()\n    print time.time() - t\n    print a\n    from IPython import embed; embed()\n'"
lib/rpn_msr/proposal_layer.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n\nimport caffe\n\nimport numpy as np\nimport yaml\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\nfrom .generate_anchors import generate_anchors\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.bbox_transform import bbox_transform_inv, clip_boxes\nfrom ..fast_rcnn.nms_wrapper import nms\n# <<<< obsolete\n\nDEBUG = False\n\nclass ProposalLayer(caffe.Layer):\n    """"""\n    Outputs object detection proposals by applying estimated bounding-box\n    transformations to a set of regular boxes (called ""anchors"").\n    """"""\n\n    def setup(self, bottom, top):\n        # parse the layer parameter string, which must be valid YAML\n        layer_params = yaml.load(self.param_str_)\n\n        self._feat_stride = layer_params[\'feat_stride\']\n        self._anchors     = generate_anchors(cfg.TRAIN.RPN_BASE_SIZE, cfg.TRAIN.RPN_ASPECTS, cfg.TRAIN.RPN_SCALES)\n        self._num_anchors = self._anchors.shape[0]\n\n        if DEBUG:\n            print \'feat_stride: {}\'.format(self._feat_stride)\n            print \'anchors:\'\n            print self._anchors\n\n        # rois blob: holds R regions of interest, each is a 5-tuple\n        # (n, x1, y1, x2, y2) specifying an image batch index n and a\n        # rectangle (x1, y1, x2, y2)\n        top[0].reshape(1, 5)\n\n        # scores blob: holds scores for R regions of interest\n        if len(top) > 1:\n            top[1].reshape(1, 1, 1, 1)\n\n    def forward(self, bottom, top):\n        # Algorithm:\n        #\n        # for each (H, W) location i\n        #   generate A anchor boxes centered on cell i\n        #   apply predicted bbox deltas at cell i to each of the A anchors\n        # clip predicted boxes to image\n        # remove predicted boxes with either height or width < threshold\n        # sort all (proposal, score) pairs by score from highest to lowest\n        # take top pre_nms_topN proposals before NMS\n        # apply NMS with threshold 0.7 to remaining proposals\n        # take after_nms_topN proposals after NMS\n        # return the top proposals (-> RoIs top, scores top)\n\n        assert bottom[0].data.shape[0] == 1, \\\n            \'Only single item batches are supported\'\n        # cfg_key = str(self.phase) # either \'TRAIN\' or \'TEST\'\n        cfg_key = \'TEST\'\n        pre_nms_topN  = cfg[cfg_key].RPN_PRE_NMS_TOP_N\n        post_nms_topN = cfg[cfg_key].RPN_POST_NMS_TOP_N\n        nms_thresh    = cfg[cfg_key].RPN_NMS_THRESH\n        min_size      = cfg[cfg_key].RPN_MIN_SIZE\n\n        # the first set of _num_anchors channels are bg probs\n        # the second set are the fg probs, which we want\n        scores = bottom[0].data[:, self._num_anchors:, :, :]\n        bbox_deltas = bottom[1].data\n        im_info = bottom[2].data[0, :]\n\n        if DEBUG:\n            print \'im_size: ({}, {})\'.format(im_info[0], im_info[1])\n            print \'scale: {}\'.format(im_info[2])\n\n        # 1. Generate proposals from bbox deltas and shifted anchors\n        height, width = scores.shape[-2:]\n\n        if DEBUG:\n            print \'score map size: {}\'.format(scores.shape)\n\n        # Enumerate all shifts\n        shift_x = np.arange(0, width) * self._feat_stride\n        shift_y = np.arange(0, height) * self._feat_stride\n        shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n        shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n\n        # Enumerate all shifted anchors:\n        #\n        # add A anchors (1, A, 4) to\n        # cell K shifts (K, 1, 4) to get\n        # shift anchors (K, A, 4)\n        # reshape to (K*A, 4) shifted anchors\n        A = self._num_anchors\n        K = shifts.shape[0]\n        anchors = self._anchors.reshape((1, A, 4)) + \\\n                  shifts.reshape((1, K, 4)).transpose((1, 0, 2))\n        anchors = anchors.reshape((K * A, 4))\n\n        # Transpose and reshape predicted bbox transformations to get them\n        # into the same order as the anchors:\n        #\n        # bbox deltas will be (1, 4 * A, H, W) format\n        # transpose to (1, H, W, 4 * A)\n        # reshape to (1 * H * W * A, 4) where rows are ordered by (h, w, a)\n        # in slowest to fastest order\n        bbox_deltas = bbox_deltas.transpose((0, 2, 3, 1)).reshape((-1, 4))\n\n        # Same story for the scores:\n        #\n        # scores are (1, A, H, W) format\n        # transpose to (1, H, W, A)\n        # reshape to (1 * H * W * A, 1) where rows are ordered by (h, w, a)\n        scores = scores.transpose((0, 2, 3, 1)).reshape((-1, 1))\n\n        # Convert anchors into proposals via bbox transformations\n        proposals = bbox_transform_inv(anchors, bbox_deltas)\n\n        # 2. clip predicted boxes to image\n        proposals = clip_boxes(proposals, im_info[:2])\n\n        # 3. remove predicted boxes with either height or width < threshold\n        # (NOTE: convert min_size to input image scale stored in im_info[2])\n        keep = _filter_boxes(proposals, min_size * im_info[2])\n        proposals = proposals[keep, :]\n        scores = scores[keep]\n\n        # 4. sort all (proposal, score) pairs by score from highest to lowest\n        # 5. take top pre_nms_topN (e.g. 6000)\n        order = scores.ravel().argsort()[::-1]\n        if pre_nms_topN > 0:\n            order = order[:pre_nms_topN]\n        proposals = proposals[order, :]\n        scores = scores[order]\n\n        # 6. apply nms (e.g. threshold = 0.7)\n        # 7. take after_nms_topN (e.g. 300)\n        # 8. return the top proposals (-> RoIs top)\n        keep = nms(np.hstack((proposals, scores)), nms_thresh)\n        if post_nms_topN > 0:\n            keep = keep[:post_nms_topN]\n        proposals = proposals[keep, :]\n        scores = scores[keep]\n        print scores.shape\n\n        # Output rois blob\n        # Our RPN implementation only supports a single input image, so all\n        # batch inds are 0\n        batch_inds = np.zeros((proposals.shape[0], 1), dtype=np.float32)\n        blob = np.hstack((batch_inds, proposals.astype(np.float32, copy=False)))\n        top[0].reshape(*(blob.shape))\n        top[0].data[...] = blob\n\n        # [Optional] output scores blob\n        if len(top) > 1:\n            top[1].reshape(*(scores.shape))\n            top[1].data[...] = scores\n\n    def backward(self, top, propagate_down, bottom):\n        """"""This layer does not propagate gradients.""""""\n        pass\n\n    def reshape(self, bottom, top):\n        """"""Reshaping happens during the call to forward.""""""\n        pass\n\ndef _filter_boxes(boxes, min_size):\n    """"""Remove all boxes with any side smaller than min_size.""""""\n    ws = boxes[:, 2] - boxes[:, 0] + 1\n    hs = boxes[:, 3] - boxes[:, 1] + 1\n    keep = np.where((ws >= min_size) & (hs >= min_size))[0]\n    return keep\n'"
lib/rpn_msr/proposal_layer_tf.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n\nimport numpy as np\nimport yaml\n\nfrom .generate_anchors import generate_anchors\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\nfrom ..fast_rcnn.bbox_transform import bbox_transform_inv, clip_boxes\nfrom ..fast_rcnn.nms_wrapper import nms\n# <<<< obsolete\n\n\nDEBUG = False\n""""""\nOutputs object detection proposals by applying estimated bounding-box\ntransformations to a set of regular boxes (called ""anchors"").\n""""""\ndef proposal_layer(rpn_cls_prob_reshape, rpn_bbox_pred, im_info, cfg_key, _feat_stride = [16,], anchor_scales = [8, 16, 32]):\n    """"""\n    Parameters\n    ----------\n    rpn_cls_prob_reshape: (1 , H , W , Ax2) outputs of RPN, prob of bg or fg\n                         NOTICE: the old version is ordered by (1, H, W, 2, A) !!!!\n    rpn_bbox_pred: (1 , H , W , Ax4), rgs boxes output of RPN\n    im_info: a list of [image_height, image_width, scale_ratios]\n    cfg_key: \'TRAIN\' or \'TEST\'\n    _feat_stride: the downsampling ratio of feature map to the original input image\n    anchor_scales: the scales to the basic_anchor (basic anchor is [16, 16])\n    ----------\n    Returns\n    ----------\n    rpn_rois : (1 x H x W x A, 5) e.g. [0, x1, y1, x2, y2]\n\n    # Algorithm:\n    #\n    # for each (H, W) location i\n    #   generate A anchor boxes centered on cell i\n    #   apply predicted bbox deltas at cell i to each of the A anchors\n    # clip predicted boxes to image\n    # remove predicted boxes with either height or width < threshold\n    # sort all (proposal, score) pairs by score from highest to lowest\n    # take top pre_nms_topN proposals before NMS\n    # apply NMS with threshold 0.7 to remaining proposals\n    # take after_nms_topN proposals after NMS\n    # return the top proposals (-> RoIs top, scores top)\n    #layer_params = yaml.load(self.param_str_)\n\n    """"""\n    _anchors = generate_anchors(scales=np.array(anchor_scales))\n    _num_anchors = _anchors.shape[0]\n    # rpn_cls_prob_reshape = np.transpose(rpn_cls_prob_reshape,[0,3,1,2]) #-> (1 , 2xA, H , W)\n    # rpn_bbox_pred = np.transpose(rpn_bbox_pred,[0,3,1,2])              # -> (1 , Ax4, H , W)\n\n    #rpn_cls_prob_reshape = np.transpose(np.reshape(rpn_cls_prob_reshape,[1,rpn_cls_prob_reshape.shape[0],rpn_cls_prob_reshape.shape[1],rpn_cls_prob_reshape.shape[2]]),[0,3,2,1])\n    #rpn_bbox_pred = np.transpose(rpn_bbox_pred,[0,3,2,1])\n    im_info = im_info[0]\n\n    assert rpn_cls_prob_reshape.shape[0] == 1, \\\n        \'Only single item batches are supported\'\n    # cfg_key = str(self.phase) # either \'TRAIN\' or \'TEST\'\n    #cfg_key = \'TEST\'\n    pre_nms_topN  = cfg[cfg_key].RPN_PRE_NMS_TOP_N\n    post_nms_topN = cfg[cfg_key].RPN_POST_NMS_TOP_N\n    nms_thresh    = cfg[cfg_key].RPN_NMS_THRESH\n    min_size      = cfg[cfg_key].RPN_MIN_SIZE\n\n    height, width = rpn_cls_prob_reshape.shape[1:3]\n\n    # the first set of _num_anchors channels are bg probs\n    # the second set are the fg probs, which we want\n    # (1, H, W, A)\n    scores = np.reshape(np.reshape(rpn_cls_prob_reshape, [1, height, width, _num_anchors, 2])[:,:,:,:,1],\n                        [1, height, width, _num_anchors])\n\n    # TODO: NOTICE: the old version is ordered by (1, H, W, 2, A) !!!!\n    # TODO: if you use the old trained model, VGGnet_fast_rcnn_iter_70000.ckpt, uncomment this line\n    # scores = rpn_cls_prob_reshape[:,:,:,_num_anchors:]\n\n    bbox_deltas = rpn_bbox_pred\n    #im_info = bottom[2].data[0, :]\n\n    if DEBUG:\n        print \'im_size: ({}, {})\'.format(im_info[0], im_info[1])\n        print \'scale: {}\'.format(im_info[2])\n\n    # 1. Generate proposals from bbox deltas and shifted anchors\n    if DEBUG:\n        print \'score map size: {}\'.format(scores.shape)\n\n    # Enumerate all shifts\n    shift_x = np.arange(0, width) * _feat_stride\n    shift_y = np.arange(0, height) * _feat_stride\n    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n    shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                        shift_x.ravel(), shift_y.ravel())).transpose()\n\n    # Enumerate all shifted anchors:\n    #\n    # add A anchors (1, A, 4) to\n    # cell K shifts (K, 1, 4) to get\n    # shift anchors (K, A, 4)\n    # reshape to (K*A, 4) shifted anchors\n    A = _num_anchors\n    K = shifts.shape[0]\n    anchors = _anchors.reshape((1, A, 4)) + \\\n              shifts.reshape((1, K, 4)).transpose((1, 0, 2))\n    anchors = anchors.reshape((K * A, 4))\n\n    # Transpose and reshape predicted bbox transformations to get them\n    # into the same order as the anchors:\n    #\n    # bbox deltas will be (1, 4 * A, H, W) format\n    # transpose to (1, H, W, 4 * A)\n    # reshape to (1 * H * W * A, 4) where rows are ordered by (h, w, a)\n    # in slowest to fastest order\n    bbox_deltas = bbox_deltas.reshape((-1, 4)) #(HxWxA, 4)\n\n    # Same story for the scores:\n    #\n    # scores are (1, A, H, W) format\n    # transpose to (1, H, W, A)\n    # reshape to (1 * H * W * A, 1) where rows are ordered by (h, w, a)\n    scores = scores.reshape((-1, 1))\n\n    # Convert anchors into proposals via bbox transformations\n    proposals = bbox_transform_inv(anchors, bbox_deltas)\n\n    # 2. clip predicted boxes to image\n    proposals = clip_boxes(proposals, im_info[:2])\n\n    # 3. remove predicted boxes with either height or width < threshold\n    # (NOTE: convert min_size to input image scale stored in im_info[2])\n    keep = _filter_boxes(proposals, min_size * im_info[2])\n    proposals = proposals[keep, :]\n    scores = scores[keep]\n\n    # # remove irregular boxes, too fat too tall\n    # keep = _filter_irregular_boxes(proposals)\n    # proposals = proposals[keep, :]\n    # scores = scores[keep]\n\n    # 4. sort all (proposal, score) pairs by score from highest to lowest\n    # 5. take top pre_nms_topN (e.g. 6000)\n    order = scores.ravel().argsort()[::-1]\n    if pre_nms_topN > 0:\n        order = order[:pre_nms_topN]\n    proposals = proposals[order, :]\n    scores = scores[order]\n\n    # 6. apply nms (e.g. threshold = 0.7)\n    # 7. take after_nms_topN (e.g. 300)\n    # 8. return the top proposals (-> RoIs top)\n    keep = nms(np.hstack((proposals, scores)), nms_thresh)\n    if post_nms_topN > 0:\n        keep = keep[:post_nms_topN]\n    proposals = proposals[keep, :]\n    scores = scores[keep]\n    # Output rois blob\n    # Our RPN implementation only supports a single input image, so all\n    # batch inds are 0\n    batch_inds = np.zeros((proposals.shape[0], 1), dtype=np.float32)\n    blob = np.hstack((batch_inds, proposals.astype(np.float32, copy=False)))\n    return blob\n    #top[0].reshape(*(blob.shape))\n    #top[0].data[...] = blob\n\n    # [Optional] output scores blob\n    #if len(top) > 1:\n    #    top[1].reshape(*(scores.shape))\n    #    top[1].data[...] = scores\n\ndef _filter_boxes(boxes, min_size):\n    """"""Remove all boxes with any side smaller than min_size.""""""\n    ws = boxes[:, 2] - boxes[:, 0] + 1\n    hs = boxes[:, 3] - boxes[:, 1] + 1\n    keep = np.where((ws >= min_size) & (hs >= min_size))[0]\n    return keep\n\ndef _filter_irregular_boxes(boxes, min_ratio = 0.2, max_ratio = 5):\n    """"""Remove all boxes with any side smaller than min_size.""""""\n    ws = boxes[:, 2] - boxes[:, 0] + 1\n    hs = boxes[:, 3] - boxes[:, 1] + 1\n    rs = ws / hs\n    keep = np.where((rs <= max_ratio) & (rs >= min_ratio))[0]\n    return keep\n'"
lib/rpn_msr/proposal_target_layer_tf.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n\nimport yaml\nimport numpy as np\nimport numpy.random as npr\nimport pdb\n\nfrom ..utils.cython_bbox import bbox_overlaps, bbox_intersections\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\nfrom ..fast_rcnn.bbox_transform import bbox_transform\n# <<<< obsolete\n\nDEBUG = False\n\ndef proposal_target_layer(rpn_rois, gt_boxes, gt_ishard, dontcare_areas, _num_classes):\n    """"""\n    Assign object detection proposals to ground-truth targets. Produces proposal\n    classification labels and bounding-box regression targets.\n    Parameters\n    ----------\n    rpn_rois:  (1 x H x W x A, 5) [0, x1, y1, x2, y2]\n    gt_boxes: (G, 5) [x1 ,y1 ,x2, y2, class] int\n    gt_ishard: (G, 1) {0 | 1} 1 indicates hard\n    dontcare_areas: (D, 4) [ x1, y1, x2, y2]\n    _num_classes\n    ----------\n    Returns\n    ----------\n    rois: (1 x H x W x A, 5) [0, x1, y1, x2, y2]\n    labels: (1 x H x W x A, 1) {0,1,...,_num_classes-1}\n    bbox_targets: (1 x H x W x A, K x4) [dx1, dy1, dx2, dy2]\n    bbox_inside_weights: (1 x H x W x A, Kx4) 0, 1 masks for the computing loss\n    bbox_outside_weights: (1 x H x W x A, Kx4) 0, 1 masks for the computing loss\n    """"""\n\n    # Proposal ROIs (0, x1, y1, x2, y2) coming from RPN\n    # (i.e., rpn.proposal_layer.ProposalLayer), or any other source\n    all_rois = rpn_rois\n    # TODO(rbg): it\'s annoying that sometimes I have extra info before\n    # and other times after box coordinates -- normalize to one format\n\n    # Include ground-truth boxes in the set of candidate rois\n    if cfg.TRAIN.PRECLUDE_HARD_SAMPLES and gt_ishard is not None and gt_ishard.shape[0] > 0:\n        assert gt_ishard.shape[0] == gt_boxes.shape[0]\n        gt_ishard = gt_ishard.astype(int)\n        gt_easyboxes = gt_boxes[gt_ishard != 1, :]\n    else:\n        gt_easyboxes = gt_boxes\n\n    """"""\n    add the ground-truth to rois will cause zero loss! not good for visuallization\n    """"""\n    jittered_gt_boxes = _jitter_gt_boxes(gt_easyboxes)\n    zeros = np.zeros((gt_easyboxes.shape[0] * 2, 1), dtype=gt_easyboxes.dtype)\n    all_rois = np.vstack((all_rois, \\\n         np.hstack((zeros, np.vstack((gt_easyboxes[:, :-1], jittered_gt_boxes[:, :-1]))))))\n\n    # Sanity check: single batch only\n    assert np.all(all_rois[:, 0] == 0), \\\n            \'Only single item batches are supported\'\n\n    num_images = 1\n    rois_per_image = cfg.TRAIN.BATCH_SIZE / num_images\n    fg_rois_per_image = int(np.round(cfg.TRAIN.FG_FRACTION * rois_per_image))\n\n    # Sample rois with classification labels and bounding box regression\n    # targets\n    labels, rois, bbox_targets, bbox_inside_weights = _sample_rois(\n        all_rois, gt_boxes, gt_ishard, dontcare_areas, fg_rois_per_image,\n        rois_per_image, _num_classes)\n\n    # _count = 1\n    # if DEBUG:\n    #     if _count == 1:\n    #         _fg_num, _bg_num = 0, 0\n    #     print \'num fg: {}\'.format((labels > 0).sum())\n    #     print \'num bg: {}\'.format((labels == 0).sum())\n    #     _count += 1\n    #     _fg_num += (labels > 0).sum()\n    #     _bg_num += (labels == 0).sum()\n    #     print \'num fg avg: {}\'.format(_fg_num / _count)\n    #     print \'num bg avg: {}\'.format(_bg_num / _count)\n    #     print \'ratio: {:.3f}\'.format(float(_fg_num) / float(_bg_num))\n\n    rois = rois.reshape(-1, 5)\n    labels = labels.reshape(-1, 1)\n    bbox_targets = bbox_targets.reshape(-1, _num_classes*4)\n    bbox_inside_weights = bbox_inside_weights.reshape(-1, _num_classes*4)\n\n    bbox_outside_weights = np.array(bbox_inside_weights > 0).astype(np.float32)\n\n    return rois, labels, bbox_targets, bbox_inside_weights, bbox_outside_weights\n\ndef _sample_rois(all_rois, gt_boxes, gt_ishard, dontcare_areas, fg_rois_per_image, rois_per_image, num_classes):\n    """"""Generate a random sample of RoIs comprising foreground and background\n    examples.\n    """"""\n    # overlaps: R x G\n    overlaps = bbox_overlaps(\n        np.ascontiguousarray(all_rois[:, 1:5], dtype=np.float),\n        np.ascontiguousarray(gt_boxes[:, :4], dtype=np.float))\n    gt_assignment = overlaps.argmax(axis=1) # R\n    max_overlaps = overlaps.max(axis=1) # R\n    labels = gt_boxes[gt_assignment, 4]\n\n    # preclude hard samples\n    ignore_inds = np.empty(shape = (0), dtype=int)\n    if cfg.TRAIN.PRECLUDE_HARD_SAMPLES and gt_ishard is not None and gt_ishard.shape[0] > 0:\n        gt_ishard = gt_ishard.astype(int)\n        gt_hardboxes = gt_boxes[gt_ishard == 1, :]\n        if gt_hardboxes.shape[0] > 0:\n            # R x H\n            hard_overlaps = bbox_overlaps(\n                np.ascontiguousarray(all_rois[:, 1:5], dtype=np.float),\n                np.ascontiguousarray(gt_hardboxes[:, :4], dtype=np.float))\n            hard_max_overlaps = hard_overlaps.max(axis=1) # R x 1\n            # hard_gt_assignment = hard_overlaps.argmax(axis=0)  # H\n            ignore_inds = np.append(ignore_inds, \\\n                                    np.where(hard_max_overlaps >= cfg.TRAIN.FG_THRESH)[0])\n            # if DEBUG:\n            #     if ignore_inds.size > 1:\n            #         print \'num hard: {:d}:\'.format(ignore_inds.size)\n            #         print \'hard box:\', gt_hardboxes\n            #         print \'rois: \'\n            #         print all_rois[ignore_inds]\n\n    # preclude dontcare areas\n    if dontcare_areas is not None and dontcare_areas.shape[0] > 0:\n        # intersec shape is D x R\n        intersecs = bbox_intersections(\n            np.ascontiguousarray(dontcare_areas, dtype=np.float),  # D x 4\n            np.ascontiguousarray(all_rois[:, 1:5], dtype=np.float)  # R x 4\n        )\n        intersecs_sum = intersecs.sum(axis=0)  # R x 1\n        ignore_inds = np.append(ignore_inds, \\\n                                np.where(intersecs_sum > cfg.TRAIN.DONTCARE_AREA_INTERSECTION_HI)[0])\n        # if ignore_inds.size >= 1:\n        #     print \'num dontcare: {:d}:\'.format(ignore_inds.size)\n        #     print \'dontcare box:\', dontcare_areas.astype(int)\n        #     print \'rois: \'\n        #     print all_rois[ignore_inds].astype(int)\n\n    # Select foreground RoIs as those with >= FG_THRESH overlap\n    fg_inds = np.where(max_overlaps >= cfg.TRAIN.FG_THRESH)[0]\n    fg_inds = np.setdiff1d(fg_inds, ignore_inds)\n    # Guard against the case when an image has fewer than fg_rois_per_image\n    # foreground RoIs\n    fg_rois_per_this_image = min(fg_rois_per_image, fg_inds.size)\n    # Sample foreground regions without replacement\n    if fg_inds.size > 0:\n        fg_inds = npr.choice(fg_inds, size=fg_rois_per_this_image, replace=False)\n\n    # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n    bg_inds = np.where((max_overlaps < cfg.TRAIN.BG_THRESH_HI) &\n                       (max_overlaps >= cfg.TRAIN.BG_THRESH_LO))[0]\n    bg_inds = np.setdiff1d(bg_inds, ignore_inds)\n    # Compute number of background RoIs to take from this image (guarding\n    # against there being fewer than desired)\n    bg_rois_per_this_image = rois_per_image - fg_rois_per_this_image\n    bg_rois_per_this_image = min(bg_rois_per_this_image, bg_inds.size)\n    # Sample background regions without replacement\n    if bg_inds.size > 0:\n        bg_inds = npr.choice(bg_inds, size=bg_rois_per_this_image, replace=False)\n\n    # The indices that we\'re selecting (both fg and bg)\n    keep_inds = np.append(fg_inds, bg_inds)\n    # Select sampled values from various arrays:\n    labels = labels[keep_inds]\n    # Clamp labels for the background RoIs to 0\n    labels[fg_rois_per_this_image:] = 0\n    rois = all_rois[keep_inds]\n\n    bbox_target_data = _compute_targets(\n        rois[:, 1:5], gt_boxes[gt_assignment[keep_inds], :4], labels)\n\n    # bbox_target_data (1 x H x W x A, 5)\n    # bbox_targets <- (1 x H x W x A, K x 4)\n    # bbox_inside_weights <- (1 x H x W x A, K x 4)\n    bbox_targets, bbox_inside_weights = \\\n        _get_bbox_regression_labels(bbox_target_data, num_classes)\n\n    return labels, rois, bbox_targets, bbox_inside_weights\n\ndef _get_bbox_regression_labels(bbox_target_data, num_classes):\n    """"""Bounding-box regression targets (bbox_target_data) are stored in a\n    compact form N x (class, tx, ty, tw, th)\n\n    This function expands those targets into the 4-of-4*K representation used\n    by the network (i.e. only one class has non-zero targets).\n\n    Returns:\n        bbox_target (ndarray): N x 4K blob of regression targets\n        bbox_inside_weights (ndarray): N x 4K blob of loss weights\n    """"""\n\n    clss = bbox_target_data[:, 0]\n    bbox_targets = np.zeros((clss.size, 4 * num_classes), dtype=np.float32)\n    bbox_inside_weights = np.zeros(bbox_targets.shape, dtype=np.float32)\n    inds = np.where(clss > 0)[0]\n    for ind in inds:\n        cls = int(clss[ind])\n        start = 4 * cls\n        end = start + 4\n        bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]\n        bbox_inside_weights[ind, start:end] = cfg.TRAIN.BBOX_INSIDE_WEIGHTS\n    return bbox_targets, bbox_inside_weights\n\n\ndef _compute_targets(ex_rois, gt_rois, labels):\n    """"""Compute bounding-box regression targets for an image.""""""\n\n    assert ex_rois.shape[0] == gt_rois.shape[0]\n    assert ex_rois.shape[1] == 4\n    assert gt_rois.shape[1] == 4\n\n    targets = bbox_transform(ex_rois, gt_rois)\n    if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n        # Optionally normalize targets by a precomputed mean and stdev\n        targets = ((targets - np.array(cfg.TRAIN.BBOX_NORMALIZE_MEANS))\n                / np.array(cfg.TRAIN.BBOX_NORMALIZE_STDS))\n    return np.hstack(\n            (labels[:, np.newaxis], targets)).astype(np.float32, copy=False)\n\ndef _jitter_gt_boxes(gt_boxes, jitter=0.05):\n    """""" jitter the gtboxes, before adding them into rois, to be more robust for cls and rgs\n    gt_boxes: (G, 5) [x1 ,y1 ,x2, y2, class] int\n    """"""\n    jittered_boxes = gt_boxes.copy()\n    ws = jittered_boxes[:, 2] - jittered_boxes[:, 0] + 1.0\n    hs = jittered_boxes[:, 3] - jittered_boxes[:, 1] + 1.0\n    width_offset = (np.random.rand(jittered_boxes.shape[0]) - 0.5) * jitter * ws\n    height_offset = (np.random.rand(jittered_boxes.shape[0]) - 0.5) * jitter * hs\n    jittered_boxes[:, 0] += width_offset\n    jittered_boxes[:, 2] += width_offset\n    jittered_boxes[:, 1] += height_offset\n    jittered_boxes[:, 3] += height_offset\n\n    return jittered_boxes\n'"
lib/utils/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\nfrom . import cython_nms\nfrom . import cython_bbox\nimport boxes_grid\nimport blob\nimport nms\nimport timer'
lib/utils/blob.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Blob helper functions.""""""\n\nimport numpy as np\nimport cv2\nfrom ..fast_rcnn.config import cfg\n\ndef im_list_to_blob(ims):\n    """"""Convert a list of images into a network input.\n\n    Assumes images are already prepared (means subtracted, BGR order, ...).\n    """"""\n    max_shape = np.array([im.shape for im in ims]).max(axis=0)\n    num_images = len(ims)\n    blob = np.zeros((num_images, max_shape[0], max_shape[1], 3),\n                    dtype=np.float32)\n    for i in xrange(num_images):\n        im = ims[i]\n        blob[i, 0:im.shape[0], 0:im.shape[1], :] = im\n\n    return blob\n\ndef prep_im_for_blob(im, pixel_means, target_size, max_size):\n    """"""Mean subtract and scale an image for use in a blob.""""""\n    im = im.astype(np.float32, copy=False)\n    im -= pixel_means\n    im_shape = im.shape\n    im_size_min = np.min(im_shape[0:2])\n    im_size_max = np.max(im_shape[0:2])\n    im_scale = float(target_size) / float(im_size_min)\n    # Prevent the biggest axis from being more than MAX_SIZE\n    if np.round(im_scale * im_size_max) > max_size:\n        im_scale = float(max_size) / float(im_size_max)\n    if cfg.TRAIN.RANDOM_DOWNSAMPLE:\n        r = 0.6 + np.random.rand() * 0.4\n        im_scale *= r\n    im = cv2.resize(im, None, None, fx=im_scale, fy=im_scale,\n                    interpolation=cv2.INTER_LINEAR)\n\n    return im, im_scale\n'"
lib/utils/boxes_grid.py,0,"b'# --------------------------------------------------------\n# Subcategory CNN\n# Copyright (c) 2015 CVGL Stanford\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Yu Xiang\n# --------------------------------------------------------\n\nimport numpy as np\nimport math\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\n\ndef get_boxes_grid(image_height, image_width):\n    """"""\n    Return the boxes on image grid.\n    calling this function when cfg.IS_MULTISCALE is True, otherwise, calling rdl_roidb.prepare_roidb(imdb) instead.\n    """"""\n\n    # fixed a bug, change cfg.TRAIN.SCALES to cfg.TRAIN.SCALES_BASE\n    # coz, here needs a ratio around 1.0, not the accutual size.\n    # height and width of the feature map\n    if cfg.NET_NAME == \'CaffeNet\':\n        height = np.floor((image_height * max(cfg.TRAIN.SCALES_BASE) - 1) / 4.0 + 1)\n        height = np.floor((height - 1) / 2.0 + 1 + 0.5)\n        height = np.floor((height - 1) / 2.0 + 1 + 0.5)\n\n        width = np.floor((image_width * max(cfg.TRAIN.SCALES_BASE) - 1) / 4.0 + 1)\n        width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n        width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n    elif cfg.NET_NAME == \'VGGnet\':\n        height = np.floor(image_height * max(cfg.TRAIN.SCALES_BASE) / 2.0 + 0.5)\n        height = np.floor(height / 2.0 + 0.5)\n        height = np.floor(height / 2.0 + 0.5)\n        height = np.floor(height / 2.0 + 0.5)\n\n        width = np.floor(image_width * max(cfg.TRAIN.SCALES_BASE) / 2.0 + 0.5)\n        width = np.floor(width / 2.0 + 0.5)\n        width = np.floor(width / 2.0 + 0.5)\n        width = np.floor(width / 2.0 + 0.5)\n    else:\n        assert (1), \'The network architecture is not supported in utils.get_boxes_grid!\'\n\n    # compute the grid box centers\n    h = np.arange(height)\n    w = np.arange(width)\n    y, x = np.meshgrid(h, w, indexing=\'ij\') \n    centers = np.dstack((x, y))\n    centers = np.reshape(centers, (-1, 2))\n    num = centers.shape[0]\n\n    # compute width and height of grid box\n    area = cfg.TRAIN.KERNEL_SIZE * cfg.TRAIN.KERNEL_SIZE\n    aspect = cfg.TRAIN.ASPECTS  # height / width\n    num_aspect = len(aspect)\n    widths = np.zeros((1, num_aspect), dtype=np.float32)\n    heights = np.zeros((1, num_aspect), dtype=np.float32)\n    for i in xrange(num_aspect):\n        widths[0,i] = math.sqrt(area / aspect[i])\n        heights[0,i] = widths[0,i] * aspect[i]\n\n    # construct grid boxes\n    centers = np.repeat(centers, num_aspect, axis=0)\n    widths = np.tile(widths, num).transpose()\n    heights = np.tile(heights, num).transpose()\n\n    x1 = np.reshape(centers[:,0], (-1, 1)) - widths * 0.5\n    x2 = np.reshape(centers[:,0], (-1, 1)) + widths * 0.5\n    y1 = np.reshape(centers[:,1], (-1, 1)) - heights * 0.5\n    y2 = np.reshape(centers[:,1], (-1, 1)) + heights * 0.5\n    \n    boxes_grid = np.hstack((x1, y1, x2, y2)) / cfg.TRAIN.SPATIAL_SCALE\n\n    return boxes_grid, centers[:,0], centers[:,1]\n'"
lib/utils/nms.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\n\ndef nms(dets, thresh):\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n'"
lib/utils/timer.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport time\n\nclass Timer(object):\n    """"""A simple timer.""""""\n    def __init__(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n\n    def tic(self):\n        # using time.time instead of time.clock because time time.clock\n        # does not normalize for multithreading\n        self.start_time = time.time()\n\n    def toc(self, average=True):\n        self.diff = time.time() - self.start_time\n        self.total_time += self.diff\n        self.calls += 1\n        self.average_time = self.total_time / self.calls\n        if average:\n            return self.average_time\n        else:\n            return self.diff\n'"
