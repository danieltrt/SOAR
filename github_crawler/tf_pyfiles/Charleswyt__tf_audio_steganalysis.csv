file_path,api_count,code
src/audio_preprocess.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport librosa\nimport numpy as np\n\n""""""\nCreated on 2018.03.05\nFinished on 2018.03.10\nModified on 2018.09.12\n\n@author: Yuntao Wang\n""""""\n\n\ndef audio_read(audio_file_path, sampling_rate=44100, channel=""left"", offset=0, duration=None):\n    """"""\n    read audio data\n    :param audio_file_path: audio file path\n    :param sampling_rate: sampling rate\n    :param channel: ""left"", ""right"", ""both""\n    :param offset: start reading after this time (in seconds), default is 0\n    :param duration: only load up to this much audio (in seconds), default is None\n    :return:\n        2-D Variable, audio data in np.float32 format\n    """"""\n\n    audio_tuple = librosa.load(audio_file_path, sr=sampling_rate, mono=False, offset=offset, duration=duration)\n    if channel == ""both"":\n        audio = audio_tuple[0]\n    elif channel == ""left"":\n        audio = audio_tuple[0][0]\n    elif channel == ""right"":\n        audio = audio_tuple[0][1]\n    else:\n        audio = None\n\n    return audio\n\n\ndef audio_read_batch(audio_files_list, sampling_rate=44100, channel=""both"", offset=0, duration=None):\n    """"""\n    read audio data in batch\n    :param audio_files_list: audio files list\n    :param sampling_rate: sampling rate\n    :param channel: ""left"", ""right"", ""both""\n    :param offset: start reading after this time (in seconds), default is 0\n    :param duration: only load up to this much audio (in seconds), default is None\n    :return:\n        data, a 3-D ndarray, [batch_size, height, width]\n    """"""\n    files_num = len(audio_files_list)\n\n    audio = audio_read(audio_files_list[0], sampling_rate=sampling_rate, channel=channel, offset=offset, duration=duration)\n    channels, samples = np.shape(audio)[0], np.shape(audio)[1]\n\n    data = np.zeros([files_num, channels, samples], dtype=np.float32)\n\n    i = 0\n    for audio_file in audio_files_list:\n        content = audio_read(audio_file, sampling_rate=sampling_rate, channel=channel, offset=offset, duration=duration)\n        data[i] = content\n        i = i + 1\n\n    return data\n\n\ndef get_mfcc_statistics(audio_data, sampling_rate=44100, n_mfcc=40):\n    """"""\n    calculate the statistics of mfcc coefficients\n    :param audio_data: audio data, ndarray [length, channel]\n    :param sampling_rate: sampling rate of audio data, default is 44100\n    :param n_mfcc: number of mfcc, default is 40\n    :return:\n    """"""\n    mfcc_coefficients = librosa.feature.mfcc(y=audio_data, sr=sampling_rate, n_mfcc=n_mfcc)\n\n    mfcc_feature = []\n\n    mfcc_max = np.max(mfcc_coefficients.T, axis=0)\n    mfcc_feature.append(mfcc_max)\n\n    mfcc_min = np.min(mfcc_coefficients.T, axis=0)\n    mfcc_feature.append(mfcc_min)\n\n    mfcc_mean = np.mean(mfcc_coefficients.T, axis=0)\n    mfcc_feature.append(mfcc_mean)\n\n    mfcc_var = np.var(mfcc_coefficients.T, axis=0)\n    mfcc_feature.append(mfcc_var)\n\n    mfcc_var_inverse = np.divide(1.0, mfcc_var)\n    mfcc_kurtosis = np.multiply(np.power(mfcc_mean, 4), np.power(mfcc_var_inverse, 4))\n    mfcc_feature.append(mfcc_kurtosis)\n\n    mfcc_skewness = np.multiply(np.power(mfcc_mean, 3), np.power(mfcc_var_inverse, 3))\n    mfcc_feature.append(mfcc_skewness)\n\n    mfcc_feature = np.array(mfcc_feature)\n\n    return mfcc_feature\n\n\ndef get_mfcc(audio_file_path, sampling_rate=44100, offset=0, duration=1.3, n_mfcc=40):\n    """"""\n    extract mel frequency ceptral coefficients of audio\n    :param audio_file_path: audio file path\n    :param sampling_rate: sampling rate\n    :param offset: start reading after this time (in seconds), default is 0\n    :param duration: only load up to this much audio (in seconds), default is 1.3\n    :param n_mfcc: number of mfcc, default is 24\n    :return:\n        MFCC vector: [frames, n_mfcc * channel_number]\n    """"""\n\n    audio_data = librosa.load(audio_file_path, sr=sampling_rate, mono=False, offset=offset, duration=duration)\n    mfcc_left = librosa.feature.mfcc(y=audio_data[0][0], sr=sampling_rate, n_mfcc=n_mfcc)\n    mfcc_right = librosa.feature.mfcc(y=audio_data[0][1], sr=sampling_rate, n_mfcc=n_mfcc)\n\n    mfcc_dif1_left = librosa.feature.mfcc(y=np.diff(audio_data[0][0], 1, 0), sr=sampling_rate, n_mfcc=n_mfcc)\n    mfcc_dif2_left = librosa.feature.mfcc(y=np.diff(audio_data[0][0], 2, 0), sr=sampling_rate, n_mfcc=n_mfcc)\n\n    mfcc_dif1_right = librosa.feature.mfcc(y=np.diff(audio_data[0][1], 1, 0), sr=sampling_rate, n_mfcc=n_mfcc)\n    mfcc_dif2_right = librosa.feature.mfcc(y=np.diff(audio_data[0][1], 2, 0), sr=sampling_rate, n_mfcc=n_mfcc)\n\n    frames = np.shape(mfcc_left)[1]\n    mfcc = np.zeros([frames, n_mfcc * 2, 3])\n    mfcc[:, :n_mfcc, 0], mfcc[:, n_mfcc:, 0] = np.transpose(mfcc_left), np.transpose(mfcc_right)\n    mfcc[:, :n_mfcc, 0], mfcc[:, n_mfcc:, 1] = np.transpose(mfcc_dif1_left), np.transpose(mfcc_dif1_right)\n    mfcc[:, :n_mfcc, 0], mfcc[:, n_mfcc:, 2] = np.transpose(mfcc_dif2_left), np.transpose(mfcc_dif2_right)\n\n    return mfcc\n\n\ndef get_mfcc_batch(audio_files_list, sampling_rate=44100, offset=0, duration=1.3, n_mfcc=40):\n    """"""\n    get mfcc vector of audio in batch\n    :param audio_files_list: audio files list\n    :param sampling_rate: sampling rate\n    :param offset: start reading after this time (in seconds), default is 0\n    :param duration: only load up to this much audio (in seconds), default is None\n    :param n_mfcc: number of mfcc, default: 24\n    :return:\n        data, a 3-D ndarray, [batch_size, height, width]\n    """"""\n    files_num = len(audio_files_list)\n\n    mfcc = get_mfcc(audio_files_list[0], sampling_rate=sampling_rate, offset=offset, duration=duration, n_mfcc=n_mfcc)\n    height, width, channel = np.shape(mfcc)[0], np.shape(mfcc)[1], np.shape(mfcc)[2]\n\n    data = np.zeros([files_num, height, width, channel], dtype=np.float32)\n\n    i = 0\n    for audio_file in audio_files_list:\n        content = get_mfcc(audio_file, sampling_rate=sampling_rate, offset=offset, duration=duration, n_mfcc=n_mfcc)\n        data[i] = content\n        i = i + 1\n\n    return data\n'"
src/config.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n""""""\nCreated on 2018.01.05\nFinished on 2018.01.05\nModified on 2018.08.23\n\n@author: Yuntao Wang\n""""""\n\nimport os\nimport json\nimport argparse\nfrom utils import *\n\n""""""\nfunction:\n        command_parse()                                     parse command line parameters\n        config_train_file_read                              read config json file for training\n        config_test_file_read                               read config json file for test\n        config_steganalysis_file_read                       read config json file for steganalysis\n""""""\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = ""2""\n\n\ndef command_parse():\n    """"""\n    :param: NULL\n    :return:\n        argument\n    """"""\n    parser = argparse.ArgumentParser(description=""Audio/Image steganalysis with CNN based on tensorflow."")\n    print(parser.description)\n\n    # mode\n    parser.add_argument(""--path_mode"", type=str, default=""full"", help=""mode of file path"")\n    parser.add_argument(""--task_name"", type=str, help=""the name of the current task"")\n    parser.add_argument(""--gpu_selection"", type=str, default=""auto"",\n                        help=""GPU selection mode, if \\""auto\\"", no serial number is needed, otherwise appoint the serial number ""\n                             ""(default: auto, another choice is manu)"")\n    parser.add_argument(""--gpu"", type=str, default=""0"", help=""the index of GPU"")\n    parser.add_argument(""--mode"", type=str, default=""train"", help=""run mode -- train | test (default: train)"")\n    parser.add_argument(""--siamese"", type=bool, default=False, help=""whether to use siamese mode (default: False)"")\n    parser.add_argument(""--checkpoint"", type=bool, default=True, help=""whether there is checkpoint or not (default: True)"")\n    parser.add_argument(""--submode"", type=str, default=""one"", help=""one | batch (default one)"")\n    parser.add_argument(""--carrier"", type=str, default=""qmdct"", help=""qmdct | image | audio (default qmdct)"")\n    parser.add_argument(""--network"", type=str, default=""network1"", help=""the index of the network (default: wasdn), ""\n                                                                        ""the detailed introduction of each network is in readme"")\n\n    # data info\n    parser.add_argument(""--height"", type=int, default=200, help=""the height of the input data matrix (default: 200)"")\n    parser.add_argument(""--width"", type=int, default=576, help=""the width of the input data matrix (default: 576)"")\n    parser.add_argument(""--channel"", type=int, default=1, help=""the channel of the input data matrix (default: 1)"")\n\n    # index\n    parser.add_argument(""--start_index_train"", type=int, default=None, help=""the start index of file in train folders (default: None)"")\n    parser.add_argument(""--end_index_train"", type=int, default=None, help=""the end index of file in train folders (default: None)"")\n    parser.add_argument(""--start_index_valid"", type=int, default=None, help=""the start index of file in valid folders (default: None)"")\n    parser.add_argument(""--end_index_valid"", type=int, default=None, help=""the end index of file in valid folders (default: None)"")\n\n    # path of steganalysis file(s)\n    parser.add_argument(""--cover_files_root"", type=str, help=""the directory of root containing cover files"")\n    parser.add_argument(""--stego_files_root"", type=str, help=""the directory of root containing stego files"")\n\n    parser.add_argument(""--cover_files_path"", type=str, default=None, help=""the directory of root containing cover files"")\n    parser.add_argument(""--stego_files_path"", type=str, default=None, help=""the directory of root containing stego files"")\n\n    parser.add_argument(""--steganalysis_file_path"", type=str, help=""the file path used for steganalysis"")\n    parser.add_argument(""--steganalysis_files_path"", type=str, help=""the files folder path used for steganalysis"")\n    parser.add_argument(""--cover_train_path"", type=str, help=""the path of directory containing cover files for train"")\n    parser.add_argument(""--cover_valid_path"", type=str, help=""the path of directory containing cover files for validation"")\n    parser.add_argument(""--cover_test_path"", type=str, help=""the path of directory containing cover files for test"")\n    parser.add_argument(""--stego_train_path"", type=str, help=""the path of directory containing stego files for train"")\n    parser.add_argument(""--stego_valid_path"", type=str, help=""the path of directory containing stego files for validation"")\n    parser.add_argument(""--stego_test_path"", type=str, help=""the path of directory containing stego files for test"")\n    parser.add_argument(""--tfrecords_path"", type=str, help=""the path of directory containing all tfrecord files"")\n    parser.add_argument(""--models_path"", type=str, help=""the path of directory containing models"")\n    parser.add_argument(""--logs_path"", type=str, help=""the path of directory containing logs"")\n\n    # hyper parameters\n    parser.add_argument(""--batch_size"", type=int, default=128, help=""batch size (default: 128 (64 cover|stego pairs))"")\n    parser.add_argument(""--learning_rate"", type=float, default=1e-3, help=""the value of initialized learning rate (default: 1e-3 (0.001))"")\n    parser.add_argument(""--seed"", type=int, default=1, help=""random seed (default: 1)"")\n    parser.add_argument(""--is_regulation"", type=bool, default=True, help=""whether regulation or not (default: True)"")\n    parser.add_argument(""--coeff_regulation"", type=float, default=1e-3, help=""the gain of regulation (default: 1e-3)"")\n    parser.add_argument(""--loss_method"", type=bool, default=""sparse_softmax_cross_entropy"", help=""the method of loss calculation (default: sparse_softmax_cross_entropy)"")\n    parser.add_argument(""--class_num"", type=int, default=2, help=""the class number (default: 2)"")\n    parser.add_argument(""--epoch"", type=int, default=500, help=""the number of epochs for network training stop (default: 500)"")\n\n    # learning rate parameters\n    parser.add_argument(""--decay_method"", type=str, default=""exponential"", help=""the method for learning rate decay (default: exponential)"")\n    parser.add_argument(""--decay_step"", type=int, default=5000, help=""the step for learning rate decay (default: 5000)"")\n    parser.add_argument(""--decay_rate"", type=float, default=0.9, help=""the rate for learning rate decay (default: 0.95)"")\n    parser.add_argument(""--staircase"", type=bool, default=False,\n                        help=""whether the decay the learning rate at discrete intervals or not (default:False)"")\n\n    # model\n    parser.add_argument(""--max_to_keep"", type=int, default=3, help=""the number of models needed to be saved (default: 3)"")\n    parser.add_argument(""--keep_checkpoint_every_n_hours"", type=float, default=0.5, help=""how often to keep checkpoints (default: 0.5)"")\n\n    arguments = parser.parse_args()\n\n    # full_samples_path\n    if arguments.path_mode == ""full"" or arguments.path_mode == ""semi"":\n        pass\n\n    # simple_samples_path\n    elif self.path_mode == ""simple"":\n        stego_method = arguments.task_name.split(""_"")[0]\n        samples_bitrate = arguments.task_name.split(""_"")[2]\n\n        arguments.cover_train_path = fullfile(fullfile(arguments.cover_files_root, samples_bitrate), ""train"")\n        arguments.cover_valid_path = fullfile(fullfile(arguments.cover_files_root, samples_bitrate), ""validation"")\n        arguments.stego_train_path = fullfile(fullfile(fullfile(arguments.stego_files_root, stego_method), arguments.task_name), ""train"")\n        arguments.stego_valid_path = fullfile(fullfile(fullfile(arguments.stego_files_root, stego_method), arguments.task_name), ""validation"")\n\n        arguments.cover_files_path, arguments.stego_files_path = None, None\n\n    else:\n        arguments.train = False\n\n    # create folder (tfrecord)\n    # level: root\n    tfrecords_path = arguments.tfrecords_path\n    folder_make(tfrecords_path)\n\n    # level: task\n    tfrecords_path_task = fullfile(tfrecords_path, arguments.task_name)\n    folder_make(tfrecords_path_task)\n\n    # create folder (models and logs)\n    # level: root\n    models_path_root, logs_path_root = arguments.models_path, arguments.logs_path\n    folder_make(models_path_root), folder_make(logs_path_root)\n\n    # level: network\n    models_path_network, logs_path_network = fullfile(models_path_root, arguments.network), fullfile(logs_path_root, arguments.network)\n    folder_make(models_path_network), folder_make(logs_path_network)\n\n    # level: task\n    models_path_task, logs_path_task = fullfile(models_path_network, arguments.task_name), fullfile(logs_path_network, arguments.task_name)\n    folder_make(models_path_task), folder_make(logs_path_task)\n\n    # process for checkpoint\n    sub_directory = get_sub_directory(models_path_task)\n    if arguments.checkpoint is True and len(sub_directory) > 0:\n        models_path_current = sub_directory[-1]\n        logs_path_current = models_path_current.replace(""models"", ""logs"")\n    else:\n        # level: training start time\n        current_time_stamp = str(get_unix_stamp(get_time()))\n        models_path_current, logs_path_current = fullfile(models_path_task, current_time_stamp), fullfile(logs_path_task, current_time_stamp)\n        folder_make(models_path_current), folder_make(logs_path_current)\n\n    arguments.model_path = models_path_current\n    arguments.log_path = logs_path_current\n\n    return arguments\n\n\ndef config_train_file_read(config_file_path):\n    with open(config_file_path, encoding=\'utf-8\') as json_file:\n        file_content = json.load(json_file)\n\n        class Variable:\n            def __init__(self):\n                self.path_mode = file_content[""path_mode""]\n                self.task_name = file_content[""task_name""]\n\n                # full_samples_path\n                if self.path_mode == ""full"":\n                    self.cover_train_path = file_content[\'full_samples_path\'][\'cover_train_path\']\n                    self.cover_valid_path = file_content[\'full_samples_path\'][\'cover_valid_path\']\n                    self.stego_train_path = file_content[\'full_samples_path\'][\'stego_train_path\']\n                    self.stego_valid_path = file_content[\'full_samples_path\'][\'stego_valid_path\']\n\n                # semi_sample_path\n                elif self.path_mode == ""semi"":\n                    self.cover_files_path = file_content[\'semi_samples_path\'][\'cover_files_path\']\n                    self.stego_files_path = file_content[\'semi_samples_path\'][\'stego_files_path\']\n\n                    self.cover_train_path, self.cover_valid_path, self.stego_train_path, self.stego_valid_path = None, None, None, None\n\n                # simple_samples_path\n                elif self.path_mode == ""simple"":\n                    self.cover_files_root = file_content[\'simple_samples_path\'][\'cover_files_root\']\n                    self.stego_files_root = file_content[\'simple_samples_path\'][\'stego_files_root\']\n                    stego_method = self.task_name.split(""_"")[0]\n                    samples_bitrate = self.task_name.split(""_"")[2]\n\n                    self.cover_train_path = fullfile(fullfile(self.cover_files_root, samples_bitrate), ""train"")\n                    self.cover_valid_path = fullfile(fullfile(self.cover_files_root, samples_bitrate), ""validation"")\n                    self.stego_train_path = fullfile(fullfile(fullfile(self.stego_files_root, stego_method), self.task_name), ""train"")\n                    self.stego_valid_path = fullfile(fullfile(fullfile(self.stego_files_root, stego_method), self.task_name), ""validation"")\n                    self.cover_files_path, self.stego_files_path = None, None\n\n                else:\n                    self.train = False\n\n                # files_path\n                self.tfrecords_path = file_content[\'files_path\'][\'tfrecords_path\']\n                self.models_path = file_content[\'files_path\'][\'models_path\']\n                self.logs_path = file_content[\'files_path\'][\'logs_path\']\n\n                # mode_config\n                self.gpu_selection = file_content[\'mode_config\'][\'gpu_selection\']\n                self.gpu = file_content[\'mode_config\'][\'gpu\']\n                self.mode = file_content[\'mode_config\'][\'mode\']\n                self.carrier = file_content[\'mode_config\'][\'carrier\']\n                self.network = file_content[\'mode_config\'][\'network\']\n                self.siamese = file_content[\'mode_config\'][\'siamese\']\n                self.checkpoint = file_content[\'mode_config\'][\'checkpoint\']\n                self.fine_tune_model_file_path = file_content[\'mode_config\'][\'fine_tune_model_file_path\']\n\n                if self.carrier == ""mfcc"" or self.carrier == ""audio"":\n                    self.file_type = ""mp3""\n                elif self.carrier == ""qmdct"":\n                    self.file_type = ""txt""\n                else:\n                    self.file_type = self.carrier\n\n                # hyper_parameters\n                self.batch_size = file_content[\'hyper_parameters\'][\'batch_size\']\n                self.learning_rate = file_content[\'hyper_parameters\'][\'learning_rate\']\n                self.seed = file_content[\'hyper_parameters\'][\'seed\']\n                self.epoch = file_content[\'hyper_parameters\'][\'epoch\']\n                self.is_regulation = file_content[\'hyper_parameters\'][\'is_regulation\']\n                self.coeff_regulation = file_content[\'hyper_parameters\'][\'coeff_regulation\']\n                self.loss_method = file_content[\'hyper_parameters\'][\'loss_method\']\n                self.class_num = file_content[\'hyper_parameters\'][\'class_num\']\n\n                # data info\n                self.height = file_content[\'shape\'][\'height\']\n                self.width = file_content[\'shape\'][\'width\']\n                self.channel = file_content[\'shape\'][\'channel\']\n\n                # learning_rate_method\n                self.decay_method = file_content[\'learning_rate_method\'][\'decay_method\']\n                self.decay_step = file_content[\'learning_rate_method\'][\'decay_step\']\n                self.decay_rate = file_content[\'learning_rate_method\'][\'decay_rate\']\n                self.staircase = file_content[\'learning_rate_method\'][\'staircase\']\n\n                # model\n                self.max_to_keep = file_content[\'model\'][\'max_to_keep\']\n                self.keep_checkpoint_every_n_hours = file_content[\'model\'][\'keep_checkpoint_every_n_hours\']\n\n                # index\n                self.start_index_train = file_content[""index""][""start_index_train""]\n                self.end_index_train = file_content[""index""][""end_index_train""]\n                self.start_index_valid = file_content[""index""][""start_index_valid""]\n                self.end_index_valid = file_content[""index""][""end_index_valid""]\n\n                # stop criterion\n                self.indicator = file_content[""stop_criterion""][""indicator""]\n                self.epoch_interval = file_content[""stop_criterion""][""epoch_interval""]\n                self.validation_accuracy_expectation = file_content[""stop_criterion""][""validation_accuracy_expectation""]\n\n                self.train = True\n\n        arguments = Variable()\n\n        # create folder (tfrecord)\n        # level: root\n        tfrecords_path = arguments.tfrecords_path\n        folder_make(tfrecords_path)\n\n        # level: task\n        tfrecords_path_task = fullfile(tfrecords_path, arguments.task_name)\n        folder_make(tfrecords_path_task)\n        arguments.tfrecord_path = tfrecords_path_task\n\n        # create folder (models and logs)\n        # level: root\n        models_path_root, logs_path_root = arguments.models_path, arguments.logs_path\n        folder_make(models_path_root), folder_make(logs_path_root)\n\n        # level: network\n        models_path_network, logs_path_network = fullfile(models_path_root, arguments.network), fullfile(logs_path_root, arguments.network)\n        folder_make(models_path_network), folder_make(logs_path_network)\n\n        # level: task\n        models_path_task, logs_path_task = fullfile(models_path_network, arguments.task_name), fullfile(logs_path_network, arguments.task_name)\n        folder_make(models_path_task), folder_make(logs_path_task)\n\n        # process for checkpoint\n        sub_directory = get_sub_directory(models_path_task)\n        if arguments.checkpoint is True and len(sub_directory) > 0:\n            models_path_current = sub_directory[-1]\n            logs_path_current = models_path_current.replace(""models"", ""logs"")\n        else:\n            # level: training start time\n            current_time_stamp = str(get_unix_stamp(get_time()))\n            models_path_current, logs_path_current = fullfile(models_path_task, current_time_stamp), fullfile(logs_path_task, current_time_stamp)\n            folder_make(models_path_current), folder_make(logs_path_current)\n\n        arguments.model_path = models_path_current\n        arguments.log_path = logs_path_current\n\n        return arguments\n\n\ndef config_test_file_read(config_file_path):\n    with open(config_file_path, encoding=\'utf-8\') as json_file:\n        file_content = json.load(json_file)\n\n        class Variable:\n            def __init__(self):\n                # files_path\n                self.cover_test_path = file_content[\'files_path\'][\'cover_test_path\']\n                self.stego_test_path = file_content[\'files_path\'][\'stego_test_path\']\n                self.models_path = file_content[\'files_path\'][\'models_path\']\n\n                # mode_config\n                self.gpu_selection = file_content[\'mode_config\'][\'gpu_selection\']\n                self.gpu = file_content[\'mode_config\'][\'gpu\']\n                self.mode = file_content[\'mode_config\'][\'mode\']\n                self.carrier = file_content[\'mode_config\'][\'carrier\']\n                self.network = file_content[\'mode_config\'][\'network\']\n\n                # hyper_parameters\n                self.batch_size = file_content[\'hyper_parameters\'][\'batch_size\']\n                self.class_num = file_content[\'hyper_parameters\'][\'class_num\']\n\n                # shape of input data\n                self.height = file_content[\'shape\'][\'height\']\n                self.width = file_content[\'shape\'][\'width\']\n                self.channel = file_content[\'shape\'][\'channel\']\n\n                # index\n                self.start_index_test = file_content[""index""][""start_index_test""]\n                self.end_index_test = file_content[""index""][""end_index_test""]\n                self.is_shuffle = file_content[""index""][""is_shuffle""]\n\n        argument = Variable()\n\n        return argument\n\n\ndef config_steganalysis_file_read(config_file_path):\n    with open(config_file_path, encoding=\'utf-8\') as json_file:\n        file_content = json.load(json_file)\n\n        class Variable:\n            def __init__(self):\n                # files_path\n                self.steganalysis_file_path = file_content[\'files_path\'][\'steganalysis_file_path\']\n                self.steganalysis_files_path = file_content[\'files_path\'][\'steganalysis_files_path\']\n                self.models_path = file_content[\'files_path\'][\'models_path\']\n\n                # mode_config\n                self.gpu_selection = file_content[\'mode_config\'][\'gpu_selection\']\n                self.gpu = file_content[\'mode_config\'][\'gpu\']\n                self.mode = file_content[\'mode_config\'][\'mode\']\n                self.submode = file_content[\'mode_config\'][\'submode\']\n                self.carrier = file_content[\'mode_config\'][\'carrier\']\n                self.network = file_content[\'mode_config\'][\'network\']\n\n                # hyper_parameters\n                self.class_num = file_content[\'hyper_parameters\'][\'class_num\']\n\n                # files index\n                self.start_index_steganalysis = file_content[\'index\'][\'start_index_steganalysis\']\n                self.end_index_steganalysis = file_content[\'index\'][\'end_index_steganalysis\']\n\n                # data info\n                self.height = file_content[\'shape\'][\'height\']\n                self.width = file_content[\'shape\'][\'width\']\n                self.channel = file_content[\'shape\'][\'channel\']\n\n        argument = Variable()\n\n        return argument\n'"
src/dataset.py,13,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2018.06.26\nFinished on 2018.06.27\nModified on 2018.09.12\n\n@author: Yuntao Wang\n""""""\n\nimport tensorflow as tf\nfrom text_preprocess import *\nfrom image_preprocess import *\nfrom audio_preprocess import *\n\n\ndef tfrecord_write(files_path_list, tfrecord_file_name):\n    """"""\n    read files from disk and write the data into tfrecord\n    :param files_path_list: data\n    :param tfrecord_file_name:\n    :return:\n    """"""\n    writer = tf.python_io.TFRecordWriter(tfrecord_file_name)\n    for index, files_path in enumerate(files_path_list):\n        files_list = get_files_list(files_path)\n        for file in files_list:\n            """"""\n            media read\n            \n            data = ""1""\n            data_raw = data.tobytes()\n            """"""\n            data = ""1""\n            data_raw = data.tobytes()\n            example = tf.train.Example(features=tf.train.Features(\n                feature={\n                    ""label"": tf.train.Feature(int64_list=tf.train.Int64List(value=[index])),\n                    ""data"": tf.train.Feature(bytes_list=tf.train.BytesList(value=[data_raw]))\n                }))\n            writer.write(example.SerializeToString())\n    writer.close()\n\n\ndef tfrecord_read(tfrecord_file_name):\n    filename_queue = tf.train.string_input_producer([tfrecord_file_name])\n    reader = tf.TFRecordReader()\n    _, serialized_example = reader.read(filename_queue)\n    features = tf.parse_single_example(serialized_example,\n                                       features={\n                                           ""label"": tf.FixedLenFeature([], tf.int64),\n                                           ""media"": tf.FixedLenFeature([], tf.string),\n                                       })\n    img = tf.decode_raw(features[\'media\'], tf.uint8)\n    img = tf.reshape(img, [128, 128, 3])  # reshape\xe4\xb8\xba128*128\xe7\x9a\x843\xe9\x80\x9a\xe9\x81\x93\xe5\x9b\xbe\xe7\x89\x87\n    img = tf.cast(img, tf.float32) * (1. / 255) - 0.5  # \xe5\x9c\xa8\xe6\xb5\x81\xe4\xb8\xad\xe6\x8a\x9b\xe5\x87\xbaimg\xe5\xbc\xa0\xe9\x87\x8f\n    label = tf.cast(features[""label""], tf.int32)  # \xe5\x9c\xa8\xe6\xb5\x81\xe4\xb8\xad\xe6\x8a\x9b\xe5\x87\xbalabel\xe5\xbc\xa0\xe9\x87\x8f\n\n    return img, label\n'"
src/distributed.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2018.07.03\nFinished on  2018.07.03\nModified on\n\n@author: Yuntao Wang\n""""""\n\n""""""\nThis script is used for distributed calculation\n\nMode:\n    1. single machine and single GPU (SMSG)\n    2. single machine and multiple GPU (SMMG)\n    3. multiple machine and multiple GPU (MMMG)\n\nIllustration:\n        The main consideration is the distribution of parameters servers and computing servers, and another consideration is the mode of update, \n    which can be divided into synchronous update and asynchronous update\n    \n    parameter server: the server which is used to collect all parameters from each computing servers and get an average value\n    computing server: the server which is used to calculate \n\n    SMSG: Just adjusting batch size is OK, and parameter server and computing server is unified in the current using machine\n    SMMG: \n""""""\n\n# Building\n\n\n\n'"
src/file_preprocess.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport os\nimport numpy as np\nimport filetype as ft\n\n""""""\nCreated on 2018.01.15\nFinished on 2018.01.15\nModified on 2018.08.29\n\n@author: Yuntao Wang\n""""""\n\n""""""\nfunction:\n    get_file_name(file_path)                                                        get the name of the file\n    get_file_size(file_path, unit=""KB"")                                             get the size of the file\n    get_file_type(file_path)                                                        get the type of the file\n    get_path_type(path)                                                             get the type of the input path (file, folder or not exist)\n""""""\n\n\ndef get_file_name(file_path):\n    """"""\n    get the name of the file\n    :param file_path: the path of the file\n    :return:\n        the name of the file\n    """"""\n    if os.path.exists(file_path):\n        file_path_new = file_path.replace(""\\\\"", ""/"")\n        file_name = file_path_new.split(sep=""/"")[-1]\n\n    else:\n        file_name = None\n\n    return file_name\n\n\ndef get_file_size(file_path, unit=""KB""):\n    """"""\n    get the size of file\n    :param file_path: file path\n    :param unit: the unit of file size (Unit: B KB MB GB TB)\n    :return:\n        file_size\n    """"""\n    units = [""B"", ""KB"", ""MB"", ""GB"", ""TB""]\n    power = units.index(unit)\n    divisor = 1024 ** power\n    if os.path.exists(file_path):\n        file_size = os.path.getsize(file_path)\n        file_size = round(file_size / divisor)\n    else:\n        file_size = None\n\n    return file_size\n\n\ndef get_file_type(file_path):\n    """"""\n    get the type of file according to the suffix of the file name\n    :param file_path: file path\n    :return:\n        file type\n    """"""\n    if os.path.exists(file_path):\n        file_type = ft.guess(file_path).extension\n    else:\n        file_type = None\n\n    return file_type\n\n\ndef get_path_type(path):\n    """"""\n    get type of input path\n    :param path: input path\n    :return:\n        path_type:\n            ""file"": the input path corresponds to a file\n            ""folder"": the input path corresponds to a folder\n            None: the input path does not exist\n    """"""\n    if os.path.exists(path):\n        if os.path.isfile(path):\n            return ""file""\n        else:\n            return ""folder""\n    else:\n        return None\n'"
src/image_preprocess.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2018.04.20\nFinished on 2018.04.20\nModified on 2018.08.27\n\n@author: Yuntao Wang\n""""""\n\nimport numpy as np\nfrom skimage import io, util\n\n""""""\nfunction:\n    image_info_show(image_file_path)                                                                        show the information of image\n    read_image(image_file_path, height=None, width=None)                                                    read the image\n    read_image_batch(image_files_list, height=None, width=None)                                             read images in batch\n""""""\n\n\ndef image_info_show(image_file_path):\n    """"""\n    show the information of the image\n    :param image_file_path: path of image\n    :return:\n        NULL\n    """"""\n    img = io.imread(image_file_path)\n    print(""The type of the image is %s"" % type(img))\n    print(""The height of the image is %d"" % img.shape[1])\n    print(""The width of the image is %d"" % img.shape[0])\n    try:\n        print(""The channel of the image is %d"" % img.shape[2])\n    except IndexError:\n        print(""The channel of the image is 1"")\n    print(""The size of the image is %d"" % img.size)\n    print(""The min value of the pixel in the image is %d"" % img.min())\n    print(""The max value of the pixel in the image is %d"" % img.max())\n    print(""The mean value of the pixel in the image is %d"" % img.mean())\n    plt.imshow(img)\n    plt.show()\n\n\ndef image_read(image_file_path, height=None, width=None, channel=None):\n    """"""\n    read image\n    :param image_file_path: the file path of image\n    :param height: the height of image (default: None)\n    :param width: the width of image (default: None)\n    :param channel: the channel of image (default: None)\n    :return:\n        image data ndarry type, shape: [height, width, channel]\n    """"""\n    if channel == 1:\n        image = io.imread(image_file_path, as_grey=True)\n    else:\n        image = io.imread(image_file_path, as_grey=False)\n\n    # reshape\n    image_shape = np.shape(image)\n    if len(image_shape) == 2:\n        image = image[:height, :width]\n        image = np.reshape(image, [heigth, width, 1])\n    else:\n        image = image[:height, :width, :]\n\n    return image\n\n\ndef image_read_batch(image_files_list, height=None, width=None, channel=None):\n    """"""\n    read images batch by batch\n    :param image_files_list: image files list\n    :param height: the height of images\n    :param width: the width of images\n    :param channel: the channel of image (default: None)\n    :return:\n        data, a 4-D ndarray, [batch_size, height, width, channel]\n    """"""\n    files_num = len(image_files_list)\n    data = np.zeros([files_num, height, width, channel], dtype=np.float32)\n\n    i = 0\n    for image_file in image_files_list:\n        content = image_read(image_file, height=height, width=width, channel=channel)\n        data[i] = content\n        i = i + 1\n\n    return data\n'"
src/layer.py,165,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2017.12.29\nFinished on 2017.12.29\nModified on 2018.09.17\n\n@author: Yuntao Wang\n""""""\n\nfrom HPFs.filters import *\nimport tensorflow as tf\nfrom math import floor\nfrom tensorflow.contrib.layers.python.layers import batch_norm\n\n\ndef pool_layer(input_data, height, width, x_stride, y_stride, name, is_max_pool=True, padding=""VALID""):\n    """"""\n    pooling layer\n    :param input_data: the input data\n    :param height: the height of the convolutional kernel, no pooling operation in the dimension of ""batch_size"" and ""channel"", so default value is 1\n    :param width: the width of the convolutional kernel\n    :param x_stride: stride in X axis\n    :param y_stride: stride in Y axis\n    :param name: the name of the pooling layer\n    :param is_max_pool: if True, max pooling, else average pooling\n    :param padding: padding=""SAME""\n    :return:\n        output: a 4-D tensor [batch_size, height, width. channel]\n    """"""\n\n    if is_max_pool is True:\n        output = tf.nn.max_pool(input_data,\n                                ksize=[1, height, width, 1],\n                                strides=[1, x_stride, y_stride, 1],\n                                padding=padding,\n                                name=name)\n        pooling_type = ""max_pooling""\n\n    else:\n        output = tf.nn.avg_pool(input_data,\n                                ksize=[1, height, width, 1],\n                                strides=[1, x_stride, y_stride, 1],\n                                padding=padding,\n                                name=name)\n        pooling_type = ""average_pooling""\n\n    shape = output.get_shape()\n\n    print(""name: %s, shape: (%d, %d, %d), type: %s"" % (name, shape[1], shape[2], shape[3], pooling_type))\n\n    return output\n\n\ndef global_pool(input_data, name, is_max_pool=True):\n    """"""\n    global pooling layer\n    :param input_data: the input data\n    :param name: the name of the pooling layer\n    :param is_max_pool: if True, max pooling, else average pooling\n    :return:\n        output: a 4-D tensor [batch_size, height, width. channel]\n    """"""\n    shape = input_data.get_shape()\n    height, width = shape[1], shape[2]\n    output = pool_layer(input_data, height, width, height, width, name=name, is_max_pool=is_max_pool)\n\n    return output\n\n\ndef batch_normalization(input_data, name, activation_method=""relu"", is_train=True):\n    """"""\n    BN layer\n    :param input_data: the input data\n    :param name: name\n    :param activation_method: the method of activation function\n    :param is_train: if False, skip this layer, default is True\n    :return:\n        output: output after batch normalization\n    """"""\n    output = batch_norm(inputs=input_data, decay=0.9, center=True, scale=True, epsilon=1e-5, scope=name, updates_collections=None,\n                        reuse=tf.AUTO_REUSE, is_training=is_train, zero_debias_moving_mean=True)\n    output = activation_layer(input_data=output,\n                              activation_method=activation_method)\n\n    print(""name: %s, activation: %s, is_train: %r"" % (name, activation_method, is_train))\n\n    return output\n\n\ndef batch_normalization_origin(input_data, name, offset=0.0, scale=1.0, variance_epsilon=1e-3):\n    """"""\n    batch normalization layer in IH & MMSec\n    :param input_data: the input data\n    :param name: name of the layer\n    :param offset: beta\n    :param scale: gamma\n    :param variance_epsilon: avoid zero divide error\n    :param variance_epsilon: variance_epsilon\n    :return:\n        output: a 4-D tensor [batch_size, height, width. channel]\n    """"""\n\n    batch_mean, batch_var = tf.nn.moments(input_data, [0])\n    output_data = tf.nn.batch_normalization(x=input_data,\n                                            mean=batch_mean,\n                                            variance=batch_var,\n                                            offset=offset,\n                                            scale=scale,\n                                            variance_epsilon=variance_epsilon,\n                                            name=name)\n    print(""name: %s"" % name)\n\n    return output_data\n\n\ndef dropout(input_data, keep_pro=0.5, name=""dropout"", seed=None, is_train=True):\n    """"""\n    dropout layer\n    :param input_data: the input data\n    :param keep_pro: the probability that each element is kept\n    :param name: name\n    :param seed: int or None. An integer or None to create random seed\n    :param is_train: if False, skip this layer, default is True\n    :return:\n        output: output with dropout\n    drop\xe7\x8e\x87\xe7\x9a\x84\xe9\x80\x89\xe6\x8b\xa9\xef\xbc\x9a\n        \xe7\xbb\x8f\xe8\xbf\x87\xe4\xba\xa4\xe5\x8f\x89\xe9\xaa\x8c\xe8\xaf\x81, \xe9\x9a\x90\xe5\x90\xab\xe8\x8a\x82\xe7\x82\xb9dropout\xe7\x8e\x87\xe7\xad\x89\xe4\xba\x8e0.5\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe6\x95\x88\xe6\x9e\x9c\xe6\x9c\x80\xe5\xa5\xbd, \xe5\x8e\x9f\xe5\x9b\xa0\xe6\x98\xaf0.5\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99dropout\xe9\x9a\x8f\xe6\x9c\xba\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\xe6\x9c\x80\xe5\xa4\x9a\n        dropout\xe4\xb9\x9f\xe5\x8f\xaf\xe8\xa2\xab\xe7\x94\xa8\xe4\xbd\x9c\xe4\xb8\x80\xe7\xa7\x8d\xe6\xb7\xbb\xe5\x8a\xa0\xe5\x99\xaa\xe5\xa3\xb0\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95, \xe7\x9b\xb4\xe6\x8e\xa5\xe5\xaf\xb9input\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x93\x8d\xe4\xbd\x9c. \xe8\xbe\x93\xe5\x85\xa5\xe5\xb1\x82\xe8\xae\xbe\xe4\xb8\xba\xe6\x9b\xb4\xe6\x8e\xa5\xe8\xbf\x911\xe7\x9a\x84\xe6\x95\xb0, \xe4\xbd\xbf\xe5\xbe\x97\xe8\xbe\x93\xe5\x85\xa5\xe5\x8f\x98\xe5\x8c\x96\xe4\xb8\x8d\xe4\xbc\x9a\xe5\xa4\xaa\xe5\xa4\xa7(0.8)\n    """"""\n    if is_train is True:\n        output = tf.nn.dropout(x=input_data,\n                               keep_prob=keep_pro,\n                               name=name,\n                               seed=seed)\n        print(""name: %s, keep_pro: %f"" % (name, keep_pro))\n    else:\n        output = input_data\n\n    return output\n\n\ndef fc_layer(input_data, output_dim, name, activation_method=""relu"", alpha=None, init_method=""xavier"", is_train=True):\n    """"""\n    fully-connected layer\n    :param input_data: the input data tensor [batch_size, height, width, channels]\n    :param output_dim: the dimension of the output data\n    :param name: name of the layer\n    :param activation_method: the type of activation function\n    :param alpha: leakey relu alpha\n    :param init_method: the method of weights initialization (default: xavier)\n    :param is_train: if False, skip this layer, default is True\n    :return:\n        output: a 2-D tensor [batch_size, channel]\n    """"""\n    if is_train is True:\n        shape = input_data.get_shape()\n        if len(shape) == 4:\n            input_dim = shape[1].value * shape[2].value * shape[3].value\n        else:\n            input_dim = shape[-1].value\n\n        flat_input_data = tf.reshape(input_data, [-1, input_dim])\n        if init_method is None:\n            output = input_data\n        else:\n            with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n                # the method of weights initialization\n                if init_method == ""xavier"":\n                    initializer = tf.contrib.layers.xavier_initializer()\n                elif init_method == ""gaussian"":\n                    initializer = tf.random_normal_initializer(stddev=0.01)\n                else:\n                    initializer = tf.truncated_normal_initializer(stddev=0.01)\n\n                weights = tf.get_variable(name=""weight"",\n                                          shape=[input_dim, output_dim],\n                                          dtype=tf.float32,\n                                          initializer=initializer)\n\n                biases = tf.get_variable(name=""biases"",\n                                         shape=[output_dim],\n                                         dtype=tf.float32,\n                                         initializer=tf.constant_initializer(0.0))\n\n                output = tf.nn.bias_add(value=tf.matmul(flat_input_data, weights),\n                                        bias=biases,\n                                        name=""fc_bias_add"")\n\n                output = activation_layer(input_data=output,\n                                          activation_method=activation_method,\n                                          alpha=alpha)\n\n                print(""name: %s, shape: %d -> %d, activation:%s, alpha = %r""\n                      % (name, input_dim, output_dim, activation_method, alpha))\n    else:\n        output = input_data\n\n    return output\n\n\ndef fconv_layer(input_data, filter_num, name, is_train=True, padding=""VALID"", init_method=""xavier"", bias_term=True, is_pretrain=True):\n    """"""\n    fully conv layer\n    :param input_data: the input data tensor [batch_size, height, width, channels]\n    :param filter_num: the number of the convolutional kernel\n    :param name: name of the layer\n    :param is_train: if False, skip this layer, default is True\n    :param padding: the padding method, ""SAME"" | ""VALID"" (default: ""VALID"")\n    :param init_method: the method of weights initialization (default: xavier)\n    :param bias_term: whether the bias term exists or not (default: False)\n    :param is_pretrain: whether the parameters are trainable (default: True)\n    :return:\n        output: a 4-D tensor [batch_size, height, width. channel]\n    """"""\n    if is_train is True:\n        shape = input_data.get_shape()\n        conv_height, conv_width, conv_channel = shape[1].value, shape[2].value, shape[3].value\n\n        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n            # the method of weights initialization\n            if init_method == ""xavier"":\n                initializer = tf.contrib.layers.xavier_initializer()\n            elif init_method == ""gaussian"":\n                initializer = tf.random_normal_initializer(stddev=0.01)\n            else:\n                initializer = tf.truncated_normal_initializer(stddev=0.01)\n\n            weights = tf.get_variable(name=""weights"",\n                                      shape=[conv_height, conv_width, conv_channel, filter_num],\n                                      dtype=tf.float32,\n                                      initializer=initializer,\n                                      trainable=is_pretrain)\n            biases = tf.get_variable(name=""biases"",\n                                     shape=[filter_num],\n                                     dtype=tf.float32,\n                                     initializer=tf.constant_initializer(0.0),\n                                     trainable=is_pretrain)\n            feature_map = tf.nn.conv2d(input=input_data,\n                                       filter=weights,\n                                       strides=[1, 1, 1, 1],\n                                       padding=padding,\n                                       name=""conv"")\n            # biases term\n            if bias_term is True:\n                output = tf.nn.bias_add(value=feature_map,\n                                        bias=biases,\n                                        name=""biases_add"")\n            else:\n                output = feature_map\n    else:\n        output = input_data\n\n    # info show\n    shape = output.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)""\n          % (name, shape[1], shape[2], shape[3]))\n\n    return output\n\n\ndef activation_layer(input_data, activation_method=""None"", alpha=0.2):\n    """"""\n    activation function layer\n    :param input_data: the input data tensor [batch_size, height, width, channels]\n    :param activation_method: the type of activation function\n    :param alpha: for leaky relu\n        ""relu"": max(features, 0)\n        ""relu6"": min(max(features, 0), 6)\n        ""tanh"": tanh(features)\n        ""sigmoid"": 1 / (1 + exp(-features))\n        ""softplus"": log(exp(features) + 1)\n        ""elu"": exp(features) - 1 if < 0, features otherwise\n        ""crelu"": [relu(features), -relu(features)]\n        ""leakrelu"": leak * features, if < 0, feature, otherwise\n        ""softsign"": features / (abs(features) + 1)\n    :return:\n        output: a 4-D tensor [batch_size, height, width. channel]\n    """"""\n    if activation_method == ""relu"":\n        output = tf.nn.relu(input_data, name=""relu"")\n    elif activation_method == ""relu6"":\n        output = tf.nn.relu6(input_data, name=""relu6"")\n    elif activation_method == ""tanh"":\n        output = tf.nn.tanh(input_data, name=""tanh"")\n    elif activation_method == ""sigmoid"":\n        output = tf.nn.sigmoid(input_data, name=""sigmoid"")\n    elif activation_method == ""softplus"":\n        output = tf.nn.softplus(input_data, name=""softplus"")\n    elif activation_method == ""crelu"":\n        output = tf.nn.crelu(input_data, ""crelu"")\n    elif activation_method == ""elu"":\n        output = tf.nn.elu(input_data, name=""elu"")\n    elif activation_method == ""softsign"":\n        output = tf.nn.softsign(input_data, ""softsign"")\n    elif activation_method == ""leakrelu"":\n        output = tf.where(input_data < 0.0, alpha * input_data, input_data)\n    else:\n        output = input_data\n\n    return output\n\n\ndef conv_layer(input_data, height, width, x_stride, y_stride, filter_num, name,\n               activation_method=""relu"", alpha=0.2, padding=""VALID"", atrous=1,\n               init_method=""xavier"", bias_term=True, is_pretrain=True):\n    """"""\n    convolutional layer\n    :param input_data: the input data tensor [batch_size, height, width, channels]\n    :param height: the height of the convolutional kernel\n    :param width: the width of the convolutional kernel\n    :param x_stride: stride in X axis\n    :param y_stride: stride in Y axis\n    :param filter_num: the number of the convolutional kernel\n    :param name: the name of the layer\n    :param activation_method: the type of activation function (default: relu)\n    :param alpha: leaky relu alpha (default: 0.2)\n    :param padding: the padding method, ""SAME"" | ""VALID"" (default: ""VALID"")\n    :param atrous: the dilation rate, if atrous == 1, conv, if atrous > 1, dilated conv (default: 1)\n    :param init_method: the method of weights initialization (default: xavier)\n    :param bias_term: whether the bias term exists or not (default: False)\n    :param is_pretrain: whether the parameters are trainable (default: True)\n\n    :return:\n        output: a 4-D tensor [number, height, width, channel]\n    """"""\n    channel = input_data.get_shape()[-1]\n\n    # the method of weights initialization\n    if init_method == ""xavier"":\n        initializer = tf.contrib.layers.xavier_initializer()\n    elif init_method == ""gaussian"":\n        initializer = tf.random_normal_initializer(stddev=0.01)\n    else:\n        initializer = tf.truncated_normal_initializer(stddev=0.01)\n\n    # the initialization of the weights and biases\n    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n        weights = tf.get_variable(name=""weights"",\n                                  shape=[height, width, channel, filter_num],\n                                  dtype=tf.float32,\n                                  initializer=initializer,\n                                  trainable=is_pretrain)\n        biases = tf.get_variable(name=""biases"",\n                                 shape=[filter_num],\n                                 dtype=tf.float32,\n                                 initializer=tf.constant_initializer(0.0),\n                                 trainable=is_pretrain)\n\n        # the method of convolution\n        if atrous == 1:\n            feature_map = tf.nn.conv2d(input=input_data,\n                                       filter=weights,\n                                       strides=[1, x_stride, y_stride, 1],\n                                       padding=padding,\n                                       name=""conv"")\n        else:\n            feature_map = tf.nn.atrous_conv2d(value=input_data,\n                                              filters=weights,\n                                              rate=atrous,\n                                              padding=padding,\n                                              name=""atrous_conv"")\n        # biases term\n        if bias_term is True:\n            output = tf.nn.bias_add(value=feature_map,\n                                    bias=biases,\n                                    name=""biases_add"")\n        else:\n            output = feature_map\n\n        # info show\n        shape = output.get_shape()\n        print(""name: %s, shape: (%d, %d, %d), activation: %s""\n              % (name, shape[1], shape[2], shape[3], activation_method))\n\n        # activation\n        output = activation_layer(output, activation_method, alpha)\n\n        return output\n\n\ndef static_conv_layer(input_data, kernel, x_stride, y_stride, name, padding=""VALID""):\n    """"""\n    convolutional layer with static kernel which can be seen as a HPF\n    :param input_data: the input data tensor [batch_size, height, width, channels]\n    :param kernel: the filter kernel\n    :param x_stride: stride in X axis\n    :param y_stride: stride in Y axis\n    :param name: the name of the layer\n    :param padding: the padding method, ""SAME"" | ""VALID"" (default: ""VALID"")\n\n    :return:\n        feature_map: 4-D tensor [number, height, width, channel]\n    """"""\n    with tf.variable_scope(name):\n        feature_map = tf.nn.conv2d(input=input_data,\n                                   filter=kernel,\n                                   strides=[1, x_stride, y_stride, 1],\n                                   padding=padding,\n                                   name=""conv"")\n        shape = feature_map.get_shape()\n        print(""name: %s, shape: (%d, %d, %d)""\n              % (name, shape[1], shape[2], shape[3]))\n\n        return feature_map\n\n\ndef phase_split(input_data, block_size=8, name=None):\n    """"""\n    get m * m (2 or 4 in general) block_split layer\n    :param input_data: the input data tensor [batch_size, height, width, channels]\n    :param block_size: size of block, 2 or 4\n    :param name: the name of the layer\n    :return:\n        feature_map: 4-D tensor [number, height, width, channel]\n    """"""\n\n    block_num = block_size * block_size\n    init_block = block_num * [0]\n    temp = init_block[:]\n    temp[0] = 1\n    phase_split_kernel = tf.constant(value=temp,\n                                     dtype=tf.float32,\n                                     shape=[block_size, block_size, 1, 1],\n                                     name=""phase_aware_"" + str(block_size) + ""x"" + str(block_size) + ""_0"")\n\n    output = tf.nn.conv2d(input=input_data,\n                          filter=phase_split_kernel,\n                          strides=[1, block_size, block_size, 1],\n                          padding=""VALID"",\n                          name=""phase_split_"" + str(block_size) + ""x"" + str(block_size) + ""_0"")\n\n    for i in range(block_num - 1):\n        temp = init_block[:]\n        temp[i + 1] = 1\n        phase_split_kernel = tf.constant(value=temp,\n                                         dtype=tf.float32,\n                                         shape=[block_size, block_size, 1, 1],\n                                         name=""phase_aware_"" + str(block_size) + ""x"" + str(block_size) + ""_"" + str(i + 1))\n\n        result = tf.nn.conv2d(input=input_data,\n                              filter=phase_split_kernel,\n                              strides=[1, block_size, block_size, 1],\n                              padding=""VALID"",\n                              name=""phase_split_"" + str(block_size) + ""x"" + str(block_size) + ""_"" + str(i + 1))\n\n        output = tf.concat([output, result], 3, ""phase_split_concat"")\n\n    shape = output.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)""\n          % (name, shape[1], shape[2], shape[3]))\n\n    return output\n\n\ndef diff_layer(input_data, is_diff, is_diff_abs, is_abs_diff, order, direction, name, padding=""SAME""):\n    """"""\n    the layer which is used for difference\n    :param input_data: the input data tensor [batch_size, height, width, channels]\n    :param is_diff: whether make difference or not\n    :param is_diff_abs: whether make difference and abs or not\n    :param is_abs_diff: whether make abs and difference or not\n    :param order: the order of difference\n    :param direction: the direction of difference, ""inter""(between row) or ""intra""(between col)\n    :param name: the name of the layer\n    :param padding: the method of padding, default is ""SAME""\n\n    :return:\n        feature_map: 4-D tensor [number, height, width, channel]\n    """"""\n\n    print(""name: %s, is_diff: %r, is_diff_abs: %r, is_abs_diff: %r, order: %d, direction: %s""\n          % (name, is_diff, is_diff_abs, is_abs_diff, order, direction))\n\n    if order == 0:\n        return input_data\n    else:\n        if order == 1 and direction == ""inter"":\n            filter_diff = tf.constant(value=[1, -1],\n                                      dtype=tf.float32,\n                                      shape=[2, 1, 1, 1],\n                                      name=""diff_inter_1"")\n        elif order == 1 and direction == ""intra"":\n            filter_diff = tf.constant(value=[1, -1],\n                                      dtype=tf.float32,\n                                      shape=[1, 2, 1, 1],\n                                      name=""diff_intra_1"")\n        elif order == 2 and direction == ""inter"":\n            filter_diff = tf.constant(value=[1, -2, 1],\n                                      dtype=tf.float32,\n                                      shape=[3, 1, 1, 1],\n                                      name=""diff_inter_2"")\n        elif order == 2 and direction == ""intra"":\n            filter_diff = tf.constant(value=[1, -2, 1],\n                                      dtype=tf.float32,\n                                      shape=[1, 3, 1, 1],\n                                      name=""diff_intra_2"")\n        else:\n            filter_diff = tf.constant(value=[1],\n                                      dtype=tf.float32,\n                                      shape=[1, 1, 1, 1],\n                                      name=""None"")\n\n        if is_diff is True:\n            output = tf.nn.conv2d(input=input_data,\n                                  filter=filter_diff,\n                                  strides=[1, 1, 1, 1],\n                                  padding=padding)\n\n            return output\n\n        elif is_diff_abs is True:\n            output = tf.nn.conv2d(input=input_data,\n                                  filter=filter_diff,\n                                  strides=[1, 1, 1, 1],\n                                  padding=padding)\n            output = tf.abs(output)\n\n            return output\n\n        elif is_abs_diff is True:\n            input_data = tf.abs(input_data)\n            output = tf.nn.conv2d(input=input_data,\n                                  filter=filter_diff,\n                                  strides=[1, 1, 1, 1],\n                                  padding=padding)\n\n            return output\n\n        else:\n            return input_data\n\n\ndef rich_hpf_layer(input_data, name):\n    """"""\n    multiple HPF processing\n    diff_layer(input_data, is_diff, is_diff_abs, is_abs_diff, order, direction, name, padding=""SAME"")\n\n    :param input_data: the input data tensor [batch_size, height, width, channels]\n    :param name: the name of the layer\n    :return:\n        feature_map: 4-D tensor [number, height, width, channel]\n    """"""\n    dif_inter_1 = diff_layer(input_data, True, False, False, 1, ""inter"", ""dif_inter_1"", padding=""SAME"")\n    dif_inter_2 = diff_layer(input_data, True, False, False, 2, ""inter"", ""dif_inter_2"", padding=""SAME"")\n    dif_intra_1 = diff_layer(input_data, True, False, False, 1, ""intra"", ""dif_intra_1"", padding=""SAME"")\n    dif_intra_2 = diff_layer(input_data, True, False, False, 2, ""intra"", ""dif_intra_2"", padding=""SAME"")\n\n    dif_abs_inter_1 = diff_layer(input_data, False, False, True, 1, ""inter"", ""abs_dif_inter_1"", padding=""SAME"")\n    dif_abs_inter_2 = diff_layer(input_data, False, False, True, 2, ""inter"", ""abs_dif_inter_2"", padding=""SAME"")\n    dif_abs_intra_1 = diff_layer(input_data, False, False, True, 1, ""intra"", ""abs_dif_intra_1"", padding=""SAME"")\n    dif_abs_intra_2 = diff_layer(input_data, False, False, True, 2, ""intra"", ""abs_dif_intra_2"", padding=""SAME"")\n\n    output = tf.concat([dif_inter_1, dif_inter_2, dif_intra_1, dif_intra_2, dif_abs_inter_1, dif_abs_inter_2, dif_abs_intra_1, dif_abs_intra_2], 3, name=name)\n\n    return output\n\n\ndef loss_layer(logits, labels, logits_siamese=None, is_regulation=False, coeff=1e-3, method=""sparse_softmax_cross_entropy""):\n    """"""\n    calculate the loss\n    :param logits: logits [batch_size, class_num]\n    :param labels: labels [batch_size, class_num]\n    :param logits_siamese: logits of siamese network [batch_size, class_num]\n    :param is_regulation: whether regulation or not\n    :param coeff: the coefficients of the regulation\n    :param method: loss method\n    :return:\n        loss_total: loss with regularization\n    """"""\n    with tf.variable_scope(""loss""):\n        if method == ""sparse_softmax_cross_entropy"":\n            loss = tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels)\n        elif method == ""siamese_loss"":\n            loss = siamese_loss(logits1=logits, logits2=logits_siamese, labels=labels)\n        else:\n            loss = tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels)\n\n        loss = tf.reduce_mean(loss)\n\n        if is_regulation is True:\n            tv = tf.trainable_variables()\n            regularization_cost = coeff * tf.reduce_sum([tf.nn.l2_loss(v) for v in tv])\n            loss_total = loss + regularization_cost\n        else:\n            loss_total = loss\n\n        return loss_total\n\n\ndef accuracy_layer(logits, labels):\n    """"""\n    calculate the accuracy\n    :param logits: logits\n    :param labels: label\n    :return: accuracy\n    """"""\n    with tf.variable_scope(""accuracy""):\n        predictions = tf.nn.in_top_k(logits, labels, 1, name=""predictions"")\n        results = tf.cast(predictions, tf.float16)\n        accuracy = tf.reduce_mean(results)\n\n        return accuracy\n\n\ndef evaluation(logits, labels):\n    """"""\n    calculate the accuracy, fpr and fnr\n    :param logits: logits\n    :param labels: label\n    :return: accuracy, false_positive_rate, false_negative_rate,\n    true_positive_rate, true_negative_rate, precision, recall, f1_score\n    """"""\n    predictions = tf.nn.softmax(logits)\n    predictions = tf.argmax(predictions, 1)\n\n    ones_like_labels = tf.ones_like(labels)\n    zeros_like_labels = tf.zeros_like(labels)\n    ones_like_predictions = tf.ones_like(predictions)\n    zeros_like_predictions = tf.zeros_like(predictions)\n\n    true_positive = tf.reduce_sum(\n        tf.cast(\n            tf.logical_and(\n                tf.equal(labels, ones_like_labels),\n                tf.equal(predictions, ones_like_predictions)), tf.float32))\n\n    true_negative = tf.reduce_sum(\n        tf.cast(\n            tf.logical_and(\n                tf.equal(labels, zeros_like_labels),\n                tf.equal(predictions, zeros_like_predictions)), tf.float32))\n\n    false_positive = tf.reduce_sum(\n        tf.cast(\n            tf.logical_and(\n                tf.equal(labels, zeros_like_labels),\n                tf.equal(predictions, ones_like_predictions)), tf.float32))\n\n    false_negative = tf.reduce_sum(\n        tf.cast(\n            tf.logical_and(\n                tf.equal(labels, ones_like_labels),\n                tf.equal(predictions, zeros_like_predictions)), tf.float32))\n\n    false_positive_rate = false_positive / (tf.add(false_positive, true_negative))\n    false_negative_rate = false_negative / (tf.add(false_negative, true_positive))\n\n    accuracy = 1 - (false_positive_rate + false_negative_rate) / 2\n\n    return accuracy, false_positive_rate, false_negative_rate\n\n\ndef evaluation_full(logits, labels):\n    """"""\n    calculate the accuracy, fpr and fnr, tpr, tnr, precision, recall, f1_score\n    :param logits: logits\n    :param labels: label\n    :return: accuracy, false_positive_rate, false_negative_rate,\n    true_positive_rate, true_negative_rate, precision, recall, f1_score\n    """"""\n    predictions = tf.nn.softmax(logits)\n    predictions = tf.argmax(predictions, 1)\n\n    ones_like_labels = tf.ones_like(labels)\n    zeros_like_labels = tf.zeros_like(labels)\n    ones_like_predictions = tf.ones_like(predictions)\n    zeros_like_predictions = tf.zeros_like(predictions)\n\n    true_positive = tf.reduce_sum(\n        tf.cast(\n            tf.logical_and(\n                tf.equal(labels, ones_like_labels),\n                tf.equal(predictions, ones_like_predictions)), tf.float32))\n\n    true_negative = tf.reduce_sum(\n        tf.cast(\n            tf.logical_and(\n                tf.equal(labels, zeros_like_labels),\n                tf.equal(predictions, zeros_like_predictions)), tf.float32))\n\n    false_positive = tf.reduce_sum(\n        tf.cast(\n            tf.logical_and(\n                tf.equal(labels, zeros_like_labels),\n                tf.equal(predictions, ones_like_predictions)), tf.float32))\n\n    false_negative = tf.reduce_sum(\n        tf.cast(\n            tf.logical_and(\n                tf.equal(labels, ones_like_labels),\n                tf.equal(predictions, zeros_like_predictions)), tf.float32))\n\n    false_positive_rate = false_positive / (tf.add(false_positive, true_negative))\n    false_negative_rate = false_negative / (tf.add(false_negative, true_positive))\n\n    accuracy = 1 - (false_positive_rate + false_negative_rate) / 2\n\n    return accuracy, false_positive_rate, false_negative_rate, true_positive_rate, true_negative_rate, precision, recall, f1_score\n\n\ndef error_layer(logits, labels):\n    """"""\n    calculate the error\n    :param logits: logits\n    :param labels: label\n    :return: error rate\n    """"""\n    with tf.variable_scope(""accuracy""):\n        logits = tf.nn.softmax(logits)\n        results = tf.cast(tf.argmax(logits, 1), tf.int32)\n        wrong_prediction = tf.not_equal(results, labels)\n        accuracy = tf.reduce_mean(tf.cast(wrong_prediction, tf.float32))\n\n        return accuracy\n\n\ndef siamese_loss(logits1, logits2, labels):\n    """"""\n    loss calculation for siamese network\n    :param logits1: logits1 of network\n    :param logits2: logits2 of network\n    :param labels: new label indicating whether logit1 and logit2 belong to one class, different - 0, same - 1\n    :return:\n    """"""\n    constant = 5\n    constant = tf.constant(constant, name=""constant"", dtype=tf.float32)\n    distance = tf.sqrt(tf.reduce_sum(tf.square(logits1 - logits2), 1))\n    pos = tf.multiply(tf.multiply(labels, 2 / constant), tf.square(distance))\n    neg = tf.multiply(tf.multiply(1 - labels, 2 * constant), tf.exp(-2.77 / constant * distance))\n    loss = tf.add(pos, neg)\n    loss = tf.reduce_mean(loss)\n    \n    return loss\n\n\ndef optimizer(losses, learning_rate, global_step, optimizer_type=""Adam"", beta1=0.9, beta2=0.999,\n              epsilon=1e-8, initial_accumulator_value=0.1, momentum=0.9, decay=0.9):\n    """"""\n    optimizer\n    :param losses: loss\n    :param learning_rate: lr\n    :param global_step: global step\n    :param optimizer_type: the type of optimizer\n    \xe5\x8f\xaf\xe9\x80\x89\xe7\xb1\xbb\xe5\x9e\x8b:\n    GradientDescent                                               -> W += learning_rate * dW\n    Adagrad         catch += dW ** 2                              -> W += -learning_rate * dW / (sqrt(catch) + epsilon)\n    Adam            m = beta1 * m + (1 - beta1) * dW\n                    v = beta2 * v + (1 - beta2) * (dW ** 2)       -> W += -learning_rate * m / (sqrt(v) + epsilon)\n    Momentum        v = (momentum * v - learning * dW)            -> W += v\n    RMSProp         catch += decay * catch + (1 - decay) * dW ** 2-> W += -learning_rate * dW /  (sqrt(catch) + epsilon)\n    Note:\n        Adam\xe9\x80\x9a\xe5\xb8\xb8\xe4\xbc\x9a\xe5\x8f\x96\xe5\xbe\x97\xe6\xaf\x94\xe8\xbe\x83\xe5\xa5\xbd\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe6\x94\xb6\xe6\x95\x9b\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xbf\xab\xe7\x9b\xb8\xe6\xaf\x94SGD\n        L-BFGS\xe9\x80\x82\xe7\x94\xa8\xe4\xba\x8e\xe5\x85\xa8batch\xe5\x81\x9a\xe4\xbc\x98\xe5\x8c\x96\xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\n        \xe6\x9c\x89\xe6\x97\xb6\xe5\x80\x99\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xa4\x9a\xe7\xa7\x8d\xe4\xbc\x98\xe5\x8c\x96\xe6\x96\xb9\xe6\xb3\x95\xe5\x90\x8c\xe6\x97\xb6\xe4\xbd\xbf\xe7\x94\xa8\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\xe4\xbd\xbf\xe7\x94\xa8SGD\xe8\xbf\x9b\xe8\xa1\x8cwarm up\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8eAdam\n        \xe5\xaf\xb9\xe4\xba\x8e\xe6\xaf\x94\xe8\xbe\x83\xe5\xa5\x87\xe6\x80\xaa\xe7\x9a\x84\xe9\x9c\x80\xe6\xb1\x82\xef\xbc\x8cdeepbit\xe4\xb8\xa4\xe4\xb8\xaaloss\xe7\x9a\x84\xe6\x94\xb6\xe6\x95\x9b\xe9\x9c\x80\xe8\xa6\x81\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x8e\xa7\xe5\x88\xb6\xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\xef\xbc\x8c\xe6\xaf\x94\xe8\xbe\x83\xe6\x85\xa2\xe7\x9a\x84SGD\xe6\xaf\x94\xe8\xbe\x83\xe9\x80\x82\xe7\x94\xa8\n\n    :param beta1: Adam, default 0.9\n    :param beta2: Adam, default 0.999\n    :param epsilon: Adam | RMSProp, default 1e-8\n    :param initial_accumulator_value: Adagrad, default 0.1\n    :param momentum: Momentum | RMSProp, default 0.9\n    :param decay: Momentum | RMSProp, default 0.9\n    :return:\n        train_op: optimizer\n    """"""\n    with tf.name_scope(""optimizer""):\n        if optimizer_type == ""GradientDescent"":\n            opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate,\n                                                    name=optimizer_type)\n        elif optimizer_type == ""Adagrad"":\n            opt = tf.train.AdagradOptimizer(learning_rate=learning_rate,\n                                            initial_accumulator_value=initial_accumulator_value,\n                                            name=optimizer_type)\n\n        elif optimizer_type == ""Adam"":\n            opt = tf.train.AdamOptimizer(learning_rate=learning_rate,\n                                         beta1=beta1,\n                                         beta2=beta2,\n                                         epsilon=epsilon,\n                                         name=optimizer_type)\n\n        elif optimizer_type == ""Momentum"":\n            opt = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n                                             momentum=momentum,\n                                             name=optimizer_type)\n\n        elif optimizer_type == ""RMSProp"":\n            opt = tf.train.RMSPropOptimizer(learning_rate=learning_rate,\n                                            decay=decay,\n                                            momentum=momentum,\n                                            epsilon=epsilon,\n                                            name=optimizer_type)\n        else:\n            opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate,\n                                                    name=optimizer_type)\n\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            train_op = opt.minimize(loss=losses,\n                                    global_step=global_step,\n                                    name=""optimizer"")\n\n        return train_op\n\n\ndef learning_rate_decay(init_learning_rate, global_step, decay_steps, decay_rate, decay_method=""exponential"", staircase=False,\n                        end_learning_rate=0.0001, power=1.0, cycle=False):\n    """"""\n    function: learning rate decay -> constant |  | inverse_time | natural_exp | polynomial\n\n    :param init_learning_rate: A scalar float32 or float64 Tensor or a Python number. The initial learning rate.\n    :param decay_method: The method of learning rate decay\n    :param global_step: A scalar int32 or int64 Tensor or a Python number. Global step to use for the decay computation. Must not be negative.\n    :param decay_steps: A scalar int32 or int64 Tensor or a Python number. Must be positive. See the decay computation above.\n    :param decay_rate: A scalar float32 or float64 Tensor or a Python number. The decay rate.\n    :param staircase: Boolean. If True decay the learning rate at discrete intervals.\n    :param end_learning_rate: A scalar float32 or float64 Tensor or a Python number. The minimal end learning rate.\n    :param power: A scalar float32 or float64 Tensor or a Python number. The power of the polynomial. Defaults to linear, 1.0.\n    :param cycle: A boolean, whether or not it should cycle beyond decay_steps.\n    :return:\n        decayed_learning_rate\n        type:\n            fixed               -> decayed_learning_rate = learning_rate\n            step                -> decayed_learning_rate = learning_rate ^ (floor(global_step / decay_steps))\n            exponential_decay   -> decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)\n            inverse_time_decay  -> decayed_learning_rate = learning_rate / (1 + decay_rate * t)\n            natural_exp_decay   -> decayed_learning_rate = learning_rate * exp(-decay_rate * global_step)\n            polynomial_decay    -> decayed_learning_rate =\n                                    (learning_rate - end_learning_rate) * (1 - global_step / decay_steps) ^ (power) + end_learning_rate\n    """"""\n    if decay_method == ""fixed"":\n        decayed_learning_rate = init_learning_rate\n    elif decay_method == ""exponential"":\n        decayed_learning_rate = tf.train.exponential_decay(init_learning_rate, global_step, decay_steps, decay_rate, staircase)\n    elif decay_method == ""inverse_time"":\n        decayed_learning_rate = tf.train.inverse_time_decay(init_learning_rate, global_step, decay_steps, decay_rate, staircase)\n    elif decay_method == ""natural_exp"":\n        decayed_learning_rate = tf.train.natural_exp_decay(init_learning_rate, global_step, decay_steps, decay_rate, staircase)\n    elif decay_method == ""polynomial"":\n        decayed_learning_rate = tf.train.polynomial_decay(init_learning_rate, global_step, decay_steps, decay_rate, end_learning_rate, power, cycle)\n    elif decay_method == ""step"":\n        decayed_learning_rate = tf.pow(init_learning_rate * decay_rate, floor(global_step / decay_steps))\n    else:\n        decayed_learning_rate = init_learning_rate\n\n    return decayed_learning_rate\n'"
src/main.py,1,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on 2017.11.16\nFinished on 2017.12.19\nModified on 2018.08.24\n\n@author: Yuntao Wang\n""""""\n\nimport os\nimport sys\nimport platform\nfrom run import *\nfrom config import *\nfrom manager import *\n\n\ndef main():\n    # command parsing\n    params_num = len(sys.argv)\n\n    # json config mode\n    if params_num == 1:\n        config_file_path = ""./config_file/config_train.json""\n        arguments = config_train_file_read(config_file_path)\n    elif params_num == 2:\n        config_file_path = ""./config_file/config_"" + sys.argv[1] + "".json""\n        command = ""config_"" + sys.argv[1] + ""_file_read("" + ""\'"" + config_file_path + ""\'"" + "")""\n        arguments = eval(command)\n\n    # command line mode\n    else:\n        arguments = command_parse()\n\n    # mode of gpu selection: auto, manu and others\n    if arguments.gpu_selection == ""auto"":\n        if platform.system() == ""Linux"":\n            gm = GPUManager()\n            allocated_gpu = gm.auto_choice()\n        else:\n            allocated_gpu = ""0""\n    elif arguments.gpu_selection == ""manu"":\n        allocated_gpu = arguments.gpu\n    else:\n        allocated_gpu = """"\n\n    tf.reset_default_graph()\n    os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""\n    os.environ[""CUDA_VISIBLE_DEVICES""] = allocated_gpu\n\n    run_mode(arguments)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
src/manager.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2018.02.07\nFinished on 2018.03.08\nModified on 2018.08.24\n\n@author: Yuntao Wang\n""""""\n\nimport os\nimport platform\nimport tensorflow as tf\nfrom tensorflow.python.client import device_lib as _device_lib\n\n""""""\nExample:\ngm = GPUManager()\nwith gm.auto_choice():\n    balabala\n""""""\n\n\ndef is_gpu_available(cuda_only=True):\n    if cuda_only:\n        result = any((x.device_type == \'GPU\') for x in _device_lib.list_local_devices())\n    else:\n        result = any((x.device_type == \'GPU\' or x.device_type == \'SYCL\') for x in _device_lib.list_local_devices())\n\n    return result\n\n\ndef parse(line, query_args):\n    """"""\n    line:\n        a line of text\n    query_args:\n        query arguments\n    return:\n        a dict of gpu info\n    Parsing a line of csv format text returned by nvidia-smi\n    """"""\n    countable_args = [""memory.free"", ""memory.total"", ""power.draw"", ""power.limit""]\n    power_manage_enable = lambda v: (""Not Support"" not in v)                                    # whether the GPU supports power management\n    to_countable = lambda v: float(v.upper().strip().replace(""MIB"", """").replace(""W"", """"))       # remove the unit\n    process = lambda k, v: ((int(to_countable(v)) if power_manage_enable(v) else 1) if k in countable_args else v.strip())\n    return {k: process(k, v) for k, v in zip(query_args, line.strip().split("",""))}\n\n\ndef query_gpu(query_args=None):\n    """"""\n    query_args:\n        query arguments\n    return:\n        a list of dict\n    Querying GPUs info\n    """"""\n    if query_args is None:\n        query_args = []\n    query_args = [""index"", ""gpu_name"", ""memory.free"", ""memory.total"", ""power.draw"", ""power.limit""] + query_args\n    cmd = ""nvidia-smi --query-gpu={} --format=csv,noheader"".format("","".join(query_args))\n    results = os.popen(cmd).readlines()\n\n    return [parse(line, query_args) for line in results]\n\n\ndef by_power(d):\n    """"""\n    helper function for sorting gpus by power\n    """"""\n    power_info = (d[""power.draw""], d[""power.limit""])\n    if any(v == 1 for v in power_info):\n        print(""Power management unable for GPU {}"".format(d[""index""]))\n        return 1\n\n    return float(d[""power.draw""]) / d[""power.limit""]\n\n\nclass GPUManager:\n    """"""\n    query_args:\n        query arguments\n    A manager which can list all available GPU devices\n    and sort them and choice the most free one.Unspecified\n    ones pref.\n    """"""\n\n    def __init__(self, query_args=None):\n        if query_args is None:\n            query_args = []\n        self.query_args = query_args\n        self.gpus = query_gpu(query_args)\n        for gpu in self.gpus:\n            gpu[""specified""] = False\n        self.gpu_num = len(self.gpus)\n\n    @staticmethod\n    def _sort_by_memory(gpus, by_size=False):\n        if by_size:\n            print(""Sorted by free memory size"")\n            return sorted(gpus, key=lambda d: d[""memory.free""], reverse=True)\n        else:\n            print(""Sorted by free memory rate"")\n            return sorted(gpus, key=lambda d: float(d[""memory.free""]) / d[""memory.total""], reverse=True)\n\n    @staticmethod\n    def _sort_by_power(gpus):\n        return sorted(gpus, key=by_power)\n\n    @staticmethod\n    def _sort_by_custom(gpus, key, reverse=False, query_args=None):\n        if query_args is None:\n            query_args = []\n        if isinstance(key, str) and (key in query_args):\n            return sorted(gpus, key=lambda d: d[key], reverse=reverse)\n        if isinstance(key, type(lambda a: a)):\n            return sorted(gpus, key=key, reverse=reverse)\n        raise ValueError(""The argument \'key\' must be a function or a key in query args,please read the documentation of nvidia-smi"")\n\n    def auto_choice(self, mode=0):\n        """"""\n        mode:\n            0: (default)sorted by free memory size\n        return:\n            a TF device object\n        Auto choice the freest GPU device,not specified ones\n        """"""\n        for old_info, new_info in zip(self.gpus, query_gpu(self.query_args)):\n            old_info.update(new_info)\n        unspecified_gpus = [gpu for gpu in self.gpus if not gpu[""specified""]] or self.gpus\n\n        if mode == 0:\n            print(""Choosing the GPU device has largest free memory..."")\n            chosen_gpu = self._sort_by_memory(unspecified_gpus, True)[0]\n        elif mode == 1:\n            print(""Choosing the GPU device has highest free memory rate..."")\n            chosen_gpu = self._sort_by_power(unspecified_gpus)[0]\n        elif mode == 2:\n            print(""Choosing the GPU device by power..."")\n            chosen_gpu = self._sort_by_power(unspecified_gpus)[0]\n        else:\n            print(""Given an unavailable mode,will be chosen by memory"")\n            chosen_gpu = self._sort_by_memory(unspecified_gpus)[0]\n        chosen_gpu[""specified""] = True\n        index = chosen_gpu[""index""]\n        print(""Using GPU {i}:\\n{info}"".format(i=index, info=""\\n"".join([str(k) + "":"" + str(v) for k, v in chosen_gpu.items()])))\n        print(""GPU-%s is selected."" % index)\n\n        return index\n'"
src/run.py,48,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2017.11.27\nFinished on 2017.11.30\nModified on 2018.09.18\n\n@author: Yuntao Wang\n""""""\n\nimport time\nimport datetime\nimport traceback\nfrom utils import *\nfrom networks.networks import networks\nfrom file_preprocess import get_file_name\nfrom networks.audio_steganalysis import *\nfrom networks.image_steganalysis import *\nfrom networks.tested_steganalysis import *\nfrom networks.image_classification import *\n\n""""""\n    function: \n        train(args)                                             # train\n        test(args)                                              # test\n        steganalysis_one(args)                                  # steganalysis for one sample\n        steganalysis_batch(args)                                # steganalysis for multiple samples\n""""""\n\n\ndef run_mode(args):\n    if args.mode == ""train"":                                    # train mode\n        train(args)\n    elif args.mode == ""test"":                                   # test mode\n        test(args)\n    elif args.mode == ""steganalysis"":                           # steganalysis mode\n        if args.submode == ""one"":\n            if get_path_type(args.steganalysis_file_path) == ""file"":\n                steganalysis_one(args)\n            else:\n                print(""The submode miss-matches the file path, please try again."")\n\n        if args.submode == ""batch"":\n            if get_path_type(args.steganalysis_files_path) == ""folder"":\n                steganalysis_batch(args)\n            else:\n                print(""The submode miss-matches the files path, please try again."")\n    else:\n        print(""Mode Error"")\n\n\ndef train(args):\n    # hyper parameters\n    batch_size = args.batch_size                                                            # batch size\n    height, width, channel = args.height, args.width, args.channel                          # height and width of input matrix\n    init_learning_rate = args.learning_rate                                                 # initialized learning rate\n    decay_method = args.decay_method                                                        # decay method\n    decay_steps, decay_rate = args.decay_step, args.decay_rate                              # decay steps | decay rate\n    loss_method = args.loss_method                                                          # the calculation method of loss function\n    is_regulation = args.is_regulation                                                      # regulation or not\n    coeff_regulation = args.coeff_regulation                                                # the gain of regulation\n    classes_num = args.class_num                                                            # classes number\n    carrier = args.carrier                                                                  # carrier (qmdct | audio | image)\n    task_name = args.task_name                                                              # task name\n    checkpoint = args.checkpoint                                                            # checkpoint\n    n_epoch = args.epoch if args.indicator == ""epoch"" else 500                              # epoch\n\n    max_to_keep = args.max_to_keep                                                          # maximum number of recent checkpoints to keep\n    keep_checkpoint_every_n_hours = args.keep_checkpoint_every_n_hours                      # how often to keep checkpoints\n    start_index_train, end_index_train = args.start_index_train, args.end_index_train       # the scale of the dataset (train)\n    start_index_valid, end_index_valid = args.start_index_valid, args.end_index_valid       # the scale of the dataset (valid)\n    step_train, step_valid = 0, 0                                                           # train step and valid step\n    file_type = args.file_type                                                              # file type\n    max_accuracy, max_accuracy_epoch = 0, 0                                                 # max validation accuracy and corresponding epoch\n    corresponding_fpr, corresponding_fnr = 0, 0                                             # the corresponding fpr and fnr in the max validation accuracy\n\n    # file path (full mode)\n    cover_train_path = args.cover_train_path\n    stego_train_path = args.stego_train_path\n    cover_valid_path = args.cover_valid_path\n    stego_valid_path = args.stego_valid_path\n\n    # file path (semi mode)\n    cover_files_path = args.cover_files_path\n    stego_files_path = args.stego_files_path\n\n    model_path = args.model_path\n    log_path = args.log_path\n\n    # information output\n    if cover_train_path is not None:\n        print(""train files path(cover): %s"" % cover_train_path)\n        print(""valid files path(cover): %s"" % cover_valid_path)\n        print(""train files path(stego): %s"" % stego_train_path)\n        print(""valid files path(stego): %s"" % stego_valid_path)\n    else:\n        print(""cover files path: %s"" % cover_files_path)\n        print(""stego files path: %s"" % stego_files_path)\n\n    print(""model files path: %s"" % model_path)\n    print(""log files path: %s"" % log_path)\n    print(""batch size: %d, total epoch: %d, class number: %d, initial learning rate: %f, decay method: %s, decay rate: %f, decay steps: %d""\n          % (batch_size, n_epoch, classes_num, init_learning_rate, decay_method, decay_rate, decay_steps))\n    print(""start load network..."")\n\n    with tf.device(""/cpu:0""):\n        global_step = tf.Variable(initial_value=0,\n                                  trainable=False,\n                                  name=""global_step"",\n                                  dtype=tf.int32)                                           # global step (Variable \xe5\x8f\x98\xe9\x87\x8f\xe4\xb8\x8d\xe8\x83\xbd\xe7\x9b\xb4\xe6\x8e\xa5\xe5\x88\x86\xe9\x85\x8dGPU\xe8\xb5\x84\xe6\xba\x90)\n\n    # learning rate decay\n    learning_rate = learning_rate_decay(init_learning_rate=init_learning_rate,\n                                        decay_method=decay_method,\n                                        global_step=global_step,\n                                        decay_steps=decay_steps,\n                                        decay_rate=decay_rate)                              # learning rate\n\n    # placeholder\n    with tf.variable_scope(""placeholder""):\n        data = tf.placeholder(dtype=tf.float32, shape=(None, height, width), name=""data"") if channel == 0 else \\\n            tf.placeholder(dtype=tf.float32, shape=(None, height, width, channel), name=""data"")\n        labels = tf.placeholder(dtype=tf.int32, shape=(None, ), name=""labels"")\n        is_bn = tf.placeholder(dtype=tf.bool, name=""is_bn"")\n\n        # placeholder for siamese network\n        data_siamese = tf.placeholder(dtype=tf.float32, shape=(None, height, width), name=""data_siamese"") if channel == 0 else \\\n            tf.placeholder(dtype=tf.float32, shape=(None, height, width, channel), name=""data_siamese"")\n        labels_siamese = tf.placeholder(dtype=tf.int32, shape=(None,), name=""labels_siamese"")\n        new_labels = tf.placeholder(dtype=tf.float32, shape=(None, ), name=""new_labels"")\n\n    # initialize the network\n    if args.network not in networks:\n        print(""Network miss-match, please try again."")\n        return False\n\n    if args.siamese is not True:\n        try:\n            command = args.network + ""(data, classes_num, is_bn)""\n            logits = eval(command)\n        except TypeError:\n            command = args.network + ""(data, classes_num)""\n            logits = eval(command)\n\n        # evaluation\n        loss_train = loss_layer(logits=logits, labels=labels, is_regulation=is_regulation, coeff=coeff_regulation, method=loss_method)\n        accuracy_train = accuracy_layer(logits=logits, labels=labels)\n        accuracy_validation, false_positive_rate_validation, false_negative_rate_validation = evaluation(logits=logits, labels=labels)\n        train_optimizer = optimizer(losses=loss_train, learning_rate=learning_rate, global_step=global_step)\n        loss_validation = loss_layer(logits=logits, labels=labels, is_regulation=is_regulation, coeff=coeff_regulation, method=loss_method)\n        # accuracy_check = accuracy_layer(logits=logits, labels=labels)\n\n    else:\n        with tf.variable_scope(\'siamese\') as scope:\n            command1 = args.network + ""(data, classes_num, is_bn)""\n            command2 = args.network + ""(data_siamese, classes_num, is_bn)""\n            logits = eval(command1)\n            scope.reuse_variables()\n            logits_siamese = eval(command2)\n\n        # evaluation\n        loss_train = loss_layer(logits=logits, logits_siamese=logits_siamese, labels=new_labels, is_regulation=is_regulation, coeff=coeff_regulation, method=""siamese_loss"")\n        accuracy1 = evaluation(logits=logits, labels=labels)\n        accuracy2 = accuracy_layer(logits=logits_siamese, labels=labels_siamese)\n        accuracy_train = tf.multiply(tf.add(accuracy1, accuracy2), 0.5)\n        train_optimizer = optimizer(losses=loss_train, learning_rate=learning_rate, global_step=global_step)\n        loss_validation = loss_layer(logits=logits, labels=labels, is_regulation=is_regulation, coeff=coeff_regulation, method=loss_method)\n        accuracy_validation, false_positive_rate_validation, false_negative_rate_validation = evaluation(logits=logits, labels=labels)\n\n    with tf.device(""/cpu:0""):\n        tf.summary.scalar(""loss_train"", loss_train)\n        tf.summary.scalar(""loss_validation"", loss_validation)\n        tf.summary.scalar(""accuracy_train"", accuracy_train)\n        tf.summary.scalar(""accuracy_validation"", accuracy_validation)\n        summary_op = tf.summary.merge_all()\n        saver = tf.train.Saver(max_to_keep=max_to_keep,\n                               keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)\n\n    # initialize\n    try:\n        config = tf.ConfigProto()\n        config.allow_soft_placement = True\n        config.gpu_options.allow_growth = True\n\n        with tf.Session(config=config) as sess:\n            train_writer_train = tf.summary.FileWriter(log_path + ""/train"", tf.get_default_graph())\n            train_writer_valid = tf.summary.FileWriter(log_path + ""/validation"", tf.get_default_graph())\n            init = tf.global_variables_initializer()\n            sess.run(init)\n\n            # restore the model and keep training from the current breakpoint\n            if checkpoint is True:\n                if args.fine_tune_model_file_path is None:\n                    model_file_path = get_model_file_path(model_path)\n                    if model_file_path is not None:\n                        saver.restore(sess, model_file_path)\n                    else:\n                        model_file_path = fullfile(model_path, task_name)\n                        print(""No such model file, training starts from the beginning."")\n                else:\n                    if not os.path.exists(args.fine_tune_model_file_path + "".index""):\n                        model_file_path = fullfile(model_path, task_name)\n                        print(""No such model file, training starts from the beginning."")\n                    else:\n                        saver.restore(sess, args.fine_tune_model_file_path)\n                        model_file_path = fullfile(model_path, task_name)\n                        print(""Fine-Tune on the model %s"" % args.fine_tune_model_file_path)\n            else:\n                model_file_path = fullfile(model_path, task_name)\n                print(""Training starts from the beginning."")\n\n            print(""Start training..."")\n            print(""Numbers of network %s: %d"" % (args.network, get_variables_number(tf.trainable_variables())))\n            print(""Input data: (%d, %d)"" % (height, width)) if channel == 0 else print(""Input data: (%d, %d, %d)"" % (height, width, channel))\n\n            start_time_all = time.time()\n            for epoch in range(n_epoch):\n\n                # read data\n                if cover_train_path is not None:\n                    # read files list (train)\n                    cover_train_data_list, cover_train_label_list, stego_train_data_list, stego_train_label_list \\\n                        = read_data(cover_train_path, stego_train_path, start_idx=start_index_train, end_idx=end_index_train, file_type=file_type)\n\n                    # read files list (validation)\n                    cover_valid_data_list, cover_valid_label_list, stego_valid_data_list, stego_valid_label_list \\\n                        = read_data(cover_valid_path, stego_valid_path, start_idx=start_index_valid, end_idx=end_index_valid, file_type=file_type)\n                else:\n                    # read files list (train)\n                    cover_train_data_list, cover_train_label_list, stego_train_data_list, stego_train_label_list \\\n                        = read_data(cover_files_path, stego_files_path, start_idx=start_index_train, end_idx=end_index_train, file_type=file_type)\n\n                    # read files list (validation)\n                    cover_valid_data_list, cover_valid_label_list, stego_valid_data_list, stego_valid_label_list \\\n                        = read_data(cover_files_path, stego_files_path, start_idx=start_index_valid, end_idx=end_index_valid, file_type=file_type)\n\n                # update the learning rate\n                lr = sess.run(learning_rate)\n\n                # train\n                train_iterations, train_loss, train_accuracy = 0, 0, 0\n                for x_train_batch, y_train_batch in \\\n                        minibatches(cover_train_data_list, cover_train_label_list, stego_train_data_list, stego_train_label_list, batch_size):\n                    # data read and process\n                    x_train_data = get_data_batch(x_train_batch, height=height, width=width, channel=channel, carrier=carrier)\n\n                    if args.siamese is not True:\n                        # get the accuracy and loss\n                        _, err, ac, summary_str_train = sess.run([train_optimizer, loss_train, accuracy_train, summary_op],\n                                                                 feed_dict={data: x_train_data, labels: y_train_batch, is_bn: True})\n                    else:\n                        x_train_data1 = x_train_data[:int(batch_size / 2)]\n                        x_train_data2 = x_train_data[int(batch_size / 2):]\n                        y_train_batch1 = y_train_batch[:int(batch_size / 2)]\n                        y_train_batch2 = y_train_batch[int(batch_size / 2):]\n\n                        new_label = list(np.array(np.array(np.array(y_train_batch1) == np.array(y_train_batch2), dtype=np.float32), dtype=np.str))\n\n                        _, err, ac, summary_str_train = sess.run([train_optimizer, loss_train, accuracy_train, summary_op],\n                                                                 feed_dict={data: x_train_data1, data_siamese: x_train_data2,\n                                                                            labels: y_train_batch1, labels_siamese: y_train_batch2,\n                                                                            new_labels: new_label, is_bn: True})\n\n                    train_loss += err\n                    train_accuracy += ac\n                    step_train += 1\n                    train_iterations += 1\n                    train_writer_train.add_summary(summary_str_train, global_step=step_train)\n\n                    et = time.time() - start_time_all\n                    et = str(datetime.timedelta(seconds=et))[:-7]\n                    print(""[network: %s, task: %s, global_step: %000005d] elapsed: %s, epoch: %003d, train iterations: %003d: train loss: %f, train accuracy: %f""\n                          % (args.network, args.task_name, step_train, et, epoch + 1, train_iterations, err, ac))\n\n                print(""="" * 150)\n\n                # validation\n                valid_iterations, valid_loss, valid_accuracy, valid_fpr, valid_fnr = 0, 0, 0, 0, 0\n                for x_valid_batch, y_valid_batch in \\\n                        minibatches(cover_valid_data_list, cover_valid_label_list, stego_valid_data_list, stego_valid_label_list, batch_size):\n                    # data read and process\n                    x_valid_data = get_data_batch(x_valid_batch, height=height, width=width, channel=channel, carrier=carrier)\n\n                    # get the accuracy and loss\n                    new_label = np.zeros(batch_size, dtype=np.float32)\n                    err, ac, fpr, fnr, summary_str_valid = sess.run([loss_validation, accuracy_validation,\n                                                                     false_negative_rate_validation, false_positive_rate_validation, summary_op],\n                                                                    feed_dict={data: x_valid_data, data_siamese: x_valid_data,\n                                                                               labels: y_valid_batch, labels_siamese: y_valid_batch,\n                                                                               new_labels: new_label, is_bn: True})\n                    valid_loss += err\n                    valid_accuracy += ac\n                    valid_fpr += fpr\n                    valid_fnr += fnr\n                    valid_iterations += 1\n                    step_valid += 1\n                    train_writer_valid.add_summary(summary_str_valid, global_step=step_valid)\n\n                    et = time.time() - start_time_all\n                    et = str(datetime.timedelta(seconds=et))[:-7]\n                    print(""[network: %s, task: %s] elapsed: %s, epoch: %003d, valid iterations: %003d, valid loss: %-8f, ""\n                          ""valid accuracy: %f, valid fpr: %f, valid fnr: %f""\n                          % (args.network, args.task_name, et, epoch + 1, valid_iterations, err, ac, fpr, fnr))\n\n                # calculate the average in a batch\n                train_loss_average = train_loss / train_iterations\n                valid_loss_average = valid_loss / valid_iterations\n                train_accuracy_average = train_accuracy / train_iterations\n                valid_accuracy_average = valid_accuracy / valid_iterations\n                valid_fpr_average = valid_fpr / valid_iterations\n                valid_fnr_average = valid_fnr / valid_iterations\n\n                # training stop and model save\n                if valid_accuracy_average > max_accuracy:\n                    max_accuracy_epoch = epoch + 1\n                    saver.save(sess, model_file_path, global_step=global_step)\n                    print(""The model is saved successfully."")\n                    max_accuracy, corresponding_fpr, corresponding_fnr = valid_accuracy_average, valid_fpr_average, valid_fnr_average\n\n                print(""[network: %s, task: %s] epoch: %003d, learning rate: %f, train loss: %f, train accuracy: %f, valid loss: %f, ""\n                      ""valid accuracy: %f, valid FPR: %f, valid FNR: %f, ""\n                      ""max valid accuracy: %f -- FPR: %f, FNR: %f, ""\n                      ""max valid acc epoch: %d""\n                      % (args.network, args.task_name, epoch + 1, lr, train_loss_average, train_accuracy_average,\n                         valid_loss_average, valid_accuracy_average, valid_fpr_average, valid_fnr_average,\n                         max_accuracy, corresponding_fpr, corresponding_fnr, max_accuracy_epoch))\n                print(""Current Time:"", time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(time.time())))\n\n                # training stop\n                if (args.indicator == ""validation_accuracy_value"" and max_accuracy > args.validation_accuracy_expectation / 100) or\\\n                        (args.indicator == ""epoch_interval"" and max_accuracy_epoch - epoch > args.epoch_interval and\n                         max_accuracy >= args.validation_accuracy_least / 100):\n                    return\n\n            train_writer_train.close()\n            train_writer_valid.close()\n\n    except Exception as e:\n        print(""An error occurred, please try again."")\n        print(""Error Type: "", e)\n        print(traceback.print_exc())\n        os.system(""rm -rf "" + model_path)\n        os.system(""rm -rf "" + log_path)\n\n\ndef test(args):\n    # hyper parameters\n    batch_size = args.batch_size                                        # batch size\n    height, width, channel = args.height, args.width, args.channel      # height and width of input matrix\n    carrier = args.carrier                                              # carrier (qmdct | audio | image)\n    classes_num = args.class_num                                        # classes number\n    start_index_test, end_index_test = args.start_index_test, args.end_index_test\n    is_shuffle = args.is_shuffle                                        # whether the files list shuffle or not\n\n    # path\n    cover_test_files_path = args.cover_test_path\n    stego_test_files_path = args.stego_test_path\n\n    # placeholder\n    data = tf.placeholder(dtype=tf.float32, shape=(batch_size, height, width, channel), name=""data"")\n    labels = tf.placeholder(dtype=tf.int32, shape=(batch_size, ), name=""label"")\n    is_bn = tf.placeholder(dtype=tf.bool, name=""is_bn"")\n\n    # initialize the network\n    if args.network not in networks:\n        print(""Network miss-match, please try again"")\n        return False\n\n    command = args.network + ""(data, classes_num, is_bn)""\n    logits = eval(command)\n    output = tf.nn.softmax(logits)\n\n    accuracy, false_positive_rate, false_negative_rate = evaluation(logits=logits, labels=labels)\n\n    # information output\n    print(""cover files path: %s"" % cover_test_files_path)\n    print(""stego files path: %s"" % stego_test_files_path)\n    print(""class number: %d"" % classes_num)\n    print(""Start load network..."")\n\n    config = tf.ConfigProto()\n    config.allow_soft_placement = True\n    config.gpu_options.allow_growth = True\n\n    start_time = time.time()\n\n    model = tf.train.Saver()\n    with tf.Session() as sess:\n        # load model\n        sess.run(tf.global_variables_initializer())\n        model_file_path = get_model_file_path(args.models_path)\n\n        if model_file_path is None:\n            print(""No model is loaded successfully."")\n        else:\n            model.restore(sess, model_file_path)\n            print(""The model is loaded successfully, model file: %s"" % model_file_path)\n            # read files list (train)\n            cover_test_data_list, cover_test_label_list, \\\n                stego_test_data_list, stego_test_label_list = read_data(cover_test_files_path, stego_test_files_path, start_index_test, end_index_test, is_shuffle=is_shuffle)\n\n            if len(cover_test_data_list) < batch_size:\n                batch_size = len(cover_test_data_list)\n\n            csv_file = open(\'/home1/wyt/code/tf_audio_steganalysis/logit1.csv\', \'w\', encoding=\'utf-8\', newline=\'\\n\')\n            csvwriter = csv.writer(datacsv, delimiter=\',\')\n            test_iterations, test_accuracy, test_fpr, test_fnr = 0, 0, 0, 0\n            for x_test_batch, y_test_batch in \\\n                    minibatches(cover_test_data_list, cover_test_label_list, stego_test_data_list, stego_test_label_list, batch_size):\n                # data read and process\n                x_test_data = get_data_batch(x_test_batch, height=height, width=width, channel=channel, carrier=carrier)\n\n                # get the accuracy and loss\n                result, acc, fpr, fnr = sess.run([output, accuracy, false_positive_rate, false_negative_rate],\n                                                 feed_dict={data: x_test_data, labels: y_test_batch, is_bn: True})\n                for content in result:\n                    csvwriter.writerow(str([content[0], content[1]]))\n\n                test_accuracy += acc\n                test_fpr += fpr\n                test_fnr += fnr\n                test_iterations += 1\n\n                # print(""Batch-%003d, accuracy: %f, fpr: %f, fnr: %f"" % (test_iterations, acc, fpr, fnr))\n\n            test_accuracy_average = test_accuracy / test_iterations\n            test_fpr_average = test_fpr / test_iterations\n            test_fnr_average = test_fnr / test_iterations\n            print(""Test accuracy: %.2f%%, FPR: %.2f%%, FNR: %.2f%%"" % (100. * test_accuracy_average, 100. * test_fpr_average, 100. * test_fnr_average))\n            csv_file.close()\n\n    et = time.time() - start_time\n    et = str(datetime.timedelta(seconds=et))[:-7]\n    print(""Run Time: %s"" % et)\n\n\ndef steganalysis_one(args):\n    # hyper parameters\n    height, width, channel = args.height, args.width, args.channel      # height and width of input matrix\n    carrier = args.carrier                                              # carrier (qmdct | audio | image)\n\n    # path\n    steganalysis_file_path = args.steganalysis_file_path\n\n    # placeholder\n    data = tf.placeholder(dtype=tf.float32, shape=(1, height, width, channel), name=""data"")\n    is_bn = tf.placeholder(dtype=tf.bool, name=""is_bn"")\n\n    # initialize the network\n    if args.network not in networks:\n        print(""Network miss-match, please try again"")\n        return False\n\n    # network\n    command = args.network + ""(data, args.class_num, is_bn)""\n    logits = eval(command)\n    logits = tf.nn.softmax(logits)\n\n    model = tf.train.Saver()\n    with tf.Session() as sess:\n        # load model\n        sess.run(tf.global_variables_initializer())\n        model_file_path = get_model_file_path(args.model_path)\n        print(model_file_path)\n        if model_file_path is None:\n            print(""No model is loaded successfully."")\n        else:\n            model.restore(sess, model_file_path)\n            print(""The model is loaded successfully, model file: %s"" % model_file_path)\n\n            steganalysis_data = get_data_batch([steganalysis_file_path], width=width, height=height, channel=channel, carrier=carrier)\n\n            if steganalysis_data is None:\n                print(""No model can be used for this carrier. (need image or audio)"")\n            else:\n                file_name = get_file_name(steganalysis_file_path)\n                ret = sess.run(logits, feed_dict={data: steganalysis_data, is_bn: False})\n                result = np.argmax(ret, 1)\n                prob = 100 * ret[0][result]\n\n                if result[0] == 0:\n                    print(""file name: %s, result: cover, prob of prediction: %.2f%%"" % (file_name, prob))\n\n                if result[0] == 1:\n                    print(""file name: %s, result: stego, prob of prediction: %.2f%%"" % (file_name, prob))\n\n\ndef steganalysis_batch(args):\n    # hyper parameters\n    height, width, channel = args.height, args.width, args.channel      # height and width of input matrix\n    carrier = args.carrier                                              # carrier (qmdct | audio | image)\n\n    # files index\n    start_index_steganalysis = args.start_index_steganalysis\n    end_index_steganalysis = args.end_index_steganalysis\n\n    # path\n    steganalysis_files_path = args.steganalysis_files_path\n\n    # placeholder\n    data = tf.placeholder(dtype=tf.float32, shape=(None, height, width, channel), name=""data"")\n    is_bn = tf.placeholder(dtype=tf.bool, name=""is_bn"")\n\n    # initialize the network\n    if args.network not in networks:\n        print(""Network miss-match, please try again"")\n        return False\n\n    # network\n    command = args.network + ""(data, args.class_num, is_bn)""\n    logits = eval(command)\n    logits = tf.nn.softmax(logits)\n\n    start_time = time.time()\n\n    model = tf.train.Saver()\n    with tf.Session() as sess:\n        # load model\n        sess.run(tf.global_variables_initializer())\n        model_file_path = get_model_file_path(args.models_path)\n\n        if model_file_path is None:\n            print(""No model is loaded successfully."")\n        else:\n            model.restore(sess, model_file_path)\n            print(""The model is loaded successfully, model file: %s"" % model_file_path)\n\n            file_list = get_files_list(steganalysis_files_path)\n            file_list = file_list[start_index_steganalysis:end_index_steganalysis]\n            print(""files path: %s"" % steganalysis_files_path)\n            count_cover, count_stego = 0, 0\n\n            for file_path in file_list:\n                steganalysis_data = get_data_batch([file_path], width=width, height=height, channel=channel, carrier=carrier)\n\n                file_name = get_file_name(file_path)\n                ret = sess.run(logits, feed_dict={data: steganalysis_data, is_bn: False})\n                sess.run(tf.local_variables_initializer())\n                print(ret)\n                result = np.argmax(ret, 1)\n                prob = 100 * ret[0][result]\n\n                if result[0] == 0:\n                    print(""file name: %s, result: cover, prob of prediction: %.2f%%"" % (file_name, prob))\n                    count_cover += 1\n\n                if result[0] == 1:\n                    print(""file name: %s, result: stego, prob of prediction: %.2f%%"" % (file_name, prob))\n                    count_stego += 1\n\n            print(""Number of cover samples: %d"" % count_cover)\n            print(""Number of stego samples: %d"" % count_stego)\n\n    et = time.time() - start_time\n    et = str(datetime.timedelta(seconds=et))[:-7]\n    print(""Run Time: %s"" % et)\n'"
src/text_preprocess.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport os\nimport numpy as np\n\n""""""\nCreated on 2017.11.27\nFinished on 2017.11.27\nModified on 2018.08.23\n\n@author: Yuntao Wang\n""""""\n\n""""""\n    function:\n        text_read(text_file_path, height=200, width=576, channel=1, separator="","")           read single QMDCT coefficients matrix into memory\n        text_read_batch(text_files_list, height=200, width=576, channel=1, separator="","")    read QMDCT coefficients matrix in batch\n""""""\n\n\ndef text_read(text_file_path, height=200, width=576, channel=1, separator="",""):\n    """"""\n    data read from one text file\n\n    :param text_file_path: the file path\n    :param height: the height of QMDCT matrix\n    :param width: the width of QMDCT matrix\n    :param channel: the channel of QMDCT matrix\n    :param separator: separator of each elements in text file\n\n    :return\n        content: QMDCT matrix  ndarray, shape: [height, width, 1]\n    """"""\n    content = []\n    try:\n        with open(text_file_path) as file:\n            # read data line by line\n            lines = file.readlines()\n            for line in lines:\n                try:\n                    numbers = [int(character) for character in line.split(separator)[:-1]]\n                except ValueError:\n                    numbers = [float(character) for character in line.split(separator)[:-1]]\n                content.append(numbers)\n\n            content = np.array(content)\n\n            # reshape\n            [h, w] = np.shape(content)\n\n            height_new = None if h < height else height\n            width_new = None if w < width else width\n\n            if channel == 0:\n                content = content[:height_new, :width_new]\n            else:\n                content = np.reshape(content, [h, w, channel])\n                content = content[:height_new, :width_new, :channel]\n\n    except ValueError:\n        print(""Error read: %s"" % text_file_path)\n\n    return content\n\n\ndef text_read_batch(text_files_list, height=200, width=576, channel=1, separator="",""):\n    """"""\n    read all txt files into the memory\n\n    :param text_files_list: text files list\n    :param height: the height of QMDCT matrix\n    :param width: the width of QMDCT matrix\n    :param channel: the channel of QMDCT matrix\n    :param separator: separator of each elements in text file\n\n    :return:\n        data: QMDCT matrixs, ndarry, shape: [files_num, height, width, 1]\n    """"""\n\n    files_num = len(text_files_list)\n    data = np.zeros([files_num, height, width], dtype=np.float32) if channel == 0 else np.zeros([files_num, height, width, channel], dtype=np.float32)\n\n    i = 0\n    for text_file_path in text_files_list:\n        content = text_read(text_file_path, height=height, width=width, channel=channel, separator=separator)\n        data[i] = content\n        i = i + 1\n\n    return data\n'"
src/utils.py,17,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2017.11.20\nFinished on 2017.11.20\nModified on 2018.08.29\n\n@author: Yuntao Wang\n""""""\n\nimport os\nimport time\nimport numpy as np\nfrom glob import glob\nimport tensorflow as tf\nfrom text_preprocess import *\nfrom image_preprocess import *\nfrom audio_preprocess import *\nfrom file_preprocess import get_path_type\nfrom tensorflow.python import pywrap_tensorflow\n\n\ndef folder_make(path):\n    """"""\n    create a folder\n    :param path: the path to be created\n    :return:\n    """"""\n    if not os.path.exists(path) and not os.path.isfile(path):\n        os.mkdir(path)\n    else:\n        pass\n\n\ndef fullfile(file_dir, file_name):\n    """"""\n    fullfile as matlab\n    :param file_dir: file dir\n    :param file_name: file name\n    :return:\n        full_file_path: the full file path\n    """"""\n    full_file_path = os.path.join(file_dir, file_name)\n    full_file_path = full_file_path.replace(""\\\\"", ""/"")\n\n    return full_file_path\n\n\ndef get_files_list(file_dir, file_type=""txt"", start_idx=None, end_idx=None):\n    """"""\n    get the files list\n    :param file_dir: file directory\n    :param file_type: type of files, ""*"" is to get all files in this folder\n    :param start_idx: start index\n    :param end_idx: end index\n    :return:\n        file_list: a list containing full file path\n    """"""\n    pattern = ""/*."" + file_type\n    file_list = sorted(glob(file_dir + pattern))\n    total_num = len(file_list)\n    if type(start_idx) is int and start_idx > total_num:\n        start_idx = None\n    if type(end_idx) is int and end_idx > total_num:\n        end_idx = None\n    file_list = file_list[start_idx:end_idx]\n\n    return file_list\n\n\ndef get_time(unix_time_stamp=None):\n    """"""\n    unix time stamp -> time in ""%Y-%m-%d %H:%M:%S"" format\n    e.g. 1522048036 -> 2018-03-26 15:07:16\n    :param unix_time_stamp: unix time stamp\n    :return:\n        time_string: time in ""%Y-%m-%d %H:%M:%S"" format\n    """"""\n    time_format = ""%Y-%m-%d %H:%M:%S""\n    if unix_time_stamp is None:\n        value = time.localtime()\n    else:\n        value = time.localtime(unix_time_stamp)\n    time_string = time.strftime(time_format, value)\n\n    return time_string\n\n\ndef get_unix_stamp(time_string=""1970-01-01 08:01:51"", time_format=""%Y-%m-%d %H:%M:%S""):\n    """"""\n    time expression with ""%Y-%m-%d %H:%M:%S"" format -> unix time stamp\n    :param time_string: time expression with ""%Y-%m-%d %H:%M:%S"" format\n    :param time_format: time format to be exchanged\n    :return:\n        unix time stamp: unix time stamp\n    """"""\n    unix_time_stamp = time.mktime(time.strptime(time_string, time_format))\n\n    return int(unix_time_stamp)\n\n\ndef read_data(cover_files_path, stego_files_path, start_idx=None, end_idx=None, file_type=""txt"", is_shuffle=True):\n    """"""\n    read file names from the storage\n    :param cover_files_path: the folder name of cover files\n    :param stego_files_path: the folder name of stego files\n    :param file_type: file type, default is ""txt""\n    :param start_idx: the start index, default is None\n    :param end_idx: the end index, default is None\n    :param is_shuffle: whether shuffle or not (default is True)\n    :return:\n        cover_data_list: list of cover data\n        cover_label_list: list of cover label\n        stego_data_list: list of stego data\n        stego_label_list: list of stego label\n    """"""\n    cover_files_list = get_files_list(file_dir=cover_files_path, file_type=file_type,\n                                      start_idx=start_idx, end_idx=end_idx)         # data list of cover files\n    stego_files_list = get_files_list(file_dir=stego_files_path, file_type=file_type,\n                                      start_idx=start_idx, end_idx=end_idx)         # data list of stego files\n    sample_num_cover = len(cover_files_list)                                        # total pairs of samples (cover)\n    sample_num_stego = len(stego_files_list)                                        # total pairs of samples (stego)\n    sample_num = min(sample_num_cover, sample_num_stego)                            # deal with the quantity inequality of cover and stego\n\n    cover_files_list = cover_files_list[:sample_num]                                # data list of cover files\n    stego_files_list = stego_files_list[:sample_num]                                # data list of stego files\n    cover_label_list = np.zeros(sample_num, np.int32)                               # label list of cover files\n    stego_label_list = np.ones(sample_num, np.int32)                                # label list of stego files\n\n    temp = np.array([cover_files_list, cover_label_list, stego_files_list, stego_label_list])\n    temp_t = temp.transpose()\n\n    if is_shuffle is True:\n        np.random.shuffle(temp_t)\n\n    cover_data_list = list(temp_t[:, 0])\n    cover_label_list = list(temp_t[:, 1])\n\n    stego_data_list = list(temp_t[:, 2])\n    stego_label_list = list(temp_t[:, 3])\n\n    return cover_data_list, cover_label_list, stego_data_list, stego_label_list\n\n\ndef minibatches(cover_datas=None, cover_labels=None, stego_datas=None, stego_labels=None, batchsize=None):\n    """"""\n    read data batch by batch\n    :param cover_datas: file name list (cover)\n    :param cover_labels: label list(cover)\n    :param stego_datas: file name list (stego)\n    :param stego_labels: label list (stego)\n    :param batchsize: batch size\n    :return:\n        yield datas and labels\n    """"""\n    for start_idx in range(0, len(cover_datas) - batchsize // 2 + 1, batchsize // 2):\n        excerpt = slice(start_idx, start_idx + batchsize // 2)\n        datas = cover_datas[excerpt]\n        datas.extend(stego_datas[excerpt])\n        labels = cover_labels[excerpt]\n        labels.extend(stego_labels[excerpt])\n\n        temp = np.array([datas, labels])\n        temp_t = temp.transpose()\n\n        np.random.shuffle(temp_t)\n\n        datas = list(temp_t[:, 0])\n        labels = list(temp_t[:, 1])\n\n        yield datas, labels\n\n\ndef get_data(file_path, height, width, channel, carrier=""qmdct""):\n    """"""\n    read data batch by batch\n    :param file_path: path of file\n    :param height: the height of the data matrix\n    :param width: the width of the data matrix\n    :param channel: the channel of the data matrix\n    :param carrier: the type of carrier (qmdct | audio | image)\n    :return:\n    """"""\n\n    if carrier == ""audio"":\n        data = audio_read(audio_file_path=file_path)\n    elif carrier == ""mfcc"":\n        data = get_mfcc(audio_file_path=file_path)\n    elif carrier == ""image"":\n        data = image_read(image_file_path=file_path, height=height, width=width, channel=channel)\n    else:\n        data = text_read(text_file_path=file_path, height=height, width=width, channel=channel)\n\n    return data\n\n\ndef get_data_batch(files_list, height, width, channel, carrier=""qmdct""):\n    """"""\n    read data batch by batch\n    :param files_list: files list (audio | image | text)\n    :param height: the height of the data matrix\n    :param width: the width of the data matrix\n    :param channel: the channel of the data matrix\n    :param carrier: the type of carrier (qmdct | audio | image)\n    :return:\n    """"""\n    if carrier == ""audio"":\n        data = audio_read_batch(audio_files_list=files_list)\n    elif carrier == ""mfcc"":\n        data = get_mfcc_batch(audio_files_list=files_list)\n    elif carrier == ""image"":\n        data = image_read_batch(image_files_list=files_list, height=height, width=width, channel=channel)\n    else:\n        data = text_read_batch(text_files_list=files_list, height=height, width=width, channel=channel)\n\n    return data\n\n\ndef evaluation(logits, labels):\n    """"""\n    calculate the false positive rate, false negative rate, accuracy rate, precision rate and recall rate\n    :param logits: prediction\n    :param labels: label\n    :return:\n        false_positive_rate: false positive rate\n        false_negative_rate: false negative rate\n        accuracy_rate: accuracy rate\n        precision_rate: precision rate\n        recall_rate: recall rate\n    """"""\n    # format exchange\n    if isinstance(logits, list):\n        logits = np.array(logits)\n        labels = np.array(labels)\n\n    if isinstance(logits, type(tf.constant([0]))):\n        sess = tf.Session()\n        logits = logits.eval(session=sess)\n        labels = labels.eval(session=sess)\n\n    # calculate\n    correct_false_num, correct_true_num, false_positive_num, false_negative_num = 0, 0, 0, 0\n    for logit, label in zip(logits, labels):\n        if logit == 0 and label == 0:\n            correct_false_num += 1\n        if logit == 1 and label == 0:\n            false_positive_num += 1\n        if logit == 0 and label == 1:\n            false_negative_num += 1\n        if logit == 1 and label == 1:\n            correct_true_num += 1\n\n    false_positive_rate = false_positive_num / str(labels.tolist()).count(""1"")\n    false_negative_rate = false_negative_num / str(labels.tolist()).count(""0"")\n    accuracy_rate = 1 - (false_positive_rate + false_negative_rate) / 2\n    precision_rate = correct_true_num / str(logits.tolist()).count(""1"")\n    recall_rate = correct_true_num / str(labels.tolist()).count(""1"")\n\n    return false_positive_rate, false_negative_rate, accuracy_rate, precision_rate, recall_rate\n\n\ndef qmdct_extractor(mp3_file_path, width=576, frame_num=50, coeff_num=576):\n    """"""\n    qmdct coefficients extraction\n    :param mp3_file_path: mp3 file path\n    :param width: the width of QMDCT coefficients matrix, default: 576\n    :param frame_num: the frame num of QMDCT coefficients extraction, default: 50\n    :param coeff_num: the num of coefficients in a channel\n    :return:\n        QMDCT coefficients matrix, size: (4 * frame_num) * coeff_num -> 200 * 576\n    """"""\n    wav_file_path = mp3_file_path.replace("".mp3"", "".wav"")\n    txt_file_path = mp3_file_path.replace("".mp3"", "".txt"")\n\n    command = ""lame_qmdct.exe "" + mp3_file_path + "" -framenum "" + str(frame_num) + "" -startind 0 "" + "" -coeffnum "" + str(coeff_num) + "" --decode""\n    os.system(command)\n    os.remove(wav_file_path)\n\n    height = frame_num * 4\n    content = text_read(text_file_path=txt_file_path, height=height, width=width)\n\n    os.remove(txt_file_path)\n\n    return content\n\n\ndef get_model_file_path(path):\n    """"""\n    get the path of trained tensorflow model\n    :param path: input path, file path or folder path\n    :return:\n        the path of trained tensorflow model\n    """"""\n    if get_path_type(path + "".meta"") == ""file"":\n        return path.split(""."")[0]\n    elif get_path_type(path) == ""folder"":\n        return tf.train.latest_checkpoint(path)\n    else:\n        return None\n\n\ndef get_sub_directory(directory_path):\n    """"""\n    get subdirectory in a directory\n    :param directory_path: directory path to be retrieved\n    :return:\n        sub_directory_list\n    """"""\n    sub_directory_list = []\n    file_path_list = os.listdir(directory_path)\n    file_path_list.sort()\n    for file_path in file_path_list:\n        full_path = fullfile(directory_path, file_path)\n        if os.path.isdir(full_path):\n            sub_directory_list.append(full_path)\n\n    return sub_directory_list\n\n\ndef write_and_encode(files_path_list, tf_record_file_path=""train.tfrecords"", files_type=""txt"",\n                     carrier=""qmdct"", data_height=200, data_width=576, data_channel=1, start_idx=None, end_idx=None):\n    """"""\n    write the data into tfrecord file\n    :param files_path_list: a files list, e.g. cover and stego\n    :param tf_record_file_path: path of tfrecord file, default: ./train.tfrecords\n    :param files_type: type of data file, default: ""txt""\n    :param carrier: type of carrier (qmdct, image, audio), default: qmdct\n    :param data_height: height of input data matrix, default: 200\n    :param data_width: width of input data matrix, default: 576\n    :param data_channel: channel of input data matrix, default: 1\n    :param start_idx: start index of files list, default: None\n    :param end_idx: end index of files list, default: None\n    :return:\n        NULL\n    """"""\n\n    if os.path.exists(tf_record_file_path):\n        pass\n    else:\n        writer = tf.python_io.TFRecordWriter(tf_record_file_path)\n\n        for index, files_path in enumerate(files_path_list):\n            files_list = get_files_list(files_path, files_type, start_idx, end_idx)\n            files_list = files_list[start_idx:end_idx]\n\n            for file in files_list:\n                data = get_data(file, height=data_height, width=data_width, channel=data_channel, carrier=carrier)\n                data_raw = data.tobytes()\n                content = tf.train.Example(features=tf.train.Features(feature={\n                    ""label"": tf.train.Feature(int64_list=tf.train.Int64List(value=[index])),\n                    ""data_raw"": tf.train.Feature(bytes_list=tf.train.BytesList(value=[data_raw]))\n                }))\n                writer.write(content.SerializeToString())\n        writer.close()\n\n\ndef read_and_decode(filename, n_epoch=3, is_shuffle=True, data_height=200, data_width=576, data_channel=1):\n    reader = tf.TFRecordReader()\n    filename_queue = tf.train.string_input_producer([filename], shuffle=is_shuffle, num_epochs=n_epoch)\n    _, serialized_example = reader.read(filename_queue)\n\n    features = tf.parse_single_example(\n        serialized_example,\n        features={\'label\': tf.FixedLenFeature([], tf.int64),\n                  \'data_raw\': tf.FixedLenFeature([], tf.string)\n                  })\n\n    data = tf.decode_raw(features[\'data_raw\'], tf.float32)\n    data = tf.reshape(data, [data_height, data_width, data_channel])\n    labels = tf.cast(features[\'label\'], tf.int32)\n\n    return data, labels\n\n\ndef create_batch(filename, batch_size):\n    data, label = read_and_decode(filename)\n\n    min_after_dequeue = 10 * batch_size\n    capacity = 2 * min_after_dequeue\n    data_batch, label_batch = tf.train.shuffle_batch([data, label],\n                                                     batch_size=batchsize,\n                                                     capacity=capacity,\n                                                     num_threads=8,\n                                                     seed=1,\n                                                     min_after_dequeue=min_after_dequeue\n                                                     )\n\n    return data_batch, label_batch\n\n\ndef get_variables_number(trainable_variables):\n    """"""\n    calculate the number of trainable variables in the current network\n    :param trainable_variables: trainable variables\n    :return:\n        total_parameters: the total number of trainable variables\n    """"""\n    total_parameters = 0\n    for variable in trainable_variables:\n        # shape is an array of tf.Dimension\n        shapes = variable.get_shape()\n        variable_parameters = 1\n        for shape in shapes:\n            variable_parameters *= shape.value\n        total_parameters += variable_parameters\n\n    return total_parameters\n'"
data_processing/python_scripts/QMDCT_extraction.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2018.01.09\nFinished on 2018.01.09\n@author: Wang Yuntao\n""""""\n\nimport os\nimport sys\n\nsys.path.append(""tools"")\n\n\ndef fullfile(file_dir, file_name):\n    """"""\n    fullfile as matlab\n    :param file_dir: file dir\n    :param file_name: file name\n    :return: a full file path\n    """"""\n    full_file_path = os.path.join(file_dir, file_name)\n    full_file_path = full_file_path.replace(""\\\\"", ""/"")\n\n    return full_file_path\n    \n\ndef get_file_type(file_path, sep="".""):\n    """"""\n    get the type of file\n    :param file_path: file path\n    :param sep: separator\n    :return: file type\n    """"""\n    if os.path.exists(file_path):\n        file_type = file_path.split(sep=sep)[-1]\n    else:\n        file_type = None\n\n    return file_type\n\n\ndef get_files_list(files_path, file_type=None):\n    """"""\n    :param files_path: path of MP3 files for move\n    :param file_type: file type, default is None\n    :return: Null\n    """"""\n    filename = os.listdir(files_path)\n    files_list = []\n    for file in filename:\n        file_path = fullfile(files_path, file)\n        if get_file_type(file_path) == file_type or file_type is None:\n            files_list.append(file_path)\n\n    return files_list\n\n\ndef qmdct_extract(files_path, frame_number=50, file_num=None, start_index=0, text_files_path=None, file_type=""mp3""):\n    """"""\n    :param files_path: the path of MP3 files for QMDCT extraction\n    :param frame_number: the number of frames for extraction, default is 50\n    :param file_num: the number of MP3 files for QMDCT extraction, default is None which means all files in the current path are extracted, default is None\n    :param start_index: the index of files\n    :param text_files_path: the path of output QMDCT text files\n    :param file_type: file type, default is ""mp3""\n    :return: Null\n    """"""\n\n    files_list = get_files_list(files_path, file_type=file_type)\n    total_num = len(files_list)\n\n    if file_num is None or file_num > total_num:\n        file_num = total_num\n    if text_files_path is None:\n        text_files_path = files_path\n\n    for i in range(start_index, start_index + file_num):\n        mp3_file = os.path.join(files_path, files_list[i])\n\n        if os.path.isfile(mp3_file):\n\n            mp3_file_name = mp3_file.split(""/"")[-1]\n            file_type = mp3_file_name.split(""."")[-1]\n            mp3_file_name = mp3_file_name.split("".mp3"")[0]\n            if file_type == ""mp3"":\n                wav_file_name = mp3_file_name + "".wav""\n                wav_file_path = os.path.join(files_path, wav_file_name)\n\n                txt_file_name = mp3_file_name + "".txt""\n                txt_file_path = os.path.join(text_files_path, txt_file_name)\n\n                if os.path.exists(txt_file_path):\n                    pass\n                else:\n                    command = ""lame_qmdct.exe "" + mp3_file + \\\n                        "" -framenum "" + str(frame_number) + "" -startind 0 -coeffnum 576 --decode""\n                    os.system(command)\n                    os.remove(wav_file_path)\n            else:\n                pass\n\n\nif __name__ == ""__main__"":\n    params_num = len(sys.argv)\n\n    if params_num == 2:\n        args_files_path = sys.argv[1]\n        qmdct_extract(args_files_path)\n    elif params_num == 3:\n        args_files_path = sys.argv[1]\n        args_frame_number = sys.argv[2]\n        qmdct_extract(args_files_path, args_frame_number)\n    elif params_num == 4:\n        args_files_path = sys.argv[1]\n        args_frame_number = sys.argv[2]\n        args_file_num = int(sys.argv[3])\n        qmdct_extract(args_files_path, args_frame_number, args_file_num)\n    elif params_num == 5:\n        args_files_path = sys.argv[1]\n        args_frame_number = sys.argv[2]\n        args_file_num = int(sys.argv[3])\n        args_start_index = int(sys.argv[4])\n        qmdct_extract(args_files_path, args_frame_number, args_file_num, args_start_index)\n    elif params_num == 6:\n        args_files_path = sys.argv[1]\n        args_frame_number = sys.argv[2]\n        args_file_num = int(sys.argv[3])\n        args_start_index = int(sys.argv[4])\n        args_text_files_path = sys.argv[5]\n\n        if not os.path.exists(args_text_files_path):\n            os.mkdir(args_text_files_path)\n\n        qmdct_extract(args_files_path, args_frame_number, args_file_num, args_start_index, args_text_files_path)\n    else:\n        print(""Please input the command as the format of {python QMDCT_extractor.py \\""files_path\\"" \\""file_num (default is None)\\""} "")\n'"
data_processing/python_scripts/blind_dataset_make.py,0,"b'import sys\nimport shutil\nfrom utils import *\n\n# global variables\ncover_files_path = ""E:/Myself/2.database/3.cover/cover_10s/192""\nHCM_stego_files_path = ""E:/Myself/2.database/4.stego/HCM/""\nEECS_stego_files_path = ""E:/Myself/2.database/4.stego/EECS/""\ntotal_num = 10000\n\n\ndef dataset_blind_make(new_dataset_path, algorithm, bitrate, interval):\n    """"""\n    :param new_dataset_path: the path of new blind dataset\n    :param algorithm: the steganographic algorithm\n    :param bitrate: the bitrate of mixed MP3 audio\n    :param interval: the interval of each parameters \n    """"""\n    stego_payload_rates = [""01"", ""03"", ""05"", ""08"", ""10""]\n    widths = [""2"", ""3"", ""4"", ""5"", ""6""]\n    files_list = get_files_list(cover_files_path, ""txt"")\n    if algorithm == ""HCM"":\n        spr_index = 0\n        for file_index in range(total_num):\n            spr = stego_payload_rates[spr_index]\n            file_name = get_file_name(files_list[file_index])\n            old_file_path = fullfile(fullfile(HCM_stego_files_path, ""HCM_B_"" + str(bitrate) + ""_ER_"" + spr), file_name)\n            new_file_path = fullfile(new_dataset_path, file_name)\n\n            if (file_index + 1) % interval == 0 and spr_index < len(stego_payload_rates):\n                spr_index += 1\n            if (file_index + 1) % interval == 0 and spr_index == len(stego_payload_rates):\n                spr_index = 0\n            \n            shutil.copy(old_file_path, new_file_path)\n    \n    if algorithm == ""EECS"":\n        w_index = 0\n        for file_index in range(total_num):\n            w = widths[w_index]\n            file_name = get_file_name(files_list[file_index])\n            old_file_path = fullfile(fullfile(EECS_stego_files_path, ""EECS_B_"" + str(bitrate) + ""_W_"" + w + ""_H_7_ER_10""), file_name)\n            new_file_path = fullfile(new_dataset_path, file_name)\n\n            if (file_index + 1) % interval == 0 and w_index < len(widths):\n                w_index += 1\n            if (file_index + 1) % interval == 0 and w_index == len(widths):\n                w_index = 0\n            \n            shutil.copy(old_file_path, new_file_path)\n\n    if algorithm == ""all"":\n        spr_index, w_index, index = 0, 0, 0\n        stego_dir = fullfile(EECS_stego_files_path, ""EECS_B_"" + str(bitrate) + ""_W_2_H_7_ER_10"")\n        spr = stego_payload_rates[spr_index]\n\n        for file_index in range(total_num):\n            index = spr_index + w_index\n            file_name = get_file_name(files_list[file_index])\n            old_file_path = fullfile(fullfile(EECS_stego_files_path, stego_dir), file_name)\n            new_file_path = fullfile(new_dataset_path, file_name)\n\n            shutil.copy(old_file_path, new_file_path)\n            \n            if (file_index + 1) % interval == 0 and w_index + 1 < len(widths) and index + 1 < len(widths):\n                w_index += 1\n                w = widths[w_index]\n                stego_dir = fullfile(EECS_stego_files_path, ""EECS_B_"" + str(bitrate) + ""_W_"" + w + ""_H_7_ER_10"")\n            \n            elif (file_index + 1) % interval == 0 and w_index + 1 == len(widths) and index + 1 == len(widths):\n                stego_dir = fullfile(HCM_stego_files_path, ""HCM_B_"" + str(bitrate) + ""_ER_"" + spr)\n                spr_index += 1\n\n            elif (file_index + 1) % interval == 0 and spr_index < len(stego_payload_rates) and (len(widths) < index + 1 < len(widths) + len(stego_payload_rates)):\n                spr = stego_payload_rates[spr_index]                \n                stego_dir = fullfile(HCM_stego_files_path, ""HCM_B_"" + str(bitrate) + ""_ER_"" + spr)\n                spr_index += 1\n\n            elif (file_index + 1) % interval == 0 and spr_index == len(stego_payload_rates) and (index + 1 > len(widths) and index + 1 == len(widths) + len(stego_payload_rates)):\n                w_index, spr_index = 0, 0\n                w = widths[w_index]\n                spr = stego_payload_rates[spr_index]\n                stego_dir = fullfile(EECS_stego_files_path, ""EECS_B_"" + str(bitrate) + ""_W_"" + w + ""_H_7_ER_10"")\n\n            else:\n                pass\n            \n\nif __name__ == ""__main__"":\n    args_params_num = len(sys.argv)\n    args_algorithm = sys.argv[1]\n    args_bitrate = sys.argv[2]\n    args_interval = int(sys.argv[3])\n    if algorithm == ""HCM"":\n        args_new_dataset_path = ""E:/Myself/2.database/blind_data/MIX_HCM_B_"" + bitrate\n    elif algorithm == ""EECS"":\n        args_new_dataset_path = ""E:/Myself/2.database/blind_data/MIX_EECS_B_"" + bitrate\n    else:\n        args_new_dataset_path = ""E:/Myself/2.database/blind_data/MIX_B_"" + bitrate\n\n    dataset_blind_make(args_new_dataset_path, args_algorithm, args_bitrate, args_interval)\n'"
data_processing/python_scripts/files_move.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2018.08.13\nFinished on 2018.08.14\nModified on \n@author: Yuntao Wang\n""""""\n\nimport os\nimport sys\nimport shutil\nfrom utils import *\n\n\ndef files_copy(directory_old, directory_new, file_type=""txt""):\n    """"""\n    files move\n    :param directory_old: old files path\n    :param directory_new: new files path\n    :param file_type: file type, default is ""txt""\n    """"""\n    if not os.path.exists(directory_new):\n        os.mkdir(directory_new)\n    files_list_old = get_files_list(directory_old, file_type)\n    for file_path_old in files_list_old:\n        file_name = get_file_name(file_path_old)\n        file_path_new = fullfile(directory_new, file_name)\n        if not os.path.exists(file_path_new):\n            shutil.copyfile(file_path_old, file_path_new)\n        else:\n            pass\n\n\ndef files_move(directory_old, directory_new, file_type=""txt""):\n    """"""\n    files move\n    :param directory_old: old files path\n    :param directory_new: new files path\n    :param file_type: file type, default is ""txt""\n    """"""\n    if not os.path.exists(directory_new):\n        os.mkdir(directory_new)\n    files_list_old = get_files_list(directory_old, file_type)\n    for file_path_old in files_list_old:\n        file_name = get_file_name(file_path_old)\n        file_path_new = fullfile(directory_new, file_name)\n        if not os.path.exists(file_path_new):\n            shutil.move(file_path_old, file_path_new)\n        else:\n            pass\n\n\nif __name__ == ""__main__"":\n    params_num = len(sys.argv)\n    if params_num == 3:\n        args_directory_old = sys.argv[1]\n        args_directory_new = sys.argv[2]\n        files_copy(args_directory_old, args_directory_new)\n    elif params_num == 4:\n        args_directory_old = sys.argv[1]\n        args_directory_new = sys.argv[2]\n        args_file_type = sys.argv[3]\n        files_copy(args_directory_old, args_directory_new, args_file_type)\n    else:\n        print(""Please input the command as the format of {python files_move.py \\""directory_old\\"" \\""directory_new\\"" \\""file_type (defalut is \\""txt\\"")\\""} "")\n'"
data_processing/python_scripts/train_test_split.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2018.08.21\nFinished on 2018.08.21\nModified on \n@author: Yuntao Wang\n""""""\n\nimport os\nimport sys\nimport shutil\n\n\ndef get_file_type(file_path, sep="".""):\n    """"""\n    get the type of file\n    :param file_path: file path\n    :param sep: separator\n    :return: file type\n    """"""\n    if os.path.exists(file_path):\n        file_type = file_path.split(sep=sep)[-1]\n    else:\n        file_type = None\n\n    return file_type\n\n\ndef get_files_list(files_path, file_type=None):\n    """"""\n    :param files_path: path of MP3 files for move\n    :param file_type: file type, default is None\n    :return: Null\n    """"""\n    filename = os.listdir(files_path)\n    files_list = []\n    for file in filename:\n        file_path = fullfile(files_path, file)\n        if get_file_type(file_path) == file_type or file_type is None:\n            files_list.append(file_path)\n\n    return files_list\n\n\ndef get_file_name(file_path, sep=""/""):\n    """"""\n    get the name of file\n    :param file_path: file path\n    :param sep: separator\n    :return: file name\n    """"""\n    if os.path.exists(file_path):\n        file_path.replace(""\\\\"", ""/"")\n        file_name = file_path.split(sep=sep)[-1]\n    else:\n        file_name = None\n    return file_name\n\n\ndef fullfile(file_dir, file_name):\n    """"""\n    fullfile as matlab\n    :param file_dir: file dir\n    :param file_name: file name\n    :return: a full file path\n    """"""\n    full_file_path = os.path.join(file_dir, file_name)\n    full_file_path = full_file_path.replace(""\\\\"", ""/"")\n\n    return full_file_path\n\n\ndef files_move(files_list, files_path_new):\n    """"""\n    files move\n    :param files_list: files list\n    :param files_path_new: new files path\n    :return\n        NULL\n    """"""\n    for file_path in files_list:\n        file_name = get_file_name(file_path)\n        file_path_new = fullfile(files_path_new, file_name)\n        if not os.path.exists(file_path_new):\n            shutil.move(file_path, file_path_new)\n        else:\n            pass\n\n\ndef make_folder(files_path):\n    """"""\n        create folder\n        :param files_path path of files to be created\n        :return\n            NULL\n    """"""\n    if not os.path.exists(files_path):\n        os.mkdir(files_path)\n    else:\n        pass\n    \n\ndef train_test_split(files_path, percent_train=0.7, percent_validation=0.3):\n    """"""\n    split the dataset into train, validation and test\n    :param files_path: path of files to be split\n    :param percent_train: percent of train data\n    :param percent_validation: percent of validation data\n    :return:\n        NULL\n    """"""\n    if not percent_train + percent_validation == 1:\n        print(""The sum of percent of all split data is not 100%, please try again."")\n    else:\n        files_list = get_files_list(files_path)\n        files_num = len(files_list)\n        data_num_test = files_num % 10000\n        train_validation_num = files_num - data_num_test\n        \n        data_num_train = int(train_validation_num * percent_train)\n        data_num_validation = int(train_validation_num * percent_validation)\n        \n        # split the dataset\n        files_list_train = files_list[:data_num_train]\n        del files_list[:data_num_train]\n\n        files_list_validation = files_list[:data_num_validation]\n        del files_list[:data_num_validation]\n\n        files_list_test = files_list[:data_num_test]\n        del files_list[:data_num_test]\n\n        # make dir\n        files_path_train = fullfile(files_path, ""train"")\n        files_path_validation = fullfile(files_path, ""validation"")\n        files_path_test = fullfile(files_path, ""test"")\n\n        make_folder(files_path_train)\n        make_folder(files_path_validation)\n        make_folder(files_path_test)\n\n        # file move\n        print(files_list_train[0])\n        files_move(files_list_train, files_path_train)\n        files_move(files_list_validation, files_path_validation)\n        files_move(files_list_test, files_path_test)\n\n\nif __name__ == ""__main__"":\n    params_num = len(sys.argv)\n    if params_num == 2:\n        args_files_path = sys.argv[1]\n        args_train_test_split(args_files_path)\n    elif params_num == 3:\n        args_files_path = sys.argv[1]\n        args_percent_train = float(sys.argv[2])\n        args_percent_validation = 1 - float(percent_train)\n        train_test_split(args_files_path, args_percent_train, args_percent_validation)\n    elif params_num == 4:\n        args_files_path = sys.argv[1]\n        args_percent_train = float(sys.argv[2])\n        args_percent_validation = float(sys.argv[3])\n        train_test_split(args_files_path, args_percent_train, args_percent_validation)\n    else:\n        print(""Please input the command as the format of {python train_test_split.py \\""files_path\\"" \\""percent_train\\"" \\""percent_validation\\""}"")\n'"
data_processing/python_scripts/utils.py,0,"b'import os\n\n\ndef get_file_name(file_path, sep=""/""):\n    """"""\n    get the name of file\n    :param file_path: file path\n    :param sep: separator\n    :return: file name\n    """"""\n    if os.path.exists(file_path):\n        file_path.replace(""\\\\"", ""/"")\n        file_name = file_path.split(sep=sep)[-1]\n    else:\n        file_name = None\n    return file_name\n\n\n\ndef fullfile(file_dir, file_name):\n    """"""\n    fullfile as matlab\n    :param file_dir: file dir\n    :param file_name: file name\n    :return: a full file path\n    """"""\n    full_file_path = os.path.join(file_dir, file_name)\n    full_file_path = full_file_path.replace(""\\\\"", ""/"")\n\n    return full_file_path\n\n\ndef get_file_type(file_path, sep="".""):\n    """"""\n    get the type of file\n    :param file_path: file path\n    :param sep: separator\n    :return: file type\n    """"""\n    if os.path.exists(file_path):\n        file_type = file_path.split(sep=sep)[-1]\n    else:\n        file_type = None\n\n    return file_type\n\n\ndef get_files_list(file_dir, file_type=""txt"", start_idx=None, end_idx=None):\n    """"""\n    :param files_path: path of MP3 files for move\n    :param file_type: file type, default is ""txt""\n    :return: Null\n    """"""\n    filename = os.listdir(file_dir)\n    files_list = []\n    for file in filename:\n        file_path = fullfile(file_dir, file)\n        if get_file_type(file_path) == file_type:\n            files_list.append(file_path)\n    \n    files_list = files_list[start_idx:end_idx]\n\n    return files_list'"
src/HPFs/filters.py,15,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2018.03.28\nFinished on 2018.03.28\nModified on 2018.08.27\n\n@author: Yuntao Wang\n""""""\n\nimport numpy as np\nimport tensorflow as tf\nfrom text_preprocess import text_read\n\n\ndef kv_kernel_generator():\n    """"""\n    kv kernel for image steganalysis\n    :return:\n        kv kernel tensor\n    """"""\n    kv_kernel_matrix = tf.constant(value=[-1, 2, -2, 2, -1, 2, -6, 8, -6, 2, -2, 8, -12, 8, -2, 2, -6, 8, -6, 2, -1, 2, -2, 2, -1],\n                                   dtype=tf.float32,\n                                   shape=[5, 5, 1, 1],\n                                   name=""kv_kernel_matrix"")\n    kv_kernel = tf.multiply(1 / 12, kv_kernel_matrix, name=""kv_kernel"")\n\n    return kv_kernel\n\n\ndef srm_kernels_generator():\n    """"""\n    SRM kernels for image steganalysis\n    :return:\n        SRM kernels tensor\n    """"""\n    srm_kernels_np = np.load(""./SRM_Kernels.npy"")\n    [height, width, channel, filter_num] = np.shape(srm_kernels_np)\n    srm_kernels = tf.constant(value=srm_kernels_np,\n                              dtype=tf.float32,\n                              shape=[height, width, channel, filter_num],\n                              name=""srm_kernels"")\n\n    return srm_kernels\n\n\ndef dct_kernel_generator(kernel_size=4):\n    """"""\n    DCT kernel for image steganalysis\n    :param kernel_size: size of DCT kernel, e.g. 2, 3, 4, 5, 6, 7, 8\n    :return:\n        a DCT kernel tensor\n    """"""\n    kernel_sizes = [2, 3, 4, 5, 6, 7, 8]\n    if kernel_size not in kernel_sizes:\n        print(""Wrong kernel size"")\n        dct_kernel_np = np.ones(kernel_size * kernel_size)\n        dct_kernel = tf.constant(value=dct_kernel_np,\n                                 dtype=tf.float32,\n                                 shape=[kernel_size, kernel_size, 1, 1],\n                                 name=""all_one_kernel"")\n    else:\n        dct_kernel_file_path = ""./dct_kernels/DCT"" + str(kernel_size) + "".txt""\n        dct_kernel_np = text_read(dct_kernel_file_path, channel=0, separator="" "")\n\n        dct_kernel = tf.constant(value=dct_kernel_np,\n                                 dtype=tf.float32,\n                                 shape=[kernel_size, kernel_size, 1, kernel_size * kernel_size],\n                                 name=""dct_kernel"")\n    return dct_kernel\n\n\ndef point_high_pass_kernel_generator():\n    """"""\n    point high-pass kernel for image steganalysis\n    :return:\n        point high-pass kernel tensor\n    """"""\n    point_high_pass_kernel = tf.constant(value=[-0, 0, 0.0199, 0, 0, 0, 0.0897, 0.1395, 0.0897, 0, -0.0199, 0.1395, -1, 0.1395, 0.0199,\n                                                0, 0.0897, 0.1395, 0.0897, 0, 0, 0, 0.0199, 0, 0],\n                                         dtype=tf.float32,\n                                         shape=[5, 5, 1, 1],\n                                         name=""point_high_pass_kernel"")\n\n    return point_high_pass_kernel\n\n\ndef gabor_2d_horizontal_kernel_generator():\n    """"""\n    horizontal 2D Gabor Filter for image steganalysis\n    :return:\n        horizontal 2D Gabor Filter\n    """"""\n    gabor_2d_horizontal_kernel = tf.constant(value=[0.0562, -0.1354, 0, 0.1354, -0.0562, 0.0818, -0.1970, 0, 0.1970, -0.0818, 0.0926, -0.2233, 0, 0.2233, -0.0926,\n                                                    0.0818, -0.1970, 0, 0.1970, -0.818, 0.0562, -0.1354, 0, 0.1354, -0.0562],\n                                             dtype=tf.float32,\n                                             shape=[5, 5, 1, 1],\n                                             name=""gabor_2d_horizontal_kernel"")\n\n    return gabor_2d_horizontal_kernel\n\n\ndef gabor_2d_vertical_kernel_generator():\n    """"""\n    vertical 2D Gabor Filter for image steganalysis\n    :return:\n        vertical 2D Gabor Filter\n    """"""\n    gabor_2d_vertical_kernel = tf.constant(value=[-0.0562, -0.0818, -0.0926, -0.0818, -0.0562, 0.1354, 0.1970, 0.2233, 0.1970, 0.1354, 0, 0, 0, 0, 0,\n                                                  -0.1354, -0.1970, -0.2233, -0.1970, -0.1354, 0.0562, 0.0818, 0.0926, 0.0818, 0.0562],\n                                           dtype=tf.float32,\n                                           shape=[5, 5, 1, 1],\n                                           name=""gabor_2d_vertical_kernel"")\n\n    return gabor_2d_vertical_kernel\n'"
src/experimental_scripts/test_arbitrary_size.py,7,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2019.03.12\nFinished on 2019.03.12\nModified on\n\n@author: Yuntao Wang\n""""""\n\nimport os\nimport time\nimport datetime\nfrom utils import *\nimport tensorflow as tf\nfrom itertools import product\nfrom networks.networks import networks\nfrom networks.tested_steganalysis import *\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = """"\n\n# hyper parameters\nbatch_size = 16\ncarrier = ""qmdct""\nclasses_num = 2\nstart_index_test, end_index_test = 0, 1038\nis_shuffle = True\n\n# path\ncover_test_files_path = ""/home1/wyt/data/size_mismatch/cover_128""\nstego_test_files_path = ""/home1/wyt/data/size_mismatch/EECS_128_W_2_H_7_ER_10""\nmodels_path = ""/home1/wyt/code/tf_audio_steganalysis/models/steganalysis/google_net3/EECS_B_128_W_2_H_7_ER_10/1552005868""\n\nheights = [200, 250, 300, 350, 400]\nwidths = [400, 430, 450, 480, 500, 530]\nchannel = 1\nnetwork = ""google_net3""\n\nfor height, width in product(heights, widths):\n    # placeholder\n    data = tf.placeholder(dtype=tf.float32, shape=(batch_size, height, width, channel), name=""data"")\n    labels = tf.placeholder(dtype=tf.int32, shape=(batch_size, ), name=""label"")\n    is_bn = tf.placeholder(dtype=tf.bool, name=""is_bn"")\n\n    command = network + ""(data, classes_num, is_bn)""\n    logits = eval(command)\n\n    accuracy, false_positive_rate, false_negative_rate = evaluation(logits=logits, labels=labels)\n\n    # information output\n    print(""cover files path: %s"" % cover_test_files_path)\n    print(""stego files path: %s"" % stego_test_files_path)\n    print(""class number: %d"" % classes_num)\n    print(""start load network..."")\n\n    config = tf.ConfigProto()\n    config.allow_soft_placement = True\n    config.gpu_options.allow_growth = True\n\n    start_time = time.time()\n\n    model = tf.train.Saver()\n    with tf.Session() as sess:\n        # load model\n        sess.run(tf.global_variables_initializer())\n        model_file_path = get_model_file_path(models_path)\n\n        if model_file_path is None:\n            print(""No model is loaded successfully."")\n        else:\n            model.restore(sess, model_file_path)\n            print(""The model is loaded successfully, model file: %s"" % model_file_path)\n            # read files list (train)\n            cover_test_data_list, cover_test_label_list, \\\n                stego_test_data_list, stego_test_label_list = read_data(cover_test_files_path, stego_test_files_path, start_index_test, end_index_test, is_shuffle=is_shuffle)\n\n            if len(cover_test_data_list) < batch_size:\n                batch_size = len(cover_test_data_list)\n\n            test_iterations, test_accuracy, test_fpr, test_fnr = 0, 0, 0, 0\n            for x_test_batch, y_test_batch in \\\n                    minibatches(cover_test_data_list, cover_test_label_list, stego_test_data_list, stego_test_label_list, batch_size):\n                # data read and process\n                x_test_data = get_data_batch(x_test_batch, height=height, width=width, channel=channel, carrier=carrier)\n\n                # get the accuracy and loss\n                acc, fpr, fnr = sess.run([accuracy, false_positive_rate, false_negative_rate],\n                                         feed_dict={data: x_test_data, labels: y_test_batch, is_bn: True})\n                test_accuracy += acc\n                test_fpr += fpr\n                test_fnr += fnr\n                test_iterations += 1\n\n                print(""Batch-%003d, accuracy: %f, fpr: %f, fnr: %f"" % (test_iterations, acc, fpr, fnr))\n\n            test_accuracy_average = test_accuracy / test_iterations\n            test_fpr_average = test_fpr / test_iterations\n            test_fnr_average = test_fnr / test_iterations\n            print(""Test accuracy: %.2f%%, FPR: %.2f%%, FNR: %.2f%%"" % (100. * test_accuracy_average, 100. * test_fpr_average, 100. * test_fnr_average))\n\n            with open(""/home1/wyt/code/tf_audio_steganalysis/results_arbitrary_size.txt"", ""a"") as file:\n                file.write(""height: %d, width: %d -- Test accuracy: %.2f%%\\n"" % (height, width, 100. * test_accuracy_average))\n\n    et = time.time() - start_time\n    et = str(datetime.timedelta(seconds=et))[:-7]\n    print(""Run Time: %s"" % et)\n'"
src/matlab_scripts/jpeg_image_read.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport matlab.engine\n\n""""""\nmatlab API for python: \n    http://ww2.mathworks.cn/help/matlab/matlab-engine-for-python.html\n""""""\n\nmatlab_engine = matlab.engine.start_matlab()           # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96matlab\xe5\xbc\x95\xe6\x93\x8e\n\nout = matlab_engine.read_jpeg_image(""I:/jpeg/img_1009.jpg"")\nprint(type(out))\n\n'"
src/modules/arbitrary_size.py,12,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2019.03.05\nFinished on 2019.03.05\nModified on\n\n@author: Yuntao Wang\n""""""\n\nimport tensorflow as tf\n\n\ndef moments_extraction(input_data):\n    """"""\n    this function is used for dimension unification in Jessica\'s paper for steganalysis of arbitrary size\n    calculate the moments of feature maps -- mean, variance, maximum and minimum\n    :param input_data: the input data tensor [batch_size, height, width, channels]\n    :return:\n        moments: a 4-D tensor [number, 1, 4, channel]\n    """"""\n\n    data_max = tf.reduce_max(input_data, axis=[1, 2], keep_dims=True, name=""moments_max"")\n    data_min = tf.reduce_max(input_data, axis=[1, 2], keep_dims=True, name=""moments_min"")\n    data_mean, data_variance = tf.nn.moments(input_data, axes=[1, 2], keep_dims=True, name=""moments_mean_var"")\n\n    moments = tf.concat([data_max, data_min, data_mean, data_variance], axis=2, name=""moments"")\n\n    return moments\n\n\ndef moments_extraction_enhancement(input_data):\n    """"""\n    this function is the enhancement version of moments extraction\n    calculate the moments of feature maps -- mean, variance, maximum and minimum, kurtosis, skewness\n    :param input_data: the input data tensor [batch_size, height, width, channels]\n    :return:\n        moments: a 4-D tensor [number, 1, 6, channel]\n    """"""\n\n    data_max = tf.reduce_max(input_data, axis=[1, 2], keep_dims=True, name=""moments_max"")\n    data_min = tf.reduce_min(input_data, axis=[1, 2], keep_dims=True, name=""moments_min"")\n    data_mean, data_variance = tf.nn.moments(input_data, axes=[1, 2], keep_dims=True, name=""moments_mean_var"")\n\n    input_data_sub_mean = tf.subtract(input_data, data_mean, name=""input_data_sub_mean"")\n    data_variance_inverse = tf.divide(1.0, data_variance, name=""data_variance_inverse"")\n    data_kurtosis = tf.multiply(tf.reduce_mean(tf.pow(input_data_sub_mean, 4)), tf.pow(data_variance_inverse, 4), name=""kurtosis"")\n    data_skewness = tf.multiply(tf.reduce_mean(tf.pow(input_data_sub_mean, 3)), tf.pow(data_variance_inverse, 3), name=""skewness"")\n\n    moments = tf.concat([data_max, data_min, data_mean, data_variance, data_kurtosis, data_skewness], axis=2, name=""moments"")\n\n    return moments\n'"
src/modules/densenet_block.py,2,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2019.03.05\nFinished on 2019.03.05\nModified on\n\n@author: Yuntao Wang\n""""""\n\nfrom layer import *\n\n\ndef dense_block(input_data, filter_nums, layers, name, is_bn=True):\n    """"""\n    basic block of dense net\n    :param input_data: the input data tensor [batch_size, height, width, channels]\n    :param filter_nums: the number of convolutional kernels\n    :param layers: the number of convolutional layers in a dense block\n    :param name: the name of the dense block\n    :param is_bn: if False, skip this layer, default is True\n    :return:\n        output: a 4-D tensor [number, height, width, channel]\n    """"""\n    layers_concat = list()\n    layers_concat.append(input_data)\n\n    output = basic_block(input_data, filter_nums=filter_nums, name=name+""_basic_block1"", is_bn=is_bn)\n    layers_concat.append(output)\n\n    for layer in range(layers - 1):\n        output = tf.concat(layers_concat, axis=3)\n        output = basic_block(output, filter_nums=filter_nums, name=name+""_basic_block""+str(layer+2), is_bn=is_bn)\n        layers_concat.append(output)\n\n    output = tf.concat(layers_concat, axis=3)\n\n    return output\n\n\ndef basic_block(input_data, filter_nums, name, is_bn=True):\n    """"""\n    basic convolutional block in dense net block\n    :param input_data: the input data tensor [batch_size, height, width, channels]\n    :param filter_nums: the number of convolutional kernels\n    :param name: the name of the basic block\n    :param is_bn: if False, skip this layer, default is True\n    :return:\n        output: a 4-D tensor [number, height, width, channel]\n    """"""\n    output = batch_normalization(input_data, name=name + ""_BN1_1"", activation_method=""tanh"", is_train=is_bn)\n    output = conv_layer(output, 1, 1, 1, 1, 4 * filter_nums, name=name + ""_conv1_2"", activation_method=""None"", padding=""SAME"")\n    output = batch_normalization(output, name=name + ""_BN1_3"", activation_method=""tanh"", is_train=is_bn)\n    output = conv_layer(output, 3, 3, 1, 1, filter_nums, name=name + ""_conv1_4"", activation_method=""None"", padding=""SAME"")\n\n    return output\n\n\ndef transition_layer(input_data, filter_nums, name, is_bn=True):\n    """"""\n    transition layer between two dense blocks\n    :param input_data: the input data tensor [batch_size, height, width, channels]\n    :param filter_nums: the number of convolutional kernels\n    :param name: the name of the layer\n    :param is_bn: if False, skip this layer, default is True\n    :return:\n        output: a 4-D tensor [number, height, width, channel]\n    """"""\n    output = batch_normalization(input_data, name=name + ""_BN1_1"", activation_method=""tanh"", is_train=is_bn)\n    output = conv_layer(output, 1, 1, 1, 1, filter_nums, name=name + ""_conv1_2"", activation_method=""None"", padding=""SAME"")\n    output = pool_layer(output, 2, 2, 2, 2, name=name+""pool1_1"", is_max_pool=True)\n\n    return output\n'"
src/modules/inception.py,4,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2019.02.28\nFinished on 2019.02.28\nModified on\n\n@author: Yuntao Wang\n""""""\n\nfrom layer import *\n\n\ndef inception_v1(input_data, filter_num, name, is_bn=True, is_max_pool=True):\n    """"""\n    the structure of inception V1\n    :param input_data: the input data tensor [batch_size, height, width, channels]\n    :param filter_num: the number of the convolutional kernel\n    :param name: the name of the layer\n    :param is_bn: if False, skip this layer, default is True\n    :param is_max_pool: whether max pooling or not (default: True)\n\n    :return: 4-D tensor\n    """"""\n    branch1 = conv_layer(input_data, 1, 1, 1, 1, filter_num, name + ""_branch_1_conv_1"",\n                         activation_method=""None"", padding=""SAME"")\n    branch1 = batch_normalization(branch1, name=name + ""_branch_2_BN"", activation_method=""None"", is_train=is_bn)\n\n    branch2 = conv_layer(input_data, 1, 1, 1, 1, filter_num, name + ""_branch_2_conv_1"",\n                         activation_method=""tanh"", padding=""SAME"")\n    branch2 = conv_layer(branch2, 3, 3, 1, 1, filter_num, name + ""_branch_2_conv_3"",\n                         activation_method=""None"", padding=""SAME"")\n    branch2 = batch_normalization(branch2, name=name + ""_branch_2_BN"", activation_method=""None"", is_train=is_bn)\n\n    branch3 = conv_layer(input_data, 1, 1, 1, 1, filter_num, name + ""_branch_3_conv_1"",\n                         activation_method=""tanh"", padding=""SAME"")\n    branch3 = conv_layer(branch3, 5, 5, 1, 1, filter_num, name + ""_branch_3_conv_5"",\n                         activation_method=""None"", padding=""SAME"")\n    branch3 = batch_normalization(branch3, name=name + ""_branch_3_BN"", activation_method=""None"", is_train=is_bn)\n\n    branch4 = pool_layer(input_data, 3, 3, 1, 1, name + ""branch_4_pool"", is_max_pool=is_max_pool, padding=""SAME"")\n    branch4 = conv_layer(branch4, 1, 1, 1, 1, filter_num, name + ""branch_4_conv_1"", activation_method=""None"", padding=""SAME"")\n    branch4 = batch_normalization(branch4, name=name + ""_branch_4_2"", activation_method=""None"", is_train=is_bn)\n\n    output = tf.concat([branch1, branch2, branch3, branch4], 3)\n\n    return output\n\n\ndef inception_v2(input_data, filter_num, name,  is_max_pool=True, is_bn=True):\n    """"""\n    the structure of inception V2\n    :param input_data: the input data tensor [batch_size, height, width, channels]\n    :param filter_num: the number of the convolutional kernel\n    :param name: the name of the layer\n    :param is_max_pool: whether max pooling or not (default: True)\n    :param is_bn: if False, skip this layer, default is True\n\n    :return: 4-D tensor\n    """"""\n    branch1 = conv_layer(input_data, 1, 1, 1, 1, filter_num, name + ""_branch_1_conv_1"", activation_method=""None"", padding=""SAME"")\n    branch1 = batch_normalization(branch1, name=name + ""_branch_2_BN"", activation_method=""None"", is_train=is_bn)\n\n    branch2 = conv_layer(input_data, 1, 1, 1, 1, filter_num, name + ""_branch_2_conv_1"", activation_method=""tanh"", padding=""SAME"")\n    branch2 = conv_layer(branch2, 3, 3, 1, 1, filter_num, name + ""_branch_2_conv_3"", activation_method=""None"", padding=""SAME"")\n    branch2 = batch_normalization(branch2, name=name+""_branch_2_BN"", activation_method=""None"", is_train=is_bn)\n\n    branch3 = conv_layer(input_data, 1, 1, 1, 1, filter_num, name + ""_branch_3_conv_1"", activation_method=""tanh"", padding=""SAME"")\n    branch3 = conv_layer(branch3, 5, 5, 1, 1, filter_num, name + ""_branch_3_conv_5"", activation_method=""None"", padding=""SAME"")\n    branch3 = batch_normalization(branch3, name=name+""_branch_3_BN"", activation_method=""None"", is_train=is_bn)\n\n    branch4 = pool_layer(input_data, 3, 3, 1, 1, name+""branch_4_pool"", is_max_pool=is_max_pool, padding=""SAME"")\n    branch4 = conv_layer(branch4, 1, 1, 1, 1, filter_num, name+""branch_4_conv_1"", activation_method=""None"", padding=""SAME"")\n    branch4 = batch_normalization(branch4, name=name + ""_branch_4_BN"", activation_method=""None"", is_train=is_bn)\n\n    output = tf.concat([branch1, branch2, branch3, branch4], 3)\n\n    input_data_branch = conv_layer(input_data, 1, 1, 1, 1, 4*filter_num, name+""_input_data_branch_conv_1"", activation_method=""None"", padding=""SAME"")\n    input_data_branch = batch_normalization(input_data_branch, name + ""_input_data_branch_BN"", activation_method=""None"", is_train=is_bn)\n\n    output = output + input_data_branch\n    output = activation_layer(output, activation_method=""tanh"")\n\n    return output\n\n\ndef inception_v3(input_data, filter_num, name, is_max_pool=True, is_bn=True):\n    """"""\n    the structure of inception V3\n    :param input_data: the input data tensor [batch_size, height, width, channels]\n    :param filter_num: the number of the convolutional kernel\n    :param name: the name of the layer\n    :param is_max_pool: whether max pooling or not (default: True)\n    :param is_bn: if False, skip this layer, default is True\n\n    :return: 4-D tensor\n    """"""\n    branch1 = conv_layer(input_data, 1, 1, 1, 1, filter_num, name+""_branch_1_conv_1"", activation_method=""None"", padding=""SAME"")\n    branch1 = batch_normalization(branch1, name=name + ""_branch_1_BN"", activation_method=""None"", is_train=is_bn)\n\n    branch2 = conv_layer(input_data, 1, 1, 1, 1, filter_num, name + ""_branch_2_conv_1"", activation_method=""tanh"", padding=""SAME"")\n    branch2 = conv_layer(branch2, 1, 3, 1, 1, filter_num, name + ""_branch_2_conv_2"", activation_method=""tanh"", padding=""SAME"")\n    branch2 = conv_layer(branch2, 3, 1, 1, 1, filter_num, name + ""_branch_2_conv_3"", activation_method=""None"", padding=""SAME"")\n    branch2 = batch_normalization(branch2, name=name + ""_branch_2_BN"", activation_method=""None"", is_train=is_bn)\n\n    branch3 = conv_layer(input_data, 1, 1, 1, 1, filter_num, name + ""_branch_3_conv_1"", activation_method=""tanh"", padding=""SAME"")\n    branch3 = conv_layer(branch3, 1, 5, 1, 1, filter_num, name + ""_branch_3_conv_2"", activation_method=""tanh"", padding=""SAME"")\n    branch3 = conv_layer(branch3, 5, 1, 1, 1, filter_num, name + ""_branch_3_conv_3"", activation_method=""None"", padding=""SAME"")\n    branch3 = batch_normalization(branch3, name=name + ""_branch_3_BN"", activation_method=""None"", is_train=is_bn)\n\n    branch4 = pool_layer(input_data, 3, 3, 1, 1, name + ""branch_4_pool"", is_max_pool=is_max_pool, padding=""SAME"")\n    branch4 = conv_layer(branch4, 1, 1, 1, 1, filter_num, name + ""branch_4_conv_1"", activation_method=""None"", padding=""SAME"")\n    branch4 = batch_normalization(branch4, name=name + ""_branch_4_BN"", activation_method=""None"", is_train=is_bn)\n\n    output = tf.concat([branch1, branch2, branch3, branch4], 3)\n\n    input_data_branch = conv_layer(input_data, 1, 1, 1, 1, 4 * filter_num, name + ""_input_data_branch_conv_1"", activation_method=""None"", padding=""SAME"")\n    input_data_branch = batch_normalization(input_data_branch, name=name + ""_input_data_branch_BN"", activation_method=""None"", is_train=is_bn)\n\n    output = output + input_data_branch\n    output = activation_layer(output, activation_method=""tanh"")\n\n    return output\n\n\ndef inception_v4(input_data, filter_num, name, activation_method=""relu"", alpha=0.2, padding=""VALID"", is_max_pool=True, is_bn=True):\n    """"""\n    the structure of inception V4\n    :param input_data: the input data tensor [batch_size, height, width, channels]\n    :param filter_num: the number of the convolutional kernel\n    :param name: the name of the layer\n    :param activation_method: the type of activation function (default: relu)\n    :param alpha: leaky relu alpha (default: 0.2)\n    :param padding: the padding method, ""SAME"" | ""VALID"" (default: ""SAME"")\n    :param is_max_pool: whether max pooling or not (default: True)\n    :param is_bn: if False, skip this layer, default is True\n\n    :return: 4-D tensor\n    """"""\n    branch1 = conv_layer(input_data, 1, 1, 1, 1, filter_num, name + ""_branch_1_conv_1"",\n                         activation_method=""None"", alpha=alpha, padding=padding)\n    branch1 = batch_normalization(branch1, name=name + ""_branch_2_BN"", activation_method=""None"", is_train=is_bn)\n\n    branch2 = conv_layer(input_data, 1, 1, 1, 1, filter_num, name + ""_branch_2_conv_1"",\n                         activation_method=""None"", alpha=alpha, padding=padding)\n    branch2 = conv_layer(branch2, 3, 3, 1, 1, filter_num, name + ""_branch_2_conv_3"",\n                         activation_method=""None"", alpha=alpha, padding=padding)\n    branch2 = batch_normalization(branch2, name=name + ""_branch_2_BN"", activation_method=""None"", is_train=is_bn)\n\n    branch3 = conv_layer(input_data, 1, 1, 1, 1, filter_num, name + ""_branch_3_conv_1"",\n                         activation_method=""None"", alpha=alpha, padding=padding)\n    branch3 = conv_layer(branch3, 5, 5, 1, 1, filter_num, name + ""_branch_3_conv_5"",\n                         activation_method=""None"", alpha=alpha, padding=padding)\n    branch3 = batch_normalization(branch3, name=name + ""_branch_3_BN"", activation_method=""None"", is_train=is_bn)\n\n    branch4 = pool_layer(input_data, 3, 3, 1, 1, name + ""branch_4_pool"", is_max_pool, padding=padding)\n    branch4 = conv_layer(branch4, 1, 1, 1, 1, filter_num, name + ""branch_4_conv_1"",\n                         activation_method=""None"", alpha=alpha, padding=padding)\n    branch4 = batch_normalization(branch4, name=name + ""_branch_4_BN"", activation_method=""None"", is_train=is_bn)\n\n    output = tf.concat([branch1, branch2, branch3, branch4], 3)\n\n    input_data_branch = conv_layer(input_data, 1, 1, 1, 1, 4 * filter_num, name + ""_input_data_branch_conv_1"",\n                                   activation_method=""None"", alpha=alpha, padding=padding)\n    input_data_branch = batch_normalization(input_data_branch, name=name + ""_input_data_branch_BN"", activation_method=""None"", is_train=is_bn)\n\n    output = output + input_data_branch\n    output = activation_layer(output, activation_method=activation_method)\n\n    return output'"
src/modules/resnet_block.py,2,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2019.03.05\nFinished on 2019.03.05\nModified on\n\n@author: Yuntao Wang\n""""""\n\nfrom layer import *\n\n\ndef res_conv_block(input_data, height, width, x_stride, y_stride, filter_num, name,\n                   activation_method=""relu"", alpha=0.2, padding=""SAME"", atrous=1,\n                   init_method=""xavier"", bias_term=True, is_pretrain=True):\n    """"""\n    residual convolutional layer\n    :param input_data: the input data tensor [batch_size, height, width, channels]\n    :param height: the height of the convolutional kernel\n    :param width: the width of the convolutional kernel\n    :param x_stride: stride in X axis\n    :param y_stride: stride in Y axis\n    :param filter_num: the number of the convolutional kernel\n    :param name: the name of the layer\n    :param activation_method: the type of activation function (default: relu)\n    :param alpha: leaky relu alpha (default: 0.2)\n    :param padding: the padding method, ""SAME"" | ""VALID"" (default: ""VALID"")\n    :param atrous: the dilation rate, if atrous == 1, conv, if atrous > 1, dilated conv (default: 1)\n    :param init_method: the method of weights initialization (default: xavier)\n    :param bias_term: whether the bias term exists or not (default: False)\n    :param is_pretrain: whether the parameters are trainable (default: True)\n\n    :return:\n        output: a 4-D tensor [number, height, width, channel]\n    """"""\n    conv1 = conv_layer(input_data, height, width, x_stride, y_stride, filter_num, name=name+""_conv1"", activation_method=activation_method, alpha=alpha,\n                       padding=padding, atrous=atrous, init_method=init_method, bias_term=bias_term, is_pretrain=is_pretrain)\n    conv2 = conv_layer(conv1, height, width, x_stride, y_stride, filter_num, name=name + ""_conv2"", activation_method=""None"", alpha=alpha,\n                       padding=padding, atrous=atrous, init_method=init_method, bias_term=bias_term, is_pretrain=is_pretrain)\n    output = tf.add(input_data, conv2, name=name+""add"")\n\n    output = activation_layer(input_data=output,\n                              activation_method=activation_method,\n                              alpha=alpha)\n\n    return output\n\n\ndef res_conv_block_beta(input_data, height, width, x_stride, y_stride, filter_num, name,\n                        activation_method=""relu"", alpha=0.2, padding=""SAME"", atrous=1,\n                        init_method=""xavier"", bias_term=True, is_pretrain=True):\n    """"""\n    residual convolutional layer\n    :param input_data: the input data tensor [batch_size, height, width, channels]\n    :param height: the height of the convolutional kernel\n    :param width: the width of the convolutional kernel\n    :param x_stride: stride in X axis\n    :param y_stride: stride in Y axis\n    :param filter_num: the number of the convolutional kernel\n    :param name: the name of the layer\n    :param activation_method: the type of activation function (default: relu)\n    :param alpha: leaky relu alpha (default: 0.2)\n    :param padding: the padding method, ""SAME"" | ""VALID"" (default: ""VALID"")\n    :param atrous: the dilation rate, if atrous == 1, conv, if atrous > 1, dilated conv (default: 1)\n    :param init_method: the method of weights initialization (default: xavier)\n    :param bias_term: whether the bias term exists or not (default: False)\n    :param is_pretrain: whether the parameters are trainable (default: True)\n\n    :return:\n        output: a 4-D tensor [number, height, width, channel]\n    """"""\n    conv1 = conv_layer(input_data, height, width, x_stride, y_stride, filter_num, name=name+""_conv1"", activation_method=activation_method, alpha=alpha,\n                       padding=padding, atrous=atrous, init_method=init_method, bias_term=bias_term, is_pretrain=is_pretrain)\n    conv2 = conv_layer(conv1, height, width, x_stride, y_stride, filter_num, name=name + ""_conv2"", activation_method=""None"", alpha=alpha,\n                       padding=padding, atrous=atrous, init_method=init_method, bias_term=bias_term, is_pretrain=is_pretrain)\n    output = tf.add(input_data, conv2, name=name+""add"")\n\n    output = activation_layer(input_data=output,\n                              activation_method=activation_method,\n                              alpha=alpha)\n\n    return output\n'"
src/networks/audio_steganalysis.py,4,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2018.08.29\nFinished on 2018.08.29\nModified on 2018.11.12\n\n@author: Yuntao Wang\n""""""\n\nfrom layer import *\nfrom HPFs.filters import *\nfrom modules.inception import *\n\n\ndef chen_net(input_data, class_num=2):\n    """"""\n    Chen-Net for wav audio steganalysis (designer: Bolin Chen)\n    """"""\n    print(""Chen-Net: Steganalytic Network for wav audio (LSBM embedding)"")\n    print(""Network Structure: "")\n\n    # preprocessing\n    conv0 = diff_layer(input_data, is_diff=True, is_abs_diff=False, is_diff_abs=False,\n                       order=2, direction=""intra"", name=""diff_intra_2"", padding=""SAME"")\n\n    # Group 1\n    conv1_1 = conv_layer(conv0, 1, 5, 1, 1, 1, name=""conv1_1"", activation_method=None, padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 1, 1, 1, 1, 8, name=""conv1_2"", activation_method=None, padding=""SAME"")\n    pool1_3 = conv_layer(conv1_2, 1, 3, 1, 2, 8, name=""pool1_3"", activation_method=None, padding=""VALID"")\n\n    # Group 2\n    conv2_1 = conv_layer(pool1_3, 1, 5, 1, 1, 8, name=""conv2_1"", activation_method=None, padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 1, 1, 1, 1, 16, name=""conv2_2"", activation_method=None, padding=""SAME"")\n    pool2_3 = conv_layer(conv2_2, 1, 3, 1, 2, 16, name=""pool2_3"", activation_method=None, padding=""VALID"")\n\n    # Group 3\n    conv3_1 = conv_layer(pool2_3, 1, 5, 1, 1, 16, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 1, 1, 1, 1, 32, name=""conv3_2"", activation_method=""tanh"", padding=""SAME"")\n    pool3_3 = pool_layer(conv3_2, 1, 3, 1, 2, name=""pool3_3"", is_max_pool=True)\n\n    # Group 4\n    conv4_1 = conv_layer(pool3_3, 1, 5, 1, 1, 32, name=""conv4_1"", activation_method=""tanh"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 64, name=""conv4_2"", activation_method=""tanh"", padding=""SAME"")\n    pool4_3 = pool_layer(conv4_2, 1, 3, 1, 2, name=""pool4_3"", is_max_pool=True)\n\n    # Group 5\n    conv5_1 = conv_layer(pool4_3, 1, 5, 1, 1, 64, name=""conv5_1"", activation_method=""tanh"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 128, name=""conv5_2"", activation_method=""tanh"", padding=""SAME"")\n    pool5_3 = pool_layer(conv5_2, 1, 3, 1, 2, name=""pool5_3"", is_max_pool=True)\n\n    # Group 6\n    conv6_1 = conv_layer(pool5_3, 1, 5, 1, 1, 128, name=""conv6_1"", activation_method=""tanh"", padding=""SAME"")\n    conv6_2 = conv_layer(conv6_1, 1, 1, 1, 1, 256, name=""conv6_2"", activation_method=""tanh"", padding=""SAME"")\n    pool6_3 = pool_layer(conv6_2, 1, 3, 1, 2, name=""pool6_3"", is_max_pool=True)\n\n    # Group 7\n    conv7_1 = conv_layer(pool6_3, 1, 5, 1, 1, 256, name=""conv7_1"", activation_method=""tanh"", padding=""SAME"")\n    conv7_2 = conv_layer(conv7_1, 1, 1, 1, 1, 512, name=""conv7_2"", activation_method=""tanh"", padding=""SAME"")\n    pool7_3 = pool_layer(conv7_2, 1, 250, 1, 250, name=""global_average_pooling"", is_max_pool=False)\n\n    # Fully connected layer\n    logits = fc_layer(pool7_3, class_num, name=""fc8"", activation_method=None)\n\n    return logits\n\n\ndef wasdn(input_data, class_num=2, is_bn=True):\n    """"""\n    The proposed network\n    """"""\n    print(""WASDN: Wang Audio Steganalysis Deep Network"")\n    print(""Network Structure: "")\n\n    # preprocessing\n    conv0 = diff_layer(input_data=input_data, is_diff=True, is_diff_abs=False, is_abs_diff=False,\n                       order=2, direction=""inter"", name=""difference"", padding=""SAME"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group1\n    conv1_1 = conv_layer(conv0_input_merge, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""tanh"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 1, 1, 1, 1, 32, name=""conv1_2"", activation_method=""tanh"", padding=""SAME"")\n    pool1_3 = pool_layer(conv1_2, 2, 2, 2, 2, name=""pool1_3"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_3, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 1, 1, 1, 1, 64, name=""conv2_2"", activation_method=""tanh"", padding=""SAME"")\n    pool2_3 = pool_layer(conv2_2, 2, 2, 2, 2, name=""pool2_3"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_3, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 1, 1, 1, 1, 128, name=""conv3_2"", activation_method=""tanh"", padding=""SAME"")\n    pool3_3 = pool_layer(conv3_2, 2, 2, 2, 2, name=""pool3_3"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_3, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""tanh"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    bn4_3 = batch_normalization(conv4_2, name=""BN4_3"", activation_method=""tanh"", is_train=is_bn)\n    pool4_4 = pool_layer(bn4_3, 2, 2, 2, 2, name=""pool4_4"", is_max_pool=True)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_4, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=""tanh"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=None, padding=""SAME"")\n    bn5_3 = batch_normalization(conv5_2, name=""BN5_3"", activation_method=""tanh"", is_train=is_bn)\n    pool5_4 = pool_layer(bn5_3, 2, 2, 2, 2, name=""pool5_4"", is_max_pool=True)\n\n    # Group6\n    conv6_1 = conv_layer(pool5_4, 3, 3, 1, 1, 512, name=""conv6_1"", activation_method=""tanh"", padding=""SAME"")\n    conv6_2 = conv_layer(conv6_1, 1, 1, 1, 1, 1024, name=""conv6_2"", activation_method=None, padding=""SAME"")\n    bn6_3 = batch_normalization(conv6_2, name=""BN6_3"", activation_method=""tanh"", is_train=is_bn)\n    pool6_4 = pool_layer(bn6_3, 2, 2, 2, 2, name=""pool6_4"", is_max_pool=True)\n\n    # Fully connected layer\n    fc7 = fc_layer(pool6_4, 4096, name=""fc7"", activation_method=None)\n    bn8 = batch_normalization(fc7, name=""BN8"", activation_method=""tanh"", is_train=is_bn)\n    fc9 = fc_layer(bn8, 512, name=""fc9"", activation_method=None)\n    bn10 = batch_normalization(fc9, name=""BN10"", activation_method=""tanh"", is_train=is_bn)\n\n    logits = fc_layer(bn10, class_num, name=""fc11"", activation_method=None)\n\n    return logits\n\n\ndef rhfcn(input_data, class_num=2, is_bn=True):\n    """"""\n    Fully CNN for MP3 Steganalysis with Rich High-pass Filtering\n    """"""\n\n    print(""RHFCN: Fully CNN for MP3 Steganalysis with Rich High-pass Filtering"")\n    print(""Network Structure: "")\n\n    # High Pass Filtering\n    conv0 = rich_hpf_layer(input_data, name=""HPFs"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group1\n    conv1_1 = conv_layer(conv0_input_merge, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""tanh"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 3, 3, 1, 1, 32, name=""conv1_2"", activation_method=None, padding=""SAME"")\n    bn1_3 = batch_normalization(conv1_2, name=""BN1_3"", activation_method=""tanh"", is_train=is_bn)\n    pool1_4 = pool_layer(bn1_3, 2, 2, 2, 2, name=""pool1_4"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_4, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 3, 3, 1, 1, 64, name=""conv2_2"", activation_method=None, padding=""SAME"")\n    bn2_3 = batch_normalization(conv2_2, name=""BN2_3"", activation_method=""tanh"", is_train=is_bn)\n    pool2_4 = pool_layer(bn2_3, 2, 2, 2, 2, name=""pool2_4"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_4, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 3, 3, 1, 1, 128, name=""conv3_2"", activation_method=None, padding=""SAME"")\n    bn3_3 = batch_normalization(conv3_2, name=""BN3_3"", activation_method=""tanh"", is_train=is_bn)\n    pool3_4 = pool_layer(bn3_3, 2, 2, 2, 2, name=""pool3_4"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_4, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""tanh"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    bn4_3 = batch_normalization(conv4_2, name=""BN4_3"", activation_method=""tanh"", is_train=is_bn)\n    pool4_4 = pool_layer(bn4_3, 2, 2, 2, 2, name=""pool4_4"", is_max_pool=True)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_4, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=""tanh"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=None, padding=""SAME"")\n    bn5_3 = batch_normalization(conv5_2, name=""BN5_3"", activation_method=""tanh"", is_train=is_bn)\n    pool5_4 = pool_layer(bn5_3, 2, 2, 2, 2, name=""pool5_4"", is_max_pool=True)\n\n    # fully conv layer\n    fconv6 = conv_layer(pool5_4, 6, 14, 1, 1, 4096, name=""fconv6"", activation_method=None, padding=""VALID"")\n    bn7 = batch_normalization(fconv6, name=""BN7"", activation_method=""tanh"", is_train=is_bn)\n\n    fconv8 = conv_layer(bn7, 1, 1, 1, 1, 512, name=""fconv8"", activation_method=None, padding=""VALID"")\n    bn9 = batch_normalization(fconv8, name=""BN9"", activation_method=""tanh"", is_train=is_bn)\n\n    fconv10 = conv_layer(bn9, 1, 1, 1, 1, 2, name=""fconv10"", activation_method=None, padding=""VALID"")\n    bn11 = batch_normalization(fconv10, name=""BN11"", activation_method=""tanh"", is_train=is_bn)\n\n    # Global Max Pooling\n    bn11_shape = bn11.get_shape()\n    bn11_height, bn11_width = bn11_shape[1], bn11_shape[2]\n    pool11 = pool_layer(bn11, bn11_height, bn11_width, bn11_height, bn11_width, name=""global_max"", is_max_pool=True)\n\n    logits = tf.reshape(pool11, [-1, class_num])\n\n    return logits\n\n\ndef google_net3(input_data, class_num=2, is_bn=True):\n    """"""\n    Google Net with inception V3\n    """"""\n\n    print(""Google Net with inception V3"")\n    print(""Network Structure: "")\n\n    # High Pass Filtering\n    conv0 = rich_hpf_layer(input_data, name=""HPFs"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group 1\n    conv1_1 = inception_v3(conv0_input_merge, 8, ""conv1_1"", is_max_pool=True, is_bn=is_bn)\n    pool1_2 = pool_layer(conv1_1, 3, 3, 2, 2, name=""pool1_2"")\n\n    # Group 2\n    conv2_1 = inception_v3(pool1_2, 16, ""conv2_1"", is_max_pool=True, is_bn=is_bn)\n    pool2_2 = pool_layer(conv2_1, 3, 3, 2, 2, name=""pool2_2"")\n\n    # Group 3\n    conv3_1 = inception_v3(pool2_2, 32, ""conv3_1"", is_max_pool=True, is_bn=is_bn)\n    pool3_2 = pool_layer(conv3_1, 3, 3, 2, 2, name=""pool3_2"")\n\n    # Group 4\n    conv4_1 = inception_v3(pool3_2, 64, ""conv4_1"", is_max_pool=True, is_bn=is_bn)\n    pool4_2 = pool_layer(conv4_1, 3, 3, 2, 2, name=""pool4_2"")\n\n    # Group 5\n    conv5_1 = inception_v3(pool4_2, 128, ""conv5_1"", is_max_pool=True, is_bn=is_bn)\n    pool5_2 = pool_layer(conv5_1, 3, 3, 2, 2, name=""pool5_2"")\n\n    # Group 6\n    conv6_1 = inception_v3(pool5_2, 256, ""conv6_1"", is_max_pool=True, is_bn=is_bn)\n    pool6_2 = global_pool(conv6_1, name=""global_pool6_2"")\n\n    # fc layers\n    fc6 = fc_layer(pool6_2, 128, name=""fc6"", activation_method=None)\n    bn7 = batch_normalization(fc6, name=""BN7"", activation_method=""tanh"", is_train=is_bn)\n\n    logits = fc_layer(bn7, class_num, name=""fc8"", activation_method=None)\n\n    return logits'"
src/networks/image_classification.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2018.08.19\nFinished on 2018.08.19\nModified on \n\n@author: Yuntao Wang\n""""""\n\nfrom layer import *\n\n\n""""""\n    function:\n        le_net: Le-Net for image classification\n        vgg16 : vgg16 for image classification\n        vgg19 : vgg19 for image classification\n""""""\n\n\ndef le_net(input_data, class_num=10):\n    """"""\n    Le-Net for image classification\n    """"""\n    print(""le_net: Remove the 1x1 conv layers."")\n    print(""Network Structure: "")\n\n    # Group1\n    conv1_1 = conv_layer(input_data, 5, 5, 1, 1, 6, ""conv1"", ""VALID"")\n    pool1_2 = pool_layer(conv1_1, 2, 2, 2, 2, ""pool1_2"")\n\n    # Group2\n    conv2_1 = conv_layer(pool1_2, 5, 5, 1, 1, 16, ""conv2_1"", ""VALID"")\n    pool2_2 = pool_layer(conv2_1, 2, 2, 2, 2, ""pool2_2"")\n\n    # Fully connected layer\n    fc4 = fc_layer(pool2_2, 120, ""fc4"", ""relu"")\n    fc5 = fc_layer(fc4, 84, ""fc5"", False)\n    fc5_drop = dropout(fc5, keep_pro=0.5, name=""fc5_drop"")\n    logits = fc_layer(fc5_drop, class_num, ""fc6"", False)\n\n    return logits\n\n\ndef vgg16(input_data, class_num=4096):\n    """"""\n    vgg16 for image classification\n    """"""\n    print(""vgg16: Remove the 1x1 conv layers."")\n    print(""Network Structure: "")\n\n    # vgg16\n    conv1_1 = conv_layer(input_data, 3, 3, 1, 1, 64, ""conv1_1"")\n    conv1_2 = conv_layer(conv1_1, 3, 3, 1, 1, 64, ""conv1_2"")\n    pool1_3 = pool_layer(conv1_2, 2, 2, 2, 2, ""pool1_3"")\n\n    conv2_1 = conv_layer(pool1_3, 3, 3, 1, 1, 128, ""conv2_1"")\n    conv2_2 = conv_layer(conv2_1, 3, 3, 1, 1, 128, ""conv2_2"")\n    pool2_3 = pool_layer(conv2_2, 2, 2, 2, 2, ""pool2_3"")\n\n    conv3_1 = conv_layer(pool2_3, 3, 3, 1, 1, 256, ""conv3_1"")\n    conv3_2 = conv_layer(conv3_1, 3, 3, 1, 1, 256, ""conv3_2"")\n    conv3_3 = conv_layer(conv3_2, 3, 3, 1, 1, 256, ""conv3_3"")\n    pool3_4 = pool_layer(conv3_3, 2, 2, 2, 2, ""pool3_4"")\n\n    conv4_1 = conv_layer(pool3_4, 3, 3, 1, 1, 512, ""conv4_1"")\n    conv4_2 = conv_layer(conv4_1, 3, 3, 1, 1, 512, ""conv4_2"")\n    conv4_3 = conv_layer(conv4_2, 3, 3, 1, 1, 512, ""conv4_3"")\n    pool4_4 = pool_layer(conv4_3, 2, 2, 2, 2, ""pool4_4"")\n\n    conv5_1 = conv_layer(pool4_4, 3, 3, 1, 1, 512, ""conv5_1"")\n    conv5_2 = conv_layer(conv5_1, 3, 3, 1, 1, 512, ""conv5_2"")\n    conv5_3 = conv_layer(conv5_2, 3, 3, 1, 1, 512, ""conv5_3"")\n    pool5_4 = pool_layer(conv5_3, 2, 2, 2, 2, ""pool5_4"")\n\n    fc6 = fc_layer(pool5_4, 4096, ""fc6"")\n    fc6_drop = dropout(fc6, keep_pro=0.5, name=""fc6_drop"")\n    fc7 = fc_layer(fc6_drop, 4096, ""fc7"")\n    fc7_drop = dropout(fc7, keep_pro=0.5, name=""fc7_drop"")\n    logits = fc_layer(fc7_drop, class_num, ""fc8"")\n\n    return logits\n\n\ndef vgg19(input_data, class_num=4096):\n    """"""\n    vgg19 for image classification\n    """"""\n    print(""vgg19: Remove the 1x1 conv layers."")\n    print(""Network Structure: "")\n\n    # vgg19\n    conv1_1 = conv_layer(input_data, 3, 3, 1, 1, 64, ""conv1_1"")\n    conv1_2 = conv_layer(conv1_1, 3, 3, 1, 1, 64, ""conv1_2"")\n    pool1_3 = pool_layer(conv1_2, 2, 2, 2, 2, ""pool1_3"")\n\n    conv2_1 = conv_layer(pool1_3, 3, 3, 1, 1, 128, ""conv2_1"")\n    conv2_2 = conv_layer(conv2_1, 3, 3, 1, 1, 128, ""conv2_2"")\n    pool2_3 = pool_layer(conv2_2, 2, 2, 2, 2, ""pool2_3"")\n\n    conv3_1 = conv_layer(pool2_3, 3, 3, 1, 1, 256, ""conv3_1"")\n    conv3_2 = conv_layer(conv3_1, 3, 3, 1, 1, 256, ""conv3_2"")\n    conv3_3 = conv_layer(conv3_2, 3, 3, 1, 1, 256, ""conv3_3"")\n    conv3_4 = conv_layer(conv3_3, 3, 3, 1, 1, 256, ""conv3_4"")\n    pool3_5 = pool_layer(conv3_4, 2, 2, 2, 2, ""pool3_5"")\n\n    conv4_1 = conv_layer(pool3_5, 3, 3, 1, 1, 512, ""conv4_1"")\n    conv4_2 = conv_layer(conv4_1, 3, 3, 1, 1, 512, ""conv4_2"")\n    conv4_3 = conv_layer(conv4_2, 3, 3, 1, 1, 512, ""conv4_3"")\n    conv4_4 = conv_layer(conv4_3, 3, 3, 1, 1, 512, ""conv4_4"")\n    pool4_5 = pool_layer(conv4_4, 2, 2, 2, 2, ""pool4_5"")\n\n    conv5_1 = conv_layer(pool4_5, 3, 3, 1, 1, 512, ""conv5_1"")\n    conv5_2 = conv_layer(conv5_1, 3, 3, 1, 1, 512, ""conv5_2"")\n    conv5_3 = conv_layer(conv5_2, 3, 3, 1, 1, 512, ""conv5_3"")\n    conv5_4 = conv_layer(conv5_3, 3, 3, 1, 1, 512, ""conv5_4"")\n    pool5_5 = pool_layer(conv5_4, 2, 2, 2, 2, ""pool5_5"")\n\n    fc6 = fc_layer(pool5_5, 4096, ""fc6"")\n    fc6_drop = dropout(fc6, keep_pro=0.5, name=""fc6_drop"")\n    fc7 = fc_layer(fc6_drop, 4096, ""fc7"")\n    fc7_drop = dropout(fc7, keep_pro=0.5, name=""fc7_drop"")\n    logits = fc_layer(fc7_drop, class_num, ""fc8"")\n\n    return logits\n'"
src/networks/image_steganalysis.py,5,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2018.08.29\nFinished on 2018.08.29\nModified on 2018.11.12\n\n@author: Yuntao Wang\n""""""\n\nfrom layer import *\n\n""""""\n    function:\n        s_xu_net: Xu-Net for spatial image steganalysis\n        ye_net: Ye-Net for spatial image steganalysis\n        yedroudj_net: Yedroudj-Net for spatial image steganalysis\n        j_xu_net: Xu-Net for JPEG image steganalysis\n""""""\n\n\ndef s_xu_net(input_data, class_num=2, is_bn=True):\n    """"""\n    Xu-Net for spatial image steganalysis\n    """"""\n    print(""S_Xu-Net: Spatial Image Steganalytic Network."")\n    print(""Network Structure: "")\n\n    # Group 0\n    conv0 = static_conv_layer(input_data, kv_kernel_generator(), 1, 1, ""HPF"")\n\n    # Group 1\n    conv1_1 = conv_layer(conv0, 5, 5, 1, 1, 8, ""conv1_1"", activation_method=None, init_method=""gaussian"", bias_term=False)\n    conv1_2 = tf.abs(conv1_1, ""conv1_abs"")\n    conv1_3 = batch_normalization(conv1_2, name=""conv1_BN"", activation_method=""tanh"", is_train=is_bn)\n    pool1_4 = pool_layer(conv1_3, 5, 5, 2, 2, ""pool1_4"", is_max_pool=False)\n\n    # Group 2\n    conv2_1 = conv_layer(pool1_4, 5, 5, 1, 1, 16, ""conv2_1"", activation_method=None, init_method=""gaussian"", bias_term=False)\n    bn2_2 = batch_normalization(conv2_1, name=""BN2_2"", activation_method=""tanh"", is_train=is_bn)\n    pool2_3 = pool_layer(bn2_2, 5, 5, 2, 2, name=""pool2_3"", is_max_pool=False)\n\n    # Group 3\n    conv3_1 = conv_layer(pool2_3, 1, 1, 1, 1, 32, ""conv3_1"", activation_method=None, init_method=""gaussian"", bias_term=False)\n    bn3_2 = batch_normalization(conv3_1, activation_method=""relu"", name=""BN3_2"", is_train=is_bn)\n    pool3_3 = pool_layer(bn3_2, 5, 5, 2, 2, ""pool3_3"", is_max_pool=False)\n\n    # Group 4\n    conv4_1 = conv_layer(pool3_3, 1, 1, 1, 1, 64, ""conv4_1"", activation_method=None, init_method=""gaussian"", bias_term=False)\n    bn4_2 = batch_normalization(conv4_1, activation_method=""relu"", name=""BN4_2"", is_train=is_bn)\n    pool4_3 = pool_layer(bn4_2, 5, 5, 2, 2, ""pool4_3"", is_max_pool=False)\n\n    # Group 5\n    conv5_1 = conv_layer(pool4_3, 1, 1, 1, 1, 128, ""conv5_1"", activation_method=None, init_method=""gaussian"", bias_term=False)\n    bn5_2 = batch_normalization(conv5_1, activation_method=""relu"", name=""BN5_2"", is_train=is_bn)\n    pool5_3 = pool_layer(bn5_2, 32, 32, 2, 2, ""pool5_3"", is_max_pool=False)\n\n    # Fully connected layer\n    logits = fc_layer(pool5_3, class_num, ""fc6"", activation_method=None)\n\n    return logits\n\n\ndef ye_net(input_data, class_num=2):\n    """"""\n    Ye-Net for spatial image steganalysis\n    """"""\n    print(""Ye-Net: Spatial Image Steganalytic Network."")\n    print(""Network Structure: "")\n\n    # Group 1\n    conv1_1 = conv_layer(input_data, 5, 5, 1, 1, 30, name=""conv1_1"", activation_method=None, padding=""SAME"")\n    conv1_1 = tf.clip_by_value(conv1_1, clip_value_min=-3, clip_value_max=3, name=""TLU"")\n\n    # Group 2\n    conv2_1 = conv_layer(conv1_1, 3, 3, 1, 1, 30, name=""conv2_1"", activation_method=""relu"")\n\n    # Group 3\n    conv3_1 = conv_layer(conv2_1, 3, 3, 1, 1, 30, name=""conv3_1"", activation_method=""relu"")\n\n    # Group 4\n    conv4_1 = conv_layer(conv3_1, 3, 3, 1, 1, 30, name=""conv4_1"", activation_method=""relu"")\n    pool4_2 = pool_layer(conv4_1, 2, 2, 2, 2, name=""pool4_2"", is_max_pool=False)\n\n    # Group 5\n    conv5_1 = conv_layer(pool4_2, 5, 5, 1, 1, 32, name=""conv5_1"", activation_method=""relu"")\n    pool5_2 = pool_layer(conv5_1, 3, 3, 2, 2, name=""pool5_2"", is_max_pool=False)\n\n    # Group 6\n    conv6_1 = conv_layer(pool5_2, 5, 5, 1, 1, 32, name=""conv6_1"", activation_method=""relu"")\n    pool6_2 = pool_layer(conv6_1, 3, 3, 2, 2, name=""pool6_2"", is_max_pool=False)\n\n    # Group 7\n    conv7_1 = conv_layer(pool6_2, 5, 5, 1, 1, 32, name=""conv7_1"", activation_method=""relu"")\n    pool7_2 = pool_layer(conv7_1, 3, 3, 2, 2, name=""pool7_2"", is_max_pool=False)\n\n    # Group 8\n    conv8_1 = conv_layer(pool7_2, 3, 3, 1, 1, 16, name=""conv8_1"", activation_method=""relu"")\n\n    # Group 9\n    conv9_1 = conv_layer(conv8_1, 3, 3, 3, 3, 16, name=""conv9_1"", activation_method=""relu"")\n\n    # Fully connected layer\n    logits = fc_layer(conv9_1, class_num, name=""fc10"", activation_method=None)\n\n    return logits\n\n\ndef yedroudj_net(input_data, class_num=2, is_bn=True):\n    """"""\n    Yedroudj-Net for spatial image steganalysis\n    """"""\n    print(""Yedroudj-Net: Spatial Image Steganalytic Network."")\n    print(""Network Structure: "")\n\n    # Group 0\n    conv0 = static_conv_layer(input_data, srm_kernels_generator(), 1, 1, ""SRMs"", padding=""SAME"")\n\n    # Group 1\n    conv1_1 = conv_layer(conv0, 5, 5, 1, 1, 30, ""conv1_1"", padding=""SAME"", activation_method=None)\n    conv1_1_abs = tf.abs(conv1_1, name=""conv1_1_abs"")\n    bn1_2 = batch_normalization(conv1_1_abs, name=""BN1_2"", is_train=is_bn)\n    trunc1_3 = tf.clip_by_value(bn1_2, clip_value_min=-3, clip_value_max=3, name=""Trunc1_3"")\n\n    # Group 2\n    conv2_1 = conv_layer(trunc1_3, 5, 5, 1, 1, 30, ""conv2_1"", padding=""SAME"", activation_method=None)\n    bn2_2 = batch_normalization(conv2_1, name=""BN2_2"", is_train=is_bn)\n    trunc2_3 = tf.clip_by_value(bn2_2, clip_value_min=-2, clip_value_max=2, name=""Trunc2_3"")\n    pool2_4 = pool_layer(trunc2_3, 5, 5, 2, 2, name=""pool2_4"")\n\n    # Group 3\n    conv3_1 = conv_layer(pool2_4, 5, 5, 1, 1, 32, ""conv3_1"", padding=""SAME"", activation_method=None)\n    bn3_2 = batch_normalization(conv3_1, name=""BN3_2"", activation_method=""relu"", is_train=is_bn)\n    pool3_3 = pool_layer(bn3_2, 5, 5, 2, 2, name=""pool3_3"", is_max_pool=False)\n\n    # Group 4\n    conv4_1 = conv_layer(pool3_3, 5, 5, 1, 1, 64, ""conv5_1"", padding=""SAME"", activation_method=None)\n    bn4_2 = batch_normalization(conv4_1, name=""BN5_2"", activation_method=""relu"", is_train=is_bn)\n    pool4_3 = pool_layer(bn4_2, 5, 5, 2, 2, name=""pool5_3"", is_max_pool=False)\n\n    # Group 5\n    conv5_1 = conv_layer(pool4_3, 5, 5, 1, 1, 128, ""conv6_1"", padding=""SAME"", activation_method=None)\n    bn5_2 = batch_normalization(conv5_1, name=""BN6_2"", activation_method=""relu"", is_train=is_bn)\n    pool5_3 = pool_layer(bn5_2, 5, 5, 2, 2, name=""pool6_3"", is_max_pool=False)\n\n    # Fully connected layer\n    fc6 = fc_layer(pool5_3, 256, name=""fc6"", activation_method=""relu"")\n    fc7 = fc_layer(fc6, 1024, name=""fc7"", activation_method=""relu"")\n    logits = fc_layer(fc7, class_num, name=""fc8"", activation_method=None)\n\n    return logits\n\n\ndef j_xu_net(input_data, class_num=2, is_bn=True):\n    """"""\n    Xu-Net for jpeg image steganalysis\n    """"""\n    print(""J_Xu-Net: JPEG Image Steganalytic Network."")\n    print(""Network Structure: "")\n\n    # Group\n    dct_to_spatial = static_conv_layer(input_data, dct_kernel_generator(4), 1, 1, name=""dct_4"", padding=""SAME"")\n\n'"
src/networks/networks.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2018.09.17\nFinished on 2018.09.17\nModified on 2019.03.05\n\n@author: Yuntao Wang\n""""""\n\n""""""\n    When you design a new network, write the name into this file for a successful running.\n""""""\n\n# networks for experiments\nrnn_network = [""rnn_lstm"", ""rnn_gru"", ""rnn_bi_lstm""]\naudio_steganalysis_ih_mmsec = [""wasdn1_1"", ""wasdn1_2"", ""wasdn1_3"", ""wasdn1_4"", ""wasdn1_5"", ""wasdn1_6"", ""wasdn1_7"", ""wasdn1_8"", ""wasdn1_9"",\n                               ""wasdn2_1"", ""wasdn2_2"", ""wasdn2_3""]\naudio_steganalysis_icassp = [""rhfcn1_1"", ""rhfcn1_2"", ""rhfcn1_3""]\naudio_steganalysis_dense_net = [""dense_net_mp3"", ""dense_net_mp3_42"", ""dense_net_mp3_18"", ""dense_net_mp3_6""]\naudio_steganalysis_dissertation = [""chap4"", ""chap4_1"", ""chap4_2"", ""chap4_3"", ""chap4_4"", ""chap4_5"", ""chap4_6"", ""chap4_7"", ""chap4_8"", ""chap4_9"", ""chap4_10"",\n                                   ""mfcc_net"", ""mfcc_net1"", ""mfcc_net2"", ""mfcc_net3"", ""mfcc_net4"",\n                                   ""google_net"", ""google_net1"", ""google_net2"", ""google_net3"", ""google_net4"", ""google_net5""]\n\n# the proposed networks\naudio_steganalysis = [""wasdn"", ""rhfcn""]\nimage_steganalysis = [""s_xu_net"", ""j_xu_net""]\nimage_classification = [""le_net"", ""vgg16"", ""vgg19""]\n\n# list of networks\nnetworks = []\n\n# networks for audio steganalysis\nnetworks.extend(audio_steganalysis)\nnetworks.extend(audio_steganalysis_ih_mmsec)\nnetworks.extend(audio_steganalysis_icassp)\nnetworks.extend(audio_steganalysis_dense_net)\nnetworks.extend(audio_steganalysis_dissertation)\nnetworks.extend(rnn_network)\n\n# networks for image steganalysis\nnetworks.extend(image_steganalysis)\n\n# networks for image classification\nnetworks.extend(image_classification)\n'"
src/networks/networks_test.py,19,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2018.11.12\nFinished on 2018.11.12\nModified on\n\n@author: Yuntao Wang\n""""""\n\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nmnist = input_data.read_data_sets(""data/"", one_hot=True)\ntrain_x, train_y, rest_x, test_y = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n\n# hyper parameters\nlr = 0.001\nbatch_size = 128\ntraining_iters = 100000\n\n# \xe4\xb8\xba\xe4\xba\x86\xe4\xbd\xbf\xe7\x94\xa8RNN\xe6\x9d\xa5\xe5\x88\x86\xe7\xb1\xbb\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe6\x8a\x8a\xe6\xaf\x8f\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe8\xa1\x8c\xe7\x9c\x8b\xe6\x88\x90\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\x83\x8f\xe7\xb4\xa0\xe5\xba\x8f\xe5\x88\x97\xef\xbc\x88sequence\xef\xbc\x89\xe3\x80\x82\xe5\x9b\xa0\xe4\xb8\xbaMNIST\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xe6\x98\xaf28\xc3\x9728\xe5\x83\x8f\xe7\xb4\xa0\xef\xbc\x8c\n# \xe6\x89\x80\xe4\xbb\xa5\xe6\x88\x91\xe4\xbb\xac\xe6\x8a\x8a\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe5\x9b\xbe\xe5\x83\x8f\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9c\x8b\xe6\x88\x90\xe4\xb8\x80\xe8\xa1\x8c\xe8\xa1\x8c\xe7\x9a\x84\xe5\xba\x8f\xe5\x88\x97\xe3\x80\x82\xe5\x9b\xa0\xe6\xad\xa4\xef\xbc\x8c\xe5\x85\xb1\xe6\x9c\x89\xef\xbc\x8828\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe7\x9a\x84\xe5\xba\x8f\xe5\x88\x97\xef\xbc\x89\xc3\x97\xef\xbc\x8828\xe8\xa1\x8c\xef\xbc\x89\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe6\xaf\x8f\xe4\xb8\x80\xe6\xad\xa5\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\xba\x8f\xe5\x88\x97\xe9\x95\xbf\xe5\xba\xa6\xe6\x98\xaf28\xef\xbc\x8c\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe6\xad\xa5\xe6\x95\xb0\xe6\x98\xaf28\xe6\xad\xa5\n# \xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\nn_inputs = 28           # \xe8\xbe\x93\xe5\x85\xa5\xe5\xb1\x82\xe7\x9a\x84n\nn_steps = 28            # 28\xe9\x95\xbf\xe5\xba\xa6\nn_hidden_units = 128    # \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe4\xb8\xaa\xe6\x95\xb0\nn_classes = 10          # \xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\xef\xbc\x8c\xe5\x8d\xb3\xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab\xef\xbc\x8c0\xef\xbd\x9e9\xe4\xb8\xaa\xe6\x95\xb0\xe5\xad\x97\xef\xbc\x8c\xe5\x85\xb1\xe6\x9c\x8910\xe4\xb8\xaa\n\n# placeholder\nx = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\ny = tf.placeholder(tf.float32, [None, n_classes])\n\n# \xe5\x8f\x82\xe6\x95\xb0\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\nweights = {\n    # (28, 128)\n    ""in"": tf.Variable(tf.random_normal([n_inputs, n_hidden_units])),\n    # (128, 10)\n    ""out"": tf.Variable(tf.random_normal([n_hidden_units, n_classes]))\n}\nbiases = {\n    # (128, )\n    ""in"": tf.Variable(tf.constant(0.1, shape=[n_hidden_units, ])),\n    # (10, )\n    ""out"": tf.Variable(tf.constant(0.1, shape=[n_classes, ]))\n}\n\n\n# \xe5\xae\x9a\xe4\xb9\x89RNN\xe6\xa8\xa1\xe5\x9e\x8b\ndef network(_input_data, _weights, _biases):\n    # \xe6\x8a\x8a\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84X\xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90X ==> (128 batch * 28 steps, 28 inputs)\n    _input_data = tf.reshape(_input_data, [-1, n_inputs])\n\n    # \xe8\xbf\x9b\xe5\x85\xa5\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\n    # X_in = (128 batch * 28 steps, 128 hidden)\n    x_in = tf.matmul(_input_data, weights[\'in\']) + biases[\'in\']\n    # X_in ==> (128 batch, 28 steps, 128 hidden)\n    x_in = tf.reshape(x_in, [-1, n_steps, n_hidden_units])\n    # \xe8\xbf\x99\xe9\x87\x8c\xe9\x87\x87\xe7\x94\xa8\xe5\x9f\xba\xe6\x9c\xac\xe7\x9a\x84LSTM\xe5\xbe\xaa\xe7\x8e\xaf\xe7\xbd\x91\xe7\xbb\x9c\xe5\x8d\x95\xe5\x85\x83\xef\xbc\x9abasic LSTM Cell\n    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden_units, forget_bias=1.0, state_is_tuple=True)\n    # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xb8\xba\xe9\x9b\xb6\xe5\x80\xbc\xef\xbc\x8clstm\xe5\x8d\x95\xe5\x85\x83\xe7\x94\xb1\xe4\xb8\xa4\xe4\xb8\xaa\xe9\x83\xa8\xe5\x88\x86\xe7\xbb\x84\xe6\x88\x90\xef\xbc\x9a(c_state, h_state)\n    init_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)\n\n    # dynamic_rnn \xe6\x8e\xa5\xe6\x94\xb6\xe5\xbc\xa0\xe9\x87\x8f(batch, steps, inputs)\xe6\x88\x96\xe8\x80\x85(steps, batch, inputs)x_in\n    outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, x_in, initial_state=init_state, time_major=False)\n    results = tf.matmul(final_state[1], _weights[\'out\']) + _biases[\'out\']\n\n    return results\n\n\n# \xe5\xae\x9a\xe4\xb9\x89\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe5\x92\x8c\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xef\xbc\x8c\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe9\x87\x87\xe7\x94\xa8AdamOptimizer\npred = network(x, weights, biases)\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\ntrain_op = tf.train.AdamOptimizer(lr).minimize(cost)\n\n# \xe5\xae\x9a\xe4\xb9\x89\xe6\xa8\xa1\xe5\x9e\x8b\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\xe5\x8f\x8a\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe8\xae\xa1\xe7\xae\x97\xe6\x96\xb9\xe6\xb3\x95\ncorrect_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n# \xe5\x9c\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe4\xbc\x9a\xe8\xaf\x9d\xe4\xb8\xad\xe5\x90\xaf\xe5\x8a\xa8\xe5\x9b\xbe\xef\xbc\x8c\xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe6\xaf\x8f20\xe6\xac\xa1\xe8\xbe\x93\xe5\x87\xba1\xe6\xac\xa1\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    step = 0\n    while step * batch_size < training_iters:\n        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n        batch_xs = batch_xs.reshape([batch_size, n_steps, n_inputs])\n        sess.run([train_op], feed_dict={x: batch_xs, y: batch_ys})\n        if step % 20 == 0:\n            print(""Step-"", step, sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys}))\n        step += 1\n'"
src/networks/tested_steganalysis.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2018.09.25\nFinished on 2018.09.25\nModified on\n\n@author: Yuntao Wang\n""""""\n\nfrom networks.tested_networks.IH_MMSec import *\nfrom networks.tested_networks.ICASSP import *\nfrom networks.tested_networks.RNN import *\nfrom networks.tested_networks.dense_net import *\nfrom networks.tested_networks.dissertation import *\n'"
src/networks/tested_networks/ICASSP.py,2,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2018.10.08\nFinished on 2018.10.08\nModified on 2019.01.08\n\n@author: Yuntao Wang\n""""""\n\nfrom layer import *\nfrom HPFs.filters import *\n\n\ndef rhfcn1_1(input_data, class_num=2, is_bn=True):\n    """"""\n    Remove High-Pass Filtering Module\n    """"""\n\n    print(""RHFCN1_1: Remove High-Pass Filtering Module"")\n    print(""Network Structure: "")\n\n    # Group1\n    conv1_1 = conv_layer(input_data, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""tanh"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 3, 3, 1, 1, 32, name=""conv1_2"", activation_method=None, padding=""SAME"")\n    bn1_3 = batch_normalization(conv1_2, name=""BN1_3"", activation_method=""tanh"", is_train=is_bn)\n    pool1_4 = pool_layer(bn1_3, 2, 2, 2, 2, name=""pool1_4"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_4, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 3, 3, 1, 1, 64, name=""conv2_2"", activation_method=None, padding=""SAME"")\n    bn2_3 = batch_normalization(conv2_2, name=""BN2_3"", activation_method=""tanh"", is_train=is_bn)\n    pool2_4 = pool_layer(bn2_3, 2, 2, 2, 2, name=""pool2_4"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_4, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 3, 3, 1, 1, 128, name=""conv3_2"", activation_method=None, padding=""SAME"")\n    bn3_3 = batch_normalization(conv3_2, name=""BN3_3"", activation_method=""tanh"", is_train=is_bn)\n    pool3_4 = pool_layer(bn3_3, 2, 2, 2, 2, name=""pool3_4"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_4, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""tanh"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    bn4_3 = batch_normalization(conv4_2, name=""BN4_3"", activation_method=""tanh"", is_train=is_bn)\n    pool4_4 = pool_layer(bn4_3, 2, 2, 2, 2, name=""pool4_4"", is_max_pool=True)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_4, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=""tanh"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=None, padding=""SAME"")\n    bn5_3 = batch_normalization(conv5_2, name=""BN5_3"", activation_method=""tanh"", is_train=is_bn)\n    pool5_4 = pool_layer(bn5_3, 2, 2, 2, 2, name=""pool5_4"", is_max_pool=True)\n\n    # fully conv layer\n    fconv6 = conv_layer(pool5_4, 6, 14, 1, 1, 4096, name=""fconv6"", activation_method=None, padding=""VALID"")\n    bn7 = batch_normalization(fconv6, name=""BN7"", activation_method=activation_method, is_train=is_bn)\n\n    fconv8 = conv_layer(bn7, 1, 1, 1, 1, 512, name=""fconv8"", activation_method=None, padding=""VALID"")\n    bn9 = batch_normalization(fconv8, name=""BN9"", activation_method=activation_method, is_train=is_bn)\n\n    fconv10 = conv_layer(bn9, 1, 1, 1, 1, 2, name=""fconv10"", activation_method=None, padding=""VALID"")\n    bn11 = batch_normalization(fconv10, name=""BN11"", activation_method=None, is_train=is_bn)\n\n    # Global Max Pooling\n    bn11_shape = bn11.get_shape()\n    bn11_height, bn11_width = bn11_shape[1], bn11_shape[2]\n    pool11 = pool_layer(bn11, bn11_height, bn11_width, bn11_height, bn11_width, name=""global_max"", is_max_pool=True)\n\n    logits = tf.reshape(pool11, [-1, class_num])\n\n    return logits\n\n\ndef rhfcn1_2(input_data, class_num=2, is_bn=True):\n    """"""\n    Quit replacing fully connected layers with fully convolutional layer\n    """"""\n\n    print(""RHFCN1_2: Quit replacing fully connected layers with fully convolutional layer"")\n    print(""Network Structure: "")\n\n    # High Pass Filtering\n    conv0 = rich_hpf_layer(input_data, name=""HPFs"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group1\n    conv1_1 = conv_layer(conv0_input_merge, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""tanh"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 3, 3, 1, 1, 32, name=""conv1_2"", activation_method=None, padding=""SAME"")\n    bn1_3 = batch_normalization(conv1_2, name=""BN1_3"", activation_method=""tanh"", is_train=is_bn)\n    pool1_4 = pool_layer(bn1_3, 2, 2, 2, 2, name=""pool1_4"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_4, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 3, 3, 1, 1, 64, name=""conv2_2"", activation_method=None, padding=""SAME"")\n    bn2_3 = batch_normalization(conv2_2, name=""BN2_3"", activation_method=""tanh"", is_train=is_bn)\n    pool2_4 = pool_layer(bn2_3, 2, 2, 2, 2, name=""pool2_4"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_4, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 3, 3, 1, 1, 128, name=""conv3_2"", activation_method=None, padding=""SAME"")\n    bn3_3 = batch_normalization(conv3_2, name=""BN3_3"", activation_method=""tanh"", is_train=is_bn)\n    pool3_4 = pool_layer(bn3_3, 2, 2, 2, 2, name=""pool3_4"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_4, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""tanh"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    bn4_3 = batch_normalization(conv4_2, name=""BN4_3"", activation_method=""tanh"", is_train=is_bn)\n    pool4_4 = pool_layer(bn4_3, 2, 2, 2, 2, name=""pool4_4"", is_max_pool=True)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_4, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=""tanh"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=None, padding=""SAME"")\n    bn5_3 = batch_normalization(conv5_2, name=""BN5_3"", activation_method=""tanh"", is_train=is_bn)\n    pool5_4 = pool_layer(bn5_3, 2, 2, 2, 2, name=""pool5_4"", is_max_pool=True)\n\n    # Fully connected layer\n    fc6 = fc_layer(pool5_4, 4096, name=""fc6"", activation_method=None)\n    bn7 = batch_normalization(fc6, name=""BN7"", activation_method=""tanh"", is_train=is_bn)\n    fc8 = fc_layer(bn7, 512, name=""fc8"", activation_method=None)\n    bn9 = batch_normalization(fc8, name=""BN9"", activation_method=""tanh"", is_train=is_bn)\n\n    logits = fc_layer(bn9, class_num, name=""fc10"", activation_method=None)\n\n    return logits\n\n\ndef rhfcn1_3(input_data, class_num=2, is_bn=True):\n    """"""\n    Remove rich HPF module and quit removing fc layers at the same time\n    """"""\n\n    print(""RHFCN1_3: Remove rich HPF module and quit removing fc layers at the same time"")\n    print(""Network Structure: "")\n\n    # Group1\n    conv1_1 = conv_layer(input_data, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""tanh"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 3, 3, 1, 1, 32, name=""conv1_2"", activation_method=None, padding=""SAME"")\n    bn1_3 = batch_normalization(conv1_2, name=""BN1_3"", activation_method=""tanh"", is_train=is_bn)\n    pool1_4 = pool_layer(bn1_3, 2, 2, 2, 2, name=""pool1_4"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_4, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 3, 3, 1, 1, 64, name=""conv2_2"", activation_method=None, padding=""SAME"")\n    bn2_3 = batch_normalization(conv2_2, name=""BN2_3"", activation_method=""tanh"", is_train=is_bn)\n    pool2_4 = pool_layer(bn2_3, 2, 2, 2, 2, name=""pool2_4"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_4, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 3, 3, 1, 1, 128, name=""conv3_2"", activation_method=None, padding=""SAME"")\n    bn3_3 = batch_normalization(conv3_2, name=""BN3_3"", activation_method=""tanh"", is_train=is_bn)\n    pool3_4 = pool_layer(bn3_3, 2, 2, 2, 2, name=""pool3_4"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_4, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""tanh"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    bn4_3 = batch_normalization(conv4_2, name=""BN4_3"", activation_method=""tanh"", is_train=is_bn)\n    pool4_4 = pool_layer(bn4_3, 2, 2, 2, 2, name=""pool4_4"", is_max_pool=True)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_4, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=""tanh"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=None, padding=""SAME"")\n    bn5_3 = batch_normalization(conv5_2, name=""BN5_3"", activation_method=""tanh"", is_train=is_bn)\n    pool5_4 = pool_layer(bn5_3, 2, 2, 2, 2, name=""pool5_4"", is_max_pool=True)\n\n    # Fully connected layer\n    fc6 = fc_layer(pool5_4, 4096, name=""fc6"", activation_method=None)\n    bn7 = batch_normalization(fc6, name=""BN7"", activation_method=""tanh"", is_train=is_bn)\n    fc8 = fc_layer(bn7, 512, name=""fc8"", activation_method=None)\n    bn9 = batch_normalization(fc8, name=""BN9"", activation_method=""tanh"", is_train=is_bn)\n\n    logits = fc_layer(bn9, class_num, name=""fc10"", activation_method=None)\n\n    return logits\n'"
src/networks/tested_networks/IH_MMSec.py,11,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-33\n\n""""""\nCreated on 2018.09.25\nFinished on 2018.09.25\nModified on 2019.01.07\n\n@author: Yuntao Wang\n""""""\n\nfrom layer import *\nfrom HPFs.filters import *\n\n\ndef wasdn1_1(input_data, class_num=2):\n    """"""\n    Remove all batch normalization layers\n    """"""\n    print(""WASDN1_1: Remove the BN layer"")\n    print(""Network Structure: "")\n\n    # preprocessing\n    conv0 = diff_layer(input_data=input_data, is_diff=True, is_diff_abs=False, is_abs_diff=False,\n                       order=2, direction=""inter"", name=""difference"", padding=""SAME"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group1\n    conv1_1 = conv_layer(conv0_input_merge, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""tanh"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 1, 1, 1, 1, 32, name=""conv1_2"", activation_method=""tanh"", padding=""SAME"")\n    pool1_3 = pool_layer(conv1_2, 2, 2, 2, 2, name=""pool1_3"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_3, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 1, 1, 1, 1, 64, name=""conv2_2"", activation_method=""tanh"", padding=""SAME"")\n    pool2_3 = pool_layer(conv2_2, 2, 2, 2, 2, name=""pool2_3"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_3, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 1, 1, 1, 1, 128, name=""conv3_2"", activation_method=""tanh"", padding=""SAME"")\n    pool3_3 = pool_layer(conv3_2, 2, 2, 2, 2, name=""pool3_3"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_3, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""tanh"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=""tanh"", padding=""SAME"")\n    pool4_3 = pool_layer(conv4_2, 2, 2, 2, 2, name=""pool4_3"", is_max_pool=True)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_3, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=""tanh"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=""tanh"", padding=""SAME"")\n    pool5_3 = pool_layer(conv5_2, 2, 2, 2, 2, name=""pool5_3"", is_max_pool=True)\n\n    # Group6\n    conv6_1 = conv_layer(pool5_3, 3, 3, 1, 1, 512, name=""conv6_1"", activation_method=""tanh"", padding=""SAME"")\n    conv6_2 = conv_layer(conv6_1, 1, 1, 1, 1, 1024, name=""conv6_2"", activation_method=""tanh"", padding=""SAME"")\n    pool6_3 = pool_layer(conv6_2, 2, 2, 2, 2, name=""pool6_3"", is_max_pool=True)\n\n    # Fully connected layer\n    fc7 = fc_layer(pool6_3, 4096, name=""fc7"", activation_method=""tanh"")\n    fc8 = fc_layer(fc7, 512, name=""fc8"", activation_method=""tanh"")\n\n    logits = fc_layer(fc8, class_num, name=""fc9"", activation_method=None)\n\n    return logits\n\n\ndef wasdn1_2(input_data, class_num=2, is_bn=True):\n    """"""\n    Average pooling layer is used for subsampling\n    """"""\n    print(""WASDN1_2: replace the max pooling layer with the average pooling layer"")\n    print(""Network Structure: "")\n\n    # preprocessing\n    conv0 = diff_layer(input_data=input_data, is_diff=True, is_diff_abs=False, is_abs_diff=False,\n                       order=2, direction=""inter"", name=""difference"", padding=""SAME"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group1\n    conv1_1 = conv_layer(conv0_input_merge, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""tanh"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 1, 1, 1, 1, 32, name=""conv1_2"", activation_method=""tanh"", padding=""SAME"")\n    pool1_3 = pool_layer(conv1_2, 2, 2, 2, 2, name=""pool1_3"", is_max_pool=False)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_3, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 1, 1, 1, 1, 64, name=""conv2_2"", activation_method=""tanh"", padding=""SAME"")\n    pool2_3 = pool_layer(conv2_2, 2, 2, 2, 2, name=""pool2_3"", is_max_pool=False)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_3, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 1, 1, 1, 1, 128, name=""conv3_2"", activation_method=""tanh"", padding=""SAME"")\n    pool3_3 = pool_layer(conv3_2, 2, 2, 2, 2, name=""pool3_3"", is_max_pool=False)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_3, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""tanh"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    bn4_3 = batch_normalization(conv4_2, name=""BN4_3"", activation_method=""tanh"", is_train=is_bn)\n    pool4_4 = pool_layer(bn4_3, 2, 2, 2, 2, name=""pool4_4"", is_max_pool=False)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_4, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=""tanh"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=None, padding=""SAME"")\n    bn5_3 = batch_normalization(conv5_2, name=""BN5_3"", activation_method=""tanh"", is_train=is_bn)\n    pool5_4 = pool_layer(bn5_3, 2, 2, 2, 2, name=""pool5_4"", is_max_pool=False)\n\n    # Group6\n    conv6_1 = conv_layer(pool5_4, 3, 3, 1, 1, 512, name=""conv6_1"", activation_method=""tanh"", padding=""SAME"")\n    conv6_2 = conv_layer(conv6_1, 1, 1, 1, 1, 1024, name=""conv6_2"", activation_method=None, padding=""SAME"")\n    bn6_3 = batch_normalization(conv6_2, name=""BN6_3"", activation_method=""tanh"", is_train=is_bn)\n    pool6_4 = pool_layer(bn6_3, 2, 2, 2, 2, name=""pool6_4"", is_max_pool=False)\n\n    # Fully connected layer\n    fc7 = fc_layer(pool6_4, 4096, name=""fc7"", activation_method=None)\n    bn8 = batch_normalization(fc7, name=""BN8"", activation_method=""tanh"", is_train=is_bn)\n    fc9 = fc_layer(bn8, 512, name=""fc9"", activation_method=None)\n    bn10 = batch_normalization(fc9, name=""BN10"", activation_method=""tanh"", is_train=is_bn)\n\n    logits = fc_layer(bn10, class_num, name=""fc11"", activation_method=None)\n\n    return logits\n\n\ndef wasdn1_3(input_data, class_num=2, is_bn=True):\n    """"""\n    Convolutional layer with stride 2 is used for subsampling\n    """"""\n    print(""WASDN1_3: replace the max pooling layer with the convolutional pooling layer"")\n    print(""Network Structure: "")\n\n    # preprocessing\n    conv0 = diff_layer(input_data=input_data, is_diff=True, is_diff_abs=False, is_abs_diff=False,\n                       order=2, direction=""inter"", name=""difference"", padding=""SAME"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group1\n    conv1_1 = conv_layer(conv0_input_merge, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""tanh"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 1, 1, 1, 1, 32, name=""conv1_2"", activation_method=""tanh"", padding=""SAME"")\n    conv_pool_1_3 = conv_layer(conv1_2, 3, 3, 2, 2, 32, name=""conv_pool_1_3"", activation_method=""None"", padding=""VALID"")\n\n    # Group2\n    conv2_1 = conv_layer(conv_pool_1_3, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 1, 1, 1, 1, 64, name=""conv2_2"", activation_method=""tanh"", padding=""SAME"")\n    conv_pool_2_3 = conv_layer(conv2_2, 3, 3, 2, 2, 64, name=""conv_pool_2_3"", activation_method=""None"", padding=""VALID"")\n\n    # Group3\n    conv3_1 = conv_layer(conv_pool_2_3, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 1, 1, 1, 1, 128, ""conv3_2"", activation_method=""tanh"", padding=""SAME"")\n    conv_pool_3_3 = conv_layer(conv3_2, 3, 3, 2, 2, 128, name=""conv_pool_3_3"", activation_method=""None"", padding=""VALID"")\n\n    # Group4\n    conv4_1 = conv_layer(conv_pool_3_3, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""tanh"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    bn4_3 = batch_normalization(conv4_2, name=""BN4_3"", activation_method=""tanh"", is_train=is_bn)\n    conv_pool_4_4 = conv_layer(bn4_3, 3, 3, 2, 2, 256, name=""conv_pool_4_4"", activation_method=""None"", padding=""VALID"")\n\n    # Group5\n    conv5_1 = conv_layer(conv_pool_4_4, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=""tanh"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=None, padding=""SAME"")\n    bn5_3 = batch_normalization(conv5_2, name=""BN5_3"", activation_method=""tanh"", is_train=is_bn)\n    conv_pool_5_4 = conv_layer(bn5_3, 3, 3, 2, 2, 512, name=""conv_pool_5_4"", activation_method=""None"", padding=""VALID"")\n\n    # Group6\n    conv6_1 = conv_layer(conv_pool_5_4, 3, 3, 1, 1, 512, name=""conv6_1"", activation_method=""tanh"", padding=""SAME"")\n    conv6_2 = conv_layer(conv6_1, 1, 1, 1, 1, 1024, name=""conv6_2"", activation_method=None, padding=""SAME"")\n    bn6_3 = batch_normalization(conv6_2, name=""BN6_3"", activation_method=""tanh"", is_train=is_bn)\n    conv_pool_6_4 = conv_layer(bn6_3, 3, 3, 2, 2, 1024, name=""conv_pool_6_4"", activation_method=""None"", padding=""VALID"")\n\n    # Fully connected layer\n    fc7 = fc_layer(conv_pool_6_4, 4096, name=""fc7"", activation_method=None)\n    bn8 = batch_normalization(fc7, name=""BN8"", activation_method=""tanh"", is_train=is_bn)\n    fc9 = fc_layer(bn8, 512, name=""fc9"", activation_method=None)\n    bn10 = batch_normalization(fc9, name=""BN10"", activation_method=""tanh"", is_train=is_bn)\n\n    logits = fc_layer(bn10, class_num, name=""fc11"", activation_method=None)\n\n    return logits\n\n\ndef wasdn1_4(input_data, class_num=2, is_bn=True):\n    """"""\n    Replace the convolutional kernel with 5x5 kernel\n    """"""\n    print(""WASDN1_4: replace the 3 x 3 kernel with the 5 x 5 kernel"")\n    print(""Network Structure: "")\n\n    # preprocessing\n    conv0 = diff_layer(input_data=input_data, is_diff=True, is_diff_abs=False, is_abs_diff=False,\n                       order=2, direction=""inter"", name=""difference"", padding=""SAME"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group1\n    conv1_1 = conv_layer(conv0_input_merge, 5, 5, 1, 1, 16, name=""conv1_1"", activation_method=""tanh"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 1, 1, 1, 1, 32, name=""conv1_2"", activation_method=""tanh"", padding=""SAME"")\n    pool1_3 = pool_layer(conv1_2, 2, 2, 2, 2, name=""pool1_3"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_3, 5, 5, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 1, 1, 1, 1, 64, name=""conv2_2"", activation_method=""tanh"", padding=""SAME"")\n    pool2_3 = pool_layer(conv2_2, 2, 2, 2, 2, name=""pool2_3"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_3, 5, 5, 1, 1, 64, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 1, 1, 1, 1, 128, name=""conv3_2"", activation_method=""tanh"", padding=""SAME"")\n    pool3_3 = pool_layer(conv3_2, 2, 2, 2, 2, name=""pool3_3"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_3, 5, 5, 1, 1, 128, name=""conv4_1"", activation_method=""tanh"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    bn4_3 = batch_normalization(conv4_2, name=""BN4_3"", activation_method=""tanh"", is_train=is_bn)\n    pool4_4 = pool_layer(bn4_3, 2, 2, 2, 2, name=""pool4_4"", is_max_pool=True)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_4, 5, 5, 1, 1, 256, name=""conv5_1"", activation_method=""tanh"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=None, padding=""SAME"")\n    bn5_3 = batch_normalization(conv5_2, name=""BN5_3"", activation_method=""tanh"", is_train=is_bn)\n    pool5_4 = pool_layer(bn5_3, 2, 2, 2, 2, name=""pool5_4"", is_max_pool=True)\n\n    # Group6\n    conv6_1 = conv_layer(pool5_4, 5, 5, 1, 1, 512, name=""conv6_1"", activation_method=""tanh"", padding=""SAME"")\n    conv6_2 = conv_layer(conv6_1, 1, 1, 1, 1, 1024, name=""conv6_2"", activation_method=None, padding=""SAME"")\n    bn6_3 = batch_normalization(conv6_2, name=""BN6_3"", activation_method=""tanh"", is_train=is_bn)\n    pool6_4 = pool_layer(bn6_3, 2, 2, 2, 2, name=""pool6_4"", is_max_pool=True)\n\n    # Fully connected layer\n    fc7 = fc_layer(pool6_4, 4096, name=""fc7"", activation_method=None)\n    bn8 = batch_normalization(fc7, name=""BN8"", activation_method=""tanh"", is_train=is_bn)\n    fc9 = fc_layer(bn8, 512, name=""fc9"", activation_method=None)\n    bn10 = batch_normalization(fc9, name=""BN10"", activation_method=""tanh"", is_train=is_bn)\n\n    logits = fc_layer(bn10, class_num, name=""fc11"", activation_method=None)\n\n    return logits\n\n\ndef wasdn1_5(input_data, class_num=2, is_bn=True):\n    """"""\n    ReLu is used as the activation function\n    """"""\n    print(""WASDN1_5: use relu as the activation function"")\n    print(""Network Structure: "")\n\n    # preprocessing\n    conv0 = diff_layer(input_data=input_data, is_diff=True, is_diff_abs=False, is_abs_diff=False,\n                       order=2, direction=""inter"", name=""difference"", padding=""SAME"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group1\n    conv1_1 = conv_layer(conv0_input_merge, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""relu"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 1, 1, 1, 1, 32, name=""conv1_2"", activation_method=""relu"", padding=""SAME"")\n    pool1_3 = pool_layer(conv1_2, 2, 2, 2, 2, name=""pool1_3"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_3, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""relu"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 1, 1, 1, 1, 64, name=""conv2_2"", activation_method=""relu"", padding=""SAME"")\n    pool2_3 = pool_layer(conv2_2, 2, 2, 2, 2, name=""pool2_3"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_3, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""relu"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 1, 1, 1, 1, 128, name=""conv3_2"", activation_method=""relu"", padding=""SAME"")\n    pool3_3 = pool_layer(conv3_2, 2, 2, 2, 2, name=""pool3_3"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_3, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""relu"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    bn4_3 = batch_normalization(conv4_2, name=""BN4_3"", activation_method=""relu"", is_train=is_bn)\n    pool4_4 = pool_layer(bn4_3, 2, 2, 2, 2, name=""pool4_4"", is_max_pool=True)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_4, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=""relu"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=None, padding=""SAME"")\n    bn5_3 = batch_normalization(conv5_2, name=""BN5_3"", activation_method=""relu"", is_train=is_bn)\n    pool5_4 = pool_layer(bn5_3, 2, 2, 2, 2, name=""pool5_4"", is_max_pool=True)\n\n    # Group6\n    conv6_1 = conv_layer(pool5_4, 3, 3, 1, 1, 512, name=""conv6_1"", activation_method=""relu"", padding=""SAME"")\n    conv6_2 = conv_layer(conv6_1, 1, 1, 1, 1, 1024, name=""conv6_2"", activation_method=None, padding=""SAME"")\n    bn6_3 = batch_normalization(conv6_2, name=""BN6_3"", activation_method=""relu"", is_train=is_bn)\n    pool6_4 = pool_layer(bn6_3, 2, 2, 2, 2, name=""pool6_4"", is_max_pool=True)\n\n    # Fully connected layer\n    fc7 = fc_layer(pool6_4, 4096, name=""fc7"", activation_method=None)\n    bn8 = batch_normalization(fc7, name=""BN8"", activation_method=""relu"", is_train=is_bn)\n    fc9 = fc_layer(bn8, 512, name=""fc9"", activation_method=None)\n    bn10 = batch_normalization(fc9, name=""BN10"", activation_method=""relu"", is_train=is_bn)\n\n    logits = fc_layer(bn10, class_num, name=""fc11"", activation_method=None)\n\n    return logits\n\n\ndef wasdn1_6(input_data, class_num=2, is_bn=True):\n    """"""\n    Leaky-ReLu is used as the activation function\n    """"""\n    print(""WASDN1_6: Use leakrelu as the activation function"")\n    print(""Network Structure: "")\n\n    # preprocessing\n    conv0 = diff_layer(input_data=input_data, is_diff=True, is_diff_abs=False, is_abs_diff=False,\n                       order=2, direction=""inter"", name=""difference"", padding=""SAME"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group1\n    conv1_1 = conv_layer(conv0_input_merge, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""leakrelu"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 1, 1, 1, 1, 32, name=""conv1_2"", activation_method=""leakrelu"", padding=""SAME"")\n    pool1_3 = pool_layer(conv1_2, 2, 2, 2, 2, name=""pool1_3"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_3, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""leakrelu"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 1, 1, 1, 1, 64, name=""conv2_2"", activation_method=""leakrelu"", padding=""SAME"")\n    pool2_3 = pool_layer(conv2_2, 2, 2, 2, 2, name=""pool2_3"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_3, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""leakrelu"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 1, 1, 1, 1, 128, name=""conv3_2"", activation_method=""leakrelu"", padding=""SAME"")\n    pool3_3 = pool_layer(conv3_2, 2, 2, 2, 2, name=""pool3_3"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_3, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""leakrelu"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    bn4_3 = batch_normalization(conv4_2, name=""BN4_3"", activation_method=""leakrelu"", is_train=is_bn)\n    pool4_4 = pool_layer(bn4_3, 2, 2, 2, 2, name=""pool4_4"", is_max_pool=True)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_4, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=""leakrelu"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=None, padding=""SAME"")\n    bn5_3 = batch_normalization(conv5_2, name=""BN5_3"", activation_method=""leakrelu"", is_train=is_bn)\n    pool5_4 = pool_layer(bn5_3, 2, 2, 2, 2, name=""pool5_4"", is_max_pool=True)\n\n    # Group6\n    conv6_1 = conv_layer(pool5_4, 3, 3, 1, 1, 512, name=""conv6_1"", activation_method=""leakrelu"", padding=""SAME"")\n    conv6_2 = conv_layer(conv6_1, 1, 1, 1, 1, 1024, name=""conv6_2"", activation_method=None, padding=""SAME"")\n    bn6_3 = batch_normalization(conv6_2, name=""BN6_3"", activation_method=""leakrelu"", is_train=is_bn)\n    pool6_4 = pool_layer(bn6_3, 2, 2, 2, 2, name=""pool6_4"", is_max_pool=True)\n\n    # Fully connected layer\n    fc7 = fc_layer(pool6_4, 4096, name=""fc7"", activation_method=None)\n    bn8 = batch_normalization(fc7, name=""BN8"", activation_method=""leakrelu"", is_train=is_bn)\n    fc9 = fc_layer(bn8, 512, name=""fc9"", activation_method=None)\n    bn10 = batch_normalization(fc9, name=""BN10"", activation_method=""leakrelu"", is_train=is_bn)\n\n    logits = fc_layer(bn10, class_num, name=""fc11"", activation_method=None)\n\n    return logits\n\n\ndef wasdn1_7(input_data, class_num=2, is_bn=True):\n    """"""\n    Deepen the network to block convolutional layers\n    """"""\n    print(""WASDN1_7: Deepen the network to 7 groups."")\n    print(""Network Structure: "")\n\n    # preprocessing\n    conv0 = diff_layer(input_data=input_data, is_diff=True, is_diff_abs=False, is_abs_diff=False,\n                       order=2, direction=""inter"", name=""difference"", padding=""SAME"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group1\n    conv1_1 = conv_layer(conv0_input_merge, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""tanh"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 1, 1, 1, 1, 32, name=""conv1_2"", activation_method=""tanh"", padding=""SAME"")\n    pool1_3 = pool_layer(conv1_2, 2, 2, 2, 2, name=""pool1_3"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_3, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 1, 1, 1, 1, 64, name=""conv2_2"", activation_method=""tanh"", padding=""SAME"")\n    pool2_3 = pool_layer(conv2_2, 2, 2, 2, 2, name=""pool2_3"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_3, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 1, 1, 1, 1, 128, name=""conv3_2"", activation_method=""tanh"", padding=""SAME"")\n    pool3_3 = pool_layer(conv3_2, 2, 2, 2, 2, name=""pool3_3"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_3, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""tanh"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    bn4_3 = batch_normalization(conv4_2, name=""BN4_3"", activation_method=""tanh"", is_train=is_bn)\n    pool4_4 = pool_layer(bn4_3, 2, 2, 2, 2, name=""pool4_4"", is_max_pool=True)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_4, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=""tanh"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=None, padding=""SAME"")\n    bn5_3 = batch_normalization(conv5_2, name=""BN5_3"", activation_method=""tanh"", is_train=is_bn)\n    pool5_4 = pool_layer(bn5_3, 2, 2, 2, 2, name=""pool5_4"", is_max_pool=True)\n\n    # Group6\n    conv6_1 = conv_layer(pool5_4, 3, 3, 1, 1, 512, name=""conv6_1"", activation_method=""tanh"", padding=""SAME"")\n    conv6_2 = conv_layer(conv6_1, 1, 1, 1, 1, 1024, name=""conv6_2"", activation_method=None, padding=""SAME"")\n    bn6_3 = batch_normalization(conv6_2, name=""BN6_3"", activation_method=""tanh"", is_train=is_bn)\n    pool6_4 = pool_layer(bn6_3, 2, 2, 2, 2, name=""pool6_4"", is_max_pool=True)\n\n    # Group7\n    conv7_1 = conv_layer(pool6_4, 3, 3, 1, 1, 1024, name=""conv7_1"", activation_method=""tanh"", padding=""SAME"")\n    conv7_2 = conv_layer(conv7_1, 1, 1, 1, 1, 2048, name=""conv7_2"", activation_method=None, padding=""SAME"")\n    bn7_3 = batch_normalization(conv7_2, name=""BN7_3"", activation_method=""tanh"", is_train=is_bn)\n    pool7_4 = pool_layer(bn7_3, 2, 2, 2, 2, name=""pool7_4"", is_max_pool=True)\n\n    # Fully connected layer\n    fc8 = fc_layer(pool7_4, 4096, name=""fc8"", activation_method=None)\n    bn9 = batch_normalization(fc8, name=""BN9"", activation_method=""tanh"", is_train=is_bn)\n    fc10 = fc_layer(bn9, 512, name=""fc10"", activation_method=None)\n    bn11 = batch_normalization(fc10, name=""BN11"", activation_method=""tanh"", is_train=is_bn)\n\n    logits = fc_layer(bn11, class_num, name=""fc12"", activation_method=None)\n\n    return logits\n\n\ndef wasdn1_8(input_data, class_num=2, is_bn=True):\n    """"""\n    Remove the 1x1 convolutional layers\n    """"""\n    print(""WASDN1_8: Remove the 1x1 conv layers."")\n    print(""Network Structure: "")\n\n    # preprocessing\n    conv0 = diff_layer(input_data=input_data, is_diff=True, is_diff_abs=False, is_abs_diff=False,\n                       order=2, direction=""inter"", name=""difference"", padding=""SAME"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group1\n    conv1_1 = conv_layer(conv0_input_merge, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""tanh"", padding=""SAME"")\n    pool1_2 = pool_layer(conv1_1, 2, 2, 2, 2, name=""pool1_2"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_2, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    pool2_2 = pool_layer(conv2_1, 2, 2, 2, 2, name=""pool2_2"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_2, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    pool3_2 = pool_layer(conv3_1, 2, 2, 2, 2, name=""pool3_2"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_2, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=None, padding=""SAME"")\n    bn4_2 = batch_normalization(conv4_1, name=""BN4_2"", activation_method=""tanh"", is_train=is_bn)\n    pool4_3 = pool_layer(bn4_2, 2, 2, 2, 2, name=""pool4_3"", is_max_pool=True)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_3, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=None, padding=""SAME"")\n    bn5_2 = batch_normalization(conv5_1, name=""BN5_2"", activation_method=""tanh"", is_train=is_bn)\n    pool5_3 = pool_layer(bn5_2, 2, 2, 2, 2, name=""pool5_3"", is_max_pool=True)\n\n    # Group6\n    conv6_1 = conv_layer(pool5_3, 3, 3, 1, 1, 512, name=""conv6_1"", activation_method=None, padding=""SAME"")\n    bn6_2 = batch_normalization(conv6_1, name=""BN6_2"", activation_method=""tanh"", is_train=is_bn)\n    pool6_3 = pool_layer(bn6_2, 2, 2, 2, 2, name=""pool6_3"", is_max_pool=True)\n\n    # Fully connected layer\n    fc7 = fc_layer(pool6_3, 4096, name=""fc7"", activation_method=None)\n    bn8 = batch_normalization(fc7, name=""BN8"", activation_method=""tanh"", is_train=is_bn)\n    fc9 = fc_layer(bn8, 512, name=""fc9"", activation_method=None)\n    bn10 = batch_normalization(fc9, name=""BN10"", activation_method=""tanh"", is_train=is_bn)\n\n    logits = fc_layer(bn10, class_num, name=""fc11"", activation_method=None)\n\n    return logits\n\n\ndef wasdn1_9(input_data, class_num=2, is_bn=True):\n    """"""\n    Remove the HPF layer\n    """"""\n    print(""WASDN1_9: Remove the HPF layer"")\n    print(""Network Structure: "")\n\n    # Group1\n    conv1_1 = conv_layer(input_data, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""tanh"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 1, 1, 1, 1, 32, name=""conv1_2"", activation_method=""tanh"", padding=""SAME"")\n    pool1_3 = pool_layer(conv1_2, 2, 2, 2, 2, name=""pool1_3"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_3, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 1, 1, 1, 1, 64, name=""conv2_2"", activation_method=""tanh"", padding=""SAME"")\n    pool2_3 = pool_layer(conv2_2, 2, 2, 2, 2, name=""pool2_3"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_3, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 1, 1, 1, 1, 128, name=""conv3_2"", activation_method=""tanh"", padding=""SAME"")\n    pool3_3 = pool_layer(conv3_2, 2, 2, 2, 2, name=""pool3_3"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_3, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""tanh"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    bn4_3 = batch_normalization(conv4_2, name=""BN4_3"", activation_method=""tanh"", is_train=is_bn)\n    pool4_4 = pool_layer(bn4_3, 2, 2, 2, 2, name=""pool4_4"", is_max_pool=True)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_4, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=""tanh"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=None, padding=""SAME"")\n    bn5_3 = batch_normalization(conv5_2, name=""BN5_3"", activation_method=""tanh"", is_train=is_bn)\n    pool5_4 = pool_layer(bn5_3, 2, 2, 2, 2, name=""pool5_4"", is_max_pool=True)\n\n    # Group6\n    conv6_1 = conv_layer(pool5_4, 3, 3, 1, 1, 512, name=""conv6_1"", activation_method=""tanh"", padding=""SAME"")\n    conv6_2 = conv_layer(conv6_1, 1, 1, 1, 1, 1024, name=""conv6_2"", activation_method=None, padding=""SAME"")\n    bn6_3 = batch_normalization(conv6_2, name=""BN6_3"", activation_method=""tanh"", is_train=is_bn)\n    pool6_4 = pool_layer(bn6_3, 2, 2, 2, 2, name=""pool6_4"", is_max_pool=True)\n\n    # Fully connected layer\n    fc7 = fc_layer(pool6_4, 4096, name=""fc7"", activation_method=None)\n    bn8 = batch_normalization(fc7, name=""BN8"", activation_method=""tanh"", is_train=is_bn)\n    fc9 = fc_layer(bn8, 512, name=""fc9"", activation_method=None)\n    bn10 = batch_normalization(fc9, name=""BN10"", activation_method=""tanh"", is_train=is_bn)\n\n    logits = fc_layer(bn10, class_num, name=""fc11"", activation_method=None)\n\n    return logits\n\n\ndef wasdn2_1(input_data, class_num=2, is_bn=True):\n    """"""\n    Remove the BN layer in the first group\n    """"""\n    print(""WASDN2_1: Remove the BN layer in the first group"")\n    print(""Network Structure: "")\n\n    # preprocessing\n    conv0 = diff_layer(input_data=input_data, is_diff=True, is_diff_abs=False, is_abs_diff=False,\n                       order=2, direction=""inter"", name=""difference"", padding=""SAME"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group1\n    conv1_1 = conv_layer(conv0_input_merge, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""tanh"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 1, 1, 1, 1, 32, name=""conv1_2"", activation_method=""tanh"", padding=""SAME"")\n    pool1_3 = pool_layer(conv1_2, 2, 2, 2, 2, name=""pool1_3"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_3, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 1, 1, 1, 1, 64, name=""conv2_2"", activation_method=None, padding=""SAME"")\n    bn2_3 = batch_normalization(conv2_2, name=""BN2_3"", activation_method=""tanh"", is_train=is_bn)\n    pool2_4 = pool_layer(bn2_3, 2, 2, 2, 2, name=""pool2_4"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_4, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 1, 1, 1, 1, 128, name=""conv3_2"", activation_method=None, padding=""SAME"")\n    bn3_3 = batch_normalization(conv3_2, name=""BN3_3"", activation_method=""tanh"", is_train=is_bn)\n    pool3_4 = pool_layer(bn3_3, 2, 2, 2, 2, name=""pool3_4"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_4, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""tanh"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    bn4_3 = batch_normalization(conv4_2, name=""BN4_3"", activation_method=""tanh"", is_train=is_bn)\n    pool4_4 = pool_layer(bn4_3, 2, 2, 2, 2, name=""pool4_4"", is_max_pool=True)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_4, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=""tanh"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=None, padding=""SAME"")\n    bn5_3 = batch_normalization(conv5_2, name=""BN5_3"", activation_method=""tanh"", is_train=is_bn)\n    pool5_4 = pool_layer(bn5_3, 2, 2, 2, 2, name=""pool5_4"", is_max_pool=True)\n\n    # Group6\n    conv6_1 = conv_layer(pool5_4, 3, 3, 1, 1, 512, name=""conv6_1"", activation_method=""tanh"", padding=""SAME"")\n    conv6_2 = conv_layer(conv6_1, 1, 1, 1, 1, 1024, name=""conv6_2"", activation_method=None, padding=""SAME"")\n    bn6_3 = batch_normalization(conv6_2, name=""BN6_3"", activation_method=""tanh"", is_train=is_bn)\n    pool6_4 = pool_layer(bn6_3, 2, 2, 2, 2, name=""pool6_4"", is_max_pool=True)\n\n    # Fully connected layer\n    fc7 = fc_layer(pool6_4, 4096, name=""fc7"", activation_method=None)\n    bn8 = batch_normalization(fc7, name=""BN8"", activation_method=""tanh"", is_train=is_bn)\n    fc9 = fc_layer(bn8, 512, name=""fc9"", activation_method=None)\n    bn10 = batch_normalization(fc9, name=""BN10"", activation_method=""tanh"", is_train=is_bn)\n\n    logits = fc_layer(bn10, class_num, name=""fc11"", activation_method=None)\n\n    return logits\n\n\ndef wasdn2_2(input_data, class_num=2, is_bn=True):\n    """"""\n    Remove the BN layer in the first two groups\n    """"""\n    print(""WASDN2_2: Remove the BN layers in the first  groups"")\n    print(""Network Structure: "")\n\n    # preprocessing\n    conv0 = diff_layer(input_data=input_data, is_diff=True, is_diff_abs=False, is_abs_diff=False,\n                       order=2, direction=""inter"", name=""difference"", padding=""SAME"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group1\n    conv1_1 = conv_layer(conv0_input_merge, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""tanh"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 1, 1, 1, 1, 32, name=""conv1_2"", activation_method=""tanh"", padding=""SAME"")\n    pool1_3 = pool_layer(conv1_2, 2, 2, 2, 2, name=""pool1_3"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_3, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 1, 1, 1, 1, 64, name=""conv2_2"", activation_method=""tanh"", padding=""SAME"")\n    pool2_3 = pool_layer(conv2_2, 2, 2, 2, 2, name=""pool2_3"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_3, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 1, 1, 1, 1, 128, name=""conv3_2"", activation_method=None, padding=""SAME"")\n    bn3_3 = batch_normalization(conv3_2, name=""BN3_3"", activation_method=""tanh"", is_train=is_bn)\n    pool3_4 = pool_layer(bn3_3, 2, 2, 2, 2, name=""pool3_4"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_4, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""tanh"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    bn4_3 = batch_normalization(conv4_2, name=""BN4_3"", activation_method=""tanh"", is_train=is_bn)\n    pool4_4 = pool_layer(bn4_3, 2, 2, 2, 2, name=""pool4_4"", is_max_pool=True)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_4, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=""tanh"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=None, padding=""SAME"")\n    bn5_3 = batch_normalization(conv5_2, name=""BN5_3"", activation_method=""tanh"", is_train=is_bn)\n    pool5_4 = pool_layer(bn5_3, 2, 2, 2, 2, name=""pool5_4"", is_max_pool=True)\n\n    # Group6\n    conv6_1 = conv_layer(pool5_4, 3, 3, 1, 1, 512, name=""conv6_1"", activation_method=""tanh"", padding=""SAME"")\n    conv6_2 = conv_layer(conv6_1, 1, 1, 1, 1, 1024, name=""conv6_2"", activation_method=None, padding=""SAME"")\n    bn6_3 = batch_normalization(conv6_2, name=""BN6_3"", activation_method=""tanh"", is_train=is_bn)\n    pool6_4 = pool_layer(bn6_3, 2, 2, 2, 2, name=""pool6_4"", is_max_pool=True)\n\n    # Fully connected layer\n    fc7 = fc_layer(pool6_4, 4096, name=""fc7"", activation_method=None)\n    bn8 = batch_normalization(fc7, name=""BN8"", activation_method=""tanh"", is_train=is_bn)\n    fc9 = fc_layer(bn8, 512, name=""fc9"", activation_method=None)\n    bn10 = batch_normalization(fc9, name=""BN10"", activation_method=""tanh"", is_train=is_bn)\n\n    logits = fc_layer(bn10, class_num, name=""fc11"", activation_method=None)\n\n    return logits\n\n\ndef wasdn2_3(input_data, class_num=2, is_bn=True):\n    """"""\n    Remove the BN layer in the first four groups\n    """"""\n    print(""WASDN2_3: Remove the BN layers in the first four groups"")\n    print(""Network Structure: "")\n\n    # preprocessing\n    conv0 = diff_layer(input_data=input_data, is_diff=True, is_diff_abs=False, is_abs_diff=False,\n                       order=2, direction=""inter"", name=""difference"", padding=""SAME"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group1\n    conv1_1 = conv_layer(conv0_input_merge, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""tanh"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 1, 1, 1, 1, 32, name=""conv1_2"", activation_method=""tanh"", padding=""SAME"")\n    pool1_3 = pool_layer(conv1_2, 2, 2, 2, 2, name=""pool1_3"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_3, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 1, 1, 1, 1, 64, name=""conv2_2"", activation_method=""tanh"", padding=""SAME"")\n    pool2_3 = pool_layer(conv2_2, 2, 2, 2, 2, name=""pool2_3"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_3, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 1, 1, 1, 1, 128, name=""conv3_2"", activation_method=""tanh"", padding=""SAME"")\n    pool3_3 = pool_layer(conv3_2, 2, 2, 2, 2, name=""pool3_3"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_3, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""tanh"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=""tanh"", padding=""SAME"")\n    pool4_3 = pool_layer(conv4_2, 2, 2, 2, 2, name=""pool4_3"", is_max_pool=True)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_3, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=""tanh"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=None, padding=""SAME"")\n    bn5_3 = batch_normalization(conv5_2, name=""BN5_3"", activation_method=""tanh"", is_train=is_bn)\n    pool5_4 = pool_layer(bn5_3, 2, 2, 2, 2, name=""pool5_4"", is_max_pool=True)\n\n    # Group6\n    conv6_1 = conv_layer(pool5_4, 3, 3, 1, 1, 512, name=""conv6_1"", activation_method=""tanh"", padding=""SAME"")\n    conv6_2 = conv_layer(conv6_1, 1, 1, 1, 1, 1024, name=""conv6_2"", activation_method=None, padding=""SAME"")\n    bn6_3 = batch_normalization(conv6_2, name=""BN6_3"", activation_method=""tanh"", is_train=is_bn)\n    pool6_4 = pool_layer(bn6_3, 2, 2, 2, 2, name=""pool6_4"", is_max_pool=True)\n\n    # Fully connected layer\n    fc7 = fc_layer(pool6_4, 4096, name=""fc7"", activation_method=None)\n    bn8 = batch_normalization(fc7, name=""BN8"", activation_method=""tanh"", is_train=is_bn)\n    fc9 = fc_layer(bn8, 512, name=""fc9"", activation_method=None)\n    bn10 = batch_normalization(fc9, name=""BN10"", activation_method=""tanh"", is_train=is_bn)\n\n    logits = fc_layer(bn10, class_num, name=""fc11"", activation_method=None)\n\n    return logits\n'"
src/networks/tested_networks/RNN.py,49,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2018.10.31\nFinished on 2018.11.01\nModified on\n\n@author: Yuntao Wang\n""""""\n\nimport tensorflow as tf\n\n\ndef rnn_lstm(input_data, class_num):\n    """"""\n    LSTM for MP3 Audio Steganalysis (input data: 200 * 200 QMDCT coefficients matrix)\n    """"""\n    print(""RNN_LSTM: LSTM for MP3 Audio Steganalysis"")\n\n    lstm_size = 1024                                                                        # hidden units\n    input_data_shape = input_data.get_shape()\n    n_steps, n_inputs = input_data_shape[1], input_data_shape[2]\n\n    weights_in = tf.get_variable(name=""weights_in"",\n                                 shape=[n_steps, lstm_size],\n                                 dtype=tf.float32,\n                                 initializer=tf.contrib.layers.xavier_initializer())\n\n    biases_in = tf.get_variable(name=""biases_in"",\n                                shape=[lstm_size],\n                                dtype=tf.float32,\n                                initializer=tf.contrib.layers.xavier_initializer())\n\n    weights_out = tf.get_variable(name=""weights_out"",\n                                  shape=[lstm_size, class_num],\n                                  dtype=tf.float32,\n                                  initializer=tf.contrib.layers.xavier_initializer())\n\n    biases_out = tf.get_variable(name=""biases_out"",\n                                 shape=[class_num],\n                                 dtype=tf.float32,\n                                 initializer=tf.constant_initializer(0.0))\n\n    # \xe5\xae\x9a\xe4\xb9\x89LSTM\xe7\x9a\x84\xe5\x9f\xba\xe6\x9c\xac\xe5\x8d\x95\xe5\x85\x83\n    input_data = tf.transpose(input_data, [1, 0, 2])\n    input_data = tf.reshape(input_data, [-1, n_inputs])\n    input_data = tf.nn.thomranh(tf.add(tf.matmul(input_data, weights_in), biases_in))\n    input_data = tf.split(input_data, n_steps, 0)\n\n    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=lstm_size,\n                                             forget_bias=1.0,\n                                             state_is_tuple=True,\n                                             activation=tf.tanh)\n    init_state = lstm_cell.zero_state(16, dtype=tf.float32)\n\n    outputs, final_state = tf.nn.dynamic_rnn(cell=lstm_cell,\n                                             inputs=input_data,\n                                             initial_state=init_state,\n                                             dtype=tf.float32)\n\n    logits = tf.add(tf.matmul(final_state[1], weights_out), biases_out)\n\n    return logits\n\n\ndef rnn_gru(input_data, class_num):\n    """"""\n    GRU for MP3 Audio Steganalysis (input data: 200 * 200 QMDCT coefficients matrix)\n    """"""\n    print(""RNN_GRU: GRU for MP3 audio steganalysis"")\n\n    lstm_size = 1024                                                                        # hidden units\n    weights = tf.get_variable(name=""weights"",\n                              shape=[lstm_size, class_num],\n                              dtype=tf.float32,\n                              initializer=tf.contrib.layers.xavier_initializer())\n\n    biases = tf.get_variable(name=""biases"",\n                             shape=[class_num],\n                             dtype=tf.float32,\n                             initializer=tf.constant_initializer(0.0))\n\n    cells = list()\n    for _ in range(3):\n        cell = tf.nn.rnn_cell.GRUCell(num_units=1024)\n        cell = tf.nn.rnn_cell.DropoutWrapper(cell=cell, output_keep_prob=0.5)\n        cells.append(cell)\n    network = tf.nn.rnn_cell.MultiRNNCell(cells=cells)\n\n    outputs, last_state = tf.nn.dynamic_rnn(cell=network, inputs=input_data, dtype=tf.float32)\n\n    # get last output\n    outputs = tf.transpose(outputs, (1, 0, 2))\n    last_output = tf.gather(outputs, int(outputs.get_shape()[0]) - 1)\n\n    logits = tf.add(tf.matmul(last_output, weights), biases)\n\n    return logits\n\n\ndef rnn_bi_lstm(input_data, class_num):\n    """"""\n    Bilateral LSTM for MP3 Audio Steganalysis (input data: 200 * 200 QMDCT coefficients matrix)\n    """"""\n    print(""RNN_Bi_LSTM: Bilateral LSTM for MP3 Audio Steganalysis"")\n\n    lstm_size = 1024                                                                        # hidden units\n\n    weights = tf.get_variable(name=""weights"",\n                              shape=[lstm_size, class_num],\n                              dtype=tf.float32,\n                              initializer=tf.contrib.layers.xavier_initializer())\n\n    biases = tf.get_variable(name=""biases"",\n                             shape=[class_num],\n                             dtype=tf.float32,\n                             initializer=tf.constant_initializer(0.0))\n\n    input_data = tf.transpose(input_data, [1, 0, 2])\n    input_data = tf.reshape(input_data, [-1, 200])\n    input_data = tf.split(input_data, 200)\n\n    lstm_fw_cell = tf.contrib.rnn.BasicLSTMCell(lstm_size, forget_bias=1.0)\n    lstm_bw_cell = tf.contrib.rnn.BasicLSTMCell(lstm_size, forget_bias=1.0)\n    outputs, _, _ = tf.contrib.rnn.static_bidirectional_rnn(lstm_fw_cell,\n                                                            lstm_bw_cell,\n                                                            input_data,\n                                                            dtype=tf.float32)\n\n    logits = tf.add(tf.matmul(outputs[-1], weights), biases)\n\n    return logits\n'"
src/networks/tested_networks/dense_net.py,1,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2019.01.08\nFinished on 2019.01.08\nModified on\n\n@author: Yuntao Wang\n""""""\n\nfrom layer import *\nfrom modules.densenet_block import *\n\n\ndef dense_net_mp3(input_data, class_num=2, is_bn=True):\n    """"""\n    Dense net for MP3 audio\n    """"""\n    print(""dense_net_mp3: Dense net for MP3 audio"")\n    print(""Network Structure: "")\n\n    conv_1 = conv_layer(input_data, 3, 3, 1, 1, 48, name=""conv1"", activation_method=""None"", padding=""SAME"")\n\n    block1 = dense_block(conv_1, filter_nums=24, layers=6, name=""block1"", is_bn=is_bn)\n    trans1 = transition_layer(block1, filter_nums=24, name=""trans1"", is_bn=is_bn)\n\n    block2 = dense_block(trans1, filter_nums=24, layers=12, name=""block2"", is_bn=is_bn)\n    trans2 = transition_layer(block2, filter_nums=24, name=""trans2"", is_bn=is_bn)\n\n    block3 = dense_block(trans2, filter_nums=24, layers=24, name=""block3"", is_bn=is_bn)\n    trans3 = transition_layer(block3, filter_nums=24, name=""trans3"", is_bn=is_bn)\n\n    block4 = dense_block(trans3, filter_nums=24, layers=48, name=""block4"", is_bn=is_bn)\n\n    bn = batch_normalization(block4, name=""BN"", activation_method=""tanh"", is_train=is_bn)\n\n    pool = global_pool(bn, name=""global_max_pool"", is_max_pool=True)\n\n    logits = fc_layer(pool, class_num, name=""out"", activation_method=None)\n\n    return logits\n\n\ndef dense_net_mp3_42(input_data, class_num=2, is_bn=True):\n    """"""\n    Dense net for MP3 audio\n    """"""\n    print(""dense_net_mp3: Dense net for MP3 audio"")\n    print(""Network Structure: "")\n\n    conv_1 = conv_layer(input_data, 3, 3, 1, 1, 48, name=""conv1"", activation_method=""None"", padding=""SAME"")\n\n    block1 = dense_block(conv_1, filter_nums=24, layers=6, name=""block1"", is_bn=is_bn)\n    trans1 = transition_layer(block1, filter_nums=24, name=""trans1"", is_bn=is_bn)\n\n    block2 = dense_block(trans1, filter_nums=24, layers=12, name=""block2"", is_bn=is_bn)\n    trans2 = transition_layer(block2, filter_nums=24, name=""trans2"", is_bn=is_bn)\n\n    block3 = dense_block(trans2, filter_nums=24, layers=24, name=""block3"", is_bn=is_bn)\n\n    bn = batch_normalization(block3, name=""BN"", activation_method=""tanh"", is_train=is_bn)\n\n    pool = global_pool(bn, name=""global_max_pool"", is_max_pool=True)\n\n    logits = fc_layer(pool, class_num, name=""out"", activation_method=None)\n\n    return logits\n\n\ndef dense_net_mp3_18(input_data, class_num=2, is_bn=True):\n    """"""\n    Dense net for MP3 audio\n    """"""\n    print(""dense_net_mp3: Dense net for MP3 audio"")\n    print(""Network Structure: "")\n    # # High Pass Filtering\n    # conv0 = rich_hpf_layer(input_data, name=""HPFs"")\n    #\n    # # HPF and input data concat\n    # conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    # concat_shape = conv0_input_merge.get_shape()\n    # print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    conv_1 = conv_layer(input_data, 3, 3, 1, 1, 48, name=""conv1"", activation_method=""None"", padding=""SAME"")\n\n    block1 = dense_block(conv_1, filter_nums=24, layers=6, name=""block1"", is_bn=is_bn)\n    trans1 = transition_layer(block1, filter_nums=24, name=""trans1"", is_bn=is_bn)\n\n    block2 = dense_block(trans1, filter_nums=24, layers=8, name=""block2"", is_bn=is_bn)\n\n    bn = batch_normalization(block2, name=""BN"", activation_method=""tanh"", is_train=is_bn)\n\n    pool = global_pool(bn, name=""global_max_pool"", is_max_pool=True)\n\n    logits = fc_layer(pool, class_num, name=""out"", activation_method=None)\n\n    return logits\n\n\ndef dense_net_mp3_6(input_data, class_num=2, is_bn=True):\n    """"""\n    Dense net for MP3 audio\n    """"""\n    print(""dense_net_mp3: Dense net for MP3 audio"")\n    print(""Network Structure: "")\n\n    conv_1 = conv_layer(input_data, 3, 3, 1, 1, 48, name=""conv1"", activation_method=""None"", padding=""SAME"")\n\n    block1 = dense_block(conv_1, filter_nums=12, layers=6, name=""block1"", is_bn=is_bn)\n    trans1 = transition_layer(block1, filter_nums=24, name=""trans1"", is_bn=is_bn)\n\n    bn = batch_normalization(trans1, name=""BN"", activation_method=""tanh"", is_train=is_bn)\n\n    pool = global_pool(bn, name=""global_max_pool"", is_max_pool=True)\n\n    logits = fc_layer(pool, class_num, name=""out"", activation_method=None)\n\n    return logits\n'"
src/networks/tested_networks/dissertation.py,22,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCreated on 2019.02.21\nFinished on 2019.02.21\nModified on\n\n@author: Yuntao Wang\n""""""\n\nfrom modules.inception import *\n\n\ndef chap4(input_data, class_num=2, is_bn=True):\n    """"""\n    Network in Chapter 4\n    """"""\n\n    print(""Network in Chapter 4"")\n    print(""Network Structure: "")\n\n    # High Pass Filtering\n    conv0 = rich_hpf_layer(input_data, name=""HPFs"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group1\n    conv1_1 = conv_layer(conv0_input_merge, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""tanh"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 3, 3, 1, 1, 32, name=""conv1_2"", activation_method=None, padding=""SAME"")\n    bn1_3 = batch_normalization(conv1_2, name=""BN1_3"", activation_method=""tanh"", is_train=is_bn)\n    pool1_4 = pool_layer(bn1_3, 2, 2, 2, 2, name=""pool1_4"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_4, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 3, 3, 1, 1, 64, name=""conv2_2"", activation_method=None, padding=""SAME"")\n    bn2_3 = batch_normalization(conv2_2, name=""BN2_3"", activation_method=""tanh"", is_train=is_bn)\n    pool2_4 = pool_layer(bn2_3, 2, 2, 2, 2, name=""pool2_4"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_4, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 3, 3, 1, 1, 128, name=""conv3_2"", activation_method=None, padding=""SAME"")\n    bn3_3 = batch_normalization(conv3_2, name=""BN3_3"", activation_method=""tanh"", is_train=is_bn)\n    pool3_4 = pool_layer(bn3_3, 2, 2, 2, 2, name=""pool3_4"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_4, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""tanh"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    bn4_3 = batch_normalization(conv4_2, name=""BN4_3"", activation_method=""tanh"", is_train=is_bn)\n    pool4_4 = pool_layer(bn4_3, 2, 2, 2, 2, name=""pool4_4"", is_max_pool=True)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_4, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=""tanh"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=None, padding=""SAME"")\n    bn5_3 = batch_normalization(conv5_2, name=""BN5_3"", activation_method=""tanh"", is_train=is_bn)\n    pool5_4 = pool_layer(bn5_3, 2, 2, 2, 2, name=""pool5_4"", is_max_pool=True)\n\n    # fully conv layer\n    fconv6 = conv_layer(pool5_4, 6, 12, 1, 1, 4096, name=""fconv6"", activation_method=None, padding=""VALID"")\n    bn7 = batch_normalization(fconv6, name=""BN7"", activation_method=""tanh"", is_train=is_bn)\n\n    fconv8 = conv_layer(bn7, 1, 1, 1, 1, 512, name=""fconv8"", activation_method=None, padding=""VALID"")\n    bn9 = batch_normalization(fconv8, name=""BN9"", activation_method=""tanh"", is_train=is_bn)\n\n    fconv10 = conv_layer(bn9, 1, 1, 1, 1, 2, name=""fconv10"", activation_method=None, padding=""VALID"")\n    bn11 = batch_normalization(fconv10, name=""BN11"", activation_method=""tanh"", is_train=is_bn)\n\n    # Global Max Pooling\n    bn11_shape = bn11.get_shape()\n    bn11_height, bn11_width = bn11_shape[1], bn11_shape[2]\n    pool11 = pool_layer(bn11, bn11_height, bn11_width, bn11_height, bn11_width, name=""global_max"", is_max_pool=True)\n\n    logits = tf.reshape(pool11, [-1, class_num])\n\n    return logits\n\n\ndef chap4_1(input_data, class_num=2, is_bn=True):\n    """"""\n    Remove HPFs\n    """"""\n\n    print(""Remove Rich High Pass Filters"")\n    print(""Network Structure: "")\n\n    # Group1\n    conv1_1 = conv_layer(input_data, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""tanh"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 1, 1, 1, 1, 32, name=""conv1_2"", activation_method=None, padding=""SAME"")\n    bn1_3 = batch_normalization(conv1_2, name=""BN1_3"", activation_method=""tanh"", is_train=is_bn)\n    pool1_4 = pool_layer(bn1_3, 2, 2, 2, 2, name=""pool1_4"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_4, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 1, 1, 1, 1, 64, name=""conv2_2"", activation_method=None, padding=""SAME"")\n    bn2_3 = batch_normalization(conv2_2, name=""BN2_3"", activation_method=""tanh"", is_train=is_bn)\n    pool2_4 = pool_layer(bn2_3, 2, 2, 2, 2, name=""pool2_4"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_4, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 1, 1, 1, 1, 128, name=""conv3_2"", activation_method=None, padding=""SAME"")\n    bn3_3 = batch_normalization(conv3_2, name=""BN3_3"", activation_method=""tanh"", is_train=is_bn)\n    pool3_4 = pool_layer(bn3_3, 2, 2, 2, 2, name=""pool3_4"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_4, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""tanh"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    bn4_3 = batch_normalization(conv4_2, name=""BN4_3"", activation_method=""tanh"", is_train=is_bn)\n    pool4_4 = pool_layer(bn4_3, 2, 2, 2, 2, name=""pool4_4"", is_max_pool=True)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_4, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=""tanh"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=None, padding=""SAME"")\n    bn5_3 = batch_normalization(conv5_2, name=""BN5_3"", activation_method=""tanh"", is_train=is_bn)\n    pool5_4 = pool_layer(bn5_3, 2, 2, 2, 2, name=""pool5_4"", is_max_pool=True)\n\n    # fully conv layer\n    fconv6 = conv_layer(pool5_4, 6, 12, 1, 1, 4096, name=""fconv6"", activation_method=None, padding=""VALID"")\n    bn7 = batch_normalization(fconv6, name=""BN7"", activation_method=""tanh"", is_train=is_bn)\n\n    fconv8 = conv_layer(bn7, 1, 1, 1, 1, 512, name=""fconv8"", activation_method=None, padding=""VALID"")\n    bn9 = batch_normalization(fconv8, name=""BN9"", activation_method=""tanh"", is_train=is_bn)\n\n    fconv10 = conv_layer(bn9, 1, 1, 1, 1, 2, name=""fconv10"", activation_method=None, padding=""VALID"")\n    bn11 = batch_normalization(fconv10, name=""BN11"", activation_method=""tanh"", is_train=is_bn)\n\n    # Global Max Pooling\n    bn11_shape = bn11.get_shape()\n    bn11_height, bn11_width = bn11_shape[1], bn11_shape[2]\n    pool11 = pool_layer(bn11, bn11_height, bn11_width, bn11_height, bn11_width, name=""global_max"", is_max_pool=True)\n\n    logits = tf.reshape(pool11, [-1, class_num])\n\n    return logits\n\n\ndef chap4_2(input_data, class_num=2, is_bn=True):\n    """"""\n    Remove the shortcut of QMDCT coefficients\n    """"""\n\n    print(""Remove the shortcut of QMDCT coefficients"")\n    print(""Network Structure: "")\n\n    # High Pass Filtering\n    conv0 = rich_hpf_layer(input_data, name=""HPFs"")\n\n    # Group1\n    conv1_1 = conv_layer(conv0, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""tanh"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 1, 1, 1, 1, 32, name=""conv1_2"", activation_method=None, padding=""SAME"")\n    bn1_3 = batch_normalization(conv1_2, name=""BN1_3"", activation_method=""tanh"", is_train=is_bn)\n    pool1_4 = pool_layer(bn1_3, 2, 2, 2, 2, name=""pool1_4"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_4, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 1, 1, 1, 1, 64, name=""conv2_2"", activation_method=None, padding=""SAME"")\n    bn2_3 = batch_normalization(conv2_2, name=""BN2_3"", activation_method=""tanh"", is_train=is_bn)\n    pool2_4 = pool_layer(bn2_3, 2, 2, 2, 2, name=""pool2_4"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_4, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 1, 1, 1, 1, 128, name=""conv3_2"", activation_method=None, padding=""SAME"")\n    bn3_3 = batch_normalization(conv3_2, name=""BN3_3"", activation_method=""tanh"", is_train=is_bn)\n    pool3_4 = pool_layer(bn3_3, 2, 2, 2, 2, name=""pool3_4"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_4, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""tanh"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    bn4_3 = batch_normalization(conv4_2, name=""BN4_3"", activation_method=""tanh"", is_train=is_bn)\n    pool4_4 = pool_layer(bn4_3, 2, 2, 2, 2, name=""pool4_4"", is_max_pool=True)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_4, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=""tanh"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=None, padding=""SAME"")\n    bn5_3 = batch_normalization(conv5_2, name=""BN5_3"", activation_method=""tanh"", is_train=is_bn)\n    pool5_4 = pool_layer(bn5_3, 2, 2, 2, 2, name=""pool5_4"", is_max_pool=True)\n\n    # fully conv layer\n    fconv6 = conv_layer(pool5_4, 6, 12, 1, 1, 4096, name=""fconv6"", activation_method=None, padding=""VALID"")\n    bn7 = batch_normalization(fconv6, name=""BN7"", activation_method=""tanh"", is_train=is_bn)\n\n    fconv8 = conv_layer(bn7, 1, 1, 1, 1, 512, name=""fconv8"", activation_method=None, padding=""VALID"")\n    bn9 = batch_normalization(fconv8, name=""BN9"", activation_method=""tanh"", is_train=is_bn)\n\n    fconv10 = conv_layer(bn9, 1, 1, 1, 1, 2, name=""fconv10"", activation_method=None, padding=""VALID"")\n    bn11 = batch_normalization(fconv10, name=""BN11"", activation_method=""tanh"", is_train=is_bn)\n\n    # Global Max Pooling\n    bn11_shape = bn11.get_shape()\n    bn11_height, bn11_width = bn11_shape[1], bn11_shape[2]\n    pool11 = pool_layer(bn11, bn11_height, bn11_width, bn11_height, bn11_width, name=""global_max"", is_max_pool=True)\n\n    logits = tf.reshape(pool11, [-1, class_num])\n\n    return logits\n\n\ndef chap4_3(input_data, class_num=2):\n    """"""\n    Remove BN layers\n    """"""\n\n    print(""Remove BN layers"")\n    print(""Network Structure: "")\n\n    # High Pass Filtering\n    conv0 = rich_hpf_layer(input_data, name=""HPFs"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group1\n    conv1_1 = conv_layer(conv0_input_merge, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""tanh"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 1, 1, 1, 1, 32, name=""conv1_2"", activation_method=None, padding=""SAME"")\n    pool1_3 = pool_layer(conv1_2, 2, 2, 2, 2, name=""pool1_3"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_3, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 1, 1, 1, 1, 64, name=""conv2_2"", activation_method=None, padding=""SAME"")\n    pool2_3 = pool_layer(conv2_2, 2, 2, 2, 2, name=""pool2_3"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_3, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 1, 1, 1, 1, 128, name=""conv3_2"", activation_method=None, padding=""SAME"")\n    pool3_3 = pool_layer(conv3_2, 2, 2, 2, 2, name=""pool3_3"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_3, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""tanh"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    pool4_3 = pool_layer(conv4_2, 2, 2, 2, 2, name=""pool4_3"", is_max_pool=True)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_3, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=""tanh"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=None, padding=""SAME"")\n    pool5_3 = pool_layer(conv5_2, 2, 2, 2, 2, name=""pool5_3"", is_max_pool=True)\n\n    # fully conv layer\n    fconv6 = conv_layer(pool5_3, 6, 12, 1, 1, 4096, name=""fconv6"", activation_method=None, padding=""VALID"")\n    fconv7 = conv_layer(fconv6, 1, 1, 1, 1, 512, name=""fconv8"", activation_method=None, padding=""VALID"")\n    fconv8 = conv_layer(fconv7, 1, 1, 1, 1, 2, name=""fconv10"", activation_method=None, padding=""VALID"")\n\n    # Global Max Pooling\n    fconv8_shape = fconv8.get_shape()\n    fconv8_height, fconv8_width = fconv8_shape[1], fconv8_shape[2]\n    pool9 = pool_layer(fconv8, fconv8_height, fconv8_width, fconv8_height, fconv8_width, name=""global_max"", is_max_pool=True)\n\n    logits = tf.reshape(pool9, [-1, class_num])\n\n    return logits\n\n\ndef chap4_4(input_data, class_num=2, is_bn=True):\n    """"""\n    Remove 1x1 Convolutional kernel\n    """"""\n\n    print(""Remove 1x1 Convolutional kernel"")\n    print(""Network Structure: "")\n\n    # High Pass Filtering\n    conv0 = rich_hpf_layer(input_data, name=""HPFs"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group1\n    conv1_1 = conv_layer(conv0_input_merge, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""tanh"", padding=""SAME"")\n    bn1_2 = batch_normalization(conv1_1, name=""BN1_2"", activation_method=""tanh"", is_train=is_bn)\n    pool1_3 = pool_layer(bn1_2, 2, 2, 2, 2, name=""pool1_3"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_3, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    bn2_2 = batch_normalization(conv2_1, name=""BN2_2"", activation_method=""tanh"", is_train=is_bn)\n    pool2_3 = pool_layer(bn2_2, 2, 2, 2, 2, name=""pool2_3"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_3, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    bn3_2 = batch_normalization(conv3_1, name=""BN3_2"", activation_method=""tanh"", is_train=is_bn)\n    pool3_3 = pool_layer(bn3_2, 2, 2, 2, 2, name=""pool3_3"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_3, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""tanh"", padding=""SAME"")\n    bn4_2 = batch_normalization(conv4_1, name=""BN4_2"", activation_method=""tanh"", is_train=is_bn)\n    pool4_3 = pool_layer(bn4_2, 2, 2, 2, 2, name=""pool4_3"", is_max_pool=True)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_3, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=""tanh"", padding=""SAME"")\n    bn5_2 = batch_normalization(conv5_1, name=""BN5_2"", activation_method=""tanh"", is_train=is_bn)\n    pool5_3 = pool_layer(bn5_2, 2, 2, 2, 2, name=""pool5_3"", is_max_pool=True)\n\n    # fully conv layer\n    fconv6 = conv_layer(pool5_3, 6, 12, 1, 1, 4096, name=""fconv6"", activation_method=None, padding=""VALID"")\n    bn7 = batch_normalization(fconv6, name=""BN7"", activation_method=""tanh"", is_train=is_bn)\n\n    fconv8 = conv_layer(bn7, 1, 1, 1, 1, 512, name=""fconv8"", activation_method=None, padding=""VALID"")\n    bn9 = batch_normalization(fconv8, name=""BN9"", activation_method=""tanh"", is_train=is_bn)\n\n    fconv10 = conv_layer(bn9, 1, 1, 1, 1, 2, name=""fconv10"", activation_method=None, padding=""VALID"")\n    bn11 = batch_normalization(fconv10, name=""BN11"", activation_method=""tanh"", is_train=is_bn)\n\n    # Global Max Pooling\n    bn11_shape = bn11.get_shape()\n    bn11_height, bn11_width = bn11_shape[1], bn11_shape[2]\n    pool11 = pool_layer(bn11, bn11_height, bn11_width, bn11_height, bn11_width, name=""global_max"", is_max_pool=True)\n\n    logits = tf.reshape(pool11, [-1, class_num])\n\n    return logits\n\n\ndef chap4_5(input_data, class_num=2, is_bn=True):\n    """"""\n    Average pooling\n    """"""\n\n    print(""Average pooling"")\n    print(""Network Structure: "")\n\n    # High Pass Filtering\n    conv0 = rich_hpf_layer(input_data, name=""HPFs"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group1\n    conv1_1 = conv_layer(conv0_input_merge, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""tanh"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 1, 1, 1, 1, 32, name=""conv1_2"", activation_method=None, padding=""SAME"")\n    bn1_3 = batch_normalization(conv1_2, name=""BN1_3"", activation_method=""tanh"", is_train=is_bn)\n    pool1_4 = pool_layer(bn1_3, 2, 2, 2, 2, name=""pool1_4"", is_max_pool=False)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_4, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 1, 1, 1, 1, 64, name=""conv2_2"", activation_method=None, padding=""SAME"")\n    bn2_3 = batch_normalization(conv2_2, name=""BN2_3"", activation_method=""tanh"", is_train=is_bn)\n    pool2_4 = pool_layer(bn2_3, 2, 2, 2, 2, name=""pool2_4"", is_max_pool=False)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_4, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 1, 1, 1, 1, 128, name=""conv3_2"", activation_method=None, padding=""SAME"")\n    bn3_3 = batch_normalization(conv3_2, name=""BN3_3"", activation_method=""tanh"", is_train=is_bn)\n    pool3_4 = pool_layer(bn3_3, 2, 2, 2, 2, name=""pool3_4"", is_max_pool=False)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_4, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""tanh"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    bn4_3 = batch_normalization(conv4_2, name=""BN4_3"", activation_method=""tanh"", is_train=is_bn)\n    pool4_4 = pool_layer(bn4_3, 2, 2, 2, 2, name=""pool4_4"", is_max_pool=False)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_4, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=""tanh"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=None, padding=""SAME"")\n    bn5_3 = batch_normalization(conv5_2, name=""BN5_3"", activation_method=""tanh"", is_train=is_bn)\n    pool5_4 = pool_layer(bn5_3, 2, 2, 2, 2, name=""pool5_4"", is_max_pool=False)\n\n    # fully conv layer\n    fconv6 = conv_layer(pool5_4, 6, 12, 1, 1, 4096, name=""fconv6"", activation_method=None, padding=""VALID"")\n    bn7 = batch_normalization(fconv6, name=""BN7"", activation_method=""tanh"", is_train=is_bn)\n\n    fconv8 = conv_layer(bn7, 1, 1, 1, 1, 512, name=""fconv8"", activation_method=None, padding=""VALID"")\n    bn9 = batch_normalization(fconv8, name=""BN9"", activation_method=""tanh"", is_train=is_bn)\n\n    fconv10 = conv_layer(bn9, 1, 1, 1, 1, 2, name=""fconv10"", activation_method=None, padding=""VALID"")\n    bn11 = batch_normalization(fconv10, name=""BN11"", activation_method=""tanh"", is_train=is_bn)\n\n    # Global Max Pooling\n    bn11_shape = bn11.get_shape()\n    bn11_height, bn11_width = bn11_shape[1], bn11_shape[2]\n    pool11 = pool_layer(bn11, bn11_height, bn11_width, bn11_height, bn11_width, name=""global_max"", is_max_pool=True)\n\n    logits = tf.reshape(pool11, [-1, class_num])\n\n    return logits\n\n\ndef chap4_6(input_data, class_num=2, is_bn=True):\n    """"""\n    5 x 5 convolutional kernel is used\n    """"""\n\n    print(""5 x 5 convolutional kernel is used"")\n    print(""Network Structure: "")\n\n    # High Pass Filtering\n    conv0 = rich_hpf_layer(input_data, name=""HPFs"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group1\n    conv1_1 = conv_layer(conv0_input_merge, 5, 5, 1, 1, 16, name=""conv1_1"", activation_method=""tanh"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 1, 1, 1, 1, 32, name=""conv1_2"", activation_method=None, padding=""SAME"")\n    bn1_3 = batch_normalization(conv1_2, name=""BN1_3"", activation_method=""tanh"", is_train=is_bn)\n    pool1_4 = pool_layer(bn1_3, 2, 2, 2, 2, name=""pool1_4"", is_max_pool=False)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_4, 5, 5, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 1, 1, 1, 1, 64, name=""conv2_2"", activation_method=None, padding=""SAME"")\n    bn2_3 = batch_normalization(conv2_2, name=""BN2_3"", activation_method=""tanh"", is_train=is_bn)\n    pool2_4 = pool_layer(bn2_3, 2, 2, 2, 2, name=""pool2_4"", is_max_pool=False)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_4, 5, 5, 1, 1, 64, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 1, 1, 1, 1, 128, name=""conv3_2"", activation_method=None, padding=""SAME"")\n    bn3_3 = batch_normalization(conv3_2, name=""BN3_3"", activation_method=""tanh"", is_train=is_bn)\n    pool3_4 = pool_layer(bn3_3, 2, 2, 2, 2, name=""pool3_4"", is_max_pool=False)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_4, 5, 5, 1, 1, 128, name=""conv4_1"", activation_method=""tanh"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    bn4_3 = batch_normalization(conv4_2, name=""BN4_3"", activation_method=""tanh"", is_train=is_bn)\n    pool4_4 = pool_layer(bn4_3, 2, 2, 2, 2, name=""pool4_4"", is_max_pool=False)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_4, 5, 5, 1, 1, 256, name=""conv5_1"", activation_method=""tanh"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=None, padding=""SAME"")\n    bn5_3 = batch_normalization(conv5_2, name=""BN5_3"", activation_method=""tanh"", is_train=is_bn)\n    pool5_4 = pool_layer(bn5_3, 2, 2, 2, 2, name=""pool5_4"", is_max_pool=False)\n\n    # fully conv layer\n    fconv6 = conv_layer(pool5_4, 6, 12, 1, 1, 4096, name=""fconv6"", activation_method=None, padding=""VALID"")\n    bn7 = batch_normalization(fconv6, name=""BN7"", activation_method=""tanh"", is_train=is_bn)\n\n    fconv8 = conv_layer(bn7, 1, 1, 1, 1, 512, name=""fconv8"", activation_method=None, padding=""VALID"")\n    bn9 = batch_normalization(fconv8, name=""BN9"", activation_method=""tanh"", is_train=is_bn)\n\n    fconv10 = conv_layer(bn9, 1, 1, 1, 1, 2, name=""fconv10"", activation_method=None, padding=""VALID"")\n    bn11 = batch_normalization(fconv10, name=""BN11"", activation_method=""tanh"", is_train=is_bn)\n\n    # Global Max Pooling\n    bn11_shape = bn11.get_shape()\n    bn11_height, bn11_width = bn11_shape[1], bn11_shape[2]\n    pool11 = pool_layer(bn11, bn11_height, bn11_width, bn11_height, bn11_width, name=""global_max"", is_max_pool=True)\n\n    logits = tf.reshape(pool11, [-1, class_num])\n\n    return logits\n\n\ndef chap4_7(input_data, class_num=2, is_bn=True):\n    """"""\n    Type of activation function: ReLu\n    """"""\n\n    print(""Network of Chapter4_5"")\n    print(""Network Structure: "")\n\n    # High Pass Filtering\n    conv0 = rich_hpf_layer(input_data, name=""HPFs"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group1\n    conv1_1 = conv_layer(conv0_input_merge, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""relu"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 1, 1, 1, 1, 32, name=""conv1_2"", activation_method=None, padding=""SAME"")\n    bn1_3 = batch_normalization(conv1_2, name=""BN1_3"", activation_method=""relu"", is_train=is_bn)\n    pool1_4 = pool_layer(bn1_3, 2, 2, 2, 2, name=""pool1_4"", is_max_pool=False)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_4, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""relu"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 1, 1, 1, 1, 64, name=""conv2_2"", activation_method=None, padding=""SAME"")\n    bn2_3 = batch_normalization(conv2_2, name=""BN2_3"", activation_method=""relu"", is_train=is_bn)\n    pool2_4 = pool_layer(bn2_3, 2, 2, 2, 2, name=""pool2_4"", is_max_pool=False)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_4, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""relu"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 1, 1, 1, 1, 128, name=""conv3_2"", activation_method=None, padding=""SAME"")\n    bn3_3 = batch_normalization(conv3_2, name=""BN3_3"", activation_method=""relu"", is_train=is_bn)\n    pool3_4 = pool_layer(bn3_3, 2, 2, 2, 2, name=""pool3_4"", is_max_pool=False)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_4, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""relu"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    bn4_3 = batch_normalization(conv4_2, name=""BN4_3"", activation_method=""relu"", is_train=is_bn)\n    pool4_4 = pool_layer(bn4_3, 2, 2, 2, 2, name=""pool4_4"", is_max_pool=False)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_4, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=""relu"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=None, padding=""SAME"")\n    bn5_3 = batch_normalization(conv5_2, name=""BN5_3"", activation_method=""relu"", is_train=is_bn)\n    pool5_4 = pool_layer(bn5_3, 2, 2, 2, 2, name=""pool5_4"", is_max_pool=False)\n\n    # fully conv layer\n    fconv6 = conv_layer(pool5_4, 6, 12, 1, 1, 4096, name=""fconv6"", activation_method=None, padding=""VALID"")\n    bn7 = batch_normalization(fconv6, name=""BN7"", activation_method=""relu"", is_train=is_bn)\n\n    fconv8 = conv_layer(bn7, 1, 1, 1, 1, 512, name=""fconv8"", activation_method=None, padding=""VALID"")\n    bn9 = batch_normalization(fconv8, name=""BN9"", activation_method=""relu"", is_train=is_bn)\n\n    fconv10 = conv_layer(bn9, 1, 1, 1, 1, 2, name=""fconv10"", activation_method=None, padding=""VALID"")\n    bn11 = batch_normalization(fconv10, name=""BN11"", activation_method=""relu"", is_train=is_bn)\n\n    # Global Max Pooling\n    bn11_shape = bn11.get_shape()\n    bn11_height, bn11_width = bn11_shape[1], bn11_shape[2]\n    pool11 = pool_layer(bn11, bn11_height, bn11_width, bn11_height, bn11_width, name=""global_max"", is_max_pool=True)\n\n    logits = tf.reshape(pool11, [-1, class_num])\n\n    return logits\n\n\ndef chap4_8(input_data, class_num=2, is_bn=True):\n    """"""\n    Type of activation function: leaky relu\n    """"""\n\n    print(""Type of activation function: leaky relu"")\n    print(""Network Structure: "")\n\n    # High Pass Filtering\n    conv0 = rich_hpf_layer(input_data, name=""HPFs"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group1\n    conv1_1 = conv_layer(conv0_input_merge, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""leakrelu"", alpha=0.2, padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 1, 1, 1, 1, 32, name=""conv1_2"", activation_method=None, padding=""SAME"")\n    bn1_3 = batch_normalization(conv1_2, name=""BN1_3"", activation_method=""leakrelu"", is_train=is_bn)\n    pool1_4 = pool_layer(bn1_3, 2, 2, 2, 2, name=""pool1_4"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_4, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 1, 1, 1, 1, 64, name=""conv2_2"", activation_method=None, padding=""SAME"")\n    bn2_3 = batch_normalization(conv2_2, name=""BN2_3"", activation_method=""leakrelu"", is_train=is_bn)\n    pool2_4 = pool_layer(bn2_3, 2, 2, 2, 2, name=""pool2_4"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_4, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""leakrelu"", alpha=0.2, padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 1, 1, 1, 1, 128, name=""conv3_2"", activation_method=None, padding=""SAME"")\n    bn3_3 = batch_normalization(conv3_2, name=""BN3_3"", activation_method=""leakrelu"", is_train=is_bn)\n    pool3_4 = pool_layer(bn3_3, 2, 2, 2, 2, name=""pool3_4"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_4, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""leakrelu"", alpha=0.2, padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    bn4_3 = batch_normalization(conv4_2, name=""BN4_3"", activation_method=""leakrelu"", is_train=is_bn)\n    pool4_4 = pool_layer(bn4_3, 2, 2, 2, 2, name=""pool4_4"", is_max_pool=True)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_4, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=""leakrelu"", alpha=0.2, padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=None, padding=""SAME"")\n    bn5_3 = batch_normalization(conv5_2, name=""BN5_3"", activation_method=""leakrelu"", is_train=is_bn)\n    pool5_4 = pool_layer(bn5_3, 2, 2, 2, 2, name=""pool5_4"", is_max_pool=True)\n\n    # fully conv layer\n    fconv6 = conv_layer(pool5_4, 6, 12, 1, 1, 4096, name=""fconv6"", activation_method=None, padding=""VALID"")\n    bn7 = batch_normalization(fconv6, name=""BN7"", activation_method=""leakrelu"", is_train=is_bn)\n    bn7 = activation_layer(bn7, activation_method=""leakrelu"", alpha=0.2)\n\n    fconv8 = conv_layer(bn7, 1, 1, 1, 1, 512, name=""fconv8"", activation_method=None, padding=""VALID"")\n    bn9 = batch_normalization(fconv8, name=""BN9"", activation_method=""leakrelu"", is_train=is_bn)\n    bn9 = activation_layer(bn9, activation_method=""leakrelu"", alpha=0.2)\n\n    fconv10 = conv_layer(bn9, 1, 1, 1, 1, 2, name=""fconv10"", activation_method=None, padding=""VALID"")\n    bn11 = batch_normalization(fconv10, name=""BN11"", activation_method=""leakrelu"", is_train=is_bn)\n    bn11 = activation_layer(bn11, activation_method=""leakrelu"", alpha=0.2)\n\n    # Global Max Pooling\n    bn11_shape = bn11.get_shape()\n    bn11_height, bn11_width = bn11_shape[1], bn11_shape[2]\n    pool11 = pool_layer(bn11, bn11_height, bn11_width, bn11_height, bn11_width, name=""global_max"", is_max_pool=True)\n\n    logits = tf.reshape(pool11, [-1, class_num])\n\n    return logits\n\n\ndef chap4_9(input_data, class_num=2, is_bn=True):\n    """"""\n    Quit replacing fc layers with conv layer\n    """"""\n\n    print(""Quit replacing fc layers with conv layer"")\n    print(""Network Structure: "")\n\n    # High Pass Filtering\n    conv0 = rich_hpf_layer(input_data, name=""HPFs"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group1\n    conv1_1 = conv_layer(conv0_input_merge, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""tanh"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 1, 1, 1, 1, 32, name=""conv1_2"", activation_method=None, padding=""SAME"")\n    bn1_3 = batch_normalization(conv1_2, name=""BN1_3"", activation_method=""tanh"", is_train=is_bn)\n    pool1_4 = pool_layer(bn1_3, 2, 2, 2, 2, name=""pool1_4"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_4, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 1, 1, 1, 1, 64, name=""conv2_2"", activation_method=None, padding=""SAME"")\n    bn2_3 = batch_normalization(conv2_2, name=""BN2_3"", activation_method=""tanh"", is_train=is_bn)\n    pool2_4 = pool_layer(bn2_3, 2, 2, 2, 2, name=""pool2_4"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_4, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 1, 1, 1, 1, 128, name=""conv3_2"", activation_method=None, padding=""SAME"")\n    bn3_3 = batch_normalization(conv3_2, name=""BN3_3"", activation_method=""tanh"", is_train=is_bn)\n    pool3_4 = pool_layer(bn3_3, 2, 2, 2, 2, name=""pool3_4"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_4, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""tanh"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    bn4_3 = batch_normalization(conv4_2, name=""BN4_3"", activation_method=""tanh"", is_train=is_bn)\n    pool4_4 = pool_layer(bn4_3, 2, 2, 2, 2, name=""pool4_4"", is_max_pool=True)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_4, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=""tanh"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=None, padding=""SAME"")\n    bn5_3 = batch_normalization(conv5_2, name=""BN5_3"", activation_method=""tanh"", is_train=is_bn)\n    pool5_4 = pool_layer(bn5_3, 2, 2, 2, 2, name=""pool5_4"", is_max_pool=True)\n\n    # fc layers\n    fc6 = fc_layer(pool5_4, 4096, name=""fc6"", activation_method=None)\n    bn7 = batch_normalization(fc6, name=""BN7"", activation_method=""tanh"", is_train=is_bn)\n    fc8 = fc_layer(bn7, 512, name=""fc8"", activation_method=None)\n    bn9 = batch_normalization(fc8, name=""BN9"", activation_method=""tanh"", is_train=is_bn)\n\n    logits = fc_layer(bn9, class_num, name=""fc10"", activation_method=None)\n\n    return logits\n\n\ndef chap4_10(input_data, class_num=2, is_bn=True):\n    """"""\n    Deepen the network\n    """"""\n\n    print(""Deepen the network"")\n    print(""Network Structure: "")\n\n    # High Pass Filtering\n    conv0 = rich_hpf_layer(input_data, name=""HPFs"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group1\n    conv1_1 = conv_layer(conv0_input_merge, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""tanh"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 1, 1, 1, 1, 32, name=""conv1_2"", activation_method=None, padding=""SAME"")\n    bn1_3 = batch_normalization(conv1_2, name=""BN1_3"", activation_method=""tanh"", is_train=is_bn)\n    pool1_4 = pool_layer(bn1_3, 2, 2, 2, 2, name=""pool1_4"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_4, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 1, 1, 1, 1, 64, name=""conv2_2"", activation_method=None, padding=""SAME"")\n    bn2_3 = batch_normalization(conv2_2, name=""BN2_3"", activation_method=""tanh"", is_train=is_bn)\n    pool2_4 = pool_layer(bn2_3, 2, 2, 2, 2, name=""pool2_4"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_4, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 1, 1, 1, 1, 128, name=""conv3_2"", activation_method=None, padding=""SAME"")\n    bn3_3 = batch_normalization(conv3_2, name=""BN3_3"", activation_method=""tanh"", is_train=is_bn)\n    pool3_4 = pool_layer(bn3_3, 2, 2, 2, 2, name=""pool3_4"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_4, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""tanh"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    bn4_3 = batch_normalization(conv4_2, name=""BN4_3"", activation_method=""tanh"", is_train=is_bn)\n    pool4_4 = pool_layer(bn4_3, 2, 2, 2, 2, name=""pool4_4"", is_max_pool=True)\n\n    # Group5\n    conv5_1 = conv_layer(pool4_4, 3, 3, 1, 1, 256, name=""conv5_1"", activation_method=""tanh"", padding=""SAME"")\n    conv5_2 = conv_layer(conv5_1, 1, 1, 1, 1, 512, name=""conv5_2"", activation_method=None, padding=""SAME"")\n    bn5_3 = batch_normalization(conv5_2, name=""BN5_3"", activation_method=""tanh"", is_train=is_bn)\n    pool5_4 = pool_layer(bn5_3, 2, 2, 2, 2, name=""pool5_4"", is_max_pool=True)\n\n    # Group6\n    conv6_1 = conv_layer(pool5_4, 3, 3, 1, 1, 256, name=""conv6_1"", activation_method=""tanh"", padding=""SAME"")\n    conv6_2 = conv_layer(conv6_1, 1, 1, 1, 1, 512, name=""conv6_2"", activation_method=None, padding=""SAME"")\n    bn6_3 = batch_normalization(conv6_2, name=""BN6_3"", activation_method=""tanh"", is_train=is_bn)\n    pool6_4 = pool_layer(bn6_3, 2, 2, 2, 2, name=""pool6_4"", is_max_pool=True)\n\n    # fully conv layer\n    fconv7 = conv_layer(pool6_4, 3, 6, 1, 1, 4096, name=""fconv6"", activation_method=None, padding=""VALID"")\n    bn8 = batch_normalization(fconv7, name=""BN7"", activation_method=""tanh"", is_train=is_bn)\n\n    fconv9 = conv_layer(bn8, 1, 1, 1, 1, 512, name=""fconv8"", activation_method=None, padding=""VALID"")\n    bn10 = batch_normalization(fconv9, name=""BN9"", activation_method=""tanh"", is_train=is_bn)\n\n    fconv11 = conv_layer(bn10, 1, 1, 1, 1, 2, name=""fconv10"", activation_method=None, padding=""VALID"")\n    bn12 = batch_normalization(fconv11, name=""BN11"", activation_method=""tanh"", is_train=is_bn)\n\n    # Global Max Pooling\n    bn12_shape = bn12.get_shape()\n    bn12_height, bn12_width = bn12_shape[1], bn12_shape[2]\n    pool13 = pool_layer(bn12, bn12_height, bn12_width, bn12_height, bn12_width, name=""global_max"", is_max_pool=True)\n\n    logits = tf.reshape(pool13, [-1, class_num])\n\n    return logits\n\n\ndef mfcc_net(input_data, class_num=2, is_bn=True):\n    """"""\n    Network with the input data of mfcc\n    """"""\n\n    print(""Network with the input data of mfcc"")\n    print(""Network Structure: "")\n\n    # Group1\n    conv1_1 = conv_layer(input_data, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""relu"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 1, 1, 1, 1, 32, name=""conv1_2"", activation_method=None, padding=""SAME"")\n    bn1_3 = batch_normalization(conv1_2, name=""BN1_3"", activation_method=""relu"", is_train=is_bn)\n    pool1_4 = pool_layer(bn1_3, 2, 2, 2, 2, name=""pool1_4"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_4, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""relu"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 1, 1, 1, 1, 64, name=""conv2_2"", activation_method=None, padding=""SAME"")\n    bn2_3 = batch_normalization(conv2_2, name=""BN2_3"", activation_method=""relu"", is_train=is_bn)\n    pool2_4 = pool_layer(bn2_3, 2, 2, 2, 2, name=""pool2_4"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_4, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""relu"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 1, 1, 1, 1, 128, name=""conv3_2"", activation_method=None, padding=""SAME"")\n    bn3_3 = batch_normalization(conv3_2, name=""BN3_3"", activation_method=""relu"", is_train=is_bn)\n    pool3_4 = pool_layer(bn3_3, 2, 2, 2, 2, name=""pool3_4"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_4, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""relu"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    bn4_3 = batch_normalization(conv4_2, name=""BN4_3"", activation_method=""relu"", is_train=is_bn)\n    pool4_4 = pool_layer(bn4_3, 2, 2, 2, 2, name=""pool4_4"", is_max_pool=True)\n\n    # fc layers\n    fc6 = fc_layer(pool4_4, 1024, name=""fc6"", activation_method=None)\n    bn7 = batch_normalization(fc6, name=""BN7"", activation_method=""tanh"", is_train=is_bn)\n    fc8 = fc_layer(bn7, 512, name=""fc8"", activation_method=None)\n    bn9 = batch_normalization(fc8, name=""BN9"", activation_method=""tanh"", is_train=is_bn)\n\n    logits = fc_layer(bn9, class_num, name=""fc10"", activation_method=None)\n\n    return logits\n\n\ndef mfcc_net1(input_data, class_num=2, is_bn=True):\n    """"""\n    Network with the input data of mfcc\n    """"""\n\n    print(""Network with the input data of mfcc"")\n    print(""Network Structure: "")\n\n    # Group1\n    conv1_1 = conv_layer(input_data, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=""tanh"", padding=""SAME"")\n    conv1_2 = conv_layer(conv1_1, 1, 1, 1, 1, 32, name=""conv1_2"", activation_method=None, padding=""SAME"")\n    bn1_3 = batch_normalization(conv1_2, name=""BN1_3"", activation_method=""tanh"", is_train=is_bn)\n    pool1_4 = pool_layer(bn1_3, 2, 2, 2, 2, name=""pool1_4"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_4, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=""tanh"", padding=""SAME"")\n    conv2_2 = conv_layer(conv2_1, 1, 1, 1, 1, 64, name=""conv2_2"", activation_method=None, padding=""SAME"")\n    bn2_3 = batch_normalization(conv2_2, name=""BN2_3"", activation_method=""tanh"", is_train=is_bn)\n    pool2_4 = pool_layer(bn2_3, 2, 2, 2, 2, name=""pool2_4"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_4, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=""tanh"", padding=""SAME"")\n    conv3_2 = conv_layer(conv3_1, 1, 1, 1, 1, 128, name=""conv3_2"", activation_method=None, padding=""SAME"")\n    bn3_3 = batch_normalization(conv3_2, name=""BN3_3"", activation_method=""tanh"", is_train=is_bn)\n    pool3_4 = pool_layer(bn3_3, 2, 2, 2, 2, name=""pool3_4"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_4, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=""tanh"", padding=""SAME"")\n    conv4_2 = conv_layer(conv4_1, 1, 1, 1, 1, 256, name=""conv4_2"", activation_method=None, padding=""SAME"")\n    bn4_3 = batch_normalization(conv4_2, name=""BN4_3"", activation_method=""tanh"", is_train=is_bn)\n    pool4_4 = pool_layer(bn4_3, 2, 2, 2, 2, name=""pool4_4"", is_max_pool=True)\n\n    # fc layers\n    fc6 = fc_layer(pool4_4, 1024, name=""fc6"", activation_method=None)\n    bn7 = batch_normalization(fc6, name=""BN7"", activation_method=""tanh"", is_train=is_bn)\n    fc8 = fc_layer(bn7, 512, name=""fc8"", activation_method=None)\n    bn9 = batch_normalization(fc8, name=""BN9"", activation_method=""tanh"", is_train=is_bn)\n\n    logits = fc_layer(bn9, class_num, name=""fc10"", activation_method=None)\n\n    return logits\n\n\ndef mfcc_net2(input_data, class_num=2, is_bn=True):\n    """"""\n    Network with the input data of mfcc\n    """"""\n\n    print(""Network with the input data of mfcc"")\n    print(""Network Structure: "")\n\n    # Group1\n    conv1_1 = conv_layer(input_data, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=None, padding=""SAME"")\n    bn1_2 = batch_normalization(conv1_1, name=""BN1_2"", activation_method=""tanh"", is_train=is_bn)\n    conv1_2 = conv_layer(bn1_2, 1, 1, 1, 1, 32, name=""conv1_3"", activation_method=None, padding=""SAME"")\n    bn1_3 = batch_normalization(conv1_2, name=""BN1_4"", activation_method=""tanh"", is_train=is_bn)\n    pool1_4 = pool_layer(bn1_3, 2, 2, 2, 2, name=""pool1_5"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_4, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=None, padding=""SAME"")\n    bn2_2 = batch_normalization(conv2_1, name=""BN2_2"", activation_method=""tanh"", is_train=is_bn)\n    conv2_3 = conv_layer(bn2_2, 1, 1, 1, 1, 64, name=""conv2_3"", activation_method=None, padding=""SAME"")\n    bn2_4 = batch_normalization(conv2_3, name=""BN2_4"", activation_method=""tanh"", is_train=is_bn)\n    pool2_5 = pool_layer(bn2_4, 2, 2, 2, 2, name=""pool2_5"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_5, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=None, padding=""SAME"")\n    bn3_2 = batch_normalization(conv3_1, name=""BN3_2"", activation_method=""tanh"", is_train=is_bn)\n    conv3_3 = conv_layer(bn3_2, 1, 1, 1, 1, 128, name=""conv3_3"", activation_method=None, padding=""SAME"")\n    bn3_4 = batch_normalization(conv3_3, name=""BN3_4"", activation_method=""tanh"", is_train=is_bn)\n    pool3_5 = pool_layer(bn3_4, 2, 2, 2, 2, name=""pool3_5"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_5, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=None, padding=""SAME"")\n    bn4_2 = batch_normalization(conv4_1, name=""BN4_2"", activation_method=""tanh"", is_train=is_bn)\n    conv4_3 = conv_layer(bn4_2, 1, 1, 1, 1, 256, name=""conv4_3"", activation_method=None, padding=""SAME"")\n    bn4_4 = batch_normalization(conv4_3, name=""BN4_4"", activation_method=""tanh"", is_train=is_bn)\n    pool4_5 = pool_layer(bn4_4, 2, 2, 2, 2, name=""pool4_5"", is_max_pool=True)\n\n    # fc layers\n    fc6 = fc_layer(pool4_5, 1024, name=""fc6"", activation_method=None)\n    bn7 = batch_normalization(fc6, name=""BN7"", activation_method=""tanh"", is_train=is_bn)\n    fc8 = fc_layer(bn7, 512, name=""fc8"", activation_method=None)\n    bn9 = batch_normalization(fc8, name=""BN9"", activation_method=""tanh"", is_train=is_bn)\n\n    logits = fc_layer(bn9, class_num, name=""fc10"", activation_method=None)\n\n    return logits\n\n\ndef mfcc_net3(input_data, class_num=2, is_bn=True):\n    """"""\n    Network with the input data of mfcc\n    """"""\n\n    print(""Network with the input data of mfcc"")\n    print(""Network Structure: "")\n\n    # Group1\n    conv1_1 = conv_layer(input_data, 3, 3, 1, 1, 16, name=""conv1_1"", activation_method=None, padding=""SAME"")\n    bn1_2 = batch_normalization(conv1_1, name=""BN1_2"", activation_method=""relu"", is_train=is_bn)\n    conv1_2 = conv_layer(bn1_2, 1, 1, 1, 1, 32, name=""conv1_3"", activation_method=None, padding=""SAME"")\n    bn1_3 = batch_normalization(conv1_2, name=""BN1_4"", activation_method=""relu"", is_train=is_bn)\n    pool1_4 = pool_layer(bn1_3, 2, 2, 2, 2, name=""pool1_5"", is_max_pool=True)\n\n    # Group2\n    conv2_1 = conv_layer(pool1_4, 3, 3, 1, 1, 32, name=""conv2_1"", activation_method=None, padding=""SAME"")\n    bn2_2 = batch_normalization(conv2_1, name=""BN2_2"", activation_method=""relu"", is_train=is_bn)\n    conv2_3 = conv_layer(bn2_2, 1, 1, 1, 1, 64, name=""conv2_3"", activation_method=None, padding=""SAME"")\n    bn2_4 = batch_normalization(conv2_3, name=""BN2_4"", activation_method=""relu"", is_train=is_bn)\n    pool2_5 = pool_layer(bn2_4, 2, 2, 2, 2, name=""pool2_5"", is_max_pool=True)\n\n    # Group3\n    conv3_1 = conv_layer(pool2_5, 3, 3, 1, 1, 64, name=""conv3_1"", activation_method=None, padding=""SAME"")\n    bn3_2 = batch_normalization(conv3_1, name=""BN3_2"", activation_method=""relu"", is_train=is_bn)\n    conv3_3 = conv_layer(bn3_2, 1, 1, 1, 1, 128, name=""conv3_3"", activation_method=None, padding=""SAME"")\n    bn3_4 = batch_normalization(conv3_3, name=""BN3_4"", activation_method=""relu"", is_train=is_bn)\n    pool3_5 = pool_layer(bn3_4, 2, 2, 2, 2, name=""pool3_5"", is_max_pool=True)\n\n    # Group4\n    conv4_1 = conv_layer(pool3_5, 3, 3, 1, 1, 128, name=""conv4_1"", activation_method=None, padding=""SAME"")\n    bn4_2 = batch_normalization(conv4_1, name=""BN4_2"", activation_method=""relu"", is_train=is_bn)\n    conv4_3 = conv_layer(bn4_2, 1, 1, 1, 1, 256, name=""conv4_3"", activation_method=None, padding=""SAME"")\n    bn4_4 = batch_normalization(conv4_3, name=""BN4_4"", activation_method=""relu"", is_train=is_bn)\n    pool4_5 = pool_layer(bn4_4, 2, 2, 2, 2, name=""pool4_5"", is_max_pool=True)\n\n    # fc layers\n    fc6 = fc_layer(pool4_5, 1024, name=""fc6"", activation_method=None)\n    bn7 = batch_normalization(fc6, name=""BN7"", activation_method=""relu"", is_train=is_bn)\n    fc8 = fc_layer(bn7, 512, name=""fc8"", activation_method=None)\n    bn9 = batch_normalization(fc8, name=""BN9"", activation_method=""relu"", is_train=is_bn)\n\n    logits = fc_layer(bn9, class_num, name=""fc10"", activation_method=None)\n\n    return logits\n\n\ndef google_net1(input_data, class_num=2, is_bn=True):\n    """"""\n    Google Net with inception V1\n    """"""\n\n    print(""Google Net with inception V1"")\n    print(""Network Structure: "")\n\n    # High Pass Filtering\n    conv0 = rich_hpf_layer(input_data, name=""HPFs"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group 1\n    conv1_1 = inception_v1(conv0_input_merge, filter_num=8, name=""conv1_1"", is_max_pool=True, is_bn=is_bn)\n    pool1_2 = pool_layer(conv1_1, 3, 3, 2, 2, name=""pool1_2"")\n\n    # Group 2\n    conv2_1 = inception_v1(pool1_2, filter_num=16, name=""conv2_1"", is_max_pool=True, is_bn=is_bn)\n    pool2_2 = pool_layer(conv2_1, 3, 3, 2, 2, name=""pool2_2"")\n\n    # Group 3\n    conv3_1 = inception_v1(pool2_2, filter_num=32, name=""conv3_1"", is_max_pool=True, is_bn=is_bn)\n    pool3_2 = pool_layer(conv3_1, 3, 3, 2, 2, name=""pool3_2"")\n\n    # Group 4\n    conv4_1 = inception_v1(pool3_2, filter_num=64, name=""conv4_1"", is_max_pool=True, is_bn=is_bn)\n    pool4_2 = pool_layer(conv4_1, 3, 3, 2, 2, name=""pool4_2"")\n\n    # Group 5\n    conv5_1 = inception_v1(pool4_2, filter_num=128, name=""conv5_1"", is_max_pool=True, is_bn=is_bn)\n    pool5_2 = pool_layer(conv5_1, 3, 3, 2, 2, name=""pool5_2"")\n\n    # Group 6\n    conv6_1 = inception_v1(pool5_2, filter_num=256, name=""conv6_1"", is_max_pool=True, is_bn=is_bn)\n    pool6_2 = global_pool(conv6_1, name=""global_pool6_2"")\n\n    # fc layers\n    fc6 = fc_layer(pool6_2, 128, name=""fc6"", activation_method=None)\n    bn7 = batch_normalization(fc6, name=""BN7"", activation_method=""tanh"", is_train=is_bn)\n\n    logits = fc_layer(bn7, class_num, name=""fc8"", activation_method=None)\n\n    return logits\n\n\ndef google_net2(input_data, class_num=2, is_bn=True):\n    """"""\n    Google Net with inception V2\n    """"""\n\n    print(""Google Net with inception V2"")\n    print(""Network Structure: "")\n\n    # High Pass Filtering\n    conv0 = rich_hpf_layer(input_data, name=""HPFs"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group 1\n    conv1_1 = inception_v2(conv0_input_merge, 8, ""conv1_1"", is_max_pool=True, is_bn=is_bn)\n    pool1_2 = pool_layer(conv1_1, 3, 3, 2, 2, name=""pool1_2"")\n\n    # Group 2\n    conv2_1 = inception_v2(pool1_2, 16, ""conv2_1"", is_max_pool=True, is_bn=is_bn)\n    pool2_2 = pool_layer(conv2_1, 3, 3, 2, 2, name=""pool2_2"")\n\n    # Group 3\n    conv3_1 = inception_v2(pool2_2, 32, ""conv3_1"", is_max_pool=True, is_bn=is_bn)\n    pool3_2 = pool_layer(conv3_1, 3, 3, 2, 2, name=""pool3_2"")\n\n    # Group 4\n    conv4_1 = inception_v2(pool3_2, 64, ""conv4_1"", is_max_pool=True, is_bn=is_bn)\n    pool4_2 = pool_layer(conv4_1, 3, 3, 2, 2, name=""pool4_2"")\n\n    # Group 5\n    conv5_1 = inception_v2(pool4_2, 128, ""conv5_1"", is_max_pool=True, is_bn=is_bn)\n    pool5_2 = pool_layer(conv5_1, 3, 3, 2, 2, name=""pool5_2"")\n\n    # Group 6\n    conv6_1 = inception_v2(pool5_2, 256, ""conv6_1"", is_max_pool=True, is_bn=is_bn)\n    pool6_2 = global_pool(conv6_1, name=""global_pool6_2"")\n\n    # fc layers\n    fc6 = fc_layer(pool6_2, 128, name=""fc6"", activation_method=None)\n    bn7 = batch_normalization(fc6, name=""BN7"", activation_method=""tanh"", is_train=is_bn)\n\n    logits = fc_layer(bn7, class_num, name=""fc8"", activation_method=None)\n\n    return logits\n\n\ndef google_net3(input_data, class_num=2, is_bn=True):\n    """"""\n    Google Net with inception V3\n    """"""\n\n    print(""Google Net with inception V3"")\n    print(""Network Structure: "")\n\n    # High Pass Filtering\n    conv0 = rich_hpf_layer(input_data, name=""HPFs"")\n\n    # HPF and input data concat\n    conv0_input_merge = tf.concat([conv0, input_data], 3, name=""conv0_input_merge"")\n    concat_shape = conv0_input_merge.get_shape()\n    print(""name: %s, shape: (%d, %d, %d)"" % (""conv0_input_merge"", concat_shape[1], concat_shape[2], concat_shape[3]))\n\n    # Group 1\n    conv1_1 = inception_v3(conv0_input_merge, 8, ""conv1_1"", is_max_pool=True, is_bn=is_bn)\n    pool1_2 = pool_layer(conv1_1, 3, 3, 2, 2, name=""pool1_2"")\n\n    # Group 2\n    conv2_1 = inception_v3(pool1_2, 16, ""conv2_1"", is_max_pool=True, is_bn=is_bn)\n    pool2_2 = pool_layer(conv2_1, 3, 3, 2, 2, name=""pool2_2"")\n\n    # Group 3\n    conv3_1 = inception_v3(pool2_2, 32, ""conv3_1"", is_max_pool=True, is_bn=is_bn)\n    pool3_2 = pool_layer(conv3_1, 3, 3, 2, 2, name=""pool3_2"")\n\n    # Group 4\n    conv4_1 = inception_v3(pool3_2, 64, ""conv4_1"", is_max_pool=True, is_bn=is_bn)\n    pool4_2 = pool_layer(conv4_1, 3, 3, 2, 2, name=""pool4_2"")\n\n    # Group 5\n    conv5_1 = inception_v3(pool4_2, 128, ""conv5_1"", is_max_pool=True, is_bn=is_bn)\n    pool5_2 = pool_layer(conv5_1, 3, 3, 2, 2, name=""pool5_2"")\n\n    # Group 6\n    conv6_1 = inception_v3(pool5_2, 256, ""conv6_1"", is_max_pool=True, is_bn=is_bn)\n    pool6_2 = global_pool(conv6_1, name=""global_pool6_2"")\n\n    # fc layers\n    fc6 = fc_layer(pool6_2, 128, name=""fc6"", activation_method=None)\n    bn7 = batch_normalization(fc6, name=""BN7"", activation_method=""tanh"", is_train=is_bn)\n\n    logits = fc_layer(bn7, class_num, name=""fc8"", activation_method=None)\n\n    return logits\n'"
