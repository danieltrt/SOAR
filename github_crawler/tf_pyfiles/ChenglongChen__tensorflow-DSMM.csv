file_path,api_count,code
src/__init__.py,0,b''
src/config.py,0,"b'\n\nDATA_DIR = ""../data""\n\nTRAIN_FILE = DATA_DIR + ""/train.csv""\nTEST_FILE = DATA_DIR + ""/test.csv""\n\nTRAIN_FEATURES_FILE = DATA_DIR + ""/train_features.npy""\nTEST_FEATURES_FILE = DATA_DIR + ""/test_features.npy""\n\nQUESTION_FILE = DATA_DIR + ""/question.csv""\n\nWORD_EMBEDDING_FILE = DATA_DIR + ""/word_embed.txt""\nCHAR_EMBEDDING_FILE = DATA_DIR + ""/char_embed.txt""\n\nSUB_DIR = ""../sub""\nSUB_FILE = ""submission.csv""\nSINGLE_SUB_FILE_PATTERN = ""submission_%s_%s.csv""\nSTACKING_SUB_FILE_PATTERN = ""submission_%s.csv""\n\n\n# missing\nMISSING_INDEX_WORD = 20891\nPADDING_INDEX_WORD = 20892\n\nMISSING_INDEX_CHAR = 3048\nPADDING_INDEX_CHAR = 3049\n\n# ratio\nPOS_RATIO_OFFLINE = 0.5191087559849992\nPOS_RATIO_ONLINE = 0.50296075348400959\n\n""""""\n1/(p0 + p1) * (P0 * (0*log(0+eps) + (1-0)*log(1-0-eps)) + P1 * (1*log(0+eps) + (1-1)*log(1-0-eps))) = 17.371649\n1/(p0 + p1) * (p0 * log(1-eps) + p1 * log(0+eps)) = 17.371649\np1/(p0 + p1) ~= 17.371649/log(eps)\n              = 17.371649/log(1e-15)\n              = 0.50296075348400959\n""""""\n\nNUM_TRAIN = 254386\nNUM_TEST = 172956\n\nTRAIN_RATIO = 0.7\n\nSPLIT_FILE = ""split.pkl""\n'"
src/main.py,4,"b'\nimport numpy as np\nimport pandas as pd\nimport pickle as pkl\nimport tensorflow as tf\n\nfrom optparse import OptionParser\n\nimport config\n\nfrom inputs.data import load_question, load_train, load_test\nfrom inputs.data import init_embedding_matrix\nfrom models.model_library import get_model\nfrom utils import log_utils, os_utils, time_utils\n\n\nparams = {\n    ""model_name"": ""semantic_matching"",\n    ""offline_model_dir"": ""./weights/semantic_matching"",\n    ""summary_dir"": ""../summary"",\n    ""construct_neg"": False,\n\n    ""augmentation_init_permutation"": 0.5,\n    ""augmentation_min_permutation"": 0.01,\n    ""augmentation_permutation_decay_steps"": 2000,\n    ""augmentation_permutation_decay_rate"": 0.975,\n\n    ""augmentation_init_dropout"": 0.5,\n    ""augmentation_min_dropout"": 0.01,\n    ""augmentation_dropout_decay_steps"": 2000,\n    ""augmentation_dropout_decay_rate"": 0.975,\n\n    ""use_features"": False,\n    ""num_features"": 1,\n\n    ""n_runs"": 10,\n    ""batch_size"": 128,\n    ""epoch"": 50,\n    ""max_batch"": -1,\n    ""l2_lambda"": 0.000,\n\n    # embedding\n    ""embedding_dropout"": 0.3,\n    ""embedding_dim_word"": init_embedding_matrix[""word""].shape[1],\n    ""embedding_dim_char"": init_embedding_matrix[""char""].shape[1],\n    ""embedding_dim"": init_embedding_matrix[""word""].shape[1],\n    ""embedding_dim_compressed"": 32,\n    ""embedding_trainable"": True,\n    ""embedding_mask_zero"": True,\n\n    ""max_num_word"": init_embedding_matrix[""word""].shape[0],\n    ""max_num_char"": init_embedding_matrix[""char""].shape[0],\n\n    ""threshold"": 0.217277,\n    ""calibration"": False,\n\n    ""max_seq_len_word"": 12,\n    ""max_seq_len_char"": 20,\n    ""pad_sequences_padding"": ""post"",\n    ""pad_sequences_truncating"": ""post"",\n\n    # optimization\n    ""optimizer_type"": ""lazyadam"",\n    ""init_lr"": 0.001,\n    ""beta1"": 0.9,\n    ""beta2"": 0.999,\n    ""decay_steps"": 2000,\n    ""decay_rate"": 0.95,\n    ""schedule_decay"": 0.004,\n    ""random_seed"": 2018,\n    ""eval_every_num_update"": 5000,\n\n    # semantic feature layer\n    ""encode_method"": ""textcnn"",\n    ""attend_method"": [""ave"", ""max"", ""min"", ""self-scalar-attention""],\n    ""attention_dim"": 64,\n    ""attention_num_heads"": 1,\n\n    # cnn\n    ""cnn_num_layers"": 1,\n    ""cnn_num_filters"": 32,\n    ""cnn_filter_sizes"": [1, 2, 3],\n    ""cnn_timedistributed"": False,\n    ""cnn_activation"": tf.nn.relu,\n    ""cnn_gated_conv"": False,\n    ""cnn_residual"": False,\n\n    ""rnn_num_units"": 32,\n    ""rnn_cell_type"": ""gru"",\n    ""rnn_num_layers"": 1,\n\n    # fc block\n    ""fc_type"": ""fc"",\n    ""fc_hidden_units"": [64*4, 64*2, 64],\n    ""fc_dropouts"": [0, 0, 0],\n\n    # True: cosine(l1, l2), sum(abs(l1 - l2))\n    # False: l1 * l2, abs(l1 - l2)\n    ""similarity_aggregation"": False,\n\n    # match pyramid\n    ""mp_num_filters"": [8, 16],\n    ""mp_filter_sizes"": [5, 3],\n    ""mp_activation"": tf.nn.relu,\n    ""mp_dynamic_pooling"": False,\n    ""mp_pool_sizes_word"": [6, 3],\n    ""mp_pool_sizes_char"": [10, 5],\n\n    # bcnn\n    ""bcnn_num_layers"": 2,\n    ""bcnn_num_filters"": 16,\n    ""bcnn_filter_size"": 3,\n    ""bcnn_activation"": tf.nn.tanh, # tf.nn.relu with euclidean/euclidean_exp produce nan\n    ""bcnn_match_score_type"": ""cosine"",\n\n    ""bcnn_mp_att_pooling"": False,\n    ""bcnn_mp_num_filters"": [8, 16],\n    ""bcnn_mp_filter_sizes"": [5, 3],\n    ""bcnn_mp_activation"": tf.nn.relu,\n    ""bcnn_mp_dynamic_pooling"": False,\n    ""bcnn_mp_pool_sizes_word"": [6, 3],\n    ""bcnn_mp_pool_sizes_char"": [10, 5],\n\n    # final layer\n    ""final_dropout"": 0.3,\n\n}\n\n\ndef get_model_data(df, features, params):\n    X = {\n        ""q1"": df.q1.values,\n        ""q2"": df.q2.values,\n        ""label"": df.label.values,\n    }\n    if params[""use_features""]:\n        X.update({\n            ""features"": features,\n        })\n        params[""num_features""] = X[""features""].shape[1]\n    return X\n\n\ndef downsample(df):\n    # downsample negative\n    num_pos = np.sum(df.label)\n    num_neg = int((1. / config.POS_RATIO_OFFLINE - 1.) * num_pos)\n    idx_pos = np.where(df.label == 1)[0]\n    idx_neg = np.where(df.label == 0)[0]\n    np.random.shuffle(idx_neg)\n    idx = np.hstack([idx_pos, idx_neg[:num_neg]])\n    return df.loc[idx]\n\n\ndef get_train_valid_test_data(augmentation=False):\n    # load data\n    Q = load_question(params)\n    dfTrain = load_train()\n    dfTest = load_test()\n    # train_features = load_feat(""train"")\n    # test_features = load_feat(""test"")\n    # params[""num_features""] = train_features.shape[1]\n\n    # load split\n    with open(config.SPLIT_FILE, ""rb"") as f:\n        train_idx, valid_idx = pkl.load(f)\n\n    # validation\n    if augmentation:\n        dfDev = pd.read_csv(config.DATA_DIR + ""/"" + ""dev_aug.csv"")\n        dfDev = downsample(dfDev)\n        params[""use_features""] = False\n        params[""augmentation_decay_steps""] = 50000\n        params[""decay_steps""] = 50000\n        X_dev = get_model_data(dfDev, None, params)\n    else:\n        X_dev = get_model_data(dfTrain.loc[train_idx], None, params)\n    X_valid = get_model_data(dfTrain.loc[valid_idx], None, params)\n\n    # submit\n    if augmentation:\n        dfTrain = pd.read_csv(config.DATA_DIR + ""/"" + ""train_aug.csv"")\n        dfTrain = downsample(dfTrain)\n        params[""use_features""] = False\n        params[""augmentation_decay_steps""] = 50000\n        params[""decay_steps""] = 50000\n        X_train = get_model_data(dfTrain, None, params)\n    else:\n        X_train = get_model_data(dfTrain, None, params)\n    X_test = get_model_data(dfTest, None, params)\n\n    return X_dev, X_valid, X_train, X_test, Q\n\n\ndef parse_args(parser):\n    parser.add_option(""-m"", ""--model"", type=""string"", dest=""model"",\n                      help=""model type"", default=""cdssm"")\n    parser.add_option(""-a"", ""--augmentation"", action=""store_true"", dest=""augmentation"",\n                      help=""augmentation"", default=False)\n    parser.add_option(""-g"", ""--granularity"", type=""string"", dest=""granularity"",\n                      help=""granularity, e.g., word or char"", default=""word"")\n\n    (options, args) = parser.parse_args()\n    return options, args\n\n\ndef main(options):\n\n    os_utils._makedirs(""../logs"")\n    os_utils._makedirs(""../output"")\n    logger = log_utils._get_logger(""../logs"", ""tf-%s.log"" % time_utils._timestamp())\n\n    params[""granularity""] = options.granularity\n\n    # save path\n    model_name = ""augmentation_%s_%s_%s""%(str(options.augmentation), options.granularity, options.model)\n    path = config.SUB_DIR + ""/"" + model_name\n    os_utils._makedirs(path)\n\n    # load data\n    X_dev, X_valid, X_train, X_test, Q = get_train_valid_test_data(options.augmentation)\n\n    # validation\n    model = get_model(options.model)(params, logger, init_embedding_matrix=init_embedding_matrix)\n    model.fit(X_dev, Q, validation_data=X_valid, shuffle=True)\n    y_pred_valid = model.predict_proba(X_valid, Q).flatten()\n    # save for stacking\n    df = pd.DataFrame({""y_pred"": y_pred_valid, ""y_true"": X_valid[""label""]})\n    df.to_csv(path + ""/valid.csv"", index=False, header=True)\n\n    # submission\n    y_proba = np.zeros((len(X_test[""label""]), params[""n_runs""]), dtype=np.float32)\n    for run in range(params[""n_runs""]):\n        params[""random_seed""] = run\n        params[""model_name""] = ""semantic_model_%s""%str(run+1)\n        model = get_model(options.model)(params, logger, init_embedding_matrix=init_embedding_matrix)\n        model.fit(X_train, Q, validation_data=None, shuffle=True)\n        y_proba[:,run] = model.predict_proba(X_test, Q).flatten()\n        df = pd.DataFrame(y_proba[:,:(run+1)], columns=[""y_proba_%d""%(i+1) for i in range(run+1)])\n        df.to_csv(path + ""/test.csv"", index=False, header=True)\n\n\nif __name__ == ""__main__"":\n\n    parser = OptionParser()\n    options, args = parse_args(parser)\n    main(options)\n'"
src/inputs/__init__.py,0,b''
src/inputs/data.py,0,"b'\nimport config\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nfrom keras.preprocessing.sequence import pad_sequences\n\n\ndef _to_ind(qid):\n    return int(qid[1:])\n\n\ndef load_raw_question():\n    df = pd.read_csv(config.QUESTION_FILE)\n    df[""words""] = df.words.str.split("" "")\n    df[""chars""] = df.chars.str.split("" "")\n    Q = {}\n    Q[""words""] = df[""words""].values\n    Q[""chars""] = df[""chars""].values\n    return Q\n\n\ndef load_question(params):\n    df = pd.read_csv(config.QUESTION_FILE)\n    df[""words""] = df.words.str.split("" "").apply(lambda x: [_to_ind(z) for z in x])\n    df[""chars""] = df.chars.str.split("" "").apply(lambda x: [_to_ind(z) for z in x])\n    Q = {}\n    Q[""seq_len_word""] = sp.minimum(df[""words""].apply(len).values, params[""max_seq_len_word""])\n    Q[""seq_len_char""] = sp.minimum(df[""chars""].apply(len).values, params[""max_seq_len_char""])\n    Q[""words""] = pad_sequences(df[""words""],\n                               maxlen=params[""max_seq_len_word""],\n                               padding=params[""pad_sequences_padding""],\n                               truncating=params[""pad_sequences_truncating""],\n                               value=config.PADDING_INDEX_WORD)\n    Q[""chars""] = pad_sequences(df[""chars""],\n                               maxlen=params[""max_seq_len_char""],\n                               padding=params[""pad_sequences_padding""],\n                               truncating=params[""pad_sequences_truncating""],\n                               value=config.PADDING_INDEX_CHAR)\n    return Q\n\n\ndef load_train():\n    df = pd.read_csv(config.TRAIN_FILE)\n    df[""q1""] = df.q1.apply(_to_ind)\n    df[""q2""] = df.q2.apply(_to_ind)\n    return df\n\n\ndef load_test():\n    df = pd.read_csv(config.TEST_FILE)\n    df[""q1""] = df.q1.apply(_to_ind)\n    df[""q2""] = df.q2.apply(_to_ind)\n    df[""label""] = np.zeros(df.shape[0])\n    return df\n\n\ndef load_embedding_matrix(embedding_file):\n    print(""read embedding from: %s "" %embedding_file)\n    d = {}\n    n = 0\n    with open(embedding_file, ""r"") as f:\n        line = f.readline()\n        while line:\n            n += 1\n            w, v = line.strip().split("" "", 1)\n            d[int(w[1:])] = v\n            line = f.readline()\n    dim = len(v.split("" ""))\n\n    # add two index for missing and padding\n    emb_matrix = np.zeros((n+2, dim), dtype=float)\n    for key ,val in d.items():\n        v = np.asarray(val.split("" ""), dtype=float)\n        emb_matrix[key] = v\n    emb_matrix = np.array(emb_matrix, dtype=np.float32)\n    return emb_matrix\n\n\ninit_embedding_matrix = {\n    ""word"": load_embedding_matrix(config.WORD_EMBEDDING_FILE),\n    ""char"": load_embedding_matrix(config.CHAR_EMBEDDING_FILE),\n}'"
src/inputs/dynamic_pooling.py,0,"b'\nimport numpy as np\n\n\n# see https://github.com/pl8787/MatchPyramid-TensorFlow\ndef dpool_index_(batch_idx, len1_one, len2_one, max_len1, max_len2):\n    stride1 = 1.0 * max_len1 / len1_one\n    stride2 = 1.0 * max_len2 / len2_one\n    idx1_one = np.arange(max_len1) / stride1\n    idx2_one = np.arange(max_len2) / stride2\n    mesh1, mesh2 = np.meshgrid(idx1_one, idx2_one)\n    index_one = np.transpose(np.stack([np.ones(mesh1.shape) * batch_idx, mesh1, mesh2]), (2, 1, 0))\n    return index_one\n\n\ndef dynamic_pooling_index(len1, len2, max_len1, max_len2):\n    index = np.zeros((len(len1), max_len1, max_len2, 3), dtype=int)\n    for i in range(len(len1)):\n        index[i] = dpool_index_(i, len1[i], len2[i], max_len1, max_len2)\n    return index\n'"
src/models/__init__.py,0,b''
src/models/base_model.py,63,"b'\nimport time\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\n\nimport config\nfrom utils import os_utils\nfrom tf_common.optimizer import *\nfrom tf_common.nn_module import word_dropout, mlp_layer\nfrom tf_common.nn_module import encode, attend\n\n\ndef sigmoid(x):\n    return 1./(1.+np.exp(-x))\n\n\nclass BaseModel(object):\n    def __init__(self, params, logger, init_embedding_matrix=None):\n        self.params = params\n        self.logger = logger\n        self.init_embedding_matrix = init_embedding_matrix\n        self.model_name = self.params[""model_name""]\n        self.threshold = self.params[""threshold""]\n        self.calibration_model = None\n        # os_utils._makedirs(self.params[""offline_model_dir""], force=True)\n\n        self._init_tf_vars()\n        self.matching_features_word, self.matching_features_char = self._get_matching_features()\n        self.logits, self.proba = self._get_prediction()\n        self.loss = self._get_loss()\n        self.train_op = self._get_train_op()\n        self.summary = self._get_summary()\n\n        self.sess, self.saver = self._init_session()\n        self.train_writer = tf.summary.FileWriter(self.params[""summary_dir""] + \'/train\', self.sess.graph)\n        self.test_writer = tf.summary.FileWriter(self.params[""summary_dir""] + \'/test\')\n        \n        \n    def _init_tf_vars(self):\n        #### training flag\n        self.training = tf.placeholder(tf.bool, shape=[], name=""training"")\n        #### labels\n        self.labels = tf.placeholder(tf.float32, shape=[None], name=""labels"")\n        #### word\n        self.seq_word_left = tf.placeholder(tf.int32, shape=[None, None], name=""seq_word_left"")\n        self.seq_word_right = tf.placeholder(tf.int32, shape=[None, None], name=""seq_word_right"")\n        #### char\n        self.seq_char_left = tf.placeholder(tf.int32, shape=[None, None], name=""seq_char_left"")\n        self.seq_char_right = tf.placeholder(tf.int32, shape=[None, None], name=""seq_char_right"")\n        #### word len\n        self.seq_len_word_left = tf.placeholder(tf.int32, shape=[None], name=""seq_len_word_left"")\n        self.seq_len_word_right = tf.placeholder(tf.int32, shape=[None], name=""seq_len_word_right"")\n        #### char len\n        self.seq_len_char_left = tf.placeholder(tf.int32, shape=[None], name=""seq_len_char_left"")\n        self.seq_len_char_right = tf.placeholder(tf.int32, shape=[None], name=""seq_len_char_right"")\n\n        #### features\n        self.features = tf.placeholder(tf.float32, shape=[None, self.params[""num_features""]], name=""features"")\n\n        #### training\n        self.global_step = tf.Variable(0, trainable=False)\n        self.learning_rate = tf.train.exponential_decay(self.params[""init_lr""], self.global_step,\n                                                        self.params[""decay_steps""], self.params[""decay_rate""])\n        self.augmentation_dropout = tf.train.exponential_decay(self.params[""augmentation_init_dropout""], self.global_step,\n                                                               self.params[""augmentation_dropout_decay_steps""],\n                                                               self.params[""augmentation_dropout_decay_rate""])\n        self.augmentation_permutation = tf.train.exponential_decay(self.params[""augmentation_init_permutation""],\n                                                               self.global_step,\n                                                               self.params[""augmentation_permutation_decay_steps""],\n                                                               self.params[""augmentation_permutation_decay_rate""])\n\n\n    def _get_embedding_matrix(self, granularity=""word""):\n        if self.init_embedding_matrix[granularity] is None:\n            std = 0.1\n            minval = -std\n            maxval = std\n            emb_matrix = tf.Variable(\n                tf.random_uniform(\n                    [self.params[""max_num_%s"" % granularity] + 1, self.params[""embedding_dim_%s"" % granularity]],\n                    minval, maxval,\n                    seed=self.params[""random_seed""],\n                    dtype=tf.float32))\n        else:\n            emb_matrix = tf.Variable(self.init_embedding_matrix[granularity],\n                                     trainable=self.params[""embedding_trainable""])\n        return emb_matrix\n\n\n    def _semantic_feature_layer(self, seq_input, seq_len, granularity=""word"", reuse=False):\n        assert granularity in [""char"", ""word""]\n        #### embed\n        emb_matrix = self._get_embedding_matrix(granularity)\n        emb_seq = tf.nn.embedding_lookup(emb_matrix, seq_input)\n\n        #### dropout\n        random_seed = np.random.randint(10000000)\n        emb_seq = word_dropout(emb_seq,\n                               training=self.training,\n                               dropout=self.params[""embedding_dropout""],\n                               seed=random_seed)\n\n        #### encode\n        input_dim = self.params[""embedding_dim""]\n        enc_seq = encode(emb_seq, method=self.params[""encode_method""],\n                         input_dim=input_dim,\n                         params=self.params,\n                         sequence_length=seq_len,\n                         mask_zero=self.params[""embedding_mask_zero""],\n                         scope_name=self.model_name + ""enc_seq_%s""%granularity, reuse=reuse,\n                         training=self.training)\n\n        #### attend\n        feature_dim = self.params[""encode_dim""]\n        context = None\n        att_seq = attend(enc_seq, context=context,\n                         encode_dim=self.params[""encode_dim""],\n                         feature_dim=feature_dim,\n                         attention_dim=self.params[""attention_dim""],\n                         method=self.params[""attend_method""],\n                         scope_name=self.model_name + ""att_seq_%s""%granularity,\n                         reuse=reuse, num_heads=self.params[""attention_num_heads""])\n\n        #### MLP nonlinear projection\n        sem_seq = mlp_layer(att_seq, fc_type=self.params[""fc_type""],\n                            hidden_units=self.params[""fc_hidden_units""],\n                            dropouts=self.params[""fc_dropouts""],\n                            scope_name=self.model_name + ""sem_seq_%s""%granularity,\n                            reuse=reuse,\n                            training=self.training,\n                            seed=self.params[""random_seed""])\n\n        return emb_seq, enc_seq, att_seq, sem_seq\n\n\n    def _interaction_semantic_feature_layer(self, seq_input_left, seq_input_right, seq_len_left, seq_len_right, granularity=""word""):\n        assert granularity in [""char"", ""word""]\n        #### embed\n        emb_matrix = self._get_embedding_matrix(granularity)\n        emb_seq_left = tf.nn.embedding_lookup(emb_matrix, seq_input_left)\n        emb_seq_right = tf.nn.embedding_lookup(emb_matrix, seq_input_right)\n\n        #### dropout\n        random_seed = np.random.randint(10000000)\n        emb_seq_left = word_dropout(emb_seq_left,\n                               training=self.training,\n                               dropout=self.params[""embedding_dropout""],\n                               seed=random_seed)\n        random_seed = np.random.randint(10000000)\n        emb_seq_right = word_dropout(emb_seq_right,\n                                    training=self.training,\n                                    dropout=self.params[""embedding_dropout""],\n                                    seed=random_seed)\n\n        #### encode\n        input_dim = self.params[""embedding_dim""]\n        enc_seq_left = encode(emb_seq_left, method=self.params[""encode_method""],\n                              input_dim=input_dim,\n                              params=self.params,\n                              sequence_length=seq_len_left,\n                              mask_zero=self.params[""embedding_mask_zero""],\n                              scope_name=self.model_name + ""enc_seq_%s""%granularity, reuse=False,\n                              training=self.training)\n        enc_seq_right = encode(emb_seq_right, method=self.params[""encode_method""],\n                               input_dim=input_dim,\n                               params=self.params,\n                               sequence_length=seq_len_right,\n                               mask_zero=self.params[""embedding_mask_zero""],\n                               scope_name=self.model_name + ""enc_seq_%s"" % granularity, reuse=True,\n                               training=self.training)\n\n        #### attend\n        # [batchsize, s1, s2]\n        att_mat = tf.einsum(""abd,acd->abc"", enc_seq_left, enc_seq_right)\n        feature_dim = self.params[""encode_dim""] + self.params[""max_seq_len_%s""%granularity]\n        att_seq_left = attend(enc_seq_left, context=att_mat, feature_dim=feature_dim,\n                                   method=self.params[""attend_method""],\n                                   scope_name=self.model_name + ""att_seq_%s""%granularity,\n                                   reuse=False)\n        att_seq_right = attend(enc_seq_right, context=tf.transpose(att_mat), feature_dim=feature_dim,\n                              method=self.params[""attend_method""],\n                              scope_name=self.model_name + ""att_seq_%s"" % granularity,\n                              reuse=True)\n\n        #### MLP nonlinear projection\n        sem_seq_left = mlp_layer(att_seq_left, fc_type=self.params[""fc_type""],\n                                 hidden_units=self.params[""fc_hidden_units""],\n                                 dropouts=self.params[""fc_dropouts""],\n                                 scope_name=self.model_name + ""sem_seq_%s""%granularity,\n                                 reuse=False,\n                                 training=self.training,\n                                 seed=self.params[""random_seed""])\n        sem_seq_right = mlp_layer(att_seq_right, fc_type=self.params[""fc_type""],\n                                  hidden_units=self.params[""fc_hidden_units""],\n                                  dropouts=self.params[""fc_dropouts""],\n                                  scope_name=self.model_name + ""sem_seq_%s"" % granularity,\n                                  reuse=True,\n                                  training=self.training,\n                                  seed=self.params[""random_seed""])\n\n        return emb_seq_left, enc_seq_left, att_seq_left, sem_seq_left, \\\n                emb_seq_right, enc_seq_right, att_seq_right, sem_seq_right\n\n\n    def _get_matching_features(self):\n        pass\n\n\n    def _get_prediction(self):\n        with tf.name_scope(self.model_name + ""/""):\n            with tf.name_scope(""prediction""):\n                lst = []\n                if ""word"" in self.params[""granularity""]:\n                    lst.append(self.matching_features_word)\n                if ""char"" in self.params[""granularity""]:\n                    lst.append(self.matching_features_char)\n                if self.params[""use_features""]:\n                    out_0 = mlp_layer(self.features, fc_type=self.params[""fc_type""],\n                                      hidden_units=self.params[""fc_hidden_units""],\n                                      dropouts=self.params[""fc_dropouts""],\n                                      scope_name=self.model_name + ""mlp_features"",\n                                      reuse=False,\n                                      training=self.training,\n                                      seed=self.params[""random_seed""])\n                    lst.append(out_0)\n                out = tf.concat(lst, axis=-1)\n                out = tf.layers.Dropout(self.params[""final_dropout""])(out, training=self.training)\n                out = mlp_layer(out, fc_type=self.params[""fc_type""],\n                                hidden_units=self.params[""fc_hidden_units""],\n                                dropouts=self.params[""fc_dropouts""],\n                                scope_name=self.model_name + ""mlp"",\n                                reuse=False,\n                                training=self.training,\n                                seed=self.params[""random_seed""])\n                logits = tf.layers.dense(out, 1, activation=None,\n                                         kernel_initializer=tf.glorot_uniform_initializer(\n                                         seed=self.params[""random_seed""]),\n                                         name=self.model_name + ""logits"")\n                logits = tf.squeeze(logits, axis=1)\n                proba = tf.nn.sigmoid(logits)\n\n        return logits, proba\n\n\n    def _get_loss(self):\n        with tf.name_scope(self.model_name + ""/""):\n            with tf.name_scope(""loss""):\n                loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.labels, logits=self.logits)\n                loss = tf.reduce_mean(loss, name=""log_loss"")\n                if self.params[""l2_lambda""] > 0:\n                    l2_losses = tf.add_n(\n                        [tf.nn.l2_loss(v) for v in tf.trainable_variables() if ""bias"" not in v.name]) * self.params[\n                                    ""l2_lambda""]\n                    loss = loss + l2_losses\n        return loss\n\n\n    def _get_train_op(self):\n        with tf.name_scope(self.model_name + ""/""):\n            with tf.name_scope(""optimization""):\n                if self.params[""optimizer_type""] == ""lazynadam"":\n                    optimizer = LazyNadamOptimizer(learning_rate=self.learning_rate, beta1=self.params[""beta1""],\n                                                   beta2=self.params[""beta2""], epsilon=1e-8,\n                                                   schedule_decay=self.params[""schedule_decay""])\n                elif self.params[""optimizer_type""] == ""adam"":\n                    optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate,\n                                                       beta1=self.params[""beta1""],\n                                                       beta2=self.params[""beta2""], epsilon=1e-8)\n                elif self.params[""optimizer_type""] == ""lazyadam"":\n                    optimizer = tf.contrib.opt.LazyAdamOptimizer(learning_rate=self.learning_rate,\n                                                                 beta1=self.params[""beta1""],\n                                                                 beta2=self.params[""beta2""], epsilon=1e-8)\n                elif self.params[""optimizer_type""] == ""adagrad"":\n                    optimizer = tf.train.AdagradOptimizer(learning_rate=self.learning_rate,\n                                                          initial_accumulator_value=1e-7)\n                elif self.params[""optimizer_type""] == ""adadelta"":\n                    optimizer = tf.train.AdadeltaOptimizer(learning_rate=self.learning_rate)\n                elif self.params[""optimizer_type""] == ""gd"":\n                    optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)\n                elif self.params[""optimizer_type""] == ""momentum"":\n                    optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=0.95)\n                elif self.params[""optimizer_type""] == ""rmsprop"":\n                    optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate, decay=0.9,\n                                                          momentum=0.9, epsilon=1e-8)\n                elif self.params[""optimizer_type""] == ""lazypowersign"":\n                    optimizer = LazyPowerSignOptimizer(learning_rate=self.learning_rate)\n                elif self.params[""optimizer_type""] == ""lazyaddsign"":\n                    optimizer = LazyAddSignOptimizer(learning_rate=self.learning_rate)\n                elif self.params[""optimizer_type""] == ""lazyamsgrad"":\n                    optimizer = LazyAMSGradOptimizer(learning_rate=self.learning_rate, beta1=self.params[""beta1""],\n                                                     beta2=self.params[""beta2""], epsilon=1e-8)\n\n                update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n                with tf.control_dependencies(update_ops):\n                    train_op = optimizer.minimize(self.loss, global_step=self.global_step)\n        return train_op\n\n\n    def _get_summary(self):\n        with tf.name_scope(self.model_name + ""/""):\n            tf.summary.scalar(""augmentation_dropout"", self.augmentation_dropout)\n            tf.summary.scalar(""logloss"", self.loss)\n            tf.summary.scalar(""lr"", self.learning_rate)\n            # error: https://blog.csdn.net/u012436149/article/details/53894364\n            # summary = tf.summary.merge_all()\n            summary = tf.summary.merge(\n                tf.get_collection(tf.GraphKeys.SUMMARIES, self.model_name)\n            )\n        return summary\n\n\n    def _init_session(self):\n        config = tf.ConfigProto(device_count={""gpu"": 1})\n        config.gpu_options.allow_growth = True\n        config.intra_op_parallelism_threads = 4\n        config.inter_op_parallelism_threads = 4\n        sess = tf.Session(config=config)\n        sess.run(tf.global_variables_initializer())\n        # max_to_keep=None, keep all the models\n        saver = tf.train.Saver(max_to_keep=None)\n        return sess, saver\n\n\n    def save_session(self):\n        self.saver.save(self.sess, self.params[""offline_model_dir""] + ""/model.checkpoint"")\n\n\n    def restore_session(self):\n        self.saver.restore(self.sess, self.params[""offline_model_dir""] + ""/model.checkpoint"")\n\n\n    def _get_batch_index(self, seq, step):\n        n = len(seq)\n        res = []\n        for i in range(0, n, step):\n            res.append(seq[i:i + step])\n        # last batch\n        if len(res) * step < n:\n            res.append(seq[len(res) * step:])\n        return res\n\n\n    def _get_pos_neg_ind(self, label):\n        length = len(label)\n        pos_ind_tmp = np.where(label == 1)[0]\n        inds = np.zeros((len(pos_ind_tmp) * length, 2), dtype=int)\n        inds[:, 0] = np.tile(pos_ind_tmp, length)\n        inds[:, 1] = list(range(length)) * len(pos_ind_tmp)\n        mask = inds[:, 0] != inds[:, 1]\n        pos_ind = inds[mask, 0]\n        neg_ind = inds[mask, 1]\n        return pos_ind, neg_ind\n\n\n    def _get_feed_dict(self, X, idx, Q, construct_neg=False, training=False, symmetric=False):\n        if training:\n            if construct_neg:\n                q1 = X[""q1""][idx]\n                q2 = X[""q2""][idx]\n                # for label=1 sample, construct negative sample within batch\n                pos_ind, neg_ind = self._get_pos_neg_ind(X[""label""][idx])\n                # original & symmetric\n                feed_dict = {\n                    self.seq_word_left: np.vstack([Q[""words""][q1],\n                                                   Q[""words""][X[""q1""][idx[pos_ind]]],\n                                                   Q[""words""][X[""q1""][idx[neg_ind]]],\n                                                   Q[""words""][q2],\n                                                   Q[""words""][X[""q2""][idx[neg_ind]]],\n                                                   Q[""words""][X[""q2""][idx[pos_ind]]]\n                                                   ]),\n                    self.seq_word_right: np.vstack([Q[""words""][q2],\n                                                    Q[""words""][X[""q2""][idx[neg_ind]]],\n                                                    Q[""words""][X[""q2""][idx[pos_ind]]],\n                                                    Q[""words""][q1],\n                                                    Q[""words""][X[""q1""][idx[pos_ind]]],\n                                                    Q[""words""][X[""q1""][idx[neg_ind]]],\n                                                    ]),\n                    self.seq_char_left: np.vstack([Q[""chars""][q1],\n                                                   Q[""chars""][X[""q1""][idx[pos_ind]]],\n                                                   Q[""chars""][X[""q1""][idx[neg_ind]]],\n                                                   Q[""chars""][q2],\n                                                   Q[""chars""][X[""q2""][idx[neg_ind]]],\n                                                   Q[""chars""][X[""q2""][idx[pos_ind]]]\n                                                   ]),\n                    self.seq_char_right: np.vstack([Q[""chars""][q2],\n                                                    Q[""chars""][X[""q2""][idx[neg_ind]]],\n                                                    Q[""chars""][X[""q2""][idx[pos_ind]]],\n                                                    Q[""chars""][q1],\n                                                    Q[""chars""][X[""q1""][idx[pos_ind]]],\n                                                    Q[""chars""][X[""q1""][idx[neg_ind]]]\n                                                    ]),\n                    self.labels: np.hstack([X[""label""][idx],\n                                            np.zeros(len(pos_ind)),\n                                            np.zeros(len(pos_ind)),\n                                            X[""label""][idx],\n                                            np.zeros(len(pos_ind)),\n                                            np.zeros(len(pos_ind))\n                                            ]),\n                    self.training: training,\n                }\n            else:\n                q1 = X[""q1""][idx]\n                q2 = X[""q2""][idx]\n                feed_dict = {\n                    self.seq_word_left: np.vstack([Q[""words""][q1],\n                                                   Q[""words""][q2],\n                                                   ]),\n                    self.seq_word_right: np.vstack([Q[""words""][q2],\n                                                    Q[""words""][q1],\n                                                    ]),\n                    self.seq_char_left: np.vstack([Q[""chars""][q1],\n                                                   Q[""chars""][q2],\n                                                   ]),\n                    self.seq_char_right: np.vstack([Q[""chars""][q2],\n                                                    Q[""chars""][q1],\n                                                    ]),\n                    self.seq_len_word_left: np.hstack([Q[""seq_len_word""][q1],\n                                                       Q[""seq_len_word""][q2],\n                                                       ]),\n                    self.seq_len_word_right: np.hstack([Q[""seq_len_word""][q2],\n                                                        Q[""seq_len_word""][q1],\n                                                        ]),\n                    self.seq_len_char_left: np.hstack([Q[""seq_len_char""][q1],\n                                                       Q[""seq_len_char""][q2],\n                                                       ]),\n                    self.seq_len_char_right: np.hstack([Q[""seq_len_char""][q2],\n                                                        Q[""seq_len_char""][q1],\n                                                        ]),\n                    self.labels: np.hstack([X[""label""][idx],\n                                            X[""label""][idx],\n                                            ]),\n                    self.training: training,\n                }\n                if self.params[""use_features""]:\n                    feed_dict.update({\n                        self.features: np.vstack([X[""features""][idx],\n                                                  X[""features""][idx],\n                                                  ]),\n                    })\n        elif not symmetric:\n            q1 = X[""q1""][idx]\n            q2 = X[""q2""][idx]\n            feed_dict = {\n                self.seq_word_left: Q[""words""][q1],\n                self.seq_word_right: Q[""words""][q2],\n                self.seq_char_left: Q[""chars""][q1],\n                self.seq_char_right: Q[""chars""][q2],\n                self.seq_len_word_left: Q[""seq_len_word""][q1],\n                self.seq_len_word_right: Q[""seq_len_word""][q2],\n                self.seq_len_char_left: Q[""seq_len_char""][q1],\n                self.seq_len_char_right: Q[""seq_len_char""][q2],\n                self.labels: X[""label""][idx],\n                self.training: training,\n            }\n            if self.params[""use_features""]:\n                feed_dict.update({\n                    self.features: X[""features""][idx],\n                })\n        else:\n            q1 = X[""q1""][idx]\n            q2 = X[""q2""][idx]\n            feed_dict = {\n                self.seq_word_left: np.vstack([Q[""words""][q1],\n                                               Q[""words""][q2],\n                                               ]),\n                self.seq_word_right: np.vstack([Q[""words""][q2],\n                                                Q[""words""][q1],\n                                                ]),\n                self.seq_char_left: np.vstack([Q[""chars""][q1],\n                                               Q[""chars""][q2],\n                                               ]),\n                self.seq_char_right: np.vstack([Q[""chars""][q2],\n                                                Q[""chars""][q1],\n                                                ]),\n                self.seq_len_word_left: np.hstack([Q[""seq_len_word""][q1],\n                                                   Q[""seq_len_word""][q2],\n                                                   ]),\n                self.seq_len_word_right: np.hstack([Q[""seq_len_word""][q2],\n                                                    Q[""seq_len_word""][q1],\n                                                    ]),\n                self.seq_len_char_left: np.hstack([Q[""seq_len_char""][q1],\n                                                   Q[""seq_len_char""][q2],\n                                                   ]),\n                self.seq_len_char_right: np.hstack([Q[""seq_len_char""][q2],\n                                                    Q[""seq_len_char""][q1],\n                                                    ]),\n                self.labels: np.hstack([X[""label""][idx],\n                                        X[""label""][idx],\n                                        ]),\n                self.training: training,\n            }\n            if self.params[""use_features""]:\n                feed_dict.update({\n                    self.features: np.vstack([X[""features""][idx],\n                                              X[""features""][idx],\n                                              ]),\n                })\n        # augmentation\n        if training:\n            if self.params[""augmentation_init_dropout""] > 0:\n                self._dropout_augmentation(feed_dict)\n            if self.params[""augmentation_init_permutation""]:\n                self._permutation_augmentation(feed_dict)\n\n        return feed_dict\n\n\n    def _dropout(self, val_arr, ind_arr, p, value):\n        new_arr = np.array(val_arr)\n        drop = np.empty(val_arr.shape, dtype=np.bool)\n        for i in range(val_arr.shape[0]):\n            drop[i, :ind_arr[i]] = np.random.choice([True, False], ind_arr[i], p=[p, 1 - p])\n        new_arr[drop] = value\n        return new_arr\n\n\n    def _dropout_augmentation(self, feed_dict):\n        p = self.sess.run(self.augmentation_dropout)\n        if p <= self.params[""augmentation_min_dropout""]:\n            return\n\n        dropout_data = self._dropout(val_arr=feed_dict[self.seq_word_left],\n                                     ind_arr=feed_dict[self.seq_len_word_left],\n                                     p=p, value=config.MISSING_INDEX_WORD)\n        feed_dict[self.seq_word_left] = np.vstack([\n            feed_dict[self.seq_word_left],\n            dropout_data,\n        ])\n\n        dropout_data = self._dropout(val_arr=feed_dict[self.seq_word_right],\n                                     ind_arr=feed_dict[self.seq_len_word_right],\n                                     p=p, value=config.MISSING_INDEX_WORD)\n        feed_dict[self.seq_word_right] = np.vstack([\n            feed_dict[self.seq_word_right],\n            dropout_data,\n        ])\n\n        dropout_data = self._dropout(val_arr=feed_dict[self.seq_char_left],\n                                     ind_arr=feed_dict[self.seq_len_char_left],\n                                     p=p, value=config.MISSING_INDEX_CHAR)\n        feed_dict[self.seq_char_left] = np.vstack([\n            feed_dict[self.seq_char_left],\n            dropout_data,\n        ])\n\n        dropout_data = self._dropout(val_arr=feed_dict[self.seq_char_right],\n                                     ind_arr=feed_dict[self.seq_len_char_right],\n                                     p=p, value=config.MISSING_INDEX_CHAR)\n        feed_dict[self.seq_char_right] = np.vstack([\n            feed_dict[self.seq_char_right],\n            dropout_data,\n        ])\n\n        # double others\n        feed_dict[self.seq_len_word_left] = np.tile(feed_dict[self.seq_len_word_left], 2)\n        feed_dict[self.seq_len_word_right] = np.tile(feed_dict[self.seq_len_word_right], 2)\n        feed_dict[self.seq_len_char_left] = np.tile(feed_dict[self.seq_len_char_left], 2)\n        feed_dict[self.seq_len_char_right] = np.tile(feed_dict[self.seq_len_char_right], 2)\n        feed_dict[self.labels] = np.tile(feed_dict[self.labels], 2)\n        if self.params[""use_features""]:\n            feed_dict[self.features] = np.tile(feed_dict[self.features], [2, 1])\n\n\n    def _permutation(self, val_arr, ind_arr, p):\n        if np.random.random() < p:\n            new_arr = np.array(val_arr)\n            for i in range(val_arr.shape[0]):\n                new_arr[i, :ind_arr[i]] = np.random.permutation(new_arr[i,:ind_arr[i]])\n            return new_arr\n        else:\n            return val_arr\n\n\n    def _permutation_augmentation(self, feed_dict):\n        p = self.sess.run(self.augmentation_permutation)\n        if p <= self.params[""augmentation_min_permutation""]:\n            return\n\n        feed_dict[self.seq_word_left] = np.vstack([\n            feed_dict[self.seq_word_left],\n            self._permutation(feed_dict[self.seq_word_left], feed_dict[self.seq_len_word_left], p),\n        ])\n        feed_dict[self.seq_word_right] = np.vstack([\n            feed_dict[self.seq_word_right],\n            self._permutation(feed_dict[self.seq_word_right], feed_dict[self.seq_len_word_right], p),\n        ])\n        feed_dict[self.seq_char_left] = np.vstack([\n            feed_dict[self.seq_char_left],\n            self._permutation(feed_dict[self.seq_char_left], feed_dict[self.seq_len_char_left], p),\n        ])\n        feed_dict[self.seq_char_right] = np.vstack([\n            feed_dict[self.seq_char_right],\n            self._permutation(feed_dict[self.seq_char_right], feed_dict[self.seq_len_char_right], p),\n        ])\n        # double others\n        feed_dict[self.seq_len_word_left] = np.tile(feed_dict[self.seq_len_word_left], 2)\n        feed_dict[self.seq_len_word_right] = np.tile(feed_dict[self.seq_len_word_right], 2)\n        feed_dict[self.seq_len_char_left] = np.tile(feed_dict[self.seq_len_char_left], 2)\n        feed_dict[self.seq_len_char_right] = np.tile(feed_dict[self.seq_len_char_right], 2)\n        feed_dict[self.labels] = np.tile(feed_dict[self.labels], 2)\n        if self.params[""use_features""]:\n            feed_dict[self.features] = np.tile(feed_dict[self.features], [2, 1])\n\n\n    def fit(self, X, Q, validation_data=None, shuffle=False, total_epoch=None):\n        start_time = time.time()\n        l = X[""label""].shape[0]\n        self.logger.info(""fit on %d sample"" % l)\n        self.logger.info(""max_batch: %d"" % self.params[""max_batch""])\n        if validation_data is not None:\n            self.logger.info(""mean: %.5f""%np.mean(validation_data[""label""]))\n        train_idx_shuffle = np.arange(l)\n        total_loss = 0.\n        loss_decay = 0.9\n        global_step = self.sess.run(self.global_step)\n        if total_epoch is None:\n            total_epoch = self.params[""epoch""]\n        for epoch in range(total_epoch):\n            self.logger.info(""epoch: %d"" % (epoch + 1))\n            np.random.seed(epoch)\n            if shuffle:\n                np.random.shuffle(train_idx_shuffle)\n            dropout_p = self.sess.run(self.augmentation_dropout)\n            batch_size = self.params[""batch_size""]\n            if dropout_p <= self.params[""augmentation_min_dropout""]:\n                batch_size *= 2\n            batches = self._get_batch_index(train_idx_shuffle, batch_size)\n            for i, idx in enumerate(batches):\n                feed_dict = self._get_feed_dict(X, idx, Q, construct_neg=self.params[""construct_neg""], training=True)\n                loss, lr, opt, summary, global_step = self.sess.run((self.loss, self.learning_rate, self.train_op, self.summary, self.global_step), feed_dict=feed_dict)\n                self.train_writer.add_summary(summary, global_step)\n                total_loss = loss_decay * total_loss + (1. - loss_decay) * loss\n                if validation_data is not None and (self.params[""eval_every_num_update""] > 0) and (global_step % self.params[""eval_every_num_update""] == 0):\n                    y_valid = validation_data[""label""]\n                    y_proba, y_proba_cal = self._predict_proba(validation_data, Q, fit_calibration=self.params[""calibration""])\n                    valid_loss = log_loss(y_valid, y_proba, eps=1e-15)\n                    valid_loss_cal = log_loss(y_valid, y_proba_cal, eps=1e-15)\n                    summary = tf.Summary()\n                    summary.value.add(tag=""logloss"", simple_value=valid_loss)\n                    self.test_writer.add_summary(summary, global_step)\n                    self.logger.info(\n                        ""[epoch-%d, batch-%d] train-loss=%.5f, valid-loss=%.5f, valid-loss-cal=%.5f, valid-proba=%.5f, predict-proba=%.5f, predict-proba-cal=%.5f, lr=%.5f [%.1f s]"" % (\n                            epoch + 1, global_step, total_loss, valid_loss, valid_loss_cal,\n                            np.mean(y_valid), np.mean(y_proba), np.mean(y_proba_cal), lr, time.time() - start_time))\n                else:\n                    self.logger.info(""[epoch-%d, batch-%d] train-loss=%.5f, lr=%.5f [%.1f s]"" % (\n                        epoch + 1, global_step, total_loss,\n                        lr, time.time() - start_time))\n                if global_step >= self.params[""max_batch""] and self.params[""max_batch""] > 0:\n                    break\n            if global_step >= self.params[""max_batch""] and self.params[""max_batch""] > 0:\n                break\n\n\n    def _predict_node(self, X, Q, node):\n        l = X[""label""].shape[0]\n        train_idx = np.arange(l)\n        batches = self._get_batch_index(train_idx, self.params[""batch_size""])\n        y_pred = []\n        y_pred_append = y_pred.append\n        for idx in batches:\n            feed_dict = self._get_feed_dict(X, idx, Q, training=False, symmetric=True)\n            pred = self.sess.run(node, feed_dict=feed_dict)\n            n = int(pred.shape[0]/2)\n            pred = (pred[:n] + pred[n:])/2.\n            y_pred_append(pred)\n        y_pred = np.hstack(y_pred).reshape((-1, 1)).astype(np.float64)\n        return y_pred\n\n\n    def _predict_proba(self, X, Q, fit_calibration=False):\n        y_logit = self._predict_node(X, Q, self.logits)\n        y_proba = sigmoid(y_logit)\n        y_proba_cal = y_proba\n        if fit_calibration:\n            y_valid = X[""label""]\n            self.calibration_model = LogisticRegression()\n            self.calibration_model.fit(y_logit, y_valid)\n        if self.calibration_model is not None:\n            y_proba_cal = self.calibration_model.predict_proba(y_logit)[:,1]\n        return y_proba, y_proba_cal\n\n\n    def predict_proba(self, X, Q):\n        _, y_proba_cal = self._predict_proba(X, Q, fit_calibration=False)\n        return y_proba_cal\n\n\n    def predict(self, X, Q):\n        proba = self.predict_proba(X, Q)\n        y = np.array(proba > self.threshold, dtype=int)\n        return y\n'"
src/models/bcnn.py,37,"b'\nfrom copy import copy\nimport tensorflow as tf\nimport numpy as np\n\nfrom inputs.dynamic_pooling import dynamic_pooling_index\nfrom models.base_model import BaseModel\nfrom tf_common import metrics\n\n\nclass BCNNBaseModel(BaseModel):\n    def __init__(self, params, logger, init_embedding_matrix):\n        super(BCNNBaseModel, self).__init__(params, logger, init_embedding_matrix)\n\n\n    def _init_tf_vars(self):\n        super(BCNNBaseModel, self)._init_tf_vars()\n        self.dpool_index_word = tf.placeholder(tf.int32, shape=[None, self.params[""max_seq_len_word""],\n                                                                self.params[""max_seq_len_word""], 3],\n                                               name=""dpool_index_word"")\n        self.dpool_index_char = tf.placeholder(tf.int32, shape=[None, self.params[""max_seq_len_char""],\n                                                                self.params[""max_seq_len_char""], 3],\n                                               name=""dpool_index_char"")\n\n\n    def _padding(self, x, name):\n        # x: [batch, s, d, 1]\n        # x => [batch, s+w*2-2, d, 1]\n        w = self.params[""bcnn_filter_size""]\n        return tf.pad(x, np.array([[0, 0], [w - 1, w - 1], [0, 0], [0, 0]]), ""CONSTANT"", name)\n\n\n    def _make_attention_matrix(self, x1, x2):\n        # x1: [batch, s1, d, 1]\n        # x2: [batch, s2, d, 1]\n        # match score\n        if ""euclidean"" in self.params[""bcnn_match_score_type""]:\n            # x1 => [batch, s1, 1, d]\n            # x2 => [batch, 1, s2, d]\n            x1_ = tf.transpose(x1, perm=[0, 1, 3, 2])\n            x2_ = tf.transpose(x2, perm=[0, 3, 1, 2])\n            euclidean = tf.sqrt(tf.reduce_sum(tf.square(x1_ - x2_), axis=-1))\n            if ""exp"" in self.params[""bcnn_match_score_type""]:\n                # exp(-euclidean / (2. * beta)) (producenan)\n                # from Convolutional Neural Network for Paraphrase Identification\n                beta = 2.\n                att = tf.exp(-euclidean / (2. * beta))\n            else:\n                # euclidean distance (produce nan)\n                att = 1. / (1. + euclidean)\n        elif self.params[""bcnn_match_score_type""] == ""cosine"":\n            # cosine similarity\n            x1_ = tf.nn.l2_normalize(x1, dim=2)\n            x2_ = tf.nn.l2_normalize(x2, dim=2)\n            sim = tf.einsum(""abcd,aecd->abe"", x1_, x2_) # value in [-1, 1]\n            att = (1. + sim) / 2. # value in [0, 1]\n        return att\n\n\n    def _convolution(self, x, d, name, reuse=False):\n        # conv: [batch, s+w-1, 1, d]\n        conv = tf.layers.conv2d(\n            inputs=x,\n            filters=self.params[""bcnn_num_filters""],\n            kernel_size=(self.params[""bcnn_filter_size""], d),\n            padding=""valid"",\n            activation=self.params[""bcnn_activation""],\n            strides=1,\n            reuse=reuse,\n            name=name)\n\n        # [batch, s+w-1, d, 1]\n        return tf.transpose(conv, perm=[0, 1, 3, 2])\n\n\n    def _w_ap(self, x, attention, name):\n        # x: [batch, s+w-1, d, 1]\n        # attention: [batch, s+w-1]\n        if attention is not None:\n            attention = tf.expand_dims(tf.expand_dims(attention, axis=-1), axis=-1)\n            x2 = x * attention\n        else:\n            x2 = x\n        w_ap = tf.layers.average_pooling2d(\n            inputs=x2,\n            pool_size=(self.params[""bcnn_filter_size""], 1),\n            strides=1,\n            padding=""valid"",\n            name=name)\n        if attention is not None:\n            w_ap = w_ap * self.params[""bcnn_filter_size""]\n\n        return w_ap\n\n\n    def _all_ap(self, x, seq_len, name):\n        if ""input"" in name:\n            pool_width = seq_len\n            d = self.params[""embedding_dim""]\n        else:\n            pool_width = seq_len + self.params[""bcnn_filter_size""] - 1\n            d = self.params[""bcnn_num_filters""]\n\n        all_ap = tf.layers.average_pooling2d(\n            inputs=x,\n            pool_size=(pool_width, 1),\n            strides=1,\n            padding=""valid"",\n            name=name)\n        all_ap_reshaped = tf.reshape(all_ap, [-1, d])\n\n        return all_ap_reshaped\n\n\n    def _expand_input(self, x1, x2, att_mat, seq_len, d, name):\n        # att_mat: [batch, s, s]\n        aW = tf.get_variable(name=name, shape=(seq_len, d))\n\n        # [batch, s, s] * [s,d] => [batch, s, d]\n        # expand dims => [batch, s, d, 1]\n        x1_a = tf.expand_dims(tf.einsum(""ijk,kl->ijl"", att_mat, aW), -1)\n        x2_a = tf.expand_dims(tf.einsum(""ijk,kl->ijl"", tf.matrix_transpose(att_mat), aW), -1)\n\n        # [batch, s, d, 2]\n        x1 = tf.concat([x1, x1_a], axis=3)\n        x2 = tf.concat([x2, x2_a], axis=3)\n\n        return x1, x2\n\n\n    def _bcnn_cnn_layer(self, x1, x2, seq_len, d, name, dpool_index, granularity=""word""):\n        return None, None, None, None, None\n\n\n    def _mp_cnn_layer(self, cross, dpool_index, filters, kernel_size, pool_size, strides, name):\n        cross_conv = tf.layers.conv2d(\n            inputs=cross,\n            filters=filters,\n            kernel_size=kernel_size,\n            padding=""same"",\n            activation=self.params[""bcnn_mp_activation""],\n            strides=1,\n            reuse=False,\n            name=name+""cross_conv"")\n        if self.params[""bcnn_mp_dynamic_pooling""] and dpool_index is not None:\n            cross_conv = tf.gather_nd(cross_conv, dpool_index)\n        cross_pool = tf.layers.max_pooling2d(\n            inputs=cross_conv,\n            pool_size=pool_size,\n            strides=strides,\n            padding=""valid"",\n            name=name+""cross_pool"")\n        return cross_pool\n\n    def _bcnn_semantic_feature_layer(self, seq_left, seq_right, dpool_index=None, granularity=""word""):\n        name = self.model_name + granularity\n        seq_len = self.params[""max_seq_len_%s"" % granularity]\n        # [batch, s, d] => [batch, s, d, 1]\n        seq_left = tf.expand_dims(seq_left, axis=-1)\n        seq_right = tf.expand_dims(seq_right, axis=-1)\n\n        left_ap_list = [None] * (self.params[""bcnn_num_layers""] + 1)\n        right_ap_list = [None] * (self.params[""bcnn_num_layers""] + 1)\n        left_ap_list[0] = self._all_ap(x=seq_left, seq_len=seq_len, name=name + ""global_pooling_input_left"")\n        right_ap_list[0] = self._all_ap(x=seq_right, seq_len=seq_len, name=name + ""global_pooling_input_right"")\n\n        x1 = seq_left\n        x2 = seq_right\n        d = self.params[""embedding_dim""]\n        outputs = []\n        for layer in range(self.params[""bcnn_num_layers""]):\n            x1, left_ap_list[layer + 1], x2, right_ap_list[layer + 1], att_pooled = self._bcnn_cnn_layer(x1=x1, x2=x2,\n                                                                                                         seq_len=seq_len,\n                                                                                                         d=d,\n                                                                                                         name=name + ""cnn_layer_%d"" % (\n                                                                                                                 layer + 1),\n                                                                                                         dpool_index=dpool_index,\n                                                                                                         granularity=granularity)\n            d = self.params[""bcnn_num_filters""]\n            if self.params[""bcnn_mp_att_pooling""] and att_pooled is not None:\n                outputs.append(att_pooled)\n\n        for l, r in zip(left_ap_list, right_ap_list):\n            outputs.append(metrics.cosine_similarity(l, r, self.params[""similarity_aggregation""]))\n            outputs.append(metrics.dot_product(l, r, self.params[""similarity_aggregation""]))\n            outputs.append(metrics.euclidean_distance(l, r, self.params[""similarity_aggregation""]))\n        return tf.concat(outputs, axis=-1)\n\n\n    def _get_attention_matrix_pooled_features(self, att_mat, seq_len, dpool_index, granularity, name):\n        # get attention matrix pooled features (as in sec. 5.3.1)\n        att_mat0 = tf.expand_dims(att_mat, axis=3)\n        # conv-pool layer 1\n        filters = self.params[""bcnn_mp_num_filters""][0]\n        kernel_size = self.params[""bcnn_mp_filter_sizes""][0]\n        # seq_len = seq_len + self.params[""bcnn_filter_size""] - 1\n        pool_size0 = self.params[""bcnn_mp_pool_sizes_%s"" % granularity][0]\n        pool_sizes = [seq_len / pool_size0, seq_len / pool_size0]\n        strides = [seq_len / pool_size0, seq_len / pool_size0]\n        conv1 = self._mp_cnn_layer(att_mat0, dpool_index, filters, kernel_size, pool_sizes, strides,\n                                   name=self.model_name + name + granularity + ""1"")\n        conv1_flatten = tf.reshape(conv1, [-1, self.params[""mp_num_filters""][0] * (pool_size0 * pool_size0)])\n\n        # conv-pool layer 2\n        filters = self.params[""bcnn_mp_num_filters""][1]\n        kernel_size = self.params[""bcnn_mp_filter_sizes""][1]\n        pool_size1 = self.params[""bcnn_mp_pool_sizes_%s"" % granularity][1]\n        pool_sizes = [pool_size0 / pool_size1, pool_size0 / pool_size1]\n        strides = [pool_size0 / pool_size1, pool_size0 / pool_size1]\n        conv2 = self._mp_cnn_layer(conv1, None, filters, kernel_size, pool_sizes, strides,\n                                   name=self.model_name + name + granularity + ""2"")\n        conv2_flatten = tf.reshape(conv2, [-1, self.params[""mp_num_filters""][1] * (pool_size1 * pool_size1)])\n\n        return conv2_flatten\n\n\n    def _get_feed_dict(self, X, idx, Q, construct_neg=False, training=False, symmetric=False):\n        feed_dict = super(BCNNBaseModel, self)._get_feed_dict(X, idx, Q, construct_neg, training, symmetric)\n        if self.params[""mp_dynamic_pooling""]:\n            dpool_index_word = dynamic_pooling_index(feed_dict[self.seq_len_word_left],\n                                                          feed_dict[self.seq_len_word_right],\n                                                          self.params[""max_seq_len_word""],\n                                                          self.params[""max_seq_len_word""])\n            dpool_index_char = dynamic_pooling_index(feed_dict[self.seq_len_char_left],\n                                                          feed_dict[self.seq_len_char_right],\n                                                          self.params[""max_seq_len_char""],\n                                                          self.params[""max_seq_len_char""])\n            feed_dict.update({\n                self.dpool_index_word: dpool_index_word,\n                self.dpool_index_char: dpool_index_char,\n            })\n        return feed_dict\n\n\n    def _get_matching_features(self):\n        with tf.name_scope(self.model_name):\n            tf.set_random_seed(self.params[""random_seed""])\n\n            with tf.name_scope(""word_network""):\n                if self.params[""attend_method""] == ""context-attention"":\n                    emb_seq_word_left, enc_seq_word_left, att_seq_word_left, sem_seq_word_left, \\\n                    emb_seq_word_right, enc_seq_word_right, att_seq_word_right, sem_seq_word_right = \\\n                        self._interaction_semantic_feature_layer(\n                            self.seq_word_left,\n                            self.seq_word_right,\n                            self.seq_len_word_left,\n                            self.seq_len_word_right,\n                            granularity=""word"")\n                else:\n                    emb_seq_word_left, enc_seq_word_left, att_seq_word_left, sem_seq_word_left = \\\n                        self._semantic_feature_layer(\n                            self.seq_word_left,\n                            self.seq_len_word_left,\n                            granularity=""word"", reuse=False)\n                    emb_seq_word_right, enc_seq_word_right, att_seq_word_right, sem_seq_word_right = \\\n                        self._semantic_feature_layer(\n                            self.seq_word_right,\n                            self.seq_len_word_right,\n                            granularity=""word"", reuse=True)\n                sim_word = self._bcnn_semantic_feature_layer(emb_seq_word_left, emb_seq_word_right, self.dpool_index_word, granularity=""word"")\n\n            with tf.name_scope(""char_network""):\n                if self.params[""attend_method""] == ""context-attention"":\n                    emb_seq_char_left, enc_seq_char_left, att_seq_char_left, sem_seq_char_left, \\\n                    emb_seq_char_right, enc_seq_char_right, att_seq_char_right, sem_seq_char_right = \\\n                        self._interaction_semantic_feature_layer(\n                            self.seq_char_left,\n                            self.seq_char_right,\n                            self.seq_len_char_left,\n                            self.seq_len_char_right,\n                            granularity=""char"")\n                else:\n                    emb_seq_char_left, enc_seq_char_left, att_seq_char_left, sem_seq_char_left = \\\n                        self._semantic_feature_layer(\n                            self.seq_char_left,\n                            self.seq_len_char_left,\n                            granularity=""char"", reuse=False)\n                    emb_seq_char_right, enc_seq_char_right, att_seq_char_right, sem_seq_char_right = \\\n                        self._semantic_feature_layer(\n                            self.seq_char_right,\n                            self.seq_len_char_right,\n                            granularity=""char"", reuse=True)\n                sim_char = self._bcnn_semantic_feature_layer(emb_seq_char_left, emb_seq_char_right, self.dpool_index_char, granularity=""char"")\n\n            with tf.name_scope(""matching_features""):\n                matching_features_word = sim_word\n                matching_features_char = sim_char\n\n        return matching_features_word, matching_features_char\n\n\nclass BCNN(BCNNBaseModel):\n    def __init__(self, params, logger, init_embedding_matrix):\n        p = copy(params)\n        p[""model_name""] = p[""model_name""] + ""bcnn""\n        super(BCNN, self).__init__(p, logger, init_embedding_matrix)\n\n\n    def _bcnn_cnn_layer(self, x1, x2, seq_len, d, name, dpool_index=None, granularity=""word""):\n        # x1, x2 = [batch, s, d, 1]\n        # att_mat0: [batch, s, s]\n        att_mat0 = self._make_attention_matrix(x1, x2)\n        left_conv = self._convolution(x=self._padding(x1, name=name+""padding_left""), d=d, name=name+""conv"", reuse=False)\n        right_conv = self._convolution(x=self._padding(x2, name=name+""padding_right""), d=d, name=name+""conv"", reuse=True)\n\n        left_attention, right_attention = None, None\n\n        left_wp = self._w_ap(x=left_conv, attention=left_attention, name=name+""attention_pooling_left"")\n        left_ap = self._all_ap(x=left_conv, seq_len=seq_len, name=name+""global_pooling_left"")\n        right_wp = self._w_ap(x=right_conv, attention=right_attention, name=name+""attention_pooling_right"")\n        right_ap = self._all_ap(x=right_conv, seq_len=seq_len, name=name+""global_pooling_right"")\n\n        # get attention matrix pooled features (as in sec. 5.3.1)\n        att_mat0_pooled = self._get_attention_matrix_pooled_features(att_mat0, seq_len, dpool_index, granularity, name+""att_pooled"")\n\n        return left_wp, left_ap, right_wp, right_ap, att_mat0_pooled\n\n\nclass ABCNN1(BCNNBaseModel):\n    def __init__(self, params, logger, init_embedding_matrix):\n        p = copy(params)\n        p[""model_name""] = p[""model_name""] + ""abcnn1""\n        super(ABCNN1, self).__init__(p, logger, init_embedding_matrix)\n\n\n    def _bcnn_cnn_layer(self, x1, x2, seq_len, d, name, dpool_index=None, granularity=""word""):\n        # x1, x2 = [batch, s, d, 1]\n        # att_mat0: [batch, s, s]\n        att_mat0 = self._make_attention_matrix(x1, x2)\n        x1, x2 = self._expand_input(x1, x2, att_mat0, seq_len, d, name=name+""expand_input"")\n\n        left_conv = self._convolution(x=self._padding(x1, name=name+""padding_left""), d=d, name=name+""conv"", reuse=False)\n        right_conv = self._convolution(x=self._padding(x2, name=name+""padding_right""), d=d, name=name+""conv"", reuse=True)\n\n        left_attention, right_attention = None, None\n\n        left_wp = self._w_ap(x=left_conv, attention=left_attention, name=name+""attention_pooling_left"")\n        left_ap = self._all_ap(x=left_conv, seq_len=seq_len, name=name+""global_pooling_left"")\n        right_wp = self._w_ap(x=right_conv, attention=right_attention, name=name+""attention_pooling_right"")\n        right_ap = self._all_ap(x=right_conv, seq_len=seq_len, name=name+""global_pooling_right"")\n\n        # get attention matrix pooled features (as in sec. 5.3.1)\n        att_mat0_pooled = self._get_attention_matrix_pooled_features(att_mat0, seq_len, dpool_index, granularity, name+""att_pooled"")\n\n        return left_wp, left_ap, right_wp, right_ap, att_mat0_pooled\n\n\nclass ABCNN2(BCNNBaseModel):\n    def __init__(self, params, logger, init_embedding_matrix):\n        p = copy(params)\n        p[""model_name""] = p[""model_name""] + ""abcnn2""\n        super(ABCNN2, self).__init__(p, logger, init_embedding_matrix)\n\n\n    def _bcnn_cnn_layer(self, x1, x2, seq_len, d, name, dpool_index=None, granularity=""word""):\n        # x1, x2 = [batch, s, d, 1]\n        att_mat0 = self._make_attention_matrix(x1, x2)\n        left_conv = self._convolution(x=self._padding(x1, name=name+""padding_left""), d=d, name=name+""conv"", reuse=False)\n        right_conv = self._convolution(x=self._padding(x2, name=name+""padding_right""), d=d, name=name+""conv"", reuse=True)\n\n        # [batch, s+w-1, s+w-1]\n        att_mat1 = self._make_attention_matrix(left_conv, right_conv)\n        # [batch, s+w-1], [batch, s+w-1]\n        left_attention, right_attention = tf.reduce_sum(att_mat1, axis=2), tf.reduce_sum(att_mat1, axis=1)\n\n        left_wp = self._w_ap(x=left_conv, attention=left_attention, name=name+""attention_pooling_left"")\n        left_ap = self._all_ap(x=left_conv, seq_len=seq_len, name=name+""global_pooling_left"")\n        right_wp = self._w_ap(x=right_conv, attention=right_attention, name=name+""attention_pooling_right"")\n        right_ap = self._all_ap(x=right_conv, seq_len=seq_len, name=name+""global_pooling_right"")\n\n        # get attention matrix pooled features (as in sec. 5.3.1)\n        att_mat0_pooled = self._get_attention_matrix_pooled_features(att_mat0, seq_len, dpool_index, granularity, name+""att_pooled"")\n\n        return left_wp, left_ap, right_wp, right_ap, att_mat0_pooled\n\n\nclass ABCNN3(BCNNBaseModel):\n    def __init__(self, params, logger, init_embedding_matrix):\n        p = copy(params)\n        p[""model_name""] = p[""model_name""] + ""abcnn3""\n        super(ABCNN3, self).__init__(p, logger, init_embedding_matrix)\n\n\n    def _bcnn_cnn_layer(self, x1, x2, seq_len, d, name, dpool_index=None, granularity=""word""):\n        # x1, x2 = [batch, s, d, 1]\n        # att_mat0: [batch, s, s\n        att_mat0 = self._make_attention_matrix(x1, x2)\n        x1, x2 = self._expand_input(x1, x2, att_mat0, seq_len, d, name=name + ""expand_input"")\n\n        left_conv = self._convolution(x=self._padding(x1, name=name+""padding_left""), d=d, name=name+""conv"", reuse=False)\n        right_conv = self._convolution(x=self._padding(x2, name=name+""padding_right""), d=d, name=name+""conv"", reuse=True)\n\n        # [batch, s+w-1, s+w-1]\n        att_mat1 = self._make_attention_matrix(left_conv, right_conv)\n        # [batch, s+w-1], [batch, s+w-1]\n        left_attention, right_attention = tf.reduce_sum(att_mat1, axis=2), tf.reduce_sum(att_mat1, axis=1)\n\n        left_wp = self._w_ap(x=left_conv, attention=left_attention, name=name+""attention_pooling_left"")\n        left_ap = self._all_ap(x=left_conv, seq_len=seq_len, name=name+""global_pooling_left"")\n        right_wp = self._w_ap(x=right_conv, attention=right_attention, name=name+""attention_pooling_right"")\n        right_ap = self._all_ap(x=right_conv, seq_len=seq_len, name=name+""global_pooling_right"")\n\n        # get attention matrix pooled features (as in sec. 5.3.1)\n        att_mat0_pooled = self._get_attention_matrix_pooled_features(att_mat0, seq_len, dpool_index, granularity, name+""att_pooled"")\n\n        return left_wp, left_ap, right_wp, right_ap, att_mat0_pooled\n'"
src/models/decatt.py,0,"b'\nfrom copy import copy\n\nfrom models.esim import ESIMDecAttBaseModel\n\n\nclass DecAtt(ESIMDecAttBaseModel):\n    def __init__(self, params, logger, init_embedding_matrix=None):\n        p = copy(params)\n        # model config\n        p.update({\n            ""model_name"": p[""model_name""] + ""dec_att"",\n            ""encode_method"": ""project"",\n            ""attend_method"": [""ave"", ""max"", ""min"", ""self-attention""],\n\n            ""project_type"": ""fc"",\n            ""project_hidden_units"": [64 * 4, 64 * 2, 64],\n            ""project_dropouts"": [0, 0, 0],\n\n            # fc block\n            ""fc_type"": ""fc"",\n            ""fc_hidden_units"": [64 * 4, 64 * 2, 64],\n            ""fc_dropouts"": [0, 0, 0],\n        })\n        super(DecAtt, self).__init__(p, logger, init_embedding_matrix)\n'"
src/models/dsmm.py,11,"b'\nfrom copy import copy\nimport tensorflow as tf\n\nfrom models.bcnn import BCNN, ABCNN1, ABCNN2, ABCNN3\nfrom models.esim import ESIMDecAttBaseModel\nfrom models.match_pyramid import MatchPyramidBaseModel\nfrom tf_common import metrics\nfrom tf_common.nn_module import mlp_layer\n\n\nclass DSMM(MatchPyramidBaseModel, ESIMDecAttBaseModel, BCNN):\n    def __init__(self, params, logger, init_embedding_matrix=None):\n        p = copy(params)\n        p[""model_name""] = p[""model_name""] + ""dsmm""\n        super(DSMM, self).__init__(p, logger, init_embedding_matrix)\n\n\n    def _get_matching_features(self):\n        with tf.name_scope(self.model_name):\n            tf.set_random_seed(self.params[""random_seed""])\n\n            with tf.name_scope(""word_network""):\n                if self.params[""attend_method""] == ""context-attention"":\n                    emb_seq_word_left, enc_seq_word_left, att_seq_word_left, sem_seq_word_left, \\\n                    emb_seq_word_right, enc_seq_word_right, att_seq_word_right, sem_seq_word_right = \\\n                        self._interaction_semantic_feature_layer(\n                            self.seq_word_left,\n                            self.seq_word_right,\n                            self.seq_len_word_left,\n                            self.seq_len_word_right,\n                            granularity=""word"")\n                else:\n                    emb_seq_word_left, enc_seq_word_left, att_seq_word_left, sem_seq_word_left = \\\n                        self._semantic_feature_layer(\n                            self.seq_word_left,\n                            self.seq_len_word_left,\n                            granularity=""word"", reuse=False)\n                    emb_seq_word_right, enc_seq_word_right, att_seq_word_right, sem_seq_word_right = \\\n                        self._semantic_feature_layer(\n                            self.seq_word_right,\n                            self.seq_len_word_right,\n                            granularity=""word"", reuse=True)\n\n                #### matching\n                # match score\n                sim_word = tf.concat([\n                    metrics.cosine_similarity(sem_seq_word_left, sem_seq_word_right, self.params[""similarity_aggregation""]),\n                    metrics.dot_product(sem_seq_word_left, sem_seq_word_right, self.params[""similarity_aggregation""]),\n                    metrics.euclidean_distance(sem_seq_word_left, sem_seq_word_right, self.params[""similarity_aggregation""]),\n                    # self._canberra_score(sem_seq_word_left, sem_seq_word_right),\n                ], axis=-1)\n\n                # match pyramid\n                match_matrix_word = self._get_match_matrix(self.seq_word_left, emb_seq_word_left, enc_seq_word_left,\n                                                           self.seq_word_right, emb_seq_word_right, enc_seq_word_right,\n                                                           granularity=""word"")\n                mp_word = self._mp_semantic_feature_layer(match_matrix_word,\n                                                          self.dpool_index_word,\n                                                          granularity=""word"")\n\n                # esim\n                esim_word = self._esim_semantic_feature_layer(emb_seq_word_left,\n                                                              emb_seq_word_right,\n                                                              self.seq_len_word_left,\n                                                              self.seq_len_word_right,\n                                                              granularity=""word"")\n\n                # bcnn\n                bcnn_word = self._bcnn_semantic_feature_layer(emb_seq_word_left,\n                                                              emb_seq_word_right,\n                                                              granularity=""word"")\n\n                # dense\n                deep_in_word = tf.concat([sem_seq_word_left, sem_seq_word_right], axis=-1)\n                deep_word = mlp_layer(deep_in_word, fc_type=self.params[""fc_type""],\n                                      hidden_units=self.params[""fc_hidden_units""],\n                                      dropouts=self.params[""fc_dropouts""],\n                                      scope_name=self.model_name + ""deep_word"",\n                                      reuse=False,\n                                      training=self.training,\n                                      seed=self.params[""random_seed""])\n\n            with tf.name_scope(""char_network""):\n                if self.params[""attend_method""] == ""context-attention"":\n                    emb_seq_char_left, enc_seq_char_left, att_seq_char_left, sem_seq_char_left, \\\n                    emb_seq_char_right, enc_seq_char_right, att_seq_char_right, sem_seq_char_right = \\\n                        self._interaction_semantic_feature_layer(\n                            self.seq_char_left,\n                            self.seq_char_right,\n                            self.seq_len_char_left,\n                            self.seq_len_char_right,\n                            granularity=""char"")\n                else:\n                    emb_seq_char_left, enc_seq_char_left, att_seq_char_left, sem_seq_char_left = \\\n                        self._semantic_feature_layer(\n                            self.seq_char_left,\n                            self.seq_len_char_left,\n                            granularity=""char"", reuse=False)\n                    emb_seq_char_right, enc_seq_char_right, att_seq_char_right, sem_seq_char_right = \\\n                        self._semantic_feature_layer(\n                            self.seq_char_right,\n                            self.seq_len_char_right,\n                            granularity=""char"", reuse=True)\n\n                # match score\n                sim_char = tf.concat([\n                    metrics.cosine_similarity(sem_seq_char_left, sem_seq_char_right, self.params[""similarity_aggregation""]),\n                    metrics.dot_product(sem_seq_char_left, sem_seq_char_right, self.params[""similarity_aggregation""]),\n                    metrics.euclidean_distance(sem_seq_char_left, sem_seq_char_right, self.params[""similarity_aggregation""]),\n                    # self._canberra_score(sem_seq_char_left, sem_seq_char_right),\n                ], axis=-1)\n\n                # match pyramid\n                match_matrix_char = self._get_match_matrix(self.seq_char_left, emb_seq_char_left, enc_seq_char_left,\n                                                           self.seq_char_right, emb_seq_char_right, enc_seq_char_right,\n                                                           granularity=""char"")\n                mp_char = self._mp_semantic_feature_layer(match_matrix_char,\n                                                          self.dpool_index_char,\n                                                          granularity=""char"")\n\n                # esim\n                esim_char = self._esim_semantic_feature_layer(emb_seq_char_left,\n                                                              emb_seq_char_right,\n                                                              self.seq_len_char_left,\n                                                              self.seq_len_char_right,\n                                                              granularity=""char"")\n\n                # bcnn\n                bcnn_char = self._bcnn_semantic_feature_layer(emb_seq_char_left,\n                                                              emb_seq_char_right,\n                                                              granularity=""char"")\n\n                # dense\n                deep_in_char = tf.concat([sem_seq_char_left, sem_seq_char_right], axis=-1)\n                deep_char = mlp_layer(deep_in_char, fc_type=self.params[""fc_type""],\n                                      hidden_units=self.params[""fc_hidden_units""],\n                                      dropouts=self.params[""fc_dropouts""],\n                                      scope_name=self.model_name + ""deep_char"",\n                                      reuse=False,\n                                      training=self.training,\n                                      seed=self.params[""random_seed""])\n\n            with tf.name_scope(""matching_features""):\n                matching_features_word = tf.concat([\n                    sim_word, mp_word, esim_word, bcnn_word, deep_word,# sem_seq_word_left, sem_seq_word_right,\n                ], axis=-1)\n                matching_features_char = tf.concat([\n                    sim_char, mp_char, esim_char, bcnn_char, deep_char,# sem_seq_char_left, sem_seq_char_right,\n                ], axis=-1)\n\n        return matching_features_word, matching_features_char\n'"
src/models/dssm.py,8,"b'\nfrom copy import copy\nimport tensorflow as tf\n\nfrom models.base_model import BaseModel\nfrom tf_common import metrics\n\n\nclass DSSMBaseModel(BaseModel):\n    def __init__(self, params, logger, init_embedding_matrix=None):\n        super(DSSMBaseModel, self).__init__(params, logger, init_embedding_matrix)\n\n\n    def _get_matching_features(self):\n        with tf.name_scope(self.model_name):\n            tf.set_random_seed(self.params[""random_seed""])\n\n            with tf.name_scope(""word_network""):\n                if self.params[""attend_method""] == ""context-attention"":\n                    emb_seq_word_left, enc_seq_word_left, att_seq_word_left, sem_seq_word_left, \\\n                    emb_seq_word_right, enc_seq_word_right, att_seq_word_right, sem_seq_word_right = \\\n                        self._interaction_semantic_feature_layer(\n                            self.seq_word_left,\n                            self.seq_word_right,\n                            self.seq_len_word_left,\n                            self.seq_len_word_right,\n                            granularity=""word"")\n                else:\n                    emb_seq_word_left, enc_seq_word_left, att_seq_word_left, sem_seq_word_left = \\\n                        self._semantic_feature_layer(\n                            self.seq_word_left,\n                            self.seq_len_word_left,\n                            granularity=""word"", reuse=False)\n                    emb_seq_word_right, enc_seq_word_right, att_seq_word_right, sem_seq_word_right = \\\n                        self._semantic_feature_layer(\n                            self.seq_word_right,\n                            self.seq_len_word_right,\n                            granularity=""word"", reuse=True)\n                # match score\n                sim_word = tf.concat([\n                    metrics.cosine_similarity(sem_seq_word_left, sem_seq_word_right, self.params[""similarity_aggregation""]),\n                    metrics.dot_product(sem_seq_word_left, sem_seq_word_right, self.params[""similarity_aggregation""]),\n                    metrics.euclidean_distance(sem_seq_word_left, sem_seq_word_right, self.params[""similarity_aggregation""]),\n                    # self._canberra_score(sem_seq_word_left, sem_seq_word_right),\n                ], axis=-1)\n\n            with tf.name_scope(""char_network""):\n                if self.params[""attend_method""] == ""context-attention"":\n                    emb_seq_char_left, enc_seq_char_left, att_seq_char_left, sem_seq_char_left, \\\n                    emb_seq_char_right, enc_seq_char_right, att_seq_char_right, sem_seq_char_right = \\\n                        self._interaction_semantic_feature_layer(\n                            self.seq_char_left,\n                            self.seq_char_right,\n                            self.seq_len_char_left,\n                            self.seq_len_char_right,\n                            granularity=""char"")\n                else:\n                    emb_seq_char_left, enc_seq_char_left, att_seq_char_left, sem_seq_char_left = \\\n                        self._semantic_feature_layer(\n                            self.seq_char_left,\n                            self.seq_len_char_left,\n                            granularity=""char"", reuse=False)\n                    emb_seq_char_right, enc_seq_char_right, att_seq_char_right, sem_seq_char_right = \\\n                        self._semantic_feature_layer(\n                            self.seq_char_right,\n                            self.seq_len_char_right,\n                            granularity=""char"", reuse=True)\n                # match score\n                sim_char = tf.concat([\n                    metrics.cosine_similarity(sem_seq_char_left, sem_seq_char_right, self.params[""similarity_aggregation""]),\n                    metrics.dot_product(sem_seq_char_left, sem_seq_char_right, self.params[""similarity_aggregation""]),\n                    metrics.euclidean_distance(sem_seq_char_left, sem_seq_char_right, self.params[""similarity_aggregation""]),\n                    # self._canberra_score(sem_seq_char_left, sem_seq_char_right),\n                ], axis=-1)\n\n            with tf.name_scope(""matching_features""):\n                matching_features_word = sim_word\n                matching_features_char = sim_char\n\n        return matching_features_word, matching_features_char\n\n\nclass DSSM(DSSMBaseModel):\n    def __init__(self, params, logger, init_embedding_matrix=None):\n        p = copy(params)\n        # model config\n        p.update({\n            ""model_name"": p[""model_name""] + ""dssm"",\n            ""encode_method"": ""fasttext"",\n            ""attend_method"": [""ave"", ""max"", ""min"", ""self-scalar-attention""],\n\n            # fc block\n            ""fc_type"": ""fc"",\n            ""fc_hidden_units"": [64 * 4, 64 * 2, 64],\n            ""fc_dropouts"": [0, 0, 0],\n        })\n        super(DSSM, self).__init__(p, logger, init_embedding_matrix)\n\n\nclass CDSSM(DSSMBaseModel):\n    def __init__(self, params, logger, init_embedding_matrix=None):\n        p = copy(params)\n        # model config\n        p.update({\n            ""model_name"": p[""model_name""] + ""cdssm"",\n            ""encode_method"": ""textcnn"",\n            ""attend_method"": [""ave"", ""max"", ""min"", ""self-scalar-attention""],\n\n            # cnn\n            ""cnn_num_layers"": 1,\n            ""cnn_num_filters"": 32,\n            ""cnn_filter_sizes"": [1, 2, 3],\n            ""cnn_timedistributed"": False,\n            ""cnn_activation"": tf.nn.relu,\n            ""cnn_gated_conv"": False,\n            ""cnn_residual"": False,\n\n            # fc block\n            ""fc_type"": ""fc"",\n            ""fc_hidden_units"": [64 * 4, 64 * 2, 64],\n            ""fc_dropouts"": [0, 0, 0],\n        })\n        super(CDSSM, self).__init__(p, logger, init_embedding_matrix)\n\n\nclass RDSSM(DSSMBaseModel):\n    def __init__(self, params, logger, init_embedding_matrix=None):\n        p = copy(params)\n        # model config\n        p.update({\n            ""model_name"": p[""model_name""] + ""rdssm"",\n            ""encode_method"": ""textbirnn"",\n            ""attend_method"": [""ave"", ""max"", ""min"", ""self-scalar-attention""],\n\n            # rnn\n            ""rnn_num_units"": 32,\n            ""rnn_cell_type"": ""gru"",\n            ""rnn_num_layers"": 1,\n\n            # fc block\n            ""fc_type"": ""fc"",\n            ""fc_hidden_units"": [64 * 4, 64 * 2, 64],\n            ""fc_dropouts"": [0, 0, 0],\n        })\n        super(RDSSM, self).__init__(p, logger, init_embedding_matrix)\n'"
src/models/esim.py,15,"b'\nfrom copy import copy\nimport numpy as np\nimport tensorflow as tf\n\nfrom models.base_model import BaseModel\nfrom tf_common.nn_module import word_dropout\nfrom tf_common.nn_module import encode, attend\n\n\nclass ESIMDecAttBaseModel(BaseModel):\n    """"""\n    Implementation of base model of ESIM and DecAtt\n    The difference between them lies in the encoder they use.\n        - ESIM: BiLSTM\n        - DecAtt: timedistributed dense projection\n\n    Reference\n    Paper:\n        - ESIM: Enhanced LSTM for Natural Language Inference\n        - DecAtt: A Decomposable Attention Model for Natural Language Inference\n    Keras:\n        https://www.kaggle.com/lamdang/dl-models\n    Pytorch:\n        https://github.com/lanwuwei/SPM_toolkit\n    """"""\n    def __init__(self, params, logger, init_embedding_matrix=None):\n        super(ESIMDecAttBaseModel, self).__init__(params, logger, init_embedding_matrix)\n\n\n    def _soft_attention_alignment(self, x1, x2):\n        ""Align text representation with neural soft attention""\n        # x1: [b, s1, d]\n        # x2: [b, s2, d]\n        # att: [b, s1, s2]\n        att = tf.einsum(""abd,acd->abc"", x1, x2)\n        w_att_1 = tf.nn.softmax(att, dim=1)\n        w_att_2 = tf.nn.softmax(att, dim=2)\n        x2_att = tf.einsum(""abd,abc->acd"", x1, w_att_1)\n        x1_att = tf.einsum(""abd,acb->acd"", x2, w_att_2)\n        return x1_att, x2_att\n\n\n    def _esim_semantic_feature_layer(self, emb_seq_left, emb_seq_right, seq_len_left, seq_len_right, granularity=""word""):\n        # for sharing embedding with other sub-graph\n        # #### embed\n        # emb_matrix = self._get_embedding_matrix(granularity)\n        # emb_seq_left = tf.nn.embedding_lookup(emb_matrix, seq_input_left)\n        # emb_seq_right = tf.nn.embedding_lookup(emb_matrix, seq_input_right)\n        #\n        # #### dropout\n        # random_seed = np.random.randint(10000000)\n        # emb_seq_left = word_dropout(emb_seq_left,\n        #                             training=self.training,\n        #                             dropout=self.params[""embedding_dropout""],\n        #                             seed=random_seed)\n        # random_seed = np.random.randint(10000000)\n        # emb_seq_right = word_dropout(emb_seq_right,\n        #                              training=self.training,\n        #                              dropout=self.params[""embedding_dropout""],\n        #                              seed=random_seed)\n\n        #### encode\n        input_dim = self.params[""embedding_dim""]\n        enc_seq_left = encode(emb_seq_left, method=self.params[""encode_method""],\n                              input_dim=input_dim,\n                              params=self.params,\n                              sequence_length=seq_len_left,\n                              mask_zero=self.params[""embedding_mask_zero""],\n                              scope_name=self.model_name + ""esim_enc_seq_%s"" % granularity, reuse=False,\n                              training=self.training)\n        enc_seq_right = encode(emb_seq_right, method=self.params[""encode_method""],\n                               input_dim=input_dim,\n                               params=self.params,\n                               sequence_length=seq_len_right,\n                               mask_zero=self.params[""embedding_mask_zero""],\n                               scope_name=self.model_name + ""esim_enc_seq_%s"" % granularity, reuse=True,\n                               training=self.training)\n\n        #### align\n        ali_seq_left, ali_seq_right = self._soft_attention_alignment(enc_seq_left, enc_seq_right)\n\n        #### compose\n        com_seq_left = tf.concat([\n            enc_seq_left,\n            ali_seq_left,\n            enc_seq_left * ali_seq_left,\n            enc_seq_left - ali_seq_left,\n        ], axis=-1)\n        com_seq_right = tf.concat([\n            enc_seq_right,\n            ali_seq_right,\n            enc_seq_right * ali_seq_right,\n            enc_seq_right - ali_seq_right,\n        ], axis=-1)\n\n        input_dim = self.params[""encode_dim""] * 4\n        compare_seq_left = encode(com_seq_left, method=self.params[""encode_method""],\n                                  input_dim=input_dim,\n                                  params=self.params,\n                                  sequence_length=seq_len_left,\n                                  mask_zero=self.params[""embedding_mask_zero""],\n                                  scope_name=self.model_name + ""compare_seq_%s"" % granularity, reuse=False,\n                                  training=self.training)\n        compare_seq_right = encode(com_seq_right, method=self.params[""encode_method""],\n                                   input_dim=input_dim,\n                                   params=self.params,\n                                   sequence_length=seq_len_right,\n                                   mask_zero=self.params[""embedding_mask_zero""],\n                                   scope_name=self.model_name + ""compare_seq_%s"" % granularity, reuse=True,\n                                   training=self.training)\n\n        #### attend\n        feature_dim = self.params[""encode_dim""]\n        att_seq_left = attend(compare_seq_left, context=None,\n                              encode_dim=self.params[""encode_dim""],\n                              feature_dim=feature_dim,\n                              attention_dim=self.params[""attention_dim""],\n                              method=self.params[""attend_method""],\n                              scope_name=self.model_name + ""agg_seq_%s"" % granularity,\n                              reuse=False, num_heads=self.params[""attention_num_heads""])\n        att_seq_right = attend(compare_seq_right, context=None,\n                               encode_dim=self.params[""encode_dim""],\n                               feature_dim=feature_dim,\n                               attention_dim=self.params[""attention_dim""],\n                               method=self.params[""attend_method""],\n                               scope_name=self.model_name + ""agg_seq_%s"" % granularity,\n                               reuse=True, num_heads=self.params[""attention_num_heads""])\n        return tf.concat([att_seq_left, att_seq_right], axis=-1)\n\n\n    def _get_matching_features(self):\n        with tf.name_scope(self.model_name):\n            tf.set_random_seed(self.params[""random_seed""])\n\n            with tf.name_scope(""word_network""):\n                emb_seq_word_left, enc_seq_word_left, att_seq_word_left, sem_seq_word_left = \\\n                    self._semantic_feature_layer(\n                        self.seq_word_left,\n                        self.seq_len_word_left,\n                        granularity=""word"", reuse=False)\n                emb_seq_word_right, enc_seq_word_right, att_seq_word_right, sem_seq_word_right = \\\n                    self._semantic_feature_layer(\n                        self.seq_word_right,\n                        self.seq_len_word_right,\n                        granularity=""word"", reuse=True)\n                sim_word = self._esim_semantic_feature_layer(\n                    emb_seq_word_left,\n                    emb_seq_word_right,\n                    self.seq_len_word_left,\n                    self.seq_len_word_right,\n                    granularity=""word"")\n\n            with tf.name_scope(""char_network""):\n                emb_seq_char_left, enc_seq_char_left, att_seq_char_left, sem_seq_char_left = \\\n                    self._semantic_feature_layer(\n                        self.seq_char_left,\n                        self.seq_len_char_left,\n                        granularity=""char"", reuse=False)\n                emb_seq_char_right, enc_seq_char_right, att_seq_char_right, sem_seq_char_right = \\\n                    self._semantic_feature_layer(\n                        self.seq_char_right,\n                        self.seq_len_char_right,\n                        granularity=""char"", reuse=True)\n                sim_char = self._esim_semantic_feature_layer(\n                    emb_seq_char_left,\n                    emb_seq_char_right,\n                    self.seq_len_char_left,\n                    self.seq_len_char_right,\n                    granularity=""char"")\n\n            with tf.name_scope(""matching_features""):\n                matching_features_word = sim_word\n                matching_features_char = sim_char\n\n        return matching_features_word, matching_features_char\n\n\nclass ESIM(ESIMDecAttBaseModel):\n    def __init__(self, params, logger, init_embedding_matrix=None):\n        p = copy(params)\n        # model config\n        p.update({\n            ""model_name"": p[""model_name""] + ""esim"",\n            ""encode_method"": ""textbirnn"",\n            ""attend_method"": [""ave"", ""max"", ""min"", ""self-attention""],\n\n            # rnn\n            ""rnn_num_units"": 32,\n            ""rnn_cell_type"": ""gru"",\n            ""rnn_num_layers"": 1,\n\n            # fc block\n            ""fc_type"": ""fc"",\n            ""fc_hidden_units"": [64 * 4, 64 * 2, 64],\n            ""fc_dropouts"": [0, 0, 0],\n        })\n        super(ESIMDecAttBaseModel, self).__init__(p, logger, init_embedding_matrix)\n'"
src/models/match_pyramid.py,48,"b'\nfrom copy import copy\nimport tensorflow as tf\n\nfrom inputs.dynamic_pooling import dynamic_pooling_index\nfrom models.base_model import BaseModel\n\n\nclass MatchPyramidBaseModel(BaseModel):\n    def __init__(self, params, logger, init_embedding_matrix=None):\n        super(MatchPyramidBaseModel, self).__init__(params, logger, init_embedding_matrix)\n\n\n    def _init_tf_vars(self):\n        super(MatchPyramidBaseModel, self)._init_tf_vars()\n        self.dpool_index_word = tf.placeholder(tf.int32, shape=[None, self.params[""max_seq_len_word""],\n                                                                self.params[""max_seq_len_word""], 3],\n                                               name=""dpool_index_word"")\n        self.dpool_index_char = tf.placeholder(tf.int32, shape=[None, self.params[""max_seq_len_char""],\n                                                                self.params[""max_seq_len_char""], 3],\n                                               name=""dpool_index_char"")\n\n\n    def _get_match_matrix(self, seq_left, emb_seq_left, enc_seq_left, seq_right, emb_seq_right, enc_seq_right,\n                          granularity=""word""):\n        # 1. word embedding\n        # 1.1 dot product: [batchsize, s1, s2, 1]\n        match_matrix_dot_product = tf.expand_dims(\n            tf.einsum(""abd,acd->abc"", emb_seq_left, emb_seq_right), axis=-1)\n        # 1.2 identity: [batchsize, s1, s2, 1]\n        match_matrix_identity = tf.expand_dims(tf.cast(\n            tf.equal(\n                tf.expand_dims(seq_left, 2),\n                tf.expand_dims(seq_right, 1)\n            ), tf.float32), axis=-1)\n\n        # 2. compressed word embedding\n        eW = tf.get_variable(""eW_%s"" % (self.model_name + granularity),\n                             initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.2, dtype=tf.float32),\n                             dtype=tf.float32,\n                             shape=[self.params[""embedding_dim_%s"" % granularity],\n                                    self.params[""embedding_dim_compressed""]])\n        emb_seq_com_left = tf.einsum(""abd,dc->abc"", emb_seq_left, eW)\n        emb_seq_com_right = tf.einsum(""abd,dc->abc"", emb_seq_right, eW)\n        # 2.1 dot product: [batchsize, s1, s2, 1]\n        match_matrix_dot_product_com = tf.expand_dims(\n            tf.einsum(""abd,acd->abc"", emb_seq_com_left, emb_seq_com_right), axis=-1)\n        # 2.2 element product: [batchsize, s1, s2, d]\n        match_matrix_element_product_com = tf.expand_dims(emb_seq_com_left, 2) * tf.expand_dims(\n            emb_seq_com_right, 1)\n        # 2.3 element concat: [batchsize, s1, s2, 2*d]\n        match_matrix_element_concat_com = tf.concat([\n            tf.tile(tf.expand_dims(emb_seq_com_left, 2), [1, 1, self.params[""max_seq_len_%s"" % granularity], 1]),\n            tf.tile(tf.expand_dims(emb_seq_com_right, 1), [1, self.params[""max_seq_len_%s"" % granularity], 1, 1]),\n        ], axis=-1)\n\n        # 3. contextual word embedding\n        # 3.1 dot product: [batchsize, s1, s2, 1]\n        match_matrix_dot_product_ctx = tf.expand_dims(\n            tf.einsum(""abd,acd->abc"", enc_seq_left, enc_seq_right), axis=-1)\n        # 2.2 element product: [batchsize, s1, s2, d]\n        match_matrix_element_product_ctx = tf.expand_dims(enc_seq_left, 2) * tf.expand_dims(\n            enc_seq_right, 1)\n        # 2.3 element concat: [batchsize, s1, s2, 2*d]\n        match_matrix_element_concat_ctx = tf.concat([\n            tf.tile(tf.expand_dims(enc_seq_left, 2), [1, 1, self.params[""max_seq_len_%s"" % granularity], 1]),\n            tf.tile(tf.expand_dims(enc_seq_right, 1), [1, self.params[""max_seq_len_%s"" % granularity], 1, 1]),\n        ], axis=-1)\n\n        match_matrix = tf.concat([\n            match_matrix_dot_product,\n            match_matrix_identity,\n            match_matrix_dot_product_com,\n            match_matrix_element_product_com,\n            match_matrix_element_concat_com,\n            match_matrix_dot_product_ctx,\n            match_matrix_element_product_ctx,\n            match_matrix_element_concat_ctx,\n        ], axis=-1)\n        return match_matrix\n\n\n    def _mp_cnn_layer(self, cross, dpool_index, filters, kernel_size, pool_size, strides, name):\n        cross_conv = tf.layers.conv2d(\n            inputs=cross,\n            filters=filters,\n            kernel_size=kernel_size,\n            padding=""same"",\n            activation=self.params[""mp_activation""],\n            strides=1,\n            reuse=False,\n            name=name+""cross_conv"")\n        if self.params[""mp_dynamic_pooling""] and dpool_index is not None:\n            cross_conv = tf.gather_nd(cross_conv, dpool_index)\n        cross_pool = tf.layers.max_pooling2d(\n            inputs=cross_conv,\n            pool_size=pool_size,\n            strides=strides,\n            padding=""valid"",\n            name=name+""cross_pool"")\n        return cross_pool\n\n\n    def _mp_semantic_feature_layer(self, match_matrix, dpool_index, granularity=""word""):\n\n        # conv-pool layer 1\n        filters = self.params[""mp_num_filters""][0]\n        kernel_size = self.params[""mp_filter_sizes""][0]\n        seq_len = self.params[""max_seq_len_%s"" % granularity]\n        pool_size0 = self.params[""mp_pool_sizes_%s"" % granularity][0]\n        pool_sizes = [seq_len / pool_size0, seq_len / pool_size0]\n        strides = [seq_len / pool_size0, seq_len / pool_size0]\n        conv1 = self._mp_cnn_layer(match_matrix, dpool_index, filters, kernel_size, pool_sizes, strides, name=self.model_name+granularity+""1"")\n        conv1_flatten = tf.reshape(conv1, [-1, self.params[""mp_num_filters""][0] * (pool_size0 * pool_size0)])\n\n        # conv-pool layer 2\n        filters = self.params[""mp_num_filters""][1]\n        kernel_size = self.params[""mp_filter_sizes""][1]\n        pool_size1 = self.params[""mp_pool_sizes_%s"" % granularity][1]\n        pool_sizes = [pool_size0 / pool_size1, pool_size0 / pool_size1]\n        strides = [pool_size0 / pool_size1, pool_size0 / pool_size1]\n        conv2 = self._mp_cnn_layer(conv1, None, filters, kernel_size, pool_sizes, strides, name=self.model_name + granularity + ""2"")\n        conv2_flatten = tf.reshape(conv2, [-1, self.params[""mp_num_filters""][1] * (pool_size1 * pool_size1)])\n\n        # cross = tf.concat([conv1_flatten, conv2_flatten], axis=-1)\n\n        return conv2_flatten\n\n\n    def _get_feed_dict(self, X, idx, Q, construct_neg=False, training=False, symmetric=False):\n        feed_dict = super(MatchPyramidBaseModel, self)._get_feed_dict(X, idx, Q, construct_neg, training, symmetric)\n        if self.params[""mp_dynamic_pooling""]:\n            dpool_index_word = dynamic_pooling_index(feed_dict[self.seq_len_word_left],\n                                                          feed_dict[self.seq_len_word_right],\n                                                          self.params[""max_seq_len_word""],\n                                                          self.params[""max_seq_len_word""])\n            dpool_index_char = dynamic_pooling_index(feed_dict[self.seq_len_char_left],\n                                                          feed_dict[self.seq_len_char_right],\n                                                          self.params[""max_seq_len_char""],\n                                                          self.params[""max_seq_len_char""])\n            feed_dict.update({\n                self.dpool_index_word: dpool_index_word,\n                self.dpool_index_char: dpool_index_char,\n            })\n        return feed_dict\n\n\nclass MatchPyramid(MatchPyramidBaseModel):\n    def __init__(self, params, logger, init_embedding_matrix=None):\n        p = copy(params)\n        p[""model_name""] = p[""model_name""] + ""match_pyramid""\n        super(MatchPyramid, self).__init__(p, logger, init_embedding_matrix)\n\n\n    def _get_matching_features(self):\n        with tf.name_scope(self.model_name):\n            tf.set_random_seed(self.params[""random_seed""])\n\n            with tf.name_scope(""word_network""):\n                if self.params[""attend_method""] == ""context-attention"":\n                    emb_seq_word_left, enc_seq_word_left, att_seq_word_left, sem_seq_word_left, \\\n                    emb_seq_word_right, enc_seq_word_right, att_seq_word_right, sem_seq_word_right = \\\n                        self._interaction_semantic_feature_layer(\n                            self.seq_word_left,\n                            self.seq_word_right,\n                            self.seq_len_word_left,\n                            self.seq_len_word_right,\n                            granularity=""word"")\n                else:\n                    emb_seq_word_left, enc_seq_word_left, att_seq_word_left, sem_seq_word_left = \\\n                        self._semantic_feature_layer(\n                            self.seq_word_left,\n                            self.seq_len_word_left,\n                            granularity=""word"", reuse=False)\n                    emb_seq_word_right, enc_seq_word_right, att_seq_word_right, sem_seq_word_right = \\\n                        self._semantic_feature_layer(\n                            self.seq_word_right,\n                            self.seq_len_word_right,\n                            granularity=""word"", reuse=True)\n                match_matrix_word = tf.einsum(""abd,acd->abc"", emb_seq_word_left, emb_seq_word_right)\n                match_matrix_word = tf.expand_dims(match_matrix_word, axis=-1)\n                sim_word = self._mp_semantic_feature_layer(match_matrix_word, self.dpool_index_word,\n                                                             granularity=""word"")\n\n            with tf.name_scope(""char_network""):\n                if self.params[""attend_method""] == ""context-attention"":\n                    emb_seq_char_left, enc_seq_char_left, att_seq_char_left, sem_seq_char_left, \\\n                    emb_seq_char_right, enc_seq_char_right, att_seq_char_right, sem_seq_char_right = \\\n                        self._interaction_semantic_feature_layer(\n                            self.seq_char_left,\n                            self.seq_char_right,\n                            self.seq_len_char_left,\n                            self.seq_len_char_right,\n                            granularity=""char"")\n                else:\n                    emb_seq_char_left, enc_seq_char_left, att_seq_char_left, sem_seq_char_left = \\\n                        self._semantic_feature_layer(\n                            self.seq_char_left,\n                            self.seq_len_char_left,\n                            granularity=""char"", reuse=False)\n                    emb_seq_char_right, enc_seq_char_right, att_seq_char_right, sem_seq_char_right = \\\n                        self._semantic_feature_layer(\n                            self.seq_char_right,\n                            self.seq_len_char_right,\n                            granularity=""char"", reuse=True)\n                match_matrix_char = tf.einsum(""abd,acd->abc"", emb_seq_char_left, emb_seq_char_right)\n                match_matrix_char = tf.expand_dims(match_matrix_char, axis=-1)\n                sim_char = self._mp_semantic_feature_layer(match_matrix_char, self.dpool_index_char,\n                                                             granularity=""char"")\n            with tf.name_scope(""matching_features""):\n                matching_features_word = sim_word\n                matching_features_char = sim_char\n\n        return matching_features_word, matching_features_char\n\n\nclass GMatchPyramid(MatchPyramidBaseModel):\n    def __init__(self, params, logger, init_embedding_matrix=None):\n        p = copy(params)\n        # model config\n        p.update({\n            ""model_name"": p[""model_name""] + ""g_match_pyramid"",\n            ""encode_method"": ""textcnn"",\n            ""attend_method"": [""ave"", ""max"", ""min"", ""self-attention""],\n\n            # cnn\n            ""cnn_num_layers"": 1,\n            ""cnn_num_filters"": 32,\n            ""cnn_filter_sizes"": [1, 2, 3],\n            ""cnn_timedistributed"": False,\n            ""cnn_activation"": tf.nn.relu,\n            ""cnn_gated_conv"": True,\n            ""cnn_residual"": True,\n\n            # fc block\n            ""fc_type"": ""fc"",\n            ""fc_hidden_units"": [64 * 4, 64 * 2, 64],\n            ""fc_dropouts"": [0, 0, 0],\n        })\n        super(GMatchPyramid, self).__init__(p, logger, init_embedding_matrix)\n\n\n    def _get_matching_features(self):\n        with tf.name_scope(self.model_name):\n            tf.set_random_seed(self.params[""random_seed""])\n\n            with tf.name_scope(""word_network""):\n                if self.params[""attend_method""] == ""context-attention"":\n                    emb_seq_word_left, enc_seq_word_left, att_seq_word_left, sem_seq_word_left, \\\n                    emb_seq_word_right, enc_seq_word_right, att_seq_word_right, sem_seq_word_right = \\\n                        self._interaction_semantic_feature_layer(\n                            self.seq_word_left,\n                            self.seq_word_right,\n                            self.seq_len_word_left,\n                            self.seq_len_word_right,\n                            granularity=""word"")\n                else:\n                    emb_seq_word_left, enc_seq_word_left, att_seq_word_left, sem_seq_word_left = \\\n                        self._semantic_feature_layer(\n                            self.seq_word_left,\n                            self.seq_len_word_left,\n                            granularity=""word"", reuse=False)\n                    emb_seq_word_right, enc_seq_word_right, att_seq_word_right, sem_seq_word_right = \\\n                        self._semantic_feature_layer(\n                            self.seq_word_right,\n                            self.seq_len_word_right,\n                            granularity=""word"", reuse=True)\n\n                match_matrix_word = self._get_match_matrix(self.seq_word_left, emb_seq_word_left, enc_seq_word_left,\n                                                           self.seq_word_right, emb_seq_word_right, enc_seq_word_right,\n                                                           granularity=""word"")\n                sim_word = self._mp_semantic_feature_layer(match_matrix_word, self.dpool_index_word, granularity=""word"")\n\n            with tf.name_scope(""char_network""):\n                if self.params[""attend_method""] == ""context-attention"":\n                    emb_seq_char_left, enc_seq_char_left, att_seq_char_left, sem_seq_char_left, \\\n                    emb_seq_char_right, enc_seq_char_right, att_seq_char_right, sem_seq_char_right = \\\n                        self._interaction_semantic_feature_layer(\n                            self.seq_char_left,\n                            self.seq_char_right,\n                            self.seq_len_char_left,\n                            self.seq_len_char_right,\n                            granularity=""char"")\n                else:\n                    emb_seq_char_left, enc_seq_char_left, att_seq_char_left, sem_seq_char_left = \\\n                        self._semantic_feature_layer(\n                            self.seq_char_left,\n                            self.seq_len_char_left,\n                            granularity=""char"", reuse=False)\n                    emb_seq_char_right, enc_seq_char_right, att_seq_char_right, sem_seq_char_right = \\\n                        self._semantic_feature_layer(\n                            self.seq_char_right,\n                            self.seq_len_char_right,\n                            granularity=""char"", reuse=True)\n\n                match_matrix_char = self._get_match_matrix(self.seq_char_left, emb_seq_char_left, enc_seq_char_left,\n                                                           self.seq_char_right, emb_seq_char_right, enc_seq_char_right,\n                                                           granularity=""char"")\n                sim_char = self._mp_semantic_feature_layer(match_matrix_char, self.dpool_index_char,\n                                                             granularity=""char"")\n\n            with tf.name_scope(""matching_features""):\n                matching_features_word = sim_word\n                matching_features_char = sim_char\n\n        return matching_features_word, matching_features_char\n'"
src/models/model_library.py,0,"b'\nfrom models.bcnn import BCNN, ABCNN1, ABCNN2, ABCNN3\nfrom models.decatt import DecAtt\nfrom models.dssm import DSSM, CDSSM, RDSSM\nfrom models.dsmm import DSMM\nfrom models.esim import ESIM\nfrom models.match_pyramid import MatchPyramid, GMatchPyramid\n\n\ndef get_model(model_type):\n    if model_type == ""dssm"":\n        return DSSM\n    elif model_type == ""cdssm"":\n        return CDSSM\n    elif model_type == ""rdssm"":\n        return RDSSM\n    elif model_type == ""match_pyramid"":\n        return MatchPyramid\n    elif model_type == ""g_match_pyramid"":\n        return GMatchPyramid\n    elif model_type == ""dsmm"":\n        return DSMM\n    elif model_type == ""bcnn"":\n        return BCNN\n    elif model_type == ""abcnn1"":\n        return ABCNN1\n    elif model_type == ""abcnn2"":\n        return ABCNN2\n    elif model_type == ""abcnn3"":\n        return ABCNN3\n    elif model_type == ""esim"":\n        return ESIM\n    elif model_type == ""decatt"":\n        return DecAtt\n    else:\n        return DSMM\n'"
src/tf_common/__init__.py,0,b''
src/tf_common/metrics.py,8,"b'\nimport tensorflow as tf\n\n\ndef cosine_similarity(v1, v2, aggregation=True):\n    v1_n = tf.nn.l2_normalize(v1, dim=1)\n    v2_n = tf.nn.l2_normalize(v2, dim=1)\n    if aggregation:\n        s = tf.reduce_sum(v1_n * v2_n, axis=1, keep_dims=True)\n    else:\n        s = v1_n * v2_n\n    return s\n\n\ndef dot_product(v1, v2, aggregation=True):\n    if aggregation:\n        s = tf.reduce_sum(v1 * v2, axis=1, keep_dims=True)\n    else:\n        s = v1 * v2\n    return s\n\n\ndef euclidean_distance(v1, v2, aggregation=True):\n    if aggregation:\n        s = tf.sqrt(tf.reduce_sum(tf.square(v1 - v2), axis=1, keep_dims=True))\n    else:\n        s = tf.abs(v1 - v2)\n    return s\n\n\ndef euclidean_score(v1, v2, aggregation=True):\n    s = euclidean_distance(v1, v2, aggregation)\n    return 1. / (1. + s)\n\n\ndef canberra_score(v1, v2, aggregation=True):\n    if aggregation:\n        s = tf.reduce_sum(tf.abs(v1 - v2) / (v1 + v2), axis=1, keep_dims=True)\n    else:\n        s = tf.abs(v1 - v2) / (v1 + v2)\n    return s'"
src/tf_common/nadam.py,9,"b'\nimport tensorflow as tf\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import resource_variable_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.training import optimizer\nfrom tensorflow.python.training import training_ops\n\n\nclass NadamOptimizer(optimizer.Optimizer):\n    def __init__(self, learning_rate=0.002, beta1=0.9, beta2=0.999, epsilon=1e-8,\n                 schedule_decay=0.004, use_locking=False, name=""Nadam""):\n        super(NadamOptimizer, self).__init__(use_locking, name)\n        self._lr = learning_rate\n        self._beta1 = beta1\n        self._beta2 = beta2\n        self._epsilon = epsilon\n        self._schedule_decay = schedule_decay\n        # momentum cache decay\n        self._momentum_cache_decay = tf.cast(0.96, tf.float32)\n        self._momentum_cache_const = tf.pow(self._momentum_cache_decay, 1. * schedule_decay)\n\n        # Tensor versions of the constructor arguments, created in _prepare().\n        self._lr_t = None\n        self._beta1_t = None\n        self._beta2_t = None\n        self._epsilon_t = None\n        self._schedule_decay_t = None\n\n        # Variables to accumulate the powers of the beta parameters.\n        # Created in _create_slots when we know the variables to optimize.\n        self._beta1_power = None\n        self._beta2_power = None\n        self._iterations = None\n        self._m_schedule = None\n\n        # Created in SparseApply if needed.\n        self._updated_lr = None\n\n\n    def _prepare(self):\n        self._lr_t = ops.convert_to_tensor(self._lr, name=""learning_rate"")\n        self._beta1_t = ops.convert_to_tensor(self._beta1, name=""beta1"")\n        self._beta2_t = ops.convert_to_tensor(self._beta2, name=""beta2"")\n        self._epsilon_t = ops.convert_to_tensor(self._epsilon, name=""epsilon"")\n        self._schedule_decay_t = ops.convert_to_tensor(self._schedule_decay, name=""schedule_decay"")\n\n    def _create_slots(self, var_list):\n        # Create the beta1 and beta2 accumulators on the same device as the first\n        # variable. Sort the var_list to make sure this device is consistent across\n        # workers (these need to go on the same PS, otherwise some updates are\n        # silently ignored).\n        first_var = min(var_list, key=lambda x: x.name)\n\n        create_new = self._iterations is None\n        if not create_new and context.in_graph_mode():\n            create_new = (self._iterations.graph is not first_var.graph)\n\n        if create_new:\n            with ops.colocate_with(first_var):\n                self._beta1_power = variable_scope.variable(self._beta1,\n                                                            name=""beta1_power"",\n                                                            trainable=False)\n                self._beta2_power = variable_scope.variable(self._beta2,\n                                                            name=""beta2_power"",\n                                                            trainable=False)\n                self._iterations = variable_scope.variable(0.,\n                                                           name=""iterations"",\n                                                           trainable=False)\n                self._m_schedule = variable_scope.variable(1.,\n                                                           name=""m_schedule"",\n                                                           trainable=False)\n        # Create slots for the first and second moments.\n        for v in var_list:\n            self._zeros_slot(v, ""m"", self._name)\n            self._zeros_slot(v, ""v"", self._name)\n\n    def _get_momentum_cache(self, schedule_decay_t, t):\n        return tf.pow(self._momentum_cache_decay, t * schedule_decay_t)\n        # return beta1_t * (1. - 0.5 * (tf.pow(self._momentum_cache_decay, t * schedule_decay_t)))\n\n\n    """"""very slow\n    we simply use the nadam update rule without warming momentum schedule\n    def _apply_dense(self, grad, var):\n        t = math_ops.cast(self._iterations, var.dtype.base_dtype) + 1.\n        m_schedule = math_ops.cast(self._m_schedule, var.dtype.base_dtype)\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n        schedule_decay_t = math_ops.cast(self._schedule_decay_t, var.dtype.base_dtype)\n\n        # Due to the recommendations in [2], i.e. warming momentum schedule\n        # see keras Nadam\n        momentum_cache_t = self._get_momentum_cache(beta1_t, schedule_decay_t, t)\n        momentum_cache_t_1 = self._get_momentum_cache(beta1_t, schedule_decay_t, t+1.)\n        m_schedule_new = m_schedule * momentum_cache_t\n        m_schedule_next = m_schedule * momentum_cache_t * momentum_cache_t_1\n\n        # the following equations given in [1]\n        # m_t = beta1 * m + (1 - beta1) * g_t\n        m = self.get_slot(var, ""m"")\n        m_t = state_ops.assign(m, beta1_t * m + (1. - beta1_t) * grad, use_locking=self._use_locking)\n        g_prime = grad / (1. - m_schedule_new)\n        m_t_prime = m_t / (1. - m_schedule_next)\n        m_t_bar = (1. - momentum_cache_t) * g_prime + momentum_cache_t_1 * m_t_prime\n\n        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n        v = self.get_slot(var, ""v"")\n        v_t = state_ops.assign(v, beta2_t * v + (1. - beta2_t) * tf.square(grad), use_locking=self._use_locking)\n        v_t_prime = v_t / (1. - tf.pow(beta2_t, t))\n\n        var_update = state_ops.assign_sub(var,\n                                      lr_t * m_t_bar / (tf.sqrt(v_t_prime) + epsilon_t),\n                                      use_locking=self._use_locking)\n\n        return control_flow_ops.group(*[var_update, m_t, v_t])\n    """"""\n    # nadam update rule without warming momentum schedule\n    def _apply_dense(self, grad, var):\n        m = self.get_slot(var, ""m"")\n        v = self.get_slot(var, ""v"")\n        return training_ops.apply_adam(\n            var,\n            m,\n            v,\n            math_ops.cast(self._beta1_power, var.dtype.base_dtype),\n            math_ops.cast(self._beta2_power, var.dtype.base_dtype),\n            math_ops.cast(self._lr_t, var.dtype.base_dtype),\n            math_ops.cast(self._beta1_t, var.dtype.base_dtype),\n            math_ops.cast(self._beta2_t, var.dtype.base_dtype),\n            math_ops.cast(self._epsilon_t, var.dtype.base_dtype),\n            grad,\n            use_locking=self._use_locking,\n            use_nesterov=True).op\n\n    def _resource_apply_dense(self, grad, var):\n        m = self.get_slot(var, ""m"")\n        v = self.get_slot(var, ""v"")\n        return training_ops.resource_apply_adam(\n            var.handle,\n            m.handle,\n            v.handle,\n            math_ops.cast(self._beta1_power, grad.dtype.base_dtype),\n            math_ops.cast(self._beta2_power, grad.dtype.base_dtype),\n            math_ops.cast(self._lr_t, grad.dtype.base_dtype),\n            math_ops.cast(self._beta1_t, grad.dtype.base_dtype),\n            math_ops.cast(self._beta2_t, grad.dtype.base_dtype),\n            math_ops.cast(self._epsilon_t, grad.dtype.base_dtype),\n            grad,\n            use_locking=self._use_locking,\n            use_nesterov=True)\n\n    # keras Nadam update rule\n    def _apply_sparse(self, grad, var):\n        t = math_ops.cast(self._iterations, var.dtype.base_dtype) + 1.\n        m_schedule = math_ops.cast(self._m_schedule, var.dtype.base_dtype)\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n        schedule_decay_t = math_ops.cast(self._schedule_decay_t, var.dtype.base_dtype)\n\n        # Due to the recommendations in [2], i.e. warming momentum schedule\n        momentum_cache_power = self._get_momentum_cache(schedule_decay_t, t)\n        momentum_cache_t = beta1_t * (1. - 0.5 * momentum_cache_power)\n        momentum_cache_t_1 = beta1_t * (1. - 0.5 * momentum_cache_power * self._momentum_cache_const)\n        m_schedule_new = m_schedule * momentum_cache_t\n        m_schedule_next = m_schedule_new * momentum_cache_t_1\n\n        # the following equations given in [1]\n        # m_t = beta1 * m + (1 - beta1) * g_t\n        m = self.get_slot(var, ""m"")\n        m_t = state_ops.scatter_update(m, grad.indices,\n                                       beta1_t * array_ops.gather(m, grad.indices) +\n                                       (1. - beta1_t) * grad.values,\n                                       use_locking=self._use_locking)\n        g_prime_slice = grad.values / (1. - m_schedule_new)\n        m_t_prime_slice = array_ops.gather(m_t, grad.indices) / (1. - m_schedule_next)\n        m_t_bar_slice = (1. - momentum_cache_t) * g_prime_slice + momentum_cache_t_1 * m_t_prime_slice\n\n        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n        v = self.get_slot(var, ""v"")\n        v_t = state_ops.scatter_update(v, grad.indices,\n                                       beta2_t * array_ops.gather(v, grad.indices) +\n                                       (1. - beta2_t) * tf.square(grad.values),\n                                       use_locking=self._use_locking)\n        v_t_prime_slice = array_ops.gather(v_t, grad.indices) / (1. - tf.pow(beta2_t, t))\n\n        var_update = state_ops.scatter_sub(var, grad.indices,\n                                           lr_t * m_t_bar_slice / (math_ops.sqrt(v_t_prime_slice) + epsilon_t),\n                                           use_locking=self._use_locking)\n\n        return control_flow_ops.group(*[var_update, m_t, v_t])\n\n    def _finish(self, update_ops, name_scope):\n        # Update the power accumulators.\n        with ops.control_dependencies(update_ops):\n            with ops.colocate_with(self._iterations):\n                update_beta1 = self._beta1_power.assign(\n                    self._beta1_power * self._beta1_t,\n                    use_locking=self._use_locking)\n                update_beta2 = self._beta2_power.assign(\n                    self._beta2_power * self._beta2_t,\n                    use_locking=self._use_locking)\n                t = self._iterations + 1.\n                update_iterations = self._iterations.assign(t, use_locking=self._use_locking)\n                momentum_cache_power = self._get_momentum_cache(self._schedule_decay_t, t)\n                momentum_cache_t = self._beta1_t * (1. - 0.5 * momentum_cache_power)\n                update_m_schedule = self._m_schedule.assign(\n                    self._m_schedule * momentum_cache_t,\n                    use_locking=self._use_locking)\n        return control_flow_ops.group(\n            *update_ops + [update_beta1, update_beta2] + [update_iterations, update_m_schedule],\n            name=name_scope)'"
src/tf_common/nn_module.py,178,"b'\nimport numpy as np\nimport tensorflow as tf\n\n""""""\nhttps://explosion.ai/blog/deep-learning-formula-nlp\nembed -> encode -> attend -> predict\n""""""\ndef batch_normalization(x, training, name):\n    # with tf.variable_scope(name, reuse=)\n    bn_train = tf.layers.batch_normalization(x, training=True, reuse=None, name=name)\n    bn_inference = tf.layers.batch_normalization(x, training=False, reuse=True, name=name)\n    z = tf.cond(training, lambda: bn_train, lambda: bn_inference)\n    return z\n\n\n#### Step 1\ndef embed(x, size, dim, seed=0, flatten=False, reduce_sum=False):\n    # std = np.sqrt(2 / dim)\n    std = 0.001\n    minval = -std\n    maxval = std\n    emb = tf.Variable(tf.random_uniform([size, dim], minval, maxval, dtype=tf.float32, seed=seed))\n    # None * max_seq_len * embed_dim\n    out = tf.nn.embedding_lookup(emb, x)\n    if flatten:\n        out = tf.layers.flatten(out)\n    if reduce_sum:\n        out = tf.reduce_sum(out, axis=1)\n    return out\n\n\ndef embed_subword(x, size, dim, sequence_length, seed=0, mask_zero=False, maxlen=None):\n    # std = np.sqrt(2 / dim)\n    std = 0.001\n    minval = -std\n    maxval = std\n    emb = tf.Variable(tf.random_uniform([size, dim], minval, maxval, dtype=tf.float32, seed=seed))\n    # None * max_seq_len * max_word_len * embed_dim\n    out = tf.nn.embedding_lookup(emb, x)\n    if mask_zero:\n        # word_len: None * max_seq_len\n        # mask: shape=None * max_seq_len * max_word_len\n        mask = tf.sequence_mask(sequence_length, maxlen)\n        mask = tf.expand_dims(mask, axis=-1)\n        mask = tf.cast(mask, tf.float32)\n        out = out * mask\n    # None * max_seq_len * embed_dim\n    # according to facebook subword paper, it\'s sum\n    out = tf.reduce_sum(out, axis=2)\n    return out\n\n\ndef word_dropout(x, training, dropout=0, seed=0):\n    # word dropout (dropout the entire embedding for some words)\n    """"""\n    tf.layers.Dropout doesn\'t work as it can\'t switch training or inference\n    """"""\n    if dropout > 0:\n        input_shape = tf.shape(x)\n        noise_shape = [input_shape[0], input_shape[1], 1]\n        x = tf.layers.Dropout(rate=dropout, noise_shape=noise_shape, seed=seed)(x, training=training)\n    return x\n\n\n#### Step 2\ndef fasttext(x):\n    return x\n\n\n# Language Modeling with Gated Convolutional Networks\n# https://github.com/anantzoid/Language-Modeling-GatedCNN\ndef gated_conv1d_op(inputs, filters=8, kernel_size=3, padding=""same"", activation=None, strides=1, reuse=False, name=""""):\n    conv_linear = tf.layers.conv1d(\n        inputs=inputs,\n        filters=filters,\n        kernel_size=kernel_size,\n        padding=""same"",\n        activation=None,\n        strides=strides,\n        reuse=reuse,\n        name=name+""_linear"")\n    conv_gated = tf.layers.conv1d(\n        inputs=inputs,\n        filters=filters,\n        kernel_size=kernel_size,\n        padding=""same"",\n        activation=tf.nn.sigmoid,\n        strides=strides,\n        reuse=reuse,\n        name=name+""_gated"")\n    conv = conv_linear * conv_gated\n    return conv\n\n\ndef residual_gated_conv1d_op(inputs, filters=8, kernel_size=3, padding=""same"", activation=None, strides=1, reuse=False, name=""""):\n    conv_linear = tf.layers.conv1d(\n        inputs=inputs,\n        filters=filters,\n        kernel_size=kernel_size,\n        padding=""same"",\n        activation=None,\n        strides=strides,\n        reuse=reuse,\n        name=name+""_linear"")\n    conv_gated = tf.layers.conv1d(\n        inputs=inputs,\n        filters=filters,\n        kernel_size=kernel_size,\n        padding=""same"",\n        activation=tf.nn.sigmoid,\n        strides=strides,\n        reuse=reuse,\n        name=name+""_gated"")\n    conv = inputs * (1. - conv_gated) + conv_linear * conv_gated\n    return conv\n\n\ndef _textcnn(x, conv_op, num_filters=8, filter_sizes=[2, 3], bn=False, training=False,\n            timedistributed=False, scope_name=""textcnn"", reuse=False, activation=tf.nn.relu):\n    # x: None * step_dim * embed_dim\n    conv_blocks = []\n    for i, filter_size in enumerate(filter_sizes):\n        scope_name_i = ""%s_textcnn_%s""%(str(scope_name), str(filter_size))\n        with tf.variable_scope(scope_name_i, reuse=reuse):\n            if timedistributed:\n                input_shape = tf.shape(x)\n                step_dim = input_shape[1]\n                embed_dim = input_shape[2]\n                x = tf.transpose(x, [0, 2, 1])\n                # None * embed_dim * step_dim\n                x = tf.reshape(x, [input_shape[0] * embed_dim, step_dim, 1])\n                conv = conv_op(\n                    inputs=x,\n                    filters=1,\n                    kernel_size=filter_size,\n                    padding=""same"",\n                    activation=activation,\n                    strides=1,\n                    reuse=reuse,\n                    name=scope_name_i)\n                conv = tf.reshape(conv, [input_shape[0], embed_dim, step_dim])\n                conv = tf.transpose(conv, [0, 2, 1])\n            else:\n                conv = conv_op(\n                    inputs=x,\n                    filters=num_filters,\n                    kernel_size=filter_size,\n                    padding=""same"",\n                    activation=activation,\n                    strides=1,\n                    reuse=reuse,\n                    name=scope_name_i)\n            if bn:\n                conv = tf.layers.BatchNormalization()(conv, training)\n            # conv = activation(conv)\n            conv_blocks.append(conv)\n    if len(conv_blocks) > 1:\n        z = tf.concat(conv_blocks, axis=-1)\n    else:\n        z = conv_blocks[0]\n    return z\n\n\ndef textcnn(x, num_layers=2, num_filters=8, filter_sizes=[2, 3], bn=False, training=False,\n            timedistributed=False, scope_name=""textcnn"", reuse=False, activation=tf.nn.relu,\n            gated_conv=False, residual=False):\n    if gated_conv:\n        if residual:\n            conv_op = residual_gated_conv1d_op\n        else:\n            conv_op = gated_conv1d_op\n    else:\n        conv_op = tf.layers.conv1d\n    conv_blocks = []\n    for i in range(num_layers):\n        scope_name_i = ""%s_textcnn_layer_%s"" % (str(scope_name), str(i))\n        x = _textcnn(x, conv_op, num_filters, filter_sizes, bn, training, timedistributed, scope_name_i, reuse, activation)\n        conv_blocks.append(x)\n    if len(conv_blocks) > 1:\n        z = tf.concat(conv_blocks, axis=-1)\n    else:\n        z = conv_blocks[0]\n    return z\n\n\ndef textrnn(x, num_units, cell_type, sequence_length, num_layers=1, mask_zero=False, scope_name=""textrnn"", reuse=False):\n    for i in range(num_layers):\n        scope_name_i = ""%s_textrnn_%s_%s_%s"" % (str(scope_name), cell_type, str(i), str(num_units))\n        with tf.variable_scope(scope_name_i, reuse=reuse):\n            if cell_type == ""gru"":\n                cell_fw = tf.nn.rnn_cell.GRUCell(num_units)\n            elif cell_type == ""lstm"":\n                cell_fw = tf.nn.rnn_cell.LSTMCell(num_units)\n            if mask_zero:\n                x, _ = tf.nn.dynamic_rnn(cell_fw, x, dtype=tf.float32, sequence_length=sequence_length, scope=scope_name_i)\n            else:\n                x, _ = tf.nn.dynamic_rnn(cell_fw, x, dtype=tf.float32, sequence_length=None, scope=scope_name_i)\n    return x\n\n\ndef textbirnn(x, num_units, cell_type, sequence_length, num_layers=1, mask_zero=False, scope_name=""textbirnn"", reuse=False):\n    for i in range(num_layers):\n        scope_name_i = ""%s_textbirnn_%s_%s_%s"" % (str(scope_name), cell_type, str(i), str(num_units))\n        with tf.variable_scope(scope_name_i, reuse=reuse):\n            if cell_type == ""gru"":\n                cell_fw = tf.nn.rnn_cell.GRUCell(num_units)\n                cell_bw = tf.nn.rnn_cell.GRUCell(num_units)\n            elif cell_type == ""lstm"":\n                cell_fw = tf.nn.rnn_cell.LSTMCell(num_units)\n                cell_bw = tf.nn.rnn_cell.LSTMCell(num_units)\n            if mask_zero:\n                (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(\n                    cell_fw, cell_bw, x, dtype=tf.float32, sequence_length=sequence_length, scope=scope_name_i)\n            else:\n                (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(\n                    cell_fw, cell_bw, x, dtype=tf.float32, sequence_length=None, scope=scope_name_i)\n            x = tf.concat([output_fw, output_bw], axis=-1)\n    return x\n\n\n\ndef encode(x, method, params, input_dim,\n           sequence_length=None, mask_zero=False,\n           scope_name=""encode"", reuse=False,\n           training=False, seed=0):\n    """"""\n    :param x: shape=(None,seqlen,dim)\n    :param params:\n    :return: shape=(None,seqlen,dim)\n    """"""\n    out_list = []\n    params[""encode_dim""] = 0\n    for m in method.split(""+""):\n        if m == ""fasttext"":\n            dim_f = input_dim  # params[""embedding_dim""]\n            z = fasttext(x)\n            out_list.append(z)\n            params[""encode_dim""] += dim_f\n        elif m == ""project"":\n            dim_p = params[""project_hidden_units""][-1]\n            step_dim = tf.shape(x)[1]\n            z = tf.reshape(x, [-1, input_dim])\n            z = mlp_layer(z, fc_type=params[""project_type""],\n                          hidden_units=params[""project_hidden_units""],\n                          dropouts=params[""project_dropouts""],\n                          scope_name=scope_name,\n                          reuse=reuse,\n                          training=training,\n                          seed=params[""random_seed""])\n            z = tf.reshape(z, [-1, step_dim, params[""project_hidden_units""][-1]])\n            out_list.append(z)\n            params[""encode_dim""] += dim_p\n        elif m == ""textcnn"":\n            dim_c = params[""cnn_num_layers""] * len(params[""cnn_filter_sizes""]) * params[""cnn_num_filters""]\n            z = textcnn(x, num_layers=params[""cnn_num_layers""], num_filters=params[""cnn_num_filters""], filter_sizes=params[""cnn_filter_sizes""],\n                        timedistributed=params[""cnn_timedistributed""], scope_name=scope_name, reuse=reuse)\n            out_list.append(z)\n            params[""encode_dim""] += dim_c\n        elif m == ""textrnn"":\n            dim_r = params[""rnn_num_units""]\n            z = textrnn(x, num_units=params[""rnn_num_units""], cell_type=params[""rnn_cell_type""], num_layers=params[""rnn_num_layers""],\n                        sequence_length=sequence_length, mask_zero=mask_zero, scope_name=scope_name, reuse=reuse)\n            out_list.append(z)\n            params[""encode_dim""] += dim_r\n        elif method == ""textbirnn"":\n            dim_b = params[""rnn_num_units""] * 2\n            z = textbirnn(x, num_units=params[""rnn_num_units""], cell_type=params[""rnn_cell_type""], num_layers=params[""rnn_num_layers""],\n                          sequence_length=sequence_length, mask_zero=mask_zero, scope_name=scope_name, reuse=reuse)\n            out_list.append(z)\n            params[""encode_dim""] += dim_b\n    z = tf.concat(out_list, axis=-1)\n    return z\n\n\ndef scalar_attention(x, encode_dim, feature_dim, attention_dim, sequence_length=None,\n                     mask_zero=False, maxlen=None, epsilon=1e-8, seed=0, scope_name=""attention"", reuse=False):\n    """"""\n    :param x: [batchsize, s, feature_dim]\n    :param encode_dim: dim of encoder output\n    :param feature_dim: dim of x (for self-attention, x is the encoder output;\n                        for context-attention, x is the concat of encoder output and contextual info)\n    :param sequence_length:\n    :param mask_zero:\n    :param maxlen:\n    :param epsilon:\n    :param seed:\n    :param scope_name:\n    :param reuse:\n    :return: [batchsize, s, 1]\n    """"""\n    with tf.variable_scope(scope_name, reuse=reuse):\n        # W1: [feature_dim]\n        W1 = tf.get_variable(""W1_%s"" % scope_name,\n                             initializer=tf.truncated_normal_initializer(\n                                 mean=0.0, stddev=0.2, dtype=tf.float32, seed=seed),\n                             dtype=tf.float32,\n                             shape=[feature_dim])\n        # b1: [1]\n        b1 = tf.get_variable(""b1_%s"" % scope_name,\n                             initializer=tf.truncated_normal_initializer(\n                                 mean=0.0, stddev=0.2, dtype=tf.float32, seed=seed),\n                             dtype=tf.float32,\n                             shape=[1])\n    e = tf.einsum(""bsf,f->bs"", x, W1) + \\\n        tf.expand_dims(b1, axis=1)\n    a = tf.exp(e)\n\n    # apply mask after the exp. will be re-normalized next\n    if mask_zero:\n        # None * s\n        mask = tf.sequence_mask(sequence_length, maxlen)\n        mask = tf.cast(mask, tf.float32)\n        a = a * mask\n\n    # in some cases especially in the early stages of training the sum may be almost zero\n    s = tf.reduce_sum(a, axis=1, keep_dims=True)\n    a /= tf.cast(s + epsilon, tf.float32)\n    a = tf.expand_dims(a, axis=-1)\n\n    return a\n\n\n# vector-based attention proposed in the following paper\n# Enhancing Sentence Embedding with Generalized Pooling\ndef vector_attention(x, encode_dim, feature_dim, attention_dim, sequence_length=None,\n                     mask_zero=False, maxlen=None, epsilon=1e-8, seed=0,\n                     scope_name=""attention"", reuse=False):\n    """"""\n    :param x: [batchsize, s, feature_dim]\n    :param encode_dim: dim of encoder output\n    :param feature_dim: dim of x (for self-attention, x is the encoder output;\n                        for context-attention, x is the concat of encoder output and contextual info)\n    :param sequence_length:\n    :param mask_zero:\n    :param maxlen:\n    :param epsilon:\n    :param seed:\n    :param scope_name:\n    :param reuse:\n    :return: [batchsize, s, encode_dim]\n    """"""\n    with tf.variable_scope(scope_name, reuse=reuse):\n        # W1: [attention_dim, feature_dim]\n        W1 = tf.get_variable(""W1_%s"" % scope_name,\n                             initializer=tf.truncated_normal_initializer(\n                                 mean=0.0, stddev=0.2, dtype=tf.float32, seed=seed),\n                             dtype=tf.float32,\n                             shape=[attention_dim, feature_dim])\n        # b1: [attention_dim]\n        b1 = tf.get_variable(""b1_%s"" % scope_name,\n                             initializer=tf.truncated_normal_initializer(\n                                 mean=0.0, stddev=0.2, dtype=tf.float32, seed=seed),\n                             dtype=tf.float32,\n                             shape=[attention_dim])\n        # W2: [encode_dim, attention_dim]\n        W2 = tf.get_variable(""W2_%s"" % scope_name,\n                             initializer=tf.truncated_normal_initializer(\n                                 mean=0.0, stddev=0.2, dtype=tf.float32, seed=seed),\n                             dtype=tf.float32,\n                             shape=[encode_dim, attention_dim])\n        # b2: [encode_dim]\n        b2 = tf.get_variable(""b2_%s"" % scope_name,\n                             initializer=tf.truncated_normal_initializer(\n                                 mean=0.0, stddev=0.2, dtype=tf.float32, seed=seed),\n                             dtype=tf.float32,\n                             shape=[encode_dim])\n    # [batchsize, attention_dim, s]\n    e = tf.nn.relu(\n        tf.einsum(""bsf,af->bas"", x, W1) + \\\n        tf.expand_dims(tf.expand_dims(b1, axis=0), axis=-1))\n    # [batchsize, s, encode_dim]\n    e = tf.einsum(""bas,ea->bse"", e, W2) + \\\n        tf.expand_dims(tf.expand_dims(b2, axis=0), axis=0)\n    a = tf.exp(e)\n\n    # apply mask after the exp. will be re-normalized next\n    if mask_zero:\n        # [batchsize, s, 1]\n        mask = tf.sequence_mask(sequence_length, maxlen)\n        mask = tf.expand_dims(tf.cast(mask, tf.float32), axis=-1)\n        a = a * mask\n\n    # in some cases especially in the early stages of training the sum may be almost zero\n    s = tf.reduce_sum(a, axis=1, keep_dims=True)\n    a /= tf.cast(s + epsilon, tf.float32)\n\n    return a\n\n\ndef _attend(x, sequence_length=None, method=""ave"", context=None, encode_dim=None,\n            feature_dim=None, attention_dim=None, mask_zero=False, maxlen=None,\n           bn=False, training=False, seed=0, scope_name=""attention"", reuse=False,\n            num_heads=1):\n    if method == ""ave"":\n        if mask_zero:\n            # None * step_dim\n            mask = tf.sequence_mask(sequence_length, maxlen)\n            mask = tf.reshape(mask, (-1, tf.shape(x)[1], 1))\n            mask = tf.cast(mask, tf.float32)\n            z = tf.reduce_sum(x * mask, axis=1)\n            l = tf.reduce_sum(mask, axis=1)\n            # in some cases especially in the early stages of training the sum may be almost zero\n            epsilon = 1e-8\n            z /= tf.cast(l + epsilon, tf.float32)\n        else:\n            z = tf.reduce_mean(x, axis=1)\n    elif method == ""sum"":\n        if mask_zero:\n            # None * step_dim\n            mask = tf.sequence_mask(sequence_length, maxlen)\n            mask = tf.reshape(mask, (-1, tf.shape(x)[1], 1))\n            mask = tf.cast(mask, tf.float32)\n            z = tf.reduce_sum(x * mask, axis=1)\n        else:\n            z = tf.reduce_sum(x, axis=1)\n    elif method == ""max"":\n        if mask_zero:\n            # None * step_dim\n            mask = tf.sequence_mask(sequence_length, maxlen)\n            mask = tf.expand_dims(mask, axis=-1)\n            mask = tf.tile(mask, (1, 1, tf.shape(x)[2]))\n            masked_data = tf.where(tf.equal(mask, tf.zeros_like(mask)),\n                                   tf.ones_like(x) * -np.inf, x)  # if masked assume value is -inf\n            z = tf.reduce_max(masked_data, axis=1)\n        else:\n            z = tf.reduce_max(x, axis=1)\n    elif method == ""min"":\n        if mask_zero:\n            # None * step_dim\n            mask = tf.sequence_mask(sequence_length, maxlen)\n            mask = tf.expand_dims(mask, axis=-1)\n            mask = tf.tile(mask, (1, 1, tf.shape(x)[2]))\n            masked_data = tf.where(tf.equal(mask, tf.zeros_like(mask)),\n                                   tf.ones_like(x) * np.inf, x)  # if masked assume value is -inf\n            z = tf.reduce_min(masked_data, axis=1)\n        else:\n            z = tf.reduce_min(x, axis=1)\n    elif ""attention"" in method:\n        if context is not None:\n            y = tf.concat([x, context], axis=-1)\n        else:\n            y = x\n        zs = []\n        for i in range(num_heads):\n            if ""vector"" in method:\n                a = vector_attention(y, encode_dim, feature_dim, attention_dim, sequence_length, mask_zero, maxlen, seed=seed, scope_name=scope_name+str(i), reuse=reuse)\n            else:\n                a = scalar_attention(y, encode_dim, feature_dim, attention_dim, sequence_length, mask_zero, maxlen, seed=seed, scope_name=scope_name+str(i), reuse=reuse)\n            zs.append(tf.reduce_sum(x * a, axis=1))\n        z = tf.concat(zs, axis=-1)\n    if bn:\n        z = tf.layers.BatchNormalization()(z, training=training)\n    return z\n\n\ndef attend(x, sequence_length=None, method=""ave"", context=None, encode_dim=None,\n           feature_dim=None, attention_dim=None, mask_zero=False, maxlen=None,\n           bn=False, training=False, seed=0, scope_name=""attention"", reuse=False,\n           num_heads=1):\n    if isinstance(method, list):\n        outputs = [None]*len(method)\n        for i,m in enumerate(method):\n            outputs[i] = _attend(x, sequence_length, m, context, encode_dim, feature_dim, attention_dim, mask_zero, maxlen,\n                                bn, training, seed, scope_name+m, reuse, num_heads)\n        return tf.concat(outputs, axis=-1)\n    else:\n        return _attend(x, sequence_length, method, context, encode_dim, feature_dim, attention_dim, mask_zero, maxlen,\n                                bn, training, seed, scope_name+method, reuse, num_heads)\n\n\n#### Step 4\ndef _dense_block_mode1(x, hidden_units, dropouts, densenet=False, scope_name=""dense_block"", reuse=False, training=False, seed=0, bn=False):\n    """"""\n    :param x:\n    :param hidden_units:\n    :param dropouts:\n    :param densenet: enable densenet\n    :return:\n    Ref: https://github.com/titu1994/DenseNet\n    """"""\n    for i, (h, d) in enumerate(zip(hidden_units, dropouts)):\n        scope_name_i = ""%s-dense_block_mode1-%s""%(str(scope_name), str(i))\n        with tf.variable_scope(scope_name, reuse=reuse):\n            z = tf.layers.dense(x, h, kernel_initializer=tf.glorot_uniform_initializer(seed=seed * i),\n                                  reuse=reuse,\n                                  name=scope_name_i)\n            if bn:\n                z = batch_normalization(z, training=training, name=scope_name_i+""-bn"")\n            z = tf.nn.relu(z)\n            z = tf.layers.Dropout(d, seed=seed * i)(z, training=training) if d > 0 else z\n            if densenet:\n                x = tf.concat([x, z], axis=-1)\n            else:\n                x = z\n    return x\n\n\ndef _dense_block_mode2(x, hidden_units, dropouts, densenet=False, training=False, seed=0, bn=False, name=""dense_block""):\n    """"""\n    :param x:\n    :param hidden_units:\n    :param dropouts:\n    :param densenet: enable densenet\n    :return:\n    Ref: https://github.com/titu1994/DenseNet\n    """"""\n    for i, (h, d) in enumerate(zip(hidden_units, dropouts)):\n        if bn:\n            z = batch_normalization(x, training=training, name=name + ""-"" + str(i))\n        z = tf.nn.relu(z)\n        z = tf.layers.Dropout(d, seed=seed * i)(z, training=training) if d > 0 else z\n        z = tf.layers.Dense(h, kernel_initializer=tf.glorot_uniform_initializer(seed=seed * i), dtype=tf.float32,\n                            bias_initializer=tf.zeros_initializer())(z)\n        if densenet:\n            x = tf.concat([x, z], axis=-1)\n        else:\n            x = z\n    return x\n\n\ndef dense_block(x, hidden_units, dropouts, densenet=False, scope_name=""dense_block"", reuse=False, training=False, seed=0, bn=False):\n    return _dense_block_mode1(x, hidden_units, dropouts, densenet, scope_name, reuse, training, seed, bn)\n\n\ndef _resnet_branch_mode1(x, hidden_units, dropouts, training, seed=0):\n    h1, h2, h3 = hidden_units\n    dr1, dr2, dr3 = dropouts\n    name = ""resnet_block""\n    # branch 2\n    x2 = tf.layers.Dense(h1, kernel_initializer=tf.glorot_uniform_initializer(seed=seed * 2), dtype=tf.float32,\n                         bias_initializer=tf.zeros_initializer())(x)\n    x2 = tf.layers.BatchNormalization()(x2, training=training)\n    # x2 = batch_normalization(x2, training=training, name=name + ""-"" + str(1))\n    x2 = tf.nn.relu(x2)\n    x2 = tf.layers.Dropout(dr1, seed=seed * 1)(x2, training=training) if dr1 > 0 else x2\n\n    x2 = tf.layers.Dense(h2, kernel_initializer=tf.glorot_uniform_initializer(seed=seed * 3), dtype=tf.float32,\n                         bias_initializer=tf.zeros_initializer())(x2)\n    x2 = tf.layers.BatchNormalization()(x2, training=training)\n    # x2 = batch_normalization(x2, training=training, name=name + ""-"" + str(2))\n    x2 = tf.nn.relu(x2)\n    x2 = tf.layers.Dropout(dr2, seed=seed * 2)(x2, training=training) if dr2 > 0 else x2\n\n    x2 = tf.layers.Dense(h3, kernel_initializer=tf.glorot_uniform_initializer(seed=seed * 4), dtype=tf.float32,\n                         bias_initializer=tf.zeros_initializer())(x2)\n    x2 = tf.layers.BatchNormalization()(x2, training=training)\n    # x2 = batch_normalization(x2, training=training, name=name + ""-"" + str(3))\n\n    return x2\n\n\ndef _resnet_block_mode1(x, hidden_units, dropouts, cardinality=1, dense_shortcut=False, training=False, seed=0):\n    """"""A block that has a dense layer at shortcut.\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: default 3, the kernel size of middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: \'a\',\'b\'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    Note that from stage 3, the first conv layer at main path is with strides=(2,2)\n    And the shortcut should have strides=(2,2) as well\n    """"""\n    h1, h2, h3 = hidden_units\n    dr1, dr2, dr3 = dropouts\n    name = ""resnet_block""\n    xs = []\n    # branch 0\n    if dense_shortcut:\n        x0 = tf.layers.Dense(h3, kernel_initializer=tf.glorot_uniform_initializer(seed=seed * 1), dtype=tf.float32,\n                             bias_initializer=tf.zeros_initializer())(x)\n        x0 = tf.layers.BatchNormalization()(x0, training=training)\n        # x0 = batch_normalization(x0, training=training, name=name + ""-"" + str(0))\n        xs.append(x0)\n    else:\n        xs.append(x)\n\n    # branch 1 ~ cardinality\n    for i in range(cardinality):\n        xs.append(_resnet_branch_mode1(x, hidden_units, dropouts, training, seed))\n\n    x = tf.add_n(xs)\n    x = tf.nn.relu(x)\n    x = tf.layers.Dropout(dr3, seed=seed * 4)(x, training=training) if dr3 > 0 else x\n    return x\n\n\ndef _resnet_branch_mode2(x, hidden_units, dropouts, training=False, seed=0, scope_name=""_resnet_branch_mode2"", reuse=False):\n    h1, h2, h3 = hidden_units\n    dr1, dr2, dr3 = dropouts\n    # name = ""resnet""\n    with tf.variable_scope(scope_name, reuse=reuse):\n        # branch 2: bn-relu->weight\n        x2 = tf.layers.BatchNormalization()(x)\n        # x2 = batch_normalization(x, training=training, name=scope_name + ""-bn-"" + str(1))\n        x2 = tf.nn.relu(x2)\n        x2 = tf.layers.Dropout(dr1)(x2, training=training) if dr1 > 0 else x2\n        x2 = tf.layers.dense(x2, h1, kernel_initializer=tf.glorot_uniform_initializer(seed * 1),\n                             bias_initializer=tf.zeros_initializer(),\n                             name=scope_name+""-dense-""+str(1),\n                             reuse=reuse)\n\n        x2 = tf.layers.BatchNormalization()(x2)\n        # x2 = batch_normalization(x2, training=training, name=scope_name + ""-bn-"" + str(2))\n        x2 = tf.nn.relu(x2)\n        x2 = tf.layers.Dropout(dr2)(x2, training=training) if dr2 > 0 else x2\n        x2 = tf.layers.dense(x2, h2, kernel_initializer=tf.glorot_uniform_initializer(seed * 2),\n                             bias_initializer=tf.zeros_initializer(),\n                             name=scope_name + ""-dense-"" + str(2),\n                             reuse=reuse)\n\n        x2 = tf.layers.BatchNormalization()(x2)\n        # x2 = batch_normalization(x2, training=training, name=scope_name + ""-bn-"" + str(3))\n        x2 = tf.nn.relu(x2)\n        x2 = tf.layers.Dropout(dr3)(x2, training=training) if dr3 > 0 else x2\n        x2 = tf.layers.dense(x2, h3, kernel_initializer=tf.glorot_uniform_initializer(seed * 3),\n                             bias_initializer=tf.zeros_initializer(),\n                             name=scope_name + ""-dense-"" + str(3),\n                             reuse=reuse)\n\n    return x2\n\n\ndef _resnet_block_mode2(x, hidden_units, dropouts, cardinality=1, dense_shortcut=False, training=False, seed=0,\n                        scope_name=""_resnet_block_mode2"", reuse=False):\n    """"""A block that has a dense layer at shortcut.\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: default 3, the kernel size of middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: \'a\',\'b\'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    Note that from stage 3, the first conv layer at main path is with strides=(2,2)\n    And the shortcut should have strides=(2,2) as well\n    """"""\n    h1, h2, h3 = hidden_units\n    dr1, dr2, dr3 = dropouts\n\n    xs = []\n    # branch 0\n    if dense_shortcut:\n        with tf.variable_scope(scope_name, reuse=reuse):\n            x0 = tf.layers.dense(x, h3, kernel_initializer=tf.glorot_uniform_initializer(seed * 1),\n                                 bias_initializer=tf.zeros_initializer(),\n                                 reuse=reuse,\n                                 name=scope_name+""-dense-""+str(""0""))\n        xs.append(x0)\n    else:\n        xs.append(x)\n\n    # branch 1 ~ cardinality\n    for i in range(cardinality):\n        xs.append(_resnet_branch_mode2(x, hidden_units, dropouts, training, seed, scope_name, reuse))\n\n    x = tf.add_n(xs)\n    return x\n\n\ndef resnet_block(input_tensor, hidden_units, dropouts, cardinality=1, dense_shortcut=False, training=False, seed=0,\n                 scope_name=""resnet_block"", reuse=False):\n    return _resnet_block_mode2(input_tensor, hidden_units, dropouts, cardinality, dense_shortcut, training, seed,\n                               scope_name, reuse)\n\n\ndef mlp_layer(input, fc_type, hidden_units, dropouts, scope_name, reuse=False, training=False, seed=0):\n    if fc_type == ""fc"":\n        output = dense_block(input, hidden_units=hidden_units, dropouts=dropouts,\n                                         densenet=False, scope_name=scope_name,\n                                         reuse=reuse,\n                                         training=training, seed=seed)\n    elif fc_type == ""densenet"":\n        output = dense_block(input, hidden_units=hidden_units, dropouts=dropouts,\n                                         densenet=True, scope_name=scope_name,\n                                         reuse=reuse,\n                                         training=training, seed=seed)\n    elif fc_type == ""resnet"":\n        output = resnet_block(input, hidden_units=hidden_units, dropouts=dropouts,\n                                          cardinality=1, dense_shortcut=True, training=training,\n                                          reuse=reuse,\n                                          seed=seed,\n                                          scope_name=scope_name)\n    return output\n'"
src/tf_common/optimizer.py,31,"b'\n""""""\nhttps://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwih7-6VlejYAhWGS98KHWeLCWQQFgg3MAE&url=https%3A%2F%2Fwww.bigdatarepublic.nl%2Fcustom-optimizer-in-tensorflow%2F&usg=AOvVaw3jmxRDqr2pkGRLvX6rNJrl\n""""""\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.training import optimizer\nfrom tensorflow.python.training import training_ops\n\n\nclass LazyPowerSignOptimizer(optimizer.Optimizer):\n    """"""Implementation of PowerSign.\n    See [Bello et. al., 2017](https://arxiv.org/abs/1709.07417)\n    @@__init__\n    """"""\n\n    def __init__(self, learning_rate=0.001, alpha=0.01, beta=0.5, use_locking=False, name=""PowerSign""):\n        super(LazyPowerSignOptimizer, self).__init__(use_locking, name)\n        self._lr = learning_rate\n        self._alpha = alpha\n        self._beta = beta\n\n        # Tensor versions of the constructor arguments, created in _prepare().\n        self._lr_t = None\n        self._alpha_t = None\n        self._beta_t = None\n\n    def _prepare(self):\n        self._lr_t = ops.convert_to_tensor(self._lr, name=""learning_rate"")\n        self._alpha_t = ops.convert_to_tensor(self._beta, name=""alpha_t"")\n        self._beta_t = ops.convert_to_tensor(self._beta, name=""beta_t"")\n\n    def _create_slots(self, var_list):\n        # Create slots for the first and second moments.\n        for v in var_list:\n            self._zeros_slot(v, ""m"", self._name)\n\n    def _apply_dense(self, grad, var):\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n        alpha_t = math_ops.cast(self._alpha_t, var.dtype.base_dtype)\n        beta_t = math_ops.cast(self._beta_t, var.dtype.base_dtype)\n\n        eps = 1e-7  # cap for moving average\n\n        m = self.get_slot(var, ""m"")\n        m_t = m.assign(tf.maximum(beta_t * m + eps, tf.abs(grad)))\n\n        var_update = state_ops.assign_sub(var, lr_t * grad * tf.exp(\n            tf.log(alpha_t) * tf.sign(grad) * tf.sign(m_t)))  # Update \'ref\' by subtracting \'value\n        # Create an op that groups multiple operations.\n        # When this op finishes, all ops in input have finished\n        return control_flow_ops.group(*[var_update, m_t])\n\n    def _apply_sparse(self, grad, var):\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n        alpha_t = math_ops.cast(self._alpha_t, var.dtype.base_dtype)\n        beta_t = math_ops.cast(self._beta_t, var.dtype.base_dtype)\n\n        eps = 1e-7  # cap for moving average\n\n        m = self.get_slot(var, ""m"")\n        m_slice = tf.gather(m, grad.indices)\n        m_t = state_ops.scatter_update(m, grad.indices,\n                                       tf.maximum(beta_t * m_slice + eps, tf.abs(grad.values)))\n        m_t_slice = tf.gather(m_t, grad.indices)\n\n        var_update = state_ops.scatter_sub(var, grad.indices, lr_t * grad.values * tf.exp(\n            tf.log(alpha_t) * tf.sign(grad.values) * tf.sign(m_t_slice)))  # Update \'ref\' by subtracting \'value\n        # Create an op that groups multiple operations.\n        # When this op finishes, all ops in input have finished\n        return control_flow_ops.group(*[var_update, m_t])\n\n\nclass LazyAddSignOptimizer(optimizer.Optimizer):\n    """"""Implementation of AddSign.\n    See [Bello et. al., 2017](https://arxiv.org/abs/1709.07417)\n    @@__init__\n    """"""\n\n    def __init__(self, learning_rate=1.001, alpha=0.01, beta=0.5, use_locking=False, name=""AddSign""):\n        super(LazyAddSignOptimizer, self).__init__(use_locking, name)\n        self._lr = learning_rate\n        self._alpha = alpha\n        self._beta = beta\n\n        # Tensor versions of the constructor arguments, created in _prepare().\n        self._lr_t = None\n        self._alpha_t = None\n        self._beta_t = None\n\n    def _prepare(self):\n        self._lr_t = ops.convert_to_tensor(self._lr, name=""learning_rate"")\n        self._alpha_t = ops.convert_to_tensor(self._beta, name=""beta_t"")\n        self._beta_t = ops.convert_to_tensor(self._beta, name=""beta_t"")\n\n    def _create_slots(self, var_list):\n        # Create slots for the first and second moments.\n        for v in var_list:\n            self._zeros_slot(v, ""m"", self._name)\n\n    def _apply_dense(self, grad, var):\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n        beta_t = math_ops.cast(self._beta_t, var.dtype.base_dtype)\n        alpha_t = math_ops.cast(self._alpha_t, var.dtype.base_dtype)\n\n        eps = 1e-7  # cap for moving average\n\n        m = self.get_slot(var, ""m"")\n        m_t = m.assign(tf.maximum(beta_t * m + eps, tf.abs(grad)))\n\n        var_update = state_ops.assign_sub(var, lr_t * grad * (1.0 + alpha_t * tf.sign(grad) * tf.sign(m_t)))\n        # Create an op that groups multiple operations\n        # When this op finishes, all ops in input have finished\n        return control_flow_ops.group(*[var_update, m_t])\n\n    def _apply_sparse(self, grad, var):\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n        beta_t = math_ops.cast(self._beta_t, var.dtype.base_dtype)\n        alpha_t = math_ops.cast(self._alpha_t, var.dtype.base_dtype)\n\n        eps = 1e-7  # cap for moving average\n\n        m = self.get_slot(var, ""m"")\n        m_slice = tf.gather(m, grad.indices)\n        m_t = state_ops.scatter_update(m, grad.indices,\n                                       tf.maximum(beta_t * m_slice + eps, tf.abs(grad.values)))\n        m_t_slice = tf.gather(m_t, grad.indices)\n\n        var_update = state_ops.scatter_sub(var, grad.indices,\n                                           lr_t * grad.values * (\n                                                   1.0 + alpha_t * tf.sign(grad.values) * tf.sign(m_t_slice)))\n\n        # Create an op that groups multiple operations\n        # When this op finishes, all ops in input have finished\n        return control_flow_ops.group(*[var_update, m_t])\n\n\nclass LazyAMSGradOptimizer(optimizer.Optimizer):\n    def __init__(self, learning_rate=0.002, beta1=0.9, beta2=0.999, epsilon=1e-8,\n                 use_locking=False, name=""AMSGrad""):\n        super(LazyAMSGradOptimizer, self).__init__(use_locking, name)\n        self._lr = learning_rate\n        self._beta1 = beta1\n        self._beta2 = beta2\n        self._epsilon = epsilon\n\n        # Tensor versions of the constructor arguments, created in _prepare().\n        self._lr_t = None\n        self._beta1_t = None\n        self._beta2_t = None\n        self._epsilon_t = None\n\n    def _prepare(self):\n        self._lr_t = ops.convert_to_tensor(self._lr, name=""learning_rate"")\n        self._beta1_t = ops.convert_to_tensor(self._beta1, name=""beta1"")\n        self._beta2_t = ops.convert_to_tensor(self._beta2, name=""beta2"")\n        self._epsilon_t = ops.convert_to_tensor(self._epsilon, name=""epsilon"")\n\n    def _create_slots(self, var_list):\n        # Create slots for the first and second moments.\n        for v in var_list:\n            self._zeros_slot(v, ""m"", self._name)\n            self._zeros_slot(v, ""v"", self._name)\n            self._zeros_slot(v, ""v_prime"", self._name)\n\n    def _apply_dense(self, grad, var):\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n\n        # the following equations given in [1]\n        # m_t = beta1 * m + (1 - beta1) * g_t\n        m = self.get_slot(var, ""m"")\n        m_t = state_ops.assign(m, beta1_t * m + (1. - beta1_t) * grad, use_locking=self._use_locking)\n\n        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n        v = self.get_slot(var, ""v"")\n        v_t = state_ops.assign(v, beta2_t * v + (1. - beta2_t) * tf.square(grad), use_locking=self._use_locking)\n        v_prime = self.get_slot(var, ""v_prime"")\n        v_t_prime = state_ops.assign(v_prime, tf.maximum(v_prime, v_t))\n\n        var_update = state_ops.assign_sub(var,\n                                          lr_t * m_t / (tf.sqrt(v_t_prime) + epsilon_t),\n                                          use_locking=self._use_locking)\n\n        return control_flow_ops.group(*[var_update, m_t, v_t, v_t_prime])\n\n    # keras Nadam update rule\n    def _apply_sparse(self, grad, var):\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n\n        # the following equations given in [1]\n        # m_t = beta1 * m + (1 - beta1) * g_t\n        m = self.get_slot(var, ""m"")\n        m_t = state_ops.scatter_update(m, grad.indices,\n                                       beta1_t * array_ops.gather(m, grad.indices) +\n                                       (1. - beta1_t) * grad.values,\n                                       use_locking=self._use_locking)\n        m_t_slice = tf.gather(m_t, grad.indices)\n\n        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n        v = self.get_slot(var, ""v"")\n        v_t = state_ops.scatter_update(v, grad.indices,\n                                       beta2_t * array_ops.gather(v, grad.indices) +\n                                       (1. - beta2_t) * tf.square(grad.values),\n                                       use_locking=self._use_locking)\n        v_prime = self.get_slot(var, ""v_prime"")\n        v_t_slice = tf.gather(v_t, grad.indices)\n        v_prime_slice = tf.gather(v_prime, grad.indices)\n        v_t_prime = state_ops.scatter_update(v_prime, grad.indices, tf.maximum(v_prime_slice, v_t_slice))\n\n        v_t_prime_slice = array_ops.gather(v_t_prime, grad.indices)\n        var_update = state_ops.scatter_sub(var, grad.indices,\n                                           lr_t * m_t_slice / (math_ops.sqrt(v_t_prime_slice) + epsilon_t),\n                                           use_locking=self._use_locking)\n\n        return control_flow_ops.group(*[var_update, m_t, v_t, v_t_prime])\n\n\nclass LazyNadamOptimizer(optimizer.Optimizer):\n    def __init__(self, learning_rate=0.002, beta1=0.9, beta2=0.999, epsilon=1e-8,\n                 schedule_decay=0.004, use_locking=False, name=""Nadam""):\n        super(LazyNadamOptimizer, self).__init__(use_locking, name)\n        self._lr = learning_rate\n        self._beta1 = beta1\n        self._beta2 = beta2\n        self._epsilon = epsilon\n        self._schedule_decay = schedule_decay\n        # momentum cache decay\n        self._momentum_cache_decay = tf.cast(0.96, tf.float32)\n        self._momentum_cache_const = tf.pow(self._momentum_cache_decay, 1. * schedule_decay)\n\n        # Tensor versions of the constructor arguments, created in _prepare().\n        self._lr_t = None\n        self._beta1_t = None\n        self._beta2_t = None\n        self._epsilon_t = None\n        self._schedule_decay_t = None\n\n        # Variables to accumulate the powers of the beta parameters.\n        # Created in _create_slots when we know the variables to optimize.\n        self._beta1_power = None\n        self._beta2_power = None\n        self._iterations = None\n        self._m_schedule = None\n\n        # Created in SparseApply if needed.\n        self._updated_lr = None\n\n    def _prepare(self):\n        self._lr_t = ops.convert_to_tensor(self._lr, name=""learning_rate"")\n        self._beta1_t = ops.convert_to_tensor(self._beta1, name=""beta1"")\n        self._beta2_t = ops.convert_to_tensor(self._beta2, name=""beta2"")\n        self._epsilon_t = ops.convert_to_tensor(self._epsilon, name=""epsilon"")\n        self._schedule_decay_t = ops.convert_to_tensor(self._schedule_decay, name=""schedule_decay"")\n\n    def _create_slots(self, var_list):\n        # Create the beta1 and beta2 accumulators on the same device as the first\n        # variable. Sort the var_list to make sure this device is consistent across\n        # workers (these need to go on the same PS, otherwise some updates are\n        # silently ignored).\n        first_var = min(var_list, key=lambda x: x.name)\n\n        create_new = self._iterations is None\n        if not create_new and context.in_graph_mode():\n            create_new = (self._iterations.graph is not first_var.graph)\n\n        if create_new:\n            with ops.colocate_with(first_var):\n                self._beta1_power = variable_scope.variable(self._beta1,\n                                                            name=""beta1_power"",\n                                                            trainable=False)\n                self._beta2_power = variable_scope.variable(self._beta2,\n                                                            name=""beta2_power"",\n                                                            trainable=False)\n                self._iterations = variable_scope.variable(0.,\n                                                           name=""iterations"",\n                                                           trainable=False)\n                self._m_schedule = variable_scope.variable(1.,\n                                                           name=""m_schedule"",\n                                                           trainable=False)\n        # Create slots for the first and second moments.\n        for v in var_list:\n            self._zeros_slot(v, ""m"", self._name)\n            self._zeros_slot(v, ""v"", self._name)\n\n    def _get_momentum_cache(self, schedule_decay_t, t):\n        return tf.pow(self._momentum_cache_decay, t * schedule_decay_t)\n        # return beta1_t * (1. - 0.5 * (tf.pow(self._momentum_cache_decay, t * schedule_decay_t)))\n\n    """"""very slow\n    we simply use the nadam update rule without warming momentum schedule\n    def _apply_dense(self, grad, var):\n        t = math_ops.cast(self._iterations, var.dtype.base_dtype) + 1.\n        m_schedule = math_ops.cast(self._m_schedule, var.dtype.base_dtype)\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n        schedule_decay_t = math_ops.cast(self._schedule_decay_t, var.dtype.base_dtype)\n\n        # Due to the recommendations in [2], i.e. warming momentum schedule\n        # see keras Nadam\n        momentum_cache_t = self._get_momentum_cache(beta1_t, schedule_decay_t, t)\n        momentum_cache_t_1 = self._get_momentum_cache(beta1_t, schedule_decay_t, t+1.)\n        m_schedule_new = m_schedule * momentum_cache_t\n        m_schedule_next = m_schedule * momentum_cache_t * momentum_cache_t_1\n\n        # the following equations given in [1]\n        # m_t = beta1 * m + (1 - beta1) * g_t\n        m = self.get_slot(var, ""m"")\n        m_t = state_ops.assign(m, beta1_t * m + (1. - beta1_t) * grad, use_locking=self._use_locking)\n        g_prime = grad / (1. - m_schedule_new)\n        m_t_prime = m_t / (1. - m_schedule_next)\n        m_t_bar = (1. - momentum_cache_t) * g_prime + momentum_cache_t_1 * m_t_prime\n\n        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n        v = self.get_slot(var, ""v"")\n        v_t = state_ops.assign(v, beta2_t * v + (1. - beta2_t) * tf.square(grad), use_locking=self._use_locking)\n        v_t_prime = v_t / (1. - tf.pow(beta2_t, t))\n\n        var_update = state_ops.assign_sub(var,\n                                      lr_t * m_t_bar / (tf.sqrt(v_t_prime) + epsilon_t),\n                                      use_locking=self._use_locking)\n\n        return control_flow_ops.group(*[var_update, m_t, v_t])\n    """"""\n\n    # nadam update rule without warming momentum schedule\n    def _apply_dense(self, grad, var):\n        m = self.get_slot(var, ""m"")\n        v = self.get_slot(var, ""v"")\n        return training_ops.apply_adam(\n            var,\n            m,\n            v,\n            math_ops.cast(self._beta1_power, var.dtype.base_dtype),\n            math_ops.cast(self._beta2_power, var.dtype.base_dtype),\n            math_ops.cast(self._lr_t, var.dtype.base_dtype),\n            math_ops.cast(self._beta1_t, var.dtype.base_dtype),\n            math_ops.cast(self._beta2_t, var.dtype.base_dtype),\n            math_ops.cast(self._epsilon_t, var.dtype.base_dtype),\n            grad,\n            use_locking=self._use_locking,\n            use_nesterov=True).op\n\n    def _resource_apply_dense(self, grad, var):\n        m = self.get_slot(var, ""m"")\n        v = self.get_slot(var, ""v"")\n        return training_ops.resource_apply_adam(\n            var.handle,\n            m.handle,\n            v.handle,\n            math_ops.cast(self._beta1_power, grad.dtype.base_dtype),\n            math_ops.cast(self._beta2_power, grad.dtype.base_dtype),\n            math_ops.cast(self._lr_t, grad.dtype.base_dtype),\n            math_ops.cast(self._beta1_t, grad.dtype.base_dtype),\n            math_ops.cast(self._beta2_t, grad.dtype.base_dtype),\n            math_ops.cast(self._epsilon_t, grad.dtype.base_dtype),\n            grad,\n            use_locking=self._use_locking,\n            use_nesterov=True)\n\n    # keras Nadam update rule\n    def _apply_sparse(self, grad, var):\n        t = math_ops.cast(self._iterations, var.dtype.base_dtype) + 1.\n        m_schedule = math_ops.cast(self._m_schedule, var.dtype.base_dtype)\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n        schedule_decay_t = math_ops.cast(self._schedule_decay_t, var.dtype.base_dtype)\n\n        # Due to the recommendations in [2], i.e. warming momentum schedule\n        momentum_cache_power = self._get_momentum_cache(schedule_decay_t, t)\n        momentum_cache_t = beta1_t * (1. - 0.5 * momentum_cache_power)\n        momentum_cache_t_1 = beta1_t * (1. - 0.5 * momentum_cache_power * self._momentum_cache_const)\n        m_schedule_new = m_schedule * momentum_cache_t\n        m_schedule_next = m_schedule_new * momentum_cache_t_1\n\n        # the following equations given in [1]\n        # m_t = beta1 * m + (1 - beta1) * g_t\n        m = self.get_slot(var, ""m"")\n        m_t = state_ops.scatter_update(m, grad.indices,\n                                       beta1_t * array_ops.gather(m, grad.indices) +\n                                       (1. - beta1_t) * grad.values,\n                                       use_locking=self._use_locking)\n        g_prime_slice = grad.values / (1. - m_schedule_new)\n        m_t_prime_slice = array_ops.gather(m_t, grad.indices) / (1. - m_schedule_next)\n        m_t_bar_slice = (1. - momentum_cache_t) * g_prime_slice + momentum_cache_t_1 * m_t_prime_slice\n\n        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n        v = self.get_slot(var, ""v"")\n        v_t = state_ops.scatter_update(v, grad.indices,\n                                       beta2_t * array_ops.gather(v, grad.indices) +\n                                       (1. - beta2_t) * tf.square(grad.values),\n                                       use_locking=self._use_locking)\n        v_t_prime_slice = array_ops.gather(v_t, grad.indices) / (1. - tf.pow(beta2_t, t))\n\n        var_update = state_ops.scatter_sub(var, grad.indices,\n                                           lr_t * m_t_bar_slice / (math_ops.sqrt(v_t_prime_slice) + epsilon_t),\n                                           use_locking=self._use_locking)\n\n        return control_flow_ops.group(*[var_update, m_t, v_t])\n\n    def _finish(self, update_ops, name_scope):\n        # Update the power accumulators.\n        with ops.control_dependencies(update_ops):\n            with ops.colocate_with(self._iterations):\n                update_beta1 = self._beta1_power.assign(\n                    self._beta1_power * self._beta1_t,\n                    use_locking=self._use_locking)\n                update_beta2 = self._beta2_power.assign(\n                    self._beta2_power * self._beta2_t,\n                    use_locking=self._use_locking)\n                t = self._iterations + 1.\n                update_iterations = self._iterations.assign(t, use_locking=self._use_locking)\n                momentum_cache_power = self._get_momentum_cache(self._schedule_decay_t, t)\n                momentum_cache_t = self._beta1_t * (1. - 0.5 * momentum_cache_power)\n                update_m_schedule = self._m_schedule.assign(\n                    self._m_schedule * momentum_cache_t,\n                    use_locking=self._use_locking)\n        return control_flow_ops.group(\n            *update_ops + [update_beta1, update_beta2] + [update_iterations, update_m_schedule],\n            name=name_scope)'"
src/utils/__init__.py,0,b''
src/utils/dist_utils.py,0,"b'# -*- coding: utf-8 -*-\n""""""\n@author: Chenglong Chen <c.chenglong@gmail.com>\n@brief: utils for distance computation\n""""""\n\nimport warnings\nwarnings.filterwarnings(""ignore"")\nimport numpy as np\ntry:\n    import lzma\n    import Levenshtein\nexcept:\n    pass\nfrom difflib import SequenceMatcher\nfrom rouge import Rouge\nfrom utils import ngram_utils, np_utils\n\n\ndef _edit_dist(str1, str2):\n    try:\n        # very fast\n        # http://stackoverflow.com/questions/14260126/how-python-levenshtein-ratio-is-computed\n        # d = Levenshtein.ratio(str1, str2)\n        d = Levenshtein.distance(str1, str2)/float(max(len(str1),len(str2)))\n    except:\n        # https://docs.python.org/2/library/difflib.html\n        d = 1. - SequenceMatcher(lambda x: x=="" "", str1, str2).ratio()\n    return d\n\n\ndef _longest_match_size(str1, str2):\n    sq = SequenceMatcher(lambda x: x=="" "", str1, str2)\n    match = sq.find_longest_match(0, len(str1), 0, len(str2))\n    return match.size\n\n\ndef _longest_match_ratio(str1, str2):\n    sq = SequenceMatcher(lambda x: x=="" "", str1, str2)\n    match = sq.find_longest_match(0, len(str1), 0, len(str2))\n    return np_utils._try_divide(match.size, min(len(str1), len(str2)))\n\n\ndef _common_num(s1, s2):\n    c = 0\n    for s1_ in s1:\n        for s2_ in s2:\n            if s1_ == s2_:\n                c += 1\n    return c\n\n\ndef _count_stats(s1, s2):\n    # length\n    l1 = len(s1)\n    l2 = len(s2)\n    len_diff = np_utils._try_divide(np.abs(l1-l2), (l1+l2)/2.)\n\n    # set\n    s1_set = set(s1)\n    s2_set = set(s2)\n\n    # unique length\n    l1_unique = len(s1_set)\n    l2_unique = len(s2_set)\n    len_diff_unique = np_utils._try_divide(np.abs(l1_unique-l2_unique), (l1_unique+l2_unique)/2.)\n\n    # unique ratio\n    r1_unique = np_utils._try_divide(l1_unique, l1)\n    r2_unique = np_utils._try_divide(l2_unique, l2)\n\n    # jaccard coef\n    li = len(s1_set.intersection(s2_set))\n    lu = len(s1_set.union(s2_set))\n    jaccard_coef = np_utils._try_divide(li, lu)\n\n    # dice coef\n    dice_coef = np_utils._try_divide(li, l1_unique + l2_unique)\n\n    # common number\n    common_ = _common_num(s1, s2)\n    common_ratio_avg = np_utils._try_divide(common_, (l1 + l2) / 2.)\n    common_ratio_max = np_utils._try_divide(common_, min(l1, l2))\n    common_ratio_min = np_utils._try_divide(common_, max(l1, l2))\n\n    # over all features\n    f = [l1, l2, len_diff,\n         l1_unique, l2_unique, len_diff_unique,\n         r1_unique, r2_unique,\n         li, lu, jaccard_coef, dice_coef,\n         common_, common_ratio_avg, common_ratio_max, common_ratio_min\n    ]\n    return np.array(f, dtype=np.float32)\n\n\nrouge = Rouge()\ndef _get_rouge_feat(s1, s2):\n    if isinstance(s1, list):\n        s1 = "" "".join(s1)\n    if isinstance(s2, list):\n        s2 = "" "".join(s2)\n    scores = rouge.get_scores(s1, s2)\n    feat = []\n    for k,v in scores[0].items():\n        feat.extend(v.values())\n    return np.array(feat, dtype=np.float32)\n\n\ndef _get_bleu(s1, s2):\n    count_dict={}\n    count_dict_clip={}\n    #1. count for each token at predict sentence side.\n    for token in s1:\n        if token not in count_dict:\n            count_dict[token]=1\n        else:\n            count_dict[token]=count_dict[token]+1\n    count=np.sum([value for key,value in count_dict.items()])\n\n    #2.count for tokens existing in predict sentence for target sentence side.\n    for token in s2:\n        if token in count_dict:\n            if token not in count_dict_clip:\n                count_dict_clip[token]=1\n            else:\n                count_dict_clip[token]=count_dict_clip[token]+1\n\n    #3. clip value to ceiling value for that token\n    count_dict_clip={key:(value if value<=count_dict[key] else count_dict[key]) for key,value in count_dict_clip.items()}\n    count_clip=np.sum([value for key,value in count_dict_clip.items()])\n    result=float(count_clip)/(float(count)+0.00000001)\n    return result\n\n\ndef _get_bleu_feat(s1, s2, ngrams=3):\n    if isinstance(s1, str):\n        s1 = s1.split("" "")\n    if isinstance(s2, str):\n        s2 = s2.split("" "")\n    feat = []\n    for ngram in range(ngrams+1):\n        s1_ngram = ngram_utils._ngrams(s1, ngram+1, ""_"")\n        s2_ngram = ngram_utils._ngrams(s2, ngram+1, ""_"")\n        feat.append(_get_bleu(s1_ngram, s2_ngram))\n    return np.array(feat, dtype=np.float32)\n\n\n\nif __name__ == ""__main__"":\n    s1 = [""W1"", ""W2"", ""W3"", ""W4"", ""W10""]\n    s2 = [""W1"", ""W2"", ""W4"", ""W6"", ""W8""]\n    print(_count_stats(s1, s2))\n    print(_edit_dist(s1, s2))\n    print(_longest_match_size(s1, s2))\n    print(_longest_match_ratio(s1, s2))\n    print(_get_rouge_feat(s1, s2))\n    print(_get_bleu_feat(s1, s2))'"
src/utils/log_utils.py,0,"b'\nimport os\nimport logging\nimport logging.handlers\n\n\ndef _get_logger(logdir, logname, loglevel=logging.INFO):\n    fmt = ""[%(asctime)s] %(levelname)s: %(message)s""\n    formatter = logging.Formatter(fmt)\n\n    handler = logging.handlers.RotatingFileHandler(\n        filename=os.path.join(logdir, logname),\n        maxBytes=2 * 1024 * 1024 * 1024,\n        backupCount=10)\n    handler.setFormatter(formatter)\n\n    logger = logging.getLogger("""")\n    logger.addHandler(handler)\n    logger.setLevel(loglevel)\n    return logger\n'"
src/utils/ngram_utils.py,0,"b'# -*- coding: utf-8 -*-\n""""""\n@author: Chenglong Chen <c.chenglong@gmail.com>\n@brief: utils for ngram\n""""""\n\n\ndef _unigrams(words):\n    """"""\n        Input: a list of words, e.g., [""I"", ""am"", ""Denny""]\n        Output: a list of unigram\n    """"""\n    assert type(words) == list\n    return words\n\n\ndef _bigrams(words, join_string, skip=0):\n    """"""\n       Input: a list of words, e.g., [""I"", ""am"", ""Denny""]\n       Output: a list of bigram, e.g., [""I_am"", ""am_Denny""]\n       I use _ as join_string for this example.\n    """"""\n    assert type(words) == list\n    L = len(words)\n    if L > 1:\n        lst = []\n        for i in range(L-1):\n            for k in range(1,skip+2):\n                if i+k < L:\n                    lst.append( join_string.join([words[i], words[i+k]]) )\n    else:\n        # set it as unigram\n        lst = _unigrams(words)\n    return lst\n\n\ndef _trigrams(words, join_string, skip=0):\n    """"""\n       Input: a list of words, e.g., [""I"", ""am"", ""Denny""]\n       Output: a list of trigram, e.g., [""I_am_Denny""]\n       I use _ as join_string for this example.\n    """"""\n    assert type(words) == list\n    L = len(words)\n    if L > 2:\n        lst = []\n        for i in range(L-2):\n            for k1 in range(1,skip+2):\n                for k2 in range(1,skip+2):\n                    if i+k1 < L and i+k1+k2 < L:\n                        lst.append( join_string.join([words[i], words[i+k1], words[i+k1+k2]]) )\n    else:\n        # set it as bigram\n        lst = _bigrams(words, join_string, skip)\n    return lst\n\n\ndef _fourgrams(words, join_string):\n    """"""\n        Input: a list of words, e.g., [""I"", ""am"", ""Denny"", ""boy""]\n        Output: a list of trigram, e.g., [""I_am_Denny_boy""]\n        I use _ as join_string for this example.\n    """"""\n    assert type(words) == list\n    L = len(words)\n    if L > 3:\n        lst = []\n        for i in range(L-3):\n            lst.append( join_string.join([words[i], words[i+1], words[i+2], words[i+3]]) )\n    else:\n        # set it as trigram\n        lst = _trigrams(words, join_string)\n    return lst\n\n\ndef _uniterms(words):\n    return _unigrams(words)\n\n\ndef _biterms(words, join_string):\n    """"""\n        Input: a list of words, e.g., [""I"", ""am"", ""Denny"", ""boy""]\n        Output: a list of biterm, e.g., [""I_am"", ""I_Denny"", ""I_boy"", ""am_Denny"", ""am_boy"", ""Denny_boy""]\n        I use _ as join_string for this example.\n    """"""\n    assert type(words) == list\n    L = len(words)\n    if L > 1:\n        lst = []\n        for i in range(L-1):\n            for j in range(i+1,L):\n                lst.append( join_string.join([words[i], words[j]]) )\n    else:\n        # set it as uniterm\n        lst = _uniterms(words)\n    return lst\n\n\ndef _triterms(words, join_string):\n    """"""\n        Input: a list of words, e.g., [""I"", ""am"", ""Denny"", ""boy""]\n        Output: a list of triterm, e.g., [""I_am_Denny"", ""I_am_boy"", ""I_Denny_boy"", ""am_Denny_boy""]\n        I use _ as join_string for this example.\n    """"""\n    assert type(words) == list\n    L = len(words)\n    if L > 2:\n        lst = []\n        for i in range(L-2):\n            for j in range(i+1,L-1):\n                for k in range(j+1,L):\n                    lst.append( join_string.join([words[i], words[j], words[k]]) )\n    else:\n        # set it as biterm\n        lst = _biterms(words, join_string)\n    return lst\n\n\ndef _fourterms(words, join_string):\n    """"""\n        Input: a list of words, e.g., [""I"", ""am"", ""Denny"", ""boy"", ""ha""]\n        Output: a list of fourterm, e.g., [""I_am_Denny_boy"", ""I_am_Denny_ha"", ""I_am_boy_ha"", ""I_Denny_boy_ha"", ""am_Denny_boy_ha""]\n        I use _ as join_string for this example.\n    """"""\n    assert type(words) == list\n    L = len(words)\n    if L > 3:\n        lst = []\n        for i in range(L-3):\n            for j in range(i+1,L-2):\n                for k in range(j+1,L-1):\n                    for l in range(k+1,L):\n                        lst.append( join_string.join([words[i], words[j], words[k], words[l]]) )\n    else:\n        # set it as triterm\n        lst = _triterms(words, join_string)\n    return lst\n\n\n_ngram_str_map = {\n    1: ""Unigram"",\n    2: ""Bigram"",\n    3: ""Trigram"",\n    4: ""Fourgram"",\n    5: ""Fivegram"",\n    12: ""UBgram"",\n    123: ""UBTgram"",\n}\n\n\ndef _ngrams(words, ngram, join_string="" ""):\n    """"""wrapper for ngram""""""\n    if ngram == 1:\n        return _unigrams(words)\n    elif ngram == 2:\n        return _bigrams(words, join_string)\n    elif ngram == 3:\n        return _trigrams(words, join_string)\n    elif ngram == 4:\n        return _fourgrams(words, join_string)\n    elif ngram == 12:\n        unigram = _unigrams(words)\n        bigram = [x for x in _bigrams(words, join_string) if len(x.split(join_string)) == 2]\n        return unigram + bigram\n    elif ngram == 123:\n        unigram = _unigrams(words)\n        bigram = [x for x in _bigrams(words, join_string) if len(x.split(join_string)) == 2]\n        trigram = [x for x in _trigrams(words, join_string) if len(x.split(join_string)) == 3]\n        return unigram + bigram + trigram\n\n\n_nterm_str_map = {\n    1: ""Uniterm"",\n    2: ""Biterm"",\n    3: ""Triterm"",\n    4: ""Fourterm"",\n    5: ""Fiveterm"",\n}\n\n\ndef _nterms(words, nterm, join_string="" ""):\n    """"""wrapper for nterm""""""\n    if nterm == 1:\n        return _uniterms(words)\n    elif nterm == 2:\n        return _biterms(words, join_string)\n    elif nterm == 3:\n        return _triterms(words, join_string)\n    elif nterm == 4:\n        return _fourterms(words, join_string)\n\n\nif __name__ == ""__main__"":\n\n    text = ""I am Denny boy ha""\n    words = text.split("" "")\n\n    assert _ngrams(words, 1) == [""I"", ""am"", ""Denny"", ""boy"", ""ha""]\n    assert _ngrams(words, 2) == [""I am"", ""am Denny"", ""Denny boy"", ""boy ha""]\n    assert _ngrams(words, 3) == [""I am Denny"", ""am Denny boy"", ""Denny boy ha""]\n    assert _ngrams(words, 4) == [""I am Denny boy"", ""am Denny boy ha""]\n\n    assert _nterms(words, 1) == [""I"", ""am"", ""Denny"", ""boy"", ""ha""]\n    assert _nterms(words, 2) == [""I am"", ""I Denny"", ""I boy"", ""I ha"", ""am Denny"", ""am boy"", ""am ha"", ""Denny boy"", ""Denny ha"", ""boy ha""]\n    assert _nterms(words, 3) == [""I am Denny"", ""I am boy"", ""I am ha"", ""I Denny boy"", ""I Denny ha"", ""I boy ha"", ""am Denny boy"", ""am Denny ha"", ""am boy ha"", ""Denny boy ha""]\n    assert _nterms(words, 4) == [""I am Denny boy"", ""I am Denny ha"", ""I am boy ha"", ""I Denny boy ha"", ""am Denny boy ha""]'"
src/utils/np_utils.py,0,"b'\nimport numpy as np\n\ndef _try_divide(x, y, val=0.0):\n    """"""try to divide two numbers""""""\n    if y != 0.0:\n        val = float(x) / y\n    return val\n'"
src/utils/os_utils.py,0,"b'\nimport os\nimport shutil\n\n\ndef _makedirs(dir, force=False):\n    if os.path.exists(dir):\n        if force:\n            shutil.rmtree(dir)\n            os.makedirs(dir)\n    else:\n        os.makedirs(dir)\n'"
src/utils/time_utils.py,0,"b'\nimport datetime\n\n\ndef _timestamp():\n    now = datetime.datetime.now()\n    now_str = now.strftime(""%Y%m%d%H%M"")\n    return now_str'"
src/utils/topk_utils.py,0,"b'\nfrom collections import defaultdict\nfrom random import randint\n\n\n# Bucket Sort\n# Time:  O(n + klogk) ~ O(n + nlogn)\n# Space: O(n)\nclass BucketSort(object):\n    def topKFrequent(self, words, k):\n        counts = defaultdict(int)\n        for ws in words:\n            for w in ws:\n                counts[w] += 1\n\n        buckets = [[]] * (sum(counts.values()) + 1)\n        for i, count in counts.items():\n            buckets[count].append(i)\n\n        result = []\n        # result_append = result.append\n        for i in reversed(range(len(buckets))):\n            for j in range(len(buckets[i])):\n                # slower\n                # result_append(buckets[i][j])\n                result.append(buckets[i][j])\n                if len(result) == k:\n                    return result\n        return result\n\n\n# Quick Select\n# Time:  O(n) ~ O(n^2), O(n) on average.\n# Space: O(n)\nclass QuickSelect(object):\n    def topKFrequent(self, words, k):\n        """"""\n        :type words: List[str]\n        :type k: int\n        :rtype: List[str]\n        """"""\n        counts = defaultdict(int)\n        for ws in words:\n            for w in ws:\n                counts[w] += 1\n        p = []\n        for key, val in counts.items():\n            p.append((-val, key))\n        self.kthElement(p, k)\n\n        result = []\n        sorted_p = sorted(p[:k])\n        for i in range(k):\n            result.append(sorted_p[i][1])\n        return result\n\n    def kthElement(self, nums, k):  # O(n) on average\n        def PartitionAroundPivot(left, right, pivot_idx, nums):\n            pivot_value = nums[pivot_idx]\n            new_pivot_idx = left\n            nums[pivot_idx], nums[right] = nums[right], nums[pivot_idx]\n            for i in range(left, right):\n                if nums[i] < pivot_value:\n                    nums[i], nums[new_pivot_idx] = nums[new_pivot_idx], nums[i]\n                    new_pivot_idx += 1\n\n            nums[right], nums[new_pivot_idx] = nums[new_pivot_idx], nums[right]\n            return new_pivot_idx\n\n        left, right = 0, len(nums) - 1\n        while left <= right:\n            pivot_idx = randint(left, right)\n            new_pivot_idx = PartitionAroundPivot(left, right, pivot_idx, nums)\n            if new_pivot_idx == k - 1:\n                return\n            elif new_pivot_idx > k - 1:\n                right = new_pivot_idx - 1\n            else:  # new_pivot_idx < k - 1.\n                left = new_pivot_idx + 1\n\n\ntop_k_selector = BucketSort()'"
