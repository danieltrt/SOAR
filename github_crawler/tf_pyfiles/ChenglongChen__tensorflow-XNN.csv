file_path,api_count,code
code/lr_schedule.py,0,"b'\nimport numpy as np\n\n""""""\nhttps://github.com/tensorflow/tensorflow/blob/3989529e6041be9b16009dd8b5b3889427b47952/tensorflow/python/training/learning_rate_decay.py\n""""""\n\ndef _cosine_decay_restarts(learning_rate, global_step, first_decay_steps,\n                           t_mul=2.0, m_mul=1.0, alpha=0.0,\n                           initial_variance=0.00, variance_decay=0.55):\n    initial_variance = min(learning_rate, initial_variance / 2.)\n    # noisy cosine decay with restarts\n    completed_fraction = global_step / first_decay_steps\n\n    def compute_step(completed_fraction, geometric=False):\n        if geometric:\n            i_restart = np.floor(np.log(1.0 - completed_fraction * (\n                    1.0 - t_mul)) / np.log(t_mul))\n\n            sum_r = (1.0 - t_mul ** i_restart) / (1.0 - t_mul)\n            completed_fraction = (completed_fraction - sum_r) / t_mul ** i_restart\n\n        else:\n            i_restart = np.floor(completed_fraction)\n            completed_fraction = completed_fraction - i_restart\n\n        return i_restart, completed_fraction\n\n    geometric = t_mul != 1.0\n    i_restart, completed_fraction = compute_step(completed_fraction, geometric)\n    m_fac = m_mul ** i_restart\n\n    # noise\n    variance = initial_variance / (np.power(1.0 + global_step, variance_decay))\n    std = np.sqrt(variance)\n    noisy_m_fac = m_fac + np.random.normal(0.0, std)\n\n    cosine_decayed = 0.5 * noisy_m_fac * (1.0 + np.cos(math.pi * completed_fraction))\n    decayed = (1 - alpha) * cosine_decayed + alpha\n\n    return learning_rate * decayed\n\n\ndef _exponential_decay(learning_rate, global_step, decay_steps, decay_rate,\n                       staircase=False):\n    p = global_step / decay_steps\n    if staircase:\n        p = np.floor(p)\n    return learning_rate * np.power(decay_rate, p)\n'"
code/main.py,3,"b'""""""\n60 mins. (4 cores / 16 GB RAM / 60 minutes run-time / 1 GB scratch and output disk space)\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport math\nimport datetime\nimport os\nimport gc\nimport glob\nimport mmh3\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport pickle as pkl\nimport shutil\nimport spacy\nimport time\nimport tensorflow as tf\nimport re\nimport string\nimport sys\nfrom collections import Counter, defaultdict\nfrom hashlib import md5\n\nfrom fastcache import clru_cache as lru_cache\n\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nfrom nltk import ToktokTokenizer\n\nfrom multiprocessing import Pool\n\nfrom six.moves import range\nfrom six.moves import zip\n\nif sys.version_info < (3,):\n    maketrans = string.maketrans\nelse:\n    maketrans = str.maketrans\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import LabelBinarizer\n\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom metrics import rmse\nfrom topk import top_k_selector\nfrom xnn import XNN\nfrom utils import _get_logger, _makedirs, _timestamp\n\n\n##############################################################################\n_makedirs(""./log"")\nlogger = _get_logger(""./log"", ""hyperopt-%s.log"" % _timestamp())\n\n##############################################################################\n\nRUNNING_MODE = ""validation""\n# RUNNING_MODE = ""submission""\nDEBUG = False\nDUMP_DATA = True\nUSE_PREPROCESSED_DATA = True\n\nUSE_MULTITHREAD = False\nif RUNNING_MODE == ""submission"":\n    N_JOBS = 4\nelse:\n    N_JOBS = 4\nNUM_PARTITIONS = 32\n\nDEBUG_SAMPLE_NUM = 200000\nLRU_MAXSIZE = 2 ** 16\n\n#######################################\n# File\nMISSING_VALUE_STRING = ""MISSINGVALUE""\nDROP_ZERO_PRICE = True\n#######################################\n\n\n# Preprocessing\nUSE_SPACY_TOKENIZER = False\nUSE_NLTK_TOKENIZER = False\nUSE_KAGGLE_TOKENIZER = False\n# default: \'!""#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\'\nKERAS_TOKENIZER_FILTERS = \'\\\'!""#%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n\'\nKERAS_TOKENIZER_FILTERS = """"\nKERAS_SPLIT = "" ""\n\nUSE_LEMMATIZER = False\nUSE_STEMMER = False\nUSE_CLEAN = True\n\nWORDREPLACER_DICT = {\n    ""bnwt"": ""brand new with tags"",\n    ""nwt"": ""new with tags"",\n    ""bnwot"": ""brand new without tags"",\n    ""nwot"": ""new without tags"",\n    ""bnip"": ""brand new in packet"",\n    ""nip"": ""new in packet"",\n    ""bnib"": ""brand new in box"",\n    ""nib"": ""new in box"",\n    ""mib"": ""mint in box"",\n    ""mwob"": ""mint without box"",\n    ""mip"": ""mint in packet"",\n    ""mwop"": ""mint without packet""\n}\n\nBRAND_NAME_PATTERN_LIST = [\n    (""nike"", ""nike""),\n    (""pink"", ""pink""),\n    (""apple"", ""iphone|ipod|ipad|iwatch|apple|mac""),\n    (""victoria\'s secret"", ""victoria""),\n    (""lularoe"", ""lularoe""),\n    (""nintendo"", ""nintendo""),\n    (""lululemon"", ""lululemon""),\n    (""forever 21"", ""forever\\s+21|forever\\s+twenty\\s+one""),\n    (""michael kors"", ""michael\\s+kors""),\n    (""american eagle"", ""american\\s+eagle""),\n    (""rae dunn"", ""rae dunn""),\n]\n\n# word count |   #word\n#    >= 1    |  195523\n#    >= 2    |   93637\n#    >= 3    |   67498\n#    >= 4    |   56265\n#    >= 5    |   49356\nMAX_NUM_WORDS = 80000\nMAX_NUM_BIGRAMS = 50000\nMAX_NUM_TRIGRAMS = 50000\nMAX_NUM_SUBWORDS = 20000\n\nNUM_TOP_WORDS_NAME = 50\nNUM_TOP_WORDS_ITEM_DESC = 50\n\nMAX_CATEGORY_NAME_LEN = 3\n\nEXTRACTED_BIGRAM = True\nEXTRACTED_TRIGRAM = True\nEXTRACTED_SUBWORD = False\nVOCAB_HASHING_TRICK = False\n\n######################\n\n####################################################################\nHYPEROPT_MAX_EVALS = 1\n\nparam_space_com = {\n    ""RUNNING_MODE"": RUNNING_MODE,\n    # size for the attention block\n    ""MAX_NUM_WORDS"": MAX_NUM_WORDS,\n    ""MAX_NUM_BIGRAMS"": MAX_NUM_BIGRAMS,\n    ""MAX_NUM_TRIGRAMS"": MAX_NUM_TRIGRAMS,\n    ""MAX_NUM_SUBWORDS"": MAX_NUM_SUBWORDS,\n\n    ""model_dir"": ""./weights"",\n\n    ""item_condition_size"": 5,\n    ""shipping_size"": 1,\n    ""num_vars_size"": 3,\n    # pad_sequences\n    ""pad_sequences_padding"": ""post"",\n    ""pad_sequences_truncating"": ""post"",\n    # optimization\n    ""optimizer_clipnorm"": 1.,\n    ""batch_size_train"": 512,\n    ""batch_size_inference"": 512*2,\n    ""shuffle_with_replacement"": False,\n    # CyclicLR\n    ""t_mul"": 1,\n    ""snapshot_every_num_cycle"": 128,\n    ""max_snapshot_num"": 14,\n    ""snapshot_every_epoch"": 4,  # for t_mult != 1\n    ""eval_every_num_update"": 1000,\n    # static param\n    ""random_seed"": 2018,\n    ""n_folds"": 1,\n    ""validation_ratio"": 0.4,\n}\n\nparam_space_best = {\n\n    #### params for input\n    # bigram/trigram/subword\n    ""use_bigram"": True,\n    ""use_trigram"": True,\n    ""use_subword"": False,\n\n    # seq len\n    ""max_sequence_length_name"": 10,\n    ""max_sequence_length_item_desc"": 50,\n    ""max_sequence_length_category_name"": 10,\n    ""max_sequence_length_item_desc_subword"": 45,\n\n    #### params for embed\n    ""embedding_dim"": 250,\n    ""embedding_dropout"": 0.,\n    ""embedding_mask_zero"": False,\n    ""embedding_mask_zero_subword"": False,\n\n    #### params for encode\n    ""encode_method"": ""fasttext"",\n    # cnn\n    ""cnn_num_filters"": 16,\n    ""cnn_filter_sizes"": [2, 3],\n    ""cnn_timedistributed"": False,\n    # rnn\n    ""rnn_num_units"": 16,\n    ""rnn_cell_type"": ""gru"",\n    #### params for attend\n    ""attend_method"": ""ave"",\n\n    #### params for predict\n    # deep\n    ""enable_deep"": True,\n    # fm\n    ""enable_fm_first_order"": True,\n    ""enable_fm_second_order"": True,\n    ""enable_fm_higher_order"": False,\n    # fc block\n    ""fc_type"": ""fc"",\n    ""fc_dim"": 64,\n    ""fc_dropout"": 0.,\n\n    #### params for optimization\n    ""optimizer_type"": ""nadam"",  # ""nadam"",  # """"lazyadam"", ""nadam""\n    ""max_lr_exp"": 0.005,\n    ""lr_decay_each_epoch_exp"": 0.9,\n    ""lr_jump_exp"": True,\n    ""max_lr_cosine"": 0.005,\n    ""base_lr"": 0.00001,  # minimum lr\n    ""lr_decay_each_epoch_cosine"": 0.5,\n    ""lr_jump_rate"": 1.,\n    ""snapshot_before_restarts"": 4,\n    ""beta1"": 0.975,\n    ""beta2"": 0.999,\n    ""schedule_decay"": 0.004,\n    # ""lr_schedule"": ""exponential_decay"",\n    ""lr_schedule"": ""cosine_decay_restarts"",\n    ""epoch"": 4,\n    # CyclicLR\n    ""num_cycle_each_epoch"": 8,\n\n    #### params ensemble\n    ""enable_snapshot_ensemble"": True,\n    ""n_runs"": 2,\n\n}\nparam_space_best.update(param_space_com)\nif RUNNING_MODE == ""submission"":\n    EXTRACTED_BIGRAM = param_space_best[""use_bigram""]\n    EXTRACTED_SUBWORD = param_space_best[""use_subword""]\n\nparam_space_hyperopt = param_space_best\n\nint_params = [\n    ""max_sequence_length_name"",\n    ""max_sequence_length_item_desc"",\n    ""max_sequence_length_item_desc_subword"",\n    ""max_sequence_length_category_name"",\n    ""embedding_dim"", ""embedding_dim"",\n    ""cnn_num_filters"", ""rnn_num_units"", ""fc_dim"",\n    ""epoch"", ""n_runs"",\n    ""num_cycle_each_epoch"", ""t_mul"", ""snapshot_every_num_cycle"",\n]\nint_params = set(int_params)\n\nif DEBUG:\n    param_space_hyperopt[""num_cycle_each_epoch""] = param_space_best[""num_cycle_each_epoch""] = 2\n    param_space_hyperopt[""snapshot_every_num_cycle""] = param_space_best[""snapshot_every_num_cycle""] = 1\n    param_space_hyperopt[""batch_size_train""] = param_space_best[""batch_size_train""] = 512\n    param_space_hyperopt[""batch_size_inference""] = param_space_best[""batch_size_inference""] = 512\n\n\n####################################################################################################\n########################################### NLP ####################################################\n####################################################################################################\ndef mmh3_hash_function(x):\n    return mmh3.hash(x, 42, signed=True)\n\n\ndef md5_hash_function(x):\n    return int(md5(x.encode()).hexdigest(), 16)\n\n\n@lru_cache(LRU_MAXSIZE)\ndef hashing_trick(string, n, hash_function=""mmh3""):\n    if hash_function == ""mmh3"":\n        hash_function = mmh3_hash_function\n    elif hash_function == ""md5"":\n        hash_function = md5_hash_function\n    i = (hash_function(string) % n) + 1\n    return i\n\n\n# 5.67 \xc2\xb5s \xc2\xb1 78.2 ns per loop (mean \xc2\xb1 std. dev. of 7 runs, 100000 loops each)\n@lru_cache(LRU_MAXSIZE)\ndef get_subword_for_word_all(word, n1=3, n2=6):\n    z = []\n    z_append = z.append\n    word = ""*"" + word + ""*""\n    l = len(word)\n    z_append(word)\n    for k in range(n1, n2 + 1):\n        for i in range(l - k + 1):\n            z_append(word[i:i + k])\n    return z\n\n\n@lru_cache(LRU_MAXSIZE)\ndef get_subword_for_word_all0(word, n1=3, n2=6):\n    z = []\n    z_append = z.append\n    word = ""*"" + word + ""*""\n    l = len(word)\n    z_append(word)\n    if l > n1:\n        n2 = min(n2, l - 1)\n        for i in range(l - n1 + 1):\n            for k in range(n1, n2 + 1):\n                if 2 * i + n2 < l:\n                    z_append(word[i:(i + k)])\n                    if i == 0:\n                        z_append(word[-(i + k + 1):])\n                    else:\n                        z_append(word[-(i + k + 1):-i])\n                else:\n                    if 2 * i + k < l:\n                        z_append(word[i:(i + k)])\n                        z_append(word[-(i + k + 1):-i])\n                    elif 2 * (i - 1) + n2 < l:\n                        z_append(word[i:(i + k)])\n    return z\n\n\n# 3.44 \xc2\xb5s \xc2\xb1 101 ns per loop (mean \xc2\xb1 std. dev. of 7 runs, 100000 loops each)\n@lru_cache(LRU_MAXSIZE)\ndef get_subword_for_word0(word, n1=4, n2=5, include_self=False):\n    """"""only extract the prefix and suffix""""""\n    l = len(word)\n    n1 = min(n1, l)\n    n2 = min(n2, l)\n    z1 = [word[:k] for k in range(n1, n2 + 1)]\n    z2 = [word[-k:] for k in range(n1, n2 + 1)]\n    z = z1 + z2\n    if include_self:\n        z.append(word)\n    return z\n\n\n# 2.49 \xc2\xb5s \xc2\xb1 104 ns per loop (mean \xc2\xb1 std. dev. of 7 runs, 100000 loops each)\n@lru_cache(LRU_MAXSIZE)\ndef get_subword_for_word(word, n1=3, n2=6, include_self=False):\n    """"""only extract the prefix and suffix""""""\n    z = []\n    if len(word) >= n1:\n        word = ""*"" + word + ""*""\n        l = len(word)\n        n1 = min(n1, l)\n        n2 = min(n2, l)\n        # bind method outside of loop to reduce overhead\n        # https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/feature_extraction/text.py#L144\n        z_append = z.append\n        if include_self:\n            z_append(word)\n        for k in range(n1, n2 + 1):\n            z_append(word[:k])\n            z_append(word[-k:])\n    return z\n\n\n# 564 \xc2\xb5s \xc2\xb1 14.9 \xc2\xb5s per loop (mean \xc2\xb1 std. dev. of 7 runs, 1000 loops each)\ndef get_subword_for_list0(input_list, n1=4, n2=5):\n    subword_lst = [get_subword_for_word(w, n1, n2) for w in input_list]\n    subword_lst = [w for ws in subword_lst for w in ws]\n    return subword_lst\n\n\n# 505 \xc2\xb5s \xc2\xb1 15 \xc2\xb5s per loop (mean \xc2\xb1 std. dev. of 7 runs, 1000 loops each)\ndef get_subword_for_list(input_list, n1=4, n2=5):\n    subwords = []\n    subwords_extend = subwords.extend\n    for w in input_list:\n        subwords_extend(get_subword_for_word(w, n1, n2))\n    return subwords\n\n\n@lru_cache(LRU_MAXSIZE)\ndef get_subword_for_text(text, n1=4, n2=5):\n    return get_subword_for_list(text.split("" ""), n1, n2)\n\n\nstopwords = [\n    ""and"",\n    ""the"",\n    ""for"",\n    ""a"",\n    ""in"",\n    ""to"",\n    ""is"",\n    # ""s"",\n    ""of"",\n    ""i"",\n    ""on"",\n    ""it"",\n    ""you"",\n    ""your"",\n    ""are"",\n    ""this"",\n    ""my"",\n]\nstopwords = set(stopwords)\n\n\n# spacy model\nclass SpacyTokenizer(object):\n    def __init__(self):\n        self.nlp = spacy.load(""en"", disable=[""parser"", ""tagger"", ""ner""])\n\n    def tokenize(self, text):\n        tokens = [tok.lower_ for tok in self.nlp(text)]\n        # tokens = get_valid_words(tokens)\n        return tokens\n\n\nLEMMATIZER = nltk.stem.wordnet.WordNetLemmatizer()\nSTEMMER = nltk.stem.snowball.EnglishStemmer()\nTOKTOKTOKENIZER = ToktokTokenizer()\n\n\n# SPACYTOKENIZER = SpacyTokenizer()\n\n\ndef get_wordnet_pos(treebank_tag):\n    if treebank_tag.startswith(\'J\'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith(\'V\'):\n        return wordnet.VERB\n    elif treebank_tag.startswith(\'N\'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith(\'R\'):\n        return wordnet.ADV\n    else:\n        return None\n\n\ndef get_valid_words(sentence):\n    res = [w.strip() for w in sentence]\n    return [w for w in res if w]\n\n\n@lru_cache(LRU_MAXSIZE)\ndef stem_word(word):\n    return STEMMER.stem(word)\n\n\n@lru_cache(LRU_MAXSIZE)\ndef lemmatize_word(word, pos=wordnet.NOUN):\n    return LEMMATIZER.lemmatize(word, pos)\n\n\ndef stem_sentence(sentence):\n    return [stem_word(w) for w in get_valid_words(sentence)]\n\n\ndef lemmatize_sentence(sentence):\n    res = []\n    sentence_ = get_valid_words(sentence)\n    for word, pos in pos_tag(sentence_):\n        wordnet_pos = get_wordnet_pos(pos) or wordnet.NOUN\n        res.append(lemmatize_word(word, pos=wordnet_pos))\n    return res\n\n\ndef stem_lemmatize_sentence(sentence):\n    return [stem_word(word) for word in lemmatize_sentence(sentence)]\n\n\nTRANSLATE_MAP = maketrans(KERAS_TOKENIZER_FILTERS, KERAS_SPLIT * len(KERAS_TOKENIZER_FILTERS))\n\n\ndef get_tokenizer():\n    if USE_LEMMATIZER and USE_STEMMER:\n        return stem_lemmatize_sentence\n    elif USE_LEMMATIZER:\n        return lemmatize_sentence\n    elif USE_STEMMER:\n        return stem_sentence\n    else:\n        return get_valid_words\n\n\ntokenizer = get_tokenizer()\n\n\n#\n# https://stackoverflow.com/questions/21883108/fast-optimize-n-gram-implementations-in-python\n# @lru_cache(LRU_MAXSIZE)\n# 40.1 \xc2\xb5s \xc2\xb1 918 ns per loop (mean \xc2\xb1 std. dev. of 7 runs, 10000 loops each)\ndef get_ngrams0(words, ngram_value):\n    # # return list\n    ngrams = ["" "".join(ngram) for ngram in zip(*[words[i:] for i in range(ngram_value)])]\n    # return generator (10x faster)\n    # ngrams = ("" "".join(ngram) for ngram in zip(*[words[i:] for i in range(ngram_value)]))\n    return ngrams\n\n\n# 36.2 \xc2\xb5s \xc2\xb1 1.04 \xc2\xb5s per loop (mean \xc2\xb1 std. dev. of 7 runs, 10000 loops each)\ndef get_ngrams(words, ngram_value):\n    tokens = []\n    tokens_append = tokens.append\n    for i in range(ngram_value):\n        tokens_append(words[i:])\n    ngrams = []\n    ngrams_append = ngrams.append\n    space_join = "" "".join\n    for ngram in zip(*tokens):\n        ngrams_append(space_join(ngram))\n    return ngrams\n\n\ndef get_bigrams(words):\n    return get_ngrams(words, 2)\n\n\ndef get_trigrams(words):\n    return get_ngrams(words, 3)\n\n\n@lru_cache(LRU_MAXSIZE)\n# 68.8 \xc2\xb5s \xc2\xb1 1.86 \xc2\xb5s per loop (mean \xc2\xb1 std. dev. of 7 runs, 10000 loops each)\ndef get_ngrams_range(text, ngram_range):\n    unigrams = text.split("" "")\n    ngrams = []\n    ngrams_extend = ngrams.extend\n    for i in range(ngram_range[0], ngram_range[1] + 1):\n        ngrams_extend(get_ngrams(unigrams, i))\n    return ngrams\n\n\n# 69.6 \xc2\xb5s \xc2\xb1 1.45 \xc2\xb5s per loop (mean \xc2\xb1 std. dev. of 7 runs, 10000 loops each)\ndef get_ngrams_range0(text, ngram_range):\n    unigrams = text.split("" "")\n    res = []\n    for i in ngram_range:\n        res += get_ngrams(unigrams, i)\n    res += unigrams\n    return res\n\n\n@lru_cache(LRU_MAXSIZE)\ndef stem(s):\n    return STEMMER.stem(s)\n\n\ntags = re.compile(r\'<.+?>\')\nwhitespace = re.compile(r\'\\s+\')\nnon_letter = re.compile(r\'\\W+\')\n\n\n@lru_cache(LRU_MAXSIZE)\ndef clean_text(text):\n    # text = text.lower()\n    text = non_letter.sub(\' \', text)\n\n    tokens = []\n\n    for t in text.split():\n        # if len(t) <= 2 and not t.isdigit():\n        #     continue\n        if t in stopwords:\n            continue\n        t = stem(t)\n        tokens.append(t)\n\n    text = \' \'.join(tokens)\n\n    text = whitespace.sub(\' \', text)\n    text = text.strip()\n    return text.split("" "")\n\n\n@lru_cache(LRU_MAXSIZE)\ndef tokenize(text):\n    if USE_NLTK_TOKENIZER:\n        # words = get_valid_words(word_tokenize(text))\n        # words = get_valid_words(wordpunct_tokenize(text))\n        words = get_valid_words(TOKTOKTOKENIZER.tokenize(text))\n    elif USE_SPACY_TOKENIZER:\n        words = get_valid_words(SPACYTOKENIZER.tokenize(text))\n    elif USE_KAGGLE_TOKENIZER:\n        words = clean_text(text)\n    else:\n        words = tokenizer(text.translate(TRANSLATE_MAP).split(KERAS_SPLIT))\n    return words\n\n\n@lru_cache(LRU_MAXSIZE)\ndef tokenize_with_subword(text, n1=4, n2=5):\n    words = tokenize(text)\n    subwords = get_subword_for_list(words, n1, n2)\n    return words + subwords\n\n\n######################################################################\n# --------------------------- Processor ---------------------------\n## base class\n## Most of the processings can be casted into the ""pattern-replace"" framework\nclass BaseReplacer:\n    def __init__(self, pattern_replace_pair_list=[]):\n        self.pattern_replace_pair_list = pattern_replace_pair_list\n\n\n## deal with word replacement\n# 1st solution in CrowdFlower\nclass WordReplacer(BaseReplacer):\n    def __init__(self, replace_dict):\n        self.replace_dict = replace_dict\n        self.pattern_replace_pair_list = []\n        for k, v in self.replace_dict.items():\n            # pattern = r""(?<=\\W|^)%s(?=\\W|$)"" % k\n            pattern = k\n            replace = v\n            self.pattern_replace_pair_list.append((pattern, replace))\n\n\nclass MerCariCleaner(BaseReplacer):\n    """"""https://stackoverflow.com/questions/7317043/regex-not-operator\n    """"""\n\n    def __init__(self):\n        self.pattern_replace_pair_list = [\n            # # remove filters\n            # (r\'[-!\\\'\\""#&()\\*\\+,-/:;<=\xef\xbc\x9d>?@\\[\\\\\\]^_`{|}~\\t\\n]+\', r""""),\n            # remove punctuation ""."", e.g.,\n            (r""(?<!\\d)\\.(?!\\d+)"", r"" ""),\n            # iphone 6/6s -> iphone 6 / 6s\n            # iphone 6:6s -> iphone 6 : 6s\n            (r""(\\W+)"", r"" \\1 ""),\n            # # non\n            # (r""[^A-Za-z0-9]+"", r"" ""),\n            # 6s -> 6 s\n            # 32gb -> 32 gb\n            # 4oz -> 4 oz\n            # 4pcs -> 4 pcs\n            (r""(\\d+)([a-zA-Z])"", r""\\1 \\2""),\n            # iphone5 -> iphone 5\n            # xbox360 -> xbox 360\n            # only split those with chars length > 3\n            (r""([a-zA-Z]{3,})(\\d+)"", r""\\1 \\2""),\n        ]\n\n\n###########################################\ndef df_lower(df):\n    return df.str.lower()\n\n\ndef df_contains(df, pat):\n    return df.str.contains(pat).astype(int)\n\n\ndef df_len(df):\n    return df.str.len().astype(float)\n\n\ndef df_num_tokens(df):\n    return df.str.split().apply(len).astype(float)\n\n\ndef df_in(df, col1, col2):\n    def _in(x):\n        return x[col1] in x[col2]\n\n    return df.apply(_in, 1).astype(int)\n\n\ndef df_brand_in_name(df):\n    return df_in(df, ""brand_name"", ""name"")\n\n\ndef df_category1_in_name(df):\n    return df_in(df, ""category_name1"", ""name"")\n\n\ndef df_category2_in_name(df):\n    return df_in(df, ""category_name2"", ""name"")\n\n\ndef df_category3_in_name(df):\n    return df_in(df, ""category_name3"", ""name"")\n\n\ndef df_brand_in_desc(df):\n    return df_in(df, ""brand_name"", ""item_desc"")\n\n\ndef df_category1_in_desc(df):\n    return df_in(df, ""category_name1"", ""item_desc"")\n\n\ndef df_category2_in_desc(df):\n    return df_in(df, ""category_name2"", ""item_desc"")\n\n\ndef df_category3_in_desc(df):\n    return df_in(df, ""category_name3"", ""item_desc"")\n\n\ndef df_clean(df):\n    for pat, repl in MerCariCleaner().pattern_replace_pair_list:\n        df = df.str.replace(pat, repl)\n    # for pat, repl in WordReplacer(WORDREPLACER_DICT).pattern_replace_pair_list:\n    #     df = df.str.replace(pat, repl)\n    return df\n\n\ndef df_tokenize(df):\n    return df.apply(tokenize)\n\n\ndef df_tokenize_with_subword(df):\n    return df.apply(tokenize_with_subword)\n\n\ndef df_get_bigram(df):\n    return df.apply(get_bigrams)\n\n\ndef df_get_trigram(df):\n    return df.apply(get_trigrams)\n\n\ndef df_get_subword(df):\n    return df.apply(get_subword_for_list)\n\n\ndef parallelize_df_func(df, func, num_partitions=NUM_PARTITIONS, n_jobs=N_JOBS):\n    df_split = np.array_split(df, num_partitions)\n    pool = Pool(n_jobs)\n    df = pd.concat(pool.map(func, df_split))\n    pool.close()\n    pool.join()\n    return df\n\n\n######################################################################\n\ndef load_train_data():\n    types_dict_train = {\n        \'train_id\': \'int32\',\n        \'item_condition_id\': \'int32\',\n        \'price\': \'float32\',\n        \'shipping\': \'int8\',\n        \'name\': \'str\',\n        \'brand_name\': \'str\',\n        \'item_desc\': \'str\',\n        \'category_name\': \'str\',\n    }\n    df = pd.read_csv(\'../input/train.tsv\', delimiter=\'\\t\', low_memory=True, dtype=types_dict_train)\n    df.rename(columns={""train_id"": ""id""}, inplace=True)\n    df.rename(columns={""item_description"": ""item_desc""}, inplace=True)\n    if DROP_ZERO_PRICE:\n        df = df[df.price > 0].copy()\n    price = np.log1p(df.price.values)\n    df.drop(""price"", axis=1, inplace=True)\n    df[""price""] = price\n    df[""is_train""] = 1\n    df[""missing_brand_name""] = df[""brand_name""].isnull().astype(int)\n    df[""missing_category_name""] = df[""category_name""].isnull().astype(int)\n    missing_ind = np.logical_or(df[""item_desc""].isnull(),\n                                df[""item_desc""].str.lower().str.contains(""no\\s+description\\s+yet""))\n    df[""missing_item_desc""] = missing_ind.astype(int)\n    df[""item_desc""][missing_ind] = df[""name""][missing_ind]\n    gc.collect()\n    if DEBUG:\n        return df.head(DEBUG_SAMPLE_NUM)\n    else:\n        return df\n\n\ndef load_test_data(chunksize=350000*2):\n    types_dict_test = {\n        \'test_id\': \'int32\',\n        \'item_condition_id\': \'int32\',\n        \'shipping\': \'int8\',\n        \'name\': \'str\',\n        \'brand_name\': \'str\',\n        \'item_description\': \'str\',\n        \'category_name\': \'str\',\n    }\n    chunks = pd.read_csv(\'../input/test.tsv\', delimiter=\'\\t\',\n                         low_memory=True, dtype=types_dict_test,\n                         chunksize=chunksize)\n    for df in chunks:\n        df.rename(columns={""test_id"": ""id""}, inplace=True)\n        df.rename(columns={""item_description"": ""item_desc""}, inplace=True)\n        df[""missing_brand_name""] = df[""brand_name""].isnull().astype(int)\n        df[""missing_category_name""] = df[""category_name""].isnull().astype(int)\n        missing_ind = np.logical_or(df[""item_desc""].isnull(),\n                                    df[""item_desc""].str.lower().str.contains(""no\\s+description\\s+yet""))\n        df[""missing_item_desc""] = missing_ind.astype(int)\n        df[""item_desc""][missing_ind] = df[""name""][missing_ind]\n        yield df\n\n\n@lru_cache(1024)\ndef split_category_name(row):\n    grps = row.split(""/"")\n    if len(grps) > MAX_CATEGORY_NAME_LEN:\n        grps = grps[:MAX_CATEGORY_NAME_LEN]\n    else:\n        grps += [MISSING_VALUE_STRING.lower()] * (MAX_CATEGORY_NAME_LEN - len(grps))\n    return tuple(grps)\n\n\n""""""\nhttps://stackoverflow.com/questions/3172173/most-efficient-way-to-calculate-frequency-of-values-in-a-python-list\n\n| approach       | american-english, |      big.txt, | time w.r.t. defaultdict |\n|                |     time, seconds | time, seconds |                         |\n|----------------+-------------------+---------------+-------------------------|\n| Counter        |             0.451 |         3.367 |                     3.6 |\n| setdefault     |             0.348 |         2.320 |                     2.5 |\n| list           |             0.277 |         1.822 |                       2 |\n| try/except     |             0.158 |         1.068 |                     1.2 |\n| defaultdict    |             0.141 |         0.925 |                       1 |\n| numpy          |             0.012 |         0.076 |                   0.082 |\n| S.Mark\'s ext.  |             0.003 |         0.019 |                   0.021 |\n| ext. in Cython |             0.001 |         0.008 |                  0.0086 |\n\ncode: https://gist.github.com/347000\n""""""\n\n\ndef get_word_index0(words, max_num, prefix):\n    word_counts = defaultdict(int)\n    for ws in words:\n        for w in ws:\n            word_counts[w] += 1\n    wcounts = list(word_counts.items())\n    wcounts.sort(key=lambda x: x[1], reverse=True)\n    sorted_voc = [wc[0] for wc in wcounts[:(max_num - 1)]]\n    word_index = dict(zip(sorted_voc, range(2, max_num)))\n    return word_index\n\n\ndef get_word_index1(words, max_num, prefix):\n    word_counts = Counter([w for ws in words for w in ws])\n    # """"""\n    wcounts = list(word_counts.items())\n    wcounts.sort(key=lambda x: x[1], reverse=True)\n    sorted_voc = [wc[0] for wc in wcounts]\n    del wcounts\n    gc.collect()\n    # only keep MAX_NUM_WORDS\n    sorted_voc = sorted_voc[:(max_num - 1)]\n    # note that index 0 is reserved, never assigned to an existing word\n    word_index = dict(zip(sorted_voc, range(2, max_num)))\n    return word_index\n\n\ndef get_word_index(words, max_num, prefix):\n    word_counts = Counter([w for ws in words for w in ws])\n    sorted_voc = [w for w, c in word_counts.most_common(max_num - 1)]\n    word_index = dict(zip(sorted_voc, range(2, max_num)))\n    return word_index\n\n\ndef get_word_index(words, max_num, prefix):\n    sorted_voc = top_k_selector.topKFrequent(words, max_num - 1)\n    word_index = dict(zip(sorted_voc, range(2, max_num)))\n    return word_index\n\n\nclass MyLabelEncoder(object):\n    """"""safely handle unknown label""""""\n\n    def __init__(self):\n        self.mapper = {}\n\n    def fit(self, X):\n        uniq_X = np.unique(X)\n        # reserve 0 for unknown\n        self.mapper = dict(zip(uniq_X, range(1, len(uniq_X) + 1)))\n        return self\n\n    def fit_transform(self, X):\n        self.fit(X)\n        return self.transform(X)\n\n    def _map(self, x):\n        return self.mapper.get(x, 0)\n\n    def transform(self, X):\n        return list(map(self._map, X))\n\n\nclass MyStandardScaler(object):\n    def __init__(self, identity=False, epsilon=1e-8):\n        self.identity = identity\n        self.mean_ = 0.\n        self.scale_ = 1.\n        self.epsilon = epsilon\n\n    def fit(self, X):\n        if not self.identity:\n            self.mean_ = np.mean(X, axis=0)\n            self.scale_ = np.std(X, axis=0)\n        else:\n            self.epsilon = 0.\n\n    def fit_transform(self, X):\n        self.fit(X)\n        return self.transform(X)\n\n    def transform(self, X):\n        return (X - self.mean_) / (self.scale_ + self.epsilon)\n\n    def inverse_transform(self, X):\n        return X * (self.scale_ + self.epsilon) + self.mean_\n\n\ndef preprocess(df, word_index=None, bigram_index=None,\n               trigram_index=None, subword_index=None,\n               label_encoder=None):\n    start_time = time.time()\n\n    #### fill na\n    df.fillna(MISSING_VALUE_STRING, inplace=True)\n    gc.collect()\n\n    #### to lower case\n    df[""name""] = df.name.str.lower()\n    df[""brand_name""] = df.brand_name.str.lower()\n    df[""category_name""] = df.category_name.str.lower()\n    df[""item_desc""] = df.item_desc.str.lower()\n    gc.collect()\n    print(""[%.5f] Done df_lower"" % (time.time() - start_time))\n\n    #### split category name\n    for i, cat in enumerate(zip(*df.category_name.apply(split_category_name))):\n        df[""category_name%d"" % (i + 1)] = cat\n        gc.collect()\n\n    #### regex based cleaning\n    if USE_CLEAN:\n        df[""name""] = parallelize_df_func(df[""name""], df_clean)\n        df[""item_desc""] = parallelize_df_func(df[""item_desc""], df_clean)\n        # df[""category_name""] = parallelize_df_func(df[""category_name""], df_clean)\n        print(""[%.5f] Done df_clean"" % (time.time() - start_time))\n        gc.collect()\n\n    #### tokenize\n    # print(""   Fitting tokenizer..."")\n    df[""seq_name""] = parallelize_df_func(df[""name""], df_tokenize)\n    df[""seq_item_desc""] = parallelize_df_func(df[""item_desc""], df_tokenize)\n    # df[""seq_brand_name""] = parallelize_df_func(df[""brand_name""], df_tokenize)\n    # df[""seq_category_name""] = parallelize_df_func(df[""category_name""], df_tokenize)\n    gc.collect()\n    print(""[%.5f] Done df_tokenize"" % (time.time() - start_time))\n    df.drop([""name""], axis=1, inplace=True)\n    df.drop([""item_desc""], axis=1, inplace=True)\n    gc.collect()\n    if USE_MULTITHREAD:\n        if EXTRACTED_BIGRAM:\n            df[""seq_bigram_item_desc""] = parallelize_df_func(df[""seq_item_desc""], df_get_bigram)\n            print(""[%.5f] Done df_get_bigram"" % (time.time() - start_time))\n        if EXTRACTED_TRIGRAM:\n            df[""seq_trigram_item_desc""] = parallelize_df_func(df[""seq_item_desc""], df_get_trigram)\n            print(""[%.5f] Done df_get_trigram"" % (time.time() - start_time))\n        if EXTRACTED_SUBWORD:\n            df[""seq_subword_item_desc""] = parallelize_df_func(df[""seq_item_desc""], df_get_subword)\n            print(""[%.5f] Done df_get_subword"" % (time.time() - start_time))\n    else:\n        if EXTRACTED_BIGRAM:\n            df[""seq_bigram_item_desc""] = df_get_bigram(df[""seq_item_desc""])\n            print(""[%.5f] Done df_get_bigram"" % (time.time() - start_time))\n        if EXTRACTED_TRIGRAM:\n            df[""seq_trigram_item_desc""] = df_get_trigram(df[""seq_item_desc""])\n            print(""[%.5f] Done df_get_trigram"" % (time.time() - start_time))\n        if EXTRACTED_SUBWORD:\n            df[""seq_subword_item_desc""] = df_get_subword(df[""seq_item_desc""])\n            print(""[%.5f] Done df_get_subword"" % (time.time() - start_time))\n    if not VOCAB_HASHING_TRICK:\n        if word_index is None:\n            ##### word_index\n            words = df.seq_name.tolist() + \\\n                    df.seq_item_desc.tolist()\n                    # df.seq_category_name.tolist()\n            word_index = get_word_index(words, MAX_NUM_WORDS, ""word"")\n            del words\n            gc.collect()\n        if EXTRACTED_BIGRAM:\n            if bigram_index is None:\n                bigrams = df.seq_bigram_item_desc.tolist()\n                bigram_index = get_word_index(bigrams, MAX_NUM_BIGRAMS, ""bigram"")\n                del bigrams\n                gc.collect()\n        if EXTRACTED_TRIGRAM:\n            if trigram_index is None:\n                trigrams = df.seq_trigram_item_desc.tolist()\n                trigram_index = get_word_index(trigrams, MAX_NUM_TRIGRAMS, ""trigram"")\n                del trigrams\n                gc.collect()\n        if EXTRACTED_SUBWORD:\n            if subword_index is None:\n                subwords = df.seq_subword_item_desc.tolist()\n                subword_index = get_word_index(subwords, MAX_NUM_SUBWORDS, ""subword"")\n                del subwords\n                gc.collect()\n        print(""[%.5f] Done building vocab"" % (time.time() - start_time))\n\n        # faster\n        # v = range(10000)\n        # k = [str(i) for i in v]\n        # vocab = dict(zip(k, v))\n        # %timeit word2ind(word_lst, vocab)\n        # 4.06 \xc2\xb5s \xc2\xb1 63.8 ns per loop (mean \xc2\xb1 std. dev. of 7 runs, 100000 loops each)\n        def word2ind0(word_lst, vocab):\n            vect = []\n            for w in word_lst:\n                if w in vocab:\n                    vect.append(vocab[w])\n            return vect\n\n        # 4.46 \xc2\xb5s \xc2\xb1 77.9 ns per loop (mean \xc2\xb1 std. dev. of 7 runs, 100000 loops each)\n        def word2ind1(word_lst, vocab):\n            vect = [vocab[w] for w in word_lst if w in vocab]\n            return vect\n\n        # 13.3 \xc2\xb5s \xc2\xb1 99.7 ns per loop (mean \xc2\xb1 std. dev. of 7 runs, 100000 loops each)\n        def word2ind2(word_lst, vocab):\n            vect = []\n            for w in word_lst:\n                i = vocab.get(w)\n                if i is not None:\n                    vect.append(i)\n            return vect\n\n        # 14.6 \xc2\xb5s \xc2\xb1 114 ns per loop (mean \xc2\xb1 std. dev. of 7 runs, 100000 loops each)\n        def word2ind3(word_lst, vocab):\n            return [vocab.get(w, 1) for w in word_lst]\n\n        word2ind = word2ind0\n\n        def wordlist2ind0(word_list_lst, vocab):\n            if len(word_list_lst) == 0:\n                vect = [[]]\n            else:\n                vect = []\n                for word_list in word_list_lst:\n                    vect_ = []\n                    for w in word_list:\n                        if w in vocab:\n                            vect_.append(vocab[w])\n                    vect.append(vect_)\n            return vect\n\n        wordlist2ind = wordlist2ind0\n\n        def word_lst_to_sequences(word_lst):\n            return word2ind(word_lst, word_index)\n\n        def df_word_lst_to_sequences(df):\n            return df.apply(word_lst_to_sequences)\n\n        def bigram_lst_to_sequences(word_lst):\n            return word2ind(word_lst, bigram_index)\n\n        def df_bigram_lst_to_sequences(df):\n            return df.apply(bigram_lst_to_sequences)\n\n        def trigram_lst_to_sequences(word_lst):\n            return word2ind(word_lst, trigram_index)\n\n        def df_trigram_lst_to_sequences(df):\n            return df.apply(trigram_lst_to_sequences)\n\n        def subword_lst_to_sequences(word_lst):\n            return word2ind(word_lst, subword_index)\n\n        def df_subword_lst_to_sequences(df):\n            return df.apply(subword_lst_to_sequences)\n\n        # print(""   Transforming text to seq..."")\n        if USE_MULTITHREAD:\n            df[""seq_name""] = parallelize_df_func(df[""seq_name""], df_word_lst_to_sequences)\n            df[""seq_item_desc""] = parallelize_df_func(df[""seq_item_desc""], df_word_lst_to_sequences)\n            # df[""seq_category_name""] = parallelize_df_func(df[""seq_category_name""], df_word_lst_to_sequences)\n            print(""[%.5f] Done df_word_lst_to_sequences"" % (time.time() - start_time))\n            if EXTRACTED_BIGRAM:\n                df[""seq_bigram_item_desc""] = parallelize_df_func(df[""seq_bigram_item_desc""],\n                                                                 df_bigram_lst_to_sequences)\n                print(""[%.5f] Done df_bigram_lst_to_sequences"" % (time.time() - start_time))\n            if EXTRACTED_TRIGRAM:\n                df[""seq_trigram_item_desc""] = parallelize_df_func(df[""seq_trigram_item_desc""],\n                                                                  df_trigram_lst_to_sequences)\n                print(""[%.5f] Done df_trigram_lst_to_sequences"" % (time.time() - start_time))\n            if EXTRACTED_SUBWORD:\n                df[""seq_subword_item_desc""] = parallelize_df_func(df[""seq_subword_item_desc""],\n                                                                  df_subword_lst_to_sequences)\n                print(""[%.5f] Done df_subword_lst_to_sequences"" % (time.time() - start_time))\n        else:\n            df[""seq_name""] = df_word_lst_to_sequences(df[""seq_name""])\n            df[""seq_item_desc""] = df_word_lst_to_sequences(df[""seq_item_desc""])\n            # df[""seq_category_name""] = df_word_lst_to_sequences(df[""seq_category_name""])\n            print(""[%.5f] Done df_word_lst_to_sequences"" % (time.time() - start_time))\n            if EXTRACTED_BIGRAM:\n                df[""seq_bigram_item_desc""] = df_bigram_lst_to_sequences(df[""seq_bigram_item_desc""])\n                print(""[%.5f] Done df_bigram_lst_to_sequences"" % (time.time() - start_time))\n            if EXTRACTED_TRIGRAM:\n                df[""seq_trigram_item_desc""] = df_trigram_lst_to_sequences(df[""seq_trigram_item_desc""])\n                print(""[%.5f] Done df_trigram_lst_to_sequences"" % (time.time() - start_time))\n            if EXTRACTED_SUBWORD:\n                df[""seq_subword_item_desc""] = df_subword_lst_to_sequences(df[""seq_subword_item_desc""])\n                print(""[%.5f] Done df_subword_lst_to_sequences"" % (time.time() - start_time))\n\n    else:\n        def word_lst_to_sequences_hash(word_lst):\n            vect = [hashing_trick(w, MAX_NUM_WORDS) for w in word_lst]\n            return vect\n\n        def df_word_lst_to_sequences_hash(df):\n            return df.apply(word_lst_to_sequences_hash)\n\n        def bigram_lst_to_sequences_hash(word_lst):\n            vect = [hashing_trick(w, MAX_NUM_BIGRAMS) for w in word_lst]\n            return vect\n\n        def df_bigram_lst_to_sequences_hash(df):\n            return df.apply(bigram_lst_to_sequences_hash)\n\n        def trigram_lst_to_sequences_hash(word_lst):\n            vect = [hashing_trick(w, MAX_NUM_TRIGRAMS) for w in word_lst]\n            return vect\n\n        def df_trigram_lst_to_sequences_hash(df):\n            return df.apply(trigram_lst_to_sequences_hash)\n\n        def subword_lst_to_sequences_hash(word_lst):\n            vect = [hashing_trick(w, MAX_NUM_SUBWORDS) for w in word_lst]\n            return vect\n\n        def df_subword_lst_to_sequences_hash(df):\n            return df.apply(subword_lst_to_sequences_hash)\n\n        # print(""   Transforming text to seq..."")\n        if USE_MULTITHREAD:\n            df[""seq_name""] = parallelize_df_func(df[""seq_name""], df_word_lst_to_sequences_hash)\n            df[""seq_item_desc""] = parallelize_df_func(df[""seq_item_desc""], df_word_lst_to_sequences_hash)\n            # df[""seq_category_name""] = parallelize_df_func(df[""seq_category_name""], df_word_lst_to_sequences_hash)\n            gc.collect()\n            print(""[%.5f] Done df_word_lst_to_sequences_hash"" % (time.time() - start_time))\n            if EXTRACTED_BIGRAM:\n                df[""seq_bigram_item_desc""] = parallelize_df_func(df[""seq_bigram_item_desc""],\n                                                                 df_bigram_lst_to_sequences_hash)\n                print(""[%.5f] Done df_bigram_lst_to_sequences_hash"" % (time.time() - start_time))\n            if EXTRACTED_TRIGRAM:\n                df[""seq_trigram_item_desc""] = parallelize_df_func(df[""seq_trigram_item_desc""],\n                                                                  df_trigram_lst_to_sequences_hash)\n                print(""[%.5f] Done df_trigram_lst_to_sequences_hash"" % (time.time() - start_time))\n            if EXTRACTED_SUBWORD:\n                df[""seq_subword_item_desc""] = parallelize_df_func(df[""seq_subword_item_desc""],\n                                                                  df_subword_lst_to_sequences_hash)\n                print(""[%.5f] Done df_subword_lst_to_sequences_hash"" % (time.time() - start_time))\n        else:\n            df[""seq_name""] = df_word_lst_to_sequences_hash(df[""seq_name""])\n            df[""seq_item_desc""] = df_word_lst_to_sequences_hash(df[""seq_item_desc""])\n            # df[""seq_category_name""] = df_word_lst_to_sequences_hash(df[""seq_category_name""])\n            gc.collect()\n            print(""[%.5f] Done df_word_lst_to_sequences_hash"" % (time.time() - start_time))\n            if EXTRACTED_BIGRAM:\n                df[""seq_bigram_item_desc""] = df_bigram_lst_to_sequences_hash(df[""seq_bigram_item_desc""])\n                print(""[%.5f] Done df_bigram_lst_to_sequences_hash"" % (time.time() - start_time))\n            if EXTRACTED_TRIGRAM:\n                df[""seq_trigram_item_desc""] = df_trigram_lst_to_sequences_hash(df[""seq_trigram_item_desc""])\n                print(""[%.5f] Done df_trigram_lst_to_sequences_hash"" % (time.time() - start_time))\n            if EXTRACTED_SUBWORD:\n                df[""seq_subword_item_desc""] = df_subword_lst_to_sequences_hash(df[""seq_subword_item_desc""])\n                print(""[%.5f] Done df_subword_lst_to_sequences_hash"" % (time.time() - start_time))\n\n    print(""[%.5f] Done tokenize data"" % (time.time() - start_time))\n\n    if RUNNING_MODE != ""submission"":\n        print(\'Average name sequence length: {}\'.format(df[""seq_name""].apply(len).mean()))\n        print(\'Average item_desc sequence length: {}\'.format(df[""seq_item_desc""].apply(len).mean()))\n        # print(\'Average brand_name sequence length: {}\'.format(df[""seq_brand_name""].apply(len).mean()))\n        # print(\'Average category_name sequence length: {}\'.format(df[""seq_category_name""].apply(len).mean()))\n        if EXTRACTED_SUBWORD:\n            print(\'Average item_desc subword sequence length: {}\'.format(\n                df[""seq_subword_item_desc""].apply(len).mean()))\n\n    #### convert categorical variables\n    if label_encoder is None:\n        label_encoder = {}\n        label_encoder[""brand_name""] = MyLabelEncoder()\n        df[""brand_name_cat""] = label_encoder[""brand_name""].fit_transform(df[""brand_name""])\n        label_encoder[""category_name""] = MyLabelEncoder()\n        df[""category_name_cat""] = label_encoder[""category_name""].fit_transform(df[""category_name""])\n        df.drop(""brand_name"", axis=1, inplace=True)\n        df.drop(""category_name"", axis=1, inplace=True)\n        gc.collect()\n        for i in range(MAX_CATEGORY_NAME_LEN):\n            label_encoder[""category_name%d"" % (i + 1)] = MyLabelEncoder()\n            df[""category_name%d_cat"" % (i + 1)] = label_encoder[""category_name%d"" % (i + 1)].fit_transform(\n                df[""category_name%d"" % (i + 1)])\n            df.drop(""category_name%d"" % (i + 1), axis=1, inplace=True)\n    else:\n        df[""brand_name_cat""] = label_encoder[""brand_name""].transform(df[""brand_name""])\n        df[""category_name_cat""] = label_encoder[""category_name""].transform(df[""category_name""])\n        df.drop(""brand_name"", axis=1, inplace=True)\n        df.drop(""category_name"", axis=1, inplace=True)\n        gc.collect()\n        for i in range(MAX_CATEGORY_NAME_LEN):\n            df[""category_name%d_cat"" % (i + 1)] = label_encoder[""category_name%d"" % (i + 1)].transform(\n                df[""category_name%d"" % (i + 1)])\n            df.drop(""category_name%d"" % (i + 1), axis=1, inplace=True)\n    print(""[%.5f] Done Handling categorical variables"" % (time.time() - start_time))\n\n\n    if DUMP_DATA and RUNNING_MODE != ""submission"":\n        try:\n            with open(pkl_file, ""wb"") as f:\n                pkl.dump(df, f)\n        except:\n            pass\n\n    return df, word_index, bigram_index, trigram_index, subword_index, label_encoder\n\n\nfeat_cols = [\n    ""missing_brand_name"", ""missing_category_name"", ""missing_item_desc"",\n]\nNUM_VARS_DIM = len(feat_cols)\n\n\ndef get_xnn_data(dataset, lbs, params):\n    start_time = time.time()\n\n    if lbs is None:\n        lbs = []\n        lb = LabelBinarizer(sparse_output=True)\n        item_condition_array = lb.fit_transform(dataset.item_condition_id).toarray()\n        lbs.append(lb)\n\n    else:\n        lb = lbs[0]\n        item_condition_array = lb.transform(dataset.item_condition_id).toarray()\n\n\n    num_vars = dataset[feat_cols].values\n\n    X = {}\n\n    X[\'seq_name\'] = pad_sequences(dataset.seq_name, maxlen=params[""max_sequence_length_name""],\n                                        padding=params[""pad_sequences_padding""],\n                                        truncating=params[""pad_sequences_truncating""])\n    X[""sequence_length_name""] = params[""max_sequence_length_name""] * np.ones(dataset.shape[0])\n\n    X[\'seq_item_desc\'] = pad_sequences(dataset.seq_item_desc, maxlen=params[""max_sequence_length_item_desc""],\n                                             padding=params[""pad_sequences_padding""],\n                                             truncating=params[""pad_sequences_truncating""])\n    X[""sequence_length_item_desc""] = params[""max_sequence_length_item_desc""] * np.ones(dataset.shape[0])\n\n    X[\'seq_bigram_item_desc\'] = pad_sequences(dataset.seq_bigram_item_desc,\n                                                    maxlen=params[""max_sequence_length_item_desc""],\n                                                    padding=params[""pad_sequences_padding""],\n                                                    truncating=params[""pad_sequences_truncating""]) if params[\n        ""use_bigram""] else None\n\n    X[\'seq_trigram_item_desc\'] = pad_sequences(dataset.seq_trigram_item_desc,\n                                                     maxlen=params[""max_sequence_length_item_desc""],\n                                                     padding=params[""pad_sequences_padding""],\n                                                     truncating=params[""pad_sequences_truncating""]) if params[\n        ""use_trigram""] else None\n\n    X[\'seq_subword_item_desc\'] = pad_sequences(dataset.seq_subword_item_desc,\n                                                     maxlen=params[""max_sequence_length_item_desc_subword""],\n                                                     padding=params[""pad_sequences_padding""],\n                                                     truncating=params[""pad_sequences_truncating""]) if params[\n        ""use_subword""] else None\n    X[""sequence_length_item_desc_subword""] = params[""max_sequence_length_item_desc_subword""] * np.ones(dataset.shape[0])\n\n    X.update({\n        \'brand_name\': dataset.brand_name_cat.values.reshape((-1, 1)),\n        # \'category_name\': dataset.category_name_cat.values.reshape((-1, 1)),\n        \'category_name1\': dataset.category_name1_cat.values.reshape((-1, 1)),\n        \'category_name2\': dataset.category_name2_cat.values.reshape((-1, 1)),\n        \'category_name3\': dataset.category_name3_cat.values.reshape((-1, 1)),\n        \'item_condition_id\': dataset.item_condition_id.values.reshape((-1, 1)),\n        \'item_condition\': item_condition_array,\n        \'num_vars\': num_vars,\n        \'shipping\': dataset.shipping.values.reshape((-1, 1)),\n\n    })\n\n    print(""[%.5f] Done get_xnn_data."" % (time.time() - start_time))\n    return X, lbs, params\n\n\n\n########################\n# MODEL TRAINING\n########################\ndef get_training_params(train_size, batch_size, params):\n    params[""num_update_each_epoch""] = int(train_size / float(batch_size))\n\n    # # cyclic lr\n    params[""m_mul""] = np.power(params[""lr_decay_each_epoch_cosine""], 1. / params[""num_cycle_each_epoch""])\n    params[""m_mul_exp""] = np.power(params[""lr_decay_each_epoch_exp""], 1. / params[""num_cycle_each_epoch""])\n    if params[""t_mul""] == 1:\n        tmp = int(params[""num_update_each_epoch""] / params[""num_cycle_each_epoch""])\n    else:\n        tmp = int(params[""num_update_each_epoch""] / params[""snapshot_every_epoch""] * (1. - params[""t_mul""]) / (\n                1. - np.power(params[""t_mul""], params[""num_cycle_each_epoch""] / params[""snapshot_every_epoch""])))\n    params[""first_decay_steps""] = max([tmp, 1])\n    params[""snapshot_every_num_cycle""] = params[""num_cycle_each_epoch""] // params[""snapshot_every_epoch""]\n    params[""snapshot_every_num_cycle""] = max(params[""snapshot_every_num_cycle""], 1)\n\n    # cnn\n    if params[""cnn_timedistributed""]:\n        params[""cnn_num_filters""] = params[""embedding_dim""]\n\n    # text dim after the encode step\n    if params[""encode_method""] == ""fasttext"":\n        encode_text_dim = params[""embedding_dim""]\n    elif params[""encode_method""] == ""textcnn"":\n        encode_text_dim = params[""cnn_num_filters""] * len(params[""cnn_filter_sizes""])\n    elif params[""encode_method""] in [""textrnn"", ""textbirnn""]:\n        encode_text_dim = params[""rnn_num_units""]\n    elif params[""encode_method""] == ""fasttext+textcnn"":\n        encode_text_dim = params[""embedding_dim""] + params[""cnn_num_filters""] * len(\n            params[""cnn_filter_sizes""])\n    elif params[""encode_method""] in [""fasttext+textrnn"", ""fasttext+textbirnn""]:\n        encode_text_dim = params[""embedding_dim""] + params[""rnn_num_units""]\n    elif params[""encode_method""] in [""fasttext+textcnn+textrnn"", ""fasttext+textcnn+textbirnn""]:\n        encode_text_dim = params[""embedding_dim""] + params[""cnn_num_filters""] * len(\n            params[""cnn_filter_sizes""]) + params[""rnn_num_units""]\n    params[""encode_text_dim""] = encode_text_dim\n\n    return params\n\n\ndef cross_validation_hyperopt(dfTrain, params, target_scaler):\n    params = ModelParamSpace()._convert_int_param(params)\n    _print_param_dict(params)\n\n    # level1, valid index\n    level1Ratio, validRatio = 0.6, 0.4\n    num_train = dfTrain.shape[0]\n    level1Size = int(level1Ratio * num_train)\n    indices = np.arange(num_train)\n    np.random.seed(params[""random_seed""])\n    np.random.shuffle(indices)\n    level1Ind, validInd = indices[:level1Size], indices[level1Size:]\n    y_level1, y_valid = dfTrain.price.values[level1Ind].reshape((-1, 1)), dfTrain.price.values[validInd].reshape(\n        (-1, 1))\n    y_valid_inv = target_scaler.inverse_transform(y_valid)\n\n    X_level1, lbs, params = get_xnn_data(dfTrain.iloc[level1Ind], lbs=None, params=params)\n    X_valid, lbs, _ = get_xnn_data(dfTrain.iloc[validInd], lbs=lbs, params=params)\n\n    params = get_training_params(train_size=len(level1Ind), batch_size=params[""batch_size_train""],\n                                 params=params)\n    model = XNN(params, target_scaler, logger)\n    model.fit(X_level1, y_level1, validation_data=(X_valid, y_valid))\n    y_valid_tf = model.predict(X_valid, mode=""raw"")\n    y_valid_tf_inv = target_scaler.inverse_transform(y_valid_tf)\n    for j in reversed(range(y_valid_tf.shape[1])):\n        rmsle = rmse(y_valid_inv, y_valid_tf_inv[:, j, np.newaxis])\n        logger.info(""valid-rmsle (tf of last %d): %.5f"" % (y_valid_tf.shape[1] - j, rmsle))\n        y_valid_tf_inv_ = np.mean(y_valid_tf_inv[:, j:], axis=1, keepdims=True)\n        rmsle = rmse(y_valid_inv, y_valid_tf_inv_)\n        logger.info(\n            ""valid-rmsle (tf snapshot ensemble with mean of last %d): %.5f"" % (y_valid_tf.shape[1] - j, rmsle))\n\n    stacking_model = LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=4)\n    stacking_model.fit(y_valid_tf, y_valid)\n    logger.info(stacking_model.intercept_)\n    logger.info(stacking_model.coef_)\n    y_valid_stack = stacking_model.predict(y_valid_tf).reshape((-1, 1))\n    y_valid_stack_inv = target_scaler.inverse_transform(y_valid_stack)\n    rmsle = rmse(y_valid_inv, y_valid_stack_inv)\n    logger.info(""rmsle (stack): %.5f"" % rmsle)\n\n    rmsle_mean = rmsle\n    rmsle_std = 0\n    logger.info(""RMSLE"")\n    logger.info(""      Mean: %.6f"" % rmsle_mean)\n    logger.info(""      Std: %.6f"" % rmsle_std)\n    ret = {\n        ""loss"": rmsle_mean,\n        ""attachments"": {\n            ""std"": rmsle_std,\n        },\n        ""status"": STATUS_OK,\n    }\n    return ret\n\n\ndef submission(params):\n    params = ModelParamSpace()._convert_int_param(params)\n    _print_param_dict(params)\n    start_time = time.time()\n\n    dfTrain = load_train_data()\n    target_scaler = MyStandardScaler()\n    dfTrain[""price""] = target_scaler.fit_transform(dfTrain[""price""].values.reshape(-1, 1))\n    dfTrain, word_index, bigram_index, trigram_index, subword_index, label_encoder = preprocess(\n        dfTrain)\n\n    X_train, lbs_tf, params = get_xnn_data(dfTrain, lbs=None, params=params)\n    y_train = dfTrain.price.values.reshape((-1, 1))\n\n    params[""MAX_NUM_BRANDS""] = dfTrain[""brand_name_cat""].max() + 1\n    params[""MAX_NUM_CATEGORIES""] = dfTrain[""category_name_cat""].max() + 1\n    params[""MAX_NUM_CATEGORIES_LST""] = [0] * MAX_CATEGORY_NAME_LEN\n    for i in range(MAX_CATEGORY_NAME_LEN):\n        params[""MAX_NUM_CATEGORIES_LST""][i] = dfTrain[""category_name%d_cat"" % (i + 1)].max() + 1\n    params[""MAX_NUM_CONDITIONS""] = dfTrain[""item_condition_id""].max()\n    params[""MAX_NUM_SHIPPINGS""] = 2\n    params[""NUM_VARS_DIM""] = NUM_VARS_DIM\n\n    del dfTrain\n    gc.collect()\n    print(\'[%.5f] Finished loading data\' % (time.time() - start_time))\n\n    params = get_training_params(train_size=len(y_train), batch_size=params[""batch_size_train""], params=params)\n    model = XNN(params, target_scaler, logger)\n    model.fit(X_train, y_train)\n    del X_train\n    del y_train\n    gc.collect()\n    print(\'[%.5f] Finished training tf\' % (time.time() - start_time))\n\n    y_test = []\n    id_test = []\n    for dfTest in load_test_data(chunksize=350000*2):\n        dfTest, _, _, _, _, _ = preprocess(dfTest, word_index, bigram_index, trigram_index, subword_index, label_encoder)\n        X_test, lbs_tf, _ = get_xnn_data(dfTest, lbs=lbs_tf, params=params)\n\n        y_test_ = model.predict(X_test, mode=""weight"")\n        y_test.append(y_test_)\n        id_test.append(dfTest.id.values.reshape((-1, 1)))\n\n    y_test = np.vstack(y_test)\n    id_test = np.vstack(id_test)\n    y_test = np.expm1(target_scaler.inverse_transform(y_test))\n    y_test = y_test.flatten()\n    id_test = id_test.flatten()\n    id_test = id_test.astype(int)\n    y_test[y_test < 0.0] = 0.0\n    submission = pd.DataFrame({""test_id"": id_test, ""price"": y_test})\n    submission.to_csv(""sample_submission.csv"", index=False)\n    print(\'[%.5f] Finished prediction\' % (time.time() - start_time))\n\n\n# -------------------------------------- fasttext ---------------------------------------------\ndef _print_param_dict(d, prefix=""      "", incr_prefix=""      ""):\n    for k, v in sorted(d.items()):\n        if isinstance(v, dict):\n            logger.info(""%s%s:"" % (prefix, k))\n            _print_param_dict(v, prefix + incr_prefix, incr_prefix)\n        else:\n            logger.info(""%s%s: %s"" % (prefix, k, v))\n\n\nclass ModelParamSpace:\n    def __init__(self):\n        pass\n\n    def _convert_int_param(self, param_dict):\n        if isinstance(param_dict, dict):\n            for k, v in param_dict.items():\n                if k in int_params:\n                    param_dict[k] = v if v is None else int(v)\n                elif isinstance(v, list) or isinstance(v, tuple):\n                    for i in range(len(v)):\n                        self._convert_int_param(v[i])\n                elif isinstance(v, dict):\n                    self._convert_int_param(v)\n        return param_dict\n\n\nif RUNNING_MODE == ""validation"":\n    load_data_success = False\n    pkl_file = ""../input/dfTrain_bigram_[MAX_NUM_WORDS_%d]_[MAX_NUM_BIGRAMS_%d]_[VOCAB_HASHING_TRICK_%s].pkl"" % (\n        MAX_NUM_WORDS, MAX_NUM_BIGRAMS, str(VOCAB_HASHING_TRICK))\n    if USE_PREPROCESSED_DATA:\n        try:\n            with open(pkl_file, ""rb"") as f:\n                dfTrain = pkl.load(f)\n            if DEBUG:\n                dfTrain = dfTrain.head(DEBUG_SAMPLE_NUM)\n            load_data_success = True\n        except:\n            pass\n    if not load_data_success:\n        dfTrain = load_train_data()\n        dfTrain, word_index, bigram_index, trigram_index, subword_index, label_encoder = preprocess(dfTrain)\n    target_scaler = MyStandardScaler()\n    dfTrain[""price""] = target_scaler.fit_transform(dfTrain[""price""].values.reshape(-1, 1))\n\n    param_space_hyperopt[""MAX_NUM_BRANDS""] = dfTrain[""brand_name_cat""].max() + 1\n    param_space_hyperopt[""MAX_NUM_CATEGORIES""] = dfTrain[""category_name_cat""].max() + 1\n    param_space_hyperopt[""MAX_NUM_CATEGORIES_LST""] = [0] * MAX_CATEGORY_NAME_LEN\n    for i in range(MAX_CATEGORY_NAME_LEN):\n        param_space_hyperopt[""MAX_NUM_CATEGORIES_LST""][i] = dfTrain[""category_name%d_cat"" % (i + 1)].max() + 1\n    param_space_hyperopt[""MAX_NUM_CONDITIONS""] = dfTrain[""item_condition_id""].max()\n    param_space_hyperopt[""MAX_NUM_SHIPPINGS""] = 2\n    param_space_hyperopt[""NUM_VARS_DIM""] = NUM_VARS_DIM\n\n    start_time = time.time()\n    trials = Trials()\n    obj = lambda param: cross_validation_hyperopt(dfTrain, param, target_scaler)\n    best = fmin(obj, param_space_hyperopt, tpe.suggest, HYPEROPT_MAX_EVALS, trials)\n    best_params = space_eval(param_space_hyperopt, best)\n    best_params = ModelParamSpace()._convert_int_param(best_params)\n    trial_rmsles = np.asarray(trials.losses(), dtype=float)\n    best_ind = np.argmin(trial_rmsles)\n    best_rmse_mean = trial_rmsles[best_ind]\n    best_rmse_std = trials.trial_attachments(trials.trials[best_ind])[""std""]\n    logger.info(""-"" * 50)\n    logger.info(""Best RMSLE"")\n    logger.info(""      Mean: %.6f"" % best_rmse_mean)\n    logger.info(""      Std: %.6f"" % best_rmse_std)\n    logger.info(""Best param"")\n    _print_param_dict(best_params)\n    logger.info(""time: %.5f"" % (time.time() - start_time))\n\nelse:\n    submission(param_space_best)'"
code/metrics.py,0,"b'\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\n\n\ndef rmse(y_true, y_pred):\n    assert y_true.shape == y_pred.shape\n    return np.sqrt(mean_squared_error(y_true, y_pred))'"
code/nn_module.py,129,"b'\nimport tensorflow as tf\n\n""""""\nhttps://explosion.ai/blog/deep-learning-formula-nlp\nembed -> encode -> attend -> predict\n""""""\ndef batch_normalization(x, training, name):\n    bn_train = tf.layers.batch_normalization(x, training=True, reuse=None, name=name)\n    bn_inference = tf.layers.batch_normalization(x, training=False, reuse=True, name=name)\n    z = tf.cond(training, lambda: bn_train, lambda: bn_inference)\n    return z\n\n\n#### Step 1\ndef embed(x, size, dim, seed=0, flatten=False, reduce_sum=False):\n    # std = np.sqrt(2 / dim)\n    std = 0.001\n    minval = -std\n    maxval = std\n    emb = tf.Variable(tf.random_uniform([size, dim], minval, maxval, dtype=tf.float32, seed=seed))\n    # None * max_seq_len * embed_dim\n    out = tf.nn.embedding_lookup(emb, x)\n    if flatten:\n        out = tf.layers.flatten(out)\n    if reduce_sum:\n        out = tf.reduce_sum(out, axis=1)\n    return out\n\n\ndef embed_subword(x, size, dim, sequence_length, seed=0, mask_zero=False, maxlen=None):\n    # std = np.sqrt(2 / dim)\n    std = 0.001\n    minval = -std\n    maxval = std\n    emb = tf.Variable(tf.random_uniform([size, dim], minval, maxval, dtype=tf.float32, seed=seed))\n    # None * max_seq_len * max_word_len * embed_dim\n    out = tf.nn.embedding_lookup(emb, x)\n    if mask_zero:\n        # word_len: None * max_seq_len\n        # mask: shape=None * max_seq_len * max_word_len\n        mask = tf.sequence_mask(sequence_length, maxlen)\n        mask = tf.expand_dims(mask, axis=-1)\n        mask = tf.cast(mask, tf.float32)\n        out = out * mask\n    # None * max_seq_len * embed_dim\n    # according to facebook subword paper, it\'s sum\n    out = tf.reduce_sum(out, axis=2)\n    return out\n\n\ndef word_dropout(x, training, dropout=0, seed=0):\n    # word dropout (dropout the entire embedding for some words)\n    """"""\n    tf.layers.Dropout doesn\'t work as it can\'t switch training or inference\n    """"""\n    if dropout > 0:\n        input_shape = tf.shape(x)\n        noise_shape = [input_shape[0], input_shape[1], 1]\n        x = tf.layers.Dropout(rate=dropout, noise_shape=noise_shape, seed=seed)(x, training=training)\n    return x\n\n\n#### Step 2\ndef fasttext(x):\n    return x\n\n\ndef timedistributed_conv1d(x, filter_size):\n    """"""not working""""""\n    # None * embed_dim * step_dim\n    input_shape = tf.shape(x)\n    step_dim = input_shape[1]\n    embed_dim = input_shape[2]\n    x = tf.transpose(x, [0, 2, 1])\n    # None * embed_dim * step_dim\n    x = tf.reshape(x, [input_shape[0] * embed_dim, step_dim, 1])\n    conv = tf.layers.Conv1D(\n        filters=1,\n        kernel_size=filter_size,\n        padding=""same"",\n        activation=None,\n        strides=1)(x)\n    conv = tf.reshape(conv, [input_shape[0], embed_dim, step_dim])\n    conv = tf.transpose(conv, [0, 2, 1])\n    return conv\n\n\ndef textcnn(x, num_filters=8, filter_sizes=[2, 3], timedistributed=False):\n    # x: None * step_dim * embed_dim\n    conv_blocks = []\n    for i, filter_size in enumerate(filter_sizes):\n        if timedistributed:\n            conv = timedistributed_conv1d(x, filter_size)\n        else:\n            conv = tf.layers.Conv1D(\n                filters=num_filters,\n                kernel_size=filter_size,\n                padding=""same"",\n                activation=None,\n                strides=1)(x)\n        conv = tf.layers.BatchNormalization()(conv)\n        conv = tf.nn.relu(conv)\n        conv_blocks.append(conv)\n    if len(conv_blocks) > 1:\n        z = tf.concat(conv_blocks, axis=-1)\n    else:\n        z = conv_blocks[0]\n    return z\n\n\ndef textrnn(x, num_units, cell_type, sequence_length, mask_zero=False, scope=None):\n    if cell_type == ""gru"":\n        cell_fw = tf.nn.rnn_cell.GRUCell(num_units)\n    elif cell_type == ""lstm"":\n        cell_fw = tf.nn.rnn_cell.LSTMCell(num_units)\n    if mask_zero:\n        x, _ = tf.nn.dynamic_rnn(cell_fw, x, dtype=tf.float32, sequence_length=sequence_length, scope=scope)\n    else:\n        x, _ = tf.nn.dynamic_rnn(cell_fw, x, dtype=tf.float32, sequence_length=None, scope=scope)\n    return x\n\n\ndef textbirnn(x, num_units, cell_type, sequence_length, mask_zero=False, scope=None):\n    if cell_type == ""gru"":\n        cell_fw = tf.nn.rnn_cell.GRUCell(num_units)\n        cell_bw = tf.nn.rnn_cell.GRUCell(num_units)\n    elif cell_type == ""lstm"":\n        cell_fw = tf.nn.rnn_cell.LSTMCell(num_units)\n        cell_bw = tf.nn.rnn_cell.LSTMCell(num_units)\n    if mask_zero:\n        (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(\n            cell_fw, cell_bw, x, dtype=tf.float32, sequence_length=sequence_length, scope=scope)\n    else:\n        (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(\n            cell_fw, cell_bw, x, dtype=tf.float32, sequence_length=None, scope=scope)\n    x = 0.5 * (output_fw + output_bw)\n    return x\n\n\ndef encode(x, method, params, sequence_length, mask_zero=False, scope=None):\n    """"""\n    :param x: shape=(None,seqlen,dim)\n    :param params:\n    :return: shape=(None,seqlen,dim)\n    """"""\n    if method == ""fasttext"":\n        z = fasttext(x)\n    elif method == ""textcnn"":\n        z = textcnn(x, num_filters=params[""cnn_num_filters""], filter_sizes=params[""cnn_filter_sizes""],\n                    timedistributed=params[""cnn_timedistributed""])\n    elif method == ""textrnn"":\n        z = textrnn(x, num_units=params[""rnn_num_units""], cell_type=params[""rnn_cell_type""],\n                    sequence_length=sequence_length, mask_zero=mask_zero, scope=scope)\n    elif method == ""textbirnn"":\n        z = textbirnn(x, num_units=params[""rnn_num_units""], cell_type=params[""rnn_cell_type""],\n                      sequence_length=sequence_length, mask_zero=mask_zero, scope=scope)\n    elif method == ""fasttext+textcnn"":\n        z_f = fasttext(x)\n        z_c = textcnn(x, num_filters=params[""cnn_num_filters""], filter_sizes=params[""cnn_filter_sizes""],\n                      timedistributed=params[""cnn_timedistributed""])\n        z = tf.concat([z_f, z_c], axis=-1)\n    elif method == ""fasttext+textrnn"":\n        z_f = fasttext(x)\n        z_r = textrnn(x, num_units=params[""rnn_num_units""], cell_type=params[""rnn_cell_type""],\n                      sequence_length=sequence_length, mask_zero=mask_zero, scope=scope)\n        z = tf.concat([z_f, z_r], axis=-1)\n    elif method == ""fasttext+textbirnn"":\n        z_f = fasttext(x)\n        z_b = textbirnn(x, num_units=params[""rnn_num_units""], cell_type=params[""rnn_cell_type""],\n                        sequence_length=sequence_length, mask_zero=mask_zero, scope=scope)\n        z = tf.concat([z_f, z_b], axis=-1)\n    elif method == ""fasttext+textcnn+textrnn"":\n        z_f = fasttext(x)\n        z_c = textcnn(x, num_filters=params[""cnn_num_filters""], filter_sizes=params[""cnn_filter_sizes""],\n                      timedistributed=params[""cnn_timedistributed""])\n        z_r = textrnn(x, num_units=params[""rnn_num_units""], cell_type=params[""rnn_cell_type""],\n                      sequence_length=sequence_length, mask_zero=mask_zero, scope=scope)\n        z = tf.concat([z_f, z_c, z_r], axis=-1)\n    elif method == ""fasttext+textcnn+textbirnn"":\n        z_f = fasttext(x)\n        z_c = textcnn(x, num_filters=params[""cnn_num_filters""], filter_sizes=params[""cnn_filter_sizes""],\n                      timedistributed=params[""cnn_timedistributed""])\n        z_b = textbirnn(x, num_units=params[""rnn_num_units""], cell_type=params[""rnn_cell_type""],\n                        sequence_length=sequence_length, mask_zero=mask_zero, scope=scope)\n        z = tf.concat([z_f, z_c, z_b], axis=-1)\n    return z\n\n\n#### Step 3\ndef attention(x, feature_dim, sequence_length, mask_zero=False, maxlen=None, epsilon=1e-8, seed=0):\n    input_shape = tf.shape(x)\n    step_dim = input_shape[1]\n    # feature_dim = input_shape[2]\n    x = tf.reshape(x, [-1, feature_dim])\n    """"""\n    The last dimension of the inputs to `Dense` should be defined. Found `None`.\n\n    cann\'t not use `tf.layers.Dense` here\n    eij = tf.layers.Dense(1)(x)\n\n    see: https://github.com/tensorflow/tensorflow/issues/13348\n    workaround: specify the feature_dim as input\n    """"""\n\n    eij = tf.layers.Dense(1, activation=tf.nn.tanh, kernel_initializer=tf.glorot_uniform_initializer(seed=seed),\n                          dtype=tf.float32, bias_initializer=tf.zeros_initializer())(x)\n    eij = tf.reshape(eij, [-1, step_dim])\n    a = tf.exp(eij)\n\n    # apply mask after the exp. will be re-normalized next\n    if mask_zero:\n        # None * step_dim\n        mask = tf.sequence_mask(sequence_length, maxlen)\n        mask = tf.cast(mask, tf.float32)\n        a = a * mask\n\n    # in some cases especially in the early stages of training the sum may be almost zero\n    a /= tf.cast(tf.reduce_sum(a, axis=1, keep_dims=True) + epsilon, tf.float32)\n\n    a = tf.expand_dims(a, axis=-1)\n    return a\n\n\ndef attend(x, sequence_length=None, method=""ave"", context=None, feature_dim=None, mask_zero=False, maxlen=None,\n           epsilon=1e-8, bn=True, training=False, seed=0, reuse=True, name=""attend""):\n    if method == ""ave"":\n        if mask_zero:\n            # None * step_dim\n            mask = tf.sequence_mask(sequence_length, maxlen)\n            mask = tf.reshape(mask, (-1, tf.shape(x)[1], 1))\n            mask = tf.cast(mask, tf.float32)\n            z = tf.reduce_sum(x * mask, axis=1)\n            l = tf.reduce_sum(mask, axis=1)\n            # in some cases especially in the early stages of training the sum may be almost zero\n            z /= tf.cast(l + epsilon, tf.float32)\n        else:\n            z = tf.reduce_mean(x, axis=1)\n    elif method == ""sum"":\n        if mask_zero:\n            # None * step_dim\n            mask = tf.sequence_mask(sequence_length, maxlen)\n            mask = tf.reshape(mask, (-1, tf.shape(x)[1], 1))\n            mask = tf.cast(mask, tf.float32)\n            z = tf.reduce_sum(x * mask, axis=1)\n        else:\n            z = tf.reduce_sum(x, axis=1)\n    elif method == ""max"":\n        if mask_zero:\n            # None * step_dim\n            mask = tf.sequence_mask(sequence_length, maxlen)\n            mask = tf.expand_dims(mask, axis=-1)\n            mask = tf.tile(mask, (1, 1, tf.shape(x)[2]))\n            masked_data = tf.where(tf.equal(mask, tf.zeros_like(mask)),\n                                   tf.ones_like(x) * -np.inf, x)  # if masked assume value is -inf\n            z = tf.reduce_max(masked_data, axis=1)\n        else:\n            z = tf.reduce_max(x, axis=1)\n    elif method == ""attention"":\n        if context is not None:\n            step_dim = tf.shape(x)[1]\n            context = tf.expand_dims(context, axis=1)\n            context = tf.tile(context, [1, step_dim, 1])\n            y = tf.concat([x, context], axis=-1)\n        else:\n            y = x\n        a = attention(y, feature_dim, sequence_length, mask_zero, maxlen, seed=seed)\n        z = tf.reduce_sum(x * a, axis=1)\n    if bn:\n        # training=False has slightly better performance\n        z = tf.layers.BatchNormalization()(z, training=False)\n        # z = batch_normalization(z, training=training, name=name)\n    return z\n\n\n#### Step 4\ndef _dense_block_mode1(x, hidden_units, dropouts, densenet=False, training=False, seed=0, bn=False, name=""dense_block""):\n    """"""\n    :param x:\n    :param hidden_units:\n    :param dropouts:\n    :param densenet: enable densenet\n    :return:\n    Ref: https://github.com/titu1994/DenseNet\n    """"""\n    for i, (h, d) in enumerate(zip(hidden_units, dropouts)):\n        z = tf.layers.Dense(h, kernel_initializer=tf.glorot_uniform_initializer(seed=seed * i),\n                            dtype=tf.float32,\n                            bias_initializer=tf.zeros_initializer())(x)\n        if bn:\n            z = batch_normalization(z, training=training, name=name+""-""+str(i))\n        z = tf.nn.relu(z)\n        # z = tf.nn.selu(z)\n        z = tf.layers.Dropout(d, seed=seed * i)(z, training=training) if d > 0 else z\n        if densenet:\n            x = tf.concat([x, z], axis=-1)\n        else:\n            x = z\n    return x\n\n\ndef _dense_block_mode2(x, hidden_units, dropouts, densenet=False, training=False, seed=0, bn=False, name=""dense_block""):\n    """"""\n    :param x:\n    :param hidden_units:\n    :param dropouts:\n    :param densenet: enable densenet\n    :return:\n    Ref: https://github.com/titu1994/DenseNet\n    """"""\n    for i, (h, d) in enumerate(zip(hidden_units, dropouts)):\n        if bn:\n            z = batch_normalization(x, training=training, name=name + ""-"" + str(i))\n        z = tf.nn.relu(z)\n        z = tf.layers.Dropout(d, seed=seed * i)(z, training=training) if d > 0 else z\n        z = tf.layers.Dense(h, kernel_initializer=tf.glorot_uniform_initializer(seed=seed * i), dtype=tf.float32,\n                            bias_initializer=tf.zeros_initializer())(z)\n        if densenet:\n            x = tf.concat([x, z], axis=-1)\n        else:\n            x = z\n    return x\n\n\ndef dense_block(x, hidden_units, dropouts, densenet=False, training=False, seed=0, bn=False, name=""dense_block""):\n    return _dense_block_mode1(x, hidden_units, dropouts, densenet, training, seed, bn, name)\n\n\ndef _resnet_branch_mode1(x, hidden_units, dropouts, training, seed=0):\n    h1, h2, h3 = hidden_units\n    dr1, dr2, dr3 = dropouts\n    # branch 2\n    x2 = tf.layers.Dense(h1, kernel_initializer=tf.glorot_uniform_initializer(seed=seed * 2), dtype=tf.float32,\n                         bias_initializer=tf.zeros_initializer())(x)\n    x2 = tf.layers.BatchNormalization()(x2)\n    x2 = tf.nn.relu(x2)\n    x2 = tf.layers.Dropout(dr1, seed=seed * 1)(x2, training=training) if dr1 > 0 else x2\n\n    x2 = tf.layers.Dense(h2, kernel_initializer=tf.glorot_uniform_initializer(seed=seed * 3), dtype=tf.float32,\n                         bias_initializer=tf.zeros_initializer())(x2)\n    x2 = tf.layers.BatchNormalization()(x2)\n    x2 = tf.nn.relu(x2)\n    x2 = tf.layers.Dropout(dr2, seed=seed * 2)(x2, training=training) if dr2 > 0 else x2\n\n    x2 = tf.layers.Dense(h3, kernel_initializer=tf.glorot_uniform_initializer(seed=seed * 4), dtype=tf.float32,\n                         bias_initializer=tf.zeros_initializer())(x2)\n    x2 = tf.layers.BatchNormalization()(x2)\n\n    return x2\n\n\ndef _resnet_block_mode1(x, hidden_units, dropouts, cardinality=1, dense_shortcut=False, training=False, seed=0):\n    """"""A block that has a dense layer at shortcut.\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: default 3, the kernel size of middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: \'a\',\'b\'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    Note that from stage 3, the first conv layer at main path is with strides=(2,2)\n    And the shortcut should have strides=(2,2) as well\n    """"""\n    h1, h2, h3 = hidden_units\n    dr1, dr2, dr3 = dropouts\n\n    xs = []\n    # branch 0\n    if dense_shortcut:\n        x0 = tf.layers.Dense(h3, kernel_initializer=tf.glorot_uniform_initializer(seed=seed * 1), dtype=tf.float32,\n                             bias_initializer=tf.zeros_initializer())(x)\n        x0 = tf.layers.BatchNormalization()(x0)\n        xs.append(x0)\n    else:\n        xs.append(x)\n\n    # branch 1 ~ cardinality\n    for i in range(cardinality):\n        xs.append(_resnet_branch_mode1(x, hidden_units, dropouts, training, seed))\n\n    x = tf.add_n(xs)\n    x = tf.nn.relu(x)\n    x = tf.layers.Dropout(dr3, seed=seed * 4)(x, training=training) if dr3 > 0 else x\n    return x\n\n\ndef _resnet_branch_mode2(x, hidden_units, dropouts, training=False, seed=0):\n    h1, h2, h3 = hidden_units\n    dr1, dr2, dr3 = dropouts\n    # branch 2: bn-relu->weight\n    x2 = tf.layers.BatchNormalization()(x)\n    x2 = tf.nn.relu(x2)\n    x2 = tf.layers.Dropout(dr1)(x2, training=training) if dr1 > 0 else x2\n    x2 = tf.layers.Dense(h1, kernel_initializer=tf.glorot_uniform_initializer(seed * 1), dtype=tf.float32,\n                         bias_initializer=tf.zeros_initializer())(x2)\n\n    x2 = tf.layers.BatchNormalization()(x2)\n    x2 = tf.nn.relu(x2)\n    x2 = tf.layers.Dropout(dr2)(x2, training=training) if dr2 > 0 else x2\n    x2 = tf.layers.Dense(h2, kernel_initializer=tf.glorot_uniform_initializer(seed * 2), dtype=tf.float32,\n                         bias_initializer=tf.zeros_initializer())(x2)\n\n    x2 = tf.layers.BatchNormalization()(x2)\n    x2 = tf.nn.relu(x2)\n    x2 = tf.layers.Dropout(dr3)(x2, training=training) if dr3 > 0 else x2\n    x2 = tf.layers.Dense(h3, kernel_initializer=tf.glorot_uniform_initializer(seed * 3), dtype=tf.float32,\n                         bias_initializer=tf.zeros_initializer())(x2)\n\n    return x2\n\n\ndef _resnet_block_mode2(x, hidden_units, dropouts, cardinality=1, dense_shortcut=False, training=False, seed=0):\n    """"""A block that has a dense layer at shortcut.\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: default 3, the kernel size of middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: \'a\',\'b\'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    Note that from stage 3, the first conv layer at main path is with strides=(2,2)\n    And the shortcut should have strides=(2,2) as well\n    """"""\n    h1, h2, h3 = hidden_units\n    dr1, dr2, dr3 = dropouts\n\n    xs = []\n    # branch 0\n    if dense_shortcut:\n        x0 = tf.layers.Dense(h3, kernel_initializer=tf.glorot_uniform_initializer(seed * 1), dtype=tf.float32,\n                             bias_initializer=tf.zeros_initializer())(x)\n        xs.append(x0)\n    else:\n        xs.append(x)\n\n    # branch 1 ~ cardinality\n    for i in range(cardinality):\n        xs.append(_resnet_branch_mode2(x, hidden_units, dropouts, training, seed))\n\n    x = tf.add_n(xs)\n    return x\n\n\ndef resnet_block(input_tensor, hidden_units, dropouts, cardinality=1, dense_shortcut=False, training=False, seed=0):\n    return _resnet_block_mode2(input_tensor, hidden_units, dropouts, cardinality, dense_shortcut, training, seed)\n\n'"
code/optimizer.py,31,"b'\n""""""\nhttps://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwih7-6VlejYAhWGS98KHWeLCWQQFgg3MAE&url=https%3A%2F%2Fwww.bigdatarepublic.nl%2Fcustom-optimizer-in-tensorflow%2F&usg=AOvVaw3jmxRDqr2pkGRLvX6rNJrl\n""""""\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.training import optimizer\nfrom tensorflow.python.training import training_ops\n\n\nclass LazyPowerSignOptimizer(optimizer.Optimizer):\n    """"""Implementation of PowerSign.\n    See [Bello et. al., 2017](https://arxiv.org/abs/1709.07417)\n    @@__init__\n    """"""\n\n    def __init__(self, learning_rate=0.001, alpha=0.01, beta=0.5, use_locking=False, name=""PowerSign""):\n        super(LazyPowerSignOptimizer, self).__init__(use_locking, name)\n        self._lr = learning_rate\n        self._alpha = alpha\n        self._beta = beta\n\n        # Tensor versions of the constructor arguments, created in _prepare().\n        self._lr_t = None\n        self._alpha_t = None\n        self._beta_t = None\n\n    def _prepare(self):\n        self._lr_t = ops.convert_to_tensor(self._lr, name=""learning_rate"")\n        self._alpha_t = ops.convert_to_tensor(self._beta, name=""alpha_t"")\n        self._beta_t = ops.convert_to_tensor(self._beta, name=""beta_t"")\n\n    def _create_slots(self, var_list):\n        # Create slots for the first and second moments.\n        for v in var_list:\n            self._zeros_slot(v, ""m"", self._name)\n\n    def _apply_dense(self, grad, var):\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n        alpha_t = math_ops.cast(self._alpha_t, var.dtype.base_dtype)\n        beta_t = math_ops.cast(self._beta_t, var.dtype.base_dtype)\n\n        eps = 1e-7  # cap for moving average\n\n        m = self.get_slot(var, ""m"")\n        m_t = m.assign(tf.maximum(beta_t * m + eps, tf.abs(grad)))\n\n        var_update = state_ops.assign_sub(var, lr_t * grad * tf.exp(\n            tf.log(alpha_t) * tf.sign(grad) * tf.sign(m_t)))  # Update \'ref\' by subtracting \'value\n        # Create an op that groups multiple operations.\n        # When this op finishes, all ops in input have finished\n        return control_flow_ops.group(*[var_update, m_t])\n\n    def _apply_sparse(self, grad, var):\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n        alpha_t = math_ops.cast(self._alpha_t, var.dtype.base_dtype)\n        beta_t = math_ops.cast(self._beta_t, var.dtype.base_dtype)\n\n        eps = 1e-7  # cap for moving average\n\n        m = self.get_slot(var, ""m"")\n        m_slice = tf.gather(m, grad.indices)\n        m_t = state_ops.scatter_update(m, grad.indices,\n                                       tf.maximum(beta_t * m_slice + eps, tf.abs(grad.values)))\n        m_t_slice = tf.gather(m_t, grad.indices)\n\n        var_update = state_ops.scatter_sub(var, grad.indices, lr_t * grad.values * tf.exp(\n            tf.log(alpha_t) * tf.sign(grad.values) * tf.sign(m_t_slice)))  # Update \'ref\' by subtracting \'value\n        # Create an op that groups multiple operations.\n        # When this op finishes, all ops in input have finished\n        return control_flow_ops.group(*[var_update, m_t])\n\n\nclass LazyAddSignOptimizer(optimizer.Optimizer):\n    """"""Implementation of AddSign.\n    See [Bello et. al., 2017](https://arxiv.org/abs/1709.07417)\n    @@__init__\n    """"""\n\n    def __init__(self, learning_rate=1.001, alpha=0.01, beta=0.5, use_locking=False, name=""AddSign""):\n        super(LazyAddSignOptimizer, self).__init__(use_locking, name)\n        self._lr = learning_rate\n        self._alpha = alpha\n        self._beta = beta\n\n        # Tensor versions of the constructor arguments, created in _prepare().\n        self._lr_t = None\n        self._alpha_t = None\n        self._beta_t = None\n\n    def _prepare(self):\n        self._lr_t = ops.convert_to_tensor(self._lr, name=""learning_rate"")\n        self._alpha_t = ops.convert_to_tensor(self._beta, name=""beta_t"")\n        self._beta_t = ops.convert_to_tensor(self._beta, name=""beta_t"")\n\n    def _create_slots(self, var_list):\n        # Create slots for the first and second moments.\n        for v in var_list:\n            self._zeros_slot(v, ""m"", self._name)\n\n    def _apply_dense(self, grad, var):\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n        beta_t = math_ops.cast(self._beta_t, var.dtype.base_dtype)\n        alpha_t = math_ops.cast(self._alpha_t, var.dtype.base_dtype)\n\n        eps = 1e-7  # cap for moving average\n\n        m = self.get_slot(var, ""m"")\n        m_t = m.assign(tf.maximum(beta_t * m + eps, tf.abs(grad)))\n\n        var_update = state_ops.assign_sub(var, lr_t * grad * (1.0 + alpha_t * tf.sign(grad) * tf.sign(m_t)))\n        # Create an op that groups multiple operations\n        # When this op finishes, all ops in input have finished\n        return control_flow_ops.group(*[var_update, m_t])\n\n    def _apply_sparse(self, grad, var):\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n        beta_t = math_ops.cast(self._beta_t, var.dtype.base_dtype)\n        alpha_t = math_ops.cast(self._alpha_t, var.dtype.base_dtype)\n\n        eps = 1e-7  # cap for moving average\n\n        m = self.get_slot(var, ""m"")\n        m_slice = tf.gather(m, grad.indices)\n        m_t = state_ops.scatter_update(m, grad.indices,\n                                       tf.maximum(beta_t * m_slice + eps, tf.abs(grad.values)))\n        m_t_slice = tf.gather(m_t, grad.indices)\n\n        var_update = state_ops.scatter_sub(var, grad.indices,\n                                           lr_t * grad.values * (\n                                                   1.0 + alpha_t * tf.sign(grad.values) * tf.sign(m_t_slice)))\n\n        # Create an op that groups multiple operations\n        # When this op finishes, all ops in input have finished\n        return control_flow_ops.group(*[var_update, m_t])\n\n\nclass LazyAMSGradOptimizer(optimizer.Optimizer):\n    def __init__(self, learning_rate=0.002, beta1=0.9, beta2=0.999, epsilon=1e-8,\n                 use_locking=False, name=""AMSGrad""):\n        super(LazyAMSGradOptimizer, self).__init__(use_locking, name)\n        self._lr = learning_rate\n        self._beta1 = beta1\n        self._beta2 = beta2\n        self._epsilon = epsilon\n\n        # Tensor versions of the constructor arguments, created in _prepare().\n        self._lr_t = None\n        self._beta1_t = None\n        self._beta2_t = None\n        self._epsilon_t = None\n\n    def _prepare(self):\n        self._lr_t = ops.convert_to_tensor(self._lr, name=""learning_rate"")\n        self._beta1_t = ops.convert_to_tensor(self._beta1, name=""beta1"")\n        self._beta2_t = ops.convert_to_tensor(self._beta2, name=""beta2"")\n        self._epsilon_t = ops.convert_to_tensor(self._epsilon, name=""epsilon"")\n\n    def _create_slots(self, var_list):\n        # Create slots for the first and second moments.\n        for v in var_list:\n            self._zeros_slot(v, ""m"", self._name)\n            self._zeros_slot(v, ""v"", self._name)\n            self._zeros_slot(v, ""v_prime"", self._name)\n\n    def _apply_dense(self, grad, var):\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n\n        # the following equations given in [1]\n        # m_t = beta1 * m + (1 - beta1) * g_t\n        m = self.get_slot(var, ""m"")\n        m_t = state_ops.assign(m, beta1_t * m + (1. - beta1_t) * grad, use_locking=self._use_locking)\n\n        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n        v = self.get_slot(var, ""v"")\n        v_t = state_ops.assign(v, beta2_t * v + (1. - beta2_t) * tf.square(grad), use_locking=self._use_locking)\n        v_prime = self.get_slot(var, ""v_prime"")\n        v_t_prime = state_ops.assign(v_prime, tf.maximum(v_prime, v_t))\n\n        var_update = state_ops.assign_sub(var,\n                                          lr_t * m_t / (tf.sqrt(v_t_prime) + epsilon_t),\n                                          use_locking=self._use_locking)\n\n        return control_flow_ops.group(*[var_update, m_t, v_t, v_t_prime])\n\n    # keras Nadam update rule\n    def _apply_sparse(self, grad, var):\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n\n        # the following equations given in [1]\n        # m_t = beta1 * m + (1 - beta1) * g_t\n        m = self.get_slot(var, ""m"")\n        m_t = state_ops.scatter_update(m, grad.indices,\n                                       beta1_t * array_ops.gather(m, grad.indices) +\n                                       (1. - beta1_t) * grad.values,\n                                       use_locking=self._use_locking)\n        m_t_slice = tf.gather(m_t, grad.indices)\n\n        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n        v = self.get_slot(var, ""v"")\n        v_t = state_ops.scatter_update(v, grad.indices,\n                                       beta2_t * array_ops.gather(v, grad.indices) +\n                                       (1. - beta2_t) * tf.square(grad.values),\n                                       use_locking=self._use_locking)\n        v_prime = self.get_slot(var, ""v_prime"")\n        v_t_slice = tf.gather(v_t, grad.indices)\n        v_prime_slice = tf.gather(v_prime, grad.indices)\n        v_t_prime = state_ops.scatter_update(v_prime, grad.indices, tf.maximum(v_prime_slice, v_t_slice))\n\n        v_t_prime_slice = array_ops.gather(v_t_prime, grad.indices)\n        var_update = state_ops.scatter_sub(var, grad.indices,\n                                           lr_t * m_t_slice / (math_ops.sqrt(v_t_prime_slice) + epsilon_t),\n                                           use_locking=self._use_locking)\n\n        return control_flow_ops.group(*[var_update, m_t, v_t, v_t_prime])\n\n\nclass LazyNadamOptimizer(optimizer.Optimizer):\n    def __init__(self, learning_rate=0.002, beta1=0.9, beta2=0.999, epsilon=1e-8,\n                 schedule_decay=0.004, use_locking=False, name=""Nadam""):\n        super(LazyNadamOptimizer, self).__init__(use_locking, name)\n        self._lr = learning_rate\n        self._beta1 = beta1\n        self._beta2 = beta2\n        self._epsilon = epsilon\n        self._schedule_decay = schedule_decay\n        # momentum cache decay\n        self._momentum_cache_decay = tf.cast(0.96, tf.float32)\n        self._momentum_cache_const = tf.pow(self._momentum_cache_decay, 1. * schedule_decay)\n\n        # Tensor versions of the constructor arguments, created in _prepare().\n        self._lr_t = None\n        self._beta1_t = None\n        self._beta2_t = None\n        self._epsilon_t = None\n        self._schedule_decay_t = None\n\n        # Variables to accumulate the powers of the beta parameters.\n        # Created in _create_slots when we know the variables to optimize.\n        self._beta1_power = None\n        self._beta2_power = None\n        self._iterations = None\n        self._m_schedule = None\n\n        # Created in SparseApply if needed.\n        self._updated_lr = None\n\n    def _prepare(self):\n        self._lr_t = ops.convert_to_tensor(self._lr, name=""learning_rate"")\n        self._beta1_t = ops.convert_to_tensor(self._beta1, name=""beta1"")\n        self._beta2_t = ops.convert_to_tensor(self._beta2, name=""beta2"")\n        self._epsilon_t = ops.convert_to_tensor(self._epsilon, name=""epsilon"")\n        self._schedule_decay_t = ops.convert_to_tensor(self._schedule_decay, name=""schedule_decay"")\n\n    def _create_slots(self, var_list):\n        # Create the beta1 and beta2 accumulators on the same device as the first\n        # variable. Sort the var_list to make sure this device is consistent across\n        # workers (these need to go on the same PS, otherwise some updates are\n        # silently ignored).\n        first_var = min(var_list, key=lambda x: x.name)\n\n        create_new = self._iterations is None\n        if not create_new and context.in_graph_mode():\n            create_new = (self._iterations.graph is not first_var.graph)\n\n        if create_new:\n            with ops.colocate_with(first_var):\n                self._beta1_power = variable_scope.variable(self._beta1,\n                                                            name=""beta1_power"",\n                                                            trainable=False)\n                self._beta2_power = variable_scope.variable(self._beta2,\n                                                            name=""beta2_power"",\n                                                            trainable=False)\n                self._iterations = variable_scope.variable(0.,\n                                                           name=""iterations"",\n                                                           trainable=False)\n                self._m_schedule = variable_scope.variable(1.,\n                                                           name=""m_schedule"",\n                                                           trainable=False)\n        # Create slots for the first and second moments.\n        for v in var_list:\n            self._zeros_slot(v, ""m"", self._name)\n            self._zeros_slot(v, ""v"", self._name)\n\n    def _get_momentum_cache(self, schedule_decay_t, t):\n        return tf.pow(self._momentum_cache_decay, t * schedule_decay_t)\n        # return beta1_t * (1. - 0.5 * (tf.pow(self._momentum_cache_decay, t * schedule_decay_t)))\n\n    """"""very slow\n    we simply use the nadam update rule without warming momentum schedule\n    def _apply_dense(self, grad, var):\n        t = math_ops.cast(self._iterations, var.dtype.base_dtype) + 1.\n        m_schedule = math_ops.cast(self._m_schedule, var.dtype.base_dtype)\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n        schedule_decay_t = math_ops.cast(self._schedule_decay_t, var.dtype.base_dtype)\n\n        # Due to the recommendations in [2], i.e. warming momentum schedule\n        # see keras Nadam\n        momentum_cache_t = self._get_momentum_cache(beta1_t, schedule_decay_t, t)\n        momentum_cache_t_1 = self._get_momentum_cache(beta1_t, schedule_decay_t, t+1.)\n        m_schedule_new = m_schedule * momentum_cache_t\n        m_schedule_next = m_schedule * momentum_cache_t * momentum_cache_t_1\n\n        # the following equations given in [1]\n        # m_t = beta1 * m + (1 - beta1) * g_t\n        m = self.get_slot(var, ""m"")\n        m_t = state_ops.assign(m, beta1_t * m + (1. - beta1_t) * grad, use_locking=self._use_locking)\n        g_prime = grad / (1. - m_schedule_new)\n        m_t_prime = m_t / (1. - m_schedule_next)\n        m_t_bar = (1. - momentum_cache_t) * g_prime + momentum_cache_t_1 * m_t_prime\n\n        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n        v = self.get_slot(var, ""v"")\n        v_t = state_ops.assign(v, beta2_t * v + (1. - beta2_t) * tf.square(grad), use_locking=self._use_locking)\n        v_t_prime = v_t / (1. - tf.pow(beta2_t, t))\n\n        var_update = state_ops.assign_sub(var,\n                                      lr_t * m_t_bar / (tf.sqrt(v_t_prime) + epsilon_t),\n                                      use_locking=self._use_locking)\n\n        return control_flow_ops.group(*[var_update, m_t, v_t])\n    """"""\n\n    # nadam update rule without warming momentum schedule\n    def _apply_dense(self, grad, var):\n        m = self.get_slot(var, ""m"")\n        v = self.get_slot(var, ""v"")\n        return training_ops.apply_adam(\n            var,\n            m,\n            v,\n            math_ops.cast(self._beta1_power, var.dtype.base_dtype),\n            math_ops.cast(self._beta2_power, var.dtype.base_dtype),\n            math_ops.cast(self._lr_t, var.dtype.base_dtype),\n            math_ops.cast(self._beta1_t, var.dtype.base_dtype),\n            math_ops.cast(self._beta2_t, var.dtype.base_dtype),\n            math_ops.cast(self._epsilon_t, var.dtype.base_dtype),\n            grad,\n            use_locking=self._use_locking,\n            use_nesterov=True).op\n\n    def _resource_apply_dense(self, grad, var):\n        m = self.get_slot(var, ""m"")\n        v = self.get_slot(var, ""v"")\n        return training_ops.resource_apply_adam(\n            var.handle,\n            m.handle,\n            v.handle,\n            math_ops.cast(self._beta1_power, grad.dtype.base_dtype),\n            math_ops.cast(self._beta2_power, grad.dtype.base_dtype),\n            math_ops.cast(self._lr_t, grad.dtype.base_dtype),\n            math_ops.cast(self._beta1_t, grad.dtype.base_dtype),\n            math_ops.cast(self._beta2_t, grad.dtype.base_dtype),\n            math_ops.cast(self._epsilon_t, grad.dtype.base_dtype),\n            grad,\n            use_locking=self._use_locking,\n            use_nesterov=True)\n\n    # keras Nadam update rule\n    def _apply_sparse(self, grad, var):\n        t = math_ops.cast(self._iterations, var.dtype.base_dtype) + 1.\n        m_schedule = math_ops.cast(self._m_schedule, var.dtype.base_dtype)\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n        schedule_decay_t = math_ops.cast(self._schedule_decay_t, var.dtype.base_dtype)\n\n        # Due to the recommendations in [2], i.e. warming momentum schedule\n        momentum_cache_power = self._get_momentum_cache(schedule_decay_t, t)\n        momentum_cache_t = beta1_t * (1. - 0.5 * momentum_cache_power)\n        momentum_cache_t_1 = beta1_t * (1. - 0.5 * momentum_cache_power * self._momentum_cache_const)\n        m_schedule_new = m_schedule * momentum_cache_t\n        m_schedule_next = m_schedule_new * momentum_cache_t_1\n\n        # the following equations given in [1]\n        # m_t = beta1 * m + (1 - beta1) * g_t\n        m = self.get_slot(var, ""m"")\n        m_t = state_ops.scatter_update(m, grad.indices,\n                                       beta1_t * array_ops.gather(m, grad.indices) +\n                                       (1. - beta1_t) * grad.values,\n                                       use_locking=self._use_locking)\n        g_prime_slice = grad.values / (1. - m_schedule_new)\n        m_t_prime_slice = array_ops.gather(m_t, grad.indices) / (1. - m_schedule_next)\n        m_t_bar_slice = (1. - momentum_cache_t) * g_prime_slice + momentum_cache_t_1 * m_t_prime_slice\n\n        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n        v = self.get_slot(var, ""v"")\n        v_t = state_ops.scatter_update(v, grad.indices,\n                                       beta2_t * array_ops.gather(v, grad.indices) +\n                                       (1. - beta2_t) * tf.square(grad.values),\n                                       use_locking=self._use_locking)\n        v_t_prime_slice = array_ops.gather(v_t, grad.indices) / (1. - tf.pow(beta2_t, t))\n\n        var_update = state_ops.scatter_sub(var, grad.indices,\n                                           lr_t * m_t_bar_slice / (math_ops.sqrt(v_t_prime_slice) + epsilon_t),\n                                           use_locking=self._use_locking)\n\n        return control_flow_ops.group(*[var_update, m_t, v_t])\n\n    def _finish(self, update_ops, name_scope):\n        # Update the power accumulators.\n        with ops.control_dependencies(update_ops):\n            with ops.colocate_with(self._iterations):\n                update_beta1 = self._beta1_power.assign(\n                    self._beta1_power * self._beta1_t,\n                    use_locking=self._use_locking)\n                update_beta2 = self._beta2_power.assign(\n                    self._beta2_power * self._beta2_t,\n                    use_locking=self._use_locking)\n                t = self._iterations + 1.\n                update_iterations = self._iterations.assign(t, use_locking=self._use_locking)\n                momentum_cache_power = self._get_momentum_cache(self._schedule_decay_t, t)\n                momentum_cache_t = self._beta1_t * (1. - 0.5 * momentum_cache_power)\n                update_m_schedule = self._m_schedule.assign(\n                    self._m_schedule * momentum_cache_t,\n                    use_locking=self._use_locking)\n        return control_flow_ops.group(\n            *update_ops + [update_beta1, update_beta2] + [update_iterations, update_m_schedule],\n            name=name_scope)\n'"
code/topk.py,0,"b'\nfrom collections import defaultdict\nfrom random import randint\n\n# Bucket Sort\n# Time:  O(n + klogk) ~ O(n + nlogn)\n# Space: O(n)\nclass BucketSort(object):\n    def topKFrequent(self, words, k):\n        counts = defaultdict(int)\n        for ws in words:\n            for w in ws:\n                counts[w] += 1\n\n        buckets = [[]] * (sum(counts.values()) + 1)\n        for i, count in counts.items():\n            buckets[count].append(i)\n\n        result = []\n        # result_append = result.append\n        for i in reversed(range(len(buckets))):\n            for j in range(len(buckets[i])):\n                # slower\n                # result_append(buckets[i][j])\n                result.append(buckets[i][j])\n                if len(result) == k:\n                    return result\n        return result\n\n\n# Quick Select\n# Time:  O(n) ~ O(n^2), O(n) on average.\n# Space: O(n)\nclass QuickSelect(object):\n    def topKFrequent(self, words, k):\n        """"""\n        :type words: List[str]\n        :type k: int\n        :rtype: List[str]\n        """"""\n        counts = defaultdict(int)\n        for ws in words:\n            for w in ws:\n                counts[w] += 1\n        p = []\n        for key, val in counts.items():\n            p.append((-val, key))\n        self.kthElement(p, k)\n\n        result = []\n        sorted_p = sorted(p[:k])\n        for i in range(k):\n            result.append(sorted_p[i][1])\n        return result\n\n    def kthElement(self, nums, k):  # O(n) on average\n        def PartitionAroundPivot(left, right, pivot_idx, nums):\n            pivot_value = nums[pivot_idx]\n            new_pivot_idx = left\n            nums[pivot_idx], nums[right] = nums[right], nums[pivot_idx]\n            for i in range(left, right):\n                if nums[i] < pivot_value:\n                    nums[i], nums[new_pivot_idx] = nums[new_pivot_idx], nums[i]\n                    new_pivot_idx += 1\n\n            nums[right], nums[new_pivot_idx] = nums[new_pivot_idx], nums[right]\n            return new_pivot_idx\n\n        left, right = 0, len(nums) - 1\n        while left <= right:\n            pivot_idx = randint(left, right)\n            new_pivot_idx = PartitionAroundPivot(left, right, pivot_idx, nums)\n            if new_pivot_idx == k - 1:\n                return\n            elif new_pivot_idx > k - 1:\n                right = new_pivot_idx - 1\n            else:  # new_pivot_idx < k - 1.\n                left = new_pivot_idx + 1\n\n\n# top_k_selector = BucketSort()\n\ntop_k_selector = QuickSelect()\n'"
code/utils.py,0,"b'\nimport datetime\nimport logging\nimport logging.handlers\nimport os\nimport shutil\n\n\ndef _timestamp():\n    now = datetime.datetime.now()\n    now_str = now.strftime(""%Y%m%d%H%M"")\n    return now_str\n\n\ndef _get_logger(logdir, logname, loglevel=logging.INFO):\n    fmt = ""[%(asctime)s] %(levelname)s: %(message)s""\n    formatter = logging.Formatter(fmt)\n\n    handler = logging.handlers.RotatingFileHandler(\n        filename=os.path.join(logdir, logname),\n        maxBytes=2 * 1024 * 1024 * 1024,\n        backupCount=10)\n    handler.setFormatter(formatter)\n\n    logger = logging.getLogger("""")\n    logger.addHandler(handler)\n    logger.setLevel(loglevel)\n    return logger\n\n\ndef _makedirs(dir, force=False):\n    if os.path.exists(dir):\n        if force:\n            shutil.rmtree(dir)\n            os.makedirs(dir)\n    else:\n        os.makedirs(dir)'"
code/xnn.py,103,"b'import time\nimport numpy as np\nimport tensorflow as tf\n\nfrom lr_schedule import _cosine_decay_restarts, _exponential_decay\nfrom metrics import rmse\nfrom nn_module import embed, encode, attend\nfrom nn_module import word_dropout\nfrom nn_module import dense_block, resnet_block\nfrom optimizer import LazyPowerSignOptimizer, LazyAddSignOptimizer, LazyAMSGradOptimizer, LazyNadamOptimizer\nfrom utils import _makedirs\n\n\nclass XNN(object):\n    def __init__(self, params, target_scaler, logger):\n        self.params = params\n        self.target_scaler = target_scaler\n        self.logger = logger\n        _makedirs(self.params[""model_dir""], force=True)\n        self._init_graph()\n        self.gvars_state_list = []\n\n        # 14\n        self.bias = 0.01228477\n        self.weights = [\n            0.00599607, 0.02999416, 0.05985384, 0.20137787, 0.03178938, 0.04612812,\n            0.05384821, 0.10121514, 0.05915169, 0.05521121, 0.06448063, 0.0944233,\n            0.08306157, 0.11769992\n        ]\n        self.weights = np.array(self.weights).reshape(-1, 1)\n\n    def _init_graph(self):\n        self.graph = tf.Graph()\n        with self.graph.as_default():\n            tf.set_random_seed(self.params[""random_seed""])\n\n            #### input\n            self.training = tf.placeholder(tf.bool, shape=[], name=""training"")\n            # seq\n            self.seq_name = tf.placeholder(tf.int32, shape=[None, None], name=""seq_name"")\n            self.seq_item_desc = tf.placeholder(tf.int32, shape=[None, None], name=""seq_item_desc"")\n            self.seq_category_name = tf.placeholder(tf.int32, shape=[None, None], name=""seq_category_name"")\n            if self.params[""use_bigram""]:\n                self.seq_bigram_item_desc = tf.placeholder(tf.int32, shape=[None, None], name=""seq_bigram_item_desc"")\n            if self.params[""use_trigram""]:\n                self.seq_trigram_item_desc = tf.placeholder(tf.int32, shape=[None, None], name=""seq_trigram_item_desc"")\n            if self.params[""use_subword""]:\n                self.seq_subword_item_desc = tf.placeholder(tf.int32, shape=[None, None], name=""seq_subword_item_desc"")\n\n            # placeholder for length\n            self.sequence_length_name = tf.placeholder(tf.int32, shape=[None], name=""sequence_length_name"")\n            self.sequence_length_item_desc = tf.placeholder(tf.int32, shape=[None], name=""sequence_length_item_desc"")\n            self.sequence_length_category_name = tf.placeholder(tf.int32, shape=[None],\n                                                                name=""sequence_length_category_name"")\n            self.sequence_length_item_desc_subword = tf.placeholder(tf.int32, shape=[None],\n                                                                    name=""sequence_length_item_desc_subword"")\n            self.word_length = tf.placeholder(tf.int32, shape=[None, None], name=""word_length"")\n\n            # other context\n            self.brand_name = tf.placeholder(tf.int32, shape=[None, 1], name=""brand_name"")\n            # self.category_name = tf.placeholder(tf.int32, shape=[None, 1], name=""category_name"")\n            self.category_name1 = tf.placeholder(tf.int32, shape=[None, 1], name=""category_name1"")\n            self.category_name2 = tf.placeholder(tf.int32, shape=[None, 1], name=""category_name2"")\n            self.category_name3 = tf.placeholder(tf.int32, shape=[None, 1], name=""category_name3"")\n            self.item_condition_id = tf.placeholder(tf.int32, shape=[None, 1], name=""item_condition_id"")\n            self.item_condition = tf.placeholder(tf.float32, shape=[None, self.params[""MAX_NUM_CONDITIONS""]], name=""item_condition"")\n            self.shipping = tf.placeholder(tf.int32, shape=[None, 1], name=""shipping"")\n            self.num_vars = tf.placeholder(tf.float32, shape=[None, self.params[""NUM_VARS_DIM""]], name=""num_vars"")\n\n            # target\n            self.target = tf.placeholder(tf.float32, shape=[None, 1], name=""target"")\n\n            #### embed\n            # embed seq\n            # std = np.sqrt(2 / self.params[""embedding_dim""])\n            std = 0.001\n            minval = -std\n            maxval = std\n            emb_word = tf.Variable(\n                tf.random_uniform([self.params[""MAX_NUM_WORDS""] + 1, self.params[""embedding_dim""]], minval, maxval,\n                                  seed=self.params[""random_seed""],\n                                  dtype=tf.float32))\n            # emb_word2 = tf.Variable(tf.random_uniform([self.params[""MAX_NUM_WORDS""] + 1, self.params[""embedding_dim""]], minval, maxval,\n            #                                     seed=self.params[""random_seed""],\n            #                                     dtype=tf.float32))\n            emb_seq_name = tf.nn.embedding_lookup(emb_word, self.seq_name)\n            if self.params[""embedding_dropout""] > 0.:\n                emb_seq_name = word_dropout(emb_seq_name, training=self.training,\n                                            dropout=self.params[""embedding_dropout""],\n                                            seed=self.params[""random_seed""])\n            emb_seq_item_desc = tf.nn.embedding_lookup(emb_word, self.seq_item_desc)\n            if self.params[""embedding_dropout""] > 0.:\n                emb_seq_item_desc = word_dropout(emb_seq_item_desc, training=self.training,\n                                                 dropout=self.params[""embedding_dropout""],\n                                                 seed=self.params[""random_seed""])\n            # emb_seq_category_name = tf.nn.embedding_lookup(emb_word, self.seq_category_name)\n            # if self.params[""embedding_dropout""] > 0.:\n            #     emb_seq_category_name = word_dropout(emb_seq_category_name, training=self.training,\n            #                                      dropout=self.params[""embedding_dropout""],\n            #                                      seed=self.params[""random_seed""])\n            if self.params[""use_bigram""]:\n                emb_seq_bigram_item_desc = embed(self.seq_bigram_item_desc, self.params[""MAX_NUM_BIGRAMS""] + 1,\n                                                 self.params[""embedding_dim""], seed=self.params[""random_seed""])\n                if self.params[""embedding_dropout""] > 0.:\n                    emb_seq_bigram_item_desc = word_dropout(emb_seq_bigram_item_desc, training=self.training,\n                                                            dropout=self.params[""embedding_dropout""],\n                                                            seed=self.params[""random_seed""])\n            if self.params[""use_trigram""]:\n                emb_seq_trigram_item_desc = embed(self.seq_trigram_item_desc, self.params[""MAX_NUM_TRIGRAMS""] + 1,\n                                                  self.params[""embedding_dim""], seed=self.params[""random_seed""])\n                if self.params[""embedding_dropout""] > 0.:\n                    emb_seq_trigram_item_desc = word_dropout(emb_seq_trigram_item_desc, training=self.training,\n                                                             dropout=self.params[""embedding_dropout""],\n                                                             seed=self.params[""random_seed""])\n            if self.params[""use_subword""]:\n                emb_seq_subword_item_desc = embed(self.seq_subword_item_desc, self.params[""MAX_NUM_SUBWORDS""] + 1,\n                                                  self.params[""embedding_dim""], seed=self.params[""random_seed""])\n                if self.params[""embedding_dropout""] > 0.:\n                    emb_seq_subword_item_desc = word_dropout(emb_seq_subword_item_desc, training=self.training,\n                                                             dropout=self.params[""embedding_dropout""],\n                                                             seed=self.params[""random_seed""])\n\n            # embed other context\n            std = 0.001\n            minval = -std\n            maxval = std\n            emb_brand = tf.Variable(\n                tf.random_uniform([self.params[""MAX_NUM_BRANDS""], self.params[""embedding_dim""]], minval, maxval,\n                                  seed=self.params[""random_seed""],\n                                  dtype=tf.float32))\n            emb_brand_name = tf.nn.embedding_lookup(emb_brand, self.brand_name)\n            # emb_brand_name = embed(self.brand_name, self.params[""MAX_NUM_BRANDS""], self.params[""embedding_dim""],\n            #                        flatten=False, seed=self.params[""random_seed""])\n            # emb_category_name = embed(self.category_name, MAX_NUM_CATEGORIES, self.params[""embedding_dim""],\n            #                           flatten=False)\n            emb_category_name1 = embed(self.category_name1, self.params[""MAX_NUM_CATEGORIES_LST""][0], self.params[""embedding_dim""],\n                                       flatten=False, seed=self.params[""random_seed""])\n            emb_category_name2 = embed(self.category_name2, self.params[""MAX_NUM_CATEGORIES_LST""][1], self.params[""embedding_dim""],\n                                       flatten=False, seed=self.params[""random_seed""])\n            emb_category_name3 = embed(self.category_name3, self.params[""MAX_NUM_CATEGORIES_LST""][2], self.params[""embedding_dim""],\n                                       flatten=False, seed=self.params[""random_seed""])\n            emb_item_condition = embed(self.item_condition_id, self.params[""MAX_NUM_CONDITIONS""] + 1, self.params[""embedding_dim""],\n                                       flatten=False, seed=self.params[""random_seed""])\n            emb_shipping = embed(self.shipping, self.params[""MAX_NUM_SHIPPINGS""], self.params[""embedding_dim""],\n                                 flatten=False, seed=self.params[""random_seed""])\n\n            #### encode\n            enc_seq_name = encode(emb_seq_name, method=self.params[""encode_method""],\n                                  params=self.params,\n                                  sequence_length=self.sequence_length_name,\n                                  mask_zero=self.params[""embedding_mask_zero""],\n                                  scope=""enc_seq_name"")\n            enc_seq_item_desc = encode(emb_seq_item_desc, method=self.params[""encode_method""],\n                                       params=self.params, sequence_length=self.sequence_length_item_desc,\n                                       mask_zero=self.params[""embedding_mask_zero""],\n                                       scope=""enc_seq_item_desc"")\n            # enc_seq_category_name = encode(emb_seq_category_name, method=self.params[""encode_method""],\n            #                                params=self.params, sequence_length=self.sequence_length_category_name,\n            #                                mask_zero=self.params[""embedding_mask_zero""],\n            #                                scope=""enc_seq_category_name"")\n            if self.params[""use_bigram""]:\n                enc_seq_bigram_item_desc = encode(emb_seq_bigram_item_desc, method=""fasttext"",\n                                                  params=self.params,\n                                                  sequence_length=self.sequence_length_item_desc,\n                                                  mask_zero=self.params[""embedding_mask_zero""],\n                                                  scope=""enc_seq_bigram_item_desc"")\n            if self.params[""use_trigram""]:\n                enc_seq_trigram_item_desc = encode(emb_seq_trigram_item_desc, method=""fasttext"",\n                                                   params=self.params,\n                                                   sequence_length=self.sequence_length_item_desc,\n                                                   mask_zero=self.params[""embedding_mask_zero""],\n                                                   scope=""enc_seq_trigram_item_desc"")\n            # use fasttext encode method for the following\n            if self.params[""use_subword""]:\n                enc_seq_subword_item_desc = encode(emb_seq_subword_item_desc, method=""fasttext"",\n                                                   params=self.params,\n                                                   sequence_length=self.sequence_length_item_desc_subword,\n                                                   mask_zero=self.params[""embedding_mask_zero""],\n                                                   scope=""enc_seq_subword_item_desc"")\n\n            context = tf.concat([\n                # att_seq_category_name,\n                tf.layers.flatten(emb_brand_name),\n                # tf.layers.flatten(emb_category_name),\n                tf.layers.flatten(emb_category_name1),\n                tf.layers.flatten(emb_category_name2),\n                tf.layers.flatten(emb_category_name3),\n                self.item_condition,\n                tf.cast(self.shipping, tf.float32),\n                self.num_vars],\n                axis=-1, name=""context"")\n            context_size = self.params[""encode_text_dim""] * 0 + \\\n                           self.params[""embedding_dim""] * 4 + \\\n                           self.params[""item_condition_size""] + \\\n                           self.params[""shipping_size""] + \\\n                           self.params[""num_vars_size""]\n\n            feature_dim = context_size + self.params[""encode_text_dim""]\n            # context = None\n            feature_dim = self.params[""encode_text_dim""]\n            att_seq_name = attend(enc_seq_name, method=self.params[""attend_method""],\n                                  context=None, feature_dim=feature_dim,\n                                  sequence_length=self.sequence_length_name,\n                                  maxlen=self.params[""max_sequence_length_name""],\n                                  mask_zero=self.params[""embedding_mask_zero""],\n                                  training=self.training,\n                                  seed=self.params[""random_seed""],\n                                  name=""att_seq_name_attend"")\n            att_seq_item_desc = attend(enc_seq_item_desc, method=self.params[""attend_method""],\n                                       context=None, feature_dim=feature_dim,\n                                       sequence_length=self.sequence_length_item_desc,\n                                       maxlen=self.params[""max_sequence_length_item_desc""],\n                                       mask_zero=self.params[""embedding_mask_zero""],\n                                       training=self.training,\n                                       seed=self.params[""random_seed""],\n                                       name=""att_seq_item_desc_attend"")\n            if self.params[""encode_text_dim""] != self.params[""embedding_dim""]:\n                att_seq_name = tf.layers.Dense(self.params[""embedding_dim""])(att_seq_name)\n                att_seq_item_desc = tf.layers.Dense(self.params[""embedding_dim""])(att_seq_item_desc)\n            # since the following use fasttext encode, the `encode_text_dim` is embedding_dim\n            feature_dim = context_size + self.params[""embedding_dim""]\n            feature_dim = self.params[""embedding_dim""]\n            if self.params[""use_bigram""]:\n                att_seq_bigram_item_desc = attend(enc_seq_bigram_item_desc, method=self.params[""attend_method""],\n                                                  context=None, feature_dim=feature_dim,\n                                                  sequence_length=self.sequence_length_item_desc,\n                                                  maxlen=self.params[""max_sequence_length_item_desc""],\n                                                  mask_zero=self.params[""embedding_mask_zero""],\n                                                  training=self.training,\n                                                  seed=self.params[""random_seed""],\n                                                  name=""att_seq_bigram_item_desc_attend"")\n                # reshape\n                if self.params[""encode_text_dim""] != self.params[""embedding_dim""]:\n                    att_seq_bigram_item_desc = tf.layers.Dense(self.params[""embedding_dim""],\n                                                               kernel_initializer=tf.glorot_uniform_initializer(),\n                                                               dtype=tf.float32, bias_initializer=tf.zeros_initializer())(att_seq_bigram_item_desc)\n            if self.params[""use_trigram""]:\n                att_seq_trigram_item_desc = attend(enc_seq_trigram_item_desc, method=self.params[""attend_method""],\n                                                   context=None, feature_dim=feature_dim,\n                                                   sequence_length=self.sequence_length_item_desc,\n                                                   maxlen=self.params[""max_sequence_length_item_desc""],\n                                                   mask_zero=self.params[""embedding_mask_zero""],\n                                                   training=self.training,\n                                                   seed=self.params[""random_seed""],\n                                                   name=""att_seq_trigram_item_desc_attend"")\n                # reshape\n                if self.params[""encode_text_dim""] != self.params[""embedding_dim""]:\n                    att_seq_trigram_item_desc = tf.layers.Dense(self.params[""embedding_dim""],\n                                                                kernel_initializer=tf.glorot_uniform_initializer(),\n                                                                dtype=tf.float32, bias_initializer=tf.zeros_initializer())(att_seq_trigram_item_desc)\n            feature_dim = context_size + self.params[""embedding_dim""]\n            if self.params[""use_subword""]:\n                att_seq_subword_item_desc = attend(enc_seq_subword_item_desc, method=""ave"",\n                                                   context=None, feature_dim=feature_dim,\n                                                   sequence_length=self.sequence_length_item_desc_subword,\n                                                   maxlen=self.params[""max_sequence_length_item_desc_subword""],\n                                                   mask_zero=self.params[""embedding_mask_zero""],\n                                                   training=self.training,\n                                                   seed=self.params[""random_seed""],\n                                                   name=""att_seq_subword_item_desc_attend"")\n                # reshape\n                if self.params[""encode_text_dim""] != self.params[""embedding_dim""]:\n                    att_seq_subword_item_desc = tf.layers.Dense(self.params[""embedding_dim""],\n                                                                kernel_initializer=tf.glorot_uniform_initializer(),\n                                                                dtype=tf.float32, bias_initializer=tf.zeros_initializer())(att_seq_subword_item_desc)\n\n            deep_list = []\n            if self.params[""enable_deep""]:\n                # common\n                common_list = [\n                    # emb_seq_category_name,\n                    emb_brand_name,\n                    # emb_category_name,\n                    emb_category_name1,\n                    emb_category_name2,\n                    emb_category_name3,\n                    emb_item_condition,\n                    emb_shipping\n\n                ]\n                tmp_common = tf.concat(common_list, axis=1)\n\n                # word level fm for seq_name and others\n                tmp_name = tf.concat([emb_seq_name, tmp_common], axis=1)\n                sum_squared_name = tf.square(tf.reduce_sum(tmp_name, axis=1))\n                squared_sum_name = tf.reduce_sum(tf.square(tmp_name), axis=1)\n                fm_name = 0.5 * (sum_squared_name - squared_sum_name)\n\n                # word level fm for seq_item_desc and others\n                tmp_item_desc = tf.concat([emb_seq_item_desc, tmp_common], axis=1)\n                sum_squared_item_desc = tf.square(tf.reduce_sum(tmp_item_desc, axis=1))\n                squared_sum_item_desc = tf.reduce_sum(tf.square(tmp_item_desc), axis=1)\n                fm_item_desc = 0.5 * (sum_squared_item_desc - squared_sum_item_desc)\n\n                #### predict\n                # concat\n                deep_list += [\n                    att_seq_name,\n                    att_seq_item_desc,\n                    context,\n                    fm_name,\n                    fm_item_desc,\n\n                ]\n                # if self.params[""use_bigram""]:\n                #     deep_list += [att_seq_bigram_item_desc]\n                # # if self.params[""use_trigram""]:\n                # #     deep_list += [att_seq_trigram_item_desc]\n                # if self.params[""use_subword""]:\n                #     deep_list += [att_seq_subword_item_desc]\n\n            # fm layer\n            fm_list = []\n            if self.params[""enable_fm_first_order""]:\n                bias_seq_name = embed(self.seq_name, self.params[""MAX_NUM_WORDS""] + 1, 1, reduce_sum=True,\n                                      seed=self.params[""random_seed""])\n                bias_seq_item_desc = embed(self.seq_item_desc, self.params[""MAX_NUM_WORDS""] + 1, 1, reduce_sum=True,\n                                           seed=self.params[""random_seed""])\n                # bias_seq_category_name = embed(self.seq_category_name, self.params[""MAX_NUM_WORDS""] + 1, 1, reduce_sum=True,\n                #                                seed=self.params[""random_seed""])\n                if self.params[""use_bigram""]:\n                    bias_seq_bigram_item_desc = embed(self.seq_bigram_item_desc, self.params[""MAX_NUM_BIGRAMS""] + 1, 1,\n                                                      reduce_sum=True, seed=self.params[""random_seed""])\n                if self.params[""use_trigram""]:\n                    bias_seq_trigram_item_desc = embed(self.seq_trigram_item_desc, self.params[""MAX_NUM_TRIGRAMS""] + 1, 1,\n                                                       reduce_sum=True, seed=self.params[""random_seed""])\n                if self.params[""use_subword""]:\n                    bias_seq_subword_item_desc = embed(self.seq_subword_item_desc, self.params[""MAX_NUM_SUBWORDS""] + 1, 1,\n                                                       reduce_sum=True, seed=self.params[""random_seed""])\n\n                bias_brand_name = embed(self.brand_name, self.params[""MAX_NUM_BRANDS""], 1, flatten=True,\n                                        seed=self.params[""random_seed""])\n                # bias_category_name = embed(self.category_name, MAX_NUM_CATEGORIES, 1, flatten=True)\n                bias_category_name1 = embed(self.category_name1, self.params[""MAX_NUM_CATEGORIES_LST""][0], 1, flatten=True,\n                                            seed=self.params[""random_seed""])\n                bias_category_name2 = embed(self.category_name2, self.params[""MAX_NUM_CATEGORIES_LST""][1], 1, flatten=True,\n                                            seed=self.params[""random_seed""])\n                bias_category_name3 = embed(self.category_name3, self.params[""MAX_NUM_CATEGORIES_LST""][2], 1, flatten=True,\n                                            seed=self.params[""random_seed""])\n                bias_item_condition = embed(self.item_condition_id, self.params[""MAX_NUM_CONDITIONS""] + 1, 1, flatten=True,\n                                            seed=self.params[""random_seed""])\n                bias_shipping = embed(self.shipping, self.params[""MAX_NUM_SHIPPINGS""], 1, flatten=True,\n                                      seed=self.params[""random_seed""])\n\n                fm_first_order_list = [\n                    bias_seq_name,\n                    bias_seq_item_desc,\n                    # bias_seq_category_name,\n                    bias_brand_name,\n                    # bias_category_name,\n                    bias_category_name1,\n                    bias_category_name2,\n                    bias_category_name3,\n                    bias_item_condition,\n                    bias_shipping,\n                ]\n                if self.params[""use_bigram""]:\n                    fm_first_order_list += [bias_seq_bigram_item_desc]\n                if self.params[""use_trigram""]:\n                    fm_first_order_list += [bias_seq_trigram_item_desc]\n                # if self.params[""use_subword""]:\n                #     fm_first_order_list += [bias_seq_subword_item_desc]\n                tmp_first_order = tf.concat(fm_first_order_list, axis=1)\n                fm_list.append(tmp_first_order)\n\n            if self.params[""enable_fm_second_order""]:\n                # second order\n                emb_list = [\n                    tf.expand_dims(att_seq_name, axis=1),\n                    tf.expand_dims(att_seq_item_desc, axis=1),\n                    # tf.expand_dims(att_seq_category_name, axis=1),\n\n                    emb_brand_name,\n                    # emb_category_name,\n                    emb_category_name1,\n                    emb_category_name2,\n                    emb_category_name3,\n                    emb_item_condition,\n                    emb_shipping,\n\n                ]\n                if self.params[""use_bigram""]:\n                    emb_list += [tf.expand_dims(att_seq_bigram_item_desc, axis=1)]\n                # if self.params[""use_trigram""]:\n                #     emb_list += [tf.expand_dims(att_seq_trigram_item_desc, axis=1)]\n                if self.params[""use_subword""]:\n                    emb_list += [tf.expand_dims(att_seq_subword_item_desc, axis=1)]\n                emb_concat = tf.concat(emb_list, axis=1)\n                emb_sum_squared = tf.square(tf.reduce_sum(emb_concat, axis=1))\n                emb_squared_sum = tf.reduce_sum(tf.square(emb_concat), axis=1)\n\n                fm_second_order = 0.5 * (emb_sum_squared - emb_squared_sum)\n                fm_list.extend([emb_sum_squared, emb_squared_sum])\n\n            if self.params[""enable_fm_second_order""] and self.params[""enable_fm_higher_order""]:\n                fm_higher_order = dense_block(fm_second_order, hidden_units=[self.params[""embedding_dim""]] * 2,\n                                              dropouts=[0.] * 2, densenet=False, training=self.training, seed=self.params[""random_seed""])\n                fm_list.append(fm_higher_order)\n\n            if self.params[""enable_deep""]:\n                deep_list.extend(fm_list)\n                deep_in = tf.concat(deep_list, axis=-1, name=""concat"")\n                # dense\n                hidden_units = [self.params[""fc_dim""]*4, self.params[""fc_dim""]*2, self.params[""fc_dim""]]\n                dropouts = [self.params[""fc_dropout""]] * len(hidden_units)\n                if self.params[""fc_type""] == ""fc"":\n                    deep_out = dense_block(deep_in, hidden_units=hidden_units, dropouts=dropouts, densenet=False,\n                                           training=self.training, seed=self.params[""random_seed""])\n                elif self.params[""fc_type""] == ""resnet"":\n                    deep_out = resnet_block(deep_in, hidden_units=hidden_units, dropouts=dropouts, cardinality=1,\n                                            dense_shortcut=True, training=self.training,\n                                            seed=self.params[""random_seed""])\n                elif self.params[""fc_type""] == ""densenet"":\n                    deep_out = dense_block(deep_in, hidden_units=hidden_units, dropouts=dropouts, densenet=True,\n                                           training=self.training, seed=self.params[""random_seed""])\n                fm_list.append(deep_out)\n\n\n            fm_list.append(self.num_vars)\n            fm_list.append(self.item_condition)\n            fm_list.append(tf.cast(self.shipping, tf.float32))\n            out = tf.concat(fm_list, axis=-1)\n\n\n            self.pred = tf.layers.Dense(1, kernel_initializer=tf.glorot_uniform_initializer(self.params[""random_seed""]),\n                                        dtype=tf.float32, bias_initializer=tf.zeros_initializer())(out)\n\n            # intermediate meta\n            self.meta = out\n\n            #### loss\n            self.rmse = tf.sqrt(tf.losses.mean_squared_error(self.target, self.pred))\n            # target is normalized, so std is 1\n            # we apply 3 sigma principle\n            std = 1.\n            self.loss = tf.losses.huber_loss(self.target, self.pred, delta=1. * std)\n            # self.loss = self.rmse\n\n            #### optimizer\n            self.learning_rate = tf.placeholder(tf.float32, shape=[], name=""learning_rate"")\n            if self.params[""optimizer_type""] == ""nadam"":\n                self.optimizer = LazyNadamOptimizer(learning_rate=self.learning_rate, beta1=self.params[""beta1""],\n                                                beta2=self.params[""beta2""], epsilon=1e-8,\n                                                schedule_decay=self.params[""schedule_decay""])\n            elif self.params[""optimizer_type""] == ""adam"":\n                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=self.params[""beta1""],\n                                                        beta2=self.params[""beta2""], epsilon=1e-8)\n            elif self.params[""optimizer_type""] == ""lazyadam"":\n                self.optimizer = tf.contrib.opt.LazyAdamOptimizer(learning_rate=self.learning_rate,\n                                                                  beta1=self.params[""beta1""],\n                                                                  beta2=self.params[""beta2""], epsilon=1e-8)\n            elif self.params[""optimizer_type""] == ""adagrad"":\n                self.optimizer = tf.train.AdagradOptimizer(learning_rate=self.learning_rate,\n                                                           initial_accumulator_value=1e-7)\n            elif self.params[""optimizer_type""] == ""gd"":\n                self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)\n            elif self.params[""optimizer_type""] == ""momentum"":\n                self.optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=0.95)\n            elif self.params[""optimizer_type""] == ""rmsprop"":\n                self.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate, decay=0.9, momentum=0.9,\n                                                           epsilon=1e-8)\n            elif self.params[""optimizer_type""] == ""lazypowersign"":\n                self.optimizer = LazyPowerSignOptimizer(learning_rate=self.learning_rate)\n            elif self.params[""optimizer_type""] == ""lazyaddsign"":\n                self.optimizer = LazyAddSignOptimizer(learning_rate=self.learning_rate)\n            elif self.params[""optimizer_type""] == ""lazyamsgrad"":\n                self.optimizer = LazyAMSGradOptimizer(learning_rate=self.learning_rate, beta1=self.params[""beta1""],\n                                                  beta2=self.params[""beta2""], epsilon=1e-8)\n\n            #### training op\n            """"""\n            https://stackoverflow.com/questions/35803425/update-only-part-of-the-word-embedding-matrix-in-tensorflow\n            TL;DR: The default implementation of opt.minimize(loss), TensorFlow will generate a sparse update for \n            word_emb that modifies only the rows of word_emb that participated in the forward pass.\n\n            The gradient of the tf.gather(word_emb, indices) op with respect to word_emb is a tf.IndexedSlices object\n             (see the implementation for more details). This object represents a sparse tensor that is zero everywhere, \n             except for the rows selected by indices. A call to opt.minimize(loss) calls \n             AdamOptimizer._apply_sparse(word_emb_grad, word_emb), which makes a call to tf.scatter_sub(word_emb, ...)* \n             that updates only the rows of word_emb that were selected by indices.\n\n            If on the other hand you want to modify the tf.IndexedSlices that is returned by \n            opt.compute_gradients(loss, word_emb), you can perform arbitrary TensorFlow operations on its indices and \n            values properties, and create a new tf.IndexedSlices that can be passed to opt.apply_gradients([(word_emb, ...)]). \n            For example, you could cap the gradients using MyCapper() (as in the example) using the following calls:\n\n            grad, = opt.compute_gradients(loss, word_emb)\n            train_op = opt.apply_gradients(\n                [tf.IndexedSlices(MyCapper(grad.values), grad.indices)])\n            Similarly, you could change the set of indices that will be modified by creating a new tf.IndexedSlices with\n             a different indices.\n\n            * In general, if you want to update only part of a variable in TensorFlow, you can use the tf.scatter_update(), \n            tf.scatter_add(), or tf.scatter_sub() operators, which respectively set, add to (+=) or subtract from (-=) the \n            value previously stored in a variable.\n            """"""\n            # # it\'s slow\n            # grads = self.optimizer.compute_gradients(self.loss)\n            # for i, (g, v) in enumerate(grads):\n            #     if g is not None:\n            #         if isinstance(g, tf.IndexedSlices):\n            #             grads[i] = (tf.IndexedSlices(tf.clip_by_norm(g.values, self.params[""optimizer_clipnorm""]), g.indices), v)\n            #         else:\n            #             grads[i] = (tf.clip_by_norm(g, self.params[""optimizer_clipnorm""]), v)\n            # self.train_op = self.optimizer.apply_gradients(grads, global_step=self.global_step)\n\n            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n            with tf.control_dependencies(update_ops):\n                self.train_op = self.optimizer.minimize(self.loss)#, global_step=self.global_step)\n\n            #### init\n            self.sess, self.saver = self._init_session()\n\n            # save model state to memory\n            # https://stackoverflow.com/questions/46393983/how-can-i-restore-tensors-to-a-past-value-without-saving-the-value-to-disk/46511601\n            # https://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model/43333803#43333803\n            # Extract the global varibles from the graph.\n            self.gvars = self.graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n            # Exract the Assign operations for later use.\n            self.assign_ops = [self.graph.get_operation_by_name(v.op.name + ""/Assign"") for v in self.gvars]\n            # Extract the initial value ops from each Assign op for later use.\n            self.init_values = [assign_op.inputs[1] for assign_op in self.assign_ops]\n\n    def _init_session(self):\n        config = tf.ConfigProto(device_count={""gpu"": 0})\n        config.gpu_options.allow_growth = True\n        # the following reduce the training time for a snapshot from 180~220s to 100s in kernel\n        config.intra_op_parallelism_threads = 4\n        config.inter_op_parallelism_threads = 4\n        sess = tf.Session(config=config)\n        sess.run(tf.global_variables_initializer())\n        # max_to_keep=None, keep all the models\n        saver = tf.train.Saver(max_to_keep=None)\n        return sess, saver\n\n    def _save_session(self, dir):\n        """"""Saves session = weights""""""\n        _makedirs(self.params[""model_dir""])\n        self.saver.save(self.sess, dir)\n\n    def _restore_session(self, dir):\n        self.saver.restore(self.sess, dir)\n\n    def _save_state(self):\n        # Record the current state of the TF global varaibles\n        gvars_state = self.sess.run(self.gvars)\n        self.gvars_state_list.append(gvars_state)\n\n    def _restore_state(self, gvars_state):\n        # Create a dictionary of the iniailizers and stored state of globals.\n        feed_dict = dict(zip(self.init_values, gvars_state))\n        # Use the initializer ops for each variable to load the stored values.\n        self.sess.run(self.assign_ops, feed_dict=feed_dict)\n\n    def _get_batch_index(self, seq, step):\n        n = len(seq)\n        res = []\n        res_append = res.append\n        for i in range(0, n, step):\n            res_append(seq[i:i + step])\n        # last batch\n        if len(res) * step < n:\n            res_append(seq[len(res) * step:])\n        return res\n\n    def _get_feed_dict(self, X, idx, dropout=0.1, training=False):\n        feed_dict = {}\n        feed_dict.update({\n            self.seq_name: X[""seq_name""][idx],\n            self.seq_item_desc: X[""seq_item_desc""][idx],\n            # self.seq_category_name: X[""seq_category_name""][idx],\n            self.brand_name: X[""brand_name""][idx],\n            # self.category_name: X[""category_name""][idx],\n            self.category_name1: X[""category_name1""][idx],\n            self.category_name2: X[""category_name2""][idx],\n            self.category_name3: X[""category_name3""][idx],\n            self.item_condition: X[""item_condition""][idx],\n            self.item_condition_id: X[""item_condition_id""][idx],\n            self.shipping: X[""shipping""][idx],\n            self.num_vars: X[""num_vars""][idx],\n        })\n        # len\n        feed_dict.update({\n            self.sequence_length_name: X[""sequence_length_name""][idx],\n            self.sequence_length_item_desc: X[""sequence_length_item_desc""][idx],\n            # self.sequence_length_category_name: X[""sequence_length_category_name""][idx],\n        })\n\n        if training and dropout > 0:\n            mask = np.random.choice([0, 1], (len(idx), self.params[""max_sequence_length_name""]),\n                                    p=[dropout, 1 - dropout])\n            feed_dict[self.seq_name] *= mask\n            mask = np.random.choice([0, 1], (len(idx), self.params[""max_sequence_length_item_desc""]),\n                                    p=[dropout, 1 - dropout])\n            feed_dict[self.seq_item_desc] *= mask\n\n        if self.params[""use_bigram""]:\n            feed_dict[self.seq_bigram_item_desc] = X[""seq_bigram_item_desc""][idx]\n            if training and dropout > 0:\n                mask = np.random.choice([0, 1], (len(idx), self.params[""max_sequence_length_item_desc""]),\n                                        p=[dropout, 1 - dropout])\n                feed_dict[self.seq_bigram_item_desc] *= mask\n\n        if self.params[""use_trigram""]:\n            feed_dict[self.seq_trigram_item_desc] = X[""seq_trigram_item_desc""][idx]\n            if training and dropout > 0:\n                mask = np.random.choice([0, 1], (len(idx), self.params[""max_sequence_length_item_desc""]),\n                                        p=[dropout, 1 - dropout])\n                feed_dict[self.seq_trigram_item_desc] *= mask\n\n        if self.params[""use_subword""]:\n            feed_dict[self.seq_subword_item_desc] = X[""seq_subword_item_desc""][idx]\n            feed_dict[self.sequence_length_item_desc_subword] = X[""sequence_length_item_desc_subword""][idx]\n            if training and dropout > 0:\n                mask = np.random.choice([0, 1], (len(idx), self.params[""max_sequence_length_item_desc_subword""]),\n                                        p=[dropout, 1 - dropout])\n                feed_dict[self.seq_subword_item_desc] *= mask\n\n        return feed_dict\n\n    def fit(self, X, y, validation_data=None):\n        y = y.reshape(-1, 1)\n        start_time = time.time()\n        l = y.shape[0]\n        train_idx_shuffle = np.arange(l)\n        epoch_best_ = 4\n        rmsle_best_ = 10.\n        cycle_num = 0\n        decay_steps = self.params[""first_decay_steps""]\n        global_step = 0\n        global_step_exp = 0\n        global_step_total = 0\n        snapshot_num = 0\n        learning_rate_need_big_jump = False\n        total_rmse = 0.\n        rmse_decay = 0.9\n        for epoch in range(self.params[""epoch""]):\n            print(""epoch: %d"" % (epoch + 1))\n            np.random.seed(epoch)\n            if snapshot_num >= self.params[""snapshot_before_restarts""] and self.params[""shuffle_with_replacement""]:\n                train_idx_shuffle = np.random.choice(np.arange(l), l)\n            else:\n                np.random.shuffle(train_idx_shuffle)\n            batches = self._get_batch_index(train_idx_shuffle, self.params[""batch_size_train""])\n            for i, idx in enumerate(batches):\n                if snapshot_num >= self.params[""max_snapshot_num""]:\n                    break\n                if learning_rate_need_big_jump:\n                    learning_rate = self.params[""lr_jump_rate""] * self.params[""max_lr_exp""]\n                    learning_rate_need_big_jump = False\n                else:\n                    learning_rate = self.params[""max_lr_exp""]\n                lr = _exponential_decay(learning_rate=learning_rate,\n                                        global_step=global_step_exp,\n                                        decay_steps=decay_steps,  # self.params[""num_update_each_epoch""],\n                                        decay_rate=self.params[""lr_decay_each_epoch_exp""])\n                feed_dict = self._get_feed_dict(X, idx, dropout=0.1, training=False)\n                feed_dict[self.target] = y[idx]\n                feed_dict[self.learning_rate] = lr\n                feed_dict[self.training] = True\n                rmse_, opt = self.sess.run((self.rmse, self.train_op), feed_dict=feed_dict)\n                if self.params[""RUNNING_MODE""] != ""submission"":\n                    # scaling rmsle\' = (1/scale_) * (raw rmsle)\n                    # raw rmsle = scaling rmsle\' * scale_\n                    total_rmse = rmse_decay * total_rmse + (1. - rmse_decay) * rmse_ * (self.target_scaler.scale_)\n                    self.logger.info(""[batch-%d] train-rmsle=%.5f, lr=%.5f [%.1f s]"" % (\n                        i + 1, total_rmse,\n                        lr, time.time() - start_time))\n                # save model\n                global_step += 1\n                global_step_exp += 1\n                global_step_total += 1\n                if self.params[""enable_snapshot_ensemble""]:\n                    if global_step % decay_steps == 0:\n                        cycle_num += 1\n                        if cycle_num % self.params[""snapshot_every_num_cycle""] == 0:\n                            snapshot_num += 1\n                            print(""snapshot num: %d"" % snapshot_num)\n                            self._save_state()\n                            self.logger.info(""[model-%d] cycle num=%d, current lr=%.5f [%.5f]"" % (\n                                snapshot_num, cycle_num, lr, time.time() - start_time))\n                            # reset global_step and first_decay_steps\n                            decay_steps = self.params[""first_decay_steps""]\n                            if self.params[""lr_jump_exp""] or snapshot_num >= self.params[""snapshot_before_restarts""]:\n                                learning_rate_need_big_jump = True\n                        if snapshot_num >= self.params[""snapshot_before_restarts""]:\n                            global_step = 0\n                            global_step_exp = 0\n                            decay_steps *= self.params[""t_mul""]\n\n                if validation_data is not None and global_step_total % self.params[""eval_every_num_update""] == 0:\n                    y_pred = self._predict(validation_data[0])\n                    y_valid_inv = self.target_scaler.inverse_transform(validation_data[1])\n                    y_pred_inv = self.target_scaler.inverse_transform(y_pred)\n                    rmsle = rmse(y_valid_inv, y_pred_inv)\n                    self.logger.info(""[step-%d] train-rmsle=%.5f, valid-rmsle=%.5f, lr=%.5f [%.1f s]"" % (\n                        global_step_total, total_rmse, rmsle, lr, time.time() - start_time))\n                    if rmsle < rmsle_best_:\n                        rmsle_best_ = rmsle\n                        epoch_best_ = epoch + 1\n\n        return rmsle_best_, epoch_best_\n\n    def _predict(self, X):\n        l = X[""seq_name""].shape[0]\n        train_idx = np.arange(l)\n        batches = self._get_batch_index(train_idx, self.params[""batch_size_inference""])\n        y = np.zeros((l, 1), dtype=np.float32)\n        y_pred = []\n        y_pred_append = y_pred.append\n        for idx in batches:\n            feed_dict = self._get_feed_dict(X, idx)\n            feed_dict[self.target] = y[idx]\n            feed_dict[self.learning_rate] = 1.0\n            feed_dict[self.training] = False\n            pred = self.sess.run((self.pred), feed_dict=feed_dict)\n            y_pred_append(pred)\n        y_pred = np.vstack(y_pred).reshape(-1, 1)\n        return y_pred\n\n    def _merge_gvars_state_list(self):\n        out = self.gvars_state_list[0].copy()\n        for ms in self.gvars_state_list[1:]:\n            for i, m in enumerate(ms):\n                out[i] += m\n        out = [o / float(len(self.gvars_state_list)) for o in out]\n        return out\n\n    def predict(self, X, mode=""mean""):\n        if self.params[""enable_snapshot_ensemble""]:\n            y = []\n            if mode == ""merge"":\n                gvars_state = self._merge_gvars_state_list()\n                self._restore_state(gvars_state)\n                y_ = self._predict(X)\n                y.append(y_)\n            else:\n                for i,gvars_state in enumerate(self.gvars_state_list):\n                    print(""predict for: %d""%(i+1))\n                    self._restore_state(gvars_state)\n                    y_ = self._predict(X)\n                    y.append(y_)\n            if len(y) == 1:\n                y = np.array(y).reshape(-1, 1)\n            else:\n                y = np.hstack(y)\n                if mode == ""median"":\n                    y = np.median(y, axis=1, keepdims=True)\n                elif mode == ""mean"":\n                    y = np.mean(y, axis=1, keepdims=True)\n                elif mode == ""weight"":\n                    y = self.bias + np.dot(y, self.weights)\n                elif mode == ""raw"":\n                    pass\n        else:\n            y = self._predict(X)\n\n        return y\n'"
