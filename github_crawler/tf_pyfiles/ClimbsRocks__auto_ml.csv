file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\nfrom codecs import open\nfrom os import path\n\nhere = path.abspath(path.dirname(__file__))\ntry:\n    # Try to format our PyPi page as rst so it displays properly\n    import pypandoc\n    with open (\'README.md\', \'rb\') as read_file:\n        readme_text = read_file.readlines()\n    # Change our README for pypi so we can get analytics tracking information for that separately\n    readme_text = [row.decode() for row in readme_text]\n    readme_text[-1] = ""[![Analytics](https://ga-beacon.appspot.com/UA-58170643-5/auto_ml/pypi)](https://github.com/igrigorik/ga-beacon)""\n\n    long_description = pypandoc.convert(\'\'.join(readme_text), \'rst\', format=\'md\')\nexcept ImportError:\n    print(\'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\')\n    print(\'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\')\n    print(\'pypandoc (and possibly pandoc) are not installed. This means the PyPi package info will be formatted as .md instead of .rst. If you are encountering this before uploading a PyPi distribution, please install these\')\n    print(\'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\')\n    print(\'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\')\n    # Get the long description from the README file\n    with open(path.join(here, \'README.md\'), encoding=\'utf-8\') as f:\n        long_description = f.read()\n\nsetup(\n    name=\'auto_ml\',\n\n    version=open(""auto_ml/_version.py"").readlines()[-1].split()[-1].strip(""\\""\'""),\n\n    description=\'Automated machine learning for production and analytics\',\n    long_description=long_description,\n\n    url=\'https://github.com/ClimbsRocks/auto_ml\',\n\n    author=\'Preston Parry\',\n    author_email=\'ClimbsBytes@gmail.com\',\n\n    license=\'MIT\',\n\n    classifiers=[\n        \'Development Status :: 5 - Production/Stable\',\n\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Information Technology\',\n        \'Intended Audience :: Science/Research\',\n\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'Topic :: Scientific/Engineering :: Information Analysis\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\',\n\n        \'License :: OSI Approved :: MIT License\',\n\n        \'Programming Language :: Python :: 2\',\n        \'Programming Language :: Python :: 2.7\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.4\',\n        \'Programming Language :: Python :: 3.4\',\n        \'Programming Language :: Python :: 3.6\',\n    ],\n\n    keywords=[\'machine learning\', \'data science\', \'automated machine learning\', \'regressor\', \'regressors\', \'regression\', \'classification\', \'classifiers\', \'classifier\', \'estimators\', \'predictors\', \'XGBoost\', \'Random Forest\', \'sklearn\', \'scikit-learn\', \'analytics\', \'analysts\', \'coefficients\', \'feature importances\' \'analytics\', \'artificial intelligence\', \'subpredictors\', \'ensembling\', \'stacking\', \'blending\', \'feature engineering\', \'feature extraction\', \'feature selection\', \'production\', \'pandas\', \'dataframes\', \'machinejs\', \'deep learning\', \'tensorflow\', \'deeplearning\', \'lightgbm\', \'gradient boosting\', \'gbm\', \'keras\', \'production ready\', \'test coverage\'],\n\n    packages=[\'auto_ml\'],\n\n    # We will allow the user to install XGBoost themselves. However, since it can be difficult to install, we will not force them to go through that install challenge if they\'re just checking out the package and want to get running with it quickly.\n    install_requires=[\n        \'dill>=0.2.5, <0.3\',\n        \'h5py>=2.7.0, <3.0\',\n        \'lightgbm>=2.0.11, <2.1\',\n        \'numpy>=1.11.0, <2.0\',\n        \'pandas>=0.18.0, <1.0\',\n        \'pathos>=0.2.1, <0.3.0\',\n        \'python-dateutil>=2.6.1, <3.0\',\n        \'scikit-learn>=0.18.1, <1.0\',\n        \'scipy>=0.14.0, <2.0\',\n        \'sklearn-deap2>=0.2.1, <0.3\',\n        \'tabulate>=0.7.5, <1.0\',\n    ],\n\n    test_suite=\'nose.collector\',\n    tests_require=[\'nose\', \'coveralls\']\n)\n'"
auto_ml/DataFrameVectorizer.py,0,"b'# Modified version of scikit-learn\'s DictVectorizer\nfrom array import array\nimport numbers\nfrom operator import itemgetter\n\nimport numpy as np\nimport pandas as pd\nimport scipy.sparse as sp\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.externals import six\n\nfrom auto_ml.utils import CustomLabelEncoder\n\n\n\nbad_vals = set([float(\'nan\'), float(\'inf\'), float(\'-inf\'), None, np.nan, \'None\', \'none\', \'NaN\', \'NAN\', \'nan\', \'NULL\', \'null\', \'\', \'inf\', \'-inf\'])\n\ndef strip_non_ascii(string):\n    \'\'\' Returns the string without non ASCII characters\'\'\'\n    stripped = (c for c in string if 0 < ord(c) < 127)\n    return \'\'.join(stripped)\n\n\nclass DataFrameVectorizer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, column_descriptions=None, dtype=np.float32, separator=""="", sparse=True, keep_cat_features=False):\n        self.dtype = dtype\n        self.separator = separator\n        self.sparse = sparse\n        if column_descriptions == None:\n            column_descriptions = {}\n        self.column_descriptions = column_descriptions\n        self.vals_to_drop = set([\'ignore\', \'output\', \'regressor\', \'classifier\'])\n        self.has_been_restricted = False\n        self.keep_cat_features = keep_cat_features\n        self.label_encoders = {}\n        self.numerical_columns = None\n        self.num_numerical_cols = None\n        self.categorical_columns = None\n        self.numeric_col_types = [\'int8\', \'int16\', \'int32\', \'int64\', \'float16\', \'float32\', \'float64\']\n        self.additional_numerical_cols = []\n\n\n\n    def get(self, prop_name, default=None):\n        try:\n            return getattr(self, prop_name)\n        except AttributeError:\n            return default\n\n    def fit(self, X, y=None):\n        print(\'Fitting DataFrameVectorizer\')\n\n\n        feature_names = []\n        vocab = {}\n\n        # Rearrange X so that all the categorical columns are first\n        numerical_columns = []\n        categorical_columns = []\n        for col in X.columns:\n            col_desc = self.column_descriptions.get(col, False)\n            if col_desc in [False, \'continuous\', \'int\', \'float\', \'numerical\']:\n                numerical_columns.append(col)\n            elif col_desc in self.vals_to_drop:\n                continue\n            elif col_desc == \'categorical\':\n                categorical_columns.append(col)\n            else:\n                print(\'We are unsure what to do with this column:\')\n                print(col)\n                print(col_desc)\n\n        self.num_numerical_cols = len(numerical_columns)\n        self.numerical_columns = numerical_columns\n        self.categorical_columns = categorical_columns\n\n        new_cols = numerical_columns + categorical_columns\n        X = X[new_cols]\n\n        for col_name in X.columns:\n\n            if self.column_descriptions.get(col_name, False) == \'categorical\' and self.keep_cat_features == True:\n                # All of these values will go in the same column, but they must be turned into ints first\n                self.label_encoders[col_name] = CustomLabelEncoder()\n                # Then, we will use the same flow below to make sure they appear in the vocab correctly\n                self.label_encoders[col_name].fit(X[col_name])\n\n\n            # We can\'t do elif here- it has to be inclusive of the logic above\n            if self.column_descriptions.get(col_name, False) == \'categorical\' and self.keep_cat_features == False:\n                # If this is a categorical column, iterate through each row to get all the possible values that we are one-hot-encoding.\n                for val in set(X[col_name]):\n                    if not isinstance(val, str):\n                        if isinstance(val, numbers.Number) or val is None:\n                            val = str(val)\n                        else:\n                            val = val.encode(\'utf-8\').decode(\'utf-8\')\n\n                    feature_name = col_name + self.separator + val\n\n                    if feature_name not in vocab:\n                        feature_names.append(feature_name)\n                        vocab[feature_name] = len(vocab)\n\n            # If this is a categorical column, do not include the column name itself, just include the feature_names as calculated above\n            elif col_name not in vocab:\n                feature_names.append(col_name)\n                vocab[col_name] = len(vocab)\n\n        self.feature_names_ = feature_names\n        self.vocabulary_ = vocab\n        return self\n\n\n    def _transform(self, X):\n\n        dtype = self.dtype\n        feature_names = self.feature_names_\n        vocab = self.vocabulary_\n\n\n        if isinstance(X, dict):\n\n            indices = array(""i"")\n            indptr = array(""i"", [0])\n            values = []\n\n            for f, val in X.items():\n                if self.column_descriptions.get(f, False) == \'categorical\':\n                    if self.get(\'keep_cat_features\', False) == False:\n                        if not isinstance(val, str):\n                            if isinstance(val, numbers.Number) or val is None:\n                                val = str(val)\n                            else:\n                                val = val.encode(\'utf-8\').decode(\'utf-8\')\n                        f = f + self.separator + val\n                        val = 1\n                    else:\n                        if val in bad_vals:\n                            val = \'_None\'\n                        val = self.get(\'label_encoders\')[f].transform([val])\n\n                if f in vocab and val not in bad_vals and (self.get(\'keep_cat_features\', False) or not np.isnan(val)):\n\n                    indices.append(vocab[f])\n                    # Convert the val to the correct dtype, then append to our values list\n                    values.append(dtype(val))\n\n            indptr.append(len(indices))\n\n            if len(indptr) == 1:\n                raise ValueError(\'The dictionary passed into DataFrameVectorizer is empty\')\n\n            indices = np.frombuffer(indices, dtype=np.intc)\n            indptr = np.frombuffer(indptr, dtype=np.intc)\n            shape = (len(indptr) - 1, len(vocab))\n\n            result_matrix = sp.csr_matrix((values, indices, indptr),\n                                          shape=shape, dtype=dtype)\n\n            if self.sparse:\n                result_matrix.sort_indices()\n\n            return result_matrix\n\n        else:\n\n            for col in self.numerical_columns:\n                if col not in X.columns:\n                    X[col] = 0\n            for col in self.categorical_columns:\n                if col not in X.columns:\n                    X[col] = 0\n            for col in self.additional_numerical_cols:\n                if col not in X.columns:\n                    X[col] = 0\n\n            X.fillna(0, inplace=True)\n\n            for idx, col in enumerate(self.numerical_columns):\n                if X[col].dtype not in self.numeric_col_types:\n                    X[col] = X[col].astype(np.float32)\n\n\n            # Running this in parallel can cause memory crashes if the dataset is too large.\n            categorical_vals = list(map(lambda col_name: self.transform_categorical_col(col_vals=list(X[col_name]), col_name=col_name), self.categorical_columns))\n\n            X = X[self.numerical_columns]\n            # X.drop(self.categorical_columns, inplace=True, axis=1)\n            X.reset_index(drop=True, inplace=True)\n            for result in categorical_vals:\n                result.reset_index(drop=True, inplace=True)\n                X[result.columns] = result\n                del result\n\n\n        if self.keep_cat_features == True:\n            return X\n        else:\n            X = sp.csr_matrix(X.values)\n            return X\n\n\n    # We are assuming that each categorical column got a contiguous block of result columns (ie, the 5 categories in City get columns 5-9, not columns 0, 8, 26, 4, and 20)\n    def transform_categorical_col(self, col_vals, col_name):\n        if self.get(\'keep_cat_features\', False) == True:\n            return_vals = self.get(\'label_encoders\')[col_name].transform(col_vals)\n            result = {\n                col_name: return_vals\n            }\n\n            result = pd.DataFrame(result)\n            # result[col_name] = pd.to_numeric(result[col_name], downcast=\'integer\')\n\n            return result\n\n        else:\n\n            num_trained_cols = 0\n            min_transformed_idx = None\n            max_transformed_idx = None\n            len_col_name = len(col_name)\n            encoded_col_names = []\n\n            for trained_feature, col_idx in self.vocabulary_.items():\n                if trained_feature[:len_col_name] == col_name:\n                    encoded_col_names.append([trained_feature, col_idx])\n                    num_trained_cols += 1\n                    if min_transformed_idx is None:\n                        min_transformed_idx = col_idx\n                        max_transformed_idx = col_idx\n                    elif col_idx > max_transformed_idx:\n                        max_transformed_idx = col_idx\n                    elif col_idx < min_transformed_idx:\n                        min_transformed_idx = col_idx\n\n            encoded_col_names = sorted(encoded_col_names, key=lambda tup: tup[1])\n            encoded_col_names = [tup[0] for tup in encoded_col_names]\n\n            result = sp.lil_matrix((len(col_vals), num_trained_cols))\n\n            if num_trained_cols == 0:\n                df_result = pd.DataFrame(result.toarray(), columns=encoded_col_names)\n                return df_result\n\n            if num_trained_cols != (max_transformed_idx - min_transformed_idx + 1):\n                print(\'We have somehow ended up with categorical column behavior we were not expecting\')\n                raise(ValueError)\n\n\n            for row_idx, val in enumerate(col_vals):\n                if not isinstance(val, str):\n                    if isinstance(val, numbers.Number) or val is None:\n                        val = str(val)\n                    else:\n                        val = val.encode(\'utf-8\').decode(\'utf-8\')\n\n                feature_name = col_name + self.separator + val\n                if feature_name in self.vocabulary_:\n                    col_idx = self.vocabulary_[feature_name]\n                    col_idx = col_idx - min_transformed_idx\n\n                    result[row_idx, col_idx] = 1\n\n\n            df_result = pd.DataFrame(result.toarray(), columns=encoded_col_names)\n            return df_result\n\n    def transform(self, X, y=None):\n        return self._transform(X)\n\n    def get_feature_names(self):\n        """"""Returns a list of feature names, ordered by their indices.\n\n        If one-of-K coding is applied to categorical features, this will\n        include the constructed feature names but not the original ones.\n        """"""\n        return self.feature_names_\n\n\n    # This is for cases where we want to add in new features, such as for feature_learning\n    def add_new_numerical_cols(self, new_feature_names):\n        # add to our vocabulary\n        for feature_name in new_feature_names:\n            if feature_name not in self.vocabulary_:\n                self.feature_names_.append(feature_name)\n                self.vocabulary_[feature_name] = len(self.vocabulary_)\n                self.additional_numerical_cols.append(feature_name)\n\n        return self\n\n    def restrict(self, support):\n        """"""Restrict the features to those in support using feature selection.\n\n        This function modifies the estimator in-place.\n\n        """"""\n        if self.has_been_restricted == True:\n            return self\n\n        new_numerical_cols = []\n        new_categorical_cols = []\n        new_additional_numerical_cols = []\n        new_feature_names = []\n        new_vocab = {}\n\n        for idx, val in enumerate(support):\n            if val == True:\n                feature_name = self.feature_names_[idx]\n                if self.separator in feature_name:\n                    base_feature_name = feature_name[:feature_name.rfind(self.separator)]\n                else:\n                    base_feature_name = feature_name\n                new_feature_names.append(feature_name)\n                new_vocab[feature_name] = len(new_vocab)\n                if feature_name in self.numerical_columns:\n                    new_numerical_cols.append(feature_name)\n                elif base_feature_name in self.categorical_columns and base_feature_name not in new_categorical_cols:\n                    new_categorical_cols.append(base_feature_name)\n                elif feature_name in self.additional_numerical_cols:\n                    new_additional_numerical_cols.append(feature_name)\n\n        self.feature_names_ = new_feature_names\n        self.vocabulary_ = new_vocab\n        self.numerical_columns = new_numerical_cols\n        self.categorical_columns = new_categorical_cols\n        self.additional_numerical_cols = new_additional_numerical_cols\n\n        self.has_been_restricted = True\n        return self\n'"
auto_ml/__init__.py,0,b'from auto_ml.predictor import Predictor\nfrom auto_ml._version import __version__\nfrom auto_ml.utils_models import load_ml_model\n'
auto_ml/_version.py,0,"b'__version__ = ""2.9.10""\n'"
auto_ml/predictor.py,0,"b'from collections import OrderedDict\nimport datetime\nimport math\nimport multiprocessing\nimport os\nimport random\nimport sys\nimport types\nimport warnings\n\nfrom deap.base import Toolbox\nimport dill\nimport pathos\n\nimport numpy as np\nimport pandas as pd\nfrom tabulate import tabulate\n\n# Ultimately, we (the authors of auto_ml) are responsible for building a project that\'s robust against warnings.\n# The classes of warnings below are ones we\'ve deemed acceptable. The user should be able to sit at a high level of abstraction, and not be bothered with the internals of how we\'re handing these things.\n# Ignore all warnings that are UserWarnings or DeprecationWarnings. We\'ll fix these ourselves as necessary.\nwarnings.filterwarnings(""ignore"", category=DeprecationWarning)\npd.options.mode.chained_assignment = None  # default=\'warn\'\n\nimport scipy\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.model_selection import GridSearchCV, KFold, train_test_split\nfrom sklearn.metrics import mean_squared_error, brier_score_loss, make_scorer, accuracy_score\nfrom sklearn.preprocessing import FunctionTransformer, StandardScaler\n\n\nfrom auto_ml._version import __version__ as auto_ml_version\nfrom auto_ml import DataFrameVectorizer\nfrom auto_ml import utils\nfrom auto_ml import utils_categorical_ensembling\nfrom auto_ml import utils_data_cleaning\nfrom auto_ml import utils_ensembling\nfrom auto_ml import utils_feature_selection\nfrom auto_ml import utils_model_training\nfrom auto_ml import utils_models\nfrom auto_ml import utils_scaling\nfrom auto_ml import utils_scoring\n\nfrom evolutionary_search import EvolutionaryAlgorithmSearchCV\n\n# For handling parallelism edge cases\ndef _pickle_method(m):\n    if m.im_self is None:\n        return getattr, (m.im_class, m.im_func.func_name)\n    else:\n        return getattr, (m.im_self, m.im_func.func_name)\n\ntry:\n    import copy_reg\n    copy_reg.pickle(types.MethodType, _pickle_method)\nexcept:\n    import copyreg\n    copyreg.pickle(types.MethodType, _pickle_method)\n\n\nclass Predictor(object):\n\n\n    def __init__(self, type_of_estimator, column_descriptions, verbose=True, name=None):\n        if type_of_estimator.lower() in [\'regressor\',\'regression\', \'regressions\', \'regressors\', \'number\', \'numeric\', \'continuous\']:\n            self.type_of_estimator = \'regressor\'\n        elif type_of_estimator.lower() in [\'classifier\', \'classification\', \'categorizer\', \'categorization\', \'categories\', \'labels\', \'labeled\', \'label\']:\n            self.type_of_estimator = \'classifier\'\n        else:\n            print(\'Invalid value for ""type_of_estimator"". Please pass in either ""regressor"" or ""classifier"". You passed in: \' + type_of_estimator)\n            raise ValueError(\'Invalid value for ""type_of_estimator"". Please pass in either ""regressor"" or ""classifier"". You passed in: \' + type_of_estimator)\n        self.column_descriptions = column_descriptions\n        self.verbose = verbose\n        self.trained_pipeline = None\n        self._scorer = None\n        self.date_cols = []\n        # Later on, if this is a regression problem, we will possibly take the natural log of our y values for training, but we will still want to return the predictions in their normal scale (not the natural log values)\n        self.took_log_of_y = False\n        self.take_log_of_y = False\n\n        self._validate_input_col_descriptions()\n\n        self.name = name\n\n\n    def _validate_input_col_descriptions(self):\n        found_output_column = False\n        self.cols_to_ignore = []\n        expected_vals = set([\'categorical\', \'text\', \'nlp\'])\n\n        for key, value in self.column_descriptions.items():\n            value = value.lower()\n            self.column_descriptions[key] = value\n            if value == \'output\':\n                self.output_column = key\n                found_output_column = True\n            elif value == \'date\':\n                self.date_cols.append(key)\n            elif value == \'ignore\':\n                self.cols_to_ignore.append(key)\n            elif value in expected_vals:\n                pass\n            else:\n                raise ValueError(\'We are not sure how to process this column of data: \' + str(value) + \'. Please pass in ""output"", ""categorical"", ""ignore"", ""nlp"", or ""date"".\')\n        if found_output_column is False:\n            print(\'Here is the column_descriptions that was passed in:\')\n            print(self.column_descriptions)\n            raise ValueError(\'In your column_descriptions, please make sure exactly one column has the value ""output"", which is the value we will be training models to predict.\')\n\n        # We will be adding one new categorical variable for each date col\n        # Be sure to add it here so the rest of the pipeline knows to handle it as a categorical column\n        for date_col in self.date_cols:\n            self.column_descriptions[date_col + \'_day_part\'] = \'categorical\'\n\n        self.cols_to_ignore = set(self.cols_to_ignore)\n\n\n    # We use _construct_pipeline at both the start and end of our training.\n    # At the start, it constructs the pipeline from scratch\n    # At the end, it takes FeatureSelection out after we\'ve used it to restrict DictVectorizer, and adds final_model back in if we did grid search on it\n    def _construct_pipeline(self, model_name=\'LogisticRegression\', trained_pipeline=None, final_model=None, feature_learning=False, final_model_step_name=\'final_model\', prediction_interval=False, keep_cat_features=False, is_hp_search=False):\n\n        pipeline_list = []\n\n        if trained_pipeline is not None:\n            keep_cat_features = trained_pipeline.keep_cat_features\n\n        if self.user_input_func is not None:\n            if trained_pipeline is not None:\n                pipeline_list.append((\'user_func\', trained_pipeline.named_steps[\'user_func\']))\n            elif self.transformation_pipeline is None:\n                print(\'Including the user_input_func in the pipeline! Please remember to return X, and not modify the length or order of X at all.\')\n                print(\'Your function will be called as the first step of the pipeline at both training and prediction times.\')\n                pipeline_list.append((\'user_func\', FunctionTransformer(func=self.user_input_func, validate=False)))\n\n        # These parts will be included no matter what.\n        if trained_pipeline is not None:\n            pipeline_list.append((\'basic_transform\', trained_pipeline.named_steps[\'basic_transform\']))\n        else:\n            pipeline_list.append((\'basic_transform\', utils_data_cleaning.BasicDataCleaning(column_descriptions=self.column_descriptions)))\n\n        if self.perform_feature_scaling is True:\n            if trained_pipeline is not None:\n                pipeline_list.append((\'scaler\', trained_pipeline.named_steps[\'scaler\']))\n            else:\n                if model_name[:12] == \'DeepLearning\':\n                    min_percentile = 0.0\n                    max_percentile = 1.0\n                    pipeline_list.append((\'scaler\', utils_scaling.CustomSparseScaler(self.column_descriptions, truncate_large_values=True)))\n                else:\n                    pipeline_list.append((\'scaler\', utils_scaling.CustomSparseScaler(self.column_descriptions)))\n\n\n        if trained_pipeline is not None:\n            pipeline_list.append((\'dv\', trained_pipeline.named_steps[\'dv\']))\n        else:\n            pipeline_list.append((\'dv\', DataFrameVectorizer.DataFrameVectorizer(sparse=True, column_descriptions=self.column_descriptions, keep_cat_features=keep_cat_features)))\n\n\n        if self.perform_feature_selection == True:\n            if trained_pipeline is not None:\n                # This is the step we are trying to remove from the trained_pipeline, since it has already been combined with dv using dv.restrict\n                pass\n            else:\n                pipeline_list.append((\'feature_selection\', utils_feature_selection.FeatureSelectionTransformer(type_of_estimator=self.type_of_estimator, column_descriptions=self.column_descriptions, feature_selection_model=\'SelectFromModel\') ))\n\n        if trained_pipeline is not None:\n            # First, check and see if we have any steps with some version of keyword matching on something like \'intermediate_model_predictions\' or \'feature_learning_model\' or \'ensemble_model\' or something like that in them.\n            # add all of those steps\n            # then try to add in the final_model that was passed in as a param\n            # if it\'s none, then we\'ve already added in the final model with our keyword matching above!\n            for step in trained_pipeline.steps:\n                step_name = step[0]\n                if step_name[-6:] == \'_model\':\n                    pipeline_list.append((step_name, trained_pipeline.named_steps[step_name]))\n\n            # Handling the case where we have run gscv on just the final model itself, and we now need to integrate it back into the rest of the pipeline\n            if final_model is not None:\n                pipeline_list.append((final_model_step_name, final_model))\n        else:\n\n            try:\n                training_features = self._get_trained_feature_names()\n            except:\n                training_features = None\n\n            training_prediction_intervals = False\n            params = None\n\n            if prediction_interval is not False:\n                params = {}\n                params[\'loss\'] = \'quantile\'\n                params[\'alpha\'] = prediction_interval\n                params[\'n_estimators\'] = 100\n                params[\'learning_rate\'] = 0.15\n                params.update(self.prediction_interval_params)\n                training_prediction_intervals = True\n\n            elif feature_learning == False:\n                # Do not pass in our training_params for the feature_learning model\n                params = self.training_params\n\n            final_model = utils_models.get_model_from_name(model_name, training_params=params)\n            pipeline_list.append((\'final_model\', utils_model_training.FinalModelATC(model=final_model, type_of_estimator=self.type_of_estimator, ml_for_analytics=self.ml_for_analytics, name=self.name, _scorer=self._scorer, feature_learning=feature_learning, uncertainty_model=self.need_to_train_uncertainty_model, training_prediction_intervals=training_prediction_intervals, column_descriptions=self.column_descriptions, training_features=training_features, keep_cat_features=keep_cat_features, is_hp_search=is_hp_search, X_test=self.X_test, y_test=self.y_test)))\n\n        constructed_pipeline = utils.ExtendedPipeline(pipeline_list, keep_cat_features=keep_cat_features, name=self.name, training_features=self.training_features)\n        return constructed_pipeline\n\n\n    def _get_estimator_names(self):\n        if self.type_of_estimator == \'regressor\':\n\n            base_estimators = [\'GradientBoostingRegressor\']\n\n            if self.compare_all_models != True:\n                return base_estimators\n            else:\n                base_estimators.append(\'RANSACRegressor\')\n                base_estimators.append(\'RandomForestRegressor\')\n                base_estimators.append(\'LinearRegression\')\n                base_estimators.append(\'AdaBoostRegressor\')\n                base_estimators.append(\'ExtraTreesRegressor\')\n                return base_estimators\n\n        elif self.type_of_estimator == \'classifier\':\n\n            base_estimators = [\'GradientBoostingClassifier\']\n\n            if self.compare_all_models != True:\n                return base_estimators\n            else:\n                base_estimators.append(\'LogisticRegression\')\n                base_estimators.append(\'RandomForestClassifier\')\n                return base_estimators\n\n        else:\n            raise(\'TypeError: type_of_estimator must be either ""classifier"" or ""regressor"".\')\n\n    def _prepare_for_training(self, X):\n\n        # We accept input as either a DataFrame, or as a list of dictionaries. Internally, we use DataFrames. So if the user gave us a list, convert it to a DataFrame here.\n        if isinstance(X, list):\n            X_df = pd.DataFrame(X)\n            del X\n        else:\n            X_df = X.copy()\n\n        # To keep this as light in memory as possible, immediately remove any columns that the user has already told us should be ignored\n        if len(self.cols_to_ignore) > 0:\n            X_df = utils.safely_drop_columns(X_df, self.cols_to_ignore)\n\n        # Having duplicate columns can really screw things up later. Remove them here, with user logging to tell them what we\'re doing\n        X_df = utils.drop_duplicate_columns(X_df)\n\n        # If we\'re writing training results to file, create the new empty file name here\n        if self.write_gs_param_results_to_file:\n            self.gs_param_file_name = \'most_recent_pipeline_grid_search_result.csv\'\n            try:\n                os.remove(self.gs_param_file_name)\n            except:\n                pass\n\n        # Remove the output column from the dataset, and store it into the y varaible\n        y = list(X_df[self.output_column])\n        X_df.drop(self.output_column, axis=1, inplace=True)\n\n        # Drop all rows that have an empty value for our output column\n        # User logging so they can adjust if they pass in a bunch of bad values:\n        X_df, y = utils.drop_missing_y_vals(X_df, y, self.output_column)\n\n        # If this is a classifier, try to turn all the y values into proper ints\n        # Some classifiers play more nicely if you give them category labels as ints rather than strings, so we\'ll make our jobs easier here if we can.\n        if self.type_of_estimator == \'classifier\':\n            # The entire column must be turned into floats. If any value fails, don\'t convert anything in the column to floats\n            try:\n                y_ints = []\n                for val in y:\n                    y_ints.append(int(val))\n                y = y_ints\n            except:\n                pass\n        else:\n            # If this is a regressor, turn all the values into floats if possible, and remove this row if they cannot be turned into floats\n            indices_to_delete = []\n            y_floats = []\n            bad_vals = []\n            for idx, val in enumerate(y):\n                try:\n                    float_val = utils_data_cleaning.clean_val(val)\n                    y_floats.append(float_val)\n                except ValueError as err:\n                    indices_to_delete.append(idx)\n                    bad_vals.append(val)\n\n            y = y_floats\n\n            # Even more verbose logging here since these values are not just missing, they\'re strings for a regression problem\n            if len(indices_to_delete) > 0:\n                print(\'The y values given included some bad values that the machine learning algorithms will not be able to train on.\')\n                print(\'The rows at these indices have been deleted because their y value could not be turned into a float:\')\n                print(indices_to_delete)\n                print(\'These were the bad values\')\n                print(bad_vals)\n                X_df.drop(X_df.index[indices_to_delete], axis=0, inplace=True)\n\n        clean_descriptions = {}\n        col_names = set(X_df.columns)\n        for k, v in self.column_descriptions.items():\n            if k in col_names or \'_day_part\' in k or v == \'output\':\n                clean_descriptions[k] = v\n        self.column_descriptions = clean_descriptions\n\n        return X_df, y\n\n\n    def _consolidate_pipeline(self, transformation_pipeline, final_model=None):\n        # First, restrict our DictVectorizer or DataFrameVectorizer\n        # This goes through and has DV only output the items that have passed our support mask\n        # This has a number of benefits: speeds up computation, reduces memory usage, and combines several transforms into a single, easy step\n        # It also significantly reduces the size of dv.vocabulary_ which can get quite large\n\n        try:\n            feature_selection = transformation_pipeline.named_steps[\'feature_selection\']\n            feature_selection_mask = feature_selection.support_mask\n            transformation_pipeline.named_steps[\'dv\'].restrict(feature_selection_mask)\n        except KeyError:\n            pass\n\n        # We have overloaded our _construct_pipeline method to work both to create a new pipeline from scratch at the start of training, and to go through a trained pipeline in exactly the same order and steps to take a dedicated FeatureSelection model out of an already trained pipeline\n        # In this way, we ensure that we only have to maintain a single centralized piece of logic for the correct order a pipeline should follow\n        trained_pipeline_without_feature_selection = self._construct_pipeline(trained_pipeline=transformation_pipeline, final_model=final_model)\n\n        return trained_pipeline_without_feature_selection\n\n    def set_params_and_defaults(self, X_df, user_input_func=None, optimize_final_model=None, write_gs_param_results_to_file=True, perform_feature_selection=None, verbose=True, X_test=None, y_test=None, ml_for_analytics=True, take_log_of_y=None, model_names=None, perform_feature_scaling=True, calibrate_final_model=False, _scorer=None, scoring=None, verify_features=False, training_params=None, grid_search_params=None, compare_all_models=False, cv=2, feature_learning=False, fl_data=None, optimize_feature_learning=False, train_uncertainty_model=None, uncertainty_data=None, uncertainty_delta=None, uncertainty_delta_units=None, calibrate_uncertainty=False, uncertainty_calibration_settings=None, uncertainty_calibration_data=None, uncertainty_delta_direction=\'both\', advanced_analytics=True, analytics_config=None, prediction_intervals=None, predict_intervals=None, ensemble_config=None, trained_transformation_pipeline=None, transformed_X=None, transformed_y=None, return_transformation_pipeline=False, X_test_already_transformed=False, skip_feature_responses=None, prediction_interval_params=None):\n\n        self.user_input_func = user_input_func\n        self.optimize_final_model = optimize_final_model\n        self.write_gs_param_results_to_file = write_gs_param_results_to_file\n        self.ml_for_analytics = ml_for_analytics\n        self.training_features = None\n\n        if X_test is not None:\n            X_test, y_test = utils.drop_missing_y_vals(X_test, y_test, self.output_column)\n\n        self.X_test = X_test\n        self.y_test = y_test\n        self.X_test_already_transformed = X_test_already_transformed\n\n        if self.type_of_estimator == \'regressor\':\n            self.take_log_of_y = take_log_of_y\n\n        self.compare_all_models = compare_all_models\n        # We expect model_names to be a list of strings\n        if isinstance(model_names, str):\n            # If the user passes in a single string, put it in a list\n            self.model_names = [model_names]\n        else:\n            self.model_names = model_names\n\n        # If the user passed in a valid value for model_names (not None, and not a list where the only thing is None)\n        if self.model_names is None or (len(self.model_names) == 1 and self.model_names[0] is None):\n            self.model_names = self._get_estimator_names()\n\n\n        if \'DeepLearningRegressor\' in self.model_names or \'DeepLearningClassifier\' in self.model_names or feature_learning == True:\n            if perform_feature_scaling is None or perform_feature_scaling == True:\n                self.perform_feature_scaling = True\n            else:\n                print(\'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\')\n                print(\'Heard that we should not perform feature_scaling, but we should train a Deep Learning model. Note that feature_scaling is typically useful and frequently essential for deep learning. We STRONGLY suggest not setting perform_feature_scaling=False.\')\n                warnings.warn(\'It is a best practice, and often necessary for training, to perform_feature_scaling while doing Deep Learning.\')\n                self.perform_feature_scaling = perform_feature_scaling\n        else:\n            if perform_feature_scaling is None:\n                if utils.is_linear_model(self.model_names):\n                    self.perform_feature_scaling = True\n                else:\n                    self.perform_feature_scaling = False\n            else:\n                self.perform_feature_scaling = perform_feature_scaling\n\n        self.calibrate_final_model = calibrate_final_model\n        self.scoring = scoring\n        if training_params is None:\n            self.training_params = {}\n        else:\n            self.training_params = training_params\n        self.user_gs_params = grid_search_params\n        if self.user_gs_params is not None:\n            self.optimize_final_model = True\n        self.cv = cv\n        if ensemble_config is None:\n            self.ensemble_config = []\n        else:\n            self.ensemble_config = ensemble_config\n\n        self.calibrate_uncertainty = calibrate_uncertainty\n        self.uncertainty_calibration_data = uncertainty_calibration_data\n        if uncertainty_delta_direction is None:\n            uncertainty_delta_direction = \'both\'\n        self.uncertainty_delta_direction = uncertainty_delta_direction.lower()\n        if self.uncertainty_delta_direction not in [\'both\', \'directional\']:\n            raise ValueError(\'Please pass in either ""both"" or ""directional"" for uncertainty_delta_direction\')\n\n        if uncertainty_calibration_settings is None:\n            self.uncertainty_calibration_settings = {\n                \'num_buckets\': 10\n                , \'percentiles\': [25, 50, 75]\n            }\n        else:\n            self.uncertainty_calibration_settings = uncertainty_calibration_settings\n\n        if advanced_analytics is None:\n            self.advanced_analytics = True\n        else:\n            self.advanced_analytics = advanced_analytics\n\n        default_analytics_config = {\n            \'percent_rows\': 0.1\n            , \'min_rows\': 10000\n            , \'cols_to_ignore\': []\n            , \'file_name\': \'auto_ml_analytics_results_\' + self.output_column + \'.csv\'\n            , \'col_std_multiplier\': 0.5\n        }\n        if analytics_config is None:\n            self.analytics_config = default_analytics_config\n        else:\n            updated_analytics_config = default_analytics_config.copy()\n            updated_analytics_config = updated_analytics_config.update(analytics_config)\n            self.analytics_config = updated_analytics_config\n\n\n        self.perform_feature_selection = perform_feature_selection\n        if skip_feature_responses != True:\n            self.skip_feature_responses = False\n        else:\n            self.skip_feature_responses = skip_feature_responses\n\n        # Let the user pass in \'prediction_intervals\' and \'predict_intervals\' interchangeably\n        if predict_intervals is not None and prediction_intervals is None:\n            prediction_intervals = predict_intervals\n\n        if prediction_interval_params is None:\n            self.prediction_interval_params = {}\n        else:\n            self.prediction_interval_params = prediction_interval_params\n\n        if prediction_intervals is None:\n            self.calculate_prediction_intervals = False\n        else:\n            if isinstance(prediction_intervals, bool):\n                # This is to allow the user to pass in their own bounds here, rather than having to just use our 5% and 95% bounds\n                self.calculate_prediction_intervals = prediction_intervals\n            else:\n                self.calculate_prediction_intervals = True\n\n            if prediction_intervals == True:\n                self.prediction_intervals = [0.05, 0.95]\n            else:\n                self.prediction_intervals = prediction_intervals\n\n        self.train_uncertainty_model = train_uncertainty_model\n        if self.train_uncertainty_model == True and self.type_of_estimator == \'classifier\':\n            print(\'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\')\n            print(\'Right now uncertainty predictions are only supported for regressors. The "".predict_proba()"" method of classifiers is a reasonable workaround if you are looking for uncertainty predictions for a classifier\')\n            print(\'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\')\n            raise ValueError(\'train_uncertainty_model is only supported for regressors\')\n        self.need_to_train_uncertainty_model = train_uncertainty_model\n        self.uncertainty_data = uncertainty_data\n\n        # TODO: more input validation for calibrate_uncertainty\n        # make sure we have all the base features in place before taking in the advanced settings\n        # make sure people include num_buckets and \'percentiles\' in their uc_settings\n        # make sure the uc_data has the output column we need for the base predictor\n        if uncertainty_delta is not None:\n            if uncertainty_delta_units is None:\n                print(\'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\')\n                print(\'We received an uncertainty_delta, but do not know the units this is measured in. Please pass in one of [""absolute"", ""percentage""]\')\n                print(\'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\')\n                raise ValueError(\'We received a value for uncertainty_delta, but the data passed in for uncertainty_delta_units is missing\')\n            self.uncertainty_delta = uncertainty_delta\n            self.uncertainty_delta_units = uncertainty_delta_units\n        else:\n            self.uncertainty_delta = \'std\'\n            self.uncertainty_delta_units = \'absolute\'\n\n        if self.train_uncertainty_model == True and self.uncertainty_data is None:\n            print(\'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\')\n            print(\'Saw that train_uncertainty_model is True, but there is no data passed in for uncertainty_data, which is needed to train the uncertainty estimator\')\n            warnings.warn(\'Please pass in uncertainty_data which is the dataset that will be used to train the uncertainty estimator.\')\n            print(\'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\')\n            raise ValueError(\'The data passed in for uncertainty_data is missing\')\n\n\n        self.optimize_feature_learning = optimize_feature_learning\n        self.feature_learning = feature_learning\n        if self.feature_learning == True:\n            if fl_data is None:\n                print(\'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\')\n                print(\'Saw that feature_learning is True, but there is no data passed in for fl_data, which is needed to train the feature_learning estimator\')\n                warnings.warn(\'Please pass in fl_data which is the dataset that will be used to train the feature_learning estimator.\')\n                print(\'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\')\n                raise ValueError(\'The data passed in for fl_data is missing\')\n            self.fl_data = fl_data\n\n            if self.perform_feature_scaling == False:\n                print(\'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\')\n                print(\'Heard that we should not perform feature_scaling, but we should perform feature_learning. Note that feature_scaling is typically useful for deep learning, which is what we use for feature_learning. If you want a little more model accuracy from the feature_learning step, consider not passing in perform_feature_scaling=False\')\n                warnings.warn(\'Consider allowing auto_ml to perform_feature_scaling in conjunction with feature_learning\')\n\n            if self.perform_feature_selection == True:\n                print(\'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\')\n                print(\'We are not currently supporting perform_feature_selection with this release of feature_learning. We will override perform_feature_selection to False and continue with training.\')\n                warnings.warn(\'perform_feature_selection=True is not currently supported with feature_learning.\')\n            self.perform_feature_selection = False\n\n            if (isinstance(X_df, pd.DataFrame) and X_df.equals(fl_data)) or (isinstance(X_df, list) and X_df == fl_data):\n                print(\'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\')\n                print(\'You must pass in different data for fl_data and your training data. This is true both philosophically (you are much more likely to overfit if fl_data == training_data), and logistically (we remove the y column from both datasets, which will throw an error)\')\n                print(\'If you are looking for a quick and easy way of splitting the data, use scikit-learn\\\'s train_test_split: df_train, fl_data = train_test_split(df_train, test_size=0.33)  \')\n                print(\'Or, if you insist on using the same dataset for both, you must at least copy it:\')\n                print(\'ml_predictor.train(df_train, feature_learning=True, fl_data=df_train.copy())\')\n                warnings.warn(\'Your fl_data and df_train must be different datasets. Use train_test_split, or at least copy the data for your fl_data\')\n\n        if trained_transformation_pipeline is None:\n            self.transformation_pipeline = None\n        else:\n            print(\'We will be using the previously trained transformation pipeline you passed in\')\n            print(\'Be cautious when passing in a trained transformation pipeline- make sure that it is trained on exactly the same data.\')\n            self.transformation_pipeline = trained_transformation_pipeline\n\n        if transformed_X is not None and transformed_y is None:\n            print(\'Please pass in both a transformed_X and transformed_y\')\n            raise(ValueError(\'Please pass in transformed_y if you are passing in transformed_X\'))\n\n        self.return_transformation_pipeline = return_transformation_pipeline\n\n\n\n    # We are taking in scoring here to deal with the unknown behavior around multilabel classification below\n    def _clean_data_and_prepare_for_training(self, data, scoring):\n\n        X_df, y = self._prepare_for_training(data)\n\n        if self.take_log_of_y:\n            y = [math.log(val) for val in y]\n            self.took_log_of_y = True\n\n        self.X_df = X_df\n        self.y = y\n\n        # Unless the user has told us to, don\'t perform feature selection unless we have a pretty decent amount of data\n        if self.perform_feature_selection is None:\n            if len(X_df.columns) < 50 or len(X_df) < 100000:\n                self.perform_feature_selection = False\n            else:\n                self.perform_feature_selection = True\n\n        self.set_scoring(y, scoring=scoring)\n\n        return X_df, y\n\n\n    def set_scoring(self, y, scoring=None):\n        # TODO: we\'re not using ClassificationScorer for multilabel classification. Why?\n        # Probably has to do with self.scoring vs self._scorer = scoring\n        if self.type_of_estimator == \'classifier\':\n            if len(set(y)) > 2 and self.scoring is None:\n                self.scoring = \'accuracy_score\'\n            else:\n                scoring = utils_scoring.ClassificationScorer(self.scoring)\n            self._scorer = scoring\n        else:\n            scoring = utils_scoring.RegressionScorer(self.scoring)\n            self._scorer = scoring\n\n\n    def fit_feature_learning_and_transformation_pipeline(self, X_df, fl_data, y):\n        fl_data_cleaned, fl_y = self._clean_data_and_prepare_for_training(fl_data, self.scoring)\n        # Only import this if we have to, because it takes a while to import in some environments\n        from keras.models import Model\n\n        len_X_df = len(X_df)\n        combined_training_data = pd.concat([X_df, fl_data_cleaned], axis=0)\n        combined_y = y + fl_y\n\n        if self.type_of_estimator == \'classifier\':\n            fl_estimator_names = [\'DeepLearningClassifier\']\n        elif self.type_of_estimator == \'regressor\':\n            fl_estimator_names = [\'DeepLearningRegressor\']\n\n        # For performance reasons, I believe it is critical to only have one transformation pipeline, no matter how many estimators we eventually build on top. Getting predictions from a trained estimator is typically super quick. We can easily get predictions from 10 trained models in a production-ready amount of time.But the transformation pipeline is not so quick that we can duplicate it 10 times.\n        combined_transformed_data = self.fit_transformation_pipeline(combined_training_data, combined_y, fl_estimator_names)\n\n        fl_indices = [i for i in range(len_X_df, combined_transformed_data.shape[0])]\n        fl_data_transformed = combined_transformed_data[fl_indices]\n\n        # fit a train_final_estimator\n        feature_learning_step = self.train_ml_estimator(fl_estimator_names, self._scorer, fl_data_transformed, fl_y, feature_learning=True)\n\n        # Split off the final layer/find a way to get the output from the penultimate layer\n        fl_model = feature_learning_step.model\n\n        feature_output_model = Model(inputs=fl_model.model.input, outputs=fl_model.model.get_layer(\'penultimate_layer\').output)\n        feature_learning_step.model = feature_output_model\n\n        # Add those to the list in our DV so we know what to do with them for analytics purposes\n        feature_learning_names = []\n        for idx in range(10):\n            feature_learning_names.append(\'feature_learning_\' + str(idx + 1))\n\n        # TODO:\n        self.transformation_pipeline.named_steps[\'dv\'].feature_names_ += feature_learning_names\n\n        # add the estimator to the end of our transformation pipeline\n        self.transformation_pipeline = self._construct_pipeline(trained_pipeline=self.transformation_pipeline, final_model=feature_learning_step, final_model_step_name=\'feature_learning_model\')\n\n        # Pass our already-transformed X_df just through the feature_learning_step.transform. This avoids duplicate computationn\n        indices = [i for i in range(len_X_df)]\n        X_df_transformed = combined_transformed_data[indices]\n        X_df = feature_learning_step.transform(X_df_transformed)\n\n        return X_df\n\n\n    def train(self, raw_training_data, user_input_func=None, optimize_final_model=None, write_gs_param_results_to_file=True, perform_feature_selection=None, verbose=True, X_test=None, y_test=None, ml_for_analytics=True, take_log_of_y=None, model_names=None, perform_feature_scaling=None, calibrate_final_model=False, _scorer=None, scoring=None, verify_features=False, training_params=None, grid_search_params=None, compare_all_models=False, cv=2, feature_learning=False, fl_data=None, optimize_feature_learning=False, train_uncertainty_model=False, uncertainty_data=None, uncertainty_delta=None, uncertainty_delta_units=None, calibrate_uncertainty=False, uncertainty_calibration_settings=None, uncertainty_calibration_data=None, uncertainty_delta_direction=None, advanced_analytics=None, analytics_config=None, prediction_intervals=None, predict_intervals=None, ensemble_config=None, trained_transformation_pipeline=None, transformed_X=None, transformed_y=None, return_transformation_pipeline=False, X_test_already_transformed=False, skip_feature_responses=None, prediction_interval_params=None):\n\n        self.set_params_and_defaults(raw_training_data, user_input_func=user_input_func, optimize_final_model=optimize_final_model, write_gs_param_results_to_file=write_gs_param_results_to_file, perform_feature_selection=perform_feature_selection, verbose=verbose, X_test=X_test, y_test=y_test, ml_for_analytics=ml_for_analytics, take_log_of_y=take_log_of_y, model_names=model_names, perform_feature_scaling=perform_feature_scaling, calibrate_final_model=calibrate_final_model, _scorer=_scorer, scoring=scoring, verify_features=verify_features, training_params=training_params, grid_search_params=grid_search_params, compare_all_models=compare_all_models, cv=cv, feature_learning=feature_learning, fl_data=fl_data, optimize_feature_learning=False, train_uncertainty_model=train_uncertainty_model, uncertainty_data=uncertainty_data, uncertainty_delta=uncertainty_delta, uncertainty_delta_units=uncertainty_delta_units, calibrate_uncertainty=calibrate_uncertainty, uncertainty_calibration_settings=uncertainty_calibration_settings, uncertainty_calibration_data=uncertainty_calibration_data, uncertainty_delta_direction=uncertainty_delta_direction, prediction_intervals=prediction_intervals, predict_intervals=predict_intervals, ensemble_config=ensemble_config, trained_transformation_pipeline=trained_transformation_pipeline, transformed_X=transformed_X, transformed_y=transformed_y, return_transformation_pipeline=return_transformation_pipeline, X_test_already_transformed=X_test_already_transformed, skip_feature_responses=skip_feature_responses, prediction_interval_params=prediction_interval_params)\n\n        if verbose:\n            print(\'Welcome to auto_ml! We\\\'re about to go through and make sense of your data using machine learning, and give you a production-ready pipeline to get predictions with.\\n\')\n            print(\'If you have any issues, or new feature ideas, let us know at http://auto.ml\')\n            print(\'You are running on version {}\'.format(auto_ml_version))\n\n        if transformed_X is None:\n            X_df, y = self._clean_data_and_prepare_for_training(raw_training_data, scoring)\n            del raw_training_data\n            self.training_features = list(X_df.columns)\n\n            if self.transformation_pipeline is None:\n                if self.feature_learning == True:\n                    X_df = self.fit_feature_learning_and_transformation_pipeline(X_df, fl_data, y)\n                else:\n                    # If the user passed in a valid value for model_names (not None, and not a list where the only thing is None)\n                    if self.model_names is not None and not (len(self.model_names) == 1 and self.model_names[0] is None):\n                        estimator_names = self.model_names\n                    else:\n                        estimator_names = self._get_estimator_names()\n\n                    X_df = self.fit_transformation_pipeline(X_df, y, estimator_names)\n            else:\n                X_df = self.transformation_pipeline.transform(X_df)\n        else:\n            X_df, y = utils.drop_missing_y_vals(transformed_X, transformed_y)\n            del transformed_X\n            del transformed_y\n            try:\n                self.training_features = list(X_df.columns)\n            except:\n                pass\n\n            self.set_scoring(y)\n\n        if self.X_test is not None and self.X_test_already_transformed == False:\n            self.X_test = self.transformation_pipeline.transform(self.X_test)\n\n        # This is our main logic for how we train the final model\n        self.trained_final_model = self.train_ml_estimator(self.model_names, self._scorer, X_df, y)\n\n        if self.ensemble_config is not None and len(self.ensemble_config) > 0:\n            self._train_ensemble(X_df, y)\n\n        if self.need_to_train_uncertainty_model == True:\n            self._create_uncertainty_model(uncertainty_data, scoring, y, uncertainty_calibration_data)\n\n        # Calibrate the probability predictions from our final model\n        if self.calibrate_final_model is True:\n            self.trained_final_model.model = self._calibrate_final_model(self.trained_final_model.model, X_test, y_test)\n\n        if self.calculate_prediction_intervals is True:\n            # TODO: parallelize these!\n            interval_predictors = []\n            for percentile in self.prediction_intervals:\n                interval_predictor = self.train_ml_estimator([\'GradientBoostingRegressor\'], self._scorer, X_df, y, prediction_interval=percentile)\n                predictor_tup = (\'interval_{}\'.format(percentile), interval_predictor)\n                interval_predictors.append(predictor_tup)\n\n            self.trained_final_model.interval_predictors = interval_predictors\n\n\n        self.trained_pipeline = self._consolidate_pipeline(self.transformation_pipeline, self.trained_final_model)\n\n        # verify_features is not enabled by default. It adds a significant amount to the file size of the saved pipelines.\n        # If you are interested in submitting a PR to reduce the saved file size, there are definitely some optimizations you can make!\n        if verify_features == True:\n            self._prepare_for_verify_features()\n\n        # Delete values that we no longer need that are just taking up space.\n        del self.X_test\n        del self.y_test\n        del self.X_test_already_transformed\n        del X_df\n\n        if self.return_transformation_pipeline:\n            return self.transformation_pipeline\n        return self\n\n\n    def _create_uncertainty_model(self, uncertainty_data, scoring, y, uncertainty_calibration_data):\n        # 1. Add base_prediction to our dv for analytics purposes\n        # Note that we will have to be cautious that things all happen in the exact same order as we expand what we do post-DV over time\n        # Adding this one directly- we don\'t want dfv to transform it necessarily, we just want it there for getting feature_names later for printing feature_importances\n        self.transformation_pipeline.named_steps[\'dv\'].feature_names_.append(\'base_prediction\')\n        # 2. Get predictions from our base predictor on our uncertainty data\n        uncertainty_data, y_uncertainty = self._clean_data_and_prepare_for_training(uncertainty_data, scoring)\n\n        uncertainty_data_transformed = self.transformation_pipeline.transform(uncertainty_data)\n\n        base_predictions = self.trained_final_model.predict(uncertainty_data_transformed)\n        base_predictions = [[val] for val in base_predictions]\n        base_predictions = np.array(base_predictions)\n        uncertainty_data_transformed = scipy.sparse.hstack([uncertainty_data_transformed, base_predictions], format=\'csr\')\n\n        # 2A. Grab the user\'s definition of uncertainty, and create the output values \'is_uncertain_prediction\'\n            # post-mvp: allow the user to pass in stuff like 1.5*std\n        if self.uncertainty_delta == \'std\':\n            y_std = np.std(y)\n            self.uncertainty_delta = 0.5 * y_std\n\n        is_uncertain_predictions = self.define_uncertain_predictions(base_predictions, y_uncertainty)\n\n        analytics_results = pd.Series(is_uncertain_predictions)\n        print(\'\\n\\nHere is the percent of values in our uncertainty training data that are classified as uncertain:\')\n        percent_uncertain = sum(is_uncertain_predictions) * 1.0 / len(is_uncertain_predictions)\n        print(percent_uncertain)\n        if percent_uncertain == 1.0:\n            print(\'Using the current definition, all rows are classified as uncertain\')\n            print(\'Here is our current definition:\')\n            print(\'self.uncertainty_delta\')\n            print(self.uncertainty_delta)\n            print(\'self.uncertainty_delta_units\')\n            print(self.uncertainty_delta_units)\n            print(\'And here is a summary of our predictions:\')\n            print(pd.Series(y_uncertainty).describe(include=\'all\'))\n            warnings.warn(\'All predictions in ojur uncertainty training data are classified as uncertain. Please redefine uncertainty so there is a mix of certain and uncertain predictions to train an uncertainty model.\')\n            return self\n\n        # 3. train our uncertainty predictor\n        uncertainty_estimator_names = [\'GradientBoostingClassifier\']\n\n        self.trained_uncertainty_model = self.train_ml_estimator(uncertainty_estimator_names, self._scorer, uncertainty_data_transformed, is_uncertain_predictions)\n\n        # 4. grab the entire uncertainty FinalModelATC object, and put it as a property on our base predictor\'s FinalModelATC (something like .trained_uncertainty_model). It\'s important to grab this entire object, for all of the edge-case handling we\'ve built in.\n        self.trained_final_model.uncertainty_model = self.trained_uncertainty_model\n\n\n        if self.calibrate_uncertainty == True:\n\n            uncertainty_calibration_data_transformed = self.transformation_pipeline.transform(self.uncertainty_calibration_data)\n            uncertainty_calibration_predictions = self.trained_final_model.predict_uncertainty(uncertainty_calibration_data_transformed)\n\n            actuals = list(uncertainty_calibration_data[self.output_column])\n            predictions = uncertainty_calibration_predictions[\'base_prediction\']\n            deltas = predictions - actuals\n            uncertainty_calibration_predictions[\'actual_deltas\'] = deltas\n\n            probas = uncertainty_calibration_predictions.uncertainty_prediction\n            num_buckets = self.uncertainty_calibration_settings[\'num_buckets\']\n\n            # If we have overlapping bucket definitions, pandas will drop those duplicates, but won\'t drop the duplicate labels\n            # So we\'ll try bucketing one time, then get the actual number of bins from that\n            try:\n                bucket_results, bins = pd.qcut(probas, q=num_buckets, retbins=True, duplicates=\'drop\')\n            except TypeError:\n                bucket_results, bins = pd.qcut(probas, q=num_buckets, retbins=True)\n\n\n            # now that we know the actual number of bins, we can create our labels, then use those to create our final set of buckets\n            bucket_labels = range(1, len(bins))\n            try:\n                bucket_results = pd.qcut(probas, q=num_buckets, labels=bucket_labels, duplicates=\'drop\')\n            except TypeError:\n                bucket_results = pd.qcut(probas, q=num_buckets, labels=bucket_labels)\n\n            uncertainty_calibration_predictions[\'bucket_num\'] = bucket_results\n\n            uc_results = OrderedDict()\n\n            for bucket in bucket_labels:\n                dataset = uncertainty_calibration_predictions[uncertainty_calibration_predictions[\'bucket_num\'] == bucket]\n\n                deltas = dataset[\'actual_deltas\']\n                uc_results[bucket] = OrderedDict()\n                uc_results[bucket][\'bucket_num\'] = bucket\n                # FUTURE: add in rmse and maybe something like median_ae\n                # FUTURE: add in max_value for each bucket_num\n                uc_results[bucket][\'max_proba\'] = np.max(dataset[\'uncertainty_prediction\'])\n\n                for perc in self.uncertainty_calibration_settings[\'percentiles\']:\n                    delta_at_percentile = np.percentile(deltas, perc)\n                    uc_results[bucket][\'percentile_\' + str(perc) + \'_delta\'] = delta_at_percentile\n\n            # make the max_proba of our last bucket_num 1\n            uc_results[bucket_labels[-1]][\'max_proba\'] = 1\n            print(\'Here are the uncertainty_calibration results, for each bucket of predicted probabilities\')\n            for num in uc_results:\n                print(uc_results[num])\n\n            self.trained_final_model.uc_results = uc_results\n\n        self.need_to_train_uncertainty_model = False\n\n    def _prepare_for_verify_features(self):\n        # Save the features we used for training to our FinalModelATC instance.\n        # This lets us provide useful information to the user when they call .predict(data, verbose=True)\n        trained_feature_names = self._get_trained_feature_names()\n        self.trained_pipeline.set_params(final_model__training_features=trained_feature_names)\n        # We will need to know which columns are categorical/ignored/nlp when verifying features\n        self.trained_pipeline.set_params(final_model__column_descriptions=self.column_descriptions)\n\n\n    def _calibrate_final_model(self, trained_model, X_test, y_test):\n\n        if X_test is None or y_test is None:\n            print(\'X_test or y_test was not present while trying to calibrate the final model\')\n            print(\'Please pass in both X_test and y_test to calibrate the final model\')\n            print(\'Skipping model calibration\')\n            return trained_model\n\n        print(\'Now calibrating the final model so the probability predictions line up with the observed probabilities in the X_test and y_test datasets you passed in.\')\n        print(\'Note: the validation scores printed above are truly validation scores: they were scored before the model was calibrated to this data.\')\n        print(\'However, now that we are calibrating on the X_test and y_test data you gave us, it is no longer accurate to call this data validation data, since the model is being calibrated to it. As such, you must now report a validation score on a different dataset, or report the validation score used above before the model was calibrated to X_test and y_test. \')\n\n        if len(X_test) < 1000:\n            calibration_method = \'sigmoid\'\n        else:\n            calibration_method = \'isotonic\'\n\n        calibrated_classifier = CalibratedClassifierCV(trained_model, method=calibration_method, cv=\'prefit\')\n\n        # We need to make sure X_test has been processed the exact same way y_test has.\n        X_test_processed = self.transformation_pipeline.transform(X_test)\n\n        try:\n            calibrated_classifier = calibrated_classifier.fit(X_test_processed, y_test)\n        except TypeError as e:\n            if scipy.sparse.issparse(X_test_processed):\n                X_test_processed = X_test_processed.toarray()\n\n                calibrated_classifier = calibrated_classifier.fit(X_test_processed, y_test)\n            else:\n                raise(e)\n\n        return calibrated_classifier\n\n\n    def fit_single_pipeline(self, X_df, y, model_name, feature_learning=False, prediction_interval=False):\n\n        full_pipeline = self._construct_pipeline(model_name=model_name, feature_learning=feature_learning, prediction_interval=prediction_interval, keep_cat_features=self.transformation_pipeline.keep_cat_features)\n        ppl = full_pipeline.named_steps[\'final_model\']\n        if self.verbose:\n            print(\'\\n\\n********************************************************************************************\')\n            if self.name is not None:\n                print(self.name)\n            if prediction_interval is not False:\n                print(\'About to fit a {} quantile regressor to predict the prediction_interval for the {}th percentile\'.format(model_name, int(prediction_interval * 100)))\n            else:\n                print(\'About to fit the pipeline for the model \' + model_name + \' to predict \' + self.output_column)\n            print(\'Started at:\')\n            start_time = datetime.datetime.now().replace(microsecond=0)\n            print(start_time)\n\n        ppl.fit(X_df, y)\n\n        if self.verbose:\n            print(\'Finished training the pipeline!\')\n            print(\'Total training time:\')\n            print(datetime.datetime.now().replace(microsecond=0) - start_time)\n\n        # Don\'t report feature_responses (or nearly anything else) if this is just the feature_learning stage\n        # That saves a considerable amount of time\n        if feature_learning == False:\n            self.print_results(model_name, ppl, X_df, y)\n\n        return ppl\n\n\n    # We have broken our model training into separate components. The first component is always going to be fitting a transformation pipeline. The great part about separating the feature transformation step is that now we can perform other work on the final step, and not have to repeat the sometimes time-consuming step of the transformation pipeline.\n    # NOTE: if included, we will be fitting a feature selection step here. This can get messy later on with ensembling if we end up training on different y values.\n    def fit_transformation_pipeline(self, X_df, y, model_names):\n\n        keep_cat_features = True\n        for model_name in model_names:\n            keep_cat_features = keep_cat_features and model_name in [\'LGBMRegressor\', \'LGBMClassifier\', \'CatBoostRegressor\', \'CatBoostClassifier\']\n\n        self.keep_cat_features = keep_cat_features\n        ppl = self._construct_pipeline(model_name=model_names[0], keep_cat_features=self.keep_cat_features)\n        ppl.steps.pop()\n\n        # We are intentionally overwriting X_df here to try to save some memory space\n        X_df = ppl.fit_transform(X_df, y)\n\n        self.transformation_pipeline = self._consolidate_pipeline(ppl)\n\n        return X_df\n\n    def create_feature_responses(self, model, X_transformed, y, top_features=None):\n        print(\'Calculating feature responses, for advanced analytics.\')\n\n        if top_features is None:\n            top_features = self._get_trained_feature_names()\n        # figure out how many rows to keep\n        orig_row_count = X_transformed.shape[0]\n        orig_column_count = X_transformed.shape[1]\n        # If we have fewer than 10000 rows, use all of them, regardless of user input\n        # This approach only works if there are a decent number of rows, so we will try to put some safeguard in place to help the user from getting results that are too misleading\n        row_multiplier = 1\n        if orig_column_count > 1000:\n            row_multiplier = 0.25\n\n        if orig_row_count <= 10000:\n            num_rows_to_use = orig_row_count\n            if row_multiplier < 1:\n                X, ignored_X, y, ignored_y = train_test_split(X_transformed, y, train_size=row_multiplier )\n            else:\n                X = X_transformed\n        else:\n            percent_row_count = int(self.analytics_config[\'percent_rows\'] * orig_row_count)\n            num_rows_to_use = min(orig_row_count, percent_row_count, 10000)\n            num_rows_to_use = int(num_rows_to_use * row_multiplier)\n            X, ignored_X, y, ignored_y = train_test_split(X_transformed, y, train_size=num_rows_to_use)\n\n        if scipy.sparse.issparse(X):\n            X = X.toarray()\n\n        # Get our baseline predictions\n        if self.type_of_estimator == \'regressor\':\n            base_predictions = model.predict(X)\n        elif self.type_of_estimator == \'classifier\':\n            base_predictions = model.predict_proba(X)\n            base_predictions = [x[1] for x in base_predictions]\n\n        feature_names = self._get_trained_feature_names()\n\n        def create_one_feature_response(col_idx, col_name):\n            if col_name not in top_features:\n                return None\n            col_result = {}\n            col_result[\'Feature Name\'] = col_name\n            if col_name[:4] != \'nlp_\' and \'=\' not in col_name and self.column_descriptions.get(col_name, False) != \'categorical\':\n\n                if isinstance(X, pd.DataFrame):\n                    col_vals = X[col_name]\n                else:\n                    col_vals = X[:, col_idx]\n\n                col_std = np.nanstd(col_vals)\n                col_delta = self.analytics_config[\'col_std_multiplier\'] * col_std\n                col_result[\'Delta\'] = col_delta\n\n                # TODO: min_delta\n                # get the unique vals\n                # sort them\n                # go through and find the min_delta between consecutive vals\n                # make srue col_std is greater than min_delta\n                # if it is not, set col_std to min_delta\n\n                # Increment the values of this column by the std\n                if isinstance(X, pd.DataFrame):\n                    X[col_name] += col_delta\n                else:\n                    X[:, col_idx] += col_delta\n                if self.type_of_estimator == \'regressor\':\n                    predictions = model.predict(X)\n                elif self.type_of_estimator == \'classifier\':\n                    predictions = model.predict_proba(X)\n                    predictions = [x[1] for x in predictions]\n\n                deltas = []\n                for pred_idx, pred in enumerate(predictions):\n                    delta = pred - base_predictions[pred_idx]\n                    deltas.append(delta)\n                col_result[\'FR_Incrementing\'] = np.mean(deltas)\n                absolute_prediction_deltas = np.absolute(deltas)\n                col_result[\'FRI_abs\'] = np.mean(absolute_prediction_deltas)\n\n                median_prediction = np.median(absolute_prediction_deltas)\n                col_result[\'FRI_MAD\'] = median_prediction\n\n\n                if isinstance(X, pd.DataFrame):\n                    X[col_name] -= 2 * col_delta\n                else:\n                    X[:, col_idx] -= 2 * col_delta\n                if self.type_of_estimator == \'regressor\':\n                    predictions = model.predict(X)\n                elif self.type_of_estimator == \'classifier\':\n                    predictions = model.predict_proba(X)\n                    predictions = [x[1] for x in predictions]\n\n                deltas = []\n                for pred_idx, pred in enumerate(predictions):\n                    delta = pred - base_predictions[pred_idx]\n                    deltas.append(delta)\n                col_result[\'FR_Decrementing\'] = np.mean(deltas)\n                absolute_prediction_deltas = np.absolute(deltas)\n                col_result[\'FRD_abs\'] = np.mean(absolute_prediction_deltas)\n\n                median_prediction = np.median(absolute_prediction_deltas)\n                col_result[\'FRD_MAD\'] = median_prediction\n\n                # Put the column back to it\'s original state\n\n                if isinstance(X, pd.DataFrame):\n                    X[col_name] += col_delta\n                else:\n                    X[:, col_idx] += col_delta\n\n            return col_result\n\n        all_results = list(map(lambda tup: create_one_feature_response(col_idx=tup[0], col_name=tup[1]), enumerate(feature_names)))\n\n        all_results = [x for x in all_results if x is not None]\n\n        df_all_results = pd.DataFrame(all_results)\n\n        return df_all_results\n\n\n\n\n    def print_results(self, model_name, model, X, y):\n        # This apparently fails in some cases. I\'m not sure what those edge cases are, but the try/except block should at least allow the rest of the script to continue\n        try:\n            if self.ml_for_analytics and model_name in (\'LogisticRegression\', \'RidgeClassifier\', \'LinearRegression\', \'Ridge\'):\n                df_model_results = self._print_ml_analytics_results_linear_model(model)\n                sorted_model_results = df_model_results.sort_values(by=\'Coefficients\', ascending=False)\n                sorted_model_results = sorted_model_results.reset_index(drop=True)\n                # only grab the top 100 features from X\n                top_features = set(sorted_model_results.head(n=100)[\'Feature Name\'])\n\n                feature_responses = self.create_feature_responses(model, X, y, top_features)\n                self._join_and_print_analytics_results(feature_responses, sorted_model_results, sort_field=\'Coefficients\')\n\n            elif self.ml_for_analytics and model_name in [\'RandomForestClassifier\', \'RandomForestRegressor\', \'XGBClassifier\', \'XGBRegressor\', \'GradientBoostingRegressor\', \'GradientBoostingClassifier\', \'LGBMRegressor\', \'LGBMClassifier\', \'CatBoostRegressor\', \'CatBoostClassifier\']:\n                try:\n                    df_model_results = self._print_ml_analytics_results_random_forest(model)\n                    sorted_model_results = df_model_results.sort_values(by=\'Importance\', ascending=False)\n                    sorted_model_results = sorted_model_results.reset_index(drop=True)\n                    top_features = set(sorted_model_results.head(n=100)[\'Feature Name\'])\n\n                    if self.skip_feature_responses == True:\n                        feature_responses = None\n                    else:\n                        feature_responses = self.create_feature_responses(model, X, y, top_features)\n                    self._join_and_print_analytics_results(feature_responses, sorted_model_results, sort_field=\'Importance\')\n                except AttributeError as e:\n                    if model_name == \'XGBRegressor\':\n                        pass\n                    else:\n                        raise(e)\n\n\n            else:\n                feature_responses = self.create_feature_responses(model, X, y)\n                feature_responses[\'FR_Incrementing_abs\'] = np.absolute(feature_responses.FR_Incrementing)\n                feature_responses = feature_responses.sort_values(by=\'FR_Incrementing_abs\', ascending=False)\n                feature_responses = feature_responses.reset_index(drop=True)\n                feature_responses = feature_responses.head(n=100)\n                feature_responses = feature_responses.sort_values(by=\'FR_Incrementing_abs\', ascending=True)\n                feature_responses = feature_responses[[\'Feature Name\', \'Delta\', \'FR_Decrementing\', \'FR_Incrementing\', \'FRD_MAD\', \'FRI_MAD\']]\n                print(\'Here are our feature responses for the trained model\')\n                print(tabulate(feature_responses, headers=\'keys\', floatfmt=\'.4f\', tablefmt=\'psql\'))\n        except:\n            pass\n\n\n    def fit_grid_search(self, X_df, y, gs_params, feature_learning=False, refit=False):\n\n        model = gs_params[\'model\']\n        # Sometimes we\'re optimizing just one model, sometimes we\'re comparing a bunch of non-optimized models.\n        if isinstance(model, list):\n            model = model[0]\n\n        if len(gs_params[\'model\']) == 1:\n            # Delete this so it doesn\'t show up in our logging\n            del gs_params[\'model\']\n        model_name = utils_models.get_name_from_model(model)\n\n        gs_params[\'_scorer\'] = [self._scorer]\n\n        full_pipeline = self._construct_pipeline(model_name=model_name, feature_learning=feature_learning, is_hp_search=True, keep_cat_features=self.transformation_pipeline.keep_cat_features)\n\n        ppl = full_pipeline.named_steps[\'final_model\']\n\n        if self.verbose:\n            grid_search_verbose = 5\n        else:\n            grid_search_verbose = 0\n\n\n\n        # We only want to run EASCV when we have more than 50 parameter combinations (it efficiently searches very large spaces, but offers no benefits in small search spaces)\n        total_combinations = 1\n        for k, v in gs_params.items():\n            total_combinations *= len(v)\n\n        n_jobs = -1\n        population_size = 35\n        tournament_size = 3\n        gene_mutation_prob = 0.1\n        generations_number = 3\n\n        if os.environ.get(\'is_test_suite\', 0) == \'True\':\n            n_jobs = 1\n            population_size = 6\n            generations_number = 1\n\n        # LightGBM doesn\'t appear to play well when fighting for CPU cycles with other things\n        # However, it does, itself, parallelize pretty nicely. So let lgbm take care of the parallelization itself, which will be less memory intensive than having to duplicate the data for all the cores on the machine\n        elif model_name in [\'LGBMRegressor\', \'LGBMClassifier\', \'DeepLearningRegressor\', \'DeeplearningClassifier\']:\n            n_jobs = 1\n\n        elif total_combinations >= 50:\n            n_jobs = multiprocessing.cpu_count()\n\n        fit_evolutionary_search = False\n        if total_combinations >= 50 and model_name not in [\'CatBoostClassifier\', \'CatBoostRegressor\']:\n            fit_evolutionary_search = True\n        # For some reason, EASCV doesn\'t play nicely with CatBoost. It blows up the memory hugely, and takes forever to train\n        if fit_evolutionary_search == True:\n            gs = EvolutionaryAlgorithmSearchCV(\n                # Fit on the pipeline.\n                ppl,\n                # Two splits of cross-validation, by default\n                cv=self.cv,\n                params=gs_params,\n                # Train across all cores.\n                n_jobs=n_jobs,\n                # Be verbose (lots of printing).\n                verbose=grid_search_verbose,\n                # Print warnings when we fail to fit a given combination of parameters, but do not raise an error.\n                # Set the score on this partition to some very negative number, so that we do not choose this estimator.\n                error_score=-1000000000,\n                scoring=self._scorer.score,\n                # Don\'t allocate memory for all jobs upfront. Instead, only allocate enough memory to handle the current jobs plus an additional 50%\n                pre_dispatch=\'1.5*n_jobs\',\n                # The number of\n                population_size=population_size,\n                gene_mutation_prob=gene_mutation_prob,\n                tournament_size=tournament_size,\n                generations_number=generations_number,\n                # Do not fit the best estimator on all the data- we will do that later, possibly after increasing epochs or n_estimators\n                refit=refit\n\n            )\n\n        else:\n            gs = GridSearchCV(\n                # Fit on the pipeline.\n                ppl,\n                # Two splits of cross-validation, by default\n                cv=self.cv,\n                param_grid=gs_params,\n                # Train across all cores.\n                n_jobs=n_jobs,\n                # Be verbose (lots of printing).\n                verbose=grid_search_verbose,\n                # Print warnings when we fail to fit a given combination of parameters, but do not raise an error.\n                # Set the score on this partition to some very negative number, so that we do not choose this estimator.\n                error_score=-1000000000,\n                scoring=self._scorer.score,\n                # Don\'t allocate memory for all jobs upfront. Instead, only allocate enough memory to handle the current jobs plus an additional 50%\n                pre_dispatch=\'1.5*n_jobs\',\n                refit=refit\n            )\n\n        if self.verbose:\n            print(\'\\n\\n********************************************************************************************\')\n            if self.optimize_final_model == True:\n                print(\'Optimizing the hyperparameters for your model now\')\n                if fit_evolutionary_search == False:\n                    print(\'About to run GridSearchCV to find the optimal hyperparameters for the model \' + model_name + \' to predict \' + self.output_column)\n                else:\n                    print(\'About to run EvolutionaryAlgorithmSearchCV to find the optimal hyperparameters for the model \' + model_name + \' to predict \' + self.output_column)\n                    print(\'Population size each generation: \' + str(population_size))\n                    print(\'Number of generations: \' + str(generations_number))\n            else:\n                print(\'About to run GridSearchCV on the pipeline for several models to predict \' + self.output_column)\n                # Note that we will only report analytics results on the final model that ultimately gets selected, and trained on the entire dataset\n\n        gs.fit(X_df, y)\n\n        if self.verbose:\n            self.print_training_summary(gs)\n\n        if refit == True:\n            trained_final_model = gs.best_estimator_\n            model_name = utils_models.get_name_from_model(trained_final_model)\n            self.print_results(model_name, trained_final_model, X_df, y)\n\n        return gs\n\n\n    def create_gs_params(self, model_name):\n        grid_search_params = {}\n\n        raw_search_params = utils_models.get_search_params(model_name)\n\n        for param_name, param_list in raw_search_params.items():\n            # We need to tell GS where to set these params. In our case, it is on the ""final_model"" object, and specifically the ""model"" attribute on that object\n            grid_search_params[\'model__\' + param_name] = param_list\n\n        # Overwrite with the user-provided gs_params if they\'re provided\n        if self.user_gs_params is not None:\n            print(\'Using the grid_search_params you passed in:\')\n            print(self.user_gs_params)\n            grid_search_params.update(self.user_gs_params)\n            print(\'Here is our final list of grid_search_params:\')\n            print(grid_search_params)\n            print(\'Please note that if you want to set the grid search params for the final model specifically, they need to be prefixed with: ""model__""\')\n\n        return grid_search_params\n\n    # When we go to perform hyperparameter optimization, the hyperparameters for a GradientBoosting model will not at all align with the hyperparameters for an SVM. Doing all of that in one giant GSCV would throw errors. So we train each model in it\'s own grid search.\n    def train_ml_estimator(self, estimator_names, scoring, X_df, y, feature_learning=False, prediction_interval=False):\n\n        if prediction_interval is not False:\n            # estimator_names = [\'GradientBoostingRegressor\']\n            trained_final_model = self.fit_single_pipeline(X_df, y, estimator_names[0], feature_learning=feature_learning, prediction_interval=prediction_interval)\n\n        # Use Case 1: Super straightforward: just train a single, non-optimized model\n        elif (feature_learning == True and self.optimize_feature_learning != True) or (len(estimator_names) == 1 and self.optimize_final_model != True):\n            trained_final_model = self.fit_single_pipeline(X_df, y, estimator_names[0], feature_learning=feature_learning, prediction_interval=False)\n\n        # Use Case 2: Compare a bunch of models, but don\'t optimize any of them\n        elif len(estimator_names) > 1 and self.optimize_final_model != True:\n            grid_search_params = {}\n\n            final_model_models = map(utils_models.get_model_from_name, estimator_names)\n\n            # We have to use GSCV here to choose between the different models\n            grid_search_params[\'model\'] = list(final_model_models)\n\n            self.grid_search_params = grid_search_params\n\n            gscv_results = self.fit_grid_search(X_df, y, grid_search_params, refit=True)\n\n            trained_final_model = gscv_results.best_estimator_\n\n        # Use Case 3: One model, and optimize it!\n        # Use Case 4: Many models, and optimize them!\n        elif (feature_learning == False and self.optimize_final_model == True) or (feature_learning == True and self.optimize_feature_learning == True):\n            # Use Cases 3 & 4 are clearly highly related\n\n            all_gs_results = []\n\n            # If we just have one model, this will obviously be a very simple loop :)\n            for model_name in estimator_names:\n\n                grid_search_params = self.create_gs_params(model_name)\n                # Adding model name to gs params just to help with logging\n                grid_search_params[\'model\'] = [utils_models.get_model_from_name(model_name)]\n                # grid_search_params[\'model_name\'] = model_name\n                self.grid_search_params = grid_search_params\n\n                gscv_results = self.fit_grid_search(X_df, y, grid_search_params, feature_learning=feature_learning)\n\n                all_gs_results.append(gscv_results)\n\n\n            # Grab the first one by default\n            best_score = all_gs_results[0].best_score_\n            best_params = all_gs_results[0].best_params_\n            model_name = estimator_names[0]\n\n            # Iterate through the rest, and see if any are better!\n            for idx, result in enumerate(all_gs_results):\n                if result.best_score_ > best_score:\n                    best_score = result.best_score_\n                    best_params = result.best_params_\n                    if \'model_name\' in best_params:\n                        model_name = best_params[\'model_name\']\n                    else:\n                        model_name = estimator_names[idx]\n\n            # Now that we\'ve got the best model, train it on quite a few more iterations/epochs/trees if applicable\n            cleaned_best_params = {}\n            for k, v in best_params.items():\n                if k in [\'_scorer\']:\n                    continue\n                elif k[:7] == \'model__\':\n                    cleaned_best_params[k[7:]] = v\n                else:\n                    cleaned_best_params[k] = v\n            best_params = cleaned_best_params\n\n            if \'epochs\' in best_params:\n                epochs = self.training_params.get(\'epochs\', 1000)\n                best_params[\'epochs\'] = epochs\n\n            self.training_params = best_params\n\n            trained_final_model = self.fit_single_pipeline(X_df, y, model_name, feature_learning=feature_learning, prediction_interval=False)\n\n            # Don\'t report feature_responses (or nearly anything else) if this is just the feature_learning stage\n            # That saves a considerable amount of time\n            # if feature_learning == False:\n            #     self.print_results(model_name, trained_final_model, X_df, y)\n\n            # If we wanted to do something tricky, here would be the place to do it\n                # Train the final model up on more epochs, or with more trees\n                # Run a two-stage GSCV. First stage figures out activation function, second stage figures out architecture\n\n        return trained_final_model\n\n    def get_relevant_categorical_rows(self, X_df, y, category):\n        mask = X_df[self.categorical_column] == category\n\n        relevant_indices = []\n        relevant_y = []\n        for idx, val in enumerate(mask):\n            if val == True:\n                relevant_indices.append(idx)\n                relevant_y.append(y[idx])\n\n        relevant_X = X_df.iloc[relevant_indices]\n\n        return relevant_X, relevant_y\n\n\n    def train_categorical_ensemble(self, data, categorical_column, default_category=None, min_category_size=5, **kwargs):\n        self.categorical_column = categorical_column\n        self.trained_category_models = {}\n        self.column_descriptions[categorical_column] = \'ignore\'\n        try:\n            self.cols_to_ignore.remove(categorical_column)\n        except:\n            pass\n        self.min_category_size = min_category_size\n\n        self.default_category = default_category\n        if self.default_category is None:\n            self.search_for_default_category = True\n            self.len_largest_category = 0\n        else:\n            self.search_for_default_category = False\n\n        self.set_params_and_defaults(data, **kwargs)\n\n\n        X_df, y = self._clean_data_and_prepare_for_training(data, self.scoring)\n        X_df = X_df.reset_index(drop=True)\n        X_df = utils_categorical_ensembling.clean_categorical_definitions(X_df, categorical_column)\n\n        print(\'Now fitting a single feature transformation pipeline that will be shared by all of our categorical estimators for the sake of space efficiency when saving the model\')\n        if self.feature_learning == True:\n            # For simplicity\'s sake, we are training one feature_learning model on all of the data, across all categories\n            # Deep Learning models love a ton of data, so we\'re giving all of it to the model\n            # This also makes stuff like serializing the model and the transformation pipeline and the saved file size all better\n            # Then, each categorical model will determine which features (if any) are useful for it\'s particular category\n            X_df_transformed = self.fit_feature_learning_and_transformation_pipeline(X_df, kwargs[\'fl_data\'], y)\n        else:\n            # If the user passed in a valid value for model_names (not None, and not a list where the only thing is None)\n            if self.model_names is not None and not (len(self.model_names) == 1 and self.model_names[0] is None):\n                estimator_names = self.model_names\n            else:\n                estimator_names = self._get_estimator_names()\n\n            X_df_transformed = self.fit_transformation_pipeline(X_df, y, estimator_names)\n\n        unique_categories = X_df[categorical_column].unique()\n\n        # Iterate through categories to find:\n        # 1. index positions of that category within X_df (and thus, X_df_transformed, and y)\n        # 2. some sorting (either alphabetical, size of category, or ideally, sorted by magnitude of y value)\n            # 3. size of category would be most efficient. if we have 8 cores and 13 categories, we don\'t want to save the largest category for the last one, even if from an analytics perspective that\'s the one we would want last\n        # 4. create a map from category name to indexes\n        # 5. sort by len(indices)\n        # 6. iterate through that, creating a new mapping from category_name to the relevant data for that category\n            # pull that data from X_df_transformed\n        # 7. map over that list to train a new predictor for each category!\n\n        categories_and_indices = []\n        for category in unique_categories:\n            rel_column = X_df[self.categorical_column]\n            indices = list(np.flatnonzero(X_df[self.categorical_column] == category))\n            categories_and_indices.append([category, indices])\n\n        categories_and_data = []\n        all_small_categories = {\n            \'relevant_transformed_rows\': []\n            , \'relevant_y\': []\n        }\n        for pair in sorted(categories_and_indices, key=lambda x: len(x[1]), reverse=True):\n            category = pair[0]\n            indices = pair[1]\n\n            relevant_transformed_rows = X_df_transformed[indices]\n            relevant_y = [y[idx_val] for idx_val in indices]\n\n            # If this category is larger than our min_category_size filter, train a model for it\n            if len(indices) > self.min_category_size:\n                categories_and_data.append([category, relevant_transformed_rows, relevant_y])\n\n            # Otherwise, add it to our ""all_small_categories"" category, and train a model on all our small categories combined\n            else:\n                # Slightly complicated because we\'re dealing with sparse matrices\n                if isinstance(all_small_categories[\'relevant_transformed_rows\'], list):\n                    all_small_categories[\'relevant_transformed_rows\'] = relevant_transformed_rows\n                else:\n                    all_small_categories[\'relevant_transformed_rows\'] = scipy.sparse.vstack([all_small_categories[\'relevant_transformed_rows\'], relevant_transformed_rows], format=\'csr\')\n                all_small_categories[\'relevant_y\'] += relevant_y\n\n        if len(all_small_categories[\'relevant_y\']) > self.min_category_size:\n            categories_and_data.insert(0, [\'_all_small_categories\', all_small_categories[\'relevant_transformed_rows\'], all_small_categories[\'relevant_y\']])\n\n        def train_one_categorical_model(category, relevant_X, relevant_y):\n            print(\'\\n\\nNow training a new estimator for the category: \' + str(category))\n\n            print(\'Some stats on the y values for this category: \' + str(category))\n            print(pd.Series(relevant_y).describe(include=\'all\'))\n\n\n            try:\n                category_trained_final_model = self.train_ml_estimator(self.model_names, self._scorer, relevant_X, relevant_y)\n            except ValueError as e:\n                if \'BinomialDeviance requires 2 classes\' in str(e) or \'BinomialDeviance requires 2 classes\' in e or \'BinomialDeviance requires 2 classes\':\n                    print(\'Found a category with only one label\')\n                    print(\'category: \' + str(category) + \', label: \' + str(relevant_y[0]))\n                    print(\'We will put in place a weak estimator trained on only this category/single-label, but consider some feature engineering work to combine this with a different category, or remove it altogether and use the default category when getting predictions for this category.\')\n                    # This handles the edge case of having only one label for a given category\n                    # In that case, some models are perfectly fine being 100% correct, while others freak out\n                    # RidgeClassifier seems ok at just picking the same value each time. And using it instead of a custom function means we don\'t need to add in any custom logic for predict_proba or anything\n                    category_trained_final_model = self.train_ml_estimator([\'RidgeClassifier\'], self._scorer, relevant_X, relevant_y)\n                else:\n                    raise\n\n            self.trained_category_models[category] = category_trained_final_model\n\n            try:\n                category_length = len(relevant_X)\n            except TypeError:\n                category_length = relevant_X.shape[0]\n\n            result = {\n                \'trained_category_model\': category_trained_final_model\n                , \'category\': category\n                , \'len_relevant_X\': category_length\n            }\n            return result\n\n\n        if os.environ.get(\'is_test_suite\', False) == \'True\':\n            # If this is the test_suite, do not run things in parallel\n            results = list(map(lambda x: train_one_categorical_model(x[0], x[1], x[2]), categories_and_data))\n        else:\n\n            pool = pathos.multiprocessing.ProcessPool()\n\n            # Since we may have already closed the pool, try to restart it\n            try:\n                pool.restart()\n            except AssertionError as e:\n                pass\n\n            try:\n                results = list(pool.map(lambda x: train_one_categorical_model(x[0], x[1], x[2]), categories_and_data))\n            except RuntimeError:\n                # Deep Learning models require a ton of recursion. I\'ve tried to work around it, but sometimes we just need to brute force the solution here\n                original_recursion_limit = sys.getrecursionlimit()\n                sys.setrecursionlimit(10000)\n                results = list(pool.map(lambda x: train_one_categorical_model(x[0], x[1], x[2]), categories_and_data))\n                sys.setrecursionlimit(original_recursion_limit)\n\n            # Once we have gotten all we need from the pool, close it so it\'s not taking up unnecessary memory\n            pool.close()\n            try:\n                pool.join()\n            except AssertionError:\n                pass\n\n        for result in results:\n            if result[\'trained_category_model\'] is not None:\n                category = result[\'category\']\n                self.trained_category_models[category] = result[\'trained_category_model\']\n                if self.search_for_default_category == True and result[\'len_relevant_X\'] > self.len_largest_category:\n                    self.default_category = category\n                    self.len_largest_category = result[\'len_relevant_X\']\n\n        print(\'Finished training all the category models!\')\n\n        if self.search_for_default_category == True:\n            print(\'By default, auto_ml finds the largest category, and uses that if asked to get predictions for any rows which come from a category that was not included in the training data (i.e., if you launch a new market and ask us to get predictions for it, we will default to using your largest market to get predictions for the market that was not included in the training data\')\n            print(\'To avoid this behavior, you can either choose your own default category (the ""default_category"" parameter to train_categorical_ensemble), or pass in ""_RAISE_ERROR"" as the value for default_category, and we will raise an error when trying to get predictions for a row coming from a category that was not included in the training data.\')\n            print(\'\\n\\nHere is the default category we selected:\')\n            print(self.default_category)\n            if self.default_category == \'_all_small_categories\':\n                print(\'In this case, it is all the categories that did not meet the min_category_size threshold, combined together into their own ""_all_small_categories"" category.\')\n\n        categorical_ensembler = utils_categorical_ensembling.CategoricalEnsembler(self.trained_category_models, self.transformation_pipeline, self.categorical_column, self.default_category)\n        self.trained_pipeline = categorical_ensembler\n\n\n    def _join_and_print_analytics_results(self, df_feature_responses, df_features, sort_field):\n\n        # Join the standard feature_importances/coefficients, with our feature_responses\n        if df_feature_responses is not None:\n            df_results = pd.merge(df_feature_responses, df_features, on=\'Feature Name\')\n\n            # Sort by coefficients or feature importances\n            df_results = df_results[[\'Feature Name\', sort_field, \'Delta\', \'FR_Decrementing\', \'FR_Incrementing\', \'FRD_abs\', \'FRI_abs\', \'FRD_MAD\', \'FRI_MAD\']]\n        else:\n            df_results = df_features\n\n        df_results = df_results.reset_index(drop=True)\n        df_results = df_results.head(n=100)\n        df_results = df_results.sort_values(by=sort_field, ascending=True)\n\n        analytics_file_name = self.analytics_config[\'file_name\']\n\n        print(\'The printed list will only contain at most the top 100 features.\')\n        # print(\'The full analytics results will be saved to a filed called: \' + analytics_file_name + \'\\n\')\n\n        df_results = df_results.head(n=100)\n        print(tabulate(df_results, headers=\'keys\', floatfmt=\'.4f\', tablefmt=\'psql\'))\n        print(\'\\n\')\n        print(\'*******\')\n        print(\'Legend:\')\n        print(\'Importance = Feature Importance\')\n        print(\'     Explanation: A weighted measure of how much of the variance the model is able to explain is due to this column\')\n        print(\'FR_delta = Feature Response Delta Amount\')\n        print(\'     Explanation: Amount this column was incremented or decremented by to calculate the feature reponses\')\n        print(\'FR_Decrementing = Feature Response From Decrementing Values In This Column By One FR_delta\')\n        print(\'     Explanation: Represents how much the predicted output values respond to subtracting one FR_delta amount from every value in this column\')\n        print(\'FR_Incrementing = Feature Response From Incrementing Values In This Column By One FR_delta\')\n        print(\'     Explanation: Represents how much the predicted output values respond to adding one FR_delta amount to every value in this column\')\n        print(\'FRD_MAD = Feature Response From Decrementing- Median Absolute Delta\')\n        print(\'     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if decrementing this feature provokes strong changes that are both positive and negative\')\n        print(\'FRI_MAD = Feature Response From Incrementing- Median Absolute Delta\')\n        print(\'     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if incrementing this feature provokes strong changes that are both positive and negative\')\n        print(\'FRD_abs = Feature Response From Decrementing Avg Absolute Change\')\n        print(\'     Explanation: What is the average absolute change in predicted output values to subtracting one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\')\n        print(\'FRI_abs = Feature Response From Incrementing Avg Absolute Change\')\n        print(\'     Explanation: What is the average absolute change in predicted output values to adding one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\')\n        print(\'*******\\n\')\n\n        df_results.to_csv(analytics_file_name, encoding=\'latin-1\')\n\n\n    def _print_ml_analytics_results_random_forest(self, trained_model_for_analytics):\n        try:\n            final_model_obj = trained_model_for_analytics.named_steps[\'final_model\']\n        except:\n            final_model_obj = trained_model_for_analytics\n\n        print(\'\\n\\nHere are the results from our \' + final_model_obj.model_name)\n        if self.name is not None:\n            print(self.name)\n        print(\'predicting \' + self.output_column)\n\n        trained_feature_names = self._get_trained_feature_names()\n\n        try:\n            trained_feature_importances = final_model_obj.model.feature_importances_\n        except AttributeError as e:\n            try:\n                # There was a version of LightGBM that had this misnamed to miss the ""s"" at the end\n                trained_feature_importances = final_model_obj.model.feature_importance_\n            except AttributeError as e:\n                # There is a version of XGBoost does not have feature_importance_\n                try:\n                    # There was a version of LightGBM that had this misnamed to miss the ""s"" at the end\n                    trained_feature_importances = final_model_obj.model.feature_importance_\n                except AttributeError as e:\n                    # There is a version of XGBoost does not have feature_importance_\n                    imp_vals = final_model_obj.model.get_booster().get_fscore()\n                except AttributeError:\n                    imp_vals = final_model_obj.model.booster().get_fscore()\n                imp_dict = {trained_feature_names[i]:float(imp_vals.get(\'f\'+str(i),0.)) \\\n                            for i in range(len(trained_feature_names))}\n                total = np.array(imp_dict.values()).sum()\n                trained_feature_importances = {k:v/total for k,v in imp_dict.items()}\n\n        feature_infos = zip(trained_feature_names, trained_feature_importances)\n\n        sorted_feature_infos = sorted(feature_infos, key=lambda x: x[1])\n\n        df_results = pd.DataFrame(sorted_feature_infos)\n\n        df_results.columns = [\'Feature Name\', \'Importance\']\n\n        return df_results\n\n\n    def _get_trained_feature_names(self):\n\n        trained_feature_names = self.transformation_pipeline.named_steps[\'dv\'].get_feature_names()\n        return trained_feature_names\n\n\n    def _print_ml_analytics_results_linear_model(self, trained_model_for_analytics):\n        try:\n            final_model_obj = trained_model_for_analytics.named_steps[\'final_model\']\n        except:\n            final_model_obj = trained_model_for_analytics\n        print(\'\\n\\nHere are the results from our \' + final_model_obj.model_name + \' model\')\n\n        trained_feature_names = self._get_trained_feature_names()\n\n        if self.type_of_estimator == \'classifier\':\n            trained_coefficients = final_model_obj.model.coef_[0]\n        else:\n            trained_coefficients = final_model_obj.model.coef_\n\n        feature_summary = []\n        for col_idx, feature_name in enumerate(trained_feature_names):\n            summary_tuple = (feature_name, trained_coefficients[col_idx])\n            feature_summary.append(summary_tuple)\n\n        sorted_feature_summary = sorted(feature_summary, key=lambda x: abs(x[1]))\n\n        df_results = pd.DataFrame(sorted_feature_summary)\n\n        df_results.columns = [\'Feature Name\', \'Coefficients\']\n\n        return df_results\n\n\n    def print_training_summary(self, gs):\n        print(\'The best CV score from our hyperparameter search (by default averaging across k-fold CV) for \' + self.output_column + \' is:\')\n        if self.took_log_of_y:\n            print(\'    Note that this score is calculated using the natural logs of the y values.\')\n        print(gs.best_score_)\n        print(\'The best params were\')\n\n        # Remove \'final_model__model\' from what we print- it\'s redundant with model name, and is difficult to read quickly in a list since it\'s a python object.\n        printing_copy = {}\n        for k, v in gs.best_params_.items():\n            if k == \'model\':\n                if isinstance(v, str):\n                    printing_copy[k] = v\n                else:\n                    printing_copy[k] = utils_models.get_name_from_model(v)\n            elif k == \'_scorer\':\n                pass\n            else:\n                printing_copy[k] = v\n\n        print(printing_copy)\n\n        if self.verbose:\n            # sklearn-deap has an annoying bug where it doesn\'t keep .cv_results_ for the first generation, and the second generation might be empty when running the test suite because we have such a small number per generation.\n            try:\n                print(\'Here are all the hyperparameters that were tried:\')\n                raw_scores = gs.cv_results_\n                df_raw_scores = pd.DataFrame(raw_scores)\n\n                df_raw_scores = df_raw_scores.sort_values(by=\'mean_test_score\', ascending=False)\n                col_name_map = {\n                    \'mean_test_score\': \'mean_score\'\n                    , \'min_test_score\': \'DROPME\'\n                    , \'max_test_score\': \'DROPME\'\n                    , \'nan_test_score?\': \'DROPME\'\n                    , \'index\': \'DROPME\'\n                    , \'param_index\': \'DROPME\'\n                    , \'std_test_score\': \'DROPME\'\n                }\n                new_cols = []\n                for col in df_raw_scores.columns:\n                    if col in col_name_map:\n                        new_cols.append(col_name_map.get(col, col))\n                    else:\n                        new_cols.append(col)\n                df_raw_scores.columns = new_cols\n                try:\n                    df_raw_scores = df_raw_scores.drop(\'DROPME\', axis=1)\n                except:\n                    pass\n\n                cleaned_params = list(df_raw_scores[\'params\'].apply(utils.clean_params))\n                df_params = pd.DataFrame(cleaned_params)\n                df_scores = pd.concat([df_raw_scores.mean_score, df_params], axis=1)\n                df_scores = df_scores.sort_values(by=\'mean_score\', ascending=True)\n                print(\'Score in the following columns always refers to cross-validation score\')\n                try:\n                    print(tabulate(df_scores, headers=\'keys\', floatfmt=\'.4f\', tablefmt=\'psql\', showindex=False))\n                except TypeError:\n                    print(tabulate(df_scores, headers=\'keys\', floatfmt=\'.4f\', tablefmt=\'psql\'))\n\n            except ValueError as e:\n                if os.environ.get(\'is_test_suite\', False) == \'True\':\n                    pass\n                else:\n                    raise(e)\n\n\n    def predict(self, prediction_data):\n        if isinstance(prediction_data, list):\n            prediction_data = pd.DataFrame(prediction_data)\n        prediction_data = prediction_data.copy()\n\n        predicted_vals = self.trained_pipeline.predict(prediction_data)\n        if self.took_log_of_y:\n            for idx, val in predicted_vals:\n                predicted_vals[idx] = math.exp(val)\n\n        return predicted_vals\n\n    def predict_uncertainty(self, prediction_data):\n        prediction_data = prediction_data.copy()\n\n        predicted_vals = self.trained_pipeline.predict_uncertainty(prediction_data)\n\n        return predicted_vals\n\n    def predict_intervals(self, prediction_data, return_type=None):\n\n        prediction_data = prediction_data.copy()\n\n        return self.trained_pipeline.predict_intervals(prediction_data, return_type=return_type)\n\n\n    def predict_proba(self, prediction_data):\n        if isinstance(prediction_data, list):\n            prediction_data = pd.DataFrame(prediction_data)\n        prediction_data = prediction_data.copy()\n\n        return self.trained_pipeline.predict_proba(prediction_data)\n\n\n    def score(self, X_test, y_test, advanced_scoring=True, verbose=2):\n\n        if isinstance(X_test, list):\n            X_test = pd.DataFrame(X_test)\n        y_test = list(y_test)\n\n        X_test, y_test = utils.drop_missing_y_vals(X_test, y_test, self.output_column)\n\n        if self._scorer is not None:\n            if self.type_of_estimator == \'regressor\':\n                return self._scorer.score(self.trained_pipeline, X_test, y_test, self.took_log_of_y, advanced_scoring=advanced_scoring, verbose=verbose, name=self.name)\n\n            elif self.type_of_estimator == \'classifier\':\n                # TODO: can probably refactor accuracy score now that we\'ve turned scoring into it\'s own class\n                if self._scorer == accuracy_score:\n                    predictions = self.trained_pipeline.predict(X_test)\n                    return self._scorer.score(y_test, predictions)\n                elif advanced_scoring:\n                    score, probas = self._scorer.score(self.trained_pipeline, X_test, y_test, advanced_scoring=advanced_scoring)\n                    utils_scoring.advanced_scoring_classifiers(probas, y_test, name=self.name)\n                    return score\n                else:\n                    return self._scorer.score(self.trained_pipeline, X_test, y_test, advanced_scoring=advanced_scoring)\n        else:\n            return self.trained_pipeline.score(X_test, y_test)\n\n\n    def define_uncertain_predictions(self, base_predictions, y):\n        if not (isinstance(base_predictions[0], float) or isinstance(base_predictions[0], int)):\n            base_predictions = [row[0] for row in base_predictions]\n\n        base_predictions = list(base_predictions)\n\n        is_uncertain_predictions = []\n\n        for idx, y_val in enumerate(y):\n\n            base_prediction_for_row = base_predictions[idx]\n            delta = base_prediction_for_row - y_val\n\n            if self.uncertainty_delta_units == \'absolute\':\n                if self.uncertainty_delta_direction == \'both\':\n                    if abs(delta) > self.uncertainty_delta:\n                        is_uncertain_predictions.append(1)\n                    else:\n                        is_uncertain_predictions.append(0)\n\n                else:\n                    # This is now the case of single-directional deltas (we only care if our predictions are higher, not lower, or lower and not higher)\n                    if self.uncertainty_delta > 0:\n                        if delta > self.uncertainty_delta:\n                            is_uncertain_predictions.append(1)\n                        else:\n                            is_uncertain_predictions.append(0)\n                    else:\n                        # This is the case where we have directional deltas, and the uncertainty_delta < 0\n                        if delta < self.uncertainty_delta:\n                            is_uncertain_predictions.append(1)\n                        else:\n                            is_uncertain_predictions.append(0)\n\n            elif self.uncertainty_delta_units == \'percentage\':\n                if self.uncertainty_delta_direction == \'both\':\n                    if abs(delta) / y_val > self.uncertainty_delta:\n                        is_uncertain_predictions.append(1)\n                    else:\n                        is_uncertain_predictions.append(0)\n                else:\n                    # This is now the case of single-directional deltas (we only care if our predictions are higher, not lower, or lower and not higher)\n                    if self.uncertainty_delta > 0:\n                        if delta / y_val > self.uncertainty_delta:\n                            is_uncertain_predictions.append(1)\n                        else:\n                            is_uncertain_predictions.append(0)\n                    else:\n                        # This is the case where we have directional deltas, and the uncertainty_delta < 0\n                        if delta / y_val < self.uncertainty_delta:\n                            is_uncertain_predictions.append(1)\n                        else:\n                            is_uncertain_predictions.append(0)\n\n        return is_uncertain_predictions\n\n\n\n    def score_uncertainty(self, X, y, advanced_scoring=True, verbose=2):\n\n        df_uncertainty_predictions = self.predict_uncertainty(X)\n        is_uncertain_predictions = self.define_uncertain_predictions(df_uncertainty_predictions.base_prediction, y)\n\n        score = utils_scoring.advanced_scoring_classifiers(df_uncertainty_predictions.uncertainty_prediction, is_uncertain_predictions)\n\n        return score\n\n\n    def transform_only(self, X):\n        return self.transformation_pipeline.transform(X)\n\n\n    def save(self, file_name=\'auto_ml_saved_pipeline.dill\', verbose=True):\n\n        make_feature_importances = True\n        try:\n            # CategoricalEnsembler doesn\'t have named_steps, and Ensembler doesn\'t have .model, so we perform some conditional logic here to add feature_importances_ whenever we can\n            feature_names = self.trained_pipeline.named_steps[\'dv\'].get_feature_names()\n        except AttributeError:\n            make_feature_importances = False\n\n        if make_feature_importances == True:\n            importances_dict = {}\n            try:\n                final_model = self.trained_pipeline.named_steps[\'final_model\'].model\n                importances = final_model.feature_importances_\n\n                for idx, name in enumerate(feature_names):\n                    importances_dict[name] = importances[idx]\n            except AttributeError:\n                for name in feature_names:\n                    importances_dict[name] = np.nan\n\n            self.trained_pipeline.feature_importances_ = importances_dict\n\n\n        def save_one_step(pipeline_step, used_deep_learning):\n            try:\n                if pipeline_step.model_name[:12] == \'DeepLearning\':\n                    used_deep_learning = True\n\n                    random_name = str(random.random())\n\n                    keras_file_name = file_name[:-5] + random_name + \'_keras_deep_learning_model.h5\'\n\n                    # Save a reference to this so we can put it back in place later\n                    keras_wrapper = pipeline_step.model\n                    model_name_map[random_name] = keras_wrapper\n\n                    # Save the Keras model (which we have to extract from the sklearn wrapper)\n                    try:\n                        pipeline_step.model.save(keras_file_name)\n                    except AttributeError as e:\n                        # I\'m not entirely clear why, but sometimes we need to access the "".model"" property within a KerasRegressor or KerasClassifier, and sometimes we don\'t\n                        pipeline_step.model.model.save(keras_file_name)\n\n                    # Now that we\'ve saved the keras model, set that spot in the pipeline to our random name, because otherwise we\'re at risk for recursionlimit errors (the model is very recursively deep)\n                    # Using the random_name allows us to find the right model later if we have several (or several thousand) models to put back into place in the pipeline when we save this later\n                    pipeline_step.model = random_name\n\n\n            except AttributeError as e:\n                pass\n\n            return used_deep_learning\n\n\n        used_deep_learning = False\n\n        # This is where we will store all of our Keras models by their name, so we can put them back in place once we\'ve taken them out and saved the rest of the pipeline\n        model_name_map = {}\n        if isinstance(self.trained_pipeline, utils_categorical_ensembling.CategoricalEnsembler):\n            for step in self.trained_pipeline.transformation_pipeline.named_steps:\n                pipeline_step = self.trained_pipeline.transformation_pipeline.named_steps[step]\n\n                used_deep_learning = save_one_step(pipeline_step, used_deep_learning)\n\n            for step in self.trained_pipeline.trained_models:\n                pipeline_step = self.trained_pipeline.trained_models[step]\n\n                used_deep_learning = save_one_step(pipeline_step, used_deep_learning)\n\n        else:\n\n            for step in self.trained_pipeline.named_steps:\n                pipeline_step = self.trained_pipeline.named_steps[step]\n\n                used_deep_learning = save_one_step(pipeline_step, used_deep_learning)\n\n        # Now, whether we had deep learning models in there or not, save the structure of the whole pipeline\n        # We\'ve already removed the deep learning models from it if they existed, so they won\'t be throwing recursion errors here\n        with open(file_name, \'wb\') as open_file_name:\n            dill.dump(self.trained_pipeline, open_file_name)\n\n\n        # If we used deep learning, put the models back in place, so the predictor instance that\'s already loaded in memory will continue to work like the user expects (rather than forcing them to load it back in from disk again)\n        if used_deep_learning == True:\n            if isinstance(self.trained_pipeline, utils_categorical_ensembling.CategoricalEnsembler):\n                for step in self.trained_pipeline.transformation_pipeline.named_steps:\n                    pipeline_step = self.trained_pipeline.transformation_pipeline.named_steps[step]\n\n                    try:\n                        model_name = pipeline_step.model\n                        pipeline_step.model = model_name_map[model_name]\n                    except AttributeError:\n                        pass\n\n                for step in self.trained_pipeline.trained_models:\n                    pipeline_step = self.trained_pipeline.trained_models[step]\n\n                    try:\n                        model_name = pipeline_step.model\n                        if isinstance(model_name, str):\n                            pipeline_step.model = model_name_map[model_name]\n                    except AttributeError:\n                        pass\n\n            else:\n\n                for step in self.trained_pipeline.named_steps:\n                    pipeline_step = self.trained_pipeline.named_steps[step]\n                    try:\n                        if pipeline_step.get(\'model_name\', \'nonsensicallongstring\')[:12] == \'DeepLearning\':\n\n                            model_name = pipeline_step.model\n                            pipeline_step.model = model_name_map[model_name]\n                    except AttributeError as e:\n                        pass\n\n\n\n        if verbose:\n            print(\'\\n\\nWe have saved the trained pipeline to a filed called ""\' + file_name + \'""\')\n            print(\'It is saved in the directory: \')\n            print(os.getcwd())\n            print(\'To use it to get predictions, please follow the following flow (adjusting for your own uses as necessary:\\n\\n\')\n            print(\'`from auto_ml.utils_models import load_ml_model\')\n            print(\'`trained_ml_pipeline = load_ml_model(""\' + file_name + \'"")\')\n            print(\'`trained_ml_pipeline.predict(data)`\\n\\n\')\n\n            if used_deep_learning == True:\n                print(\'While saving the trained_ml_pipeline, we found a number of deep learning models that we saved separately.\')\n                print(\'Make sure to transfer these files to whichever environment you plan to load the trained pipeline in\')\n                print(\'Specifically, we saved \' + str(len(model_name_map.keys())) + \' deep learning models to separate files\')\n\n            print(\'Note that this pickle/dill file can only be loaded in an environment with the same modules installed, and running the same Python version.\')\n            print(\'This version of Python is:\')\n            print(sys.version_info)\n\n            print(\'\\n\\nWhen passing in new data to get predictions on, columns that were not present (or were not found to be useful) in the training data will be silently ignored.\')\n            print(\'It is worthwhile to make sure that you feed in all the most useful data points though, to make sure you can get the highest quality predictions.\')\n\n        return os.path.join(os.getcwd(), file_name)\n\n\n    def _train_ensemble(self, X_train, y_train):\n\n        print(\'We are now training an ensemble of different predictors\')\n        print(\'We will print out analytics info for each one as we train it\')\n        # loop through all the ensemble configs, and train one model per config\n\n        self.trained_final_model.name = \'default_estimator\'\n\n        # Grab the trained_final_model we\'ve already trained, and make that part of our ensemble\n        trained_ensemble_models = [self.trained_final_model]\n        for idx, model_params in enumerate(self.ensemble_config):\n            # FUTURE todo: subset the data here, pass through transformation_pipeline again to transform it\n            trained_model = self.train_ml_estimator([model_params[\'model_name\']], scoring=self.scoring, X_df=X_train, y=y_train)\n\n            default_name = \'{}_{}\'.format(model_params[\'model_name\'], idx)\n            predictor_name = model_params.get(\'model_name\', default_name)\n\n            trained_model.name = predictor_name\n            trained_ensemble_models.append(trained_model)\n\n        ensemble_method = \'average\'\n        if ensemble_method != \'average\' and self.type_of_estimator == \'classifier\':\n            print(\'Because we are taking the minimum prediction for each class, these predicted probabilities are not expected to sum to 1 for each row\')\n            print(\'If you want the predicted probabilities to sum to 1, you should use ensemble_method=""average""\')\n            warnings.warn(\'Predicted probabilities are not expected to add up to 1 if ensemble_method is not ""average""\')\n\n        num_classes = None\n        if self.type_of_estimator == \'classifier\':\n            num_classes = len(set(y_train))\n\n        # create Ensembler\n        ensembler = utils_ensembling.Ensembler(ensemble_predictors=trained_ensemble_models, type_of_estimator=self.type_of_estimator, ensemble_method=ensemble_method, num_classes = num_classes)\n\n        # ensembler will be added to pipeline later back inside main train section\n        self.trained_final_model = ensembler\n'"
auto_ml/utils.py,0,"b'import csv\nimport datetime\nimport numbers\nimport os\nimport pkg_resources\n\nimport numpy as np\nimport pandas as pd\nimport scipy\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils.metaestimators import if_delegate_has_method\nfrom sklearn.utils import column_or_1d\n\nfrom auto_ml._version import __version__ as auto_ml_version\n\n\ndef is_linear_model(model_names):\n    linear_models = set([\'RANSACRegressor\', \'LinearRegression\', \'Ridge\', \'Lasso\', \'ElasticNet\', \'LassoLars\', \'OrthogonalMatchingPursuit\', \'BayesianRidge\', \'ARDRegression\', \'SGDRegressor\', \'PassiveAggressiveRegressor\', \'LogisticRegression\', \'RidgeClassifier\', \'SGDClassifier\', \'Perceptron\', \'PassiveAggressiveClassifier\'])\n\n    if len(linear_models & (set(model_names))) > 0:\n        return True\n    else:\n        return False\n\n\ndef write_gs_param_results_to_file(trained_gs, most_recent_filename):\n\n    timestamp_time = datetime.datetime.now()\n    write_most_recent_gs_result_to_file(trained_gs, most_recent_filename, timestamp_time)\n\n    grid_scores = trained_gs.grid_scores_\n    scorer = trained_gs.scorer_\n    best_score = trained_gs.best_score_\n\n    file_name = \'pipeline_grid_search_results.csv\'\n    write_header = False\n    if not os.path.isfile(file_name):\n        write_header = True\n\n    with open(file_name, \'a\') as results_file:\n        writer = csv.writer(results_file, dialect=\'excel\')\n        if write_header:\n            writer.writerow([\'timestamp\', \'scorer\', \'best_score\', \'all_grid_scores\'])\n        writer.writerow([timestamp_time, scorer, best_score, grid_scores])\n\n\ndef write_most_recent_gs_result_to_file(trained_gs, most_recent_filename, timestamp):\n\n    timestamp_time = timestamp\n    grid_scores = trained_gs.grid_scores_\n    scorer = trained_gs.scorer_\n    best_score = trained_gs.best_score_\n\n    file_name = most_recent_filename\n\n    write_header = False\n    make_header = False\n    if not os.path.isfile(most_recent_filename):\n        header_row = [\'timestamp\', \'scorer\', \'best_score\', \'cv_mean\', \'cv_all\']\n        write_header = True\n        make_header = True\n\n    rows_to_write = []\n\n    for score in grid_scores:\n\n        row = [timestamp_time, scorer, best_score, score[1], score[2]]\n\n        for k, v in score[0].items():\n            if make_header:\n                header_row.append(k)\n            row.append(v)\n        rows_to_write.append(row)\n        make_header = False\n\n\n    with open(file_name, \'a\') as results_file:\n        writer = csv.writer(results_file, dialect=\'excel\')\n        if write_header:\n            writer.writerow(header_row)\n        for row in rows_to_write:\n            writer.writerow(row)\n\n\ndef safely_drop_columns(df, cols_to_drop):\n    safe_cols_to_drop = []\n    for col in cols_to_drop:\n        if col in df.columns:\n            safe_cols_to_drop.append(col)\n\n    df.drop(safe_cols_to_drop, axis=1, inplace=True)\n    return df\n\n\ndef drop_duplicate_columns(df):\n    count_cols_to_drop = 0\n    cols = list(df.columns)\n    for idx, item in enumerate(df.columns):\n        if item in df.columns[:idx]:\n            print(\'#####################################################\')\n            print(\'We found a duplicate column, and will be removing it\')\n            print(\'If you intended to send in two different pieces of information, please make sure they have different column names\')\n            print(\'Here is the duplicate column:\')\n            print(item)\n            print(\'#####################################################\')\n            cols[idx] = \'DROPME\'\n            count_cols_to_drop += 1\n\n    if count_cols_to_drop > 0:\n        df.columns = cols\n\n        df.drop(\'DROPME\', axis=1, inplace=True)\n    return df\n\n\ndef get_boston_dataset():\n    boston = load_boston()\n    df_boston = pd.DataFrame(boston.data)\n    df_boston.columns = boston.feature_names\n    df_boston[\'MEDV\'] = boston[\'target\']\n    df_boston_train, df_boston_test = train_test_split(df_boston, test_size=0.2, random_state=42)\n    return df_boston_train, df_boston_test\n\nbad_vals_as_strings = set([str(float(\'nan\')), str(float(\'inf\')), str(float(\'-inf\')), \'None\', \'none\', \'NaN\', \'NAN\', \'nan\', \'NULL\', \'null\', \'\', \'inf\', \'-inf\'])\n\n\ndef delete_rows_csr(mat, indices):\n    """"""\n    Remove the rows denoted by ``indices`` form the CSR sparse matrix ``mat``.\n    """"""\n    if not isinstance(mat, scipy.sparse.csr_matrix):\n        raise ValueError(""works only for CSR format -- use .tocsr() first"")\n    indices = list(indices)\n    mask = np.ones(mat.shape[0], dtype=bool)\n    mask[indices] = False\n    return mat[mask]\n\n\ndef drop_missing_y_vals(df, y, output_column=None):\n\n    y = list(y)\n    indices_to_drop = []\n    indices_to_keep = []\n    for idx, val in enumerate(y):\n        if not isinstance(val, str):\n            if isinstance(val, numbers.Number) or val is None or isinstance(val, np.generic):\n                val = str(val)\n            else:\n                val = val.encode(\'utf-8\').decode(\'utf-8\')\n\n        if val in bad_vals_as_strings:\n            indices_to_drop.append(idx)\n\n    if len(indices_to_drop) > 0:\n        set_of_indices_to_drop = set(indices_to_drop)\n\n        print(\'We encountered a number of missing values for this output column\')\n        if output_column is not None:\n            print(output_column)\n        print(\'And here is the number of missing (nan, None, etc.) values for this column:\')\n        print(len(indices_to_drop))\n        print(\'Here are some example missing values\')\n        for idx, df_idx in enumerate(indices_to_drop):\n            if idx >= 5:\n                break\n            print(y[df_idx])\n        print(\'We will remove these values, and continue with training on the cleaned dataset\')\n\n        support_mask = [True if idx not in set_of_indices_to_drop else False for idx in range(df.shape[0]) ]\n        if isinstance(df, pd.DataFrame):\n            df.drop(df.index[indices_to_drop], axis=0, inplace=True)\n            # df = df.loc[support_mask,]\n        elif scipy.sparse.issparse(df):\n            df = delete_rows_csr(df, indices_to_drop)\n        elif isinstance(df, np.ndarray):\n            df = np.delete(df, indices_to_drop, axis=0)\n        y = [val for idx, val in enumerate(y) if idx not in set_of_indices_to_drop]\n\n\n    return df, y\n\n\nclass CustomLabelEncoder():\n\n    def __init__(self):\n        self.label_map = {}\n\n\n    def fit(self, list_of_labels):\n        if not isinstance(list_of_labels, pd.Series):\n            list_of_labels = pd.Series(list_of_labels)\n        unique_labels = list_of_labels.unique()\n        try:\n            unique_labels = sorted(unique_labels)\n        except TypeError:\n            unique_labels = unique_labels\n\n        for idx, val in enumerate(unique_labels):\n            self.label_map[val] = idx\n        return self\n\n\n    def transform(self, in_vals):\n        return_vals = []\n        for val in in_vals:\n            if not isinstance(val, str):\n                if isinstance(val, float) or isinstance(val, int) or val is None or isinstance(val, np.generic):\n                    val = str(val)\n                else:\n                    val = val.encode(\'utf-8\').decode(\'utf-8\')\n\n            if val not in self.label_map:\n                self.label_map[val] = len(self.label_map.keys())\n            return_vals.append(self.label_map[val])\n\n        if len(in_vals) == 1:\n            return return_vals[0]\n        else:\n            return return_vals\n\n\nclass ExtendedLabelEncoder(LabelEncoder):\n\n    def __init__(self):\n        super(self.__class__, self).__init__()\n\n    def transform(self, y):\n        y = column_or_1d(y, warn=True)\n\n        classes = np.unique(y)\n        if len(np.intersect1d(classes, self.classes_)) < len(classes):\n            diff = np.setdiff1d(classes, self.classes_)\n            self.classes_ = np.hstack((self.classes_, diff))\n        return np.searchsorted(self.classes_, y)[0]\n\n\ndef get_versions():\n\n    libraries_to_check = [\'dill\', \'h5py\', \'keras\', \'lightgbm\', \'numpy\', \'pandas\', \'pathos\', \'python\', \'scikit-learn\', \'scipy\', \'sklearn-deap2\', \'tabulate\', \'tensorflow\', \'xgboost\']\n\n    versions = {\n        \'auto_ml\': auto_ml_version\n    }\n\n    for lib in libraries_to_check:\n        try:\n            versions[lib] = pkg_resources.get_distribution(lib).version\n        except:\n            pass\n\n    return versions\n\n\nclass ExtendedPipeline(Pipeline):\n\n    def __init__(self, steps, keep_cat_features=False, name=None, training_features=None):\n        super(self.__class__, self).__init__(steps)\n        self.keep_cat_features = keep_cat_features\n        self.__versions__ = get_versions()\n        self.name = name\n        self.feature_importances_ = None\n        self.training_features = training_features\n\n\n    @if_delegate_has_method(delegate=\'_final_estimator\')\n    def predict_uncertainty(self, X):\n        Xt = X\n        for name, transform in self.steps[:-1]:\n            if transform is not None:\n                Xt = transform.transform(Xt)\n        return self.steps[-1][-1].predict_uncertainty(Xt)\n\n\n    @if_delegate_has_method(delegate=\'_final_estimator\')\n    def score_uncertainty(self, X):\n        Xt = X\n        for name, transform in self.steps[:-1]:\n            if transform is not None:\n                Xt = transform.transform(Xt)\n        return self.steps[-1][-1].score_uncertainty(Xt)\n\n\n    @if_delegate_has_method(delegate=\'_final_estimator\')\n    def transform_only(self, X):\n        Xt = X\n        for name, transform in self.steps[:-1]:\n            if transform is not None:\n                Xt = transform.transform(Xt)\n        return self.steps[-1][-1].transform_only(Xt)\n\n\n    @if_delegate_has_method(delegate=\'_final_estimator\')\n    def predict_intervals(self, X, return_type=None):\n        Xt = X\n        for name, transform in self.steps[:-1]:\n            if transform is not None:\n                Xt = transform.transform(Xt)\n\n        return self.steps[-1][-1].predict_intervals(Xt, return_type=return_type)\n\n\ndef clean_params(params):\n    cleaned_params = {}\n    for k, v in params.items():\n\n        if k[:7] == \'model__\':\n            cleaned_params[k[7:]] = v\n\n    return cleaned_params\n'"
auto_ml/utils_categorical_ensembling.py,0,"b'import pandas as pd\n\nclass CategoricalEnsembler(object):\n\n    def __init__(self, trained_models, transformation_pipeline, categorical_column, default_category):\n        self.trained_models = trained_models\n        self.categorical_column = categorical_column\n        self.transformation_pipeline = transformation_pipeline\n        self.default_category = default_category\n        self.is_categorical_ensembler = True\n\n\n    def get(self, prop_name, default=None):\n        try:\n            return getattr(self, prop_name)\n        except AttributeError:\n            return default\n\n\n    def predict(self, data):\n        # For now, we are assuming that data is a list of dictionaries, so if we have a single dict, put it in a list\n        if isinstance(data, dict):\n            data = [data]\n\n        if isinstance(data, pd.DataFrame):\n            data = data.to_dict(\'records\')\n\n        predictions = []\n        for row in data:\n            category = row[self.categorical_column]\n            if str(category) == \'nan\':\n                category = \'nan\'\n            try:\n                model = self.trained_models[category]\n            except KeyError as e:\n                if self.default_category == \'_RAISE_ERROR\':\n                    raise(e)\n                model = self.trained_models[self.default_category]\n\n            transformed_row = self.transformation_pipeline.transform(row)\n            prediction = model.predict(transformed_row)\n            predictions.append(prediction)\n\n        if len(predictions) == 1:\n            return predictions[0]\n        else:\n            return predictions\n\n    def predict_proba(self, data):\n        # For now, we are assuming that data is a list of dictionaries, so if we have a single dict, put it in a list\n        if isinstance(data, dict):\n            data = [data]\n\n        if isinstance(data, pd.DataFrame):\n            data = data.to_dict(\'records\')\n\n        predictions = []\n        for row in data:\n            category = row[self.categorical_column]\n            if str(category) == \'nan\':\n                category = \'nan\'\n\n            try:\n                model = self.trained_models[category]\n            except KeyError as e:\n                if self.default_category == \'_RAISE_ERROR\':\n                    raise(e)\n                model = self.trained_models[self.default_category]\n\n            transformed_row = self.transformation_pipeline.transform(row)\n            prediction = model.predict_proba(transformed_row)\n            predictions.append(prediction)\n\n        if len(predictions) == 1:\n            return predictions[0]\n        else:\n            return predictions\n\n\n# Remove nans from our categorical ensemble column\ndef clean_categorical_definitions(df, categorical_column):\n    sum_of_nan_values = df[categorical_column].isnull().sum().sum()\n    if sum_of_nan_values > 0:\n        print(\'Found \' + str(sum_of_nan_values) + \' nan values in the categorical_column.\')\n        print(\'We will default to making these values a string ""nan"" instead, since that can be used as a key\')\n        print(\'If this is not the behavior you want, consider changing these categorical_column values yourself\')\n\n        df[categorical_column].fillna(\'nan\', inplace=True)\n\n    return df\n'"
auto_ml/utils_data_cleaning.py,0,"b'import datetime\nimport os\n\nimport dateutil\nimport numpy as np\nimport pandas as pd\nfrom pandas import __version__ as pandas_version\nimport pathos\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport warnings\n\n# The easiest way to check against a bunch of different bad values is to convert whatever val we have into a string, then check it against a set containing the string representation of a bunch of bad values\nbad_vals_as_strings = set([str(float(\'nan\')), str(float(\'inf\')), str(float(\'-inf\')), \'None\', \'none\', \'NaN\', \'nan\', \'NULL\', \'null\', \'\', \'inf\', \'-inf\'])\n\n# clean_val will try to turn a value into a float.\n# If it fails, it will attempt to strip commas and then attempt to turn it into a float again\n# Additionally, it will check to make sure the value is not in a set of bad vals (nan, None, inf, etc.)\n# This function will either return a clean value, or raise an error if we cannot turn the value into a float or the value is a bad val\ndef clean_val(val):\n    if str(val) in bad_vals_as_strings:\n        raise(ValueError(\'clean_val failed\'))\n    else:\n        try:\n            float_val = float(val)\n        except ValueError:\n            # This will throw a ValueError if it fails\n            # remove any commas in the string, and try to turn into a float again\n            try:\n                cleaned_string = val.replace(\',\', \'\')\n                float_val = float(cleaned_string)\n            except TypeError:\n                return None\n        return float_val\n\n# Same as above, except this version returns float(\'nan\') when it fails\n# This plays more nicely with df.apply, and assumes we will be handling nans appropriately when doing DataFrameVectorizer later.\ndef clean_val_nan_version(key, val, replacement_val=np.nan):\n    try:\n        str_val = str(val)\n    except UnicodeEncodeError as e:\n        str_val = val.encode(\'ascii\', \'ignore\').decode(\'ascii\')\n        print(\'Here is the value that causes the UnicodeEncodeError to be thrown:\')\n        print(val)\n        print(\'Here is the feature name:\')\n        print(key)\n        raise(e)\n\n    if str_val in bad_vals_as_strings:\n        return replacement_val\n    else:\n        try:\n            float_val = float(val)\n        except ValueError:\n            # remove any commas in the string, and try to turn into a float again\n            try:\n                cleaned_string = val.replace(\',\', \'\')\n            except TypeError:\n                print(\'*************************************\')\n                print(\'We expected this value to be numeric, but were unable to convert it to a float:\')\n                print(val)\n                print(\'Here is the feature name:\')\n                print(key)\n                print(\'*************************************\')\n                return replacement_val\n            try:\n                float_val = float(cleaned_string)\n            except:\n                return replacement_val\n        except TypeError:\n            # This is what happens if you feed in a datetime object to float\n            print(\'*************************************\')\n            print(\'We expected this value to be numeric, but were unable to convert it to a float:\')\n            print(val)\n            print(\'Here is the feature name:\')\n            print(key)\n            print(\'*************************************\')\n            return replacement_val\n\n        return float_val\n\n\n\nclass BasicDataCleaning(BaseEstimator, TransformerMixin):\n\n\n    def __init__(self, column_descriptions=None):\n        self.column_descriptions = column_descriptions\n        self.transformed_column_descriptions = column_descriptions.copy()\n        self.text_col_indicators = set([\'text\', \'nlp\'])\n        self.numeric_col_types = [\'int8\', \'int16\', \'int32\', \'int64\', \'float16\', \'float32\', \'float64\']\n\n\n        self.text_columns = {}\n        for key, val in self.column_descriptions.items():\n            if val in self.text_col_indicators:\n                self.text_columns[key] = TfidfVectorizer(\n                    # If we have any documents that cannot be decoded properly, just ignore them and keep going as planned with everything else\n                    decode_error=\'ignore\'\n                    # Try to strip accents from characters. Using unicode is slightly slower but more comprehensive than \'ascii\'\n                    , strip_accents=\'unicode\'\n                    # Can also choose \'character\', which will likely increase accuracy, at the cost of much more space, generally\n                    , analyzer=\'word\'\n                    # Remove commonly found english words (\'it\', \'a\', \'the\') which do not typically contain much signal\n                    , stop_words=\'english\'\n                    # Convert all characters to lowercase\n                    , lowercase=True\n                    # Only consider words that appear in fewer than max_df percent of all documents\n                    # In this case, ignore all words that appear in 90% of all documents\n                    , max_df=0.9\n                    # Consider only the most frequently occurring 3000 words, after taking into account all the other filtering going on\n                    , max_features=3000\n                )\n\n    def get(self, prop_name, default=None):\n        try:\n            return getattr(self, prop_name)\n        except AttributeError:\n            return default\n\n    def fit(self, X_df, y=None):\n        print(\'Running basic data cleaning\')\n\n        self.vals_to_drop = set([\'ignore\', \'output\', \'regressor\', \'classifier\'])\n\n        # See if we should fit TfidfVectorizer or not\n        for key in X_df.columns:\n\n            if X_df[key].dtype == \'object\' and self.column_descriptions.get(key, False) not in [\'categorical\', \'ignore\', \'nlp\']:\n\n                # First, make sure that the values in this column are not just ints, or float(\'nan\')\n                vals = X_df[key].sample(n=10)\n                is_categorical = False\n                for val in vals:\n                    try:\n                        if val is not None:\n                            float(val)\n                    except Exception as e:\n                        print(e)\n                        is_categorical = True\n\n                if is_categorical:\n                    print(\'\\n\')\n                    print(\'Encountered a column that is not marked as categorical, but is an ""object"" pandas type, which typically indicates a categorical column.\')\n                    print(\'The name of this columns is: ""{}""\'.format(key))\n                    print(\'Some example features in this column are: {}\'.format(list(X_df[key].sample(n=5))))\n                    print(\'If this is a categorical column, please mark it as `{}: ""categorical""` as part of your column_descriptions\'.format(key))\n                    print(\'If this is not a categorical column, please consider converting its dtype before passing data into auto_ml\')\n                    print(\'\\n\')\n                    warnings.warn(\'Consider marking the ""{}"" column as categorical\'.format(key))\n\n            if self.transformed_column_descriptions.get(key) is None:\n                self.transformed_column_descriptions[key] = \'continuous\'\n\n            if key in self.text_columns:\n                X_df[key].fillna(\'nan\', inplace=True)\n                if pandas_version < \'0.20.0\':\n                    text_col = X_df[key].astype(str, raise_on_error=False)\n                else:\n                    text_col = X_df[key].astype(str, errors=\'ignore\')\n                self.text_columns[key].fit(text_col)\n\n                col_names = self.text_columns[key].get_feature_names()\n\n                # Make weird characters play nice, or just ignore them :)\n                for idx, word in enumerate(col_names):\n                    try:\n                        col_names[idx] = str(word)\n                    except:\n                        col_names[idx] = \'non_ascii_word_\' + str(idx)\n\n                col_names = [\'nlp_\' + key + \'_\' + str(word) for word in col_names]\n\n                self.text_columns[key].cleaned_feature_names = col_names\n\n        return self\n\n    def transform(self, X, y=None):\n\n        ignore_none_fields = False\n        if self.get(\'transformed_column_descriptions\', None) is not None:\n            ignore_none_fields = True\n        column_descriptions = self.get(\'transformed_column_descriptions\', self.column_descriptions)\n\n\n        # Convert input to DataFrame if we were given a list of dictionaries\n        if isinstance(X, list):\n            X = pd.DataFrame(X)\n        X = X.copy()\n\n\n        # All of these are values we will not want to keep for training this particular estimator.\n        # Note that we have already split out the output column and saved it into it\'s own variable\n\n\n        if isinstance(X, dict):\n\n            dict_copy = {}\n\n            for key, val in X.items():\n                col_desc = column_descriptions.get(key, None)\n\n                if col_desc is None:\n                    continue\n                elif col_desc in (None, \'continuous\', \'numerical\', \'float\', \'int\'):\n                    dict_copy[key] = clean_val_nan_version(key, val, replacement_val=0)\n                elif col_desc == \'date\':\n                    date_feature_dict = add_date_features_dict(X, key)\n                    dict_copy.update(date_feature_dict)\n                elif col_desc == \'categorical\':\n                    dict_copy[key] = val\n                elif key in self.text_columns:\n\n                    col_names = self.text_columns[key].cleaned_feature_names\n\n                    try:\n                        text_val = str(X[key])\n                    except UnicodeEncodeError:\n                        text_val = X[key].encode(\'ascii\', \'ignore\').decode(\'ascii\')\n\n                    # the transform function expects a list\n                    text_val = [text_val]\n\n                    nlp_matrix = self.text_columns[key].transform(text_val)\n\n                    # From here, it\'s all about transforming the output from the tf-idf transform into a dictionary\n                    # Borrowed from: http://stackoverflow.com/a/40696119/3823857\n                    # it outputs a sparse csr matrics\n                    # first, we transform to coo\n                    nlp_matrix = nlp_matrix.tocoo()\n                    # Then, we grab the relevant column names\n                    relevant_col_names = []\n                    for col_idx in nlp_matrix.col:\n                        relevant_col_names.append(col_names[col_idx])\n\n                    # Then we zip together the relevant columns and the sparse data into a dictionary\n                    relevant_nlp_cols = {k:v for k,v in zip(relevant_col_names, nlp_matrix.data)}\n\n                    dict_copy.update(relevant_nlp_cols)\n\n                else:\n                    pass\n            return dict_copy\n\n        else:\n            X.reset_index(drop=True, inplace=True)\n\n            # Run data cleaning only for columns that are not already pandas numeric dtypes\n            cols_to_clean = []\n            dtypes = X.dtypes\n            for idx, col in enumerate(X.columns):\n                if dtypes[idx] not in self.numeric_col_types:\n                    cols_to_clean.append(col)\n\n            if len(cols_to_clean) > 0:\n\n                df_to_clean = X[cols_to_clean]\n                X.drop(cols_to_clean, axis=1, inplace=True)\n\n\n\n                if df_to_clean.shape[0] > 100000 or os.environ.get(\'is_test_suite\', 0) == \'True\':\n                    results = list(map(lambda col: self.process_one_column(col_vals=df_to_clean[col], col_name=col), df_to_clean.columns))\n                else:\n                    pool = pathos.multiprocessing.ProcessPool()\n                    try:\n                        pool.restart()\n                    except AssertionError as e:\n                        pass\n\n                    results = list(pool.map(lambda col: self.process_one_column(col_vals=df_to_clean[col], col_name=col), df_to_clean.columns))\n                    pool.close()\n                    try:\n                        pool.join()\n                    except AssertionError:\n                        pass\n\n\n                result = {}\n                for val in results:\n                    result.update(val)\n                    del val\n                df_result = pd.DataFrame(result)\n                X[df_result.columns] = df_result\n\n            return X\n\n\n    def process_one_column(self, col_vals, col_name):\n        ignore_none_fields = False\n        if self.get(\'transformed_column_descriptions\', None) is not None:\n            ignore_none_fields = True\n        column_descriptions = self.get(\'transformed_column_descriptions\', self.column_descriptions)\n\n        col_desc = column_descriptions.get(col_name)\n\n\n        # This is what we do to columns that were not present at fitting time.\n        # All columns that were present at fitting time that had no entry in column_descriptions were filled in with \'continuous\'\n        if col_desc is None:\n            result = {}\n\n        elif col_desc == \'categorical\':\n            # We will handle categorical data later, one-hot-encoding it inside DataFrameVectorizer (or LabelEncoding it for lgbm)\n            result = {\n                col_name: col_vals\n            }\n\n        elif col_desc in (None, \'continuous\', \'numerical\', \'float\', \'int\'):\n            # For all of our numerical columns, try to turn all of these values into floats\n            # This function handles commas inside strings that represent numbers, and returns nan if we cannot turn this value into a float. nans are ignored in DataFrameVectorizer\n            try:\n                col_vals = col_vals.apply(lambda x: clean_val_nan_version(col_name, x, replacement_val=0))\n                result = {\n                    col_name: col_vals\n                }\n            except TypeError as e:\n                raise(e)\n            except UnicodeEncodeError as e:\n                print(\'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\')\n                print(\'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\')\n                print(\'We have found a column that is not marked as a categorical column that has unicode values in it\')\n                print(\'Here is the column name:\')\n                print(col_name)\n                print(\'The actual value that caused the issue is logged right above the exclamation points\')\n                print(\'Please either mark this column as categorical, or clean up the values in this column\')\n                print(\'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\')\n                print(\'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\')\n\n        elif col_desc == \'date\':\n            result = add_date_features_df(col_vals, col_name)\n\n        elif col_name in self.text_columns:\n\n            col_names = self.text_columns[col_name].cleaned_feature_names\n\n            col_vals.fillna(\'nan\', inplace=True)\n            if pandas_version < \'0.20.0\':\n                nlp_matrix = self.text_columns[col_name].transform(col_vals.astype(str, raise_on_error=False))\n            else:\n                nlp_matrix = self.text_columns[col_name].transform(col_vals.astype(str, errors=\'ignore\'))\n\n            nlp_matrix = nlp_matrix.toarray()\n\n            text_df = pd.DataFrame(nlp_matrix)\n            text_df.columns = col_names\n\n            result = {}\n            for col_vals in text_df.columns:\n                result[col_vals] = text_df[col_vals].astype(int)\n\n        elif col_desc in self.vals_to_drop:\n            result = {}\n        else:\n            # If we have gotten here, the value is not any that we recognize\n            # This is most likely a typo that the user would want to be informed of, or a case while we\'re developing on auto_ml itself.\n            # In either case, it\'s useful to log it.\n            print(\'When transforming the data, we have encountered a value in column_descriptions that is not currently supported. The column has been dropped to allow the rest of the pipeline to run. Here\\\'s the name of the column:\' )\n            print(col_name)\n            print(\'And here is the value for this column passed into column_descriptions:\')\n            print(col_desc)\n            warnings.warn(\'UnknownValueInColumnDescriptions: Please make sure all the values you pass into column_descriptions are valid.\')\n            result = {}\n\n\n        return result\n\n\n\ndef minutes_into_day_parts(minutes_into_day):\n    if minutes_into_day < 6 * 60:\n        return \'late_night\'\n    elif minutes_into_day < 10 * 60:\n        return \'morning\'\n    elif minutes_into_day < 11.5 * 60:\n        return \'mid_morning\'\n    elif minutes_into_day < 14 * 60:\n        return \'lunchtime\'\n    elif minutes_into_day < 18 * 60:\n        return \'afternoon\'\n    elif minutes_into_day < 20.5 * 60:\n        return \'dinnertime\'\n    elif minutes_into_day < 23.5 * 60:\n        return \'early_night\'\n    else:\n        return \'late_night\'\n\n# Note: assumes that the column is already formatted as a pandas date type\ndef add_date_features_df(col_data, date_col):\n    # Pandas nicely tries to prevent you from doing stupid things, like setting values on a copy of a df, not your real one\n    # However, it\'s a bit overzealous in this case, so we\'ll side-step a bunch of warnings by setting is_copy to false here\n\n    result = {}\n\n    col_data = pd.to_datetime(col_data)\n\n    if pandas_version < \'0.20.0\':\n        result[date_col + \'_day_of_week\'] = col_data.apply(lambda x: x.weekday()).astype(int, raise_on_error=False)\n    else:\n        result[date_col + \'_day_of_week\'] = col_data.apply(lambda x: x.weekday()).astype(int, errors=\'ignore\')\n\n    try:\n        if pandas_version < \'0.20.0\':\n            result[date_col + \'_hour\'] = col_data.apply(lambda x: x.hour).astype(int, raise_on_error=False)\n        else:\n            result[date_col + \'_hour\'] = col_data.apply(lambda x: x.hour).astype(int, errors=\'ignore\')\n\n\n        result[date_col + \'_minutes_into_day\'] = col_data.apply(lambda x: x.hour * 60 + x.minute)\n\n        result[date_col + \'_hour\'] = result[date_col + \'_hour\'].fillna(0)\n        result[date_col + \'_minutes_into_day\'] = result[date_col + \'_minutes_into_day\'].fillna(0)\n    except AttributeError:\n        pass\n\n    result[date_col + \'_is_weekend\'] = col_data.apply(lambda x: x.weekday() in (5,6))\n    result[date_col + \'_day_part\'] = result[date_col + \'_minutes_into_day\'].apply(minutes_into_day_parts)\n\n    result[date_col + \'_day_of_week\'] = result[date_col + \'_day_of_week\'].fillna(0)\n    result[date_col + \'_is_weekend\'] = result[date_col + \'_is_weekend\'].fillna(0)\n    result[date_col + \'_day_part\'] = result[date_col + \'_day_part\'].fillna(0)\n    return result\n\n# Same logic as above, except implemented for a single dictionary, which is much faster at prediction time when getting just a single prediction\ndef add_date_features_dict(row, date_col):\n\n    date_feature_dict = {}\n\n    # Handle cases where the val for the date_col is None\n    try:\n        date_val = row[date_col]\n        if date_val == None:\n            return date_feature_dict\n        if not isinstance(date_val, (datetime.datetime, datetime.date)):\n            date_val = dateutil.parser.parse(date_val)\n    except:\n        return date_feature_dict\n\n    # Make a copy of all the engineered features from the date, without modifying the original object at all\n    # This way the same original object can be passed into a number of different trained auto_ml predictors\n\n\n    date_feature_dict[date_col + \'_day_of_week\'] = date_val.weekday()\n    # nesting this inside a try/except block because the date might be a datetime.date, not a datetime.datetime\n    try:\n        date_feature_dict[date_col + \'_hour\'] = date_val.hour\n\n        date_feature_dict[date_col + \'_minutes_into_day\'] = date_val.hour * 60 + date_val.minute\n    except AttributeError:\n        pass\n\n    date_feature_dict[date_col + \'_is_weekend\'] = date_val.weekday() in (5,6)\n\n    return date_feature_dict\n\n\n'"
auto_ml/utils_ensembling.py,0,"b""import os\n\nimport numpy as np\nimport pandas as pd\nimport pathos\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\n\n\nclass Ensembler(BaseEstimator, TransformerMixin):\n\n\n    def __init__(self, ensemble_predictors, type_of_estimator, ensemble_method='average', num_classes=None):\n        self.ensemble_predictors = ensemble_predictors\n        self.type_of_estimator = type_of_estimator\n        self.ensemble_method = ensemble_method\n        self.num_classes = num_classes\n\n\n    # ################################\n    # Get a dataframe that is all the predictions from all the sub-models\n    # ################################\n    # Note that we will get these predictions in parallel (relatively quick)\n\n    def get_all_predictions(self, X):\n\n        def get_predictions_for_one_estimator(estimator, X):\n            estimator_name = estimator.name\n\n            if self.type_of_estimator == 'regressor':\n                predictions = estimator.predict(X)\n            else:\n                # For classifiers\n                predictions = list(estimator.predict_proba(X))\n            return_obj = {estimator_name: predictions}\n            return return_obj\n\n\n        # Don't bother parallelizing if this is a single dictionary\n        if X.shape[0] == 1:\n            predictions_from_all_estimators = map(lambda predictor: get_predictions_for_one_estimator(predictor, X), self.ensemble_predictors)\n\n        else:\n\n            # Pathos doesn't like datasets beyond a certain size. So fall back on single, non-parallel predictions instead.\n            # try:\n            if os.environ.get('is_test_suite', False) == 'True':\n                predictions_from_all_estimators = map(lambda predictor: get_predictions_for_one_estimator(predictor, X), self.ensemble_predictors)\n\n            else:\n                # Open a new multiprocessing pool\n                pool = pathos.multiprocessing.ProcessPool()\n\n                # Since we may have already closed the pool, try to restart it\n                try:\n                    pool.restart()\n                except AssertionError as e:\n                    pass\n                predictions_from_all_estimators = pool.map(lambda predictor: get_predictions_for_one_estimator(predictor, X), self.ensemble_predictors)\n\n                # Once we have gotten all we need from the pool, close it so it's not taking up unnecessary memory\n                pool.close()\n                try:\n                    pool.join()\n                except AssertionError:\n                    pass\n\n        predictions_from_all_estimators = list(predictions_from_all_estimators)\n\n        results = {}\n        for result_dict in predictions_from_all_estimators:\n            results.update(result_dict)\n\n        # if this is a single row we are getting predictions from, just return a dictionary with single values for all the predictions\n        if X.shape[0] == 1:\n            return results\n        else:\n            predictions_df = pd.DataFrame.from_dict(results, orient='columns')\n\n            return predictions_df\n\n\n\n    def fit(self, X, y):\n        return self\n\n\n    # ################################\n    # Public API to get a single prediction from each row, where that single prediction is somehow an ensemble of all our trained subpredictors\n    # ################################\n\n    def predict(self, X):\n\n        predictions = self.get_all_predictions(X)\n\n        # If this is just a single dictionary we're getting predictions from:\n        if X.shape[0] == 1:\n            # predictions is just a dictionary where all the values are the predicted values from one of our subpredictors. we'll want that as a list\n            predicted_vals = list(predictions.values())\n            if self.ensemble_method == 'median':\n                return np.median(predicted_vals)\n            elif self.ensemble_method == 'average' or self.ensemble_method == 'mean' or self.ensemble_method == 'avg':\n                return np.average(predicted_vals)\n            elif self.ensemble_method == 'max':\n                return np.max(predicted_vals)\n            elif self.ensemble_method == 'min':\n                return np.min(predicted_vals)\n\n        else:\n\n            if self.ensemble_method == 'median':\n                return predictions.apply(np.median, axis=1).values\n            elif self.ensemble_method == 'average' or self.ensemble_method == 'mean' or self.ensemble_method == 'avg':\n                return predictions.apply(np.average, axis=1).values\n            elif self.ensemble_method == 'max':\n                return predictions.apply(np.max, axis=1).values\n            elif self.ensemble_method == 'min':\n                return predictions.apply(np.min, axis=1).values\n\n\n\n    def get_predictions_by_class(self, predictions):\n        predictions_by_class = []\n        for class_idx in range(self.num_classes):\n            class_preds = [pred[class_idx] for pred in predictions]\n            predictions_by_class.append(class_preds)\n\n        return predictions_by_class\n\n\n    def predict_proba(self, X):\n\n        predictions = self.get_all_predictions(X)\n\n\n        # If this is just a single dictionary we're getting predictions from:\n        if X.shape[0] == 1:\n            # predictions is just a dictionary where all the values are the predicted values from one of our subpredictors. we'll want that as a list\n            predicted_vals = list(predictions.values())\n            predicted_vals = self.get_predictions_by_class(predicted_vals)\n\n            if self.ensemble_method == 'median':\n                return [np.median(class_preds) for class_preds in predicted_vals]\n            elif self.ensemble_method == 'average' or self.ensemble_method == 'mean' or self.ensemble_method == 'avg':\n                return [np.average(class_preds) for class_preds in predicted_vals]\n            elif self.ensemble_method == 'max':\n                return [np.max(class_preds) for class_preds in predicted_vals]\n            elif self.ensemble_method == 'min':\n                return [np.min(class_preds) for class_preds in predicted_vals]\n\n        else:\n            classed_predictions = predictions.apply(self.get_predictions_by_class, axis=1)\n\n            if self.ensemble_method == 'median':\n                return classed_predictions.apply(np.median, axis=1)\n            elif self.ensemble_method == 'average' or self.ensemble_method == 'mean' or self.ensemble_method == 'avg':\n                return classed_predictions.apply(np.average, axis=1)\n            elif self.ensemble_method == 'max':\n                return classed_predictions.apply(np.max, axis=1)\n            elif self.ensemble_method == 'min':\n                return classed_predictions.apply(np.min, axis=1)\n\n"""
auto_ml/utils_feature_selection.py,0,"b""from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n\n\nimport scipy\nimport itertools\nfrom sklearn.feature_selection import GenericUnivariateSelect, RFECV, SelectFromModel\n\n\ndef get_feature_selection_model_from_name(type_of_estimator, model_name):\n    model_map = {\n        'classifier': {\n            'SelectFromModel': SelectFromModel(RandomForestClassifier(n_jobs=-1, max_depth=10, n_estimators=15), threshold='20*mean'),\n            'RFECV': RFECV(estimator=RandomForestClassifier(n_jobs=-1), step=0.1),\n            'GenericUnivariateSelect': GenericUnivariateSelect(),\n            'KeepAll': 'KeepAll'\n        },\n        'regressor': {\n            'SelectFromModel': SelectFromModel(RandomForestRegressor(n_jobs=-1, max_depth=10, n_estimators=15), threshold='0.7*mean'),\n            'RFECV': RFECV(estimator=RandomForestRegressor(n_jobs=-1), step=0.1),\n            'GenericUnivariateSelect': GenericUnivariateSelect(),\n            'KeepAll': 'KeepAll'\n        }\n    }\n\n    return model_map[type_of_estimator][model_name]\n\n\nclass FeatureSelectionTransformer(BaseEstimator, TransformerMixin):\n\n\n    def __init__(self, type_of_estimator, column_descriptions, feature_selection_model='SelectFromModel'):\n\n        self.column_descriptions = column_descriptions\n        self.type_of_estimator = type_of_estimator\n        self.feature_selection_model = feature_selection_model\n\n\n    def get(self, prop_name, default=None):\n        try:\n            return getattr(self, prop_name)\n        except AttributeError:\n            return default\n\n\n    def fit(self, X, y=None):\n        print('Performing feature selection')\n\n\n        self.selector = get_feature_selection_model_from_name(self.type_of_estimator, self.feature_selection_model)\n\n        if self.selector == 'KeepAll':\n            if scipy.sparse.issparse(X):\n                num_cols = X.shape[0]\n            else:\n                num_cols = len(X[0])\n\n            self.support_mask = [True for col_idx in range(num_cols) ]\n        else:\n            if self.feature_selection_model == 'SelectFromModel':\n                num_cols = X.shape[1]\n                num_rows = X.shape[0]\n                if self.type_of_estimator == 'regressor':\n                    self.estimator = RandomForestRegressor(n_jobs=-1, max_depth=10, n_estimators=15)\n                else:\n                    self.estimator = RandomForestClassifier(n_jobs=-1, max_depth=10, n_estimators=15)\n\n                self.estimator.fit(X, y)\n\n                feature_importances = self.estimator.feature_importances_\n\n                # Two ways of doing feature selection\n\n                # 1. Any feature with a feature importance of at least 1/100th of our max feature\n                max_feature_importance = max(feature_importances)\n                threshold_by_relative_importance = 0.01 * max_feature_importance\n\n                # 2. 1/4 the number of rows (so 100 rows means 25 columns)\n                sorted_importances = sorted(feature_importances, reverse=True)\n                max_cols = int(num_rows * 0.25)\n                try:\n                    threshold_by_max_cols = sorted_importances[max_cols]\n                except IndexError:\n                    threshold_by_max_cols = sorted_importances[-1]\n\n                threshold = max(threshold_by_relative_importance, threshold_by_max_cols)\n                self.support_mask = [True if x > threshold else False for x in feature_importances]\n\n            else:\n                self.selector.fit(X, y)\n                self.support_mask = self.selector.get_support()\n\n        # Get a mask of which indices it is we want to keep\n        self.index_mask = [idx for idx, val in enumerate(self.support_mask) if val == True]\n        return self\n\n\n    def transform(self, X, y=None):\n\n        if self.selector == 'KeepAll':\n            return X\n\n        if scipy.sparse.issparse(X):\n            if X.getformat() == 'csr':\n                # convert to a csc (column) matrix, rather than a csr (row) matrix\n                X = X.tocsc()\n\n            # Slice that column matrix to only get the relevant columns that we already calculated in fit:\n            X = X[:, self.index_mask]\n\n            # convert back to a csr matrix\n            return X.tocsr()\n\n        # If this is a dense matrix:\n        else:\n            X = X[:, self.index_mask]\n            return X\n\n"""
auto_ml/utils_model_training.py,0,"b'from collections import Iterable\nfrom copy import deepcopy\nimport datetime\nimport gc\nimport os\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport scipy\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import __version__ as sklearn_version\n\nfrom auto_ml import utils_models\nfrom auto_ml.utils_models import get_name_from_model\nkeras_imported = False\n\n# This is the Air Traffic Controller (ATC) that is a wrapper around sklearn estimators.\n# In short, it wraps all the methods the pipeline will look for (fit, score, predict, predict_proba, etc.)\n# However, it also gives us the ability to optimize this stage in conjunction with the rest of the pipeline.\n# It also gives us more granular control over things like turning the input for GradientBoosting into dense matrices, or appending a set of dummy 1\'s to the end of sparse matrices getting predictions from XGBoost.\n\nclass FinalModelATC(BaseEstimator, TransformerMixin):\n\n\n    def __init__(self, model, model_name=None, ml_for_analytics=False, type_of_estimator=\'classifier\', output_column=None, name=None, _scorer=None, training_features=None, column_descriptions=None, feature_learning=False, uncertainty_model=None, uc_results = None, training_prediction_intervals=False, min_step_improvement=0.0001, interval_predictors=None, keep_cat_features=False, is_hp_search=None, X_test=None, y_test=None):\n\n        self.model = model\n        self.model_name = model_name\n        self.ml_for_analytics = ml_for_analytics\n        self.type_of_estimator = type_of_estimator\n        self.name = name\n        self.training_features = training_features\n        self.column_descriptions = column_descriptions\n        self.feature_learning = feature_learning\n        self.uncertainty_model = uncertainty_model\n        self.uc_results = uc_results\n        self.training_prediction_intervals = training_prediction_intervals\n        self.min_step_improvement = min_step_improvement\n        self.interval_predictors = interval_predictors\n        self.is_hp_search = is_hp_search\n        self.keep_cat_features = keep_cat_features\n        self.X_test = X_test\n        self.y_test = y_test\n        self.memory_optimized = False\n\n\n        if self.type_of_estimator == \'classifier\':\n            self._scorer = _scorer\n        else:\n            self._scorer = _scorer\n\n\n    def get(self, prop_name, default=None):\n        try:\n            return getattr(self, prop_name)\n        except AttributeError:\n            return default\n\n\n    def fit(self, X, y):\n\n        global keras_imported, KerasRegressor, KerasClassifier, EarlyStopping, ModelCheckpoint, TerminateOnNaN, keras_load_model\n        self.model_name = get_name_from_model(self.model)\n\n        X_fit = X\n\n        if self.model_name[:12] == \'DeepLearning\' or self.model_name in [\'BayesianRidge\', \'LassoLars\', \'OrthogonalMatchingPursuit\', \'ARDRegression\', \'Perceptron\', \'PassiveAggressiveClassifier\', \'SGDClassifier\', \'RidgeClassifier\', \'LogisticRegression\', \'XGBClassifier\', \'XGBRegressor\']:\n\n            if self.model_name[:3] == \'XGB\' and scipy.sparse.issparse(X):\n                ones = [[1] for x in range(X.shape[0])]\n                # Trying to force XGBoost to play nice with sparse matrices\n                X_fit = scipy.sparse.hstack((X, ones))\n\n            elif scipy.sparse.issparse(X_fit):\n                X_fit = X_fit.todense()\n\n            if self.model_name[:12] == \'DeepLearning\':\n                if keras_imported == False:\n                    # Suppress some level of logs\n                    os.environ[\'TF_CPP_MIN_VLOG_LEVEL\'] = \'3\'\n                    os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n                    from keras.callbacks import EarlyStopping, ModelCheckpoint, TerminateOnNaN\n                    from keras.models import load_model as keras_load_model\n                    from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n\n                    keras_imported = True\n\n\n                # For Keras, we need to tell it how many input nodes to expect, which is our num_cols\n                num_cols = X_fit.shape[1]\n\n                model_params = self.model.get_params()\n                del model_params[\'build_fn\']\n                try:\n                    del model_params[\'feature_learning\']\n                except:\n                    pass\n                try:\n                    del model_params[\'num_cols\']\n                except:\n                    pass\n\n                if self.type_of_estimator == \'regressor\':\n                    self.model = KerasRegressor(build_fn=utils_models.make_deep_learning_model, num_cols=num_cols, feature_learning=self.feature_learning, **model_params)\n                elif self.type_of_estimator == \'classifier\':\n                    self.model = KerasClassifier(build_fn=utils_models.make_deep_learning_classifier, num_cols=num_cols, feature_learning=self.feature_learning, **model_params)\n\n        if self.model_name[:12] == \'DeepLearning\':\n            try:\n\n                if self.is_hp_search == True:\n                    patience = 5\n                    verbose = 0\n                else:\n                    patience = 25\n                    verbose = 2\n\n                X_fit, y, X_test, y_test = self.get_X_test(X_fit, y)\n                if isinstance(X_test, pd.DataFrame):\n                    X_test = X_test.values\n                else:\n                    try:\n                        X_test = X_test.toarray()\n                    except AttributeError as e:\n                        pass\n\n                if not self.is_hp_search:\n                    print(\'\\nWe will stop training early if we have not seen an improvement in validation accuracy in {} epochs\'.format(patience))\n                    print(\'To measure validation accuracy, we will split off a random 10 percent of your training data set\')\n\n                early_stopping = EarlyStopping(monitor=\'val_loss\', patience=patience, verbose=verbose)\n                terminate_on_nan = TerminateOnNaN()\n\n                now_time = datetime.datetime.now()\n                time_string = str(now_time.year) + \'_\' + str(now_time.month) + \'_\' + str(now_time.day) + \'_\' + str(now_time.hour) + \'_\' + str(now_time.minute)\n\n\n                temp_file_name = \'tmp_dl_model_checkpoint_\' + time_string + str(random.random()) + \'.h5\'\n                model_checkpoint = ModelCheckpoint(temp_file_name, monitor=\'val_loss\', save_best_only=True, mode=\'min\', period=1)\n\n                callbacks = [early_stopping, terminate_on_nan]\n                if not self.is_hp_search:\n                    callbacks.append(model_checkpoint)\n\n                self.model.fit(X_fit, y, callbacks=callbacks, validation_data=(X_test, y_test), verbose=verbose)\n\n                # TODO: give some kind of logging on how the model did here! best epoch, best accuracy, etc.\n\n                if self.is_hp_search is False:\n                    self.model = keras_load_model(temp_file_name)\n\n                try:\n                    os.remove(temp_file_name)\n                except OSError as e:\n                    pass\n            except KeyboardInterrupt as e:\n                print(\'Stopping training at this point because we heard a KeyboardInterrupt\')\n                print(\'If the deep learning model is functional at this point, we will output the model in its latest form\')\n                print(\'Note that this feature is an unofficial beta-release feature that is known to fail on occasion\')\n\n                if self.is_hp_search is False:\n                    self.model = keras_load_model(temp_file_name)\n                try:\n                    os.remove(temp_file_name)\n                except OSError as e:\n                    pass\n\n\n        elif self.model_name[:4] == \'LGBM\':\n\n            import lightgbm as lgb\n\n            if scipy.sparse.issparse(X_fit):\n                X_fit = X_fit.toarray()\n\n\n\n            verbose = True\n            if self.is_hp_search == True:\n                verbose = False\n\n            train_dynamic_n_estimators = False\n            if self.model.get_params()[\'n_estimators\'] == 2000:\n                train_dynamic_n_estimators = True\n\n                X_fit, y, X_test, y_test = self.get_X_test(X_fit, y)\n\n                try:\n                    X_test = X_test.toarray()\n                except AttributeError as e:\n                    pass\n\n                if self.X_test is not None:\n                    eval_name = \'X_test_the_user_passed_in\'\n                else:\n                    eval_name = \'random_holdout_set_from_training_data\'\n\n                if self.type_of_estimator == \'regressor\':\n                    if self.training_prediction_intervals == True:\n                        eval_metric = \'quantile\'\n                    else:\n                        eval_metric = \'rmse\'\n                elif self.type_of_estimator == \'classifier\':\n                    if len(set(y_test)) > 2:\n                        eval_metric = \'multi_logloss\'\n                    else:\n                        eval_metric = \'binary_logloss\'\n\n            cat_feature_indices = self.get_categorical_feature_indices()\n            if self.memory_optimized == True:\n                X_fit.to_csv(\'_lgbm_dataset.csv\')\n                del X_fit\n\n            if cat_feature_indices is None:\n                if train_dynamic_n_estimators:\n                    self.model.fit(X_fit, y, eval_set=[(X_test, y_test)], early_stopping_rounds=100, eval_metric=eval_metric, eval_names=[eval_name], verbose=verbose)\n                else:\n                    self.model.fit(X_fit, y, verbose=verbose)\n            else:\n                if train_dynamic_n_estimators:\n                    self.model.fit(X_fit, y, eval_set=[(X_test, y_test)], early_stopping_rounds=100, eval_metric=eval_metric, eval_names=[eval_name], categorical_feature=cat_feature_indices, verbose=verbose)\n                else:\n                    self.model.fit(X_fit, y, categorical_feature=cat_feature_indices, verbose=verbose)\n\n        elif self.model_name[:8] == \'CatBoost\':\n            if isinstance(X_fit, pd.DataFrame):\n                X_fit = X_fit.values\n            else:\n                X_fit = X_fit.toarray()\n\n            if self.type_of_estimator == \'classifier\' and len(pd.Series(y).unique()) > 2:\n                # TODO: we might have to modify the format of the y values, converting them all to ints, then back again (sklearn has a useful inverse_transform on some preprocessing classes)\n                self.model.set_params(loss_function=\'MultiClass\')\n\n            cat_feature_indices = self.get_categorical_feature_indices()\n\n            self.model.fit(X_fit, y, cat_features=cat_feature_indices)\n\n        elif self.model_name[:16] == \'GradientBoosting\':\n            if not sklearn_version > \'0.18.1\':\n                if isinstance(X_fit, pd.DataFrame):\n                    X_fit = X_fit.values\n                else:\n                    X_fit = X_fit.toarray()\n\n            patience = 20\n            best_val_loss = -10000000000\n            num_worse_rounds = 0\n            best_model = deepcopy(self.model)\n            X_fit, y, X_test, y_test = self.get_X_test(X_fit, y)\n\n            # Add a variable number of trees each time, depending how far into the process we are\n            if os.environ.get(\'is_test_suite\', False) == \'True\':\n                num_iters = list(range(1, 50, 1)) + list(range(50, 100, 2)) + list(range(100, 250, 3))\n            else:\n                num_iters = list(range(1, 50, 1)) + list(range(50, 100, 2)) + list(range(100, 250, 3)) + list(range(250, 500, 5)) + list(range(500, 1000, 10)) + list(range(1000, 2000, 20)) + list(range(2000, 10000, 100))\n            # TODO: get n_estimators from the model itself, and reduce this list to only those values that come under the value from the model\n\n            try:\n                for num_iter in num_iters:\n                    warm_start = True\n                    if num_iter == 1:\n                        warm_start = False\n\n                    self.model.set_params(n_estimators=num_iter, warm_start=warm_start)\n                    self.model.fit(X_fit, y)\n\n                    if self.training_prediction_intervals == True:\n                        val_loss = self.model.score(X_test, y_test)\n                    else:\n                        try:\n                            val_loss = self._scorer.score(self, X_test, y_test)\n                        except Exception as e:\n                            val_loss = self.model.score(X_test, y_test)\n\n                    if val_loss - self.min_step_improvement > best_val_loss:\n                        best_val_loss = val_loss\n                        num_worse_rounds = 0\n                        best_model = deepcopy(self.model)\n                    else:\n                        num_worse_rounds += 1\n                    print(\'[\' + str(num_iter) + \'] random_holdout_set_from_training_data\\\'s score is: \' + str(round(val_loss, 3)))\n                    if num_worse_rounds >= patience:\n                        break\n            except KeyboardInterrupt:\n                print(\'Heard KeyboardInterrupt. Stopping training, and using the best checkpointed GradientBoosting model\')\n                pass\n\n            self.model = best_model\n            print(\'The number of estimators that were the best for this training dataset: \' + str(self.model.get_params()[\'n_estimators\']))\n            print(\'The best score on the holdout set: \' + str(best_val_loss))\n\n        else:\n            self.model.fit(X_fit, y)\n\n        if self.X_test is not None:\n            del self.X_test\n            del self.y_test\n        gc.collect()\n        return self\n\n    def remove_categorical_values(self, features):\n        clean_features = set([])\n        for feature in features:\n            if \'=\' not in feature:\n                clean_features.add(feature)\n            else:\n                clean_features.add(feature[:feature.index(\'=\')])\n\n        return clean_features\n\n    def verify_features(self, X, raw_features_only=False):\n\n        if self.column_descriptions is None:\n            print(\'This feature is not enabled by default. Depending on the shape of the training data, it can add hundreds of KB to the saved file size.\')\n            print(\'Please pass in `ml_predictor.train(data, verify_features=True)` when training a model, and we will enable this function, at the cost of a potentially larger file size.\')\n            warnings.warn(\'Please pass verify_features=True when invoking .train() on the ml_predictor instance.\')\n            return None\n\n        print(\'\\n\\nNow verifying consistency between training features and prediction features\')\n        if isinstance(X, dict):\n            prediction_features = set(X.keys())\n        elif isinstance(X, pd.DataFrame):\n            prediction_features = set(X.columns)\n\n        # If the user passed in categorical features, we will effectively one-hot-encode them ourselves here\n        # Note that this assumes we\'re using the ""="" as the separater in DictVectorizer/DataFrameVectorizer\n        date_col_names = []\n        categorical_col_names = []\n        for key, value in self.column_descriptions.items():\n            if value == \'categorical\' and \'day_part\' not in key:\n                try:\n                    # This covers the case that the user passes in a value in column_descriptions that is not present in their prediction data\n                    column_vals = X[key].unique()\n                    for val in column_vals:\n                        prediction_features.add(key + \'=\' + str(val))\n\n                    categorical_col_names.append(key)\n                except:\n                    print(\'\\nFound a column in your column_descriptions that is not present in your prediction data:\')\n                    print(key)\n\n            elif \'day_part\' in key:\n                # We have found a date column. Make sure this date column is in our prediction data\n                # It is outside the scope of this function to make sure that the same date parts are available in both our training and testing data\n                raw_date_col_name = key[:key.index(\'day_part\') - 1]\n                date_col_names.append(raw_date_col_name)\n\n            elif value == \'output\':\n                try:\n                    prediction_features.remove(key)\n                except KeyError:\n                    pass\n\n        # Now that we\'ve added in all the one-hot-encoded categorical columns (name=val1, name=val2), remove the base name from our prediction data\n        prediction_features = prediction_features - set(categorical_col_names)\n\n        # Get only the unique raw_date_col_names\n        date_col_names = set(date_col_names)\n\n        training_features = set(self.training_features)\n\n        # Remove all of the transformed date column feature names from our training data\n        features_to_remove = []\n        for feature in training_features:\n            for raw_date_col_name in date_col_names:\n                if raw_date_col_name in feature:\n                    features_to_remove.append(feature)\n        training_features = training_features - set(features_to_remove)\n\n        # Make sure the raw_date_col_name is in our training data after we have removed all the transformed feature names\n        training_features = training_features | date_col_names\n\n        # MVP means ignoring text features\n        print_nlp_warning = False\n        nlp_example = None\n        for feature in training_features:\n            if \'nlp_\' in feature:\n                print_nlp_warning = True\n                nlp_example = feature\n                training_features.remove(feature)\n\n        if print_nlp_warning == True:\n            print(\'\\n\\nWe found an NLP column in the training data\')\n            print(\'verify_features() currently does not support checking all of the values within an NLP column, so if the text of your NLP column has dramatically changed, you will have to check that yourself.\')\n            print(\'Here is one example of an NLP feature in the training data:\')\n            print(nlp_example)\n\n        training_not_prediction = training_features - prediction_features\n\n        if raw_features_only == True:\n            training_not_prediction = self.remove_categorical_values(training_not_prediction)\n\n        if len(training_not_prediction) > 0:\n\n            print(\'\\n\\nHere are the features this model was trained on that were not present in this prediction data:\')\n            print(sorted(list(training_not_prediction)))\n        else:\n            print(\'All of the features this model was trained on are included in the prediction data\')\n\n        prediction_not_training = prediction_features - training_features\n        if raw_features_only == True:\n            prediction_not_training = self.remove_categorical_values(prediction_not_training)\n\n        if len(prediction_not_training) > 0:\n\n            # Separate out those values we were told to ignore by column_descriptions\n            ignored_features = []\n            for feature in prediction_not_training:\n                if self.column_descriptions.get(feature, \'False\') == \'ignore\':\n                    ignored_features.append(feature)\n            prediction_not_training = prediction_not_training - set(ignored_features)\n\n            print(\'\\n\\nHere are the features available in the prediction data that were not part of the training data:\')\n            print(sorted(list(prediction_not_training)))\n\n            if len(ignored_features) > 0:\n                print(\'\\n\\nAdditionally, we found features in the prediction data that we were told to ignore in the training data\')\n                print(sorted(list(ignored_features)))\n\n        else:\n            print(\'All of the features in the prediction data were in this model\\\'s training data\')\n\n        print(\'\\n\\n\')\n        return {\n            \'training_not_prediction\': training_not_prediction\n            , \'prediction_not_training\': prediction_not_training\n        }\n\n\n    def score(self, X, y, verbose=False):\n        # At the time of writing this, GradientBoosting does not support sparse matrices for predictions\n        if (self.model_name[:16] == \'GradientBoosting\' or self.model_name in [\'BayesianRidge\', \'LassoLars\', \'OrthogonalMatchingPursuit\', \'ARDRegression\']) and scipy.sparse.issparse(X):\n            X = X.todense()\n\n        if self._scorer is not None:\n            if self.type_of_estimator == \'regressor\':\n                return self._scorer.score(self, X, y)\n            elif self.type_of_estimator == \'classifier\':\n                return self._scorer.score(self, X, y)\n\n\n        else:\n            return self.model.score(X, y)\n\n\n    def predict_proba(self, X, verbose=False):\n\n        if self.model_name[:3] == \'XGB\':\n            ones = [[1] for x in range(X.shape[0])]\n            if scipy.sparse.issparse(X):\n                # Trying to force XGBoost to play nice with sparse matrices\n                X = scipy.sparse.hstack((X, ones))\n            else:\n                X = np.column_stack([X, ones])\n\n        X_predict = X\n\n        if (self.model_name[:16] == \'GradientBoosting\' or self.model_name[:12] == \'DeepLearning\' or self.model_name in [\'BayesianRidge\', \'LassoLars\', \'OrthogonalMatchingPursuit\', \'ARDRegression\']):\n            if scipy.sparse.issparse(X):\n                X = X.todense()\n            elif isinstance(X, pd.DataFrame):\n                X = X.values\n        elif (self.model_name[:8] == \'CatBoost\' or self.model_name[:4] == \'LGBM\'):\n            if scipy.sparse.issparse(X):\n                X = X.toarray()\n            elif isinstance(X, pd.DataFrame):\n                X = X.values\n\n        try:\n            if self.model_name[:4] == \'LGBM\':\n                try:\n                    best_iteration = self.model.best_iteration\n                except AttributeError:\n                    best_iteration = self.model.best_iteration_\n                predictions = self.model.predict_proba(X, num_iteration=best_iteration)\n            else:\n                predictions = self.model.predict_proba(X)\n\n        except AttributeError as e:\n            try:\n                predictions = self.model.predict(X)\n            except TypeError as e:\n                if scipy.sparse.issparse(X):\n                    X = X.todense()\n                predictions = self.model.predict(X)\n\n        except TypeError as e:\n            if scipy.sparse.issparse(X):\n                X = X.todense()\n            predictions = self.model.predict_proba(X)\n\n        # If this model does not have predict_proba, and we have fallen back on predict, we want to make sure we give results back in the same format the user would expect for predict_proba, namely each prediction is a list of predicted probabilities for each class.\n        # Note that this DOES NOT WORK for multi-label problems, or problems that are not reduced to 0,1\n        # If this is not an iterable (ignoring strings, which might be iterable), then we will want to turn our predictions into tupled predictions\n        if not (hasattr(predictions[0], \'__iter__\') and not isinstance(predictions[0], str)):\n            tupled_predictions = []\n            for prediction in predictions:\n                if prediction == 1:\n                    tupled_predictions.append([0,1])\n                else:\n                    tupled_predictions.append([1,0])\n            predictions = tupled_predictions\n\n\n        # This handles an annoying edge case with libraries like Keras that, for a binary classification problem, with return a single predicted probability in a list, rather than the probability of both classes in a list\n        if len(predictions[0]) == 1:\n            tupled_predictions = []\n            for prediction in predictions:\n                tupled_predictions.append([1 - prediction[0], prediction[0]])\n            predictions = tupled_predictions\n\n        if X.shape[0] == 1:\n            return predictions[0]\n        else:\n            return predictions\n\n    def predict(self, X, verbose=False):\n\n        if self.model_name[:3] == \'XGB\':\n            ones = [[1] for x in range(X.shape[0])]\n            if scipy.sparse.issparse(X):\n                # Trying to force XGBoost to play nice with sparse matrices\n                X = scipy.sparse.hstack((X, ones))\n            else:\n                X = np.column_stack([X, ones])\n\n        X_predict = X\n\n        if (self.model_name[:16] == \'GradientBoosting\' or self.model_name[:12] == \'DeepLearning\' or self.model_name in [\'BayesianRidge\', \'LassoLars\', \'OrthogonalMatchingPursuit\', \'ARDRegression\']):\n            if scipy.sparse.issparse(X):\n                X_predict = X.todense()\n            elif isinstance(X, pd.DataFrame):\n                X_predict = X.values\n        elif self.model_name[:8] == \'CatBoost\':\n            if scipy.sparse.issparse(X):\n                X_predict = X.toarray()\n            elif isinstance(X, pd.DataFrame):\n                X_predict = X.values\n        else:\n            X_predict = X\n\n\n        if self.model_name[:4] == \'LGBM\':\n            best_iteration = 0\n            try:\n                best_iteration = self.model.best_iteration_\n            except AttributeError:\n                best_iteration = self.model.best_iteration\n            if best_iteration is None:\n                best_iteration = 0\n            predictions = self.model.predict(X, num_iteration=best_iteration)\n        else:\n            predictions = self.model.predict(X_predict)\n        # Handle cases of getting a prediction for a single item.\n        # It makes a cleaner interface just to get just the single prediction back, rather than a list with the prediction hidden inside.\n\n        if isinstance(predictions, np.ndarray):\n            predictions = predictions.tolist()\n            if isinstance(predictions, float) or isinstance(predictions, int) or isinstance(predictions, str):\n                return predictions\n\n        if isinstance(predictions[0], list) and len(predictions[0]) == 1:\n            predictions = [row[0] for row in predictions]\n\n        if len(predictions) == 1:\n            return predictions[0]\n        else:\n            return predictions\n\n    def predict_intervals(self, X, return_type=None):\n\n        if self.interval_predictors is None:\n            print(\'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\')\n            print(\'This model was not trained to predict intervals\')\n            print(\'Please follow the documentation to tell this model at training time to learn how to predict intervals\')\n            print(\'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\')\n            raise ValueError(\'This model was not trained to predict intervals\')\n\n        base_prediction = self.predict(X)\n\n        result = {\n            \'prediction\': base_prediction\n        }\n        for tup in self.interval_predictors:\n            predictor_name = tup[0]\n            predictor = tup[1]\n            result[predictor_name] = predictor.predict(X)\n\n        if scipy.sparse.issparse(X):\n            len_input = X.shape[0]\n        else:\n            len_input = len(X)\n\n        if (len_input == 1 and return_type is None) or return_type == \'dict\':\n            return result\n\n        elif (len_input > 1 and return_type is None) or return_type == \'df\' or return_type == \'dataframe\':\n            return pd.DataFrame(result)\n\n        elif return_type == \'list\':\n            if len_input == 1:\n                list_result = [base_prediction]\n                for tup in self.interval_predictors:\n                    list_result.append(result[tup[0]])\n            else:\n                list_result = []\n                for idx in range(len_input):\n                    row_result = [base_prediction[idx]]\n                    for tup in self.interval_predictors:\n                        row_result.append(result[tup[0]][idx])\n                    list_result.append(row_result)\n\n            return list_result\n\n        else:\n            print(\'Please pass in a return_type value of one of the following: [""dict"", ""dataframe"", ""df"", ""list""]\')\n            raise(ValueError(\'Please pass in a return_type value of one of the following: [""dict"", ""dataframe"", ""df"", ""list""]\'))\n\n\n    # transform is initially designed to be used with feature_learning\n    def transform(self, X):\n        predicted_features = self.predict(X)\n        predicted_features = list(predicted_features)\n\n        X = scipy.sparse.hstack([X, predicted_features], format=\'csr\')\n        return X\n\n    # Allows the user to get the fully transformed data\n    def transform_only(self, X):\n        return X\n\n    def predict_uncertainty(self, X):\n        if self.uncertainty_model is None:\n            print(\'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\')\n            print(\'This model was not trained to predict uncertainties\')\n            print(\'Please follow the documentation to tell this model at training time to learn how to predict uncertainties\')\n            print(\'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\')\n            raise ValueError(\'This model was not trained to predict uncertainties\')\n\n        base_predictions = self.predict(X)\n\n        if isinstance(base_predictions, Iterable):\n            base_predictions_col = [[val] for val in base_predictions]\n            base_predictions_col = np.array(base_predictions_col)\n        else:\n            base_predictions_col = [base_predictions]\n\n        X_combined = scipy.sparse.hstack([X, base_predictions_col], format=\'csr\')\n\n        uncertainty_predictions = self.uncertainty_model.predict_proba(X_combined)\n\n        results = {\n            \'base_prediction\': base_predictions\n            , \'uncertainty_prediction\': uncertainty_predictions\n        }\n\n\n\n        if isinstance(base_predictions, Iterable):\n\n            results[\'uncertainty_prediction\'] = [row[1] for row in results[\'uncertainty_prediction\']]\n\n            results = pd.DataFrame.from_dict(results, orient=\'columns\')\n\n            if self.uc_results is not None:\n                calibration_results = {}\n                # grab the relevant properties from our uc_results, and make them each their own list in calibration_results\n                for key, value in self.uc_results[1].items():\n                    calibration_results[key] = []\n\n                for proba in results[\'uncertainty_prediction\']:\n                    max_bucket_proba = 0\n                    bucket_num = 1\n                    while proba > max_bucket_proba:\n                        calibration_result = self.uc_results[bucket_num]\n                        max_bucket_proba = self.uc_results[bucket_num][\'max_proba\']\n                        bucket_num += 1\n\n                    for key, value in calibration_result.items():\n                        calibration_results[key].append(value)\n                # TODO: grab the uncertainty_calibration data for DataFrames\n                df_calibration_results = pd.DataFrame.from_dict(calibration_results, orient=\'columns\')\n                del df_calibration_results[\'max_proba\']\n\n                results = pd.concat([results, df_calibration_results], axis=1)\n\n        else:\n            if self.uc_results is not None:\n                # TODO: grab the uncertainty_calibration data for dictionaries\n                for bucket_name, bucket_result in self.uc_results.items():\n                    if proba > bucket_result[\'max_proba\']:\n                        break\n                    results.update(bucket_result)\n                    del results[\'max_proba\']\n\n\n\n\n        return results\n\n\n    def score_uncertainty(self, X, y, verbose=False):\n        return self.uncertainty_model.score(X, y, verbose=False)\n\n\n    def get_categorical_feature_indices(self):\n        cat_feature_indices = None\n        if self.keep_cat_features == True:\n            cat_feature_names = [k for k, v in self.column_descriptions.items() if v == \'categorical\']\n            cat_feature_indices = [self.training_features.index(cat_name) for cat_name in cat_feature_names]\n\n        return cat_feature_indices\n\n\n    def get_X_test(self, X_fit, y):\n\n        if self.X_test is not None:\n            return X_fit, y, self.X_test, self.y_test\n        else:\n            X_fit, X_test, y, y_test = train_test_split(X_fit, y, test_size=0.15)\n            return X_fit, y, X_test, y_test\n\n\n\n\n\n\n'"
auto_ml/utils_models.py,0,"b'import dill\nimport os\nimport random\nimport sys\n\nfrom auto_ml import utils\nfrom auto_ml import utils_categorical_ensembling\n\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier\n\nfrom sklearn.linear_model import RANSACRegressor, LinearRegression, Ridge, Lasso, ElasticNet, LassoLars, OrthogonalMatchingPursuit, BayesianRidge, ARDRegression, SGDRegressor, PassiveAggressiveRegressor, LogisticRegression, RidgeClassifier, SGDClassifier, Perceptron, PassiveAggressiveClassifier\n\nfrom sklearn.svm import LinearSVC, LinearSVR\n\nfrom sklearn.cluster import MiniBatchKMeans\n\nxgb_installed = False\ntry:\n    from xgboost import XGBClassifier, XGBRegressor\n    xgb_installed = True\nexcept ImportError:\n    pass\n\nlgb_installed = False\ntry:\n    from lightgbm import LGBMRegressor, LGBMClassifier\n    lgb_installed = True\nexcept ImportError:\n    pass\nexcept OSError:\n    pass\n\ncatboost_installed = False\ntry:\n    from catboost import CatBoostRegressor, CatBoostClassifier\n    catboost_installed = True\nexcept ImportError:\n    pass\n\n\nkeras_imported = False\nmaxnorm = None\nDense = None\nDropout = None\nLeakyReLU = None\nThresholdedReLU = None\nPReLU = None\nELU = None\nSequential = None\nkeras_load_model = None\nregularizers = None\noptimizers = None\nKerasRegressor = None\nKerasClassifier = None\nActivation = None\n\n# Note: it\'s important that importing tensorflow come last. We can run into OpenCL issues if we import it ahead of some other packages. At the moment, it\'s a known behavior with tensorflow, but everyone\'s ok with this workaround.\n\n\n\n\n\ndef get_model_from_name(model_name, training_params=None, is_hp_search=False):\n    global keras_imported\n\n    # For Keras\n    epochs = 1000\n    # if os.environ.get(\'is_test_suite\', 0) == \'True\' and model_name[:12] == \'DeepLearning\':\n    #     print(\'Heard that this is the test suite. Limiting number of epochs, which will increase training speed dramatically at the expense of model accuracy\')\n    #     epochs = 100\n\n    all_model_params = {\n        \'LogisticRegression\': {},\n        \'RandomForestClassifier\': {\'n_jobs\': -2, \'n_estimators\': 30},\n        \'ExtraTreesClassifier\': {\'n_jobs\': -1},\n        \'AdaBoostClassifier\': {},\n        \'SGDClassifier\': {\'n_jobs\': -1},\n        \'Perceptron\': {\'n_jobs\': -1},\n        \'LinearSVC\': {\'dual\': False},\n        \'LinearRegression\': {\'n_jobs\': -2},\n        \'RandomForestRegressor\': {\'n_jobs\': -2, \'n_estimators\': 30},\n        \'LinearSVR\': {\'dual\': False, \'loss\': \'squared_epsilon_insensitive\'},\n        \'ExtraTreesRegressor\': {\'n_jobs\': -1},\n        \'MiniBatchKMeans\': {\'n_clusters\': 8},\n        \'GradientBoostingRegressor\': {\'presort\': False, \'learning_rate\': 0.1, \'warm_start\': True},\n        \'GradientBoostingClassifier\': {\'presort\': False, \'learning_rate\': 0.1, \'warm_start\': True},\n        \'SGDRegressor\': {\'shuffle\': False},\n        \'PassiveAggressiveRegressor\': {\'shuffle\': False},\n        \'AdaBoostRegressor\': {},\n        \'LGBMRegressor\': {\'n_estimators\': 2000, \'learning_rate\': 0.15, \'num_leaves\': 8, \'lambda_l2\': 0.001, \'histogram_pool_size\': 16384},\n        \'LGBMClassifier\': {\'n_estimators\': 2000, \'learning_rate\': 0.15, \'num_leaves\': 8, \'lambda_l2\': 0.001, \'histogram_pool_size\': 16384},\n        \'DeepLearningRegressor\': {\'epochs\': epochs, \'batch_size\': 50, \'verbose\': 2},\n        \'DeepLearningClassifier\': {\'epochs\': epochs, \'batch_size\': 50, \'verbose\': 2},\n        \'CatBoostRegressor\': {},\n        \'CatBoostClassifier\': {}\n    }\n\n    # if os.environ.get(\'is_test_suite\', 0) == \'True\':\n    #     all_model_params\n\n    model_params = all_model_params.get(model_name, None)\n    if model_params is None:\n        model_params = {}\n\n    if is_hp_search == True:\n        if model_name[:12] == \'DeepLearning\':\n            model_params[\'epochs\'] = 50\n        if model_name[:4] == \'LGBM\':\n            model_params[\'n_estimators\'] = 500\n\n\n    if training_params is not None:\n        print(\'Now using the model training_params that you passed in:\')\n        print(training_params)\n        # Overwrite our stock params with what the user passes in (i.e., if the user wants 10,000 trees, we will let them do it)\n        model_params.update(training_params)\n        print(\'After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\')\n        print(model_params)\n\n\n    model_map = {\n        # Classifiers\n        \'LogisticRegression\': LogisticRegression(),\n        \'RandomForestClassifier\': RandomForestClassifier(),\n        \'RidgeClassifier\': RidgeClassifier(),\n        \'GradientBoostingClassifier\': GradientBoostingClassifier(),\n        \'ExtraTreesClassifier\': ExtraTreesClassifier(),\n        \'AdaBoostClassifier\': AdaBoostClassifier(),\n\n\n        \'LinearSVC\': LinearSVC(),\n\n        # Regressors\n        \'LinearRegression\': LinearRegression(),\n        \'RandomForestRegressor\': RandomForestRegressor(),\n        \'Ridge\': Ridge(),\n        \'LinearSVR\': LinearSVR(),\n        \'ExtraTreesRegressor\': ExtraTreesRegressor(),\n        \'AdaBoostRegressor\': AdaBoostRegressor(),\n        \'RANSACRegressor\': RANSACRegressor(),\n        \'GradientBoostingRegressor\': GradientBoostingRegressor(),\n\n        \'Lasso\': Lasso(),\n        \'ElasticNet\': ElasticNet(),\n        \'LassoLars\': LassoLars(),\n        \'OrthogonalMatchingPursuit\': OrthogonalMatchingPursuit(),\n        \'BayesianRidge\': BayesianRidge(),\n        \'ARDRegression\': ARDRegression(),\n\n        # Clustering\n        \'MiniBatchKMeans\': MiniBatchKMeans(),\n    }\n\n    try:\n        model_map[\'SGDClassifier\'] = SGDClassifier(max_iter=1000, tol=0.001)\n        model_map[\'Perceptron\'] = Perceptron(max_iter=1000, tol=0.001)\n        model_map[\'PassiveAggressiveClassifier\'] = PassiveAggressiveClassifier(max_iter=1000, tol=0.001)\n        model_map[\'SGDRegressor\'] = SGDRegressor(max_iter=1000, tol=0.001)\n        model_map[\'PassiveAggressiveRegressor\'] = PassiveAggressiveRegressor(max_iter=1000, tol=0.001)\n    except TypeError:\n        model_map[\'SGDClassifier\'] = SGDClassifier()\n        model_map[\'Perceptron\'] = Perceptron()\n        model_map[\'PassiveAggressiveClassifier\'] = PassiveAggressiveClassifier()\n        model_map[\'SGDRegressor\'] = SGDRegressor()\n        model_map[\'PassiveAggressiveRegressor\'] = PassiveAggressiveRegressor()\n\n    if xgb_installed:\n        model_map[\'XGBClassifier\'] = XGBClassifier()\n        model_map[\'XGBRegressor\'] = XGBRegressor()\n\n    if lgb_installed:\n        model_map[\'LGBMRegressor\'] = LGBMRegressor()\n        model_map[\'LGBMClassifier\'] = LGBMClassifier()\n\n    if catboost_installed:\n        model_map[\'CatBoostRegressor\'] = CatBoostRegressor(calc_feature_importance=True)\n        model_map[\'CatBoostClassifier\'] = CatBoostClassifier(calc_feature_importance=True)\n\n    if model_name[:12] == \'DeepLearning\':\n        if keras_imported == False:\n            # Suppress some level of logs if TF is installed (but allow it to not be installed, and use Theano instead)\n            try:\n                os.environ[\'TF_CPP_MIN_VLOG_LEVEL\'] = \'3\'\n                os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n                from tensorflow import logging\n                logging.set_verbosity(logging.INFO)\n            except:\n                pass\n\n            global maxnorm\n            global Dense, Dropout\n            global LeakyReLU, PReLU, ThresholdedReLU, ELU\n            global Sequential\n            global keras_load_model\n            global regularizers, optimizers\n            global Activation\n            global KerasRegressor, KerasClassifier\n\n            from keras.constraints import maxnorm\n            from keras.layers import Activation, Dense, Dropout\n            from keras.layers.advanced_activations import LeakyReLU, PReLU, ThresholdedReLU, ELU\n            from keras.models import Sequential\n            from keras.models import load_model as keras_load_model\n            from keras import regularizers, optimizers\n            from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n            keras_imported = True\n\n        model_map[\'DeepLearningClassifier\'] = KerasClassifier(build_fn=make_deep_learning_classifier)\n        model_map[\'DeepLearningRegressor\'] = KerasRegressor(build_fn=make_deep_learning_model)\n\n    try:\n        model_without_params = model_map[model_name]\n    except KeyError as e:\n        print(\'It appears you are trying to use a library that is not available when we try to import it, or using a value for model_names that we do not recognize\')\n        raise(e)\n\n    if os.environ.get(\'is_test_suite\', False) == \'True\':\n        if \'n_jobs\' in model_params:\n            model_params[\'n_jobs\'] = 1\n    model_with_params = model_without_params.set_params(**model_params)\n\n    return model_with_params\n\n\ndef get_name_from_model(model):\n    if isinstance(model, LogisticRegression):\n        return \'LogisticRegression\'\n    if isinstance(model, RandomForestClassifier):\n        return \'RandomForestClassifier\'\n    if isinstance(model, RidgeClassifier):\n        return \'RidgeClassifier\'\n    if isinstance(model, GradientBoostingClassifier):\n        return \'GradientBoostingClassifier\'\n    if isinstance(model, ExtraTreesClassifier):\n        return \'ExtraTreesClassifier\'\n    if isinstance(model, AdaBoostClassifier):\n        return \'AdaBoostClassifier\'\n    if isinstance(model, SGDClassifier):\n        return \'SGDClassifier\'\n    if isinstance(model, Perceptron):\n        return \'Perceptron\'\n    if isinstance(model, PassiveAggressiveClassifier):\n        return \'PassiveAggressiveClassifier\'\n    if isinstance(model, LinearRegression):\n        return \'LinearRegression\'\n    if isinstance(model, RandomForestRegressor):\n        return \'RandomForestRegressor\'\n    if isinstance(model, Ridge):\n        return \'Ridge\'\n    if isinstance(model, ExtraTreesRegressor):\n        return \'ExtraTreesRegressor\'\n    if isinstance(model, AdaBoostRegressor):\n        return \'AdaBoostRegressor\'\n    if isinstance(model, RANSACRegressor):\n        return \'RANSACRegressor\'\n    if isinstance(model, GradientBoostingRegressor):\n        return \'GradientBoostingRegressor\'\n    if isinstance(model, Lasso):\n        return \'Lasso\'\n    if isinstance(model, ElasticNet):\n        return \'ElasticNet\'\n    if isinstance(model, LassoLars):\n        return \'LassoLars\'\n    if isinstance(model, OrthogonalMatchingPursuit):\n        return \'OrthogonalMatchingPursuit\'\n    if isinstance(model, BayesianRidge):\n        return \'BayesianRidge\'\n    if isinstance(model, ARDRegression):\n        return \'ARDRegression\'\n    if isinstance(model, SGDRegressor):\n        return \'SGDRegressor\'\n    if isinstance(model, PassiveAggressiveRegressor):\n        return \'PassiveAggressiveRegressor\'\n    if isinstance(model, MiniBatchKMeans):\n        return \'MiniBatchKMeans\'\n    if isinstance(model, LinearSVR):\n        return \'LinearSVR\'\n    if isinstance(model, LinearSVC):\n        return \'LinearSVC\'\n\n    if xgb_installed:\n        if isinstance(model, XGBClassifier):\n            return \'XGBClassifier\'\n        if isinstance(model, XGBRegressor):\n            return \'XGBRegressor\'\n\n    if keras_imported:\n        if isinstance(model, KerasRegressor):\n            return \'DeepLearningRegressor\'\n        if isinstance(model, KerasClassifier):\n            return \'DeepLearningClassifier\'\n\n    if lgb_installed:\n        if isinstance(model, LGBMClassifier):\n            return \'LGBMClassifier\'\n        if isinstance(model, LGBMRegressor):\n            return \'LGBMRegressor\'\n\n    if catboost_installed:\n        if isinstance(model, CatBoostClassifier):\n            return \'CatBoostClassifier\'\n        if isinstance(model, CatBoostRegressor):\n            return \'CatBoostRegressor\'\n\n# Hyperparameter search spaces for each model\ndef get_search_params(model_name):\n    grid_search_params = {\n        \'DeepLearningRegressor\': {\n            \'hidden_layers\': [\n                [1],\n                [1, 0.1],\n                [1, 1, 1],\n                [1, 0.5, 0.1],\n                [2],\n                [5],\n                [1, 0.5, 0.25, 0.1, 0.05],\n                [1, 1, 1, 1],\n                [1, 1]\n\n                # [1],\n                # [0.5],\n                # [2],\n                # [1, 1],\n                # [0.5, 0.5],\n                # [2, 2],\n                # [1, 1, 1],\n                # [1, 0.5, 0.5],\n                # [0.5, 1, 1],\n                # [1, 0.5, 0.25],\n                # [1, 2, 1],\n                # [1, 1, 1, 1],\n                # [1, 0.66, 0.33, 0.1],\n                # [1, 2, 2, 1]\n            ]\n            , \'dropout_rate\': [0.0, 0.2, 0.4, 0.6, 0.8]\n            , \'kernel_initializer\': [\'uniform\', \'lecun_uniform\', \'normal\', \'zero\', \'glorot_normal\', \'glorot_uniform\', \'he_normal\', \'he_uniform\']\n            , \'activation\': [\'tanh\', \'softmax\', \'elu\', \'softplus\', \'softsign\', \'relu\', \'sigmoid\', \'hard_sigmoid\', \'linear\', \'LeakyReLU\', \'PReLU\', \'ELU\', \'ThresholdedReLU\']\n            , \'batch_size\': [16, 32, 64, 128, 256, 512]\n            , \'optimizer\': [\'SGD\', \'RMSprop\', \'Adagrad\', \'Adadelta\', \'Adam\', \'Adamax\', \'Nadam\']\n        },\n        \'DeepLearningClassifier\': {\n            \'hidden_layers\': [\n                [1],\n                [0.5],\n                [2],\n                [1, 1],\n                [0.5, 0.5],\n                [2, 2],\n                [1, 1, 1],\n                [1, 0.5, 0.5],\n                [0.5, 1, 1],\n                [1, 0.5, 0.25],\n                [1, 2, 1],\n                [1, 1, 1, 1],\n                [1, 0.66, 0.33, 0.1],\n                [1, 2, 2, 1]\n            ]\n            , \'batch_size\': [16, 32, 64, 128, 256, 512]\n            , \'optimizer\': [\'SGD\', \'RMSprop\', \'Adagrad\', \'Adadelta\', \'Adam\', \'Adamax\', \'Nadam\']\n            , \'activation\': [\'tanh\', \'softmax\', \'elu\', \'softplus\', \'softsign\', \'relu\', \'sigmoid\', \'hard_sigmoid\', \'linear\', \'LeakyReLU\', \'PReLU\', \'ELU\', \'ThresholdedReLU\']\n            # , \'epochs\': [2, 4, 6, 10, 20]\n            # , \'batch_size\': [10, 25, 50, 100, 200, 1000]\n            # , \'lr\': [0.001, 0.01, 0.1, 0.3]\n            # , \'momentum\': [0.0, 0.3, 0.6, 0.8, 0.9]\n            # , \'init_mode\': [\'uniform\', \'lecun_uniform\', \'normal\', \'zero\', \'glorot_normal\', \'glorot_uniform\', \'he_normal\', \'he_uniform\']\n            # , \'activation\': [\'softmax\', \'softplus\', \'softsign\', \'relu\', \'tanh\', \'sigmoid\', \'hard_sigmoid\', \'linear\']\n            # , \'weight_constraint\': [1, 3, 5]\n            , \'dropout_rate\': [0.0, 0.3, 0.6, 0.8, 0.9]\n        },\n        \'XGBClassifier\': {\n            \'max_depth\': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15],\n            \'learning_rate\': [0.01, 0.05, 0.1, 0.2],\n            \'n_estimators\': [50, 75, 100, 150, 200, 375, 500, 750, 1000],\n            \'min_child_weight\': [1, 5, 10, 50],\n            \'subsample\': [0.5, 0.8, 1.0],\n            \'colsample_bytree\': [0.5, 0.8, 1.0]\n            # \'subsample\': [0.5, 1.0]\n            # \'lambda\': [0.9, 1.0]\n        },\n        \'XGBRegressor\': {\n            # Add in max_delta_step if classes are extremely imbalanced\n            \'max_depth\': [1, 3, 8, 25],\n            # \'lossl\': [\'ls\', \'lad\', \'huber\', \'quantile\'],\n            # \'booster\': [\'gbtree\', \'gblinear\', \'dart\'],\n            # \'objective\': [\'reg:linear\', \'reg:gamma\'],\n            # \'learning_rate\': [0.01, 0.1],\n            \'subsample\': [0.5, 1.0]\n            # \'subsample\': [0.4, 0.5, 0.58, 0.63, 0.68, 0.76],\n\n        },\n        \'GradientBoostingRegressor\': {\n            # Add in max_delta_step if classes are extremely imbalanced\n            \'max_depth\': [1, 2, 3, 4, 5, 7, 10, 15],\n            \'max_features\': [\'sqrt\', \'log2\', None],\n            \'loss\': [\'ls\', \'huber\'],\n            \'learning_rate\': [0.001, 0.01, 0.05,  0.1, 0.2],\n            \'n_estimators\': [10, 50, 75, 100, 125, 150, 200, 500, 1000, 2000],\n            \'subsample\': [0.5, 0.65, 0.8, 0.9, 0.95, 1.0]\n        },\n        \'GradientBoostingClassifier\': {\n            \'loss\': [\'deviance\', \'exponential\'],\n            \'max_depth\': [1, 2, 3, 4, 5, 7, 10, 15],\n            \'max_features\': [\'sqrt\', \'log2\', None],\n            \'learning_rate\': [0.001, 0.01, 0.05,  0.1, 0.2],\n            \'subsample\': [0.5, 0.65, 0.8, 0.9, 0.95, 1.0],\n            \'n_estimators\': [10, 50, 75, 100, 125, 150, 200, 500, 1000, 2000],\n\n        },\n\n        \'LogisticRegression\': {\n            \'C\': [.0001, .001, .01, .1, 1, 10, 100, 1000],\n            \'class_weight\': [None, \'balanced\'],\n            \'solver\': [\'newton-cg\', \'lbfgs\', \'sag\']\n        },\n        \'LinearRegression\': {\n            \'fit_intercept\': [True, False],\n            \'normalize\': [True, False]\n        },\n        \'RandomForestClassifier\': {\n            \'criterion\': [\'entropy\', \'gini\'],\n            \'class_weight\': [None, \'balanced\'],\n            \'max_features\': [\'sqrt\', \'log2\', None],\n            \'min_samples_split\': [2, 5, 20, 50, 100],\n            \'min_samples_leaf\': [1, 2, 5, 20, 50, 100],\n            \'bootstrap\': [True, False]\n        },\n        \'RandomForestRegressor\': {\n            \'max_features\': [\'auto\', \'sqrt\', \'log2\', None],\n            \'min_samples_split\': [2, 5, 20, 50, 100],\n            \'min_samples_leaf\': [1, 2, 5, 20, 50, 100],\n            \'bootstrap\': [True, False]\n        },\n        \'RidgeClassifier\': {\n            \'alpha\': [.0001, .001, .01, .1, 1, 10, 100, 1000],\n            \'class_weight\': [None, \'balanced\'],\n            \'solver\': [\'auto\', \'svd\', \'cholesky\', \'lsqr\', \'sparse_cg\', \'sag\']\n        },\n        \'Ridge\': {\n            \'alpha\': [.0001, .001, .01, .1, 1, 10, 100, 1000],\n            \'solver\': [\'auto\', \'svd\', \'cholesky\', \'lsqr\', \'sparse_cg\', \'sag\']\n        },\n        \'ExtraTreesRegressor\': {\n            \'max_features\': [\'auto\', \'sqrt\', \'log2\', None],\n            \'min_samples_split\': [2, 5, 20, 50, 100],\n            \'min_samples_leaf\': [1, 2, 5, 20, 50, 100],\n            \'bootstrap\': [True, False]\n        },\n        \'AdaBoostRegressor\': {\n            \'base_estimator\': [None, LinearRegression(n_jobs=-1)],\n            \'loss\': [\'linear\',\'square\',\'exponential\']\n        },\n        \'RANSACRegressor\': {\n            \'min_samples\': [None, .1, 100, 1000, 10000],\n            \'stop_probability\': [0.99, 0.98, 0.95, 0.90]\n        },\n        \'Lasso\': {\n            \'selection\': [\'cyclic\', \'random\'],\n            \'tol\': [.0000001, .000001, .00001, .0001, .001],\n            \'positive\': [True, False]\n        },\n\n        \'ElasticNet\': {\n            \'l1_ratio\': [0.1, 0.3, 0.5, 0.7, 0.9],\n            \'selection\': [\'cyclic\', \'random\'],\n            \'tol\': [.0000001, .000001, .00001, .0001, .001],\n            \'positive\': [True, False]\n        },\n\n        \'LassoLars\': {\n            \'positive\': [True, False],\n            \'max_iter\': [50, 100, 250, 500, 1000]\n        },\n\n        \'OrthogonalMatchingPursuit\': {\n            \'n_nonzero_coefs\': [None, 3, 5, 10, 25, 50, 75, 100, 200, 500]\n        },\n\n        \'BayesianRidge\': {\n            \'tol\': [.0000001, .000001, .00001, .0001, .001],\n            \'alpha_1\': [.0000001, .000001, .00001, .0001, .001],\n            \'lambda_1\': [.0000001, .000001, .00001, .0001, .001],\n            \'lambda_2\': [.0000001, .000001, .00001, .0001, .001]\n        },\n\n        \'ARDRegression\': {\n            \'tol\': [.0000001, .000001, .00001, .0001, .001],\n            \'alpha_1\': [.0000001, .000001, .00001, .0001, .001],\n            \'alpha_2\': [.0000001, .000001, .00001, .0001, .001],\n            \'lambda_1\': [.0000001, .000001, .00001, .0001, .001],\n            \'lambda_2\': [.0000001, .000001, .00001, .0001, .001],\n            \'threshold_lambda\': [100, 1000, 10000, 100000, 1000000]\n        },\n\n        \'SGDRegressor\': {\n            \'loss\': [\'squared_loss\', \'huber\', \'epsilon_insensitive\', \'squared_epsilon_insensitive\'],\n            \'penalty\': [\'none\', \'l2\', \'l1\', \'elasticnet\'],\n            \'learning_rate\': [\'constant\', \'optimal\', \'invscaling\'],\n            \'alpha\': [.0000001, .000001, .00001, .0001, .001]\n        },\n\n        \'PassiveAggressiveRegressor\': {\n            \'epsilon\': [0.01, 0.05, 0.1, 0.2, 0.5],\n            \'loss\': [\'epsilon_insensitive\', \'squared_epsilon_insensitive\'],\n            \'C\': [.0001, .001, .01, .1, 1, 10, 100, 1000],\n        },\n\n        \'SGDClassifier\': {\n            \'loss\': [\'hinge\', \'log\', \'modified_huber\', \'squared_hinge\', \'perceptron\', \'squared_loss\', \'huber\', \'epsilon_insensitive\', \'squared_epsilon_insensitive\'],\n            \'penalty\': [\'none\', \'l2\', \'l1\', \'elasticnet\'],\n            \'alpha\': [.0000001, .000001, .00001, .0001, .001],\n            \'learning_rate\': [\'constant\', \'optimal\', \'invscaling\'],\n            \'class_weight\': [\'balanced\', None]\n        },\n\n        \'Perceptron\': {\n            \'penalty\': [\'none\', \'l2\', \'l1\', \'elasticnet\'],\n            \'alpha\': [.0000001, .000001, .00001, .0001, .001],\n            \'class_weight\': [\'balanced\', None]\n        },\n\n        \'PassiveAggressiveClassifier\': {\n            \'loss\': [\'hinge\', \'squared_hinge\'],\n            \'class_weight\': [\'balanced\', None],\n            \'C\': [0.01, 0.3, 0.5, 0.7, 0.8, 0.9, 0.95, 0.99, 1.0]\n        }\n\n        , \'LGBMClassifier\': {\n            \'boosting_type\': [\'gbdt\', \'dart\']\n            , \'min_child_samples\': [1, 5, 7, 10, 15, 20, 35, 50, 100, 200, 500, 1000]\n            , \'num_leaves\': [2, 4, 7, 10, 15, 20, 25, 30, 35, 40, 50, 65, 80, 100, 125, 150, 200, 250]\n            , \'colsample_bytree\': [0.7, 0.9, 1.0]\n            , \'subsample\': [0.7, 0.9, 1.0]\n            , \'learning_rate\': [0.01, 0.05, 0.1]\n            , \'n_estimators\': [5, 20, 35, 50, 75, 100, 150, 200, 350, 500, 750, 1000]\n        }\n\n        , \'LGBMRegressor\': {\n            \'boosting_type\': [\'gbdt\', \'dart\']\n            , \'min_child_samples\': [1, 5, 7, 10, 15, 20, 35, 50, 100, 200, 500, 1000]\n            , \'num_leaves\': [2, 4, 7, 10, 15, 20, 25, 30, 35, 40, 50, 65, 80, 100, 125, 150, 200, 250]\n            , \'colsample_bytree\': [0.7, 0.9, 1.0]\n            , \'subsample\': [0.7, 0.9, 1.0]\n            , \'learning_rate\': [0.01, 0.05, 0.1]\n            , \'n_estimators\': [5, 20, 35, 50, 75, 100, 150, 200, 350, 500, 750, 1000]\n        }\n\n        , \'CatBoostClassifier\': {\n            \'depth\': [1, 2, 3, 5, 7, 9, 12, 15, 20, 32]\n            , \'l2_leaf_reg\': [.0000001, .000001, .00001, .0001, .001, .01, .1]\n            , \'learning_rate\': [0.01, 0.05, 0.1, 0.15, 0.2, 0.3]\n\n            # , random_strength\n            # , bagging_temperature\n        }\n\n        , \'CatBoostRegressor\': {\n            \'depth\': [1, 2, 3, 5, 7, 9, 12, 15, 20, 32]\n            , \'l2_leaf_reg\': [.0000001, .000001, .00001, .0001, .001, .01, .1]\n            , \'learning_rate\': [0.01, 0.05, 0.1, 0.15, 0.2, 0.3]\n\n            # , random_strength\n            # , bagging_temperature\n        }\n\n        , \'LinearSVR\': {\n            \'C\': [0.5, 0.75, 0.85, 0.95, 1.0]\n            , \'epsilon\': [0, 0.05, 0.1, 0.15, 0.2]\n        }\n\n        , \'LinearSVC\': {\n            \'C\': [0.5, 0.75, 0.85, 0.95, 1.0]\n        }\n\n    }\n\n    # Some of these are super expensive to compute. So if we\'re running this in a test suite, let\'s make sure the structure works, but reduce the compute time\n    params = grid_search_params[model_name]\n    if os.environ.get(\'is_test_suite\', 0) == \'True\' and model_name[:8] == \'CatBoost\':\n        simplified_params = {}\n        for k, v in params.items():\n            # Grab the first two items for each thing we want to test\n            simplified_params[k] = v[:2]\n        params = simplified_params\n\n    return params\n\n\ndef insert_deep_learning_model(pipeline_step, file_name):\n    # This is where we saved the random_name for this model\n    random_name = pipeline_step.model\n    # Load the Keras model here\n    keras_file_name = file_name[:-5] + random_name + \'_keras_deep_learning_model.h5\'\n\n    model = keras_load_model(keras_file_name)\n\n    # Put the model back in place so that we can still use it to get predictions without having to load it back in from disk\n    return model\n\ndef load_ml_model(file_name):\n\n    with open(file_name, \'rb\') as read_file:\n        base_pipeline = dill.load(read_file)\n\n    if isinstance(base_pipeline, utils_categorical_ensembling.CategoricalEnsembler):\n        for step in base_pipeline.transformation_pipeline.named_steps:\n            pipeline_step = base_pipeline.transformation_pipeline.named_steps[step]\n\n            try:\n                if pipeline_step.get(\'model_name\', \'reallylongnonsensicalstring\')[:12] == \'DeepLearning\':\n                    pipeline_step.model = insert_deep_learning_model(pipeline_step, file_name)\n            except AttributeError:\n                pass\n\n        for step in base_pipeline.trained_models:\n            pipeline_step = base_pipeline.trained_models[step]\n\n            try:\n                if pipeline_step.get(\'model_name\', \'reallylongnonsensicalstring\')[:12] == \'DeepLearning\':\n                    pipeline_step.model = insert_deep_learning_model(pipeline_step, file_name)\n            except AttributeError:\n                pass\n\n    else:\n\n        for step in base_pipeline.named_steps:\n            pipeline_step = base_pipeline.named_steps[step]\n            try:\n                if pipeline_step.get(\'model_name\', \'reallylongnonsensicalstring\')[:12] == \'DeepLearning\':\n                    pipeline_step.model = insert_deep_learning_model(pipeline_step, file_name)\n            except AttributeError:\n                pass\n\n    return base_pipeline\n\n# Keeping this here for legacy support\ndef load_keras_model(file_name):\n    return load_ml_model(file_name)\n\n# For many activations, we can just pass the activation name into Activations\n# For some others, we have to import them as their own standalone activation function\ndef get_activation_layer(activation):\n    if activation == \'LeakyReLU\':\n        return LeakyReLU()\n    if activation == \'PReLU\':\n        return PReLU()\n    if activation == \'ELU\':\n        return ELU()\n    if activation == \'ThresholdedReLU\':\n        return ThresholdedReLU()\n\n    return Activation(activation)\n\n# TODO: same for optimizers, including clipnorm\ndef get_optimizer(name=\'Adadelta\'):\n    if name == \'SGD\':\n        return optimizers.SGD(clipnorm=1.)\n    if name == \'RMSprop\':\n        return optimizers.RMSprop(clipnorm=1.)\n    if name == \'Adagrad\':\n        return optimizers.Adagrad(clipnorm=1.)\n    if name == \'Adadelta\':\n        return optimizers.Adadelta(clipnorm=1.)\n    if name == \'Adam\':\n        return optimizers.Adam(clipnorm=1.)\n    if name == \'Adamax\':\n        return optimizers.Adamax(clipnorm=1.)\n    if name == \'Nadam\':\n        return optimizers.Nadam(clipnorm=1.)\n\n    return optimizers.Adam(clipnorm=1.)\n\n\n\ndef make_deep_learning_model(hidden_layers=None, num_cols=None, optimizer=\'Adadelta\', dropout_rate=0.2, weight_constraint=0, feature_learning=False, kernel_initializer=\'normal\', activation=\'elu\'):\n\n    if feature_learning == True and hidden_layers is None:\n        hidden_layers = [1, 0.75, 0.25]\n\n    if hidden_layers is None:\n        hidden_layers = [1, 0.75, 0.25]\n\n    # The hidden_layers passed to us is simply describing a shape. it does not know the num_cols we are dealing with, it is simply values of 0.5, 1, and 2, which need to be multiplied by the num_cols\n    scaled_layers = []\n    for layer in hidden_layers:\n        scaled_layers.append(min(int(num_cols * layer), 10))\n\n    # If we\'re training this model for feature_learning, our penultimate layer (our final hidden layer before the ""output"" layer) will always have 10 neurons, meaning that we always output 10 features from our feature_learning model\n    if feature_learning == True:\n        scaled_layers.append(10)\n\n    model = Sequential()\n\n    model.add(Dense(scaled_layers[0], input_dim=num_cols, kernel_initializer=kernel_initializer, kernel_regularizer=regularizers.l2(0.01)))\n    model.add(get_activation_layer(activation))\n\n    for layer_size in scaled_layers[1:-1]:\n        model.add(Dense(layer_size, kernel_initializer=kernel_initializer, kernel_regularizer=regularizers.l2(0.01)))\n        model.add(get_activation_layer(activation))\n\n    # There are times we will want the output from our penultimate layer, not the final layer, so give it a name that makes the penultimate layer easy to find\n    model.add(Dense(scaled_layers[-1], kernel_initializer=kernel_initializer, name=\'penultimate_layer\', kernel_regularizer=regularizers.l2(0.01)))\n    model.add(get_activation_layer(activation))\n\n    # For regressors, we want an output layer with a single node\n    model.add(Dense(1, kernel_initializer=kernel_initializer))\n\n\n    # The final step is to compile the model\n    model.compile(loss=\'mean_squared_error\', optimizer=get_optimizer(optimizer), metrics=[\'mean_absolute_error\', \'mean_absolute_percentage_error\'])\n\n    return model\n\n\ndef make_deep_learning_classifier(hidden_layers=None, num_cols=None, optimizer=\'Adadelta\', dropout_rate=0.2, weight_constraint=0, final_activation=\'sigmoid\', feature_learning=False, activation=\'elu\', kernel_initializer=\'normal\'):\n\n    if feature_learning == True and hidden_layers is None:\n        hidden_layers = [1, 0.75, 0.25]\n\n    if hidden_layers is None:\n        hidden_layers = [1, 0.75, 0.25]\n\n    # The hidden_layers passed to us is simply describing a shape. it does not know the num_cols we are dealing with, it is simply values of 0.5, 1, and 2, which need to be multiplied by the num_cols\n    scaled_layers = []\n    for layer in hidden_layers:\n        scaled_layers.append(min(int(num_cols * layer), 10))\n\n    # If we\'re training this model for feature_learning, our penultimate layer (our final hidden layer before the ""output"" layer) will always have 10 neurons, meaning that we always output 10 features from our feature_learning model\n    if feature_learning == True:\n        scaled_layers.append(10)\n\n\n    model = Sequential()\n\n    # There are times we will want the output from our penultimate layer, not the final layer, so give it a name that makes the penultimate layer easy to find\n    model.add(Dense(scaled_layers[0], input_dim=num_cols, kernel_initializer=kernel_initializer, kernel_regularizer=regularizers.l2(0.01)))\n    model.add(get_activation_layer(activation))\n\n    for layer_size in scaled_layers[1:-1]:\n        model.add(Dense(layer_size, kernel_initializer=kernel_initializer, kernel_regularizer=regularizers.l2(0.01)))\n        model.add(get_activation_layer(activation))\n\n    model.add(Dense(scaled_layers[-1], kernel_initializer=kernel_initializer, name=\'penultimate_layer\', kernel_regularizer=regularizers.l2(0.01)))\n    model.add(get_activation_layer(activation))\n\n    model.add(Dense(1, kernel_initializer=kernel_initializer, activation=final_activation))\n    model.compile(loss=\'binary_crossentropy\', optimizer=get_optimizer(optimizer), metrics=[\'accuracy\', \'poisson\'])\n    return model\n'"
auto_ml/utils_scaling.py,0,"b""from sklearn.base import BaseEstimator, TransformerMixin\n\nfrom auto_ml import utils\n\nbooleans = set([True, False, 'true', 'false', 'True', 'False', 'TRUE', 'FALSE'])\n# Used in CustomSparseScaler\ndef calculate_scaling_ranges(X, col, min_percentile=0.05, max_percentile=0.95):\n\n    series_vals = X[col]\n    good_vals_indexes = series_vals.notnull()\n\n    series_vals = list(series_vals[good_vals_indexes])\n    series_vals = sorted(series_vals)\n\n    max_val_idx = int(max_percentile * len(series_vals)) - 1\n    min_val_idx = int(min_percentile * len(series_vals))\n\n    if len(series_vals) > 0:\n        max_val = series_vals[max_val_idx]\n        min_val = series_vals[min_val_idx]\n    else:\n        return 'ignore'\n\n    if max_val in booleans or min_val in booleans:\n        return 'pass_on_col'\n\n    inner_range = max_val - min_val\n\n    if inner_range == 0:\n        # Used to do recursion here, which is prettier and uses less code, but since we've already got the filtered and sorted series_vals, it makes sense to use those to avoid duplicate computation\n        # Grab the absolute largest max and min vals, and see if there is any difference in them, since our 95th and 5th percentile vals had no difference between them\n        max_val = series_vals[len(series_vals) - 1]\n        min_val = series_vals[0]\n        inner_range = max_val - min_val\n\n        if inner_range == 0:\n            # If this is a binary field, keep all the values in it, just make sure they're scaled to 1 or 0.\n            if max_val == 1:\n                min_val = 0\n                inner_range = 1\n            else:\n                # If this is just a column that holds all the same values for everything though, delete the column to save some space\n                return 'ignore'\n\n    col_summary = {\n        'max_val': max_val\n        , 'min_val': min_val\n        , 'inner_range': inner_range\n    }\n\n    return col_summary\n\n# Scale sparse data to the 95th and 5th percentile\n# Only do so for values that actuall exist (do absolutely nothing with rows that do not have this data point)\nclass CustomSparseScaler(BaseEstimator, TransformerMixin):\n\n\n    def __init__(self, column_descriptions, truncate_large_values=False, perform_feature_scaling=True, min_percentile=0.05, max_percentile=0.95):\n        self.column_descriptions = column_descriptions\n\n        self.numeric_col_descs = set([None, 'continuous', 'numerical', 'numeric', 'float', 'int'])\n        # Everything in column_descriptions (except numeric_col_descs) is a non-numeric column, and thus, cannot be scaled\n        self.cols_to_avoid = set([k for k, v in column_descriptions.items() if v not in self.numeric_col_descs])\n\n        # Setting these here so that they can be grid searchable\n        # Truncating large values is an interesting strategy. It forces all values to fit inside the 5th - 95th percentiles.\n        # Essentially, it turns any really large (or small) values into reasonably large (or small) values.\n        self.truncate_large_values = truncate_large_values\n        self.perform_feature_scaling = perform_feature_scaling\n        self.min_percentile = min_percentile\n        self.max_percentile = max_percentile\n\n\n    def get(self, prop_name, default=None):\n        try:\n            return getattr(self, prop_name)\n        except AttributeError:\n            return default\n\n\n    def fit(self, X, y=None):\n        print('Performing feature scaling')\n        self.column_ranges = {}\n        self.cols_to_ignore = []\n\n        if self.perform_feature_scaling:\n\n            for col in X.columns:\n                if col not in self.cols_to_avoid:\n                    col_summary = calculate_scaling_ranges(X, col, min_percentile=self.min_percentile, max_percentile=self.max_percentile)\n                    if col_summary == 'ignore':\n                        self.cols_to_ignore.append(col)\n                    elif col_summary == 'pass_on_col':\n                        pass\n                    else:\n                        self.column_ranges[col] = col_summary\n\n        return self\n\n\n    # Perform basic min/max scaling, with the minor caveat that our min and max values are the 10th and 90th percentile values, to avoid outliers.\n    def transform(self, X, y=None):\n\n        if isinstance(X, dict):\n            for col, col_dict in self.column_ranges.items():\n                if col in X:\n                    X[col] = scale_val(val=X[col], min_val=col_dict['min_val'], total_range=col_dict['inner_range'], truncate_large_values=self.truncate_large_values)\n        else:\n\n            if len(self.cols_to_ignore) > 0:\n                X = utils.safely_drop_columns(X, self.cols_to_ignore)\n\n            for col, col_dict in self.column_ranges.items():\n                if col in X.columns:\n                    min_val = col_dict['min_val']\n                    inner_range = col_dict['inner_range']\n                    X[col] = X[col].apply(lambda x: scale_val(x, min_val, inner_range, self.truncate_large_values))\n\n        return X\n\n\ndef scale_val(val, min_val, total_range, truncate_large_values=False):\n    scaled_value = (val - min_val) / total_range\n    if truncate_large_values:\n        if scaled_value < 0:\n            scaled_value = 0\n        elif scaled_value > 1:\n            scaled_value = 1\n\n    return scaled_value\n\n"""
auto_ml/utils_scoring.py,0,"b'from collections import OrderedDict\nimport math\n\nfrom auto_ml import utils\nimport pandas as pd\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nfrom sklearn.metrics import mean_squared_error, make_scorer, brier_score_loss, accuracy_score, explained_variance_score, mean_absolute_error, median_absolute_error, r2_score, log_loss, roc_auc_score\nimport numpy as np\nfrom tabulate import tabulate\n\nbad_vals_as_strings = set([str(float(\'nan\')), str(float(\'inf\')), str(float(\'-inf\')), \'None\', \'none\', \'NaN\', \'NAN\', \'nan\', \'NULL\', \'null\', \'\', \'inf\', \'-inf\', \'np.nan\', \'numpy.nan\'])\n\ndef advanced_scoring_classifiers(probas, actuals, name=None):\n    # pandas Series don\'t play nice here. Make sure our actuals list is indeed a list\n    actuals = list(actuals)\n    predictions = list(probas)\n\n    print(\'Here is our brier-score-loss, which is the default value we optimized for while training, and is the value returned from .score() unless you requested a custom scoring metric\')\n    print(\'It is a measure of how close the PROBABILITY predictions are.\')\n    if name != None:\n        print(name)\n\n    # Sometimes we will be given ""flattened"" probabilities (only the probability of our positive label), while other times we might be given ""nested"" probabilities (probabilities of both positive and negative, in a list, for each item).\n    try:\n        probas = [proba[1] for proba in probas]\n    except:\n        pass\n\n    brier_score = brier_score_loss(actuals, probas)\n    print(format(brier_score, \'.4f\'))\n\n\n    print(\'\\nHere is the trained estimator\\\'s overall accuracy (when it predicts a label, how frequently is that the correct label?)\')\n    predicted_labels = []\n    for pred in probas:\n        if pred >= 0.5:\n            predicted_labels.append(1)\n        else:\n            predicted_labels.append(0)\n    print(format(accuracy_score(y_true=actuals, y_pred=predicted_labels) * 100, \'.1f\') + \'%\')\n\n\n    print(\'\\nHere is a confusion matrix showing predictions vs. actuals by label:\')\n    #it would make sense to use sklearn\'s confusion_matrix here but it apparently has no labels\n    #took this idea instead from: http://stats.stackexchange.com/a/109015\n    conf = pd.crosstab(pd.Series(actuals), pd.Series(predicted_labels), rownames=[\'v Actual v\'], colnames=[\'Predicted >\'], margins=True)\n    print(conf)\n\n    #I like knowing the per class accuracy to see if the model is mishandling imbalanced data.\n    #For example, if it is predicting 100% of observations to one class just because it is the majority\n    #Wikipedia seems to call that Positive/negative predictive value\n    print(\'\\nHere is predictive value by class:\')\n    df = pd.concat([pd.Series(actuals,name=\'actuals\'),pd.Series(predicted_labels,name=\'predicted\')],axis=1)\n    targets = list(df.predicted.unique())\n    for i in range(0,len(targets)):\n        tot_count = len(df[df.predicted==targets[i]])\n        true_count = len(df[(df.predicted==targets[i]) & (df.actuals == targets[i])])\n        print(\'Class: \',targets[i],\'=\',float(true_count)/tot_count)\n\n\n    # qcut is super fickle. so, try to use 10 buckets first, then 5 if that fails, then nothing\n    try:\n        try:\n            bucket_results = pd.qcut(probas, q=10, duplicates=\'drop\')\n        except:\n            bucket_results = pd.qcut(probas, q=5, duplicates=\'drop\')\n\n        df_probas = pd.DataFrame(probas, columns=[\'Predicted Probability Of Bucket\'])\n        df_probas[\'Actual Probability of Bucket\'] = actuals\n        df_probas[\'Bucket Edges\'] = bucket_results\n\n        df_buckets = df_probas.groupby(df_probas[\'Bucket Edges\'])\n        try:\n            print(tabulate(df_buckets.mean(), headers=\'keys\', floatfmt=\'.4f\', tablefmt=\'psql\', showindex=\'always\'))\n        except TypeError:\n            print(tabulate(df_buckets.mean(), headers=\'keys\', floatfmt=\'.4f\', tablefmt=\'psql\'))\n        print(\'\\nHere is the accuracy of our trained estimator at each level of predicted probabilities\')\n        print(\'For a verbose description of what this means, please visit the docs:\')\n        print(\'http://auto-ml.readthedocs.io/en/latest/analytics.html#interpreting-predicted-probability-buckets-for-classifiers\')\n\n    except:\n        pass\n\n\n    print(\'\\n\\n\')\n    return brier_score\n\n\ndef calculate_and_print_differences(predictions, actuals, name=None):\n    pos_differences = []\n    neg_differences = []\n    # Technically, we\'re ignoring cases where we are spot on\n    for idx, pred in enumerate(predictions):\n        difference = pred - actuals[idx]\n        if difference > 0:\n            pos_differences.append(difference)\n        elif difference < 0:\n            neg_differences.append(difference)\n\n    if name != None:\n        print(name)\n    print(\'Count of positive differences (prediction > actual):\')\n    print(len(pos_differences))\n    print(\'Count of negative differences:\')\n    print(len(neg_differences))\n    if len(pos_differences) > 0:\n        print(\'Average positive difference:\')\n        print(sum(pos_differences) * 1.0 / len(pos_differences))\n    if len(neg_differences) > 0:\n        print(\'Average negative difference:\')\n        print(sum(neg_differences) * 1.0 / len(neg_differences))\n\n\ndef advanced_scoring_regressors(predictions, actuals, verbose=2, name=None):\n    # pandas Series don\'t play nice here. Make sure our actuals list is indeed a list\n    actuals = list(actuals)\n    predictions = list(predictions)\n\n    print(\'\\n\\n***********************************************\')\n    if name != None:\n        print(name)\n    print(\'Advanced scoring metrics for the trained regression model on this particular dataset:\\n\')\n\n    # 1. overall RMSE\n    print(\'Here is the overall RMSE for these predictions:\')\n    rmse = mean_squared_error(actuals, predictions)**0.5\n    print(rmse)\n\n    # 2. overall avg predictions\n    print(\'\\nHere is the average of the predictions:\')\n    print(sum(predictions) * 1.0 / len(predictions))\n\n    # 3. overall avg actuals\n    print(\'\\nHere is the average actual value on this validation set:\')\n    print(sum(actuals) * 1.0 / len(actuals))\n\n    # 2(a). median predictions\n    print(\'\\nHere is the median prediction:\')\n    print(np.median(predictions))\n\n    # 3(a). median actuals\n    print(\'\\nHere is the median actual value:\')\n    print(np.median(actuals))\n\n    # 4. avg differences (not RMSE)\n    print(\'\\nHere is the mean absolute error:\')\n    print(mean_absolute_error(actuals, predictions))\n\n    print(\'\\nHere is the median absolute error (robust to outliers):\')\n    print(median_absolute_error(actuals, predictions))\n\n    print(\'\\nHere is the explained variance:\')\n    print(explained_variance_score(actuals, predictions))\n\n    print(\'\\nHere is the R-squared value:\')\n    print(r2_score(actuals, predictions))\n\n    # 5. pos and neg differences\n    calculate_and_print_differences(predictions=predictions, actuals=actuals, name=name)\n\n    actuals_preds = list(zip(actuals, predictions))\n    # Sort by PREDICTED value, since this is what what we will know at the time we make a prediction\n    actuals_preds.sort(key=lambda pair: pair[1])\n    actuals_sorted = [act for act, pred in actuals_preds]\n    predictions_sorted = [pred for act, pred in actuals_preds]\n\n    if verbose > 2:\n        print(\'Here\\\'s how the trained predictor did on each successive decile (ten percent chunk) of the predictions:\')\n        for i in range(1,11):\n            print(\'\\n**************\')\n            print(\'Bucket number:\')\n            print(i)\n            # There\'s probably some fenceposting error here\n            min_idx = int((i - 1) / 10.0 * len(actuals_sorted))\n            max_idx = int(i / 10.0 * len(actuals_sorted))\n            actuals_for_this_decile = actuals_sorted[min_idx:max_idx]\n            predictions_for_this_decile = predictions_sorted[min_idx:max_idx]\n\n            print(\'Avg predicted val in this bucket\')\n            print(sum(predictions_for_this_decile) * 1.0 / len(predictions_for_this_decile))\n            print(\'Avg actual val in this bucket\')\n            print(sum(actuals_for_this_decile) * 1.0 / len(actuals_for_this_decile))\n            print(\'RMSE for this bucket\')\n            print(mean_squared_error(actuals_for_this_decile, predictions_for_this_decile)**0.5)\n            calculate_and_print_differences(predictions_for_this_decile, actuals_for_this_decile)\n\n    print(\'\')\n    print(\'\\n***********************************************\\n\\n\')\n    return rmse\n\ndef rmse_func(y, predictions):\n    return mean_squared_error(y, predictions)**0.5\n\n\nscoring_name_function_map = {\n    \'rmse\': rmse_func\n    , \'median_absolute_error\': median_absolute_error\n    , \'r2\': r2_score\n    , \'r-squared\': r2_score\n    , \'mean_absolute_error\': mean_absolute_error\n    , \'accuracy\': accuracy_score\n    , \'accuracy_score\': accuracy_score\n    , \'log_loss\': log_loss\n    , \'roc_auc\': roc_auc_score\n    , \'brier_score_loss\': brier_score_loss\n}\n\n\nclass RegressionScorer(object):\n\n    def __init__(self, scoring_method=None):\n\n        if scoring_method is None:\n            scoring_method = \'rmse\'\n\n        self.scoring_method = scoring_method\n\n        if callable(scoring_method):\n            self.scoring_func = scoring_method\n        else:\n            self.scoring_func = scoring_name_function_map[scoring_method]\n\n        self.scoring_method = scoring_method\n\n\n    def get(self, prop_name, default=None):\n        try:\n            return getattr(self, prop_name)\n        except AttributeError:\n            return default\n\n\n    def score(self, estimator, X, y, took_log_of_y=False, advanced_scoring=False, verbose=2, name=None):\n        X, y = utils.drop_missing_y_vals(X, y, output_column=None)\n\n        if isinstance(estimator, GradientBoostingRegressor):\n            X = X.toarray()\n\n        predictions = estimator.predict(X)\n\n        if took_log_of_y:\n            for idx, val in enumerate(predictions):\n                predictions[idx] = math.exp(val)\n\n        try:\n            score = self.scoring_func(y, predictions)\n        except ValueError:\n\n            bad_val_indices = []\n            for idx, val in enumerate(y):\n                if str(val) in bad_vals_as_strings or str(predictions[idx]) in bad_vals_as_strings:\n                    bad_val_indices.append(idx)\n\n            predictions = [val for idx, val in enumerate(predictions) if idx not in bad_val_indices]\n            y = [val for idx, val in enumerate(y) if idx not in bad_val_indices]\n\n            print(\'Found \' + str(len(bad_val_indices)) + \' null or infinity values in the predicted or y values. We will ignore these, and report the score on the rest of the dataset\')\n            score = self.scoring_func(y, predictions)\n\n        if advanced_scoring == True:\n            if hasattr(estimator, \'name\'):\n                print(estimator.name)\n            advanced_scoring_regressors(predictions, y, verbose=verbose, name=name)\n        return - 1 * score\n\n\nclass ClassificationScorer(object):\n\n    def __init__(self, scoring_method=None):\n\n        if scoring_method is None:\n            scoring_method = \'brier_score_loss\'\n\n        self.scoring_method = scoring_method\n\n        if callable(scoring_method):\n            self.scoring_func = scoring_method\n        else:\n            self.scoring_func = scoring_name_function_map[scoring_method]\n\n\n    def get(self, prop_name, default=None):\n        try:\n            return getattr(self, prop_name)\n        except AttributeError:\n            return default\n\n\n    def clean_probas(self, probas):\n        print(\'Warning: We have found some values in the predicted probabilities that fall outside the range {0, 1}\')\n        print(\'This is likely the result of a model being trained on too little data, or with a bad set of hyperparameters. If you get this warning while doing a hyperparameter search, for instance, you can probably safely ignore it\')\n        print(\'We will cap those values at 0 or 1 for the purposes of scoring, but you should be careful to have similar safeguards in place in prod if you use this model\')\n        if not isinstance(probas[0], list):\n            probas = [val if str(val) not in bad_vals_as_strings else 0 for val in probas]\n            probas = [min(max(pred, 0), 1) for pred in probas]\n            return probas\n        else:\n            cleaned_probas = []\n            for proba_tuple in probas:\n                cleaned_tuple = []\n                for item in proba_tuple:\n                    if str(item) in bad_vals_as_strings:\n                        item = 0\n                    cleaned_tuple.append(max(min(item, 1), 0))\n                cleaned_probas.append(cleaned_tuple)\n            return cleaned_probas\n\n\n\n    def score(self, estimator, X, y, advanced_scoring=False):\n\n        X, y = utils.drop_missing_y_vals(X, y, output_column=None)\n\n        if isinstance(estimator, GradientBoostingClassifier):\n            X = X.toarray()\n\n        predictions = estimator.predict_proba(X)\n\n\n        if self.scoring_method == \'brier_score_loss\':\n            # At the moment, Microsoft\'s LightGBM returns probabilities > 1 and < 0, which can break some scoring functions. So we have to take the max of 1 and the pred, and the min of 0 and the pred.\n            probas = [max(min(row[1], 1), 0) for row in predictions]\n            predictions = probas\n\n        try:\n            score = self.scoring_func(y, predictions)\n        except ValueError as e:\n            bad_val_indices = []\n            for idx, val in enumerate(y):\n                if str(val) in bad_vals_as_strings:\n                    bad_val_indices.append(idx)\n\n            predictions = [val for idx, val in enumerate(predictions) if idx not in bad_val_indices]\n            y = [val for idx, val in enumerate(y) if idx not in bad_val_indices]\n\n            print(\'Found \' + str(len(bad_val_indices)) + \' null or infinity values in the y values. We will ignore these, and report the score on the rest of the dataset\')\n            try:\n                score = self.scoring_func(y, predictions)\n            except ValueError:\n                # Sometimes, particularly for a badly fit model using either too little data, or a really bad set of hyperparameters during a grid search, we can predict probas that are > 1 or < 0. We\'ll cap those here, while warning the user about them, because they\'re unlikely to occur in a model that\'s properly trained with enough data and reasonable params\n                predictions = self.clean_probas(predictions)\n                score = self.scoring_func(y, predictions)\n\n\n        if advanced_scoring:\n            return (-1 * score, predictions)\n        else:\n            return -1 * score\n'"
tests/utils_testing.py,0,"b""import sys, os\nsys.path = [os.path.abspath(os.path.dirname(__file__))] + sys.path\nos.environ['is_test_suite'] = 'True'\n\nimport pandas as pd\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import brier_score_loss, mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nfrom auto_ml import Predictor\n\ndef get_boston_regression_dataset():\n    boston = load_boston()\n    df_boston = pd.DataFrame(boston.data)\n    df_boston.columns = boston.feature_names\n    df_boston['MEDV'] = boston['target']\n    df_boston_train, df_boston_test = train_test_split(df_boston, test_size=0.33, random_state=42)\n    return df_boston_train, df_boston_test\n\ndef get_titanic_binary_classification_dataset(basic=True):\n\n    dir_name = os.path.abspath(os.path.dirname(__file__))\n    file_name = os.path.join(dir_name, 'titanic.csv')\n    print('file_name')\n    print(file_name)\n    print('dir_name')\n    print(dir_name)\n    try:\n        df_titanic = pd.read_csv(file_name)\n    except Exception as e:\n        print('Error')\n        print(e)\n        dataset_url = 'http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.csv'\n        df_titanic = pd.read_csv(dataset_url)\n        # Do not write the index that pandas automatically creates\n        df_titanic.to_csv(file_name, index=False)\n\n    df_titanic = df_titanic.drop(['boat', 'body'], axis=1)\n\n    if basic == True:\n        df_titanic = df_titanic.drop(['name', 'ticket', 'cabin', 'home.dest'], axis=1)\n\n    df_titanic_train, df_titanic_test = train_test_split(df_titanic, test_size=0.33, random_state=42)\n    return df_titanic_train, df_titanic_test\n\n\ndef train_basic_binary_classifier(df_titanic_train):\n    column_descriptions = {\n        'survived': 'output'\n        , 'sex': 'categorical'\n        , 'embarked': 'categorical'\n        , 'pclass': 'categorical'\n    }\n\n    ml_predictor = Predictor(type_of_estimator='classifier', column_descriptions=column_descriptions)\n    ml_predictor.train(df_titanic_train)\n\n    return ml_predictor\n\n\ndef train_basic_regressor(df_boston_train):\n    column_descriptions = {\n        'MEDV': 'output'\n        , 'CHAS': 'categorical'\n    }\n\n    ml_predictor = Predictor(type_of_estimator='regressor', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_boston_train, verbose=False)\n    return ml_predictor\n\ndef calculate_rmse(actuals, preds):\n    return mean_squared_error(actuals, preds)**0.5 * -1\n\ndef calculate_brier_score_loss(actuals, probas):\n    return -1 * brier_score_loss(actuals, probas)\n\n\n\ndef get_twitter_sentiment_multilabel_classification_dataset():\n\n    file_name = os.path.join('tests', 'twitter_sentiment.h5')\n\n    try:\n        df_twitter = pd.read_hdf(file_name)\n    except Exception as e:\n        print('Error')\n        print(e)\n        dataset_url = 'https://raw.githubusercontent.com/ClimbsRocks/sample_datasets/master/twitter_airline_sentiment.csv'\n        df_twitter = pd.read_csv(dataset_url, encoding='latin-1')\n        # Do not write the index that pandas automatically creates\n\n        df_twitter.to_hdf(file_name, key='df', format='fixed')\n\n    # Grab only 10% of the dataset- runs much faster this way\n    df_twitter = df_twitter.sample(frac=0.1)\n\n    df_twitter['tweet_created'] = pd.to_datetime(df_twitter.tweet_created)\n\n    df_twitter_train, df_twitter_test = train_test_split(df_twitter, test_size=0.33, random_state=42)\n    return df_twitter_train, df_twitter_test\n\n\ndef train_basic_multilabel_classifier(df_twitter_train):\n    column_descriptions = {\n        'airline_sentiment': 'output'\n        , 'airline': 'categorical'\n        , 'text': 'ignore'\n        , 'tweet_location': 'categorical'\n        , 'user_timezone': 'categorical'\n        , 'tweet_created': 'date'\n    }\n\n    ml_predictor = Predictor(type_of_estimator='classifier', column_descriptions=column_descriptions)\n    ml_predictor.train(df_twitter_train)\n\n    return ml_predictor\n\n\nimport pandas as pd\nimport datetime\ndef make_test_df():\n    today = datetime.datetime.today()\n    raw_input = {\n        'a': [1,2,3,4,5]\n        , 'b': [6,7,8,9,10]\n        , 'text_col': ['hi', 'there', 'mesmerizingly', 'intriguing', 'world']\n        , 'date_col': [today, today - datetime.timedelta(days=1), today - datetime.timedelta(days=2), today - datetime.timedelta(days=3), today - datetime.timedelta(days=4)]\n    }\n    df = pd.DataFrame(raw_input)\n    return df\n"""
docs/source/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# auto_ml documentation build configuration file, created by\n# sphinx-quickstart on Sun Aug  7 20:25:48 2016.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = []\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n#\n# source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'auto_ml\'\ncopyright = u\'2016, Preston Parry\'\nauthor = u\'Preston Parry\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = u\'0.1.0\'\n# The full version, including alpha/beta/rc tags.\nrelease = u\'0.1.0\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#\n# today = \'\'\n#\n# Else, today_fmt is used as the format for a strftime call.\n#\n# today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = []\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#\n# default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n#\n# add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#\n# add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#\n# show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n# modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n# keep_warnings = False\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n\n# Use read the docs theme locally, but not when deployed, to avoid confusing their own built in process.\n# on_rtd is whether we are on readthedocs.org, this line of code grabbed from docs.readthedocs.org\non_rtd = os.environ.get(\'READTHEDOCS\', None) == \'True\'\n\nif not on_rtd:  # only import and set the theme if we\'re building docs locally\n    import sphinx_rtd_theme\n    html_theme = \'sphinx_rtd_theme\'\n    html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# otherwise, readthedocs.org uses their theme by default, so no need to specify it\n\n# html_theme = \'alabaster\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n# html_theme_path = []\n\n# The name for this set of Sphinx documents.\n# ""<project> v<release> documentation"" by default.\n#\n# html_title = u\'auto_ml v0.1.0\'\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#\n# html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#\n# html_logo = None\n\n# The name of an image file (relative to this directory) to use as a favicon of\n# the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#\n# html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\n# html_static_path = [\'_static\']\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#\n# html_extra_path = []\n\n# If not None, a \'Last updated on:\' timestamp is inserted at every page\n# bottom, using the given strftime format.\n# The empty string is equivalent to \'%b %d, %Y\'.\n#\n# html_last_updated_fmt = None\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#\n# html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#\n# html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#\n# html_additional_pages = {}\n\n# If false, no module index is generated.\n#\n# html_domain_indices = True\n\n# If false, no index is generated.\n#\n# html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#\n# html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#\n# html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n#\n# html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n#\n# html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#\n# html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n# html_file_suffix = None\n\n# Language to be used for generating the HTML full-text search index.\n# Sphinx supports the following languages:\n#   \'da\', \'de\', \'en\', \'es\', \'fi\', \'fr\', \'hu\', \'it\', \'ja\'\n#   \'nl\', \'no\', \'pt\', \'ro\', \'ru\', \'sv\', \'tr\', \'zh\'\n#\n# html_search_language = \'en\'\n\n# A dictionary with options for the search language support, empty by default.\n# \'ja\' uses this config value.\n# \'zh\' user can custom change `jieba` dictionary path.\n#\n# html_search_options = {\'type\': \'default\'}\n\n# The name of a javascript file (relative to the configuration directory) that\n# implements a search results scorer. If empty, the default will be used.\n#\n# html_search_scorer = \'scorer.js\'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'auto_mldoc\'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n     # The paper size (\'letterpaper\' or \'a4paper\').\n     #\n     # \'papersize\': \'letterpaper\',\n\n     # The font size (\'10pt\', \'11pt\' or \'12pt\').\n     #\n     # \'pointsize\': \'10pt\',\n\n     # Additional stuff for the LaTeX preamble.\n     #\n     # \'preamble\': \'\',\n\n     # Latex figure (float) alignment\n     #\n     # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'auto_ml.tex\', u\'auto\\\\_ml Documentation\',\n     u\'Preston Parry\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#\n# latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#\n# latex_use_parts = False\n\n# If true, show page references after internal links.\n#\n# latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#\n# latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#\n# latex_appendices = []\n\n# It false, will not define \\strong, \\code, \titleref, \\crossref ... but only\n# \\sphinxstrong, ..., \\sphinxtitleref, ... To help avoid clash with user added\n# packages.\n#\n# latex_keep_old_macro_names = True\n\n# If false, no module index is generated.\n#\n# latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'auto_ml\', u\'auto_ml Documentation\',\n     [author], 1)\n]\n\n# If true, show URL addresses after external links.\n#\n# man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'auto_ml\', u\'auto_ml Documentation\',\n     author, \'auto_ml\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n# Documents to append as an appendix to all manuals.\n#\n# texinfo_appendices = []\n\n# If false, no module index is generated.\n#\n# texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#\n# texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n#\n# texinfo_no_detailmenu = False\n'"
tests/advanced_tests/advanced_install_tests.py,0,"b'import datetime\nimport os\nimport random\nimport sys\nimport warnings\nsys.path = [os.path.abspath(os.path.dirname(__file__))] + sys.path\nos.environ[\'is_test_suite\'] = \'True\'\n\nfrom auto_ml import Predictor\nfrom auto_ml.utils_models import load_ml_model\n\nimport dill\nimport numpy as np\nfrom nose.tools import assert_equal, assert_not_equal, with_setup\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nimport utils_testing as utils\n\n\ndef test_feature_learning_getting_single_predictions_classification(model_name=None):\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    # NOTE: this is bad practice to pass in our same training set as our fl_data set, but we don\'t have enough data to do it any other way\n    df_titanic_train, fl_data = train_test_split(df_titanic_train, test_size=0.2)\n    ml_predictor.train(df_titanic_train, model_names=model_name, feature_learning=True, fl_data=fl_data)\n\n    file_name = ml_predictor.save(str(random.random()))\n\n    saved_ml_pipeline = load_ml_model(file_name)\n\n    os.remove(file_name)\n    try:\n        keras_file_name = file_name[:-5] + \'_keras_deep_learning_model.h5\'\n        os.remove(keras_file_name)\n    except:\n        pass\n\n\n    df_titanic_test_dictionaries = df_titanic_test.to_dict(\'records\')\n\n    # 1. make sure the accuracy is the same\n\n    predictions = []\n    for row in df_titanic_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict_proba(row)[1])\n\n    print(\'predictions\')\n    print(predictions)\n\n    first_score = utils.calculate_brier_score_loss(df_titanic_test.survived, predictions)\n    print(\'first_score\')\n    print(first_score)\n    # Make sure our score is good, but not unreasonably good\n\n    lower_bound = -0.16\n    if model_name == \'DeepLearningClassifier\':\n        lower_bound = -0.187\n\n    assert lower_bound < first_score < -0.133\n\n    # 2. make sure the speed is reasonable (do it a few extra times)\n    data_length = len(df_titanic_test_dictionaries)\n    start_time = datetime.datetime.now()\n    for idx in range(1000):\n        row_num = idx % data_length\n        saved_ml_pipeline.predict(df_titanic_test_dictionaries[row_num])\n    end_time = datetime.datetime.now()\n    duration = end_time - start_time\n\n    print(\'duration.total_seconds()\')\n    print(duration.total_seconds())\n\n    # It\'s very difficult to set a benchmark for speed that will work across all machines.\n    # On my 2013 bottom of the line 15"" MacBook Pro, this runs in about 0.8 seconds for 1000 predictions\n    # That\'s about 1 millisecond per prediction\n    # Assuming we might be running on a test box that\'s pretty weak, multiply by 3\n    # Also make sure we\'re not running unreasonably quickly\n    assert 0.2 < duration.total_seconds() < 15\n\n\n    # 3. make sure we\'re not modifying the dictionaries (the score is the same after running a few experiments as it is the first time)\n\n    predictions = []\n    for row in df_titanic_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict_proba(row)[1])\n\n    print(\'predictions\')\n    print(predictions)\n    print(\'df_titanic_test_dictionaries\')\n    print(df_titanic_test_dictionaries)\n    second_score = utils.calculate_brier_score_loss(df_titanic_test.survived, predictions)\n    print(\'second_score\')\n    print(second_score)\n    # Make sure our score is good, but not unreasonably good\n\n    assert lower_bound < second_score < -0.133\n\n\ndef test_feature_learning_categorical_ensembling_getting_single_predictions_classification(model_name=None):\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    # NOTE: this is bad practice to pass in our same training set as our fl_data set, but we don\'t have enough data to do it any other way\n    df_titanic_train, fl_data = train_test_split(df_titanic_train, test_size=0.2)\n    ml_predictor.train_categorical_ensemble(df_titanic_train, model_names=model_name, feature_learning=True, fl_data=fl_data, categorical_column=\'embarked\')\n\n    file_name = ml_predictor.save(str(random.random()))\n\n    from auto_ml.utils_models import load_ml_model\n\n    saved_ml_pipeline = load_ml_model(file_name)\n\n    os.remove(file_name)\n    try:\n        keras_file_name = file_name[:-5] + \'_keras_deep_learning_model.h5\'\n        os.remove(keras_file_name)\n    except:\n        pass\n\n\n    df_titanic_test_dictionaries = df_titanic_test.to_dict(\'records\')\n\n    # 1. make sure the accuracy is the same\n\n    predictions = []\n    for row in df_titanic_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict_proba(row)[1])\n\n    print(\'predictions\')\n    print(predictions)\n\n    first_score = utils.calculate_brier_score_loss(df_titanic_test.survived, predictions)\n    print(\'first_score\')\n    print(first_score)\n    # Make sure our score is good, but not unreasonably good\n\n    lower_bound = -0.17\n    if model_name == \'DeepLearningClassifier\':\n        lower_bound = -0.245\n    if model_name == \'CatBoostClassifier\':\n        lower_bound = -0.265\n\n    assert lower_bound < first_score < -0.140\n\n    # 2. make sure the speed is reasonable (do it a few extra times)\n    data_length = len(df_titanic_test_dictionaries)\n    start_time = datetime.datetime.now()\n    for idx in range(1000):\n        row_num = idx % data_length\n        saved_ml_pipeline.predict(df_titanic_test_dictionaries[row_num])\n    end_time = datetime.datetime.now()\n    duration = end_time - start_time\n\n    print(\'duration.total_seconds()\')\n    print(duration.total_seconds())\n\n    # It\'s very difficult to set a benchmark for speed that will work across all machines.\n    # On my 2013 bottom of the line 15"" MacBook Pro, this runs in about 0.8 seconds for 1000 predictions\n    # That\'s about 1 millisecond per prediction\n    # Assuming we might be running on a test box that\'s pretty weak, multiply by 3\n    # Also make sure we\'re not running unreasonably quickly\n    assert 0.2 < duration.total_seconds() < 15\n\n\n    # 3. make sure we\'re not modifying the dictionaries (the score is the same after running a few experiments as it is the first time)\n\n    predictions = []\n    for row in df_titanic_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict_proba(row)[1])\n\n    print(\'predictions\')\n    print(predictions)\n    print(\'df_titanic_test_dictionaries\')\n    print(df_titanic_test_dictionaries)\n    second_score = utils.calculate_brier_score_loss(df_titanic_test.survived, predictions)\n    print(\'second_score\')\n    print(second_score)\n    # Make sure our score is good, but not unreasonably good\n\n    assert lower_bound < second_score < -0.147\n\n\ndef test_feature_learning_getting_single_predictions_regression(model_name=None):\n    np.random.seed(0)\n\n    df_boston_train, df_boston_test = utils.get_boston_regression_dataset()\n\n    column_descriptions = {\n        \'MEDV\': \'output\'\n        , \'CHAS\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'regressor\', column_descriptions=column_descriptions)\n\n    # NOTE: this is bad practice to pass in our same training set as our fl_data set, but we don\'t have enough data to do it any other way\n    df_boston_train, fl_data = train_test_split(df_boston_train, test_size=0.2)\n    ml_predictor.train(df_boston_train, model_names=model_name, feature_learning=True, fl_data=fl_data)\n\n    file_name = ml_predictor.save(str(random.random()))\n\n    # from auto_ml.utils_models import load_keras_model\n\n    # saved_ml_pipeline = load_keras_model(file_name)\n\n    saved_ml_pipeline = load_ml_model(file_name)\n\n    os.remove(file_name)\n    try:\n        keras_file_name = file_name[:-5] + \'_keras_deep_learning_model.h5\'\n        os.remove(keras_file_name)\n    except:\n        pass\n\n\n\n    df_boston_test_dictionaries = df_boston_test.to_dict(\'records\')\n\n    # 1. make sure the accuracy is the same\n\n    predictions = []\n    for row in df_boston_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict(row))\n\n    first_score = utils.calculate_rmse(df_boston_test.MEDV, predictions)\n    print(\'first_score\')\n    print(first_score)\n    # Make sure our score is good, but not unreasonably good\n\n    lower_bound = -4.0\n\n    assert lower_bound < first_score < -2.8\n\n    # 2. make sure the speed is reasonable (do it a few extra times)\n    data_length = len(df_boston_test_dictionaries)\n    start_time = datetime.datetime.now()\n    for idx in range(1000):\n        row_num = idx % data_length\n        saved_ml_pipeline.predict(df_boston_test_dictionaries[row_num])\n    end_time = datetime.datetime.now()\n    duration = end_time - start_time\n\n    print(\'duration.total_seconds()\')\n    print(duration.total_seconds())\n\n    # It\'s very difficult to set a benchmark for speed that will work across all machines.\n    # On my 2013 bottom of the line 15"" MacBook Pro, this runs in about 0.8 seconds for 1000 predictions\n    # That\'s about 1 millisecond per prediction\n    # Assuming we might be running on a test box that\'s pretty weak, multiply by 3\n    # Also make sure we\'re not running unreasonably quickly\n    assert 0.2 < duration.total_seconds() / 1.0 < 15\n\n\n    # 3. make sure we\'re not modifying the dictionaries (the score is the same after running a few experiments as it is the first time)\n\n    predictions = []\n    for row in df_boston_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict(row))\n\n    second_score = utils.calculate_rmse(df_boston_test.MEDV, predictions)\n    print(\'second_score\')\n    print(second_score)\n    # Make sure our score is good, but not unreasonably good\n\n    assert lower_bound < second_score < -2.8\n\n\ndef test_feature_learning_categorical_ensembling_getting_single_predictions_regression(model_name=None):\n    np.random.seed(0)\n\n    df_boston_train, df_boston_test = utils.get_boston_regression_dataset()\n\n    column_descriptions = {\n        \'MEDV\': \'output\'\n        , \'CHAS\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'regressor\', column_descriptions=column_descriptions)\n\n    # NOTE: this is bad practice to pass in our same training set as our fl_data set, but we don\'t have enough data to do it any other way\n    df_boston_train, fl_data = train_test_split(df_boston_train, test_size=0.2)\n    ml_predictor.train_categorical_ensemble(df_boston_train, model_names=model_name, feature_learning=True, fl_data=fl_data, categorical_column=\'CHAS\')\n\n    # print(\'Score on training data\')\n    # ml_predictor.score(df_boston_train, df_boston_train.MEDV)\n\n    file_name = ml_predictor.save(str(random.random()))\n\n    from auto_ml.utils_models import load_ml_model\n\n    saved_ml_pipeline = load_ml_model(file_name)\n\n    # with open(file_name, \'rb\') as read_file:\n    #     saved_ml_pipeline = dill.load(read_file)\n    os.remove(file_name)\n    try:\n        keras_file_name = file_name[:-5] + \'_keras_deep_learning_model.h5\'\n        os.remove(keras_file_name)\n    except:\n        pass\n\n\n\n    df_boston_test_dictionaries = df_boston_test.to_dict(\'records\')\n\n    # 1. make sure the accuracy is the same\n\n    predictions = []\n    for row in df_boston_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict(row))\n\n    first_score = utils.calculate_rmse(df_boston_test.MEDV, predictions)\n    print(\'first_score\')\n    print(first_score)\n    # Make sure our score is good, but not unreasonably good\n\n    lower_bound = -4.5\n\n    assert lower_bound < first_score < -3.4\n\n    # 2. make sure the speed is reasonable (do it a few extra times)\n    data_length = len(df_boston_test_dictionaries)\n    start_time = datetime.datetime.now()\n    for idx in range(1000):\n        row_num = idx % data_length\n        saved_ml_pipeline.predict(df_boston_test_dictionaries[row_num])\n    end_time = datetime.datetime.now()\n    duration = end_time - start_time\n\n    print(\'duration.total_seconds()\')\n    print(duration.total_seconds())\n\n    # It\'s very difficult to set a benchmark for speed that will work across all machines.\n    # On my 2013 bottom of the line 15"" MacBook Pro, this runs in about 0.8 seconds for 1000 predictions\n    # That\'s about 1 millisecond per prediction\n    # Assuming we might be running on a test box that\'s pretty weak, multiply by 3\n    # Also make sure we\'re not running unreasonably quickly\n    assert 0.2 < duration.total_seconds() / 1.0 < 15\n\n\n    # 3. make sure we\'re not modifying the dictionaries (the score is the same after running a few experiments as it is the first time)\n\n    predictions = []\n    for row in df_boston_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict(row))\n\n    second_score = utils.calculate_rmse(df_boston_test.MEDV, predictions)\n    print(\'second_score\')\n    print(second_score)\n    # Make sure our score is good, but not unreasonably good\n\n    assert lower_bound < second_score < -3.4\n\n\ndef test_all_algos_classification(model_name=None):\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_titanic_train, model_names=[\'LogisticRegression\', \'RandomForestClassifier\', \'RidgeClassifier\', \'GradientBoostingClassifier\', \'ExtraTreesClassifier\', \'AdaBoostClassifier\', \'SGDClassifier\', \'Perceptron\', \'PassiveAggressiveClassifier\', \'DeepLearningClassifier\', \'XGBClassifier\', \'LGBMClassifier\', \'LinearSVC\'])\n\n    test_score = ml_predictor.score(df_titanic_test, df_titanic_test.survived)\n\n    print(\'test_score\')\n    print(test_score)\n\n    # Linear models aren\'t super great on this dataset...\n    assert -0.215 < test_score < -0.131\n\ndef test_all_algos_regression():\n    # a random seed of 42 has ExtraTreesRegressor getting the best CV score, and that model doesn\'t generalize as well as GradientBoostingRegressor.\n    np.random.seed(0)\n\n    df_boston_train, df_boston_test = utils.get_boston_regression_dataset()\n\n    column_descriptions = {\n        \'MEDV\': \'output\'\n        , \'CHAS\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'regressor\', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_boston_train, model_names=[\'LinearRegression\', \'RandomForestRegressor\', \'Ridge\', \'GradientBoostingRegressor\', \'AdaBoostRegressor\', \'SGDRegressor\', \'PassiveAggressiveRegressor\', \'Lasso\', \'LassoLars\', \'ElasticNet\', \'OrthogonalMatchingPursuit\', \'BayesianRidge\', \'ARDRegression\', \'MiniBatchKMeans\', \'DeepLearningRegressor\', \'LGBMRegressor\', \'XGBClassifier\',  \'LinearSVR\', \'CatBoostRegressor\'])\n\n    test_score = ml_predictor.score(df_boston_test, df_boston_test.MEDV)\n\n    print(\'test_score\')\n    print(test_score)\n\n    assert -3.4 < test_score < -2.8\n\ndef test_throws_warning_when_fl_data_equals_df_train():\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    with warnings.catch_warnings(record=True) as w:\n\n        try:\n            ml_predictor.train(df_titanic_train, feature_learning=True, fl_data=df_titanic_train)\n        except KeyError as e:\n            pass\n        # We should not be getting to this line- we should be throwing an error above\n        for thing in w:\n            print(thing)\n        assert len(w) >= 1\n    assert True\n\n'"
tests/advanced_tests/automated_tests.py,0,"b""from collections import OrderedDict\nimport os\nimport sys\nsys.path = [os.path.abspath(os.path.dirname(__file__))] + sys.path\nos.environ['is_test_suite'] = 'True'\n\nimport classifiers as classifier_tests\nimport regressors as regressor_tests\n\n\ntraining_parameters = {\n    'model_names': ['DeepLearning', 'GradientBoosting', 'XGB', 'LGBM', 'CatBoost']\n}\n\n\n# Make this an OrderedDict so that we run the tests in a consistent order\ntest_names = OrderedDict([\n    ('getting_single_predictions_multilabel_classification', classifier_tests.getting_single_predictions_multilabel_classification),\n    # ('getting_single_predictions_classification', classifier_tests.getting_single_predictions_classification),\n    ('optimize_final_model_classification', classifier_tests.optimize_final_model_classification)\n    # ('feature_learning_getting_single_predictions_classification', classifier_tests.feature_learning_getting_single_predictions_classification),\n    # ('categorical_ensembling_classification', classifier_tests.categorical_ensembling_classification),\n    # ('feature_learning_categorical_ensembling_getting_single_predictions_classification', classifier_tests.feature_learning_categorical_ensembling_getting_single_predictions_classification)\n])\n\n\ndef test_generator():\n    for model_name in training_parameters['model_names']:\n        for test_name, test in test_names.items():\n            test_model_name = model_name + 'Classifier'\n            # test_model_name = model_name\n\n            test.description = str(test_model_name) + '_' + test_name\n            yield test, test_model_name\n"""
tests/advanced_tests/automated_tests_regressors.py,0,"b""# Splitting this in half to see if it helps with how nosetests splits up our parallel tests\nfrom collections import OrderedDict\nimport os\nimport sys\nsys.path = [os.path.abspath(os.path.dirname(__file__))] + sys.path\nos.environ['is_test_suite'] = 'True'\n\nimport classifiers as classifier_tests\nimport regressors as regressor_tests\n\n\ntraining_parameters = {\n    'model_names': ['DeepLearning', 'GradientBoosting', 'XGB', 'LGBM', 'CatBoost']\n}\n\n\n# Make this an OrderedDict so that we run the tests in a consistent order\ntest_names = OrderedDict([\n    ('optimize_final_model_regression', regressor_tests.optimize_final_model_regression),\n    # ('getting_single_predictions_regression', regressor_tests.getting_single_predictions_regression),\n    # ('feature_learning_getting_single_predictions_regression', regressor_tests.feature_learning_getting_single_predictions_regression),\n    # ('categorical_ensembling_regression', regressor_tests.categorical_ensembling_regression),\n    # ('feature_learning_categorical_ensembling_getting_single_predictions_regression', regressor_tests.feature_learning_categorical_ensembling_getting_single_predictions_regression)\n])\n\ndef test_generator():\n    for model_name in training_parameters['model_names']:\n        for test_name, test in test_names.items():\n            test_model_name = model_name + 'Regressor'\n\n            test.description = str(test_model_name) + '_' + test_name\n            yield test, test_model_name\n"""
tests/advanced_tests/classifiers.py,0,"b'import datetime\nimport os\nimport random\nimport sys\nsys.path = [os.path.abspath(os.path.dirname(__file__))] + sys.path\n\nfrom auto_ml import Predictor\nfrom auto_ml.utils_models import load_ml_model\n\n\nimport dill\nimport numpy as np\nimport pandas as pd\nfrom nose.tools import assert_equal, assert_not_equal, with_setup\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nimport utils_testing as utils\n\ndef optimize_final_model_classification(model_name=None):\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    # We just want to make sure these run, not necessarily make sure that they\'re super accurate (which takes more time, and is dataset dependent)\n    df_titanic_train = df_titanic_train.sample(frac=0.5)\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_titanic_train, optimize_final_model=True, model_names=model_name)\n\n    test_score = ml_predictor.score(df_titanic_test, df_titanic_test.survived)\n\n    print(\'test_score\')\n    print(test_score)\n\n    # Small sample sizes mean there\'s a fair bit of noise here\n    lower_bound = -0.18\n    if model_name == \'DeepLearningClassifier\':\n        lower_bound = -0.255\n    if model_name == \'LGBMClassifier\':\n        lower_bound = -0.221\n    if model_name == \'GradientBoostingClassifier\':\n        lower_bound = -0.225\n    if model_name == \'CatBoostClassifier\':\n        lower_bound = -0.221\n\n    assert lower_bound < test_score < -0.135\n\n\ndef categorical_ensembling_classification(model_name=None):\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    ml_predictor.train_categorical_ensemble(df_titanic_train, model_names=model_name, categorical_column=\'embarked\')\n\n    test_score = ml_predictor.score(df_titanic_test, df_titanic_test.survived)\n\n    print(\'test_score\')\n    print(test_score)\n\n    lower_bound = -0.18\n    upper_bound = -0.145\n\n    if model_name == \'DeepLearningClassifier\':\n        lower_bound = -0.215\n\n    # CatBoost is super inconsistent\n    if model_name == \'CatBoostClassifier\':\n        upper_bound = -0.137\n\n\n    assert lower_bound < test_score < upper_bound\n\n\ndef getting_single_predictions_classification(model_name=None):\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_titanic_train, model_names=model_name)\n\n    file_name = ml_predictor.save(str(random.random()))\n\n    saved_ml_pipeline = load_ml_model(file_name)\n\n    os.remove(file_name)\n    try:\n        keras_file_name = file_name[:-5] + \'_keras_deep_learning_model.h5\'\n        os.remove(keras_file_name)\n    except:\n        pass\n\n\n    df_titanic_test_dictionaries = df_titanic_test.to_dict(\'records\')\n\n    # 1. make sure the accuracy is the same\n\n    predictions = []\n    for row in df_titanic_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict_proba(row)[1])\n\n    print(\'predictions\')\n    print(predictions)\n\n    first_score = utils.calculate_brier_score_loss(df_titanic_test.survived, predictions)\n    print(\'first_score\')\n    print(first_score)\n    # Make sure our score is good, but not unreasonably good\n\n    lower_bound = -0.18\n    upper_bound = -0.135\n    if model_name == \'DeepLearningClassifier\':\n        lower_bound = -0.195\n    if model_name == \'CatBoostClassifier\':\n        lower_bound = -0.215\n        upper_bound = -0.128\n\n    assert lower_bound < first_score < upper_bound\n\n    # 2. make sure the speed is reasonable (do it a few extra times)\n    data_length = len(df_titanic_test_dictionaries)\n    start_time = datetime.datetime.now()\n    for idx in range(1000):\n        row_num = idx % data_length\n        saved_ml_pipeline.predict(df_titanic_test_dictionaries[row_num])\n    end_time = datetime.datetime.now()\n    duration = end_time - start_time\n\n    print(\'duration.total_seconds()\')\n    print(duration.total_seconds())\n\n    # It\'s very difficult to set a benchmark for speed that will work across all machines.\n    # On my 2013 bottom of the line 15"" MacBook Pro, this runs in about 0.8 seconds for 1000 predictions\n    # That\'s about 1 millisecond per prediction\n    # Assuming we might be running on a test box that\'s pretty weak, multiply by 3\n    # Also make sure we\'re not running unreasonably quickly\n    assert 0.2 < duration.total_seconds() < 60\n\n\n    # 3. make sure we\'re not modifying the dictionaries (the score is the same after running a few experiments as it is the first time)\n\n    predictions = []\n    for row in df_titanic_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict_proba(row)[1])\n\n    print(\'predictions\')\n    print(predictions)\n    print(\'df_titanic_test_dictionaries\')\n    print(df_titanic_test_dictionaries)\n    second_score = utils.calculate_brier_score_loss(df_titanic_test.survived, predictions)\n    print(\'second_score\')\n    print(second_score)\n    # Make sure our score is good, but not unreasonably good\n\n    assert lower_bound < second_score < upper_bound\n\n\ndef getting_single_predictions_multilabel_classification(model_name=None):\n    # auto_ml does not support multilabel classification for deep learning at the moment\n    if model_name == \'DeepLearningClassifier\' or model_name == \'CatBoostClassifier\':\n        return\n\n    np.random.seed(0)\n\n    df_twitter_train, df_twitter_test = utils.get_twitter_sentiment_multilabel_classification_dataset()\n\n    column_descriptions = {\n        \'airline_sentiment\': \'output\'\n        , \'airline\': \'categorical\'\n        , \'text\': \'ignore\'\n        , \'tweet_location\': \'categorical\'\n        , \'user_timezone\': \'categorical\'\n        , \'tweet_created\': \'date\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n    ml_predictor.train(df_twitter_train, model_names=model_name)\n\n    file_name = ml_predictor.save(str(random.random()))\n\n    saved_ml_pipeline = load_ml_model(file_name)\n\n    os.remove(file_name)\n    try:\n        keras_file_name = file_name[:-5] + \'_keras_deep_learning_model.h5\'\n        os.remove(keras_file_name)\n    except:\n        pass\n\n\n    df_twitter_test_dictionaries = df_twitter_test.to_dict(\'records\')\n\n    # 1. make sure the accuracy is the same\n\n    predictions = []\n    for row in df_twitter_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict(row))\n\n    print(\'predictions\')\n    print(predictions)\n\n    first_score = accuracy_score(df_twitter_test.airline_sentiment, predictions)\n    print(\'first_score\')\n    print(first_score)\n    # Make sure our score is good, but not unreasonably good\n    lower_bound = 0.67\n    # LGBM is super finnicky here- sometimes it\'s fine, but sometimes it does pretty terribly.\n    if model_name == \'LGBMClassifier\':\n        lower_bound = 0.6\n    assert lower_bound < first_score < 0.79\n\n    # 2. make sure the speed is reasonable (do it a few extra times)\n    data_length = len(df_twitter_test_dictionaries)\n    start_time = datetime.datetime.now()\n    for idx in range(1000):\n        row_num = idx % data_length\n        saved_ml_pipeline.predict(df_twitter_test_dictionaries[row_num])\n    end_time = datetime.datetime.now()\n    duration = end_time - start_time\n\n    print(\'duration.total_seconds()\')\n    print(duration.total_seconds())\n\n    # It\'s very difficult to set a benchmark for speed that will work across all machines.\n    # On my 2013 bottom of the line 15"" MacBook Pro, this runs in about 0.8 seconds for 1000 predictions\n    # That\'s about 1 millisecond per prediction\n    # Assuming we might be running on a test box that\'s pretty weak, multiply by 3\n    # Also make sure we\'re not running unreasonably quickly\n    assert 0.2 < duration.total_seconds() < 60\n\n\n    # 3. make sure we\'re not modifying the dictionaries (the score is the same after running a few experiments as it is the first time)\n\n    predictions = []\n    for row in df_twitter_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict(row))\n\n    print(\'predictions\')\n    print(predictions)\n    print(\'df_twitter_test_dictionaries\')\n    print(df_twitter_test_dictionaries)\n    second_score = accuracy_score(df_twitter_test.airline_sentiment, predictions)\n    print(\'second_score\')\n    print(second_score)\n    # Make sure our score is good, but not unreasonably good\n    assert lower_bound < second_score < 0.79\n\n\ndef feature_learning_getting_single_predictions_classification(model_name=None):\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    # NOTE: this is bad practice to pass in our same training set as our fl_data set, but we don\'t have enough data to do it any other way\n    df_titanic_train, fl_data = train_test_split(df_titanic_train, test_size=0.2)\n    ml_predictor.train(df_titanic_train, model_names=model_name, feature_learning=True, fl_data=fl_data)\n\n    file_name = ml_predictor.save(str(random.random()))\n\n    saved_ml_pipeline = load_ml_model(file_name)\n\n    os.remove(file_name)\n    try:\n        keras_file_name = file_name[:-5] + \'_keras_deep_learning_model.h5\'\n        os.remove(keras_file_name)\n    except:\n        pass\n\n\n    df_titanic_test_dictionaries = df_titanic_test.to_dict(\'records\')\n\n    # 1. make sure the accuracy is the same\n\n    predictions = []\n    for row in df_titanic_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict_proba(row)[1])\n\n    print(\'predictions\')\n    print(predictions)\n\n    first_score = utils.calculate_brier_score_loss(df_titanic_test.survived, predictions)\n    print(\'first_score\')\n    print(first_score)\n    # Make sure our score is good, but not unreasonably good\n\n    lower_bound = -0.16\n    if model_name == \'DeepLearningClassifier\':\n        lower_bound = -0.187\n\n    assert lower_bound < first_score < -0.133\n\n    # 2. make sure the speed is reasonable (do it a few extra times)\n    data_length = len(df_titanic_test_dictionaries)\n    start_time = datetime.datetime.now()\n    for idx in range(1000):\n        row_num = idx % data_length\n        saved_ml_pipeline.predict(df_titanic_test_dictionaries[row_num])\n    end_time = datetime.datetime.now()\n    duration = end_time - start_time\n\n    print(\'duration.total_seconds()\')\n    print(duration.total_seconds())\n\n    # It\'s very difficult to set a benchmark for speed that will work across all machines.\n    # On my 2013 bottom of the line 15"" MacBook Pro, this runs in about 0.8 seconds for 1000 predictions\n    # That\'s about 1 millisecond per prediction\n    # Assuming we might be running on a test box that\'s pretty weak, multiply by 3\n    # Also make sure we\'re not running unreasonably quickly\n    assert 0.2 < duration.total_seconds() < 60\n\n\n    # 3. make sure we\'re not modifying the dictionaries (the score is the same after running a few experiments as it is the first time)\n\n    predictions = []\n    for row in df_titanic_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict_proba(row)[1])\n\n    print(\'predictions\')\n    print(predictions)\n    print(\'df_titanic_test_dictionaries\')\n    print(df_titanic_test_dictionaries)\n    second_score = utils.calculate_brier_score_loss(df_titanic_test.survived, predictions)\n    print(\'second_score\')\n    print(second_score)\n    # Make sure our score is good, but not unreasonably good\n\n    assert lower_bound < second_score < -0.133\n\n\ndef feature_learning_categorical_ensembling_getting_single_predictions_classification(model_name=None):\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    # NOTE: this is bad practice to pass in our same training set as our fl_data set, but we don\'t have enough data to do it any other way\n    df_titanic_train, fl_data = train_test_split(df_titanic_train, test_size=0.2)\n    ml_predictor.train_categorical_ensemble(df_titanic_train, model_names=model_name, feature_learning=True, fl_data=fl_data, categorical_column=\'embarked\')\n\n    file_name = ml_predictor.save(str(random.random()))\n\n    from auto_ml.utils_models import load_ml_model\n\n    saved_ml_pipeline = load_ml_model(file_name)\n\n    os.remove(file_name)\n    try:\n        keras_file_name = file_name[:-5] + \'_keras_deep_learning_model.h5\'\n        os.remove(keras_file_name)\n    except:\n        pass\n\n\n    df_titanic_test_dictionaries = df_titanic_test.to_dict(\'records\')\n\n    # 1. make sure the accuracy is the same\n\n    predictions = []\n    for row in df_titanic_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict_proba(row)[1])\n\n    print(\'predictions\')\n    print(predictions)\n\n    first_score = utils.calculate_brier_score_loss(df_titanic_test.survived, predictions)\n    print(\'first_score\')\n    print(first_score)\n    # Make sure our score is good, but not unreasonably good\n\n    lower_bound = -0.17\n    if model_name == \'DeepLearningClassifier\':\n        lower_bound = -0.245\n    if model_name == \'CatBoostClassifier\':\n        lower_bound = -0.265\n\n    assert lower_bound < first_score < -0.147\n\n    # 2. make sure the speed is reasonable (do it a few extra times)\n    data_length = len(df_titanic_test_dictionaries)\n    start_time = datetime.datetime.now()\n    for idx in range(1000):\n        row_num = idx % data_length\n        saved_ml_pipeline.predict(df_titanic_test_dictionaries[row_num])\n    end_time = datetime.datetime.now()\n    duration = end_time - start_time\n\n    print(\'duration.total_seconds()\')\n    print(duration.total_seconds())\n\n    # It\'s very difficult to set a benchmark for speed that will work across all machines.\n    # On my 2013 bottom of the line 15"" MacBook Pro, this runs in about 0.8 seconds for 1000 predictions\n    # That\'s about 1 millisecond per prediction\n    # Assuming we might be running on a test box that\'s pretty weak, multiply by 3\n    # Also make sure we\'re not running unreasonably quickly\n    assert 0.2 < duration.total_seconds() < 60\n\n\n    # 3. make sure we\'re not modifying the dictionaries (the score is the same after running a few experiments as it is the first time)\n\n    predictions = []\n    for row in df_titanic_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict_proba(row)[1])\n\n    print(\'predictions\')\n    print(predictions)\n    print(\'df_titanic_test_dictionaries\')\n    print(df_titanic_test_dictionaries)\n    second_score = utils.calculate_brier_score_loss(df_titanic_test.survived, predictions)\n    print(\'second_score\')\n    print(second_score)\n    # Make sure our score is good, but not unreasonably good\n\n    assert lower_bound < second_score < -0.147\n'"
tests/advanced_tests/regressors.py,0,"b'import datetime\nimport os\nimport random\nimport sys\nsys.path = [os.path.abspath(os.path.dirname(__file__))] + sys.path\n\nfrom auto_ml import Predictor\nfrom auto_ml.utils_models import load_ml_model\n\nimport dill\nfrom nose.tools import assert_equal, assert_not_equal, with_setup\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nimport utils_testing as utils\n\ndef optimize_final_model_regression(model_name=None):\n    np.random.seed(0)\n\n    df_boston_train, df_boston_test = utils.get_boston_regression_dataset()\n\n    # We just want to make sure these run, not necessarily make sure that they\'re super accurate (which takes more time, and is dataset dependent)\n    df_boston_train = df_boston_train.sample(frac=0.5)\n\n    column_descriptions = {\n        \'MEDV\': \'output\'\n        , \'CHAS\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'regressor\', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_boston_train, optimize_final_model=True, model_names=model_name)\n\n    test_score = ml_predictor.score(df_boston_test, df_boston_test.MEDV)\n\n    print(\'test_score\')\n    print(test_score)\n\n    # the random seed gets a score of -3.21 on python 3.5\n    # There\'s a ton of noise here, due to small sample sizes\n    lower_bound = -3.4\n    if model_name == \'DeepLearningRegressor\':\n        lower_bound = -24\n    if model_name == \'LGBMRegressor\':\n        lower_bound = -16\n    if model_name == \'GradientBoostingRegressor\':\n        lower_bound = -5.1\n    if model_name == \'CatBoostRegressor\':\n        lower_bound = -4.5\n    if model_name == \'XGBRegressor\':\n        lower_bound = -4.8\n\n    assert lower_bound < test_score < -2.75\n\n\n\ndef getting_single_predictions_regression(model_name=None):\n    np.random.seed(0)\n\n    df_boston_train, df_boston_test = utils.get_boston_regression_dataset()\n\n    column_descriptions = {\n        \'MEDV\': \'output\'\n        , \'CHAS\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'regressor\', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_boston_train, model_names=model_name)\n\n    file_name = ml_predictor.save(str(random.random()))\n\n    saved_ml_pipeline = load_ml_model(file_name)\n\n    os.remove(file_name)\n    try:\n        keras_file_name = file_name[:-5] + \'_keras_deep_learning_model.h5\'\n        os.remove(keras_file_name)\n    except:\n        pass\n\n\n    df_boston_test_dictionaries = df_boston_test.to_dict(\'records\')\n\n    # 1. make sure the accuracy is the same\n\n    predictions = []\n    for row in df_boston_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict(row))\n\n    print(\'predictions\')\n    print(predictions)\n    print(\'predictions[0]\')\n    print(predictions[0])\n    print(\'type(predictions)\')\n    print(type(predictions))\n    first_score = utils.calculate_rmse(df_boston_test.MEDV, predictions)\n    print(\'first_score\')\n    print(first_score)\n    # Make sure our score is good, but not unreasonably good\n\n    lower_bound = -2.9\n    if model_name == \'DeepLearningRegressor\':\n        lower_bound = -7.8\n    if model_name == \'LGBMRegressor\':\n        lower_bound = -4.95\n    if model_name == \'XGBRegressor\':\n        lower_bound = -3.4\n    if model_name == \'CatBoostRegressor\':\n        lower_bound = -3.7\n\n    assert lower_bound < first_score < -2.7\n\n    # 2. make sure the speed is reasonable (do it a few extra times)\n    data_length = len(df_boston_test_dictionaries)\n    start_time = datetime.datetime.now()\n    for idx in range(1000):\n        row_num = idx % data_length\n        saved_ml_pipeline.predict(df_boston_test_dictionaries[row_num])\n    end_time = datetime.datetime.now()\n    duration = end_time - start_time\n\n    print(\'duration.total_seconds()\')\n    print(duration.total_seconds())\n\n    # It\'s very difficult to set a benchmark for speed that will work across all machines.\n    # On my 2013 bottom of the line 15"" MacBook Pro, this runs in about 0.8 seconds for 1000 predictions\n    # That\'s about 1 millisecond per prediction\n    # Assuming we might be running on a test box that\'s pretty weak, multiply by 3\n    # Also make sure we\'re not running unreasonably quickly\n    assert 0.1 < duration.total_seconds() / 1.0 < 60\n\n\n    # 3. make sure we\'re not modifying the dictionaries (the score is the same after running a few experiments as it is the first time)\n\n    predictions = []\n    for row in df_boston_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict(row))\n\n    second_score = utils.calculate_rmse(df_boston_test.MEDV, predictions)\n    print(\'second_score\')\n    print(second_score)\n    # Make sure our score is good, but not unreasonably good\n\n    assert lower_bound < second_score < -2.7\n\n'"
tests/advanced_tests/utils_testing.py,0,"b""import sys, os\nsys.path = [os.path.abspath(os.path.dirname(__file__))] + sys.path\n\nimport pandas as pd\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import brier_score_loss, mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nfrom auto_ml import Predictor\n\ndef get_boston_regression_dataset():\n    boston = load_boston()\n    df_boston = pd.DataFrame(boston.data)\n    df_boston.columns = boston.feature_names\n    df_boston['MEDV'] = boston['target']\n    df_boston_train, df_boston_test = train_test_split(df_boston, test_size=0.33, random_state=42)\n    return df_boston_train, df_boston_test\n\ndef get_titanic_binary_classification_dataset(basic=True):\n    try:\n        df_titanic = pd.read_csv(os.path.join('tests', 'titanic.csv'))\n    except Exception as e:\n        print('Error')\n        print(e)\n        dataset_url = 'http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.csv'\n        df_titanic = pd.read_csv(dataset_url)\n        # Do not write the index that pandas automatically creates\n        df_titanic.to_csv(os.path.join('tests', 'titanic.csv'), index=False)\n\n    df_titanic = df_titanic.drop(['boat', 'body'], axis=1)\n\n    if basic == True:\n        df_titanic = df_titanic.drop(['name', 'ticket', 'cabin', 'home.dest'], axis=1)\n\n    df_titanic_train, df_titanic_test = train_test_split(df_titanic, test_size=0.33, random_state=42)\n    return df_titanic_train, df_titanic_test\n\n\ndef train_basic_binary_classifier(df_titanic_train):\n    column_descriptions = {\n        'survived': 'output'\n        , 'sex': 'categorical'\n        , 'embarked': 'categorical'\n        , 'pclass': 'categorical'\n    }\n\n    ml_predictor = Predictor(type_of_estimator='classifier', column_descriptions=column_descriptions)\n    ml_predictor.train(df_titanic_train)\n\n    return ml_predictor\n\n\ndef train_basic_regressor(df_boston_train):\n    column_descriptions = {\n        'MEDV': 'output'\n        , 'CHAS': 'categorical'\n    }\n\n    ml_predictor = Predictor(type_of_estimator='regressor', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_boston_train, verbose=False)\n    return ml_predictor\n\ndef calculate_rmse(actuals, preds):\n    return mean_squared_error(actuals, preds)**0.5 * -1\n\ndef calculate_brier_score_loss(actuals, probas):\n    return -1 * brier_score_loss(actuals, probas)\n\n\n\ndef get_twitter_sentiment_multilabel_classification_dataset():\n\n    file_name = os.path.join('tests', 'twitter_sentiment.csv')\n\n    try:\n        df_twitter = pd.read_csv(open(file_name,'rU'), encoding='latin-1', engine='python')\n    except Exception as e:\n        print('Error')\n        print(e)\n        dataset_url = 'https://raw.githubusercontent.com/ClimbsRocks/sample_datasets/master/twitter_airline_sentiment.csv'\n        df_twitter = pd.read_csv(dataset_url, encoding='latin-1')\n        # Do not write the index that pandas automatically creates\n\n        df_twitter.to_csv(file_name, index=False, encoding='latin-1')\n\n    # Grab only 10% of the dataset- runs much faster this way\n    df_twitter = df_twitter.sample(frac=0.1)\n\n    df_twitter['tweet_created'] = pd.to_datetime(df_twitter.tweet_created)\n\n    df_twitter_train, df_twitter_test = train_test_split(df_twitter, test_size=0.33, random_state=42)\n    return df_twitter_train, df_twitter_test\n\n\ndef train_basic_multilabel_classifier(df_twitter_train):\n    column_descriptions = {\n        'airline_sentiment': 'output'\n        , 'airline': 'categorical'\n        , 'text': 'ignore'\n        , 'tweet_location': 'categorical'\n        , 'user_timezone': 'categorical'\n        , 'tweet_created': 'date'\n    }\n\n    ml_predictor = Predictor(type_of_estimator='classifier', column_descriptions=column_descriptions)\n    ml_predictor.train(df_twitter_train)\n\n    return ml_predictor\n"""
tests/backwards_compatibility_tests/backwards_compatibility_test.py,0,"b'import datetime\nimport os\nimport sys\nsys.path = [os.path.abspath(os.path.dirname(__file__))] + sys.path\nsys.path = [os.path.abspath(os.path.dirname(os.path.dirname(__file__)))] + sys.path\n\nfrom auto_ml import Predictor, __version__ as auto_ml_version\nfrom auto_ml.utils_models import load_ml_model\nimport dill\nimport numpy as np\nimport utils_testing as utils\n\n\nif \'backwards_compatibility\' in os.environ.get(\'TESTS_TO_RUN\', \'blank\'):\n    def test_backwards_compatibility_with_version_2_1_6():\n        np.random.seed(0)\n        print(\'auto_ml_version\')\n        print(auto_ml_version)\n        if auto_ml_version <= \'2.9.0\':\n            raise(TypeError)\n\n        df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n        saved_ml_pipeline = load_ml_model(os.path.join(\'tests\', \'backwards_compatibility_tests\', \'trained_ml_model_v_2_1_6.dill\'))\n\n        df_titanic_test_dictionaries = df_titanic_test.to_dict(\'records\')\n\n        # 1. make sure the accuracy is the same\n\n        predictions = []\n        for row in df_titanic_test_dictionaries:\n            predictions.append(saved_ml_pipeline.predict_proba(row)[1])\n\n        print(\'predictions\')\n        print(predictions)\n\n        first_score = utils.calculate_brier_score_loss(df_titanic_test.survived, predictions)\n        print(\'first_score\')\n        print(first_score)\n        # Make sure our score is good, but not unreasonably good\n\n        lower_bound = -0.215\n\n        assert lower_bound < first_score < -0.17\n\n        # 2. make sure the speed is reasonable (do it a few extra times)\n        data_length = len(df_titanic_test_dictionaries)\n        start_time = datetime.datetime.now()\n        for idx in range(1000):\n            row_num = idx % data_length\n            saved_ml_pipeline.predict(df_titanic_test_dictionaries[row_num])\n        end_time = datetime.datetime.now()\n        duration = end_time - start_time\n\n        print(\'duration.total_seconds()\')\n        print(duration.total_seconds())\n\n        # It\'s very difficult to set a benchmark for speed that will work across all machines.\n        # On my 2013 bottom of the line 15"" MacBook Pro, this runs in about 0.8 seconds for 1000 predictions\n        # That\'s about 1 millisecond per prediction\n        # Assuming we might be running on a test box that\'s pretty weak, multiply by 3\n        # Also make sure we\'re not running unreasonably quickly\n        assert 0.2 < duration.total_seconds() < 15\n\n\n        # 3. make sure we\'re not modifying the dictionaries (the score is the same after running a few experiments as it is the first time)\n\n        predictions = []\n        for row in df_titanic_test_dictionaries:\n            predictions.append(saved_ml_pipeline.predict_proba(row)[1])\n\n        print(\'predictions\')\n        print(predictions)\n        print(\'df_titanic_test_dictionaries\')\n        print(df_titanic_test_dictionaries)\n        second_score = utils.calculate_brier_score_loss(df_titanic_test.survived, predictions)\n        print(\'second_score\')\n        print(second_score)\n        # Make sure our score is good, but not unreasonably good\n\n        assert lower_bound < second_score < -0.17\n\n\ndef train_old_model():\n    print(\'auto_ml_version\')\n    print(auto_ml_version)\n    if auto_ml_version > \'2.1.6\':\n        raise(TypeError)\n\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_titanic_train)\n\n    file_name = ml_predictor.save(\'trained_ml_model_v_2_1_6.dill\')\n\n    saved_ml_pipeline = load_ml_model(file_name)\n\n    df_titanic_test_dictionaries = df_titanic_test.to_dict(\'records\')\n\n    # 1. make sure the accuracy is the same\n\n    predictions = []\n    for row in df_titanic_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict_proba(row)[1])\n\n    first_score = utils.calculate_brier_score_loss(df_titanic_test.survived, predictions)\n    # Make sure our score is good, but not unreasonably good\n\n    lower_bound = -0.16\n\n    assert -0.16 < first_score < -0.135\n\n    # 2. make sure the speed is reasonable (do it a few extra times)\n    data_length = len(df_titanic_test_dictionaries)\n    start_time = datetime.datetime.now()\n    for idx in range(1000):\n        row_num = idx % data_length\n        saved_ml_pipeline.predict(df_titanic_test_dictionaries[row_num])\n    end_time = datetime.datetime.now()\n    duration = end_time - start_time\n\n    print(\'duration.total_seconds()\')\n    print(duration.total_seconds())\n\n    # It\'s very difficult to set a benchmark for speed that will work across all machines.\n    # On my 2013 bottom of the line 15"" MacBook Pro, this runs in about 0.8 seconds for 1000 predictions\n    # That\'s about 1 millisecond per prediction\n    # Assuming we might be running on a test box that\'s pretty weak, multiply by 3\n    # Also make sure we\'re not running unreasonably quickly\n    assert 0.2 < duration.total_seconds() < 15\n\n\n    # 3. make sure we\'re not modifying the dictionaries (the score is the same after running a few experiments as it is the first time)\n\n    predictions = []\n    for row in df_titanic_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict_proba(row)[1])\n\n    second_score = utils.calculate_brier_score_loss(df_titanic_test.survived, predictions)\n    # Make sure our score is good, but not unreasonably good\n\n    assert -0.16 < second_score < -0.135\n\nif __name__ == \'__main__\':\n    train_old_model()\n'"
tests/core_tests/api_coverage_tests_classifiers.py,0,"b'# This file is just to test passing a bunch of different parameters into train to make sure that things work\n# At first, it is not necessarily testing whether those things have the intended effect or not\n\nimport datetime\nimport os\nimport random\nimport sys\nsys.path = [os.path.abspath(os.path.dirname(__file__))] + sys.path\nsys.path = [os.path.abspath(os.path.dirname(os.path.dirname(__file__)))] + sys.path\n\nos.environ[\'is_test_suite\'] = \'True\'\n\nfrom auto_ml import Predictor\nfrom auto_ml.utils_models import load_ml_model\n\nimport dill\nimport numpy as np\nfrom nose.tools import assert_equal, assert_not_equal, with_setup\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nimport utils_testing as utils\n\ndef test_perform_feature_selection_false_classification():\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_titanic_train, perform_feature_selection=False)\n\n    test_score = ml_predictor.score(df_titanic_test, df_titanic_test.survived)\n\n    print(\'test_score\')\n    print(test_score)\n\n    assert -0.16 < test_score < -0.135\n\n# For some reason, this test now causes a Segmentation Default on travis when run on python 3.5.\n# home/travis/.travis/job_stages: line 53:  8810 Segmentation fault      (core dumped) nosetests -v --with-coverage --cover-package auto_ml tests\n# It didn\'t error previously\n# It appears to be an environment issue (possibly cuased by running too many parallelized things, which only happens in a test suite), not an issue with auto_ml. So we\'ll run this test to make sure the library functionality works, but only on some environments\nif os.environ.get(\'TRAVIS_PYTHON_VERSION\', \'0\') != \'3.5\':\n    def test_compare_all_models_classification():\n        np.random.seed(0)\n\n        df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n        column_descriptions = {\n            \'survived\': \'output\'\n            , \'sex\': \'categorical\'\n            , \'embarked\': \'categorical\'\n            , \'pclass\': \'categorical\'\n        }\n\n        ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n        ml_predictor.train(df_titanic_train, compare_all_models=True)\n\n        test_score = ml_predictor.score(df_titanic_test, df_titanic_test.survived)\n\n        print(\'test_score\')\n        print(test_score)\n\n        assert -0.16 < test_score < -0.135\n\n\n\n\ndef test_perform_feature_selection_true_classification():\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_titanic_train, perform_feature_selection=True)\n\n    test_score = ml_predictor.score(df_titanic_test, df_titanic_test.survived)\n\n    print(\'test_score\')\n    print(test_score)\n\n    assert -0.16 < test_score < -0.124\n\n\ndef test_perform_feature_scaling_true_classification():\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_titanic_train, perform_feature_scaling=True)\n\n    test_score = ml_predictor.score(df_titanic_test, df_titanic_test.survived)\n\n    print(\'test_score\')\n    print(test_score)\n\n    assert -0.16 < test_score < -0.135\n\ndef test_perform_feature_scaling_false_classification():\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_titanic_train, perform_feature_scaling=False)\n\n    test_score = ml_predictor.score(df_titanic_test, df_titanic_test.survived)\n\n    print(\'test_score\')\n    print(test_score)\n\n    assert -0.16 < test_score < -0.14\n\n\ndef test_user_input_func_classification():\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    def age_bucketing(data):\n\n        def define_buckets(age):\n            if age <= 17:\n                return \'youth\'\n            elif age <= 40:\n                return \'adult\'\n            elif age <= 60:\n                return \'adult2\'\n            else:\n                return \'over_60\'\n\n        if isinstance(data, dict):\n            data[\'age_bucket\'] = define_buckets(data[\'age\'])\n        else:\n            data[\'age_bucket\'] = data.age.apply(define_buckets)\n\n        return data\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n        , \'age_bucket\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_titanic_train, user_input_func=age_bucketing)\n\n\n    file_name = ml_predictor.save(str(random.random()))\n\n    saved_ml_pipeline = load_ml_model(file_name)\n\n    os.remove(file_name)\n    try:\n        keras_file_name = file_name[:-5] + \'_keras_deep_learning_model.h5\'\n        os.remove(keras_file_name)\n    except:\n        pass\n\n\n    df_titanic_test_dictionaries = df_titanic_test.to_dict(\'records\')\n\n    # 1. make sure the accuracy is the same\n\n    predictions = []\n    for row in df_titanic_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict_proba(row)[1])\n\n    print(\'predictions\')\n    print(predictions)\n\n    first_score = utils.calculate_brier_score_loss(df_titanic_test.survived, predictions)\n    print(\'first_score\')\n    print(first_score)\n    # Make sure our score is good, but not unreasonably good\n\n    lower_bound = -0.16\n\n    assert -0.16 < first_score < -0.135\n\n    # 2. make sure the speed is reasonable (do it a few extra times)\n    data_length = len(df_titanic_test_dictionaries)\n    start_time = datetime.datetime.now()\n    for idx in range(1000):\n        row_num = idx % data_length\n        saved_ml_pipeline.predict(df_titanic_test_dictionaries[row_num])\n    end_time = datetime.datetime.now()\n    duration = end_time - start_time\n\n    print(\'duration.total_seconds()\')\n    print(duration.total_seconds())\n\n    # It\'s very difficult to set a benchmark for speed that will work across all machines.\n    # On my 2013 bottom of the line 15"" MacBook Pro, this runs in about 0.8 seconds for 1000 predictions\n    # That\'s about 1 millisecond per prediction\n    # Assuming we might be running on a test box that\'s pretty weak, multiply by 3\n    # Also make sure we\'re not running unreasonably quickly\n    assert 0.2 < duration.total_seconds() < 15\n\n\n    # 3. make sure we\'re not modifying the dictionaries (the score is the same after running a few experiments as it is the first time)\n\n    predictions = []\n    for row in df_titanic_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict_proba(row)[1])\n\n    print(\'predictions\')\n    print(predictions)\n    print(\'df_titanic_test_dictionaries\')\n    print(df_titanic_test_dictionaries)\n    second_score = utils.calculate_brier_score_loss(df_titanic_test.survived, predictions)\n    print(\'second_score\')\n    print(second_score)\n    # Make sure our score is good, but not unreasonably good\n\n    assert -0.16 < second_score < -0.135\n\n\ndef test_binary_classification_predict_on_Predictor_instance():\n    np.random.seed(0)\n\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n    ml_predictor = utils.train_basic_binary_classifier(df_titanic_train)\n\n    predictions = ml_predictor.predict(df_titanic_test)\n    test_score = accuracy_score(predictions, df_titanic_test.survived)\n    # Make sure our score is good, but not unreasonably good\n    print(test_score)\n    assert .77 < test_score < .805\n\n\n\ndef test_multilabel_classification_predict_on_Predictor_instance():\n    np.random.seed(0)\n\n    df_twitter_train, df_twitter_test = utils.get_twitter_sentiment_multilabel_classification_dataset()\n    # Note that this does not take \'text\' into account, intentionally (as that takes a while longer to train)\n    ml_predictor = utils.train_basic_multilabel_classifier(df_twitter_train)\n\n    predictions = ml_predictor.predict(df_twitter_test)\n    test_score = accuracy_score(predictions, df_twitter_test.airline_sentiment)\n    # Make sure our score is good, but not unreasonably good\n    print(\'test_score\')\n    print(test_score)\n    assert 0.72 < test_score < 0.77\n\n\ndef test_binary_classification_predict_proba_on_Predictor_instance():\n    np.random.seed(0)\n\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n    ml_predictor = utils.train_basic_binary_classifier(df_titanic_train)\n\n    #\n    predictions = ml_predictor.predict_proba(df_titanic_test)\n    predictions = [pred[1] for pred in predictions]\n    test_score = utils.calculate_brier_score_loss(df_titanic_test.survived, predictions)\n    # Make sure our score is good, but not unreasonably good\n    print(test_score)\n    assert -0.16 < test_score < -0.135\n\n\ndef test_pass_in_list_of_dictionaries_train_classification():\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    list_titanic_train = df_titanic_train.to_dict(\'records\')\n\n    ml_predictor.train(list_titanic_train)\n\n    test_score = ml_predictor.score(df_titanic_test, df_titanic_test.survived)\n\n    print(\'test_score\')\n    print(test_score)\n\n    assert -0.16 < test_score < -0.135\n\n\ndef test_pass_in_list_of_dictionaries_predict_classification():\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    list_titanic_train = df_titanic_train.to_dict(\'records\')\n\n    ml_predictor.train(df_titanic_train)\n\n    test_score = ml_predictor.score(df_titanic_test.to_dict(\'records\'), df_titanic_test.survived)\n\n    print(\'test_score\')\n    print(test_score)\n\n    assert -0.16 < test_score < -0.135\n\n\ndef test_include_bad_y_vals_train_classification():\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    df_titanic_train.iloc[1][\'survived\'] = None\n    df_titanic_train.iloc[8][\'survived\'] = None\n    df_titanic_train.iloc[26][\'survived\'] = None\n\n    ml_predictor.train(df_titanic_train)\n\n    test_score = ml_predictor.score(df_titanic_test.to_dict(\'records\'), df_titanic_test.survived)\n\n    print(\'test_score\')\n    print(test_score)\n\n    assert -0.17 < test_score < -0.135\n\n\n\ndef test_include_bad_y_vals_predict_classification():\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    df_titanic_test.iloc[1][\'survived\'] = float(\'nan\')\n    df_titanic_test.iloc[8][\'survived\'] = float(\'inf\')\n    df_titanic_test.iloc[26][\'survived\'] = None\n\n    ml_predictor.train(df_titanic_train)\n\n    test_score = ml_predictor.score(df_titanic_test.to_dict(\'records\'), df_titanic_test.survived)\n\n    print(\'test_score\')\n    print(test_score)\n\n    assert -0.16 < test_score < -0.135\n\n\ndef test_list_of_single_model_name_classification():\n    np.random.seed(0)\n    model_name = \'GradientBoostingClassifier\'\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_titanic_train, model_names=[model_name])\n\n    test_score = ml_predictor.score(df_titanic_test, df_titanic_test.survived)\n\n    print(\'test_score\')\n    print(test_score)\n\n    assert -0.16 < test_score < -0.135\n\nif os.environ.get(\'TRAVIS_PYTHON_VERSION\', \'0\') != \'3.5\':\n    def test_getting_single_predictions_nlp_date_multilabel_classification():\n\n        np.random.seed(0)\n\n        df_twitter_train, df_twitter_test = utils.get_twitter_sentiment_multilabel_classification_dataset()\n\n        column_descriptions = {\n            \'airline_sentiment\': \'output\'\n            , \'airline\': \'categorical\'\n            , \'text\': \'nlp\'\n            , \'tweet_location\': \'categorical\'\n            , \'user_timezone\': \'categorical\'\n            , \'tweet_created\': \'date\'\n        }\n\n        ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n        ml_predictor.train(df_twitter_train)\n\n        file_name = ml_predictor.save(str(random.random()))\n\n        saved_ml_pipeline = load_ml_model(file_name)\n\n        os.remove(file_name)\n        try:\n            keras_file_name = file_name[:-5] + \'_keras_deep_learning_model.h5\'\n            os.remove(keras_file_name)\n        except:\n            pass\n\n        df_twitter_test_dictionaries = df_twitter_test.to_dict(\'records\')\n\n        # 1. make sure the accuracy is the same\n\n        predictions = []\n        for row in df_twitter_test_dictionaries:\n            predictions.append(saved_ml_pipeline.predict(row))\n\n        print(\'predictions\')\n        print(predictions)\n\n        first_score = accuracy_score(df_twitter_test.airline_sentiment, predictions)\n        print(\'first_score\')\n        print(first_score)\n        # Make sure our score is good, but not unreasonably good\n        lower_bound = 0.73\n        assert lower_bound < first_score < 0.79\n\n        # 2. make sure the speed is reasonable (do it a few extra times)\n        data_length = len(df_twitter_test_dictionaries)\n        start_time = datetime.datetime.now()\n        for idx in range(1000):\n            row_num = idx % data_length\n            saved_ml_pipeline.predict(df_twitter_test_dictionaries[row_num])\n        end_time = datetime.datetime.now()\n        duration = end_time - start_time\n\n        print(\'duration.total_seconds()\')\n        print(duration.total_seconds())\n\n        # It\'s very difficult to set a benchmark for speed that will work across all machines.\n        # On my 2013 bottom of the line 15"" MacBook Pro, this runs in about 0.8 seconds for 1000 predictions\n        # That\'s about 1 millisecond per prediction\n        # Assuming we might be running on a test box that\'s pretty weak, multiply by 3\n        # Also make sure we\'re not running unreasonably quickly\n        assert 0.2 < duration.total_seconds() < 15\n\n\n        # 3. make sure we\'re not modifying the dictionaries (the score is the same after running a few experiments as it is the first time)\n\n        predictions = []\n        for row in df_twitter_test_dictionaries:\n            predictions.append(saved_ml_pipeline.predict(row))\n\n        print(\'predictions\')\n        print(predictions)\n        print(\'df_twitter_test_dictionaries\')\n        print(df_twitter_test_dictionaries)\n        second_score = accuracy_score(df_twitter_test.airline_sentiment, predictions)\n        print(\'second_score\')\n        print(second_score)\n        # Make sure our score is good, but not unreasonably good\n        assert lower_bound < second_score < 0.79\n\n'"
tests/core_tests/api_coverage_tests_regressors.py,0,"b""# This file is just to test passing a bunch of different parameters into train to make sure that things work\n# At first, it is not necessarily testing whether those things have the intended effect or not\n\nimport datetime\nimport os\nimport random\nimport sys\nsys.path = [os.path.abspath(os.path.dirname(__file__))] + sys.path\nsys.path = [os.path.abspath(os.path.dirname(os.path.dirname(__file__)))] + sys.path\n\nos.environ['is_test_suite'] = 'True'\n\nfrom auto_ml import Predictor\nfrom auto_ml.utils_models import load_ml_model\n\nimport dill\nfrom nose.tools import assert_equal, assert_not_equal, with_setup\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nimport utils_testing as utils\n\n\n# Tests on regression models:\n\ndef test_perform_feature_selection_true_regression(model_name=None):\n    np.random.seed(0)\n\n    df_boston_train, df_boston_test = utils.get_boston_regression_dataset()\n\n    column_descriptions = {\n        'MEDV': 'output'\n        , 'CHAS': 'categorical'\n    }\n\n    ml_predictor = Predictor(type_of_estimator='regressor', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_boston_train, perform_feature_selection=True, model_names=model_name)\n\n    test_score = ml_predictor.score(df_boston_test, df_boston_test.MEDV)\n\n    print('test_score')\n    print(test_score)\n\n    # Bumping this up since without these features our score drops\n    lower_bound = -4.0\n    if model_name == 'DeepLearningRegressor':\n        lower_bound = -14.5\n    if model_name == 'LGBMRegressor':\n        lower_bound = -4.95\n\n\n    assert lower_bound < test_score < -2.8\n\n\ndef test_perform_feature_selection_false_regression(model_name=None):\n    np.random.seed(0)\n\n    df_boston_train, df_boston_test = utils.get_boston_regression_dataset()\n\n    column_descriptions = {\n        'MEDV': 'output'\n        , 'CHAS': 'categorical'\n    }\n\n    ml_predictor = Predictor(type_of_estimator='regressor', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_boston_train, perform_feature_selection=False, model_names=model_name)\n\n    test_score = ml_predictor.score(df_boston_test, df_boston_test.MEDV)\n\n    print('test_score')\n    print(test_score)\n\n    lower_bound = -3.0\n\n    assert lower_bound < test_score < -2.7\n\ndef test_perform_feature_scaling_true_regression(model_name=None):\n    np.random.seed(0)\n\n    df_boston_train, df_boston_test = utils.get_boston_regression_dataset()\n\n    column_descriptions = {\n        'MEDV': 'output'\n        , 'CHAS': 'categorical'\n    }\n\n    ml_predictor = Predictor(type_of_estimator='regressor', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_boston_train, perform_feature_scaling=True)\n\n    test_score = ml_predictor.score(df_boston_test, df_boston_test.MEDV)\n\n    print('test_score')\n    print(test_score)\n\n    assert -3.0 < test_score < -2.7\n\ndef test_perform_feature_scaling_false_regression(model_name=None):\n    np.random.seed(0)\n\n    df_boston_train, df_boston_test = utils.get_boston_regression_dataset()\n\n    column_descriptions = {\n        'MEDV': 'output'\n        , 'CHAS': 'categorical'\n    }\n\n    ml_predictor = Predictor(type_of_estimator='regressor', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_boston_train, perform_feature_scaling=False, model_names=model_name)\n\n    test_score = ml_predictor.score(df_boston_test, df_boston_test.MEDV)\n\n    print('test_score')\n    print(test_score)\n\n    lower_bound = -3.0\n\n    assert lower_bound < test_score < -2.7\n\ndef test_compare_all_models_regression():\n    np.random.seed(0)\n\n    df_boston_train, df_boston_test = utils.get_boston_regression_dataset()\n\n    column_descriptions = {\n        'MEDV': 'output'\n        , 'CHAS': 'categorical'\n    }\n\n    ml_predictor = Predictor(type_of_estimator='regressor', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_boston_train, compare_all_models=True)\n\n    test_score = ml_predictor.score(df_boston_test, df_boston_test.MEDV)\n\n    print('test_score')\n    print(test_score)\n\n    # ExtraTrees again throws this off\n    assert -3.6 < test_score < -2.8\n\n\n"""
tests/core_tests/basic_tests.py,0,"b'""""""\nTo get standard out, run nosetests as follows:\nnosetests -sv tests\nnosetests --verbosity=2 --detailed-errors --nologcapture --processes=4 --process-restartworker --process-timeout=1000 tests\n""""""\nimport datetime\nimport os\nimport random\nimport sys\nsys.path = [os.path.abspath(os.path.dirname(__file__))] + sys.path\nsys.path = [os.path.abspath(os.path.dirname(os.path.dirname(__file__)))] + sys.path\n\nos.environ[\'is_test_suite\'] = \'True\'\n\nfrom auto_ml import Predictor\nfrom auto_ml.utils_models import load_ml_model\n\nfrom nose.tools import assert_equal, assert_not_equal, with_setup\nfrom sklearn.metrics import accuracy_score\n\nimport dill\nimport numpy as np\nimport utils_testing as utils\n\n\ndef test_linear_model_analytics_classification(model_name=None):\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_titanic_train, model_names=\'RidgeClassifier\')\n\n    test_score = ml_predictor.score(df_titanic_test, df_titanic_test.survived)\n\n    print(\'test_score\')\n    print(test_score)\n\n    assert -0.21 < test_score < -0.131\n\ndef test_input_df_unmodified():\n    np.random.seed(42)\n\n    df_boston_train, df_boston_test = utils.get_boston_regression_dataset()\n\n    column_descriptions = {\n        \'MEDV\': \'output\'\n        , \'CHAS\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'regressor\', column_descriptions=column_descriptions)\n\n    df_shape = df_boston_train.shape\n    ml_predictor.train(df_boston_train)\n\n    training_shape = df_boston_train.shape\n    assert training_shape[0] == df_shape[0]\n    assert training_shape[1] == df_shape[1]\n\n\n    test_score = ml_predictor.score(df_boston_test, df_boston_test.MEDV)\n\n    print(\'test_score\')\n    print(test_score)\n\n    assert -3.35 < test_score < -2.8\n\ndef test_model_uses_user_provided_training_params(model_name=None):\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    try:\n        ml_predictor.train(df_titanic_train, model_names=\'RidgeClassifier\', training_params={\'this_param_is_not_valid\': True})\n        assert False\n    except ValueError as e:\n        assert True\n\n\ndef test_ignores_new_invalid_features():\n\n    # One of the great unintentional features of auto_ml is that you can pass in new features at prediction time, that weren\'t present at training time, and they\'re silently ignored!\n    # One edge case here is new features that are strange objects (lists, datetimes, intervals, or anything else that we can\'t process in our default data processing pipeline). Initially, we just ignored them in dict_vectorizer, but we need to ignore them earlier.\n    np.random.seed(0)\n\n    df_boston_train, df_boston_test = utils.get_boston_regression_dataset()\n\n    column_descriptions = {\n        \'MEDV\': \'output\'\n        , \'CHAS\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'regressor\', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_boston_train)\n\n    file_name = ml_predictor.save(str(random.random()))\n\n    saved_ml_pipeline = load_ml_model(file_name)\n\n    os.remove(file_name)\n    try:\n        keras_file_name = file_name[:-5] + \'_keras_deep_learning_model.h5\'\n        os.remove(keras_file_name)\n    except:\n        pass\n\n\n    df_boston_test_dictionaries = df_boston_test.to_dict(\'records\')\n\n    # 1. make sure the accuracy is the same\n\n    predictions = []\n    for row in df_boston_test_dictionaries:\n        if random.random() > 0.9:\n            row[\'totally_new_feature\'] = datetime.datetime.now()\n            row[\'really_strange_feature\'] = random.random\n            row[\'we_should_really_ignore_this\'] = Predictor\n            row[\'pretty_vanilla_ignored_field\'] = 8\n            row[\'potentially_confusing_things_here\'] = float(\'nan\')\n            row[\'potentially_confusing_things_again\'] = float(\'inf\')\n            row[\'this_is_a_list\'] = [1,2,3,4,5]\n        predictions.append(saved_ml_pipeline.predict(row))\n\n    print(\'predictions\')\n    print(predictions)\n    print(\'predictions[0]\')\n    print(predictions[0])\n    print(\'type(predictions)\')\n    print(type(predictions))\n    first_score = utils.calculate_rmse(df_boston_test.MEDV, predictions)\n    print(\'first_score\')\n    print(first_score)\n    # Make sure our score is good, but not unreasonably good\n\n    lower_bound = -3.0\n    assert lower_bound < first_score < -2.7\n\n    # 2. make sure the speed is reasonable (do it a few extra times)\n    data_length = len(df_boston_test_dictionaries)\n    start_time = datetime.datetime.now()\n    for idx in range(1000):\n        row_num = idx % data_length\n        saved_ml_pipeline.predict(df_boston_test_dictionaries[row_num])\n    end_time = datetime.datetime.now()\n    duration = end_time - start_time\n\n    print(\'duration.total_seconds()\')\n    print(duration.total_seconds())\n\n    # It\'s very difficult to set a benchmark for speed that will work across all machines.\n    # On my 2013 bottom of the line 15"" MacBook Pro, this runs in about 0.8 seconds for 1000 predictions\n    # That\'s about 1 millisecond per prediction\n    # Assuming we might be running on a test box that\'s pretty weak, multiply by 3\n    # Also make sure we\'re not running unreasonably quickly\n    assert 0.1 < duration.total_seconds() / 1.0 < 15\n\n\n    # 3. make sure we\'re not modifying the dictionaries (the score is the same after running a few experiments as it is the first time)\n\n    predictions = []\n    for row in df_boston_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict(row))\n\n    second_score = utils.calculate_rmse(df_boston_test.MEDV, predictions)\n    print(\'second_score\')\n    print(second_score)\n    # Make sure our score is good, but not unreasonably good\n\n    assert lower_bound < second_score < -2.7\n'"
tests/core_tests/calibrate_classifier_tests.py,0,"b""import os\nimport sys\nsys.path = [os.path.abspath(os.path.dirname(__file__))] + sys.path\nsys.path = [os.path.abspath(os.path.dirname(os.path.dirname(__file__)))] + sys.path\n\nos.environ['is_test_suite'] = 'True'\n\nfrom auto_ml import Predictor\n\nimport dill\nfrom nose.tools import assert_equal, assert_not_equal, with_setup\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nimport utils_testing as utils\n\ndef test_calibrate_final_model_classification():\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    # Take a third of our test data (a tenth of our overall data) for calibration\n    df_titanic_test, df_titanic_calibration = train_test_split(df_titanic_test, test_size=0.33, random_state=42)\n\n    column_descriptions = {\n        'survived': 'output'\n        , 'sex': 'categorical'\n        , 'embarked': 'categorical'\n        , 'pclass': 'categorical'\n    }\n\n\n    ml_predictor = Predictor(type_of_estimator='classifier', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_titanic_train, calibrate_final_model=True, X_test=df_titanic_calibration, y_test=df_titanic_calibration.survived)\n\n    test_score = ml_predictor.score(df_titanic_test, df_titanic_test.survived)\n\n    print('test_score')\n    print(test_score)\n\n    assert -0.14 < test_score < -0.12\n\ndef test_calibrate_final_model_missing_X_test_y_test_classification():\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    # Take a third of our test data (a tenth of our overall data) for calibration\n    df_titanic_test, df_titanic_calibration = train_test_split(df_titanic_test, test_size=0.33, random_state=42)\n\n    column_descriptions = {\n        'survived': 'output'\n        , 'sex': 'categorical'\n        , 'embarked': 'categorical'\n        , 'pclass': 'categorical'\n    }\n\n\n    ml_predictor = Predictor(type_of_estimator='classifier', column_descriptions=column_descriptions)\n\n    # This should still work, just with warning printed\n    ml_predictor.train(df_titanic_train, calibrate_final_model=True)\n\n    test_score = ml_predictor.score(df_titanic_test, df_titanic_test.survived)\n\n    print('test_score')\n    print(test_score)\n\n    assert -0.14 < test_score < -0.12\n\n"""
tests/core_tests/categorical_ensembling_test.py,0,"b""import os\nimport sys\nsys.path = [os.path.abspath(os.path.dirname(__file__))] + sys.path\nsys.path = [os.path.abspath(os.path.dirname(os.path.dirname(__file__)))] + sys.path\n\nos.environ['is_test_suite'] = 'True'\n\nfrom auto_ml import Predictor\n\nimport dill\nimport numpy as np\nfrom nose.tools import assert_equal, assert_not_equal, with_setup\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nimport utils_testing as utils\n\ndef test_categorical_ensemble_basic_classifier():\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        'survived': 'output'\n        , 'pclass': 'categorical'\n        , 'embarked': 'categorical'\n        , 'sex': 'categorical'\n    }\n\n    ml_predictor = Predictor(type_of_estimator='classifier', column_descriptions=column_descriptions)\n\n    ml_predictor.train_categorical_ensemble(df_titanic_train, categorical_column='pclass', optimize_final_model=False)\n\n    test_score = ml_predictor.score(df_titanic_test, df_titanic_test.survived)\n\n    print('test_score')\n    print(test_score)\n\n    # Small sample sizes mean there's a fair bit of noise here\n    assert -0.155 < test_score < -0.135\n\n\ndef test_categorical_ensembling_regression(model_name=None):\n    np.random.seed(0)\n\n    df_boston_train, df_boston_test = utils.get_boston_regression_dataset()\n\n    column_descriptions = {\n        'MEDV': 'output'\n        , 'CHAS': 'categorical'\n    }\n\n    ml_predictor = Predictor(type_of_estimator='regressor', column_descriptions=column_descriptions)\n\n    ml_predictor.train_categorical_ensemble(df_boston_train, perform_feature_selection=True, model_names=model_name, categorical_column='CHAS')\n\n    test_score = ml_predictor.score(df_boston_test, df_boston_test.MEDV)\n\n    print('test_score')\n    print(test_score)\n\n    lower_bound = -4.2\n\n    assert lower_bound < test_score < -2.8\n\n\n"""
tests/core_tests/ensemble_tests.py,0,"b'import datetime\nimport os\nimport random\nimport sys\nsys.path = [os.path.abspath(os.path.dirname(__file__))] + sys.path\nsys.path = [os.path.abspath(os.path.dirname(os.path.dirname(__file__)))] + sys.path\n\nos.environ[\'is_test_suite\'] = \'True\'\n\nfrom auto_ml import Predictor\nfrom auto_ml.utils_models import load_ml_model\n\nfrom nose.tools import assert_equal, assert_not_equal, with_setup\nfrom sklearn.metrics import accuracy_score\n\nimport dill\nimport numpy as np\nimport utils_testing as utils\n\n\ndef ensemble_classifier_basic_test(model_name=None):\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ensemble_config = [\n        {\n            \'model_name\': \'LGBMClassifier\'\n        }\n        , {\n            \'model_name\': \'RandomForestClassifier\'\n        }\n\n    ]\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n\n    ml_predictor.train(df_titanic_train, ensemble_config=ensemble_config)\n\n    test_score = ml_predictor.score(df_titanic_test, df_titanic_test.survived)\n\n    print(\'test_score\')\n    print(test_score)\n\n    assert -0.15 < test_score < -0.131\n\n\ndef ensemble_regressor_basic_test():\n    np.random.seed(0)\n\n    df_boston_train, df_boston_test = utils.get_boston_regression_dataset()\n\n    column_descriptions = {\n        \'MEDV\': \'output\'\n        , \'CHAS\': \'categorical\'\n    }\n\n    ensemble_config = [\n        {\n            \'model_name\': \'LGBMRegressor\'\n        }\n        , {\n            \'model_name\': \'RandomForestRegressor\'\n        }\n\n    ]\n\n\n    ml_predictor = Predictor(type_of_estimator=\'regressor\', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_boston_train, ensemble_config=None)\n\n    test_score = ml_predictor.score(df_boston_test, df_boston_test.MEDV)\n\n    print(\'test_score\')\n    print(test_score)\n\n    assert -3.0 < test_score < -2.8\n\n\n# TODO: test for warning when passing in ensemble_method!=""average"" and is_classifier\n# TODO: make sure this works for single predictions and batch\ndef getting_single_predictions_classifier_test():\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n        , \'age_bucket\': \'categorical\'\n    }\n\n    ensemble_config = [\n        {\n            \'model_name\': \'LGBMClassifier\'\n        }\n        , {\n            \'model_name\': \'RandomForestClassifier\'\n        }\n\n    ]\n\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_titanic_train, ensemble_config=ensemble_config)\n\n\n    file_name = ml_predictor.save(str(random.random()))\n\n    saved_ml_pipeline = load_ml_model(file_name)\n\n    os.remove(file_name)\n    try:\n        keras_file_name = file_name[:-5] + \'_keras_deep_learning_model.h5\'\n        os.remove(keras_file_name)\n    except:\n        pass\n\n\n    df_titanic_test_dictionaries = df_titanic_test.to_dict(\'records\')\n\n    # 1. make sure the accuracy is the same\n\n    predictions = []\n    for row in df_titanic_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict_proba(row)[1])\n\n    print(\'predictions\')\n    print(predictions)\n\n    first_score = utils.calculate_brier_score_loss(df_titanic_test.survived, predictions)\n    print(\'first_score\')\n    print(first_score)\n    # Make sure our score is good, but not unreasonably good\n\n    lower_bound = -0.16\n\n    assert -0.15 < first_score < -0.135\n\n    # 2. make sure the speed is reasonable (do it a few extra times)\n    data_length = len(df_titanic_test_dictionaries)\n    start_time = datetime.datetime.now()\n    for idx in range(1000):\n        row_num = idx % data_length\n        saved_ml_pipeline.predict(df_titanic_test_dictionaries[row_num])\n    end_time = datetime.datetime.now()\n    duration = end_time - start_time\n\n    print(\'duration.total_seconds()\')\n    print(duration.total_seconds())\n\n    # It\'s very difficult to set a benchmark for speed that will work across all machines.\n    # On my 2013 bottom of the line 15"" MacBook Pro, this runs in about 0.8 seconds for 1000 predictions\n    # That\'s about 1 millisecond per prediction\n    # Assuming we might be running on a test box that\'s pretty weak, multiply by 3\n    # Also make sure we\'re not running unreasonably quickly\n    assert 0.2 < duration.total_seconds() < 60\n\n\n    # 3. make sure we\'re not modifying the dictionaries (the score is the same after running a few experiments as it is the first time)\n\n    predictions = []\n    for row in df_titanic_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict_proba(row)[1])\n\n    print(\'predictions\')\n    print(predictions)\n    print(\'df_titanic_test_dictionaries\')\n    print(df_titanic_test_dictionaries)\n    second_score = utils.calculate_brier_score_loss(df_titanic_test.survived, predictions)\n    print(\'second_score\')\n    print(second_score)\n    # Make sure our score is good, but not unreasonably good\n\n    assert -0.15 < second_score < -0.135\n\n\ndef getting_single_predictions_regressor_test():\n    np.random.seed(0)\n\n    df_boston_train, df_boston_test = utils.get_boston_regression_dataset()\n\n    column_descriptions = {\n        \'MEDV\': \'output\'\n        , \'CHAS\': \'categorical\'\n    }\n\n    ensemble_config = [\n        {\n            \'model_name\': \'LGBMRegressor\'\n        }\n        , {\n            \'model_name\': \'RandomForestRegressor\'\n        }\n\n    ]\n\n    ml_predictor = Predictor(type_of_estimator=\'regressor\', column_descriptions=column_descriptions)\n\n    # NOTE: this is bad practice to pass in our same training set as our fl_data set, but we don\'t have enough data to do it any other way\n    ml_predictor.train(df_boston_train, ensemble_config=ensemble_config)\n\n    test_score = ml_predictor.score(df_boston_test, df_boston_test.MEDV)\n\n    print(\'test_score\')\n    print(test_score)\n\n    assert -3.5 < test_score < -2.8\n\n\n    file_name = ml_predictor.save(str(random.random()))\n\n    saved_ml_pipeline = load_ml_model(file_name)\n\n    os.remove(file_name)\n    try:\n        keras_file_name = file_name[:-5] + \'_keras_deep_learning_model.h5\'\n        os.remove(keras_file_name)\n    except:\n        pass\n\n\n\n    df_boston_test_dictionaries = df_boston_test.to_dict(\'records\')\n\n    # 1. make sure the accuracy is the same\n\n    predictions = []\n    for row in df_boston_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict(row))\n\n    first_score = utils.calculate_rmse(df_boston_test.MEDV, predictions)\n    print(\'first_score\')\n    print(first_score)\n    # Make sure our score is good, but not unreasonably good\n\n    lower_bound = -3.5\n\n    assert lower_bound < first_score < -2.8\n\n    # 2. make sure the speed is reasonable (do it a few extra times)\n    data_length = len(df_boston_test_dictionaries)\n    start_time = datetime.datetime.now()\n    for idx in range(1000):\n        row_num = idx % data_length\n        saved_ml_pipeline.predict(df_boston_test_dictionaries[row_num])\n    end_time = datetime.datetime.now()\n    duration = end_time - start_time\n\n    print(\'duration.total_seconds()\')\n    print(duration.total_seconds())\n\n    # It\'s very difficult to set a benchmark for speed that will work across all machines.\n    # On my 2013 bottom of the line 15"" MacBook Pro, this runs in about 0.8 seconds for 1000 predictions\n    # That\'s about 1 millisecond per prediction\n    # Assuming we might be running on a test box that\'s pretty weak, multiply by 3\n    # Also make sure we\'re not running unreasonably quickly\n    assert 0.2 < duration.total_seconds() / 1.0 < 60\n\n\n    # 3. make sure we\'re not modifying the dictionaries (the score is the same after running a few experiments as it is the first time)\n\n    predictions = []\n    for row in df_boston_test_dictionaries:\n        predictions.append(saved_ml_pipeline.predict(row))\n\n    second_score = utils.calculate_rmse(df_boston_test.MEDV, predictions)\n    print(\'second_score\')\n    print(second_score)\n    # Make sure our score is good, but not unreasonably good\n\n    assert lower_bound < second_score < -2.8\n\n\n'"
tests/core_tests/quick_test.py,0,"b'# """"""\n# nosetests -sv --nologcapture tests/quick_test.py\n# nosetests --verbosity=2 --detailed-errors --nologcapture --processes=4 --process-restartworker --process-timeout=1000 tests/quick_test.py\n# """"""\n\nimport datetime\nimport os\nimport random\nimport sys\nsys.path = [os.path.abspath(os.path.dirname(__file__))] + sys.path\nsys.path = [os.path.abspath(os.path.dirname(os.path.dirname(__file__)))] + sys.path\n\nos.environ[\'is_test_suite\'] = \'True\'\n# os.environ[\'KERAS_BACKEND\'] = \'theano\'\n\nfrom auto_ml import Predictor\nfrom auto_ml.utils_models import load_ml_model\n\nfrom nose.tools import assert_equal, assert_not_equal, with_setup\nfrom sklearn.metrics import accuracy_score\n\nimport dill\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\n\n\ndef get_boston_regression_dataset():\n    boston = load_boston()\n    df_boston = pd.DataFrame(boston.data)\n    df_boston.columns = boston.feature_names\n    df_boston[\'MEDV\'] = boston[\'target\']\n    df_boston_train, df_boston_test = train_test_split(df_boston, test_size=0.33, random_state=42)\n    return df_boston_train, df_boston_test\n\n\ndef regression_test():\n    # a random seed of 42 has ExtraTreesRegressor getting the best CV score, and that model doesn\'t generalize as well as GradientBoostingRegressor.\n    np.random.seed(0)\n    model_name = \'LGBMRegressor\'\n\n    df_boston_train, df_boston_test = get_boston_regression_dataset()\n    many_dfs = []\n    for i in range(100):\n        many_dfs.append(df_boston_train)\n    df_boston_train = pd.concat(many_dfs)\n\n\n    column_descriptions = {\n        \'MEDV\': \'output\'\n        , \'CHAS\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'regressor\', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_boston_train, model_names=[model_name], perform_feature_scaling=False)\n\n    test_score = ml_predictor.score(df_boston_test, df_boston_test.MEDV)\n\n    print(\'test_score\')\n    print(test_score)\n\n    lower_bound = -3.2\n    if model_name == \'DeepLearningRegressor\':\n        lower_bound = -7.8\n    if model_name == \'LGBMRegressor\':\n        lower_bound = -4.95\n    if model_name == \'XGBRegressor\':\n        lower_bound = -3.4\n\n    assert lower_bound < test_score < -2.8\n\n\ndef get_titanic_binary_classification_dataset(basic=True):\n    try:\n        df_titanic = pd.read_csv(os.path.join(\'tests\', \'titanic.csv\'))\n    except Exception as e:\n        print(\'Error\')\n        print(e)\n        dataset_url = \'http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.csv\'\n        df_titanic = pd.read_csv(dataset_url)\n        # Do not write the index that pandas automatically creates\n        df_titanic.to_csv(os.path.join(\'tests\', \'titanic.csv\'), index=False)\n\n    df_titanic = df_titanic.drop([\'boat\', \'body\'], axis=1)\n\n    if basic == True:\n        df_titanic = df_titanic.drop([\'name\', \'ticket\', \'cabin\', \'home.dest\'], axis=1)\n\n    df_titanic_train, df_titanic_test = train_test_split(df_titanic, test_size=0.33, random_state=42)\n    return df_titanic_train, df_titanic_test\n\n\ndef classification_test():\n    np.random.seed(0)\n    # model_name = \'GradientBoostingClassifier\'\n    model_name = \'LGBMClassifier\'\n\n    df_titanic_train, df_titanic_test = get_titanic_binary_classification_dataset()\n    df_titanic_train[\'DELETE_THIS_FIELD\'] = 1\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n        , \'sex\': \'categorical\'\n        , \'this_does_not_exist\': \'ignore\'\n        , \'DELETE_THIS_FIELD\': \'ignore\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_titanic_train, model_names=model_name)\n\n    test_score = ml_predictor.score(df_titanic_test, df_titanic_test.survived)\n\n    print(\'test_score\')\n    print(test_score)\n\n    lower_bound = -0.16\n    if model_name == \'DeepLearningClassifier\':\n        lower_bound = -0.245\n    if model_name == \'LGBMClassifier\':\n        lower_bound = -0.225\n\n    assert lower_bound < test_score < -0.135\n\nif __name__ == \'__main__\':\n    classification_test()\n'"
tests/core_tests/scoring_tests.py,0,"b""import os\nimport sys\nsys.path = [os.path.abspath(os.path.dirname(__file__))] + sys.path\nsys.path = [os.path.abspath(os.path.dirname(os.path.dirname(__file__)))] + sys.path\n\nos.environ['is_test_suite'] = 'True'\n\nfrom auto_ml import Predictor\nimport numpy as np\n\nimport utils_testing as utils\n\ndef always_return_ten_thousand(estimator=None, actuals=None, probas=None):\n    return 10000\n\ndef test_binary_classification():\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        'survived': 'output'\n        , 'sex': 'categorical'\n        , 'embarked': 'categorical'\n        , 'pclass': 'categorical'\n    }\n\n    ml_predictor = Predictor(type_of_estimator='classifier', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_titanic_train, scoring=always_return_ten_thousand)\n\n    test_score = ml_predictor.score(df_titanic_test, df_titanic_test.survived)\n\n    print('test_score')\n    print(test_score)\n\n    assert test_score == -10000\n\n\n\n\n"""
tests/core_tests/test_prediction_intervals.py,0,"b""import os\nimport sys\nsys.path = [os.path.abspath(os.path.dirname(__file__))] + sys.path\nsys.path = [os.path.abspath(os.path.dirname(os.path.dirname(__file__)))] + sys.path\n\n\nos.environ['is_test_suite'] = 'True'\n\nfrom auto_ml import Predictor\n\nimport dill\nfrom nose.tools import assert_equal, assert_not_equal, with_setup\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nimport utils_testing as utils\n\n\n\ndef test_predict_uncertainty_true():\n    np.random.seed(0)\n\n    df_boston_train, df_boston_test = utils.get_boston_regression_dataset()\n\n    column_descriptions = {\n        'MEDV': 'output'\n        , 'CHAS': 'categorical'\n    }\n\n    ml_predictor = Predictor(type_of_estimator='regressor', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_boston_train, predict_intervals=True)\n\n    intervals = ml_predictor.predict_intervals(df_boston_test)\n\n    assert isinstance(intervals, pd.DataFrame)\n    assert intervals.shape[0] == df_boston_test.shape[0]\n\n    result_list = ml_predictor.predict_intervals(df_boston_test, return_type='list')\n\n    assert isinstance(result_list, list)\n    assert len(result_list) == df_boston_test.shape[0]\n    for idx, row in enumerate(result_list):\n        assert isinstance(row, list)\n        assert len(row) == 3\n\n    singles = df_boston_test.head().to_dict('records')\n\n    for row in singles:\n        result = ml_predictor.predict_intervals(row)\n        assert isinstance(result, dict)\n        assert 'prediction' in result\n        assert 'interval_0.05' in result\n        assert 'interval_0.95' in result\n\n    for row in singles:\n        result = ml_predictor.predict_intervals(row, return_type='list')\n        assert isinstance(result, list)\n        assert len(result) == 3\n\n    df_intervals = ml_predictor.predict_intervals(df_boston_test, return_type='df')\n    assert isinstance(df_intervals, pd.DataFrame)\n\n    try:\n        ml_predictor.predict_intervals(df_boston_test, return_type='this will not work')\n        assert False\n    except ValueError:\n        assert True\n\n\ndef test_prediction_intervals_actually_work():\n    np.random.seed(0)\n\n    df_boston_train, df_boston_test = utils.get_boston_regression_dataset()\n\n    column_descriptions = {\n        'MEDV': 'output'\n        , 'CHAS': 'categorical'\n    }\n\n    ml_predictor = Predictor(type_of_estimator='regressor', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_boston_train, predict_intervals=[0.05, 0.95])\n\n    df_boston_test = df_boston_test.reset_index(drop=True)\n    intervals = ml_predictor.predict_intervals(df_boston_test)\n    actuals = df_boston_test.MEDV\n\n    count_under = 0\n    count_over = 0\n    # print(intervals)\n    for idx, row in intervals.iterrows():\n        actual = actuals.iloc[idx]\n\n        if actual < row['interval_0.05']:\n            count_under += 1\n        if actual > row['interval_0.95']:\n            count_over += 1\n\n    len_intervals = len(intervals)\n\n    pct_under = count_under * 1.0 / len_intervals\n    pct_over = count_over * 1.0 / len_intervals\n    # There's a decent bit of noise since this is such a small dataset\n    assert pct_under < 0.15\n    assert pct_over < 0.1\n\n\ndef test_prediction_intervals_lets_the_user_specify_number_of_intervals():\n    np.random.seed(0)\n\n    df_boston_train, df_boston_test = utils.get_boston_regression_dataset()\n\n    column_descriptions = {\n        'MEDV': 'output'\n        , 'CHAS': 'categorical'\n    }\n\n    ml_predictor = Predictor(type_of_estimator='regressor', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_boston_train, predict_intervals=True, prediction_intervals=[.2])\n\n    intervals = ml_predictor.predict_intervals(df_boston_test, return_type='list')\n\n    assert len(intervals[0]) == 2\n\n\ndef test_predict_intervals_should_fail_if_not_trained():\n    np.random.seed(0)\n\n    df_boston_train, df_boston_test = utils.get_boston_regression_dataset()\n\n    column_descriptions = {\n        'MEDV': 'output'\n        , 'CHAS': 'categorical'\n    }\n\n    ml_predictor = Predictor(type_of_estimator='regressor', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_boston_train)\n\n    try:\n        intervals = ml_predictor.predict_intervals(df_boston_test)\n        assert False\n    except ValueError:\n        assert True\n\n\n\n\ndef test_predict_intervals_takes_in_custom_intervals():\n    np.random.seed(0)\n\n    df_boston_train, df_boston_test = utils.get_boston_regression_dataset()\n\n    column_descriptions = {\n        'MEDV': 'output'\n        , 'CHAS': 'categorical'\n    }\n\n    ml_predictor = Predictor(type_of_estimator='regressor', column_descriptions=column_descriptions)\n\n    # df_boston_train = pd.concat([df_boston_train, df_boston_train, df_boston_train])\n\n    ml_predictor.train(df_boston_train, predict_intervals=[0.4, 0.6])\n\n    custom_intervals = ml_predictor.predict_intervals(df_boston_test, return_type='list')\n\n    assert isinstance(custom_intervals, list)\n\n    singles = df_boston_test.head().to_dict('records')\n\n    acceptable_keys = set(['prediction', 'interval_0.4', 'interval_0.6'])\n    for row in singles:\n        result = ml_predictor.predict_intervals(row)\n        assert isinstance(result, dict)\n        assert 'prediction' in result\n        assert 'interval_0.4' in result\n        assert 'interval_0.6' in result\n        for key in result.keys():\n            assert key in acceptable_keys\n\n    for row in singles:\n        result = ml_predictor.predict_intervals(row, return_type='list')\n        assert isinstance(result, list)\n        assert len(result) == 3\n\n    df_intervals = ml_predictor.predict_intervals(df_boston_test, return_type='df')\n    assert df_intervals.shape[0] == df_boston_test.shape[0]\n    assert isinstance(df_intervals, pd.DataFrame)\n\n\n    # Now make sure that the interval values are actually different\n    ml_predictor = Predictor(type_of_estimator='regressor', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_boston_train, predict_intervals=True)\n\n    default_intervals = ml_predictor.predict_intervals(df_boston_test, return_type='list')\n\n    # This is a super flaky test, because we've got such a small datasize, and we're trying to get distributions from it\n    len_intervals = len(custom_intervals)\n    num_failures = 0\n    for idx, custom_row in enumerate(custom_intervals):\n        default_row = default_intervals[idx]\n\n        if int(custom_row[1]) <= int(default_row[1]):\n            num_failures += 1\n            print('{} should be higher than {}'.format(custom_row[1], default_row[1]))\n        if int(custom_row[2]) >= int(default_row[2]):\n            print('{} should be lower than {}'.format(custom_row[1], default_row[1]))\n            num_failures += 1\n\n    assert num_failures < 0.18 * len_intervals\n"""
tests/core_tests/trained_transformation_pipeline_tests.py,0,"b""import os\nimport sys\nsys.path = [os.path.abspath(os.path.dirname(__file__))] + sys.path\nsys.path = [os.path.abspath(os.path.dirname(os.path.dirname(__file__)))] + sys.path\n\nos.environ['is_test_suite'] = 'True'\n\n\nfrom auto_ml import Predictor\n\nimport dill\nfrom nose.tools import assert_equal, assert_not_equal, with_setup\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nimport utils_testing as utils\n\ndef test_already_transformed_X():\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    # Take a third of our test data (a tenth of our overall data) for calibration\n    df_titanic_test, df_titanic_calibration = train_test_split(df_titanic_test, test_size=0.33, random_state=42)\n\n    column_descriptions = {\n        'survived': 'output'\n        , 'sex': 'categorical'\n        , 'embarked': 'categorical'\n        , 'pclass': 'categorical'\n    }\n\n\n    ml_predictor = Predictor(type_of_estimator='classifier', column_descriptions=column_descriptions)\n\n    # pass in return_trans_pipeline, and get the trans pipeline\n    trans_pipeline = ml_predictor.train(df_titanic_train, return_transformation_pipeline=True)\n\n    # get transformed X through transformation_only\n    X_train_transformed = ml_predictor.transform_only(df_titanic_train)\n\n    # create a new predictor\n    ml_predictor = Predictor(type_of_estimator='classifier', column_descriptions=column_descriptions)\n\n    # pass in trained trans pipeline, and make sure it works\n    ml_predictor.train(df_titanic_train, trained_transformation_pipeline=trans_pipeline)\n    test_score = ml_predictor.score(df_titanic_test, df_titanic_test.survived)\n\n    print('test_score')\n    print(test_score)\n\n    assert -0.14 < test_score < -0.12\n\n    # pass in both a trans pipeline and a previously transformed X, and make sure that works\n    ml_predictor = Predictor(type_of_estimator='classifier', column_descriptions=column_descriptions)\n    ml_predictor.train(None, trained_transformation_pipeline=trans_pipeline, transformed_X=X_train_transformed, transformed_y=df_titanic_train.survived)\n    test_score = ml_predictor.score(df_titanic_test, df_titanic_test.survived)\n\n    print('test_score')\n    print(test_score)\n\n    assert -0.14 < test_score < -0.12\n"""
tests/core_tests/uncertainty_tests.py,0,"b'import os\nimport sys\nsys.path = [os.path.abspath(os.path.dirname(__file__))] + sys.path\nsys.path = [os.path.abspath(os.path.dirname(os.path.dirname(__file__)))] + sys.path\n\nos.environ[\'is_test_suite\'] = \'True\'\n\nfrom auto_ml import Predictor\n\nimport dill\nfrom nose.tools import assert_equal, assert_not_equal, with_setup\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nimport utils_testing as utils\n\n\n\ndef test_predict_uncertainty_returns_pandas_DataFrame_for_more_than_one_value():\n    np.random.seed(0)\n\n    df_boston_train, df_boston_test = utils.get_boston_regression_dataset()\n\n    column_descriptions = {\n        \'MEDV\': \'output\'\n        , \'CHAS\': \'categorical\'\n    }\n\n    df_boston_train, uncertainty_data = train_test_split(df_boston_train, test_size=0.5)\n\n    ml_predictor = Predictor(type_of_estimator=\'regressor\', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_boston_train, perform_feature_selection=True, train_uncertainty_model=True, uncertainty_data=uncertainty_data)\n\n    uncertainties = ml_predictor.predict_uncertainty(df_boston_test)\n\n    assert isinstance(uncertainties, pd.DataFrame)\n\n\ndef test_predict_uncertainty_returns_dict_for_one_value():\n    np.random.seed(0)\n\n    df_boston_train, df_boston_test = utils.get_boston_regression_dataset()\n\n    column_descriptions = {\n        \'MEDV\': \'output\'\n        , \'CHAS\': \'categorical\'\n    }\n\n    df_boston_train, uncertainty_data = train_test_split(df_boston_train, test_size=0.5)\n\n    ml_predictor = Predictor(type_of_estimator=\'regressor\', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_boston_train, perform_feature_selection=True, train_uncertainty_model=True, uncertainty_data=uncertainty_data)\n\n    test_list = df_boston_test.to_dict(\'records\')\n\n    for item in test_list:\n        prediction = ml_predictor.predict_uncertainty(item)\n        assert isinstance(prediction, dict)\n\n\ndef test_score_uncertainty():\n    np.random.seed(0)\n\n    df_boston_train, df_boston_test = utils.get_boston_regression_dataset()\n\n    column_descriptions = {\n        \'MEDV\': \'output\'\n        , \'CHAS\': \'categorical\'\n    }\n\n    df_boston_train, uncertainty_data = train_test_split(df_boston_train, test_size=0.5)\n\n    ml_predictor = Predictor(type_of_estimator=\'regressor\', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_boston_train, perform_feature_selection=True, train_uncertainty_model=True, uncertainty_data=uncertainty_data)\n\n    uncertainty_score = ml_predictor.score_uncertainty(df_boston_test, df_boston_test.MEDV)\n\n    print(\'uncertainty_score\')\n    print(uncertainty_score)\n\n    assert uncertainty_score > -0.2\n\n\ndef test_calibrate_uncertainty():\n    np.random.seed(0)\n\n    df_boston_train, df_boston_test = utils.get_boston_regression_dataset()\n\n    column_descriptions = {\n        \'MEDV\': \'output\'\n        , \'CHAS\': \'categorical\'\n    }\n\n    df_boston_train, uncertainty_data = train_test_split(df_boston_train, test_size=0.5)\n    uncertainty_data, uncertainty_calibration_data = train_test_split(uncertainty_data, test_size=0.5)\n\n    ml_predictor = Predictor(type_of_estimator=\'regressor\', column_descriptions=column_descriptions)\n\n    uncertainty_calibration_settings = {\n        \'num_buckets\': 3\n        , \'percentiles\': [25, 50, 75]\n    }\n    ml_predictor.train(df_boston_train, perform_feature_selection=True, train_uncertainty_model=True, uncertainty_data=uncertainty_data, calibrate_uncertainty=True, uncertainty_calibration_settings=uncertainty_calibration_settings, uncertainty_calibration_data=uncertainty_calibration_data)\n\n    uncertainty_score = ml_predictor.predict_uncertainty(df_boston_test)\n\n\n    assert \'percentile_25_delta\' in list(uncertainty_score.columns)\n    assert \'percentile_50_delta\' in list(uncertainty_score.columns)\n    assert \'percentile_75_delta\' in list(uncertainty_score.columns)\n    assert \'bucket_num\' in list(uncertainty_score.columns)\n\n\n    # API:\n    # calibrate_uncertainty=False\n    # uncertainty_calibration_settings = {\n    #    \'num_buckets\': 5\n    #    , \'percentiles\': [25, 50, 75]\n    # }\n    # uncertainty_calibration_data = None\n    # methodology:\n    # 1. get predictions on our uncertainty_calibration_data\n    # 2. get actual deltas between true values, and predicted values for each row\n        # we will need to make sure the uncertainty_calibration_data has a y column that contains the true values to our base regression problem\n    # 3. divide our uc_data into ""num_buckets"" buckets based on the predicted_uncertainty percentages\n        # ""here is the group of deliveries we predicted 0-20% uncertainty for""\n        # worth noting that the buckets will probably be based on the distribution of hte uc_data, rather than on fixed percentage intervals. so, for 5 buckets, we won\'t have 0-20, 20 - 40, etc., we will instead have ""the lowest 20% of uncertainty predictions go from 0-1%, the next lowest 20% of buckets fall from 1-6%"", etc.\n    # 4. for each bucket, figure out what the actual deltas are at each percentile\n        # so, for this lowest predicted 20% of the uc_data, their 13th percentile actual delta was...\n        # maybe also include the rmse and std of this group\n    # essentially, what we\'re getting is:\n        # the model predicted a base value of A for the regression problem. THe uncertainty model predicted a probability of B that we will be off by some amount. For all rows where we predicted roughly B, what was the distribution by which we were actually off?\n\n    # TODO:\n    # Do we want to have a score_uncertainty function? all it would do is probably call .score on the underlying model (nested one level)\n    # we definitely want to have a calibrate_uncertainty function\n\n\n\n    # test_score = ml_predictor.score(df_boston_test, df_boston_test.MEDV)\n\n    # print(\'test_score\')\n    # print(test_score)\n\n    # # Bumping this up since without these features our score drops\n    # lower_bound = -4.0\n    # if model_name == \'DeepLearningRegressor\':\n    #     lower_bound = -14.5\n    # if model_name == \'LGBMRegressor\':\n    #     lower_bound = -4.95\n\n\n    # assert lower_bound < test_score < -2.8\n\n    # ml_predictor.get_uncertainty_prediction(df_boston_test)\n\n'"
tests/core_tests/user_logging_tests.py,0,"b'# This set of tests id specifically designed to make sure auto_ml is user friendly- throwing useful warnings where possible about what specific actions the user can take to avoid an error, instead of throwing the non-obvious error messages that the underlying libraries will choke on.\nimport datetime\nimport dill\nfrom nose.tools import raises\nimport numpy as np\nimport os\nimport random\nimport sys\nimport warnings\nsys.path = [os.path.abspath(os.path.dirname(__file__))] + sys.path\nsys.path = [os.path.abspath(os.path.dirname(os.path.dirname(__file__)))] + sys.path\n\nos.environ[\'is_test_suite\'] = \'True\'\n\nfrom auto_ml import Predictor\nimport utils_testing as utils\n\n\n@raises(ValueError)\ndef test_bad_val_in_column_descriptions():\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n        , \'fare\': \'this_is_a_bad_value\'\n    }\n\n    with warnings.catch_warnings(record=True) as w:\n\n        ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n        print(\'we should be throwing a warning for the user to give them useful feedback\')\n        assert len(w) == 1\n\n    assert True\n\n@raises(ValueError)\ndef test_missing_output_col_in_column_descriptions():\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        # \'survived\': \'output\'\n        \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n@raises(ValueError)\ndef test_bad_val_for_type_of_estimator():\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        # \'survived\': \'output\'\n        \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'invalid_type_of_estimator\', column_descriptions=column_descriptions)\n\n\ndef test_nans_in_output_column():\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_titanic_train)\n\n    test_score = ml_predictor.score(df_titanic_test, df_titanic_test.survived)\n\n    print(\'test_score\')\n    print(test_score)\n\n    assert -0.215 < test_score < -0.13\n\ndef test_verify_features_finds_missing_prediction_features():\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n    ml_predictor.train(df_titanic_train, verify_features=True)\n\n    file_name = ml_predictor.save(str(random.random()))\n\n    with open(file_name, \'rb\') as read_file:\n        saved_ml_pipeline = dill.load(read_file)\n    os.remove(file_name)\n    try:\n        keras_file_name = file_name[:-5] + \'_keras_deep_learning_model.h5\'\n        os.remove(keras_file_name)\n    except:\n        pass\n\n\n    # Remove the ""age"" column from our prediction data\n    df_titanic_test = df_titanic_test.drop(\'age\', axis=1)\n\n    missing_features = saved_ml_pipeline.named_steps[\'final_model\'].verify_features(df_titanic_test)\n    print(\'missing_features\')\n    print(missing_features)\n\n\n    print(""len(missing_features[\'prediction_not_training\'])"")\n    print(len(missing_features[\'prediction_not_training\']))\n    print(""len(missing_features[\'training_not_prediction\'])"")\n    print(len(missing_features[\'training_not_prediction\']))\n    assert len(missing_features[\'prediction_not_training\']) == 0\n    assert len(missing_features[\'training_not_prediction\']) == 1\n\n\n\n\n\ndef test_verify_features_finds_missing_training_features():\n    np.random.seed(0)\n\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    # Remove the ""sibsp"" column from our training data\n    df_titanic_train = df_titanic_train.drop(\'sibsp\', axis=1)\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n    ml_predictor.train(df_titanic_train, verify_features=True)\n\n    file_name = ml_predictor.save(str(random.random()))\n\n    with open(file_name, \'rb\') as read_file:\n        saved_ml_pipeline = dill.load(read_file)\n    os.remove(file_name)\n    try:\n        keras_file_name = file_name[:-5] + \'_keras_deep_learning_model.h5\'\n        os.remove(keras_file_name)\n    except:\n        pass\n\n\n\n    missing_features = saved_ml_pipeline.named_steps[\'final_model\'].verify_features(df_titanic_test)\n    print(\'missing_features\')\n    print(missing_features)\n\n\n    print(""len(missing_features[\'prediction_not_training\'])"")\n    print(len(missing_features[\'prediction_not_training\']))\n    print(""len(missing_features[\'training_not_prediction\'])"")\n    print(len(missing_features[\'training_not_prediction\']))\n    assert len(missing_features[\'prediction_not_training\']) == 1\n    assert len(missing_features[\'training_not_prediction\']) == 0\n\n\n\n\ndef test_verify_features_finds_no_missing_features_when_none_are_missing():\n        np.random.seed(0)\n\n        df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n        column_descriptions = {\n            \'survived\': \'output\'\n            , \'sex\': \'categorical\'\n            , \'embarked\': \'categorical\'\n            , \'pclass\': \'categorical\'\n        }\n\n\n        ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n        ml_predictor.train(df_titanic_train, verify_features=True)\n\n        file_name = ml_predictor.save(str(random.random()))\n\n        with open(file_name, \'rb\') as read_file:\n            saved_ml_pipeline = dill.load(read_file)\n        os.remove(file_name)\n\n        missing_features = saved_ml_pipeline.named_steps[\'final_model\'].verify_features(df_titanic_test)\n        print(\'missing_features\')\n        print(missing_features)\n\n\n        print(""len(missing_features[\'prediction_not_training\'])"")\n        print(len(missing_features[\'prediction_not_training\']))\n        print(""len(missing_features[\'training_not_prediction\'])"")\n        print(len(missing_features[\'training_not_prediction\']))\n        assert len(missing_features[\'prediction_not_training\']) == 0\n        assert len(missing_features[\'training_not_prediction\']) == 0\n\n\ndef test_unexpected_datetime_column_handled_without_errors():\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    ml_predictor.train(df_titanic_train)\n\n    test_dict = df_titanic_test.sample(frac=0.1).to_dict(\'records\')[0]\n\n    test_dict[\'unexpected_column\'] = datetime.date.today()\n    test_dict[\'anoter_unexpected_column\'] = datetime.datetime.today()\n\n    ml_predictor.predict(test_dict)\n\n    # We want to make sure the above does not throw an error\n    assert True\n\ndef test_unmarked_categorical_column_throws_warning():\n    df_titanic_train, df_titanic_test = utils.get_titanic_binary_classification_dataset()\n\n    column_descriptions = {\n        \'survived\': \'output\'\n        # This is the column we are ""forgetting"" to mark as categorical\n        # , \'sex\': \'categorical\'\n        , \'embarked\': \'categorical\'\n        , \'pclass\': \'categorical\'\n    }\n\n    ml_predictor = Predictor(type_of_estimator=\'classifier\', column_descriptions=column_descriptions)\n\n    with warnings.catch_warnings(record=True) as caught_w:\n\n        ml_predictor.train(df_titanic_train)\n        print(\'we should be throwing a warning for the user to give them useful feedback on the unlabeled categorical column\')\n        assert len(caught_w) == 1\n\n\n    ml_predictor.predict(df_titanic_test)\n\n    # We want to make sure the above does not throw an error\n    assert True\n\n\n\n'"
