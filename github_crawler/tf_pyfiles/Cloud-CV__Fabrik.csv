file_path,api_count,code
manage.py,0,"b'#!/usr/bin/env python\nimport os\nimport sys\n\nif __name__ == ""__main__"":\n    os.environ.setdefault(""DJANGO_SETTINGS_MODULE"", ""settings"")\n\n    from django.core.management import execute_from_command_line\n\n    execute_from_command_line(sys.argv)\n'"
backendAPI/__init__.py,0,b''
backendAPI/admin.py,0,b'# Register your models here.\nfrom django.contrib import admin # noqa\n'
backendAPI/apps.py,0,"b""# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.apps import AppConfig\n\n\nclass BackendapiConfig(AppConfig):\n    name = 'backendAPI'\n"""
backendAPI/urls.py,0,"b""from django.conf.urls import url\nfrom views import check_login\n\nurlpatterns = [\n    url(r'^checkLogin$', check_login)\n]\n"""
backendAPI/views.py,0,"b""# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\nfrom django.http import JsonResponse\nfrom django.contrib.auth.models import User\n\n\ndef check_login(request):\n    try:\n        if request.GET.get('isOAuth') == 'false':\n            username = request.GET['username']\n            password = request.GET['password']\n            user = User.objects.get(username=username)\n            user_id = user.id\n\n            if not user.check_password(password):\n                return JsonResponse({\n                    'result': False,\n                    'error': 'Please enter valid credentials'\n                })\n\n            is_authenticated = user.is_authenticated()\n            if (is_authenticated):\n                username = user.username\n\n            return JsonResponse({\n                'result': is_authenticated,\n                'user_id': user_id,\n                'username': username,\n            })\n        else:\n            user = User.objects.get(username=request.user.username)\n            user_id = user.id\n            username = 'Anonymous'\n\n            is_authenticated = user.is_authenticated()\n            if (is_authenticated):\n                username = user.username\n\n            return JsonResponse({\n                'result': is_authenticated,\n                'user_id': user_id,\n                'username': username\n            })\n    except Exception as e:\n        return JsonResponse({\n            'result': False,\n            'error': str(e)\n        })\n"""
caffe_app/__init__.py,0,b''
caffe_app/admin.py,0,"b'# Register your models here.\nfrom django.contrib import admin\nfrom .models import SharedWith, Network\n\nadmin.site.register(SharedWith)\nadmin.site.register(Network)\n'"
caffe_app/apps.py,0,"b""from __future__ import unicode_literals\n\nfrom django.apps import AppConfig\n\n\nclass CaffeAppConfig(AppConfig):\n    name = 'caffe_app'\n"""
caffe_app/consumers.py,0,"b'import json\nimport yaml\nimport urlparse\nfrom channels import Group\nfrom channels.auth import channel_session_user, channel_session_user_from_http\nfrom caffe_app.models import Network, NetworkVersion, NetworkUpdates\nfrom ide.views import get_network_version\nfrom ide.tasks import export_caffe_prototxt, export_keras_json\n\n\ndef create_network_version(network, netObj):\n    # creating a unique version of network to allow revert and view hitory\n    network_version = NetworkVersion(network=netObj)\n    network_version.network_def = network\n    network_version.save()\n    return network_version\n\n\ndef create_network_update(network_version, updated_data, tag):\n    network_update = NetworkUpdates(network_version=network_version,\n                                    updated_data=updated_data,\n                                    tag=tag)\n    return network_update\n\n\ndef fetch_network_version(netObj):\n    network_version = NetworkVersion.objects.filter(network=netObj).order_by(\'-created_on\')[0]\n    updates_batch = NetworkUpdates.objects.filter(network_version=network_version)\n\n    # Batching updates\n    # Note - size of batch is 20 for now, optimization can be done\n    if len(updates_batch) == 2:\n        data = get_network_version(netObj)\n        network_version = NetworkVersion(network=netObj, network_def=json.dumps(data[\'network\']))\n        network_version.save()\n\n        network_update = NetworkUpdates(network_version=network_version,\n                                        updated_data=json.dumps({\'nextLayerId\': data[\'next_layer_id\']}),\n                                        tag=\'CheckpointCreated\')\n        network_update.save()\n    return network_version\n\n\ndef update_data(data, required_data, version_id=0):\n    \'\'\'\n        Parses data to include only required keys and returns the required object\n    \'\'\'\n\n    updated_data = {key: data[key] for key in required_data}\n    group_data = updated_data.copy()\n    group_data[\'action\'] = data[\'action\']\n\n    if (\'randomId\' in data):\n        group_data[\'randomId\'] = data[\'randomId\']\n    group_data[\'version_id\'] = version_id\n\n    group_data = {""text"": json.dumps(group_data)}\n\n    return updated_data, group_data\n\n\n@channel_session_user_from_http\ndef ws_connect(message):\n    print(\'Connection being established...\')\n    message.reply_channel.send({\n        \'accept\': True\n    })\n    # extracting id of network from url params\n    params = urlparse.parse_qs(message.content[\'query_string\'])\n    networkId = params.get(\'id\', (\'Not Supplied\',))[0]\n    message.channel_session[\'networkId\'] = networkId\n    # adding socket to a group based on networkId to send updates of network\n    Group(\'model-{0}\'.format(networkId)).add(message.reply_channel)\n\n\n@channel_session_user\ndef ws_disconnect(message):\n    networkId = message.channel_session[\'networkId\']\n    Group(\'model-{0}\'.format(networkId)).discard(message.reply_channel)\n    print(\'Disconnected...\')\n\n\n@channel_session_user\ndef ws_receive(message):\n    print(\'Message received...\')\n    # param initialization\n    data = yaml.safe_load(message[\'text\'])\n    action = data[\'action\']\n\n    update_params = {\n        \'UpdateHighlight\': [\'addHighlightTo\', \'removeHighlightFrom\', \'userId\', \'highlightColor\', \'username\'],\n        \'UpdateParam\': [\'layerId\', \'param\', \'value\', \'isProp\'],\n        \'DeleteLayer\': [\'layerId\'],\n        \'AddLayer\': [\'layer\', \'layerId\', \'prevLayerId\', \'nextLayerId\'],\n        \'AddComment\': [\'layerId\', \'comment\']\n    }\n\n    if (\'networkId\' in message.channel_session):\n        networkId = message.channel_session[\'networkId\']\n\n    if (action == \'ExportNet\'):\n        # async export call\n        framework = data[\'framework\']\n        net = data[\'net\']\n        net_name = data[\'net_name\']\n\n        reply_channel = message.reply_channel.name\n\n        if (framework == \'caffe\'):\n            export_caffe_prototxt.delay(net, net_name, reply_channel)\n        elif (framework == \'keras\'):\n            export_keras_json.delay(net, net_name, False, reply_channel)\n        elif (framework == \'tensorflow\'):\n            export_keras_json.delay(net, net_name, True, reply_channel)\n\n    elif (action == \'UpdateHighlight\'):\n        group_data = update_data(data, update_params[\'UpdateHighlight\'])[1]\n\n        Group(\'model-{0}\'.format(networkId)).send(group_data)\n    elif (action in update_params):\n        # get the net object on which update is made\n        netObj = Network.objects.get(id=int(networkId))\n        network_version = fetch_network_version(netObj)\n\n        updated_data, group_data = update_data(data, update_params[action])\n\n        network_update = create_network_update(network_version, json.dumps(updated_data), data[\'action\'])\n        network_update.save()\n\n        Group(\'model-{0}\'.format(networkId)).send(group_data)\n'"
caffe_app/models.py,0,"b""from __future__ import unicode_literals\nfrom django.contrib.auth.models import User\n\nfrom django.db import models\nfrom django.contrib.postgres.fields import JSONField\n\n\nclass Network(models.Model):\n    name = models.CharField(max_length=100)\n    author = models.ForeignKey(User, blank=True, null=True)\n    public_sharing = models.BooleanField(default=False)\n\n    def __unicode__(self):\n        return self.id\n\n\nclass NetworkVersion(models.Model):\n    network = models.ForeignKey(Network)\n    network_def = JSONField()\n    created_on = models.DateTimeField(auto_now_add=True)\n\n    def __unicode__(self):\n        return self.id\n\n\nclass NetworkUpdates(models.Model):\n    network_version = models.ForeignKey(NetworkVersion)\n    updated_data = JSONField()\n    tag = models.CharField(max_length=100)\n    created_on = models.DateTimeField(auto_now_add=True)\n\n    def __unicode__(self):\n        return self.tag\n\n\nclass SharedWith(models.Model):\n    ACCESS_PRIVILEGE = (\n        ('E', 'Can Edit'),\n        ('V', 'Can View'),\n        ('C', 'Can Comment')\n    )\n    network = models.ForeignKey(Network)\n    user = models.ForeignKey(User)\n    access_privilege = models.CharField(max_length=1, choices=ACCESS_PRIVILEGE)\n    created_on = models.DateField(auto_now_add=True)\n    updated_on = models.DateField(auto_now_add=True)\n\n    def __unicode__(self):\n        return self.user.username\n"""
caffe_app/tests.py,0,b''
caffe_app/urls.py,0,"b""from django.conf.urls import url\nfrom views.import_prototxt import import_prototxt\nfrom views.export_prototxt import export_to_caffe\n\nurlpatterns = [\n    url(r'^export$', export_to_caffe, name='caffe-export'),\n    url(r'^import$', import_prototxt, name='caffe-import'),\n]\n"""
ide/__init__.py,0,b'from __future__ import absolute_import\n\n# This will make sure the app is always imported when\n# Django starts so that shared_task will use this app.\nfrom .celery_app import app as celery_app  # noqa\n'
ide/asgi.py,0,"b'""""""\nASGI entrypoint. Configures Django and then runs the application\ndefined in the ASGI_APPLICATION setting.\n""""""\n\nimport os\n\nfrom channels.asgi import get_channel_layer\n\nos.environ.setdefault(""DJANGO_SETTINGS_MODULE"", ""settings"")\n\nchannel_layer = get_channel_layer()\n'"
ide/celery_app.py,0,"b""from __future__ import absolute_import\nimport os\nfrom celery import Celery\nfrom django.conf import settings\n\n# set the default Django settings module for the 'celery' program.\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'settings')\napp = Celery('app', broker='redis://redis:6379/0', backend='redis://redis:6379/0', include=['ide.tasks'])\n\n# Using a string here means the worker will not have to\n# pickle the object when using Windows.\napp.config_from_object('settings')\napp.autodiscover_tasks(lambda: settings.INSTALLED_APPS)\n\n\n@app.task(bind=True)\ndef debug_task(self):\n    print('Request: {0!r}'.format(self.request))\n"""
ide/routing.py,0,"b'from channels.routing import route, include\nfrom channels.staticfiles import StaticFilesConsumer # noqa: ignore=F405\nfrom caffe_app.consumers import ws_connect, ws_disconnect, ws_receive\n\n# routes defined for channel calls\n# this is similar to the Django urls, but specifically for Channels\nws_routing = [\n    route(\'websocket.connect\', ws_connect),\n    route(\'websocket.receive\', ws_receive),\n    route(\'websocket.disconnect\', ws_disconnect)\n]\n\nchannel_routing = [\n    include(ws_routing, path=r""^/ws/connect""),\n]\n'"
ide/tasks.py,1,"b'import json\nimport os\nimport sys\nimport imp\nimport yaml\nimport random\nimport string\nfrom datetime import datetime\nfrom channels import Channel\nfrom utils.jsonToPrototxt import json_to_prototxt\nfrom celery.decorators import task\nfrom keras.models import Model\nfrom keras_app.views.layers_export import data, convolution, deconvolution, pooling, dense, dropout, embed,\\\n    recurrent, batch_norm, activation, flatten, reshape, eltwise, concat, upsample, locally_connected,\\\n    permute, repeat_vector, regularization, masking, gaussian_noise, gaussian_dropout, alpha_dropout, \\\n    bidirectional, time_distributed, lrn, depthwiseConv\nfrom keras_app.custom_layers import config as custom_layers_config\nfrom keras.models import model_from_json\nimport tensorflow as tf\nfrom keras import backend as K\n\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\n\ndef randomword(length):\n    return \'\'.join(random.choice(string.lowercase) for i in range(length))\n\n\n@task(name=""export_to_caffe"", bind=True)\ndef export_caffe_prototxt(self, net, net_name, reply_channel):\n    net = yaml.safe_load(net)\n    if net_name == \'\':\n        net_name = \'Net\'\n    try:\n        prototxt, input_dim = json_to_prototxt(net, net_name)\n        randomId = datetime.now().strftime(\'%Y%m%d%H%M%S\')+randomword(5)\n\n        with open(BASE_DIR + \'/media/\' + randomId + \'.prototxt\', \'w+\') as f:\n            f.write(prototxt)\n\n        Channel(reply_channel).send({\n            \'text\': json.dumps({\n                \'result\': \'success\',\n                \'action\': \'ExportNet\',\n                \'name\': randomId + \'.prototxt\',\n                \'url\': \'/media/\' + randomId + \'.prototxt\'\n            })\n        })\n    except:\n        Channel(reply_channel).send({\n            \'text\': json.dumps({\n                \'result\': \'error\',\n                \'action\': \'ExportNet\',\n                \'error\': str(sys.exc_info()[1])\n            })\n        })\n\n\n@task(name=""export_to_keras"")\ndef export_keras_json(net, net_name, is_tf, reply_channel):\n    net = yaml.safe_load(net)\n    if net_name == \'\':\n        net_name = \'Net\'\n\n    layer_map = {\n        \'ImageData\': data,\n        \'Data\': data,\n        \'Input\': data,\n        \'WindowData\': data,\n        \'MemoryData\': data,\n        \'DummyData\': data,\n        \'InnerProduct\': dense,\n        \'Softmax\': activation,\n        \'SELU\': activation,\n        \'Softplus\': activation,\n        \'Softsign\': activation,\n        \'ReLU\': activation,\n        \'TanH\': activation,\n        \'Sigmoid\': activation,\n        \'HardSigmoid\': activation,\n        \'Linear\': activation,\n        \'Dropout\': dropout,\n        \'Flatten\': flatten,\n        \'Reshape\': reshape,\n        \'Permute\': permute,\n        \'RepeatVector\': repeat_vector,\n        \'Regularization\': regularization,\n        \'Masking\': masking,\n        \'Convolution\': convolution,\n        \'Deconvolution\': deconvolution,\n        \'DepthwiseConv\': depthwiseConv,\n        \'Upsample\': upsample,\n        \'Pooling\': pooling,\n        \'LocallyConnected\': locally_connected,\n        \'RNN\': recurrent,\n        \'GRU\': recurrent,\n        \'LSTM\': recurrent,\n        \'Embed\': embed,\n        \'Concat\': concat,\n        \'Eltwise\': eltwise,\n        \'PReLU\': activation,\n        \'ELU\': activation,\n        \'ThresholdedReLU\': activation,\n        \'BatchNorm\': batch_norm,\n        \'GaussianNoise\': gaussian_noise,\n        \'GaussianDropout\': gaussian_dropout,\n        \'AlphaDropout\': alpha_dropout,\n        \'Scale\': \'\',\n        \'TimeDistributed\': time_distributed,\n        \'Bidirectional\': bidirectional\n    }\n\n    custom_layers_map = {\n        \'LRN\': lrn\n    }\n\n    # Remove any duplicate activation layers (timedistributed and bidirectional layers)\n    redundant_layers = []\n    for layerId in net:\n        if (net[layerId][\'connection\'][\'input\']\n                and net[net[layerId][\'connection\'][\'input\'][0]][\'info\'][\'type\'] in\n                [\'TimeDistributed\', \'Bidirectional\']):\n            if len(net[layerId][\'connection\'][\'output\']) > 0:\n                target = net[layerId][\'connection\'][\'output\'][0]\n                outputs = net[target][\'connection\'][\'output\']\n                if len(outputs) > 0:\n                    net[layerId][\'connection\'][\'output\'] = outputs\n                    for j in outputs:\n                        net[j][\'connection\'][\'input\'] = [\n                            x if (x != target) else layerId for x in net[j][\'connection\'][\'input\']]\n                    redundant_layers.append(target)\n        elif (net[layerId][\'info\'][\'type\'] == \'Input\'\n              and net[net[layerId][\'connection\'][\'output\'][0]][\'info\'][\'type\'] in\n              [\'TimeDistributed\', \'Bidirectional\']):\n            connected_layer = net[layerId][\'connection\'][\'output\'][0]\n            net[connected_layer][\'params\'][\'batch_input_shape\'] = net[layerId][\'params\'][\'dim\']\n    for i in redundant_layers:\n        del net[i]\n\n    # Check if conversion is possible\n    # Note : Error handling can be improved further\n    error = []\n    custom_layers = []\n    for key, value in custom_layers_map.iteritems():\n        layer_map[key] = value\n    for layerId in net:\n        layerType = net[layerId][\'info\'][\'type\']\n        if (layerType in custom_layers_map):\n            custom_layers.append(layerType)\n        if (\'Loss\' in layerType or layerType ==\n                \'Accuracy\' or layerType in layer_map):\n            pass\n        else:\n            error.append(layerId + \'(\' + layerType + \')\')\n            break\n    if len(error):\n        Channel(reply_channel).send({\n            \'text\': json.dumps({\n                \'result\': \'error\',\n                \'action\': \'ExportNet\',\n                \'error\': \'Cannot convert \' + \', \'.join(error) + \' to Keras\'\n            })\n        })\n        return\n\n    stack = []\n    net_out = {}\n    dataLayers = [\'ImageData\', \'Data\', \'HDF5Data\', \'Input\', \'WindowData\',\n                  \'MemoryData\', \'DummyData\', \'Bidirectional\',\n                  \'TimeDistributed\']\n    processedLayer = {}\n    inputLayerId = []\n    outputLayerId = []\n\n    def isProcessPossible(layerId):\n        inputs = net[layerId][\'connection\'][\'input\']\n        for layerId in inputs:\n            if processedLayer[layerId] is False:\n                return False\n        return True\n\n    # Finding the data layer\n    for layerId in net:\n        processedLayer[layerId] = False\n        if (net[layerId][\'info\'][\'type\'] == \'Python\'):\n            error.append(layerId + \'(Python)\')\n            continue\n        if(net[layerId][\'info\'][\'type\'] in dataLayers):\n            stack.append(layerId)\n        if (not net[layerId][\'connection\'][\'input\']):\n            inputLayerId.append(layerId)\n        if (not net[layerId][\'connection\'][\'output\']):\n            outputLayerId.append(layerId)\n    if len(error):\n        Channel(reply_channel).send({\n            \'text\': json.dumps({\n                \'result\': \'error\',\n                \'action\': \'ExportNet\',\n                \'error\': \'Cannot convert \' + \', \'.join(error) + \' to Keras\'\n            })\n        })\n        return\n\n    while(len(stack)):\n        if (\'Loss\' in net[layerId][\'info\'][\'type\'] or\n                net[layerId][\'info\'][\'type\'] == \'Accuracy\'):\n            pass\n        elif (net[layerId][\'info\'][\'type\'] in layer_map):\n            i = len(stack) - 1\n            while isProcessPossible(stack[i]) is False:\n                i = i - 1\n            layerId = stack[i]\n            stack.remove(layerId)\n            if (net[layerId][\'info\'][\'type\'] != \'Scale\'):\n                layer_in = [net_out[inputId]\n                            for inputId in net[layerId][\'connection\'][\'input\']]\n            # Need to check if next layer is Scale\n            if (net[layerId][\'info\'][\'type\'] == \'BatchNorm\'):\n                idNext = net[layerId][\'connection\'][\'output\'][0]\n                nextLayer = net[idNext]\n                # If the BN layer is followed by Scale, then we need to pass both layers\n                # as in Keras parameters from both go into one single layer\n                net_out.update(layer_map[net[layerId][\'info\'][\'type\']](\n                    net[layerId], layer_in, layerId, idNext, nextLayer))\n            elif (net[layerId][\'info\'][\'type\'] == \'Scale\'):\n                type = net[net[layerId][\'connection\']\n                           [\'input\'][0]][\'info\'][\'type\']\n                if (type != \'BatchNorm\'):\n                    Channel(reply_channel).send({\n                        \'text\': json.dumps({\n                            \'result\': \'error\',\n                            \'action\': \'ExportNet\',\n                            \'error\': \'Cannot convert \' +\n                                      net[layerId][\'info\'][\'type\'] + \' to Keras\'\n                        })\n                    })\n\n            elif (net[layerId][\'info\'][\'type\'] in [\'TimeDistributed\', \'Bidirectional\']):\n                idNext = net[layerId][\'connection\'][\'output\'][0]\n                net_out.update(\n                    layer_map[net[layerId][\'info\'][\'type\']](layerId, idNext, net, layer_in, layer_map))\n                if len(net[idNext][\'connection\'][\'output\']) > 0:\n                    net[net[idNext][\'connection\'][\'output\'][0]\n                        ][\'connection\'][\'input\'] = [layerId]\n                processedLayer[idNext] = True\n                processedLayer[layerId] = True\n            else:\n                if (net[layerId][\'info\'][\'type\'] in layer_map):\n                    net_out.update(layer_map[net[layerId][\'info\'][\'type\']](\n                        net[layerId], layer_in, layerId))\n                else:\n                    error.append(\n                        layerId + \'(\' + net[layerId][\'info\'][\'type\'] + \')\')\n                    break\n            for outputId in net[layerId][\'connection\'][\'output\']:\n                if outputId not in stack:\n                    stack.append(outputId)\n            processedLayer[layerId] = True\n        else:\n            error.append(\n                layerId + \'(\' + net[layerId][\'info\'][\'type\'] + \')\')\n            break\n\n    if len(error) > 0:\n        Channel(reply_channel).send({\n            \'text\': json.dumps({\n                \'result\': \'error\',\n                \'action\': \'ExportNet\',\n                \'error\': \'Cannot convert \' + \', \'.join(error) + \' to Keras\'\n            })\n        })\n        return\n\n    final_input = []\n    final_output = []\n    for i in inputLayerId:\n        final_input.append(net_out[i])\n\n    for j in outputLayerId:\n        if (net[net[j][\'connection\'][\'input\'][0]][\'info\'][\'type\'] in\n                [\'TimeDistributed\', \'Bidirectional\']):\n            final_output.append(net_out[net[j][\'connection\'][\'input\'][0]])\n        else:\n            final_output.append(net_out[j])\n\n    model = Model(inputs=final_input, outputs=final_output, name=net_name)\n    json_string = Model.to_json(model)\n\n    randomId = datetime.now().strftime(\'%Y%m%d%H%M%S\') + randomword(5)\n    with open(BASE_DIR + \'/media/\' + randomId + \'.json\', \'w\') as f:\n        json.dump(json.loads(json_string), f, indent=4)\n\n    custom_layers_response = []\n    for layer in set(custom_layers):\n        layer_data = {\'name\': layer}\n        layer_data.update(custom_layers_config.config[layer])\n        custom_layers_response.append(layer_data)\n\n    if(is_tf):\n        # export part for tensorflow from keras model\n        input_file = randomId + \'.json\'\n        output_file = randomId\n\n        K.set_learning_phase(0)\n\n        output_fld = BASE_DIR + \'/media/\'\n\n        with open(output_fld + input_file, \'r\') as f:\n            json_str = f.read()\n\n        json_str = json_str.strip(""\'<>() "").replace(\'\\\'\', \'\\""\')\n        lrnLayer = imp.load_source(\'LRN\', BASE_DIR + \'/keras_app/custom_layers/lrn.py\')\n\n        model = model_from_json(json_str, {\'LRN\': lrnLayer.LRN})\n\n        sess = K.get_session()\n        tf.train.write_graph(sess.graph.as_graph_def(add_shapes=True), output_fld,\n                             output_file + \'.pbtxt\', as_text=True)\n\n        Channel(reply_channel).send({\n            \'text\': json.dumps({\n                \'result\': \'success\',\n                \'action\': \'ExportNet\',\n                \'id\': \'randomId\',\n                \'name\': randomId + \'.pbtxt\',\n                \'url\': \'/media/\' + randomId + \'.pbtxt\',\n                \'customLayers\': custom_layers_response\n            })\n        })\n    else:\n        Channel(reply_channel).send({\n            \'text\': json.dumps({\n                \'result\': \'success\',\n                \'action\': \'ExportNet\',\n                \'id\': \'randomId\',\n                \'name\': randomId + \'.json\',\n                \'url\': \'/media/\' + randomId + \'.json\',\n                \'customLayers\': custom_layers_response\n            })\n        })\n'"
ide/urls.py,0,"b""from django.conf.urls import url, include\nfrom django.contrib import admin\nfrom django.conf.urls.static import static\nfrom django.conf import settings\nfrom views import index, calculate_parameter, fetch_layer_shape\nfrom views import load_from_db, save_to_db, fetch_model_history\n\nurlpatterns = [\n    url(r'^$', index),\n    url(r'^admin/', admin.site.urls),\n    url(r'^accounts/', include('allauth.urls')),\n    url(r'^backendAPI/', include('backendAPI.urls')),\n    url(r'^caffe/', include('caffe_app.urls')),\n    url(r'^keras/', include('keras_app.urls')),\n    url(r'^tensorflow/', include('tensorflow_app.urls')),\n    url(r'^save$', save_to_db, name='saveDB'),\n    url(r'^load*', load_from_db, name='loadDB'),\n    url(r'^model_history', fetch_model_history, name='model-history'),\n    url(r'^model_parameter/', calculate_parameter, name='calculate-parameter'),\n    url(r'^layer_parameter/', fetch_layer_shape, name='fetch-layer-shape')\n] + static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT) + \\\n    static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n"""
ide/views.py,0,"b'import copy\nimport sys\nimport yaml\nimport json\n\nfrom caffe_app.models import Network, NetworkVersion, NetworkUpdates\nfrom django.shortcuts import render\nfrom django.http import JsonResponse\nfrom django.views.decorators.csrf import csrf_exempt\nfrom django.contrib.auth.models import User\nfrom utils.shapes import get_shapes, get_layer_shape, handle_concat_layer\n\n\ndef index(request):\n    return render(request, \'index.html\')\n\n\n@csrf_exempt\ndef fetch_layer_shape(request):\n    if request.method == \'POST\':\n        net = yaml.safe_load(request.POST.get(\'net\'))\n        layerId = request.POST.get(\'layerId\')\n        try:\n            net[layerId][\'shape\'] = {}\n            net[layerId][\'shape\'][\'input\'] = None\n            net[layerId][\'shape\'][\'output\'] = None\n            dataLayers = [\'ImageData\', \'Data\', \'HDF5Data\', \'Input\', \'WindowData\', \'MemoryData\', \'DummyData\']\n\n            # Obtain input shape of new layer\n            if (net[layerId][\'info\'][\'type\'] == ""Concat""):\n                for parentLayerId in net[layerId][\'connection\'][\'input\']:\n                    # Check if parent layer have shapes\n                    if (net[parentLayerId][\'shape\'][\'output\']):\n                        net[layerId][\'shape\'][\'input\'] = handle_concat_layer(net[layerId], net[parentLayerId])\n            elif (not (net[layerId][\'info\'][\'type\'] in dataLayers)):\n                if (len(net[layerId][\'connection\'][\'input\']) > 0):\n                    parentLayerId = net[layerId][\'connection\'][\'input\'][0]\n                    # Check if parent layer have shapes\n                    if (net[parentLayerId][\'shape\'][\'output\']):\n                        net[layerId][\'shape\'][\'input\'] = net[parentLayerId][\'shape\'][\'output\'][:]\n\n            # Obtain output shape of new layer\n            if (net[layerId][\'info\'][\'type\'] in dataLayers):\n                # handling Data Layers separately\n                if (\'dim\' in net[layerId][\'params\'] and len(net[layerId][\'params\'][\'dim\'])):\n                    # layers with empty dim parameter can\'t be passed\n                    net[layerId][\'shape\'][\'input\'], net[layerId][\'shape\'][\'output\'] =\\\n                            get_layer_shape(net[layerId])\n                elif (\'dim\' not in net[layerId][\'params\']):\n                    # shape calculation for layers with no dim param\n                    net[layerId][\'shape\'][\'input\'], net[layerId][\'shape\'][\'output\'] =\\\n                            get_layer_shape(net[layerId])\n            else:\n                if (net[layerId][\'shape\'][\'input\']):\n                    net[layerId][\'shape\'][\'output\'] = get_layer_shape(net[layerId])\n        except BaseException:\n            return JsonResponse({\n                \'result\': \'error\', \'error\': str(sys.exc_info()[1])})\n        return JsonResponse({\'result\': \'success\', \'net\': net})\n\n\n@csrf_exempt\ndef calculate_parameter(request):\n    if request.method == \'POST\':\n        net = yaml.safe_load(request.POST.get(\'net\'))\n        try:\n            # While calling get_shapes we need to remove the flag\n            # added in frontend to show the parameter on pane\n            netObj = copy.deepcopy(net)\n            for layerId in netObj:\n                for param in netObj[layerId][\'params\']:\n                    netObj[layerId][\'params\'][param] = netObj[layerId][\'params\'][param][0]\n            # use get_shapes method to obtain shapes of each layer\n            netObj = get_shapes(netObj)\n            for layerId in net:\n                net[layerId][\'shape\'] = {}\n                net[layerId][\'shape\'][\'input\'] = netObj[layerId][\'shape\'][\'input\']\n                net[layerId][\'shape\'][\'output\'] = netObj[layerId][\'shape\'][\'output\']\n        except BaseException:\n            return JsonResponse({\n                \'result\': \'error\', \'error\': str(sys.exc_info()[1])})\n        return JsonResponse({\'result\': \'success\', \'net\': net})\n\n\n@csrf_exempt\ndef save_to_db(request):\n    if request.method == \'POST\':\n        net = request.POST.get(\'net\')\n        net_name = request.POST.get(\'net_name\')\n        user_id = request.POST.get(\'user_id\')\n        next_layer_id = request.POST.get(\'nextLayerId\')\n        public_sharing = True\n        user = None\n        if net_name == \'\':\n            net_name = \'Net\'\n        try:\n            # making model sharing public by default for now\n            # TODO: Prvilege on Sharing\n            if user_id:\n                user_id = int(user_id)\n                user = User.objects.get(id=user_id)\n\n            # create a new model on share event\n            model = Network(name=net_name, public_sharing=public_sharing, author=user)\n            model.save()\n            # create first version of model\n            model_version = NetworkVersion(network=model, network_def=net)\n            model_version.save()\n            # create initial update for nextLayerId\n            model_update = NetworkUpdates(network_version=model_version,\n                                          updated_data=json.dumps({\'nextLayerId\': next_layer_id}),\n                                          tag=\'ModelShared\')\n            model_update.save()\n\n            return JsonResponse({\'result\': \'success\', \'id\': model.id})\n        except:\n            return JsonResponse({\'result\': \'error\', \'error\': str(sys.exc_info()[1])})\n\n\ndef create_network_version(network_def, updates_batch):\n    network_def = yaml.safe_load(network_def)\n    next_layer_id = 0\n\n    for network_update in updates_batch:\n        updated_data = json.loads(network_update.updated_data)\n        tag = network_update.tag\n\n        if \'nextLayerId\' in updated_data:\n            next_layer_id = updated_data[\'nextLayerId\']\n\n        if tag == \'UpdateParam\':\n            # Update Param UI event handling\n            param = updated_data[\'param\']\n            layer_id = updated_data[\'layerId\']\n            value = updated_data[\'value\']\n\n            if updated_data[\'isProp\']:\n                network_def[layer_id][\'props\'][param] = value\n            else:\n                network_def[layer_id][\'params\'][param][0] = value\n\n        elif tag == \'DeleteLayer\':\n            # Delete layer UI event handling\n            layer_id = updated_data[\'layerId\']\n            input_layer_ids = network_def[layer_id][\'connection\'][\'input\']\n            output_layer_ids = network_def[layer_id][\'connection\'][\'output\']\n\n            for input_layer_id in input_layer_ids:\n                network_def[input_layer_id][\'connection\'][\'output\'].remove(layer_id)\n\n            for output_layer_id in output_layer_ids:\n                network_def[output_layer_id][\'connection\'][\'input\'].remove(layer_id)\n\n            del network_def[layer_id]\n\n        elif tag == \'AddLayer\':\n            # Add layer UI event handling\n            prev_layer_id = updated_data[\'prevLayerId\']\n            new_layer_id = updated_data[\'layerId\']\n\n            if isinstance(prev_layer_id, list):\n                for layer_id in prev_layer_id:\n                    network_def[layer_id][\'connection\'][\'output\'].append(new_layer_id)\n            else:\n                network_def[prev_layer_id][\'connection\'][\'output\'].append(new_layer_id)\n            network_def[new_layer_id] = updated_data[\'layer\']\n\n        elif tag == \'AddComment\':\n            layer_id = updated_data[\'layerId\']\n            comment = updated_data[\'comment\']\n\n            if (\'comments\' not in network_def[layer_id]):\n                network_def[layer_id][\'comments\'] = []\n            network_def[layer_id][\'comments\'].append(comment)\n\n    return {\n        \'network\': network_def,\n        \'next_layer_id\': next_layer_id\n    }\n\n\ndef get_network_version(netObj):\n    network_version = NetworkVersion.objects.filter(network=netObj).order_by(\'-created_on\')[0]\n    updates_batch = NetworkUpdates.objects.filter(network_version=network_version).order_by(\'created_on\')\n\n    return create_network_version(network_version.network_def, updates_batch)\n\n\ndef get_checkpoint_version(netObj, checkpoint_id):\n    network_update = NetworkUpdates.objects.get(id=checkpoint_id)\n    network_version = network_update.network_version\n\n    updates_batch = NetworkUpdates.objects.filter(network_version=network_version)\\\n                                          .filter(created_on__lte=network_update.created_on)\\\n                                          .order_by(\'created_on\')\n    return create_network_version(network_version.network_def, updates_batch)\n\n\n@csrf_exempt\ndef load_from_db(request):\n    if request.method == \'POST\':\n        if \'proto_id\' in request.POST:\n            try:\n                model = Network.objects.get(id=int(request.POST[\'proto_id\']))\n                version_id = None\n                data = {}\n\n                if \'version_id\' in request.POST and request.POST[\'version_id\'] != \'\':\n                    # added for loading any previous version of model\n                    version_id = int(request.POST[\'version_id\'])\n                    data = get_checkpoint_version(model, version_id)\n                else:\n                    # fetch the required version of model\n                    data = get_network_version(model)\n\n                net = data[\'network\']\n                next_layer_id = data[\'next_layer_id\']\n\n                # authorizing the user for access to model\n                if not model.public_sharing:\n                    return JsonResponse({\'result\': \'error\',\n                                         \'error\': \'Permission denied for access to model\'})\n            except Exception:\n                return JsonResponse({\'result\': \'error\',\n                                     \'error\': \'No network file found\'})\n            return JsonResponse({\'result\': \'success\', \'net\': net, \'net_name\': model.name,\n                                 \'next_layer_id\': next_layer_id})\n\n    if request.method == \'GET\':\n        return index(request)\n\n\n@csrf_exempt\ndef fetch_model_history(request):\n    if request.method == \'POST\':\n        try:\n            network_id = int(request.POST[\'net_id\'])\n            network = Network.objects.get(id=network_id)\n            network_versions = NetworkVersion.objects.filter(network=network).order_by(\'created_on\')\n\n            modelHistory = {}\n            for version in network_versions:\n                network_updates = NetworkUpdates.objects.filter(network_version=version)\\\n                                                        .order_by(\'created_on\')\n                for update in network_updates:\n                    modelHistory[update.id] = update.tag\n\n            return JsonResponse({\n                \'result\': \'success\',\n                \'data\': modelHistory\n            })\n        except Exception:\n            return JsonResponse({\n                \'result\': \'error\',\n                \'error\': \'Unable to load model history\'\n            })\n'"
ide/wsgi.py,0,"b'""""""\nWSGI config for ide project.\n\nIt exposes the WSGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/1.9/howto/deployment/wsgi/\n""""""\n\nimport os\n\nfrom django.core.wsgi import get_wsgi_application\n\nos.environ.setdefault(""DJANGO_SETTINGS_MODULE"", ""settings"")\n\napplication = get_wsgi_application()\n'"
keras_app/__init__.py,0,b''
keras_app/admin.py,0,b'# Register your models here.\n'
keras_app/apps.py,0,"b""from __future__ import unicode_literals\n\nfrom django.apps import AppConfig\n\nimport startup\n\n\nclass KerasAppConfig(AppConfig):\n    name = 'keras_app'\n\nstartup.run()\n"""
keras_app/models.py,0,b'from __future__ import unicode_literals\n\n# Create your models here.\n'
keras_app/startup.py,0,"b""import os\nfrom custom_layers import config\n\n\nBASE_DIR = os.path.dirname(\n    os.path.dirname(\n        os.path.abspath(__file__)))\n\n\ndef run():\n    for key, layer in config.config.iteritems():\n        os.system('cp ' + BASE_DIR + '/keras_app/custom_layers/' + layer['filename'] + ' '\n                  + BASE_DIR + '/media')\n"""
keras_app/tests.py,0,b'# Create your tests here.\n'
keras_app/urls.py,0,"b""from django.conf.urls import url\nfrom views.import_json import import_json\nfrom views.export_json import export_json\n\nurlpatterns = [\n    url(r'^import$', import_json, name='keras-import'),\n    url(r'^export$', export_json, name='keras-export')\n]\n"""
settings/__init__.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\n\n# TODO: Add support for production environment settings\n\nimport sys\n\nTEST = [arg for arg in sys.argv if \'test\' in arg]\nif TEST:\n    print(""Using Test settings"")\n    from .test import * # noqa\nelse:\n    try:\n        from .dev import *  # noqa\n        print(""Using Dev settings"")\n    except ImportError:\n        pass\n'"
settings/common.py,0,"b'import os\n\n# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\nSTATICFILES_DIRS = (\n    os.path.join(BASE_DIR, \'ide/static\'),\n)\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = \'s9&vp1jq1yzr!1c_temg#v_)j-a)i5+@vbsekmi6pbjl4l1&u@\'\n\n# SECURITY WARNING: don\'t run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = [\'*\']\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    \'channels\',\n    \'caffe_app.apps.CaffeAppConfig\',\n    \'keras_app.apps.KerasAppConfig\',\n    \'tensorflow_app.apps.TensorflowAppConfig\',\n    \'backendAPI.apps.BackendapiConfig\',\n    \'django.contrib.admin\',\n    \'django.contrib.auth\',\n    \'django.contrib.contenttypes\',\n    \'django.contrib.sessions\',\n    \'django.contrib.messages\',\n    \'django.contrib.staticfiles\',\n    \'django.contrib.sites\',\n    \'allauth\',\n    \'allauth.account\',\n    \'allauth.socialaccount\',\n    \'allauth.socialaccount.providers.github\',\n    \'allauth.socialaccount.providers.google\'\n]\n\nMIDDLEWARE_CLASSES = [\n    \'django.middleware.security.SecurityMiddleware\',\n    \'django.contrib.sessions.middleware.SessionMiddleware\',\n    \'django.middleware.common.CommonMiddleware\',\n    \'django.middleware.csrf.CsrfViewMiddleware\',\n    \'django.contrib.auth.middleware.AuthenticationMiddleware\',\n    \'django.contrib.auth.middleware.SessionAuthenticationMiddleware\',\n    \'django.contrib.messages.middleware.MessageMiddleware\',\n    \'django.middleware.clickjacking.XFrameOptionsMiddleware\',\n]\n\nROOT_URLCONF = \'ide.urls\'\n\nTEMPLATES = [\n    {\n        \'BACKEND\': \'django.template.backends.django.DjangoTemplates\',\n        \'DIRS\': [os.path.join(BASE_DIR, \'ide/templates\')],\n        \'APP_DIRS\': True,\n        \'OPTIONS\': {\n            \'context_processors\': [\n                \'django.template.context_processors.debug\',\n                \'django.template.context_processors.request\',\n                \'django.contrib.auth.context_processors.auth\',\n                \'django.contrib.messages.context_processors.messages\',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = \'ide.wsgi.application\'\n\nSITE_ID = 1\n\n# Internationalization\n# https://docs.djangoproject.com/en/1.9/topics/i18n/\n\nLANGUAGE_CODE = \'en-us\'\n\nTIME_ZONE = \'UTC\'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/1.9/howto/static-files/\n\nSTATIC_URL = \'/static/\'\nSTATIC_ROOT = os.path.join(BASE_DIR, \'static\')\n\nMEDIA_ROOT = os.path.join(BASE_DIR, \'media\')\nMEDIA_URL = \'/media/\'\n\n\nAUTHENTICATION_BACKENDS = (\n    ""django.contrib.auth.backends.ModelBackend"",\n    ""allauth.account.auth_backends.AuthenticationBackend"",\n)\n\nLOGIN_REDIRECT_URL = \'/\'\nACCOUNT_LOGOUT_ON_GET = True\n\n\nEMAIL_BACKEND = \'django.core.mail.backends.console.EmailBackend\'\n\n\nCHANNEL_LAYERS = {\n    ""default"": {\n        ""BACKEND"": ""asgi_redis.RedisChannelLayer"",\n        ""CONFIG"": {\n            # replace redis hostname to localhost if running on local system\n            ""hosts"": [(""redis"", 6379)],\n            ""prefix"": u\'fabrik:\',\n        },\n        ""ROUTING"": ""ide.routing.channel_routing"",\n    },\n}\n\nCELERY_RESULT_BACKEND = \'redis://redis:6379/0\'\n'"
settings/dev.sample.py,0,"b'from .common import * # noqa: ignore=F405\nimport os\n\n# Database\n# https://docs.djangoproject.com/en/1.9/ref/settings/#databases\n\nDEBUG = True\n\nDATABASES = {\n    \'default\': {\n        \'ENGINE\': \'django.db.backends.postgresql_psycopg2\',\n        \'NAME\': os.environ.get(""POSTGRES_NAME"", \'postgres\'),\n        \'USER\': os.environ.get(""POSTGRES_USER"", \'postgres\'),\n        \'PASSWORD\': os.environ.get(""POSTGRES_PASSWORD"", \'postgres\'),\n        \'HOST\': os.environ.get(""POSTGRES_HOST"", \'db\'),\n        \'PORT\': os.environ.get(""POSTGRES_PORT"", 5432),\n    }\n}\n'"
settings/test.py,0,"b""from .common import * # noqa: ignore=F405\n\n# Database\n# https://docs.djangoproject.com/en/1.9/ref/settings/#databases\n\nDEBUG = False\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql_psycopg2',\n        'NAME': 'fabrik',\n        'USER': 'admin',\n        'PASSWORD': 'fabrik',\n        'HOST': 'localhost',\n        'PORT': 5432,\n    }\n}\n\nTEST = True\n"""
tensorflow_app/__init__.py,0,b''
tensorflow_app/admin.py,0,b'# Register your models here.\n'
tensorflow_app/apps.py,0,"b""from __future__ import unicode_literals\n\nfrom django.apps import AppConfig\n\n\nclass TensorflowAppConfig(AppConfig):\n    name = 'tensorflow_app'\n"""
tensorflow_app/models.py,0,b'from __future__ import unicode_literals\n\n# Create your models here.\n'
tensorflow_app/tests.py,0,b'# Create your tests here.\n'
tensorflow_app/urls.py,0,"b""from django.conf.urls import url\nfrom views.import_graphdef import import_graph_def\nfrom views.export_graphdef import export_to_tensorflow\n\nurlpatterns = [\n    url(r'^export$', export_to_tensorflow, name='tf-export'),\n    url(r'^import$', import_graph_def, name='tf-import'),\n]\n"""
tests/__init__.py,0,b''
caffe_app/migrations/__init__.py,0,b''
caffe_app/views/DB.py,0,"b""from django.shortcuts import render\n\n\ndef index(request):\n    return render(request, 'index.html')\n"""
caffe_app/views/__init__.py,0,b''
caffe_app/views/export_prototxt.py,0,"b""from django.views.decorators.csrf import csrf_exempt\nfrom django.http import JsonResponse\nimport yaml\nfrom datetime import datetime\nimport random\nimport string\nimport sys\nimport os\nfrom ide.utils.jsonToPrototxt import json_to_prototxt\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\n\ndef randomword(length):\n    return ''.join(random.choice(string.lowercase) for i in range(length))\n\n\n@csrf_exempt\ndef export_to_caffe(request):\n    # Note : Remove the views for export by adding unittest for celery tasks\n    if request.method == 'POST':\n        net = yaml.safe_load(request.POST.get('net'))\n        net_name = request.POST.get('net_name')\n        if net_name == '':\n            net_name = 'Net'\n        try:\n            prototxt, input_dim = json_to_prototxt(net, net_name)\n            randomId = datetime.now().strftime('%Y%m%d%H%M%S')+randomword(5)\n            with open(BASE_DIR+'/media/'+randomId+'.prototxt', 'w') as f:\n                f.write(prototxt)\n            return JsonResponse({'result': 'success', 'id': randomId,\n                                'name': randomId+'.prototxt', 'url': '/media/'+randomId+'.prototxt'})\n        except:\n            return JsonResponse({'result': 'error', 'error': str(sys.exc_info()[1])})\n"""
caffe_app/views/import_prototxt.py,0,"b'from django.views.decorators.csrf import csrf_exempt\nfrom django.http import JsonResponse\nfrom django.conf import settings\nimport os\nfrom caffe.proto import caffe_pb2\nfrom google.protobuf import text_format\nimport tempfile\nimport subprocess\nimport urllib2\nfrom urlparse import urlparse\n\n# ******Data Layers******\n\n\ndef ImageData(layer):\n    params = {}\n    params[\'source\'] = layer.image_data_param.source\n    params[\'batch_size\'] = layer.image_data_param.batch_size\n    params[\'rand_skip\'] = layer.image_data_param.rand_skip\n    params[\'shuffle\'] = layer.image_data_param.shuffle\n    params[\'new_height\'] = layer.image_data_param.new_height\n    params[\'new_width\'] = layer.image_data_param.new_width\n    params[\'is_color\'] = layer.image_data_param.is_color\n    params[\'root_folder\'] = layer.image_data_param.root_folder\n    return params\n\n\ndef Data(layer):\n    params = {}\n    params[\'source\'] = layer.data_param.source\n    params[\'batch_size\'] = layer.data_param.batch_size\n    params[\'backend\'] = layer.data_param.backend\n    if (params[\'backend\'] == 0):\n        params[\'backend\'] = \'LEVELDB\'\n    else:\n        params[\'backend\'] = \'LMDB\'\n    params[\'rand_skip\'] = layer.data_param.rand_skip\n    params[\'prefetch\'] = layer.data_param.prefetch\n    return params\n\n\ndef HDF5Data(layer):\n    params = {}\n    params[\'source\'] = layer.hdf5_data_param.source\n    params[\'batch_size\'] = layer.hdf5_data_param.batch_size\n    params[\'shuffle\'] = layer.hdf5_data_param.shuffle\n    return params\n\n\ndef HDF5Output(layer):\n    params = {}\n    params[\'file_name\'] = layer.hdf5_output_param.file_name\n    return params\n\n\ndef Input(layer):\n    params = {}\n    params[\'dim\'] = str(map(int, layer.input_param.shape[0].dim))[1:-1]\n    return params\n\n\ndef WindowData(layer):\n    params = {}\n    params[\'source\'] = layer.window_data_param.source\n    params[\'batch_size\'] = layer.window_data_param.batch_size\n    params[\'fg_threshold\'] = layer.window_data_param.fg_threshold\n    params[\'bg_threshold\'] = layer.window_data_param.bg_threshold\n    params[\'fg_fraction\'] = layer.window_data_param.fg_fraction\n    params[\'context_pad\'] = layer.window_data_param.context_pad\n    params[\'crop_mode\'] = layer.window_data_param.crop_mode\n    params[\'cache_images\'] = layer.window_data_param.cache_images\n    params[\'root_folder\'] = layer.window_data_param.root_folder\n    return params\n\n\ndef MemoryData(layer):\n    params = {}\n    params[\'batch_size\'] = layer.memory_data_param.batch_size\n    params[\'channels\'] = layer.memory_data_param.channels\n    params[\'height\'] = layer.memory_data_param.height\n    params[\'width\'] = layer.memory_data_param.width\n    return params\n\n\ndef DummyData(layer):\n    params = {}\n    params[\'dim\'] = str(map(int, layer.dummy_data_param.shape[0].dim))[1:-1]\n    params[\'type\'] = str(layer.dummy_data_param.data_filler[0].type)\n    return params\n\n\n# ********** Vision Layers **********\ndef Convolution(layer):\n    params = {}\n    if len(layer.convolution_param.kernel_size):\n        params[\'kernel_h\'] = layer.convolution_param.kernel_size[0]\n        params[\'kernel_w\'] = layer.convolution_param.kernel_size[0]\n    if layer.convolution_param.kernel_w:\n        params[\'kernel_w\'] = layer.convolution_param.kernel_w\n    if layer.convolution_param.kernel_h:\n        params[\'kernel_h\'] = layer.convolution_param.kernel_h\n    if len(layer.convolution_param.pad):\n        params[\'pad_h\'] = layer.convolution_param.pad[0]\n        params[\'pad_w\'] = layer.convolution_param.pad[0]\n    if layer.convolution_param.pad_w:\n        params[\'pad_w\'] = layer.convolution_param.pad_w\n    if layer.convolution_param.pad_h:\n        params[\'pad_h\'] = layer.convolution_param.pad_h\n    if len(layer.convolution_param.stride):\n        params[\'stride_h\'] = layer.convolution_param.stride_h \\\n            or layer.convolution_param.stride[0]\n        params[\'stride_w\'] = layer.convolution_param.stride_w \\\n            or layer.convolution_param.stride[0]\n    if len(layer.convolution_param.dilation):\n        params[\'dilation_h\'] = layer.convolution_param.dilation[0]\n        params[\'dilation_w\'] = layer.convolution_param.dilation[0]\n    params[\'weight_filler\'] = layer.convolution_param.weight_filler.type\n    params[\'bias_filler\'] = layer.convolution_param.bias_filler.type\n    params[\'num_output\'] = layer.convolution_param.num_output\n    params[\'use_bias\'] = layer.convolution_param.bias_term\n    params[\'layer_type\'] = \'2D\'\n    return params\n\n\ndef Pooling(layer):\n    params = {}\n    params[\'pad_h\'] = layer.pooling_param.pad_h or layer.pooling_param.pad\n    params[\'pad_w\'] = layer.pooling_param.pad_w or layer.pooling_param.pad\n    params[\'stride_h\'] = layer.pooling_param.stride_h or layer.pooling_param.stride\n    params[\'stride_w\'] = layer.pooling_param.stride_w or layer.pooling_param.stride\n    params[\'kernel_h\'] = layer.pooling_param.kernel_h or layer.pooling_param.kernel_size\n    params[\'kernel_w\'] = layer.pooling_param.kernel_w or layer.pooling_param.kernel_size\n    params[\'pool\'] = layer.pooling_param.pool\n    if (params[\'pool\'] == 0):\n        params[\'pool\'] = \'MAX\'\n    elif (params[\'pool\'] == 1):\n        params[\'pool\'] = \'AVE\'\n    else:\n        params[\'pool\'] = \'STOCHASTIC\'\n    params[\'layer_type\'] = \'2D\'\n    return params\n\n\ndef SPP(layer):\n    params = {}\n    params[\'pool\'] = layer.spp_param.pool\n    params[\'pyramid_height\'] = layer.spp_param.pyramid_height\n    return params\n\n\ndef Crop(layer):\n    params = {}\n    if layer.crop_param.axis:\n        params[\'axis\'] = layer.crop_param.axis\n    if len(layer.crop_param.offset):\n        params[\'offset\'] = layer.crop_param.offset[0]\n    return params\n\n\ndef Deconvolution(layer):\n    params = {}\n    if len(layer.convolution_param.kernel_size):\n        params[\'kernel_h\'] = layer.convolution_param.kernel_size[0]\n        params[\'kernel_w\'] = layer.convolution_param.kernel_size[0]\n    if layer.convolution_param.kernel_w:\n        params[\'kernel_w\'] = layer.convolution_param.kernel_w\n    if layer.convolution_param.kernel_h:\n        params[\'kernel_h\'] = layer.convolution_param.kernel_h\n    if len(layer.convolution_param.pad):\n        params[\'pad_h\'] = layer.convolution_param.pad[0]\n        params[\'pad_w\'] = layer.convolution_param.pad[0]\n    if layer.convolution_param.pad_w:\n        params[\'pad_w\'] = layer.convolution_param.pad_w\n    if layer.convolution_param.pad_h:\n        params[\'pad_h\'] = layer.convolution_param.pad_h\n    if len(layer.convolution_param.stride):\n        params[\'stride_h\'] = layer.convolution_param.stride_h \\\n            or layer.convolution_param.stride[0]\n        params[\'stride_w\'] = layer.convolution_param.stride_w \\\n            or layer.convolution_param.stride[0]\n    if len(layer.convolution_param.dilation):\n        params[\'dilation_h\'] = layer.convolution_param.dilation[0]\n        params[\'dilation_w\'] = layer.convolution_param.dilation[0]\n    params[\'weight_filler\'] = layer.convolution_param.weight_filler.type\n    params[\'bias_filler\'] = layer.convolution_param.bias_filler.type\n    params[\'num_output\'] = layer.convolution_param.num_output\n    params[\'use_bias\'] = layer.convolution_param.bias_term\n    return params\n\n\n# ********** Recurrent Layers **********\ndef Recurrent(layer):\n    params = {}\n    params[\'num_output\'] = layer.recurrent_param.num_output\n    params[\'weight_filler\'] = layer.recurrent_param.weight_filler.type\n    params[\'bias_filler\'] = layer.recurrent_param.bias_filler.type\n    params[\'debug_info\'] = layer.recurrent_param.debug_info\n    params[\'expose_hidden\'] = layer.recurrent_param.expose_hidden\n    return params\n\n\n# ********** Common Layers **********\ndef InnerProduct(layer):\n    params = {}\n    params[\'num_output\'] = layer.inner_product_param.num_output\n    params[\'weight_filler\'] = layer.inner_product_param.weight_filler.type\n    params[\'bias_filler\'] = layer.inner_product_param.bias_filler.type\n    params[\'use_bias\'] = layer.inner_product_param.bias_term\n    return params\n\n\ndef Dropout(layer):\n    params = {}\n    if(layer.top == layer.bottom):\n        params[\'inplace\'] = True\n    return params\n\n\ndef Embed(layer):\n    params = {}\n    params[\'bias_term\'] = layer.embed_param.bias_term\n    params[\'input_dim\'] = layer.embed_param.input_dim\n    params[\'num_output\'] = layer.embed_param.num_output\n    params[\'weight_filler\'] = layer.embed_param.weight_filler.type\n    params[\'bias_filler\'] = layer.embed_param.bias_filler.type\n    return params\n\n\n# ********** Normalisation Layers **********\ndef LRN(layer):\n    params = {}\n    if(layer.top == layer.bottom):\n        params[\'inplace\'] = True\n    params[\'local_size\'] = layer.lrn_param.local_size\n    params[\'alpha\'] = layer.lrn_param.alpha\n    params[\'beta\'] = layer.lrn_param.beta\n    params[\'k\'] = layer.lrn_param.k\n    if layer.lrn_param.norm_region:\n        params[\'norm_region\'] = layer.lrn_param.norm_region\n    else:\n        params[\'norm_region\'] = \'ACROSS_CHANNELS\'\n    return params\n\n\ndef MVN(layer):\n    params = {}\n    if(layer.top == layer.bottom):\n        params[\'inplace\'] = True\n    params[\'normalize_variance\'] = layer.mvn_param.normalize_variance\n    params[\'across_channels\'] = layer.mvn_param.across_channels\n    params[\'eps\'] = layer.mvn_param.eps\n    return params\n\n\ndef BatchNorm(layer):\n    params = {}\n    if(layer.top == layer.bottom):\n        params[\'inplace\'] = True\n    params[\'use_global_stats\'] = layer.batch_norm_param.use_global_stats\n    params[\'moving_average_fraction\'] = layer.batch_norm_param.moving_average_fraction\n    params[\'eps\'] = layer.batch_norm_param.eps\n    return params\n\n\n# ********** Activation/Neuron Layers **********\ndef ReLU(layer):\n    params = {}\n    if(layer.top == layer.bottom):\n        params[\'inplace\'] = True\n    params[\'negative_slope\'] = layer.relu_param.negative_slope\n    return params\n\n\ndef PReLU(layer):\n    params = {}\n    if(layer.top == layer.bottom):\n        params[\'inplace\'] = True\n    params[\'channel_shared\'] = layer.prelu_param.channel_shared\n    return params\n\n\ndef ELU(layer):\n    params = {}\n    if(layer.top == layer.bottom):\n        params[\'inplace\'] = True\n    params[\'alpha\'] = layer.elu_param.alpha\n    return params\n\n\ndef Sigmoid(layer):\n    params = {}\n    if(layer.top == layer.bottom):\n        params[\'inplace\'] = True\n    return params\n\n\ndef TanH(layer):\n    params = {}\n    if(layer.top == layer.bottom):\n        params[\'inplace\'] = True\n    return params\n\n\ndef AbsVal(layer):\n    params = {}\n    if(layer.top == layer.bottom):\n        params[\'inplace\'] = True\n    return params\n\n\ndef Power(layer):\n    params = {}\n    if(layer.top == layer.bottom):\n        params[\'inplace\'] = True\n    params[\'power\'] = layer.power_param.power\n    params[\'scale\'] = layer.power_param.scale\n    params[\'shift\'] = layer.power_param.shift\n    return params\n\n\ndef Exp(layer):\n    params = {}\n    if(layer.top == layer.bottom):\n        params[\'inplace\'] = True\n    params[\'base\'] = layer.exp_param.base\n    params[\'scale\'] = layer.exp_param.scale\n    params[\'shift\'] = layer.exp_param.shift\n    return params\n\n\ndef Log(layer):\n    params = {}\n    if(layer.top == layer.bottom):\n        params[\'inplace\'] = True\n    params[\'base\'] = layer.log_param.base\n    params[\'scale\'] = layer.log_param.scale\n    params[\'shift\'] = layer.log_param.shift\n    return params\n\n\ndef BNLL(layer):\n    params = {}\n    if(layer.top == layer.bottom):\n        params[\'inplace\'] = True\n    return params\n\n\ndef Threshold(layer):\n    params = {}\n    if(layer.top == layer.bottom):\n        params[\'inplace\'] = True\n    params[\'threshold\'] = layer.threshold_param.threshold\n    return params\n\n\ndef Bias(layer):\n    params = {}\n    params[\'axis\'] = layer.bias_param.axis\n    params[\'num_axes\'] = layer.bias_param.num_axes\n    params[\'filler\'] = layer.bias_param.filler.type\n    return params\n\n\ndef Scale(layer):\n    params = {}\n    params[\'scale\'] = True\n    params[\'axis\'] = layer.scale_param.axis\n    params[\'num_axes\'] = layer.scale_param.num_axes\n    params[\'filler\'] = layer.scale_param.filler.type\n    params[\'bias_term\'] = layer.scale_param.bias_term\n    params[\'bias_filler\'] = layer.scale_param.bias_filler.type\n    return params\n\n\n# ********** Utility Layers **********\ndef Flatten(layer):\n    params = {}\n    params[\'axis\'] = layer.flatten_param.axis\n    params[\'end_axis\'] = layer.flatten_param.end_axis\n    return params\n\n\ndef Reshape(layer):\n    params = {}\n    params[\'dim\'] = str(map(int, layer.reshape_param.shape.dim))[1:-1]\n    return params\n\n\ndef Slice(layer):\n    params = {}\n    params[\'slice_point\'] = str(map(int, layer.slice_param.slice_point))[1:-1]\n    params[\'axis\'] = layer.slice_param.axis\n    params[\'slice_dim\'] = layer.slice_param.slice_dim\n    return params\n\n\ndef Eltwise(layer):\n    params = {}\n    opMap = {\n        0: \'Product\',\n        1: \'Sum\',\n        2: \'Maximum\'\n    }\n    if layer.eltwise_param.operation:\n        params[\'layer_type\'] = opMap[layer.eltwise_param.operation]\n    else:\n        params[\'layer_type\'] = \'Sum\'\n    return params\n\n\ndef Reduction(layer):\n    params = {}\n    if layer.reduction_param.operation:\n        params[\'operation\'] = layer.reduction_param.operation\n        if (params[\'operation\'] == 1):\n            params[\'operation\'] = \'SUM\'\n        elif (params[\'operation\'] == 2):\n            params[\'operation\'] = \'ASUM\'\n        elif (params[\'operation\'] == 3):\n            params[\'operation\'] = \'SUMSQ\'\n        else:\n            params[\'operation\'] = \'MEAN\'\n    else:\n        params[\'operation\'] = \'SUM\'\n    params[\'axis\'] = layer.reduction_param.axis\n    params[\'coeff\'] = layer.reduction_param.coeff\n    return params\n\n\ndef ArgMax(layer):\n    params = {}\n    params[\'out_max_val\'] = layer.argmax_param.out_max_val\n    params[\'top_k\'] = layer.argmax_param.top_k\n    params[\'axis\'] = layer.argmax_param.axis\n    return params\n\n\n# ********** Loss Layers **********\ndef InfogainLoss(layer):\n    params = {}\n    params[\'source\'] = layer.infogain_loss_param.source\n    params[\'axis\'] = layer.infogain_loss_param.axis\n    return params\n\n\ndef SoftmaxWithLoss(layer):\n    params = {}\n    params[\'axis\'] = layer.softmax_param.axis\n    return params\n\n\ndef HingeLoss(layer):\n    params = {}\n    params[\'norm\'] = layer.hinge_loss_param.norm\n    return params\n\n\ndef Accuracy(layer):\n    params = {}\n    params[\'top_k\'] = layer.accuracy_param.top_k\n    params[\'axis\'] = layer.accuracy_param.axis\n    return params\n\n\ndef ContrastiveLoss(layer):\n    params = {}\n    params[\'margin\'] = layer.contrastive_loss_param.margin\n    params[\'legacy_version\'] = layer.contrastive_loss_param.legacy_version\n    return params\n\n\ndef Concat(layer):\n    params = {}\n    if (layer.concat_param.axis is not None):\n        params[\'axis\'] = layer.concat_param.axis\n    else:\n        # default value for axis of concat in caffe\n        params[\'axis\'] = 1\n    return params\n\n\n# ********** Python Layer **********\ndef Python(layer):\n    params = {}\n    if (layer.python_param.module):\n        params[\'module\'] = layer.python_param.module\n    if (layer.python_param.layer):\n        params[\'layer\'] = layer.python_param.layer\n    if (layer.python_param.param_str):\n        params.update(eval(layer.python_param.param_str))\n    if (layer.loss_weight):\n        params[\'loss_weight\'] = layer.loss_weight[0]\n    \'\'\' If its a loss layer (\'1,0\'), there will be no source endpoint, if\n    its a data layer (\'0,1\') there will be no target endpoint, otherwise there\n    will be both endpoints (\'1,1\')\'\'\'\n    if (not layer.bottom):\n        params[\'endPoint\'] = \'1, 0\'\n    elif (\'loss\' in layer.name.lower()):\n        params[\'endPoint\'] = \'0, 1\'\n    else:\n        params[\'endPoint\'] = \'1, 1\'\n    for param in params:\n        if isinstance(params[param], list):\n            params[param] = str(params[param])[1:-1]\n    return params\n\n\nlayer_dict = {\'Accuracy\': Accuracy,\n              \'WindowData\': WindowData,\n              \'Convolution\': Convolution,\n              \'SoftmaxWithLoss\': SoftmaxWithLoss,\n              \'InnerProduct\': InnerProduct,\n              \'HDF5Data\': HDF5Data,\n              \'Threshold\': Threshold,\n              \'Deconvolution\': Deconvolution,\n              \'Embed\': Embed,\n              \'Log\': Log,\n              \'Reduction\': Reduction,\n              \'Slice\': Slice,\n              \'Eltwise\': Eltwise,\n              \'Dropout\': Dropout,\n              \'PReLU\': PReLU,\n              \'BatchNorm\': BatchNorm,\n              \'MVN\': MVN,\n              \'Recurrent\': Recurrent,\n              \'Bias\': Bias,\n              \'ContrastiveLoss\': ContrastiveLoss,\n              \'Input\': Input,\n              \'Exp\': Exp,\n              \'ImageData\': ImageData,\n              \'ReLU\': ReLU,\n              \'MemoryData\': MemoryData,\n              \'Crop\': Crop,\n              \'SPP\': SPP,\n              \'Pooling\': Pooling,\n              \'Scale\': Scale,\n              \'HingeLoss\': HingeLoss,\n              \'Flatten\': Flatten,\n              \'ArgMax\': ArgMax,\n              \'BNLL\': BNLL,\n              \'Data\': Data,\n              \'HDF5Output\': HDF5Output,\n              \'ELU\': ELU,\n              \'DummyData\': DummyData,\n              \'InfogainLoss\': InfogainLoss,\n              \'TanH\': TanH,\n              \'AbsVal\': AbsVal,\n              \'Reshape\': Reshape,\n              \'Power\': Power,\n              \'Sigmoid\': Sigmoid,\n              \'Python\': Python,\n              \'LRN\': LRN,\n              \'LSTM\': Recurrent,\n              \'RNN\': Recurrent,\n              \'Concat\': Concat\n              }\n\n\n@csrf_exempt\ndef import_prototxt(request):\n    prototxtIsText = False\n    if request.method == \'POST\':\n        if (\'file\' in request.FILES) and \\\n           (request.FILES[\'file\'].content_type == \'application/octet-stream\' or\n                request.FILES[\'file\'].content_type == \'text/plain\'):\n            try:\n                prototxt = request.FILES[\'file\']\n            except Exception:\n                return JsonResponse({\'result\': \'error\',\n                                     \'error\': \'No Prototxt model file found\'})\n        elif \'sample_id\' in request.POST:\n            try:\n                prototxt = open(os.path.join(settings.BASE_DIR,\n                                             \'example\', \'caffe\',\n                                             request.POST[\'sample_id\'] + \'.prototxt\'), \'r\')\n            except Exception:\n                return JsonResponse({\'result\': \'error\',\n                                     \'error\': \'No Prototxt model file found\'})\n        elif \'config\' in request.POST:\n            prototxt = request.POST[\'config\']\n            prototxtIsText = True\n        elif \'url\' in request.POST:\n            try:\n                url = urlparse(request.POST[\'url\'])\n                if url.netloc == \'github.com\':\n                    url = url._replace(netloc=\'raw.githubusercontent.com\')\n                    url = url._replace(path=url.path.replace(\'blob/\', \'\'))\n                prototxt = urllib2.urlopen(url.geturl())\n            except Exception as ex:\n                return JsonResponse({\'result\': \'error\', \'error\': \'Invalid URL\\n\'+str(ex)})\n        caffe_net = caffe_pb2.NetParameter()\n\n        # try to convert to new prototxt\n        try:\n            if prototxtIsText is True:\n                content = prototxt\n            else:\n                content = prototxt.read()\n            tempFile = tempfile.NamedTemporaryFile()\n            tempFile.write(content)\n            tempFile.seek(0)\n            subprocess.call(""~/caffe/caffe/build/tools/upgrade_net_proto_text ""\n                            + tempFile.name + "" "" + tempFile.name, shell=True)\n            tempFile.seek(0)\n            content = tempFile.read()\n            tempFile.close()\n        except Exception as ex:\n            return JsonResponse({\'result\': \'error\', \'error\': \'Invalid Prototxt\\n\'+str(ex)})\n\n        try:\n            text_format.Merge(content, caffe_net)\n        except Exception as ex:\n            return JsonResponse({\'result\': \'error\', \'error\': \'Invalid Prototxt\\n\'+str(ex)})\n\n        net = {}\n        i = 0\n        blobMap = {}\n        net_name = caffe_net.name\n        hasTransformParam = [\'ImageData\', \'Data\', \'WindowData\']\n        for layer in caffe_net.layer:\n            id = ""l"" + str(i)\n            input = []\n\n            # this logic for phase has to be improved\n            if len(layer.include):\n                if (layer.include[0].HasField(\'phase\')):\n                    phase = layer.include[0].phase\n                else:\n                    phase = None\n            else:\n                phase = None\n\n            params = {}\n            if (layer.type in hasTransformParam):\n                params[\'scale\'] = layer.transform_param.scale\n                params[\'mirror\'] = layer.transform_param.mirror\n                params[\'crop_size\'] = layer.transform_param.crop_size\n                if (layer.transform_param.mean_file != \'\'):\n                    params[\'mean_file\'] = layer.transform_param.mean_file\n                elif (layer.transform_param.mean_value):\n                    params[\'mean_value\'] = str(\n                        map(int, layer.transform_param.mean_value))[1:-1]\n                params[\'force_color\'] = layer.transform_param.force_color\n                params[\'force_gray\'] = layer.transform_param.force_gray\n\n            if layer.type in layer_dict:\n                layer_params = layer_dict[layer.type](layer)\n                params.update(layer_params)\n\n            jsonLayer = {\n                \'info\': {\n                    \'type\': layer.type,\n                    \'phase\': phase\n                },\n                \'connection\': {\n                    \'input\': [],\n                    \'output\': []\n                },\n                \'params\': params\n            }\n            # this logic was written for a scenario where train and test layers are mixed up\n            # But as we know, the only differences between the train and test phase are:\n            # 1) input layer with different source in test\n            # 2) some accuracy layers in test\n            # If we consider these constraint, the below logic can be vastly reduced\n            for bottom_blob in layer.bottom:\n                if (bottom_blob != \'label\'):\n                    # if the current layer has a phase\n                    # then only connect with layers of same phase\n                    # if it has no phase then connect with all layers\n                    if jsonLayer[\'info\'][\'phase\'] is not None:\n                        phase = jsonLayer[\'info\'][\'phase\']\n                        for bottomLayerId in blobMap[bottom_blob]:\n                            if (net[bottomLayerId][\'info\'][\'phase\'] == phase) or\\\n                                    (net[bottomLayerId][\'info\'][\'phase\'] is None):\n                                input.append(bottomLayerId)\n                                net[bottomLayerId][\'connection\'][\'output\'].append(\n                                    id)\n                    else:\n                        for bottomLayerId in blobMap[bottom_blob]:\n                            input.append(bottomLayerId)\n                            net[bottomLayerId][\'connection\'][\'output\'].append(\n                                id)\n            for top_blob in layer.top:\n                if (top_blob != \'label\'):\n                    if top_blob in blobMap:\n                        if top_blob in layer.bottom:\n                            # check for in-place operations\n                            # layer has no phase\n                            # then remove all layer history\n                            # and add this one to the top\n                            # layer has phase then remove all layers with same phase and append this\n                            if jsonLayer[\'info\'][\'phase\'] is not None:\n                                phase = jsonLayer[\'info\'][\'phase\']\n                                for layerId in blobMap[bottom_blob]:\n                                    if net[layerId][\'info\'][\'phase\'] == phase:\n                                        blobMap[bottom_blob].remove(layerId)\n                                blobMap[top_blob].append(id)\n                            else:\n                                blobMap[top_blob] = [id]\n                        else:\n                            blobMap[top_blob].append(id)\n                    else:\n                        blobMap[top_blob] = [id]\n            jsonLayer[\'connection\'][\'input\'] = input\n            net[id] = jsonLayer\n            i = i + 1\n\n        return JsonResponse({\'result\': \'success\', \'net\': net, \'net_name\': net_name})\n'"
docs/graphvis_research/draw_graph.py,0,"b'import networkx as nx\nimport json\n\nwith open(\'state_net.json\', \'r\') as f:\n    network = json.loads(f.read())\n\nnetwork_map = {}\nfor node, params in network.items():\n    new_name = (node + \' \' + params[\'info\'][\'type\'] + ""\\n"" +\n                str(tuple(params[""shape""][""output""])))\n    network_map[node] = new_name\n\ngraph = nx.DiGraph()\nfor node, params in network.items():\n    output_nodes = params[\'connection\'][\'output\']\n    for o_node in output_nodes:\n        graph.add_edge(network_map[node], network_map[o_node])\n\ndotgraph = nx.nx_pydot.to_pydot(graph)\ndotgraph.set(\'rankdir\', \'LR\')\ndotgraph.set(\'dpi\', 300)\ndotgraph.write(\'PureVis.png\', format=\'png\')\n'"
docs/graphvis_research/print_keras_model.py,0,"b'from keras.models import model_from_json\nfrom keras.utils import plot_model\nimport sys\n\ntry:\n    json_file = sys.argv[1]\n    output_file = sys.argv[2]\nexcept KeyError:\n    print(""Usage: python print_keras_model.py <json file name> <image name>"")\n\nwith open(json_file, \'r\') as f:\n    loaded_model = model_from_json(f.read())\n\nplot_model(loaded_model,\n           to_file=json_file + \'.png\',\n           rankdir=\'LR\',\n           show_shapes=True,\n           show_layer_names=False)\n'"
docs/source/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Fabrik documentation build configuration file, created by\n# sphinx-quickstart on Sat Nov  4 01:38:47 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\n\nimport sphinx_rtd_theme\n\nfrom recommonmark.parser import CommonMarkParser\n\nsource_parsers = {\'.md\': CommonMarkParser, }\n\nextensions = [\n    \'sphinx.ext.githubpages\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = [\'.rst\', \'.md\']\n\n# The encoding of source files.\n#\n# source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'Fabrik\'\ncopyright = u\'2017, CloudCV Team\'\nauthor = u\'CloudCV Team\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = u\'0.2\'\n# The full version, including alpha/beta/rc tags.\nrelease = u\'0.2\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#\n# today = \'\'\n#\n# Else, today_fmt is used as the format for a strftime call.\n#\n# today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = []\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#\n# default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n#\n# add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#\n# add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#\n# show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n# modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n# keep_warnings = False\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n# html_theme_path = []\n\n# The name for this set of Sphinx documents.\n# ""<project> v<release> documentation"" by default.\n#\n# html_title = u\'Fabrik v0.2\'\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#\n# html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#\n# html_logo = None\n\n# The name of an image file (relative to this directory) to use as a favicon of\n# the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#\n# html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#\n# html_extra_path = []\n\n# If not None, a \'Last updated on:\' timestamp is inserted at every page\n# bottom, using the given strftime format.\n# The empty string is equivalent to \'%b %d, %Y\'.\n#\n# html_last_updated_fmt = None\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#\n# html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#\n# html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#\n# html_additional_pages = {}\n\n# If false, no module index is generated.\n#\n# html_domain_indices = True\n\n# If false, no index is generated.\n#\n# html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#\n# html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#\n# html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n#\n# html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n#\n# html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#\n# html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n# html_file_suffix = None\n\n# Language to be used for generating the HTML full-text search index.\n# Sphinx supports the following languages:\n#   \'da\', \'de\', \'en\', \'es\', \'fi\', \'fr\', \'hu\', \'it\', \'ja\'\n#   \'nl\', \'no\', \'pt\', \'ro\', \'ru\', \'sv\', \'tr\', \'zh\'\n#\n# html_search_language = \'en\'\n\n# A dictionary with options for the search language support, empty by default.\n# \'ja\' uses this config value.\n# \'zh\' user can custom change `jieba` dictionary path.\n#\n# html_search_options = {\'type\': \'default\'}\n\n# The name of a javascript file (relative to the configuration directory) that\n# implements a search results scorer. If empty, the default will be used.\n#\n# html_search_scorer = \'scorer.js\'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'Fabrikdoc\'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n     # The paper size (\'letterpaper\' or \'a4paper\').\n     #\n     # \'papersize\': \'letterpaper\',\n\n     # The font size (\'10pt\', \'11pt\' or \'12pt\').\n     #\n     # \'pointsize\': \'10pt\',\n\n     # Additional stuff for the LaTeX preamble.\n     #\n     # \'preamble\': \'\',\n\n     # Latex figure (float) alignment\n     #\n     # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'Fabrik.tex\', u\'Fabrik Documentation\',\n     u\'CloudCV Team\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#\n# latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#\n# latex_use_parts = False\n\n# If true, show page references after internal links.\n#\n# latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#\n# latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#\n# latex_appendices = []\n\n# It false, will not define \\strong, \\code, \titleref, \\crossref ... but only\n# \\sphinxstrong, ..., \\sphinxtitleref, ... To help avoid clash with user added\n# packages.\n#\n# latex_keep_old_macro_names = True\n\n# If false, no module index is generated.\n#\n# latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'fabrik\', u\'Fabrik Documentation\',\n     [author], 1)\n]\n\n# If true, show URL addresses after external links.\n#\n# man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'Fabrik\', u\'Fabrik Documentation\',\n     author, \'Fabrik\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n# Documents to append as an appendix to all manuals.\n#\n# texinfo_appendices = []\n\n# If false, no module index is generated.\n#\n# texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#\n# texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n#\n# texinfo_no_detailmenu = False\n'"
ide/utils/__init__.py,0,b''
ide/utils/jsonToPrototxt.py,0,"b'import caffe\nfrom caffe import layers as L\nimport re\n\n\ndef get_iterable(x):\n    return (x,)\n\n\n# Weight/Bias filler mapping from Keras to Caffe,\n# some which are not in Caffe were mapped to Xavier\nfillerMap = {\n    \'Zeros\': \'constant\',\n    \'Ones\': \'constant\',\n    \'Constant\': \'constant\',\n    \'RandomNormal\': \'uniform\',\n    \'RandomUniform\': \'gaussian\',\n    \'TruncatedNormal\': \'gaussian\',\n    \'VarianceScaling\': \'gaussian\',\n    \'Orthogonal\': \'xavier\',\n    \'Identity\': \'constant\',\n    \'lecun_uniform\': \'uniform\',\n    \'glorot_normal\': \'xavier\',\n    \'glorot_uniform\': \'xavier\',\n    \'he_normal\': \'msra\',\n    \'he_uniform\': \'msra\'\n}\n\n\ndef export_ImageData(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    transform_param = {}\n    transform_param[\'scale\'] = layerParams[\'scale\']\n    transform_param[\'mirror\'] = layerParams[\'mirror\']\n    transform_param[\'crop_size\'] = layerParams[\'crop_size\']\n    transform_param[\'force_color\'] = layerParams[\'force_color\']\n    transform_param[\'force_gray\'] = layerParams[\'force_gray\']\n    if (layerParams[\'mean_value\'] != \'\'):\n        transform_param[\'mean_value\'] = map(\n            int, layerParams[\'mean_value\'].split(\',\'))\n    elif (layerParams[\'mean_file\'] != \'\'):\n        transform_param[\'mean_file\'] = layerParams[\'mean_file\']\n\n    image_data_param = {}\n    image_data_param[\'source\'] = layerParams[\'source\']\n    image_data_param[\'batch_size\'] = layerParams[\'batch_size\']\n    image_data_param[\'rand_skip\'] = layerParams[\'rand_skip\']\n    image_data_param[\'shuffle\'] = layerParams[\'shuffle\']\n    image_data_param[\'new_height\'] = layerParams[\'new_height\']\n    image_data_param[\'new_width\'] = layerParams[\'new_width\']\n    image_data_param[\'is_color\'] = layerParams[\'is_color\']\n    image_data_param[\'root_folder\'] = layerParams[\'root_folder\']\n    if layerPhase is not None:\n        caffeLayer = get_iterable(L.ImageData(\n            transform_param=transform_param,\n            image_data_param=image_data_param,\n            include={\n                \'phase\': int(layerPhase)\n            }))\n        if int(layerPhase) == 0:\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns_train[key] = value\n        elif int(layerPhase) == 1:\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns_test[key] = value\n    else:\n        for ns in (ns_train, ns_test):\n            caffeLayer = get_iterable(L.ImageData(\n                transform_param=transform_param,\n                image_data_param=image_data_param))\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Data(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    transform_param = {}\n    transform_param[\'scale\'] = layerParams[\'scale\']\n    transform_param[\'mirror\'] = layerParams[\'mirror\']\n    transform_param[\'crop_size\'] = layerParams[\'crop_size\']\n    transform_param[\'force_color\'] = layerParams[\'force_color\']\n    transform_param[\'force_gray\'] = layerParams[\'force_gray\']\n    if (layerParams[\'mean_value\'] != \'\'):\n        transform_param[\'mean_value\'] = map(\n            int, layerParams[\'mean_value\'].split(\',\'))\n    elif (layerParams[\'mean_file\'] != \'\'):\n        transform_param[\'mean_file\'] = layerParams[\'mean_file\']\n\n    data_param = {}\n    data_param[\'source\'] = layerParams[\'source\']\n    data_param[\'batch_size\'] = layerParams[\'batch_size\']\n    data_param[\'backend\'] = layerParams[\'backend\']\n    if (data_param[\'backend\'] == \'LEVELDB\'):\n        data_param[\'backend\'] = 0\n    elif (data_param[\'backend\'] == \'LMDB\'):\n        data_param[\'backend\'] = 1\n    data_param[\'rand_skip\'] = layerParams[\'rand_skip\']\n    data_param[\'prefetch\'] = layerParams[\'prefetch\']\n    if layerPhase is not None:\n        caffeLayer = get_iterable(L.Data(\n            transform_param=transform_param,\n            data_param=data_param,\n            include={\n                \'phase\': int(layerPhase)\n            }))\n        if int(layerPhase) == 0:\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns_train[key] = value\n        elif int(layerPhase) == 1:\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns_test[key] = value\n    else:\n        for ns in (ns_train, ns_test):\n            caffeLayer = get_iterable(L.Data(\n                transform_param=transform_param,\n                data_param=data_param))\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_HDF5Data(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    hdf5_data_param = {}\n    hdf5_data_param[\'source\'] = layerParams[\'source\']\n    hdf5_data_param[\'batch_size\'] = layerParams[\'batch_size\']\n    hdf5_data_param[\'shuffle\'] = layerParams[\'shuffle\']\n    if layerPhase is not None:\n        caffeLayer = get_iterable(L.HDF5Data(\n            hdf5_data_param=hdf5_data_param,\n            include={\n                \'phase\': int(layerPhase)\n            }))\n        if int(layerPhase) == 0:\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns_train[key] = value\n        elif int(layerPhase) == 1:\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns_test[key] = value\n    else:\n        for ns in (ns_train, ns_test):\n            caffeLayer = get_iterable(L.HDF5Data(\n                hdf5_data_param=hdf5_data_param))\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_HDF5Output(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    hdf5_output_param = {\'file_name\': layerParams[\'file_name\']}\n    if layerPhase is not None:\n        if int(layerPhase) == 0:\n            caffeLayer = get_iterable(L.HDF5Output(\n                *[ns_train[x] for x in blobNames[layerId][\'bottom\']],\n                hdf5_output_param=hdf5_output_param,\n                include={\n                    \'phase\': int(layerPhase)\n                }))\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns_train[key] = value\n        elif int(layerPhase) == 1:\n            caffeLayer = get_iterable(L.HDF5Output(\n                *[ns_test[x] for x in blobNames[layerId][\'bottom\']],\n                hdf5_output_param=hdf5_output_param,\n                include={\n                    \'phase\': int(layerPhase)\n                }))\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns_test[key] = value\n    else:\n        for ns in (ns_train, ns_test):\n            caffeLayer = get_iterable(L.HDF5Output(\n                *[ns[x] for x in blobNames[layerId][\'bottom\']],\n                hdf5_output_param=hdf5_output_param))\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Input(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    input_param = {\'shape\': {\'dim\': map(int, layerParams[\'dim\'].split(\',\'))}}\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.Input(\n            input_param=input_param))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_WindowData(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    transform_param = {}\n    transform_param[\'scale\'] = layerParams[\'scale\']\n    transform_param[\'mirror\'] = layerParams[\'mirror\']\n    transform_param[\'crop_size\'] = layerParams[\'crop_size\']\n    transform_param[\'force_color\'] = layerParams[\'force_color\']\n    transform_param[\'force_gray\'] = layerParams[\'force_gray\']\n    if (layerParams[\'mean_value\'] != \'\'):\n        transform_param[\'mean_value\'] = map(\n            int, layerParams[\'mean_value\'].split(\',\'))\n    elif (layerParams[\'mean_file\'] != \'\'):\n        transform_param[\'mean_file\'] = layerParams[\'mean_file\']\n\n    window_data_param = {}\n    window_data_param[\'source\'] = layerParams[\'source\']\n    window_data_param[\'batch_size\'] = layerParams[\'batch_size\']\n    window_data_param[\'fg_threshold\'] = layerParams[\'fg_threshold\']\n    window_data_param[\'bg_threshold\'] = layerParams[\'bg_threshold\']\n    window_data_param[\'fg_fraction\'] = layerParams[\'fg_fraction\']\n    window_data_param[\'context_pad\'] = layerParams[\'context_pad\']\n    window_data_param[\'crop_mode\'] = layerParams[\'crop_mode\']\n    window_data_param[\'cache_images\'] = layerParams[\'cache_images\']\n    window_data_param[\'root_folder\'] = layerParams[\'root_folder\']\n    if layerPhase is not None:\n        caffeLayer = get_iterable(L.WindowData(\n            transform_param=transform_param,\n            window_data_param=window_data_param,\n            include={\n                \'phase\': int(layerPhase)\n            }))\n        if int(layerPhase) == 0:\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns_train[key] = value\n        elif int(layerPhase) == 1:\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns_test[key] = value\n    else:\n        for ns in (ns_train, ns_test):\n            caffeLayer = get_iterable(L.WindowData(\n                transform_param=transform_param,\n                window_data_param=window_data_param))\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_MemoryData(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    memory_data_param = {}\n    memory_data_param[\'batch_size\'] = layerParams[\'batch_size\']\n    memory_data_param[\'channels\'] = layerParams[\'channels\']\n    memory_data_param[\'height\'] = layerParams[\'height\']\n    memory_data_param[\'width\'] = layerParams[\'width\']\n    if layerPhase is not None:\n        caffeLayer = get_iterable(L.MemoryData(\n            memory_data_param=memory_data_param,\n            include={\n                \'phase\': int(layerPhase)\n            }))\n        if int(layerPhase) == 0:\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns_train[key] = value\n        elif int(layerPhase) == 1:\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns_test[key] = value\n    else:\n        for ns in (ns_train, ns_test):\n            caffeLayer = get_iterable(L.MemoryData(\n                memory_data_param=memory_data_param))\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_DummyData(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    # Adding a default size\n    dummy_data_param = {}\n    dummy_data_param[\'shape\'] = {\'dim\': map(\n        int, layerParams[\'dim\'].split(\',\'))}\n    dummy_data_param[\'data_filler\'] = {\'type\': layerParams[\'type\']}\n    if layerPhase is not None:\n        caffeLayer = get_iterable(L.DummyData(\n            dummy_data_param=dummy_data_param,\n            include={\n                \'phase\': int(layerPhase)\n            }))\n        if int(layerPhase) == 0:\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns_train[key] = value\n        elif int(layerPhase) == 1:\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns_test[key] = value\n    else:\n        for ns in (ns_train, ns_test):\n            caffeLayer = get_iterable(L.DummyData(\n                dummy_data_param=dummy_data_param))\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Convolution(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    convolution_param = {}\n    if layerParams[\'kernel_h\'] != \'\':\n        convolution_param[\'kernel_h\'] = int(float(layerParams[\'kernel_h\']))\n    if layerParams[\'kernel_w\'] != \'\':\n        convolution_param[\'kernel_w\'] = int(float(layerParams[\'kernel_w\']))\n    if layerParams[\'stride_h\'] != \'\':\n        convolution_param[\'stride_h\'] = int(float(layerParams[\'stride_h\']))\n    if layerParams[\'stride_w\'] != \'\':\n        convolution_param[\'stride_w\'] = int(float(layerParams[\'stride_w\']))\n    if layerParams[\'num_output\'] != \'\':\n        convolution_param[\'num_output\'] = int(float(layerParams[\'num_output\']))\n    if layerParams[\'pad_h\'] != \'\':\n        convolution_param[\'pad_h\'] = int(float(layerParams[\'pad_h\']))\n    if layerParams[\'pad_w\'] != \'\':\n        convolution_param[\'pad_w\'] = int(float(layerParams[\'pad_w\']))\n    if layerParams[\'weight_filler\'] != \'\':\n        convolution_param[\'weight_filler\'] = {}\n        try:\n            convolution_param[\'weight_filler\'][\'type\'] = \\\n                fillerMap[layerParams[\'weight_filler\']]\n        except:\n            convolution_param[\'weight_filler\'][\'type\'] = layerParams[\'weight_filler\']\n    if layerParams[\'bias_filler\'] != \'\':\n        convolution_param[\'bias_filler\'] = {}\n        try:\n            convolution_param[\'bias_filler\'][\'type\'] = \\\n                fillerMap[layerParams[\'bias_filler\']]\n        except:\n            convolution_param[\'bias_filler\'][\'type\'] = layerParams[\'bias_filler\']\n    convolution_param[\'dilation\'] = layerParams[\'dilation_h\']\n    convolution_param[\'bias_term\'] = layerParams[\'use_bias\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.Convolution(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            convolution_param=convolution_param,\n            param=[\n                {\n                    \'lr_mult\': 1\n                },\n                {\n                    \'lr_mult\': 2\n                }\n            ]))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Pooling(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    pooling_param = {}\n    if layerParams[\'kernel_h\'] != \'\':\n        pooling_param[\'kernel_h\'] = int(float(layerParams[\'kernel_h\']))\n    if layerParams[\'kernel_w\'] != \'\':\n        pooling_param[\'kernel_w\'] = int(float(layerParams[\'kernel_w\']))\n    if layerParams[\'stride_h\'] != \'\':\n        pooling_param[\'stride_h\'] = int(float(layerParams[\'stride_h\']))\n    if layerParams[\'stride_w\'] != \'\':\n        pooling_param[\'stride_w\'] = int(float(layerParams[\'stride_w\']))\n    if layerParams[\'pad_h\'] != \'\':\n        pooling_param[\'pad_h\'] = int(float(layerParams[\'pad_h\']))\n    if layerParams[\'pad_w\'] != \'\':\n        pooling_param[\'pad_w\'] = int(float(layerParams[\'pad_w\']))\n    if layerParams[\'pool\'] != \'\':\n        pool = layerParams[\'pool\']\n        if(pool == \'MAX\'):\n            pool = 0\n        elif(pool == \'AVE\'):\n            pool = 1\n        elif(pool == \'STOCHASTIC\'):\n            pool = 2\n        pooling_param[\'pool\'] = pool\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.Pooling(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            pooling_param=pooling_param))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Crop(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    crop_param = {}\n    if layerParams[\'axis\'] != \'\':\n        crop_param[\'axis\'] = int(float(layerParams[\'axis\']))\n    if layerParams[\'offset\'] != \'\':\n        crop_param[\'offset\'] = int(float(layerParams[\'offset\']))\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.Crop(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            crop_param=crop_param))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_SPP(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    spp_param = {}\n    spp_param[\'pool\'] = layerParams[\'pool\']\n    spp_param[\'pyramid_height\'] = layerParams[\'pyramid_height\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.SPP(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            spp_param=spp_param))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Deconvolution(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    convolution_param = {}\n    if layerParams[\'kernel_h\'] != \'\':\n        convolution_param[\'kernel_h\'] = int(float(layerParams[\'kernel_h\']))\n    if layerParams[\'kernel_w\'] != \'\':\n        convolution_param[\'kernel_w\'] = int(float(layerParams[\'kernel_w\']))\n    if layerParams[\'stride_h\'] != \'\':\n        convolution_param[\'stride_h\'] = int(float(layerParams[\'stride_h\']))\n    if layerParams[\'stride_w\'] != \'\':\n        convolution_param[\'stride_w\'] = int(float(layerParams[\'stride_w\']))\n    if layerParams[\'num_output\'] != \'\':\n        convolution_param[\'num_output\'] = int(float(layerParams[\'num_output\']))\n    if layerParams[\'pad_h\'] != \'\':\n        convolution_param[\'pad_h\'] = int(float(layerParams[\'pad_h\']))\n    if layerParams[\'pad_w\'] != \'\':\n        convolution_param[\'pad_w\'] = int(float(layerParams[\'pad_w\']))\n    if layerParams[\'weight_filler\'] != \'\':\n        convolution_param[\'weight_filler\'] = {}\n        try:\n            convolution_param[\'weight_filler\'][\'type\'] = \\\n                fillerMap[layerParams[\'weight_filler\']]\n        except:\n            convolution_param[\'weight_filler\'][\'type\'] = layerParams[\'weight_filler\']\n    if layerParams[\'bias_filler\'] != \'\':\n        convolution_param[\'bias_filler\'] = {}\n        try:\n            convolution_param[\'bias_filler\'][\'type\'] = \\\n                fillerMap[layerParams[\'bias_filler\']]\n        except:\n            convolution_param[\'bias_filler\'][\'type\'] = layerParams[\'bias_filler\']\n    convolution_param[\'dilation\'] = layerParams[\'dilation_h\']\n    convolution_param[\'bias_term\'] = layerParams[\'use_bias\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.Deconvolution(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            convolution_param=convolution_param,\n            param=[\n                {\n                    \'lr_mult\': 1\n                },\n                {\n                    \'lr_mult\': 2\n                }\n            ]))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Recurrent(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    recurrent_param = {}\n    recurrent_param[\'num_output\'] = int(layerParams[\'num_output\'])\n    if layerParams[\'weight_filler\'] != \'\':\n        recurrent_param[\'weight_filler\'] = {}\n        try:\n            recurrent_param[\'weight_filler\'][\'type\'] = \\\n                fillerMap[layerParams[\'weight_filler\']]\n        except:\n            recurrent_param[\'weight_filler\'][\'type\'] = layerParams[\'weight_filler\']\n    if layerParams[\'bias_filler\'] != \'\':\n        recurrent_param[\'bias_filler\'] = {}\n        try:\n            recurrent_param[\'bias_filler\'][\'type\'] = \\\n                fillerMap[layerParams[\'bias_filler\']]\n        except:\n            recurrent_param[\'bias_filler\'][\'type\'] = layerParams[\'bias_filler\']\n    recurrent_param[\'debug_info\'] = layerParams[\'debug_info\']\n    recurrent_param[\'expose_hidden\'] = layerParams[\'expose_hidden\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.Recurrent(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            recurrent_param=recurrent_param))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_RNN(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    recurrent_param = {}\n    recurrent_param[\'num_output\'] = int(layerParams[\'num_output\'])\n    if layerParams[\'weight_filler\'] != \'\':\n        recurrent_param[\'weight_filler\'] = {}\n        try:\n            recurrent_param[\'weight_filler\'][\'type\'] = \\\n                fillerMap[layerParams[\'weight_filler\']]\n        except:\n            recurrent_param[\'weight_filler\'][\'type\'] = layerParams[\'weight_filler\']\n    if layerParams[\'bias_filler\'] != \'\':\n        recurrent_param[\'bias_filler\'] = {}\n        try:\n            recurrent_param[\'bias_filler\'][\'type\'] = \\\n                fillerMap[layerParams[\'bias_filler\']]\n        except:\n            recurrent_param[\'bias_filler\'][\'type\'] = layerParams[\'bias_filler\']\n    recurrent_param[\'debug_info\'] = layerParams[\'debug_info\']\n    recurrent_param[\'expose_hidden\'] = layerParams[\'expose_hidden\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.RNN(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            recurrent_param=recurrent_param))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_LSTM(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    recurrent_param = {}\n    recurrent_param[\'num_output\'] = int(layerParams[\'num_output\'])\n    if layerParams[\'weight_filler\'] != \'\':\n        recurrent_param[\'weight_filler\'] = {}\n        try:\n            recurrent_param[\'weight_filler\'][\'type\'] = \\\n                fillerMap[layerParams[\'weight_filler\']]\n        except:\n            recurrent_param[\'weight_filler\'][\'type\'] = layerParams[\'weight_filler\']\n    if layerParams[\'bias_filler\'] != \'\':\n        recurrent_param[\'bias_filler\'] = {}\n        try:\n            recurrent_param[\'bias_filler\'][\'type\'] = \\\n                fillerMap[layerParams[\'bias_filler\']]\n        except:\n            recurrent_param[\'bias_filler\'][\'type\'] = layerParams[\'bias_filler\']\n    recurrent_param[\'debug_info\'] = layerParams[\'debug_info\']\n    recurrent_param[\'expose_hidden\'] = layerParams[\'expose_hidden\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.LSTM(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            recurrent_param=recurrent_param))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_InnerProduct(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    inner_product_param = {}\n    if layerParams[\'num_output\'] != \'\':\n        inner_product_param[\'num_output\'] = int(\n            float(layerParams[\'num_output\']))\n    if layerParams[\'weight_filler\'] != \'\':\n        inner_product_param[\'weight_filler\'] = {}\n        try:\n            inner_product_param[\'weight_filler\'][\'type\'] = \\\n                fillerMap[layerParams[\'weight_filler\']]\n        except:\n            inner_product_param[\'weight_filler\'][\'type\'] = layerParams[\'weight_filler\']\n    if layerParams[\'bias_filler\'] != \'\':\n        inner_product_param[\'bias_filler\'] = {}\n        try:\n            inner_product_param[\'bias_filler\'][\'type\'] = \\\n                fillerMap[layerParams[\'bias_filler\']]\n        except:\n            inner_product_param[\'bias_filler\'][\'type\'] = layerParams[\'bias_filler\']\n    inner_product_param[\'bias_term\'] = layerParams[\'use_bias\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.InnerProduct(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            inner_product_param=inner_product_param,\n            param=[\n                {\n                    \'lr_mult\': 1\n                },\n                {\n                    \'lr_mult\': 2\n                }\n            ]))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Dropout(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    # inplace dropout? caffe-tensorflow do not work\n    inplace = layerParams[\'inplace\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.Dropout(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            in_place=inplace))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Embed(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    embed_param = {}\n    if layerParams[\'num_output\'] != \'\':\n        embed_param[\'num_output\'] = int(float(layerParams[\'num_output\']))\n    if layerParams[\'input_dim\'] != \'\':\n        embed_param[\'input_dim\'] = int(float(layerParams[\'input_dim\']))\n    if layerParams[\'weight_filler\'] != \'\':\n        embed_param[\'weight_filler\'] = {}\n        try:\n            embed_param[\'weight_filler\'][\'type\'] = \\\n                fillerMap[layerParams[\'weight_filler\']]\n        except:\n            embed_param[\'weight_filler\'][\'type\'] = layerParams[\'weight_filler\']\n    if layerParams[\'bias_filler\'] != \'\':\n        embed_param[\'bias_filler\'] = {}\n        try:\n            embed_param[\'bias_filler\'][\'type\'] = \\\n                fillerMap[layerParams[\'bias_filler\']]\n        except:\n            embed_param[\'bias_filler\'][\'type\'] = layerParams[\'bias_filler\']\n    embed_param[\'bias_term\'] = layerParams[\'bias_term\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.Embed(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            embed_param=embed_param,\n            param=[\n                {\n                    \'lr_mult\': 1,\n                    \'decay_mult\': 1\n                },\n                {\n                    \'lr_mult\': 2,\n                    \'decay_mult\': 0\n                }\n            ]))\n        # *([ns[x] for x in blobNames[layerId][\'bottom\']] + [ns.label])))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_LRN(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    inplace = layerParams[\'inplace\']\n    lrn_param = {}\n    lrn_param[\'local_size\'] = layerParams[\'local_size\']\n    lrn_param[\'alpha\'] = layerParams[\'alpha\']\n    lrn_param[\'beta\'] = layerParams[\'beta\']\n    lrn_param[\'k\'] = layerParams[\'k\']\n    if(layerParams[\'norm_region\'] == \'ACROSS_CHANNELS\'):\n        lrn_param[\'norm_region\'] = 0\n    else:\n        lrn_param[\'norm_region\'] = 1\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.LRN(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            lrn_param=lrn_param, in_place=inplace))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_MVN(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    inplace = layerParams[\'inplace\']\n    mvn_param = {}\n    mvn_param[\'normalize_variance\'] = layerParams[\'normalize_variance\']\n    mvn_param[\'across_channels\'] = layerParams[\'across_channels\']\n    # JS converts 1e-9 to string\n    mvn_param[\'eps\'] = float(layerParams[\'eps\'])\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.MVN(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            mvn_param=mvn_param, in_place=inplace))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_BatchNorm(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    inplace = layerParams[\'inplace\']\n    batch_norm_param = {}\n    batch_norm_param[\'use_global_stats\'] = layerParams[\'use_global_stats\']\n    batch_norm_param[\'moving_average_fraction\'] = layerParams[\'moving_average_fraction\']\n    batch_norm_param[\'eps\'] = float(layerParams[\'eps\'])\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.BatchNorm(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            batch_norm_param=batch_norm_param, in_place=inplace))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_ReLU(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    inplace = layerParams[\'inplace\']\n    relu_param = {}\n    relu_param[\'negative_slope\'] = layerParams[\'negative_slope\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.ReLU(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            in_place=inplace, relu_param=relu_param))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_PReLU(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    inplace = layerParams[\'inplace\']\n    prelu_param = {}\n    prelu_param[\'channel_shared\'] = layerParams[\'channel_shared\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.PReLU(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            in_place=inplace, prelu_param=prelu_param))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_ELU(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    inplace = layerParams[\'inplace\']\n    elu_param = {}\n    elu_param[\'alpha\'] = layerParams[\'alpha\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.ELU(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            in_place=inplace, elu_param=elu_param))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Sigmoid(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    inplace = layerParams[\'inplace\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.Sigmoid(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            in_place=inplace))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_TanH(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    inplace = layerParams[\'inplace\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.TanH(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            in_place=inplace))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_AbsVal(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    inplace = layerParams[\'inplace\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.AbsVal(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            in_place=inplace))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Power(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    power_param = {}\n    inplace = layerParams[\'inplace\']\n    power_param[\'power\'] = layerParams[\'power\']\n    power_param[\'scale\'] = layerParams[\'scale\']\n    power_param[\'shift\'] = layerParams[\'shift\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.Power(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            in_place=inplace, power_param=power_param))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Exp(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    exp_param = {}\n    inplace = layerParams[\'inplace\']\n    exp_param[\'base\'] = layerParams[\'base\']\n    exp_param[\'scale\'] = layerParams[\'scale\']\n    exp_param[\'shift\'] = layerParams[\'shift\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.Exp(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            in_place=inplace, exp_param=exp_param))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Log(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    log_param = {}\n    inplace = layerParams[\'inplace\']\n    log_param[\'base\'] = layerParams[\'base\']\n    log_param[\'scale\'] = layerParams[\'scale\']\n    log_param[\'shift\'] = layerParams[\'shift\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.Log(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            in_place=inplace, log_param=log_param))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_BNLL(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    inplace = layerParams[\'inplace\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.BNLL(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            in_place=inplace))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Threshold(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    inplace = layerParams[\'inplace\']\n    threshold_param = {}\n    threshold_param[\'threshold\'] = layerParams[\'threshold\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.Threshold(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            in_place=inplace, threshold_param=threshold_param))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Bias(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    bias_param = {}\n    bias_param[\'axis\'] = layerParams[\'axis\']\n    bias_param[\'num_axes\'] = layerParams[\'num_axes\']\n    if layerParams[\'filler\'] != \'\':\n        bias_param[\'filler\'] = {}\n        try:\n            bias_param[\'filler\'][\'type\'] = \\\n                fillerMap[layerParams[\'filler\']]\n        except:\n            bias_param[\'filler\'][\'type\'] = layerParams[\'filler\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.Bias(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            bias_param=bias_param\n        ))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Scale(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    scale_param = {}\n    scale_param[\'axis\'] = layerParams[\'axis\']\n    scale_param[\'num_axes\'] = layerParams[\'num_axes\']\n    if layerParams[\'filler\'] != \'\':\n        scale_param[\'filler\'] = {}\n        try:\n            scale_param[\'filler\'][\'type\'] = \\\n                fillerMap[layerParams[\'filler\']]\n        except:\n            scale_param[\'filler\'][\'type\'] = layerParams[\'filler\']\n    scale_param[\'bias_term\'] = layerParams[\'bias_term\']\n    if layerParams[\'bias_filler\'] != \'\':\n        scale_param[\'bias_filler\'] = {}\n        try:\n            scale_param[\'bias_filler\'][\'type\'] = \\\n                fillerMap[layerParams[\'bias_filler\']]\n        except:\n            scale_param[\'bias_filler\'][\'type\'] = layerParams[\'bias_filler\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.Scale(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            scale_param=scale_param\n        ))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Flatten(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    flatten_param = {}\n    flatten_param[\'axis\'] = layerParams[\'axis\']\n    flatten_param[\'end_axis\'] = layerParams[\'end_axis\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.Flatten(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            flatten_param=flatten_param))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Reshape(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    reshape_param = {\'shape\': {\'dim\': map(int, layerParams[\'dim\'].split(\',\'))}}\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.Reshape(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            reshape_param=reshape_param))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_BatchReindex(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.BatchReindex(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']]))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Split(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.Split(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']]))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Concat(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.Concat(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            ntop=len(blobNames[layerId][\'top\'])))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Slice(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    slice_param = {}\n    slice_param[\'slice_point\'] = map(\n        int, layerParams[\'slice_point\'].split(\',\'))\n    slice_param[\'axis\'] = layerParams[\'axis\']\n    slice_param[\'slice_dim\'] = layerParams[\'slice_dim\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.Slice(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            slice_param=slice_param))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Eltwise(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    eltwise_param = {}\n    if layerParams[\'layer_type\'] != \'\':\n        elt = layerParams[\'layer_type\']\n        if(elt == \'Product\'):\n            elt = 0\n        elif(elt == \'Sum\'):\n            elt = 1\n        elif(elt == \'Maximum\'):\n            elt = 2\n    else:\n        elt = 1  # Default is sum\n    eltwise_param[\'operation\'] = elt\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.Eltwise(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            eltwise_param=eltwise_param))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Filter(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.Filter(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            ntop=len(blobNames[layerId][\'top\'])))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\n# This layer is currently not supported as there is no bottom blob\n# def export_Parameter(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n#     parameter_param = {}\n#     parameter_param[\'shape\'] = map(int, layerParams[\'shape\'].split(\',\'))\n#     for ns in (ns_train, ns_test):\n#        caffeLayer = get_iterable(L.Parameter(\n#            parameter_param=parameter_param))\n#        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n#            ns[key] = value\n#     return ns_train, ns_test\n\n\ndef export_Reduction(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    reduction_param = {}\n    if(layerParams[\'operation\'] == \'SUM\'):\n        reduction_param[\'operation\'] = 1\n    elif(layerParams[\'operation\'] == \'ASUM\'):\n        reduction_param[\'operation\'] = 2\n    elif(layerParams[\'operation\'] == \'SUMSQ\'):\n        reduction_param[\'operation\'] = 3\n    elif(layerParams[\'operation\'] == \'MEAN\'):\n        reduction_param[\'operation\'] = 4\n    reduction_param[\'axis\'] = layerParams[\'axis\']\n    reduction_param[\'coeff\'] = layerParams[\'coeff\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.Reduction(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            reduction_param=reduction_param))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Silence(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.Silence(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            ntop=len(blobNames[layerId][\'top\'])))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_ArgMax(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    argmax_param = {}\n    argmax_param[\'out_max_val\'] = layerParams[\'out_max_val\']\n    argmax_param[\'top_k\'] = layerParams[\'top_k\']\n    argmax_param[\'axis\'] = layerParams[\'axis\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.ArgMax(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            argmax_param=argmax_param))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Softmax(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.Softmax(\n            *([ns[x] for x in blobNames[layerId][\'bottom\']])))\n        # *([ns[x] for x in blobNames[layerId][\'bottom\']] + [ns.label])))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_MultinomialLogisticLoss(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.MultinomialLogisticLoss(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']]))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_InfogainLoss(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    infogain_loss_param = {}\n    infogain_loss_param[\'source\'] = layerParams[\'source\']\n    infogain_loss_param[\'axis\'] = layerParams[\'axis\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.MultinomialLogisticLoss(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            infogain_loss_param=infogain_loss_param))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_SoftmaxWithLoss(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    softmax_param = {\'axis\': layerParams[\'axis\']}\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.SoftmaxWithLoss(  # try L[\'SoftmaxWithLoss\']\n            *([ns[x] for x in blobNames[layerId][\'bottom\']]),\n            softmax_param=softmax_param))\n        # *([ns[x] for x in blobNames[layerId][\'bottom\']] + [ns.label])))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_EuclideanLoss(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.EuclideanLoss(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']]))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_HingeLoss(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    hinge_loss_param = {\'norm\': layerParams[\'norm\']}\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.HingeLoss(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            hinge_loss_param=hinge_loss_param))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_SigmoidCrossEntropyLoss(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.SigmoidCrossEntropyLoss(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']]))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Accuracy(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    accuracy_param = {}\n    accuracy_param[\'top_k\'] = layerParams[\'top_k\']\n    accuracy_param[\'axis\'] = layerParams[\'axis\']\n    if layerPhase is not None:\n        if int(layerPhase) == 0:\n            caffeLayer = get_iterable(L.Accuracy(\n                *([ns_train[x] for x in blobNames[layerId][\'bottom\']]),\n                accuracy_param=accuracy_param,\n                # *([ns[x] for x in blobNames[layerId][\'bottom\']] + [ns.label]),\n                include={\n                    \'phase\': int(layerPhase)\n                }))\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns_train[key] = value\n        elif int(layerPhase) == 1:\n            caffeLayer = get_iterable(L.Accuracy(\n                *([ns_test[x] for x in blobNames[layerId][\'bottom\']]),\n                accuracy_param=accuracy_param,\n                # *([ns[x] for x in blobNames[layerId][\'bottom\']] + [ns.label]),\n                include={\n                    \'phase\': int(layerPhase)\n                }))\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns_test[key] = value\n    else:\n        for ns in (ns_train, ns_test):\n            caffeLayer = get_iterable(L.Accuracy(\n                *([ns[x] for x in blobNames[layerId][\'bottom\']]),\n                accuracy_param=accuracy_param))\n            # *([ns[x] for x in blobNames[layerId][\'bottom\']] + [ns.label])))\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_ContrastiveLoss(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    contrastive_loss_param = {}\n    contrastive_loss_param[\'margin\'] = layerParams[\'margin\']\n    contrastive_loss_param[\'legacy_version\'] = layerParams[\'legacy_version\']\n    for ns in (ns_train, ns_test):\n        caffeLayer = get_iterable(L.ContrastiveLoss(\n            *[ns[x] for x in blobNames[layerId][\'bottom\']],\n            contrastive_loss_param=contrastive_loss_param))\n        for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n            ns[key] = value\n    return ns_train, ns_test\n\n\ndef export_Python(layerId, layerParams, layerPhase, ns_train, ns_test, blobNames):\n    # Parameters not to be included in param_str\n    notParamStr = [\'module\', \'layer\', \'endPoint\',\n                   \'loss_weight\', \'dragDrop\', \'param_str\']\n    hasParamStr = False\n    python_param = {}\n    python_param[\'module\'] = layerParams[\'module\']\n    python_param[\'layer\'] = layerParams[\'layer\']\n    for param in layerParams:\n        if (param not in notParamStr):\n            hasParamStr = True\n            if \'param_str\' not in python_param.keys():\n                python_param[\'param_str\'] = {}\n            if isinstance(layerParams[param], str):\n                try:\n                    python_param[\'param_str\'][param] = map(int,\n                                                           layerParams[param].split(\',\'))\n                except:\n                    python_param[\'param_str\'][param] = layerParams[param]\n            else:\n                python_param[\'param_str\'][param] = layerParams[param]\n    if \'dragDrop\' in layerParams.keys():\n        python_param[\'param_str\'] = layerParams[\'param_str\']\n    if (hasParamStr):\n        python_param[\'param_str\'] = str(python_param[\'param_str\'])\n    if \'loss_weight\' in layerParams:\n        for ns in (ns_train, ns_test):\n            caffeLayer = get_iterable(L.Python(\n                *[ns[x] for x in blobNames[layerId][\'bottom\']],\n                python_param=python_param, loss_weight=layerParams[\'loss_weight\']))\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns[key] = value\n    else:\n        for ns in (ns_train, ns_test):\n            caffeLayer = get_iterable(L.Python(\n                *[ns[x] for x in blobNames[layerId][\'bottom\']],\n                python_param=python_param))\n            for key, value in zip(blobNames[layerId][\'top\'], caffeLayer):\n                ns[key] = value\n    return ns_train, ns_test\n\n\nlayer_map = {\n    \'ImageData\': export_ImageData,\n    \'Data\': export_Data,\n    \'HDF5Data\': export_HDF5Data,\n    \'HDF5Output\': export_HDF5Output,\n    \'Input\': export_Input,\n    \'WindowData\': export_WindowData,\n    \'MemoryData\': export_MemoryData,\n    \'DummyData\': export_DummyData,\n    \'Convolution\': export_Convolution,\n    \'Pooling\': export_Pooling,\n    \'Crop\': export_Crop,\n    \'SPP\': export_SPP,\n    \'Deconvolution\': export_Deconvolution,\n    \'Recurrent\': export_Recurrent,\n    \'RNN\': export_RNN,\n    \'LSTM\': export_LSTM,\n    \'InnerProduct\': export_InnerProduct,\n    \'Dropout\': export_Dropout,\n    \'Embed\': export_Embed,\n    \'LRN\': export_LRN,\n    \'MVN\': export_MVN,\n    \'BatchNorm\': export_BatchNorm,\n    \'ReLU\': export_ReLU,\n    \'PReLU\': export_PReLU,\n    \'ELU\': export_ELU,\n    \'Sigmoid\': export_Sigmoid,\n    \'TanH\': export_TanH,\n    \'AbsVal\': export_AbsVal,\n    \'Power\': export_Power,\n    \'Exp\': export_Exp,\n    \'Log\': export_Log,\n    \'BNLL\': export_BNLL,\n    \'Threshold\': export_Threshold,\n    \'Bias\': export_Bias,\n    \'Scale\': export_Scale,\n    \'Flatten\': export_Flatten,\n    \'Reshape\': export_Reshape,\n    \'BatchReindex\': export_BatchReindex,\n    \'Split\': export_Split,\n    \'Concat\': export_Concat,\n    \'Slice\': export_Slice,\n    \'Eltwise\': export_Eltwise,\n    \'Filter\': export_Filter,\n    # \'Parameter\': export_Parameter,\n    \'Reduction\': export_Reduction,\n    \'Silence\': export_Silence,\n    \'ArgMax\': export_ArgMax,\n    \'Softmax\': export_Softmax,\n    \'MultinomialLogisticLoss\': export_MultinomialLogisticLoss,\n    \'InfogainLoss\': export_InfogainLoss,\n    \'SoftmaxWithLoss\': export_SoftmaxWithLoss,\n    \'EuclideanLoss\': export_EuclideanLoss,\n    \'HingeLoss\': export_HingeLoss,\n    \'SigmoidCrossEntropyLoss\': export_SigmoidCrossEntropyLoss,\n    \'Accuracy\': export_Accuracy,\n    \'ContrastiveLoss\': export_ContrastiveLoss,\n    \'Python\': export_Python\n}\n\n\ndef json_to_prototxt(net, net_name):\n    # assumption: a layer can accept only one input blob\n    # the data layer produces two blobs: data and label\n    # the loss layer requires two blobs: <someData> and label\n    # the label blob is hardcoded.\n    # layers name have to be unique\n\n    # custom DFS of the network\n    input_dim = None\n    stack = []\n    layersProcessed = {}\n    processOrder = []\n    blobNames = {}\n    for layerId in net:\n        layersProcessed[layerId] = False\n        blobNames[layerId] = {\n            \'bottom\': [],\n            \'top\': [],\n        }\n    blobId = 0\n\n    def isProcessPossible(layerId):\n        inputs = net[layerId][\'connection\'][\'input\']\n        for layerId in inputs:\n            if layersProcessed[layerId] is False:\n                return False\n        return True\n\n    # finding the data layer\n    dataLayers = [\'ImageData\', \'Data\', \'HDF5Data\',\n                  \'Input\', \'WindowData\', \'MemoryData\', \'DummyData\']\n    for layerId in net:\n        if (net[layerId][\'info\'][\'type\'] == \'Python\'):\n            if (\'endPoint\' not in net[layerId][\'params\'].keys()):\n                net[layerId][\'params\'][\'dragDrop\'] = True\n                if (not net[layerId][\'connection\'][\'input\']):\n                    stack.append(layerId)\n            else:\n                if (net[layerId][\'params\'][\'endPoint\'] == ""1, 0""):\n                    stack.append(layerId)\n        if(net[layerId][\'info\'][\'type\'] in dataLayers):\n            stack.append(layerId)\n\n    def changeTopBlobName(layerId, newName):\n        blobNames[layerId][\'top\'] = newName\n\n    while len(stack):\n\n        i = len(stack) - 1\n\n        while isProcessPossible(stack[i]) is False:\n            i = i - 1\n\n        layerId = stack[i]\n        stack.remove(stack[i])\n\n        inputs = net[layerId][\'connection\'][\'input\']\n        if len(inputs) > 0:\n            if len(inputs) == 2 and (net[inputs[0]][\'info\'][\'phase\'] is not None) \\\n                    and (net[inputs[1]][\'info\'][\'phase\']):\n                commonBlobName = blobNames[inputs[0]][\'top\']\n                changeTopBlobName(inputs[1], commonBlobName)\n                blobNames[layerId][\'bottom\'] = commonBlobName\n            else:\n                inputBlobNames = []\n                for inputId in inputs:\n                    inputBlobNames.extend(blobNames[inputId][\'top\'])\n                blobNames[layerId][\'bottom\'] = inputBlobNames\n\n        blobNames[layerId][\'top\'] = [\'blob\' + str(blobId)]\n        blobId = blobId + 1\n\n        for outputId in net[layerId][\'connection\'][\'output\']:\n            if outputId not in stack:\n                stack.append(outputId)\n\n        layersProcessed[layerId] = True\n        processOrder.append(layerId)\n\n    ns_train = caffe.NetSpec()\n    ns_test = caffe.NetSpec()\n\n    for layerId in processOrder:\n\n        layer = net[layerId]\n        layerParams = layer[\'params\']\n        layerType = layer[\'info\'][\'type\']\n        layerPhase = layer[\'info\'][\'phase\']\n\n        if (str(layerType) == ""Input""):\n            input_dim = layerParams[\'dim\']\n\n        if (not layerParams[\'caffe\']):\n            if (\'layer_type\' in layerParams):\n                raise Exception(\'Cannot export layer of type \' + layerType + \' \' + layerParams[\'layer_type\']\n                                + \' to Caffe.\')\n            else:\n                raise Exception(\'Cannot export layer of type \' +\n                                layerType + \' to Caffe.\')\n        ns_train, ns_test = layer_map[layerType](layerId, layerParams, layerPhase,\n                                                 ns_train, ns_test, blobNames)\n\n    train = \'name: ""\' + net_name + \'""\\n\' + str(ns_train.to_proto())\n    test = str(ns_test.to_proto())\n\n    # merge the train and test prototxt to get a single train_test prototxt\n    testIndex = [m.start() for m in re.finditer(\'layer\', test)]\n\n    previousIndex = -1\n    for i in range(len(testIndex)):\n        if i < len(testIndex) - 1:\n            layer = test[testIndex[i]:testIndex[i + 1]]\n        else:\n            layer = test[testIndex[i]:]\n        a = train.find(layer)\n        if a != -1:\n            l = test[testIndex[previousIndex + 1]:testIndex[i]]\n            train = train[0:a] + l + train[a:]\n            previousIndex = i\n    if previousIndex < len(testIndex) - 1:\n        l = test[testIndex[previousIndex + 1]:]\n        train = train + l\n\n    prototxt = train\n    return prototxt, input_dim\n'"
ide/utils/shapes.py,0,"b'import numpy as np\nfrom collections import deque\n\n\ndef data(layer):\n    Input = []\n    if (layer[\'info\'][\'type\'] in [\'ImageData\', \'Data\', \'WindowData\']):\n        if ((\'crop_size\' in layer[\'params\']) and (layer[\'params\'][\'crop_size\'] != 0)):\n            Output = [3] + [layer[\'params\'][\'crop_size\']]*2\n        elif ((\'new_height\' in layer[\'params\']) and (\'new_width\' in layer[\'params\'])):\n            Output = [3, layer[\'params\'][\'new_height\'], layer[\'params\'][\'new_width\']]\n        else:\n            # When a new layer is created with default parameters\n            Output = []\n    elif (layer[\'info\'][\'type\'] in [\'Input\', \'DummyData\']):\n        Output = map(int, layer[\'params\'][\'dim\'].split(\',\'))[1:]\n    elif (layer[\'info\'][\'type\'] == \'MemoryData\'):\n        Output = [3, layer[\'params\'][\'height\'], layer[\'params\'][\'width\']]\n    else:\n        raise Exception(\'Cannot determine shape of \' + layer[\'info\'][\'type\'] + \' layer.\')\n    return Input, Output\n\n\ndef identity(layer):\n    return layer[\'shape\'][\'input\']\n\n\ndef filter(layer):\n    if (layer[\'info\'][\'type\'] == \'Pooling\'):\n        num_out = layer[\'shape\'][\'input\'][0]\n    else:\n        num_out = layer[\'params\'][\'num_output\']\n    if (layer[\'info\'][\'type\'] == \'Deconvolution\'):\n        _, i_h, i_w = layer[\'shape\'][\'input\']\n        k_h, k_w = layer[\'params\'][\'kernel_h\'], layer[\'params\'][\'kernel_w\']\n        s_h, s_w = layer[\'params\'][\'stride_h\'], layer[\'params\'][\'stride_w\']\n        p_h, p_w = layer[\'params\'][\'pad_h\'], layer[\'params\'][\'pad_w\']\n\n        o_h = i_h * s_h\n        o_w = i_w * s_w\n        if (\'padding\' in layer[\'params\'] and layer[\'params\'][\'padding\'] == \'VALID\'):\n            # handling tensorflow deconv layer separately\n            o_h += max(k_h - s_h, 0)\n            o_w += max(k_w - s_w, 0)\n\n        return [num_out, o_h, o_w]\n    elif (layer[\'info\'][\'type\'] == \'DepthwiseConv\'):\n        _, i_h, i_w = layer[\'shape\'][\'input\']\n        k_h, k_w = layer[\'params\'][\'kernel_h\'], layer[\'params\'][\'kernel_w\']\n        s_h, s_w = layer[\'params\'][\'stride_h\'], layer[\'params\'][\'stride_w\']\n        p_h, p_w = layer[\'params\'][\'pad_h\'], layer[\'params\'][\'pad_w\']\n        o_h = int((i_h - 1)*s_h + k_h - 2*p_h)\n        o_w = int((i_w - 1)*s_w + k_w - 2*p_w)\n\n        return [num_out, o_h, o_w]\n    else:\n        if (layer[\'params\'][\'layer_type\'] == \'1D\'):\n            try:\n                _, i_w = layer[\'shape\'][\'input\']\n                k_w = layer[\'params\'][\'kernel_w\']\n                s_w = layer[\'params\'][\'stride_w\']\n                p_w = layer[\'params\'][\'pad_w\']\n                o_w = int((i_w + 2 * p_w - k_w) / float(s_w) + 1)\n            except:\n                return [num_out, 0]\n            return [num_out, o_w]\n        elif (layer[\'params\'][\'layer_type\'] == \'2D\'):\n            try:\n                _, i_h, i_w = layer[\'shape\'][\'input\']\n                k_h, k_w = layer[\'params\'][\'kernel_h\'], layer[\'params\'][\'kernel_w\']\n                s_h, s_w = layer[\'params\'][\'stride_h\'], layer[\'params\'][\'stride_w\']\n                p_h, p_w = layer[\'params\'][\'pad_h\'], layer[\'params\'][\'pad_w\']\n                o_h = int((i_h + 2 * p_h - k_h) / float(s_h) + 1)\n                o_w = int((i_w + 2 * p_w - k_w) / float(s_w) + 1)\n            except:\n                return [num_out, 0, 0]\n            return [num_out, o_h, o_w]\n        else:\n            try:\n                _, i_d, i_h, i_w = layer[\'shape\'][\'input\']\n                k_h, k_w, k_d = layer[\'params\'][\'kernel_h\'], layer[\'params\'][\'kernel_w\'],\\\n                    layer[\'params\'][\'kernel_d\']\n                s_h, s_w, s_d = layer[\'params\'][\'stride_h\'], layer[\'params\'][\'stride_w\'],\\\n                    layer[\'params\'][\'stride_d\']\n                p_h, p_w, p_d = layer[\'params\'][\'pad_h\'], layer[\'params\'][\'pad_w\'],\\\n                    layer[\'params\'][\'pad_d\']\n                o_h = int((i_h + 2 * p_h - k_h) / float(s_h) + 1)\n                o_w = int((i_w + 2 * p_w - k_w) / float(s_w) + 1)\n                o_d = int((i_d + 2 * p_d - k_d) / float(s_d) + 1)\n            except:\n                return [num_out, 0, 0, 0]\n            return [num_out, o_d, o_h, o_w]\n\n\ndef upsample(layer):\n    if (layer[\'params\'][\'layer_type\'] == \'1D\'):\n        num_out, i_w = layer[\'shape\'][\'input\']\n        s_w = layer[\'params\'][\'size_w\']\n        o_w = int(i_w*s_w)\n        return [num_out, o_w]\n    elif (layer[\'params\'][\'layer_type\'] == \'2D\'):\n        num_out, i_h, i_w = layer[\'shape\'][\'input\']\n        s_h, s_w = layer[\'params\'][\'size_h\'], layer[\'params\'][\'size_w\']\n        o_w = int(i_w*s_w)\n        o_h = int(i_h*s_h)\n        return [num_out, o_h, o_w]\n    else:\n        num_out, i_h, i_w, i_d = layer[\'shape\'][\'input\']\n        s_h, s_w, s_d = layer[\'params\'][\'size_h\'], layer[\'params\'][\'size_w\'],\\\n            layer[\'params\'][\'size_d\']\n        o_w = int(i_w*s_w)\n        o_h = int(i_h*s_h)\n        o_d = int(i_d*s_d)\n        return [num_out, o_h, o_w, o_d]\n\n\ndef output(layer):\n    return [layer[\'params\'][\'num_output\']]\n\n\ndef flatten(layer):\n    out = 1\n    for i in layer[\'shape\'][\'input\']:\n        if (i > 0):\n            out *= i\n    return [out]\n\n\ndef reshape(layer):\n    temp = np.zeros(layer[\'shape\'][\'input\'])\n    shape = map(int, layer[\'params\'][\'dim\'].split(\',\'))[1:]\n    temp = np.reshape(temp, shape)\n    return list(temp.shape[::-1])\n\n\ndef repeat(layer):\n    shape = layer[\'shape\'][\'input\']\n    shape = shape + [layer[\'params\'][\'n\']]\n    return shape\n\n\ndef handle_concat_layer(outputLayer, inputLayer):\n    if(\'input\' not in outputLayer[\'shape\']):\n        shape = inputLayer[\'shape\'][\'output\'][:]\n    else:\n        old_num_output = outputLayer[\'shape\'][\'input\'][0]\n        shape = inputLayer[\'shape\'][\'output\'][:]\n        shape[0] += old_num_output\n    return shape\n\n\ndef get_layer_shape(layer):\n    # separating checking the type of layer inorder to make it modular\n    # which can be reused in case we only want to get shapes of a single\n    # layer, for example: if a new layer is added to already drawn model\n    dataLayers = [\'ImageData\', \'Data\', \'HDF5Data\', \'Input\', \'WindowData\', \'MemoryData\', \'DummyData\']\n\n    if(layer[\'info\'][\'type\'] in dataLayers):\n        return data(layer)\n\n    elif(layer[\'info\'][\'type\'] in [\'Convolution\', \'Pooling\', \'Deconvolution\', \'DepthwiseConv\']):\n        return filter(layer)\n\n    elif(layer[\'info\'][\'type\'] in [\'InnerProduct\', \'Recurrent\', \'RNN\', \'LSTM\', \'Embed\']):\n        return output(layer)\n\n    elif(layer[\'info\'][\'type\'] == \'Flatten\'):\n        return flatten(layer)\n\n    elif(layer[\'info\'][\'type\'] == \'Reshape\'):\n        return reshape(layer)\n\n    elif(layer[\'info\'][\'type\'] == \'Upsample\'):\n        return upsample(layer)\n\n    elif(layer[\'info\'][\'type\'] == \'RepeatVector\'):\n        return repeat(layer)\n\n    elif(layer[\'info\'][\'type\'] in [\'SPP\', \'Crop\']):\n        raise Exception(\'Cannot determine shape of \' + layer[\'info\'][\'type\'] + \'layer.\')\n\n    else:\n        return identity(layer)\n\n\ndef get_shapes(net):\n    queue = deque([])\n    dataLayers = [\'ImageData\', \'Data\', \'HDF5Data\', \'Input\', \'WindowData\', \'MemoryData\', \'DummyData\']\n    processedLayer = {}\n    layer_indegree = {}\n\n    # Finding the data layer\n    for layerId in net:\n        processedLayer[layerId] = False\n        # store indegree of every layer for Topological sort\n        layer_indegree[layerId] = len(net[layerId][\'connection\'][\'input\'])\n        net[layerId][\'shape\'] = {}\n        if (net[layerId][\'info\'][\'type\'] == \'Python\'):\n            if (\'endPoint\' not in net[layerId][\'params\'].keys()):\n                if (not net[layerId][\'connection\'][\'input\']):\n                    raise Exception(\'Cannot determine shape of Python layer.\')\n            else:\n                if (net[layerId][\'params\'][\'endPoint\'] == ""1, 0""):\n                    raise Exception(\'Cannot determine shape of Python layer.\')\n        if(net[layerId][\'info\'][\'type\'] in dataLayers):\n            queue.append(layerId)\n\n    while(len(queue)):\n        # using deque as stack\n        layerId = queue.pop()\n\n        if(net[layerId][\'info\'][\'type\'] in dataLayers):\n            net[layerId][\'shape\'][\'input\'], net[layerId][\'shape\'][\'output\'] = get_layer_shape(net[layerId])\n        else:\n            net[layerId][\'shape\'][\'output\'] = get_layer_shape(net[layerId])\n\n        for outputId in net[layerId][\'connection\'][\'output\']:\n            if (not processedLayer[outputId]):\n                # Handling Concat layer separately\n                if (net[outputId][\'info\'][\'type\'] == ""Concat""):\n                    net[outputId][\'shape\'][\'input\'] = handle_concat_layer(net[outputId], net[layerId])\n                else:\n                    net[outputId][\'shape\'][\'input\'] = net[layerId][\'shape\'][\'output\'][:]\n\n                # Decrement indegree of every output node of current layer\n                layer_indegree[outputId] -= 1\n\n                if layer_indegree[outputId] == 0:\n                    queue.append(outputId)\n            else:\n                if (net[outputId][\'info\'][\'type\'] == ""Concat""):\n                    net[outputId][\'shape\'][\'input\'] = handle_concat_layer(net[outputId], net[layerId])\n\n        processedLayer[layerId] = True\n\n    return net\n'"
keras_app/custom_layers/__init__.py,0,b''
keras_app/custom_layers/config.py,0,"b""# Make sure to fill in all data!\n\nconfig = {\n    'LRN': {\n        'filename': 'lrn.py',\n        'url': '/media/lrn.py'\n    }\n}\n"""
keras_app/custom_layers/lrn.py,0,"b'# Implementation for Custom LRN layer used from\n# https://github.com/keras-team/keras/issues/1549\nfrom keras.layers.core import Layer\nfrom keras import backend as K\n\n\nclass LRN(Layer):\n\n    def __init__(self, alpha=1e-4, k=2, beta=0.75, n=5, **kwargs):\n        if n % 2 == 0:\n            raise NotImplementedError(""LRN only works with odd n. n provided: "" + str(n))\n        super(LRN, self).__init__(**kwargs)\n        self.alpha = alpha\n        self.k = k\n        self.beta = beta\n        self.n = n\n\n    def get_output(self, train):\n        X = self.get_input(train)\n        b, ch, r, c = K.shape(X)\n        half_n = self.n // 2\n        input_sqr = K.square(X)\n        extra_channels = K.zeros((b, ch + 2 * half_n, r, c))\n        input_sqr = K.concatenate([extra_channels[:, :half_n, :, :],\n                                   input_sqr,\n                                   extra_channels[:, half_n + ch:, :, :]],\n                                  axis=1)\n        scale = self.k\n        for i in range(self.n):\n            scale += self.alpha * input_sqr[:, i:i + ch, :, :]\n        scale = scale ** self.beta\n        return X / scale\n\n    def get_config(self):\n        config = {""name"": self.__class__.__name__,\n                  ""alpha"": self.alpha,\n                  ""k"": self.k,\n                  ""beta"": self.beta,\n                  ""n"": self.n,\n                  ""name"": self.name}\n        base_config = super(LRN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n'"
keras_app/views/__init__.py,0,"b""'''\nFor Keras import or export, the following layers will require TF version >1.0\n* Concatenate\n* Embedding\n* SimpleRNN\n* LSTM\n'''\n"""
keras_app/views/export_json.py,0,"b""import json\nimport os\nimport random\nimport string\nimport yaml\nfrom datetime import datetime\nfrom django.views.decorators.csrf import csrf_exempt\nfrom django.http import JsonResponse\nfrom keras.models import Model\nfrom layers_export import data, convolution, deconvolution, pooling, dense, dropout, embed,\\\n    recurrent, batch_norm, activation, flatten, reshape, eltwise, concat, upsample, locally_connected,\\\n    permute, repeat_vector, regularization, masking, gaussian_noise, gaussian_dropout, alpha_dropout, \\\n    bidirectional, time_distributed, lrn, depthwiseConv\nfrom ..custom_layers import config as custom_layers_config\n\n\nBASE_DIR = os.path.dirname(\n    os.path.dirname(\n        os.path.dirname(\n            os.path.abspath(__file__))))\n\n\ndef randomword(length):\n    return ''.join(random.choice(string.lowercase) for i in range(length))\n\n\n@csrf_exempt\ndef export_json(request, is_tf=False):\n    # Note : Remove the views for export by adding unittest for celery tasks\n    if request.method == 'POST':\n        net = yaml.safe_load(request.POST.get('net'))\n        net_name = request.POST.get('net_name')\n        if net_name == '':\n            net_name = 'Net'\n\n        layer_map = {\n            'ImageData': data,\n            'Data': data,\n            'Input': data,\n            'WindowData': data,\n            'MemoryData': data,\n            'DummyData': data,\n            'InnerProduct': dense,\n            'Softmax': activation,\n            'SELU': activation,\n            'Softplus': activation,\n            'Softsign': activation,\n            'ReLU': activation,\n            'TanH': activation,\n            'Sigmoid': activation,\n            'HardSigmoid': activation,\n            'Linear': activation,\n            'Dropout': dropout,\n            'Flatten': flatten,\n            'Reshape': reshape,\n            'Permute': permute,\n            'RepeatVector': repeat_vector,\n            'Regularization': regularization,\n            'Masking': masking,\n            'Convolution': convolution,\n            'Deconvolution': deconvolution,\n            'DepthwiseConv': depthwiseConv,\n            'Upsample': upsample,\n            'Pooling': pooling,\n            'LocallyConnected': locally_connected,\n            'RNN': recurrent,\n            'GRU': recurrent,\n            'LSTM': recurrent,\n            'Embed': embed,\n            'Concat': concat,\n            'Eltwise': eltwise,\n            'PReLU': activation,\n            'ELU': activation,\n            'ThresholdedReLU': activation,\n            'BatchNorm': batch_norm,\n            'GaussianNoise': gaussian_noise,\n            'GaussianDropout': gaussian_dropout,\n            'AlphaDropout': alpha_dropout,\n            'Scale': '',\n            'TimeDistributed': time_distributed,\n            'Bidirectional': bidirectional\n        }\n\n        custom_layers_map = {\n            'LRN': lrn\n        }\n\n        # Remove any duplicate activation layers (timedistributed and bidirectional layers)\n        redundant_layers = []\n        for layerId in net:\n            if (net[layerId]['connection']['input']\n                    and net[net[layerId]['connection']['input'][0]]['info']['type'] in\n                    ['TimeDistributed', 'Bidirectional']):\n                if len(net[layerId]['connection']['output']) > 0:\n                    target = net[layerId]['connection']['output'][0]\n                    outputs = net[target]['connection']['output']\n                    if len(outputs) > 0:\n                        net[layerId]['connection']['output'] = outputs\n                        for j in outputs:\n                            net[j]['connection']['input'] = [\n                                x if (x != target) else layerId for x in net[j]['connection']['input']]\n                        redundant_layers.append(target)\n            elif (net[layerId]['info']['type'] == 'Input'\n                  and net[net[layerId]['connection']['output'][0]]['info']['type'] in\n                  ['TimeDistributed', 'Bidirectional']):\n                connected_layer = net[layerId]['connection']['output'][0]\n                net[connected_layer]['params']['batch_input_shape'] = net[layerId]['params']['dim']\n        for i in redundant_layers:\n            del net[i]\n\n        # Check if conversion is possible\n        error = []\n        custom_layers = []\n        for key, value in custom_layers_map.iteritems():\n            layer_map[key] = value\n        for layerId in net:\n            layerType = net[layerId]['info']['type']\n            if (layerType in custom_layers_map):\n                custom_layers.append(layerType)\n            if ('Loss' in layerType or layerType ==\n                    'Accuracy' or layerType in layer_map):\n                pass\n            else:\n                error.append(layerId + '(' + layerType + ')')\n        if len(error):\n            return JsonResponse(\n                {'result': 'error', 'error': 'Cannot convert ' + ', '.join(error) + ' to Keras'})\n\n        stack = []\n        net_out = {}\n        dataLayers = ['ImageData', 'Data', 'HDF5Data', 'Input', 'WindowData',\n                      'MemoryData', 'DummyData', 'Bidirectional',\n                      'TimeDistributed']\n        processedLayer = {}\n        inputLayerId = []\n        outputLayerId = []\n\n        def isProcessPossible(layerId):\n            inputs = net[layerId]['connection']['input']\n            for layerId in inputs:\n                if processedLayer[layerId] is False:\n                    return False\n            return True\n\n        # Finding the data layer\n        for layerId in net:\n            processedLayer[layerId] = False\n            if (net[layerId]['info']['type'] == 'Python'):\n                error.append(layerId + '(Python)')\n                continue\n            if(net[layerId]['info']['type'] in dataLayers):\n                stack.append(layerId)\n            if (not net[layerId]['connection']['input']):\n                inputLayerId.append(layerId)\n            if (not net[layerId]['connection']['output']):\n                outputLayerId.append(layerId)\n        if len(error):\n            return JsonResponse(\n                {'result': 'error', 'error': 'Cannot convert ' + ', '.join(error) + ' to Keras'})\n\n        while(len(stack)):\n            if ('Loss' in net[layerId]['info']['type'] or\n                    net[layerId]['info']['type'] == 'Accuracy'):\n                pass\n            elif (net[layerId]['info']['type'] in layer_map):\n                i = len(stack) - 1\n                while isProcessPossible(stack[i]) is False:\n                    i = i - 1\n                layerId = stack[i]\n                stack.remove(layerId)\n                if (net[layerId]['info']['type'] != 'Scale'):\n                    layer_in = [net_out[inputId]\n                                for inputId in net[layerId]['connection']['input']]\n                # Need to check if next layer is Scale\n                if (net[layerId]['info']['type'] == 'BatchNorm'):\n                    idNext = net[layerId]['connection']['output'][0]\n                    nextLayer = net[idNext]\n                    # If the BN layer is followed by Scale, then we need to pass both layers\n                    # as in Keras parameters from both go into one single layer\n                    net_out.update(layer_map[net[layerId]['info']['type']](\n                        net[layerId], layer_in, layerId, idNext, nextLayer))\n                elif (net[layerId]['info']['type'] == 'Scale'):\n                    type = net[net[layerId]['connection']\n                               ['input'][0]]['info']['type']\n                    if (type != 'BatchNorm'):\n                        return JsonResponse({'result': 'error', 'error': 'Cannot convert ' +\n                                             net[layerId]['info']['type'] + ' to Keras'})\n                elif (net[layerId]['info']['type'] in ['TimeDistributed', 'Bidirectional']):\n                    idNext = net[layerId]['connection']['output'][0]\n                    net_out.update(\n                        layer_map[net[layerId]['info']['type']](layerId, idNext, net, layer_in, layer_map))\n                    if len(net[idNext]['connection']['output']) > 0:\n                        net[net[idNext]['connection']['output'][0]\n                            ]['connection']['input'] = [layerId]\n                    processedLayer[idNext] = True\n                    processedLayer[layerId] = True\n                else:\n                    net_out.update(layer_map[net[layerId]['info']['type']](\n                        net[layerId], layer_in, layerId))\n                for outputId in net[layerId]['connection']['output']:\n                    if outputId not in stack:\n                        stack.append(outputId)\n                processedLayer[layerId] = True\n            else:\n                error.append(\n                    layerId + '(' + net[layerId]['info']['type'] + ')')\n        if len(error):\n            return JsonResponse(\n                {'result': 'error', 'error': 'Cannot convert ' + ', '.join(error) + ' to Keras'})\n\n        final_input = []\n        final_output = []\n        for i in inputLayerId:\n            final_input.append(net_out[i])\n\n        for j in outputLayerId:\n            if (net[net[j]['connection']['input'][0]]['info']['type'] in\n                    ['TimeDistributed', 'Bidirectional']):\n                final_output.append(net_out[net[j]['connection']['input'][0]])\n            else:\n                final_output.append(net_out[j])\n\n        model = Model(inputs=final_input, outputs=final_output, name=net_name)\n        json_string = Model.to_json(model)\n\n        randomId = datetime.now().strftime('%Y%m%d%H%M%S') + randomword(5)\n        with open(BASE_DIR + '/media/' + randomId + '.json', 'w') as f:\n            json.dump(json.loads(json_string), f, indent=4)\n\n        custom_layers_response = []\n        for layer in set(custom_layers):\n            layer_data = {'name': layer}\n            layer_data.update(custom_layers_config.config[layer])\n            custom_layers_response.append(layer_data)\n\n        if not is_tf:\n            return JsonResponse({'result': 'success',\n                                 'id': randomId,\n                                 'name': randomId + '.json',\n                                 'url': '/media/' + randomId + '.json',\n                                 'customLayers': custom_layers_response\n                                 })\n        else:\n            return {'randomId': randomId, 'customLayers': custom_layers_response}\n"""
keras_app/views/import_json.py,0,"b""import json\nimport os\nimport urllib2\nfrom urlparse import urlparse\n\nfrom django.conf import settings\nfrom django.http import JsonResponse\nfrom django.views.decorators.csrf import csrf_exempt\nfrom layers_import import Input, Convolution, Deconvolution, Pooling, Dense, Dropout, Embed,\\\n    Recurrent, BatchNorm, Activation, LeakyReLU, PReLU, ELU, Scale, Flatten, Reshape, Concat, \\\n    Eltwise, Padding, Upsample, LocallyConnected, ThresholdedReLU, Permute, RepeatVector,\\\n    ActivityRegularization, Masking, GaussianNoise, GaussianDropout, AlphaDropout, \\\n    TimeDistributed, Bidirectional, DepthwiseConv, lrn\nfrom keras.models import model_from_json, Sequential\nfrom keras.layers import deserialize\nfrom ..custom_layers.lrn import LRN\n\n\n@csrf_exempt\ndef import_json(request):\n    loadFromText = False\n    if request.method == 'POST':\n        if ('file' in request.FILES):\n            f = request.FILES['file']\n        elif 'sample_id' in request.POST:\n            try:\n                f = open(os.path.join(settings.BASE_DIR,\n                                      'example', 'keras',\n                                      request.POST['sample_id'] + '.json'), 'r')\n            except Exception:\n                return JsonResponse({'result': 'error',\n                                     'error': 'No JSON model file found'})\n        elif 'config' in request.POST:\n            loadFromText = True\n        elif 'url' in request.POST:\n            try:\n                url = urlparse(request.POST['url'])\n                if url.netloc == 'github.com':\n                    url = url._replace(netloc='raw.githubusercontent.com')\n                    url = url._replace(path=url.path.replace('blob/', ''))\n                f = urllib2.urlopen(url.geturl())\n            except Exception as ex:\n                return JsonResponse({'result': 'error', 'error': 'Invalid URL\\n' + str(ex)})\n        try:\n            if loadFromText is True:\n                model = json.loads(request.POST['config'])\n            else:\n                model = json.load(f)\n        except Exception:\n            return JsonResponse({'result': 'error', 'error': 'Invalid JSON'})\n\n    model = model_from_json(json.dumps(model), custom_objects={'LRN': LRN})\n    layer_map = {\n        'InputLayer': Input,\n        'Dense': Dense,\n        'Activation': Activation,\n        'softmax': Activation,\n        'selu': Activation,\n        'softplus': Activation,\n        'softsign': Activation,\n        'relu': Activation,\n        'tanh': Activation,\n        'sigmoid': Activation,\n        'hard_sigmoid': Activation,\n        'linear': Activation,\n        'Dropout': Dropout,\n        'Flatten': Flatten,\n        'Reshape': Reshape,\n        'Permute': Permute,\n        'RepeatVector': RepeatVector,\n        'ActivityRegularization': ActivityRegularization,\n        'Masking': Masking,\n        'Conv1D': Convolution,\n        'Conv2D': Convolution,\n        'Conv2DTranspose': Deconvolution,\n        'Conv3D': Convolution,\n        'SeparableConv2D': DepthwiseConv,\n        'UpSampling1D': Upsample,\n        'UpSampling2D': Upsample,\n        'UpSampling3D': Upsample,\n        'ZeroPadding1D': Padding,\n        'ZeroPadding2D': Padding,\n        'ZeroPadding3D': Padding,\n        'MaxPooling1D': Pooling,\n        'MaxPooling2D': Pooling,\n        'MaxPooling3D': Pooling,\n        'AveragePooling1D': Pooling,\n        'AveragePooling2D': Pooling,\n        'AveragePooling3D': Pooling,\n        'GlobalMaxPooling1D': Pooling,\n        'GlobalAveragePooling1D': Pooling,\n        'GlobalMaxPooling2D': Pooling,\n        'GlobalAveragePooling2D': Pooling,\n        'LocallyConnected1D': LocallyConnected,\n        'LocallyConnected2D': LocallyConnected,\n        'SimpleRNN': Recurrent,\n        'GRU': Recurrent,\n        'LSTM': Recurrent,\n        'Embedding': Embed,\n        'Add': Eltwise,\n        'Multiply': Eltwise,\n        'Average': Eltwise,\n        'Maximum': Eltwise,\n        'Concatenate': Concat,\n        'Dot': Eltwise,\n        'LeakyReLU': LeakyReLU,\n        'PReLU': PReLU,\n        'elu': ELU,\n        'ELU': ELU,\n        'ThresholdedReLU': ThresholdedReLU,\n        'BatchNormalization': BatchNorm,\n        'GaussianNoise': GaussianNoise,\n        'GaussianDropout': GaussianDropout,\n        'AlphaDropout': AlphaDropout,\n        'TimeDistributed': TimeDistributed,\n        'Bidirectional': Bidirectional,\n        'LRN': lrn\n    }\n\n    hasActivation = ['Conv1D', 'Conv2D', 'Conv3D', 'Conv2DTranspose', 'Dense', 'LocallyConnected1D',\n                     'LocallyConnected2D', 'SeparableConv2D', 'LSTM', 'SimpleRNN', 'GRU']\n\n    net = {}\n    # Add dummy input layer if sequential model\n    if (isinstance(model, Sequential)):\n        input_layer = model.layers[0].inbound_nodes[0].inbound_layers[0]\n        # If embedding is the first layer, the input has shape (None, None)\n        if (model.layers[0].__class__.__name__ == 'Embedding'):\n            input_layer.batch_input_shape = (None, model.layers[0].input_dim)\n        net[input_layer.name] = Input(input_layer)\n        net[input_layer.name]['connection']['output'] = [model.layers[0].name]\n    for idx, layer in enumerate(model.layers):\n        name = ''\n        class_name = layer.__class__.__name__\n        wrapped = False\n        if (class_name in layer_map):\n            # This is to handle wrappers and the wrapped layers.\n            if class_name == 'InputLayer':\n                found = 0\n                for find_layer in model.layers:\n                    if len(find_layer.inbound_nodes[0].inbound_layers):\n                        if find_layer.inbound_nodes[0].inbound_layers[0].__class__.__name__ == 'InputLayer':\n                            net[layer.name] = Input(layer)\n                            if find_layer.__class__.__name__ in ['Bidirectional', 'TimeDistributed']:\n                                net[layer.name]['connection']['output'] = [\n                                    find_layer.name]\n                                found = 1\n                                break\n                if not found:\n                    net[layer.name] = Input(layer)\n\n            elif class_name in ['Bidirectional', 'TimeDistributed']:\n                net[layer.name] = layer_map[class_name](layer)\n                wrapped_layer = layer.get_config()['layer']\n                name = wrapped_layer['config']['name']\n                new_layer = deserialize({\n                    'class_name': wrapped_layer['class_name'],\n                    'config': wrapped_layer['config']\n                })\n                new_layer.wrapped = True\n                new_layer.wrapper = [layer.name]\n                if new_layer.activation.func_name != 'linear':\n                    net[name + wrapped_layer['class_name']\n                        ] = layer_map[wrapped_layer['class_name']](new_layer)\n                    net[name] = layer_map[new_layer.activation.func_name](\n                        new_layer)\n                    net[name + wrapped_layer['class_name']\n                        ]['connection']['output'].append(name)\n                    net[name]['connection']['input'] = [\n                        name + wrapped_layer['class_name']]\n                    net[layer.name]['connection']['output'] = [\n                        name + wrapped_layer['class_name']]\n                else:\n                    net[name] = layer_map[wrapped_layer['class_name']](\n                        new_layer)\n                    net[name]['connection']['input'] = [layer.name]\n                    net[layer.name]['connection']['output'] = [name]\n                if len(model.layers) >= idx + 2:\n                    net[name]['connection']['output'] = [\n                        model.layers[idx + 1].name]\n                    model.layers[idx +\n                                 1].inbound_nodes[0].inbound_layers = [new_layer]\n                else:\n                    net[name]['connection']['output'] = []\n                wrapped = True\n            # This extra logic is to handle connections if the layer has an Activation\n            elif (class_name in hasActivation and layer.activation.func_name != 'linear'):\n                net[layer.name + class_name] = layer_map[class_name](layer)\n                net[layer.name] = layer_map[layer.activation.func_name](layer)\n                net[layer.name +\n                    class_name]['connection']['output'].append(layer.name)\n                name = layer.name + class_name\n            # To check if a Scale layer is required\n            elif (class_name == 'BatchNormalization' and (\n                    layer.center or layer.scale)):\n                net[layer.name + class_name] = layer_map[class_name](layer)\n                net[layer.name] = Scale(layer)\n                net[layer.name +\n                    class_name]['connection']['output'].append(layer.name)\n                name = layer.name + class_name\n            else:\n                net[layer.name] = layer_map[class_name](layer)\n                name = layer.name\n            if (layer.inbound_nodes[0].inbound_layers) and not wrapped:\n                for node in layer.inbound_nodes[0].inbound_layers:\n                    net[node.name]['connection']['output'].append(name)\n        else:\n            return JsonResponse({'result': 'error',\n                                 'error': 'Cannot import layer of ' + layer.__class__.__name__ + ' type'})\n            raise Exception('Cannot import layer of ' +\n                            layer.__class__.__name__ + ' type')\n    # collect names of all zeroPad layers\n    zeroPad = []\n    # Transfer parameters and connections from zero pad\n    # The 'pad' param is a list with upto 3 elements\n    for node in net:\n        if (net[node]['info']['type'] == 'Pad'):\n            net[net[node]['connection']['output'][0]]['connection']['input'] = \\\n                net[node]['connection']['input']\n            net[net[node]['connection']['input'][0]]['connection']['output'] = \\\n                net[node]['connection']['output']\n            net[net[node]['connection']['output'][0]]['params']['pad_w'] += \\\n                net[node]['params']['pad'][0]\n            if (net[net[node]['connection']['output'][0]]['params']['layer_type'] == '2D'):\n                net[net[node]['connection']['output'][0]]['params']['pad_h'] += \\\n                    net[node]['params']['pad'][1]\n            elif (net[net[node]['connection']['output'][0]]['params']['layer_type'] == '3D'):\n                net[net[node]['connection']['output'][0]]['params']['pad_h'] += \\\n                    net[node]['params']['pad'][1]\n                net[net[node]['connection']['output'][0]]['params']['pad_d'] += \\\n                    net[node]['params']['pad'][2]\n            zeroPad.append(node)\n        # Switching connection order to handle visualization\n        elif (net[node]['info']['type'] == 'Eltwise'):\n            net[node]['connection']['input'] = net[node]['connection']['input'][::-1]\n    for node in zeroPad:\n        net.pop(node, None)\n    return JsonResponse({'result': 'success', 'net': net, 'net_name': model.name})\n"""
keras_app/views/layers_export.py,0,"b""import numpy as np\n\nfrom keras.layers import Dense, Activation, Dropout, Flatten, Reshape, Permute, RepeatVector\nfrom keras.layers import ActivityRegularization, Masking\nfrom keras.layers import Conv1D, Conv2D, Conv3D, Conv2DTranspose, SeparableConv2D\nfrom keras.layers import UpSampling1D, UpSampling2D, UpSampling3D\nfrom keras.layers import MaxPooling1D, MaxPooling2D, MaxPooling3D\nfrom keras.layers import AveragePooling1D, AveragePooling2D, AveragePooling3D\nfrom keras.layers import ZeroPadding1D, ZeroPadding2D, ZeroPadding3D\nfrom keras.layers import LocallyConnected1D, LocallyConnected2D\nfrom keras.layers import SimpleRNN, LSTM, GRU\nfrom keras.layers import Embedding\nfrom keras.layers import add, multiply, maximum, concatenate, average, dot\nfrom keras.layers.advanced_activations import LeakyReLU, PReLU, ELU, ThresholdedReLU\nfrom keras.layers import BatchNormalization\nfrom keras.layers import GaussianNoise, GaussianDropout, AlphaDropout\nfrom keras.layers import Input\nfrom keras.layers import TimeDistributed, Bidirectional\nfrom keras import regularizers\nfrom ..custom_layers.lrn import LRN\n\nfillerMap = {\n    'constant': 'Constant',\n    'uniform': 'RandomUniform',\n    'gaussian': 'RandomNormal',\n    'xavier': 'glorot_normal',\n    'msra': 'he_normal'\n}\n\nregularizerMap = {\n    'l1': regularizers.l1(),\n    'l2': regularizers.l2(),\n    'l1_l2': regularizers.l1_l2(),\n    'L1L2': regularizers.l1_l2(),\n    'None': None\n}\n\nconstraintMap = {\n    'max_norm': 'max_norm',\n    'non_neg': 'non_neg',\n    'unit_norm': 'unit_norm',\n    'MaxNorm': 'max_norm',\n    'NonNeg': 'non_neg',\n    'UnitNorm': 'unit_norm',\n    'None': None\n}\n\n\n# ********** Data Layers **********\ndef data(layer, layer_in, layerId):\n    out = {layerId: Input(layer['shape']['output']\n                          [1:] + layer['shape']['output'][:1])}\n    return out\n\n\n# ********** Core Layers **********\ndef dense(layer, layer_in, layerId, tensor=True):\n    out = {}\n    if (len(layer['shape']['input']) > 1):\n        out[layerId + 'Flatten'] = Flatten()(*layer_in)\n        layer_in = [out[layerId + 'Flatten']]\n    units = layer['params']['num_output']\n    if (layer['params']['weight_filler'] in fillerMap):\n        kernel_initializer = fillerMap[layer['params']['weight_filler']]\n    else:\n        kernel_initializer = layer['params']['weight_filler']\n    if (layer['params']['bias_filler'] in fillerMap):\n        bias_initializer = fillerMap[layer['params']['bias_filler']]\n    else:\n        bias_initializer = layer['params']['bias_filler']\n    # safety checks to avoid runtime errors\n    kernel_regularizer = None\n    bias_regularizer = None\n    activity_regularizer = None\n    kernel_constraint = None\n    bias_constraint = None\n    if 'kernel_regularizer' in layer['params']:\n        kernel_regularizer = regularizerMap[layer['params']['kernel_regularizer']]\n    if 'bias_regularizer' in layer['params']:\n        bias_regularizer = regularizerMap[layer['params']['bias_regularizer']]\n    if 'activity_regularizer' in layer['params']:\n        activity_regularizer = regularizerMap[layer['params']\n                                              ['activity_regularizer']]\n    if 'kernel_constraint' in layer['params']:\n        kernel_constraint = constraintMap[layer['params']['kernel_constraint']]\n    if 'bias_constraint' in layer['params']:\n        bias_constraint = constraintMap[layer['params']['bias_constraint']]\n    use_bias = layer['params']['use_bias']\n    out[layerId] = Dense(units=units, kernel_initializer=kernel_initializer,\n                         kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer,\n                         activity_regularizer=activity_regularizer, bias_constraint=bias_constraint,\n                         kernel_constraint=kernel_constraint, use_bias=use_bias,\n                         bias_initializer=bias_initializer)\n    if tensor:\n        out[layerId] = out[layerId](*layer_in)\n    return out\n\n\ndef activation(layer, layer_in, layerId, tensor=True):\n    out = {}\n    if (layer['info']['type'] == 'ReLU'):\n        if ('negative_slope' in layer['params'] and layer['params']['negative_slope'] != 0):\n            out[layerId] = LeakyReLU(alpha=layer['params']['negative_slope'])\n        else:\n            out[layerId] = Activation('relu')\n    elif (layer['info']['type'] == 'PReLU'):\n        out[layerId] = PReLU()\n    elif (layer['info']['type'] == 'ELU'):\n        out[layerId] = ELU(alpha=layer['params']['alpha'])\n    elif (layer['info']['type'] == 'ThresholdedReLU'):\n        out[layerId] = ThresholdedReLU(theta=layer['params']['theta'])\n    elif (layer['info']['type'] == 'Sigmoid'):\n        out[layerId] = Activation('sigmoid')\n    elif (layer['info']['type'] == 'TanH'):\n        out[layerId] = Activation('tanh')\n    elif (layer['info']['type'] == 'Softmax'):\n        out[layerId] = Activation('softmax')\n    elif (layer['info']['type'] == 'SELU'):\n        out[layerId] = Activation('selu')\n    elif (layer['info']['type'] == 'Softplus'):\n        out[layerId] = Activation('softplus')\n    elif (layer['info']['type'] == 'Softsign'):\n        out[layerId] = Activation('softsign')\n    elif (layer['info']['type'] == 'HardSigmoid'):\n        out[layerId] = Activation('hard_sigmoid')\n    elif (layer['info']['type'] == 'Linear'):\n        out[layerId] = Activation('linear')\n    if tensor:\n        out[layerId] = out[layerId](*layer_in)\n    return out\n\n\ndef dropout(layer, layer_in, layerId, tensor=True):\n    out = {layerId: Dropout(0.5)}\n    if tensor:\n        out[layerId] = out[layerId](*layer_in)\n    return out\n\n\ndef flatten(layer, layer_in, layerId, tensor=True):\n    out = {layerId: Flatten()}\n    if tensor:\n        out[layerId] = out[layerId](*layer_in)\n    return out\n\n\ndef reshape(layer, layer_in, layerId, tensor=True):\n    shape = map(int, layer['params']['dim'].split(','))\n    out = {layerId: Reshape(shape[2:] + shape[1:2])}\n    if tensor:\n        out[layerId] = out[layerId](*layer_in)\n    return out\n\n\ndef permute(layer, layer_in, layerId, tensor=True):\n    out = {layerId: Permute(map(int, layer['params']['dim'].split(',')))}\n    if tensor:\n        out[layerId] = out[layerId](*layer_in)\n    return out\n\n\ndef repeat_vector(layer, layer_in, layerId, tensor=True):\n    out = {layerId: RepeatVector(layer['params']['n'])}\n    if tensor:\n        out[layerId] = out[layerId](*layer_in)\n    return out\n\n\ndef regularization(layer, layer_in, layerId, tensor=True):\n    l1 = layer['params']['l1']\n    l2 = layer['params']['l2']\n    out = {layerId: ActivityRegularization(l1=l1, l2=l2)}\n    if tensor:\n        out[layerId] = out[layerId](*layer_in)\n    return out\n\n\ndef masking(layer, layer_in, layerId, tensor=True):\n    out = {layerId: Masking(mask_value=layer['params']['mask_value'])}\n    if tensor:\n        out[layerId] = out[layerId](*layer_in)\n    return out\n\n\n# ********** Convolution Layers **********\ndef convolution(layer, layer_in, layerId, tensor=True):\n    convMap = {\n        '1D': Conv1D,\n        '2D': Conv2D,\n        '3D': Conv3D\n    }\n    out = {}\n    padding = get_padding(layer)\n    if (layer['params']['weight_filler'] in fillerMap):\n        kernel_initializer = fillerMap[layer['params']['weight_filler']]\n    else:\n        kernel_initializer = layer['params']['weight_filler']\n    if (layer['params']['bias_filler'] in fillerMap):\n        bias_initializer = fillerMap[layer['params']['bias_filler']]\n    else:\n        bias_initializer = layer['params']['bias_filler']\n    # safety checks to avoid runtime errors\n    filters = layer['params']['num_output']\n    kernel_regularizer = None\n    bias_regularizer = None\n    activity_regularizer = None\n    kernel_constraint = None\n    bias_constraint = None\n    if 'kernel_regularizer' in layer['params']:\n        kernel_regularizer = regularizerMap[layer['params']['kernel_regularizer']]\n    if 'bias_regularizer' in layer['params']:\n        bias_regularizer = regularizerMap[layer['params']['bias_regularizer']]\n    if 'activity_regularizer' in layer['params']:\n        activity_regularizer = regularizerMap[layer['params']\n                                              ['activity_regularizer']]\n    if 'kernel_constraint' in layer['params']:\n        kernel_constraint = constraintMap[layer['params']['kernel_constraint']]\n    if 'bias_constraint' in layer['params']:\n        bias_constraint = constraintMap[layer['params']['bias_constraint']]\n    use_bias = layer['params']['use_bias']\n    layer_type = layer['params']['layer_type']\n    if (layer_type == '1D'):\n        strides = layer['params']['stride_w']\n        kernel = layer['params']['kernel_w']\n        dilation_rate = layer['params']['dilation_w']\n        if (padding == 'custom'):\n            p_w = layer['params']['pad_w']\n            out[layerId + 'Pad'] = ZeroPadding1D(padding=p_w)(*layer_in)\n            padding = 'valid'\n            layer_in = [out[layerId + 'Pad']]\n    elif (layer_type == '2D'):\n        strides = (layer['params']['stride_h'], layer['params']['stride_w'])\n        kernel = (layer['params']['kernel_h'], layer['params']['kernel_w'])\n        dilation_rate = (layer['params']['dilation_h'],\n                         layer['params']['dilation_w'])\n        if (padding == 'custom'):\n            p_h, p_w = layer['params']['pad_h'], layer['params']['pad_w']\n            out[layerId + 'Pad'] = ZeroPadding2D(padding=(p_h, p_w))(*layer_in)\n            padding = 'valid'\n            layer_in = [out[layerId + 'Pad']]\n    else:\n        strides = (layer['params']['stride_h'], layer['params']['stride_w'],\n                   layer['params']['stride_d'])\n        kernel = (layer['params']['kernel_h'], layer['params']['kernel_w'],\n                  layer['params']['kernel_d'])\n        dilation_rate = (layer['params']['dilation_h'], layer['params']['dilation_w'],\n                         layer['params']['dilation_d'])\n        if (padding == 'custom'):\n            p_h, p_w, p_d = layer['params']['pad_h'], layer['params']['pad_w'],\\\n                layer['params']['pad_d']\n            out[layerId +\n                'Pad'] = ZeroPadding3D(padding=(p_h, p_w, p_d))(*layer_in)\n            padding = 'valid'\n            layer_in = [out[layerId + 'Pad']]\n    out[layerId] = convMap[layer_type](filters, kernel, strides=strides, padding=padding,\n                                       dilation_rate=dilation_rate,\n                                       kernel_initializer=kernel_initializer,\n                                       bias_initializer=bias_initializer,\n                                       kernel_regularizer=kernel_regularizer,\n                                       bias_regularizer=bias_regularizer,\n                                       activity_regularizer=activity_regularizer, use_bias=use_bias,\n                                       bias_constraint=bias_constraint,\n                                       kernel_constraint=kernel_constraint)\n    if tensor:\n        out[layerId] = out[layerId](*layer_in)\n    return out\n\n\n# Separable Convolution is currently not supported with Theano backend\n\ndef depthwiseConv(layer, layer_in, layerId, tensor=True):\n    out = {}\n    padding = get_padding(layer)\n    filters = layer['params']['num_output']\n    k_h, k_w = layer['params']['kernel_h'], layer['params']['kernel_w']\n    s_h, s_w = layer['params']['stride_h'], layer['params']['stride_w']\n    depth_multiplier = layer['params']['depth_multiplier']\n    use_bias = layer['params']['use_bias']\n    depthwise_initializer = layer['params']['depthwise_initializer']\n    pointwise_initializer = layer['params']['pointwise_initializer']\n    bias_initializer = layer['params']['bias_initializer']\n    if (padding == 'custom'):\n        p_h, p_w = layer['params']['pad_h'], layer['params']['pad_w']\n        out[layerId + 'Pad'] = ZeroPadding2D(padding=(p_h, p_w))(*layer_in)\n        padding = 'valid'\n        layer_in = [out[layerId + 'Pad']]\n    depthwise_regularizer = regularizerMap[layer['params']['depthwise_regularizer']]\n    pointwise_regularizer = regularizerMap[layer['params']['pointwise_regularizer']]\n    bias_regularizer = regularizerMap[layer['params']['bias_regularizer']]\n    activity_regularizer = regularizerMap[layer['params']['activity_regularizer']]\n    depthwise_constraint = constraintMap[layer['params']['depthwise_constraint']]\n    pointwise_constraint = constraintMap[layer['params']['pointwise_constraint']]\n    bias_constraint = constraintMap[layer['params']['bias_constraint']]\n    out[layerId] = SeparableConv2D(filters, [k_h, k_w], strides=(s_h, s_w), padding=padding,\n                                   depth_multiplier=depth_multiplier, use_bias=use_bias,\n                                   depthwise_initializer=depthwise_initializer,\n                                   pointwise_initializer=pointwise_initializer,\n                                   bias_initializer=bias_initializer,\n                                   depthwise_regularizer=depthwise_regularizer,\n                                   pointwise_regularizer=pointwise_regularizer,\n                                   bias_regularizer=bias_regularizer,\n                                   activity_regularizer=activity_regularizer,\n                                   depthwise_constraint=depthwise_constraint,\n                                   pointwise_constraint=pointwise_constraint,\n                                   bias_constraint=bias_constraint,)(*layer_in)\n    return out\n\n\ndef deconvolution(layer, layer_in, layerId, tensor=True):\n    out = {}\n    padding = get_padding(layer)\n    k_h, k_w = layer['params']['kernel_h'], layer['params']['kernel_w']\n    s_h, s_w = layer['params']['stride_h'], layer['params']['stride_w']\n    d_h, d_w = layer['params']['dilation_h'], layer['params']['dilation_w']\n    if (layer['params']['weight_filler'] in fillerMap):\n        kernel_initializer = fillerMap[layer['params']['weight_filler']]\n    else:\n        kernel_initializer = layer['params']['weight_filler']\n    if (layer['params']['bias_filler'] in fillerMap):\n        bias_initializer = fillerMap[layer['params']['bias_filler']]\n    else:\n        bias_initializer = layer['params']['bias_filler']\n    filters = layer['params']['num_output']\n    if (padding == 'custom'):\n        p_h, p_w = layer['params']['pad_h'], layer['params']['pad_w']\n        out[layerId + 'Pad'] = ZeroPadding2D(padding=(p_h, p_w))(*layer_in)\n        padding = 'valid'\n        layer_in = [out[layerId + 'Pad']]\n    kernel_regularizer = regularizerMap[layer['params']['kernel_regularizer']]\n    bias_regularizer = regularizerMap[layer['params']['bias_regularizer']]\n    activity_regularizer = regularizerMap[layer['params']\n                                          ['activity_regularizer']]\n    kernel_constraint = constraintMap[layer['params']['kernel_constraint']]\n    bias_constraint = constraintMap[layer['params']['bias_constraint']]\n    use_bias = layer['params']['use_bias']\n    out[layerId] = Conv2DTranspose(filters, [k_h, k_w], strides=(s_h, s_w), padding=padding,\n                                   dilation_rate=(\n                                       d_h, d_w), kernel_initializer=kernel_initializer,\n                                   bias_initializer=bias_initializer,\n                                   kernel_regularizer=kernel_regularizer,\n                                   bias_regularizer=bias_regularizer,\n                                   activity_regularizer=activity_regularizer, use_bias=use_bias,\n                                   bias_constraint=bias_constraint,\n                                   kernel_constraint=kernel_constraint)\n    if tensor:\n        out[layerId] = out[layerId](*layer_in)\n    return out\n\n\ndef upsample(layer, layer_in, layerId, tensor=True):\n    upsampleMap = {\n        '1D': UpSampling1D,\n        '2D': UpSampling2D,\n        '3D': UpSampling3D\n    }\n    out = {}\n    layer_type = layer['params']['layer_type']\n    if (layer_type == '1D'):\n        size = layer['params']['size_w']\n    elif (layer_type == '2D'):\n        size = (layer['params']['size_h'], layer['params']['size_w'])\n    else:\n        size = (layer['params']['size_h'], layer['params']['size_w'],\n                layer['params']['size_d'])\n    out[layerId] = upsampleMap[layer_type](size=size)\n    if tensor:\n        out[layerId] = out[layerId](*layer_in)\n    return out\n\n\n# ********** Pooling Layers **********\ndef pooling(layer, layer_in, layerId, tensor=True):\n    poolMap = {\n        ('1D', 'MAX'): MaxPooling1D,\n        ('2D', 'MAX'): MaxPooling2D,\n        ('3D', 'MAX'): MaxPooling3D,\n        ('1D', 'AVE'): AveragePooling1D,\n        ('2D', 'AVE'): AveragePooling2D,\n        ('3D', 'AVE'): AveragePooling3D,\n    }\n    out = {}\n    layer_type = layer['params']['layer_type']\n    pool_type = layer['params']['pool']\n    padding = get_padding(layer)\n    if (layer_type == '1D'):\n        strides = layer['params']['stride_w']\n        kernel = layer['params']['kernel_w']\n        if (padding == 'custom'):\n            p_w = layer['params']['pad_w']\n            out[layerId + 'Pad'] = ZeroPadding1D(padding=p_w)(*layer_in)\n            padding = 'valid'\n            layer_in = [out[layerId + 'Pad']]\n    elif (layer_type == '2D'):\n        strides = (layer['params']['stride_h'], layer['params']['stride_w'])\n        kernel = (layer['params']['kernel_h'], layer['params']['kernel_w'])\n        if (padding == 'custom'):\n            p_h, p_w = layer['params']['pad_h'], layer['params']['pad_w']\n            out[layerId + 'Pad'] = ZeroPadding2D(padding=(p_h, p_w))(*layer_in)\n            padding = 'valid'\n            layer_in = [out[layerId + 'Pad']]\n    else:\n        strides = (layer['params']['stride_h'], layer['params']['stride_w'],\n                   layer['params']['stride_d'])\n        kernel = (layer['params']['kernel_h'], layer['params']['kernel_w'],\n                  layer['params']['kernel_d'])\n        if (padding == 'custom'):\n            p_h, p_w, p_d = layer['params']['pad_h'], layer['params']['pad_w'],\\\n                layer['params']['pad_d']\n            out[layerId +\n                'Pad'] = ZeroPadding3D(padding=(p_h, p_w, p_d))(*layer_in)\n            padding = 'valid'\n            layer_in = [out[layerId + 'Pad']]\n    # Note - figure out a permanent fix for padding calculation of layers\n    # in case padding is given in layer attributes\n    # if ('padding' in layer['params']):\n    #    padding = layer['params']['padding']\n    out[layerId] = poolMap[(layer_type, pool_type)](\n        pool_size=kernel, strides=strides, padding=padding)\n    if tensor:\n        out[layerId] = out[layerId](*layer_in)\n    return out\n\n\n# ********** Locally-connected Layers **********\ndef locally_connected(layer, layer_in, layerId, tensor=True):\n    localMap = {\n        '1D': LocallyConnected1D,\n        '2D': LocallyConnected2D,\n    }\n    out = {}\n    kernel_initializer = layer['params']['kernel_initializer']\n    bias_initializer = layer['params']['bias_initializer']\n    filters = layer['params']['filters']\n    kernel_regularizer = regularizerMap[layer['params']['kernel_regularizer']]\n    bias_regularizer = regularizerMap[layer['params']['bias_regularizer']]\n    activity_regularizer = regularizerMap[layer['params']\n                                          ['activity_regularizer']]\n    kernel_constraint = constraintMap[layer['params']['kernel_constraint']]\n    bias_constraint = constraintMap[layer['params']['bias_constraint']]\n    use_bias = layer['params']['use_bias']\n    layer_type = layer['params']['layer_type']\n    if (layer_type == '1D'):\n        strides = layer['params']['stride_w']\n        kernel = layer['params']['kernel_w']\n    else:\n        strides = (layer['params']['stride_h'], layer['params']['stride_w'])\n        kernel = (layer['params']['kernel_h'], layer['params']['kernel_w'])\n    out[layerId] = localMap[layer_type](filters, kernel, strides=strides, padding='valid',\n                                        kernel_initializer=kernel_initializer,\n                                        bias_initializer=bias_initializer,\n                                        kernel_regularizer=kernel_regularizer,\n                                        bias_regularizer=bias_regularizer,\n                                        activity_regularizer=activity_regularizer, use_bias=use_bias,\n                                        bias_constraint=bias_constraint,\n                                        kernel_constraint=kernel_constraint)\n    if tensor:\n        out[layerId] = out[layerId](*layer_in)\n    return out\n\n\n# ********** Recurrent Layers **********\ndef recurrent(layer, layer_in, layerId, tensor=True):\n    out = {}\n    units = layer['params']['num_output']\n    if (layer['params']['weight_filler'] in fillerMap):\n        kernel_initializer = fillerMap[layer['params']['weight_filler']]\n    else:\n        kernel_initializer = layer['params']['weight_filler']\n    if (layer['params']['bias_filler'] in fillerMap):\n        bias_initializer = fillerMap[layer['params']['bias_filler']]\n    else:\n        bias_initializer = layer['params']['bias_filler']\n    recurrent_initializer = layer['params']['recurrent_initializer']\n    kernel_regularizer = regularizerMap[layer['params']['kernel_regularizer']]\n    recurrent_regularizer = regularizerMap[layer['params']\n                                           ['recurrent_regularizer']]\n    bias_regularizer = regularizerMap[layer['params']['bias_regularizer']]\n    activity_regularizer = regularizerMap[layer['params']\n                                          ['activity_regularizer']]\n    kernel_constraint = constraintMap[layer['params']['kernel_constraint']]\n    recurrent_constraint = constraintMap[layer['params']\n                                         ['recurrent_constraint']]\n    bias_constraint = constraintMap[layer['params']['bias_constraint']]\n    use_bias = layer['params']['use_bias']\n    dropout = layer['params']['dropout']\n    recurrent_dropout = layer['params']['recurrent_dropout']\n    if ('return_sequences' in layer['params']):\n        return_sequences = layer['params']['return_sequences']\n    else:\n        return_sequences = False\n    if (layer['info']['type'] == 'GRU'):\n        recurrent_activation = layer['params']['recurrent_activation']\n        out[layerId] = GRU(units, kernel_initializer=kernel_initializer,\n                           bias_initializer=bias_initializer,\n                           recurrent_activation=recurrent_activation,\n                           recurrent_initializer=recurrent_initializer,\n                           kernel_regularizer=kernel_regularizer,\n                           recurrent_regularizer=recurrent_regularizer,\n                           bias_regularizer=bias_regularizer, activity_regularizer=activity_regularizer,\n                           kernel_constraint=kernel_constraint, recurrent_constraint=recurrent_constraint,\n                           bias_constraint=bias_constraint, use_bias=use_bias, dropout=dropout,\n                           recurrent_dropout=recurrent_dropout)\n    elif (layer['info']['type'] == 'LSTM'):\n        recurrent_activation = layer['params']['recurrent_activation']\n        unit_forget_bias = layer['params']['unit_forget_bias']\n        out[layerId] = LSTM(units, kernel_initializer=kernel_initializer,\n                            bias_initializer=bias_initializer,\n                            recurrent_activation=recurrent_activation, unit_forget_bias=unit_forget_bias,\n                            recurrent_initializer=recurrent_initializer,\n                            kernel_regularizer=kernel_regularizer,\n                            recurrent_regularizer=recurrent_regularizer,\n                            bias_regularizer=bias_regularizer, activity_regularizer=activity_regularizer,\n                            kernel_constraint=kernel_constraint, recurrent_constraint=recurrent_constraint,\n                            bias_constraint=bias_constraint, use_bias=use_bias, dropout=dropout,\n                            recurrent_dropout=recurrent_dropout, return_sequences=return_sequences)\n    else:\n        out[layerId] = SimpleRNN(units, kernel_initializer=kernel_initializer,\n                                 bias_initializer=bias_initializer,\n                                 recurrent_initializer=recurrent_initializer,\n                                 kernel_regularizer=kernel_regularizer,\n                                 recurrent_regularizer=recurrent_regularizer,\n                                 bias_regularizer=bias_regularizer,\n                                 activity_regularizer=activity_regularizer,\n                                 kernel_constraint=kernel_constraint,\n                                 recurrent_constraint=recurrent_constraint,\n                                 bias_constraint=bias_constraint,\n                                 use_bias=use_bias, dropout=dropout,\n                                 recurrent_dropout=recurrent_dropout)\n    if tensor:\n        out[layerId] = out[layerId](*layer_in)\n    return out\n\n\n# ********** Embedding Layers **********\ndef embed(layer, layer_in, layerId, tensor=True):\n    out = {}\n    if (layer['params']['weight_filler'] in fillerMap):\n        embeddings_initializer = fillerMap[layer['params']['weight_filler']]\n    else:\n        embeddings_initializer = layer['params']['weight_filler']\n    embeddings_regularizer = regularizerMap[layer['params']\n                                            ['embeddings_regularizer']]\n    embeddings_constraint = constraintMap[layer['params']\n                                          ['embeddings_constraint']]\n    mask_zero = layer['params']['mask_zero']\n    if (layer['params']['input_length']):\n        input_length = layer['params']['input_length']\n    else:\n        input_length = None\n    out[layerId] = Embedding(layer['params']['input_dim'], layer['params']['num_output'],\n                             embeddings_initializer=embeddings_initializer,\n                             embeddings_regularizer=embeddings_regularizer,\n                             embeddings_constraint=embeddings_constraint,\n                             mask_zero=mask_zero, input_length=input_length)\n    if tensor:\n        out[layerId] = out[layerId](*layer_in)\n    return out\n\n\n# ********** Merge Layers **********\ndef eltwise(layer, layer_in, layerId):\n    out = {}\n    if (layer['params']['layer_type'] == 'Multiply'):\n        # This input reverse is to handle visualization\n        out[layerId] = multiply(layer_in[::-1])\n    elif (layer['params']['layer_type'] == 'Sum'):\n        out[layerId] = add(layer_in[::-1])\n    elif (layer['params']['layer_type'] == 'Average'):\n        out[layerId] = average(layer_in[::-1])\n    elif (layer['params']['layer_type'] == 'Dot'):\n        out[layerId] = dot(layer_in[::-1], -1)\n    else:\n        out[layerId] = maximum(layer_in[::-1])\n    return out\n\n\ndef concat(layer, layer_in, layerId):\n    out = {layerId: concatenate(layer_in)}\n    return out\n\n\n# ********** Noise Layers **********\ndef gaussian_noise(layer, layer_in, layerId, tensor=True):\n    stddev = layer['params']['stddev']\n    out = {layerId: GaussianNoise(stddev=stddev)}\n    if tensor:\n        out[layerId] = out[layerId](*layer_in)\n    return out\n\n\ndef gaussian_dropout(layer, layer_in, layerId, tensor=True):\n    rate = layer['params']['rate']\n    out = {layerId: GaussianDropout(rate=rate)}\n    if tensor:\n        out[layerId] = out[layerId](*layer_in)\n    return out\n\n\ndef alpha_dropout(layer, layer_in, layerId, tensor=True):\n    rate = layer['params']['rate']\n    seed = layer['params']['seed']\n    out = {layerId: AlphaDropout(rate=rate, seed=seed)}\n    if tensor:\n        out[layerId] = out[layerId](*layer_in)\n    return out\n\n\n# ********** Normalisation Layers **********\ndef batch_norm(layer, layer_in, layerId, idNext, nextLayer):\n    out = {}\n    momentum = layer['params']['moving_average_fraction']\n    eps = float(layer['params']['eps'])\n    if (eps <= 1e-5):\n        eps = 1e-4  # In Keras the smallest epsilon allowed is 1e-5\n    moving_mean_initializer = layer['params']['moving_mean_initializer']\n    moving_variance_initializer = layer['params']['moving_variance_initializer']\n    if (nextLayer['info']['type'] == 'Scale'):\n        axis = nextLayer['params']['axis']\n        # In Caffe the first dimension has number of filters/outputs but in Keras it is the last\n        # dimension\n        if (axis == 1):\n            axis = -1\n        center = nextLayer['params']['bias_term']\n        scale = nextLayer['params']['scale']\n        if (nextLayer['params']['filler'] in fillerMap):\n            gamma_initializer = fillerMap[nextLayer['params']['filler']]\n        else:\n            gamma_initializer = nextLayer['params']['filler']\n        if (nextLayer['params']['bias_filler'] in fillerMap):\n            beta_initializer = fillerMap[nextLayer['params']['bias_filler']]\n        else:\n            beta_initializer = nextLayer['params']['bias_filler']\n        gamma_regularizer = regularizerMap[nextLayer['params']\n                                           ['gamma_regularizer']]\n        beta_regularizer = regularizerMap[nextLayer['params']\n                                          ['beta_regularizer']]\n        gamma_constraint = constraintMap[nextLayer['params']\n                                         ['gamma_constraint']]\n        beta_constraint = constraintMap[nextLayer['params']['beta_constraint']]\n        out[idNext] = BatchNormalization(axis=axis, momentum=momentum, epsilon=eps,\n                                         moving_mean_initializer=moving_mean_initializer,\n                                         moving_variance_initializer=moving_variance_initializer,\n                                         center=center, scale=scale,\n                                         gamma_initializer=gamma_initializer,\n                                         beta_initializer=beta_initializer,\n                                         gamma_regularizer=gamma_regularizer,\n                                         beta_regularizer=beta_regularizer,\n                                         gamma_constraint=gamma_constraint,\n                                         beta_constraint=beta_constraint)(*layer_in)\n    else:\n        out[layerId] = BatchNormalization(momentum=momentum, epsilon=eps,\n                                          moving_mean_initializer=moving_mean_initializer,\n                                          moving_variance_initializer=moving_variance_initializer,\n                                          scale=False, center=False)(*layer_in)\n    return out\n\n\ndef bidirectional(layerId, idNext, net, layer_in, layer_map):\n    out = {}\n    if net[layerId]['params']['merge_mode'] == '':\n        net[layerId]['params']['merge_mode'] = None\n    mode = net[layerId]['params']['merge_mode']\n    out[layerId] = Bidirectional(\n        layer_map[net[idNext]['info']['type']](\n            net[idNext], layer_in, idNext, False)[idNext],\n        merge_mode=mode)(*layer_in)\n    return out\n\n\ndef time_distributed(layerId, idNext, net, layer_in, layer_map):\n    out = {}\n    out[layerId] = TimeDistributed(\n        layer_map[net[idNext]['info']['type']](net[idNext], layer_in, idNext, False)[idNext])(*layer_in)\n\n\n# Custom LRN for Tensorflow export and Keras export\ndef lrn(layer, layer_in, layerId):\n    alpha = layer['params']['alpha']\n    beta = layer['params']['beta']\n    k = layer['params']['beta']\n    n = layer['params']['local_size']\n    out = {}\n    out[layerId] = LRN(alpha=alpha, beta=beta, k=k, n=n)(*layer_in)\n    return out\n\n\n# logic as used in caffe-tensorflow\n# https://github.com/ethereon/caffe-tensorflow/blob/master/kaffe/tensorflow/transformer.py\ndef get_padding(layer):\n    if (layer['info']['type'] in ['Deconvolution', 'DepthwiseConv']):\n        _, i_h, i_w = layer['shape']['output']\n        _, o_h, o_w = layer['shape']['input']\n        k_h, k_w = layer['params']['kernel_h'], layer['params']['kernel_w']\n        s_h, s_w = layer['params']['stride_h'], layer['params']['stride_w']\n        layer['params']['layer_type'] = '2D'\n    else:\n        if (layer['params']['layer_type'] == '1D'):\n            i_w = layer['shape']['input'][0]\n            o_w = layer['shape']['output'][1]\n            k_w = layer['params']['kernel_w']\n            s_w = layer['params']['stride_w']\n\n        elif (layer['params']['layer_type'] == '2D'):\n            _, i_h, i_w = layer['shape']['input']\n            _, o_h, o_w = layer['shape']['output']\n            k_h, k_w = layer['params']['kernel_h'], layer['params']['kernel_w']\n            s_h, s_w = layer['params']['stride_h'], layer['params']['stride_w']\n        else:\n            _, i_h, i_w, i_d = layer['shape']['input']\n            _, o_h, o_w, o_d = layer['shape']['output']\n            k_h, k_w, k_d = layer['params']['kernel_h'], layer['params']['kernel_w'],\\\n                layer['params']['kernel_d']\n            s_h, s_w, s_d = layer['params']['stride_h'], layer['params']['stride_w'],\\\n                layer['params']['stride_d']\n    if (layer['params']['layer_type'] == '1D'):\n        s_o_w = np.ceil(i_w / float(s_w))\n        if (o_w == s_o_w):\n            return 'same'\n        v_o_w = np.ceil((i_w - k_w + 1.0) / float(s_w))\n        if (o_w == v_o_w):\n            return 'valid'\n        return 'custom'\n    elif (layer['params']['layer_type'] == '2D'):\n        s_o_h = np.ceil(i_h / float(s_h))\n        s_o_w = np.ceil(i_w / float(s_w))\n        if (o_h == s_o_h) and (o_w == s_o_w):\n            return 'same'\n        v_o_h = np.ceil((i_h - k_h + 1.0) / float(s_h))\n        v_o_w = np.ceil((i_w - k_w + 1.0) / float(s_w))\n        if (o_h == v_o_h) and (o_w == v_o_w):\n            return 'valid'\n        return 'custom'\n    else:\n        s_o_h = np.ceil(i_h / float(s_h))\n        s_o_w = np.ceil(i_w / float(s_w))\n        s_o_d = np.ceil(i_d / float(s_d))\n        if (o_h == s_o_h) and (o_w == s_o_w) and (o_d == s_o_d):\n            return 'same'\n        v_o_h = np.ceil((i_h - k_h + 1.0) / float(s_h))\n        v_o_w = np.ceil((i_w - k_w + 1.0) / float(s_w))\n        v_o_d = np.ceil((i_d - k_d + 1.0) / float(s_d))\n        if (o_h == v_o_h) and (o_w == v_o_w) and (o_d == v_o_d):\n            return 'valid'\n        return 'custom'\n"""
keras_app/views/layers_import.py,0,"b""import numpy as np\n\n\n# ********** Data Layers **********\ndef Input(layer):\n    params = {}\n    shape = layer.batch_input_shape\n    if (len(shape) == 2):\n        params['dim'] = str([1, shape[1]])[1:-1]\n    else:\n        params['dim'] = str([1, shape[-1]] + list(shape[1:-1]))[1:-1]\n    return jsonLayer('Input', params, layer)\n\n\n# ********** Core Layers **********\ndef Dense(layer):\n    params = {}\n    params['weight_filler'] = layer.kernel_initializer.__class__.__name__\n    params['bias_filler'] = layer.bias_initializer.__class__.__name__\n    params['num_output'] = layer.units\n    if (layer.kernel_regularizer):\n        params['kernel_regularizer'] = layer.kernel_regularizer.__class__.__name__\n    if (layer.bias_regularizer):\n        params['bias_regularizer'] = layer.bias_regularizer.__class__.__name__\n    if (layer.activity_regularizer):\n        params['activity_regularizer'] = layer.activity_regularizer.__class__.__name__\n    if (layer.kernel_constraint):\n        params['kernel_constraint'] = layer.kernel_constraint.__class__.__name__\n    if (layer.bias_constraint):\n        params['bias_constraint'] = layer.bias_constraint.__class__.__name__\n    params['use_bias'] = layer.use_bias\n    return jsonLayer('InnerProduct', params, layer)\n\n\ndef Activation(layer):\n    activationMap = {\n        'softmax': 'Softmax',\n        'relu': 'ReLU',\n        'tanh': 'TanH',\n        'sigmoid': 'Sigmoid',\n        'selu': 'SELU',\n        'softplus': 'Softplus',\n        'softsign': 'Softsign',\n        'hard_sigmoid': 'HardSigmoid',\n        'linear': 'Linear'\n    }\n    if (layer.__class__.__name__ == 'Activation'):\n        return jsonLayer(activationMap[layer.activation.func_name], {}, layer)\n    else:\n        tempLayer = {}\n        tempLayer['inbound_nodes'] = [\n            [[layer.name + layer.__class__.__name__]]]\n        return jsonLayer(activationMap[layer.activation.func_name], {}, tempLayer)\n\n\ndef Dropout(layer):\n    params = {}\n    if (layer.rate is not None):\n        params['rate'] = layer.rate\n    if (layer.seed is not None):\n        params['seed'] = layer.seed\n    if (layer.trainable is not None):\n        params['trainable'] = layer.trainable\n    return jsonLayer('Dropout', params, layer)\n\n\ndef Flatten(layer):\n    return jsonLayer('Flatten', {}, layer)\n\n\ndef Reshape(layer):\n    params = {}\n    shape = layer.target_shape\n    params['dim'] = str([1] + list(shape))[1:-1]\n    return jsonLayer('Reshape', params, layer)\n\n\ndef Permute(layer):\n    params = {}\n    params['dim'] = str(layer.dims)[1:-1]\n    return jsonLayer('Permute', params, layer)\n\n\ndef RepeatVector(layer):\n    params = {}\n    params['n'] = layer.n\n    return jsonLayer('RepeatVector', params, layer)\n\n\ndef ActivityRegularization(layer):\n    params = {}\n    params['l1'] = layer.l1\n    params['l2'] = layer.l2\n    return jsonLayer('Regularization', params, layer)\n\n\ndef Masking(layer):\n    params = {}\n    params['mask_value'] = layer.mask_value\n    return jsonLayer('Masking', params, layer)\n\n\n# ********** Convolutional Layers **********\ndef Convolution(layer):\n    params = {}\n    if (layer.__class__.__name__ == 'Conv1D'):\n        params['layer_type'] = '1D'\n        params['kernel_w'] = layer.kernel_size[0]\n        params['stride_w'] = layer.strides[0]\n        params['dilation_w'] = layer.dilation_rate[0]\n        params['pad_w'] = get_padding([params['kernel_w'], -1, -1,\n                                       params['stride_w'], -1, -1],\n                                      layer.input_shape, layer.output_shape,\n                                      layer.padding.lower(), '1D')\n    elif (layer.__class__.__name__ == 'Conv2D'):\n        params['layer_type'] = '2D'\n        params['kernel_h'], params['kernel_w'] = layer.kernel_size\n        params['stride_h'], params['stride_w'] = layer.strides\n        params['dilation_h'], params['dilation_w'] = layer.dilation_rate\n        params['pad_h'], params['pad_w'] = get_padding([params['kernel_w'], params['kernel_h'], -1,\n                                                        params['stride_w'], params['stride_h'], -1],\n                                                       layer.input_shape, layer.output_shape,\n                                                       layer.padding.lower(), '2D')\n    else:\n        params['layer_type'] = '3D'\n        params['kernel_h'], params['kernel_w'], params['kernel_d'] = layer.kernel_size\n        params['stride_h'], params['stride_w'], params['stride_d'] = layer.strides\n        params['dilation_h'], params['dilation_w'], params['dilation_d'] = layer.dilation_rate\n        params['pad_h'], params['pad_w'], params['pad_d'] = get_padding([params['kernel_w'],\n                                                                         params['kernel_h'],\n                                                                         params['kernel_d'],\n                                                                         params['stride_w'],\n                                                                         params['stride_h'],\n                                                                         params['stride_d']],\n                                                                        layer.input_shape,\n                                                                        layer.output_shape,\n                                                                        layer.padding.lower(), '3D')\n    params['weight_filler'] = layer.kernel_initializer.__class__.__name__\n    params['bias_filler'] = layer.bias_initializer.__class__.__name__\n    params['num_output'] = layer.filters\n    if (layer.kernel_regularizer):\n        params['kernel_regularizer'] = layer.kernel_regularizer.__class__.__name__\n    if (layer.bias_regularizer):\n        params['bias_regularizer'] = layer.bias_regularizer.__class__.__name__\n    if (layer.activity_regularizer):\n        params['activity_regularizer'] = layer.activity_regularizer.__class__.__name__\n    if (layer.kernel_constraint):\n        params['kernel_constraint'] = layer.kernel_constraint.__class__.__name__\n    if (layer.bias_constraint):\n        params['bias_constraint'] = layer.bias_constraint.__class__.__name__\n    params['use_bias'] = layer.use_bias\n    return jsonLayer('Convolution', params, layer)\n\n\n# Separable Convolution is currently not supported with Theano backend\n\ndef DepthwiseConv(layer):\n    params = {}\n    params['num_output'] = layer.filters\n    params['kernel_h'], params['kernel_w'] = layer.kernel_size\n    params['stride_h'], params['stride_w'] = layer.strides\n    params['pad_h'], params['pad_w'] = get_padding([params['kernel_w'], params['kernel_h'], -1,\n                                                    params['stride_w'], params['stride_h'], -1],\n                                                   layer.input_shape, layer.output_shape,\n                                                   layer.padding.lower(), '2D')\n    params['depth_multiplier'] = layer.depth_multiplier\n    params['use_bias'] = layer.use_bias\n    params['depthwise_initializer'] = layer.depthwise_initializer.__class__.__name__\n    params['pointwise_initializer'] = layer.pointwise_initializer.__class__.__name__\n    params['bias_initializer'] = layer.bias_initializer.__class__.__name__\n    if (layer.depthwise_regularizer):\n        params['depthwise_regularizer'] = layer.depthwise_regularizer.__class__.__name__\n    if (layer.pointwise_regularizer):\n        params['pointwise_regularizer'] = layer.pointwise_regularizer.__class__.__name__\n    if (layer.bias_regularizer):\n        params['bias_regularizer'] = layer.bias_regularizer.__class__.__name__\n    if (layer.activity_regularizer):\n        params['activity_regularizer'] = layer.activity_regularizer.__class__.__name__\n    if (layer.depthwise_constraint):\n        params['depthwise_constraint'] = layer.depthwise_constraint.__class__.__name__\n    if (layer.pointwise_constraint):\n        params['pointwise_constraint'] = layer.pointwise_constraint.__class__.__name__\n    if (layer.bias_constraint):\n        params['bias_constraint'] = layer.bias_constraint.__class__.__name__\n    return jsonLayer('DepthwiseConv', params, layer)\n\n\ndef Deconvolution(layer):\n    params = {}\n    params['kernel_h'], params['kernel_w'] = layer.kernel_size\n    params['stride_h'], params['stride_w'] = layer.strides\n    params['dilation_h'], params['dilation_w'] = layer.dilation_rate\n    params['pad_h'], params['pad_w'] = get_padding([params['kernel_w'], params['kernel_h'], -1,\n                                                    params['stride_w'], params['stride_h'], -1],\n                                                   layer.input_shape, layer.output_shape,\n                                                   layer.padding.lower(), '2D')\n    params['padding'] = layer.padding.upper()\n    params['weight_filler'] = layer.kernel_initializer.__class__.__name__\n    params['bias_filler'] = layer.bias_initializer.__class__.__name__\n    params['num_output'] = layer.filters\n    if (layer.kernel_regularizer):\n        params['kernel_regularizer'] = layer.kernel_regularizer.__class__.__name__\n    if (layer.bias_regularizer):\n        params['bias_regularizer'] = layer.bias_regularizer.__class__.__name__\n    if (layer.activity_regularizer):\n        params['activity_regularizer'] = layer.activity_regularizer.__class__.__name__\n    if (layer.kernel_constraint):\n        params['kernel_constraint'] = layer.kernel_constraint.__class__.__name__\n    if (layer.bias_constraint):\n        params['bias_constraint'] = layer.bias_constraint.__class__.__name__\n    params['use_bias'] = layer.use_bias\n    return jsonLayer('Deconvolution', params, layer)\n\n\ndef Upsample(layer):\n    params = {}\n    if (layer.__class__.__name__ == 'UpSampling1D'):\n        params['size_w'] = layer.size\n        params['layer_type'] = '1D'\n    elif (layer.__class__.__name__ == 'UpSampling2D'):\n        params['size_w'], params['size_h'] = layer.size\n        params['layer_type'] = '2D'\n    else:\n        params['size_w'], params['size_h'], params['size_d'] = layer.size\n        params['layer_type'] = '3D'\n    return jsonLayer('Upsample', params, layer)\n\n\n# ********** Pooling Layers **********\ndef Pooling(layer):\n    params = {}\n    poolMap = {\n        'MaxPooling1D': 'MAX',\n        'MaxPooling2D': 'MAX',\n        'MaxPooling3D': 'MAX',\n        'AveragePooling1D': 'AVE',\n        'AveragePooling2D': 'AVE',\n        'AveragePooling3D': 'AVE',\n        'GlobalMaxPooling1D': 'MAX',\n        'GlobalMaxPooling2D': 'MAX',\n        'GlobalAveragePooling1D': 'AVE',\n        'GlobalAveragePooling2D': 'AVE'\n    }\n    if (layer.__class__.__name__ in ['GlobalAveragePooling1D', 'GlobalMaxPooling1D']):\n        input_shape = layer.input_shape\n        params['kernel_w'] = params['stride_w'] = input_shape[1]\n        padding = 'valid'\n        params['layer_type'] = '1D'\n        params['pad_w'] = get_padding([params['kernel_w'], -1, -1,\n                                       params['stride_w'], -1, -1],\n                                      layer.input_shape, layer.output_shape,\n                                      padding, '1D')\n    elif (layer.__class__.__name__ in ['GlobalAveragePooling2D', 'GlobalMaxPooling2D']):\n        input_shape = layer.input_shape\n        params['kernel_h'] = params['stride_h'] = input_shape[2]\n        params['kernel_w'] = params['stride_w'] = input_shape[1]\n        padding = 'valid'\n        params['layer_type'] = '2D'\n        params['pad_h'], params['pad_w'] = get_padding([params['kernel_w'], params['kernel_h'], -1,\n                                                        params['stride_w'], params['stride_h'], -1],\n                                                       layer.input_shape, layer.output_shape,\n                                                       padding, '2D')\n    else:\n        padding = layer.padding.lower()\n        if (layer.__class__.__name__ in ['MaxPooling1D', 'AveragePooling1D']):\n            params['kernel_w'] = layer.pool_size[0]\n            params['stride_w'] = layer.strides[0]\n            params['layer_type'] = '1D'\n            params['pad_w'] = get_padding([params['kernel_w'], -1, -1,\n                                           params['stride_w'], -1, -1],\n                                          layer.input_shape, layer.output_shape,\n                                          padding, '1D')\n        elif (layer.__class__.__name__ in ['MaxPooling2D', 'AveragePooling2D']):\n            params['kernel_w'], params['kernel_h'] = layer.pool_size\n            params['stride_w'], params['stride_h'] = layer.strides\n            params['layer_type'] = '2D'\n            params['pad_h'], params['pad_w'] = get_padding([params['kernel_w'], params['kernel_h'], -1,\n                                                            params['stride_w'], params['stride_h'], -1],\n                                                           layer.input_shape, layer.output_shape,\n                                                           padding, '2D')\n        else:\n            params['kernel_h'], params['kernel_w'], params['kernel_d'] = layer.pool_size\n            params['stride_h'], params['stride_w'], params['stride_d'] = layer.strides\n            params['layer_type'] = '3D'\n            params['pad_h'], params['pad_w'], params['pad_d'] = get_padding([params['kernel_w'],\n                                                                             params['kernel_h'],\n                                                                             params['kernel_d'],\n                                                                             params['stride_w'],\n                                                                             params['stride_h'],\n                                                                             params['stride_d']],\n                                                                            layer.input_shape,\n                                                                            layer.output_shape,\n                                                                            padding, '3D')\n    params['pool'] = poolMap[layer.__class__.__name__]\n    return jsonLayer('Pooling', params, layer)\n\n\n# ********** Locally-connected Layers **********\ndef LocallyConnected(layer):\n    params = {}\n    if (layer.__class__.__name__ == 'LocallyConnected1D'):\n        params['layer_type'] = '1D'\n        params['kernel_w'] = layer.kernel_size[0]\n        params['stride_w'] = layer.strides[0]\n    else:\n        params['layer_type'] = '2D'\n        params['kernel_h'], params['kernel_w'] = layer.kernel_size\n        params['stride_h'], params['stride_w'] = layer.strides\n    params['kernel_initializer'] = layer.kernel_initializer.__class__.__name__\n    params['bias_initializer'] = layer.bias_initializer.__class__.__name__\n    params['filters'] = layer.filters\n    if (layer.kernel_regularizer):\n        params['kernel_regularizer'] = layer.kernel_regularizer.__class__.__name__\n    if (layer.bias_regularizer):\n        params['bias_regularizer'] = layer.bias_regularizer.__class__.__name__\n    if (layer.activity_regularizer):\n        params['activity_regularizer'] = layer.activity_regularizer.__class__.__name__\n    if (layer.kernel_constraint):\n        params['kernel_constraint'] = layer.kernel_constraint.__class__.__name__\n    if (layer.bias_constraint):\n        params['bias_constraint'] = layer.bias_constraint.__class__.__name__\n    params['use_bias'] = layer.use_bias\n    return jsonLayer('LocallyConnected', params, layer)\n\n\n# ********** Recurrent Layers **********\ndef Recurrent(layer):\n    recurrentMap = {\n        'SimpleRNN': 'RNN',\n        'GRU': 'GRU',\n        'LSTM': 'LSTM'\n    }\n    params = {}\n    params['num_output'] = layer.units\n    params['weight_filler'] = layer.kernel_initializer.__class__.__name__\n    params['recurrent_initializer'] = layer.recurrent_initializer.__class__.__name__\n    params['bias_filler'] = layer.bias_initializer.__class__.__name__\n    if (layer.kernel_regularizer):\n        params['kernel_regularizer'] = layer.kernel_regularizer.__class__.__name__\n    if (layer.recurrent_regularizer):\n        params['recurrent_regularizer'] = layer.recurrent_regularizer.__class__.__name__\n    if (layer.bias_regularizer):\n        params['bias_regularizer'] = layer.bias_regularizer.__class__.__name__\n    if (layer.activity_regularizer):\n        params['activity_regularizer'] = layer.activity_regularizer.__class__.__name__\n    if (layer.kernel_constraint):\n        params['kernel_constraint'] = layer.kernel_constraint.__class__.__name__\n    if (layer.recurrent_constraint):\n        params['recurrent_constraint'] = layer.recurrent_constraint.__class__.__name__\n    if (layer.bias_constraint):\n        params['bias_constraint'] = layer.bias_constraint.__class__.__name__\n    params['use_bias'] = layer.use_bias\n    params['dropout'] = layer.dropout\n    params['recurrent_dropout'] = layer.recurrent_dropout\n    if (layer.__class__.__name__ == 'GRU'):\n        params['recurrent_activation'] = layer.recurrent_activation.func_name\n    elif (layer.__class__.__name__ == 'LSTM'):\n        params['recurrent_activation'] = layer.recurrent_activation.func_name\n        params['unit_forget_bias'] = layer.unit_forget_bias\n        params['return_sequences'] = layer.return_sequences\n    return jsonLayer(recurrentMap[layer.__class__.__name__], params, layer)\n\n\n# ********** Embedding Layers **********\ndef Embed(layer):\n    params = {}\n    params['input_dim'] = layer.input_dim\n    params['num_output'] = layer.output_dim\n    params['weight_filler'] = layer.embeddings_initializer.__class__.__name__\n    if (layer.embeddings_regularizer):\n        params['embeddings_regularizer'] = layer.embeddings_regularizer.__class__.__name__\n    if (layer.embeddings_constraint):\n        params['embeddings_constraint'] = layer.embeddings_constraint.__class__.__name__\n    if (layer.input_length):\n        params['input_length'] = layer.input_length\n    params['mask_zero'] = layer.mask_zero\n    return jsonLayer('Embed', params, layer)\n\n\n# ********** Merge Layers **********\ndef Concat(layer):\n    params = {}\n    params['axis'] = layer.axis\n    return jsonLayer('Concat', params, layer)\n\n\ndef Eltwise(layer):\n    eltwiseMap = {\n        'Add': 'Sum',\n        'Multiply': 'Product',\n        'Maximum': 'Maximum',\n        'Dot': 'Dot',\n        'Average': 'Average'\n    }\n    params = {'layer_type': eltwiseMap[layer.__class__.__name__]}\n    return jsonLayer('Eltwise', params, layer)\n\n\n# ********** Advanced Activations Layers **********\ndef LeakyReLU(layer):\n    params = {'negative_slope': layer.alpha.tolist()}\n    return jsonLayer('ReLU', params, layer)\n\n\ndef PReLU(layer):\n    return jsonLayer('PReLU', {}, layer)\n\n\ndef ELU(layer):\n    params = {'alpha': layer.alpha.tolist()}\n    return jsonLayer('ELU', params, layer)\n\n\ndef ThresholdedReLU(layer):\n    params = {'theta': layer.theta.tolist()}\n    return jsonLayer('ThresholdedReLU', params, layer)\n\n\n# ********** Normalisation Layers **********\ndef BatchNorm(layer):\n    params = {}\n    params['eps'] = layer.epsilon\n    params['moving_average_fraction'] = layer.momentum\n    params['moving_mean_initializer'] = layer.moving_mean_initializer.__class__.__name__\n    params['moving_variance_initializer'] = layer.moving_variance_initializer.__class__.__name__\n    return jsonLayer('BatchNorm', params, layer)\n\n\n# ********** Noise Layers **********\ndef GaussianNoise(layer):\n    params = {}\n    params['stddev'] = layer.stddev\n    return jsonLayer('GaussianNoise', params, layer)\n\n\ndef GaussianDropout(layer):\n    params = {}\n    params['rate'] = layer.rate\n    return jsonLayer('GaussianDropout', params, layer)\n\n\ndef AlphaDropout(layer):\n    params = {}\n    params['rate'] = layer.rate\n    if (layer.seed):\n        params['seed'] = layer.seed\n    return jsonLayer('AlphaDropout', params, layer)\n\n\n# ********** Utility Layers **********\ndef Scale(layer):\n    tempLayer = {}\n    params = {}\n    params['axis'] = layer.axis\n    params['bias_term'] = layer.center\n    params['scale'] = layer.scale\n    params['filler'] = layer.gamma_initializer.__class__.__name__\n    params['bias_filler'] = layer.beta_initializer.__class__.__name__\n    if (layer.beta_regularizer):\n        params['beta_regularizer'] = layer.beta_regularizer.__class__.__name__\n    if (layer.gamma_regularizer):\n        params['gamma_regularizer'] = layer.gamma_regularizer.__class__.__name__\n    if (layer.beta_constraint):\n        params['beta_constraint'] = layer.beta_constraint.__class__.__name__\n    if (layer.gamma_constraint):\n        params['gamma_constraint'] = layer.gamma_constraint.__class__.__name__\n    tempLayer['inbound_nodes'] = [[[layer.name + layer.__class__.__name__]]]\n    return jsonLayer('Scale', params, tempLayer)\n\n\ndef Padding(layer):\n    pad = np.asarray(layer.padding)\n    if (len(pad.shape) == 1):\n        pad = [pad[0]]\n    else:\n        pad = pad[:, 0].tolist()\n    params = {'pad': pad}\n    return jsonLayer('Pad', params, layer)\n\n\ndef TimeDistributed(layer):\n    return jsonLayer('TimeDistributed', {}, layer)\n\n\ndef Bidirectional(layer):\n    params = {}\n    params['merge_mode'] = layer.merge_mode\n    return jsonLayer('Bidirectional', params, layer)\n\n\ndef lrn(layer):\n    params = {}\n    params['k'] = layer.k\n    params['beta'] = layer.beta\n    params['alpha'] = layer.alpha\n    params['local_size'] = layer.n\n    return jsonLayer('LRN', params, layer)\n\n\n# ********** Helper functions **********\n\n# padding logic following\n# https://github.com/Yangqing/caffe2/blob/master/caffe2/proto/caffe2_legacy.proto\ndef get_padding(params, input_shape, output_shape, pad_type, type):\n    k_w, k_h, k_d, s_w, s_h, s_d = params\n    if (type == '1D'):\n        if (pad_type == 'valid'):\n            return 0\n        else:\n            pad_w = ((output_shape[1] - 1) * s_w + k_w - input_shape[1]) / 2\n            return pad_w\n    elif (type == '2D'):\n        if (pad_type == 'valid'):\n            return [0, 0]\n        else:\n            pad_h = ((output_shape[2] - 1) * s_h + k_h - input_shape[2]) / 2\n            pad_w = ((output_shape[1] - 1) * s_w + k_w - input_shape[1]) / 2\n            return (pad_h, pad_w)\n    else:\n        if (pad_type == 'valid'):\n            return [0, 0, 0]\n        else:\n            pad_h = ((output_shape[2] - 1) * s_h + k_h - input_shape[2]) / 2\n            pad_w = ((output_shape[1] - 1) * s_w + k_w - input_shape[1]) / 2\n            pad_d = ((output_shape[3] - 1) * s_d + k_d - input_shape[3]) / 2\n            return (pad_h, pad_w, pad_d)\n\n\ndef jsonLayer(type, params, layer):\n    input = []\n    if hasattr(layer, 'wrapped'):\n        input.append(layer.wrapper[0])\n    else:\n        if isinstance(layer, dict):\n            for node in layer['inbound_nodes'][0]:\n                input.append(node[0])\n        elif (len(layer.inbound_nodes[0].inbound_layers)):\n            for node in layer.inbound_nodes[0].inbound_layers:\n                input.append(node.name)\n    layer = {\n        'info': {\n            'type': type,\n            'phase': None\n        },\n        'connection': {\n            'input': input,\n            'output': []\n        },\n        'params': params\n    }\n    return layer\n"""
tensorflow_app/views/__init__.py,0,b''
tensorflow_app/views/export_graphdef.py,0,"b""import os\nimport string\nimport random\nfrom django.views.decorators.csrf import csrf_exempt\nfrom django.http import JsonResponse\nfrom keras_app.views.export_json import export_json\n\nBASE_DIR = os.path.dirname(\n    os.path.dirname(\n        os.path.dirname(\n            os.path.abspath(__file__))))\n\n\ndef randomword(length):\n    return ''.join(random.choice(string.lowercase) for i in range(length))\n\n\n@csrf_exempt\ndef export_to_tensorflow(request):\n    # Note : Remove the views for export by adding unittest for celery tasks\n    response = export_json(request, is_tf=True)\n    if isinstance(response, JsonResponse):\n        return response\n    randomId = response['randomId']\n    customLayers = response['customLayers']\n    os.chdir(BASE_DIR + '/tensorflow_app/views/')\n    os.system('KERAS_BACKEND=tensorflow python json2pbtxt.py -input_file ' +\n              randomId + '.json -output_file ' + randomId)\n    return JsonResponse({'result': 'success',\n                         'id': randomId,\n                         'name': randomId + '.pbtxt',\n                         'url': '/media/' + randomId + '.pbtxt',\n                         'customLayers': customLayers})\n"""
tensorflow_app/views/import_graphdef.py,4,"b""import tensorflow as tf\nfrom google.protobuf import text_format\nfrom tensorflow.core.framework import graph_pb2\nfrom django.views.decorators.csrf import csrf_exempt\nfrom django.http import JsonResponse\nimport urllib2\nfrom urlparse import urlparse\n\nfrom layers_import import import_placeholder, import_conv2d, import_conv3d, import_deconvolution, \\\n                        import_depthwise_convolution, import_pooling2d, import_pooling3d, \\\n                        import_inner_product, import_batchnorm, import_eltwise, import_activation, \\\n                        import_dropout, import_flatten, import_concat, import_lrn\n\nfrom layers_import import get_layer_name, get_layer_type, jsonLayer, activation_layers\n\nlayer_map = {\n    'Placeholder': import_placeholder,\n    'Conv2D': import_conv2d,\n    'Conv3D': import_conv3d,\n    'MaxPool': import_pooling2d,\n    'MaxPool3D': import_pooling3d,\n    'AvgPool3D': import_pooling3d,\n    'DepthwiseConv2dNative': import_depthwise_convolution,\n    'FusedBatchNorm': import_batchnorm,\n    'Conv2DBackpropInput': import_deconvolution,\n    'LRN': import_lrn,\n    'MatMul': import_inner_product,\n    'Prod': import_inner_product,\n    'Concat': import_concat,\n    'AvgPool': import_pooling2d,\n    'Reshape': import_flatten\n}\n\nname_map = {\n    'flatten': import_flatten,\n    'dropout': import_dropout,\n    'lrn': import_lrn,\n    'concatenate': import_concat,\n    'batch': import_batchnorm,\n    'BatchNorm': import_batchnorm,\n    'add': import_eltwise,\n    'mul': import_eltwise\n}\n\n\ndef get_all_ops_in_layer(layer_name, all_ops):\n    ops_from_same_layer = []\n    for op in all_ops:\n        if get_layer_name(op.name) == layer_name:\n            ops_from_same_layer.append(op)\n    return ops_from_same_layer\n\n\n@csrf_exempt\ndef import_graph_def(request):\n    if request.method == 'POST':\n        if ('file' in request.FILES) and \\\n           (request.FILES['file'].content_type == 'application/octet-stream' or\n                request.FILES['file'].content_type == 'text/plain'):\n            try:\n                f = request.FILES['file']\n                config = f.read()\n                f.close()\n            except Exception:\n                return JsonResponse({'result': 'error', 'error': 'No GraphDef model file found'})\n        elif 'config' in request.POST:\n            config = request.POST['config']\n        elif 'url' in request.POST:\n            try:\n                url = urlparse(request.POST['url'])\n                if url.netloc == 'github.com':\n                    url = url._replace(netloc='raw.githubusercontent.com')\n                    url = url._replace(path=url.path.replace('blob/', ''))\n                config = urllib2.urlopen(url.geturl()).read()\n            except Exception as ex:\n                return JsonResponse({'result': 'error', 'error': 'Invalid URL\\n'+str(ex)})\n        else:\n            return JsonResponse({'result': 'error', 'error': 'No GraphDef model found'})\n\n        tf.reset_default_graph()\n        graph_def = graph_pb2.GraphDef()\n\n        try:\n            text_format.Merge(config, graph_def)\n        except Exception:\n            return JsonResponse({'result': 'error', 'error': 'Invalid GraphDef'})\n\n        tf.import_graph_def(graph_def, name='')\n        graph = tf.get_default_graph()\n        session = tf.Session(graph=graph)\n        all_ops = graph.get_operations()\n\n        net = {}\n        processed_layers = []\n        layers_with_inplace_relu = {}\n\n        for node in all_ops:\n\n            layer_name = get_layer_name(node.name)\n            layer_type = get_layer_type(node.name)\n\n            if layer_name in processed_layers:\n                continue\n\n            if node.type == 'NoOp':\n                init_op = session.graph.get_operation_by_name(node.name)\n                session.run(init_op)\n                continue\n\n            all_ops_in_layer = get_all_ops_in_layer(layer_name, all_ops)\n            for op in all_ops_in_layer:\n                if op.type == 'FusedBatchNorm':\n                    net[layer_name] = import_batchnorm(all_ops_in_layer)\n                    processed_layers.append(layer_name)\n\n            if node.type in layer_map:\n                for i, op in enumerate(all_ops_in_layer):\n                    # if the layer has an inplace relu operation, separate the relu op\n                    # this prevents net[layer_name] from being overwritten by an inplace\n                    # relu layer when the layer might actually contain another important\n                    # layer like a dense layer for example\n                    if op.type == 'Relu':\n                        del(all_ops_in_layer[i])\n                        relu_layer = jsonLayer('ReLU', {}, [layer_name])\n                        relu_layer_name = layer_name + '_relu'\n                        net[relu_layer_name] = relu_layer\n                        layers_with_inplace_relu[layer_name] = relu_layer_name\n                json_layer = layer_map[node.type](all_ops_in_layer)\n                net[layer_name] = json_layer\n                processed_layers.append(layer_name)\n\n            elif node.type in activation_layers:\n                json_layer = import_activation(all_ops_in_layer)\n                net[layer_name] = json_layer\n                processed_layers.append(layer_name)\n\n            elif layer_type in name_map:\n                json_layer = name_map[layer_type](all_ops_in_layer)\n                net[layer_name] = json_layer\n                processed_layers.append(layer_name)\n\n        # connect layers with the previous layer's inplace relu ops, if any\n        for layer_name in net:\n            for i, input_layer in enumerate(net[layer_name]['connection']['input']):\n                if (input_layer in layers_with_inplace_relu.keys()) and \\\n                        layers_with_inplace_relu[input_layer] != layer_name:\n                    net[layer_name]['connection']['input'][i] = layers_with_inplace_relu[input_layer]\n\n        # fill in outputs of every layer in net using inputs of consumer layers\n        outputs = {}\n        for layer_name in net.keys():\n            for input_layer_name in net[layer_name]['connection']['input']:\n                if input_layer_name not in outputs:\n                    outputs[input_layer_name] = []\n                if layer_name not in outputs[input_layer_name]:\n                    outputs[input_layer_name].append(layer_name)\n        for layer in outputs:\n            net[layer]['connection']['output'] = outputs[layer]\n\n        # add a scale layer next to batch normalization layers\n        scale_layers = {}\n        for layer_name in net:\n            if net[layer_name]['info']['type'] == 'BatchNorm':\n                batch_norm_outputs = net[layer_name]['connection']['output'][:]\n                scale_layer_name = layer_name + '_scale'\n                scale_layer = jsonLayer('Scale', {}, [layer_name], batch_norm_outputs)\n                net[layer_name]['connection']['output'] = [scale_layer_name]\n                scale_layers[scale_layer_name] = scale_layer\n        for scale_layer_name in scale_layers:\n            net[scale_layer_name] = scale_layers[scale_layer_name]\n\n        session.close()\n\n        return JsonResponse({'result': 'success', 'net': net, 'net_name': ''})\n"""
tensorflow_app/views/json2pbtxt.py,1,"b'from keras.models import model_from_json\nimport tensorflow as tf\nfrom keras import backend as K\nimport argparse\nimport os\nimport imp\n\nparser = argparse.ArgumentParser(description=\'set input arguments\')\nparser.add_argument(\'-input_file\', action=""store"",\n                    dest=\'input_file\', type=str, default=\'model.json\')\nparser.add_argument(\'-output_file\', action=""store"",\n                    dest=\'output_file\', type=str, default=\'model.pbtxt\')\nargs = parser.parse_args()\ninput_file = args.input_file\noutput_file = args.output_file\n\nK.set_learning_phase(0)\n\nBASE_DIR = os.path.dirname(\n    os.path.dirname(\n        os.path.dirname(\n            os.path.abspath(__file__))))\n\noutput_fld = BASE_DIR + \'/media/\'\n\nwith open(output_fld + input_file, \'r\') as f:\n    json_str = f.read()\n\njson_str = json_str.strip(""\'<>() "").replace(\'\\\'\', \'\\""\')\nlrn = imp.load_source(\'LRN\', BASE_DIR + \'/keras_app/custom_layers/lrn.py\')\nmodel = model_from_json(json_str, {\'LRN\': lrn.LRN})\n\nsess = K.get_session()\ntf.train.write_graph(sess.graph.as_graph_def(add_shapes=True), output_fld,\n                     output_file + \'.pbtxt\', as_text=True)\n'"
tensorflow_app/views/layers_import.py,0,"b'import math\nimport re\n\n\ninitializer_map = {\'random_uniform\': \'RandomUniform\', \'random_normal\': \'RandomNormal\',\n                   \'Const\': \'Constant\', \'zeros\': \'Zeros\', \'ones\': \'Ones\',\n                   \'eye\': \'Identity\', \'truncated_normal\': \'TruncatedNormal\'}\n\nactivation_layers = [\n    \'Sigmoid\',\n    \'Softplus\',\n    \'Softsign\',\n    \'Elu\',\n    \'LeakyRelu\',\n    \'Softmax\',\n    \'Relu\',\n    \'Tanh\',\n    \'SELU\'\n]\n\n\ndef get_layer_name(node_name):\n    i = node_name.find(\'/\')\n    if i == -1:\n        name = str(node_name)\n    elif str(node_name[:i]) in [\'Repeat\', \'Stack\']:\n        name = str(node_name.split(\'/\')[1])\n    else:\n        name = str(node_name[:i])\n    return name\n\n\ndef get_layer_type(node_name):\n    return node_name.split(\'_\')[0]\n\n\ndef get_padding(node, kernel_shape, strides):\n    if node.type in [""Conv3D"", ""MaxPool3D"", ""AvgPool3D""]:\n        input_tensor = node.inputs[0]\n        output_tensor = node.outputs[0]\n        input_shape = [1 if i.value is None else int(i) for i in input_tensor.shape]\n        output_shape = [1 if i.value is None else int(i) for i in output_tensor.shape]\n\n        kernel_d = kernel_shape[0]\n        kernel_h = kernel_shape[1]\n        kernel_w = kernel_shape[2]\n        stride_d = strides[1]\n        stride_h = strides[2]\n        stride_w = strides[3]\n\n        pad_d = ((int(output_shape[1]) - 1) * stride_d +\n                 kernel_d - int(input_shape[1])) / float(2)\n        pad_h = ((int(output_shape[2]) - 1) * stride_h +\n                 kernel_h - int(input_shape[2])) / float(2)\n        pad_w = ((int(output_shape[3]) - 1) * stride_w +\n                 kernel_w - int(input_shape[3])) / float(2)\n\n        if node.type == ""Conv3D"":\n            pad_d = math.ceil(pad_d)\n            pad_h = math.ceil(pad_h)\n            pad_w = math.ceil(pad_w)\n        elif node.type in [""MaxPool3D"", ""AvgPool3D""]:\n            pad_d = math.floor(pad_d)\n            pad_h = math.floor(pad_h)\n            pad_w = math.floor(pad_w)\n\n        return int(pad_d), int(pad_h), int(pad_w)\n\n    elif node.type == ""Conv2DBackpropInput"":\n        input_tensor = node.inputs[2]\n        output_tensor = node.outputs[0]\n        input_shape = [1 if i.value is None else int(i) for i in input_tensor.shape]\n        output_shape = [1 if i.value is None else int(i) for i in output_tensor.shape]\n\n        # if deconvolution layer padding calculation logic changes\n        if (\'padding\' in node.node_def.attr):\n            kernel_h = kernel_shape[0]\n            kernel_w = kernel_shape[1]\n            stride_h = strides[1]\n            stride_w = strides[2]\n            pad_h = ((int(input_shape[1]) - 1) * stride_h +\n                     kernel_h - int(output_shape[1])) / float(2)\n            pad_w = ((int(input_shape[2]) - 1) * stride_w +\n                     kernel_w - int(output_shape[2])) / float(2)\n\n            return int(math.floor(pad_h)), int(math.floor(pad_w))\n\n    else:\n        input_tensor = node.inputs[0]\n        output_tensor = node.outputs[0]\n        input_shape = [1 if i.value is None else int(i) for i in input_tensor.shape]\n        output_shape = [1 if i.value is None else int(i) for i in output_tensor.shape]\n        kernel_h = kernel_shape[0]\n        kernel_w = kernel_shape[1]\n        stride_h = strides[1]\n        stride_w = strides[2]\n\n        pad_h = ((int(output_shape[1]) - 1) * stride_h +\n                 kernel_h - int(input_shape[1])) / float(2)\n        pad_w = ((int(output_shape[2]) - 1) * stride_w +\n                 kernel_w - int(input_shape[2])) / float(2)\n\n        # check this logic (see caffe-tensorflow/caffe/shapes.py)\n        if node.type == ""Conv2D"":\n            pad_h = math.ceil(pad_h)\n            pad_w = math.ceil(pad_w)\n        elif node.type in [""MaxPool"", ""AvgPool""]:\n            pad_h = math.floor(pad_h)\n            pad_w = math.floor(pad_w)\n\n        return int(pad_h), int(pad_w)\n\n\ndef get_initializer_type(layer_ops):\n    """"""Returns a dict mapping variables (weight, bias etc) to initializer types.\n    The returned dict maybe empty if no initializers are found.\n    """"""\n    weight_name_patterns = [r\'.*/weight/*\', r\'.*/kernel/*\']\n    bias_name_patterns = [r\'.*/bias/*\']\n    pointwise_weight_name_patterns = [r\'.*/pointwise_weights/*\']\n    depthwise_weight_name_patterns = [r\'.*/depthwise_weights/*\']\n\n    initializers = {}\n    for op in layer_ops:\n        # extracting weights initializer\n        for weight_name_pattern in weight_name_patterns:\n            if re.match(weight_name_pattern, str(op.name)) and op.type in initializer_map.keys():\n                initializers[\'weight\'] = initializer_map[op.type]\n        # extracting bias initializer\n        for bias_name_pattern in bias_name_patterns:\n            if re.match(bias_name_pattern, str(op.name)) and op.type in initializer_map.keys():\n                initializers[\'bias\'] = initializer_map[op.type]\n        # extracting pointwise wei\n        for pointwise_weight_name_pattern in pointwise_weight_name_patterns:\n            if re.match(pointwise_weight_name_pattern, str(op.name)) and op.type in initializer_map.keys():\n                initializers[\'pointwise_weight\'] = initializer_map[op.type]\n        for depthwise_weight_name_pattern in depthwise_weight_name_patterns:\n            if re.match(depthwise_weight_name_pattern, str(op.name)) and op.type in initializer_map.keys():\n                initializers[\'depthwise_weight\'] = initializer_map[op.type]\n\n    return initializers\n\n\ndef get_input_layers(layer_ops):\n    \'\'\'\n    return the name of the layers directly preceeding the layer of layer_ops.\n    layer_ops is a list of all ops of the layer we want the inputs of.\n    \'\'\'\n    input_layer_names = []\n    name = get_layer_name(layer_ops[0].name)\n    for node in layer_ops:\n        for input_tensor in node.inputs:\n            input_layer_name = get_layer_name(input_tensor.op.name)\n            if input_layer_name != name:\n                input_layer_names.append(input_layer_name)\n    return input_layer_names\n\n\ndef import_activation(layer_ops):\n    layer_type = \'\'\n    layer_params = {}\n\n    activation_op = next((x for x in layer_ops if x.type in activation_layers), None)\n\n    if activation_op.type == \'Relu\':\n        layer_type = \'ReLU\'\n\n    elif activation_op.type == \'LeakyRelu\':\n        if \'alpha\' in activation_op.node_def.attr:\n            layer_params[\'negative_slope\'] = activation_op.get_attr(\'alpha\')\n        layer_type = \'ReLU\'\n\n    elif activation_op.type == \'Elu\':\n        layer_params[\'alpha\'] = 1\n        layer_type = \'ELU\'\n\n    elif activation_op.type == \'Tanh\':\n        layer_type = \'TanH\'\n\n    else:\n        # rest of the activations have the same name in TF and Fabrik\n        layer_type = activation_op.type\n\n    return jsonLayer(layer_type, layer_params, get_input_layers(layer_ops), [])\n\n\ndef import_placeholder(layer_ops):\n    placeholder_op = layer_ops[0]\n    layer_params = {}\n    layer_dim = [int(dim.size) for dim in placeholder_op.get_attr(\'shape\').dim]\n\n    # make batch size 1 if it is -1\n    if layer_dim[0] == 0:\n        layer_dim[0] = 1\n\n    # change tensor format from tensorflow default (NHWC/NDHWC)\n    # to (NCHW/NCDHW)\n    temp = layer_dim[1]\n    layer_dim[1] = layer_dim[-1]\n    layer_dim[-1] = temp\n    layer_params[\'dim\'] = str(layer_dim)[1:-1]\n\n    return jsonLayer(\'Input\', layer_params, get_input_layers(layer_ops), [])\n\n\ndef import_conv2d(layer_ops):\n    conv2d_op = next((x for x in layer_ops if x.type == \'Conv2D\'), None)\n    layer_params = {}\n    layer_params[\'layer_type\'] = \'2D\'\n\n    strides = [int(i) for i in conv2d_op.get_attr(\'strides\')]\n    kernel_shape = [int(i) for i in conv2d_op.inputs[1].shape]\n    layer_params[\'stride_h\'] = strides[1]\n    layer_params[\'stride_w\'] = strides[2]\n    layer_params[\'kernel_h\'] = kernel_shape[0]\n    layer_params[\'kernel_w\'] = kernel_shape[1]\n    layer_params[\'num_output\'] = kernel_shape[3]\n    layer_params[\'pad_h\'], layer_params[\'pad_w\'] = get_padding(conv2d_op, kernel_shape, strides)\n\n    initializers = get_initializer_type(layer_ops)\n    try:\n        layer_params[\'weight_filler\'] = initializers[\'kernel\']\n        layer_params[\'bias_filler\'] = initializers[\'bias\']\n    except KeyError:\n        # no initializers found, continue\n        pass\n\n    return jsonLayer(\'Convolution\', layer_params, get_input_layers(layer_ops), [])\n\n\ndef import_conv3d(layer_ops):\n    conv3d_op = next((x for x in layer_ops if x.type == \'Conv3D\'), None)\n    layer_params = {}\n    layer_params[\'layer_type\'] = \'3D\'\n\n    kernel_shape = [int(i) for i in conv3d_op.inputs[1].shape]\n    layer_params[\'kernel_d\'] = kernel_shape[0]\n    layer_params[\'kernel_h\'] = kernel_shape[1]\n    layer_params[\'kernel_w\'] = kernel_shape[2]\n    layer_params[\'num_output\'] = kernel_shape[4]\n\n    strides = [int(i) for i in conv3d_op.get_attr(\'strides\')]\n    layer_params[\'stride_d\'] = strides[1]\n    layer_params[\'stride_h\'] = strides[2]\n    layer_params[\'stride_w\'] = strides[3]\n\n    pad_d, pad_h, pad_w = get_padding(conv3d_op, kernel_shape, strides)\n    layer_params[\'pad_d\'] = pad_d\n    layer_params[\'pad_h\'] = pad_h\n    layer_params[\'pad_w\'] = pad_w\n\n    initializers = get_initializer_type(layer_ops)\n    try:\n        layer_params[\'weight_filler\'] = initializers[\'kernel\']\n        layer_params[\'bias_filler\'] = initializers[\'bias\']\n    except KeyError:\n        # no initializers found, continue\n        pass\n\n    return jsonLayer(\'Convolution\', layer_params, get_input_layers(layer_ops), [])\n\n\ndef import_deconvolution(layer_ops):\n    deconv_op = next((x for x in layer_ops if x.type == \'Conv2DBackpropInput\'), None)\n    layer_params = {}\n    layer_params[\'layer_type\'] = \'2D\'\n\n    kernel_shape = [int(i) for i in deconv_op.inputs[1].shape]\n    strides = [int(i) for i in deconv_op.get_attr(\'strides\')]\n    layer_params[\'padding\'] = deconv_op.get_attr(\'padding\')\n    layer_params[\'kernel_h\'] = kernel_shape[0]\n    layer_params[\'kernel_w\'] = kernel_shape[1]\n    layer_params[\'num_output\'] = kernel_shape[3]\n    layer_params[\'pad_h\'], layer_params[\'pad_w\'] = get_padding(deconv_op, kernel_shape, strides)\n\n    initializers = get_initializer_type(layer_ops)\n    try:\n        layer_params[\'weight_filler\'] = initializers[\'kernel\']\n        layer_params[\'bias_filler\'] = initializers[\'bias\']\n    except KeyError:\n        # no initializers found, continue\n        pass\n\n    return jsonLayer(\'Deconvolution\', layer_params, get_input_layers(layer_ops), [])\n\n\ndef import_depthwise_convolution(layer_ops):\n    depthwise_conv_op = next((x for x in layer_ops if x.type == \'DepthwiseConv2dNative\'), None)\n    layer_params = {}\n    if \'3D\' in depthwise_conv_op.type:\n        raise ValueError(\'3D depthwise convolution cannot be imported.\')\n\n    kernel_shape = [int(i) for i in depthwise_conv_op.inputs[1].shape]\n    layer_params[\'kernel_h\'] = kernel_shape[0]\n    layer_params[\'kernel_w\'] = kernel_shape[1]\n    layer_params[\'num_output\'] = kernel_shape[2]\n    layer_params[\'depth_multiplier\'] = kernel_shape[3]\n\n    if \'padding\' in depthwise_conv_op.node_def.attr:\n        layer_params[\'padding\'] = str(depthwise_conv_op.get_attr(\'padding\'))\n    strides = [int(i) for i in depthwise_conv_op.get_attr(\'strides\')]\n    layer_params[\'stride_h\'] = strides[1]\n    layer_params[\'stride_w\'] = strides[2]\n    layer_params[\'pad_h\'], layer_params[\'pad_w\'] = get_padding(depthwise_conv_op, kernel_shape, strides)\n\n    initializers = get_initializer_type(layer_ops)\n    try:\n        layer_params[\'pointwise_weight\'] = initializers[\'pointwise_initializer\']\n        layer_params[\'depthwise_weight\'] = initializers[\'depthwise_initializer\']\n    except KeyError:\n        # no initializers found, continue\n        pass\n\n    return jsonLayer(\'DepthwiseConv\', layer_params, get_input_layers(layer_ops), [])\n\n\ndef import_pooling2d(layer_ops):\n    pooling2d_op = next((x for x in layer_ops if x.type in [\'MaxPool\', \'AvgPool\']))\n    layer_params = {}\n    layer_params[\'layer_type\'] = \'2D\'\n\n    # checking type of pooling layer\n    if pooling2d_op.type == \'MaxPool\':\n        layer_params[\'pool\'] = \'MAX\'\n    elif pooling2d_op.type == \'AvgPool\':\n        layer_params[\'pool\'] = \'AVE\'\n\n    kernel_shape = [int(i) for i in pooling2d_op.get_attr(\'ksize\')]\n    strides = [int(i) for i in pooling2d_op.get_attr(\'strides\')]\n    layer_params[\'kernel_h\'] = kernel_shape[1]\n    layer_params[\'kernel_w\'] = kernel_shape[2]\n    layer_params[\'stride_h\'] = strides[1]\n    layer_params[\'stride_w\'] = strides[2]\n    layer_params[\'padding\'] = str(pooling2d_op.get_attr(\'padding\'))\n    layer_params[\'pad_h\'], layer_params[\'pad_w\'] = get_padding(pooling2d_op, kernel_shape, strides)\n\n    return jsonLayer(\'Pooling\', layer_params, get_input_layers(layer_ops), [])\n\n\ndef import_pooling3d(layer_ops):\n    pooling3d_op = next((x for x in layer_ops if x.type in [\'MaxPool3D\', \'AvgPool3D\']))\n    layer_params = {}\n    layer_params[\'layer_type\'] = \'3D\'\n    layer_params[\'padding\'] = str(pooling3d_op.get_attr(\'padding\'))\n\n    # checking type of pooling layer\n    if pooling3d_op.type == \'MaxPool\':\n        layer_params[\'pool\'] = \'MAX\'\n    elif pooling3d_op.type == \'AvgPool\':\n        layer_params[\'pool\'] = \'AVE\'\n\n    kernel_shape = [int(i) for i in pooling3d_op.get_attr(\'ksize\')]\n    strides = [int(i) for i in pooling3d_op.get_attr(\'strides\')]\n    layer_params[\'kernel_d\'] = kernel_shape[1]\n    layer_params[\'kernel_h\'] = kernel_shape[2]\n    layer_params[\'kernel_w\'] = kernel_shape[3]\n    layer_params[\'stride_d\'] = strides[1]\n    layer_params[\'stride_h\'] = strides[2]\n    layer_params[\'stride_w\'] = strides[3]\n\n    pad_d, pad_h, pad_w = get_padding(pooling3d_op, kernel_shape, strides)\n    layer_params[\'pad_d\'] = pad_d\n    layer_params[\'pad_h\'] = pad_h\n    layer_params[\'pad_w\'] = pad_w\n\n    return jsonLayer(\'Pooling\', layer_params, get_input_layers(layer_ops), [])\n\n\ndef import_inner_product(layer_ops):\n    inner_product_op = next((x for x in layer_ops if x.type in [\'Prod\', \'MatMul\']))\n    layer_params = {}\n    if inner_product_op.type == \'MatMul\':\n        layer_params[\'num_output\'] = int(inner_product_op.inputs[1].shape[1])\n\n    return jsonLayer(\'InnerProduct\', layer_params, get_input_layers(layer_ops), [])\n\n\ndef import_batchnorm(layer_ops):\n    layer_params = {}\n    name = get_layer_name(layer_ops[0].name)\n\n    for node in layer_ops:\n        if re.match(\'.*\\/batchnorm[_]?[0-9]?\\/add.*\', str(node.name)):\n            try:\n                layer_params[\'eps\'] = node.get_attr(\'value\').float_val[0]\n            except:\n                pass\n        if (node.type == \'FusedBatchNorm\'):\n            layer_params[\'eps\'] = float(node.get_attr(\'epsilon\'))\n        # searching for moving_mean/Initializer ops to extract moving\n        # mean initializer of batchnorm layer\n        if name + \'/moving_mean/Initializer\' in str(node.name):\n            layer_params[\'moving_mean_initializer\'] = \\\n                initializer_map[str(node.name).split(\'/\')[3]]\n        # searching for AssignMovingAvg/decay ops to extract moving\n        # average fraction of batchnorm layer also considering repeat & stack layer\n        # as prefixes\n        if str(node.name) in [name + \'/AssignMovingAvg/decay\',\n                              \'Repeat/\' + name + \'/AssignMovingAvg/decay\',\n                              \'Stack/\' + name + \'/AssignMovingAvg/decay\']:\n            layer_params[\'moving_average_fraction\'] = node.get_attr(\n                \'value\').float_val[0]\n\n    return jsonLayer(\'BatchNorm\', layer_params, get_input_layers(layer_ops), [])\n\n\ndef import_eltwise(layer_ops):\n    eltwise_op = next((x for x in layer_ops if x.type in [\'add\', \'mul\', \'dot\']))\n    layer_params = {}\n    if eltwise_op.type == \'add\':\n        layer_params[\'layer_type\'] = \'Sum\'\n    if eltwise_op.type == \'mul\':\n        layer_params[\'layer_type\'] = \'Product\'\n    if eltwise_op.type == \'dot\':\n        layer_params[\'layer_type\'] = \'Dot\'\n\n    return jsonLayer(\'Eltwise\', layer_params, get_input_layers(layer_ops), [])\n\n\ndef import_dropout(layer_ops):\n    layer_params = {}\n    for node in layer_ops:\n        if (\'rate\' in node.node_def.attr):\n            layer_params[\'rate\'] = node.get_attr(\'rate\')\n        if (\'seed\' in node.node_def.attr):\n            layer_params[\'seed\'] = node.get_attr(\'seed\')\n        if (\'training\' in node.node_def.attr):\n            layer_params[\'trainable\'] = node.get_attr(\'training\')\n\n    return jsonLayer(\'Dropout\', layer_params, get_input_layers(layer_ops), [])\n\n\ndef import_flatten(layer_ops):\n    return jsonLayer(\'Flatten\', [], get_input_layers(layer_ops), [])\n\n\ndef import_concat(layer_ops):\n    layer_params = {}\n    for node in layer_ops:\n        if \'axis\' in node.node_def.attr:\n            layer_params[\'axis\'] = node.get_attr(\'axis\')\n\n    return jsonLayer(\'Concat\', layer_params, get_input_layers(layer_ops), [])\n\n\ndef import_lrn(layer_ops):\n    layer_params = {}\n    for node in layer_ops:\n        if (\'alpha\' in node.node_def.attr):\n            layer_params[\'alpha\'] = node.get_attr(\'alpha\')\n        if (\'beta\' in node.node_def.attr):\n            layer_params[\'beta\'] = node.get_attr(\'beta\')\n        if (\'local_size\' in node.node_def.attr):\n            layer_params[\'local_size\'] = node.get_attr(\'depth_radius\')\n        if (\'bias\' in node.node_def.attr):\n            layer_params[\'k\'] = node.get_attr(\'bias\')\n\n    return jsonLayer(\'LRN\', layer_params, get_input_layers(layer_ops), [])\n\n\ndef jsonLayer(layer_type, layer_params={}, inputs=[], outputs=[]):\n    layer = {\n        \'info\': {\n            \'type\': layer_type,\n            \'phase\': None\n        },\n        \'connection\': {\n            \'input\': inputs,\n            \'output\': outputs\n        },\n        \'params\': layer_params\n    }\n    return layer\n'"
tests/unit/__init__.py,0,b''
example/caffe/code_samples/caffe_sample.py,0,"b'import subprocess\nimport sys\n\n\n# Get the command line arguments\nmodel_file = \'\'\ntry:\n    model_file = sys.argv[1]\nexcept IndexError:\n    print(\'Usage: python caffe_sample.py PATH_TO_MODEL\')\n    exit()\n\nsolver = [\n    \'net: ""{}""\'.format(model_file),\n    \'test_iter: 200\',\n    \'test_interval: 500\',\n    \'base_lr: 1e-5\',\n    \'lr_policy: ""step""\',\n    \'gamma: 0.1\',\n    \'stepsize: 5000\',\n    \'display: 20\',\n    \'max_iter: 450000\',\n    \'momentum: 0.9\',\n    \'weight_decay: 0.0005\',\n    \'snapshot: 2000\',\n    \'snapshot_prefix: ""model/caffe_sample""\',\n    \'solver_mode: GPU\',\n]\n\n# Create solver.prototxt\nwith open(\'solver.prototxt\', \'w\') as file:\n    for line in solver:\n        file.write(line + \'\\n\')\n\n# Train the model\nsubprocess.call([\'caffe\', \'train\', \'-gpu\', \'0\', \'-solver\', \'solver.prototxt\'])\n'"
example/keras/code_samples/keras_sample_cifar10.py,0,"b'from keras.datasets import cifar10\nfrom keras.models import model_from_json\nimport sys\n\n# Get the command line arguments\nmodel_file_name = \'\'\ntry:\n    model_file_name = sys.argv[1]\nexcept IndexError:\n    print(\'Usage: python train.py model_json_file\')\n    exit()\n\n# Load the dataset (keras.datasets.cifar10)\n# To use other datasets from keras.datasets, replace cifar10 in line 1 with your preferred dataset.\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\n# Load the model from JSON file\njson_file = open(model_file_name, \'r\')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n\n# Print the model summary\nloaded_model.summary()\n\n# Configure model for training and testing with accuracy evaluation\nloaded_model.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\'])\n\n# Train the model\nloaded_model.fit(x_train, y_train, epochs=150, batch_size=10, verbose=0)\n\n# Evaluate the model\nscores = loaded_model.evaluate(x_test, y_test, verbose=0)\n\n# Print final accuracy\nprint(""%s: %.2f%%"" % (loaded_model.metrics_names[1], scores[1] * 100))\n'"
example/tensorflow/code_template/tensorflow_sample.py,3,"b""import tensorflow as tf\nfrom google.protobuf import text_format\nimport sys\n\n# Get the model file name\ntry:\n    model_file_name = sys.argv[1]\nexcept IndexError:\n    print('Usage: python tensorflow_sample.py <MODEL_FILE.pbtxt>')\n\n# Read the protobuf text and build a tf.GraphDef\nwith open(model_file_name, 'r') as model_file:\n    model_protobuf = text_format.Parse(model_file.read(),\n                                       tf.GraphDef())\n\n# Import the GraphDef built above into the default graph\ntf.import_graph_def(model_protobuf)\n\n# You can now add operations on top of the imported graph\n"""
tests/unit/caffe_app/__init__.py,0,b''
tests/unit/caffe_app/test_db.py,0,"b'import json\nimport os\nimport unittest\nfrom django.conf import settings\nfrom django.core.urlresolvers import reverse\nfrom django.test import Client\nfrom django.contrib.auth.models import User\nfrom caffe_app.models import Network, NetworkVersion\n\n\nclass SaveToDBTest(unittest.TestCase):\n\n    def setUp(self):\n        self.client = Client()\n\n    def test_save_json(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        net = json.load(tests)[\'net\']\n        response = self.client.post(\n            reverse(\'saveDB\'),\n            {\'net\': net, \'net_name\': \'netname\'})\n        response = json.loads(response.content)\n        self.assertEqual(response[\'result\'], \'success\')\n\n    def test_load(self):\n        u_1 = User(id=1, username=\'user_1\')\n        u_1.save()\n        u_2 = User(id=2, username=\'user_2\')\n        u_2.save()\n        model = Network(name=\'net\')\n        model.save()\n        model_version = NetworkVersion(network=model, network_def={})\n        model_version.save()\n\n        response = self.client.post(\n            reverse(\'saveDB\'),\n            {\'net\': \'{""net"": ""testnet""}\', \'net_name\': \'name\'})\n        response = json.loads(response.content)\n        self.assertEqual(response[\'result\'], \'success\')\n        self.assertTrue(\'id\' in response)\n        proto_id = response[\'id\']\n        response = self.client.post(reverse(\'loadDB\'), {\'proto_id\': proto_id})\n        response = json.loads(response.content)\n        self.assertEqual(response[\'result\'], \'success\')\n        self.assertEqual(response[\'net_name\'], \'name\')\n\n    def test_load_nofile(self):\n        response = self.client.post(reverse(\'loadDB\'),\n                                    {\'proto_id\': \'inexistent\'})\n        response = json.loads(response.content)\n        self.assertEqual(response[\'result\'], \'error\')\n        self.assertEqual(response[\'error\'], \'No network file found\')\n'"
tests/unit/caffe_app/test_urls.py,0,b''
tests/unit/caffe_app/test_views.py,0,"b'import caffe\nimport json\nimport os\nimport unittest\n\nfrom caffe import layers as L, params as P, to_proto\nfrom django.conf import settings\nfrom django.core.urlresolvers import reverse\nfrom django.test import Client\n\n\nclass ImportPrototxtTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        sample_file = open(os.path.join(settings.BASE_DIR,\n                                        \'example/caffe\',\n                                        \'GoogleNet.prototxt\'), \'r\')\n        # Test 1\n        response = self.client.post(reverse(\'caffe-import\'),\n                                    {\'file\': sample_file})\n        response = json.loads(response.content)\n        self.assertEqual(response[\'result\'], \'success\')\n        # Test 2\n        sample_file = open(os.path.join(settings.BASE_DIR,\n                                        \'example/keras\',\n                                        \'vgg16.json\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'),\n                                    {\'file\': sample_file})\n        response = json.loads(response.content)\n        self.assertEqual(response[\'result\'], \'error\')\n        self.assertEqual(response[\'error\'], \'Invalid Prototxt\\n\'\n                                            \'local variable \\\'prototxt\\\' referenced before assignment\')\n\n    def test_caffe_import_by_input(self):\n        sample_file = open(os.path.join(settings.BASE_DIR,\n                                        \'example/caffe\',\n                                        \'GoogleNet.prototxt\'), \'r\')\n        # Test 1\n        response = self.client.post(reverse(\'caffe-import\'),\n                                    {\'config\': sample_file.read()})\n        response = json.loads(response.content)\n        self.assertEqual(response[\'result\'], \'success\')\n        # Test 2\n        sample_file = open(os.path.join(settings.BASE_DIR,\n                                        \'example/keras\',\n                                        \'vgg16.json\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'),\n                                    {\'config\': sample_file.read()})\n        response = json.loads(response.content)\n        self.assertEqual(response[\'result\'], \'error\')\n        self.assertEqual(response[\'error\'], \'Invalid Prototxt\\n\'\n                                            \'1:1 : Expected identifier or number, got {.\')\n\n    def test_caffe_import_by_url(self):\n        url = \'https://github.com/Cloud-CV/Fabrik/blob/master/example/caffe/All_CNN.prototxt\'\n        # Test 1\n        response = self.client.post(reverse(\'caffe-import\'),\n                                    {\'url\': url})\n        response = json.loads(response.content)\n        self.assertEqual(response[\'result\'], \'success\')\n        # Test 2\n        url = \'https://github.com/Cloud-CV/Fabrik/blob/master/some_typo_here\'\n        response = self.client.post(reverse(\'caffe-import\'),\n                                    {\'url\': url})\n        response = json.loads(response.content)\n        self.assertEqual(response[\'result\'], \'error\')\n        self.assertEqual(response[\'error\'], \'Invalid URL\\nHTTP Error 404: Not Found\')\n\n    def test_caffe_import_by_sample_id(self):\n        response = self.client.post(reverse(\'caffe-import\'),\n                                    {\'sample_id\': \'GoogleNet\'})\n        response = json.loads(response.content)\n        self.assertEqual(response[\'result\'], \'success\')\n        response = self.client.post(reverse(\'caffe-import\'),\n                                    {\'sample_id\': \'vgg15\'})\n        response = json.loads(response.content)\n        self.assertEqual(response[\'result\'], \'error\')\n        self.assertEqual(response[\'error\'], \'No Prototxt model file found\')\n\n\nclass ExportPrototxtTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_export(self):\n        data = L.Input(shape={\'dim\': [10, 3, 224, 224]})\n        top = L.Convolution(data, kernel_size=3, pad=1, stride=1, num_output=128, dilation=1,\n                            weight_filler={\'type\': \'xavier\'}, bias_filler={\'type\': \'constant\'})\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        response[\'net\'][\'l0\'][\'params\'][\'caffe\'] = True\n        response[\'net\'][\'l1\'][\'params\'][\'caffe\'] = True\n        response = self.client.post(reverse(\'caffe-export\'), {\'net\': json.dumps(response[\'net\']),\n                                                              \'net_name\': \'\'})\n        response = json.loads(response.content)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass ExportPrototxtFailTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_export(self):\n        data = L.Input(shape={\'dim\': [10, 3, 16, 224, 224]})\n        top = L.Convolution(data, kernel_size=3, pad=1, stride=1, num_output=128, dilation=1,\n                            weight_filler={\'type\': \'xavier\'}, bias_filler={\'type\': \'constant\'})\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        response[\'net\'][\'l0\'][\'params\'][\'caffe\'] = True\n        response[\'net\'][\'l1\'][\'params\'][\'layer_type\'] = \'3D\'\n        response[\'net\'][\'l1\'][\'params\'][\'caffe\'] = False\n        response = self.client.post(reverse(\'caffe-export\'), {\'net\': json.dumps(response[\'net\']),\n                                                              \'net_name\': \'\'})\n        response = json.loads(response.content)\n        self.assertEqual(response[\'result\'], \'error\')\n\n\n# ********** Data Layers Test **********\nclass ImageDataLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        # Test 1\n        data, label = L.ImageData(source=\'/dummy/source/\', batch_size=32, ntop=2, rand_skip=0,\n                                  shuffle=False, new_height=256, new_width=256, is_color=False,\n                                  root_folder=\'/dummy/folder/\',\n                                  transform_param=dict(crop_size=227, mean_value=[104, 117, 123],\n                                                       mirror=True, force_color=False,\n                                                       force_gray=False))\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(data, label)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 13)\n        self.assertEqual(response[\'result\'], \'success\')\n        # Test 2\n        data, label = L.ImageData(source=\'/dummy/source/\', batch_size=32, ntop=2, rand_skip=0,\n                                  shuffle=False, new_height=256, new_width=256, is_color=False,\n                                  root_folder=\'/dummy/folder/\', include=dict(phase=caffe.TRAIN),\n                                  transform_param=dict(crop_size=227, mean_file=\'/path/to/file\',\n                                                       mirror=True, force_color=False,\n                                                       force_gray=False))\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(data, label)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 13)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass DataLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        # Test 1\n        data, label = L.Data(source=\'/dummy/source/\', backend=P.Data.LMDB, batch_size=32, ntop=2,\n                             rand_skip=0, prefetch=10,\n                             transform_param=dict(crop_size=227, mean_value=[104, 117, 123],\n                                                  mirror=True, force_color=False,\n                                                  force_gray=False))\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(data, label)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 10)\n        self.assertEqual(response[\'result\'], \'success\')\n        # Test 2\n        data, label = L.Data(source=\'/dummy/source/\', backend=P.Data.LEVELDB, batch_size=32, ntop=2,\n                             rand_skip=0, prefetch=10,\n                             transform_param=dict(crop_size=227, mean_value=[104, 117, 123],\n                                                  mirror=True, force_color=False,\n                                                  force_gray=False))\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(data, label)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 10)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass HDF5DataLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        data, label = L.HDF5Data(source=\'/dummy/source/\', batch_size=32, ntop=2, shuffle=False)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(data, label)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 3)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass HDF5OutputLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.HDF5Output(file_name=\'/dummy/filename\')\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 1)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass InputLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        data = L.Input(shape={\'dim\': [10, 3, 224, 224]})\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(data)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 1)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass WindowDataLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        data, label = L.WindowData(source=\'/dummy/source/\', batch_size=32, ntop=2,\n                                   fg_threshold=0.5, bg_threshold=0.5, fg_fraction=0.25,\n                                   context_pad=0, crop_mode=\'warp\', cache_images=False,\n                                   root_folder=\'/dummy/folder/\',\n                                   transform_param=dict(crop_size=227, mean_value=[104, 117, 123],\n                                                        mirror=True, force_color=False,\n                                                        force_gray=False))\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(data, label)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 14)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass MemoryDataLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        data, label = L.MemoryData(batch_size=32, ntop=2, channels=3, height=224, width=224)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(data, label)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 4)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass DummyDataLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        data = L.DummyData(shape={\'dim\': [10, 3, 224, 224]},\n                           data_filler={\'type\': \'constant\'})\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(data)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 1)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\n# ********** Vision Layers Test **********\nclass ConvolutionLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        # Test 1\n        top = L.Convolution(kernel_size=3, pad=1, stride=1, num_output=128,\n                            weight_filler={\'type\': \'xavier\'}, bias_filler={\'type\': \'constant\'})\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 6)\n        self.assertEqual(response[\'result\'], \'success\')\n        # Test 2\n        top = L.Convolution(kernel_w=3, kernel_h=3, pad_w=1, pad_h=1, stride=1, num_output=128,\n                            weight_filler={\'type\': \'xavier\'}, bias_filler={\'type\': \'constant\'})\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 6)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass PoolingLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        # Test 1\n        top = L.Pooling(kernel_size=2, pad=0, stride=2, pool=1)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 4)\n        self.assertEqual(response[\'result\'], \'success\')\n        # Test 2\n        top = L.Pooling(kernel_size=2, pad=0, stride=2, pool=2)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 4)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass SPPLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.SPP(pyramid_height=2, pool=1)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 2)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass CropLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.Crop(axis=2, offset=2)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 2)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass DeconvolutionLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        # Test 1\n        top = L.Deconvolution(convolution_param=dict(kernel_size=3, pad=1, stride=1, num_output=128,\n                              weight_filler={\'type\': \'xavier\'}, bias_filler={\'type\': \'constant\'}))\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 6)\n        self.assertEqual(response[\'result\'], \'success\')\n        # Test 2\n        top = L.Deconvolution(convolution_param=dict(kernel_w=3, kernel_h=3, pad_w=1, pad_h=1, stride=1,\n                              num_output=128, dilation=1, weight_filler={\'type\': \'xavier\'},\n                              bias_filler={\'type\': \'constant\'}))\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 6)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\n# ********** Recurrent Layers Test **********\nclass RecurrentLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.Recurrent(recurrent_param=dict(num_output=128, debug_info=False,\n                          expose_hidden=False, weight_filler={\'type\': \'xavier\'},\n                          bias_filler={\'type\': \'constant\'}))\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 5)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass RNNLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.RNN(recurrent_param=dict(num_output=128, debug_info=False,\n                    expose_hidden=False, weight_filler={\'type\': \'xavier\'},\n                    bias_filler={\'type\': \'constant\'}))\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 5)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass LSTMLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.LSTM(recurrent_param=dict(num_output=128, debug_info=False,\n                     expose_hidden=False, weight_filler={\'type\': \'xavier\'},\n                     bias_filler={\'type\': \'constant\'}))\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 5)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\n# ********** Common Layers Test **********\nclass InnerProductLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.InnerProduct(num_output=128, weight_filler={\'type\': \'xavier\'},\n                             bias_filler={\'type\': \'constant\'})\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 3)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass DropoutLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.Dropout()\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass EmbedLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.Embed(num_output=128, input_dim=2, bias_term=False,\n                      weight_filler={\'type\': \'xavier\'})\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 4)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\n# ********** Normalisation Layers Test **********\nclass LRNLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.LRN(local_size=5, alpha=1, beta=0.75, k=1, norm_region=1, in_place=True)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 5)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass MVNLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.MVN(normalize_variance=True, eps=1e-9, across_channels=False, in_place=True)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 3)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass BatchNormLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.BatchNorm(use_global_stats=True, moving_average_fraction=0.999, eps=1e-5, in_place=True)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 3)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\n# ********** Activation / Neuron Layers Test **********\nclass ReLULayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.ReLU(negative_slope=0, in_place=True)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 1)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass PReLULayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.PReLU(channel_shared=False, in_place=True)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 1)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass ELULayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.ELU(alpha=1, in_place=True)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 1)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass SigmoidLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.Sigmoid(in_place=True)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass TanHLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.TanH(in_place=True)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass AbsValLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.AbsVal(in_place=True)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass PowerLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.Power(power=1.0, scale=1.0, shift=0.0, in_place=True)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 3)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass ExpLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.Exp(base=-1.0, scale=1.0, shift=0.0, in_place=True)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 3)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass LogLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.Log(base=-1.0, scale=1.0, shift=0.0, in_place=True)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 3)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass BNLLLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.BNLL(in_place=True)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass ThresholdLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.Threshold(threshold=1.0, in_place=True)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 1)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass BiasLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.Bias(axis=1, num_axes=1, filler={\'type\': \'constant\'})\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 3)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass ScaleLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.Scale(bias_term=False)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 1)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\n# ********** Utility Layers Test **********\nclass FlattenLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.Flatten(axis=1, end_axis=-1)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 2)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass ReshapeLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.Reshape(shape={\'dim\': [2, -1]})\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 1)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass BatchReindexLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.BatchReindex()\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass SplitLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.Split()\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass ConcatLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.Concat()\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass SliceLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.Slice(axis=1, slice_dim=1, slice_point=[1, 2])\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 3)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass EltwiseLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        # Test 1\n        top = L.Eltwise(operation=2)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 1)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass FilterLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.Filter()\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertEqual(response[\'result\'], \'success\')\n\n\n# This layer is currently not supported as there is no bottom blob\n\'\'\'class ParameterLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.Parameter(shape={\'dim\': [10, 3, 224, 224]})\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 1)\n        self.assertEqual(response[\'result\'], \'success\')\n\'\'\'\n\n\nclass ReductionLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        # Test 1\n        top = L.Reduction(operation=1, axis=0, coeff=1.0)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 3)\n        self.assertEqual(response[\'result\'], \'success\')\n        # Test 2\n        top = L.Reduction(operation=2, axis=0, coeff=1.0)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 3)\n        self.assertEqual(response[\'result\'], \'success\')\n        # Test 3\n        top = L.Reduction(operation=3, axis=0, coeff=1.0)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 3)\n        self.assertEqual(response[\'result\'], \'success\')\n        # Test 4\n        top = L.Reduction(operation=4, axis=0, coeff=1.0)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 3)\n        self.assertEqual(response[\'result\'], \'success\')\n        # Test 5\n        top = L.Reduction(axis=0, coeff=1.0)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 3)\n        self.assertEqual(response[\'net\'][\'l0\'][\'params\'][\'operation\'], \'SUM\')\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass SilenceLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.Silence()\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass ArgMaxLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.ArgMax(out_max_val=False, top_k=1, axis=0)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 3)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass SoftmaxLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.Softmax()\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertEqual(response[\'result\'], \'success\')\n\n\n# ********** Loss Layers Test **********\nclass MultinomialLogisticLossLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.MultinomialLogisticLoss()\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass InfogainLossLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.InfogainLoss(source=\'/dummy/source/\', axis=1)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 2)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass SoftmaxWithLossLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.SoftmaxWithLoss(softmax_param=dict(axis=1))\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 1)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass EuclideanLossLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.EuclideanLoss()\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass HingeLossLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.HingeLoss(norm=2)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 1)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass SigmoidCrossEntropyLossLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.SigmoidCrossEntropyLoss()\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass AccuracyLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        data = L.Input(shape={\'dim\': [10, 100]})\n        top = L.Accuracy(data, axis=1, top_k=1, include=dict(phase=caffe.TEST))\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l1\'][\'params\']), 2)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\nclass ContrastiveLossLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        top = L.ContrastiveLoss(margin=1.0, legacy_version=False)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 2)\n        self.assertEqual(response[\'result\'], \'success\')\n\n\n# ********** Python Layer Test **********\nclass PythonLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_caffe_import(self):\n        # Test 1\n        data = L.Input(shape={\'dim\': [10, 3, 224, 224]})\n        top = L.Python(data, module=\'pyloss\', layer=\'EuclideanLossLayer\', loss_weight=1, name=\'eucLoss\')\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l1\'][\'params\']), 4)\n        self.assertEqual(response[\'result\'], \'success\')\n        # Test 2\n        top = L.Python(module=\'pascal_multilabel_datalayers\', layer=\'PascalMultilabelDataLayerSync\',\n                       param_str=""{\\\'pascal_root\\\': \\\'../data/pascal/VOC2007\\\', \\\'im_shape\\\': [227, 227], \\\n                        \\\'split\\\': \\\'train\\\', \\\'batch_size\\\': 128}"")\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        os.remove(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'))\n        self.assertGreaterEqual(len(response[\'net\'][\'l0\'][\'params\']), 6)\n        self.assertEqual(response[\'result\'], \'success\')\n'"
tests/unit/ide/__init__.py,0,b''
tests/unit/ide/test_urls.py,0,b''
tests/unit/ide/test_views.py,0,"b'import caffe\nimport json\nimport os\nimport sys\nimport unittest\nimport yaml\n\nfrom caffe import layers as L, to_proto\nfrom django.conf import settings\nfrom django.core.urlresolvers import reverse\nfrom django.test import Client\nfrom ide.utils.jsonToPrototxt import json_to_prototxt\nfrom ide.utils.shapes import get_shapes\nfrom keras.models import model_from_json\n\n\n# ********** Data Layers Test **********\nclass ImageDataLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'ImageData\']}\n        # Test 1\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l0\'][\'info\'][\'type\'], \'ImageData\')\n        # Test 2\n        net[\'l0\'][\'info\'][\'phase\'] = 0\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l0\'][\'info\'][\'type\'], \'ImageData\')\n        # Test 3\n        net[\'l0\'][\'info\'][\'phase\'] = 1\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l0\'][\'info\'][\'type\'], \'ImageData\')\n\n\nclass DataLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Data\']}\n        # Test 1\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l0\'][\'info\'][\'type\'], \'Data\')\n        # Test 2\n        net[\'l0\'][\'info\'][\'phase\'] = 0\n        net[\'l0\'][\'params\'][\'mean_value\'] = \'\'\n        net[\'l0\'][\'params\'][\'mean_file\'] = \'/path/to/mean/file\'\n        net[\'l0\'][\'params\'][\'backend\'] = ""LEVELDB""\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l0\'][\'info\'][\'type\'], \'Data\')\n        # Test 3\n        net[\'l0\'][\'info\'][\'phase\'] = 1\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l0\'][\'info\'][\'type\'], \'Data\')\n\n\nclass HDF5DataLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'HDF5Data\']}\n        # Test 1\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l0\'][\'info\'][\'type\'], \'HDF5Data\')\n        # Test 2\n        net[\'l0\'][\'info\'][\'phase\'] = 0\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l0\'][\'info\'][\'type\'], \'HDF5Data\')\n        # Test 3\n        net[\'l0\'][\'info\'][\'phase\'] = 1\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l0\'][\'info\'][\'type\'], \'HDF5Data\')\n\n\nclass HDF5OutputLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'HDF5Output\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        # Test 1\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'HDF5Output\')\n        # Test 2\n        net[\'l1\'][\'info\'][\'phase\'] = 0\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'HDF5Output\')\n        # Test 3\n        net[\'l1\'][\'info\'][\'phase\'] = 1\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'HDF5Output\')\n\n\nclass InputLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\']}\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertEqual(net[\'l0\'][\'info\'][\'type\'], \'Input\')\n\n\nclass WindowDataLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'WindowData\']}\n        # Test 1\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l0\'][\'info\'][\'type\'], \'WindowData\')\n        # Test 2\n        net[\'l0\'][\'info\'][\'phase\'] = 0\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l0\'][\'info\'][\'type\'], \'WindowData\')\n        # Test 3\n        net[\'l0\'][\'info\'][\'phase\'] = 1\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l0\'][\'info\'][\'type\'], \'WindowData\')\n\n\nclass MemoryDataLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'MemoryData\']}\n        # Test 1\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l0\'][\'info\'][\'type\'], \'MemoryData\')\n        # Test 2\n        net[\'l0\'][\'info\'][\'phase\'] = 0\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l0\'][\'info\'][\'type\'], \'MemoryData\')\n        # Test 3\n        net[\'l0\'][\'info\'][\'phase\'] = 1\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l0\'][\'info\'][\'type\'], \'MemoryData\')\n\n\nclass DummyDataLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'DummyData\']}\n        # Test 1\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l0\'][\'info\'][\'type\'], \'DummyData\')\n        # Test 2\n        net[\'l0\'][\'info\'][\'phase\'] = 0\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l0\'][\'info\'][\'type\'], \'DummyData\')\n        # Test 3\n        net[\'l0\'][\'info\'][\'phase\'] = 1\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l0\'][\'info\'][\'type\'], \'DummyData\')\n\n\n# ********** Vision Layers Test **********\nclass ConvolutionLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'Convolution\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Convolution\')\n\n\nclass PoolingLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'Pooling\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        # Test 1\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Pooling\')\n        # Test 2\n        net[\'l1\'][\'params\'][\'pool\'] = \'AVE\'\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Pooling\')\n        # Test 3\n        net[\'l1\'][\'params\'][\'pool\'] = \'STOCHASTIC\'\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Pooling\')\n\n\nclass SPPLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'SPP\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'SPP\')\n\n\nclass CropLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'Crop\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Crop\')\n\n\nclass DeconvolutionLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'Deconvolution\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Deconvolution\')\n\n\n# ********** Recurrent Layers Test **********\nclass RecurrentLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'Recurrent\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Recurrent\')\n\n\nclass RNNLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'RNN\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'RNN\')\n\n\nclass LSTMLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'LSTM\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'LSTM\')\n\n\n# ********** Common Layers Test **********\nclass InnerProductLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'InnerProduct\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'InnerProduct\')\n\n\nclass DropoutLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'Dropout\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Dropout\')\n\n\nclass EmbedLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'Embed\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Embed\')\n\n\n# ********** Normalisation Layers Test **********\nclass LRNLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'LRN\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        # Test 1\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'LRN\')\n        # Test 2\n        net[\'l1\'][\'params\'][\'norm_region\'] = \'ACROSS_CHANNELS\'\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'LRN\')\n\n\nclass MVNLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'MVN\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'MVN\')\n\n\nclass BatchNormLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'BatchNorm\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'BatchNorm\')\n\n\n# ********** Activation / Neuron Layers Test **********\nclass ReLULayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'ReLU\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'ReLU\')\n\n\nclass PReLULayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'PReLU\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'PReLU\')\n\n\nclass ELULayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'ELU\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'ELU\')\n\n\nclass SigmoidLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'Sigmoid\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Sigmoid\')\n\n\nclass TanHLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'TanH\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'TanH\')\n\n\nclass AbsValLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'AbsVal\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'AbsVal\')\n\n\nclass PowerLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'Power\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Power\')\n\n\nclass ExpLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'Exp\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Exp\')\n\n\nclass LogLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'Log\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Log\')\n\n\nclass BNLLLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'BNLL\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'BNLL\')\n\n\nclass ThresholdLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'Threshold\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Threshold\')\n\n\nclass BiasLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'Bias\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Bias\')\n\n\nclass ScaleLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'Scale\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Scale\')\n\n\n# ********** Utility Layers Test **********\nclass FlattenLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'Flatten\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Flatten\')\n\n\nclass ReshapeLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'Reshape\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Reshape\')\n\n\nclass BatchReindexLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'BatchReindex\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'BatchReindex\')\n\n\nclass SplitLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'Split\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Split\')\n\n\nclass ConcatLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'Concat\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Concat\')\n\n\nclass SliceLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'Slice\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Slice\')\n\n\nclass EltwiseLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'Eltwise\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        # Test 1\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Eltwise\')\n        # Test 2\n        net[\'l1\'][\'params\'][\'layer_type\'] = \'Sum\'\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Eltwise\')\n        # Test 3\n        net[\'l1\'][\'params\'][\'layer_type\'] = \'Maximum\'\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Eltwise\')\n        # Test 4\n        net[\'l1\'][\'params\'][\'layer_type\'] = \'\'\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Eltwise\')\n\n\nclass FilterLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'Filter\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Filter\')\n\n\n# This layer is currently not supported as there is no bottom blob\n\'\'\'class ParameterLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        data = L.Input(shape={\'dim\': [10, 3, 224, 224]})\n        top = L.Parameter(data, shape={\'dim\': [10, 3, 224, 224]})\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        sample_file = open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\')\n        response = self.client.post(reverse(\'caffe-import\'), {\'file\': sample_file})\n        response = json.loads(response.content)\n        with open(\'/home/utsav/Fabrik_Tests/ImageData.json\', \'w\') as outfile:\n            json.dump(response, outfile)\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'ImageData\')\'\'\'\n\n\nclass ReductionLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'Reduction\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        # Test 1\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Reduction\')\n        # Test 2\n        net[\'l1\'][\'params\'][\'operation\'] = \'SUM\'\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Reduction\')\n        # Test 3\n        net[\'l1\'][\'params\'][\'operation\'] = \'ASUM\'\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Reduction\')\n        # Test 4\n        net[\'l1\'][\'params\'][\'operation\'] = \'SUMSQ\'\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Reduction\')\n\n\nclass SilenceLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'Silence\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Silence\')\n\n\nclass ArgMaxLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'ArgMax\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'ArgMax\')\n\n\nclass SoftmaxLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'Softmax\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Softmax\')\n\n\n# ********** Loss Layers Test **********\nclass MultinomialLogisticLossLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'MultinomialLogisticLoss\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'MultinomialLogisticLoss\')\n\n\nclass InfogainLossLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'InfogainLoss\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'InfogainLoss\')\n\n\nclass SoftmaxWithLossLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'SoftmaxWithLoss\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'SoftmaxWithLoss\')\n\n\nclass EuclideanLossLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'EuclideanLoss\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'EuclideanLoss\')\n\n\nclass HingeLossLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'HingeLoss\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'HingeLoss\')\n\n\nclass SigmoidCrossEntropyLossLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'SigmoidCrossEntropyLoss\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'SigmoidCrossEntropyLoss\')\n\n\nclass AccuracyLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'Accuracy\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        # Test 1\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Accuracy\')\n        # Test 2\n        net[\'l1\'][\'info\'][\'phase\'] = 0\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Accuracy\')\n        # Test 3\n        net[\'l1\'][\'info\'][\'phase\'] = 1\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Accuracy\')\n\n\nclass ContrastiveLossLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'ContrastiveLoss\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'ContrastiveLoss\')\n\n\n# ********** Python Layer Test **********\nclass PythonDataLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'PythonData\']}\n        # Test 1\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l0\'][\'info\'][\'type\'], \'Python\')\n        # Test 2\n        net[\'l0\'][\'params\'][\'endPoint\'] = ""1, 0""\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l0\'][\'info\'][\'type\'], \'Python\')\n\n\nclass PythonLossLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_json_to_prototxt(self):\n        tests = open(os.path.join(settings.BASE_DIR, \'tests\', \'unit\', \'ide\',\n                                  \'caffe_export_test.json\'), \'r\')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response[\'net\']))\n        net = {\'l0\': net[\'Input\'], \'l1\': net[\'PythonLoss\']}\n        net[\'l0\'][\'connection\'][\'output\'].append(\'l1\')\n        prototxt, input_dim = json_to_prototxt(net, response[\'net_name\'])\n        self.assertGreater(len(prototxt), 9)\n        self.assertEqual(net[\'l1\'][\'info\'][\'type\'], \'Python\')\n\n\n# ********** Shape Calculation Test **********\nclass ShapeCalculationTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def caffe_test(self, path, key, success, layer=None):\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'r\') as f:\n            response = self.client.post(reverse(\'caffe-import\'), {\'file\': f})\n        response = json.loads(response.content)\n        if success:\n            net = get_shapes(response[\'net\'])\n            caffe_net = caffe.Net(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), caffe.TEST)\n            self.assertEqual(list(caffe_net.blobs[key].data.shape[1:]), net[\'l0\'][\'shape\'][\'output\'])\n        else:\n            try:\n                net = get_shapes(response[\'net\'])\n            except:\n                message = \'Cannot determine shape of \' + layer + \' layer.\'\n                self.assertEqual(str(sys.exc_info()[1]), message)\n\n    def keras_test(self, filename):\n        with open(filename, \'r\') as f:\n            response = self.client.post(reverse(\'keras-import\'), {\'file\': f})\n        response = json.loads(response.content)\n        net = get_shapes(response[\'net\'])\n        with open(filename, \'r\') as f:\n            model = model_from_json(json.dumps(json.load(f)))\n        for layer in model.layers:\n            self.assertEqual(list(layer.output_shape[::-1][:-1]), net[layer.name][\'shape\'][\'output\'])\n\n    def test_shapes(self):\n        # Test 1\n        image_path = os.path.join(settings.BASE_DIR, \'media\', \'image_list.txt\')\n        data, _ = L.ImageData(source=image_path, batch_size=32, ntop=2, rand_skip=0,\n                              shuffle=False, new_height=256, new_width=256, is_color=True,\n                              root_folder=os.path.join(settings.BASE_DIR, \'ide/static/img/\'),\n                              transform_param=dict(crop_size=227), name=\'l0\')\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(data)))\n        self.caffe_test(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'ImageData1\', True)\n        # Test 2\n        image_path = os.path.join(settings.BASE_DIR, \'media\', \'image_list.txt\')\n        data, _ = L.ImageData(source=image_path, batch_size=32, ntop=2, rand_skip=0,\n                              shuffle=False, new_height=256, new_width=256, is_color=True,\n                              root_folder=os.path.join(settings.BASE_DIR, \'ide/static/img/\'), name=\'l0\')\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(data)))\n        self.caffe_test(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'ImageData1\', True)\n        # Test 3\n        data, _ = L.MemoryData(batch_size=32, ntop=2, channels=3, height=224, width=224)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(data)))\n        self.caffe_test(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'MemoryData1\', True)\n        # Test 4\n        data, _ = L.HDF5Data(source=\'/dummy/source/\', batch_size=32, ntop=2, shuffle=False)\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(data)))\n        self.caffe_test(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'HDF5Data1\', False,\n                        \'HDF5Data\')\n        # Test 5\n        top = L.Python(module=\'pascal_multilabel_datalayers\', layer=\'PascalMultilabelDataLayerSync\',\n                       param_str=""{\\\'pascal_root\\\': \\\'../data/pascal/VOC2007\\\', \\\'im_shape\\\': [227, 227], \\\n                        \\\'split\\\': \\\'train\\\', \\\'batch_size\\\': 128}"")\n        with open(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'w\') as f:\n            f.write(str(to_proto(top)))\n        self.caffe_test(os.path.join(settings.BASE_DIR, \'media\', \'test.prototxt\'), \'HDF5Data1\', False,\n                        \'Python\')\n        # Test 6\n        self.keras_test(os.path.join(settings.BASE_DIR, \'example/keras\', \'shapeCheck1D.json\'))\n        # Test 7\n        self.keras_test(os.path.join(settings.BASE_DIR, \'example/keras\', \'shapeCheck2D.json\'))\n        # Test 8\n        self.keras_test(os.path.join(settings.BASE_DIR, \'example/keras\', \'shapeCheck3D.json\'))\n'"
tests/unit/keras_app/__init__.py,0,b''
tests/unit/keras_app/test_urls.py,0,b''
tests/unit/keras_app/test_views.py,0,"b""import json\nimport os\nimport unittest\nimport yaml\n\nfrom django.conf import settings\nfrom django.core.urlresolvers import reverse\nfrom django.test import Client\nfrom keras.layers import Dense, Activation, Dropout, Flatten\nfrom keras.layers import Reshape, Permute, RepeatVector\nfrom keras.layers import ActivityRegularization, Masking\nfrom keras.layers import Conv1D, Conv2D, Conv3D, Conv2DTranspose, \\\n    SeparableConv2D\nfrom keras.layers import UpSampling1D, UpSampling2D, UpSampling3D\nfrom keras.layers import GlobalMaxPooling1D, GlobalMaxPooling2D\nfrom keras.layers import MaxPooling1D, MaxPooling2D, MaxPooling3D\nfrom keras.layers import ZeroPadding1D, ZeroPadding2D, ZeroPadding3D\nfrom keras.layers import LocallyConnected1D, LocallyConnected2D\nfrom keras.layers import SimpleRNN, LSTM, GRU\nfrom keras.layers import Embedding\nfrom keras.layers import add, concatenate\nfrom keras.layers.advanced_activations import LeakyReLU, PReLU, \\\n    ELU, ThresholdedReLU\nfrom keras.layers import BatchNormalization\nfrom keras.layers import GaussianNoise, GaussianDropout, AlphaDropout\nfrom keras.layers import Input\nfrom keras import regularizers\nfrom keras.models import Model, Sequential\nfrom keras import backend as K\nfrom keras_app.views.layers_export import data, convolution, deconvolution, \\\n    pooling, dense, dropout, embed, recurrent, batch_norm, activation, \\\n    flatten, reshape, eltwise, concat, upsample, locally_connected, permute, \\\n    repeat_vector, regularization, masking, gaussian_noise, \\\n    gaussian_dropout, alpha_dropout\nfrom ide.utils.shapes import get_shapes\n\n\nclass ImportJsonTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import(self):\n        # Test 1\n        sample_file = open(os.path.join(settings.BASE_DIR,\n                                        'example/keras',\n                                        'vgg16.json'), 'r')\n        response = self.client.post(reverse('keras-import'),\n                                    {'file': sample_file})\n        response = json.loads(response.content)\n        self.assertEqual(response['result'], 'success')\n        # Test 2\n        sample_file = open(os.path.join(settings.BASE_DIR,\n                                        'example/caffe',\n                                        'GoogleNet.prototxt'), 'r')\n        response = self.client.post(reverse('keras-import'),\n                                    {'file': sample_file})\n        response = json.loads(response.content)\n        self.assertEqual(response['result'], 'error')\n        self.assertEqual(response['error'], 'Invalid JSON')\n\n    def test_keras_import_input(self):\n        # Test 1\n        sample_file = open(os.path.join(settings.BASE_DIR,\n                                        'example/keras',\n                                        'vgg16.json'), 'r')\n        response = self.client.post(reverse('keras-import'),\n                                    {'config': sample_file.read()})\n        response = json.loads(response.content)\n        self.assertEqual(response['result'], 'success')\n        # Test 2\n        sample_file = open(os.path.join(settings.BASE_DIR,\n                                        'example/caffe',\n                                        'GoogleNet.prototxt'), 'r')\n        response = self.client.post(reverse('keras-import'),\n                                    {'config': sample_file.read()})\n        response = json.loads(response.content)\n        self.assertEqual(response['result'], 'error')\n        self.assertEqual(response['error'], 'Invalid JSON')\n\n    def test_keras_import_by_url(self):\n        url = 'https://github.com/Cloud-CV/Fabrik/blob/master/example/keras/resnet50.json'\n        # Test 1\n        response = self.client.post(reverse('keras-import'),\n                                    {'url': url})\n        response = json.loads(response.content)\n        self.assertEqual(response['result'], 'success')\n        # Test 2\n        url = 'https://github.com/Cloud-CV/Fabrik/blob/master/some_typo_here'\n        response = self.client.post(reverse('keras-import'),\n                                    {'url': url})\n        response = json.loads(response.content)\n        self.assertEqual(response['result'], 'error')\n        self.assertEqual(response['error'], 'Invalid URL\\nHTTP Error 404: Not Found')\n\n    def test_keras_import_sample_id(self):\n        # Test 1\n        response = self.client.post(\n            reverse('keras-import'),\n            {'sample_id': 'vgg16'})\n        response = json.loads(response.content)\n        self.assertEqual(response['result'], 'success')\n        self.assertEqual(response['net_name'], 'vgg16')\n        self.assertTrue('net' in response)\n        # Test 2\n        response = self.client.post(\n            reverse('keras-import'),\n            {'sample_id': 'shapeCheck4D'})\n        response = json.loads(response.content)\n        self.assertEqual(response['result'], 'error')\n        self.assertEqual(response['error'], 'No JSON model file found')\n\n\nclass ExportJsonTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        # Test 1\n        img_input = Input((224, 224, 3))\n        model = Conv2D(64, (3, 3), padding='same', dilation_rate=1, use_bias=True,\n                       kernel_regularizer=regularizers.l1(), bias_regularizer='l1',\n                       activity_regularizer='l1', kernel_constraint='max_norm',\n                       bias_constraint='max_norm')(img_input)\n        model = BatchNormalization(center=True, scale=True, beta_regularizer=regularizers.l2(0.01),\n                                   gamma_regularizer=regularizers.l2(0.01),\n                                   beta_constraint='max_norm', gamma_constraint='max_norm',)(model)\n        model = Model(img_input, model)\n        json_string = Model.to_json(model)\n        with open(os.path.join(settings.BASE_DIR, 'media', 'test.json'), 'w') as out:\n            json.dump(json.loads(json_string), out, indent=4)\n        sample_file = open(os.path.join(settings.BASE_DIR, 'media', 'test.json'), 'r')\n        response = self.client.post(reverse('keras-import'), {'file': sample_file})\n        response = json.loads(response.content)\n        net = get_shapes(response['net'])\n        response = self.client.post(reverse('keras-export'), {'net': json.dumps(net),\n                                                              'net_name': ''})\n        response = json.loads(response.content)\n        self.assertEqual(response['result'], 'success')\n        # Test 2\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'ide',\n                                  'caffe_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['HDF5Data']}\n        # Currently we can't determine shape of HDF5Data Layer\n        response = self.client.post(reverse('keras-export'), {'net': json.dumps(net),\n                                                              'net_name': ''})\n        response = json.loads(response.content)\n        self.assertEqual(response['result'], 'error')\n\n\n# *********** Keras Backend Test **********\nclass KerasBackendTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_backend(self):\n        dim_order = K.image_dim_ordering()\n        backend = K.backend()\n        if(backend == 'tensorflow'):\n            self.assertEqual(dim_order, 'tf')\n        elif(backend == 'theano'):\n            self.assertNotEqual(dim_order, 'th')\n            self.assertEqual(dim_order, 'tf')\n        else:\n            self.fail('%s backend not supported' % backend)\n\n\n# ********** Import json tests **********\nclass HelperFunctions():\n    def setUp(self):\n        self.client = Client()\n\n    def keras_type_test(self, model, id, type):\n        json_string = Model.to_json(model)\n        with open(os.path.join(settings.BASE_DIR, 'media', 'test.json'), 'w') as out:\n            json.dump(json.loads(json_string), out, indent=4)\n        sample_file = open(os.path.join(settings.BASE_DIR, 'media', 'test.json'), 'r')\n        response = self.client.post(reverse('keras-import'), {'file': sample_file})\n        response = json.loads(response.content)\n        layerId = sorted(response['net'].keys())\n        self.assertEqual(response['result'], 'success')\n        self.assertEqual(response['net'][layerId[id]]['info']['type'], type)\n\n    def keras_param_test(self, model, id, params):\n        json_string = Model.to_json(model)\n        with open(os.path.join(settings.BASE_DIR, 'media', 'test.json'), 'w') as out:\n            json.dump(json.loads(json_string), out, indent=4)\n        sample_file = open(os.path.join(settings.BASE_DIR, 'media', 'test.json'), 'r')\n        response = self.client.post(reverse('keras-import'), {'file': sample_file})\n        response = json.loads(response.content)\n        layerId = sorted(response['net'].keys())\n        self.assertEqual(response['result'], 'success')\n        self.assertGreaterEqual(len(response['net'][layerId[id]]['params']), params)\n\n\n# ********** Data Layers **********\nclass InputImportTest(unittest.TestCase, HelperFunctions):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import(self):\n        model = Input((224, 224, 3))\n        model = Model(model, model)\n        self.keras_param_test(model, 0, 1)\n\n\n# ********** Core Layers **********\nclass DenseImportTest(unittest.TestCase, HelperFunctions):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import(self):\n        model = Sequential()\n        model.add(Dense(100, kernel_regularizer=regularizers.l2(0.01), bias_regularizer=regularizers.l2(0.01),\n                        activity_regularizer=regularizers.l2(0.01), kernel_constraint='max_norm',\n                        bias_constraint='max_norm', activation='relu', input_shape=(16,)))\n        model.build()\n        self.keras_param_test(model, 1, 3)\n\n\nclass ActivationImportTest(unittest.TestCase, HelperFunctions):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import(self):\n        # softmax\n        model = Sequential()\n        model.add(Activation('softmax', input_shape=(15,)))\n        model.build()\n        self.keras_type_test(model, 0, 'Softmax')\n        # relu\n        model = Sequential()\n        model.add(Activation('relu', input_shape=(15,)))\n        model.build()\n        self.keras_type_test(model, 0, 'ReLU')\n        # tanh\n        model = Sequential()\n        model.add(Activation('tanh', input_shape=(15,)))\n        model.build()\n        self.keras_type_test(model, 0, 'TanH')\n        # sigmoid\n        model = Sequential()\n        model.add(Activation('sigmoid', input_shape=(15,)))\n        model.build()\n        self.keras_type_test(model, 0, 'Sigmoid')\n        # selu\n        model = Sequential()\n        model.add(Activation('selu', input_shape=(15,)))\n        model.build()\n        self.keras_type_test(model, 0, 'SELU')\n        # softplus\n        model = Sequential()\n        model.add(Activation('softplus', input_shape=(15,)))\n        model.build()\n        self.keras_type_test(model, 0, 'Softplus')\n        # softsign\n        model = Sequential()\n        model.add(Activation('softsign', input_shape=(15,)))\n        model.build()\n        self.keras_type_test(model, 0, 'Softsign')\n        # hard_sigmoid\n        model = Sequential()\n        model.add(Activation('hard_sigmoid', input_shape=(15,)))\n        model.build()\n        self.keras_type_test(model, 0, 'HardSigmoid')\n        # LeakyReLU\n        model = Sequential()\n        model.add(LeakyReLU(alpha=1, input_shape=(15,)))\n        model.build()\n        self.keras_type_test(model, 0, 'ReLU')\n        # PReLU\n        model = Sequential()\n        model.add(PReLU(input_shape=(15,)))\n        model.build()\n        self.keras_type_test(model, 0, 'PReLU')\n        # ELU\n        model = Sequential()\n        model.add(ELU(alpha=1, input_shape=(15,)))\n        model.build()\n        self.keras_type_test(model, 0, 'ELU')\n        # ThresholdedReLU\n        model = Sequential()\n        model.add(ThresholdedReLU(theta=1, input_shape=(15,)))\n        model.build()\n        self.keras_type_test(model, 0, 'ThresholdedReLU')\n        # Linear\n        model = Sequential()\n        model.add(Activation('linear', input_shape=(15,)))\n        model.build()\n        self.keras_type_test(model, 0, 'Linear')\n\n\nclass DropoutImportTest(unittest.TestCase, HelperFunctions):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import(self):\n        model = Sequential()\n        model.add(Dropout(0.5, input_shape=(64, 10)))\n        model.build()\n        self.keras_type_test(model, 0, 'Dropout')\n\n\nclass FlattenImportTest(unittest.TestCase, HelperFunctions):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import(self):\n        model = Sequential()\n        model.add(Flatten(input_shape=(64, 10)))\n        model.build()\n        self.keras_type_test(model, 0, 'Flatten')\n\n\nclass ReshapeImportTest(unittest.TestCase, HelperFunctions):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import(self):\n        model = Sequential()\n        model.add(Reshape((5, 2), input_shape=(10,)))\n        model.build()\n        self.keras_type_test(model, 0, 'Reshape')\n\n\nclass PermuteImportTest(unittest.TestCase, HelperFunctions):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import(self):\n        model = Sequential()\n        model.add(Permute((2, 1), input_shape=(64, 10)))\n        model.build()\n        self.keras_type_test(model, 0, 'Permute')\n\n\nclass RepeatVectorImportTest(unittest.TestCase, HelperFunctions):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import(self):\n        model = Sequential()\n        model.add(RepeatVector(3, input_shape=(10,)))\n        model.build()\n        self.keras_type_test(model, 0, 'RepeatVector')\n\n\nclass ActivityRegularizationImportTest(unittest.TestCase, HelperFunctions):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import(self):\n        model = Sequential()\n        model.add(ActivityRegularization(l1=2, input_shape=(10,)))\n        model.build()\n        self.keras_type_test(model, 0, 'Regularization')\n\n\nclass MaskingImportTest(unittest.TestCase, HelperFunctions):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import(self):\n        model = Sequential()\n        model.add(Masking(mask_value=0., input_shape=(100, 5)))\n        model.build()\n        self.keras_type_test(model, 0, 'Masking')\n\n\n# ********** Convolutional Layers **********\nclass ConvolutionImportTest(unittest.TestCase, HelperFunctions):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import(self):\n        # Conv 1D\n        model = Sequential()\n        model.add(Conv1D(32, 10, kernel_regularizer=regularizers.l2(0.01),\n                         bias_regularizer=regularizers.l2(0.01),\n                         activity_regularizer=regularizers.l2(0.01), kernel_constraint='max_norm',\n                         bias_constraint='max_norm', activation='relu', input_shape=(10, 1)))\n        model.build()\n        self.keras_param_test(model, 1, 9)\n        # Conv 2D\n        model = Sequential()\n        model.add(Conv2D(32, (3, 3), kernel_regularizer=regularizers.l2(0.01),\n                         bias_regularizer=regularizers.l2(0.01),\n                         activity_regularizer=regularizers.l2(0.01), kernel_constraint='max_norm',\n                         bias_constraint='max_norm', activation='relu', input_shape=(16, 16, 1)))\n        model.build()\n        self.keras_param_test(model, 1, 13)\n        # Conv 3D\n        model = Sequential()\n        model.add(Conv3D(32, (3, 3, 3), kernel_regularizer=regularizers.l2(0.01),\n                         bias_regularizer=regularizers.l2(0.01),\n                         activity_regularizer=regularizers.l2(0.01), kernel_constraint='max_norm',\n                         bias_constraint='max_norm', activation='relu', input_shape=(16, 16, 16, 1)))\n        model.build()\n        self.keras_param_test(model, 1, 17)\n\n\nclass DepthwiseConvolutionImportTest(unittest.TestCase, HelperFunctions):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import(self):\n        model = Sequential()\n        model.add(SeparableConv2D(32, 3, depthwise_regularizer=regularizers.l2(0.01),\n                                  pointwise_regularizer=regularizers.l2(0.01),\n                                  bias_regularizer=regularizers.l2(0.01),\n                                  activity_regularizer=regularizers.l2(0.01), depthwise_constraint='max_norm',\n                                  bias_constraint='max_norm', pointwise_constraint='max_norm',\n                                  activation='relu', input_shape=(16, 16, 1)))\n        self.keras_param_test(model, 1, 12)\n\n    def test_keras_export(self):\n        model_file = open(os.path.join(settings.BASE_DIR, 'example/keras',\n                                       'SeparableConvKerasTest.json'), 'r')\n        response = self.client.post(reverse('keras-import'), {'file': model_file})\n        response = json.loads(response.content)\n        net = get_shapes(response['net'])\n        response = self.client.post(reverse('keras-export'), {'net': json.dumps(net),\n                                                              'net_name': ''})\n        response = json.loads(response.content)\n        self.assertEqual(response['result'], 'success')\n\n\nclass LRNImportTest(unittest.TestCase, HelperFunctions):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import_export(self):\n        model_file = open(os.path.join(settings.BASE_DIR, 'example/keras',\n                                       'AlexNet.json'), 'r')\n        response = self.client.post(reverse('keras-import'), {'file': model_file})\n        response = json.loads(response.content)\n        net = get_shapes(response['net'])\n        response = self.client.post(reverse('keras-export'), {'net': json.dumps(net),\n                                                              'net_name': ''})\n        response = json.loads(response.content)\n        self.assertEqual(response['result'], 'success')\n\n\nclass DeconvolutionImportTest(unittest.TestCase, HelperFunctions):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import(self):\n        model = Sequential()\n        model.add(Conv2DTranspose(32, (3, 3), kernel_regularizer=regularizers.l2(0.01),\n                                  bias_regularizer=regularizers.l2(0.01),\n                                  activity_regularizer=regularizers.l2(0.01), kernel_constraint='max_norm',\n                                  bias_constraint='max_norm', activation='relu', input_shape=(16, 16, 1)))\n        model.build()\n        self.keras_param_test(model, 1, 13)\n\n\nclass UpsampleImportTest(unittest.TestCase, HelperFunctions):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import(self):\n        # Upsample 1D\n        model = Sequential()\n        model.add(UpSampling1D(size=2, input_shape=(16, 1)))\n        model.build()\n        self.keras_param_test(model, 0, 2)\n        # Upsample 2D\n        model = Sequential()\n        model.add(UpSampling2D(size=(2, 2), input_shape=(16, 16, 1)))\n        model.build()\n        self.keras_param_test(model, 0, 3)\n        # Upsample 3D\n        model = Sequential()\n        model.add(UpSampling3D(size=(2, 2, 2), input_shape=(16, 16, 16, 1)))\n        model.build()\n        self.keras_param_test(model, 0, 4)\n\n\n# ********** Pooling Layers **********\nclass PoolingImportTest(unittest.TestCase, HelperFunctions):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import(self):\n        # Global Pooling 1D\n        model = Sequential()\n        model.add(GlobalMaxPooling1D(input_shape=(16, 1)))\n        model.build()\n        self.keras_param_test(model, 0, 5)\n        # Global Pooling 2D\n        model = Sequential()\n        model.add(GlobalMaxPooling2D(input_shape=(16, 16, 1)))\n        model.build()\n        self.keras_param_test(model, 0, 8)\n        # Pooling 1D\n        model = Sequential()\n        model.add(MaxPooling1D(pool_size=2, strides=2, padding='same', input_shape=(16, 1)))\n        model.build()\n        self.keras_param_test(model, 0, 5)\n        # Pooling 2D\n        model = Sequential()\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', input_shape=(16, 16, 1)))\n        model.build()\n        self.keras_param_test(model, 0, 8)\n        # Pooling 3D\n        model = Sequential()\n        model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='same',\n                               input_shape=(16, 16, 16, 1)))\n        model.build()\n        self.keras_param_test(model, 0, 11)\n\n\n# ********** Locally-connected Layers **********\nclass LocallyConnectedImportTest(unittest.TestCase, HelperFunctions):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import(self):\n        # Conv 1D\n        model = Sequential()\n        model.add(LocallyConnected1D(32, 3, kernel_regularizer=regularizers.l2(0.01),\n                                     bias_regularizer=regularizers.l2(0.01),\n                                     activity_regularizer=regularizers.l2(0.01), kernel_constraint='max_norm',\n                                     bias_constraint='max_norm', activation='relu', input_shape=(16, 10)))\n        model.build()\n        self.keras_param_test(model, 1, 12)\n        # Conv 2D\n        model = Sequential()\n        model.add(LocallyConnected2D(32, (3, 3), kernel_regularizer=regularizers.l2(0.01),\n                                     bias_regularizer=regularizers.l2(0.01),\n                                     activity_regularizer=regularizers.l2(0.01), kernel_constraint='max_norm',\n                                     bias_constraint='max_norm', activation='relu', input_shape=(16, 16, 10)))\n        model.build()\n        self.keras_param_test(model, 1, 14)\n\n\n# ********** Recurrent Layers **********\nclass RecurrentImportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import(self):\n        model = Sequential()\n        model.add(LSTM(64, return_sequences=True, input_shape=(10, 64)))\n        model.add(SimpleRNN(32, return_sequences=True))\n        model.add(GRU(10, kernel_regularizer=regularizers.l2(0.01),\n                      bias_regularizer=regularizers.l2(0.01), recurrent_regularizer=regularizers.l2(0.01),\n                      activity_regularizer=regularizers.l2(0.01), kernel_constraint='max_norm',\n                      bias_constraint='max_norm', recurrent_constraint='max_norm'))\n        model.build()\n        json_string = Model.to_json(model)\n        with open(os.path.join(settings.BASE_DIR, 'media', 'test.json'), 'w') as out:\n            json.dump(json.loads(json_string), out, indent=4)\n        sample_file = open(os.path.join(settings.BASE_DIR, 'media', 'test.json'), 'r')\n        response = self.client.post(reverse('keras-import'), {'file': sample_file})\n        response = json.loads(response.content)\n        layerId = sorted(response['net'].keys())\n        self.assertEqual(response['result'], 'success')\n        self.assertGreaterEqual(len(response['net'][layerId[1]]['params']), 7)\n        self.assertGreaterEqual(len(response['net'][layerId[3]]['params']), 7)\n        self.assertGreaterEqual(len(response['net'][layerId[6]]['params']), 7)\n\n\n# ********** Embedding Layers **********\nclass EmbeddingImportTest(unittest.TestCase, HelperFunctions):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import(self):\n        model = Sequential()\n        model.add(Embedding(1000, 64, input_length=10, embeddings_regularizer=regularizers.l2(0.01),\n                            embeddings_constraint='max_norm'))\n        model.build()\n        self.keras_param_test(model, 0, 7)\n\n\n# ********** Merge Layers **********\nclass ConcatImportTest(unittest.TestCase, HelperFunctions):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import(self):\n        img_input = Input((224, 224, 3))\n        model = Conv2D(64, (3, 3), padding='same')(img_input)\n        model = concatenate([img_input, model])\n        model = Model(img_input, model)\n        self.keras_type_test(model, 0, 'Concat')\n\n\nclass EltwiseImportTest(unittest.TestCase, HelperFunctions):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import(self):\n        img_input = Input((224, 224, 64))\n        model = Conv2D(64, (3, 3), padding='same')(img_input)\n        model = add([img_input, model])\n        model = Model(img_input, model)\n        self.keras_type_test(model, 0, 'Eltwise')\n\n\n# ********** Normalisation Layers **********\nclass BatchNormImportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import(self):\n        model = Sequential()\n        model.add(BatchNormalization(center=True, scale=True, beta_regularizer=regularizers.l2(0.01),\n                                     gamma_regularizer=regularizers.l2(0.01),\n                                     beta_constraint='max_norm', gamma_constraint='max_norm',\n                                     input_shape=(16, 10)))\n        model.build()\n        json_string = Model.to_json(model)\n        with open(os.path.join(settings.BASE_DIR, 'media', 'test.json'), 'w') as out:\n            json.dump(json.loads(json_string), out, indent=4)\n        sample_file = open(os.path.join(settings.BASE_DIR, 'media', 'test.json'), 'r')\n        response = self.client.post(reverse('keras-import'), {'file': sample_file})\n        response = json.loads(response.content)\n        layerId = sorted(response['net'].keys())\n        self.assertEqual(response['result'], 'success')\n        self.assertEqual(response['net'][layerId[0]]['info']['type'], 'Scale')\n        self.assertEqual(response['net'][layerId[1]]['info']['type'], 'BatchNorm')\n\n\n# ********** Noise Layers **********\nclass GaussianNoiseImportTest(unittest.TestCase, HelperFunctions):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import(self):\n        model = Sequential()\n        model.add(GaussianNoise(stddev=0.1, input_shape=(16, 1)))\n        model.build()\n        self.keras_param_test(model, 0, 1)\n\n\nclass GaussianDropoutImportTest(unittest.TestCase, HelperFunctions):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import(self):\n        model = Sequential()\n        model.add(GaussianDropout(rate=0.5, input_shape=(16, 1)))\n        model.build()\n        self.keras_param_test(model, 0, 1)\n\n\nclass AlphaDropoutImportTest(unittest.TestCase, HelperFunctions):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_import(self):\n        model = Sequential()\n        model.add(AlphaDropout(rate=0.5, seed=5, input_shape=(16, 1)))\n        model.build()\n        self.keras_param_test(model, 0, 1)\n\n\n# ********** Utility Layers **********\nclass PaddingImportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def pad_test(self, model, field, value):\n        json_string = Model.to_json(model)\n        with open(os.path.join(settings.BASE_DIR, 'media', 'test.json'), 'w') as out:\n            json.dump(json.loads(json_string), out, indent=4)\n        sample_file = open(os.path.join(settings.BASE_DIR, 'media', 'test.json'), 'r')\n        response = self.client.post(reverse('keras-import'), {'file': sample_file})\n        response = json.loads(response.content)\n        layerId = sorted(response['net'].keys())\n        self.assertEqual(response['result'], 'success')\n        self.assertEqual(response['net'][layerId[0]]['params'][field], value)\n\n    def test_keras_import(self):\n        # Pad 1D\n        model = Sequential()\n        model.add(ZeroPadding1D(2, input_shape=(224, 3)))\n        model.add(Conv1D(32, 7, strides=2))\n        model.build()\n        self.pad_test(model, 'pad_w', 2)\n        # Pad 2D\n        model = Sequential()\n        model.add(ZeroPadding2D(2, input_shape=(224, 224, 3)))\n        model.add(Conv2D(32, 7, strides=2))\n        model.build()\n        self.pad_test(model, 'pad_w', 2)\n        # Pad 3D\n        model = Sequential()\n        model.add(ZeroPadding3D(2, input_shape=(224, 224, 224, 3)))\n        model.add(Conv3D(32, 7, strides=2))\n        model.build()\n        self.pad_test(model, 'pad_w', 2)\n\n\n# ********** Export json tests **********\n\n# ********** Data Layers Test **********\nclass InputExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input']}\n        net = data(net['l0'], '', 'l0')\n        model = Model(net['l0'], net['l0'])\n        self.assertEqual(model.layers[0].__class__.__name__, 'InputLayer')\n\n\n# ********** Core Layers **********\nclass DenseExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input2'], 'l1': net['InnerProduct']}\n        net['l0']['connection']['output'].append('l1')\n        # Test 1\n        inp = data(net['l0'], '', 'l0')['l0']\n        temp = dense(net['l1'], [inp], 'l1')\n        model = Model(inp, temp['l1'])\n        self.assertEqual(model.layers[2].__class__.__name__, 'Dense')\n        # Test 2\n        net['l1']['params']['weight_filler'] = 'glorot_normal'\n        net['l1']['params']['bias_filler'] = 'glorot_normal'\n        inp = data(net['l0'], '', 'l0')['l0']\n        temp = dense(net['l1'], [inp], 'l1')\n        model = Model(inp, temp['l1'])\n        self.assertEqual(model.layers[2].__class__.__name__, 'Dense')\n\n\nclass ReLUExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input'], 'l1': net['ReLU']}\n        # Test 1\n        net['l0']['connection']['output'].append('l1')\n        inp = data(net['l0'], '', 'l0')['l0']\n        temp = activation(net['l1'], [inp], 'l1')\n        model = Model(inp, temp['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'Activation')\n        # Test 2\n        net['l1']['params']['negative_slope'] = 1\n        net['l0']['connection']['output'].append('l1')\n        inp = data(net['l0'], '', 'l0')['l0']\n        temp = activation(net['l1'], [inp], 'l1')\n        model = Model(inp, temp['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'LeakyReLU')\n\n\nclass PReLUExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input'], 'l1': net['PReLU']}\n        net['l0']['connection']['output'].append('l1')\n        inp = data(net['l0'], '', 'l0')['l0']\n        net = activation(net['l1'], [inp], 'l1')\n        model = Model(inp, net['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'PReLU')\n\n\nclass ELUExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input'], 'l1': net['ELU']}\n        net['l0']['connection']['output'].append('l1')\n        inp = data(net['l0'], '', 'l0')['l0']\n        net = activation(net['l1'], [inp], 'l1')\n        model = Model(inp, net['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'ELU')\n\n\nclass ThresholdedReLUExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input'], 'l1': net['ThresholdedReLU']}\n        net['l0']['connection']['output'].append('l1')\n        inp = data(net['l0'], '', 'l0')['l0']\n        net = activation(net['l1'], [inp], 'l1')\n        model = Model(inp, net['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'ThresholdedReLU')\n\n\nclass SigmoidExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input'], 'l1': net['Sigmoid']}\n        net['l0']['connection']['output'].append('l1')\n        inp = data(net['l0'], '', 'l0')['l0']\n        net = activation(net['l1'], [inp], 'l1')\n        model = Model(inp, net['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'Activation')\n\n\nclass TanHExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input'], 'l1': net['TanH']}\n        inp = data(net['l0'], '', 'l0')['l0']\n        net = activation(net['l1'], [inp], 'l1')\n        model = Model(inp, net['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'Activation')\n\n\nclass SoftmaxExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input'], 'l1': net['Softmax']}\n        net['l0']['connection']['output'].append('l1')\n        inp = data(net['l0'], '', 'l0')['l0']\n        net = activation(net['l1'], [inp], 'l1')\n        model = Model(inp, net['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'Activation')\n\n\nclass SELUExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input'], 'l1': net['SELU']}\n        net['l0']['connection']['output'].append('l1')\n        inp = data(net['l0'], '', 'l0')['l0']\n        net = activation(net['l1'], [inp], 'l1')\n        model = Model(inp, net['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'Activation')\n\n\nclass SoftplusExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input'], 'l1': net['Softplus']}\n        net['l0']['connection']['output'].append('l1')\n        inp = data(net['l0'], '', 'l0')['l0']\n        net = activation(net['l1'], [inp], 'l1')\n        model = Model(inp, net['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'Activation')\n\n\nclass SoftsignExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input'], 'l1': net['Softsign']}\n        net['l0']['connection']['output'].append('l1')\n        inp = data(net['l0'], '', 'l0')['l0']\n        net = activation(net['l1'], [inp], 'l1')\n        model = Model(inp, net['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'Activation')\n\n\nclass HardSigmoidExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input'], 'l1': net['HardSigmoid']}\n        net['l0']['connection']['output'].append('l1')\n        inp = data(net['l0'], '', 'l0')['l0']\n        net = activation(net['l1'], [inp], 'l1')\n        model = Model(inp, net['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'Activation')\n\n\nclass LinearActivationExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input'], 'l1': net['Linear']}\n        net['l0']['connection']['output'].append('l1')\n        inp = data(net['l0'], '', 'l0')['l0']\n        net = activation(net['l1'], [inp], 'l1')\n        model = Model(inp, net['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'Activation')\n\n\nclass DropoutExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input3'], 'l1': net['Dropout']}\n        net['l0']['connection']['output'].append('l1')\n        inp = data(net['l0'], '', 'l0')['l0']\n        net = dropout(net['l1'], [inp], 'l1')\n        model = Model(inp, net['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'Dropout')\n\n\nclass FlattenExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input'], 'l1': net['Flatten']}\n        net['l0']['connection']['output'].append('l1')\n        inp = data(net['l0'], '', 'l0')['l0']\n        net = flatten(net['l1'], [inp], 'l1')\n        model = Model(inp, net['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'Flatten')\n\n\nclass ReshapeExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input'], 'l1': net['Reshape']}\n        net['l0']['connection']['output'].append('l1')\n        inp = data(net['l0'], '', 'l0')['l0']\n        net = reshape(net['l1'], [inp], 'l1')\n        model = Model(inp, net['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'Reshape')\n\n\nclass PermuteExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input2'], 'l1': net['Permute']}\n        net['l0']['connection']['output'].append('l1')\n        inp = data(net['l0'], '', 'l0')['l0']\n        net = permute(net['l1'], [inp], 'l1')\n        model = Model(inp, net['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'Permute')\n\n\nclass RepeatVectorExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input3'], 'l1': net['RepeatVector']}\n        net['l0']['connection']['output'].append('l1')\n        inp = data(net['l0'], '', 'l0')['l0']\n        net = repeat_vector(net['l1'], [inp], 'l1')\n        model = Model(inp, net['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'RepeatVector')\n\n\nclass RegularizationExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input3'], 'l1': net['Regularization']}\n        net['l0']['connection']['output'].append('l1')\n        inp = data(net['l0'], '', 'l0')['l0']\n        net = regularization(net['l1'], [inp], 'l1')\n        model = Model(inp, net['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'ActivityRegularization')\n\n\nclass MaskingExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input2'], 'l1': net['Masking']}\n        net['l0']['connection']['output'].append('l1')\n        inp = data(net['l0'], '', 'l0')['l0']\n        net = masking(net['l1'], [inp], 'l1')\n        model = Model(inp, net['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'Masking')\n\n\n# ********** Vision Layers Test **********\nclass ConvolutionExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input'], 'l1': net['Input2'], 'l2': net['Input4'], 'l3': net['Convolution']}\n        # Conv 1D\n        net['l1']['connection']['output'].append('l3')\n        net['l3']['connection']['input'] = ['l1']\n        net['l3']['params']['layer_type'] = '1D'\n        net['l3']['shape']['input'] = net['l1']['shape']['output']\n        net['l3']['shape']['output'] = [128, 12]\n        inp = data(net['l1'], '', 'l1')['l1']\n        temp = convolution(net['l3'], [inp], 'l3')\n        model = Model(inp, temp['l3'])\n        self.assertEqual(model.layers[2].__class__.__name__, 'Conv1D')\n        # Conv 2D\n        net['l0']['connection']['output'].append('l0')\n        net['l3']['connection']['input'] = ['l0']\n        net['l3']['params']['layer_type'] = '2D'\n        net['l3']['shape']['input'] = net['l0']['shape']['output']\n        net['l3']['shape']['output'] = [128, 226, 226]\n        inp = data(net['l0'], '', 'l0')['l0']\n        temp = convolution(net['l3'], [inp], 'l3')\n        model = Model(inp, temp['l3'])\n        self.assertEqual(model.layers[2].__class__.__name__, 'Conv2D')\n        # Conv 3D\n        net['l2']['connection']['output'].append('l3')\n        net['l3']['connection']['input'] = ['l2']\n        net['l3']['params']['layer_type'] = '3D'\n        net['l3']['shape']['input'] = net['l2']['shape']['output']\n        net['l3']['shape']['output'] = [128, 226, 226, 18]\n        inp = data(net['l2'], '', 'l2')['l2']\n        temp = convolution(net['l3'], [inp], 'l3')\n        model = Model(inp, temp['l3'])\n        self.assertEqual(model.layers[2].__class__.__name__, 'Conv3D')\n\n\nclass DeconvolutionExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input'], 'l1': net['Deconvolution']}\n        net['l0']['connection']['output'].append('l1')\n        # Test 1\n        inp = data(net['l0'], '', 'l0')['l0']\n        temp = deconvolution(net['l1'], [inp], 'l1')\n        model = Model(inp, temp['l1'])\n        self.assertEqual(model.layers[2].__class__.__name__, 'Conv2DTranspose')\n        # Test 2\n        net['l1']['params']['weight_filler'] = 'xavier'\n        net['l1']['params']['bias_filler'] = 'xavier'\n        inp = data(net['l0'], '', 'l0')['l0']\n        temp = deconvolution(net['l1'], [inp], 'l1')\n        model = Model(inp, temp['l1'])\n        self.assertEqual(model.layers[2].__class__.__name__, 'Conv2DTranspose')\n\n\nclass UpsampleExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input'], 'l1': net['Input2'], 'l2': net['Input4'], 'l3': net['Upsample']}\n        # Conv 1D\n        net['l1']['connection']['output'].append('l3')\n        net['l3']['connection']['input'] = ['l1']\n        net['l3']['params']['layer_type'] = '1D'\n        inp = data(net['l1'], '', 'l1')['l1']\n        temp = upsample(net['l3'], [inp], 'l3')\n        model = Model(inp, temp['l3'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'UpSampling1D')\n        # Conv 2D\n        net['l0']['connection']['output'].append('l0')\n        net['l3']['connection']['input'] = ['l0']\n        net['l3']['params']['layer_type'] = '2D'\n        inp = data(net['l0'], '', 'l0')['l0']\n        temp = upsample(net['l3'], [inp], 'l3')\n        model = Model(inp, temp['l3'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'UpSampling2D')\n        # Conv 3D\n        net['l2']['connection']['output'].append('l3')\n        net['l3']['connection']['input'] = ['l2']\n        net['l3']['params']['layer_type'] = '3D'\n        inp = data(net['l2'], '', 'l2')['l2']\n        temp = upsample(net['l3'], [inp], 'l3')\n        model = Model(inp, temp['l3'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'UpSampling3D')\n\n\nclass PoolingExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input'], 'l1': net['Input2'], 'l2': net['Input4'], 'l3': net['Pooling']}\n        # Pool 1D\n        net['l1']['connection']['output'].append('l3')\n        net['l3']['connection']['input'] = ['l1']\n        net['l3']['params']['layer_type'] = '1D'\n        net['l3']['shape']['input'] = net['l1']['shape']['output']\n        net['l3']['shape']['output'] = [12, 12]\n        inp = data(net['l1'], '', 'l1')['l1']\n        temp = pooling(net['l3'], [inp], 'l3')\n        model = Model(inp, temp['l3'])\n        self.assertEqual(model.layers[2].__class__.__name__, 'MaxPooling1D')\n        # Pool 2D\n        net['l0']['connection']['output'].append('l0')\n        net['l3']['connection']['input'] = ['l0']\n        net['l3']['params']['layer_type'] = '2D'\n        net['l3']['shape']['input'] = net['l0']['shape']['output']\n        net['l3']['shape']['output'] = [3, 226, 226]\n        inp = data(net['l0'], '', 'l0')['l0']\n        temp = pooling(net['l3'], [inp], 'l3')\n        model = Model(inp, temp['l3'])\n        self.assertEqual(model.layers[2].__class__.__name__, 'MaxPooling2D')\n        # Pool 3D\n        net['l2']['connection']['output'].append('l3')\n        net['l3']['connection']['input'] = ['l2']\n        net['l3']['params']['layer_type'] = '3D'\n        net['l3']['shape']['input'] = net['l2']['shape']['output']\n        net['l3']['shape']['output'] = [3, 226, 226, 18]\n        inp = data(net['l2'], '', 'l2')['l2']\n        temp = pooling(net['l3'], [inp], 'l3')\n        model = Model(inp, temp['l3'])\n        self.assertEqual(model.layers[2].__class__.__name__, 'MaxPooling3D')\n\n\n# ********** Locally-connected Layers **********\nclass LocallyConnectedExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input'], 'l1': net['Input2'], 'l3': net['LocallyConnected']}\n        # LocallyConnected 1D\n        net['l1']['connection']['output'].append('l3')\n        net['l3']['connection']['input'] = ['l1']\n        net['l3']['params']['layer_type'] = '1D'\n        inp = data(net['l1'], '', 'l1')['l1']\n        temp = locally_connected(net['l3'], [inp], 'l3')\n        model = Model(inp, temp['l3'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'LocallyConnected1D')\n        # LocallyConnected 2D\n        net['l0']['connection']['output'].append('l0')\n        net['l0']['shape']['output'] = [3, 10, 10]\n        net['l3']['connection']['input'] = ['l0']\n        net['l3']['params']['layer_type'] = '2D'\n        inp = data(net['l0'], '', 'l0')['l0']\n        temp = locally_connected(net['l3'], [inp], 'l3')\n        model = Model(inp, temp['l3'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'LocallyConnected2D')\n\n\n# ********** Recurrent Layers Test **********\nclass RNNExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input2'], 'l1': net['RNN']}\n        net['l0']['connection']['output'].append('l1')\n        # # net = get_shapes(net)\n        inp = data(net['l0'], '', 'l0')['l0']\n        net = recurrent(net['l1'], [inp], 'l1')\n        model = Model(inp, net['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'SimpleRNN')\n\n\nclass LSTMExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input2'], 'l1': net['LSTM']}\n        net['l0']['connection']['output'].append('l1')\n        inp = data(net['l0'], '', 'l0')['l0']\n        net = recurrent(net['l1'], [inp], 'l1')\n        model = Model(inp, net['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'LSTM')\n\n\nclass GRUExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input2'], 'l1': net['GRU']}\n        net['l0']['connection']['output'].append('l1')\n        inp = data(net['l0'], '', 'l0')['l0']\n        net = recurrent(net['l1'], [inp], 'l1')\n        model = Model(inp, net['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'GRU')\n\n\n# ********** Embed Layer Test *********\nclass EmbedExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input3'], 'l1': net['Embed']}\n        net['l0']['connection']['output'].append('l1')\n        # Test 1\n        inp = data(net['l0'], '', 'l0')['l0']\n        temp = embed(net['l1'], [inp], 'l1')\n        model = Model(inp, temp['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'Embedding')\n        # Test 2\n        net['l1']['params']['input_length'] = None\n        net['l1']['params']['weight_filler'] = 'VarianceScaling'\n        inp = data(net['l0'], '', 'l0')['l0']\n        temp = embed(net['l1'], [inp], 'l1')\n        model = Model(inp, temp['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'Embedding')\n\n\n# ********** Merge Layers Test **********\nclass EltwiseExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input'], 'l1': net['Eltwise']}\n        net['l0']['connection']['output'].append('l1')\n        # Test 1\n        inp = data(net['l0'], '', 'l0')['l0']\n        temp = eltwise(net['l1'], [inp, inp], 'l1')\n        model = Model(inp, temp['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'Multiply')\n        # Test 2\n        net['l1']['params']['layer_type'] = 'Sum'\n        inp = data(net['l0'], '', 'l0')['l0']\n        temp = eltwise(net['l1'], [inp, inp], 'l1')\n        model = Model(inp, temp['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'Add')\n        # Test 3\n        net['l1']['params']['layer_type'] = 'Average'\n        inp = data(net['l0'], '', 'l0')['l0']\n        temp = eltwise(net['l1'], [inp, inp], 'l1')\n        model = Model(inp, temp['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'Average')\n        # Test 4\n        net['l1']['params']['layer_type'] = 'Dot'\n        inp = data(net['l0'], '', 'l0')['l0']\n        temp = eltwise(net['l1'], [inp, inp], 'l1')\n        model = Model(inp, temp['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'Dot')\n        # Test 5\n        net['l1']['params']['layer_type'] = 'Maximum'\n        inp = data(net['l0'], '', 'l0')['l0']\n        temp = eltwise(net['l1'], [inp, inp], 'l1')\n        model = Model(inp, temp['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'Maximum')\n\n\nclass ConcatExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input'], 'l1': net['Concat']}\n        net['l0']['connection']['output'].append('l1')\n        inp = data(net['l0'], '', 'l0')['l0']\n        net = concat(net['l1'], [inp, inp], 'l1')\n        model = Model(inp, net['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'Concatenate')\n\n\n# ********** Noise Layers Test **********\nclass GaussianNoiseExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input'], 'l1': net['GaussianNoise']}\n        net['l0']['connection']['output'].append('l1')\n        inp = data(net['l0'], '', 'l0')['l0']\n        net = gaussian_noise(net['l1'], [inp], 'l1')\n        model = Model(inp, net['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'GaussianNoise')\n\n\nclass GaussianDropoutExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input'], 'l1': net['GaussianDropout']}\n        net['l0']['connection']['output'].append('l1')\n        inp = data(net['l0'], '', 'l0')['l0']\n        net = gaussian_dropout(net['l1'], [inp], 'l1')\n        model = Model(inp, net['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'GaussianDropout')\n\n\nclass AlphaDropoutExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input'], 'l1': net['AlphaDropout']}\n        net['l0']['connection']['output'].append('l1')\n        inp = data(net['l0'], '', 'l0')['l0']\n        net = alpha_dropout(net['l1'], [inp], 'l1')\n        model = Model(inp, net['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'AlphaDropout')\n\n\n# ********** Normalisation Layers Test **********\nclass BatchNormExportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_keras_export(self):\n        tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app',\n                                  'keras_export_test.json'), 'r')\n        response = json.load(tests)\n        tests.close()\n        net = yaml.safe_load(json.dumps(response['net']))\n        net = {'l0': net['Input'], 'l1': net['BatchNorm'], 'l2': net['Scale']}\n        net['l0']['connection']['output'].append('l1')\n        # Test 1\n        inp = data(net['l0'], '', 'l0')['l0']\n        temp = batch_norm(net['l1'], [inp], 'l1', 'l2', net['l2'])\n        model = Model(inp, temp['l2'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'BatchNormalization')\n        # Test 2\n        net['l2']['params']['filler'] = 'VarianceScaling'\n        net['l2']['params']['bias_filler'] = 'VarianceScaling'\n        inp = data(net['l0'], '', 'l0')['l0']\n        temp = batch_norm(net['l1'], [inp], 'l1', 'l2', net['l2'])\n        model = Model(inp, temp['l2'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'BatchNormalization')\n        # Test 3\n        inp = data(net['l0'], '', 'l0')['l0']\n        temp = batch_norm(net['l1'], [inp], 'l1', 'l0', net['l0'])\n        model = Model(inp, temp['l1'])\n        self.assertEqual(model.layers[1].__class__.__name__, 'BatchNormalization')\n"""
tests/unit/tensorflow_app/__init__.py,0,b''
tests/unit/tensorflow_app/test_urls.py,0,b''
tests/unit/tensorflow_app/test_views.py,0,"b""import json\nimport os\nimport unittest\n\nfrom django.conf import settings\nfrom django.core.urlresolvers import reverse\nfrom django.test import Client\nfrom ide.utils.shapes import get_shapes\n\n\nclass UploadTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_tf_import(self):\n        sample_file = open(os.path.join(settings.BASE_DIR, 'example/tensorflow', 'GoogleNet.pbtxt'),\n                           'r')\n        response = self.client.post(reverse('tf-import'), {'file': sample_file})\n        response = json.loads(response.content)\n        self.assertEqual(response['result'], 'success')\n\n\nclass ConvLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_tf_import(self):\n        model_file = open(os.path.join(settings.BASE_DIR, 'example/tensorflow', 'Conv3DCheck.pbtxt'),\n                          'r')\n        response = self.client.post(reverse('tf-import'), {'file': model_file})\n        response = json.loads(response.content)\n        self.assertEqual(response['result'], 'success')\n\n\nclass DeconvLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_tf_import(self):\n        model_file = open(os.path.join(settings.BASE_DIR, 'example/tensorflow', 'denoiseAutoEncoder.pbtxt'),\n                          'r')\n        response = self.client.post(reverse('tf-import'), {'file': model_file})\n        response = json.loads(response.content)\n        self.assertEqual(response['result'], 'success')\n\n\nclass PoolLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_tf_import(self):\n        model_file = open(os.path.join(settings.BASE_DIR, 'example/tensorflow', 'Pool3DCheck.pbtxt'),\n                          'r')\n        response = self.client.post(reverse('tf-import'), {'file': model_file})\n        response = json.loads(response.content)\n        self.assertEqual(response['result'], 'success')\n\n\nclass RepeatLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_tf_import(self):\n        model_file = open(os.path.join(settings.BASE_DIR, 'example/tensorflow', 'Conv2DRepeat.pbtxt'),\n                          'r')\n        response = self.client.post(reverse('tf-import'), {'file': model_file})\n        response = json.loads(response.content)\n        self.assertEqual(response['result'], 'success')\n\n\nclass StackLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_tf_import(self):\n        model_file = open(os.path.join(settings.BASE_DIR, 'example/tensorflow', 'FCStack.pbtxt'),\n                          'r')\n        response = self.client.post(reverse('tf-import'), {'file': model_file})\n        response = json.loads(response.content)\n        self.assertEqual(response['result'], 'success')\n\n\nclass DepthwiseConvLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_tf_import(self):\n        model_file = open(os.path.join(settings.BASE_DIR, 'example/tensorflow', 'DepthwiseConv.pbtxt'),\n                          'r')\n        response = self.client.post(reverse('tf-import'), {'file': model_file})\n        response = json.loads(response.content)\n        self.assertEqual(response['result'], 'success')\n\n\nclass BatchNormLayerTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_tf_import(self):\n        model_file = open(os.path.join(settings.BASE_DIR, 'example/tensorflow', 'BatchNorm.pbtxt'),\n                          'r')\n        response = self.client.post(reverse('tf-import'), {'file': model_file})\n        response = json.loads(response.content)\n        self.assertEqual(response['result'], 'success')\n\n\nclass LRNImportTest(unittest.TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_tf_export(self):\n        model_file = open(os.path.join(settings.BASE_DIR, 'example/keras',\n                                       'AlexNet.json'), 'r')\n        response = self.client.post(reverse('keras-import'), {'file': model_file})\n        response = json.loads(response.content)\n        net = get_shapes(response['net'])\n        response = self.client.post(reverse('tf-export'), {'net': json.dumps(net),\n                                                           'net_name': ''})\n        response = json.loads(response.content)\n        self.assertEqual(response['result'], 'success')\n\n    def test_custom_lrn_tf_import(self):\n        model_file = open(os.path.join(settings.BASE_DIR, 'example/tensorflow', 'LRN.pbtxt'),\n                          'r')\n        response = self.client.post(reverse('tf-import'), {'file': model_file})\n        response = json.loads(response.content)\n        self.assertEqual(response['result'], 'success')\n"""
