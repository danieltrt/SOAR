file_path,api_count,code
algorithm/__init__.py,0,b''
baseclass/DeepRecommender.py,9,"b'from baseclass.IterativeRecommender import IterativeRecommender\nfrom tool import config\nimport numpy as np\nfrom random import shuffle\nimport tensorflow as tf\n\nclass DeepRecommender(IterativeRecommender):\n    def __init__(self,conf,trainingSet,testSet,fold=\'[1]\'):\n        super(DeepRecommender, self).__init__(conf,trainingSet,testSet,fold)\n\n    def readConfiguration(self):\n        super(DeepRecommender, self).readConfiguration()\n        # set the reduced dimension\n        self.batch_size = int(self.config[\'batch_size\'])\n\n\n    def printAlgorConfig(self):\n        super(DeepRecommender, self).printAlgorConfig()\n\n\n    def initModel(self):\n        super(DeepRecommender, self).initModel()\n        self.u_idx = tf.placeholder(tf.int32, name=""u_idx"")\n        self.v_idx = tf.placeholder(tf.int32, name=""v_idx"")\n\n        self.r = tf.placeholder(tf.float32, name=""rating"")\n\n        self.user_embeddings = tf.Variable(tf.truncated_normal(shape=[self.num_users, self.embed_size], stddev=0.005), name=\'U\')\n        self.item_embeddings = tf.Variable(tf.truncated_normal(shape=[self.num_items, self.embed_size], stddev=0.005), name=\'V\')\n\n        self.u_embedding = tf.nn.embedding_lookup(self.user_embeddings, self.u_idx)\n        self.v_embedding = tf.nn.embedding_lookup(self.item_embeddings, self.v_idx)\n\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        self.sess = tf.Session(config=config)\n\n\n\n    def saveModel(self):\n        pass\n\n    def loadModel(self):\n        pass\n\n    def predictForRanking(self,u):\n        \'used to rank all the items for the user\'\n        pass\n\n    def isConverged(self,iter):\n        from math import isnan\n        if isnan(self.loss):\n            print \'Loss = NaN or Infinity: current settings does not fit the recommender! Change the settings and try again!\'\n            exit(-1)\n        deltaLoss = (self.lastLoss-self.loss)\n        if self.ranking.isMainOn():\n            measure = self.ranking_performance()\n            print \'%s %s iteration %d: loss = %.4f, delta_loss = %.5f learning_Rate = %.5f %s %s (Top-10 On 300 users)\' \\\n                  %(self.algorName,self.foldInfo,iter,self.loss,deltaLoss,self.lRate, measure[-3].strip()[:11], measure[-2].strip()[:12])\n        else:\n            measure = self.rating_performance()\n            print \'%s %s iteration %d: loss = %.4f, delta_loss = %.5f learning_Rate = %.5f %5s %5s\' \\\n                  % (self.algorName, self.foldInfo, iter, self.loss, deltaLoss, self.lRate, measure[0].strip()[:11], measure[1].strip()[:12])\n        #check if converged\n        cond = abs(deltaLoss) < 1e-8\n        converged = cond\n        if not converged:\n            self.updateLearningRate(iter)\n        self.lastLoss = self.loss\n        shuffle(self.data.trainingData)\n        return converged\n\n'"
baseclass/IterativeRecommender.py,11,"b'from baseclass.Recommender import Recommender\nfrom tool import config\nimport numpy as np\nfrom random import shuffle\nfrom tool.qmath import denormalize\nfrom evaluation.measure import Measure\n\nclass IterativeRecommender(Recommender):\n    def __init__(self,conf,trainingSet,testSet,fold=\'[1]\'):\n        super(IterativeRecommender, self).__init__(conf,trainingSet,testSet,fold)\n\n    def readConfiguration(self):\n        super(IterativeRecommender, self).readConfiguration()\n        # set the reduced dimension\n        self.embed_size = int(self.config[\'num.factors\'])\n        # set maximum iteration\n        self.maxIter = int(self.config[\'num.max.iter\'])\n        # set learning rate\n        learningRate = config.LineConfig(self.config[\'learnRate\'])\n        self.lRate = float(learningRate[\'-init\'])\n        self.maxLRate = float(learningRate[\'-max\'])\n        if self.evalSettings.contains(\'-tf\'):\n            self.batch_size = int(self.config[\'batch_size\'])\n        # regularization parameter\n        regular = config.LineConfig(self.config[\'reg.lambda\'])\n        self.regU,self.regI,self.regB= float(regular[\'-u\']),float(regular[\'-i\']),float(regular[\'-b\'])\n\n    def printAlgorConfig(self):\n        super(IterativeRecommender, self).printAlgorConfig()\n        print \'Reduced Dimension:\',self.embed_size\n        print \'Maximum Iteration:\',self.maxIter\n        print \'Regularization parameter: regU %.3f, regI %.3f, regB %.3f\' %(self.regU,self.regI,self.regB)\n        print \'=\'*80\n\n    def initModel(self):\n        self.P = np.random.rand(len(self.data.user), self.embed_size)/3 # latent user matrix\n        self.Q = np.random.rand(len(self.data.item), self.embed_size)/3  # latent item matrix\n        self.loss, self.lastLoss = 0, 0\n\n\n    def buildModel_tf(self):\n        # initialization\n        import tensorflow as tf\n        self.u_idx = tf.placeholder(tf.int32, [None], name=""u_idx"")\n        self.v_idx = tf.placeholder(tf.int32, [None], name=""v_idx"")\n        self.r = tf.placeholder(tf.float32, [None], name=""rating"")\n\n        self.U = tf.Variable(tf.truncated_normal(shape=[self.num_users, self.embed_size], stddev=0.005), name=\'U\')\n        self.V = tf.Variable(tf.truncated_normal(shape=[self.num_items, self.embed_size], stddev=0.005), name=\'V\')\n\n        self.user_biases = tf.Variable(tf.truncated_normal(shape=[self.num_users, 1], stddev=0.005), name=\'U\')\n        self.item_biases = tf.Variable(tf.truncated_normal(shape=[self.num_items, 1], stddev=0.005), name=\'U\')\n\n        self.user_bias = tf.nn.embedding_lookup(self.user_biases, self.u_idx)\n        self.item_bias = tf.nn.embedding_lookup(self.item_biases, self.v_idx)\n\n        self.user_embedding = tf.nn.embedding_lookup(self.U, self.u_idx)\n        self.item_embedding = tf.nn.embedding_lookup(self.V, self.v_idx)\n\n\n    def saveModel(self):\n        pass\n\n    def loadModel(self):\n        pass\n\n    def updateLearningRate(self,iter):\n        if iter > 1:\n            if abs(self.lastLoss) > abs(self.loss):\n                self.lRate *= 1.05\n            else:\n                self.lRate *= 0.5\n\n        if self.lRate > self.maxLRate > 0:\n            self.lRate = self.maxLRate\n\n\n    def predict(self,u,i):\n\n        if self.data.containsUser(u) and self.data.containsItem(i):\n            return self.P[self.data.user[u]].dot(self.Q[self.data.item[i]])\n        elif self.data.containsUser(u) and not self.data.containsItem(i):\n            return self.data.userMeans[u]\n        elif not self.data.containsUser(u) and self.data.containsItem(i):\n            return self.data.itemMeans[i]\n        else:\n            return self.data.globalMean\n\n\n    def predictForRanking(self,u):\n        \'used to rank all the items for the user\'\n        if self.data.containsUser(u):\n            return self.Q.dot(self.P[self.data.user[u]])\n        else:\n            return [self.data.globalMean]*self.num_items\n\n    def isConverged(self,iter):\n        from math import isnan\n        if isnan(self.loss):\n            print \'Loss = NaN or Infinity: current settings does not fit the recommender! Change the settings and try again!\'\n            exit(-1)\n        deltaLoss = (self.lastLoss-self.loss)\n        if self.ranking.isMainOn():\n            print \'%s %s iteration %d: loss = %.4f, delta_loss = %.5f learning_Rate = %.5f\' \\\n                  %(self.algorName,self.foldInfo,iter,self.loss,deltaLoss,self.lRate)\n            measure = self.ranking_performance()\n        else:\n            measure = self.rating_performance()\n            print \'%s %s iteration %d: loss = %.4f, delta_loss = %.5f learning_Rate = %.5f %5s %5s\' \\\n                  % (self.algorName, self.foldInfo, iter, self.loss, deltaLoss, self.lRate, measure[0].strip()[:11], measure[1].strip()[:12])\n        #check if converged\n        cond = abs(deltaLoss) < 1e-3\n        converged = cond\n        if not converged:\n            self.updateLearningRate(iter)\n        self.lastLoss = self.loss\n        shuffle(self.data.trainingData)\n        return converged\n\n    def rating_performance(self):\n\n        res = []\n        for ind, entry in enumerate(self.data.testData):\n            user, item, rating = entry\n\n            # predict\n            prediction = self.predict(user, item)\n            # denormalize\n            #prediction = denormalize(prediction, self.data.rScale[-1], self.data.rScale[0])\n            #####################################\n            pred = self.checkRatingBoundary(prediction)\n            # add prediction in order to measure\n            res.append([user,item,rating,pred])\n\n        self.measure = Measure.ratingMeasure(res)\n\n        return self.measure\n\n    def ranking_performance(self):\n        N = 20\n        recList = {}\n        testSample = {}\n        for user in self.data.testSet_u:\n            if len(testSample) == 1000:\n                break\n            testSample[user] = self.data.testSet_u[user]\n\n        for user in testSample:\n            itemSet = {}\n            predictedItems = self.predictForRanking(user)\n            for id, rating in enumerate(predictedItems):\n                itemSet[self.data.id2item[id]] = rating\n\n            ratedList, ratingList = self.data.userRated(user)\n            for item in ratedList:\n                del itemSet[item]\n\n            Nrecommendations = []\n            for item in itemSet:\n                if len(Nrecommendations) < N:\n                    Nrecommendations.append((item, itemSet[item]))\n                else:\n                    break\n\n            Nrecommendations.sort(key=lambda d: d[1], reverse=True)\n            recommendations = [item[1] for item in Nrecommendations]\n            resNames = [item[0] for item in Nrecommendations]\n\n            # find the K biggest scores\n            for item in itemSet:\n                ind = N\n                l = 0\n                r = N - 1\n\n                if recommendations[r] < itemSet[item]:\n                    while True:\n                        mid = (l + r) / 2\n                        if recommendations[mid] >= itemSet[item]:\n                            l = mid + 1\n                        elif recommendations[mid] < itemSet[item]:\n                            r = mid - 1\n                        if r < l:\n                            ind = r\n                            break\n                # ind = bisect(recommendations, itemSet[item])\n                            # move the items backwards\n                if ind < N - 2:\n                    recommendations[ind + 2:] = recommendations[ind + 1:-1]\n                    resNames[ind + 2:] = resNames[ind + 1:-1]\n                if ind < N - 1:\n                    recommendations[ind + 1] = itemSet[item]\n                    resNames[ind + 1] = item\n            recList[user] = zip(resNames, recommendations)\n        measure = Measure.rankingMeasure(testSample, recList, [10,20])\n        print \'-\'*80\n        print \'Ranking Performance \'+self.foldInfo+\' (Top-10 On 1000 sampled users)\'\n        for m in measure[1:]:\n            print m.strip()\n        print \'-\'*80\n        #self.record.append(measure[3].strip()+\' \'+measure[4])\n        return measure\n\n'"
baseclass/Recommender.py,0,"b'# Copyright (C) 2016 School of Software Engineering, Chongqing University\n#\n# This file is part of RecQ.\n#\n# RecQ is a free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\nfrom data.rating import RatingDAO\nfrom tool.file import FileIO\nimport pandas as pd\nfrom tool.config import Config,LineConfig\nfrom os.path import abspath\nfrom time import strftime,localtime,time\nfrom evaluation.measure import Measure\nclass Recommender(object):\n    def __init__(self,conf,trainingSet,testSet,fold=\'[1]\'):\n        self.config = conf\n        self.data = None\n        self.isSaveModel = False\n        self.ranking = None\n        self.isLoadModel = False\n        self.output = None\n        self.isOutput = True\n        self.data = RatingDAO(self.config, trainingSet, testSet)\n        self.foldInfo = fold\n        self.evalSettings = LineConfig(self.config[\'evaluation.setup\'])\n        self.measure = []\n        self.record = []\n        if self.evalSettings.contains(\'-cold\'):\n            #evaluation on cold-start users\n            threshold = int(self.evalSettings[\'-cold\'])\n            removedUser = {}\n            for user in self.data.testSet_u:\n                if self.data.trainSet_u.has_key(user) and len(self.data.trainSet_u[user])>threshold:\n                    removedUser[user]=1\n\n            for user in removedUser:\n                del self.data.testSet_u[user]\n\n            testData = []\n            for item in self.data.testData:\n                if not removedUser.has_key(item[0]):\n                    testData.append(item)\n            self.data.testData = testData\n\n        self.num_users, self.num_items, self.train_size = self.data.trainingSize()\n\n\n    def readConfiguration(self):\n        self.algorName = self.config[\'recommender\']\n        self.output = LineConfig(self.config[\'output.setup\'])\n        self.isOutput = self.output.isMainOn()\n        self.ranking = LineConfig(self.config[\'item.ranking\'])\n\n    def printAlgorConfig(self):\n        ""show algorithm\'s configuration""\n        print \'Algorithm:\',self.config[\'recommender\']\n        print \'Ratings dataset:\',abspath(self.config[\'ratings\'])\n        if LineConfig(self.config[\'evaluation.setup\']).contains(\'-testSet\'):\n            print \'Test set:\',abspath(LineConfig(self.config[\'evaluation.setup\']).getOption(\'-testSet\'))\n        #print \'Count of the users in training set: \',len()\n        print \'Training set size: (user count: %d, item count %d, record count: %d)\' %(self.data.trainingSize())\n        print \'Test set size: (user count: %d, item count %d, record count: %d)\' %(self.data.testSize())\n        print \'=\'*80\n\n    def initModel(self):\n        pass\n\n    def buildModel(self):\n        \'build the model (for model-based algorithms )\'\n        pass\n\n    def buildModel_tf(self):\n        \'training model on tensorflow\'\n        pass\n\n    def saveModel(self):\n        pass\n\n    def loadModel(self):\n        pass\n\n    def predict(self,u,i):\n        pass\n\n    def predictForRanking(self,u):\n        pass\n\n\n    def checkRatingBoundary(self,prediction):\n        if prediction > self.data.rScale[-1]:\n            return self.data.rScale[-1]\n        elif prediction < self.data.rScale[0]:\n            return self.data.rScale[0]\n        else:\n            return round(prediction,3)\n\n    def evalRatings(self):\n        res = list() #used to contain the text of the result\n        res.append(\'userId  itemId  original  prediction\\n\')\n        #predict\n        for ind,entry in enumerate(self.data.testData):\n            user,item,rating = entry\n\n            #predict\n            prediction = self.predict(user,item)\n            #denormalize\n            #prediction = denormalize(prediction,self.data.rScale[-1],self.data.rScale[0])\n            #####################################\n            pred = self.checkRatingBoundary(prediction)\n            # add prediction in order to measure\n            self.data.testData[ind].append(pred)\n            res.append(user+\' \'+item+\' \'+str(rating)+\' \'+str(pred)+\'\\n\')\n        currentTime = strftime(""%Y-%m-%d %H-%M-%S"",localtime(time()))\n        #output prediction result\n        if self.isOutput:\n            outDir = self.output[\'-dir\']\n            fileName = self.config[\'recommender\']+\'@\'+currentTime+\'-rating-predictions\'+self.foldInfo+\'.txt\'\n            FileIO.writeFile(outDir,fileName,res)\n            print \'The result has been output to \',abspath(outDir),\'.\'\n        #output evaluation result\n        outDir = self.output[\'-dir\']\n        fileName = self.config[\'recommender\'] + \'@\'+currentTime +\'-measure\'+ self.foldInfo + \'.txt\'\n        self.measure = Measure.ratingMeasure(self.data.testData)\n        FileIO.writeFile(outDir, fileName, self.measure)\n        print \'The result of %s %s:\\n%s\' % (self.algorName, self.foldInfo, \'\'.join(self.measure))\n\n    def evalRanking(self):\n        res = []  # used to contain the text of the result\n\n        if self.ranking.contains(\'-topN\'):\n            top = self.ranking[\'-topN\'].split(\',\')\n            top = [int(num) for num in top]\n            N = int(top[-1])\n            if N > 100 or N < 0:\n                print \'N can not be larger than 100! It has been reassigned with 10\'\n                N = 10\n            if N > len(self.data.item):\n                N = len(self.data.item)\n        else:\n            print \'No correct evaluation metric is specified!\'\n            exit(-1)\n\n        res.append(\'userId: recommendations in (itemId, ranking score) pairs, * means the item matches.\\n\')\n        # predict\n        recList = {}\n        userN = {}\n        userCount = len(self.data.testSet_u)\n        #rawRes = {}\n        for i, user in enumerate(self.data.testSet_u):\n            itemSet = {}\n            line = user + \':\'\n            predictedItems = self.predictForRanking(user)\n            # predictedItems = denormalize(predictedItems, self.data.rScale[-1], self.data.rScale[0])\n            for id, rating in enumerate(predictedItems):\n                # if not self.data.rating(user, self.data.id2item[id]):\n                # prediction = self.checkRatingBoundary(prediction)\n                # pred = self.checkRatingBoundary(prediction)\n                #####################################\n                # add prediction in order to measure\n\n                itemSet[self.data.id2item[id]] = rating\n\n            ratedList, ratingList = self.data.userRated(user)\n            for item in ratedList:\n                del itemSet[item]\n\n            Nrecommendations = []\n            for item in itemSet:\n                if len(Nrecommendations) < N:\n                    Nrecommendations.append((item, itemSet[item]))\n                else:\n                    break\n\n            Nrecommendations.sort(key=lambda d: d[1], reverse=True)\n            recommendations = [item[1] for item in Nrecommendations]\n            resNames = [item[0] for item in Nrecommendations]\n\n\n            # find the N biggest scores\n            for item in itemSet:\n                ind = N\n                l = 0\n                r = N - 1\n\n                if recommendations[r] < itemSet[item]:\n                    while r>=l:\n                        mid = (r-l) / 2 + l\n                        if recommendations[mid] >= itemSet[item]:\n                            l = mid + 1\n                        elif recommendations[mid] < itemSet[item]:\n                            r = mid - 1\n\n                        if r < l:\n                            ind = r\n                            break\n                #move the items backwards\n                if ind < N - 2:\n                    recommendations[ind+2:]=recommendations[ind+1:-1]\n                    resNames[ind+2:]=resNames[ind+1:-1]\n                if ind < N - 1:\n                    recommendations[ind+1] = itemSet[item]\n                    resNames[ind+1] = item\n\n            recList[user] = zip(resNames, recommendations)\n\n\n            if i % 100 == 0:\n                print self.algorName, self.foldInfo, \'progress:\' + str(i) + \'/\' + str(userCount)\n            for item in recList[user]:\n                line += \' (\' + item[0] + \',\' + str(item[1]) + \')\'\n                if self.data.testSet_u[user].has_key(item[0]):\n                    line += \'*\'\n\n            line += \'\\n\'\n            res.append(line)\n        currentTime = strftime(""%Y-%m-%d %H-%M-%S"", localtime(time()))\n        # output prediction result\n        if self.isOutput:\n            fileName = \'\'\n            outDir = self.output[\'-dir\']\n            fileName = self.config[\'recommender\'] + \'@\' + currentTime + \'-top-\' + str(\n            N) + \'items\' + self.foldInfo + \'.txt\'\n            FileIO.writeFile(outDir, fileName, res)\n            print \'The result has been output to \', abspath(outDir), \'.\'\n        # output evaluation result\n        outDir = self.output[\'-dir\']\n        fileName = self.config[\'recommender\'] + \'@\' + currentTime + \'-measure\' + self.foldInfo + \'.txt\'\n        self.measure = Measure.rankingMeasure(self.data.testSet_u, recList, top)\n        FileIO.writeFile(outDir, fileName, self.measure)\n        print \'The result of %s %s:\\n%s\' % (self.algorName, self.foldInfo, \'\'.join(self.measure))\n\n    def execute(self):\n        self.readConfiguration()\n        if self.foldInfo == \'[1]\':\n            self.printAlgorConfig()\n        #load model from disk or build model\n        if self.isLoadModel:\n            print \'Loading model %s...\' %self.foldInfo\n            self.loadModel()\n        else:\n            print \'Initializing model %s...\' %self.foldInfo\n            self.initModel()\n            print \'Building Model %s...\' %self.foldInfo\n            try:\n                import tensorflow\n                if self.evalSettings.contains(\'-tf\'):\n                    self.buildModel_tf()\n                else:\n                    self.buildModel()\n            except ImportError:\n                self.buildModel()\n\n        #preict the ratings or item ranking\n        print \'Predicting %s...\' %self.foldInfo\n        if self.ranking.isMainOn():\n            self.evalRanking()\n        else:\n            self.evalRatings()\n\n        #save model\n        if self.isSaveModel:\n            print \'Saving model %s...\' %self.foldInfo\n            self.saveModel()\n        # with open(self.foldInfo+\'measure.txt\',\'w\') as f:\n        #     f.writelines(self.record)\n        return self.measure\n\n\n\n'"
baseclass/SocialRecommender.py,0,"b""from baseclass.IterativeRecommender import IterativeRecommender\nfrom data.social import SocialDAO\nfrom tool import config\nfrom os.path import abspath\nclass SocialRecommender(IterativeRecommender):\n    def __init__(self,conf,trainingSet,testSet,relation,fold='[1]'):\n        super(SocialRecommender, self).__init__(conf,trainingSet,testSet,fold)\n        self.social = SocialDAO(self.config,relation) #social relations access control\n\n        # data clean\n        cleanList = []\n        cleanPair = []\n        for user in self.social.followees:\n            if not self.data.user.has_key(user):\n                cleanList.append(user)\n            for u2 in self.social.followees[user]:\n                if not self.data.user.has_key(u2):\n                    cleanPair.append((user, u2))\n        for u in cleanList:\n            del self.social.followees[u]\n\n        for pair in cleanPair:\n            if self.social.followees.has_key(pair[0]):\n                del self.social.followees[pair[0]][pair[1]]\n\n        cleanList = []\n        cleanPair = []\n        for user in self.social.followers:\n            if not self.data.user.has_key(user):\n                cleanList.append(user)\n            for u2 in self.social.followers[user]:\n                if not self.data.user.has_key(u2):\n                    cleanPair.append((user, u2))\n        for u in cleanList:\n            del self.social.followers[u]\n\n        for pair in cleanPair:\n            if self.social.followers.has_key(pair[0]):\n                del self.social.followers[pair[0]][pair[1]]\n\n        idx = []\n        for n,pair in enumerate(self.social.relation):\n            if pair[0] not in self.data.user or pair[1] not in self.data.user:\n                idx.append(n)\n\n        for item in reversed(idx):\n            del self.social.relation[item]\n\n    def readConfiguration(self):\n        super(SocialRecommender, self).readConfiguration()\n        regular = config.LineConfig(self.config['reg.lambda'])\n        self.regS = float(regular['-s'])\n\n    def printAlgorConfig(self):\n        super(SocialRecommender, self).printAlgorConfig()\n        print 'Social dataset:',abspath(self.config['social'])\n        print 'Social size ','(User count:',len(self.social.user),'Trust statement count:'+str(len(self.social.relation))+')'\n        print 'Social Regularization parameter: regS %.3f' % (self.regS)\n        print '=' * 80\n\n"""
baseclass/__init__.py,0,b''
data/__init__.py,0,b''
data/rating.py,0,"b""import numpy as np\nfrom structure import sparseMatrix,new_sparseMatrix\nfrom tool.config import Config,LineConfig\nfrom tool.qmath import normalize\nfrom evaluation.dataSplit import DataSplit\nimport os.path\nfrom re import split\nfrom collections import defaultdict\nclass RatingDAO(object):\n    'data access control'\n    def __init__(self,config,trainingSet, testSet):\n        self.config = config\n        self.ratingConfig = LineConfig(config['ratings.setup'])\n        self.user = {} #used to store the order of users in the training set\n        self.item = {} #used to store the order of items in the training set\n        self.id2user = {}\n        self.id2item = {}\n        self.all_Item = {}\n        self.all_User = {}\n        self.userMeans = {} #used to store the mean values of users's ratings\n        self.itemMeans = {} #used to store the mean values of items's ratings\n        self.globalMean = 0\n        self.timestamp = {}\n        self.trainSet_u = defaultdict(dict)\n        self.trainSet_i = defaultdict(dict)\n        self.testSet_u = defaultdict(dict) # used to store the test set by hierarchy user:[item,rating]\n        self.testSet_i = defaultdict(dict) # used to store the test set by hierarchy item:[user,rating]\n        self.rScale = []\n\n        self.trainingData = trainingSet[:]\n        self.testData = testSet[:]\n\n        self.__generateSet()\n\n        self.__computeItemMean()\n        self.__computeUserMean()\n        self.__globalAverage()\n\n\n\n    def __generateSet(self):\n        triple = []\n        scale = set()\n        # find the maximum rating and minimum value\n\n        for i,entry in enumerate(self.trainingData):\n            userName,itemName,rating = entry\n            # makes the rating within the range [0, 1].\n            #rating = normalize(float(rating), self.rScale[-1], self.rScale[0])\n            #self.trainingData[i][2] = rating\n            # order the user\n            if userName not in self.user:\n                self.user[userName] = len(self.user)\n                self.id2user[self.user[userName]] = userName\n            # order the item\n            if itemName not in self.item:\n                self.item[itemName] = len(self.item)\n                self.id2item[self.item[itemName]] = itemName\n                # userList.append\n            self.trainSet_u[userName][itemName] = rating\n            self.trainSet_i[itemName][userName] = rating\n            scale.add(float(rating))\n        self.rScale = list(scale)\n        self.rScale.sort()\n\n        self.all_User.update(self.user)\n        self.all_Item.update(self.item)\n        for entry in self.testData:\n            userName, itemName, rating = entry\n            # order the user\n            if userName not in self.user:\n                self.all_User[userName] = len(self.all_User)\n            # order the item\n            if itemName not in self.item:\n                self.all_Item[itemName] = len(self.all_Item)\n\n            self.testSet_u[userName][itemName] = rating\n            self.testSet_i[itemName][userName] = rating\n\n\n\n\n    def __globalAverage(self):\n        total = sum(self.userMeans.values())\n        if total==0:\n            self.globalMean = 0\n        else:\n            self.globalMean = total/len(self.userMeans)\n\n    def __computeUserMean(self):\n        for u in self.user:\n            # n = self.row(u) > 0\n            # mean = 0\n            #\n            # if not self.containsUser(u):  # no data about current user in training set\n            #     pass\n            # else:\n            #     sum = float(self.row(u)[0].sum())\n            #     try:\n            #         mean =  sum/ n[0].sum()\n            #     except ZeroDivisionError:\n            #         mean = 0\n            self.userMeans[u] = sum(self.trainSet_u[u].values())/float(len(self.trainSet_u[u]))\n\n    def __computeItemMean(self):\n        for c in self.item:\n            self.itemMeans[c] = sum(self.trainSet_i[c].values()) / float(len(self.trainSet_i[c]))\n\n    def getUserId(self,u):\n        if u in self.user:\n            return self.user[u]\n\n    def getItemId(self,i):\n        if i in self.item:\n            return self.item[i]\n\n    def trainingSize(self):\n        return (len(self.user),len(self.item),len(self.trainingData))\n\n    def testSize(self):\n        return (len(self.testSet_u),len(self.testSet_i),len(self.testData))\n\n    def contains(self,u,i):\n        'whether user u rated item i'\n        if u in self.user and i in self.trainSet_u[u]:\n            return True\n        else:\n            return False\n\n\n    def containsUser(self,u):\n        'whether user is in training set'\n        if u in self.user:\n            return True\n        else:\n            return False\n\n    def containsItem(self,i):\n        'whether item is in training set'\n        if i in self.item:\n            return True\n        else:\n            return False\n\n    def userRated(self,u):\n        return self.trainSet_u[u].keys(),self.trainSet_u[u].values()\n\n    def itemRated(self,i):\n        return self.trainSet_i[i].keys(),self.trainSet_i[i].values()\n\n    def row(self,u):\n        k,v = self.userRated(u)\n        vec = np.zeros(len(self.item))\n        #print vec\n        for pair in zip(k,v):\n            iid = self.item[pair[0]]\n            vec[iid]=pair[1]\n        return vec\n\n    def col(self,i):\n        k,v = self.itemRated(i)\n        vec = np.zeros(len(self.user))\n        #print vec\n        for pair in zip(k,v):\n            uid = self.user[pair[0]]\n            vec[uid]=pair[1]\n        return vec\n\n    def matrix(self):\n        m = np.zeros((len(self.user),len(self.item)))\n        for u in self.user:\n            k, v = self.userRated(u)\n            vec = np.zeros(len(self.item))\n            # print vec\n            for pair in zip(k, v):\n                iid = self.item[pair[0]]\n                vec[iid] = pair[1]\n            m[self.user[u]]=vec\n        return m\n    # def row(self,u):\n    #     return self.trainingMatrix.row(self.getUserId(u))\n    #\n    # def col(self,c):\n    #     return self.trainingMatrix.col(self.getItemId(c))\n\n    def sRow(self,u):\n        return self.trainSet_u[u]\n\n    def sCol(self,c):\n        return self.trainSet_i[c]\n\n    def rating(self,u,c):\n        if self.contains(u,c):\n            return self.trainSet_u[u][c]\n        return -1\n\n    def ratingScale(self):\n        return (self.rScale[0],self.rScale[1])\n\n    def elemCount(self):\n        return len(self.trainingData)\n"""
data/social.py,0,"b""import numpy as np\nfrom structure import sparseMatrix,new_sparseMatrix\nfrom collections import defaultdict\n\nclass SocialDAO(object):\n    def __init__(self,conf,relation=None):\n        self.config = conf\n        self.user = {} #used to store the order of users\n        self.relation = relation\n        self.followees = defaultdict(dict)\n        self.followers = defaultdict(dict)\n        self.trustMatrix = self.__generateSet()\n\n    def __generateSet(self):\n        triple = []\n        for line in self.relation:\n            userId1,userId2,weight = line\n            #add relations to dict\n            self.followees[userId1][userId2] = weight\n            self.followers[userId2][userId1] = weight\n            # order the user\n            if userId1 not in self.user:\n                self.user[userId1] = len(self.user)\n            if userId2 not in self.user:\n                self.user[userId2] = len(self.user)\n            triple.append([self.user[userId1], self.user[userId2], weight])\n        return new_sparseMatrix.SparseMatrix(triple)\n\n    def row(self,u):\n        #return user u's followees\n        return self.trustMatrix.row(self.user[u])\n\n    def col(self,u):\n        #return user u's followers\n        return self.trustMatrix.col(self.user[u])\n\n    def elem(self,u1,u2):\n        return self.trustMatrix.elem(u1,u2)\n\n    def weight(self,u1,u2):\n        if u1 in self.followees and u2 in self.followees[u1]:\n            return self.followees[u1][u2]\n        else:\n            return 0\n\n    def trustSize(self):\n        return self.trustMatrix.size\n\n    def getFollowers(self,u):\n        if self.followers.has_key(u):\n            return self.followers[u]\n        else:\n            return {}\n\n    def getFollowees(self,u):\n        if self.followees.has_key(u):\n            return self.followees[u]\n        else:\n            return {}\n\n    def hasFollowee(self,u1,u2):\n        if u1 in self.followees:\n            if u2 in self.followees[u1]:\n                return True\n            else:\n                return False\n        return False\n\n    def hasFollower(self,u1,u2):\n        if u1 in self.followers:\n            if u2 in self.followers[u1]:\n                return True\n            else:\n                return False\n        return False\n"""
evaluation/__init__.py,0,b''
evaluation/dataSplit.py,0,"b""from random import random\nfrom tool.file import FileIO\nclass DataSplit(object):\n\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def dataSplit(data,test_ratio = 0.3,output=False,path='./',order=1,binarized = False):\n        if test_ratio>=1 or test_ratio <=0:\n            test_ratio = 0.3\n        testSet = []\n        trainingSet = []\n        for entry in data:\n            if random() < test_ratio:\n                if binarized:\n                    if entry[2]:\n                        testSet.append(entry)\n                else:\n                    testSet.append(entry)\n            else:\n                trainingSet.append(entry)\n\n        if output:\n            FileIO.writeFile(path,'testSet['+str(order)+']',testSet)\n            FileIO.writeFile(path, 'trainingSet[' + str(order) + ']', trainingSet)\n        return trainingSet,testSet\n\n    @staticmethod\n    def crossValidation(data,k,output=False,path='./',order=1,binarized=False):\n        if k<=1 or k>10:\n            k=3\n        for i in range(k):\n            trainingSet = []\n            testSet = []\n            for ind,line in enumerate(data):\n                if ind%k == i:\n                    if binarized:\n                        if line[2]:\n                            testSet.append(line[:])\n                    else:\n                        testSet.append(line[:])\n                else:\n                    trainingSet.append(line[:])\n            yield trainingSet,testSet\n\n\n"""
evaluation/measure.py,0,"b""import math\nclass Measure(object):\n    def __init__(self):\n        pass\n    @staticmethod\n    def ratingMeasure(res):\n        measure = []\n        mae = Measure.MAE(res)\n        measure.append('MAE:'+str(mae)+'\\n')\n        rmse = Measure.RMSE(res)\n        measure.append('RMSE:' + str(rmse)+'\\n')\n\n        return measure\n\n    @staticmethod\n    def hits(origin, res):\n        hitCount = {}\n        for user in origin:\n            items = origin[user].keys()\n            predicted = [item[0] for item in res[user]]\n            hitCount[user] = len(set(items).intersection(set(predicted)))\n        return hitCount\n\n    @staticmethod\n    def rankingMeasure(origin, res, N):\n        measure = []\n        for n in N:\n            predicted = {}\n            for user in res:\n                predicted[user] = res[user][:n]\n            indicators = []\n            if len(origin) != len(predicted):\n                print 'The Lengths of test set and predicted set are not match!'\n                exit(-1)\n            hits = Measure.hits(origin, predicted)\n            prec = Measure.precision(hits, n)\n            indicators.append('Precision:' + str(prec) + '\\n')\n            recall = Measure.recall(hits, origin)\n            indicators.append('Recall:' + str(recall) + '\\n')\n            F1 = Measure.F1(prec, recall)\n            indicators.append('F1:' + str(F1) + '\\n')\n            MAP = Measure.MAP(origin, predicted, n)\n            indicators.append('MAP:' + str(MAP) + '\\n')\n            NDCG = Measure.NDCG(origin, predicted, n)\n            indicators.append('NDCG:' + str(NDCG) + '\\n')\n            # AUC = Measure.AUC(origin,res,rawRes)\n            # measure.append('AUC:' + str(AUC) + '\\n')\n            measure.append('Top ' + str(n) + '\\n')\n            measure += indicators\n        return measure\n\n    @staticmethod\n    def precision(hits, N):\n        prec = sum([hits[user] for user in hits])\n        return float(prec) / (len(hits) * N)\n\n    @staticmethod\n    def MAP(origin, res, N):\n        sum_prec = 0\n        for user in res:\n            hits = 0\n            precision = 0\n            for n, item in enumerate(res[user]):\n                if origin[user].has_key(item[0]):\n                    hits += 1\n                    precision += hits / (n + 1.0)\n            sum_prec += precision / (min(len(origin[user]), N) + 0.0)\n        return sum_prec / (len(res))\n\n    @staticmethod\n    def NDCG(origin,res,N):\n        sum_NDCG = 0\n        for user in res:\n            DCG = 0\n            IDCG = 0\n            #1 = related, 0 = unrelated\n            for n, item in enumerate(res[user]):\n                if origin[user].has_key(item[0]):\n                    DCG+= 1.0/math.log(n+2)\n            for n, item in enumerate(origin[user].keys()[:N]):\n                IDCG+=1.0/math.log(n+2)\n            sum_NDCG += DCG / IDCG\n        return sum_NDCG / (len(res))\n    # @staticmethod\n    # def AUC(origin, res, rawRes):\n    #\n    #     from random import choice\n    #     sum_AUC = 0\n    #     for user in origin:\n    #         count = 0\n    #         larger = 0\n    #         itemList = rawRes[user].keys()\n    #         for item in origin[user]:\n    #             item2 = choice(itemList)\n    #             count += 1\n    #             try:\n    #                 if rawRes[user][item] > rawRes[user][item2]:\n    #                     larger += 1\n    #             except KeyError:\n    #                 count -= 1\n    #         if count:\n    #             sum_AUC += float(larger) / count\n    #\n    #     return float(sum_AUC) / len(origin)\n\n    @staticmethod\n    def recall(hits, origin):\n        recallList = [float(hits[user]) / len(origin[user]) for user in hits]\n        recall = sum(recallList) / float(len(recallList))\n        return recall\n\n    @staticmethod\n    def F1(prec, recall):\n        if (prec + recall) != 0:\n            return 2 * prec * recall / (prec + recall)\n        else:\n            return 0\n\n    @staticmethod\n    def MAE(res):\n        error = 0\n        count = 0\n        for entry in res:\n            error+=abs(entry[2]-entry[3])\n            count+=1\n        if count==0:\n            return error\n        return float(error)/count\n\n    @staticmethod\n    def RMSE(res):\n        error = 0\n        count = 0\n        for entry in res:\n            error += (entry[2] - entry[3])**2\n            count += 1\n        if count==0:\n            return error\n        return math.sqrt(float(error)/count)\n\n\n\n\n"""
main/RecQ.py,0,"b'import sys\nfrom re import split\nfrom tool.config import Config,LineConfig\nfrom tool.file import FileIO\nfrom evaluation.dataSplit import *\nfrom multiprocessing import Process,Manager\nfrom tool.file import FileIO\nfrom time import strftime,localtime,time\nimport mkl\nclass RecQ(object):\n    def __init__(self,config):\n        self.trainingData = []  # training data\n        self.testData = []  # testData\n        self.relation = []\n        self.measure = []\n        self.config =config\n        self.ratingConfig = LineConfig(config[\'ratings.setup\'])\n        if self.config.contains(\'evaluation.setup\'):\n            self.evaluation = LineConfig(config[\'evaluation.setup\'])\n            binarized = False\n            bottom = 0\n            if self.evaluation.contains(\'-b\'):\n                binarized = True\n                bottom = float(self.evaluation[\'-b\'])\n            if self.evaluation.contains(\'-testSet\'):\n                #specify testSet\n\n                self.trainingData = FileIO.loadDataSet(config, config[\'ratings\'],binarized=binarized,threshold=bottom)\n                self.testData = FileIO.loadDataSet(config, self.evaluation[\'-testSet\'], bTest=True,binarized=binarized,threshold=bottom)\n\n            elif self.evaluation.contains(\'-ap\'):\n                #auto partition\n\n                self.trainingData = FileIO.loadDataSet(config,config[\'ratings\'],binarized=binarized,threshold=bottom)\n                self.trainingData,self.testData = DataSplit.\\\n                    dataSplit(self.trainingData,test_ratio=float(self.evaluation[\'-ap\']),binarized=binarized)\n            elif self.evaluation.contains(\'-cv\'):\n                #cross validation\n                self.trainingData = FileIO.loadDataSet(config, config[\'ratings\'],binarized=binarized,threshold=bottom)\n                #self.trainingData,self.testData = DataSplit.crossValidation(self.trainingData,int(self.evaluation[\'-cv\']))\n\n        else:\n            print \'Evaluation is not well configured!\'\n            exit(-1)\n\n        if config.contains(\'social\'):\n            self.socialConfig = LineConfig(self.config[\'social.setup\'])\n            self.relation = FileIO.loadRelationship(config,self.config[\'social\'])\n\n        print \'preprocessing...\'\n\n\n\n\n\n\n    def execute(self):\n        #import the algorithm module\n        try:\n            importStr = \'from algorithm.rating.\' + self.config[\'recommender\'] + \' import \' + self.config[\'recommender\']\n            exec (importStr)\n        except ImportError:\n            importStr = \'from algorithm.ranking.\' + self.config[\'recommender\'] + \' import \' + self.config[\'recommender\']\n            exec (importStr)\n        if self.evaluation.contains(\'-cv\'):\n            k = int(self.evaluation[\'-cv\'])\n            if k <= 1 or k > 10:\n                k = 3\n            mkl.set_num_threads(max(1,mkl.get_max_threads()/k))\n            #create the manager\n            manager = Manager()\n            m = manager.dict()\n            i = 1\n            tasks = []\n\n            binarized = False\n            if self.evaluation.contains(\'-b\'):\n                binarized = True\n\n            for train,test in DataSplit.crossValidation(self.trainingData,k,binarized=binarized):\n                fold = \'[\'+str(i)+\']\'\n                if self.config.contains(\'social\'):\n                    recommender = self.config[\'recommender\'] + ""(self.config,train,test,self.relation,fold)""\n                else:\n                    recommender = self.config[\'recommender\']+ ""(self.config,train,test,fold)""\n               #create the process\n                p = Process(target=run,args=(m,eval(recommender),i))\n                tasks.append(p)\n                i+=1\n            #start the processes\n            for p in tasks:\n                p.start()\n                if not self.evaluation.contains(\'-p\'):\n                    p.join()\n            #wait until all processes are completed\n            if self.evaluation.contains(\'-p\'):\n                for p in tasks:\n                    p.join()\n            #compute the mean error of k-fold cross validation\n            self.measure = [dict(m)[i] for i in range(1,k+1)]\n            res = []\n            for i in range(len(self.measure[0])):\n                if self.measure[0][i][:3] == \'Top\':\n                    res.append(self.measure[0][i])\n                    continue\n                measure = self.measure[0][i].split(\':\')[0]\n                total = 0\n                for j in range(k):\n                    total += float(self.measure[j][i].split(\':\')[1])\n                res.append(measure + \':\' + str(total / k) + \'\\n\')\n            #output result\n            currentTime = strftime(""%Y-%m-%d %H-%M-%S"", localtime(time()))\n            outDir = LineConfig(self.config[\'output.setup\'])[\'-dir\']\n            fileName = self.config[\'recommender\'] +\'@\'+currentTime+\'-\'+str(k)+\'-fold-cv\' + \'.txt\'\n            FileIO.writeFile(outDir,fileName,res)\n            print \'The result of %d-fold cross validation:\\n%s\' %(k,\'\'.join(res))\n\n\n        else:\n            if self.config.contains(\'social\'):\n                recommender = self.config[\'recommender\']+\'(self.config,self.trainingData,self.testData,self.relation)\'\n            else:\n                recommender = self.config[\'recommender\'] + \'(self.config,self.trainingData,self.testData)\'\n            eval(recommender).execute()\n\n\ndef run(measure,algor,order):\n    measure[order] = algor.execute()'"
main/__init__.py,0,b''
main/main.py,0,"b'import sys\nsys.path.append("".."")\nfrom RecQ import RecQ\nfrom tool.config import Config\nfrom visual.display import Display\n\n\n\nif __name__ == \'__main__\':\n\n    print \'=\'*80\n    print \'   RecQ: An effective python-based recommender algorithm library.   \'\n    print \'=\'*80\n    print \'0. Analyze the input data.(Configure the visual.conf in config/visual first.)\'\n    print \'-\' * 80\n    print \'Generic Recommenders:\'\n    print \'1. UserKNN        2. ItemKNN        3. BasicMF        4. SlopeOne        5. SVD\'\n    print \'6. PMF            7. SVD++          8. EE             9. BPR             10. WRMF\'\n    print \'11. ExpoMF\'\n\n    print \'Social Recommenders:\'\n    print \'s1. RSTE          s2. SoRec         s3. SoReg         s4. SocialMF       s5. SBPR\'\n    print \'s6. SREE          s7. LOCABAL       s8. SocialFD      s9. TBPR           s10. SERec\'\n\n    print \'Network Embedding based Recommenders:\'\n    print \'a1. CoFactor      a2. CUNE-MF       a3. CUNE-BPR      a4. IF-BPR\'\n\n    print \'Deep Recommenders:\'\n    print \'d1. APR           d2. CDAE          d3. DMF           d4. NeuMF           d5. CFGAN\'\n    print \'d6. IRGAN         d7. RSGAN         d8. NGCF          d9. AGR\'\n\n    print \'Baselines:\'\n    print \'b1. UserMean      b2. ItemMean      b3. MostPopular   b4. Rand\'\n\n\n    print \'=\'*80\n    algor = -1\n    conf = -1\n    order = raw_input(\'please enter the num of the algorithm to run it:\')\n    import time\n    s = time.time()\n    if order == \'0\':\n        try:\n            import seaborn as sns\n        except ImportError:\n            print \'!!!To obtain nice data charts, \' \\\n                  \'we strongly recommend you to install the third-party package <seaborn>!!!\'\n        conf = Config(\'../config/visual/visual.conf\')\n        Display(conf).render()\n        exit(0)\n\n    algorthms = {\'1\':\'UserKNN\',\'2\':\'ItemKNN\',\'3\':\'BasicMF\',\'4\':\'SlopeOne\',\'5\':\'SVD\',\'6\':\'PMF\',\n                 \'7\':\'SVD++\',\'8\':\'EE\',\'9\':\'BPR\',\'10\':\'WRMF\',\'11\':\'ExpoMF\',\n                 \'s1\':\'RSTE\',\'s2\':\'SoRec\',\'s3\':\'SoReg\',\'s4\':\'SocialMF\',\'s5\':\'SBPR\',\'s6\':\'SREE\',\n                 \'s7\':\'LOCABAL\',\'s8\':\'SocialFD\',\'s9\':\'TBPR\',\'s10\':\'SEREC\',\'a1\':\'CoFactor\',\n                 \'a2\':\'CUNE_MF\',\'a3\':\'CUNE_BPR\',\'a4\':\'IF_BPR\',\n                 \'d1\':\'APR\',\'d2\':\'CDAE\',\'d3\':\'DMF\',\'d4\':\'NeuMF\',\'d5\':\'CFGAN\',\'d6\':\'IRGAN\',\'d7\':\'RSGAN\',\'d8\':\'NGCF\',\n                 \'d9\':\'AGR\',\n                 \'b1\':\'UserMean\',\'b2\':\'ItemMean\',\'b3\':\'MostPopular\',\'b4\':\'Rand\'}\n\n    try:\n        conf = Config(\'../config/\'+algorthms[order]+\'.conf\')\n    except KeyError:\n        print \'Error num!\'\n        exit(-1)\n    recSys = RecQ(conf)\n    recSys.execute()\n    e = time.time()\n    print ""Run time: %f s"" % (e - s)\n'"
structure/__init__.py,0,b''
structure/new_sparseMatrix.py,0,"b""import numpy as np\n#class Triple(object):\n\n\nclass SparseMatrix():\n    'matrix used to store raw data'\n    def __init__(self,triple):\n        self.matrix_User = {}\n        self.matrix_Item = {}\n        for item in triple:\n            if not self.matrix_User.has_key(item[0]):\n                self.matrix_User[item[0]] = {}\n            if not self.matrix_Item.has_key(item[1]):\n                self.matrix_Item[item[1]] = {}\n            self.matrix_User[item[0]][item[1]]=item[2]\n            self.matrix_Item[item[1]][item[0]]=item[2]\n        self.elemNum = len(triple)\n        self.size = (len(self.matrix_User),len(self.matrix_Item))\n\n    def sRow(self,r):\n        if not self.matrix_User.has_key(r):\n            return {}\n        else:\n            return self.matrix_User[r]\n\n    def sCol(self,c):\n        if not self.matrix_Item.has_key(c):\n            return {}\n        else:\n            return self.matrix_Item[c]\n\n\n\n    def row(self,r):\n        if not self.matrix_User.has_key(r):\n            return np.zeros((1,self.size[1]))\n        else:\n            array = np.zeros((1,self.size[1]))\n            ind = self.matrix_User[r].keys()\n            val = self.matrix_User[r].values()\n            array[0][ind] = val\n            return array\n\n    def col(self,c):\n        if not self.matrix_Item.has_key(c):\n            return np.zeros((1,self.size[0]))\n        else:\n            array = np.zeros((1,self.size[0]))\n            ind = self.matrix_Item[c].keys()\n            val = self.matrix_Item[c].values()\n            array[0][ind] = val\n            return array\n    def elem(self,r,c):\n        if not self.contains(r,c):\n            return 0\n        return self.matrix_User[r][c]\n\n    def contains(self,r,c):\n        if self.matrix_User.has_key(r) and self.matrix_User[r].has_key(c):\n            return True\n        return False\n\n\n    def elemCount(self):\n        return self.elemNum\n\n    def size(self):\n        return self.size\n    # def sRow(self,r):\n    #     'return the sparse row'\n    #     return self.matrix.getrow(r)\n    # def sCol(self,c):\n    #     'return the sparse column'\n    #     return self.matrix.getcol(c)\n    # def toDense(self):\n    #     return self.matrix.todense()\n\n\n"""
structure/sparseMatrix.py,0,"b""from scipy.sparse import csr_matrix\nimport numpy as np\n\n\nclass SparseMatrix():\n    'matrix used to store raw data'\n    def __init__(self,data,indices,indptr,shape=None):\n        self.matrix = csr_matrix((data,indices,indptr),shape)\n        self.shape = self.matrix.shape\n\n    def row(self,r):\n        if r >= self.shape[0]:\n            return np.zeros((1,self.shape[1]))\n        return self.matrix.getrow(r).toarray()\n    def col(self,c):\n        if c >= self.shape[1]:\n            return np.zeros((1, self.shape[0]))\n        return self.matrix.getcol(c).toarray().transpose()\n    def elem(self,r,c):\n        if r >= self.shape[0] or c >= self.shape[1]:\n            return 0\n        return self.matrix.getrow(r).toarray()[0][c]\n    def sRow(self,r):\n        'return the sparse row'\n        return self.matrix.getrow(r)\n    def sCol(self,c):\n        'return the sparse column'\n        return self.matrix.getcol(c)\n    def toDense(self):\n        return self.matrix.todense()\n\n\n"""
structure/symmetricMatrix.py,0,"b""import numpy as np\n\nclass SymmetricMatrix(object):\n    def __init__(self, shape):\n        self.symMatrix = {}\n        self.shape = (shape,shape)\n\n    def __getitem__(self, item):\n        if self.symMatrix.has_key(item):\n            return self.symMatrix[item]\n        return {}\n\n    def set(self,i,j,val):\n        if not self.symMatrix.has_key(i):\n            self.symMatrix[i] = {}\n        self.symMatrix[i][j]=val\n        if not self.symMatrix.has_key(j):\n            self.symMatrix[j] = {}\n        self.symMatrix[j][i] = val\n\n\n    def get(self,i,j):\n        if not self.symMatrix.has_key(i) or not self.symMatrix[i].has_key(j):\n            return 0\n        return self.symMatrix[i][j]\n\n    def contains(self,i,j):\n        if self.symMatrix.has_key(i) and self.symMatrix[i].has_key(j):\n            return True\n        else:\n            return False\n\n    # def row(self, r):\n    #     if not self.symMatrix.has_key(r):\n    #         return np.zeros((1, self.shape[1]))\n    #     else:\n    #         array = np.zeros((1, self.shape[1]))\n    #         for item in self.symMatrix[r]:\n    #             array[0][item] = self.symMatrix[r][item]\n    #         return array\n    #\n    #\n    # def col(self, c):\n    #     return self.row(c)\n    #\n    #\n    # def elem(self, r, c):\n    #     self.get(r,c)\n        # def sRow(self,r):\n        #     'return the sparse row'\n        #     return self.matrix.getrow(r)\n        # def sCol(self,c):\n        #     'return the sparse column'\n        #     return self.matrix.getcol(c)\n        # def toDense(self):\n        #     return self.matrix.todense()"""
tool/__init__.py,0,b''
tool/config.py,0,"b""import os.path\nclass Config(object):\n    def __init__(self,fileName):\n        self.config = {}\n        self.readConfiguration(fileName)\n\n    def __getitem__(self, item):\n        if not self.contains(item):\n            print 'parameter '+item+' is invalid!'\n            exit(-1)\n        return self.config[item]\n\n    def getOptions(self,item):\n        if not self.contains(item):\n            print 'parameter '+item+' is invalid!'\n            exit(-1)\n        return self.config[item]\n\n    def contains(self,key):\n        return self.config.has_key(key)\n\n    def readConfiguration(self,fileName):\n        path = '../config/'+fileName\n        if not os.path.exists(path):\n            print 'config file is not found!'\n            raise IOError\n        with open(path) as f:\n            for ind,line in enumerate(f):\n                if line.strip()<>'':\n                    try:\n                        key,value=line.strip().split('=')\n                        self.config[key]=value\n                    except ValueError:\n                        print 'config file is not in the correct format! Error Line:%d'%(ind)\n\n\n\nclass LineConfig(object):\n    def __init__(self,content):\n        self.line = content.strip().split(' ')\n        self.options = {}\n        self.mainOption = False\n        if self.line[0] == 'on':\n            self.mainOption = True\n        elif self.line[0] == 'off':\n            self.mainOption = False\n        for i,item in enumerate(self.line):\n            if (item.startswith('-') or item.startswith('--')) and  not item[1:].isdigit():\n                ind = i+1\n                for j,sub in enumerate(self.line[ind:]):\n                    if (sub.startswith('-') or sub.startswith('--')) and  not sub[1:].isdigit():\n                        ind = j\n                        break\n                    if j == len(self.line[ind:])-1:\n                        ind=j+1\n                        break\n                try:\n                    self.options[item] = ' '.join(self.line[i+1:i+1+ind])\n                except IndexError:\n                    self.options[item] = 1\n\n\n    def __getitem__(self, item):\n        if not self.contains(item):\n            print 'parameter '+item+' is invalid!'\n            exit(-1)\n        return self.options[item]\n\n    def getOption(self,key):\n        if not self.contains(key):\n            print 'parameter '+key+' is invalid!'\n            exit(-1)\n        return self.options[key]\n\n    def isMainOn(self):\n        return self.mainOption\n\n    def contains(self,key):\n        return self.options.has_key(key)\n\n\n"""
tool/file.py,0,"b""import os.path\nfrom os import makedirs,remove\nfrom re import compile,findall,split\nfrom config import LineConfig\nclass FileIO(object):\n    def __init__(self):\n        pass\n\n    # @staticmethod\n    # def writeFile(filePath,content,op = 'w'):\n    #     reg = compile('(.+[/|\\\\\\]).+')\n    #     dirs = findall(reg,filePath)\n    #     if not os.path.exists(filePath):\n    #         os.makedirs(dirs[0])\n    #     with open(filePath,op) as f:\n    #         f.write(str(content))\n\n    @staticmethod\n    def writeFile(dir,file,content,op = 'w'):\n        if not os.path.exists(dir):\n            os.makedirs(dir)\n        with open(dir+file,op) as f:\n            f.writelines(content)\n\n    @staticmethod\n    def deleteFile(filePath):\n        if os.path.exists(filePath):\n            remove(filePath)\n\n    @staticmethod\n    def loadDataSet(conf, file, bTest=False,binarized = False, threshold = 3.0):\n        trainingData = []\n        testData = []\n        ratingConfig = LineConfig(conf['ratings.setup'])\n        if not bTest:\n            print 'loading training data...'\n        else:\n            print 'loading test data...'\n        with open(file) as f:\n            ratings = f.readlines()\n        # ignore the headline\n        if ratingConfig.contains('-header'):\n            ratings = ratings[1:]\n        # order of the columns\n        order = ratingConfig['-columns'].strip().split()\n        delim = ' |,|\\t'\n        if ratingConfig.contains('-delim'):\n            delim=ratingConfig['-delim']\n        for lineNo, line in enumerate(ratings):\n            items = split(delim,line.strip())\n            if not bTest and len(order) < 2:\n                print 'The rating file is not in a correct format. Error: Line num %d' % lineNo\n                exit(-1)\n            try:\n                userId = items[int(order[0])]\n                itemId = items[int(order[1])]\n                if len(order)<3:\n                    rating = 1 #default value\n                else:\n                    rating  = items[int(order[2])]\n                if binarized:\n                    if float(items[int(order[2])])<threshold:\n                        continue\n                    else:\n                        rating = 1\n            except ValueError:\n                print 'Error! Have you added the option -header to the rating.setup?'\n                exit(-1)\n            if not bTest:\n                trainingData.append([userId, itemId, float(rating)])\n            else:\n                if binarized:\n                    if rating==1:\n                        testData.append([userId, itemId, float(rating)])\n                    else:\n                        continue\n                testData.append([userId, itemId, float(rating)])\n        if not bTest:\n            return trainingData\n        else:\n            return testData\n\n    @staticmethod\n    def loadRelationship(conf, filePath):\n        socialConfig = LineConfig(conf['social.setup'])\n        relation = []\n        print 'loading social data...'\n        with open(filePath) as f:\n            relations = f.readlines()\n            # ignore the headline\n        if socialConfig.contains('-header'):\n            relations = relations[1:]\n        # order of the columns\n        order = socialConfig['-columns'].strip().split()\n        if len(order) <= 2:\n            print 'The social file is not in a correct format.'\n        for lineNo, line in enumerate(relations):\n            items = split(' |,|\\t', line.strip())\n            if len(order) < 2:\n                print 'The social file is not in a correct format. Error: Line num %d' % lineNo\n                exit(-1)\n            userId1 = items[int(order[0])]\n            userId2 = items[int(order[1])]\n            if len(order) < 3:\n                weight = 1\n            else:\n                weight = float(items[int(order[2])])\n            relation.append([userId1, userId2, weight])\n        return relation\n\n\n\n\n"""
tool/log.py,0,b''
tool/qmath.py,0,"b""from sklearn.metrics.pairwise import pairwise_distances,cosine_similarity\nimport numpy as np\nfrom numpy.linalg import norm\nfrom scipy.stats.stats import pearsonr\nfrom math import sqrt,exp\n\ndef l1(x):\n    return norm(x,ord=1)\n\ndef l2(x):\n    return norm(x)\n\ndef common(x1,x2):\n    # find common ratings\n    common = (x1<>0)&(x2<>0)\n    new_x1 = x1[common]\n    new_x2 = x2[common]\n    return new_x1,new_x2\n\ndef cosine_sp(x1,x2):\n    'x1,x2 are dicts,this version is for sparse representation'\n    total = 0\n    denom1 = 0\n    denom2 =0\n    try:\n        for k in x1:\n            if x2.has_key(k):\n                total+=x1[k]*x2[k]\n                denom1+=x1[k]**2\n                denom2+=x2[k]**2\n        return (total + 0.0) / (sqrt(denom1) * sqrt(denom2))\n    except ZeroDivisionError:\n        return 0\n\ndef euclidean_sp(x1,x2):\n    'x1,x2 are dicts,this version is for sparse representation'\n    total = 0\n    try:\n        for k in x1:\n            if x2.has_key(k):\n                total+=x1[k]**2-x2[k]**2\n        return 1/total\n    except ZeroDivisionError:\n        return 0\n\ndef cosine(x1,x2):\n    #find common ratings\n    #new_x1, new_x2 = common(x1,x2)\n    #compute the cosine similarity between two vectors\n    sum = x1.dot(x2)\n    denom = sqrt(x1.dot(x1)*x2.dot(x2))\n    try:\n        return float(sum)/denom\n    except ZeroDivisionError:\n        return 0\n\n    #return cosine_similarity(x1,x2)[0][0]\n\ndef pearson_sp(x1,x2):\n    total = 0\n    denom1 = 0\n    denom2 = 0\n    overlapped=False\n    try:\n        mean1 = sum(x1.values())/(len(x1)+0.0)\n        mean2 = sum(x2.values()) / (len(x2) + 0.0)\n        for k in x1:\n            if x2.has_key(k):\n                total += (x1[k]-mean1) * (x2[k]-mean2)\n                denom1 += (x1[k]-mean1) ** 2\n                denom2 += (x2[k]-mean2) ** 2\n                overlapped=True\n        return (total + 0.0) / (sqrt(denom1) * sqrt(denom2))\n    except ZeroDivisionError:\n        if overlapped:\n            return 1\n        return 0\n\ndef euclidean(x1,x2):\n    #find common ratings\n    new_x1, new_x2 = common(x1, x2)\n    #compute the euclidean between two vectors\n    diff = new_x1-new_x2\n    denom = sqrt((diff.dot(diff)))\n    try:\n        return 1/denom\n    except ZeroDivisionError:\n        return 0\n\n\ndef pearson(x1,x2):\n    #find common ratings\n    #new_x1, new_x2 = common(x1, x2)\n    #compute the pearson similarity between two vectors\n    #ind1 = new_x1 > 0\n    #ind2 = new_x2 > 0\n    try:\n        mean_x1 = float(x1.sum())/len(x1)\n        mean_x2 = float(x2.sum())/len(x2)\n        new_x1 = x1 - mean_x1\n        new_x2 = x2 - mean_x2\n        sum = new_x1.dot(new_x2)\n        denom = sqrt((new_x1.dot(new_x1))*(new_x2.dot(new_x2)))\n        return float(sum) / denom\n    except ZeroDivisionError:\n        return 0\n\n\ndef similarity(x1,x2,sim):\n    if sim == 'pcc':\n        return pearson_sp(x1,x2)\n    if sim == 'euclidean':\n        return euclidean_sp(x1,x2)\n    else:\n        return cosine_sp(x1, x2)\n\n\ndef normalize(vec,maxVal,minVal):\n    'get the normalized value using min-max normalization'\n    if maxVal > minVal:\n        return float(vec-minVal)/(maxVal-minVal)+0.01\n    elif maxVal==minVal:\n        return vec/maxVal\n    else:\n        print 'error... maximum value is less than minimum value.'\n        raise ArithmeticError\n\ndef sigmoid(val):\n    return 1/(1+exp(-val))\n\n\ndef denormalize(vec,maxVal,minVal):\n    return minVal+(vec-0.01)*(maxVal-minVal)\n"""
visual/__init__.py,0,b''
visual/chart.py,0,"b'import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport os.path\nclass Chart(object):\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def distribution(y, title=\'\',xLabel=\'\',yLabel=\'\',savePath=\'../visual/visualization/p1\'):\n        fig, ax1 = plt.subplots(1, 1, figsize=(8, 6), sharex=True)\n\n        #sns.set(style=""white"")\n        x = np.linspace(0, len(y), len(y))\n        y.sort(reverse = True)\n        plt.plot(x, y, color=\'green\')\n        ax1.set_xlabel(xLabel, fontsize=16)\n        ax1.set_ylabel(yLabel, fontsize=16)\n        ax1.set_xlim(0, len(y))\n        # ax1.set_ylim(0,25000)\n        ax1.set_title(title, fontsize=20)\n        ax1.tick_params(axis=\'x\', labelsize=16)\n        ax1.tick_params(axis=\'y\', labelsize=16)\n        plt.grid(True)\n        plt.savefig(savePath,bbox_inches=\'tight\')\n        #plt.show()\n        plt.close(\'all\')\n\n    @staticmethod\n    def scatter(x, y, color, title=\'\',xLabel=\'\',yLabel=\'\',savePath=\'../visual/visualization/p2\'):\n        fig, ax1 = plt.subplots(1, 1, figsize=(8, 6), sharex=True)\n\n        # sns.set(style=""white"")\n        ax1.set_ylim(-10, max(y) + 20)\n        ax1.set_xlim(-10, max(x) + 20)\n        ax1.set_title(title, fontsize=20)\n        ax1.set_xlabel(xLabel, fontsize=16)\n        ax1.set_ylabel(yLabel, fontsize=16)\n        ax1.tick_params(axis=\'x\', labelsize=16)\n        ax1.tick_params(axis=\'y\', labelsize=16)\n        plt.scatter(x, y, c=color, alpha=0.7, )\n        plt.grid(True)\n        plt.savefig(savePath,bbox_inches=\'tight\')\n        #plt.show()\n        plt.close(\'all\')\n\n    @staticmethod\n    def hist(x,y, bins, color,title=\'\',xLabel=\'\',yLabel=\'\',savePath=\'../visual/visualization/p3\'):\n        fig, ax1 = plt.subplots(1, 1, figsize=(8, 6))\n        #sns.set(style=""white"")\n\n        ax1.grid(True)\n        ax1.set_title(title, fontsize=20)\n        ax1.set_xlabel(xLabel, fontsize=16)\n        ax1.set_ylabel(yLabel, fontsize=16)\n        ax1.tick_params(axis=\'x\', labelsize=16)\n        ax1.tick_params(axis=\'y\', labelsize=16)\n        ind = np.arange(0,1,1.0/len(x))\n        ax1.set_xticks(ind+1.0/(2*len(x)))\n        ax1.set_xticklabels(x)\n        ax1.hist(y, bins, color=color)\n        plt.savefig(savePath,bbox_inches=\'tight\')\n        #plt.show()\n        plt.close(\'all\')\n'"
visual/display.py,0,"b'from chart import Chart\nfrom tool.config import Config\nfrom data.rating import RatingDAO\nfrom data.social import SocialDAO\nfrom tool.file import FileIO\nfrom tool.qmath import denormalize\nfrom os.path import abspath\nimport webbrowser\nfrom tool.file import FileIO\nclass Display(object):\n    def __init__(self,conf):\n        self.conf = conf\n        if not conf.contains(\'ratings\') and not conf.contains(\'social\'):\n            print \'The config file is not in the correct format!\'\n            exit(-1)\n        if conf.contains(\'ratings\'):\n            ratingData =  FileIO.loadDataSet(conf,conf[\'ratings\'])\n            self.dao = RatingDAO(conf,ratingData)\n        if conf.contains(\'social\'):\n            relationData = FileIO.loadRelationship(conf,conf[\'social\'])\n            self.sao = SocialDAO(conf,relationData)\n\n\n    def draw(self):\n        print \'draw chart...\'\n        #rating\n        if self.conf.contains(\'ratings\'):\n            y = [triple[2] for triple in self.dao.trainingData]\n            x = self.dao.rScale\n            if len(x) <20:\n                Chart.hist(x,y,len(self.dao.rScale),\'#058edc\',\n                       \'Rating Histogram\',\'Rating Scale\',\'Count\',\'../visual/visualization/images/rh\')\n            y = [len(self.dao.userRated(u)[0]) for u in self.dao.user]\n            Chart.distribution(y,\'Rating Count Distribution\',\'\',\n                               \'Rated items count per user\',\'../visual/visualization/images/rcu\')\n            y = [len(self.dao.itemRated(i)[0]) for i in self.dao.item]\n            Chart.distribution(y,\'Rating Count Distribution\',\'\',\n                               \'user Rated count per item\',\'../visual/visualization/images/rci\')\n\n        #social\n        if self.conf.contains(\'social\'):\n            x = [len(self.sao.getFollowers(u)) for u in self.sao.user]\n            y = [len(self.sao.getFollowees(u)) for u in self.sao.user]\n            Chart.scatter(x,y,\'red\',\'Follower&Followee\',\n                          \'Follower count\',\'Followee count\',\'../visual/visualization/images/ff\')\n            y = [len(self.sao.getFollowers(u)) for u in self.sao.user]\n            Chart.distribution(y, \'Followers Distribution\', \'\',\n                               \'Followers count per user\',\'../visual/visualization/images/fd1\')\n            y = [len(self.sao.getFollowees(u)) for u in self.sao.user]\n            Chart.distribution(y, \'Followees Distribution\', \'\',\n                           \'Followees count per user\',\'../visual/visualization/images/fd2\')\n\n    def render(self):\n        self.draw()\n        html =""<html><head><title>Data Analysis</title>\\n"" \\\n              ""<link rel=\'stylesheet\' type=\'text/css\' href=\'reportStyle.css\'/></head>\\n"" \\\n              ""<body><div class=\'reportTitle\'><div class=\'in\'>Data Analysis</div></div>\\n"" \\\n              ""<div class=\'main\'><div class=\'area1\'>\\n"" \\\n              ""<div class=\'title\'><h3>Data Files</h3></div><div class=\'text\'>""\n        if self.conf.contains(\'ratings\'):\n            html+=""<b>Rating Data</b>: {rating}"".format(rating = abspath(self.conf[\'ratings\']))\n        if self.conf.contains(\'social\'):\n            html+=""<br><b>Social Data</b>: {social}"".format(social = abspath(self.conf[\'social\']))\n        html+=""</div></div><div style=\'padding-top:20px\'><center>"" \\\n              ""<img src=\'images/header2.png\'/></center></div>\\n""\n        if self.conf.contains(\'ratings\'):\n            html+=""<div class=\'area1\'><div class=\'title\'><h3>Rating Data</h3></div>\\n""\n            html += ""<div class=\'text\'><b>Rating Scale</b>: {scale}</br>"".format(scale=\' \'.join([str(item) for item in self.dao.rScale]))\n            html += ""<b>User Count</b>: {user}<br><b>Item Count</b>: {item}<br><b>Record Count</b>: {record}<br><b>Global Mean</b>: {mean}</div>\\n""\\\n                .format(user = str(len(self.dao.user)),item=str(len(self.dao.item)),record = str(len(self.dao.trainingData)),\n                        mean = str(round(denormalize(self.dao.globalMean,self.dao.rScale[-1],self.dao.rScale[0]),3)))\n            html+=""<center><div class=\'img\'><img src=\'images/rh.png\' width=\'640px\' height=\'480px\'/></div></center>\\n""\n            html += ""<center><div class=\'img\'><img src=\'images/rcu.png\' width=\'640px\' height=\'480px\'/></div></center>\\n""\n            html += ""<center><div class=\'img\'><img src=\'images/rci.png\' width=\'640px\' height=\'480px\'/></div></center>\\n""\n            html += ""</div><div style=\'padding-top:20px\'><center>"" \\\n              ""<img src=\'images/header2.png\'/></center></div>\\n""\n        if self.conf.contains(\'social\'):\n            html += ""<div class=\'area1\'><div class=\'title\'><h3>Social Data</h3></div>\\n""\n            html += ""<div class=\'text\'><b>User Count</b>: {user}<br><b>Relation Count</b>: {relation}<br></div>\\n"" \\\n                .format(user=str(len(self.sao.user)), relation=str(len(self.sao.relation)))\n            html += ""<center><div class=\'img\'><img src=\'images/ff.png\' width=\'640px\' height=\'480px\'/></div></center>\\n""\n            html += ""<center><div class=\'img\'><img src=\'images/fd1.png\' width=\'640px\' height=\'480px\'/></div></center>\\n""\n            html += ""<center><div class=\'img\'><img src=\'images/fd2.png\' width=\'640px\' height=\'480px\'/></div></center>\\n""\n            html += ""</div><div style=\'padding-top:20px\'><center>"" \\\n                    ""<img src=\'images/header2.png\'/></center></div>\\n""\n\n        html+=""</div></body></html>""\n        FileIO.writeFile(\'../visual/visualization/\',\'analysis.html\',html)\n        print \'The report has been output to\',abspath(\'../visual/visualization/analysis.html\')\n        webbrowser.open(abspath(\'../visual/visualization/analysis.html\'), new=0, autoraise=True)\n\n'"
algorithm/ranking/AGR.py,70,"b'#coding:utf8\nfrom baseclass.DeepRecommender import DeepRecommender\nfrom baseclass.SocialRecommender import SocialRecommender\nfrom random import choice\nimport tensorflow as tf\nfrom collections import defaultdict\nimport numpy as np\nfrom math import sqrt\n\n\ndef gumbel_softmax(logits, temperature=0.2):\n    eps = 1e-20\n    u = tf.random_uniform(tf.shape(logits), minval=0, maxval=1)\n    gumbel_noise = -tf.log(-tf.log(u + eps) + eps)\n    y = tf.log(logits + eps) + gumbel_noise\n    return tf.nn.softmax(y / temperature)\n\n\nclass AGR(SocialRecommender,DeepRecommender):\n\n    def __init__(self,conf,trainingSet=None,testSet=None,relation=None,fold=\'[1]\'):\n        DeepRecommender.__init__(self, conf=conf, trainingSet=trainingSet, testSet=testSet, fold=fold)\n        SocialRecommender.__init__(self, conf=conf, trainingSet=trainingSet, testSet=testSet, relation=relation,fold=fold)\n\n    def next_batch(self):\n        batch_id = 0\n        while batch_id < self.train_size:\n            if batch_id + self.batch_size <= self.train_size:\n                users = [self.data.trainingData[idx][0] for idx in range(batch_id, self.batch_size + batch_id)]\n                items = [self.data.trainingData[idx][1] for idx in range(batch_id, self.batch_size + batch_id)]\n                batch_id += self.batch_size\n            else:\n                users = [self.data.trainingData[idx][0] for idx in range(batch_id, self.train_size)]\n                items = [self.data.trainingData[idx][1] for idx in range(batch_id, self.train_size)]\n                batch_id = self.train_size\n\n            u_idx, i_idx, j_idx = [], [], []\n            item_list = self.data.item.keys()\n            for i, user in enumerate(users):\n\n                i_idx.append(self.data.item[items[i]])\n                u_idx.append(self.data.user[user])\n\n                neg_item = choice(item_list)\n                while neg_item in self.data.trainSet_u[user]:\n                    neg_item = choice(item_list)\n                j_idx.append(self.data.item[neg_item])\n\n            yield u_idx, i_idx, j_idx\n\n    def sampling(self,vec):\n\n        vec = tf.nn.softmax(vec)\n\n        logits = gumbel_softmax(vec, 0.1)\n        return logits\n\n\n    def buildGraph(self):\n\n        self.weights = dict()\n        initializer = tf.contrib.layers.xavier_initializer()\n        #Generator\n        self.g_params = []\n\n        # self.CUNet = defaultdict(dict)\n        # print \'Building the collaborative user net. It may take a few seconds...\'\n        # self.implictConnection = []\n\n        self.isTraining = tf.placeholder(tf.int32)\n        self.isTraining = tf.cast(self.isTraining, tf.bool)\n\n        # for user1 in self.data.trainSet_u:\n        #     s1 = set(self.data.trainSet_u[user1])\n        #     for user2 in self.data.trainSet_u:\n        #         if user1 <> user2:\n        #             s2 = set(self.data.trainSet_u[user2])\n        #             weight = len(s1.intersection(s2))\n        #             if weight > 0:\n        #                 self.CUNet[user1][user2] = weight\n        #                 self.implictConnection.append((user1, user2, weight))\n\n        indices = [[self.data.user[item[0]], self.num_users + self.data.item[item[1]]] for item in\n                   self.data.trainingData]\n        indices += [[self.num_users + self.data.item[item[1]], self.data.user[item[0]]] for item in\n                    self.data.trainingData]\n        values = [float(item[2]) / sqrt(len(self.data.trainSet_u[item[0]])) / sqrt(\n            len(self.data.trainSet_i[item[1]])) for item in self.data.trainingData] * 2\n\n        R = tf.SparseTensor(indices=indices, values=values,\n                                   dense_shape=[self.num_users + self.num_items, self.num_users + self.num_items])\n\n\n        with tf.name_scope(""generator""):\n            user_embeddings = tf.Variable(tf.truncated_normal(shape=[self.num_users, self.embed_size], stddev=0.005), name=\'g_U\')\n            item_embeddings = tf.Variable(tf.truncated_normal(shape=[self.num_items, self.embed_size], stddev=0.005), name=\'g_I\')\n\n            ego_embeddings = tf.concat([self.user_embeddings, self.item_embeddings], axis=0)\n\n            self.g_params.append(user_embeddings)\n            self.g_params.append(item_embeddings)\n\n            indices+= [[self.data.user[item[0]], self.data.user[item[1]]] for item in\n                self.social.relation]\n\n            values += [float(item[2]) / sqrt(len(self.social.followees[item[0]])+1) / sqrt(\n                len(self.social.followers[item[1]])+1) for item in self.social.relation]\n\n            S_R = tf.SparseTensor(indices=indices, values=values,\n                                   dense_shape=[self.num_users+self.num_items, self.num_users+self.num_items])\n\n\n            weight_size = [self.embed_size, self.embed_size, self.embed_size]\n            weight_size_list = [self.embed_size] + weight_size\n\n            social_graph_layers = 3\n\n            # initialize parameters\n            for k in range(social_graph_layers):\n                self.weights[\'g_W_%d_1\' % k] = tf.Variable(\n                    initializer([weight_size_list[k], weight_size_list[k + 1]]), name=\'g_W_%d_1\' % k)\n                self.weights[\'g_W_%d_2\' % k] = tf.Variable(\n                    initializer([weight_size_list[k], weight_size_list[k + 1]]), name=\'g_W_%d_2\' % k)\n                self.g_params.append(self.weights[\'g_W_%d_1\' % k])\n                self.g_params.append(self.weights[\'g_W_%d_2\' % k])\n\n            all_g_embeddings = [ego_embeddings]\n            for k in range(social_graph_layers):\n                side_embeddings = tf.sparse_tensor_dense_matmul(S_R, ego_embeddings)\n                sum_embeddings = tf.matmul(side_embeddings + ego_embeddings, self.weights[\'g_W_%d_1\' % k])\n                bi_embeddings = tf.multiply(ego_embeddings, side_embeddings)\n                bi_embeddings = tf.matmul(bi_embeddings, self.weights[\'g_W_%d_2\' % k])\n\n                ego_embeddings = tf.nn.leaky_relu(sum_embeddings + bi_embeddings)\n\n                # message dropout.\n                def without_dropout():\n                    return ego_embeddings\n\n                def dropout():\n                    return tf.nn.dropout(ego_embeddings, keep_prob=0.9)\n\n                ego_embeddings = tf.cond(self.isTraining, lambda: dropout(), lambda: without_dropout())\n\n                # normalize the distribution of embeddings.\n                norm_embeddings = tf.math.l2_normalize(ego_embeddings, axis=1)\n\n                all_g_embeddings += [norm_embeddings]\n\n\n            g_total_embeddings = sum(all_g_embeddings)/social_graph_layers\n\n            g_user_embeddings, g_item_embeddings = tf.split(g_total_embeddings,[self.num_users, self.num_items], 0)\n\n\n            self.g_u_embedding = tf.nn.embedding_lookup(g_user_embeddings, self.u_idx)\n            self.g_i_embedding = tf.nn.embedding_lookup(g_item_embeddings, self.v_idx)\n        #MLP (friend and item generation)\n        with tf.name_scope(""item_generator""):\n\n            u_features = self.g_u_embedding + self.g_i_embedding + tf.multiply(self.g_u_embedding,self.g_i_embedding)\n\n            u_scores = tf.matmul(u_features,g_user_embeddings,transpose_b=True)\n\n            #one_hot implicit friend\n            self.implicit_friend = self.sampling(u_scores)\n            indices = [[self.data.item[item[1]], self.data.user[item[0]]] for item in self.data.trainingData]\n            values = [item[2] for item in self.data.trainingData]\n            self.i_u_matrix = tf.SparseTensor(indices=indices, values=values,\n                                              dense_shape=[self.num_items, self.num_users])\n\n            self.item_selection = tf.get_variable(\'item_selection\', initializer=tf.constant_initializer(0.01),\n                                                  shape=[self.num_users, self.num_items])\n            self.g_params.append(self.item_selection)\n\n            # get candidate list (items)\n            self.candidateItems = tf.transpose(\n                tf.sparse_tensor_dense_matmul(self.i_u_matrix, tf.transpose(self.implicit_friend)))\n\n            # f_features = tf.matmul(self.implicit_friend,g_user_embeddings)\n            # f_features += self.g_i_embedding + tf.multiply(f_features,self.g_i_embedding)\n            # i_scores = tf.matmul(f_features,g_item_embeddings,transpose_b=True)\n\n            embedding_selection = tf.nn.embedding_lookup(self.item_selection, self.u_idx, name=\'e_s\')\n\n            self.virtual_items = self.sampling(tf.multiply(self.candidateItems, embedding_selection))\n\n\n        self.d_weights = dict()\n        #Discriminator\n        self.d_params = [self.user_embeddings, self.item_embeddings]\n        with tf.name_scope(""discrminator""):\n            ego_embeddings = tf.concat([self.user_embeddings, self.item_embeddings], axis=0)\n\n            weight_size = [self.embed_size, self.embed_size, self.embed_size]\n            weight_size_list = [self.embed_size] + weight_size\n\n            self.n_layers = 3\n\n            # initialize parameters\n            for k in range(self.n_layers):\n                self.weights[\'W_%d_1\' % k] = tf.Variable(\n                    initializer([weight_size_list[k], weight_size_list[k + 1]]), name=\'W_%d_1\' % k)\n                self.weights[\'W_%d_2\' % k] = tf.Variable(\n                    initializer([weight_size_list[k], weight_size_list[k + 1]]), name=\'W_%d_2\' % k)\n                self.d_params.append(self.weights[\'W_%d_1\' % k])\n                self.d_params.append(self.weights[\'W_%d_2\' % k])\n\n            all_embeddings = [ego_embeddings]\n            for k in range(self.n_layers):\n                side_embeddings = tf.sparse_tensor_dense_matmul(R, ego_embeddings)\n                #friend_embeddings = tf.matmul(self.implicit_friend, ego_embeddings[:self.num_users])\n\n                sum_embeddings = tf.matmul(side_embeddings + ego_embeddings, self.weights[\'W_%d_1\' % k])\n                bi_embeddings = tf.multiply(ego_embeddings, side_embeddings)\n                bi_embeddings = tf.matmul(bi_embeddings, self.weights[\'W_%d_2\' % k])\n\n                ego_embeddings = tf.nn.leaky_relu(sum_embeddings + bi_embeddings)\n\n                # message dropout.\n                def without_dropout():\n                    return ego_embeddings\n\n                def dropout():\n                    return tf.nn.dropout(ego_embeddings, keep_prob=0.9)\n\n                ego_embeddings = tf.cond(self.isTraining, lambda: dropout(), lambda: without_dropout())\n\n                # normalize the distribution of embeddings.\n                norm_embeddings = tf.math.l2_normalize(ego_embeddings, axis=1)\n\n                all_embeddings += [norm_embeddings]\n\n            total_embeddings = tf.concat(all_embeddings, 1)\n\n            self.multi_user_embeddings, self.multi_item_embeddings = tf.split(total_embeddings,\n                                                                              [self.num_users, self.num_items], 0)\n\n\n            self.neg_idx = tf.placeholder(tf.int32, name=""neg_holder"")\n            self.neg_item_embedding = tf.nn.embedding_lookup(self.multi_item_embeddings, self.neg_idx)\n            self.u_embedding = tf.nn.embedding_lookup(self.multi_user_embeddings, self.u_idx)\n            self.v_embedding = tf.nn.embedding_lookup(self.multi_item_embeddings, self.v_idx)\n            self.v_i_embedding = tf.matmul(self.virtual_items, self.multi_item_embeddings, transpose_a=False,\n                                           transpose_b=False)\n\n    def initModel(self):\n        super(AGR, self).initModel()\n        self.buildGraph()\n\n    def buildModel(self):\n\n        y_uf = tf.reduce_sum(tf.multiply(self.u_embedding, self.v_embedding), 1) - \\\n               tf.reduce_sum(tf.multiply(self.u_embedding, self.v_i_embedding), 1)\n\n        y_fs = tf.reduce_sum(tf.multiply(self.u_embedding, self.v_i_embedding), 1) - \\\n               tf.reduce_sum(tf.multiply(self.u_embedding, self.neg_item_embedding), 1)\n\n        self.d_loss = -tf.reduce_sum(tf.log(tf.sigmoid(y_uf))) - tf.reduce_sum(tf.log(tf.sigmoid(y_fs))) + \\\n                      self.regU * (tf.nn.l2_loss(self.u_embedding) + tf.nn.l2_loss(self.v_embedding) + tf.nn.l2_loss(\n            self.neg_item_embedding))\n        #\n        self.g_loss = 30 * tf.reduce_sum(y_uf)  # better performance\n\n\n        self.d_output = tf.reduce_sum(tf.multiply(self.u_embedding, self.multi_item_embeddings), 1)\n\n        d_opt = tf.train.AdamOptimizer(self.lRate)\n\n        self.d_update = d_opt.minimize(self.d_loss, var_list=self.d_params)\n\n        g_opt = tf.train.AdamOptimizer(self.lRate)\n        self.g_update = g_opt.minimize(self.g_loss, var_list=self.g_params)\n\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n        print \'Training GAN...\'\n\n        for i in range(self.maxIter):\n\n            for num, batch in enumerate(self.next_batch()):\n                user_idx, i_idx, j_idx = batch\n\n                # generator\n                _, loss = self.sess.run([self.g_update, self.g_loss],\n                                        feed_dict={self.u_idx: user_idx, self.neg_idx: j_idx,\n                                                   self.v_idx: i_idx,self.isTraining:1})\n\n                # discriminator\n                _, loss = self.sess.run([self.d_update, self.d_loss],\n                                        feed_dict={self.u_idx: user_idx, self.neg_idx: j_idx,\n                                                   self.v_idx: i_idx,self.isTraining:1})\n\n                print \'training:\', i + 1, \'batch_id\', num, \'discriminator loss:\', loss\n\n    def predictForRanking(self, u):\n        \'invoked to rank all the items for the user\'\n        if self.data.containsUser(u):\n            u = self.data.user[u]\n\n            # In our experiments, discriminator performs better than generator\n            res = self.sess.run(self.d_output, {self.u_idx:u,self.isTraining:0})\n            return res\n\n        else:\n            return [self.data.globalMean] * self.num_items'"
algorithm/ranking/APR.py,29,"b'#coding:utf8\nfrom baseclass.DeepRecommender import DeepRecommender\n\nimport numpy as np\nimport random\nfrom tool import config\n\ntry:\n    import tensorflow as tf\nexcept ImportError:\n    print \'This method can only run on tensorflow!\'\n    exit(-1)\nfrom tensorflow import set_random_seed\nset_random_seed(2)\n\nclass APR(DeepRecommender):\n\n    # APR\xef\xbc\x9aAdversarial Personalized Ranking for Recommendation\n\n    def __init__(self,conf,trainingSet=None,testSet=None,fold=\'[1]\'):\n        super(APR, self).__init__(conf,trainingSet,testSet,fold)\n\n    # def readConfiguration(self):\n    #     super(APR, self).readConfiguration()\n\n    def readConfiguration(self):\n        super(APR, self).readConfiguration()\n        args = config.LineConfig(self.config[\'APR\'])\n        self.eps = float(args[\'-eps\'])\n        self.regAdv = float(args[\'-regA\'])\n        self.advEpoch = int(args[\'-advEpoch\'])\n\n\n    def _create_variables(self):\n        #perturbation vectors\n        self.adv_U = tf.Variable(tf.zeros(shape=[self.num_users, self.embed_size]),dtype=tf.float32, trainable=False)\n        self.adv_V = tf.Variable(tf.zeros(shape=[self.num_items, self.embed_size]),dtype=tf.float32, trainable=False)\n\n        self.neg_idx = tf.placeholder(tf.int32, [None], name=""n_idx"")\n        self.V_neg_embed = tf.nn.embedding_lookup(self.V, self.neg_idx)\n        #parameters\n        self.eps = tf.constant(self.eps,dtype=tf.float32)\n        self.regAdv = tf.constant(self.regAdv,dtype=tf.float32)\n\n    def _create_inference(self):\n        result = tf.subtract(tf.reduce_sum(tf.multiply(self.U_embed, self.V_embed), 1),\n                                  tf.reduce_sum(tf.multiply(self.U_embed, self.V_neg_embed), 1))\n        return result\n\n    def _create_adv_inference(self):\n        self.U_plus_delta = tf.add(self.U_embed, tf.nn.embedding_lookup(self.adv_U, self.u_idx))\n        self.V_plus_delta = tf.add(self.V_embed, tf.nn.embedding_lookup(self.adv_V, self.v_idx))\n        self.V_neg_plus_delta = tf.add(self.V_neg_embed, tf.nn.embedding_lookup(self.adv_V, self.neg_idx))\n        result = tf.subtract(tf.reduce_sum(tf.multiply(self.U_plus_delta, self.V_plus_delta), 1),\n                             tf.reduce_sum(tf.multiply(self.U_plus_delta, self.V_neg_plus_delta), 1))\n        return result\n\n    def _create_adversarial(self):\n        #get gradients of Delta\n        self.grad_U, self.grad_V = tf.gradients(self.loss_adv, [self.adv_U,self.adv_V])\n\n        # convert the IndexedSlice Data to Dense Tensor\n        self.grad_U_dense = tf.stop_gradient(self.grad_U)\n        self.grad_V_dense = tf.stop_gradient(self.grad_V)\n\n        # normalization: new_grad = (grad / |grad|) * eps\n        self.update_U = self.adv_U.assign(tf.nn.l2_normalize(self.grad_U_dense, 1) * self.eps)\n        self.update_V = self.adv_V.assign(tf.nn.l2_normalize(self.grad_V_dense, 1) * self.eps)\n\n\n    def _create_loss(self):\n        self.reg_lambda = tf.constant(self.regU, dtype=tf.float32)\n        self.loss = tf.reduce_sum(tf.nn.softplus(-self._create_inference()))\n        self.reg_loss = tf.add(tf.multiply(self.reg_lambda, tf.nn.l2_loss(self.U_embed)),\n                               tf.multiply(self.reg_lambda, tf.nn.l2_loss(self.V_embed)))\n\n        self.total_loss = tf.add(self.loss, self.reg_loss)\n        #loss of adversarial training\n        self.loss_adv = tf.multiply(self.regAdv,tf.reduce_sum(tf.nn.softplus(-self._create_adv_inference())))\n        self.loss_adv = tf.add(self.total_loss,self.loss_adv)\n\n    def _create_optimizer(self):\n        self.optimizer = tf.train.AdamOptimizer(self.lRate)\n        self.train = self.optimizer.minimize(self.total_loss)\n\n        self.optimizer_adv = tf.train.AdamOptimizer(self.lRate)\n        self.train_adv = self.optimizer.minimize(self.loss_adv)\n\n\n    def initModel(self):\n        super(APR, self).initModel()\n        self._create_variables()\n        self._create_loss()\n        self._create_adversarial()\n        self._create_optimizer()\n\n\n    def next_batch(self):\n        batch_idx = np.random.randint(self.train_size, size=self.batch_size)\n\n        users = [self.data.trainingData[idx][0] for idx in batch_idx]\n        items = [self.data.trainingData[idx][1] for idx in batch_idx]\n        user_idx,item_idx=[],[]\n        neg_item_idx = []\n        for i,user in enumerate(users):\n\n            item_j = random.randint(0,self.num_items-1)\n\n            while self.data.trainSet_u[user].has_key(self.data.id2item[item_j]):\n                item_j = random.randint(0, self.num_items - 1)\n\n            user_idx.append(self.data.user[user])\n            item_idx.append(self.data.item[items[i]])\n            neg_item_idx.append(item_j)\n\n        return user_idx,item_idx,neg_item_idx\n\n\n    def buildModel(self):\n        print \'training...\'\n        iteration = 0\n        with tf.Session() as sess:\n            init = tf.global_variables_initializer()\n            sess.run(init)\n\n            # train the model until converged\n            for epoch in range(self.maxIter):\n\n                user_idx,item_idx,neg_item_idx = self.next_batch()\n                _,loss = sess.run([self.train,self.total_loss],feed_dict={self.u_idx: user_idx, self.v_idx: item_idx, self.neg_idx:neg_item_idx})\n                print \'iteration:\', epoch, \'loss:\',loss\n\n\n                self.P = sess.run(self.U)\n                self.Q = sess.run(self.V)\n                if epoch%100==0 and epoch>0:\n                    self.ranking_performance()\n\n            # start adversarial training\n            for epoch in range(self.advEpoch):\n\n                user_idx,item_idx,neg_item_idx = self.next_batch()\n                sess.run([self.update_U, self.update_V],\n                         feed_dict={self.u_idx: user_idx, self.v_idx: item_idx, self.neg_idx: neg_item_idx})\n                _,loss = sess.run([self.train_adv,self.loss_adv],feed_dict={self.u_idx: user_idx, self.v_idx: item_idx, self.neg_idx:neg_item_idx})\n\n                print \'iteration:\', epoch, \'loss:\',loss\n\n                self.P = sess.run(self.U)\n                self.Q = sess.run(self.V)\n                if epoch % 100 == 0 and epoch > 0:\n                    self.ranking_performance()\n\n\n    def predictForRanking(self, u):\n        \'invoked to rank all the items for the user\'\n        if self.data.containsUser(u):\n            u = self.data.getUserId(u)\n            return self.Q.dot(self.P[u])\n        else:\n            return [self.data.globalMean] * self.num_items\n\n\n'"
algorithm/ranking/BPR.py,10,"b'#coding:utf8\nfrom baseclass.IterativeRecommender import IterativeRecommender\nfrom random import choice\nfrom tool.qmath import sigmoid\nfrom math import log\nfrom collections import defaultdict\nimport tensorflow as tf\nclass BPR(IterativeRecommender):\n\n    # BPR\xef\xbc\x9aBayesian Personalized Ranking from Implicit Feedback\n    # Steffen Rendle,Christoph Freudenthaler,Zeno Gantner and Lars Schmidt-Thieme\n\n    def __init__(self,conf,trainingSet=None,testSet=None,fold=\'[1]\'):\n        super(BPR, self).__init__(conf,trainingSet,testSet,fold)\n\n    # def readConfiguration(self):\n    #     super(BPR, self).readConfiguration()\n\n    def initModel(self):\n        super(BPR, self).initModel()\n\n\n    def buildModel(self):\n\n        print \'Preparing item sets...\'\n        self.PositiveSet = defaultdict(dict)\n        #self.NegativeSet = defaultdict(list)\n\n        for user in self.data.user:\n            for item in self.data.trainSet_u[user]:\n                if self.data.trainSet_u[user][item] >= 1:\n                    self.PositiveSet[user][item] = 1\n                # else:\n                #     self.NegativeSet[user].append(item)\n        print \'training...\'\n        iteration = 0\n        itemList = self.data.item.keys()\n        while iteration < self.maxIter:\n            self.loss = 0\n            for user in self.PositiveSet:\n                u = self.data.user[user]\n                for item in self.PositiveSet[user]:\n                    i = self.data.item[item]\n\n                    item_j = choice(itemList)\n                    while (self.PositiveSet[user].has_key(item_j)):\n                        item_j = choice(itemList)\n                    j = self.data.item[item_j]\n                    self.optimization(u,i,j)\n\n            self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()\n            iteration += 1\n            if self.isConverged(iteration):\n                break\n\n    def optimization(self,u,i,j):\n        s = sigmoid(self.P[u].dot(self.Q[i]) - self.P[u].dot(self.Q[j]))\n        self.P[u] += self.lRate * (1 - s) * (self.Q[i] - self.Q[j])\n        self.Q[i] += self.lRate * (1 - s) * self.P[u]\n        self.Q[j] -= self.lRate * (1 - s) * self.P[u]\n\n        self.P[u] -= self.lRate * self.regU * self.P[u]\n        self.Q[i] -= self.lRate * self.regI * self.Q[i]\n        self.Q[j] -= self.lRate * self.regI * self.Q[j]\n        self.loss += -log(s)\n\n    def predict(self,user,item):\n\n        if self.data.containsUser(user) and self.data.containsItem(item):\n            u = self.data.getUserId(user)\n            i = self.data.getItemId(item)\n            predictRating = sigmoid(self.Q[i].dot(self.P[u]))\n            return predictRating\n        else:\n            return sigmoid(self.data.globalMean)\n\n    def next_batch(self):\n        batch_id=0\n        while batch_id<self.train_size:\n            if batch_id+self.batch_size<=self.train_size:\n                users = [self.data.trainingData[idx][0] for idx in range(batch_id,self.batch_size+batch_id)]\n                items = [self.data.trainingData[idx][1] for idx in range(batch_id,self.batch_size+batch_id)]\n                batch_id+=self.batch_size\n            else:\n                users = [self.data.trainingData[idx][0] for idx in range(batch_id, self.train_size)]\n                items = [self.data.trainingData[idx][1] for idx in range(batch_id, self.train_size)]\n                batch_id=self.train_size\n\n            u_idx,i_idx,j_idx = [],[],[]\n            item_list = self.data.item.keys()\n            for i,user in enumerate(users):\n\n                i_idx.append(self.data.item[items[i]])\n                u_idx.append(self.data.user[user])\n\n                neg_item = choice(item_list)\n                while neg_item in self.data.trainSet_u[user]:\n                    neg_item = choice(item_list)\n                j_idx.append(self.data.item[neg_item])\n\n            yield u_idx,i_idx,j_idx\n\n    def buildModel_tf(self):\n        super(BPR, self).buildModel_tf()\n        self.neg_idx = tf.placeholder(tf.int32, name=""neg_holder"")\n        self.neg_item_embedding = tf.nn.embedding_lookup(self.V, self.neg_idx)\n        y = tf.reduce_sum(tf.multiply(self.user_embedding,self.item_embedding),1)\\\n                                 -tf.reduce_sum(tf.multiply(self.user_embedding,self.neg_item_embedding),1)\n        loss = -tf.reduce_sum(tf.log(tf.sigmoid(y))) + self.regU * (tf.nn.l2_loss(self.user_embedding) +\n                                                                       tf.nn.l2_loss(self.item_embedding) +\n                                                                       tf.nn.l2_loss(self.neg_item_embedding))\n        opt = tf.train.AdamOptimizer(self.lRate)\n\n        train = opt.minimize(loss)\n\n        with tf.Session() as sess:\n            init = tf.global_variables_initializer()\n            sess.run(init)\n            for iteration in range(self.maxIter):\n                for n,batch in enumerate(self.next_batch()):\n                    user_idx, i_idx, j_idx = batch\n                    _, l = sess.run([train, loss], feed_dict={self.u_idx: user_idx, self.neg_idx: j_idx,self.v_idx: i_idx})\n                    print \'training:\', iteration + 1, \'batch\', n, \'loss:\', l\n            self.P,self.Q = sess.run([self.U,self.V])\n\n\n\n    def predictForRanking(self, u):\n        \'invoked to rank all the items for the user\'\n        if self.data.containsUser(u):\n            u = self.data.getUserId(u)\n            return self.Q.dot(self.P[u])\n        else:\n            return [self.data.globalMean] * self.num_items\n\n\n'"
algorithm/ranking/CDAE.py,23,"b'#coding:utf8\nfrom baseclass.DeepRecommender import DeepRecommender\nimport numpy as np\nfrom random import choice,random\nfrom tool import config\nimport tensorflow as tf\n\n\n\nclass CDAE(DeepRecommender):\n\n    def __init__(self,conf,trainingSet=None,testSet=None,fold=\'[1]\'):\n        super(CDAE, self).__init__(conf,trainingSet,testSet,fold)\n\n    def encoder(self,x,v):\n        layer = tf.nn.sigmoid(tf.matmul(x, self.weights[\'encoder\'])+self.biases[\'encoder\']+v)\n        return layer\n\n    def decoder(self,x):\n        layer = tf.nn.sigmoid(tf.matmul(x, self.weights[\'decoder\'])+self.biases[\'decoder\'])\n        return layer\n\n    def next_batch(self):\n        X = np.zeros((self.batch_size,self.num_items))\n        uids = []\n        sample = np.zeros((self.batch_size, self.num_items))\n        userList = self.data.user.keys()\n        itemList = self.data.item.keys()\n        for n in range(self.batch_size):\n            user = choice(userList)\n            uids.append(self.data.user[user])\n            vec = self.data.row(user)\n            ratedItems, values = self.data.userRated(user)\n            for item in ratedItems:\n                iid = self.data.item[item]\n                sample[n][iid]=1\n            for i in range(self.negative_sp*len(ratedItems)):\n                ng = choice(itemList)\n                while self.data.trainSet_u.has_key(ng):\n                    ng = choice(itemList)\n                n_id = self.data.item[ng]\n                sample[n][n_id]=1\n            X[n]=vec\n        return X,uids,sample\n\n    def readConfiguration(self):\n        super(CDAE, self).readConfiguration()\n        args = config.LineConfig(self.config[\'CDAE\'])\n        self.corruption_level = float(args[\'-co\'])\n        self.n_hidden = int(args[\'-nh\'])\n\n    def initModel(self):\n        super(CDAE, self).initModel()\n\n        self.negative_sp = 5\n        initializer = tf.contrib.layers.xavier_initializer()\n        self.X = tf.placeholder(tf.float32, [None, self.num_items])\n        self.mask_corruption = tf.placeholder(tf.float32, [None, self.num_items])\n        self.sample = tf.placeholder(tf.float32, [None, self.num_items])\n\n        self.U = tf.Variable(initializer([self.num_users, self.n_hidden]))\n\n        self.U_embed = tf.nn.embedding_lookup(self.U, self.u_idx)\n\n\n\n        self.weights = {\n            \'encoder\': tf.Variable(initializer([self.num_items, self.n_hidden])),\n            \'decoder\': tf.Variable(initializer([self.n_hidden, self.num_items])),\n        }\n        self.biases = {\n            \'encoder\': tf.Variable(initializer([self.n_hidden])),\n            \'decoder\': tf.Variable(initializer([self.num_items])),\n        }\n\n\n    def buildModel(self):\n        self.corrupted_input = tf.multiply(self.X,self.mask_corruption)\n        self.encoder_op = self.encoder(self.corrupted_input,self.U_embed)\n        self.decoder_op = self.decoder(self.encoder_op)\n\n\n        self.y_pred = tf.multiply(self.sample,self.decoder_op)\n\n        y_true = tf.multiply(self.sample,self.corrupted_input)\n\n        self.y_pred = tf.maximum(1e-6, self.y_pred)\n\n        self.loss = -tf.multiply(y_true,tf.log(self.y_pred))-tf.multiply((1-y_true),tf.log(1-self.y_pred))\n\n\n        self.reg_loss = self.regU*(tf.nn.l2_loss(self.weights[\'encoder\'])+tf.nn.l2_loss(self.weights[\'decoder\'])+\n                                   tf.nn.l2_loss(self.biases[\'encoder\'])+tf.nn.l2_loss(self.biases[\'decoder\']))\n\n        self.reg_loss = self.reg_loss + self.regU*tf.nn.l2_loss(self.U_embed)\n        self.loss = self.loss + self.reg_loss\n        self.loss = tf.reduce_mean(self.loss)\n\n        optimizer = tf.train.AdamOptimizer(self.lRate).minimize(self.loss)\n\n\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n\n\n        for epoch in range(self.maxIter):\n\n            mask = np.random.binomial(1, self.corruption_level,(self.batch_size, self.num_items))\n            batch_xs,users,sample = self.next_batch()\n\n            _, loss,y = self.sess.run([optimizer, self.loss,self.y_pred], feed_dict={self.X: batch_xs,self.mask_corruption:mask,self.u_idx:users,self.sample:sample})\n\n            print self.foldInfo,""Epoch:"", \'%04d\' % (epoch + 1),""loss="", ""{:.9f}"".format(loss)\n            #print y\n            #self.ranking_performance()\n        print(""Optimization Finished!"")\n\n\n\n    def predictForRanking(self, u):\n        \'invoked to rank all the items for the user\'\n        if self.data.containsUser(u):\n            vec = self.data.row(u).reshape((1,self.num_items))\n            uid = [self.data.user[u]]\n            return self.sess.run(self.decoder_op,feed_dict={self.X:vec,self.mask_corruption:np.ones((1,self.num_items)),self.u_idx:uid})[0]\n        else:\n            return [self.data.globalMean] * self.num_items\n\n\n'"
algorithm/ranking/CFGAN.py,38,"b'#coding:utf8\nfrom baseclass.DeepRecommender import DeepRecommender\nimport numpy as np\nfrom random import randint,choice\nimport tensorflow as tf\n\n\n\nclass CFGAN(DeepRecommender):\n\n    def __init__(self,conf,trainingSet=None,testSet=None,fold=\'[1]\'):\n        super(CFGAN, self).__init__(conf,trainingSet,testSet,fold)\n        #It is quite interesting and confusing that when I set ratio_zr = 0 and ratio_zp = 0, CFGAN reaches the best performance.\n        self.S_zr = 0.001\n        self.S_pm = 0.001\n        self.alpha = 0.01\n\n    def next_batch(self):\n        C_u = np.zeros((self.batch_size,self.num_items))\n        N_zr = np.zeros((self.batch_size,self.num_items))\n\n        mask = np.zeros((self.batch_size,self.num_items)) #e_u + k_u\n\n        userList = self.data.user.keys()\n        itemList = self.data.item.keys()\n\n        for n in range(self.batch_size):\n            user = choice(userList)\n            vec = self.data.row(user)\n\n            ratedItems, values = self.data.userRated(user)\n            for item in ratedItems:\n                iid = self.data.item[item]\n                mask[n][iid]=1\n\n            for i in range(int(self.S_zr*self.num_items)):\n                ng = choice(itemList)\n                while self.data.trainSet_u[user].has_key(ng):\n                    ng = choice(itemList)\n                ng = self.data.item[ng]\n                N_zr[n][ng] = 1\n\n            for i in range(int(self.S_pm*self.num_items)):\n                ng = choice(itemList)\n                while self.data.trainSet_u[user].has_key(ng):\n                    ng = choice(itemList)\n                ng = self.data.item[ng]\n                mask[n][ng] = 1\n\n            C_u[n]=vec\n        return C_u,mask,N_zr\n\n    def initModel(self):\n        super(CFGAN, self).initModel()\n        G_regularizer = tf.contrib.layers.l2_regularizer(scale=0.001)\n        D_regularizer = tf.contrib.layers.l2_regularizer(scale=0.001)\n        xavier_init = tf.contrib.layers.xavier_initializer()\n\n        with tf.variable_scope(""Generator""):\n            # Generator Net\n            self.C = tf.placeholder(tf.float32, shape=[None, self.num_items], name=\'C\')\n\n            G_W1 = tf.get_variable(name=\'G_W1\',initializer=xavier_init([self.num_items,self.num_items]), regularizer=G_regularizer)\n            G_b1 = tf.get_variable(name=\'G_b1\',initializer=tf.zeros(shape=[self.num_items]), regularizer=G_regularizer)\n\n            # G_W2 = tf.get_variable(name=\'G_W2\',initializer=xavier_init([300,200]), regularizer=G_regularizer)\n            # G_b2 = tf.get_variable(name=\'G_b2\',initializer=tf.zeros(shape=[200]), regularizer=G_regularizer)\n            #\n            # G_W3 = tf.get_variable(initializer=xavier_init([200,self.num_items]), name=\'G_W3\',regularizer=G_regularizer)\n            # G_b3 = tf.get_variable(initializer=tf.zeros(shape=[self.num_items]), name=\'G_b3\',regularizer=G_regularizer)\n\n            theta_G = [G_W1, G_b1]#G_W2, G_W3, G_b1, G_b2, G_b3]\n\n        with tf.variable_scope(""Discriminator""):\n            # Discriminator Net\n            self.X = tf.placeholder(tf.float32, shape=[None, self.num_items], name=\'X\')\n\n            D_W1 = tf.get_variable(initializer=xavier_init([self.num_items*2,1]), name=\'D_W1\',regularizer=D_regularizer)\n            D_b1 = tf.get_variable(initializer=tf.zeros(shape=[1]), name=\'D_b1\',regularizer=D_regularizer)\n\n            # D_W2 = tf.get_variable(name=\'D_W2\', initializer=xavier_init([300, 200]), regularizer=D_regularizer)\n            # D_b2 = tf.get_variable(name=\'D_b2\', initializer=tf.zeros(shape=[200]), regularizer=D_regularizer)\n            #\n            # D_W3 = tf.get_variable(initializer=xavier_init([200,1]), name=\'D_W3\',regularizer=D_regularizer)\n            # D_b3 = tf.get_variable(initializer=tf.zeros(shape=[1]), name=\'D_b3\',regularizer=D_regularizer)\n\n            theta_D = [D_W1, D_b1]#D_W2, D_W3, D_b1, D_b2, D_b3]\n\n        self.mask = tf.placeholder(tf.float32, shape=[None, self.num_items], name=\'mask\')\n        self.N_zr = tf.placeholder(tf.float32, shape=[None, self.num_items], name=\'mask\')\n\n\n\n        #inference\n        def generator():\n            r_hat = tf.nn.sigmoid(tf.matmul(self.C, G_W1) + G_b1)\n            # G_h2 = tf.nn.relu(tf.matmul(G_h1, G_W2) + G_b2)\n            # r_hat = tf.nn.sigmoid(tf.matmul(G_h2, G_W3) + G_b3)\n            fake_data = tf.multiply(r_hat,self.mask)\n            return fake_data\n\n        def discriminator(x):\n            D_output = tf.nn.sigmoid(tf.matmul(x, D_W1) + D_b1)\n            # D_h2 = tf.nn.relu(tf.matmul(D_h1, D_W2) + D_b2)\n            # D_output = tf.nn.sigmoid(tf.matmul(D_h2, D_W3) + D_b3)\n            return  D_output\n\n        def r_hat():\n            r_hat = tf.nn.sigmoid(tf.matmul(self.C, G_W1) + G_b1)\n            # G_h2 = tf.nn.relu(tf.matmul(G_h1, G_W2) + G_b2)\n            # r_hat = tf.nn.sigmoid(tf.matmul(G_h2, G_W3) + G_b3)\n            return r_hat\n\n        G_sample = generator()\n        self.r_hat = r_hat()\n        D_real = discriminator(tf.concat([self.C,self.C],1))\n        D_fake = discriminator(tf.concat([G_sample,self.C],1))\n\n\n        self.D_loss = -tf.reduce_mean(tf.log(D_real+10e-5) + tf.log(1. - D_fake+10e-5))\n        self.G_loss = tf.reduce_mean(tf.log(1.-D_fake+10e-5)+self.alpha*tf.nn.l2_loss(tf.multiply(self.N_zr,G_sample)))\n\n        # Only update D(X)\'s parameters, so var_list = theta_D\n        self.D_solver = tf.train.AdamOptimizer(self.lRate).minimize(self.D_loss, var_list=theta_D)\n        # Only update G(X)\'s parameters, so var_list = theta_G\n        self.G_solver = tf.train.AdamOptimizer(self.lRate).minimize(self.G_loss, var_list=theta_G)\n\n\n    def buildModel(self):\n\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n\n        print \'pretraining...\'\n\n\n        print \'training...\'\n        for epoch in range(self.maxIter):\n            G_loss = 0\n\n            C_u, mask, N_zr = self.next_batch()\n            _, D_loss = self.sess.run([self.D_solver, self.D_loss], feed_dict={self.C: C_u,self.mask:mask,self.N_zr:N_zr})\n\n            for i in range(3):\n                _, G_loss = self.sess.run([self.G_solver, self.G_loss], feed_dict={self.C: C_u,self.mask:mask,self.N_zr:N_zr})\n\n            #C_u, mask, N_u = self.next_batch()\n            print \'iteration:\', epoch, \'D_loss:\', D_loss, \'G_loss\', G_loss\n\n\n\n    def predictForRanking(self, u):\n        \'invoked to rank all the items for the user\'\n        if self.data.containsUser(u):\n            vec = self.data.row(u).reshape(1,self.num_items)\n            res = self.sess.run([self.r_hat], feed_dict={self.C: vec})[0]\n            #print res[0]\n            return res[0]\n\n        else:\n            return [self.data.globalMean] * self.num_items\n\n\n\n'"
algorithm/ranking/CUNE_BPR.py,0,"b""\nfrom baseclass.IterativeRecommender import IterativeRecommender\nfrom tool import config\nfrom random import randint\nfrom random import shuffle,choice\nfrom collections import defaultdict\nimport numpy as np\nfrom tool.qmath import sigmoid,cosine\nfrom math import log\nimport gensim.models.word2vec as w2v\nfrom structure.symmetricMatrix import SymmetricMatrix\n# class Node(object):\n#     def __init__(self):\n#         self.val = 0\n#         self.last = None\n#         self.next = None\n#\n# class OrderedLinkList(object):\n#     def __init__(self):\n#         self.head=None\n#         self.tail=None\n#         self.length = 0\n#\n#     def __len__(self):\n#         return self.length\n#\n#     def insert(self,node):\n#         self.length+=1\n#         if self.head:\n#             tmp = self.head\n#             while tmp.val < node.val:\n#                 if tmp==self.tail:\n#                     break\n#                 tmp = tmp.next\n#\n#             if tmp is self.head:\n#\n#                 if self.head.val < node.val:\n#                     node.next = self.head\n#                     self.head.last = node\n#                     self.head = node\n#                 else:\n#                     node.next = self.head\n#                     self.head.last = node\n#                     self.head = node\n#                 return\n#\n#             node.next = tmp.next\n#             tmp.next = node\n#             node.last = tmp\n#             if not node.next:\n#                 self.tail = node\n#\n#         else:\n#             self.head = node\n#             self.tail = node\n#\n#     def removeHead(self):\n#         if self.head:\n#             self.head = self.head.next\n#             self.length -= 1\n#\n#     def removeNode(self,node):\n#         if self.head:\n#             tmp = self.head\n#             while tmp is not node and tmp.next:\n#                 tmp = tmp.next\n#             if tmp.next:\n#                 tmp.last.next = tmp.next\n#                 tmp.next.last = tmp.last\n#             self.length-=1\n#\n#\n# class HTreeNode(object):\n#     def __init__(self,left,right,freq,id,code=None):\n#         self.left = left\n#         self.right = right\n#         self.weight = freq\n#         self.id = id\n#         self.code = code\n#\n#     def __lt__(self, other):\n#         if self.weight < other.weight:\n#             return True\n#         else:\n#             return False\n#\n# class HuffmanTree(object):\n#     def __init__(self,root=None,vecLength=10):\n#         self.root = root\n#         self.weight = 0\n#         self.code = {}\n#         self.vecLength = vecLength\n#         self.vector = {}\n#\n#     def buildFromTrees(self,left,right):\n#         root = HTreeNode(left.val,right.val,left.val.weight+right.val.weight,None)\n#         return root\n#\n#     def buildTree(self,nodeList):\n#         if len(nodeList)<2:\n#             self.root = nodeList.head\n#             return\n#\n#         while(len(nodeList)>1):\n#             left = nodeList.head\n#             right = nodeList.head.next\n#             nodeList.removeHead()\n#             nodeList.removeHead()\n#             tree = self.buildFromTrees(left,right)\n#             node = Node()\n#             node.val = tree\n#             nodeList.insert(node)\n#\n#         self.root = nodeList.head.val\n#\n#     def coding(self,root,prefix,hierarchy):\n#         if root:\n#             root.code = prefix\n#             self.vector[prefix] = np.random.random(self.vecLength)\n#             if root.id:\n#                 self.code[root.id] = prefix\n#\n#             # if root.id:\n#             #     print 'level', hierarchy\n#             #     print root.id,prefix,root.weight\n#\n#             self.coding(root.left,prefix+'0',hierarchy+1)\n#             self.coding(root.right,prefix+'1',hierarchy+1)\n\nclass CUNE_BPR(IterativeRecommender):\n    def __init__(self,conf,trainingSet=None,testSet=None,fold='[1]'):\n        super(CUNE_BPR, self).__init__(conf,trainingSet,testSet,fold)\n        self.nonLeafVec = {}\n        self.leafVec = {}\n\n\n    def readConfiguration(self):\n        super(CUNE_BPR, self).readConfiguration()\n        options = config.LineConfig(self.config['CUNE-BPR'])\n        self.walkCount = int(options['-T'])\n        self.walkLength = int(options['-L'])\n        self.walkDim = int(options['-l'])\n        self.winSize = int(options['-w'])\n        self.topK = int(options['-k'])\n        self.s = float(options['-s'])\n        self.epoch = int(options['-ep'])\n\n    def printAlgorConfig(self):\n        super(CUNE_BPR, self).printAlgorConfig()\n        print 'Specified Arguments of', self.config['recommender'] + ':'\n        print 'Walks count per user', self.walkCount\n        print 'Length of each walk', self.walkLength\n        print 'Dimension of user embedding', self.walkDim\n        print '='*80\n\n    def buildModel(self):\n        print 'Kind Note: This method will probably take much time.'\n        #build C-U-NET\n        print 'Building collaborative user network...'\n        #filter isolated nodes\n        self.itemNet = {}\n        for item in self.data.trainSet_i:\n            if len(self.data.trainSet_i[item])>1:\n                self.itemNet[item] = self.data.trainSet_i[item]\n\n        self.filteredRatings = defaultdict(list)\n        for item in self.itemNet:\n            for user in self.itemNet[item]:\n                if self.itemNet[item][user]>=1:\n                    self.filteredRatings[user].append(item)\n\n        self.CUNet = defaultdict(list)\n\n        for user1 in self.filteredRatings:\n            s1 = set(self.filteredRatings[user1])\n            for user2 in self.filteredRatings:\n                if user1 <> user2:\n                    s2 = set(self.filteredRatings[user2])\n                    weight = len(s1.intersection(s2))\n                    if weight > 0:\n                        self.CUNet[user1]+=[user2]*weight\n\n\n        #build Huffman Tree First\n        #get weight\n        # print 'Building Huffman tree...'\n        # #To accelerate the method, the weight is estimated roughly\n        # nodes = {}\n        # for user in self.CUNet:\n        #     nodes[user] = len(self.CUNet[user])\n        # nodes = sorted(nodes.iteritems(),key=lambda d:d[1])\n        # nodes = [HTreeNode(None,None,user[1],user[0]) for user in nodes]\n        # nodeList = OrderedLinkList()\n        # for node in nodes:\n        #     listNode = Node()\n        #     listNode.val = node\n        #     try:\n        #         nodeList.insert(listNode)\n        #     except AttributeError:\n        #         pass\n        # self.HTree = HuffmanTree(vecLength=self.walkDim)\n        # self.HTree.buildTree(nodeList)\n        # print 'Coding for all users...'\n        # self.HTree.coding(self.HTree.root,'',0)\n\n\n        print 'Generating random deep walks...'\n        self.walks = []\n        self.visited = defaultdict(dict)\n        for user in self.CUNet:\n            for t in range(self.walkCount):\n                path = [user]\n                lastNode = user\n                for i in range(1,self.walkLength):\n                    nextNode = choice(self.CUNet[lastNode])\n                    count=0\n                    while(self.visited[lastNode].has_key(nextNode)):\n                        nextNode = choice(self.CUNet[lastNode])\n                        #break infinite loop\n                        count+=1\n                        if count==10:\n                            break\n                    path.append(nextNode)\n                    self.visited[user][nextNode] = 1\n                    lastNode = nextNode\n                self.walks.append(path)\n                #print path\n        shuffle(self.walks)\n\n        #Training get top-k friends\n        print 'Generating user embedding...'\n        # iteration = 1\n        # while iteration <= self.epoch:\n        #     loss = 0\n        #     #slide windows randomly\n        #\n        #     for n in range(self.walkLength/self.winSize):\n        #\n        #         for walk in self.walks:\n        #             center = randint(0, len(walk)-1)\n        #             s = max(0,center-self.winSize/2)\n        #             e = min(center+self.winSize/2,len(walk)-1)\n        #             for user in walk[s:e]:\n        #                 centerUser = walk[center]\n        #                 if user <> centerUser:\n        #                     code = self.HTree.code[user]\n        #                     centerCode = self.HTree.code[centerUser]\n        #                     x = self.HTree.vector[centerCode]\n        #                     for i in range(1,len(code)):\n        #                         prefix = code[0:i]\n        #                         w = self.HTree.vector[prefix]\n        #                         self.HTree.vector[prefix] += self.lRate*(1-sigmoid(w.dot(x)))*x\n        #                         self.HTree.vector[centerCode] += self.lRate*(1-sigmoid(w.dot(x)))*w\n        #                         loss += -log(sigmoid(w.dot(x)),2)\n        #     print 'iteration:', iteration, 'loss:', loss\n        #     iteration+=1\n        model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)\n        print 'User embedding generated.'\n\n        print 'Constructing similarity matrix...'\n        self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10\n        self.topKSim = {}\n        i = 0\n        for user1 in self.CUNet:\n            # prefix1 = self.HTree.code[user1]\n            # vec1 = self.HTree.vector[prefix1]\n            sims = []\n            u1 = self.data.user[user1]\n            self.W[u1] = model.wv[user1]\n            for user2 in self.CUNet:\n                if user1 <> user2:\n                    u2 = self.data.user[user2]\n                    self.W[u2] = model.wv[user2]\n                    sims.append((user2,cosine(self.W[u1],self.W[u2])))\n            self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]\n            i += 1\n            if i % 200 == 0:\n                print 'progress:', i, '/', len(self.CUNet)\n        print 'Similarity matrix finished.'\n        #print self.topKSim\n\n        #prepare Pu set, IPu set, and Nu set\n        print 'Preparing item sets...'\n        self.PositiveSet = defaultdict(dict)\n        self.IPositiveSet = defaultdict(dict)\n        #self.NegativeSet = defaultdict(list)\n\n        for user in self.topKSim:\n            for item in self.data.trainSet_u[user]:\n                 self.PositiveSet[user][item]=1\n                # else:\n                #     self.NegativeSet[user].append(item)\n\n            for friend in self.topKSim[user]:\n                for item in self.data.trainSet_u[friend[0]]:\n                    if not self.PositiveSet[user].has_key(item):\n                        self.IPositiveSet[user][item]=1\n\n\n        print 'Training...'\n        iteration = 0\n        while iteration < self.maxIter:\n            self.loss = 0\n            itemList = self.data.item.keys()\n            for user in self.PositiveSet:\n                u = self.data.user[user]\n                kItems = self.IPositiveSet[user].keys()\n                for item in self.PositiveSet[user]:\n                    i = self.data.item[item]\n                    for n in range(3): #negative sampling for 3 times\n                        if len(self.IPositiveSet[user]) > 0:\n                            item_k = choice(kItems)\n                            k = self.data.item[item_k]\n                            self.P[u] += self.lRate * (1 - sigmoid(self.P[u].dot(self.Q[i]) - self.P[u].dot(self.Q[k]))) * (\n                            self.Q[i] - self.Q[k])\n                            self.Q[i] += self.lRate * (1 - sigmoid(self.P[u].dot(self.Q[i]) - self.P[u].dot(self.Q[k]))) * \\\n                                         self.P[u]\n                            self.Q[k] -= self.lRate * (1 - sigmoid(self.P[u].dot(self.Q[i]) - self.P[u].dot(self.Q[k]))) * \\\n                                         self.P[u]\n\n                            item_j = ''\n                            # if len(self.NegativeSet[user])>0:\n                            #     item_j = choice(self.NegativeSet[user])\n                            # else:\n                            item_j = choice(itemList)\n                            while (self.PositiveSet[user].has_key(item_j) or self.IPositiveSet.has_key(item_j)):\n                                item_j = choice(itemList)\n                            j = self.data.item[item_j]\n                            self.P[u] += (1 / self.s) * self.lRate * (\n                            1 - sigmoid((1 / self.s) * (self.P[u].dot(self.Q[k]) - self.P[u].dot(self.Q[j])))) * (\n                                         self.Q[k] - self.Q[j])\n                            self.Q[k] += (1 / self.s) * self.lRate * (\n                            1 - sigmoid((1 / self.s) * (self.P[u].dot(self.Q[k]) - self.P[u].dot(self.Q[j])))) * self.P[u]\n                            self.Q[j] -= (1 / self.s) * self.lRate * (\n                            1 - sigmoid((1 / self.s) * (self.P[u].dot(self.Q[k]) - self.P[u].dot(self.Q[j])))) * self.P[u]\n\n                            self.P[u] -= self.lRate * self.regU * self.P[u]\n                            self.Q[i] -= self.lRate * self.regI * self.Q[i]\n                            self.Q[j] -= self.lRate * self.regI * self.Q[j]\n                            self.Q[k] -= self.lRate * self.regI * self.Q[k]\n\n                            self.loss += -log(sigmoid(self.P[u].dot(self.Q[i]) - self.P[u].dot(self.Q[k]))) - \\\n                                         log(sigmoid((1 / self.s) * (self.P[u].dot(self.Q[k]) - self.P[u].dot(self.Q[j]))))\n                        else:\n                            item_j = choice(itemList)\n                            while (self.PositiveSet[user].has_key(item_j)):\n                                item_j = choice(itemList)\n                            j = self.data.item[item_j]\n                            self.P[u] += self.lRate * (1 - sigmoid(self.P[u].dot(self.Q[i]) - self.P[u].dot(self.Q[j]))) * (\n                                self.Q[i] - self.Q[j])\n                            self.Q[i] += self.lRate * (1 - sigmoid(self.P[u].dot(self.Q[i]) - self.P[u].dot(self.Q[j]))) * \\\n                                         self.P[u]\n                            self.Q[j] -= self.lRate * (1 - sigmoid(self.P[u].dot(self.Q[i]) - self.P[u].dot(self.Q[j]))) * \\\n                                         self.P[u]\n\n                            self.loss += -log(sigmoid(self.P[u].dot(self.Q[i]) - self.P[u].dot(self.Q[j])))\n\n\n                self.loss += self.regU*(self.P*self.P).sum() + self.regI*(self.Q*self.Q).sum()\n            iteration += 1\n            if self.isConverged(iteration):\n                break\n\n    def predict(self,u,i):\n        if self.data.containsUser(u) and self.data.containsItem(i):\n            return sigmoid(self.P[self.data.user[u]].dot(self.Q[self.data.item[i]]))\n        elif self.data.containsUser(u) and not self.data.containsItem(i):\n            return self.data.userMeans[u]\n        elif not self.data.containsUser(u) and self.data.containsItem(i):\n            return self.data.itemMeans[i]\n        else:\n            return self.data.globalMean\n\n    def predictForRanking(self, u):\n        'invoked to rank all the items for the user'\n        if self.data.containsUser(u):\n            u = self.data.getUserId(u)\n            return self.Q.dot(self.P[u])\n        else:\n            return [self.data.globalMean] * self.num_items"""
algorithm/ranking/CoFactor.py,0,"b""from baseclass.IterativeRecommender import IterativeRecommender\nimport numpy as np\nfrom tool import config\nfrom collections import defaultdict\nfrom math import log,exp\nfrom scipy.sparse import *\nfrom scipy import *\n\nclass CoFactor(IterativeRecommender):\n    def __init__(self, conf, trainingSet=None, testSet=None, fold='[1]'):\n        super(CoFactor, self).__init__(conf, trainingSet, testSet, fold)\n\n    def readConfiguration(self):\n        super(CoFactor, self).readConfiguration()\n        extraSettings = config.LineConfig(self.config['CoFactor'])\n        self.negCount = int(extraSettings['-k']) #the number of negative samples\n        if self.negCount < 1:\n            self.negCount = 1\n        self.regR = float(extraSettings['-gamma'])\n        self.filter = int(extraSettings['-filter'])\n\n    def printAlgorConfig(self):\n        super(CoFactor, self).printAlgorConfig()\n        print 'Specified Arguments of', self.config['recommender'] + ':'\n        print 'k: %d' % self.negCount\n        print 'regR: %.5f' %self.regR\n        print 'filter: %d' %self.filter\n        print '=' * 80\n\n    def initModel(self):\n        super(CoFactor, self).initModel()\n\n        #constructing SPPMI matrix\n        self.SPPMI = defaultdict(dict)\n        print 'Constructing SPPMI matrix...'\n        #for larger data set has many items, the process will be time consuming\n        occurrence = defaultdict(dict)\n        i=0\n        for item1 in self.data.item:\n            i += 1\n            if i % 100 == 0:\n                print str(i) + '/' + str(self.num_items)\n            uList1, rList1 = self.data.itemRated(item1)\n\n            if len(uList1) < self.filter:\n                continue\n            for item2 in self.data.item:\n                if item1 == item2:\n                    continue\n                if not occurrence[item1].has_key(item2):\n                    uList2, rList2 = self.data.itemRated(item2)\n                    if len(uList2) < self.filter:\n                        continue\n                    count = len(set(uList1).intersection(set(uList2)))\n                    if count > self.filter:\n                        occurrence[item1][item2] = count\n                        occurrence[item2][item1] = count\n\n        maxVal = 0\n        frequency = {}\n        for item1 in occurrence:\n            frequency[item1] = sum(occurrence[item1].values()) * 1.0\n        D = sum(frequency.values()) * 1.0\n        # maxx = -1\n        for item1 in occurrence:\n            for item2 in occurrence[item1]:\n                try:\n                    val = max([log(occurrence[item1][item2] * D / (frequency[item1] * frequency[item2])) - log(\n                        self.negCount), 0])\n                except ValueError:\n                    print self.SPPMI[item1][item2]\n                    print self.SPPMI[item1][item2] * D / (frequency[item1] * frequency[item2])\n\n                if val > 0:\n                    if maxVal < val:\n                        maxVal = val\n                    self.SPPMI[item1][item2] = val\n                    self.SPPMI[item2][item1] = self.SPPMI[item1][item2]\n\n\n        #normalize\n        for item1 in self.SPPMI:\n            for item2 in self.SPPMI[item1]:\n                self.SPPMI[item1][item2] = self.SPPMI[item1][item2]/maxVal\n\n\n    def buildModel(self):\n        iteration = 0\n\n        self.X=self.P*10 #Theta\n        self.Y=self.Q*10 #Beta\n        self.w = np.random.rand(self.num_items) / 10  # bias value of item\n        self.c = np.random.rand(self.num_items) / 10  # bias value of context\n        self.G = np.random.rand(self.num_items, self.embed_size) / 10  # context embedding\n\n        print 'training...'\n        iteration = 0\n        while iteration < self.maxIter:\n            self.loss = 0\n            YtY = self.Y.T.dot(self.Y)\n            for user in self.data.user:\n                # C_u = np.ones(self.data.getSize(self.recType))\n                H = np.ones(self.num_items)\n                val, pos = [],[]\n                P_u = np.zeros(self.num_items)\n                uid = self.data.user[user]\n                for item in self.data.trainSet_u[user]:\n                    iid = self.data.item[item]\n                    r_ui = float(self.data.trainSet_u[user][item])\n                    pos.append(iid)\n                    val.append(10 * r_ui)\n                    H[iid] += 10 * r_ui\n                    P_u[iid] = 1\n                    error = (P_u[iid] - self.X[uid].dot(self.Y[iid]))\n                    self.loss += pow(error, 2)\n                # sparse matrix\n                C_u = coo_matrix((val, (pos, pos)), shape=(self.num_items, self.num_items))\n                A = (YtY + np.dot(self.Y.T, C_u.dot(self.Y)) + self.regU * np.eye(self.embed_size))\n                self.X[uid] = np.dot(np.linalg.inv(A), (self.Y.T * H).dot(P_u))\n\n            XtX = self.X.T.dot(self.X)\n            for item in self.data.item:\n                P_i = np.zeros(self.num_users)\n                iid = self.data.item[item]\n                H = np.ones(self.num_users)\n                val,pos = [],[]\n                for user in self.data.trainSet_i[item]:\n                    uid = self.data.user[user]\n                    r_ui = float(self.data.trainSet_i[item][user])\n                    pos.append(uid)\n                    val.append(10 * r_ui)\n                    H[uid] += 10 * r_ui\n                    P_i[uid] = 1\n\n                matrix_g1 = np.zeros((self.embed_size,self.embed_size))\n                matrix_g2 = np.zeros((self.embed_size,self.embed_size))\n                vector_m1 = np.zeros(self.embed_size)\n                vector_m2 = np.zeros(self.embed_size)\n                update_w = 0\n                update_c = 0\n\n                if len(self.SPPMI[item])>0:\n                    for context in self.SPPMI[item]:\n                        cid = self.data.item[context]\n                        gamma = self.G[cid]\n                        beta = self.Y[cid]\n                        matrix_g1 += gamma.reshape(self.embed_size,1).dot(gamma.reshape(1,self.embed_size))\n                        vector_m1 += (self.SPPMI[item][context]-self.w[iid]-\n                                      self.c[cid])*gamma\n\n                        matrix_g2 += beta.reshape(self.embed_size,1).dot(beta.reshape(1,self.embed_size))\n                        vector_m2 += (self.SPPMI[item][context] - self.w[cid]\n                                      - self.c[iid]) * beta\n\n                        update_w += self.SPPMI[item][context]-self.Y[iid].dot(gamma)-self.c[cid]\n                        update_c += self.SPPMI[item][context]-beta.dot(self.G[iid])-self.w[cid]\n\n                C_i = coo_matrix((val, (pos, pos)), shape=(self.num_users, self.num_users))\n                A = (XtX + np.dot(self.X.T, C_i.dot(self.X)) + self.regU * np.eye(self.embed_size) + matrix_g1)\n                self.Y[iid] = np.dot(np.linalg.inv(A), (self.X.T * H).dot(P_i)+vector_m1)\n                if len(self.SPPMI[item]) > 0:\n                    self.G[iid] = np.dot(np.linalg.inv(matrix_g2+self.regR * np.eye(self.embed_size)),vector_m2)\n                    self.w[iid] = update_w/len(self.SPPMI[item])\n                    self.c[iid] = update_c/len(self.SPPMI[item])\n\n            # self.loss += (self.X * self.X).sum() + (self.Y * self.Y).sum()\n            iteration += 1\n            print 'iteration:', iteration, 'loss:', self.loss\n            # if self.isConverged(iteration):\n            #     break\n\n    def predictForRanking(self,u):\n        'invoked to rank all the items for the user'\n        if self.data.containsUser(u):\n            u = self.data.getUserId(u)\n            return self.Y.dot(self.X[u])\n        else:\n            return [self.data.globalMean] * self.num_items\n"""
algorithm/ranking/DMF.py,31,"b'#coding:utf8\nfrom baseclass.DeepRecommender import DeepRecommender\nimport numpy as np\nfrom random import choice,random,randint,shuffle\nfrom tool import config\nimport tensorflow as tf\n\n\n#According to the paper, we only\nclass DMF(DeepRecommender):\n\n    def __init__(self,conf,trainingSet=None,testSet=None,fold=\'[1]\'):\n        super(DMF, self).__init__(conf,trainingSet,testSet,fold)\n\n\n    def next_batch(self,i):\n        rows = np.zeros(((self.negative_sp+1)*self.batch_size,self.num_items))\n        cols = np.zeros(((self.negative_sp+1)*self.batch_size,self.num_users))\n        batch_idx = range(self.batch_size*i,self.batch_size*(i+1))\n\n        users = [self.data.trainingData[idx][0] for idx in batch_idx]\n        items = [self.data.trainingData[idx][1] for idx in batch_idx]\n        u_idx = [self.data.user[u] for u in users]\n        v_idx = [self.data.item[i] for i in items]\n        ratings = [float(self.data.trainingData[idx][2]) for idx in batch_idx]\n\n        for i,user in enumerate(users):\n            rows[i] = self.data.row(user)\n        for i,item in enumerate(items):\n            cols[i] = self.data.col(item)\n\n        userList = self.data.user.keys()\n        itemList = self.data.item.keys()\n        #negative sample\n        for i in range(self.negative_sp*self.batch_size):\n            u = choice(userList)\n            v = choice(itemList)\n            while self.data.contains(u,v):\n                u = choice(userList)\n                v = choice(itemList)\n            rows[self.batch_size-1+i]=self.data.row(u)\n            cols[self.batch_size-1+i]=self.data.col(i)\n            u_idx.append(self.data.user[u])\n            v_idx.append(self.data.item[v])\n            ratings.append(0)\n        return rows,cols,np.array(ratings),np.array(u_idx),np.array(v_idx)\n\n    def initModel(self):\n        super(DMF, self).initModel()\n        n_input_u = len(self.data.item)\n        n_input_i = len(self.data.user)\n        self.negative_sp = 5\n        self.n_hidden_u=[256,512]\n        self.n_hidden_i=[256,512]\n        self.input_u = tf.placeholder(tf.float, [None, n_input_u])\n        self.input_i = tf.placeholder(tf.float, [None, n_input_i])\n\n\n    def buildModel(self):\n        super(DMF, self).buildModel_tf()\n\n        initializer = tf.contrib.layers.xavier_initializer()\n        #user net\n        user_W1 = tf.Variable(initializer([self.num_items, self.n_hidden_u[0]],stddev=0.01))\n        self.user_out = tf.nn.relu(tf.matmul(self.input_u, user_W1))\n        self.regLoss = tf.nn.l2_loss(user_W1)\n        for i in range(1, len(self.n_hidden_u)):\n            W = tf.Variable(initializer([self.n_hidden_u[i-1], self.n_hidden_u[i]],stddev=0.01))\n            b = tf.Variable(initializer([self.n_hidden_u[i]],stddev=0.01))\n            self.regLoss = tf.add(self.regLoss,tf.nn.l2_loss(W))\n            self.regLoss = tf.add(self.regLoss, tf.nn.l2_loss(b))\n            self.user_out = tf.nn.relu(tf.add(tf.matmul(self.user_out, W), b))\n\n        #item net\n        item_W1 = tf.Variable(initializer([self.num_users, self.n_hidden_i[0]],stddev=0.01))\n        self.item_out = tf.nn.relu(tf.matmul(self.input_i, item_W1))\n        self.regLoss = tf.add(self.regLoss, tf.nn.l2_loss(item_W1))\n        for i in range(1, len(self.n_hidden_i)):\n            W = tf.Variable(initializer([self.n_hidden_i[i-1], self.n_hidden_i[i]],stddev=0.01))\n            b = tf.Variable(initializer([self.n_hidden_i[i]],stddev=0.01))\n            self.regLoss = tf.add(self.regLoss, tf.nn.l2_loss(W))\n            self.regLoss = tf.add(self.regLoss, tf.nn.l2_loss(b))\n            self.item_out = tf.nn.relu(tf.add(tf.matmul(self.item_out, W), b))\n\n        norm_user_output = tf.sqrt(tf.reduce_sum(tf.square(self.user_out), axis=1))\n        norm_item_output = tf.sqrt(tf.reduce_sum(tf.square(self.item_out), axis=1))\n\n        self.y_ = tf.reduce_sum(tf.multiply(self.user_out, self.item_out), axis=1) / (\n                norm_item_output * norm_user_output)\n        self.y_ = tf.maximum(1e-6, self.y_)\n\n        self.loss = self.r*tf.log(self.y_) + (1 - self.r) * tf.log(1 - self.y_)#tf.nn.sigmoid_cross_entropy_with_logits(logits=self.y_,labels=self.r)\n        #self.loss = tf.nn.l2_loss(tf.subtract(self.y_,self.r))\n        self.loss = -tf.reduce_sum(self.loss)\n        reg_lambda = tf.constant(self.regU, dtype=tf.float32)\n        self.regLoss = tf.multiply(reg_lambda,self.regLoss)\n        self.loss = tf.add(self.loss,self.regLoss)\n\n        optimizer = tf.train.AdamOptimizer(self.lRate).minimize(self.loss)\n\n        self.U = np.zeros((self.num_users, self.n_hidden_u[-1]))\n        self.V = np.zeros((self.num_items, self.n_hidden_u[-1]))\n\n\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n\n        total_batch = int(len(self.data.trainingData)/ self.batch_size)\n        for epoch in range(self.maxIter):\n            shuffle(self.data.trainingData)\n            for i in range(total_batch):\n                users,items,ratings,u_idx,v_idx = self.next_batch(i)\n\n                shuffle_idx=np.random.permutation(range(len(users)))\n                users = users[shuffle_idx]\n                items = items[shuffle_idx]\n                ratings = ratings[shuffle_idx]\n                u_idx = u_idx[shuffle_idx]\n                v_idx = v_idx[shuffle_idx]\n\n                _,loss= self.sess.run([optimizer, self.loss], feed_dict={self.input_u: users,self.input_i:items,self.r:ratings})\n                print self.foldInfo, ""Epoch:"", \'%04d\' % (epoch + 1), ""Batch:"", \'%03d\' % (i + 1), ""loss="", ""{:.9f}"".format(loss)\n            #save the output layer\n\n                U_embedding, V_embedding = self.sess.run([self.user_out, self.item_out], feed_dict={self.input_u: users,self.input_i:items})\n                for ue,u in zip(U_embedding,u_idx):\n                    self.U[u]=ue\n                for ve,v in zip(V_embedding,v_idx):\n                    self.V[v]=ve\n            self.normalized_V = np.sqrt(np.sum(self.V * self.V, axis=1))\n            self.normalized_U = np.sqrt(np.sum(self.U * self.U, axis=1))\n            self.ranking_performance()\n        print(""Optimization Finished!"")\n\n\n\n\n    def predictForRanking(self, u):\n        \'invoked to rank all the items for the user\'\n        if self.data.containsUser(u):\n            uid = self.data.user[u]\n            return np.divide(self.V.dot(self.U[uid]),self.normalized_U[uid]*self.normalized_V)\n        else:\n            return [self.data.globalMean] * self.num_items\n\n\n'"
algorithm/ranking/ExpoMF.py,0,"b""from baseclass.IterativeRecommender import IterativeRecommender\nfrom scipy.sparse import *\nfrom scipy import *\nimport numpy as np\n\n\nfrom numpy import linalg as LA\n\nfrom joblib import Parallel, delayed\nfrom math import sqrt\n\nEPS = 1e-8\n# this algorithm refers to the following paper:\n# #########----  Modeling User Exposure in Recommendation   ----#############\n\nclass ExpoMF(IterativeRecommender):\n    def __init__(self,conf,trainingSet=None,testSet=None,fold='[1]'):\n        super(ExpoMF, self).__init__(conf,trainingSet,testSet,fold)\n\n    def initModel(self):\n        super(ExpoMF, self).initModel()\n        self.lam_theta = 1e-5\n        self.lam_beta = 1e-5\n        self.lam_y = 1.0\n        self.init_mu = 0.01\n        self.a = 1.0\n        self.b = 99.0\n        self.init_std = 0.01\n        self.theta = self.init_std * \\\n            np.random.randn(self.num_users, self.embed_size).astype(np.float32)\n        self.beta = self.init_std * \\\n            np.random.randn(self.num_items, self.embed_size).astype(np.float32)\n        self.mu = self.init_mu * np.ones(self.num_items, dtype=np.float32)\n        self.n_jobs=4\n        self.batch_size=300\n        row,col,val = [],[],[]\n        for user in self.data.trainSet_u:\n            for item in self.data.trainSet_u[user]:\n                u = self.data.user[user]\n                i = self.data.item[item]\n                row.append(u)\n                col.append(i)\n                val.append(1)\n\n        self.X = csr_matrix((np.array(val),(np.array(row),np.array(col))),(self.num_users,self.num_items))\n\n    def buildModel(self):\n        print 'training...'\n        n_users = self.X.shape[0]\n        XT = self.X.T.tocsr()  # pre-compute this\n        for i in xrange(self.maxIter):\n            print 'ITERATION #%d' % i\n            self._update_factors(self.X, XT)\n            self._update_expo(self.X, n_users)\n\n\n    def _update_factors(self, X, XT):\n        '''Update user and item collaborative factors with ALS'''\n        self.theta = recompute_factors(self.beta, self.theta, X,\n                                       self.lam_theta / self.lam_y,\n                                       self.lam_y,\n                                       self.mu,\n                                       self.n_jobs,\n                                       batch_size=self.batch_size)\n\n        self.beta = recompute_factors(self.theta, self.beta, XT,\n                                      self.lam_beta / self.lam_y,\n                                      self.lam_y,\n                                      self.mu,\n                                      self.n_jobs,\n                                      batch_size=self.batch_size)\n\n\n    def _update_expo(self, X, n_users):\n        '''Update exposure prior'''\n        print '\\tUpdating exposure prior...'\n\n        start_idx = range(0, n_users, self.batch_size)\n        end_idx = start_idx[1:] + [n_users]\n\n        A_sum = np.zeros_like(self.mu)\n        for lo, hi in zip(start_idx, end_idx):\n            A_sum += a_row_batch(X[lo:hi], self.theta[lo:hi], self.beta,\n                                 self.lam_y, self.mu).sum(axis=0)\n        print self.mu\n        self.mu = (self.a + A_sum - 1) / (self.a + self.b + n_users - 2)\n\n\n    def predictForRanking(self,u):\n        'invoked to rank all the items for the user'\n        if self.data.containsUser(u):\n            u = self.data.getUserId(u)\n            return self.beta.dot(self.theta[u])\n        else:\n            return [self.data.globalMean] * self.num_items\n\n# Utility functions #\n\n\n\ndef get_row(Y, i):\n    '''Given a scipy.sparse.csr_matrix Y, get the values and indices of the\n    non-zero values in i_th row'''\n    lo, hi = Y.indptr[i], Y.indptr[i + 1]\n    return Y.data[lo:hi], Y.indices[lo:hi]\n\ndef a_row_batch(Y_batch, theta_batch, beta, lam_y, mu):\n    '''Compute the posterior of exposure latent variables A by batch'''\n    pEX = sqrt(lam_y / 2 / np.pi) * \\\n          np.exp(-lam_y * theta_batch.dot(beta.T) ** 2 / 2)\n    #print pEX.shape,mu.shape\n    A = (pEX + EPS) / (pEX + EPS + (1 - mu) / mu)\n    A[Y_batch.nonzero()] = 1.\n    return A\n\ndef _solve(k, A_k, X, Y, f, lam, lam_y, mu):\n    '''Update one single factor'''\n    s_u, i_u = get_row(Y, k)\n    a = np.dot(s_u * A_k[i_u], X[i_u])\n    B = X.T.dot(A_k[:, np.newaxis] * X) + lam * np.eye(f)\n    return LA.solve(B, a)\n\ndef _solve_batch(lo, hi, X, X_old_batch, Y, m, f, lam, lam_y, mu):\n    '''Update factors by batch, will eventually call _solve() on each factor to\n    keep the parallel process busy'''\n    assert X_old_batch.shape[0] == hi - lo\n\n    if mu.size == X.shape[0]:  # update users\n        A_batch = a_row_batch(Y[lo:hi], X_old_batch, X, lam_y, mu)\n    else:  # update items\n        A_batch = a_row_batch(Y[lo:hi], X_old_batch, X, lam_y, mu[lo:hi,\n                                                               np.newaxis])\n\n    X_batch = np.empty_like(X_old_batch, dtype=X_old_batch.dtype)\n    for ib, k in enumerate(xrange(lo, hi)):\n        X_batch[ib] = _solve(k, A_batch[ib], X, Y, f, lam, lam_y, mu)\n    return X_batch\n\ndef recompute_factors(X, X_old, Y, lam, lam_y, mu, n_jobs, batch_size=1000):\n    '''Regress X to Y with exposure matrix (computed on-the-fly with X_old) and\n    ridge term lam by embarrassingly parallelization. All the comments below\n    are in the view of computing user factors'''\n    m, n = Y.shape  # m = number of users, n = number of items\n    assert X.shape[0] == n\n    assert X_old.shape[0] == m\n    f = X.shape[1]  # f = number of factors\n\n    start_idx = range(0, m, batch_size)\n    end_idx = start_idx[1:] + [m]\n\n    res = Parallel(n_jobs=n_jobs)(delayed(_solve_batch)(\n        lo, hi, X, X_old[lo:hi], Y, m, f, lam, lam_y, mu)\n                                  for lo, hi in zip(start_idx, end_idx))\n\n    X_new = np.vstack(res)\n    return X_new\n\n"""
algorithm/ranking/IF_BPR.py,0,"b""from baseclass.SocialRecommender import SocialRecommender\nfrom tool import config\nfrom random import shuffle, choice\nfrom collections import defaultdict\nimport numpy as np\nfrom tool.qmath import sigmoid, cosine\nfrom math import log\nimport gensim.models.word2vec as w2v\n\n\n\nclass IF_BPR(SocialRecommender):\n    def __init__(self, conf, trainingSet=None, testSet=None, relation=None, fold='[1]'):\n        SocialRecommender.__init__(self, conf=conf, trainingSet=trainingSet, testSet=testSet, relation=relation,fold=fold)\n\n    def readConfiguration(self):\n        super(IF_BPR, self).readConfiguration()\n        options = config.LineConfig(self.config['IF_BPR'])\n        self.walkCount = int(options['-T'])\n        self.walkLength = int(options['-L'])\n        self.walkDim = int(options['-l'])\n        self.winSize = int(options['-w'])\n        self.topK = int(options['-k'])\n        self.alpha = float(options['-a'])\n        self.epoch = int(options['-ep'])\n        self.neg = int(options['-neg'])\n        self.rate = float(options['-r'])\n\n    def printAlgorConfig(self):\n        super(IF_BPR, self).printAlgorConfig()\n        print 'Specified Arguments of', self.config['recommender'] + ':'\n        print 'Walks count per user', self.walkCount\n        print 'Length of each walk', self.walkLength\n        print 'Dimension of user embedding', self.walkDim\n        print '=' * 80\n\n    def readNegativeFeedbacks(self):\n        self.negative = defaultdict(list)\n        self.nItems = defaultdict(list)\n        filename = self.config['ratings'][:-4]+'_n.txt'\n        with open(filename) as f:\n            for line in f:\n                items = line.strip().split()\n                self.negative[items[0]].append(items[1])\n                self.nItems[items[1]].append(items[0])\n                if items[0] not in self.data.user:\n                    self.data.user[items[0]]=len(self.data.user)\n                    self.data.id2user[self.data.user[items[0]]] = items[0]\n\n    def initModel(self):\n        super(IF_BPR, self).initModel()\n        self.positive = defaultdict(list)\n        self.pItems = defaultdict(list)\n        for user in self.data.trainSet_u:\n            for item in self.data.trainSet_u[user]:\n                self.positive[user].append(item)\n                self.pItems[item].append(user)\n        self.readNegativeFeedbacks()\n        self.P = np.ones((len(self.data.user), self.embed_size))*0.1  # latent user matrix\n        self.threshold = {}\n        self.avg_sim = {}\n        self.thres_d = dict.fromkeys(self.data.user.keys(),0) #derivatives for learning thresholds\n        self.thres_count = dict.fromkeys(self.data.user.keys(),0)\n\n        print 'Preparing item sets...'\n        self.PositiveSet = defaultdict(dict)\n        self.NegSets = defaultdict(dict)\n\n        for user in self.data.user:\n            for item in self.data.trainSet_u[user]:\n                self.PositiveSet[user][item] = 1\n\n        for user in self.data.user:\n            for item in self.negative[user]:\n                if self.data.item.has_key(item):\n                    self.NegSets[user][item] = 1\n\n    def randomWalks(self):\n        print 'Kind Note: This method will probably take much time.'\n        # build U-F-NET\n        print 'Building weighted user-friend network...'\n        # filter isolated nodes and low ratings\n        # Definition of Meta-Path\n        p1 = 'UIU'\n        p2 = 'UFU'\n        p3 = 'UTU'\n        p4 = 'UFIU'\n        p5 = 'UFUIU'\n        mPaths = [p1, p2, p3, p4, p5]\n\n        self.G = np.random.rand(self.data.trainingSize()[0], self.walkDim) * 0.1\n        self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) * 0.1\n\n        self.UFNet = defaultdict(list) # a -> b #a trusts b\n        for u in self.social.followees:\n            s1 = set(self.social.followees[u])\n            for v in self.social.followees[u]:\n                if v in self.social.followees:  # make sure that v has out links\n                    if u <> v:\n                        s2 = set(self.social.followees[v])\n                        weight = len(s1.intersection(s2))\n                        self.UFNet[u] += [v] * (weight + 1)\n\n        self.UTNet = defaultdict(list) # a <- b #a is trusted by b\n        for u in self.social.followers:\n            s1 = set(self.social.followers[u])\n            for v in self.social.followers[u]:\n                if self.social.followers.has_key(v):  # make sure that v has out links\n                    if u <> v:\n                        s2 = set(self.social.followers[v])\n                        weight = len(s1.intersection(s2))\n                        self.UTNet[u] += [v] * (weight + 1)\n\n        print 'Generating random meta-path random walks... (Positive)'\n        self.pWalks = []\n        # self.usercovered = {}\n\n        # positive\n        for user in self.data.user:\n            for mp in mPaths:\n                if mp == p1:\n                    self.walkCount = 10\n                if mp == p2:\n                    self.walkCount = 8\n                if mp == p3:\n                    self.walkCount = 8\n                if mp == p4:\n                    self.walkCount = 5\n                if mp == p5:\n                    self.walkCount = 5\n                for t in range(self.walkCount):\n                    path = ['U' + user]\n                    lastNode = user\n                    nextNode = user\n                    lastType = 'U'\n                    for i in range(self.walkLength / len(mp[1:])):\n                        for tp in mp[1:]:\n                            try:\n                                if tp == 'I':\n                                    nextNode = choice(self.positive[lastNode])\n\n                                if tp == 'U':\n                                    if lastType == 'I':\n                                        nextNode = choice(self.pItems[lastNode])\n                                    elif lastType == 'F':\n                                        nextNode = choice(self.UFNet[lastNode])\n                                        while not self.data.user.has_key(nextNode):\n                                            nextNode = choice(self.UFNet[lastNode])\n                                    elif lastType == 'T':\n                                        nextNode = choice(self.UTNet[lastNode])\n                                        while not self.data.user.has_key(nextNode):\n                                            nextNode = choice(self.UTNet[lastNode])\n\n                                if tp == 'F':\n                                    nextNode = choice(self.UFNet[lastNode])\n                                    while not self.data.user.has_key(nextNode):\n                                        nextNode = choice(self.UFNet[lastNode])\n\n                                if tp == 'T':\n                                    nextNode = choice(self.UTNet[lastNode])\n                                    while not self.data.user.has_key(nextNode):\n                                        nextNode = choice(self.UTNet[lastNode])\n\n                                path.append(tp + nextNode)\n                                lastNode = nextNode\n                                lastType = tp\n\n                            except (KeyError, IndexError):\n                                path = []\n                                break\n\n                    if path:\n                        self.pWalks.append(path)\n\n        self.nWalks = []\n        # self.usercovered = {}\n\n        # negative\n        for user in self.data.user:\n            for mp in mPaths:\n                if mp == p1:\n                    self.walkCount = 10\n                if mp == p2:\n                    self.walkCount = 8\n                if mp == p3:\n                    self.walkCount = 8\n                if mp == p4:\n                    self.walkCount = 5\n                if mp == p5:\n                    self.walkCount = 5\n                for t in range(self.walkCount):\n                    path = ['U' + user]\n                    lastNode = user\n                    nextNode = user\n                    lastType = 'U'\n                    for i in range(self.walkLength / len(mp[1:])):\n                        for tp in mp[1:]:\n                            try:\n                                if tp == 'I':\n                                    nextNode = choice(self.negative[lastNode])\n\n                                if tp == 'U':\n                                    if lastType == 'I':\n                                        nextNode = choice(self.nItems[lastNode])\n                                    elif lastType == 'F':\n                                        nextNode = choice(self.UFNet[lastNode])\n                                        while not self.data.user.has_key(nextNode):\n                                            nextNode = choice(self.UFNet[lastNode])\n                                    elif lastType == 'T':\n                                        nextNode = choice(self.UTNet[lastNode])\n                                        while not self.data.user.has_key(nextNode):\n                                            nextNode = choice(self.UTNet[lastNode])\n\n                                if tp == 'F':\n                                    nextNode = choice(self.UFNet[lastNode])\n                                    while not self.data.user.has_key(nextNode):\n                                        nextNode = choice(self.UFNet[lastNode])\n\n                                if tp == 'T':\n                                    nextNode = choice(self.UTNet[lastNode])\n                                    while not self.data.user.has_key(nextNode):\n                                        nextNode = choice(self.UTNet[lastNode])\n\n                                path.append(tp + nextNode)\n                                lastNode = nextNode\n                                lastType = tp\n\n                            except (KeyError, IndexError):\n                                path = []\n                                break\n\n                    if path:\n                        self.nWalks.append(path)\n\n        shuffle(self.pWalks)\n        print 'pwalks:', len(self.pWalks)\n        print 'nwalks:', len(self.nWalks)\n\n    def computeSimilarity(self):\n        # Training get top-k friends\n        print 'Generating user embedding...'\n        self.pTopKSim = {}\n        self.nTopKSim = {}\n        self.pSimilarity = defaultdict(dict)\n        self.nSimilarity = defaultdict(dict)\n        pos_model = w2v.Word2Vec(self.pWalks, size=self.walkDim, window=5, min_count=0, iter=10)\n        neg_model = w2v.Word2Vec(self.nWalks, size=self.walkDim, window=5, min_count=0, iter=10)\n        for user in self.positive:\n            uid = self.data.user[user]\n            try:\n                self.W[uid] = pos_model.wv['U' + user]\n            except KeyError:\n                continue\n        for user in self.negative:\n            uid = self.data.user[user]\n            try:\n                self.G[uid] = neg_model.wv['U' + user]\n            except KeyError:\n                continue\n        print 'User embedding generated.'\n\n        print 'Constructing similarity matrix...'\n        i = 0\n        for user1 in self.positive:\n            uSim = []\n            i += 1\n            if i % 200 == 0:\n                print i, '/', len(self.positive)\n            vec1 = self.W[self.data.user[user1]]\n            for user2 in self.positive:\n                if user1 <> user2:\n                    vec2 = self.W[self.data.user[user2]]\n                    sim = cosine(vec1, vec2)\n                    uSim.append((user2, sim))\n            fList = sorted(uSim, key=lambda d: d[1], reverse=True)[:self.topK]\n            self.threshold[user1] = fList[self.topK / 2][1]\n            for pair in fList:\n                self.pSimilarity[user1][pair[0]] = pair[1]\n            self.pTopKSim[user1] = [item[0] for item in fList]\n            self.avg_sim[user1] = sum([item[1] for item in fList][:self.topK / 2]) / (self.topK / 2)\n\n        i = 0\n        for user1 in self.negative:\n            uSim = []\n            i += 1\n            if i % 200 == 0:\n                print i, '/', len(self.negative)\n            vec1 = self.G[self.data.user[user1]]\n            for user2 in self.negative:\n                if user1 <> user2:\n                    vec2 = self.G[self.data.user[user2]]\n                    sim = cosine(vec1, vec2)\n                    uSim.append((user2, sim))\n            fList = sorted(uSim, key=lambda d: d[1], reverse=True)[:self.topK]\n            for pair in fList:\n                self.nSimilarity[user1][pair[0]] = pair[1]\n            self.nTopKSim[user1] = [item[0] for item in fList]\n\n        self.trueTopKFriends = defaultdict(list)\n        for user in self.pTopKSim:\n            trueFriends = list(set(self.pTopKSim[user]).intersection(set(self.nTopKSim[user])))\n            self.trueTopKFriends[user] = trueFriends\n            self.pTopKSim[user] = list(set(self.pTopKSim[user]).difference(set(trueFriends)))\n\n    def updateSets(self):\n        self.JointSet = defaultdict(dict)\n        self.PS_Set = defaultdict(dict)\n        for user in self.data.user:\n            if user in self.trueTopKFriends:\n                for friend in self.trueTopKFriends[user]:\n                    if friend in self.data.user and self.pSimilarity[user][friend] >= self.threshold[user]:\n                        for item in self.positive[friend]:\n                            if item not in self.PositiveSet[user] and item not in self.NegSets[user]:\n                                self.JointSet[user][item] = friend\n\n            if self.pTopKSim.has_key(user):\n                for friend in self.pTopKSim[user][:self.topK]:\n                    if friend in self.data.user and self.pSimilarity[user][friend] >= self.threshold[user]:\n                        for item in self.positive[friend]:\n                            if item not in self.PositiveSet[user] and item not in self.JointSet[user] \\\n                                    and item not in self.NegSets[user]:\n                                self.PS_Set[user][item] = friend\n\n            if self.nTopKSim.has_key(user):\n                for friend in self.nTopKSim[user][:self.topK]:\n                    if friend in self.data.user and self.nSimilarity[user][friend]>=self.threshold[user]:\n                        for item in self.negative[friend]:\n                            if item in self.data.item:\n                                if item not in self.PositiveSet[user] and item not in self.JointSet[user] \\\n                                        and item not in self.PS_Set[user]:\n                                    self.NegSets[user][item] = friend\n\n    def buildModel(self):\n\n        self.randomWalks()\n        self.computeSimilarity()\n\n        print 'Decomposing...'\n        iteration = 0\n        while iteration < self.maxIter:\n            self.loss = 0\n            self.updateSets()\n            itemList = self.data.item.keys()\n            for user in self.PositiveSet:\n                #itemList = self.NegSets[user].keys()\n                kItems = self.JointSet[user].keys()\n                pItems = self.PS_Set[user].keys()\n                nItems = self.NegSets[user].keys()\n\n                u = self.data.user[user]\n\n                for item in self.PositiveSet[user]:\n                    i = self.data.item[item]\n                    selectedItems = [i]\n                    #select items from different sets\n                    if len(kItems) > 0:\n                        item_k = choice(kItems)                        \n                        uf = self.JointSet[user][item_k]\n                        k = self.data.item[item_k]\n                        selectedItems.append(k)\n                        self.optimization_thres(u,i,k,user,uf)\n                    if len(pItems)>0:\n                        item_p = choice(pItems)\n                        p = self.data.item[item_p]\n                        selectedItems.append(p)\n\n                    item_r = choice(itemList)\n                    while item_r in self.PositiveSet[user] or item_r in self.JointSet[user]\\\n                        or item_r in self.PS_Set[user] or item_r in self.NegSets[user]:\n                        item_r = choice(itemList)\n                    r = self.data.item[item_r]\n                    selectedItems.append(r)\n\n                    if len(nItems)>0:\n                        item_n = choice(nItems)\n                        n = self.data.item[item_n]\n                        selectedItems.append(n)\n\n                    #optimization\n                    for ind,item in enumerate(selectedItems[:-1]):\n                        self.optimization(u,item,selectedItems[ind+1])\n\n\n                if self.thres_count[user]>0:\n                    self.threshold[user] -= self.lRate * self.thres_d[user] / self.thres_count[user]\n                    self.thres_d[user]=0\n                    self.thres_count[user]=0\n                    li = [sim for sim in self.pSimilarity[user].values() if sim>=self.threshold[user]]\n                    if len(li)==0:\n                        self.avg_sim[user] = self.threshold[user]\n                    else:\n                        self.avg_sim[user]= sum(li)/(len(li)+0.0)\n\n                for friend in self.trueTopKFriends[user]:\n                    if self.pSimilarity[user][friend]>self.threshold[user]:\n                        u = self.data.user[user]\n                        f = self.data.user[friend]\n                        self.P[u] -= self.alpha*self.lRate*(self.P[u]-self.P[f])\n\n            self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()\n            iteration += 1\n            if self.isConverged(iteration):\n                 break\n            print self.foldInfo,'iteration:',iteration\n        self.ranking_performance()\n\n\n    def optimization(self, u, i, j):\n        s = sigmoid(self.P[u].dot(self.Q[i]) - self.P[u].dot(self.Q[j]))\n        self.P[u] += self.lRate * (1 - s) * (self.Q[i] - self.Q[j])\n        self.Q[i] += self.lRate * (1 - s) * self.P[u]\n        self.Q[j] -= self.lRate * (1 - s) * self.P[u]\n        self.loss += -log(s)\n        self.P[u] -= self.lRate * self.regU * self.P[u]\n        self.Q[i] -= self.lRate * self.regI * self.Q[i]\n        self.Q[j] -= self.lRate * self.regI * self.Q[j]\n\n    def optimization_thres(self, u, i, j,user,friend):\n        #print 'inner', (self.pSimilarity[user][friend]-self.threshold[user])/(self.avg_sim[user]-self.threshold[user])\n        try:\n            g_theta = sigmoid((self.pSimilarity[user][friend]-self.threshold[user])/(self.avg_sim[user]-self.threshold[user]))\n        except OverflowError:\n            print 'threshold',self.threshold[user],'smilarity',self.pSimilarity[user][friend],'avg',self.avg_sim[user]\n            print (self.pSimilarity[user][friend]-self.threshold[user]),(self.avg_sim[user]-self.threshold[user])\n            print (self.pSimilarity[user][friend]-self.threshold[user])/(self.avg_sim[user]-self.threshold[user])\n            exit(-1)\n        #print 'g_theta',g_theta\n\n        s = sigmoid((self.P[u].dot(self.Q[i]) - self.P[u].dot(self.Q[j]))/(1+g_theta))\n        self.P[u] += self.lRate * (1 - s) * (self.Q[i] - self.Q[j])\n        self.Q[i] += self.lRate * (1 - s) * self.P[u]\n        self.Q[j] -= self.lRate * (1 - s) * self.P[u]\n        self.loss += -log(s)\n        self.P[u] -= self.lRate * self.regU * self.P[u]\n        self.Q[i] -= self.lRate * self.regI * self.Q[i]\n        self.Q[j] -= self.lRate * self.regI * self.Q[j]\n        t_derivative = -g_theta*(1-g_theta)*(1-s)*(self.P[u].dot(self.Q[i]) - self.P[u].dot(self.Q[j]))\\\n                       *(self.pSimilarity[user][friend]-self.avg_sim[user])/(self.avg_sim[user]-\n                       self.threshold[user])**2/(1+g_theta)**2 + 0.005*self.threshold[user]\n        #print 'derivative', t_derivative\n        self.thres_d[user] += t_derivative\n        self.thres_count[user] += 1\n\n    def predictForRanking(self, u):\n        'invoked to rank all the items for the user'\n        if self.data.containsUser(u):\n            u = self.data.getUserId(u)\n            return self.Q.dot(self.P[u])\n        else:\n            return [self.data.globalMean] * self.num_items\n\n"""
algorithm/ranking/IRGAN.py,52,"b""#coding:utf8\nfrom baseclass.DeepRecommender import DeepRecommender\nimport numpy as np\nfrom random import randint,choice\ntry:\n    import tensorflow as tf\nexcept ImportError:\n    print 'This method can only run on tensorflow!'\n    exit(-1)\nfrom tensorflow import set_random_seed\nset_random_seed(2)\n\n###\n\n# We just transformed the code released by the authors to RecQ and slightly modified the code in order to invoke\n# the interfaces of RecQ\n\n###\nclass GEN():\n    def __init__(self, itemNum, userNum, emb_dim, lamda, param=None, initdelta=0.05, learning_rate=0.05):\n        self.itemNum = itemNum\n        self.userNum = userNum\n        self.emb_dim = emb_dim\n        self.lamda = lamda  # regularization parameters\n        self.param = param\n        self.initdelta = initdelta\n        self.learning_rate = learning_rate\n        self.g_params = []\n\n        with tf.variable_scope('generator'):\n            if self.param == None:\n                self.user_embeddings = tf.Variable(\n                    tf.random_uniform([self.userNum, self.emb_dim], minval=-self.initdelta, maxval=self.initdelta,\n                                      dtype=tf.float32))\n                self.item_embeddings = tf.Variable(\n                    tf.random_uniform([self.itemNum, self.emb_dim], minval=-self.initdelta, maxval=self.initdelta,\n                                      dtype=tf.float32))\n                self.item_bias = tf.Variable(tf.zeros([self.itemNum]))\n            else:\n                self.user_embeddings = tf.Variable(self.param[0])\n                self.item_embeddings = tf.Variable(self.param[1])\n                self.item_bias = tf.Variable(param[2])\n\n            self.g_params = [self.user_embeddings, self.item_embeddings, self.item_bias]\n\n        self.u = tf.placeholder(tf.int32)\n        self.i = tf.placeholder(tf.int32)\n        self.reward = tf.placeholder(tf.float32)\n\n        self.u_embedding = tf.nn.embedding_lookup(self.user_embeddings, self.u)\n        self.i_embedding = tf.nn.embedding_lookup(self.item_embeddings, self.i)\n        self.i_bias = tf.gather(self.item_bias, self.i)\n\n        self.all_logits = tf.reduce_sum(tf.multiply(self.u_embedding, self.item_embeddings), 1) + self.item_bias\n        self.i_prob = tf.gather(\n            tf.reshape(tf.nn.softmax(tf.reshape(self.all_logits, [1, -1])), [-1]),\n            self.i)\n\n        self.gan_loss = -tf.reduce_mean(tf.log(self.i_prob) * self.reward) + self.lamda * (\n            tf.nn.l2_loss(self.u_embedding) + tf.nn.l2_loss(self.i_embedding) + tf.nn.l2_loss(self.i_bias))\n\n        g_opt = tf.train.AdamOptimizer(self.learning_rate)\n        self.gan_updates = g_opt.minimize(self.gan_loss, var_list=self.g_params)\n\n        # for test stage, self.u: [self.batch_size]\n        self.all_rating = tf.matmul(self.u_embedding, self.item_embeddings, transpose_a=False,\n                                    transpose_b=True) + self.item_bias\n\n    # def save_model(self, sess, filename):\n    #     param = sess.run(self.g_params)\n    #     cPickle.dump(param, open(filename, 'w'))\n    \n\nclass DIS():\n    def __init__(self, itemNum, userNum, emb_dim, lamda, param=None, initdelta=0.05, learning_rate=0.05):\n        self.itemNum = itemNum\n        self.userNum = userNum\n        self.emb_dim = emb_dim\n        self.lamda = lamda  # regularization parameters\n        self.param = param\n        self.initdelta = initdelta\n        self.learning_rate = learning_rate\n        self.d_params = []\n\n        with tf.variable_scope('discriminator'):\n            if self.param == None:\n                self.user_embeddings = tf.Variable(\n                    tf.random_uniform([self.userNum, self.emb_dim], minval=-self.initdelta, maxval=self.initdelta,\n                                      dtype=tf.float32))\n                self.item_embeddings = tf.Variable(\n                    tf.random_uniform([self.itemNum, self.emb_dim], minval=-self.initdelta, maxval=self.initdelta,\n                                      dtype=tf.float32))\n                self.item_bias = tf.Variable(tf.zeros([self.itemNum]))\n            else:\n                self.user_embeddings = tf.Variable(self.param[0])\n                self.item_embeddings = tf.Variable(self.param[1])\n                self.item_bias = tf.Variable(self.param[2])\n\n        self.d_params = [self.user_embeddings, self.item_embeddings, self.item_bias]\n\n        # placeholder definition\n        self.u = tf.placeholder(tf.int32)\n        self.i = tf.placeholder(tf.int32)\n        self.label = tf.placeholder(tf.float32)\n\n        self.u_embedding = tf.nn.embedding_lookup(self.user_embeddings, self.u)\n        self.i_embedding = tf.nn.embedding_lookup(self.item_embeddings, self.i)\n        self.i_bias = tf.gather(self.item_bias, self.i)\n\n        self.pre_logits = tf.reduce_sum(tf.multiply(self.u_embedding, self.i_embedding), 1) + self.i_bias\n        self.pre_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.label,\n                                                                logits=self.pre_logits) + self.lamda * (\n            tf.nn.l2_loss(self.u_embedding) + tf.nn.l2_loss(self.i_embedding) + tf.nn.l2_loss(self.i_bias)\n        )\n\n        d_opt = tf.train.AdamOptimizer(self.learning_rate)\n        self.d_updates = d_opt.minimize(self.pre_loss, var_list=self.d_params)\n\n        self.reward_logits = tf.reduce_sum(tf.multiply(self.u_embedding, self.i_embedding),\n                                           1) + self.i_bias\n        self.reward = 2 * (tf.sigmoid(self.reward_logits) - 0.5)\n\n\n        self.all_rating = tf.matmul(self.u_embedding, self.item_embeddings, transpose_a=False,\n                                    transpose_b=True) + self.item_bias\n\n        self.all_logits = tf.reduce_sum(tf.multiply(self.u_embedding, self.item_embeddings), 1) + self.item_bias\n        self.NLL = -tf.reduce_mean(tf.log(\n            tf.gather(tf.reshape(tf.nn.softmax(tf.reshape(self.all_logits, [1, -1])), [-1]), self.i))\n        )\n\n\n    # def save_model(self, sess, filename):\n    #     param = sess.run(self.d_params)\n    #     cPickle.dump(param, open(filename, 'w'))\n\n\n\n\nclass IRGAN(DeepRecommender):\n\n    def __init__(self,conf,trainingSet=None,testSet=None,fold='[1]'):\n        super(IRGAN, self).__init__(conf,trainingSet,testSet,fold)\n\n    def next_batch(self):\n        #only for pre-training\n        batch_idx = np.random.randint(self.train_size, size=self.batch_size)\n        users = [self.data.trainingData[idx][0] for idx in batch_idx]\n        items = [self.data.trainingData[idx][1] for idx in batch_idx]\n        user_idx, item_idx = [], []\n        y = []\n        for i, user in enumerate(users):\n            user_idx.append(self.data.user[user])\n            item_idx.append(self.data.item[items[i]])\n            y.append(1)\n            # According to the paper, we sampled four negative instances per positive instance\n            for instance in range(4):\n                item_j = randint(0, self.num_items - 1)\n                while self.data.trainSet_u[user].has_key(self.data.id2item[item_j]):\n                    item_j = randint(0, self.num_items - 1)\n                user_idx.append(self.data.user[user])\n                item_idx.append(item_j)\n                y.append(0)\n        return user_idx, item_idx, y\n\n    def get_data(self,model):\n\n        user_list,items,label = [],[],[]\n        for user in self.data.trainSet_u:\n            pos,values = self.data.userRated(user)\n            pos = [self.data.item[item] for item in pos]\n            u = self.data.user[user]\n\n            rating = self.sess.run(model.all_rating, {model.u: [u]})\n            rating = np.array(rating[0]) / 0.2  # Temperature\n            exp_rating = np.exp(rating)\n            exp_rating[np.array(pos)] = 0\n            prob = exp_rating / np.sum(exp_rating)\n\n            neg = np.random.choice(np.arange(self.num_items), size=2*len(pos), p=prob)\n            for i in range(len(pos)):\n                user_list.append(u)\n                items.append(pos[i])\n                label.append(1.)\n            for i in range(len(neg)):\n                user_list.append(u)\n                items.append(neg[i])\n                label.append(0.)\n\n\n        return (user_list,items,label),len(user_list)\n\n    def get_batch(self,data,index,size):\n        user,item,label = data\n        return (user[index:index+size],item[index:index+size],label[index:index+size])\n\n\n    def initModel(self):\n        super(IRGAN, self).initModel()\n        self.generator = GEN(self.num_items, self.num_users, self.k, lamda=self.regU, param=None,\n                             initdelta=0.05,learning_rate=self.lRate)\n        self.discriminator = DIS(self.num_items, self.num_users, self.k, lamda=self.regU, param=None,\n                                 initdelta=0.05,learning_rate=self.lRate)\n\n\n\n    def buildModel(self):\n        # minimax training\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n        #pretrain the discriminator\n\n        # for i in range(100):\n        #     input_user, input_item, input_label = self.next_batch()\n        #     _ = self.sess.run(self.discriminator.d_updates,\n        #                       feed_dict={self.discriminator.u: input_user, self.discriminator.i: input_item,\n        #                                  self.discriminator.label: input_label})\n\n\n        for epoch in range(self.maxIter):\n\n            print 'Update discriminator...'\n            for d_epoch in range(20):\n                if d_epoch % 5 == 0:\n                    data,train_size = self.get_data(self.generator)\n                index = 0\n                while True:\n                    if index > train_size:\n                        break\n                    if index + self.batch_size <= train_size:\n                        input_user, input_item, input_label = self.get_batch(data, index, self.batch_size)\n                    else:\n                        input_user, input_item, input_label = self.get_batch(data, index, train_size - index)\n\n                    index += self.batch_size\n\n                    _ = self.sess.run(self.discriminator.d_updates,\n                                 feed_dict={self.discriminator.u: input_user, self.discriminator.i: input_item,\n                                            self.discriminator.label: input_label})\n\n                print 'epoch:',epoch+1,'d_epoch:', d_epoch+1\n\n            # Train G\n            print 'Update generator...'\n            for g_epoch in range(50):\n                for user in self.data.trainSet_u:\n                    sample_lambda = 0.2\n                    pos, values = self.data.userRated(user)\n                    pos = [self.data.item[item] for item in pos]\n                    u = self.data.user[user]\n                    rating = self.sess.run(self.generator.all_logits, {self.generator.u: u})\n\n                    exp_rating = np.exp(rating)\n                    prob = exp_rating / np.sum(exp_rating)  # prob is generator distribution p_\\theta\n\n                    # Here is the importance sampling. Actually I have some problems in understandings these two\n                    # lines and the paper doesn't give details about the importance sampling.\n                    pn = (1 - sample_lambda) * prob\n                    pn[pos] += sample_lambda * 1.0 / len(pos)\n                    # Now, pn is the Pn in importance sampling, prob is generator distribution p_\\theta\n\n                    sample = np.random.choice(np.arange(self.num_items), 3 * len(pos), p=pn)\n                    ###########################################################################\n                    # Get reward and adapt it with importance sampling\n                    ###########################################################################\n                    reward = self.sess.run(self.discriminator.reward,\n                                           {self.discriminator.u: u, self.discriminator.i: sample})\n                    reward = reward * prob[sample] / pn[sample]\n                    ###########################################################################\n                    # Update G\n                    ###########################################################################\n                    _ = self.sess.run(self.generator.gan_updates,\n                                      {self.generator.u: u, self.generator.i: sample,\n                                       self.generator.reward: reward})\n\n                print 'epoch:', epoch+1, 'g_epoch:', g_epoch+1\n\n            self.ranking_performance()\n\n\n    def predictForRanking(self, u):\n        'invoked to rank all the items for the user'\n        if self.data.containsUser(u):\n            u = self.data.user[u]\n\n            #In our experiments, discriminator performs better than generator\n            res = self.sess.run(self.discriminator.all_rating, {self.discriminator.u: [u]})\n            return res[0]\n\n        else:\n            return [self.data.globalMean] * self.num_items\n\n\n\n"""
algorithm/ranking/MostPopular.py,0,"b""#coding:utf8\nfrom baseclass.Recommender import Recommender\nimport numpy as np\nclass MostPopular(Recommender):\n\n    # Recommend the most popular items for every user\n\n    def __init__(self,conf,trainingSet=None,testSet=None,fold='[1]'):\n        super(MostPopular, self).__init__(conf,trainingSet,testSet,fold)\n\n    # def readConfiguration(self):\n    #     super(BPR, self).readConfiguration()\n\n    def initModel(self):\n        self.popularItemList = np.random.random(self.data.trainingSize()[1])\n        for item in self.data.trainSet_i:\n            ind = self.data.item[item]\n            self.popularItemList[ind] = len(self.data.trainSet_i[item])\n\n\n    def predict(self,user,item):\n        return 0\n\n    def predictForRanking(self, u):\n        'invoked to rank all the items for the user'\n        if self.data.containsUser(u):\n            return self.popularItemList\n        else:\n            return [self.data.globalMean] * self.num_items\n\n\n"""
algorithm/ranking/NGCF.py,29,"b'#coding:utf8\nfrom baseclass.DeepRecommender import DeepRecommender\nfrom random import choice\nimport tensorflow as tf\nimport numpy as np\nfrom math import sqrt\nclass NGCF(DeepRecommender):\n\n    def __init__(self,conf,trainingSet=None,testSet=None,fold=\'[1]\'):\n        super(NGCF, self).__init__(conf,trainingSet,testSet,fold)\n\n    def next_batch(self):\n        batch_id = 0\n        while batch_id < self.train_size:\n            if batch_id + self.batch_size <= self.train_size:\n                users = [self.data.trainingData[idx][0] for idx in range(batch_id, self.batch_size + batch_id)]\n                items = [self.data.trainingData[idx][1] for idx in range(batch_id, self.batch_size + batch_id)]\n                batch_id += self.batch_size\n            else:\n                users = [self.data.trainingData[idx][0] for idx in range(batch_id, self.train_size)]\n                items = [self.data.trainingData[idx][1] for idx in range(batch_id, self.train_size)]\n                batch_id = self.train_size\n\n            u_idx, i_idx, j_idx = [], [], []\n            item_list = self.data.item.keys()\n            for i, user in enumerate(users):\n\n                i_idx.append(self.data.item[items[i]])\n                u_idx.append(self.data.user[user])\n\n                neg_item = choice(item_list)\n                while neg_item in self.data.trainSet_u[user]:\n                    neg_item = choice(item_list)\n                j_idx.append(self.data.item[neg_item])\n\n            yield u_idx, i_idx, j_idx\n\n    def initModel(self):\n        super(NGCF, self).initModel()\n        self.isTraining = tf.placeholder(tf.int32)\n        self.isTraining = tf.cast(self.isTraining, tf.bool)\n        ego_embeddings = tf.concat([self.user_embeddings,self.item_embeddings], axis=0)\n\n        indices = [[self.data.user[item[0]],self.num_users+self.data.item[item[1]]] for item in self.data.trainingData]\n        indices += [[self.num_users+self.data.item[item[1]],self.data.user[item[0]]] for item in self.data.trainingData]\n        values = [float(item[2])/sqrt(len(self.data.trainSet_u[item[0]]))/sqrt(len(self.data.trainSet_i[item[1]])) for item in self.data.trainingData]*2\n\n        norm_adj = tf.SparseTensor(indices=indices, values=values, dense_shape=[self.num_users+self.num_items,self.num_users+self.num_items])\n\n        self.weights = dict()\n\n        initializer = tf.contrib.layers.xavier_initializer()\n        weight_size = [self.embed_size,self.embed_size,self.embed_size] #can be changed\n        weight_size_list = [self.embed_size] + weight_size\n\n        self.n_layers = 3\n\n        #initialize parameters\n        for k in range(self.n_layers):\n            self.weights[\'W_%d_1\' % k] = tf.Variable(\n                initializer([weight_size_list[k], weight_size_list[k + 1]]), name=\'W_%d_1\' % k)\n            self.weights[\'W_%d_2\' % k] = tf.Variable(\n                initializer([weight_size_list[k], weight_size_list[k + 1]]), name=\'W_%d_2\' % k)\n\n        all_embeddings = [ego_embeddings]\n        for k in range(self.n_layers):\n            side_embeddings = tf.sparse_tensor_dense_matmul(norm_adj,ego_embeddings)\n            sum_embeddings = tf.matmul(side_embeddings+ego_embeddings, self.weights[\'W_%d_1\' % k])\n            bi_embeddings = tf.multiply(ego_embeddings, side_embeddings)\n            bi_embeddings = tf.matmul(bi_embeddings, self.weights[\'W_%d_2\' % k])\n\n            ego_embeddings = tf.nn.leaky_relu(sum_embeddings+bi_embeddings)\n\n            # message dropout.\n            def without_dropout():\n                return ego_embeddings\n            def dropout():\n                return tf.nn.dropout(ego_embeddings, keep_prob=0.9)\n\n            ego_embeddings = tf.cond(self.isTraining,lambda:dropout(),lambda:without_dropout())\n\n            # normalize the distribution of embeddings.\n            norm_embeddings = tf.math.l2_normalize(ego_embeddings, axis=1)\n\n            all_embeddings += [norm_embeddings]\n\n        all_embeddings = tf.concat(all_embeddings, 1)\n        self.multi_user_embeddings, self.multi_item_embeddings = tf.split(all_embeddings, [self.num_users, self.num_items], 0)\n\n        self.neg_idx = tf.placeholder(tf.int32, name=""neg_holder"")\n        self.neg_item_embedding = tf.nn.embedding_lookup(self.multi_item_embeddings, self.neg_idx)\n        self.u_embedding = tf.nn.embedding_lookup(self.multi_user_embeddings, self.u_idx)\n        self.v_embedding = tf.nn.embedding_lookup(self.multi_item_embeddings, self.v_idx)\n\n        self.test = tf.reduce_sum(tf.multiply(self.u_embedding,self.multi_item_embeddings),1)\n\n    def buildModel(self):\n\n        y = tf.reduce_sum(tf.multiply(self.u_embedding, self.v_embedding), 1) \\\n            - tf.reduce_sum(tf.multiply(self.u_embedding, self.neg_item_embedding), 1)\n\n        loss = -tf.reduce_sum(tf.log(tf.sigmoid(y))) + self.regU * (tf.nn.l2_loss(self.u_embedding) +\n                                                                    tf.nn.l2_loss(self.v_embedding) +\n                                                                    tf.nn.l2_loss(self.neg_item_embedding))\n        opt = tf.train.AdamOptimizer(self.lRate)\n\n        train = opt.minimize(loss)\n\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n        for iteration in range(self.maxIter):\n            for n, batch in enumerate(self.next_batch()):\n                user_idx, i_idx, j_idx = batch\n                _, l = self.sess.run([train, loss],\n                                feed_dict={self.u_idx: user_idx, self.neg_idx: j_idx, self.v_idx: i_idx,self.isTraining:1})\n                print \'training:\', iteration + 1, \'batch\', n, \'loss:\', l\n\n    def predictForRanking(self, u):\n        \'invoked to rank all the items for the user\'\n        if self.data.containsUser(u):\n            u = self.data.getUserId(u)\n            return self.sess.run(self.test,feed_dict={self.u_idx:u,self.isTraining:0})\n        else:\n            return [self.data.globalMean] * self.num_items'"
algorithm/ranking/NeuMF.py,48,"b'#coding:utf8\nfrom baseclass.DeepRecommender import DeepRecommender\nimport numpy as np\nfrom random import randint\nimport tensorflow as tf\n\nclass NeuMF(DeepRecommender):\n\n    def __init__(self,conf,trainingSet=None,testSet=None,fold=\'[1]\'):\n        super(NeuMF, self).__init__(conf,trainingSet,testSet,fold)\n\n\n    def next_batch(self):\n        batch_id=0\n        while batch_id<self.train_size:\n            if batch_id+self.batch_size<=self.train_size:\n                users = [self.data.trainingData[idx][0] for idx in range(batch_id,self.batch_size+batch_id)]\n                items = [self.data.trainingData[idx][1] for idx in range(batch_id,self.batch_size+batch_id)]\n                batch_id+=self.batch_size\n            else:\n                users = [self.data.trainingData[idx][0] for idx in range(batch_id, self.train_size)]\n                items = [self.data.trainingData[idx][1] for idx in range(batch_id, self.train_size)]\n                batch_id=self.train_size\n            u_idx,i_idx,y = [],[],[]\n            for i,user in enumerate(users):\n\n                i_idx.append(self.data.item[items[i]])\n                u_idx.append(self.data.user[user])\n                y.append(1)\n                for instance in range(4):\n                    item_j = randint(0, self.num_items - 1)\n                    while self.data.trainSet_u[user].has_key(self.data.id2item[item_j]):\n                        item_j = randint(0, self.num_items - 1)\n                    u_idx.append(self.data.user[user])\n                    i_idx.append(item_j)\n                    y.append(0)\n            yield u_idx,i_idx,y\n\n\n    def initModel(self):\n        super(NeuMF, self).initModel()\n        # parameters used are consistent with default settings in the original paper\n        mlp_regularizer = tf.contrib.layers.l2_regularizer(scale=0.001)\n        initializer = tf.contrib.layers.xavier_initializer()\n        with tf.variable_scope(""latent_factors""):\n            self.PG = tf.get_variable(name=\'PG\',initializer=initializer([self.num_users, self.embed_size]))\n            self.QG = tf.get_variable(name=\'QG\',initializer=initializer([self.num_items, self.embed_size]))\n\n            self.PM = tf.get_variable(name=\'PM\', initializer=initializer([self.num_users, self.embed_size]),regularizer=mlp_regularizer)\n            self.QM = tf.get_variable(name=\'QM\', initializer=initializer([self.num_items, self.embed_size]),regularizer=mlp_regularizer)\n\n        with tf.name_scope(""input""):\n            self.r = tf.placeholder(tf.float32, [None], name=""rating"")\n            self.u_idx = tf.placeholder(tf.int32, [None], name=""u_idx"")\n            self.i_idx = tf.placeholder(tf.int32, [None], name=""i_idx"")\n            self.UG_embedding = tf.nn.embedding_lookup(self.PG, self.u_idx)\n            self.IG_embedding = tf.nn.embedding_lookup(self.QG, self.i_idx)\n            self.UM_embedding = tf.nn.embedding_lookup(self.PM, self.u_idx)\n            self.IM_embedding = tf.nn.embedding_lookup(self.QM, self.i_idx)\n\n        # Generic Matrix Factorization\n        with tf.variable_scope(""mf_output""):\n            self.GMF_Layer = tf.multiply(self.UG_embedding,self.IG_embedding)\n            self.h_mf = tf.get_variable(name=\'mf_out\', initializer=initializer([self.embed_size]))\n\n        # MLP\n        with tf.variable_scope(""mlp_params""):\n            MLP_W1 = tf.get_variable(name=\'W1\',initializer=initializer([self.embed_size*2, self.embed_size*5]), regularizer=mlp_regularizer)\n            MLP_b1 = tf.get_variable(name=\'b1\',initializer=tf.zeros(shape=[self.embed_size*5]), regularizer=mlp_regularizer)\n            self.h_out = tf.nn.relu(tf.add(tf.matmul(tf.concat([self.UM_embedding,self.IM_embedding], 1), MLP_W1), MLP_b1))\n\n            MLP_W2 = tf.get_variable(name=\'W2\',initializer=initializer([self.embed_size*5, self.embed_size*2]), regularizer=mlp_regularizer)\n            MLP_b2 = tf.get_variable(name=\'b2\',initializer=tf.zeros(shape=[self.embed_size*2]), regularizer=mlp_regularizer)\n            self.h_out = tf.nn.relu(tf.add(tf.matmul(self.h_out,MLP_W2), MLP_b2))\n\n            MLP_W3 = tf.get_variable(name=\'W3\',initializer=initializer([self.embed_size*2, self.embed_size]),regularizer=mlp_regularizer)\n            MLP_b3 = tf.get_variable(name=\'b3\',initializer=tf.zeros(shape=[self.embed_size]), regularizer=mlp_regularizer)\n            self.MLP_Layer = tf.nn.relu(tf.add(tf.matmul(self.h_out,MLP_W3), MLP_b3))\n            self.h_mlp = tf.get_variable(name=\'mlp_out\', initializer=initializer([self.embed_size]), regularizer=mlp_regularizer)\n\n\n        #single inference\n        #GMF\n        self.y_mf = tf.reduce_sum(tf.multiply(self.GMF_Layer,self.h_mf),1)\n        self.y_mf = tf.sigmoid(self.y_mf)\n        self.mf_loss = self.r * tf.log(self.y_mf+10e-10) + (1 - self.r) * tf.log(1 - self.y_mf+10e-10)\n        mf_reg = self.regU*(tf.nn.l2_loss(self.UG_embedding)+tf.nn.l2_loss(self.IG_embedding) + tf.nn.l2_loss(self.h_mf))\n\n        self.mf_loss = -tf.reduce_sum(self.mf_loss) + mf_reg\n\n        self.mf_optimizer = tf.train.AdamOptimizer(self.lRate).minimize(self.mf_loss)\n        #MLP\n        self.y_mlp = tf.reduce_sum(tf.multiply(self.MLP_Layer,self.h_mlp),1)\n        self.y_mlp = tf.sigmoid(self.y_mlp)\n        self.mlp_loss = self.r * tf.log(self.y_mlp+10e-10) + (1 - self.r) * tf.log(1 - self.y_mlp+10e-10)\n        self.mlp_loss = -tf.reduce_sum(self.mlp_loss)\n        self.mlp_optimizer = tf.train.AdamOptimizer(self.lRate).minimize(self.mlp_loss)\n\n        #fusion\n        self.NeuMF_Layer = tf.concat([self.GMF_Layer,self.MLP_Layer], 1)\n        self.h_NeuMF = tf.concat([0.5*self.h_mf,0.5*self.h_mlp], 0)\n        self.y_neu = tf.reduce_sum(tf.multiply(self.NeuMF_Layer, self.h_NeuMF), 1)\n        self.y_neu = tf.sigmoid(self.y_neu)\n        self.neu_loss = self.r * tf.log(self.y_neu+10e-10) + (1 - self.r) * tf.log(1 - self.y_neu+10e-10)\n\n        self.neu_loss = -tf.reduce_sum(self.neu_loss)+ mf_reg + self.regU*tf.nn.l2_loss(self.h_NeuMF)\n        ###it seems Adam is better than SGD here...\n        self.neu_optimizer = tf.train.AdamOptimizer(self.lRate).minimize(self.neu_loss)\n\n    def buildModel(self):\n\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n\n        print \'pretraining... (GMF)\'\n        for iteration in range(self.maxIter):\n            for num,batch in enumerate(self.next_batch()):\n                user_idx, item_idx, r = batch\n\n                _, loss,y_mf = self.sess.run([self.mf_optimizer, self.mf_loss,self.y_mf],\n                                   feed_dict={self.u_idx: user_idx, self.i_idx: item_idx, self.r: r})\n                print \'iteration:\', iteration, \'batch:\', num, \'loss:\', loss\n\n        print \'pretraining... (MLP)\'\n        for iteration in range(self.maxIter/2):\n            for num, batch in enumerate(self.next_batch()):\n                user_idx, item_idx, r = batch\n                _, loss, y_mlp = self.sess.run([self.mlp_optimizer, self.mlp_loss, self.y_mlp],\n                                          feed_dict={self.u_idx: user_idx, self.i_idx: item_idx, self.r: r})\n                print \'iteration:\', iteration, \'batch:\', num, \'loss:\', loss\n\n        print \'training... (NeuMF)\'\n        for iteration in range(self.maxIter/5):\n            for num, batch in enumerate(self.next_batch()):\n                user_idx, item_idx, r = batch\n                _, loss, y_neu = self.sess.run([self.neu_optimizer, self.neu_loss, self.y_neu],\n                                          feed_dict={self.u_idx: user_idx, self.i_idx: item_idx, self.r: r})\n                print \'iteration:\', iteration, \'batch:\', num, \'loss:\', loss\n\n    def predict_mlp(self,uid):\n        user_idx = [uid]*self.num_items\n        y_mlp = self.sess.run([self.y_mlp],feed_dict={self.u_idx: user_idx, self.i_idx: range(self.num_items)})\n        return y_mlp[0]\n\n    def predict_mf(self,uid):\n        user_idx = [uid]*self.num_items\n        y_mf = self.sess.run([self.y_mf],feed_dict={self.u_idx: user_idx, self.i_idx: range(self.num_items)})\n        return y_mf[0]\n\n    def predict_neu(self,uid):\n        user_idx = [uid]*self.num_items\n        y_neu = self.sess.run([self.y_neu],feed_dict={self.u_idx: user_idx, self.i_idx: range(self.num_items)})\n        return y_neu[0]\n\n    def predictForRanking(self, u):\n        \'invoked to rank all the items for the user\'\n        if self.data.containsUser(u):\n            u = self.data.user[u]\n            return self.predict_neu(u)\n        else:\n            return [self.data.globalMean] * self.num_items'"
algorithm/ranking/RSGAN.py,57,"b'# coding:utf8\nfrom baseclass.DeepRecommender import DeepRecommender\nfrom baseclass.SocialRecommender import SocialRecommender\nimport numpy as np\nfrom random import randint, choice,shuffle\nfrom collections import defaultdict\nimport tensorflow as tf\nimport gensim.models.word2vec as w2v\nfrom tool.qmath import sigmoid, cosine\n\ndef gumbel_softmax(logits, temperature=0.2):\n    eps = 1e-20\n    u = tf.random_uniform(tf.shape(logits), minval=0, maxval=1)\n    gumbel_noise = -tf.log(-tf.log(u + eps) + eps)\n    y = tf.log(logits + eps) + gumbel_noise\n    return tf.nn.softmax(y / temperature)\n\n\n\nclass RSGAN(SocialRecommender,DeepRecommender):\n    def __init__(self, conf, trainingSet=None, testSet=None, relation=None, fold=\'[1]\'):\n        DeepRecommender.__init__(self, conf=conf, trainingSet=trainingSet, testSet=testSet, fold=fold)\n        SocialRecommender.__init__(self, conf=conf, trainingSet=trainingSet, testSet=testSet, relation=relation,fold=fold)\n\n    def readNegativeFeedbacks(self):\n        self.negative = defaultdict(list)\n        self.nItems = defaultdict(list)\n        filename = self.config[\'ratings\'][:-4]+\'_n.txt\'\n        with open(filename) as f:\n            for line in f:\n                items = line.strip().split()\n                self.negative[items[0]].append(items[1])\n                self.nItems[items[1]].append(items[0])\n                if items[0] not in self.data.user:\n                    self.data.user[items[0]]=len(self.data.user)\n                    self.data.id2user[self.data.user[items[0]]] = items[0]\n                    self.num_users+=1\n\n\n    def randomWalks(self):\n\n        self.positive = defaultdict(list)\n        self.pItems = defaultdict(list)\n        for user in self.data.trainSet_u:\n            for item in self.data.trainSet_u[user]:\n                self.positive[user].append(item)\n                self.pItems[item].append(user)\n\n        print \'Kind Note: This method will probably take much time.\'\n        # build U-F-NET\n        print \'Building weighted user-friend network...\'\n        # filter isolated nodes and low ratings\n        # Definition of Meta-Path\n        p1 = \'UIU\'\n        p2 = \'UFU\'\n        p3 = \'UTU\'\n        p4 = \'UFIU\'\n        p5 = \'UFUIU\'\n        mPaths = [p1, p2, p3, p4, p5]\n\n        self.walkLength=30\n        self.topK = 100\n\n        self.G = np.random.rand(self.num_users, 50) * 0.1\n        self.W = np.random.rand(self.num_items, 50) * 0.1\n\n        self.UFNet = defaultdict(list) # a -> b #a trusts b\n        for u in self.social.followees:\n            s1 = set(self.social.followees[u])\n            for v in self.social.followees[u]:\n                if v in self.social.followees:  # make sure that v has out links\n                    if u <> v:\n                        s2 = set(self.social.followees[v])\n                        weight = len(s1.intersection(s2))\n                        self.UFNet[u] += [v] * (weight + 1)\n\n        self.UTNet = defaultdict(list) # a <- b #a is trusted by b\n        for u in self.social.followers:\n            s1 = set(self.social.followers[u])\n            for v in self.social.followers[u]:\n                if self.social.followers.has_key(v):  # make sure that v has out links\n                    if u <> v:\n                        s2 = set(self.social.followers[v])\n                        weight = len(s1.intersection(s2))\n                        self.UTNet[u] += [v] * (weight + 1)\n\n        print \'Generating random meta-path random walks... (Positive)\'\n        self.pWalks = []\n        # self.usercovered = {}\n\n        # positive\n        for user in self.data.user:\n            for mp in mPaths:\n                if mp == p1:\n                    self.walkCount = 10\n                if mp == p2:\n                    self.walkCount = 8\n                if mp == p3:\n                    self.walkCount = 8\n                if mp == p4:\n                    self.walkCount = 5\n                if mp == p5:\n                    self.walkCount = 5\n\n                for t in range(self.walkCount):\n                    path = [\'U\' + user]\n                    lastNode = user\n                    nextNode = user\n                    lastType = \'U\'\n                    for i in range(self.walkLength / len(mp[1:])):\n                        for tp in mp[1:]:\n                            try:\n                                if tp == \'I\':\n                                    nextNode = choice(self.positive[lastNode])\n\n                                if tp == \'U\':\n                                    if lastType == \'I\':\n                                        nextNode = choice(self.pItems[lastNode])\n                                    elif lastType == \'F\':\n                                        nextNode = choice(self.UFNet[lastNode])\n                                        while not self.data.user.has_key(nextNode):\n                                            nextNode = choice(self.UFNet[lastNode])\n                                    elif lastType == \'T\':\n                                        nextNode = choice(self.UTNet[lastNode])\n                                        while not self.data.user.has_key(nextNode):\n                                            nextNode = choice(self.UTNet[lastNode])\n\n                                if tp == \'F\':\n                                    nextNode = choice(self.UFNet[lastNode])\n                                    while not self.data.user.has_key(nextNode):\n                                        nextNode = choice(self.UFNet[lastNode])\n\n                                if tp == \'T\':\n                                    nextNode = choice(self.UFNet[lastNode])\n                                    while not self.data.user.has_key(nextNode):\n                                        nextNode = choice(self.UFNet[lastNode])\n\n                                path.append(tp + nextNode)\n                                lastNode = nextNode\n                                lastType = tp\n\n                            except (KeyError, IndexError):\n                                path = []\n                                break\n\n                    if path:\n                        self.pWalks.append(path)\n\n        self.nWalks = []\n        # self.usercovered = {}\n\n        # negative\n        for user in self.data.user:\n            for mp in mPaths:\n                if mp == p1:\n                    self.walkCount = 10\n                if mp == p2:\n                    self.walkCount = 8\n                if mp == p3:\n                    self.walkCount = 8\n                if mp == p4:\n                    self.walkCount = 5\n                if mp == p5:\n                    self.walkCount = 5\n                for t in range(self.walkCount):\n                    path = [\'U\' + user]\n                    lastNode = user\n                    nextNode = user\n                    lastType = \'U\'\n                    for i in range(self.walkLength / len(mp[1:])):\n                        for tp in mp[1:]:\n                            try:\n                                if tp == \'I\':\n                                    nextNode = choice(self.negative[lastNode])\n\n                                if tp == \'U\':\n                                    if lastType == \'I\':\n                                        nextNode = choice(self.nItems[lastNode])\n                                    elif lastType == \'F\':\n                                        nextNode = choice(self.UFNet[lastNode])\n                                        while not self.data.user.has_key(nextNode):\n                                            nextNode = choice(self.UFNet[lastNode])\n                                    elif lastType == \'T\':\n                                        nextNode = choice(self.UTNet[lastNode])\n                                        while not self.data.user.has_key(nextNode):\n                                            nextNode = choice(self.UTNet[lastNode])\n\n                                if tp == \'F\':\n                                    nextNode = choice(self.UFNet[lastNode])\n                                    while not self.data.user.has_key(nextNode):\n                                        nextNode = choice(self.UFNet[lastNode])\n\n                                if tp == \'T\':\n                                    nextNode = choice(self.UFNet[lastNode])\n                                    while not self.data.user.has_key(nextNode):\n                                        nextNode = choice(self.UFNet[lastNode])\n\n                                path.append(tp + nextNode)\n                                lastNode = nextNode\n                                lastType = tp\n\n                            except (KeyError, IndexError):\n                                path = []\n                                break\n\n                    if path:\n                        self.nWalks.append(path)\n\n        shuffle(self.pWalks)\n        print \'pwalks:\', len(self.pWalks)\n        print \'nwalks:\', len(self.nWalks)\n\n    def computeSimilarity(self):\n        # Training get top-k friends\n        print \'Generating user embedding...\'\n        self.pTopKSim = {}\n        self.nTopKSim = {}\n        self.pSimilarity = defaultdict(dict)\n        self.nSimilarity = defaultdict(dict)\n        pos_model = w2v.Word2Vec(self.pWalks, size=50, window=5, min_count=0, iter=10)\n        neg_model = w2v.Word2Vec(self.nWalks, size=50, window=5, min_count=0, iter=10)\n        for user in self.positive:\n            uid = self.data.user[user]\n            try:\n                self.W[uid] = pos_model.wv[\'U\' + user]\n            except KeyError:\n                continue\n        for user in self.negative:\n            uid = self.data.user[user]\n            try:\n                self.G[uid] = neg_model.wv[\'U\' + user]\n            except KeyError:\n                continue\n        print \'User embedding generated.\'\n\n        print \'Constructing similarity matrix...\'\n        i = 0\n        for user1 in self.positive:\n            uSim = []\n            i += 1\n            if i % 200 == 0:\n                print i, \'/\', len(self.positive)\n            vec1 = self.W[self.data.user[user1]]\n            for user2 in self.positive:\n                if user1 <> user2:\n                    vec2 = self.W[self.data.user[user2]]\n                    sim = cosine(vec1, vec2)\n                    uSim.append((user2, sim))\n            fList = sorted(uSim, key=lambda d: d[1], reverse=True)[:self.topK]\n\n            self.pTopKSim[user1] = [item[0] for item in fList]\n\n\n        i = 0\n        for user1 in self.negative:\n            uSim = []\n            i += 1\n            if i % 200 == 0:\n                print i, \'/\', len(self.negative)\n            vec1 = self.G[self.data.user[user1]]\n            for user2 in self.negative:\n                if user1 <> user2:\n                    vec2 = self.G[self.data.user[user2]]\n                    sim = cosine(vec1, vec2)\n                    uSim.append((user2, sim))\n            fList = sorted(uSim, key=lambda d: d[1], reverse=True)[:self.topK]\n            for pair in fList:\n                self.nSimilarity[user1][pair[0]] = pair[1]\n            self.nTopKSim[user1] = [item[0] for item in fList]\n\n        self.seededFriends = defaultdict(list)\n        self.firend_item_set = defaultdict(list)\n        for user in self.pTopKSim:\n            trueFriends = list(set(self.pTopKSim[user]).intersection(set(self.nTopKSim[user])))\n            self.seededFriends[user] = trueFriends+self.pTopKSim[user][:50]\n\n        for user in self.pTopKSim:\n            for friend in self.seededFriends[user]:\n                self.firend_item_set[user]+=self.data.trainSet_u[friend].keys()\n\n    def sampling(self,vec):\n\n        vec = tf.nn.softmax(vec)\n\n        logits = gumbel_softmax(vec, 0.1)\n        return logits\n\n\n    def build_graph(self):\n\n        indices = [[self.data.item[item[1]], self.data.user[item[0]]] for item in self.data.trainingData]\n        values = [item[2] for item in self.data.trainingData]\n        self.i_u_matrix = tf.SparseTensor(indices=indices, values=values, dense_shape=[self.num_items, self.num_users])\n        self.pos = tf.placeholder(tf.int32, name=""positive_item"")\n        self.fnd = tf.placeholder(tf.int32, name=""friend_item"")\n        self.neg = tf.placeholder(tf.int32, name=""neg_holder"")\n        self.i = tf.placeholder(tf.int32, name=""item_holder"")\n\n        with tf.name_scope(""generator""):\n\n            #AutoEncoder\n            initializer = tf.contrib.layers.xavier_initializer()\n            self.X = tf.placeholder(tf.float32, [None, self.num_users])\n            #self.sample = tf.placeholder(tf.float32, [None, self.num_users])\n\n            self.weights = {\n                \'encoder\': tf.Variable(initializer([self.num_users, 200])),\n                \'decoder\': tf.Variable(initializer([200, self.num_users])),\n            }\n            self.biases = {\n                \'encoder\': tf.Variable(initializer([200])),\n                \'decoder\': tf.Variable(initializer([self.num_users])),\n            }\n\n            self.g_params = [self.weights, self.biases]\n\n\n            layer = tf.nn.sigmoid(tf.matmul(self.X, self.weights[\'encoder\']) + self.biases[\'encoder\'])\n            self.g_output = tf.nn.sigmoid(tf.matmul(layer, self.weights[\'decoder\']) + self.biases[\'decoder\'])\n\n\n            self.y_pred = tf.multiply(self.X, self.g_output)\n            self.y_pred = tf.maximum(1e-6, self.y_pred)\n\n            cross_entropy = -tf.multiply(self.X, tf.log(self.y_pred)) - tf.multiply((1 - self.X),\n                                                                                    tf.log(1 - self.y_pred))\n            self.reconstruction = tf.reduce_sum(cross_entropy) + self.regU * (\n                    tf.nn.l2_loss(self.weights[\'encoder\']) + tf.nn.l2_loss(self.weights[\'decoder\']) +\n                    tf.nn.l2_loss(self.biases[\'encoder\']) + tf.nn.l2_loss(self.biases[\'decoder\']))\n\n            g_pre = tf.train.AdamOptimizer(self.lRate)\n            self.g_pretrain = g_pre.minimize(self.reconstruction, var_list=self.g_params)\n\n\n\n        with tf.variable_scope(\'discriminator\'):\n\n            self.item_selection = tf.get_variable(\'item_selection\',initializer=tf.constant_initializer(0.01),shape=[self.num_users, self.num_items])\n            self.g_params.append(self.item_selection)\n            self.d_params = [self.user_embeddings, self.item_embeddings]\n\n            # placeholder definition\n            self.u_embedding = tf.nn.embedding_lookup(self.user_embeddings, self.u_idx,name=\'u_e\')\n            self.i_embedding = tf.nn.embedding_lookup(self.item_embeddings, self.pos,name=\'i_e\')\n            #self.f_embedding = tf.nn.embedding_lookup(self.item_embeddings, self.fnd,name=\'f_e\')\n            self.j_embedding = tf.nn.embedding_lookup(self.item_embeddings, self.neg,name=\'j_e\')\n            #self.i_embedding = tf.nn.embedding_lookup(self.item_embeddings, self.i,name=\'f_e\')\n\n            #generate virtual friends by gumbel-softmax\n            self.virtualFriends = self.sampling(self.g_output) #one-hot\n\n            #get candidate list (items)\n            self.candidateItems = tf.transpose(tf.sparse_tensor_dense_matmul(self.i_u_matrix,tf.transpose(self.virtualFriends)))\n\n            self.embedding_selection = tf.nn.embedding_lookup(self.item_selection, self.u_idx,name=\'e_s\')\n\n            self.virtual_items = self.sampling(tf.multiply(self.candidateItems,self.embedding_selection))\n\n            #self.weights = tf.reduce_sum(tf.multiply(self.virtual_items,self.popularty),1)\n\n            self.v_i_embedding = tf.matmul(self.virtual_items,self.item_embeddings,transpose_a=False,transpose_b=False)\n\n\n            y_us = tf.reduce_sum(tf.multiply(self.u_embedding,self.i_embedding),1)\\\n                                 -tf.reduce_sum(tf.multiply(self.u_embedding,self.j_embedding),1)\n\n            self.d_pretrain_loss = -tf.reduce_sum(tf.log(tf.sigmoid(y_us)))+self.regU*(tf.nn.l2_loss(self.u_embedding)+\n                                                                                       tf.nn.l2_loss(self.j_embedding)+\n                                                                                       tf.nn.l2_loss(self.i_embedding))\n\n            y_uf = tf.reduce_sum(tf.multiply(self.u_embedding, self.i_embedding), 1) - \\\n                 tf.reduce_sum(tf.multiply(self.u_embedding, self.v_i_embedding), 1)\n\n            y_fs = tf.reduce_sum(tf.multiply(self.u_embedding, self.v_i_embedding), 1)-\\\n                 tf.reduce_sum(tf.multiply(self.u_embedding, self.j_embedding), 1)\n\n\n\n            self.d_loss = -tf.reduce_sum(tf.log(tf.sigmoid(y_uf)))-tf.reduce_sum(tf.log(tf.sigmoid(y_fs)))+\\\n                          self.regU*(tf.nn.l2_loss(self.u_embedding)+tf.nn.l2_loss(self.i_embedding)+tf.nn.l2_loss(self.j_embedding))\n            #\n            self.g_loss = 30*tf.reduce_sum(y_uf) #better performance\n\n\n            d_pre = tf.train.AdamOptimizer(self.lRate)\n\n            self.d_pretrain = d_pre.minimize(self.d_pretrain_loss, var_list=self.d_params)\n\n\n\n\n            self.d_output = tf.reduce_sum(tf.multiply(self.u_embedding, self.item_embeddings),1)\n\n        d_opt = tf.train.AdamOptimizer(self.lRate)\n        self.d_update = d_opt.minimize(self.d_loss,var_list=self.d_params)\n        g_opt = tf.train.AdamOptimizer(self.lRate)\n        self.g_update = g_opt.minimize(self.g_loss,var_list=self.g_params)\n\n\n    def next_batch_d(self):\n        batch_id=0\n        while batch_id<self.train_size:\n            if batch_id+self.batch_size<=self.train_size:\n                users = [self.data.trainingData[idx][0] for idx in range(batch_id,self.batch_size+batch_id)]\n                items = [self.data.trainingData[idx][1] for idx in range(batch_id,self.batch_size+batch_id)]\n                batch_id+=self.batch_size\n            else:\n                users = [self.data.trainingData[idx][0] for idx in range(batch_id, self.train_size)]\n                items = [self.data.trainingData[idx][1] for idx in range(batch_id, self.train_size)]\n                batch_id=self.train_size\n\n            u_idx,i_idx,j_idx = [],[],[]\n            item_list = self.data.item.keys()\n            for i,user in enumerate(users):\n\n                i_idx.append(self.data.item[items[i]])\n                u_idx.append(self.data.user[user])\n\n                neg_item = choice(item_list)\n                while neg_item in self.data.trainSet_u[user]:\n                    neg_item = choice(item_list)\n                j_idx.append(self.data.item[neg_item])\n\n            yield u_idx,i_idx,j_idx\n\n    def next_batch_g(self):\n        userList = self.data.user.keys()\n        batch_id=0\n        while batch_id<self.num_users:\n            if batch_id + self.batch_size <= self.num_users:\n                profiles = np.zeros((self.batch_size, self.num_users))\n                for i,user in enumerate(userList[batch_id:self.batch_size+batch_id]):\n                    ind = [self.data.user[friend] for friend in self.seededFriends[user]]\n                    profiles[i][ind]=1\n                    batch_id+=self.batch_size\n\n            else:\n                profiles = np.zeros((self.num_users-batch_id, self.num_users))\n                for i, user in enumerate(userList[self.num_users-batch_id:self.num_users]):\n                    ind = [self.data.user[friend] for friend in self.seededFriends[user]]\n                    profiles[i][ind] = 1\n                    batch_id=self.num_users\n\n            yield profiles\n\n\n    def initModel(self):\n        super(RSGAN, self).initModel()\n\n        self.popularty = np.zeros(self.num_items)\n        for item in self.data.item:\n            iid = self.data.item[item]\n            self.popularty[iid]=len(self.data.trainSet_i[item])\n        #collect implicit friends\n\n        self.readNegativeFeedbacks()\n        self.randomWalks()\n        self.computeSimilarity()\n        self.build_graph()\n\n\n    def buildModel(self):\n        # minimax training\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n        # pretraining\n\n        #print \'pretraining for discriminator...\'\n        # self.friend_item_set(self.seed_friends)\n        # for i in range(30):\n        #     batch_id=0\n        #     while batch_id<self.train_size:\n        #         batch_id,user_idx, i_idx, j_idx = self.next_batch_d(batch_id)\n        #         _, loss = self.sess.run([self.d_pretrain, self.d_pretrain_loss],\n        #                                 feed_dict={self.u: user_idx, self.neg: j_idx,\n        #                                            self.pos:i_idx})\n        #\n        #         print \'pretraining:\', i + 1, \'batch_id\',batch_id,\'discriminator loss:\', loss\n        #\n        # self.ranking_performance()\n\n        f = open(self.foldInfo+\'RSGAN.txt\',\'w\')\n        res = []\n\n        print \'pretraining for generator...\'\n        for i in range(30):\n            for num,batch in enumerate(self.next_batch_g()):\n                profiles = batch\n                _,loss = self.sess.run([self.g_pretrain,self.reconstruction],feed_dict={self.X:profiles})\n                print \'pretraining:\', i + 1, \'batch\',num,\'generator loss:\', loss\n\n\n        print \'Training GAN...\'\n\n        for i in range(self.maxIter):\n            batch_id = 0\n            for num,batch in enumerate(self.next_batch_d()):\n                user_idx, i_idx, j_idx = batch\n\n                profiles = np.zeros((len(user_idx),self.num_users))\n                for n,u in enumerate(user_idx):\n                    u_name = self.data.id2user[u]\n                    idx = [self.data.user[friend] for friend in self.seededFriends[u_name]]\n                    profiles[n][idx]=1\n\n\n                #generator\n                _,loss = self.sess.run([self.g_update,self.g_loss],feed_dict={self.u_idx: user_idx,self.neg:j_idx,\n                                                   self.pos: i_idx,self.X:profiles})\n                #discriminator\n                _, loss = self.sess.run([self.d_update, self.d_loss],\n                                        feed_dict={self.u_idx: user_idx,self.neg:j_idx,\n                                                   self.pos: i_idx,self.X:profiles})\n\n                print \'training:\', i + 1, \'batch_id\', num, \'discriminator loss:\', loss\n\n        #     results = self.ranking_performance()\n        #     res+=results\n        #\n        # f.writelines(res)\n\n\n    def predictForRanking(self, u):\n        \'invoked to rank all the items for the user\'\n        if self.data.containsUser(u):\n            u = self.data.user[u]\n\n            # In our experiments, discriminator performs better than generator\n            res = self.sess.run(self.d_output, {self.u_idx:u})\n            return res\n\n        else:\n            return [self.data.globalMean] * self.num_items\n\n\n\n'"
algorithm/ranking/Rand.py,0,"b""#coding:utf8\nfrom baseclass.Recommender import Recommender\nimport numpy as np\nclass Rand(Recommender):\n\n    # Recommend items for every user at random\n\n    def __init__(self,conf,trainingSet=None,testSet=None,fold='[1]'):\n        super(Rand, self).__init__(conf,trainingSet,testSet,fold)\n\n\n\n    def predict(self,user,item):\n        return 0\n\n    def predictForRanking(self, u):\n        'invoked to rank all the items for the user'\n        if self.data.containsUser(u):\n            return np.random.random(self.data.trainingSize()[1])\n        else:\n            return [self.data.globalMean] * self.num_items\n\n\n"""
algorithm/ranking/SBPR.py,19,"b'from baseclass.SocialRecommender import SocialRecommender\nfrom math import log\nimport numpy as np\nimport tensorflow as tf\nfrom tool.qmath import sigmoid\nfrom random import choice\nfrom collections import defaultdict\nclass SBPR(SocialRecommender):\n    def __init__(self,conf,trainingSet=None,testSet=None,relation=None,fold=\'[1]\'):\n        super(SBPR, self).__init__(conf,trainingSet,testSet,relation,fold)\n\n\n    def initModel(self):\n        super(SBPR, self).initModel()\n        print \'Preparing item sets...\'\n        self.PositiveSet = defaultdict(dict)\n        self.FPSet = defaultdict(dict)\n        # self.NegativeSet = defaultdict(list)\n\n        for user in self.data.user:\n            for item in self.data.trainSet_u[user]:\n                if self.data.trainSet_u[user][item] >= 1:\n                    self.PositiveSet[user][item] = 1\n                    # else:\n                    #     self.NegativeSet[user].append(item)\n            if self.social.user.has_key(user):\n                for friend in self.social.getFollowees(user):\n                    if self.data.user.has_key(friend):\n                        for item in self.data.trainSet_u[friend]:\n                            if not self.PositiveSet[user].has_key(item):\n                                if not self.FPSet[user].has_key(item):\n                                    self.FPSet[user][item] = 1\n                                else:\n                                    self.FPSet[user][item] += 1\n\n    def buildModel(self):\n        self.b = np.random.random(self.num_items)\n\n        print \'Training...\'\n        iteration = 0\n        while iteration < self.maxIter:\n            self.loss = 0\n            itemList = self.data.item.keys()\n            for user in self.PositiveSet:\n                u = self.data.user[user]\n                kItems = self.FPSet[user].keys()\n                for item in self.PositiveSet[user]:\n                    i = self.data.item[item]\n\n                    if len(self.FPSet[user]) > 0:\n                        item_k = choice(kItems)\n                        k = self.data.item[item_k]\n                        Suk = self.FPSet[user][kItems]\n                        s = sigmoid((self.P[u].dot(self.Q[i]) - self.P[u].dot(self.Q[k])+self.b[i]-self.b[k])/ (Suk+1))\n                        self.P[u] += 1 / (Suk+1) *self.lRate * (1 - s) * (self.Q[i] - self.Q[k])\n                        self.Q[i] += 1 / (Suk+1) *self.lRate * (1 - s) * self.P[u]\n                        self.Q[k] -= 1 / (Suk+1) *self.lRate * (1 - s) * self.P[u]\n\n\n                        item_j = choice(itemList)\n                        while (self.PositiveSet[user].has_key(item_j) or self.FPSet.has_key(item_j)):\n                            item_j = choice(itemList)\n                        j = self.data.item[item_j]\n                        s = sigmoid(self.P[u].dot(self.Q[k]) - self.P[u].dot(self.Q[j])+self.b[k]-self.b[j])\n                        self.P[u] +=  self.lRate * (1 - s) * (self.Q[k] - self.Q[j])\n                        self.Q[k] += self.lRate * (1 - s) * self.P[u]\n                        self.Q[j] -= self.lRate * (1 - s) * self.P[u]\n\n\n                        self.P[u] -= self.lRate * self.regU * self.P[u]\n                        self.Q[i] -= self.lRate * self.regI * self.Q[i]\n                        self.Q[j] -= self.lRate * self.regI * self.Q[j]\n                        self.Q[k] -= self.lRate * self.regI * self.Q[k]\n\n                        self.loss += -log(sigmoid((self.P[u].dot(self.Q[i]) - self.P[u].dot(self.Q[k]))/ (Suk+1))) \\\n                                     - log(sigmoid(self.P[u].dot(self.Q[k]) - self.P[u].dot(self.Q[j])))\n                    else:\n                        item_j = choice(itemList)\n                        while (self.PositiveSet[user].has_key(item_j)):\n                            item_j = choice(itemList)\n                        j = self.data.item[item_j]\n                        s = sigmoid(self.P[u].dot(self.Q[i]) - self.P[u].dot(self.Q[j])+self.b[i]-self.b[j])\n                        self.P[u] += self.lRate * (1 - s) * (self.Q[i] - self.Q[j])\n                        self.Q[i] += self.lRate * (1 - s) * self.P[u]\n                        self.Q[j] -= self.lRate * (1 - s) * self.P[u]\n\n\n                        self.loss += -log(s)\n\n                self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()+self.b.dot(self.b)\n            iteration += 1\n            if self.isConverged(iteration):\n                break\n\n\n    def next_batch(self):\n        batch_id=0\n        while batch_id<self.train_size:\n            if batch_id+self.batch_size<=self.train_size:\n                users = [self.data.trainingData[idx][0] for idx in range(batch_id,self.batch_size+batch_id)]\n                items = [self.data.trainingData[idx][1] for idx in range(batch_id,self.batch_size+batch_id)]\n                batch_id+=self.batch_size\n            else:\n                users = [self.data.trainingData[idx][0] for idx in range(batch_id, self.train_size)]\n                items = [self.data.trainingData[idx][1] for idx in range(batch_id, self.train_size)]\n                batch_id=self.train_size\n\n            u_idx,i_idx,f_idx,j_idx,weights = [],[],[],[],[]\n            item_list = self.data.item.keys()\n            for i,user in enumerate(users):\n\n                i_idx.append(self.data.item[items[i]])\n                u_idx.append(self.data.user[user])\n\n                if len(self.FPSet[user])==0:\n                    f_item = choice(item_list)\n                    weights.append(0)\n                else:\n                    f_item = choice(self.FPSet[user].keys())\n                    weights.append(self.FPSet[user][f_item])\n\n                f_idx.append(self.data.item[f_item])\n\n                neg_item = choice(item_list)\n                while neg_item in self.data.trainSet_u[user]:\n                    neg_item = choice(item_list)\n                j_idx.append(self.data.item[neg_item])\n\n            yield u_idx,i_idx,f_idx,j_idx,weights\n\n    def buildModel_tf(self):\n        super(SBPR, self).buildModel_tf()\n        self.social_idx = tf.placeholder(tf.int32, name=""social_holder"")\n        self.neg_idx = tf.placeholder(tf.int32, name=""neg_holder"")\n        self.weights = tf.placeholder(tf.float32, name=""weights"")\n\n        self.neg_item_embedding = tf.nn.embedding_lookup(self.V, self.neg_idx)\n        self.social_item_embedding = tf.nn.embedding_lookup(self.V, self.social_idx)\n        # self.pos_item_bias = tf.nn.embedding_lookup(self.item_biases, self.u_idx)\n        # self.neg_item_bias = tf.nn.embedding_lookup(self.item_biases, self.neg_idx)\n        # self.social_item_bias = tf.nn.embedding_lookup(self.item_biases, self.social_idx)\n\n        y_ik = (tf.reduce_sum(tf.multiply(self.user_embedding, self.item_embedding), 1)\n                -tf.reduce_sum(tf.multiply(self.user_embedding, self.social_item_embedding), 1))/(self.weights+1)\n        y_kj = tf.reduce_sum(tf.multiply(self.user_embedding, self.social_item_embedding), 1)\\\n               -tf.reduce_sum(tf.multiply(self.user_embedding, self.neg_item_embedding), 1)\n        loss = -tf.reduce_sum(tf.log(tf.sigmoid(y_ik))) - tf.reduce_sum(tf.log(tf.sigmoid(y_kj)))\n        + self.regU * (tf.nn.l2_loss(self.user_embedding) + tf.nn.l2_loss(self.item_embedding)\n                       + tf.nn.l2_loss(self.neg_item_embedding)+tf.nn.l2_loss(self.social_item_embedding))\n                       #+tf.nn.l2_loss(self.pos_item_bias)+tf.nn.l2_loss(self.social_item_bias)+tf.nn.l2_loss(self.neg_item_bias))\n\n        opt = tf.train.AdamOptimizer(self.lRate)\n\n        train = opt.minimize(loss)\n\n        with tf.Session() as sess:\n            init = tf.global_variables_initializer()\n            sess.run(init)\n            for iteration in range(self.maxIter):\n                for n, batch in enumerate(self.next_batch()):\n                    user_idx, i_idx, s_idx,j_idx,weights = batch\n                    _, l = sess.run([train, loss],\n                                    feed_dict={self.u_idx: user_idx, self.neg_idx: j_idx, self.v_idx: i_idx,self.social_idx:s_idx,self.weights:weights})\n                    print \'training:\', iteration + 1, \'batch\', n, \'loss:\', l\n            self.P, self.Q,self.b = sess.run([self.U, self.V,self.item_biases])\n            self.b = self.b.reshape(self.num_items)\n\n\n\n    def predict(self,user,item):\n\n        if self.data.containsUser(user) and self.data.containsItem(item):\n            u = self.data.getUserId(user)\n            i = self.data.getItemId(item)\n            predictRating = sigmoid(self.Q[i].dot(self.P[u]))\n            return predictRating\n        else:\n            return sigmoid(self.data.globalMean)\n\n\n    def predictForRanking(self, u):\n        \'invoked to rank all the items for the user\'\n        if self.data.containsUser(u):\n            u = self.data.getUserId(u)\n            return self.Q.dot(self.P[u])\n        else:\n            return [self.data.globalMean] * self.num_items\n\n\n'"
algorithm/ranking/SERec.py,0,"b""from baseclass.SocialRecommender import SocialRecommender\nfrom scipy.sparse import *\nfrom scipy import *\nimport numpy as np\nfrom numpy import linalg as LA\nfrom joblib import Parallel, delayed\nfrom math import sqrt\n\nEPS = 1e-8\n# this algorithm refers to the following paper:\n# #########----  Collaborative Filtering with Social Exposure: A Modular Approach to Social Recommendation   ----#############\n# SEREC_boost\n\nclass SERec(SocialRecommender):\n    def __init__(self,conf,trainingSet=None,testSet=None,relation=None,fold='[1]'):\n        super(SERec, self).__init__(conf,trainingSet,testSet,relation,fold)\n\n    def initModel(self):\n        super(SERec, self).initModel()\n        self.lam_theta = 1e-5\n        self.lam_beta = 1e-5\n        self.lam_y = 0.01\n        self.init_mu = 0.01\n        self.a = 1.0\n        self.b = 99.0\n        self.s= 2.2\n        self.init_std = 0.5\n        self.theta = self.init_std * \\\n            np.random.randn(self.num_users, self.embed_size).astype(np.float32)\n        self.beta = self.init_std * \\\n            np.random.randn(self.num_items, self.embed_size).astype(np.float32)\n        self.mu = self.init_mu * np.ones((self.num_users,self.num_items), dtype=np.float32)\n        self.n_jobs=4\n        self.batch_size=1000\n        row,col,val = [],[],[]\n        for user in self.data.trainSet_u:\n            for item in self.data.trainSet_u[user]:\n                u = self.data.user[user]\n                i = self.data.item[item]\n                row.append(u)\n                col.append(i)\n                val.append(1)\n\n        self.X = csr_matrix((np.array(val),(np.array(row),np.array(col))),(self.num_users,self.num_items))\n        row,col,val = [],[],[]\n        for user in self.social.followees:\n            for f in self.social.followees[user]:\n                u = self.data.user[user]\n                i = self.data.user[f]\n                row.append(u)\n                col.append(i)\n                val.append(1)\n        self.T = csr_matrix((np.array(val), (np.array(row), np.array(col))), (self.num_users, self.num_users))\n\n\n    def buildModel(self):\n        print 'training...'\n        iteration = 0\n\n        self._update(self.X)\n\n    def _update(self, X):\n        '''Model training and evaluation on validation set'''\n        n_users = X.shape[0]\n        XT = X.T.tocsr()  # pre-compute this\n        self.vad_ndcg = -np.inf\n        for i in xrange(self.maxIter):\n\n            print 'ITERATION #%d' % i\n            self._update_factors(X, XT)\n            print self.mu\n            self._update_expo(X, n_users)\n            self.ranking_performance()\n\n\n    def _update_factors(self, X, XT):\n        '''Update user and item collaborative factors with ALS'''\n        self.theta = recompute_factors(self.beta, self.theta, X,\n                                       self.lam_theta / self.lam_y,\n                                       self.lam_y,\n                                       self.mu,\n                                       self.n_jobs,\n                                       batch_size=self.batch_size)\n\n        self.beta = recompute_factors(self.theta, self.beta, XT,\n                                      self.lam_beta / self.lam_y,\n                                      self.lam_y,\n                                      self.mu,\n                                      self.n_jobs,\n                                      batch_size=self.batch_size)\n\n\n    def _update_expo(self, X, n_users):\n        '''Update exposure prior'''\n        print '\\tUpdating exposure prior...'\n\n        start_idx = range(0, n_users, self.batch_size)\n        end_idx = start_idx[1:] + [n_users]\n\n        A_sum = np.zeros(self.num_items)\n        for lo, hi in zip(start_idx, end_idx):\n            A_sum += a_row_batch(X[lo:hi], self.theta[lo:hi], self.beta,\n                                 self.lam_y, self.mu[lo:hi]).sum(axis=0)\n\n        A_sum=np.tile(A_sum,[self.num_users,1])\n        S_sum = self.T.dot(A_sum)\n        self.mu = (self.a + A_sum +(self.s-1)*S_sum- 1) / (self.a + self.b + (self.s-1)*S_sum+n_users - 2)\n\n\n    def predictForRanking(self,u):\n        'invoked to rank all the items for the user'\n        if self.data.containsUser(u):\n            u = self.data.getUserId(u)\n            return self.beta.dot(self.theta[u])\n        else:\n            return [self.data.globalMean] * self.num_items\n\n# Utility functions #\n\n\n\ndef get_row(Y, i):\n    '''Given a scipy.sparse.csr_matrix Y, get the values and indices of the\n    non-zero values in i_th row'''\n    lo, hi = Y.indptr[i], Y.indptr[i + 1]\n    return Y.data[lo:hi], Y.indices[lo:hi]\n\ndef a_row_batch(Y_batch, theta_batch, beta, lam_y, mu):\n    '''Compute the posterior of exposure latent variables A by batch'''\n    pEX = sqrt(lam_y / 2 / np.pi) * \\\n          np.exp(-lam_y * theta_batch.dot(beta.T) ** 2 / 2)\n\n    A = (pEX + EPS) / (pEX + EPS + (1 - mu) / mu)\n    A[Y_batch.nonzero()] = 1.\n    return A\n\ndef _solve(k, A_k, X, Y, f, lam, lam_y, mu):\n    '''Update one single factor'''\n    s_u, i_u = get_row(Y, k)\n    a = np.dot(s_u * A_k[i_u], X[i_u])\n    B = X.T.dot(A_k[:, np.newaxis] * X) + lam * np.eye(f)\n    return LA.solve(B, a)\n\ndef _solve_batch(lo, hi, X, X_old_batch, Y, m, f, lam, lam_y, mu):\n    '''Update factors by batch, will eventually call _solve() on each factor to\n    keep the parallel process busy'''\n    assert X_old_batch.shape[0] == hi - lo\n\n    if mu.shape[1] == X.shape[0]:  # update users\n        A_batch = a_row_batch(Y[lo:hi], X_old_batch, X, lam_y, mu[lo:hi])\n    else:  # update items\n        A_batch = a_row_batch(Y[lo:hi], X_old_batch, X, lam_y, mu.T[lo:hi])\n\n    X_batch = np.empty_like(X_old_batch, dtype=X_old_batch.dtype)\n    for ib, k in enumerate(xrange(lo, hi)):\n        X_batch[ib] = _solve(k, A_batch[ib], X, Y, f, lam, lam_y, mu)\n    return X_batch\n\ndef recompute_factors(X, X_old, Y, lam, lam_y, mu, n_jobs, batch_size=1000):\n    '''Regress X to Y with exposure matrix (computed on-the-fly with X_old) and\n    ridge term lam by embarrassingly parallelization. All the comments below\n    are in the view of computing user factors'''\n    m, n = Y.shape  # m = number of users, n = number of items\n    assert X.shape[0] == n\n    assert X_old.shape[0] == m\n    f = X.shape[1]  # f = number of factors\n\n    start_idx = range(0, m, batch_size)\n    end_idx = start_idx[1:] + [m]\n    res = Parallel(n_jobs=n_jobs)(delayed(_solve_batch)(\n        lo, hi, X, X_old[lo:hi], Y, m, f, lam, lam_y, mu)\n                                   for lo, hi in zip(start_idx, end_idx))\n    # res = []\n    # for lo, hi in zip(start_idx, end_idx):\n    #     res.append(_solve_batch(lo, hi, X, X_old[lo:hi], Y, m, f, lam, lam_y, mu))\n\n    X_new = np.vstack(res)\n    return X_new\n\n"""
algorithm/ranking/TBPR.py,0,"b""from baseclass.SocialRecommender import SocialRecommender\nfrom math import log\nimport numpy as np\nfrom tool import config\nfrom tool.qmath import sigmoid\nfrom random import choice\nfrom collections import defaultdict\nclass TBPR(SocialRecommender):\n    def __init__(self,conf,trainingSet=None,testSet=None,relation=list(),fold='[1]'):\n        super(TBPR, self).__init__(conf,trainingSet,testSet,relation,fold)\n\n    def readConfiguration(self):\n        super(TBPR, self).readConfiguration()\n        options = config.LineConfig(self.config['TBPR'])\n        self.regT = float(options['-regT'])\n\n\n    def initModel(self):\n        super(TBPR, self).initModel()\n        self.strength = defaultdict(dict)\n        self.weakTies = defaultdict(dict)\n        self.strongTies = defaultdict(dict)\n        self.weights = []\n        for u1 in self.social.user:\n            N_u1 = self.social.getFollowees(u1).keys()\n            for u2 in self.social.getFollowees(u1):\n                if u1==u2:\n                    continue\n                N_u2 = self.social.getFollowees(u2).keys()\n                s = len(set(N_u1).intersection(set(N_u2)))/(len(set(N_u1).union(set(N_u2)))+0.0)\n                self.strength[u1][u2]=s\n                self.weights.append(s)\n        self.weights.sort()\n        self.weights = np.array(self.weights)\n        self.theta = np.median(self.weights)\n        for u1 in self.strength:\n            for u2 in self.strength[u1]:\n                if self.strength[u1][u2]>self.theta:\n                    self.strongTies[u1][u2]=self.strength[u1][u2]\n                else:\n                    self.weakTies[u1][u2]=self.strength[u1][u2]\n        self.t_s = self.weights[len(self.weights)/2+1:].sum()/(len(self.weights[len(self.weights)/2+1:])+0.0)\n        self.t_w = self.weights[0:len(self.weights)/2].sum()/(len(self.weights[0:len(self.weights)/2])+0.0)\n\n    \n    \n    \n    def optimization(self,u,i,j):\n        s = sigmoid(self.P[u].dot(self.Q[i]) - self.P[u].dot(self.Q[j]))\n        self.P[u] += self.lRate * (1 - s) * (self.Q[i] - self.Q[j])\n        self.Q[i] += self.lRate * (1 - s) * self.P[u]\n        self.Q[j] -= self.lRate * (1 - s) * self.P[u]\n        self.loss += -log(s)\n        self.P[u] -= self.lRate * self.regU * self.P[u]\n        self.Q[i] -= self.lRate * self.regI * self.Q[i]\n        self.Q[j] -= self.lRate * self.regI * self.Q[j]\n\n    def optimization_theta(self,u,i,j):\n        # s = sigmoid(self.P[u].dot(self.Q[i]) - self.P[u].dot(self.Q[j]))\n        # self.P[u] += self.lRate * (1 - s) * (self.Q[i] - self.Q[j])\n        # self.Q[i] += self.lRate * (1 - s) * self.P[u]\n        # self.Q[j] -= self.lRate * (1 - s) * self.P[u]\n        # self.loss += -log(s)\n        # self.P[u] -= self.lRate * self.regU * self.P[u]\n        # self.Q[i] -= self.lRate * self.regI * self.Q[i]\n        # self.Q[j] -= self.lRate * self.regI * self.Q[j]\n        s = sigmoid((self.P[u].dot(self.Q[i]) - self.P[u].dot(self.Q[j]))/(1+1/self.g_theta))\n        self.P[u] += self.lRate * 1/(1+1/self.g_theta)*(1 - s) * (self.Q[i] - self.Q[j])\n        self.Q[i] += self.lRate * 1/(1+1/self.g_theta)*(1 - s) * self.P[u]\n        self.Q[j] -= self.lRate * 1/(1+1/self.g_theta)*(1 - s) * self.P[u]\n        self.loss += -log(s)\n        self.P[u] -= self.lRate * self.regU * self.P[u]\n        self.Q[i] -= self.lRate * self.regI * self.Q[i]\n        self.Q[j] -= self.lRate * self.regI * self.Q[j]\n        self.theta_derivative += self.regT * self.theta + ((1 - s)*(self.P[u].dot(self.Q[i]) - self.P[u].dot(self.Q[j]))*(self.t_w+self.t_s-2*self.theta))/(self.g_theta+1)**2\n        self.theta_count+=1\n\n    def buildModel(self):\n        self.positiveSet = defaultdict(dict)\n        for user in self.data.user:\n            for item in self.data.trainSet_u[user]:\n                if self.data.trainSet_u[user][item] >= 1:\n                    self.positiveSet[user][item] = 1\n\n        print 'Training...'\n        iteration = 0\n        while iteration < self.maxIter:\n            self.theta_derivative=0\n            self.theta_count = 0\n            if self.theta>self.weights.max():\n                self.theta=self.weights.max()-0.01\n            if self.theta<self.weights.min():\n                self.theta=self.weights.min()+0.01\n            try:\n                self.t_s = sum([item for item in self.weights if item >= self.theta])/len([item for item in self.weights if item >= self.theta])\n                self.t_w = sum([item for item in self.weights if item <= self.theta])/len([item for item in self.weights if item <= self.theta])\n            except ZeroDivisionError:\n                self.t_w = 0.01\n                self.theta=0.02\n                #pass\n            # if self.theta==0:\n            #     self.theta=0.02\n            self.g_theta = (self.t_s-self.theta)*(self.theta-self.t_w)\n            print 'Theta:',self.theta\n            print 'g_theta:',self.g_theta\n            print 'Preparing item sets...'\n\n            self.jointSet = defaultdict(dict)\n            self.strongSet = defaultdict(dict)\n            self.weakSet = defaultdict(dict)\n\n            for u1 in self.social.user:\n                if self.data.user.has_key(u1):\n                    for u2 in self.strongTies[u1]:\n                        for item in self.data.trainSet_u[u2]:\n                            if self.data.trainSet_u[u2][item] >= 1 and not self.positiveSet[u1].has_key(item):\n                                self.strongSet[u1][item]=1\n\n                    for u2 in self.weakTies[u1]:\n                        for item in self.data.trainSet_u[u2]:\n                            if self.data.trainSet_u[u2][item] >= 1 and not self.positiveSet[u1].has_key(item):\n                                self.weakSet[u1][item]=1\n\n            for u1 in self.social.user:\n                if self.data.user.has_key(u1):\n                    self.jointSet[u1] = dict.fromkeys(set(self.strongSet[u1].keys()).intersection(set(self.weakSet[u1].keys())),1)\n\n            for u1 in self.jointSet:\n                for item in self.jointSet[u1]:\n                    del self.strongSet[u1][item]\n                    del self.weakSet[u1][item]\n                    if len(self.strongSet[u1])==0:\n                        del self.strongSet[u1]\n                    if len(self.weakSet[u1])==0:\n                        del self.weakSet[u1]\n\n            print 'Computing...'\n            self.loss = 0\n            itemList = self.data.item.keys()\n            for user in self.positiveSet:\n                #print user\n                u = self.data.user[user]\n                jItems = self.jointSet[user].keys()\n                wItems = self.weakSet[user].keys()\n                sItems = self.strongSet[user].keys()\n                for item in self.positiveSet[user]:\n                    i = self.data.item[item]\n                    selectedItems = [i]\n                    if len(jItems)>0:\n                        item_j = choice(jItems)\n                        j = self.data.item[item_j]\n                        selectedItems.append(j)\n\n                    if len(wItems) > 0:\n                        item_w = choice(wItems)\n                        w = self.data.item[item_w]\n                        selectedItems.append(w)\n\n\n                    if len(sItems) > 0:\n                        item_s = choice(sItems)\n                        s = self.data.item[item_s]\n                        selectedItems.append(s)\n\n                    item_k = choice(itemList)\n                    while (self.positiveSet[user].has_key(item_k)):\n                        item_k = choice(itemList)\n                    k = self.data.item[item_k]\n                    selectedItems.append(k)\n\n                    # optimization\n                    for ind, item in enumerate(selectedItems[:-1]):\n                        self.optimization(u, item, selectedItems[ind + 1])\n\n                self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()\n            if self.theta_count>0:\n                self.theta -= self.lRate*self.theta_derivative/self.theta_count\n                self.weakTies = defaultdict(dict)\n                self.strongTies = defaultdict(dict)\n                for u1 in self.strength:\n                    for u2 in self.strength[u1]:\n                        if self.strength[u1][u2] > self.theta:\n                            self.strongTies[u1][u2] = self.strength[u1][u2]\n                        else:\n                            self.weakTies[u1][u2] = self.strength[u1][u2]\n            iteration += 1\n            if self.isConverged(iteration):\n                break\n\n\n\n\n    def predictForRanking(self, u):\n        'invoked to rank all the items for the user'\n        if self.data.containsUser(u):\n            u = self.data.getUserId(u)\n            return self.Q.dot(self.P[u])\n        else:\n            return [self.data.globalMean] * self.num_items\n\n"""
algorithm/ranking/WRMF.py,0,"b""################# Confidence Frequency Matrix Factorization #################\n#                   Weighted Rating Matrix  Factorization                   #\n# this algorithm refers to the following paper:\n# Yifan Hu et al.Collaborative Filtering for Implicit Feedback Datasets\nfrom baseclass.IterativeRecommender import IterativeRecommender\nfrom scipy.sparse import *\nfrom scipy import *\nimport numpy as np\nclass WRMF(IterativeRecommender):\n    def __init__(self,conf,trainingSet=None,testSet=None,fold='[1]'):\n        super(WRMF, self).__init__(conf,trainingSet,testSet,fold)\n\n    def initModel(self):\n        super(WRMF, self).initModel()\n        self.X=self.P*10\n        self.Y=self.Q*10\n\n\n    def buildModel(self):\n        print 'training...'\n        iteration = 0\n        while iteration < self.maxIter:\n            self.loss = 0\n            YtY = self.Y.T.dot(self.Y)\n            for user in self.data.user:\n                #C_u = np.ones(self.data.getSize(self.recType))\n                H = np.ones(self.num_items)\n                val = []\n                pos = []\n                P_u = np.zeros(self.num_items)\n                uid = self.data.user[user]\n                for item in self.data.trainSet_u[user]:\n                    iid = self.data.item[item]\n                    r_ui = float(self.data.trainSet_u[user][item])\n                    pos.append(iid)\n                    val.append(10*r_ui)\n                    H[iid]+=10*r_ui\n                    P_u[iid]=1\n                    error = (P_u[iid]-self.X[uid].dot(self.Y[iid]))\n                    self.loss+=pow(error,2)\n                #sparse matrix\n                C_u = coo_matrix((val,(pos,pos)),shape=(self.num_items,self.num_items))\n                A = (YtY + np.dot(self.Y.T,C_u.dot(self.Y))+self.regU*np.eye(self.embed_size))\n                self.X[uid] = np.dot(np.linalg.inv(A),(self.Y.T*H).dot(P_u))\n\n            XtX = self.X.T.dot(self.X)\n            for item in self.data.item:\n                P_i = np.zeros(self.num_users)\n                iid = self.data.item[item]\n                H = np.ones(self.num_users)\n                val = []\n                pos = []\n                for user in self.data.trainSet_i[item]:\n                    uid = self.data.user[user]\n                    r_ui = float(self.data.trainSet_i[item][user])\n                    pos.append(uid)\n                    val.append(10*r_ui)\n                    H[uid] += 10*r_ui\n                    P_i[uid] = 1\n                # sparse matrix\n                C_i = coo_matrix((val, (pos, pos)),shape=(self.num_users,self.num_users))\n                A = (XtX + np.dot(self.X.T,C_i.dot(self.X))+self.regU*np.eye(self.embed_size))\n                self.Y[iid]=np.dot(np.linalg.inv(A), (self.X.T*H).dot(P_i))\n\n            #self.loss += (self.X * self.X).sum() + (self.Y * self.Y).sum()\n            iteration += 1\n            print 'iteration:',iteration,'loss:',self.loss\n            # if self.isConverged(iteration):\n            #     break\n\n    def predictForRanking(self,u):\n        'invoked to rank all the items for the user'\n        if self.data.containsUser(u):\n            u = self.data.getUserId(u)\n            return self.Y.dot(self.X[u])\n        else:\n            return [self.data.globalMean] * self.num_items"""
algorithm/ranking/__init__.py,0,b''
algorithm/rating/BasicMF.py,5,"b""#coding:utf-8\nfrom baseclass.IterativeRecommender import IterativeRecommender\nimport numpy as np\n\nclass BasicMF(IterativeRecommender):\n    def __init__(self,conf,trainingSet=None,testSet=None,fold='[1]'):\n        super(BasicMF, self).__init__(conf,trainingSet,testSet,fold)\n\n    def buildModel(self):\n        iteration = 0\n        while iteration < self.maxIter:\n            self.loss = 0\n            for entry in self.data.trainingData:\n                user, item, rating = entry\n                u = self.data.user[user]\n                i = self.data.item[item]\n                error = rating - self.P[u].dot(self.Q[i])\n                self.loss += error**2\n                p = self.P[u]\n                q = self.Q[i]\n\n                #update latent vectors\n                self.P[u] += self.lRate*error*q\n                self.Q[i] += self.lRate*error*p\n\n\n            iteration += 1\n            if self.isConverged(iteration):\n                break\n\n    def buildModel_tf(self):\n        super(BasicMF, self).buildModel_tf()\n\n        import tensorflow as tf\n        # \xe6\x9e\x84\xe9\x80\xa0\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0 \xe8\xae\xbe\xe7\xbd\xae\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\n\n        self.r_hat = tf.reduce_sum(tf.multiply(self.user_embedding, self.item_embedding), axis=1)\n        self.total_loss = tf.nn.l2_loss(self.r- self.r_hat)\n\n        self.optimizer = tf.train.AdamOptimizer(self.lRate)\n        self.train = self.optimizer.minimize(self.total_loss, var_list=[self.U, self.V])\n\n        # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xbc\x9a\xe8\xaf\x9d\n        with tf.Session() as sess:\n            init = tf.global_variables_initializer()\n            sess.run(init)\n\n            # \xe8\xbf\xad\xe4\xbb\xa3\xef\xbc\x8c\xe4\xbc\xa0\xe9\x80\x92\xe5\x8f\x98\xe9\x87\x8f\n            for step in range(self.maxIter):\n                # \xe6\x8c\x89\xe6\x89\xb9\xe4\xbc\x98\xe5\x8c\x96\n                batch_size = self.batch_size\n\n                batch_idx = np.random.randint(self.train_size, size=batch_size)\n\n                user_idx = [self.data.user[self.data.trainingData[idx][0]] for idx in batch_idx]\n                item_idx = [self.data.item[self.data.trainingData[idx][1]] for idx in batch_idx]\n                rating = [self.data.trainingData[idx][2] for idx in batch_idx]\n                sess.run(self.train, feed_dict={self.r: rating, self.u_idx: user_idx, self.v_idx: item_idx})\n                print 'iteration:', step, 'loss:', sess.run(self.total_loss,\n                                                            feed_dict={self.r: rating, self.u_idx: user_idx,\n                                                                       self.v_idx: item_idx})\n\n            # \xe8\xbe\x93\xe5\x87\xba\xe8\xae\xad\xe7\xbb\x83\xe5\xae\x8c\xe6\xaf\x95\xe7\x9a\x84\xe7\x9f\xa9\xe9\x98\xb5\n            self.P = sess.run(self.U)\n            self.Q = sess.run(self.V)\n\n"""
algorithm/rating/CUNE_MF.py,0,"b""from baseclass.IterativeRecommender import IterativeRecommender\nfrom tool import config\nfrom random import randint\nfrom random import shuffle,choice\nfrom collections import defaultdict\nimport numpy as np\nfrom tool.qmath import sigmoid,cosine\nfrom math import log\nimport gensim.models.word2vec as w2v\nfrom structure.symmetricMatrix import SymmetricMatrix\n# class Node(object):\n#     def __init__(self):\n#         self.val = 0\n#         self.last = None\n#         self.next = None\n#\n# class OrderedLinkList(object):\n#     def __init__(self):\n#         self.head=None\n#         self.tail=None\n#         self.length = 0\n#\n#     def __len__(self):\n#         return self.length\n#\n#     def insert(self,node):\n#         self.length+=1\n#         if self.head:\n#             tmp = self.head\n#             while tmp.val < node.val:\n#                 if tmp==self.tail:\n#                     break\n#                 tmp = tmp.next\n#\n#             if tmp is self.head:\n#\n#                 if self.head.val < node.val:\n#                     node.next = self.head\n#                     self.head.last = node\n#                     self.head = node\n#                 else:\n#                     node.next = self.head\n#                     self.head.last = node\n#                     self.head = node\n#                 return\n#\n#             node.next = tmp.next\n#             tmp.next = node\n#             node.last = tmp\n#             if not node.next:\n#                 self.tail = node\n#\n#         else:\n#             self.head = node\n#             self.tail = node\n#\n#     def removeHead(self):\n#         if self.head:\n#             self.head = self.head.next\n#             self.length -= 1\n#\n#     def removeNode(self,node):\n#         if self.head:\n#             tmp = self.head\n#             while tmp is not node and tmp.next:\n#                 tmp = tmp.next\n#             if tmp.next:\n#                 tmp.last.next = tmp.next\n#                 tmp.next.last = tmp.last\n#             self.length-=1\n#\n#\n# class HTreeNode(object):\n#     def __init__(self,left,right,freq,id,code=None):\n#         self.left = left\n#         self.right = right\n#         self.weight = freq\n#         self.id = id\n#         self.code = code\n#\n#     def __lt__(self, other):\n#         if self.weight < other.weight:\n#             return True\n#         else:\n#             return False\n#\n# class HuffmanTree(object):\n#     def __init__(self,root=None,vecLength=10):\n#         self.root = root\n#         self.weight = 0\n#         self.code = {}\n#         self.vecLength = vecLength\n#         self.vector = {}\n#\n#     def buildFromTrees(self,left,right):\n#         root = HTreeNode(left.val,right.val,left.val.weight+right.val.weight,None)\n#         return root\n#\n#     def buildTree(self,nodeList):\n#         if len(nodeList)<2:\n#             self.root = nodeList.head\n#             return\n#\n#         while(len(nodeList)>1):\n#             left = nodeList.head\n#             right = nodeList.head.next\n#             nodeList.removeHead()\n#             nodeList.removeHead()\n#             tree = self.buildFromTrees(left,right)\n#             node = Node()\n#             node.val = tree\n#             nodeList.insert(node)\n#\n#         self.root = nodeList.head.val\n#\n#     def coding(self,root,prefix,hierarchy):\n#         if root:\n#             root.code = prefix\n#             self.vector[prefix] = np.random.random(self.vecLength)\n#             if root.id:\n#                 self.code[root.id] = prefix\n#\n#             # if root.id:\n#             #     print 'level', hierarchy\n#             #     print root.id,prefix,root.weight\n#\n#             self.coding(root.left,prefix+'0',hierarchy+1)\n#             self.coding(root.right,prefix+'1',hierarchy+1)\n\nclass CUNE_MF(IterativeRecommender):\n    def __init__(self,conf,trainingSet=None,testSet=None,fold='[1]'):\n        super(CUNE_MF, self).__init__(conf,trainingSet,testSet,fold)\n        self.nonLeafVec = {}\n        self.leafVec = {}\n\n\n    def readConfiguration(self):\n        super(CUNE_MF, self).readConfiguration()\n        options = config.LineConfig(self.config['CUNE-MF'])\n        self.walkCount = int(options['-T'])\n        self.walkLength = int(options['-L'])\n        self.walkDim = int(options['-l'])\n        self.winSize = int(options['-w'])\n        self.topK = int(options['-k'])\n        self.epoch = int(options['-ep'])\n        self.alpha = float(options['-a'])\n\n    def printAlgorConfig(self):\n        super(CUNE_MF, self).printAlgorConfig()\n        print 'Specified Arguments of', self.config['recommender'] + ':'\n        print 'Walks count per user', self.walkCount\n        print 'Length of each walk', self.walkLength\n        print 'Dimension of user embedding', self.walkDim\n        print '='*80\n\n    def buildModel(self):\n        print 'Kind Note: This method will probably take much time.'\n        #build C-U-NET\n        print 'Building collaborative user network...'\n        #filter isolated nodes\n        self.itemNet = {}\n        for item in self.data.trainSet_i:\n            if len(self.data.trainSet_i[item])>1:\n                self.itemNet[item] = self.data.trainSet_i[item]\n\n        self.filteredRatings = defaultdict(list)\n        for item in self.itemNet:\n            for user in self.itemNet[item]:\n                if self.itemNet[item][user]>=1:\n                    self.filteredRatings[user].append(item)\n\n        self.CUNet = defaultdict(list)\n\n        for user1 in self.filteredRatings:\n            s1 = set(self.filteredRatings[user1])\n            for user2 in self.filteredRatings:\n                if user1 <> user2:\n                    s2 = set(self.filteredRatings[user2])\n                    weight = len(s1.intersection(s2))\n                    if weight > 0:\n                        self.CUNet[user1]+=[user2]*weight\n\n\n        #build Huffman Tree First\n        #get weight\n        # print 'Building Huffman tree...'\n        # #To accelerate the method, the weight is estimated roughly\n        # nodes = {}\n        # for user in self.CUNet:\n        #     nodes[user] = len(self.CUNet[user])\n        # nodes = sorted(nodes.iteritems(),key=lambda d:d[1])\n        # nodes = [HTreeNode(None,None,user[1],user[0]) for user in nodes]\n        # nodeList = OrderedLinkList()\n        # for node in nodes:\n        #     listNode = Node()\n        #     listNode.val = node\n        #     try:\n        #         nodeList.insert(listNode)\n        #     except AttributeError:\n        #         pass\n        # self.HTree = HuffmanTree(vecLength=self.walkDim)\n        # self.HTree.buildTree(nodeList)\n        # print 'Coding for all users...'\n        # self.HTree.coding(self.HTree.root,'',0)\n\n\n        print 'Generating random deep walks...'\n        self.walks = []\n        self.visited = defaultdict(dict)\n        for user in self.CUNet:\n            for t in range(self.walkCount):\n                path = [user]\n                lastNode = user\n                for i in range(1,self.walkLength):\n                    nextNode = choice(self.CUNet[lastNode])\n                    count=0\n                    while(self.visited[lastNode].has_key(nextNode)):\n                        nextNode = choice(self.CUNet[lastNode])\n                        #break infinite loop\n                        count+=1\n                        if count==10:\n                            break\n                    path.append(nextNode)\n                    self.visited[user][nextNode] = 1\n                    lastNode = nextNode\n                self.walks.append(path)\n                #print path\n        shuffle(self.walks)\n\n        #Training get top-k friends\n        print 'Generating user embedding...'\n        # iteration = 1\n        # while iteration <= self.epoch:\n        #     loss = 0\n        #     #slide windows randomly\n        #\n        #     for n in range(self.walkLength/self.winSize):\n        #\n        #         for walk in self.walks:\n        #             center = randint(0, len(walk)-1)\n        #             s = max(0,center-self.winSize/2)\n        #             e = min(center+self.winSize/2,len(walk)-1)\n        #             for user in walk[s:e]:\n        #                 centerUser = walk[center]\n        #                 if user <> centerUser:\n        #                     code = self.HTree.code[user]\n        #                     centerCode = self.HTree.code[centerUser]\n        #                     x = self.HTree.vector[centerCode]\n        #                     for i in range(1,len(code)):\n        #                         prefix = code[0:i]\n        #                         w = self.HTree.vector[prefix]\n        #                         self.HTree.vector[prefix] += self.lRate*(1-sigmoid(w.dot(x)))*x\n        #                         self.HTree.vector[centerCode] += self.lRate*(1-sigmoid(w.dot(x)))*w\n        #                         loss += -log(sigmoid(w.dot(x)),2)\n        #     print 'iteration:', iteration, 'loss:', loss\n        #     iteration+=1\n        model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)\n        print 'User embedding generated.'\n\n        print 'Constructing similarity matrix...'\n        self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10\n        self.topKSim = {}\n        i = 0\n        for user1 in self.CUNet:\n            # prefix1 = self.HTree.code[user1]\n            # vec1 = self.HTree.vector[prefix1]\n            sims = []\n            u1 = self.data.user[user1]\n            self.W[u1] = model.wv[user1]\n            for user2 in self.CUNet:\n                if user1 <> user2:\n                    u2 = self.data.user[user2]\n                    self.W[u2] = model.wv[user2]\n                    sims.append((user2,cosine(self.W[u1],self.W[u2])))\n            self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]\n            i += 1\n            if i % 200 == 0:\n                print 'progress:', i, '/', len(self.CUNet)\n        print 'Similarity matrix finished.'\n        \n        #print self.topKSim\n\n        #matrix decomposition\n        print 'Decomposing...'\n\n        iteration = 0\n        while iteration < self.maxIter:\n            self.loss = 0\n            for entry in self.data.trainingData:\n                user, item, rating = entry\n                u = self.data.user[user] #get user id\n                i = self.data.item[item] #get item id\n                error = rating - self.P[u].dot(self.Q[i])\n                self.loss += error**2\n                p = self.P[u]\n                q = self.Q[i]\n\n                #update latent vectors\n                self.P[u] += self.lRate*(error*q-self.regU*p)\n                self.Q[i] += self.lRate*(error*p-self.regI*q)\n\n            for user in self.CUNet:\n\n                u = self.data.user[user]\n                friends = self.topKSim[user]\n                for friend in friends:\n                    uf = self.data.user[friend[0]]\n                    self.P[u] -= self.lRate*(self.P[u]-self.P[uf])*self.alpha\n                    self.loss += self.alpha * (self.P[u]-self.P[uf]).dot(self.P[u]-self.P[uf])\n\n            self.loss += self.regU*(self.P*self.P).sum() + self.regI*(self.Q*self.Q).sum()\n            iteration += 1\n            if self.isConverged(iteration):\n                break\n\n"""
algorithm/rating/EE.py,21,"b'from baseclass.IterativeRecommender import IterativeRecommender\nimport numpy as np\nfrom tool import config\n\n\nclass EE(IterativeRecommender):\n    def __init__(self, conf, trainingSet=None, testSet=None, fold=\'[1]\'):\n        super(EE, self).__init__(conf, trainingSet, testSet, fold)\n\n    # def readConfiguration(self):\n    #     super(EE, self).readConfiguration()\n    #     Dim = config.LineConfig(self.config[\'EE\'])\n    #     self.Dim = int(Dim[\'-d\'])\n\n    def initModel(self):\n        super(EE, self).initModel()\n        self.Bu = np.random.rand(self.data.trainingSize()[0])/10  # bias value of user\n        self.Bi = np.random.rand(self.data.trainingSize()[1])/10  # bias value of item\n        # self.X = np.random.rand(self.data.trainingSize()[0], self.Dim)/10\n        # self.Y = np.random.rand(self.data.trainingSize()[1], self.Dim)/10\n\n    def buildModel(self):\n        iteration = 0\n        while iteration < self.maxIter:\n            self.loss = 0\n            for entry in self.data.trainingData:\n                user, item, rating = entry\n                error = rating - self.predict(user,item)\n                u = self.data.user[user]\n                i = self.data.item[item]\n                self.loss += error ** 2\n                self.loss += self.regU * (self.P[u] - self.Q[i]).dot(self.P[u] - self.Q[i])\n                bu = self.Bu[u]\n                bi = self.Bi[i]\n                #self.loss += self.regB * bu ** 2 + self.regB * bi ** 2\n                # update latent vectors\n                self.P[u] -= self.lRate * (error + self.regU) * (self.P[u] - self.Q[i])\n                self.Q[i] += self.lRate * (error + self.regI) * (self.P[u] - self.Q[i])\n                self.Bu[u] += self.lRate * (error - self.regB * bu)\n                self.Bi[i] += self.lRate * (error - self.regB * bi)\n            self.loss+=self.regB*(self.Bu*self.Bu).sum()+self.regB*(self.Bi*self.Bi).sum()\n            iteration += 1\n            self.isConverged(iteration)\n\n    def buildModel_tf(self):\n        super(EE, self).buildModel_tf()\n\n        import tensorflow as tf\n\n        global_mean = tf.placeholder(tf.float32, [None], name=""mean"")\n        reg_lambda = tf.constant(self.regU, dtype=tf.float32)\n        reg_biase = tf.constant(self.regB, dtype=tf.float32)\n\n\n\n        self.U_bias = tf.Variable(tf.truncated_normal(shape=[self.num_users], stddev=0.005,mean=0.02), name=\'U_bias\')\n        self.V_bias = tf.Variable(tf.truncated_normal(shape=[self.num_items], stddev=0.005,mean=0.02), name=\'V_bias\')\n\n        self.U_bias_embed = tf.nn.embedding_lookup(self.U_bias, self.u_idx)\n        self.V_bias_embed = tf.nn.embedding_lookup(self.V_bias, self.v_idx)\n\n        difference = tf.subtract(self.user_embedding, self.item_embedding)\n        self.r_hat = tf.reduce_sum(tf.multiply(difference, difference), axis=1)\n        self.r_hat = tf.subtract(self.U_bias_embed, self.r_hat)\n        self.r_hat = tf.add(self.r_hat, self.V_bias_embed)\n        self.r_hat = tf.add(self.r_hat, global_mean)\n\n        self.loss = tf.nn.l2_loss(tf.subtract(self.r, self.r_hat))\n        reg_loss = tf.add(tf.multiply(reg_lambda, tf.nn.l2_loss(self.user_embedding)),\n                          tf.multiply(reg_lambda, tf.nn.l2_loss(self.item_embedding)))\n        reg_loss = tf.add(reg_loss,tf.multiply(reg_biase, tf.nn.l2_loss(self.U_bias)))\n        reg_loss = tf.add(reg_loss, tf.multiply(reg_biase, tf.nn.l2_loss(self.V_bias)))\n        self.total_loss = tf.add(self.loss, reg_loss)\n        optimizer = tf.train.AdamOptimizer(self.lRate)\n        train_U = optimizer.minimize(self.total_loss, var_list=[self.U, self.U_bias])\n        train_V = optimizer.minimize(self.total_loss, var_list=[self.V, self.V_bias])\n\n        with tf.Session() as sess:\n            init = tf.global_variables_initializer()\n            sess.run(init)\n\n\n            for step in range(self.maxIter):\n\n                batch_size = self.batch_size\n\n                batch_idx = np.random.randint(self.train_size, size=batch_size)\n\n                user_idx = [self.data.user[self.data.trainingData[idx][0]] for idx in batch_idx]\n                item_idx = [self.data.item[self.data.trainingData[idx][1]] for idx in batch_idx]\n                g_mean = [self.data.globalMean]*batch_size\n                rating = [self.data.trainingData[idx][2] for idx in batch_idx]\n\n                sess.run(train_U, feed_dict={self.r: rating, self.u_idx: user_idx, self.v_idx: item_idx,global_mean:g_mean})\n                sess.run(train_V, feed_dict={self.r: rating, self.u_idx: user_idx, self.v_idx: item_idx, global_mean: g_mean})\n\n                print \'iteration:\', step, \'loss:\', sess.run(self.total_loss,\n                                                            feed_dict={self.r: rating, self.u_idx: user_idx, self.v_idx: item_idx,global_mean:g_mean})\n\n\n            self.P = sess.run(self.U)\n            self.Q = sess.run(self.V)\n            self.Bu = sess.run(self.U_bias)\n            self.Bi = sess.run(self.V_bias)\n\n    def predict(self, u, i):\n        if self.data.containsUser(u) and self.data.containsItem(i):\n            u = self.data.user[u]\n            i = self.data.item[i]\n            return self.data.globalMean + self.Bi[i] + self.Bu[u] - (self.P[u] - self.Q[i]).dot(self.P[u] - self.Q[i])\n        else:\n            return self.data.globalMean\n\n    def predictForRanking(self,u):\n        \'invoked to rank all the items for the user\'\n        if self.data.containsUser(u):\n            u = self.data.user[u]\n            res = ((self.Q-self.P[u])*(self.Q-self.P[u])).sum(axis=1)+self.Bi+self.Bu[u]+self.data.globalMean\n            return res\n        else:\n            return [self.data.globalMean]*self.num_items\n\n'"
algorithm/rating/ItemKNN.py,0,"b'from baseclass.Recommender import Recommender\nfrom tool import qmath\nfrom structure.symmetricMatrix import SymmetricMatrix\n\nclass ItemKNN(Recommender):\n    def __init__(self,conf,trainingSet=None,testSet=None,fold=\'[1]\'):\n        super(ItemKNN, self).__init__(conf,trainingSet,testSet,fold)\n        self.itemSim = SymmetricMatrix(len(self.data.user)) #used to store the similarity among items\n\n    def readConfiguration(self):\n        super(ItemKNN, self).readConfiguration()\n        self.sim = self.config[\'similarity\']\n        self.shrinkage =int(self.config[\'num.shrinkage\'])\n        self.neighbors = int(self.config[\'num.neighbors\'])\n\n    def printAlgorConfig(self):\n        ""show algorithm\'s configuration""\n        super(ItemKNN, self).printAlgorConfig()\n        print \'Specified Arguments of\',self.config[\'recommender\']+\':\'\n        print \'num.neighbors:\',self.config[\'num.neighbors\']\n        print \'num.shrinkage:\', self.config[\'num.shrinkage\']\n        print \'similarity:\', self.config[\'similarity\']\n        print \'=\'*80\n\n    def initModel(self):\n        self.topItems = {}\n        self.computeCorr()\n\n    def predict(self,u,i):\n        #find the closest neighbors of item i\n        topItems = self.topItems[i]\n        itemCount = self.neighbors\n        if itemCount > len(topItems):\n            itemCount = len(topItems)\n        #predict\n        sum = 0\n        denom = 0\n        for n in range(itemCount):\n            similarItem = topItems[n][0]\n            #if user n has rating on item i\n            if self.data.contains(u,similarItem):\n                similarity = topItems[n][1]\n                rating = self.data.rating(u,similarItem)\n                sum += similarity*(rating-self.data.itemMeans[similarItem])\n                denom += similarity\n        if sum == 0:\n            #no items have rating on item i,return the average rating of item i\n            if not self.data.containsItem(i):\n                # item i has no ratings in the training set\n                return self.data.globalMean\n            return self.data.itemMeans[i]\n        pred = self.data.itemMeans[i]+sum/float(denom)\n        return pred\n\n    def computeCorr(self):\n        \'compute correlation among items\'\n        print \'Computing item similarities...\'\n        for idx,i1 in enumerate(self.data.testSet_i):\n\n            for i2 in self.data.item:\n                if i1 <> i2:\n                    if self.itemSim.contains(i1,i2):\n                        continue\n                    sim = qmath.similarity(self.data.sCol(i1),self.data.sCol(i2),self.sim)\n                    self.itemSim.set(i1,i2,sim)\n            self.topItems[i1] = sorted(self.itemSim[i1].iteritems(),key = lambda d:d[1],reverse=True)\n            if idx%100==0:\n                print \'progress:\',idx,\'/\',len(self.data.testSet_i)\n        print \'The item similarities have been calculated.\'\n\n\n    def predictForRanking(self,u):\n        print \'Using Memory based algorithms to rank items is extremely time-consuming. So ranking for all items in ItemKNN is not available.\'\n        exit(0)\n'"
algorithm/rating/ItemMean.py,0,"b""from baseclass.Recommender import Recommender\n\nclass ItemMean(Recommender):\n    def __init__(self,conf,trainingSet=None,testSet=None,fold='[1]'):\n        super(ItemMean, self).__init__(conf,trainingSet,testSet,fold)\n\n    def predict(self,u,i):\n        if self.data.containsItem(i):\n            return self.data.itemMeans[i]\n        else:\n            return self.data.globalMean"""
algorithm/rating/LOCABAL.py,0,"b""from baseclass.SocialRecommender import SocialRecommender\nfrom tool import config\nimport numpy as np\nimport networkx as nx\nimport math\nfrom tool import qmath\n\nclass LOCABAL(SocialRecommender):\n    def __init__(self,conf,trainingSet=None,testSet=None,relation=list(),fold='[1]'):\n        super(LOCABAL, self).__init__(conf,trainingSet,testSet,relation,fold)\n\n    def readConfiguration(self):\n        super(LOCABAL, self).readConfiguration()\n        alpha = config.LineConfig(self.config['LOCABAL'])\n        self.alpha = float(alpha['-alpha'])\n\n    def printAlgorConfig(self):\n        super(LOCABAL, self).printAlgorConfig()\n        print 'Specified Arguments of',self.config['recommender']+':'\n        print 'alpha: %.3f' %self.alpha\n        print '='*80\n\n    def initModel(self):\n        super(LOCABAL, self).initModel()\n        self.H = np.random.rand(self.embed_size,self.embed_size)\n        G = nx.DiGraph()\n        for re in self.social.relation:\n            G.add_edge(re[0], re[1])\n        pr = nx.pagerank(G, alpha=0.85)\n        pr = sorted(pr.iteritems(),key=lambda d:d[1],reverse=True)\n        pr = [(u[0],ind+1) for ind,u in enumerate(pr)]\n        self.W = {}\n        for user in pr:\n            self.W[user[0]] = 1/(1+math.log(user[1]))\n        self.S = {}\n        for line in self.social.relation:\n            u1,u2,weight = line\n            if self.data.containsUser(u1) and self.data.containsUser(u2):\n                uvec1=self.data.trainSet_u[u1]\n                uvec2=self.data.trainSet_u[u2]\n            #add relations to dict\n                if not self.S.has_key(u1):\n                    self.S[u1] = {}\n                self.S[u1][u2] = qmath.cosine_sp(uvec1,uvec2)\n\n    def buildModel(self):\n        iteration = 0\n        while iteration < self.maxIter:\n            self.loss = 0\n            for entry in self.data.trainingData:\n                user, item, r = entry\n                error = r - self.predict(user,item)\n                i = self.data.getItemId(item)\n                u = self.data.getUserId(user)\n                if self.W.has_key(user):\n                    self.loss += self.W[user]*error ** 2\n                else:\n                    self.loss += error ** 2\n                p = self.P[u].copy()\n                q = self.Q[i].copy()\n                #update latent vectors\n                if self.W.has_key(user):\n                    self.P[u] += self.lRate * (self.W[user]*error * q - self.regU * p)\n                    self.Q[i] += self.lRate * (self.W[user]*error * p - self.regI * q)\n                #else:\n                self.P[u] += self.lRate * (error * q - self.regU * p)\n                self.Q[i] += self.lRate * (error * p - self.regI * q)\n            for user in self.S:\n                for friend in self.S[user]:\n                    k = self.data.getUserId(friend)\n                    u = self.data.getUserId(user)\n                    p = self.P[u].copy()\n                    q = self.P[k].copy()\n                    error = self.S[user][friend] - np.dot(np.dot(p,self.H),q)\n                    self.loss+=self.alpha*error**2\n                    #update latent vectors\n                    self.H+=self.lRate*self.alpha*error*(p.reshape(self.embed_size,1).dot(q.reshape(1,self.embed_size)))\n                    self.H-=self.lRate*self.regS*self.H\n                    self.P[u]+=self.lRate*self.alpha*error*(self.H.dot(q))\n                    self.P[k]+=self.lRate*self.alpha*error*(p.T.dot(self.H))\n\n\n\n            self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()+self.regS*(self.H*self.H).sum()\n            iteration += 1\n            self.isConverged(iteration)"""
algorithm/rating/PMF.py,6,"b""#coding:utf-8\nfrom baseclass.IterativeRecommender import IterativeRecommender\nimport numpy as np\n\nclass PMF(IterativeRecommender):\n    def __init__(self,conf,trainingSet=None,testSet=None,fold='[1]'):\n        super(PMF, self).__init__(conf,trainingSet,testSet,fold)\n\n    def buildModel(self):\n        iteration = 0\n        while iteration < self.maxIter:\n            self.loss = 0\n            for entry in self.data.trainingData:\n                user, item, rating = entry\n                u = self.data.user[user] #get user id\n                i = self.data.item[item] #get item id\n                error = rating - self.P[u].dot(self.Q[i])\n                self.loss += error**2\n                p = self.P[u]\n                q = self.Q[i]\n\n                #update latent vectors\n                self.P[u] += self.lRate*(error*q-self.regU*p)\n                self.Q[i] += self.lRate*(error*p-self.regI*q)\n\n            self.loss += self.regU*(self.P*self.P).sum() + self.regI*(self.Q*self.Q).sum()\n            iteration += 1\n            if self.isConverged(iteration):\n                break\n\n    def buildModel_tf(self):\n        super(PMF, self).buildModel_tf()\n\n        import tensorflow as tf\n        # \xe6\x9e\x84\xe9\x80\xa0\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0 \xe8\xae\xbe\xe7\xbd\xae\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\n\n        self.r_hat = tf.reduce_sum(tf.multiply(self.user_embedding, self.item_embedding), axis=1)\n        self.loss = tf.nn.l2_loss(tf.subtract(self.r, self.r_hat))\n        reg_loss = self.regU*tf.nn.l2_loss(self.user_embedding) +  self.regI*tf.nn.l2_loss(self.item_embedding)\n        self.total_loss = self.loss+reg_loss\n        self.optimizer = tf.train.AdamOptimizer(self.lRate)\n        self.train = self.optimizer.minimize(self.total_loss, var_list=[self.U, self.V])\n\n        # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xbc\x9a\xe8\xaf\x9d\n        with tf.Session() as sess:\n            init = tf.global_variables_initializer()\n            sess.run(init)\n\n            # \xe8\xbf\xad\xe4\xbb\xa3\xef\xbc\x8c\xe4\xbc\xa0\xe9\x80\x92\xe5\x8f\x98\xe9\x87\x8f\n            for step in range(self.maxIter):\n                # \xe6\x8c\x89\xe6\x89\xb9\xe4\xbc\x98\xe5\x8c\x96\n                batch_size = self.batch_size\n\n                batch_idx = np.random.randint(self.train_size, size=batch_size)\n\n                user_idx = [self.data.user[self.data.trainingData[idx][0]] for idx in batch_idx]\n                item_idx = [self.data.item[self.data.trainingData[idx][1]] for idx in batch_idx]\n                rating = [self.data.trainingData[idx][2] for idx in batch_idx]\n                sess.run(self.train, feed_dict={self.r: rating, self.u_idx: user_idx, self.v_idx: item_idx})\n                print 'iteration:', step, 'loss:', sess.run(self.total_loss, feed_dict={self.r: rating, self.u_idx: user_idx, self.v_idx: item_idx})\n\n            # \xe8\xbe\x93\xe5\x87\xba\xe8\xae\xad\xe7\xbb\x83\xe5\xae\x8c\xe6\xaf\x95\xe7\x9a\x84\xe7\x9f\xa9\xe9\x98\xb5\n            self.P = sess.run(self.U)\n            self.Q = sess.run(self.V)"""
algorithm/rating/RSTE.py,0,"b""from baseclass.SocialRecommender import SocialRecommender\nfrom tool import config\nimport numpy as np\nclass RSTE(SocialRecommender):\n    def __init__(self,conf,trainingSet=None,testSet=None,relation=list(),fold='[1]'):\n        super(RSTE, self).__init__(conf,trainingSet,testSet,relation,fold)\n\n    def readConfiguration(self):\n        super(RSTE, self).readConfiguration()\n        alpha = config.LineConfig(self.config['RSTE'])\n        self.alpha = float(alpha['-alpha'])\n\n    def printAlgorConfig(self):\n        super(RSTE, self).printAlgorConfig()\n        print 'Specified Arguments of',self.config['recommender']+':'\n        print 'alpha: %.3f' %self.alpha\n        print '='*80\n\n    def initModel(self):\n        super(RSTE, self).initModel()\n\n    def buildModel(self):\n        iteration = 0\n        while iteration < self.maxIter:\n            self.loss = 0\n            for entry in self.data.trainingData:\n                user, item, rating = entry\n                error = rating - self.predict(user,item)\n                i = self.data.item[item]\n                u = self.data.user[user]\n                self.loss += error ** 2\n                p = self.P[u]\n                q = self.Q[i]\n\n                # update latent vectors\n                self.P[u] += self.lRate * (self.alpha*error * q - self.regU * p)\n                self.Q[i] += self.lRate * (self.alpha*error * p - self.regI * q)\n            self.loss+= self.regU*(self.P*self.P).sum() + self.regI*(self.Q*self.Q).sum()\n            iteration += 1\n            self.isConverged(iteration)\n\n\n    def predict(self,u,i):\n        if self.data.containsUser(u) and self.data.containsItem(i):   \n            i = self.data.getItemId(i)\n            fPred = 0\n            denom = 0\n            relations = self.social.getFollowees(u)\n            weights = []\n            indexes = []\n            for followee in relations:\n                if  self.data.containsUser(followee):  # followee is in rating set\n                    indexes.append(self.data.user[followee])\n                    weights.append(relations[followee])\n            weights = np.array(weights)\n            indexes = np.array(indexes)\n            denom = weights.sum()\n            u = self.data.user[u]\n            if denom <> 0:\n                fPred += weights.dot((self.P[indexes].dot(self.Q[i])))\n\n                return self.alpha * self.P[u].dot(self.Q[i])+(1-self.alpha)*fPred / denom\n            else:\n                return self.P[u].dot(self.Q[i])\n\n        else:\n            return self.data.globalMean\n\n    def predictForRanking(self,u):\n        if self.data.containsUser(u):\n            fPred = 0\n            denom = 0\n            relations = self.social.getFollowees(u)\n            for followee in relations:\n                weight = relations[followee]\n                if self.data.containsUser(followee):  # followee is in rating set\n                    uf = self.data.user[followee]\n                    fPred += weight * self.Q.dot(self.P[uf])\n                    denom += weight\n            u = self.data.user[u]\n            if denom <> 0:\n                return self.alpha * self.Q.dot(self.P[u]) + (1 - self.alpha) * fPred / denom\n            else:\n                return self.Q.dot(self.P[u])\n        else:\n            return [self.data.globalMean] * len(self.data.item)"""
algorithm/rating/SREE.py,0,"b""from baseclass.SocialRecommender import SocialRecommender\nimport numpy as np\nfrom tool import config\n\n###################################\n#NOTE: WE CONSIDER THAT THE SOCIAL TERM IN THE RATING PREDICTION EQUATION SHOULD\n#BE MOVED OUT. THE LOSS FUNCTION SHOULD BE (RATING ERROR)^2 + SOCIAL TERM + PENALTY TERMS\n#THEREFORE, THE IMPLEMENTATION IS DIFFERENT FROM THE ORIGINAL PAPER.\n###################################\n\nclass SREE(SocialRecommender):\n    def __init__(self, conf, trainingSet=None, testSet=None, relation=list(),fold='[1]'):\n        super(SREE, self).__init__(conf, trainingSet, testSet, relation,fold)\n\n    def readConfiguration(self):\n        super(SREE, self).readConfiguration()\n        par = config.LineConfig(self.config['SREE'])\n        self.alpha = float(par['-alpha'])\n\n    def initModel(self):\n        super(SREE, self).initModel()\n        self.Bu = np.random.rand(self.data.trainingSize()[0])/10  # bias value of user\n        self.Bi = np.random.rand(self.data.trainingSize()[1])/10  # bias value of item\n        # self.X = np.random.rand(self.data.trainingSize()[0], self.Dim)/10\n        # self.Y = np.random.rand(self.data.trainingSize()[1], self.Dim)/10\n\n    def buildModel(self):\n        iteration = 0\n        while iteration < self.maxIter:\n            self.loss = 0\n            for entry in self.data.trainingData:\n                user, item, rating = entry\n                error = rating - self.predict(user,item)\n                u = self.data.user[user]\n                i = self.data.item[item]\n                self.loss += error ** 2\n                self.loss += self.regU * (self.P[u] - self.Q[i]).dot(self.P[u] - self.Q[i])\n                bu = self.Bu[u]\n                bi = self.Bi[i]\n                #self.loss += self.regB * bu ** 2 + self.regB * bi ** 2\n                # update latent vectors\n                self.P[u] -= self.lRate * (error + self.regU) * (self.P[u] - self.Q[i])\n                self.Q[i] += self.lRate * (error + self.regI) * (self.P[u] - self.Q[i])\n                self.Bu[u] += self.lRate * (error - self.regB * bu)\n                self.Bi[i] += self.lRate * (error - self.regB * bi)\n            self.loss+=self.regB*(self.Bu*self.Bu).sum()+self.regB*(self.Bi*self.Bi).sum()\n\n            for user in self.social.user:\n                if self.data.containsUser(user):\n                    u = self.data.user[user]\n                    followees = self.social.getFollowees(user)\n                    for friend in followees:\n                        if self.data.containsUser(friend):\n                            v = self.data.user[friend]\n                            weight = followees[friend]\n                            p = self.P[u]\n                            z = self.P[v]\n                            # update latent vectors\n                            self.P[u] -= self.lRate * self.alpha*weight*(p-z)\n                            self.loss += self.alpha*weight*(p-z).dot(p-z)\n\n            iteration += 1\n            self.isConverged(iteration)\n\n    def predict(self, u, i):\n        if self.data.containsUser(u) and self.data.containsItem(i):\n            u = self.data.user[u]\n            i = self.data.item[i]\n            return self.data.globalMean + self.Bi[i] + self.Bu[u] - (self.P[u] - self.Q[i]).dot(self.P[u] - self.Q[i])\n        else:\n            return self.data.globalMean\n\n    def predictForRanking(self,u):\n        'invoked to rank all the items for the user'\n        if self.data.containsUser(u):\n            u = self.data.user[u]\n            res = ((self.Q-self.P[u])*(self.Q-self.P[u])).sum(axis=1)+self.Bi+self.Bu[u]+self.data.globalMean\n            return res\n        else:\n            return [self.data.globalMean]*len(self.data.item)\n\n"""
algorithm/rating/SVD.py,13,"b'from baseclass.IterativeRecommender import IterativeRecommender\nimport numpy as np\n\nclass SVD(IterativeRecommender):\n    def __init__(self,conf,trainingSet=None,testSet=None,fold=\'[1]\'):\n        super(SVD, self).__init__(conf,trainingSet,testSet,fold)\n\n    def initModel(self):\n        super(SVD, self).initModel()\n        self.Bu = np.random.rand(self.data.trainingSize()[0])/5  # bias value of user\n        self.Bi = np.random.rand(self.data.trainingSize()[1])/5  # bias value of item\n\n    def buildModel(self):\n        iteration = 0\n        while iteration < self.maxIter:\n            self.loss = 0\n            for entry in self.data.trainingData:\n                user, item, rating = entry\n                u = self.data.user[user]\n                i = self.data.item[item]\n                error = rating-self.predict(user,item)\n                self.loss+=error**2\n                p = self.P[u]\n                q = self.Q[i]\n\n                bu = self.Bu[u]\n                bi = self.Bi[i]\n\n                #update latent vectors\n                self.P[u] += self.lRate*(error*q-self.regU*p)\n                self.Q[i] += self.lRate*(error*p-self.regI*q)\n                self.Bu[u] += self.lRate*(error-self.regB*bu)\n                self.Bi[i] += self.lRate*(error-self.regB*bi)\n            self.loss += self.regU*(self.P*self.P).sum() + self.regI*(self.Q*self.Q).sum()\\\n               +self.regB*((self.Bu*self.Bu).sum()+(self.Bi*self.Bi).sum())\n            iteration += 1\n            self.isConverged(iteration)\n\n\n    def buildModel_tf(self):\n        super(SVD, self).buildModel_tf()\n\n        import tensorflow as tf\n\n        global_mean = tf.placeholder(tf.float32, [None], name=""mean"")\n        reg_lambda = tf.constant(self.regU, dtype=tf.float32)\n        reg_biase = tf.constant(self.regB, dtype=tf.float32)\n\n\n\n        self.U_bias = tf.Variable(tf.truncated_normal(shape=[self.num_users], stddev=0.005,mean=0.02), name=\'U_bias\')\n        self.V_bias = tf.Variable(tf.truncated_normal(shape=[self.num_items], stddev=0.005,mean=0.02), name=\'V_bias\')\n\n        self.U_bias_embed = tf.nn.embedding_lookup(self.U_bias, self.u_idx)\n        self.V_bias_embed = tf.nn.embedding_lookup(self.V_bias, self.v_idx)\n\n        self.r_hat = tf.reduce_sum(tf.multiply(self.user_embedding, self.item_embedding), axis=1)\n        self.r_hat = self.r_hat + self.U_bias_embed\n        self.r_hat = self.r_hat + self.V_bias_embed\n        self.r_hat = self.r_hat + global_mean\n\n        self.loss = tf.nn.l2_loss(self.r-self.r_hat)\n        reg_loss = self.regU * tf.nn.l2_loss(self.user_embedding) + self.regI * tf.nn.l2_loss(self.item_embedding)\n        reg_loss += self.regB*self.U_bias_embed+ self.regB*self.U_bias_embed\n        self.total_loss = self.loss + reg_loss\n        optimizer = tf.train.AdamOptimizer(self.lRate)\n        train_U = optimizer.minimize(self.total_loss, var_list=[self.U, self.U_bias])\n        train_V = optimizer.minimize(self.total_loss, var_list=[self.V, self.V_bias])\n\n        with tf.Session() as sess:\n            init = tf.global_variables_initializer()\n            sess.run(init)\n\n\n            for step in range(self.maxIter):\n\n                batch_size = self.batch_size\n\n                batch_idx = np.random.randint(self.train_size, size=batch_size)\n\n                user_idx = [self.data.user[self.data.trainingData[idx][0]] for idx in batch_idx]\n                item_idx = [self.data.item[self.data.trainingData[idx][1]] for idx in batch_idx]\n                g_mean = [self.data.globalMean]*batch_size\n                rating = [self.data.trainingData[idx][2] for idx in batch_idx]\n\n                sess.run(train_U, feed_dict={self.r: rating, self.u_idx: user_idx, self.v_idx: item_idx,global_mean:g_mean})\n                sess.run(train_V, feed_dict={self.r: rating, self.u_idx: user_idx, self.v_idx: item_idx, global_mean: g_mean})\n\n                print \'iteration:\', step, \'loss:\', sess.run(self.total_loss,\n                                                            feed_dict={self.r: rating, self.u_idx: user_idx, self.v_idx: item_idx,global_mean:g_mean})\n\n\n            self.P = sess.run(self.U)\n            self.Q = sess.run(self.V)\n            self.Bu = sess.run(self.U_bias)\n            self.Bi = sess.run(self.V_bias)\n\n    def predict(self,u,i):\n        if self.data.containsUser(u) and self.data.containsItem(i):\n            u = self.data.user[u]\n            i = self.data.item[i]\n            return self.P[u].dot(self.Q[i])+self.data.globalMean+self.Bi[i]+self.Bu[u]\n        else:\n            return self.data.globalMean\n\n    def predictForRanking(self,u):\n        \'invoked to rank all the items for the user\'\n        if self.data.containsUser(u):\n            u = self.data.getUserId(u)\n            return self.Q.dot(self.P[u])+self.data.globalMean + self.Bi + self.Bu[u]\n        else:\n            return [self.data.globalMean] * self.num_items\n\n'"
algorithm/rating/SVDPlusPlus.py,0,"b""from baseclass.IterativeRecommender import IterativeRecommender\nimport numpy as np\nfrom tool import config\nimport math\nclass SVDPlusPlus(IterativeRecommender):\n    def __init__(self,conf,trainingSet=None,testSet=None,fold='[1]'):\n        super(SVDPlusPlus, self).__init__(conf,trainingSet,testSet,fold)\n\n    def readConfiguration(self):\n        super(SVDPlusPlus, self).readConfiguration()\n        regY = config.LineConfig(self.config['SVDPlusPlus'])\n        self.regY = float( regY['-y'])\n\n\n    def printAlgorConfig(self):\n        super(SVDPlusPlus, self).printAlgorConfig()\n        print 'Specified Arguments of', self.config['recommender'] + ':'\n        print 'regY: %.3f' % self.regY\n        print '=' * 80\n\n    def initModel(self):\n        super(SVDPlusPlus, self).initModel()\n        self.Bu = np.random.rand(self.data.trainingSize()[0])  # biased value of user\n        self.Bi = np.random.rand(self.data.trainingSize()[1])  # biased value of item\n        self.Y = np.random.rand(self.data.trainingSize()[1], self.k)\n\n\n    def buildModel(self):\n        iteration = 0\n        while iteration < self.maxIter:\n            self.loss = 0\n            for entry in self.data.trainingData:\n                user, item, rating = entry\n                items, ratings = self.data.userRated(user)\n                w = len(items)\n                #w = math.sqrt(len(itemIndexs))\n                error = rating - self.predict(user, item)\n                u = self.data.user[user]\n                i = self.data.item[item]\n                self.loss += error ** 2\n                p = self.P[u]\n                q = self.Q[i]\n                bu = self.Bu[u]\n                bi = self.Bi[i]\n\n                #update latent vectors\n                self.Bu[u] += self.lRate*(error-self.regB*bu)\n                self.Bi[i] += self.lRate*(error-self.regB*bi)\n                sum = 0\n                if w> 1:\n                    indexes = []\n                    for j in items:\n                        j = self.data.item[j]\n                        if i!=j:\n                            indexes.append(j)\n\n                    y = self.Y[indexes]\n                    sum += y.sum(axis=0)\n                    self.Y[indexes] += self.lRate * (error * q / (w-1) - self.regY * y)\n                    self.Q[i] += self.lRate * error * sum/(w-1)\n\n                self.P[u] += self.lRate * (error * q - self.regU * p)\n                self.Q[i] += self.lRate * (error * p - self.regI * q)\n\n\n            self.loss+=self.regU*(self.P*self.P).sum() + self.regI*(self.Q*self.Q).sum() \\\n               + self.regY*(self.Y*self.Y).sum() + self.regB*((self.Bu*self.Bu).sum()+(self.Bi*self.Bi).sum())\n            iteration += 1\n            self.isConverged(iteration)\n\n\n    def predict(self,u,i):\n        pred = 0\n        if self.data.containsUser(u) and self.data.containsItem(i):\n            itemIndexs,rating = self.data.userRated(u)\n            w = len(itemIndexs)\n            # w = math.sqrt(len(itemIndexs))\n            u = self.data.user[u]\n            i = self.data.item[i]\n            sum = 0\n            if w> 0:\n                for j in itemIndexs:\n                    j = self.data.item[j]\n                    sum += self.Y[j]\n                pred+= (sum/w).dot(self.Q[i])\n            pred += self.P[u].dot(self.Q[i]) + self.data.globalMean + self.Bi[i] + self.Bu[u]\n\n        else:\n            pred = self.data.globalMean\n        return pred\n\n    def predictForRanking(self,u):\n        pred = 0\n        if self.data.containsUser(u):\n            itemIndexs, rating = self.data.userRated(u)\n            w = len(itemIndexs)\n            # w = math.sqrt(len(itemIndexs))\n            u = self.data.user[u]\n            sum = 0\n            if w > 0:\n                for j in itemIndexs:\n                    j = self.data.item[j]\n                    sum += self.Y[j]\n                pred += self.Q.dot(sum / w)\n            pred += self.Q.dot(self.P[u]) + self.data.globalMean + self.Bi + self.Bu[u]\n\n        else:\n            pred = [self.data.globalMean] * len(self.data.item)\n        return pred\n"""
algorithm/rating/SlopeOne.py,0,"b'from baseclass.Recommender import Recommender\nfrom tool import qmath\nfrom structure.symmetricMatrix import SymmetricMatrix\n\nclass SlopeOne(Recommender,):\n    def __init__(self,conf,trainingSet=None,testSet=None,fold=\'[1]\'):\n        super(SlopeOne, self).__init__(conf,trainingSet,testSet,fold)\n        self.diffAverage = {}\n        self.freq = {}\n\n    def initModel(self):\n        self.computeAverage()\n\n    def computeAverage(self):\n        for item in self.data.testSet_i:\n            freq_sub = {}\n            diffAverage_sub = {}\n            for item2 in self.data.item:\n                x1 = self.data.sCol(item)\n                x2 = self.data.sCol(item2)\n                diff = 0.0\n                commonItem = 0\n                for key in x1:\n                    if x2.has_key(key):\n                        diff+=x1[key]-x2[key]\n                        commonItem+=1\n                if commonItem==0:\n                    diffAverage_sub.setdefault(item2, 0)\n                else:\n                    diffAverage_sub.setdefault(item2,diff/commonItem)\n                freq_sub.setdefault(item2,commonItem)\n            print \'item \'+ item +"" finished.""\n            self.diffAverage[item] = diffAverage_sub\n            self.freq[item] = freq_sub\n\n\n    def predict(self,u,i):\n        pred = 0\n        # check if the user existed in trainSet or not\n        if self.data.containsUser(u):\n            sum = 0\n            freqSum = 0\n            itemRated,ratings = self.data.userRated(u)\n            for item,rating in zip(itemRated,ratings):\n                diff = self.diffAverage[i][item]\n                count = self.freq[i][item]\n                sum += (rating + diff) * count\n                freqSum += count\n            try:\n                pred = float(sum)/freqSum\n            except ZeroDivisionError:\n                pred = self.data.userMeans[u]\n        elif self.data.containsItem(i):\n            pred = self.data.itemMeans[i]\n        else:\n            pred = self.data.globalMean\n\n        return pred\n\n'"
algorithm/rating/SoRec.py,0,"b""from baseclass.SocialRecommender import SocialRecommender\nimport math\nimport numpy as np\nfrom tool import config\n#Social Recommendation Using Probabilistic Matrix Factorization\nclass SoRec(SocialRecommender ):\n    def __init__(self,conf,trainingSet=None,testSet=None,relation=list(),fold='[1]'):\n        super(SoRec, self).__init__(conf,trainingSet,testSet,relation,fold)\n\n\n    def readConfiguration(self):\n        super(SoRec, self).readConfiguration()\n        regZ = config.LineConfig(self.config['SoRec'])\n        self.regZ = float( regZ['-z'])\n\n    def initModel(self):\n        super(SoRec, self).initModel()\n        self.Z = np.random.rand(self.data.trainingSize()[0], self.embed_size)/10\n\n    def printAlgorConfig(self):\n        super(SoRec, self).printAlgorConfig()\n        print 'Specified Arguments of', self.config['recommender'] + ':'\n        print 'regZ: %.3f' % self.regZ\n        print '=' * 80\n\n\n    def buildModel(self):\n        iteration = 0\n        while iteration < self.maxIter:\n            self.loss = 0\n            #ratings\n            for entry in self.data.trainingData:\n                user, item, rating = entry\n                error = rating - self.predict(user, item)\n                i = self.data.item[item]\n                u = self.data.user[user]\n                self.loss += error ** 2\n                p = self.P[u]\n                q = self.Q[i]\n\n                # update latent vectors\n                self.P[u] += self.lRate * (error * q - self.regU * p)\n                self.Q[i] += self.lRate * (error * p - self.regI * q)\n\n            #relations\n            for entry in self.social.relation:\n                u, v, tuv = entry\n                if self.data.containsUser(u) and self.data.containsUser(v):\n                    vminus = len(self.social.getFollowers(v))# ~ d - (k)\n                    uplus = len(self.social.getFollowees(u))#~ d + (i)\n                    try:\n                        weight = math.sqrt(vminus / (uplus + vminus + 0.0))\n                    except ZeroDivisionError:\n                        weight = 1\n                    v = self.data.user[v]\n                    u = self.data.user[u]\n                    euv = weight * tuv - self.P[u].dot(self.Z[v])  # weight * tuv~ cik *\n                    self.loss += self.regS * (euv ** 2)\n                    p = self.P[u]\n                    z = self.Z[v]\n\n                    # update latent vectors\n                    self.P[u] += self.lRate * (self.regS * euv * z)\n                    self.Z[v] += self.lRate * (self.regS * euv * p - self.regZ * z)\n\n            self.loss += self.regU*(self.P*self.P).sum() + self.regI*(self.Q*self.Q).sum() + self.regZ*(self.Z*self.Z).sum()\n            iteration += 1\n            if self.isConverged(iteration):\n                break\n\n"""
algorithm/rating/SoReg.py,0,"b""from baseclass.SocialRecommender import SocialRecommender\nfrom tool import config\nfrom tool import qmath\n\n\nclass SoReg(SocialRecommender):\n    def __init__(self,conf,trainingSet=None,testSet=None,relation=list(),fold='[1]'):\n        super(SoReg, self).__init__(conf,trainingSet,testSet,relation,fold)\n\n    def readConfiguration(self):\n        super(SoReg, self).readConfiguration()\n        alpha = config.LineConfig(self.config['SoReg'])\n        self.alpha = float(alpha['-alpha'])\n\n    def printAlgorConfig(self):\n        super(SoReg, self).printAlgorConfig()\n        print 'Specified Arguments of',self.config['recommender']+':'\n        print 'alpha: %.3f' %self.alpha\n        print '='*80\n\n    def initModel(self):\n        super(SoReg, self).initModel()\n        # compute similarity\n        from collections import defaultdict\n        self.Sim = defaultdict(dict)\n        print 'constructing similarity matrix...'\n        for user in self.data.user:\n            for f in self.social.getFollowees(user):\n                if self.Sim.has_key(user) and self.Sim[user].has_key(f):\n                    pass\n                else:\n                    self.Sim[user][f]=self.sim(user,f)\n                    self.Sim[f][user]=self.Sim[user][f]\n\n\n    def sim(self,u,v):\n        return (qmath.pearson_sp(self.data.sRow(u), self.data.sRow(v))+self.social.weight(u,v))/2.0\n\n    def buildModel(self):\n        iteration = 0\n        while iteration < self.maxIter:\n            self.loss = 0\n            for entry in self.data.trainingData:\n                user, item, rating = entry\n                uid = self.data.user[user]\n                vid = self.data.item[item]\n\n                # add the followees' influence\n\n                error = rating - self.P[uid].dot(self.Q[vid])\n                p = self.P[uid]\n                q = self.Q[vid]\n\n                self.loss += error**2\n\n                #update latent vectors\n                self.P[uid] += self.lRate*(error*q - self.regU * p)\n                self.Q[vid] += self.lRate*(error*p - self.regI * q)\n\n            for user in self.social.user:\n                simSum = 0\n                simSumf1 = 0\n                if not self.data.containsUser(user):\n                    continue\n                uid = self.data.user[user]\n                for f in self.social.getFollowees(user):\n                    if self.data.containsUser(f):\n                        fid = self.data.user[f]\n                        simSumf1 += self.Sim[user][f] * (self.P[uid] - self.P[fid])\n                        simSum += self.Sim[user][f] * ((self.P[uid] - self.P[fid]).dot(self.P[uid] - self.P[fid]))\n                        self.loss += simSum\n\n                simSumf2 = 0\n                for g in self.social.getFollowers(user):\n                    if self.data.containsUser(g):\n                        gid = self.data.user[g]\n                        simSumf2 += self.Sim[user][g] * (self.P[uid]-self.P[gid])\n\n                self.P[uid] += self.lRate * (- self.alpha * (simSumf1+simSumf2))\n\n            self.loss += self.regU*(self.P*self.P).sum() + self.regI*(self.Q*self.Q).sum()\n            iteration += 1\n            if self.isConverged(iteration):\n                break\n"""
algorithm/rating/SocialFD.py,0,"b""from baseclass.SocialRecommender import SocialRecommender\nfrom tool import config\nimport numpy as np\n\n\n\nclass SocialFD(SocialRecommender):\n    def __init__(self,conf,trainingSet=None,testSet=None,relation=list(),fold='[1]'):\n        super(SocialFD, self).__init__(conf,trainingSet,testSet,relation,fold)\n\n    def initModel(self):\n        super(SocialFD, self).initModel()\n        self.Bu = np.random.rand(self.data.trainingSize()[0])/5 # biased value of user\n        self.Bi = np.random.rand(self.data.trainingSize()[1])/5  # biased value of item\n        self.H = np.random.rand(self.embed_size, self.embed_size)/5\n        self.P /= 10\n        self.Q /= 10\n\n\n    def readConfiguration(self):\n        super(SocialFD, self).readConfiguration()\n        eps = config.LineConfig(self.config['SocialFD'])\n        self.alpha = float(eps['-alpha'])\n        self.eta = float(eps['-eta'])\n        self.beta = float(eps['-beta'])\n\n\n    def buildModel(self):\n        iteration = 0\n        while iteration < self.maxIter:\n            self.loss = 0\n            for entry in self.data.trainingData:\n                u, i, r = entry\n                error = r - self.predict(u, i)\n                u = self.data.getUserId(u)\n                i = self.data.getItemId(i)\n                self.loss += error ** 2\n\n                bu = self.Bu[u]\n                bi = self.Bi[i]\n                x = self.P[u]\n                y = self.Q[i]\n                d = (x - y).dot(self.H).dot(self.H.T).dot((x - y).T)\n                derivative_d = self.H.dot((x - y).T.dot(x - y))\n                if r > 0.7: #high ratings, ratings are compressed to range (0.01,1.01)\n                    self.loss += self.eta*self.alpha * d\n\n                    # update latent vectors\n                    self.H -= self.lRate * ((error + self.eta*self.alpha) * derivative_d)\n                    W = (self.H.dot(self.H.T) + self.H.dot(self.H.T).T)\n                    self.P[u] -= self.lRate * ((error + self.eta*self.alpha) * (W.dot(np.array([x - y]).T)).T[0])\n                    self.Q[i] += self.lRate * ((error + self.eta*self.alpha) * (W.dot(np.array([x - y]).T)).T[0])\n\n\n                elif r <= 0.5: #low ratings\n\n                    self.loss += self.eta*self.alpha * abs(1 - min(d, 1))\n                    # update latent vectors\n                    if d < 1:\n                        self.H += self.lRate * ((self.eta*self.alpha - error) * derivative_d)\n                    else:\n                        self.H -= self.lRate * error * derivative_d\n                    W = (self.H.dot(self.H.T) + self.H.dot(self.H.T).T)\n                    if d < 1:\n                        self.P[u] += self.lRate * ((-error + self.eta*self.alpha) * (W.dot(np.array([x - y]).T)).T[0])\n                        self.Q[i] += self.lRate * ((error - self.eta*self.alpha) * (W.dot(np.array([x - y]).T)).T[0])\n                    else:\n                        self.P[u] += self.lRate * (-error * (W.dot(np.array([x - y]).T)).T[0])\n                        self.Q[i] += self.lRate * (error * (W.dot(np.array([x - y]).T)).T[0])\n\n\n                else: #medium\n                    # update latent vectors\n                    self.H -= self.lRate * ((error) * derivative_d)\n                    W = (self.H.dot(self.H.T) + self.H.dot(self.H.T).T)\n                    self.P[u] += self.lRate * ((-error) * (W.dot(np.array([x - y]).T)).T[0] - self.regU * x)\n                    self.Q[i] += self.lRate * ((error) * (W.dot(np.array([x - y]).T)).T[0] - self.regI * y)\n\n                self.Bu[u] += self.lRate * (error - self.regU * bu)\n                self.Bi[i] += self.lRate * (error - self.regI * bi)\n                self.P[u] += self.lRate*(error*x-self.regU*x)\n                self.Q[i] += self.lRate*(error*y-self.regI*y)\n\n            for user in self.data.user:\n                relations = self.social.getFollowees(user)\n                u = self.data.user[user]\n                x = self.P[u]\n                for followee in relations:\n                    uf = self.data.getUserId(followee)\n                    if uf <> -1 and self.data.containsUser(followee):  # followee is in rating set\n                        self.loss += (x - self.P[uf]).dot(self.H).dot(self.H.T).dot((x - self.P[uf]).T)\n                        R = (self.H.dot(self.H.T) + self.H.T.dot(self.H))\n                        derivative_s = self.H.dot((x - self.P[uf]).T.dot(x - self.P[uf]))\n                        delta = R.dot(np.array([x - self.P[uf]]).T).T[0]\n                        self.P[u] -= self.lRate * self.eta*self.beta * delta\n                        self.H -= self.lRate * self.eta*self.beta * derivative_s\n            iteration += 1\n            self.loss+=self.regU*self.Bu.dot(self.Bu)+self.regI*self.Bi.dot(self.Bi)\n            if self.isConverged(iteration):\n                break\n\n    def predict(self,u,i):\n        if self.data.containsUser(u) and self.data.containsItem(i):\n            u = self.data.getUserId(u)\n            i = self.data.getItemId(i)\n            x = self.P[u]\n            y = self.Q[i]\n            d = (x-y).dot(self.H).dot(self.H.T).dot((x-y).T)\n            return self.Bi[i] + self.Bu[u] +self.data.globalMean - d\n        else:\n            return self.data.globalMean\n\n\n    def predictForRanking(self, u):\n        if self.data.containsUser(u):\n            u = self.data.user[u]\n            x = self.P[u]\n            res = np.array([0] * self.data.trainingSize()[1], dtype=float)\n            A = self.H.dot(self.H.T)\n            for i, y in enumerate(self.Q):\n                res[i] = self.Bi[i] + self.Bu[u] +self.data.globalMean-((x - y).dot(A).dot((x - y).T))\n            return res\n        else:\n            return np.array([self.data.globalMean] * len(self.data.item))\n\n\n"""
algorithm/rating/SocialMF.py,0,"b""from baseclass.IterativeRecommender import IterativeRecommender\nfrom baseclass.SocialRecommender import SocialRecommender\nimport numpy as np\nfrom tool import config\nclass SocialMF(SocialRecommender ):\n    def __init__(self,conf,trainingSet=None,testSet=None,relation=list(),fold='[1]'):\n        super(SocialMF, self).__init__(conf,trainingSet,testSet,relation,fold)\n\n    def readConfiguration(self):\n        super(SocialMF, self).readConfiguration()\n\n    def buildModel(self):\n        iteration = 0\n        while iteration < self.maxIter:\n            self.loss = 0\n            for entry in self.data.trainingData:\n                user, item, rating = entry\n                u = self.data.user[user]\n                i = self.data.item[item]\n                error = rating - self.P[u].dot(self.Q[i])\n                self.loss += error**2\n                p = self.P[u].copy()\n                q = self.Q[i].copy()\n                self.P[u] += self.lRate * (error * q - self.regU * p)\n                self.Q[i] += self.lRate * (error * p - self.regI * q)\n\n            for user in self.social.user:\n                if self.data.containsUser(user):\n                    fPred = 0\n                    denom = 0\n                    u = self.data.user[user]\n                    relationLoss = np.zeros(self.embed_size)\n                    followees = self.social.getFollowees(user)\n                    for followee in followees:\n                        weight= followees[followee]\n                        if self.data.containsUser(followee):\n                            uf = self.data.user[followee]\n                            fPred += weight * self.P[uf]\n                            denom += weight\n                    if denom <> 0:\n                        relationLoss = self.P[u] - fPred / denom\n\n                    self.loss +=  self.regS *  relationLoss.dot(relationLoss)\n\n                    # update latent vectors\n                    self.P[u] -= self.lRate * self.regS * relationLoss\n\n\n            self.loss+=self.regU*(self.P*self.P).sum() + self.regI*(self.Q*self.Q).sum()\n            iteration += 1\n            if self.isConverged(iteration):\n                break\n"""
algorithm/rating/UserKNN.py,0,"b'from baseclass.Recommender import Recommender\nfrom tool import qmath\nfrom structure.symmetricMatrix import SymmetricMatrix\nimport numpy as np\n\nclass UserKNN(Recommender):\n    def __init__(self,conf,trainingSet=None,testSet=None,fold=\'[1]\'):\n        super(UserKNN, self).__init__(conf,trainingSet,testSet,fold)\n        self.userSim = SymmetricMatrix(len(self.data.user))\n\n    def readConfiguration(self):\n        super(UserKNN, self).readConfiguration()\n        self.sim = self.config[\'similarity\']\n        self.shrinkage =int(self.config[\'num.shrinkage\'])\n        self.neighbors = int(self.config[\'num.neighbors\'])\n\n    def printAlgorConfig(self):\n        ""show algorithm\'s configuration""\n        super(UserKNN, self).printAlgorConfig()\n        print \'Specified Arguments of\',self.config[\'recommender\']+\':\'\n        print \'num.neighbors:\',self.config[\'num.neighbors\']\n        print \'num.shrinkage:\', self.config[\'num.shrinkage\']\n        print \'similarity:\', self.config[\'similarity\']\n        print \'=\'*80\n\n    def initModel(self):\n        self.topUsers = {}\n        self.computeCorr()\n\n    def predict(self,u,i):\n        #find the closest neighbors of user u\n        topUsers = self.topUsers[u]\n        userCount = self.neighbors\n        if userCount > len(topUsers):\n            userCount = len(topUsers)\n        #predict\n        sum,denom = 0,0\n        for n in range(userCount):\n            #if user n has rating on item i\n            similarUser = topUsers[n][0]\n            if self.data.rating(similarUser,i) != -1:\n                similarity = topUsers[n][1]\n                rating = self.data.rating(similarUser,i)\n                sum += similarity*(rating-self.data.userMeans[similarUser])\n                denom += similarity\n        if sum == 0:\n            #no users have rating on item i,return the average rating of user u\n            if not self.data.containsUser(u):\n                #user u has no ratings in the training set,return the global mean\n                return self.data.globalMean\n            return self.data.userMeans[u]\n        pred = self.data.userMeans[u]+sum/float(denom)\n        return pred\n\n\n    def computeCorr(self):\n        \'compute correlation among users\'\n        print \'Computing user similarities...\'\n        for idx,u1 in enumerate(self.data.testSet_u):\n\n            for u2 in self.data.user:\n                if u1 <> u2:\n                    if self.userSim.contains(u1,u2):\n                        continue\n                    sim = qmath.similarity(self.data.sRow(u1),self.data.sRow(u2),self.sim)\n                    self.userSim.set(u1,u2,sim)\n            self.topUsers[u1]=sorted(self.userSim[u1].iteritems(), key=lambda d: d[1], reverse=True)\n            if idx%100==0:\n                print \'progress:\',idx,\'/\',len(self.data.testSet_u)\n\n        print \'The user similarities have been calculated.\'\n\n\n    def predictForRanking(self,u):\n        print \'Using Memory based algorithms to rank items is extremely time-consuming. So ranking for all items in UserKNN is not available.\'\n        exit(0)\n'"
algorithm/rating/UserMean.py,0,"b""from baseclass.Recommender import Recommender\n\nclass UserMean(Recommender):\n    def __init__(self,conf,trainingSet=None,testSet=None,fold='[1]'):\n        super(UserMean, self).__init__(conf,trainingSet,testSet,fold)\n\n    def predict(self,u,i):\n        if self.data.containsUser(u):\n            return self.data.userMeans[u]\n        else:\n            return self.data.globalMean"""
algorithm/rating/__init__.py,0,b''
dataset/FilmTrust/divide.py,0,"b""test = []\ntrain = []\nimport random\nwith open('ratings.txt') as f:\n    for line in f:\n        #items= line.strip().split()\n        if random.random()>0.05:\n            train.append(line)\n        else:\n            test.append(line)\n\nwith open('testset.txt','w') as f:\n    f.writelines(test)\nwith open('trainset.txt','w') as f:\n    f.writelines(train)"""
