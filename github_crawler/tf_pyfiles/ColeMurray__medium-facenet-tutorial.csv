file_path,api_count,code
medium_facenet_tutorial/__init__.py,0,b''
medium_facenet_tutorial/align_dlib.py,0,"b'# Copyright 2015-2016 Carnegie Mellon University\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Module for dlib-based alignment.""""""\n\n# NOTE: This file has been copied from the openface project.\n#  https://github.com/cmusatyalab/openface/blob/master/openface/align_dlib.py\n\nimport cv2\nimport dlib\nimport numpy as np\n\nTEMPLATE = np.float32([\n    (0.0792396913815, 0.339223741112), (0.0829219487236, 0.456955367943),\n    (0.0967927109165, 0.575648016728), (0.122141515615, 0.691921601066),\n    (0.168687863544, 0.800341263616), (0.239789390707, 0.895732504778),\n    (0.325662452515, 0.977068762493), (0.422318282013, 1.04329000149),\n    (0.531777802068, 1.06080371126), (0.641296298053, 1.03981924107),\n    (0.738105872266, 0.972268833998), (0.824444363295, 0.889624082279),\n    (0.894792677532, 0.792494155836), (0.939395486253, 0.681546643421),\n    (0.96111933829, 0.562238253072), (0.970579841181, 0.441758925744),\n    (0.971193274221, 0.322118743967), (0.163846223133, 0.249151738053),\n    (0.21780354657, 0.204255863861), (0.291299351124, 0.192367318323),\n    (0.367460241458, 0.203582210627), (0.4392945113, 0.233135599851),\n    (0.586445962425, 0.228141644834), (0.660152671635, 0.195923841854),\n    (0.737466449096, 0.182360984545), (0.813236546239, 0.192828009114),\n    (0.8707571886, 0.235293377042), (0.51534533827, 0.31863546193),\n    (0.516221448289, 0.396200446263), (0.517118861835, 0.473797687758),\n    (0.51816430343, 0.553157797772), (0.433701156035, 0.604054457668),\n    (0.475501237769, 0.62076344024), (0.520712933176, 0.634268222208),\n    (0.565874114041, 0.618796581487), (0.607054002672, 0.60157671656),\n    (0.252418718401, 0.331052263829), (0.298663015648, 0.302646354002),\n    (0.355749724218, 0.303020650651), (0.403718978315, 0.33867711083),\n    (0.352507175597, 0.349987615384), (0.296791759886, 0.350478978225),\n    (0.631326076346, 0.334136672344), (0.679073381078, 0.29645404267),\n    (0.73597236153, 0.294721285802), (0.782865376271, 0.321305281656),\n    (0.740312274764, 0.341849376713), (0.68499850091, 0.343734332172),\n    (0.353167761422, 0.746189164237), (0.414587777921, 0.719053835073),\n    (0.477677654595, 0.706835892494), (0.522732900812, 0.717092275768),\n    (0.569832064287, 0.705414478982), (0.635195811927, 0.71565572516),\n    (0.69951672331, 0.739419187253), (0.639447159575, 0.805236879972),\n    (0.576410514055, 0.835436670169), (0.525398405766, 0.841706377792),\n    (0.47641545769, 0.837505914975), (0.41379548902, 0.810045601727),\n    (0.380084785646, 0.749979603086), (0.477955996282, 0.74513234612),\n    (0.523389793327, 0.748924302636), (0.571057789237, 0.74332894691),\n    (0.672409137852, 0.744177032192), (0.572539621444, 0.776609286626),\n    (0.5240106503, 0.783370783245), (0.477561227414, 0.778476346951)])\n\nINV_TEMPLATE = np.float32([\n    (-0.04099179660567834, -0.008425234314031194, 2.575498465013183),\n    (0.04062510634554352, -0.009678089746831375, -1.2534351452524177),\n    (0.0003666902601348179, 0.01810332406086298, -0.32206331976076663)])\n\nTPL_MIN, TPL_MAX = np.min(TEMPLATE, axis=0), np.max(TEMPLATE, axis=0)\nMINMAX_TEMPLATE = (TEMPLATE - TPL_MIN) / (TPL_MAX - TPL_MIN)\n\n\nclass AlignDlib:\n    """"""\n    Use `dlib\'s landmark estimation <http://blog.dlib.net/2014/08/real-time-face-pose-estimation.html>`_ to align faces.\n\n    The alignment preprocess faces for input into a neural network.\n    Faces are resized to the same size (such as 96x96) and transformed\n    to make landmarks (such as the eyes and nose) appear at the same\n    location on every image.\n\n    Normalized landmarks:\n\n    .. image:: ../images/dlib-landmark-mean.png\n    """"""\n\n    #: Landmark indices corresponding to the inner eyes and bottom lip.\n    INNER_EYES_AND_BOTTOM_LIP = [39, 42, 57]\n\n    #: Landmark indices corresponding to the outer eyes and nose.\n    OUTER_EYES_AND_NOSE = [36, 45, 33]\n\n    def __init__(self, facePredictor):\n        """"""\n        Instantiate an \'AlignDlib\' object.\n\n        :param facePredictor: The path to dlib\'s\n        :type facePredictor: str\n        """"""\n        assert facePredictor is not None\n\n        # pylint: disable=no-member\n        self.detector = dlib.get_frontal_face_detector()\n        self.predictor = dlib.shape_predictor(facePredictor)\n\n    def getAllFaceBoundingBoxes(self, rgbImg):\n        """"""\n        Find all face bounding boxes in an image.\n\n        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n        :type rgbImg: numpy.ndarray\n        :return: All face bounding boxes in an image.\n        :rtype: dlib.rectangles\n        """"""\n        assert rgbImg is not None\n\n        try:\n            return self.detector(rgbImg, 1)\n        except Exception as e:  # pylint: disable=broad-except\n            print(""Warning: {}"".format(e))\n            # In rare cases, exceptions are thrown.\n            return []\n\n    def getLargestFaceBoundingBox(self, rgbImg, skipMulti=False):\n        """"""\n        Find the largest face bounding box in an image.\n\n        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n        :type rgbImg: numpy.ndarray\n        :param skipMulti: Skip image if more than one face detected.\n        :type skipMulti: bool\n        :return: The largest face bounding box in an image, or None.\n        :rtype: dlib.rectangle\n        """"""\n        assert rgbImg is not None\n\n        faces = self.getAllFaceBoundingBoxes(rgbImg)\n        if (not skipMulti and len(faces) > 0) or len(faces) == 1:\n            return max(faces, key=lambda rect: rect.width() * rect.height())\n        else:\n            return None\n\n    def findLandmarks(self, rgbImg, bb):\n        """"""\n        Find the landmarks of a face.\n\n        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n        :type rgbImg: numpy.ndarray\n        :param bb: Bounding box around the face to find landmarks for.\n        :type bb: dlib.rectangle\n        :return: Detected landmark locations.\n        :rtype: list of (x,y) tuples\n        """"""\n        assert rgbImg is not None\n        assert bb is not None\n\n        points = self.predictor(rgbImg, bb)\n        # return list(map(lambda p: (p.x, p.y), points.parts()))\n        return [(p.x, p.y) for p in points.parts()]\n\n    # pylint: disable=dangerous-default-value\n    def align(self, imgDim, rgbImg, bb=None,\n              landmarks=None, landmarkIndices=INNER_EYES_AND_BOTTOM_LIP,\n              skipMulti=False, scale=1.0):\n        r""""""align(imgDim, rgbImg, bb=None, landmarks=None, landmarkIndices=INNER_EYES_AND_BOTTOM_LIP)\n\n        Transform and align a face in an image.\n\n        :param imgDim: The edge length in pixels of the square the image is resized to.\n        :type imgDim: int\n        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n        :type rgbImg: numpy.ndarray\n        :param bb: Bounding box around the face to align. \\\n                   Defaults to the largest face.\n        :type bb: dlib.rectangle\n        :param landmarks: Detected landmark locations. \\\n                          Landmarks found on `bb` if not provided.\n        :type landmarks: list of (x,y) tuples\n        :param landmarkIndices: The indices to transform to.\n        :type landmarkIndices: list of ints\n        :param skipMulti: Skip image if more than one face detected.\n        :type skipMulti: bool\n        :param scale: Scale image before cropping to the size given by imgDim.\n        :type scale: float\n        :return: The aligned RGB image. Shape: (imgDim, imgDim, 3)\n        :rtype: numpy.ndarray\n        """"""\n        assert imgDim is not None\n        assert rgbImg is not None\n        assert landmarkIndices is not None\n\n        if bb is None:\n            bb = self.getLargestFaceBoundingBox(rgbImg, skipMulti)\n            if bb is None:\n                return\n\n        if landmarks is None:\n            landmarks = self.findLandmarks(rgbImg, bb)\n\n        npLandmarks = np.float32(landmarks)\n        npLandmarkIndices = np.array(landmarkIndices)\n\n        # pylint: disable=maybe-no-member\n        H = cv2.getAffineTransform(npLandmarks[npLandmarkIndices],\n                                   imgDim * MINMAX_TEMPLATE[npLandmarkIndices] * scale + imgDim * (1 - scale) / 2)\n        thumbnail = cv2.warpAffine(rgbImg, H, (imgDim, imgDim))\n\n        return thumbnail\n'"
medium_facenet_tutorial/download_and_extract_model.py,0,"b'import argparse\n\nimport logging\nimport os\nimport requests\nimport zipfile\n\n""""""\nThis file is copied from:\nhttps://github.com/davidsandberg/facenet/blob/master/src/download_and_extract_model.py\n""""""\n\nmodel_dict = {\n    \'20170511-185253\': \'0B5MzpY9kBtDVOTVnU3NIaUdySFE\'\n}\n\n\ndef download_and_extract_model(model_name, data_dir):\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    file_id = model_dict[model_name]\n    destination = os.path.join(data_dir, model_name + \'.zip\')\n    if not os.path.exists(destination):\n        print(\'Downloading model to %s\' % destination)\n        download_file_from_google_drive(file_id, destination)\n        with zipfile.ZipFile(destination, \'r\') as zip_ref:\n            print(\'Extracting model to %s\' % data_dir)\n            zip_ref.extractall(data_dir)\n\n\ndef download_file_from_google_drive(file_id, destination):\n    URL = ""https://drive.google.com/uc?export=download""\n\n    session = requests.Session()\n\n    response = session.get(URL, params={\'id\': file_id}, stream=True)\n    token = get_confirm_token(response)\n\n    if token:\n        params = {\'id\': file_id, \'confirm\': token}\n        response = session.get(URL, params=params, stream=True)\n\n    save_response_content(response, destination)\n\n\ndef get_confirm_token(response):\n    for key, value in response.cookies.items():\n        if key.startswith(\'download_warning\'):\n            return value\n\n    return None\n\n\ndef save_response_content(response, destination):\n    CHUNK_SIZE = 32768\n\n    with open(destination, ""wb"") as f:\n        for chunk in response.iter_content(CHUNK_SIZE):\n            if chunk:  # filter out keep-alive new chunks\n                f.write(chunk)\n\n\nif __name__ == \'__main__\':\n    logging.basicConfig(level=logging.DEBUG)\n    parser = argparse.ArgumentParser(add_help=True)\n    parser.add_argument(\'--model-dir\', type=str, action=\'store\', dest=\'model_dir\',\n                        help=\'Path to model protobuf graph\')\n\n    args = parser.parse_args()\n\n    download_and_extract_model(\'20170511-185253\', args.model_dir)\n'"
medium_facenet_tutorial/lfw_input.py,11,"b'import logging\nimport os\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\n\nlogger = logging.getLogger(__name__)\n\n\ndef read_data(image_paths, label_list, image_size, batch_size, max_nrof_epochs, num_threads, shuffle, random_flip,\n              random_brightness, random_contrast):\n    """"""\n    Creates Tensorflow Queue to batch load images. Applies transformations to images as they are loaded.\n    :param random_brightness: \n    :param random_flip: \n    :param image_paths: image paths to load\n    :param label_list: class labels for image paths\n    :param image_size: size to resize images to\n    :param batch_size: num of images to load in batch\n    :param max_nrof_epochs: total number of epochs to read through image list\n    :param num_threads: num threads to use\n    :param shuffle: Shuffle images\n    :param random_flip: Random Flip image\n    :param random_brightness: Apply random brightness transform to image\n    :param random_contrast: Apply random contrast transform to image\n    :return: images and labels of batch_size\n    """"""\n\n    images = ops.convert_to_tensor(image_paths, dtype=tf.string)\n    labels = ops.convert_to_tensor(label_list, dtype=tf.int32)\n\n    # Makes an input queue\n    input_queue = tf.train.slice_input_producer((images, labels),\n                                                num_epochs=max_nrof_epochs, shuffle=shuffle, )\n\n    images_labels = []\n    imgs = []\n    lbls = []\n    for _ in range(num_threads):\n        image, label = read_image_from_disk(filename_to_label_tuple=input_queue)\n        image = tf.random_crop(image, size=[image_size, image_size, 3])\n        image.set_shape((image_size, image_size, 3))\n        image = tf.image.per_image_standardization(image)\n\n        if random_flip:\n            image = tf.image.random_flip_left_right(image)\n\n        if random_brightness:\n            image = tf.image.random_brightness(image, max_delta=0.3)\n\n        if random_contrast:\n            image = tf.image.random_contrast(image, lower=0.2, upper=1.8)\n\n        imgs.append(image)\n        lbls.append(label)\n        images_labels.append([image, label])\n\n    image_batch, label_batch = tf.train.batch_join(images_labels,\n                                                   batch_size=batch_size,\n                                                   capacity=4 * num_threads,\n                                                   enqueue_many=False,\n                                                   allow_smaller_final_batch=True)\n    return image_batch, label_batch\n\n\ndef read_image_from_disk(filename_to_label_tuple):\n    """"""\n    Consumes input tensor and loads image\n    :param filename_to_label_tuple: \n    :type filename_to_label_tuple: list\n    :return: tuple of image and label\n    """"""\n    label = filename_to_label_tuple[1]\n    file_contents = tf.read_file(filename_to_label_tuple[0])\n    example = tf.image.decode_jpeg(file_contents, channels=3)\n    return example, label\n\n\ndef get_image_paths_and_labels(dataset):\n    image_paths_flat = []\n    labels_flat = []\n    for i in range(int(len(dataset))):\n        image_paths_flat += dataset[i].image_paths\n        labels_flat += [i] * len(dataset[i].image_paths)\n    return image_paths_flat, labels_flat\n\n\ndef get_dataset(input_directory):\n    dataset = []\n\n    classes = os.listdir(input_directory)\n    classes.sort()\n    nrof_classes = len(classes)\n    for i in range(nrof_classes):\n        class_name = classes[i]\n        facedir = os.path.join(input_directory, class_name)\n        if os.path.isdir(facedir):\n            images = os.listdir(facedir)\n            image_paths = [os.path.join(facedir, img) for img in images]\n            dataset.append(ImageClass(class_name, image_paths))\n\n    return dataset\n\n\ndef filter_dataset(dataset, min_images_per_label=10):\n    filtered_dataset = []\n    for i in range(len(dataset)):\n        if len(dataset[i].image_paths) < min_images_per_label:\n            logger.info(\'Skipping class: {}\'.format(dataset[i].name))\n            continue\n        else:\n            filtered_dataset.append(dataset[i])\n    return filtered_dataset\n\n\ndef split_dataset(dataset, split_ratio=0.8):\n    train_set = []\n    test_set = []\n    min_nrof_images = 2\n    for cls in dataset:\n        paths = cls.image_paths\n        np.random.shuffle(paths)\n        split = int(round(len(paths) * split_ratio))\n        if split < min_nrof_images:\n            continue  # Not enough images for test set. Skip class...\n        train_set.append(ImageClass(cls.name, paths[0:split]))\n        test_set.append(ImageClass(cls.name, paths[split:-1]))\n    return train_set, test_set\n\n\nclass ImageClass():\n    def __init__(self, name, image_paths):\n        self.name = name\n        self.image_paths = image_paths\n\n    def __str__(self):\n        return self.name + \', \' + str(len(self.image_paths)) + \' images\'\n\n    def __len__(self):\n        return len(self.image_paths)\n'"
medium_facenet_tutorial/preprocess.py,0,"b'import argparse\nimport glob\nimport logging\nimport multiprocessing as mp\nimport os\nimport time\n\nimport cv2\n\nfrom medium_facenet_tutorial.align_dlib import AlignDlib\n\nlogger = logging.getLogger(__name__)\n\nalign_dlib = AlignDlib(os.path.join(os.path.dirname(__file__), \'shape_predictor_68_face_landmarks.dat\'))\n\n\ndef main(input_dir, output_dir, crop_dim):\n    start_time = time.time()\n    pool = mp.Pool(processes=mp.cpu_count())\n\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    for image_dir in os.listdir(input_dir):\n        image_output_dir = os.path.join(output_dir, os.path.basename(os.path.basename(image_dir)))\n        if not os.path.exists(image_output_dir):\n            os.makedirs(image_output_dir)\n\n    image_paths = glob.glob(os.path.join(input_dir, \'**/*.jpg\'))\n    for index, image_path in enumerate(image_paths):\n        image_output_dir = os.path.join(output_dir, os.path.basename(os.path.dirname(image_path)))\n        output_path = os.path.join(image_output_dir, os.path.basename(image_path))\n        pool.apply_async(preprocess_image, (image_path, output_path, crop_dim))\n\n    pool.close()\n    pool.join()\n    logger.info(\'Completed in {} seconds\'.format(time.time() - start_time))\n\n\ndef preprocess_image(input_path, output_path, crop_dim):\n    """"""\n    Detect face, align and crop :param input_path. Write output to :param output_path\n    :param input_path: Path to input image\n    :param output_path: Path to write processed image\n    :param crop_dim: dimensions to crop image to\n    """"""\n    image = _process_image(input_path, crop_dim)\n    if image is not None:\n        logger.debug(\'Writing processed file: {}\'.format(output_path))\n        cv2.imwrite(output_path, image)\n    else:\n        logger.warning(""Skipping filename: {}"".format(input_path))\n\n\ndef _process_image(filename, crop_dim):\n    image = None\n    aligned_image = None\n\n    image = _buffer_image(filename)\n\n    if image is not None:\n        aligned_image = _align_image(image, crop_dim)\n    else:\n        raise IOError(\'Error buffering image: {}\'.format(filename))\n\n    return aligned_image\n\n\ndef _buffer_image(filename):\n    logger.debug(\'Reading image: {}\'.format(filename))\n    image = cv2.imread(filename, )\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    return image\n\n\ndef _align_image(image, crop_dim):\n    bb = align_dlib.getLargestFaceBoundingBox(image)\n    aligned = align_dlib.align(crop_dim, image, bb, landmarkIndices=AlignDlib.INNER_EYES_AND_BOTTOM_LIP)\n    if aligned is not None:\n        aligned = cv2.cvtColor(aligned, cv2.COLOR_BGR2RGB)\n    return aligned\n\n\nif __name__ == \'__main__\':\n    logging.basicConfig(level=logging.INFO)\n    parser = argparse.ArgumentParser(add_help=True)\n    parser.add_argument(\'--input-dir\', type=str, action=\'store\', default=\'data\', dest=\'input_dir\')\n    parser.add_argument(\'--output-dir\', type=str, action=\'store\', default=\'output\', dest=\'output_dir\')\n    parser.add_argument(\'--crop-dim\', type=int, action=\'store\', default=180, dest=\'crop_dim\',\n                        help=\'Size to crop images to\')\n\n    args = parser.parse_args()\n\n    main(args.input_dir, args.output_dir, args.crop_dim)\n'"
medium_facenet_tutorial/train_classifier.py,10,"b'import argparse\nimport logging\nimport os\nimport pickle\nimport sys\nimport time\n\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.svm import SVC\nfrom tensorflow.python.platform import gfile\n\nfrom lfw_input import filter_dataset, split_dataset, get_dataset\nfrom medium_facenet_tutorial import lfw_input\n\nlogger = logging.getLogger(__name__)\n\n\ndef main(input_directory, model_path, classifier_output_path, batch_size, num_threads, num_epochs,\n         min_images_per_labels, split_ratio, is_train=True):\n    """"""\n    Loads images from :param input_dir, creates embeddings using a model defined at :param model_path, and trains\n     a classifier outputted to :param output_path\n     \n    :param input_directory: Path to directory containing pre-processed images\n    :param model_path: Path to protobuf graph file for facenet model\n    :param classifier_output_path: Path to write pickled classifier\n    :param batch_size: Batch size to create embeddings\n    :param num_threads: Number of threads to utilize for queuing\n    :param num_epochs: Number of epochs for each image\n    :param min_images_per_labels: Minimum number of images per class\n    :param split_ratio: Ratio to split train/test dataset\n    :param is_train: bool denoting if training or evaluate\n    """"""\n\n    start_time = time.time()\n    with tf.Session(config=tf.ConfigProto(log_device_placement=False)) as sess:\n        train_set, test_set = _get_test_and_train_set(input_directory, min_num_images_per_label=min_images_per_labels,\n                                                      split_ratio=split_ratio)\n        if is_train:\n            images, labels, class_names = _load_images_and_labels(train_set, image_size=160, batch_size=batch_size,\n                                                                  num_threads=num_threads, num_epochs=num_epochs,\n                                                                  random_flip=True, random_brightness=True,\n                                                                  random_contrast=True)\n        else:\n            images, labels, class_names = _load_images_and_labels(test_set, image_size=160, batch_size=batch_size,\n                                                                  num_threads=num_threads, num_epochs=1)\n\n        _load_model(model_filepath=model_path)\n\n        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n        sess.run(init_op)\n\n        images_placeholder = tf.get_default_graph().get_tensor_by_name(""input:0"")\n        embedding_layer = tf.get_default_graph().get_tensor_by_name(""embeddings:0"")\n        phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(""phase_train:0"")\n\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n\n        emb_array, label_array = _create_embeddings(embedding_layer, images, labels, images_placeholder,\n                                                    phase_train_placeholder, sess)\n\n        coord.request_stop()\n        coord.join(threads=threads)\n        logger.info(\'Created {} embeddings\'.format(len(emb_array)))\n\n        classifier_filename = classifier_output_path\n\n        if is_train:\n            _train_and_save_classifier(emb_array, label_array, class_names, classifier_filename)\n        else:\n            _evaluate_classifier(emb_array, label_array, classifier_filename)\n\n        logger.info(\'Completed in {} seconds\'.format(time.time() - start_time))\n\n\ndef _get_test_and_train_set(input_dir, min_num_images_per_label, split_ratio=0.7):\n    """"""\n    Load train and test dataset. Classes with < :param min_num_images_per_label will be filtered out.\n    :param input_dir: \n    :param min_num_images_per_label: \n    :param split_ratio: \n    :return: \n    """"""\n    dataset = get_dataset(input_dir)\n    dataset = filter_dataset(dataset, min_images_per_label=min_num_images_per_label)\n    train_set, test_set = split_dataset(dataset, split_ratio=split_ratio)\n\n    return train_set, test_set\n\n\ndef _load_images_and_labels(dataset, image_size, batch_size, num_threads, num_epochs, random_flip=False,\n                            random_brightness=False, random_contrast=False):\n    class_names = [cls.name for cls in dataset]\n    image_paths, labels = lfw_input.get_image_paths_and_labels(dataset)\n    images, labels = lfw_input.read_data(image_paths, labels, image_size, batch_size, num_epochs, num_threads,\n                                         shuffle=False, random_flip=random_flip, random_brightness=random_brightness,\n                                         random_contrast=random_contrast)\n    return images, labels, class_names\n\n\ndef _load_model(model_filepath):\n    """"""\n    Load frozen protobuf graph\n    :param model_filepath: Path to protobuf graph\n    :type model_filepath: str\n    """"""\n    model_exp = os.path.expanduser(model_filepath)\n    if os.path.isfile(model_exp):\n        logging.info(\'Model filename: %s\' % model_exp)\n        with gfile.FastGFile(model_exp, \'rb\') as f:\n            graph_def = tf.GraphDef()\n            graph_def.ParseFromString(f.read())\n            tf.import_graph_def(graph_def, name=\'\')\n    else:\n        logger.error(\'Missing model file. Exiting\')\n        sys.exit(-1)\n\n\ndef _create_embeddings(embedding_layer, images, labels, images_placeholder, phase_train_placeholder, sess):\n    """"""\n    Uses model to generate embeddings from :param images.\n    :param embedding_layer: \n    :param images: \n    :param labels: \n    :param images_placeholder: \n    :param phase_train_placeholder: \n    :param sess: \n    :return: (tuple): image embeddings and labels\n    """"""\n    emb_array = None\n    label_array = None\n    try:\n        i = 0\n        while True:\n            batch_images, batch_labels = sess.run([images, labels])\n            logger.info(\'Processing iteration {} batch of size: {}\'.format(i, len(batch_labels)))\n            emb = sess.run(embedding_layer,\n                           feed_dict={images_placeholder: batch_images, phase_train_placeholder: False})\n\n            emb_array = np.concatenate([emb_array, emb]) if emb_array is not None else emb\n            label_array = np.concatenate([label_array, batch_labels]) if label_array is not None else batch_labels\n            i += 1\n\n    except tf.errors.OutOfRangeError:\n        pass\n\n    return emb_array, label_array\n\n\ndef _train_and_save_classifier(emb_array, label_array, class_names, classifier_filename_exp):\n    logger.info(\'Training Classifier\')\n    model = SVC(kernel=\'linear\', probability=True, verbose=False)\n    model.fit(emb_array, label_array)\n\n    with open(classifier_filename_exp, \'wb\') as outfile:\n        pickle.dump((model, class_names), outfile)\n    logging.info(\'Saved classifier model to file ""%s""\' % classifier_filename_exp)\n\n\ndef _evaluate_classifier(emb_array, label_array, classifier_filename):\n    logger.info(\'Evaluating classifier on {} images\'.format(len(emb_array)))\n    if not os.path.exists(classifier_filename):\n        raise ValueError(\'Pickled classifier not found, have you trained first?\')\n\n    with open(classifier_filename, \'rb\') as f:\n        model, class_names = pickle.load(f)\n\n        predictions = model.predict_proba(emb_array, )\n        best_class_indices = np.argmax(predictions, axis=1)\n        best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\n\n        for i in range(len(best_class_indices)):\n            print(\'%4d  %s: %.3f\' % (i, class_names[best_class_indices[i]], best_class_probabilities[i]))\n\n        accuracy = np.mean(np.equal(best_class_indices, label_array))\n        print(\'Accuracy: %.3f\' % accuracy)\n\n\nif __name__ == \'__main__\':\n    logging.basicConfig(level=logging.INFO)\n    parser = argparse.ArgumentParser(add_help=True)\n    parser.add_argument(\'--model-path\', type=str, action=\'store\', dest=\'model_path\',\n                        help=\'Path to model protobuf graph\')\n    parser.add_argument(\'--input-dir\', type=str, action=\'store\', dest=\'input_dir\',\n                        help=\'Input path of data to train on\')\n    parser.add_argument(\'--batch-size\', type=int, action=\'store\', dest=\'batch_size\',\n                        help=\'Input path of data to train on\', default=128)\n    parser.add_argument(\'--num-threads\', type=int, action=\'store\', dest=\'num_threads\', default=16,\n                        help=\'Number of threads to utilize for queue\')\n    parser.add_argument(\'--num-epochs\', type=int, action=\'store\', dest=\'num_epochs\', default=3,\n                        help=\'Path to output trained classifier model\')\n    parser.add_argument(\'--split-ratio\', type=float, action=\'store\', dest=\'split_ratio\', default=0.7,\n                        help=\'Ratio to split train/test dataset\')\n    parser.add_argument(\'--min-num-images-per-class\', type=int, action=\'store\', default=10,\n                        dest=\'min_images_per_class\', help=\'Minimum number of images per class\')\n    parser.add_argument(\'--classifier-path\', type=str, action=\'store\', dest=\'classifier_path\',\n                        help=\'Path to output trained classifier model\')\n    parser.add_argument(\'--is-train\', action=\'store_true\', dest=\'is_train\', default=False,\n                        help=\'Flag to determine if train or evaluate\')\n\n    args = parser.parse_args()\n\n    main(input_directory=args.input_dir, model_path=args.model_path, classifier_output_path=args.classifier_path,\n         batch_size=args.batch_size, num_threads=args.num_threads, num_epochs=args.num_epochs,\n         min_images_per_labels=args.min_images_per_class, split_ratio=args.split_ratio, is_train=args.is_train)\n'"
