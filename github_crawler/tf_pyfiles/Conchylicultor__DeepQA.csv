file_path,api_count,code
main.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2015 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nMain script. See README.md for more information\n\nUse python 3\n""""""\n\nfrom chatbot import chatbot\n\n\nif __name__ == ""__main__"":\n    chatbot = chatbot.Chatbot()\n    chatbot.main()\n'"
testsuite.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2015 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\n""""""\nTest the chatbot by launching some unit tests\nWarning: it does not check the performances of the program\n\n""""""\n\nimport unittest\nimport io\nimport sys\n\nfrom chatbot import chatbot\n\n\nclass TestChatbot(unittest.TestCase):\n    def setUp(self):\n        self.chatbot = chatbot.Chatbot()\n\n    def test_training_simple(self):\n        self.chatbot.main([\n            \'--maxLength\', \'3\', \n            \'--numEpoch\', \'1\', \n            \'--modelTag\', \'unit-test\'\n        ])\n\n    def test_training_watson(self):\n        pass\n\n    def test_testing_all(self):\n        pass\n\n    def test_testing_interactive(self):\n        progInput = io.StringIO()\n        progInput.write(\'Hi!\\n\')\n        progInput.write(\'How are you ?\\n\')\n        progInput.write(\'aersdsd azej qsdfs\\n\')  # Unknown words\n        progInput.write(\'\xc3\xa9""[)=\xc3\xa8^$*::!\\n\')  # Encoding\n        progInput.write(\'ae e qsd, qsd 45 zeo h qfo k zedo. h et qsd qsfjze sfnj zjksdf zehkqf jkzae?\\n\')  # Too long sentences\n        progInput.write(\'exit\\n\')\n\n        #sys.stdin = progInput\n\n        #self.chatbot.main([\'--test\', \'interactive\', \'--modelTag\', \'unit-test\'])\n\n    def test_testing_daemon(self):\n        pass\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
chatbot/__init__.py,0,"b'__all__ = [""chatbot""]\n'"
chatbot/chatbot.py,13,"b'# Copyright 2015 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nMain script. See README.md for more information\n\nUse python 3\n""""""\n\nimport argparse  # Command line parsing\nimport configparser  # Saving the models parameters\nimport datetime  # Chronometer\nimport os  # Files management\nimport tensorflow as tf\nimport numpy as np\nimport math\n\nfrom tqdm import tqdm  # Progress bar\nfrom tensorflow.python import debug as tf_debug\n\nfrom chatbot.textdata import TextData\nfrom chatbot.model import Model\n\n\nclass Chatbot:\n    """"""\n    Main class which launch the training or testing mode\n    """"""\n\n    class TestMode:\n        """""" Simple structure representing the different testing modes\n        """"""\n        ALL = \'all\'\n        INTERACTIVE = \'interactive\'  # The user can write his own questions\n        DAEMON = \'daemon\'  # The chatbot runs on background and can regularly be called to predict something\n\n    def __init__(self):\n        """"""\n        """"""\n        # Model/dataset parameters\n        self.args = None\n\n        # Task specific object\n        self.textData = None  # Dataset\n        self.model = None  # Sequence to sequence model\n\n        # Tensorflow utilities for convenience saving/logging\n        self.writer = None\n        self.saver = None\n        self.modelDir = \'\'  # Where the model is saved\n        self.globStep = 0  # Represent the number of iteration for the current model\n\n        # TensorFlow main session (we keep track for the daemon)\n        self.sess = None\n\n        # Filename and directories constants\n        self.MODEL_DIR_BASE = \'save\' + os.sep + \'model\'\n        self.MODEL_NAME_BASE = \'model\'\n        self.MODEL_EXT = \'.ckpt\'\n        self.CONFIG_FILENAME = \'params.ini\'\n        self.CONFIG_VERSION = \'0.5\'\n        self.TEST_IN_NAME = \'data\' + os.sep + \'test\' + os.sep + \'samples.txt\'\n        self.TEST_OUT_SUFFIX = \'_predictions.txt\'\n        self.SENTENCES_PREFIX = [\'Q: \', \'A: \']\n\n    @staticmethod\n    def parseArgs(args):\n        """"""\n        Parse the arguments from the given command line\n        Args:\n            args (list<str>): List of arguments to parse. If None, the default sys.argv will be parsed\n        """"""\n\n        parser = argparse.ArgumentParser()\n\n        # Global options\n        globalArgs = parser.add_argument_group(\'Global options\')\n        globalArgs.add_argument(\'--test\',\n                                nargs=\'?\',\n                                choices=[Chatbot.TestMode.ALL, Chatbot.TestMode.INTERACTIVE, Chatbot.TestMode.DAEMON],\n                                const=Chatbot.TestMode.ALL, default=None,\n                                help=\'if present, launch the program try to answer all sentences from data/test/ with\'\n                                     \' the defined model(s), in interactive mode, the user can wrote his own sentences,\'\n                                     \' use daemon mode to integrate the chatbot in another program\')\n        globalArgs.add_argument(\'--createDataset\', action=\'store_true\', help=\'if present, the program will only generate the dataset from the corpus (no training/testing)\')\n        globalArgs.add_argument(\'--playDataset\', type=int, nargs=\'?\', const=10, default=None,  help=\'if set, the program  will randomly play some samples(can be use conjointly with createDataset if this is the only action you want to perform)\')\n        globalArgs.add_argument(\'--reset\', action=\'store_true\', help=\'use this if you want to ignore the previous model present on the model directory (Warning: the model will be destroyed with all the folder content)\')\n        globalArgs.add_argument(\'--verbose\', action=\'store_true\', help=\'When testing, will plot the outputs at the same time they are computed\')\n        globalArgs.add_argument(\'--debug\', action=\'store_true\', help=\'run DeepQA with Tensorflow debug mode. Read TF documentation for more details on this.\')\n        globalArgs.add_argument(\'--keepAll\', action=\'store_true\', help=\'If this option is set, all saved model will be kept (Warning: make sure you have enough free disk space or increase saveEvery)\')  # TODO: Add an option to delimit the max size\n        globalArgs.add_argument(\'--modelTag\', type=str, default=None, help=\'tag to differentiate which model to store/load\')\n        globalArgs.add_argument(\'--rootDir\', type=str, default=None, help=\'folder where to look for the models and data\')\n        globalArgs.add_argument(\'--watsonMode\', action=\'store_true\', help=\'Inverse the questions and answer when training (the network try to guess the question)\')\n        globalArgs.add_argument(\'--autoEncode\', action=\'store_true\', help=\'Randomly pick the question or the answer and use it both as input and output\')\n        globalArgs.add_argument(\'--device\', type=str, default=None, help=\'\\\'gpu\\\' or \\\'cpu\\\' (Warning: make sure you have enough free RAM), allow to choose on which hardware run the model\')\n        globalArgs.add_argument(\'--seed\', type=int, default=None, help=\'random seed for replication\')\n\n        # Dataset options\n        datasetArgs = parser.add_argument_group(\'Dataset options\')\n        datasetArgs.add_argument(\'--corpus\', choices=TextData.corpusChoices(), default=TextData.corpusChoices()[0], help=\'corpus on which extract the dataset.\')\n        datasetArgs.add_argument(\'--datasetTag\', type=str, default=\'\', help=\'add a tag to the dataset (file where to load the vocabulary and the precomputed samples, not the original corpus). Useful to manage multiple versions. Also used to define the file used for the lightweight format.\')  # The samples are computed from the corpus if it does not exist already. There are saved in \\\'data/samples/\\\'\n        datasetArgs.add_argument(\'--ratioDataset\', type=float, default=1.0, help=\'ratio of dataset used to avoid using the whole dataset\')  # Not implemented, useless ?\n        datasetArgs.add_argument(\'--maxLength\', type=int, default=10, help=\'maximum length of the sentence (for input and output), define number of maximum step of the RNN\')\n        datasetArgs.add_argument(\'--filterVocab\', type=int, default=1, help=\'remove rarelly used words (by default words used only once). 0 to keep all words.\')\n        datasetArgs.add_argument(\'--skipLines\', action=\'store_true\', help=\'Generate training samples by only using even conversation lines as questions (and odd lines as answer). Useful to train the network on a particular person.\')\n        datasetArgs.add_argument(\'--vocabularySize\', type=int, default=40000, help=\'Limit the number of words in the vocabulary (0 for unlimited)\')\n\n        # Network options (Warning: if modifying something here, also make the change on save/loadParams() )\n        nnArgs = parser.add_argument_group(\'Network options\', \'architecture related option\')\n        nnArgs.add_argument(\'--hiddenSize\', type=int, default=512, help=\'number of hidden units in each RNN cell\')\n        nnArgs.add_argument(\'--numLayers\', type=int, default=2, help=\'number of rnn layers\')\n        nnArgs.add_argument(\'--softmaxSamples\', type=int, default=0, help=\'Number of samples in the sampled softmax loss function. A value of 0 deactivates sampled softmax\')\n        nnArgs.add_argument(\'--initEmbeddings\', action=\'store_true\', help=\'if present, the program will initialize the embeddings with pre-trained word2vec vectors\')\n        nnArgs.add_argument(\'--embeddingSize\', type=int, default=64, help=\'embedding size of the word representation\')\n        nnArgs.add_argument(\'--embeddingSource\', type=str, default=""GoogleNews-vectors-negative300.bin"", help=\'embedding file to use for the word representation\')\n\n        # Training options\n        trainingArgs = parser.add_argument_group(\'Training options\')\n        trainingArgs.add_argument(\'--numEpochs\', type=int, default=30, help=\'maximum number of epochs to run\')\n        trainingArgs.add_argument(\'--saveEvery\', type=int, default=2000, help=\'nb of mini-batch step before creating a model checkpoint\')\n        trainingArgs.add_argument(\'--batchSize\', type=int, default=256, help=\'mini-batch size\')\n        trainingArgs.add_argument(\'--learningRate\', type=float, default=0.002, help=\'Learning rate\')\n        trainingArgs.add_argument(\'--dropout\', type=float, default=0.9, help=\'Dropout rate (keep probabilities)\')\n\n        return parser.parse_args(args)\n\n    def main(self, args=None):\n        """"""\n        Launch the training and/or the interactive mode\n        """"""\n        print(\'Welcome to DeepQA v0.1 !\')\n        print()\n        print(\'TensorFlow detected: v{}\'.format(tf.__version__))\n\n        # General initialisation\n\n        self.args = self.parseArgs(args)\n\n        if not self.args.rootDir:\n            self.args.rootDir = os.getcwd()  # Use the current working directory\n\n        #tf.logging.set_verbosity(tf.logging.INFO) # DEBUG, INFO, WARN (default), ERROR, or FATAL\n\n        self.loadModelParams()  # Update the self.modelDir and self.globStep, for now, not used when loading Model (but need to be called before _getSummaryName)\n\n        self.textData = TextData(self.args)\n        # TODO: Add a mode where we can force the input of the decoder // Try to visualize the predictions for\n        # each word of the vocabulary / decoder input\n        # TODO: For now, the model are trained for a specific dataset (because of the maxLength which define the\n        # vocabulary). Add a compatibility mode which allow to launch a model trained on a different vocabulary (\n        # remap the word2id/id2word variables).\n        if self.args.createDataset:\n            print(\'Dataset created! Thanks for using this program\')\n            return  # No need to go further\n\n        # Prepare the model\n        with tf.device(self.getDevice()):\n            self.model = Model(self.args, self.textData)\n\n        # Saver/summaries\n        self.writer = tf.summary.FileWriter(self._getSummaryName())\n        self.saver = tf.train.Saver(max_to_keep=200)\n\n        # TODO: Fixed seed (WARNING: If dataset shuffling, make sure to do that after saving the\n        # dataset, otherwise, all which cames after the shuffling won\'t be replicable when\n        # reloading the dataset). How to restore the seed after loading ??\n        # Also fix seed for random.shuffle (does it works globally for all files ?)\n\n        # Running session\n        self.sess = tf.Session(config=tf.ConfigProto(\n            allow_soft_placement=True,  # Allows backup device for non GPU-available operations (when forcing GPU)\n            log_device_placement=False)  # Too verbose ?\n        )  # TODO: Replace all sess by self.sess (not necessary a good idea) ?\n\n        if self.args.debug:\n            self.sess = tf_debug.LocalCLIDebugWrapperSession(self.sess)\n            self.sess.add_tensor_filter(""has_inf_or_nan"", tf_debug.has_inf_or_nan)\n\n        print(\'Initialize variables...\')\n        self.sess.run(tf.global_variables_initializer())\n\n        # Reload the model eventually (if it exist.), on testing mode, the models are not loaded here (but in predictTestset)\n        if self.args.test != Chatbot.TestMode.ALL:\n            self.managePreviousModel(self.sess)\n\n        # Initialize embeddings with pre-trained word2vec vectors\n        if self.args.initEmbeddings:\n            self.loadEmbedding(self.sess)\n\n        if self.args.test:\n            if self.args.test == Chatbot.TestMode.INTERACTIVE:\n                self.mainTestInteractive(self.sess)\n            elif self.args.test == Chatbot.TestMode.ALL:\n                print(\'Start predicting...\')\n                self.predictTestset(self.sess)\n                print(\'All predictions done\')\n            elif self.args.test == Chatbot.TestMode.DAEMON:\n                print(\'Daemon mode, running in background...\')\n            else:\n                raise RuntimeError(\'Unknown test mode: {}\'.format(self.args.test))  # Should never happen\n        else:\n            self.mainTrain(self.sess)\n\n        if self.args.test != Chatbot.TestMode.DAEMON:\n            self.sess.close()\n            print(""The End! Thanks for using this program"")\n\n    def mainTrain(self, sess):\n        """""" Training loop\n        Args:\n            sess: The current running session\n        """"""\n\n        # Specific training dependent loading\n\n        self.textData.makeLighter(self.args.ratioDataset)  # Limit the number of training samples\n\n        mergedSummaries = tf.summary.merge_all()  # Define the summary operator (Warning: Won\'t appear on the tensorboard graph)\n        if self.globStep == 0:  # Not restoring from previous run\n            self.writer.add_graph(sess.graph)  # First time only\n\n        # If restoring a model, restore the progression bar ? and current batch ?\n\n        print(\'Start training (press Ctrl+C to save and exit)...\')\n\n        try:  # If the user exit while training, we still try to save the model\n            for e in range(self.args.numEpochs):\n\n                print()\n                print(""----- Epoch {}/{} ; (lr={}) -----"".format(e+1, self.args.numEpochs, self.args.learningRate))\n\n                batches = self.textData.getBatches()\n\n                # TODO: Also update learning parameters eventually\n\n                tic = datetime.datetime.now()\n                for nextBatch in tqdm(batches, desc=""Training""):\n                    # Training pass\n                    ops, feedDict = self.model.step(nextBatch)\n                    assert len(ops) == 2  # training, loss\n                    _, loss, summary = sess.run(ops + (mergedSummaries,), feedDict)\n                    self.writer.add_summary(summary, self.globStep)\n                    self.globStep += 1\n\n                    # Output training status\n                    if self.globStep % 100 == 0:\n                        perplexity = math.exp(float(loss)) if loss < 300 else float(""inf"")\n                        tqdm.write(""----- Step %d -- Loss %.2f -- Perplexity %.2f"" % (self.globStep, loss, perplexity))\n\n                    # Checkpoint\n                    if self.globStep % self.args.saveEvery == 0:\n                        self._saveSession(sess)\n\n                toc = datetime.datetime.now()\n\n                print(""Epoch finished in {}"".format(toc-tic))  # Warning: Will overflow if an epoch takes more than 24 hours, and the output isn\'t really nicer\n        except (KeyboardInterrupt, SystemExit):  # If the user press Ctrl+C while testing progress\n            print(\'Interruption detected, exiting the program...\')\n\n        self._saveSession(sess)  # Ultimate saving before complete exit\n\n    def predictTestset(self, sess):\n        """""" Try predicting the sentences from the samples.txt file.\n        The sentences are saved on the modelDir under the same name\n        Args:\n            sess: The current running session\n        """"""\n\n        # Loading the file to predict\n        with open(os.path.join(self.args.rootDir, self.TEST_IN_NAME), \'r\') as f:\n            lines = f.readlines()\n\n        modelList = self._getModelList()\n        if not modelList:\n            print(\'Warning: No model found in \\\'{}\\\'. Please train a model before trying to predict\'.format(self.modelDir))\n            return\n\n        # Predicting for each model present in modelDir\n        for modelName in sorted(modelList):  # TODO: Natural sorting\n            print(\'Restoring previous model from {}\'.format(modelName))\n            self.saver.restore(sess, modelName)\n            print(\'Testing...\')\n\n            saveName = modelName[:-len(self.MODEL_EXT)] + self.TEST_OUT_SUFFIX  # We remove the model extension and add the prediction suffix\n            with open(saveName, \'w\') as f:\n                nbIgnored = 0\n                for line in tqdm(lines, desc=\'Sentences\'):\n                    question = line[:-1]  # Remove the endl character\n\n                    answer = self.singlePredict(question)\n                    if not answer:\n                        nbIgnored += 1\n                        continue  # Back to the beginning, try again\n\n                    predString = \'{x[0]}{0}\\n{x[1]}{1}\\n\\n\'.format(question, self.textData.sequence2str(answer, clean=True), x=self.SENTENCES_PREFIX)\n                    if self.args.verbose:\n                        tqdm.write(predString)\n                    f.write(predString)\n                print(\'Prediction finished, {}/{} sentences ignored (too long)\'.format(nbIgnored, len(lines)))\n\n    def mainTestInteractive(self, sess):\n        """""" Try predicting the sentences that the user will enter in the console\n        Args:\n            sess: The current running session\n        """"""\n        # TODO: If verbose mode, also show similar sentences from the training set with the same words (include in mainTest also)\n        # TODO: Also show the top 10 most likely predictions for each predicted output (when verbose mode)\n        # TODO: Log the questions asked for latter re-use (merge with test/samples.txt)\n\n        print(\'Testing: Launch interactive mode:\')\n        print(\'\')\n        print(\'Welcome to the interactive mode, here you can ask to Deep Q&A the sentence you want. Don\\\'t have high \'\n              \'expectation. Type \\\'exit\\\' or just press ENTER to quit the program. Have fun.\')\n\n        while True:\n            question = input(self.SENTENCES_PREFIX[0])\n            if question == \'\' or question == \'exit\':\n                break\n\n            questionSeq = []  # Will be contain the question as seen by the encoder\n            answer = self.singlePredict(question, questionSeq)\n            if not answer:\n                print(\'Warning: sentence too long, sorry. Maybe try a simpler sentence.\')\n                continue  # Back to the beginning, try again\n\n            print(\'{}{}\'.format(self.SENTENCES_PREFIX[1], self.textData.sequence2str(answer, clean=True)))\n\n            if self.args.verbose:\n                print(self.textData.batchSeq2str(questionSeq, clean=True, reverse=True))\n                print(self.textData.sequence2str(answer))\n\n            print()\n\n    def singlePredict(self, question, questionSeq=None):\n        """""" Predict the sentence\n        Args:\n            question (str): the raw input sentence\n            questionSeq (List<int>): output argument. If given will contain the input batch sequence\n        Return:\n            list <int>: the word ids corresponding to the answer\n        """"""\n        # Create the input batch\n        batch = self.textData.sentence2enco(question)\n        if not batch:\n            return None\n        if questionSeq is not None:  # If the caller want to have the real input\n            questionSeq.extend(batch.encoderSeqs)\n\n        # Run the model\n        ops, feedDict = self.model.step(batch)\n        output = self.sess.run(ops[0], feedDict)  # TODO: Summarize the output too (histogram, ...)\n        answer = self.textData.deco2sentence(output)\n\n        return answer\n\n    def daemonPredict(self, sentence):\n        """""" Return the answer to a given sentence (same as singlePredict() but with additional cleaning)\n        Args:\n            sentence (str): the raw input sentence\n        Return:\n            str: the human readable sentence\n        """"""\n        return self.textData.sequence2str(\n            self.singlePredict(sentence),\n            clean=True\n        )\n\n    def daemonClose(self):\n        """""" A utility function to close the daemon when finish\n        """"""\n        print(\'Exiting the daemon mode...\')\n        self.sess.close()\n        print(\'Daemon closed.\')\n\n    def loadEmbedding(self, sess):\n        """""" Initialize embeddings with pre-trained word2vec vectors\n        Will modify the embedding weights of the current loaded model\n        Uses the GoogleNews pre-trained values (path hardcoded)\n        """"""\n\n        # Fetch embedding variables from model\n        with tf.variable_scope(""embedding_rnn_seq2seq/rnn/embedding_wrapper"", reuse=True):\n            em_in = tf.get_variable(""embedding"")\n        with tf.variable_scope(""embedding_rnn_seq2seq/embedding_rnn_decoder"", reuse=True):\n            em_out = tf.get_variable(""embedding"")\n\n        # Disable training for embeddings\n        variables = tf.get_collection_ref(tf.GraphKeys.TRAINABLE_VARIABLES)\n        variables.remove(em_in)\n        variables.remove(em_out)\n\n        # If restoring a model, we can leave here\n        if self.globStep != 0:\n            return\n\n        # New model, we load the pre-trained word2vec data and initialize embeddings\n        embeddings_path = os.path.join(self.args.rootDir, \'data\', \'embeddings\', self.args.embeddingSource)\n        embeddings_format = os.path.splitext(embeddings_path)[1][1:]\n        print(""Loading pre-trained word embeddings from %s "" % embeddings_path)\n        with open(embeddings_path, ""rb"") as f:\n            header = f.readline()\n            vocab_size, vector_size = map(int, header.split())\n            binary_len = np.dtype(\'float32\').itemsize * vector_size\n            initW = np.random.uniform(-0.25,0.25,(len(self.textData.word2id), vector_size))\n            for line in tqdm(range(vocab_size)):\n                word = []\n                while True:\n                    ch = f.read(1)\n                    if ch == b\' \':\n                        word = b\'\'.join(word).decode(\'utf-8\')\n                        break\n                    if ch != b\'\\n\':\n                        word.append(ch)\n                if word in self.textData.word2id:\n                    if embeddings_format == \'bin\':\n                        vector = np.fromstring(f.read(binary_len), dtype=\'float32\')\n                    elif embeddings_format == \'vec\':\n                        vector = np.fromstring(f.readline(), sep=\' \', dtype=\'float32\')\n                    else:\n                        raise Exception(""Unkown format for embeddings: %s "" % embeddings_format)\n                    initW[self.textData.word2id[word]] = vector\n                else:\n                    if embeddings_format == \'bin\':\n                        f.read(binary_len)\n                    elif embeddings_format == \'vec\':\n                        f.readline()\n                    else:\n                        raise Exception(""Unkown format for embeddings: %s "" % embeddings_format)\n\n        # PCA Decomposition to reduce word2vec dimensionality\n        if self.args.embeddingSize < vector_size:\n            U, s, Vt = np.linalg.svd(initW, full_matrices=False)\n            S = np.zeros((vector_size, vector_size), dtype=complex)\n            S[:vector_size, :vector_size] = np.diag(s)\n            initW = np.dot(U[:, :self.args.embeddingSize], S[:self.args.embeddingSize, :self.args.embeddingSize])\n\n        # Initialize input and output embeddings\n        sess.run(em_in.assign(initW))\n        sess.run(em_out.assign(initW))\n\n\n    def managePreviousModel(self, sess):\n        """""" Restore or reset the model, depending of the parameters\n        If the destination directory already contains some file, it will handle the conflict as following:\n         * If --reset is set, all present files will be removed (warning: no confirmation is asked) and the training\n         restart from scratch (globStep & cie reinitialized)\n         * Otherwise, it will depend of the directory content. If the directory contains:\n           * No model files (only summary logs): works as a reset (restart from scratch)\n           * Other model files, but modelName not found (surely keepAll option changed): raise error, the user should\n           decide by himself what to do\n           * The right model file (eventually some other): no problem, simply resume the training\n        In any case, the directory will exist as it has been created by the summary writer\n        Args:\n            sess: The current running session\n        """"""\n\n        print(\'WARNING: \', end=\'\')\n\n        modelName = self._getModelName()\n\n        if os.listdir(self.modelDir):\n            if self.args.reset:\n                print(\'Reset: Destroying previous model at {}\'.format(self.modelDir))\n            # Analysing directory content\n            elif os.path.exists(modelName):  # Restore the model\n                print(\'Restoring previous model from {}\'.format(modelName))\n                self.saver.restore(sess, modelName)  # Will crash when --reset is not activated and the model has not been saved yet\n            elif self._getModelList():\n                print(\'Conflict with previous models.\')\n                raise RuntimeError(\'Some models are already present in \\\'{}\\\'. You should check them first (or re-try with the keepAll flag)\'.format(self.modelDir))\n            else:  # No other model to conflict with (probably summary files)\n                print(\'No previous model found, but some files found at {}. Cleaning...\'.format(self.modelDir))  # Warning: No confirmation asked\n                self.args.reset = True\n\n            if self.args.reset:\n                fileList = [os.path.join(self.modelDir, f) for f in os.listdir(self.modelDir)]\n                for f in fileList:\n                    print(\'Removing {}\'.format(f))\n                    os.remove(f)\n\n        else:\n            print(\'No previous model found, starting from clean directory: {}\'.format(self.modelDir))\n\n    def _saveSession(self, sess):\n        """""" Save the model parameters and the variables\n        Args:\n            sess: the current session\n        """"""\n        tqdm.write(\'Checkpoint reached: saving model (don\\\'t stop the run)...\')\n        self.saveModelParams()\n        model_name = self._getModelName()\n        with open(model_name, \'w\') as f:  # HACK: Simulate the old model existance to avoid rewriting the file parser\n            f.write(\'This file is used internally by DeepQA to check the model existance. Please do not remove.\\n\')\n        self.saver.save(sess, model_name)  # TODO: Put a limit size (ex: 3GB for the modelDir)\n        tqdm.write(\'Model saved.\')\n\n    def _getModelList(self):\n        """""" Return the list of the model files inside the model directory\n        """"""\n        return [os.path.join(self.modelDir, f) for f in os.listdir(self.modelDir) if f.endswith(self.MODEL_EXT)]\n\n    def loadModelParams(self):\n        """""" Load the some values associated with the current model, like the current globStep value\n        For now, this function does not need to be called before loading the model (no parameters restored). However,\n        the modelDir name will be initialized here so it is required to call this function before managePreviousModel(),\n        _getModelName() or _getSummaryName()\n        Warning: if you modify this function, make sure the changes mirror saveModelParams, also check if the parameters\n        should be reset in managePreviousModel\n        """"""\n        # Compute the current model path\n        self.modelDir = os.path.join(self.args.rootDir, self.MODEL_DIR_BASE)\n        if self.args.modelTag:\n            self.modelDir += \'-\' + self.args.modelTag\n\n        # If there is a previous model, restore some parameters\n        configName = os.path.join(self.modelDir, self.CONFIG_FILENAME)\n        if not self.args.reset and not self.args.createDataset and os.path.exists(configName):\n            # Loading\n            config = configparser.ConfigParser()\n            config.read(configName)\n\n            # Check the version\n            currentVersion = config[\'General\'].get(\'version\')\n            if currentVersion != self.CONFIG_VERSION:\n                raise UserWarning(\'Present configuration version {0} does not match {1}. You can try manual changes on \\\'{2}\\\'\'.format(currentVersion, self.CONFIG_VERSION, configName))\n\n            # Restoring the the parameters\n            self.globStep = config[\'General\'].getint(\'globStep\')\n            self.args.watsonMode = config[\'General\'].getboolean(\'watsonMode\')\n            self.args.autoEncode = config[\'General\'].getboolean(\'autoEncode\')\n            self.args.corpus = config[\'General\'].get(\'corpus\')\n\n            self.args.datasetTag = config[\'Dataset\'].get(\'datasetTag\')\n            self.args.maxLength = config[\'Dataset\'].getint(\'maxLength\')  # We need to restore the model length because of the textData associated and the vocabulary size (TODO: Compatibility mode between different maxLength)\n            self.args.filterVocab = config[\'Dataset\'].getint(\'filterVocab\')\n            self.args.skipLines = config[\'Dataset\'].getboolean(\'skipLines\')\n            self.args.vocabularySize = config[\'Dataset\'].getint(\'vocabularySize\')\n\n            self.args.hiddenSize = config[\'Network\'].getint(\'hiddenSize\')\n            self.args.numLayers = config[\'Network\'].getint(\'numLayers\')\n            self.args.softmaxSamples = config[\'Network\'].getint(\'softmaxSamples\')\n            self.args.initEmbeddings = config[\'Network\'].getboolean(\'initEmbeddings\')\n            self.args.embeddingSize = config[\'Network\'].getint(\'embeddingSize\')\n            self.args.embeddingSource = config[\'Network\'].get(\'embeddingSource\')\n\n            # No restoring for training params, batch size or other non model dependent parameters\n\n            # Show the restored params\n            print()\n            print(\'Warning: Restoring parameters:\')\n            print(\'globStep: {}\'.format(self.globStep))\n            print(\'watsonMode: {}\'.format(self.args.watsonMode))\n            print(\'autoEncode: {}\'.format(self.args.autoEncode))\n            print(\'corpus: {}\'.format(self.args.corpus))\n            print(\'datasetTag: {}\'.format(self.args.datasetTag))\n            print(\'maxLength: {}\'.format(self.args.maxLength))\n            print(\'filterVocab: {}\'.format(self.args.filterVocab))\n            print(\'skipLines: {}\'.format(self.args.skipLines))\n            print(\'vocabularySize: {}\'.format(self.args.vocabularySize))\n            print(\'hiddenSize: {}\'.format(self.args.hiddenSize))\n            print(\'numLayers: {}\'.format(self.args.numLayers))\n            print(\'softmaxSamples: {}\'.format(self.args.softmaxSamples))\n            print(\'initEmbeddings: {}\'.format(self.args.initEmbeddings))\n            print(\'embeddingSize: {}\'.format(self.args.embeddingSize))\n            print(\'embeddingSource: {}\'.format(self.args.embeddingSource))\n            print()\n\n        # For now, not arbitrary  independent maxLength between encoder and decoder\n        self.args.maxLengthEnco = self.args.maxLength\n        self.args.maxLengthDeco = self.args.maxLength + 2\n\n        if self.args.watsonMode:\n            self.SENTENCES_PREFIX.reverse()\n\n\n    def saveModelParams(self):\n        """""" Save the params of the model, like the current globStep value\n        Warning: if you modify this function, make sure the changes mirror loadModelParams\n        """"""\n        config = configparser.ConfigParser()\n        config[\'General\'] = {}\n        config[\'General\'][\'version\']  = self.CONFIG_VERSION\n        config[\'General\'][\'globStep\']  = str(self.globStep)\n        config[\'General\'][\'watsonMode\'] = str(self.args.watsonMode)\n        config[\'General\'][\'autoEncode\'] = str(self.args.autoEncode)\n        config[\'General\'][\'corpus\'] = str(self.args.corpus)\n\n        config[\'Dataset\'] = {}\n        config[\'Dataset\'][\'datasetTag\'] = str(self.args.datasetTag)\n        config[\'Dataset\'][\'maxLength\'] = str(self.args.maxLength)\n        config[\'Dataset\'][\'filterVocab\'] = str(self.args.filterVocab)\n        config[\'Dataset\'][\'skipLines\'] = str(self.args.skipLines)\n        config[\'Dataset\'][\'vocabularySize\'] = str(self.args.vocabularySize)\n\n        config[\'Network\'] = {}\n        config[\'Network\'][\'hiddenSize\'] = str(self.args.hiddenSize)\n        config[\'Network\'][\'numLayers\'] = str(self.args.numLayers)\n        config[\'Network\'][\'softmaxSamples\'] = str(self.args.softmaxSamples)\n        config[\'Network\'][\'initEmbeddings\'] = str(self.args.initEmbeddings)\n        config[\'Network\'][\'embeddingSize\'] = str(self.args.embeddingSize)\n        config[\'Network\'][\'embeddingSource\'] = str(self.args.embeddingSource)\n\n        # Keep track of the learning params (but without restoring them)\n        config[\'Training (won\\\'t be restored)\'] = {}\n        config[\'Training (won\\\'t be restored)\'][\'learningRate\'] = str(self.args.learningRate)\n        config[\'Training (won\\\'t be restored)\'][\'batchSize\'] = str(self.args.batchSize)\n        config[\'Training (won\\\'t be restored)\'][\'dropout\'] = str(self.args.dropout)\n\n        with open(os.path.join(self.modelDir, self.CONFIG_FILENAME), \'w\') as configFile:\n            config.write(configFile)\n\n    def _getSummaryName(self):\n        """""" Parse the argument to decide were to save the summary, at the same place that the model\n        The folder could already contain logs if we restore the training, those will be merged\n        Return:\n            str: The path and name of the summary\n        """"""\n        return self.modelDir\n\n    def _getModelName(self):\n        """""" Parse the argument to decide were to save/load the model\n        This function is called at each checkpoint and the first time the model is load. If keepAll option is set, the\n        globStep value will be included in the name.\n        Return:\n            str: The path and name were the model need to be saved\n        """"""\n        modelName = os.path.join(self.modelDir, self.MODEL_NAME_BASE)\n        if self.args.keepAll:  # We do not erase the previously saved model by including the current step on the name\n            modelName += \'-\' + str(self.globStep)\n        return modelName + self.MODEL_EXT\n\n    def getDevice(self):\n        """""" Parse the argument to decide on which device run the model\n        Return:\n            str: The name of the device on which run the program\n        """"""\n        if self.args.device == \'cpu\':\n            return \'/cpu:0\'\n        elif self.args.device == \'gpu\':\n            return \'/gpu:0\'\n        elif self.args.device is None:  # No specified device (default)\n            return None\n        else:\n            print(\'Warning: Error in the device name: {}, use the default device\'.format(self.args.device))\n            return None\n'"
chatbot/model.py,29,"b'# Copyright 2015 Conchylicultor. All Rights Reserved.\n# Modifications copyright (C) 2016 Carlos Segura\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nModel to predict the next sentence given an input sequence\n\n""""""\n\nimport tensorflow as tf\n\nfrom chatbot.textdata import Batch\n\n\nclass ProjectionOp:\n    """""" Single layer perceptron\n    Project input tensor on the output dimension\n    """"""\n    def __init__(self, shape, scope=None, dtype=None):\n        """"""\n        Args:\n            shape: a tuple (input dim, output dim)\n            scope (str): encapsulate variables\n            dtype: the weights type\n        """"""\n        assert len(shape) == 2\n\n        self.scope = scope\n\n        # Projection on the keyboard\n        with tf.variable_scope(\'weights_\' + self.scope):\n            self.W_t = tf.get_variable(\n                \'weights\',\n                shape,\n                # initializer=tf.truncated_normal_initializer()  # TODO: Tune value (fct of input size: 1/sqrt(input_dim))\n                dtype=dtype\n            )\n            self.b = tf.get_variable(\n                \'bias\',\n                shape[0],\n                initializer=tf.constant_initializer(),\n                dtype=dtype\n            )\n            self.W = tf.transpose(self.W_t)\n\n    def getWeights(self):\n        """""" Convenience method for some tf arguments\n        """"""\n        return self.W, self.b\n\n    def __call__(self, X):\n        """""" Project the output of the decoder into the vocabulary space\n        Args:\n            X (tf.Tensor): input value\n        """"""\n        with tf.name_scope(self.scope):\n            return tf.matmul(X, self.W) + self.b\n\n\nclass Model:\n    """"""\n    Implementation of a seq2seq model.\n    Architecture:\n        Encoder/decoder\n        2 LTSM layers\n    """"""\n\n    def __init__(self, args, textData):\n        """"""\n        Args:\n            args: parameters of the model\n            textData: the dataset object\n        """"""\n        print(""Model creation..."")\n\n        self.textData = textData  # Keep a reference on the dataset\n        self.args = args  # Keep track of the parameters of the model\n        self.dtype = tf.float32\n\n        # Placeholders\n        self.encoderInputs  = None\n        self.decoderInputs  = None  # Same that decoderTarget plus the <go>\n        self.decoderTargets = None\n        self.decoderWeights = None  # Adjust the learning to the target sentence size\n\n        # Main operators\n        self.lossFct = None\n        self.optOp = None\n        self.outputs = None  # Outputs of the network, list of probability for each words\n\n        # Construct the graphs\n        self.buildNetwork()\n\n    def buildNetwork(self):\n        """""" Create the computational graph\n        """"""\n\n        # TODO: Create name_scopes (for better graph visualisation)\n        # TODO: Use buckets (better perfs)\n\n        # Parameters of sampled softmax (needed for attention mechanism and a large vocabulary size)\n        outputProjection = None\n        # Sampled softmax only makes sense if we sample less than vocabulary size.\n        if 0 < self.args.softmaxSamples < self.textData.getVocabularySize():\n            outputProjection = ProjectionOp(\n                (self.textData.getVocabularySize(), self.args.hiddenSize),\n                scope=\'softmax_projection\',\n                dtype=self.dtype\n            )\n\n            def sampledSoftmax(labels, inputs):\n                labels = tf.reshape(labels, [-1, 1])  # Add one dimension (nb of true classes, here 1)\n\n                # We need to compute the sampled_softmax_loss using 32bit floats to\n                # avoid numerical instabilities.\n                localWt     = tf.cast(outputProjection.W_t,             tf.float32)\n                localB      = tf.cast(outputProjection.b,               tf.float32)\n                localInputs = tf.cast(inputs,                           tf.float32)\n\n                return tf.cast(\n                    tf.nn.sampled_softmax_loss(\n                        localWt,  # Should have shape [num_classes, dim]\n                        localB,\n                        labels,\n                        localInputs,\n                        self.args.softmaxSamples,  # The number of classes to randomly sample per batch\n                        self.textData.getVocabularySize()),  # The number of classes\n                    self.dtype)\n\n        # Creation of the rnn cell\n        def create_rnn_cell():\n            encoDecoCell = tf.contrib.rnn.BasicLSTMCell(  # Or GRUCell, LSTMCell(args.hiddenSize)\n                self.args.hiddenSize,\n            )\n            if not self.args.test:  # TODO: Should use a placeholder instead\n                encoDecoCell = tf.contrib.rnn.DropoutWrapper(\n                    encoDecoCell,\n                    input_keep_prob=1.0,\n                    output_keep_prob=self.args.dropout\n                )\n            return encoDecoCell\n        encoDecoCell = tf.contrib.rnn.MultiRNNCell(\n            [create_rnn_cell() for _ in range(self.args.numLayers)],\n        )\n\n        # Network input (placeholders)\n\n        with tf.name_scope(\'placeholder_encoder\'):\n            self.encoderInputs  = [tf.placeholder(tf.int32,   [None, ]) for _ in range(self.args.maxLengthEnco)]  # Batch size * sequence length * input dim\n\n        with tf.name_scope(\'placeholder_decoder\'):\n            self.decoderInputs  = [tf.placeholder(tf.int32,   [None, ], name=\'inputs\') for _ in range(self.args.maxLengthDeco)]  # Same sentence length for input and output (Right ?)\n            self.decoderTargets = [tf.placeholder(tf.int32,   [None, ], name=\'targets\') for _ in range(self.args.maxLengthDeco)]\n            self.decoderWeights = [tf.placeholder(tf.float32, [None, ], name=\'weights\') for _ in range(self.args.maxLengthDeco)]\n\n        # Define the network\n        # Here we use an embedding model, it takes integer as input and convert them into word vector for\n        # better word representation\n        decoderOutputs, states = tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq(\n            self.encoderInputs,  # List<[batch=?, inputDim=1]>, list of size args.maxLength\n            self.decoderInputs,  # For training, we force the correct output (feed_previous=False)\n            encoDecoCell,\n            self.textData.getVocabularySize(),\n            self.textData.getVocabularySize(),  # Both encoder and decoder have the same number of class\n            embedding_size=self.args.embeddingSize,  # Dimension of each word\n            output_projection=outputProjection.getWeights() if outputProjection else None,\n            feed_previous=bool(self.args.test)  # When we test (self.args.test), we use previous output as next input (feed_previous)\n        )\n\n        # TODO: When the LSTM hidden size is too big, we should project the LSTM output into a smaller space (4086 => 2046): Should speed up\n        # training and reduce memory usage. Other solution, use sampling softmax\n\n        # For testing only\n        if self.args.test:\n            if not outputProjection:\n                self.outputs = decoderOutputs\n            else:\n                self.outputs = [outputProjection(output) for output in decoderOutputs]\n\n            # TODO: Attach a summary to visualize the output\n\n        # For training only\n        else:\n            # Finally, we define the loss function\n            self.lossFct = tf.contrib.legacy_seq2seq.sequence_loss(\n                decoderOutputs,\n                self.decoderTargets,\n                self.decoderWeights,\n                self.textData.getVocabularySize(),\n                softmax_loss_function= sampledSoftmax if outputProjection else None  # If None, use default SoftMax\n            )\n            tf.summary.scalar(\'loss\', self.lossFct)  # Keep track of the cost\n\n            # Initialize the optimizer\n            opt = tf.train.AdamOptimizer(\n                learning_rate=self.args.learningRate,\n                beta1=0.9,\n                beta2=0.999,\n                epsilon=1e-08\n            )\n            self.optOp = opt.minimize(self.lossFct)\n\n    def step(self, batch):\n        """""" Forward/training step operation.\n        Does not perform run on itself but just return the operators to do so. Those have then to be run\n        Args:\n            batch (Batch): Input data on testing mode, input and target on output mode\n        Return:\n            (ops), dict: A tuple of the (training, loss) operators or (outputs,) in testing mode with the associated feed dictionary\n        """"""\n\n        # Feed the dictionary\n        feedDict = {}\n        ops = None\n\n        if not self.args.test:  # Training\n            for i in range(self.args.maxLengthEnco):\n                feedDict[self.encoderInputs[i]]  = batch.encoderSeqs[i]\n            for i in range(self.args.maxLengthDeco):\n                feedDict[self.decoderInputs[i]]  = batch.decoderSeqs[i]\n                feedDict[self.decoderTargets[i]] = batch.targetSeqs[i]\n                feedDict[self.decoderWeights[i]] = batch.weights[i]\n\n            ops = (self.optOp, self.lossFct)\n        else:  # Testing (batchSize == 1)\n            for i in range(self.args.maxLengthEnco):\n                feedDict[self.encoderInputs[i]]  = batch.encoderSeqs[i]\n            feedDict[self.decoderInputs[0]]  = [self.textData.goToken]\n\n            ops = (self.outputs,)\n\n        # Return one pass operator\n        return ops, feedDict\n'"
chatbot/textdata.py,1,"b'# Copyright 2015 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nLoads the dialogue corpus, builds the vocabulary\n""""""\n\nimport numpy as np\nimport nltk  # For tokenize\nfrom tqdm import tqdm  # Progress bar\nimport pickle  # Saving the data\nimport math  # For float comparison\nimport os  # Checking file existance\nimport random\nimport string\nimport collections\n\nfrom chatbot.corpus.cornelldata import CornellData\nfrom chatbot.corpus.opensubsdata import OpensubsData\nfrom chatbot.corpus.scotusdata import ScotusData\nfrom chatbot.corpus.ubuntudata import UbuntuData\nfrom chatbot.corpus.lightweightdata import LightweightData\n\n\nclass Batch:\n    """"""Struct containing batches info\n    """"""\n    def __init__(self):\n        self.encoderSeqs = []\n        self.decoderSeqs = []\n        self.targetSeqs = []\n        self.weights = []\n\n\nclass TextData:\n    """"""Dataset class\n    Warning: No vocabulary limit\n    """"""\n\n    availableCorpus = collections.OrderedDict([  # OrderedDict because the first element is the default choice\n        (\'cornell\', CornellData),\n        (\'opensubs\', OpensubsData),\n        (\'scotus\', ScotusData),\n        (\'ubuntu\', UbuntuData),\n        (\'lightweight\', LightweightData),\n    ])\n\n    @staticmethod\n    def corpusChoices():\n        """"""Return the dataset availables\n        Return:\n            list<string>: the supported corpus\n        """"""\n        return list(TextData.availableCorpus.keys())\n\n    def __init__(self, args):\n        """"""Load all conversations\n        Args:\n            args: parameters of the model\n        """"""\n        # Model parameters\n        self.args = args\n\n        # Path variables\n        self.corpusDir = os.path.join(self.args.rootDir, \'data\', self.args.corpus)\n        basePath = self._constructBasePath()\n        self.fullSamplesPath = basePath + \'.pkl\'  # Full sentences length/vocab\n        self.filteredSamplesPath = basePath + \'-length{}-filter{}-vocabSize{}.pkl\'.format(\n            self.args.maxLength,\n            self.args.filterVocab,\n            self.args.vocabularySize,\n        )  # Sentences/vocab filtered for this model\n\n        self.padToken = -1  # Padding\n        self.goToken = -1  # Start of sequence\n        self.eosToken = -1  # End of sequence\n        self.unknownToken = -1  # Word dropped from vocabulary\n\n        self.trainingSamples = []  # 2d array containing each question and his answer [[input,target]]\n\n        self.word2id = {}\n        self.id2word = {}  # For a rapid conversion (Warning: If replace dict by list, modify the filtering to avoid linear complexity with del)\n        self.idCount = {}  # Useful to filters the words (TODO: Could replace dict by list or use collections.Counter)\n\n        self.loadCorpus()\n\n        # Plot some stats:\n        self._printStats()\n\n        if self.args.playDataset:\n            self.playDataset()\n\n    def _printStats(self):\n        print(\'Loaded {}: {} words, {} QA\'.format(self.args.corpus, len(self.word2id), len(self.trainingSamples)))\n\n    def _constructBasePath(self):\n        """"""Return the name of the base prefix of the current dataset\n        """"""\n        path = os.path.join(self.args.rootDir, \'data\' + os.sep + \'samples\' + os.sep)\n        path += \'dataset-{}\'.format(self.args.corpus)\n        if self.args.datasetTag:\n            path += \'-\' + self.args.datasetTag\n        return path\n\n    def makeLighter(self, ratioDataset):\n        """"""Only keep a small fraction of the dataset, given by the ratio\n        """"""\n        #if not math.isclose(ratioDataset, 1.0):\n        #    self.shuffle()  # Really ?\n        #    print(\'WARNING: Ratio feature not implemented !!!\')\n        pass\n\n    def shuffle(self):\n        """"""Shuffle the training samples\n        """"""\n        print(\'Shuffling the dataset...\')\n        random.shuffle(self.trainingSamples)\n\n    def _createBatch(self, samples):\n        """"""Create a single batch from the list of sample. The batch size is automatically defined by the number of\n        samples given.\n        The inputs should already be inverted. The target should already have <go> and <eos>\n        Warning: This function should not make direct calls to args.batchSize !!!\n        Args:\n            samples (list<Obj>): a list of samples, each sample being on the form [input, target]\n        Return:\n            Batch: a batch object en\n        """"""\n\n        batch = Batch()\n        batchSize = len(samples)\n\n        # Create the batch tensor\n        for i in range(batchSize):\n            # Unpack the sample\n            sample = samples[i]\n            if not self.args.test and self.args.watsonMode:  # Watson mode: invert question and answer\n                sample = list(reversed(sample))\n            if not self.args.test and self.args.autoEncode:  # Autoencode: use either the question or answer for both input and output\n                k = random.randint(0, 1)\n                sample = (sample[k], sample[k])\n            # TODO: Why re-processed that at each epoch ? Could precompute that\n            # once and reuse those every time. Is not the bottleneck so won\'t change\n            # much ? and if preprocessing, should be compatible with autoEncode & cie.\n            batch.encoderSeqs.append(list(reversed(sample[0])))  # Reverse inputs (and not outputs), little trick as defined on the original seq2seq paper\n            batch.decoderSeqs.append([self.goToken] + sample[1] + [self.eosToken])  # Add the <go> and <eos> tokens\n            batch.targetSeqs.append(batch.decoderSeqs[-1][1:])  # Same as decoder, but shifted to the left (ignore the <go>)\n\n            # Long sentences should have been filtered during the dataset creation\n            assert len(batch.encoderSeqs[i]) <= self.args.maxLengthEnco\n            assert len(batch.decoderSeqs[i]) <= self.args.maxLengthDeco\n\n            # TODO: Should use tf batch function to automatically add padding and batch samples\n            # Add padding & define weight\n            batch.encoderSeqs[i]   = [self.padToken] * (self.args.maxLengthEnco  - len(batch.encoderSeqs[i])) + batch.encoderSeqs[i]  # Left padding for the input\n            batch.weights.append([1.0] * len(batch.targetSeqs[i]) + [0.0] * (self.args.maxLengthDeco - len(batch.targetSeqs[i])))\n            batch.decoderSeqs[i] = batch.decoderSeqs[i] + [self.padToken] * (self.args.maxLengthDeco - len(batch.decoderSeqs[i]))\n            batch.targetSeqs[i]  = batch.targetSeqs[i]  + [self.padToken] * (self.args.maxLengthDeco - len(batch.targetSeqs[i]))\n\n        # Simple hack to reshape the batch\n        encoderSeqsT = []  # Corrected orientation\n        for i in range(self.args.maxLengthEnco):\n            encoderSeqT = []\n            for j in range(batchSize):\n                encoderSeqT.append(batch.encoderSeqs[j][i])\n            encoderSeqsT.append(encoderSeqT)\n        batch.encoderSeqs = encoderSeqsT\n\n        decoderSeqsT = []\n        targetSeqsT = []\n        weightsT = []\n        for i in range(self.args.maxLengthDeco):\n            decoderSeqT = []\n            targetSeqT = []\n            weightT = []\n            for j in range(batchSize):\n                decoderSeqT.append(batch.decoderSeqs[j][i])\n                targetSeqT.append(batch.targetSeqs[j][i])\n                weightT.append(batch.weights[j][i])\n            decoderSeqsT.append(decoderSeqT)\n            targetSeqsT.append(targetSeqT)\n            weightsT.append(weightT)\n        batch.decoderSeqs = decoderSeqsT\n        batch.targetSeqs = targetSeqsT\n        batch.weights = weightsT\n\n        # # Debug\n        # self.printBatch(batch)  # Input inverted, padding should be correct\n        # print(self.sequence2str(samples[0][0]))\n        # print(self.sequence2str(samples[0][1]))  # Check we did not modified the original sample\n\n        return batch\n\n    def getBatches(self):\n        """"""Prepare the batches for the current epoch\n        Return:\n            list<Batch>: Get a list of the batches for the next epoch\n        """"""\n        self.shuffle()\n\n        batches = []\n\n        def genNextSamples():\n            """""" Generator over the mini-batch training samples\n            """"""\n            for i in range(0, self.getSampleSize(), self.args.batchSize):\n                yield self.trainingSamples[i:min(i + self.args.batchSize, self.getSampleSize())]\n\n        # TODO: Should replace that by generator (better: by tf.queue)\n\n        for samples in genNextSamples():\n            batch = self._createBatch(samples)\n            batches.append(batch)\n        return batches\n\n    def getSampleSize(self):\n        """"""Return the size of the dataset\n        Return:\n            int: Number of training samples\n        """"""\n        return len(self.trainingSamples)\n\n    def getVocabularySize(self):\n        """"""Return the number of words present in the dataset\n        Return:\n            int: Number of word on the loader corpus\n        """"""\n        return len(self.word2id)\n\n    def loadCorpus(self):\n        """"""Load/create the conversations data\n        """"""\n        datasetExist = os.path.isfile(self.filteredSamplesPath)\n        if not datasetExist:  # First time we load the database: creating all files\n            print(\'Training samples not found. Creating dataset...\')\n\n            datasetExist = os.path.isfile(self.fullSamplesPath)  # Try to construct the dataset from the preprocessed entry\n            if not datasetExist:\n                print(\'Constructing full dataset...\')\n\n                optional = \'\'\n                if self.args.corpus == \'lightweight\':\n                    if not self.args.datasetTag:\n                        raise ValueError(\'Use the --datasetTag to define the lightweight file to use.\')\n                    optional = os.sep + self.args.datasetTag  # HACK: Forward the filename\n\n                # Corpus creation\n                corpusData = TextData.availableCorpus[self.args.corpus](self.corpusDir + optional)\n                self.createFullCorpus(corpusData.getConversations())\n                self.saveDataset(self.fullSamplesPath)\n            else:\n                self.loadDataset(self.fullSamplesPath)\n            self._printStats()\n\n            print(\'Filtering words (vocabSize = {} and wordCount > {})...\'.format(\n                self.args.vocabularySize,\n                self.args.filterVocab\n            ))\n            self.filterFromFull()  # Extract the sub vocabulary for the given maxLength and filterVocab\n\n            # Saving\n            print(\'Saving dataset...\')\n            self.saveDataset(self.filteredSamplesPath)  # Saving tf samples\n        else:\n            self.loadDataset(self.filteredSamplesPath)\n\n        assert self.padToken == 0\n\n    def saveDataset(self, filename):\n        """"""Save samples to file\n        Args:\n            filename (str): pickle filename\n        """"""\n\n        with open(os.path.join(filename), \'wb\') as handle:\n            data = {  # Warning: If adding something here, also modifying loadDataset\n                \'word2id\': self.word2id,\n                \'id2word\': self.id2word,\n                \'idCount\': self.idCount,\n                \'trainingSamples\': self.trainingSamples\n            }\n            pickle.dump(data, handle, -1)  # Using the highest protocol available\n\n    def loadDataset(self, filename):\n        """"""Load samples from file\n        Args:\n            filename (str): pickle filename\n        """"""\n        dataset_path = os.path.join(filename)\n        print(\'Loading dataset from {}\'.format(dataset_path))\n        with open(dataset_path, \'rb\') as handle:\n            data = pickle.load(handle)  # Warning: If adding something here, also modifying saveDataset\n            self.word2id = data[\'word2id\']\n            self.id2word = data[\'id2word\']\n            self.idCount = data.get(\'idCount\', None)\n            self.trainingSamples = data[\'trainingSamples\']\n\n            self.padToken = self.word2id[\'<pad>\']\n            self.goToken = self.word2id[\'<go>\']\n            self.eosToken = self.word2id[\'<eos>\']\n            self.unknownToken = self.word2id[\'<unknown>\']  # Restore special words\n\n    def filterFromFull(self):\n        """""" Load the pre-processed full corpus and filter the vocabulary / sentences\n        to match the given model options\n        """"""\n\n        def mergeSentences(sentences, fromEnd=False):\n            """"""Merge the sentences until the max sentence length is reached\n            Also decrement id count for unused sentences.\n            Args:\n                sentences (list<list<int>>): the list of sentences for the current line\n                fromEnd (bool): Define the question on the answer\n            Return:\n                list<int>: the list of the word ids of the sentence\n            """"""\n            # We add sentence by sentence until we reach the maximum length\n            merged = []\n\n            # If question: we only keep the last sentences\n            # If answer: we only keep the first sentences\n            if fromEnd:\n                sentences = reversed(sentences)\n\n            for sentence in sentences:\n\n                # If the total length is not too big, we still can add one more sentence\n                if len(merged) + len(sentence) <= self.args.maxLength:\n                    if fromEnd:  # Append the sentence\n                        merged = sentence + merged\n                    else:\n                        merged = merged + sentence\n                else:  # If the sentence is not used, neither are the words\n                    for w in sentence:\n                        self.idCount[w] -= 1\n            return merged\n\n        newSamples = []\n\n        # 1st step: Iterate over all words and add filters the sentences\n        # according to the sentence lengths\n        for inputWords, targetWords in tqdm(self.trainingSamples, desc=\'Filter sentences:\', leave=False):\n            inputWords = mergeSentences(inputWords, fromEnd=True)\n            targetWords = mergeSentences(targetWords, fromEnd=False)\n\n            newSamples.append([inputWords, targetWords])\n        words = []\n\n        # WARNING: DO NOT FILTER THE UNKNOWN TOKEN !!! Only word which has count==0 ?\n\n        # 2nd step: filter the unused words and replace them by the unknown token\n        # This is also where we update the correnspondance dictionaries\n        specialTokens = {  # TODO: bad HACK to filter the special tokens. Error prone if one day add new special tokens\n            self.padToken,\n            self.goToken,\n            self.eosToken,\n            self.unknownToken\n        }\n        newMapping = {}  # Map the full words ids to the new one (TODO: Should be a list)\n        newId = 0\n\n        selectedWordIds = collections \\\n            .Counter(self.idCount) \\\n            .most_common(self.args.vocabularySize or None)  # Keep all if vocabularySize == 0\n        selectedWordIds = {k for k, v in selectedWordIds if v > self.args.filterVocab}\n        selectedWordIds |= specialTokens\n\n        for wordId, count in [(i, self.idCount[i]) for i in range(len(self.idCount))]:  # Iterate in order\n            if wordId in selectedWordIds:  # Update the word id\n                newMapping[wordId] = newId\n                word = self.id2word[wordId]  # The new id has changed, update the dictionaries\n                del self.id2word[wordId]  # Will be recreated if newId == wordId\n                self.word2id[word] = newId\n                self.id2word[newId] = word\n                newId += 1\n            else:  # Cadidate to filtering, map it to unknownToken (Warning: don\'t filter special token)\n                newMapping[wordId] = self.unknownToken\n                del self.word2id[self.id2word[wordId]]  # The word isn\'t used anymore\n                del self.id2word[wordId]\n\n        # Last step: replace old ids by new ones and filters empty sentences\n        def replace_words(words):\n            valid = False  # Filter empty sequences\n            for i, w in enumerate(words):\n                words[i] = newMapping[w]\n                if words[i] != self.unknownToken:  # Also filter if only contains unknown tokens\n                    valid = True\n            return valid\n\n        self.trainingSamples.clear()\n\n        for inputWords, targetWords in tqdm(newSamples, desc=\'Replace ids:\', leave=False):\n            valid = True\n            valid &= replace_words(inputWords)\n            valid &= replace_words(targetWords)\n            valid &= targetWords.count(self.unknownToken) == 0  # Filter target with out-of-vocabulary target words ?\n\n            if valid:\n                self.trainingSamples.append([inputWords, targetWords])  # TODO: Could replace list by tuple\n\n        self.idCount.clear()  # Not usefull anymore. Free data\n\n    def createFullCorpus(self, conversations):\n        """"""Extract all data from the given vocabulary.\n        Save the data on disk. Note that the entire corpus is pre-processed\n        without restriction on the sentence length or vocab size.\n        """"""\n        # Add standard tokens\n        self.padToken = self.getWordId(\'<pad>\')  # Padding (Warning: first things to add > id=0 !!)\n        self.goToken = self.getWordId(\'<go>\')  # Start of sequence\n        self.eosToken = self.getWordId(\'<eos>\')  # End of sequence\n        self.unknownToken = self.getWordId(\'<unknown>\')  # Word dropped from vocabulary\n\n        # Preprocessing data\n\n        for conversation in tqdm(conversations, desc=\'Extract conversations\'):\n            self.extractConversation(conversation)\n\n        # The dataset will be saved in the same order it has been extracted\n\n    def extractConversation(self, conversation):\n        """"""Extract the sample lines from the conversations\n        Args:\n            conversation (Obj): a conversation object containing the lines to extract\n        """"""\n\n        if self.args.skipLines:  # WARNING: The dataset won\'t be regenerated if the choice evolve (have to use the datasetTag)\n            step = 2\n        else:\n            step = 1\n\n        # Iterate over all the lines of the conversation\n        for i in tqdm_wrap(\n            range(0, len(conversation[\'lines\']) - 1, step),  # We ignore the last line (no answer for it)\n            desc=\'Conversation\',\n            leave=False\n        ):\n            inputLine  = conversation[\'lines\'][i]\n            targetLine = conversation[\'lines\'][i+1]\n\n            inputWords  = self.extractText(inputLine[\'text\'])\n            targetWords = self.extractText(targetLine[\'text\'])\n\n            if inputWords and targetWords:  # Filter wrong samples (if one of the list is empty)\n                self.trainingSamples.append([inputWords, targetWords])\n\n    def extractText(self, line):\n        """"""Extract the words from a sample lines\n        Args:\n            line (str): a line containing the text to extract\n        Return:\n            list<list<int>>: the list of sentences of word ids of the sentence\n        """"""\n        sentences = []  # List[List[str]]\n\n        # Extract sentences\n        sentencesToken = nltk.sent_tokenize(line)\n\n        # We add sentence by sentence until we reach the maximum length\n        for i in range(len(sentencesToken)):\n            tokens = nltk.word_tokenize(sentencesToken[i])\n\n            tempWords = []\n            for token in tokens:\n                tempWords.append(self.getWordId(token))  # Create the vocabulary and the training sentences\n\n            sentences.append(tempWords)\n\n        return sentences\n\n    def getWordId(self, word, create=True):\n        """"""Get the id of the word (and add it to the dictionary if not existing). If the word does not exist and\n        create is set to False, the function will return the unknownToken value\n        Args:\n            word (str): word to add\n            create (Bool): if True and the word does not exist already, the world will be added\n        Return:\n            int: the id of the word created\n        """"""\n        # Should we Keep only words with more than one occurrence ?\n\n        word = word.lower()  # Ignore case\n\n        # At inference, we simply look up for the word\n        if not create:\n            wordId = self.word2id.get(word, self.unknownToken)\n        # Get the id if the word already exist\n        elif word in self.word2id:\n            wordId = self.word2id[word]\n            self.idCount[wordId] += 1\n        # If not, we create a new entry\n        else:\n            wordId = len(self.word2id)\n            self.word2id[word] = wordId\n            self.id2word[wordId] = word\n            self.idCount[wordId] = 1\n\n        return wordId\n\n    def printBatch(self, batch):\n        """"""Print a complete batch, useful for debugging\n        Args:\n            batch (Batch): a batch object\n        """"""\n        print(\'----- Print batch -----\')\n        for i in range(len(batch.encoderSeqs[0])):  # Batch size\n            print(\'Encoder: {}\'.format(self.batchSeq2str(batch.encoderSeqs, seqId=i)))\n            print(\'Decoder: {}\'.format(self.batchSeq2str(batch.decoderSeqs, seqId=i)))\n            print(\'Targets: {}\'.format(self.batchSeq2str(batch.targetSeqs, seqId=i)))\n            print(\'Weights: {}\'.format(\' \'.join([str(weight) for weight in [batchWeight[i] for batchWeight in batch.weights]])))\n\n    def sequence2str(self, sequence, clean=False, reverse=False):\n        """"""Convert a list of integer into a human readable string\n        Args:\n            sequence (list<int>): the sentence to print\n            clean (Bool): if set, remove the <go>, <pad> and <eos> tokens\n            reverse (Bool): for the input, option to restore the standard order\n        Return:\n            str: the sentence\n        """"""\n\n        if not sequence:\n            return \'\'\n\n        if not clean:\n            return \' \'.join([self.id2word[idx] for idx in sequence])\n\n        sentence = []\n        for wordId in sequence:\n            if wordId == self.eosToken:  # End of generated sentence\n                break\n            elif wordId != self.padToken and wordId != self.goToken:\n                sentence.append(self.id2word[wordId])\n\n        if reverse:  # Reverse means input so no <eos> (otherwise pb with previous early stop)\n            sentence.reverse()\n\n        return self.detokenize(sentence)\n\n    def detokenize(self, tokens):\n        """"""Slightly cleaner version of joining with spaces.\n        Args:\n            tokens (list<string>): the sentence to print\n        Return:\n            str: the sentence\n        """"""\n        return \'\'.join([\n            \' \' + t if not t.startswith(\'\\\'\') and\n                       t not in string.punctuation\n                    else t\n            for t in tokens]).strip().capitalize()\n\n    def batchSeq2str(self, batchSeq, seqId=0, **kwargs):\n        """"""Convert a list of integer into a human readable string.\n        The difference between the previous function is that on a batch object, the values have been reorganized as\n        batch instead of sentence.\n        Args:\n            batchSeq (list<list<int>>): the sentence(s) to print\n            seqId (int): the position of the sequence inside the batch\n            kwargs: the formatting options( See sequence2str() )\n        Return:\n            str: the sentence\n        """"""\n        sequence = []\n        for i in range(len(batchSeq)):  # Sequence length\n            sequence.append(batchSeq[i][seqId])\n        return self.sequence2str(sequence, **kwargs)\n\n    def sentence2enco(self, sentence):\n        """"""Encode a sequence and return a batch as an input for the model\n        Return:\n            Batch: a batch object containing the sentence, or none if something went wrong\n        """"""\n\n        if sentence == \'\':\n            return None\n\n        # First step: Divide the sentence in token\n        tokens = nltk.word_tokenize(sentence)\n        if len(tokens) > self.args.maxLength:\n            return None\n\n        # Second step: Convert the token in word ids\n        wordIds = []\n        for token in tokens:\n            wordIds.append(self.getWordId(token, create=False))  # Create the vocabulary and the training sentences\n\n        # Third step: creating the batch (add padding, reverse)\n        batch = self._createBatch([[wordIds, []]])  # Mono batch, no target output\n\n        return batch\n\n    def deco2sentence(self, decoderOutputs):\n        """"""Decode the output of the decoder and return a human friendly sentence\n        decoderOutputs (list<np.array>):\n        """"""\n        sequence = []\n\n        # Choose the words with the highest prediction score\n        for out in decoderOutputs:\n            sequence.append(np.argmax(out))  # Adding each predicted word ids\n\n        return sequence  # We return the raw sentence. Let the caller do some cleaning eventually\n\n    def playDataset(self):\n        """"""Print a random dialogue from the dataset\n        """"""\n        print(\'Randomly play samples:\')\n        for i in range(self.args.playDataset):\n            idSample = random.randint(0, len(self.trainingSamples) - 1)\n            print(\'Q: {}\'.format(self.sequence2str(self.trainingSamples[idSample][0], clean=True)))\n            print(\'A: {}\'.format(self.sequence2str(self.trainingSamples[idSample][1], clean=True)))\n            print()\n        pass\n\n\ndef tqdm_wrap(iterable, *args, **kwargs):\n    """"""Forward an iterable eventually wrapped around a tqdm decorator\n    The iterable is only wrapped if the iterable contains enough elements\n    Args:\n        iterable (list): An iterable object which define the __len__ method\n        *args, **kwargs: the tqdm parameters\n    Return:\n        iter: The iterable eventually decorated\n    """"""\n    if len(iterable) > 100:\n        return tqdm(iterable, *args, **kwargs)\n    return iterable\n'"
chatbot/trainner.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2015 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nTrain the program by launching it with random parametters\n""""""\n\nfrom tqdm import tqdm\nimport os\n\n\ndef main():\n    """"""\n    Launch the training with different parametters\n    """"""\n\n    # TODO: define:\n    # step+noize\n    # log scale instead of uniform\n\n    # Define parametter: [min, max]\n    dictParams = {\n        ""batchSize"": [int, [1, 3]]\n        ""learningRate"": [float, [1, 3]]\n        }\n\n    # Training multiple times with different parametters\n    for i in range(10):\n        # Generate the command line arguments\n        trainingArgs = """"\n        for keyArg, valueArg in dictParams:\n            value = str(random(valueArg[0], max=valueArg[1]))\n            trainingArgs += "" --"" + keyArg + "" "" + value\n\n        # Launch the program\n        os.run(""main.py"" + trainingArgs)\n\n        # TODO: Save params/results ? or already inside training args ?\n\n\nif __name__ == ""__main__"":\n    main()\n'"
chatbot_website/manage.py,0,"b'#!/usr/bin/env python\nimport os\nimport sys\n\nif __name__ == ""__main__"":\n    os.environ.setdefault(""DJANGO_SETTINGS_MODULE"", ""chatbot_website.settings"")\n    try:\n        from django.core.management import execute_from_command_line\n    except ImportError:\n        # The above import may fail for some other reason. Ensure that the\n        # issue is really that Django is missing to avoid masking other\n        # exceptions on Python 2.\n        try:\n            import django\n        except ImportError:\n            raise ImportError(\n                ""Couldn\'t import Django. Are you sure it\'s installed and ""\n                ""available on your PYTHONPATH environment variable? Did you ""\n                ""forget to activate a virtual environment?""\n            )\n        raise\n    execute_from_command_line(sys.argv)\n'"
chatbot/corpus/cornelldata.py,0,"b'# Copyright 2015 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\nimport ast\n\n""""""\nLoad the cornell movie dialog corpus.\n\nAvailable from here:\nhttp://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\n\n""""""\n\nclass CornellData:\n    """"""\n\n    """"""\n\n    def __init__(self, dirName):\n        """"""\n        Args:\n            dirName (string): directory where to load the corpus\n        """"""\n        self.lines = {}\n        self.conversations = []\n\n        MOVIE_LINES_FIELDS = [""lineID"",""characterID"",""movieID"",""character"",""text""]\n        MOVIE_CONVERSATIONS_FIELDS = [""character1ID"",""character2ID"",""movieID"",""utteranceIDs""]\n\n        self.lines = self.loadLines(os.path.join(dirName, ""movie_lines.txt""), MOVIE_LINES_FIELDS)\n        self.conversations = self.loadConversations(os.path.join(dirName, ""movie_conversations.txt""), MOVIE_CONVERSATIONS_FIELDS)\n\n        # TODO: Cleaner program (merge copy-paste) !!\n\n    def loadLines(self, fileName, fields):\n        """"""\n        Args:\n            fileName (str): file to load\n            field (set<str>): fields to extract\n        Return:\n            dict<dict<str>>: the extracted fields for each line\n        """"""\n        lines = {}\n\n        with open(fileName, \'r\', encoding=\'iso-8859-1\') as f:  # TODO: Solve Iso encoding pb !\n            for line in f:\n                values = line.split("" +++$+++ "")\n\n                # Extract fields\n                lineObj = {}\n                for i, field in enumerate(fields):\n                    lineObj[field] = values[i]\n\n                lines[lineObj[\'lineID\']] = lineObj\n\n        return lines\n\n    def loadConversations(self, fileName, fields):\n        """"""\n        Args:\n            fileName (str): file to load\n            field (set<str>): fields to extract\n        Return:\n            dict<dict<str>>: the extracted fields for each line\n        """"""\n        conversations = []\n\n        with open(fileName, \'r\', encoding=\'iso-8859-1\') as f:  # TODO: Solve Iso encoding pb !\n            for line in f:\n                values = line.split("" +++$+++ "")\n\n                # Extract fields\n                convObj = {}\n                for i, field in enumerate(fields):\n                    convObj[field] = values[i]\n\n                # Convert string to list (convObj[""utteranceIDs""] == ""[\'L598485\', \'L598486\', ...]"")\n                lineIds = ast.literal_eval(convObj[""utteranceIDs""])\n\n                # Reassemble lines\n                convObj[""lines""] = []\n                for lineId in lineIds:\n                    convObj[""lines""].append(self.lines[lineId])\n\n                conversations.append(convObj)\n\n        return conversations\n\n    def getConversations(self):\n        return self.conversations\n'"
chatbot/corpus/lightweightdata.py,0,"b'# Copyright 2015 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\n\n""""""\nLoad data from a dataset of simply-formatted data\n\nfrom A to B\nfrom B to A\nfrom A to B\nfrom B to A\nfrom A to B\n===\nfrom C to D\nfrom D to C\nfrom C to D\nfrom D to C\nfrom C to D\nfrom D to C\n...\n\n`===` lines just separate linear conversations between 2 people.\n\n""""""\n\nclass LightweightData:\n    """"""\n    """"""\n\n    def __init__(self, lightweightFile):\n        """"""\n        Args:\n            lightweightFile (string): file containing our lightweight-formatted corpus\n        """"""\n        self.CONVERSATION_SEP = ""===""\n        self.conversations = []\n        self.loadLines(lightweightFile + \'.txt\')\n\n    def loadLines(self, fileName):\n        """"""\n        Args:\n            fileName (str): file to load\n        """"""\n\n        linesBuffer = []\n        with open(fileName, \'r\') as f:\n            for line in f:\n                l = line.strip()\n                if l == self.CONVERSATION_SEP:\n                    self.conversations.append({""lines"": linesBuffer})\n                    linesBuffer = []\n                else:\n                    linesBuffer.append({""text"": l})\n            if len(linesBuffer):  # Eventually flush the last conversation\n                self.conversations.append({""lines"": linesBuffer})\n\n    def getConversations(self):\n        return self.conversations\n'"
chatbot/corpus/opensubsdata.py,0,"b'# Based on code from https://github.com/AlJohri/OpenSubtitles\n# by Al Johri <al.johri@gmail.com>\n\nimport xml.etree.ElementTree as ET\nimport datetime\nimport os\nimport sys\nimport json\nimport re\nimport pprint\n\nfrom gzip import GzipFile\nfrom tqdm import tqdm\n\n""""""\nLoad the opensubtitles dialog corpus.\n""""""\n\nclass OpensubsData:\n    """"""\n\n    """"""\n\n    def __init__(self, dirName):\n        """"""\n        Args:\n            dirName (string): directory where to load the corpus\n        """"""\n\n        # Hack this to filter on subset of Opensubtitles\n        # dirName = ""%s/en/Action"" % dirName\n\n        print(""Loading OpenSubtitles conversations in %s."" % dirName)\n        self.conversations = []\n        self.tag_re = re.compile(r\'(<!--.*?-->|<[^>]*>)\')\n        self.conversations = self.loadConversations(dirName)\n\n    def loadConversations(self, dirName):\n        """"""\n        Args:\n            dirName (str): folder to load\n        Return:\n            array(question, answer): the extracted QA pairs\n        """"""\n        conversations = []\n        dirList = self.filesInDir(dirName)\n        for filepath in tqdm(dirList, ""OpenSubtitles data files""):\n            if filepath.endswith(\'gz\'):\n                try:\n                    doc = self.getXML(filepath)\n                    conversations.extend(self.genList(doc))\n                except ValueError:\n                    tqdm.write(""Skipping file %s with errors."" % filepath)\n                except:\n                    print(""Unexpected error:"", sys.exc_info()[0])\n                    raise\n        return conversations\n\n    def getConversations(self):\n        return self.conversations\n\n    def genList(self, tree):\n        root = tree.getroot()\n\n        timeFormat = \'%H:%M:%S\'\n        maxDelta = datetime.timedelta(seconds=1)\n\n        startTime = datetime.datetime.min\n        strbuf = \'\'\n        sentList = []\n\n        for child in root:\n            for elem in child:\n                if elem.tag == \'time\':\n                    elemID = elem.attrib[\'id\']\n                    elemVal = elem.attrib[\'value\'][:-4]\n                    if elemID[-1] == \'S\':\n                        startTime = datetime.datetime.strptime(elemVal, timeFormat)\n                    else:\n                        sentList.append((strbuf.strip(), startTime, datetime.datetime.strptime(elemVal, timeFormat)))\n                        strbuf = \'\'\n                else:\n                    try:\n                        strbuf = strbuf + "" "" + elem.text\n                    except:\n                        pass\n\n        conversations = []\n        for idx in range(0, len(sentList) - 1):\n            cur = sentList[idx]\n            nxt = sentList[idx + 1]\n            if nxt[1] - cur[2] <= maxDelta and cur and nxt:\n                tmp = {}\n                tmp[""lines""] = []\n                tmp[""lines""].append(self.getLine(cur[0]))\n                tmp[""lines""].append(self.getLine(nxt[0]))\n                if self.filter(tmp):\n                    conversations.append(tmp)\n\n        return conversations\n\n    def getLine(self, sentence):\n        line = {}\n        line[""text""] = self.tag_re.sub(\'\', sentence).replace(\'\\\\\\\'\',\'\\\'\').strip().lower()\n        return line\n\n    def filter(self, lines):\n        # Use the followint to customize filtering of QA pairs\n        #\n        # startwords = (""what"", ""how"", ""when"", ""why"", ""where"", ""do"", ""did"", ""is"", ""are"", ""can"", ""could"", ""would"", ""will"")\n        # question = lines[""lines""][0][""text""]\n        # if not question.endswith(\'?\'):\n        #     return False\n        # if not question.split(\' \')[0] in startwords:\n        #     return False\n        #\n        return True\n\n    def getXML(self, filepath):\n        fext = os.path.splitext(filepath)[1]\n        if fext == \'.gz\':\n            tmp = GzipFile(filename=filepath)\n            return ET.parse(tmp)\n        else:\n            return ET.parse(filepath)\n\n    def filesInDir(self, dirname):\n        result = []\n        for dirpath, dirs, files in os.walk(dirname):\n            for filename in files:\n                fname = os.path.join(dirpath, filename)\n                result.append(fname)\n        return result\n'"
chatbot/corpus/scotusdata.py,0,"b'# Copyright 2015 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\n\n""""""\nLoad transcripts from the Supreme Court of the USA.\n\nAvailable from here:\nhttps://github.com/pender/chatbot-rnn\n\n""""""\n\nclass ScotusData:\n    """"""\n    """"""\n\n    def __init__(self, dirName):\n        """"""\n        Args:\n            dirName (string): directory where to load the corpus\n        """"""\n        self.lines = self.loadLines(os.path.join(dirName, ""scotus""))\n        self.conversations = [{""lines"": self.lines}]\n\n\n    def loadLines(self, fileName):\n        """"""\n        Args:\n            fileName (str): file to load\n        Return:\n            list<dict<str>>: the extracted fields for each line\n        """"""\n        lines = []\n\n        with open(fileName, \'r\') as f:\n            for line in f:\n                l = line[line.index("":"")+1:].strip()  # Strip name of speaker.\n\n                lines.append({""text"": l})\n\n        return lines\n\n\n    def getConversations(self):\n        return self.conversations\n'"
chatbot/corpus/ubuntudata.py,0,"b'# Copyright 2015 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\n\nfrom tqdm import tqdm\n\n""""""\nUbuntu Dialogue Corpus\n\nhttp://arxiv.org/abs/1506.08909\n\n""""""\n\nclass UbuntuData:\n    """"""\n    """"""\n\n    def __init__(self, dirName):\n        """"""\n        Args:\n            dirName (string): directory where to load the corpus\n        """"""\n        self.MAX_NUMBER_SUBDIR = 10\n        self.conversations = []\n        __dir = os.path.join(dirName, ""dialogs"")\n        number_subdir = 0\n        for sub in tqdm(os.scandir(__dir), desc=""Ubuntu dialogs subfolders"", total=len(os.listdir(__dir))):\n            if number_subdir == self.MAX_NUMBER_SUBDIR:\n                print(""WARNING: Early stoping, only extracting {} directories"".format(self.MAX_NUMBER_SUBDIR))\n                return\n\n            if sub.is_dir():\n                number_subdir += 1\n                for f in os.scandir(sub.path):\n                    if f.name.endswith("".tsv""):\n                        self.conversations.append({""lines"": self.loadLines(f.path)})\n\n\n    def loadLines(self, fileName):\n        """"""\n        Args:\n            fileName (str): file to load\n        Return:\n            list<dict<str>>: the extracted fields for each line\n        """"""\n        lines = []\n        with open(fileName, \'r\') as f:\n            for line in f:\n                l = line[line.rindex(""\\t"")+1:].strip()  # Strip metadata (timestamps, speaker names)\n\n                lines.append({""text"": l})\n\n        return lines\n\n\n    def getConversations(self):\n        return self.conversations\n'"
chatbot_website/chatbot_interface/__init__.py,0,"b""default_app_config = 'chatbot_interface.chatbotmanager.ChatbotManager'\n"""
chatbot_website/chatbot_interface/admin.py,0,b'from django.contrib import admin\n\n# Register your models here.\n'
chatbot_website/chatbot_interface/apps.py,0,"b""from django.apps import AppConfig\n\n\nclass ChatbotInterfaceConfig(AppConfig):\n    name = 'chatbot_interface'\n"""
chatbot_website/chatbot_interface/chatbotmanager.py,0,"b'from django.conf import settings\nimport logging\nimport sys\n\nfrom django.apps import AppConfig\nimport sys\nimport os\n\nchatbotPath = ""/"".join(settings.BASE_DIR.split(\'/\')[:-1])\nsys.path.append(chatbotPath)\nfrom chatbot import chatbot\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass ChatbotManager(AppConfig):\n    """""" Manage a single instance of the chatbot shared over the website\n    """"""\n    name = \'chatbot_interface\'\n    verbose_name = \'Chatbot Interface\'\n\n    bot = None\n\n    def ready(self):\n        """""" Called by Django only once during startup\n        """"""\n        # Initialize the chatbot daemon (should be launched only once)\n        if (os.environ.get(\'RUN_MAIN\') == \'true\' and  # HACK: Avoid the autoreloader executing the startup code twice (could also use: python manage.py runserver --noreload) (see http://stackoverflow.com/questions/28489863/why-is-run-called-twice-in-the-django-dev-server)\n            not any(x in sys.argv for x in [\'makemigrations\', \'migrate\'])):  # HACK: Avoid initialisation while migrate\n            ChatbotManager.initBot()\n\n    @staticmethod\n    def initBot():\n        """""" Instantiate the chatbot for later use\n        Should be called only once\n        """"""\n        if not ChatbotManager.bot:\n            logger.info(\'Initializing bot...\')\n            ChatbotManager.bot = chatbot.Chatbot()\n            ChatbotManager.bot.main([\'--modelTag\', \'server\', \'--test\', \'daemon\', \'--rootDir\', chatbotPath])\n        else:\n            logger.info(\'Bot already initialized.\')\n\n    @staticmethod\n    def callBot(sentence):\n        """""" Use the previously instantiated bot to predict a response to the given sentence\n        Args:\n            sentence (str): the question to answer\n        Return:\n            str: the answer\n        """"""\n        if ChatbotManager.bot:\n            return ChatbotManager.bot.daemonPredict(sentence)\n        else:\n            logger.error(\'Error: Bot not initialized!\')\n'"
chatbot_website/chatbot_interface/consumer.py,0,"b'from channels import Group\nfrom channels.sessions import channel_session\nimport logging\nimport sys\nimport json\n\nfrom .chatbotmanager import ChatbotManager\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef _getClientName(client):\n    """""" Return the unique id for the client\n    Args:\n        client list<>: the client which send the message of the from [ip (str), port (int)]\n    Return:\n        str: the id associated with the client\n    """"""\n    return \'room-\' + client[0] + \'-\' + str(client[1])\n\n\n@channel_session\ndef ws_connect(message):\n    """""" Called when a client try to open a WebSocket\n    Args:\n        message (Obj): object containing the client query\n    """"""\n    if message[\'path\'] == \'/chat\':  # Check we are on the right channel\n        clientName = _getClientName(message[\'client\'])\n        logger.info(\'New client connected: {}\'.format(clientName))\n        Group(clientName).add(message.reply_channel)  # Answer back to the client\n        message.channel_session[\'room\'] = clientName\n        message.reply_channel.send({\'accept\': True})\n\n\n@channel_session\ndef ws_receive(message):\n    """""" Called when a client send a message\n    Args:\n        message (Obj): object containing the client query\n    """"""\n    # Get client info\n    clientName = message.channel_session[\'room\']\n    data = json.loads(message[\'text\'])\n\n    # Compute the prediction\n    question = data[\'message\']\n    try:\n        answer = ChatbotManager.callBot(question)\n    except:  # Catching all possible mistakes\n        logger.error(\'{}: Error with this question {}\'.format(clientName, question))\n        logger.error(""Unexpected error:"", sys.exc_info()[0])\n        answer = \'Error: Internal problem\'\n\n    # Check eventual error\n    if not answer:\n        answer = \'Error: Try a shorter sentence\'\n\n    logger.info(\'{}: {} -> {}\'.format(clientName, question, answer))\n\n    # Send the prediction back\n    Group(clientName).send({\'text\': json.dumps({\'message\': answer})})\n\n@channel_session\ndef ws_disconnect(message):\n    """""" Called when a client disconnect\n    Args:\n        message (Obj): object containing the client query\n    """"""\n    clientName = message.channel_session[\'room\']\n    logger.info(\'Client disconnected: {}\'.format(clientName))\n    Group(clientName).discard(message.reply_channel)\n'"
chatbot_website/chatbot_interface/models.py,0,b'from django.db import models\n\n# Create your models here.\n'
chatbot_website/chatbot_interface/routing.py,0,"b""from . import consumer\n\nchannel_routing = {\n    # TODO: From the original examples, there is more (https://github.com/jacobian/channels-example/)\n    'websocket.connect': consumer.ws_connect,\n    'websocket.receive': consumer.ws_receive,\n    'websocket.disconnect': consumer.ws_disconnect,\n}\n"""
chatbot_website/chatbot_interface/tests.py,0,b'from django.test import TestCase\n\n# Create your tests here.\n'
chatbot_website/chatbot_interface/urls.py,0,"b""from django.conf.urls import url\nfrom . import views\n\nfrom .chatbotmanager import ChatbotManager\n\nurlpatterns = [\n    url(r'^$', views.mainView),\n]\n"""
chatbot_website/chatbot_interface/views.py,0,"b'from django.shortcuts import render\n\ndef mainView(request):\n    """""" Main view which launch and handle the chatbot view\n    Args:\n        request (Obj): django request object\n    """"""\n    return render(request, \'index.html\', {})\n'"
chatbot_website/chatbot_website/__init__.py,0,b''
chatbot_website/chatbot_website/asgi.py,0,"b'""""""\nASGI config for chatbot_website project.\n\nUsed for WebSockets\n""""""\n\nimport os\nimport channels.asgi\n\nos.environ.setdefault(""DJANGO_SETTINGS_MODULE"", ""chatbot_website.settings"")\nchannel_layer = channels.asgi.get_channel_layer()\n'"
chatbot_website/chatbot_website/settings.py,0,"b'""""""\nDjango settings for chatbot_website project.\n\nGenerated by \'django-admin startproject\' using Django 1.10.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/1.10/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/1.10/ref/settings/\n""""""\n\nimport os\n\n# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/1.10/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = os.environ[\'CHATBOT_SECRET_KEY\']\n\n# SECURITY WARNING: don\'t run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = []\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    \'django.contrib.admin\',\n    \'django.contrib.auth\',\n    \'django.contrib.contenttypes\',\n    \'django.contrib.sessions\',\n    \'django.contrib.messages\',\n    \'django.contrib.staticfiles\',\n    \'channels\',\n    \'chatbot_interface\',\n]\n\nMIDDLEWARE = [\n    \'django.middleware.security.SecurityMiddleware\',\n    \'django.contrib.sessions.middleware.SessionMiddleware\',\n    \'django.middleware.common.CommonMiddleware\',\n    \'django.middleware.csrf.CsrfViewMiddleware\',\n    \'django.contrib.auth.middleware.AuthenticationMiddleware\',\n    \'django.contrib.messages.middleware.MessageMiddleware\',\n    \'django.middleware.clickjacking.XFrameOptionsMiddleware\',\n]\n\nROOT_URLCONF = \'chatbot_website.urls\'\n\nTEMPLATES = [\n    {\n        \'BACKEND\': \'django.template.backends.django.DjangoTemplates\',\n        \'DIRS\': [],\n        \'APP_DIRS\': True,\n        \'OPTIONS\': {\n            \'context_processors\': [\n                \'django.template.context_processors.debug\',\n                \'django.template.context_processors.request\',\n                \'django.contrib.auth.context_processors.auth\',\n                \'django.contrib.messages.context_processors.messages\',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = \'chatbot_website.wsgi.application\'\nASGI_APPLICATION = \'chatbot_website.routing.application\'\n\n# Database\n# https://docs.djangoproject.com/en/1.10/ref/settings/#databases\n\nDATABASES = {\n    \'default\': {\n        \'ENGINE\': \'django.db.backends.sqlite3\',\n        \'NAME\': os.path.join(BASE_DIR, \'db.sqlite3\'),\n    }\n}\n\n\n# Password validation\n# https://docs.djangoproject.com/en/1.10/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        \'NAME\': \'django.contrib.auth.password_validation.UserAttributeSimilarityValidator\',\n    },\n    {\n        \'NAME\': \'django.contrib.auth.password_validation.MinimumLengthValidator\',\n    },\n    {\n        \'NAME\': \'django.contrib.auth.password_validation.CommonPasswordValidator\',\n    },\n    {\n        \'NAME\': \'django.contrib.auth.password_validation.NumericPasswordValidator\',\n    },\n]\n\nredis_url = os.environ.get(\'CHATBOT_REDIS_URL\', \'localhost\')\nCHANNEL_LAYERS = {\n    ""default"": {\n        ""BACKEND"": ""asgi_redis.RedisChannelLayer"",\n        ""CONFIG"": {\n            ""hosts"": [os.environ.get(\'REDIS_URL\', \'redis://{}:6379\'.format(redis_url))],\n        },\n        ""ROUTING"": ""chatbot_interface.routing.channel_routing"",\n    },\n}\n\nLOGGING = {\n    \'version\': 1,\n    \'disable_existing_loggers\': False,\n    \'handlers\': {\n        \'file_django\': {\n            \'level\': \'DEBUG\',\n            \'class\': \'logging.FileHandler\',\n            \'filename\': \'logs/debug_django.log\',\n        },\n        \'file_chatbot\': {\n            \'level\': \'DEBUG\',\n            \'class\': \'logging.FileHandler\',\n            \'filename\': \'logs/debug_chatbot.log\',\n        },\n        \'console\': {\n            \'level\': \'DEBUG\',\n            \'class\': \'logging.StreamHandler\',\n            \'stream\': \'ext://sys.stdout\',\n        },\n    },\n    \'loggers\': {\n        \'django\': {\n            \'handlers\': [\'console\', \'file_django\'],\n            \'level\': \'INFO\',\n            \'propagate\': True,\n        },\n        \'chatbot_interface\': {\n            \'handlers\': [\'console\', \'file_chatbot\'],\n            \'level\': os.getenv(\'DJANGO_LOG_LEVEL\', \'INFO\'),\n        },\n    },\n}\n\n# Internationalization\n# https://docs.djangoproject.com/en/1.10/topics/i18n/\n\nLANGUAGE_CODE = \'en-us\'\n\nTIME_ZONE = \'UTC\'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/1.10/howto/static-files/\n\nSTATIC_URL = \'/static/\'\n'"
chatbot_website/chatbot_website/urls.py,0,"b'""""""chatbot_website URL Configuration\n\nThe `urlpatterns` list routes URLs to views. For more information please see:\n    https://docs.djangoproject.com/en/1.10/topics/http/urls/\nExamples:\nFunction views\n    1. Add an import:  from my_app import views\n    2. Add a URL to urlpatterns:  url(r\'^$\', views.home, name=\'home\')\nClass-based views\n    1. Add an import:  from other_app.views import Home\n    2. Add a URL to urlpatterns:  url(r\'^$\', Home.as_view(), name=\'home\')\nIncluding another URLconf\n    1. Import the include() function: from django.conf.urls import url, include\n    2. Add a URL to urlpatterns:  url(r\'^blog/\', include(\'blog.urls\'))\n""""""\nfrom django.conf.urls import include, url\nfrom django.contrib import admin\n\nurlpatterns = [\n    url(r\'^admin/\', admin.site.urls),\n    url(r\'\', include(\'chatbot_interface.urls\')),\n]\n'"
chatbot_website/chatbot_website/wsgi.py,0,"b'""""""\nWSGI config for chatbot_website project.\n\nIt exposes the WSGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/1.10/howto/deployment/wsgi/\n""""""\n\nimport os\n\nfrom django.core.wsgi import get_wsgi_application\n\nos.environ.setdefault(""DJANGO_SETTINGS_MODULE"", ""chatbot_website.settings"")\n\napplication = get_wsgi_application()\n'"
data/embeddings/vec2bin.py,0,"b'#!/usr/bin/python\n\nimport sys\nimport getopt\nimport numpy as np\n\nfrom tqdm import tqdm\n\ninput_path = \'wiki.fr.vec\'\noutput_path = \'wifi.fr.bin\'\n\ndef vec2bin(input_path, output_path):\n    input_fd  = open(input_path, ""rb"")\n    output_fd = open(output_path, ""wb"")\n\n    header = input_fd.readline()\n    output_fd.write(header)\n\n    vocab_size, vector_size = map(int, header.split())\n\n    for line in tqdm(range(vocab_size)):\n        word = []\n        while True:\n            ch = input_fd.read(1)\n            output_fd.write(ch)\n            if ch == b\' \':\n                word = b\'\'.join(word).decode(\'utf-8\')\n                break\n            if ch != b\'\\n\':\n                word.append(ch)\n        vector = np.fromstring(input_fd.readline(), sep=\' \', dtype=\'float32\')\n        output_fd.write(vector.tostring())\n\n    input_fd.close()\n    output_fd.close()\n\n\ndef main(argv):\n   inputfile = False\n   outputfile = False\n   try:\n      opts, args = getopt.getopt(argv,""hi:o:"",[""ifile="",""ofile=""])\n   except getopt.GetoptError:\n      print(\'vec2bin.py -i <inputfile> -o <outputfile>\')\n      sys.exit(2)\n   for opt, arg in opts:\n      if opt == \'-h\':\n         print(\'test.py -i <inputfile> -o <outputfile>\')\n         sys.exit()\n      elif opt in (""-i"", ""--ifile""):\n         inputfile = arg\n      elif opt in (""-o"", ""--ofile""):\n         outputfile = arg\n\n   if not inputfile or not outputfile:\n       print(\'vec2bin.py -i <inputfile> -o <outputfile>\')\n       sys.exit(2)\n\n   print(\'Converting %s to binary file format\' % inputfile)\n   vec2bin(inputfile, outputfile)\n\nif __name__ == ""__main__"":\n   main(sys.argv[1:])\n'"
chatbot_website/chatbot_interface/migrations/__init__.py,0,b''
