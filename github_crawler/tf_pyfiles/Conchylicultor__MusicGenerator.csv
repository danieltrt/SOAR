file_path,api_count,code
main.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2016 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nMain script. See README.md for more information\n\nUse python 3\n""""""\n\nimport deepmusic\n\n\nif __name__ == ""__main__"":\n    composer = deepmusic.Composer()\n    composer.main()\n'"
utils.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2016 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nSome utilities functions, to easily manipulate large volume of downloaded files.\nIndependent of the main program but useful to extract/create the dataset\n""""""\n\nimport os\nimport subprocess\nimport glob\n\n\ndef extract_files():\n    """""" Recursively extract all files from a given directory\n    """"""\n    input_dir = \'../www.chopinmusic.net/\'\n    output_dir = \'chopin_clean/\'\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    print(\'Extracting:\')\n    i = 0\n    for filename in glob.iglob(os.path.join(input_dir, \'**/*.mid\'), recursive=True):\n        print(filename)\n        os.rename(filename, os.path.join(output_dir, os.path.basename(filename)))\n        i += 1\n    print(\'{} files extracted.\'.format(i))\n\n\ndef rename_files():\n    """""" Rename all files of the given directory following some rules\n    """"""\n    input_dir = \'chopin/\'\n    output_dir = \'chopin_clean/\'\n\n    assert os.path.exists(input_dir)\n    os.makedirs(output_dir, exist_ok=True)\n\n    list_files = [f for f in os.listdir(input_dir) if f.endswith(\'.mid\')]\n\n    print(\'Renaming {} files:\'.format(len(list_files)))\n    for prev_name in list_files:\n        new_name = prev_name.replace(\'midi.asp?file=\', \'\')\n        new_name = new_name.replace(\'%2F\', \'_\')\n        print(\'{} -> {}\'.format(prev_name, new_name))\n        os.rename(os.path.join(input_dir, prev_name), os.path.join(output_dir, new_name))\n\n\ndef convert_midi2mp3():\n    """""" Convert all midi files of the given directory to mp3\n    """"""\n    input_dir = \'docs/midi/\'\n    output_dir = \'docs/mp3/\'\n\n    assert os.path.exists(input_dir)\n    os.makedirs(output_dir, exist_ok=True)\n\n    print(\'Converting:\')\n    i = 0\n    for filename in glob.iglob(os.path.join(input_dir, \'**/*.mid\'), recursive=True):\n        print(filename)\n        in_name = filename\n        out_name = os.path.join(output_dir, os.path.splitext(os.path.basename(filename))[0] + \'.mp3\')\n        command = \'timidity {} -Ow -o - | ffmpeg -i - -acodec libmp3lame -ab 64k {}\'.format(in_name, out_name)  # TODO: Redirect stdout to avoid polluting the screen (have cleaner printing)\n        subprocess.call(command, shell=True)\n        i += 1\n    print(\'{} files converted.\'.format(i))\n\n\nif __name__ == \'__main__\':\n    convert_midi2mp3()\n'"
deepmusic/__init__.py,0,"b'__all__ = [""composer""]\n\nfrom deepmusic.composer import Composer\n'"
deepmusic/composer.py,9,"b'# Copyright 2016 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nMusic composer. Act as the coordinator. Orchestrate and call the different models, see the readme for more details.\n\nUse python 3\n""""""\n\nimport argparse  # Command line parsing\nimport configparser  # Saving the models parameters\nimport datetime  # Chronometer\nimport os  # Files management\nfrom tqdm import tqdm  # Progress bar\nimport tensorflow as tf\nimport gc  # Manual garbage collect before each epoch\n\nfrom deepmusic.moduleloader import ModuleLoader\nfrom deepmusic.musicdata import MusicData\nfrom deepmusic.midiconnector import MidiConnector\nfrom deepmusic.imgconnector import ImgConnector\nfrom deepmusic.model import Model\n\n\nclass Composer:\n    """"""\n    Main class which launch the training or testing mode\n    """"""\n\n    class TestMode:\n        """""" Simple structure representing the different testing modes\n        """"""\n        ALL = \'all\'  # The network try to generate a new original composition with all models present (with the tag)\n        DAEMON = \'daemon\'  # Runs on background and can regularly be called to predict something (Not implemented)\n        INTERACTIVE = \'interactive\'  # The user start a melodie and the neural network complete (Not implemented)\n\n        @staticmethod\n        def get_test_modes():\n            """""" Return the list of the different testing modes\n            Useful on when parsing the command lines arguments\n            """"""\n            return [Composer.TestMode.ALL, Composer.TestMode.DAEMON, Composer.TestMode.INTERACTIVE]\n\n    def __init__(self):\n        """"""\n        """"""\n        # Model/dataset parameters\n        self.args = None\n\n        # Task specific objects\n        self.music_data = None  # Dataset\n        self.model = None  # Base model class\n\n        # TensorFlow utilities for convenience saving/logging\n        self.writer = None\n        self.writer_test = None\n        self.saver = None\n        self.model_dir = \'\'  # Where the model is saved\n        self.glob_step = 0  # Represent the number of iteration for the current model\n\n        # TensorFlow main session (we keep track for the daemon)\n        self.sess = None\n\n        # Filename and directories constants\n        self.MODEL_DIR_BASE = \'save/model\'\n        self.MODEL_NAME_BASE = \'model\'\n        self.MODEL_EXT = \'.ckpt\'\n        self.CONFIG_FILENAME = \'params.ini\'\n        self.CONFIG_VERSION = \'0.5\'  # Ensure to raise a warning if there is a change in the format\n\n        self.TRAINING_VISUALIZATION_STEP = 1000  # Plot a training sample every x iterations (Warning: There is a really low probability that on a epoch, it\'s always the same testing bach which is visualized)\n        self.TRAINING_VISUALIZATION_DIR = \'progression\'\n        self.TESTING_VISUALIZATION_DIR = \'midi\'  # Would \'generated\', \'output\' or \'testing\' be a best folder name ?\n\n    @staticmethod\n    def _parse_args(args):\n        """"""\n        Parse the arguments from the given command line\n        Args:\n            args (list<str>): List of arguments to parse. If None, the default sys.argv will be parsed\n        """"""\n\n        parser = argparse.ArgumentParser()\n\n        # Global options\n        global_args = parser.add_argument_group(\'Global options\')\n        global_args.add_argument(\'--test\', nargs=\'?\', choices=Composer.TestMode.get_test_modes(), const=Composer.TestMode.ALL, default=None,\n                                 help=\'if present, launch the program try to answer all sentences from data/test/ with\'\n                                      \' the defined model(s), in interactive mode, the user can wrote his own sentences,\'\n                                      \' use daemon mode to integrate the chatbot in another program\')\n        global_args.add_argument(\'--reset\', action=\'store_true\', help=\'use this if you want to ignore the previous model present on the model directory (Warning: the model will be destroyed with all the folder content)\')\n        global_args.add_argument(\'--keep_all\', action=\'store_true\', help=\'if this option is set, all saved model will be keep (Warning: make sure you have enough free disk space or increase save_every)\')  # TODO: Add an option to delimit the max size\n        global_args.add_argument(\'--model_tag\', type=str, default=None, help=\'tag to differentiate which model to store/load\')\n        global_args.add_argument(\'--sample_length\', type=int, default=40, help=\'number of time units (steps) of a training sentence, length of the sequence to generate\')  # Warning: the unit is defined by the MusicData.MAXIMUM_SONG_RESOLUTION parameter\n        global_args.add_argument(\'--root_dir\', type=str, default=None, help=\'folder where to look for the models and data\')\n        global_args.add_argument(\'--device\', type=str, default=None, help=\'\\\'gpu\\\' or \\\'cpu\\\' (Warning: make sure you have enough free RAM), allow to choose on which hardware run the model\')\n        global_args.add_argument(\'--temperature\', type=float, default=1.0, help=\'Used when testing, control the ouput sampling\')\n\n        # Dataset options\n        dataset_args = parser.add_argument_group(\'Dataset options\')\n        dataset_args.add_argument(\'--dataset_tag\', type=str, default=\'ragtimemusic\', help=\'tag to differentiate which data use (if the data are not present, the program will try to generate from the midi folder)\')\n        dataset_args.add_argument(\'--create_dataset\', action=\'store_true\', help=\'if present, the program will only generate the dataset from the corpus (no training/testing)\')\n        dataset_args.add_argument(\'--play_dataset\', type=int, nargs=\'?\', const=10, default=None,  help=\'if set, the program  will randomly play some samples(can be use conjointly with create_dataset if this is the only action you want to perform)\')  # TODO: Play midi ? / Or show sample images ? Both ?\n        dataset_args.add_argument(\'--ratio_dataset\', type=float, default=0.9, help=\'ratio of songs between training/testing. The ratio is fixed at the beginning and cannot be changed\')\n        ModuleLoader.batch_builders.add_argparse(dataset_args, \'Control the song representation for the inputs of the neural network.\')\n\n        # Network options (Warning: if modifying something here, also make the change on save/restore_params() )\n        nn_args = parser.add_argument_group(\'Network options\', \'architecture related option\')\n        ModuleLoader.enco_cells.add_argparse(nn_args, \'Encoder cell used.\')\n        ModuleLoader.deco_cells.add_argparse(nn_args, \'Decoder cell used.\')\n        nn_args.add_argument(\'--hidden_size\', type=int, default=512, help=\'Size of one neural network layer\')\n        nn_args.add_argument(\'--num_layers\', type=int, default=2, help=\'Nb of layers of the RNN\')\n        nn_args.add_argument(\'--scheduled_sampling\', type=str, nargs=\'+\', default=[Model.ScheduledSamplingPolicy.NONE], help=\'Define the schedule sampling policy. If set, have to indicates the parameters of the chosen policy\')\n        nn_args.add_argument(\'--target_weights\', nargs=\'?\', choices=Model.TargetWeightsPolicy.get_policies(), default=Model.TargetWeightsPolicy.LINEAR,\n                             help=\'policy to choose the loss contribution of each step\')\n        ModuleLoader.loop_processings.add_argparse(nn_args, \'Transformation to apply on each ouput.\')\n\n        # Training options (Warning: if modifying something here, also make the change on save/restore_params() )\n        training_args = parser.add_argument_group(\'Training options\')\n        training_args.add_argument(\'--num_epochs\', type=int, default=0, help=\'maximum number of epochs to run (0 for infinity)\')\n        training_args.add_argument(\'--save_every\', type=int, default=1000, help=\'nb of mini-batch step before creating a model checkpoint\')\n        training_args.add_argument(\'--batch_size\', type=int, default=64, help=\'mini-batch size\')\n        ModuleLoader.learning_rate_policies.add_argparse(training_args, \'Learning rate initial value and decay policy.\')\n        training_args.add_argument(\'--testing_curve\', type=int, default=10, help=\'Also record the testing curve each every x iteration (given by the parameter)\')\n\n        return parser.parse_args(args)\n\n    def main(self, args=None):\n        """"""\n        Launch the training and/or the interactive mode\n        """"""\n        print(\'Welcome to DeepMusic v0.1 !\')\n        print()\n        print(\'TensorFlow detected: v{}\'.format(tf.__version__))\n\n        # General initialisations\n\n        tf.logging.set_verbosity(tf.logging.INFO)  # DEBUG, INFO, WARN (default), ERROR, or FATAL\n\n        ModuleLoader.register_all()  # Load available modules\n        self.args = self._parse_args(args)\n        if not self.args.root_dir:\n            self.args.root_dir = os.getcwd()  # Use the current working directory\n\n        self._restore_params()  # Update the self.model_dir and self.glob_step, for now, not used when loading Model\n        self._print_params()\n\n        self.music_data = MusicData(self.args)\n        if self.args.create_dataset:\n            print(\'Dataset created! You can start training some models.\')\n            return  # No need to go further\n\n        with tf.device(self._get_device()):\n            self.model = Model(self.args)\n\n        # Saver/summaries\n        self.writer = tf.train.SummaryWriter(os.path.join(self.model_dir, \'train\'))\n        self.writer_test = tf.train.SummaryWriter(os.path.join(self.model_dir, \'test\'))\n        self.saver = tf.train.Saver(max_to_keep=200)  # Set the arbitrary limit ?\n\n        # TODO: Fixed seed (WARNING: If dataset shuffling, make sure to do that after saving the\n        # dataset, otherwise, all what comes after the shuffling won\'t be replicable when\n        # reloading the dataset). How to restore the seed after loading ?? (with get_state/set_state)\n        # Also fix seed for np.random (does it works globally for all files ?)\n\n        # Running session\n\n        self.sess = tf.Session()\n\n        print(\'Initialize variables...\')\n        self.sess.run(tf.initialize_all_variables())\n\n        # Reload the model eventually (if it exist), on testing mode, the models are not loaded here (but in main_test())\n        self._restore_previous_model(self.sess)\n\n        if self.args.test:\n            if self.args.test == Composer.TestMode.ALL:\n                self._main_test()\n            elif self.args.test == Composer.TestMode.DAEMON:\n                print(\'Daemon mode, running in background...\')\n                raise NotImplementedError(\'No daemon mode\')  # Come back later\n            else:\n                raise RuntimeError(\'Unknown test mode: {}\'.format(self.args.test))  # Should never happen\n        else:\n            self._main_train()\n\n        if self.args.test != Composer.TestMode.DAEMON:\n            self.sess.close()\n            print(\'The End! Thanks for using this program\')\n\n    def _main_train(self):\n        """""" Training loop\n        """"""\n        assert self.sess\n\n        # Specific training dependent loading (Warning: When restoring a model, we don\'t restore the progression\n        # bar, nor the current batch.)\n\n        merged_summaries = tf.merge_all_summaries()\n        if self.glob_step == 0:  # Not restoring from previous run\n            self.writer.add_graph(self.sess.graph)  # First time only\n\n        print(\'Start training (press Ctrl+C to save and exit)...\')\n\n        try:  # If the user exit while training, we still try to save the model\n            e = 0\n            while self.args.num_epochs == 0 or e < self.args.num_epochs:  # Main training loop (infinite if num_epoch==0)\n                e += 1\n\n                print()\n                print(\'------- Epoch {} (lr={}) -------\'.format(\n                    \'{}/{}\'.format(e, self.args.num_epochs) if self.args.num_epochs else \'{}\'.format(e),\n                    self.model.learning_rate_policy.get_learning_rate(self.glob_step))\n                )\n\n                # Explicit garbage collector call (clear the previous batches)\n                gc.collect()  # TODO: Better memory management (use generators,...)\n\n                batches_train, batches_test = self.music_data.get_batches()\n\n                # Also update learning parameters eventually ?? (Some is done in the model class with the policy classes)\n\n                tic = datetime.datetime.now()\n                for next_batch in tqdm(batches_train, desc=\'Training\'):  # Iterate over the batches\n                    # TODO: Could compute the perfs (time forward pass vs time batch pre-processing)\n                    # Indicate if the output should be computed or not\n                    is_output_visualized = self.glob_step % self.TRAINING_VISUALIZATION_STEP == 0\n\n                    # Training pass\n                    ops, feed_dict = self.model.step(\n                        next_batch,\n                        train_set=True,\n                        glob_step=self.glob_step,\n                        ret_output=is_output_visualized\n                    )\n                    outputs_train = self.sess.run((merged_summaries,) + ops, feed_dict)\n                    self.writer.add_summary(outputs_train[0], self.glob_step)\n\n                    # Testing pass (record the testing curve and visualize some testing predictions)\n                    # TODO: It makes no sense to completely disable the ground truth feeding (it\'s impossible to the\n                    # network to do a good prediction with only the first step)\n                    if is_output_visualized or (self.args.testing_curve and self.glob_step % self.args.testing_curve == 0):\n                        next_batch_test = batches_test[self.glob_step % len(batches_test)]  # Generate test batches in a cycling way (test set smaller than train set)\n                        ops, feed_dict = self.model.step(\n                            next_batch_test,\n                            train_set=False,\n                            ret_output=is_output_visualized\n                        )\n                        outputs_test = self.sess.run((merged_summaries,) + ops, feed_dict)\n                        self.writer_test.add_summary(outputs_test[0], self.glob_step)\n\n                    # Some visualisation (we compute some training/testing samples and compare them to the ground truth)\n                    if is_output_visualized:\n                        visualization_base_name = os.path.join(self.model_dir, self.TRAINING_VISUALIZATION_DIR, str(self.glob_step))\n                        tqdm.write(\'Visualizing: \' + visualization_base_name)\n                        self._visualize_output(\n                            visualization_base_name,\n                            outputs_train[-1],\n                            outputs_test[-1]  # The network output will always be the last operator returned by model.step()\n                        )\n\n                    # Checkpoint\n                    self.glob_step += 1  # Iterate here to avoid saving at the first iteration\n                    if self.glob_step % self.args.save_every == 0:\n                        self._save_session(self.sess)\n\n                toc = datetime.datetime.now()\n\n                print(\'Epoch finished in {}\'.format(toc-tic))  # Warning: Will overflow if an epoch takes more than 24 hours, and the output isn\'t really nicer\n        except (KeyboardInterrupt, SystemExit):  # If the user press Ctrl+C while testing progress\n            print(\'Interruption detected, exiting the program...\')\n\n        self._save_session(self.sess)  # Ultimate saving before complete exit\n\n    def _main_test(self):\n        """""" Generate some songs\n        The midi files will be saved on the same model_dir\n        """"""\n        assert self.sess\n        assert self.args.batch_size == 1\n\n        print(\'Start predicting...\')\n\n        model_list = self._get_model_list()\n        if not model_list:\n            print(\'Warning: No model found in \\\'{}\\\'. Please train a model before trying to predict\'.format(self.model_dir))\n            return\n\n        batches, names = self.music_data.get_batches_test_old()\n        samples = list(zip(batches, names))\n\n        # Predicting for each model present in modelDir\n        for model_name in tqdm(sorted(model_list), desc=\'Model\', unit=\'model\'):  # TODO: Natural sorting / TODO: tqdm ?\n            self.saver.restore(self.sess, model_name)\n\n            for next_sample in tqdm(samples, desc=\'Generating ({})\'.format(os.path.basename(model_name)), unit=\'songs\'):\n                batch = next_sample[0]\n                name = next_sample[1]  # Unzip\n\n                ops, feed_dict = self.model.step(batch)\n                assert len(ops) == 2  # sampling, output\n                chosen_labels, outputs = self.sess.run(ops, feed_dict)\n\n                model_dir, model_filename = os.path.split(model_name)\n                model_dir = os.path.join(model_dir, self.TESTING_VISUALIZATION_DIR)\n                model_filename = model_filename[:-len(self.MODEL_EXT)] + \'-\' + name\n\n                # Save piano roll as image (color map red/blue to see the prediction confidence)\n                # Save the midi file\n                self.music_data.visit_recorder(\n                    outputs,\n                    model_dir,\n                    model_filename,\n                    [ImgConnector, MidiConnector],\n                    chosen_labels=chosen_labels\n                )\n                # TODO: Print song statistics (nb of generated notes, closest songs in dataset ?, try to compute a\n                # score to indicate potentially interesting songs (low score if too repetitive) ?,...). Create new\n                # visited recorder class ?\n                # TODO: Include infos on potentially interesting songs (include metric in the name ?), we should try to detect\n                # the loops, simple metric: nb of generated notes, nb of unique notes (Metric: 2d\n                # tensor [NB_NOTES, nb_of_time_the_note_is played], could plot histogram normalized by nb of\n                # notes). Is piano roll enough ?\n\n        print(\'Prediction finished, {} songs generated\'.format(self.args.batch_size * len(model_list) * len(batches)))\n\n    def _visualize_output(self, visualization_base_name, outputs_train, outputs_test):\n        """""" Record some result/generated songs during training.\n        This allow to see the training progression and get an idea of what the network really learned\n        Args:\n            visualization_base_name (str):\n            outputs_train: Output of the forward pass(training set)\n            outputs_test: Output of the forward pass (testing set)\n        """"""\n        # Record:\n        # * Training/testing:\n        #   * Prediction/ground truth:\n        #     * piano roll\n        #     * midi file\n        # Format name: <glob_step>-<train/test>-<pred/truth>-<mini_batch_id>.<png/mid>\n        # TODO: Also records the ground truth\n\n        model_dir, model_filename = os.path.split(visualization_base_name)\n        for output, set_name in [(outputs_train, \'train\'), (outputs_test, \'test\')]:\n            self.music_data.visit_recorder(\n                output,\n                model_dir,\n                model_filename + \'-\' + set_name,\n                [ImgConnector, MidiConnector]\n            )\n\n    def _restore_previous_model(self, sess):\n        """""" Restore or reset the model, depending of the parameters\n        If testing mode is set, the function has no effect\n        If the destination directory already contains some file, it will handle the conflict as following:\n         * If --reset is set, all present files will be removed (warning: no confirmation is asked) and the training\n         restart from scratch (globStep & cie reinitialized)\n         * Otherwise, it will depend of the directory content. If the directory contains:\n           * No model files (only summary logs): works as a reset (restart from scratch)\n           * Other model files, but model_name not found (surely keep_all option changed): raise error, the user should\n           decide by himself what to do\n           * The right model file (eventually some other): no problem, simply resume the training\n        In any case, the directory will exist as it has been created by the summary writer\n        Args:\n            sess: The current running session\n        """"""\n\n        if self.args.test == Composer.TestMode.ALL:  # On testing, the models are not restored here\n            return\n\n        print(\'WARNING: \', end=\'\')\n\n        model_name = self._get_model_name()\n\n        if os.listdir(self.model_dir):\n            if self.args.reset:\n                print(\'Reset: Destroying previous model at {}\'.format(self.model_dir))\n            # Analysing directory content\n            elif os.path.exists(model_name):  # Restore the model\n                print(\'Restoring previous model from {}\'.format(model_name))\n                self.saver.restore(sess, model_name)  # Will crash when --reset is not activated and the model has not been saved yet\n                print(\'Model restored.\')\n            elif self._get_model_list():\n                print(\'Conflict with previous models.\')\n                raise RuntimeError(\'Some models are already present in \\\'{}\\\'. You should check them first\'.format(self.model_dir))\n            else:  # No other model to conflict with (probably summary files)\n                print(\'No previous model found, but some files/folders found at {}. Cleaning...\'.format(self.model_dir))  # Warning: No confirmation asked\n                self.args.reset = True\n\n            if self.args.reset:\n                # WARNING: No confirmation is asked. All subfolders will be deleted\n                for root, dirs, files in os.walk(self.model_dir, topdown=False):\n                    for name in files:\n                        file_path = os.path.join(root, name)\n                        print(\'Removing {}\'.format(file_path))\n                        os.remove(file_path)\n        else:\n            print(\'No previous model found, starting from clean directory: {}\'.format(self.model_dir))\n\n    def _save_session(self, sess):\n        """""" Save the model parameters and the variables\n        Args:\n            sess: the current session\n        """"""\n        tqdm.write(\'Checkpoint reached: saving model (don\\\'t stop the run)...\')\n        self._save_params()\n        self.saver.save(sess, self._get_model_name())  # Put a limit size (ex: 3GB for the model_dir) ?\n        tqdm.write(\'Model saved.\')\n\n    def _restore_params(self):\n        """""" Load the some values associated with the current model, like the current glob_step value.\n        Needs to be called before any other function because it initialize some variables used on the rest of the\n        program\n\n        Warning: if you modify this function, make sure the changes mirror _save_params, also check if the parameters\n        should be reset in manage_previous_model\n        """"""\n        # Compute the current model path\n        self.model_dir = os.path.join(self.args.root_dir, self.MODEL_DIR_BASE)\n        if self.args.model_tag:\n            self.model_dir += \'-\' + self.args.model_tag\n\n        # If there is a previous model, restore some parameters\n        config_name = os.path.join(self.model_dir, self.CONFIG_FILENAME)\n        if not self.args.reset and not self.args.create_dataset and os.path.exists(config_name):\n            # Loading\n            config = configparser.ConfigParser()\n            config.read(config_name)\n\n            # Check the version\n            current_version = config[\'General\'].get(\'version\')\n            if current_version != self.CONFIG_VERSION:\n                raise UserWarning(\'Present configuration version {0} does not match {1}. You can try manual changes on \\\'{2}\\\'\'.format(current_version, self.CONFIG_VERSION, config_name))\n\n            # Restoring the the parameters\n            self.glob_step = config[\'General\'].getint(\'glob_step\')\n            self.args.keep_all = config[\'General\'].getboolean(\'keep_all\')\n            self.args.dataset_tag = config[\'General\'].get(\'dataset_tag\')\n            if not self.args.test:  # When testing, we don\'t use the training length\n                self.args.sample_length = config[\'General\'].getint(\'sample_length\')\n\n            self.args.hidden_size = config[\'Network\'].getint(\'hidden_size\')\n            self.args.num_layers = config[\'Network\'].getint(\'num_layers\')\n            self.args.target_weights = config[\'Network\'].get(\'target_weights\')\n            self.args.scheduled_sampling = config[\'Network\'].get(\'scheduled_sampling\').split(\' \')\n\n            self.args.batch_size = config[\'Training\'].getint(\'batch_size\')\n            self.args.save_every = config[\'Training\'].getint(\'save_every\')\n            self.args.ratio_dataset = config[\'Training\'].getfloat(\'ratio_dataset\')\n            self.args.testing_curve = config[\'Training\'].getint(\'testing_curve\')\n\n            ModuleLoader.load_all(self.args, config)\n\n            # Show the restored params\n            print(\'Warning: Restoring parameters from previous configuration (you should manually edit the file if you want to change one of those)\')\n\n        # When testing, only predict one song at the time\n        if self.args.test:\n            self.args.batch_size = 1\n            self.args.scheduled_sampling = [Model.ScheduledSamplingPolicy.NONE]\n\n    def _save_params(self):\n        """""" Save the params of the model, like the current glob_step value\n        Warning: if you modify this function, make sure the changes mirror load_params\n        """"""\n        config = configparser.ConfigParser()\n        config[\'General\'] = {}\n        config[\'General\'][\'version\'] = self.CONFIG_VERSION\n        config[\'General\'][\'glob_step\'] = str(self.glob_step)\n        config[\'General\'][\'keep_all\'] = str(self.args.keep_all)\n        config[\'General\'][\'dataset_tag\'] = self.args.dataset_tag\n        config[\'General\'][\'sample_length\'] = str(self.args.sample_length)\n\n        config[\'Network\'] = {}\n        config[\'Network\'][\'hidden_size\'] = str(self.args.hidden_size)\n        config[\'Network\'][\'num_layers\'] = str(self.args.num_layers)\n        config[\'Network\'][\'target_weights\'] = self.args.target_weights  # Could be modified manually\n        config[\'Network\'][\'scheduled_sampling\'] = \' \'.join(self.args.scheduled_sampling)\n\n        # Keep track of the learning params (are not model dependent so can be manually edited)\n        config[\'Training\'] = {}\n        config[\'Training\'][\'batch_size\'] = str(self.args.batch_size)\n        config[\'Training\'][\'save_every\'] = str(self.args.save_every)\n        config[\'Training\'][\'ratio_dataset\'] = str(self.args.ratio_dataset)\n        config[\'Training\'][\'testing_curve\'] = str(self.args.testing_curve)\n\n        # Save the chosen modules and their configuration\n        ModuleLoader.save_all(config)\n\n        with open(os.path.join(self.model_dir, self.CONFIG_FILENAME), \'w\') as config_file:\n            config.write(config_file)\n\n    def _print_params(self):\n        """""" Print the current params\n        """"""\n        print()\n        print(\'Current parameters:\')\n        print(\'glob_step: {}\'.format(self.glob_step))\n        print(\'keep_all: {}\'.format(self.args.keep_all))\n        print(\'dataset_tag: {}\'.format(self.args.dataset_tag))\n        print(\'sample_length: {}\'.format(self.args.sample_length))\n\n        print(\'hidden_size: {}\'.format(self.args.hidden_size))\n        print(\'num_layers: {}\'.format(self.args.num_layers))\n        print(\'target_weights: {}\'.format(self.args.target_weights))\n        print(\'scheduled_sampling: {}\'.format(\' \'.join(self.args.scheduled_sampling)))\n\n        print(\'batch_size: {}\'.format(self.args.batch_size))\n        print(\'save_every: {}\'.format(self.args.save_every))\n        print(\'ratio_dataset: {}\'.format(self.args.ratio_dataset))\n        print(\'testing_curve: {}\'.format(self.args.testing_curve))\n\n        ModuleLoader.print_all(self.args)\n        print()\n\n    def _get_model_name(self):\n        """""" Parse the argument to decide were to save/load the model\n        This function is called at each checkpoint and the first time the model is load. If keep_all option is set, the\n        glob_step value will be included in the name.\n        Return:\n            str: The path and name were the model need to be saved\n        """"""\n        model_name = os.path.join(self.model_dir, self.MODEL_NAME_BASE)\n        if self.args.keep_all:  # We do not erase the previously saved model by including the current step on the name\n            model_name += \'-\' + str(self.glob_step)\n        return model_name + self.MODEL_EXT\n\n    def _get_model_list(self):\n        """""" Return the list of the model files inside the model directory\n        """"""\n        return [os.path.join(self.model_dir, f) for f in os.listdir(self.model_dir) if f.endswith(self.MODEL_EXT)]\n\n    def _get_device(self):\n        """""" Parse the argument to decide on which device run the model\n        Return:\n            str: The name of the device on which run the program\n        """"""\n        if self.args.device == \'cpu\':\n            return \'""/cpu:0\'\n        elif self.args.device == \'gpu\':  # Won\'t work in case of multiple GPUs\n            return \'/gpu:0\'\n        elif self.args.device is None:  # No specified device (default)\n            return None\n        else:\n            print(\'Warning: Error in the device name: {}, use the default device\'.format(self.args.device))\n            return None\n'"
deepmusic/imgconnector.py,0,"b'# Copyright 2016 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nImage connector interface\n\n""""""\n\nimport cv2 as cv\nimport numpy as np\n\nimport deepmusic.songstruct as music  # Should we use that to tuncate the top and bottom image ?\n\n\nclass ImgConnector:\n    """""" Class to read and write songs (piano roll arrays) as images\n    """"""\n\n    @staticmethod\n    def load_file(filename):\n        """""" Extract data from midi file\n        Args:\n            filename (str): a valid img file\n        Return:\n            np.array: the piano roll associated with the\n        """"""\n        # TODO ? Could be useful to load initiators created with Gimp (more intuitive than the current version)\n\n    @staticmethod\n    def write_song(piano_roll, filename):\n        """""" Save the song on disk\n        Args:\n            piano_roll (np.array): a song object containing the tracks and melody\n            filename (str): the path were to save the song (don\'t add the file extension)\n        """"""\n        note_played = piano_roll > 0.5\n        piano_roll_int = np.uint8(piano_roll*255)\n\n        b = piano_roll_int * (~note_played).astype(np.uint8)  # Note silenced\n        g = np.zeros(piano_roll_int.shape, dtype=np.uint8)    # Empty channel\n        r = piano_roll_int * note_played.astype(np.uint8)     # Notes played\n\n        img = cv.merge((b, g, r))\n\n        # TODO: We could insert a first column indicating the piano keys (black/white key)\n\n        cv.imwrite(filename + \'.png\', img)\n\n    @staticmethod\n    def get_input_type():\n        return \'array\'\n'"
deepmusic/keyboardcell.py,5,"b'# Copyright 2016 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nMain cell which predict the next keyboard configuration\n\n""""""\n\nimport collections\nimport tensorflow as tf\n\nfrom deepmusic.moduleloader import ModuleLoader\nimport deepmusic.songstruct as music\n\n\nclass KeyboardCell(tf.contrib.rnn.RNNCell):\n    """""" Cell which wrap the encoder/decoder network\n    """"""\n\n    def __init__(self, args):\n        self.args = args\n        self.is_init = False\n\n        # Get the chosen enco/deco\n        self.encoder = ModuleLoader.enco_cells.build_module(self.args)\n        self.decoder = ModuleLoader.deco_cells.build_module(self.args)\n\n    @property\n    def state_size(self):\n        raise NotImplementedError(\'Abstract method\')\n\n    @property\n    def output_size(self):\n        raise NotImplementedError(\'Abstract method\')\n\n    def __call__(self, prev_keyboard, prev_state, scope=None):\n        """""" Run the cell at step t\n        Args:\n            prev_keyboard: keyboard configuration for the step t-1 (Ground truth or previous step)\n            prev_state: a tuple (prev_state_enco, prev_state_deco)\n            scope: TensorFlow scope\n        Return:\n            Tuple: the keyboard configuration and the enco and deco states\n        """"""\n\n        # First time only (we do the initialisation here to be on the global rnn loop scope)\n        if not self.is_init:\n            with tf.variable_scope(\'weights_keyboard_cell\'):\n                # TODO: With self.args, see which network we have chosen (create map \'network name\':class)\n                self.encoder.build()\n                self.decoder.build()\n\n                prev_state = self.encoder.init_state(), self.decoder.init_state()\n                self.is_init = True\n\n        # TODO: If encoder act as VAE, we should sample here, from the previous state\n\n        # Encoder/decoder network\n        with tf.variable_scope(scope or type(self).__name__):\n            with tf.variable_scope(\'Encoder\'):\n                # TODO: Should be enco_output, enco_state\n                next_state_enco = self.encoder.get_cell(prev_keyboard, prev_state)\n            with tf.variable_scope(\'Decoder\'):  # Reset gate and update gate.\n                next_keyboard, next_state_deco = self.decoder.get_cell(prev_keyboard, (next_state_enco, prev_state[1]))\n        return next_keyboard, (next_state_enco, next_state_deco)\n'"
deepmusic/midiconnector.py,0,"b'# Copyright 2016 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nMid-level interface for the python files\n""""""\n\nimport mido  # Midi lib\n\nimport deepmusic.songstruct as music\n\n\nclass MidiInvalidException(Exception):\n    pass\n\n\nclass MidiConnector:\n    """""" Class which manage the midi files at the message level\n    """"""\n    META_INFO_TYPES = [  # Can safely be ignored\n        \'midi_port\',\n        \'track_name\',\n        \'lyrics\',\n        \'end_of_track\',\n        \'copyright\',\n        \'marker\',\n        \'text\'\n    ]\n    META_TEMPO_TYPES = [  # Have an impact on how the song is played\n        \'key_signature\',\n        \'set_tempo\',\n        \'time_signature\'\n    ]\n\n    MINIMUM_TRACK_LENGTH = 4  # Bellow this value, the track will be ignored\n\n    MIDI_CHANNEL_DRUMS = 10  # The channel reserved for the drums (according to the specs)\n\n    # Define a max song length ?\n\n    #self.resolution = 0  # bpm\n    #self.initial_tempo = 0.0\n\n    #self.data = None  # Sparse tensor of size [NB_KEYS,nb_bars*BAR_DIVISION] or simply a list of note ?\n\n    @staticmethod\n    def load_file(filename):\n        """""" Extract data from midi file\n        Args:\n            filename (str): a valid midi file\n        Return:\n            Song: a song object containing the tracks and melody\n        """"""\n        # Load in the MIDI data using the midi module\n        midi_data = mido.MidiFile(filename)\n\n        # Get header values\n\n        # 3 midi types:\n        # * type 0 (single track): all messages are saved in one multi-channel track\n        # * type 1 (synchronous): all tracks start at the same time\n        # * type 2 (asynchronous): each track is independent of the others\n\n        # Division (ticks per beat notes or SMTPE timecode)\n        # If negative (first byte=1), the mode is SMTPE timecode (unsupported)\n        # 1 MIDI clock = 1 beat = 1 quarter note\n\n        # Assert\n        if midi_data.type != 1:\n            raise MidiInvalidException(\'Only type 1 supported ({} given)\'.format(midi_data.type))\n        if not 0 < midi_data.ticks_per_beat < 128:\n            raise MidiInvalidException(\'SMTPE timecode not supported ({} given)\'.format(midi_data.ticks_per_beat))\n\n        # TODO: Support at least for type 0\n\n        # Get tracks messages\n\n        # The tracks are a mix of meta messages, which determine the tempo and signature, and note messages, which\n        # correspond to the melodie.\n        # Generally, the meta event are set at the beginning of each tracks. In format 1, these meta-events should be\n        # contained in the first track (known as \'Tempo Map\').\n\n        # If not set, default parameters are:\n        #  * time signature: 4/4\n        #  * tempo: 120 beats per minute\n\n        # Each event contain begins by a delta time value, which correspond to the number of ticks from the previous\n        # event (0 for simultaneous event)\n\n        tempo_map = midi_data.tracks[0]  # Will contains the tick scales\n        # TODO: smpte_offset\n\n        # Warning: The drums are filtered\n\n        # Merge tracks ? < Not when creating the dataset\n        #midi_data.tracks = [mido.merge_tracks(midi_data.tracks)] ??\n\n        new_song = music.Song()\n\n        new_song.ticks_per_beat = midi_data.ticks_per_beat\n\n        # TODO: Normalize the ticks per beats (same for all songs)\n\n        for message in tempo_map:\n            # TODO: Check we are only 4/4 (and there is no tempo changes ?)\n            if not isinstance(message, mido.MetaMessage):\n                raise MidiInvalidException(\'Tempo map should not contains notes\')\n            if message.type in MidiConnector.META_INFO_TYPES:\n                pass\n            elif message.type == \'set_tempo\':\n                new_song.tempo_map.append(message)\n            elif message.type in MidiConnector.META_TEMPO_TYPES:  # We ignore the key signature and time_signature ?\n                pass\n            elif message.type == \'smpte_offset\':\n                pass  # TODO\n            else:\n                err_msg = \'Header track contains unsupported meta-message type ({})\'.format(message.type)\n                raise MidiInvalidException(err_msg)\n\n        for i, track in enumerate(midi_data.tracks[1:]):  # We ignore the tempo map\n            i += 1  # Warning: We have skipped the track 0 so shift the track id\n            #tqdm.write(\'Track {}: {}\'.format(i, track.name))\n\n            new_track = music.Track()\n\n            buffer_notes = []  # Store the current notes (pressed but not released)\n            abs_tick = 0  # Absolute nb of ticks from the beginning of the track\n            for message in track:\n                abs_tick += message.time\n                if isinstance(message, mido.MetaMessage):  # Lyrics, track name and other meta info\n                    if message.type in MidiConnector.META_INFO_TYPES:\n                        pass\n                    elif message.type in MidiConnector.META_TEMPO_TYPES:\n                        # TODO: Could be just a warning\n                        raise MidiInvalidException(\'Track {} should not contain {}\'.format(i, message.type))\n                    else:\n                        err_msg = \'Track {} contains unsupported meta-message type ({})\'.format(i, message.type)\n                        raise MidiInvalidException(err_msg)\n                    # What about \'sequence_number\', cue_marker ???\n                else:  # Note event\n                    if message.type == \'note_on\' and message.velocity != 0:  # Note added\n                        new_note = music.Note()\n                        new_note.tick = abs_tick\n                        new_note.note = message.note\n                        if message.channel+1 != i and message.channel+1 != MidiConnector.MIDI_CHANNEL_DRUMS:  # Warning: Mido shift the channels (start at 0) # TODO: Channel management for type 0\n                            raise MidiInvalidException(\'Notes belong to the wrong tracks ({} instead of {})\'.format(i, message.channel))  # Warning: May not be an error (drums ?) but probably\n                        buffer_notes.append(new_note)\n                    elif message.type == \'note_off\' or message.type == \'note_on\':  # Note released\n                        for note in buffer_notes:\n                            if note.note == message.note:\n                                note.duration = abs_tick - note.tick\n                                buffer_notes.remove(note)\n                                new_track.notes.append(note)\n                    elif message.type == \'program_change\':  # Instrument change\n                        if not new_track.set_instrument(message):\n                            # TODO: We should create another track with the new instrument\n                            raise MidiInvalidException(\'Track {} as already a program defined\'.format(i))\n                        pass\n                    elif message.type == \'control_change\':  # Damper pedal, mono/poly, channel volume,...\n                        # Ignored\n                        pass\n                    elif message.type == \'aftertouch\':  # Signal send after a key has been press. What real effect ?\n                        # Ignored ?\n                        pass\n                    elif message.type == \'pitchwheel\':  # Modulate the song\n                        # Ignored\n                        pass\n                    else:\n                        err_msg = \'Track {} contains unsupported message type ({})\'.format(i, message)\n                        raise MidiInvalidException(err_msg)\n                # Message read\n            # Track read\n\n            # Assert\n            if buffer_notes:  # All notes should have ended\n                raise MidiInvalidException(\'Some notes ({}) did not ended\'.format(len(buffer_notes)))\n            if len(new_track.notes) < MidiConnector.MINIMUM_TRACK_LENGTH:\n                #tqdm.write(\'Track {} ignored (too short): {} notes\'.format(i, len(new_track.notes)))\n                continue\n            if new_track.is_drum:\n                #tqdm.write(\'Track {} ignored (is drum)\'.format(i))\n                continue\n\n            new_song.tracks.append(new_track)\n        # All track read\n\n        if not new_song.tracks:\n            raise MidiInvalidException(\'Empty song. No track added\')\n\n        return new_song\n\n    @staticmethod\n    def write_song(song, filename):\n        """""" Save the song on disk\n        Args:\n            song (Song): a song object containing the tracks and melody\n            filename (str): the path were to save the song (don\'t add the file extension)\n        """"""\n\n        midi_data = mido.MidiFile(ticks_per_beat=song.ticks_per_beat)\n\n        # Define track 0\n        new_track = mido.MidiTrack()\n        midi_data.tracks.append(new_track)\n        new_track.extend(song.tempo_map)\n\n        for i, track in enumerate(song.tracks):\n            # Define the track\n            new_track = mido.MidiTrack()\n            midi_data.tracks.append(new_track)\n            new_track.append(mido.Message(\'program_change\', program=0, time=0))  # Played with standard piano\n\n            messages = []\n            for note in track.notes:\n                # Add all messages in absolute time\n                messages.append(mido.Message(\n                    \'note_on\',\n                    note=note.note,  # WARNING: The note should be int (NOT np.int64)\n                    velocity=64,\n                    channel=i,\n                    time=note.tick))\n                messages.append(mido.Message(\n                    \'note_off\',\n                    note=note.note,\n                    velocity=64,\n                    channel=i,\n                    time=note.tick+note.duration)\n                )\n\n            # Reorder the messages chronologically\n            messages.sort(key=lambda x: x.time)\n\n            # Convert absolute tick in relative tick\n            last_time = 0\n            for message in messages:\n                message.time -= last_time\n                last_time += message.time\n\n                new_track.append(message)\n\n        midi_data.save(filename + \'.mid\')\n\n    @staticmethod\n    def get_input_type():\n        return \'song\'\n'"
deepmusic/model.py,19,"b'# Copyright 2016 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nModel to generate new songs\n\n""""""\n\nimport numpy as np  # To generate random numbers\nimport tensorflow as tf\n\nfrom deepmusic.moduleloader import ModuleLoader\nfrom deepmusic.keyboardcell import KeyboardCell\nimport deepmusic.songstruct as music\n\n\nclass Model:\n    """"""\n    Base class which manage the different models and experimentation.\n    """"""\n\n    class TargetWeightsPolicy:\n        """""" Structure to represent the different policy for choosing the target weights\n        This is used to scale the contribution of each timestep to the global loss\n        """"""\n        NONE = \'none\'  # All weights equals (=1.0) (default behavior)\n        LINEAR = \'linear\'  # The first outputs are less penalized than the last ones\n        STEP = \'step\'  # We start penalizing only after x steps (enco/deco behavior)\n\n        def __init__(self, args):\n            """"""\n            Args:\n                args: parameters of the model\n            """"""\n            self.args = args\n\n        def get_weight(self, i):\n            """""" Return the target weight for the given step i using the chosen policy\n            """"""\n            if not self.args.target_weights or self.args.target_weights == Model.TargetWeightsPolicy.NONE:\n                return 1.0\n            elif self.args.target_weights == Model.TargetWeightsPolicy.LINEAR:\n                return i / (self.args.sample_length - 1)  # Gradually increment the loss weight\n            elif self.args.target_weights == Model.TargetWeightsPolicy.STEP:\n                raise NotImplementedError(\'Step target weight policy not implemented yet, please consider another policy\')\n            else:\n                raise ValueError(\'Unknown chosen target weight policy: {}\'.format(self.args.target_weights))\n\n        @staticmethod\n        def get_policies():\n            """""" Return the list of the different modes\n            Useful when parsing the command lines arguments\n            """"""\n            return [\n                Model.TargetWeightsPolicy.NONE,\n                Model.TargetWeightsPolicy.LINEAR,\n                Model.TargetWeightsPolicy.STEP\n            ]\n\n    class ScheduledSamplingPolicy:\n        """""" Container for the schedule sampling policy\n        See http://arxiv.org/abs/1506.03099 for more details\n        """"""\n        NONE = \'none\'  # No scheduled sampling (always take the given input)\n        ALWAYS = \'always\'  # Always samples from the predicted output\n        LINEAR = \'linear\'  # Gradually increase the sampling rate\n\n        def __init__(self, args):\n            self.sampling_policy_fct = None\n\n            assert args.scheduled_sampling\n            assert len(args.scheduled_sampling) > 0\n\n            policy = args.scheduled_sampling[0]\n            if policy == Model.ScheduledSamplingPolicy.NONE:\n                self.sampling_policy_fct = lambda step: 1.0\n            elif policy == Model.ScheduledSamplingPolicy.ALWAYS:\n                self.sampling_policy_fct = lambda step: 0.0\n            elif policy == Model.ScheduledSamplingPolicy.LINEAR:\n                if len(args.scheduled_sampling) != 5:\n                    raise ValueError(\'Not the right arguments for the sampling linear policy ({} instead of 4)\'.format(len(args.scheduled_sampling)-1))\n\n                start_step = int(args.scheduled_sampling[1])\n                end_step = int(args.scheduled_sampling[2])\n                start_value = float(args.scheduled_sampling[3])\n                end_value = float(args.scheduled_sampling[4])\n\n                if (start_step >= end_step or\n                   not (0.0 <= start_value <= 1.0) or\n                   not (0.0 <= end_value <= 1.0)):\n                    raise ValueError(\'Some schedule sampling parameters incorrect.\')\n\n                # TODO: Add default values (as optional arguments)\n\n                def linear_policy(step):\n                    if step < start_step:\n                        threshold = start_value\n                    elif start_step <= step < end_step:\n                        slope = (start_value-end_value)/(start_step-end_step)  # < 0 (because end_step>start_step and start_value>end_value)\n                        threshold = slope*(step-start_step) + start_value\n                    elif end_step <= step:\n                        threshold = end_value\n                    else:\n                        raise RuntimeError(\'Invalid value for the sampling policy\')  # Parameters have not been correctly defined!\n                    assert 0.0 <= threshold <= 1.0\n                    return threshold\n\n                self.sampling_policy_fct = linear_policy\n            else:\n                raise ValueError(\'Unknown chosen schedule sampling policy: {}\'.format(policy))\n\n        def get_prev_threshold(self, glob_step, i=0):\n            """""" Return the previous sampling probability for the current step.\n            If above, the RNN should use the previous step instead of the given input.\n            Args:\n                glob_step (int): the global iteration step for the training\n                i (int): the timestep of the RNN (TODO: implement incrementive slope (progression like -\\|), remove the \'=0\')\n            """"""\n            return self.sampling_policy_fct(glob_step)\n\n    def __init__(self, args):\n        """"""\n        Args:\n            args: parameters of the model\n        """"""\n        print(\'Model creation...\')\n\n        self.args = args  # Keep track of the parameters of the model\n\n        # Placeholders\n        self.inputs = None\n        self.targets = None\n        self.use_prev = None  # Boolean tensor which say at Graph evaluation time if we use the input placeholder or the previous output.\n        self.current_learning_rate = None  # Allow to have a dynamic learning rate\n\n        # Main operators\n        self.opt_op = None  # Optimizer\n        self.outputs = None  # Outputs of the network\n        self.final_state = None  # When testing, we feed this value as initial state ?\n\n        # Other options\n        self.target_weights_policy = None\n        self.schedule_policy = None\n        self.learning_rate_policy = None\n        self.loop_processing = None\n\n        # Construct the graphs\n        self._build_network()\n\n    def _build_network(self):\n        """""" Create the computational graph\n        """"""\n        input_dim = ModuleLoader.batch_builders.get_module().get_input_dim()\n\n        # Placeholders (Use tf.SparseTensor with training=False instead) (TODO: Try restoring dynamic batch_size)\n        with tf.name_scope(\'placeholder_inputs\'):\n            self.inputs = [\n                tf.placeholder(\n                    tf.float32,  # -1.0/1.0 ? Probably better for the sigmoid\n                    [self.args.batch_size, input_dim],  # TODO: Get input size from batch_builder\n                    name=\'input\')\n                for _ in range(self.args.sample_length)\n                ]\n        with tf.name_scope(\'placeholder_targets\'):\n            self.targets = [\n                tf.placeholder(\n                    tf.int32,  # 0/1  # TODO: Int for sofmax, Float for sigmoid\n                    [self.args.batch_size,],  # TODO: For softmax, only 1d, for sigmoid, 2d (batch_size, num_class)\n                    name=\'target\')\n                for _ in range(self.args.sample_length)\n                ]\n        with tf.name_scope(\'placeholder_use_prev\'):\n            self.use_prev = [\n                tf.placeholder(\n                    tf.bool,\n                    [],\n                    name=\'use_prev\')\n                for _ in range(self.args.sample_length)  # The first value will never be used (always takes self.input for the first step)\n                ]\n\n        # Define the network\n        self.loop_processing = ModuleLoader.loop_processings.build_module(self.args)\n        def loop_rnn(prev, i):\n            """""" Loop function used to connect one output of the rnn to the next input.\n            The previous input and returned value have to be from the same shape.\n            This is useful to use the same network for both training and testing.\n            Args:\n                prev: the previous predicted keyboard configuration at step i-1\n                i: the current step id (Warning: start at 1, 0 is ignored)\n            Return:\n                tf.Tensor: the input at the step i\n            """"""\n            next_input = self.loop_processing(prev)\n\n            # On training, we force the correct input, on testing, we use the previous output as next input\n            return tf.cond(self.use_prev[i], lambda: next_input, lambda: self.inputs[i])\n\n        # TODO: Try attention decoder/use dynamic_rnn instead\n        self.outputs, self.final_state = tf.nn.seq2seq.rnn_decoder(\n            decoder_inputs=self.inputs,\n            initial_state=None,  # The initial state is defined inside KeyboardCell\n            cell=KeyboardCell(self.args),\n            loop_function=loop_rnn\n        )\n\n        # For training only\n        if not self.args.test:\n            # Finally, we define the loss function\n\n            # The network will predict a mix a wrong and right notes. For the loss function, we would like to\n            # penalize note which are wrong. Eventually, the penalty should be less if the network predict the same\n            # note but not in the right pitch (ex: C4 instead of C5), with a decay the further the prediction\n            # is (D5 and D1 more penalized than D4 and D3 if the target is D2)\n\n            # For the piano roll mode, by using sigmoid_cross_entropy_with_logits, the task is formulated as a NB_NOTES binary\n            # classification problems\n\n            # For the relative note experiment, it use a standard SoftMax where the label is the relative position to the previous\n            # note\n\n            self.schedule_policy = Model.ScheduledSamplingPolicy(self.args)\n            self.target_weights_policy = Model.TargetWeightsPolicy(self.args)\n            self.learning_rate_policy = ModuleLoader.learning_rate_policies.build_module(self.args)  # Load the chosen policies\n\n            # TODO: If train on different length, check that the loss is proportional to the length or average ???\n            loss_fct = tf.nn.seq2seq.sequence_loss(\n                self.outputs,\n                self.targets,\n                [tf.constant(self.target_weights_policy.get_weight(i), shape=self.targets[0].get_shape()) for i in range(len(self.targets))],  # Weights\n                #softmax_loss_function=tf.nn.softmax_cross_entropy_with_logits,  # Previous: tf.nn.sigmoid_cross_entropy_with_logits TODO: Use option to choose. (new module ?)\n                average_across_timesteps=True,  # Before: I think it\'s best for variables length sequences (specially with the target weights=0), isn\'t it (it implies also that short sequences are less penalized than long ones) ? (TODO: For variables length sequences, be careful about the target weights)\n                average_across_batch=True  # Before: Penalize by sample (should allows dynamic batch size) Warning: need to tune the learning rate\n            )\n            tf.scalar_summary(\'training_loss\', loss_fct)  # Keep track of the cost\n\n            self.current_learning_rate = tf.placeholder(tf.float32, [])\n\n            # Initialize the optimizer\n            opt = tf.train.AdamOptimizer(\n                learning_rate=self.current_learning_rate,\n                beta1=0.9,\n                beta2=0.999,\n                epsilon=1e-08\n            )\n\n            # TODO: Also keep track of magnitudes (how much is updated)\n            self.opt_op = opt.minimize(loss_fct)\n\n    def step(self, batch, train_set=True, glob_step=-1, ret_output=False):\n        """""" Forward/training step operation.\n        Does not perform run on itself but just return the operators to do so. Those have then to be run by the\n        main program.\n        If the output operator is returned, it will always be the last one on the list\n        Args:\n            batch (Batch): Input data on testing mode, input and target on output mode\n            train_set (Bool): indicate if the batch come from the test/train set (not used when generating)\n            glob_step (int): indicate the global step for the schedule sampling\n            ret_output (Bool): for the training mode, if true,\n        Return:\n            Tuple[ops], dict: The list of the operators to run (training_step or outputs) with the associated feed dictionary\n        """"""\n        # TODO: Could optimize feeding between train/test/generating (compress code)\n\n        feed_dict = {}\n        ops = ()  # For small length, it seems (from my investigations) that tuples are faster than list for merging\n        batch.generate(target=False if self.args.test else True)\n\n        # Feed placeholders and choose the ops\n        if not self.args.test:  # Training\n            if train_set:  # We update the learning rate every x iterations # TODO: What happens when we don\'t feed the learning rate ??? Stays at the last value ?\n                assert glob_step >= 0\n                feed_dict[self.current_learning_rate] = self.learning_rate_policy.get_learning_rate(glob_step)\n\n            for i in range(self.args.sample_length):\n                feed_dict[self.inputs[i]] = batch.inputs[i]\n                feed_dict[self.targets[i]] = batch.targets[i]\n                #if np.random.rand() >= self.schedule_policy.get_prev_threshold(glob_step)*self.target_weights_policy.get_weight(i):  # Regular Schedule sample (TODO: Try sampling with the weigths or a mix of weights/sampling)\n                if np.random.rand() >= self.schedule_policy.get_prev_threshold(glob_step):  # Weight the threshold by the target weights (don\'t schedule sample if weight=0)\n                    feed_dict[self.use_prev[i]] = True\n                else:\n                    feed_dict[self.use_prev[i]] = False\n\n            if train_set:\n                ops += (self.opt_op,)\n            if ret_output:\n                ops += (self.outputs,)\n        else:  # Generating (batch_size == 1)\n            # TODO: What to put for initialisation state (empty ? random ?) ?\n            # TODO: Modify use_prev\n            for i in range(self.args.sample_length):\n                if i < len(batch.inputs):\n                    feed_dict[self.inputs[i]] = batch.inputs[i]\n                    feed_dict[self.use_prev[i]] = False\n                else:  # Even not used, we still need to feed a placeholder\n                    feed_dict[self.inputs[i]] = batch.inputs[0]  # Could be anything but we need it to be from the right shape\n                    feed_dict[self.use_prev[i]] = True  # When we don\'t have an input, we use the previous output instead\n\n            ops += (self.loop_processing.get_op(), self.outputs,)  # The loop_processing operator correspond to the recorded softmax sampled\n\n        # Return one pass operator\n        return ops, feed_dict\n'"
deepmusic/model_old.py,31,"b'# Copyright 2016 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nModel to generate new songs\n\n""""""\n\nimport numpy as np  # To generate random numbers\nimport tensorflow as tf\n\nfrom deepmusic.musicdata import Batch\nimport deepmusic.songstruct as music\n\n\nclass Model:\n    """"""\n    Base class which manage the different models and experimentation.\n    """"""\n\n    class TargetWeightsPolicy:\n        """""" Structure to represent the different policy for choosing the target weights\n        This is used to scale the contribution of each timestep to the global loss\n        """"""\n        NONE = \'none\'  # All weights equals (=1.0) (default behavior)\n        LINEAR = \'linear\'  # The first outputs are less penalized than the last ones\n        STEP = \'step\'  # We start penalizing only after x steps (enco/deco behavior)\n\n        def __init__(self, args):\n            """"""\n            Args:\n                args: parameters of the model\n            """"""\n            self.args = args\n\n        def get_weight(self, i):\n            """""" Return the target weight for the given step i using the chosen policy\n            """"""\n            if not self.args.target_weights or self.args.target_weights == Model.TargetWeightsPolicy.NONE:\n                return 1.0\n            elif self.args.target_weights == Model.TargetWeightsPolicy.LINEAR:\n                return i / (self.args.sample_length - 1)  # Gradually increment the loss weight\n            elif self.args.target_weights == Model.TargetWeightsPolicy.STEP:\n                raise NotImplementedError(\'Step target weight policy not implemented yet, please consider another policy\')\n            else:\n                raise ValueError(\'Unknown chosen target weight policy: {}\'.format(self.args.target_weights))\n\n        @staticmethod\n        def get_policies():\n            """""" Return the list of the different modes\n            Useful when parsing the command lines arguments\n            """"""\n            return [\n                Model.TargetWeightsPolicy.NONE,\n                Model.TargetWeightsPolicy.LINEAR,\n                Model.TargetWeightsPolicy.STEP\n            ]\n\n    class ScheduledSamplingPolicy:\n        """""" Container for the schedule sampling policy\n        See http://arxiv.org/abs/1506.03099 for more details\n        """"""\n        NONE = \'none\'  # No scheduled sampling (always take the given input)\n        ALWAYS = \'always\'  # Always samples from the predicted output\n        LINEAR = \'linear\'  # Gradually increase the sampling rate\n\n        def __init__(self, args):\n            self.sampling_policy_fct = None\n\n            assert args.scheduled_sampling\n            assert len(args.scheduled_sampling) > 0\n\n            policy = args.scheduled_sampling[0]\n            if policy == Model.ScheduledSamplingPolicy.NONE:\n                self.sampling_policy_fct = lambda step: 1.0\n            elif policy == Model.ScheduledSamplingPolicy.ALWAYS:\n                self.sampling_policy_fct = lambda step: 0.0\n            elif policy == Model.ScheduledSamplingPolicy.LINEAR:\n                if len(args.scheduled_sampling) != 5:\n                    raise ValueError(\'Not the right arguments for the sampling linear policy ({} instead of 4)\'.format(len(args.scheduled_sampling)-1))\n\n                start_step = int(args.scheduled_sampling[1])\n                end_step = int(args.scheduled_sampling[2])\n                start_value = float(args.scheduled_sampling[3])\n                end_value = float(args.scheduled_sampling[4])\n\n                if (start_step >= end_step or\n                   not (0.0 <= start_value <= 1.0) or\n                   not (0.0 <= end_value <= 1.0)):\n                    raise ValueError(\'Some schedule sampling parameters incorrect.\')\n\n                # TODO: Add default values (as optional arguments)\n\n                def linear_policy(step):\n                    if step < start_step:\n                        threshold = start_value\n                    elif start_step <= step < end_step:\n                        slope = (start_value-end_value)/(start_step-end_step)  # < 0 (because end_step>start_step and start_value>end_value)\n                        threshold = slope*(step-start_step) + start_value\n                    elif end_step <= step:\n                        threshold = end_value\n                    else:\n                        raise RuntimeError(\'Invalid value for the sampling policy\')  # Parameters have not been correctly defined!\n                    assert 0.0 <= threshold <= 1.0\n                    return threshold\n\n                self.sampling_policy_fct = linear_policy\n            else:\n                raise ValueError(\'Unknown chosen schedule sampling policy: {}\'.format(policy))\n\n        def get_prev_threshold(self, glob_step, i=0):\n            """""" Return the previous sampling probability for the current step.\n            If above, the RNN should use the previous step instead of the given input.\n            Args:\n                glob_step (int): the global iteration step for the training\n                i (int): the timestep of the RNN (TODO: implement incrementive slope (progression like -\\|), remove the \'=0\')\n            """"""\n            return self.sampling_policy_fct(glob_step)\n\n    class LearningRatePolicy:\n        """""" Contains the different policies for the learning rate decay\n        """"""\n        CST = \'cst\'  # Fixed learning rate over all steps (default behavior)\n        STEP = \'step\'  # We divide the learning rate every x iterations\n        EXPONENTIAL = \'exponential\'  #\n\n        @staticmethod\n        def get_policies():\n            """""" Return the list of the different modes\n            Useful when parsing the command lines arguments\n            """"""\n            return [\n                Model.LearningRatePolicy.CST,\n                Model.LearningRatePolicy.STEP,\n                Model.LearningRatePolicy.EXPONENTIAL\n            ]\n\n        def __init__(self, args):\n            """"""\n            Args:\n                args: parameters of the model\n            """"""\n            self.learning_rate_fct = None\n\n            assert args.learning_rate\n            assert len(args.learning_rate) > 0\n\n            policy = args.learning_rate[0]\n\n            if policy == Model.LearningRatePolicy.CST:\n                if not len(args.learning_rate) == 2:\n                    raise ValueError(\'Learning rate cst policy should be on the form: {} lr_value\'.format(Model.LearningRatePolicy.CST))\n                self.learning_rate_init = float(args.learning_rate[1])\n                self.learning_rate_fct = self._lr_cst\n\n            elif policy == Model.LearningRatePolicy.STEP:\n                if not len(args.learning_rate) == 3:\n                    raise ValueError(\'Learning rate step policy should be on the form: {} lr_init decay_period\'.format(Model.LearningRatePolicy.STEP))\n                self.learning_rate_init = float(args.learning_rate[1])\n                self.decay_period = int(args.learning_rate[2])\n                self.learning_rate_fct = self._lr_step\n\n            elif policy == Model.LearningRatePolicy.EXPONENTIAL:\n                raise NotImplementedError(\'Exponential learning rate policy not implemented yet, please consider another policy\')\n\n            else:\n                raise ValueError(\'Unknown chosen learning rate policy: {}\'.format(policy))\n\n        def _lr_cst(self, glob_step):\n            """""" Just a constant learning rate\n            """"""\n            return self.learning_rate_init\n\n        def _lr_step(self, glob_step):\n            """""" Every decay period, the learning rate is divided by 2\n            """"""\n            return self.learning_rate_init / 2**(glob_step//self.decay_period)\n\n        def get_learning_rate(self, glob_step):\n            """""" Return the learning rate associated at the current training step\n            Args:\n                glob_step (int): Number of iterations since the beginning of training\n            Return:\n                float: the learning rate at the given step\n            """"""\n            return self.learning_rate_fct(glob_step)\n\n    def __init__(self, args):\n        """"""\n        Args:\n            args: parameters of the model\n        """"""\n        print(\'Model creation...\')\n\n        self.args = args  # Keep track of the parameters of the model\n\n        # Placeholders\n        self.inputs = None\n        self.targets = None\n        self.use_prev = None  # Boolean tensor which say at Graph evaluation time if we use the input placeholder or the previous output.\n        self.current_learning_rate = None  # Allow to have a dynamic learning rate\n\n        # Main operators\n        self.opt_op = None  # Optimizer\n        self.outputs = None  # Outputs of the network\n        self.final_state = None  # When testing, we feed this value as initial state ?\n\n        # Other options\n        self.target_weights_policy = None\n        self.schedule_policy = None\n        self.learning_rate_policy = None\n\n        # Construct the graphs\n        self._build_network()\n\n    def _build_network(self):\n        """""" Create the computational graph\n        """"""\n\n        # Placeholders (Use tf.SparseTensor with training=False instead) (TODO: Try restoring dynamic batch_size)\n        with tf.name_scope(\'placeholder_inputs\'):\n            self.inputs = [\n                tf.placeholder(\n                    tf.float32,  # -1.0/1.0 ? Probably better for the sigmoid\n                    [self.args.batch_size, music.NB_NOTES],\n                    name=\'input\')\n                for _ in range(self.args.sample_length)\n                ]\n        with tf.name_scope(\'placeholder_targets\'):\n            self.targets = [\n                tf.placeholder(\n                    tf.float32,  # 0/1\n                    [self.args.batch_size, music.NB_NOTES],\n                    name=\'target\')\n                for _ in range(self.args.sample_length)\n                ]\n        with tf.name_scope(\'placeholder_use_prev\'):\n            self.use_prev = [\n                tf.placeholder(\n                    tf.bool,\n                    [],\n                    name=\'use_prev\')\n                for _ in range(self.args.sample_length)  # The first value will never be used (always takes self.input for the first step)\n                ]\n\n        # Projection on the keyboard\n        with tf.name_scope(\'note_projection_weights\'):\n            W = tf.Variable(\n                tf.truncated_normal([self.args.hidden_size, music.NB_NOTES]),\n                name=\'weights\'\n            )\n            b = tf.Variable(\n                tf.truncated_normal([music.NB_NOTES]),  # Tune the initializer ?\n                name=\'bias\',\n            )\n\n        def project_note(X):\n            with tf.name_scope(\'note_projection\'):\n                return tf.matmul(X, W) + b  # [batch_size, NB_NOTE]\n\n        # RNN network\n        rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(self.args.hidden_size, state_is_tuple=True)  # Or GRUCell, LSTMCell(args.hidden_size)\n        #rnn_cell = tf.nn.rnn_cell.DropoutWrapper(rnn_cell, input_keep_prob=1.0, output_keep_prob=1.0)  # TODO: Custom values (WARNING: No dropout when testing !!!, possible to use placeholder ?)\n        rnn_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell] * self.args.num_layers, state_is_tuple=True)\n\n        initial_state = rnn_cell.zero_state(batch_size=self.args.batch_size, dtype=tf.float32)\n\n        def loop_rnn(prev, i):\n            """""" Loop function used to connect one output of the rnn to the next input.\n            Will re-adapt the output shape to the input one.\n            This is useful to use the same network for both training and testing. Warning: Because of the fixed\n            batch size, we have to predict batch_size sequences when testing.\n            """"""\n            # Predict the output from prev and scale the result on [-1, 1]\n            next_input = project_note(prev)\n            next_input = tf.sub(tf.mul(2.0, tf.nn.sigmoid(next_input)), 1.0)  # x_{i} = 2*sigmoid(y_{i-1}) - 1\n\n            # On training, we force the correct input, on testing, we use the previous output as next input\n            return tf.cond(self.use_prev[i], lambda: next_input, lambda: self.inputs[i])\n\n        (outputs, self.final_state) = tf.nn.seq2seq.rnn_decoder(\n            decoder_inputs=self.inputs,\n            initial_state=initial_state,\n            cell=rnn_cell,\n            loop_function=loop_rnn\n        )\n\n        # Final projection\n        with tf.name_scope(\'final_output\'):\n            self.outputs = []\n            for output in outputs:\n                proj = project_note(output)\n                self.outputs.append(proj)\n\n        # For training only\n        if not self.args.test:\n            # Finally, we define the loss function\n\n            # The network will predict a mix a wrong and right notes. For the loss function, we would like to\n            # penalize note which are wrong. Eventually, the penalty should be less if the network predict the same\n            # note but not in the right pitch (ex: C4 instead of C5), with a decay the further the prediction\n            # is (D5 and D1 more penalized than D4 and D3 if the target is D2)\n\n            # For now, by using sigmoid_cross_entropy_with_logits, the task is formulated as a NB_NOTES binary\n            # classification problems\n\n            self.schedule_policy = Model.ScheduledSamplingPolicy(self.args)\n            self.target_weights_policy = Model.TargetWeightsPolicy(self.args)\n            self.learning_rate_policy = Model.LearningRatePolicy(self.args)  # Load the chosen policies\n\n            # TODO: If train on different length, check that the loss is proportional to the length or average ???\n            loss_fct = tf.nn.seq2seq.sequence_loss(\n                self.outputs,\n                self.targets,\n                [tf.constant(self.target_weights_policy.get_weight(i), shape=self.targets[0].get_shape()) for i in range(len(self.targets))],  # Weights\n                softmax_loss_function=tf.nn.sigmoid_cross_entropy_with_logits,\n                average_across_timesteps=False,  # I think it\'s best for variables length sequences (specially with the target weights=0), isn\'t it (it implies also that short sequences are less penalized than long ones) ? (TODO: For variables length sequences, be careful about the target weights)\n                average_across_batch=False  # Penalize by sample (should allows dynamic batch size) Warning: need to tune the learning rate\n            )\n            tf.scalar_summary(\'training_loss\', loss_fct)  # Keep track of the cost\n\n            self.current_learning_rate = tf.placeholder(tf.float32, [])\n\n            # Initialize the optimizer\n            opt = tf.train.AdamOptimizer(\n                learning_rate=self.current_learning_rate,\n                beta1=0.9,\n                beta2=0.999,\n                epsilon=1e-08\n            )\n\n            # TODO: Also keep track of magnitudes (how much is updated)\n            self.opt_op = opt.minimize(loss_fct)\n\n    def step(self, batch, train_set=True, glob_step=-1, ret_output=False):\n        """""" Forward/training step operation.\n        Does not perform run on itself but just return the operators to do so. Those have then to be run by the\n        main program.\n        If the output operator is returned, it will always be the last one on the list\n        Args:\n            batch (Batch): Input data on testing mode, input and target on output mode\n            train_set (Bool): indicate if the batch come from the test/train set\n            glob_step (int): indicate the global step for the schedule sampling\n            ret_output (Bool): for the training mode, if true,\n        Return:\n            Tuple[ops], dict: The list of the operators to run (training_step or outputs) with the associated feed dictionary\n        """"""\n        # TODO: Could optimize feeding between train/test/generating (compress code)\n\n        # Feed the dictionary\n        feed_dict = {}\n        ops = ()  # For small length, it seems (from my investigations) that tuples are faster than list for merging\n\n        # Feed placeholders and choose the ops\n        if not self.args.test:  # Training\n            if train_set:\n                assert glob_step >= 0\n                feed_dict[self.current_learning_rate] = self.learning_rate_policy.get_learning_rate(glob_step)\n\n            for i in range(self.args.sample_length):\n                feed_dict[self.inputs[i]] = batch.inputs[i]\n                feed_dict[self.targets[i]] = batch.targets[i]\n                #if not train_set or np.random.rand() > self.schedule_policy.get_prev_threshold(glob_step)*self.target_weights_policy.get_weight(i):  # Regular Schedule sample (TODO: Try sampling with the weigths or a mix of weights/sampling)\n                if np.random.rand() > self.schedule_policy.get_prev_threshold(glob_step):  # Weight the threshold by the target weights (don\'t schedule sample if weight=0)\n                    feed_dict[self.use_prev[i]] = True\n                else:\n                    feed_dict[self.use_prev[i]] = False\n\n            if train_set:\n                ops += (self.opt_op,)\n            if ret_output:\n                ops += (self.outputs,)\n        else:  # Generating (batch_size == 1)\n            # TODO: What to put for initialisation state (empty ? random ?) ?\n            # TODO: Modify use_prev\n            for i in range(self.args.sample_length):\n                if i < len(batch.inputs):\n                    feed_dict[self.inputs[i]] = batch.inputs[i]\n                    feed_dict[self.use_prev[i]] = False\n                else:  # Even not used, we still need to feed a placeholder\n                    feed_dict[self.inputs[i]] = batch.inputs[0]  # Could be anything but we need it to be from the right shape\n                    feed_dict[self.use_prev[i]] = True  # When we don\'t have an input, we use the previous output instead\n\n            ops += (self.outputs,)\n\n        # Return one pass operator\n        return ops, feed_dict\n'"
deepmusic/moduleloader.py,0,"b'# Copyright 2016 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n"""""" Register all available modules\nAll new module should be added here\n""""""\n\nfrom deepmusic.modulemanager import ModuleManager\n\nfrom deepmusic.modules import batchbuilder\nfrom deepmusic.modules import learningratepolicy\nfrom deepmusic.modules import encoder\nfrom deepmusic.modules import decoder\nfrom deepmusic.modules import loopprocessing\n\n\nclass ModuleLoader:\n    """""" Global module manager, synchronize the loading, printing, parsing of\n    all modules.\n    The modules are then instantiated and use in their respective class\n    """"""\n    enco_cells = None\n    deco_cells = None\n    batch_builders = None\n    learning_rate_policies = None\n    loop_processings = None\n\n    @staticmethod\n    def register_all():\n        """""" List all available modules for the current session\n        This function should be called only once at the beginning of the\n        program, before parsing the command lines arguments\n        It doesn\'t instantiate anything here (just notify the program).\n        The module manager name will define the command line flag\n        which will be used\n        """"""\n        ModuleLoader.batch_builders = ModuleManager(\'batch_builder\')\n        ModuleLoader.batch_builders.register(batchbuilder.Relative)\n        ModuleLoader.batch_builders.register(batchbuilder.PianoRoll)\n\n        ModuleLoader.learning_rate_policies = ModuleManager(\'learning_rate\')\n        ModuleLoader.learning_rate_policies.register(learningratepolicy.Cst)\n        ModuleLoader.learning_rate_policies.register(learningratepolicy.StepsWithDecay)\n        ModuleLoader.learning_rate_policies.register(learningratepolicy.Adaptive)\n\n        ModuleLoader.enco_cells = ModuleManager(\'enco_cell\')\n        ModuleLoader.enco_cells.register(encoder.Identity)\n        ModuleLoader.enco_cells.register(encoder.Rnn)\n        ModuleLoader.enco_cells.register(encoder.Embedding)\n\n        ModuleLoader.deco_cells = ModuleManager(\'deco_cell\')\n        ModuleLoader.deco_cells.register(decoder.Lstm)\n        ModuleLoader.deco_cells.register(decoder.Perceptron)\n        ModuleLoader.deco_cells.register(decoder.Rnn)\n\n        ModuleLoader.loop_processings = ModuleManager(\'loop_processing\')\n        ModuleLoader.loop_processings.register(loopprocessing.SampleSoftmax)\n        ModuleLoader.loop_processings.register(loopprocessing.ActivateScale)\n\n    @staticmethod\n    def save_all(config):\n        """""" Save the modules configurations\n        """"""\n        config[\'Modules\'] = {}\n        ModuleLoader.batch_builders.save(config[\'Modules\'])\n        ModuleLoader.learning_rate_policies.save(config[\'Modules\'])\n        ModuleLoader.enco_cells.save(config[\'Modules\'])\n        ModuleLoader.deco_cells.save(config[\'Modules\'])\n        ModuleLoader.loop_processings.save(config[\'Modules\'])\n\n    @staticmethod\n    def load_all(args, config):\n        """""" Restore the module configuration\n        """"""\n        ModuleLoader.batch_builders.load(args, config[\'Modules\'])\n        ModuleLoader.learning_rate_policies.load(args, config[\'Modules\'])\n        ModuleLoader.enco_cells.load(args, config[\'Modules\'])\n        ModuleLoader.deco_cells.load(args, config[\'Modules\'])\n        ModuleLoader.loop_processings.load(args, config[\'Modules\'])\n\n    @staticmethod\n    def print_all(args):\n        """""" Print modules current configuration\n        """"""\n        ModuleLoader.batch_builders.print(args)\n        ModuleLoader.learning_rate_policies.print(args)\n        ModuleLoader.enco_cells.print(args)\n        ModuleLoader.deco_cells.print(args)\n        ModuleLoader.loop_processings.print(args)\n'"
deepmusic/modulemanager.py,0,"b'# Copyright 2016 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n"""""" Module manager class definition\n""""""\n\nfrom collections import OrderedDict\n\n\nclass ModuleManager:\n    """""" Class which manage modules\n    A module can be any class, as long as it implement the static method\n    get_module_id and has a compatible constructor. The role of the module\n    manager is to ensure that only one of the registered classes are used\n    on the program.\n    The first added module will be the one used by default.\n    For now the modules can\'t save their state.\n    """"""\n    def __init__(self, name):\n        """"""\n        Args:\n            name (str): the name of the module manager (useful for saving/printing)\n        """"""\n        self.name = name\n        self.modules = OrderedDict()  # The order on which the modules are added is conserved\n        self.module_instance = None  # Reference to the chosen module\n        self.module_name = \'\'  # Type of module chosen (useful when saving/loading)\n        self.module_parameters = []  # Arguments passed (for saving/loading)\n\n    def register(self, module):\n        """""" Register a new module\n        The only restriction is that the given class has to implement the static\n        method get_module_id\n        Args:\n            module (Class): the module class to register\n        """"""\n        assert not module.get_module_id() in self.modules  # Overwriting not allowed\n        self.modules[module.get_module_id()] = module\n\n    def get_modules_ids(self):\n        """""" Return the list of added modules\n        Useful for instance for the command line parser\n        Returns:\n            list[str]: the list of modules\n        """"""\n        return self.modules.keys()\n\n    def get_chosen_name(self):\n        """""" Return the name of the chosen module\n        Name is defined by get_module_id\n        Returns:\n            str: the name of the chosen module\n        """"""\n        return self.module_name\n\n    def get_module(self):\n        """""" Return the chosen module\n        Returns:\n            Obj: the reference on the module instance\n        """"""\n        assert self.module_instance is not None\n        return self.module_instance\n\n    def build_module(self, args):\n        """""" Instantiate the chosen module\n        This function can be called only once when initializing the module\n        Args:\n            args (Obj): the global program parameters\n        Returns:\n            Obj: the created module\n        """"""\n        assert self.module_instance is None\n\n        module_args = getattr(args, self.name)  # Get the name of the module and its eventual additional parameters (Exception will be raised if the user try incorrect module)\n\n        self.module_name = module_args[0]\n        self.module_parameters = module_args[1:]\n        self.module_instance = self.modules[self.module_name](args, *self.module_parameters)\n        return self.module_instance\n\n    def add_argparse(self, group_args, comment):\n        """""" Add the module to the command line parser\n        All modules have to be registered before that call\n        Args:\n            group_args (ArgumentParser):\n            comment (str): help to add\n        """"""\n        assert len(self.modules.keys())   # Should contain at least 1 module\n\n        keys = list(self.modules.keys())\n        group_args.add_argument(\n            \'--{}\'.format(self.name),\n            type=str,\n            nargs=\'+\',\n            default=[keys[0]],  # No defaults optional argument (implemented in the module class)\n            help=comment + \' Choices available: {}\'.format(\', \'.join(keys))\n        )\n\n    def save(self, config_group):\n        """""" Save the current module parameters\n        Args:\n            config_group (dict): dictionary where to write the configuration\n        """"""\n        config_group[self.name] = \' \'.join([self.module_name] + self.module_parameters)\n        # TODO: The module state should be saved here\n\n    def load(self, args, config_group):\n        """""" Restore the parameters from the configuration group\n        Args:\n            args (parse_args() returned Obj): the parameters of the models (will be modified)\n            config_group (dict): the module group parameters to extract\n        Warning: Only restore the arguments. The instantiation is not done here\n        """"""\n        setattr(args, self.name, config_group.get(self.name).split(\' \'))\n\n    def print(self, args):\n        """""" Just print the current module configuration\n        We use the args parameters because the function is called\n        before build_module\n        Args:\n            args (parse_args() returned Obj): the parameters of the models\n        """"""\n        print(\'{}: {}\'.format(\n            self.name,\n            \' \'.join(getattr(args, self.name))\n        ))\n'"
deepmusic/musicdata.py,0,"b'# Copyright 2016 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nLoads the midi song, build the dataset\n""""""\n\nfrom tqdm import tqdm  # Progress bar when creating dataset\nimport pickle  # Saving the data\nimport os  # Checking file existence\nimport numpy as np  # Batch data\nimport json  # Load initiators (inputs for generating new songs)\n\nfrom deepmusic.moduleloader import ModuleLoader\nfrom deepmusic.midiconnector import MidiConnector\nfrom deepmusic.midiconnector import MidiInvalidException\nimport deepmusic.songstruct as music\n\n\nclass MusicData:\n    """"""Dataset class\n    """"""\n\n    def __init__(self, args):\n        """"""Load all conversations\n        Args:\n            args: parameters of the model\n        """"""\n\n        # Filename and directories constants\n        self.DATA_VERSION = \'0.2\'  # Assert compatibility between versions\n        self.DATA_DIR_MIDI = \'data/midi\'  # Originals midi files\n        self.DATA_DIR_PLAY = \'data/play\'  # Target folder to show the reconstructed files\n        self.DATA_DIR_SAMPLES = \'data/samples\'  # Training/testing samples after pre-processing\n        self.DATA_SAMPLES_RAW = \'raw\'  # Unpreprocessed songs container tag\n        self.DATA_SAMPLES_EXT = \'.pkl\'\n        self.TEST_INIT_FILE = \'data/test/initiator.json\'  # Initial input for the generated songs\n        self.FILE_EXT = \'.mid\'  # Could eventually add support for other format later ?\n\n        # Model parameters\n        self.args = args\n\n        # Dataset\n        self.songs = []\n        self.songs_train = None\n        self.songs_test = None\n\n        # TODO: Dynamic loading of the the associated dataset flag (ex: data/samples/pianoroll/...)\n        self.batch_builder = ModuleLoader.batch_builders.build_module(args)\n\n        if not self.args.test:  # No need to load the dataset when testing\n            self._restore_dataset()\n\n            if self.args.play_dataset:\n                print(\'Play some songs from the formatted data\')\n                # Generate songs\n                for i in range(min(10, len(self.songs))):\n                    raw_song = self.batch_builder.reconstruct_song(self.songs[i])\n                    MidiConnector.write_song(raw_song, os.path.join(self.DATA_DIR_PLAY, str(i)))\n                # TODO: Display some images corresponding to the loaded songs\n                raise NotImplementedError(\'Can\\\'t play a song for now\')\n\n            self._split_dataset()  # Warning: the list order will determine the train/test sets (so important that it don\'t change from run to run)\n\n            # Plot some stats:\n            print(\'Loaded: {} songs ({} train/{} test)\'.format(\n                len(self.songs_train) + len(self.songs_test),\n                len(self.songs_train),\n                len(self.songs_test))\n            )  # TODO: Print average, max, min duration\n\n    def _restore_dataset(self):\n        """"""Load/create the conversations data\n        Done in two steps:\n         * Extract the midi files as a raw song format\n         * Transform this raw song as neural networks compatible input\n        """"""\n\n        # Construct the dataset names\n        samples_path_generic = os.path.join(\n            self.args.root_dir,\n            self.DATA_DIR_SAMPLES,\n            self.args.dataset_tag + \'-{}\' + self.DATA_SAMPLES_EXT\n        )\n        samples_path_raw = samples_path_generic.format(self.DATA_SAMPLES_RAW)\n        samples_path_preprocessed = samples_path_generic.format(ModuleLoader.batch_builders.get_chosen_name())\n\n        # TODO: the _restore_samples from the raw songs and precomputed database should have different versions number\n\n        # Restoring precomputed database\n        if os.path.exists(samples_path_preprocessed):\n            print(\'Restoring dataset from {}...\'.format(samples_path_preprocessed))\n            self._restore_samples(samples_path_preprocessed)\n\n        # First time we load the database: creating all files\n        else:\n            print(\'Training samples not found. Creating dataset from the songs...\')\n            # Restoring raw songs\n            if os.path.exists(samples_path_raw):\n                print(\'Restoring songs from {}...\'.format(samples_path_raw))\n                self._restore_samples(samples_path_raw)\n\n            # First time we load the database: creating all files\n            else:\n                print(\'Raw songs not found. Extracting from midi files...\')\n                self._create_raw_songs()\n                print(\'Saving raw songs...\')\n                self._save_samples(samples_path_raw)\n\n            # At this point, self.songs contain the list of the raw songs. Each\n            # song is then preprocessed by the batch builder\n\n            # Generating the data from the raw songs\n            print(\'Pre-processing songs...\')\n            for i, song in tqdm(enumerate(self.songs), total=len(self.songs)):\n                self.songs[i] = self.batch_builder.process_song(song)\n\n            print(\'Saving dataset...\')\n            np.random.shuffle(self.songs)  # Important to do that before saving so the train/test set will be fixed each time we reload the dataset\n            self._save_samples(samples_path_preprocessed)\n\n    def _restore_samples(self, samples_path):\n        """""" Load samples from file\n        Args:\n            samples_path (str): The path where to load the model (all dirs should exist)\n        Return:\n            List[Song]: The training data\n        """"""\n        with open(samples_path, \'rb\') as handle:\n            data = pickle.load(handle)  # Warning: If adding something here, also modifying saveDataset\n\n            # Check the version\n            current_version = data[\'version\']\n            if current_version != self.DATA_VERSION:\n                raise UserWarning(\'Present configuration version {0} does not match {1}.\'.format(current_version, self.DATA_VERSION))\n\n            # Restore parameters\n            self.songs = data[\'songs\']\n\n    def _save_samples(self, samples_path):\n        """""" Save samples to file\n        Args:\n            samples_path (str): The path where to save the model (all dirs should exist)\n        """"""\n\n        with open(samples_path, \'wb\') as handle:\n            data = {  # Warning: If adding something here, also modifying loadDataset\n                \'version\': self.DATA_VERSION,\n                \'songs\': self.songs\n            }\n            pickle.dump(data, handle, -1)  # Using the highest protocol available\n\n    def _create_raw_songs(self):\n        """""" Create the database from the midi files\n        """"""\n        midi_dir = os.path.join(self.args.root_dir, self.DATA_DIR_MIDI, self.args.dataset_tag)\n        midi_files = [os.path.join(midi_dir, f) for f in os.listdir(midi_dir) if f.endswith(self.FILE_EXT)]\n\n        for filename in tqdm(midi_files):\n\n            try:\n                new_song = MidiConnector.load_file(filename)\n            except MidiInvalidException as e:\n                tqdm.write(\'File ignored ({}): {}\'.format(filename, e))\n            else:\n                self.songs.append(new_song)\n                tqdm.write(\'Song loaded {}: {} tracks, {} notes, {} ticks/beat\'.format(\n                    filename,\n                    len(new_song.tracks),\n                    sum([len(t.notes) for t in new_song.tracks]),\n                    new_song.ticks_per_beat\n                ))\n\n        if not self.songs:\n            raise ValueError(\'Empty dataset. Check that the folder exist and contains supported midi files.\')\n\n    def _convert_song2array(self, song):\n        """""" Convert a given song to a numpy multi-dimensional array (piano roll)\n        The song is temporally normalized, meaning that all ticks and duration will be converted to a specific\n        ticks_per_beat independent unit.\n        For now, the changes of tempo are ignored. Only 4/4 is supported.\n        Warning: The duration is ignored: All note have the same duration (1 unit)\n        Args:\n            song (Song): The song to convert\n        Return:\n            Array: the numpy array: a binary matrix of shape [NB_NOTES, song_length]\n        """"""\n\n        # Convert the absolute ticks in standardized unit\n        song_length = len(song)\n        scale = self._get_scale(song)\n\n        # TODO: Not sure why this plot a decimal value (x.66). Investigate...\n        # print(song_length/scale)\n\n        # Use sparse array instead ?\n        piano_roll = np.zeros([music.NB_NOTES, int(np.ceil(song_length/scale))], dtype=int)\n\n        # Adding all notes\n        for track in song.tracks:\n            for note in track.notes:\n                piano_roll[note.get_relative_note()][note.tick//scale] = 1\n\n        return piano_roll\n\n    def _convert_array2song(self, array):\n        """""" Create a new song from a numpy array\n        A note will be created for each non empty case of the array. The song will contain a single track, and use the\n        default beats_per_tick as midi resolution\n        For now, the changes of tempo are ignored. Only 4/4 is supported.\n        Warning: All note have the same duration, the default value defined in music.Note\n        Args:\n            np.array: the numpy array (Warning: could be a array of int or float containing the prediction before the sigmoid)\n        Return:\n            song (Song): The song to convert\n        """"""\n\n        new_song = music.Song()\n        main_track = music.Track()\n\n        scale = self._get_scale(new_song)\n\n        for index, x in np.ndenumerate(array):  # Add some notes\n            if x > 1e-12:  # Note added (TODO: What should be the condition, =1 ? sigmoid>0.5 ?)\n                new_note = music.Note()\n\n                new_note.set_relative_note(index[0])\n                new_note.tick = index[1] * scale  # Absolute time in tick from the beginning\n\n                main_track.notes.append(new_note)\n\n        new_song.tracks.append(main_track)\n\n        return new_song\n\n    def _split_dataset(self):\n        """""" Create the test/train set from the loaded songs\n        The dataset has been shuffled when calling this function (Warning: the shuffling\n        is done and fixed before saving the dataset the first time so it is important to\n        NOT call shuffle a second time)\n        """"""\n        split_nb = int(self.args.ratio_dataset * len(self.songs))\n        self.songs_train = self.songs[:split_nb]\n        self.songs_test = self.songs[split_nb:]\n        self.songs = None  # Not needed anymore (free some memory)\n\n    def get_batches(self):\n        """""" Prepare the batches for the current epoch\n        WARNING: The songs are not shuffled in this functions. We leave the choice\n        to the batch_builder to manage the shuffling\n        Return:\n            list[Batch], list[Batch]: The batches for the training and testing set (can be generators)\n        """"""\n        return (\n            self.batch_builder.get_list(self.songs_train, name=\'train\'),\n            self.batch_builder.get_list(self.songs_test, name=\'test\'),\n        )\n\n    # def get_batches_test(self, ):  # TODO: Should only return a single batch (loading done in main class)\n    #     """""" Return the batch which initiate the RNN when generating\n    #     The initial batches are loaded from a json file containing the first notes of the song. The note values\n    #     are the standard midi ones. Here is an examples of an initiator file:\n    #     Args:\n    #         TODO\n    #     Return:\n    #         Batch: The generated batch\n    #     """"""\n    #     assert self.args.batch_size == 1\n    #     batch = None  # TODO\n    #     return batch\n\n    def get_batches_test_old(self):  # TODO: This is the old version. Ideally should use the version above\n        """""" Return the batches which initiate the RNN when generating\n        The initial batches are loaded from a json file containing the first notes of the song. The note values\n        are the standard midi ones. Here is an examples of an initiator file:\n        ```\n        {""initiator"":[\n            {""name"":""Simple_C4"",\n             ""seq"":[\n                {""notes"":[60]}\n            ]},\n            {""name"":""some_chords"",\n             ""seq"":[\n                {""notes"":[60,64]}\n                {""notes"":[66,68,71]}\n                {""notes"":[60,64]}\n            ]}\n        ]}\n        ```\n        Return:\n            List[Batch], List[str]: The generated batches with the associated names\n        """"""\n        assert self.args.batch_size == 1\n\n        batches = []\n        names = []\n\n        with open(self.TEST_INIT_FILE) as init_file:\n            initiators = json.load(init_file)\n\n        for initiator in initiators[\'initiator\']:\n            raw_song = music.Song()\n            main_track = music.Track()\n\n            current_tick = 0\n            for seq in initiator[\'seq\']:  # We add a few notes\n                for note_pitch in seq[\'notes\']:\n                    new_note = music.Note()\n                    new_note.note = note_pitch\n                    new_note.tick = current_tick\n                    main_track.notes.append(new_note)\n                current_tick += 1\n\n            raw_song.tracks.append(main_track)\n            raw_song.normalize(inverse=True)\n\n            batch = self.batch_builder.process_batch(raw_song)\n\n            names.append(initiator[\'name\'])\n            batches.append(batch)\n\n        return batches, names\n\n    @staticmethod\n    def _convert_to_piano_rolls(outputs):\n        """""" Create songs from the decoder outputs.\n        Reshape the list of outputs to list of piano rolls\n        Args:\n            outputs (List[np.array]): The list of the predictions of the decoder\n        Return:\n            List[np.array]: the list of the songs (one song by batch) as piano roll\n        """"""\n\n        # Extract the batches and recreate the array for each batch\n        piano_rolls = []\n        for i in range(outputs[0].shape[0]):  # Iterate over the batches\n            piano_roll = None\n            for j in range(len(outputs)):  # Iterate over the sample length\n                # outputs[j][i, :] has shape [NB_NOTES, 1]\n                if piano_roll is None:\n                    piano_roll = [outputs[j][i, :]]\n                else:\n                    piano_roll = np.append(piano_roll, [outputs[j][i, :]], axis=0)\n            piano_rolls.append(piano_roll.T)\n\n        return piano_rolls\n\n    def visit_recorder(self, outputs, base_dir, base_name, recorders, chosen_labels=None):\n        """""" Save the predicted output songs using the given recorder\n        Args:\n            outputs (List[np.array]): The list of the predictions of the decoder\n            base_dir (str): Path were to save the outputs\n            base_name (str): filename of the output (without the extension)\n            recorders (List[Obj]): Interfaces called to convert the song into a file (ex: midi or png). The recorders\n                need to implement the method write_song (the method has to add the file extension) and the\n                method get_input_type.\n            chosen_labels (list[np.Array[batch_size, int]]): the chosen class at each timestep (useful to reconstruct the generated song)\n        """"""\n\n        if not os.path.exists(base_dir):\n            os.makedirs(base_dir)\n\n        for batch_id in range(outputs[0].shape[0]):  # Loop over batch_size\n            song = self.batch_builder.reconstruct_batch(outputs, batch_id, chosen_labels)\n            for recorder in recorders:\n                if recorder.get_input_type() == \'song\':\n                    input = song\n                elif recorder.get_input_type() == \'array\':\n                    #input = self._convert_song2array(song)\n                    continue  # TODO: For now, pianoroll desactivated\n                else:\n                    raise ValueError(\'Unknown recorder input type.\'.format(recorder.get_input_type()))\n                base_path = os.path.join(base_dir, base_name + \'-\' + str(batch_id))\n                recorder.write_song(input, base_path)\n'"
deepmusic/songstruct.py,0,"b'# Copyright 2016 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nHierarchical data structures of a song\n""""""\n\nimport operator  # To rescale the song\n\n\nMIDI_NOTES_RANGE = [21, 108]  # Min and max (included) midi note on a piano\n# TODO: Warn/throw when we try to add a note outside this range\n# TODO: Easy conversion from this range to tensor vector id (midi_note2tf_id)\n\nNB_NOTES = MIDI_NOTES_RANGE[1] - MIDI_NOTES_RANGE[0] + 1  # Vertical dimension of a song (=88 of keys for a piano)\n\nBAR_DIVISION = 16  # Nb of tics in a bar (What about waltz ? is 12 better ?)\n\n\nclass Note:\n    """""" Structure which encapsulate the song data\n    """"""\n    def __init__(self):\n        self.tick = 0\n        self.note = 0\n        self.duration = 32  # TODO: Define the default duration / TODO: Use standard musical units (quarter note/eighth note) ?, don\'t convert here\n\n    def get_relative_note(self):\n        """""" Convert the absolute midi position into the range given by MIDI_NOTES_RANGE\n        Return\n            int: The new position relative to the range (position on keyboard)\n        """"""\n        return self.note - MIDI_NOTES_RANGE[0]\n\n    def set_relative_note(self, rel):\n        """""" Convert given note into a absolute midi position\n        Args:\n            rel (int): The new position relative to the range (position on keyboard)\n        """"""\n        # TODO: assert range (rel < NB_NOTES)?\n        self.note = rel + MIDI_NOTES_RANGE[0]\n\n\nclass Track:\n    """""" Structure which encapsulate a track of the song\n    Ideally, each track should correspond to a single instrument and one channel. Multiple tracks could correspond\n    to the same channel if different instruments use the same channel.\n    """"""\n    def __init__(self):\n        #self.tempo_map = None  # Use a global tempo map\n        self.instrument = None\n        self.notes = []  # List[Note]\n        #self.color = (0, 0, 0)  # Color of the track for visual plotting\n        self.is_drum = False\n\n    def set_instrument(self, msg):\n        """""" Initialize from a mido message\n        Args:\n            msg (mido.MidiMessage): a valid control_change message\n        """"""\n        if self.instrument is not None:  # Already an instrument set\n            return False\n\n        assert msg.type == \'program_change\'\n\n        self.instrument = msg.program\n        if msg.channel == 9 or msg.program > 112:  # Warning: Mido shift the channels (start at 0)\n            self.is_drum = True\n\n        return True\n\n\nclass Song:\n    """""" Structure which encapsulate the song data\n    """"""\n\n    # Define the time unit\n    # TODO: musicdata should have possibility to modify those parameters (through self.args)\n    # Invert of time note which define the maximum resolution for a song. Ex: 2 for 1/2 note, 4 for 1/4 of note\n    MAXIMUM_SONG_RESOLUTION = 4\n    NOTES_PER_BAR = 4  # Waltz not supported\n\n    def __init__(self):\n        self.ticks_per_beat = 96\n        self.tempo_map = []\n        self.tracks = []  # List[Track]\n\n    def __len__(self):\n        """""" Return the absolute tick when the last note end\n        Note that the length is recomputed each time the function is called\n        """"""\n        return max([max([n.tick + n.duration for n in t.notes]) for t in self.tracks])\n\n    def _get_scale(self):\n        """""" Compute the unit scale factor for the song\n        The scale factor allow to have a tempo independent time unit, to represent the song as an array\n        of dimension [key, time_unit]. Once computed, one has just to divide (//) the ticks or multiply\n        the time units to go from one representation to the other.\n\n        Return:\n            int: the scale factor for the current song\n        """"""\n\n        # TODO: Assert that the scale factor is not a float (the % =0)\n        return 4 * self.ticks_per_beat // (Song.MAXIMUM_SONG_RESOLUTION*Song.NOTES_PER_BAR)\n\n    def normalize(self, inverse=False):\n        """""" Transform the song into a tempo independent song\n        Warning: If the resolution of the song is is more fine that the given\n        scale, some information will be definitively lost\n        Args:\n            inverse (bool): if true, we reverse the normalization\n        """"""\n        scale = self._get_scale()\n        op = operator.floordiv if not inverse else operator.mul\n\n        # TODO: Not sure why this plot a decimal value (x.66). Investigate...\n        # print(song_length/scale)\n\n        # Shifting all notes\n        for track in self.tracks:\n            for note in track.notes:\n                note.tick = op(note.tick, scale)  # //= or *=\n'"
deepmusic/tfutils.py,14,"b'# Copyright 2016 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nSome functions to help define neural networks\n""""""\n\nimport tensorflow as tf\n\n\ndef single_layer_perceptron(shape, scope_name):\n    """""" Single layer perceptron\n    Project X on the output dimension\n    Args:\n        shape: a tuple (input dim, output dim)\n        scope_name (str): encapsulate variables\n    Return:\n        tf.Ops: The projection operator (see project_fct())\n    """"""\n    assert len(shape) == 2\n\n    # Projection on the keyboard\n    with tf.variable_scope(\'weights_\' + scope_name):\n        W = tf.get_variable(\n            \'weights\',\n            shape,\n            initializer=tf.truncated_normal_initializer()  # TODO: Tune value (fct of input size: 1/sqrt(input_dim))\n        )\n        b = tf.get_variable(\n            \'bias\',\n            shape[1],\n            initializer=tf.constant_initializer()\n        )\n\n    def project_fct(X):\n        """""" Project the output of the decoder into the note space\n        Args:\n            X (tf.Tensor): input value\n        """"""\n        # TODO: Could we add an activation function as option ?\n        with tf.name_scope(scope_name):\n            return tf.matmul(X, W) + b\n\n    return project_fct\n\n\ndef get_rnn_cell(args, scope_name):\n    """""" Return RNN cell, constructed from the parameters\n    Args:\n        args: the rnn parameters\n        scope_name (str): encapsulate variables\n    Return:\n        tf.RNNCell: a cell\n    """"""\n    with tf.variable_scope(\'weights_\' + scope_name):\n        rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(args.hidden_size, state_is_tuple=True)  # Or GRUCell, LSTMCell(args.hidden_size)\n        #rnn_cell = tf.nn.rnn_cell.DropoutWrapper(rnn_cell, input_keep_prob=1.0, output_keep_prob=1.0)  # TODO: Custom values (WARNING: No dropout when testing !!!, possible to use placeholder ?)\n        rnn_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell] * args.num_layers, state_is_tuple=True)\n    return rnn_cell\n'"
deepmusic/modules/batchbuilder.py,3,"b'# Copyright 2016 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nThe batch builder convert the songs into data readable by the neural networks.\nUsed for training, testing and generating\n""""""\n\nimport random  # Shuffling\nimport operator  # Multi-level sorting\nimport numpy as np\n\nimport deepmusic.songstruct as music\n\n\nclass Batch:\n    """"""Structure containing batches info\n    Should be in a tf placeholder compatible format\n    """"""\n    def __init__(self):\n        self.inputs = []\n        self.targets = []\n\n    def generate(self, target=True):\n        """""" Is called just before feeding the placeholder, allows additional\n        pre-processing\n        Args:\n            target(Bool): is true if the bach also need to generate the target\n        """"""\n        pass\n\n\nclass BatchBuilder:\n    """""" Class which create and manage batches\n    Batches are created from the songs\n    Define the song representation input (and output) format so the network must\n    support the format\n    The class has the choice to either entirely create the\n    batches when get list is called or to create the batches\n    as the training progress (more memory efficient)\n    """"""\n    # TODO: Should add a option to pre-compute a lot of batches and\n    # cache them in the hard drive\n    # TODO: For generating mode, add another function\n    # TODO: Add a function to get the length too (for tqdm when generators) ?\n    def __init__(self, args):\n        """"""\n        """"""\n        self.args = args\n\n    @staticmethod\n    def get_module_id():\n        """""" Return the unique id associated with the builder\n        Ultimately, the id will be used for saving/loading the dataset, and\n        as parameter argument.\n        Returns:\n            str: The name of the builder\n        """"""\n        raise NotImplementedError(\'Abstract class\')\n\n    def get_list(self, dataset, name):\n        """""" Compute the batches for the current epoch\n        Is called twice (for training and testing)\n        Args:\n            dataset (list[Objects]): the training/testing set\n            name (str): indicate the dataset type\n        Return:\n            list[Batch]: the batches to process\n        """"""\n        raise NotImplementedError(\'Abstract class\')\n\n    def build_next(self, batch):\n        """""" In case of a generator (batches non precomputed), compute the batch given\n        the batch id passed\n        Args:\n            batch: the current testing or training batch or id of batch to generate\n        Return:\n            Batch: the computed batch\n        """"""\n        # TODO: Unused function. Instead Batch.generate does the same thing. Is it\n        # a good idea ? Probably not. Instead should prefer this factory function\n        return batch\n\n    def build_placeholder_input(self):\n        """""" Create a placeholder compatible with the batch input\n        Allow to control the dimensions\n        Return:\n            tf.placeholder: the placeholder for a single timestep\n        """"""\n        raise NotImplementedError(\'Abstract class\')\n\n    def build_placeholder_target(self):\n        """""" Create a placeholder compatible with the target\n        Allow to control the dimensions\n        Return:\n            tf.placeholder: the placeholder for a single timestep\n        """"""\n        # TODO: The target also depend of the loss function (sigmoid, softmax,...) How to redefined that ?\n        raise NotImplementedError(\'Abstract class\')\n\n    def process_song(self, song):\n        """""" Apply some pre-processing to the songs so the song\n        already get the right input representation.\n        Do it once globally for all songs\n        Args:\n            song (Song): the training/testing set\n        Return:\n            Object: the song after formatting\n        """"""\n        return song  # By default no pre-processing\n\n    def reconstruct_song(self, song):\n        """""" Reconstruct the original raw song from the preprocessed data\n        We should have:\n            reconstruct_song(process_song(my_song)) == my_song\n\n        Args:\n            song (Object): the training/testing set\n        Return:\n            Song: the song after formatting\n        """"""\n        return song  # By default no pre-processing\n\n    def process_batch(self, raw_song):\n        """""" Create the batch associated with the song\n        Called when generating songs to create the initial input batch\n        Args:\n            raw_song (Song): The song to convert\n        Return:\n            Batch\n        """"""\n        raise NotImplementedError(\'Abstract class\')\n\n    def reconstruct_batch(self, output, batch_id, chosen_labels=None):\n        """""" Create the song associated with the network output\n        Args:\n            output (list[np.Array]): The ouput of the network (size batch_size*output_dim)\n            batch_id (int): The batch that we must reconstruct\n            chosen_labels (list[np.Array[batch_size, int]]): the sampled class at each timestep (useful to reconstruct the generated song)\n        Return:\n            Song: The reconstructed song\n        """"""\n        raise NotImplementedError(\'Abstract class\')\n\n    def get_input_dim():\n        """""" Return the input dimension\n        Return:\n            int:\n        """"""\n        raise NotImplementedError(\'Abstract class\')\n\n\nclass Relative(BatchBuilder):\n    """""" Prepare the batches for the current epoch.\n    Generate batches of the form:\n        12 values for relative position with previous notes (modulo 12)\n        14 values for the relative pitch (+/-7)\n        12 values for the relative positions with the previous note\n    """"""\n    NB_NOTES_SCALE = 12\n    OFFSET_SCALES = 0  # Start at A0\n    NB_SCALES = 7  # Up to G7 (Official order is A6, B6, C7, D7, E7,... G7)\n\n    # Experiments on the relative note representation:\n    # Experiment 1:\n    # * As baseline, we only project the note on one scale (C5: 51)\n    BASELINE_OFFSET = 51\n\n    # Options:\n    # * Note absolute (A,B,C,...G) vs relative ((current-prev)%12)\n    NOTE_ABSOLUTE = False\n    # * Use separation token between the notes (a note with class_pitch=-1 is a separation token)\n    HAS_EMPTY = True\n\n    class RelativeNote:\n        """""" Struct which define a note in a relative way with respect to\n        the previous note\n        Can only play 7 octave (so the upper and lower notes of the\n        piano are never reached (not that important in practice))\n        """"""\n        def __init__(self):\n            # TODO: Should the network get some information about the absolute pitch ?? An other way could be to\n            # always start by a note from the base\n            # TODO: Define behavior when saturating\n            # TODO: Try with time relative to prev vs next\n            # TODO: Try to randomly permute chords vs low to high pitch\n            # TODO: Try pitch %7 vs fixed +/-7\n            # TODO: Try to add a channel number for each note (2 class SoftMax) <= Would require a clean database where the melodie/bass are clearly separated\n            self.pitch_class = 0  # A, B, C,... +/- %12\n            self.scale = 0  # Octave +/- % 7\n            self.prev_tick = 0  # Distance from previous note (from -0 up to -MAXIMUM_SONG_RESOLUTION*NOTES_PER_BAR (=1 bar))\n\n    class RelativeSong:\n        """""" Struct which define a song in a relative way (intern class format)\n        Can only play 7 octave (so the upper and lower notes of the\n        piano are never reached (not that important in practice))\n        """"""\n        def __init__(self):\n            """""" All attribute are defined with respect with the previous one\n            """"""\n            self.first_note = None  # Define the reference note\n            self.notes = []\n\n    class RelativeBatch(Batch):\n        """""" Struct which contains temporary information necessary to reconstruct the\n        batch\n        """"""\n        class SongExtract:  # Define a subsong\n            def __init__(self):\n                self.song = None  # The song reference\n                self.begin = 0\n                self.end = 0\n\n        def __init__(self, extracts):\n            """"""\n            Args:\n                extracts(list[SongExtract]): Should be of length batch_size, or at least all from the same size\n            """"""\n            super().__init__()\n            self.extracts = extracts\n\n        def generate(self, target=True):\n            """"""\n            Args:\n                target(Bool): is true if the bach also need to generate the target\n            """"""\n            # TODO: Could potentially be optimized (one big numpy array initialized only one, each input is a sub-arrays)\n            # TODO: Those inputs should be cleared once the training pass has be run (Use class with generator, __next__ and __len__)\n            sequence_length = self.extracts[0].end - self.extracts[0].begin\n            shape_input = (len(self.extracts), Relative.RelativeBatch.get_input_dim())  # (batch_size, note_space) +1 because of the <next> token\n\n            def gen_input(i):\n                array = np.zeros(shape_input)\n                for j, extract in enumerate(self.extracts):  # Iterate over the batches\n                    # Set the one-hot vector (chose label between <next>,A,...,G)\n                    label = extract.song.notes[extract.begin + i].pitch_class\n                    array[j, 0 if not label else label + 1] = 1\n                return array\n\n            def gen_target(i):  # TODO: Could merge with the previous function to optimize the calls\n                array = np.zeros([len(self.extracts)], dtype=int)  # Int for SoftMax compatibility\n                for j, extract in enumerate(self.extracts):  # Iterate over the batches\n                    # Set the one-hot label (chose label between <next>,A,...,G)\n                    label = extract.song.notes[extract.begin + i + 1].pitch_class  # Warning: +1 because targets are shifted with respect to the inputs\n                    array[j] = 0 if not label else label + 1\n                return array\n\n            self.inputs = [gen_input(i) for i in range(sequence_length)]  # Generate each input sequence\n            if target:\n                self.targets = [gen_target(i) for i in range(sequence_length)]\n\n        @staticmethod\n        def get_input_dim():\n            """"""\n            """"""\n            # TODO: Refactoring. Where to place this functions ?? Should be accessible from model, and batch and depend of\n            # batch_builder, also used in enco/deco modules. Ideally should not be static\n            return 1 + Relative.NB_NOTES_SCALE  # +1 because of the <next> token\n\n    def __init__(self, args):\n        super().__init__(args)\n\n    @staticmethod\n    def get_module_id():\n        return \'relative\'\n\n    def process_song(self, old_song):\n        """""" Pre-process the data once globally\n        Do it once globally.\n        Args:\n            old_song (Song): original song\n        Returns:\n            list[RelativeSong]: the new formatted song\n        """"""\n        new_song = Relative.RelativeSong()\n\n        old_song.normalize()\n\n        # Gather all notes and sort them by absolute time\n        all_notes = []\n        for track in old_song.tracks:\n            for note in track.notes:\n                all_notes.append(note)\n        all_notes.sort(key=operator.attrgetter(\'tick\', \'note\'))  # Sort first by tick, then by pitch\n\n        # Compute the relative position for each note\n        prev_note = all_notes[0]\n        new_song.first_note = prev_note  # TODO: What if the song start by a chord ?\n        for note in all_notes[1:]:\n            # Check if we should insert an empty token\n            temporal_distance = note.tick - prev_note.tick\n            assert temporal_distance >= 0\n            if Relative.HAS_EMPTY and temporal_distance > 0:\n                for i in range(temporal_distance):\n                    separator = Relative.RelativeNote()  # Separation token\n                    separator.pitch_class = None\n                    new_song.notes.append(separator)\n\n            # Insert the new relative note\n            new_note = Relative.RelativeNote()\n            if Relative.NOTE_ABSOLUTE:\n                new_note.pitch_class = note.note % Relative.NB_NOTES_SCALE\n            else:\n                new_note.pitch_class = (note.note - prev_note.note) % Relative.NB_NOTES_SCALE\n            new_note.scale = (note.note//Relative.NB_NOTES_SCALE - prev_note.note//Relative.NB_NOTES_SCALE) % Relative.NB_SCALES  # TODO: add offset for the notes ? (where does the game begins ?)\n            new_note.prev_tick = temporal_distance\n            new_song.notes.append(new_note)\n\n            prev_note = note\n\n        return new_song\n\n    def reconstruct_song(self, rel_song):\n        """""" Reconstruct the original raw song from the preprocessed data\n        See parent class for details\n\n        Some information will be lost compare to the original song:\n            * Only one track left\n            * Original tempo lost\n        Args:\n            rel_song (RelativeSong): the song to reconstruct\n        Return:\n            Song: the reconstructed song\n        """"""\n        raw_song = music.Song()\n        main_track = music.Track()\n\n        prev_note = rel_song.first_note\n        main_track.notes.append(rel_song.first_note)\n        current_tick = rel_song.first_note.tick\n        for next_note in rel_song.notes:\n            # Case of separator\n            if next_note.pitch_class is None:\n                current_tick += 1\n                continue\n\n            # Adding the new note\n            new_note = music.Note()\n            # * Note\n            if Relative.NOTE_ABSOLUTE:\n                new_note.note = Relative.BASELINE_OFFSET + next_note.pitch_class\n            else:\n                new_note.note = Relative.BASELINE_OFFSET + ((prev_note.note-Relative.BASELINE_OFFSET) + next_note.pitch_class) % Relative.NB_NOTES_SCALE\n            # * Tick\n            if Relative.HAS_EMPTY:\n                new_note.tick = current_tick\n            else:\n                new_note.tick = prev_note.tick + next_note.prev_tick\n            # * Scale\n            # ...\n            main_track.notes.append(new_note)\n            prev_note = new_note\n\n        raw_song.tracks.append(main_track)\n        raw_song.normalize(inverse=True)\n        return raw_song\n\n    def process_batch(self, raw_song):\n        """""" Create the batch associated with the song\n        Args:\n            raw_song (Song): The song to convert\n        Return:\n            RelativeBatch\n        """"""\n        processed_song = self.process_song(raw_song)\n        extract = self.create_extract(processed_song, 0, len(processed_song.notes))\n        batch = Relative.RelativeBatch([extract])\n        return batch\n\n    def reconstruct_batch(self, output, batch_id, chosen_labels=None):\n        """""" Create the song associated with the network output\n        Args:\n            output (list[np.Array]): The ouput of the network (size batch_size*output_dim)\n            batch_id (int): The batch id\n            chosen_labels (list[np.Array[batch_size, int]]): the sampled class at each timestep (useful to reconstruct the generated song)\n        Return:\n            Song: The reconstructed song\n        """"""\n        assert Relative.HAS_EMPTY == True\n\n        processed_song = Relative.RelativeSong()\n        processed_song.first_note = music.Note()\n        processed_song.first_note.note = 56  # TODO: Define what should be the first note\n        print(\'Reconstruct\')\n        for i, note in enumerate(output):\n            relative = Relative.RelativeNote()\n            # Here if we did sample the output, we should get which has heen the selected output\n            if not chosen_labels or i == len(chosen_labels):  # If chosen_labels, the last generated note has not been sampled\n                chosen_label = int(np.argmax(note[batch_id,:]))  # Cast np.int64 to int to avoid compatibility with mido\n            else:\n                chosen_label = int(chosen_labels[i][batch_id])\n            print(chosen_label, end=\' \')  # TODO: Add a text output connector\n            if chosen_label == 0:  # <next> token\n                relative.pitch_class = None\n                #relative.scale = # Note used\n                #relative.prev_tick =\n            else:\n                relative.pitch_class = chosen_label-1\n                #relative.scale =\n                #relative.prev_tick =\n            processed_song.notes.append(relative)\n        print()\n        return self.reconstruct_song(processed_song)\n\n    def create_extract(self, processed_song, start, length):\n        """""" preprocessed song > batch\n        """"""\n        extract = Relative.RelativeBatch.SongExtract()\n        extract.song = processed_song\n        extract.begin = start\n        extract.end = extract.begin + length\n        return extract\n\n    # TODO: How to optimize !! (precompute all values, use sparse arrays ?)\n    def get_list(self,  dataset, name):\n        """""" See parent class for more details\n        Args:\n            dataset (list[Song]): the training/testing set\n            name (str): indicate the dataset type\n        Return:\n            list[Batch]: the batches to process\n        """"""\n        # Randomly extract subsamples of the songs\n        print(\'Subsampling the songs ({})...\'.format(name))\n\n        extracts = []\n        sample_subsampling_length = self.args.sample_length+1  # We add 1 because each input has to predict the next output\n        for song in dataset:\n            len_song = len(song.notes)\n            max_start = len_song - sample_subsampling_length\n            assert max_start >= 0  # TODO: Error handling (and if =0, compatible with randint ?)\n            nb_sample_song = 2*len_song // self.args.sample_length  # The number of subsample is proportional to the song length (TODO: Could control the factor)\n            for _ in range(nb_sample_song):\n                extracts.append(self.create_extract(\n                    song,\n                    random.randrange(max_start),  # Begin TODO: Add mode to only start at the beginning of a bar\n                    self.args.sample_length # End\n                ))\n\n        # Shuffle the song extracts\n        print(\'Shuffling the dataset...\')\n        random.shuffle(extracts)\n\n        # Group the samples together to create the batches\n        print(\'Generating batches...\')\n\n        def gen_next_samples():\n            """""" Generator over the mini-batch training samples\n            Warning: the last samples will be ignored if the number of batch does not match the number of samples\n            """"""\n            nb_samples = len(extracts)\n            for i in range(nb_samples//self.args.batch_size):\n                yield extracts[i*self.args.batch_size:(i+1)*self.args.batch_size]\n\n        batch_set = [Relative.RelativeBatch(e) for e in gen_next_samples()]\n        return batch_set\n\n    def get_input_dim(self):\n        """""" In the case of the relative song, the input dim is the number of\n        note on the scale (12) + 1 for the next token\n        Return:\n            int:\n        """"""\n        return Relative.RelativeBatch.get_input_dim()\n\n\nclass PianoRoll(BatchBuilder):\n    """""" Old piano roll format (legacy code). Won\'t work as it is\n    """"""\n    def __init__(self, args):\n        super().__init__(args)\n\n    @staticmethod\n    def get_module_id():\n        return \'pianoroll\'\n\n    def get_list(self, dataset):\n\n        # On the original version, the songs were directly converted to piano roll\n        # self._convert_song2array()\n\n        batches = []\n\n        # TODO: Create batches (randomly cut each song in some small parts (need to know the total length for that)\n        # then create the big matrix (NB_NOTE*sample_length) and turn that into batch). If process too long,\n        # could save the created batches in a new folder, data/samples or save/model.\n\n        # TODO: Create batches from multiples length (buckets). How to change the loss functions weights (longer\n        # sequences more penalized ?)\n\n        # TODO: Optimize memory management\n\n        # First part: Randomly extract subsamples of the songs\n        print(\'Subsampling songs ({})...\'.format(\'train\' if train_set else \'test\'))\n\n        sample_subsampling_length = self.args.sample_length+1  # We add 1 because each input has to predict the next output\n\n        sub_songs = []\n        songs_set = dataset\n        for song in songs_set:\n            len_song = song.shape[-1]  # The last dimension correspond to the song duration\n            max_start = len_song - sample_subsampling_length\n            assert max_start >= 0  # TODO: Error handling (and if =0, compatible with randint ?)\n            nb_sample_song = 2*len_song // self.args.sample_length  # The number of subsample is proportional to the song length\n            for _ in range(nb_sample_song):\n                start = np.random.randint(max_start)  # TODO: Add mode to only start at the begining of a bar\n                sub_song = song[:, start:start+sample_subsampling_length]\n                sub_songs.append(sub_song)\n\n        # Second part: Shuffle the song extracts\n        print(""Shuffling the dataset..."")\n        np.random.shuffle(sub_songs)\n\n        # Third part: Group the samples together to create the batches\n        print(""Generating batches..."")\n\n        def gen_next_samples():\n            """""" Generator over the mini-batch training samples\n            Warning: the last samples will be ignored if the number of batch does not match the number of samples\n            """"""\n            nb_samples = len(sub_songs)\n            for i in range(nb_samples//self.args.batch_size):\n                yield sub_songs[i*self.args.batch_size:(i+1)*self.args.batch_size]\n\n        for samples in gen_next_samples():  # TODO: tqdm with persist = False / will this work with generators ?\n            batch = Batch()\n\n            # samples has shape [batch_size, NB_NOTES, sample_subsampling_length]\n            assert len(samples) == self.args.batch_size\n            assert samples[0].shape == (music.NB_NOTES, sample_subsampling_length)\n\n            # Define targets and inputs\n            for i in range(self.args.sample_length):\n                input = -np.ones([len(samples), music.NB_NOTES])\n                target = np.zeros([len(samples), music.NB_NOTES])\n                for j, sample in enumerate(samples):  # len(samples) == self.args.batch_size\n                    # TODO: Could reuse boolean idx computed (from target to next input)\n                    input[j, sample[:, i] == 1] = 1.0\n                    target[j, sample[:, i+1] == 1] = 1.0\n\n                batch.inputs.append(input)\n                batch.targets.append(target)\n\n            batches.append(batch)\n\n        # Use tf.train.batch() ??\n\n        # TODO: Save some batches as midi to see if correct\n\n        return batches\n\n    def get_batches_test(self):  # TODO: Move that to BatchBuilder\n        """""" Return the batches which initiate the RNN when generating\n        The initial batches are loaded from a json file containing the first notes of the song. The note values\n        are the standard midi ones. Here is an examples of an initiator file:\n\n        ```\n        {""initiator"":[\n            {""name"":""Simple_C4"",\n             ""seq"":[\n                {""notes"":[60]}\n            ]},\n            {""name"":""some_chords"",\n             ""seq"":[\n                {""notes"":[60,64]}\n                {""notes"":[66,68,71]}\n                {""notes"":[60,64]}\n            ]}\n        ]}\n        ```\n\n        Return:\n            List[Batch], List[str]: The generated batches with the associated names\n        """"""\n        assert self.args.batch_size == 1\n\n        batches = []\n        names = []\n\n        with open(self.TEST_INIT_FILE) as init_file:\n            initiators = json.load(init_file)\n\n        for initiator in initiators[\'initiator\']:\n            batch = Batch()\n\n            for seq in initiator[\'seq\']:  # We add a few notes\n                new_input = -np.ones([self.args.batch_size, music.NB_NOTES])  # No notes played by default\n                for note in seq[\'notes\']:\n                    new_input[0, note] = 1.0\n                batch.inputs.append(new_input)\n\n            names.append(initiator[\'name\'])\n            batches.append(batch)\n\n        return batches, names\n'"
deepmusic/modules/decoder.py,8,"b'# Copyright 2016 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\n""""""\n\nimport tensorflow as tf\n\nimport deepmusic.tfutils as tfutils\nimport deepmusic.songstruct as music\n\n\n# TODO: Some class from the encoder and decoder are really similar. Could they be merged ?\nclass DecoderNetwork:\n    """""" Predict a keyboard configuration at step t\n    This is just an abstract class\n    Warning: To encapsulate the weights in the right tf scope, they should be defined\n    within the build function\n    """"""\n    def __init__(self, args):\n        """"""\n        Args:\n            args: parameters of the model\n        """"""\n        self.args = args\n\n    def build(self):\n        """""" Initialize the weights of the model\n        """"""\n        pass\n\n    def init_state(self):\n        """""" Return the initial cell state\n        """"""\n        return None\n\n    def get_cell(self, prev_keyboard, prev_state_enco):\n        """""" Predict the next keyboard state\n        Args:\n            prev_keyboard (?): the previous keyboard configuration\n            prev_state_enco (?): the encoder output state\n        Return:\n            Tuple: A tuple containing the predicted keyboard configuration and last decoder state\n        """"""\n        raise NotImplementedError(\'Abstract class\')\n\n\nclass Rnn(DecoderNetwork):\n    """""" Predict a keyboard configuration at step t\n    Use a RNN to predict the next configuration\n    """"""\n    @staticmethod\n    def get_module_id():\n        return \'rnn\'\n\n    def __init__(self, args):\n        """"""\n        Args:\n            args: parameters of the model\n        """"""\n        super().__init__(args)\n        self.rnn_cell = None\n        self.project_key = None  # Fct which project the decoder output into a single key space\n\n    def build(self):\n        """""" Initialize the weights of the model\n        """"""\n        self.rnn_cell = tfutils.get_rnn_cell(self.args, ""deco_cell"")\n        self.project_key = tfutils.single_layer_perceptron([self.args.hidden_size, 1],\n                                                   \'project_key\')\n\n    def init_state(self):\n        """""" Return the initial cell state\n        """"""\n        return self.rnn_cell.zero_state(batch_size=self.args.batch_size, dtype=tf.float32)\n\n    def get_cell(self, prev_keyboard, prev_state_enco):\n        """""" a RNN decoder\n        See parent class for arguments details\n        """"""\n\n        axis = 1  # The first dimension is the batch, we split the keys\n        assert prev_keyboard.get_shape()[axis].value == music.NB_NOTES\n        inputs = tf.split(axis, music.NB_NOTES, prev_keyboard)\n\n        outputs, final_state = tf.nn.seq2seq.rnn_decoder(\n            decoder_inputs=inputs,\n            initial_state=prev_state_enco,\n            cell=self.rnn_cell\n            # TODO: Which loop function (should use prediction) ? : Should take the previous generated input/ground truth (as the global model loop_fct). Need to add a new bool placeholder\n        )\n\n        # Is it better to do the projection before or after the packing ?\n        next_keys = []\n        for output in outputs:\n            next_keys.append(self.project_key(output))\n\n        next_keyboard = tf.concat(axis, next_keys)\n\n        return next_keyboard, final_state\n\n\nclass Perceptron(DecoderNetwork):\n    """""" Single layer perceptron. Just a proof of concept for the architecture\n    """"""\n    @staticmethod\n    def get_module_id():\n        return \'perceptron\'\n\n    def __init__(self, args):\n        """"""\n        Args:\n            args: parameters of the model\n        """"""\n        super().__init__(args)\n\n        self.project_hidden = None  # Fct which decode the previous state\n        self.project_keyboard = None  # Fct which project the decoder output into the keyboard space\n\n    def build(self):\n        """""" Initialize the weights of the model\n        """"""\n        # For projecting on the keyboard space\n        self.project_hidden = tfutils.single_layer_perceptron([music.NB_NOTES, self.args.hidden_size],\n                                                              \'project_hidden\')\n\n        # For projecting on the keyboard space\n        self.project_keyboard = tfutils.single_layer_perceptron([self.args.hidden_size, music.NB_NOTES],\n                                                                \'project_keyboard\')  # Should we do the activation sigmoid here ?\n\n    def get_cell(self, prev_keyboard, prev_state_enco):\n        """""" Simple 1 hidden layer perceptron\n        See parent class for arguments details\n        """"""\n        # Don\'t change the state\n        next_state_deco = prev_state_enco  # Return the last state (Useful ?)\n\n        # Compute the next output\n        hidden_state = self.project_hidden(prev_keyboard)\n        next_keyboard = self.project_keyboard(hidden_state)  # Should we do the activation sigmoid here ? Maybe not because the loss function does it\n\n        return next_keyboard, next_state_deco\n\n\nclass Lstm(DecoderNetwork):\n    """""" Multi-layer Lstm. Just a wrapper around the official tf\n    """"""\n    @staticmethod\n    def get_module_id():\n        return \'lstm\'\n\n    def __init__(self, args, *module_args):\n        """"""\n        Args:\n            args: parameters of the model\n        """"""\n        super().__init__(args)\n        self.args = args\n\n        self.rnn_cell = None\n        self.project_keyboard = None  # Fct which project the decoder output into the ouput space\n\n    def build(self):\n        """""" Initialize the weights of the model\n        """"""\n        # TODO: Control over the the Cell using module arguments instead of global arguments (hidden_size and num_layer) !!\n        # RNN network\n        rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(self.args.hidden_size, state_is_tuple=True)  # Or GRUCell, LSTMCell(args.hidden_size)\n        if not self.args.test:  # TODO: Should use a placeholder instead\n            rnn_cell = tf.nn.rnn_cell.DropoutWrapper(rnn_cell, input_keep_prob=1.0, output_keep_prob=0.9)  # TODO: Custom values\n        rnn_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell] * self.args.num_layers, state_is_tuple=True)\n\n        self.rnn_cell = rnn_cell\n\n        # For projecting on the keyboard space\n        self.project_output = tfutils.single_layer_perceptron([self.args.hidden_size, 12 + 1],  # TODO: HACK: Input/output space hardcoded !!!\n                                                               \'project_output\')  # Should we do the activation sigmoid here ?\n\n    def init_state(self):\n        """""" Return the initial cell state\n        """"""\n        return self.rnn_cell.zero_state(batch_size=self.args.batch_size, dtype=tf.float32)\n\n    def get_cell(self, prev_input, prev_states):\n        """"""\n        """"""\n        next_output, next_state = self.rnn_cell(prev_input, prev_states[1])\n        next_output = self.project_output(next_output)\n        # No activation function here: SoftMax is computed by the loss function\n\n        return next_output, next_state\n'"
deepmusic/modules/encoder.py,7,"b'# Copyright 2016 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\n""""""\n\nimport tensorflow as tf\n\nimport deepmusic.tfutils as tfutils\nimport deepmusic.songstruct as music\n\n\nclass EncoderNetwork:\n    """""" From the previous keyboard configuration, prepare the state for the next one\n    Encode the keyboard configuration at a state t\n    This abstract class has no effect be is here to be subclasses\n    Warning: To encapsulate the weights in the right tf scope, they should be defined\n    within the build function\n    """"""\n    def __init__(self, args):\n        """"""\n        Args:\n            args: parameters of the model\n        """"""\n        self.args = args\n\n    def build(self):\n        """""" Initialize the weights of the model\n        """"""\n\n    def init_state(self):\n        """""" Return the initial cell state\n        """"""\n        return None\n\n    def get_cell(self, prev_keyboard, prev_state):\n        """""" Predict the next keyboard state\n        Args:\n            prev_keyboard (tf.Tensor): the previous keyboard configuration\n            prev_state (Tuple): the previous decoder state\n        Return:\n            tf.Tensor: the final encoder state\n        """"""\n        raise NotImplementedError(\'Abstract Class\')\n\n\nclass Identity(EncoderNetwork):\n    """""" Implement lookup for note embedding\n    """"""\n\n    @staticmethod\n    def get_module_id():\n        return \'identity\'\n\n    def __init__(self, args):\n        """"""\n        Args:\n            args: parameters of the model\n        """"""\n        super().__init__(args)\n\n    def get_cell(self, prev_keyboard, prev_state):\n        """""" Predict the next keyboard state\n        Args:\n            prev_keyboard (tf.Tensor): the previous keyboard configuration\n            prev_state (Tuple): the previous decoder state\n        Return:\n            tf.Tensor: the final encoder state\n        """"""\n        prev_state_enco, prev_state_deco = prev_state\n\n        # This simple class just pass the previous state\n        next_state_enco = prev_state_enco\n\n        return next_state_enco\n\n\nclass Rnn(EncoderNetwork):\n    """""" Read each keyboard configuration note by note and encode it\'s configuration\n    """"""\n    @staticmethod\n    def get_module_id():\n        return \'rnn\'\n\n    def __init__(self, args):\n        """"""\n        Args:\n            args: parameters of the model\n        """"""\n        super().__init__(args)\n        self.rnn_cell = None\n\n    def build(self):\n        """""" Initialize the weights of the model\n        """"""\n        self.rnn_cell = tfutils.get_rnn_cell(self.args, ""deco_cell"")\n\n    def init_state(self):\n        """""" Return the initial cell state\n        """"""\n        return self.rnn_cell.zero_state(batch_size=self.args.batch_size, dtype=tf.float32)\n\n    def get_cell(self, prev_keyboard, prev_state):\n        """""" a RNN encoder\n        See parent class for arguments details\n        """"""\n        prev_state_enco, prev_state_deco = prev_state\n\n        axis = 1  # The first dimension is the batch, we split the keys\n        assert prev_keyboard.get_shape()[axis].value == music.NB_NOTES\n        inputs = tf.split(axis, music.NB_NOTES, prev_keyboard)\n\n        _, final_state = tf.nn.rnn(\n            self.rnn_cell,\n            inputs,\n            initial_state=prev_state_deco\n        )\n\n        return final_state\n\n\nclass Embedding(EncoderNetwork):\n    """""" Implement lookup for note embedding\n    """"""\n    @staticmethod\n    def get_module_id():\n        return \'embedding\'\n\n    def __init__(self, args):\n        """"""\n        Args:\n            args: parameters of the model\n        """"""\n        super().__init__(args)\n\n    def build(self):\n        """""" Initialize the weights of the model\n        """"""\n\n    def init_state(self):\n        """""" Return the initial cell state\n        """"""\n\n    def get_cell(self, prev_keyboard, prev_state):\n        """""" a RNN encoder\n        See parent class for arguments details\n        """"""\n        # TODO:\n        return\n\n'"
deepmusic/modules/learningratepolicy.py,0,"b'# Copyright 2016 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nThe learning rate policy control the evolution of the learning rate during\nthe training\n""""""\n\n\nclass LearningRatePolicy:\n    """""" Contains the different policies for the learning rate decay\n    """"""\n    def __init__(self, args):\n        """"""\n        Args:\n            args: parameters of the model\n        """"""\n\n    def get_learning_rate(self, glob_step):\n        """""" Return the learning rate associated at the current training step\n        Args:\n            glob_step (int): Number of iterations since the beginning of training\n        Return:\n            float: the learning rate at the given step\n        """"""\n        raise NotImplementedError(\'Abstract class\')\n\n\nclass LearningRatePolicyOld:\n    """""" Contains the different policies for the learning rate decay\n    """"""\n    CST = \'cst\'  # Fixed learning rate over all steps (default behavior)\n    STEP = \'step\'  # We divide the learning rate every x iterations\n    EXPONENTIAL = \'exponential\'  #\n\n    @staticmethod\n    def get_policies():\n        """""" Return the list of the different modes\n        Useful when parsing the command lines arguments\n        """"""\n        return [\n            LearningRatePolicy.CST,\n            LearningRatePolicy.STEP,\n            LearningRatePolicy.EXPONENTIAL\n        ]\n\n    def __init__(self, args):\n        """"""\n        Args:\n            args: parameters of the model\n        """"""\n        self.learning_rate_fct = None\n\n        assert args.learning_rate\n        assert len(args.learning_rate) > 0\n\n        policy = args.learning_rate[0]\n\n        if policy == LearningRatePolicy.CST:\n            if not len(args.learning_rate) == 2:\n                raise ValueError(\n                    \'Learning rate cst policy should be on the form: {} lr_value\'.format(Model.LearningRatePolicy.CST))\n            self.learning_rate_init = float(args.learning_rate[1])\n            self.learning_rate_fct = self._lr_cst\n\n        elif policy == LearningRatePolicy.STEP:\n            if not len(args.learning_rate) == 3:\n                raise ValueError(\'Learning rate step policy should be on the form: {} lr_init decay_period\'.format(\n                    LearningRatePolicy.STEP))\n            self.learning_rate_init = float(args.learning_rate[1])\n            self.decay_period = int(args.learning_rate[2])\n            self.learning_rate_fct = self._lr_step\n\n        else:\n            raise ValueError(\'Unknown chosen learning rate policy: {}\'.format(policy))\n\n    def _lr_cst(self, glob_step):\n        """""" Just a constant learning rate\n        """"""\n        return self.learning_rate_init\n\n    def _lr_step(self, glob_step):\n        """""" Every decay period, the learning rate is divided by 2\n        """"""\n        return self.learning_rate_init / 2 ** (glob_step // self.decay_period)\n\n    def get_learning_rate(self, glob_step):\n        """""" Return the learning rate associated at the current training step\n        Args:\n            glob_step (int): Number of iterations since the beginning of training\n        Return:\n            float: the learning rate at the given step\n        """"""\n        return self.learning_rate_fct(glob_step)\n\n\nclass Cst(LearningRatePolicy):\n    """""" Fixed learning rate over all steps (default behavior)\n    """"""\n    @staticmethod\n    def get_module_id():\n        return \'cst\'\n\n    def __init__(self, args, lr=0.0001):\n        """"""\n        Args:\n            args: parameters of the model\n        """"""\n        self.lr = lr\n\n    def get_learning_rate(self, glob_step):\n        """""" Return the learning rate associated at the current training step\n        Args:\n            glob_step (int): Number of iterations since the beginning of training\n        Return:\n            float: the learning rate at the given step\n        """"""\n        return self.lr\n\n\nclass StepsWithDecay(LearningRatePolicy):\n    """"""\n    """"""\n\n    @staticmethod\n    def get_module_id():\n        return \'step\'\n\n\nclass Adaptive(LearningRatePolicy):\n    """""" Decrease the learning rate when training error\n    reach a plateau\n    """"""\n\n    @staticmethod\n    def get_module_id():\n        return \'adaptive\'\n'"
deepmusic/modules/loopprocessing.py,18,"b'# Copyright 2016 Conchylicultor. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\n""""""\n\nimport tensorflow as tf\n\n\nclass LoopProcessing:\n    """""" Apply some processing to the rnn loop which connect the output to\n    the next input.\n    Is called on the loop_function attribute of the rnn decoder\n    """"""\n    def __init__(self, args):\n        pass\n\n    def __call__(self, prev_output):\n        """""" Function which apply the preprocessing\n        Args:\n            prev_output (tf.Tensor): the ouput on which applying the transformation\n        Return:\n            tf.Ops: the processing operator\n        """"""\n        raise NotImplementedError(\'Abstract Class\')\n\n    def get_op(self):\n        """""" Return the chosen labels from the softmax distribution\n        Allows to reconstruct the song\n        """"""\n        return ()  # Empty tuple\n\n\nclass SampleSoftmax(LoopProcessing):\n    """""" Sample from the softmax distribution\n    """"""\n    @staticmethod\n    def get_module_id():\n        return \'sample_softmax\'\n\n    def __init__(self, args, *args_module):\n\n        self.temperature = args.temperature  # Control the sampling (more or less concervative predictions) (TODO: Could be a argument of modeule, but in this case will automatically be restored when --test, should also be in the save name)\n        self.chosen_labels = []  # Keep track of the chosen labels (to reconstruct the chosen song)\n\n    def __call__(self, prev_output):\n        """""" Use TODO formula\n        Args:\n            prev_output (tf.Tensor): the ouput on which applying the transformation\n        Return:\n            tf.Ops: the processing operator\n        """"""\n        # prev_output size: [batch_size, nb_labels]\n        nb_labels = prev_output.get_shape().as_list()[-1]\n\n        if False:  # TODO: Add option to control argmax\n            #label_draws = tf.argmax(prev_output, 1)\n            label_draws = tf.multinomial(tf.log(prev_output), 1)  # Draw 1 sample from the distribution\n            label_draws = tf.squeeze(label_draws, [1])\n            self.chosen_labels.append(label_draws)\n            next_input = tf.one_hot(label_draws, nb_labels)\n            return next_input\n        # Could use the Gumbel-Max trick to sample from a softmax distribution ?\n\n        soft_values = tf.exp(tf.div(prev_output, self.temperature))  # Pi = exp(pi/t)\n        # soft_values size: [batch_size, nb_labels]\n\n        normalisation_coeff = tf.expand_dims(tf.reduce_sum(soft_values, 1), -1)\n        # normalisation_coeff size: [batch_size, 1]\n        probs = tf.div(soft_values, normalisation_coeff + 1e-8)  # = Pi / sum(Pk)\n        # probs size: [batch_size, nb_labels]\n        label_draws = tf.multinomial(tf.log(probs), 1)  # Draw 1 sample from the log-probability distribution\n        # probs label_draws: [batch_size, 1]\n        label_draws = tf.squeeze(label_draws, [1])\n        # label_draws size: [batch_size,]\n        self.chosen_labels.append(label_draws)\n        next_input = tf.one_hot(label_draws, nb_labels)  # Reencode the next input vector\n        # next_input size: [batch_size, nb_labels]\n        return next_input\n\n    def get_op(self):\n        """""" Return the chosen labels from the softmax distribution\n        Allows to reconstruct the song\n        """"""\n        return self.chosen_labels\n\n\nclass ActivateScale(LoopProcessing):\n    """""" Activate using sigmoid and scale the prediction on [-1, 1]\n    """"""\n    @staticmethod\n    def get_module_id():\n        return \'activate_and_scale\'\n\n    def __init__(self, args):\n        pass\n\n    def __call__(X):\n        """""" Predict the output from prev and scale the result on [-1, 1]\n        Use sigmoid activation\n        Args:\n            X (tf.Tensor): the input\n        Return:\n            tf.Ops: the activate_and_scale operator\n        """"""\n        # TODO: Use tanh instead ? tanh=2*sigm(2*x)-1\n        with tf.name_scope(\'activate_and_scale\'):\n            return tf.sub(tf.mul(2.0, tf.nn.sigmoid(X)), 1.0)  # x_{i} = 2*sigmoid(y_{i-1}) - 1\n'"
