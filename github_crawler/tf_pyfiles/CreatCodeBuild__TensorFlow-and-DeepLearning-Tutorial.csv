file_path,api_count,code
PythonTips/coroutine.py,2,"b""###\r\n#2# \xe6\x88\x91\xe4\xbb\xac\xe6\x9d\xa5\xe5\x86\x99\xe4\xb8\x80\xe4\xb8\xaa\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84 Coroutine \xe6\xa1\x86\xe6\x9e\xb6\xe3\x80\x82\r\n#3# \xe8\x99\xbd\xe7\x84\xb6Python\xe6\x9c\x89Coroutine\xe5\xba\x93\xe3\x80\x82\r\n#3# \xe4\xbd\x86\xe6\x98\xaf\xef\xbc\x8c\xe7\xbc\x96\xe7\xa8\x8b\xe5\x98\x9b\xef\xbc\x8c\xe6\x9c\x80\xe4\xb8\xbb\xe8\xa6\x81\xe7\x9a\x84\xe6\x98\xaf\xe5\xbc\x80\xe5\xbf\x83\xef\xbc\x81\r\n###\r\n\r\nimport socket\t\t\t# on top of TCP\r\nimport time\r\nfrom selectors import DefaultSelector, EVENT_WRITE, EVENT_READ\r\n# select: System Call -----> watch the readiness of a unix-file(socket) i/o\r\n# only socket is possible in Windows\r\n# non-blocking socket\r\n\r\n\r\nselector = DefaultSelector()\r\n\r\n\r\nclass Future:\t\t\t\t# ~=Promise, return the caller scope a promise\r\n\t\t\t\t\t\t\t# about something in the future\r\n\tdef __init__(self):\r\n\t\tself.callbacks = []\r\n\r\n\tdef resolve(self):\t\t# on future event callback\r\n\t\tfor func in self.callbacks:\r\n\t\t\tfunc()\r\n\r\n\r\nclass Task:\t\t\t\t\t# responsible for calling next() on generators\r\n\t\t\t\t\t\t\t# in charge of the async functions\r\n\tdef __init__(self, gen, eventLoop):\r\n\t\tself.gen = gen\r\n\t\tself.step()\r\n\r\n\tdef step(self):\t\t\t# go to next step/next yield\r\n\t\ttry:\r\n\t\t\tf = next(self.gen)\r\n\t\t\tf.callbacks.append(self.step)\r\n\t\texcept StopIteration as e:\r\n\t\t\t# Task is finished\r\n\t\t\teventLoop.n_task -= 1\r\n\r\n\r\nclass EventLoop:\r\n\tdef __init__(self):\r\n\t\tself.n_task = 0\r\n\r\n\tdef add_task(self, generator):\r\n\t\tself.n_task += 1\r\n\t\tTask(generator, self)\r\n\r\n\tdef start(self):\r\n\t\twhile self.n_task > 0:\r\n\t\t\tevents = selector.select()\r\n\t\t\tfor event, mask in events:\r\n\t\t\t\tf = event.data\r\n\t\t\t\tf.resolve()\r\n\r\n\r\ndef pause(s, event):\r\n\tf = Future()\r\n\tselector.register(s.fileno(), event, data=f)\r\n\tyield f\t\t# pause this function\r\n\r\ndef resume(s):\r\n\tselector.unregister(s.fileno())\r\n\r\ndef async_await(s, event):\r\n\tyield from pause(s, event)\r\n\tresume(s)\r\n\r\ndef async_get(path):\r\n\ts = socket.socket()\r\n\ts.setblocking(False)\r\n\ttry:\r\n\t\ts.connect(('localhost', 3000))\r\n\texcept BlockingIOError as e:\r\n\t\tprint(e)\r\n\r\n\tyield from async_await(s, EVENT_WRITE)\r\n\r\n\trequest = 'GET %s HTTP/1.0\\r\\n\\r\\n' % path\r\n\ts.send(request.encode())\r\n\r\n\ttotalReceived = []\r\n\twhile True:\r\n\t\tyield from async_await(s, EVENT_READ)\r\n\r\n\t\treceived = s.recv(1000)\r\n\t\tif received:\r\n\t\t\ttotalReceived.append(received)\r\n\t\telse:\r\n\t\t\tbody = (b''.join(totalReceived)).decode()\r\n\t\t\tprint('--------------------------------------')\r\n\t\t\tprint(body)\r\n\t\t\tprint('--------------------------------------', 'Byte Received:', len(body), '\\n\\n')\r\n\t\t\treturn\r\n\r\n\r\nif __name__ == '__main__':\r\n\tstart = time.time()\r\n\teventLoop = EventLoop()\r\n\r\n\tfor i in range(50):\r\n\t\teventLoop.add_task(async_get('/super-slow'))\r\n\r\n\teventLoop.start()\r\n\r\n\tprint('%.1f sec' % (time.time() - start))\r\n"""
PythonTips/generator.py,0,"b""'''\r\n\xe4\xb9\x8b\xe5\x89\x8d\xe6\x9c\x89\xe5\x90\x8c\xe5\xad\xa6\xe6\x8f\x90\xe5\x88\xb0\xe8\xaf\xb4\xef\xbc\x8c\xe6\x83\xb3\xe7\x9f\xa5\xe9\x81\x93\xe6\x9b\xb4\xe5\xa4\x9a\xe5\x85\xb3\xe4\xba\x8eGenerator\xe7\x9a\x84\xe4\xb8\x9c\xe8\xa5\xbf\xe3\x80\x82\r\n\r\n\xe9\x82\xa3\xe4\xb9\x88\xe8\xbf\x99\xe4\xb8\x80\xe6\x9c\x9f\xe5\xb0\xb1\xe7\xbb\x99\xe5\xa4\xa7\xe5\xae\xb6\xe8\xae\xb2\xe4\xb8\x80\xe8\xae\xb2Generator\xe6\x98\xaf\xe4\xbb\x80\xe4\xb9\x88\xef\xbc\x9f\xe4\xb8\xba\xe4\xbb\x80\xe4\xb9\x88\xe8\xa6\x81\xe4\xbd\xbf\xe7\x94\xa8Generator\xef\xbc\x9f\r\n'''\r\n\r\ndef example1():\r\n\r\n\tdef generator_function():\r\n\t\tyield 1\t\t# \xe4\xb8\xad\xe6\x96\x87\xe9\x87\x8c \xe6\x9c\x89\xe4\xba\xa7\xe5\x87\xba \xe6\x88\x96\xe8\x80\x85 \xe8\xae\xa9\xe6\xad\xa5\r\n\t\tyield 2\r\n\t\treturn 3\r\n\r\n\tgeneratorObject = generator_function()\r\n\tprint(next(generatorObject))\r\n\tprint(next(generatorObject))\r\n\ttry:\r\n\t\tprint(next(generatorObject))\r\n\texcept:\r\n\t\tprint('We have reached the end of iteration')\r\n\r\ndef example2():\r\n\tdef generator_function(number):\r\n\t\twhile number > 0:\r\n\t\t\tyield number\r\n\t\t\tnumber -= 1\r\n\tfor number in generator_function(10):\r\n\t\tprint(number)\r\n\r\ndef example3():\r\n\tdef fun():\r\n\t\treturn 'fun'\r\n\tdef generator_function(number):\r\n\t\tif number > 0:\r\n\t\t\tyield number\r\n\t\t\tprint(fun())\r\n\t\t\tyield from generator_function(number-1)\r\n\tfor number in generator_function(10):\r\n\t\tprint(number)\r\n\r\n# example1()\r\n# example2()\r\n# example3()\r\n\r\n\r\n'''\r\nGenerator\r\nAsync Operation -> I/O Bound\r\n\t\t\t\t-> Memory Efficiency\r\n'''\r\n\r\nimport socket\r\nimport time\r\n\r\n\r\ndef get(path):\r\n\ts = socket.socket()\r\n\ts.connect(('localhost', 3000))\r\n\r\n\trequest = 'GET %s HTTP/1.0\\r\\n\\r\\n' % path\r\n\ts.send(request.encode())\r\n\r\n\tchunks = []\r\n\twhile True:\r\n\t\tchunk = s.recv(1000) # 1000 bytes\r\n\t\tif chunk:\r\n\t\t\tchunks.append(chunk)\r\n\t\telse:\r\n\t\t\tbody = (b''.join(chunks)).decode()\r\n\t\t\tprint('--------------------------------------')\r\n\t\t\tprint(body)\r\n\t\t\tprint('--------------------------------------\\n\\n')\r\n\t\t\treturn\r\n\r\nstart = time.time()\r\nget('/slow')\r\nget('/super-slow')\r\nget('/slower')\r\nprint('%.1f sec' % (time.time() - start))\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n#\r\n"""
Season1/1-3/run.py,21,"b""# encoding: utf-8\n# \xe4\xb8\xba\xe4\xba\x86 Python3 \xe7\x9a\x84\xe5\x85\xbc\xe5\xae\xb9\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe4\xbd\xa0\xe7\x94\xa8\xe7\x9a\x84 Python2.7\nfrom __future__ import print_function, division\nimport tensorflow as tf\n\nprint('Loaded TF version', tf.__version__, '\\n\\n')\n\n# Tensor \xe5\x9c\xa8\xe6\x95\xb0\xe5\xad\xa6\xe4\xb8\xad\xe6\x98\xaf\xe2\x80\x9c\xe5\xbc\xa0\xe9\x87\x8f\xe2\x80\x9d\n# \xe6\xa0\x87\xe9\x87\x8f\xef\xbc\x8c\xe7\x9f\xa2\xe9\x87\x8f/\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x8c\xe5\xbc\xa0\xe9\x87\x8f\n\n# \xe7\xae\x80\xe5\x8d\x95\xe5\x9c\xb0\xe7\x90\x86\xe8\xa7\xa3\n# \xe6\xa0\x87\xe9\x87\x8f\xe8\xa1\xa8\xe7\xa4\xba\xe5\x80\xbc\n# \xe7\x9f\xa2\xe9\x87\x8f\xe8\xa1\xa8\xe7\xa4\xba\xe4\xbd\x8d\xe7\xbd\xae\xef\xbc\x88\xe7\xa9\xba\xe9\x97\xb4\xe4\xb8\xad\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe7\x82\xb9\xef\xbc\x89\n# \xe5\xbc\xa0\xe9\x87\x8f\xe8\xa1\xa8\xe7\xa4\xba\xe6\x95\xb4\xe4\xb8\xaa\xe7\xa9\xba\xe9\x97\xb4\n\n# \xe4\xb8\x80\xe7\xbb\xb4\xe6\x95\xb0\xe7\xbb\x84\xe6\x98\xaf\xe7\x9f\xa2\xe9\x87\x8f\n# \xe5\xa4\x9a\xe7\xbb\xb4\xe6\x95\xb0\xe7\xbb\x84\xe6\x98\xaf\xe5\xbc\xa0\xe9\x87\x8f, \xe7\x9f\xa9\xe9\x98\xb5\xe4\xb9\x9f\xe6\x98\xaf\xe5\xbc\xa0\xe9\x87\x8f\n\n\n# 4\xe4\xb8\xaa\xe9\x87\x8d\xe8\xa6\x81\xe7\x9a\x84\xe7\xb1\xbb\xe5\x9e\x8b\n# @Variable\t\t\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe8\xb0\xb1\xe4\xb8\xad\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8f\n# @Tensor\t\t\xe4\xb8\x80\xe4\xb8\xaa\xe5\xa4\x9a\xe7\xbb\xb4\xe7\x9f\xa9\xe9\x98\xb5\xef\xbc\x8c\xe5\xb8\xa6\xe6\x9c\x89\xe5\xbe\x88\xe5\xa4\x9a\xe6\x96\xb9\xe6\xb3\x95\n# @Graph\t\t\xe4\xb8\x80\xe4\xb8\xaa\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe8\xb0\xb1\n# @Session\t\t\xe7\x94\xa8\xe6\x9d\xa5\xe8\xbf\x90\xe8\xa1\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe8\xb0\xb1\n\n\n# \xe4\xb8\x89\xe4\xb8\xaa\xe9\x87\x8d\xe8\xa6\x81\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\n\n# Variable \xe5\x8f\x98\xe9\x87\x8f\n# tf.Variable.__init__(\n#\tinitial_value=None, @Tensor\n#\ttrainable=True,\n#\tcollections=None,\n#\tvalidate_shape=True,\n#\tcaching_device=None,\n#\tname=None,\n#\tvariable_def=None,\n#\tdtype=None)\n# \xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x9aVariable\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaaClass\xef\xbc\x8cTensor\xe4\xb9\x9f\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaaClass\n\n# Constant \xe5\xb8\xb8\xe6\x95\xb0\n# tf.constant(value, dtype=None, shape=None, name='Const')\n# return: a constant @Tensor\n\n# Placeholder \xe6\x9a\x82\xe6\x97\xb6\xe5\x8f\x98\xe9\x87\x8f\xef\xbc\x9f\n# tf.placeholder(dtype, shape=None, name=None)\n# return: \xe4\xb8\x80\xe4\xb8\xaa\xe8\xbf\x98\xe5\xb0\x9a\xe6\x9c\xaa\xe5\xad\x98\xe5\x9c\xa8\xe7\x9a\x84 @Tensor\n\n\n\n# \xe8\xae\xa9\xe6\x88\x91\xe4\xbb\xac\xe7\x94\xa8\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe8\xb0\xb1\xe6\x9d\xa5\xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\x80\xe4\xba\x9b\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\n# + - * / \xe5\x9b\x9b\xe5\x88\x99\xe8\xbf\x90\xe7\xae\x97\ndef basic_operation():\n\tv1 = tf.Variable(10)\n\tv2 = tf.Variable(5)\n\taddv = v1 + v2\n\tprint(addv)\n\tprint(type(addv))\n\tprint(type(v1))\n\n\tc1 = tf.constant(10)\n\tc2 = tf.constant(5)\n\taddc = c1 + c2\n\tprint(addc)\n\tprint(type(addc))\n\tprint(type(c1))\n\n\t# \xe7\x94\xa8\xe6\x9d\xa5\xe8\xbf\x90\xe8\xa1\x8c\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe8\xb0\xb1\xe7\x9a\x84\xe5\xaf\xb9\xe8\xb1\xa1/\xe5\xae\x9e\xe4\xbe\x8b\xef\xbc\x9f\n\t# session is a runtime\n\tsess = tf.Session()\n\n\t# Variable -> \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96 -> \xe6\x9c\x89\xe5\x80\xbc\xe7\x9a\x84Tensor\n\ttf.initialize_all_variables().run(session=sess)\n\n\tprint('\xe5\x8f\x98\xe9\x87\x8f\xe6\x98\xaf\xe9\x9c\x80\xe8\xa6\x81\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\x9a\x84')\n\tprint('\xe5\x8a\xa0\xe6\xb3\x95(v1, v2) = ', addv.eval(session=sess))\n\tprint('\xe5\x8a\xa0\xe6\xb3\x95(v1, v2) = ', sess.run(addv))\n\tprint('\xe5\x8a\xa0\xe6\xb3\x95(c1, c2) = ', addc.eval(session=sess))\n\tprint('\\n\\n')\n\t#\xe8\xbf\x99\xe7\xa7\x8d\xe5\xae\x9a\xe4\xb9\x89\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe5\x86\x8d\xe6\x89\xa7\xe8\xa1\x8c\xe6\x93\x8d\xe4\xbd\x9c\xe7\x9a\x84\xe6\xa8\xa1\xe5\xbc\x8f\xe8\xa2\xab\xe7\xa7\xb0\xe4\xb9\x8b\xe4\xb8\xba\xe2\x80\x9c\xe7\xac\xa6\xe5\x8f\xb7\xe5\xbc\x8f\xe7\xbc\x96\xe7\xa8\x8b\xe2\x80\x9d Symbolic Programming\n\n\t# tf.Graph.__init__()\n\t# Creates a new, empty Graph.\n\tgraph = tf.Graph()\n\twith graph.as_default():\n\t\tvalue1 = tf.constant([1,2])\n\t\tvalue2 = tf.Variable([3,4])\n\t\tmul = value1 / value2\n\n\twith tf.Session(graph=graph) as mySess:\n\t\ttf.initialize_all_variables().run()\n\t\tprint('\xe4\xb8\x80\xe4\xb8\x80\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe9\x99\xa4\xe6\xb3\x95(value1, value2) = ', mySess.run(mul))\n\t\tprint('\xe4\xb8\x80\xe4\xb8\x80\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe9\x99\xa4\xe6\xb3\x95(value1, value2) = ', mul.eval())\n\n\t# tensor.eval(session=sess)\n\t# sess.run(tensor)\n\n\n# \xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98\xef\xbc\x9fplaceholder\xe6\x89\x8d\xe6\x98\xaf\xe7\x8e\x8b\xe9\x81\x93\n# def use_placeholder():\n\tgraph = tf.Graph()\n\twith graph.as_default():\n\t\tvalue1 = tf.placeholder(dtype=tf.float64)\n\t\tvalue2 = tf.Variable([3, 4], dtype=tf.float64)\n\t\tmul = value1 * value2\n\n\twith tf.Session(graph=graph) as mySess:\n\t\ttf.initialize_all_variables().run()\n\t\t# \xe6\x88\x91\xe4\xbb\xac\xe6\x83\xb3\xe8\xb1\xa1\xe4\xb8\x80\xe4\xb8\x8b\xe8\xbf\x99\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe6\x98\xaf\xe4\xbb\x8e\xe8\xbf\x9c\xe7\xa8\x8b\xe5\x8a\xa0\xe8\xbd\xbd\xe8\xbf\x9b\xe6\x9d\xa5\xe7\x9a\x84\n\t\t# \xe6\x96\x87\xe4\xbb\xb6\xef\xbc\x8c\xe7\xbd\x91\xe7\xbb\x9c\n\t\t# \xe5\x81\x87\xe8\xa3\x85\xe6\x98\xaf 10 GB\n\t\tvalue = load_from_remote()\n\t\tfor partialValue in load_partial(value, 2):\n\t\t\t# runResult = mySess.run(mul, feed_dict={value1: partialValue})\n\t\t\tevalResult = mul.eval(feed_dict={value1: partialValue})\n\t\t\tprint('\xe4\xb9\x98\xe6\xb3\x95(value1, value2) = ', runResult)\n\t\t# cross validation\n\ndef load_from_remote():\n\treturn [-x for x in range(1000)]\n\n\n# \xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84 Iterator\n# yield\xef\xbc\x8c generator function\ndef load_partial(value, step):\n\tindex = 0\n\twhile index < len(value):\n\t\tyield value[index:index+step]\n\t\tindex += step\n\treturn\n\nif __name__ == '__main__':\n\tbasic_operation()\n\t# use_placeholder()\n"""
Season1/10-11/dp.py,39,"b""# \xe4\xb8\xba\xe4\xba\x86 Python2 \xe7\x8e\xa9\xe5\xae\xb6\xe4\xbb\xac\nfrom __future__ import print_function, division\n\n# \xe7\xac\xac\xe4\xb8\x89\xe6\x96\xb9\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\n# \xe6\x88\x91\xe4\xbb\xac\xe8\x87\xaa\xe5\xb7\xb1\nimport load\n\ntrain_samples, train_labels = load._train_samples, load._train_labels\ntest_samples, test_labels = load._test_samples,  load._test_labels\n\nprint('Training set', train_samples.shape, train_labels.shape)\nprint('    Test set', test_samples.shape, test_labels.shape)\n\nimage_size = load.image_size\nnum_labels = load.num_labels\nnum_channels = load.num_channels\n\ndef get_chunk(samples, labels, chunkSize):\n\t'''\n\tIterator/Generator: get a batch of data\n\t\xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8/\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\x80\xe6\xac\xa1\xe5\x8f\xaa\xe5\xbe\x97\xe5\x88\xb0 chunkSize \xe8\xbf\x99\xe4\xb9\x88\xe5\xa4\x9a\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n\t\xe7\x94\xa8\xe4\xba\x8e for loop\xef\xbc\x8c just like range() function\n\t'''\n\tif len(samples) != len(labels):\n\t\traise Exception('Length of samples and labels must equal')\n\tstepStart = 0\t# initial step\n\ti = 0\n\twhile stepStart < len(samples):\n\t\tstepEnd = stepStart + chunkSize\n\t\tif stepEnd < len(samples):\n\t\t\tyield i, samples[stepStart:stepEnd], labels[stepStart:stepEnd]\n\t\t\ti += 1\n\t\tstepStart = stepEnd\n\n\nclass Network():\n\tdef __init__(self, num_hidden, batch_size):\n\t\t'''\n\t\t@num_hidden: \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\x9a\x84\xe8\x8a\x82\xe7\x82\xb9\xe6\x95\xb0\xe9\x87\x8f\n\t\t@batch_size\xef\xbc\x9a\xe5\x9b\xa0\xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe8\xa6\x81\xe8\x8a\x82\xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x88\x86\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\xe6\xaf\x8f\xe4\xb8\x80\xe6\x89\xb9\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x87\x8f\xe3\x80\x82\n\t\t'''\n\t\tself.batch_size = batch_size\n\t\tself.test_batch_size = 500\n\n\t\t# Hyper Parameters\n\t\tself.num_hidden = num_hidden\n\n\t\t# Graph Related\n\t\tself.graph = tf.Graph()\n\t\tself.tf_train_samples = None\n\t\tself.tf_train_labels = None\n\t\tself.tf_test_samples = None\n\t\tself.tf_test_labels = None\n\t\tself.tf_test_prediction = None\n\n\t\t# \xe7\xbb\x9f\xe8\xae\xa1\n\t\tself.merged = None\n\n\t\t# \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n\t\tself.define_graph()\n\t\tself.session = tf.Session(graph=self.graph)\n\t\tself.writer = tf.train.SummaryWriter('./board', self.graph)\n\n\tdef define_graph(self):\n\t\t'''\n\t\t\xe5\xae\x9a\xe4\xb9\x89\xe6\x88\x91\xe7\x9a\x84\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe8\xb0\xb1\n\t\t'''\n\t\twith self.graph.as_default():\n\t\t\t# \xe8\xbf\x99\xe9\x87\x8c\xe5\x8f\xaa\xe6\x98\xaf\xe5\xae\x9a\xe4\xb9\x89\xe5\x9b\xbe\xe8\xb0\xb1\xe4\xb8\xad\xe7\x9a\x84\xe5\x90\x84\xe7\xa7\x8d\xe5\x8f\x98\xe9\x87\x8f\n\t\t\twith tf.name_scope('inputs'):\n\t\t\t\tself.tf_train_samples = tf.placeholder(\n\t\t\t\t\ttf.float32, shape=(self.batch_size, image_size, image_size, num_channels), name='tf_train_samples'\n\t\t\t\t)\n\t\t\t\tself.tf_train_labels  = tf.placeholder(\n\t\t\t\t\ttf.float32, shape=(self.batch_size, num_labels), name='tf_train_labels'\n\t\t\t\t)\n\t\t\t\tself.tf_test_samples  = tf.placeholder(\n\t\t\t\t\ttf.float32, shape=(self.test_batch_size, image_size, image_size, num_channels), name='tf_test_samples'\n\t\t\t\t)\n\n\t\t\t# fully connected layer 1, fully connected\n\t\t\twith tf.name_scope('fc1'):\n\t\t\t\tfc1_weights = tf.Variable(\n\t\t\t\t\ttf.truncated_normal([image_size * image_size, self.num_hidden], stddev=0.1), name='fc1_weights'\n\t\t\t\t)\n\t\t\t\tfc1_biases = tf.Variable(tf.constant(0.1, shape=[self.num_hidden]), name='fc1_biases')\n\t\t\t\ttf.histogram_summary('fc1_weights', fc1_weights)\n\t\t\t\ttf.histogram_summary('fc1_biases', fc1_biases)\n\n\t\t\t# fully connected layer 2 --> output layer\n\t\t\twith tf.name_scope('fc2'):\n\t\t\t\tfc2_weights = tf.Variable(\n\t\t\t\t\ttf.truncated_normal([self.num_hidden, num_labels], stddev=0.1), name='fc2_weights'\n\t\t\t\t)\n\t\t\t\tfc2_biases = tf.Variable(tf.constant(0.1, shape=[num_labels]), name='fc2_biases')\n\t\t\t\ttf.histogram_summary('fc2_weights', fc2_weights)\n\t\t\t\ttf.histogram_summary('fc2_biases', fc2_biases)\n\n\n\t\t\t# \xe6\x83\xb3\xe5\x9c\xa8\xe6\x9d\xa5\xe5\xae\x9a\xe4\xb9\x89\xe5\x9b\xbe\xe8\xb0\xb1\xe7\x9a\x84\xe8\xbf\x90\xe7\xae\x97\n\t\t\tdef model(data):\n\t\t\t\t# fully connected layer 1\n\t\t\t\tshape = data.get_shape().as_list()\n\t\t\t\treshape = tf.reshape(data, [shape[0], shape[1] * shape[2] * shape[3]])\n\n\t\t\t\twith tf.name_scope('fc1_model'):\n\t\t\t\t\tfc1_model = tf.matmul(reshape, fc1_weights) + fc1_biases\n\t\t\t\t\thidden = tf.nn.relu(fc1_model)\n\n\t\t\t\t# fully connected layer 2\n\t\t\t\twith tf.name_scope('fc2_model'):\n\t\t\t\t\treturn tf.matmul(hidden, fc2_weights) + fc2_biases\n\n\t\t\t# Training computation.\n\t\t\tlogits = model(self.tf_train_samples)\n\t\t\twith tf.name_scope('loss'):\n\t\t\t\tself.loss = tf.reduce_mean(\n\t\t\t\t\ttf.nn.softmax_cross_entropy_with_logits(logits, self.tf_train_labels)\n\t\t\t\t)\n\t\t\t\ttf.scalar_summary('Loss', self.loss)\n\n\n\t\t\t# Optimizer.\n\t\t\twith tf.name_scope('optimizer'):\n\t\t\t\tself.optimizer = tf.train.GradientDescentOptimizer(0.0001).minimize(self.loss)\n\n\t\t\t# Predictions for the training, validation, and test data.\n\t\t\twith tf.name_scope('predictions'):\n\t\t\t\tself.train_prediction = tf.nn.softmax(logits, name='train_prediction')\n\t\t\t\tself.test_prediction = tf.nn.softmax(model(self.tf_test_samples), name='test_prediction')\n\n\t\t\tself.merged = tf.merge_all_summaries()\n\n\tdef run(self):\n\t\t'''\n\t\t\xe7\x94\xa8\xe5\x88\xb0Session\n\t\t'''\n\t\t# private function\n\t\tdef print_confusion_matrix(confusionMatrix):\n\t\t\tprint('Confusion    Matrix:')\n\t\t\tfor i, line in enumerate(confusionMatrix):\n\t\t\t\tprint(line, line[i]/np.sum(line))\n\t\t\ta = 0\n\t\t\tfor i, column in enumerate(np.transpose(confusionMatrix, (1, 0))):\n\t\t\t\ta += (column[i]/np.sum(column))*(np.sum(column)/26000)\n\t\t\t\tprint(column[i]/np.sum(column),)\n\t\t\tprint('\\n',np.sum(confusionMatrix), a)\n\n\n\t\twith self.session as session:\n\t\t\ttf.initialize_all_variables().run()\n\n\t\t\t### \xe8\xae\xad\xe7\xbb\x83\n\t\t\tprint('Start Training')\n\t\t\t# batch 1000\n\t\t\tfor i, samples, labels in get_chunk(train_samples, train_labels, chunkSize=self.batch_size):\n\t\t\t\t_, l, predictions, summary = session.run(\n\t\t\t\t\t[self.optimizer, self.loss, self.train_prediction, self.merged],\n\t\t\t\t\tfeed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels}\n\t\t\t\t)\n\t\t\t\tself.writer.add_summary(summary, i)\n\t\t\t\t# labels is True Labels\n\t\t\t\taccuracy, _ = self.accuracy(predictions, labels)\n\t\t\t\tif i % 50 == 0:\n\t\t\t\t\tprint('Minibatch loss at step %d: %f' % (i, l))\n\t\t\t\t\tprint('Minibatch accuracy: %.1f%%' % accuracy)\n\t\t\t###\n\n\t\t\t### \xe6\xb5\x8b\xe8\xaf\x95\n\t\t\taccuracies = []\n\t\t\tconfusionMatrices = []\n\t\t\tfor i, samples, labels in get_chunk(test_samples, test_labels, chunkSize=self.test_batch_size):\n\t\t\t\tresult = self.test_prediction.eval(feed_dict={self.tf_test_samples: samples})\n\t\t\t\taccuracy, cm = self.accuracy(result, labels, need_confusion_matrix=True)\n\t\t\t\taccuracies.append(accuracy)\n\t\t\t\tconfusionMatrices.append(cm)\n\t\t\t\tprint('Test Accuracy: %.1f%%' % accuracy)\n\t\t\tprint(' Average  Accuracy:', np.average(accuracies))\n\t\t\tprint('Standard Deviation:', np.std(accuracies))\n\t\t\tprint_confusion_matrix(np.add.reduce(confusionMatrices))\n\t\t\t###\n\n\tdef accuracy(self, predictions, labels, need_confusion_matrix=False):\n\t\t'''\n\t\t\xe8\xae\xa1\xe7\xae\x97\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87\xe4\xb8\x8e\xe5\x8f\xac\xe5\x9b\x9e\xe7\x8e\x87\n\t\t@return: accuracy and confusionMatrix as a tuple\n\t\t'''\n\t\t_predictions = np.argmax(predictions, 1)\n\t\t_labels = np.argmax(labels, 1)\n\t\tcm = confusion_matrix(_labels, _predictions) if need_confusion_matrix else None\n\t\t# == is overloaded for numpy array\n\t\taccuracy = (100.0 * np.sum(_predictions == _labels) / predictions.shape[0])\n\t\treturn accuracy, cm\n\n\nif __name__ == '__main__':\n\tnet = Network(num_hidden=128, batch_size=100)\n\tnet.run()\n"""
Season1/10-11/load.py,0,"b""# encoding:utf-8\n# Python2 \xe5\x85\xbc\xe5\xae\xb9\nfrom __future__ import print_function, division\nfrom scipy.io import loadmat as load\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef reformat(samples, labels):\n\t# \xe6\x94\xb9\xe5\x8f\x98\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\n\t#  0       1       2      3          3       0       1      2\n\t# (\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0) -> (\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0)\n\tnew = np.transpose(samples, (3, 0, 1, 2)).astype(np.float32)\n\n\t# labels \xe5\x8f\x98\xe6\x88\x90 one-hot encoding, [2] -> [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n\t# digit 0 , represented as 10\n\t# labels \xe5\x8f\x98\xe6\x88\x90 one-hot encoding, [10] -> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\tlabels = np.array([x[0] for x in labels])\t# slow code, whatever\n\tone_hot_labels = []\n\tfor num in labels:\n\t\tone_hot = [0.0] * 10\n\t\tif num == 10:\n\t\t\tone_hot[0] = 1.0\n\t\telse:\n\t\t\tone_hot[num] = 1.0\n\t\tone_hot_labels.append(one_hot)\n\tlabels = np.array(one_hot_labels).astype(np.float32)\n\treturn new, labels\n\ndef normalize(samples):\n\t'''\n\t\xe5\xb9\xb6\xe4\xb8\x94\xe7\x81\xb0\xe5\xba\xa6\xe5\x8c\x96: \xe4\xbb\x8e\xe4\xb8\x89\xe8\x89\xb2\xe9\x80\x9a\xe9\x81\x93 -> \xe5\x8d\x95\xe8\x89\xb2\xe9\x80\x9a\xe9\x81\x93     \xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98 + \xe5\x8a\xa0\xe5\xbf\xab\xe8\xae\xad\xe7\xbb\x83\xe9\x80\x9f\xe5\xba\xa6\n\t(R + G + B) / 3\n\t\xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe4\xbb\x8e 0 ~ 255 \xe7\xba\xbf\xe6\x80\xa7\xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0 -1.0 ~ +1.0\n\t@samples: numpy array\n\t'''\n\ta = np.add.reduce(samples, keepdims=True, axis=3)  # shape (\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0)\n\ta = a/3.0\n\treturn a/128.0 - 1.0\n\n\ndef distribution(labels, name):\n\t# \xe6\x9f\xa5\xe7\x9c\x8b\xe4\xb8\x80\xe4\xb8\x8b\xe6\xaf\x8f\xe4\xb8\xaalabel\xe7\x9a\x84\xe5\x88\x86\xe5\xb8\x83\xef\xbc\x8c\xe5\x86\x8d\xe7\x94\xbb\xe4\xb8\xaa\xe7\xbb\x9f\xe8\xae\xa1\xe5\x9b\xbe\n\t# keys:\n\t# 0\n\t# 1\n\t# 2\n\t# ...\n\t# 9\n\tcount = {}\n\tfor label in labels:\n\t\tkey = 0 if label[0] == 10 else label[0]\n\t\tif key in count:\n\t\t\tcount[key] += 1\n\t\telse:\n\t\t\tcount[key] = 1\n\tx = []\n\ty = []\n\tfor k, v in count.items():\n\t\t# print(k, v)\n\t\tx.append(k)\n\t\ty.append(v)\n\n\ty_pos = np.arange(len(x))\n\tplt.bar(y_pos, y, align='center', alpha=0.5)\n\tplt.xticks(y_pos, x)\n\tplt.ylabel('Count')\n\tplt.title(name + ' Label Distribution')\n\tplt.show()\n\ndef inspect(dataset, labels, i):\n\t# \xe6\x98\xbe\xe7\xa4\xba\xe5\x9b\xbe\xe7\x89\x87\xe7\x9c\x8b\xe7\x9c\x8b\n\tif dataset.shape[3] == 1:\n\t\tshape = dataset.shape\n\t\tdataset = dataset.reshape(shape[0], shape[1], shape[2])\n\tprint(labels[i])\n\tplt.imshow(dataset[i])\n\tplt.show()\n\n\ntrain = load('../data/train_32x32.mat')\ntest = load('../data/test_32x32.mat')\n# extra = load('../data/extra_32x32.mat')\n\n# print('Train Samples Shape:', train['X'].shape)\n# print('Train  Labels Shape:', train['y'].shape)\n\n# print('Train Samples Shape:', test['X'].shape)\n# print('Train  Labels Shape:', test['y'].shape)\n\n# print('Train Samples Shape:', extra['X'].shape)\n# print('Train  Labels Shape:', extra['y'].shape)\n\ntrain_samples = train['X']\ntrain_labels = train['y']\ntest_samples = test['X']\ntest_labels = test['y']\n# extra_samples = extra['X']\n# extra_labels = extra['y']\n\nn_train_samples, _train_labels = reformat(train_samples, train_labels)\nn_test_samples, _test_labels = reformat(test_samples, test_labels)\n\n_train_samples = normalize(n_train_samples)\n_test_samples = normalize(n_test_samples)\n\nnum_labels = 10\nimage_size = 32\nnum_channels = 1\n\nif __name__ == '__main__':\n\t# \xe6\x8e\xa2\xe7\xb4\xa2\xe6\x95\xb0\xe6\x8d\xae\n\tpass\n\t# inspect(_train_samples, _train_labels, 1234)\n\t# _train_samples = normalize(_train_samples)\n\t# inspect(_train_samples, _train_labels, 1234)\n\t# distribution(train_labels, 'Train Labels')\n\t# distribution(test_labels, 'Test Labels')\n"""
Season1/12-15/dp.py,79,"b""# \xe4\xb8\xba\xe4\xba\x86 Python2 \xe7\x8e\xa9\xe5\xae\xb6\xe4\xbb\xac\nfrom __future__ import print_function, division\n\n# \xe7\xac\xac\xe4\xb8\x89\xe6\x96\xb9\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\n# \xe6\x88\x91\xe4\xbb\xac\xe8\x87\xaa\xe5\xb7\xb1\nimport load\n\ntrain_samples, train_labels = load._train_samples, load._train_labels\ntest_samples, test_labels = load._test_samples,  load._test_labels\n\nprint('Training set', train_samples.shape, train_labels.shape)\nprint('    Test set', test_samples.shape, test_labels.shape)\n\nimage_size = load.image_size\nnum_labels = load.num_labels\nnum_channels = load.num_channels\n\n\ndef get_chunk(samples, labels, chunkSize):\n\t'''\n\tIterator/Generator: get a batch of data\n\t\xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8/\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\x80\xe6\xac\xa1\xe5\x8f\xaa\xe5\xbe\x97\xe5\x88\xb0 chunkSize \xe8\xbf\x99\xe4\xb9\x88\xe5\xa4\x9a\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n\t\xe7\x94\xa8\xe4\xba\x8e for loop\xef\xbc\x8c just like range() function\n\t'''\n\tif len(samples) != len(labels):\n\t\traise Exception('Length of samples and labels must equal')\n\tstepStart = 0\t# initial step\n\ti = 0\n\twhile stepStart < len(samples):\n\t\tstepEnd = stepStart + chunkSize\n\t\tif stepEnd < len(samples):\n\t\t\tyield i, samples[stepStart:stepEnd], labels[stepStart:stepEnd]\n\t\t\ti += 1\n\t\tstepStart = stepEnd\n\n\nclass Network():\n\tdef __init__(self, num_hidden, batch_size, conv_depth, patch_size, pooling_scale):\n\t\t'''\n\t\t@num_hidden: \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\x9a\x84\xe8\x8a\x82\xe7\x82\xb9\xe6\x95\xb0\xe9\x87\x8f\n\t\t@batch_size\xef\xbc\x9a\xe5\x9b\xa0\xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe8\xa6\x81\xe8\x8a\x82\xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x88\x86\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\xe6\xaf\x8f\xe4\xb8\x80\xe6\x89\xb9\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x87\x8f\xe3\x80\x82\n\t\t'''\n\t\tself.batch_size = batch_size\n\t\tself.test_batch_size = 500\n\n\t\t# Hyper Parameters\n\t\tself.num_hidden = num_hidden\n\t\tself.patch_size = patch_size # \xe6\xbb\x91\xe7\xaa\x97\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\n\t\tself.conv1_depth = conv_depth\n\t\tself.conv2_depth = conv_depth\n\t\tself.conv3_depth = conv_depth\n\t\tself.conv4_depth = conv_depth\n\t\tself.last_conv_depth = self.conv4_depth\n\t\tself.pooling_scale = pooling_scale\n\t\tself.pooling_stride = self.pooling_scale\t# Max Pooling Stride\n\n\t\t# Graph Related\n\t\tself.graph = tf.Graph()\n\t\tself.tf_train_samples = None\n\t\tself.tf_train_labels = None\n\t\tself.tf_test_samples = None\n\t\tself.tf_test_labels = None\n\t\tself.tf_test_prediction = None\n\n\t\t# \xe7\xbb\x9f\xe8\xae\xa1\n\t\tself.merged = None\n\t\tself.train_summaries = []\n\t\tself.test_summaries = []\n\n\t\t# \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n\t\tself.define_graph()\n\t\tself.session = tf.Session(graph=self.graph)\n\t\tself.writer = tf.train.SummaryWriter('./board', self.graph)\n\n\tdef define_graph(self):\n\t\t'''\n\t\t\xe5\xae\x9a\xe4\xb9\x89\xe6\x88\x91\xe7\x9a\x84\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe8\xb0\xb1\n\t\t'''\n\t\twith self.graph.as_default():\n\t\t\t# \xe8\xbf\x99\xe9\x87\x8c\xe5\x8f\xaa\xe6\x98\xaf\xe5\xae\x9a\xe4\xb9\x89\xe5\x9b\xbe\xe8\xb0\xb1\xe4\xb8\xad\xe7\x9a\x84\xe5\x90\x84\xe7\xa7\x8d\xe5\x8f\x98\xe9\x87\x8f\n\t\t\twith tf.name_scope('inputs'):\n\t\t\t\tself.tf_train_samples = tf.placeholder(\n\t\t\t\t\ttf.float32, shape=(self.batch_size, image_size, image_size, num_channels), name='tf_train_samples'\n\t\t\t\t)\n\t\t\t\tself.tf_train_labels  = tf.placeholder(\n\t\t\t\t\ttf.float32, shape=(self.batch_size, num_labels), name='tf_train_labels'\n\t\t\t\t)\n\t\t\t\tself.tf_test_samples  = tf.placeholder(\n\t\t\t\t\ttf.float32, shape=(self.test_batch_size, image_size, image_size, num_channels), name='tf_test_samples'\n\t\t\t\t)\n\n\t\t\twith tf.name_scope('conv1'):\n\t\t\t\tconv1_weights = tf.Variable(\n\t\t\t\t\ttf.truncated_normal([self.patch_size, self.patch_size, num_channels, self.conv1_depth], stddev=0.1))\n\t\t\t\tconv1_biases = tf.Variable(tf.zeros([self.conv1_depth]))\n\n\t\t\twith tf.name_scope('conv2'):\n\t\t\t\tconv2_weights = tf.Variable(\n\t\t\t\t\ttf.truncated_normal([self.patch_size, self.patch_size, self.conv1_depth, self.conv2_depth], stddev=0.1))\n\t\t\t\tconv2_biases = tf.Variable(tf.constant(0.1, shape=[self.conv2_depth]))\n\n\t\t\twith tf.name_scope('conv3'):\n\t\t\t\tconv3_weights = tf.Variable(\n\t\t\t\t\ttf.truncated_normal([self.patch_size, self.patch_size, self.conv2_depth, self.conv3_depth], stddev=0.1))\n\t\t\t\tconv3_biases = tf.Variable(tf.constant(0.1, shape=[self.conv3_depth]))\n\n\t\t\twith tf.name_scope('conv4'):\n\t\t\t\tconv4_weights = tf.Variable(\n\t\t\t\t\ttf.truncated_normal([self.patch_size, self.patch_size, self.conv3_depth, self.conv4_depth], stddev=0.1))\n\t\t\t\tconv4_biases = tf.Variable(tf.constant(0.1, shape=[self.conv4_depth]))\n\n\t\t\t# fully connected layer 1, fully connected\n\t\t\twith tf.name_scope('fc1'):\n\t\t\t\tdown_scale = self.pooling_scale ** 2\t# because we do 2 times pooling of stride 2\n\t\t\t\tfc1_weights = tf.Variable(\n\t\t\t\t\ttf.truncated_normal(\n\t\t\t\t\t\t[(image_size // down_scale) * (image_size // down_scale) * self.last_conv_depth, self.num_hidden], stddev=0.1))\n\t\t\t\tfc1_biases = tf.Variable(tf.constant(0.1, shape=[self.num_hidden]))\n\n\t\t\t\tself.train_summaries.append(tf.histogram_summary('fc1_weights', fc1_weights))\n\t\t\t\tself.train_summaries.append(tf.histogram_summary('fc1_biases', fc1_biases))\n\n\t\t\t# fully connected layer 2 --> output layer\n\t\t\twith tf.name_scope('fc2'):\n\t\t\t\tfc2_weights = tf.Variable(tf.truncated_normal([self.num_hidden, num_labels], stddev=0.1), name='fc2_weights')\n\t\t\t\tfc2_biases = tf.Variable(tf.constant(0.1, shape=[num_labels]), name='fc2_biases')\n\t\t\t\tself.train_summaries.append(tf.histogram_summary('fc2_weights', fc2_weights))\n\t\t\t\tself.train_summaries.append(tf.histogram_summary('fc2_biases', fc2_biases))\n\n\t\t\t# \xe6\x83\xb3\xe5\x9c\xa8\xe6\x9d\xa5\xe5\xae\x9a\xe4\xb9\x89\xe5\x9b\xbe\xe8\xb0\xb1\xe7\x9a\x84\xe8\xbf\x90\xe7\xae\x97\n\t\t\tdef model(data, train=True):\n\t\t\t\t'''\n\t\t\t\t@data: original inputs\n\t\t\t\t@return: logits\n\t\t\t\t'''\n\t\t\t\twith tf.name_scope('conv1_model'):\n\t\t\t\t\twith tf.name_scope('convolution'):\n\t\t\t\t\t\tconv1 = tf.nn.conv2d(data, filter=conv1_weights, strides=[1, 1, 1, 1], padding='SAME')\n\t\t\t\t\t\taddition = conv1 + conv1_biases\n\t\t\t\t\thidden = tf.nn.relu(addition)\n\n\t\t\t\t\tif not train:\n\t\t\t\t\t\t# transpose the output of an activation to image\n\t\t\t\t\t\t# conv1_activation_relu shape: (8, 32, 32, 64)\n\t\t\t\t\t\t# 64 filter maps from this convolution, that's 64 grayscale images\n\t\t\t\t\t\t# image size is 32x32\n\t\t\t\t\t\t# 8 is the batch_size, which means 8 times of convolution was performed\n\t\t\t\t\t\t# just use the last one (index 7) as record\n\n\t\t\t\t\t\tfilter_map = hidden[-1]\n\t\t\t\t\t\tfilter_map = tf.transpose(filter_map, perm=[2, 0, 1])\n\t\t\t\t\t\tfilter_map = tf.reshape(filter_map, (self.conv1_depth, 32, 32, 1))\n\t\t\t\t\t\tself.test_summaries.append(tf.image_summary('conv1_relu', tensor=filter_map, max_images=self.conv1_depth))\n\n\t\t\t\twith tf.name_scope('conv2_model'):\n\t\t\t\t\twith tf.name_scope('convolution'):\n\t\t\t\t\t\tconv2 = tf.nn.conv2d(hidden, filter=conv2_weights, strides=[1, 1, 1, 1], padding='SAME')\n\t\t\t\t\t\taddition = conv2 + conv2_biases\n\t\t\t\t\thidden = tf.nn.relu(addition)\n\t\t\t\t\thidden = tf.nn.max_pool(\n\t\t\t\t\t\thidden,\n\t\t\t\t\t\tksize=[1,self.pooling_scale,self.pooling_scale,1],\n\t\t\t\t\t\tstrides=[1,self.pooling_stride,self.pooling_stride,1],\n\t\t\t\t\t\tpadding='SAME')\n\n\t\t\t\twith tf.name_scope('conv3_model'):\n\t\t\t\t\twith tf.name_scope('convolution'):\n\t\t\t\t\t\tconv3 = tf.nn.conv2d(hidden, filter=conv3_weights, strides=[1, 1, 1, 1], padding='SAME')\n\t\t\t\t\t\taddition = conv3 + conv3_biases\n\t\t\t\t\thidden = tf.nn.relu(addition)\n\n\t\t\t\twith tf.name_scope('conv4_model'):\n\t\t\t\t\twith tf.name_scope('convolution'):\n\t\t\t\t\t\tconv4 = tf.nn.conv2d(hidden, filter=conv4_weights, strides=[1, 1, 1, 1], padding='SAME')\n\t\t\t\t\t\taddition = conv4 + conv4_biases\n\t\t\t\t\thidden = tf.nn.relu(addition)\n\t\t\t\t\t# if not train:\n\t\t\t\t\t# \tfilter_map = hidden[-1]\n\t\t\t\t\t# \tfilter_map = tf.transpose(filter_map, perm=[2, 0, 1])\n\t\t\t\t\t# \tfilter_map = tf.reshape(filter_map, (self.conv4_depth, 16, 16, 1))\n\t\t\t\t\t# \ttf.image_summary('conv4_relu', tensor=filter_map, max_images=self.conv4_depth)\n\t\t\t\t\thidden = tf.nn.max_pool(\n\t\t\t\t\t\thidden,\n\t\t\t\t\t\tksize=[1,self.pooling_scale,self.pooling_scale,1],\n\t\t\t\t\t\tstrides=[1,self.pooling_stride,self.pooling_stride,1],\n\t\t\t\t\t\tpadding='SAME')\n\n\n\t\t\t\t# fully connected layer 1\n\t\t\t\tshape = hidden.get_shape().as_list()\n\t\t\t\treshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n\n\t\t\t\twith tf.name_scope('fc1_model'):\n\t\t\t\t\tfc1_model = tf.matmul(reshape, fc1_weights) + fc1_biases\n\t\t\t\t\thidden = tf.nn.relu(fc1_model)\n\n\t\t\t\t# fully connected layer 2\n\t\t\t\twith tf.name_scope('fc2_model'):\n\t\t\t\t\treturn tf.matmul(hidden, fc2_weights) + fc2_biases\n\n\t\t\t# Training computation.\n\t\t\tlogits = model(self.tf_train_samples)\n\t\t\twith tf.name_scope('loss'):\n\t\t\t\tself.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, self.tf_train_labels))\n\t\t\t\tself.train_summaries.append(tf.scalar_summary('Loss', self.loss))\n\n\t\t\t# Optimizer.\n\t\t\twith tf.name_scope('optimizer'):\n\t\t\t\tself.optimizer = tf.train.GradientDescentOptimizer(0.0001).minimize(self.loss)\n\n\t\t\t# Predictions for the training, validation, and test data.\n\t\t\twith tf.name_scope('train'):\n\t\t\t\tself.train_prediction = tf.nn.softmax(logits, name='train_prediction')\n\t\t\twith tf.name_scope('test'):\n\t\t\t\tself.test_prediction = tf.nn.softmax(model(self.tf_test_samples, train=False), name='test_prediction')\n\n\t\t\tself.merged_train_summary = tf.merge_summary(self.train_summaries)\n\t\t\tself.merged_test_summary = tf.merge_summary(self.test_summaries)\n\n\tdef run(self):\n\t\t'''\n\t\t\xe7\x94\xa8\xe5\x88\xb0Session\n\t\t'''\n\t\t# private function\n\t\tdef print_confusion_matrix(confusionMatrix):\n\t\t\tprint('Confusion    Matrix:')\n\t\t\tfor i, line in enumerate(confusionMatrix):\n\t\t\t\tprint(line, line[i]/np.sum(line))\n\t\t\ta = 0\n\t\t\tfor i, column in enumerate(np.transpose(confusionMatrix, (1, 0))):\n\t\t\t\ta += (column[i]/np.sum(column))*(np.sum(column)/26000)\n\t\t\t\tprint(column[i]/np.sum(column),)\n\t\t\tprint('\\n',np.sum(confusionMatrix), a)\n\n\n\t\twith self.session as session:\n\t\t\ttf.initialize_all_variables().run()\n\n\t\t\t### \xe8\xae\xad\xe7\xbb\x83\n\t\t\tprint('Start Training')\n\t\t\t# batch 1000\n\t\t\tfor i, samples, labels in get_chunk(train_samples, train_labels, chunkSize=self.batch_size):\n\t\t\t\t_, l, predictions, summary = session.run(\n\t\t\t\t\t[self.optimizer, self.loss, self.train_prediction, self.merged_train_summary],\n\t\t\t\t\tfeed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels}\n\t\t\t\t)\n\t\t\t\tself.writer.add_summary(summary, i)\n\t\t\t\t# labels is True Labels\n\t\t\t\taccuracy, _ = self.accuracy(predictions, labels)\n\t\t\t\tif i % 50 == 0:\n\t\t\t\t\tprint('Minibatch loss at step %d: %f' % (i, l))\n\t\t\t\t\tprint('Minibatch accuracy: %.1f%%' % accuracy)\n\t\t\t###\n\n\t\t\t### \xe6\xb5\x8b\xe8\xaf\x95\n\t\t\taccuracies = []\n\t\t\tconfusionMatrices = []\n\t\t\tfor i, samples, labels in get_chunk(test_samples, test_labels, chunkSize=self.test_batch_size):\n\t\t\t\tresult, summary = session.run(\n\t\t\t\t\t[self.test_prediction, self.merged_test_summary],\n\t\t\t\t\tfeed_dict={self.tf_test_samples: samples}\n\t\t\t\t)\n\t\t\t\t# result = self.test_prediction.eval()\n\t\t\t\tself.writer.add_summary(summary, i)\n\t\t\t\taccuracy, cm = self.accuracy(result, labels, need_confusion_matrix=True)\n\t\t\t\taccuracies.append(accuracy)\n\t\t\t\tconfusionMatrices.append(cm)\n\t\t\t\tprint('Test Accuracy: %.1f%%' % accuracy)\n\t\t\tprint(' Average  Accuracy:', np.average(accuracies))\n\t\t\tprint('Standard Deviation:', np.std(accuracies))\n\t\t\tprint_confusion_matrix(np.add.reduce(confusionMatrices))\n\t\t\t###\n\n\tdef accuracy(self, predictions, labels, need_confusion_matrix=False):\n\t\t'''\n\t\t\xe8\xae\xa1\xe7\xae\x97\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87\xe4\xb8\x8e\xe5\x8f\xac\xe5\x9b\x9e\xe7\x8e\x87\n\t\t@return: accuracy and confusionMatrix as a tuple\n\t\t'''\n\t\t_predictions = np.argmax(predictions, 1)\n\t\t_labels = np.argmax(labels, 1)\n\t\tcm = confusion_matrix(_labels, _predictions) if need_confusion_matrix else None\n\t\t# == is overloaded for numpy array\n\t\taccuracy = (100.0 * np.sum(_predictions == _labels) / predictions.shape[0])\n\t\treturn accuracy, cm\n\n\nif __name__ == '__main__':\n\tnet = Network(num_hidden=16, batch_size=64, patch_size=3, conv_depth=16, pooling_scale=2)\n\tnet.run()\n"""
Season1/12-15/dp_refined_api.py,39,"b""# \xe6\x96\xb0\xe7\x9a\x84 refined api \xe4\xb8\x8d\xe6\x94\xaf\xe6\x8c\x81 Python2\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\n\nclass Network():\n\tdef __init__(self, train_batch_size, test_batch_size, pooling_scale):\n\t\t'''\n\t\t@num_hidden: \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\x9a\x84\xe8\x8a\x82\xe7\x82\xb9\xe6\x95\xb0\xe9\x87\x8f\n\t\t@batch_size\xef\xbc\x9a\xe5\x9b\xa0\xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe8\xa6\x81\xe8\x8a\x82\xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x88\x86\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\xe6\xaf\x8f\xe4\xb8\x80\xe6\x89\xb9\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x87\x8f\xe3\x80\x82\n\t\t'''\n\t\tself.train_batch_size = train_batch_size\n\t\tself.test_batch_size = test_batch_size\n\n\t\t# Hyper Parameters\n\t\tself.conv_config = []\t\t# list of dict\n\t\tself.fc_config = []\t\t\t# list of dict\n\t\tself.conv_weights = []\n\t\tself.conv_biases = []\n\t\tself.fc_weights = []\n\t\tself.fc_biases = []\n\t\tself.pooling_scale = pooling_scale\n\t\tself.pooling_stride = pooling_scale\n\n\t\t# Graph Related\n\t\tself.tf_train_samples = None\n\t\tself.tf_train_labels = None\n\t\tself.tf_test_samples = None\n\t\tself.tf_test_labels = None\n\n\t\t# \xe7\xbb\x9f\xe8\xae\xa1\n\t\tself.merged = None\n\t\tself.train_summaries = []\n\t\tself.test_summaries = []\n\n\tdef add_conv(self, *, patch_size, in_depth, out_depth, activation='relu', pooling=False, name):\n\t\t'''\n\t\tThis function does not define operations in the graph, but only store config in self.conv_layer_config\n\t\t'''\n\t\tself.conv_config.append({\n\t\t\t'patch_size': patch_size,\n\t\t\t'in_depth': in_depth,\n\t\t\t'out_depth': out_depth,\n\t\t\t'activation': activation,\n\t\t\t'pooling': pooling,\n\t\t\t'name': name\n\t\t})\n\t\twith tf.name_scope(name):\n\t\t\tweights = tf.Variable(\n\t\t\t\ttf.truncated_normal([patch_size, patch_size, in_depth, out_depth], stddev=0.1), name=name+'_weights')\n\t\t\tbiases = tf.Variable(tf.constant(0.1, shape=[out_depth]), name=name+'_biases')\n\t\t\tself.conv_weights.append(weights)\n\t\t\tself.conv_biases.append(biases)\n\n\tdef add_fc(self, *, in_num_nodes, out_num_nodes, activation='relu', name):\n\t\t'''\n\t\tadd fc layer config to slef.fc_layer_config\n\t\t'''\n\t\tself.fc_config.append({\n\t\t\t'in_num_nodes': in_num_nodes,\n\t\t\t'out_num_nodes': out_num_nodes,\n\t\t\t'activation': activation,\n\t\t\t'name': name\n\t\t})\n\t\twith tf.name_scope(name):\n\t\t\tweights = tf.Variable(tf.truncated_normal([in_num_nodes, out_num_nodes], stddev=0.1))\n\t\t\tbiases = tf.Variable(tf.constant(0.1, shape=[out_num_nodes]))\n\t\t\tself.fc_weights.append(weights)\n\t\t\tself.fc_biases.append(biases)\n\t\t\tself.train_summaries.append(tf.histogram_summary(str(len(self.fc_weights))+'_weights', weights))\n\t\t\tself.train_summaries.append(tf.histogram_summary(str(len(self.fc_biases))+'_biases', biases))\n\n\t# should make the definition as an exposed API, instead of implemented in the function\n\tdef define_inputs(self, *, train_samples_shape, train_labels_shape, test_samples_shape):\n\t\t# \xe8\xbf\x99\xe9\x87\x8c\xe5\x8f\xaa\xe6\x98\xaf\xe5\xae\x9a\xe4\xb9\x89\xe5\x9b\xbe\xe8\xb0\xb1\xe4\xb8\xad\xe7\x9a\x84\xe5\x90\x84\xe7\xa7\x8d\xe5\x8f\x98\xe9\x87\x8f\n\t\twith tf.name_scope('inputs'):\n\t\t\tself.tf_train_samples = tf.placeholder(tf.float32, shape=train_samples_shape, name='tf_train_samples')\n\t\t\tself.tf_train_labels = tf.placeholder(tf.float32, shape=train_labels_shape, name='tf_train_labels')\n\t\t\tself.tf_test_samples = tf.placeholder(tf.float32, shape=test_samples_shape, name='tf_test_samples')\n\n\tdef define_model(self):\n\t\t'''\n\t\t\xe5\xae\x9a\xe4\xb9\x89\xe6\x88\x91\xe7\x9a\x84\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe8\xb0\xb1\n\t\t'''\n\t\tdef model(data_flow, train=True):\n\t\t\t'''\n\t\t\t@data: original inputs\n\t\t\t@return: logits\n\t\t\t'''\n\t\t\t# Define Convolutional Layers\n\t\t\tfor i, (weights, biases, config) in enumerate(zip(self.conv_weights, self.conv_biases, self.conv_config)):\n\t\t\t\twith tf.name_scope(config['name'] + '_model'):\n\t\t\t\t\twith tf.name_scope('convolution'):\n\t\t\t\t\t\t# default 1,1,1,1 stride and SAME padding\n\t\t\t\t\t\tdata_flow = tf.nn.conv2d(data_flow, filter=weights, strides=[1, 1, 1, 1], padding='SAME')\n\t\t\t\t\t\tdata_flow = data_flow + biases\n\t\t\t\t\t\tif not train:\n\t\t\t\t\t\t\tself.visualize_filter_map(data_flow, how_many=config['out_depth'], display_size=32//(i//2+1), name=config['name']+'_conv')\n\t\t\t\t\tif config['activation'] == 'relu':\n\t\t\t\t\t\tdata_flow = tf.nn.relu(data_flow)\n\t\t\t\t\t\tif not train:\n\t\t\t\t\t\t\tself.visualize_filter_map(data_flow, how_many=config['out_depth'], display_size=32//(i//2+1), name=config['name']+'_relu')\n\t\t\t\t\telse:\n\t\t\t\t\t\traise Exception('Activation Func can only be Relu right now. You passed', config['activation'])\n\t\t\t\t\tif config['pooling']:\n\t\t\t\t\t\tdata_flow = tf.nn.max_pool(\n\t\t\t\t\t\t\tdata_flow,\n\t\t\t\t\t\t\tksize=[1, self.pooling_scale, self.pooling_scale, 1],\n\t\t\t\t\t\t\tstrides=[1, self.pooling_stride, self.pooling_stride, 1],\n\t\t\t\t\t\t\tpadding='SAME')\n\t\t\t\t\t\tif not train:\n\t\t\t\t\t\t\tself.visualize_filter_map(data_flow, how_many=config['out_depth'], display_size=32//(i//2+1)//2, name=config['name']+'_pooling')\n\n\t\t\t# Define Fully Connected Layers\n\t\t\tfor i, (weights, biases, config) in enumerate(zip(self.fc_weights, self.fc_biases, self.fc_config)):\n\t\t\t\tif i == 0:\n\t\t\t\t\tshape = data_flow.get_shape().as_list()\n\t\t\t\t\tdata_flow = tf.reshape(data_flow, [shape[0], shape[1] * shape[2] * shape[3]])\n\t\t\t\twith tf.name_scope(config['name'] + 'model'):\n\t\t\t\t\tdata_flow = tf.matmul(data_flow, weights) + biases\n\t\t\t\t\tif config['activation'] == 'relu':\n\t\t\t\t\t\tdata_flow = tf.nn.relu(data_flow)\n\t\t\t\t\telif config['activation'] is None:\n\t\t\t\t\t\tpass\n\t\t\t\t\telse:\n\t\t\t\t\t\traise Exception('Activation Func can only be Relu or None right now. You passed', config['activation'])\n\t\t\treturn data_flow\n\n\t\t# Training computation.\n\t\tlogits = model(self.tf_train_samples)\n\t\twith tf.name_scope('loss'):\n\t\t\tself.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, self.tf_train_labels))\n\t\t\tself.train_summaries.append(tf.scalar_summary('Loss', self.loss))\n\n\t\t# Optimizer.\n\t\twith tf.name_scope('optimizer'):\n\t\t\tself.optimizer = tf.train.GradientDescentOptimizer(0.0001).minimize(self.loss)\n\n\t\t# Predictions for the training, validation, and test data.\n\t\twith tf.name_scope('train'):\n\t\t\tself.train_prediction = tf.nn.softmax(logits, name='train_prediction')\n\t\twith tf.name_scope('test'):\n\t\t\tself.test_prediction = tf.nn.softmax(model(self.tf_test_samples, train=False), name='test_prediction')\n\n\t\tself.merged_train_summary = tf.merge_summary(self.train_summaries)\n\t\tself.merged_test_summary = tf.merge_summary(self.test_summaries)\n\n\tdef run(self, data_iterator, train_samples, train_labels, test_samples, test_labels):\n\t\t'''\n\t\t\xe7\x94\xa8\xe5\x88\xb0Session\n\t\t:data_iterator: a function that yields chuck of data\n\t\t'''\n\t\t# private function\n\t\tdef print_confusion_matrix(confusionMatrix):\n\t\t\tprint('Confusion    Matrix:')\n\t\t\tfor i, line in enumerate(confusionMatrix):\n\t\t\t\tprint(line, line[i] / np.sum(line))\n\t\t\ta = 0\n\t\t\tfor i, column in enumerate(np.transpose(confusionMatrix, (1, 0))):\n\t\t\t\ta += (column[i] / np.sum(column)) * (np.sum(column) / 26000)\n\t\t\t\tprint(column[i] / np.sum(column), )\n\t\t\tprint('\\n', np.sum(confusionMatrix), a)\n\n\t\tself.writer = tf.train.SummaryWriter('./board', tf.get_default_graph())\n\n\t\twith tf.Session(graph=tf.get_default_graph()) as session:\n\t\t\ttf.initialize_all_variables().run()\n\n\t\t\t### \xe8\xae\xad\xe7\xbb\x83\n\t\t\tprint('Start Training')\n\t\t\t# batch 1000\n\t\t\tfor i, samples, labels in data_iterator(train_samples, train_labels, chunkSize=self.train_batch_size):\n\t\t\t\t_, l, predictions, summary = session.run(\n\t\t\t\t\t[self.optimizer, self.loss, self.train_prediction, self.merged_train_summary],\n\t\t\t\t\tfeed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels}\n\t\t\t\t)\n\t\t\t\tself.writer.add_summary(summary, i)\n\t\t\t\t# labels is True Labels\n\t\t\t\taccuracy, _ = self.accuracy(predictions, labels)\n\t\t\t\tif i % 50 == 0:\n\t\t\t\t\tprint('Minibatch loss at step %d: %f' % (i, l))\n\t\t\t\t\tprint('Minibatch accuracy: %.1f%%' % accuracy)\n\t\t\t###\n\n\t\t\t# ### \xe6\xb5\x8b\xe8\xaf\x95\n\t\t\taccuracies = []\n\t\t\tconfusionMatrices = []\n\t\t\tfor i, samples, labels in data_iterator(test_samples, test_labels, chunkSize=self.test_batch_size):\n\t\t\t\tprint('samples shape', samples.shape)\n\t\t\t\tresult, summary = session.run(\n\t\t\t\t\t[self.test_prediction, self.merged_test_summary],\n\t\t\t\t\tfeed_dict={self.tf_test_samples: samples}\n\t\t\t\t)\n\t\t\t\t# result = self.test_prediction.eval(feed_dict={self.tf_test_samples: samples})\n\t\t\t\tself.writer.add_summary(summary, i)\n\t\t\t\taccuracy, cm = self.accuracy(result, labels, need_confusion_matrix=True)\n\t\t\t\taccuracies.append(accuracy)\n\t\t\t\tconfusionMatrices.append(cm)\n\t\t\t\tprint('Test Accuracy: %.1f%%' % accuracy)\n\t\t\tprint(' Average  Accuracy:', np.average(accuracies))\n\t\t\tprint('Standard Deviation:', np.std(accuracies))\n\t\t\tprint_confusion_matrix(np.add.reduce(confusionMatrices))\n\t\t###\n\n\tdef accuracy(self, predictions, labels, need_confusion_matrix=False):\n\t\t'''\n\t\t\xe8\xae\xa1\xe7\xae\x97\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87\xe4\xb8\x8e\xe5\x8f\xac\xe5\x9b\x9e\xe7\x8e\x87\n\t\t@return: accuracy and confusionMatrix as a tuple\n\t\t'''\n\t\t_predictions = np.argmax(predictions, 1)\n\t\t_labels = np.argmax(labels, 1)\n\t\tcm = confusion_matrix(_labels, _predictions) if need_confusion_matrix else None\n\t\t# == is overloaded for numpy array\n\t\taccuracy = (100.0 * np.sum(_predictions == _labels) / predictions.shape[0])\n\t\treturn accuracy, cm\n\n\tdef visualize_filter_map(self, tensor, *, how_many, display_size, name):\n\t\tprint(tensor.get_shape)\n\t\tfilter_map = tensor[-1]\n\t\tprint(filter_map.get_shape())\n\t\tfilter_map = tf.transpose(filter_map, perm=[2, 0, 1])\n\t\tprint(filter_map.get_shape())\n\t\tfilter_map = tf.reshape(filter_map, (how_many, display_size, display_size, 1))\n\t\tprint(how_many)\n\t\tself.test_summaries.append(tf.image_summary(name, tensor=filter_map, max_images=how_many))\n"""
Season1/12-15/load.py,0,"b""# encoding:utf-8\n# Python2 \xe5\x85\xbc\xe5\xae\xb9\nfrom __future__ import print_function, division\nfrom scipy.io import loadmat as load\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef reformat(samples, labels):\n\t# \xe6\x94\xb9\xe5\x8f\x98\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\n\t#  0       1       2      3          3       0       1      2\n\t# (\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0) -> (\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0)\n\tnew = np.transpose(samples, (3, 0, 1, 2)).astype(np.float32)\n\n\t# labels \xe5\x8f\x98\xe6\x88\x90 one-hot encoding, [2] -> [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n\t# digit 0 , represented as 10\n\t# labels \xe5\x8f\x98\xe6\x88\x90 one-hot encoding, [10] -> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\tlabels = np.array([x[0] for x in labels])\t# slow code, whatever\n\tone_hot_labels = []\n\tfor num in labels:\n\t\tone_hot = [0.0] * 10\n\t\tif num == 10:\n\t\t\tone_hot[0] = 1.0\n\t\telse:\n\t\t\tone_hot[num] = 1.0\n\t\tone_hot_labels.append(one_hot)\n\tlabels = np.array(one_hot_labels).astype(np.float32)\n\treturn new, labels\n\ndef normalize(samples):\n\t'''\n\t\xe5\xb9\xb6\xe4\xb8\x94\xe7\x81\xb0\xe5\xba\xa6\xe5\x8c\x96: \xe4\xbb\x8e\xe4\xb8\x89\xe8\x89\xb2\xe9\x80\x9a\xe9\x81\x93 -> \xe5\x8d\x95\xe8\x89\xb2\xe9\x80\x9a\xe9\x81\x93     \xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98 + \xe5\x8a\xa0\xe5\xbf\xab\xe8\xae\xad\xe7\xbb\x83\xe9\x80\x9f\xe5\xba\xa6\n\t(R + G + B) / 3\n\t\xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe4\xbb\x8e 0 ~ 255 \xe7\xba\xbf\xe6\x80\xa7\xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0 -1.0 ~ +1.0\n\t@samples: numpy array\n\t'''\n\ta = np.add.reduce(samples, keepdims=True, axis=3)  # shape (\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0)\n\ta = a/3.0\n\treturn a/128.0 - 1.0\n\n\ndef distribution(labels, name):\n\t# \xe6\x9f\xa5\xe7\x9c\x8b\xe4\xb8\x80\xe4\xb8\x8b\xe6\xaf\x8f\xe4\xb8\xaalabel\xe7\x9a\x84\xe5\x88\x86\xe5\xb8\x83\xef\xbc\x8c\xe5\x86\x8d\xe7\x94\xbb\xe4\xb8\xaa\xe7\xbb\x9f\xe8\xae\xa1\xe5\x9b\xbe\n\t# keys:\n\t# 0\n\t# 1\n\t# 2\n\t# ...\n\t# 9\n\tcount = {}\n\tfor label in labels:\n\t\tkey = 0 if label[0] == 10 else label[0]\n\t\tif key in count:\n\t\t\tcount[key] += 1\n\t\telse:\n\t\t\tcount[key] = 1\n\tx = []\n\ty = []\n\tfor k, v in count.items():\n\t\t# print(k, v)\n\t\tx.append(k)\n\t\ty.append(v)\n\n\ty_pos = np.arange(len(x))\n\tplt.bar(y_pos, y, align='center', alpha=0.5)\n\tplt.xticks(y_pos, x)\n\tplt.ylabel('Count')\n\tplt.title(name + ' Label Distribution')\n\tplt.show()\n\ndef inspect(dataset, labels, i):\n\t# \xe6\x98\xbe\xe7\xa4\xba\xe5\x9b\xbe\xe7\x89\x87\xe7\x9c\x8b\xe7\x9c\x8b\n\tif dataset.shape[3] == 1:\n\t\tshape = dataset.shape\n\t\tdataset = dataset.reshape(shape[0], shape[1], shape[2])\n\tprint(labels[i])\n\tplt.imshow(dataset[i])\n\tplt.show()\n\n\ntrain = load('../data/train_32x32.mat')\ntest = load('../data/test_32x32.mat')\n# extra = load('../data/extra_32x32.mat')\n\n# print('Train Samples Shape:', train['X'].shape)\n# print('Train  Labels Shape:', train['y'].shape)\n\n# print('Train Samples Shape:', test['X'].shape)\n# print('Train  Labels Shape:', test['y'].shape)\n\n# print('Train Samples Shape:', extra['X'].shape)\n# print('Train  Labels Shape:', extra['y'].shape)\n\ntrain_samples = train['X']\ntrain_labels = train['y']\ntest_samples = test['X']\ntest_labels = test['y']\n# extra_samples = extra['X']\n# extra_labels = extra['y']\n\nn_train_samples, _train_labels = reformat(train_samples, train_labels)\nn_test_samples, _test_labels = reformat(test_samples, test_labels)\n\n_train_samples = normalize(n_train_samples)\n_test_samples = normalize(n_test_samples)\n\nnum_labels = 10\nimage_size = 32\nnum_channels = 1\n\nif __name__ == '__main__':\n\t# \xe6\x8e\xa2\xe7\xb4\xa2\xe6\x95\xb0\xe6\x8d\xae\n\tpass\n\tinspect(_train_samples, _train_labels, 1234)\n\t# _train_samples = normalize(_train_samples)\n\t# inspect(_train_samples, _train_labels, 1234)\n\t# distribution(train_labels, 'Train Labels')\n\t# distribution(test_labels, 'Test Labels')\n"""
Season1/12-15/main.py,0,"b'if __name__ == \'__main__\':\n\timport load\n\tfrom dp_refined_api import Network\n\n\ttrain_samples, train_labels = load._train_samples, load._train_labels\n\ttest_samples, test_labels = load._test_samples, load._test_labels\n\n\tprint(\'Training set\', train_samples.shape, train_labels.shape)\n\tprint(\'    Test set\', test_samples.shape, test_labels.shape)\n\n\timage_size = load.image_size\n\tnum_labels = load.num_labels\n\tnum_channels = load.num_channels\n\n\tdef get_chunk(samples, labels, chunkSize):\n\t\t\'\'\'\n\t\tIterator/Generator: get a batch of data\n\t\t\xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8/\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\x80\xe6\xac\xa1\xe5\x8f\xaa\xe5\xbe\x97\xe5\x88\xb0 chunkSize \xe8\xbf\x99\xe4\xb9\x88\xe5\xa4\x9a\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n\t\t\xe7\x94\xa8\xe4\xba\x8e for loop\xef\xbc\x8c just like range() function\n\t\t\'\'\'\n\t\tif len(samples) != len(labels):\n\t\t\traise Exception(\'Length of samples and labels must equal\')\n\t\tstepStart = 0  # initial step\n\t\ti = 0\n\t\twhile stepStart < len(samples):\n\t\t\tstepEnd = stepStart + chunkSize\n\t\t\tif stepEnd < len(samples):\n\t\t\t\tyield i, samples[stepStart:stepEnd], labels[stepStart:stepEnd]\n\t\t\t\ti += 1\n\t\t\tstepStart = stepEnd\n\n\n\tnet = Network(train_batch_size=64, test_batch_size=500, pooling_scale=2)\n\tnet.define_inputs(\n\t\t\ttrain_samples_shape=(64, image_size, image_size, num_channels),\n\t\t\ttrain_labels_shape=(64, num_labels),\n\t\t\ttest_samples_shape=(500, image_size, image_size, num_channels)\n\t\t)\n\t#\n\tnet.add_conv(patch_size=3, in_depth=num_channels, out_depth=16, activation=\'relu\', pooling=False, name=\'conv1\')\n\tnet.add_conv(patch_size=3, in_depth=16, out_depth=16, activation=\'relu\', pooling=True, name=\'conv2\')\n\tnet.add_conv(patch_size=3, in_depth=16, out_depth=16, activation=\'relu\', pooling=False, name=\'conv3\')\n\tnet.add_conv(patch_size=3, in_depth=16, out_depth=16, activation=\'relu\', pooling=True, name=\'conv4\')\n\n\t# 4 = \xe4\xb8\xa4\xe6\xac\xa1 pooling, \xe6\xaf\x8f\xe4\xb8\x80\xe6\xac\xa1\xe7\xbc\xa9\xe5\xb0\x8f\xe4\xb8\xba 1/2\n\t# 16 = conv4 out_depth\n\tnet.add_fc(in_num_nodes=(image_size // 4) * (image_size // 4) * 16, out_num_nodes=16, activation=\'relu\', name=\'fc1\')\n\tnet.add_fc(in_num_nodes=16, out_num_nodes=10, activation=None, name=\'fc2\')\n\n\tnet.define_model()\n\tnet.run(get_chunk, train_samples, train_labels, test_samples, test_labels)\n\nelse:\n\traise Exception(\'main.py: Should Not Be Imported!!! Must Run by ""python main.py""\')\n'"
Season1/16-19/dp.py,45,"b""# \xe6\x96\xb0\xe7\x9a\x84 refined api \xe4\xb8\x8d\xe6\x94\xaf\xe6\x8c\x81 Python2\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\n\nclass Network():\n\tdef __init__(self, train_batch_size, test_batch_size, pooling_scale,\n\t\t\t\t       optimizeMethod='adam'):\n\t\t'''\n\t\t@num_hidden: \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\x9a\x84\xe8\x8a\x82\xe7\x82\xb9\xe6\x95\xb0\xe9\x87\x8f\n\t\t@batch_size\xef\xbc\x9a\xe5\x9b\xa0\xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe8\xa6\x81\xe8\x8a\x82\xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x88\x86\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\xe6\xaf\x8f\xe4\xb8\x80\xe6\x89\xb9\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x87\x8f\xe3\x80\x82\n\t\t'''\n\t\tself.optimizeMethod = optimizeMethod;\n\n\t\tself.train_batch_size = train_batch_size\n\t\tself.test_batch_size = test_batch_size\n\n\t\t# Hyper Parameters\n\t\tself.conv_config = []\t\t# list of dict\n\t\tself.fc_config = []\t\t\t# list of dict\n\t\tself.conv_weights = []\n\t\tself.conv_biases = []\n\t\tself.fc_weights = []\n\t\tself.fc_biases = []\n\t\tself.pooling_scale = pooling_scale\n\t\tself.pooling_stride = pooling_scale\n\n\t\t# Graph Related\n\t\tself.tf_train_samples = None\n\t\tself.tf_train_labels = None\n\t\tself.tf_test_samples = None\n\t\tself.tf_test_labels = None\n\n\t\t# \xe7\xbb\x9f\xe8\xae\xa1\n\t\tself.merged = None\n\t\tself.train_summaries = []\n\t\tself.test_summaries = []\n\n\tdef add_conv(self, *, patch_size, in_depth, out_depth, activation='relu', pooling=False, name):\n\t\t'''\n\t\tThis function does not define operations in the graph, but only store config in self.conv_layer_config\n\t\t'''\n\t\tself.conv_config.append({\n\t\t\t'patch_size': patch_size,\n\t\t\t'in_depth': in_depth,\n\t\t\t'out_depth': out_depth,\n\t\t\t'activation': activation,\n\t\t\t'pooling': pooling,\n\t\t\t'name': name\n\t\t})\n\t\twith tf.name_scope(name):\n\t\t\tweights = tf.Variable(\n\t\t\t\ttf.truncated_normal([patch_size, patch_size, in_depth, out_depth], stddev=0.1), name=name+'_weights')\n\t\t\tbiases = tf.Variable(tf.constant(0.1, shape=[out_depth]), name=name+'_biases')\n\t\t\tself.conv_weights.append(weights)\n\t\t\tself.conv_biases.append(biases)\n\n\tdef add_fc(self, *, in_num_nodes, out_num_nodes, activation='relu', name):\n\t\t'''\n\t\tadd fc layer config to slef.fc_layer_config\n\t\t'''\n\t\tself.fc_config.append({\n\t\t\t'in_num_nodes': in_num_nodes,\n\t\t\t'out_num_nodes': out_num_nodes,\n\t\t\t'activation': activation,\n\t\t\t'name': name\n\t\t})\n\t\twith tf.name_scope(name):\n\t\t\tweights = tf.Variable(tf.truncated_normal([in_num_nodes, out_num_nodes], stddev=0.1))\n\t\t\tbiases = tf.Variable(tf.constant(0.1, shape=[out_num_nodes]))\n\t\t\tself.fc_weights.append(weights)\n\t\t\tself.fc_biases.append(biases)\n\t\t\tself.train_summaries.append(tf.histogram_summary(str(len(self.fc_weights))+'_weights', weights))\n\t\t\tself.train_summaries.append(tf.histogram_summary(str(len(self.fc_biases))+'_biases', biases))\n\n\tdef apply_regularization(self, _lambda):\n\t\t# L2 regularization for the fully connected parameters\n\t\tregularization = 0.0\n\t\tfor weights, biases in zip(self.fc_weights, self.fc_biases):\n\t\t\tregularization += tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n\t\t# 1e5\n\t\treturn _lambda * regularization\n\n\t# should make the definition as an exposed API, instead of implemented in the function\n\tdef define_inputs(self, *, train_samples_shape, train_labels_shape, test_samples_shape):\n\t\t# \xe8\xbf\x99\xe9\x87\x8c\xe5\x8f\xaa\xe6\x98\xaf\xe5\xae\x9a\xe4\xb9\x89\xe5\x9b\xbe\xe8\xb0\xb1\xe4\xb8\xad\xe7\x9a\x84\xe5\x90\x84\xe7\xa7\x8d\xe5\x8f\x98\xe9\x87\x8f\n\t\twith tf.name_scope('inputs'):\n\t\t\tself.tf_train_samples = tf.placeholder(tf.float32, shape=train_samples_shape, name='tf_train_samples')\n\t\t\tself.tf_train_labels = tf.placeholder(tf.float32, shape=train_labels_shape, name='tf_train_labels')\n\t\t\tself.tf_test_samples = tf.placeholder(tf.float32, shape=test_samples_shape, name='tf_test_samples')\n\n\tdef define_model(self):\n\t\t'''\n\t\t\xe5\xae\x9a\xe4\xb9\x89\xe6\x88\x91\xe7\x9a\x84\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe8\xb0\xb1\n\t\t'''\n\t\tdef model(data_flow, train=True):\n\t\t\t'''\n\t\t\t@data: original inputs\n\t\t\t@return: logits\n\t\t\t'''\n\t\t\t# Define Convolutional Layers\n\t\t\tfor i, (weights, biases, config) in enumerate(zip(self.conv_weights, self.conv_biases, self.conv_config)):\n\t\t\t\twith tf.name_scope(config['name'] + '_model'):\n\t\t\t\t\twith tf.name_scope('convolution'):\n\t\t\t\t\t\t# default 1,1,1,1 stride and SAME padding\n\t\t\t\t\t\tdata_flow = tf.nn.conv2d(data_flow, filter=weights, strides=[1, 1, 1, 1], padding='SAME')\n\t\t\t\t\t\tdata_flow = data_flow + biases\n\t\t\t\t\t\tif not train:\n\t\t\t\t\t\t\tself.visualize_filter_map(data_flow, how_many=config['out_depth'], display_size=32//(i//2+1), name=config['name']+'_conv')\n\t\t\t\t\tif config['activation'] == 'relu':\n\t\t\t\t\t\tdata_flow = tf.nn.relu(data_flow)\n\t\t\t\t\t\tif not train:\n\t\t\t\t\t\t\tself.visualize_filter_map(data_flow, how_many=config['out_depth'], display_size=32//(i//2+1), name=config['name']+'_relu')\n\t\t\t\t\telse:\n\t\t\t\t\t\traise Exception('Activation Func can only be Relu right now. You passed', config['activation'])\n\t\t\t\t\tif config['pooling']:\n\t\t\t\t\t\tdata_flow = tf.nn.max_pool(\n\t\t\t\t\t\t\tdata_flow,\n\t\t\t\t\t\t\tksize=[1, self.pooling_scale, self.pooling_scale, 1],\n\t\t\t\t\t\t\tstrides=[1, self.pooling_stride, self.pooling_stride, 1],\n\t\t\t\t\t\t\tpadding='SAME')\n\t\t\t\t\t\tif not train:\n\t\t\t\t\t\t\tself.visualize_filter_map(data_flow, how_many=config['out_depth'], display_size=32//(i//2+1)//2, name=config['name']+'_pooling')\n\n\t\t\t# Define Fully Connected Layers\n\t\t\tfor i, (weights, biases, config) in enumerate(zip(self.fc_weights, self.fc_biases, self.fc_config)):\n\t\t\t\tif i == 0:\n\t\t\t\t\tshape = data_flow.get_shape().as_list()\n\t\t\t\t\tdata_flow = tf.reshape(data_flow, [shape[0], shape[1] * shape[2] * shape[3]])\n\t\t\t\twith tf.name_scope(config['name'] + 'model'):\n\n\t\t\t\t\t### Dropout\n\t\t\t\t\tif train and i == len(self.fc_weights) - 1:\n\t\t\t\t\t\tdata_flow =  tf.nn.dropout(data_flow, 0.5, seed=4926)\n\t\t\t\t\t###\n\n\t\t\t\t\tdata_flow = tf.matmul(data_flow, weights) + biases\n\t\t\t\t\tif config['activation'] == 'relu':\n\t\t\t\t\t\tdata_flow = tf.nn.relu(data_flow)\n\t\t\t\t\telif config['activation'] is None:\n\t\t\t\t\t\tpass\n\t\t\t\t\telse:\n\t\t\t\t\t\traise Exception('Activation Func can only be Relu or None right now. You passed', config['activation'])\n\t\t\treturn data_flow\n\n\t\t# Training computation.\n\t\tlogits = model(self.tf_train_samples)\n\t\twith tf.name_scope('loss'):\n\t\t\tself.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, self.tf_train_labels))\n\t\t\tself.loss += self.apply_regularization(_lambda=5e-4)\n\t\t\tself.train_summaries.append(tf.scalar_summary('Loss', self.loss))\n\n\t\t# learning rate decay\n\t\tglobal_step = tf.Variable(0)\n\t\tlr = 0.001\n\t\tdr = 0.99\n\t\tlearning_rate = tf.train.exponential_decay(\n\t\t\tlearning_rate=lr,\n\t\t\tglobal_step=global_step*self.train_batch_size,\n\t\t\tdecay_steps=100,\n\t\t\tdecay_rate=dr,\n\t\t\tstaircase=True\n\t\t)\n\n\t\t# Optimizer.\n\t\twith tf.name_scope('optimizer'):\n\t\t\tif(self.optimizeMethod=='gradient'):\n\t\t\t\tself.optimizer = tf.train \\\n\t\t\t\t\t.GradientDescentOptimizer(learning_rate) \\\n\t\t\t\t\t.minimize(self.loss)\n\t\t\telif(self.optimizeMethod=='momentum'):\n\t\t\t\tself.optimizer = tf.train \\\n\t\t\t\t\t.MomentumOptimizer(learning_rate, 0.5) \\\n\t\t\t\t\t.minimize(self.loss)\n\t\t\telif(self.optimizeMethod=='adam'):\n\t\t\t\tself.optimizer = tf.train \\\n\t\t\t\t\t.AdamOptimizer(learning_rate) \\\n\t\t\t\t\t.minimize(self.loss)\n\n\t\t# Predictions for the training, validation, and test data.\n\t\twith tf.name_scope('train'):\n\t\t\tself.train_prediction = tf.nn.softmax(logits, name='train_prediction')\n\t\twith tf.name_scope('test'):\n\t\t\tself.test_prediction = tf.nn.softmax(model(self.tf_test_samples, train=False), name='test_prediction')\n\n\t\tself.merged_train_summary = tf.merge_summary(self.train_summaries)\n\t\tself.merged_test_summary = tf.merge_summary(self.test_summaries)\n\n\tdef run(self, data_iterator, train_samples, train_labels, test_samples, test_labels):\n\t\t'''\n\t\t\xe7\x94\xa8\xe5\x88\xb0Session\n\t\t:data_iterator: a function that yields chuck of data\n\t\t'''\n\t\t# private function\n\t\tdef print_confusion_matrix(confusionMatrix):\n\t\t\tprint('Confusion    Matrix:')\n\t\t\tfor i, line in enumerate(confusionMatrix):\n\t\t\t\tprint(line, line[i] / np.sum(line))\n\t\t\ta = 0\n\t\t\tfor i, column in enumerate(np.transpose(confusionMatrix, (1, 0))):\n\t\t\t\ta += (column[i] / np.sum(column)) * (np.sum(column) / 26000)\n\t\t\t\tprint(column[i] / np.sum(column), )\n\t\t\tprint('\\n', np.sum(confusionMatrix), a)\n\n\t\tself.writer = tf.train.SummaryWriter('./board', tf.get_default_graph())\n\n\t\twith tf.Session(graph=tf.get_default_graph()) as session:\n\t\t\ttf.initialize_all_variables().run()\n\n\t\t\t### \xe8\xae\xad\xe7\xbb\x83\n\t\t\tprint('Start Training')\n\t\t\t# batch 1000\n\t\t\tfor i, samples, labels in data_iterator(train_samples, train_labels, chunkSize=self.train_batch_size):\n\t\t\t\t_, l, predictions, summary = session.run(\n\t\t\t\t\t[self.optimizer, self.loss, self.train_prediction, self.merged_train_summary],\n\t\t\t\t\tfeed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels}\n\t\t\t\t)\n\t\t\t\tself.writer.add_summary(summary, i)\n\t\t\t\t# labels is True Labels\n\t\t\t\taccuracy, _ = self.accuracy(predictions, labels)\n\t\t\t\tif i % 50 == 0:\n\t\t\t\t\tprint('Minibatch loss at step %d: %f' % (i, l))\n\t\t\t\t\tprint('Minibatch accuracy: %.1f%%' % accuracy)\n\t\t\t###\n\n\t\t\t### \xe6\xb5\x8b\xe8\xaf\x95\n\t\t\taccuracies = []\n\t\t\tconfusionMatrices = []\n\t\t\tfor i, samples, labels in data_iterator(test_samples, test_labels, chunkSize=self.test_batch_size):\n\t\t\t\tresult, summary = session.run(\n\t\t\t\t\t[self.test_prediction, self.merged_test_summary],\n\t\t\t\t\tfeed_dict={self.tf_test_samples: samples}\n\t\t\t\t)\n\t\t\t\tself.writer.add_summary(summary, i)\n\t\t\t\taccuracy, cm = self.accuracy(result, labels, need_confusion_matrix=True)\n\t\t\t\taccuracies.append(accuracy)\n\t\t\t\tconfusionMatrices.append(cm)\n\t\t\t\tprint('Test Accuracy: %.1f%%' % accuracy)\n\t\t\tprint(' Average  Accuracy:', np.average(accuracies))\n\t\t\tprint('Standard Deviation:', np.std(accuracies))\n\t\t\tprint_confusion_matrix(np.add.reduce(confusionMatrices))\n\t\t###\n\n\tdef accuracy(self, predictions, labels, need_confusion_matrix=False):\n\t\t'''\n\t\t\xe8\xae\xa1\xe7\xae\x97\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87\xe4\xb8\x8e\xe5\x8f\xac\xe5\x9b\x9e\xe7\x8e\x87\n\t\t@return: accuracy and confusionMatrix as a tuple\n\t\t'''\n\t\t_predictions = np.argmax(predictions, 1)\n\t\t_labels = np.argmax(labels, 1)\n\t\tcm = confusion_matrix(_labels, _predictions) if need_confusion_matrix else None\n\t\t# == is overloaded for numpy array\n\t\taccuracy = (100.0 * np.sum(_predictions == _labels) / predictions.shape[0])\n\t\treturn accuracy, cm\n\n\tdef visualize_filter_map(self, tensor, *, how_many, display_size, name):\n\t\tprint(tensor.get_shape)\n\t\tfilter_map = tensor[-1]\n\t\tprint(filter_map.get_shape())\n\t\tfilter_map = tf.transpose(filter_map, perm=[2, 0, 1])\n\t\tprint(filter_map.get_shape())\n\t\tfilter_map = tf.reshape(filter_map, (how_many, display_size, display_size, 1))\n\t\tprint(how_many)\n\t\tself.test_summaries.append(tf.image_summary(name, tensor=filter_map, max_images=how_many))\n"""
Season1/16-19/load.py,0,"b""# encoding:utf-8\n# Python2 \xe5\x85\xbc\xe5\xae\xb9\nfrom __future__ import print_function, division\nfrom scipy.io import loadmat as load\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef reformat(samples, labels):\n\t# \xe6\x94\xb9\xe5\x8f\x98\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\n\t#  0       1       2      3          3       0       1      2\n\t# (\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0) -> (\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0)\n\tnew = np.transpose(samples, (3, 0, 1, 2)).astype(np.float32)\n\n\t# labels \xe5\x8f\x98\xe6\x88\x90 one-hot encoding, [2] -> [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n\t# digit 0 , represented as 10\n\t# labels \xe5\x8f\x98\xe6\x88\x90 one-hot encoding, [10] -> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\tlabels = np.array([x[0] for x in labels])\t# slow code, whatever\n\tone_hot_labels = []\n\tfor num in labels:\n\t\tone_hot = [0.0] * 10\n\t\tif num == 10:\n\t\t\tone_hot[0] = 1.0\n\t\telse:\n\t\t\tone_hot[num] = 1.0\n\t\tone_hot_labels.append(one_hot)\n\tlabels = np.array(one_hot_labels).astype(np.float32)\n\treturn new, labels\n\ndef normalize(samples):\n\t'''\n\t\xe5\xb9\xb6\xe4\xb8\x94\xe7\x81\xb0\xe5\xba\xa6\xe5\x8c\x96: \xe4\xbb\x8e\xe4\xb8\x89\xe8\x89\xb2\xe9\x80\x9a\xe9\x81\x93 -> \xe5\x8d\x95\xe8\x89\xb2\xe9\x80\x9a\xe9\x81\x93     \xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98 + \xe5\x8a\xa0\xe5\xbf\xab\xe8\xae\xad\xe7\xbb\x83\xe9\x80\x9f\xe5\xba\xa6\n\t(R + G + B) / 3\n\t\xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe4\xbb\x8e 0 ~ 255 \xe7\xba\xbf\xe6\x80\xa7\xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0 -1.0 ~ +1.0\n\t@samples: numpy array\n\t'''\n\ta = np.add.reduce(samples, keepdims=True, axis=3)  # shape (\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0)\n\ta = a/3.0\n\treturn a/128.0 - 1.0\n\n\ndef distribution(labels, name):\n\t# \xe6\x9f\xa5\xe7\x9c\x8b\xe4\xb8\x80\xe4\xb8\x8b\xe6\xaf\x8f\xe4\xb8\xaalabel\xe7\x9a\x84\xe5\x88\x86\xe5\xb8\x83\xef\xbc\x8c\xe5\x86\x8d\xe7\x94\xbb\xe4\xb8\xaa\xe7\xbb\x9f\xe8\xae\xa1\xe5\x9b\xbe\n\t# keys:\n\t# 0\n\t# 1\n\t# 2\n\t# ...\n\t# 9\n\tcount = {}\n\tfor label in labels:\n\t\tkey = 0 if label[0] == 10 else label[0]\n\t\tif key in count:\n\t\t\tcount[key] += 1\n\t\telse:\n\t\t\tcount[key] = 1\n\tx = []\n\ty = []\n\tfor k, v in count.items():\n\t\t# print(k, v)\n\t\tx.append(k)\n\t\ty.append(v)\n\n\ty_pos = np.arange(len(x))\n\tplt.bar(y_pos, y, align='center', alpha=0.5)\n\tplt.xticks(y_pos, x)\n\tplt.ylabel('Count')\n\tplt.title(name + ' Label Distribution')\n\tplt.show()\n\ndef inspect(dataset, labels, i):\n\t# \xe6\x98\xbe\xe7\xa4\xba\xe5\x9b\xbe\xe7\x89\x87\xe7\x9c\x8b\xe7\x9c\x8b\n\tif dataset.shape[3] == 1:\n\t\tshape = dataset.shape\n\t\tdataset = dataset.reshape(shape[0], shape[1], shape[2])\n\tprint(labels[i])\n\tplt.imshow(dataset[i])\n\tplt.show()\n\n\ntrain = load('../data/train_32x32.mat')\ntest = load('../data/test_32x32.mat')\n# extra = load('../data/extra_32x32.mat')\n\n# print('Train Samples Shape:', train['X'].shape)\n# print('Train  Labels Shape:', train['y'].shape)\n\n# print('Train Samples Shape:', test['X'].shape)\n# print('Train  Labels Shape:', test['y'].shape)\n\n# print('Train Samples Shape:', extra['X'].shape)\n# print('Train  Labels Shape:', extra['y'].shape)\n\ntrain_samples = train['X']\ntrain_labels = train['y']\ntest_samples = test['X']\ntest_labels = test['y']\n# extra_samples = extra['X']\n# extra_labels = extra['y']\n\nn_train_samples, _train_labels = reformat(train_samples, train_labels)\nn_test_samples, _test_labels = reformat(test_samples, test_labels)\n\n_train_samples = normalize(n_train_samples)\n_test_samples = normalize(n_test_samples)\n\nnum_labels = 10\nimage_size = 32\nnum_channels = 1\n\nif __name__ == '__main__':\n\t# \xe6\x8e\xa2\xe7\xb4\xa2\xe6\x95\xb0\xe6\x8d\xae\n\tpass\n\tinspect(_train_samples, _train_labels, 1234)\n\t# _train_samples = normalize(_train_samples)\n\t# inspect(_train_samples, _train_labels, 1234)\n\t# distribution(train_labels, 'Train Labels')\n\t# distribution(test_labels, 'Test Labels')\n"""
Season1/16-19/main.py,0,"b'if __name__ == \'__main__\':\n\timport load\n\tfrom dp import Network\n\n\ttrain_samples, train_labels = load._train_samples, load._train_labels\n\ttest_samples, test_labels = load._test_samples, load._test_labels\n\n\tprint(\'Training set\', train_samples.shape, train_labels.shape)\n\tprint(\'    Test set\', test_samples.shape, test_labels.shape)\n\n\timage_size = load.image_size\n\tnum_labels = load.num_labels\n\tnum_channels = load.num_channels\n\n\tdef get_chunk(samples, labels, chunkSize):\n\t\t\'\'\'\n\t\tIterator/Generator: get a batch of data\n\t\t\xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8/\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\x80\xe6\xac\xa1\xe5\x8f\xaa\xe5\xbe\x97\xe5\x88\xb0 chunkSize \xe8\xbf\x99\xe4\xb9\x88\xe5\xa4\x9a\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n\t\t\xe7\x94\xa8\xe4\xba\x8e for loop\xef\xbc\x8c just like range() function\n\t\t\'\'\'\n\t\tif len(samples) != len(labels):\n\t\t\traise Exception(\'Length of samples and labels must equal\')\n\t\tstepStart = 0  # initial step\n\t\ti = 0\n\t\twhile stepStart < len(samples):\n\t\t\tstepEnd = stepStart + chunkSize\n\t\t\tif stepEnd < len(samples):\n\t\t\t\tyield i, samples[stepStart:stepEnd], labels[stepStart:stepEnd]\n\t\t\t\ti += 1\n\t\t\tstepStart = stepEnd\n\n\n\tnet = Network(train_batch_size=64, test_batch_size=500, pooling_scale=2)\n\tnet.define_inputs(\n\t\t\ttrain_samples_shape=(64, image_size, image_size, num_channels),\n\t\t\ttrain_labels_shape=(64, num_labels),\n\t\t\ttest_samples_shape=(500, image_size, image_size, num_channels)\n\t\t)\n\t#\n\tnet.add_conv(patch_size=3, in_depth=num_channels, out_depth=16, activation=\'relu\', pooling=False, name=\'conv1\')\n\tnet.add_conv(patch_size=3, in_depth=16, out_depth=16, activation=\'relu\', pooling=True, name=\'conv2\')\n\tnet.add_conv(patch_size=3, in_depth=16, out_depth=16, activation=\'relu\', pooling=False, name=\'conv3\')\n\tnet.add_conv(patch_size=3, in_depth=16, out_depth=16, activation=\'relu\', pooling=True, name=\'conv4\')\n\n\t# 4 = \xe4\xb8\xa4\xe6\xac\xa1 pooling, \xe6\xaf\x8f\xe4\xb8\x80\xe6\xac\xa1\xe7\xbc\xa9\xe5\xb0\x8f\xe4\xb8\xba 1/2\n\t# 16 = conv4 out_depth\n\tnet.add_fc(in_num_nodes=(image_size // 4) * (image_size // 4) * 16, out_num_nodes=16, activation=\'relu\', name=\'fc1\')\n\tnet.add_fc(in_num_nodes=16, out_num_nodes=10, activation=None, name=\'fc2\')\n\n\tnet.define_model()\n\tnet.run(get_chunk, train_samples, train_labels, test_samples, test_labels)\n\nelse:\n\traise Exception(\'main.py: Should Not Be Imported!!! Must Run by ""python main.py""\')\n'"
Season1/20/dp.py,56,"b'# \xe6\x96\xb0\xe7\x9a\x84 refined api \xe4\xb8\x8d\xe6\x94\xaf\xe6\x8c\x81 Python2\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\n\nclass Network():\n\tdef __init__(self, train_batch_size, test_batch_size, pooling_scale,\n\t\t\t\t       dropout_rate, base_learning_rate, decay_rate,\n\t\t\t\t\t   optimizeMethod=\'adam\', save_path=\'model/default.ckpt\'):\n\t\t\'\'\'\n\t\t@num_hidden: \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\x9a\x84\xe8\x8a\x82\xe7\x82\xb9\xe6\x95\xb0\xe9\x87\x8f\n\t\t@batch_size\xef\xbc\x9a\xe5\x9b\xa0\xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe8\xa6\x81\xe8\x8a\x82\xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x88\x86\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\xe6\xaf\x8f\xe4\xb8\x80\xe6\x89\xb9\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x87\x8f\xe3\x80\x82\n\t\t\'\'\'\n\t\tself.optimizeMethod = optimizeMethod\n\t\tself.dropout_rate=dropout_rate\n\t\tself.base_learning_rate=base_learning_rate\n\t\tself.decay_rate=decay_rate\n\n\t\tself.train_batch_size = train_batch_size\n\t\tself.test_batch_size = test_batch_size\n\n\t\t# Hyper Parameters\n\t\tself.conv_config = []\t\t# list of dict\n\t\tself.fc_config = []\t\t\t# list of dict\n\t\tself.conv_weights = []\n\t\tself.conv_biases = []\n\t\tself.fc_weights = []\n\t\tself.fc_biases = []\n\t\tself.pooling_scale = pooling_scale\n\t\tself.pooling_stride = pooling_scale\n\n\t\t# Graph Related\n\t\tself.tf_train_samples = None\n\t\tself.tf_train_labels = None\n\t\tself.tf_test_samples = None\n\t\tself.tf_test_labels = None\n\n\t\t# \xe7\xbb\x9f\xe8\xae\xa1\n\t\tself.writer = None\n\t\tself.merged = None\n\t\tself.train_summaries = []\n\t\tself.test_summaries = []\n\n\t\t# save \xe4\xbf\x9d\xe5\xad\x98\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n\t\tself.saver = None\n\t\tself.save_path = save_path\n\n\tdef add_conv(self, *, patch_size, in_depth, out_depth, activation=\'relu\', pooling=False, name):\n\t\t\'\'\'\n\t\tThis function does not define operations in the graph, but only store config in self.conv_layer_config\n\t\t\'\'\'\n\t\tself.conv_config.append({\n\t\t\t\'patch_size\': patch_size,\n\t\t\t\'in_depth\': in_depth,\n\t\t\t\'out_depth\': out_depth,\n\t\t\t\'activation\': activation,\n\t\t\t\'pooling\': pooling,\n\t\t\t\'name\': name\n\t\t})\n\t\twith tf.name_scope(name):\n\t\t\tweights = tf.Variable(\n\t\t\t\ttf.truncated_normal([patch_size, patch_size, in_depth, out_depth], stddev=0.1), name=name+\'_weights\')\n\t\t\tbiases = tf.Variable(tf.constant(0.1, shape=[out_depth]), name=name+\'_biases\')\n\t\t\tself.conv_weights.append(weights)\n\t\t\tself.conv_biases.append(biases)\n\n\tdef add_fc(self, *, in_num_nodes, out_num_nodes, activation=\'relu\', name):\n\t\t\'\'\'\n\t\tadd fc layer config to slef.fc_layer_config\n\t\t\'\'\'\n\t\tself.fc_config.append({\n\t\t\t\'in_num_nodes\': in_num_nodes,\n\t\t\t\'out_num_nodes\': out_num_nodes,\n\t\t\t\'activation\': activation,\n\t\t\t\'name\': name\n\t\t})\n\t\twith tf.name_scope(name):\n\t\t\tweights = tf.Variable(tf.truncated_normal([in_num_nodes, out_num_nodes], stddev=0.1))\n\t\t\tbiases = tf.Variable(tf.constant(0.1, shape=[out_num_nodes]))\n\t\t\tself.fc_weights.append(weights)\n\t\t\tself.fc_biases.append(biases)\n\t\t\tself.train_summaries.append(tf.histogram_summary(str(len(self.fc_weights))+\'_weights\', weights))\n\t\t\tself.train_summaries.append(tf.histogram_summary(str(len(self.fc_biases))+\'_biases\', biases))\n\n\tdef apply_regularization(self, _lambda):\n\t\t# L2 regularization for the fully connected parameters\n\t\tregularization = 0.0\n\t\tfor weights, biases in zip(self.fc_weights, self.fc_biases):\n\t\t\tregularization += tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n\t\t# 1e5\n\t\treturn _lambda * regularization\n\n\t# should make the definition as an exposed API, instead of implemented in the function\n\tdef define_inputs(self, *, train_samples_shape, train_labels_shape, test_samples_shape):\n\t\t# \xe8\xbf\x99\xe9\x87\x8c\xe5\x8f\xaa\xe6\x98\xaf\xe5\xae\x9a\xe4\xb9\x89\xe5\x9b\xbe\xe8\xb0\xb1\xe4\xb8\xad\xe7\x9a\x84\xe5\x90\x84\xe7\xa7\x8d\xe5\x8f\x98\xe9\x87\x8f\n\t\twith tf.name_scope(\'inputs\'):\n\t\t\tself.tf_train_samples = tf.placeholder(tf.float32, shape=train_samples_shape, name=\'tf_train_samples\')\n\t\t\tself.tf_train_labels = tf.placeholder(tf.float32, shape=train_labels_shape, name=\'tf_train_labels\')\n\t\t\tself.tf_test_samples = tf.placeholder(tf.float32, shape=test_samples_shape, name=\'tf_test_samples\')\n\n\tdef define_model(self):\n\t\t\'\'\'\n\t\t\xe5\xae\x9a\xe4\xb9\x89\xe6\x88\x91\xe7\x9a\x84\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe8\xb0\xb1\n\t\t\'\'\'\n\t\tdef model(data_flow, train=True):\n\t\t\t\'\'\'\n\t\t\t@data: original inputs\n\t\t\t@return: logits\n\t\t\t\'\'\'\n\t\t\t# Define Convolutional Layers\n\t\t\tfor i, (weights, biases, config) in enumerate(zip(self.conv_weights, self.conv_biases, self.conv_config)):\n\t\t\t\twith tf.name_scope(config[\'name\'] + \'_model\'):\n\t\t\t\t\twith tf.name_scope(\'convolution\'):\n\t\t\t\t\t\t# default 1,1,1,1 stride and SAME padding\n\t\t\t\t\t\tdata_flow = tf.nn.conv2d(data_flow, filter=weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n\t\t\t\t\t\tdata_flow = data_flow + biases\n\t\t\t\t\t\tif not train:\n\t\t\t\t\t\t\tself.visualize_filter_map(data_flow, how_many=config[\'out_depth\'], display_size=32//(i//2+1), name=config[\'name\']+\'_conv\')\n\t\t\t\t\tif config[\'activation\'] == \'relu\':\n\t\t\t\t\t\tdata_flow = tf.nn.relu(data_flow)\n\t\t\t\t\t\tif not train:\n\t\t\t\t\t\t\tself.visualize_filter_map(data_flow, how_many=config[\'out_depth\'], display_size=32//(i//2+1), name=config[\'name\']+\'_relu\')\n\t\t\t\t\telse:\n\t\t\t\t\t\traise Exception(\'Activation Func can only be Relu right now. You passed\', config[\'activation\'])\n\t\t\t\t\tif config[\'pooling\']:\n\t\t\t\t\t\tdata_flow = tf.nn.max_pool(\n\t\t\t\t\t\t\tdata_flow,\n\t\t\t\t\t\t\tksize=[1, self.pooling_scale, self.pooling_scale, 1],\n\t\t\t\t\t\t\tstrides=[1, self.pooling_stride, self.pooling_stride, 1],\n\t\t\t\t\t\t\tpadding=\'SAME\')\n\t\t\t\t\t\tif not train:\n\t\t\t\t\t\t\tself.visualize_filter_map(data_flow, how_many=config[\'out_depth\'], display_size=32//(i//2+1)//2, name=config[\'name\']+\'_pooling\')\n\n\t\t\t# Define Fully Connected Layers\n\t\t\tfor i, (weights, biases, config) in enumerate(zip(self.fc_weights, self.fc_biases, self.fc_config)):\n\t\t\t\tif i == 0:\n\t\t\t\t\tshape = data_flow.get_shape().as_list()\n\t\t\t\t\tdata_flow = tf.reshape(data_flow, [shape[0], shape[1] * shape[2] * shape[3]])\n\t\t\t\twith tf.name_scope(config[\'name\'] + \'model\'):\n\n\t\t\t\t\t### Dropout\n\t\t\t\t\tif train and i == len(self.fc_weights) - 1:\n\t\t\t\t\t\tdata_flow =  tf.nn.dropout(data_flow, self.dropout_rate, seed=4926)\n\t\t\t\t\t###\n\n\t\t\t\t\tdata_flow = tf.matmul(data_flow, weights) + biases\n\t\t\t\t\tif config[\'activation\'] == \'relu\':\n\t\t\t\t\t\tdata_flow = tf.nn.relu(data_flow)\n\t\t\t\t\telif config[\'activation\'] is None:\n\t\t\t\t\t\tpass\n\t\t\t\t\telse:\n\t\t\t\t\t\traise Exception(\'Activation Func can only be Relu or None right now. You passed\', config[\'activation\'])\n\t\t\treturn data_flow\n\n\t\t# Training computation.\n\t\tlogits = model(self.tf_train_samples)\n\t\twith tf.name_scope(\'loss\'):\n\t\t\tself.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, self.tf_train_labels))\n\t\t\tself.loss += self.apply_regularization(_lambda=5e-4)\n\t\t\tself.train_summaries.append(tf.scalar_summary(\'Loss\', self.loss))\n\n\t\t# learning rate decay\n\t\tglobal_step = tf.Variable(0)\n\t\tlearning_rate = tf.train.exponential_decay(\n\t\t\tlearning_rate=self.base_learning_rate,\n\t\t\tglobal_step=global_step*self.train_batch_size,\n\t\t\tdecay_steps=100,\n\t\t\tdecay_rate=self.decay_rate,\n\t\t\tstaircase=True\n\t\t)\n\n\t\t# Optimizer.\n\t\twith tf.name_scope(\'optimizer\'):\n\t\t\tif(self.optimizeMethod==\'gradient\'):\n\t\t\t\tself.optimizer = tf.train \\\n\t\t\t\t\t.GradientDescentOptimizer(learning_rate) \\\n\t\t\t\t\t.minimize(self.loss)\n\t\t\telif(self.optimizeMethod==\'momentum\'):\n\t\t\t\tself.optimizer = tf.train \\\n\t\t\t\t\t.MomentumOptimizer(learning_rate, 0.5) \\\n\t\t\t\t\t.minimize(self.loss)\n\t\t\telif(self.optimizeMethod==\'adam\'):\n\t\t\t\tself.optimizer = tf.train \\\n\t\t\t\t\t.AdamOptimizer(learning_rate) \\\n\t\t\t\t\t.minimize(self.loss)\n\n\t\t# Predictions for the training, validation, and test data.\n\t\twith tf.name_scope(\'train\'):\n\t\t\tself.train_prediction = tf.nn.softmax(logits, name=\'train_prediction\')\n\t\t\ttf.add_to_collection(""prediction"", self.train_prediction)\n\t\twith tf.name_scope(\'test\'):\n\t\t\tself.test_prediction = tf.nn.softmax(model(self.tf_test_samples, train=False), name=\'test_prediction\')\n\t\t\ttf.add_to_collection(""prediction"", self.test_prediction)\n\n\t\t\tsingle_shape = (1, 32, 32, 1)\n\t\t\tsingle_input = tf.placeholder(tf.float32, shape=single_shape, name=\'single_input\')\n\t\t\tself.single_prediction = tf.nn.softmax(model(single_input, train=False), name=\'single_prediction\')\n\t\t\ttf.add_to_collection(""prediction"", self.single_prediction)\n\n\t\tself.merged_train_summary = tf.merge_summary(self.train_summaries)\n\t\tself.merged_test_summary = tf.merge_summary(self.test_summaries)\n\n\t\t# \xe6\x94\xbe\xe5\x9c\xa8\xe5\xae\x9a\xe4\xb9\x89Graph\xe4\xb9\x8b\xe5\x90\x8e\xef\xbc\x8c\xe4\xbf\x9d\xe5\xad\x98\xe8\xbf\x99\xe5\xbc\xa0\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\n\t\tself.saver = tf.train.Saver(tf.all_variables())\n\n\tdef run(self, train_samples, train_labels, test_samples, test_labels, *, train_data_iterator, iteration_steps, test_data_iterator):\n\t\t\'\'\'\n\t\t\xe7\x94\xa8\xe5\x88\xb0Session\n\t\t:data_iterator: a function that yields chuck of data\n\t\t\'\'\'\n\t\tself.writer = tf.train.SummaryWriter(\'./board\', tf.get_default_graph())\n\n\t\twith tf.Session(graph=tf.get_default_graph()) as session:\n\t\t\ttf.initialize_all_variables().run()\n\n\t\t\t### \xe8\xae\xad\xe7\xbb\x83\n\t\t\tprint(\'Start Training\')\n\t\t\t# batch 1000\n\t\t\tfor i, samples, labels in train_data_iterator(train_samples, train_labels, iteration_steps=iteration_steps, chunkSize=self.train_batch_size):\n\t\t\t\t_, l, predictions, summary = session.run(\n\t\t\t\t\t[self.optimizer, self.loss, self.train_prediction, self.merged_train_summary],\n\t\t\t\t\tfeed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels}\n\t\t\t\t)\n\t\t\t\tself.writer.add_summary(summary, i)\n\t\t\t\t# labels is True Labels\n\t\t\t\taccuracy, _ = self.accuracy(predictions, labels)\n\t\t\t\tif i % 50 == 0:\n\t\t\t\t\tprint(\'Minibatch loss at step %d: %f\' % (i, l))\n\t\t\t\t\tprint(\'Minibatch accuracy: %.1f%%\' % accuracy)\n\t\t\t###\n\n\t\t\t### \xe6\xb5\x8b\xe8\xaf\x95\n\t\t\taccuracies = []\n\t\t\tconfusionMatrices = []\n\t\t\tfor i, samples, labels in test_data_iterator(test_samples, test_labels, chunkSize=self.test_batch_size):\n\t\t\t\tresult, summary = session.run(\n\t\t\t\t\t[self.test_prediction, self.merged_test_summary],\n\t\t\t\t\tfeed_dict={self.tf_test_samples: samples}\n\t\t\t\t)\n\t\t\t\tself.writer.add_summary(summary, i)\n\t\t\t\taccuracy, cm = self.accuracy(result, labels, need_confusion_matrix=True)\n\t\t\t\taccuracies.append(accuracy)\n\t\t\t\tconfusionMatrices.append(cm)\n\t\t\t\tprint(\'Test Accuracy: %.1f%%\' % accuracy)\n\t\t\tprint(\' Average  Accuracy:\', np.average(accuracies))\n\t\t\tprint(\'Standard Deviation:\', np.std(accuracies))\n\t\t\tself.print_confusion_matrix(np.add.reduce(confusionMatrices))\n\t\t\t###\n\n\tdef train(self, train_samples, train_labels, *, data_iterator, iteration_steps):\n\t\tself.writer = tf.train.SummaryWriter(\'./board\', tf.get_default_graph())\n\t\twith tf.Session(graph=tf.get_default_graph()) as session:\n\t\t\ttf.initialize_all_variables().run()\n\n\t\t\t### \xe8\xae\xad\xe7\xbb\x83\n\t\t\tprint(\'Start Training\')\n\t\t\t# batch 1000\n\t\t\tfor i, samples, labels in data_iterator(train_samples, train_labels, iteration_steps=iteration_steps, chunkSize=self.train_batch_size):\n\t\t\t\t_, l, predictions, summary = session.run(\n\t\t\t\t\t[self.optimizer, self.loss, self.train_prediction, self.merged_train_summary],\n\t\t\t\t\tfeed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels}\n\t\t\t\t)\n\t\t\t\tself.writer.add_summary(summary, i)\n\t\t\t\t# labels is True Labels\n\t\t\t\taccuracy, _ = self.accuracy(predictions, labels)\n\t\t\t\tif i % 50 == 0:\n\t\t\t\t\tprint(\'Minibatch loss at step %d: %f\' % (i, l))\n\t\t\t\t\tprint(\'Minibatch accuracy: %.1f%%\' % accuracy)\n\t\t\t###\n\n\t\t\t# \xe6\xa3\x80\xe6\x9f\xa5\xe8\xa6\x81\xe5\xad\x98\xe6\x94\xbe\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xe5\x80\xbc\xe5\x90\xa6\xe5\xad\x98\xe5\x9c\xa8\xe3\x80\x82\xe8\xbf\x99\xe9\x87\x8c\xe5\x81\x87\xe5\xae\x9a\xe5\x8f\xaa\xe6\x9c\x89\xe4\xb8\x80\xe5\xb1\x82\xe8\xb7\xaf\xe5\xbe\x84\xe3\x80\x82\n\t\t\timport os\n\t\t\tif os.path.isdir(self.save_path.split(\'/\')[0]):\n\t\t\t\tsave_path = self.saver.save(session, self.save_path)\n\t\t\t\tprint(""Model saved in file: %s"" % save_path)\n\t\t\telse:\n\t\t\t\tos.makedirs(self.save_path.split(\'/\')[0])\n\t\t\t\tsave_path = self.saver.save(session, self.save_path)\n\t\t\t\tprint(""Model saved in file: %s"" % save_path)\n\n\tdef test(self, test_samples, test_labels, *, data_iterator):\n\t\tif self.saver is None:\n\t\t\tself.define_model()\n\t\tif self.writer is None:\n\t\t\tself.writer = tf.train.SummaryWriter(\'./board\', tf.get_default_graph())\n\t\t\n\t\tprint(\'Before session\')\n\t\twith tf.Session(graph=tf.get_default_graph()) as session:\n\t\t\tself.saver.restore(session, self.save_path)\n\t\t\t### \xe6\xb5\x8b\xe8\xaf\x95\n\t\t\taccuracies = []\n\t\t\tconfusionMatrices = []\n\t\t\tfor i, samples, labels in data_iterator(test_samples, test_labels, chunkSize=self.test_batch_size):\n\t\t\t\tresult= session.run(\n\t\t\t\t\tself.test_prediction,\n\t\t\t\t\tfeed_dict={self.tf_test_samples: samples}\n\t\t\t\t)\n\t\t\t\t#self.writer.add_summary(summary, i)\n\t\t\t\taccuracy, cm = self.accuracy(result, labels, need_confusion_matrix=True)\n\t\t\t\taccuracies.append(accuracy)\n\t\t\t\tconfusionMatrices.append(cm)\n\t\t\t\tprint(\'Test Accuracy: %.1f%%\' % accuracy)\n\t\t\tprint(\' Average  Accuracy:\', np.average(accuracies))\n\t\t\tprint(\'Standard Deviation:\', np.std(accuracies))\n\t\t\tself.print_confusion_matrix(np.add.reduce(confusionMatrices))\n\t\t\t###\n\n\n\tdef accuracy(self, predictions, labels, need_confusion_matrix=False):\n\t\t\'\'\'\n\t\t\xe8\xae\xa1\xe7\xae\x97\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87\xe4\xb8\x8e\xe5\x8f\xac\xe5\x9b\x9e\xe7\x8e\x87\n\t\t@return: accuracy and confusionMatrix as a tuple\n\t\t\'\'\'\n\t\t_predictions = np.argmax(predictions, 1)\n\t\t_labels = np.argmax(labels, 1)\n\t\tcm = confusion_matrix(_labels, _predictions) if need_confusion_matrix else None\n\t\t# == is overloaded for numpy array\n\t\taccuracy = (100.0 * np.sum(_predictions == _labels) / predictions.shape[0])\n\t\treturn accuracy, cm\n\n\tdef visualize_filter_map(self, tensor, *, how_many, display_size, name):\n\t\t#print(tensor.get_shape)\n\t\tfilter_map = tensor[-1]\n\t\t#print(filter_map.get_shape())\n\t\tfilter_map = tf.transpose(filter_map, perm=[2, 0, 1])\n\t\t#print(filter_map.get_shape())\n\t\tfilter_map = tf.reshape(filter_map, (how_many, display_size, display_size, 1))\n\t\t#print(how_many)\n\t\tself.test_summaries.append(tf.image_summary(name, tensor=filter_map, max_images=how_many))\n\n\tdef print_confusion_matrix(self, confusionMatrix):\n\t\tprint(\'Confusion    Matrix:\')\n\t\tfor i, line in enumerate(confusionMatrix):\n\t\t\tprint(line, line[i] / np.sum(line))\n\t\ta = 0\n\t\tfor i, column in enumerate(np.transpose(confusionMatrix, (1, 0))):\n\t\t\ta += (column[i] / np.sum(column)) * (np.sum(column) / 26000)\n\t\t\tprint(column[i] / np.sum(column), )\n\t\tprint(\'\\n\', np.sum(confusionMatrix), a)\n'"
Season1/20/load.py,0,"b""# encoding:utf-8\n# Python2 \xe5\x85\xbc\xe5\xae\xb9\nfrom __future__ import print_function, division\nfrom scipy.io import loadmat as load\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef reformat(samples, labels):\n\t# \xe6\x94\xb9\xe5\x8f\x98\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\n\t#  0       1       2      3          3       0       1      2\n\t# (\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0) -> (\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0)\n\tnew = np.transpose(samples, (3, 0, 1, 2)).astype(np.float32)\n\n\t# labels \xe5\x8f\x98\xe6\x88\x90 one-hot encoding, [2] -> [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n\t# digit 0 , represented as 10\n\t# labels \xe5\x8f\x98\xe6\x88\x90 one-hot encoding, [10] -> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\tlabels = np.array([x[0] for x in labels])\t# slow code, whatever\n\tone_hot_labels = []\n\tfor num in labels:\n\t\tone_hot = [0.0] * 10\n\t\tif num == 10:\n\t\t\tone_hot[0] = 1.0\n\t\telse:\n\t\t\tone_hot[num] = 1.0\n\t\tone_hot_labels.append(one_hot)\n\tlabels = np.array(one_hot_labels).astype(np.float32)\n\treturn new, labels\n\ndef normalize(samples):\n\t'''\n\t\xe5\xb9\xb6\xe4\xb8\x94\xe7\x81\xb0\xe5\xba\xa6\xe5\x8c\x96: \xe4\xbb\x8e\xe4\xb8\x89\xe8\x89\xb2\xe9\x80\x9a\xe9\x81\x93 -> \xe5\x8d\x95\xe8\x89\xb2\xe9\x80\x9a\xe9\x81\x93     \xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98 + \xe5\x8a\xa0\xe5\xbf\xab\xe8\xae\xad\xe7\xbb\x83\xe9\x80\x9f\xe5\xba\xa6\n\t(R + G + B) / 3\n\t\xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe4\xbb\x8e 0 ~ 255 \xe7\xba\xbf\xe6\x80\xa7\xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0 -1.0 ~ +1.0\n\t@samples: numpy array\n\t'''\n\ta = np.add.reduce(samples, keepdims=True, axis=3)  # shape (\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0)\n\ta = a/3.0\n\treturn a/128.0 - 1.0\n\n\ndef distribution(labels, name):\n\t# \xe6\x9f\xa5\xe7\x9c\x8b\xe4\xb8\x80\xe4\xb8\x8b\xe6\xaf\x8f\xe4\xb8\xaalabel\xe7\x9a\x84\xe5\x88\x86\xe5\xb8\x83\xef\xbc\x8c\xe5\x86\x8d\xe7\x94\xbb\xe4\xb8\xaa\xe7\xbb\x9f\xe8\xae\xa1\xe5\x9b\xbe\n\t# keys:\n\t# 0\n\t# 1\n\t# 2\n\t# ...\n\t# 9\n\tcount = {}\n\tfor label in labels:\n\t\tkey = 0 if label[0] == 10 else label[0]\n\t\tif key in count:\n\t\t\tcount[key] += 1\n\t\telse:\n\t\t\tcount[key] = 1\n\tx = []\n\ty = []\n\tfor k, v in count.items():\n\t\t# print(k, v)\n\t\tx.append(k)\n\t\ty.append(v)\n\n\ty_pos = np.arange(len(x))\n\tplt.bar(y_pos, y, align='center', alpha=0.5)\n\tplt.xticks(y_pos, x)\n\tplt.ylabel('Count')\n\tplt.title(name + ' Label Distribution')\n\tplt.show()\n\ndef inspect(dataset, labels, i):\n\t# \xe6\x98\xbe\xe7\xa4\xba\xe5\x9b\xbe\xe7\x89\x87\xe7\x9c\x8b\xe7\x9c\x8b\n\tif dataset.shape[3] == 1:\n\t\tshape = dataset.shape\n\t\tdataset = dataset.reshape(shape[0], shape[1], shape[2])\n\tprint(labels[i])\n\tplt.imshow(dataset[i])\n\tplt.show()\n\n\ntrain = load('../data/train_32x32.mat')\ntest = load('../data/test_32x32.mat')\n# extra = load('../data/extra_32x32.mat')\n\n# print('Train Samples Shape:', train['X'].shape)\n# print('Train  Labels Shape:', train['y'].shape)\n\n# print('Train Samples Shape:', test['X'].shape)\n# print('Train  Labels Shape:', test['y'].shape)\n\n# print('Train Samples Shape:', extra['X'].shape)\n# print('Train  Labels Shape:', extra['y'].shape)\n\ntrain_samples = train['X']\ntrain_labels = train['y']\ntest_samples = test['X']\ntest_labels = test['y']\n# extra_samples = extra['X']\n# extra_labels = extra['y']\n\nn_train_samples, _train_labels = reformat(train_samples, train_labels)\nn_test_samples, _test_labels = reformat(test_samples, test_labels)\n\n_train_samples = normalize(n_train_samples)\n_test_samples = normalize(n_test_samples)\n\nnum_labels = 10\nimage_size = 32\nnum_channels = 1\n\nif __name__ == '__main__':\n\t# \xe6\x8e\xa2\xe7\xb4\xa2\xe6\x95\xb0\xe6\x8d\xae\n\tpass\n\tinspect(_train_samples, _train_labels, 1234)\n\t# _train_samples = normalize(_train_samples)\n\t# inspect(_train_samples, _train_labels, 1234)\n\t# distribution(train_labels, 'Train Labels')\n\t# distribution(test_labels, 'Test Labels')\n"""
Season1/20/main.py,0,"b'if __name__ == \'__main__\':\n\timport load\n\tfrom dp import Network\n\n\ttrain_samples, train_labels = load._train_samples, load._train_labels\n\ttest_samples, test_labels = load._test_samples, load._test_labels\n\n\tprint(\'Training set\', train_samples.shape, train_labels.shape)\n\tprint(\'    Test set\', test_samples.shape, test_labels.shape)\n\n\timage_size = load.image_size\n\tnum_labels = load.num_labels\n\tnum_channels = load.num_channels\n\n\tdef train_data_iterator(samples, labels, iteration_steps, chunkSize):\n\t\t\'\'\'\n\t\tIterator/Generator: get a batch of data\n\t\t\xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8/\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\x80\xe6\xac\xa1\xe5\x8f\xaa\xe5\xbe\x97\xe5\x88\xb0 chunkSize \xe8\xbf\x99\xe4\xb9\x88\xe5\xa4\x9a\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n\t\t\xe7\x94\xa8\xe4\xba\x8e for loop\xef\xbc\x8c just like range() function\n\t\t\'\'\'\n\t\tif len(samples) != len(labels):\n\t\t\traise Exception(\'Length of samples and labels must equal\')\n\t\tstepStart = 0  # initial step\n\t\ti = 0\n\t\twhile i < iteration_steps:\n\t\t\tstepStart = (i * chunkSize) % (labels.shape[0] - chunkSize)\n\t\t\tyield i, samples[stepStart:stepStart + chunkSize], labels[stepStart:stepStart + chunkSize]\n\t\t\ti += 1\n\n\tdef test_data_iterator(samples, labels, chunkSize):\n\t\t\'\'\'\n\t\tIterator/Generator: get a batch of data\n\t\t\xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8/\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\x80\xe6\xac\xa1\xe5\x8f\xaa\xe5\xbe\x97\xe5\x88\xb0 chunkSize \xe8\xbf\x99\xe4\xb9\x88\xe5\xa4\x9a\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n\t\t\xe7\x94\xa8\xe4\xba\x8e for loop\xef\xbc\x8c just like range() function\n\t\t\'\'\'\n\t\tif len(samples) != len(labels):\n\t\t\traise Exception(\'Length of samples and labels must equal\')\n\t\tstepStart = 0  # initial step\n\t\ti = 0\n\t\twhile stepStart < len(samples):\n\t\t\tstepEnd = stepStart + chunkSize\n\t\t\tif stepEnd < len(samples):\n\t\t\t\tyield i, samples[stepStart:stepEnd], labels[stepStart:stepEnd]\n\t\t\t\ti += 1\n\t\t\tstepStart = stepEnd\n\n\n\tnet = Network(\n\t\ttrain_batch_size=64, test_batch_size=500, pooling_scale=2,\n\t\tdropout_rate = 0.9,\n\t\tbase_learning_rate = 0.001, decay_rate=0.99)\n\tnet.define_inputs(\n\t\t\ttrain_samples_shape=(64, image_size, image_size, num_channels),\n\t\t\ttrain_labels_shape=(64, num_labels),\n\t\t\ttest_samples_shape=(500, image_size, image_size, num_channels),\n\t\t)\n\t#\n\tnet.add_conv(patch_size=3, in_depth=num_channels, out_depth=32, activation=\'relu\', pooling=False, name=\'conv1\')\n\tnet.add_conv(patch_size=3, in_depth=32, out_depth=32, activation=\'relu\', pooling=True, name=\'conv2\')\n\tnet.add_conv(patch_size=3, in_depth=32, out_depth=32, activation=\'relu\', pooling=False, name=\'conv3\')\n\tnet.add_conv(patch_size=3, in_depth=32, out_depth=32, activation=\'relu\', pooling=True, name=\'conv4\')\n\n\t# 4 = \xe4\xb8\xa4\xe6\xac\xa1 pooling, \xe6\xaf\x8f\xe4\xb8\x80\xe6\xac\xa1\xe7\xbc\xa9\xe5\xb0\x8f\xe4\xb8\xba 1/2\n\t# 32 = conv4 out_depth\n\tnet.add_fc(in_num_nodes=(image_size // 4) * (image_size // 4) * 32, out_num_nodes=128, activation=\'relu\', name=\'fc1\')\n\tnet.add_fc(in_num_nodes=128, out_num_nodes=10, activation=None, name=\'fc2\')\n\n\tnet.define_model()\n\t#net.run(train_samples, train_labels, test_samples, test_labels, train_data_iterator=train_data_iterator, iteration_steps=3000, test_data_iterator=test_data_iterator)\n\t#net.train(train_samples, train_labels, data_iterator=train_data_iterator, iteration_steps=2000)\n\tnet.test(test_samples, test_labels, data_iterator=test_data_iterator)\n\nelse:\n\traise Exception(\'main.py: Should Not Be Imported!!! Must Run by ""python main.py""\')\n'"
Season1/4-6/load.py,0,"b""# encoding:utf-8\n# Python2 \xe5\x85\xbc\xe5\xae\xb9\nfrom __future__ import print_function, division\nfrom scipy.io import loadmat as load\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef reformat(samples, labels):\n\t# \xe6\x94\xb9\xe5\x8f\x98\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\n\t#  0       1       2      3          3       0       1      2\n\t# (\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0) -> (\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0)\n\tnew = np.transpose(samples, (3, 0, 1, 2)).astype(np.float32)\n\n\t# labels \xe5\x8f\x98\xe6\x88\x90 one-hot encoding, [2] -> [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n\t# digit 0 , represented as 10\n\t# labels \xe5\x8f\x98\xe6\x88\x90 one-hot encoding, [10] -> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\tlabels = np.array([x[0] for x in labels])\t# slow code, whatever\n\tone_hot_labels = []\n\tfor num in labels:\n\t\tone_hot = [0.0] * 10\n\t\tif num == 10:\n\t\t\tone_hot[0] = 1.0\n\t\telse:\n\t\t\tone_hot[num] = 1.0\n\t\tone_hot_labels.append(one_hot)\n\tlabels = np.array(one_hot_labels).astype(np.float32)\n\treturn new, labels\n\ndef normalize(samples):\n\t'''\n\t\xe5\xb9\xb6\xe4\xb8\x94\xe7\x81\xb0\xe5\xba\xa6\xe5\x8c\x96: \xe4\xbb\x8e\xe4\xb8\x89\xe8\x89\xb2\xe9\x80\x9a\xe9\x81\x93 -> \xe5\x8d\x95\xe8\x89\xb2\xe9\x80\x9a\xe9\x81\x93     \xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98 + \xe5\x8a\xa0\xe5\xbf\xab\xe8\xae\xad\xe7\xbb\x83\xe9\x80\x9f\xe5\xba\xa6\n\t(R + G + B) / 3\n\t\xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe4\xbb\x8e 0 ~ 255 \xe7\xba\xbf\xe6\x80\xa7\xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0 -1.0 ~ +1.0\n\t@samples: numpy array\n\t'''\n\ta = np.add.reduce(samples, keepdims=True, axis=3)  # shape (\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0)\n\ta = a/3.0\n\treturn a/128.0 - 1.0\n\n\ndef distribution(labels, name):\n\t# \xe6\x9f\xa5\xe7\x9c\x8b\xe4\xb8\x80\xe4\xb8\x8b\xe6\xaf\x8f\xe4\xb8\xaalabel\xe7\x9a\x84\xe5\x88\x86\xe5\xb8\x83\xef\xbc\x8c\xe5\x86\x8d\xe7\x94\xbb\xe4\xb8\xaa\xe7\xbb\x9f\xe8\xae\xa1\xe5\x9b\xbe\n\t# keys:\n\t# 0\n\t# 1\n\t# 2\n\t# ...\n\t# 9\n\tcount = {}\n\tfor label in labels:\n\t\tkey = 0 if label[0] == 10 else label[0]\n\t\tif key in count:\n\t\t\tcount[key] += 1\n\t\telse:\n\t\t\tcount[key] = 1\n\tx = []\n\ty = []\n\tfor k, v in count.items():\n\t\t# print(k, v)\n\t\tx.append(k)\n\t\ty.append(v)\n\n\ty_pos = np.arange(len(x))\n\tplt.bar(y_pos, y, align='center', alpha=0.5)\n\tplt.xticks(y_pos, x)\n\tplt.ylabel('Count')\n\tplt.title(name + ' Label Distribution')\n\tplt.show()\n\ndef inspect(dataset, labels, i):\n\t# \xe6\x98\xbe\xe7\xa4\xba\xe5\x9b\xbe\xe7\x89\x87\xe7\x9c\x8b\xe7\x9c\x8b\n\tprint(labels[i])\n\t'''\n\tif dataset.shape[3] == 1:\n\t\tshape = dataset.shape\n\t\tdataset = dataset.reshape(shape[0], shape[1], shape[2])\n\tplt.imshow(dataset[i])\n\t'''#\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x94\xb9\xe4\xb8\xba\xe4\xbb\xa5\xe4\xb8\x8b\t\n\tplt.imshow(dataset[i].squeeze())\n\tplt.show()\n\n\ntrain = load('../data/train_32x32.mat')\ntest = load('../data/test_32x32.mat')\n# extra = load('../data/extra_32x32.mat')\n\nprint('Train Samples Shape:', train['X'].shape)\nprint('Train  Labels Shape:', train['y'].shape)\n\nprint('Train Samples Shape:', test['X'].shape)\nprint('Train  Labels Shape:', test['y'].shape)\n\n# print('Train Samples Shape:', extra['X'].shape)\n# print('Train  Labels Shape:', extra['y'].shape)\n\ntrain_samples = train['X']\ntrain_labels = train['y']\n# test_samples = test['X']\n# test_labels = test['y']\n# test_samples = extra['X']\n# test_labels = extra['y']\n\n_train_samples, _train_labels = reformat(train_samples, train_labels)\n# _test_samples, _test_labels = reformat(test_samples, test_labels)\n#\n# _train_dataset = normalize(n_train_dataset)\n# _test_dataset = normalize(n_test_dataset)\n\nnum_labels = 10\nimage_size = 32\n\nif __name__ == '__main__':\n\t# \xe6\x8e\xa2\xe7\xb4\xa2\xe6\x95\xb0\xe6\x8d\xae\n\tpass\n\t# inspect(_train_samples, _train_labels, 1234)\n\t# _train_samples = normalize(_train_samples)\n\t# inspect(_train_samples, _train_labels, 1234)\n\t# distribution(train_labels, 'Train Labels')\n\t# distribution(test_labels, 'Test Labels')\n"""
Season1/4-6/run.py,0,"b'#\xe3\x80\x8aTF Girls \xe4\xbf\xae\xe7\x82\xbc\xe6\x8c\x87\xe5\x8d\x97\xe3\x80\x8b\xe7\xac\xac\xe5\x9b\x9b\xe6\x9c\x9f\n\n# \xe6\xad\xa3\xe5\xbc\x8f\xe5\xbc\x80\xe5\xa7\x8b\xe6\x9c\xba\xe5\x99\xa8\xe5\xad\xa6\xe4\xb9\xa0\n\n# \xe9\xa6\x96\xe5\x85\x88\xe6\x88\x91\xe4\xbb\xac\xe8\xa6\x81\xe7\xa1\xae\xe5\xae\x9a\xe4\xb8\x80\xe4\xb8\xaa\xe7\x9b\xae\xe6\xa0\x87: \xe5\x9b\xbe\xe5\x83\x8f\xe8\xaf\x86\xe5\x88\xab\n\n# \xe6\x88\x91\xe8\xbf\x99\xe9\x87\x8c\xe5\xb0\xb1\xe7\x94\xa8Udacity Deep Learning\xe7\x9a\x84\xe4\xbd\x9c\xe4\xb8\x9a\xe4\xbd\x9c\xe4\xb8\xba\xe8\xbe\x85\xe5\x8a\xa9\xe4\xba\x86\n\n# 1. \xe4\xb8\x8b\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae  http://ufldl.stanford.edu/housenumbers/\n# 2. \xe6\x8e\xa2\xe7\xb4\xa2\xe6\x95\xb0\xe6\x8d\xae\n# 3. \xe5\xa4\x84\xe7\x90\x86\xe6\x95\xb0\xe6\x8d\xae\n# 4. \xe6\x9e\x84\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x9f\xba\xe6\x9c\xac\xe7\xbd\x91\xe7\xbb\x9c, \xe5\x9f\xba\xe6\x9c\xac\xe7\x9a\x84\xe6\xa6\x82\xe5\xbf\xb5+\xe4\xbb\xa3\xe7\xa0\x81 \xef\xbc\x8c TensorFlow\xe7\x9a\x84\xe4\xb8\x96\xe7\x95\x8c\n# 5. \xe5\x8d\xb7\xe7\xa7\xafji\n# 6. \xe6\x9d\xa5\xe5\xae\x9e\xe9\xaa\x8c\xe5\x90\xa7\n# 7. \xe5\xbe\xae\xe8\xb0\x83\xe4\xb8\x8e\xe7\xbb\x93\xe6\x9e\x9c\n'"
Season1/7-9/dp.py,23,"b""from __future__ import print_function, division\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nimport load\n\ntrain_samples, train_labels = load._train_samples, load._train_labels\ntest_samples, test_labels = load._test_samples,  load._test_labels\n\nprint('Training set', train_samples.shape, train_labels.shape)\nprint('    Test set', test_samples.shape, test_labels.shape)\n\nimage_size = load.image_size\nnum_labels = load.num_labels\nnum_channels = load.num_channels\n\ndef get_chunk(samples, labels, chunkSize):\n\t'''\n\tIterator/Generator: get a batch of data\n\t\xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8/\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\x80\xe6\xac\xa1\xe5\x8f\xaa\xe5\xbe\x97\xe5\x88\xb0 chunkSize \xe8\xbf\x99\xe4\xb9\x88\xe5\xa4\x9a\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n\t\xe7\x94\xa8\xe4\xba\x8e for loop\xef\xbc\x8c just like range() function\n\t'''\n\tif len(samples) != len(labels):\n\t\traise Exception('Length of samples and labels must equal')\n\tstepStart = 0\t# initial step\n\ti = 0\n\twhile stepStart < len(samples):\n\t\tstepEnd = stepStart + chunkSize\n\t\tif stepEnd < len(samples):\n\t\t\tyield i, samples[stepStart:stepEnd], labels[stepStart:stepEnd]\n\t\t\ti += 1\n\t\tstepStart = stepEnd\n\n\nclass Network():\n\tdef __init__(self, num_hidden, batch_size):\n\t\t'''\n\t\t@num_hidden: \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\x9a\x84\xe8\x8a\x82\xe7\x82\xb9\xe6\x95\xb0\xe9\x87\x8f\n\t\t@batch_size\xef\xbc\x9a\xe5\x9b\xa0\xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe8\xa6\x81\xe8\x8a\x82\xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x88\x86\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\xe6\xaf\x8f\xe4\xb8\x80\xe6\x89\xb9\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x87\x8f\xe3\x80\x82\n\t\t'''\n\t\tself.batch_size = batch_size\n\t\tself.test_batch_size = 500\n\n\t\t# Hyper Parameters\n\t\tself.num_hidden = num_hidden\n\n\t\t# Graph Related\n\t\tself.graph = tf.Graph()\n\t\tself.tf_train_samples = None\n\t\tself.tf_train_labels = None\n\t\tself.tf_test_samples = None\n\t\tself.tf_test_labels = None\n\t\tself.tf_test_prediction = None\n\n\tdef define_graph(self):\n\t\t'''\n\t\t\xe5\xae\x9a\xe4\xb9\x89\xe6\x88\x91\xe7\x9a\x84\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe8\xb0\xb1\n\t\t'''\n\t\twith self.graph.as_default():\n\t\t\t# \xe8\xbf\x99\xe9\x87\x8c\xe5\x8f\xaa\xe6\x98\xaf\xe5\xae\x9a\xe4\xb9\x89\xe5\x9b\xbe\xe8\xb0\xb1\xe4\xb8\xad\xe7\x9a\x84\xe5\x90\x84\xe7\xa7\x8d\xe5\x8f\x98\xe9\x87\x8f\n\t\t\tself.tf_train_samples = tf.placeholder(\n\t\t\t\ttf.float32, shape=(self.batch_size, image_size, image_size, num_channels)\n\t\t\t)\n\t\t\tself.tf_train_labels  = tf.placeholder(\n\t\t\t\ttf.float32, shape=(self.batch_size, num_labels)\n\t\t\t)\n\t\t\tself.tf_test_samples  = tf.placeholder(\n\t\t\t\ttf.float32, shape=(self.test_batch_size, image_size, image_size, num_channels)\n\t\t\t)\n\n\t\t\t# fully connected layer 1, fully connected\n\t\t\tfc1_weights = tf.Variable(\n\t\t\t\ttf.truncated_normal([image_size * image_size, self.num_hidden], stddev=0.1)\n\t\t\t)\n\t\t\tfc1_biases = tf.Variable(tf.constant(0.1, shape=[self.num_hidden]))\n\n\t\t\t# fully connected layer 2 --> output layer\n\t\t\tfc2_weights = tf.Variable(\n\t\t\t\ttf.truncated_normal([self.num_hidden, num_labels], stddev=0.1)\n\t\t\t)\n\t\t\tfc2_biases = tf.Variable(tf.constant(0.1, shape=[num_labels]))\n\n\t\t\t# \xe6\x83\xb3\xe5\x9c\xa8\xe6\x9d\xa5\xe5\xae\x9a\xe4\xb9\x89\xe5\x9b\xbe\xe8\xb0\xb1\xe7\x9a\x84\xe8\xbf\x90\xe7\xae\x97\n\t\t\tdef model(data):\n\t\t\t\t# fully connected layer 1\n\t\t\t\tshape = data.get_shape().as_list()\n\t\t\t\tprint(data.get_shape(), shape)\n\t\t\t\treshape = tf.reshape(data, [shape[0], shape[1] * shape[2] * shape[3]])\n\t\t\t\tprint(reshape.get_shape(), fc1_weights.get_shape(), fc1_biases.get_shape())\n\t\t\t\thidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n\n\t\t\t\t# fully connected layer 2\n\t\t\t\treturn tf.matmul(hidden, fc2_weights) + fc2_biases\n\n\t\t\t# Training computation.\n\t\t\tlogits = model(self.tf_train_samples)\n\t\t\tself.loss = tf.reduce_mean(\n\t\t\t\ttf.nn.softmax_cross_entropy_with_logits(logits, self.tf_train_labels)\n\t\t\t)\n\n\t\t\t# Optimizer.\n\t\t\tself.optimizer = tf.train.GradientDescentOptimizer(0.0001).minimize(self.loss)\n\n\t\t\t# Predictions for the training, validation, and test data.\n\t\t\tself.train_prediction = tf.nn.softmax(logits)\n\t\t\tself.test_prediction = tf.nn.softmax(model(self.tf_test_samples))\n\n\tdef run(self):\n\t\t'''\n\t\t\xe7\x94\xa8\xe5\x88\xb0Session\n\t\t'''\n\t\t# private function\n\t\tdef print_confusion_matrix(confusionMatrix):\n\t\t\tprint('Confusion    Matrix:')\n\t\t\tfor i, line in enumerate(confusionMatrix):\n\t\t\t\tprint(line, line[i]/np.sum(line))\n\t\t\ta = 0\n\t\t\tfor i, column in enumerate(np.transpose(confusionMatrix, (1, 0))):\n\t\t\t\ta += (column[i]/np.sum(column))*(np.sum(column)/26000)\n\t\t\t\tprint(column[i]/np.sum(column),)\n\t\t\tprint('\\n',np.sum(confusionMatrix), a)\n\n\n\t\tself.session = tf.Session(graph=self.graph)\n\t\twith self.session as session:\n\t\t\ttf.initialize_all_variables().run()\n\n\t\t\t### \xe8\xae\xad\xe7\xbb\x83\n\t\t\tprint('Start Training')\n\t\t\t# batch 1000\n\t\t\tfor i, samples, labels in get_chunk(train_samples, train_labels, chunkSize=self.batch_size):\n\t\t\t\t_, l, predictions = session.run(\n\t\t\t\t\t[self.optimizer, self.loss, self.train_prediction],\n\t\t\t\t\tfeed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels}\n\t\t\t\t)\n\t\t\t\t# labels is True Labels\n\t\t\t\taccuracy, _ = self.accuracy(predictions, labels)\n\t\t\t\tif i % 50 == 0:\n\t\t\t\t\tprint('Minibatch loss at step %d: %f' % (i, l))\n\t\t\t\t\tprint('Minibatch accuracy: %.1f%%' % accuracy)\n\t\t\t###\n\n\t\t\t### \xe6\xb5\x8b\xe8\xaf\x95\n\t\t\taccuracies = []\n\t\t\tconfusionMatrices = []\n\t\t\tfor i, samples, labels in get_chunk(test_samples, test_labels, chunkSize=self.test_batch_size):\n\t\t\t\tresult = self.test_prediction.eval(feed_dict={self.tf_test_samples: samples})\n\t\t\t\taccuracy, cm = self.accuracy(result, labels, need_confusion_matrix=True)\n\t\t\t\taccuracies.append(accuracy)\n\t\t\t\tconfusionMatrices.append(cm)\n\t\t\t\tprint('Test Accuracy: %.1f%%' % accuracy)\n\t\t\tprint(' Average  Accuracy:', np.average(accuracies))\n\t\t\tprint('Standard Deviation:', np.std(accuracies))\n\t\t\tprint_confusion_matrix(np.add.reduce(confusionMatrices))\n\t\t\t###\n\n\tdef accuracy(self, predictions, labels, need_confusion_matrix=False):\n\t\t'''\n\t\t\xe8\xae\xa1\xe7\xae\x97\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87\xe4\xb8\x8e\xe5\x8f\xac\xe5\x9b\x9e\xe7\x8e\x87\n\t\t@return: accuracy and confusionMatrix as a tuple\n\t\t'''\n\t\t_predictions = np.argmax(predictions, 1)\n\t\t_labels = np.argmax(labels, 1)\n\t\tcm = confusion_matrix(_labels, _predictions) if need_confusion_matrix else None\n\t\t# == is overloaded for numpy array\n\t\taccuracy = (100.0 * np.sum(_predictions == _labels) / predictions.shape[0])\n\t\treturn accuracy, cm\n\n\nif __name__ == '__main__':\n\tnet = Network(num_hidden=128, batch_size=100)\n\tnet.define_graph()\n\tnet.run()\n"""
Season1/7-9/load.py,0,"b""# encoding:utf-8\n# Python2 \xe5\x85\xbc\xe5\xae\xb9\nfrom __future__ import print_function, division\nfrom scipy.io import loadmat as load\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef reformat(samples, labels):\n\t# \xe6\x94\xb9\xe5\x8f\x98\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\n\t#  0       1       2      3          3       0       1      2\n\t# (\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0) -> (\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0)\n\tnew = np.transpose(samples, (3, 0, 1, 2)).astype(np.float32)\n\n\t# labels \xe5\x8f\x98\xe6\x88\x90 one-hot encoding, [2] -> [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n\t# digit 0 , represented as 10\n\t# labels \xe5\x8f\x98\xe6\x88\x90 one-hot encoding, [10] -> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\tlabels = np.array([x[0] for x in labels])\t# slow code, whatever\n\tone_hot_labels = []\n\tfor num in labels:\n\t\tone_hot = [0.0] * 10\n\t\tif num == 10:\n\t\t\tone_hot[0] = 1.0\n\t\telse:\n\t\t\tone_hot[num] = 1.0\n\t\tone_hot_labels.append(one_hot)\n\tlabels = np.array(one_hot_labels).astype(np.float32)\n\treturn new, labels\n\ndef normalize(samples):\n\t'''\n\t\xe5\xb9\xb6\xe4\xb8\x94\xe7\x81\xb0\xe5\xba\xa6\xe5\x8c\x96: \xe4\xbb\x8e\xe4\xb8\x89\xe8\x89\xb2\xe9\x80\x9a\xe9\x81\x93 -> \xe5\x8d\x95\xe8\x89\xb2\xe9\x80\x9a\xe9\x81\x93     \xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98 + \xe5\x8a\xa0\xe5\xbf\xab\xe8\xae\xad\xe7\xbb\x83\xe9\x80\x9f\xe5\xba\xa6\n\t(R + G + B) / 3\n\t\xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe4\xbb\x8e 0 ~ 255 \xe7\xba\xbf\xe6\x80\xa7\xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0 -1.0 ~ +1.0\n\t@samples: numpy array\n\t'''\n\ta = np.add.reduce(samples, keepdims=True, axis=3)  # shape (\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0)\n\ta = a/3.0\n\treturn a/128.0 - 1.0\n\n\ndef distribution(labels, name):\n\t# \xe6\x9f\xa5\xe7\x9c\x8b\xe4\xb8\x80\xe4\xb8\x8b\xe6\xaf\x8f\xe4\xb8\xaalabel\xe7\x9a\x84\xe5\x88\x86\xe5\xb8\x83\xef\xbc\x8c\xe5\x86\x8d\xe7\x94\xbb\xe4\xb8\xaa\xe7\xbb\x9f\xe8\xae\xa1\xe5\x9b\xbe\n\t# keys:\n\t# 0\n\t# 1\n\t# 2\n\t# ...\n\t# 9\n\tcount = {}\n\tfor label in labels:\n\t\tkey = 0 if label[0] == 10 else label[0]\n\t\tif key in count:\n\t\t\tcount[key] += 1\n\t\telse:\n\t\t\tcount[key] = 1\n\tx = []\n\ty = []\n\tfor k, v in count.items():\n\t\t# print(k, v)\n\t\tx.append(k)\n\t\ty.append(v)\n\n\ty_pos = np.arange(len(x))\n\tplt.bar(y_pos, y, align='center', alpha=0.5)\n\tplt.xticks(y_pos, x)\n\tplt.ylabel('Count')\n\tplt.title(name + ' Label Distribution')\n\tplt.show()\n\ndef inspect(dataset, labels, i):\n\t# \xe6\x98\xbe\xe7\xa4\xba\xe5\x9b\xbe\xe7\x89\x87\xe7\x9c\x8b\xe7\x9c\x8b\n\tif dataset.shape[3] == 1:\n\t\tshape = dataset.shape\n\t\tdataset = dataset.reshape(shape[0], shape[1], shape[2])\n\tprint(labels[i])\n\tplt.imshow(dataset[i])\n\tplt.show()\n\n\ntrain = load('../data/train_32x32.mat')\ntest = load('../data/test_32x32.mat')\n# extra = load('../data/extra_32x32.mat')\n\n# print('Train Samples Shape:', train['X'].shape)\n# print('Train  Labels Shape:', train['y'].shape)\n\n# print('Train Samples Shape:', test['X'].shape)\n# print('Train  Labels Shape:', test['y'].shape)\n\n# print('Train Samples Shape:', extra['X'].shape)\n# print('Train  Labels Shape:', extra['y'].shape)\n\ntrain_samples = train['X']\ntrain_labels = train['y']\ntest_samples = test['X']\ntest_labels = test['y']\n# extra_samples = extra['X']\n# extra_labels = extra['y']\n\nn_train_samples, _train_labels = reformat(train_samples, train_labels)\nn_test_samples, _test_labels = reformat(test_samples, test_labels)\n\n_train_samples = normalize(n_train_samples)\n_test_samples = normalize(n_test_samples)\n\nnum_labels = 10\nimage_size = 32\nnum_channels = 1\n\nif __name__ == '__main__':\n\t# \xe6\x8e\xa2\xe7\xb4\xa2\xe6\x95\xb0\xe6\x8d\xae\n\tpass\n\t# inspect(_train_samples, _train_labels, 1234)\n\t# _train_samples = normalize(_train_samples)\n\t# inspect(_train_samples, _train_labels, 1234)\n\t# distribution(train_labels, 'Train Labels')\n\t# distribution(test_labels, 'Test Labels')\n"""
Season1_Tensorflow1.1_Python3.5/1-3/run.py,21,"b'# encoding: utf-8\n# \xe4\xb8\xba\xe4\xba\x86 Python3 \xe7\x9a\x84\xe5\x85\xbc\xe5\xae\xb9\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe4\xbd\xa0\xe7\x94\xa8\xe7\x9a\x84 Python2.7\nfrom __future__ import print_function, division\nimport tensorflow as tf\n\nprint(\'Loaded TF version\', tf.__version__, \'\\n\\n\')\n\n# Tensor \xe5\x9c\xa8\xe6\x95\xb0\xe5\xad\xa6\xe4\xb8\xad\xe6\x98\xaf\xe2\x80\x9c\xe5\xbc\xa0\xe9\x87\x8f\xe2\x80\x9d\n# \xe6\xa0\x87\xe9\x87\x8f\xef\xbc\x8c\xe7\x9f\xa2\xe9\x87\x8f/\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x8c\xe5\xbc\xa0\xe9\x87\x8f\n\n# \xe7\xae\x80\xe5\x8d\x95\xe5\x9c\xb0\xe7\x90\x86\xe8\xa7\xa3\n# \xe6\xa0\x87\xe9\x87\x8f\xe8\xa1\xa8\xe7\xa4\xba\xe5\x80\xbc\n# \xe7\x9f\xa2\xe9\x87\x8f\xe8\xa1\xa8\xe7\xa4\xba\xe4\xbd\x8d\xe7\xbd\xae\xef\xbc\x88\xe7\xa9\xba\xe9\x97\xb4\xe4\xb8\xad\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe7\x82\xb9\xef\xbc\x89\n# \xe5\xbc\xa0\xe9\x87\x8f\xe8\xa1\xa8\xe7\xa4\xba\xe6\x95\xb4\xe4\xb8\xaa\xe7\xa9\xba\xe9\x97\xb4\n\n# \xe4\xb8\x80\xe7\xbb\xb4\xe6\x95\xb0\xe7\xbb\x84\xe6\x98\xaf\xe7\x9f\xa2\xe9\x87\x8f\n# \xe5\xa4\x9a\xe7\xbb\xb4\xe6\x95\xb0\xe7\xbb\x84\xe6\x98\xaf\xe5\xbc\xa0\xe9\x87\x8f, \xe7\x9f\xa9\xe9\x98\xb5\xe4\xb9\x9f\xe6\x98\xaf\xe5\xbc\xa0\xe9\x87\x8f\n\n\n# 4\xe4\xb8\xaa\xe9\x87\x8d\xe8\xa6\x81\xe7\x9a\x84\xe7\xb1\xbb\xe5\x9e\x8b\n# @Variable\t\t\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe8\xb0\xb1\xe4\xb8\xad\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8f\n# @Tensor\t\t\xe4\xb8\x80\xe4\xb8\xaa\xe5\xa4\x9a\xe7\xbb\xb4\xe7\x9f\xa9\xe9\x98\xb5\xef\xbc\x8c\xe5\xb8\xa6\xe6\x9c\x89\xe5\xbe\x88\xe5\xa4\x9a\xe6\x96\xb9\xe6\xb3\x95\n# @Graph\t\t\xe4\xb8\x80\xe4\xb8\xaa\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe8\xb0\xb1\n# @Session\t\t\xe7\x94\xa8\xe6\x9d\xa5\xe8\xbf\x90\xe8\xa1\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe8\xb0\xb1\n\n\n# \xe4\xb8\x89\xe4\xb8\xaa\xe9\x87\x8d\xe8\xa6\x81\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\n\n# Variable \xe5\x8f\x98\xe9\x87\x8f\n""""""\ntf.Variable.__init__(\n    initial_value=None, @Tensor\n    trainable=True,\n    collections=None,\n    validate_shape=True,\n    caching_device=None,\n    name=None,\n    variable_def=None,\n    dtype=None)\n""""""\n\n\n# \xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x9aVariable\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaaClass\xef\xbc\x8cTensor\xe4\xb9\x9f\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaaClass\n\n# Constant \xe5\xb8\xb8\xe6\x95\xb0\n# tf.constant(value, dtype=None, shape=None, name=\'Const\')\n# return: a constant @Tensor\n\n# Placeholder \xe6\x9a\x82\xe6\x97\xb6\xe5\x8f\x98\xe9\x87\x8f\xef\xbc\x9f\n# tf.placeholder(dtype, shape=None, name=None)\n# return: \xe4\xb8\x80\xe4\xb8\xaa\xe8\xbf\x98\xe5\xb0\x9a\xe6\x9c\xaa\xe5\xad\x98\xe5\x9c\xa8\xe7\x9a\x84 @Tensor\n\n# \xe8\xae\xa9\xe6\x88\x91\xe4\xbb\xac\xe7\x94\xa8\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe8\xb0\xb1\xe6\x9d\xa5\xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\x80\xe4\xba\x9b\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\n# + - * / \xe5\x9b\x9b\xe5\x88\x99\xe8\xbf\x90\xe7\xae\x97\ndef basic_operation():\n    v1 = tf.Variable(10)\n    v2 = tf.Variable(5)\n    addv = v1 + v2\n    print(addv)\n    print(type(addv))\n    print(type(v1))\n\n    c1 = tf.constant(10)\n    c2 = tf.constant(5)\n    addc = c1 + c2\n    print(addc)\n    print(type(addc))\n    print(type(c1))\n\n    # \xe7\x94\xa8\xe6\x9d\xa5\xe8\xbf\x90\xe8\xa1\x8c\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe8\xb0\xb1\xe7\x9a\x84\xe5\xaf\xb9\xe8\xb1\xa1/\xe5\xae\x9e\xe4\xbe\x8b\xef\xbc\x9f\n    # session is a runtime\n    sess = tf.Session()\n\n    # Variable -> \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96 -> \xe6\x9c\x89\xe5\x80\xbc\xe7\x9a\x84Tensor\n    tf.initialize_all_variables().run(session=sess)\n\n    print(\'\xe5\x8f\x98\xe9\x87\x8f\xe6\x98\xaf\xe9\x9c\x80\xe8\xa6\x81\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\x9a\x84\')\n    print(\'\xe5\x8a\xa0\xe6\xb3\x95(v1, v2) = \', addv.eval(session=sess))\n    print(\'\xe5\x8a\xa0\xe6\xb3\x95(v1, v2) = \', sess.run(addv))\n    print(\'\xe5\x8a\xa0\xe6\xb3\x95(c1, c2) = \', addc.eval(session=sess))\n    print(\'\\n\\n\')\n    # \xe8\xbf\x99\xe7\xa7\x8d\xe5\xae\x9a\xe4\xb9\x89\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe5\x86\x8d\xe6\x89\xa7\xe8\xa1\x8c\xe6\x93\x8d\xe4\xbd\x9c\xe7\x9a\x84\xe6\xa8\xa1\xe5\xbc\x8f\xe8\xa2\xab\xe7\xa7\xb0\xe4\xb9\x8b\xe4\xb8\xba\xe2\x80\x9c\xe7\xac\xa6\xe5\x8f\xb7\xe5\xbc\x8f\xe7\xbc\x96\xe7\xa8\x8b\xe2\x80\x9d Symbolic Programming\n\n    # tf.Graph.__init__()\n    # Creates a new, empty Graph.\n    graph = tf.Graph()\n    with graph.as_default():\n        value1 = tf.constant([1, 2])\n        value2 = tf.Variable([3, 4])\n        mul = value1 / value2\n\n    with tf.Session(graph=graph) as mySess:\n        tf.initialize_all_variables().run()\n        print(\'\xe4\xb8\x80\xe4\xb8\x80\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe9\x99\xa4\xe6\xb3\x95(value1, value2) = \', mySess.run(mul))\n        print(\'\xe4\xb8\x80\xe4\xb8\x80\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe9\x99\xa4\xe6\xb3\x95(value1, value2) = \', mul.eval())\n\n    # tensor.eval(session=sess)\n    # sess.run(tensor)\n\n    # \xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98\xef\xbc\x9fplaceholder\xe6\x89\x8d\xe6\x98\xaf\xe7\x8e\x8b\xe9\x81\x93\n    # def use_placeholder():\n    graph = tf.Graph()\n    with graph.as_default():\n        value1 = tf.placeholder(dtype=tf.float64)\n        value2 = tf.Variable([3, 4], dtype=tf.float64)\n        mul = value1 * value2\n\n    with tf.Session(graph=graph) as mySess:\n        tf.initialize_all_variables().run()\n        # \xe6\x88\x91\xe4\xbb\xac\xe6\x83\xb3\xe8\xb1\xa1\xe4\xb8\x80\xe4\xb8\x8b\xe8\xbf\x99\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe6\x98\xaf\xe4\xbb\x8e\xe8\xbf\x9c\xe7\xa8\x8b\xe5\x8a\xa0\xe8\xbd\xbd\xe8\xbf\x9b\xe6\x9d\xa5\xe7\x9a\x84\n        # \xe6\x96\x87\xe4\xbb\xb6\xef\xbc\x8c\xe7\xbd\x91\xe7\xbb\x9c\n        # \xe5\x81\x87\xe8\xa3\x85\xe6\x98\xaf 10 GB\n        value = load_from_remote()\n        for partialValue in load_partial(value, 2):\n            runResult = mySess.run(mul, feed_dict={value1: partialValue})\n            # evalResult = mul.eval(feed_dict={value1: partialValue})\n            print(\'\xe4\xb9\x98\xe6\xb3\x95(value1, value2) = \', runResult)\n            # cross validation\n\n\ndef load_from_remote():\n    return [-x for x in range(1000)]\n\n\n# \xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84 Iterator\n# yield\xef\xbc\x8c generator function\ndef load_partial(value, step):\n    index = 0\n    while index < len(value):\n        yield value[index:index + step]\n        index += step\n    return\n\n\nif __name__ == \'__main__\':\n    basic_operation()\n# use_placeholder()\n'"
Season1_Tensorflow1.1_Python3.5/10-11/dp.py,39,"b'# \xe4\xb8\xba\xe4\xba\x86 Python2 \xe7\x8e\xa9\xe5\xae\xb6\xe4\xbb\xac\nfrom __future__ import print_function, division\n\n# \xe7\xac\xac\xe4\xb8\x89\xe6\x96\xb9\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\n# \xe6\x88\x91\xe4\xbb\xac\xe8\x87\xaa\xe5\xb7\xb1\nimport load\n\ntrain_samples, train_labels = load._train_samples, load._train_labels\ntest_samples, test_labels = load._test_samples, load._test_labels\n\nprint(\'Training set\', train_samples.shape, train_labels.shape)\nprint(\'    Test set\', test_samples.shape, test_labels.shape)\n\nimage_size = load.image_size\nnum_labels = load.num_labels\nnum_channels = load.num_channels\n\n\ndef get_chunk(samples, labels, chunkSize):\n    """"""\n    Iterator/Generator: get a batch of data\n    \xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8/\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\x80\xe6\xac\xa1\xe5\x8f\xaa\xe5\xbe\x97\xe5\x88\xb0 chunkSize \xe8\xbf\x99\xe4\xb9\x88\xe5\xa4\x9a\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n    \xe7\x94\xa8\xe4\xba\x8e for loop\xef\xbc\x8c just like range() function\n    """"""\n    if len(samples) != len(labels):\n        raise Exception(\'Length of samples and labels must equal\')\n    stepStart = 0  # initial step\n    i = 0\n    while stepStart < len(samples):\n        stepEnd = stepStart + chunkSize\n        if stepEnd < len(samples):\n            yield i, samples[stepStart:stepEnd], labels[stepStart:stepEnd]\n            i += 1\n        stepStart = stepEnd\n\n\nclass Network():\n    def __init__(self, num_hidden, batch_size):\n        """"""\n        @num_hidden: \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\x9a\x84\xe8\x8a\x82\xe7\x82\xb9\xe6\x95\xb0\xe9\x87\x8f\n        @batch_size\xef\xbc\x9a\xe5\x9b\xa0\xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe8\xa6\x81\xe8\x8a\x82\xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x88\x86\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\xe6\xaf\x8f\xe4\xb8\x80\xe6\x89\xb9\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x87\x8f\xe3\x80\x82\n        """"""\n        self.batch_size = batch_size\n        self.test_batch_size = 500\n\n        # Hyper Parameters\n        self.num_hidden = num_hidden\n\n        # Graph Related\n        self.graph = tf.Graph()\n        self.tf_train_samples = None\n        self.tf_train_labels = None\n        self.tf_test_samples = None\n        self.tf_test_labels = None\n        self.tf_test_prediction = None\n\n        # \xe7\xbb\x9f\xe8\xae\xa1\n        self.merged = None\n\n        # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n        self.define_graph()\n        self.session = tf.Session(graph=self.graph)\n        self.writer = tf.summary.FileWriter(\'./board\', self.graph)\n\n    def define_graph(self):\n        """"""\n        \xe5\xae\x9a\xe4\xb9\x89\xe6\x88\x91\xe7\x9a\x84\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe8\xb0\xb1\n        """"""\n        with self.graph.as_default():\n            # \xe8\xbf\x99\xe9\x87\x8c\xe5\x8f\xaa\xe6\x98\xaf\xe5\xae\x9a\xe4\xb9\x89\xe5\x9b\xbe\xe8\xb0\xb1\xe4\xb8\xad\xe7\x9a\x84\xe5\x90\x84\xe7\xa7\x8d\xe5\x8f\x98\xe9\x87\x8f\n            with tf.name_scope(\'inputs\'):\n                self.tf_train_samples = tf.placeholder(\n                    tf.float32, shape=(self.batch_size, image_size, image_size, num_channels), name=\'tf_train_samples\'\n                )\n                self.tf_train_labels = tf.placeholder(\n                    tf.float32, shape=(self.batch_size, num_labels), name=\'tf_train_labels\'\n                )\n                self.tf_test_samples = tf.placeholder(\n                    tf.float32, shape=(self.test_batch_size, image_size, image_size, num_channels),\n                    name=\'tf_test_samples\'\n                )\n\n            # fully connected layer 1, fully connected\n            with tf.name_scope(\'fc1\'):\n                fc1_weights = tf.Variable(\n                    tf.truncated_normal([image_size * image_size, self.num_hidden], stddev=0.1), name=\'fc1_weights\'\n                )\n                fc1_biases = tf.Variable(tf.constant(0.1, shape=[self.num_hidden]), name=\'fc1_biases\')\n                tf.summary.histogram(\'fc1_weights\', fc1_weights)\n                tf.summary.histogram(\'fc1_biases\', fc1_biases)\n\n            # fully connected layer 2 --> output layer\n            with tf.name_scope(\'fc2\'):\n                fc2_weights = tf.Variable(\n                    tf.truncated_normal([self.num_hidden, num_labels], stddev=0.1), name=\'fc2_weights\'\n                )\n                fc2_biases = tf.Variable(tf.constant(0.1, shape=[num_labels]), name=\'fc2_biases\')\n                tf.summary.histogram(\'fc2_weights\', fc2_weights)\n                tf.summary.histogram(\'fc2_biases\', fc2_biases)\n\n            # \xe6\x83\xb3\xe5\x9c\xa8\xe6\x9d\xa5\xe5\xae\x9a\xe4\xb9\x89\xe5\x9b\xbe\xe8\xb0\xb1\xe7\x9a\x84\xe8\xbf\x90\xe7\xae\x97\n            def model(data):\n                # fully connected layer 1\n                shape = data.get_shape().as_list()\n                reshape = tf.reshape(data, [shape[0], shape[1] * shape[2] * shape[3]])\n\n                with tf.name_scope(\'fc1_model\'):\n                    fc1_model = tf.matmul(reshape, fc1_weights) + fc1_biases\n                    hidden = tf.nn.relu(fc1_model)\n\n                # fully connected layer 2\n                with tf.name_scope(\'fc2_model\'):\n                    return tf.matmul(hidden, fc2_weights) + fc2_biases\n\n            # Training computation.\n            logits = model(self.tf_train_samples)\n            with tf.name_scope(\'loss\'):\n                self.loss = tf.reduce_mean(\n                    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.tf_train_labels)\n                )\n                tf.summary.scalar(\'Loss\', self.loss)\n\n            # Optimizer.\n            with tf.name_scope(\'optimizer\'):\n                self.optimizer = tf.train.GradientDescentOptimizer(0.0001).minimize(self.loss)\n\n            # Predictions for the training, validation, and test data.\n            with tf.name_scope(\'predictions\'):\n                self.train_prediction = tf.nn.softmax(logits, name=\'train_prediction\')\n                self.test_prediction = tf.nn.softmax(model(self.tf_test_samples), name=\'test_prediction\')\n\n            self.merged = tf.summary.merge_all()\n\n    def run(self):\n        """"""\n        \xe7\x94\xa8\xe5\x88\xb0Session\n        """"""\n\n        # private function\n        def print_confusion_matrix(confusionMatrix):\n            print(\'Confusion    Matrix:\')\n            for i, line in enumerate(confusionMatrix):\n                print(line, line[i] / np.sum(line))\n            a = 0\n            for i, column in enumerate(np.transpose(confusionMatrix, (1, 0))):\n                a += (column[i] / np.sum(column)) * (np.sum(column) / 26000)\n                print(column[i] / np.sum(column), )\n            print(\'\\n\', np.sum(confusionMatrix), a)\n\n        with self.session as session:\n            tf.initialize_all_variables().run()\n\n            # \xe8\xae\xad\xe7\xbb\x83\n            print(\'Start Training\')\n            # batch 1000\n            for i, samples, labels in get_chunk(train_samples, train_labels, chunkSize=self.batch_size):\n                _, l, predictions, summary = session.run(\n                    [self.optimizer, self.loss, self.train_prediction, self.merged],\n                    feed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels}\n                )\n                self.writer.add_summary(summary, i)\n                # labels is True Labels\n                accuracy, _ = self.accuracy(predictions, labels)\n                if i % 50 == 0:\n                    print(\'Minibatch loss at step %d: %f\' % (i, l))\n                    print(\'Minibatch accuracy: %.1f%%\' % accuracy)\n            #\n\n            # \xe6\xb5\x8b\xe8\xaf\x95\n            accuracies = []\n            confusionMatrices = []\n            for i, samples, labels in get_chunk(test_samples, test_labels, chunkSize=self.test_batch_size):\n                result = self.test_prediction.eval(feed_dict={self.tf_test_samples: samples})\n                accuracy, cm = self.accuracy(result, labels, need_confusion_matrix=True)\n                accuracies.append(accuracy)\n                confusionMatrices.append(cm)\n                print(\'Test Accuracy: %.1f%%\' % accuracy)\n            print(\' Average  Accuracy:\', np.average(accuracies))\n            print(\'Standard Deviation:\', np.std(accuracies))\n            print_confusion_matrix(np.add.reduce(confusionMatrices))\n        #\n\n    def accuracy(self, predictions, labels, need_confusion_matrix=False):\n        """"""\n        \xe8\xae\xa1\xe7\xae\x97\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87\xe4\xb8\x8e\xe5\x8f\xac\xe5\x9b\x9e\xe7\x8e\x87\n        @return: accuracy and confusionMatrix as a tuple\n        """"""\n        _predictions = np.argmax(predictions, 1)\n        _labels = np.argmax(labels, 1)\n        cm = confusion_matrix(_labels, _predictions) if need_confusion_matrix else None\n        # == is overloaded for numpy array\n        accuracy = (100.0 * np.sum(_predictions == _labels) / predictions.shape[0])\n        return accuracy, cm\n\n\nif __name__ == \'__main__\':\n    net = Network(num_hidden=128, batch_size=100)\n    net.run()\n'"
Season1_Tensorflow1.1_Python3.5/10-11/load.py,0,"b'# encoding:utf-8\n# Python2 \xe5\x85\xbc\xe5\xae\xb9\nfrom __future__ import print_function, division\nfrom scipy.io import loadmat as load\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef reformat(samples, labels):\n    # \xe6\x94\xb9\xe5\x8f\x98\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\n    #  0       1       2      3          3       0       1      2\n    # (\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0) -> (\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0)\n    new = np.transpose(samples, (3, 0, 1, 2)).astype(np.float32)\n\n    # labels \xe5\x8f\x98\xe6\x88\x90 one-hot encoding, [2] -> [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n    # digit 0 , represented as 10\n    # labels \xe5\x8f\x98\xe6\x88\x90 one-hot encoding, [10] -> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n    labels = np.array([x[0] for x in labels])  # slow code, whatever\n    one_hot_labels = []\n    for num in labels:\n        one_hot = [0.0] * 10\n        if num == 10:\n            one_hot[0] = 1.0\n        else:\n            one_hot[num] = 1.0\n        one_hot_labels.append(one_hot)\n    labels = np.array(one_hot_labels).astype(np.float32)\n    return new, labels\n\n\ndef normalize(samples):\n    """"""\n    \xe5\xb9\xb6\xe4\xb8\x94\xe7\x81\xb0\xe5\xba\xa6\xe5\x8c\x96: \xe4\xbb\x8e\xe4\xb8\x89\xe8\x89\xb2\xe9\x80\x9a\xe9\x81\x93 -> \xe5\x8d\x95\xe8\x89\xb2\xe9\x80\x9a\xe9\x81\x93     \xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98 + \xe5\x8a\xa0\xe5\xbf\xab\xe8\xae\xad\xe7\xbb\x83\xe9\x80\x9f\xe5\xba\xa6\n    (R + G + B) / 3\n    \xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe4\xbb\x8e 0 ~ 255 \xe7\xba\xbf\xe6\x80\xa7\xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0 -1.0 ~ +1.0\n    @samples: numpy array\n    """"""\n    a = np.add.reduce(samples, keepdims=True, axis=3)  # shape (\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0)\n    a = a / 3.0\n    return a / 128.0 - 1.0\n\n\ndef distribution(labels, name):\n    # \xe6\x9f\xa5\xe7\x9c\x8b\xe4\xb8\x80\xe4\xb8\x8b\xe6\xaf\x8f\xe4\xb8\xaalabel\xe7\x9a\x84\xe5\x88\x86\xe5\xb8\x83\xef\xbc\x8c\xe5\x86\x8d\xe7\x94\xbb\xe4\xb8\xaa\xe7\xbb\x9f\xe8\xae\xa1\xe5\x9b\xbe\n    # keys:\n    # 0\n    # 1\n    # 2\n    # ...\n    # 9\n    count = {}\n    for label in labels:\n        key = 0 if label[0] == 10 else label[0]\n        if key in count:\n            count[key] += 1\n        else:\n            count[key] = 1\n    x = []\n    y = []\n    for k, v in count.items():\n        # print(k, v)\n        x.append(k)\n        y.append(v)\n\n    y_pos = np.arange(len(x))\n    plt.bar(y_pos, y, align=\'center\', alpha=0.5)\n    plt.xticks(y_pos, x)\n    plt.ylabel(\'Count\')\n    plt.title(name + \' Label Distribution\')\n    plt.show()\n\n\ndef inspect(dataset, labels, i):\n    # \xe6\x98\xbe\xe7\xa4\xba\xe5\x9b\xbe\xe7\x89\x87\xe7\x9c\x8b\xe7\x9c\x8b\n    if dataset.shape[3] == 1:\n        shape = dataset.shape\n        dataset = dataset.reshape(shape[0], shape[1], shape[2])\n    print(labels[i])\n    plt.imshow(dataset[i])\n    plt.show()\n\n\ntrain = load(\'../data/train_32x32.mat\')\ntest = load(\'../data/test_32x32.mat\')\n# extra = load(\'../data/extra_32x32.mat\')\n\n# print(\'Train Samples Shape:\', train[\'X\'].shape)\n# print(\'Train  Labels Shape:\', train[\'y\'].shape)\n\n# print(\'Train Samples Shape:\', test[\'X\'].shape)\n# print(\'Train  Labels Shape:\', test[\'y\'].shape)\n\n# print(\'Train Samples Shape:\', extra[\'X\'].shape)\n# print(\'Train  Labels Shape:\', extra[\'y\'].shape)\n\ntrain_samples = train[\'X\']\ntrain_labels = train[\'y\']\ntest_samples = test[\'X\']\ntest_labels = test[\'y\']\n# extra_samples = extra[\'X\']\n# extra_labels = extra[\'y\']\n\nn_train_samples, _train_labels = reformat(train_samples, train_labels)\nn_test_samples, _test_labels = reformat(test_samples, test_labels)\n\n_train_samples = normalize(n_train_samples)\n_test_samples = normalize(n_test_samples)\n\nnum_labels = 10\nimage_size = 32\nnum_channels = 1\n\nif __name__ == \'__main__\':\n    # \xe6\x8e\xa2\xe7\xb4\xa2\xe6\x95\xb0\xe6\x8d\xae\n    pass\n# inspect(_train_samples, _train_labels, 1234)\n# _train_samples = normalize(_train_samples)\n# inspect(_train_samples, _train_labels, 1234)\n# distribution(train_labels, \'Train Labels\')\n# distribution(test_labels, \'Test Labels\')\n'"
Season1_Tensorflow1.1_Python3.5/12-15/dp.py,80,"b'# \xe4\xb8\xba\xe4\xba\x86 Python2 \xe7\x8e\xa9\xe5\xae\xb6\xe4\xbb\xac\nfrom __future__ import print_function, division\n\n# \xe7\xac\xac\xe4\xb8\x89\xe6\x96\xb9\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\n# \xe6\x88\x91\xe4\xbb\xac\xe8\x87\xaa\xe5\xb7\xb1\nimport load\n\ntrain_samples, train_labels = load._train_samples, load._train_labels\ntest_samples, test_labels = load._test_samples, load._test_labels\n\nprint(\'Training set\', train_samples.shape, train_labels.shape)\nprint(\'    Test set\', test_samples.shape, test_labels.shape)\n\nimage_size = load.image_size\nnum_labels = load.num_labels\nnum_channels = load.num_channels\n\n\ndef get_chunk(samples, labels, chunkSize):\n    """"""\n    Iterator/Generator: get a batch of data\n    \xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8/\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\x80\xe6\xac\xa1\xe5\x8f\xaa\xe5\xbe\x97\xe5\x88\xb0 chunkSize \xe8\xbf\x99\xe4\xb9\x88\xe5\xa4\x9a\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n    \xe7\x94\xa8\xe4\xba\x8e for loop\xef\xbc\x8c just like range() function\n    """"""\n    if len(samples) != len(labels):\n        raise Exception(\'Length of samples and labels must equal\')\n    stepStart = 0  # initial step\n    i = 0\n    while stepStart < len(samples):\n        stepEnd = stepStart + chunkSize\n        if stepEnd < len(samples):\n            yield i, samples[stepStart:stepEnd], labels[stepStart:stepEnd]\n            i += 1\n        stepStart = stepEnd\n\n\nclass Network():\n    def __init__(self, num_hidden, batch_size, conv_depth, patch_size, pooling_scale):\n        """"""\n        @num_hidden: \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\x9a\x84\xe8\x8a\x82\xe7\x82\xb9\xe6\x95\xb0\xe9\x87\x8f\n        @batch_size\xef\xbc\x9a\xe5\x9b\xa0\xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe8\xa6\x81\xe8\x8a\x82\xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x88\x86\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\xe6\xaf\x8f\xe4\xb8\x80\xe6\x89\xb9\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x87\x8f\xe3\x80\x82\n        """"""\n        self.batch_size = batch_size\n        self.test_batch_size = 500\n\n        # Hyper Parameters\n        self.num_hidden = num_hidden\n        self.patch_size = patch_size  # \xe6\xbb\x91\xe7\xaa\x97\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\n        self.conv1_depth = conv_depth\n        self.conv2_depth = conv_depth\n        self.conv3_depth = conv_depth\n        self.conv4_depth = conv_depth\n        self.last_conv_depth = self.conv4_depth\n        self.pooling_scale = pooling_scale\n        self.pooling_stride = self.pooling_scale  # Max Pooling Stride\n\n        # Graph Related\n        self.graph = tf.Graph()\n        self.tf_train_samples = None\n        self.tf_train_labels = None\n        self.tf_test_samples = None\n        self.tf_test_labels = None\n        self.tf_test_prediction = None\n\n        # \xe7\xbb\x9f\xe8\xae\xa1\n        self.merged = None\n        self.train_summaries = []\n        self.test_summaries = []\n\n        # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n        self.define_graph()\n        self.session = tf.Session(graph=self.graph)\n        self.writer = tf.summary.FileWriter(\'./board\', self.graph)\n\n    def define_graph(self):\n        """"""\n        \xe5\xae\x9a\xe4\xb9\x89\xe6\x88\x91\xe7\x9a\x84\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe8\xb0\xb1\n        """"""\n        with self.graph.as_default():\n            # \xe8\xbf\x99\xe9\x87\x8c\xe5\x8f\xaa\xe6\x98\xaf\xe5\xae\x9a\xe4\xb9\x89\xe5\x9b\xbe\xe8\xb0\xb1\xe4\xb8\xad\xe7\x9a\x84\xe5\x90\x84\xe7\xa7\x8d\xe5\x8f\x98\xe9\x87\x8f\n            with tf.name_scope(\'inputs\'):\n                self.tf_train_samples = tf.placeholder(\n                    tf.float32, shape=(self.batch_size, image_size, image_size, num_channels), name=\'tf_train_samples\'\n                )\n                self.tf_train_labels = tf.placeholder(\n                    tf.float32, shape=(self.batch_size, num_labels), name=\'tf_train_labels\'\n                )\n                self.tf_test_samples = tf.placeholder(\n                    tf.float32, shape=(self.test_batch_size, image_size, image_size, num_channels), name=\'tf_test_samples\'\n                )\n\n            with tf.name_scope(\'conv1\'):\n                conv1_weights = tf.Variable(\n                    tf.truncated_normal([self.patch_size, self.patch_size, num_channels, self.conv1_depth], stddev=0.1))\n                conv1_biases = tf.Variable(tf.zeros([self.conv1_depth]))\n\n            with tf.name_scope(\'conv2\'):\n                conv2_weights = tf.Variable(\n                    tf.truncated_normal([self.patch_size, self.patch_size, self.conv1_depth, self.conv2_depth],\n                                        stddev=0.1))\n                conv2_biases = tf.Variable(tf.constant(0.1, shape=[self.conv2_depth]))\n\n            with tf.name_scope(\'conv3\'):\n                conv3_weights = tf.Variable(\n                    tf.truncated_normal([self.patch_size, self.patch_size, self.conv2_depth, self.conv3_depth],\n                                        stddev=0.1))\n                conv3_biases = tf.Variable(tf.constant(0.1, shape=[self.conv3_depth]))\n\n            with tf.name_scope(\'conv4\'):\n                conv4_weights = tf.Variable(\n                    tf.truncated_normal([self.patch_size, self.patch_size, self.conv3_depth, self.conv4_depth],\n                                        stddev=0.1))\n                conv4_biases = tf.Variable(tf.constant(0.1, shape=[self.conv4_depth]))\n\n            # fully connected layer 1, fully connected\n            with tf.name_scope(\'fc1\'):\n                down_scale = self.pooling_scale ** 2  # because we do 2 times pooling of stride 2\n                fc1_weights = tf.Variable(\n                    tf.truncated_normal(\n                        [(image_size // down_scale) * (image_size // down_scale) * self.last_conv_depth,\n                         self.num_hidden], stddev=0.1))\n                fc1_biases = tf.Variable(tf.constant(0.1, shape=[self.num_hidden]))\n\n                self.train_summaries.append(tf.summary.histogram(\'fc1_weights\', fc1_weights))\n                self.train_summaries.append(tf.summary.histogram(\'fc1_biases\', fc1_biases))\n\n            # fully connected layer 2 --> output layer\n            with tf.name_scope(\'fc2\'):\n                fc2_weights = tf.Variable(tf.truncated_normal([self.num_hidden, num_labels], stddev=0.1),\n                                          name=\'fc2_weights\')\n                fc2_biases = tf.Variable(tf.constant(0.1, shape=[num_labels]), name=\'fc2_biases\')\n                self.train_summaries.append(tf.summary.histogram(\'fc2_weights\', fc2_weights))\n                self.train_summaries.append(tf.summary.histogram(\'fc2_biases\', fc2_biases))\n\n            # \xe6\x83\xb3\xe5\x9c\xa8\xe6\x9d\xa5\xe5\xae\x9a\xe4\xb9\x89\xe5\x9b\xbe\xe8\xb0\xb1\xe7\x9a\x84\xe8\xbf\x90\xe7\xae\x97\n            def model(data, train=True):\n                """"""\n                @data: original inputs\n                @return: logits\n                """"""\n                with tf.name_scope(\'conv1_model\'):\n                    with tf.name_scope(\'convolution\'):\n                        conv1 = tf.nn.conv2d(data, filter=conv1_weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n                        addition = conv1 + conv1_biases\n                    hidden = tf.nn.relu(addition)\n\n                    if not train:\n                        # transpose the output of an activation to image\n                        # conv1_activation_relu shape: (8, 32, 32, 64)\n                        # 64 filter maps from this convolution, that\'s 64 grayscale images\n                        # image size is 32x32\n                        # 8 is the batch_size, which means 8 times of convolution was performed\n                        # just use the last one (index 7) as record\n\n                        filter_map = hidden[-1]\n                        filter_map = tf.transpose(filter_map, perm=[2, 0, 1])\n                        filter_map = tf.reshape(filter_map, (self.conv1_depth, 32, 32, 1))\n                        self.test_summaries.append(\n                            tf.summary.image(\'conv1_relu\', tensor=filter_map, max_outputs=self.conv1_depth))\n\n                with tf.name_scope(\'conv2_model\'):\n                    with tf.name_scope(\'convolution\'):\n                        conv2 = tf.nn.conv2d(hidden, filter=conv2_weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n                        addition = conv2 + conv2_biases\n                    hidden = tf.nn.relu(addition)\n                    hidden = tf.nn.max_pool(\n                        hidden,\n                        ksize=[1, self.pooling_scale, self.pooling_scale, 1],\n                        strides=[1, self.pooling_stride, self.pooling_stride, 1],\n                        padding=\'SAME\')\n\n                with tf.name_scope(\'conv3_model\'):\n                    with tf.name_scope(\'convolution\'):\n                        conv3 = tf.nn.conv2d(hidden, filter=conv3_weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n                        addition = conv3 + conv3_biases\n                    hidden = tf.nn.relu(addition)\n\n                with tf.name_scope(\'conv4_model\'):\n                    with tf.name_scope(\'convolution\'):\n                        conv4 = tf.nn.conv2d(hidden, filter=conv4_weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n                        addition = conv4 + conv4_biases\n                    hidden = tf.nn.relu(addition)\n                    # if not train:\n                    # \tfilter_map = hidden[-1]\n                    # \tfilter_map = tf.transpose(filter_map, perm=[2, 0, 1])\n                    # \tfilter_map = tf.reshape(filter_map, (self.conv4_depth, 16, 16, 1))\n                    # \ttf.image_summary(\'conv4_relu\', tensor=filter_map, max_images=self.conv4_depth)\n                    hidden = tf.nn.max_pool(\n                        hidden,\n                        ksize=[1, self.pooling_scale, self.pooling_scale, 1],\n                        strides=[1, self.pooling_stride, self.pooling_stride, 1],\n                        padding=\'SAME\')\n\n                # fully connected layer 1\n                shape = hidden.get_shape().as_list()\n                reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n\n                with tf.name_scope(\'fc1_model\'):\n                    fc1_model = tf.matmul(reshape, fc1_weights) + fc1_biases\n                    hidden = tf.nn.relu(fc1_model)\n\n                # fully connected layer 2\n                with tf.name_scope(\'fc2_model\'):\n                    return tf.matmul(hidden, fc2_weights) + fc2_biases\n\n            # Training computation.\n            logits = model(self.tf_train_samples)\n            with tf.name_scope(\'loss\'):\n                self.loss = tf.reduce_mean(\n                    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.tf_train_labels))\n                self.train_summaries.append(tf.summary.scalar(\'Loss\', self.loss))\n\n            # Optimizer.\n            with tf.name_scope(\'optimizer\'):\n                self.optimizer = tf.train.GradientDescentOptimizer(0.0001).minimize(self.loss)\n\n            # Predictions for the training, validation, and test data.\n            with tf.name_scope(\'train\'):\n                self.train_prediction = tf.nn.softmax(logits, name=\'train_prediction\')\n            with tf.name_scope(\'test\'):\n                self.test_prediction = tf.nn.softmax(model(self.tf_test_samples, train=False), name=\'test_prediction\')\n\n            self.merged_train_summary = tf.summary.merge_all()\n            self.merged_test_summary = tf.summary.merge_all()\n\n    def run(self):\n        """"""\n        \xe7\x94\xa8\xe5\x88\xb0Session\n        """"""\n\n        # private function\n        def print_confusion_matrix(confusionMatrix):\n            print(\'Confusion    Matrix:\')\n            for i, line in enumerate(confusionMatrix):\n                print(line, line[i] / np.sum(line))\n            a = 0\n            for i, column in enumerate(np.transpose(confusionMatrix, (1, 0))):\n                a += (column[i] / np.sum(column)) * (np.sum(column) / 26000)\n                print(column[i] / np.sum(column), )\n            print(\'\\n\', np.sum(confusionMatrix), a)\n\n        with self.session as session:\n            tf.initialize_all_variables().run()\n\n            ### \xe8\xae\xad\xe7\xbb\x83\n            print(\'Start Training\')\n            # batch 1000\n            for i, samples, labels in get_chunk(train_samples, train_labels, chunkSize=self.batch_size):\n                _, l, predictions, summary = session.run(\n                    [self.optimizer, self.loss, self.train_prediction, self.merged_train_summary],\n                    feed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels}\n                )\n                self.writer.add_summary(summary, i)\n                # labels is True Labels\n                accuracy, _ = self.accuracy(predictions, labels)\n                if i % 50 == 0:\n                    print(\'Minibatch loss at step %d: %f\' % (i, l))\n                    print(\'Minibatch accuracy: %.1f%%\' % accuracy)\n            ###\n\n            ### \xe6\xb5\x8b\xe8\xaf\x95\n            accuracies = []\n            confusionMatrices = []\n            for i, samples, labels in get_chunk(test_samples, test_labels, chunkSize=self.test_batch_size):\n                result, summary = session.run(\n                    [self.test_prediction, self.merged_test_summary],\n                    feed_dict={self.tf_test_samples: samples}\n                )\n                # result = self.test_prediction.eval()\n                self.writer.add_summary(summary, i)\n                accuracy, cm = self.accuracy(result, labels, need_confusion_matrix=True)\n                accuracies.append(accuracy)\n                confusionMatrices.append(cm)\n                print(\'Test Accuracy: %.1f%%\' % accuracy)\n            print(\' Average  Accuracy:\', np.average(accuracies))\n            print(\'Standard Deviation:\', np.std(accuracies))\n            print_confusion_matrix(np.add.reduce(confusionMatrices))\n        ###\n\n    def accuracy(self, predictions, labels, need_confusion_matrix=False):\n        """"""\n        \xe8\xae\xa1\xe7\xae\x97\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87\xe4\xb8\x8e\xe5\x8f\xac\xe5\x9b\x9e\xe7\x8e\x87\n        @return: accuracy and confusionMatrix as a tuple\n        """"""\n        _predictions = np.argmax(predictions, 1)\n        _labels = np.argmax(labels, 1)\n        cm = confusion_matrix(_labels, _predictions) if need_confusion_matrix else None\n        # == is overloaded for numpy array\n        accuracy = (100.0 * np.sum(_predictions == _labels) / predictions.shape[0])\n        return accuracy, cm\n\n\nif __name__ == \'__main__\':\n    net = Network(num_hidden=16, batch_size=64, patch_size=3, conv_depth=16, pooling_scale=2)\n    net.run()\n'"
Season1_Tensorflow1.1_Python3.5/12-15/dp_refined_api.py,40,"b'# \xe6\x96\xb0\xe7\x9a\x84 refined api \xe4\xb8\x8d\xe6\x94\xaf\xe6\x8c\x81 Python2\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\n\nclass Network():\n    def __init__(self, train_batch_size, test_batch_size, pooling_scale):\n        """"""\n        @num_hidden: \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\x9a\x84\xe8\x8a\x82\xe7\x82\xb9\xe6\x95\xb0\xe9\x87\x8f\n        @batch_size\xef\xbc\x9a\xe5\x9b\xa0\xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe8\xa6\x81\xe8\x8a\x82\xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x88\x86\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\xe6\xaf\x8f\xe4\xb8\x80\xe6\x89\xb9\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x87\x8f\xe3\x80\x82\n        """"""\n        self.train_batch_size = train_batch_size\n        self.test_batch_size = test_batch_size\n\n        # Hyper Parameters\n        self.conv_config = []  # list of dict\n        self.fc_config = []  # list of dict\n        self.conv_weights = []\n        self.conv_biases = []\n        self.fc_weights = []\n        self.fc_biases = []\n        self.pooling_scale = pooling_scale\n        self.pooling_stride = pooling_scale\n\n        # Graph Related\n        self.tf_train_samples = None\n        self.tf_train_labels = None\n        self.tf_test_samples = None\n        self.tf_test_labels = None\n\n        # \xe7\xbb\x9f\xe8\xae\xa1\n        self.merged = None\n        self.train_summaries = []\n        self.test_summaries = []\n\n    def add_conv(self, *, patch_size, in_depth, out_depth, activation=\'relu\', pooling=False, name):\n        """"""\n        This function does not define operations in the graph, but only store config in self.conv_layer_config\n        """"""\n        self.conv_config.append({\n            \'patch_size\': patch_size,\n            \'in_depth\': in_depth,\n            \'out_depth\': out_depth,\n            \'activation\': activation,\n            \'pooling\': pooling,\n            \'name\': name\n        })\n        with tf.name_scope(name):\n            weights = tf.Variable(\n                tf.truncated_normal([patch_size, patch_size, in_depth, out_depth], stddev=0.1), name=name + \'_weights\')\n            biases = tf.Variable(tf.constant(0.1, shape=[out_depth]), name=name + \'_biases\')\n            self.conv_weights.append(weights)\n            self.conv_biases.append(biases)\n\n    def add_fc(self, *, in_num_nodes, out_num_nodes, activation=\'relu\', name):\n        """"""\n        add fc layer config to slef.fc_layer_config\n        """"""\n        self.fc_config.append({\n            \'in_num_nodes\': in_num_nodes,\n            \'out_num_nodes\': out_num_nodes,\n            \'activation\': activation,\n            \'name\': name\n        })\n        with tf.name_scope(name):\n            weights = tf.Variable(tf.truncated_normal([in_num_nodes, out_num_nodes], stddev=0.1))\n            biases = tf.Variable(tf.constant(0.1, shape=[out_num_nodes]))\n            self.fc_weights.append(weights)\n            self.fc_biases.append(biases)\n            self.train_summaries.append(tf.summary.histogram(str(len(self.fc_weights)) + \'_weights\', weights))\n            self.train_summaries.append(tf.summary.histogram(str(len(self.fc_biases)) + \'_biases\', biases))\n\n    # should make the definition as an exposed API, instead of implemented in the function\n    def define_inputs(self, *, train_samples_shape, train_labels_shape, test_samples_shape):\n        # \xe8\xbf\x99\xe9\x87\x8c\xe5\x8f\xaa\xe6\x98\xaf\xe5\xae\x9a\xe4\xb9\x89\xe5\x9b\xbe\xe8\xb0\xb1\xe4\xb8\xad\xe7\x9a\x84\xe5\x90\x84\xe7\xa7\x8d\xe5\x8f\x98\xe9\x87\x8f\n        with tf.name_scope(\'inputs\'):\n            self.tf_train_samples = tf.placeholder(tf.float32, shape=train_samples_shape, name=\'tf_train_samples\')\n            self.tf_train_labels = tf.placeholder(tf.float32, shape=train_labels_shape, name=\'tf_train_labels\')\n            self.tf_test_samples = tf.placeholder(tf.float32, shape=test_samples_shape, name=\'tf_test_samples\')\n\n    def define_model(self):\n        """"""\n        \xe5\xae\x9a\xe4\xb9\x89\xe6\x88\x91\xe7\x9a\x84\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe8\xb0\xb1\n        """"""\n\n        def model(data_flow, train=True):\n            """"""\n            @data: original inputs\n            @return: logits\n            """"""\n            # Define Convolutional Layers\n            for i, (weights, biases, config) in enumerate(zip(self.conv_weights, self.conv_biases, self.conv_config)):\n                with tf.name_scope(config[\'name\'] + \'_model\'):\n                    with tf.name_scope(\'convolution\'):\n                        # default 1,1,1,1 stride and SAME padding\n                        data_flow = tf.nn.conv2d(data_flow, filter=weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n                        data_flow = data_flow + biases\n                        if not train:\n                            self.visualize_filter_map(data_flow, how_many=config[\'out_depth\'],\n                                                      display_size=32 // (i // 2 + 1), name=config[\'name\'] + \'_conv\')\n                    if config[\'activation\'] == \'relu\':\n                        data_flow = tf.nn.relu(data_flow)\n                        if not train:\n                            self.visualize_filter_map(data_flow, how_many=config[\'out_depth\'],\n                                                      display_size=32 // (i // 2 + 1), name=config[\'name\'] + \'_relu\')\n                    else:\n                        raise Exception(\'Activation Func can only be Relu right now. You passed\', config[\'activation\'])\n                    if config[\'pooling\']:\n                        data_flow = tf.nn.max_pool(\n                            data_flow,\n                            ksize=[1, self.pooling_scale, self.pooling_scale, 1],\n                            strides=[1, self.pooling_stride, self.pooling_stride, 1],\n                            padding=\'SAME\')\n                        if not train:\n                            self.visualize_filter_map(data_flow, how_many=config[\'out_depth\'],\n                                                      display_size=32 // (i // 2 + 1) // 2,\n                                                      name=config[\'name\'] + \'_pooling\')\n\n            # Define Fully Connected Layers\n            for i, (weights, biases, config) in enumerate(zip(self.fc_weights, self.fc_biases, self.fc_config)):\n                if i == 0:\n                    shape = data_flow.get_shape().as_list()\n                    data_flow = tf.reshape(data_flow, [shape[0], shape[1] * shape[2] * shape[3]])\n                with tf.name_scope(config[\'name\'] + \'model\'):\n                    data_flow = tf.matmul(data_flow, weights) + biases\n                    if config[\'activation\'] == \'relu\':\n                        data_flow = tf.nn.relu(data_flow)\n                    elif config[\'activation\'] is None:\n                        pass\n                    else:\n                        raise Exception(\'Activation Func can only be Relu or None right now. You passed\',\n                                        config[\'activation\'])\n            return data_flow\n\n        # Training computation.\n        logits = model(self.tf_train_samples)\n        with tf.name_scope(\'loss\'):\n            self.loss = tf.reduce_mean(\n                tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.tf_train_labels))\n            self.train_summaries.append(tf.summary.scalar(\'Loss\', self.loss))\n\n        # Optimizer.\n        with tf.name_scope(\'optimizer\'):\n            self.optimizer = tf.train.GradientDescentOptimizer(0.0001).minimize(self.loss)\n\n        # Predictions for the training, validation, and test data.\n        with tf.name_scope(\'train\'):\n            self.train_prediction = tf.nn.softmax(logits, name=\'train_prediction\')\n        with tf.name_scope(\'test\'):\n            self.test_prediction = tf.nn.softmax(model(self.tf_test_samples, train=False), name=\'test_prediction\')\n\n        # \xe6\xb3\xa8\xe6\x84\x8f\xe8\xbf\x99\xe9\x87\x8c\xe4\xb8\x8d\xe8\x83\xbd\xe9\x9a\x8f\xe4\xbe\xbf\xe6\x9b\xbf\xe6\x8d\xa2\xe4\xb8\xbamerge_all\n        self.merged_train_summary = tf.summary.merge(self.train_summaries)\n        self.merged_test_summary = tf.summary.merge(self.test_summaries)\n\n    def run(self, data_iterator, train_samples, train_labels, test_samples, test_labels):\n        """"""\n        \xe7\x94\xa8\xe5\x88\xb0Session\n        :data_iterator: a function that yields chuck of data\n        """"""\n\n        # private function\n        def print_confusion_matrix(confusionMatrix):\n            print(\'Confusion    Matrix:\')\n            for i, line in enumerate(confusionMatrix):\n                print(line, line[i] / np.sum(line))\n            a = 0\n            for i, column in enumerate(np.transpose(confusionMatrix, (1, 0))):\n                a += (column[i] / np.sum(column)) * (np.sum(column) / 26000)\n                print(column[i] / np.sum(column), )\n            print(\'\\n\', np.sum(confusionMatrix), a)\n\n        self.writer = tf.summary.FileWriter(\'./board\', tf.get_default_graph())\n\n        with tf.Session(graph=tf.get_default_graph()) as session:\n            tf.initialize_all_variables().run()\n\n            # \xe8\xae\xad\xe7\xbb\x83\n            print(\'Start Training\')\n            # batch 1000\n            for i, samples, labels in data_iterator(train_samples, train_labels, self.train_batch_size):\n                _, l, predictions, summary = session.run(\n                    [self.optimizer, self.loss, self.train_prediction, self.merged_train_summary],\n                    feed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels}\n                )\n                self.writer.add_summary(summary, i)\n                # labels is True Labels\n                accuracy, _ = self.accuracy(predictions, labels)\n                if i % 50 == 0:\n                    print(\'Minibatch loss at step %d: %f\' % (i, l))\n                    print(\'Minibatch accuracy: %.1f%%\' % accuracy)\n            #\n\n            # # \xe6\xb5\x8b\xe8\xaf\x95\n            accuracies = []\n            confusionMatrices = []\n            for i, samples, labels in data_iterator(test_samples, test_labels, self.test_batch_size):\n                print(\'samples shape\', samples.shape)\n                result, summary = session.run(\n                    [self.test_prediction, self.merged_test_summary],\n                    feed_dict={self.tf_test_samples: samples}\n                )\n                # result = self.test_prediction.eval(feed_dict={self.tf_test_samples: samples})\n                self.writer.add_summary(summary, i)\n                accuracy, cm = self.accuracy(result, labels, need_confusion_matrix=True)\n                accuracies.append(accuracy)\n                confusionMatrices.append(cm)\n                print(\'Test Accuracy: %.1f%%\' % accuracy)\n            print(\' Average  Accuracy:\', np.average(accuracies))\n            print(\'Standard Deviation:\', np.std(accuracies))\n            print_confusion_matrix(np.add.reduce(confusionMatrices))\n        #\n\n    def accuracy(self, predictions, labels, need_confusion_matrix=False):\n        """"""\n        \xe8\xae\xa1\xe7\xae\x97\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87\xe4\xb8\x8e\xe5\x8f\xac\xe5\x9b\x9e\xe7\x8e\x87\n        @return: accuracy and confusionMatrix as a tuple\n        """"""\n        _predictions = np.argmax(predictions, 1)\n        _labels = np.argmax(labels, 1)\n        cm = confusion_matrix(_labels, _predictions) if need_confusion_matrix else None\n        # == is overloaded for numpy array\n        accuracy = (100.0 * np.sum(_predictions == _labels) / predictions.shape[0])\n        return accuracy, cm\n\n    def visualize_filter_map(self, tensor, *, how_many, display_size, name):\n        print(tensor.get_shape)\n        filter_map = tensor[-1]\n        print(filter_map.get_shape())\n        filter_map = tf.transpose(filter_map, perm=[2, 0, 1])\n        print(filter_map.get_shape())\n        filter_map = tf.reshape(filter_map, (how_many, display_size, display_size, 1))\n        print(how_many)\n        self.test_summaries.append(tf.summary.image(name, tensor=filter_map, max_outputs=how_many))\n'"
Season1_Tensorflow1.1_Python3.5/12-15/load.py,0,"b'# encoding:utf-8\n# Python2 \xe5\x85\xbc\xe5\xae\xb9\nfrom __future__ import print_function, division\nfrom scipy.io import loadmat as load\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef reformat(samples, labels):\n    # \xe6\x94\xb9\xe5\x8f\x98\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\n    #  0       1       2      3          3       0       1      2\n    # (\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0) -> (\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0)\n    new = np.transpose(samples, (3, 0, 1, 2)).astype(np.float32)\n\n    # labels \xe5\x8f\x98\xe6\x88\x90 one-hot encoding, [2] -> [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n    # digit 0 , represented as 10\n    # labels \xe5\x8f\x98\xe6\x88\x90 one-hot encoding, [10] -> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n    labels = np.array([x[0] for x in labels])  # slow code, whatever\n    one_hot_labels = []\n    for num in labels:\n        one_hot = [0.0] * 10\n        if num == 10:\n            one_hot[0] = 1.0\n        else:\n            one_hot[num] = 1.0\n        one_hot_labels.append(one_hot)\n    labels = np.array(one_hot_labels).astype(np.float32)\n    return new, labels\n\n\ndef normalize(samples):\n    """"""\n    \xe5\xb9\xb6\xe4\xb8\x94\xe7\x81\xb0\xe5\xba\xa6\xe5\x8c\x96: \xe4\xbb\x8e\xe4\xb8\x89\xe8\x89\xb2\xe9\x80\x9a\xe9\x81\x93 -> \xe5\x8d\x95\xe8\x89\xb2\xe9\x80\x9a\xe9\x81\x93     \xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98 + \xe5\x8a\xa0\xe5\xbf\xab\xe8\xae\xad\xe7\xbb\x83\xe9\x80\x9f\xe5\xba\xa6\n    (R + G + B) / 3\n    \xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe4\xbb\x8e 0 ~ 255 \xe7\xba\xbf\xe6\x80\xa7\xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0 -1.0 ~ +1.0\n    @samples: numpy array\n    """"""\n    a = np.add.reduce(samples, keepdims=True, axis=3)  # shape (\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0)\n    a = a / 3.0\n    return a / 128.0 - 1.0\n\n\ndef distribution(labels, name):\n    # \xe6\x9f\xa5\xe7\x9c\x8b\xe4\xb8\x80\xe4\xb8\x8b\xe6\xaf\x8f\xe4\xb8\xaalabel\xe7\x9a\x84\xe5\x88\x86\xe5\xb8\x83\xef\xbc\x8c\xe5\x86\x8d\xe7\x94\xbb\xe4\xb8\xaa\xe7\xbb\x9f\xe8\xae\xa1\xe5\x9b\xbe\n    # keys:\n    # 0\n    # 1\n    # 2\n    # ...\n    # 9\n    count = {}\n    for label in labels:\n        key = 0 if label[0] == 10 else label[0]\n        if key in count:\n            count[key] += 1\n        else:\n            count[key] = 1\n    x = []\n    y = []\n    for k, v in count.items():\n        # print(k, v)\n        x.append(k)\n        y.append(v)\n\n    y_pos = np.arange(len(x))\n    plt.bar(y_pos, y, align=\'center\', alpha=0.5)\n    plt.xticks(y_pos, x)\n    plt.ylabel(\'Count\')\n    plt.title(name + \' Label Distribution\')\n    plt.show()\n\n\ndef inspect(dataset, labels, i):\n    # \xe6\x98\xbe\xe7\xa4\xba\xe5\x9b\xbe\xe7\x89\x87\xe7\x9c\x8b\xe7\x9c\x8b\n    if dataset.shape[3] == 1:\n        shape = dataset.shape\n        dataset = dataset.reshape(shape[0], shape[1], shape[2])\n    print(labels[i])\n    plt.imshow(dataset[i])\n    plt.show()\n\n\ntrain = load(\'../data/train_32x32.mat\')\ntest = load(\'../data/test_32x32.mat\')\n# extra = load(\'../data/extra_32x32.mat\')\n\n# print(\'Train Samples Shape:\', train[\'X\'].shape)\n# print(\'Train  Labels Shape:\', train[\'y\'].shape)\n\n# print(\'Train Samples Shape:\', test[\'X\'].shape)\n# print(\'Train  Labels Shape:\', test[\'y\'].shape)\n\n# print(\'Train Samples Shape:\', extra[\'X\'].shape)\n# print(\'Train  Labels Shape:\', extra[\'y\'].shape)\n\ntrain_samples = train[\'X\']\ntrain_labels = train[\'y\']\ntest_samples = test[\'X\']\ntest_labels = test[\'y\']\n# extra_samples = extra[\'X\']\n# extra_labels = extra[\'y\']\n\nn_train_samples, _train_labels = reformat(train_samples, train_labels)\nn_test_samples, _test_labels = reformat(test_samples, test_labels)\n\n_train_samples = normalize(n_train_samples)\n_test_samples = normalize(n_test_samples)\n\nnum_labels = 10\nimage_size = 32\nnum_channels = 1\n\nif __name__ == \'__main__\':\n    # \xe6\x8e\xa2\xe7\xb4\xa2\xe6\x95\xb0\xe6\x8d\xae\n    pass\n    inspect(_train_samples, _train_labels, 1234)\n# _train_samples = normalize(_train_samples)\n# inspect(_train_samples, _train_labels, 1234)\n# distribution(train_labels, \'Train Labels\')\n# distribution(test_labels, \'Test Labels\')\n'"
Season1_Tensorflow1.1_Python3.5/12-15/main.py,0,"b'if __name__ == \'__main__\':\n    import load\n    from dp_refined_api import Network\n\n    train_samples, train_labels = load._train_samples, load._train_labels\n    test_samples, test_labels = load._test_samples, load._test_labels\n\n    print(\'Training set\', train_samples.shape, train_labels.shape)\n    print(\'    Test set\', test_samples.shape, test_labels.shape)\n\n    image_size = load.image_size\n    num_labels = load.num_labels\n    num_channels = load.num_channels\n\n\n    def get_chunk(samples, labels, chunk_size):\n        """"""\n        Iterator/Generator: get a batch of data\n        \xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8/\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\x80\xe6\xac\xa1\xe5\x8f\xaa\xe5\xbe\x97\xe5\x88\xb0 chunk_size \xe8\xbf\x99\xe4\xb9\x88\xe5\xa4\x9a\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n        \xe7\x94\xa8\xe4\xba\x8e for loop\xef\xbc\x8c just like range() function\n        """"""\n        if len(samples) != len(labels):\n            raise Exception(\'Length of samples and labels must equal\')\n        stepStart = 0  # initial step\n        i = 0\n        while stepStart < len(samples):\n            stepEnd = stepStart + chunk_size\n            if stepEnd < len(samples):\n                yield i, samples[stepStart:stepEnd], labels[stepStart:stepEnd]\n                i += 1\n            stepStart = stepEnd\n\n\n    net = Network(train_batch_size=64, test_batch_size=500, pooling_scale=2)\n    net.define_inputs(\n        train_samples_shape=(64, image_size, image_size, num_channels),\n        train_labels_shape=(64, num_labels),\n        test_samples_shape=(500, image_size, image_size, num_channels)\n    )\n    #\n    net.add_conv(patch_size=3, in_depth=num_channels, out_depth=16, activation=\'relu\', pooling=False, name=\'conv1\')\n    net.add_conv(patch_size=3, in_depth=16, out_depth=16, activation=\'relu\', pooling=True, name=\'conv2\')\n    net.add_conv(patch_size=3, in_depth=16, out_depth=16, activation=\'relu\', pooling=False, name=\'conv3\')\n    net.add_conv(patch_size=3, in_depth=16, out_depth=16, activation=\'relu\', pooling=True, name=\'conv4\')\n\n    # 4 = \xe4\xb8\xa4\xe6\xac\xa1 pooling, \xe6\xaf\x8f\xe4\xb8\x80\xe6\xac\xa1\xe7\xbc\xa9\xe5\xb0\x8f\xe4\xb8\xba 1/2\n    # 16 = conv4 out_depth\n    net.add_fc(in_num_nodes=(image_size // 4) * (image_size // 4) * 16, out_num_nodes=16, activation=\'relu\', name=\'fc1\')\n    net.add_fc(in_num_nodes=16, out_num_nodes=10, activation=None, name=\'fc2\')\n\n    net.define_model()\n    net.run(get_chunk, train_samples, train_labels, test_samples, test_labels)\n\nelse:\n    raise Exception(\'main.py: Should Not Be Imported!!! Must Run by ""python main.py""\')\n'"
Season1_Tensorflow1.1_Python3.5/16-19/dp.py,46,"b'# \xe6\x96\xb0\xe7\x9a\x84 refined api \xe4\xb8\x8d\xe6\x94\xaf\xe6\x8c\x81 Python2\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\n\nclass Network():\n    def __init__(self, train_batch_size, test_batch_size, pooling_scale,\n                 optimize_method=\'adam\'):\n        """"""\n        @num_hidden: \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\x9a\x84\xe8\x8a\x82\xe7\x82\xb9\xe6\x95\xb0\xe9\x87\x8f\n        @batch_size\xef\xbc\x9a\xe5\x9b\xa0\xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe8\xa6\x81\xe8\x8a\x82\xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x88\x86\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\xe6\xaf\x8f\xe4\xb8\x80\xe6\x89\xb9\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x87\x8f\xe3\x80\x82\n        """"""\n        self.optimize_method = optimize_method\n\n        self.train_batch_size = train_batch_size\n        self.test_batch_size = test_batch_size\n\n        # Hyper Parameters\n        self.conv_config = []  # list of dict\n        self.fc_config = []  # list of dict\n        self.conv_weights = []\n        self.conv_biases = []\n        self.fc_weights = []\n        self.fc_biases = []\n        self.pooling_scale = pooling_scale\n        self.pooling_stride = pooling_scale\n\n        # Graph Related\n        self.tf_train_samples = None\n        self.tf_train_labels = None\n        self.tf_test_samples = None\n        self.tf_test_labels = None\n\n        # \xe7\xbb\x9f\xe8\xae\xa1\n        self.merged = None\n        self.train_summaries = []\n        self.test_summaries = []\n\n    def add_conv(self, *, patch_size, in_depth, out_depth, activation=\'relu\', pooling=False, name):\n        """"""\n        This function does not define operations in the graph, but only store config in self.conv_layer_config\n        """"""\n        self.conv_config.append({\n            \'patch_size\': patch_size,\n            \'in_depth\': in_depth,\n            \'out_depth\': out_depth,\n            \'activation\': activation,\n            \'pooling\': pooling,\n            \'name\': name\n        })\n        with tf.name_scope(name):\n            weights = tf.Variable(\n                tf.truncated_normal([patch_size, patch_size, in_depth, out_depth], stddev=0.1), name=name + \'_weights\')\n            biases = tf.Variable(tf.constant(0.1, shape=[out_depth]), name=name + \'_biases\')\n            self.conv_weights.append(weights)\n            self.conv_biases.append(biases)\n\n    def add_fc(self, *, in_num_nodes, out_num_nodes, activation=\'relu\', name):\n        """"""\n        add fc layer config to slef.fc_layer_config\n        """"""\n        self.fc_config.append({\n            \'in_num_nodes\': in_num_nodes,\n            \'out_num_nodes\': out_num_nodes,\n            \'activation\': activation,\n            \'name\': name\n        })\n        with tf.name_scope(name):\n            weights = tf.Variable(tf.truncated_normal([in_num_nodes, out_num_nodes], stddev=0.1))\n            biases = tf.Variable(tf.constant(0.1, shape=[out_num_nodes]))\n            self.fc_weights.append(weights)\n            self.fc_biases.append(biases)\n            self.train_summaries.append(tf.summary.histogram(str(len(self.fc_weights)) + \'_weights\', weights))\n            self.train_summaries.append(tf.summary.histogram(str(len(self.fc_biases)) + \'_biases\', biases))\n\n    def apply_regularization(self, _lambda):\n        # L2 regularization for the fully connected parameters\n        regularization = 0.0\n        for weights, biases in zip(self.fc_weights, self.fc_biases):\n            regularization += tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n        # 1e5\n        return _lambda * regularization\n\n    # should make the definition as an exposed API, instead of implemented in the function\n    def define_inputs(self, *, train_samples_shape, train_labels_shape, test_samples_shape):\n        # \xe8\xbf\x99\xe9\x87\x8c\xe5\x8f\xaa\xe6\x98\xaf\xe5\xae\x9a\xe4\xb9\x89\xe5\x9b\xbe\xe8\xb0\xb1\xe4\xb8\xad\xe7\x9a\x84\xe5\x90\x84\xe7\xa7\x8d\xe5\x8f\x98\xe9\x87\x8f\n        with tf.name_scope(\'inputs\'):\n            self.tf_train_samples = tf.placeholder(tf.float32, shape=train_samples_shape, name=\'tf_train_samples\')\n            self.tf_train_labels = tf.placeholder(tf.float32, shape=train_labels_shape, name=\'tf_train_labels\')\n            self.tf_test_samples = tf.placeholder(tf.float32, shape=test_samples_shape, name=\'tf_test_samples\')\n\n    def define_model(self):\n        """"""\n        \xe5\xae\x9a\xe4\xb9\x89\xe6\x88\x91\xe7\x9a\x84\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe8\xb0\xb1\n        """"""\n\n        def model(data_flow, train=True):\n            """"""\n            @data: original inputs\n            @return: logits\n            """"""\n            # Define Convolutional Layers\n            for i, (weights, biases, config) in enumerate(zip(self.conv_weights, self.conv_biases, self.conv_config)):\n                with tf.name_scope(config[\'name\'] + \'_model\'):\n                    with tf.name_scope(\'convolution\'):\n                        # default 1,1,1,1 stride and SAME padding\n                        data_flow = tf.nn.conv2d(data_flow, filter=weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n                        data_flow = data_flow + biases\n                        if not train:\n                            self.visualize_filter_map(data_flow, how_many=config[\'out_depth\'],\n                                                      display_size=32 // (i // 2 + 1), name=config[\'name\'] + \'_conv\')\n                    if config[\'activation\'] == \'relu\':\n                        data_flow = tf.nn.relu(data_flow)\n                        if not train:\n                            self.visualize_filter_map(data_flow, how_many=config[\'out_depth\'],\n                                                      display_size=32 // (i // 2 + 1), name=config[\'name\'] + \'_relu\')\n                    else:\n                        raise Exception(\'Activation Func can only be Relu right now. You passed\', config[\'activation\'])\n                    if config[\'pooling\']:\n                        data_flow = tf.nn.max_pool(\n                            data_flow,\n                            ksize=[1, self.pooling_scale, self.pooling_scale, 1],\n                            strides=[1, self.pooling_stride, self.pooling_stride, 1],\n                            padding=\'SAME\')\n                        if not train:\n                            self.visualize_filter_map(data_flow, how_many=config[\'out_depth\'],\n                                                      display_size=32 // (i // 2 + 1) // 2,\n                                                      name=config[\'name\'] + \'_pooling\')\n\n            # Define Fully Connected Layers\n            for i, (weights, biases, config) in enumerate(zip(self.fc_weights, self.fc_biases, self.fc_config)):\n                if i == 0:\n                    shape = data_flow.get_shape().as_list()\n                    data_flow = tf.reshape(data_flow, [shape[0], shape[1] * shape[2] * shape[3]])\n                with tf.name_scope(config[\'name\'] + \'model\'):\n\n                    # Dropout\n                    if train and i == len(self.fc_weights) - 1:\n                        data_flow = tf.nn.dropout(data_flow, 0.5, seed=4926)\n                    #\n\n                    data_flow = tf.matmul(data_flow, weights) + biases\n                    if config[\'activation\'] == \'relu\':\n                        data_flow = tf.nn.relu(data_flow)\n                    elif config[\'activation\'] is None:\n                        pass\n                    else:\n                        raise Exception(\'Activation Func can only be Relu or None right now. You passed\',\n                                        config[\'activation\'])\n            return data_flow\n\n        # Training computation.\n        logits = model(self.tf_train_samples)\n        with tf.name_scope(\'loss\'):\n            self.loss = tf.reduce_mean(\n                tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.tf_train_labels))\n            self.loss += self.apply_regularization(_lambda=5e-4)\n            self.train_summaries.append(tf.summary.scalar(\'Loss\', self.loss))\n\n        # learning rate decay\n        global_step = tf.Variable(0)\n        lr = 0.001\n        dr = 0.99\n        learning_rate = tf.train.exponential_decay(\n            learning_rate=lr,\n            global_step=global_step * self.train_batch_size,\n            decay_steps=100,\n            decay_rate=dr,\n            staircase=True\n        )\n\n        # Optimizer.\n        with tf.name_scope(\'optimizer\'):\n            if self.optimize_method == \'gradient\':\n                self.optimizer = tf.train \\\n                    .GradientDescentOptimizer(learning_rate) \\\n                    .minimize(self.loss)\n            elif self.optimize_method == \'momentum\':\n                self.optimizer = tf.train \\\n                    .MomentumOptimizer(learning_rate, 0.5) \\\n                    .minimize(self.loss)\n            elif (self.optimize_method == \'adam\'):\n                self.optimizer = tf.train \\\n                    .AdamOptimizer(learning_rate) \\\n                    .minimize(self.loss)\n\n        # Predictions for the training, validation, and test data.\n        with tf.name_scope(\'train\'):\n            self.train_prediction = tf.nn.softmax(logits, name=\'train_prediction\')\n        with tf.name_scope(\'test\'):\n            self.test_prediction = tf.nn.softmax(model(self.tf_test_samples, train=False), name=\'test_prediction\')\n\n        self.merged_train_summary = tf.summary.merge(self.train_summaries)\n        self.merged_test_summary = tf.summary.merge(self.test_summaries)\n\n    def run(self, data_iterator, train_samples, train_labels, test_samples, test_labels):\n        """"""\n        \xe7\x94\xa8\xe5\x88\xb0Session\n        :data_iterator: a function that yields chuck of data\n        """"""\n\n        # private function\n        def print_confusion_matrix(confusionMatrix):\n            print(\'Confusion    Matrix:\')\n            for i, line in enumerate(confusionMatrix):\n                print(line, line[i] / np.sum(line))\n            a = 0\n            for i, column in enumerate(np.transpose(confusionMatrix, (1, 0))):\n                a += (column[i] / np.sum(column)) * (np.sum(column) / 26000)\n                print(column[i] / np.sum(column), )\n            print(\'\\n\', np.sum(confusionMatrix), a)\n\n        self.writer = tf.summary.FileWriter(\'./board\', tf.get_default_graph())\n\n        with tf.Session(graph=tf.get_default_graph()) as session:\n            tf.initialize_all_variables().run()\n\n            # \xe8\xae\xad\xe7\xbb\x83\n            print(\'Start Training\')\n            # batch 1000\n            for i, samples, labels in data_iterator(train_samples, train_labels, chunk_size=self.train_batch_size):\n                _, l, predictions, summary = session.run(\n                    [self.optimizer, self.loss, self.train_prediction, self.merged_train_summary],\n                    feed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels}\n                )\n                self.writer.add_summary(summary, i)\n                # labels is True Labels\n                accuracy, _ = self.accuracy(predictions, labels)\n                if i % 50 == 0:\n                    print(\'Minibatch loss at step %d: %f\' % (i, l))\n                    print(\'Minibatch accuracy: %.1f%%\' % accuracy)\n            #\n\n            # \xe6\xb5\x8b\xe8\xaf\x95\n            accuracies = []\n            confusionMatrices = []\n            for i, samples, labels in data_iterator(test_samples, test_labels, self.test_batch_size):\n                result, summary = session.run(\n                    [self.test_prediction, self.merged_test_summary],\n                    feed_dict={self.tf_test_samples: samples}\n                )\n                self.writer.add_summary(summary, i)\n                accuracy, cm = self.accuracy(result, labels, need_confusion_matrix=True)\n                accuracies.append(accuracy)\n                confusionMatrices.append(cm)\n                print(\'Test Accuracy: %.1f%%\' % accuracy)\n            print(\' Average  Accuracy:\', np.average(accuracies))\n            print(\'Standard Deviation:\', np.std(accuracies))\n            print_confusion_matrix(np.add.reduce(confusionMatrices))\n            #\n\n    def accuracy(self, predictions, labels, need_confusion_matrix=False):\n        """"""\n        \xe8\xae\xa1\xe7\xae\x97\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87\xe4\xb8\x8e\xe5\x8f\xac\xe5\x9b\x9e\xe7\x8e\x87\n        @return: accuracy and confusionMatrix as a tuple\n        """"""\n        _predictions = np.argmax(predictions, 1)\n        _labels = np.argmax(labels, 1)\n        cm = confusion_matrix(_labels, _predictions) if need_confusion_matrix else None\n        # == is overloaded for numpy array\n        accuracy = (100.0 * np.sum(_predictions == _labels) / predictions.shape[0])\n        return accuracy, cm\n\n    def visualize_filter_map(self, tensor, *, how_many, display_size, name):\n        print(tensor.get_shape)\n        filter_map = tensor[-1]\n        print(filter_map.get_shape())\n        filter_map = tf.transpose(filter_map, perm=[2, 0, 1])\n        print(filter_map.get_shape())\n        filter_map = tf.reshape(filter_map, (how_many, display_size, display_size, 1))\n        print(how_many)\n        self.test_summaries.append(tf.summary.image(name, tensor=filter_map, max_outputs=how_many))\n'"
Season1_Tensorflow1.1_Python3.5/16-19/load.py,0,"b'# encoding:utf-8\n# Python2 \xe5\x85\xbc\xe5\xae\xb9\nfrom __future__ import print_function, division\nfrom scipy.io import loadmat as load\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef reformat(samples, labels):\n    # \xe6\x94\xb9\xe5\x8f\x98\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\n    #  0       1       2      3          3       0       1      2\n    # (\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0) -> (\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0)\n    new = np.transpose(samples, (3, 0, 1, 2)).astype(np.float32)\n\n    # labels \xe5\x8f\x98\xe6\x88\x90 one-hot encoding, [2] -> [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n    # digit 0 , represented as 10\n    # labels \xe5\x8f\x98\xe6\x88\x90 one-hot encoding, [10] -> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n    labels = np.array([x[0] for x in labels])  # slow code, whatever\n    one_hot_labels = []\n    for num in labels:\n        one_hot = [0.0] * 10\n        if num == 10:\n            one_hot[0] = 1.0\n        else:\n            one_hot[num] = 1.0\n        one_hot_labels.append(one_hot)\n    labels = np.array(one_hot_labels).astype(np.float32)\n    return new, labels\n\n\ndef normalize(samples):\n    """"""\n    \xe5\xb9\xb6\xe4\xb8\x94\xe7\x81\xb0\xe5\xba\xa6\xe5\x8c\x96: \xe4\xbb\x8e\xe4\xb8\x89\xe8\x89\xb2\xe9\x80\x9a\xe9\x81\x93 -> \xe5\x8d\x95\xe8\x89\xb2\xe9\x80\x9a\xe9\x81\x93     \xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98 + \xe5\x8a\xa0\xe5\xbf\xab\xe8\xae\xad\xe7\xbb\x83\xe9\x80\x9f\xe5\xba\xa6\n    (R + G + B) / 3\n    \xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe4\xbb\x8e 0 ~ 255 \xe7\xba\xbf\xe6\x80\xa7\xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0 -1.0 ~ +1.0\n    @samples: numpy array\n    """"""\n    a = np.add.reduce(samples, keepdims=True, axis=3)  # shape (\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0)\n    a = a / 3.0\n    return a / 128.0 - 1.0\n\n\ndef distribution(labels, name):\n    # \xe6\x9f\xa5\xe7\x9c\x8b\xe4\xb8\x80\xe4\xb8\x8b\xe6\xaf\x8f\xe4\xb8\xaalabel\xe7\x9a\x84\xe5\x88\x86\xe5\xb8\x83\xef\xbc\x8c\xe5\x86\x8d\xe7\x94\xbb\xe4\xb8\xaa\xe7\xbb\x9f\xe8\xae\xa1\xe5\x9b\xbe\n    # keys:\n    # 0\n    # 1\n    # 2\n    # ...\n    # 9\n    count = {}\n    for label in labels:\n        key = 0 if label[0] == 10 else label[0]\n        if key in count:\n            count[key] += 1\n        else:\n            count[key] = 1\n    x = []\n    y = []\n    for k, v in count.items():\n        # print(k, v)\n        x.append(k)\n        y.append(v)\n\n    y_pos = np.arange(len(x))\n    plt.bar(y_pos, y, align=\'center\', alpha=0.5)\n    plt.xticks(y_pos, x)\n    plt.ylabel(\'Count\')\n    plt.title(name + \' Label Distribution\')\n    plt.show()\n\n\ndef inspect(dataset, labels, i):\n    # \xe6\x98\xbe\xe7\xa4\xba\xe5\x9b\xbe\xe7\x89\x87\xe7\x9c\x8b\xe7\x9c\x8b\n    if dataset.shape[3] == 1:\n        shape = dataset.shape\n        dataset = dataset.reshape(shape[0], shape[1], shape[2])\n    print(labels[i])\n    plt.imshow(dataset[i])\n    plt.show()\n\n\ntrain = load(\'../data/train_32x32.mat\')\ntest = load(\'../data/test_32x32.mat\')\n# extra = load(\'../data/extra_32x32.mat\')\n\n# print(\'Train Samples Shape:\', train[\'X\'].shape)\n# print(\'Train  Labels Shape:\', train[\'y\'].shape)\n\n# print(\'Train Samples Shape:\', test[\'X\'].shape)\n# print(\'Train  Labels Shape:\', test[\'y\'].shape)\n\n# print(\'Train Samples Shape:\', extra[\'X\'].shape)\n# print(\'Train  Labels Shape:\', extra[\'y\'].shape)\n\ntrain_samples = train[\'X\']\ntrain_labels = train[\'y\']\ntest_samples = test[\'X\']\ntest_labels = test[\'y\']\n# extra_samples = extra[\'X\']\n# extra_labels = extra[\'y\']\n\nn_train_samples, _train_labels = reformat(train_samples, train_labels)\nn_test_samples, _test_labels = reformat(test_samples, test_labels)\n\n_train_samples = normalize(n_train_samples)\n_test_samples = normalize(n_test_samples)\n\nnum_labels = 10\nimage_size = 32\nnum_channels = 1\n\nif __name__ == \'__main__\':\n    # \xe6\x8e\xa2\xe7\xb4\xa2\xe6\x95\xb0\xe6\x8d\xae\n    pass\n    inspect(_train_samples, _train_labels, 1234)\n# _train_samples = normalize(_train_samples)\n# inspect(_train_samples, _train_labels, 1234)\n# distribution(train_labels, \'Train Labels\')\n# distribution(test_labels, \'Test Labels\')\n'"
Season1_Tensorflow1.1_Python3.5/16-19/main.py,0,"b'if __name__ == \'__main__\':\n    import load\n    from dp import Network\n\n    train_samples, train_labels = load._train_samples, load._train_labels\n    test_samples, test_labels = load._test_samples, load._test_labels\n\n    print(\'Training set\', train_samples.shape, train_labels.shape)\n    print(\'    Test set\', test_samples.shape, test_labels.shape)\n\n    image_size = load.image_size\n    num_labels = load.num_labels\n    num_channels = load.num_channels\n\n\n    def get_chunk(samples, labels, chunk_size):\n        """"""\n        Iterator/Generator: get a batch of data\n        \xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8/\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\x80\xe6\xac\xa1\xe5\x8f\xaa\xe5\xbe\x97\xe5\x88\xb0 chunk_size \xe8\xbf\x99\xe4\xb9\x88\xe5\xa4\x9a\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n        \xe7\x94\xa8\xe4\xba\x8e for loop\xef\xbc\x8c just like range() function\n        """"""\n        if len(samples) != len(labels):\n            raise Exception(\'Length of samples and labels must equal\')\n        step_start = 0  # initial step\n        i = 0\n        while step_start < len(samples):\n            step_end = step_start + chunk_size\n            if step_end < len(samples):\n                yield i, samples[step_start:step_end], labels[step_start:step_end]\n                i += 1\n            step_start = step_end\n\n    print(\'num:\', image_size, num_channels, num_channels)\n    net = Network(train_batch_size=64, test_batch_size=500, pooling_scale=2)\n    net.define_inputs(\n        train_samples_shape=(64, image_size, image_size, num_channels),\n        train_labels_shape=(64, num_labels),\n        test_samples_shape=(500, image_size, image_size, num_channels)\n    )\n    #\n    net.add_conv(patch_size=3, in_depth=num_channels, out_depth=16, activation=\'relu\', pooling=False, name=\'conv1\')\n    net.add_conv(patch_size=3, in_depth=16, out_depth=16, activation=\'relu\', pooling=True, name=\'conv2\')\n    net.add_conv(patch_size=3, in_depth=16, out_depth=16, activation=\'relu\', pooling=False, name=\'conv3\')\n    net.add_conv(patch_size=3, in_depth=16, out_depth=16, activation=\'relu\', pooling=True, name=\'conv4\')\n\n    # 4 = \xe4\xb8\xa4\xe6\xac\xa1 pooling, \xe6\xaf\x8f\xe4\xb8\x80\xe6\xac\xa1\xe7\xbc\xa9\xe5\xb0\x8f\xe4\xb8\xba 1/2\n    # 16 = conv4 out_depth\n    net.add_fc(in_num_nodes=(image_size // 4) * (image_size // 4) * 16, out_num_nodes=16, activation=\'relu\', name=\'fc1\')\n    net.add_fc(in_num_nodes=16, out_num_nodes=10, activation=None, name=\'fc2\')\n\n    net.define_model()\n    net.run(get_chunk, train_samples, train_labels, test_samples, test_labels)\n\nelse:\n    raise Exception(\'main.py: Should Not Be Imported!!! Must Run by ""python main.py""\')\n'"
Season1_Tensorflow1.1_Python3.5/20/dp.py,56,"b'# \xe6\x96\xb0\xe7\x9a\x84 refined api \xe4\xb8\x8d\xe6\x94\xaf\xe6\x8c\x81 Python2\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\n\nclass Network():\n    def __init__(self, train_batch_size, test_batch_size, pooling_scale,\n                 dropout_rate, base_learning_rate, decay_rate,\n                 optimizeMethod=\'adam\', save_path=\'model/default.ckpt\'):\n        """"""\n        @num_hidden: \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\x9a\x84\xe8\x8a\x82\xe7\x82\xb9\xe6\x95\xb0\xe9\x87\x8f\n        @batch_size\xef\xbc\x9a\xe5\x9b\xa0\xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe8\xa6\x81\xe8\x8a\x82\xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x88\x86\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\xe6\xaf\x8f\xe4\xb8\x80\xe6\x89\xb9\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x87\x8f\xe3\x80\x82\n        """"""\n        self.optimizeMethod = optimizeMethod\n        self.dropout_rate = dropout_rate\n        self.base_learning_rate = base_learning_rate\n        self.decay_rate = decay_rate\n\n        self.train_batch_size = train_batch_size\n        self.test_batch_size = test_batch_size\n\n        # Hyper Parameters\n        self.conv_config = []  # list of dict\n        self.fc_config = []  # list of dict\n        self.conv_weights = []\n        self.conv_biases = []\n        self.fc_weights = []\n        self.fc_biases = []\n        self.pooling_scale = pooling_scale\n        self.pooling_stride = pooling_scale\n\n        # Graph Related\n        self.tf_train_samples = None\n        self.tf_train_labels = None\n        self.tf_test_samples = None\n        self.tf_test_labels = None\n\n        # \xe7\xbb\x9f\xe8\xae\xa1\n        self.writer = None\n        self.merged = None\n        self.train_summaries = []\n        self.test_summaries = []\n\n        # save \xe4\xbf\x9d\xe5\xad\x98\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n        self.saver = None\n        self.save_path = save_path\n\n    def add_conv(self, *, patch_size, in_depth, out_depth, activation=\'relu\', pooling=False, name):\n        """"""\n        This function does not define operations in the graph, but only store config in self.conv_layer_config\n        """"""\n        self.conv_config.append({\n            \'patch_size\': patch_size,\n            \'in_depth\': in_depth,\n            \'out_depth\': out_depth,\n            \'activation\': activation,\n            \'pooling\': pooling,\n            \'name\': name\n        })\n        with tf.name_scope(name):\n            weights = tf.Variable(\n                tf.truncated_normal([patch_size, patch_size, in_depth, out_depth], stddev=0.1), name=name + \'_weights\')\n            biases = tf.Variable(tf.constant(0.1, shape=[out_depth]), name=name + \'_biases\')\n            self.conv_weights.append(weights)\n            self.conv_biases.append(biases)\n\n    def add_fc(self, *, in_num_nodes, out_num_nodes, activation=\'relu\', name):\n        """"""\n        add fc layer config to slef.fc_layer_config\n        """"""\n        self.fc_config.append({\n            \'in_num_nodes\': in_num_nodes,\n            \'out_num_nodes\': out_num_nodes,\n            \'activation\': activation,\n            \'name\': name\n        })\n        with tf.name_scope(name):\n            weights = tf.Variable(tf.truncated_normal([in_num_nodes, out_num_nodes], stddev=0.1))\n            biases = tf.Variable(tf.constant(0.1, shape=[out_num_nodes]))\n            self.fc_weights.append(weights)\n            self.fc_biases.append(biases)\n            self.train_summaries.append(tf.summary.histogram(str(len(self.fc_weights)) + \'_weights\', weights))\n            self.train_summaries.append(tf.summary.histogram(str(len(self.fc_biases)) + \'_biases\', biases))\n\n    def apply_regularization(self, _lambda):\n        # L2 regularization for the fully connected parameters\n        regularization = 0.0\n        for weights, biases in zip(self.fc_weights, self.fc_biases):\n            regularization += tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n        # 1e5\n        return _lambda * regularization\n\n    # should make the definition as an exposed API, instead of implemented in the function\n    def define_inputs(self, *, train_samples_shape, train_labels_shape, test_samples_shape):\n        # \xe8\xbf\x99\xe9\x87\x8c\xe5\x8f\xaa\xe6\x98\xaf\xe5\xae\x9a\xe4\xb9\x89\xe5\x9b\xbe\xe8\xb0\xb1\xe4\xb8\xad\xe7\x9a\x84\xe5\x90\x84\xe7\xa7\x8d\xe5\x8f\x98\xe9\x87\x8f\n        with tf.name_scope(\'inputs\'):\n            self.tf_train_samples = tf.placeholder(tf.float32, shape=train_samples_shape, name=\'tf_train_samples\')\n            self.tf_train_labels = tf.placeholder(tf.float32, shape=train_labels_shape, name=\'tf_train_labels\')\n            self.tf_test_samples = tf.placeholder(tf.float32, shape=test_samples_shape, name=\'tf_test_samples\')\n\n    def define_model(self):\n        """"""\n        \xe5\xae\x9a\xe4\xb9\x89\xe6\x88\x91\xe7\x9a\x84\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe8\xb0\xb1\n        """"""\n\n        def model(data_flow, train=True):\n            """"""\n            @data: original inputs\n            @return: logits\n            """"""\n            # Define Convolutional Layers\n            for i, (weights, biases, config) in enumerate(zip(self.conv_weights, self.conv_biases, self.conv_config)):\n                with tf.name_scope(config[\'name\'] + \'_model\'):\n                    with tf.name_scope(\'convolution\'):\n                        # default 1,1,1,1 stride and SAME padding\n                        data_flow = tf.nn.conv2d(data_flow, filter=weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n                        data_flow = data_flow + biases\n                        if not train:\n                            self.visualize_filter_map(data_flow, how_many=config[\'out_depth\'],\n                                                      display_size=32 // (i // 2 + 1), name=config[\'name\'] + \'_conv\')\n                    if config[\'activation\'] == \'relu\':\n                        data_flow = tf.nn.relu(data_flow)\n                        if not train:\n                            self.visualize_filter_map(data_flow, how_many=config[\'out_depth\'],\n                                                      display_size=32 // (i // 2 + 1), name=config[\'name\'] + \'_relu\')\n                    else:\n                        raise Exception(\'Activation Func can only be Relu right now. You passed\', config[\'activation\'])\n                    if config[\'pooling\']:\n                        data_flow = tf.nn.max_pool(\n                            data_flow,\n                            ksize=[1, self.pooling_scale, self.pooling_scale, 1],\n                            strides=[1, self.pooling_stride, self.pooling_stride, 1],\n                            padding=\'SAME\')\n                        if not train:\n                            self.visualize_filter_map(data_flow, how_many=config[\'out_depth\'],\n                                                      display_size=32 // (i // 2 + 1) // 2,\n                                                      name=config[\'name\'] + \'_pooling\')\n\n            # Define Fully Connected Layers\n            for i, (weights, biases, config) in enumerate(zip(self.fc_weights, self.fc_biases, self.fc_config)):\n                if i == 0:\n                    shape = data_flow.get_shape().as_list()\n                    data_flow = tf.reshape(data_flow, [shape[0], shape[1] * shape[2] * shape[3]])\n                with tf.name_scope(config[\'name\'] + \'model\'):\n\n                    ### Dropout\n                    if train and i == len(self.fc_weights) - 1:\n                        data_flow = tf.nn.dropout(data_flow, self.dropout_rate, seed=4926)\n                    ###\n\n                    data_flow = tf.matmul(data_flow, weights) + biases\n                    if config[\'activation\'] == \'relu\':\n                        data_flow = tf.nn.relu(data_flow)\n                    elif config[\'activation\'] is None:\n                        pass\n                    else:\n                        raise Exception(\'Activation Func can only be Relu or None right now. You passed\',\n                                        config[\'activation\'])\n            return data_flow\n\n        # Training computation.\n        logits = model(self.tf_train_samples)\n        with tf.name_scope(\'loss\'):\n            self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.tf_train_labels))\n            self.loss += self.apply_regularization(_lambda=5e-4)\n            self.train_summaries.append(tf.summary.scalar(\'Loss\', self.loss))\n\n        # learning rate decay\n        global_step = tf.Variable(0)\n        learning_rate = tf.train.exponential_decay(\n            learning_rate=self.base_learning_rate,\n            global_step=global_step * self.train_batch_size,\n            decay_steps=100,\n            decay_rate=self.decay_rate,\n            staircase=True\n        )\n\n        # Optimizer.\n        with tf.name_scope(\'optimizer\'):\n            if (self.optimizeMethod == \'gradient\'):\n                self.optimizer = tf.train \\\n                    .GradientDescentOptimizer(learning_rate) \\\n                    .minimize(self.loss)\n            elif (self.optimizeMethod == \'momentum\'):\n                self.optimizer = tf.train \\\n                    .MomentumOptimizer(learning_rate, 0.5) \\\n                    .minimize(self.loss)\n            elif (self.optimizeMethod == \'adam\'):\n                self.optimizer = tf.train \\\n                    .AdamOptimizer(learning_rate) \\\n                    .minimize(self.loss)\n\n        # Predictions for the training, validation, and test data.\n        with tf.name_scope(\'train\'):\n            self.train_prediction = tf.nn.softmax(logits, name=\'train_prediction\')\n            tf.add_to_collection(""prediction"", self.train_prediction)\n        with tf.name_scope(\'test\'):\n            self.test_prediction = tf.nn.softmax(model(self.tf_test_samples, train=False), name=\'test_prediction\')\n            tf.add_to_collection(""prediction"", self.test_prediction)\n\n            single_shape = (1, 32, 32, 1)\n            single_input = tf.placeholder(tf.float32, shape=single_shape, name=\'single_input\')\n            self.single_prediction = tf.nn.softmax(model(single_input, train=False), name=\'single_prediction\')\n            tf.add_to_collection(""prediction"", self.single_prediction)\n\n        self.merged_train_summary = tf.summary.merge(self.train_summaries)\n        self.merged_test_summary = tf.summary.merge(self.test_summaries)\n\n        # \xe6\x94\xbe\xe5\x9c\xa8\xe5\xae\x9a\xe4\xb9\x89Graph\xe4\xb9\x8b\xe5\x90\x8e\xef\xbc\x8c\xe4\xbf\x9d\xe5\xad\x98\xe8\xbf\x99\xe5\xbc\xa0\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\n        self.saver = tf.train.Saver(tf.all_variables())\n\n    def run(self, train_samples, train_labels, test_samples, test_labels, *, train_data_iterator, iteration_steps,\n            test_data_iterator):\n        """"""\n        \xe7\x94\xa8\xe5\x88\xb0Session\n        :data_iterator: a function that yields chuck of data\n        """"""\n        self.writer = tf.summary.FileWriter(\'./board\', tf.get_default_graph())\n\n        with tf.Session(graph=tf.get_default_graph()) as session:\n            tf.initialize_all_variables().run()\n\n            ### \xe8\xae\xad\xe7\xbb\x83\n            print(\'Start Training\')\n            # batch 1000\n            for i, samples, labels in train_data_iterator(train_samples, train_labels, iteration_steps=iteration_steps,\n                                                          chunkSize=self.train_batch_size):\n                _, l, predictions, summary = session.run(\n                    [self.optimizer, self.loss, self.train_prediction, self.merged_train_summary],\n                    feed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels}\n                )\n                self.writer.add_summary(summary, i)\n                # labels is True Labels\n                accuracy, _ = self.accuracy(predictions, labels)\n                if i % 50 == 0:\n                    print(\'Minibatch loss at step %d: %f\' % (i, l))\n                    print(\'Minibatch accuracy: %.1f%%\' % accuracy)\n            ###\n\n            ### \xe6\xb5\x8b\xe8\xaf\x95\n            accuracies = []\n            confusionMatrices = []\n            for i, samples, labels in test_data_iterator(test_samples, test_labels, chunkSize=self.test_batch_size):\n                result, summary = session.run(\n                    [self.test_prediction, self.merged_test_summary],\n                    feed_dict={self.tf_test_samples: samples}\n                )\n                self.writer.add_summary(summary, i)\n                accuracy, cm = self.accuracy(result, labels, need_confusion_matrix=True)\n                accuracies.append(accuracy)\n                confusionMatrices.append(cm)\n                print(\'Test Accuracy: %.1f%%\' % accuracy)\n            print(\' Average  Accuracy:\', np.average(accuracies))\n            print(\'Standard Deviation:\', np.std(accuracies))\n            self.print_confusion_matrix(np.add.reduce(confusionMatrices))\n            ###\n\n    def train(self, train_samples, train_labels, *, data_iterator, iteration_steps):\n        self.writer = tf.summary.FileWriter(\'./board\', tf.get_default_graph())\n        with tf.Session(graph=tf.get_default_graph()) as session:\n            tf.initialize_all_variables().run()\n\n            ### \xe8\xae\xad\xe7\xbb\x83\n            print(\'Start Training\')\n            # batch 1000\n            for i, samples, labels in data_iterator(train_samples, train_labels, iteration_steps=iteration_steps,\n                                                    chunkSize=self.train_batch_size):\n                _, l, predictions, summary = session.run(\n                    [self.optimizer, self.loss, self.train_prediction, self.merged_train_summary],\n                    feed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels}\n                )\n                self.writer.add_summary(summary, i)\n                # labels is True Labels\n                accuracy, _ = self.accuracy(predictions, labels)\n                if i % 50 == 0:\n                    print(\'Minibatch loss at step %d: %f\' % (i, l))\n                    print(\'Minibatch accuracy: %.1f%%\' % accuracy)\n            ###\n\n            # \xe6\xa3\x80\xe6\x9f\xa5\xe8\xa6\x81\xe5\xad\x98\xe6\x94\xbe\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xe5\x80\xbc\xe5\x90\xa6\xe5\xad\x98\xe5\x9c\xa8\xe3\x80\x82\xe8\xbf\x99\xe9\x87\x8c\xe5\x81\x87\xe5\xae\x9a\xe5\x8f\xaa\xe6\x9c\x89\xe4\xb8\x80\xe5\xb1\x82\xe8\xb7\xaf\xe5\xbe\x84\xe3\x80\x82\n            import os\n            if os.path.isdir(self.save_path.split(\'/\')[0]):\n                save_path = self.saver.save(session, self.save_path)\n                print(""Model saved in file: %s"" % save_path)\n            else:\n                os.makedirs(self.save_path.split(\'/\')[0])\n                save_path = self.saver.save(session, self.save_path)\n                print(""Model saved in file: %s"" % save_path)\n\n    def test(self, test_samples, test_labels, *, data_iterator):\n        if self.saver is None:\n            self.define_model()\n        if self.writer is None:\n            self.writer = tf.summary.FileWriter(\'./board\', tf.get_default_graph())\n\n        print(\'Before session\')\n        with tf.Session(graph=tf.get_default_graph()) as session:\n            self.saver.restore(session, self.save_path)\n            ### \xe6\xb5\x8b\xe8\xaf\x95\n            accuracies = []\n            confusionMatrices = []\n            for i, samples, labels in data_iterator(test_samples, test_labels, chunkSize=self.test_batch_size):\n                result = session.run(\n                    self.test_prediction,\n                    feed_dict={self.tf_test_samples: samples}\n                )\n                # self.writer.add_summary(summary, i)\n                accuracy, cm = self.accuracy(result, labels, need_confusion_matrix=True)\n                accuracies.append(accuracy)\n                confusionMatrices.append(cm)\n                print(\'Test Accuracy: %.1f%%\' % accuracy)\n            print(\' Average  Accuracy:\', np.average(accuracies))\n            print(\'Standard Deviation:\', np.std(accuracies))\n            self.print_confusion_matrix(np.add.reduce(confusionMatrices))\n            ###\n\n    def accuracy(self, predictions, labels, need_confusion_matrix=False):\n        """"""\n        \xe8\xae\xa1\xe7\xae\x97\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87\xe4\xb8\x8e\xe5\x8f\xac\xe5\x9b\x9e\xe7\x8e\x87\n        @return: accuracy and confusionMatrix as a tuple\n        """"""\n        _predictions = np.argmax(predictions, 1)\n        _labels = np.argmax(labels, 1)\n        cm = confusion_matrix(_labels, _predictions) if need_confusion_matrix else None\n        # == is overloaded for numpy array\n        accuracy = (100.0 * np.sum(_predictions == _labels) / predictions.shape[0])\n        return accuracy, cm\n\n    def visualize_filter_map(self, tensor, *, how_many, display_size, name):\n        # print(tensor.get_shape)\n        filter_map = tensor[-1]\n        # print(filter_map.get_shape())\n        filter_map = tf.transpose(filter_map, perm=[2, 0, 1])\n        # print(filter_map.get_shape())\n        filter_map = tf.reshape(filter_map, (how_many, display_size, display_size, 1))\n        # print(how_many)\n        self.test_summaries.append(tf.summary.image(name, tensor=filter_map, max_outputs=how_many))\n\n    def print_confusion_matrix(self, confusionMatrix):\n        print(\'Confusion    Matrix:\')\n        for i, line in enumerate(confusionMatrix):\n            print(line, line[i] / np.sum(line))\n        a = 0\n        for i, column in enumerate(np.transpose(confusionMatrix, (1, 0))):\n            a += (column[i] / np.sum(column)) * (np.sum(column) / 26000)\n            print(column[i] / np.sum(column), )\n        print(\'\\n\', np.sum(confusionMatrix), a)\n'"
Season1_Tensorflow1.1_Python3.5/20/load.py,0,"b'# encoding:utf-8\n# Python2 \xe5\x85\xbc\xe5\xae\xb9\nfrom __future__ import print_function, division\nfrom scipy.io import loadmat as load\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef reformat(samples, labels):\n    # \xe6\x94\xb9\xe5\x8f\x98\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\n    #  0       1       2      3          3       0       1      2\n    # (\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0) -> (\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0)\n    new = np.transpose(samples, (3, 0, 1, 2)).astype(np.float32)\n\n    # labels \xe5\x8f\x98\xe6\x88\x90 one-hot encoding, [2] -> [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n    # digit 0 , represented as 10\n    # labels \xe5\x8f\x98\xe6\x88\x90 one-hot encoding, [10] -> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n    labels = np.array([x[0] for x in labels])  # slow code, whatever\n    one_hot_labels = []\n    for num in labels:\n        one_hot = [0.0] * 10\n        if num == 10:\n            one_hot[0] = 1.0\n        else:\n            one_hot[num] = 1.0\n        one_hot_labels.append(one_hot)\n    labels = np.array(one_hot_labels).astype(np.float32)\n    return new, labels\n\n\ndef normalize(samples):\n    """"""\n    \xe5\xb9\xb6\xe4\xb8\x94\xe7\x81\xb0\xe5\xba\xa6\xe5\x8c\x96: \xe4\xbb\x8e\xe4\xb8\x89\xe8\x89\xb2\xe9\x80\x9a\xe9\x81\x93 -> \xe5\x8d\x95\xe8\x89\xb2\xe9\x80\x9a\xe9\x81\x93     \xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98 + \xe5\x8a\xa0\xe5\xbf\xab\xe8\xae\xad\xe7\xbb\x83\xe9\x80\x9f\xe5\xba\xa6\n    (R + G + B) / 3\n    \xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe4\xbb\x8e 0 ~ 255 \xe7\xba\xbf\xe6\x80\xa7\xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0 -1.0 ~ +1.0\n    @samples: numpy array\n    """"""\n    a = np.add.reduce(samples, keepdims=True, axis=3)  # shape (\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0)\n    a = a / 3.0\n    return a / 128.0 - 1.0\n\n\ndef distribution(labels, name):\n    # \xe6\x9f\xa5\xe7\x9c\x8b\xe4\xb8\x80\xe4\xb8\x8b\xe6\xaf\x8f\xe4\xb8\xaalabel\xe7\x9a\x84\xe5\x88\x86\xe5\xb8\x83\xef\xbc\x8c\xe5\x86\x8d\xe7\x94\xbb\xe4\xb8\xaa\xe7\xbb\x9f\xe8\xae\xa1\xe5\x9b\xbe\n    # keys:\n    # 0\n    # 1\n    # 2\n    # ...\n    # 9\n    count = {}\n    for label in labels:\n        key = 0 if label[0] == 10 else label[0]\n        if key in count:\n            count[key] += 1\n        else:\n            count[key] = 1\n    x = []\n    y = []\n    for k, v in count.items():\n        # print(k, v)\n        x.append(k)\n        y.append(v)\n\n    y_pos = np.arange(len(x))\n    plt.bar(y_pos, y, align=\'center\', alpha=0.5)\n    plt.xticks(y_pos, x)\n    plt.ylabel(\'Count\')\n    plt.title(name + \' Label Distribution\')\n    plt.show()\n\n\ndef inspect(dataset, labels, i):\n    # \xe6\x98\xbe\xe7\xa4\xba\xe5\x9b\xbe\xe7\x89\x87\xe7\x9c\x8b\xe7\x9c\x8b\n    if dataset.shape[3] == 1:\n        shape = dataset.shape\n        dataset = dataset.reshape(shape[0], shape[1], shape[2])\n    print(labels[i])\n    plt.imshow(dataset[i])\n    plt.show()\n\n\ntrain = load(\'../data/train_32x32.mat\')\ntest = load(\'../data/test_32x32.mat\')\n# extra = load(\'../data/extra_32x32.mat\')\n\n# print(\'Train Samples Shape:\', train[\'X\'].shape)\n# print(\'Train  Labels Shape:\', train[\'y\'].shape)\n\n# print(\'Train Samples Shape:\', test[\'X\'].shape)\n# print(\'Train  Labels Shape:\', test[\'y\'].shape)\n\n# print(\'Train Samples Shape:\', extra[\'X\'].shape)\n# print(\'Train  Labels Shape:\', extra[\'y\'].shape)\n\ntrain_samples = train[\'X\']\ntrain_labels = train[\'y\']\ntest_samples = test[\'X\']\ntest_labels = test[\'y\']\n# extra_samples = extra[\'X\']\n# extra_labels = extra[\'y\']\n\nn_train_samples, _train_labels = reformat(train_samples, train_labels)\nn_test_samples, _test_labels = reformat(test_samples, test_labels)\n\n_train_samples = normalize(n_train_samples)\n_test_samples = normalize(n_test_samples)\n\nnum_labels = 10\nimage_size = 32\nnum_channels = 1\n\nif __name__ == \'__main__\':\n    # \xe6\x8e\xa2\xe7\xb4\xa2\xe6\x95\xb0\xe6\x8d\xae\n    pass\n    inspect(_train_samples, _train_labels, 1234)\n# _train_samples = normalize(_train_samples)\n# inspect(_train_samples, _train_labels, 1234)\n# distribution(train_labels, \'Train Labels\')\n# distribution(test_labels, \'Test Labels\')\n'"
Season1_Tensorflow1.1_Python3.5/20/main.py,0,"b'if __name__ == \'__main__\':\n    import load\n    from dp import Network\n\n    train_samples, train_labels = load._train_samples, load._train_labels\n    test_samples, test_labels = load._test_samples, load._test_labels\n\n    print(\'Training set\', train_samples.shape, train_labels.shape)\n    print(\'    Test set\', test_samples.shape, test_labels.shape)\n\n    image_size = load.image_size\n    num_labels = load.num_labels\n    num_channels = load.num_channels\n\n\n    def train_data_iterator(samples, labels, iteration_steps, chunkSize):\n        """"""\n        Iterator/Generator: get a batch of data\n        \xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8/\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\x80\xe6\xac\xa1\xe5\x8f\xaa\xe5\xbe\x97\xe5\x88\xb0 chunkSize \xe8\xbf\x99\xe4\xb9\x88\xe5\xa4\x9a\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n        \xe7\x94\xa8\xe4\xba\x8e for loop\xef\xbc\x8c just like range() function\n        """"""\n        if len(samples) != len(labels):\n            raise Exception(\'Length of samples and labels must equal\')\n        stepStart = 0  # initial step\n        i = 0\n        while i < iteration_steps:\n            stepStart = (i * chunkSize) % (labels.shape[0] - chunkSize)\n            yield i, samples[stepStart:stepStart + chunkSize], labels[stepStart:stepStart + chunkSize]\n            i += 1\n\n\n    def test_data_iterator(samples, labels, chunkSize):\n        """"""\n        Iterator/Generator: get a batch of data\n        \xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8/\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\x80\xe6\xac\xa1\xe5\x8f\xaa\xe5\xbe\x97\xe5\x88\xb0 chunkSize \xe8\xbf\x99\xe4\xb9\x88\xe5\xa4\x9a\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n        \xe7\x94\xa8\xe4\xba\x8e for loop\xef\xbc\x8c just like range() function\n        """"""\n        if len(samples) != len(labels):\n            raise Exception(\'Length of samples and labels must equal\')\n        stepStart = 0  # initial step\n        i = 0\n        while stepStart < len(samples):\n            stepEnd = stepStart + chunkSize\n            if stepEnd < len(samples):\n                yield i, samples[stepStart:stepEnd], labels[stepStart:stepEnd]\n                i += 1\n            stepStart = stepEnd\n\n\n    net = Network(\n        train_batch_size=64, test_batch_size=500, pooling_scale=2,\n        dropout_rate=0.9,\n        base_learning_rate=0.001, decay_rate=0.99)\n    net.define_inputs(\n        train_samples_shape=(64, image_size, image_size, num_channels),\n        train_labels_shape=(64, num_labels),\n        test_samples_shape=(500, image_size, image_size, num_channels),\n    )\n    #\n    net.add_conv(patch_size=3, in_depth=num_channels, out_depth=32, activation=\'relu\', pooling=False, name=\'conv1\')\n    net.add_conv(patch_size=3, in_depth=32, out_depth=32, activation=\'relu\', pooling=True, name=\'conv2\')\n    net.add_conv(patch_size=3, in_depth=32, out_depth=32, activation=\'relu\', pooling=False, name=\'conv3\')\n    net.add_conv(patch_size=3, in_depth=32, out_depth=32, activation=\'relu\', pooling=True, name=\'conv4\')\n\n    # 4 = \xe4\xb8\xa4\xe6\xac\xa1 pooling, \xe6\xaf\x8f\xe4\xb8\x80\xe6\xac\xa1\xe7\xbc\xa9\xe5\xb0\x8f\xe4\xb8\xba 1/2\n    # 32 = conv4 out_depth\n    net.add_fc(in_num_nodes=(image_size // 4) * (image_size // 4) * 32, out_num_nodes=128, activation=\'relu\',\n               name=\'fc1\')\n    net.add_fc(in_num_nodes=128, out_num_nodes=10, activation=None, name=\'fc2\')\n\n    net.define_model()\n    # net.run(train_samples, train_labels, test_samples, test_labels, train_data_iterator=train_data_iterator,\n    #         iteration_steps=3000, test_data_iterator=test_data_iterator)\n    net.train(train_samples, train_labels, data_iterator=train_data_iterator, iteration_steps=2000)\n    net.test(test_samples, test_labels, data_iterator=test_data_iterator)\n\nelse:\n    raise Exception(\'main.py: Should Not Be Imported!!! Must Run by ""python main.py""\')\n'"
Season1_Tensorflow1.1_Python3.5/4-6/load.py,0,"b'# encoding:utf-8\n# Python2 \xe5\x85\xbc\xe5\xae\xb9\nfrom __future__ import print_function, division\nfrom scipy.io import loadmat as load\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef reformat(samples, labels):\n    # \xe6\x94\xb9\xe5\x8f\x98\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\n    #  0       1       2      3          3       0       1      2\n    # (\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0) -> (\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0)\n    new = np.transpose(samples, (3, 0, 1, 2)).astype(np.float32)\n\n    # labels \xe5\x8f\x98\xe6\x88\x90 one-hot encoding, [2] -> [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n    # digit 0 , represented as 10\n    # labels \xe5\x8f\x98\xe6\x88\x90 one-hot encoding, [10] -> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n    labels = np.array([x[0] for x in labels])  # slow code, whatever\n    one_hot_labels = []\n    for num in labels:\n        one_hot = [0.0] * 10\n        if num == 10:\n            one_hot[0] = 1.0\n        else:\n            one_hot[num] = 1.0\n        one_hot_labels.append(one_hot)\n    labels = np.array(one_hot_labels).astype(np.float32)\n    return new, labels\n\n\ndef normalize(samples):\n    """"""\n    \xe5\xb9\xb6\xe4\xb8\x94\xe7\x81\xb0\xe5\xba\xa6\xe5\x8c\x96: \xe4\xbb\x8e\xe4\xb8\x89\xe8\x89\xb2\xe9\x80\x9a\xe9\x81\x93 -> \xe5\x8d\x95\xe8\x89\xb2\xe9\x80\x9a\xe9\x81\x93     \xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98 + \xe5\x8a\xa0\xe5\xbf\xab\xe8\xae\xad\xe7\xbb\x83\xe9\x80\x9f\xe5\xba\xa6\n    (R + G + B) / 3\n    \xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe4\xbb\x8e 0 ~ 255 \xe7\xba\xbf\xe6\x80\xa7\xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0 -1.0 ~ +1.0\n    @samples: numpy array\n    """"""\n    a = np.add.reduce(samples, keepdims=True, axis=3)  # shape (\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0)\n    a = a / 3.0\n    return a / 128.0 - 1.0\n\n\ndef distribution(labels, name):\n    # \xe6\x9f\xa5\xe7\x9c\x8b\xe4\xb8\x80\xe4\xb8\x8b\xe6\xaf\x8f\xe4\xb8\xaalabel\xe7\x9a\x84\xe5\x88\x86\xe5\xb8\x83\xef\xbc\x8c\xe5\x86\x8d\xe7\x94\xbb\xe4\xb8\xaa\xe7\xbb\x9f\xe8\xae\xa1\xe5\x9b\xbe\n    # keys:\n    # 0\n    # 1\n    # 2\n    # ...\n    # 9\n    count = {}\n    for label in labels:\n        key = 0 if label[0] == 10 else label[0]\n        if key in count:\n            count[key] += 1\n        else:\n            count[key] = 1\n    x = []\n    y = []\n    for k, v in count.items():\n        # print(k, v)\n        x.append(k)\n        y.append(v)\n\n    y_pos = np.arange(len(x))\n    plt.bar(y_pos, y, align=\'center\', alpha=0.5)\n    plt.xticks(y_pos, x)\n    plt.ylabel(\'Count\')\n    plt.title(name + \' Label Distribution\')\n    plt.show()\n\n\ndef inspect(dataset, labels, i):\n    # \xe6\x98\xbe\xe7\xa4\xba\xe5\x9b\xbe\xe7\x89\x87\xe7\x9c\x8b\xe7\x9c\x8b\n    print(labels[i])\n    \'\'\'\n    if dataset.shape[3] == 1:\n        shape = dataset.shape\n        dataset = dataset.reshape(shape[0], shape[1], shape[2])\n    plt.imshow(dataset[i])\n    \'\'\'  # \xe5\x8f\xaf\xe4\xbb\xa5\xe6\x94\xb9\xe4\xb8\xba\xe4\xbb\xa5\xe4\xb8\x8b\n    plt.imshow(dataset[i].squeeze())\n    plt.show()\n\n\ntrain = load(\'../data/train_32x32.mat\')\ntest = load(\'../data/test_32x32.mat\')\n# extra = load(\'../data/extra_32x32.mat\')\n\nprint(\'Train Samples Shape:\', train[\'X\'].shape)\nprint(\'Train  Labels Shape:\', train[\'y\'].shape)\n\nprint(\'Train Samples Shape:\', test[\'X\'].shape)\nprint(\'Train  Labels Shape:\', test[\'y\'].shape)\n\n# print(\'Train Samples Shape:\', extra[\'X\'].shape)\n# print(\'Train  Labels Shape:\', extra[\'y\'].shape)\n\ntrain_samples = train[\'X\']\ntrain_labels = train[\'y\']\n# test_samples = test[\'X\']\n# test_labels = test[\'y\']\n# test_samples = extra[\'X\']\n# test_labels = extra[\'y\']\n\n_train_samples, _train_labels = reformat(train_samples, train_labels)\n# _test_samples, _test_labels = reformat(test_samples, test_labels)\n#\n# _train_dataset = normalize(n_train_dataset)\n# _test_dataset = normalize(n_test_dataset)\n\nnum_labels = 10\nimage_size = 32\n\nif __name__ == \'__main__\':\n    # \xe6\x8e\xa2\xe7\xb4\xa2\xe6\x95\xb0\xe6\x8d\xae\n    pass\n# inspect(_train_samples, _train_labels, 1234)\n# _train_samples = normalize(_train_samples)\n# inspect(_train_samples, _train_labels, 1234)\n# distribution(train_labels, \'Train Labels\')\n# distribution(test_labels, \'Test Labels\')\n'"
Season1_Tensorflow1.1_Python3.5/4-6/run.py,0,"b'#\xe3\x80\x8aTF Girls \xe4\xbf\xae\xe7\x82\xbc\xe6\x8c\x87\xe5\x8d\x97\xe3\x80\x8b\xe7\xac\xac\xe5\x9b\x9b\xe6\x9c\x9f\n\n# \xe6\xad\xa3\xe5\xbc\x8f\xe5\xbc\x80\xe5\xa7\x8b\xe6\x9c\xba\xe5\x99\xa8\xe5\xad\xa6\xe4\xb9\xa0\n\n# \xe9\xa6\x96\xe5\x85\x88\xe6\x88\x91\xe4\xbb\xac\xe8\xa6\x81\xe7\xa1\xae\xe5\xae\x9a\xe4\xb8\x80\xe4\xb8\xaa\xe7\x9b\xae\xe6\xa0\x87: \xe5\x9b\xbe\xe5\x83\x8f\xe8\xaf\x86\xe5\x88\xab\n\n# \xe6\x88\x91\xe8\xbf\x99\xe9\x87\x8c\xe5\xb0\xb1\xe7\x94\xa8Udacity Deep Learning\xe7\x9a\x84\xe4\xbd\x9c\xe4\xb8\x9a\xe4\xbd\x9c\xe4\xb8\xba\xe8\xbe\x85\xe5\x8a\xa9\xe4\xba\x86\n\n# 1. \xe4\xb8\x8b\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae  http://ufldl.stanford.edu/housenumbers/\n# 2. \xe6\x8e\xa2\xe7\xb4\xa2\xe6\x95\xb0\xe6\x8d\xae\n# 3. \xe5\xa4\x84\xe7\x90\x86\xe6\x95\xb0\xe6\x8d\xae\n# 4. \xe6\x9e\x84\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x9f\xba\xe6\x9c\xac\xe7\xbd\x91\xe7\xbb\x9c, \xe5\x9f\xba\xe6\x9c\xac\xe7\x9a\x84\xe6\xa6\x82\xe5\xbf\xb5+\xe4\xbb\xa3\xe7\xa0\x81 \xef\xbc\x8c TensorFlow\xe7\x9a\x84\xe4\xb8\x96\xe7\x95\x8c\n# 5. \xe5\x8d\xb7\xe7\xa7\xafji\n# 6. \xe6\x9d\xa5\xe5\xae\x9e\xe9\xaa\x8c\xe5\x90\xa7\n# 7. \xe5\xbe\xae\xe8\xb0\x83\xe4\xb8\x8e\xe7\xbb\x93\xe6\x9e\x9c\n'"
Season1_Tensorflow1.1_Python3.5/7-9/dp.py,23,"b'from __future__ import print_function, division\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nimport load\n\ntrain_samples, train_labels = load._train_samples, load._train_labels\ntest_samples, test_labels = load._test_samples, load._test_labels\n\nprint(\'Training set\', train_samples.shape, train_labels.shape)\nprint(\'    Test set\', test_samples.shape, test_labels.shape)\n\nimage_size = load.image_size\nnum_labels = load.num_labels\nnum_channels = load.num_channels\n\n\ndef get_chunk(samples, labels, chunkSize):\n    """"""\n    Iterator/Generator: get a batch of data\n    \xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8/\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\x80\xe6\xac\xa1\xe5\x8f\xaa\xe5\xbe\x97\xe5\x88\xb0 chunkSize \xe8\xbf\x99\xe4\xb9\x88\xe5\xa4\x9a\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n    \xe7\x94\xa8\xe4\xba\x8e for loop\xef\xbc\x8c just like range() function\n    """"""\n    if len(samples) != len(labels):\n        raise Exception(\'Length of samples and labels must equal\')\n    stepStart = 0  # initial step\n    i = 0\n    while stepStart < len(samples):\n        stepEnd = stepStart + chunkSize\n        if stepEnd < len(samples):\n            yield i, samples[stepStart:stepEnd], labels[stepStart:stepEnd]\n            i += 1\n        stepStart = stepEnd\n\n\nclass Network():\n    def __init__(self, num_hidden, batch_size):\n        """"""\n        @num_hidden: \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\x9a\x84\xe8\x8a\x82\xe7\x82\xb9\xe6\x95\xb0\xe9\x87\x8f\n        @batch_size\xef\xbc\x9a\xe5\x9b\xa0\xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe8\xa6\x81\xe8\x8a\x82\xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x88\x86\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\xe6\xaf\x8f\xe4\xb8\x80\xe6\x89\xb9\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x87\x8f\xe3\x80\x82\n        """"""\n        self.batch_size = batch_size\n        self.test_batch_size = 500\n\n        # Hyper Parameters\n        self.num_hidden = num_hidden\n\n        # Graph Related\n        self.graph = tf.Graph()\n        self.tf_train_samples = None\n        self.tf_train_labels = None\n        self.tf_test_samples = None\n        self.tf_test_labels = None\n        self.tf_test_prediction = None\n\n    def define_graph(self):\n        """"""\n        \xe5\xae\x9a\xe4\xb9\x89\xe6\x88\x91\xe7\x9a\x84\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe8\xb0\xb1\n        """"""\n        with self.graph.as_default():\n            # \xe8\xbf\x99\xe9\x87\x8c\xe5\x8f\xaa\xe6\x98\xaf\xe5\xae\x9a\xe4\xb9\x89\xe5\x9b\xbe\xe8\xb0\xb1\xe4\xb8\xad\xe7\x9a\x84\xe5\x90\x84\xe7\xa7\x8d\xe5\x8f\x98\xe9\x87\x8f\n            self.tf_train_samples = tf.placeholder(\n                tf.float32, shape=(self.batch_size, image_size, image_size, num_channels)\n            )\n            self.tf_train_labels = tf.placeholder(\n                tf.float32, shape=(self.batch_size, num_labels)\n            )\n            self.tf_test_samples = tf.placeholder(\n                tf.float32, shape=(self.test_batch_size, image_size, image_size, num_channels)\n            )\n\n            # fully connected layer 1, fully connected\n            fc1_weights = tf.Variable(\n                tf.truncated_normal([image_size * image_size, self.num_hidden], stddev=0.1)\n            )\n            fc1_biases = tf.Variable(tf.constant(0.1, shape=[self.num_hidden]))\n\n            # fully connected layer 2 --> output layer\n            fc2_weights = tf.Variable(\n                tf.truncated_normal([self.num_hidden, num_labels], stddev=0.1)\n            )\n            fc2_biases = tf.Variable(tf.constant(0.1, shape=[num_labels]))\n\n            # \xe6\x83\xb3\xe5\x9c\xa8\xe6\x9d\xa5\xe5\xae\x9a\xe4\xb9\x89\xe5\x9b\xbe\xe8\xb0\xb1\xe7\x9a\x84\xe8\xbf\x90\xe7\xae\x97\n            def model(data):\n                # fully connected layer 1\n                shape = data.get_shape().as_list()\n                print(data.get_shape(), shape)\n                reshape = tf.reshape(data, [shape[0], shape[1] * shape[2] * shape[3]])\n                print(reshape.get_shape(), fc1_weights.get_shape(), fc1_biases.get_shape())\n                hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n\n                # fully connected layer 2\n                return tf.matmul(hidden, fc2_weights) + fc2_biases\n\n            # Training computation.\n            logits = model(self.tf_train_samples)\n            self.loss = tf.reduce_mean(\n                tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.tf_train_labels)\n            )\n\n            # Optimizer.\n            self.optimizer = tf.train.GradientDescentOptimizer(0.0001).minimize(self.loss)\n\n            # Predictions for the training, validation, and test data.\n            self.train_prediction = tf.nn.softmax(logits)\n            self.test_prediction = tf.nn.softmax(model(self.tf_test_samples))\n\n    def run(self):\n        """"""\n        \xe7\x94\xa8\xe5\x88\xb0Session\n        """"""\n\n        # private function\n        def print_confusion_matrix(confusionMatrix):\n            print(\'Confusion    Matrix:\')\n            for i, line in enumerate(confusionMatrix):\n                print(line, line[i] / np.sum(line))\n            a = 0\n            for i, column in enumerate(np.transpose(confusionMatrix, (1, 0))):\n                a += (column[i] / np.sum(column)) * (np.sum(column) / 26000)\n                print(column[i] / np.sum(column), )\n            print(\'\\n\', np.sum(confusionMatrix), a)\n\n        self.session = tf.Session(graph=self.graph)\n        with self.session as session:\n            tf.initialize_all_variables().run()\n\n            ### \xe8\xae\xad\xe7\xbb\x83\n            print(\'Start Training\')\n            # batch 1000\n            for i, samples, labels in get_chunk(train_samples, train_labels, chunkSize=self.batch_size):\n                _, l, predictions = session.run(\n                    [self.optimizer, self.loss, self.train_prediction],\n                    feed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels}\n                )\n                # labels is True Labels\n                accuracy, _ = self.accuracy(predictions, labels)\n                if i % 50 == 0:\n                    print(\'Minibatch loss at step %d: %f\' % (i, l))\n                    print(\'Minibatch accuracy: %.1f%%\' % accuracy)\n            ###\n\n            ### \xe6\xb5\x8b\xe8\xaf\x95\n            accuracies = []\n            confusionMatrices = []\n            for i, samples, labels in get_chunk(test_samples, test_labels, chunkSize=self.test_batch_size):\n                result = self.test_prediction.eval(feed_dict={self.tf_test_samples: samples})\n                accuracy, cm = self.accuracy(result, labels, need_confusion_matrix=True)\n                accuracies.append(accuracy)\n                confusionMatrices.append(cm)\n                print(\'Test Accuracy: %.1f%%\' % accuracy)\n            print(\' Average  Accuracy:\', np.average(accuracies))\n            print(\'Standard Deviation:\', np.std(accuracies))\n            print_confusion_matrix(np.add.reduce(confusionMatrices))\n        ###\n\n    def accuracy(self, predictions, labels, need_confusion_matrix=False):\n        """"""\n        \xe8\xae\xa1\xe7\xae\x97\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87\xe4\xb8\x8e\xe5\x8f\xac\xe5\x9b\x9e\xe7\x8e\x87\n        @return: accuracy and confusionMatrix as a tuple\n        """"""\n        _predictions = np.argmax(predictions, 1)\n        _labels = np.argmax(labels, 1)\n        cm = confusion_matrix(_labels, _predictions) if need_confusion_matrix else None\n        # == is overloaded for numpy array\n        accuracy = (100.0 * np.sum(_predictions == _labels) / predictions.shape[0])\n        return accuracy, cm\n\n\nif __name__ == \'__main__\':\n    net = Network(num_hidden=128, batch_size=100)\n    net.define_graph()\n    net.run()\n'"
Season1_Tensorflow1.1_Python3.5/7-9/load.py,0,"b'# encoding:utf-8\n# Python2 \xe5\x85\xbc\xe5\xae\xb9\nfrom __future__ import print_function, division\nfrom scipy.io import loadmat as load\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef reformat(samples, labels):\n    # \xe6\x94\xb9\xe5\x8f\x98\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\n    #  0       1       2      3          3       0       1      2\n    # (\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0) -> (\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0)\n    new = np.transpose(samples, (3, 0, 1, 2)).astype(np.float32)\n\n    # labels \xe5\x8f\x98\xe6\x88\x90 one-hot encoding, [2] -> [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n    # digit 0 , represented as 10\n    # labels \xe5\x8f\x98\xe6\x88\x90 one-hot encoding, [10] -> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n    labels = np.array([x[0] for x in labels])  # slow code, whatever\n    one_hot_labels = []\n    for num in labels:\n        one_hot = [0.0] * 10\n        if num == 10:\n            one_hot[0] = 1.0\n        else:\n            one_hot[num] = 1.0\n        one_hot_labels.append(one_hot)\n    labels = np.array(one_hot_labels).astype(np.float32)\n    return new, labels\n\n\ndef normalize(samples):\n    """"""\n    \xe5\xb9\xb6\xe4\xb8\x94\xe7\x81\xb0\xe5\xba\xa6\xe5\x8c\x96: \xe4\xbb\x8e\xe4\xb8\x89\xe8\x89\xb2\xe9\x80\x9a\xe9\x81\x93 -> \xe5\x8d\x95\xe8\x89\xb2\xe9\x80\x9a\xe9\x81\x93     \xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98 + \xe5\x8a\xa0\xe5\xbf\xab\xe8\xae\xad\xe7\xbb\x83\xe9\x80\x9f\xe5\xba\xa6\n    (R + G + B) / 3\n    \xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe4\xbb\x8e 0 ~ 255 \xe7\xba\xbf\xe6\x80\xa7\xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0 -1.0 ~ +1.0\n    @samples: numpy array\n    """"""\n    a = np.add.reduce(samples, keepdims=True, axis=3)  # shape (\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0)\n    a = a / 3.0\n    return a / 128.0 - 1.0\n\n\ndef distribution(labels, name):\n    # \xe6\x9f\xa5\xe7\x9c\x8b\xe4\xb8\x80\xe4\xb8\x8b\xe6\xaf\x8f\xe4\xb8\xaalabel\xe7\x9a\x84\xe5\x88\x86\xe5\xb8\x83\xef\xbc\x8c\xe5\x86\x8d\xe7\x94\xbb\xe4\xb8\xaa\xe7\xbb\x9f\xe8\xae\xa1\xe5\x9b\xbe\n    # keys:\n    # 0\n    # 1\n    # 2\n    # ...\n    # 9\n    count = {}\n    for label in labels:\n        key = 0 if label[0] == 10 else label[0]\n        if key in count:\n            count[key] += 1\n        else:\n            count[key] = 1\n    x = []\n    y = []\n    for k, v in count.items():\n        # print(k, v)\n        x.append(k)\n        y.append(v)\n\n    y_pos = np.arange(len(x))\n    plt.bar(y_pos, y, align=\'center\', alpha=0.5)\n    plt.xticks(y_pos, x)\n    plt.ylabel(\'Count\')\n    plt.title(name + \' Label Distribution\')\n    plt.show()\n\n\ndef inspect(dataset, labels, i):\n    # \xe6\x98\xbe\xe7\xa4\xba\xe5\x9b\xbe\xe7\x89\x87\xe7\x9c\x8b\xe7\x9c\x8b\n    if dataset.shape[3] == 1:\n        shape = dataset.shape\n        dataset = dataset.reshape(shape[0], shape[1], shape[2])\n    print(labels[i])\n    plt.imshow(dataset[i])\n    plt.show()\n\n\ntrain = load(\'../data/train_32x32.mat\')\ntest = load(\'../data/test_32x32.mat\')\n# extra = load(\'../data/extra_32x32.mat\')\n\n# print(\'Train Samples Shape:\', train[\'X\'].shape)\n# print(\'Train  Labels Shape:\', train[\'y\'].shape)\n\n# print(\'Train Samples Shape:\', test[\'X\'].shape)\n# print(\'Train  Labels Shape:\', test[\'y\'].shape)\n\n# print(\'Train Samples Shape:\', extra[\'X\'].shape)\n# print(\'Train  Labels Shape:\', extra[\'y\'].shape)\n\ntrain_samples = train[\'X\']\ntrain_labels = train[\'y\']\ntest_samples = test[\'X\']\ntest_labels = test[\'y\']\n# extra_samples = extra[\'X\']\n# extra_labels = extra[\'y\']\n\nn_train_samples, _train_labels = reformat(train_samples, train_labels)\nn_test_samples, _test_labels = reformat(test_samples, test_labels)\n\n_train_samples = normalize(n_train_samples)\n_test_samples = normalize(n_test_samples)\n\nnum_labels = 10\nimage_size = 32\nnum_channels = 1\n\nif __name__ == \'__main__\':\n    # \xe6\x8e\xa2\xe7\xb4\xa2\xe6\x95\xb0\xe6\x8d\xae\n    pass\n# inspect(_train_samples, _train_labels, 1234)\n# _train_samples = normalize(_train_samples)\n# inspect(_train_samples, _train_labels, 1234)\n# distribution(train_labels, \'Train Labels\')\n# distribution(test_labels, \'Test Labels\')\n'"
Season2/1 Word2Vec/word2vec_tf.py,27,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport math\nimport os\nimport random\nimport zipfile\n\nimport numpy as np\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\nfrom tensorflow.contrib.tensorboard.plugins import projector\n\nfrom pandas import DataFrame\n\n# Step 1: Download the data.\nurl = \'http://mattmahoney.net/dc/\'\n\n\ndef maybe_download(filename, expected_bytes):\n  """"""Download a file if not present, and make sure it\'s the right size.""""""\n  if not os.path.exists(filename):\n    filename, _ = urllib.request.urlretrieve(url + filename, filename)\n  statinfo = os.stat(filename)\n  if statinfo.st_size == expected_bytes:\n    print(\'Found and verified\', filename)\n  else:\n    print(statinfo.st_size)\n    raise Exception(\n        \'Failed to verify \' + filename + \'. Can you get to it with a browser?\')\n  return filename\n\nfilename = maybe_download(\'text8.zip\', 31344016)\n\n\n# Read the data into a list of strings.\ndef read_data(filename):\n  """"""Extract the first file enclosed in a zip file as a list of words""""""\n  with zipfile.ZipFile(filename) as f:\n    data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n  return data\n\nwords = read_data(filename)\nprint(\'Data size\', len(words))\n\n# Step 2: Build the dictionary and replace rare words with UNK token.\nvocabulary_size = 50000\n\n\ndef build_dataset(words, vocabulary_size):\n  count = [[\'UNK\', -1]]\n  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n  dictionary = dict()\n  for word, _ in count:\n    dictionary[word] = len(dictionary)\n  data = list()\n  unk_count = 0\n  for word in words:\n    if word in dictionary:\n      index = dictionary[word]\n    else:\n      index = 0  # dictionary[\'UNK\']\n      unk_count += 1\n    data.append(index)\n  count[0][1] = unk_count\n  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n  return data, count, dictionary, reverse_dictionary\n\ndata, count, dictionary, reverse_dictionary = build_dataset(words, vocabulary_size)\ndel words  # Hint to reduce memory.\nprint(\'Most common words (+UNK)\', count[:5])\nprint(\'Sample data\', data[:10], [reverse_dictionary[i] for i in data[:10]])\n\ndata_index = 0\n\n\n# Step 3: Function to generate a training batch for the skip-gram model.\ndef generate_batch(batch_size, num_skips, skip_window):\n  global data_index\n  assert batch_size % num_skips == 0\n  assert num_skips <= 2 * skip_window\n  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n  span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n  buffer = collections.deque(maxlen=span)\n  for _ in range(span):\n    buffer.append(data[data_index])\n    data_index = (data_index + 1) % len(data)\n  for i in range(batch_size // num_skips):\n    target = skip_window  # target label at the center of the buffer\n    targets_to_avoid = [skip_window]\n    for j in range(num_skips):\n      while target in targets_to_avoid:\n        target = random.randint(0, span - 1)\n      targets_to_avoid.append(target)\n      batch[i * num_skips + j] = buffer[skip_window]\n      labels[i * num_skips + j, 0] = buffer[target]\n    buffer.append(data[data_index])\n    data_index = (data_index + 1) % len(data)\n  # Backtrack a little bit to avoid skipping words in the end of a batch\n  data_index = (data_index + len(data) - span) % len(data)\n  return batch, labels\n\nbatch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\nfor i in range(8):\n  print(batch[i], reverse_dictionary[batch[i]], \'->\', labels[i, 0], reverse_dictionary[labels[i, 0]])\n\n\nif __name__ == ""__main__"":\n  # Step 4: Build and train a skip-gram model.\n  batch_size = 128\n  embedding_size = 128  # Dimension of the embedding vector.\n  skip_window = 1       # How many words to consider left and right.\n  num_skips = 2         # How many times to reuse an input to generate a label.\n\n  # We pick a random validation set to sample nearest neighbors. Here we limit the\n  # validation samples to the words that have a low numeric ID, which by\n  # construction are also the most frequent.\n  valid_size = 16     # Random set of words to evaluate similarity on.\n  valid_window = 100  # Only pick dev samples in the head of the distribution.\n  valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n  num_sampled = 64    # Number of negative examples to sample.\n\n\n  graph = tf.Graph()\n\n  with graph.as_default():\n\n    # Input data.\n    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n\n    # Ops and variables pinned to the CPU because of missing GPU implementation\n    with tf.device(\'/cpu:0\'):\n      # Look up embeddings for inputs.\n      embeddings = tf.Variable(\n          tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n      embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n\n      # Construct the variables for the NCE loss\n      nce_weights = tf.Variable(\n          tf.truncated_normal([vocabulary_size, embedding_size],\n                              stddev=1.0 / math.sqrt(embedding_size)))\n      nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n\n    # Compute the average NCE loss for the batch.\n    # tf.nce_loss automatically draws a new sample of the negative labels each\n    # time we evaluate the loss.\n    loss = tf.reduce_mean(\n        tf.nn.nce_loss(weights=nce_weights,\n                       biases=nce_biases,\n                       labels=train_labels,\n                       inputs=embed,\n                       num_sampled=num_sampled,\n                       num_classes=vocabulary_size))\n\n    # Construct the SGD optimizer using a learning rate of 1.0.\n    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n\n    # Compute the cosine similarity between minibatch examples and all embeddings.\n    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n    normalized_embeddings = embeddings / norm\n    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n\n    # Add variable initializer.\n    tf_major_ver = int(tf.__version__.split(""."")[0])\n    tf_minor_ver = int(tf.__version__.split(""."")[1])\n    if(tf_major_ver==0 and tf_minor_ver<12):\n      init = tf.initialize_all_variables()\n    else:\n      init = tf.global_variables_initializer()\n\n  # Step 5: Begin training.\n  num_steps = 100001\n  LOG_DIR = \'./log/\'\n\n  with tf.Session(graph=graph) as session:\n    # We must initialize all variables before we use them.\n    init.run()\n    print(""Initialized"")\n\n    average_loss = 0\n    for step in xrange(num_steps):\n      batch_inputs, batch_labels = generate_batch(\n          batch_size, num_skips, skip_window)\n      feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n\n      # We perform one update step by evaluating the optimizer op (including it\n      # in the list of returned values for session.run()\n      _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n      average_loss += loss_val\n\n      if step % 2000 == 0:\n        if step > 0:\n          average_loss /= 2000\n        # The average loss is an estimate of the loss over the last 2000 batches.\n        print(""Average loss at step "", step, "": "", average_loss)\n        average_loss = 0\n\n    """"""\n    Use TensorBoard to visualize our model. \n    This is not included in the TensorFlow website tutorial.\n    """"""\n    words_to_visualize = 3000\n    final_embeddings = normalized_embeddings.eval()[:words_to_visualize]\n    embedding_var = tf.Variable(final_embeddings)\n    session.run(embedding_var.initializer)\n    saver = tf.train.Saver([embedding_var])\n    saver.save(session, os.path.join(LOG_DIR, ""model.ckpt""), 0)\n\n    # Format: tensorflow/contrib/tensorboard/plugins/projector/projector_config.proto\n    config = projector.ProjectorConfig()\n\n    # You can add multiple embeddings. Here we add only one.\n    embedding = config.embeddings.add()\n    embedding.tensor_name = embedding_var.name\n    # Link this tensor to its metadata file (e.g. labels).\n    embedding.metadata_path = os.path.join(LOG_DIR, \'metadata.tsv\')\n\n    # Use the same LOG_DIR where you stored your checkpoint.\n    summary_writer = tf.summary.FileWriter(LOG_DIR)\n    summary_writer.add_graph(graph)\n\n    # The next line writes a projector_config.pbtxt in the LOG_DIR. TensorBoard will\n    # read this file during startup.\n    projector.visualize_embeddings(summary_writer, config)\n\n    labels = [(reverse_dictionary[i], i) for i in range(words_to_visualize)]\n    DataFrame(labels, columns=[\'word\', \'freq_rank\']).to_csv(\'log/metadata.tsv\', index=False, sep=\'\\t\')\n\n'"
