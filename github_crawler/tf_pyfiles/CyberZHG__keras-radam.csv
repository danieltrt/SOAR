file_path,api_count,code
setup.py,0,"b'import os\nimport re\nimport codecs\nfrom setuptools import setup, find_packages\n\ncurrent_path = os.path.abspath(os.path.dirname(__file__))\n\n\ndef read_file(*parts):\n    with codecs.open(os.path.join(current_path, *parts), \'r\', \'utf8\') as reader:\n        return reader.read()\n\n\ndef get_requirements(*parts):\n    with codecs.open(os.path.join(current_path, *parts), \'r\', \'utf8\') as reader:\n        return list(map(lambda x: x.strip(), reader.readlines()))\n\n\ndef find_version(*file_paths):\n    version_file = read_file(*file_paths)\n    version_match = re.search(r""^__version__ = [\'\\""]([^\'\\""]*)[\'\\""]"", version_file, re.M)\n    if version_match:\n        return version_match.group(1)\n    raise RuntimeError(\'Unable to find version string.\')\n\n\nsetup(\n    name=\'keras-rectified-adam\',\n    version=find_version(\'keras_radam\', \'__init__.py\'),\n    packages=find_packages(),\n    url=\'https://github.com/CyberZHG/keras-radam\',\n    license=\'MIT\',\n    author=\'CyberZHG\',\n    author_email=\'CyberZHG@users.noreply.github.com\',\n    description=\'RAdam implemented in Keras & TensorFlow\',\n    long_description=read_file(\'README.md\'),\n    long_description_content_type=\'text/markdown\',\n    install_requires=get_requirements(\'requirements.txt\'),\n    classifiers=[\n        ""Programming Language :: Python :: 3.6"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Operating System :: OS Independent"",\n    ],\n)\n'"
keras_radam/__init__.py,0,"b""from .selection import *\n\n__version__ = '0.17.0'\n"""
keras_radam/backend.py,2,"b""import os\n\nfrom distutils.util import strtobool\n\n__all__ = [\n    'keras', 'utils', 'activations', 'applications', 'backend', 'datasets', 'engine',\n    'layers', 'preprocessing', 'wrappers', 'callbacks', 'constraints', 'initializers',\n    'metrics', 'models', 'losses', 'optimizers', 'regularizers', 'TF_KERAS', 'EAGER_MODE'\n]\n\nTF_KERAS = False\nEAGER_MODE = False\n\nif strtobool(os.environ.get('TF_KERAS', '0')):\n    import tensorflow as tf\n    from tensorflow.python import keras\n    TF_KERAS = True\n    if strtobool(os.environ.get('TF_EAGER', '0')):\n        try:\n            tf.enable_eager_execution()\n            raise AttributeError()\n        except AttributeError as e:\n            pass\n    EAGER_MODE = tf.executing_eagerly()\nelse:\n    import keras\n\nutils = keras.utils\nactivations = keras.activations\napplications = keras.applications\nbackend = keras.backend\ndatasets = keras.datasets\nengine = keras.engine\nlayers = keras.layers\npreprocessing = keras.preprocessing\nwrappers = keras.wrappers\ncallbacks = keras.callbacks\nconstraints = keras.constraints\ninitializers = keras.initializers\nmetrics = keras.metrics\nmodels = keras.models\nlosses = keras.losses\noptimizers = keras.optimizers\nregularizers = keras.regularizers\n"""
keras_radam/optimizer_v2.py,5,"b'import tensorflow as tf\nfrom tensorflow.python.keras.optimizer_v2.optimizer_v2 import OptimizerV2\nfrom tensorflow.python import ops, math_ops, state_ops, control_flow_ops\nfrom tensorflow.python.keras import backend as K\n\n__all__ = [\'RAdam\']\n\n\nclass RAdam(OptimizerV2):\n    """"""RAdam optimizer.\n\n    According to the paper\n    [On The Variance Of The Adaptive Learning Rate And Beyond](https://arxiv.org/pdf/1908.03265v1.pdf).\n    """"""\n\n    def __init__(self,\n                 learning_rate=0.001,\n                 beta_1=0.9,\n                 beta_2=0.999,\n                 epsilon=1e-7,\n                 weight_decay=0.,\n                 amsgrad=False,\n                 total_steps=0,\n                 warmup_proportion=0.1,\n                 min_lr=0.,\n                 name=\'RAdam\',\n                 **kwargs):\n        r""""""Construct a new Adam optimizer.\n\n        Args:\n            learning_rate: A Tensor or a floating point value.    The learning rate.\n            beta_1: A float value or a constant float tensor. The exponential decay\n                rate for the 1st moment estimates.\n            beta_2: A float value or a constant float tensor. The exponential decay\n                rate for the 2nd moment estimates.\n            epsilon: A small constant for numerical stability. This epsilon is\n                ""epsilon hat"" in the Kingma and Ba paper (in the formula just before\n                Section 2.1), not the epsilon in Algorithm 1 of the paper.\n            weight_decay: A floating point value. Weight decay for each param.\n            amsgrad: boolean. Whether to apply AMSGrad variant of this algorithm from\n                the paper ""On the Convergence of Adam and beyond"".\n            total_steps: An integer. Total number of training steps.\n                Enable warmup by setting a positive value.\n            warmup_proportion: A floating point value. The proportion of increasing steps.\n            min_lr: A floating point value. Minimum learning rate after warmup.\n            name: Optional name for the operations created when applying gradients.\n                Defaults to ""Adam"".    @compatibility(eager) When eager execution is\n                enabled, `learning_rate`, `beta_1`, `beta_2`, and `epsilon` can each be\n                a callable that takes no arguments and returns the actual value to use.\n                This can be useful for changing these values across different\n                invocations of optimizer functions. @end_compatibility\n            **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\n                `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip\n                gradients by value, `decay` is included for backward compatibility to\n                allow time inverse decay of learning rate. `lr` is included for backward\n                compatibility, recommended to use `learning_rate` instead.\n        """"""\n\n        super(RAdam, self).__init__(name, **kwargs)\n        self._set_hyper(\'learning_rate\', kwargs.get(\'lr\', learning_rate))\n        self._set_hyper(\'beta_1\', beta_1)\n        self._set_hyper(\'beta_2\', beta_2)\n        self._set_hyper(\'decay\', self._initial_decay)\n        self._set_hyper(\'weight_decay\', weight_decay)\n        self._set_hyper(\'total_steps\', float(total_steps))\n        self._set_hyper(\'warmup_proportion\', warmup_proportion)\n        self._set_hyper(\'min_lr\', min_lr)\n        self.epsilon = epsilon or K.epsilon()\n        self.amsgrad = amsgrad\n        self._initial_weight_decay = weight_decay\n        self._initial_total_steps = total_steps\n\n    def _create_slots(self, var_list):\n        for var in var_list:\n            self.add_slot(var, \'m\')\n        for var in var_list:\n            self.add_slot(var, \'v\')\n        if self.amsgrad:\n            for var in var_list:\n                self.add_slot(var, \'vhat\')\n\n    def set_weights(self, weights):\n        params = self.weights\n        num_vars = int((len(params) - 1) / 2)\n        if len(weights) == 3 * num_vars + 1:\n            weights = weights[:len(params)]\n        super(RAdam, self).set_weights(weights)\n\n    def _resource_apply_dense(self, grad, var):\n        var_dtype = var.dtype.base_dtype\n        lr_t = self._decayed_lr(var_dtype)\n        m = self.get_slot(var, \'m\')\n        v = self.get_slot(var, \'v\')\n        beta_1_t = self._get_hyper(\'beta_1\', var_dtype)\n        beta_2_t = self._get_hyper(\'beta_2\', var_dtype)\n        epsilon_t = ops.convert_to_tensor(self.epsilon, var_dtype)\n        local_step = math_ops.cast(self.iterations + 1, var_dtype)\n        beta_1_power = math_ops.pow(beta_1_t, local_step)\n        beta_2_power = math_ops.pow(beta_2_t, local_step)\n\n        if self._initial_total_steps > 0:\n            total_steps = self._get_hyper(\'total_steps\', var_dtype)\n            warmup_steps = total_steps * self._get_hyper(\'warmup_proportion\', var_dtype)\n            min_lr = self._get_hyper(\'min_lr\', var_dtype)\n            decay_steps = K.maximum(total_steps - warmup_steps, 1)\n            decay_rate = (min_lr - lr_t) / decay_steps\n            lr_t = tf.where(\n                local_step <= warmup_steps,\n                lr_t * (local_step / warmup_steps),\n                lr_t + decay_rate * K.minimum(local_step - warmup_steps, decay_steps),\n            )\n\n        sma_inf = 2.0 / (1.0 - beta_2_t) - 1.0\n        sma_t = sma_inf - 2.0 * local_step * beta_2_power / (1.0 - beta_2_power)\n\n        m_t = state_ops.assign(m,\n                               beta_1_t * m + (1.0 - beta_1_t) * grad,\n                               use_locking=self._use_locking)\n        m_corr_t = m_t / (1.0 - beta_1_power)\n\n        v_t = state_ops.assign(v,\n                               beta_2_t * v + (1.0 - beta_2_t) * math_ops.square(grad),\n                               use_locking=self._use_locking)\n        if self.amsgrad:\n            vhat = self.get_slot(var, \'vhat\')\n            vhat_t = state_ops.assign(vhat,\n                                      math_ops.maximum(vhat, v_t),\n                                      use_locking=self._use_locking)\n            v_corr_t = math_ops.sqrt(vhat_t / (1.0 - beta_2_power))\n        else:\n            vhat_t = None\n            v_corr_t = math_ops.sqrt(v_t / (1.0 - beta_2_power))\n\n        r_t = math_ops.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n                            (sma_t - 2.0) / (sma_inf - 2.0) *\n                            sma_inf / sma_t)\n\n        var_t = tf.where(sma_t >= 5.0, r_t * m_corr_t / (v_corr_t + epsilon_t), m_corr_t)\n\n        if self._initial_weight_decay > 0.0:\n            var_t += self._get_hyper(\'weight_decay\', var_dtype) * var\n\n        var_update = state_ops.assign_sub(var,\n                                          lr_t * var_t,\n                                          use_locking=self._use_locking)\n\n        updates = [var_update, m_t, v_t]\n        if self.amsgrad:\n            updates.append(vhat_t)\n        return control_flow_ops.group(*updates)\n\n    def _resource_apply_sparse(self, grad, var, indices):\n        var_dtype = var.dtype.base_dtype\n        lr_t = self._decayed_lr(var_dtype)\n        beta_1_t = self._get_hyper(\'beta_1\', var_dtype)\n        beta_2_t = self._get_hyper(\'beta_2\', var_dtype)\n        epsilon_t = ops.convert_to_tensor(self.epsilon, var_dtype)\n        local_step = math_ops.cast(self.iterations + 1, var_dtype)\n        beta_1_power = math_ops.pow(beta_1_t, local_step)\n        beta_2_power = math_ops.pow(beta_2_t, local_step)\n\n        if self._initial_total_steps > 0:\n            total_steps = self._get_hyper(\'total_steps\', var_dtype)\n            warmup_steps = total_steps * self._get_hyper(\'warmup_proportion\', var_dtype)\n            min_lr = self._get_hyper(\'min_lr\', var_dtype)\n            decay_steps = K.maximum(total_steps - warmup_steps, 1)\n            decay_rate = (min_lr - lr_t) / decay_steps\n            lr_t = tf.where(\n                local_step <= warmup_steps,\n                lr_t * (local_step / warmup_steps),\n                lr_t + decay_rate * K.minimum(local_step - warmup_steps, decay_steps),\n            )\n\n        sma_inf = 2.0 / (1.0 - beta_2_t) - 1.0\n        sma_t = sma_inf - 2.0 * local_step * beta_2_power / (1.0 - beta_2_power)\n\n        m = self.get_slot(var, \'m\')\n        m_scaled_g_values = grad * (1 - beta_1_t)\n        m_t = state_ops.assign(m, m * beta_1_t, use_locking=self._use_locking)\n        with ops.control_dependencies([m_t]):\n            m_t = self._resource_scatter_add(m, indices, m_scaled_g_values)\n        m_corr_t = m_t / (1.0 - beta_1_power)\n\n        v = self.get_slot(var, \'v\')\n        v_scaled_g_values = (grad * grad) * (1 - beta_2_t)\n        v_t = state_ops.assign(v, v * beta_2_t, use_locking=self._use_locking)\n        with ops.control_dependencies([v_t]):\n            v_t = self._resource_scatter_add(v, indices, v_scaled_g_values)\n\n        if self.amsgrad:\n            vhat = self.get_slot(var, \'vhat\')\n            vhat_t = state_ops.assign(vhat,\n                                      math_ops.maximum(vhat, v_t),\n                                      use_locking=self._use_locking)\n            v_corr_t = math_ops.sqrt(vhat_t / (1.0 - beta_2_power))\n        else:\n            vhat_t = None\n            v_corr_t = math_ops.sqrt(v_t / (1.0 - beta_2_power))\n\n        r_t = math_ops.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n                            (sma_t - 2.0) / (sma_inf - 2.0) *\n                            sma_inf / sma_t)\n\n        var_t = tf.where(sma_t >= 5.0, r_t * m_corr_t / (v_corr_t + epsilon_t), m_corr_t)\n\n        if self._initial_weight_decay > 0.0:\n            var_t += self._get_hyper(\'weight_decay\', var_dtype) * var\n\n        var_update = self._resource_scatter_add(var, indices, tf.gather(-lr_t * var_t, indices))\n\n        updates = [var_update, m_t, v_t]\n        if self.amsgrad:\n            updates.append(vhat_t)\n        return control_flow_ops.group(*updates)\n\n    def get_config(self):\n        config = super(RAdam, self).get_config()\n        config.update({\n            \'learning_rate\': self._serialize_hyperparameter(\'learning_rate\'),\n            \'beta_1\': self._serialize_hyperparameter(\'beta_1\'),\n            \'beta_2\': self._serialize_hyperparameter(\'beta_2\'),\n            \'decay\': self._serialize_hyperparameter(\'decay\'),\n            \'weight_decay\': self._serialize_hyperparameter(\'weight_decay\'),\n            \'epsilon\': self.epsilon,\n            \'amsgrad\': self.amsgrad,\n            \'total_steps\': self._serialize_hyperparameter(\'total_steps\'),\n            \'warmup_proportion\': self._serialize_hyperparameter(\'warmup_proportion\'),\n            \'min_lr\': self._serialize_hyperparameter(\'min_lr\'),\n        })\n        return config\n'"
keras_radam/optimizers.py,0,"b'from .backend import keras\nfrom .backend import backend as K\n\n\n__all__ = [\'RAdam\']\n\n\nclass RAdam(keras.optimizers.Optimizer):\n    """"""RAdam optimizer.\n\n    # Arguments\n        learning_rate: float >= 0. Learning rate.\n        beta_1: float, 0 < beta < 1. Generally close to 1.\n        beta_2: float, 0 < beta < 1. Generally close to 1.\n        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n        decay: float >= 0. Learning rate decay over each update.\n        weight_decay: float >= 0. Weight decay for each param.\n        amsgrad: boolean. Whether to apply the AMSGrad variant of this\n            algorithm from the paper ""On the Convergence of Adam and\n            Beyond"".\n        total_steps: int >= 0. Total number of training steps. Enable warmup by setting a positive value.\n        warmup_proportion: 0 < warmup_proportion < 1. The proportion of increasing steps.\n        min_lr: float >= 0. Minimum learning rate after warmup.\n    # References\n        - [Adam - A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980v8)\n        - [On the Convergence of Adam and Beyond](https://openreview.net/forum?id=ryQu7f-RZ)\n        - [On The Variance Of The Adaptive Learning Rate And Beyond](https://arxiv.org/pdf/1908.03265v1.pdf)\n    """"""\n\n    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999,\n                 epsilon=None, decay=0., weight_decay=0., amsgrad=False,\n                 total_steps=0, warmup_proportion=0.1, min_lr=0., **kwargs):\n        learning_rate = kwargs.pop(\'lr\', learning_rate)\n        super(RAdam, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype=\'int64\', name=\'iterations\')\n            self.learning_rate = K.variable(learning_rate, name=\'learning_rate\')\n            self.beta_1 = K.variable(beta_1, name=\'beta_1\')\n            self.beta_2 = K.variable(beta_2, name=\'beta_2\')\n            self.decay = K.variable(decay, name=\'decay\')\n            self.weight_decay = K.variable(weight_decay, name=\'weight_decay\')\n            self.total_steps = K.variable(total_steps, name=\'total_steps\')\n            self.warmup_proportion = K.variable(warmup_proportion, name=\'warmup_proportion\')\n            self.min_lr = K.variable(min_lr, name=\'min_lr\')\n        if epsilon is None:\n            epsilon = K.epsilon()\n        self.epsilon = epsilon\n        self.initial_decay = decay\n        self.initial_weight_decay = weight_decay\n        self.initial_total_steps = total_steps\n        self.amsgrad = amsgrad\n\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n\n        if self.initial_decay > 0:\n            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations, K.dtype(self.decay))))\n\n        t = K.cast(self.iterations, K.floatx()) + 1\n\n        if self.initial_total_steps > 0:\n            warmup_steps = self.total_steps * self.warmup_proportion\n            decay_steps = K.maximum(self.total_steps - warmup_steps, 1)\n            decay_rate = (self.min_lr - lr) / decay_steps\n            lr = K.switch(\n                t <= warmup_steps,\n                lr * (t / warmup_steps),\n                lr + decay_rate * K.minimum(t - warmup_steps, decay_steps),\n            )\n\n        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name=\'m_\' + str(i)) for (i, p) in enumerate(params)]\n        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name=\'v_\' + str(i)) for (i, p) in enumerate(params)]\n\n        if self.amsgrad:\n            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name=\'vhat_\' + str(i)) for (i, p) in enumerate(params)]\n        else:\n            vhats = [K.zeros(1, name=\'vhat_\' + str(i)) for i in range(len(params))]\n\n        self.weights = [self.iterations] + ms + vs + vhats\n\n        beta_1_t = K.pow(self.beta_1, t)\n        beta_2_t = K.pow(self.beta_2, t)\n\n        sma_inf = 2.0 / (1.0 - self.beta_2) - 1.0\n        sma_t = sma_inf - 2.0 * t * beta_2_t / (1.0 - beta_2_t)\n\n        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n\n            m_corr_t = m_t / (1.0 - beta_1_t)\n            if self.amsgrad:\n                vhat_t = K.maximum(vhat, v_t)\n                v_corr_t = K.sqrt(vhat_t / (1.0 - beta_2_t))\n                self.updates.append(K.update(vhat, vhat_t))\n            else:\n                v_corr_t = K.sqrt(v_t / (1.0 - beta_2_t))\n\n            r_t = K.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n                         (sma_t - 2.0) / (sma_inf - 2.0) *\n                         sma_inf / sma_t)\n\n            p_t = K.switch(sma_t >= 5, r_t * m_corr_t / (v_corr_t + self.epsilon), m_corr_t)\n\n            if self.initial_weight_decay > 0:\n                p_t += self.weight_decay * p\n\n            p_t = p - lr * p_t\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, \'constraint\', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    @property\n    def lr(self):\n        return self.learning_rate\n\n    @lr.setter\n    def lr(self, learning_rate):\n        self.learning_rate = learning_rate\n\n    def get_config(self):\n        config = {\n            \'learning_rate\': float(K.get_value(self.learning_rate)),\n            \'beta_1\': float(K.get_value(self.beta_1)),\n            \'beta_2\': float(K.get_value(self.beta_2)),\n            \'decay\': float(K.get_value(self.decay)),\n            \'weight_decay\': float(K.get_value(self.weight_decay)),\n            \'epsilon\': self.epsilon,\n            \'amsgrad\': self.amsgrad,\n            \'total_steps\': float(K.get_value(self.total_steps)),\n            \'warmup_proportion\': float(K.get_value(self.warmup_proportion)),\n            \'min_lr\': float(K.get_value(self.min_lr)),\n        }\n        base_config = super(RAdam, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n'"
keras_radam/selection.py,0,"b""from .backend import TF_KERAS\n\n__all__ = ['RAdam']\n\n\nif TF_KERAS:\n    from .optimizer_v2 import RAdam\nelse:\n    from .optimizers import RAdam\n"""
keras_radam/training.py,4,"b'import tensorflow as tf\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops, state_ops, array_ops\nfrom tensorflow.python.ops import resource_variable_ops\nfrom tensorflow.python.training import optimizer\n\n\n__all__ = [\'RAdamOptimizer\']\n\n\nclass RAdamOptimizer(optimizer.Optimizer):\n    """"""RAdam optimizer.\n\n    According to the paper\n    [On The Variance Of The Adaptive Learning Rate And Beyond](https://arxiv.org/pdf/1908.03265v1.pdf).\n    """"""\n\n    def __init__(self,\n                 learning_rate=0.001,\n                 beta1=0.9,\n                 beta2=0.999,\n                 epsilon=1e-7,\n                 weight_decay=0.,\n                 amsgrad=False,\n                 total_steps=0,\n                 warmup_proportion=0.1,\n                 min_lr=0.,\n                 use_locking=False,\n                 name=""RAdam""):\n        r""""""Construct a new Adam optimizer.\n\n        Args:\n            learning_rate: A Tensor or a floating point value.    The learning rate.\n            beta1: A float value or a constant float tensor. The exponential decay\n                rate for the 1st moment estimates.\n            beta2: A float value or a constant float tensor. The exponential decay\n                rate for the 2nd moment estimates.\n            epsilon: A small constant for numerical stability. This epsilon is\n                ""epsilon hat"" in the Kingma and Ba paper (in the formula just before\n                Section 2.1), not the epsilon in Algorithm 1 of the paper.\n            weight_decay: A floating point value. Weight decay for each param.\n            amsgrad: boolean. Whether to apply AMSGrad variant of this algorithm from\n                the paper ""On the Convergence of Adam and beyond"".\n            total_steps: An integer. Total number of training steps.\n                Enable warmup by setting a positive value.\n            warmup_proportion: A floating point value. The proportion of increasing steps.\n            min_lr: A floating point value. Minimum learning rate after warmup.\n            name: Optional name for the operations created when applying gradients.\n                Defaults to ""Adam"".    @compatibility(eager) When eager execution is\n                enabled, `learning_rate`, `beta_1`, `beta_2`, and `epsilon` can each be\n                a callable that takes no arguments and returns the actual value to use.\n                This can be useful for changing these values across different\n                invocations of optimizer functions. @end_compatibility\n            **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\n                `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip\n                gradients by value, `decay` is included for backward compatibility to\n                allow time inverse decay of learning rate. `lr` is included for backward\n                compatibility, recommended to use `learning_rate` instead.\n        """"""\n        super(RAdamOptimizer, self).__init__(use_locking, name)\n        self._lr = learning_rate\n        self._beta1 = beta1\n        self._beta2 = beta2\n        self._epsilon = epsilon\n        self._weight_decay = weight_decay\n        self._amsgrad = amsgrad\n        self._total_steps = float(total_steps)\n        self._warmup_proportion = warmup_proportion\n        self._min_lr = min_lr\n        self._initial_weight_decay = weight_decay\n        self._initial_total_steps = total_steps\n\n        self._lr_t = None\n        self._step_t = None\n        self._beta1_t = None\n        self._beta2_t = None\n        self._epsilon_t = None\n        self._weight_decay_t = None\n        self._total_steps_t = None\n        self._warmup_proportion_t = None\n        self._min_lr_t = None\n\n    def _get_beta_accumulators(self):\n        with ops.init_scope():\n            if context.executing_eagerly():\n                graph = None\n            else:\n                graph = ops.get_default_graph()\n            return (self._get_non_slot_variable(""step"", graph=graph),\n                    self._get_non_slot_variable(""beta1_power"", graph=graph),\n                    self._get_non_slot_variable(""beta2_power"", graph=graph))\n\n    def _create_slots(self, var_list):\n        first_var = min(var_list, key=lambda x: x.name)\n        self._create_non_slot_variable(initial_value=1.0, name=""step"", colocate_with=first_var)\n        self._create_non_slot_variable(initial_value=self._beta1, name=""beta1_power"", colocate_with=first_var)\n        self._create_non_slot_variable(initial_value=self._beta2, name=""beta2_power"", colocate_with=first_var)\n        for v in var_list:\n            self._zeros_slot(v, ""m"", self._name)\n            self._zeros_slot(v, ""v"", self._name)\n            if self._amsgrad:\n                self._zeros_slot(v, ""vhat"", self._name)\n\n    def _prepare(self):\n        lr = self._call_if_callable(self._lr)\n        beta1 = self._call_if_callable(self._beta1)\n        beta2 = self._call_if_callable(self._beta2)\n        epsilon = self._call_if_callable(self._epsilon)\n        weight_decay = self._call_if_callable(self._weight_decay)\n        total_steps = self._call_if_callable(self._total_steps)\n        warmup_proportion = self._call_if_callable(self._warmup_proportion)\n        min_lr = self._call_if_callable(self._min_lr)\n\n        self._lr_t = ops.convert_to_tensor(lr, name=""learning_rate"")\n        self._beta1_t = ops.convert_to_tensor(beta1, name=""beta1"")\n        self._beta2_t = ops.convert_to_tensor(beta2, name=""beta2"")\n        self._epsilon_t = ops.convert_to_tensor(epsilon, name=""epsilon"")\n        self._weight_decay_t = ops.convert_to_tensor(weight_decay, name=""weight_decay"")\n        self._total_steps_t = ops.convert_to_tensor(total_steps, name=""total_steps"")\n        self._warmup_proportion_t = ops.convert_to_tensor(warmup_proportion, name=""warmup_proportion"")\n        self._min_lr_t = ops.convert_to_tensor(min_lr, name=""min_lr"")\n\n    def _apply_dense(self, grad, var):\n        return self._resource_apply_dense(grad, var)\n\n    def _resource_apply_dense(self, grad, var):\n        step, beta1_power, beta2_power = self._get_beta_accumulators()\n        beta1_power = math_ops.cast(beta1_power, var.dtype.base_dtype)\n        beta2_power = math_ops.cast(beta2_power, var.dtype.base_dtype)\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n\n        if self._initial_total_steps > 0:\n            total_steps = math_ops.cast(self._total_steps_t, var.dtype.base_dtype)\n            warmup_proportion = math_ops.cast(self._warmup_proportion_t, var.dtype.base_dtype)\n            min_lr = math_ops.cast(self._min_lr_t, var.dtype.base_dtype)\n            warmup_steps = total_steps * warmup_proportion\n            decay_steps = math_ops.maximum(total_steps - warmup_steps, 1)\n            decay_rate = (min_lr - lr_t) / decay_steps\n            lr_t = tf.where(\n                step <= warmup_steps,\n                lr_t * (step / warmup_steps),\n                lr_t + decay_rate * math_ops.minimum(step - warmup_steps, decay_steps),\n            )\n\n        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n\n        sma_inf = 2.0 / (1.0 - beta2_t) - 1.0\n        sma_t = sma_inf - 2.0 * step * beta2_power / (1.0 - beta2_power)\n\n        m = self.get_slot(var, ""m"")\n        m_t = state_ops.assign(m, beta1_t * m + (1.0 - beta1_t) * grad, use_locking=self._use_locking)\n        m_corr_t = m_t / (1.0 - beta1_power)\n\n        v = self.get_slot(var, ""v"")\n        v_t = state_ops.assign(v, beta2_t * v + (1.0 - beta2_t) * math_ops.square(grad), use_locking=self._use_locking)\n        if self._amsgrad:\n            vhat = self.get_slot(var, \'vhat\')\n            vhat_t = state_ops.assign(vhat, math_ops.maximum(vhat, v_t), use_locking=self._use_locking)\n            v_corr_t = math_ops.sqrt(vhat_t / (1.0 - beta2_power))\n        else:\n            v_corr_t = math_ops.sqrt(v_t / (1.0 - beta2_power))\n\n        r_t = math_ops.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n                            (sma_t - 2.0) / (sma_inf - 2.0) *\n                            sma_inf / sma_t)\n\n        var_t = tf.where(sma_t >= 5.0, r_t * m_corr_t / (v_corr_t + epsilon_t), m_corr_t)\n\n        if self._initial_weight_decay > 0.0:\n            var_t += math_ops.cast(self._weight_decay_t, var.dtype.base_dtype) * var\n\n        var_update = state_ops.assign_sub(var, lr_t * var_t, use_locking=self._use_locking)\n\n        updates = [var_update, m_t, v_t]\n        if self._amsgrad:\n            updates.append(vhat_t)\n        return control_flow_ops.group(*updates)\n\n    def _apply_sparse_shared(self, grad, var, indices, scatter_add):\n        step, beta1_power, beta2_power = self._get_beta_accumulators()\n        beta1_power = math_ops.cast(beta1_power, var.dtype.base_dtype)\n        beta2_power = math_ops.cast(beta2_power, var.dtype.base_dtype)\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n\n        if self._initial_total_steps > 0:\n            total_steps = math_ops.cast(self._total_steps_t, var.dtype.base_dtype)\n            warmup_proportion = math_ops.cast(self._warmup_proportion_t, var.dtype.base_dtype)\n            min_lr = math_ops.cast(self._min_lr_t, var.dtype.base_dtype)\n            warmup_steps = total_steps * warmup_proportion\n            decay_steps = math_ops.maximum(total_steps - warmup_steps, 1)\n            decay_rate = (min_lr - lr_t) / decay_steps\n            lr_t = tf.where(\n                step <= warmup_steps,\n                lr_t * (step / warmup_steps),\n                lr_t + decay_rate * math_ops.minimum(step - warmup_steps, decay_steps),\n            )\n\n        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n\n        sma_inf = 2.0 / (1.0 - beta2_t) - 1.0\n        sma_t = sma_inf - 2.0 * step * beta2_power / (1.0 - beta2_power)\n\n        m = self.get_slot(var, ""m"")\n        m_scaled_g_values = grad * (1 - beta1_t)\n        m_t = state_ops.assign(m, m * beta1_t, use_locking=self._use_locking)\n        with ops.control_dependencies([m_t]):\n            m_t = scatter_add(m, indices, m_scaled_g_values)\n        m_corr_t = m_t / (1.0 - beta1_power)\n\n        v = self.get_slot(var, ""v"")\n        v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n        v_t = state_ops.assign(v, v * beta2_t, use_locking=self._use_locking)\n        with ops.control_dependencies([v_t]):\n            v_t = scatter_add(v, indices, v_scaled_g_values)\n        if self._amsgrad:\n            vhat = self.get_slot(var, \'vhat\')\n            vhat_t = state_ops.assign(vhat, math_ops.maximum(vhat, v_t), use_locking=self._use_locking)\n            v_corr_t = math_ops.sqrt(vhat_t / (1.0 - beta2_power))\n        else:\n            v_corr_t = math_ops.sqrt(v_t / (1.0 - beta2_power))\n\n        r_t = math_ops.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n                            (sma_t - 2.0) / (sma_inf - 2.0) *\n                            sma_inf / sma_t)\n\n        var_t = tf.where(sma_t >= 5.0, r_t * m_corr_t / (v_corr_t + epsilon_t), m_corr_t)\n\n        if self._initial_weight_decay > 0.0:\n            var_t += math_ops.cast(self._weight_decay_t, var.dtype.base_dtype) * var\n\n        var_t = lr_t * var_t\n        var_update = state_ops.scatter_sub(\n                    var,\n                    indices,\n                    array_ops.gather(var_t, indices),\n                    use_locking=self._use_locking)\n\n        updates = [var_update, m_t, v_t]\n        if self._amsgrad:\n            updates.append(vhat_t)\n        return control_flow_ops.group(*updates)\n\n    def _apply_sparse(self, grad, var):\n        return self._apply_sparse_shared(\n            grad.values,\n            var,\n            grad.indices,\n            lambda x, i, v: state_ops.scatter_add(x, i, v, use_locking=self._use_locking))\n\n    def _resource_scatter_add(self, x, i, v):\n        with ops.control_dependencies([resource_variable_ops.resource_scatter_add(x.handle, i, v)]):\n            return x.value()\n\n    def _resource_apply_sparse(self, grad, var, indices):\n        return self._apply_sparse_shared(grad, var, indices, self._resource_scatter_add)\n\n    def _finish(self, update_ops, name_scope):\n        with ops.control_dependencies(update_ops):\n            step, beta1_power, beta2_power = self._get_beta_accumulators()\n            with ops.colocate_with(beta1_power):\n                update_step = step.assign(step + 1.0, use_locking=self._use_locking)\n                update_beta1 = beta1_power.assign(beta1_power * self._beta1_t, use_locking=self._use_locking)\n                update_beta2 = beta2_power.assign(beta2_power * self._beta2_t, use_locking=self._use_locking)\n        return control_flow_ops.group(*update_ops + [update_step, update_beta1, update_beta2], name=name_scope)\n'"
tests/__init__.py,0,b''
tests/official.py,0,"b""import math\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass RAdam(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.buffer = [[None, None, None] for _ in range(10)]\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('RAdam does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state['step'] += 1\n                buffered = self.buffer[int(state['step'] % 10)]\n                if state['step'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it's an approximated value\n                    if N_sma >= 5:\n                        step_size = group['lr'] *\\\n                                    math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma *\n                                              N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n                    else:\n                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n                    buffered[2] = step_size\n\n                if group['weight_decay'] != 0:\n                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n\n                # more conservative since it's an approximated value\n                if N_sma >= 5:\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size, exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss\n"""
tests/test_optimizers.py,7,"b'import os\nimport tempfile\nfrom unittest import TestCase\n\nimport numpy as np\n\nfrom keras_radam.backend import keras, TF_KERAS, EAGER_MODE\nfrom keras_radam import RAdam\n\n\nclass TestRAdam(TestCase):\n\n    @staticmethod\n    def gen_linear_model(optimizer) -> keras.models.Model:\n        model = keras.models.Sequential()\n        model.add(keras.layers.Dense(\n            input_shape=(17,),\n            units=3,\n            bias_constraint=keras.constraints.max_norm(),\n            name=\'Dense\',\n        ))\n        model.compile(optimizer, loss=\'mse\')\n        return model\n\n    @staticmethod\n    def gen_linear_data(w=None) -> (np.ndarray, np.ndarray):\n        np.random.seed(0xcafe)\n        x = np.random.standard_normal((4096 * 30, 17))\n        if w is None:\n            w = np.random.standard_normal((17, 3))\n        y = np.dot(x, w)\n        return x, y, w\n\n    def _test_fit(self, optimizer, atol=1e-2):\n        x, y, w = self.gen_linear_data()\n        model = self.gen_linear_model(optimizer)\n\n        callbacks = [keras.callbacks.EarlyStopping(monitor=\'loss\', patience=3, min_delta=1e-8)]\n        if isinstance(optimizer, RAdam):\n            model_path = os.path.join(tempfile.gettempdir(), \'test_accumulation_%f.h5\' % np.random.random())\n            model.save(model_path)\n            model = keras.models.load_model(model_path, custom_objects={\'RAdam\': RAdam})\n            callbacks.append(keras.callbacks.ReduceLROnPlateau(monitor=\'loss\', min_lr=1e-8, patience=2, verbose=True))\n\n        model.fit(x, y,\n                  epochs=100,\n                  batch_size=32,\n                  callbacks=callbacks)\n\n        model_path = os.path.join(tempfile.gettempdir(), \'test_accumulation_%f.h5\' % np.random.random())\n        model.save(model_path)\n        model = keras.models.load_model(model_path, custom_objects={\'RAdam\': RAdam})\n\n        x, y, w = self.gen_linear_data(w)\n        predicted = model.predict(x)\n        self.assertLess(np.max(np.abs(predicted - y)), atol)\n\n    def test_amsgrad(self):\n        self._test_fit(RAdam(amsgrad=True))\n\n    def test_training_amsgrad(self):\n        if not TF_KERAS:\n            return\n        from keras_radam.training import RAdamOptimizer\n        self._test_fit(RAdamOptimizer(amsgrad=True))\n\n    def test_decay(self):\n        self._test_fit(RAdam(decay=1e-4, weight_decay=1e-6), atol=0.1)\n\n    def test_training_decay(self):\n        if not TF_KERAS:\n            return\n        from keras_radam.training import RAdamOptimizer\n        self._test_fit(RAdamOptimizer(weight_decay=1e-8), atol=0.1)\n\n    def test_warmup(self):\n        self._test_fit(RAdam(total_steps=38400, warmup_proportion=0.1, min_lr=1e-6))\n\n    def test_training_warmup(self):\n        if not TF_KERAS:\n            return\n        from keras_radam.training import RAdamOptimizer\n        self._test_fit(RAdamOptimizer(total_steps=38400, warmup_proportion=0.1, min_lr=1e-6))\n\n    def test_fit_embed(self):\n        optimizers = [RAdam]\n        if TF_KERAS:\n            from keras_radam.training import RAdamOptimizer\n            optimizers.append(RAdamOptimizer)\n        for optimizer in optimizers:\n            for amsgrad in [False, True]:\n                model = keras.models.Sequential()\n                model.add(keras.layers.Embedding(\n                    input_shape=(None,),\n                    input_dim=5,\n                    output_dim=16,\n                    mask_zero=True,\n                ))\n                model.add(keras.layers.Bidirectional(keras.layers.LSTM(units=8)))\n                model.add(keras.layers.Dense(units=2, activation=\'softmax\'))\n                model.compile(optimizer(\n                    total_steps=38400,\n                    warmup_proportion=0.1,\n                    min_lr=1e-6,\n                    weight_decay=1e-6,\n                    amsgrad=amsgrad,\n                ), loss=\'sparse_categorical_crossentropy\')\n\n                x = np.random.randint(0, 5, (64, 3))\n                y = []\n                for i in range(x.shape[0]):\n                    if 2 in x[i]:\n                        y.append(1)\n                    else:\n                        y.append(0)\n                y = np.array(y)\n                model.fit(x, y, epochs=10)\n\n                model_path = os.path.join(tempfile.gettempdir(), \'test_accumulation_%f.h5\' % np.random.random())\n                model.save(model_path)\n                keras.models.load_model(model_path, custom_objects={\'RAdam\': RAdam})\n\n    def test_tensor(self):\n        if not TF_KERAS or EAGER_MODE:\n            return\n        import tensorflow as tf\n        from keras_radam.training import RAdamOptimizer\n\n        x = tf.compat.v1.placeholder(""float"")\n        y = tf.compat.v1.placeholder(""float"")\n        w = tf.Variable([1.0, 2.0], name=""w"")\n        y_model = tf.multiply(x, w[0]) + w[1]\n        loss = tf.square(y - y_model)\n        train_op = RAdamOptimizer().minimize(loss)\n\n        model = tf.global_variables_initializer()\n\n        with tf.Session() as session:\n            session.run(model)\n            for i in range(10000):\n                x_value = np.random.rand()\n                y_value = x_value * 2 + 6\n                session.run(train_op, feed_dict={x: x_value, y: y_value})\n            w_value = session.run(w)\n            print(""Predicted model: {a:.3f}x + {b:.3f}"".format(a=w_value[0], b=w_value[1]))\n'"
tests/test_similar.py,0,"b""from unittest import TestCase\n\nimport torch\nimport numpy as np\n\nfrom keras_radam.backend import keras, TF_KERAS\nfrom keras_radam.backend import backend as K\nfrom keras_radam import RAdam\n\nfrom .official import RAdam as OfficialRAdam\n\n\nclass TestSimilar(TestCase):\n\n    @staticmethod\n    def gen_torch_linear(w, b):\n        linear = torch.nn.Linear(3, 5)\n        linear.weight = torch.nn.Parameter(torch.Tensor(w.transpose().tolist()))\n        linear.bias = torch.nn.Parameter(torch.Tensor(b.tolist()))\n        return linear\n\n    @staticmethod\n    def gen_keras_linear(optimizer, w, b):\n        model = keras.models.Sequential()\n        model.add(keras.layers.Dense(input_shape=(3,), units=5, name='Dense'))\n        model.get_layer('Dense').set_weights([w, b])\n        model.compile(optimizer=optimizer, loss='mse')\n        return model\n\n    @staticmethod\n    def gen_random_weights():\n        return np.random.standard_normal((3, 5)), np.random.standard_normal((5,))\n\n    def _test_same(self, optimizer):\n        w, b = self.gen_random_weights()\n        torch_linear = self.gen_torch_linear(w, b)\n        keras_linear = self.gen_keras_linear(optimizer, w, b)\n        w, b = self.gen_random_weights()\n        criterion = torch.nn.MSELoss()\n        optimizer = OfficialRAdam(torch_linear.parameters(), lr=1e-3, weight_decay=1e-3, eps=K.epsilon())\n        for i in range(500):\n            x = np.random.standard_normal((1, 3))\n            y = np.dot(x, w) + b\n            optimizer.zero_grad()\n            y_hat = torch_linear(torch.Tensor(x.tolist()))\n            loss = criterion(y_hat, torch.Tensor(y.tolist()))\n            torch_loss = loss.tolist()\n            loss.backward()\n            optimizer.step()\n            keras_loss = keras_linear.train_on_batch(x, y).tolist()\n            print(i, torch_loss, keras_loss)\n        self.assertLess(abs(torch_loss - keras_loss), 0.1)\n        self.assertTrue(np.allclose(\n            torch_linear.weight.detach().numpy().transpose(),\n            keras_linear.get_weights()[0],\n            atol=1e-2,\n        ))\n        self.assertTrue(np.allclose(\n            torch_linear.bias.detach().numpy(),\n            keras_linear.get_weights()[1],\n            atol=1e-2,\n        ))\n\n    def test_same_keras(self):\n        self._test_same(RAdam(learning_rate=1e-3, weight_decay=1e-3))\n\n    def test_same_tf(self):\n        if not TF_KERAS:\n            return\n        from keras_radam.training import RAdamOptimizer\n        self._test_same(RAdamOptimizer(learning_rate=1e-3, weight_decay=1e-3))\n"""
