file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\n\nfrom setuptools import setup, find_packages\n\n__version__ = None\nexec(open(\'dltk/version.py\').read())\n\ntest_require = [\'pytest\', \'pytest-flake8\', \'pytest-cov\', \'python-coveralls\']\n\nsetup(name=\'dltk\',\n      version=__version__,\n      description=\'Deep Learning Toolkit for Medical Image Analysis\',\n      author=\'DLTK contributors\',\n      url=\'https://dltk.github.io\',\n      packages=find_packages(exclude=[\'docs\', \'contrib\', \'data\', \'examples\']),\n      keywords=[\'machine learning\', \'tensorflow\', \'deep learning\',\n                \'biomedical imaging\'],\n      license=\'Apache License 2.0\',\n      classifiers=[\'Intended Audience :: Developers\',\n                   \'Intended Audience :: Education\',\n                   \'Intended Audience :: Science/Research\',\n                   \'License :: OSI Approved :: Apache Software License\',\n                   \'Operating System :: POSIX :: Linux\',\n                   \'Operating System :: MacOS :: MacOS X\',\n                   \'Operating System :: Microsoft :: Windows\',\n                   \'Programming Language :: Python :: 2.7\',\n                   \'Programming Language :: Python :: 3.4\'],\n      install_requires=[\'numpy>=1.14.0\', \'scipy>=0.19.0\', \'pandas>=0.19.0\',\n                        \'matplotlib>=1.5.3\', \'future>=0.16.0\', \'xlrd>=1.1.0\',\n                        \'scikit-image>=0.13.0\', \'SimpleITK>=1.0.0\',\n                        \'jupyter>=1.0.0\', \'argparse\'],\n      tests_require=test_require,\n      extras_require={\'docs\': [\'sphinx==1.5.6\', \'sphinx-rtd-theme\',\n                               \'recommonmark\', \'sphinx-autobuild\',\n                               \'sphinxcontrib-versioning\'],\n                      \'tests\': test_require}\n      )\n\nprint(""\\nWelcome to DLTK!"")\nprint(""If any questions please visit documentation page ""\n      ""https://dltk.github.io/dltk"")\nprint(""or join community chat on https://gitter.im/DLTK/DLTK"")\n\n\ntry:\n    import tensorflow\nexcept ImportError:\n    print(\'We did not find TensorFlow on your system. Please install it via \'\n          \'`pip install tensorflow-gpu` if you have a CUDA-enabled GPU or with \'\n          \'`pip install tensorflow` without GPU support.\')\n'"
dltk/__init__.py,0,b''
dltk/utils.py,2,"b'from __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport numpy as np\n\n\nclass SlidingWindow(object):\n    """"""SlidingWindow\n    Sliding window iterator which produces slice objects to slice in a sliding\n    window. This is useful for inference.\n    """"""\n\n    def __init__(self, img_shape, window_shape, has_batch_dim=True,\n                 striding=None):\n        """"""Constructs a sliding window iterator\n\n        Args:\n            img_shape (array_like): shape of the image to slide over\n\n            window_shape (array_like): shape of the window to extract\n\n            has_batch_dim (bool, optional): flag to indicate whether a batch\n                dimension is present\n\n            striding (array_like, optional): amount to move the window between\n                each position\n        """"""\n\n        self.img_shape = img_shape\n        self.window_shape = window_shape\n        self.rank = len(img_shape)\n        self.curr_pos = [0] * self.rank\n        self.end_pos = [0] * self.rank\n        self.done = False\n        self.striding = window_shape\n        self.has_batch_dim = has_batch_dim\n        if striding:\n            self.striding = striding\n\n    def __iter__(self):\n        return self\n\n    # py 2.* compatability hack\n    def next(self):\n        return self.__next__()\n\n    def __next__(self):\n        if self.done:\n            raise StopIteration()\n\n        if self.has_batch_dim:\n            slicer = [slice(None)] * (self.rank + 1)\n        else:\n            slicer = [slice(None)] * self.rank\n\n        move_dim = True\n        for dim, pos in enumerate(self.curr_pos):\n            low = pos\n            high = pos + self.window_shape[dim]\n            if move_dim:\n                if high >= self.img_shape[dim]:\n                    self.curr_pos[dim] = 0\n                    move_dim = True\n                else:\n                    self.curr_pos[dim] += self.striding[dim]\n                    move_dim = False\n            if high >= self.img_shape[dim]:\n                low = self.img_shape[dim] - self.window_shape[dim]\n                high = self.img_shape[dim]\n\n            if self.has_batch_dim:\n                slicer[dim + 1] = slice(low, high)\n            else:\n                slicer[dim] = slice(low, high)\n\n        if (np.array(self.curr_pos) == self.end_pos).all():\n            self.done = True\n        return slicer\n\n\ndef sliding_window_segmentation_inference(session,\n                                          ops_list,\n                                          sample_dict,\n                                          batch_size=1,\n                                          striding=None):\n    """"""\n    Utility function to perform sliding window inference for segmentation\n\n    Args:\n        session (tf.Session): TensorFlow session to run ops with\n\n        ops_list (array_like): Operators to fetch assemble with sliding window\n\n        sample_dict (dict): Dictionary with tf.Placeholder keys mapping the\n        placeholders to their respective input\n\n        batch_size (int, optional): Number of sliding windows to batch for\n            calculation\n\n        striding (array_like): Striding of the sliding window. Can be used to\n            adjust overlap etc.\n\n    Returns:\n        list: List of np.arrays corresponding to the assembled outputs of\n            ops_list\n    """"""\n\n    # TODO: asserts\n    assert batch_size > 0, \'Batch size has to be 1 or bigger\'\n\n    pl_shape = list(sample_dict.keys())[0].get_shape().as_list()\n\n    pl_bshape = pl_shape[1:-1]\n\n    inp_shape = list(list(sample_dict.values())[0].shape)\n    inp_bshape = inp_shape[1:-1]\n\n    out_dummies = [np.zeros(\n        [inp_shape[0], ] + inp_bshape + [op.get_shape().as_list()[-1]]\n        if len(op.get_shape().as_list()) == len(inp_shape) else []) for op in ops_list]\n\n    out_dummy_counter = [np.zeros_like(o) for o in out_dummies]\n\n    op_shape = list(ops_list[0].get_shape().as_list())\n    op_bshape = op_shape[1:-1]\n\n    out_diff = np.array(pl_bshape) - np.array(op_bshape)\n\n    padding = [[0, 0]] + [[diff // 2, diff - diff // 2] for diff\n                          in out_diff] + [[0, 0]]\n\n    padded_dict = {k: np.pad(v, padding, mode=\'constant\') for k, v\n                   in sample_dict.items()}\n\n    f_bshape = list(padded_dict.values())[0].shape[1:-1]\n\n    if not striding:\n        striding = (list(np.maximum(1, np.array(op_bshape) // 2))\n                    if all(out_diff == 0) else op_bshape)\n\n    sw = SlidingWindow(f_bshape, pl_bshape, striding=striding)\n    out_sw = SlidingWindow(inp_bshape, op_bshape, striding=striding)\n\n    if batch_size > 1:\n        slicers = []\n        out_slicers = []\n\n    done = False\n    while True:\n        try:\n            slicer = next(sw)\n            out_slicer = next(out_sw)\n        except StopIteration:\n            done = True\n\n        if batch_size == 1:\n            sw_dict = {k: v[slicer] for k, v in padded_dict.items()}\n            op_parts = session.run(ops_list, feed_dict=sw_dict)\n\n            for idx in range(len(op_parts)):\n                out_dummies[idx][out_slicer] += op_parts[idx]\n                out_dummy_counter[idx][out_slicer] += 1\n        else:\n            slicers.append(slicer)\n            out_slicers.append(out_slicer)\n            if len(slicers) == batch_size or done:\n                slices_dict = {k: np.concatenate(\n                    [v[slicer] for slicer in slicers], 0) for k, v in padded_dict.items()}\n\n                all_op_parts = session.run(ops_list, feed_dict=slices_dict)\n\n                zipped_parts = zip(*[np.array_split(part, len(slicers)) for\n                                     part in all_op_parts])\n\n                for out_slicer, op_parts in zip(out_slicers, zipped_parts):\n                    for idx in range(len(op_parts)):\n                        out_dummies[idx][out_slicer] += op_parts[idx]\n                        out_dummy_counter[idx][out_slicer] += 1\n\n                slicers = []\n                out_slicers = []\n\n        if done:\n            break\n\n    return [o / c for o, c in zip(out_dummies, out_dummy_counter)]\n'"
dltk/version.py,0,"b""from __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\n\n__version__ = '0.2.1'\n"""
tests/test_activations.py,4,"b""import tensorflow as tf\nfrom dltk.core.activations import leaky_relu\nimport numpy as np\n\n\ndef test_leaky_relu():\n    test_alpha = tf.constant(0.1)\n    test_inp_1 = tf.constant(1.)\n    test_inp_2 = tf.constant(-1.)\n\n    test_relu_1 = leaky_relu(test_inp_1, test_alpha)\n    test_relu_2 = leaky_relu(test_inp_2, test_alpha)\n\n    with tf.Session() as s:\n        out_1 = s.run(test_relu_1)\n        assert np.isclose(out_1, 1.), \\\n            'Got {} but expected {}'.format(out_1, 1.)\n\n        out_2 = s.run(test_relu_2)\n        assert np.isclose(out_2, -0.1), \\\n            'Got {} but expected {}'.format(out_2, -0.1)\n"""
tests/test_sliding_window_segmentation.py,3,"b""import tensorflow as tf\nfrom dltk.utils import sliding_window_segmentation_inference\nimport numpy as np\n\n\ndef test_sw_inference():\n\n    inp = tf.placeholder(tf.float32, [1, 1, 2, 1])\n    op = tf.ones([1, 1, 2, 1], tf.float32)\n    np_inp = np.ones([1, 4, 4, 1])\n\n    with tf.Session() as s:\n        out = sliding_window_segmentation_inference(s, [op], {inp: np_inp})[0]\n        assert np.isclose(out, np_inp).all(), \\\n            'Got {} but expected {}'.format(out, np_inp)\n"""
data/IXI_Guys/download_IXI_Guys.py,0,"b'# -*- coding: utf-8 -*-\n""""""Download and extract the IXI Guy\'s Hospital 1.5T dataset\n\nurl: http://brain-development.org/ixi-dataset/\nref: IXI \xe2\x80\x93 Information eXtraction from Images (EPSRC GR/S21533/02)\n\n""""""\nfrom __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\nfrom future.standard_library import install_aliases  # py 2/3 compatability\ninstall_aliases()\n\nfrom urllib.request import FancyURLopener\nimport os.path\nimport tarfile\nimport pandas as pd\nimport glob\nimport SimpleITK as sitk\nimport numpy as np\n\nDOWNLOAD_IMAGES = True\nEXTRACT_IMAGES = True\nPROCESS_OTHER = True\nRESAMPLE_IMAGES = True\nCLEAN_UP = True\n\n\ndef resample_image(itk_image, out_spacing=[1.0, 1.0, 1.0], is_label=False):\n    original_spacing = itk_image.GetSpacing()\n    original_size = itk_image.GetSize()\n\n    out_size = [\n        int(np.round(original_size[0] * (original_spacing[0] / out_spacing[0]))),\n        int(np.round(original_size[1] * (original_spacing[1] / out_spacing[1]))),\n        int(np.round(original_size[2] * (original_spacing[2] / out_spacing[2])))\n    ]\n\n    resample = sitk.ResampleImageFilter()\n    resample.SetOutputSpacing(out_spacing)\n    resample.SetSize(out_size)\n    resample.SetOutputDirection(itk_image.GetDirection())\n    resample.SetOutputOrigin(itk_image.GetOrigin())\n    resample.SetTransform(sitk.Transform())\n    resample.SetDefaultPixelValue(itk_image.GetPixelIDValue())\n\n    if is_label:\n        resample.SetInterpolator(sitk.sitkNearestNeighbor)\n    else:\n        resample.SetInterpolator(sitk.sitkBSpline)\n\n    return resample.Execute(itk_image)\n\n\ndef reslice_image(itk_image, itk_ref, is_label=False):\n    resample = sitk.ResampleImageFilter()\n    resample.SetReferenceImage(itk_ref)\n\n    if is_label:\n        resample.SetInterpolator(sitk.sitkNearestNeighbor)\n    else:\n        resample.SetInterpolator(sitk.sitkBSpline)\n\n    return resample.Execute(itk_image)\n\n\nurls = {}\nurls[\'t1\'] = \'http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI-T1.tar\'\nurls[\'t2\'] = \'http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI-T2.tar\'\nurls[\'pd\'] = \'http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI-PD.tar\'\nurls[\'mra\'] = \'http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI-MRA.tar\'\nurls[\'demographic\'] = \'http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI.xls\'\n\nfnames = {}\nfnames[\'t1\'] = \'t1.tar\'\nfnames[\'t2\'] = \'t2.tar\'\nfnames[\'pd\'] = \'pd.tar\'\nfnames[\'mra\'] = \'mra.tar\'\nfnames[\'demographic\'] = \'demographic.xls\'\n\nif DOWNLOAD_IMAGES:\n    # Download all IXI data\n    for key, url in urls.items():\n        if not os.path.isfile(fnames[key]):\n            print(\'Downloading {} from {}\'.format(fnames[key], url))\n            curr_file = FancyURLopener()\n            curr_file.retrieve(url, fnames[key])\n        else:\n            print(\'File {} already exists. Skipping download.\'.format(\n                fnames[key]))\n\nif EXTRACT_IMAGES:\n    # Extract the Guys subset of IXI\n    for key, fname in fnames.items():\n        if (fname.endswith(\'.tar\')):\n            print(\'Extracting IXI Guys data from {}.\'.format(fnames[key]))\n            output_dir = os.path.join(\'./orig/\', key)\n\n            if not os.path.exists(output_dir):\n                os.makedirs(output_dir)\n\n            t = tarfile.open(fname, \'r\')\n            for member in t.getmembers():\n                if \'-Guys-\' in member.name:\n                    t.extract(member, output_dir)\n\n\nif PROCESS_OTHER:\n    # Process the demographic xls data and save to csv\n    xls = pd.ExcelFile(\'demographic.xls\')\n    print(xls.sheet_names)\n\n    df = xls.parse(\'Table\')\n    for index, row in df.iterrows():\n        IXI_id = \'IXI{:03d}\'.format(row[\'IXI_ID\'])\n        df.loc[index, \'IXI_ID\'] = IXI_id\n\n        t1_exists = len(glob.glob(\'./orig/t1/{}*.nii.gz\'.format(IXI_id)))\n        t2_exists = len(glob.glob(\'./orig/t2/{}*.nii.gz\'.format(IXI_id)))\n        pd_exists = len(glob.glob(\'./orig/pd/{}*.nii.gz\'.format(IXI_id)))\n        mra_exists = len(glob.glob(\'./orig/mra/{}*.nii.gz\'.format(IXI_id)))\n\n        # Check if each entry is complete and drop if not\n        # if not t1_exists and not t2_exists and not pd_exists and not mra\n        # exists:\n        if not (t1_exists and t2_exists and pd_exists and mra_exists):\n            df.drop(index, inplace=True)\n\n    # Write to csv file\n    df.to_csv(\'demographic_Guys.csv\', index=False)\n\nif RESAMPLE_IMAGES:\n    # Resample the IXI Guys T2 images to 1mm isotropic and reslice all others\n    # to it\n    df = pd.read_csv(\'demographic_Guys.csv\', dtype=object,\n                     keep_default_na=False, na_values=[]).as_matrix()\n\n    for i in df:\n        IXI_id = i[0]\n        print(\'Resampling {}\'.format(IXI_id))\n\n        t1_fn = glob.glob(\'./orig/t1/{}*.nii.gz\'.format(IXI_id))[0]\n        t2_fn = glob.glob(\'./orig/t2/{}*.nii.gz\'.format(IXI_id))[0]\n        pd_fn = glob.glob(\'./orig/pd/{}*.nii.gz\'.format(IXI_id))[0]\n        mra_fn = glob.glob(\'./orig/mra/{}*.nii.gz\'.format(IXI_id))[0]\n\n        t1 = sitk.ReadImage(t1_fn)\n        t2 = sitk.ReadImage(t2_fn)\n        pd = sitk.ReadImage(pd_fn)\n        mra = sitk.ReadImage(mra_fn)\n\n        # Resample to 1mm isotropic resolution\n        t2_1mm = resample_image(t2)\n        t1_1mm = reslice_image(t1, t2_1mm)\n        pd_1mm = reslice_image(pd, t2_1mm)\n        mra_1mm = reslice_image(mra, t2_1mm)\n\n        output_dir = os.path.join(\'./1mm/\', IXI_id)\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        print(\'T1: {} {}\'.format(t1_1mm.GetSize(), t1_1mm.GetSpacing()))\n        print(\'T2: {} {}\'.format(t2_1mm.GetSize(), t2_1mm.GetSpacing()))\n        print(\'PD: {} {}\'.format(pd_1mm.GetSize(), pd_1mm.GetSpacing()))\n        print(\'MRA: {} {}\'.format(mra_1mm.GetSize(), mra_1mm.GetSpacing()))\n\n        sitk.WriteImage(t1_1mm, os.path.join(output_dir, \'T1_1mm.nii.gz\'))\n        sitk.WriteImage(t2_1mm, os.path.join(output_dir, \'T2_1mm.nii.gz\'))\n        sitk.WriteImage(pd_1mm, os.path.join(output_dir, \'PD_1mm.nii.gz\'))\n        sitk.WriteImage(mra_1mm, os.path.join(output_dir, \'MRA_1mm.nii.gz\'))\n\n        # Resample to 2mm isotropic resolution\n        t2_2mm = resample_image(t2, out_spacing=[2.0, 2.0, 2.0])\n        t1_2mm = reslice_image(t1, t2_2mm)\n        pd_2mm = reslice_image(pd, t2_2mm)\n        mra_2mm = reslice_image(mra, t2_2mm)\n\n        output_dir = os.path.join(\'./2mm/\', IXI_id)\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        print(\'T1: {} {}\'.format(t2_2mm.GetSize(), t1_2mm.GetSpacing()))\n        print(\'T2: {} {}\'.format(t2_2mm.GetSize(), t2_2mm.GetSpacing()))\n        print(\'PD: {} {}\'.format(pd_2mm.GetSize(), pd_2mm.GetSpacing()))\n        print(\'MRA: {} {}\'.format(mra_2mm.GetSize(), mra_2mm.GetSpacing()))\n\n        sitk.WriteImage(t1_2mm, os.path.join(output_dir, \'T1_2mm.nii.gz\'))\n        sitk.WriteImage(t2_2mm, os.path.join(output_dir, \'T2_2mm.nii.gz\'))\n        sitk.WriteImage(pd_2mm, os.path.join(output_dir, \'PD_2mm.nii.gz\'))\n        sitk.WriteImage(mra_2mm, os.path.join(output_dir, \'MRA_2mm.nii.gz\'))\n\n\nif CLEAN_UP:\n    # Remove the .tar files\n    for key, fname in fnames.items():\n        if (fname.endswith(\'.tar\')):\n            os.remove(fname)\n\n    # Remove all data in original resolution\n    os.system(\'rm -rf orig\')\n'"
data/IXI_HH/download_IXI_HH.py,0,"b'# -*- coding: utf-8 -*-\n""""""Download and extract the IXI Hammersmith Hospital 3T dataset\n\nurl: http://brain-development.org/ixi-dataset/\nref: IXI \xe2\x80\x93 Information eXtraction from Images (EPSRC GR/S21533/02)\n\n""""""\nfrom __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\nfrom future.standard_library import install_aliases  # py 2/3 compatability\ninstall_aliases()\n\nfrom urllib.request import FancyURLopener\n\nimport os.path\nimport tarfile\nimport pandas as pd\nimport glob\nimport SimpleITK as sitk\nimport numpy as np\n\nDOWNLOAD_IMAGES = True\nEXTRACT_IMAGES = True\nPROCESS_OTHER = True\nRESAMPLE_IMAGES = True\nCLEAN_UP = True\n\n\ndef resample_image(itk_image, out_spacing=(1.0, 1.0, 1.0), is_label=False):\n    original_spacing = itk_image.GetSpacing()\n    original_size = itk_image.GetSize()\n\n    out_size = [int(np.round(original_size[0] * (original_spacing[0] / out_spacing[0]))),\n                int(np.round(original_size[1] * (original_spacing[1] / out_spacing[1]))),\n                int(np.round(original_size[2] * (original_spacing[2] / out_spacing[2])))]\n\n    resample = sitk.ResampleImageFilter()\n    resample.SetOutputSpacing(out_spacing)\n    resample.SetSize(out_size)\n    resample.SetOutputDirection(itk_image.GetDirection())\n    resample.SetOutputOrigin(itk_image.GetOrigin())\n    resample.SetTransform(sitk.Transform())\n    resample.SetDefaultPixelValue(itk_image.GetPixelIDValue())\n\n    if is_label:\n        resample.SetInterpolator(sitk.sitkNearestNeighbor)\n    else:\n        resample.SetInterpolator(sitk.sitkBSpline)\n\n    return resample.Execute(itk_image)\n\n\ndef reslice_image(itk_image, itk_ref, is_label=False):\n    resample = sitk.ResampleImageFilter()\n    resample.SetReferenceImage(itk_ref)\n\n    if is_label:\n        resample.SetInterpolator(sitk.sitkNearestNeighbor)\n    else:\n        resample.SetInterpolator(sitk.sitkBSpline)\n\n    return resample.Execute(itk_image)\n\n\nurls = {}\nurls[\'t1\'] = \'http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI-T1.tar\'\nurls[\'t2\'] = \'http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI-T2.tar\'\nurls[\'pd\'] = \'http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI-PD.tar\'\nurls[\'mra\'] = \'http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI-MRA.tar\'\nurls[\'demographic\'] = \'http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI.xls\'\n\nfnames = {}\nfnames[\'t1\'] = \'t1.tar\'\nfnames[\'t2\'] = \'t2.tar\'\nfnames[\'pd\'] = \'pd.tar\'\nfnames[\'mra\'] = \'mra.tar\'\nfnames[\'demographic\'] = \'demographic.xls\'\n\n\nif DOWNLOAD_IMAGES:\n    # Download all IXI data\n    for key, url in urls.items():\n\n        if not os.path.isfile(fnames[key]):\n            print(\'Downloading {} from {}\'.format(fnames[key], url))\n            curr_file = FancyURLopener()\n            curr_file.retrieve(url, fnames[key])\n        else:\n            print(\'File {} already exists. Skipping download.\'.format(\n                fnames[key]))\n\nif EXTRACT_IMAGES:\n    # Extract the HH subset of IXI\n    for key, fname in fnames.items():\n\n        if (fname.endswith(\'.tar\')):\n            print(\'Extracting IXI HH data from {}.\'.format(fnames[key]))\n            output_dir = os.path.join(\'./orig/\', key)\n\n            if not os.path.exists(output_dir):\n                os.makedirs(output_dir)\n\n            t = tarfile.open(fname, \'r\')\n            for member in t.getmembers():\n                if \'-HH-\' in member.name:\n                    t.extract(member, output_dir)\n\n\nif PROCESS_OTHER:\n    # Process the demographic xls data and save to csv\n    xls = pd.ExcelFile(\'demographic.xls\')\n    print(xls.sheet_names)\n\n    df = xls.parse(\'Table\')\n    for index, row in df.iterrows():\n        IXI_id = \'IXI{:03d}\'.format(row[\'IXI_ID\'])\n        df.loc[index, \'IXI_ID\'] = IXI_id\n\n        t1_exists = len(glob.glob(\'./orig/t1/{}*.nii.gz\'.format(IXI_id)))\n        t2_exists = len(glob.glob(\'./orig/t2/{}*.nii.gz\'.format(IXI_id)))\n        pd_exists = len(glob.glob(\'./orig/pd/{}*.nii.gz\'.format(IXI_id)))\n        mra_exists = len(glob.glob(\'./orig/mra/{}*.nii.gz\'.format(IXI_id)))\n\n        # Check if each entry is complete and drop if not\n        # if not t1_exists and not t2_exists and not pd_exists and not mra\n        # exists:\n        if not (t1_exists and t2_exists and pd_exists and mra_exists):\n            df.drop(index, inplace=True)\n\n    # Write to csv file\n    df.to_csv(\'demographic_HH.csv\', index=False)\n\nif RESAMPLE_IMAGES:\n    # Resample the IXI HH T2 images to 1mm isotropic and reslice all\n    # others to it\n    df = pd.read_csv(\'demographic_HH.csv\', dtype=object, keep_default_na=False,\n                     na_values=[]).as_matrix()\n\n    for i in df:\n        IXI_id = i[0]\n        print(\'Resampling {}\'.format(IXI_id))\n\n        t1_fn = glob.glob(\'./orig/t1/{}*.nii.gz\'.format(IXI_id))[0]\n        t2_fn = glob.glob(\'./orig/t2/{}*.nii.gz\'.format(IXI_id))[0]\n        pd_fn = glob.glob(\'./orig/pd/{}*.nii.gz\'.format(IXI_id))[0]\n        mra_fn = glob.glob(\'./orig/mra/{}*.nii.gz\'.format(IXI_id))[0]\n\n        t1 = sitk.ReadImage(t1_fn)\n        t2 = sitk.ReadImage(t2_fn)\n        pd = sitk.ReadImage(pd_fn)\n        mra = sitk.ReadImage(mra_fn)\n\n        # Resample to 1mm isotropic resolution\n        t2_1mm = resample_image(t2)\n        t1_1mm = reslice_image(t1, t2_1mm)\n        pd_1mm = reslice_image(pd, t2_1mm)\n        mra_1mm = reslice_image(mra, t2_1mm)\n\n        output_dir = os.path.join(\'./1mm/\', IXI_id)\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        print(\'T1: {} {}\'.format(t1_1mm.GetSize(), t1_1mm.GetSpacing()))\n        print(\'T2: {} {}\'.format(t2_1mm.GetSize(), t2_1mm.GetSpacing()))\n        print(\'PD: {} {}\'.format(pd_1mm.GetSize(), pd_1mm.GetSpacing()))\n        print(\'MRA: {} {}\'.format(mra_1mm.GetSize(), mra_1mm.GetSpacing()))\n\n        sitk.WriteImage(t1_1mm, os.path.join(output_dir, \'T1_1mm.nii.gz\'))\n        sitk.WriteImage(t2_1mm, os.path.join(output_dir, \'T2_1mm.nii.gz\'))\n        sitk.WriteImage(pd_1mm, os.path.join(output_dir, \'PD_1mm.nii.gz\'))\n        sitk.WriteImage(mra_1mm, os.path.join(output_dir, \'MRA_1mm.nii.gz\'))\n\n        # Resample to 2mm isotropic resolution\n        t2_2mm = resample_image(t2, out_spacing=[2.0, 2.0, 2.0])\n        t1_2mm = reslice_image(t1, t2_2mm)\n        pd_2mm = reslice_image(pd, t2_2mm)\n        mra_2mm = reslice_image(mra, t2_2mm)\n\n        output_dir = os.path.join(\'./2mm/\', IXI_id)\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        print(\'T1: {} {}\'.format(t2_2mm.GetSize(), t1_2mm.GetSpacing()))\n        print(\'T2: {} {}\'.format(t2_2mm.GetSize(), t2_2mm.GetSpacing()))\n        print(\'PD: {} {}\'.format(pd_2mm.GetSize(), pd_2mm.GetSpacing()))\n        print(\'MRA: {} {}\'.format(mra_2mm.GetSize(), mra_2mm.GetSpacing()))\n\n        sitk.WriteImage(t1_2mm, os.path.join(output_dir, \'T1_2mm.nii.gz\'))\n        sitk.WriteImage(t2_2mm, os.path.join(output_dir, \'T2_2mm.nii.gz\'))\n        sitk.WriteImage(pd_2mm, os.path.join(output_dir, \'PD_2mm.nii.gz\'))\n        sitk.WriteImage(mra_2mm, os.path.join(output_dir, \'MRA_2mm.nii.gz\'))\n\n\nif CLEAN_UP:\n    # Remove the .tar files\n    for key, fname in fnames.items():\n        if (fname.endswith(\'.tar\')):\n            os.remove(fname)\n\n    # Remove all data in original resolution\n    os.system(\'rm -rf orig\')\n'"
dltk/core/__init__.py,0,b''
dltk/core/activations.py,8,"b'from __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport tensorflow as tf\n\n\ndef prelu(inputs, alpha_initializer=tf.constant_initializer()):\n    """"""Probabilistic ReLu activation function\n\n    Args:\n        (tf.Tensor): input Tensor\n        alpha_initializer (float, optional): an initial value for alpha\n\n    Returns:\n        tf.Tensor: a PreLu activated tensor\n    """"""\n\n    alpha = tf.get_variable(\'alpha\',\n                            shape=[],\n                            dtype=tf.float32,\n                            initializer=alpha_initializer)\n\n    return leaky_relu(inputs, alpha)\n\n\ndef leaky_relu(inputs, alpha=0.1):\n    """"""Leaky ReLu activation function\n\n    Args:\n        inputs (tf.Tensor): input Tensor\n        alpha (float): leakiness parameter\n\n    Returns:\n        tf.Tensor: a leaky ReLu activated tensor\n    """"""\n    return tf.maximum(inputs, alpha * inputs)\n'"
dltk/core/losses.py,33,"b'from __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport tensorflow as tf\nimport numpy as np\n\n\ndef sparse_balanced_crossentropy(logits, labels):\n    """"""\n    Calculates a class frequency balanced crossentropy loss from sparse labels.\n\n    Args:\n        logits (tf.Tensor): logits prediction for which to calculate\n            crossentropy error\n        labels (tf.Tensor): sparse labels used for crossentropy error\n            calculation\n\n    Returns:\n        tf.Tensor: Tensor scalar representing the mean loss\n    """"""\n\n    epsilon = tf.constant(np.finfo(np.float32).tiny)\n\n    num_classes = tf.cast(tf.shape(logits)[-1], tf.int32)\n\n    probs = tf.nn.softmax(logits)\n    probs += tf.cast(tf.less(probs, epsilon), tf.float32) * epsilon\n    log = -1. * tf.log(probs)\n\n    onehot_labels = tf.one_hot(labels, num_classes)\n\n    class_frequencies = tf.stop_gradient(tf.bincount(\n        labels, minlength=num_classes, dtype=tf.float32))\n\n    weights = (1. / (class_frequencies + tf.constant(1e-8)))\n    weights *= (tf.cast(tf.reduce_prod(tf.shape(labels)), tf.float32) / tf.cast(num_classes, tf.float32))\n\n    new_shape = (([1, ] * len(labels.get_shape().as_list())) + [logits.get_shape().as_list()[-1]])\n\n    weights = tf.reshape(weights, new_shape)\n\n    loss = tf.reduce_mean(tf.reduce_sum(onehot_labels * log * weights, axis=-1))\n\n    return loss\n\n\ndef dice_loss(logits,\n              labels,\n              num_classes,\n              smooth=1e-5,\n              include_background=True,\n              only_present=False):\n    """"""Calculates a smooth Dice coefficient loss from sparse labels.\n\n    Args:\n        logits (tf.Tensor): logits prediction for which to calculate\n            crossentropy error\n        labels (tf.Tensor): sparse labels used for crossentropy error\n            calculation\n        num_classes (int): number of class labels to evaluate on\n        smooth (float): smoothing coefficient for the loss computation\n        include_background (bool): flag to include a loss on the background\n            label or not\n        only_present (bool): flag to include only labels present in the\n            inputs or not\n\n    Returns:\n        tf.Tensor: Tensor scalar representing the loss\n    """"""\n\n    # Get a softmax probability of the logits predictions and a one hot\n    # encoding of the labels tensor\n    probs = tf.nn.softmax(logits)\n    onehot_labels = tf.one_hot(\n        indices=labels,\n        depth=num_classes,\n        dtype=tf.float32,\n        name=\'onehot_labels\')\n\n    # Compute the Dice similarity coefficient\n    label_sum = tf.reduce_sum(onehot_labels, axis=[1, 2, 3], name=\'label_sum\')\n    pred_sum = tf.reduce_sum(probs, axis=[1, 2, 3], name=\'pred_sum\')\n    intersection = tf.reduce_sum(onehot_labels * probs, axis=[1, 2, 3],\n                                 name=\'intersection\')\n\n    per_sample_per_class_dice = (2. * intersection + smooth)\n    per_sample_per_class_dice /= (label_sum + pred_sum + smooth)\n\n    # Include or exclude the background label for the computation\n    if include_background:\n        flat_per_sample_per_class_dice = tf.reshape(\n            per_sample_per_class_dice, (-1, ))\n        flat_label = tf.reshape(label_sum, (-1, ))\n    else:\n        flat_per_sample_per_class_dice = tf.reshape(\n            per_sample_per_class_dice[:, 1:], (-1, ))\n        flat_label = tf.reshape(label_sum[:, 1:], (-1, ))\n\n    # Include or exclude non-present labels for the computation\n    if only_present:\n        masked_dice = tf.boolean_mask(flat_per_sample_per_class_dice,\n                                      tf.logical_not(tf.equal(flat_label, 0)))\n    else:\n        masked_dice = tf.boolean_mask(\n            flat_per_sample_per_class_dice,\n            tf.logical_not(tf.is_nan(flat_per_sample_per_class_dice)))\n\n    dice = tf.reduce_mean(masked_dice)\n    loss = 1. - dice\n\n    return loss\n'"
dltk/core/metrics.py,0,"b'from __future__ import division\nfrom __future__ import absolute_import\nfrom __future__ import print_function\n\nimport numpy as np\n\n\ndef dice(predictions, labels, num_classes):\n    """"""Calculates the categorical Dice similarity coefficients for each class\n        between labels and predictions.\n\n    Args:\n        predictions (np.ndarray): predictions\n        labels (np.ndarray): labels\n        num_classes (int): number of classes to calculate the dice\n            coefficient for\n\n    Returns:\n        np.ndarray: dice coefficient per class or NaN if class not present\n    """"""\n\n    dice_scores = np.zeros((num_classes))\n    for i in range(num_classes):\n        tmp_den = (np.sum(predictions == i) + np.sum(labels == i))\n        tmp_dice = 2. * np.sum((predictions == i) * (labels == i)) / tmp_den\n        dice_scores[i] = tmp_dice\n    return dice_scores.astype(np.float32)\n\n\ndef abs_vol_difference(predictions, labels, num_classes):\n    """"""Calculates the absolute volume difference for each class between\n        labels and predictions.\n\n    Args:\n        predictions (np.ndarray): predictions\n        labels (np.ndarray): labels\n        num_classes (int): number of classes to calculate avd for\n\n    Returns:\n        np.ndarray: avd per class\n    """"""\n\n    avd = np.zeros((num_classes))\n    eps = 1e-6\n    for i in range(num_classes):\n        avd[i] = np.abs(np.sum(predictions == i) - np.sum(labels == i)\n                        ) / (np.float(np.sum(labels == i)) + eps)\n\n    return avd.astype(np.float32)\n\n\ndef crossentropy(predictions, labels, logits=True):\n    """"""Calculates the crossentropy loss between predictions and labels\n\n    Args:\n        prediction (np.ndarray): predictions\n        labels (np.ndarray): labels\n        logits (bool): flag whether predictions are logits or probabilities\n\n    Returns:\n        float: crossentropy error\n    """"""\n\n    if logits:\n        maxes = np.amax(predictions, axis=-1, keepdims=True)\n        softexp = np.exp(predictions - maxes)\n        softm = softexp / np.sum(softexp, axis=-1, keepdims=True)\n    else:\n        softm = predictions\n    loss = np.mean(-1. * np.sum(labels * np.log(softm + 1e-8), axis=-1))\n    return loss.astype(np.float32)\n'"
dltk/core/residual_unit.py,19,"b'from __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport tensorflow as tf\nimport numpy as np\n\n\ndef vanilla_residual_unit_3d(inputs,\n                             out_filters,\n                             kernel_size=(3, 3, 3),\n                             strides=(1, 1, 1),\n                             mode=tf.estimator.ModeKeys.EVAL,\n                             use_bias=False,\n                             activation=tf.nn.relu6,\n                             kernel_initializer=tf.initializers.variance_scaling(distribution=\'uniform\'),\n                             bias_initializer=tf.zeros_initializer(),\n                             kernel_regularizer=None,\n                             bias_regularizer=None):\n    """"""Implementation of a 3D residual unit according to [1]. This\n        implementation supports strided convolutions and automatically\n        handles different input and output filters.\n\n    [1] K. He et al. Identity Mappings in Deep Residual Networks. ECCV 2016.\n\n    Args:\n        inputs (tf.Tensor): Input tensor to the residual unit. Is required to\n            have a rank of 5 (i.e. [batch, x, y, z, channels]).\n        out_filters (int): Number of convolutional filters used in\n            the sub units.\n        kernel_size (tuple, optional): Size of the convoltional kernels\n            used in the sub units\n        strides (tuple, optional): Convolution strides in (x,y,z) of sub\n            unit 0. Allows downsampling of the input tensor via strides\n            convolutions.\n        mode (str, optional): One of the tf.estimator.ModeKeys: TRAIN, EVAL or\n            PREDICT\n        activation (optional): A function to use as activation function.\n        use_bias (bool, optional): Train a bias with each convolution.\n        kernel_initializer (TYPE, optional): Initialisation of convolution kernels\n        bias_initializer (TYPE, optional): Initialisation of bias\n        kernel_regularizer (None, optional): Additional regularisation op\n        bias_regularizer (None, optional): Additional regularisation op\n\n    Returns:\n        tf.Tensor: Output of the residual unit\n    """"""\n\n    pool_op = tf.layers.max_pooling3d\n\n    conv_params = {\'padding\': \'same\',\n                   \'use_bias\': use_bias,\n                   \'kernel_initializer\': kernel_initializer,\n                   \'bias_initializer\': bias_initializer,\n                   \'kernel_regularizer\': kernel_regularizer,\n                   \'bias_regularizer\': bias_regularizer}\n\n    in_filters = inputs.get_shape().as_list()[-1]\n    assert in_filters == inputs.get_shape().as_list()[-1], \\\n        \'Module was initialised for a different input shape\'\n\n    x = inputs\n    orig_x = x\n\n    # Handle strided convolutions\n    if np.prod(strides) != 1:\n        orig_x = pool_op(inputs=orig_x,\n                         pool_size=strides,\n                         strides=strides,\n                         padding=\'valid\')\n\n    # Sub unit 0\n    with tf.variable_scope(\'sub_unit0\'):\n\n        # Adjust the strided conv kernel size to prevent losing information\n        k = [s * 2 if s > 1 else k for k, s in zip(kernel_size, strides)]\n\n        x = tf.layers.batch_normalization(\n            x, training=mode == tf.estimator.ModeKeys.TRAIN)\n        x = activation(x)\n\n        x = tf.layers.conv3d(\n            inputs=x,\n            filters=out_filters,\n            kernel_size=k, strides=strides,\n            **conv_params)\n\n    # Sub unit 1\n    with tf.variable_scope(\'sub_unit1\'):\n        x = tf.layers.batch_normalization(\n            x, training=mode == tf.estimator.ModeKeys.TRAIN)\n        x = activation(x)\n\n        x = tf.layers.conv3d(\n            inputs=x,\n            filters=out_filters,\n            kernel_size=kernel_size,\n            strides=(1, 1, 1),\n            **conv_params)\n\n    # Add the residual\n    with tf.variable_scope(\'sub_unit_add\'):\n\n        # Handle differences in input and output filter sizes\n        if in_filters < out_filters:\n            orig_x = tf.pad(\n                tensor=orig_x,\n                paddings=[[0, 0]] * (len(x.get_shape().as_list()) - 1) + [[\n                    int(np.floor((out_filters - in_filters) / 2.)),\n                    int(np.ceil((out_filters - in_filters) / 2.))]])\n\n        elif in_filters > out_filters:\n            orig_x = tf.layers.conv3d(\n                inputs=orig_x,\n                filters=out_filters,\n                kernel_size=kernel_size,\n                strides=(1, 1, 1),\n                **conv_params)\n        x += orig_x\n\n    return x\n'"
dltk/core/upsample.py,9,"b'from __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport tensorflow as tf\nimport numpy as np\n\n\ndef get_linear_upsampling_kernel(kernel_spatial_shape,\n                                 out_filters,\n                                 in_filters,\n                                 trainable=False):\n    """"""Builds a kernel for linear upsampling with the shape\n        [kernel_spatial_shape] + [out_filters, in_filters]. Can be set to\n        trainable to potentially learn a better upsamling.\n\n    Args:\n        kernel_spatial_shape (list or tuple): Spatial dimensions of the\n            upsampling kernel. Is required to be of rank 2 or 3,\n            (i.e. [dim_x, dim_y] or [dim_x, dim_y, dim_z])\n        out_filters (int): Number of output filters.\n        in_filters (int): Number of input filters.\n        trainable (bool, optional): Flag to set the returned tf.Variable\n            to be trainable or not.\n\n    Returns:\n        tf.Variable: Linear upsampling kernel\n    """"""\n\n    rank = len(list(kernel_spatial_shape))\n    assert 1 < rank < 4, \\\n        \'Transposed convolutions are only supported in 2D and 3D\'\n\n    kernel_shape = tuple(kernel_spatial_shape + [out_filters, in_filters])\n    size = kernel_spatial_shape\n    factor = (np.array(size) + 1) // 2\n    center = np.zeros_like(factor, np.float)\n\n    for i in range(len(factor)):\n        if size[i] % 2 == 1:\n            center[i] = factor[i] - 1\n        else:\n            center[i] = factor[i] - 0.5\n\n    weights = np.zeros(kernel_shape)\n    if rank == 2:\n        og = np.ogrid[:size[0], :size[1]]\n        x_filt = (1 - abs(og[0] - center[0]) / np.float(factor[0]))\n        y_filt = (1 - abs(og[1] - center[1]) / np.float(factor[1]))\n\n        filt = x_filt * y_filt\n\n        for i in range(out_filters):\n            weights[:, :, i, i] = filt\n    else:\n        og = np.ogrid[:size[0], :size[1], :size[2]]\n        x_filt = (1 - abs(og[0] - center[0]) / np.float(factor[0]))\n        y_filt = (1 - abs(og[1] - center[1]) / np.float(factor[1]))\n        z_filt = (1 - abs(og[2] - center[2]) / np.float(factor[2]))\n\n        filt = x_filt * y_filt * z_filt\n\n        for i in range(out_filters):\n            weights[:, :, :, i, i] = filt\n\n    init = tf.constant_initializer(value=weights, dtype=tf.float32)\n\n    return tf.get_variable(name=""linear_up_kernel"",\n                           initializer=init,\n                           shape=weights.shape,\n                           trainable=trainable)\n\n\ndef linear_upsample_3d(inputs,\n                       strides=(2, 2, 2),\n                       use_bias=False,\n                       trainable=False,\n                       name=\'linear_upsample_3d\'):\n    """"""Linear upsampling layer in 3D using strided transpose convolutions. The\n        upsampling kernel size will be automatically computed to avoid\n        information loss.\n\n    Args:\n        inputs (tf.Tensor): Input tensor to be upsampled\n        strides (tuple, optional): The strides determine the upsampling factor\n            in each dimension.\n        use_bias (bool, optional): Flag to train an additional bias.\n        trainable (bool, optional): Flag to set the variables to be trainable or not.\n        name (str, optional): Name of the layer.\n\n    Returns:\n        tf.Tensor: Upsampled Tensor\n    """"""\n    static_inp_shape = tuple(inputs.get_shape().as_list())\n    dyn_inp_shape = tf.shape(inputs)\n    rank = len(static_inp_shape)\n\n    num_filters = static_inp_shape[-1]\n    strides_5d = [1, ] + list(strides) + [1, ]\n    kernel_size = [2 * s if s > 1 else 1 for s in strides]\n\n    kernel = get_linear_upsampling_kernel(\n        kernel_spatial_shape=kernel_size,\n        out_filters=num_filters,\n        in_filters=num_filters,\n        trainable=trainable)\n\n    dyn_out_shape = [dyn_inp_shape[i] * strides_5d[i] for i in range(rank)]\n    dyn_out_shape[-1] = num_filters\n\n    static_out_shape = [static_inp_shape[i] * strides_5d[i]\n                        if isinstance(static_inp_shape[i], int)\n                        else None for i in range(rank)]\n\n    static_out_shape[-1] = num_filters\n    tf.logging.info(\'Upsampling from {} to {}\'.format(\n        static_inp_shape, static_out_shape))\n\n    upsampled = tf.nn.conv3d_transpose(\n        value=inputs,\n        filter=kernel,\n        output_shape=dyn_out_shape,\n        strides=strides_5d,\n        padding=\'SAME\',\n        name=\'upsample\')\n\n    upsampled.set_shape(static_out_shape)\n\n    return upsampled\n'"
dltk/io/__init__.py,0,b''
dltk/io/abstract_reader.py,11,"b'from __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport tensorflow as tf\nimport traceback\n\n\nclass IteratorInitializerHook(tf.train.SessionRunHook):\n    """"""Hook to initialise data iterator after Session is created.""""""\n\n    def __init__(self):\n        super(IteratorInitializerHook, self).__init__()\n        self.iterator_initializer_func = None\n\n    def after_create_session(self, session, coord):\n        """"""Initialise the iterator after the session has been created.""""""\n        self.iterator_initializer_func(session)\n\n\nclass Reader(object):\n    """"""Wrapper for dataset generation given a read function""""""\n\n    def __init__(self, read_fn, dtypes):\n        """"""Constructs a Reader instance\n\n        Args:\n            read_fn: Input function returning features which is a dictionary of\n                string feature name to `Tensor` or `SparseTensor`. If it\n                returns a tuple, first item is extracted as features.\n                Prediction continues until `input_fn` raises an end-of-input\n                exception (`OutOfRangeError` or `StopIteration`).\n            dtypes:  A nested structure of tf.DType objects corresponding to\n                each component of an element yielded by generator.\n\n        """"""\n        self.dtypes = dtypes\n\n        self.read_fn = read_fn\n\n    def get_inputs(self,\n                   file_references,\n                   mode,\n                   example_shapes=None,\n                   shuffle_cache_size=100,\n                   batch_size=4,\n                   params=None):\n        """"""\n        Function to provide the input_fn for a tf.Estimator.\n\n        Args:\n            file_references: An array like structure that holds the reference\n                to the file to read. It can also be None if not needed.\n            mode: A tf.estimator.ModeKeys. It is passed on to `read_fn` to\n                trigger specific functions there.\n            example_shapes (optional): A nested structure of lists or tuples\n                corresponding to the shape of each component of an element\n                yielded by generator.\n            shuffle_cache_size (int, optional): An `int` determining the\n                number of examples that are held in the shuffle queue.\n            batch_size (int, optional): An `int` specifying the number of\n                examples returned in a batch.\n            params (dict, optional): A `dict` passed on to the `read_fn`.\n\n        Returns:\n            function: a handle to the `input_fn` to be passed the relevant\n                tf estimator functions.\n            tf.train.SessionRunHook: A hook to initialize the queue within\n                the dataset.\n        """"""\n        iterator_initializer_hook = IteratorInitializerHook()\n\n        def train_inputs():\n            def f():\n                def clean_ex(ex, compare):\n                    # Clean example dictionary by recursively deleting\n                    # non-relevant entries. However, this does not look into\n                    # dictionaries nested into lists\n                    for k in list(ex.keys()):\n                        if k not in list(compare.keys()):\n                            del ex[k]\n                        elif isinstance(ex[k], dict) and isinstance(compare[k], dict):\n                            clean_ex(ex[k], compare[k])\n                        elif (isinstance(ex[k], dict) and not isinstance(compare[k], dict)) or \\\n                             (not isinstance(ex[k], dict) and isinstance(compare[k], dict)):\n                            raise ValueError(\'Entries between example and \'\n                                             \'dtypes incompatible for key {}\'\n                                             \'\'.format(k))\n                        elif (isinstance(ex[k], list) and not isinstance(compare[k], list)) or \\\n                            (not isinstance(ex[k], list) and isinstance(compare[k], list)) or \\\n                                (isinstance(ex[k], list) and isinstance(compare[k], list) and not\n                                    len(ex[k]) == len(compare[k])):\n                            raise ValueError(\'Entries between example and \'\n                                             \'dtypes incompatible for key {}\'\n                                             \'\'.format(k))\n                    for k in list(compare):\n                        if k not in list(ex.keys()):\n                            raise ValueError(\'Key {} not found in ex but is \'\n                                             \'present in dtypes. Found keys: \'\n                                             \'{}\'.format(k, ex.keys()))\n                    return ex\n\n                fn = self.read_fn(file_references, mode, params)\n                # iterate over all entries - this loop is terminated by the\n                # tf.errors.OutOfRangeError or StopIteration thrown by the\n                # read_fn\n                while True:\n                    try:\n                        ex = next(fn)\n\n                        if ex.get(\'labels\') is None:\n                            ex[\'labels\'] = None\n\n                        if not isinstance(ex, dict):\n                            raise ValueError(\'The read_fn has to return \'\n                                             \'dictionaries\')\n\n                        ex = clean_ex(ex, self.dtypes)\n                        yield ex\n                    except (tf.errors.OutOfRangeError, StopIteration):\n                        raise\n                    except Exception as e:\n                        print(\'got error `{} from `_read_sample`:\'.format(e))\n                        print(traceback.format_exc())\n                        raise\n\n            dataset = tf.data.Dataset.from_generator(\n                f, self.dtypes, example_shapes)\n            dataset = dataset.repeat(None)\n            dataset = dataset.shuffle(shuffle_cache_size)\n            dataset = dataset.batch(batch_size)\n            dataset = dataset.prefetch(1)\n\n            iterator = dataset.make_initializable_iterator()\n            next_dict = iterator.get_next()\n\n            # Set runhook to initialize iterator\n            iterator_initializer_hook.iterator_initializer_func = \\\n                lambda sess: sess.run(iterator.initializer)\n\n            # Return batched (features, labels)\n            return next_dict[\'features\'], next_dict.get(\'labels\')\n\n        # Return function and hook\n        return train_inputs, iterator_initializer_hook\n\n    def serving_input_receiver_fn(self, placeholder_shapes):\n        """"""Build the serving inputs.\n\n        Args:\n            placeholder_shapes: A nested structure of lists or tuples\n                corresponding to the shape of each component of the feature\n                elements yieled by the read_fn.\n\n        Returns:\n            function: A function to be passed to the tf.estimator.Estimator\n            instance when exporting a saved model with estimator.export_savedmodel.\n        """"""\n\n        def f():\n            inputs = {k: tf.placeholder(\n                shape=[None] + list(placeholder_shapes[\'features\'][k]),\n                dtype=self.dtypes[\'features\'][k]) for k in list(self.dtypes[\'features\'].keys())}\n\n            return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n        return f\n'"
dltk/io/augmentation.py,0,"b'from __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport numpy as np\nfrom scipy.ndimage.interpolation import map_coordinates\nfrom scipy.ndimage.filters import gaussian_filter\n\n\ndef flip(imagelist, axis=1):\n    """"""Randomly flip spatial dimensions\n\n    Args:\n        imagelist (np.ndarray or list or tuple): image(s) to be flipped\n        axis (int): axis along which to flip the images\n\n    Returns:\n        np.ndarray or list or tuple: same as imagelist but randomly flipped\n            along axis\n    """"""\n\n    # Check if a single image or a list of images has been passed\n    was_singular = False\n    if isinstance(imagelist, np.ndarray):\n        imagelist = [imagelist]\n        was_singular = True\n\n    # With a probility of 0.5 flip the image(s) across `axis`\n    do_flip = np.random.random(1)\n    if do_flip > 0.5:\n        for i in range(len(imagelist)):\n            imagelist[i] = np.flip(imagelist[i], axis=axis)\n    if was_singular:\n        return imagelist[0]\n    return imagelist\n\n\ndef add_gaussian_offset(image, sigma=0.1):\n    """"""\n    Add Gaussian offset to an image. Adds the offset to each channel\n    independently.\n\n    Args:\n        image (np.ndarray): image to add noise to\n        sigma (float): stddev of the Gaussian distribution to generate noise\n            from\n\n    Returns:\n        np.ndarray: same as image but with added offset to each channel\n    """"""\n\n    offsets = np.random.normal(0, sigma, ([1] * (image.ndim - 1) + [image.shape[-1]]))\n    image += offsets\n    return image\n\n\ndef add_gaussian_noise(image, sigma=0.05):\n    """"""\n    Add Gaussian noise to an image\n\n    Args:\n        image (np.ndarray): image to add noise to\n        sigma (float): stddev of the Gaussian distribution to generate noise\n            from\n\n    Returns:\n        np.ndarray: same as image but with added offset to each channel\n    """"""\n\n    image += np.random.normal(0, sigma, image.shape)\n    return image\n\n\ndef elastic_transform(image, alpha, sigma):\n    """"""\n    Elastic deformation of images as described in [1].\n\n    [1] Simard, Steinkraus and Platt, ""Best Practices for Convolutional\n        Neural Networks applied to Visual Document Analysis"", in Proc. of the\n        International Conference on Document Analysis and Recognition, 2003.\n\n    Based on gist https://gist.github.com/erniejunior/601cdf56d2b424757de5\n\n    Args:\n        image (np.ndarray): image to be deformed\n        alpha (list): scale of transformation for each dimension, where larger\n            values have more deformation\n        sigma (list): Gaussian window of deformation for each dimension, where\n            smaller values have more localised deformation\n\n    Returns:\n        np.ndarray: deformed image\n    """"""\n\n    assert len(alpha) == len(sigma), \\\n        ""Dimensions of alpha and sigma are different""\n\n    channelbool = image.ndim - len(alpha)\n    out = np.zeros((len(alpha) + channelbool, ) + image.shape)\n\n    # Generate a Gaussian filter, leaving channel dimensions zeroes\n    for jj in range(len(alpha)):\n        array = (np.random.rand(*image.shape) * 2 - 1)\n        out[jj] = gaussian_filter(array, sigma[jj],\n                                  mode=""constant"", cval=0) * alpha[jj]\n\n    # Map mask to indices\n    shapes = list(map(lambda x: slice(0, x, None), image.shape))\n    grid = np.broadcast_arrays(*np.ogrid[shapes])\n    indices = list(map((lambda x: np.reshape(x, (-1, 1))), grid + np.array(out)))\n\n    # Transform image based on masked indices\n    transformed_image = map_coordinates(image, indices, order=0,\n                                        mode=\'reflect\').reshape(image.shape)\n\n    return transformed_image\n\n\ndef extract_class_balanced_example_array(image,\n                                         label,\n                                         example_size=[1, 64, 64],\n                                         n_examples=1,\n                                         classes=2,\n                                         class_weights=None):\n    """"""Extract training examples from an image (and corresponding label) subject\n        to class balancing. Returns an image example array and the\n        corresponding label array.\n\n    Args:\n        image (np.ndarray): image to extract class-balanced patches from\n        label (np.ndarray): labels to use for balancing the classes\n        example_size (list or tuple): shape of the patches to extract\n        n_examples (int): number of patches to extract in total\n        classes (int or list or tuple): number of classes or list of classes\n            to extract\n\n    Returns:\n        np.ndarray, np.ndarray: class-balanced patches extracted from full\n            images with the shape [batch, example_size..., image_channels]\n    """"""\n    assert image.shape[:-1] == label.shape, \'Image and label shape must match\'\n    assert image.ndim - 1 == len(example_size), \\\n        \'Example size doesnt fit image size\'\n    assert all([i_s >= e_s for i_s, e_s in zip(image.shape, example_size)]), \\\n        \'Image must be larger than example shape\'\n    rank = len(example_size)\n\n    if isinstance(classes, int):\n        classes = tuple(range(classes))\n    n_classes = len(classes)\n\n    assert n_examples >= n_classes, \\\n        \'n_examples need to be greater than n_classes\'\n\n    if class_weights is None:\n        n_ex_per_class = np.ones(n_classes).astype(int) * int(np.round(n_examples / n_classes))\n    else:\n        assert len(class_weights) == n_classes, \\\n            \'Class_weights must match number of classes\'\n        class_weights = np.array(class_weights)\n        n_ex_per_class = np.round((class_weights / class_weights.sum()) * n_examples).astype(int)\n\n    # Compute an example radius to define the region to extract around a\n    # center location\n    ex_rad = np.array(list(zip(np.floor(np.array(example_size) / 2.0),\n                               np.ceil(np.array(example_size) / 2.0))),\n                      dtype=np.int)\n\n    class_ex_images = []\n    class_ex_lbls = []\n    min_ratio = 1.\n    for c_idx, c in enumerate(classes):\n        # Get valid, random center locations belonging to that class\n        idx = np.argwhere(label == c)\n\n        ex_images = []\n        ex_lbls = []\n\n        if len(idx) == 0 or n_ex_per_class[c_idx] == 0:\n            class_ex_images.append([])\n            class_ex_lbls.append([])\n            continue\n\n        # Extract random locations\n        r_idx_idx = np.random.choice(len(idx),\n                                     size=min(n_ex_per_class[c_idx], len(idx)),\n                                     replace=False).astype(int)\n        r_idx = idx[r_idx_idx]\n\n        # Shift the random to valid locations if necessary\n        r_idx = np.array(\n            [np.array([max(min(r[dim], image.shape[dim] - ex_rad[dim][1]),\n                           ex_rad[dim][0]) for dim in range(rank)])\n             for r in r_idx])\n\n        for i in range(len(r_idx)):\n            # Extract class-balanced examples from the original image\n            slicer = [slice(r_idx[i][dim] - ex_rad[dim][0], r_idx[i][dim] + ex_rad[dim][1]) for dim in range(rank)]\n\n            ex_image = image[slicer][np.newaxis, :]\n\n            ex_lbl = label[slicer][np.newaxis, :]\n\n            # Concatenate them and return the examples\n            ex_images = np.concatenate((ex_images, ex_image), axis=0) \\\n                if (len(ex_images) != 0) else ex_image\n            ex_lbls = np.concatenate((ex_lbls, ex_lbl), axis=0) \\\n                if (len(ex_lbls) != 0) else ex_lbl\n\n        class_ex_images.append(ex_images)\n        class_ex_lbls.append(ex_lbls)\n\n        ratio = n_ex_per_class[c_idx] / len(ex_images)\n        min_ratio = ratio if ratio < min_ratio else min_ratio\n\n    indices = np.floor(n_ex_per_class * min_ratio).astype(int)\n\n    ex_images = np.concatenate([cimage[:idxs] for cimage, idxs in zip(class_ex_images, indices)\n                                if len(cimage) > 0], axis=0)\n    ex_lbls = np.concatenate([clbl[:idxs] for clbl, idxs in zip(class_ex_lbls, indices)\n                              if len(clbl) > 0], axis=0)\n\n    return ex_images, ex_lbls\n\n\ndef extract_random_example_array(image_list,\n                                 example_size=[1, 64, 64],\n                                 n_examples=1):\n    """"""Randomly extract training examples from image (and a corresponding label).\n        Returns an image example array and the corresponding label array.\n\n    Args:\n        image_list (np.ndarray or list or tuple): image(s) to extract random\n            patches from\n        example_size (list or tuple): shape of the patches to extract\n        n_examples (int): number of patches to extract in total\n\n    Returns:\n        np.ndarray, np.ndarray: class-balanced patches extracted from full\n        images with the shape [batch, example_size..., image_channels]\n    """"""\n\n    assert n_examples > 0\n\n    was_singular = False\n    if isinstance(image_list, np.ndarray):\n        image_list = [image_list]\n        was_singular = True\n\n    assert all([i_s >= e_s for i_s, e_s in zip(image_list[0].shape, example_size)]), \\\n        \'Image must be bigger than example shape\'\n    assert (image_list[0].ndim - 1 == len(example_size) or image_list[0].ndim == len(example_size)), \\\n        \'Example size doesnt fit image size\'\n\n    for i in image_list:\n        if len(image_list) > 1:\n            assert (i.ndim - 1 == image_list[0].ndim or i.ndim == image_list[0].ndim or i.ndim + 1 == image_list[0].ndim),\\\n                \'Example size doesnt fit image size\'\n\n            assert all([i0_s == i_s for i0_s, i_s in zip(image_list[0].shape, i.shape)]), \\\n                \'Image shapes must match\'\n\n    rank = len(example_size)\n\n    # Extract random examples from image and label\n    valid_loc_range = [image_list[0].shape[i] - example_size[i] for i in range(rank)]\n\n    rnd_loc = [np.random.randint(valid_loc_range[dim], size=n_examples)\n               if valid_loc_range[dim] > 0\n               else np.zeros(n_examples, dtype=int) for dim in range(rank)]\n\n    examples = [[]] * len(image_list)\n    for i in range(n_examples):\n        slicer = [slice(rnd_loc[dim][i], rnd_loc[dim][i] + example_size[dim])\n                  for dim in range(rank)]\n\n        for j in range(len(image_list)):\n            ex_image = image_list[j][slicer][np.newaxis]\n            # Concatenate and return the examples\n            examples[j] = np.concatenate((examples[j], ex_image), axis=0) \\\n                if (len(examples[j]) != 0) else ex_image\n\n    if was_singular:\n        return examples[0]\n    return examples\n'"
dltk/io/preprocessing.py,0,"b'from __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport numpy as np\n\n\ndef whitening(image):\n    """"""Whitening. Normalises image to zero mean and unit variance.""""""\n\n    image = image.astype(np.float32)\n\n    mean = np.mean(image)\n    std = np.std(image)\n\n    if std > 0:\n        ret = (image - mean) / std\n    else:\n        ret = image * 0.\n    return ret\n\n\ndef normalise_zero_one(image):\n    """"""Image normalisation. Normalises image to fit [0, 1] range.""""""\n\n    image = image.astype(np.float32)\n\n    minimum = np.min(image)\n    maximum = np.max(image)\n\n    if maximum > minimum:\n        ret = (image - minimum) / (maximum - minimum)\n    else:\n        ret = image * 0.\n    return ret\n\n\ndef normalise_one_one(image):\n    """"""Image normalisation. Normalises image to fit [-1, 1] range.""""""\n\n    ret = normalise_zero_one(image)\n    ret *= 2.\n    ret -= 1.\n    return ret\n\n\ndef resize_image_with_crop_or_pad(image, img_size=(64, 64, 64), **kwargs):\n    """"""Image resizing. Resizes image by cropping or padding dimension\n     to fit specified size.\n\n    Args:\n        image (np.ndarray): image to be resized\n        img_size (list or tuple): new image size\n        kwargs (): additional arguments to be passed to np.pad\n\n    Returns:\n        np.ndarray: resized image\n    """"""\n\n    assert isinstance(image, (np.ndarray, np.generic))\n    assert (image.ndim - 1 == len(img_size) or image.ndim == len(img_size)), \\\n        \'Example size doesnt fit image size\'\n\n    # Get the image dimensionality\n    rank = len(img_size)\n\n    # Create placeholders for the new shape\n    from_indices = [[0, image.shape[dim]] for dim in range(rank)]\n    to_padding = [[0, 0] for dim in range(rank)]\n\n    slicer = [slice(None)] * rank\n\n    # For each dimensions find whether it is supposed to be cropped or padded\n    for i in range(rank):\n        if image.shape[i] < img_size[i]:\n            to_padding[i][0] = (img_size[i] - image.shape[i]) // 2\n            to_padding[i][1] = img_size[i] - image.shape[i] - to_padding[i][0]\n        else:\n            from_indices[i][0] = int(np.floor((image.shape[i] - img_size[i]) / 2.))\n            from_indices[i][1] = from_indices[i][0] + img_size[i]\n\n        # Create slicer object to crop or leave each dimension\n        slicer[i] = slice(from_indices[i][0], from_indices[i][1])\n\n    # Pad the cropped image to extend the missing dimension\n    return np.pad(image[slicer], to_padding, **kwargs)\n'"
dltk/networks/__init__.py,0,b''
docs/source/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# DLTK documentation build configuration file, created by\n# sphinx-quickstart on Mon Jun 12 11:24:11 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nimport sphinx_rtd_theme\nsys.path.insert(0, os.path.abspath(\'../../\'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\'sphinx.ext.autodoc\',\n              \'sphinx.ext.mathjax\',\n              \'sphinx.ext.viewcode\',\n              \'sphinx.ext.napoleon\',\n              \'sphinx.ext.doctest\',\n              \'sphinx.ext.intersphinx\']\n\nnapoleon_google_docstring = True\nnapoleon_use_param = False\nnapoleon_use_ivar = True\n\nautoclass_content = \'both\'\n\n# Markdown support, see <http://www.sphinx-doc.org/en/stable/markdown.html>\nsource_parsers = {\n    \'.md\': \'recommonmark.parser.CommonMarkParser\',\n}\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\nsource_suffix = [\'.rst\', \'.md\']\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'DLTK\'\ncopyright = u\'2017, DLTK contributors\'\nauthor = u\'DLTK contributors\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\n__version__ = None\nexec(open(\'../../dltk/version.py\').read())\nversion = ""."".join(__version__.split(""."")[:2])\n# The full version, including alpha/beta/rc tags.\nrelease = __version__\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {\n    \'collapse_navigation\': False,\n    \'display_version\': True,\n    \'logo_only\': True,\n}\n\nhtml_logo = \'_static/img/logo-dltk.svg\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\nhtml_context = {\'css_files\': [\'_static/theme_overrides.css\']}\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'DLTKdoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'DLTK.tex\', u\'DLTK Documentation\',\n     u\'DLTK Contributors\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'dltk\', u\'DLTK Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'DLTK\', u\'DLTK Documentation\',\n     author, \'DLTK\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n'"
dltk/networks/autoencoder/__init__.py,0,b''
dltk/networks/autoencoder/convolutional_autoencoder.py,32,"b'from __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport tensorflow as tf\nimport numpy as np\n\n\ndef convolutional_autoencoder_3d(inputs, num_convolutions=1,\n                                 num_hidden_units=128, filters=(16, 32, 64),\n                                 strides=((2, 2, 2), (2, 2, 2), (2, 2, 2)),\n                                 mode=tf.estimator.ModeKeys.TRAIN,\n                                 use_bias=False,\n                                 activation=tf.nn.relu6,\n                                 kernel_initializer=tf.initializers.variance_scaling(distribution=\'uniform\'),\n                                 bias_initializer=tf.zeros_initializer(),\n                                 kernel_regularizer=None,\n                                 bias_regularizer=None):\n    """"""Convolutional autoencoder with num_convolutions on len(filters)\n        resolution scales. The downsampling of features is done via strided\n        convolutions and upsampling via strided transpose convolutions. On each\n        resolution scale s are num_convolutions with filter size = filters[s].\n        strides[s] determine the downsampling factor at each resolution scale.\n\n    Args:\n        inputs (tf.Tensor): Input tensor to the network, required to be of\n            rank 5.\n        num_convolutions (int, optional): Number of convolutions per resolution\n            scale.\n        num_hidden_units (int, optional): Number of hidden units.\n        filters (tuple or list, optional): Number of filters for all\n            convolutions at each resolution scale.\n        strides (tuple or list, optional): Stride of the first convolution on a\n            resolution scale.\n        mode (str, optional): One of the tf.estimator.ModeKeys strings: TRAIN,\n            EVAL or PREDICT\n        use_bias (bool, optional): Boolean, whether the layer uses a bias.\n        activation (optional): A function to use as activation function.\n        kernel_initializer (TYPE, optional): An initializer for the convolution\n            kernel.\n        bias_initializer (TYPE, optional): An initializer for the bias vector.\n            If None, no bias will be applied.\n        kernel_regularizer (None, optional): Optional regularizer for the\n            convolution kernel.\n        bias_regularizer (None, optional): Optional regularizer for the bias\n            vector.\n\n    Returns:\n        dict: dictionary of output tensors\n\n    """"""\n    outputs = {}\n    assert len(strides) == len(filters)\n    assert len(inputs.get_shape().as_list()) == 5, \\\n        \'inputs are required to have a rank of 5.\'\n\n    conv_op = tf.layers.conv3d\n    tp_conv_op = tf.layers.conv3d_transpose\n\n    conv_params = {\'padding\': \'same\',\n                   \'use_bias\': use_bias,\n                   \'kernel_initializer\': kernel_initializer,\n                   \'bias_initializer\': bias_initializer,\n                   \'kernel_regularizer\': kernel_regularizer,\n                   \'bias_regularizer\': bias_regularizer}\n\n    x = inputs\n    tf.logging.info(\'Input tensor shape {}\'.format(x.get_shape()))\n\n    # Convolutional feature encoding blocks with num_convolutions at different\n    # resolution scales res_scales\n    for res_scale in range(0, len(filters)):\n\n        for i in range(0, num_convolutions - 1):\n            with tf.variable_scope(\'enc_unit_{}_{}\'.format(res_scale, i)):\n                x = conv_op(inputs=x,\n                            filters=filters[res_scale],\n                            kernel_size=(3, 3, 3),\n                            strides=(1, 1, 1),\n                            **conv_params)\n\n                x = tf.layers.batch_normalization(\n                    inputs=x,\n                    training=mode == tf.estimator.ModeKeys.TRAIN)\n                x = activation(x)\n                tf.logging.info(\'Encoder at res_scale {} shape: {}\'.format(\n                    res_scale, x.get_shape()))\n\n        # Employ strided convolutions to downsample\n        with tf.variable_scope(\'enc_unit_{}_{}\'.format(\n                res_scale,\n                num_convolutions)):\n\n            # Adjust the strided conv kernel size to prevent losing information\n            k_size = [s * 2 if s > 1 else 3 for s in strides[res_scale]]\n\n            x = conv_op(inputs=x,\n                        filters=filters[res_scale],\n                        kernel_size=k_size,\n                        strides=strides[res_scale],\n                        **conv_params)\n\n            x = tf.layers.batch_normalization(\n                x, training=mode == tf.estimator.ModeKeys.TRAIN)\n            x = activation(x)\n            tf.logging.info(\'Encoder at res_scale {} tensor shape: {}\'.format(\n                res_scale, x.get_shape()))\n\n    # Densely connected layer of hidden units\n    x_shape = x.get_shape().as_list()\n    x = tf.reshape(x, (tf.shape(x)[0], np.prod(x_shape[1:])))\n\n    x = tf.layers.dense(inputs=x,\n                        units=num_hidden_units,\n                        use_bias=conv_params[\'use_bias\'],\n                        kernel_initializer=conv_params[\'kernel_initializer\'],\n                        bias_initializer=conv_params[\'bias_initializer\'],\n                        kernel_regularizer=conv_params[\'kernel_regularizer\'],\n                        bias_regularizer=conv_params[\'bias_regularizer\'],\n                        name=\'hidden_units\')\n\n    outputs[\'hidden_units\'] = x\n    tf.logging.info(\'Hidden units tensor shape: {}\'.format(x.get_shape()))\n\n    x = tf.layers.dense(inputs=x,\n                        units=np.prod(x_shape[1:]),\n                        activation=activation,\n                        use_bias=conv_params[\'use_bias\'],\n                        kernel_initializer=conv_params[\'kernel_initializer\'],\n                        bias_initializer=conv_params[\'bias_initializer\'],\n                        kernel_regularizer=conv_params[\'kernel_regularizer\'],\n                        bias_regularizer=conv_params[\'bias_regularizer\'])\n\n    x = tf.reshape(x, [tf.shape(x)[0]] + list(x_shape)[1:])\n    tf.logging.info(\'Decoder input tensor shape: {}\'.format(x.get_shape()))\n\n    # Decoding blocks with num_convolutions at different resolution scales\n    # res_scales\n    for res_scale in reversed(range(0, len(filters))):\n\n        # Employ strided transpose convolutions to upsample\n        with tf.variable_scope(\'dec_unit_{}_0\'.format(res_scale)):\n\n            # Adjust the strided tp conv kernel size to prevent losing\n            # information\n            k_size = [s * 2 if s > 1 else 3 for s in strides[res_scale]]\n\n            x = tp_conv_op(inputs=x,\n                           filters=filters[res_scale],\n                           kernel_size=k_size,\n                           strides=strides[res_scale],\n                           **conv_params)\n\n            x = tf.layers.batch_normalization(\n                x, training=mode == tf.estimator.ModeKeys.TRAIN)\n            x = activation(x)\n            tf.logging.info(\'Decoder at res_scale {} tensor shape: {}\'.format(\n                res_scale, x.get_shape()))\n\n        for i in range(1, num_convolutions):\n            with tf.variable_scope(\'dec_unit_{}_{}\'.format(res_scale, i)):\n\n                x = conv_op(inputs=x,\n                            filters=filters[res_scale],\n                            kernel_size=(3, 3, 3),\n                            strides=(1, 1, 1),\n                            **conv_params)\n\n                x = tf.layers.batch_normalization(\n                    x, training=mode == tf.estimator.ModeKeys.TRAIN)\n                x = activation(x)\n            tf.logging.info(\'Decoder at res_scale {} tensor shape: {}\'.format(\n                res_scale, x.get_shape()))\n\n    # A final convolution reduces the number of output features to those of\n    # the inputs\n    x = conv_op(inputs=x,\n                filters=inputs.get_shape().as_list()[-1],\n                kernel_size=(1, 1, 1),\n                strides=(1, 1, 1),\n                **conv_params)\n\n    tf.logging.info(\'Output tensor shape: {}\'.format(x.get_shape()))\n    outputs[\'x_\'] = x\n\n    return outputs\n'"
dltk/networks/gan/__init__.py,0,b''
dltk/networks/gan/dcgan.py,26,"b'from __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport tensorflow as tf\nimport numpy as np\nfrom dltk.core.upsample import linear_upsample_3d\nfrom dltk.core.activations import leaky_relu\n\n\ndef dcgan_generator_3d(inputs,\n                       filters=(256, 128, 64, 32, 1),\n                       kernel_size=((4, 4, 4), (3, 3, 3), (3, 3, 3), (3, 3, 3),\n                                    (4, 4, 4)),\n                       strides=((4, 4, 4), (1, 2, 2), (1, 2, 2), (1, 2, 2),\n                                (1, 2, 2)),\n                       mode=tf.estimator.ModeKeys.TRAIN,\n                       use_bias=False):\n    """"""\n    Deep convolutional generative adversial network (DCGAN) generator\n    network. with num_convolutions on len(filters) resolution scales. The\n    upsampling of features is done via strided transpose convolutions. On\n    each resolution scale s are num_convolutions with filter size = filters[\n    s]. strides[s] determine the upsampling factor at each resolution scale.\n\n    Args:\n        inputs (tf.Tensor): Input noise tensor to the network.\n        out_filters (int): Number of output filters.\n        num_convolutions (int, optional): Number of convolutions per resolution\n            scale.\n        filters (tuple, optional): Number of filters for all convolutions at\n            each resolution scale.\n        strides (tuple, optional): Stride of the first convolution on a\n            resolution scale.\n        mode (TYPE, optional): One of the tf.estimator.ModeKeys strings: TRAIN,\n            EVAL or PREDICT\n        use_bias (bool, optional): Boolean, whether the layer uses a bias.\n\n    Returns:\n        dict: dictionary of output tensors\n\n    """"""\n    outputs = {}\n    assert len(strides) == len(filters)\n    assert len(inputs.get_shape().as_list()) == 5, \\\n        \'inputs are required to have a rank of 5.\'\n\n    conv_op = tf.layers.conv3d\n\n    conv_params = {\'padding\': \'same\',\n                   \'use_bias\': use_bias,\n                   \'kernel_initializer\': tf.uniform_unit_scaling_initializer(),\n                   \'bias_initializer\': tf.zeros_initializer(),\n                   \'kernel_regularizer\': None,\n                   \'bias_regularizer\': None}\n\n    x = inputs\n    tf.logging.info(\'Input tensor shape {}\'.format(x.get_shape()))\n\n    for res_scale in range(0, len(filters)):\n        with tf.variable_scope(\'gen_unit_{}\'.format(res_scale)):\n\n            tf.logging.info(\'Generator at res_scale before up {} tensor \'\n                            \'shape: {}\'.format(res_scale, x.get_shape()))\n\n            x = linear_upsample_3d(x, strides[res_scale], trainable=True)\n\n            x = conv_op(inputs=x,\n                        filters=filters[res_scale],\n                        kernel_size=kernel_size[res_scale],\n                        **conv_params)\n\n            tf.logging.info(\'Generator at res_scale after up {} tensor \'\n                            \'shape: {}\'.format(res_scale, x.get_shape()))\n\n            x = tf.layers.batch_normalization(x, training=mode == tf.estimator.ModeKeys.TRAIN)\n\n            x = leaky_relu(x, 0.2)\n            tf.logging.info(\'Generator at res_scale {} tensor shape: \'\n                            \'{}\'.format(res_scale, x.get_shape()))\n\n    outputs[\'gen\'] = x\n\n    return outputs\n\n\ndef dcgan_discriminator_3d(inputs,\n                           filters=(64, 128, 256, 512),\n                           strides=((2, 2, 2), (2, 2, 2), (1, 2, 2), (1, 2, 2)),\n                           mode=tf.estimator.ModeKeys.EVAL,\n                           use_bias=False):\n    """"""\n    Deep convolutional generative adversarial network (DCGAN) discriminator\n    network with num_convolutions on len(filters) resolution scales. The\n    downsampling of features is done via strided convolutions. On each\n    resolution scale s are num_convolutions with filter size = filters[s].\n    strides[s] determine the downsampling factor at each resolution scale.\n\n    Args:\n        inputs (tf.Tensor): Input tensor to the network, required to be of\n            rank 5.\n        num_convolutions (int, optional): Number of convolutions per resolution\n            scale.\n        filters (tuple, optional): Number of filters for all convolutions at\n            each resolution scale.\n        strides (tuple, optional): Stride of the first convolution on a\n            resolution scale.\n        mode (TYPE, optional): One of the tf.estimator.ModeKeys strings: TRAIN,\n            EVAL or PREDICT.\n        use_bias (bool, optional): Boolean, whether the layer uses a bias.\n\n    Returns:\n        dict: dictionary of output tensors\n\n    """"""\n    outputs = {}\n    assert len(strides) == len(filters)\n    assert len(inputs.get_shape().as_list()) == 5,\\\n        \'inputs are required to have a rank of 5.\'\n\n    conv_op = tf.layers.conv3d\n\n    conv_params = {\'padding\': \'same\',\n                   \'use_bias\': use_bias,\n                   \'kernel_initializer\': tf.uniform_unit_scaling_initializer(),\n                   \'bias_initializer\': tf.zeros_initializer(),\n                   \'kernel_regularizer\': None,\n                   \'bias_regularizer\': None}\n\n    x = inputs\n    tf.logging.info(\'Input tensor shape {}\'.format(x.get_shape()))\n\n    for res_scale in range(0, len(filters)):\n        with tf.variable_scope(\'disc_unit_{}\'.format(res_scale)):\n\n            x = conv_op(inputs=x,\n                        filters=filters[res_scale],\n                        kernel_size=(3, 3, 3),\n                        strides=strides[res_scale],\n                        **conv_params)\n\n            x = tf.layers.batch_normalization(\n                x, training=mode == tf.estimator.ModeKeys.TRAIN)\n\n            x = leaky_relu(x, 0.2)\n\n    x_shape = x.get_shape().as_list()\n    x = tf.reshape(x, (tf.shape(x)[0], np.prod(x_shape[1:])))\n\n    x = tf.layers.dense(inputs=x,\n                        units=1,\n                        use_bias=True,\n                        kernel_initializer=conv_params[\'kernel_initializer\'],\n                        bias_initializer=conv_params[\'bias_initializer\'],\n                        kernel_regularizer=conv_params[\'kernel_regularizer\'],\n                        bias_regularizer=conv_params[\'bias_regularizer\'],\n                        name=\'out\')\n\n    outputs[\'logits\'] = x\n\n    outputs[\'probs\'] = tf.nn.sigmoid(x)\n\n    outputs[\'pred\'] = tf.cast((x > 0.5), tf.int32)\n\n    return outputs\n'"
dltk/networks/regression_classification/__init__.py,0,b''
dltk/networks/regression_classification/resnet.py,24,"b'from __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport tensorflow as tf\n\nfrom dltk.core.residual_unit import vanilla_residual_unit_3d\n\n\ndef resnet_3d(inputs,\n              num_classes,\n              num_res_units=1,\n              filters=(16, 32, 64, 128),\n              strides=((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2)),\n              mode=tf.estimator.ModeKeys.EVAL,\n              use_bias=False,\n              activation=tf.nn.relu6,\n              kernel_initializer=tf.initializers.variance_scaling(distribution=\'uniform\'),\n              bias_initializer=tf.zeros_initializer(),\n              kernel_regularizer=None, bias_regularizer=None):\n    """"""\n    Regression/classification network based on a flexible resnet\n    architecture [1] using residual units proposed in [2]. The downsampling\n    of features is done via strided convolutions. On each resolution scale s\n    are num_convolutions with filter size = filters[s]. strides[s]\n    determine the downsampling factor at each resolution scale.\n\n    [1] K. He et al. Deep residual learning for image recognition. CVPR 2016.\n    [2] K. He et al. Identity Mappings in Deep Residual Networks. ECCV 2016.\n\n    Args:\n        inputs (tf.Tensor): Input feature tensor to the network (rank 5\n            required).\n        num_classes (int): Number of output channels or classes.\n        num_res_units (int, optional): Number of residual units per resolution\n            scale.\n        filters (tuple, optional): Number of filters for all residual units at\n            each resolution scale.\n        strides (tuple, optional): Stride of the first unit on a resolution\n            scale.\n        mode (TYPE, optional): One of the tf.estimator.ModeKeys strings: TRAIN,\n            EVAL or PREDICT\n        use_bias (bool, optional): Boolean, whether the layer uses a bias.\n        activation (optional): A function to use as activation function.\n        kernel_initializer (TYPE, optional): An initializer for the convolution\n            kernel.\n        bias_initializer (TYPE, optional): An initializer for the bias vector.\n            If None, no bias will be applied.\n        kernel_regularizer (None, optional): Optional regularizer for the\n            convolution kernel.\n        bias_regularizer (None, optional): Optional regularizer for the bias\n            vector.\n\n    Returns:\n        dict: dictionary of output tensors\n\n    """"""\n    outputs = {}\n    assert len(strides) == len(filters)\n    assert len(inputs.get_shape().as_list()) == 5, \\\n        \'inputs are required to have a rank of 5.\'\n\n    relu_op = tf.nn.relu6\n\n    conv_params = {\'padding\': \'same\',\n                   \'use_bias\': use_bias,\n                   \'kernel_initializer\': kernel_initializer,\n                   \'bias_initializer\': bias_initializer,\n                   \'kernel_regularizer\': kernel_regularizer,\n                   \'bias_regularizer\': bias_regularizer}\n\n    x = inputs\n\n    # Inital convolution with filters[0]\n    k = [s * 2 if s > 1 else 3 for s in strides[0]]\n    x = tf.layers.conv3d(x, filters[0], k, strides[0], **conv_params)\n    tf.logging.info(\'Init conv tensor shape {}\'.format(x.get_shape()))\n\n    # Residual feature encoding blocks with num_res_units at different\n    # resolution scales res_scales\n    res_scales = [x]\n    saved_strides = []\n    for res_scale in range(1, len(filters)):\n\n        # Features are downsampled via strided convolutions. These are defined\n        # in `strides` and subsequently saved\n        with tf.variable_scope(\'unit_{}_0\'.format(res_scale)):\n\n            x = vanilla_residual_unit_3d(\n                inputs=x,\n                out_filters=filters[res_scale],\n                strides=strides[res_scale],\n                activation=activation,\n                mode=mode)\n        saved_strides.append(strides[res_scale])\n\n        for i in range(1, num_res_units):\n\n            with tf.variable_scope(\'unit_{}_{}\'.format(res_scale, i)):\n\n                x = vanilla_residual_unit_3d(\n                    inputs=x,\n                    out_filters=filters[res_scale],\n                    strides=(1, 1, 1),\n                    activation=activation,\n                    mode=mode)\n        res_scales.append(x)\n        tf.logging.info(\'Encoder at res_scale {} tensor shape: {}\'.format(\n            res_scale, x.get_shape()))\n\n    # Global pool and last unit\n    with tf.variable_scope(\'pool\'):\n        x = tf.layers.batch_normalization(\n            x, training=mode == tf.estimator.ModeKeys.TRAIN)\n        x = relu_op(x)\n\n        axis = tuple(range(len(x.get_shape().as_list())))[1:-1]\n        x = tf.reduce_mean(x, axis=axis, name=\'global_avg_pool\')\n\n        tf.logging.info(\'Global pool shape {}\'.format(x.get_shape()))\n\n    with tf.variable_scope(\'last\'):\n        x = tf.layers.dense(inputs=x,\n                            units=num_classes,\n                            activation=None,\n                            use_bias=conv_params[\'use_bias\'],\n                            kernel_initializer=conv_params[\'kernel_initializer\'],\n                            bias_initializer=conv_params[\'bias_initializer\'],\n                            kernel_regularizer=conv_params[\'kernel_regularizer\'],\n                            bias_regularizer=conv_params[\'bias_regularizer\'],\n                            name=\'hidden_units\')\n\n        tf.logging.info(\'Output tensor shape {}\'.format(x.get_shape()))\n\n    # Define the outputs\n    outputs[\'logits\'] = x\n\n    with tf.variable_scope(\'pred\'):\n\n        y_prob = tf.nn.softmax(x)\n        outputs[\'y_prob\'] = y_prob\n\n        y_ = tf.argmax(x, axis=-1) \\\n            if num_classes > 1 \\\n            else tf.cast(tf.greater_equal(x[..., 0], 0.5), tf.int32)\n        outputs[\'y_\'] = y_\n\n    return outputs\n'"
dltk/networks/segmentation/__init__.py,0,b''
dltk/networks/segmentation/deepmedic.py,40,"b'# WARNING/NOTE\n# This implementation is work in progress and an attempt to implement a\n# scalable version of the original DeepMedic [1] source. It will NOT\n# yield the same accuracy performance as described in the paper.\n# If you are running comparative experiments, please refer to the\n# original code base in [1].\n#\n# [1] https://github.com/Kamnitsask/deepmedic\n\nfrom __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport tensorflow as tf\n\nfrom dltk.core.upsample import linear_upsample_3d\nfrom dltk.core.activations import prelu, leaky_relu\n\n\ndef crop_central_block(x, size):\n    assert all([i >= s for i, s in zip(x.get_shape().as_list()[1:], size)]), \\\n        \'Output size must not be bigger than input size. But was {} compared \' \\\n        \'to {}\'.format(x.get_shape().as_list()[1:], size)\n\n    slicer = [slice(None)] * len(x.get_shape().as_list())\n\n    for i in range(len(size)):\n        # use i + 1 to account for batch dimension\n        start = (x.get_shape().as_list()[i + 1] - size[i]) // 2\n        end = start + size[i]\n        slicer[i + 1] = slice(start, end)\n\n    return x[slicer]\n\n\ndef deepmedic_3d(inputs, num_classes,\n                 normal_filters=(30, 30, 40, 40, 40, 40, 50, 50),\n                 normal_strides=((1, 1, 1), (1, 1, 1), (1, 1, 1), (1, 1, 1),\n                                 (1, 1, 1), (1, 1, 1), (1, 1, 1), (1, 1, 1)),\n                 normal_kernels=((3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3),\n                                 (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3)),\n                 normal_residuals=(4, 6, 8),\n                 normal_input_shape=(25, 25, 25),\n                 subsampled_filters=((30, 30, 40, 40, 40, 40, 50, 50),),\n                 subsampled_strides=(((1, 1, 1), (1, 1, 1), (1, 1, 1),\n                                      (1, 1, 1), (1, 1, 1), (1, 1, 1),\n                                      (1, 1, 1), (1, 1, 1)),),\n                 subsampled_kernels=(((3, 3, 3), (3, 3, 3), (3, 3, 3),\n                                      (3, 3, 3), (3, 3, 3), (3, 3, 3),\n                                      (3, 3, 3), (3, 3, 3)),),\n                 subsampled_residuals=((4, 6, 8),),\n                 subsampled_input_shapes=((57, 57, 57),),\n                 subsample_factors=((3, 3, 3),),\n                 fc_filters=(150, 150),\n                 first_fc_kernel=(3, 3, 3),\n                 fc_residuals=(2, ),\n                 padding=\'VALID\',\n                 use_prelu=True,\n                 mode=tf.estimator.ModeKeys.EVAL,\n                 use_bias=True,\n                 kernel_initializer=tf.initializers.variance_scaling(distribution=\'uniform\'),\n                 bias_initializer=tf.zeros_initializer(),\n                 kernel_regularizer=None,\n                 bias_regularizer=None):\n    """"""\n    Image segmentation network based on a DeepMedic architecture [1, 2].\n    Downsampling of features is done via strided convolutions. The architecture\n    uses multiple processing paths with different resolutions. The different\n    pathways are concatenated and then fed to the convolutional fc layers.\n\n    [1] Konstantinos Kamnitsas et al. Efficient Multi-Scale 3D CNN with Fully\n        Connected CRF for Accurate Brain Lesion Segmentation. Medical Image\n        Analysis, 2016.\n    [2] Konstantinos Kamnitsas et al. Multi-Scale 3D CNNs for segmentation of\n        brain Lesions in multi-modal MRI. ISLES challenge, MICCAI 2015.\n\n    Note: We are currently using bilinear upsampling whereas the original\n    implementation (https://github.com/Kamnitsask/deepmedic) uses repeat\n    upsampling.\n\n    Args:\n        inputs (tf.Tensor): Input feature tensor to the network (rank 5\n            required).\n        num_classes (int): Number of output classes.\n        normal_filters (array_like, optional): Number of filters for each layer\n            for normal path.\n        normal_strides (array_like, optional): Strides for each layer for\n            normal path.\n        normal_kernels (array_like, optional): Kernel size for each layer for\n            normal path.\n        normal_residuals (array_like, optional): Location of residual\n            connections for normal path.\n        normal_input_shape (array_like, optional): Shape of input to normal\n            path. Input to the network is center cropped to this shape.\n        subsampled_filters (array_like, optional): Number of filters for each\n            layer for each subsampled path.\n        subsampled_strides (array_like, optional): Strides for each layer for\n            each subsampled path.\n        subsampled_kernels (array_like, optional): Kernel size for each layer\n            for each subsampled path.\n        subsampled_residuals (array_like, optional): Location of residual\n            connections for each subsampled path.\n        subsampled_input_shapes (array_like, optional): Shape of input to\n            subsampled paths. Input to the network is downsampled and then\n            center cropped to this shape.\n        subsample_factors (array_like, optional): Downsampling factors for\n            each subsampled path.\n        fc_filters (array_like, optional): Number of filters for the fc layers.\n        first_fc_kernel (array_like, optional): Shape of the kernel of the\n            first fc layer.\n        fc_residuals (array_like, optional): Location of residual connections\n            for the fc layers.\n        padding (string, optional): Type of padding used for convolutions.\n            Standard is `VALID`\n        use_prelu (bool, optional): Flag to enable PReLU activation.\n            Alternatively leaky ReLU is used. Defaults to `True`.\n        mode (TYPE, optional): One of the tf.estimator.ModeKeys strings: TRAIN,\n            EVAL or PREDICT\n        use_bias (bool, optional): Boolean, whether the layer uses a bias.\n        kernel_initializer (TYPE, optional): An initializer for the convolution\n            kernel.\n        bias_initializer (TYPE, optional): An initializer for the bias vector.\n            If None, no bias will be applied.\n        kernel_regularizer (None, optional): Optional regularizer for the\n            convolution kernel.\n        bias_regularizer (None, optional): Optional regularizer for the bias\n            vector.\n\n    Returns:\n        dict: dictionary of output tensors\n\n    """"""\n    outputs = {}\n    assert len(normal_filters) == len(normal_strides)\n    assert len(normal_filters) == len(normal_kernels)\n    assert len(inputs.get_shape().as_list()) == 5, \\\n        \'inputs are required to have a rank of 5.\'\n\n    conv_params = {\'use_bias\': use_bias,\n                   \'kernel_initializer\': kernel_initializer,\n                   \'bias_initializer\': bias_initializer,\n                   \'kernel_regularizer\': kernel_regularizer,\n                   \'bias_regularizer\': bias_regularizer,\n                   \'padding\': padding}\n\n    def _residual_connection(x, prev_x):\n        # crop previous to current size:\n        prev_x = crop_central_block(prev_x, x.get_shape().as_list()[1:-1])\n\n        # add prev_x to first channels of x\n\n        to_pad = [[0, 0]] * (len(x.get_shape().as_list()) - 1)\n        to_pad += [[0, x.get_shape().as_list()[-1] - prev_x.get_shape().as_list()[-1]]]\n        prev_x = tf.pad(prev_x, to_pad)\n\n        return x + prev_x\n\n    def _build_normal_pathway(x):\n        with tf.variable_scope(\'normal_pathway\'):\n            tf.logging.info(\'Building normal pathway\')\n            center_crop = crop_central_block(x, normal_input_shape)\n            tf.logging.info(\'Input is {}\'.format(\n                center_crop.get_shape().as_list()))\n\n            layers = []\n\n            x = center_crop\n            for i in range(len(normal_filters)):\n                with tf.variable_scope(\'layer_{}\'.format(i)):\n                    layers.append(x)\n                    if i > 0:\n                        x = tf.layers.batch_normalization(\n                            x, training=mode == tf.estimator.ModeKeys.TRAIN)\n                        x = prelu(x) if use_prelu else leaky_relu(x, 0.01)\n                    x = tf.layers.conv3d(x,\n                                         normal_filters[i],\n                                         normal_kernels[i],\n                                         normal_strides[i],\n                                         **conv_params)\n                    # TODO: add pooling and dropout?!\n                    if i + 1 in normal_residuals:\n                        x = _residual_connection(x, layers[i - 1])\n                    tf.logging.info(\'Output of layer {} is {}\'.format(\n                        i, x.get_shape().as_list()))\n        tf.logging.info(\'Output is {}\'.format(x.get_shape().as_list()))\n        return x\n\n    def _downsample(x, factor):\n        if isinstance(factor, int):\n            factor = [factor] * (len(x.get_shape().as_list()) - 2)\n        pool_func = tf.nn.avg_pool3d\n\n        factor = list(factor)\n\n        x = pool_func(x, [1, ] + factor + [1, ], [1, ] + factor + [1, ],\n                      \'VALID\')\n        return x\n\n    def _build_subsampled_pathways(x):\n        pathways = []\n        for pathway in range(len(subsample_factors)):\n            with tf.variable_scope(\'subsampled_pathway_{}\'.format(pathway)):\n                tf.logging.info(\n                    \'Building subsampled pathway {}\'.format(pathway))\n                center_crop = crop_central_block(\n                    x, subsampled_input_shapes[pathway])\n                tf.logging.info(\'Input is {}\'.format(\n                    center_crop.get_shape().as_list()))\n\n                layers = []\n\n                x = center_crop\n                x = _downsample(x, subsample_factors[pathway])\n                tf.logging.info(\'Downsampled input is {}\'.format(\n                    x.get_shape().as_list()))\n\n                for i in range(len(subsampled_filters[pathway])):\n                    with tf.variable_scope(\'layer_{}\'.format(i)):\n                        layers.append(x)\n                        if i > 0:\n                            x = tf.layers.batch_normalization(\n                                x, training=mode == tf.estimator.ModeKeys.TRAIN)\n                            x = prelu(x) if use_prelu else leaky_relu(x, 0.01)\n                        x = tf.layers.conv3d(x, subsampled_filters[pathway][i],\n                                             subsampled_kernels[pathway][i],\n                                             subsampled_strides[pathway][i],\n                                             **conv_params)\n                        # TODO: add pooling and dropout?!\n                        if i + 1 in subsampled_residuals:\n                            x = _residual_connection(x, layers[i - 1])\n                        tf.logging.info(\'Output of layer {} is {}\'.format(\n                            i, x.get_shape().as_list()))\n\n                x = _upsample(x, subsample_factors[pathway])\n                tf.logging.info(\'Output is {}\'.format(x.get_shape().as_list()))\n                pathways.append(x)\n        return pathways\n\n    def _upsample(x, factor):\n        if isinstance(factor, int):\n            factor = [factor] * (len(x.get_shape().as_list()) - 2)\n\n        # TODO: build repeat upsampling\n\n        x = linear_upsample_3d(x, strides=factor)\n        return x\n\n    x = inputs\n\n    normal = _build_normal_pathway(x)\n    pathways = _build_subsampled_pathways(x)\n\n    normal_shape = normal.get_shape().as_list()[1:-1]\n    paths = [normal]\n    for x in pathways:\n        paths.append(crop_central_block(x, normal_shape))\n\n    x = tf.concat(paths, -1)\n\n    layers = []\n    for i in range(len(fc_filters)):\n        with tf.variable_scope(\'fc_{}\'.format(i)):\n            layers.append(x)\n            if i == 0 and any([k > 1 for k in first_fc_kernel]):\n                x_shape = x.get_shape().as_list()\n                # CAUTION: https://docs.python.org/2/faq/programming.html#how-do-i-create-a-multidimensional-list\n                x_pad = [[0, 0] for _ in range(len(x_shape))]\n                for j in range(len(first_fc_kernel)):\n                    to_pad = (first_fc_kernel[j] - 1)\n                    x_pad[j + 1][0] = to_pad // 2\n                    x_pad[j + 1][1] = to_pad - x_pad[j + 1][0]\n                    print(x_pad)\n                x = tf.pad(x, x_pad, mode=\'SYMMETRIC\')\n\n            x = tf.layers.batch_normalization(\n                x, training=mode == tf.estimator.ModeKeys.TRAIN)\n            x = prelu(x) if use_prelu else leaky_relu(x, 0.01)\n            x = tf.layers.conv3d(x, fc_filters[i],\n                                 first_fc_kernel if i == 0 else 1,\n                                 **conv_params)\n            if i + 1 in fc_residuals:\n                x = _residual_connection(x, layers[i - 1])\n\n    with tf.variable_scope(\'last\'):\n        x = tf.layers.batch_normalization(\n            x, training=mode == tf.estimator.ModeKeys.TRAIN)\n        x = prelu(x) if use_prelu else leaky_relu(x, 0.01)\n        conv_params[\'use_bias\'] = True\n        x = tf.layers.conv3d(x, num_classes, 1, **conv_params)\n\n    outputs[\'logits\'] = x\n    tf.logging.info(\'last conv shape %s\', x.get_shape())\n\n    with tf.variable_scope(\'pred\'):\n        y_prob = tf.nn.softmax(x)\n        outputs[\'y_prob\'] = y_prob\n        y_ = tf.argmax(x, axis=-1)\n        outputs[\'y_\'] = y_\n\n    return outputs\n'"
dltk/networks/segmentation/fcn.py,32,"b'from __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport tensorflow as tf\n\nfrom dltk.core.residual_unit import vanilla_residual_unit_3d\nfrom dltk.core.upsample import linear_upsample_3d\n\n\ndef upscore_layer_3d(inputs,\n                     inputs2,\n                     out_filters,\n                     in_filters=None,\n                     strides=(2, 2, 2),\n                     mode=tf.estimator.ModeKeys.EVAL,\n                     use_bias=False,\n                     kernel_initializer=tf.initializers.variance_scaling(distribution=\'uniform\'),\n                     bias_initializer=tf.zeros_initializer(),\n                     kernel_regularizer=None,\n                     bias_regularizer=None):\n    """"""Upscore layer according to [1].\n\n    [1] J. Long et al. Fully convolutional networks for semantic segmentation.\n    CVPR 2015.\n\n    Args:\n        inputs (tf.Tensor): Input features to be upscored.\n        inputs2 (tf.Tensor): Higher resolution features from the encoder to add.\n            out_filters (int): Number of output filters (typically, number of\n            segmentation classes)\n        in_filters (None, optional): None or number of input filters.\n        strides (tuple, optional): Upsampling factor for a strided transpose\n            convolution.\n        mode (TYPE, optional): One of the tf.estimator.ModeKeys strings: TRAIN,\n            EVAL or PREDICT\n        use_bias (bool, optional): Boolean, whether the layer uses a bias.\n        kernel_initializer (TYPE, optional): An initializer for the convolution\n            kernel.\n        bias_initializer (TYPE, optional): An initializer for the bias vector.\n            If None, no bias will be applied.\n        kernel_regularizer (None, optional): Optional regularizer for the\n            convolution kernel.\n        bias_regularizer (None, optional): Optional regularizer for the bias\n            vector.\n\n    Returns:\n        tf.Tensor: Upscore tensor\n\n    """"""\n    conv_params = {\'use_bias\': use_bias,\n                   \'kernel_initializer\': kernel_initializer,\n                   \'bias_initializer\': bias_initializer,\n                   \'kernel_regularizer\': kernel_regularizer,\n                   \'bias_regularizer\': bias_regularizer}\n\n    # Compute an upsampling shape dynamically from the input tensor. Input\n    # filters are required to be static.\n    if in_filters is None:\n        in_filters = inputs.get_shape().as_list()[-1]\n\n    assert len(inputs.get_shape().as_list()) == 5, \\\n        \'inputs are required to have a rank of 5.\'\n    assert len(inputs.get_shape().as_list()) == len(inputs2.get_shape().as_list()), \\\n        \'Ranks of input and input2 differ\'\n\n    # Account for differences in the number of input and output filters\n    if in_filters != out_filters:\n        x = tf.layers.conv3d(inputs=inputs,\n                             filters=out_filters,\n                             kernel_size=(1, 1, 1),\n                             strides=(1, 1, 1),\n                             padding=\'same\',\n                             name=\'filter_conversion\',\n                             **conv_params)\n    else:\n        x = inputs\n\n    # Upsample inputs\n    x = linear_upsample_3d(inputs=x, strides=strides)\n\n    # Skip connection\n    x2 = tf.layers.conv3d(inputs=inputs2,\n                          filters=out_filters,\n                          kernel_size=(1, 1, 1),\n                          strides=(1, 1, 1),\n                          padding=\'same\',\n                          **conv_params)\n\n    x2 = tf.layers.batch_normalization(\n        x2, training=mode == tf.estimator.ModeKeys.TRAIN)\n\n    # Return the element-wise sum\n    return tf.add(x, x2)\n\n\ndef residual_fcn_3d(inputs,\n                    num_classes,\n                    num_res_units=1,\n                    filters=(16, 32, 64, 128),\n                    strides=((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2)),\n                    mode=tf.estimator.ModeKeys.EVAL,\n                    use_bias=False,\n                    activation=tf.nn.relu6,\n                    kernel_initializer=tf.initializers.variance_scaling(distribution=\'uniform\'),\n                    bias_initializer=tf.zeros_initializer(),\n                    kernel_regularizer=None,\n                    bias_regularizer=None):\n    """"""\n    Image segmentation network based on an FCN architecture [1] using\n    residual units [2] as feature extractors. Downsampling and upsampling\n    of features is done via strided convolutions and transpose convolutions,\n    respectively. On each resolution scale s are num_residual_units with\n    filter size = filters[s]. strides[s] determine the downsampling factor\n    at each resolution scale.\n\n    [1] J. Long et al. Fully convolutional networks for semantic segmentation.\n        CVPR 2015.\n    [2] K. He et al. Identity Mappings in Deep Residual Networks. ECCV 2016.\n\n    Args:\n        inputs (tf.Tensor): Input feature tensor to the network (rank 5\n            required).\n        num_classes (int): Number of output classes.\n        num_res_units (int, optional): Number of residual units at each\n            resolution scale.\n        filters (tuple, optional): Number of filters for all residual units at\n            each resolution scale.\n        strides (tuple, optional): Stride of the first unit on a resolution\n            scale.\n        mode (TYPE, optional): One of the tf.estimator.ModeKeys strings:\n            TRAIN, EVAL or PREDICT\n        use_bias (bool, optional): Boolean, whether the layer uses a bias.\n        activation (optional): A function to use as activation function.\n        kernel_initializer (TYPE, optional): An initializer for the convolution\n            kernel.\n        bias_initializer (TYPE, optional): An initializer for the bias vector.\n            If None, no bias will be applied.\n        kernel_regularizer (None, optional): Optional regularizer for the\n            convolution kernel.\n        bias_regularizer (None, optional): Optional regularizer for the bias\n            vector.\n\n    Returns:\n        dict: dictionary of output tensors\n    """"""\n    outputs = {}\n    assert len(strides) == len(filters)\n    assert len(inputs.get_shape().as_list()) == 5, \\\n        \'inputs are required to have a rank of 5.\'\n\n    conv_params = {\'use_bias\': use_bias,\n                   \'kernel_initializer\': kernel_initializer,\n                   \'bias_initializer\': bias_initializer,\n                   \'kernel_regularizer\': kernel_regularizer,\n                   \'bias_regularizer\': bias_regularizer}\n\n    x = inputs\n\n    # Inital convolution with filters[0]\n    x = tf.layers.conv3d(inputs=x,\n                         filters=filters[0],\n                         kernel_size=(3, 3, 3),\n                         strides=strides[0],\n                         padding=\'same\',\n                         **conv_params)\n\n    tf.logging.info(\'Init conv tensor shape {}\'.format(x.get_shape()))\n\n    # Residual feature encoding blocks with num_res_units at different\n    # resolution scales res_scales\n    res_scales = [x]\n    saved_strides = []\n    for res_scale in range(1, len(filters)):\n\n        # Features are downsampled via strided convolutions. These are defined\n        # in `strides` and subsequently saved\n        with tf.variable_scope(\'unit_{}_0\'.format(res_scale)):\n\n            x = vanilla_residual_unit_3d(\n                inputs=x,\n                out_filters=filters[res_scale],\n                strides=strides[res_scale],\n                activation=activation,\n                mode=mode)\n        saved_strides.append(strides[res_scale])\n\n        for i in range(1, num_res_units):\n\n            with tf.variable_scope(\'unit_{}_{}\'.format(res_scale, i)):\n\n                x = vanilla_residual_unit_3d(\n                    inputs=x,\n                    out_filters=filters[res_scale],\n                    strides=(1, 1, 1),\n                    activation=activation,\n                    mode=mode)\n        res_scales.append(x)\n\n        tf.logging.info(\'Encoder at res_scale {} tensor shape: {}\'.format(\n            res_scale, x.get_shape()))\n\n    # Upscore layers [2] reconstruct the predictions to higher resolution\n    # scales\n    for res_scale in range(len(filters) - 2, -1, -1):\n\n        with tf.variable_scope(\'upscore_{}\'.format(res_scale)):\n\n            x = upscore_layer_3d(\n                inputs=x,\n                inputs2=res_scales[res_scale],\n                out_filters=num_classes,\n                strides=saved_strides[res_scale],\n                mode=mode,\n                **conv_params)\n\n        tf.logging.info(\'Decoder at res_scale {} tensor shape: {}\'.format(\n            res_scale, x.get_shape()))\n\n    # Last convolution\n    with tf.variable_scope(\'last\'):\n        x = tf.layers.conv3d(inputs=x,\n                             filters=num_classes,\n                             kernel_size=(1, 1, 1),\n                             strides=(1, 1, 1),\n                             padding=\'same\',\n                             **conv_params)\n\n    tf.logging.info(\'Output tensor shape {}\'.format(x.get_shape()))\n\n    # Define the outputs\n    outputs[\'logits\'] = x\n\n    with tf.variable_scope(\'pred\'):\n\n        y_prob = tf.nn.softmax(x)\n        outputs[\'y_prob\'] = y_prob\n\n        y_ = tf.argmax(x, axis=-1) \\\n            if num_classes > 1 \\\n            else tf.cast(tf.greater_equal(x[..., 0], 0.5), tf.int32)\n\n        outputs[\'y_\'] = y_\n\n    return outputs\n'"
dltk/networks/segmentation/unet.py,42,"b'from __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport tensorflow as tf\n\nfrom dltk.core.residual_unit import vanilla_residual_unit_3d\nfrom dltk.core.upsample import linear_upsample_3d\nfrom dltk.core.activations import leaky_relu\n\n\ndef upsample_and_concat(inputs, inputs2, strides=(2, 2, 2)):\n    """"""Upsampling and concatenation layer according to [1].\n\n    [1] O. Ronneberger et al. U-Net: Convolutional Networks for Biomedical Image\n        Segmentation. MICCAI 2015.\n\n    Args:\n        inputs (TYPE): Input features to be upsampled.\n        inputs2 (TYPE): Higher resolution features from the encoder to\n            concatenate.\n        strides (tuple, optional): Upsampling factor for a strided transpose\n            convolution.\n\n    Returns:\n        tf.Tensor: Upsampled feature tensor\n    """"""\n    assert len(inputs.get_shape().as_list()) == 5, \\\n        \'inputs are required to have a rank of 5.\'\n    assert len(inputs.get_shape().as_list()) == len(inputs2.get_shape().as_list()), \\\n        \'Ranks of input and input2 differ\'\n\n    # Upsample inputs\n    inputs = linear_upsample_3d(inputs, strides)\n\n    return tf.concat(axis=-1, values=[inputs2, inputs])\n\n\ndef residual_unet_3d(inputs,\n                     num_classes,\n                     num_res_units=1,\n                     filters=(16, 32, 64, 128),\n                     strides=((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2)),\n                     mode=tf.estimator.ModeKeys.EVAL,\n                     use_bias=False,\n                     activation=leaky_relu,\n                     kernel_initializer=tf.initializers.variance_scaling(distribution=\'uniform\'),\n                     bias_initializer=tf.zeros_initializer(),\n                     kernel_regularizer=None,\n                     bias_regularizer=None):\n    """"""\n    Image segmentation network based on a flexible UNET architecture [1]\n    using residual units [2] as feature extractors. Downsampling and\n    upsampling of features is done via strided convolutions and transpose\n    convolutions, respectively. On each resolution scale s are\n    num_residual_units with filter size = filters[s]. strides[s] determine\n    the downsampling factor at each resolution scale.\n\n    [1] O. Ronneberger et al. U-Net: Convolutional Networks for Biomedical Image\n        Segmentation. MICCAI 2015.\n    [2] K. He et al. Identity Mappings in Deep Residual Networks. ECCV 2016.\n\n    Args:\n        inputs (tf.Tensor): Input feature tensor to the network (rank 5\n            required).\n        num_classes (int): Number of output classes.\n        num_res_units (int, optional): Number of residual units at each\n            resolution scale.\n        filters (tuple, optional): Number of filters for all residual units at\n            each resolution scale.\n        strides (tuple, optional): Stride of the first unit on a resolution\n            scale.\n        mode (TYPE, optional): One of the tf.estimator.ModeKeys strings: TRAIN,\n            EVAL or PREDICT\n        use_bias (bool, optional): Boolean, whether the layer uses a bias.\n        activation (optional): A function to use as activation function.\n        kernel_initializer (TYPE, optional): An initializer for the convolution\n            kernel.\n        bias_initializer (TYPE, optional): An initializer for the bias vector.\n            If None, no bias will be applied.\n        kernel_regularizer (None, optional): Optional regularizer for the\n            convolution kernel.\n        bias_regularizer (None, optional): Optional regularizer for the bias\n            vector.\n\n    Returns:\n        dict: dictionary of output tensors\n\n    """"""\n    outputs = {}\n    assert len(strides) == len(filters)\n    assert len(inputs.get_shape().as_list()) == 5, \\\n        \'inputs are required to have a rank of 5.\'\n\n    conv_params = {\'padding\': \'same\',\n                   \'use_bias\': use_bias,\n                   \'kernel_initializer\': kernel_initializer,\n                   \'bias_initializer\': bias_initializer,\n                   \'kernel_regularizer\': kernel_regularizer,\n                   \'bias_regularizer\': bias_regularizer}\n\n    x = inputs\n\n    # Initial convolution with filters[0]\n    x = tf.layers.conv3d(inputs=x,\n                         filters=filters[0],\n                         kernel_size=(3, 3, 3),\n                         strides=strides[0],\n                         **conv_params)\n\n    tf.logging.info(\'Init conv tensor shape {}\'.format(x.get_shape()))\n\n    # Residual feature encoding blocks with num_res_units at different\n    # resolution scales res_scales\n    res_scales = [x]\n    saved_strides = []\n    for res_scale in range(1, len(filters)):\n\n        # Features are downsampled via strided convolutions. These are defined\n        # in `strides` and subsequently saved\n        with tf.variable_scope(\'enc_unit_{}_0\'.format(res_scale)):\n\n            x = vanilla_residual_unit_3d(\n                inputs=x,\n                out_filters=filters[res_scale],\n                strides=strides[res_scale],\n                activation=activation,\n                mode=mode)\n        saved_strides.append(strides[res_scale])\n\n        for i in range(1, num_res_units):\n\n            with tf.variable_scope(\'enc_unit_{}_{}\'.format(res_scale, i)):\n\n                x = vanilla_residual_unit_3d(\n                    inputs=x,\n                    out_filters=filters[res_scale],\n                    strides=(1, 1, 1),\n                    activation=activation,\n                    mode=mode)\n        res_scales.append(x)\n\n        tf.logging.info(\'Encoder at res_scale {} tensor shape: {}\'.format(\n            res_scale, x.get_shape()))\n\n    # Upsample and concat layers [1] reconstruct the predictions to higher\n    # resolution scales\n    for res_scale in range(len(filters) - 2, -1, -1):\n\n        with tf.variable_scope(\'up_concat_{}\'.format(res_scale)):\n\n            x = upsample_and_concat(\n                inputs=x,\n                inputs2=res_scales[res_scale],\n                strides=saved_strides[res_scale])\n\n        for i in range(0, num_res_units):\n\n            with tf.variable_scope(\'dec_unit_{}_{}\'.format(res_scale, i)):\n\n                x = vanilla_residual_unit_3d(\n                    inputs=x,\n                    out_filters=filters[res_scale],\n                    strides=(1, 1, 1),\n                    mode=mode)\n        tf.logging.info(\'Decoder at res_scale {} tensor shape: {}\'.format(\n            res_scale, x.get_shape()))\n\n    # Last convolution\n    with tf.variable_scope(\'last\'):\n\n        x = tf.layers.conv3d(inputs=x,\n                             filters=num_classes,\n                             kernel_size=(1, 1, 1),\n                             strides=(1, 1, 1),\n                             **conv_params)\n\n    tf.logging.info(\'Output tensor shape {}\'.format(x.get_shape()))\n\n    # Define the outputs\n    outputs[\'logits\'] = x\n\n    with tf.variable_scope(\'pred\'):\n        y_prob = tf.nn.softmax(x)\n        outputs[\'y_prob\'] = y_prob\n\n        y_ = tf.argmax(x, axis=-1) \\\n            if num_classes > 1 \\\n            else tf.cast(tf.greater_equal(x[..., 0], 0.5), tf.int32)\n\n        outputs[\'y_\'] = y_\n\n    return outputs\n\n\ndef asymmetric_residual_unet_3d(\n        inputs,\n        num_classes,\n        num_res_units=1,\n        filters=(16, 32, 64, 128),\n        strides=((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2)),\n        mode=tf.estimator.ModeKeys.EVAL,\n        use_bias=False,\n        activation=leaky_relu,\n        kernel_initializer=tf.initializers.variance_scaling(distribution=\'uniform\'),\n        bias_initializer=tf.zeros_initializer(),\n        kernel_regularizer=None,\n        bias_regularizer=None):\n    """"""\n    Image segmentation network based on a flexible UNET architecture [1]\n    using residual units [2] as feature extractors. Downsampling and\n    upsampling of features is done via strided convolutions and transpose\n    convolutions, respectively. On each resolution scale s are\n    num_residual_units with filter size = filters[s]. strides[s] determine\n    the downsampling factor at each resolution scale.\n\n    [1] O. Ronneberger et al. U-Net: Convolutional Networks for Biomedical Image\n        Segmentation. MICCAI 2015.\n    [2] K. He et al. Identity Mappings in Deep Residual Networks. ECCV 2016.\n\n    Args:\n        inputs (tf.Tensor): Input feature tensor to the network (rank 5\n            required).\n        num_classes (int): Number of output classes.\n        num_res_units (int, optional): Number of residual units at each\n            resolution scale.\n        filters (tuple, optional): Number of filters for all residual units at\n            each resolution scale.\n        strides (tuple, optional): Stride of the first unit on a resolution\n            scale.\n        mode (TYPE, optional): One of the tf.estimator.ModeKeys strings: TRAIN,\n            EVAL or PREDICT\n        use_bias (bool, optional): Boolean, whether the layer uses a bias.\n        activation (optional): A function to use as activation function.\n        kernel_initializer (TYPE, optional): An initializer for the convolution\n            kernel.\n        bias_initializer (TYPE, optional): An initializer for the bias vector.\n            If None, no bias will be applied.\n        kernel_regularizer (None, optional): Optional regularizer for the\n            convolution kernel.\n        bias_regularizer (None, optional): Optional regularizer for the bias\n            vector.\n\n    Returns:\n        dict: dictionary of output tensors\n\n    """"""\n    outputs = {}\n    assert len(strides) == len(filters)\n    assert len(inputs.get_shape().as_list()) == 5, \\\n        \'inputs are required to have a rank of 5.\'\n\n    conv_params = {\'padding\': \'same\',\n                   \'use_bias\': use_bias,\n                   \'kernel_initializer\': kernel_initializer,\n                   \'bias_initializer\': bias_initializer,\n                   \'kernel_regularizer\': kernel_regularizer,\n                   \'bias_regularizer\': bias_regularizer}\n\n    x = inputs\n\n    # Initial convolution with filters[0]\n    x = tf.layers.conv3d(inputs=x,\n                         filters=filters[0],\n                         kernel_size=(3, 3, 3),\n                         strides=strides[0],\n                         **conv_params)\n\n    tf.logging.info(\'Init conv tensor shape {}\'.format(x.get_shape()))\n\n    # Residual feature encoding blocks with num_res_units at different\n    # resolution scales res_scales\n    res_scales = [x]\n    saved_strides = []\n    for res_scale in range(1, len(filters)):\n\n        # Features are downsampled via strided convolutions. These are defined\n        # in `strides` and subsequently saved\n        with tf.variable_scope(\'enc_unit_{}_0\'.format(res_scale)):\n\n            x = vanilla_residual_unit_3d(\n                inputs=x,\n                out_filters=filters[res_scale],\n                strides=strides[res_scale],\n                activation=activation,\n                mode=mode)\n        saved_strides.append(strides[res_scale])\n\n        for i in range(1, num_res_units):\n\n            with tf.variable_scope(\'enc_unit_{}_{}\'.format(res_scale, i)):\n\n                x = vanilla_residual_unit_3d(\n                    inputs=x,\n                    out_filters=filters[res_scale],\n                    strides=(1, 1, 1),\n                    activation=activation,\n                    mode=mode)\n        res_scales.append(x)\n\n        tf.logging.info(\'Encoder at res_scale {} tensor shape: {}\'.format(\n            res_scale, x.get_shape()))\n\n    # Upsample and concat layers [1] reconstruct the predictions to higher\n    # resolution scales\n    for res_scale in range(len(filters) - 2, -1, -1):\n\n        with tf.variable_scope(\'up_concat_{}\'.format(res_scale)):\n\n            x = upsample_and_concat(\n                inputs=x,\n                inputs2=res_scales[res_scale],\n                strides=saved_strides[res_scale])\n\n        with tf.variable_scope(\'dec_unit_{}\'.format(res_scale)):\n\n            x = vanilla_residual_unit_3d(\n                inputs=x,\n                out_filters=filters[res_scale],\n                strides=(1, 1, 1),\n                activation=activation,\n                mode=mode)\n        tf.logging.info(\'Decoder at res_scale {} tensor shape: {}\'.format(\n            res_scale, x.get_shape()))\n\n    # Last convolution\n    with tf.variable_scope(\'last\'):\n\n        x = tf.layers.conv3d(inputs=x,\n                             filters=num_classes,\n                             kernel_size=(1, 1, 1),\n                             strides=(1, 1, 1),\n                             **conv_params)\n\n    tf.logging.info(\'Output tensor shape {}\'.format(x.get_shape()))\n\n    # Define the outputs\n    outputs[\'logits\'] = x\n\n    with tf.variable_scope(\'pred\'):\n        y_prob = tf.nn.softmax(x)\n        outputs[\'y_prob\'] = y_prob\n\n        y_ = tf.argmax(x, axis=-1) \\\n            if num_classes > 1 \\\n            else tf.cast(tf.greater_equal(x[..., 0], 0.5), tf.int32)\n\n        outputs[\'y_\'] = y_\n\n    return outputs\n'"
dltk/networks/super_resolution/__init__.py,0,b''
dltk/networks/super_resolution/simple_super_resolution.py,15,"b'from __future__ import division\nfrom __future__ import absolute_import\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef simple_super_resolution_3d(inputs,\n                               num_convolutions=1,\n                               filters=(16, 32, 64),\n                               upsampling_factor=(2, 2, 2),\n                               mode=tf.estimator.ModeKeys.EVAL,\n                               use_bias=False,\n                               activation=tf.nn.relu6,\n                               kernel_initializer=tf.initializers.variance_scaling(distribution=\'uniform\'),\n                               bias_initializer=tf.zeros_initializer(),\n                               kernel_regularizer=None,\n                               bias_regularizer=None):\n    """"""Simple super resolution network with num_convolutions per feature\n        extraction block. Each convolution in a block b has a filter size\n        of filters[b].\n\n    Args:\n        inputs (tf.Tensor): Input feature tensor to the network (rank 5\n            required).\n        num_convolutions (int, optional): Number of convolutions.\n        filters (tuple, optional): filters (tuple, optional): Number of filters.\n        upsampling_factor (tuple, optional): Upsampling factor of the low\n            resolution to the high resolution image.\n        mode (TYPE, optional): One of the tf.estimator.ModeKeys strings: TRAIN,\n            EVAL or PREDICT\n        use_bias (bool, optional): Boolean, whether the layer uses a bias.\n        activation (optional): A function to use as activation function.\n        kernel_initializer (TYPE, optional): An initializer for the convolution\n            kernel.\n        bias_initializer (TYPE, optional): An initializer for the bias vector.\n            If None, no bias will be applied.\n        kernel_regularizer (None, optional): Optional regularizer for the\n            convolution kernel.\n        bias_regularizer (None, optional): Optional regularizer for the bias\n            vector.\n\n    Returns:\n        dict: dictionary of output tensors\n\n    """"""\n\n    outputs = {}\n    assert len(inputs.get_shape().as_list()) == 5, \\\n        \'inputs are required to have a rank of 5.\'\n    assert len(upsampling_factor) == 3, \\\n        \'upsampling factor is required to be of length 3.\'\n\n    conv_op = tf.layers.conv3d\n    tp_conv_op = tf.layers.conv3d_transpose\n\n    conv_params = {\'padding\': \'same\',\n                   \'use_bias\': use_bias,\n                   \'kernel_initializer\': kernel_initializer,\n                   \'bias_initializer\': bias_initializer,\n                   \'kernel_regularizer\': kernel_regularizer,\n                   \'bias_regularizer\': bias_regularizer}\n\n    x = inputs\n    tf.logging.info(\'Input tensor shape {}\'.format(x.get_shape()))\n\n    # Convolutional feature encoding blocks with num_convolutions at different\n    # resolution scales res_scales\n    for unit in range(0, len(filters)):\n\n        for i in range(0, num_convolutions):\n\n            with tf.variable_scope(\'enc_unit_{}_{}\'.format(unit, i)):\n\n                x = conv_op(inputs=x,\n                            filters=filters[unit],\n                            kernel_size=(3, 3, 3),\n                            strides=(1, 1, 1),\n                            **conv_params)\n\n                x = tf.layers.batch_normalization(\n                    x, training=mode == tf.estimator.ModeKeys.TRAIN)\n\n                x = activation(x)\n\n                tf.logging.info(\'Encoder at unit_{}_{} tensor \'\n                                \'shape: {}\'.format(unit, i, x.get_shape()))\n\n    # Upsampling\n    with tf.variable_scope(\'upsampling_unit\'):\n\n        # Adjust the strided tp conv kernel size to prevent losing information\n        k_size = [u * 2 for u in upsampling_factor]\n        x = tp_conv_op(inputs=x,\n                       filters=inputs.get_shape().as_list()[-1],\n                       kernel_size=k_size,\n                       strides=upsampling_factor,\n                       **conv_params)\n\n    tf.logging.info(\'Output tensor shape: {}\'.format(x.get_shape()))\n    outputs[\'x_\'] = x\n\n    return outputs\n'"
examples/applications/IXI_HH_DCGAN/reader.py,2,"b'from __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport SimpleITK as sitk\nimport tensorflow as tf\nimport os\nimport numpy as np\n\nfrom dltk.io.augmentation import extract_random_example_array\nfrom dltk.io.preprocessing import normalise_one_one\nimport scipy\n\n\ndef read_fn(file_references, mode, params=None):\n    """"""A custom python read function for interfacing with nii image files.\n\n    Args:\n        file_references (list): A list of lists containing file references,\n            such as [[\'id_0\', \'image_filename_0\', target_value_0], ...,\n             [\'id_N\', \'image_filename_N\', target_value_N]].\n        mode (str): One of the tf.estimator.ModeKeys strings: TRAIN, EVAL or\n            PREDICT.\n        params (dict, optional): A dictionary to parametrise read_fn ouputs\n            (e.g. reader_params = {\'n_examples\': 10, \'example_size\':\n            [64, 64, 64], \'extract_examples\': True}, etc.).\n\n    Yields:\n        dict: A dictionary of reader outputs for dltk.io.abstract_reader.\n    """"""\n\n    for f in file_references:\n        subject_id = f[0]\n\n        data_path = \'../../../data/IXI_HH/1mm\'\n\n        # Read the image nii with sitk\n        t1_fn = os.path.join(data_path, \'{}/T1_1mm.nii.gz\'.format(subject_id))\n        t1 = sitk.GetArrayFromImage(sitk.ReadImage(str(t1_fn)))\n\n        # Normalise volume images\n        t1 = t1[..., np.newaxis]\n\n        # restrict to slices around center slice\n        t1 = t1[len(t1) // 2 - 5:len(t1) // 2 + 5]\n\n        t1 = normalise_one_one(t1)\n\n        images = t1\n\n        noise = np.random.normal(size=(1, 1, 1, 100))\n\n        if mode == tf.estimator.ModeKeys.PREDICT:\n            yield {\'labels\': images, \'features\': {\'noise\': noise}}\n\n        # Check if the reader is supposed to return training examples or full\n        # images\n        if params[\'extract_examples\']:\n            images = extract_random_example_array(\n                image_list=images,\n                example_size=params[\'example_size\'],\n                n_examples=params[\'n_examples\'])\n\n            for e in range(params[\'n_examples\']):\n                zoomed = scipy.ndimage.zoom(\n                    images[e], (1, 64. / 224., 64. / 224., 1)).astype(\n                    np.float32)\n                yield {\'labels\': zoomed,\n                       \'features\': {\'noise\': noise}}\n        else:\n            yield {\'labels\': images, \'features\': {\'noise\': noise}}\n\n    return\n'"
examples/applications/IXI_HH_DCGAN/train.py,38,"b'# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport argparse\nimport os\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np\n\nfrom dltk.networks.gan.dcgan import dcgan_discriminator_3d, dcgan_generator_3d\nfrom dltk.io.abstract_reader import Reader\n\nfrom reader import read_fn\n\nBATCH_SIZE = 8\nMAX_STEPS = 35000\nSAVE_SUMMARY_STEPS = 100\n\n\ndef train(args):\n    np.random.seed(42)\n    tf.set_random_seed(42)\n\n    print(\'Setting up...\')\n\n    # Parse csv files for file names\n    all_filenames = pd.read_csv(\n        args.data_csv,\n        dtype=object,\n        keep_default_na=False,\n        na_values=[]).as_matrix()\n\n    train_filenames = all_filenames\n\n    # Set up a data reader to handle the file i/o.\n    reader_params = {\'n_examples\': 10,\n                     \'example_size\': [4, 224, 224],\n                     \'extract_examples\': True}\n\n    reader_example_shapes = {\'labels\': [4, 64, 64, 1],\n                             \'features\': {\'noise\': [1, 1, 1, 100]}}\n\n    reader = Reader(read_fn, {\'features\': {\'noise\': tf.float32},\n                              \'labels\': tf.float32})\n\n    # Get input functions and queue initialisation hooks for data\n    train_input_fn, train_qinit_hook = reader.get_inputs(\n        file_references=train_filenames,\n        mode=tf.estimator.ModeKeys.TRAIN,\n        example_shapes=reader_example_shapes,\n        batch_size=BATCH_SIZE,\n        params=reader_params)\n\n    # See TFGAN\'s `train.py` for a description of the generator and\n    # discriminator API.\n    def generator_fn(generator_inputs):\n        """"""Generator function to build fake data samples. It creates a network\n        given input features (e.g. from a dltk.io.abstract_reader). Further,\n        custom Tensorboard summary ops can be added. For additional\n        information, please refer to https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/gan/estimator/GANEstimator.\n\n        Args:\n            generator_inputs (tf.Tensor): Noise input to generate samples from.\n\n        Returns:\n            tf.Tensor: Generated data samples\n        """"""\n        gen = dcgan_generator_3d(\n            inputs=generator_inputs[\'noise\'],\n            mode=tf.estimator.ModeKeys.TRAIN)\n        gen = gen[\'gen\']\n        gen = tf.nn.tanh(gen)\n        return gen\n\n    def discriminator_fn(data, conditioning):\n        """"""Discriminator function to discriminate real and fake data. It creates\n        a network given input features (e.g. from a dltk.io.abstract_reader).\n        Further, custom Tensorboard summary ops can be added. For additional\n        information, please refer to https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/gan/estimator/GANEstimator.\n\n        Args:\n            generator_inputs (tf.Tensor): Noise input to generate samples from.\n\n        Returns:\n            tf.Tensor: Generated data samples\n        """"""\n        tf.summary.image(\'data\', data[:, 0])\n\n        disc = dcgan_discriminator_3d(\n            inputs=data,\n            mode=tf.estimator.ModeKeys.TRAIN)\n\n        return disc[\'logits\']\n\n    # get input tensors from queue\n    features, labels = train_input_fn()\n\n    # build generator\n    with tf.variable_scope(\'generator\'):\n        gen = generator_fn(features)\n\n    # build discriminator on fake data\n    with tf.variable_scope(\'discriminator\'):\n        disc_fake = discriminator_fn(gen, None)\n\n    # build discriminator on real data, reusing the previously created variables\n    with tf.variable_scope(\'discriminator\', reuse=True):\n        disc_real = discriminator_fn(labels, None)\n\n    # building an LSGAN loss for the real examples\n    d_loss_real = tf.losses.mean_squared_error(\n        disc_real, tf.ones_like(disc_real))\n\n    # calculating a pseudo accuracy for the discriminator detecting a real\n    # sample and logging that\n    d_pred_real = tf.cast(tf.greater(disc_real, 0.5), tf.float32)\n    _, d_acc_real = tf.metrics.accuracy(tf.ones_like(disc_real), d_pred_real)\n    tf.summary.scalar(\'disc/real_acc\', d_acc_real)\n\n    # building an LSGAN loss for the fake examples\n    d_loss_fake = tf.losses.mean_squared_error(\n        disc_fake, tf.zeros_like(disc_fake))\n\n    # calculating a pseudo accuracy for the discriminator detecting a fake\n    # sample and logging that\n    d_pred_fake = tf.cast(tf.greater(disc_fake, 0.5), tf.float32)\n    _, d_acc_fake = tf.metrics.accuracy(tf.zeros_like(disc_fake), d_pred_fake)\n    tf.summary.scalar(\'disc/fake_acc\', d_acc_fake)\n\n    # building an LSGAN loss for the generator\n    g_loss = tf.losses.mean_squared_error(\n        disc_fake, tf.ones_like(disc_fake))\n    tf.summary.scalar(\'loss/gen\', g_loss)\n\n    # combining the discriminator losses\n    d_loss = d_loss_fake + d_loss_real\n    tf.summary.scalar(\'loss/disc\', d_loss)\n\n    # getting the list of discriminator variables\n    d_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n                               \'discriminator\')\n\n    # building the discriminator optimizer\n    d_opt = tf.train.AdamOptimizer(\n        0.001, 0.5, epsilon=1e-5).minimize(d_loss, var_list=d_vars)\n\n    # getting the list of generator variables\n    g_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n                               \'generator\')\n\n    # building the generator optimizer\n    g_opt = tf.train.AdamOptimizer(\n        0.001, 0.5, epsilon=1e-5).minimize(g_loss, var_list=g_vars)\n\n    # getting a variable to hold the global step\n    global_step = tf.train.get_or_create_global_step()\n    # build op to increment the global step - important for TensorBoard logging\n    inc_step = global_step.assign_add(1)\n\n    # build the training session.\n    # NOTE: we are not using a tf.estimator here, because they prevent some\n    # flexibility in the training procedure\n    s = tf.train.MonitoredTrainingSession(checkpoint_dir=args.model_path,\n                                          save_summaries_steps=100,\n                                          save_summaries_secs=None,\n                                          hooks=[train_qinit_hook])\n\n    # build dummy logging string\n    log = \'Step {} with Loss D: {}, Loss G: {}, Acc Real: {} Acc Fake: {}\'\n\n    # start training\n    print(\'Starting training...\')\n    loss_d = 0\n    loss_g = 0\n    try:\n        for step in range(MAX_STEPS):\n            # if discriminator is too good, only train generator\n            if not loss_g > 3 * loss_d:\n                s.run(d_opt)\n\n            # if generator is too good, only train discriminator\n            if not loss_d > 3 * loss_g:\n                s.run(g_opt)\n\n            # increment global step for logging hooks\n            s.run(inc_step)\n\n            # get statistics for training scheduling\n            loss_d, loss_g, acc_d, acc_g = s.run(\n                [d_loss, g_loss, d_acc_real, d_acc_fake])\n\n            # print stats for information\n            if step % SAVE_SUMMARY_STEPS == 0:\n                print(log.format(step, loss_d, loss_g, acc_d, acc_g))\n    except KeyboardInterrupt:\n        pass\n    print(\'Stopping now.\')\n\n\nif __name__ == \'__main__\':\n    # Set up argument parser\n    parser = argparse.ArgumentParser(description=\'Example: IXI HH LSGAN training script\')\n    parser.add_argument(\'--run_validation\', default=True)\n    parser.add_argument(\'--restart\', default=False, action=\'store_true\')\n    parser.add_argument(\'--verbose\', default=False, action=\'store_true\')\n    parser.add_argument(\'--cuda_devices\', \'-c\', default=\'0\')\n\n    parser.add_argument(\'--model_path\', \'-p\', default=\'/tmp/IXI_dcgan/\')\n    parser.add_argument(\'--data_csv\', default=\'../../../data/IXI_HH/demographic_HH.csv\')\n\n    args = parser.parse_args()\n\n    # Set verbosity\n    if args.verbose:\n        os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'1\'\n        tf.logging.set_verbosity(tf.logging.INFO)\n    else:\n        os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n        tf.logging.set_verbosity(tf.logging.ERROR)\n\n    # GPU allocation options\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.cuda_devices\n\n    # Handle restarting and resuming training\n    if args.restart:\n        print(\'Restarting training from scratch.\')\n        os.system(\'rm -rf {}\'.format(args.model_path))\n\n    if not os.path.isdir(args.model_path):\n        os.system(\'mkdir -p {}\'.format(args.model_path))\n    else:\n        print(\'Resuming training on model_path {}\'.format(args.model_path))\n\n    # Call training\n    train(args)\n'"
examples/applications/IXI_HH_age_regression_resnet/deploy.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport time\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.contrib import predictor\n\nfrom dltk.io.augmentation import extract_random_example_array\n\nfrom reader import read_fn\n\nREADER_PARAMS = {\'extract_examples\': False}\nN_VALIDATION_SUBJECTS = 28\n\n\ndef predict(args):\n    # Read in the csv with the file names you would want to predict on\n    file_names = pd.read_csv(\n        args.csv,\n        dtype=object,\n        keep_default_na=False,\n        na_values=[]).as_matrix()\n\n    # We trained on the first 4 subjects, so we predict on the rest\n    file_names = file_names[-N_VALIDATION_SUBJECTS:]\n\n    # From the model_path, parse the latest saved model and restore a\n    # predictor from it\n    export_dir = [os.path.join(args.model_path, o) for o in sorted(\n        os.listdir(args.model_path)) if os.path.isdir(\n        os.path.join(args.model_path, o)) and o.isdigit()][-1]\n\n    print(\'Loading from {}\'.format(export_dir))\n    my_predictor = predictor.from_saved_model(export_dir)\n\n    # Iterate through the files, predict on the full volumes and compute a Dice\n    # coefficient\n    mae = []\n    for output in read_fn(file_references=file_names,\n                          mode=tf.estimator.ModeKeys.EVAL,\n                          params=READER_PARAMS):\n        t0 = time.time()\n\n        # Parse the read function output and add a dummy batch dimension as\n        # required\n        img = output[\'features\'][\'x\']\n        lbl = output[\'labels\'][\'y\']\n        test_id = output[\'img_id\']\n\n        # We know, that the training input shape of [64, 96, 96] will work with\n        # our model strides, so we collect several crops of the test image and\n        # average the predictions. Alternatively, we could pad or crop the input\n        # to any shape that is compatible with the resolution scales of the\n        # model:\n\n        num_crop_predictions = 4\n        crop_batch = extract_random_example_array(\n            image_list=img,\n            example_size=[64, 96, 96],\n            n_examples=num_crop_predictions)\n\n        y_ = my_predictor.session.run(\n            fetches=my_predictor._fetch_tensors[\'logits\'],\n            feed_dict={my_predictor._feed_tensors[\'x\']: crop_batch})\n\n        # Average the predictions on the cropped test inputs:\n        y_ = np.mean(y_)\n\n        # Calculate the absolute error for this subject\n        mae.append(np.abs(y_ - lbl))\n\n        # Print outputs\n        print(\'id={}; pred={:0.2f} yrs; true={:0.2f} yrs; run time={:0.2f} s; \'\n              \'\'.format(test_id, y_, lbl[0], time.time() - t0))\n    print(\'mean absolute err={:0.3f} yrs\'.format(np.mean(mae)))\n\n\nif __name__ == \'__main__\':\n    # Set up argument parser\n    parser = argparse.ArgumentParser(\n        description=\'IXI HH example age regression deploy script\')\n    parser.add_argument(\'--verbose\', default=False, action=\'store_true\')\n    parser.add_argument(\'--cuda_devices\', \'-c\', default=\'0\')\n\n    parser.add_argument(\'--model_path\', \'-p\',\n                        default=\'/tmp/IXI_age_regression/\')\n    parser.add_argument(\'--csv\', default=\'../../../data/IXI_HH/demographic_HH.csv\')\n\n    args = parser.parse_args()\n\n    # Set verbosity\n    if args.verbose:\n        os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'1\'\n        tf.logging.set_verbosity(tf.logging.INFO)\n    else:\n        os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n        tf.logging.set_verbosity(tf.logging.ERROR)\n\n    # GPU allocation options\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.cuda_devices\n\n    # Call training\n    predict(args)\n'"
examples/applications/IXI_HH_age_regression_resnet/reader.py,3,"b'from __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport SimpleITK as sitk\nimport tensorflow as tf\nimport os\nimport numpy as np\n\nfrom dltk.io.augmentation import flip, extract_random_example_array\nfrom dltk.io.preprocessing import whitening\n\n\ndef read_fn(file_references, mode, params=None):\n    """"""A custom python read function for interfacing with nii image files.\n\n    Args:\n        file_references (list): A list of lists containing file references, such\n            as [[\'id_0\', \'image_filename_0\', target_value_0], ...,\n            [\'id_N\', \'image_filename_N\', target_value_N]].\n        mode (str): One of the tf.estimator.ModeKeys strings: TRAIN, EVAL or\n            PREDICT.\n        params (dict, optional): A dictionary to parametrise read_fn ouputs\n            (e.g. reader_params = {\'n_examples\': 10, \'example_size\':\n            [64, 64, 64], \'extract_examples\': True}, etc.).\n\n    Yields:\n        dict: A dictionary of reader outputs for dltk.io.abstract_reader.\n    """"""\n\n    def _augment(img):\n        """"""An image augmentation function""""""\n        return flip(img, axis=2)\n\n    for f in file_references:\n        subject_id = f[0]\n\n        data_path = \'../../../data/IXI_HH/2mm\'\n\n        # Read the image nii with sitk\n        t1_fn = os.path.join(data_path, \'{}/T1_2mm.nii.gz\'.format(subject_id))\n        t1 = sitk.GetArrayFromImage(sitk.ReadImage(str(t1_fn)))\n\n        # Normalise volume image\n        t1 = whitening(t1)\n\n        # Create a 4D image (i.e. [x, y, z, channels])\n        images = np.expand_dims(t1, axis=-1).astype(np.float32)\n\n        if mode == tf.estimator.ModeKeys.PREDICT:\n            yield {\'features\': {\'x\': images}, \'img_id\': subject_id}\n\n        # Parse the regression targets from the file_references\n        age = np.float(f[11])\n        y = np.expand_dims(age, axis=-1).astype(np.float32)\n\n        # Augment if used in training mode\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            images = _augment(images)\n\n        # Check if the reader is supposed to return training examples or full\n        #  images\n        if params[\'extract_examples\']:\n            images = extract_random_example_array(\n                image_list=images,\n                example_size=params[\'example_size\'],\n                n_examples=params[\'n_examples\'])\n\n            for e in range(params[\'n_examples\']):\n                yield {\'features\': {\'x\': images[e].astype(np.float32)},\n                       \'labels\': {\'y\': y.astype(np.float32)},\n                       \'img_id\': subject_id}\n\n        else:\n            yield {\'features\': {\'x\': images},\n                   \'labels\': {\'y\': y.astype(np.float32)},\n                   \'img_id\': subject_id}\n\n    return\n'"
examples/applications/IXI_HH_age_regression_resnet/train.py,29,"b'# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport argparse\nimport os\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np\n\nfrom dltk.networks.regression_classification.resnet import resnet_3d\nfrom dltk.io.abstract_reader import Reader\n\nfrom reader import read_fn\n\nEVAL_EVERY_N_STEPS = 100\nEVAL_STEPS = 5\n\nNUM_CLASSES = 1\nNUM_CHANNELS = 1\n\nBATCH_SIZE = 8\nSHUFFLE_CACHE_SIZE = 32\n\nMAX_STEPS = 50000\n\n\ndef model_fn(features, labels, mode, params):\n    """"""Model function to construct a tf.estimator.EstimatorSpec. It creates a\n        network given input features (e.g. from a dltk.io.abstract_reader) and\n        training targets (labels). Further, loss, optimiser, evaluation ops and\n        custom tensorboard summary ops can be added. For additional information,\n        please refer to\n        https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#model_fn.\n\n    Args:\n        features (tf.Tensor): Tensor of input features to train from. Required\n            rank and dimensions are determined by the subsequent ops (i.e.\n            the network).\n        labels (tf.Tensor): Tensor of training targets or labels. Required rank\n            and dimensions are determined by the network output.\n        mode (str): One of the tf.estimator.ModeKeys: TRAIN, EVAL or PREDICT\n        params (dict, optional): A dictionary to parameterise the model_fn\n            (e.g. learning_rate)\n\n    Returns:\n        tf.estimator.EstimatorSpec: A custom EstimatorSpec for this experiment\n    """"""\n\n    # 1. create a model and its outputs\n    net_output_ops = resnet_3d(\n        inputs=features[\'x\'],\n        num_res_units=2,\n        num_classes=NUM_CLASSES,\n        filters=(16, 32, 64, 128, 256),\n        strides=((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2)),\n        mode=mode,\n        kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4))\n\n    # 1.1 Generate predictions only (for `ModeKeys.PREDICT`)\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(\n            mode=mode,\n            predictions=net_output_ops,\n            export_outputs={\'out\': tf.estimator.export.PredictOutput(net_output_ops)})\n\n    # 2. set up a loss function\n    loss = tf.losses.mean_squared_error(\n        labels=labels[\'y\'],\n        predictions=net_output_ops[\'logits\'])\n\n    # 3. define a training op and ops for updating moving averages (i.e.\n    # for batch normalisation)\n    global_step = tf.train.get_global_step()\n    optimiser = tf.train.AdamOptimizer(\n        learning_rate=params[""learning_rate""],\n        epsilon=1e-5)\n\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n        train_op = optimiser.minimize(loss, global_step=global_step)\n\n    # 4.1 (optional) create custom image summaries for tensorboard\n    my_image_summaries = {}\n    my_image_summaries[\'feat_t1\'] = features[\'x\'][0, 32, :, :, 0]\n\n    expected_output_size = [1, 96, 96, 1]  # [B, W, H, C]\n    [tf.summary.image(name, tf.reshape(image, expected_output_size))\n     for name, image in my_image_summaries.items()]\n\n    # 4.2 (optional) track the rmse (scaled back by 100, see reader.py)\n    rmse = tf.metrics.root_mean_squared_error\n    mae = tf.metrics.mean_absolute_error\n    eval_metric_ops = {""rmse"": rmse(labels[\'y\'], net_output_ops[\'logits\']),\n                       ""mae"": mae(labels[\'y\'], net_output_ops[\'logits\'])}\n\n    # 5. Return EstimatorSpec object\n    return tf.estimator.EstimatorSpec(mode=mode,\n                                      predictions=net_output_ops,\n                                      loss=loss,\n                                      train_op=train_op,\n                                      eval_metric_ops=eval_metric_ops)\n\n\ndef train(args):\n    np.random.seed(42)\n    tf.set_random_seed(42)\n\n    print(\'Setting up...\')\n\n    # Parse csv files for file names\n    all_filenames = pd.read_csv(\n        args.data_csv,\n        dtype=object,\n        keep_default_na=False,\n        na_values=[]).as_matrix()\n\n    train_filenames = all_filenames[:150]\n    val_filenames = all_filenames[150:]\n\n    # Set up a data reader to handle the file i/o.\n    reader_params = {\'n_examples\': 2,\n                     \'example_size\': [64, 96, 96],\n                     \'extract_examples\': True}\n\n    reader_example_shapes = {\'features\': {\'x\': reader_params[\'example_size\'] + [NUM_CHANNELS, ]},\n                             \'labels\': {\'y\': [1]}}\n\n    reader = Reader(read_fn, {\'features\': {\'x\': tf.float32},\n                              \'labels\': {\'y\': tf.float32}})\n\n    # Get input functions and queue initialisation hooks for training and\n    # validation data\n    train_input_fn, train_qinit_hook = reader.get_inputs(\n        file_references=train_filenames,\n        mode=tf.estimator.ModeKeys.TRAIN,\n        example_shapes=reader_example_shapes,\n        batch_size=BATCH_SIZE,\n        shuffle_cache_size=SHUFFLE_CACHE_SIZE,\n        params=reader_params)\n\n    val_input_fn, val_qinit_hook = reader.get_inputs(\n        file_references=val_filenames,\n        mode=tf.estimator.ModeKeys.EVAL,\n        example_shapes=reader_example_shapes,\n        batch_size=BATCH_SIZE,\n        shuffle_cache_size=SHUFFLE_CACHE_SIZE,\n        params=reader_params)\n\n    # Instantiate the neural network estimator\n    nn = tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir=args.model_path,\n        params={""learning_rate"": 0.001},\n        config=tf.estimator.RunConfig())\n\n    # Hooks for validation summaries\n    val_summary_hook = tf.contrib.training.SummaryAtEndHook(\n        os.path.join(args.model_path, \'eval\'))\n    step_cnt_hook = tf.train.StepCounterHook(\n        every_n_steps=EVAL_EVERY_N_STEPS, output_dir=args.model_path)\n\n    print(\'Starting training...\')\n    try:\n        for _ in range(MAX_STEPS // EVAL_EVERY_N_STEPS):\n            nn.train(input_fn=train_input_fn,\n                     hooks=[train_qinit_hook, step_cnt_hook],\n                     steps=EVAL_EVERY_N_STEPS)\n\n            if args.run_validation:\n                results_val = nn.evaluate(input_fn=val_input_fn,\n                                          hooks=[val_qinit_hook, val_summary_hook],\n                                          steps=EVAL_STEPS)\n                print(\'Step = {}; val loss = {:.5f};\'.format(\n                    results_val[\'global_step\'],\n                    results_val[\'loss\']))\n\n    except KeyboardInterrupt:\n        pass\n\n    print(\'Stopping now.\')\n\n    # When exporting we set the expected input shape to be arbitrary.\n    export_dir = nn.export_savedmodel(\n        export_dir_base=args.model_path,\n        serving_input_receiver_fn=reader.serving_input_receiver_fn(\n            {\'features\': {\'x\': [None, None, None, NUM_CHANNELS]},\n             \'labels\': {\'y\': [1]}}))\n    print(\'Model saved to {}.\'.format(export_dir))\n\n\nif __name__ == \'__main__\':\n    # Set up argument parser\n    parser = argparse.ArgumentParser(description=\'Example: IXI HH resnet age regression training script\')\n    parser.add_argument(\'--run_validation\', default=True)\n    parser.add_argument(\'--restart\', default=False, action=\'store_true\')\n    parser.add_argument(\'--verbose\', default=False, action=\'store_true\')\n    parser.add_argument(\'--cuda_devices\', \'-c\', default=\'0\')\n\n    parser.add_argument(\'--model_path\', \'-p\', default=\'/tmp/IXI_age_regression/\')\n    parser.add_argument(\'--data_csv\', default=\'../../../data/IXI_HH/demographic_HH.csv\')\n\n    args = parser.parse_args()\n\n    # Set verbosity\n    if args.verbose:\n        os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'1\'\n        tf.logging.set_verbosity(tf.logging.INFO)\n    else:\n        os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n        tf.logging.set_verbosity(tf.logging.ERROR)\n\n    # GPU allocation options\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.cuda_devices\n\n    # Handle restarting and resuming training\n    if args.restart:\n        print(\'Restarting training from scratch.\')\n        os.system(\'rm -rf {}\'.format(args.model_path))\n\n    if not os.path.isdir(args.model_path):\n        os.system(\'mkdir -p {}\'.format(args.model_path))\n    else:\n        print(\'Resuming training on model_path {}\'.format(args.model_path))\n\n    # Call training\n    train(args)\n'"
examples/applications/IXI_HH_representation_learning_cae/reader.py,3,"b'from __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport SimpleITK as sitk\nimport tensorflow as tf\nimport os\nimport numpy as np\n\nfrom dltk.io.augmentation import add_gaussian_noise, extract_random_example_array\nfrom dltk.io.preprocessing import whitening\n\n\ndef read_fn(file_references, mode, params=None):\n    """"""A custom python read function for interfacing with nii image files.\n\n    Args:\n        file_references (list): A list of lists containing file references,\n            such as [[\'id_0\', \'image_filename_0\', target_value_0], ...,\n            [\'id_N\', \'image_filename_N\', target_value_N]].\n        mode (str): One of the tf.estimator.ModeKeys strings: TRAIN, EVAL or\n            PREDICT.\n        params (dict, optional): A dictionary to parametrise read_fn outputs\n            (e.g. reader_params = {\'n_examples\': 10, \'example_size\':\n            [64, 64, 64], \'extract_examples\': True}, etc.).\n\n    Yields:\n        dict: A dictionary of reader outputs for dltk.io.abstract_reader.\n    """"""\n\n    def _augment(img):\n        """"""An image augmentation function""""""\n        return add_gaussian_noise(img, sigma=0.1)\n\n    for f in file_references:\n        subject_id = f[0]\n\n        data_path = \'../../../data/IXI_HH/1mm\'\n\n        # Read the image nii with sitk\n        t1_fn = os.path.join(data_path, \'{}/T1_1mm.nii.gz\'.format(subject_id))\n        t2_fn = os.path.join(data_path, \'{}/T2_1mm.nii.gz\'.format(subject_id))\n        pd_fn = os.path.join(data_path, \'{}/PD_1mm.nii.gz\'.format(subject_id))\n\n        t1 = sitk.GetArrayFromImage(sitk.ReadImage(str(t1_fn)))\n        t2 = sitk.GetArrayFromImage(sitk.ReadImage(str(t2_fn)))\n        pd = sitk.GetArrayFromImage(sitk.ReadImage(str(pd_fn)))\n\n        # Normalise volume images\n        t1 = whitening(t1)\n        t2 = whitening(t2)\n        pd = whitening(pd)\n\n        # Create a 4D multi-sequence image (i.e. [x, y, z, channels])\n        images = np.stack([t1, t2, pd], axis=-1).astype(np.float32)\n\n        if mode == tf.estimator.ModeKeys.PREDICT:\n            yield {\'features\': {\'x\': images}}\n\n        # Augment if used in training mode\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            images = _augment(images)\n\n        # Check if the reader is supposed to return training examples or full\n        # images\n        if params[\'extract_examples\']:\n            images = extract_random_example_array(\n                image_list=images,\n                example_size=params[\'example_size\'],\n                n_examples=params[\'n_examples\'])\n\n            for e in range(params[\'n_examples\']):\n                yield {\'features\': {\'x\': images[e].astype(np.float32)}}\n        else:\n            yield {\'features\': {\'x\': images}}\n\n    return\n'"
examples/applications/IXI_HH_representation_learning_cae/train.py,28,"b'# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport argparse\nimport os\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np\n\nfrom dltk.networks.autoencoder.convolutional_autoencoder import convolutional_autoencoder_3d\nfrom dltk.io.abstract_reader import Reader\n\nfrom reader import read_fn\n\n\nEVAL_EVERY_N_STEPS = 100\nEVAL_STEPS = 10\n\nNUM_CHANNELS = 3\n\nBATCH_SIZE = 8\nSHUFFLE_CACHE_SIZE = 64\n\nMAX_STEPS = 100000\n\n\ndef model_fn(features, labels, mode, params):\n    """"""Model function to construct a tf.estimator.EstimatorSpec. It creates a\n        network given input features (e.g. from a dltk.io.abstract_reader) and\n        training targets (labels). Further, loss, optimiser, evaluation ops and\n        custom tensorboard summary ops can be added. For additional information,\n        please refer to https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#model_fn.\n\n    Args:\n        features (tf.Tensor): Tensor of input features to train from. Required\n            rank and dimensions are determined by the subsequent ops\n            (i.e. the network).\n        labels (tf.Tensor): Tensor of training targets or labels. Required rank\n            and dimensions are determined by the network output.\n        mode (str): One of the tf.estimator.ModeKeys: TRAIN, EVAL or PREDICT\n        params (dict, optional): A dictionary to parameterise the model_fn\n            (e.g. learning_rate)\n\n    Returns:\n        tf.estimator.EstimatorSpec: A custom EstimatorSpec for this experiment\n    """"""\n\n    # 1. create a model and its outputs\n    net_output_ops = convolutional_autoencoder_3d(\n        inputs=features[\'x\'],\n        num_convolutions=2,\n        num_hidden_units=1024,\n        filters=(16, 32, 64, 128, 256),\n        strides=((1, 2, 2), (1, 2, 2), (1, 2, 2), (1, 2, 2), (1, 2, 2)),\n        mode=mode)\n\n    # 1.1 Generate predictions only (for `ModeKeys.PREDICT`)\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(\n            mode=mode,\n            predictions=net_output_ops,\n            export_outputs={\'out\': tf.estimator.export.PredictOutput(net_output_ops)})\n\n    # 2. set up a loss function\n    loss = tf.losses.mean_squared_error(\n        labels=features[\'x\'],\n        predictions=net_output_ops[\'x_\'])\n\n    # 3. define a training op and ops for updating moving averages (i.e.\n    # for batch normalisation)\n    global_step = tf.train.get_global_step()\n    optimiser = tf.train.AdamOptimizer(\n        learning_rate=params[""learning_rate""],\n        epsilon=1e-5)\n\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n        train_op = optimiser.minimize(loss, global_step=global_step)\n\n    # 4.1 (optional) create custom image summaries for tensorboard\n    my_image_summaries = {}\n    my_image_summaries[\'feat_t1\'] = features[\'x\'][0, 0, :, :, 0]\n    my_image_summaries[\'feat_t2\'] = features[\'x\'][0, 0, :, :, 1]\n    my_image_summaries[\'feat_pd\'] = features[\'x\'][0, 0, :, :, 2]\n    my_image_summaries[\'pred_t1\'] = tf.cast(net_output_ops[\'x_\'], tf.float32)[0, 0, :, :, 0]\n    my_image_summaries[\'pred_t2\'] = tf.cast(net_output_ops[\'x_\'], tf.float32)[0, 0, :, :, 1]\n    my_image_summaries[\'pred_pd\'] = tf.cast(net_output_ops[\'x_\'], tf.float32)[0, 0, :, :, 2]\n\n    expected_output_size = [1, 224, 224, 1]  # [B, W, H, C]\n    [tf.summary.image(name, tf.reshape(image, expected_output_size))\n     for name, image in my_image_summaries.items()]\n\n    # 5. Return EstimatorSpec object\n    return tf.estimator.EstimatorSpec(mode=mode,\n                                      predictions=net_output_ops,\n                                      loss=loss,\n                                      train_op=train_op,\n                                      eval_metric_ops=None)\n\n\ndef train(args):\n    np.random.seed(42)\n    tf.set_random_seed(42)\n\n    print(\'Setting up...\')\n\n    # Parse csv files for file names\n    all_filenames = pd.read_csv(\n        args.data_csv,\n        dtype=object,\n        keep_default_na=False,\n        na_values=[]).as_matrix()\n\n    train_filenames = all_filenames[:100]\n    val_filenames = all_filenames[100:]\n\n    # Set up a data reader to handle the file i/o.\n    reader_params = {\'n_examples\': 10,\n                     \'example_size\': [1, 224, 224],\n                     \'extract_examples\': True}\n    reader_example_shapes = {\'features\': {\'x\': reader_params[\'example_size\'] + [NUM_CHANNELS, ]}}\n    reader = Reader(read_fn, {\'features\': {\'x\': tf.float32}})\n\n    # Get input functions and queue initialisation hooks for training and\n    # validation data\n    train_input_fn, train_qinit_hook = reader.get_inputs(\n        file_references=train_filenames,\n        mode=tf.estimator.ModeKeys.TRAIN,\n        example_shapes=reader_example_shapes,\n        batch_size=BATCH_SIZE,\n        shuffle_cache_size=SHUFFLE_CACHE_SIZE,\n        params=reader_params)\n\n    val_input_fn, val_qinit_hook = reader.get_inputs(\n        file_references=val_filenames,\n        mode=tf.estimator.ModeKeys.EVAL,\n        example_shapes=reader_example_shapes,\n        batch_size=BATCH_SIZE,\n        shuffle_cache_size=SHUFFLE_CACHE_SIZE,\n        params=reader_params)\n\n    # Instantiate the neural network estimator\n    nn = tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir=args.model_path,\n        params={""learning_rate"": 0.01},\n        config=tf.estimator.RunConfig())\n\n    # Hooks for validation summaries\n    val_summary_hook = tf.contrib.training.SummaryAtEndHook(\n        os.path.join(args.model_path, \'eval\'))\n    step_cnt_hook = tf.train.StepCounterHook(\n        every_n_steps=EVAL_EVERY_N_STEPS,\n        output_dir=args.model_path)\n\n    print(\'Starting training...\')\n    try:\n        for _ in range(MAX_STEPS // EVAL_EVERY_N_STEPS):\n            nn.train(\n                input_fn=train_input_fn,\n                hooks=[train_qinit_hook, step_cnt_hook],\n                steps=EVAL_EVERY_N_STEPS)\n\n            if args.run_validation:\n                results_val = nn.evaluate(\n                    input_fn=val_input_fn,\n                    hooks=[val_qinit_hook, val_summary_hook],\n                    steps=EVAL_STEPS)\n\n                print(\'Step = {}; val loss = {:.5f};\'.format(\n                    results_val[\'global_step\'], results_val[\'loss\']))\n\n    except KeyboardInterrupt:\n        pass\n\n    print(\'Stopping now.\')\n    export_dir = nn.export_savedmodel(\n        export_dir_base=args.model_path,\n        serving_input_receiver_fn=reader.serving_input_receiver_fn(reader_example_shapes))\n    print(\'Model saved to {}.\'.format(export_dir))\n\n\nif __name__ == \'__main__\':\n    # Set up argument parser\n    parser = argparse.ArgumentParser(description=\'Example: IXI HH convolutional\'\n                                                 \' autoencoder training script\')\n    parser.add_argument(\'--run_validation\', default=True)\n    parser.add_argument(\'--restart\', default=False, action=\'store_true\')\n    parser.add_argument(\'--verbose\', default=False, action=\'store_true\')\n    parser.add_argument(\'--cuda_devices\', \'-c\', default=\'0\')\n\n    parser.add_argument(\'--model_path\', \'-p\', default=\'/tmp/IXI_autoencoder/\')\n    parser.add_argument(\'--data_csv\', default=\'../../../data/IXI_HH/demographic_HH.csv\')\n\n    args = parser.parse_args()\n\n    # Set verbosity\n    if args.verbose:\n        os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'1\'\n        tf.logging.set_verbosity(tf.logging.INFO)\n    else:\n        os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n        tf.logging.set_verbosity(tf.logging.ERROR)\n\n    # GPU allocation options\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.cuda_devices\n\n    # Handle restarting and resuming training\n    if args.restart:\n        print(\'Restarting training from scratch.\')\n        os.system(\'rm -rf {}\'.format(args.model_path))\n\n    if not os.path.isdir(args.model_path):\n        os.system(\'mkdir -p {}\'.format(args.model_path))\n    else:\n        print(\'Resuming training on model_path {}\'.format(args.model_path))\n\n    # Call training\n    train(args)\n'"
examples/applications/IXI_HH_sex_classification_resnet/deploy.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport time\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom tensorflow.contrib import predictor\n\nfrom dltk.io.augmentation import extract_random_example_array\n\nfrom reader import read_fn\n\nREADER_PARAMS = {\'extract_examples\': False}\nN_VALIDATION_SUBJECTS = 28\n\n\ndef predict(args):\n    # Read in the csv with the file names you would want to predict on\n    file_names = pd.read_csv(\n        args.csv,\n        dtype=object,\n        keep_default_na=False,\n        na_values=[]).as_matrix()\n\n    # We trained on the first 4 subjects, so we predict on the rest\n    file_names = file_names[-N_VALIDATION_SUBJECTS:]\n\n    # From the model_path, parse the latest saved model and restore a\n    # predictor from it\n    export_dir = \\\n        [os.path.join(args.model_path, o) for o in sorted(os.listdir(args.model_path))\n         if os.path.isdir(os.path.join(args.model_path, o)) and o.isdigit()][-1]\n    print(\'Loading from {}\'.format(export_dir))\n    my_predictor = predictor.from_saved_model(export_dir)\n\n    # Iterate through the files, predict on the full volumes and compute a Dice\n    # coefficient\n    accuracy = []\n    for output in read_fn(file_references=file_names,\n                          mode=tf.estimator.ModeKeys.EVAL,\n                          params=READER_PARAMS):\n        t0 = time.time()\n\n        # Parse the read function output and add a dummy batch dimension as\n        # required\n        img = output[\'features\'][\'x\']\n        lbl = output[\'labels\'][\'y\']\n        test_id = output[\'img_id\']\n\n        # We know, that the training input shape of [64, 96, 96] will work with\n        # our model strides, so we collect several crops of the test image and\n        # average the predictions. Alternatively, we could pad or crop the input\n        # to any shape that is compatible with the resolution scales of the\n        # model:\n\n        num_crop_predictions = 4\n        crop_batch = extract_random_example_array(\n            image_list=img,\n            example_size=[64, 96, 96],\n            n_examples=num_crop_predictions)\n\n        y_ = my_predictor.session.run(\n            fetches=my_predictor._fetch_tensors[\'y_prob\'],\n            feed_dict={my_predictor._feed_tensors[\'x\']: crop_batch})\n\n        # Average the predictions on the cropped test inputs:\n        y_ = np.mean(y_, axis=0)\n        predicted_class = np.argmax(y_)\n\n        # Calculate the accuracy for this subject\n        accuracy.append(predicted_class == lbl)\n\n        # Print outputs\n        print(\'id={}; pred={}; true={}; run time={:0.2f} s; \'\n              \'\'.format(test_id, predicted_class, lbl[0], time.time() - t0))\n    print(\'accuracy={}\'.format(np.mean(accuracy)))\n\n\nif __name__ == \'__main__\':\n    # Set up argument parser\n    parser = argparse.ArgumentParser(\n        description=\'IXI HH example sex classification deploy script\')\n    parser.add_argument(\'--verbose\', default=False, action=\'store_true\')\n    parser.add_argument(\'--cuda_devices\', \'-c\', default=\'0\')\n\n    parser.add_argument(\'--model_path\', \'-p\',\n                        default=\'/tmp/IXI_sex_classification/\')\n    parser.add_argument(\'--csv\', default=\'../../../data/IXI_HH/demographic_HH.csv\')\n\n    args = parser.parse_args()\n\n    # Set verbosity\n    if args.verbose:\n        os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'1\'\n        tf.logging.set_verbosity(tf.logging.INFO)\n    else:\n        os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n        tf.logging.set_verbosity(tf.logging.ERROR)\n\n    # GPU allocation options\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.cuda_devices\n\n    # Call training\n    predict(args)\n'"
examples/applications/IXI_HH_sex_classification_resnet/reader.py,3,"b'from __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport SimpleITK as sitk\nimport tensorflow as tf\nimport os\nimport numpy as np\n\nfrom dltk.io.augmentation import extract_random_example_array, flip\nfrom dltk.io.preprocessing import whitening\n\n\ndef read_fn(file_references, mode, params=None):\n    """"""A custom python read function for interfacing with nii image files.\n\n    Args:\n        file_references (list): A list of lists containing file references,\n            such as [[\'id_0\', \'image_filename_0\', target_value_0], ...,\n            [\'id_N\', \'image_filename_N\', target_value_N]].\n        mode (str): One of the tf.estimator.ModeKeys strings: TRAIN, EVAL or\n            PREDICT.\n        params (dict, optional): A dictionary to parametrise read_fn outputs\n            (e.g. reader_params = {\'n_examples\': 10, \'example_size\':\n            [64, 64, 64], \'extract_examples\': True}, etc.).\n\n    Yields:\n        dict: A dictionary of reader outputs for dltk.io.abstract_reader.\n    """"""\n\n    def _augment(img):\n        """"""An image augmentation function""""""\n        return flip(img, axis=2)\n\n    for f in file_references:\n        subject_id = f[0]\n\n        data_path = \'../../../data/IXI_HH/2mm\'\n\n        # Read the image nii with sitk\n        t1_fn = os.path.join(data_path, \'{}/T1_2mm.nii.gz\'.format(subject_id))\n        t1 = sitk.GetArrayFromImage(sitk.ReadImage(str(t1_fn)))\n\n        # Normalise volume image\n        t1 = whitening(t1)\n\n        images = np.expand_dims(t1, axis=-1).astype(np.float32)\n\n        if mode == tf.estimator.ModeKeys.PREDICT:\n            yield {\'features\': {\'x\': images}, \'img_id\': subject_id}\n\n        # Parse the sex classes from the file_references [1,2] and shift them\n        # to [0,1]\n        sex = np.int(f[1]) - 1\n        y = np.expand_dims(sex, axis=-1).astype(np.int32)\n\n        # Augment if used in training mode\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            images = _augment(images)\n\n        # Check if the reader is supposed to return training examples or full\n        # images\n        if params[\'extract_examples\']:\n            images = extract_random_example_array(\n                image_list=images,\n                example_size=params[\'example_size\'],\n                n_examples=params[\'n_examples\'])\n\n            for e in range(params[\'n_examples\']):\n                yield {\'features\': {\'x\': images[e].astype(np.float32)},\n                       \'labels\': {\'y\': y.astype(np.float32)},\n                       \'img_id\': subject_id}\n\n        else:\n            yield {\'features\': {\'x\': images},\n                   \'labels\': {\'y\': y.astype(np.float32)},\n                   \'img_id\': subject_id}\n\n    return\n'"
examples/applications/IXI_HH_sex_classification_resnet/train.py,30,"b'# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport argparse\nimport os\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np\n\nfrom dltk.networks.regression_classification.resnet import resnet_3d\nfrom dltk.io.abstract_reader import Reader\n\nfrom reader import read_fn\n\n\nEVAL_EVERY_N_STEPS = 100\nEVAL_STEPS = 5\n\nNUM_CLASSES = 2\nNUM_CHANNELS = 1\n\nBATCH_SIZE = 8\nSHUFFLE_CACHE_SIZE = 32\n\nMAX_STEPS = 50000\n\n\ndef model_fn(features, labels, mode, params):\n    """"""Model function to construct a tf.estimator.EstimatorSpec. It creates a\n        network given input features (e.g. from a dltk.io.abstract_reader) and\n        training targets (labels). Further, loss, optimiser, evaluation ops and\n        custom tensorboard summary ops can be added. For additional information,\n         please refer to https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#model_fn.\n\n    Args:\n        features (tf.Tensor): Tensor of input features to train from. Required\n            rank and dimensions are determined by the subsequent ops\n            (i.e. the network).\n        labels (tf.Tensor): Tensor of training targets or labels. Required rank\n            and dimensions are determined by the network output.\n        mode (str): One of the tf.estimator.ModeKeys: TRAIN, EVAL or PREDICT\n        params (dict, optional): A dictionary to parameterise the model_fn\n            (e.g. learning_rate)\n\n    Returns:\n        tf.estimator.EstimatorSpec: A custom EstimatorSpec for this experiment\n    """"""\n\n    # 1. create a model and its outputs\n    net_output_ops = resnet_3d(\n        features[\'x\'],\n        num_res_units=2,\n        num_classes=NUM_CLASSES,\n        filters=(16, 32, 64, 128, 256),\n        strides=((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2)),\n        mode=mode,\n        kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3))\n\n    # 1.1 Generate predictions only (for `ModeKeys.PREDICT`)\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(\n            mode=mode,\n            predictions=net_output_ops,\n            export_outputs={\'out\': tf.estimator.export.PredictOutput(net_output_ops)})\n\n    # 2. set up a loss function\n    one_hot_labels = tf.reshape(tf.one_hot(labels[\'y\'], depth=NUM_CLASSES), [-1, NUM_CLASSES])\n\n    loss = tf.losses.softmax_cross_entropy(\n        onehot_labels=one_hot_labels,\n        logits=net_output_ops[\'logits\'])\n\n    # 3. define a training op and ops for updating moving averages (i.e. for\n    # batch normalisation)\n    global_step = tf.train.get_global_step()\n    optimiser = tf.train.AdamOptimizer(\n        learning_rate=params[""learning_rate""],\n        epsilon=1e-5)\n\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n        train_op = optimiser.minimize(loss, global_step=global_step)\n\n    # 4.1 (optional) create custom image summaries for tensorboard\n    my_image_summaries = {}\n    my_image_summaries[\'feat_t1\'] = features[\'x\'][0, 32, :, :, 0]\n\n    expected_output_size = [1, 96, 96, 1]  # [B, W, H, C]\n    [tf.summary.image(name, tf.reshape(image, expected_output_size))\n     for name, image in my_image_summaries.items()]\n\n    # 4.2 (optional) track the rmse (scaled back by 100, see reader.py)\n    acc = tf.metrics.accuracy\n    prec = tf.metrics.precision\n    eval_metric_ops = {""accuracy"": acc(labels[\'y\'], net_output_ops[\'y_\']),\n                       ""precision"": prec(labels[\'y\'], net_output_ops[\'y_\'])}\n\n    # 5. Return EstimatorSpec object\n    return tf.estimator.EstimatorSpec(mode=mode,\n                                      predictions=net_output_ops,\n                                      loss=loss,\n                                      train_op=train_op,\n                                      eval_metric_ops=eval_metric_ops)\n\n\ndef train(args):\n    np.random.seed(42)\n    tf.set_random_seed(42)\n\n    print(\'Setting up...\')\n\n    # Parse csv files for file names\n    all_filenames = pd.read_csv(\n        args.data_csv,\n        dtype=object,\n        keep_default_na=False,\n        na_values=[]).as_matrix()\n\n    train_filenames = all_filenames[:150]\n    val_filenames = all_filenames[150:]\n\n    # Set up a data reader to handle the file i/o.\n    reader_params = {\'n_examples\': 2,\n                     \'example_size\': [64, 96, 96],\n                     \'extract_examples\': True}\n\n    reader_example_shapes = {\'features\': {\'x\': reader_params[\'example_size\'] + [NUM_CHANNELS]},\n                             \'labels\': {\'y\': [1]}}\n    reader = Reader(read_fn,\n                    {\'features\': {\'x\': tf.float32},\n                     \'labels\': {\'y\': tf.int32}})\n\n    # Get input functions and queue initialisation hooks for training and\n    # validation data\n    train_input_fn, train_qinit_hook = reader.get_inputs(\n        file_references=train_filenames,\n        mode=tf.estimator.ModeKeys.TRAIN,\n        example_shapes=reader_example_shapes,\n        batch_size=BATCH_SIZE,\n        shuffle_cache_size=SHUFFLE_CACHE_SIZE,\n        params=reader_params)\n\n    val_input_fn, val_qinit_hook = reader.get_inputs(\n        file_references=val_filenames,\n        mode=tf.estimator.ModeKeys.EVAL,\n        example_shapes=reader_example_shapes,\n        batch_size=BATCH_SIZE,\n        shuffle_cache_size=SHUFFLE_CACHE_SIZE,\n        params=reader_params)\n\n    # Instantiate the neural network estimator\n    nn = tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir=args.model_path,\n        params={""learning_rate"": 0.001},\n        config=tf.estimator.RunConfig())\n\n    # Hooks for validation summaries\n    val_summary_hook = tf.contrib.training.SummaryAtEndHook(\n        os.path.join(args.model_path, \'eval\'))\n    step_cnt_hook = tf.train.StepCounterHook(every_n_steps=EVAL_EVERY_N_STEPS,\n                                             output_dir=args.model_path)\n\n    print(\'Starting training...\')\n    try:\n        for _ in range(MAX_STEPS // EVAL_EVERY_N_STEPS):\n            nn.train(\n                input_fn=train_input_fn,\n                hooks=[train_qinit_hook, step_cnt_hook],\n                steps=EVAL_EVERY_N_STEPS)\n\n            if args.run_validation:\n                results_val = nn.evaluate(\n                    input_fn=val_input_fn,\n                    hooks=[val_qinit_hook, val_summary_hook],\n                    steps=EVAL_STEPS)\n                print(\'Step = {}; val loss = {:.5f};\'.format(\n                    results_val[\'global_step\'],\n                    results_val[\'loss\']))\n\n    except KeyboardInterrupt:\n        pass\n\n    # When exporting we set the expected input shape to be arbitrary.\n    export_dir = nn.export_savedmodel(\n        export_dir_base=args.model_path,\n        serving_input_receiver_fn=reader.serving_input_receiver_fn(\n            {\'features\': {\'x\': [None, None, None, NUM_CHANNELS]},\n             \'labels\': {\'y\': [1]}}))\n    print(\'Model saved to {}.\'.format(export_dir))\n\n\nif __name__ == \'__main__\':\n    # Set up argument parser\n    parser = argparse.ArgumentParser(description=\'Example: IXI HH resnet sex classification training\')\n    parser.add_argument(\'--run_validation\', default=True)\n    parser.add_argument(\'--restart\', default=False, action=\'store_true\')\n    parser.add_argument(\'--verbose\', default=False, action=\'store_true\')\n    parser.add_argument(\'--cuda_devices\', \'-c\', default=\'0\')\n\n    parser.add_argument(\'--model_path\', \'-p\', default=\'/tmp/IXI_sex_classification/\')\n    parser.add_argument(\'--data_csv\', default=\'../../../data/IXI_HH/demographic_HH.csv\')\n\n    args = parser.parse_args()\n\n    # Set verbosity\n    if args.verbose:\n        os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'1\'\n        tf.logging.set_verbosity(tf.logging.INFO)\n    else:\n        os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n        tf.logging.set_verbosity(tf.logging.ERROR)\n\n    # GPU allocation options\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.cuda_devices\n\n    # Handle restarting and resuming training\n    if args.restart:\n        print(\'Restarting training from scratch.\')\n        os.system(\'rm -rf {}\'.format(args.model_path))\n\n    if not os.path.isdir(args.model_path):\n        os.system(\'mkdir -p {}\'.format(args.model_path))\n    else:\n        print(\'Resuming training on model_path {}\'.format(args.model_path))\n\n    # Call training\n    train(args)\n'"
examples/applications/IXI_HH_superresolution/reader.py,3,"b'\nfrom __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport SimpleITK as sitk\nimport tensorflow as tf\nimport os\nimport numpy as np\n\nfrom dltk.io.augmentation import flip, extract_random_example_array\nfrom dltk.io.preprocessing import whitening\n\n\ndef read_fn(file_references, mode, params=None):\n    """"""A custom python read function for interfacing with nii image files.\n\n    Args:\n        file_references (list): A list of lists containing file references, such\n            as [[\'id_0\', \'image_filename_0\', target_value_0], ...,\n            [\'id_N\', \'image_filename_N\', target_value_N]].\n        mode (str): One of the tf.estimator.ModeKeys strings: TRAIN, EVAL or\n            PREDICT.\n        params (dict, optional): A dictionary to parameterise read_fn ouputs\n            (e.g. reader_params = {\'n_examples\': 10, \'example_size\':\n            [64, 64, 64], \'extract_examples\': True}, etc.).\n\n    Yields:\n        dict: A dictionary of reader outputs for dltk.io.abstract_reader.\n    """"""\n\n    def _augment(img):\n        """"""An image augmentation function""""""\n        return flip(img, axis=2)\n\n    for f in file_references:\n        subject_id = f[0]\n\n        data_path = \'../../../data/IXI_HH/1mm\'\n\n        # Read the image nii with sitk\n        t1_fn = os.path.join(data_path, \'{}/T1_1mm.nii.gz\'.format(subject_id))\n        t1 = sitk.GetArrayFromImage(sitk.ReadImage(str(t1_fn)))\n\n        # Normalise volume images\n        t1 = whitening(t1)\n\n        # Create a 4D image (i.e. [x, y, z, channels])\n        images = np.expand_dims(t1, axis=-1).astype(np.float32)\n\n        if mode == tf.estimator.ModeKeys.PREDICT:\n            yield {\'features\': {\'x\': images}}\n\n        # Augment if used in training mode\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            images = _augment(images)\n\n        # Check if the reader is supposed to return training examples or full\n        # images\n        if params[\'extract_examples\']:\n            images = extract_random_example_array(\n                image_list=images,\n                example_size=params[\'example_size\'],\n                n_examples=params[\'n_examples\'])\n\n            for e in range(params[\'n_examples\']):\n                yield {\'features\': {\'x\': images[e].astype(np.float32)}}\n        else:\n            yield {\'features\': {\'x\': images}}\n\n    return\n'"
examples/applications/IXI_HH_superresolution/train.py,30,"b'# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport argparse\nimport os\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np\n\nfrom dltk.core.upsample import linear_upsample_3d\nfrom dltk.networks.super_resolution.simple_super_resolution import simple_super_resolution_3d\nfrom dltk.io.abstract_reader import Reader\n\nfrom reader import read_fn\n\nEVAL_EVERY_N_STEPS = 100\nEVAL_STEPS = 2\n\nNUM_CHANNELS = 1\n\nBATCH_SIZE = 8\nSHUFFLE_CACHE_SIZE = 64\n\nMAX_STEPS = 250000\n\nUPSAMPLING_FACTOR = [4, 4, 4]\n\n\ndef model_fn(features, labels, mode, params):\n    """"""Model function to construct a tf.estimator.EstimatorSpec. It creates a\n        network given input features (e.g. from a dltk.io.abstract_reader) and\n        training targets (labels). Further, loss, optimiser, evaluation ops and\n        custom tensorboard summary ops can be added. For additional information,\n        please refer to https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#model_fn.\n\n    Args:\n        features (tf.Tensor): Tensor of input features to train from. Required\n            rank and dimensions are determined by the subsequent ops\n            (i.e. the network).\n        labels (tf.Tensor): Tensor of training targets or labels. Required rank\n            and dimensions are determined by the network output.\n        mode (str): One of the tf.estimator.ModeKeys: TRAIN, EVAL or PREDICT\n        params (dict, optional): A dictionary to parameterise the model_fn\n            (e.g. learning_rate)\n\n    Returns:\n        tf.estimator.EstimatorSpec: A custom EstimatorSpec for this experiment\n    """"""\n\n    assert len(params[\'upsampling_factor\']) == 3\n\n    # During training downsample the original input to create a lower resolution\n    #  image\n    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n        lo_res = tf.layers.average_pooling3d(\n            inputs=features[\'x\'],\n            pool_size=[2 * s if s > 1 else 1 for s in params[\'upsampling_factor\']],\n            strides=params[\'upsampling_factor\'],\n            padding=\'same\',\n            name=\'training_downsample\')\n\n        # Compute some linear upsampling for comparison:\n        lin_up = linear_upsample_3d(inputs=lo_res,\n                                    strides=params[\'upsampling_factor\'])\n    else:\n        lo_res = features[\'x\']\n\n    # 1. create a model and its outputs\n    net_output_ops = simple_super_resolution_3d(\n        inputs=lo_res,\n        num_convolutions=2,\n        upsampling_factor=params[\'upsampling_factor\'],\n        filters=(16, 32, 64, 128),\n        mode=mode)\n\n    # 1.1 Generate predictions only (for `ModeKeys.PREDICT`)\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(\n            mode=mode,\n            predictions=net_output_ops,\n            export_outputs={\'out\': tf.estimator.export.PredictOutput(net_output_ops)})\n\n    # 2. set up a loss function\n    loss = tf.losses.mean_squared_error(\n        labels=features[\'x\'],\n        predictions=net_output_ops[\'x_\'])\n\n    # 3. define a training op and ops for updating moving averages\n    # (i.e. for batch normalisation)\n    global_step = tf.train.get_global_step()\n    optimiser = tf.train.AdamOptimizer(\n        learning_rate=params[\'learning_rate\'],\n        epsilon=1e-5)\n\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n        train_op = optimiser.minimize(loss, global_step=global_step)\n\n    # 4.1 (optional) create custom image summaries for tensorboard\n    tf.summary.image(\'feat_lo_res\', tf.reshape(lo_res[0, 4, :, :, 0:1], [1, 32, 32, 1]))\n\n    my_image_summaries = {}\n    my_image_summaries[\'feat_hi_res\'] = features[\'x\'][0, 16, :, :, 0:1]\n    my_image_summaries[\'linear_up_hi_res\'] = tf.cast(lin_up, tf.float32)[0, 16, :, :, 0:1]\n    my_image_summaries[\'pred_hi_res\'] = tf.cast(net_output_ops[\'x_\'], tf.float32)[0, 16, :, :, 0:1]\n\n    expected_output_size = [1, 128, 128, 1]  # [B, W, H, C]\n    [tf.summary.image(name, tf.reshape(image, expected_output_size))\n     for name, image in my_image_summaries.items()]\n\n    # 5. Return EstimatorSpec object\n    return tf.estimator.EstimatorSpec(mode=mode,\n                                      predictions=net_output_ops,\n                                      loss=loss,\n                                      train_op=train_op,\n                                      eval_metric_ops=None)\n\n\ndef train(args):\n    np.random.seed(42)\n    tf.set_random_seed(42)\n\n    print(\'Setting up...\')\n\n    # Parse csv files for file names\n    all_filenames = pd.read_csv(\n        args.data_csv,\n        dtype=object,\n        keep_default_na=False,\n        na_values=[]).as_matrix()\n\n    train_filenames = all_filenames[:100]\n    val_filenames = all_filenames[100:]\n\n    # Set up a data reader to handle the file i/o.\n    reader_params = {\'n_examples\': 8,\n                     \'example_size\': [32, 128, 128],\n                     \'extract_examples\': True}\n    reader_example_shapes = {\'features\': {\'x\': reader_params[\'example_size\'] + [NUM_CHANNELS, ]}}\n\n    reader = Reader(read_fn, {\'features\': {\'x\': tf.float32}})\n\n    # Get input functions and queue initialisation hooks for training and\n    # validation data\n    train_input_fn, train_qinit_hook = reader.get_inputs(\n        file_references=train_filenames,\n        mode=tf.estimator.ModeKeys.TRAIN,\n        example_shapes=reader_example_shapes,\n        batch_size=BATCH_SIZE,\n        shuffle_cache_size=SHUFFLE_CACHE_SIZE,\n        params=reader_params)\n\n    val_input_fn, val_qinit_hook = reader.get_inputs(\n        file_references=val_filenames,\n        mode=tf.estimator.ModeKeys.EVAL,\n        example_shapes=reader_example_shapes,\n        batch_size=BATCH_SIZE,\n        shuffle_cache_size=SHUFFLE_CACHE_SIZE,\n        params=reader_params)\n\n    # Instantiate the neural network estimator\n    nn = tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir=args.model_path,\n        params={\'learning_rate\': 0.01, \'upsampling_factor\': UPSAMPLING_FACTOR},\n        config=tf.estimator.RunConfig())\n\n    # Hooks for validation summaries\n    val_summary_hook = tf.contrib.training.SummaryAtEndHook(\n        os.path.join(args.model_path, \'eval\'))\n    step_cnt_hook = tf.train.StepCounterHook(\n        every_n_steps=EVAL_EVERY_N_STEPS,\n        output_dir=args.model_path)\n\n    print(\'Starting training...\')\n    try:\n        for _ in range(MAX_STEPS // EVAL_EVERY_N_STEPS):\n            nn.train(\n                input_fn=train_input_fn,\n                hooks=[train_qinit_hook, step_cnt_hook],\n                steps=EVAL_EVERY_N_STEPS)\n\n            if args.run_validation:\n                results_val = nn.evaluate(\n                    input_fn=val_input_fn,\n                    hooks=[val_qinit_hook, val_summary_hook],\n                    steps=EVAL_STEPS)\n                print(\'Step = {}; val loss = {:.5f};\'.format(\n                    results_val[\'global_step\'], results_val[\'loss\']))\n\n    except KeyboardInterrupt:\n        pass\n\n    print(\'Stopping now.\')\n    export_dir = nn.export_savedmodel(\n        export_dir_base=args.model_path,\n        serving_input_receiver_fn=reader.serving_input_receiver_fn(reader_example_shapes))\n    print(\'Model saved to {}.\'.format(export_dir))\n\n\nif __name__ == \'__main__\':\n    # Set up argument parser\n    parser = argparse.ArgumentParser(\n        description=\'Example: IXI HH simple super-resolution training script\')\n    parser.add_argument(\'--run_validation\', default=True)\n    parser.add_argument(\'--restart\', default=False, action=\'store_true\')\n    parser.add_argument(\'--verbose\', default=False, action=\'store_true\')\n    parser.add_argument(\'--cuda_devices\', \'-c\', default=\'0\')\n\n    parser.add_argument(\'--model_path\', \'-p\', default=\'/tmp/IXI_super_resolution/\')\n    parser.add_argument(\'--data_csv\', default=\'../../../data/IXI_HH/demographic_HH.csv\')\n\n    args = parser.parse_args()\n\n    # Set verbosity\n    if args.verbose:\n        os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'1\'\n        tf.logging.set_verbosity(tf.logging.INFO)\n    else:\n        os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n        tf.logging.set_verbosity(tf.logging.ERROR)\n\n    # GPU allocation options\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.cuda_devices\n\n    # Handle restarting and resuming training\n    if args.restart:\n        print(\'Restarting training from scratch.\')\n        os.system(\'rm -rf {}\'.format(args.model_path))\n\n    if not os.path.isdir(args.model_path):\n        os.system(\'mkdir -p {}\'.format(args.model_path))\n    else:\n        print(\'Resuming training on model_path {}\'.format(args.model_path))\n\n    # Call training\n    train(args)\n'"
examples/applications/MRBrainS13_tissue_segmentation/deploy.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport time\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport SimpleITK as sitk\n\nfrom tensorflow.contrib import predictor\n\nfrom dltk.core import metrics as metrics\n\nfrom dltk.utils import sliding_window_segmentation_inference\n\n\nfrom reader import read_fn\n\nREADER_PARAMS = {\'extract_examples\': False}\nN_VALIDATION_SUBJECTS = 1\n\n\ndef predict(args):\n    # Read in the csv with the file names you would want to predict on\n    file_names = pd.read_csv(\n        args.csv,\n        dtype=object,\n        keep_default_na=False,\n        na_values=[]).as_matrix()\n\n    # We trained on the first 4 subjects, so we predict on the rest\n    file_names = file_names[-N_VALIDATION_SUBJECTS:]\n\n    # From the model_path, parse the latest saved model and restore a\n    # predictor from it\n    export_dir = [os.path.join(args.model_path, o) for o in os.listdir(args.model_path)\n                  if os.path.isdir(os.path.join(args.model_path, o)) and o.isdigit()][-1]\n    print(\'Loading from {}\'.format(export_dir))\n    my_predictor = predictor.from_saved_model(export_dir)\n\n    # Fetch the output probability op of the trained network\n    y_prob = my_predictor._fetch_tensors[\'y_prob\']\n    num_classes = y_prob.get_shape().as_list()[-1]\n\n    # Iterate through the files, predict on the full volumes and compute a Dice\n    # coefficient\n    for output in read_fn(file_references=file_names,\n                          mode=tf.estimator.ModeKeys.EVAL,\n                          params=READER_PARAMS):\n        t0 = time.time()\n\n        # Parse the read function output and add a dummy batch dimension as\n        # required\n        img = np.expand_dims(output[\'features\'][\'x\'], axis=0)\n        lbl = np.expand_dims(output[\'labels\'][\'y\'], axis=0)\n\n        # Do a sliding window inference with our DLTK wrapper\n        pred = sliding_window_segmentation_inference(\n            session=my_predictor.session,\n            ops_list=[y_prob],\n            sample_dict={my_predictor._feed_tensors[\'x\']: img},\n            batch_size=32)[0]\n\n        # Calculate the prediction from the probabilities\n        pred = np.argmax(pred, -1)\n\n        # Calculate the Dice coefficient\n        dsc = np.nanmean(metrics.dice(pred, lbl, num_classes)[1:])\n\n        # Save the file as .nii.gz using the header information from the\n        # original sitk image\n        output_fn = os.path.join(args.model_path, \'{}_seg.nii.gz\'.format(output[\'subject_id\']))\n\n        new_sitk = sitk.GetImageFromArray(pred[0].astype(np.int32))\n        new_sitk.CopyInformation(output[\'sitk\'])\n\n        sitk.WriteImage(new_sitk, output_fn)\n\n        # Print outputs\n        print(\'Id={}; Dice={:0.4f}; time={:0.2} secs; output_path={};\'.format(\n            output[\'subject_id\'], dsc, time.time() - t0, output_fn))\n\n\nif __name__ == \'__main__\':\n    # Set up argument parser\n    parser = argparse.ArgumentParser(description=\'MRBrainS13 example segmentation deploy script\')\n    parser.add_argument(\'--verbose\', default=False, action=\'store_true\')\n    parser.add_argument(\'--cuda_devices\', \'-c\', default=\'0\')\n\n    parser.add_argument(\'--model_path\', \'-p\', default=\'/tmp/mrbrains_segmentation/\')\n    parser.add_argument(\'--csv\', default=\'mrbrains.csv\')\n\n    args = parser.parse_args()\n\n    # Set verbosity\n    if args.verbose:\n        os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'1\'\n        tf.logging.set_verbosity(tf.logging.INFO)\n    else:\n        os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n        tf.logging.set_verbosity(tf.logging.ERROR)\n\n    # GPU allocation options\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.cuda_devices\n\n    # Call training\n    predict(args)\n'"
examples/applications/MRBrainS13_tissue_segmentation/reader.py,3,"b'from __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport SimpleITK as sitk\nimport tensorflow as tf\nimport os\nimport numpy as np\n\nfrom dltk.io.augmentation import add_gaussian_noise, flip, extract_class_balanced_example_array\nfrom dltk.io.preprocessing import whitening\n\n\ndef read_fn(file_references, mode, params=None):\n    """"""A custom python read function for interfacing with nii image files.\n\n    Args:\n        file_references (list): A list of lists containing file references, such\n            as [[\'id_0\', \'image_filename_0\', target_value_0], ...,\n            [\'id_N\', \'image_filename_N\', target_value_N]].\n        mode (str): One of the tf.estimator.ModeKeys strings: TRAIN, EVAL or\n            PREDICT.\n        params (dict, optional): A dictionary to parameterise read_fn ouputs\n            (e.g. reader_params = {\'n_examples\': 10, \'example_size\':\n            [64, 64, 64], \'extract_examples\': True}, etc.).\n\n    Yields:\n        dict: A dictionary of reader outputs for dltk.io.abstract_reader.\n    """"""\n\n    def _augment(img, lbl):\n        """"""An image augmentation function""""""\n        img = add_gaussian_noise(img, sigma=0.1)\n        [img, lbl] = flip([img, lbl], axis=1)\n\n        return img, lbl\n\n    for f in file_references:\n        subject_id = f[0]\n        img_fn = f[1]\n\n        # Read the image nii with sitk and keep the pointer to the sitk.Image\n        # of an input\n        t1_sitk = sitk.ReadImage(str(os.path.join(img_fn, \'T1.nii\')))\n        t1 = sitk.GetArrayFromImage(t1_sitk)\n        t1_ir = sitk.GetArrayFromImage(\n            sitk.ReadImage(str(os.path.join(img_fn, \'T1_IR.nii\'))))\n        t2_fl = sitk.GetArrayFromImage(\n            sitk.ReadImage(str(os.path.join(img_fn, \'T2_FLAIR.nii\'))))\n\n        # Normalise volume images\n        t1 = whitening(t1)\n        t1_ir = whitening(t1_ir)\n        t2_fl = whitening(t2_fl)\n\n        # Create a 4D multi-sequence image (i.e. [channels, x, y, z])\n        images = np.stack([t1, t1_ir, t2_fl], axis=-1).astype(np.float32)\n\n        if mode == tf.estimator.ModeKeys.PREDICT:\n            yield {\'features\': {\'x\': images},\n                   \'labels\': None,\n                   \'sitk\': t1_sitk,\n                   \'subject_id\': subject_id}\n\n        lbl = sitk.GetArrayFromImage(sitk.ReadImage(str(os.path.join(\n            img_fn,\n            \'LabelsForTraining.nii\')))).astype(np.int32)\n\n        # Augment if used in training mode\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            images, lbl = _augment(images, lbl)\n\n        # Check if the reader is supposed to return training examples or full\n        #  images\n        if params[\'extract_examples\']:\n            n_examples = params[\'n_examples\']\n            example_size = params[\'example_size\']\n\n            images, lbl = extract_class_balanced_example_array(\n                image=images,\n                label=lbl,\n                example_size=example_size,\n                n_examples=n_examples,\n                classes=9)\n\n            for e in range(len(images)):\n                yield {\'features\': {\'x\': images[e].astype(np.float32)},\n                       \'labels\': {\'y\': lbl[e].astype(np.int32)},\n                       \'subject_id\': subject_id}\n        else:\n            yield {\'features\': {\'x\': images},\n                   \'labels\': {\'y\': lbl},\n                   \'sitk\': t1_sitk,\n                   \'subject_id\': subject_id}\n\n    return\n'"
examples/applications/MRBrainS13_tissue_segmentation/train.py,33,"b'# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport argparse\nimport os\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np\n\nfrom dltk.core.metrics import dice\nfrom dltk.networks.segmentation.unet import residual_unet_3d\nfrom dltk.io.abstract_reader import Reader\n\nfrom reader import read_fn\n\n\nEVAL_EVERY_N_STEPS = 100\nEVAL_STEPS = 1\n\nNUM_CLASSES = 9\nNUM_CHANNELS = 3\n\nNUM_FEATURES_IN_SUMMARIES = min(4, NUM_CHANNELS)\n\nBATCH_SIZE = 16\nSHUFFLE_CACHE_SIZE = 64\n\nMAX_STEPS = 50000\n\n\ndef model_fn(features, labels, mode, params):\n    """"""Model function to construct a tf.estimator.EstimatorSpec. It creates a\n        network given input features (e.g. from a dltk.io.abstract_reader) and\n        training targets (labels). Further, loss, optimiser, evaluation ops and\n        custom tensorboard summary ops can be added. For additional information,\n        please refer to https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#model_fn.\n\n    Args:\n        features (tf.Tensor): Tensor of input features to train from. Required\n            rank and dimensions are determined by the subsequent ops\n            (i.e. the network).\n        labels (tf.Tensor): Tensor of training targets or labels. Required rank\n            and dimensions are determined by the network output.\n        mode (str): One of the tf.estimator.ModeKeys: TRAIN, EVAL or PREDICT\n        params (dict, optional): A dictionary to parameterise the model_fn\n            (e.g. learning_rate)\n\n    Returns:\n        tf.estimator.EstimatorSpec: A custom EstimatorSpec for this experiment\n    """"""\n\n    # 1. create a model and its outputs\n    net_output_ops = residual_unet_3d(\n        inputs=features[\'x\'],\n        num_classes=NUM_CLASSES,\n        num_res_units=2,\n        filters=(16, 32, 64, 128),\n        strides=((1, 1, 1), (1, 2, 2), (1, 2, 2), (1, 2, 2)),\n        mode=mode,\n        kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4))\n\n    # 1.1 Generate predictions only (for `ModeKeys.PREDICT`)\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(\n            mode=mode,\n            predictions=net_output_ops,\n            export_outputs={\'out\': tf.estimator.export.PredictOutput(net_output_ops)})\n\n    # 2. set up a loss function\n    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        logits=net_output_ops[\'logits\'],\n        labels=labels[\'y\'])\n    loss = tf.reduce_mean(ce)\n\n    # 3. define a training op and ops for updating moving averages\n    # (i.e. for batch normalisation)\n    global_step = tf.train.get_global_step()\n    optimiser = tf.train.MomentumOptimizer(\n        learning_rate=params[""learning_rate""],\n        momentum=0.9)\n\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n        train_op = optimiser.minimize(loss, global_step=global_step)\n\n    # 4.1 (optional) create custom image summaries for tensorboard\n    my_image_summaries = {}\n    my_image_summaries[\'feat_t1\'] = features[\'x\'][0, 0, :, :, 0]\n    my_image_summaries[\'feat_t1_ir\'] = features[\'x\'][0, 0, :, :, 1]\n    my_image_summaries[\'feat_t2_flair\'] = features[\'x\'][0, 0, :, :, 2]\n    my_image_summaries[\'labels\'] = tf.cast(labels[\'y\'], tf.float32)[0, 0, :, :]\n    my_image_summaries[\'predictions\'] = tf.cast(net_output_ops[\'y_\'], tf.float32)[0, 0, :, :]\n\n    expected_output_size = [1, 128, 128, 1]  # [B, W, H, C]\n    [tf.summary.image(name, tf.reshape(image, expected_output_size))\n     for name, image in my_image_summaries.items()]\n\n    # 4.2 (optional) create custom metric summaries for tensorboard\n    dice_tensor = tf.py_func(dice, [net_output_ops[\'y_\'],\n                                    labels[\'y\'],\n                                    tf.constant(NUM_CLASSES)], tf.float32)\n    [tf.summary.scalar(\'dsc_l{}\'.format(i), dice_tensor[i])\n     for i in range(NUM_CLASSES)]\n\n    # 5. Return EstimatorSpec object\n    return tf.estimator.EstimatorSpec(mode=mode,\n                                      predictions=net_output_ops,\n                                      loss=loss,\n                                      train_op=train_op,\n                                      eval_metric_ops=None)\n\n\ndef train(args):\n\n    np.random.seed(42)\n    tf.set_random_seed(42)\n\n    print(\'Setting up...\')\n\n    # Parse csv files for file names\n    all_filenames = pd.read_csv(\n        args.train_csv,\n        dtype=object,\n        keep_default_na=False,\n        na_values=[]).as_matrix()\n\n    train_filenames = all_filenames[1:4]\n    val_filenames = all_filenames[4:5]\n\n    # Set up a data reader to handle the file i/o.\n    reader_params = {\'n_examples\': 18,\n                     \'example_size\': [4, 128, 128],\n                     \'extract_examples\': True}\n    reader_example_shapes = {\'features\': {\'x\': reader_params[\'example_size\'] + [NUM_CHANNELS, ]},\n                             \'labels\': {\'y\': reader_params[\'example_size\']}}\n    reader = Reader(read_fn,\n                    {\'features\': {\'x\': tf.float32},\n                     \'labels\': {\'y\': tf.int32}})\n\n    # Get input functions and queue initialisation hooks for training and\n    # validation data\n    train_input_fn, train_qinit_hook = reader.get_inputs(\n        file_references=train_filenames,\n        mode=tf.estimator.ModeKeys.TRAIN,\n        example_shapes=reader_example_shapes,\n        batch_size=BATCH_SIZE,\n        shuffle_cache_size=SHUFFLE_CACHE_SIZE,\n        params=reader_params)\n\n    val_input_fn, val_qinit_hook = reader.get_inputs(\n        file_references=val_filenames,\n        mode=tf.estimator.ModeKeys.EVAL,\n        example_shapes=reader_example_shapes,\n        batch_size=BATCH_SIZE,\n        shuffle_cache_size=SHUFFLE_CACHE_SIZE,\n        params=reader_params)\n\n    # Instantiate the neural network estimator\n    nn = tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir=args.model_path,\n        params={""learning_rate"": 0.001},\n        config=tf.estimator.RunConfig())\n\n    # Hooks for validation summaries\n    val_summary_hook = tf.contrib.training.SummaryAtEndHook(\n        os.path.join(args.model_path, \'eval\'))\n    step_cnt_hook = tf.train.StepCounterHook(\n        every_n_steps=EVAL_EVERY_N_STEPS,\n        output_dir=args.model_path)\n\n    print(\'Starting training...\')\n    try:\n        for _ in range(MAX_STEPS // EVAL_EVERY_N_STEPS):\n            nn.train(\n                input_fn=train_input_fn,\n                hooks=[train_qinit_hook, step_cnt_hook],\n                steps=EVAL_EVERY_N_STEPS)\n\n            if args.run_validation:\n                results_val = nn.evaluate(\n                    input_fn=val_input_fn,\n                    hooks=[val_qinit_hook, val_summary_hook],\n                    steps=EVAL_STEPS)\n                print(\'Step = {}; val loss = {:.5f};\'.format(\n                    results_val[\'global_step\'], results_val[\'loss\']))\n\n    except KeyboardInterrupt:\n        pass\n\n    print(\'Stopping now.\')\n    export_dir = nn.export_savedmodel(\n        export_dir_base=args.model_path,\n        serving_input_receiver_fn=reader.serving_input_receiver_fn(reader_example_shapes))\n    print(\'Model saved to {}.\'.format(export_dir))\n\n\nif __name__ == \'__main__\':\n\n    # Set up argument parser\n    parser = argparse.ArgumentParser(\n        description=\'Example: MRBrainS13 example segmentation training script\')\n    parser.add_argument(\'--run_validation\', default=True)\n    parser.add_argument(\'--restart\', default=False, action=\'store_true\')\n    parser.add_argument(\'--verbose\', default=False, action=\'store_true\')\n    parser.add_argument(\'--cuda_devices\', \'-c\', default=\'0\')\n\n    parser.add_argument(\'--model_path\', \'-p\', default=\'/tmp/mrbrains_segmentation/\')\n    parser.add_argument(\'--train_csv\', default=\'mrbrains.csv\')\n\n    args = parser.parse_args()\n\n    # Set verbosity\n    if args.verbose:\n        os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'1\'\n        tf.logging.set_verbosity(tf.logging.INFO)\n    else:\n        os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n        tf.logging.set_verbosity(tf.logging.ERROR)\n\n    # GPU allocation options\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.cuda_devices\n\n    # Handle restarting and resuming training\n    if args.restart:\n        print(\'Restarting training from scratch.\')\n        os.system(\'rm -rf {}\'.format(args.model_path))\n\n    if not os.path.isdir(args.model_path):\n        os.system(\'mkdir -p {}\'.format(args.model_path))\n    else:\n        print(\'Resuming training on model_path {}\'.format(args.model_path))\n\n    # Call training\n    train(args)\n'"
