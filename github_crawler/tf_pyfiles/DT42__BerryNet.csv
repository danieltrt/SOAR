file_path,api_count,code
setup.py,0,"b'import subprocess\n\nfrom setuptools import setup\nfrom setuptools import find_packages\n\n\nsetup(\n    name=\'berrynet\',\n    version=\'v3.7.0\',\n    description=\'BerryNet\',\n    long_description=\n        \'TBD\',\n    url=\'https://github.com/DT42/BerryNet\',\n    author=\'DT42 Inc.\',\n    author_email=\'berrynet@dt42.io\',\n    license=\'GPLv3\',\n    # https://pypi.python.org/pypi?%3Aaction=list_classifiers\n    classifiers=[\n        \'Development Status :: 4 - Beta\',\n        \'Intended Audience :: Science/Research\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'License :: OSI Approved :: GNU General Public License v3 (GPLv3)\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.4\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n    ],\n    keywords=[\'wheels\'],\n    packages=find_packages(exclude=[\'tests\']),\n    install_requires=[\n        \'logzero\',\n        \'paho-mqtt\'\n    ],\n    # Without TF: pip3 install berrynet\n    # TF CPU mode: pip3 install berrynet[tf]\n    # TF GPU mode: pip3 install berrynet[tf_gpu]\n    # TF GPU mode and Wheel-version OpenCV: pip3 install berrynet[tf_gpu, opencv]\n    extra_require={\n        \'tf\': [\'tensorflow>=1.2.1\'],\n        \'tf_gpu\': [\'tensorflow-gpu>=1.2.1\'],\n        \'opencv\': [\'opencv-python\']\n    },\n    python_requires=\'>=3\',  # recognized by pip >= 9.0.0\n    # The ""Including Data Files"" session\n    # https://setuptools.readthedocs.io/en/latest/setuptools.html\n    #\n    # Warning: package_data only looks for module\'s top level\n    #package_data={\n    #},\n    #data_files=[\n    #    (\'docs\', [\'docs/cheatsheet.txt\', \'docs/references.txt\']),\n    #],\n    #scripts=[\n    #    \'bin/purescript.sh\'\n    #],\n    # The ""Automatic Script Creation"" session\n    # https://setuptools.readthedocs.io/en/latest/setuptools.html\n    #\n    # http://python-packaging.readthedocs.io/en/latest/command-line-scripts.html\n    entry_points={\n        \'console_scripts\': [\n            \'bn_camera=berrynet.client.camera:main\',\n            \'bn_dashboard=berrynet.client.fbdashboard:main\',\n            \'bn_data_collector=berrynet.client.data_collector:main\',\n            \'bn_gmail=berrynet.client.gmail:main\',\n            \'bn_telegram=berrynet.client.telegram_bot:main\',\n            \'bn_tflite=berrynet.service.tflite_service:main\',\n            \'bn_openvino=berrynet.service.openvino_service:main\',\n            \'bn_darknet=berrynet.service.darknet_service:main\'\n        ]\n    },\n    #ext_modules=[\n    #    Extension(\n    #        \'hellowheels.lib.clib\',\n    #        [\n    #            \'hellowheels/lib/clib.c\',\n    #            \'hellowheels/lib/ext.c\'\n    #        ]\n    #    )\n    #],\n    #cmdclass={\n    #    \'install\': CustomInstallCommand\n    #},\n    test_suite=\'tests\'\n)\n'"
berrynet/__init__.py,0,"b""import os\n\nfrom logzero import setup_logger\n\n\n# Save log file at different place to prevent permission error.\nif os.geteuid() == 0:  # root\n    LOGGING_FLLEPATH='/tmp/berrynet.log'\nelse:\n    LOGGING_FLLEPATH='{}/.cache/berrynet.log'.format(os.getenv('HOME'))\n\nlogger = setup_logger(name='berrynet-logger', logfile=LOGGING_FLLEPATH)\n"""
berrynet/dlmodelmgr.py,0,"b'# Copyright 2017 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""\nDL Model Manager, following the DLModelBox model package\nspeccification.\n""""""\n\nfrom __future__ import print_function\n\nimport argparse\nimport json\nimport os\n\nfrom berrynet import logger\n\n\nclass DLModelManager(object):\n    def __init__(self):\n        self.basedir = \'/usr/share/dlmodels\'\n\n    def get_model_names(self):\n        return os.listdir(self.basedir)\n\n    def get_model_meta(self, modelname):\n        meta_filepath = os.path.join(self.basedir, modelname, \'meta.json\')\n        with open(meta_filepath, \'r\') as f:\n            meta = json.load(f)\n        meta[\'model\'] = os.path.join(self.basedir, modelname, meta[\'model\'])\n        meta[\'label\'] = os.path.join(self.basedir, modelname, meta[\'label\'])\n        for k, v in meta[\'config\'].items():\n            meta[\'config\'][k] = os.path.join(self.basedir, modelname,\n                                             meta[\'config\'][k])\n        return meta\n\n\ndef parse_args():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\'--modelname\',\n                    help=\'Model package name (without version)\')\n    return vars(ap.parse_args())\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    logger.debug(\'model package name: \', args[\'modelname\'])\n\n    dlmm = DLModelManager()\n    for name in dlmm.get_model_names():\n        print(dlmm.get_model_meta(name))\n'"
berrynet/utils.py,0,"b'# Copyright 2018 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Utility Functions.\n""""""\n\nimport math\n\nimport cv2\n\nfrom berrynet.comm import payload\n\n\ndef generate_class_color(class_num=20):\n    """"""Generate a RGB color set based on given class number.\n\n    Args:\n        class_num: Default is VOC dataset class number.\n\n    Returns:\n        A tuple containing RGB colors.\n    """"""\n    colors = [(1, 0, 1), (0, 0, 1), (0, 1, 1),\n              (0, 1, 0), (1, 1, 0), (1, 0, 0)]\n    const = 1234567  # only for offset calculation\n\n    colorset = []\n    for cls_i in range(class_num):\n        offset = cls_i * const % class_num\n\n        ratio = (float(offset) / class_num) * (len(colors) - 1)\n        i = math.floor(ratio)\n        j = math.ceil(ratio)\n        ratio -= i\n\n        rgb = []\n        for ch_i in range(3):\n            r = (1 - ratio) * colors[i][ch_i] + ratio * colors[j][ch_i]\n            rgb.append(math.ceil(r * 255))\n        colorset.append(tuple(rgb[::-1]))\n    return tuple(colorset)\n\n\ndef draw_bb(bgr_nparr, infres, class_colors, labels):\n    """"""Draw bounding boxes on an image.\n\n    Args:\n        bgr_nparr: image data in numpy array format\n        infres: Darkflow inference results\n        class_colors: Bounding box color candidates, list of RGB tuples.\n\n    Returens:\n        Generalized result whose image data is drew w/ bounding boxes.\n    """"""\n    for res in infres[\'annotations\']:\n        left = int(res[\'left\'])\n        top = int(res[\'top\'])\n        right = int(res[\'right\'])\n        bottom = int(res[\'bottom\'])\n        label = res[\'label\']\n        color = class_colors[labels.index(label)]\n        confidence = res[\'confidence\']\n        imgHeight, imgWidth, _ = bgr_nparr.shape\n        thick = int((imgHeight + imgWidth) // 300)\n\n        cv2.rectangle(bgr_nparr,(left, top), (right, bottom), color, thick)\n        cv2.putText(bgr_nparr, label, (left, top - 12), 0, 1e-3 * imgHeight,\n            color, thick//3)\n    #cv2.imwrite(\'prediction.jpg\', bgr_nparr)\n    infres[\'bytes\'] = payload.stringify_jpg(\n                                    cv2.imencode(\'.jpg\', bgr_nparr)[1])\n    return infres\n\n\ndef draw_box(image, annotations):\n    """"""Draw information of annotations onto image.\n\n    Args:\n        image: Image nparray.\n        annotations: List of detected object information.\n\n    Returns: Image nparray containing object information on it.\n    """"""\n    print(\'draw_box, annotations: {}\'.format(annotations))\n    img = image.copy()\n\n    for anno in annotations:\n        # draw bounding box\n        box_color = (0, 0, 255)\n        box_thickness = 1\n        cv2.rectangle(img,\n                      (anno[\'left\'], anno[\'top\']),\n                      (anno[\'right\'], anno[\'bottom\']),\n                      box_color,\n                      box_thickness)\n\n        # draw label\n        label_background_color = box_color\n        label_text_color = (255, 255, 255)\n        if \'track_id\' in anno.keys():\n            label = \'ID:{} {}\'.format(anno[\'track_id\'], anno[\'label\'])\n        else:\n            label = anno[\'label\']\n        label_text = \'{} ({} %)\'.format(label,\n                                        int(anno[\'confidence\'] * 100))\n        label_size = cv2.getTextSize(label_text,\n                                     cv2.FONT_HERSHEY_SIMPLEX,\n                                     0.5,\n                                     1)[0]\n        label_left = anno[\'left\']\n        label_top = anno[\'top\'] - label_size[1]\n        if (label_top < 1):\n            label_top = 1\n        label_right = label_left + label_size[0]\n        label_bottom = label_top + label_size[1]\n        cv2.rectangle(img,\n                      (label_left - 1, label_top - 1),\n                      (label_right + 1, label_bottom + 1),\n                      label_background_color,\n                      -1)\n        cv2.putText(img,\n                    label_text,\n                    (label_left, label_bottom),\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    0.5,\n                    label_text_color,\n                    1)\n    return img\n\n\ndef overlay_on_image(display_image, object_info):\n    """"""Modulized version of overlay_on_image function\n    """"""\n    if isinstance(object_info, type(None)):\n        print(\'WARNING: object info is None\')\n        return display_image\n\n    return draw_box(display_image, object_info)\n\n\ndef draw_label(bgr_nparr, infres, class_color, save_image_path=None):\n    """"""Draw bounding boxes on an image.\n\n    Args:\n        bgr_nparr: image data in numpy array format\n        infres: Inference results followed generic format specification.\n        class_color: Label color, a RGB tuple.\n\n    Returens:\n        Generalized result whose image data is drew w/ labels.\n    """"""\n    left = 0\n    top = 0\n    for res in infres[\'annotations\']:\n        imgHeight, imgWidth, _ = bgr_nparr.shape\n        thick = int((imgHeight + imgWidth) // 300)\n\n        # putText can not handle newline char yet,\n        # so we have to put multiple texts manually.\n        cv2.putText(bgr_nparr,\n                    \'{0}: {1}\'.format(res[\'label\'], res[\'confidence\']),\n                    (left + 10, top + 20),  # bottom-left corner of text\n                    0,                      # fontFace\n                    1e-3 * imgHeight,       # fontScale\n                    class_color,\n                    thick // 3)\n        top += 20\n    infres[\'bytes\'] = payload.stringify_jpg(\n                                    cv2.imencode(\'.jpg\', bgr_nparr)[1])\n\n    if save_image_path:\n        cv2.imwrite(save_image_path, bgr_nparr)\n\n    return infres\n'"
inference/classify_caffe2_server.py,0,"b'# Copyright 2017 DT42\n# \n# This file is part of BerryNet.\n# \n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n# \n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n# \n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Simple image classification server with Inception.\n\nThe server monitors image_dir and run inferences on new images added to the\ndirectory. Every image file should come with another empty file with \'.done\'\nsuffix to signal readiness. Inference result of a image can be read from the\n\'.txt\' file of that image after \'.txt.done\' is spotted.\n\nThis is an example the server expects clients to do. Note the order.\n\n# cp cat.jpg /run/image_dir\n# touch /run/image_dir/cat.jpg.done\n\nClients should wait for appearance of \'cat.jpg.txt.done\' before getting\nresult from \'cat.jpg.txt\'.\n""""""\n\n\nfrom __future__ import print_function\nimport os\nimport sys\nimport time\nfrom caffe2.proto import caffe2_pb2\nimport numpy as np\nimport skimage.io\nimport skimage.transform\nimport threading\nimport multiprocessing\nimport Queue\nimport signal\nfrom watchdog.observers import Observer\nfrom watchdog.events import PatternMatchingEventHandler\nfrom caffe2.python import core, workspace\nimport urllib2\n\nimage_dir = \'/run/image_dir\'\nimage_queue = Queue.Queue()\nsess = None\nthreads = []\n\ndef logging(*args):\n    print(""[%08.3f]"" % time.time(), \' \'.join(args))\n\ndef touch(fname, times=None):\n    with open(fname, \'a\'):\n        os.utime(fname, times)\n\ndef crop_center(img,cropx,cropy):\n    y,x,c = img.shape\n    startx = x//2-(cropx//2)\n    starty = y//2-(cropy//2)\n    return img[starty:starty+cropy,startx:startx+cropx]\n\ndef rescale(img, input_height, input_width):\n    aspect = img.shape[1]/float(img.shape[0])\n    if(aspect>1):\n        # landscape orientation - wide image\n        res = int(aspect * input_height)\n        imgScaled = skimage.transform.resize(img, (input_width, res))\n    if(aspect<1):\n        # portrait orientation - tall image\n        res = int(input_width/aspect)\n        imgScaled = skimage.transform.resize(img, (res, input_height))\n    if(aspect == 1):\n        imgScaled = skimage.transform.resize(img, (input_width, input_height))\n    return imgScaled\n\ndef server(labels):\n    """"""Infinite loop serving inference requests""""""\n\n    global image_queue, sess\n    CAFFE2_ROOT = ""/caffe2""\n    CAFFE_MODELS = ""/caffe2/caffe2/python/models""\n    MODEL = \'squeezenet\', \'exec_net.pb\', \'predict_net.pb\', \'ilsvrc_2012_mean.npy\', 227\n    codes =  ""https://gist.githubusercontent.com/aaronmarkham/cd3a6b6ac071eca6f7b4a6e40e6038aa/raw/9edb4038a37da6b5a44c3b5bc52e448ff09bfe5b/alexnet_codes""\n\n    logging(threading.current_thread().getName(), ""is running"")\n    CAFFE2_ROOT = os.path.expanduser(CAFFE2_ROOT)\n    CAFFE_MODELS = os.path.expanduser(CAFFE_MODELS)\n    MEAN_FILE = os.path.join(CAFFE_MODELS, MODEL[0], MODEL[3])\n    if not os.path.exists(MEAN_FILE):\n        mean = 128\n    else:\n        mean = np.load(MEAN_FILE).mean(1).mean(1)\n        mean = mean[:, np.newaxis, np.newaxis]\n    INPUT_IMAGE_SIZE = MODEL[4]\n    INIT_NET = os.path.join(CAFFE_MODELS, MODEL[0], MODEL[1])\n    PREDICT_NET = os.path.join(CAFFE_MODELS, MODEL[0], MODEL[2])\n\n    while True:\n        input_name = image_queue.get()\n        img = skimage.img_as_float(skimage.io.imread(input_name)).astype(np.float32)\n        img = rescale(img, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE)\n        img = crop_center(img, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE)\n            \n        img = img.swapaxes(1, 2).swapaxes(0, 1)\n        img = img[(2, 1, 0), :, :]\n        img = img * 255 - mean\n        img = img[np.newaxis, :, :, :].astype(np.float32)\n\n        with open(INIT_NET, \'rb\') as f:\n            init_net = f.read()\n        with open(PREDICT_NET, \'rb\') as f:\n            predict_net = f.read()\n            \n        p = workspace.Predictor(init_net, predict_net)\n            \n        # run the net and return prediction\n        results = p.run([img])\n        results = np.asarray(results)\n        results = np.delete(results, 1)\n        index = 0\n        highest = 0\n        arr = np.empty((0,2), dtype=object)\n        arr[:,0] = int(10)\n        arr[:,1:] = float(10)\n        for i, r in enumerate(results):\n            # imagenet index begins with 1!\n            i=i+1\n            arr = np.append(arr, np.array([[i,r]]), axis=0)\n            if (r > highest):\n                highest = r\n                index = i\n        response = urllib2.urlopen(codes)\n        output_name = input_name+\'.txt\'\n        output_done_name = output_name+\'.done\'\n        output = open(output_name, \'w\')\n        for line in response:\n            code, result = line.partition("":"")[::2]\n            if (code.strip() == str(index)):\n                human_string = result.strip()[1:-2]\n                score = highest\n                print(""%s (score = %.5f)"" % (human_string, score), file=output)\n        output.close()\n        touch(output_done_name)\n        logging(input_name, "" classified!"")\n\n\nclass EventHandler(PatternMatchingEventHandler):\n    def process(self, event):\n        """"""\n        event.event_type\n            \'modified\' | \'created\' | \'moved\' | \'deleted\'\n        event.is_directory\n            True | False\n        event.src_path\n            path/to/observed/file\n        """"""\n        # the file will be processed there\n        global image_queue\n\n        _msg = event.src_path\n        image_queue.put(_msg.rstrip(\'.done\'))\n        os.remove(_msg)\n        logging(_msg, event.event_type)\n\n    # ignore all other types of events except \'modified\'\n    def on_created(self, event):\n        self.process(event)\n\n\ndef main(_):\n    """"""Called by Tensorflow""""""\n \n\n    # Create a server thread for each CPU core\n    cpu_count = multiprocessing.cpu_count()\n    for i in xrange(cpu_count/4):\n        threads.append(threading.Thread(target=server,\n                                        name=\'Server thread %d\' % i,\n                                        args=({},)))\n    for t in threads: t.start()\n    for t in threads: t.join()\n \n\nif __name__ == \'__main__\':\n    global sess, threads\n\n    pid = str(os.getpid())\n    pidfile = ""/tmp/classify_server.pid""\n\n    if os.path.isfile(pidfile):\n        logging(""%s already exists, exiting"" % pidfile)\n        sys.exit(1)\n\n    with open(pidfile, \'w\') as f:\n        f.write(pid)\n\n    # workaround the issue that SIGINT cannot be received (fork a child to \n    # avoid blocking the main process in Thread.join()\n    child_pid = os.fork()\n    if child_pid == 0:\n        # child\n        # observer handles event in a different thread\n        observer = Observer()\n        observer.schedule(EventHandler([\'*.jpg.done\']), path=image_dir)\n        observer.start()\n        # Create a server thread for each CPU core\n        cpu_count = multiprocessing.cpu_count()\n        for i in xrange(cpu_count/4):\n            threads.append(threading.Thread(target=server,\n                                            name=\'Server thread %d\' % i,\n                                            args=({},)))\n        for t in threads: t.start()\n        for t in threads: t.join()\n    else:\n        # parent\n        try:\n            os.wait()\n        except KeyboardInterrupt:\n            os.kill(child_pid, signal.SIGKILL)\n            os.unlink(pidfile)\n'"
inference/classify_caffe_server.py,0,"b'#!/usr/bin/env python3\n#\n# Copyright 2018 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Simple image classification server with Inception.\n\nThe server monitors image_dir and run inferences on new images added to the\ndirectory. Every image file should come with another empty file with \'.done\'\nsuffix to signal readiness. Inference result of a image can be read from the\n\'.txt\' file of that image after \'.txt.done\' is spotted.\n\nThis is an example the server expects clients to do. Note the order.\n\n# cp cat.jpg /run/image_dir\n# touch /run/image_dir/cat.jpg.done\n\nClients should wait for appearance of \'cat.jpg.txt.done\' before getting\nresult from \'cat.jpg.txt\'.\n""""""\n\n\nfrom __future__ import print_function\nimport os\nimport sys\nimport time\nimport numpy as np\nimport threading\nimport multiprocessing\nimport queue\nimport signal\nfrom watchdog.observers import Observer\nfrom watchdog.events import PatternMatchingEventHandler\nimport caffe\nimport hashlib\nimport urllib.request\nimport tempfile\nimport shutil\n\nimage_queue = queue.Queue()\nsess = None\nthreads = []\nimage_dir = \'/run/image_dir\'\ncaffe_classifier = None\ncaffe_labels = []\nmodel_meta_file = \'/usr/share/doc/caffe-doc/models/bvlc_reference_caffenet/readme.md\'\nlabel_file = \'/tmp/synset_words.txt\'\npretrained_model = None\n\ndef logging(*args):\n    print(""[%08.3f]"" % time.time(), \' \'.join(args))\n\ndef touch(fname, times=None):\n    with open(fname, \'a\'):\n        os.utime(fname, times)\n\ndef load_labels(filename):\n    """"""Read in labels, one label per line.""""""\n    return [line.rstrip() for line in open(filename)]\n\ndef read_model_meta_file(meta_file):\n    """"""Read model meta file. The meta file is inside caffe-doc package""""""\n    # We believe we shouldn\'t read this file for downloading and checking\n    # model. Instead we should package some model if there is free one.\n    url = None\n    sha1sum = None\n    filename = None\n    for line in open(meta_file):\n        l1 = line.rstrip()\n        if (l1.startswith(""sha1:"")):\n            sha1sum = l1[len(""sha1:""):].strip()\n        if (l1.startswith(""caffemodel_url:"")):\n            url = l1[len(""caffemodel_url:""):].strip()\n        if (l1.startswith(""caffemodel:"")):\n            filename = l1[len(""caffemodel:""):].strip()\n        if ((sha1sum != None) and (url != None) and (filename != None)):\n            break\n    if ((url != None) and (sha1sum != None) and (filename != None)):\n        return {\'url\': url, \'sha1sum\': sha1sum, \'filename\': filename}\n    return None\n\ndef sha1sum(filename):\n    """"""calculate sha1sum""""""\n    BUF_SIZE=1024\n    sha1 = hashlib.sha1()\n    with open(filename, \'rb\') as f:\n        while True:\n            data = f.read(BUF_SIZE)\n            if not data:\n                break\n            sha1.update(data)\n    return sha1.hexdigest()\n\ndef download_model():\n    """"""Download pretrained model""""""\n    # Downloading model from network isn\'t good for Debian. We need to package\n    # the model.\n    global pretrained_model\n    meta_data = read_model_meta_file(model_meta_file)\n    if (meta_data is None):\n        logging(\'Cannot load %s\'%(meta_data))\n        return None\n    # FIXME: using /tmp/ will be in-secure.\n    pretrained_model = os.path.join(\'/\',\'tmp\',meta_data[\'filename\'])\n    if (os.path.isfile(pretrained_model)):\n        sha1 = sha1sum(pretrained_model)\n        if (sha1 != meta_data[\'sha1sum\']):\n            logging(\'Model %s SHA1 is not equal to %s\'%(pretrained_model, meta_data[\'sha1sum\']))\n            pretrained_model = None\n            return None\n        else:\n            logging(\'Model already exists\')\n            pass\n    else:\n        logging(\'Downloading model file from %s\'%(meta_data[\'url\']))\n        urllib.request.urlretrieve(meta_data[\'url\'], pretrained_model)\n        logging(\'Checking SHA1...\')\n        sha1 = sha1sum(pretrained_model)\n        if (sha1 != meta_data[\'sha1sum\']):\n            logging(\'Model %s SHA1 is not equal to %s\'%(pretrained_model, meta_data[\'sha1sum\']))\n            pretrained_model = None\n            return None\n        else:\n            logging(\'Model downloaded\')\n            pass\n    return None\n\ndef download_label():\n    """"""Download label file""""""\n    # Using the scripts inside caffe Debian package to download label file.\n    # This could also be wrong. Why we don\'t package the label file?\n    global label_file\n    if (os.path.isfile(label_file)):\n        logging(""Label file exists"");\n        pass\n    else:\n        logging(""Label file not exists. Downloading..."");\n        tmpdir = tempfile.mkdtemp()\n        s1 = shutil.copy2(os.path.join(\'/\', \'usr\', \'share\', \'doc\', \'caffe-doc\',\n                                       \'data\', \'ilsvrc12\',\n                                       \'get_ilsvrc_aux.sh\'),\n                          tmpdir)\n        os.system(\'sh \\\'%s\\\'\'%(s1));\n        # FIXME: using /tmp/ will be in-secure.\n        shutil.copy2(os.path.join(tmpdir, \'synset_words.txt\'), \'/tmp\')\n\ndef create_classifier(pretrained_model):\n    """"""Creates a model from saved caffemodel file and returns a classifier.""""""\n    # Creates model from saved .caffemodel.\n\n    # The following file are shipped inside caffe-doc Debian package\n    model_def = os.path.join(""/"", ""usr"", ""share"", ""doc"", ""caffe-doc"",\n                             ""models"",""bvlc_reference_caffenet"",\n                             ""deploy.prototxt"")\n    image_dims = [ 256, 256 ]\n    # The following file are shipped inside python3-caffe-cpu Debian package\n    mean = np.load(os.path.join(\'/\', \'usr\', \'lib\', \'python3\',\n                                \'dist-packages\', \'caffe\', \'imagenet\',\n                                \'ilsvrc_2012_mean.npy\'))\n    channel_swap = [2, 1, 0]\n    raw_scale = 255.0\n\n    caffe.set_mode_cpu()\n    classifier = caffe.Classifier(model_def, pretrained_model,\n                                  image_dims=image_dims, mean=mean,\n                                  raw_scale=raw_scale,\n                                  channel_swap=channel_swap)\n    return classifier\n\ndef server(labels):\n    """"""Infinite loop serving inference requests""""""\n    global image_queue, sess\n\n    logging(threading.current_thread().getName(), ""is running"")\n\n    while True:\n        input_name = image_queue.get()\n        if (input_name.endswith(\'npy\')):\n            inputs = np.load(input_name)\n        else:\n            inputs = [caffe.io.load_image(input_name)]\n\n        predictions = caffe_classifier.predict(inputs, False)\n        # make tuples\n        predictions_list = predictions[0].tolist()\n        data = zip(predictions_list, caffe_labels)\n        output_name = input_name+\'.txt\'\n        output_done_name = output_name+\'.done\'\n        output = open(output_name, \'wt\')\n        for d in sorted(data, reverse=True):\n            human_string = d[1]\n            score = d[0]\n            print(""%s (score = %.5f)"" % (human_string, score), file=output)\n            if (score < 0.00001):\n                break\n        output.close()\n        touch(output_done_name)\n        logging(input_name, "" classified!"")\n\nclass EventHandler(PatternMatchingEventHandler):\n    def process(self, event):\n        """"""\n        event.event_type\n            \'modified\' | \'created\' | \'moved\' | \'deleted\'\n        event.is_directory\n            True | False\n        event.src_path\n            path/to/observed/file\n        """"""\n        # the file will be processed there\n        global image_queue\n\n        _msg = event.src_path\n        image_queue.put(_msg.rstrip(\'.done\'))\n        os.remove(_msg)\n        logging(_msg, event.event_type)\n\n    # ignore all other types of events except \'modified\'\n    def on_created(self, event):\n        self.process(event)\n\nif __name__ == \'__main__\':\n\n    pid = str(os.getpid())\n    pidfile = ""/tmp/classify_server.pid""\n\n    if os.path.isfile(pidfile):\n        logging(""%s already exists, exiting"" % pidfile)\n        sys.exit(1)\n\n    with open(pidfile, \'w\') as f:\n        f.write(pid)\n\n    # Please read /usr/share/doc/caffe-doc/models/bvlc_reference_caffenet/readme.md\n    download_model()\n    download_label()\n    caffe_labels = load_labels(label_file)\n    caffe_classifier = create_classifier(pretrained_model)\n\n    # workaround the issue that SIGINT cannot be received (fork a child to\n    # avoid blocking the main process in Thread.join()\n    child_pid = os.fork()\n    if child_pid == 0:\n        # child\n        # observer handles event in a different thread\n        observer = Observer()\n        observer.schedule(EventHandler([\'*.jpg.done\']), path=image_dir)\n        observer.start()\n\n        # Create a server thread for each CPU core\n        cpu_count = multiprocessing.cpu_count()\n        for i in range(1):\n            threads.append(threading.Thread(target=server,\n                                            name=\'Server thread %d\' % i,\n                                            args=({},)))\n        for t in threads: t.start()\n        for t in threads: t.join()\n    else:\n        # parent\n        try:\n            os.wait()\n        except KeyboardInterrupt:\n            os.kill(child_pid, signal.SIGKILL)\n            os.unlink(pidfile)\n'"
inference/classify_movidius_server.py,0,"b'# Copyright 2017 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Simple image classification server with Inception.\n\nThe server monitors image_dir and run inferences on new images added to the\ndirectory. Every image file should come with another empty file with \'.done\'\nsuffix to signal readiness. Inference result of a image can be read from the\n\'.txt\' file of that image after \'.txt.done\' is spotted.\n\nThis is an example the server expects clients to do. Note the order.\n\n# cp cat.jpg /run/image_dir\n# touch /run/image_dir/cat.jpg.done\n\nClients should wait for appearance of \'cat.jpg.txt.done\' before getting\nresult from \'cat.jpg.txt\'.\n""""""\n\n\nfrom __future__ import print_function\n\nimport argparse\nimport json\nimport multiprocessing\nimport os\nimport signal\nimport sys\nimport threading\nimport time\n\nimport cv2\nimport movidius as mv\nimport numpy as np\nimport Queue\n\nfrom watchdog.observers import Observer\nfrom watchdog.events import PatternMatchingEventHandler\n\n\nimage_queue = Queue.Queue()\nthreads = []\nmvng = None\ngraph_filepath = \'\'\nlabels_filepath = \'\'\n\n\ndef logging(*args):\n    print(""[%08.3f]"" % time.time(), \' \'.join(args))\n\n\ndef touch(fname, times=None):\n    with open(fname, \'a\'):\n        os.utime(fname, times)\n\n\ndef generalize_inference_result(ncs_classifications):\n    """"""Result format conversion from NCS classification to generic format.\n\n    param list ncs_classifications: [(label, confidence), ...]\n    return dict r: generic inference result format\n    """"""\n    r = {\n        \'annotations\': []\n    }\n    conf_digits = 2\n    for label, confidence in ncs_classifications:\n        r[\'annotations\'].append({\n            \'type\': \'classification\',\n            \'label\': label,\n            \'confidence\': round(float(confidence), conf_digits)\n        })\n    return r\n\n\ndef server():\n    """"""Infinite loop serving inference requests""""""\n\n    global image_queue\n    global mvng\n\n    logging(threading.current_thread().getName(), ""is running"")\n\n    while True:\n        input_name = image_queue.get()\n        image_data = cv2.imread(input_name).astype(np.float32)\n        image_data = mv.process_inceptionv3_input(image_data)\n\n        output = mvng.inference(image_data)\n        inceptionv3_outputs = mv.process_inceptionv3_output(output,\n                                                            mvng.get_labels())\n\n        output_name = input_name + \'.txt\'\n        output_done_name = output_name + \'.done\'\n        inference_result = generalize_inference_result(inceptionv3_outputs)\n        logging(json.dumps(inference_result, indent=4))\n        with open(output_name, \'w\') as f:\n            json.dump(inference_result, f, indent=4)\n        #with open(output_name, \'w\') as f:\n        #    for i in inceptionv3_outputs:\n        #        print(""%s (score = %.5f)"" % (i[0], i[1]), file=f)\n        touch(output_done_name)\n        logging(input_name, "" classified!"")\n\n\nclass EventHandler(PatternMatchingEventHandler):\n    def process(self, event):\n        """"""\n        event.event_type\n            \'modified\' | \'created\' | \'moved\' | \'deleted\'\n        event.is_directory\n            True | False\n        event.src_path\n            path/to/observed/file\n        """"""\n        # the file will be processed there\n        global image_queue\n\n        _msg = event.src_path\n        image_queue.put(_msg.rstrip(\'.done\'))\n        os.remove(_msg)\n        logging(_msg, event.event_type)\n\n    # ignore all other types of events except \'modified\'\n    def on_created(self, event):\n        self.process(event)\n\n\ndef main(args):\n    global threads\n\n    # Create a server thread for each CPU core\n    cpu_count = multiprocessing.cpu_count()\n    for i in xrange(cpu_count/4):\n        threads.append(\n            threading.Thread(target=server,\n                             name=\'Server thread %d\' % i))\n    for t in threads:\n        t.start()\n    for t in threads:\n        t.join()\n\n\ndef parse_args():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\'--model\', required=True, help=\'Model file path\')\n    ap.add_argument(\'--label\', required=True, help=\'Label file path\')\n    ap.add_argument(\'--image_dir\', required=True, help=\'Path to image file\')\n    ap.add_argument(\'--num_top_predictions\', default=5,\n                    help=\'Display this many predictions\')\n    return vars(ap.parse_args())\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n\n    mvng = mv.MovidiusNeuralGraph(args[\'model\'], args[\'label\'])\n\n    pid = str(os.getpid())\n    pidfile = ""/tmp/classify_movidius_server.pid""\n\n    if os.path.isfile(pidfile):\n        logging(""%s already exists, exiting"" % pidfile)\n        sys.exit(1)\n\n    with open(pidfile, \'w\') as f:\n        f.write(pid)\n\n    logging(""model filepath: "", args[\'model\'])\n    logging(""label filepath: "", args[\'label\'])\n    logging(""image_dir: "", args[\'image_dir\'])\n\n    # workaround the issue that SIGINT cannot be received (fork a child to\n    # avoid blocking the main process in Thread.join()\n    child_pid = os.fork()\n    if child_pid == 0:  # child\n        # observer handles event in a different thread\n        observer = Observer()\n        observer.schedule(EventHandler([\'*.jpg.done\']), path=args[\'image_dir\'])\n        observer.start()\n        main(args)\n    else:  # parent\n        try:\n            os.wait()\n        except KeyboardInterrupt:\n            os.kill(child_pid, signal.SIGKILL)\n            os.unlink(pidfile)\n'"
inference/classify_server.py,13,"b'# Copyright 2017 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Simple image classification server with Inception.\n\nThe server monitors image_dir and run inferences on new images added to the\ndirectory. Every image file should come with another empty file with \'.done\'\nsuffix to signal readiness. Inference result of a image can be read from the\n\'.txt\' file of that image after \'.txt.done\' is spotted.\n\nThis is an example the server expects clients to do. Note the order.\n\n# cp cat.jpg /run/image_dir\n# touch /run/image_dir/cat.jpg.done\n\nClients should wait for appearance of \'cat.jpg.txt.done\' before getting\nresult from \'cat.jpg.txt\'.\n""""""\n\n\nfrom __future__ import print_function\nimport os\nimport sys\nimport time\nimport numpy as np\nimport threading\nimport multiprocessing\nimport tensorflow as tf\nimport Queue\nimport signal\nfrom watchdog.observers import Observer\nfrom watchdog.events import PatternMatchingEventHandler\n\nFLAGS = tf.app.flags.FLAGS\nimage_queue = Queue.Queue()\nsess = None\nthreads = []\n\n# classify_image_graph_def.pb:\n#   Binary representation of the GraphDef protocol buffer.\n# imagenet_synset_to_human_label_map.txt:\n#   Map from synset ID to a human readable string.\n# imagenet_2012_challenge_label_map_proto.pbtxt:\n#   Text representation of a protocol buffer mapping a label to synset ID.\ntf.app.flags.DEFINE_string(\n    \'model_dir\', \'model\',\n    """"""Path to output_graph.pb and output_labels.txt."""""")\ntf.app.flags.DEFINE_string(\'image_dir\', \'image\',\n                           """"""Path to image file."""""")\ntf.app.flags.DEFINE_string(\'output_layer\', \'softmax:0\',\n                           """"""Name of the result operation"""""")\ntf.app.flags.DEFINE_string(\'input_layer\', \'DecodeJpeg/contents:0\',\n                           """"""Name of the input operation"""""")\ntf.app.flags.DEFINE_integer(\'num_top_predictions\', 5,\n                            """"""Display this many predictions."""""")\n\n\ndef logging(*args):\n    print(""[%08.3f]"" % time.time(), \' \'.join(args))\n\ndef touch(fname, times=None):\n    with open(fname, \'a\'):\n        os.utime(fname, times)\n\ndef load_labels(filename):\n    """"""Read in labels, one label per line.""""""\n    return [line.rstrip() for line in tf.gfile.FastGFile(filename)]\n\n\ndef create_graph():\n    """"""Creates a graph from saved GraphDef file and returns a saver.""""""\n    # Creates graph from saved graph_def.pb.\n    with tf.gfile.FastGFile(os.path.join(\n        FLAGS.model_dir, \'output_graph.pb\'), \'rb\') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n        _ = tf.import_graph_def(graph_def, name=\'\')\n\n\ndef server(labels):\n    """"""Infinite loop serving inference requests""""""\n\n    global image_queue, sess\n\n    logging(threading.current_thread().getName(), ""is running"")\n\n    with sess.as_default():\n        # Some useful tensors:\n        # \'softmax:0\': A tensor containing the normalized prediction across\n        #   1000 labels.\n        # \'pool_3:0\': A tensor containing the next-to-last layer containing 2048\n        #   float description of the image.\n        # \'DecodeJpeg/contents:0\': A tensor containing a string providing JPEG\n        #   encoding of the image.\n\n        while True:\n            input_name = image_queue.get()\n            image_data = tf.gfile.FastGFile(input_name, \'rb\').read()\n\n            predictions = sess.run(FLAGS.output_layer,\n                                   {FLAGS.input_layer: image_data})\n            predictions = np.squeeze(predictions)\n            top_k = predictions.argsort()[-FLAGS.num_top_predictions:][::-1]\n\n            output_name = input_name+\'.txt\'\n            output_done_name = output_name+\'.done\'\n            output = open(output_name, \'w\')\n            for node_id in top_k:\n                human_string = labels[node_id]\n                score = predictions[node_id]\n                print(""%s (score = %.5f)"" % (human_string, score), file=output)\n            output.close()\n            touch(output_done_name)\n            logging(input_name, "" classified!"")\n\n\nclass EventHandler(PatternMatchingEventHandler):\n    def process(self, event):\n        """"""\n        event.event_type\n            \'modified\' | \'created\' | \'moved\' | \'deleted\'\n        event.is_directory\n            True | False\n        event.src_path\n            path/to/observed/file\n        """"""\n        # the file will be processed there\n        global image_queue\n\n        _msg = event.src_path\n        image_queue.put(_msg.rstrip(\'.done\'))\n        os.remove(_msg)\n        logging(_msg, event.event_type)\n\n    # ignore all other types of events except \'modified\'\n    def on_created(self, event):\n        self.process(event)\n\n\ndef main(_):\n    """"""Called by Tensorflow""""""\n\n    global sess, threads\n\n    # Creates graph from saved GraphDef.\n    create_graph()\n\n    # Reuse the same session for all threads processing requests\n    sess = tf.Session()\n    # Creates node ID --> English string lookup.\n    labels = load_labels(os.path.join(FLAGS.model_dir, \'output_labels.txt\'))\n\n    # Create a server thread for each CPU core\n    cpu_count = multiprocessing.cpu_count()\n    for i in xrange(cpu_count/4):\n        threads.append(threading.Thread(target=server,\n                                        name=\'Server thread %d\' % i,\n                                        args=(labels,)))\n    for t in threads: t.start()\n    for t in threads: t.join()\n\n\nif __name__ == \'__main__\':\n    pid = str(os.getpid())\n    pidfile = ""/tmp/classify_server.pid""\n\n    if os.path.isfile(pidfile):\n        logging(""%s already exists, exiting"" % pidfile)\n        sys.exit(1)\n\n    with open(pidfile, \'w\') as f:\n        f.write(pid)\n\n    logging(""model_dir: "", FLAGS.model_dir)\n    logging(""image_dir: "", FLAGS.image_dir)\n\n    # workaround the issue that SIGINT cannot be received (fork a child to\n    # avoid blocking the main process in Thread.join()\n    child_pid = os.fork()\n    if child_pid == 0:\n        # child\n        # observer handles event in a different thread\n        observer = Observer()\n        observer.schedule(EventHandler([\'*.jpg.done\']), path=FLAGS.image_dir)\n        observer.start()\n        tf.app.run()\n    else:\n        # parent\n        try:\n            os.wait()\n        except KeyboardInterrupt:\n            os.kill(child_pid, signal.SIGKILL)\n            os.unlink(pidfile)\n'"
inference/darkflow_engine.py,0,"b'# Copyright 2017 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Darkflow inference engine.\n""""""\n\nfrom __future__ import print_function\n\nimport argparse\nimport logging\n\nimport cv2\n\nfrom darkflow.net.build import TFNet\nfrom engineservice import EngineService\nfrom engineservice import DLEngine\nfrom dlmodelmgr import DLModelManager\n\n\n# FIXME: Make these variables configurable\nSystemSnapshot = \'/usr/local/berrynet/dashboard/www/freeboard/snapshot.jpg\'\n\n\nclass DarkflowEngine(DLEngine):\n    def __init__(self, model, label, config):\n        super(DarkflowEngine, self).__init__()\n        self.engine_options = {\n            \'model\': config,\n            \'load\': model,\n            #\'model\': ""cfg/tiny-yolo.cfg"",\n            #\'load\': ""bin/tiny-yolo.weights"",\n            \'verbalise\': True,\n            ""threshold"": 0.1\n        }\n\n    def create(self):\n        self.tfnet = TFNet(self.engine_options)\n\n    def inference(self, tensor):\n        return self.tfnet.return_predict(tensor)\n\n    def save_cache(self):\n        #with open(self.cache[\'model_output_filepath\'], \'w\') as f:\n        #    f.write(str(self.cache[\'model_output\']))\n        drawBoundingBoxes(self.cache[\'model_input\'],\n                          #self.cache[\'model_output_filepath\'] + \'.jpg\',\n                          SystemSnapshot,\n                          self.cache[\'model_output\'],\n                          self.tfnet.meta[\'colors\'])\n\n\ndef drawBoundingBoxes(imageData, imageOutputPath, inferenceResults, colorMap):\n    """"""Draw bounding boxes on an image.\n\n    imageData: image data in numpy array format\n    imageOutputPath: output image file path\n    inferenceResults: Darkflow inference results\n    colorMap: Bounding box color candidates, list of RGB tuples.\n    """"""\n    # TODO: return raw data instead of save image\n    for res in inferenceResults:\n        left = res[\'topleft\'][\'x\']\n        top = res[\'topleft\'][\'y\']\n        right = res[\'bottomright\'][\'x\']\n        bottom = res[\'bottomright\'][\'y\']\n        colorIndex = res[\'coloridx\']\n        color = colorMap[colorIndex]\n        label = res[\'label\']\n        confidence = res[\'confidence\']\n        imgHeight, imgWidth, _ = imageData.shape\n        thick = int((imgHeight + imgWidth) // 300)\n\n        cv2.rectangle(imageData,(left, top), (right, bottom), color, thick)\n        cv2.putText(imageData, label, (left, top - 12), 0, 1e-3 * imgHeight,\n            color, thick//3)\n    cv2.imwrite(imageOutputPath, imageData)\n    logging.debug(\'Save bounding box result image to {}\'.format(imageOutputPath))\n\n\ndef parse_args():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\'--model\',\n                    help=\'Model file path\')\n    ap.add_argument(\'--label\',\n                    help=\'Label file path\')\n    ap.add_argument(\'--model_package\',\n                    default=\'\',\n                    help=\'Model package ""name-version"" naming\')\n    ap.add_argument(\'--image_dir\', required=True,\n                    help=\'Path to image file\')\n    ap.add_argument(\'--service_name\', required=True,\n                    help=\'Engine service name used as PID filename\')\n    ap.add_argument(\'--num_top_predictions\', default=5,\n                    help=\'Display this many predictions\')\n    return vars(ap.parse_args())\n\n\nif __name__ == \'__main__\':\n    logging.basicConfig(level=logging.DEBUG)\n    args = parse_args()\n    if args[\'model_package\'] != \'\':\n        dlmm = DLModelManager()\n        meta = dlmm.get_model_meta(args[\'model_package\'])\n        args[\'model\'] = meta[\'model\']\n        args[\'label\'] = meta[\'label\']\n        args[\'config\'] =  meta[\'config\'][\'graph\']\n    logging.debug(\'model filepath: \' + args[\'model\'])\n    logging.debug(\'label filepath: \' + args[\'label\'])\n    logging.debug(\'image_dir: \' + args[\'image_dir\'])\n\n    darkflow_engine = DarkflowEngine(args[\'model\'], args[\'label\'], args[\'config\'])\n    engine_service = EngineService(args[\'service_name\'], darkflow_engine)\n    engine_service.run(args)\n\n    # this code block works\n    #import cv2\n    #input_tensor = cv2.imread(\'/tmp/berrynet/dog.jpg\')\n    #tensor = darkflow_engine.process_input(input_tensor)\n    #output = darkflow_engine.inference(tensor)\n    #output = darkflow_engine.process_output(output)\n'"
inference/detect_movidius_server.py,0,"b'# Copyright 2017 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Simple image classification server with Inception.\n\nThe server monitors image_dir and run inferences on new images added to the\ndirectory. Every image file should come with another empty file with \'.done\'\nsuffix to signal readiness. Inference result of a image can be read from the\n\'.txt\' file of that image after \'.txt.done\' is spotted.\n\nThis is an example the server expects clients to do. Note the order.\n\n# cp cat.jpg /run/image_dir\n# touch /run/image_dir/cat.jpg.done\n\nClients should wait for appearance of \'cat.jpg.txt.done\' before getting\nresult from \'cat.jpg.txt\'.\n""""""\n\n\nfrom __future__ import print_function\n\nimport argparse\nimport multiprocessing\nimport os\nimport signal\nimport sys\nimport threading\nimport time\n\nfrom datetime import datetime\n\nimport cv2\nimport movidius as mv\nimport numpy as np\nimport Queue\n\nfrom watchdog.observers import Observer\nfrom watchdog.events import PatternMatchingEventHandler\n\n\nimage_queue = Queue.Queue()\nthreads = []\nmvng = None\nSystemSnapshot = \'/usr/local/berrynet/dashboard/www/freeboard/snapshot.jpg\'\n\n\ndef logging(*args):\n    print(""[%08.3f]"" % time.time(), \' \'.join(args))\n\n\ndef touch(fname, times=None):\n    with open(fname, \'a\'):\n        os.utime(fname, times)\n\n\ndef server():\n    """"""Infinite loop serving inference requests""""""\n\n    global image_queue\n    global mvng\n\n    logging(threading.current_thread().getName(), ""is running"")\n\n    while True:\n        input_name = image_queue.get()\n        image_data = cv2.imread(input_name)\n        processed_data = mv.process_yolo_input(image_data)\n\n        t_start = datetime.now()\n        output = mvng.inference(processed_data)\n        t_end = datetime.now()\n        t_inference = t_end - t_start\n        logging(\'inference time: {} ms\'.format(t_inference.total_seconds() * 1000))\n        interpreted_output = mv.interpret_yolo_output(output,\n                                                      image_data.shape[1],\n                                                      image_data.shape[0])\n        yolo_outputs = mv.process_yolo_output(image_data,\n                                              interpreted_output,\n                                              SystemSnapshot)\n\n        output_name = input_name + \'.txt\'\n        output_done_name = output_name + \'.done\'\n        mv.save_yolo_output_text(output_name, yolo_outputs)\n        touch(output_done_name)\n        logging(input_name, "" detected!"")\n\n\nclass EventHandler(PatternMatchingEventHandler):\n    def process(self, event):\n        """"""\n        event.event_type\n            \'modified\' | \'created\' | \'moved\' | \'deleted\'\n        event.is_directory\n            True | False\n        event.src_path\n            path/to/observed/file\n        """"""\n        # the file will be processed there\n        global image_queue\n\n        _msg = event.src_path\n        image_queue.put(_msg.rstrip(\'.done\'))\n        os.remove(_msg)\n        logging(_msg, event.event_type)\n\n    # ignore all other types of events except \'modified\'\n    def on_created(self, event):\n        self.process(event)\n\n\ndef main(args):\n    global threads\n\n    # Create a server thread for each CPU core\n    cpu_count = multiprocessing.cpu_count()\n    for i in xrange(cpu_count/4):\n        threads.append(\n            threading.Thread(target=server,\n                             name=\'Server thread %d\' % i))\n    for t in threads:\n        t.start()\n    for t in threads:\n        t.join()\n\n\ndef parse_args():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\'--model\', required=True, help=\'Model file path\')\n    ap.add_argument(\'--label\', required=True, help=\'Label file path\')\n    ap.add_argument(\'--image_dir\', required=True, help=\'Path to image file\')\n    ap.add_argument(\'--num_top_predictions\', default=5,\n                    help=\'Display this many predictions\')\n    return vars(ap.parse_args())\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n\n    mvng = mv.MovidiusNeuralGraph(args[\'model\'], args[\'label\'])\n\n    pid = str(os.getpid())\n    pidfile = ""/tmp/detect_movidius_server.pid""\n\n    if os.path.isfile(pidfile):\n        logging(""%s already exists, exiting"" % pidfile)\n        sys.exit(1)\n\n    with open(pidfile, \'w\') as f:\n        f.write(pid)\n\n    logging(""model filepath: "", args[\'model\'])\n    logging(""label filepath: "", args[\'label\'])\n    logging(""image_dir: "", args[\'image_dir\'])\n\n    # workaround the issue that SIGINT cannot be received (fork a child to\n    # avoid blocking the main process in Thread.join()\n    child_pid = os.fork()\n    if child_pid == 0:  # child\n        # observer handles event in a different thread\n        observer = Observer()\n        observer.schedule(EventHandler([\'*.jpg.done\']), path=args[\'image_dir\'])\n        observer.start()\n        main(args)\n    else:  # parent\n        try:\n            os.wait()\n        except KeyboardInterrupt:\n            os.kill(child_pid, signal.SIGKILL)\n            os.unlink(pidfile)\n'"
inference/detect_movidius_server_cv.py,0,"b'# Copyright 2017 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Simple image classification server with Inception.\n\nThe server monitors image_dir and run inferences on new images added to the\ndirectory. Every image file should come with another empty file with \'.done\'\nsuffix to signal readiness. Inference result of a image can be read from the\n\'.txt\' file of that image after \'.txt.done\' is spotted.\n\nThis is an example the server expects clients to do. Note the order.\n\n# cp cat.jpg /run/image_dir\n# touch /run/image_dir/cat.jpg.done\n\nClients should wait for appearance of \'cat.jpg.txt.done\' before getting\nresult from \'cat.jpg.txt\'.\n""""""\n\n\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport sys\nimport time\n\nfrom datetime import datetime\n\nimport cv2\nimport movidius as mv\nimport numpy as np\n\nmvng = None\n\n\ndef logging(*args):\n    print(""[%08.3f]"" % time.time(), \' \'.join(args))\n\n\ndef run(mvng):\n    """"""Infinite loop serving inference requests""""""\n\n    logging(\'detection service is running\')\n\n    capture = cv2.VideoCapture(0)\n    while True:\n        t_start = datetime.now()\n        status, image_data = capture.read()\n        t_end = datetime.now()\n        t_capture = t_end - t_start\n        logging(\'capture time: {} ms\'.format(t_capture.total_seconds() * 1000))\n\n        t_start = datetime.now()\n        processed_data = mv.process_yolo_input(image_data)\n        t_end = datetime.now()\n        t_preprocess = t_end - t_start\n        logging(\'preprocess time: {} ms\'.format(t_preprocess.total_seconds() * 1000))\n\n        t_start = datetime.now()\n        output = mvng.inference(processed_data)\n        t_end = datetime.now()\n        t_inference = t_end - t_start\n        logging(\'inference time: {} ms\'.format(t_inference.total_seconds() * 1000))\n\n        t_start = datetime.now()\n        interpreted_output = mv.interpret_yolo_output(output,\n                                                      image_data.shape[1],\n                                                      image_data.shape[0])\n        yolo_outputs = mv.process_yolo_output(image_data,\n                                              interpreted_output,\n                                              \'/tmp/yolo_resutl.jpg\')\n        t_end = datetime.now()\n        t_postprocess = t_end - t_start\n        logging(\'postprocess time: {} ms\'.format(t_postprocess.total_seconds() * 1000))\n\n        #output_name = input_name + \'.txt\'\n        #mv.save_yolo_output_text(output_name, yolo_outputs)\n        #logging(input_name, "" detected!"")\n\n\ndef parse_args():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\'--model\', required=True, help=\'Model file path\')\n    ap.add_argument(\'--label\', required=True, help=\'Label file path\')\n    ap.add_argument(\'--image_dir\', required=True, help=\'Path to image file\')\n    #ap.add_argument(\'--num_top_predictions\', default=5,\n    #                help=\'Display this many predictions\')\n    return vars(ap.parse_args())\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n\n    mvng = mv.MovidiusNeuralGraph(args[\'model\'], args[\'label\'])\n\n    logging(""model filepath: "", args[\'model\'])\n    logging(""label filepath: "", args[\'label\'])\n    logging(""image_dir: "", args[\'image_dir\'])\n\n    # workaround the issue that SIGINT cannot be received (fork a child to\n    # avoid blocking the main process in Thread.join()\n    child_pid = os.fork()\n    if child_pid == 0:  # child\n        run(mvng)\n    else:  # parent\n        try:\n            os.wait()\n        except KeyboardInterrupt:\n            os.kill(child_pid, signal.SIGKILL)\n            os.unlink(pidfile)\n'"
inference/detection_server.py,0,"b'# Copyright 2017 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Simple image detection server with Tiny-YOLO.\n\nThe server monitors image_dir and run inferences on new images added to the\ndirectory. Every image file should come with another empty file with \'.done\'\nsuffix to signal readiness. Inference result of a image can be read from the\n\'.txt\' file of that image after \'.txt.done\' is spotted.\n\nThis is an example the server expects clients to do. Note the order.\n\n# cp cat.jpg /run/image_dir\n# touch /run/image_dir/cat.jpg.done\n\nClients should wait for appearance of \'cat.jpg.txt.done\' before getting\nresult from \'cat.jpg.txt\'.\n""""""\n\nfrom __future__ import print_function\n\nimport logging\nimport multiprocessing\nimport os\nimport Queue\nimport signal\nimport sys\nimport threading\nimport time\n\nimport cv2\nimport numpy as np\n\nfrom os.path import join as pjoin\nfrom watchdog.observers import Observer\nfrom watchdog.events import PatternMatchingEventHandler\nfrom darkflow.net.build import TFNet\n\n\nimage_queue = Queue.Queue()\n\n# FIXME: Make these variables configurable\nImageDir = \'../image\'\nSystemSnapshot = \'../../dashboard/www/freeboard/snapshot.jpg\'\n\n\ndef _logging(*args):\n    print(""[%08.3f]"" % time.time(), \' \'.join(args))\n\n\ndef touch(fname, times=None):\n    with open(fname, \'a\'):\n        os.utime(fname, times)\n\n\ndef drawBoundingBoxes(imageData, imageOutputPath, inferenceResults, colorMap):\n    """"""Draw bounding boxes on an image.\n\n    imageData: image data in numpy array format\n    imageOutputPath: output image file path\n    inferenceResults: Darkflow inference results\n    colorMap: Bounding box color candidates, list of RGB tuples.\n    """"""\n    # TODO: return raw data instead of save image\n    for res in inferenceResults:\n        left = res[\'topleft\'][\'x\']\n        top = res[\'topleft\'][\'y\']\n        right = res[\'bottomright\'][\'x\']\n        bottom = res[\'bottomright\'][\'y\']\n        colorIndex = res[\'coloridx\']\n        color = colorMap[colorIndex]\n        label = res[\'label\']\n        confidence = res[\'confidence\']\n        imgHeight, imgWidth, _ = imageData.shape\n        thick = int((imgHeight + imgWidth) // 300)\n\n        cv2.rectangle(imageData,(left, top), (right, bottom), color, thick)\n        cv2.putText(imageData, label, (left, top - 12), 0, 1e-3 * imgHeight,\n            color, thick//3)\n    cv2.imwrite(imageOutputPath, imageData)\n    logging.debug(\'Save bounding box result image to {}\'.format(imageOutputPath))\n\n\ndef server(tfnet):\n    """"""Infinite loop serving inference requests""""""\n\n    global image_queue\n\n    _logging(threading.current_thread().getName(), ""is running"")\n\n    while True:\n        input_name = image_queue.get()\n\n        _logging(\'input image \' + input_name)\n        imgcv = cv2.imread(input_name)\n        _logging(\'start inference\')\n        result = tfnet.return_predict(imgcv)\n        _logging(\'inference result: {}\'.format(result))\n\n        # overwrite existing input snapshot by the result image with\n        # bounding boxes.\n        drawBoundingBoxes(imgcv, SystemSnapshot, result, tfnet.meta[\'colors\'])\n        logging.debug(\'System snapshot path: %s\' % pjoin(os.getcwd(), SystemSnapshot))\n\n        output_name = input_name+\'.txt\'\n        output_done_name = output_name+\'.done\'\n        with open(output_name, \'w\') as f:\n            f.write(str(result))\n        touch(output_done_name)\n        _logging(input_name, "" classified!"")\n\n\nclass EventHandler(PatternMatchingEventHandler):\n    def process(self, event):\n        """"""\n        event.event_type\n            \'modified\' | \'created\' | \'moved\' | \'deleted\'\n        event.is_directory\n            True | False\n        event.src_path\n            path/to/observed/file\n        """"""\n        # the file will be processed there\n        global image_queue\n\n        _msg = event.src_path\n        image_queue.put(_msg.rstrip(\'.done\'))\n        os.remove(_msg)\n        _logging(_msg, event.event_type)\n\n    # ignore all other types of events except \'modified\'\n    def on_created(self, event):\n        self.process(event)\n\n\ndef main():\n    options = {\n        ""model"": ""cfg/tiny-yolo.cfg"",\n        ""load"": ""bin/tiny-yolo.weights"",\n        \'verbalise\': True,\n        #""threshold"": 0.1\n    }\n    tfnet = TFNet(options)\n    _logging(\'model dir: {}\'.format(options[\'load\']))\n    _logging(\'config dir: {}\'.format(options[\'model\']))\n\n    server(tfnet)\n\n\nif __name__ == \'__main__\':\n    logging.basicConfig(filename=\'/tmp/dlDetector.log\', level=logging.DEBUG)\n\n    pid = str(os.getpid())\n    pidfile = ""/tmp/detection_server.pid""\n\n    if os.path.isfile(pidfile):\n        _logging(""%s already exists, exiting"" % pidfile)\n        sys.exit(1)\n\n    with open(pidfile, \'w\') as f:\n        f.write(pid)\n\n    # workaround the issue that SIGINT cannot be received (fork a child to\n    # avoid blocking the main process in Thread.join()\n    child_pid = os.fork()\n    if child_pid == 0:\n        # child\n        # observer handles event in a different thread\n        observer = Observer()\n        observer.schedule(EventHandler([\'*.jpg.done\']), ImageDir)\n        observer.start()\n        main()\n    else:\n        # parent\n        try:\n            os.wait()\n        except KeyboardInterrupt:\n            os.kill(child_pid, signal.SIGKILL)\n            os.unlink(pidfile)\n'"
inference/dlmodelmgr.py,0,"b'# Copyright 2017 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""\nDL Model Manager, following the DLModelBox model package\nspeccification.\n""""""\n\nfrom __future__ import print_function\n\nimport argparse\nimport json\nimport logging\nimport os\n\n\nclass DLModelManager(object):\n    def __init__(self):\n        self.basedir = \'/var/lib/dlmodels\'\n\n    def get_model_names(self):\n        return os.listdir(self.basedir)\n\n    def get_model_meta(self, modelname):\n        meta_filepath = os.path.join(self.basedir, modelname, \'meta.json\')\n        with open(meta_filepath, \'r\') as f:\n            meta = json.load(f)\n        meta[\'model\'] = os.path.join(self.basedir, modelname, meta[\'model\'])\n        meta[\'label\'] = os.path.join(self.basedir, modelname, meta[\'label\'])\n        for k, v in meta[\'config\'].items():\n            meta[\'config\'][k] = os.path.join(self.basedir, modelname,\n                                             meta[\'config\'][k])\n        return meta\n\n\ndef parse_args():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\'--modelname\',\n                    help=\'Model package name (without version)\')\n    return vars(ap.parse_args())\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    logging.debug(\'model package name: \', args[\'modelname\'])\n\n    dlmm = DLModelManager()\n    for name in dlmm.get_model_names():\n        print(dlmm.get_model_meta(name))\n'"
inference/engineservice.py,0,"b'# Copyright 2017 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Engine service is a bridge between incoming data and inference engine.\n""""""\n\n\nfrom __future__ import print_function\n\nimport argparse\nimport logging\nimport multiprocessing\nimport os\nimport signal\nimport sys\nimport threading\nimport time\n\nimport cv2\nimport numpy as np\nimport Queue\n\nfrom dlmodelmgr import DLModelManager\nfrom watchdog.observers import Observer\nfrom watchdog.events import PatternMatchingEventHandler\n\n\nclass DLEngine(object):\n    def __init__(self):\n        self.model_input_cache = []\n        self.model_output_cache = []\n        self.cache = {\n            \'model_input\': [],\n            \'model_output\': \'\',\n            \'model_output_filepath\': \'\'\n        }\n\n    def create(self):\n        # Workaround to posepone TensorFlow initialization.\n        # If TF is initialized in __init__, and pass an engine instance\n        # to engine service, TF session will stuck in run().\n        pass\n\n    def process_input(self, tensor):\n        return tensor\n\n    def inference(self, tensor):\n        output = None\n        return output\n\n    def process_output(self, output):\n        return output\n\n    def cache_data(self, key, value):\n        self.cache[key] = value\n\n    def save_cache(self):\n        with open(self.cache[\'model_output_filepath\'], \'w\') as f:\n            f.write(str(self.cache[\'model_output\']))\n\n\nclass EventHandler(PatternMatchingEventHandler):\n    def process(self, event):\n        """"""\n        event.event_type\n            \'modified\' | \'created\' | \'moved\' | \'deleted\'\n        event.is_directory\n            True | False\n        event.src_path\n            path/to/observed/file\n        """"""\n        # the file will be processed there\n        _msg = event.src_path\n        self.image_queue.put(_msg.rstrip(\'.done\'))\n        os.remove(_msg)\n        logging.debug(_msg + \' \' + event.event_type)\n\n    # ignore all other types of events except \'modified\'\n    def on_created(self, event):\n        self.process(event)\n\n\nclass EngineService(object):\n    def __init__(self, service_name, engine):\n        self.service_name = service_name\n        self.engine = engine\n        self.image_queue = Queue.Queue()\n        self.event_handler = EventHandler([\'*.jpg.done\'])\n        # NOTE: Increase object reference count (share memory)\n        #       instead of object creation.\n        self.event_handler.image_queue = self.image_queue\n\n    def touch(self, fname, times=None):\n        with open(fname, \'a\'):\n            os.utime(fname, times)\n\n    def server(self):\n        """"""Infinite loop serving inference requests""""""\n\n        logging.info(threading.current_thread().getName() + "" is running"")\n\n        self.engine.create()\n        while True:\n            input_name = self.image_queue.get()\n            image_data = cv2.imread(input_name).astype(np.float32)\n            self.engine.cache_data(\'model_input\', image_data)\n            image_data = self.engine.process_input(image_data)\n\n            output = self.engine.inference(image_data)\n            model_outputs = self.engine.process_output(output)\n            self.engine.cache_data(\'model_output\', model_outputs)\n\n            output_name = input_name + \'.txt\'\n            output_done_name = output_name + \'.done\'\n            self.engine.cache_data(\'model_output_filepath\', output_name)\n            self.engine.save_cache()\n            self.touch(output_done_name)\n            logging.debug(input_name + "" classified!"")\n\n    def run(self, args):\n        self.record_pid()\n\n        # workaround the issue that SIGINT cannot be received (fork a child to\n        # avoid blocking the main process in Thread.join()\n        child_pid = os.fork()\n        if child_pid == 0:  # child\n            # observer handles event in a different thread\n            observer = Observer()\n            observer.schedule(self.event_handler, path=args[\'image_dir\'])\n            observer.start()\n            self.server()\n        else:  # parent\n            try:\n                os.wait()\n            except KeyboardInterrupt:\n                os.kill(child_pid, signal.SIGKILL)\n                self.erase_pid()\n\n    def record_pid(self):\n        """"""Write a PID pidfile /tmp/<service_name>.pid.\n        """"""\n        pid = str(os.getpid())\n        pidfile = \'/tmp/{}.pid\'.format(self.service_name)\n        if os.path.isfile(pidfile):\n            logging.critical(""%s already exists, exiting"" % pidfile)\n            sys.exit(1)\n        with open(pidfile, \'w\') as f:\n            f.write(pid)\n\n    def erase_pid(self):\n        pidfile = \'/tmp/{}.pid\'.format(self.service_name)\n        os.unlink(pidfile)\n\n\ndef parse_args():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\'--model\',\n                    help=\'Model file path\')\n    ap.add_argument(\'--label\',\n                    help=\'Label file path\')\n    ap.add_argument(\'--model_package\',\n                    default=\'\',\n                    help=\'Model package name\')\n    ap.add_argument(\'--image_dir\', required=True,\n                    help=\'Path to image file\')\n    ap.add_argument(\'--service_name\', required=True,\n                    help=\'Engine service name used as PID filename\')\n    ap.add_argument(\'--num_top_predictions\', default=5,\n                    help=\'Display this many predictions\')\n    return vars(ap.parse_args())\n\n\nif __name__ == \'__main__\':\n    import movidius as mv\n\n    logging.basicConfig(level=logging.DEBUG)\n    args = parse_args()\n    if args[\'model_package\'] != \'\':\n        dlmm = DLModelManager()\n        meta = dlmm.get_model_meta(args[\'model_package\'])\n        args[\'model\'] = meta[\'model\']\n        args[\'label\'] = meta[\'label\']\n    logging.debug(\'model filepath: \' + args[\'model\'])\n    logging.debug(\'label filepath: \' + args[\'label\'])\n    logging.debug(\'image_dir: \' + args[\'image_dir\'])\n\n    mvng = mv.MovidiusNeuralGraph(args[\'model\'], args[\'label\'])\n    engine_service = EngineService(args[\'service_name\'], mvng)\n    engine_service.run(args)\n'"
inference/movidius.py,0,"b""#!/usr/bin/python\n#\n# Copyright 2017 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\nimport os\n\nimport cv2\nimport numpy as np\n\nfrom mvnc import mvncapi as mvnc\nfrom skimage.transform import resize\n\n\nclass MovidiusNeuralGraph(object):\n    def __init__(self, graph_filepath, label_filepath):\n        # mvnc.SetGlobalOption(mvnc.GlobalOption.LOGLEVEL, 2)\n        devices = mvnc.EnumerateDevices()\n        if len(devices) == 0:\n            raise Exception('No devices found')\n        self.device = mvnc.Device(devices[0])\n        self.device.OpenDevice()\n\n        # Load graph\n        with open(graph_filepath, mode='rb') as f:\n            graphfile = f.read()\n        self.graph = self.device.AllocateGraph(graphfile)\n\n        # Load labels\n        self.labels = []\n        with open(label_filepath, 'r') as f:\n            for line in f:\n                label = line.split('\\n')[0]\n                if label != 'classes':\n                    self.labels.append(label)\n            f.close()\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.graph.DeallocateGraph()\n        self.device.CloseDevice()\n\n    def inference(self, data):\n        self.graph.LoadTensor(data.astype(np.float16), 'user object')\n        output, userobj = self.graph.GetResult()\n        return output\n\n    def get_graph(self):\n        return self.graph\n\n    def get_labels(self):\n        return self.labels\n\n\ndef process_inceptionv3_input(img):\n    image_size = 299\n    mean = 128\n    std = 1.0/128\n\n    dx, dy, dz = img.shape\n    delta = float(abs(dy - dx))\n    if dx > dy:  # crop the x dimension\n        img = img[int(0.5*delta):dx-int(0.5*delta), 0:dy]\n    else:\n        img = img[0:dx, int(0.5*delta):dy-int(0.5*delta)]\n    img = cv2.resize(img, (image_size, image_size))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    for i in range(3):\n        img[:, :, i] = (img[:, :, i] - mean) * std\n    return img\n\n\ndef process_inceptionv3_output(output, labels):\n    top_inds = output.argsort()[::-1][:5]\n    return [(labels[top_inds[i]], output[top_inds[i]]) for i in range(5)]\n\n\ndef print_inceptionv3_output(output, labels):\n    top_inds = output.argsort()[::-1][:5]\n\n    for i in range(5):\n        print(top_inds[i], labels[top_inds[i]], output[top_inds[i]])\n\n\ndef interpret_yolo_output(output, img_width, img_height):\n    output = output.astype(np.float32)\n\n    classes = [\n        'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n        'bus', 'car', 'cat', 'chair', 'cow',\n        'diningtable', 'dog', 'horse', 'motorbike', 'person',\n        'pottedplant', 'sheep', 'sofa', 'train','tvmonitor'\n    ]\n    threshold = 0.2\n    iou_threshold = 0.5\n    num_class = 20\n    num_box = 2\n    grid_size = 7\n    probs = np.zeros((7, 7, 2, 20))\n    class_probs = (np.reshape(output[0:980], (7, 7, 20)))  #.copy()\n    scales = (np.reshape(output[980:1078], (7, 7, 2)))  #.copy()\n    boxes = (np.reshape(output[1078:], (7, 7, 2, 4)))  #.copy()\n    offset = np.transpose(\n        np.reshape(np.array([np.arange(7)] * 14), (2, 7, 7)),\n        (1, 2, 0)\n    )\n    #boxes.setflags(write=1)\n    boxes[:, :, :, 0] += offset\n    boxes[:, :, :, 1] += np.transpose(offset, (1, 0, 2))\n    boxes[:, :, :, 0:2] = boxes[:, :, :, 0:2] / 7.0\n    boxes[:, :, :, 2] = np.multiply(boxes[:, :, :, 2], boxes[:, :, :, 2])\n    boxes[:, :, :, 3] = np.multiply(boxes[:, :, :, 3], boxes[:, :, :, 3])\n\n    boxes[:, :, :, 0] *= img_width\n    boxes[:, :, :, 1] *= img_height\n    boxes[:, :, :, 2] *= img_width\n    boxes[:, :, :, 3] *= img_height\n\n    for i in range(2):\n        for j in range(20):\n            probs[:, :, i, j] = np.multiply(class_probs[:, :, j],\n                                            scales[:, :, i])\n    #print (probs)\n    filter_mat_probs = np.array(probs >= threshold, dtype='bool')\n    filter_mat_boxes = np.nonzero(filter_mat_probs)\n    boxes_filtered = boxes[filter_mat_boxes[0],\n                           filter_mat_boxes[1],\n                           filter_mat_boxes[2]]\n    probs_filtered = probs[filter_mat_probs]\n    classes_num_filtered = np.argmax(probs, axis=3)[filter_mat_boxes[0],\n                                                    filter_mat_boxes[1],\n                                                    filter_mat_boxes[2]]\n\n    argsort = np.array(np.argsort(probs_filtered))[::-1]\n    boxes_filtered = boxes_filtered[argsort]\n    probs_filtered = probs_filtered[argsort]\n    classes_num_filtered = classes_num_filtered[argsort]\n\n    for i in range(len(boxes_filtered)):\n        if probs_filtered[i] == 0:\n            continue\n        for j in range(i + 1, len(boxes_filtered)):\n            if iou(boxes_filtered[i], boxes_filtered[j]) > iou_threshold:\n                probs_filtered[j] = 0.0\n\n    filter_iou = np.array(probs_filtered > 0.0, dtype='bool')\n    boxes_filtered = boxes_filtered[filter_iou]\n    probs_filtered = probs_filtered[filter_iou]\n    classes_num_filtered = classes_num_filtered[filter_iou]\n\n    result = []\n    for i in range(len(boxes_filtered)):\n        result.append([classes[classes_num_filtered[i]],\n                       boxes_filtered[i][0],\n                       boxes_filtered[i][1],\n                       boxes_filtered[i][2],\n                       boxes_filtered[i][3],\n                       probs_filtered[i]])\n\n    return result\n\n\ndef iou(box1, box2):\n    tb = (min(box1[0] + 0.5 * box1[2], box2[0] + 0.5 * box2[2]) -\n          max(box1[0] - 0.5 * box1[2], box2[0] - 0.5 * box2[2]))\n    lr = (min(box1[1] + 0.5 * box1[3], box2[1] + 0.5 * box2[3]) -\n          max(box1[1] - 0.5 * box1[3], box2[1] - 0.5 * box2[3]))\n    if tb < 0 or lr < 0:\n        intersection = 0\n    else:\n        intersection =  tb*lr\n    return intersection / (box1[2] * box1[3] + box2[2] * box2[3] - intersection)\n\n\ndef process_yolo_output(img, results, img_filepath):\n    img_width = img.shape[1]\n    img_height = img.shape[0]\n    img_cp = img.copy()\n\n    print_yolo_output(results)\n\n    # draw bounding boxes on input image\n    for i in range(len(results)):\n        x = int(results[i][1])\n        y = int(results[i][2])\n        w = int(results[i][3]) // 2\n        h = int(results[i][4]) // 2\n        xmin = x - w\n        xmax = x + w\n        ymin = y - h\n        ymax = y + h\n        if xmin < 0:\n            xmin = 0\n        if ymin < 0:\n            ymin = 0\n        if xmax > img_width:\n            xmax = img_width\n        if ymax > img_height:\n            ymax = img_height\n        cv2.rectangle(img_cp,\n                      (xmin, ymin),\n                      (xmax, ymax),\n                      (0, 255, 0),\n                      2)\n        cv2.rectangle(img_cp,\n                      (xmin, ymin - 20),\n                      (xmax, ymin),\n                      (125, 125, 125),\n                      -1)\n        cv2.putText(img_cp,results[i][0] + ' : %.2f' % results[i][5],\n                    (xmin + 5, ymin - 7),\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    0.5,\n                    (0, 0, 0),\n                    1)\n    save_yolo_output_image(img_filepath, img_cp)\n    return results\n\n\ndef process_yolo_input(rgb_data):\n    from datetime import datetime\n    input_dim = (448, 448)\n\n    t_start = datetime.now()\n    tmp_data = rgb_data.copy()\n    t_end = datetime.now()\n    t_pass = t_end - t_start\n    print('copy: {} ms'.format(t_pass.total_seconds() * 1000))\n\n    t_start = datetime.now()\n    #tmp_data = resize(tmp_data / 255.0, input_dim, 1)  # ~270 ms\n    tmp_data = cv2.resize(tmp_data / 255.0, input_dim)  # ~65 ms\n    t_end = datetime.now()\n    t_pass = t_end - t_start\n    print('resize: {} ms'.format(t_pass.total_seconds() * 1000))\n\n    t_start = datetime.now()\n    tmp_data[:, :, (2, 1, 0)]  # BGR2RGB\n    t_end = datetime.now()\n    t_pass = t_end - t_start\n    print('BGR2RGB: {} ms'.format(t_pass.total_seconds() * 1000))\n\n    t_start = datetime.now()\n    input_data = tmp_data.astype(np.float16)\n    t_end = datetime.now()\n    t_pass = t_end - t_start\n    print('astype: {} ms'.format(t_pass.total_seconds() * 1000))\n\n    return input_data\n\n\ndef save_yolo_output_text(text_filepath, output):\n    with open(text_filepath, 'w') as f:\n        f.write(str(output))\n\n\ndef save_yolo_output_image(image_filepath, image_data):\n    cv2.imwrite(image_filepath, image_data)\n\n\ndef print_yolo_output(output):\n    for i in range(len(output)):\n        x = int(output[i][1])\n        y = int(output[i][2])\n        #w = int(output[i][3]) // 2\n        #h = int(output[i][4]) // 2\n        print('\\tclass = {label}'.format(label=output[i][0]))\n        print('\\t[x, y, w, h] = [{x}, {y}, {w}, {h}]'.format(\n            x=str(x),\n            y=str(y),\n            w=str(int(output[i][3])),\n            h=str(int(output[i][4]))))\n        print('\\tconfidence = {conf}'.format(conf=str(output[i][5])))\n\n\nif __name__ == '__main__':\n    graph_filepath = ''  # model filepath\n    label_filepath = ''  # label filepath\n    path_to_images = ''  # image dirpath\n    image_filenames = [os.path.join(path_to_images, image_name)\n                       for image_name in []]  # image filename list\n\n    movidius = MovidiusNeuralGraph(graph_filepath, label_filepath)\n    labels = movidius.get_labels()\n\n    print(''.join(['*' for i in range(79)]))\n    print('inception-v3 on NCS')\n    for image_filename in image_filenames:\n        img = cv2.imread(image_filename).astype(np.float32)\n        img = process_inceptionv3_input(img)\n        print(''.join(['*' for i in range(79)]))\n        print('Start download to NCS...')\n        output = movidius.inference(img)\n        print_inceptionv3_output(output, labels)\n\n    print(''.join(['*' for i in range(79)]))\n    print('Finished')\n"""
inference/movidius_engine.py,0,"b'# Copyright 2017 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Movidius inference engine.\n""""""\n\nfrom __future__ import print_function\n\nimport argparse\nimport logging\n\nimport movidius as mv\nfrom engineservice import EngineService\nfrom engineservice import DLEngine\nfrom dlmodelmgr import DLModelManager\n\n\nclass MovidiusEngine(DLEngine):\n    def __init__(self, model, label):\n        super(MovidiusEngine, self).__init__()\n        self.mvng = mv.MovidiusNeuralGraph(model, label)\n\n    def process_input(self, tensor):\n        return mv.process_inceptionv3_input(tensor)\n\n    def inference(self, tensor):\n        return self.mvng.inference(tensor)\n\n    def process_output(self, output):\n        return mv.process_inceptionv3_output(\n                   output,\n                   self.mvng.get_labels())\n\n    def save_cache(self):\n        with open(self.cache[\'model_output_filepath\'], \'w\') as f:\n            for i in self.cache[\'model_output\']:\n                print(""%s (score = %.5f)"" % (i[0], i[1]), file=f)\n\n\ndef parse_args():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\'--model\',\n                    help=\'Model file path\')\n    ap.add_argument(\'--label\',\n                    help=\'Label file path\')\n    ap.add_argument(\'--model_package\',\n                    default=\'\',\n                    help=\'Model package ""name-version"" naming\')\n    ap.add_argument(\'--image_dir\', required=True,\n                    help=\'Path to image file\')\n    ap.add_argument(\'--service_name\', required=True,\n                    help=\'Engine service name used as PID filename\')\n    ap.add_argument(\'--num_top_predictions\', default=5,\n                    help=\'Display this many predictions\')\n    return vars(ap.parse_args())\n\n\nif __name__ == \'__main__\':\n    logging.basicConfig(level=logging.DEBUG)\n    args = parse_args()\n    if args[\'model_package\'] != \'\':\n        dlmm = DLModelManager()\n        meta = dlmm.get_model_meta(args[\'model_package\'])\n        args[\'model\'] = meta[\'model\']\n        args[\'label\'] = meta[\'label\']\n    logging.debug(\'model filepath: \' + args[\'model\'])\n    logging.debug(\'label filepath: \' + args[\'label\'])\n    logging.debug(\'image_dir: \' + args[\'image_dir\'])\n\n    movidius_engine = MovidiusEngine(args[\'model\'], args[\'label\'])\n    engine_service = EngineService(args[\'service_name\'], movidius_engine)\n    engine_service.run(args)\n'"
inference/yoloutils.py,0,"b'import os\nimport sys\nimport time\n\nfrom datetime import datetime\n\nimport cv2\nimport numpy as np\n\nfrom mvnc import mvncapi as mvnc\nfrom skimage.transform import resize\n\n\ndef interpret_yolo_output(output, img_width, img_height):\n    output = output.astype(np.float32)\n\n    classes = [\n        \'aeroplane\', \'bicycle\', \'bird\', \'boat\', \'bottle\',\n        \'bus\', \'car\', \'cat\', \'chair\', \'cow\',\n        \'diningtable\', \'dog\', \'horse\', \'motorbike\', \'person\',\n        \'pottedplant\', \'sheep\', \'sofa\', \'train\',\'tvmonitor\'\n    ]\n    threshold = 0.2\n    iou_threshold = 0.5\n    num_class = 20\n    num_box = 2\n    grid_size = 7\n    probs = np.zeros((7, 7, 2, 20))\n    class_probs = (np.reshape(output[0:980], (7, 7, 20)))  #.copy()\n    scales = (np.reshape(output[980:1078], (7, 7, 2)))  #.copy()\n    boxes = (np.reshape(output[1078:], (7, 7, 2, 4)))  #.copy()\n    offset = np.transpose(\n        np.reshape(np.array([np.arange(7)] * 14), (2, 7, 7)),\n        (1, 2, 0)\n    )\n    #boxes.setflags(write=1)\n    boxes[:, :, :, 0] += offset\n    boxes[:, :, :, 1] += np.transpose(offset, (1, 0, 2))\n    boxes[:, :, :, 0:2] = boxes[:, :, :, 0:2] / 7.0\n    boxes[:, :, :, 2] = np.multiply(boxes[:, :, :, 2], boxes[:, :, :, 2])\n    boxes[:, :, :, 3] = np.multiply(boxes[:, :, :, 3], boxes[:, :, :, 3])\n\n    boxes[:, :, :, 0] *= img_width\n    boxes[:, :, :, 1] *= img_height\n    boxes[:, :, :, 2] *= img_width\n    boxes[:, :, :, 3] *= img_height\n\n    for i in range(2):\n        for j in range(20):\n            probs[:, :, i, j] = np.multiply(class_probs[:, :, j],\n                                            scales[:, :, i])\n    #print (probs)\n    filter_mat_probs = np.array(probs >= threshold, dtype=\'bool\')\n    filter_mat_boxes = np.nonzero(filter_mat_probs)\n    boxes_filtered = boxes[filter_mat_boxes[0],\n                           filter_mat_boxes[1],\n                           filter_mat_boxes[2]]\n    probs_filtered = probs[filter_mat_probs]\n    classes_num_filtered = np.argmax(probs, axis=3)[filter_mat_boxes[0],\n                                                    filter_mat_boxes[1],\n                                                    filter_mat_boxes[2]]\n\n    argsort = np.array(np.argsort(probs_filtered))[::-1]\n    boxes_filtered = boxes_filtered[argsort]\n    probs_filtered = probs_filtered[argsort]\n    classes_num_filtered = classes_num_filtered[argsort]\n\n    for i in range(len(boxes_filtered)):\n        if probs_filtered[i] == 0:\n            continue\n        for j in range(i + 1, len(boxes_filtered)):\n            if iou(boxes_filtered[i], boxes_filtered[j]) > iou_threshold:\n                probs_filtered[j] = 0.0\n\n    filter_iou = np.array(probs_filtered > 0.0, dtype=\'bool\')\n    boxes_filtered = boxes_filtered[filter_iou]\n    probs_filtered = probs_filtered[filter_iou]\n    classes_num_filtered = classes_num_filtered[filter_iou]\n\n    result = []\n    for i in range(len(boxes_filtered)):\n        result.append([classes[classes_num_filtered[i]],\n                       boxes_filtered[i][0],\n                       boxes_filtered[i][1],\n                       boxes_filtered[i][2],\n                       boxes_filtered[i][3],\n                       probs_filtered[i]])\n\n    return result\n\n\ndef iou(box1, box2):\n    tb = (min(box1[0] + 0.5 * box1[2], box2[0] + 0.5 * box2[2]) -\n          max(box1[0] - 0.5 * box1[2], box2[0] - 0.5 * box2[2]))\n    lr = (min(box1[1] + 0.5 * box1[3], box2[1] + 0.5 * box2[3]) -\n          max(box1[1] - 0.5 * box1[3], box2[1] - 0.5 * box2[3]))\n    if tb < 0 or lr < 0:\n        intersection = 0\n    else:\n        intersection =  tb*lr\n    return intersection / (box1[2] * box1[3] + box2[2] * box2[3] - intersection)\n\n\ndef process_yolo_output(img, results):\n    img_width = img.shape[1]\n    img_height = img.shape[0]\n    img_cp = img.copy()\n\n    print_yolo_output(results)\n\n    # draw bounding boxes on input image\n    for i in range(len(results)):\n        x = int(results[i][1])\n        y = int(results[i][2])\n        w = int(results[i][3]) // 2\n        h = int(results[i][4]) // 2\n        xmin = x - w\n        xmax = x + w\n        ymin = y - h\n        ymax = y + h\n        if xmin < 0:\n            xmin = 0\n        if ymin < 0:\n            ymin = 0\n        if xmax > img_width:\n            xmax = img_width\n        if ymax > img_height:\n            ymax = img_height\n        cv2.rectangle(img_cp,\n                      (xmin, ymin),\n                      (xmax, ymax),\n                      (0, 255, 0),\n                      2)\n        cv2.rectangle(img_cp,\n                      (xmin, ymin - 20),\n                      (xmax, ymin),\n                      (125, 125, 125),\n                      -1)\n        cv2.putText(img_cp,results[i][0] + \' : %.2f\' % results[i][5],\n                    (xmin + 5, ymin - 7),\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    0.5,\n                    (0, 0, 0),\n                    1)\n    cv2.imwrite(\'/tmp/yolo_result.jpg\', img_cp)\n\n\ndef process_yolo_input(rgb_data):\n    input_dim = (448, 448)\n    tmp_data = rgb_data.copy()\n    tmp_data = resize(tmp_data / 255.0, input_dim, 1)\n    tmp_data[:, :, (2, 1, 0)]  # BGR2RGB\n    input_data = tmp_data.astype(np.float16)\n    return input_data\n\n\ndef print_yolo_output(output):\n    for i in range(len(output)):\n        x = int(output[i][1])\n        y = int(output[i][2])\n        #w = int(output[i][3]) // 2\n        #h = int(output[i][4]) // 2\n        print(\'\\tclass = {label}\'.format(label=output[i][0]))\n        print(\'\\t[x, y, w, h] = [{x}, {y}, {w}, {h}]\'.format(\n            x=str(x),\n            y=str(y),\n            w=str(int(output[i][3])),\n            h=str(int(output[i][4]))))\n        print(\'\\tconfidence = {conf}\'.format(conf=str(results[i][5])))\n\n\nif __name__ == \'__main__\':\n    if len(sys.argv) != 2:\n        print (""YOLOv1 Tiny example: python3 py_examples/yolo_example.py images/dog.jpg"")\n        sys.exit()\n\n    network_blob=\'/home/pi/codes/yoloNCS/graph\'\n    # configuration NCS\n    mvnc.SetGlobalOption(mvnc.GlobalOption.LOG_LEVEL, 2)\n    devices = mvnc.EnumerateDevices()\n    if len(devices) == 0:\n        print(\'No devices found\')\n        quit()\n    device = mvnc.Device(devices[0])\n    device.OpenDevice()\n    opt = device.GetDeviceOption(mvnc.DeviceOption.OPTIMISATION_LIST)\n    # load blob\n    with open(network_blob, mode=\'rb\') as f:\n        blob = f.read()\n    graph = device.AllocateGraph(blob)\n    graph.SetGraphOption(mvnc.GraphOption.ITERATIONS, 1)\n    iterations = graph.GetGraphOption(mvnc.GraphOption.ITERATIONS)\n\n    # image preprocess\n    img = cv2.imread(sys.argv[1])\n    input_data = process_yolo_input(img)\n\n    # start MOD\n    start = datetime.now()\n    graph.LoadTensor(input_data, \'user object\')\n    out, userobj = graph.GetResult()\n    end = datetime.now()\n    elapsedTime = end-start\n    print(\'total time is "" milliseconds\', elapsedTime.total_seconds()*1000)\n\n    # fc27 instead of fc12 for yolo_small\n    results = interpret_yolo_output(out,\n                                    img.shape[1],\n                                    img.shape[0])\n    #print (results)\n    #cv2.imshow(\'YOLO detection\',img_cv)\n    process_yolo_output(img, results)\n    #cv2.waitKey(10000)\n\n    graph.DeallocateGraph()\n    device.CloseDevice()\n'"
test/test_load_config.py,0,"b'#!/usr/bin/python\n\nimport json\nimport subprocess\n\n\ndef loadConfig():\n    cmd = (""node -e \\""const config = ""\n        ""require(\'../config\'); console.log(JSON.stringify(config));\\"""")\n    config = json.loads(subprocess.check_output(cmd, shell=True))\n    print(config)\n\n\nif __name__ == \'__main__\':\n    loadConfig()\n'"
tests/__init__.py,0,b''
tests/test_tensorflow_engine.py,0,"b""import unittest\n\nimport cv2\n\nfrom berrynet.engine.tensorflow_engine import TensorFlowEngine\n\n\nclass TestTensorFlowEngine(unittest.TestCase):\n    def test_engine(self):\n        model = 'berrynet/engine/inception_v3_2016_08_28_frozen.pb'\n        label = 'berrynet/engine/imagenet_slim_labels.txt'\n        jpg_filepath = 'berrynet/engine/grace_hopper.jpg'\n        input_layer = 'input:0'\n        output_layer = 'InceptionV3/Predictions/Reshape_1:0'\n\n        tfe = TensorFlowEngine(model, label, input_layer, output_layer)\n        tfe.create()\n        rgb_array = cv2.cvtColor(\n            cv2.imread(jpg_filepath),\n            cv2.COLOR_BGR2RGB)\n        tfe.process_output(tfe.inference(tfe.process_input(rgb_array)))\n        tfe.process_output(tfe.inference(tfe.process_input(rgb_array)))\n        #self.assertEqual('foo'.upper(), 'FOO')\n\n    #def test_isupper(self):\n    #    self.assertTrue('FOO'.isupper())\n    #    self.assertFalse('Foo'.isupper())\n\n    #def test_split(self):\n    #    s = 'hello world'\n    #    self.assertEqual(s.split(), ['hello', 'world'])\n    #    # check that s.split fails when the separator is not a string\n    #    with self.assertRaises(TypeError):\n    #        s.split(2)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
utils/demo.py,0,"b'#!/usr/bin/python\n\nimport subprocess\n\nfrom Tkinter import *\n\nclass Application(Frame):\n    def publishSnapshotMessage(self):\n        cmd = \'mosquitto_pub -h localhost -t berrynet/event/camera -m snapshot_picam\'\n        subprocess.call(cmd, shell=True)\n\n    def startGateway(self):\n        cmd = \'bash ~/codes/BerryNet/berrynet-manager start\'\n        subprocess.call(cmd, shell=True)\n\n    def stopGateway(self):\n        cmd = \'bash ~/codes/BerryNet/berrynet-manager stop\'\n        subprocess.call(cmd, shell=True)\n\n    def startChromium(self):\n        cmd = \'bash -c ""sensible-browser http://localhost:8080/index.html#source=dashboard.json"" &\'\n        subprocess.call(cmd, shell=True)\n\n    def stopChromium(self):\n        cmd = \'bash -c ""kill -9 $(pgrep chromium)""\'\n        subprocess.call(cmd, shell=True)\n\n    def startDemo(self):\n        self.startGateway()\n        self.startChromium()\n\n    def stopDemo(self):\n        self.stopChromium()\n        self.stopGateway()\n\n    def cleanSnapshots(self):\n        cmd = \'bash -c ""sudo rm /usr/local/berrynet/inference/image/snapshot*""\'\n        subprocess.call(cmd, shell=True)\n\n    def createWidgets(self):\n        self.snapshotButton = Button(self, text=\'Snapshot\', width=16, height=5)\n        self.snapshotButton[""command""] = self.publishSnapshotMessage\n        self.snapshotButton.pack(fill=X)\n\n        self.startDemoButton = Button(self, text=\'Start Demo\', width=16, height=5)\n        self.startDemoButton[""command""] = self.startDemo\n        self.startDemoButton.pack(fill=X)\n\n        self.stopDemoButton = Button(self, text=\'Stop Demo\', width=16, height=5)\n        self.stopDemoButton[""command""] = self.stopDemo\n        self.stopDemoButton.pack(fill=X)\n\n        self.cleanButton = Button(self, text=\'Clean Snapshots\', width=16, height=5)\n        self.cleanButton[""command""] = self.cleanSnapshots\n        self.cleanButton.pack(fill=X)\n\n    def __init__(self, master=None):\n        Frame.__init__(self, master)\n        self.pack()\n        self.createWidgets()\n\nroot = Tk()\napp = Application(master=root)\napp.master.title(\'Cheese\')\napp.master.lift()\napp.master.attributes(\'-topmost\', True)\napp.mainloop()\nroot.destroy()\n'"
utils/snapshot-button.py,0,"b'#!/usr/bin/python\n\nimport subprocess\n\nfrom Tkinter import *\n\nclass Application(Frame):\n    def say_hi(self):\n        print ""hi there, everyone!""\n\n    def publishSnapshotMessage(self):\n        cmd = \'mosquitto_pub -h localhost -t berrynet/event/camera -m snapshot_picam\'\n        subprocess.call(cmd, shell=True)\n\n    def createWidgets(self):\n        self.hi_there = Button(self, text=\'Snapshot\', width=16, height=5)\n        self.hi_there[""command""] = self.publishSnapshotMessage\n        self.hi_there.pack({""side"": ""left""})\n\n    def __init__(self, master=None):\n        Frame.__init__(self, master)\n        self.pack()\n        self.createWidgets()\n\nroot = Tk()\napp = Application(master=root)\napp.master.title(\'Cheese\')\napp.master.lift()\napp.master.attributes(\'-topmost\', True)\napp.mainloop()\nroot.destroy()\n'"
utils/xmlTotxt.py,0,"b'""""""\n\n$python xmlTotxt.py -n $FOLDER -u $LABELME_USER\n --classes $CLASS_1 $CLASS_2 $CLASS_3...\n\nFOLDER: LabelMe project folder name\nLABELME_USER: LabelMe user name\nCLASS_i: Class labels defined on LabelMe\n\n""""""\nimport argparse\nimport logging\nimport os\nimport xml.etree.ElementTree as ET\n\nfrom os.path import join\nfrom shutil import copyfile\n\n\ndef convert(size, in_x, in_y):\n    dw = 1./size[0]\n    dh = 1./size[1]\n    x = (in_x[0] + in_x[1])/2.0\n    y = (in_y[0] + in_y[1])/2.0\n    w = in_x[1] - in_x[0]\n    h = in_y[1] - in_y[0]\n    x = x*dw\n    w = w*dw\n    y = y*dh\n    h = h*dh\n    return (x, y, w, h)\n\n\ndef convert_annotation(in_dir, out_dir, image_id, out_id, classes):\n    in_file = open(""%s/%s.xml"" % (in_dir, image_id))\n    o_file = open(out_dir + ""/%s.txt"" % out_id, ""w"")\n    tree = ET.parse(in_file)\n    root = tree.getroot()\n    size = root.find(""imagesize"")\n    h = float(size.find(""nrows"").text)\n    w = float(size.find(""ncols"").text)\n\n    for obj in root.iter(""object""):\n        X = []\n        Y = []\n        cls = obj.find(""name"").text\n        if cls not in classes:\n            logging.debug(""%s is not in the selected class"" % cls)\n            continue\n        cls_id = classes.index(cls)\n        for pt in obj.find(""polygon"").findall(""pt""):\n            X.append(float(pt.find(""x"").text))\n            Y.append(float(pt.find(""y"").text))\n        if (len(X) < 2 or len(Y) < 2):\n            logging.warning(""%s doesn\'t have sufficient info, ignore"" % cls)\n            continue\n        X = list(set(X))\n        X.sort()\n        Y = list(set(Y))\n        Y.sort()\n        bb = convert((w, h), X, Y)\n        o_file.write(str(cls_id) + "" "" + "" "".join([str(a) for a in bb]) + ""\\n"")\n\n\ndef find_output_id(out_img_dir, image_id, suffix):\n    counter = 0\n    out_img_path = join(out_img_dir, image_id + ""."" + suffix)\n    out_id = image_id\n    while os.path.exists(out_img_path):\n        logging.info(""%s exists"" % out_img_path)\n        counter += 1\n        out_id = image_id + ""_"" + str(counter)\n        out_img_path = join(out_img_dir, out_id + ""."" + suffix)\n    return out_id, out_img_path\n\n\ndef main():\n\n    parser = argparse.ArgumentParser(\n        description=""Simple tool to make the scene image better.""\n        )\n    parser.add_argument(\n        ""-v"", ""--verbosity"", action=""count"",\n        help=""increase output verbosity""\n        )\n    parser.add_argument(\n        ""-r"", ""--root"", type=str, default=None,\n        help=""Specify the root directory (default: PWD)""\n        )\n    parser.add_argument(\n        ""-n"", ""--name"", type=str, default=""youtube_09"",\n        help=""Specify the name of the original video (default: youtube_09)""\n        )\n    parser.add_argument(\n        ""-u"", ""--user"", type=str, default=""V"",\n        help=""Specify the username (default: V)""\n        )\n    parser.add_argument(\n        ""-o"", ""--outdir"", type=str, default=""test2017"",\n        help=""Output dir in root, must end with 2017 (default: test2017)""\n        )\n    parser.add_argument(\n        ""--classes"", nargs=""+"", type=str, default=[""fighting"", ""dog""],\n        help=""Classes to be trained. Default: [fighting, dog]""\n        )\n    parser.add_argument(\n        ""-d"", ""--delete"", action=""store_true"",\n        help=""Use this option to clean up train.txt""\n        )\n\n    args = parser.parse_args()\n\n    log_level = logging.WARNING\n    if args.verbosity == 1:\n        log_level = logging.INFO\n    elif args.verbosity >= 2:\n        log_level = logging.DEBUG\n    logging.basicConfig(level=log_level,\n                        format=""[xmlTotxt: %(levelname)s] %(message)s"")\n\n    logging.info(args.classes)\n\n    if args.root is None:\n        args.root = os.getcwd()\n    in_dir = join(args.root, ""Annotations"", ""users"",\n                  args.user, args.name)\n    in_img_dir = join(args.root, ""Images"", ""users"",\n                      args.user, args.name)\n    out_dir = join(args.root, args.outdir, ""labels"")\n    out_img_dir = join(args.root, args.outdir, ""JPEGImages"")\n\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n\n    if not os.path.exists(out_img_dir):\n        os.makedirs(out_img_dir)\n\n    foutput = join(args.root, ""train.txt"")\n    if args.delete:\n        output = open(foutput, ""w"")\n    else:\n        output = open(foutput, ""a"")\n\n    for _img_path in os.listdir(in_img_dir):\n        img_path = os.path.join(in_img_dir, _img_path)\n        suffix = os.path.basename(_img_path).split(""."")[1]\n        logging.debug(""Find %s"" % img_path)\n        image_id = os.path.basename(_img_path).split(""."")[0]\n        out_id, out_img_path = find_output_id(out_img_dir, image_id, suffix)\n        convert_annotation(in_dir, out_dir, image_id, out_id, args.classes)\n        logging.info(""Copy %s to %s"" % (img_path, out_img_path))\n        copyfile(img_path, out_img_path)\n\n        output.write(out_img_path + ""\\n"")\n\n\nif __name__ == \'__main__\':\n    main()\n'"
berrynet/client/__init__.py,0,b''
berrynet/client/camera.py,0,"b""#!/usr/bin/python3\n#\n# Copyright 2018 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\nimport argparse\nimport logging\nimport time\n\nfrom datetime import datetime\n\nimport cv2\n\nfrom berrynet import logger\nfrom berrynet.comm import Communicator\nfrom berrynet.comm import payload\n\n\ndef parse_args():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\n        '--mode',\n        default='stream',\n        help='Camera creates frame(s) from stream or file. (default: stream)'\n    )\n    ap.add_argument(\n        '--stream-src',\n        type=str,\n        default='0',\n        help=('Camera stream source. '\n              'It can be device node ID or RTSP URL. '\n              '(default: 0)')\n    )\n    ap.add_argument(\n        '--fps',\n        type=float,\n        default=1,\n        help='Frame per second in streaming mode. (default: 1)'\n    )\n    ap.add_argument(\n        '--filepath',\n        default='',\n        help='Input image path in file mode. (default: empty)'\n    )\n    ap.add_argument(\n        '--broker-ip',\n        default='localhost',\n        help='MQTT broker IP.'\n    )\n    ap.add_argument(\n        '--broker-port',\n        default=1883,\n        type=int,\n        help='MQTT broker port.'\n    )\n    ap.add_argument('--topic',\n        default='berrynet/data/rgbimage',\n        help='The topic to send the captured frames.'\n    )\n    ap.add_argument('--display',\n        action='store_true',\n        help=('Open a window and display the sent out frames. '\n              'This argument is only effective in stream mode.')\n    )\n    ap.add_argument('--hash',\n        action='store_true',\n        help='Add md5sum of a captured frame into the result.'\n    )\n    ap.add_argument('--debug',\n        action='store_true',\n        help='Debug mode toggle'\n    )\n    return vars(ap.parse_args())\n\n\ndef main():\n    args = parse_args()\n    if args['debug']:\n        logger.setLevel(logging.DEBUG)\n    else:\n        logger.setLevel(logging.INFO)\n\n    comm_config = {\n        'subscribe': {},\n        'broker': {\n            'address': args['broker_ip'],\n            'port': args['broker_port']\n        }\n    }\n    comm = Communicator(comm_config, debug=True)\n\n    duration = lambda t: (datetime.now() - t).microseconds / 1000\n\n    if args['mode'] == 'stream':\n        counter = 0\n        fail_counter = 0\n\n        # Check input stream source\n        if args['stream_src'].isdigit():\n            # source is a physically connected camera\n            stream_source = int(args['stream_src'])\n        else:\n            # source is an IP camera\n            stream_source = args['stream_src']\n        capture = cv2.VideoCapture(stream_source)\n        cam_fps = capture.get(cv2.CAP_PROP_FPS)\n        if cam_fps > 30 or cam_fps < 1:\n            logger.warn('Camera FPS is {} (>30 or <1). Set it to 30.'.format(cam_fps))\n            cam_fps = 30\n        out_fps = args['fps']\n        interval = int(cam_fps / out_fps)\n\n        # warmup\n        #t_warmup_start = time.time()\n        #t_warmup_now = time.time()\n        #warmup_counter = 0\n        #while t_warmup_now - t_warmup_start < 1:\n        #    capture.read()\n        #    warmup_counter += 1\n        #    t_warmup_now = time.time()\n\n        logger.debug('===== VideoCapture Information =====')\n        if stream_source.isdigit():\n            stream_source_uri = '/dev/video{}'.format(stream_source)\n        else:\n            stream_source_uri = stream_source\n        logger.debug('Stream Source: {}'.format(stream_source_uri))\n        logger.debug('Camera FPS: {}'.format(cam_fps))\n        logger.debug('Output FPS: {}'.format(out_fps))\n        logger.debug('Interval: {}'.format(interval))\n        logger.debug('Send MQTT Topic: {}'.format(args['topic']))\n        #logger.debug('Warmup Counter: {}'.format(warmup_counter))\n        logger.debug('====================================')\n\n        while True:\n            status, im = capture.read()\n\n            # To verify whether the input source is alive, you should use the\n            # return value of capture.read(). It will not work by capturing\n            # exception of a capture instance, or by checking the return value\n            # of capture.isOpened().\n            #\n            # Two reasons:\n            # 1. If a dead stream is alive again, capture will not notify\n            #    that input source is dead.\n            #\n            # 2. If you check capture.isOpened(), it will keep retruning\n            #    True if a stream is dead afterward. So you can not use\n            #    the capture return value (capture status) to determine\n            #    whether a stream is alive or not.\n            if (status is True):\n                counter += 1\n                if counter == interval:\n                    logger.debug('Drop frames: {}'.format(counter-1))\n                    counter = 0\n\n                    # Open a window and display the ready-to-send frame.\n                    # This is useful for development and debugging.\n                    if args['display']:\n                        cv2.imshow('Frame', im)\n                        cv2.waitKey(1)\n\n                    t = datetime.now()\n                    retval, jpg_bytes = cv2.imencode('.jpg', im)\n                    mqtt_payload = payload.serialize_jpg(jpg_bytes, args['hash'])\n                    comm.send(args['topic'], mqtt_payload)\n                    logger.debug('send: {} ms'.format(duration(t)))\n                else:\n                    pass\n            else:\n                fail_counter += 1\n                logger.critical('ERROR: Failure #{} happened when reading frame'.format(fail_counter))\n\n                # Re-create capture.\n                capture.release()\n                logger.critical('Re-create a capture and reconnect to {} after 5s'.format(stream_source))\n                time.sleep(5)\n                capture = cv2.VideoCapture(stream_source)\n    elif args['mode'] == 'file':\n        # Prepare MQTT payload\n        im = cv2.imread(args['filepath'])\n        retval, jpg_bytes = cv2.imencode('.jpg', im)\n\n        t = datetime.now()\n        mqtt_payload = payload.serialize_jpg(jpg_bytes, args['hash'])\n        logger.debug('payload: {} ms'.format(duration(t)))\n        logger.debug('payload size: {}'.format(len(mqtt_payload)))\n\n        # Client publishes payload\n        t = datetime.now()\n        comm.send(args['topic'], mqtt_payload)\n        logger.debug('mqtt.publish: {} ms'.format(duration(t)))\n        logger.debug('publish at {}'.format(datetime.now().isoformat()))\n    else:\n        logger.error('User assigned unknown mode {}'.format(args['mode']))\n\n\nif __name__ == '__main__':\n    main()\n"""
berrynet/client/dashboard.py,0,"b'# Copyright 2018 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Dashboard agent service.\n""""""\n\nimport argparse\nimport json\n\nfrom os.path import join as pjoin\n\nfrom berrynet import logger\nfrom berrynet.comm import Communicator\nfrom berrynet.comm import payload\n\n\nclass DashboardService(object):\n    def __init__(self, service_name, comm_config):\n        self.service_name = service_name\n        self.comm_config = comm_config\n        self.comm_config[\'subscribe\'][\'berrynet/engine/tensorflow/result\'] = self.update\n        self.comm_config[\'subscribe\'][\'berrynet/engine/mvclassification/result\'] = self.update\n        self.comm = Communicator(self.comm_config, debug=True)\n        self.basedir = \'/usr/local/berrynet/dashboard/www/freeboard\'\n\n    def update(self, pl):\n        payload_json = payload.deserialize_payload(pl.decode(\'utf-8\'))\n        jpg_bytes = payload.destringify_jpg(payload_json[\'bytes\'])\n        inference_result = [\n            \'{0}: {1}<br>\'.format(anno[\'label\'], anno[\'confidence\'])\n            for anno in payload_json[\'annotations\']\n        ]\n        logger.debug(\'inference results: {}\'.format(inference_result))\n\n        with open(pjoin(self.basedir, \'snapshot.jpg\'), \'wb\') as f:\n            f.write(jpg_bytes)\n        self.comm.send(\'berrynet/dashboard/snapshot\', \'snapshot.jpg\')\n        self.comm.send(\'berrynet/dashboard/inferenceResult\',\n                       json.dumps(inference_result))\n\n    def run(self, args):\n        """"""Infinite loop serving inference requests""""""\n        self.comm.run()\n\n\ndef parse_args():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\'--service_name\', required=True,\n                    help=\'Engine service name used as PID filename\')\n    return vars(ap.parse_args())\n\n\ndef main():\n    args = parse_args()\n\n    comm_config = {\n        \'subscribe\': {},\n        \'broker\': {\n            \'address\': \'localhost\',\n            \'port\': 1883\n        }\n    }\n    dashboard_service = DashboardService(args[\'service_name\'],\n                                         comm_config)\n    dashboard_service.run(args)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
berrynet/client/data_collector.py,0,"b'# Copyright 2018 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Data collector service.\n""""""\n\nimport argparse\nimport json\nimport os\n\nfrom datetime import datetime\nfrom os.path import join as pjoin\n\nfrom berrynet import logger\nfrom berrynet.comm import Communicator\nfrom berrynet.comm import payload\n\n\nclass DataCollectorService(object):\n    def __init__(self, comm_config, data_dirpath):\n        self.comm_config = comm_config\n        for topic, functor in self.comm_config[\'subscribe\'].items():\n            self.comm_config[\'subscribe\'][topic] = eval(functor)\n        self.comm_config[\'subscribe\'][\'berrynet/engine/tensorflow/result\'] = self.update\n        self.comm_config[\'subscribe\'][\'berrynet/engine/mvclassification/result\'] = self.update\n        self.comm = Communicator(self.comm_config, debug=True)\n        self.data_dirpath = data_dirpath\n\n    def update(self, pl):\n        if not os.path.exists(self.data_dirpath):\n            try:\n                os.mkdir(self.data_dirpath)\n            except Exception as e:\n                logger.warn(\'Failed to create {}\'.format(self.data_dirpath))\n                raise(e)\n\n        payload_json = payload.deserialize_payload(pl.decode(\'utf-8\'))\n        jpg_bytes = payload.destringify_jpg(payload_json[\'bytes\'])\n        payload_json.pop(\'bytes\')\n        logger.debug(\'inference text result: {}\'.format(payload_json))\n\n        timestamp = datetime.now().isoformat()\n        with open(pjoin(self.data_dirpath, timestamp + \'.jpg\'), \'wb\') as f:\n            f.write(jpg_bytes)\n        with open(pjoin(self.data_dirpath, timestamp + \'.json\'), \'w\') as f:\n            f.write(json.dumps(payload_json, indent=4))\n\n    def save_pipeline_result(self, pl):\n        if not os.path.exists(self.data_dirpath):\n            try:\n                os.mkdir(self.data_dirpath)\n            except Exception as e:\n                logger.warn(\'Failed to create {}\'.format(self.data_dirpath))\n                raise(e)\n\n        payload_json = payload.deserialize_payload(pl.decode(\'utf-8\'))\n        jpg_bytes = payload.destringify_jpg(payload_json[\'image_blob\'])\n        payload_json.pop(\'image_blob\')\n        logger.debug(\'inference text result: {}\'.format(payload_json))\n\n        timestamp = datetime.now().isoformat()\n        with open(pjoin(self.data_dirpath, timestamp + \'.jpg\'), \'wb\') as f:\n            f.write(jpg_bytes)\n        with open(pjoin(self.data_dirpath, timestamp + \'.json\'), \'w\') as f:\n            f.write(json.dumps(payload_json, indent=4))\n\n    def run(self, args):\n        """"""Infinite loop serving inference requests""""""\n        self.comm.run()\n\n\ndef parse_args():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\n        \'--data-dirpath\',\n        default=\'/tmp/berrynet-data\',\n        help=\'Dirpath where to store collected data.\'\n    )\n    ap.add_argument(\n        \'--broker-ip\',\n        default=\'localhost\',\n        help=\'MQTT broker IP.\'\n    )\n    ap.add_argument(\n        \'--broker-port\',\n        default=1883,\n        type=int,\n        help=\'MQTT broker port.\'\n    )\n    ap.add_argument(\n        \'--topic-config\',\n        default=None,\n        help=\'Path of the MQTT topic subscription JSON.\'\n    )\n    return vars(ap.parse_args())\n\n\ndef main():\n    args = parse_args()\n\n    if args[\'topic_config\']:\n        with open(args[\'topic_config\']) as f:\n            topic_config = json.load(f)\n    else:\n        topic_config = {}\n    comm_config = {\n        \'subscribe\': topic_config,\n        \'broker\': {\n            \'address\': args[\'broker_ip\'],\n            \'port\': args[\'broker_port\']\n        }\n    }\n    dc_service = DataCollectorService(comm_config,\n                                      args[\'data_dirpath\'])\n    dc_service.run(args)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
berrynet/client/data_collector_ui.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2018 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Data collector with UI showing inference result for human\n""""""\n\nimport argparse\nimport json\nimport os\nimport sys\nimport threading\nimport tkinter as tk\n\nfrom datetime import datetime\nfrom os.path import join as pjoin\n\nimport cv2\nimport numpy as np\n\nfrom berrynet import logger\nfrom berrynet.comm import Communicator\nfrom berrynet.comm import payload\nfrom PIL import Image\nfrom PIL import ImageTk\n\n\nclass DataCollectorService(object):\n    def __init__(self, comm_config, data_dirpath):\n        self.comm_config = comm_config\n        for topic, functor in self.comm_config[\'subscribe\'].items():\n            self.comm_config[\'subscribe\'][topic] = eval(functor)\n        #self.comm_config[\'subscribe\'][\'berrynet/data/rgbimage\'] = self.update\n        self.comm_config[\'subscribe\'][\'berrynet/engine/pipeline/result\'] = self.save_pipeline_result\n        self.comm = Communicator(self.comm_config, debug=True)\n        self.data_dirpath = data_dirpath\n\n    def update(self, pl):\n        payload_json = payload.deserialize_payload(pl.decode(\'utf-8\'))\n\n        # update UI with the latest inference result\n        self.ui.update(payload_json, \'bytes\')\n\n        if self.data_dirpath:\n            if not os.path.exists(self.data_dirpath):\n                try:\n                    os.mkdir(self.data_dirpath)\n                except Exception as e:\n                    logger.warn(\'Failed to create {}\'.format(self.data_dirpath))\n                    raise(e)\n\n            jpg_bytes = payload.destringify_jpg(payload_json[\'bytes\'])\n            payload_json.pop(\'bytes\')\n            logger.debug(\'inference text result: {}\'.format(payload_json))\n\n            timestamp = datetime.now().isoformat()\n            with open(pjoin(self.data_dirpath, timestamp + \'.jpg\'), \'wb\') as f:\n                f.write(jpg_bytes)\n            with open(pjoin(self.data_dirpath, timestamp + \'.json\'), \'w\') as f:\n                f.write(json.dumps(payload_json, indent=4))\n\n    def save_pipeline_result(self, pl):\n        payload_json = payload.deserialize_payload(pl.decode(\'utf-8\'))\n\n        # update UI with the latest inference result\n        self.ui.update(payload_json, \'image_blob\')\n\n        if self.data_dirpath:\n            if not os.path.exists(self.data_dirpath):\n                try:\n                    os.mkdir(self.data_dirpath)\n                except Exception as e:\n                    logger.warn(\'Failed to create {}\'.format(self.data_dirpath))\n                    raise(e)\n\n            jpg_bytes = payload.destringify_jpg(payload_json[\'image_blob\'])\n            payload_json.pop(\'image_blob\')\n            logger.debug(\'inference text result: {}\'.format(payload_json))\n\n            timestamp = datetime.now().isoformat()\n            with open(pjoin(self.data_dirpath, timestamp + \'.jpg\'), \'wb\') as f:\n                f.write(jpg_bytes)\n            with open(pjoin(self.data_dirpath, timestamp + \'.json\'), \'w\') as f:\n                f.write(json.dumps(payload_json, indent=4))\n\n    def send_snapshot_trigger(self):\n        payload = {}\n        payload[\'timestamp\'] = datetime.now().isoformat()\n        mqtt_payload = json.dumps(payload)\n        self.comm.send(\'berrynet/trigger/controller/snapshot\', mqtt_payload)\n\n    def run(self, args):\n        """"""Infinite loop serving inference requests""""""\n        self.comm.run()\n\n\nclass UI(object):\n    def __init__(self, dc_service, dc_kwargs):\n        # Create data collector attributes\n        self.dc_service = dc_service\n        self.dc_kwargs = dc_kwargs\n        self.dc_service.ui = self\n\n        # Create UI attributes\n        self.window = tk.Tk()\n        self.window.title(\'BerryNet Inference Dashboard\')\n        self.window.protocol(\'WM_DELETE_WINDOW\', self.on_closing)\n        self.canvas_w = dc_kwargs[\'image_width\']\n        self.canvas_h = dc_kwargs[\'image_height\']\n        self.crowd_factor = 3\n\n        # Add label: inference result text\n        self.result = tk.Label(self.window,\n                               text=\'TBD\',\n                               font=(\'Courier New\', 10),\n                               justify=tk.LEFT)\n        #self.result.pack(expand=True, side=tk.LEFT)\n        self.result.grid(row=0, column=0, padx=10)\n        #self.result.columnconfigure(1, weight=2)\n\n        # Add canvas: inference result image\n        #self.canvas = tk.Canvas(self.window, width=1920, height=1080)\n        self.canvas = tk.Canvas(self.window)\n        self.photo = ImageTk.PhotoImage(\n                         image=Image.fromarray(\n                             np.zeros((self.canvas_h, self.canvas_w, 3), dtype=np.uint8)))\n        self.image_id = self.canvas.create_image(\n                            0, 0, image=self.photo, anchor=tk.NW)\n        #self.canvas.pack(side=tk.LEFT)\n        self.canvas.grid(row=0, column=1, rowspan=2, columnspan=4, sticky=\'nesw\')\n\n        # Add button: snapshot trigger\n        self.snapshot_button = tk.Button(self.window,\n                                         text=\'Query\',\n                                         command=self.snapshot)\n        #self.snapshot_button.pack(expand=True)\n        self.snapshot_button.grid(row=1, column=0)\n\n        # Add button and label: threshold controller\n        self.threshold = tk.Label(self.window,\n                               text=self.crowd_factor,\n                               font=(\'Courier New\', 10),\n                               justify=tk.LEFT)\n        self.threshold.grid(row=1, column=1)\n\n        self.snapshot_button = tk.Button(self.window,\n                                         text=\'+\',\n                                         command=self.increase_threshold)\n        self.snapshot_button.grid(row=1, column=2)\n\n        self.snapshot_button = tk.Button(self.window,\n                                         text=\'-\',\n                                         command=self.decrease_threshold)\n        self.snapshot_button.grid(row=1, column=3)\n\n        # Create data collector thread\n        t = threading.Thread(name=\'Data Collector\',\n                             target=self.dc_service.run,\n                             args=(self.dc_kwargs,))\n        t.start()\n\n        # Start the main UI program\n        self.window.mainloop()\n\n    def update(self, data, imgkey=\'bytes\'):\n        \'\'\'\n        Args:\n            data: Inference result loaded from JSON object\n        \'\'\'\n        # Retrieve result image\n        jpg_bytes = payload.destringify_jpg(data[imgkey])\n        img = payload.jpg2rgb(jpg_bytes)\n\n        # Retrieve result text, and update text area\n        data.pop(imgkey)\n        result_text = self.process_output(data)\n        if \'safely\' in result_text:\n            text_color = \'blue\'\n        else:\n            text_color = \'red\'\n        self.result.config(text=result_text, fg=text_color)\n\n        # update image area\n        resized_img = Image.fromarray(img).resize((self.canvas_h, self.canvas_w))\n        self.photo = ImageTk.PhotoImage(image=resized_img)\n        win_w = self.photo.width() + self.result.winfo_width()\n        win_h = self.photo.height() + self.snapshot_button.winfo_height()\n        self.window.geometry(\'{}x{}\'.format(win_w, win_h))\n        self.canvas.itemconfig(self.image_id, image=self.photo)\n\n    def snapshot(self):\n        self.dc_service.send_snapshot_trigger()\n\n    def increase_threshold(self):\n        self.crowd_factor += 1\n        self.threshold.config(text=self.crowd_factor)\n\n    def decrease_threshold(self):\n        self.crowd_factor -= 1\n        self.threshold.config(text=self.crowd_factor)\n\n    def process_output(self, output):\n        \'\'\'\n        Args:\n            output: Inference result, JSON object\n\n        Returns:\n            Stringified JSON data.\n        \'\'\'\n        if \'annotations\' in output.keys():\n            count = 0\n            for obj in output[\'annotations\']:\n                if obj[\'label\'] == \'person\':\n                    count += 1\n                #logger.info(\'label = {}\'.format(k))\n            msg = \'{} persons at the corner\\n\\n\'.format(count)\n            if count > self.crowd_factor:\n                msg += \'Too crowded,\\nsuggest to go straight\'\n            else:\n                msg += \'You can turn right safely\'\n            return msg\n        else:\n            return json.dumps(output, indent=4)\n\n    def on_closing(self):\n        self.dc_service.comm.disconnect()\n        self.window.destroy()\n\n\ndef parse_args():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\n        \'--data-dirpath\',\n        default=None,\n        help=\'Dirpath where to store collected data.\'\n    )\n    ap.add_argument(\n        \'--broker-ip\',\n        default=\'localhost\',\n        help=\'MQTT broker IP.\'\n    )\n    ap.add_argument(\n        \'--broker-port\',\n        default=1883,\n        type=int,\n        help=\'MQTT broker port.\'\n    )\n    ap.add_argument(\n        \'--topic-config\',\n        default=None,\n        help=\'Path of the MQTT topic subscription JSON.\'\n    )\n    ap.add_argument(\n        \'--image-width\',\n        type=int,\n        default=300,\n        help=\'Image display width in pixel.\'\n    )\n    ap.add_argument(\n        \'--image-height\',\n        type=int,\n        default=300,\n        help=\'Image display height in pixel.\'\n    )\n    return vars(ap.parse_args())\n\n\ndef main():\n    args = parse_args()\n\n    if args[\'topic_config\']:\n        with open(args[\'topic_config\']) as f:\n            topic_config = json.load(f)\n    else:\n        topic_config = {}\n    comm_config = {\n        \'subscribe\': topic_config,\n        \'broker\': {\n            \'address\': args[\'broker_ip\'],\n            \'port\': args[\'broker_port\']\n        }\n    }\n    dc_service = DataCollectorService(comm_config,\n                                      args[\'data_dirpath\'])\n    UI(dc_service, args)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
berrynet/client/fbdashboard.py,0,"b'# Copyright 2018 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Framebuffer dashboard.\n""""""\n\nimport argparse\nimport json\nimport logging\nimport os\nimport random\nimport sys\nimport time\n\nfrom datetime import datetime\nfrom os.path import join as pjoin\n\nimport cv2\n\nfrom berrynet import logger\nfrom berrynet.comm import Communicator\nfrom berrynet.comm import payload\nfrom OpenGL.GL import *\nfrom OpenGL.GLU import *\nfrom OpenGL.GLUT import *\n\n\nclass FBDashboardService(object):\n    def __init__(self, comm_config, data_dirpath=None, no_decoration=False,\n                 debug=False, save_frame=False):\n        self.comm_config = comm_config\n        for topic, functor in self.comm_config[\'subscribe\'].items():\n            self.comm_config[\'subscribe\'][topic] = eval(functor)\n        self.comm = Communicator(self.comm_config, debug=True)\n        self.data_dirpath = data_dirpath\n        self.no_decoration = no_decoration\n        self.frame = None\n        self.debug = debug\n        self.save_frame = save_frame\n\n    def update(self, pl):\n        payload_json = payload.deserialize_payload(pl.decode(\'utf-8\'))\n        if \'bytes\' in payload_json.keys():\n            img_k = \'bytes\'\n        elif \'image_blob\' in payload_json.keys():\n            img_k = \'image_blob\'\n        else:\n            raise Exception(\'No image data in MQTT payload\')\n        jpg_bytes = payload.destringify_jpg(payload_json[img_k])\n        payload_json.pop(img_k)\n        logger.debug(\'inference text result: {}\'.format(payload_json))\n\n        img = payload.jpg2rgb(jpg_bytes)\n\n        if self.no_decoration:\n            self.frame = img\n        else:\n            try:\n                res = payload_json[\'annotations\']\n            except KeyError:\n                res = [\n                    {\n                        \'label\': \'hello\',\n                        \'confidence\': 0.42,\n                        \'left\': random.randint(50, 60),\n                        \'top\': random.randint(50, 60),\n                        \'right\': random.randint(300, 400),\n                        \'bottom\': random.randint(300, 400)\n                    }\n                ]\n            self.frame = overlay_on_image(img, res)\n\n        # Save frames for analysis or debugging\n        if self.debug and self.save_frame:\n            if not os.path.exists(self.data_dirpath):\n                try:\n                    os.mkdir(self.data_dirpath)\n                except Exception as e:\n                    logger.warn(\'Failed to create {}\'.format(self.data_dirpath))\n                    raise(e)\n\n            timestamp = datetime.now().isoformat()\n            with open(pjoin(self.data_dirpath, timestamp + \'.jpg\'), \'wb\') as f:\n                f.write(jpg_bytes)\n            with open(pjoin(self.data_dirpath, timestamp + \'.json\'), \'w\') as f:\n                f.write(json.dumps(payload_json, indent=4))\n\n    def update_fb(self):\n        if self.frame is not None:\n            gl_draw_fbimage(self.frame)\n\n    def run(self, args):\n        """"""Infinite loop serving inference requests""""""\n        self.comm.start_nb()\n\n\ndef gl_draw_fbimage(rgbimg):\n    h, w = rgbimg.shape[:2]\n\n    glTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, w, h, 0, GL_RGB, GL_UNSIGNED_BYTE, rgbimg)\n    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)\n    glColor3f(1.0, 1.0, 1.0)\n    glEnable(GL_TEXTURE_2D)\n    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR)\n    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR)\n    glBegin(GL_QUADS)\n    glTexCoord2d(0.0, 1.0)\n    glVertex3d(-1.0, -1.0,  0.0)\n    glTexCoord2d(1.0, 1.0)\n    glVertex3d( 1.0, -1.0,  0.0)\n    glTexCoord2d(1.0, 0.0)\n    glVertex3d( 1.0,  1.0,  0.0)\n    glTexCoord2d(0.0, 0.0)\n    glVertex3d(-1.0,  1.0,  0.0)\n    glEnd()\n    glFlush()\n    glutSwapBuffers()\n\n\ndef init():\n    glClearColor(0.7, 0.7, 0.7, 0.7)\n\n\ndef idle():\n    glutPostRedisplay()\n\n\ndef keyboard(key, x, y):\n    key = key.decode(\'utf-8\')\n    if key == \'q\':\n        print(""\\n\\nFinished\\n\\n"")\n        sys.exit()\n\n\ndef opencv_frame(src, w=None, h=None, fps=30):\n    vidcap = cv2.VideoCapture(src)\n    if not vidcap.isOpened():\n        print(\'opened failed\')\n        sys.exit(errno.ENOENT)\n\n    # set frame w/h if indicated\n    if w and h:\n        vidcap.set(cv2.CAP_PROP_FRAME_WIDTH, w)\n        vidcap.set(cv2.CAP_PROP_FRAME_HEIGHT, h)\n\n    # set FPS\n    rate = int(vidcap.get(cv2.CAP_PROP_FPS))\n    if rate > fps or rate < 1:\n        print(\'Illegal data rate {} (1-30)\'.format(rate))\n        rate = fps\n    print(\'fps: {}\'.format(rate))\n\n    # frame generator\n    while True:\n        success, image = vidcap.read()\n        if not success:\n            print(\'Failed to read frame\')\n            break\n        yield image\n\n\n#Vcap = opencv_frame(0, w=320, h=240)\nVcap = opencv_frame(0)\n\n\ndef draw_box(image, annotations):\n    """"""Draw information of annotations onto image.\n\n    Args:\n        image: Image nparray.\n        annotations: List of detected object information.\n\n    Returns: Image nparray containing object information on it.\n    """"""\n    print(\'draw_box, annotations: {}\'.format(annotations))\n    img = image.copy()\n\n    for anno in annotations:\n        # draw bounding box\n        box_color = (0, 0, 255)\n        box_thickness = 1\n        cv2.rectangle(img,\n                      (int(anno[\'left\']), int(anno[\'top\'])),\n                      (int(anno[\'right\']), int(anno[\'bottom\'])),\n                      box_color,\n                      box_thickness)\n\n        # draw label\n        label_background_color = box_color\n        label_text_color = (255, 255, 255)\n        if \'track_id\' in anno.keys():\n            label = \'ID:{} {}\'.format(anno[\'track_id\'], anno[\'label\'])\n        else:\n            label = anno[\'label\']\n        label_text = \'{} ({} %)\'.format(label,\n                                        int(anno[\'confidence\'] * 100))\n        label_size = cv2.getTextSize(label_text,\n                                     cv2.FONT_HERSHEY_SIMPLEX,\n                                     0.5,\n                                     1)[0]\n        label_left = anno[\'left\']\n        label_top = anno[\'top\'] - label_size[1]\n        if (label_top < 1):\n            label_top = 1\n        label_right = label_left + label_size[0]\n        label_bottom = label_top + label_size[1]\n        cv2.rectangle(img,\n                      (int(label_left - 1), int(label_top - 1)),\n                      (int(label_right + 1), int(label_bottom + 1)),\n                      label_background_color,\n                      -1)\n        cv2.putText(img,\n                    label_text,\n                    (int(label_left), int(label_bottom)),\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    0.5,\n                    label_text_color,\n                    1)\n    return img\n\n\ndef overlay_on_image(display_image, object_info):\n    """"""Modulized version of overlay_on_image function\n    """"""\n    if isinstance(object_info, type(None)):\n        print(\'WARNING: object info is None\')\n        return display_image\n\n    return draw_box(display_image, object_info)\n\n\ndef parse_args():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\n        \'--data-dirpath\',\n        default=\'/tmp/berrynet-data\',\n        help=\'Dirpath where to store collected data.\'\n    )\n    ap.add_argument(\n        \'--broker-ip\',\n        default=\'localhost\',\n        help=\'MQTT broker IP.\'\n    )\n    ap.add_argument(\n        \'--broker-port\',\n        default=1883,\n        type=int,\n        help=\'MQTT broker port.\'\n    )\n    ap.add_argument(\n        \'--topic\',\n        nargs=\'*\',\n        default=[\'berrynet/engine/tflitedetector/result\'],\n        help=\'The topic to listen, and can be indicated multiple times.\'\n    )\n    ap.add_argument(\n        \'--topic-action\',\n        default=\'self.update\',\n        help=\'The action for the indicated topics.\'\n    )\n    ap.add_argument(\n        \'--topic-config\',\n        default=None,\n        help=\'Path of the MQTT topic subscription JSON.\'\n    )\n    ap.add_argument(\n        \'--no-decoration\',\n        action=\'store_true\',\n        help=\'Display image in payload without applying result information.\'\n    )\n    ap.add_argument(\n        \'--no-full-screen\',\n        action=\'store_true\',\n        help=\'Display fbdashboard in a window.\'\n    )\n    ap.add_argument(\'--debug\',\n        action=\'store_true\',\n        help=\'Debug mode toggle\'\n    )\n    ap.add_argument(\'--debug-save-frame\',\n        action=\'store_true\',\n        help=\'Save frames for debugging. --debug also needs to be set.\'\n    )\n    return vars(ap.parse_args())\n\n\ndef main():\n    args = parse_args()\n    if args[\'debug\']:\n        logger.setLevel(logging.DEBUG)\n    else:\n        logger.setLevel(logging.INFO)\n\n    # Topics and actions can come from two sources: CLI and config file.\n    # Setup topic_config by parsing values from the two sources.\n    if args[\'topic_config\']:\n        with open(args[\'topic_config\']) as f:\n            topic_config = json.load(f)\n    else:\n        topic_config = {}\n    topic_config.update({t:args[\'topic_action\'] for t in args[\'topic\']})\n\n    comm_config = {\n        \'subscribe\': topic_config,\n        \'broker\': {\n            \'address\': args[\'broker_ip\'],\n            \'port\': args[\'broker_port\']\n        }\n    }\n    fbd_service = FBDashboardService(comm_config,\n                                     args[\'data_dirpath\'],\n                                     args[\'no_decoration\'],\n                                     args[\'debug\'],\n                                     args[\'debug_save_frame\'])\n    fbd_service.run(args)\n\n    glutInitWindowPosition(0, 0)\n    glutInit(sys.argv)\n    glutInitDisplayMode(GLUT_RGBA | GLUT_DOUBLE)\n    glutCreateWindow(""BerryNet Result Dashboard, q to quit"")\n    glutDisplayFunc(fbd_service.update_fb)\n    glutKeyboardFunc(keyboard)\n    init()\n    glutIdleFunc(idle)\n    if args[\'no_full_screen\']:\n        pass\n    else:\n        glutFullScreen()\n    glutMainLoop()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
berrynet/client/gmail.py,0,"b'# Copyright 2019 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n# Reference\n# https://www.geeksforgeeks.org/send-mail-attachment-gmail-account-using-python/\n\n""""""Gmail client sends an email with inference result.\n\nThe email will contain two attachments: image and text.\n""""""\n\nimport argparse\nimport json\nimport logging\nimport os\nimport smtplib\n\nfrom datetime import datetime\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\nfrom email.mime.base import MIMEBase\nfrom email import encoders\n\nfrom os.path import join as pjoin\n\nfrom berrynet import logger\nfrom berrynet.comm import Communicator\nfrom berrynet.comm import payload\n\n\ndef create_mime_attachment(filepath):\n    filename = os.path.basename(filepath)\n    attachment = open(filepath, ""rb"")\n\n    # instance of MIMEBase and named as p\n    p = MIMEBase(\'application\', \'octet-stream\')\n    # To change the payload into encoded form\n    p.set_payload(attachment.read())\n    # encode into base64\n    encoders.encode_base64(p)\n    p.add_header(\'Content-Disposition\',\n                 ""attachment; filename= %s"" % filename)\n    return p\n\n\ndef send_email_text(sender_address,\n                    sender_password,\n                    receiver_address,\n                    body=\'\',\n                    subject=\'BerryNet mail client notification\',\n                    attachments=None):\n    # instance of MIMEMultipart\n    msg = MIMEMultipart()\n\n    msg[\'From\'] = sender_address\n    msg[\'To\'] = receiver_address\n    msg[\'Subject\'] = subject\n    logger.debug(\'Sender: {}\'.format(msg[\'From\']))\n    logger.debug(\'Receiver: {}\'.format(msg[\'To\']))\n    logger.debug(\'Subject: {}\'.format(msg[\'Subject\']))\n\n    # attach the body with the msg instance\n    msg.attach(MIMEText(body, \'plain\'))\n\n    for fpath in attachments:\n        logger.debug(\'Attachment: {}\'.format(fpath))\n        msg.attach(create_mime_attachment(fpath))\n\n    # creates SMTP session\n    s = smtplib.SMTP(\'smtp.gmail.com\', 587)\n    # start TLS for security\n    s.starttls()\n    # Authentication\n    s.login(sender_address, sender_password)\n    # Converts the Multipart msg into a string\n    text = msg.as_string()\n    # sending the mail\n    s.sendmail(sender_address, receiver_address, text)\n    # terminating the session\n    s.quit()\n\n\nclass GmailService(object):\n    def __init__(self, comm_config):\n        self.comm_config = comm_config\n        for topic, functor in self.comm_config[\'subscribe\'].items():\n            self.comm_config[\'subscribe\'][topic] = eval(functor)\n        self.comm = Communicator(self.comm_config, debug=True)\n        self.email = comm_config[\'email\']\n        self.pipeline_compatible = comm_config[\'pipeline_compatible\']\n        self.target_label = comm_config[\'target_label\']\n\n    def find_target_label(self, target_label, generalized_result):\n        label_list = [i[\'label\'] for i in generalized_result[\'annotations\']]\n        logger.debug(\'Result labels: {}\'.format(label_list))\n        return target_label in label_list\n\n    def update(self, pl):\n        payload_json = payload.deserialize_payload(pl.decode(\'utf-8\'))\n        if self.pipeline_compatible:\n            b64img_key = \'image_blob\'\n        else:\n            b64img_key = \'bytes\'\n        jpg_bytes = payload.destringify_jpg(payload_json[b64img_key])\n        payload_json.pop(b64img_key)\n        logger.debug(\'inference text result: {}\'.format(payload_json))\n\n        match_target_label = self.find_target_label(self.target_label,\n                                                    payload_json)\n\n        logger.debug(\'Find target label {0}: {1}\'.format(\n            self.target_label, match_target_label))\n\n        if match_target_label:\n            timestamp = datetime.now().isoformat()\n            notification_image = pjoin(\'/tmp\', timestamp + \'.jpg\')\n            notification_text = pjoin(\'/tmp\', timestamp + \'.json\')\n            with open(notification_image, \'wb\') as f:\n                f.write(jpg_bytes)\n            with open(notification_text, \'w\') as f:\n                f.write(json.dumps(payload_json, indent=4))\n\n            try:\n                send_email_text(\n                    self.email[\'sender_address\'],\n                    self.email[\'sender_password\'],\n                    self.email[\'receiver_address\'],\n                    body=(\'Target label {} is found. \'\n                          \'Please check the attachments.\'\n                          \'\'.format(self.target_label)),\n                    subject=\'BerryNet mail client notification\',\n                    attachments=set([notification_image, notification_text]))\n            except Exception as e:\n                logger.warn(e)\n\n            os.remove(notification_image)\n            os.remove(notification_text)\n        else:\n            # target label is not in generalized result, do nothing\n            pass\n\n    def run(self, args):\n        """"""Infinite loop serving inference requests""""""\n        self.comm.run()\n\n\ndef parse_args():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\n        \'--sender-address\',\n        required=True,\n        help=\'Email address of sender. Ex: foo@email.org\'\n    )\n    ap.add_argument(\n        \'--sender-password\',\n        required=True,\n        help=\'Password of sender email address.\'\n    )\n    ap.add_argument(\n        \'--receiver-address\',\n        required=True,\n        help=\'Email address of receiver. Ex: bar@email.org\'\n    )\n    ap.add_argument(\n        \'--target-label\',\n        required=True,\n        help=\'Send notification email if the label is in inference result.\'\n    )\n    ap.add_argument(\n        \'--broker-ip\',\n        default=\'localhost\',\n        help=\'MQTT broker IP.\'\n    )\n    ap.add_argument(\n        \'--broker-port\',\n        default=1883,\n        type=int,\n        help=\'MQTT broker port.\'\n    )\n    ap.add_argument(\n        \'--topic\',\n        nargs=\'*\',\n        default=[\'berrynet/engine/tflitedetector/result\'],\n        help=\'The topic to listen, and can be indicated multiple times.\'\n    )\n    ap.add_argument(\n        \'--topic-action\',\n        default=\'self.update\',\n        help=\'The action for the indicated topics.\'\n    )\n    ap.add_argument(\n        \'--topic-config\',\n        default=None,\n        help=\'Path of the MQTT topic subscription JSON.\'\n    )\n    ap.add_argument(\n        \'--pipeline-compatible\',\n        action=\'store_true\',\n        help=(\n            \'Change key of b64 image string in generalized result \'\n            \'from bytes to image_blob. \'\n            \'Note: This is an experimental parameter.\'\n        )\n    )\n    ap.add_argument(\'--debug\',\n        action=\'store_true\',\n        help=\'Debug mode toggle\'\n    )\n    return vars(ap.parse_args())\n\n\ndef main():\n    args = parse_args()\n    if args[\'debug\']:\n        logger.setLevel(logging.DEBUG)\n    else:\n        logger.setLevel(logging.INFO)\n\n    # Topics and actions can come from two sources: CLI and config file.\n    # Setup topic_config by parsing values from the two sources.\n    if args[\'topic_config\']:\n        with open(args[\'topic_config\']) as f:\n            topic_config = json.load(f)\n    else:\n        topic_config = {}\n    topic_config.update({t:args[\'topic_action\'] for t in args[\'topic\']})\n\n    comm_config = {\n        \'subscribe\': topic_config,\n        \'broker\': {\n            \'address\': args[\'broker_ip\'],\n            \'port\': args[\'broker_port\']\n        },\n        \'email\': {\n            \'sender_address\': args[\'sender_address\'],\n            \'sender_password\': args[\'sender_password\'],\n            \'receiver_address\': args[\'receiver_address\']\n        },\n        \'pipeline_compatible\': args[\'pipeline_compatible\'],\n        \'target_label\': args[\'target_label\']\n    }\n    dc_service = GmailService(comm_config)\n    dc_service.run(args)\n\n\nif __name__ == \'__main__\':\n    main()\n\n\n'"
berrynet/client/snapshot.py,0,"b'# Copyright 2018 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""\nSnapshot service will listen to a trigger event (MQTT topic),\nand send a snapshot retrieved from camera.\n""""""\n\nimport argparse\nimport json\n\nfrom datetime import datetime\n\nimport cv2\n\nfrom berrynet import logger\nfrom berrynet.comm import Communicator\nfrom berrynet.comm import payload\n\n\nclass SnapshotService(object):\n    def __init__(self, comm_config):\n        self.comm_config = comm_config\n        for topic, functor in self.comm_config[\'subscribe\'].items():\n            self.comm_config[\'subscribe\'][topic] = eval(functor)\n        self.comm_config[\'subscribe\'][\'berrynet/trigger/controller/snapshot\'] = self.snapshot\n        self.comm = Communicator(self.comm_config, debug=True)\n\n    def snapshot(self, pl):\n        \'\'\'Send camera snapshot.\n\n        The functionality is the same as using camera client in file mode.\n\n        The difference is that snapshot client retrieves image from camera\n        instead of given filepath.\n        \'\'\'\n        duration = lambda t: (datetime.now() - t).microseconds / 1000\n\n        # WORKAROUND: Prevent VideoCapture from buffering frames.\n        #     VideoCapture will buffer frames automatically, and we need\n        #     to find a way to disable it.\n        self.capture = cv2.VideoCapture(0)\n        status, im = self.capture.read()\n        if (status is False):\n            logger.warn(\'ERROR: Failure happened when reading frame\')\n\n        t = datetime.now()\n        retval, jpg_bytes = cv2.imencode(\'.jpg\', im)\n        mqtt_payload = payload.serialize_jpg(jpg_bytes)\n        self.comm.send(\'berrynet/data/rgbimage\', mqtt_payload)\n        logger.debug(\'send: {} ms\'.format(duration(t)))\n        self.capture.release()\n\n    def run(self, args):\n        """"""Infinite loop serving inference requests""""""\n        self.comm.run()\n\n\ndef parse_args():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\n        \'--broker-ip\',\n        default=\'localhost\',\n        help=\'MQTT broker IP.\'\n    )\n    ap.add_argument(\n        \'--broker-port\',\n        default=1883,\n        type=int,\n        help=\'MQTT broker port.\'\n    )\n    ap.add_argument(\n        \'--topic-config\',\n        default=None,\n        help=\'Path of the MQTT topic subscription JSON.\'\n    )\n    return vars(ap.parse_args())\n\n\ndef main():\n    args = parse_args()\n\n    if args[\'topic_config\']:\n        with open(args[\'topic_config\']) as f:\n            topic_config = json.load(f)\n    else:\n        topic_config = {}\n    comm_config = {\n        \'subscribe\': topic_config,\n        \'broker\': {\n            \'address\': args[\'broker_ip\'],\n            \'port\': args[\'broker_port\']\n        }\n    }\n    dc_service = SnapshotService(comm_config)\n    dc_service.run(args)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
berrynet/client/telegram_bot.py,0,"b'#!/usr/bin/env python3\n#\n# Copyright 2019 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\nimport argparse\nimport json\nimport io\nimport logging\nimport os\nimport tempfile\nimport tarfile\nimport time\n\nimport telegram.ext\nfrom berrynet import logger\nfrom berrynet.comm import Communicator\nfrom berrynet.comm import payload\n\n\nclass TelegramBotService(object):\n    def __init__(self, comm_config, token, target_label=\'\', debug=False):\n        self.comm_config = comm_config\n        for topic, functor in self.comm_config[\'subscribe\'].items():\n            self.comm_config[\'subscribe\'][topic] = eval(functor)\n        # NOTE: Maybe change the hard-coding topic to parameter in the future.\n        self.comm_config[\'subscribe\'][\'berrynet/data/rgbimage\'] = self.single_shot\n        self.comm = Communicator(self.comm_config, debug=True)\n        if os.path.isfile(token):\n            self.token = self.get_token_from_config(token)\n        else:\n            self.token = token\n        self.target_label = target_label\n        self.debug = debug\n\n        # Telegram Updater employs Telegram Dispatcher which dispatches\n        # updates to its registered handlers.\n        self.updater = telegram.ext.Updater(self.token,\n                                            use_context=True)\n        self.cameraHandlers = []\n\n        self.shot = False\n        self.single_shot_chat_id = None\n\n    def get_token_from_config(self, config):\n        with open(config) as f:\n            cfg = json.load(f)\n        return cfg[\'token\']\n\n    def match_target_label(self, target_label, bn_result):\n        labels = [r[\'label\'] for r in bn_result[\'annotations\']]\n        if target_label in labels:\n            logger.debug(\'Find {0} in inference result {1}\'.format(target_label, labels))\n            return True\n        else:\n            logger.debug(\'Not find {0} in inference result {1}\'.format(target_label, labels))\n            return False\n\n    def update(self, pl):\n        try:\n            payload_json = payload.deserialize_payload(pl.decode(\'utf-8\'))\n            jpg_bytes = payload.destringify_jpg(payload_json[""bytes""])\n            jpg_file_descriptor = io.BytesIO(jpg_bytes)\n\n            for u in self.cameraHandlers:\n                if self.updater is None:\n                    continue\n\n                if self.target_label == \'\':\n                    if len(payload_json[\'annotations\']) > 0:\n                        logger.debug(""Send photo to %s"" % u)\n                        self.updater.bot.send_photo(chat_id = u, photo=jpg_file_descriptor)\n                    else:\n                        logger.debug(""Does not detect any object, no action"")\n                elif self.match_target_label(self.target_label, payload_json):\n                    logger.info(""Send notification photo with result to %s"" % u)\n                    self.updater.bot.send_photo(chat_id = u, photo=jpg_file_descriptor)\n                else:\n                    pass\n        except Exception as e:\n            logger.info(e)\n\n    def single_shot(self, pl):\n        """"""Capture an image from camera client and send to the client.\n        """"""\n        if self.shot is True:\n            try:\n                payload_json = payload.deserialize_payload(pl.decode(\'utf-8\'))\n                # WORKAROUND: Support customized camera client.\n                #\n                # Original camera client sends an `obj` in payload,\n                # Customized camera client sends an `[obj]` in payload.\n                #\n                # We are unifying the rules. Before that, checking the type\n                # as workaround.\n                if type(payload_json) is list:\n                    logger.debug(\'WORDAROUND: receive and unpack [obj]\')\n                    payload_json = payload_json[0]\n                jpg_bytes = payload.destringify_jpg(payload_json[""bytes""])\n                jpg_file_descriptor = io.BytesIO(jpg_bytes)\n\n                logger.info(\'Send single shot\')\n                self.updater.bot.send_photo(chat_id=self.single_shot_chat_id,\n                                            photo=jpg_file_descriptor)\n            except Exception as e:\n                logger.info(e)\n\n            self.shot = False\n        else:\n            logger.debug(\'Single shot is disabled, do nothing.\')\n\n    def run(self, args):\n        """"""Infinite loop serving inference requests""""""\n        self.comm.start_nb()\n        self.connect_telegram(args)\n\n    def connect_telegram(self, args):\n        try:\n            self.updater.dispatcher.add_handler(\n                telegram.ext.CommandHandler(\'help\', self.handler_help))\n            self.updater.dispatcher.add_handler(\n                telegram.ext.CommandHandler(\'hi\', self.handler_hi))\n            self.updater.dispatcher.add_handler(\n                telegram.ext.CommandHandler(\'camera\', self.handler_camera))\n            self.updater.dispatcher.add_handler(\n                telegram.ext.CommandHandler(\'stop\', self.handler_stop))\n            self.updater.dispatcher.add_handler(\n                telegram.ext.CommandHandler(\'shot\', self.handler_shot))\n            if (args[""has_getlog""]):\n                self.updater.dispatcher.add_handler(\n                    telegram.ext.CommandHandler(\'getlog\', self.handler_getlog))\n            self.updater.start_polling()\n        except Exception as e:\n            logger.critical(e)\n\n    def handler_help(self, update, context):\n        logger.info(""Received command `help`"")\n        update.message.reply_text((\n            \'I support these commands:\\n\\n\'\n            \'help - Display help message.\\n\'\n            \'hi - Test Telegram client.\\n\'\n            \'camera - Start camera.\\n\'\n            \'stop - Stop camera.\\n\'\n            \'shot - Take a shot from camera.\'))\n\n    def handler_hi(self, update, context):\n        logger.info(""Received command `hi`"")\n        update.message.reply_text(\n            \'Hi, {}\'.format(update.message.from_user.first_name))\n\n    def handler_camera(self, update, context):\n        logger.info(""Received command `camera`, chat id: %s"" % update.message.chat_id)\n        # Register the chat-id for receiving images\n        if (update.message.chat_id not in self.cameraHandlers):\n            self.cameraHandlers.append (update.message.chat_id)\n        update.message.reply_text(\'Dear, I am ready to help send notification\')\n\n    def handler_stop(self, update, context):\n        logger.info(""Received command `stop`, chat id: %s"" % update.message.chat_id)\n        # Register the chat-id for receiving images\n        while (update.message.chat_id in self.cameraHandlers):\n            self.cameraHandlers.remove (update.message.chat_id)\n        update.message.reply_text(\'Bye\')\n\n    def handler_shot(self, update, context):\n        logger.info(""Received command `shot`, chat id: %s"" % update.message.chat_id)\n        # Register the chat-id for receiving images\n        self.shot = True\n        self.single_shot_chat_id = update.message.chat_id\n        logger.debug(\'Enable single shot.\')\n\n    def handler_getlog(self, update, context):\n        logger.info(""Received command `getlog`, chat id: %s"" % update.message.chat_id)\n        # Create temporary tar.xz file\n        tmpTGZ1 = tempfile.NamedTemporaryFile(suffix="".tar.xz"")\n        tmpTGZ = tarfile.open(fileobj=tmpTGZ1, mode=""w:xz"")\n        tmpTGZPath = tmpTGZ1.name\n\n        # Traverse /var/log\n        varlogDir = os.path.abspath(os.path.join(os.sep, ""var"", ""log""))\n        for root, dirs, files in os.walk(varlogDir):\n            for file in files:\n                fullPath = os.path.join(root, file)\n                # Check if the file is a regular file\n                if not os.path.isfile(fullPath):\n                    continue\n                # Check if the file is accessable\n                if not os.access(fullPath, os.R_OK):\n                    continue\n                # Pack the file\n                tmpTGZ.add(name = fullPath, recursive=False)\n        tmpTGZ.close()\n        self.updater.bot.send_document(chat_id = update.message.chat_id, document = open(tmpTGZPath, \'rb\'), filename=time.strftime(\'berrynet-varlog_%Y%m%d_%H%M%S.tar.xz\'))\n\ndef parse_args():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\n        \'--token\',\n        help=(\'Telegram token got from BotFather, \'\n              \'or filepath of a JSON config file with token.\')\n    )\n    ap.add_argument(\n        \'--target-label\',\n        default=\'\',\n        help=\'Send a notification if the target label is in the result.\'\n    )\n    ap.add_argument(\n        \'--broker-ip\',\n        default=\'localhost\',\n        help=\'MQTT broker IP.\'\n    )\n    ap.add_argument(\n        \'--broker-port\',\n        default=1883,\n        type=int,\n        help=\'MQTT broker port.\'\n    )\n    ap.add_argument(\n        \'--topic\',\n        nargs=\'*\',\n        default=[\'berrynet/engine/tflitedetector/result\'],\n        help=\'The topic to listen, and can be indicated multiple times.\'\n    )\n    ap.add_argument(\n        \'--topic-action\',\n        default=\'self.update\',\n        help=\'The action for the indicated topics.\'\n    )\n    ap.add_argument(\n        \'--topic-config\',\n        default=None,\n        help=\'Path of the MQTT topic subscription JSON.\'\n    )\n    ap.add_argument(\'--debug\',\n        action=\'store_true\',\n        help=\'Debug mode toggle\'\n    )\n    ap.add_argument(\'--has-getlog\',\n        action=\'store_true\',\n        help=\'Enable getlog command\'\n    )\n    return vars(ap.parse_args())\n\n\ndef main():\n    args = parse_args()\n    if args[\'debug\']:\n        logger.setLevel(logging.DEBUG)\n    else:\n        logger.setLevel(logging.INFO)\n\n    # Topics and actions can come from two sources: CLI and config file.\n    # Setup topic_config by parsing values from the two sources.\n    if args[\'topic_config\']:\n        with open(args[\'topic_config\']) as f:\n            topic_config = json.load(f)\n    else:\n        topic_config = {}\n    topic_config.update({t:args[\'topic_action\'] for t in args[\'topic\']})\n\n    comm_config = {\n        \'subscribe\': topic_config,\n        \'broker\': {\n            \'address\': args[\'broker_ip\'],\n            \'port\': args[\'broker_port\']\n        }\n    }\n    telbot_service = TelegramBotService(comm_config,\n                                        args[\'token\'],\n                                        args[\'target_label\'],\n                                        args[\'debug\'])\n    telbot_service.run(args)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
berrynet/comm/__init__.py,0,"b'#!/usr/bin/python3\n\nimport paho.mqtt.client as mqtt\nimport paho.mqtt.publish as publish\n\nfrom berrynet import logger\nfrom logzero import setup_logger\n\n\ndef on_connect(client, userdata, flags, rc):\n    logger.debug(\'Connected with result code \' + str(rc))\n    for topic in client.comm_config[\'subscribe\'].keys():\n        logger.debug(\'Subscribe topic {}\'.format(topic))\n        client.subscribe(topic)\n\n\ndef on_message(client, userdata, msg):\n    """"""Dispatch received message to its bound functor.\n    """"""\n    logger.debug(\'Receive message from topic {}\'.format(msg.topic))\n    #logger.debug(\'Message payload {}\'.format(msg.payload))\n    client.comm_config[\'subscribe\'][msg.topic](msg.payload)\n\n\nclass Communicator(object):\n    def __init__(self, comm_config, debug=False):\n        self.client = mqtt.Client()\n        self.client.comm_config = comm_config\n        self.client.on_connect = on_connect\n        self.client.on_message = on_message\n\n    def run(self):\n        self.client.connect(\n            self.client.comm_config[\'broker\'][\'address\'],\n            self.client.comm_config[\'broker\'][\'port\'],\n            60)\n        self.client.loop_forever()\n\n    def start_nb(self):\n        self.client.connect(\n            self.client.comm_config[\'broker\'][\'address\'],\n            self.client.comm_config[\'broker\'][\'port\'],\n            60)\n        self.client.loop_start()\n\n    def stop_nb(self):\n        self.client.loop_stop()\n\n    def send(self, topic, payload):\n        logger.debug(\'Send message to topic {}\'.format(topic))\n        #logger.debug(\'Message payload {}\'.format(payload))\n        publish.single(topic, payload,\n                       hostname=self.client.comm_config[\'broker\'][\'address\'])\n\n    def disconnect(self):\n        self.client.disconnect()\n'"
berrynet/comm/payload.py,0,"b'# Copyright 2018 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\nimport base64\nimport hashlib\nimport json\n\nfrom datetime import datetime\n\nimport cv2\nimport numpy as np\n\n\ndef stringify_jpg(jpg_bytes):\n    return base64.b64encode(jpg_bytes).decode(\'utf-8\')\n\n\ndef destringify_jpg(stringified_jpg):\n    """"""\n    :return: JPEG bytes\n    :rtype: bytes\n    """"""\n    return base64.b64decode(stringified_jpg.encode(\'utf-8\'))\n\n\ndef jpg2bgr(jpg_bytes):\n    """"""\n    :return: BGR bytes\n    :rtype: numpy array\n    """"""\n    array = np.frombuffer(jpg_bytes, dtype=np.uint8)\n    return cv2.imdecode(array, flags=1)\n\n\ndef jpg2rgb(jpg_bytes):\n    """"""\n    :return: RGB bytes\n    :rtype: numpy array\n    """"""\n    return cv2.cvtColor(jpg2bgr(jpg_bytes), cv2.COLOR_BGR2RGB)\n\n\ndef bgr2rgb(bgr_nparray):\n    """"""Convert image nparray from BGR to RGB.\n\n    Args:\n        bgr_nparray: Image nparray in BGR color model.\n\n    Returns:\n        Image nparray in RGB color model.\n    """"""\n    return cv2.cvtColor(bgr_nparray, cv2.COLOR_BGR2RGB)\n\n\ndef rgb2bgr(rgb_nparray):\n    """"""Convert image nparray from RGB to BGR.\n\n    Args:\n        rgb_nparray: Image nparray in RGB color model.\n\n    Returns:\n        Image nparray in BGR color model.\n    """"""\n    return cv2.cvtColor(rgb_nparray, cv2.COLOR_RGB2BGR)\n\n\ndef generate_bytes_md5sum(content_bytes):\n    content_b64 = base64.b64encode(content_bytes)\n    return hashlib.md5(content_b64).hexdigest()\n\n\ndef serialize_payload(json_object):\n    return json.dumps(json_object)\n\n\ndef serialize_jpg(jpg_bytes, md5sum=False):\n    """"""Create Serialized JSON object consisting of image bytes and meta\n\n    :param imarray: JPEG bytes\n    :type imarray: bytes\n    :return: serialized image JSON\n    :rtype: string\n    """"""\n    obj = {}\n    obj[\'timestamp\'] = datetime.now().isoformat()\n    obj[\'bytes\'] = stringify_jpg(jpg_bytes)\n    if md5sum:\n        obj[\'md5sum\'] = generate_bytes_md5sum(jpg_bytes)\n    return json.dumps(obj)\n\n\ndef deserialize_payload(payload):\n    return json.loads(payload)\n\n\n#def deserialize_jpg(jpg_json):\n#    """"""Deserialized JSON object created by josnify_image.\n#\n#    :param string :\n#    :return:\n#    :rtype:\n#    """"""\n#    return json.loads(jpg_json)\n\n\nif __name__ == \'__main__\':\n    im = cv2.imread(\'/home/debug/codes/darknet/data/dog.jpg\')\n    retval, jpg_bytes = cv2.imencode(\'.jpg\', im)\n\n    # size of stringified dog.jpg is 1.33x larger than original\n    s_jpg = serialize_jpg(jpg_bytes)\n    d_jpg = deserialize_payload(s_jpg)\n    # TODO: Can we write JPEG bytes into file directly to prevent\n    #       bytes -> numpy array -> decode RGB -> write encoded JPEG\n    cv2.imwrite(\'/tmp/dog.jpg\', jpg2bgr(destringify_jpg(d_jpg[\'bytes\'])))\n'"
berrynet/engine/__init__.py,0,"b'# Copyright 2018 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""\nDeep learning engine template provides unified interfaces for\ndifferent backends (e.g. TensorFlow, Caffe2, etc.)\n""""""\n\nclass DLEngine(object):\n    def __init__(self):\n        self.model_input_cache = []\n        self.model_output_cache = []\n        self.cache = {\n            \'model_input\': [],\n            \'model_output\': \'\',\n            \'model_output_filepath\': \'\'\n        }\n\n    def create(self):\n        # Workaround to posepone TensorFlow initialization.\n        # If TF is initialized in __init__, and pass an engine instance\n        # to engine service, TF session will stuck in run().\n        pass\n\n    def process_input(self, tensor):\n        return tensor\n\n    def inference(self, tensor):\n        output = None\n        return output\n\n    def process_output(self, output):\n        return output\n\n    def cache_data(self, key, value):\n        self.cache[key] = value\n\n    def save_cache(self):\n        with open(self.cache[\'model_output_filepath\'], \'w\') as f:\n            f.write(str(self.cache[\'model_output\']))\n'"
berrynet/engine/caffe_engine.py,0,"b'# Copyright 2017 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""TensorFlow inference engine.\n""""""\n\nfrom __future__ import print_function\n\nimport argparse\nimport json\n\nimport numpy as np\nimport caffe\n\nfrom berrynet import logger\n#from berrynet.dlmodelmgr import DLModelManager\nfrom berrynet.engine import DLEngine\n\n\nclass CaffeEngine(DLEngine):\n    # FIXME: Get model information by model manager\n    def __init__(self, model_def, pretrained_model, mean_file, label, image_dims = [256,256], channel_swap=[2,1,0], raw_scale=255.0, top_k=5):\n        super(CaffeEngine, self).__init__()\n\n        # Load model\n        caffe.set_mode_cpu()\n        self.classifier = caffe.Classifier(model_def, pretrained_model, image_dims=image_dims, mean=mean_file, raw_scale=raw_scale, channel_swap=channel_swap)\n        \n        # Load labels\n        self.labels = [line.rstrip() for line in open(label)]\n\n        self.top_k = top_k\n\n    def create(self):\n        pass\n\n    def process_input(self, rgb_array):\n        self.inputs = rgb_array\n        return self.inputs\n    \n    def inference(self, tensor):\n        self.predictions = self.classifier.predict(self.inputs, False)\n        return self.predictions\n\n    def process_output(self, output):\n        predictions_list = self.predictions[0].tolist()\n        data = zip(predictions_list, caffe_labels)\n        processed_output = {\'annotations\': []}\n        i=0\n        for d in sorted(data, reverse=True):\n            human_string = d[1]\n            score = d[0]\n            anno = {\n                \'type\': \'classification\',\n                \'label\': human_string,\n                \'confidence\': score\n            }\n            processed_output[\'annotations\'].append(anno)\n            i = i + 1\n            if (i >= self.top_k):\n                break\n        return processed_output\n\n    def save_cache(self):\n        pass\n'"
berrynet/engine/darknet_engine.py,0,"b'# Copyright 2018 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Darknet inference engine.\n""""""\n\nfrom __future__ import print_function\n\nimport argparse\nimport json\nimport math\nimport time\n\nimport cv2\nimport numpy as np\n\nfrom berrynet import logger\n#from berrynet.dlmodelmgr import DLModelManager\nfrom berrynet.engine import DLEngine\nfrom ctypes import *\n\n\nclass BOX(Structure):\n    _fields_ = [(""x"", c_float),\n                (""y"", c_float),\n                (""w"", c_float),\n                (""h"", c_float)]\n\nclass IMAGE(Structure):\n    _fields_ = [(""w"", c_int),\n                (""h"", c_int),\n                (""c"", c_int),\n                (""data"", POINTER(c_float))]\n\nclass METADATA(Structure):\n    _fields_ = [(""classes"", c_int),\n                (""names"", POINTER(c_char_p))]\n\n\nlib = CDLL(""/usr/lib/libdarknet.so"", RTLD_GLOBAL)\nlib.network_width.argtypes = [c_void_p]\nlib.network_width.restype = c_int\nlib.network_height.argtypes = [c_void_p]\nlib.network_height.restype = c_int\n\npredict = lib.network_predict\npredict.argtypes = [c_void_p, POINTER(c_float)]\npredict.restype = POINTER(c_float)\n\nmake_image = lib.make_image\nmake_image.argtypes = [c_int, c_int, c_int]\nmake_image.restype = IMAGE\n\nmake_boxes = lib.make_boxes\nmake_boxes.argtypes = [c_void_p]\nmake_boxes.restype = POINTER(BOX)\n\nfree_ptrs = lib.free_ptrs\nfree_ptrs.argtypes = [POINTER(c_void_p), c_int]\n\nnum_boxes = lib.num_boxes\nnum_boxes.argtypes = [c_void_p]\nnum_boxes.restype = c_int\n\nmake_probs = lib.make_probs\nmake_probs.argtypes = [c_void_p]\nmake_probs.restype = POINTER(POINTER(c_float))\n\nreset_rnn = lib.reset_rnn\nreset_rnn.argtypes = [c_void_p]\n\nload_net = lib.load_network\nload_net.argtypes = [c_char_p, c_char_p, c_int]\nload_net.restype = c_void_p\n\nfree_image = lib.free_image\nfree_image.argtypes = [IMAGE]\n\nletterbox_image = lib.letterbox_image\nletterbox_image.argtypes = [IMAGE, c_int, c_int]\nletterbox_image.restype = IMAGE\n\nload_meta = lib.get_metadata\nlib.get_metadata.argtypes = [c_char_p]\nlib.get_metadata.restype = METADATA\n\nload_image = lib.load_image_color\nload_image.argtypes = [c_char_p, c_int, c_int]\nload_image.restype = IMAGE\n\nrgbgr_image = lib.rgbgr_image\nrgbgr_image.argtypes = [IMAGE]\n\npredict_image = lib.network_predict_image\npredict_image.argtypes = [c_void_p, IMAGE]\npredict_image.restype = POINTER(c_float)\n\nnetwork_detect = lib.network_detect\nnetwork_detect.argtypes = [c_void_p, IMAGE, c_float, c_float, c_float, POINTER(BOX), POINTER(POINTER(c_float))]\n\n\ndef c_array(ctype, values):\n    arr = (ctype*len(values))()\n    arr[:] = values\n    return arr\n\n\ndef nparray_to_image(arr):\n    """"""Convert nparray to Darknet image struct.\n    Args:\n        arr: nparray containing source image in BGR color model.\n\n    Returns:\n        Darknet image struct, whose data is a C array\n        containing flatten image in BGR color model.\n    """"""\n    arr = arr.transpose(2,0,1)\n    c = arr.shape[0]\n    h = arr.shape[1]\n    w = arr.shape[2]\n    arr = (arr/255.0).flatten()\n    data = c_array(c_float, arr)\n    im = IMAGE(w, h, c, data)\n    rgbgr_image(im)\n    return im\n\n\ndef detect_np(net, meta, np_img, thresh=.3, hier_thresh=.5, nms=.45):\n    im = nparray_to_image(np_img)\n    boxes = make_boxes(net)\n    probs = make_probs(net)\n    num = num_boxes(net)\n    t_start = time.time()\n    network_detect(net, im, thresh, hier_thresh, nms, boxes, probs)\n    t_end = time.time()\n    logger.debug(\'inference time: {} s\'.format(t_end - t_start))\n    res = []\n    for j in range(num):\n        for i in range(meta.classes):\n            if probs[j][i] > 0:\n                res.append(\n                    {\n                        \'type\': \'detection\',\n                        \'label\': meta.names[i].decode(\'utf-8\'),\n                        \'confidence\': probs[j][i],\n                        \'left\': boxes[j].x - (boxes[j].w / 2),\n                        \'top\': boxes[j].y - (boxes[j].h / 2),\n                        \'right\': boxes[j].x + (boxes[j].w / 2),\n                        \'bottom\': boxes[j].y + (boxes[j].h / 2),\n                        \'id\': -1\n                    }\n                )\n    free_ptrs(cast(probs, POINTER(c_void_p)), num)\n    return res\n\n\nclass DarknetEngine(DLEngine):\n    # FIXME: Get model information by model manager\n    def __init__(self, config, model, meta=\'\'):\n        super(DarknetEngine, self).__init__()\n\n        self.net = load_net(config, model, 0)\n        self.meta = load_meta(meta)\n        self.classes = self.meta.classes\n        self.labels = [self.meta.names[i].decode(\'utf-8\')\n                       for i in range(self.classes)]\n\n        # Warmup\n        zero_image = np.zeros(shape=(416, 416, 3), dtype=np.uint8)\n        detect_np(self.net, self.meta, zero_image)\n\n    def process_input(self, rgb_array):\n        return rgb_array\n\n    def inference(self, tensor):\n        return detect_np(self.net, self.meta, tensor)\n\n    def process_output(self, output):\n        return {\'annotations\': output}\n\n\nif __name__ == \'__main__\':\n    engine = DarknetEngine(\n        config=b\'/usr/share/dlmodels/tinyyolovoc-20170816/tiny-yolo-voc.cfg\',\n        model=b\'/usr/share/dlmodels/tinyyolovoc-20170816/tiny-yolo-voc.weights\',\n        meta=b\'/usr/share/dlmodels/tinyyolovoc-20170816/voc.data\'\n    )\n    im = cv2.imread(\'data/dog.jpg\')\n    for i in range(3):\n        r = engine.inference(im)\n        print(r)\n'"
berrynet/engine/movidius.py,0,"b'#!/usr/bin/python\n#\n# Copyright 2017 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\nimport os\n\nimport cv2\nimport numpy as np\n\nfrom mvnc import mvncapi as mvnc\n\n\nclass MovidiusNeuralGraph(object):\n    def __init__(self, graph_filepath, label_filepath):\n        # mvnc.SetGlobalOption(mvnc.GlobalOption.LOGLEVEL, 2)\n        devices = mvnc.EnumerateDevices()\n        if len(devices) == 0:\n            raise Exception(\'No devices found\')\n        self.device = mvnc.Device(devices[0])\n        self.device.OpenDevice()\n\n        # Load graph\n        with open(graph_filepath, mode=\'rb\') as f:\n            graphfile = f.read()\n        self.graph = self.device.AllocateGraph(graphfile)\n\n        # Load labels\n        self.labels = []\n        with open(label_filepath, \'r\') as f:\n            for line in f:\n                label = line.split(\'\\n\')[0]\n                if label != \'classes\':\n                    self.labels.append(label)\n            f.close()\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.graph.DeallocateGraph()\n        self.device.CloseDevice()\n\n    def inference(self, data):\n        self.graph.LoadTensor(data.astype(np.float16), \'user object\')\n        output, userobj = self.graph.GetResult()\n        return output\n\n    def get_graph(self):\n        return self.graph\n\n    def get_labels(self):\n        return self.labels\n\n\ndef process_inceptionv3_input(img):\n    image_size = 299\n    mean = 128\n    std = 1.0/128\n\n    dx, dy, dz = img.shape\n    delta = float(abs(dy - dx))\n    if dx > dy:  # crop the x dimension\n        img = img[int(0.5*delta):dx-int(0.5*delta), 0:dy]\n    else:\n        img = img[0:dx, int(0.5*delta):dy-int(0.5*delta)]\n    img = cv2.resize(img, (image_size, image_size))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    for i in range(3):\n        img[:, :, i] = (img[:, :, i] - mean) * std\n    return img\n\n\ndef process_inceptionv3_output(output, labels):\n    processed_output = {\'annotations\': []}\n    decimal_digits = 2\n    top_k = 5\n    top_inds = output.argsort()[::-1][:top_k]\n    for i in range(top_k):\n        human_string = labels[top_inds[i]]\n        score = round(float(output[top_inds[i]]), decimal_digits)\n        anno = {\n            \'type\': \'classification\',\n            \'label\': human_string,\n            \'confidence\': score\n        }\n        processed_output[\'annotations\'].append(anno)\n    return processed_output\n    #return [(labels[top_inds[i]], output[top_inds[i]]) for i in range(5)]\n\n\ndef print_inceptionv3_output(output, labels):\n    top_inds = output.argsort()[::-1][:5]\n\n    for i in range(5):\n        print(top_inds[i], labels[top_inds[i]], output[top_inds[i]])\n\n\ndef process_mobilenetssd_input(bgr_nparray):\n    """"""Normalize MobileNet SSD input image.\n\n    Args:\n        img: Image nparray in BGR color model.\n\n    Returns:\n        Pre-processed image nparray in BGR color model.\n    """"""\n    img = cv2.resize(bgr_nparray, (300, 300))\n    img = img - 127.5\n    img = img * 0.007843\n    return img\n\n\ndef process_mobilenetssd_output(output, img_w, img_h, labels, threshold=0.1):\n    """"""\n    More details about inference result format:\n    https://github.com/movidius/ncappzoo/blob/master/caffe/SSD_MobileNet/run.py\n\n    Args:\n        output: Inference result returned by Graph.GetResult().\n        img_w: Width of input image.\n        img_h: Height of input image.\n        labels:\n        threshold:\n\n    Returns:\n        Annotations as dictionary, key is ""annotations"" and\n        value a list of parsed results.\n\n        Example:\n\n            \'annotations\': [\n                {\n                    ""label"": ""car"",\n                    ""confidence"": 0.93,\n                    ""left"": 100,\n                    ""top"": 100,\n                    ""right"": 200,\n                    ""bottom"": 200\n                },\n                ...\n            ]\n    """"""\n    boxnum_index = 0\n    result_index = 7\n    result_size = 7\n    num_valid_boxes = int(output[boxnum_index])\n    annotations = []\n\n    for i in range(num_valid_boxes):\n        base_index = result_index + result_size * i\n        result_objinfo = output[base_index:(base_index + result_size)]\n\n        anno = {}\n        anno[\'label\'] = labels[int(result_objinfo[1])]\n        anno[\'confidence\'] = float(result_objinfo[2])\n        anno[\'left\'] = int(result_objinfo[3] * img_w)\n        anno[\'top\'] = int(result_objinfo[4] * img_h)\n        anno[\'right\'] = int(result_objinfo[5] * img_w)\n        anno[\'bottom\'] = int(result_objinfo[6] * img_h)\n\n        if anno[\'confidence\'] >= threshold:\n            annotations.append(anno)\n\n    return {\'annotations\': annotations}\n\n\nif __name__ == \'__main__\':\n    graph_filepath = \'\'  # model filepath\n    label_filepath = \'\'  # label filepath\n    path_to_images = \'\'  # image dirpath\n    image_filenames = [os.path.join(path_to_images, image_name)\n                       for image_name in []]  # image filename list\n\n    movidius = MovidiusNeuralGraph(graph_filepath, label_filepath)\n    labels = movidius.get_labels()\n\n    print(\'\'.join([\'*\' for i in range(79)]))\n    print(\'inception-v3 on NCS\')\n    for image_filename in image_filenames:\n        img = cv2.imread(image_filename).astype(np.float32)\n        img = process_inceptionv3_input(img)\n        print(\'\'.join([\'*\' for i in range(79)]))\n        print(\'Start download to NCS...\')\n        output = movidius.inference(img)\n        print_inceptionv3_output(output, labels)\n\n    print(\'\'.join([\'*\' for i in range(79)]))\n    print(\'Finished\')\n'"
berrynet/engine/movidius_engine.py,0,"b'# Copyright 2018 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Movidius classification inference engine.\n""""""\n\nfrom __future__ import print_function\n\nfrom berrynet.engine import DLEngine\nfrom berrynet.engine import movidius as mv\n\n\nclass MovidiusEngine(DLEngine):\n    def __init__(self, model, label):\n        super(MovidiusEngine, self).__init__()\n        self.mvng = mv.MovidiusNeuralGraph(model, label)\n\n    def process_input(self, tensor):\n        return mv.process_inceptionv3_input(tensor)\n\n    def inference(self, tensor):\n        return self.mvng.inference(tensor)\n\n    def process_output(self, output):\n        return mv.process_inceptionv3_output(\n                   output,\n                   self.mvng.get_labels())\n\n    def save_cache(self):\n        with open(self.cache[\'model_output_filepath\'], \'w\') as f:\n            for i in self.cache[\'model_output\']:\n                print(""%s (score = %.5f)"" % (i[0], i[1]), file=f)\n\n\nclass MovidiusMobileNetSSDEngine(DLEngine):\n    def __init__(self, model, label):\n        super(MovidiusMobileNetSSDEngine, self).__init__()\n        self.mvng = mv.MovidiusNeuralGraph(model, label)\n        self.labels = self.mvng.get_labels()\n        self.classes = len(self.labels)\n\n    def process_input(self, tensor):\n        self.img_w = tensor.shape[1]\n        self.img_h = tensor.shape[0]\n        return mv.process_mobilenetssd_input(tensor)\n\n    def inference(self, tensor):\n        return self.mvng.inference(tensor)\n\n    def process_output(self, output):\n        return mv.process_mobilenetssd_output(\n                   output,\n                   self.img_w,\n                   self.img_h,\n                   self.mvng.get_labels())\n'"
berrynet/engine/openvino_engine.py,0,"b'# Copyright 2018 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""OpenVINO classification inference engine.\n""""""\n\nimport logging\nimport os\nimport sys\n\nfrom argparse import ArgumentParser\nfrom time import time\n\nfrom berrynet.engine import DLEngine\nimport cv2\nimport numpy as np\n\nfrom berrynet import logger\nfrom openvino.inference_engine import IENetwork, IEPlugin\n\n\nclass OpenVINOClassifierEngine(DLEngine):\n    def __init__(self, model, labels=None, top_k=3, device=\'CPU\'):\n        """"""\n        Args:\n            model: Path to an .xml file with a trained model.\n\n            device: Specify the target device to infer on; CPU, GPU, FPGA\n                    or MYRIAD is acceptable. Sample will look for a suitable\n                    plugin for device specified (CPU by default)\n\n            labels: Labels mapping file\n\n            top_k: Number of top results\n        """"""\n        super(OpenVINOClassifierEngine, self).__init__()\n\n        model_bin = model\n        model_xml = os.path.splitext(model_bin)[0] + "".xml""\n        if labels:\n            with open(labels, \'r\') as f:\n                # Allow label name with spaces. To use onlyh the 1st word,\n                # uncomment another labels_map implementation below.\n                self.labels_map = [l.strip() for l in f.readlines()]\n                #self.labels_map = [x.split(sep=\' \', maxsplit=1)[-1].strip()\n                #                   for x in f]\n        else:\n            self.labels_map = None\n        self.top_k = top_k\n\n        # Plugin initialization for specified device and\n        # load extensions library if specified\n        #\n        # Note: MKLDNN CPU-targeted custom layer support is not included\n        #       because we do not use it yet.\n        self.plugin = IEPlugin(device=device, plugin_dirs=None)\n\n        # Read IR\n        logger.debug(\'Loading network files:\'\n                     \'\\n\\txml: {0}\\n\\tbin: {1}\'.format(model_xml, model_bin))\n        net = IENetwork.from_ir(model=model_xml, weights=model_bin)\n\n        if self.plugin.device == ""CPU"":\n            supported_layers = self.plugin.get_supported_layers(net)\n            not_supported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n            if len(not_supported_layers) != 0:\n                logger.error(""Following layers are not supported by the plugin for specified device {}:\\n {}"".format(self.plugin.device, \', \'.join(not_supported_layers)))\n                sys.exit(1)\n\n        assert len(net.inputs.keys()) == 1, ""Sample supports only single input topologies""\n        assert len(net.outputs) == 1, ""Sample supports only single output topologies""\n\n        # input_blob and and out_blob are the layer names in string format.\n        logger.debug(""Preparing input blobs"")\n        self.input_blob = next(iter(net.inputs))\n        self.out_blob = next(iter(net.outputs))\n        net.batch_size = 1\n\n        self.n, self.c, self.h, self.w = net.inputs[self.input_blob].shape\n\n        # Loading model to the plugin\n        logger.debug(""Loading model to the plugin"")\n        self.exec_net = self.plugin.load(network=net)\n\n        del net\n\n    def __delete__(self, instance):\n        del self.exec_net\n        del self.plugin\n\n    def process_input(self, tensor):\n        """"""Resize tensor (if needed) and change layout from HWC to CHW.\n\n        Args:\n            tensor: Input BGR tensor (OpenCV convention)\n\n        Returns:\n            Resized and transposed tensor\n        """"""\n        if tensor.shape[:-1] != (self.h, self.w):\n            logger.warning(""Input tensor is resized from {} to {}"".format(\n                tensor.shape[:-1], (self.h, self.w)))\n            tensor = cv2.resize(tensor, (self.w, self.h))\n        tensor = tensor.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n        return tensor\n\n    def inference(self, tensor):\n        logger.debug(""Starting inference"")\n        res = self.exec_net.infer(inputs={self.input_blob: tensor})\n        return res[self.out_blob]\n\n    def process_output(self, output):\n        logger.debug(""Processing output blob"")\n        logger.debug(""Top {} results: "".format(self.top_k))\n\n        annotations = []\n        for i, probs in enumerate(output):\n            probs = np.squeeze(probs)\n            top_ind = np.argsort(probs)[-self.top_k:][::-1]\n            for id in top_ind:\n                det_label = self.labels_map[id] if self.labels_map else ""#{}"".format(id)\n                logger.debug(""\\t{:.7f} label {}"".format(probs[id], det_label))\n\n                annotations.append({\n                    \'type\': \'classification\',\n                    \'label\': det_label,\n                    \'confidence\': float(probs[id])\n                })\n        return {\'annotations\': annotations}\n\n\nclass OpenVINODetectorEngine(DLEngine):\n    def __init__(self, model, labels=None, threshold=0.3, device=\'CPU\'):\n        super(OpenVINODetectorEngine, self).__init__()\n\n        # Prepare model and labels\n        model_bin = model\n        model_xml = os.path.splitext(model_bin)[0] + "".xml""\n        if labels:\n            with open(labels, \'r\') as f:\n                self.labels_map = [x.strip() for x in f]\n        else:\n            self.labels_map = None\n        self.threshold = threshold\n\n        # Plugin initialization for specified device and\n        # load extensions library if specified\n        #\n        # Note: MKLDNN CPU-targeted custom layer support is not included\n        #       because we do not use it yet.\n        if device == \'CPU\':\n            # Since OpenVINO 2020.1, cpu_extension content has been merged\n            # into MKLDNN plugin and it is used automatically.\n            #\n            # We do not need to call these functions below anymore:\n            #     self.plugin.add_cpu_extension(plugin_dirs + \'/libcpu_extension_avx2.so\')\n            #     self.plugin.add_cpu_extension(plugin_dirs + \'/libcpu_extension_avx512.so\')\n            #     self.plugin.add_cpu_extension(plugin_dirs + \'/libcpu_extension_sse4.so\')\n            plugin_dirs = \'/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64\'\n            self.plugin = IEPlugin(device=device, plugin_dirs=plugin_dirs)\n            logger.debug(\'plugin dirs: {}\'.format(plugin_dirs))\n        else:\n            plugin_dirs = None\n            self.plugin = IEPlugin(device=device, plugin_dirs=plugin_dirs)\n\n        # Read IR\n        logger.debug(\'Loading network files:\'\n                     \'\\n\\txml: {0}\\n\\tbin: {1}\'.format(model_xml, model_bin))\n        net = IENetwork(model=model_xml, weights=model_bin)\n\n        if self.plugin.device == ""CPU"":\n            supported_layers = self.plugin.get_supported_layers(net)\n            not_supported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n            if len(not_supported_layers) != 0:\n                logger.error(""Following layers are not supported by the plugin for specified device {}:\\n {}"".\n                          format(self.plugin.device, \', \'.join(not_supported_layers)))\n                logger.error(""Please try to specify cpu extensions library path in demo\'s command line parameters using -l ""\n                          ""or --cpu_extension command line argument"")\n                sys.exit(1)\n\n        assert len(net.inputs.keys()) == 1, ""Demo supports only single input topologies""\n        assert len(net.outputs) == 1, ""Demo supports only single output topologies""\n\n        # input_blob and and out_blob are the layer names in string format.\n        logger.debug(""Preparing input blobs"")\n        self.input_blob = next(iter(net.inputs))\n        self.out_blob = next(iter(net.outputs))\n\n        self.n, self.c, self.h, self.w = net.inputs[self.input_blob].shape\n\n        # Loading model to the plugin\n        self.exec_net = self.plugin.load(network=net, num_requests=2)\n\n        del net\n\n        # Initialize engine mode: sync or async\n        #\n        # FIXME: async mode does not work currently.\n        #        process_input needs to provide two input tensors for async.\n        self.is_async_mode = False\n        self.cur_request_id = 0\n        self.next_request_id = 1\n\n    def __delete__(self, instance):\n        del self.exec_net\n        del self.plugin\n\n    def process_input(self, tensor, next_tensor=None):\n        frame = tensor\n        next_frame = next_tensor\n\n        # original input shape will be used in process_output\n        self.img_w = tensor.shape[1]\n        self.img_h = tensor.shape[0]\n\n        # Main sync point:\n        # in the truly Async mode we start the NEXT infer request, while waiting for the CURRENT to complete\n        # in the regular mode we start the CURRENT request and immediately wait for it\'s completion\n        if self.is_async_mode:\n            in_frame = cv2.resize(next_frame, (self.w, self.h))\n            in_frame = in_frame.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n            in_frame = in_frame.reshape((self.n, self.c, self.h, self.w))\n        else:\n            in_frame = cv2.resize(frame, (self.w, self.h))\n            in_frame = in_frame.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n            in_frame = in_frame.reshape((self.n, self.c, self.h, self.w))\n        return in_frame\n\n    def inference(self, tensor):\n        inf_start = time()\n        if self.is_async_mode:\n            self.exec_net.start_async(request_id=self.next_request_id, inputs={self.input_blob: tensor})\n        else:\n            self.exec_net.start_async(request_id=self.cur_request_id, inputs={self.input_blob: tensor})\n\n        if self.exec_net.requests[self.cur_request_id].wait(-1) == 0:\n            inf_end = time()\n            det_time = inf_end - inf_start\n            if self.is_async_mode:\n                logger.debug(\'Inference time: N\\A for async mode\')\n            else:\n                logger.debug(""Inference time: {:.3f} ms"".format(det_time * 1000))\n\n            # Parse detection results of the current request\n            res = self.exec_net.requests[self.cur_request_id].outputs[self.out_blob]\n        else:\n            res = None\n\n        return res\n\n        # FIXME: async mode does not work currently.\n        #        process_input needs to provide two input tensors for async.\n        if self.is_async_mode:\n            self.cur_request_id, self.next_request_id = self.next_request_id, self.cur_request_id\n            frame = next_frame\n\n    def process_output(self, output):\n        logger.debug(""Processing output blob"")\n        logger.debug(""Threshold: {}"".format(self.threshold))\n\n        annotations = []\n        for obj in output[0][0]:\n            # Collect objects when probability more than specified threshold\n            if obj[2] > self.threshold:\n                xmin = int(obj[3] * self.img_w)\n                ymin = int(obj[4] * self.img_h)\n                xmax = int(obj[5] * self.img_w)\n                ymax = int(obj[6] * self.img_h)\n                class_id = int(obj[1])\n                det_label = self.labels_map[class_id] if self.labels_map else str(class_id)\n                annotations.append({\n                    \'label\': det_label,\n                    \'confidence\': float(obj[2]),\n                    \'left\': xmin,\n                    \'top\': ymin,\n                    \'right\': xmax,\n                    \'bottom\': ymax\n                })\n        return {\'annotations\': annotations}\n\n\ndef get_distribution_info():\n    """"""Get Debuan or Ubuntu distribution information.\n    """"""\n    info = {}\n    with open(\'/etc/lsb-release\') as f:\n        info_l = [i.strip().split(\'=\') for i in f.readlines()]\n    for i in info_l:\n        info[i[0]] = i[1]\n    logger.debug(\'Distribution info: {}\'.format(info))\n    return info\n\n\ndef set_openvino_environment():\n    """"""The same effect as executing <openvino>/bin/setupvars.sh.\n    """"""\n    dist_info = get_distribution_info()\n    python_version = 3.5\n\n    os.environ[\'INSTALLDIR\'] = \'/opt/intel/computer_vision_sdk_2018.4.420\'\n    os.environ[\'INTEL_CVSDK_DIR\'] = os.environ[\'INSTALLDIR\']\n    os.environ[\'LD_LIBRARY_PATH\'] = (\n        \'{installdir}/deployment_tools/model_optimizer/bin:\'\n        \'{ld_library_path}\').format(\n            installdir = os.environ.get(\'INSTALLDIR\'),\n            ld_library_path = os.environ.get(\'LD_LIBRARY_PATH\' or \'\')\n    )\n    os.environ[\'InferenceEngine_DIR\'] = os.path.join(\n        os.environ.get(\'INTEL_CVSDK_DIR\'),\n        \'deployment_tools/inference_engine/share\'\n    )\n    os.environ[\'IE_PLUGINS_PATH\'] = os.path.join(\n        os.environ.get(\'INTEL_CVSDK_DIR\'),\n        \'deployment_tools/inference_engine/lib/ubuntu_{}.04/intel64\'.format(\n            dist_info[\'DISTRIB_RELEASE\'])\n    )\n    os.environ[\'LD_LIBRARY_PATH\'] = (\n        \'/opt/intel/opencl:\'\n        \'{installdir}/deployment_tools/inference_engine/external/gna/lib:\'\n        \'{installdir}/deployment_tools/inference_engine/external/mkltiny_lnx/lib:\'\n        \'{installdir}/deployment_tools/inference_engine/external/omp/lib:\'\n        \'{ie_plugins_path}:\'\n        \'{ld_library_path}\').format(\n            installdir = os.environ.get(\'INSTALLDIR\'),\n            ie_plugins_path = os.environ.get(\'IE_PLUGINS_PATH\'),\n            ld_library_path = os.environ.get(\'LD_LIBRARY_PATH\' or \'\')\n    )\n    os.environ[\'PATH\'] = (\n        \'{intel_cvsdk_dir}/deployment_tools/model_optimizer:\'\n        \'{path}\').format(\n            intel_cvsdk_dir = os.environ.get(\'INTEL_CVSDK_DIR\'),\n            path = os.environ.get(\'PATH\'),\n    )\n    os.environ[\'PYTHONPATH\'] = (\n        \'{intel_cvsdk_dir}/deployment_tools/model_optimizer:\'\n        \'{pythonpath}\').format(\n            intel_cvsdk_dir = os.environ.get(\'INTEL_CVSDK_DIR\'),\n            pythonpath = os.environ.get(\'PYTHONPATH\' or \'\')\n    )\n    os.environ[\'PYTHONPATH\'] = (\n        \'{intel_cvsdk_dir}/python/python$python_version:\'\n        \'{intel_cvsdk_dir}/python/python$python_version/ubuntu16:\'\n        \'{pythonpath}\').format(\n            intel_cvsdk_dir = os.environ.get(\'INTEL_CVSDK_DIR\'),\n            pythonpath = os.environ.get(\'PYTHONPATH\' or \'\')\n    )\n\n\ndef parse_argsr():\n    parser = ArgumentParser()\n    parser.add_argument(\n        ""-e"", ""--engine"",\n        help=(""Classifier or Detector engine. ""\n              ""classifier, or detector is acceptable. ""\n              ""(classifier by default)""),\n        default=""classifier"",\n        type=str)\n    parser.add_argument(\n        ""-m"", ""--model"",\n        help=""Path to an .xml file with a trained model."",\n        required=True,\n        type=str)\n    parser.add_argument(\n        ""-l"", ""--labels"",\n        help=""Labels mapping file"",\n        default=None,\n        type=str)\n    parser.add_argument(\n        ""--top_k"",\n        help=""Number of top results"",\n        default=10,\n        type=int)\n    parser.add_argument(\n        ""-d"", ""--device"",\n        help=""Specify the target device to infer on; CPU, GPU, FPGA or MYRIAD is acceptable. Sample will look for a suitable plugin for device specified (CPU by default)"",\n        default=""CPU"",\n        type=str)\n    parser.add_argument(\n        ""-i"", ""--input"",\n        help=""Path to a folder with images or path to an image files"",\n        required=True,\n        type=str)\n    parser.add_argument(\n        ""--debug"",\n        help=""Debug mode toggle"",\n        default=False,\n        action=""store_true"")\n\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_argsr()\n\n    if args.debug:\n        logger.setLevel(logging.DEBUG)\n    else:\n        logger.setLevel(logging.INFO)\n\n    if args.engine == \'classifier\':\n        engine = OpenVINOClassifierEngine(\n                     model = args.model,\n                     device = args.device,\n                     labels = args.labels,\n                     top_k = args.top_k)\n    elif args.engine == \'detector\':\n        engine = OpenVINODetectorEngine(\n                     model = args.model,\n                     device = args.device,\n                     labels = args.labels)\n    else:\n        raise Exception(\'Illegal engine {}, it should be \'\n                        \'classifier or detector\'.format(args.engine))\n\n    #set_openvino_environment()\n    #if args.debug:\n    #    logger.debug(\'OpenVINO environment vars\')\n    #    target_vars = [\'INSTALLDIR\',\n    #                   \'INTEL_CVSDK_DIR\',\n    #                   \'LD_LIBRARY_PATH\',\n    #                   \'InferenceEngine_DIR\',\n    #                   \'IE_PLUGINS_PATH\',\n    #                   \'PATH\',\n    #                   \'PYTHONPATH\']\n    #    for i in target_vars:\n    #        logger.debug(\'\\t{var}: {val}\'.format(\n    #            var = i,\n    #            val = os.environ.get(i)))\n\n    bgr_array = cv2.imread(args.input)\n    t = time()\n    image_data = engine.process_input(bgr_array)\n    output = engine.inference(image_data)\n    model_outputs = engine.process_output(output)\n    logger.debug(\'Result: {}\'.format(model_outputs))\n    logger.debug(\'Classification takes {} s\'.format(time() - t))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
berrynet/engine/tensorflow_engine.py,10,"b'# Copyright 2017 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""TensorFlow inference engine.\n""""""\n\nfrom __future__ import print_function\n\nimport argparse\nimport json\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\n\nfrom berrynet import logger\n#from berrynet.dlmodelmgr import DLModelManager\nfrom berrynet.engine import DLEngine\n\n\nclass TensorFlowEngine(DLEngine):\n    # FIXME: Get model information by model manager\n    def __init__(self, model, label, input_layer, output_layer, top_k=3):\n        super(TensorFlowEngine, self).__init__()\n\n        # Load model\n        with tf.gfile.FastGFile(model, \'rb\') as f:\n            graph_def = tf.GraphDef()\n            graph_def.ParseFromString(f.read())\n            _ = tf.import_graph_def(graph_def, name=\'\')\n\n        # Load labels\n        self.labels = [line.rstrip() for line in tf.gfile.FastGFile(label)]\n\n        # Load other configs\n        self.input_layer = input_layer\n        self.output_layer = output_layer\n        self.top_k = top_k\n\n        # NOTE: Do NOT call read_tensor_from_nparray twice to prevent from\n        #       recreating unexpected placeholders.\n        self.tensor_op = self.read_tensor_from_nparray(\n            input_height=299,\n            input_width=299,\n            input_mean=0,\n            input_std=255)\n\n    def create(self):\n        # Create session\n        self.sess = tf.Session()\n\n    def process_input(self, rgb_array):\n        return self.sess.run(self.tensor_op,\n                             feed_dict={\'inarray:0\': rgb_array})\n\n    def inference(self, tensor):\n        return self.sess.run(self.output_layer,\n                             {self.input_layer: tensor})\n\n    def process_output(self, output):\n        processed_output = {\'annotations\': []}\n        decimal_digits = 2\n        predictions = np.squeeze(output)\n        top_k_index = predictions.argsort()[-self.top_k:][::-1]\n\n        for node_id in top_k_index:\n            human_string = self.labels[node_id]\n            score = round(float(predictions[node_id]), decimal_digits)\n            anno = {\n                \'type\': \'classification\',\n                \'label\': human_string,\n                \'confidence\': score\n            }\n            processed_output[\'annotations\'].append(anno)\n            logger.debug(\'%s (score = %.5f)\' % (human_string, score))\n        return processed_output\n\n    def save_cache(self):\n        pass\n\n    # NOTE: Copied from trainer.component\n    def read_tensor_from_nparray(self, input_height=192, input_width=192,\n                                 input_mean=0, input_std=255):\n        """""" Create normalized tensor based on input numpy array """"""\n        image_reader = tf.placeholder(tf.uint8, name=\'inarray\')\n        float_caster = tf.cast(image_reader, tf.float32)\n        dims_expander = tf.expand_dims(float_caster, 0)\n        resized = tf.image.resize_bilinear(dims_expander,\n                                           [input_height, input_width])\n        normalized = tf.divide(tf.subtract(resized, [input_mean]), [input_std])\n        return normalized\n'"
berrynet/engine/tflite_engine.py,6,"b'import logging\nimport time\n\nfrom argparse import ArgumentParser\nfrom os import path\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\n\nfrom berrynet.engine import DLEngine\nfrom berrynet import logger\n\n\nclass TFLiteDetectorEngine(DLEngine):\n    def __init__(self, model, labels, threshold=0.5, num_threads=1):\n        """"""\n        Builds Tensorflow graph, load model and labels\n        """"""\n        # Load labels\n        self.labels = self._load_label(labels)\n        self.classes = len(self.labels)\n\n        # Define lite graph and Load Tensorflow Lite model into memory\n        self.interpreter = tf.lite.Interpreter(\n            model_path=model)\n        self.interpreter.allocate_tensors()\n        self.input_details = self.interpreter.get_input_details()\n        self.output_details = self.interpreter.get_output_details()\n        self.input_dtype = self.input_details[0][\'dtype\']\n        self.num_threads = num_threads\n        self.threshold = threshold\n\n    def __delete__(self, instance):\n        #tf.reset_default_graph()\n        #self.sess = tf.InteractiveSession()\n        del self.interpreter\n\n    def process_input(self, tensor):\n        """"""Resize and normalize image for network input""""""\n\n        self.img_w = tensor.shape[1]\n        self.img_h = tensor.shape[0]\n\n        frame = cv2.cvtColor(tensor, cv2.COLOR_BGR2RGB)\n        frame = cv2.resize(frame, (300, 300))\n        frame = np.expand_dims(frame, axis=0)\n        if self.input_dtype == np.float32:\n            frame = (2.0 / 255.0) * frame - 1.0\n            frame = frame.astype(\'float32\')\n        else:\n            # default data type returned by cv2.imread is np.unit8\n            pass\n        return frame\n\n    def inference(self, tensor):\n        self.interpreter.set_num_threads(int(self.num_threads));\n        self.interpreter.set_tensor(self.input_details[0][\'index\'], tensor)\n        self.interpreter.invoke()\n\n        # get results\n        boxes = self.interpreter.get_tensor(\n            self.output_details[0][\'index\'])\n        classes = self.interpreter.get_tensor(\n            self.output_details[1][\'index\'])\n        scores = self.interpreter.get_tensor(\n            self.output_details[2][\'index\'])\n        num = self.interpreter.get_tensor(\n            self.output_details[3][\'index\'])\n        return {\n            \'boxes\': boxes,\n            \'classes\': classes,\n            \'scores\': scores,\n            \'num\': num\n        }\n\n    def process_output(self, output):\n        # get results\n        boxes = np.squeeze(output[\'boxes\'][0])\n        classes = np.squeeze(output[\'classes\'][0] + 1).astype(np.int32)\n        scores = np.squeeze(output[\'scores\'][0])\n        num = output[\'num\'][0]\n\n        annotations = []\n        number_boxes = boxes.shape[0]\n        for i in range(number_boxes):\n            box = tuple(boxes[i].tolist())\n            ymin, xmin, ymax, xmax = box\n\n            if scores[i] < self.threshold:\n                continue\n            annotations.append({\n                \'label\': self.labels[classes[i]],\n                \'confidence\': float(scores[i]),\n                \'left\': int(xmin * self.img_w),\n                \'top\': int(ymin * self.img_h),\n                \'right\': int(xmax * self.img_w),\n                \'bottom\': int(ymax * self.img_h)\n            })\n        return {\'annotations\': annotations}\n\n    def _load_label(self, path):\n        with open(path, \'r\') as f:\n            labels = list(map(str.strip, f.readlines()))\n        return labels\n\n\nclass TFLiteClassifierEngine(DLEngine):\n    def __init__(self, model, labels, top_k=3, num_threads=1,\n                 input_mean=127.5, input_std=127.5):\n        """"""\n        Builds Tensorflow graph, load model and labels\n        """"""\n        # Load labels\n        self.labels = self._load_label(labels)\n        self.classes = len(self.labels)\n\n        # Define lite graph and Load Tensorflow Lite model into memory\n        self.interpreter = tf.lite.Interpreter(\n            model_path=model)\n        self.interpreter.allocate_tensors()\n        self.input_details = self.interpreter.get_input_details()\n        self.output_details = self.interpreter.get_output_details()\n        self.floating_model = False\n        if self.input_details[0][\'dtype\'] == np.float32:\n            self.floating_model = True\n        self.input_mean = input_mean\n        self.input_std = input_std\n        self.top_k = int(top_k)\n        self.num_threads = num_threads\n\n    def __delete__(self, instance):\n        #tf.reset_default_graph()\n        #self.sess = tf.InteractiveSession()\n        del self.interpreter\n\n    def process_input(self, tensor):\n        """"""Resize and normalize image for network input""""""\n\n        self.img_w = tensor.shape[1]\n        self.img_h = tensor.shape[0]\n\n        frame = cv2.cvtColor(tensor, cv2.COLOR_BGR2RGB)\n        frame = cv2.resize(frame, (self.input_details[0][\'shape\'][2],\n                                   self.input_details[0][\'shape\'][1]))\n        frame = np.expand_dims(frame, axis=0)\n        if self.floating_model:\n            frame = (np.float32(frame) - self.input_mean) / self.input_std\n        return frame\n\n    def inference(self, tensor):\n        self.interpreter.set_num_threads(int(self.num_threads));\n        self.interpreter.set_tensor(self.input_details[0][\'index\'], tensor)\n        self.interpreter.invoke()\n        output_data = self.interpreter.get_tensor(self.output_details[0][\'index\'])\n        results = np.squeeze(output_data)\n        return {\n            \'scores\': results,\n        }\n\n    def process_output(self, output):\n        # get results\n        scores = output[\'scores\']\n        top_k_results = scores.argsort()[-self.top_k:][::-1]\n\n        processed_output = {\'annotations\': []}\n\n        for i in top_k_results:\n            human_string = self.labels[i]\n            if self.floating_model:\n                score = float(scores[i])\n            else:\n                score = float(scores[i])/255.0\n            anno = {\n                \'type\': \'classification\',\n                \'label\': human_string,\n                \'confidence\': score\n            }\n            processed_output[\'annotations\'].append(anno)\n\n        return processed_output\n\n    def _load_label(self, path):\n        with open(path, \'r\') as f:\n            labels = list(map(str.strip, f.readlines()))\n        return labels\n\n\ndef parse_argsr():\n    parser = ArgumentParser()\n    parser.add_argument(\n        ""-e"", ""--engine"",\n        help=(""Classifier or Detector engine. ""\n              ""classifier, or detector is acceptable. ""\n              ""(classifier by default)""),\n        default=""classifier"",\n        type=str)\n    parser.add_argument(\n        ""-m"", ""--model"",\n        help=""Path to an .xml file with a trained model."",\n        required=True,\n        type=str)\n    parser.add_argument(\n        ""-l"", ""--labels"",\n        help=""Labels mapping file"",\n        default=None,\n        type=str)\n    parser.add_argument(\n        ""--top_k"",\n        help=""Number of top results"",\n        default=3,\n        type=int)\n    parser.add_argument(\n        ""--num_threads"",\n        help=""Number of threads"",\n        default=1,\n        type=int)\n    parser.add_argument(\n        ""-i"", ""--input"",\n        help=""Path to a folder with images or path to an image files"",\n        required=True,\n        type=str)\n    parser.add_argument(\n        ""--debug"",\n        help=""Debug mode toggle"",\n        default=False,\n        action=""store_true"")\n\n    return parser.parse_args()\n\ndef main():\n    # Example command\n    #     $ python3 tflite_engine.py -e detector \\\n    #           -m detect.tflite --labels labels.txt -i dog.jpg --debug\n    args = parse_argsr()\n\n    if args.debug:\n        logger.setLevel(logging.DEBUG)\n    else:\n        logger.setLevel(logging.INFO)\n\n    if args.engine == \'classifier\':\n        engine = TFLiteClassifierEngine(\n                     model = args.model,\n                     labels = args.labels,\n                     top_k = args.top_k,\n                     num_threads = args.num_threads)\n    elif args.engine == \'detector\':\n        engine = TFLiteDetectorEngine(\n                     model = args.model,\n                     labels = args.labels,\n                     num_threads = args.num_threads)\n    else:\n        raise Exception(\'Illegal engine {}, it should be \'\n                        \'classifier or detector\'.format(args.engine))\n\n    for i in range(5):\n        bgr_array = cv2.imread(args.input)\n        t = time.time()\n        image_data = engine.process_input(bgr_array)\n        output = engine.inference(image_data)\n        model_outputs = engine.process_output(output)\n        # Reference result\n        #     input:\n        #         darknet/data/dog.jpg\n        #     output:\n        #         Inference takes 0.7533011436462402 s\n        #         Inference takes 0.5741658210754395 s\n        #         Inference takes 0.6120760440826416 s\n        #         Inference takes 0.6191139221191406 s\n        #         Inference takes 0.5809791088104248 s\n        #         label: bicycle  conf: 0.9563907980918884  (139, 116) (567, 429)\n        #         label: car  conf: 0.8757821917533875  (459, 80) (690, 172)\n        #         label: dog  conf: 0.869189441204071  (131, 218) (314, 539)\n        #         label: car  conf: 0.40003547072410583  (698, 122) (724, 152)\n        logger.debug(\'Inference takes {} s\'.format(time.time() - t))\n\n    if args.engine == \'classifier\':\n        for r in model_outputs[\'annotations\']:\n            logger.debug(\'label: {0}  conf: {1}\'.format(\n                r[\'label\'],\n                r[\'confidence\']\n            ))\n    elif args.engine == \'detector\':\n        for r in model_outputs[\'annotations\']:\n            logger.debug(\'label: {0}  conf: {1}  ({2}, {3}) ({4}, {5})\'.format(\n                r[\'label\'],\n                r[\'confidence\'],\n                r[\'left\'],\n                r[\'top\'],\n                r[\'right\'],\n                r[\'bottom\']\n            ))\n    else:\n        raise Exception(\'Can not get result \'\n                        \'from illegal engine {}\'.format(args.engine))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
berrynet/service/__init__.py,0,"b'# Copyright 2018 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Engine service is a bridge between incoming data and inference engine.\n""""""\n\nimport os\nimport time\n\nfrom datetime import datetime\n\nfrom berrynet import logger\nfrom berrynet.comm import Communicator\nfrom berrynet.comm import payload\n\n\nclass EngineService(object):\n    def __init__(self, service_name, engine, comm_config):\n        self.service_name = service_name\n        self.engine = engine\n        self.comm_config = comm_config\n        for topic, functor in self.comm_config[\'subscribe\'].items():\n            self.comm_config[\'subscribe\'][topic] = eval(functor)\n        self.comm_config[\'subscribe\'][\'berrynet/data/rgbimage\'] = self.inference\n        self.comm = Communicator(self.comm_config, debug=True)\n\n    def inference(self, pl):\n        duration = lambda t: (datetime.now() - t).microseconds / 1000\n\n        t = datetime.now()\n        logger.debug(\'payload size: {}\'.format(len(pl)))\n        logger.debug(\'payload type: {}\'.format(type(pl)))\n        jpg_json = payload.deserialize_payload(pl.decode(\'utf-8\'))\n        jpg_bytes = payload.destringify_jpg(jpg_json[\'bytes\'])\n        logger.debug(\'destringify_jpg: {} ms\'.format(duration(t)))\n\n        t = datetime.now()\n        rgb_array = payload.jpg2rgb(jpg_bytes)\n        logger.debug(\'jpg2rgb: {} ms\'.format(duration(t)))\n\n        t = datetime.now()\n        image_data = self.engine.process_input(rgb_array)\n        output = self.engine.inference(image_data)\n        model_outputs = self.engine.process_output(output)\n        logger.debug(\'Result: {}\'.format(model_outputs))\n        logger.debug(\'Classification takes {} ms\'.format(duration(t)))\n\n        #self.engine.cache_data(\'model_output\', model_outputs)\n        #self.engine.cache_data(\'model_output_filepath\', output_name)\n        #self.engine.save_cache()\n\n        self.result_hook(self.generalize_result(jpg_json, model_outputs))\n\n    def generalize_result(self, eng_input, eng_output):\n        eng_input.update(eng_output)\n        return eng_input\n\n    def result_hook(self, generalized_result):\n        logger.debug(\'base result_hook\')\n\n    def run(self, args):\n        """"""Infinite loop serving inference requests""""""\n        self.engine.create()\n        self.comm.run()\n'"
berrynet/service/darknet_service.py,0,"b'# Copyright 2018 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Engine service is a bridge between incoming data and inference engine.\n""""""\n\nimport argparse\nimport logging\nimport math\n\nimport cv2\n\nfrom berrynet import logger\nfrom berrynet.comm import payload\nfrom berrynet.dlmodelmgr import DLModelManager\nfrom berrynet.engine.darknet_engine import DarknetEngine\nfrom berrynet.service import EngineService\nfrom berrynet.utils import generate_class_color\nfrom berrynet.utils import draw_bb\n\n\nclass DarknetService(EngineService):\n    def __init__(self, service_name, engine, comm_config, draw=False):\n        super(DarknetService, self).__init__(service_name,\n                                                engine,\n                                                comm_config)\n        self.draw = draw\n\n    def inference(self, pl):\n        jpg_json = payload.deserialize_payload(pl.decode(\'utf-8\'))\n        jpg_bytes = payload.destringify_jpg(jpg_json[\'bytes\'])\n\n        bgr_array = payload.jpg2bgr(jpg_bytes)\n\n        image_data = self.engine.process_input(bgr_array)\n        output = self.engine.inference(image_data)\n        model_outputs = self.engine.process_output(output)\n\n        classes = self.engine.classes\n        labels = self.engine.labels\n\n        if self.draw is False:\n            self.result_hook(self.generalize_result(jpg_json, model_outputs))\n        else:\n            self.result_hook(\n                draw_bb(bgr_array,\n                        self.generalize_result(jpg_json, model_outputs),\n                        generate_class_color(class_num=classes),\n                        labels))\n\n    def result_hook(self, generalized_result):\n        logger.debug(\'result_hook, annotations: {}\'.format(generalized_result[\'annotations\']))\n        self.comm.send(\'berrynet/engine/darknet/result\',\n                       payload.serialize_payload(generalized_result))\n\n\ndef parse_args():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\n        \'--service_name\',\n        default=\'darknet\',\n        help=\'Human-readable service name for service management.\')\n    ap.add_argument(\n        \'-m\', \'--model\',\n        help=\'Model file path\')\n    ap.add_argument(\n        \'-l\', \'--label\',\n        help=\'Label file path\')\n    ap.add_argument(\n        \'-p\', \'--model_package\',\n        default=\'\',\n        help=\'Model package name. Find model and label file paths automatically.\')\n    ap.add_argument(\n        \'--draw\',\n        action=\'store_true\',\n        help=\'Draw bounding boxes on image in result\')\n    ap.add_argument(\n        \'--debug\',\n        action=\'store_true\',\n        help=\'Debug mode toggle\')\n    return vars(ap.parse_args())\n\n\ndef main():\n    args = parse_args()\n    if args[\'debug\']:\n        logger.setLevel(logging.DEBUG)\n    else:\n        logger.setLevel(logging.INFO)\n    if args[\'model_package\'] != \'\':\n        dlmm = DLModelManager()\n        meta = dlmm.get_model_meta(args[\'model_package\'])\n        args[\'model\'] = meta[\'model\']\n        args[\'label\'] = meta[\'label\']\n    logger.debug(\'model filepath: \' + args[\'model\'])\n    logger.debug(\'label filepath: \' + args[\'label\'])\n\n    engine = DarknetEngine(\n        config=meta[\'config\'][\'config\'].encode(\'utf-8\'),\n        model=args[\'model\'].encode(\'utf-8\'),\n        meta=meta[\'config\'][\'meta\'].encode(\'utf-8\')\n    )\n    comm_config = {\n        \'subscribe\': {},\n        \'broker\': {\n            \'address\': \'localhost\',\n            \'port\': 1883\n        }\n    }\n    engine_service = DarknetService(args[\'service_name\'],\n                                    engine,\n                                    comm_config,\n                                    args[\'draw\'])\n    engine_service.run(args)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
berrynet/service/mockup_service.py,0,"b'# Copyright 2018 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Mockup service with relay engine (default engine).\n""""""\n\nimport argparse\nimport json\nimport logging\nimport os\n\nfrom berrynet import logger\nfrom berrynet.comm import payload\nfrom berrynet.engine import DLEngine\nfrom berrynet.service import EngineService\n\n\nclass MockupEngine(DLEngine):\n    def inference(self, tensor):\n        return {\n            \'annotations\': {\n                \'label\': \'dt42\',\n                \'confidence\': 0.99\n            }\n        }\n\n\nclass MockupService(EngineService):\n    def __init__(self, service_name, engine, comm_config):\n        super().__init__(service_name,\n                         engine,\n                         comm_config)\n        if not os.path.exists(\'/tmp/mockup\'):\n            os.mkdir(\'/tmp/mockup\')\n\n    #def generalize_result(self, eng_input, eng_output):\n    #    logger.debug()\n    #    eng_input.update(eng_output)\n    #    return eng_input\n\n    def result_hook(self, generalized_result):\n        gr = generalized_result\n        jpg_bytes = payload.destringify_jpg(gr.pop(\'bytes\'))\n        logger.debug(\'generalized result (readable only): {}\'.format(gr))\n        with open(\'/tmp/mockup/{}.jpg\'.format(gr[\'timestamp\']), \'wb\') as f:\n            f.write(jpg_bytes)\n        with open(\'/tmp/mockup/{}.json\'.format(gr[\'timestamp\']), \'w\') as f:\n            f.write(json.dumps(gr, indent=4))\n\n\ndef parse_args():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\'--debug\',\n                    action=\'store_true\',\n                    help=\'Debug mode toggle\')\n    return vars(ap.parse_args())\n\n\ndef main():\n    args = parse_args()\n    if args[\'debug\']:\n        logger.setLevel(logging.DEBUG)\n    else:\n        logger.setLevel(logging.INFO)\n\n    eng = MockupEngine()\n    comm_config = {\n        \'subscribe\': {},\n        \'broker\': {\n            \'address\': \'localhost\',\n            \'port\': 1883\n        }\n    }\n    engine_service = MockupService(\'mockup service\', eng, comm_config)\n    engine_service.run(args)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
berrynet/service/movidius_service.py,0,"b'# Copyright 2017 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Engine service is a bridge between incoming data and inference engine.\n""""""\n\nimport argparse\nimport logging\n\nfrom datetime import datetime\n\nfrom berrynet import logger\nfrom berrynet.comm import payload\nfrom berrynet.dlmodelmgr import DLModelManager\nfrom berrynet.engine.movidius_engine import MovidiusEngine\nfrom berrynet.engine.movidius_engine import MovidiusMobileNetSSDEngine\nfrom berrynet.service import EngineService\nfrom berrynet.utils import draw_bb\nfrom berrynet.utils import generate_class_color\n\n\nclass MovidiusClassificationService(EngineService):\n    def __init__(self, service_name, engine, comm_config):\n        super(MovidiusClassificationService, self).__init__(service_name,\n                                                            engine,\n                                                            comm_config)\n\n    def result_hook(self, generalized_result):\n        logger.debug(\'result_hook, annotations: {}\'.format(generalized_result[\'annotations\']))\n        self.comm.send(\'berrynet/engine/mvclassification/result\',\n                       payload.serialize_payload(generalized_result))\n\n\nclass MovidiusMobileNetSSDService(EngineService):\n    def __init__(self, service_name, engine, comm_config, draw=False):\n        super(MovidiusMobileNetSSDService, self).__init__(service_name,\n                                                          engine,\n                                                          comm_config)\n        self.draw = draw\n\n    def inference(self, pl):\n        duration = lambda t: (datetime.now() - t).microseconds / 1000\n\n        t = datetime.now()\n        logger.debug(\'payload size: {}\'.format(len(pl)))\n        logger.debug(\'payload type: {}\'.format(type(pl)))\n        jpg_json = payload.deserialize_payload(pl.decode(\'utf-8\'))\n        jpg_bytes = payload.destringify_jpg(jpg_json[\'bytes\'])\n        logger.debug(\'destringify_jpg: {} ms\'.format(duration(t)))\n\n        t = datetime.now()\n        bgr_array = payload.jpg2bgr(jpg_bytes)\n        logger.debug(\'jpg2bgr: {} ms\'.format(duration(t)))\n\n        t = datetime.now()\n        image_data = self.engine.process_input(bgr_array)\n        output = self.engine.inference(image_data)\n        model_outputs = self.engine.process_output(output)\n        logger.debug(\'Result: {}\'.format(model_outputs))\n        logger.debug(\'Detection takes {} ms\'.format(duration(t)))\n\n        classes = self.engine.classes\n        labels = self.engine.labels\n\n        logger.debug(\'draw = {}\'.format(self.draw))\n        if self.draw is False:\n            self.result_hook(self.generalize_result(jpg_json, model_outputs))\n        else:\n            self.result_hook(\n                draw_bb(bgr_array,\n                        self.generalize_result(jpg_json, model_outputs),\n                        generate_class_color(class_num=classes),\n                        labels))\n\n    def result_hook(self, generalized_result):\n        logger.debug(\'result_hook, annotations: {}\'.format(generalized_result[\'annotations\']))\n        self.comm.send(\'berrynet/engine/mvmobilenetssd/result\',\n                       payload.serialize_payload(generalized_result))\n\n\ndef parse_args():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\'--model\',\n                    help=\'Model file path\')\n    ap.add_argument(\'--label\',\n                    help=\'Label file path\')\n    ap.add_argument(\'--model_package\',\n                    default=\'\',\n                    help=\'Model package name\')\n    ap.add_argument(\'--service_name\', required=True,\n                    help=\'Valid value: Classification, MobileNetSSD\')\n    ap.add_argument(\'--num_top_predictions\', default=5,\n                    help=\'Display this many predictions\')\n    ap.add_argument(\'--draw\',\n                    action=\'store_true\',\n                    help=\'Draw bounding boxes on image in result\')\n    ap.add_argument(\'--debug\',\n                    action=\'store_true\',\n                    help=\'Debug mode toggle\')\n    return vars(ap.parse_args())\n\n\ndef main():\n    # Test Movidius engine\n    args = parse_args()\n    if args[\'debug\']:\n        logger.setLevel(logging.DEBUG)\n    else:\n        logger.setLevel(logging.INFO)\n    if args[\'model_package\'] != \'\':\n        dlmm = DLModelManager()\n        meta = dlmm.get_model_meta(args[\'model_package\'])\n        args[\'model\'] = meta[\'model\']\n        args[\'label\'] = meta[\'label\']\n    logger.debug(\'model filepath: \' + args[\'model\'])\n    logger.debug(\'label filepath: \' + args[\'label\'])\n\n    comm_config = {\n        \'subscribe\': {},\n        \'broker\': {\n            \'address\': \'localhost\',\n            \'port\': 1883\n        }\n    }\n    if args[\'service_name\'] == \'Classification\':\n        mvng = MovidiusEngine(args[\'model\'], args[\'label\'])\n        service_functor = MovidiusClassificationService\n    elif args[\'service_name\'] == \'MobileNetSSD\':\n        mvng = MovidiusMobileNetSSDEngine(args[\'model\'], args[\'label\'])\n        service_functor = MovidiusMobileNetSSDService\n    else:\n        logger.critical(\'Legal service names are Classification, MobileNetSSD\')\n    engine_service = service_functor(args[\'service_name\'],\n                                     mvng,\n                                     comm_config,\n                                     draw=args[\'draw\'])\n    engine_service.run(args)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
berrynet/service/openvino_service.py,0,"b'# Copyright 2017 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Engine service is a bridge between incoming data and inference engine.\n""""""\n\nimport argparse\nimport logging\n\nfrom datetime import datetime\n\nfrom berrynet import logger\nfrom berrynet.comm import payload\nfrom berrynet.dlmodelmgr import DLModelManager\nfrom berrynet.engine.openvino_engine import OpenVINOClassifierEngine\nfrom berrynet.engine.openvino_engine import OpenVINODetectorEngine\nfrom berrynet.service import EngineService\nfrom berrynet.utils import draw_bb\nfrom berrynet.utils import generate_class_color\n\n\nclass OpenVINOClassifierService(EngineService):\n    def __init__(self, service_name, engine, comm_config, draw=False):\n        super(OpenVINOClassifierService, self).__init__(service_name,\n                                                        engine,\n                                                        comm_config)\n        self.draw = draw\n\n    def inference(self, pl):\n        duration = lambda t: (datetime.now() - t).microseconds / 1000\n\n        t = datetime.now()\n        logger.debug(\'payload size: {}\'.format(len(pl)))\n        logger.debug(\'payload type: {}\'.format(type(pl)))\n        jpg_json = payload.deserialize_payload(pl.decode(\'utf-8\'))\n        jpg_bytes = payload.destringify_jpg(jpg_json[\'bytes\'])\n        logger.debug(\'destringify_jpg: {} ms\'.format(duration(t)))\n\n        t = datetime.now()\n        bgr_array = payload.jpg2bgr(jpg_bytes)\n        logger.debug(\'jpg2bgr: {} ms\'.format(duration(t)))\n\n        t = datetime.now()\n        image_data = self.engine.process_input(bgr_array)\n        output = self.engine.inference(image_data)\n        model_outputs = self.engine.process_output(output)\n        logger.debug(\'Result: {}\'.format(model_outputs))\n        logger.debug(\'Detection takes {} ms\'.format(duration(t)))\n\n        #classes = self.engine.classes\n        #labels = self.engine.labels\n\n        logger.debug(\'draw = {}\'.format(self.draw))\n        if self.draw is False:\n            self.result_hook(self.generalize_result(jpg_json, model_outputs))\n        else:\n            #self.result_hook(\n            #    draw_bb(bgr_array,\n            #            self.generalize_result(jpg_json, model_outputs),\n            #            generate_class_color(class_num=classes),\n            #            labels))\n            self.result_hook(self.generalize_result(jpg_json, model_outputs))\n\n    def result_hook(self, generalized_result):\n        logger.debug(\'result_hook, annotations: {}\'.format(generalized_result[\'annotations\']))\n        self.comm.send(\'berrynet/engine/ovclassifier/result\',\n                       payload.serialize_payload(generalized_result))\n\n\nclass OpenVINODetectorService(EngineService):\n    def __init__(self, service_name, engine, comm_config, draw=False):\n        super(OpenVINODetectorService, self).__init__(service_name,\n                                                      engine,\n                                                      comm_config)\n        self.draw = draw\n\n    def inference(self, pl):\n        duration = lambda t: (datetime.now() - t).microseconds / 1000\n\n        t = datetime.now()\n        logger.debug(\'payload size: {}\'.format(len(pl)))\n        logger.debug(\'payload type: {}\'.format(type(pl)))\n        jpg_json = payload.deserialize_payload(pl.decode(\'utf-8\'))\n        jpg_bytes = payload.destringify_jpg(jpg_json[\'bytes\'])\n        logger.debug(\'destringify_jpg: {} ms\'.format(duration(t)))\n\n        t = datetime.now()\n        bgr_array = payload.jpg2bgr(jpg_bytes)\n        logger.debug(\'jpg2bgr: {} ms\'.format(duration(t)))\n\n        t = datetime.now()\n        image_data = self.engine.process_input(bgr_array)\n        output = self.engine.inference(image_data)\n        model_outputs = self.engine.process_output(output)\n        logger.debug(\'Result: {}\'.format(model_outputs))\n        logger.debug(\'Detection takes {} ms\'.format(duration(t)))\n\n        classes = len(self.engine.labels_map)\n        labels = self.engine.labels_map\n\n        logger.debug(\'draw = {}\'.format(self.draw))\n        if self.draw is False:\n            self.result_hook(self.generalize_result(jpg_json, model_outputs))\n        else:\n            self.result_hook(\n                draw_bb(bgr_array,\n                        self.generalize_result(jpg_json, model_outputs),\n                        generate_class_color(class_num=classes),\n                        labels))\n\n    def result_hook(self, generalized_result):\n        logger.debug(\'result_hook, annotations: {}\'.format(generalized_result[\'annotations\']))\n        self.comm.send(\'berrynet/engine/ovdetector/result\',\n                       payload.serialize_payload(generalized_result))\n\n\ndef parse_args():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\n        \'-s\', \'--service\',\n        help=(\'Classifier or Detector service. \'\n              \'classifier, or detector is acceptable. \'\n              \'(classifier by default)\'),\n        default=\'classifier\',\n        type=str)\n    ap.add_argument(\n        \'--service_name\',\n        default=\'openvino_classifier\',\n        help=\'Human-readable service name for service management.\')\n    ap.add_argument(\n        \'-m\', \'--model\',\n        help=\'Model file path\')\n    ap.add_argument(\n        \'-l\', \'--label\',\n        help=\'Label file path\')\n    ap.add_argument(\n        \'-p\', \'--model_package\',\n        default=\'\',\n        help=\'Model package name. Find model and label file paths automatically.\')\n    ap.add_argument(\n        \'--top_k\',\n        help=\'Display top K classification results.\',\n        default=3,\n        type=int)\n    ap.add_argument(\n        \'-d\', \'--device\',\n        help=\'Specify the target device to infer on; CPU, GPU, FPGA or MYRIAD is acceptable. Sample will look for a suitable plugin for device specified (CPU by default)\',\n        default=\'CPU\',\n        type=str)\n    ap.add_argument(\n        \'--draw\',\n        action=\'store_true\',\n        help=\'Draw bounding boxes on image in result\')\n    ap.add_argument(\n        \'--debug\',\n        action=\'store_true\',\n        help=\'Debug mode toggle\')\n    return vars(ap.parse_args())\n\n\ndef main():\n    # Test OpenVINO classifier engine\n    args = parse_args()\n    if args[\'debug\']:\n        logger.setLevel(logging.DEBUG)\n    else:\n        logger.setLevel(logging.INFO)\n    if args[\'model_package\'] != \'\':\n        dlmm = DLModelManager()\n        meta = dlmm.get_model_meta(args[\'model_package\'])\n        args[\'model\'] = meta[\'model\']\n        args[\'label\'] = meta[\'label\']\n    logger.debug(\'model filepath: \' + args[\'model\'])\n    logger.debug(\'label filepath: \' + args[\'label\'])\n\n    comm_config = {\n        \'subscribe\': {},\n        \'broker\': {\n            \'address\': \'localhost\',\n            \'port\': 1883\n        }\n    }\n\n    if args[\'service\'] == \'classifier\':\n        engine = OpenVINOClassifierEngine(\n                     model = args[\'model\'],\n                     labels = args[\'label\'],\n                     top_k = args[\'top_k\'],\n                     device = args[\'device\'])\n        service_functor = OpenVINOClassifierService\n    elif args[\'service\'] == \'detector\':\n        engine = OpenVINODetectorEngine(\n                     model = args[\'model\'],\n                     labels = args[\'label\'],\n                     device = args[\'device\'])\n        service_functor = OpenVINODetectorService\n    else:\n        raise Exception(\'Illegal service {}, it should be \'\n                        \'classifier or detector\'.format(args[\'service\']))\n\n    engine_service = service_functor(args[\'service_name\'],\n                                     engine,\n                                     comm_config,\n                                     draw=args[\'draw\'])\n    engine_service.run(args)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
berrynet/service/tensorflow_service.py,0,"b'# Copyright 2017 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Engine service is a bridge between incoming data and inference engine.\n""""""\n\nimport argparse\nimport logging\n\nfrom berrynet import logger\nfrom berrynet.comm import payload\nfrom berrynet.dlmodelmgr import DLModelManager\nfrom berrynet.engine.tensorflow_engine import TensorFlowEngine\nfrom berrynet.service import EngineService\n\n\nclass TensorFlowService(EngineService):\n    def __init__(self, service_name, engine, comm_config):\n        super(TensorFlowService, self).__init__(service_name,\n                                                engine,\n                                                comm_config)\n\n    def result_hook(self, generalized_result):\n        logger.debug(\'result_hook, annotations: {}\'.format(generalized_result[\'annotations\']))\n        self.comm.send(\'berrynet/engine/tensorflow/result\',\n                       payload.serialize_payload(generalized_result))\n\n\ndef parse_args():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\'--model\',\n                    help=\'Model file path\')\n    ap.add_argument(\'--label\',\n                    help=\'Label file path\')\n    ap.add_argument(\'--model_package\',\n                    default=\'\',\n                    help=\'Model package name. Find model and label file paths automatically.\')\n    ap.add_argument(\'--service_name\',\n                    default=\'tensorflow\',\n                    help=\'Human-readable service name for service management.\')\n    ap.add_argument(\'--num_top_predictions\',\n                    help=\'Display this many predictions\',\n                    default=3,\n                    type=int)\n    ap.add_argument(\'--debug\',\n                    action=\'store_true\',\n                    help=\'Debug mode toggle\')\n    return vars(ap.parse_args())\n\n\ndef main():\n    # Test TensorFlow engine\n    args = parse_args()\n    if args[\'debug\']:\n        logger.setLevel(logging.DEBUG)\n    else:\n        logger.setLevel(logging.INFO)\n    logger.debug(\'model filepath: \' + args[\'model\'])\n    logger.debug(\'label filepath: \' + args[\'label\'])\n\n    model = \'berrynet/engine/inception_v3_2016_08_28_frozen.pb\'\n    label = \'berrynet/engine/imagenet_slim_labels.txt\'\n    jpg_filepath = \'berrynet/engine/grace_hopper.jpg\'\n    input_layer = \'input:0\'\n    output_layer = \'InceptionV3/Predictions/Reshape_1:0\'\n\n    tfe = TensorFlowEngine(model, label, input_layer, output_layer)\n    comm_config = {\n        \'subscribe\': {},\n        \'broker\': {\n            \'address\': \'localhost\',\n            \'port\': 1883\n        }\n    }\n    engine_service = TensorFlowService(args[\'service_name\'],\n                                       tfe,\n                                       comm_config)\n    engine_service.run(args)\n\n\nif __name__ == \'__main__\':\n    # Test Movidius engine\n    #import movidius as mv\n\n    #logging.basicConfig(level=logging.DEBUG)\n    #args = parse_args()\n    #if args[\'model_package\'] != \'\':\n    #    dlmm = DLModelManager()\n    #    meta = dlmm.get_model_meta(args[\'model_package\'])\n    #    args[\'model\'] = meta[\'model\']\n    #    args[\'label\'] = meta[\'label\']\n    #logging.debug(\'model filepath: \' + args[\'model\'])\n    #logging.debug(\'label filepath: \' + args[\'label\'])\n    #logging.debug(\'image_dir: \' + args[\'image_dir\'])\n\n    #mvng = mv.MovidiusNeuralGraph(args[\'model\'], args[\'label\'])\n    #engine_service = EngineService(args[\'service_name\'], mvng)\n    #engine_service.run(args)\n\n    main()\n'"
berrynet/service/tflite_service.py,0,"b'# Copyright 2019 DT42\n#\n# This file is part of BerryNet.\n#\n# BerryNet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# BerryNet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with BerryNet.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Engine service is a bridge between incoming data and inference engine.\n""""""\n\nimport argparse\nimport logging\nimport time\n\nfrom berrynet import logger\nfrom berrynet.comm import payload\nfrom berrynet.dlmodelmgr import DLModelManager\nfrom berrynet.engine.tflite_engine import TFLiteClassifierEngine\nfrom berrynet.engine.tflite_engine import TFLiteDetectorEngine\nfrom berrynet.service import EngineService\nfrom berrynet.utils import draw_bb\nfrom berrynet.utils import generate_class_color\n\n\nclass TFLiteClassifierService(EngineService):\n    def __init__(self, service_name, engine, comm_config, draw=False):\n        super(TFLiteClassifierService, self).__init__(service_name,\n                                                      engine,\n                                                      comm_config)\n        self.draw = draw\n\n    def inference(self, pl):\n        t0 = time.time()\n        logger.debug(\'payload size: {}\'.format(len(pl)))\n        logger.debug(\'payload type: {}\'.format(type(pl)))\n        jpg_json = payload.deserialize_payload(pl.decode(\'utf-8\'))\n        jpg_bytes = payload.destringify_jpg(jpg_json[\'bytes\'])\n        logger.debug(\'destringify_jpg: {} ms\'.format(time.time() - t0))\n\n        t1 = time.time()\n        bgr_array = payload.jpg2bgr(jpg_bytes)\n        logger.debug(\'jpg2bgr: {} ms\'.format(time.time() - t1))\n\n        t2 = time.time()\n        image_data = self.engine.process_input(bgr_array)\n        logger.debug(\'Input processing takes {} ms\'.format(time.time() - t2))\n\n        t3 = time.time()\n        output = self.engine.inference(image_data)\n        model_outputs = self.engine.process_output(output)\n        logger.debug(\'Result: {}\'.format(model_outputs))\n        logger.debug(\'Classification takes {} ms\'.format(time.time() - t3))\n\n        classes = self.engine.classes\n        labels = self.engine.labels\n\n        logger.debug(\'draw = {}\'.format(self.draw))\n        if self.draw is False:\n            self.result_hook(self.generalize_result(jpg_json, model_outputs))\n        else:\n            self.result_hook(\n                draw_label(bgr_array,\n                           self.generalize_result(jpg_json, model_outputs),\n                           color,\n                           labels))\n\n    def result_hook(self, generalized_result):\n        logger.debug(\'result_hook, annotations: {}\'.format(generalized_result[\'annotations\']))\n        self.comm.send(\'berrynet/engine/tfliteclassifier/result\',\n                       payload.serialize_payload(generalized_result))\n\n\nclass TFLiteDetectorService(EngineService):\n    def __init__(self, service_name, engine, comm_config, draw=False):\n        super(TFLiteDetectorService, self).__init__(service_name,\n                                                   engine,\n                                                   comm_config)\n        self.draw = draw\n\n    def inference(self, pl):\n        t0 = time.time()\n        logger.debug(\'payload size: {}\'.format(len(pl)))\n        logger.debug(\'payload type: {}\'.format(type(pl)))\n        jpg_json = payload.deserialize_payload(pl.decode(\'utf-8\'))\n        jpg_bytes = payload.destringify_jpg(jpg_json[\'bytes\'])\n        logger.debug(\'destringify_jpg: {} ms\'.format(time.time() - t0))\n\n        t1 = time.time()\n        bgr_array = payload.jpg2bgr(jpg_bytes)\n        logger.debug(\'jpg2bgr: {} ms\'.format(time.time() - t1))\n\n        t2 = time.time()\n        image_data = self.engine.process_input(bgr_array)\n        output = self.engine.inference(image_data)\n        model_outputs = self.engine.process_output(output)\n        logger.debug(\'Result: {}\'.format(model_outputs))\n        logger.debug(\'Detection takes {} ms\'.format(time.time() - t2))\n\n        classes = self.engine.classes\n        labels = self.engine.labels\n\n        logger.debug(\'draw = {}\'.format(self.draw))\n        if self.draw is False:\n            self.result_hook(self.generalize_result(jpg_json, model_outputs))\n        else:\n            self.result_hook(\n                draw_bb(bgr_array,\n                        self.generalize_result(jpg_json, model_outputs),\n                        generate_class_color(class_num=classes),\n                        labels))\n\n    def result_hook(self, generalized_result):\n        logger.debug(\'result_hook, annotations: {}\'.format(generalized_result[\'annotations\']))\n        self.comm.send(\'berrynet/engine/tflitedetector/result\',\n                       payload.serialize_payload(generalized_result))\n\n\ndef parse_args():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\n        \'-s\', \'--service\',\n        help=(\'Classifier or Detector service. \'\n              \'classifier, or detector is acceptable. \'\n              \'(classifier by default)\'),\n        default=\'classifier\',\n        type=str)\n    ap.add_argument(\n        \'--service_name\',\n        default=\'tflite_classifier\',\n        help=\'Human-readable service name for service management.\')\n    ap.add_argument(\n        \'-m\', \'--model\',\n        help=\'Model file path\')\n    ap.add_argument(\n        \'-l\', \'--label\',\n        help=\'Label file path\')\n    ap.add_argument(\n        \'-p\', \'--model_package\',\n        default=\'\',\n        help=\'Model package name. Find model and label file paths automatically.\')\n    ap.add_argument(\n        \'--top_k\',\n        help=\'Display top K classification results.\',\n        default=3,\n        type=int)\n    ap.add_argument(\n        \'--num_threads\',\n        default=1,\n        help=""Number of threads for running inference."")\n    ap.add_argument(\n        \'--draw\',\n        action=\'store_true\',\n        help=\'Draw bounding boxes on image in result\')\n    ap.add_argument(\n        \'--debug\',\n        action=\'store_true\',\n        help=\'Debug mode toggle\')\n    return vars(ap.parse_args())\n\n\ndef main():\n    # Test TFLite engines\n    args = parse_args()\n    if args[\'debug\']:\n        logger.setLevel(logging.DEBUG)\n    else:\n        logger.setLevel(logging.INFO)\n    if args[\'model_package\'] != \'\':\n        dlmm = DLModelManager()\n        meta = dlmm.get_model_meta(args[\'model_package\'])\n        args[\'model\'] = meta[\'model\']\n        args[\'label\'] = meta[\'label\']\n    logger.debug(\'model filepath: \' + args[\'model\'])\n    logger.debug(\'label filepath: \' + args[\'label\'])\n\n    comm_config = {\n        \'subscribe\': {},\n        \'broker\': {\n            \'address\': \'localhost\',\n            \'port\': 1883\n        }\n    }\n\n    if args[\'service\'] == \'classifier\':\n        engine = TFLiteClassifierEngine(\n                     model = args[\'model\'],\n                     labels = args[\'label\'],\n                     top_k = args[\'top_k\'],\n                     num_threads = args[\'num_threads\'])\n        service_functor = TFLiteClassifierService\n    elif args[\'service\'] == \'detector\':\n        engine = TFLiteDetectorEngine(\n                     model = args[\'model\'],\n                     labels = args[\'label\'],\n                     num_threads = args[\'num_threads\'])\n        service_functor = TFLiteDetectorService\n    else:\n        raise Exception(\'Illegal service {}, it should be \'\n                        \'classifier or detector\'.format(args[\'service\']))\n\n    engine_service = service_functor(args[\'service_name\'],\n                                     engine,\n                                     comm_config,\n                                     draw=args[\'draw\'])\n    engine_service.run(args)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
utils/darknet/detectord.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n# Copyright 2016 dt42.io. All Rights Reserved.\n#\n# 09-01-2016 joseph@dt42.io Initial version\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\n""""""Simple image classification server with Inception.\n\nThe server monitors image_dir and run inferences on new images added to the\ndirectory. Every image file should come with another empty file with \'.done\'\nsuffix to signal readiness. Inference result of a image can be read from the\n\'.txt\' file of that image after \'.txt.done\' is spotted.\n\nThis is an example the server expects clients to do. Note the order.\n\n# cp cat.jpg /run/image_dir\n# touch /run/image_dir/cat.jpg.done\n\nClients should wait for appearance of \'cat.jpg.txt.done\' before getting\nresult from \'cat.jpg.txt\'.\n""""""\n\n\nfrom __future__ import print_function\nimport os\nimport sys\nimport time\nimport signal\nimport argparse\nimport subprocess\nimport errno\nfrom watchdog.observers import Observer\nfrom watchdog.events import PatternMatchingEventHandler\n\n\ndef logging(*args):\n    print(""[%08.3f]"" % time.time(), \' \'.join(args))\n\n\nclass EventHandler(PatternMatchingEventHandler):\n    def process(self, event):\n        """"""\n        event.event_type\n            \'modified\' | \'created\' | \'moved\' | \'deleted\'\n        event.is_directory\n            True | False\n        event.src_path\n            path/to/observed/file\n        """"""\n        # the file will be processed there\n        _msg = event.src_path\n        os.remove(_msg)\n        logging(_msg, event.event_type)\n        darknet.stdin.write(_msg.rstrip(\'.done\').encode(\'utf8\') + b\'\\n\')\n\n    # ignore all other types of events except \'modified\'\n    def on_created(self, event):\n        self.process(event)\n\n\ndef check_pid(pid):\n    try:\n        os.kill(pid, 0)\n    except OSError as err:\n        if err.errno == errno.ESRCH:\n            # ESRCH == No such process\n            return False\n        elif err.errno == errno.EPERM:\n            # EPERM means no permission, and the process exists to deny the\n            # access\n            return True\n        else:\n            raise\n    else:\n        return True\n\nif __name__ == \'__main__\':\n    ap = argparse.ArgumentParser()\n    pid = str(os.getpid())\n    basename = os.path.splitext(os.path.basename(__file__))[0]\n    ap.add_argument(\'input_dir\')\n    ap.add_argument(\n        \'-p\', \'--pid\', default=\'/tmp/%s.pid\' % basename,\n        help=\'pid file path\')\n    ap.add_argument(\n        \'-fi\', \'--fifo\', default=\'/tmp/acti_yolo\',\n        help=\'fifo pipe path\')\n    ap.add_argument(\n        \'-d\', \'--data\', default=\'cfg/coco.data\',\n        help=\'fifo pipe path\')\n    ap.add_argument(\n        \'-c\', \'--config\', default=\'cfg/yolo.cfg\',\n        help=\'fifo pipe path\')\n    ap.add_argument(\n        \'-w\', \'--weight\', default=\'yolo.weights\',\n        help=\'fifo pipe path\')\n    args = vars(ap.parse_args())\n    WATCH_DIR = os.path.abspath(args[\'input_dir\'])\n    FIFO_PIPE = os.path.abspath(args[\'fifo\'])\n    data = args[\'data\']\n    cfg = args[\'config\']\n    weight = args[\'weight\']\n    pidfile = args[\'pid\']\n\n    if os.path.isfile(pidfile):\n        with open(pidfile) as f:\n            prev_pid = int(f.readline())\n            if check_pid(prev_pid):\n                logging(""{} already exists and process {} is still running, exists."".format(\n                    pidfile, prev_pid))\n                sys.exit(1)\n            else:\n                logging(""{} exists but process {} died, clean it up."".format(pidfile, prev_pid))\n                os.unlink(pidfile)\n\n    with open(pidfile, \'w\') as f:\n        f.write(pid)\n\n    logging(""watch_dir: "", WATCH_DIR)\n    logging(""pid: "", pidfile)\n\n    cmd = [\'./darknet\', \'detector\', \'test\', data, cfg, weight, \'-out\', \'/usr/local/berrynet/dashboard/www/freeboard/snapshot\']\n    darknet = subprocess.Popen(cmd, bufsize=0,\n            stdin=subprocess.PIPE,\n            stderr=subprocess.STDOUT)\n\n    observer = Observer()\n    observer.schedule(\n        EventHandler([\'*.jpg.done\', \'*.png.done\']),\n        path=WATCH_DIR, recursive=True)\n    observer.start()\n    try:\n        darknet.wait()\n    except KeyboardInterrupt:\n        logging(""Interrupt by user, clean up"")\n        os.kill(darknet.pid, signal.SIGKILL)\n        os.unlink(pidfile)\n'"
